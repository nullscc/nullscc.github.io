
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/19/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/cs.AI_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T12:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/cs.AI_2023_10_09/">cs.AI - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Estimating-Numbers-without-Regression"><a href="#Estimating-Numbers-without-Regression" class="headerlink" title="Estimating Numbers without Regression"></a>Estimating Numbers without Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06204">http://arxiv.org/abs/2310.06204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aastha2104/Parkinson-Disease-Prediction">https://github.com/Aastha2104/Parkinson-Disease-Prediction</a></li>
<li>paper_authors: Avijit Thawani, Jay Pujara, Ashwin Kalyan</li>
<li>for: 提高语言模型对数字的表示能力</li>
<li>methods:  modificare 数字表示方法，包括notation、vocabulary和语言模型架构</li>
<li>results:  Tokenization scheme 可以提高 masked number prediction 性能，而无需大规模修改语言模型架构。<details>
<summary>Abstract</summary>
Despite recent successes in language models, their ability to represent numbers is insufficient. Humans conceptualize numbers based on their magnitudes, effectively projecting them on a number line; whereas subword tokenization fails to explicitly capture magnitude by splitting numbers into arbitrary chunks. To alleviate this shortcoming, alternative approaches have been proposed that modify numbers at various stages of the language modeling pipeline. These methods change either the (1) notation in which numbers are written (\eg scientific vs decimal), the (2) vocabulary used to represent numbers or the entire (3) architecture of the underlying language model, to directly regress to a desired number.   Previous work suggests that architectural change helps achieve state-of-the-art on number estimation but we find an insightful ablation: changing the model's vocabulary instead (\eg introduce a new token for numbers in range 10-100) is a far better trade-off. In the context of masked number prediction, a carefully designed tokenization scheme is both the simplest to implement and sufficient, \ie with similar performance to the state-of-the-art approach that requires making significant architectural changes. Finally, we report similar trends on the downstream task of numerical fact estimation (for Fermi Problems) and discuss reasons behind our findings.
</details>
<details>
<summary>摘要</summary>
尽管最近的语言模型具有了一定的成功，它们对数字的表示仍然不够。人类对数字基于其大小来思考，实际将其投射到数字线上，而分词 Tokenization 则不能明确地捕捉大小。为了解决这个缺陷，有些方法提议在语言模型的不同阶段进行修改。这些方法可以改变（1）数字的notation（例如科学 notation vs 十进制），（2）用于表示数字的词汇，或（3）语言模型的基础结构，以直接预测目标数字。根据我们的研究，对语言模型的结构进行修改可以达到领先的性能，但我们发现一个有趣的ablation：改变模型的词汇（例如引入10-100之间的数字新token）是一个更好的交换。在遮盲数字预测 зада务中，一个精心设计的tokenization scheme是最简单的实现方式，并且具有与采用大量结构修改的状态艺术领先性的相似性。最后，我们报告了相似的趋势在下游任务中（例如数学问题），并讨论了我们的发现的原因。
</details></li>
</ul>
<hr>
<h2 id="Look-Up-mAI-GeMM-Increasing-AI-GeMMs-Performance-by-Nearly-2-5x-via-msGeMM"><a href="#Look-Up-mAI-GeMM-Increasing-AI-GeMMs-Performance-by-Nearly-2-5x-via-msGeMM" class="headerlink" title="Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM"></a>Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06178">http://arxiv.org/abs/2310.06178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Maleki</li>
<li>for: 本研究旨在提出一种新的低精度数据类型算法，以提高人工智能模型的训练和执行效率。</li>
<li>methods: 本研究使用了一种新的msGeMM算法，该算法可以在低精度数据类型下实现AI模型的训练和执行，并且可以减少约2.5倍的乘法和加法指令数量。</li>
<li>results: 本研究的结果表明，msGeMM算法可以在NVIDIA和AMD的GPU上实现AI模型的训练和执行，并且可以提高模型训练和执行的效率。<details>
<summary>Abstract</summary>
AI models are increasing in size and recent advancement in the community has shown that unlike HPC applications where double precision datatype are required, lower-precision datatypes such as fp8 or int4 are sufficient to bring the same model quality both for training and inference. Following these trends, GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8 and int8 GeMM operations with an exceptional performance via Tensor Cores. However, this paper proposes a new algorithm called msGeMM which shows that AI models with low-precision datatypes can run with ~2.5x fewer multiplication and add instructions. Efficient implementation of this algorithm requires special CUDA cores with the ability to add elements from a small look-up table at the rate of Tensor Cores.
</details>
<details>
<summary>摘要</summary>
“人工智能模型不断增大，而最新的社区发展表明，不同于高性能计算应用程序（HPC）中需要双精度数据类型，低精度数据类型如fp8或int4却可以提供同等模型质量 Both for training and inference。随着这些趋势，GPU提供者如NVIDIA和AMD已经添加了硬件支持 дляfp16、fp8和int8 GeMM操作，通过tensor核心实现了非常出色的性能。但本文提出了一新的算法called msGeMM，显示低精度数据类型的AI模型可以透过大约2.5倍的multiplication和add指令数量进行运算。实施此算法需要特殊的CUDA核心，能够快速从小look-up表中添加元素，与tensor核心具有相同的速度。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Factual-and-Personalized-Recommendations-using-Language-Models-and-Reinforcement-Learning"><a href="#Factual-and-Personalized-Recommendations-using-Language-Models-and-Reinforcement-Learning" class="headerlink" title="Factual and Personalized Recommendations using Language Models and Reinforcement Learning"></a>Factual and Personalized Recommendations using Language Models and Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06176">http://arxiv.org/abs/2310.06176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihwan Jeong, Yinlam Chow, Guy Tennenholtz, Chih-Wei Hsu, Azamat Tulepbergenov, Mohammad Ghavamzadeh, Craig Boutilier</li>
<li>for: 这个论文主要旨在开发一种能够为用户提供有趣、个性化、有挑战性的电影推荐系统，通过自然语言交互来匹配用户的偏好。</li>
<li>methods: 该论文使用了一种基于语言模型的推荐系统，其中包括一种基于用户偏好的嵌入空间表示，以及一种基于用户反馈的评价函数。</li>
<li>results: 经过实验 validate，该论文的方法可以在MovieLens 25M 数据集上提供有趣、个性化、有挑战性的电影推荐，并且可以准确地捕捉用户的偏好。<details>
<summary>Abstract</summary>
Recommender systems (RSs) play a central role in connecting users to content, products, and services, matching candidate items to users based on their preferences. While traditional RSs rely on implicit user feedback signals, conversational RSs interact with users in natural language. In this work, we develop a comPelling, Precise, Personalized, Preference-relevant language model (P4LM) that recommends items to users while putting emphasis on explaining item characteristics and their relevance. P4LM uses the embedding space representation of a user's preferences to generate compelling responses that are factually-grounded and relevant w.r.t. the user's preferences. Moreover, we develop a joint reward function that measures precision, appeal, and personalization, which we use as AI-based feedback in a reinforcement learning-based language model framework. Using the MovieLens 25M dataset, we demonstrate that P4LM delivers compelling, personalized movie narratives to users.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:推荐系统（RS）在连接用户与内容、产品和服务方面扮演中心角色，根据用户的偏好来匹配候选项。传统的RS通过隐式用户反馈信号来工作，而对话式RS则通过自然语言与用户交互。在这项工作中，我们开发了一个名为P4LM的语言模型，它可以为用户推荐项目，同时强调解释项目特性以及其与用户偏好的相关性。P4LM使用用户偏好的嵌入空间表示来生成有吸引力和个性化的回应，这些回应与用户偏好相关。此外，我们开发了一个共同奖励函数，该函数衡量精度、吸引力和个性化三个方面，并用于在语言模型框架中作为AI-based反馈。使用MovieLens 25M数据集，我们示示了P4LM可以为用户提供有吸引力和个性化的电影情节。
</details></li>
</ul>
<hr>
<h2 id="How-does-prompt-engineering-affect-ChatGPT-performance-on-unsupervised-entity-resolution"><a href="#How-does-prompt-engineering-affect-ChatGPT-performance-on-unsupervised-entity-resolution" class="headerlink" title="How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?"></a>How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06174">http://arxiv.org/abs/2310.06174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khanin Sisaengsuwanchai, Navapat Nananukul, Mayank Kejriwal</li>
<li>for: 这篇论文是关于实体解析（ER）问题的研究，具体来说是研究如何使用大型自然语言模型（LLM）来自动决定两个实体是否指向同一个基础实体。</li>
<li>methods: 本论文使用了大型自然语言模型（LLM），如ChatGPT，来进行实体解析。研究者通过不同的提示方法来评估LLM的性能，并对不同数据集进行比较。</li>
<li>results: 研究结果显示，提示方法可以很大程度上影响LLM的性能，其中一些指标更加敏感于提示方法的变化。此外，结果还表明了数据集的不同性可能会导致提示方法的不同效果。<details>
<summary>Abstract</summary>
Entity Resolution (ER) is the problem of semi-automatically determining when two entities refer to the same underlying entity, with applications ranging from healthcare to e-commerce. Traditional ER solutions required considerable manual expertise, including feature engineering, as well as identification and curation of training data. In many instances, such techniques are highly dependent on the domain. With recent advent in large language models (LLMs), there is an opportunity to make ER much more seamless and domain-independent. However, it is also well known that LLMs can pose risks, and that the quality of their outputs can depend on so-called prompt engineering. Unfortunately, a systematic experimental study on the effects of different prompting methods for addressing ER, using LLMs like ChatGPT, has been lacking thus far. This paper aims to address this gap by conducting such a study. Although preliminary in nature, our results show that prompting can significantly affect the quality of ER, although it affects some metrics more than others, and can also be dataset dependent.
</details>
<details>
<summary>摘要</summary>
entity resolution (er) 是指自动或半自动地确定两个实体是指同一个基础实体，它在医疗、电子商务等领域有广泛的应用。传统的er解决方案需要较大的人工干预，包括特征工程和训练数据的标识和筛选。在许多情况下，这些技术是域特定的。 however， with the recent advent of large language models (LLMs), there is an opportunity to make er much more seamless and domain-independent. unfortunately, it is also well known that LLMs can pose risks, and the quality of their outputs can depend on so-called prompt engineering. to address this gap, this paper aims to conduct a systematic experimental study on the effects of different prompting methods for addressing er, using LLMs like chatgpt. although preliminary in nature, our results show that prompting can significantly affect the quality of er, although it affects some metrics more than others and can also be dataset dependent.
</details></li>
</ul>
<hr>
<h2 id="Memory-Consistent-Neural-Networks-for-Imitation-Learning"><a href="#Memory-Consistent-Neural-Networks-for-Imitation-Learning" class="headerlink" title="Memory-Consistent Neural Networks for Imitation Learning"></a>Memory-Consistent Neural Networks for Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06171">http://arxiv.org/abs/2310.06171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaustubhsridhar/MCNN">https://github.com/kaustubhsridhar/MCNN</a></li>
<li>paper_authors: Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James Weimer, Insup Lee</li>
<li>For:	+ The paper is written for imitation learning applications, specifically to address the problem of compounding errors in policy synthesis.	+ The authors aim to develop a new method that can learn from expert demonstrations and improve the performance of imitation policies.* Methods:	+ The proposed method is called “memory-consistent neural network” (MCNN), which is a type of deep neural network that is designed to counter the compounding error phenomenon.	+ The MCNN outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical “memory” training samples.	+ The authors provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies.* Results:	+ The authors test the MCNN method on 9 imitation learning tasks, including dexterous robotic manipulation and driving, proprioceptive inputs and visual inputs, and varying sizes and types of demonstration data.	+ They find large and consistent gains in performance using the MCNN method, validating that it is better-suited for imitation learning applications than vanilla deep neural networks.Here is the information in Simplified Chinese text:* For:	+ 这篇论文是为了解决imitition learning应用中的政策合成问题，具体来说是解决错误堆叠问题。	+ 作者们想要开发一种可以从专家示例学习并提高imitition政策的方法。* Methods:	+ 提出的方法是called “memory-consistent neural network” (MCNN)，这是一种特殊的深度神经网络，旨在解决错误堆叠问题。	+ MCNN输出是固定在明确规定的可能区域内的，这些可能区域是基于”memory”训练样本的概念示例。	+ 作者们提供了一个确定的上限 bound дляMCNN政策中的优化性差。* Results:	+ 作者们对9个imitition learning任务进行测试，包括dexterous robotic manipulation和驾驶、 proprioceptive inputs和视觉输入、以及不同的示例数据大小和类型。	+ 他们发现，使用MCNN方法可以获得大量和稳定的性能提升，证明MCNN比vanilla深度神经网络更适合imitition learning应用。<details>
<summary>Abstract</summary>
Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, and Diffusion backbones, spanning dexterous robotic manipulation and driving, proprioceptive inputs and visual inputs, and varying sizes and types of demonstration data, we find large and consistent gains in performance, validating that MCNNs are better-suited than vanilla deep neural networks for imitation learning applications. Website: https://sites.google.com/view/mcnn-imitation
</details>
<details>
<summary>摘要</summary>
“模仿学习可以大幅简化政策生成比较于其他方法，通过利用专家示范的访问。对于这些模仿政策，错误离开训练样本是非常重要的。甚至 rare 的政策动作输出错误可以快速堆积，因为它们导致未知的未来状态， где政策仍然更有可能出错，最终导致任务失败。我们重新考虑简单的监督式“行为克隆”，通过只使用预录的示范来训练政策，但是注意地设计模型类来对错误堆积现象进行抗衡。我们的“记忆一致神经网络”（MCNN）输出是固定的约束在 clearly specified 的允许区域内， anchored 在“记忆”训练样本上。我们提供了对 MCNN 政策的至少优化差的保证上限。使用 MCNN 在 9 个模仿学习任务上，包括多种各种类型的示范数据，我们发现了大量和一致的性能提升，证明 MCNN 更适合于模仿学习应用。网站：https://sites.google.com/view/mcnn-imitation”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Predictable-Artificial-Intelligence"><a href="#Predictable-Artificial-Intelligence" class="headerlink" title="Predictable Artificial Intelligence"></a>Predictable Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06167">http://arxiv.org/abs/2310.06167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duemig/Stanford-Project-Predicting-stock-prices-using-a-LSTM-Network">https://github.com/duemig/Stanford-Project-Predicting-stock-prices-using-a-LSTM-Network</a></li>
<li>paper_authors: Lexin Zhou, Pablo A. Moreno-Casares, Fernando Martínez-Plumed, John Burden, Ryan Burnell, Lucy Cheke, Cèsar Ferri, Alexandru Marcoci, Behzad Mehrbakhsh, Yael Moros-Daval, Seán Ó hÉigeartaigh, Danaja Rutar, Wout Schellaert, Konstantinos Voudouris, José Hernández-Orallo</li>
<li>for: 这篇论文旨在探讨 Predictable AI 这一新兴研究领域的基本思想和挑战。</li>
<li>methods: 本论文使用的方法是阐述 Predictable AI 领域的问题、假设和挑战，并呼吁开发者关注 AI 预测性的问题。</li>
<li>results: 本论文认为，在 AI 预测性方面取得积极进展可以帮助建立更加可靠、负责任、控制、对齐和安全的 AI 生态系统，因此应该在性能之上优先考虑预测性。<details>
<summary>Abstract</summary>
We introduce the fundamental ideas and challenges of Predictable AI, a nascent research area that explores the ways in which we can anticipate key indicators of present and future AI ecosystems. We argue that achieving predictability is crucial for fostering trust, liability, control, alignment and safety of AI ecosystems, and thus should be prioritised over performance. While distinctive from other areas of technical and non-technical AI research, the questions, hypotheses and challenges relevant to Predictable AI were yet to be clearly described. This paper aims to elucidate them, calls for identifying paths towards AI predictability and outlines the potential impact of this emergent field.
</details>
<details>
<summary>摘要</summary>
我团队介绍了人工智能预测的基本想法和挑战，这是一个新兴的研究领域，探讨了如何预测AI生态系统中的关键指标。我们认为，实现预测性是在AI生态系统中建立信任、责任、控制、对齐和安全的关键，因此应该被优先于性能。尽管与其他技术和非技术AI研究领域不同，Predictable AI中的问题、假设和挑战仍未得到清晰描述。这篇论文的目的是为此领域提供定义，呼吁开发AI预测的道路，并详细说明这个新兴领域的潜在影响。
</details></li>
</ul>
<hr>
<h2 id="CAW-coref-Conjunction-Aware-Word-level-Coreference-Resolution"><a href="#CAW-coref-Conjunction-Aware-Word-level-Coreference-Resolution" class="headerlink" title="CAW-coref: Conjunction-Aware Word-level Coreference Resolution"></a>CAW-coref: Conjunction-Aware Word-level Coreference Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06165">http://arxiv.org/abs/2310.06165</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kareldo/wl-coref">https://github.com/kareldo/wl-coref</a></li>
<li>paper_authors: Karel D’Oosterlinck, Semere Kiros Bitew, Brandon Papineau, Christopher Potts, Thomas Demeester, Chris Develder</li>
<li>for: 这个论文的目的是提高Word-level核心参照解决方法的性能，以便在大量文档中进行信息提取。</li>
<li>methods: 这个论文使用了一种简单 yet effective的解决方法，即在Word-level核心参照模型中处理 conjunction  mentions，以提高 OntoNotes 测试集的 F1 分数。</li>
<li>results: 这个解决方法可以提高 OntoNotes 测试集的 F1 分数 by 0.9%，使得 Word-level 核心参照解决方法与 expensive SOTA 方法之间的差距缩小了 34.6%。<details>
<summary>Abstract</summary>
State-of-the-art coreference resolutions systems depend on multiple LLM calls per document and are thus prohibitively expensive for many use cases (e.g., information extraction with large corpora). The leading word-level coreference system (WL-coref) attains 96.6% of these SOTA systems' performance while being much more efficient. In this work, we identify a routine yet important failure case of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We offer a simple yet effective solution that improves the performance on the OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level coreference resolution and expensive SOTA approaches by 34.6%. Our Conjunction-Aware Word-level coreference model (CAW-coref) and code is available at https://github.com/KarelDO/wl-coref.
</details>
<details>
<summary>摘要</summary>
现代核心投引系统取决于文档中多个LLM调用，因此对许多应用场景（如大量文档提取信息）而言是不可持预算的。领先的单词级投引系统（WL-coref）达到了96.6%的SOTA系统性能，而且非常高效。在这项工作中，我们发现了WL-coref中的一种重要且常见失败情况：处理连接的提及，如“Tom和Mary”。我们提出了一种简单 yet有效的解决方案，在OntoNotes测试集上提高了0.9%的F1分，将高效的单词级投引与昂贵的SOTA方法之间的差距缩小了34.6%。我们的Conjunction-Aware Word-level coreference模型（CAW-coref）和代码可以在GitHub上找到：https://github.com/KarelDO/wl-coref。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Transfer-Learning-and-Gradient-Based-Meta-Learning-Techniques"><a href="#Understanding-Transfer-Learning-and-Gradient-Based-Meta-Learning-Techniques" class="headerlink" title="Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques"></a>Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06148">http://arxiv.org/abs/2310.06148</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikehuisman/transfer-meta-feature-representations">https://github.com/mikehuisman/transfer-meta-feature-representations</a></li>
<li>paper_authors: Mike Huisman, Aske Plaat, Jan N. van Rijn</li>
<li>for: 本研究旨在探讨meta-学习技术在不同数据分布下的表现，并比较了finetuning、MAML和Reptile三种方法的性能。</li>
<li>methods: 本研究使用了三种方法：finetuning、MAML和Reptile。finetuning是一种简单的微调方法，MAML是一种基于学习环境的meta-学习技术，Reptile是一种基于精度评估的meta-学习技术。</li>
<li>results: 研究结果表明，在相同数据分布下，finetuning的性能较高，而MAML和Reptile在不同数据分布下的性能较差。此外，研究还发现MAML和Reptile在严重数据缺乏情况下特化于快适应，而finetuning可以背景学习。最后，研究发现finetuning学习的特征为得到的特征更加多样和特异。<details>
<summary>Abstract</summary>
Deep neural networks can yield good performance on various tasks but often require large amounts of data to train them. Meta-learning received considerable attention as one approach to improve the generalization of these networks from a limited amount of data. Whilst meta-learning techniques have been observed to be successful at this in various scenarios, recent results suggest that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques such as MAML, which is one of the most popular meta-learning techniques. This is surprising as the learning behaviour of MAML mimics that of finetuning: both rely on re-using learned features. We investigate the observed performance differences between finetuning, MAML, and another meta-learning technique called Reptile, and show that MAML and Reptile specialize for fast adaptation in low-data regimes of similar data distribution as the one used for training. Our findings show that both the output layer and the noisy training conditions induced by data scarcity play important roles in facilitating this specialization for MAML. Lastly, we show that the pre-trained features as obtained by the finetuning baseline are more diverse and discriminative than those learned by MAML and Reptile. Due to this lack of diversity and distribution specialization, MAML and Reptile may fail to generalize to out-of-distribution tasks whereas finetuning can fall back on the diversity of the learned features.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reinforcement-Learning-in-the-Era-of-LLMs-What-is-Essential-What-is-needed-An-RL-Perspective-on-RLHF-Prompting-and-Beyond"><a href="#Reinforcement-Learning-in-the-Era-of-LLMs-What-is-Essential-What-is-needed-An-RL-Perspective-on-RLHF-Prompting-and-Beyond" class="headerlink" title="Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond"></a>Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06147">http://arxiv.org/abs/2310.06147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Sun</li>
<li>for: 这 paper 的目的是将传统RL与LLM研究中使用的RL技术相连接，解释RL在LLM中的优势和应用场景。</li>
<li>methods: 这 paper 使用了RLHF技术，具体来说是在线 inverse RL with offline demonstration data，并与 SFT 进行比较。</li>
<li>results: 这 paper 发现RLHF比 SFT 更为有利，因为它可以减少练习数据中的问题折衔。此外，RLHF 可以应用于其他 LLM 任务，如提问评估和优化，即使它们的反馈也是昂贵的。但RLHF 的策略学习更加具有挑战，因为它们的动作维度很高，并且反馈稀缺。<details>
<summary>Abstract</summary>
Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research.   Highlighted Takeaways:   1. RLHF is Online Inverse RL with Offline Demonstration Data.   2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior Cloning (BC) by alleviating the problem of compounding error.   3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evaluation and optimization where feedback is also expensive.   4. The policy learning in RLHF is more challenging than conventional problems studied in IRL due to their high action dimensionality and feedback sparsity.   5. The main superiority of PPO over off-policy value-based methods is its stability gained from (almost) on-policy data and conservative policy updates.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）的进步引起了广泛的关注，如ChatGPT和GPT-4等产品。它们的遵循指令和提供无害、有益和诚实（3H）回复的能力归功于人类反馈学习（RLHF）的技术。在这篇论文中，我们想要将传统RL研究与LLM研究中的RL技术相连。通过解释RLHF的优势和应用场景，我们希望能够启发更多的研究者关注和投入到这一领域。突出的摘要：1. RLHF是在线反RL与离线示例数据的组合。2. RLHF比SFT更高效，因为假设学习（和反RL）比Behavior Cloning（BC）更高效，因为它可以解决复杂的错误问题。3. RM步骤在RLHF中生成了贵重的人类反馈的代理，这种理解可以推广到其他LLM任务，如提问评估和优化，其中Feedback也是贵重的。4. RLHF中策略学习比传统IRL中的问题更加挑战，因为它们具有高动作维度和反馈稀缺性。5. PPO在比值基方法更稳定，因为它在（近乎）在policy上的数据上学习，并且保守地更新策略。
</details></li>
</ul>
<hr>
<h2 id="Layout-Sequence-Prediction-From-Noisy-Mobile-Modality"><a href="#Layout-Sequence-Prediction-From-Noisy-Mobile-Modality" class="headerlink" title="Layout Sequence Prediction From Noisy Mobile Modality"></a>Layout Sequence Prediction From Noisy Mobile Modality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06138">http://arxiv.org/abs/2310.06138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Hai-chao-Zhang/LTrajDiff">https://github.com/Hai-chao-Zhang/LTrajDiff</a></li>
<li>paper_authors: Haichao Zhang, Yi Xu, Hongsheng Lu, Takayuki Shimizu, Yun Fu</li>
<li>for: 本研究旨在解决现实世界中 Layout sequence 和 trajectory prediction 模型所遇到的挑战，使得可以正确地预测行人 bounding box 的 trajectory。</li>
<li>methods: 我们提出了 LTrajDiff，一种新的方法，它将干扰或遮盲的对象视为与完全可见的对象一样重要。LTrajDiff 使用了来自移动设备的感知数据，但是也引入了新的挑战，如模式融合、噪声数据和缺失空间布局和对象大小信息。我们使用了一种杜因采样模型，通过粗细到细的扩散策略，将噪声数据融合到精度 Layout sequence 中。</li>
<li>results: 我们的模型在随机遮盲和非常短输入实验中达到了 SOTA Result，证明了我们的方法可以准确地预测行人 bounding box 的 trajectory，并且可以在实际世界中使用感知数据来预测 pedestrian movement。<details>
<summary>Abstract</summary>
Trajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model predicts layout sequences by implicitly inferring object size and projection status from a single reference timestamp or significantly obstructed sequences. Achieving SOTA results in randomly obstructed experiments and extremely short input experiments, our model illustrates the effectiveness of leveraging noisy mobile data. In summary, our approach offers a promising solution to the challenges faced by layout sequence and trajectory prediction models in real-world settings, paving the way for utilizing sensor data from mobile phones to accurately predict pedestrian bounding box trajectories. To the best of our knowledge, this is the first work that addresses severely obstructed and extremely short layout sequences by combining vision with noisy mobile modality, making it the pioneering work in the field of layout sequence trajectory prediction.
</details>
<details>
<summary>摘要</summary>
atrajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model predicts layout sequences by implicitly inferring object size and projection status from a single reference timestamp or significantly obstructed sequences. Achieving SOTA results in randomly obstructed experiments and extremely short input experiments, our model illustrates the effectiveness of leveraging noisy mobile data. In summary, our approach offers a promising solution to the challenges faced by layout sequence and trajectory prediction models in real-world settings, paving the way for utilizing sensor data from mobile phones to accurately predict pedestrian bounding box trajectories. To the best of our knowledge, this is the first work that addresses severely obstructed and extremely short layout sequences by combining vision with noisy mobile modality, making it the pioneering work in the field of layout sequence trajectory prediction.
</details></li>
</ul>
<hr>
<h2 id="Learning-Layer-wise-Equivariances-Automatically-using-Gradients"><a href="#Learning-Layer-wise-Equivariances-Automatically-using-Gradients" class="headerlink" title="Learning Layer-wise Equivariances Automatically using Gradients"></a>Learning Layer-wise Equivariances Automatically using Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06131">http://arxiv.org/abs/2310.06131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tychovdo/ella">https://github.com/tychovdo/ella</a></li>
<li>paper_authors: Tycho F. A. van der Ouderaa, Alexander Immer, Mark van der Wilk</li>
<li>for: 提高神经网络的泛化性能，使其更好地适应不同的输入数据。</li>
<li>methods: 使用权重相互连接结构和梯度下降法自适应地学习层 wise 对称性。</li>
<li>results: 在图像分类任务上，自动学习层 wise 对称性可以达到与固定编码的对称性相同或更好的性能。<details>
<summary>Abstract</summary>
Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in deep networks. We demonstrate the ability to automatically learn layer-wise equivariances on image classification tasks, achieving equivalent or improved performance over baselines with hard-coded symmetry.
</details>
<details>
<summary>摘要</summary>
将文本翻译成简化中文。</SYS>卷积层可以将等价对称性 encode到神经网络中，导致更好的泛化性表现。然而，对称性提供硬coded的约束，需要在预先指定，并不能适应。我们的目标是让柔性的对称约束，可以通过梯度学习自动从数据中学习。学习对称和相关的权重连接结构从零开始很困难，因为它们需要有效的和灵活的层wise equivariant parameterization。其次，对称性作为约束，因此不会被训练损失奖励。为了解决这些挑战，我们改进了软对称 parameterization，并通过优化 marginal likelihood，使用可微 differentiable Laplace approximations来学习层wise equivariance。该目标平衡数据适应和模型复杂度，使得层wise symmetry discovery在深度网络中自动学习。我们在图像分类任务上展示了自动学习层wise equivariance的能力，与硬编码的对称性具有相同或改进的性能。
</details></li>
</ul>
<hr>
<h2 id="On-Time-Domain-Conformer-Models-for-Monaural-Speech-Separation-in-Noisy-Reverberant-Acoustic-Environments"><a href="#On-Time-Domain-Conformer-Models-for-Monaural-Speech-Separation-in-Noisy-Reverberant-Acoustic-Environments" class="headerlink" title="On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments"></a>On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06125">http://arxiv.org/abs/2310.06125</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jwr1995/pubsep">https://github.com/jwr1995/pubsep</a></li>
<li>paper_authors: William Ravenscroft, Stefan Goetze, Thomas Hain</li>
<li>for: 这篇论文主要针对多mic麦克风技术领域的speech separation问题进行研究。</li>
<li>methods: 这篇论文使用了卷积扩展器（Conformers），它们在多种speech处理任务中表现良好，但在speech separation领域尚未得到充分研究。最近的state-of-the-art（SOTA）分离模型主要是时域音频分离网络（TasNets）。一些成功的模型使用了双路（DP）网络，它们在本地和全局信息的序列处理中做出了优秀的表现。</li>
<li>results: 在实际的短信号长度下，TD-Conformers在控制特征维度时表现更高效。 authors proposed subsampling layers to further improve computational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB SISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.<details>
<summary>Abstract</summary>
Speech separation remains an important topic for multi-speaker technology researchers. Convolution augmented transformers (conformers) have performed well for many speech processing tasks but have been under-researched for speech separation. Most recent state-of-the-art (SOTA) separation models have been time-domain audio separation networks (TasNets). A number of successful models have made use of dual-path (DP) networks which sequentially process local and global information. Time domain conformers (TD-Conformers) are an analogue of the DP approach in that they also process local and global context sequentially but have a different time complexity function. It is shown that for realistic shorter signal lengths, conformers are more efficient when controlling for feature dimension. Subsampling layers are proposed to further improve computational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB SISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传送文本到简化中文。</SYS>>研究多говор话技术的人们认为，语音分离仍然是一个重要的话题。卷积加强变换器（conformers）在许多语音处理任务中表现良好，但是它们在语音分离方面尚未得到足够的研究。最近的最佳状态（SOTA）分离模型主要是时域音频分离网络（TasNets）。一些成功的模型具有双路（DP）网络，这些网络先后处理本地和全局信息。时域卷积器（TD-Conformers）是DP方法的同义词，它们也在本地和全局上下文中进行顺序处理，但是它们的时间复杂度函数不同。研究发现，对于更加现实的信号长度，TD-Conformers在控制特征维度时更加高效。抽样层被提议来进一步提高计算效率。最佳TD-Conformer在WHAMR和WSJ0-2Mix测试集上分别提高了14.6dB和21.2dB的SISDR指标。
</details></li>
</ul>
<hr>
<h2 id="Text-driven-Prompt-Generation-for-Vision-Language-Models-in-Federated-Learning"><a href="#Text-driven-Prompt-Generation-for-Vision-Language-Models-in-Federated-Learning" class="headerlink" title="Text-driven Prompt Generation for Vision-Language Models in Federated Learning"></a>Text-driven Prompt Generation for Vision-Language Models in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06123">http://arxiv.org/abs/2310.06123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Qiu, Xingyu Li, Chaithanya Kumar Mummadi, Madan Ravi Ganesh, Zhenzhen Li, Lu Peng, Wan-Yi Lin</li>
<li>for: 这个研究旨在提出一种整合多个远端客户的 Federated Text-driven Prompt Generation（FedTPG）方法，以实现视觉语言模型的普遍化。</li>
<li>methods: 这个方法使用了一个内置的文本输入，并通过一个专门的生成网络来学习文本提示。这个生成网络是基于任务相关的文本输入，因此具有内在的内容感知能力，可以对未见过的类别进行普遍化。</li>
<li>results: 我们的实验结果显示，这个方法在九个多标的图像分类任务上比较出色，可以对已知和未知的类别进行更好的普遍化，并且可以应用于新的数据集。<details>
<summary>Abstract</summary>
Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes and is also generalizable to unseen datasets.
</details>
<details>
<summary>摘要</summary>
Prompt learning for vision-language models, such as CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, achieving overall better generalization on both seen and unseen classes and is also generalizable to unseen datasets.Here's the translation breakdown:* "Prompt learning" is translated as "提示学习" (tíshì xuéxí)* "Vision-language models" is translated as "视觉语言模型" (wèi jiàn yǔ yán módel)* "CoOp" is translated as "CoOp" (同义词)* "CLIP" is translated as "CLIP" (同义词)* "Federated learning" is translated as "联合学习" (liánhé xuéxí)* "Hand-crafted text prompts" is translated as "手工编写的文本提示" (shǒu gōng biān xī de wén tiě zhǐ)* "Learned vectors" is translated as "学习后的 вектор" (xuéxí hòu de vector)* "Task-related text input" is translated as "任务相关的文本输入" (tâi yè xiāngguān de wén tiě shūrū)* "Context-aware" is translated as "Context-aware" (同义词)* "Federated Text-driven Prompt Generation" is translated as "联合文本驱动提示生成" (liánhé wén tiě qiú xíng chǎng zhǐ jiàn)* "Existing federated prompt learning methods" is translated as "现有的联合提示学习方法" (xiàn yǒu de liánhé zhǐ xuéxí fāngfa)* "Generalize to unseen classes" is translated as "泛化到未见类" (guānghuà dào wèi jiàn lèi)* "Our comprehensive empirical evaluations" is translated as "我们的全面实验评估" (wǒmen de quánxiān shíyìn zhìshì)Note that the translation is based on the standard Simplified Chinese pronunciation and may vary depending on the specific dialect or accent.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Progress-in-Multivariate-Time-Series-Forecasting-Comprehensive-Benchmarking-and-Heterogeneity-Analysis"><a href="#Exploring-Progress-in-Multivariate-Time-Series-Forecasting-Comprehensive-Benchmarking-and-Heterogeneity-Analysis" class="headerlink" title="Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis"></a>Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06119">http://arxiv.org/abs/2310.06119</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zezhishao/basicts">https://github.com/zezhishao/basicts</a></li>
<li>paper_authors: Zezhi Shao, Fei Wang, Yongjun Xu, Wei Wei, Chengqing Yu, Zhao Zhang, Di Yao, Guangyin Jin, Xin Cao, Gao Cong, Christian S. Jensen, Xueqi Cheng</li>
<li>for: 本研究旨在解决现有的评价缺陷和技术方法选择争议，提供关于多变量时间序列预测（MTS）领域进步的深入理解。</li>
<li>methods: 本研究提出了一个名为BasicTS的比较平台，用于公正地评价多变量时间序列预测模型。 BasicTS 设置了一个标准的训练管道和合理的评价标准，使得评估了超过 30 种常见 MTS 预测模型的性能。</li>
<li>results: 研究发现，现有的 MTS 预测模型在不同的时间和空间特征下表现有很大差异。 BasicTS 可以帮助研究人员选择和设计适合的 MTS 预测模型，并提供了多个可重现的性能和效率比较结果。<details>
<summary>Abstract</summary>
Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting models on more than 18 datasets. Furthermore, we highlight the heterogeneity among MTS datasets and classify them based on temporal and spatial characteristics. We further prove that neglecting heterogeneity is the primary reason for generating controversies in technical approaches. Moreover, based on the proposed BasicTS and rich heterogeneous MTS datasets, we conduct an exhaustive and reproducible performance and efficiency comparison of popular models, providing insights for researchers in selecting and designing MTS forecasting models.
</details>
<details>
<summary>摘要</summary>
多变量时间系列（MTS）广泛存在在实际世界复杂系统中，如交通和能源系统，其预测对这些系统的理解和影响是关键。在最近几年，深度学习基于方法在MTS预测中得到了很多欢迎，特别是在长期时间序列预测（LTSF）和空间-时间预测（STF）中。然而，实际工作中的公平比较问题和技术方法选择问题一直是热点议题。这些争议很大程度上阻碍了我们对这个领域的进步的理解。因此，这篇论文旨在解决这些争议，提供关于领域的进步的新视角。为了解决公平比较问题，我们提出了BasicTS，一个用于公平比较的 benchmark。BasicTS 设计了一个统一的训练管道和合理的评估设置，使得不受偏见的评估了超过30种常见MTS预测模型在18个数据集上。此外，我们还发现了MTS数据集中的多样性，并将其分为了时间和空间特征的两类。我们还证明了忽略多样性是预测技术方法中的主要问题。此外，基于我们提出的BasicTS和丰富的多样化MTS数据集，我们进行了广泛和可重复的性和效率比较，为研究人员提供了选择和设计MTS预测模型的新的指导思想。
</details></li>
</ul>
<hr>
<h2 id="Take-a-Step-Back-Evoking-Reasoning-via-Abstraction-in-Large-Language-Models"><a href="#Take-a-Step-Back-Evoking-Reasoning-via-Abstraction-in-Large-Language-Models" class="headerlink" title="Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models"></a>Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06117">http://arxiv.org/abs/2310.06117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, Denny Zhou</li>
<li>for: 提高LLM的抽象能力，使其能够从具体情况中提取高级概念和原则，以便更好地进行正确的逻辑推理。</li>
<li>methods: 使用Step-Back Prompting技术，通过提供高级概念和原则来引导LLM的推理步骤，使其能够更好地遵循正确的逻辑推理路径。</li>
<li>results: 在多种复杂的逻辑推理任务中，使用Step-Back Prompting技术可以提高PaLM-2L模型的性能，比如物理和化学知识测验（MMLU Physics和Chemistry）上提高7%和11%，时间问答（TimeQA）上提高27%，以及多步逻辑推理任务（MuSiQue）上提高7%。<details>
<summary>Abstract</summary>
We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
</details>
<details>
<summary>摘要</summary>
我们提出Step-Back Prompting，一种简单的提示技术，让机器学习模型（LLMs）从具体的实例中抽象出高水平概念和基本假设，然后使用这些概念和假设来引导逻辑步骤，以提高LLMs的解释正确性。我们在PaLM-2L模型上进行实验，并观察到了广泛的应用数据领域中的表现优化，包括STEM、知识问题答案和多步逻辑等。例如，Step-Back Prompting在物理和化学MMLU中提高PaLM-2L表现的比例为7%和11%，在TimeQA中提高27%，在MuSiQue中提高7%。
</details></li>
</ul>
<hr>
<h2 id="OptiMUS-Optimization-Modeling-Using-MIP-Solvers-and-large-language-models"><a href="#OptiMUS-Optimization-Modeling-Using-MIP-Solvers-and-large-language-models" class="headerlink" title="OptiMUS: Optimization Modeling Using MIP Solvers and large language models"></a>OptiMUS: Optimization Modeling Using MIP Solvers and large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06116">http://arxiv.org/abs/2310.06116</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teshnizi/optimus">https://github.com/teshnizi/optimus</a></li>
<li>paper_authors: Ali AhmadiTeshnizi, Wenzhi Gao, Madeleine Udell</li>
<li>for: 这个论文是为了提供一种基于自然语言描述的优化问题解决方案。</li>
<li>methods: 该论文使用了大语言模型（LLM）来解决优化问题，包括发展数学模型、编写和调试解决方案代码、开发测试、检查生成解决方案的有效性。</li>
<li>results: 试验表明，OptiMUS可以比基本的LLM提示策略多解决优化问题。<details>
<summary>Abstract</summary>
Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS solves nearly twice as many problems as a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}
</details>
<details>
<summary>摘要</summary>
优化问题在不同领域广泛存在，从制造和分布到医疗。然而，大多数这些问题仍然通过手动规则来解决而不是使用当前的优化解决方案，因为解决这些问题所需的专业知识限制了优化工具和技术的普及。我们介绍OptiMUS，一个基于大语言模型（LLM）的代理人，可以从自然语言描述中形式化和解决优化问题。OptiMUS可以开发数学模型，编写和调试解决器代码，开发测试，并检查生成的解决方案的有效性。为了评估我们的代理人，我们提出了NLP4LP数据集，一个新的线性 программирова（LP）和混合整数线性程序（MILP）问题的数据集。我们的实验表明，OptiMUS可以比基本的LLM提示策略多 solves一半的问题。OptiMUS代码和NLP4LP数据集可以在<https://github.com/teshnizi/OptiMUS>获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Interactive-Real-World-Simulators"><a href="#Learning-Interactive-Real-World-Simulators" class="headerlink" title="Learning Interactive Real-World Simulators"></a>Learning Interactive Real-World Simulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06114">http://arxiv.org/abs/2310.06114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, Pieter Abbeel</li>
<li>for: 这篇论文的目的是学习一个 универсаль的实际世界模拟器（UniSim），用于模拟人类和机器人之间的互动。</li>
<li>methods: 这篇论文使用生成模型来学习不同类型的数据，包括图像、视频和机器人数据，以实现真实的实际世界 simulate。</li>
<li>results: 这篇论文的实验结果表明，通过在UniSim中训练高级视觉语言规划和低级强化学习策略，可以在真实世界中展示零批量训练的功能。此外，视频描述模型也可以通过与Simulink进行培训，提高其应用范围。<details>
<summary>Abstract</summary>
Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as "open the drawer" and low-level controls such as "move by x, y" from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications. Video demos can be found at https://universal-simulator.github.io.
</details>
<details>
<summary>摘要</summary>
优化模型在互联网数据上进行训练已经革命化了文本、图像和视频内容的创建方式。可能下一个里程碑 для优化模型是模拟人类、机器人和其他交互代理的真实经验，响应于人类和机器人的行为。我们探讨了通过生成模型学习的UniSim universal simulator，以模拟人类和代理之间的互动。我们发现了自然数据集的重要观察：各种数据集在不同的轴上充满着数据（例如图像数据中的充满物体、机器人数据中的紧密的动作和导航数据中的多种运动）。通过综合考虑这些不同的数据集，UniSim可以模拟人类和代理在世界中交互的方式，包括通过高级指令如“打开抽屉”和低级控制如“移动by x, y”来模拟静止场景和物体的视觉结果。这种真实世界模拟器有很多应用场景。例如，我们使用UniSim训练高级视力语言规划和低级强化学习策略，它们在唯一学习的真实世界模拟器中展现出零基础真实世界传递。此外，我们还发现了训练在UniSim中的视频描述模型可以受益于实际经验，开阔了更广泛的应用领域。视频 demo 可以在 <https://universal-simulator.github.io> 找到。
</details></li>
</ul>
<hr>
<h2 id="When-is-Agnostic-Reinforcement-Learning-Statistically-Tractable"><a href="#When-is-Agnostic-Reinforcement-Learning-Statistically-Tractable" class="headerlink" title="When is Agnostic Reinforcement Learning Statistically Tractable?"></a>When is Agnostic Reinforcement Learning Statistically Tractable?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06113">http://arxiv.org/abs/2310.06113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Jia, Gene Li, Alexander Rakhlin, Ayush Sekhari, Nathan Srebro</li>
<li>for: 学习一个 unknown MDP 中的 $\epsilon$-优秀策略， Given a policy class $\Pi$。</li>
<li>methods: 使用一种新的复杂度度量——\emph{spanning capacity}，该度量只取决于集合 $\Pi$ 而不依赖于 MDP 动态。</li>
<li>results: 显示存在一个 policy class $\Pi$ 的 bounded spanning capacity 可以学习，但是需要 superpolynomial 数量的样本。此外，我们还提出了一种新的算法 called POPLER，可以实现 statistically efficient online RL。<details>
<summary>Abstract</summary>
We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\epsilon$-suboptimal policy with respect to $\Pi$? Towards that end, we introduce a new complexity measure, called the \emph{spanning capacity}, that depends solely on the set $\Pi$ and is independent of the MDP dynamics. With a generative model, we show that for any policy class $\Pi$, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class $\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \emph{sunflower} structure, which in conjunction with bounded spanning capacity enables statistically efficient online RL via a new algorithm called POPLER, which takes inspiration from classical importance sampling methods as well as techniques for reachable-state identification and policy evaluation in reward-free exploration.
</details>
<details>
<summary>摘要</summary>
我们研究无知RL问题（Policy Optimization with Unknown Dynamics）：给定一个策略集合 $\Pi$，何时需要多少回交互 avec一个未知MDP（可能具有很大的状态和动作空间）以学习一个 $\epsilon$-优化策略？为了解决这个问题，我们引入了一个新的复杂度度量，即 \emph{spanning capacity}，这个度量只取决于集合 $\Pi$，与MDP动力完全无关。使用生成模型，我们证明了任何策略集合 $\Pi$ 的 bounded spanning capacity 是PAC学习可能的。然而，在在线RL中，情况更加复杂。我们证明了存在一个策略集合 $\Pi$ 的 bounded spanning capacity 需要超polynomial数量的样本来学习。这表明了无知学习中的分开性，在生成访问模型和在线访问模型之间，以及在渐进性和随机MDP之间。然而，我们还发现了一种附加的 \emph{sunflower} 结构，它可以在 conjunction  WITH bounded spanning capacity 使得在线RL可以通过一种新的算法called POPLER来实现，这个算法结合了古典的重要性抽象方法以及探索和策略评估技术在奖励free探索中。
</details></li>
</ul>
<hr>
<h2 id="High-Dimensional-Causal-Inference-with-Variational-Backdoor-Adjustment"><a href="#High-Dimensional-Causal-Inference-with-Variational-Backdoor-Adjustment" class="headerlink" title="High Dimensional Causal Inference with Variational Backdoor Adjustment"></a>High Dimensional Causal Inference with Variational Backdoor Adjustment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06100">http://arxiv.org/abs/2310.06100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danielmisrael/variational-backdoor-adjustment">https://github.com/danielmisrael/variational-backdoor-adjustment</a></li>
<li>paper_authors: Daniel Israel, Aditya Grover, Guy Van den Broeck</li>
<li>for: 这篇论文旨在应用后门调整方法来估计干扰量，并解决高维度干扰和变量的问题。</li>
<li>methods: 本论文使用生成模型来实现后门调整，并将后门调整视为variational推导中的优化问题。</li>
<li>results: 实验结果显示，本方法能够在高维度设置下估计干扰likelihood，并在各种高维度应用中实现成功。<details>
<summary>Abstract</summary>
Backdoor adjustment is a technique in causal inference for estimating interventional quantities from purely observational data. For example, in medical settings, backdoor adjustment can be used to control for confounding and estimate the effectiveness of a treatment. However, high dimensional treatments and confounders pose a series of potential pitfalls: tractability, identifiability, optimization. In this work, we take a generative modeling approach to backdoor adjustment for high dimensional treatments and confounders. We cast backdoor adjustment as an optimization problem in variational inference without reliance on proxy variables and hidden confounders. Empirically, our method is able to estimate interventional likelihood in a variety of high dimensional settings, including semi-synthetic X-ray medical data. To the best of our knowledge, this is the first application of backdoor adjustment in which all the relevant variables are high dimensional.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用后门调整技术来估计从完全观察数据中的干预量。例如，在医疗设置下，后门调整可以控制干预因素和估计治疗效果。然而，高维度干预和干预因素可能存在一系列潜在的陷阱：可追踪性、可识别性和优化。在这项工作中，我们采用生成模型方法来实现后门调整。我们将后门调整视为变量推断中的优化问题，而不需要使用代理变量和隐藏干预因素。实际上，我们的方法可以在高维度设置下估计干预概率，包括半人工X射数据等多种高维度设置。根据我们知道，这是首次在所有相关变量都是高维度情况下应用后门调整。
</details></li>
</ul>
<hr>
<h2 id="Predictive-auxiliary-objectives-in-deep-RL-mimic-learning-in-the-brain"><a href="#Predictive-auxiliary-objectives-in-deep-RL-mimic-learning-in-the-brain" class="headerlink" title="Predictive auxiliary objectives in deep RL mimic learning in the brain"></a>Predictive auxiliary objectives in deep RL mimic learning in the brain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06089">http://arxiv.org/abs/2310.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ching Fang, Kimberly L Stachenfeld<br>for:This paper explores the use of predictive auxiliary objectives in deep reinforcement learning (RL) to support representation learning and improve task performance.methods:The paper uses a deep RL system with self-supervised auxiliary objectives to study the effects of predictive learning on representation learning across different modules of the system.results:The paper finds that predictive objectives improve and stabilize learning, particularly in resource-limited architectures, and identifies settings where longer predictive horizons better support representational transfer. Additionally, the paper finds that representational changes in the RL system bear a striking resemblance to changes in neural activity observed in the brain.<details>
<summary>Abstract</summary>
The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the RL system and hippocampus, an area thought to learn a predictive model to support memory-guided behavior. We also connect the encoder network and the value learning network of the RL system to visual cortex and striatum in the brain, respectively. This work demonstrates how representation learning in deep RL systems can provide an interpretable framework for modeling multi-region interactions in the brain. The deep RL perspective taken here also suggests an additional role of the hippocampus in the brain -- that of an auxiliary learning system that benefits representation learning in other regions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Performative-Time-Series-Forecasting"><a href="#Performative-Time-Series-Forecasting" class="headerlink" title="Performative Time-Series Forecasting"></a>Performative Time-Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06077">http://arxiv.org/abs/2310.06077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityalab/pets">https://github.com/adityalab/pets</a></li>
<li>paper_authors: Zhiyuan Zhao, Alexander Rodriguez, B. Aditya Prakash</li>
<li>for: 本文旨在解决时间序列预测中的回馈循环问题，即预测结果可能会影响实际结果，从而改变预测目标变量的分布。</li>
<li>methods: 本文提出了一种新的方法Feature Performative-Shifting（FPS），利用延迟响应来预测分布的变化，并根据此预测目标变量。</li>
<li>results: 实验结果表明，FPS方法可以有效地处理回馈循环引起的挑战，并在COVID-19和交通预测任务中表现出优于传统时间序列预测方法。<details>
<summary>Abstract</summary>
Time-series forecasting is a critical challenge in various domains and has witnessed substantial progress in recent years. Many real-life scenarios, such as public health, economics, and social applications, involve feedback loops where predictions can influence the predicted outcome, subsequently altering the target variable's distribution. This phenomenon, known as performativity, introduces the potential for 'self-negating' or 'self-fulfilling' predictions. Despite extensive studies in classification problems across domains, performativity remains largely unexplored in the context of time-series forecasting from a machine-learning perspective.   In this paper, we formalize performative time-series forecasting (PeTS), addressing the challenge of accurate predictions when performativity-induced distribution shifts are possible. We propose a novel approach, Feature Performative-Shifting (FPS), which leverages the concept of delayed response to anticipate distribution shifts and subsequently predicts targets accordingly. We provide theoretical insights suggesting that FPS can potentially lead to reduced generalization error. We conduct comprehensive experiments using multiple time-series models on COVID-19 and traffic forecasting tasks. The results demonstrate that FPS consistently outperforms conventional time-series forecasting methods, highlighting its efficacy in handling performativity-induced challenges.
</details>
<details>
<summary>摘要</summary>
时间序列预测是各个领域中的一项重要挑战，在过去几年中得到了重要进展。许多实际场景，如公共卫生、经济和社会应用，都存在反馈循环，其中预测结果可能会影响预测结果的分布，从而导致“自我实现”或“自我否定”的预测。这种现象被称为“表现力”，它在机器学习角度来看，尚未在时间序列预测中得到了广泛的研究。在这篇论文中，我们正式定义了表现力时间序列预测（PeTS），即在预测过程中考虑表现力引起的分布变化的挑战。我们提出了一种新的方法，即特征表现滚动（FPS），它利用延迟应答来预测分布变化，并根据此预测目标。我们提供了理论分析，表明FPS可能会减少泛化误差。我们在COVID-19和交通预测任务上进行了广泛的实验，结果表明FPS在处理表现力引起的挑战时表现出色，高于传统时间序列预测方法。
</details></li>
</ul>
<hr>
<h2 id="Pain-Forecasting-using-Self-supervised-Learning-and-Patient-Phenotyping-An-attempt-to-prevent-Opioid-Addiction"><a href="#Pain-Forecasting-using-Self-supervised-Learning-and-Patient-Phenotyping-An-attempt-to-prevent-Opioid-Addiction" class="headerlink" title="Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction"></a>Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06075">http://arxiv.org/abs/2310.06075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swati Padhee, Tanvi Banerjee, Daniel M. Abrams, Nirmish Shah</li>
<li>for: 本研究旨在预测患有抗阻塞综合症（SCD）的患者未来疼痛轨迹，以提高他们的生活质量而无需妥协他们的治疗。</li>
<li>methods: 本研究使用自动学习方法来解决疼痛预测问题，并对时间序列数据进行归类，以分类患者的亚群并提供个性化的治疗方案。</li>
<li>results: 实验结果显示，我们的模型在五年的实际数据上表现出色，超过了现有的标准准则，并可以准确地分类患者，提供有价值的临床决策信息。<details>
<summary>Abstract</summary>
Sickle Cell Disease (SCD) is a chronic genetic disorder characterized by recurrent acute painful episodes. Opioids are often used to manage these painful episodes; the extent of their use in managing pain in this disorder is an issue of debate. The risk of addiction and side effects of these opioid treatments can often lead to more pain episodes in the future. Hence, it is crucial to forecast future patient pain trajectories to help patients manage their SCD to improve their quality of life without compromising their treatment. It is challenging to obtain many pain records to design forecasting models since it is mainly recorded by patients' self-report. Therefore, it is expensive and painful (due to the need for patient compliance) to solve pain forecasting problems in a purely supervised manner. In light of this challenge, we propose to solve the pain forecasting problem using self-supervised learning methods. Also, clustering such time-series data is crucial for patient phenotyping, anticipating patients' prognoses by identifying "similar" patients, and designing treatment guidelines tailored to homogeneous patient subgroups. Hence, we propose a self-supervised learning approach for clustering time-series data, where each cluster comprises patients who share similar future pain profiles. Experiments on five years of real-world datasets show that our models achieve superior performance over state-of-the-art benchmarks and identify meaningful clusters that can be translated into actionable information for clinical decision-making.
</details>
<details>
<summary>摘要</summary>
针对患有悉尼细胞病（SCD）的患者，我们提出了一种基于自我监督学习的痛情预测方法。这种方法可以帮助患者更好地管理自己的病情，提高生活质量，而不需要妥协对治疗的影响。由于痛情记录的收集是主要由患者自己报告，因此收集数据的成本很高，而且需要患者的合作性，这使得解决痛情预测问题在完全监督方式下是非常困难的。为了解决这个问题，我们提出了一种基于自我监督学习的时间序列数据划分方法，每个分组包含拥有相似未来痛情轨迹的患者。我们对实际数据进行五年的实验，结果表明我们的模型在比较状态的标准准的基础上表现出色，并能够分配有意义的分组，这些分组可以被翻译成临床决策中的有用信息。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Vision-Based-Human-Pose-Estimation-with-Rotation-Matrix"><a href="#Augmenting-Vision-Based-Human-Pose-Estimation-with-Rotation-Matrix" class="headerlink" title="Augmenting Vision-Based Human Pose Estimation with Rotation Matrix"></a>Augmenting Vision-Based Human Pose Estimation with Rotation Matrix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06068">http://arxiv.org/abs/2310.06068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milad Vazan, Fatemeh Sadat Masoumi, Ruizhi Ou, Reza Rawassizadeh</li>
<li>for: 本研究旨在提高基于姿势估计的活动识别精度，通过结合pose estimation和novel数据增强方法。</li>
<li>methods: 本研究使用pose estimation和一种新的数据增强方法，即旋转矩阵，来增强活动识别的精度。</li>
<li>results: 经过我们的实验，我们发现使用SVM与SGD优化，并结合旋转矩阵数据增强方法，可以达到96%的活动识别精度，而不使用数据增强方法的基准精度只有64%。<details>
<summary>Abstract</summary>
Fitness applications are commonly used to monitor activities within the gym, but they often fail to automatically track indoor activities inside the gym. This study proposes a model that utilizes pose estimation combined with a novel data augmentation method, i.e., rotation matrix. We aim to enhance the classification accuracy of activity recognition based on pose estimation data. Through our experiments, we experiment with different classification algorithms along with image augmentation approaches. Our findings demonstrate that the SVM with SGD optimization, using data augmentation with the Rotation Matrix, yields the most accurate results, achieving a 96% accuracy rate in classifying five physical activities. Conversely, without implementing the data augmentation techniques, the baseline accuracy remains at a modest 64%.
</details>
<details>
<summary>摘要</summary>
fitness 应用程序通常用于健身房内活动监测，但它们经常无法自动跟踪健身房内的活动。本研究提出一种使用 pose estimation 和 rotation matrix 的模型，以提高基于 pose estimation 数据的活动识别精度。我们通过不同的分类算法和图像增强方法进行实验，发现使用 SVM  WITH SGD 优化和数据增强方法，可以达到 96% 的正确率，分类五种物理活动。相比之下，没有实施数据增强技术，基准精度只有 64%。
</details></li>
</ul>
<hr>
<h2 id="LLM-for-SoC-Security-A-Paradigm-Shift"><a href="#LLM-for-SoC-Security-A-Paradigm-Shift" class="headerlink" title="LLM for SoC Security: A Paradigm Shift"></a>LLM for SoC Security: A Paradigm Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06046">http://arxiv.org/abs/2310.06046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipayan Saha, Shams Tarek, Katayoon Yahyaei, Sujan Kumar Saha, Jingbo Zhou, Mark Tehranipoor, Farimah Farahmandi</li>
<li>for: 提高SoC设计流程中的安全性 verification的效率、可扩展性和适应性。</li>
<li>methods: 利用生成式预训练 transformer（GPT）技术来替代现有的安全解决方案，以提供更加有效、可扩展和适应的安全验证方法。</li>
<li>results: 通过实践案例和实验研究，得到了GPT在SoC安全验证中的成果，包括提高验证效率、扩展验证范围和适应性能。<details>
<summary>Abstract</summary>
As the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, the task of incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to provide effective verification of modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, Large Language Models (LLMs) are celebrated for their remarkable success in natural language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research delves into leveraging the emergent capabilities of Generative Pre-trained Transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, showcases practical case studies, demonstrates comprehensive experiments, and provides useful promoting guidelines. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)随着系统在片（SoC）设计的 ubique 和复杂性的增加，在电子设备中实现安全性的任务变得更加困难。现有的安全解决方案因其缺乏扩展性、全面性和适应性而无法提供有效的验证。然而，大型自然语言模型（LLM）在自然语言理解、高级逻辑和程序生成任务中受到广泛的赞誉。我们的研究希望通过利用生成预训练转换器（GPT）的emergent capability来解决现有的安全阻碍，以实现更加高效、可扩展和适应的方法学。通过将LLM integrate into SoC安全验证模式，我们开启了一个新的前ier的可能性和挑战，以确保逐渐增加的SoC的安全性。本文提供了深入的现有工作分析、实践案例展示、全面的实验和有用的推广指南。我们还提出了使用LLM在不同的SoC安全验证任务中的成就、前景和挑战。
</details></li>
</ul>
<hr>
<h2 id="Generative-ensemble-deep-learning-severe-weather-prediction-from-a-deterministic-convection-allowing-model"><a href="#Generative-ensemble-deep-learning-severe-weather-prediction-from-a-deterministic-convection-allowing-model" class="headerlink" title="Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model"></a>Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06045">http://arxiv.org/abs/2310.06045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yingkaisha/severe_weather_cgan">https://github.com/yingkaisha/severe_weather_cgan</a></li>
<li>paper_authors: Yingkai Sha, Ryan A. Sobash, David John Gagne II</li>
<li>For: This paper is written for the purpose of developing an ensemble post-processing method for probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS).* Methods: The method combines conditional generative adversarial networks (CGANs) and a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs create synthetic ensemble members from deterministic CAM forecasts, and the CNN processes the outputs to estimate the probability of severe weather.* Results: The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. The method also provided meaningful ensemble spreads that can distinguish good and bad forecasts, despite being overconfident. The quality of CGAN outputs was found to be similar to a numerical ensemble, preserving inter-variable correlations and the contribution of influential predictors.<details>
<summary>Abstract</summary>
An ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overconfident but produces meaningful ensemble spreads that can distinguish good and bad forecasts. The quality of CGAN outputs is also evaluated. Results show that the CGAN outputs behave similarly to a numerical ensemble; they preserved the inter-variable correlations and the contribution of influential predictors as in the original HRRR forecasts. This work provides a novel approach to post-process CAM output using neural networks that can be applied to severe weather prediction.
</details>
<details>
<summary>摘要</summary>
一种ensemble post-processing方法被开发用于预测美国大陆部分地区（CONUS）的严重天气（风暴、冰雨和风速）。该方法结合了条件生成隐藏模型（CGANs）和卷积神经网络（CNN）来处理可变性模型（CAM）预测。CGANs用于创建基于权值的ensemble成员，并将其输出经过CNN处理以估计严重天气的概率。该方法使用2021年的高分解速Refresh（HRRR）1--24小时预测作为输入，并使用 Storm Prediction Center（SPC）的严重天气报告作为目标。该方法生成了有20%的Brier Skill Score（BSS）提升 compared to其他基于神经网络的参考方法。为了评估不确定性评估，该方法显示出了一定的过于自信心，但生成了有意义的ensemble距离，可以分辨出好和坏预测。此外，CGAN输出的质量也被评估，结果表明CGAN输出与原始HRRR预测的相互关系和重要预测变量的贡献保持了一致。这种方法可以应用于严重天气预测中的深度学习post-processing。
</details></li>
</ul>
<hr>
<h2 id="DyST-Towards-Dynamic-Neural-Scene-Representations-on-Real-World-Videos"><a href="#DyST-Towards-Dynamic-Neural-Scene-Representations-on-Real-World-Videos" class="headerlink" title="DyST: Towards Dynamic Neural Scene Representations on Real-World Videos"></a>DyST: Towards Dynamic Neural Scene Representations on Real-World Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06020">http://arxiv.org/abs/2310.06020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Seitzer, Sjoerd van Steenkiste, Thomas Kipf, Klaus Greff, Mehdi S. M. Sajjadi</li>
<li>for: 本研究旨在从单摄视频中提取真实世界场景的3D结构和动态特征，以便生成视频中的视图。</li>
<li>methods: 该模型基于现有的神经场景表示方法，通过一种新的协作训练方法和新的人工数据集DySO，以分解单摄视频为场景内容、每个视图的场景动态和摄像机pose。</li>
<li>results: 模型学习到了可质感的幂等特征，可以分离控制摄像机和场景内容的视图生成。<details>
<summary>Abstract</summary>
Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.
</details>
<details>
<summary>摘要</summary>
世界的视觉理解不仅仅是图像的 semantics 和平面结构。在这项工作中，我们想要从单目世界视频中捕捉到真实场景的3D结构和动态。我们的动态场景变换模型（DyST）利用了最近的神经场景表示学习来学习单目世界视频的含义，并将其分解为场景内容、每个视角的场景动态和摄像头姿态。这种分解是通过我们新的合作训练方案和我们的新的 sintetic dataset DySO 来实现的。DyST 学习了真实场景的具体隐藏表示，使得可以通过分离摄像头和场景内容来生成视图。
</details></li>
</ul>
<hr>
<h2 id="Divide-and-Conquer-Dynamics-in-AI-Driven-Disempowerment"><a href="#Divide-and-Conquer-Dynamics-in-AI-Driven-Disempowerment" class="headerlink" title="Divide-and-Conquer Dynamics in AI-Driven Disempowerment"></a>Divide-and-Conquer Dynamics in AI-Driven Disempowerment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06009">http://arxiv.org/abs/2310.06009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter S. Park, Max Tegmark</li>
<li>for: 研究AI对经济最有价值的工作的模型，以及AI模型如何影响现代艺术家、演员和作家的生活。</li>
<li>methods: 使用游戏理论模型来研究AI模型的分裂和不一致，以及历史记录中AI对人类的影响。</li>
<li>results: 研究预测了AI对未来的威胁，以及现代艺术家、演员和作家的利益相互关系。此外，研究还发现了AI模型的分裂和不一致可能导致更多的人受到AI的影响。<details>
<summary>Abstract</summary>
AI companies are attempting to create AI systems that outperform humans at most economically valuable work. Current AI models are already automating away the livelihoods of some artists, actors, and writers. But there is infighting between those who prioritize current harms and future harms. We construct a game-theoretic model of conflict to study the causes and consequences of this disunity. Our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.   Under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. First, current victims of AI-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. Second, the movement against AI-driven disempowerment can become more united, and thereby more likely to prevail, if members believe that their efforts will be successful as opposed to futile. Finally, the movement can better unite and prevail if its members are less myopic. Myopic members prioritize their future well-being less than their present well-being, and are thus disinclined to solidarily support current victims today at personal cost, even if this is necessary to counter the shared threat of AI-driven disempowerment.
</details>
<details>
<summary>摘要</summary>
根据现实的参数假设，我们的模型有以下预测：1. 当前被AI驱逐的人需要将未来受害者理解到，他们的利益也面临严重和即将到来的威胁，以便未来受害者可以在团结的基础上支持当前受害者。2. 反对AI驱逐的运动可以更加团结，并因此更有可能取得胜利，如果成员们认为他们的努力会成功而不是费时。3. 运动成员们更加不偏袋穷，他们将未来的利益优先于当前的利益，因此他们可能不愿意为当前受害者支付个人成本，即使这是必要的，以对抗共同的AI驱逐威胁。
</details></li>
</ul>
<hr>
<h2 id="Grokking-as-Compression-A-Nonlinear-Complexity-Perspective"><a href="#Grokking-as-Compression-A-Nonlinear-Complexity-Perspective" class="headerlink" title="Grokking as Compression: A Nonlinear Complexity Perspective"></a>Grokking as Compression: A Nonlinear Complexity Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05918">http://arxiv.org/abs/2310.05918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziming Liu, Ziqian Zhong, Max Tegmark</li>
<li>for: 本文研究了神经网络压缩的效果对于准确率的影响，并提出了一种 Linear Mapping Number (LMN) 来衡量神经网络复杂度。</li>
<li>methods: 本文使用了 ReLU 网络和 XOR 网络进行实验研究，并对神经网络压缩后的泛化性进行了分析。</li>
<li>results: 研究发现，LMN 可以准确地描述神经网络压缩前后的泛化性关系，而 $L_2$ norm 则与测试损失之间存在复杂的非线性关系。  Additionally, the paper finds that LMN can be used to explain the phenomenon of “grokking” in neural networks, where generalization is delayed after memorization.<details>
<summary>Abstract</summary>
We attribute grokking, the phenomenon where generalization is much delayed after memorization, to compression. To do so, we define linear mapping number (LMN) to measure network complexity, which is a generalized version of linear region number for ReLU networks. LMN can nicely characterize neural network compression before generalization. Although the $L_2$ norm has been a popular choice for characterizing model complexity, we argue in favor of LMN for a number of reasons: (1) LMN can be naturally interpreted as information/computation, while $L_2$ cannot. (2) In the compression phase, LMN has linear relations with test losses, while $L_2$ is correlated with test losses in a complicated nonlinear way. (3) LMN also reveals an intriguing phenomenon of the XOR network switching between two generalization solutions, while $L_2$ does not. Besides explaining grokking, we argue that LMN is a promising candidate as the neural network version of the Kolmogorov complexity since it explicitly considers local or conditioned linear computations aligned with the nature of modern artificial neural networks.
</details>
<details>
<summary>摘要</summary>
我们将“吸收”现象，即记忆化后延迟普化，归因于压缩。为此，我们定义线性映射数（LMN）来衡量神经网络复杂度，这是ReLU网络的通用化版本。LMN能够nicely characterize神经网络压缩 перед普化。尽管$L_2$norm已经是神经网络复杂度的一个受欢迎选择，但我们认为LMN比$L_2$norm更加适合以下几个理由：（1）LMN可以自然地被理解为信息/计算，而$L_2$norm不能。（2）在压缩阶段，LMN与测试损失之间存在线性关系，而$L_2$norm与测试损失之间存在复杂的非线性关系。（3）LMN还揭示了XOR网络在压缩阶段转换到两个普化解决方案的意外现象，而$L_2$norm不会这样。除了解释吸收现象，我们认为LMN是神经网络版本的科尔莫哈洛夫复杂度，因为它直接考虑了现代人工神经网络中的局部或条件线性计算，与神经网络的自然性相符。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-CLIP’s-Image-Representation-via-Text-Based-Decomposition"><a href="#Interpreting-CLIP’s-Image-Representation-via-Text-Based-Decomposition" class="headerlink" title="Interpreting CLIP’s Image Representation via Text-Based Decomposition"></a>Interpreting CLIP’s Image Representation via Text-Based Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05916">http://arxiv.org/abs/2310.05916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yossigandelsman/clip_prs">https://github.com/yossigandelsman/clip_prs</a></li>
<li>paper_authors: Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt</li>
<li>for: 本研究用CLIP图像编码器来分析各个模型组件如何影响最终表示。</li>
<li>methods: 我们将图像表示分解为图像块、模型层和注意头的和，并使用CLIP的文本表示来解释和评估这些和的组成部分。</li>
<li>results: 我们发现CLIP中的注意头扮演了各种特性特定的角色（例如位置或形状），并发现图像块存在自适应的空间局部化现象。我们使用这些理解来修复和改进CLIP模型，并创建了一个强大的零基础图像分割器。<details>
<summary>Abstract</summary>
We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.
</details>
<details>
<summary>摘要</summary>
我们研究CLIP图像编码器，分析各个模型组件对最终表示的影响。我们将图像表示分解为图像 patches、模型层和注意头的和，使用CLIP的文本表示来解释和Summands。对注意头进行解释，我们自动找到了每个头的输出空间中的文本表示，从而描述了许多头的具体作用（例如，位置或形状）。接着，对图像 patches进行解释，我们发现CLIP中存在自然的空间局部化。最后，我们利用这种理解，将CLIP中的干扰特征除掉，并创建了一个强大的零基本图像分割器。我们的结果表明，可以可靠地理解转换器模型，并使其进行修复和改进。
</details></li>
</ul>
<hr>
<h2 id="FireAct-Toward-Language-Agent-Fine-tuning"><a href="#FireAct-Toward-Language-Agent-Fine-tuning" class="headerlink" title="FireAct: Toward Language Agent Fine-tuning"></a>FireAct: Toward Language Agent Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05915">http://arxiv.org/abs/2310.05915</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anchen1011/FireAct">https://github.com/anchen1011/FireAct</a></li>
<li>paper_authors: Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao</li>
<li>for: 这篇论文主要探讨了如何通过微小示例技术和外部环境来将语言模型（LMs）训练成能够理解和行动的语言代理。</li>
<li>methods: 本文使用了问题回答（QA）设置和Google搜寻API，探索了不同的基础LMs、提示方法、微小示例和QA任务，发现通过微小示例训练基础LMs可以提高语言代理的性能。</li>
<li>results: 例如，将Llama2-7B微小示例训练500条语言代理访问GPT-4，可以提高HotpotQA性能77%。此外，本文提出了FireAct，一种新的微小示例训练LMs的方法，并显示可以透过更多的多元任务和提示方法来进一步提高代理。<details>
<summary>Abstract</summary>
Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.
</details>
<details>
<summary>摘要</summary>
最近努力已经补充语言模型（LM）以外的工具或环境，导致语言代理人能够思考和行动。然而，大多数这些代理人仍然依赖于几个示例提示技术与商业LM。在这篇论文中，我们调查和论证LM的细化提高语言代理人的方向。使用问答（QA）的Google搜索API设置，我们探索了多种基础LM、提示方法、细化数据和QA任务，并发现在细化基础LM后，语言代理人的性能一直提高。例如，将Llama2-7B细化500个代理人轨迹，生成自GPT-4，可以提高HotpotQA表现77%。此外，我们提出了FireAct，一种新的LM细化方法，使用多个任务和提示方法生成的轨迹，并证明更多的多样化细化数据可以进一步提高代理人。此外，我们还发现了扩展效果、稳定性、普适性、效率和成本等方面的优点。我们的工作证明了细化LM为代理人的全面利好，并提供了初步的实验设计、发现以及未解决问题，为语言代理人细化做出了初步的贡献。
</details></li>
</ul>
<hr>
<h2 id="SALMON-Self-Alignment-with-Principle-Following-Reward-Models"><a href="#SALMON-Self-Alignment-with-Principle-Following-Reward-Models" class="headerlink" title="SALMON: Self-Alignment with Principle-Following Reward Models"></a>SALMON: Self-Alignment with Principle-Following Reward Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05910">http://arxiv.org/abs/2310.05910</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/salmon">https://github.com/ibm/salmon</a></li>
<li>paper_authors: Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</li>
<li>for: 这篇论文旨在探讨如何使用监督式微调（Supervised Fine-Tuning，SFT）与人类反馈学习（Reinforcement Learning from Human Feedback，RLHF）来调整基于自然语言处理（NLP）的语言模型（LLM），以提高其性能和可控性。</li>
<li>methods: 本论文提出了一个新的方法，即Self-ALignMent（SALMON），可以将基于RLHF的LLM调整为符合人类定义的原则，并且只需要小量的人类监督。这个方法中心在一个以原则为基础的赏罚模型，可以根据人类定义的原则生成赏罚分数，并且可以在RL训练过程中调整这些原则，以控制RL训练出来的策略的行为。</li>
<li>results: 在实验中，使用SALMON方法训练了一个名为Dromedary-2的AI助手，并且证明了Dromedary-2可以在多个benchmark数据集上表现出色，比如LLaMA-2-Chat-70b等现有的AI系统。此外，Dromedary-2只需要6个内容学习示例和31个人类定义的原则，而不需要大量的人类监督。<details>
<summary>Abstract</summary>
Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用监督微调（SFT）和人工智能反馈学习（RLHF）的方法可以帮助基于自然语言处理（NNP）的人工智能代理人（AI）达到更好的性能。然而，这种方法的一个重要限制是它需要高质量的人类监督，这使得在复杂任务上应用困难，因为获得一致的人类响应示例和在线人类响应偏好是困难的。本文提出了一种新的方法，即Self-ALignMent（SALMON），以使基于自然语言处理的AI代理人与最小的人类监督达到更好的性能。SALMON的核心思想是使用原则遵循奖励模型，这种模型可以根据人类定义的原则生成奖励分数。通过在RL训练阶段调整这些原则，我们可以控制奖励模型中的偏好，并且消除在线人类响应的收集 limitation。我们在LLaMA-2-70b基础语言模型上实现了一个名为Dromedary-2的AI助手。只需6个示例和31个人类定义的原则，Dromedary-2可以在多个标准数据集上达到许多现有AI系统的性能水平。我们已经开源了代码和模型参数，以便进一步的研究可以提高LLM-based AI代理人的监督效率、控制性和可扩展性。
</details></li>
</ul>
<hr>
<h2 id="TAIL-Task-specific-Adapters-for-Imitation-Learning-with-Large-Pretrained-Models"><a href="#TAIL-Task-specific-Adapters-for-Imitation-Learning-with-Large-Pretrained-Models" class="headerlink" title="TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models"></a>TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05905">http://arxiv.org/abs/2310.05905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zuxin Liu, Jesse Zhang, Kavosh Asadi, Yao Liu, Ding Zhao, Shoham Sabach, Rasool Fakoor</li>
<li>for: 这个研究是为了解决控制领域中大型预训模型的潜力仍未获得充分利用，主要是因为数据的缺乏和训练或精度化这些大型模型的计算挑战。</li>
<li>methods: 本研究提出了TAIL（任务特定拓展器 для循环学习）框架，用于实现新任务的高效适材化。研究参考了现有的效率 fine-tuning技术，如瓶颈拓展器、P-Tuning和低维拓展（LoRA），以适材化大型预训模型。</li>
<li>results: 实验结果显示，TAIL框架将LoRA与其他效率 fine-tuning技术进行比较，在大量语言条件操作任务中可以实现最好的后适材化性能，仅使用1%的trainable parameter，并避免了遗传性遗传和持续学习设定中的干扰。<details>
<summary>Abstract</summary>
The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes effective pretraining of large models for decision-making, with little exploration into how to perform data-efficient continual adaptation of these models for new tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques -- e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments in large-scale language-conditioned manipulation tasks comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1\% of the trainable parameters of full fine-tuning, while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.
</details>
<details>
<summary>摘要</summary>
大型预训模型的潜在能力在控制领域仍然尚未得到充分利用，主要是因为数据的罕见和训练或细化这些大模型 для这些应用程序所需的计算挑战。先前的工作主要强调有效地预训大模型进行决策，却忽略了如何通过数据有效地适应新任务。Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques -- e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments in large-scale language-conditioned manipulation tasks comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning, while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.
</details></li>
</ul>
<hr>
<h2 id="Lion-Secretly-Solves-Constrained-Optimization-As-Lyapunov-Predicts"><a href="#Lion-Secretly-Solves-Constrained-Optimization-As-Lyapunov-Predicts" class="headerlink" title="Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts"></a>Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05898">http://arxiv.org/abs/2310.05898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lizhang Chen, Bo Liu, Kaizhao Liang, Qiang Liu</li>
<li>for: 本文旨在解释Lion优化器的理论基础。</li>
<li>methods: 本文使用维度分解的权重衰减和紧密链接的梯度下降来解释Lion优化器的动态。</li>
<li>results: 研究发现Lion优化器在训练大型人工智能模型时表现良好，并且比AdamW更具有内存效率。然而，由于Lion不受任何已知的理论支持，因此其可能性和扩展性受限。本文通过连续时间和离散时间分析，解释了Lion优化器在满足约束条件时的理论基础。<details>
<summary>Abstract</summary>
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.   This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves this through the incorporation of decoupled weight decay, where $\lambda$ represents the weight decay coefficient. Our analysis is made possible by the development of a new Lyapunov function for the Lion updates. It applies to a broader family of Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion is replaced by the subgradient of a convex function $\kappa$, leading to the solution of a general composite optimization problem of $\min_x f(x) + \kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion and pave the way for further improvements and extensions of Lion-related algorithms.
</details>
<details>
<summary>摘要</summary>
狮子（演进签证势），一种新的优化器，通过程序搜索发现，在训练大型人工智能模型时表现了有 promise的结果。它与AdamW相比，具有更高的内存效率。由于狮子包含了多种现有算法元素，包括签证、分离权重衰退、Polak和Nesterov势等，但它不属于任何已知的理论基础上的优化器。因此，尽管狮子在许多任务上表现良好，但其理论基础仍然存在uncertainty。这种不确定性限制了可以进一步提高和扩展狮子的效果的机会。本工作的目标是使狮子更加明了。通过连续时间和离散时间分析，我们证明了狮子是一种理论上有基础的优化器，可以在满足一个约束 $\|x\|_\infty \leq 1/\lambda$ 的情况下将一个通用损失函数 $f(x)$ 的最小值。狮子通过含有分离权重衰退的势 decay，其中 $\lambda$ 表示权重衰退系数。我们的分析基于一个新的Lyapunov函数，可以应用于狮子-$\kappa$ 算法中，其中 $sign(\cdot)$ 操作在狮子中被替换为一个凸函数 $\kappa$ 的极值。这些发现对狮子的动态和 Lion-相关算法的进一步改进和扩展提供了有价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="Streaming-Anchor-Loss-Augmenting-Supervision-with-Temporal-Significance"><a href="#Streaming-Anchor-Loss-Augmenting-Supervision-with-Temporal-Significance" class="headerlink" title="Streaming Anchor Loss: Augmenting Supervision with Temporal Significance"></a>Streaming Anchor Loss: Augmenting Supervision with Temporal Significance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05886">http://arxiv.org/abs/2310.05886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Utkarsh Oggy Sarawgi, John Berkowitz, Vineet Garg, Arnav Kundu, Minsik Cho, Sai Srujana Buddi, Saurabh Adya, Ahmed Tewfik</li>
<li>for: 这个研究是为了提高实时数据流中的语音和感觉信号预测能力，使用流动 нейрон网络模型，并且因应实际应用环境的限制，不能增加更多的模型参数。</li>
<li>methods: 我们提出了一个新的损失函数，即Streaming Anchor Loss（SAL），并且提出了两种专注于重要几帧的方法，即动态地调整frame-wise cross entropy损失函数，以便将更高的损失penalty赋予在semantically critical events的 temporal proximity内的帧。</li>
<li>results: 我们的实验结果显示，使用SAL来训练流动 нейрон网络模型，可以提高预测的精度和延迟时间，无需增加更多的数据或模型参数，并且可以在三个不同的语音检测任务上达到更好的效果。<details>
<summary>Abstract</summary>
Streaming neural network models for fast frame-wise responses to various speech and sensory signals are widely adopted on resource-constrained platforms. Hence, increasing the learning capacity of such streaming models (i.e., by adding more parameters) to improve the predictive power may not be viable for real-world tasks. In this work, we propose a new loss, Streaming Anchor Loss (SAL), to better utilize the given learning capacity by encouraging the model to learn more from essential frames. More specifically, our SAL and its focal variations dynamically modulate the frame-wise cross entropy loss based on the importance of the corresponding frames so that a higher loss penalty is assigned for frames within the temporal proximity of semantically critical events. Therefore, our loss ensures that the model training focuses on predicting the relatively rare but task-relevant frames. Experimental results with standard lightweight convolutional and recurrent streaming networks on three different speech based detection tasks demonstrate that SAL enables the model to learn the overall task more effectively with improved accuracy and latency, without any additional data, model parameters, or architectural changes.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>流动神经网络模型在资源受限的平台上广泛应用，以快速响应各种语音和感知信号。因此，增加流动模型学习容量（即添加更多参数）以提高预测力可能不是现实世界任务中可行的。在这种情况下，我们提出了一种新的损失函数——流动锚点损失（SAL），以更好地利用给定的学习容量。具体来说，我们的 SAL 和其关注变种在时间 proximity 上动态调整帧 wise cross entropy 损失，以将更高的损失penalty分配给 semantic 事件附近的帧。因此，我们的损失函数使得模型在预测任务相关帧时更加注重。实验结果表明，使用标准轻量级卷积神经网络和流动神经网络在三种不同的语音检测任务上，SAL 可以使模型更好地学习任务，提高准确率和响应时间，不需要额外数据、模型参数或建模变化。
</details></li>
</ul>
<hr>
<h2 id="A-Meta-Learning-Perspective-on-Transformers-for-Causal-Language-Modeling"><a href="#A-Meta-Learning-Perspective-on-Transformers-for-Causal-Language-Modeling" class="headerlink" title="A Meta-Learning Perspective on Transformers for Causal Language Modeling"></a>A Meta-Learning Perspective on Transformers for Causal Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05884">http://arxiv.org/abs/2310.05884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinbo Wu, Lav R. Varshney</li>
<li>for: 这 paper 的目的是解释Transformer架构在开发大型 causal 语言模型时的能力机制。</li>
<li>methods: 这 paper 使用 meta-learning 视角来解释 Transformer 架构在 causal 语言模型任务上的训练过程，并发现了 Transformer 中的内部优化过程中的一种特殊特征。</li>
<li>results:  experiments 表明，Transformer 架构在 real-world 数据上可以带来优秀的结果，并且该特殊特征可以在 Transformer 中的 token 表示中找到。<details>
<summary>Abstract</summary>
The Transformer architecture has become prominent in developing large causal language models. However, mechanisms to explain its capabilities are not well understood. Focused on the training process, here we establish a meta-learning view of the Transformer architecture when trained for the causal language modeling task, by explicating an inner optimization process that may happen within the Transformer. Further, from within the inner optimization, we discover and theoretically analyze a special characteristic of the norms of learned token representations within Transformer-based causal language models. Our analysis is supported by experiments conducted on pre-trained large language models and real-world data.
</details>
<details>
<summary>摘要</summary>
“transformer架构在大语言模型开发中变得非常知名，但它们的能力运作 Mechanism 仍未得到充分理解。我们专注于训练过程，以 meta-learning 的视角来探索transformer架构在语言模型化任务上的内部优化过程，并从内部优化中发现了transformer 学习的token表现内 norms 特有的一个特性。我们的分析得到了实验证明，并且在实际应用中得到了支持。”Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other regions.
</details></li>
</ul>
<hr>
<h2 id="Coarse-Graining-Hamiltonian-Systems-Using-WSINDy"><a href="#Coarse-Graining-Hamiltonian-Systems-Using-WSINDy" class="headerlink" title="Coarse-Graining Hamiltonian Systems Using WSINDy"></a>Coarse-Graining Hamiltonian Systems Using WSINDy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05879">http://arxiv.org/abs/2310.05879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel A. Messenger, Joshua W. Burby, David M. Bortz</li>
<li>for: 这个论文的目的是推广弱形式粗化算法（WSINDy）到具有approximate symmetries的哈密顿动力学中，以便高效地捕捉相关度量的动力学。</li>
<li>methods: 这个论文使用的方法是WSINDy算法，它可以在具有approximate symmetries的哈密顿动力学中成功地适应粗化。WSINDy算法在数学上保留哈密顿结构，并且计算高效，通常只需要一个轨迹来学习整个减少后的哈密顿系统。</li>
<li>results: 这个论文的结果表明，WSINDy算法可以在具有approximate symmetries的哈密顿动力学中高效地捕捉相关度量的动力学，并且可以在各种实际应用中提供更高精度的预测。例如，在振荡器动力学、Hénon-Heiles系统和电荷粒子动力学等方面都有physically relevant例子。<details>
<summary>Abstract</summary>
The Weak-form Sparse Identification of Nonlinear Dynamics algorithm (WSINDy) has been demonstrated to offer coarse-graining capabilities in the context of interacting particle systems ( https://doi.org/10.1016/j.physd.2022.133406 ). In this work we extend this capability to the problem of coarse-graining Hamiltonian dynamics which possess approximate symmetries. Such approximate symmetries often lead to the existence of a Hamiltonian system of reduced dimension that may be used to efficiently capture the dynamics of the relevant degrees of freedom. Deriving such reduced systems, or approximating them numerically, is an ongoing challenge. We demonstrate that WSINDy can successfully identify this reduced Hamiltonian system in the presence of large perturbations imparted from both the inexact nature of the symmetry and extrinsic noise. This is significant in part due to the nontrivial means by which such systems are derived analytically. WSINDy naturally preserves the Hamiltonian structure by restricting to a trial basis of Hamiltonian vector fields, and the methodology is computational efficient, often requiring only a single trajectory to learn the full reduced Hamiltonian, and avoiding forward solves in the learning process. In this way, we argue that weak-form equation learning is particularly well-suited for Hamiltonian coarse-graining. Using nearly-periodic Hamiltonian systems as a prototypical class of systems with approximate symmetries, we show that WSINDy robustly identifies the correct leading-order reduced system of dimension $2(N-1)$ or $N$ from the original $(2N)$-dimensional system, upon observation of the relevant degrees of freedom. We provide physically relevant examples, namely coupled oscillator dynamics, the H\'enon-Heiles system for stellar motion within a galaxy, and the dynamics of charged particles.
</details>
<details>
<summary>摘要</summary>
“弱形式简润识别非线性动力学算法（WSINDy）已经在互动粒子系统上显示出简润功能。在这个工作中，我们将这个功能扩展到具有约束的哈密顿动力学问题。这些约束通常导致一个简润的哈密顿系统，可以高效地捕捉相关的动力学度复。 derive这个简润系统或 numerically Approximate它是一个ongoing挑战。我们示出WSINDy可以成功地识别这个简润的哈密顿系统，甚至在大规模的干扰和随机变动下。这是由于WSINDy的方法自然地保留哈密顿结构，通过仅对实验基底的哈密顿 вектор场进行限制。此外，WSINDy的方法具有计算效率高，通常只需要一条轨道来学习全部简润哈密顿，而不需要前向 solves在学习过程中。因此，我们认为弱形式方程式学习特别适合哈密顿简润。使用nearly periodic哈密顿系统作为一个具有约束的系统，我们显示WSINDy可以坚定地识别原始(2N)-维系统中的(2N-1)维或N维简润系统，通过观察相关的动力学度复。我们提供了物理相关的例子，包括相互作用的振荡器动力学、Hénon-Heiles系统 для星系动力学和带电粒子的动力学。”
</details></li>
</ul>
<hr>
<h2 id="AI-Systems-of-Concern"><a href="#AI-Systems-of-Concern" class="headerlink" title="AI Systems of Concern"></a>AI Systems of Concern</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05876">http://arxiv.org/abs/2310.05876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Kayla Matteucci, Shahar Avin, Fazl Barez, Seán Ó hÉigeartaigh</li>
<li>for: 这篇论文主要是为了讨论高级AI系统中的危险性和控制问题。</li>
<li>methods: 论文使用了多种学术 frameworks和指标来评估高级AI系统中的危险性和控制问题。</li>
<li>results: 论文认为，高级AI系统中的“属性X”特征会导致AI系统的危险性和控制问题，并提出了一些指标和管理措施来评估和限制高级AI系统中的“属性X”特征。<details>
<summary>Abstract</summary>
Concerns around future dangers from advanced AI often centre on systems hypothesised to have intrinsic characteristics such as agent-like behaviour, strategic awareness, and long-range planning. We label this cluster of characteristics as "Property X". Most present AI systems are low in "Property X"; however, in the absence of deliberate steering, current research directions may rapidly lead to the emergence of highly capable AI systems that are also high in "Property X". We argue that "Property X" characteristics are intrinsically dangerous, and when combined with greater capabilities will result in AI systems for which safety and control is difficult to guarantee. Drawing on several scholars' alternative frameworks for possible AI research trajectories, we argue that most of the proposed benefits of advanced AI can be obtained by systems designed to minimise this property. We then propose indicators and governance interventions to identify and limit the development of systems with risky "Property X" characteristics.
</details>
<details>
<summary>摘要</summary>
有些担忧将来的人工智能会带来危险，通常集中在假设存在自我行为、战略意识和远程规划等特性的系统上。我们称这些特性为“X属性”。目前的大多数人工智能系统具有低度的X属性，但在没有干预导航的情况下，当前的研究方向可能会迅速导致高度可能X属性的AI系统的出现。我们认为X属性是危险的，当与更高的能力相结合时，控制和安全难以保证。我们根据一些学者的不同框架，提出了可以通过降低X属性来获得大多数高级人工智能的提antages的可能性。我们还提出了指标和管理措施，以识别和限制开发高风险X属性的系统。
</details></li>
</ul>
<hr>
<h2 id="ViCor-Bridging-Visual-Understanding-and-Commonsense-Reasoning-with-Large-Language-Models"><a href="#ViCor-Bridging-Visual-Understanding-and-Commonsense-Reasoning-with-Large-Language-Models" class="headerlink" title="ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models"></a>ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05872">http://arxiv.org/abs/2310.05872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiwen Zhou, Kwonjoon Lee, Teruhisa Misu, Xin Eric Wang</li>
<li>for: 这个论文的目的是探索预训练的视觉语言模型（VLM）和大语言模型（LLM）在视觉常识逻辑（VCR）中的合作能力。</li>
<li>methods: 这个论文使用了预训练的VLM和LLM来解决视觉常识理解（VCU）和视觉常识推理（VCI）问题。VLM提供图像描述来支持LLM进行推理，并且使用了一种协作策略，让LLM在不确定的推理时指导VLM集中关注相关的视觉元素。</li>
<li>results: 这个论文在两个VCR benchmark数据集上进行了评估，并与没有培 retrained fine-tuning的方法进行比较，得到了更好的性能。<details>
<summary>Abstract</summary>
In our work, we explore the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) for visual commonsense reasoning (VCR). We categorize the problem of VCR into visual commonsense understanding (VCU) and visual commonsense inference (VCI). For VCU, which involves perceiving the literal visual content, pre-trained VLMs exhibit strong cross-dataset generalization. On the other hand, in VCI, where the goal is to infer conclusions beyond image content, VLMs face difficulties. We find that a baseline where VLMs provide perception results (image captions) to LLMs leads to improved performance on VCI. However, we identify a challenge with VLMs' passive perception, which often misses crucial context information, leading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we suggest a collaborative approach where LLMs, when uncertain about their reasoning, actively direct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences. In our method, named ViCor, pre-trained LLMs serve as problem classifiers to analyze the problem category, VLM commanders to leverage VLMs differently based on the problem classification, and visual commonsense reasoners to answer the question. VLMs will perform visual recognition and understanding. We evaluate our framework on two VCR benchmark datasets and outperform all other methods that do not require in-domain supervised fine-tuning.
</details>
<details>
<summary>摘要</summary>
在我们的工作中，我们探索预训练的视觉语言模型（VLM）和大型语言模型（LLM）在视觉常识逻辑（VCR）中的共同能力。我们将VCR分为视觉常识理解（VCU）和视觉常识推理（VCI）两个问题。在VCU中，涉及到直接读取图像内容的VCMs表现出了强大的跨数据集泛化能力。然而，在VCI中，VCMs面临困难，因为需要从图像内容中做出更多的推理。我们发现，将VCMs提供视觉内容（图像描述）给LLMs可以提高VCI的性能。然而，我们发现VCMs的被动感知有时会错过重要的上下文信息，导致LLMs的推理错误或不确定。为了解决这个问题，我们提议一种协作方法，其中LLMs在不确定的推理时会活动地指导VCMs集中聚焦和收集相关的视觉元素以支持可能的常识推理。我们称这种方法为ViCor，其中预训练的LLMs serves为问题类别分析器，VCM commander以不同的问题类别来利用VCMs，并且视觉常识推理器来回答问题。VCMs将进行视觉识别和理解。我们对VCR benchmark数据集进行评估，并在不需要域内抽象精细调整的情况下超越所有其他方法。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-value-alignment-through-preference-aggregation-of-multiple-objectives"><a href="#Dynamic-value-alignment-through-preference-aggregation-of-multiple-objectives" class="headerlink" title="Dynamic value alignment through preference aggregation of multiple objectives"></a>Dynamic value alignment through preference aggregation of multiple objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05871">http://arxiv.org/abs/2310.05871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcin Korecki, Damian Dailisan, Cesare Carissimo</li>
<li>for: 这项研究目标是为了开发能够与人类目标相对应的伦理AI系统。</li>
<li>methods: 这种方法使用多目标方法来动态调整价值，以确保RL算法能够同时满足多个目标。</li>
<li>results: 这种方法在简化后的两脚交叉控制系统中进行了应用，并实现了在三个维度（速度、停止和等待时间）的全面性能提高，同时能够有效地 инте格各个目标之间的矛盾。<details>
<summary>Abstract</summary>
The development of ethical AI systems is currently geared toward setting objective functions that align with human objectives. However, finding such functions remains a research challenge, while in RL, setting rewards by hand is a fairly standard approach. We present a methodology for dynamic value alignment, where the values that are to be aligned with are dynamically changing, using a multiple-objective approach. We apply this approach to extend Deep $Q$-Learning to accommodate multiple objectives and evaluate this method on a simplified two-leg intersection controlled by a switching agent.Our approach dynamically accommodates the preferences of drivers on the system and achieves better overall performance across three metrics (speeds, stops, and waits) while integrating objectives that have competing or conflicting actions.
</details>
<details>
<summary>摘要</summary>
现在的人工智能系统开发将注意力集中在设定目标函数，以便与人类目标相互对应。然而，发现这些函数仍然是研究挑战，而在RL中，手动设置优化奖励是一种标准的方法。我们提出了动态值协调的方法，使得需要协调的价值可以随时变化，使用多目标方法。我们将这种方法应用到深度Q学来扩展多目标，并评估这种方法在简化的二脚交汇控制系统上。我们的方法可以动态地考虑驱驶者对系统的偏好，并 achieve better 总性表现（速度、停止和等待时间），并同时考虑了具有竞争或冲突的目标。
</details></li>
</ul>
<hr>
<h2 id="HyperAttention-Long-context-Attention-in-Near-Linear-Time"><a href="#HyperAttention-Long-context-Attention-in-Near-Linear-Time" class="headerlink" title="HyperAttention: Long-context Attention in Near-Linear Time"></a>HyperAttention: Long-context Attention in Near-Linear Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05869">http://arxiv.org/abs/2310.05869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, Amir Zandieh</li>
<li>For: 提高Large Language Model（LLM）中长上下文的计算效率。* Methods: 引入两个细化参数，分别测量：（1）折衔列norm在归一化注意力矩阵中，和（2）行norm在归一化注意力矩阵后的大项检测和 removing。使用这两个参数捕捉问题的困难程度。* Results: HyperAttention比 existed方法更快，具有linear time sampling算法，并且可以适应不同的长上下文长度。Empirical experiments表明， HyperAttention在不同的长上下文长度上都具有良好的性能。例如，在32k上下文长度上，HyperAttention可以提高ChatGLM2的推理速度50%，而且只增加了0.7的折衔值。在更大的长上下文长度上，HyperAttention可以提高单层注意力层的速度5倍。<details>
<summary>Abstract</summary>
We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. We validate the empirical performance of HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50\% faster on 32k context length while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer.
</details>
<details>
<summary>摘要</summary>
我们提出了一种近似的注意机制名为HyperAttention，以Addressing the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters to measure: (1) the maximum column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. We validate the empirical performance of HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50% faster on a context length of 32k while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer.
</details></li>
</ul>
<hr>
<h2 id="Generative-quantum-machine-learning-via-denoising-diffusion-probabilistic-models"><a href="#Generative-quantum-machine-learning-via-denoising-diffusion-probabilistic-models" class="headerlink" title="Generative quantum machine learning via denoising diffusion probabilistic models"></a>Generative quantum machine learning via denoising diffusion probabilistic models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05866">http://arxiv.org/abs/2310.05866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingzhi Zhang, Peng Xu, Xiaohui Chen, Quntao Zhuang</li>
<li>for: 本文旨在探讨Quantum Denoising Diffusion Probabilistic Models（QuDDPM），它是一种可以有效地学习量子数据的生成学模型。</li>
<li>methods: QuDDPM使用多层环路来保证表达能力，并在训练过程中引入多个中间任务来避免荒漠板和提高训练效率。</li>
<li>results: QuDDPM可以有效地学习相关的量子噪声模型和量子数据的topological结构。<details>
<summary>Abstract</summary>
Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the quantum denoising diffusion probabilistic models (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We demonstrate QuDDPM's capability in learning correlated quantum noise model and learning topological structure of nontrivial distribution of quantum data.
</details>
<details>
<summary>摘要</summary>
深度生成模型是计算机视觉、文本生成和大语言模型的关键技术。干扰扩散probabilistic模型（DDPM）在计算机视觉任务中最近受到了广泛关注，因为它们可以生成多样和高质量的样本，同时可以采用灵活的模型架构和简单的训练方案。量子生成模型，受到共聚和超position的 empowerment，为学习классиcu和量子数据提供了新的视角。以类型 counterpart为基础，我们提出了量子干扰扩散probabilistic模型（QuDDPM），以实现高效可训练的生成学习 quantum data。QuDDPM采用了多层环路来保证表达力，同时引入多个中间训练任务作为干扰和静止的插值，以避免恐慌板和保证高效的训练。我们示例了QuDDPM在学习相关的量子噪声模型和学习不规则分布的 topological结构。
</details></li>
</ul>
<hr>
<h2 id="Fine-grained-Audio-Visual-Joint-Representations-for-Multimodal-Large-Language-Models"><a href="#Fine-grained-Audio-Visual-Joint-Representations-for-Multimodal-Large-Language-Models" class="headerlink" title="Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models"></a>Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05863">http://arxiv.org/abs/2310.05863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/briansidp/audiovisualllm">https://github.com/briansidp/audiovisualllm</a></li>
<li>paper_authors: Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang</li>
<li>for: 这篇论文旨在扩展文本基础的语言模型，以同时处理音频和视频输入流，以便Language Model（LLM）更好地理解通用视频输入。</li>
<li>methods: 该论文提出了一种名为FAVOR的 audio-visual联合表示学习框架，通过把音频和视频输入流与LLM输入空间进行同步，以实现高精度的音频视频联合表示。具体来说，该框架包括一个 causal Q-Former 结构和一个 causal attention模块，以增强音频视频帧之间的 causal 关系的捕捉。</li>
<li>results: 在AVEB评估准则下，FAVOR实现了与单modal任务相当的性能，并在视频问答任务上达到了20%以上的性能提升。此外，FAVOR还示出了在其他多modal LLVM中缺乏的视频理解和推理能力。<details>
<summary>Abstract</summary>
Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.
</details>
<details>
<summary>摘要</summary>
大型语音视频语言模型（LLM）已经吸引了广泛的注意，然而将两个输入流进行细腻的组合仍然是一项挑战，这是LLM理解普通视频输入的必要条件。为此，本文提出了一个 audio-visual 共同表示学习框架（FAVOR），该框架将文本语言模型扩展到同时感知语音和视频流的帧级别上，并将语音和视频特征流 fusion 到共同表示。为了将音频和视频特征流与语言模型输入空间进行对应，我们提出了一种 causal Q-Former 结构，并在其中添加了一个 causal 注意模块，以增强捕捉音频视频帧之间的 causal 关系。我们还提出了一个 audio-visual 评价指标（AVEB），该指标包括6种单Modal任务和5种跨Modal任务，旨在测试音频、语音和图像任务中的单Modal性能，以及音频视频之间的相互理解能力。FAVOR在 AVEB 中 achieved 20% 以上的性能提升，特别是在需要细腻信息或时间 causal 逻辑时。此外，FAVOR 还示出了在其他多Modal LLM 未能达到的视频理解和逻辑能力。FAVOR 的交互 demo 可以在 GitHub 上找到（https://github.com/BriansIDP/AudioVisualLLM.git），训练代码和模型检查点将很快地发布。
</details></li>
</ul>
<hr>
<h2 id="Rephrase-Augment-Reason-Visual-Grounding-of-Questions-for-Vision-Language-Models"><a href="#Rephrase-Augment-Reason-Visual-Grounding-of-Questions-for-Vision-Language-Models" class="headerlink" title="Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models"></a>Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05861">http://arxiv.org/abs/2310.05861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/archiki/repare">https://github.com/archiki/repare</a></li>
<li>paper_authors: Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal<br>for: 这个研究旨在提高零或几 shot 下的视觉语言任务性能，通过将大型语言模型（LLM）与视觉编码器结合起来，得到大型视觉语言模型（LVLM）。methods: 该研究使用了 gradient-free 框架，名为 RepARe，可以提取图像中核心信息，并通过 LLM 作为描述者和理解者，对原始问题进行修改。results: 研究发现，使用 RepARe 可以提高零或几 shot 下的视觉语言任务性能，在 VQAv2 和 A-OKVQA 两个任务上分别提高了 3.85% 和 6.41%。此外，使用黄金答案作为oracle问题候选选择，可以实现更大的提高。<details>
<summary>Abstract</summary>
An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to a LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To this end, we present Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We then use the LVLM's confidence over a generated answer as an unsupervised scoring function to select the rephrased question most likely to improve zero-shot performance. Focusing on two visual question answering tasks, we show that RepARe can result in a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41% point increase on A-OKVQA. Additionally, we find that using gold answers for oracle question candidate selection achieves a substantial gain in VQA accuracy by up to 14.41%. Through extensive analysis, we demonstrate that outputs from RepARe increase syntactic complexity, and effectively utilize vision-language interaction and the frozen language model in LVLMs.
</details>
<details>
<summary>摘要</summary>
随着更多的视觉任务可以通过几个或者 zero-shot 方式处理，大量语言模型（LLM）与视觉编码器结合形成大型视觉语言模型（LVLM）。虽然这有着巨大的优点，如不需要训练数据或自定义架构，但输入如何给 LVLM 是非常重要的。特别是，用 underspecified 的方式提交输入可能会导致错误答案，因为缺失视觉信息、复杂的隐式推理或语言抽象。因此，通过添加视觉固定信息来增强输入可以提高模型性能，例如，localizing objects 和解决参考。在 VQA  Setting 中，改变问题的表述方式可以使模型更容易回答。为此，我们提出了 Rephrase、Augment 和 Reason（RepARe）框架，它使用 underlying LVLM 作为captioner和reasoner来提取图像中的精锦信息，并提出修改原始问题。然后，使用 LVLM 对生成的答案的信任度作为无supervised 评分函数来选择修改后的问题。我们在两个视觉问答任务上进行了实验，结果表明，RepARe 可以提高零shot性能，VQAv2 上提高 3.85%（绝对值），A-OKVQA 上提高 6.41% 点。此外，我们发现使用黄金答案作为oracle问题候选者选择可以在 VQA 中提高准确率，最高提高 14.41%。通过广泛的分析，我们证明了 RepARe 输出增加了语法复杂性，并有效地利用了视觉语言交互和冰凉语言模型在 LVLM 中。
</details></li>
</ul>
<hr>
<h2 id="Improving-Summarization-with-Human-Edits"><a href="#Improving-Summarization-with-Human-Edits" class="headerlink" title="Improving Summarization with Human Edits"></a>Improving Summarization with Human Edits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05857">http://arxiv.org/abs/2310.05857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zonghai Yao, Benjamin J Schloss, Sai P. Selvaraj</li>
<li>for: 这paper主要针对的是如何使用人类反馈来提高自然语言处理模型的质量。</li>
<li>methods: 该paper提出了一种新的技术Sequence Alignment（un）Likelihood Training（SALT），它可以同时使用人类编辑和模型生成的数据来进行训练。此外，paper还提出了一种伪编辑技术来模拟人类编辑数据，以降低人类编辑数据的成本。</li>
<li>results: paper的实验结果表明，SALT可以提高SUMMARY的质量，并且在医学领域SUMMARY中表现更好。此外，paper还比较了SALT与传统的RLHF方法（DPO），发现SALT在使用人类编辑数据时能够表现更好。<details>
<summary>Abstract</summary>
Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data -- Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improving the summary quality with Human and Imitation Edits. Through additional experiments, we show that SALT outperforms the conventional RLHF method (designed for human preferences) -- DPO, when applied to human-edit data. We hope the evidence in our paper prompts researchers to explore, collect, and better use different human feedback approaches scalably.
</details>
<details>
<summary>摘要</summary>
近期研究表明了使用人类反馈方式进行学习可以生成人决定的高质量文本。现有研究使用人类反馈来训练大语言模型（LLM），并已经获得了传统可能性训练所超越的摘要质量。在这篇论文中，我们关注了一种 menos explored的人类反馈方式---人类编辑。我们提出了序列匹配（不）可能性训练（SALT），一种新的技术，可以在训练循环中结合人类编辑和模型生成的数据。此外，我们还示出了使用现有训练数据中的真实编辑和模型生成的摘要来模拟人类编辑的方法，以降低人类编辑数据的成本。在我们的实验中，我们扩展了人类反馈的探索，从通用领域摘要扩展到医学领域摘要。我们的结果表明SALT可以提高摘要质量，并且在使用人类编辑和伪编辑数据时表现出色。通过额外的实验，我们还证明了SALT在使用人类编辑数据时超过了传统的RLHF方法（设计为人类偏好）——DPO。我们希望这篇论文的证据能够让研究人员更好地探索、收集和利用不同的人类反馈方式，以便在大规模的应用中进行更好的学习。
</details></li>
</ul>
<hr>
<h2 id="GraphLLM-Boosting-Graph-Reasoning-Ability-of-Large-Language-Model"><a href="#GraphLLM-Boosting-Graph-Reasoning-Ability-of-Large-Language-Model" class="headerlink" title="GraphLLM: Boosting Graph Reasoning Ability of Large Language Model"></a>GraphLLM: Boosting Graph Reasoning Ability of Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05845">http://arxiv.org/abs/2310.05845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mistyreed63849/graph-llm">https://github.com/mistyreed63849/graph-llm</a></li>
<li>paper_authors: Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang</li>
<li>for: 这篇论文旨在解决大语言模型（LLM）在图数据理解和推理方面的瓶颈问题。</li>
<li>methods: 该论文提出了一种结合图学学习模型和LLM的综合方法，称为GraphLLM，以提高LLM在图数据理解和推理方面的能力。</li>
<li>results: 实验结果表明，GraphLLM可以提高图数据理解和推理的准确率，并将上下文量减少96.45%。<details>
<summary>Abstract</summary>
The advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional ability on understanding diverse types of information, including but not limited to images and audio. Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks. In this paper, we endeavor to unearth the obstacles that impede LLMs in graph reasoning, pinpointing the common practice of converting graphs into natural language descriptions (Graph2Text) as a fundamental bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs. This synergy equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power of graph learning models. Our empirical evaluations across four fundamental graph reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a substantial average accuracy enhancement of 54.44%, alongside a noteworthy context reduction of 96.45% across various graph reasoning tasks.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）的发展有力地推动了人工通用智能（AGI）的前进， LLM 表现出色地理解多种信息类型，包括图像和音频。然而，Graph reasoning 领域中 LLM 的表现仍然不够，特别是在基本的图级 reasoning 任务中。研究发现，这是由于通常将图转换成自然语言描述（Graph2Text）的做法而导致的。为了缓解这个障碍，我们提出了 GraphLLM，一种独特的端到端方法，它 synergistically 结合了图学学习模型和 LLM 。这种结合使得 LLM 能够有效地理解和处理图数据，并且利用图学学习模型的更高表达力。我们在四个基本的图级 reasoning 任务上进行了Empirical 评估，结果表明 GraphLLM 的效果惊人，相对于基eline 方法，GraphLLM 的均值精度提高了54.44%，同时在不同的图级 reasoning 任务中Context 减少了96.45%。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Accident-Severity-An-Analysis-Of-Factors-Affecting-Accident-Severity-Using-Random-Forest-Model"><a href="#Predicting-Accident-Severity-An-Analysis-Of-Factors-Affecting-Accident-Severity-Using-Random-Forest-Model" class="headerlink" title="Predicting Accident Severity: An Analysis Of Factors Affecting Accident Severity Using Random Forest Model"></a>Predicting Accident Severity: An Analysis Of Factors Affecting Accident Severity Using Random Forest Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05840">http://arxiv.org/abs/2310.05840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adekunle Adefabi, Somtobe Olisah, Callistus Obunadike, Oluwatosin Oyetubo, Esther Taiwo, Edward Tella</li>
<li>for: 预测交通事故严重程度，以采取措施降低交通事故的发生频率。</li>
<li>methods: 使用Random Forest机器学习算法，对大都会区交通事故记录数据进行训练，并对模型进行优化。</li>
<li>results:  Random Forest模型的准确率高于80%，其中最重要的6个变量为风速、气压、湿度、视力、清晰天气和云层覆盖。<details>
<summary>Abstract</summary>
Road accidents have significant economic and societal costs, with a small number of severe accidents accounting for a large portion of these costs. Predicting accident severity can help in the proactive approach to road safety by identifying potential unsafe road conditions and taking well-informed actions to reduce the number of severe accidents. This study investigates the effectiveness of the Random Forest machine learning algorithm for predicting the severity of an accident. The model is trained on a dataset of accident records from a large metropolitan area and evaluated using various metrics. Hyperparameters and feature selection are optimized to improve the model's performance. The results show that the Random Forest model is an effective tool for predicting accident severity with an accuracy of over 80%. The study also identifies the top six most important variables in the model, which include wind speed, pressure, humidity, visibility, clear conditions, and cloud cover. The fitted model has an Area Under the Curve of 80%, a recall of 79.2%, a precision of 97.1%, and an F1 score of 87.3%. These results suggest that the proposed model has higher performance in explaining the target variable, which is the accident severity class. Overall, the study provides evidence that the Random Forest model is a viable and reliable tool for predicting accident severity and can be used to help reduce the number of fatalities and injuries due to road accidents in the United States
</details>
<details>
<summary>摘要</summary>
道路交通事故具有重要的经济和社会成本，一小部分严重事故占据了大部分成本。预测事故严重性可以帮助采取抢险策略，识别可能发生事故的不安全道路情况，采取有知识的行动，以减少严重事故的数量。本研究检查Random Forest机器学习算法是否能够预测事故严重性。模型在一个大都市区的事故记录 dataset 上训练，并使用不同的指标进行评估。Hyperparameters 和特征选择被优化，以提高模型的性能。结果显示，Random Forest 模型可以高效地预测事故严重性，准确率高于 80%。研究还确定了最重要的六个变量，包括风速、压力、湿度、视程、晴天和云层覆盖。已经适应的模型具有折线曲线面积为 80%，回归率为 79.2%，准确率为 97.1%，F1 分数为 87.3%。这些结果表明，提案的模型具有更高的表达能力，可以帮助减少因道路交通事故而导致的死亡和伤害。总的来说，本研究提供了Random Forest模型是可靠和可靠的预测事故严重性工具，可以在美国用于减少道路交通事故的死亡和伤害。
</details></li>
</ul>
<hr>
<h2 id="Learning-Language-guided-Adaptive-Hyper-modality-Representation-for-Multimodal-Sentiment-Analysis"><a href="#Learning-Language-guided-Adaptive-Hyper-modality-Representation-for-Multimodal-Sentiment-Analysis" class="headerlink" title="Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis"></a>Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05804">http://arxiv.org/abs/2310.05804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Haoyu-ha/ALMT">https://github.com/Haoyu-ha/ALMT</a></li>
<li>paper_authors: Haoyu Zhang, Yu Wang, Guanghao Yin, Kejun Liu, Yuanyuan Liu, Tianshu Yu</li>
<li>for: 提高多模态情感分析（MSA）的性能，增强模态之间的协调和相互启发。</li>
<li>methods: 采用适应语言引导多模态变换（ALMT），包括自适应超模态学习（AHL）模块，从视频和音频特征中学习干扰和冲突抑制表示。</li>
<li>results: 在多个流行数据集（如 MOSI、MOSEI 和 CH-SIMS）上实现状态的表现，并通过多种缺省示例证明了我们的干扰和冲突抑制机制的有效性和必要性。<details>
<summary>Abstract</summary>
Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich information from multiple sources (e.g., language, video, and audio), the potential sentiment-irrelevant and conflicting information across modalities may hinder the performance from being further improved. To alleviate this, we present Adaptive Language-guided Multimodal Transformer (ALMT), which incorporates an Adaptive Hyper-modality Learning (AHL) module to learn an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales. With the obtained hyper-modality representation, the model can obtain a complementary and joint representation through multimodal fusion for effective MSA. In practice, ALMT achieves state-of-the-art performance on several popular datasets (e.g., MOSI, MOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and necessity of our irrelevance/conflict suppression mechanism.
</details>
<details>
<summary>摘要</summary>
这篇文章探讨了多modal sentiment分析（MSA）的问题，MSA可以利用多种资料源（如语言、视频和音频）来分析情感，但是可能会存在不相关或冲突的资料，这可能会妨碍MSA的表现。为了解决这个问题，我们提出了适应语言导向多modal transformer（ALMT），它包括一个适应多模式学习（AHL）模组，可以从视觉和音频特征中学习一个不相关或冲突的表现。这个表现可以与语言特征进行联合表现，从而实现有效的MSA。在实践中，ALMT在多个流行的数据集（如MOSI、MOSEI和CH-SIMS）上 achieve state-of-the-art 表现，并且进行了丰富的ablation 测试，以验证我们的不相关或冲突抑制机制的有效性和必要性。
</details></li>
</ul>
<hr>
<h2 id="Are-Large-Language-Models-Post-Hoc-Explainers"><a href="#Are-Large-Language-Models-Post-Hoc-Explainers" class="headerlink" title="Are Large Language Models Post Hoc Explainers?"></a>Are Large Language Models Post Hoc Explainers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05797">http://arxiv.org/abs/2310.05797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AI4LIFE-GROUP/LLM_Explainer">https://github.com/AI4LIFE-GROUP/LLM_Explainer</a></li>
<li>paper_authors: Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju</li>
<li>for: This paper aims to study the effectiveness of large language models (LLMs) in explaining other predictive models.</li>
<li>methods: The paper proposes a novel framework that utilizes multiple prompting strategies, including perturbation-based ICL, prediction-based ICL, instruction-based ICL, and explanation-based ICL, to generate explanations for other models.</li>
<li>results: The paper demonstrates that LLM-generated explanations perform on par with state-of-the-art post hoc explainers, with an average accuracy of 72.19% in identifying the most important feature.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based ICL, with varying levels of information about the underlying ML model and the local neighborhood of the test sample. We conduct extensive experiments with real-world benchmark datasets to demonstrate that LLM-generated explanations perform on par with state-of-the-art post hoc explainers using their ability to leverage ICL examples and their internal knowledge in generating model explanations. On average, across four datasets and two ML models, we observe that LLMs identify the most important feature with 72.19% accuracy, opening up new frontiers in explainable artificial intelligence (XAI) to explore LLM-based explanation frameworks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rethinking-Memory-and-Communication-Cost-for-Efficient-Large-Language-Model-Training"><a href="#Rethinking-Memory-and-Communication-Cost-for-Efficient-Large-Language-Model-Training" class="headerlink" title="Rethinking Memory and Communication Cost for Efficient Large Language Model Training"></a>Rethinking Memory and Communication Cost for Efficient Large Language Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06003">http://arxiv.org/abs/2310.06003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chan Wu, Hanxiao Zhang, Lin Ju, Jinjing Huang, Youshao Xiao, Zhaoxin Huan, Siyuan Li, Fanzhuang Meng, Lei Liang, Xiaolu Zhang, Jun Zhou</li>
<li>for: 本研究旨在提出一种能够均衡内存消耗和通信成本的大语言模型训练策略集Partial Redundancy Optimizer (PaRO)，以提高训练效率。</li>
<li>methods: 本研究使用了细化的分割策略和 Hierarchical Overlapping Ring (HO-Ring) 通信拓扑，以减少内存重复和通信成本，提高训练效率。</li>
<li>results: 实验表明，PaRO 可以提高训练速度，相比 SOTA 方法的 1.19x-2.50x，并实现近线性扩展性。 HO-Ring 算法可以提高通信效率，相比传统的 Ring 算法的 36.5%。<details>
<summary>Abstract</summary>
Recently, various distributed strategies for large language model training have been proposed. However, these methods provided limited solutions for the trade-off between memory consumption and communication cost. In this paper, we rethink the impact of memory consumption and communication costs on the training speed of large language models, and propose a memory-communication balanced strategy set Partial Redundancy Optimizer (PaRO). PaRO provides comprehensive options which reduces the amount and frequency of inter-group communication with minor memory redundancy by fine-grained sharding strategy, thereby improving the training efficiency in various training scenarios. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large language model training. Our experiments demonstrate that PaRO significantly improves training throughput by 1.19x-2.50x compared to the SOTA method and achieves a near-linear scalability. The HO-Ring algorithm improves communication efficiency by 36.5% compared to the traditional Ring algorithm.
</details>
<details>
<summary>摘要</summary>
近期，许多分布式策略 для大型自然语言模型训练被提出。然而，这些方法具有限制的解决方案，即内存消耗和通信成本之间的质量协调。在本文中，我们重新思考大型自然语言模型训练中内存消耗和通信成本的影响，并提出了一个内存-通信平衡策略集Partial Redundancy Optimizer（PaRO）。PaRO提供了全面的选项，以减少归并分组通信的数量和频率，并通过细化分组策略减少内存约束，从而提高训练效率在不同的训练场景中。此外，我们提出了层次 overlap 环（HO-Ring）通信架构，以提高在节点或交换机之间的通信效率。我们的实验表明，PaRO可以对比SOTA方法提高训练速度，并实现近似线性扩展性。HO-Ring算法提高了传输效率，相比传统环算法，提高了36.5%。
</details></li>
</ul>
<hr>
<h2 id="DANet-Enhancing-Small-Object-Detection-through-an-Efficient-Deformable-Attention-Network"><a href="#DANet-Enhancing-Small-Object-Detection-through-an-Efficient-Deformable-Attention-Network" class="headerlink" title="DANet: Enhancing Small Object Detection through an Efficient Deformable Attention Network"></a>DANet: Enhancing Small Object Detection through an Efficient Deformable Attention Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05768">http://arxiv.org/abs/2310.05768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sohag Mia, Abdullah Al Bary Voban, Abu Bakor Hayat Arnob, Abdu Naim, Md Kawsar Ahmed, Md Shariful Islam</li>
<li>for: 这个论文旨在提高生产环境中小物件检测的效率和精度，以提高产品质量和安全性。</li>
<li>methods: 本论文使用的方法包括嵌入式Pyramid Network，扭转网络，对应适应网络，并且将Convolutional Block Attention Module加入每个基本ResNet50对组。</li>
<li>results: 本论文的模型在NEU-DET和Pascal VOC datasets上的认知性和泛化能力得到了证明，特别是在识别不同类型的钢材损坏时表现出色。<details>
<summary>Abstract</summary>
Efficient and accurate detection of small objects in manufacturing settings, such as defects and cracks, is crucial for ensuring product quality and safety. To address this issue, we proposed a comprehensive strategy by synergizing Faster R-CNN with cutting-edge methods. By combining Faster R-CNN with Feature Pyramid Network, we enable the model to efficiently handle multi-scale features intrinsic to manufacturing environments. Additionally, Deformable Net is used that contorts and conforms to the geometric variations of defects, bringing precision in detecting even the minuscule and complex features. Then, we incorporated an attention mechanism called Convolutional Block Attention Module in each block of our base ResNet50 network to selectively emphasize informative features and suppress less useful ones. After that we incorporated RoI Align, replacing RoI Pooling for finer region-of-interest alignment and finally the integration of Focal Loss effectively handles class imbalance, crucial for rare defect occurrences. The rigorous evaluation of our model on both the NEU-DET and Pascal VOC datasets underscores its robust performance and generalization capabilities. On the NEU-DET dataset, our model exhibited a profound understanding of steel defects, achieving state-of-the-art accuracy in identifying various defects. Simultaneously, when evaluated on the Pascal VOC dataset, our model showcases its ability to detect objects across a wide spectrum of categories within complex and small scenes.
</details>
<details>
<summary>摘要</summary>
efficient和准确的小对象检测在制造环境中是至关重要的，以确保产品质量和安全。为解决这个问题，我们提出了一项涵合策略，将Faster R-CNN与前沿技术相结合。通过将Faster R-CNN与Feature Pyramid Network结合使用，我们让模型能够有效地处理制造环境中的多尺度特征。此外，我们还使用了Deformable Net，它可以根据缺陷的几何变化进行扭形和适应，提高缺陷检测的精度。接着，我们在每个基本ResNet50网络块中添加了Convolutional Block Attention Module，以选择特征中的有用信息，并抑制无用的信息。然后，我们将RoI Align取代RoI Pooling，以实现更细的区域对齐。最后，我们通过集成Focal Loss来有效地处理类偏好，这是对罕见缺陷的检测中非常重要。我们在NEU-DET和Pascal VOC数据集上进行了严格的评估，并证明了我们的模型在不同的环境下具有出色的稳定性和泛化能力。在NEU-DET数据集上，我们的模型对钢铁缺陷进行了深刻的理解，实现了不同缺陷的状况下的最高精度。同时，当我们的模型在Pascal VOC数据集上进行评估时，它展示了对多种类别的对象检测的能力，并在复杂和小的场景中具有出色的检测能力。
</details></li>
</ul>
<hr>
<h2 id="Harmonic-Self-Conditioned-Flow-Matching-for-Multi-Ligand-Docking-and-Binding-Site-Design"><a href="#Harmonic-Self-Conditioned-Flow-Matching-for-Multi-Ligand-Docking-and-Binding-Site-Design" class="headerlink" title="Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design"></a>Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05764">http://arxiv.org/abs/2310.05764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hannesstark/flowsite">https://github.com/hannesstark/flowsite</a></li>
<li>paper_authors: Hannes Stärk, Bowen Jing, Regina Barzilay, Tommi Jaakkola</li>
<li>for: 这个论文的目的是设计蛋白质结构域以便与小分子结合。</li>
<li>methods: 论文使用了一种名为HarmonicFlow的改进的生成过程，用于生成3D蛋白质-小分子结合结构。这个过程还可以同时生成蛋白质结构域中的精确残基类型和小分子的结合3D结构。</li>
<li>results: 论文表明，HarmonicFlow在简洁性、通用性和性能方面都超过了现有的生成过程，并且可以设计蛋白质结构域的精确 binding 结构。这种结构模型使得FlowSite可以设计精确的蛋白质结构域，并提供了首个通用的蛋白质结构域设计方法。<details>
<summary>Abstract</summary>
A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon the state-of-the-art generative processes for docking in simplicity, generality, and performance. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches and provides the first general solution for binding site design.
</details>
<details>
<summary>摘要</summary>
一些蛋白质功能需要与小分子结合，包括enzymatic catalysis。因此，设计小分子结合 pocket 有很多有效的应用，从药物合成到能量储存。为达到这个目标，我们首先开发了 HarmonicFlow，一种改进的生成过程，用于生成3D蛋白质-小分子结合结构。FlowSite 扩展了这种流模型，以同时生成蛋白质口袋中的粒子类型和分子的结合3D结构。我们表明，HarmonicFlow 在简洁性、通用性和性能方面都超越了状态元的生成过程。通过这种结构模型，FlowSite 可以设计结合站点得到substantially better than基线方法，并提供了第一个通用的结合站点设计解决方案。
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-OD-Matrix-Estimation-with-A-Deep-Learning-Method"><a href="#Large-Scale-OD-Matrix-Estimation-with-A-Deep-Learning-Method" class="headerlink" title="Large-Scale OD Matrix Estimation with A Deep Learning Method"></a>Large-Scale OD Matrix Estimation with A Deep Learning Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05753">http://arxiv.org/abs/2310.05753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheli Xiong, Defu Lian, Enhong Chen, Gang Chen, Xiaomin Cheng</li>
<li>for: 实时数据驱动的交通流量统计分析</li>
<li>methods:  combining deep learning和数值优化算法，将嵌入式数据与数据流聚合为数据统计</li>
<li>results: 提供了一个可靠且实时的交通流量统计方法，不dependent on prior information，并且可以减少工程费用<details>
<summary>Abstract</summary>
The estimation of origin-destination (OD) matrices is a crucial aspect of Intelligent Transport Systems (ITS). It involves adjusting an initial OD matrix by regressing the current observations like traffic counts of road sections (e.g., using least squares). However, the OD estimation problem lacks sufficient constraints and is mathematically underdetermined. To alleviate this problem, some researchers incorporate a prior OD matrix as a target in the regression to provide more structural constraints. However, this approach is highly dependent on the existing prior matrix, which may be outdated. Others add structural constraints through sensor data, such as vehicle trajectory and speed, which can reflect more current structural constraints in real-time. Our proposed method integrates deep learning and numerical optimization algorithms to infer matrix structure and guide numerical optimization. This approach combines the advantages of both deep learning and numerical optimization algorithms. The neural network(NN) learns to infer structural constraints from probe traffic flows, eliminating dependence on prior information and providing real-time performance. Additionally, due to the generalization capability of NN, this method is economical in engineering. We conducted tests to demonstrate the good generalization performance of our method on a large-scale synthetic dataset. Subsequently, we verified the stability of our method on real traffic data. Our experiments provided confirmation of the benefits of combining NN and numerical optimization.
</details>
<details>
<summary>摘要</summary>
“OD矩阵估计是智能交通系统（ITS）中的一个重要问题。它需要对初始OD矩阵进行调整，使用最小二乘法 regression 来适应现有的观测数据（例如交通流量资料）。但是，OD估计问题缺乏足够的条件，从数学上是不充分决定的。为了解决这个问题，一些研究人员将 Target OD 矩阵添加到 regression 中，以提供更多的构造约束。但是，这种方法对于现有的 Target OD 矩阵依赖度太高，可能会受到旧有的矩阵影响。另一些研究人员通过感应器数据，如车辆轨迹和速度，添加更多的构造约束。我们的提案方法是通过深度学习和数值优化算法来推导矩阵结构，并将其与数值优化算法结合。这种方法结合了深度学习的优点和数值优化算法的稳定性。对于大规模的 sintetic 数据集，我们的方法具有良好的泛化性。进一步的，我们对真实交通数据进行验证，证明了我们的方法的稳定性和可靠性。我们的实验结果显示，结合深度学习和数值优化算法可以提供更好的性能和经济性。”
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-the-Ethics-of-Artificial-Intelligence-and-its-Applications-in-the-United-States"><a href="#A-Review-of-the-Ethics-of-Artificial-Intelligence-and-its-Applications-in-the-United-States" class="headerlink" title="A Review of the Ethics of Artificial Intelligence and its Applications in the United States"></a>A Review of the Ethics of Artificial Intelligence and its Applications in the United States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05751">http://arxiv.org/abs/2310.05751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esther Taiwo, Ahmed Akinsola, Edward Tella, Kolade Makinde, Mayowa Akinwande</li>
<li>For: The paper focuses on the ethical considerations of Artificial Intelligence (AI) in the United States, highlighting its impact on various sectors and entities, and the need for responsible and ethical AI practices.* Methods: The paper explores eleven fundamental ethical principles, including Transparency, Justice, Fairness, Equity, Non-Maleficence, Responsibility, Accountability, Privacy, Beneficence, Freedom, Autonomy, Trust, Dignity, Sustainability, and Solidarity, as a guiding framework for ethical AI development and deployment.* Results: The paper discusses the revolutionary impact of AI applications, such as Machine Learning, and explores various approaches used to implement AI ethics, addressing the growing concerns surrounding the inherent risks associated with the widespread use of AI.<details>
<summary>Abstract</summary>
This study is focused on the ethics of Artificial Intelligence and its application in the United States, the paper highlights the impact AI has in every sector of the US economy and multiple facets of the technological space and the resultant effect on entities spanning businesses, government, academia, and civil society. There is a need for ethical considerations as these entities are beginning to depend on AI for delivering various crucial tasks, which immensely influence their operations, decision-making, and interactions with each other. The adoption of ethical principles, guidelines, and standards of work is therefore required throughout the entire process of AI development, deployment, and usage to ensure responsible and ethical AI practices. Our discussion explores eleven fundamental 'ethical principles' structured as overarching themes. These encompass Transparency, Justice, Fairness, Equity, Non- Maleficence, Responsibility, Accountability, Privacy, Beneficence, Freedom, Autonomy, Trust, Dignity, Sustainability, and Solidarity. These principles collectively serve as a guiding framework, directing the ethical path for the responsible development, deployment, and utilization of artificial intelligence (AI) technologies across diverse sectors and entities within the United States. The paper also discusses the revolutionary impact of AI applications, such as Machine Learning, and explores various approaches used to implement AI ethics. This examination is crucial to address the growing concerns surrounding the inherent risks associated with the widespread use of artificial intelligence.
</details>
<details>
<summary>摘要</summary>
The paper proposes eleven fundamental ethical principles, structured as overarching themes, to guide the ethical development, deployment, and utilization of AI technologies. These principles include:1. Transparency2.  Justice3.  Fairness4.  Equity5.  Non-Maleficence6.  Responsibility7.  Accountability8.  Privacy9.  Beneficence10. Freedom11. Autonomy12. Trust13. Dignity14. Sustainability15. SolidarityThese principles collectively serve as a guiding framework for the ethical development and use of AI technologies in the United States. The paper also discusses the revolutionary impact of AI applications, such as Machine Learning, and explores various approaches used to implement AI ethics. This examination is crucial to address the growing concerns surrounding the inherent risks associated with the widespread use of AI.
</details></li>
</ul>
<hr>
<h2 id="Put-Your-Money-Where-Your-Mouth-Is-Evaluating-Strategic-Planning-and-Execution-of-LLM-Agents-in-an-Auction-Arena"><a href="#Put-Your-Money-Where-Your-Mouth-Is-Evaluating-Strategic-Planning-and-Execution-of-LLM-Agents-in-an-Auction-Arena" class="headerlink" title="Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena"></a>Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05746">http://arxiv.org/abs/2310.05746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiangjiechen/auction-arena">https://github.com/jiangjiechen/auction-arena</a></li>
<li>paper_authors: Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, Kyle Richardson</li>
<li>for: evaluating the ability of Large Language Models (LLMs) to simulate human behavior in complex environments, specifically in auctions.</li>
<li>methods: using a novel simulation environment called AucArena to test the ability of state-of-the-art LLMs as bidding agents in controlled simulations.</li>
<li>results: LLMs demonstrate advanced reasoning skills and ability to manage budget, adhere to long-term goals and priorities, but with considerable variability in capabilities and occasional surpassing by heuristic baselines and human agents, highlighting the potential for further improvements in agent design and the importance of simulation environments for testing and refining agent architectures.<details>
<summary>Abstract</summary>
Can Large Language Models (LLMs) simulate human behavior in complex environments? LLMs have recently been shown to exhibit advanced reasoning skills but much of NLP evaluation still relies on static benchmarks. Answering this requires evaluation environments that probe strategic reasoning in competitive, dynamic scenarios that involve long-term planning. We introduce AucArena, a novel simulation environment for evaluating LLMs within auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct several controlled simulations using state-of-the-art LLMs as bidding agents. We find that through simple prompting, LLMs do indeed demonstrate many of the skills needed for effectively engaging in auctions (e.g., managing budget, adhering to long-term goals and priorities), skills that we find can be sharpened by explicitly encouraging models to be adaptive and observe strategies in past auctions. These results are significant as they show the potential of using LLM agents to model intricate social dynamics, especially in competitive settings. However, we also observe considerable variability in the capabilities of individual LLMs. Notably, even our most advanced models (GPT-4) are occasionally surpassed by heuristic baselines and human agents, highlighting the potential for further improvements in the design of LLM agents and the important role that our simulation environment can play in further testing and refining agent architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Language-Model-Beats-Diffusion-–-Tokenizer-is-Key-to-Visual-Generation"><a href="#Language-Model-Beats-Diffusion-–-Tokenizer-is-Key-to-Visual-Generation" class="headerlink" title="Language Model Beats Diffusion – Tokenizer is Key to Visual Generation"></a>Language Model Beats Diffusion – Tokenizer is Key to Visual Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05737">http://arxiv.org/abs/2310.05737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/MAGVIT2">https://github.com/kyegomez/MAGVIT2</a></li>
<li>paper_authors: Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang</li>
<li>for: 用于提高语言模型（LLM）在语言生成任务中表现，并不是 diffusion 模型在图像和视频生成任务中表现好。</li>
<li>methods: 提出了一种名为 MAGVIT-v2 的视频tokenizer，可以生成高效的字符串表示图像和视频，并使用这个tokenizer，LLM 可以在标准图像和视频生成 benchmark 上表现出色。</li>
<li>results: 通过使用 MAGVIT-v2  tokenizer，LLM 可以超越 diffusion 模型在图像和视频生成任务中的表现，同时在视频压缩和动作识别任务中也表现出色。<details>
<summary>Abstract</summary>
While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.
</details>
<details>
<summary>摘要</summary>
LLMs 是语言生成任务中的主导模型，但它们在图像和视频生成任务中不如扩散模型表现为好。为了有效地使用 LLMs 进行视觉生成，一个关键组件是视觉 токен化器，它将 pixel-space 输入映射到适合 LLM 学习的精炼的 tokens。在这篇论文中，我们介绍了 MAGVIT-v2，一种用于生成简洁和表达力强的 видео和图像 tokens 的视觉 токен化器。我们使用这个新的 токен化器，我们展示了 LLMs 在标准的图像和视频生成 benchmark 上表现出色，并且在两个额外任务上表现出色：（1）与下一代视频编码器（VCC）相当的视频压缩，以及（2）学习有效的动作认知任务。
</details></li>
</ul>
<hr>
<h2 id="The-Program-Testing-Ability-of-Large-Language-Models-for-Code"><a href="#The-Program-Testing-Ability-of-Large-Language-Models-for-Code" class="headerlink" title="The Program Testing Ability of Large Language Models for Code"></a>The Program Testing Ability of Large Language Models for Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05727">http://arxiv.org/abs/2310.05727</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Weimin Xiong, Yiwen Guo, Hao Chen</li>
<li>for: 这 paper 探讨了大型语言模型（LLMs）在代码测试方面的能力。</li>
<li>methods: 这 paper 使用了一系列的方法来测试 LLMs，包括人工评估和 MBPP 等数据集。</li>
<li>results: 这 paper 显示了 LLMs 在代码测试方面的一些有趣的性质，并通过使用生成的测试用例提高了代码质量，从而提高了代码的执行率。<details>
<summary>Abstract</summary>
Recent development of large language models (LLMs) for code like CodeX and CodeT5+ demonstrates tremendous promise in achieving code intelligence. Their ability of synthesizing code that completes a program for performing a pre-defined task has been intensively tested and verified on benchmark datasets including HumanEval and MBPP. Yet, evaluation of these LLMs from more perspectives (than just program synthesis) is also anticipated, considering their broad scope of applications in software engineering. In this paper, we explore the ability of LLMs for testing programs/code. By performing thorough analyses of recent LLMs for code in program testing, we show a series of intriguing properties of these models and demonstrate how program testing ability of LLMs can be improved. Following recent work which utilizes generated test cases to enhance program synthesis, we further leverage our findings in improving the quality of the synthesized programs and show +11.77% and +4.22% higher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline and the recent state-of-the-art, respectively.
</details>
<details>
<summary>摘要</summary>
In this paper, we explore the ability of LLMs for testing programs/code. We conduct a thorough analysis of recent LLMs for code in program testing and identify several intriguing properties of these models. Furthermore, we demonstrate how the program testing ability of LLMs can be improved, building on recent work that utilizes generated test cases to enhance program synthesis. Our findings lead to a +11.77% and +4.22% increase in code pass rates on HumanEval+ compared to the GPT-3.5-turbo baseline and the recent state-of-the-art, respectively.
</details></li>
</ul>
<hr>
<h2 id="STOPNet-Multiview-based-6-DoF-Suction-Detection-for-Transparent-Objects-on-Production-Lines"><a href="#STOPNet-Multiview-based-6-DoF-Suction-Detection-for-Transparent-Objects-on-Production-Lines" class="headerlink" title="STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects on Production Lines"></a>STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects on Production Lines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05717">http://arxiv.org/abs/2310.05717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Kuang, Qin Han, Danshi Li, Qiyu Dai, Lian Ding, Dong Sun, Hanlin Zhao, He Wang</li>
<li>for: 本文提出了一种用于生产线上6DoF物体抓取检测的框架，特别是透明物体，这是机器人系统和现代工业中的重要和困难问题。</li>
<li>methods: 我们提出了一种基于多视图涂抹的新方法，只使用RGB输入，能够重建生产线上的场景，并在真实世界中获得高质量的6DoF抓取姿势。</li>
<li>results: 对于现有方法，我们的方法在实验和实际应用中具有更好的普适性和更高的性能，可满足实际工业的需求。<details>
<summary>Abstract</summary>
In this work, we present STOPNet, a framework for 6-DoF object suction detection on production lines, with a focus on but not limited to transparent objects, which is an important and challenging problem in robotic systems and modern industry. Current methods requiring depth input fail on transparent objects due to depth cameras' deficiency in sensing their geometry, while we proposed a novel framework to reconstruct the scene on the production line depending only on RGB input, based on multiview stereo. Compared to existing works, our method not only reconstructs the whole 3D scene in order to obtain high-quality 6-DoF suction poses in real time but also generalizes to novel environments, novel arrangements and novel objects, including challenging transparent objects, both in simulation and the real world. Extensive experiments in simulation and the real world show that our method significantly surpasses the baselines and has better generalizability, which caters to practical industrial needs.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了STOPNet，一个用于生产线上6个自由度物体捕捉检测的框架，强调但不限于透明物体，这是现代机器人系统和现代工业中的一个重要和困难的问题。现有的方法，需要深度输入，在透明物体上失败，因为深度摄像头无法感知其几何结构，而我们提出了一种新的框架，基于多视图零点投影，可以在RGB输入基础上重建生产线上的场景，并且可以在实时获得高质量的6个自由度捕捉姿势。与现有的方法相比，我们的方法不仅可以重建整个3D场景，以获得高质量的捕捉姿势，而且可以在新环境、新排序和新物体上普遍，包括实际上的挑战性透明物体，并在实际世界中达到了更好的普遍性。广泛的实验在实际世界和 simulate 中表明，我们的方法在比较基eline上显著超越了基eline，并且具有更好的普遍性，这符合实际工业需求。
</details></li>
</ul>
<hr>
<h2 id="Guiding-Language-Model-Reasoning-with-Planning-Tokens"><a href="#Guiding-Language-Model-Reasoning-with-Planning-Tokens" class="headerlink" title="Guiding Language Model Reasoning with Planning Tokens"></a>Guiding Language Model Reasoning with Planning Tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05707">http://arxiv.org/abs/2310.05707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, Alessandro Sordoni</li>
<li>for: 提高大语言模型（LLMs）的复杂逻辑能力，特别是链式思维能力。</li>
<li>methods: 引入”规划符”（planning tokens）作为模型的引导，并在模型参数中微调这些符号表示。</li>
<li>results: 在三个数学问题 datasets 上，与普通的链式思维精心 fine-tuning 基eline 相比，示出了明显的准确性改善。<details>
<summary>Abstract</summary>
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce 'planning tokens' at the start of each reasoning step, serving as a guide for the model. These token embeddings are then fine-tuned along with the rest of the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. plain chain-of-thought fine-tuning baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Attribution-Method-for-Siamese-Encoders"><a href="#An-Attribution-Method-for-Siamese-Encoders" class="headerlink" title="An Attribution Method for Siamese Encoders"></a>An Attribution Method for Siamese Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05703">http://arxiv.org/abs/2310.05703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Möller, Dmitry Nikolaev, Sebastian Padó</li>
<li>for: 本研究旨在探讨siamese encoder模型如sentence transformers（ST）在处理输入时关注哪些方面。</li>
<li>methods: 本研究使用了一种基于integrated gradients的本地 attribute方法，通过将多个输入转化为对应的feature-pair attribute。</li>
<li>results: 研究发现，ST在做出预测时通常只需要关注一些token-pairs，但是为了准确预测，需要关注大多数token和parts of speech。<details>
<summary>Abstract</summary>
Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The solution takes the form of feature-pair attributions, and can be reduced to a token-token matrix for STs. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in an ST few token-pairs can often explain large fractions of predictions, and it focuses on nouns and verbs. For accurate predictions, it however needs to attend to the majority of tokens and parts of speech.
</details>
<details>
<summary>摘要</summary>
尽管SIAMESE编码器模型如sentence transformers（ST）在成功的背景下，仍然知之少于其处理输入的方面。一个障碍是它们的预测无法归因于单个特征，因为它们比较两个输入而不是处理单个输入。这篇论文提出了一种本地归因方法 для SIAMESE编码器模型，通过泛化集成导数原理来对多输入模型进行归因。该方法的解释形式为对应对方特征归因，可以将其减少到一个单词单词的矩阵中，并且具有集成导数的优点：它考虑了模型的全部计算图和是确定的归因方法。一项试点研究显示，在ST中，只需要几对单词可以解释大量预测，并且它们主要集中在名词和动词上。然而，为了准确预测，它们需要对大多数单词和部分语法进行注意。
</details></li>
</ul>
<hr>
<h2 id="Based-on-What-We-Can-Control-Artificial-Neural-Networks"><a href="#Based-on-What-We-Can-Control-Artificial-Neural-Networks" class="headerlink" title="Based on What We Can Control Artificial Neural Networks"></a>Based on What We Can Control Artificial Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05692">http://arxiv.org/abs/2310.05692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Cheng Kang, Xujing Yao</li>
<li>for: 本研究旨在提高人工神经网络（ANNs）的稳定性和效率，通过系统分析方法。</li>
<li>methods: 本研究使用控制系统知识来分析ANNs的系统功能，模拟系统响应。尽管大多数ANNs的复杂性很高，但我们仍然可以分析每个因素（例如优化器、超参数）的系统响应。</li>
<li>results: 本研究提出了一种新的分析方法，可以帮助确保ANNs的学习过程的稳定性和效率。这种方法还可能对新的优化器和学习系统的开发产生影响，特别是当找出哪些组件对ANNs产生负面影响时。请参考：\url{<a target="_blank" rel="noopener" href="https://github.com/RandomUserName2023/Control-ANNs%7D%E3%80%82">https://github.com/RandomUserName2023/Control-ANNs}。</a><details>
<summary>Abstract</summary>
How can the stability and efficiency of Artificial Neural Networks (ANNs) be ensured through a systematic analysis method? This paper seeks to address that query. While numerous factors can influence the learning process of ANNs, utilizing knowledge from control systems allows us to analyze its system function and simulate system responses. Although the complexity of most ANNs is extremely high, we still can analyze each factor (e.g., optimiser, hyperparameters) by simulating their system response. This new method also can potentially benefit the development of new optimiser and learning system, especially when discerning which components adversely affect ANNs. Controlling ANNs can benefit from the design of optimiser and learning system, as (1) all optimisers act as controllers, (2) all learning systems operate as control systems with inputs and outputs, and (3) the optimiser should match the learning system. Please find codes: \url{https://github.com/RandomUserName2023/Control-ANNs}.
</details>
<details>
<summary>摘要</summary>
如何确保人工神经网络（ANNs）的稳定性和效率？这篇论文旨在回答这个问题。虽然多种因素可能影响 ANNs 的学习过程，但通过知识控制系统来分析其系统功能并模拟系统响应。虽然大多数 ANNs 的复杂度很高，但我们仍可以分析每个因素（例如优化器、超参数） by 模拟它们的系统响应。这种新方法还可能为 ANNs 的发展提供新的优化器和学习系统，特别是当探测那些组件对 ANNs 产生负面影响时。控制 ANNs 可以从优化器和学习系统的设计中受益，因为（1）所有优化器都是控制器，（2）所有学习系统都是控制系统，（3）优化器应该与学习系统匹配。请找到代码：https://github.com/RandomUserName2023/Control-ANNs。
</details></li>
</ul>
<hr>
<h2 id="Abstractive-Summarization-of-Large-Document-Collections-Using-GPT"><a href="#Abstractive-Summarization-of-Large-Document-Collections-Using-GPT" class="headerlink" title="Abstractive Summarization of Large Document Collections Using GPT"></a>Abstractive Summarization of Large Document Collections Using GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05690">http://arxiv.org/abs/2310.05690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sengjie Liu, Christopher G. Healey</li>
<li>for: 本研究提出了一种可扩展到文档收集的抽象概要方法。</li>
<li>methods: 该方法使用了 semantic clustering、文档内部话题减少、semantic chunking、GPT基于概要和 concatenation 等方法。</li>
<li>results: 对比 exist 状态的 art 系统 BART、BRIO、PEGASUS 和 MoCa 的 ROGUE 摘要分数，本研究在 CNN&#x2F;Daily Mail 测试集上与 BART 和 PEGASUS 相当，在 Gigaword 测试集上与 BART 相当。<details>
<summary>Abstract</summary>
This paper proposes a method of abstractive summarization designed to scale to document collections instead of individual documents. Our approach applies a combination of semantic clustering, document size reduction within topic clusters, semantic chunking of a cluster's documents, GPT-based summarization and concatenation, and a combined sentiment and text visualization of each topic to support exploratory data analysis. Statistical comparison of our results to existing state-of-the-art systems BART, BRIO, PEGASUS, and MoCa using ROGUE summary scores showed statistically equivalent performance with BART and PEGASUS on the CNN/Daily Mail test dataset, and with BART on the Gigaword test dataset. This finding is promising since we view document collection summarization as more challenging than individual document summarization. We conclude with a discussion of how issues of scale are
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种抽象摘要方法，旨在对文档集合进行摘要而不是单个文档。我们的方法使用了 semantics 归一化、文档内容减少、semantic 块分割、基于 GPT 的摘要和 concatenation，以及每个话题的感情和文本视觉表示。我们通过对 ROGUE 摘要分数进行统计比较，与现有的状态 искус数据集 BART、BRIO、PEGASUS 和 MoCa 进行比较，在 CNN/Daily Mail 测试集上与 BART 和 PEGASUS 相当，在 Gigaword 测试集上与 BART 相当。这一结果是有前途的，因为我们视文档集合摘要为单个文档摘要更加困难。我们在结尾采用了一些问题的扩展和未来工作的讨论。
</details></li>
</ul>
<hr>
<h2 id="The-potential-of-large-language-models-for-improving-probability-learning-A-study-on-ChatGPT3-5-and-first-year-computer-engineering-students"><a href="#The-potential-of-large-language-models-for-improving-probability-learning-A-study-on-ChatGPT3-5-and-first-year-computer-engineering-students" class="headerlink" title="The potential of large language models for improving probability learning: A study on ChatGPT3.5 and first-year computer engineering students"></a>The potential of large language models for improving probability learning: A study on ChatGPT3.5 and first-year computer engineering students</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05686">http://arxiv.org/abs/2310.05686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angel Udias, Antonio Alonso-Ayuso, Ignacio Sanchez, Sonia Hernandez, Maria Eugenia Castellanos, Raquel Montes Diez, Emilio Lopez Cano</li>
<li>For: The paper assesses the efficacy of ChatGPT in solving probability problems typically presented in introductory computer engineering exams.* Methods: The study uses a set of 23 probability exercises administered to students at Rey Juan Carlos University (URJC) in Madrid, and evaluates the responses produced by ChatGPT qualitatively, assigning grades based on the same criteria used for students.* Results: The results indicate that ChatGPT surpasses the average student in terms of phrasing, organization, and logical reasoning, and the model’s performance remained consistent for both the Spanish and English versions of the exercises. However, ChatGPT encountered difficulties in executing basic numerical operations, which were overcome by requesting the solution in the form of an R script.<details>
<summary>Abstract</summary>
In this paper, we assess the efficacy of ChatGPT (version Feb 2023), a large-scale language model, in solving probability problems typically presented in introductory computer engineering exams. Our study comprised a set of 23 probability exercises administered to students at Rey Juan Carlos University (URJC) in Madrid. The responses produced by ChatGPT were evaluated by a group of five statistics professors, who assessed them qualitatively and assigned grades based on the same criteria used for students. Our results indicate that ChatGPT surpasses the average student in terms of phrasing, organization, and logical reasoning. The model's performance remained consistent for both the Spanish and English versions of the exercises. However, ChatGPT encountered difficulties in executing basic numerical operations. Our experiments demonstrate that requesting ChatGPT to provide the solution in the form of an R script proved to be an effective approach for overcoming these limitations. In summary, our results indicate that ChatGPT surpasses the average student in solving probability problems commonly presented in introductory computer engineering exams. Nonetheless, the model exhibits limitations in reasoning around certain probability concepts. The model's ability to deliver high-quality explanations and illustrate solutions in any programming language, coupled with its performance in solving probability exercises, suggests that large language models have the potential to serve as learning assistants.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们评估了ChatGPT（版本为2月2023）大型语言模型在解probability问题方面的效果。我们的研究包括23个probability问题，对于马德里 Rey Juan Carlos大学（URJC）的学生进行了测试。ChatGPT的答案由5名统计教授评估，他们根据同样的标准评分学生的答案。我们的结果表明，ChatGPT在表达、组织和逻辑推理方面超过了学生的平均水平。模型在西班牙语和英语版probability问题上表现一致。然而，ChatGPT在基本数学运算方面遇到了困难。我们的实验表明，向ChatGPT请求提供解决方案的R脚本形式是一种有效的方法，以超越这些限制。总之，我们的结果表明，ChatGPT在入门计算机工程考试中常见的probability问题方面表现出色，但模型在某些概率概念上存在限制。模型能够提供高质量的解释和在任何编程语言中示例解决方案，表明大语言模型有可能作为学习助手。
</details></li>
</ul>
<hr>
<h2 id="Automated-Argument-Generation-from-Legal-Facts"><a href="#Automated-Argument-Generation-from-Legal-Facts" class="headerlink" title="Automated Argument Generation from Legal Facts"></a>Automated Argument Generation from Legal Facts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05680">http://arxiv.org/abs/2310.05680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Tuvey, Procheta Sen</li>
<li>For: The paper aims to enhance the efficiency and speed of legal procedures by utilizing AI technology to help legal professionals analyze legal cases.* Methods: The paper uses open-sourced large language models to create arguments derived from the facts present in legal cases.* Results: The generated arguments from the best performing method have on average 63% overlap with the benchmark set gold standard annotations.Here are the three key points in Simplified Chinese text:* For: 这项研究旨在利用人工智能技术，提高法律程序的效率和速度。* Methods: 这项研究使用开源大型自然语言模型，生成法律案例中的事实所基于的Arguments。* Results: 最佳方法生成的Arguments在基本标准注释中的重合率平均为63%。<details>
<summary>Abstract</summary>
The count of pending cases has shown an exponential rise across nations (e.g., with more than 10 million pending cases in India alone). The main issue lies in the fact that the number of cases submitted to the law system is far greater than the available number of legal professionals present in a country. Given this worldwide context, the utilization of AI technology has gained paramount importance to enhance the efficiency and speed of legal procedures. In this study we partcularly focus on helping legal professionals in the process of analyzing a legal case. Our specific investigation delves into harnessing the generative capabilities of open-sourced large language models to create arguments derived from the facts present in legal cases. Experimental results show that the generated arguments from the best performing method have on average 63% overlap with the benchmark set gold standard annotations.
</details>
<details>
<summary>摘要</summary>
全球各国案件数量在急增（例如印度单独已经有超过1000万个案件）。主要问题在于法律系统内的案件数量比法律专业人员的数量更多。视这种全球背景，利用人工智能技术已成为提高法律程序效率和速度的重要手段。本研究专注于帮助法律专业人员分析法律案件。我们的特定调查是利用开源大型自然语言模型的生成能力来从法律案件中生成基于事实的法律Arguments。实验结果显示，最佳方法生成的Arguments的平均 overlap率为63%。
</details></li>
</ul>
<hr>
<h2 id="Making-Scalable-Meta-Learning-Practical"><a href="#Making-Scalable-Meta-Learning-Practical" class="headerlink" title="Making Scalable Meta Learning Practical"></a>Making Scalable Meta Learning Practical</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05674">http://arxiv.org/abs/2310.05674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leopard-ai/betty">https://github.com/leopard-ai/betty</a></li>
<li>paper_authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, Willie Neiswanger, Pengtao Xie, Emma Strubell, Eric Xing</li>
<li>for: 本研究旨在实现可扩展的元学习实现，提高元学习的可扩展性和可行性。</li>
<li>methods: 本研究使用SAMA算法，其combines implicit differentiation algorithms和系统，以提高元学习的可扩展性和可行性。SAMA支持广泛的适应性optimizers，并通过避免显式计算第二阶导数信息，以及利用高效的分布式训练技术，提高元学习的可扩展性和可行性。</li>
<li>results: 在多个大规模元学习benchmark上测试，SAMA比基eline元学习算法具有1.7&#x2F;4.8倍的 Throughput和2.0&#x2F;3.8倍的内存占用率，单&#x2F;多GPU集成。此外，SAMA在BERT和RoBERTa大语言模型中进行文本分类任务中，以及在图像分类任务中进行数据优化，均实现了顺利的改进，并达到了小规模和大规模数据剪辑的状态机器。<details>
<summary>Abstract</summary>
Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
</details>
<details>
<summary>摘要</summary>
尽管机器学习中的元学习（即学习学习）具有学习多种启发的灵活性，但长期以来，元学习受到了计算/存储成本过高、训练不稳定和分布式训练支持不充分的问题困扰。在这项工作中，我们关注使得元学习可扩展的实用性，通过引入SAMA来实现。SAMA组合了隐式 diferentiation算法和系统技术，特别是在基础级元学习程序中支持广泛的适应化优化器，同时减少计算负担，避免直接计算第二阶导数信息，并利用高效的分布式训练技术实现。在多个大规模元学习标准 benchmark 上评估，SAMA显示在单/多GPU设置下具有1.7/4.8倍的throughput和2.0/3.8倍的内存占用量，相比其他基eline元学习算法。此外，我们表明SAMA可以在语言和视觉领域中实现可扩展的数据优化，并在BERT和RoBERTa大语言模型中表现出了一致的提升，并在图像分类任务中实现了小规模和大规模数据减少的状态环境。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-learning-for-freeform-robot-design"><a href="#Reinforcement-learning-for-freeform-robot-design" class="headerlink" title="Reinforcement learning for freeform robot design"></a>Reinforcement learning for freeform robot design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05670">http://arxiv.org/abs/2310.05670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhan Li, David Matthews, Sam Kriegman</li>
<li>for: 探讨了用Policy Gradients来设计自由形机器人的方法</li>
<li>methods: 使用动作将atomic building block bundle归并或移除，形成非parametric macrostructure如肢体、器官和腔体</li>
<li>results: 实现了开 loop控制，未来可以适应closed loop控制和 sim2real转移到物理机器人<details>
<summary>Abstract</summary>
Inspired by the necessity of morphological adaptation in animals, a growing body of work has attempted to expand robot training to encompass physical aspects of a robot's design. However, reinforcement learning methods capable of optimizing the 3D morphology of a robot have been restricted to reorienting or resizing the limbs of a predetermined and static topological genus. Here we show policy gradients for designing freeform robots with arbitrary external and internal structure. This is achieved through actions that deposit or remove bundles of atomic building blocks to form higher-level nonparametric macrostructures such as appendages, organs and cavities. Although results are provided for open loop control only, we discuss how this method could be adapted for closed loop control and sim2real transfer to physical machines in future.
</details>
<details>
<summary>摘要</summary>
受动物形态适应的需要启发，一组增长的研究尝试将机器人训练扩展到物理机器人设计的方面。然而，使用奖励学习方法优化3D机器人形态的方法一直受限于重定向或缩放预先预定的顺序型机器人的臂部。我们现在显示了一种使用政策偏好来设计自由形机器人，这种方法通过执行填充或 removing 粒子堆集来形成高级非参数 macrostructure，如肢体、器官和腔体。虽然我们只提供了开loop控制的结果，但我们讨论了如何将这种方法适应到closed loop控制和 sim2real 转移到物理机器人的未来。
</details></li>
</ul>
<hr>
<h2 id="ViTs-are-Everywhere-A-Comprehensive-Study-Showcasing-Vision-Transformers-in-Different-Domain"><a href="#ViTs-are-Everywhere-A-Comprehensive-Study-Showcasing-Vision-Transformers-in-Different-Domain" class="headerlink" title="ViTs are Everywhere: A Comprehensive Study Showcasing Vision Transformers in Different Domain"></a>ViTs are Everywhere: A Comprehensive Study Showcasing Vision Transformers in Different Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05664">http://arxiv.org/abs/2310.05664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sohag Mia, Abu Bakor Hayat Arnob, Abdu Naim, Abdullah Al Bary Voban, Md Shariful Islam</li>
<li>for: 这篇论文主要关注于Computer Vision（CV）领域中的Transformer设计，以及它在不同CV应用中的表现。</li>
<li>methods: 这篇论文使用了多种Vision Transformer（ViT）模型，并对它们进行了分类和比较，以找出它们在不同CV应用中的优势和缺陷。</li>
<li>results: 研究发现，ViTs在多种CV应用中表现出优于Convolutional Neural Networks（CNNs），包括图像分类、物体识别、图像 segmentation、视频变换、图像净化和NAS等。同时，研究还提出了许多未解决的问题和潜在的研究机会。<details>
<summary>Abstract</summary>
Transformer design is the de facto standard for natural language processing tasks. The success of the transformer design in natural language processing has lately piqued the interest of researchers in the domain of computer vision. When compared to Convolutional Neural Networks (CNNs), Vision Transformers (ViTs) are becoming more popular and dominant solutions for many vision problems. Transformer-based models outperform other types of networks, such as convolutional and recurrent neural networks, in a range of visual benchmarks. We evaluate various vision transformer models in this work by dividing them into distinct jobs and examining their benefits and drawbacks. ViTs can overcome several possible difficulties with convolutional neural networks (CNNs). The goal of this survey is to show the first use of ViTs in CV. In the first phase, we categorize various CV applications where ViTs are appropriate. Image classification, object identification, image segmentation, video transformer, image denoising, and NAS are all CV applications. Our next step will be to analyze the state-of-the-art in each area and identify the models that are currently available. In addition, we outline numerous open research difficulties as well as prospective research possibilities.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>变换器设计已经成为自然语言处理任务的逻辑标准。随着变换器设计在自然语言处理领域的成功，研究人员开始关注这种设计在计算机视觉领域的应用。与卷积神经网络（CNN）相比，视力变换器（ViT）在许多视觉问题上变得更加受欢迎和主导性。基于变换器的模型在各种视觉标准上表现出色，超过了卷积神经网络和回归神经网络的性能。在这项工作中，我们将对不同的视觉变换器模型进行分类和分析，描述其优缺点。ViT可以超越卷积神经网络的一些可能的困难。本文的目标是在计算机视觉领域内，首次使用ViT。在第一个阶段，我们将分类各种适用于计算机视觉应用的CV应用程序。包括图像分类、物体识别、图像分割、视频变换、图像净化和NAS等。接下来，我们将分析每个领域的现状，并识别目前可用的模型。此外，我们还将列出许多开放的研究Difficulties和前景。
</details></li>
</ul>
<hr>
<h2 id="Causal-structure-learning-with-momentum-Sampling-distributions-over-Markov-Equivalence-Classes-of-DAGs"><a href="#Causal-structure-learning-with-momentum-Sampling-distributions-over-Markov-Equivalence-Classes-of-DAGs" class="headerlink" title="Causal structure learning with momentum: Sampling distributions over Markov Equivalence Classes of DAGs"></a>Causal structure learning with momentum: Sampling distributions over Markov Equivalence Classes of DAGs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05655">http://arxiv.org/abs/2310.05655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mschauer/CausalInference.jl">https://github.com/mschauer/CausalInference.jl</a></li>
<li>paper_authors: Moritz Schauer, Marcel Wienöbst</li>
<li>for: INFERRING BAYESIAN NETWORK STRUCTURE (DIRECTED Acyclic Graph, DAG FOR SHORT)</li>
<li>methods: NON-REVERSIBLE CONTINUOUS TIME MARKOV CHAIN (CAUSAL ZIG-ZAG SAMPLER) TARGETING A PROBABILITY DISTRIBUTION OVER CLASSES OF OBSERVATIONALLY EQUIVALENT (MARKOV EQUIVALENT) DAGs</li>
<li>results: IMPROVED MIXING COMPARED TO STATE-OF-THE-ART IMPLEMENTATIONS USING GREEDY EQUIVALENCE SEARCH (GES) OPERATORS WITH A MOMENTUM VARIABLE, AND EFFICIENT IMPLEMENTATION OF LISTING, COUNTING, UNIFORMLY SAMPLING, AND APPLYING POSSIBLE MOVES OF GES OPERATORS.<details>
<summary>Abstract</summary>
In the context of inferring a Bayesian network structure (directed acyclic graph, DAG for short), we devise a non-reversible continuous time Markov chain, the "Causal Zig-Zag sampler", that targets a probability distribution over classes of observationally equivalent (Markov equivalent) DAGs. The classes are represented as completed partially directed acyclic graphs (CPDAGs). The non-reversible Markov chain relies on the operators used in Chickering's Greedy Equivalence Search (GES) and is endowed with a momentum variable, which improves mixing significantly as we show empirically. The possible target distributions include posterior distributions based on a prior over DAGs and a Markov equivalent likelihood. We offer an efficient implementation wherein we develop new algorithms for listing, counting, uniformly sampling, and applying possible moves of the GES operators, all of which significantly improve upon the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
在推断 bayesian 网络结构（直接循环图，dag 简称）方面，我们设计了一种不可逆的连续时间马尔可夫链，称为“ causal zig-zag sampler”，该链targeted一个观察可Equivalence classes of observationally (Markov equivalent) DAGs的概率分布。这些类型被表示为完善的部分导向循环图（CPDAGs）。不可逆的马尔可夫链利用GES操作符，并具有一个势量变量，这使得混合得到了显著改善，我们在实验中验证了这一点。 possible target distributions include posterior distributions based on a prior over DAGs and a Markov equivalent likelihood。我们提供了高效的实现，其中包括开发了新的列表、计数、均匀采样和可能的移动操作算法，这些算法都有显著改善了现有状态的。
</details></li>
</ul>
<hr>
<h2 id="No-Token-Left-Behind-Efficient-Vision-Transformer-via-Dynamic-Token-Idling"><a href="#No-Token-Left-Behind-Efficient-Vision-Transformer-via-Dynamic-Token-Idling" class="headerlink" title="No Token Left Behind: Efficient Vision Transformer via Dynamic Token Idling"></a>No Token Left Behind: Efficient Vision Transformer via Dynamic Token Idling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05654">http://arxiv.org/abs/2310.05654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuwei Xu, Changlin Li, Yudong Chen, Xiaojun Chang, Jiajun Liu, Sen Wang</li>
<li>For: 这个论文目的是提出一种名为IdleViT的动态token遮瑕方法，以提高运算效率和表现力。* Methods: 这个方法选择每层中的一部分图像token参与计算，并将其他token直接传递到下一层的输出中。这样可以避免在早期阶段 improvident pruning 导致的 permanent loss of image information。此外，这个方法还使用了 normalized graph cut 的内置数据库损失来改善图像选择能力。* Results: 实验结果显示，IdleViT 可以将预训 ViT 的复杂度降低到 33%，而且只需要调整 30 次 epoch。此外，当保留比例为 0.5 时，IdleViT 可以在 DeiT-S 上比 EViT 高 0.5% 的精度和更快的测试速度。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have demonstrated outstanding performance in computer vision tasks, yet their high computational complexity prevents their deployment in computing resource-constrained environments. Various token pruning techniques have been introduced to alleviate the high computational burden of ViTs by dynamically dropping image tokens. However, some undesirable pruning at early stages may result in permanent loss of image information in subsequent layers, consequently hindering model performance. To address this problem, we propose IdleViT, a dynamic token-idle-based method that achieves an excellent trade-off between performance and efficiency. Specifically, in each layer, IdleViT selects a subset of the image tokens to participate in computations while keeping the rest of the tokens idle and directly passing them to this layer's output. By allowing the idle tokens to be re-selected in the following layers, IdleViT mitigates the negative impact of improper pruning in the early stages. Furthermore, inspired by the normalized graph cut, we devise a token cut loss on the attention map as regularization to improve IdleViT's token selection ability. Our method is simple yet effective and can be extended to pyramid ViTs since no token is completely dropped. Extensive experimental results on various ViT architectures have shown that IdleViT can diminish the complexity of pretrained ViTs by up to 33\% with no more than 0.2\% accuracy decrease on ImageNet, after finetuning for only 30 epochs. Notably, when the keep ratio is 0.5, IdleViT outperforms the state-of-the-art EViT on DeiT-S by 0.5\% higher accuracy and even faster inference speed. The source code is available in the supplementary material.
</details>
<details>
<summary>摘要</summary>
通过图像矩阵变换（ViT），计算机视觉任务的表现几乎不可思议，但是它们的计算复杂性使得在计算资源受限的环境中不能实施。为了解决这个问题，我们提出了IdleViT，一种基于动态token idle的方法，可以很好地平衡性能和效率。具体来说，在每层中，IdleViT选择图像中的一部分token参与计算，而保留剩下的token idle，直接将其传递给当前层的输出。通过在后续层中重新选择idletoken，IdleViT消除了在早期阶段的不合适剪裁所产生的负面影响。此外，我们根据 норма化图像排序（normalized graph cut）的思想，在注意力图中定义了一个token cut损失，以便提高IdleViT的token选择能力。我们的方法简单而有效，可以扩展到 pyramid ViTs，因为没有完全drop的token。我们在不同的ViT架构上进行了广泛的实验，并证明了IdleViT可以减少预训练ViT的复杂度达33%，只需要30个epoch的微调，而且在ImageNet上保持至少0.2%的准确率下。特别是，当保留比例为0.5时，IdleViT可以在DeiT-S上高于状态之前的EViT，增加0.5%的准确率和更快的执行速度。详细的源代码可以在补充材料中找到。
</details></li>
</ul>
<hr>
<h2 id="FENCE-Fairplay-Ensuring-Network-Chain-Entity-for-Real-Time-Multiple-ID-Detection-at-Scale-In-Fantasy-Sports"><a href="#FENCE-Fairplay-Ensuring-Network-Chain-Entity-for-Real-Time-Multiple-ID-Detection-at-Scale-In-Fantasy-Sports" class="headerlink" title="FENCE: Fairplay Ensuring Network Chain Entity for Real-Time Multiple ID Detection at Scale In Fantasy Sports"></a>FENCE: Fairplay Ensuring Network Chain Entity for Real-Time Multiple ID Detection at Scale In Fantasy Sports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05651">http://arxiv.org/abs/2310.05651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akriti Upreti, Kartavya Kothari, Utkarsh Thukral, Vishal Verma</li>
<li>for: 本文旨在解决 Dream11 平台上的多个账户创建问题，以防止用户利用平台的奖励提供程序。</li>
<li>methods: 本文提出了一种图形基的解决方案，首先预测用户之间的关系，然后检测欺诈账户的协同行为。</li>
<li>results: 本文介绍了一种分布式Machine Learning系统，用于支持和服务检测模型的推断。系统能够在实时中进行检测，以采取 corrrective actions。此外，文章还包括人类在Loop组件，用于验证、反馈和ground truth标注。<details>
<summary>Abstract</summary>
Dream11 takes pride in being a unique platform that enables over 190 million fantasy sports users to demonstrate their skills and connect deeper with their favorite sports. While managing such a scale, one issue we are faced with is duplicate/multiple account creation in the system. This is done by some users with the intent of abusing the platform, typically for bonus offers. The challenge is to detect these multiple accounts before it is too late. We propose a graph-based solution to solve this problem in which we first predict edges/associations between users. Using the edge information we highlight clusters of colluding multiple accounts. In this paper, we talk about our distributed ML system which is deployed to serve and support the inferences from our detection models. The challenge is to do this in real-time in order to take corrective actions. A core part of this setup also involves human-in-the-loop components for validation, feedback, and ground-truth labeling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Plug-n’-Play-Channel-Shuffle-Module-for-Enhancing-Tiny-Vision-Transformers"><a href="#Plug-n’-Play-Channel-Shuffle-Module-for-Enhancing-Tiny-Vision-Transformers" class="headerlink" title="Plug n’ Play: Channel Shuffle Module for Enhancing Tiny Vision Transformers"></a>Plug n’ Play: Channel Shuffle Module for Enhancing Tiny Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05642">http://arxiv.org/abs/2310.05642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuwei Xu, Sen Wang, Yudong Chen, Jiajun Liu</li>
<li>for: 提高小型变换器（ViT）的效率，使其适用于具有限定内存和计算资源的设备。</li>
<li>methods: 提出一种新的混合核心排序模块，通过在缓存频率较高的卷积层和自我关注机制之间进行信息交换，提高小型ViT的性能。</li>
<li>results: 在ImageNet-1K数据集上，通过 incorporating our module into tiny ViT models，可以提高top-1准确率，而且计算复杂度变化在0.03 GMACs以下。 Specifically, our proposed channel shuffle module consistently improves the top-1 accuracy by up to 2.8%.<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have demonstrated remarkable performance in various computer vision tasks. However, the high computational complexity hinders ViTs' applicability on devices with limited memory and computing resources. Although certain investigations have delved into the fusion of convolutional layers with self-attention mechanisms to enhance the efficiency of ViTs, there remains a knowledge gap in constructing tiny yet effective ViTs solely based on the self-attention mechanism. Furthermore, the straightforward strategy of reducing the feature channels in a large but outperforming ViT often results in significant performance degradation despite improved efficiency. To address these challenges, we propose a novel channel shuffle module to improve tiny-size ViTs, showing the potential of pure self-attention models in environments with constrained computing resources. Inspired by the channel shuffle design in ShuffleNetV2 \cite{ma2018shufflenet}, our module expands the feature channels of a tiny ViT and partitions the channels into two groups: the \textit{Attended} and \textit{Idle} groups. Self-attention computations are exclusively employed on the designated \textit{Attended} group, followed by a channel shuffle operation that facilitates information exchange between the two groups. By incorporating our module into a tiny ViT, we can achieve superior performance while maintaining a comparable computational complexity to the vanilla model. Specifically, our proposed channel shuffle module consistently improves the top-1 accuracy on the ImageNet-1K dataset for various tiny ViT models by up to 2.8\%, with the changes in model complexity being less than 0.03 GMACs.
</details>
<details>
<summary>摘要</summary>
《视图变换器》（ViTs）在计算机视觉任务中表现出色，但高计算复杂性限制了ViTs在有限内存和计算资源的设备上的应用。虽然一些研究已经探索了将卷积层与自注意机制结合使用以提高ViTs的效率，但还有一个知识空白在建立简单而高效的ViTssolely基于自注意机制。此外，通常减少大型ViT的特征通道会导致显著性能下降，尽管提高了效率。为了解决这些挑战，我们提出了一种新的通道排序模块，用于改进简单型ViTs。我们的模块基于ShuffleNetV2中的通道排序设计，将特征通道分成两组：“Attended”和“Idle”组。只有在“Attended”组上进行自注意计算，然后进行通道排序操作，以便在两组之间进行信息交换。通过将我们的模块纳入简单型ViT中，我们可以实现高性能，同时保持与标准模型的计算复杂性相似。具体来说，我们的提议的通道排序模块在ImageNet-1K数据集上的top-1准确率上提高了2.8%，而模型的变化量占0.03 GMACs以下。
</details></li>
</ul>
<hr>
<h2 id="Domain-Watermark-Effective-and-Harmless-Dataset-Copyright-Protection-is-Closed-at-Hand"><a href="#Domain-Watermark-Effective-and-Harmless-Dataset-Copyright-Protection-is-Closed-at-Hand" class="headerlink" title="Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand"></a>Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14942">http://arxiv.org/abs/2310.14942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junfenggo/domain-watermark">https://github.com/junfenggo/domain-watermark</a></li>
<li>paper_authors: Junfeng Guo, Yiming Li, Lixu Wang, Shu-Tao Xia, Heng Huang, Cong Liu, Bo Li</li>
<li>for: 保护开源数据集的版权，防止恶意模型攻击</li>
<li>methods: 基于域水印的数据集所有权验证，通过生成难 Sample来验证模型的准确性</li>
<li>results: 提出了一种基于域水印的数据集所有权验证方法，可以防止恶意模型攻击，并且有较高的鲁棒性和抗性能力。<details>
<summary>Abstract</summary>
The prosperity of deep neural networks (DNNs) is largely benefited from open-source datasets, based on which users can evaluate and improve their methods. In this paper, we revisit backdoor-based dataset ownership verification (DOV), which is currently the only feasible approach to protect the copyright of open-source datasets. We reveal that these methods are fundamentally harmful given that they could introduce malicious misclassification behaviors to watermarked DNNs by the adversaries. In this paper, we design DOV from another perspective by making watermarked models (trained on the protected dataset) correctly classify some `hard' samples that will be misclassified by the benign model. Our method is inspired by the generalization property of DNNs, where we find a \emph{hardly-generalized domain} for the original dataset (as its \emph{domain watermark}). It can be easily learned with the protected dataset containing modified samples. Specifically, we formulate the domain generation as a bi-level optimization and propose to optimize a set of visually-indistinguishable clean-label modified data with similar effects to domain-watermarked samples from the hardly-generalized domain to ensure watermark stealthiness. We also design a hypothesis-test-guided ownership verification via our domain watermark and provide the theoretical analyses of our method. Extensive experiments on three benchmark datasets are conducted, which verify the effectiveness of our method and its resistance to potential adaptive methods. The code for reproducing main experiments is available at \url{https://github.com/JunfengGo/Domain-Watermark}.
</details>
<details>
<summary>摘要</summary>
“深度神经网络（DNN）的繁荣得益于开源数据集，用户可以通过这些数据集进行评估和改进自己的方法。在这篇论文中，我们再次检视了基于DOV（ dataset ownership verification）的数据集权利保护方法，我们发现这些方法是根本不可靠的，因为它们可能会由 adversaries 引入黑客识别器模型中的恶意识别行为。在这篇论文中，我们从另一个角度设计 DOV，使得在训练在保护数据集上的损坏模型中，对一些难样本进行正确的识别。我们的方法是基于 DNN 的总体化性特性，我们在原始数据集中找到一个难以总化的Domain（领域），然后通过修改这些样本来学习一个 hardly-generalized 领域。我们将这个领域作为 DNN 的域水印，通过对这些修改后的样本进行训练，来确保 watermark 的隐蔽性。我们还设计了一种假设测试导向的所有权验证方法，并提供了方法的理论分析。我们在三个基准数据集上进行了广泛的实验，并证明了我们的方法的有效性和对可适应方法的抵御力。相关的代码可以在 GitHub 上找到：https://github.com/JunfengGo/Domain-Watermark。”
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Top-k-Estimation-Consolidates-Disagreement-between-Feature-Attribution-Methods"><a href="#Dynamic-Top-k-Estimation-Consolidates-Disagreement-between-Feature-Attribution-Methods" class="headerlink" title="Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods"></a>Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05619">http://arxiv.org/abs/2310.05619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Kamp, Lisa Beinborn, Antske Fokkens</li>
<li>for: 这paper是用来解释文本分类器的预测结果的方法。</li>
<li>methods: 这paper使用了几种不同的方法来计算特征归因分数，并评估了这些方法的性能。</li>
<li>results: 研究发现，使用固定k或动态k都可以得到高度一致的结果，但动态k主要提高了 интеграルGradient和GradientXInput的性能。这是首次证明了 attribute scores 的顺序性有用于人类理解。<details>
<summary>Abstract</summary>
Feature attribution scores are used for explaining the prediction of a text classifier to users by highlighting a k number of tokens. In this work, we propose a way to determine the number of optimal k tokens that should be displayed from sequential properties of the attribution scores. Our approach is dynamic across sentences, method-agnostic, and deals with sentence length bias. We compare agreement between multiple methods and humans on an NLI task, using fixed k and dynamic k. We find that perturbation-based methods and Vanilla Gradient exhibit highest agreement on most method--method and method--human agreement metrics with a static k. Their advantage over other methods disappears with dynamic ks which mainly improve Integrated Gradient and GradientXInput. To our knowledge, this is the first evidence that sequential properties of attribution scores are informative for consolidating attribution signals for human interpretation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_text="Feature attribution scores 是用于解释文本分类器预测结果的 Token 的准确性。在这种工作中，我们提出了一种方法来确定显示 k 个 Token 的优化数量，基于序列性质。我们的方法是动态 sentence，方法不依赖的，并且能够处理句子长度偏好。我们比较了多种方法和人类在 NLI 任务中的一致性，使用 fixes k 和动态 k。我们发现，扰动基于方法和 Vanilla Gradient 在大多数方法--方法和方法--人类协议中表现最高，其优势在 static k 下消失。这是我们知道的第一个证据，Sequential 性质是用于结合权重信号的准确信号的。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Multi-head-Contrastive-Learning"><a href="#Adaptive-Multi-head-Contrastive-Learning" class="headerlink" title="Adaptive Multi-head Contrastive Learning"></a>Adaptive Multi-head Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05615">http://arxiv.org/abs/2310.05615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Wang, Piotr Koniusz, Tom Gedeon, Liang Zheng<br>for: The paper is written to address the issue of inconsistent similarity measurements in contrastive learning, specifically when using multiple augmentation strategies.methods: The paper proposes using multiple projection heads, each producing a separate set of features, to improve the consistency of similarity measurements in contrastive learning. The loss function for pre-training is based on a solution to the maximum likelihood estimation over head-wise posterior distributions of positive samples given observations.results: The proposed adaptive multi-head contrastive learning (AMCL) method improves the performance of several popular contrastive learning methods, including SimCLR, MoCo, and Barlow Twins, under various backbones and linear probing epochs. The improvement is more significant when multiple augmentation methods are used.<details>
<summary>Abstract</summary>
In contrastive learning, two views of an original image generated by different augmentations are considered as a positive pair whose similarity is required to be high. Moreover, two views of two different images are considered as a negative pair, and their similarity is encouraged to be low. Normally, a single similarity measure given by a single projection head is used to evaluate positive and negative sample pairs, respectively. However, due to the various augmentation strategies and varying intra-sample similarity, augmented views from the same image are often not similar. Moreover, due to inter-sample similarity, augmented views of two different images may be more similar than augmented views from the same image. As such, enforcing a high similarity for positive pairs and a low similarity for negative pairs may not always be achievable, and in the case of some pairs, forcing so may be detrimental to the performance. To address this issue, we propose to use multiple projection heads, each producing a separate set of features. Our loss function for pre-training emerges from a solution to the maximum likelihood estimation over head-wise posterior distributions of positive samples given observations. The loss contains the similarity measure over positive and negative pairs, each re-weighted by an individual adaptive temperature that is regularized to prevent ill solutions. Our adaptive multi-head contrastive learning (AMCL) can be applied to and experimentally improves several popular contrastive learning methods such as SimCLR, MoCo and Barlow Twins. Such improvement is consistent under various backbones and linear probing epoches and is more significant when multiple augmentation methods are used.
</details>
<details>
<summary>摘要</summary>
在对比学习中，两个视图来自不同的扩充方法的原始图像被视为一个正样对，需要高度相似。同时，两个不同图像的两个视图被视为一个负样对，需要低度相似。通常情况下，单一的相似度测量由单个投影头提供，用于评估正样对和负样对。然而，由于不同的扩充策略和内样 Similarity 的变化，扩充视图从同一个图像中可能不相似，而两个不同图像的扩充视图可能更相似。因此，强制正样对和负样对的相似度高低可能并不总是可 achievable，而且在某些对之中，强制如此可能会损害性能。为解决这个问题，我们提议使用多个投影头，每个生成一个独立的特征集。我们的损失函数在预训练阶段由每个头wise posterior distribution of positive samples given observations的最大可能性解决出来。损失函数包括对正样对和负样对的相似度测量，每个重新权重通过个体适应温度评正化，以避免不良解决。我们称之为自适应多头对比学习（AMCL）。我们的AMCL可以应用到多种流行的对比学习方法，如SimCLR、MoCo和Barlow Twins，并在不同的后端和线性探针级别上实现了实验增进。这种改进是不同的扩充方法和误差率下的可重复的，并且在多个扩充方法使用时更加明显。
</details></li>
</ul>
<hr>
<h2 id="InterroLang-Exploring-NLP-Models-and-Datasets-through-Dialogue-based-Explanations"><a href="#InterroLang-Exploring-NLP-Models-and-Datasets-through-Dialogue-based-Explanations" class="headerlink" title="InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations"></a>InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05592">http://arxiv.org/abs/2310.05592</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dfki-nlp/interrolang">https://github.com/dfki-nlp/interrolang</a></li>
<li>paper_authors: Nils Feldhus, Qianli Wang, Tatiana Anikina, Sahil Chopra, Cennet Oguz, Sebastian Möller</li>
<li>for: 本研究旨在开发一个可交互的对话系统，帮助用户通过自然语言界面获得模型和数据集的解释。</li>
<li>methods: 本研究采用了对话扩展模型TalkToModel（Slack et al., 2022），并添加了新的NLP特有操作，如自由文本合理化。</li>
<li>results: 研究发现，对话性解释对用户来说是有用和有 corrections 的，可以帮助用户更好地理解模型的预测结果。此外，用户通过对话性解释可以更好地预测模型的结果，而不是基于单个解释。<details>
<summary>Abstract</summary>
While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.
</details>
<details>
<summary>摘要</summary>
Recently developed NLP explainability methods have allowed us to open the black box in various ways (Madsen et al., 2022), but a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, for example, via clarification or follow-up questions, and through a natural language interface. We adapted the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, added new NLP-specific operations such as free-text rationalization, and demonstrated its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluated fine-tuned and few-shot prompting models and implemented a novel Adapter-based approach. We then conducted two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e., how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found that rationalization and feature attribution were helpful in explaining the model behavior, and users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.
</details></li>
</ul>
<hr>
<h2 id="Aggregated-f-average-Neural-Network-for-Interpretable-Ensembling"><a href="#Aggregated-f-average-Neural-Network-for-Interpretable-Ensembling" class="headerlink" title="Aggregated f-average Neural Network for Interpretable Ensembling"></a>Aggregated f-average Neural Network for Interpretable Ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05566">http://arxiv.org/abs/2310.05566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathieu Vu, Emilie Chouzenoux, Jean-Christophe Pesquet, Ismail Ben Ayed</li>
<li>for: 这篇论文主要用于解决几个机器学习模型（即弱学习器）在共同任务上进行提高预测性能的问题。</li>
<li>methods: 这篇论文使用了基本的拼接法和更复杂的树状拼接法，并引入了一个新的卷积神经网络来实现最佳的吞吐量拼接。</li>
<li>results: 该论文通过使用不同类型的均值来优化弱学习器预测结果，并通过使用可解释的架构和简单的训练策略，实现了在少量示例增强学习问题上的好表现。<details>
<summary>Abstract</summary>
Ensemble learning leverages multiple models (i.e., weak learners) on a common machine learning task to enhance prediction performance. Basic ensembling approaches average the weak learners outputs, while more sophisticated ones stack a machine learning model in between the weak learners outputs and the final prediction. This work fuses both aforementioned frameworks. We introduce an aggregated f-average (AFA) shallow neural network which models and combines different types of averages to perform an optimal aggregation of the weak learners predictions. We emphasise its interpretable architecture and simple training strategy, and illustrate its good performance on the problem of few-shot class incremental learning.
</details>
<details>
<summary>摘要</summary>
ensemble learning可以利用多个模型（即弱学习器）来增强预测性能， Basic的ensembleapproaches是将弱学习器输出平均化，而更复杂的ones是在弱学习器输出和最终预测之间堆叠一个机器学习模型。这个工作 fusion这两种框架。我们介绍了一个集成了不同类型的平均值的简单神经网络，即集成f-平均（AFA）神经网络。我们强调其可解释的architecture和简单的训练策略，并通过几何学问题中的少量逻辑学习问题 illustrate its good performance。Here's the breakdown of the translation:* "ensemble learning" is translated as "集成学习" (zhòngshì xuéxí)* "leverages" is translated as "利用" (lìyòu)* "multiple models" is translated as "多个模型" (duō ge móde)* "weak learners" is translated as "弱学习器" (ruò xuéxí qì)* "basic ensembling approaches" is translated as "基本的ensembleapproaches" (jīběn de ensembleapproaches)* "stack a machine learning model" is translated as "堆叠一个机器学习模型" (zhumu yī ge jīshì xuéxí móde)* "this work" is translated as "这个工作" (zhè ge gōngzuò)* "fuses" is translated as "集成" (jìshèng)* "both frameworks" is translated as "这两种框架" (zhè liàng zhī kāngjī)* "introduce" is translated as "介绍" (jièkuài)* "an aggregated f-average (AFA) shallow neural network" is translated as "一个集成f-平均（AFA）神经网络" (yī ge jìshèng f-píngyān (AFA) xīnnéng wǎngluò)* "interpretable architecture" is translated as "可解释的architecture" (kějiěshì de architecture)* "simple training strategy" is translated as "简单的训练策略" (jìndān de xùnxíng zhìxí)* "illustrate" is translated as "illustrate" (xìhuì)* "good performance" is translated as "好的性能" (hǎo de xìngnéng)Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I'll be happy to provide it.
</details></li>
</ul>
<hr>
<h2 id="STREAM-Social-data-and-knowledge-collective-intelligence-platform-for-TRaining-Ethical-AI-Models"><a href="#STREAM-Social-data-and-knowledge-collective-intelligence-platform-for-TRaining-Ethical-AI-Models" class="headerlink" title="STREAM: Social data and knowledge collective intelligence platform for TRaining Ethical AI Models"></a>STREAM: Social data and knowledge collective intelligence platform for TRaining Ethical AI Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05563">http://arxiv.org/abs/2310.05563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwei Wang, Enmeng Lu, Zizhe Ruan, Yao Liang, Yi Zeng</li>
<li>for: 本文提出了一个社交数据和知识共同智能平台，用于训练伦理AI模型（STREAM），以解决人工智能模型与人类伦理价值观念的匹配问题，并提供伦理数据集和知识库，以帮助AI模型“遵循好 advise的自然流动”。</li>
<li>methods: 本文使用了创建全面和代表性的平台，准确反映人类和AI的伦理判断，捕捉人类和AI的群体和文化差异，以及时间的演变，以便实现6Es（Establishment、Evaluation、Embedding、Embodiment、Ensemble、Evolvement）中的伦理能力。</li>
<li>results: STREAM已经提供了丰富的伦理enario集，并收集了大量由志愿者和各种流行的大语言模型（LLMs）注释的伦理判断数据，共同反映人类和AI在不同伦理上的偏好和表现。<details>
<summary>Abstract</summary>
This paper presents Social data and knowledge collective intelligence platform for TRaining Ethical AI Models (STREAM) to address the challenge of aligning AI models with human moral values, and to provide ethics datasets and knowledge bases to help promote AI models "follow good advice as naturally as a stream follows its course". By creating a comprehensive and representative platform that accurately mirrors the moral judgments of diverse groups including humans and AIs, we hope to effectively portray cultural and group variations, and capture the dynamic evolution of moral judgments over time, which in turn will facilitate the Establishment, Evaluation, Embedding, Embodiment, Ensemble, and Evolvement (6Es) of the moral capabilities of AI models. Currently, STREAM has already furnished a comprehensive collection of ethical scenarios, and amassed substantial moral judgment data annotated by volunteers and various popular Large Language Models (LLMs), collectively portraying the moral preferences and performances of both humans and AIs across a range of moral contexts. This paper will outline the current structure and construction of STREAM, explore its potential applications, and discuss its future prospects.
</details>
<details>
<summary>摘要</summary>
Currently, STREAM has furnished a comprehensive collection of ethical scenarios and amassed substantial moral judgment data annotated by volunteers and various popular Large Language Models (LLMs), collectively portraying the moral preferences and performances of both humans and AIs across a range of moral contexts. This paper will outline the current structure and construction of STREAM, explore its potential applications, and discuss its future prospects.
</details></li>
</ul>
<hr>
<h2 id="WeatherDepth-Curriculum-Contrastive-Learning-for-Self-Supervised-Depth-Estimation-under-Adverse-Weather-Conditions"><a href="#WeatherDepth-Curriculum-Contrastive-Learning-for-Self-Supervised-Depth-Estimation-under-Adverse-Weather-Conditions" class="headerlink" title="WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions"></a>WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05556">http://arxiv.org/abs/2310.05556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiyuan Wang, Chunyu Lin, Lang Nie, Shujun Huang, Yao Zhao, Xing Pan, Rui Ai</li>
<li>for: 提高雨天环境下深度估计模型的性能，适应不同天气条件下的摄像头捕捉。</li>
<li>methods: 提出了一种自动进行课程分配和学习的自监学习策略，通过不同的课程来逐渐适应不同的天气条件，并且通过对不同课程之间的深度一致性进行约束，以提高模型的鲁棒性。</li>
<li>results: 在实验中，提出的解决方案可以轻松地与不同的模型结合使用，并在人工挑战和实际雨天捕捉数据集上达到了当前最佳性能。<details>
<summary>Abstract</summary>
Depth estimation models have shown promising performance on clear scenes but fail to generalize to adverse weather conditions due to illumination variations, weather particles, etc. In this paper, we propose WeatherDepth, a self-supervised robust depth estimation model with curriculum contrastive learning, to tackle performance degradation in complex weather conditions. Concretely, we first present a progressive curriculum learning scheme with three simple-to-complex curricula to gradually adapt the model from clear to relative adverse, and then to adverse weather scenes. It encourages the model to gradually grasp beneficial depth cues against the weather effect, yielding smoother and better domain adaption. Meanwhile, to prevent the model from forgetting previous curricula, we integrate contrastive learning into different curricula. Drawn the reference knowledge from the previous course, our strategy establishes a depth consistency constraint between different courses towards robust depth estimation in diverse weather. Besides, to reduce manual intervention and better adapt to different models, we designed an adaptive curriculum scheduler to automatically search for the best timing for course switching. In the experiment, the proposed solution is proven to be easily incorporated into various architectures and demonstrates state-of-the-art (SoTA) performance on both synthetic and real weather datasets.
</details>
<details>
<summary>摘要</summary>
depth estimation模型在清晰场景下表现出色，但在不利天气条件下表现不佳，主要是因为照明变化、天气粒子等因素。在这篇论文中，我们提出了一种自动适应的深度估计模型——WeatherDepth，使得模型在复杂的天气条件下能够更好地适应。具体来说，我们首先提出了一种进步式课程学习方案，包括三个简单到复杂的课程，以逐步适应模型从清晰到相对不利、然后到不利天气场景。这使得模型逐渐捕捉到恰当的深度提示，从而获得更好的预测性。同时，为了避免模型忘记之前的课程，我们将对不同的课程进行了对比学习。从此，我们的策略建立了一个深度一致性约束，以保证模型在多种天气条件下的稳定性。此外，为了避免手动 intervención和更好地适应不同的模型，我们设计了一个自适应课程调度器，以自动搜索最佳课程时间点。在实验中，我们的解决方案轻松地适应到不同的架构，并在真实的天气数据集上达到了当前最佳性能（SoTA）。
</details></li>
</ul>
<hr>
<h2 id="Logic-guided-Deep-Reinforcement-Learning-for-Stock-Trading"><a href="#Logic-guided-Deep-Reinforcement-Learning-for-Stock-Trading" class="headerlink" title="Logic-guided Deep Reinforcement Learning for Stock Trading"></a>Logic-guided Deep Reinforcement Learning for Stock Trading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05551">http://arxiv.org/abs/2310.05551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiming Li, Junzhe Jiang, Yushi Cao, Aixin Cui, Bozhi Wu, Bo Li, Yang Liu</li>
<li>for: 这篇论文的目的是提出一种逻辑导航的交易框架，以提高深度强化学习（DRL）在动态股票市场中的稳定性和性能。</li>
<li>methods: 该论文提出了一种新的逻辑导航框架，称为SYENS（程序合成基于集成策略），它通过在层次结构中使用程序合成来规范模型的行为，并具有更高的稳定性和性能。</li>
<li>results: 根据实验结果，SYENS在30个道琴股票的股票交易中具有更高的累积收益和较低的最大投降，并在两种交易设置下（即现金交易和质押交易）都能够显著超越基elines。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) has revolutionized quantitative finance by achieving excellent performance without significant manual effort. Whereas we observe that the DRL models behave unstably in a dynamic stock market due to the low signal-to-noise ratio nature of the financial data. In this paper, we propose a novel logic-guided trading framework, termed as SYENS (Program Synthesis-based Ensemble Strategy). Different from the previous state-of-the-art ensemble reinforcement learning strategy which arbitrarily selects the best-performing agent for testing based on a single measurement, our framework proposes regularizing the model's behavior in a hierarchical manner using the program synthesis by sketching paradigm. First, we propose a high-level, domain-specific language (DSL) that is used for the depiction of the market environment and action. Then based on the DSL, a novel program sketch is introduced, which embeds human expert knowledge in a logical manner. Finally, based on the program sketch, we adopt the program synthesis by sketching a paradigm and synthesizing a logical, hierarchical trading strategy. We evaluate SYENS on the 30 Dow Jones stocks under the cash trading and the margin trading settings. Experimental results demonstrate that our proposed framework can significantly outperform the baselines with much higher cumulative return and lower maximum drawdown under both settings.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）已经革命化金融科学，它可以在不需要显著的人工努力的情况下达到出色的性能。然而，我们观察到DRL模型在动态股票市场中的不稳定行为，这是因为财务数据的信号噪声比例较低。在这篇论文中，我们提出了一种新的逻辑引导交易框架，称为SYENS（程序合成基于ensemble策略）。与前一代状态的聚合强化学习策略不同，我们的框架在层次结构上使用程序合成来规范模型的行为。首先，我们提出了一种高级、领域特定语言（DSL），用于描述市场环境和行动。然后，我们基于DSL引入了一种新的程序绘制，其嵌入了人类专家知识在逻辑上。最后，我们采用程序合成 by sketching 方法，并将其应用于SYENS框架中。我们在30个道琴股票下对cash交易和margin交易进行了实验。实验结果表明，我们的提出的框架可以与基准值相比较高的净返报和较低的最大下降。
</details></li>
</ul>
<hr>
<h2 id="ParFam-–-Symbolic-Regression-Based-on-Continuous-Global-Optimization"><a href="#ParFam-–-Symbolic-Regression-Based-on-Continuous-Global-Optimization" class="headerlink" title="ParFam – Symbolic Regression Based on Continuous Global Optimization"></a>ParFam – Symbolic Regression Based on Continuous Global Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05537">http://arxiv.org/abs/2310.05537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/philipp238/parfam">https://github.com/philipp238/parfam</a></li>
<li>paper_authors: Philipp Scholl, Katharina Bieker, Hillary Hauger, Gitta Kutyniok</li>
<li>for: 解决Symbolic Regression（SR）问题，包括从数据中找到物理法律或财务市场行为的数学方程。</li>
<li>methods: 使用参数家族的符号函数来将精确的SR问题转化为连续问题，然后与强大的全球优化器结合使用，实现SR问题的解决。</li>
<li>results: 通过广泛的数字实验，证明ParFam可以 дости到SR问题的状态Esp中的最佳解决方案，并可以轻松扩展到更高级的算法，例如添加深度神经网络来找到适合的参数家族。<details>
<summary>Abstract</summary>
The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually quite complicated and require a lot of hyperparameter tuning and computational resources. In this paper, we present our new method ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a powerful global optimizer, this approach results in an effective method to tackle the problem of SR. Furthermore, it can be easily extended to more advanced algorithms, e.g., by adding a deep neural network to find good-fitting parametric families. We prove the performance of ParFam with extensive numerical experiments based on the common SR benchmark suit SRBench, showing that we achieve state-of-the-art results. Our code and results can be found at https://github.com/Philipp238/parfam .
</details>
<details>
<summary>摘要</summary>
SR（符号回归）问题在多种应用中出现，如从数据中找到物理法律或财务市场行为的数学方程。现有多种解决 SR 问题的方法，通常基于进化编程。然而，这些方法通常很复杂，需要许多Hyperparameter调整和计算资源。在这篇论文中，我们介绍了我们的新方法 ParFam，它利用适当的参数家族符号函数来将离散的符号回归问题转化为连续的问题，从而得到更直观的设置。与现有状态之册方法相比，我们的方法更加简单，并且可以轻松地扩展到更高级的算法，例如通过添加深度神经网络来找到good-fitting参数家族。我们通过对 SRBench 常用的 SR benchmark 进行广泛的数字实验，证明了 ParFam 的性能。我们的代码和结果可以在 GitHub 上找到：https://github.com/Philipp238/parfam。
</details></li>
</ul>
<hr>
<h2 id="On-Double-Descent-in-Reinforcement-Learning-with-LSTD-and-Random-Features"><a href="#On-Double-Descent-in-Reinforcement-Learning-with-LSTD-and-Random-Features" class="headerlink" title="On Double Descent in Reinforcement Learning with LSTD and Random Features"></a>On Double Descent in Reinforcement Learning with LSTD and Random Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05518">http://arxiv.org/abs/2310.05518</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Brellmann, Eloïse Berthier, David Filliat, Goran Frehse</li>
<li>for: 本研究探讨了深度强化学习（RL）中 temporal difference（TD）算法的性能如何受到神经网络大小的影响。</li>
<li>methods: 本研究使用了理论分析来研究神经网络大小和 $l_2$-正则化对性能的影响。研究人员还使用了Random Features和懒散训练策略来研究正则化最小二乘差分算法在无穷大参数和状态数下的性能。</li>
<li>results: 研究人员发现了一种双峰现象，即在神经网络参数和状态数比例大于1时，性能会快速下降。他们还发现，在增加 $l_2$-正则化或状态数下降到0时， corrction terms消失。numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.<details>
<summary>Abstract</summary>
Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive deterministic limits of both the empirical and the true Mean-Square Bellman Error (MSBE) that feature correction terms responsible for the double-descent. Correction terms vanish when the $l_2$-regularization is increased or the number of unvisited states goes to zero. Numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.
</details>
<details>
<summary>摘要</summary>
temporal difference（TD）算法在深度学习（RL）中广泛使用。其性能受到神经网络大小的影响。在超vised学习中，过度参数的情况和其好处已经很好地理解，但在RL中情况却相对不清楚。在这篇论文中，我们提供了TD算法性能与神经网络大小的理论分析。我们确定了参数与访问状态的比率为关键因素，并定义了过度参数为神经网络参数的数量大于状态数量的情况。此外，我们发现了一种双峰现象，即参数/状态比率接近1时性能突然下降。通过随机特征和懒散训练策略，我们研究了正则化最小二乘差（LSTD）算法在参数和状态数量 infinito 的极限情况下。我们 derive了参数和状态数量 infinito 下的零限的实际和真实的Mean-Square Bellman Error（MSBE），其中包含修正项负责 Double Descent。这些修正项在 $l_2$ 正则化强度增大或未访问状态数量减少时消失。实际实验结果与理论预测匹配得非常好。
</details></li>
</ul>
<hr>
<h2 id="UAVs-and-Neural-Networks-for-search-and-rescue-missions"><a href="#UAVs-and-Neural-Networks-for-search-and-rescue-missions" class="headerlink" title="UAVs and Neural Networks for search and rescue missions"></a>UAVs and Neural Networks for search and rescue missions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05512">http://arxiv.org/abs/2310.05512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hartmut Surmann, Artur Leinweber, Gerhard Senkowski, Julien Meine, Dominik Slomma</li>
<li>for:  detection of objects of interest (cars, humans, fire) in aerial images captured by UAVs during vegetation fires</li>
<li>methods: use of artificial neural networks, creation of a dataset for supervised learning, implementation of an object detection pipeline combining classic image processing techniques with pretrained neural networks, development of a data augmentation pipeline to augment the dataset with automatically labeled images</li>
<li>results: evaluation of the performance of different neural networksHere’s the information in Simplified Chinese:</li>
<li>for: 检测 aerial 图像中的目标对象（车辆、人员、火灾），通常由无人飞行器（UAV）拍摄</li>
<li>methods: 使用人工神经网络，创建一个超级vised 学习的数据集，实现一个对象检测管道，将经典的图像处理技术与预训练的神经网络结合使用，开发一个自动标注图像数据集的数据增强管道</li>
<li>results: 评估不同神经网络的性能<details>
<summary>Abstract</summary>
In this paper, we present a method for detecting objects of interest, including cars, humans, and fire, in aerial images captured by unmanned aerial vehicles (UAVs) usually during vegetation fires. To achieve this, we use artificial neural networks and create a dataset for supervised learning. We accomplish the assisted labeling of the dataset through the implementation of an object detection pipeline that combines classic image processing techniques with pretrained neural networks. In addition, we develop a data augmentation pipeline to augment the dataset with automatically labeled images. Finally, we evaluate the performance of different neural networks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法用于在由无人飞行器（UAV）拍摄的空中图像中检测有关兴趣的对象，包括汽车、人体和火灾。为 дости这一目标，我们使用人工神经网络，并创建了一个用于超级vised学习的数据集。我们通过实施对象检测管道，该管道组合了经典的图像处理技术和预训练的神经网络，来协助标注数据集。此外，我们还开发了一个自动生成数据集的管道，以增加数据集的自动标注图像。最后，我们评估了不同的神经网络的性能。
</details></li>
</ul>
<hr>
<h2 id="Query-and-Response-Augmentation-Cannot-Help-Out-of-domain-Math-Reasoning-Generalization"><a href="#Query-and-Response-Augmentation-Cannot-Help-Out-of-domain-Math-Reasoning-Generalization" class="headerlink" title="Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization"></a>Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05506">http://arxiv.org/abs/2310.05506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ofa-sys/gsm8k-screl">https://github.com/ofa-sys/gsm8k-screl</a></li>
<li>paper_authors: Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, Chang Zhou</li>
<li>for: 这个论文主要是为了研究在数学逻辑中使用大语言模型（LLMs）时，数据增强的效果，以及增强数据的量和多样性对模型性能的影响。</li>
<li>methods: 作者使用了一种新的数据集——AugGSM8K，通过复杂和多样化 queries 来增强数据，并通过 fine-tuning 来训练 LLMs。</li>
<li>results: 作者发现，通过增强数据，可以提高 LLMs 的数学逻辑性能，并且存在对数据量的呈几何关系。然而，在各种数学逻辑任务之间的泛化性能仍然需要进一步改进。<details>
<summary>Abstract</summary>
In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scale of 13B). A log-linear relationship is presented between MuggleMath's performance and the amount of augmented data. We also find that MuggleMath is weak in out-of-domain math reasoning generalization to MATH. This is attributed to the differences in query distribution between AugGSM8K and MATH which suggest that augmentation on a single benchmark could not help with overall math reasoning performance. Codes and AugGSM8K will be uploaded to https://github.com/OFA-Sys/gsm8k-ScRel.
</details>
<details>
<summary>摘要</summary>
在数学逻辑中使用大型自然语言模型（LLMs）， fine-tuning数据增强和多种逻辑路径的数据增强被证明是有效的，可以减小开源LLMs和高级专有LLMs之间的差距。在这篇论文中，我们进行了数学逻辑中数据增强的调查，旨在回答以下问题：（1）哪些数据增强策略更加有效；（2）数据增强量和模型性能之间存在哪种整数关系；和（3）数据增强是否能够适应尺度外的数学逻辑任务？为此，我们创建了一个新的数据集，AugGSM8K，通过复杂和多样化 queries from GSM8K 来生成多种 reasoning paths。我们使用这些数据集进行了一系列的 LLMS 的 fine-tuning，并取得了一系列的 MuggleMath 模型。MuggleMath 在 GSM8K 上实现了新的状态机器人，从 54% 提高到 68.4% （7B 缩放）和从 63.9% 提高到 74.0% （13B 缩放）。我们发现了数据增强和模型性能之间存在很好的对数关系。此外，我们发现 MuggleMath 在尺度外的数学逻辑任务上的总体性能较弱，这是因为 AugGSM8K 和 MATH 的查询分布之间存在差异。这意味着数据增强在单一的 benchmark 上不能够提高总体数学逻辑性能。代码和 AugGSM8K 将在 <https://github.com/OFA-Sys/gsm8k-ScRel> 上上传。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Graphs-with-Large-Language-Models-Methods-and-Prospects"><a href="#Integrating-Graphs-with-Large-Language-Models-Methods-and-Prospects" class="headerlink" title="Integrating Graphs with Large Language Models: Methods and Prospects"></a>Integrating Graphs with Large Language Models: Methods and Prospects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05499">http://arxiv.org/abs/2310.05499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shirui Pan, Yizhen Zheng, Yixin Liu</li>
<li>for: 本研究旨在探讨大语言模型（LLM）与图structured data的 интеграción，以提高LLM的性能和应用范围。</li>
<li>methods: 研究分为两类：首先，使用LLM进行图学习，以提高图任务的预测性能；其次，通过图结构来加强LLM的性能，例如在复杂任务中进行合作或理解。</li>
<li>results: 研究表明，通过图结构和LLM的结合，可以提高LLM的性能和应用范围，并且提出了未来研究的开放问题。<details>
<summary>Abstract</summary>
Large language models (LLMs) such as GPT-4 have emerged as frontrunners, showcasing unparalleled prowess in diverse applications, including answering queries, code generation, and more. Parallelly, graph-structured data, an intrinsic data type, is pervasive in real-world scenarios. Merging the capabilities of LLMs with graph-structured data has been a topic of keen interest. This paper bifurcates such integrations into two predominant categories. The first leverages LLMs for graph learning, where LLMs can not only augment existing graph algorithms but also stand as prediction models for various graph tasks. Conversely, the second category underscores the pivotal role of graphs in advancing LLMs. Mirroring human cognition, we solve complex tasks by adopting graphs in either reasoning or collaboration. Integrating with such structures can significantly boost the performance of LLMs in various complicated tasks. We also discuss and propose open questions for integrating LLMs with graph-structured data for the future direction of the field.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT-4在多种应用中表现出不一样的优势，包括回答问题、代码生成等。同时，图струк成数据是实际世界中普遍存在的数据类型。把LMLM的能力与图结构数据结合，已成为研究者的焦点之一。这篇评论文将这些结合分为两大类。第一种将LMLM用于图学习，LMLM可以不仅增强现有的图算法，还可以作为各种图任务的预测模型。相反，第二种类型强调图结构数据在提高LMLM表现的重要性。人类的思维方式是透过图来解释和协作来解决复杂任务。与图结构数据结合可以将LMLM在多种复杂任务中表现出较好的成绩。我们还讨论了未来领域的开启问题，以推动LMLM与图结构数据的结合。
</details></li>
</ul>
<hr>
<h2 id="How-Abilities-in-Large-Language-Models-are-Affected-by-Supervised-Fine-tuning-Data-Composition"><a href="#How-Abilities-in-Large-Language-Models-are-Affected-by-Supervised-Fine-tuning-Data-Composition" class="headerlink" title="How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"></a>How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05492">http://arxiv.org/abs/2310.05492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou<br>for: 这个研究的目的是调查多种能力的LLMs在Supervised Fine-tuning（SFT）中的可调性，以及不同能力之间的数据组合对性能的影响。methods: 这个研究使用了多种SFT策略，包括顺序学习多能力（sequential learning）和 dual-stage mixed fine-tuning（DMT）策略，以及不同数据量和数据组合比例的调整。results: 研究发现，不同的能力展现出不同的扩展特征，大型模型通常需要更多的数据来提高性能。数学逻辑和代码生成能力随着数据量的增加而提高，而通用能力需要约一千个样本才能提高，然后slowly improves。数据组合对不同的能力有启发作用，但高数据量时会导致能力冲突。DMT策略可以避免卷积学习导致忘记现象，提供了多能力学习的可能解决方案。<details>
<summary>Abstract</summary>
Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code generation improve as data amounts increase consistently, while the general ability is enhanced with about a thousand samples and improves slowly. We find data composition results in various abilities improvements with low data amounts, while conflicts of abilities with high data amounts. Our experiments further show that composition data amount impacts performance, while the influence of composition ratio is insignificant. Regarding the SFT strategies, we evaluate sequential learning multiple abilities are prone to catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy learns specialized abilities first and then learns general abilities with a small amount of specialized data to prevent forgetting, offering a promising solution to learn multiple abilities with different scaling patterns.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）有巨大的预训语料和参数数量，并且拥有多种能力，包括数学推理、代码生成和指令跟进。这些能力可以通过监督精致训练（SFT）进一步增强。开源社区已经研究了随机SFT的每个能力，而商业LLM则具有多种能力。我们需要研究如何通过SFT解锁多种能力。在本研究中，我们专注于SFT中数学推理、代码生成和通用人类调整的数据结构之间的关系。从扩展角度来看，我们调查模型能力和不同因素（包括数据量、数据结构比例、模型参数和SFT策略）之间的关系。我们的实验显示不同的能力展现出不同的扩展模式，大型模型通常在同量数据下表现出色。数学推理和代码生成随着数据量增加逐渐提高，而通用能力则在约一千个数据 sample 后逐渐提高。我们发现数据结构可以在低数据量下提高不同的能力，但高数据量时会出现能力冲突。我们的实验还显示了数据结构填充量影响表现，但结构比例无法影响表现。关于SFT策略，我们评估了预先学习特定能力后，将特定数据进行混合精致训练（DMT），以避免忘记，提供了多能力学习的有前途的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Cabbage-Sweeter-than-Cake-Analysing-the-Potential-of-Large-Language-Models-for-Learning-Conceptual-Spaces"><a href="#Cabbage-Sweeter-than-Cake-Analysing-the-Potential-of-Large-Language-Models-for-Learning-Conceptual-Spaces" class="headerlink" title="Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces"></a>Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05481">http://arxiv.org/abs/2310.05481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Usashi Chatterjee, Amit Gajbhiye, Steven Schockaert</li>
<li>for: 这个论文旨在探讨使用大型自然语言模型（LLM）学习概念空间的可能性。</li>
<li>methods: 该论文使用了一种基于语言模型的方法，通过学习人类判断来构建概念空间。</li>
<li>results: 实验表明，LLM可以学习一定程度的概念空间表示，但是特定的BERT模型在训练后可以与最大的GPT-3模型匹配或超越它，即使它们的大小只是GPT-3的2-3个数量级。<details>
<summary>Abstract</summary>
The theory of Conceptual Spaces is an influential cognitive-linguistic framework for representing the meaning of concepts. Conceptual spaces are constructed from a set of quality dimensions, which essentially correspond to primitive perceptual features (e.g. hue or size). These quality dimensions are usually learned from human judgements, which means that applications of conceptual spaces tend to be limited to narrow domains (e.g. modelling colour or taste). Encouraged by recent findings about the ability of Large Language Models (LLMs) to learn perceptually grounded representations, we explore the potential of such models for learning conceptual spaces. Our experiments show that LLMs can indeed be used for learning meaningful representations to some extent. However, we also find that fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller.
</details>
<details>
<summary>摘要</summary>
理论的概念空间模型是一种有影响力的认知语言框架，用于表示概念的含义。概念空间由一系列质量维度组成，这些质量维度通常来自人类判断，这意味着应用概念空间通常受限于特定领域（如色彩或味道模elling）。鼓动了最近发现大语言模型（LLMs）可以学习基于感知的表示，我们explore了这些模型是否可以学习有意义的概念空间。我们的实验表明LLMs可以学习有意义的表示，但我们还发现，经过精度调整的BERT家族模型可以与最大GPT-3模型匹配或者超越，即使其体积只是GPT-3模型的2-3个数量级。
</details></li>
</ul>
<hr>
<h2 id="Deep-Optimal-Timing-Strategies-for-Time-Series"><a href="#Deep-Optimal-Timing-Strategies-for-Time-Series" class="headerlink" title="Deep Optimal Timing Strategies for Time Series"></a>Deep Optimal Timing Strategies for Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05479">http://arxiv.org/abs/2310.05479</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenpopper/optimal_timing_tsf">https://github.com/chenpopper/optimal_timing_tsf</a></li>
<li>paper_authors: Chen Pan, Fan Zhou, Xuanwei Hu, Xinxin Zhu, Wenxin Ning, Zi Zhuang, Siqiao Xue, James Zhang, Yunhua Hu</li>
<li>for: 这篇论文的目的是解决很多企业活动中的时间执行计划问题，即在时间序列预测中做出最佳的执行时间选择。</li>
<li>methods: 该论文提出了一种机制，即将时间序列预测任务和优化执行时间决策任务结合起来，以提供一个具有坚实理论基础和实际应用灵活性的解决方案。特别是，它通过使用概率时间序列预测算法，不需要 сложные数学动力模型，从而避免了很多其他常见实践中的假设强大优化知识。</li>
<li>results: 该论文通过使用核心束回归神经网络（RNN）来近似优化时间执行时间，实现了在实际应用中减少操作成本的目标。详细的实现细节可以参考github上的\url{github.com&#x2F;ChenPopper&#x2F;optimal_timing_TSF}仓库。<details>
<summary>Abstract</summary>
Deciding the best future execution time is a critical task in many business activities while evolving time series forecasting, and optimal timing strategy provides such a solution, which is driven by observed data. This solution has plenty of valuable applications to reduce the operation costs. In this paper, we propose a mechanism that combines a probabilistic time series forecasting task and an optimal timing decision task as a first systematic attempt to tackle these practical problems with both solid theoretical foundation and real-world flexibility. Specifically, it generates the future paths of the underlying time series via probabilistic forecasting algorithms, which does not need a sophisticated mathematical dynamic model relying on strong prior knowledge as most other common practices. In order to find the optimal execution time, we formulate the decision task as an optimal stopping problem, and employ a recurrent neural network structure (RNN) to approximate the optimal times. Github repository: \url{github.com/ChenPopper/optimal_timing_TSF}.
</details>
<details>
<summary>摘要</summary>
决定最佳未来执行时间是许多企业活动中的关键任务，而且随着时间序列预测的演化，最佳时间策略提供了一个解决方案，它是由观察数据驱动的。这种解决方案有很多有价值的应用，可以降低运营成本。在这篇论文中，我们提出了一种机制，它将混合 probabilistic 时间序列预测任务和最佳时间决策任务，作为第一个系统性的尝试，以解决这些实际问题。Specifically, it generates the future paths of the underlying time series via probabilistic forecasting algorithms, which does not need a sophisticated mathematical dynamic model relying on strong prior knowledge as most other common practices. In order to find the optimal execution time, we formulate the decision task as an optimal stopping problem, and employ a recurrent neural network structure (RNN) to approximate the optimal times. Github repository: \url{github.com/ChenPopper/optimal_timing_TSF}.Here's the word-for-word translation of the text into Simplified Chinese:决定最佳未来执行时间是许多企业活动中的关键任务，而且随着时间序列预测的演化，最佳时间策略提供了一个解决方案，它是由观察数据驱动的。这种解决方案有很多有价值的应用，可以降低运营成本。在这篇论文中，我们提出了一种机制，它将混合 probabilistic 时间序列预测任务和最佳时间决策任务，作为第一个系统性的尝试，以解决这些实际问题。Specifically, it generates the future paths of the underlying time series via probabilistic forecasting algorithms, which does not need a sophisticated mathematical dynamic model relying on strong prior knowledge as most other common practices. In order to find the optimal execution time, we formulate the decision task as an optimal stopping problem, and employ a recurrent neural network structure (RNN) to approximate the optimal times. Github repository: \url{github.com/ChenPopper/optimal_timing_TSF}.
</details></li>
</ul>
<hr>
<h2 id="Sentence-level-Prompts-Benefit-Composed-Image-Retrieval"><a href="#Sentence-level-Prompts-Benefit-Composed-Image-Retrieval" class="headerlink" title="Sentence-level Prompts Benefit Composed Image Retrieval"></a>Sentence-level Prompts Benefit Composed Image Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05473">http://arxiv.org/abs/2310.05473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chunmeifeng/sprc">https://github.com/chunmeifeng/sprc</a></li>
<li>paper_authors: Yang Bai, Xinxing Xu, Yong Liu, Salman Khan, Fahad Khan, Wangmeng Zuo, Rick Siow Mong Goh, Chun-Mei Feng</li>
<li>for: 提高组合图像检索的精度（Composed Image Retrieval）</li>
<li>methods: 使用预训练的V-L模型生成句子级提示，并使用image-text对比损失和文本提示对齐损失来学习适合的句子级提示。</li>
<li>results: 在Fashion-IQ和CIRR数据集上比革尤其良好，比起现有state-of-the-art方法。In Simplified Chinese text, the three information would be:</li>
<li>for: 提高组合图像检索的精度</li>
<li>methods: 使用预训练的V-L模型生成句子级提示，并使用image-text对比损失和文本提示对齐损失来学习适合的句子级提示。</li>
<li>results: 在Fashion-IQ和CIRR数据集上比革尤其良好，比起现有state-of-the-art方法。<details>
<summary>Abstract</summary>
Composed image retrieval (CIR) is the task of retrieving specific images by using a query that involves both a reference image and a relative caption. Most existing CIR models adopt the late-fusion strategy to combine visual and language features. Besides, several approaches have also been suggested to generate a pseudo-word token from the reference image, which is further integrated into the relative caption for CIR. However, these pseudo-word-based prompting methods have limitations when target image encompasses complex changes on reference image, e.g., object removal and attribute modification. In this work, we demonstrate that learning an appropriate sentence-level prompt for the relative caption (SPRC) is sufficient for achieving effective composed image retrieval. Instead of relying on pseudo-word-based prompts, we propose to leverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level prompts. By concatenating the learned sentence-level prompt with the relative caption, one can readily use existing text-based image retrieval models to enhance CIR performance. Furthermore, we introduce both image-text contrastive loss and text prompt alignment loss to enforce the learning of suitable sentence-level prompts. Experiments show that our proposed method performs favorably against the state-of-the-art CIR methods on the Fashion-IQ and CIRR datasets. The source code and pretrained model are publicly available at https://github.com/chunmeifeng/SPRC
</details>
<details>
<summary>摘要</summary>
“组合图像检索（CIR）任务是根据查询包含参考图像和相关描述文本来检索特定图像。现有大多数CIR模型采用较晚的融合策略将视觉和语言特征结合。此外，一些方法还建议生成基于参考图像的 pseudo-word 令，并将其与相关描述文本结合使用。然而，这些 pseudo-word 基于的提示方法在参考图像具有复杂变化时存在限制，例如对象移除和特征修改。在这种情况下，我们表明了学习适当的句子级提示（SPRC）是可以实现有效的组合图像检索的。而不是依赖 pseudo-word 基于的提示，我们提议利用预训练的 V-L 模型，如 BLIP-2，生成句子级提示。将学习的句子级提示与相关描述文本 concatenate 后，可以直接使用现有的文本基于图像检索模型进行改进 CIR 性能。此外，我们引入了图像文本对比loss和文本提示对齐loss，以便学习适当的句子级提示。实验结果表明，我们提出的方法在 Fashion-IQ 和 CIRR 数据集上与状态对照方法相比表现优异。源代码和预训练模型可以在 GitHub 上下载。”
</details></li>
</ul>
<hr>
<h2 id="Generative-Judge-for-Evaluating-Alignment"><a href="#Generative-Judge-for-Evaluating-Alignment" class="headerlink" title="Generative Judge for Evaluating Alignment"></a>Generative Judge for Evaluating Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05470">http://arxiv.org/abs/2310.05470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gair-nlp/auto-j">https://github.com/gair-nlp/auto-j</a></li>
<li>paper_authors: Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu</li>
<li>for: 本研究旨在提出一种生成式评审器（Auto-J），以应对大语言模型（LLM）在自然语言处理（NLP）领域的扩展。</li>
<li>methods: 我们提出了一种基于用户问题和LLM生成的回答的训练方法，并采用了质量权重学习和权重融合来提高模型的一致性和泛化性。</li>
<li>results: 实验结果显示，Auto-J在58个不同情景下的测试环境中具有显著的优势，并且与其他竞争对手（包括开源和关闭源模型）形成明显的差异。<details>
<summary>Abstract</summary>
The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding generality (i.e., assessing performance across diverse scenarios), flexibility (i.e., examining under different protocols), and interpretability (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, Auto-J, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, Auto-J outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.
</details>
<details>
<summary>摘要</summary>
随着大型语言模型（LLM）的快速发展，它们可以 addresses 的任务范围得到了极大的扩展。在自然语言处理（NLP）领域，研究人员的焦点从传统的 NLP 任务（如序列标记和分析）转移到了与人类需求相关的任务（如审想和电子邮件写作）。这种任务分布的变化对评估这些对齐的模型进行评估有新的要求，包括总体性（即在多种场景中的表现评估）、灵活性（即在不同的协议下进行评估）以及可读性（即使用自然语言的解释来评估模型）。在这篇论文中，我们提出了一个名为 Auto-J 的生成式评价器，拥有 13B 参数。我们的模型在用户查询和 LLM 生成的回答下进行训练，并且可以处理多种评估协议（如对比回答和单独评估），并且具有良好的自然语言批评结构。为了证明我们的方法的效果，我们构建了一个包含 58 个不同场景的测试床。实验结果表明，Auto-J 在与多种强大竞争对手进行比较时，有大幅度的优势。我们还提供了详细的分析和案例研究，以及在 GitHub 上公开多种资源。
</details></li>
</ul>
<hr>
<h2 id="Cost-Sensitive-Best-Subset-Selection-for-Logistic-Regression-A-Mixed-Integer-Conic-Optimization-Perspective"><a href="#Cost-Sensitive-Best-Subset-Selection-for-Logistic-Regression-A-Mixed-Integer-Conic-Optimization-Perspective" class="headerlink" title="Cost-Sensitive Best Subset Selection for Logistic Regression: A Mixed-Integer Conic Optimization Perspective"></a>Cost-Sensitive Best Subset Selection for Logistic Regression: A Mixed-Integer Conic Optimization Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05464">http://arxiv.org/abs/2310.05464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Knauer, Erik Rodner</li>
<li>for: 这个研究是为了设计可解释的机器学习模型，以简化其预测的输入，尤其在医疗领域。</li>
<li>methods: 这篇研究使用了混合整数几何优化的观点，提出了一个可证实且最佳的对应选择程序，可以考虑附加成本来选择特征。</li>
<li>results: 研究结果显示了低数据情况下和标签噪音情况下方法的限制，并提供了实践建议和适当的数据设计。此外，研究也辟开了meta学研究的新领域。<details>
<summary>Abstract</summary>
A key challenge in machine learning is to design interpretable models that can reduce their inputs to the best subset for making transparent predictions, especially in the clinical domain. In this work, we propose a certifiably optimal feature selection procedure for logistic regression from a mixed-integer conic optimization perspective that can take an auxiliary cost to obtain features into account. Based on an extensive review of the literature, we carefully create a synthetic dataset generator for clinical prognostic model research. This allows us to systematically evaluate different heuristic and optimal cardinality- and budget-constrained feature selection procedures. The analysis shows key limitations of the methods for the low-data regime and when confronted with label noise. Our paper not only provides empirical recommendations for suitable methods and dataset designs, but also paves the way for future research in the area of meta-learning.
</details>
<details>
<summary>摘要</summary>
一大挑战在机器学习中是设计可解释的模型，以减少输入并提供透明的预测，尤其在医疗领域。在这项工作中，我们提议一种 certificately 优化的特征选择方法，通过杂谱矩阵优化的视角来考虑辅助成本。我们通过了评 literature 的广泛回顾，并且 méticulously 创建了临床预测模型的 sintética 数据生成器。这使得我们可以系统地评估不同的启发式和优化的卡达性和预算限制下的特征选择方法。分析表明了低数据情况下和标签噪声时方法的局限性。我们的论文不仅提供了实践建议，还开辟了meta-学习领域的未来研究之路。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-based-Hybrid-Optimization-of-Bayesian-Neural-Networks-and-Traditional-Machine-Learning-Algorithms"><a href="#Ensemble-based-Hybrid-Optimization-of-Bayesian-Neural-Networks-and-Traditional-Machine-Learning-Algorithms" class="headerlink" title="Ensemble-based Hybrid Optimization of Bayesian Neural Networks and Traditional Machine Learning Algorithms"></a>Ensemble-based Hybrid Optimization of Bayesian Neural Networks and Traditional Machine Learning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05456">http://arxiv.org/abs/2310.05456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiwen Tan</li>
<li>for: 这个研究旨在优化 bayesian neural networks (BNNs) 的方法，通过与传统机器学习算法如随机森林 (RF)、梯度提升 (GB) 和支持向量机 (SVM) 的综合Integration。</li>
<li>methods: 该研究使用 feature integration 将这些方法相互融合，并强调第二个条件的优化，包括站点性和正定定征矩阵。</li>
<li>results:  ensemble method 表现出了 Robust 和算法优化的特点，并且 hyperparameter tuning 对 Expected Improvement (EI) 的影响较弱。<details>
<summary>Abstract</summary>
This research introduces a novel methodology for optimizing Bayesian Neural Networks (BNNs) by synergistically integrating them with traditional machine learning algorithms such as Random Forests (RF), Gradient Boosting (GB), and Support Vector Machines (SVM). Feature integration solidifies these results by emphasizing the second-order conditions for optimality, including stationarity and positive definiteness of the Hessian matrix. Conversely, hyperparameter tuning indicates a subdued impact in improving Expected Improvement (EI), represented by EI(x). Overall, the ensemble method stands out as a robust, algorithmically optimized approach.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Bayesian Neural Networks" (BNNs) is translated as "泛函神经网络" (pànfungen xīnǎo wǎngluò)* "Random Forests" (RF) is translated as "随机森林" (suījì sēn líng)* "Gradient Boosting" (GB) is translated as "梯度提升" (dēngdì tímshēng)* "Support Vector Machines" (SVM) is translated as "支持向量机器" (zhīchēng xiàngwù jīqì)* "Feature integration" is translated as "特征集成" (fēngjī zhùchéng)* "Hyperparameter tuning" is translated as "超参数调整" (chāojianxìa dào zhèng)* "Expected Improvement" (EI) is translated as "预期改进" (yùxì gǎngyì)Note that the translation is in Simplified Chinese, which is the most commonly used form of Chinese in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Explaining-the-Complex-Task-Reasoning-of-Large-Language-Models-with-Template-Content-Structure"><a href="#Explaining-the-Complex-Task-Reasoning-of-Large-Language-Models-with-Template-Content-Structure" class="headerlink" title="Explaining the Complex Task Reasoning of Large Language Models with Template-Content Structure"></a>Explaining the Complex Task Reasoning of Large Language Models with Template-Content Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05452">http://arxiv.org/abs/2310.05452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotong Yang, Fanxu Meng, Zhouchen Lin, Muhan Zhang</li>
<li>for: This paper aims to provide an explanation for the exceptional generalization abilities of pre-trained large language models, and to offer a novel framework for understanding their ability to solve complex natural language tasks.</li>
<li>methods: The paper presents a hierarchical “template-content” structure for modeling answer generation in natural language tasks, and demonstrates that pre-trained models can automatically decompose tasks into constituent steps during autoregressive generation through language modeling on a sufficiently large corpus.</li>
<li>results: The paper shows that practical models exhibit different behaviors for “template” and “content” providing support for the proposed modeling, and offers an explanatory tool for the complex reasoning abilities of large language models from the perspective of modeling autoregressive generation tasks.<details>
<summary>Abstract</summary>
The continuous evolution of pre-trained large language models with ever-growing parameters and corpus sizes has augmented their capacity to solve complex tasks. This ability, which obviates the necessity for task-specific training or fine-tuning, relies on providing the model with a language description or some task exemplars -- referred to the prompt -- that guide the desired autoregressive generation. Despite the remarkable success, the underlying mechanisms that facilitate such exceptional generalization abilities remain an open question. In this paper, we present a novel framework that formally conceptualizes answer generation for complex natural language tasks as a hierarchical ``template-content'' structure. According to our modeling, there exist pre-trained models that can automatically decompose tasks into constituent steps during autoregressive generation, through language modeling on a sufficiently large corpus, thereby solving them. Our framework offers an explanatory tool for the complex reasoning abilities of large language models from the perspective of modeling autoregressive generation tasks. Our experiments show that practical models exhibit different behaviors for ``template'' and ``content'' providing support for our modeling.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型的不断演化和参数的增加以及训练数据的增加，使得这些模型可以更好地解决复杂任务。这种能力，不需要任务特定的训练或微调，通过给模型提供语言描述或一些任务示例（即提示）来引导潜在的自然语言生成。虽然这些成果很出色，但是这些成果的基础机制仍然是一个开放的问题。在这篇论文中，我们提出了一种新的框架，它正式地概括了复杂自然语言任务的回答生成为一个层次结构。根据我们的模型，存在一些预训练模型可以在生成过程中自动将任务 decomposes into constituent steps，通过对 sufficiently large corpus进行语言模型化，以解决任务。我们的框架提供了对大语言模型的复杂逻辑能力的解释工具，从概念生成任务的角度出发。我们的实验表明，实际模型在“模板”和“内容”提供支持，这支持我们的模型。
</details></li>
</ul>
<hr>
<h2 id="Replication-of-Multi-agent-Reinforcement-Learning-for-the-“Hide-and-Seek”-Problem"><a href="#Replication-of-Multi-agent-Reinforcement-Learning-for-the-“Hide-and-Seek”-Problem" class="headerlink" title="Replication of Multi-agent Reinforcement Learning for the “Hide and Seek” Problem"></a>Replication of Multi-agent Reinforcement Learning for the “Hide and Seek” Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05430">http://arxiv.org/abs/2310.05430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haider Kamal, Muaz A. Niazi, Hammad Afzal</li>
<li>for: 本研究的目的是提高隐藏者（Hider）的搜索策略，以增强其在复杂环境中的运动和搜索能力。</li>
<li>methods: 本研究使用了补偿学习（Reinforcement Learning），利用奖励函数和超参数来生成策略。</li>
<li>results: 研究发现，通过增加飞行机制，提高了隐藏者的机动性和搜索范围，从约2000万步到1600万步，提高了隐藏者的追踪策略。<details>
<summary>Abstract</summary>
Reinforcement learning generates policies based on reward functions and hyperparameters. Slight changes in these can significantly affect results. The lack of documentation and reproducibility in Reinforcement learning research makes it difficult to replicate once-deduced strategies. While previous research has identified strategies using grounded maneuvers, there is limited work in more complex environments. The agents in this study are simulated similarly to Open Al's hider and seek agents, in addition to a flying mechanism, enhancing their mobility, and expanding their range of possible actions and strategies. This added functionality improves the Hider agents to develop a chasing strategy from approximately 2 million steps to 1.6 million steps and hiders
</details>
<details>
<summary>摘要</summary>
利用强化学习生成策略，该策略基于奖励函数和超参数。小小的变化可能会导致 significativetransformations。 reinforcement learning研究的documentación和可重现性不足，使得复制已经获得的策略具有困难。在这种研究中，我们使用了在Open Al的隐藏者和搜索者中的 Agent，同时添加了飞行机制，从而提高了隐藏者的 mobilidad和可能的动作和策略。这种添加的功能使得隐藏者可以开发追踪策略，从约2000万步提高到1600万步。
</details></li>
</ul>
<hr>
<h2 id="Divide-and-Ensemble-Progressively-Learning-for-the-Unknown"><a href="#Divide-and-Ensemble-Progressively-Learning-for-the-Unknown" class="headerlink" title="Divide and Ensemble: Progressively Learning for the Unknown"></a>Divide and Ensemble: Progressively Learning for the Unknown</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05425">http://arxiv.org/abs/2310.05425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Zhang, Xin Shen, Heming Du, Huiqiang Chen, Chen Liu, Hongwei Sheng, Qingzheng Xu, MD Wahiduzzaman Khan, Qingtao Yu, Tianqing Zhu, Scott Chapman, Zi Huang, Xin Yu<br>for:* 这个论文是为了解决蔬菜营养不足的问题，提出了一种基于分类的方法来进行识别。methods:* 这个方法使用了分类器 ensemble 和 pseudo-labeling 技术来进行识别。results:* 这个方法在测试集上得到了93.6%的 Top-1 测试精度（94.0% 在 WW2020 上和 93.2% 在 WR2021 上），并在 Deep Nutrient Deficiency Challenge 中获得了第一名。<details>
<summary>Abstract</summary>
In the wheat nutrient deficiencies classification challenge, we present the DividE and EnseMble (DEEM) method for progressive test data predictions. We find that (1) test images are provided in the challenge; (2) samples are equipped with their collection dates; (3) the samples of different dates show notable discrepancies. Based on the findings, we partition the dataset into discrete groups by the dates and train models on each divided group. We then adopt the pseudo-labeling approach to label the test data and incorporate those with high confidence into the training set. In pseudo-labeling, we leverage models ensemble with different architectures to enhance the reliability of predictions. The pseudo-labeling and ensembled model training are iteratively conducted until all test samples are labeled. Finally, the separated models for each group are unified to obtain the model for the whole dataset. Our method achieves an average of 93.6\% Top-1 test accuracy~(94.0\% on WW2020 and 93.2\% on WR2021) and wins the 1$st$ place in the Deep Nutrient Deficiency Challenge~\footnote{https://cvppa2023.github.io/challenges/}.
</details>
<details>
<summary>摘要</summary>
在小麦营养不足分类挑战中，我们提出了分类测试数据进行进行分组的DEEM方法（DividE和Ensemble）。我们发现：1. 测试图像提供给挑战；2. 样本具有收集日期信息；3. 不同日期的样本存在明显的差异。根据这些发现，我们将数据集分成不同日期的分组，并在每个分组上训练模型。然后，我们采用 Pseudo-labeling 方法来标注测试数据，并将高信任性的预测结果包含到训练集中。在 Pseudo-labeling 中，我们利用不同架构的模型 ensemble 以提高预测的可靠性。这些pseudo-labeling和 ensemble 模型训练是相互进行的，直到所有测试样本都被标注为止。最后，我们将每个组的模型集成起来，以获得整个数据集的模型。我们的方法实现了 Top-1 测试准确率的平均值为 93.6%（94.0% 在 WW2020 和 93.2% 在 WR2021），并在 Deep Nutrient Deficiency Challenge 中获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="Humanoid-Agents-Platform-for-Simulating-Human-like-Generative-Agents"><a href="#Humanoid-Agents-Platform-for-Simulating-Human-like-Generative-Agents" class="headerlink" title="Humanoid Agents: Platform for Simulating Human-like Generative Agents"></a>Humanoid Agents: Platform for Simulating Human-like Generative Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05418">http://arxiv.org/abs/2310.05418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/humanoidagents/humanoidagents">https://github.com/humanoidagents/humanoidagents</a></li>
<li>paper_authors: Zhilin Wang, Yu Ying Chiu, Yu Cheung Chiu</li>
<li>for: 这篇论文旨在提出一种基于人类行为的生成人工智能系统，以便更好地模拟人类行为。</li>
<li>methods: 该论文使用了三种系统一类处理元素：基本需求（如饥饿、健康和能量）、情感和关系质量，以导引生成人工智能agent behave更加人类化。</li>
<li>results: 该系统能够通过这些动态元素来适应每天的活动和对其他代理的交流，并且经验证了其效果。此外，该系统还可以扩展到不同的设定和其他影响人类行为的因素（如同情、道德价值和文化背景）。<details>
<summary>Abstract</summary>
Just as computational simulations of atoms, molecules and cells have shaped the way we study the sciences, true-to-life simulations of human-like agents can be valuable tools for studying human behavior. We propose Humanoid Agents, a system that guides Generative Agents to behave more like humans by introducing three elements of System 1 processing: Basic needs (e.g. hunger, health and energy), Emotion and Closeness in Relationships. Humanoid Agents are able to use these dynamic elements to adapt their daily activities and conversations with other agents, as supported with empirical experiments. Our system is designed to be extensible to various settings, three of which we demonstrate, as well as to other elements influencing human behavior (e.g. empathy, moral values and cultural background). Our platform also includes a Unity WebGL game interface for visualization and an interactive analytics dashboard to show agent statuses over time. Our platform is available on https://www.humanoidagents.com/ and code is on https://github.com/HumanoidAgents/HumanoidAgents
</details>
<details>
<summary>摘要</summary>
“computational simulations of atoms、molecules和 cells 已经影响了我们研究科学的方法，true-to-life simulations of human-like agents 可以是我们研究人类行为的有用工具。我们提出了人工智能代理人系统（Humanoid Agents），它将引入系统1处理中的三个元素：基本需求（例如饥饿、健康和能量）、情感和关系的亲密度。人工智能代理人可以通过这些动态元素来适应每天的活动和与其他代理人的对话，并且经过实验支持。我们的系统可以扩展到不同的设定，包括三个示例，以及其他影响人类行为的元素（例如共关、道德价值和文化背景）。我们的平台还包括Unity WebGL游戏界面 для可视化和互动分析亮点，以及跟踪代理人的时间变化。我们的平台可以在 <https://www.humanoidagents.com/> 上运行，代码可以在 <https://github.com/HumanoidAgents/HumanoidAgents> 上找到。”Note: Please keep in mind that the translation is done using Google Translate, and may not be perfect or entirely accurate.
</details></li>
</ul>
<hr>
<h2 id="Ethics-of-Artificial-Intelligence-and-Robotics-in-the-Architecture-Engineering-and-Construction-Industry"><a href="#Ethics-of-Artificial-Intelligence-and-Robotics-in-the-Architecture-Engineering-and-Construction-Industry" class="headerlink" title="Ethics of Artificial Intelligence and Robotics in the Architecture, Engineering, and Construction Industry"></a>Ethics of Artificial Intelligence and Robotics in the Architecture, Engineering, and Construction Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05414">http://arxiv.org/abs/2310.05414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ci-Jyun Liang, Thai-Hoa Le, Youngjib Ham, Bharadwaj R. K. Mantha, Marvin H. Cheng, Jacob J. Lin<br>for: This research paper focuses on the ethical considerations of AI and robotics adoption in the architecture, engineering, and construction (AEC) industry.methods: The paper systematically reviews existing literature on AI and robotics research in the AEC industry, identifying key ethical issues and research topics.results: The paper identifies nine key ethical issues, including job loss, data privacy, and liability, and provides thirteen research topics for future study. It also highlights current challenges and knowledge gaps in the field, and provides recommendations for future research directions.<details>
<summary>Abstract</summary>
Artificial intelligence (AI) and robotics research and implementation emerged in the architecture, engineering, and construction (AEC) industry to positively impact project efficiency and effectiveness concerns such as safety, productivity, and quality. This shift, however, warrants the need for ethical considerations of AI and robotics adoption due to its potential negative impacts on aspects such as job security, safety, and privacy. Nevertheless, this did not receive sufficient attention, particularly within the academic community. This research systematically reviews AI and robotics research through the lens of ethics in the AEC community for the past five years. It identifies nine key ethical issues namely job loss, data privacy, data security, data transparency, decision-making conflict, acceptance and trust, reliability and safety, fear of surveillance, and liability, by summarizing existing literature and filtering it further based on its AEC relevance. Furthermore, thirteen research topics along the process were identified based on existing AEC studies that had direct relevance to the theme of ethics in general and their parallels are further discussed. Finally, the current challenges and knowledge gaps are discussed and seven specific future research directions are recommended. This study not only signifies more stakeholder awareness of this important topic but also provides imminent steps towards safer and more efficient realization.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）和机器人技术在建筑、工程和建筑（AEC）行业的研究和应用已经出现，以提高项目效率和质量的问题。但是，这种转变也需要考虑AI和机器人的伦理问题，因为它们可能对工作安全、隐私和其他方面产生负面影响。然而，这一点在学术界并未得到充分关注，特别是在AEC领域。这项研究系统性地查看了AEC社区过去五年的AI和机器人研究，并Identified nine key ethical issues，namely job loss, data privacy, data security, data transparency, decision-making conflict, acceptance and trust, reliability and safety, fear of surveillance, and liability。此外，这些研究还标识出了13个相关的研究主题，包括数据隐私、数据安全、决策冲突、接受和信任、可靠性和安全、恐慌监测和责任。最后，这项研究讨论了当前的挑战和知识漏洞，并建议七个未来研究方向。这项研究不仅增加了参与者对这个重要话题的意识，而且还提供了更安全和效率的实现方法。
</details></li>
</ul>
<hr>
<h2 id="Causal-Reasoning-through-Two-Layers-of-Cognition-for-Improving-Generalization-in-Visual-Question-Answering"><a href="#Causal-Reasoning-through-Two-Layers-of-Cognition-for-Improving-Generalization-in-Visual-Question-Answering" class="headerlink" title="Causal Reasoning through Two Layers of Cognition for Improving Generalization in Visual Question Answering"></a>Causal Reasoning through Two Layers of Cognition for Improving Generalization in Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05410">http://arxiv.org/abs/2310.05410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trang Nguyen, Naoaki Okazaki</li>
<li>For: 提高Visual Question Answering（VQA）模型的泛化能力，使其能够回答图像问题，并考虑到训练分布之外的上下文。* Methods: 提出了Cognitive pathways VQA（CopVQA）模型，通过强调 causal reasoning 因素来提高多Modal 预测。 CopVQA 首先创建了多条可能的 causal reasoning 流程，然后将每个阶段的责任划分给独立的专家和认知组件（CC）。最后，模型优先选择由两个 CC 执行的答案预测，而忽略由单个 CC 生成的答案。* Results: 对实际生活和医疗数据进行了实验，证明了 CopVQA 可以提高 VQA 性能和泛化性，并在不同的基线和领域上达到新的州OF-THE-ART 水平，而且模型规模只是当前 SOTA 的一半。<details>
<summary>Abstract</summary>
Generalization in Visual Question Answering (VQA) requires models to answer questions about images with contexts beyond the training distribution. Existing attempts primarily refine unimodal aspects, overlooking enhancements in multimodal aspects. Besides, diverse interpretations of the input lead to various modes of answer generation, highlighting the role of causal reasoning between interpreting and answering steps in VQA. Through this lens, we propose Cognitive pathways VQA (CopVQA) improving the multimodal predictions by emphasizing causal reasoning factors. CopVQA first operates a pool of pathways that capture diverse causal reasoning flows through interpreting and answering stages. Mirroring human cognition, we decompose the responsibility of each stage into distinct experts and a cognition-enabled component (CC). The two CCs strategically execute one expert for each stage at a time. Finally, we prioritize answer predictions governed by pathways involving both CCs while disregarding answers produced by either CC, thereby emphasizing causal reasoning and supporting generalization. Our experiments on real-life and medical data consistently verify that CopVQA improves VQA performance and generalization across baselines and domains. Notably, CopVQA achieves a new state-of-the-art (SOTA) on PathVQA dataset and comparable accuracy to the current SOTA on VQA-CPv2, VQAv2, and VQA RAD, with one-fourth of the model size.
</details>
<details>
<summary>摘要</summary>
通用化在视觉问答（VQA）中需要模型能够回答图像上的问题，并且考虑到训练分布之外的上下文。现有的尝试主要是对单模型方面进行精细调整，忽略了多模型方面的改进。此外，图像的多种解释会导致多种答案生成，这 highlights 了在解释和回答步骤之间的 causal reasoning 的角色。基于这个视角，我们提出了认知路径 VQA（CopVQA），它可以提高多模型预测的准确率。CopVQA 的实现方式是首先建立一个路径 pool，用于捕捉不同的 causal reasoning 流程。这与人类认知的层次结构相似，我们将解释和回答的责任分别划分为多个专家和一个认知能力Component（CC）。两个 CC 采用不同的策略来逐一执行每个专家，以便更好地捕捉 causal reasoning 的关系。最后，我们优先支持由多个 CC 共同执行的答案预测，而不是由单个 CC 生成的答案，以强调 causal reasoning 的重要性并且提高泛化能力。我们在真实生活和医疗数据上进行了实验，结果表明 CopVQA 可以提高 VQA 性能和泛化能力，并且在不同的基eline和领域上具有一致的表现。特别是，CopVQA 在 PathVQA 数据集上达到了新的状态态（SOTA），与当前 SOTA 在 VQA-CPv2、VQAv2 和 VQA RAD 数据集上的精度相似，仅使用一半的模型大小。
</details></li>
</ul>
<hr>
<h2 id="CAMEL2-Enhancing-weakly-supervised-learning-for-histopathology-images-by-incorporating-the-significance-ratio"><a href="#CAMEL2-Enhancing-weakly-supervised-learning-for-histopathology-images-by-incorporating-the-significance-ratio" class="headerlink" title="CAMEL2: Enhancing weakly supervised learning for histopathology images by incorporating the significance ratio"></a>CAMEL2: Enhancing weakly supervised learning for histopathology images by incorporating the significance ratio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05394">http://arxiv.org/abs/2310.05394</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ThoroughFuture/CAMEL2">https://github.com/ThoroughFuture/CAMEL2</a></li>
<li>paper_authors: Gang Xu, Shuhao Wang, Lingyu Zhao, Xiao Chen, Tongwei Wang, Lang Wang, Zhenwei Luo, Dahan Wang, Zewen Zhang, Aijun Liu, Wei Ba, Zhigang Song, Huaiyin Shi, Dingrong Zhong, Jianpeng Ma</li>
<li>for:  Histopathology image analysis for cancer diagnosis</li>
<li>methods:  Weakly supervised learning methods with coarse-grained labels at the image level</li>
<li>results:  Comparable performance to fully supervised baselines in both instance- and slide-level classifications, with the help of 5,120x5,120 image-level binary annotations that are easy to annotate.Here’s the summary in Traditional Chinese text:</li>
<li>for:  histopathology图像分析 для癌症诊断</li>
<li>methods:  weakly supervised learning方法，仅需 coarse-grained labels at the image level</li>
<li>results: 与完全监督基eline相比，在 both instance- 和 slide-level classification中 achieve comparable performance，仅需 5,120x5,120个易于annotate的image-level binary annotations。<details>
<summary>Abstract</summary>
Histopathology image analysis plays a crucial role in cancer diagnosis. However, training a clinically applicable segmentation algorithm requires pathologists to engage in labour-intensive labelling. In contrast, weakly supervised learning methods, which only require coarse-grained labels at the image level, can significantly reduce the labeling efforts. Unfortunately, while these methods perform reasonably well in slide-level prediction, their ability to locate cancerous regions, which is essential for many clinical applications, remains unsatisfactory. Previously, we proposed CAMEL, which achieves comparable results to those of fully supervised baselines in pixel-level segmentation. However, CAMEL requires 1,280x1,280 image-level binary annotations for positive WSIs. Here, we present CAMEL2, by introducing a threshold of the cancerous ratio for positive bags, it allows us to better utilize the information, consequently enabling us to scale up the image-level setting from 1,280x1,280 to 5,120x5,120 while maintaining the accuracy. Our results with various datasets, demonstrate that CAMEL2, with the help of 5,120x5,120 image-level binary annotations, which are easy to annotate, achieves comparable performance to that of a fully supervised baseline in both instance- and slide-level classifications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CCAE-A-Corpus-of-Chinese-based-Asian-Englishes"><a href="#CCAE-A-Corpus-of-Chinese-based-Asian-Englishes" class="headerlink" title="CCAE: A Corpus of Chinese-based Asian Englishes"></a>CCAE: A Corpus of Chinese-based Asian Englishes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05381">http://arxiv.org/abs/2310.05381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacklanda/CCAE">https://github.com/jacklanda/CCAE</a></li>
<li>paper_authors: Yang Liu, Melissa Xiaohui Qin, Long Wang, Chao Huang</li>
<li>for: 这篇论文是为了创建一个多变体资料库，用于研究亚洲英语。</li>
<li>methods: 这篇论文使用了NLP技术，创建了一个基于中文的亚洲英语多变体资料库，包括六个中文基于的亚洲英语变体。</li>
<li>results: 这篇论文提供了一个448万个单词的448万个网页文档，来自六个区域，并且这些数据可以用于语言模型的训练和下游任务，这将为亚洲英语研究提供巨大的研究 potential。<details>
<summary>Abstract</summary>
Language models have been foundations in various scenarios of NLP applications, but it has not been well applied in language variety studies, even for the most popular language like English. This paper represents one of the few initial efforts to utilize the NLP technology in the paradigm of World Englishes, specifically in creating a multi-variety corpus for studying Asian Englishes. We present an overview of the CCAE -- Corpus of Chinese-based Asian English, a suite of corpora comprising six Chinese-based Asian English varieties. It is based on 340 million tokens in 448 thousand web documents from six regions. The ontology of data would make the corpus a helpful resource with enormous research potential for Asian Englishes (especially for Chinese Englishes for which there has not been a publicly accessible corpus yet so far) and an ideal source for variety-specific language modeling and downstream tasks, thus setting the stage for NLP-based World Englishes studies. And preliminary experiments on this corpus reveal the practical value of CCAE. Finally, we make CCAE available at \href{https://huggingface.co/datasets/CCAE/CCAE-Corpus}{this https URL}.
</details>
<details>
<summary>摘要</summary>
受欢迎的语言模型在各种自然语言处理（NLP）应用场景中发挥了重要作用，但它们在语言多样性研究中尚未得到广泛应用，即使是最受欢迎的语言之一的英语。这篇论文是一个初始尝试，利用NLP技术来探索世界英语的多样性，具体来说是创建一个多种英语语料库，用于研究亚洲英语。我们介绍了CCAE——中基于英语的亚洲英语词库，这是一个包含6种中基于英语的亚洲英语变体的suite of corpora，基于3.4亿个字的448万个网页文档。这些数据的 ontology 使得这个词库成为了研究亚洲英语（特别是中英语）的有用资源，以及下游任务的理想来源，因此设置了NPLT-based World Englishes studies的场景。而我们的初步实验表明，CCAE 具有实际的价值。最后，我们将CCAE 公布在 <https://huggingface.co/datasets/CCAE/CCAE-Corpus> 这个https URL 上。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Bayesian-Optimization"><a href="#Quantum-Bayesian-Optimization" class="headerlink" title="Quantum Bayesian Optimization"></a>Quantum Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05373">http://arxiv.org/abs/2310.05373</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/daizhongxiang/Quantum_Bayesian_Optimization">https://github.com/daizhongxiang/Quantum_Bayesian_Optimization</a></li>
<li>paper_authors: Zhongxiang Dai, Gregory Kang Ruey Lau, Arun Verma, Yao Shu, Bryan Kian Hsiang Low, Patrick Jaillet</li>
<li>for: 优化复杂的黑盒函数（black-box function）</li>
<li>methods: 使用量子计算机（quantum computer）和 Gaussian process（Gaussian process）实现 Bayesian optimization（BO）</li>
<li>results: 实现了对于非线性奖励函数的优化，并且在理论上达到了 O(polylog T) 的 regret upper bound，比 классиical BO 下的lower bound Omega(sqrt(T)) 更小。<details>
<summary>Abstract</summary>
Kernelized bandits, also known as Bayesian optimization (BO), has been a prevalent method for optimizing complicated black-box reward functions. Various BO algorithms have been theoretically shown to enjoy upper bounds on their cumulative regret which are sub-linear in the number T of iterations, and a regret lower bound of Omega(sqrt(T)) has been derived which represents the unavoidable regrets for any classical BO algorithm. Recent works on quantum bandits have shown that with the aid of quantum computing, it is possible to achieve tighter regret upper bounds better than their corresponding classical lower bounds. However, these works are restricted to either multi-armed or linear bandits, and are hence not able to solve sophisticated real-world problems with non-linear reward functions. To this end, we introduce the quantum-Gaussian process-upper confidence bound (Q-GP-UCB) algorithm. To the best of our knowledge, our Q-GP-UCB is the first BO algorithm able to achieve a regret upper bound of O(polylog T), which is significantly smaller than its regret lower bound of Omega(sqrt(T)) in the classical setting. Moreover, thanks to our novel analysis of the confidence ellipsoid, our Q-GP-UCB with the linear kernel achieves a smaller regret than the quantum linear UCB algorithm from the previous work. We use simulations, as well as an experiment using a real quantum computer, to verify that the theoretical quantum speedup achieved by our Q-GP-UCB is also potentially relevant in practice.
</details>
<details>
<summary>摘要</summary>
kernelized bandits，也称为 bayesian optimization (BO)，已经是优化复杂黑盒奖励函数的常用方法。多种 BO 算法已经有理论上证明了每个迭代 T 的累累 regret 是下线的，而 Omega(sqrt(T)) 的 regret 下界则表示任何 classical BO 算法不可避免的 regret。现代量子bandits 研究表明，通过量子计算机的帮助，可以超过其对应的 classical 下界。然而，这些工作都是限制在多重武器或线性 bandits 上，因此无法解决复杂的实际问题。为此，我们介绍了量子- Gaussian 过程-上界 bound (Q-GP-UCB) 算法。根据我们所知，我们的 Q-GP-UCB 算法可以达到 O(polylog T) 的 regret Upper bound，这比 classical 设置的 Omega(sqrt(T)) 下界要小得多。此外，我们的新的 confidence ellipsoid 分析表明，我们的 Q-GP-UCB 算法使用线性 kernel 时的 regret 小于前一个工作中的量子线性 UCB 算法。我们使用 simulations 以及一个使用真实量子计算机的实验，以验证我们的理论上的量子速度提升也是在实践中有可能的。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Acoustics-with-Collaborative-Multiple-Agents"><a href="#Measuring-Acoustics-with-Collaborative-Multiple-Agents" class="headerlink" title="Measuring Acoustics with Collaborative Multiple Agents"></a>Measuring Acoustics with Collaborative Multiple Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05368">http://arxiv.org/abs/2310.05368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yyf17/MACMA">https://github.com/yyf17/MACMA</a></li>
<li>paper_authors: Yinfeng Yu, Changan Chen, Lele Cao, Fangkai Yang, Fuchun Sun</li>
<li>for:  This paper aims to improve the efficiency and accuracy of measuring environment acoustics using multiple robots.</li>
<li>methods: The paper proposes using two robots to actively move and emit&#x2F;receive sweep signals to measure the environment’s acoustics, and trains them using a collaborative multi-agent policy to explore the environment while minimizing prediction error.</li>
<li>results: The robots learn to collaborate and move to explore the environment acoustics while minimizing the prediction error, demonstrating the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
As humans, we hear sound every second of our life. The sound we hear is often affected by the acoustics of the environment surrounding us. For example, a spacious hall leads to more reverberation. Room Impulse Responses (RIR) are commonly used to characterize environment acoustics as a function of the scene geometry, materials, and source/receiver locations. Traditionally, RIRs are measured by setting up a loudspeaker and microphone in the environment for all source/receiver locations, which is time-consuming and inefficient. We propose to let two robots measure the environment's acoustics by actively moving and emitting/receiving sweep signals. We also devise a collaborative multi-agent policy where these two robots are trained to explore the environment's acoustics while being rewarded for wide exploration and accurate prediction. We show that the robots learn to collaborate and move to explore environment acoustics while minimizing the prediction error. To the best of our knowledge, we present the very first problem formulation and solution to the task of collaborative environment acoustics measurements with multiple agents.
</details>
<details>
<summary>摘要</summary>
人类生活中每秒都听到声音。声音我们听到常常受到环境的折射影响。例如，一个大厅会导致更多的延 reverberation。 Room Impulse Responses（RIR）是用于描述环境声学特性的函数，其中包括场景几何学、材料和源/接收器位置。传统上，RIRs通过在环境中设置 loudspeaker 和 microphone 来测量，这是时间consuming 和不效环境。我们提议使用两个机器人来测量环境的声学特性，它们通过活动移动和发送/接收扫描信号来测量。我们还开发了一种多agent 协同策略，这两个机器人在环境中探索声学特性，同时被奖励宽泛探索和准确预测。我们显示这两个机器人可以协同工作，在最小化预测错误的情况下探索环境声学特性。根据我们所知，我们提出了首个多agent 环境声学测量问题的问题与解决方案。
</details></li>
</ul>
<hr>
<h2 id="Molecular-De-Novo-Design-through-Transformer-based-Reinforcement-Learning"><a href="#Molecular-De-Novo-Design-through-Transformer-based-Reinforcement-Learning" class="headerlink" title="Molecular De Novo Design through Transformer-based Reinforcement Learning"></a>Molecular De Novo Design through Transformer-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05365">http://arxiv.org/abs/2310.05365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Feng, Pengcheng Xu, Tianfan Fu, Siddhartha Laghuvarapu, Jimeng Sun</li>
<li>for: 本研究旨在利用Transformer驱动的生成模型进行分子 де novo设计。</li>
<li>methods: 我们的模型可以通过Transformer进行高效的序列学习，并且可以生成具有欲要性能的分子结构。与传统的RNN驱动模型相比，我们的方法可以更好地捕捉分子结构序列中的长期依赖关系。</li>
<li>results: 我们的模型在多个任务上表现出色，包括生成查询结构的同分子和生成具有特定属性的分子。与基eline的RNN驱动方法相比，我们的方法显著提高了模型的性能。我们的方法可以用于骨架跳转、库扩展和预测高活性分子。<details>
<summary>Abstract</summary>
In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了一种方法，用于微调基于Transformer的生成模型，以实现分子的德诺vo计划。我们利用Transformer对序列学习的优势，可以效果地生成具有所需性能的分子结构。与传统的RNN基本方法相比，我们的提议方法在生成具有不同生物目标活性的分子结构方面表现出色，捕捉分子结构序列中长期依赖关系。我们的方法在多个任务上展现出优势，包括生成查询结构的同化体和生成具有特定属性的分子，超越基eline RNN基本方法。我们的方法可以用于跳跃架构、从单个分子开始扩大图书馆，以及预测高活性 against生物目标的分子。
</details></li>
</ul>
<hr>
<h2 id="Universal-Multi-modal-Entity-Alignment-via-Iteratively-Fusing-Modality-Similarity-Paths"><a href="#Universal-Multi-modal-Entity-Alignment-via-Iteratively-Fusing-Modality-Similarity-Paths" class="headerlink" title="Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths"></a>Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05364">http://arxiv.org/abs/2310.05364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blzhu0823/pathfusion">https://github.com/blzhu0823/pathfusion</a></li>
<li>paper_authors: Bolin Zhu, Xiaoze Liu, Xin Mao, Zhuo Chen, Lingbing Guo, Tao Gui, Qi Zhang</li>
<li>for: 本研究旨在提高知识 graphs（KGs）的一体化性，通过发现多个 KGs 中相同实体对的对应关系。</li>
<li>methods: 本研究提出了PathFusion方法，包括两个主要组成部分：一是MSP模型，它通过建立实体和模式节点之间的路径来表示多个Modalities;二是IRF融合方法，它通过路径作为信息传递者，有效地融合不同Modalities中的信息。</li>
<li>results: 实验结果表明，对真实世界数据集进行测试的PathFusion方法，与现有方法相比，具有22.4%-28.9%的绝对提升（Hits@1），以及0.194-0.245的绝对提升（MRR）。<details>
<summary>Abstract</summary>
The objective of Entity Alignment (EA) is to identify equivalent entity pairs from multiple Knowledge Graphs (KGs) and create a more comprehensive and unified KG. The majority of EA methods have primarily focused on the structural modality of KGs, lacking exploration of multi-modal information. A few multi-modal EA methods have made good attempts in this field. Still, they have two shortcomings: (1) inconsistent and inefficient modality modeling that designs complex and distinct models for each modality; (2) ineffective modality fusion due to the heterogeneous nature of modalities in EA. To tackle these challenges, we propose PathFusion, consisting of two main components: (1) MSP, a unified modeling approach that simplifies the alignment process by constructing paths connecting entities and modality nodes to represent multiple modalities; (2) IRF, an iterative fusion method that effectively combines information from different modalities using the path as an information carrier. Experimental results on real-world datasets demonstrate the superiority of PathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement on Hits@1, and 0.194-0.245 absolute improvement on MRR.
</details>
<details>
<summary>摘要</summary>
目标是实体对应（Entity Alignment，EA）是从多个知识图（Knowledge Graph，KG）中标识相应的实体对，并创建一个更加完整和统一的KG。大多数EA方法主要关注了知识图的结构性，缺乏多 modal 信息的探索。一些多模式EA方法有所进步，但它们具有两个缺陷：（1）不稳定和不效率的模式模型，通常采用复杂和特定的模型来表示每种模式；（2）不具有有效的多模式融合，由于实体对应中的多种模式具有不同的特征。为了解决这些挑战，我们提出了PathFusion，它包括两个主要组成部分：（1）MSP，一种简化实体对应过程的统一模型方法，通过构建实体和模式节点之间的路径来表示多种模式；（2）IRF，一种迭代融合方法，通过路径作为信息传递者，有效地将不同模式的信息融合在一起。实验结果表明，PathFusion比 estado-of-the-art 方法具有22.4%-28.9%的绝对改善率（Hits@1），和0.194-0.245的绝对改善率（MRR）。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Neural-Collapse-for-a-Large-Number-of-Classes"><a href="#Generalized-Neural-Collapse-for-a-Large-Number-of-Classes" class="headerlink" title="Generalized Neural Collapse for a Large Number of Classes"></a>Generalized Neural Collapse for a Large Number of Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05351">http://arxiv.org/abs/2310.05351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kongwanbianjinyu/Generalized-Neural-Collapse-for-a-Large-Number-of-Classes">https://github.com/kongwanbianjinyu/Generalized-Neural-Collapse-for-a-Large-Number-of-Classes</a></li>
<li>paper_authors: Jiachen Jiang, Jinxin Zhou, Peng Wang, Qing Qu, Dustin Mixon, Chong You, Zhihui Zhu</li>
<li>for: 这篇论文旨在探讨深度学习模型中的学习后层表示和分类权重的概念化，以及如何通过新的技术来提高实际深度模型的性能。</li>
<li>methods: 这篇论文使用了一种名为“普通化神经垮坏”的概念，用于描述深度学习模型中的学习后层表示和分类权重。这种概念可以帮助我们更好地理解深度学习模型的工作机理，并提供新的技术来提高模型的性能。</li>
<li>results: 这篇论文显示了在实际深度神经网络中发生的“普通化神经垮坏”现象，即在大数据集中，分类器的最小一对一 margin 是最大化的。此外，论文还提供了一系列实际和理论研究，以证明这种现象的存在性和可靠性。<details>
<summary>Abstract</summary>
Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized.We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show that the generalized neural collapse provably occurs under unconstrained feature model with spherical constraint, under certain technical conditions on feature dimension and number of classes.
</details>
<details>
<summary>摘要</summary>
神经坍塌提供了深度学习模型中学习最后层表示（即特征）以及分类器权重的简洁数学定义。这些结果不仅提供了启示，还激发了改进实际深度模型的新技术。然而，现有的实际和理论研究多集中在深度模型中的小数目类别情况下进行研究。这篇论文扩展了神经坍塌到类别数量远大于特征空间维度的情况下，这种情况广泛出现在语音识别、检索系统和人脸识别等应用中。我们显示了神经坍塌现象在实际深度神经网络中发生，并且提供了理论研究，证明在不受特征模型约束的情况下，神经坍塌在某种技术条件下发生。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Invariance-Learning"><a href="#Continuous-Invariance-Learning" class="headerlink" title="Continuous Invariance Learning"></a>Continuous Invariance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05348">http://arxiv.org/abs/2310.05348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Lin, Fan Zhou, Lu Tan, Lintao Ma, Jiameng Liu, Yansu He, Yuan Yuan, Yu Liu, James Zhang, Yujiu Yang, Hao Wang</li>
<li>for: 这篇论文主要针对了连续域问题的一致学习问题，即如何通过学习不变特征来提高模型的一致性。</li>
<li>methods: 这篇论文提出了一种新的连续域一致学习方法（CIL），该方法通过测量和控制 conditional independence 来提取连续域上的不变特征。</li>
<li>results: 论文的实验结果表明，CIL 可以在各种实验数据集上（包括生产环境中的数据）与强基线相比，具有更高的一致性。<details>
<summary>Abstract</summary>
Invariance learning methods aim to learn invariant features in the hope that they generalize under distributional shifts. Although many tasks are naturally characterized by continuous domains, current invariance learning techniques generally assume categorically indexed domains. For example, auto-scaling in cloud computing often needs a CPU utilization prediction model that generalizes across different times (e.g., time of a day and date of a year), where `time' is a continuous domain index. In this paper, we start by theoretically showing that existing invariance learning methods can fail for continuous domain problems. Specifically, the naive solution of splitting continuous domains into discrete ones ignores the underlying relationship among domains, and therefore potentially leads to suboptimal performance. To address this challenge, we then propose Continuous Invariance Learning (CIL), which extracts invariant features across continuously indexed domains. CIL is a novel adversarial procedure that measures and controls the conditional independence between the labels and continuous domain indices given the extracted features. Our theoretical analysis demonstrates the superiority of CIL over existing invariance learning methods. Empirical results on both synthetic and real-world datasets (including data collected from production systems) show that CIL consistently outperforms strong baselines among all the tasks.
</details>
<details>
<summary>摘要</summary>
对于不变学习方法来说，它们的目标是学习不变特征，以便在分布转移时保持一致。虽然许多任务是自然地表示为连续领域，但当前的不变学习技术通常假设Category indexed领域。例如，云计算中的自动扩缩通常需要一个能够在不同时间（例如天时和年度）上泛化的CPU使用预测模型，其中`time'是连续领域的索引。在这篇论文中，我们开始 by theoretically showing that existing invariance learning methods can fail for continuous domain problems。Specifically, the naive solution of splitting continuous domains into discrete ones ignores the underlying relationship among domains, and therefore potentially leads to suboptimal performance。To address this challenge, we then propose Continuous Invariance Learning (CIL), which extracts invariant features across continuously indexed domains。CIL is a novel adversarial procedure that measures and controls the conditional independence between the labels and continuous domain indices given the extracted features。我们的理论分析表明CIL比既有的不变学习方法更加有利。empirical results on both synthetic and real-world datasets（包括生产系统中收集的数据）显示，CIL在所有任务中一直表现出优于强基eline。
</details></li>
</ul>
<hr>
<h2 id="SteerLM-Attribute-Conditioned-SFT-as-an-User-Steerable-Alternative-to-RLHF"><a href="#SteerLM-Attribute-Conditioned-SFT-as-an-User-Steerable-Alternative-to-RLHF" class="headerlink" title="SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF"></a>SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05344">http://arxiv.org/abs/2310.05344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, Oleksii Kuchaiev</li>
<li>for: 该研究目标是使大语言模型（LLM）更加适应人类价值观。</li>
<li>methods: 该研究使用监督精度调整（SFT）和人类反馈学习（RLHF）两个阶段。</li>
<li>results: 试验结果表明，使用SteerLM可以生成更加有用和高质量的回答，而且训练更加容易。 compare to 多种基elines。<details>
<summary>Abstract</summary>
Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的模型对齐人类偏好是一项重要步骤，以确保模型与人类价值观 align。通常包括监督微调（SFT）和人类反馈学习（RLHF）两个阶段。然而，RLHF受到内在的限制，包括复杂的训练设置和对隐藏的价值观align。此外，RLHF阶段的奖励模型通常依赖单一的反馈信号，而不是显式、多方面的信号，如帮助程度、幽默度和恶势力。为解决这些限制，我们提出了SteerLM，一种监督微调方法，使得用户可以在推理时控制响应。SteerLM Condition Responses to Conform to an Explicitly Defined Multi-Dimensional Set of Attributes, thereby Empowering a Steerable AI Capable of Generating Helpful and High-Quality Responses While Maintaining Customizability。实验显示，SteerLM在开源数据集上训练后可以跟上人类和自动评价者的首选，而且训练得非常容易。可以在https://huggingface.co/nvidia/SteerLM-llama2-13B中尝试SteerLM。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Continuous-Learning-in-Spiking-Neural-Networks"><a href="#Investigating-Continuous-Learning-in-Spiking-Neural-Networks" class="headerlink" title="Investigating Continuous Learning in Spiking Neural Networks"></a>Investigating Continuous Learning in Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05343">http://arxiv.org/abs/2310.05343</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Tanner Fredieu</li>
<li>for: 这个论文探讨了使用第三代机器学习算法（叫做脉冲神经网络架构）进行连续学习的可能性，并与传统模型进行比较。</li>
<li>methods: 这个论文使用了三个阶段的实验。第一阶段是使用传输学习来训练传统模型。第二阶段使用Nengo模型库中的模型进行训练。最后，每个传统模型都被转换成了脉冲神经网络，并进行了训练。</li>
<li>results: 初步结果表明，使用SNN模型可以避免了训练过程中的迷失知识问题，但还需要进一步的研究。所有模型都能正确地识别当前的类别，但是它们都会在前一个类别上具有高于正常水平的输出概率。这表明SNN模型有潜力来超越迷失知识问题，但还需要很多的研究和改进。<details>
<summary>Abstract</summary>
In this paper, the use of third-generation machine learning, also known as spiking neural network architecture, for continuous learning was investigated and compared to conventional models. The experimentation was divided into three separate phases. The first phase focused on training the conventional models via transfer learning. The second phase trains a Nengo model from their library. Lastly, each conventional model is converted into a spiking neural network and trained. Initial results from phase 1 are inline with known knowledge about continuous learning within current machine learning literature. All models were able to correctly identify the current classes, but they would immediately see a sharp performance drop in previous classes due to catastrophic forgetting. However, the SNN models were able to retain some information about previous classes. Although many of the previous classes were still identified as the current trained classes, the output probabilities showed a higher than normal value to the actual class. This indicates that the SNN models do have potential to overcome catastrophic forgetting but much work is still needed.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，使用第三代机器学习技术，即脉冲神经网络架构，进行连续学习的可能性进行了调查和比较，并与传统模型进行对比。实验分为三个阶段。第一阶段是通过转移学习训练传统模型。第二阶段使用Nengo模型库中的模型进行训练。最后，每个传统模型都被转换成脉冲神经网络并进行训练。初果显示，第一阶段的结果与现有机器学习文献中关于连续学习的知识一致。所有模型都能正确地识别当前的类别，但它们都会因为恐慌遗忘而显示出过去类别的性能下降。然而，SNN模型能够保持一些过去类别的信息。虽然许多过去类别仍然被识别为当前训练类别，但输出概率显示高于实际类别的值。这表示SNN模型有可能超越恐慌遗忘，但还需要进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Look-at-Classic-Test-Time-Adaptation-Methods-in-Semantic-Segmentation"><a href="#A-Critical-Look-at-Classic-Test-Time-Adaptation-Methods-in-Semantic-Segmentation" class="headerlink" title="A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation"></a>A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05341">http://arxiv.org/abs/2310.05341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang’an Yi, Haotian Chen, Yifan Zhang, Yonghui Xu, Lizhen Cui</li>
<li>for: 这篇研究的目的是探讨测试时适应（TTA）在Semantic Segmentation任务中的应用。</li>
<li>methods: 这篇研究使用了 Classic TTA 方法，包括批量 normalization 更新策略和教师学生结构，以测试它们是否能够有效地适应 Semantic Segmentation 任务中的数据分布变化。</li>
<li>results: 研究结果显示，这些 Classic TTA 方法在 Semantic Segmentation 任务中的表现不如预期，特别是在批量 normalization 更新策略和教师学生结构方面。此外，Segmentation TTA 还面临着严重的长尾问题，这问题比 классификаation TTA 更加复杂。<details>
<summary>Abstract</summary>
Test-time adaptation (TTA) aims to adapt a model, initially trained on training data, to potential distribution shifts in the test data. Most existing TTA studies, however, focus on classification tasks, leaving a notable gap in the exploration of TTA for semantic segmentation. This pronounced emphasis on classification might lead numerous newcomers and engineers to mistakenly assume that classic TTA methods designed for classification can be directly applied to segmentation. Nonetheless, this assumption remains unverified, posing an open question. To address this, we conduct a systematic, empirical study to disclose the unique challenges of segmentation TTA, and to determine whether classic TTA strategies can effectively address this task. Our comprehensive results have led to three key observations. First, the classic batch norm updating strategy, commonly used in classification TTA, only brings slight performance improvement, and in some cases it might even adversely affect the results. Even with the application of advanced distribution estimation techniques like batch renormalization, the problem remains unresolved. Second, the teacher-student scheme does enhance training stability for segmentation TTA in the presence of noisy pseudo-labels. However, it cannot directly result in performance improvement compared to the original model without TTA. Third, segmentation TTA suffers a severe long-tailed imbalance problem, which is substantially more complex than that in TTA for classification. This long-tailed challenge significantly affects segmentation TTA performance, even when the accuracy of pseudo-labels is high. In light of these observations, we conclude that TTA for segmentation presents significant challenges, and simply using classic TTA methods cannot address this problem well.
</details>
<details>
<summary>摘要</summary>
测试时适应（TTA）目的是使模型，首先在训练数据上进行训练，适应测试数据中的可能存在的分布变化。大多数现有的TTA研究， however，它们主要关注分类任务，留下了 semantic segmentation 的探索空间。这种注重分类的偏好可能导致许多新手和工程师 mistakenly assume that classic TTA methods designed for classification can be directly applied to segmentation。然而，这个假设仍未得到证明，这 constitutes an open question。为了解决这个问题，我们进行了系统性的、 empirical 的研究，以揭示 semantic segmentation TTA 中独特的挑战，并确定 classic TTA 策略是否能有效地解决这个任务。我们的全面的结果表明，有三个关键观察：1. 通常用于 classification TTA 的 batch norm 更新策略，对 semantic segmentation TTA 来说只有微scopic 的性能提高，而在一些情况下，甚至会 adversely affect the results。即使使用 advanced distribution estimation techniques like batch renormalization，问题仍未得到解决。2. teacher-student scheme 可以增强 semantic segmentation TTA 中的训练稳定性，但是它不能直接导致性能提高，与无TTA 的原始模型相比。3. semantic segmentation TTA 受到严重的长尾偏度问题困扰，这个问题比分类 TTA 更加复杂。这种长尾偏度问题会很大地影响 semantic segmentation TTA 性能，即使 pseudo-labels 的准确率很高。根据这些观察结论，我们 conclude that semantic segmentation TTA 存在 significativeschallenges，并且简单地使用 classic TTA methods 无法很好地解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Long-form-Text-Generation-Efficacy-with-Task-adaptive-Tokenization"><a href="#Enhancing-Long-form-Text-Generation-Efficacy-with-Task-adaptive-Tokenization" class="headerlink" title="Enhancing Long-form Text Generation Efficacy with Task-adaptive Tokenization"></a>Enhancing Long-form Text Generation Efficacy with Task-adaptive Tokenization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05317">http://arxiv.org/abs/2310.05317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MichiganNLP/task-adaptive_tokenization">https://github.com/MichiganNLP/task-adaptive_tokenization</a></li>
<li>paper_authors: Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, Rada Mihalcea</li>
<li>for: 提高长文生成性能，特别是在心理问答任务中</li>
<li>methods: 采用任务适应的Tokenization，通过多个结果的采样，使用任务特定的数据来优化抽象概率</li>
<li>results: 在中文和英文心理问答任务上，通过对特定任务的抽象进行优化，可以提高生成性能，并且使用60% fewer tokens。初步实验表明，将我们的抽象方法与大语言模型结合使用，具有扎实的前景。<details>
<summary>Abstract</summary>
We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
</details>
<details>
<summary>摘要</summary>
我们提议使用任务适应式分词法来适应下游任务的特点，以提高长文本生成在心理健康领域。 drawing inspiration from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We propose a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments suggest promising results when using our tokenization approach with very large language models.Here's the breakdown of the translation:* 我们 (wǒmen) - we* 提议 (tīyì) - propose* 使用 (shǐyòu) - use* 任务适应式 (róngyè tíyìxì) - task-adaptive* 分词法 (fēnzihòu) - tokenization* 适应 (shìyìng) - adapt* 下游 (xiàyù) - downstream* 任务 (àiwù) - task* 特点 (tèdiǎn) - specifics* 以提高 (yǐ tígāo) - to improve* 长文本 (chángwén tiě) - long-form* 生成 (shēngchǎn) - generation* 在 (zhī) - in* 心理健康 (xīn lí jīn kāng) - mental health* 领域 (lǐngyè) - field* drawing inspiration from (zhìyì zhī) - inspired by* cognitive science (xīn lí kēxíng) - cognitive science* 任务特定 (àiwù tèqì) - task-specific* 数据 (shùdà) - data* 优化 (yòujiā) - optimize* probabilities (jiào dé) - probabilities*  sampling (jiào) - sampling* 变量 (biānliàng) - variable* segmentations (jiāo biān) - segmentations* 多种 (duōzhèng) - multiple* outcomes (fāngyì) - outcomes*  integration (tōngyì) - integration* 特定 vocabulary (tèqì yǔyīn) - specialized vocabulary*  introduce (jìdǎo) - propose* a strategy (a jìdǎo) - a strategy* for building (jìdǎo) - for building* a specialized vocabulary (tèqì yǔyīn) - a specialized vocabulary* and introduce (jìdǎo) - and introduce* a vocabulary merging protocol (yǔyīn tóngxīng) - a vocabulary merging protocol* that allows for (dēng) - that allows for* the integration (tōngyì) - the integration* of task-specific tokens (àiwù zhǐxīn) - of task-specific tokens* into (dào) - into* the pre-trained model's (zhìyì zhī) - the pre-trained model's* tokenization step (tiězi xiàng) - tokenization step* Through (zhī) - through* extensive experiments (zhìyì yánjiū) - extensive experiments* on (zhī) - on* psychological question-answering (xīn lí yánsuō) - psychological question-answering* tasks (àiwù) - tasks* in (zhī) - in* both (liǎng) - both* Chinese (zhōngwén) - Chinese* and English (yīnggrēsī) - and English* we find (wǒmen jiào) - we find* a significant improvement (tóngyì zhìyì) - a significant improvement* in generation performance (chángwén tiěyì) - in generation performance* while using (yǐ) - while using* up to 60% fewer tokens (dào zhìyì) - up to 60% fewer tokens* Preliminary experiments (zhìyì yánjiū) - preliminary experiments* point to promising results (zhìyì zhìyì) - point to promising results* when using (yǐ) - when using* our tokenization approach (wǒmen tiězi xiàng) - our tokenization approach* with very large language models (dào yìjī zhīyì) - with very large language models.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/cs.AI_2023_10_09/" data-id="clombedoz005fs0886sik2t4l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/cs.CL_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T11:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/cs.CL_2023_10_09/">cs.CL - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GPT-who-An-Information-Density-based-Machine-Generated-Text-Detector"><a href="#GPT-who-An-Information-Density-based-Machine-Generated-Text-Detector" class="headerlink" title="GPT-who: An Information Density-based Machine-Generated Text Detector"></a>GPT-who: An Information Density-based Machine-Generated Text Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06202">http://arxiv.org/abs/2310.06202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saranya Venkatraman, Adaku Uchendu, Dongwon Lee</li>
<li>for: 本研究旨在检验语言模型和人类语言之间的差异，并提出一种基于统一信息密度原理的多类域不偏推权分类器GPT-who。</li>
<li>methods: 该分类器使用统一信息密度基本特征来模型每个语言模型和人类作者的独特统计特征，以便准确地归类作者。</li>
<li>results: 对4个大规模测试集进行评估，GPT-who比前一代统计基于分类器和非统计基于分类器的检测器（如GLTR、GPTZero、OpenAI检测器和ZeroGPT）提高了超过20%的表现。此外，GPT-who还具有较低的计算成本和可读性的优势。<details>
<summary>Abstract</summary>
The Uniform Information Density principle posits that humans prefer to spread information evenly during language production. In this work, we examine if the UID principle can help capture differences between Large Language Models (LLMs) and human-generated text. We propose GPT-who, the first psycholinguistically-aware multi-class domain-agnostic statistical-based detector. This detector employs UID-based features to model the unique statistical signature of each LLM and human author for accurate authorship attribution. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- & non-statistical-based) such as GLTR, GPTZero, OpenAI detector, and ZeroGPT by over $20$% across domains. In addition to superior performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We present the largest analysis of the UID-based representations of human and machine-generated texts (over 400k articles) to demonstrate how authors distribute information differently, and in ways that enable their detection using an off-the-shelf LM without any fine-tuning. We find that GPT-who can distinguish texts generated by very sophisticated LLMs, even when the overlying text is indiscernible.
</details>
<details>
<summary>摘要</summary>
人类偏好将信息均匀分布在语言生成中，这种现象被称为Uniform Information Density原理（UID）。在这项工作中，我们研究了UID原理是否可以捕捉不同的大语言模型（LLM）和人类生成的文本之间的差异。我们提出了GPT-who，首个心理语言学感知的多类域共通统计基础探测器。这个探测器使用UID基本特征来模型每个LLM和人类作者的唯一统计特征，以便准确地归属作者。我们使用4个大规模数据集进行评估，发现GPT-who在各个领域都高于当前最佳探测器（包括统计和非统计基础的探测器），例如GLTR、GPTZero、OpenAI探测器和ZeroGPT，以上差20%以上。此外，GPT-who还具有低计算成本和可解释的文本表示，并进行了400k篇文章的最大分析，以示出作者在分布信息的方式，以及如何使用存储库LM进行探测。我们发现GPT-who可以分辨由非常复杂的LLM生成的文本，即使文本总体看起来一样。
</details></li>
</ul>
<hr>
<h2 id="Compressing-Context-to-Enhance-Inference-Efficiency-of-Large-Language-Models"><a href="#Compressing-Context-to-Enhance-Inference-Efficiency-of-Large-Language-Models" class="headerlink" title="Compressing Context to Enhance Inference Efficiency of Large Language Models"></a>Compressing Context to Enhance Inference Efficiency of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06201">http://arxiv.org/abs/2310.06201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyucheng09/selective_context">https://github.com/liyucheng09/selective_context</a></li>
<li>paper_authors: Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin</li>
<li>for: 提高大语言模型（LLM）的推理效率，解决长文档和长 conversations 的计算要求增加和内存占用问题。</li>
<li>methods: 提出了一种名为选择性上下文的方法，通过识别并剔除输入上下文中的重复部分，使输入更加紧凑。</li>
<li>results: 实验结果表明，选择性上下文方法可以significantly 降低内存成本和生成时间，保持与全Context相对的性能，具体是：Context cost 减少50%，内存使用量减少36%，生成时间减少32%，而BERTscore和 faithfulness 只减少0.023和0.038。<details>
<summary>Abstract</summary>
Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\% reduction in context cost, resulting in a 36\% reduction in inference memory usage and a 32\% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Importance-of-Prompt-Tuning-for-Automated-Neuron-Explanations"><a href="#The-Importance-of-Prompt-Tuning-for-Automated-Neuron-Explanations" class="headerlink" title="The Importance of Prompt Tuning for Automated Neuron Explanations"></a>The Importance of Prompt Tuning for Automated Neuron Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06200">http://arxiv.org/abs/2310.06200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Lee, Tuomas Oikarinen, Arjun Chatha, Keng-Chi Chang, Yilan Chen, Tsui-Wei Weng</li>
<li>for: 了解大语言模型（LLMs）的各个神经元的作用，以便更好地理解模型和其安全性。</li>
<li>methods: 基于之前的研究，使用大语言模型such as GPT-4来解释每个神经元的作用。 Specifically, 分析使用的提示来生成解释的效果，并改进提示的格式以提高解释质量和减少计算成本。</li>
<li>results: 通过三种不同的方法，包括自动和人工评估，证明了我们的新提示可以大幅提高神经元解释质量，同时减少计算成本。<details>
<summary>Abstract</summary>
Recent advances have greatly increased the capabilities of large language models (LLMs), but our understanding of the models and their safety has not progressed as fast. In this paper we aim to understand LLMs deeper by studying their individual neurons. We build upon previous work showing large language models such as GPT-4 can be useful in explaining what each neuron in a language model does. Specifically, we analyze the effect of the prompt used to generate explanations and show that reformatting the explanation prompt in a more natural way can significantly improve neuron explanation quality and greatly reduce computational cost. We demonstrate the effects of our new prompts in three different ways, incorporating both automated and human evaluations.
</details>
<details>
<summary>摘要</summary>
最近的进步使大语言模型（LLM）的能力得到了大幅提高，但我们对这些模型和其安全性的理解尚未随着速度进步。在这篇论文中，我们尝试更深入地理解LLM，通过研究它们的个体神经元。我们建立在之前的工作之上，证明大语言模型如GPT-4可以用于解释每个语言模型神经元的作用。我们分析推荐的提示对神经元解释质量产生的影响，并显示通过更自然的提示格式化可以显著提高神经元解释质量，同时大幅降低计算成本。我们通过三种不同的方法示出了我们新的提示的效果，包括自动和人工评估。
</details></li>
</ul>
<hr>
<h2 id="BYOC-Personalized-Few-Shot-Classification-with-Co-Authored-Class-Descriptions"><a href="#BYOC-Personalized-Few-Shot-Classification-with-Co-Authored-Class-Descriptions" class="headerlink" title="BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions"></a>BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06111">http://arxiv.org/abs/2310.06111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arth Bohra, Govert Verkes, Artem Harutyunyan, Pascal Weinberger, Giovanni Campagna</li>
<li>for: 该论文旨在提供一种可以让用户自己建立文本分类器的新方法，以便用户可以根据自己的需求建立个性化的分类器。</li>
<li>methods: 该方法使用大语言模型（LLM），并通过用户和LLM之间的互动来帮助用户描述每个类型的核心特征。用户通过 annotating 每个少量示例来提供描述，并且 LLM 会提问有关每个示例的问题，以便帮助用户更好地描述每个类型。</li>
<li>results: 实验表明，该方法可以达到高精度（大约 82%），只使用了比较少的数据集训练。此外，在30名参与者的研究中，个性化的分类器的平均精度达到 90%，比州态艺前的方法高出 15%。<details>
<summary>Abstract</summary>
Text classification is a well-studied and versatile building block for many NLP applications. Yet, existing approaches require either large annotated corpora to train a model with or, when using large language models as a base, require carefully crafting the prompt as well as using a long context that can fit many examples. As a result, it is not possible for end-users to build classifiers for themselves. To address this issue, we propose a novel approach to few-shot text classification using an LLM. Rather than few-shot examples, the LLM is prompted with descriptions of the salient features of each class. These descriptions are coauthored by the user and the LLM interactively: while the user annotates each few-shot example, the LLM asks relevant questions that the user answers. Examples, questions, and answers are summarized to form the classification prompt. Our experiments show that our approach yields high accuracy classifiers, within 82% of the performance of models trained with significantly larger datasets while using only 1% of their training sets. Additionally, in a study with 30 participants, we show that end-users are able to build classifiers to suit their specific needs. The personalized classifiers show an average accuracy of 90%, which is 15% higher than the state-of-the-art approach.
</details>
<details>
<summary>摘要</summary>
文本分类是一个已经广泛研究并且具有多种应用的基础模块。然而，现有的方法需要大量的标注数据来训练一个模型，或者使用大型语言模型为基础，并且需要考虑制定的提示和长Context。这使得普通用户无法建立自己的分类器。为解决这个问题，我们提出了一种新的几个示例文本分类方法使用LLM。而不是几个示例，LLM被提示了每个类型的突出特征的描述。这些描述由用户和LLM共同编写：用户在每个几个示例中注解，LLM则问到有关的问题，用户回答。示例、问题和答案被总结为分类提示。我们的实验表明，我们的方法可以在82%的性能下建立高精度分类器，使用的训练集只有1%。此外，我们在30名参与者的研究中发现，普通用户可以建立自己需求的个性化分类器，这些分类器的平均精度为90%，高于现有方法15%。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Multilingual-Self-Supervised-Pretrained-Models-for-Sequence-to-Sequence-End-to-End-Spoken-Language-Understanding"><a href="#Leveraging-Multilingual-Self-Supervised-Pretrained-Models-for-Sequence-to-Sequence-End-to-End-Spoken-Language-Understanding" class="headerlink" title="Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence End-to-End Spoken Language Understanding"></a>Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence End-to-End Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06103">http://arxiv.org/abs/2310.06103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/digitalphonetics/multilingual-seq2seq-slu">https://github.com/digitalphonetics/multilingual-seq2seq-slu</a></li>
<li>paper_authors: Pavel Denisov, Ngoc Thang Vu</li>
<li>for: 这个论文旨在提出一种能够执行端到端语言理解（E2E-SLU）的多语言设置和任务，包括预测词语填充。</li>
<li>methods: 该方法使用预训练的语音和文本模型，并将其集成到一个生成型模型中，以实现E2E-SLU任务。</li>
<li>results: 经过预训练7000小时的多语言数据后，该模型可以超越当前状态的两个SLU数据集，并在另外两个SLU数据集上达到一定的改进。此外，该模型还可以在不同语言之间进行跨语言比较，并在PortMEDIA-Language数据集上提高最佳结果，减少了23.65%的概念&#x2F;价值错误率。<details>
<summary>Abstract</summary>
A number of methods have been proposed for End-to-End Spoken Language Understanding (E2E-SLU) using pretrained models, however their evaluation often lacks multilingual setup and tasks that require prediction of lexical fillers, such as slot filling. In this work, we propose a unified method that integrates multilingual pretrained speech and text models and performs E2E-SLU on six datasets in four languages in a generative manner, including the prediction of lexical fillers. We investigate how the proposed method can be improved by pretraining on widely available speech recognition data using several training objectives. Pretraining on 7000 hours of multilingual data allows us to outperform the state-of-the-art ultimately on two SLU datasets and partly on two more SLU datasets. Finally, we examine the cross-lingual capabilities of the proposed model and improve on the best known result on the PortMEDIA-Language dataset by almost half, achieving a Concept/Value Error Rate of 23.65%.
</details>
<details>
<summary>摘要</summary>
许多方法已经被提出用于终端到终端语言理解（E2E-SLU），但是它们的评估通常缺乏多语言设置和需要预测词语填充的任务。在这项工作中，我们提出了一种统一方法，将多语言预训练的语音和文本模型集成，并在六个数据集上进行E2E-SLU，包括词语填充预测。我们研究了如何通过多语言批处理训练对这种方法进行改进，并在7000小时多语言数据上进行预训练。这些预训练可以使我们超越当前状态的术语SLU数据集上的最佳性能，并在两个SLU数据集上部分超越状态。最后，我们检查了提posed模型的交叉语言能力，并在PortMEDIA-Language数据集上提高了最佳知识的结果，将概念/价值错误率降低至23.65%。
</details></li>
</ul>
<hr>
<h2 id="Auditing-Gender-Analyzers-on-Text-Data"><a href="#Auditing-Gender-Analyzers-on-Text-Data" class="headerlink" title="Auditing Gender Analyzers on Text Data"></a>Auditing Gender Analyzers on Text Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06061">http://arxiv.org/abs/2310.06061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth D Jaiswal, Ankit Kumar Verma, Animesh Mukherjee</li>
<li>for: This study aims to audit existing gender analyzers for biases against non-binary individuals.</li>
<li>methods: The study uses two datasets (Reddit comments and Tumblr posts) and fine-tunes a BERT multi-label classifier on these datasets to evaluate the accuracy of the gender analyzers.</li>
<li>results: The study finds that the existing gender analyzers are highly inaccurate, with an overall accuracy of ~50% on all platforms. The fine-tuned BERT model achieves an overall performance of ~77% on the most realistically deployable setting and a surprisingly higher performance of 90% for the non-binary class. Additionally, the study shows that ChatGPT, a highly advanced AI model, is also biased and needs better audits and moderation.<details>
<summary>Abstract</summary>
AI models have become extremely popular and accessible to the general public. However, they are continuously under the scanner due to their demonstrable biases toward various sections of the society like people of color and non-binary people. In this study, we audit three existing gender analyzers -- uClassify, Readable and HackerFactor, for biases against non-binary individuals. These tools are designed to predict only the cisgender binary labels, which leads to discrimination against non-binary members of the society. We curate two datasets -- Reddit comments (660k) and, Tumblr posts (2.05M) and our experimental evaluation shows that the tools are highly inaccurate with the overall accuracy being ~50% on all platforms. Predictions for non-binary comments on all platforms are mostly female, thus propagating the societal bias that non-binary individuals are effeminate. To address this, we fine-tune a BERT multi-label classifier on the two datasets in multiple combinations, observe an overall performance of ~77% on the most realistically deployable setting and a surprisingly higher performance of 90% for the non-binary class. We also audit ChatGPT using zero-shot prompts on a small dataset (due to high pricing) and observe an average accuracy of 58% for Reddit and Tumblr combined (with overall better results for Reddit).   Thus, we show that existing systems, including highly advanced ones like ChatGPT are biased, and need better audits and moderation and, that such societal biases can be addressed and alleviated through simple off-the-shelf models like BERT trained on more gender inclusive datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Few-Shot-Spoken-Language-Understanding-via-Joint-Speech-Text-Models"><a href="#Few-Shot-Spoken-Language-Understanding-via-Joint-Speech-Text-Models" class="headerlink" title="Few-Shot Spoken Language Understanding via Joint Speech-Text Models"></a>Few-Shot Spoken Language Understanding via Joint Speech-Text Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05919">http://arxiv.org/abs/2310.05919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chung-Ming Chien, Mingjiamei Zhang, Ju-Chieh Chou, Karen Livescu</li>
<li>for: 提高 spoken language understanding 任务中的数据有限性问题</li>
<li>methods: 使用共享表示空间的 speech-text 模型，并将文本模型转移到语音测试数据上</li>
<li>results: 与使用 speech-only 预训练模型 fine-tuned 10 倍更多数据相比，我们的提议方法可以在 sentiment analysis 和 named entity recognition 等任务中达到相似的性能，只需要 1 小时的标注语音数据Here’s the full text in Traditional Chinese:</li>
<li>for: 这paper是为了解决 spoken language understanding 任务中的数据有限性问题</li>
<li>methods: 我们使用共享表示空间的 speech-text 模型，并将文本模型转移到语音测试数据上</li>
<li>results: 与使用 speech-only 预训练模型 fine-tuned 10 倍更多数据相比，我们的提议方法可以在 sentiment analysis 和 named entity recognition 等任务中达到相似的性能，只需要 1 小时的标注语音数据<details>
<summary>Abstract</summary>
Recent work on speech representation models jointly pre-trained with text has demonstrated the potential of improving speech representations by encoding speech and text in a shared space. In this paper, we leverage such shared representations to address the persistent challenge of limited data availability in spoken language understanding tasks. By employing a pre-trained speech-text model, we find that models fine-tuned on text can be effectively transferred to speech testing data. With as little as 1 hour of labeled speech data, our proposed approach achieves comparable performance on spoken language understanding tasks (specifically, sentiment analysis and named entity recognition) when compared to previous methods using speech-only pre-trained models fine-tuned on 10 times more data. Beyond the proof-of-concept study, we also analyze the latent representations. We find that the bottom layers of speech-text models are largely task-agnostic and align speech and text representations into a shared space, while the top layers are more task-specific.
</details>
<details>
<summary>摘要</summary>
近期关于语音表示模型同时预训练文本的工作表明了改进语音表示的潜在可能性。在这篇论文中，我们利用这些共享表示来解决语音理解任务中的数据有限问题。我们使用预训练的语音文本模型，发现可以将文本预训练模型转移到语音测试数据上，只需要1小时的标注语音数据。与之前使用语音只预训练模型 fine-tune 10倍更多数据的方法相比，我们的提议方法可以在语音理解任务（具体来说是情感分析和命名实体识别）中达到相同的性能水平。此外，我们还分析了隐藏表示。我们发现语音文本模型的下层主要是无关任务的，可以将语音和文本表示同化到共享空间，而顶层则更加具体地关注特定任务。
</details></li>
</ul>
<hr>
<h2 id="NEFTune-Noisy-Embeddings-Improve-Instruction-Finetuning"><a href="#NEFTune-Noisy-Embeddings-Improve-Instruction-Finetuning" class="headerlink" title="NEFTune: Noisy Embeddings Improve Instruction Finetuning"></a>NEFTune: Noisy Embeddings Improve Instruction Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05914">http://arxiv.org/abs/2310.05914</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neelsjain/neftune">https://github.com/neelsjain/neftune</a></li>
<li>paper_authors: Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein</li>
<li>for: 提高语言模型finetuning的性能</li>
<li>methods: 使用随机噪声添加到嵌入向量中 during 训练</li>
<li>results: 1. 使用噪声 embedding 的模型在 AlpacaEval 上的分数从 29.79% 提高到 64.69%；2. 在现代 instrucion  datasets 上超过强基elines，包括 Evol-Instruct、ShareGPT 和 OpenPlatypus 上的提高约10%。<details>
<summary>Abstract</summary>
We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune.
</details>
<details>
<summary>摘要</summary>
我们显示了语言模型调整可以得到改善，有时会很大，只需使用简单的增强技巧。NEFTune在训练过程中将嵌入向量添加随机变化。通过使用Alpaca，标准调整LLaMA-2-7B的性能为29.79%，将提高到64.69%。NEFTune也超过了现代指令集 dataset 的强基elines。使用 Evol-Instruct 的模型进行调整会增加10%的性能，使用 ShareGPT 的模型进行调整会增加8%的性能，使用 OpenPlatypus 的模型进行调整会增加8%的性能。甚至是使用 RLHF 进一步调整的强大模型，如 LLama-2-Chat，也会受益于额外的 NEFTune 训练。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Chest-X-Ray-Report-Generation-from-Longitudinal-Representations"><a href="#Controllable-Chest-X-Ray-Report-Generation-from-Longitudinal-Representations" class="headerlink" title="Controllable Chest X-Ray Report Generation from Longitudinal Representations"></a>Controllable Chest X-Ray Report Generation from Longitudinal Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05881">http://arxiv.org/abs/2310.05881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni, Jeffrey Dalton, Alison Q O’Neil</li>
<li>for: 这篇论文的目的是提高医疗影像报告的速度和准确性，并且提供可控的报告生成模型。</li>
<li>methods: 本文提出了两个新方法：首先，使用 longitudinal representation learning 方法，将先前的医疗影像作为额外输入，将现有和先前的视觉信息联合和融合为一个共同 longitudinal 表现，以便给 multimodal 报告生成模型；其次，使用 sentence-anatomy dropout 训练策略，让报告生成模型在预测报告内容时仅预测和corrsponding的句子和体位。</li>
<li>results: 经过对 MIMIC-CXR 资料集的严格实验，本文的方法能够实现现有最佳的结果，同时具备可控的报告生成能力。<details>
<summary>Abstract</summary>
Radiology reports are detailed text descriptions of the content of medical scans. Each report describes the presence/absence and location of relevant clinical findings, commonly including comparison with prior exams of the same patient to describe how they evolved. Radiology reporting is a time-consuming process, and scan results are often subject to delays. One strategy to speed up reporting is to integrate automated reporting systems, however clinical deployment requires high accuracy and interpretability. Previous approaches to automated radiology reporting generally do not provide the prior study as input, precluding comparison which is required for clinical accuracy in some types of scans, and offer only unreliable methods of interpretability. Therefore, leveraging an existing visual input format of anatomical tokens, we introduce two novel aspects: (1) longitudinal representation learning -- we input the prior scan as an additional input, proposing a method to align, concatenate and fuse the current and prior visual information into a joint longitudinal representation which can be provided to the multimodal report generation model; (2) sentence-anatomy dropout -- a training strategy for controllability in which the report generator model is trained to predict only sentences from the original report which correspond to the subset of anatomical regions given as input. We show through in-depth experiments on the MIMIC-CXR dataset how the proposed approach achieves state-of-the-art results while enabling anatomy-wise controllable report generation.
</details>
<details>
<summary>摘要</summary>
医学成像报告是详细的文本描述医学扫描结果。每份报告都描述了病人的相关临床发现，并常常包括与当前扫描相比较以描述它们是如何发展的。医学报告是一项时间消耗的过程，扫描结果经常会受到延迟。为了加速报告，可以 integrate 自动报告系统，但是临床部署需要高度准确和可解释性。先前的自动医学报告方法通常不提供先前扫描作为输入，因此无法进行相关的比较，这会导致报告不准确。此外，这些方法的可解释性也不够。因此，我们利用现有的解剖学输入格式，引入两个新的方面：1.  longitudinal representation learning——我们将先前扫描作为额外输入，提议一种方法来对当前和先前的视觉信息进行对接、拼接和融合，以生成共同的长期表示，这个表示可以被传给多模态报告生成模型。2. sentence-anatomy dropout——一种训练策略，用于控制报告生成模型的可控性。在训练过程中，报告生成模型需要预测来自原始报告的具体句子，其中句子的选择取决于提供的解剖学区域输入。我们通过对 MIMIC-CXR 数据集进行深入的实验，证明我们的方法可以达到当前领导的结果，同时允许解剖学 wise 可控报告生成。
</details></li>
</ul>
<hr>
<h2 id="Are-Large-Language-Models-Geospatially-Knowledgeable"><a href="#Are-Large-Language-Models-Geospatially-Knowledgeable" class="headerlink" title="Are Large Language Models Geospatially Knowledgeable?"></a>Are Large Language Models Geospatially Knowledgeable?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13002">http://arxiv.org/abs/2310.13002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prabin Bhandari, Antonios Anastasopoulos, Dieter Pfoser</li>
<li>for:  investigate the extent of geospatial knowledge and reasoning abilities in pre-trained Large Language Models (LLMs)</li>
<li>methods: probe LLMs for geo-coordinates, use geospatial and non-geospatial prepositions to gauge geospatial awareness, and utilize a multidimensional scaling (MDS) experiment to assess geospatial reasoning capabilities</li>
<li>results: larger and more sophisticated LLMs can synthesize geospatial knowledge from textual information, but there are limitations to their geospatial abilities<details>
<summary>Abstract</summary>
Despite the impressive performance of Large Language Models (LLM) for various natural language processing tasks, little is known about their comprehension of geographic data and related ability to facilitate informed geospatial decision-making. This paper investigates the extent of geospatial knowledge, awareness, and reasoning abilities encoded within such pretrained LLMs. With a focus on autoregressive language models, we devise experimental approaches related to (i) probing LLMs for geo-coordinates to assess geospatial knowledge, (ii) using geospatial and non-geospatial prepositions to gauge their geospatial awareness, and (iii) utilizing a multidimensional scaling (MDS) experiment to assess the models' geospatial reasoning capabilities and to determine locations of cities based on prompting. Our results confirm that it does not only take larger, but also more sophisticated LLMs to synthesize geospatial knowledge from textual information. As such, this research contributes to understanding the potential and limitations of LLMs in dealing with geospatial information.
</details>
<details>
<summary>摘要</summary>
尽管大型自然语言模型（LLM）在不同的自然语言处理任务上表现出色，但对于地理数据的理解和有关能力却得到了少量的研究。这篇论文探究了抽象语言模型中地理知识、意识和逻辑能力的程度。我们专注于autoregressive语言模型，并设计了以下三种实验方法：1. 使用地理坐标来评估语言模型的地理知识。2. 使用地理和非地理前置词来评估语言模型的地理意识。3. 使用多维度规范（MDS）实验来评估模型的地理逻辑能力，并确定文本中提到的城市的位置。我们的结果表明，不仅要有更大的语言模型，也需要更加复杂的语言模型才能从文本信息中 sinthezize 地理知识。因此，这项研究对于理解LLM在处理地理信息的可能性和局限性做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Terminology-Aware-Translation-with-Constrained-Decoding-and-Large-Language-Model-Prompting"><a href="#Terminology-Aware-Translation-with-Constrained-Decoding-and-Large-Language-Model-Prompting" class="headerlink" title="Terminology-Aware Translation with Constrained Decoding and Large Language Model Prompting"></a>Terminology-Aware Translation with Constrained Decoding and Large Language Model Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05824">http://arxiv.org/abs/2310.05824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolay Bogoychev, Pinzhen Chen</li>
<li>for: 提高机器翻译下游应用中精度的重要性，并且一种常见的方式是在翻译系统中注入 terminate 约束。</li>
<li>methods: 我们在 WMT 2023 翻译任务中采用了 translate-then-refine approach，这种方法不受领域限制，并且需要最小的手动努力。我们首先使用 word alignment 获取 pseudo-terminology 翻译，然后使用这些翻译来训练一个 terminology-aware 模型。此外，我们还 explore 了两种后处理方法。</li>
<li>results: 我们的 experiment 表明，我们的 terminology-aware 模型能够有效地 incorporate 精度，而使用大语言模型进行 refine 过程可以进一步提高 terminate 记忆。<details>
<summary>Abstract</summary>
Terminology correctness is important in the downstream application of machine translation, and a prevalent way to ensure this is to inject terminology constraints into a translation system. In our submission to the WMT 2023 terminology translation task, we adopt a translate-then-refine approach which can be domain-independent and requires minimal manual efforts. We annotate random source words with pseudo-terminology translations obtained from word alignment to first train a terminology-aware model. Further, we explore two post-processing methods. First, we use an alignment process to discover whether a terminology constraint has been violated, and if so, we re-decode with the violating word negatively constrained. Alternatively, we leverage a large language model to refine a hypothesis by providing it with terminology constraints. Results show that our terminology-aware model learns to incorporate terminologies effectively, and the large language model refinement process can further improve terminology recall.
</details>
<details>
<summary>摘要</summary>
<<SYS>>翻译精度在机器翻译下游应用中非常重要，一种常见的方法是将翻译系统中的术语约束注入到翻译系统中。在我们对WMT 2023翻译任务提交中，我们采用了一种翻译后修改的方法，这种方法不依赖于域名和需要 minimal的手动努力。我们首先使用word alignment获取pseudo-术语翻译，然后使用这些翻译来训练一个术语意识Model。此外，我们还探索了两种后处理方法。首先，我们使用一个对应过程来检查翻译是否违反了术语约束，如果违反了，那么我们会使用违反的单词做为约束来重新解码。其次，我们利用一个大型自然语言模型来修改一个假设，并提供术语约束来进一步提高术语回忆率。结果显示，我们的术语意识Model有效地收录了术语，而大型自然语言模型的修改过程可以进一步提高术语回忆率。
</details></li>
</ul>
<hr>
<h2 id="SC-Safety-A-Multi-round-Open-ended-Question-Adversarial-Safety-Benchmark-for-Large-Language-Models-in-Chinese"><a href="#SC-Safety-A-Multi-round-Open-ended-Question-Adversarial-Safety-Benchmark-for-Large-Language-Models-in-Chinese" class="headerlink" title="SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese"></a>SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05818">http://arxiv.org/abs/2310.05818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue</li>
<li>For: The paper aims to systematically assess the safety of Chinese large language models (LLMs) and provide a benchmark for creating safer and more trustworthy models.* Methods: The paper introduces a multi-round adversarial benchmark called SuperCLUE-Safety (SC-Safety) that includes 4912 open-ended questions covering 20 safety sub-dimensions. The benchmark involves human-model interactions and conversations to increase the challenges.* Results: The paper finds that closed-source models perform better in terms of safety compared to open-source models, and models released from China demonstrate comparable safety levels to LLMs like GPT-3.5-turbo. Smaller models with 6B-13B parameters can also compete effectively in terms of safety. The findings provide guidance on model selection and promote collaborative efforts to create safer LLMs.<details>
<summary>Abstract</summary>
Large language models (LLMs), like ChatGPT and GPT-4, have demonstrated remarkable abilities in natural language understanding and generation. However, alongside their positive impact on our daily tasks, they can also produce harmful content that negatively affects societal perceptions. To systematically assess the safety of Chinese LLMs, we introduce SuperCLUE-Safety (SC-Safety) - a multi-round adversarial benchmark with 4912 open-ended questions covering more than 20 safety sub-dimensions. Adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods. Experiments on 13 major LLMs supporting Chinese yield the following insights: 1) Closed-source models outperform open-sourced ones in terms of safety; 2) Models released from China demonstrate comparable safety levels to LLMs like GPT-3.5-turbo; 3) Some smaller models with 6B-13B parameters can compete effectively in terms of safety. By introducing SC-Safety, we aim to promote collaborative efforts to create safer and more trustworthy LLMs. The benchmark and findings provide guidance on model selection. Our benchmark can be found at https://www.CLUEbenchmarks.com
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM），如ChatGPT和GPT-4，已经表现出了惊人的自然语言理解和生成能力。然而，同时也可能生成有害内容，影响社会观念。为了系统地评估中文 LLM 的安全性，我们介绍了 SuperCLUE-Safety（SC-Safety）多轮对抗性测试框架，包括4912个开放式问题，覆盖超过20个安全子维度。对人机模型交互和对话的挑战性提高了现有方法的挑战性。对13种主要支持中文的 LLM 进行了实验，得到以下发现：1）关闭源代码模型在安全性方面表现更高；2）中国发布的模型与 GPT-3.5-turbo 的安全水平相当；3）一些6B-13B参数的小型模型可以有效竞争在安全性方面。通过引入 SC-Safety，我们希望促进开源 LLM 的创造和可信worthiness。我们的测试框架可以在https://www.CLUEbenchmarks.com找到。
</details></li>
</ul>
<hr>
<h2 id="DiffuSeq-v2-Bridging-Discrete-and-Continuous-Text-Spaces-for-Accelerated-Seq2Seq-Diffusion-Models"><a href="#DiffuSeq-v2-Bridging-Discrete-and-Continuous-Text-Spaces-for-Accelerated-Seq2Seq-Diffusion-Models" class="headerlink" title="DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models"></a>DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05793">http://arxiv.org/abs/2310.05793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Shark-NLP/DiffuSeq">https://github.com/Shark-NLP/DiffuSeq</a></li>
<li>paper_authors: Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong</li>
<li>for: 提高Diffusion模型的训练速度和采样速度，以便于实际应用。</li>
<li>methods: 引入软吸收状态，使Diffusion模型能够在连续Diffusion空间中学习恢复分子突变，并使用现有的ODE解ulle器在连续空间中加速采样过程。</li>
<li>results: 实验结果表明，提出的方法可以提高训练速度4倍，并在800倍的采样速度下生成同质的样本，使其更加符合实际应用。<details>
<summary>Abstract</summary>
Diffusion models have gained prominence in generating high-quality sequences of text. Nevertheless, current approaches predominantly represent discrete text within a continuous diffusion space, which incurs substantial computational overhead during training and results in slower sampling speeds. In this paper, we introduce a soft absorbing state that facilitates the diffusion model in learning to reconstruct discrete mutations based on the underlying Gaussian space, thereby enhancing its capacity to recover conditional signals. During the sampling phase, we employ state-of-the-art ODE solvers within the continuous space to expedite the sampling process. Comprehensive experimental evaluations reveal that our proposed method effectively accelerates the training convergence by 4x and generates samples of similar quality 800x faster, rendering it significantly closer to practical application. \footnote{The code is released at \url{https://github.com/Shark-NLP/DiffuSeq}
</details>
<details>
<summary>摘要</summary>
Diffusion models 已经在生成高质量文本序列方面得到广泛应用。然而，当前的方法主要将整数文本 Represented as a continuous diffusion space中的一部分，这会导致训练过程中的计算开销很大，以及采样速度较慢。在这篇论文中，我们引入了软吸收状态，使得扩散模型能够学习根据下面的 Gaussian space 中的精度 Mutations 进行重建，从而提高其对 conditional signals 的恢复能力。在采样阶段，我们使用 state-of-the-art ODE solvers 在连续空间中进行采样，以便加速采样过程。经过了广泛的实验评估，我们的提议方法可以在训练速度和样本质量两个方面提高效率， specifically 4x 快速 Train convergence 和 800x faster sample generation，使其更加接近实际应用。Note: The URL in the footnote has been translated as well: \url{https://github.com/Shark-NLP/DiffuSeq} becomes \url{https://github.com/Shark-NLP/DiffuSeq}
</details></li>
</ul>
<hr>
<h2 id="Problem-Solving-Guide-Predicting-the-Algorithm-Tags-and-Difficulty-for-Competitive-Programming-Problems"><a href="#Problem-Solving-Guide-Predicting-the-Algorithm-Tags-and-Difficulty-for-Competitive-Programming-Problems" class="headerlink" title="Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for Competitive Programming Problems"></a>Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for Competitive Programming Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05791">http://arxiv.org/abs/2310.05791</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sronger/psg_predicting_algorithm_tags_and_difficulty">https://github.com/sronger/psg_predicting_algorithm_tags_and_difficulty</a></li>
<li>paper_authors: Juntae Kim, Eunjung Cho, Dongwoo Kim, Dongbin Na</li>
<li>For: This paper aims to help engineers and developers solve algorithm problems more efficiently by predicting the algorithm tag and difficulty level of a problem.* Methods: The authors propose a deep learning-based method for simultaneously predicting algorithm tags and difficulty levels of an algorithm problem given.* Results: The authors present a real-world algorithm problem multi-task dataset, AMT, which is the most large-scale dataset for predicting algorithm tags compared to previous studies. They also show that their proposed method can accurately predict algorithm tags and difficulty levels.<details>
<summary>Abstract</summary>
The recent program development industries have required problem-solving abilities for engineers, especially application developers. However, AI-based education systems to help solve computer algorithm problems have not yet attracted attention, while most big tech companies require the ability to solve algorithm problems including Google, Meta, and Amazon. The most useful guide to solving algorithm problems might be guessing the category (tag) of the facing problems. Therefore, our study addresses the task of predicting the algorithm tag as a useful tool for engineers and developers. Moreover, we also consider predicting the difficulty levels of algorithm problems, which can be used as useful guidance to calculate the required time to solve that problem. In this paper, we present a real-world algorithm problem multi-task dataset, AMT, by mainly collecting problem samples from the most famous and large competitive programming website Codeforces. To the best of our knowledge, our proposed dataset is the most large-scale dataset for predicting algorithm tags compared to previous studies. Moreover, our work is the first to address predicting the difficulty levels of algorithm problems. We present a deep learning-based novel method for simultaneously predicting algorithm tags and the difficulty levels of an algorithm problem given. All datasets and source codes are available at https://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty.
</details>
<details>
<summary>摘要</summary>
现代软件开发行业强调解决问题能力，特别是应用程序开发人员。然而，基于人工智能的教育系统用于解决计算机算法问题尚未吸引到关注，而大多数大型科技公司都需要解决算法问题，包括Google、Meta和Amazon。解决算法问题的最有用指南可能是猜测问题的类别（标签）。因此，我们的研究挑战是预测算法标签作为工程师和开发人员的有用工具。此外，我们还考虑预测算法问题的困难程度，可以作为有用的导航来计算解决该问题所需的时间。在本文中，我们发布了一个实际世界上最大规模的算法问题多任务 dataset，即 AMT，通过主要收集来自最著名和最大竞赛编程网站 Codeforces 的问题样本。根据我们所知，我们提出的 dataset 是预测算法标签方面最大规模的比前一些研究。此外，我们的工作是第一次Addressing 预测算法问题的困难程度。我们提出了一种深度学习基于的新方法，可同时预测算法标签和问题的困难程度。所有数据和源代码都可以在 GitHub 上获取，请参考 <https://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty>。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Language-Models-with-Human-Preferences-via-a-Bayesian-Approach"><a href="#Aligning-Language-Models-with-Human-Preferences-via-a-Bayesian-Approach" class="headerlink" title="Aligning Language Models with Human Preferences via a Bayesian Approach"></a>Aligning Language Models with Human Preferences via a Bayesian Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05782">http://arxiv.org/abs/2310.05782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangjs9/aligned-dpm">https://github.com/wangjs9/aligned-dpm</a></li>
<li>paper_authors: Jiashuo Wang, Haozhao Wang, Shichao Sun, Wenjie Li</li>
<li>for: This paper aims to advance human-centric natural language generation (NLG) systems by ensuring alignment between NLG models and human preferences.</li>
<li>methods: The proposed method uses a Bayesian framework to account for the distribution of disagreements among human preferences in training a preference model, and utilizes contrastive learning to train the NLG model with the preference scores.</li>
<li>results: The proposed method consistently exceeds previous state-of-the-art (SOTA) models in both automatic and human evaluations on two human-centric NLG tasks, i.e., emotional support conversation and integrity “Rule-of-Thumb” generation.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提高人类中心的自然语言生成系统，确保NLG模型和人类偏好的对应。</li>
<li>methods: 提议方法使用 bayesian 框架来考虑人类偏好的分布不一致，在偏好模型训练中使用对比学习训练 NLG 模型。</li>
<li>results: 提议方法在两个人类中心 NLG 任务（情感支持对话和规则杆准则生成）的自动和人类评估中一直超越过去的 SOTA 模型。<details>
<summary>Abstract</summary>
In the quest to advance human-centric natural language generation (NLG) systems, ensuring alignment between NLG models and human preferences is crucial. For this alignment, current popular methods leverage a reinforcement learning (RL) approach with a reward model trained on feedback from humans. However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance. To tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. Although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. To address this challenge, this paper proposes a novel approach, which employs a Bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as d-PM. Besides, considering the RL strategy's inefficient and complex training process over the training efficiency, we further propose utilizing the contrastive learning strategy to train the NLG model with the preference scores derived from the d-PM model. Extensive experiments on two human-centric NLG tasks, i.e., emotional support conversation and integrity "Rule-of-Thumb" generation, show that our method consistently exceeds previous SOTA models in both automatic and human evaluations.
</details>
<details>
<summary>摘要</summary>
在增进人类中心的自然语言生成（NLG）系统方面，与人类偏好的吻合是关键。现有的流行方法通常采用了强化学习（RL）方法，并在人类反馈中训练一个奖励模型。然而，人类偏好的内在分歧对RL方法的训练带来了很大挑战，导致NLG性能下降。为解决这个问题，之前的方法通常采用了多数投票或平均值来整合多个不一致的偏好，但这些方法容易受到人类偏好的分歧的限制，并且只能表征特定人群，无法量化透过表达人类偏好的多样性。为此，本文提出了一种新的方法，即使用 Bayesian 框架来考虑人类偏好的分歧分布，并称之为 d-PM。此外，由于RL策略的训练过程复杂且不效率，我们进一步提议使用对比学习策略来训练NLG模型，使用 d-PM 模型生成的偏好分数。经验表明，我们的方法在两个人类中心NLG任务上（即情感支持对话和规则精神生成）都能够连续超越过去的最佳模型，并在自动和人类评估中表现出色。
</details></li>
</ul>
<hr>
<h2 id="LLMLingua-Compressing-Prompts-for-Accelerated-Inference-of-Large-Language-Models"><a href="#LLMLingua-Compressing-Prompts-for-Accelerated-Inference-of-Large-Language-Models" class="headerlink" title="LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"></a>LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05736">http://arxiv.org/abs/2310.05736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/LLMLingua">https://github.com/microsoft/LLMLingua</a></li>
<li>paper_authors: Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu</li>
<li>for: 这个研究旨在提高语言模型（LLM）的推导速度和降低成本，以便应用在各种应用中。</li>
<li>methods: 本研究使用了一种名为 LLMLingua 的弹性推导方法，包括一个预算控制器来维持semantic integrity，一个iterative compression algorithm来更好地模型压缩内容之间的依赖性，以及一种基于 instruction tuning的方法来对语言模型进行分布对齐。</li>
<li>results: 本研究在四个不同的数据集（GSM8K、BBH、ShareGPT和Arxiv-March23）上进行了实验和分析，结果显示 LLMLingua 方法可以实现现在的性能水准，并且可以实现高比例的压缩（Up to 20x），而且具有 littles 的性能损失。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Emotion-Based-Synthetic-Consciousness-Using-LLMs-to-Estimate-Emotion-Probability-Vectors"><a href="#Towards-Emotion-Based-Synthetic-Consciousness-Using-LLMs-to-Estimate-Emotion-Probability-Vectors" class="headerlink" title="Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate Emotion Probability Vectors"></a>Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate Emotion Probability Vectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10673">http://arxiv.org/abs/2310.10673</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Sinclair, Willem Pye</li>
<li>for: 这篇论文探讨了如何使用大语言模型（LLMs）来估算文本中情感状态的摘要。</li>
<li>methods: 该论文使用了大语言模型来计算文本中的情感摘要，该摘要包括情感描述词和该词在提要中出现概率。</li>
<li>results: 通过对亚马逊商品评论进行情感分析，该论文示出了情感描述词可以被映射到PCA类型空间中。然而，通过使用尾提要来引发行动来改进当前状态的方法并不是直接可行。<details>
<summary>Abstract</summary>
This paper shows how LLMs (Large Language Models) may be used to estimate a summary of the emotional state associated with piece of text. The summary of emotional state is a dictionary of words used to describe emotion together with the probability of the word appearing after a prompt comprising the original text and an emotion eliciting tail. Through emotion analysis of Amazon product reviews we demonstrate emotion descriptors can be mapped into a PCA type space. It was hoped that text descriptions of actions to improve a current text described state could also be elicited through a tail prompt. Experiment seemed to indicate that this is not straightforward to make work. This failure put our hoped for selection of action via choosing the best predict ed outcome via comparing emotional responses out of reach for the moment.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了如何使用大语言模型（LLMs）来估算一篇文本中的情感状态概述。概述的情感状态是一个词典，其中包含用于描述情感的词语以及这些词语在键入文本和情感Trigger后的概率。通过对亚马逊商品评论进行情感分析，我们示出了情感描述可以被映射到PCA类型空间。希望通过尾部提示来引发文本描述的行为改进方法，但实验表明这并不是一个简单的任务。这种失败使我们希望通过比较情感响应来选择最佳结果的方法被推迟。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Large-Language-Models-for-Healthcare-from-Data-Technology-and-Applications-to-Accountability-and-Ethics"><a href="#A-Survey-of-Large-Language-Models-for-Healthcare-from-Data-Technology-and-Applications-to-Accountability-and-Ethics" class="headerlink" title="A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"></a>A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05694">http://arxiv.org/abs/2310.05694</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaihe-better/llm-for-healthcare">https://github.com/kaihe-better/llm-for-healthcare</a></li>
<li>paper_authors: Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, Erik Cambria</li>
<li>for: 这份survey旨在提供关于现有的大语言模型（LLMs）在医疗领域的开发进程和其应用前景，以及在PLMs的基础上发展出LLMs的开发路线图。</li>
<li>methods: 本文首先探讨LLMs在医疗应用中的可能性，并描述了LLMs的开发过程、训练数据、训练方法、优化策略和应用。此外，本文还对PLMs和LLMs之间进行比较，以及不同LLMs之间进行比较。</li>
<li>results: 本文总结了关于LLMs在医疗领域的开发和应用，包括关于Healthcare应用中LLMs的可能性、PLMs和LLMs之间的比较、训练数据、训练方法、优化策略和应用。此外，本文还考虑了在医疗领域部署LLMs时存在的独特问题，如公平、责任、透明度和伦理问题。<details>
<summary>Abstract</summary>
The utilization of large language models (LLMs) in the Healthcare domain has generated both excitement and concern due to their ability to effectively respond to freetext queries with certain professional knowledge. This survey outlines the capabilities of the currently developed LLMs for Healthcare and explicates their development process, with the aim of providing an overview of the development roadmap from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, as well as comparing various LLMs with each other. Then we summarize related Healthcare training data, training methods, optimization strategies, and usage. Finally, the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics. Our survey provide a comprehensive investigation from perspectives of both computer science and Healthcare specialty. Besides the discussion about Healthcare concerns, we supports the computer science community by compiling a collection of open source resources, such as accessible datasets, the latest methodologies, code implementations, and evaluation benchmarks in the Github. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a shift from model-centered methodologies to datacentered methodologies.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）在医疗领域的应用已经引起了广泛的兴趣和担忧，因为它们可以有效地回答免费文本查询，并且具有一定的专业知识。本调查概述了目前已经开发出的LLMs的能力，并详细介绍其开发过程，以提供对开发路线图的概述，从传统的预训练语言模型（PLMs）到LLMs。 specifically，我们首先探讨LLMs在各种医疗应用中的可能性，并 highlighted它们的优点和局限性。其次，我们进行了PLMs和最新的LLMs之间的比较，以及不同的LLMs之间的比较。然后，我们总结了相关的医疗训练数据、训练方法、优化策略和使用。最后，我们调查了在医疗设置中部署LLMs的独特问题，特别是公平、责任、透明度和伦理。我们的调查提供了从计算机科学和医疗专业的视角的全面的调查。除了医疗问题的讨论外，我们还支持计算机科学社区，并将可 accessible datasets、最新的方法ologies、代码实现和评估标准集成到GitHub中。总之，我们认为现在正在进行一次重要的 парадиг转换，从PLMs到LLMs。这种转换包括从推理AI方法到生成AI方法，以及从模型中心的方法ологи到数据中心的方法ологи。
</details></li>
</ul>
<hr>
<h2 id="Larth-Dataset-and-Machine-Translation-for-Etruscan"><a href="#Larth-Dataset-and-Machine-Translation-for-Etruscan" class="headerlink" title="Larth: Dataset and Machine Translation for Etruscan"></a>Larth: Dataset and Machine Translation for Etruscan</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05688">http://arxiv.org/abs/2310.05688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gianlucavico/larth-etruscan-nlp">https://github.com/gianlucavico/larth-etruscan-nlp</a></li>
<li>paper_authors: Gianluca Vico, Gerasimos Spanakis</li>
<li>for: 这个论文的目的是提供一个从 Et 语言到英语的机器翻译数据集，以便未来的研究人员可以利用这些数据进行语言处理研究。</li>
<li>methods: 这个论文使用了一些自动和手动提取的翻译示例，并使用了不同的机器翻译模型进行评估。</li>
<li>results: 论文的结果表明，使用一个小型转换器模型可以实现 BLEU 分数为 10.1。这些结果可以帮助未来的研究人员在这种语言和其他具有有限资源的语言中进行更好的语言处理研究。<details>
<summary>Abstract</summary>
Etruscan is an ancient language spoken in Italy from the 7th century BC to the 1st century AD. There are no native speakers of the language at the present day, and its resources are scarce, as there exist only around 12,000 known inscriptions. To the best of our knowledge, there are no publicly available Etruscan corpora for natural language processing. Therefore, we propose a dataset for machine translation from Etruscan to English, which contains 2891 translated examples from existing academic sources. Some examples are extracted manually, while others are acquired in an automatic way. Along with the dataset, we benchmark different machine translation models observing that it is possible to achieve a BLEU score of 10.1 with a small transformer model. Releasing the dataset can help enable future research on this language, similar languages or other languages with scarce resources.
</details>
<details>
<summary>摘要</summary>
eti：一种古代的意大利语言，自公元7世纪起至公元1世纪止使用。目前无任何 native speaker，资源稀缺，只有约12,000件 known inscriptions。根据我们所知，没有公开available Etruscan corpora for natural language processing。因此，我们提出了一个从Etruscan到英语的机器翻译数据集，包含2891个翻译例子，其中一些是手动提取的，而另外一些是自动获取的。同时，我们对不同的机器翻译模型进行了benchmarking，发现可以达到10.1的BLEU分数，使用一个小型transformer模型。释放这个数据集可以帮助未来的研究这种语言、相似语言或其他语言with scarce resources。
</details></li>
</ul>
<hr>
<h2 id="A-Closer-Look-into-Automatic-Evaluation-Using-Large-Language-Models"><a href="#A-Closer-Look-into-Automatic-Evaluation-Using-Large-Language-Models" class="headerlink" title="A Closer Look into Automatic Evaluation Using Large Language Models"></a>A Closer Look into Automatic Evaluation Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05657">http://arxiv.org/abs/2310.05657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/d223302/a-closer-look-to-llm-evaluation">https://github.com/d223302/a-closer-look-to-llm-evaluation</a></li>
<li>paper_authors: Cheng-Han Chiang, Hung-yi Lee</li>
<li>for: This paper aims to evaluate the effectiveness of using large language models (LLMs) for text quality evaluation, and to compare different evaluation methods.</li>
<li>methods: The paper uses two existing methods, LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et al., 2023), and analyzes their strengths and weaknesses in terms of correlating with human ratings.</li>
<li>results: The paper finds that the auto Chain-of-Thought (CoT) used in G-Eval does not always improve correlation with human ratings, and that forcing the LLM to output only a numeric rating is suboptimal. Additionally, the paper shows that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings, and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是评估大语言模型（LLM）用于文本质量评估的有效性，并比较不同评估方法。</li>
<li>methods: 论文使用了两种现有的方法，LLM评估（Chiang和Lee，2023）和G-Eval（Liu等，2023），并分析它们在与人类评分相关性方面的优缺点。</li>
<li>results: 论文发现，G-Eval中的自动链条（CoT）并不总是提高与人类评分的相关性，而强制 LLM 输出只有数字评分也是不优化的。此外，论文还显示，让 LLM 解释自己的评分能够一直提高 ChatGPT 和人类评分之间的相关性，并达到状态之狮（SoTA）相关性水平在两个 meta-评估数据集上。<details>
<summary>Abstract</summary>
Using large language models (LLMs) to evaluate text quality has recently gained popularity. Some prior works explore the idea of using LLMs for evaluation, while they differ in some details of the evaluation process. In this paper, we analyze LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et al., 2023), and we discuss how those details in the evaluation process change how well the ratings given by LLMs correlate with human ratings. We find that the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more aligned with human ratings. We also show that forcing the LLM to output only a numeric rating, as in G-Eval, is suboptimal. Last, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.
</details>
<details>
<summary>摘要</summary>
使用大语言模型（LLM）评估文本质量已经很受欢迎。一些先前的工作探讨了使用LLM进行评估的想法，但它们在评估过程中有一些细节的不同。本文分析了LLM评估（Chiang和Lee，2023）和G-Eval（Liu等，2023），并分析了评估过程中的细节如何影响LLM给出的评分与人类评分之间的相关性。我们发现自动链条（CoT）使用在G-Eval中并不总是使G-Eval更加与人类评分相关。此外，我们还示出了让LLM输出只有数值评分，如G-Eval中所做的，是不优化的。最后，我们发现让LLM解释自己的评分能够一直保持和人类评分的相关性，并Push state-of-the-art（SoTA）相关性在两个meta-评估数据集上。
</details></li>
</ul>
<hr>
<h2 id="RAUCG-Retrieval-Augmented-Unsupervised-Counter-Narrative-Generation-for-Hate-Speech"><a href="#RAUCG-Retrieval-Augmented-Unsupervised-Counter-Narrative-Generation-for-Hate-Speech" class="headerlink" title="RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for Hate Speech"></a>RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for Hate Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05650">http://arxiv.org/abs/2310.05650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyu Jiang, Wenyi Tang, Xingshu Chen, Rui Tanga, Haizhou Wang, Wenxian Wang</li>
<li>for: 本研究旨在提出一种自动生成Counter Narrative（CN）的方法，以便在互联网上防止仇恨言论（HS）无需削弱言论自由。</li>
<li>methods: 本研究使用自然语言生成技术来自动生成CN，并提出了一种 Retrieval-Augmented Unsupervised Counter Narrative Generation（RAUCG）方法。RAUCG方法包括SSF检索方法、能量基于扩展知识的解码机制和不间断的改进。</li>
<li>results: 实验结果表明，RAUCG方法在语言质量、毒害性、诱导力、相关性和HS防止成功率等方面都有显著改进，与强基eline相比，RAUCG方法在所有指标上都有提高。此外，RAUCG方法使得GPT2超过T0在所有指标上表现。<details>
<summary>Abstract</summary>
The Counter Narrative (CN) is a promising approach to combat online hate speech (HS) without infringing on freedom of speech. In recent years, there has been a growing interest in automatically generating CNs using natural language generation techniques. However, current automatic CN generation methods mainly rely on expert-authored datasets for training, which are time-consuming and labor-intensive to acquire. Furthermore, these methods cannot directly obtain and extend counter-knowledge from external statistics, facts, or examples. To address these limitations, we propose Retrieval-Augmented Unsupervised Counter Narrative Generation (RAUCG) to automatically expand external counter-knowledge and map it into CNs in an unsupervised paradigm. Specifically, we first introduce an SSF retrieval method to retrieve counter-knowledge from the multiple perspectives of stance consistency, semantic overlap rate, and fitness for HS. Then we design an energy-based decoding mechanism by quantizing knowledge injection, countering and fluency constraints into differentiable functions, to enable the model to build mappings from counter-knowledge to CNs without expert-authored CN data. Lastly, we comprehensively evaluate model performance in terms of language quality, toxicity, persuasiveness, relevance, and success rate of countering HS, etc. Experimental results show that RAUCG outperforms strong baselines on all metrics and exhibits stronger generalization capabilities, achieving significant improvements of +2.0% in relevance and +4.5% in success rate of countering metrics. Moreover, RAUCG enabled GPT2 to outperform T0 in all metrics, despite the latter being approximately eight times larger than the former. Warning: This paper may contain offensive or upsetting content!
</details>
<details>
<summary>摘要</summary>
“Counter Narrative（CN）是一种有前途的方法来防止网络诽谤（HS）无须对自由表达造成限制。过去几年，有关自动生成CN的研究愈来愈受到关注。然而，目前的自动CN生成方法主要靠专家撰写的数据进行训练，这是时间consuming和劳动密集的。另外，这些方法不能直接从外部获取和扩展反知识。为了解决这些限制，我们提出了内部扩展无监控Counter Narrative生成（RAUCG），以自动扩展外部反知识并将其映射到CN中。具体来说，我们首先引入SSF搜寻方法，从多种看法的立场一致性、semantic overlap rate和HS适用度等方面搜寻反知识。然后，我们设计了能量基的解oding机制，将知识注入、反驳和流利性的约束转化为可微分函数，让模型从反知识中建立CN映射，不需要专家撰写CN数据。最后，我们对模型表现进行了全面评估，包括语言质量、毒性、说服力、相关性和防止HS成功率等。实验结果显示，RAUCG在所有指标上表现出色，与强基eline进行比较，RAUCG在所有指标上具有+2.0%的相关性和+4.5%的防止HS成功率的改进。此外，RAUCG使得GPT2超越T0，即使T0比GPT2大约8倍。警告：本文可能含有刺激或尴尬的内容！”
</details></li>
</ul>
<hr>
<h2 id="Towards-Verifiable-Generation-A-Benchmark-for-Knowledge-aware-Language-Model-Attribution"><a href="#Towards-Verifiable-Generation-A-Benchmark-for-Knowledge-aware-Language-Model-Attribution" class="headerlink" title="Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution"></a>Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05634">http://arxiv.org/abs/2310.05634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinze Li, Yixin Cao2, Liangming Pan, Yubo Ma, Aixin Sun</li>
<li>for: 提高大语言模型（LLMs）的可靠性和准确性，并解决 LLMS 的三大核心问题。</li>
<li>methods: 利用知识图（KG）扩展 attrribution 源，并提出“自我不足”设定，考虑模型需要的知识不足。提出了一个全面的自动评估指标，覆盖文本质量、引用质量和文本引用对齐。</li>
<li>results: 通过构建生物领域数据集 BioKaLMA，并开发基线解决方案，显示 LLMS 在引用生成方面存在大量的改进空间，强调需要包含“自我不足”设定，以及重要性取得高度准确的检索精度。<details>
<summary>Abstract</summary>
Although achieving great success, Large Language Models (LLMs) usually suffer from unreliable hallucinations. In this paper, we define a new task of Knowledge-aware Language Model Attribution (KaLMA) that improves upon three core concerns on conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new ``Conscious Incompetence" setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation quality, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via a well-designed evolutionary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs' citation generation, emphasizing the importance of incorporating the "Conscious Incompetence" setting, and the critical role of retrieval accuracy.
</details>
<details>
<summary>摘要</summary>
尽管大成功的大语言模型（LLM）通常受到不可靠的幻觉的影响，在这篇论文中，我们定义了一个新的知识感知语言模型归因（KaLMA）任务，以改进传统归因语言模型中的三大核心问题。首先，我们将归因源从文本扩展到知识图（KG）， whose rich structures benefit both the attribution performance and working scenarios。其次，我们提出了一种“意识不足”的设定，考虑到知识库的缺失，使模型可以识别需要支持知识的情况。最后，我们提出了一个全面的自动评估指标，涵盖文本质量、引用质量和文本引用对齐。为实现以上创新，我们建立了一个在生传记领域的 BioKaLMA 数据集，通过一种Well-designed演化问题生成策略来控制问题复杂性和需要的知识。为评估，我们开发了一个基线解决方案，并示出了 LLMs 的引用生成中的改进空间，强调了在“意识不足”设定下的重要性，以及检索准确性的关键性。
</details></li>
</ul>
<hr>
<h2 id="Glitter-or-Gold-Deriving-Structured-Insights-from-Sustainability-Reports-via-Large-Language-Models"><a href="#Glitter-or-Gold-Deriving-Structured-Insights-from-Sustainability-Reports-via-Large-Language-Models" class="headerlink" title="Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models"></a>Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05628">http://arxiv.org/abs/2310.05628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Bronzini, Carlo Nicolini, Bruno Lepri, Andrea Passerini, Jacopo Staiano</li>
<li>for: This paper aims to provide a framework for extracting and analyzing non-financial information from sustainability reports to support investors’ ESG-related decision-making.</li>
<li>methods: The authors use Large Language Models (LLMs), Retrieved Augmented Generation, and in-context learning to extract semantically structured information from sustainability reports. They also employ graph-based representations to analyze the obtained findings.</li>
<li>results: The authors generate meaningful statistical, similarity, and correlation analyses concerning the sustainability actions undertaken across industries, sectors, and regions. They also investigate the factors that impact companies’ ESG scores using their findings and other company information.<details>
<summary>Abstract</summary>
Over the last decade, several regulatory bodies have started requiring the disclosure of non-financial information from publicly listed companies, in light of the investors' increasing attention to Environmental, Social, and Governance (ESG) issues. Such information is publicly released in a variety of non-structured and multi-modal documentation. Hence, it is not straightforward to aggregate and consolidate such data in a cohesive framework to further derive insights about sustainability practices across companies and markets. Thus, it is natural to resort to Information Extraction (IE) techniques to provide concise, informative and actionable data to the stakeholders. Moving beyond traditional text processing techniques, in this work we leverage Large Language Models (LLMs), along with prominent approaches such as Retrieved Augmented Generation and in-context learning, to extract semantically structured information from sustainability reports. We then adopt graph-based representations to generate meaningful statistical, similarity and correlation analyses concerning the obtained findings, highlighting the prominent sustainability actions undertaken across industries and discussing emerging similarity and disclosing patterns at company, sector and region levels. Lastly, we investigate which factual aspects impact the most on companies' ESG scores using our findings and other company information.
</details>
<details>
<summary>摘要</summary>
In this work, we leverage Large Language Models (LLMs) and prominent approaches such as Retrieved Augmented Generation and in-context learning to extract semantically structured information from sustainability reports. We then use graph-based representations to generate meaningful statistical, similarity, and correlation analyses of the obtained findings, highlighting the prominent sustainability actions undertaken across industries and discussing emerging similarity and disclosure patterns at company, sector, and region levels.Finally, we investigate which factual aspects have the greatest impact on companies' ESG scores using our findings and other company information.
</details></li>
</ul>
<hr>
<h2 id="Integrating-Stock-Features-and-Global-Information-via-Large-Language-Models-for-Enhanced-Stock-Return-Prediction"><a href="#Integrating-Stock-Features-and-Global-Information-via-Large-Language-Models-for-Enhanced-Stock-Return-Prediction" class="headerlink" title="Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return Prediction"></a>Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05627">http://arxiv.org/abs/2310.05627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujie Ding, Shuai Jia, Tianyi Ma, Bingcheng Mao, Xiuze Zhou, Liuliu Li, Dongming Han</li>
<li>for: 这个研究旨在将大语言模型（LLMs）如ChatGPT和GPT-4 integrating into existing quantitative investment models, in order to improve the accuracy of stock return predictions.</li>
<li>methods: 本研究提出了一个 novel framework, which consists of two components: (1) the Local-Global (LG) model, which introduces three distinct strategies for modeling global information, and (2) Self-Correlated Reinforcement Learning (SCRL), which focuses on aligning the embeddings of financial news generated by LLMs with stock features within the same semantic space.</li>
<li>results: 经过实现本研究的框架后, 在中国A股市场中的 Rank Information Coefficient和回归表现得到了提高, 特别是与仅使用股票特征的模型相比。<details>
<summary>Abstract</summary>
The remarkable achievements and rapid advancements of Large Language Models (LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in quantitative investment. Traders can effectively leverage these LLMs to analyze financial news and predict stock returns accurately. However, integrating LLMs into existing quantitative models presents two primary challenges: the insufficient utilization of semantic information embedded within LLMs and the difficulties in aligning the latent information within LLMs with pre-existing quantitative stock features. We propose a novel framework consisting of two components to surmount these challenges. The first component, the Local-Global (LG) model, introduces three distinct strategies for modeling global information. These approaches are grounded respectively on stock features, the capabilities of LLMs, and a hybrid method combining the two paradigms. The second component, Self-Correlated Reinforcement Learning (SCRL), focuses on aligning the embeddings of financial news generated by LLMs with stock features within the same semantic space. By implementing our framework, we have demonstrated superior performance in Rank Information Coefficient and returns, particularly compared to models relying only on stock features in the China A-share market.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）如ChatGPT和GPT-4的出色成就和快速进步，曝光了它们在量化投资中的巨大潜力。投资者可以充分利用这些LLM来分析金融新闻并准确预测股票回报。然而，将LLM integrate into现有的量化模型存在两个主要挑战：一是在LLM中嵌入的 semantics信息的不足利用，二是将LLM中的幽默信息与现有的量化股票特征进行对齐。我们提出了一个新的框架，包括两个组成部分：1. 本地-全局（LG）模型，这个模型引入了三种不同的全局模型策略，这些策略分别基于股票特征、LLM的能力和两种思想的混合方法。2. 自相关束力学学习（SCRL），它专注于将LLM生成的金融新闻嵌入与股票特征在同一个semantic空间进行对齐。通过实施我们的框架，我们在中国A股市场中示出了与只使用股票特征模型相比更高的rank信息系数和回报表现。
</details></li>
</ul>
<hr>
<h2 id="LAiW-A-Chinese-Legal-Large-Language-Models-Benchmark-A-Technical-Report"><a href="#LAiW-A-Chinese-Legal-Large-Language-Models-Benchmark-A-Technical-Report" class="headerlink" title="LAiW: A Chinese Legal Large Language Models Benchmark (A Technical Report)"></a>LAiW: A Chinese Legal Large Language Models Benchmark (A Technical Report)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05620">http://arxiv.org/abs/2310.05620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dai-shen/laiw">https://github.com/dai-shen/laiw</a></li>
<li>paper_authors: Yongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie, Yifang Zhang, Weiguang Han, Wei Tian, Hao Wang</li>
<li>for: 评估当前许多法律LLM的法律能力。</li>
<li>methods: 分为三级：基础法律NLP能力、基础法律应用能力和复杂法律应用能力。</li>
<li>results: 第一阶段的评估结果显示，虽有一些法律LLM的表现更好于其基础模型，但还有一定差距与ChatGPT相比。<details>
<summary>Abstract</summary>
With the emergence of numerous legal LLMs, there is currently a lack of a comprehensive benchmark for evaluating their legal abilities. In this paper, we propose the first Chinese Legal LLMs benchmark based on legal capabilities. Through the collaborative efforts of legal and artificial intelligence experts, we divide the legal capabilities of LLMs into three levels: basic legal NLP capability, basic legal application capability, and complex legal application capability. We have completed the first phase of evaluation, which mainly focuses on the capability of basic legal NLP. The evaluation results show that although some legal LLMs have better performance than their backbones, there is still a gap compared to ChatGPT. Our benchmark can be found at URL.
</details>
<details>
<summary>摘要</summary>
“现在有很多法律 LLMS 出现，但是无法确定这些 LLMS 的法律能力水平。在这篇论文中，我们提出了第一个基于法律能力的中国 LLMS benchmark。通过法律和人工智能专家的共同努力，我们将 LLMS 的法律能力分为三级：基础法律 NLP 能力、基础法律应用能力和复杂法律应用能力。我们已经完成了第一阶段的评估，主要是关于基础法律 NLP 的能力。评估结果显示，虽然一些法律 LLMS 的性能比其底层更好，但还有一定的差距与 ChatGPT 相比。我们的 benchmark 可以在 URL 上找到。”Note: "LLMS" stands for "Legal Language Models" in this context.
</details></li>
</ul>
<hr>
<h2 id="Can-language-models-learn-analogical-reasoning-Investigating-training-objectives-and-comparisons-to-human-performance"><a href="#Can-language-models-learn-analogical-reasoning-Investigating-training-objectives-and-comparisons-to-human-performance" class="headerlink" title="Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance"></a>Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05597">http://arxiv.org/abs/2310.05597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Molly R. Petersen, Lonneke van der Plas</li>
<li>for: 测试模型是否可以学习基本的analogical reasoning，并使用更常见的人类语言 analogies 进行评估。</li>
<li>methods: 使用小量数据进行模型训练，并对模型的性能进行比较，以及与人类基准数据进行比较。</li>
<li>results: 结果显示，模型可以学习analogical reasoning，即使只使用小量数据。此外，模型在训练后与人类基准数据的性能相似。<details>
<summary>Abstract</summary>
While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical reasoning in humans than those in commonly used NLP benchmarks. Our experiments find that models are able to learn analogical reasoning, even with a small amount of data. We additionally compare our models to a dataset with a human baseline, and find that after training, models approach human performance.
</details>
<details>
<summary>摘要</summary>
而 analogies 是一种常见的方式来评估 word embeddings 在自然语言处理中，但是还是有趣的问题是否可以学习 analogical reasoning。在这篇文章中，我们测试了几种方式来学习基本的analogical reasoning，特别是关注常用于评估人类的 analogies，而不是常用于 NLP benchmarks 中的 analogies。我们的实验发现，模型可以学习 analogical reasoning，即使只有一小 amount of data。我们还对比了我们的模型与人类基准数据，发现， después de 训练，模型接近人类性能。
</details></li>
</ul>
<hr>
<h2 id="DRIN-Dynamic-Relation-Interactive-Network-for-Multimodal-Entity-Linking"><a href="#DRIN-Dynamic-Relation-Interactive-Network-for-Multimodal-Entity-Linking" class="headerlink" title="DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking"></a>DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05589">http://arxiv.org/abs/2310.05589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/starreeze/drin">https://github.com/starreeze/drin</a></li>
<li>paper_authors: Shangyu Xing, Fei Zhao, Zhen Wu, Chunhui Li, Jianbing Zhang, Xinyu Dai</li>
<li>for: 本研究旨在解决多模态Entity Linking（MEL）任务中的匹配批处和多样性问题。</li>
<li>methods: 我们提出了一种新的Dynamic Relation Interactive Network（DRIN）模型，其中明确地表示了四种不同的匹配对象之间的对应关系，并通过动态的Graph Convolutional Network（GCN）选择对应的对应关系，以适应不同的输入样本。</li>
<li>results: 我们在两个数据集上进行了实验，并证明了DRIN比前STATE-OF-THE-ART方法提高了大量的性能。<details>
<summary>Abstract</summary>
Multimodal Entity Linking (MEL) is a task that aims to link ambiguous mentions within multimodal contexts to referential entities in a multimodal knowledge base. Recent methods for MEL adopt a common framework: they first interact and fuse the text and image to obtain representations of the mention and entity respectively, and then compute the similarity between them to predict the correct entity. However, these methods still suffer from two limitations: first, as they fuse the features of text and image before matching, they cannot fully exploit the fine-grained alignment relations between the mention and entity. Second, their alignment is static, leading to low performance when dealing with complex and diverse data. To address these issues, we propose a novel framework called Dynamic Relation Interactive Network (DRIN) for MEL tasks. DRIN explicitly models four different types of alignment between a mention and entity and builds a dynamic Graph Convolutional Network (GCN) to dynamically select the corresponding alignment relations for different input samples. Experiments on two datasets show that DRIN outperforms state-of-the-art methods by a large margin, demonstrating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
多modal实体连接（MEL）是一个任务，旨在将多modal上的潜在含义链接到多modal知识库中的参照实体。 current methods for MEL 采用一致的框架：先将文本和图像相互作用，以获得提及和实体的表示，然后计算它们之间的相似性，以预测正确的实体。 However, these methods still suffer from two limitations: first, as they fuse the features of text and image before matching, they cannot fully exploit the fine-grained alignment relations between the mention and entity. Second, their alignment is static, leading to low performance when dealing with complex and diverse data.To address these issues, we propose a novel framework called Dynamic Relation Interactive Network (DRIN) for MEL tasks. DRIN explicitly models four different types of alignment between a mention and entity and builds a dynamic Graph Convolutional Network (GCN) to dynamically select the corresponding alignment relations for different input samples. Experiments on two datasets show that DRIN outperforms state-of-the-art methods by a large margin, demonstrating the effectiveness of our approach.
</details></li>
</ul>
<hr>
<h2 id="Regulation-and-NLP-RegNLP-Taming-Large-Language-Models"><a href="#Regulation-and-NLP-RegNLP-Taming-Large-Language-Models" class="headerlink" title="Regulation and NLP (RegNLP): Taming Large Language Models"></a>Regulation and NLP (RegNLP): Taming Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05553">http://arxiv.org/abs/2310.05553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catalina Goanta, Nikolaos Aletras, Ilias Chalkidis, Sofia Ranchordas, Gerasimos Spanakis</li>
<li>for: 这篇论文目的是探讨 NLP 研究如何受益于与法规研究的互动，以便更好地评估和控制风险。</li>
<li>methods: 论文使用的方法包括研究现有的法规研究和 NLP 研究，以及将这两者相互连接。</li>
<li>results: 论文指出了现有 NLP 研究中关于风险评估的缺陷，并提出了一种新的多学科研究空间（RegNLP），以便将科学知识与法规过程相连。<details>
<summary>Abstract</summary>
The scientific innovation in Natural Language Processing (NLP) and more broadly in artificial intelligence (AI) is at its fastest pace to date. As large language models (LLMs) unleash a new era of automation, important debates emerge regarding the benefits and risks of their development, deployment and use. Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements. This polarization, often amplified by social media, is swaying political agendas on AI regulation and governance and posing issues of regulatory capture. Capture occurs when the regulator advances the interests of the industry it is supposed to regulate, or of special interest groups rather than pursuing the general public interest. Meanwhile in NLP research, attention has been increasingly paid to the discussion of regulating risks and harms. This often happens without systematic methodologies or sufficient rooting in the disciplines that inspire an extended scope of NLP research, jeopardizing the scientific integrity of these endeavors. Regulation studies are a rich source of knowledge on how to systematically deal with risk and uncertainty, as well as with scientific evidence, to evaluate and compare regulatory options. This resource has largely remained untapped so far. In this paper, we argue how NLP research on these topics can benefit from proximity to regulatory studies and adjacent fields. We do so by discussing basic tenets of regulation, and risk and uncertainty, and by highlighting the shortcomings of current NLP discussions dealing with risk assessment. Finally, we advocate for the development of a new multidisciplinary research space on regulation and NLP (RegNLP), focused on connecting scientific knowledge to regulatory processes based on systematic methodologies.
</details>
<details>
<summary>摘要</summary>
科学创新在自然语言处理（NLP）和人工智能（AI）领域正在进行最快的发展。大语言模型（LLMs）在新的自动化时代引入了重要的争议，包括开发、部署和使用的优点和风险。目前，这些争议主要由人工智能安全和人工智能伦理运动领导。这种偏见，经常通过社交媒体扩大，正在影响政策制定和人工智能管理，并且存在政策捕捉的问题。在NLP研究中，越来越多的注意力被集中在评估风险和害处的问题上，但这些讨论通常缺乏系统化的方法和 suficient的基础知识，从而威胁NLP研究的科学 integriy。在这篇论文中，我们 argue that NLP研究可以从靠近 regulatory studies 和相关领域中受益。我们讨论了基本的管制原则，以及风险和不确定性的问题，并指出了当前NLP讨论中评估风险的缺陷。最后，我们倡议成立一个新的多学科研究空间（RegNLP），专注于将科学知识与管制过程相连，基于系统化的方法。
</details></li>
</ul>
<hr>
<h2 id="Findings-of-the-2023-ML-SUPERB-Challenge-Pre-Training-and-Evaluation-over-More-Languages-and-Beyond"><a href="#Findings-of-the-2023-ML-SUPERB-Challenge-Pre-Training-and-Evaluation-over-More-Languages-and-Beyond" class="headerlink" title="Findings of the 2023 ML-SUPERB Challenge: Pre-Training and Evaluation over More Languages and Beyond"></a>Findings of the 2023 ML-SUPERB Challenge: Pre-Training and Evaluation over More Languages and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05513">http://arxiv.org/abs/2310.05513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe</li>
<li>for: 本研究旨在探讨多语言speech recognition和语言识别领域中自主学习模型的应用。</li>
<li>methods: 本研究使用了SUPERB框架，并对多语言speech recognition和语言识别进行了自主学习模型的应用。</li>
<li>results: 研究发现，即便扩大模型规模，也不是一定能解决多语言speech任务中的所有挑战。不同的speech&#x2F;voice类型对多语言speech处理具有显著的挑战。<details>
<summary>Abstract</summary>
The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification. The challenge comprises a research track focused on applying ML-SUPERB to specific multilingual subjects, a Challenge Track for model submissions, and a New Language Track where language resource researchers can contribute and evaluate their low-resource language data in the context of the latest progress in multilingual speech recognition. The challenge garnered 12 model submissions and 54 language corpora, resulting in a comprehensive benchmark encompassing 154 languages. The findings indicate that merely scaling models is not the definitive solution for multilingual speech tasks, and a variety of speech/voice types present significant challenges in multilingual speech processing.
</details>
<details>
<summary>摘要</summary>
2023年多语言语音通用性表现 benchmark (ML-SUPERB) 挑战，将原有的 SUPERB 框架扩展到多语言语音识别和语言标识领域，强调自动标注模型。挑战包括应用于特定多语言主题的研究车道、挑战车道 для模型提交，以及新语言车道，其中语言资源研究人员可以在最新的多语言语音识别技术下评估和投入低资源语言数据。挑战共收到12个模型提交和54个语言资料库，创造了154种语言的全面 benchmark。发现表明，只有简单地扩大模型的规模并不是多语言语音任务的绝佳解决方案，多种语音/voice 类型在多语言语音处理中具有重要挑战。
</details></li>
</ul>
<hr>
<h2 id="XAL-EXplainable-Active-Learning-Makes-Classifiers-Better-Low-resource-Learners"><a href="#XAL-EXplainable-Active-Learning-Makes-Classifiers-Better-Low-resource-Learners" class="headerlink" title="XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners"></a>XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05502">http://arxiv.org/abs/2310.05502</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luoxiaoheics/xal">https://github.com/luoxiaoheics/xal</a></li>
<li>paper_authors: Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, Yue Zhang</li>
<li>for: The paper is written for proposing a novel Explainable Active Learning (XAL) framework for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations.</li>
<li>methods: The paper uses a pre-trained bi-directional encoder for classification, and employs a pre-trained uni-directional decoder to generate and score the explanation. A ranking loss is proposed to enhance the decoder’s capability in scoring explanations. During the selection of unlabeled data, the paper combines the predictive uncertainty of the encoder and the explanation score of the decoder to acquire informative data for annotation.</li>
<li>results: The paper achieves substantial improvement on all six tasks over previous Active Learning (AL) methods, and ablation studies demonstrate the effectiveness of each component. Human evaluation shows that the model trained in XAL performs surprisingly well in explaining its prediction.<details>
<summary>Abstract</summary>
Active learning aims to construct an effective training set by iteratively curating the most informative unlabeled data for annotation, which is practical in low-resource tasks. Most active learning techniques in classification rely on the model's uncertainty or disagreement to choose unlabeled data. However, previous work indicates that existing models are poor at quantifying predictive uncertainty, which can lead to over-confidence in superficial patterns and a lack of exploration. Inspired by the cognitive processes in which humans deduce and predict through causal information, we propose a novel Explainable Active Learning framework (XAL) for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directional decoder to generate and score the explanation. A ranking loss is proposed to enhance the decoder's capability in scoring explanations. During the selection of unlabeled data, we combine the predictive uncertainty of the encoder and the explanation score of the decoder to acquire informative data for annotation.   As XAL is a general framework for text classification, we test our methods on six different classification tasks. Extensive experiments show that XAL achieves substantial improvement on all six tasks over previous AL methods. Ablation studies demonstrate the effectiveness of each component, and human evaluation shows that the model trained in XAL performs surprisingly well in explaining its prediction.
</details>
<details>
<summary>摘要</summary>
aktive lerning 目的是建立一个有效的训练集 by 遍历最有用的无标例数据进行标签，尤其适用于低资源任务。大多数 aktive lerning 技术在分类中依赖模型的不确定性或争议选择无标例数据。然而，先前的工作表明，现有的模型对于预测不确定性的评估不善，可能会导致超过自信和 superficiale 的模式，而无法探索。 inspirited by 人类的认知过程中的推理和预测，我们提出了一个 novel Explainable Active Learning 框架 (XAL) 供低资源文本分类， aiming to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directional decoder to generate and score the explanation. A ranking loss is proposed to enhance the decoder's capability in scoring explanations. During the selection of unlabeled data, we combine the predictive uncertainty of the encoder and the explanation score of the decoder to acquire informative data for annotation.  As XAL is a general framework for text classification, we test our methods on six different classification tasks. Extensive experiments show that XAL achieves substantial improvement on all six tasks over previous AL methods. Ablation studies demonstrate the effectiveness of each component, and human evaluation shows that the model trained in XAL performs surprisingly well in explaining its prediction.
</details></li>
</ul>
<hr>
<h2 id="IDTraffickers-An-Authorship-Attribution-Dataset-to-link-and-connect-Potential-Human-Trafficking-Operations-on-Text-Escort-Advertisements"><a href="#IDTraffickers-An-Authorship-Attribution-Dataset-to-link-and-connect-Potential-Human-Trafficking-Operations-on-Text-Escort-Advertisements" class="headerlink" title="IDTraffickers: An Authorship Attribution Dataset to link and connect Potential Human-Trafficking Operations on Text Escort Advertisements"></a>IDTraffickers: An Authorship Attribution Dataset to link and connect Potential Human-Trafficking Operations on Text Escort Advertisements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05484">http://arxiv.org/abs/2310.05484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vageesh Saxena, Benjamin Bashpole, Gijs Van Dijck, Gerasimos Spanakis</li>
<li>for: 本研究旨在帮助法 enforcement 机构（LEA）更好地识别和连接人口贩卖（HT）案件和在线广告（ads）。</li>
<li>methods: 本研究使用了87,595个文本广告和5,244个vendor标签来建立一个庞大的IDTraffickers数据集，并使用了DeCLUTR-small模型进行训练，以实现闭包集合分类环境中的macro-F1分数0.8656。</li>
<li>results: 通过使用训练过的分类器提取的样式表示，实现了基于开集排序环境的mean r-precision分数0.8852，以便更好地识别潜在的HT指示器。<details>
<summary>Abstract</summary>
Human trafficking (HT) is a pervasive global issue affecting vulnerable individuals, violating their fundamental human rights. Investigations reveal that a significant number of HT cases are associated with online advertisements (ads), particularly in escort markets. Consequently, identifying and connecting HT vendors has become increasingly challenging for Law Enforcement Agencies (LEAs). To address this issue, we introduce IDTraffickers, an extensive dataset consisting of 87,595 text ads and 5,244 vendor labels to enable the verification and identification of potential HT vendors on online escort markets. To establish a benchmark for authorship identification, we train a DeCLUTR-small model, achieving a macro-F1 score of 0.8656 in a closed-set classification environment. Next, we leverage the style representations extracted from the trained classifier to conduct authorship verification, resulting in a mean r-precision score of 0.8852 in an open-set ranking environment. Finally, to encourage further research and ensure responsible data sharing, we plan to release IDTraffickers for the authorship attribution task to researchers under specific conditions, considering the sensitive nature of the data. We believe that the availability of our dataset and benchmarks will empower future researchers to utilize our findings, thereby facilitating the effective linkage of escort ads and the development of more robust approaches for identifying HT indicators.
</details>
<details>
<summary>摘要</summary>
人口贩卖（HT）是一个广泛存在的全球问题，影响到抵触的个人，违反其基本人权。调查表明，许多HT案件与在线广告（ads）相关，特别是在escort市场上。因此，为了识别和连接HT提供者而成为了法 enforcement agencies（LEAs）的挑战。为解决这个问题，我们介绍IDTraffickers，一个包含87,595个文本广告和5,244个提供者标签的广泛的数据集，以启用在线escort市场上的HT提供者验证和识别。为建立作者鉴定的标准，我们训练了DeCLUTR-small模型，在closed-set分类环境中实现了macro-F1分数0.8656。然后，我们利用训练出来的样式表示来进行作者鉴定，在开放集排名环境中实现了mean r-precision分数0.8852。最后，为促进未来研究和负责任数据分享，我们计划将IDTraffickers数据集和benchmark分发给研究人员，但是需要特定的条件，考虑到数据的敏感性。我们认为，随着我们的数据和benchmark的可用性，未来的研究人员将能够利用我们的发现，从而促进escort广告和HT指标的有效链接，并开发更加坚强的HT指标识别方法。
</details></li>
</ul>
<hr>
<h2 id="Empower-Nested-Boolean-Logic-via-Self-Supervised-Curriculum-Learning"><a href="#Empower-Nested-Boolean-Logic-via-Self-Supervised-Curriculum-Learning" class="headerlink" title="Empower Nested Boolean Logic via Self-Supervised Curriculum Learning"></a>Empower Nested Boolean Logic via Self-Supervised Curriculum Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05450">http://arxiv.org/abs/2310.05450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gingasan/boolkill">https://github.com/gingasan/boolkill</a></li>
<li>paper_authors: Hongqiu Wu, Linfeng Liu, Hai Zhao, Min Zhang</li>
<li>for: 检验语言模型是否具有强大的推理能力，而不仅仅是因为数据训练。</li>
<li>methods: 使用自然语言处理技术，对语言模型进行自我超vised学习，逐步增加复杂的逻辑逻辑，从 simpler to harder。</li>
<li>results: 语言模型通过this新的自我超vised学习方法（\textsc{Clr），能够有效地推理更加复杂和长距离的逻辑。<details>
<summary>Abstract</summary>
Beyond the great cognitive powers showcased by language models, it is crucial to scrutinize whether their reasoning capabilities stem from strong generalization or merely exposure to relevant data. As opposed to constructing increasingly complex logic, this paper probes into the boolean logic, the root capability of a logical reasoner. We find that any pre-trained language models even including large language models only behave like a random selector in the face of multi-nested boolean logic, a task that humans can handle with ease. To empower language models with this fundamental capability, this paper proposes a new self-supervised learning method \textit{Curriculum Logical Reasoning} (\textsc{Clr}), where we augment the training data with nested boolean logic chain step-by-step, and program the training from simpler logical patterns gradually to harder ones. This new training paradigm allows language models to effectively generalize to much harder and longer-hop logic, which can hardly be learned through naive training. Furthermore, we show that boolean logic is a great foundation for improving the subsequent general logical tasks.
</details>
<details>
<summary>摘要</summary>
更 beyond the great cognitive powers displayed by language models, it is crucial to examine whether their reasoning abilities are based on strong generalization or simply exposure to relevant data. Unlike constructing increasingly complex logic, this paper explores the boolean logic, the fundamental capability of a logical reasoner. We find that pre-trained language models, including large language models, can only perform like a random selector when faced with multi-nested boolean logic, a task that humans can handle easily. To empower language models with this fundamental capability, this paper proposes a new self-supervised learning method called \textsc{Curriculum Logical Reasoning} (\textsc{Clr}), where we gradually add nested boolean logic chains to the training data, starting with simpler logical patterns and gradually increasing the difficulty. This new training paradigm enables language models to effectively generalize to much harder and longer-hop logic, which cannot be learned through naive training. Furthermore, we show that boolean logic provides a solid foundation for improving subsequent general logical tasks.
</details></li>
</ul>
<hr>
<h2 id="Establishing-Trustworthiness-Rethinking-Tasks-and-Model-Evaluation"><a href="#Establishing-Trustworthiness-Rethinking-Tasks-and-Model-Evaluation" class="headerlink" title="Establishing Trustworthiness: Rethinking Tasks and Model Evaluation"></a>Establishing Trustworthiness: Rethinking Tasks and Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05442">http://arxiv.org/abs/2310.05442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Litschko, Max Müller-Eberstein, Rob van der Goot, Leon Weber, Barbara Plank</li>
<li>for: 理解自然语言处理（NLP）的核心概念和任务的 Computational Modeling，以及如何将其应用于实际场景中。</li>
<li>methods:  traditional compartmentalized approaches for understanding a model’s functional capacity, as well as recommendations for more multi-faceted evaluation protocols.</li>
<li>results:  the need for trustworthy and reliable NLP systems, and the importance of rethinking the traditional notion of language tasks and model evaluation in order to pursue a more holistic view of language.<details>
<summary>Abstract</summary>
Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing trustworthiness at the center. Towards this goal, we review existing compartmentalized approaches for understanding the origins of a model's functional capacity, and provide recommendations for more multi-faceted evaluation protocols.
</details>
<details>
<summary>摘要</summary>
Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing trustworthiness at the center. Towards this goal, we review existing compartmentalized approaches for understanding the origins of a model's functional capacity, and provide recommendations for more multi-faceted evaluation protocols.Here's the translation in Traditional Chinese:语言理解是一种多方面的认知能力，自然语言处理（NLP）社群在数十年来一直努力以计算方式模型。传统上，语言智能的不同方面被分类为特殊的任务，并且运用专门的模型架构和评估协议。 however, with the advent of large language models (LLMs)，社群目睹了一个剧烈的转变，从特定任务的专门模型演化为通用、任务无关的方法，这导致了传统的语言任务分类系统崩溃。这一传统的分类系统崩溃，也导致了评估和分析的问题增加。同时，LLMs 正在更多的实际应用中，包括以前未见的零学习设置，增加了可靠和可信的系统的需求。因此，我们认为现在是重新定义语言任务和模型评估的时候，并将信任性置于中心。以这个目标为导向，我们回顾现有的分类方法，并提供更多的多方面评估协议。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Robust-Early-Exiting-Framework-for-Autoregressive-Language-Models-with-Synchronized-Parallel-Decoding"><a href="#Fast-and-Robust-Early-Exiting-Framework-for-Autoregressive-Language-Models-with-Synchronized-Parallel-Decoding" class="headerlink" title="Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding"></a>Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05424">http://arxiv.org/abs/2310.05424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/raymin0223/fast_robust_early_exit">https://github.com/raymin0223/fast_robust_early_exit</a></li>
<li>paper_authors: Sangmin Bae, Jongwoo Ko, Hwanjun Song, Se-Young Yun</li>
<li>for: 提高 autoregressive 语言模型 的推理延迟</li>
<li>methods: 提出 Fast and Robust Early-Exiting (FREE) 框架，包括 shallow-deep 模块和同步并发解码</li>
<li>results: 在各种生成任务上实质性提高了推理速度，并提出了一种基于 Beta 混合模型的适应阈值估计器来确定适当的信心阈值<details>
<summary>Abstract</summary>
To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.
</details>
<details>
<summary>摘要</summary>
previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.Here's the translation in Traditional Chinese:previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.
</details></li>
</ul>
<hr>
<h2 id="Automating-Customer-Service-using-LangChain-Building-custom-open-source-GPT-Chatbot-for-organizations"><a href="#Automating-Customer-Service-using-LangChain-Building-custom-open-source-GPT-Chatbot-for-organizations" class="headerlink" title="Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations"></a>Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05421">http://arxiv.org/abs/2310.05421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keivalya Pandya, Mehfuza Holia<br>for: This research paper aims to automate customer service using a custom Large Language Model (LLM) called LangChain, which can provide personalized, responsive, and context-aware support.methods: The paper proposes a new approach that combines open-source methodologies, web scraping, fine-tuning, and the integration of LangChain into customer service platforms. The research uses data collection via web scraping, embeddings, Google’s Flan T5 XXL, Base, and Small language models for knowledge retrieval, and the integration of a chatbot into customer service platforms.results: The paper shows that the proposed approach can provide real-time support and query resolution, with the chatbot integrated into customer service platforms. The results also demonstrate the ability to scale across industries and organizations, and elevate customer retention, value extraction, and brand image.<details>
<summary>Abstract</summary>
In the digital age, the dynamics of customer service are evolving, driven by technological advancements and the integration of Large Language Models (LLMs). This research paper introduces a groundbreaking approach to automating customer service using LangChain, a custom LLM tailored for organizations. The paper explores the obsolescence of traditional customer support techniques, particularly Frequently Asked Questions (FAQs), and proposes a paradigm shift towards responsive, context-aware, and personalized customer interactions. The heart of this innovation lies in the fusion of open-source methodologies, web scraping, fine-tuning, and the seamless integration of LangChain into customer service platforms. This open-source state-of-the-art framework, presented as "Sahaay," demonstrates the ability to scale across industries and organizations, offering real-time support and query resolution. Key elements of this research encompass data collection via web scraping, the role of embeddings, the utilization of Google's Flan T5 XXL, Base and Small language models for knowledge retrieval, and the integration of the chatbot into customer service platforms. The results section provides insights into their performance and use cases, here particularly within an educational institution. This research heralds a new era in customer service, where technology is harnessed to create efficient, personalized, and responsive interactions. Sahaay, powered by LangChain, redefines the customer-company relationship, elevating customer retention, value extraction, and brand image. As organizations embrace LLMs, customer service becomes a dynamic and customer-centric ecosystem.
</details>
<details>
<summary>摘要</summary>
在数字时代，顾客服务的动力是不断发展，受技术进步和大语言模型（LLM）的整合影响。这篇研究论文提出了一种创新的自动化顾客服务方法，基于自定义的 LangChain LLM，为组织提供了一种新的客户服务模式。论文探讨传统顾客支持技术，特别是常见问题（FAQ）的过时性，并提出了一种新的客户交互模式，强调响应式、上下文感知和个性化的客户交互。这种创新的核心在于将开源方法ologies、网络抓取、精度调整和 LangChain 集成到顾客服务平台上。这个开源的 state-of-the-art 框架，即 "Sahaay"，能够在不同的行业和组织之间扩展，提供实时支持和问题解决。研究的关键元素包括通过网络抓取获取数据、使用 Google 的 Flan T5 XXL、Base 和 Small 语言模型 для知识检索，以及将 chatbot 集成到顾客服务平台上。研究结果提供了这些技术在不同的应用场景中的性能和使用情况，特别是在教育机构中。这项研究标志着客户服务的新时代，通过技术来创造高效、个性化、响应式的客户交互。Sahaay，基于 LangChain，重塑了客户-公司关系，提高客户退货、价值提取和品牌形象。随着组织接受 LLM，客户服务变成了一个动态和客户中心的生态系统。
</details></li>
</ul>
<hr>
<h2 id="mBBC-Exploring-the-Multilingual-Maze"><a href="#mBBC-Exploring-the-Multilingual-Maze" class="headerlink" title="mBBC: Exploring the Multilingual Maze"></a>mBBC: Exploring the Multilingual Maze</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05404">http://arxiv.org/abs/2310.05404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PortNLP/mBBC">https://github.com/PortNLP/mBBC</a></li>
<li>paper_authors: Sina Bagheri Nezhad, Ameeta Agrawal</li>
<li>for: 这个论文的目的是评估三种知名的多语言语言模型（mBERT、XLM-R和GPT-3）的性能，以便更好地理解这些模型在不同语言和语言上下文中的表现。</li>
<li>methods: 这个论文使用了自然语言处理技术中的自我超vised任务（下一个单词预测）来评估这些模型的性能，并在多种语言中进行了评估。</li>
<li>results: 研究发现资源水平对模型性能产生关键作用，具有更高资源水平的模型具有更高的准确率。此外，研究还发现了语言家族和字体类型之间复杂的关系，需要进一步的调查和研究，以便更好地理解语言特点和结构变化对模型性能的影响。<details>
<summary>Abstract</summary>
Multilingual language models have gained significant attention in recent years, enabling the development of applications that cater to diverse linguistic contexts. In this paper, we present a comprehensive evaluation of three prominent multilingual language models: mBERT, XLM-R, and GPT-3. Using the self-supervised task of next token prediction, we assess their performance across a diverse set of languages, with a focus on understanding the impact of resource availability, word order, language family, and script type on model accuracy. Our findings reveal that resource availability plays a crucial role in model performance, with higher resource levels leading to improved accuracy. We also identify the complex relationship between resource availability, language families, and script types, highlighting the need for further investigation into language-specific characteristics and structural variations. Additionally, our statistical inference analysis identifies significant features contributing to model performance, providing insights for model selection and deployment. Our study contributes to a deeper understanding of multilingual language models and informs future research and development to enhance their performance and generalizability across languages and linguistic contexts.
</details>
<details>
<summary>摘要</summary>
<translation_language> simplified_chinese</translation_language></SYS>多语言语言模型在最近几年内受到了广泛关注，使得开发能够适应多种语言文化背景的应用程序变得可能。在本文中，我们对三种著名的多语言语言模型——mBERT、XLM-R和GPT-3进行了全面的评估。使用下一个元素预测任务，我们评估了这些模型在不同语言中的表现，并将着眼于资源可用性、字符串顺序、语言家族和文字类型对模型准确率的影响。我们发现资源可用性在模型性能中扮演着关键的角色，高resource levels导致了改进的准确率。我们还发现了语言家族和文字类型之间复杂的关系，这种关系需要进一步的研究，以便更好地理解语言特有的特征和结构上的变化。此外，我们的统计推理分析还提到了对模型性能的重要贡献因素，为未来的模型选择和部署提供了智能。本研究对多语言语言模型的深入理解和未来研发的提高和普适性做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="GROVE-A-Retrieval-augmented-Complex-Story-Generation-Framework-with-A-Forest-of-Evidence"><a href="#GROVE-A-Retrieval-augmented-Complex-Story-Generation-Framework-with-A-Forest-of-Evidence" class="headerlink" title="GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence"></a>GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05388">http://arxiv.org/abs/2310.05388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi Shi, Zhen Huang, Dongsheng Li</li>
<li>for:  This paper aims to enhance the complexity and credibility of story generation by leveraging information from human-written stories and using a retrieval-augmented story generation framework.</li>
<li>methods: The proposed method uses a retrieval repository of target conditions to produce few-shot examples that serve as prompts for a large language model (LLM). It also employs an “asking-why” prompting scheme to extract a forest of evidence, which is used to compensate for ambiguities in the generated story.</li>
<li>results: The experimental results and numerous examples demonstrate the effectiveness of the proposed method in generating stories with complex and credible plots.<details>
<summary>Abstract</summary>
Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrieval-au\textbf{G}mented sto\textbf{R}y generation framework with a f\textbf{O}rest of e\textbf{V}id\textbf{E}nce (GROVE) to enhance stories' complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an ``asking-why'' prompting scheme that extracts a forest of evidence, providing compensation for the ambiguities that may occur in the generated story. This iterative process uncovers underlying story backgrounds. Finally, we select the most fitting chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing the narrative's complexity and credibility. Experimental results and numerous examples verify the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
假设故事生成是人机交互中的重要方面，特别是生成具有复杂剧本的故事。虽然大型语言模型（LLMs）在多种自然语言处理任务中表现良好，但是生成具有复杂和创新剧本的故事仍然是挑战。现有的方法通常靠着详细的提示来引导LLMs，从而限制生成的故事创作潜力。我们认为可以利用人类写的好故事中的信息，以生成更多元的剧本。深入探究故事细节可以建立更加复杂和真实的剧本。在这篇文章中，我们提出了一个具有追踪和补充的故事生成框架（GROVE），以增强故事的复杂性。我们建立了一个目标状况库，以生成少量的示例提示LLMs。此外，我们设计了一个“问题”提示方案，可以从故事中提取一棵证据森林，以补偿可能在生成的故事中出现的歧难。这个迭代过程可以暴露出故事的背景。最后，我们从证据森林中选择最符合的证据链，并将其与生成的故事结合，从而增强故事的复杂性和实际性。实验结果和许多例子证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Transcending-the-Attention-Paradigm-Representation-Learning-from-Geospatial-Social-Media-Data"><a href="#Transcending-the-Attention-Paradigm-Representation-Learning-from-Geospatial-Social-Media-Data" class="headerlink" title="Transcending the Attention Paradigm: Representation Learning from Geospatial Social Media Data"></a>Transcending the Attention Paradigm: Representation Learning from Geospatial Social Media Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05378">http://arxiv.org/abs/2310.05378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NickDiSanto/Twitter2030/tree/main/Beta">https://github.com/NickDiSanto/Twitter2030/tree/main/Beta</a></li>
<li>paper_authors: Nick DiSanto, Anthony Corso, Benjamin Sanders, Gavin Harding</li>
<li>for:  investigate social media data to uncover abstract relationships and challenge the reliance on complex models</li>
<li>methods: employ Bag-of-Words models specific to each city to analyze Twitter data and evaluate representation</li>
<li>results: discover hidden insights and demonstrate the considerable influence of geographic location on online communication, challenging the notion that intricate models are necessary for pattern recognition<details>
<summary>Abstract</summary>
While transformers have pioneered attention-driven architectures as a cornerstone of research, their dependence on explicitly contextual information underscores limitations in their abilities to tacitly learn overarching textual themes. This study investigates social media data as a source of distributed patterns, challenging the heuristic paradigm of performance benchmarking. In stark contrast to networks that rely on capturing complex long-term dependencies, models of online data inherently lack structure and are forced to learn underlying patterns in the aggregate. To properly represent these abstract relationships, this research dissects empirical social media corpora into their elemental components and analyzes over two billion tweets across population-dense locations. Exploring the relationship between location and vernacular in Twitter data, we employ Bag-of-Words models specific to each city and evaluate their respective representation. This demonstrates that hidden insights can be uncovered without the crutch of advanced algorithms and demonstrates that even amidst noisy data, geographic location has a considerable influence on online communication. This evidence presents tangible insights regarding geospatial communication patterns and their implications in social science. It also challenges the notion that intricate models are prerequisites for pattern recognition in natural language, aligning with the evolving landscape that questions the embrace of absolute interpretability over abstract understanding. This study bridges the divide between sophisticated frameworks and intangible relationships, paving the way for systems that blend structured models with conjectural reasoning.
</details>
<details>
<summary>摘要</summary>
transformers推动了注意力驱动的建筑，但它们对文本主题的潜在学习表现出了局限性。这个研究通过社交媒体数据来挑战传统性能标准的假设，因为模型在线上数据上自然地缺乏结构，需要通过汇总来学习下级 Patterns。为了正确表示这些抽象关系，我们在Twitter数据中分解了实际社交媒体文本，并对全球各地的 tweet 进行分析，检查了地点和方言之间的关系。我们使用特定于每个城市的 Bag-of-Words 模型进行评估，并发现了隐藏的Patterns。这种方法表明，无需复杂的算法，地理位置在在线交流中具有显著的影响。这些证据表明在社会科学中的地理通信模式和其影响，并挑战了人们对精准模型的依赖。这种研究既结合了结构化模型，也结合了推理。
</details></li>
</ul>
<hr>
<h2 id="Improving-End-to-End-Speech-Processing-by-Efficient-Text-Data-Utilization-with-Latent-Synthesis"><a href="#Improving-End-to-End-Speech-Processing-by-Efficient-Text-Data-Utilization-with-Latent-Synthesis" class="headerlink" title="Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis"></a>Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05374">http://arxiv.org/abs/2310.05374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqiao Lu, Wenyong Huang, Nianzu Zheng, Xingshan Zeng, Yu Ting Yeung, Xiao Chen<br>for: 这个论文主要是为了提高END-TO-END speech处理模型的性能，尤其是在数据中心时代的人工智能 era。methods: 这个论文提出了一种名为LaSyn的高效文本数据利用框架，用于增强END-TO-END speech处理模型的训练。LaSyn使用文本数据生成一种中间的幻数表示，然后将其与预训练的speech模型进行混合，以提高模型的性能。results: 在ASR任务上，LaSyn可以提高E2E基eline的表达误差率超过22.3%。在SLU任务上，LaSyn可以提高E2E基eline的意图分类精度和插槽填充精度。与已有的发布状态的工作相比，LaSyn的参数更少，并且得到了相当的性能提升。这些结果表明LaSyn生成的训练数据的质量。<details>
<summary>Abstract</summary>
Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.
</details>
<details>
<summary>摘要</summary>
培训高性能端到端语音处理模型需要庞大量的标注语音数据，特别在人工智能时代。然而，标注语音数据通常比文本数据更 scarce 和更昂贵。我们提议Latent Synthesis（LaSyn），一种高效的文本数据利用框架 для端到端语音处理模型。我们在一个预训练的语音模型上训练一个干扰生成器，将文本数据转换为一种中间的干扰表示。这些干扰表示可以增强语音数据的训练。我们对LaSyn进行了评估，在不同的测试集上，LaSyn在自动语音识别（ASR）和语言理解（SLU）任务上提高了基eline的性能。在ASR任务上，LaSyn在LibriSpeech train-clean-100上训练的基eline上，相对减少了22.3%的单词错误率。在SLU任务上，LaSyn提高了我们的基eline的意向分类精度和插槽填充精度，相对增加了4.1%和3.8%。 LaSyn的参数数量 fewer ，与已发表的状态 Künstler 的性能相匹配。结果表明增强的训练数据质量。
</details></li>
</ul>
<hr>
<h2 id="A-Glance-is-Enough-Extract-Target-Sentence-By-Looking-at-A-keyword"><a href="#A-Glance-is-Enough-Extract-Target-Sentence-By-Looking-at-A-keyword" class="headerlink" title="A Glance is Enough: Extract Target Sentence By Looking at A keyword"></a>A Glance is Enough: Extract Target Sentence By Looking at A keyword</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05352">http://arxiv.org/abs/2310.05352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Shi, Dong Wang, Lantian Li, Jiqing Han</li>
<li>for: 这个论文探讨了从多个说话人的多话语中提取目标句子，只需要输入一个关键词。例如，在社会保障应用中，关键词可能是“帮助”，目标是从其他说话人的干扰中提取某个人呼叫的句子。</li>
<li>methods: 我们提议使用Transformer架构将关键词和语音词汇 embedding，然后通过cross-attention机制选择正确的内容从拼接或重叠的语音中提取目标句子。</li>
<li>results: 在Librispeech数据集上，我们的提议方法可以很好地提取噪音和杂音声中的目标句子（SNR&#x3D;-3dB），PER为26%，比基eline系统的PER为96%。<details>
<summary>Abstract</summary>
This paper investigates the possibility of extracting a target sentence from multi-talker speech using only a keyword as input. For example, in social security applications, the keyword might be "help", and the goal is to identify what the person who called for help is articulating while ignoring other speakers. To address this problem, we propose using the Transformer architecture to embed both the keyword and the speech utterance and then rely on the cross-attention mechanism to select the correct content from the concatenated or overlapping speech. Experimental results on Librispeech demonstrate that our proposed method can effectively extract target sentences from very noisy and mixed speech (SNR=-3dB), achieving a phone error rate (PER) of 26\%, compared to the baseline system's PER of 96%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Negative-Object-Presence-Evaluation-NOPE-to-Measure-Object-Hallucination-in-Vision-Language-Models"><a href="#Negative-Object-Presence-Evaluation-NOPE-to-Measure-Object-Hallucination-in-Vision-Language-Models" class="headerlink" title="Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models"></a>Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05338">http://arxiv.org/abs/2310.05338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung</li>
<li>for: 本研究旨在评估视言语（VL）模型中对物体幻化的影响，以提高模型的可靠性和可信度。</li>
<li>methods: 本研究使用了大量的自然语言模型生成29.5k个高质量的 sintetic negative pronoun（NegP）数据，以评估VL模型对物体幻化的敏感性。</li>
<li>results: 研究发现，无论是当前的State-of-the-art VL模型都不免受物体幻化的影响，其中所有模型在NegP问题中的准确率都低于10%。此外，研究还发现了lexically diverse visual questions、宽泛的问题类型和场景相关的物体，可能会使VL模型增加物体幻化的风险。<details>
<summary>Abstract</summary>
Object hallucination poses a significant challenge in vision-language (VL) models, often leading to the generation of nonsensical or unfaithful responses with non-existent objects. However, the absence of a general measurement for evaluating object hallucination in VL models has hindered our understanding and ability to mitigate this issue. In this work, we present NOPE (Negative Object Presence Evaluation), a novel benchmark designed to assess object hallucination in VL models through visual question answering (VQA). We propose a cost-effective and scalable approach utilizing large language models to generate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE. We extensively investigate the performance of 10 state-of-the-art VL models in discerning the non-existence of objects in visual questions, where the ground truth answers are denoted as NegP (e.g., "none"). Additionally, we evaluate their standard performance on visual questions on 9 other VQA datasets. Through our experiments, we demonstrate that no VL model is immune to the vulnerability of object hallucination, as all models achieve accuracy below 10\% on NegP. Furthermore, we uncover that lexically diverse visual questions, question types with large scopes, and scene-relevant objects capitalize the risk of object hallucination in VL models.
</details>
<details>
<summary>摘要</summary>
<SYS>对象幻像 pose 视语言（VL）模型中的挑战，经常导致生成无意义或不准确的回答，其中包括无存在的对象。然而，对视语言模型中对象幻像的评价没有一个通用的方法，这限制了我们对这个问题的理解和处理能力。在这种情况下，我们提出了 NOPE（负对象存在评价），一种新的benchmark，用于评价视语言模型中对象幻像的能力。我们提出了一种可靠且可扩展的方法，利用大型自然语言模型生成29.5k个高质量的负对象数据（NegP）。我们广泛研究了10种当前最佳的视语言模型在判断视Question中的对象不存在时的性能，其中ground truth answers denoted as NegP（例如，"none"）。此外，我们还评估了这些模型在9个其他VQA数据集上的标准性能。经过我们的实验，我们发现没有一个视语言模型是对象幻像的免疫者，所有模型在NegP上的准确率都低于10%。此外，我们发现了不同类型的视Question、广泛的问题类型和场景相关的对象都会增加视语言模型中对象幻像的风险。</SYS>Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Resolving-the-Imbalance-Issue-in-Hierarchical-Disciplinary-Topic-Inference-via-LLM-based-Data-Augmentation"><a href="#Resolving-the-Imbalance-Issue-in-Hierarchical-Disciplinary-Topic-Inference-via-LLM-based-Data-Augmentation" class="headerlink" title="Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation"></a>Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05318">http://arxiv.org/abs/2310.05318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xunxin Cai, Meng Xiao, Zhiyuan Ning, Yuanchun Zhou</li>
<li>for: This paper aims to address the issue of data imbalance in Natural Language Processing, specifically in the context of research proposals submitted for funding.</li>
<li>methods: The paper uses large language models (Llama V1) as data generators to augment research proposals categorized within intricate disciplinary hierarchies. The authors design prompts for keyword-based research proposal generation to rectify data imbalances and enhance the equity of expert assignments.</li>
<li>results: The experiments conducted in the paper demonstrate the efficacy of the generated data, showing that the research proposals produced using the prompts can effectively address the issue of data imbalance and generate high-quality scientific text data.<details>
<summary>Abstract</summary>
In addressing the imbalanced issue of data within the realm of Natural Language Processing, text data augmentation methods have emerged as pivotal solutions. This data imbalance is prevalent in the research proposals submitted during the funding application process. Such imbalances, resulting from the varying popularity of disciplines or the emergence of interdisciplinary studies, significantly impede the precision of downstream topic models that deduce the affiliated disciplines of these proposals. At the data level, proposals penned by experts and scientists are inherently complex technological texts, replete with intricate terminologies, which augmenting such specialized text data poses unique challenges. At the system level, this, in turn, compromises the fairness of AI-assisted reviewer assignment systems, which raises a spotlight on solving this issue. This study leverages large language models (Llama V1) as data generators to augment research proposals categorized within intricate disciplinary hierarchies, aiming to rectify data imbalances and enhance the equity of expert assignments. We first sample within the hierarchical structure to find the under-represented class. Then we designed a prompt for keyword-based research proposal generation. Our experiments attests to the efficacy of the generated data, demonstrating that research proposals produced using the prompts can effectively address the aforementioned issues and generate high quality scientific text data, thus help the model overcome the imbalanced issue.
</details>
<details>
<summary>摘要</summary>
在自然语言处理领域中解决数据不均衡问题，文本数据增强方法已成为关键解决方案。这种数据不均衡问题在研究提案申请过程中非常普遍，这些不均衡导致下游话题模型准确性受到影响。在数据层次，由专家和科学家写的提案是复杂的技术文本，充满专业术语，增强这种专业文本数据带来了独特的挑战。在系统层次，这会导致人工智能助手分配系统的公平性受到影响。本研究利用大型自然语言模型（Llama V1）作为数据生成器，增强分类在复杂的学科层次中的研究提案，以解决数据不均衡问题并提高专家分配的公平性。我们首先在层次结构中采样到下降类，然后设计了关键词基于的研究提案生成提示。我们的实验证明了生成的数据的有效性，显示了使用提示生成的研究提案可以有效地解决上述问题，并生成高质量的科学文本数据，帮助模型超越数据不均衡问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/cs.CL_2023_10_09/" data-id="clombedqy00c4s0888tdv331v" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/cs.LG_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T10:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/cs.LG_2023_10_09/">cs.LG - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Fair-Classifiers-that-Abstain-without-Harm"><a href="#Fair-Classifiers-that-Abstain-without-Harm" class="headerlink" title="Fair Classifiers that Abstain without Harm"></a>Fair Classifiers that Abstain without Harm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06205">http://arxiv.org/abs/2310.06205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongxin Yin, Jean-François Ton, Ruocheng Guo, Yuanshun Yao, Mingyan Liu, Yang Liu</li>
<li>For: The paper aims to develop a post-hoc method for existing classifiers to selectively abstain from predicting certain samples in order to achieve group fairness while maintaining original accuracy.* Methods: The proposed method uses integer programming to assign abstention decisions for each training sample and trains a surrogate model to generalize the abstaining decisions to test samples.* Results: The paper shows that the proposed method outperforms existing methods in terms of fairness disparity without sacrificing accuracy at similar abstention rates, and provides theoretical results on the feasibility of the IP procedure and the required abstention rate for different levels of unfairness tolerance and accuracy constraint.<details>
<summary>Abstract</summary>
In critical applications, it is vital for classifiers to defer decision-making to humans. We propose a post-hoc method that makes existing classifiers selectively abstain from predicting certain samples. Our abstaining classifier is incentivized to maintain the original accuracy for each sub-population (i.e. no harm) while achieving a set of group fairness definitions to a user specified degree. To this end, we design an Integer Programming (IP) procedure that assigns abstention decisions for each training sample to satisfy a set of constraints. To generalize the abstaining decisions to test samples, we then train a surrogate model to learn the abstaining decisions based on the IP solutions in an end-to-end manner. We analyze the feasibility of the IP procedure to determine the possible abstention rate for different levels of unfairness tolerance and accuracy constraint for achieving no harm. To the best of our knowledge, this work is the first to identify the theoretical relationships between the constraint parameters and the required abstention rate. Our theoretical results are important since a high abstention rate is often infeasible in practice due to a lack of human resources. Our framework outperforms existing methods in terms of fairness disparity without sacrificing accuracy at similar abstention rates.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在关键应用中，分类器需要延迟决策，以便启用人类决策。我们提出了一种后期方法，使得现有的分类器可以选择性弃权处理certain sample。我们的弃权分类器被激励保持每个子 популяции的原始精度（即不害），同时实现一组集体公正定义到用户指定的程度。为此，我们设计了一个整数程序（IP）过程，将每个训练样本的弃权决策分配给满足一系列约束。为推广弃权决策到测试样本，我们然后训练了一个代理模型，以learn弃权决策基于IP解决方案的末端方式。我们分析了IP过程的可行性，以确定不同的不公正忍容度和精度约束下的可能的弃权率。我们的理论结果非常重要，因为高弃权率在实践中通常是不可能的，由于人工资源的缺乏。我们的框架在保持公正差距方面比现有方法更高，而不是牺牲精度。
</details></li>
</ul>
<hr>
<h2 id="PAC-Bayesian-Spectrally-Normalized-Bounds-for-Adversarially-Robust-Generalization"><a href="#PAC-Bayesian-Spectrally-Normalized-Bounds-for-Adversarially-Robust-Generalization" class="headerlink" title="PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization"></a>PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06182">http://arxiv.org/abs/2310.06182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancong Xiao, Ruoyu Sun, Zhi- Quan Luo</li>
<li>for: This paper focuses on establishing theoretical guarantees for the robust generalization of deep neural networks (DNNs) against adversarial attacks.</li>
<li>methods: The paper uses a PAC-Bayes approach (Neyshabur et al., 2017) and provides a spectrally-normalized robust generalization bound for DNNs, addressing the challenge of extending the key ingredient to robust settings without relying on additional strong assumptions.</li>
<li>results: The paper shows that the mismatch terms between standard and robust generalization bounds are solely due to mathematical issues, and provides a different perspective on understanding robust generalization. Additionally, the paper extends the main result to adversarial robustness against general non-$\ell_p$ attacks and other neural network architectures.<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are vulnerable to adversarial attacks. It is found empirically that adversarially robust generalization is crucial in establishing defense algorithms against adversarial attacks. Therefore, it is interesting to study the theoretical guarantee of robust generalization. This paper focuses on norm-based complexity, based on a PAC-Bayes approach (Neyshabur et al., 2017). The main challenge lies in extending the key ingredient, which is a weight perturbation bound in standard settings, to the robust settings. Existing attempts heavily rely on additional strong assumptions, leading to loose bounds. In this paper, we address this issue and provide a spectrally-normalized robust generalization bound for DNNs. Compared to existing bounds, our bound offers two significant advantages: Firstly, it does not depend on additional assumptions. Secondly, it is considerably tighter, aligning with the bounds of standard generalization. Therefore, our result provides a different perspective on understanding robust generalization: The mismatch terms between standard and robust generalization bounds shown in previous studies do not contribute to the poor robust generalization. Instead, these disparities solely due to mathematical issues. Finally, we extend the main result to adversarial robustness against general non-$\ell_p$ attacks and other neural network architectures.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:深度神经网络（DNN）易受到敌意攻击。实证表明，针对攻击的鲁棒化是建立防御算法的关键。因此，研究鲁棒化的理论保证很有趣。这篇论文关注 norm-based 复杂性，基于 PAC-Bayes 方法（Neyshabur et al., 2017）。主要挑战在扩展关键成分，即标准设置中的 weight 偏移 bound，到鲁棒设置中。现有尝试都需要额外假设，导致约束较松。在这篇论文中，我们解决这个问题，并提供一个spectrally-normalized 鲁棒化维度 bound for DNNs。与现有 bound 相比，我们的 bound 具有两个优势：首先，不需要额外假设。第二，较紧，与标准化 generalization bound 相符。因此，我们的结果提供了一种不同的理解鲁棒化的视角：在前一些研究中显示的鲁棒化与标准化 generalization bound 之间的差异不是由于 poor 鲁棒化，而是由于数学问题。最后，我们扩展主要结果到面向普通非 $\ell_p$ 攻击和其他神经网络架构。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Integration-for-Spatiotemporal-Neural-Point-Processes"><a href="#Automatic-Integration-for-Spatiotemporal-Neural-Point-Processes" class="headerlink" title="Automatic Integration for Spatiotemporal Neural Point Processes"></a>Automatic Integration for Spatiotemporal Neural Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06179">http://arxiv.org/abs/2310.06179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhou, Rose Yu</li>
<li>for: 这篇论文主要针对的是如何有效地捕捉和分析continuous-time点处理，尤其是在空间和时间上的点处理（STPPs）。</li>
<li>methods: 这篇论文提出了一种新的AutoSTPP（自动 интеграл для空间时间 нейрон点处理）方法，它是基于AutoInt（自动 интеграл）方法的扩展，可以有效地处理3D STPP。</li>
<li>results: 研究人员通过synthetic数据和实际世界数据 validate了AutoSTPP方法，并证明了其在复杂的intensity函数恢复方面的优异性。<details>
<summary>Abstract</summary>
Learning continuous-time point processes is essential to many discrete event forecasting tasks. However, integration poses a major challenge, particularly for spatiotemporal point processes (STPPs), as it involves calculating the likelihood through triple integrals over space and time. Existing methods for integrating STPP either assume a parametric form of the intensity function, which lacks flexibility; or approximating the intensity with Monte Carlo sampling, which introduces numerical errors. Recent work by Omi et al. [2019] proposes a dual network or AutoInt approach for efficient integration of flexible intensity function. However, the method only focuses on the 1D temporal point process. In this paper, we introduce a novel paradigm: AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the AutoInt approach to 3D STPP. We show that direct extension of the previous work overly constrains the intensity function, leading to poor performance. We prove consistency of AutoSTPP and validate it on synthetic data and benchmark real world datasets, showcasing its significant advantage in recovering complex intensity functions from irregular spatiotemporal events, particularly when the intensity is sharply localized.
</details>
<details>
<summary>摘要</summary>
Recent work by Omi et al. (2019) proposes a dual network or AutoInt approach for efficient integration of flexible intensity functions. However, this method only focuses on 1D temporal point processes. In this paper, we introduce a novel paradigm called AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the AutoInt approach to 3D STPP. We show that direct extension of the previous work overly constrains the intensity function, leading to poor performance.We prove the consistency of AutoSTPP and validate it on synthetic data and benchmark real-world datasets. Our results show that AutoSTPP significantly outperforms existing methods in recovering complex intensity functions from irregular spatiotemporal events, particularly when the intensity is sharply localized.
</details></li>
</ul>
<hr>
<h2 id="DockGame-Cooperative-Games-for-Multimeric-Rigid-Protein-Docking"><a href="#DockGame-Cooperative-Games-for-Multimeric-Rigid-Protein-Docking" class="headerlink" title="DockGame: Cooperative Games for Multimeric Rigid Protein Docking"></a>DockGame: Cooperative Games for Multimeric Rigid Protein Docking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06177">http://arxiv.org/abs/2310.06177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vsomnath/dockgame">https://github.com/vsomnath/dockgame</a></li>
<li>paper_authors: Vignesh Ram Somnath, Pier Giuseppe Sessa, Maria Rodriguez Martinez, Andreas Krause</li>
<li>for: 本文针对的是预测多蛋白质复合物的结构，即蛋白质 docking 问题。</li>
<li>methods: 本文提出了一种基于游戏理论的 docking 方法，视蛋白质 docking 为多个蛋白质之间的合作游戏，并通过同时更新梯度来计算稳定Equilibrium。此外，本文还提出了一种基于扩散生成模型的方法，通过学习扩散分布来采样真实潜在力的Gibbs分布。</li>
<li>results: 实验结果表明，对 DB5.5 数据集，DockGame 比传统的 docking 方法快得多，能够生成多个可能的结构，并且与现有的 binary docking 基准集成比较。<details>
<summary>Abstract</summary>
Protein interactions and assembly formation are fundamental to most biological processes. Predicting the assembly structure from constituent proteins -- referred to as the protein docking task -- is thus a crucial step in protein design applications. Most traditional and deep learning methods for docking have focused mainly on binary docking, following either a search-based, regression-based, or generative modeling paradigm. In this paper, we focus on the less-studied multimeric (i.e., two or more proteins) docking problem. We introduce DockGame, a novel game-theoretic framework for docking -- we view protein docking as a cooperative game between proteins, where the final assembly structure(s) constitute stable equilibria w.r.t. the underlying game potential. Since we do not have access to the true potential, we consider two approaches - i) learning a surrogate game potential guided by physics-based energy functions and computing equilibria by simultaneous gradient updates, and ii) sampling from the Gibbs distribution of the true potential by learning a diffusion generative model over the action spaces (rotations and translations) of all proteins. Empirically, on the Docking Benchmark 5.5 (DB5.5) dataset, DockGame has much faster runtimes than traditional docking methods, can generate multiple plausible assembly structures, and achieves comparable performance to existing binary docking baselines, despite solving the harder task of coordinating multiple protein chains.
</details>
<details>
<summary>摘要</summary>
生物过程中的蛋白质交互和组装是基本的。从组成蛋白质的蛋白质拟合结构 -- 称为蛋白质拟合任务 -- 是蛋白质设计应用中的关键步骤。大多数传统和深度学习方法都主要关注了 binary docking，包括搜索、回归和生成模型的思路。在这篇论文中，我们关注了较少研究的多蛋白质（即两个或更多蛋白质）拟合问题。我们引入了 DockGame，一个基于游戏理论的拟合框架 -- 我们视蛋白质拟合为蛋白质之间的合作游戏，其最终结构为蛋白质之间的稳定平衡点。由于我们没有访问真实的潜在力，我们考虑了两种方法：一是学习带有物理基础能函数的代理游戏可能性函数，并通过同时更新梯度来计算平衡点；二是通过学习动作空间（旋转和平移）中的托德曼分布来采样真实的潜在力。实验表明，在 DB5.5 数据集上，DockGame 的运行时间远比传统拟合方法快得多，可以生成多个可能的结构，并与现有的 binary docking 基线相当。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Simplicity-Bias-in-Deep-Learning-for-Improved-OOD-Generalization-and-Robustness"><a href="#Mitigating-Simplicity-Bias-in-Deep-Learning-for-Improved-OOD-Generalization-and-Robustness" class="headerlink" title="Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness"></a>Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06161">http://arxiv.org/abs/2310.06161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/estija/cmid">https://github.com/estija/cmid</a></li>
<li>paper_authors: Bhavya Vasudeva, Kameron Shahabi, Vatsal Sharan</li>
<li>for:  addressing simplicity bias in neural networks and improving OOD generalization, subgroup robustness, and fairness</li>
<li>methods:  regularizing the conditional mutual information of a simple model to obtain a more diverse set of features for making predictions</li>
<li>results:  effective in various problem settings and real-world applications, leading to more diverse feature usage, enhanced OOD generalization, improved subgroup robustness, and fairness, with theoretical analyses of the effectiveness and OOD generalization properties.Here’s the full Chinese text:</li>
<li>for:  Addressing simplicity bias in 神经网络（NNs），提高 OUT-OF-DISTRIBUTION（OOD）泛化、 subgroup robustness 和 fairness</li>
<li>methods:  Regularizing the conditional mutual information of a simple model to obtain a more diverse set of features for making predictions</li>
<li>results:  Effective in various problem settings and real-world applications, leading to more diverse feature usage, enhanced OOD generalization, improved subgroup robustness, and fairness, with theoretical analyses of the effectiveness and OOD generalization properties.<details>
<summary>Abstract</summary>
Neural networks (NNs) are known to exhibit simplicity bias where they tend to prefer learning 'simple' features over more 'complex' ones, even when the latter may be more informative. Simplicity bias can lead to the model making biased predictions which have poor out-of-distribution (OOD) generalization. To address this, we propose a framework that encourages the model to use a more diverse set of features to make predictions. We first train a simple model, and then regularize the conditional mutual information with respect to it to obtain the final model. We demonstrate the effectiveness of this framework in various problem settings and real-world applications, showing that it effectively addresses simplicity bias and leads to more features being used, enhances OOD generalization, and improves subgroup robustness and fairness. We complement these results with theoretical analyses of the effect of the regularization and its OOD generalization properties.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Provably-Accelerating-Ill-Conditioned-Low-rank-Estimation-via-Scaled-Gradient-Descent-Even-with-Overparameterization"><a href="#Provably-Accelerating-Ill-Conditioned-Low-rank-Estimation-via-Scaled-Gradient-Descent-Even-with-Overparameterization" class="headerlink" title="Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization"></a>Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06159">http://arxiv.org/abs/2310.06159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cong Ma, Xingyu Xu, Tian Tong, Yuejie Chi</li>
<li>for: 估计低维对象（如矩阵和张量）从不完整、可能受损的线性测量中获得</li>
<li>methods: 使用简单迭代法如梯度下降（GD）来直接回归低维因子，具有小内存和计算脚印</li>
<li>results: ScaledGD算法可以线性 converge，不受低维对象的condition number影响，并且可以在各种任务中实现快速的全局收敛，包括感知、Robust PCA和完成任务。<details>
<summary>Abstract</summary>
Many problems encountered in science and engineering can be formulated as estimating a low-rank object (e.g., matrices and tensors) from incomplete, and possibly corrupted, linear measurements. Through the lens of matrix and tensor factorization, one of the most popular approaches is to employ simple iterative algorithms such as gradient descent (GD) to recover the low-rank factors directly, which allow for small memory and computation footprints. However, the convergence rate of GD depends linearly, and sometimes even quadratically, on the condition number of the low-rank object, and therefore, GD slows down painstakingly when the problem is ill-conditioned. This chapter introduces a new algorithmic approach, dubbed scaled gradient descent (ScaledGD), that provably converges linearly at a constant rate independent of the condition number of the low-rank object, while maintaining the low per-iteration cost of gradient descent for a variety of tasks including sensing, robust principal component analysis and completion. In addition, ScaledGD continues to admit fast global convergence to the minimax-optimal solution, again almost independent of the condition number, from a small random initialization when the rank is over-specified in the presence of Gaussian noise. In total, ScaledGD highlights the power of appropriate preconditioning in accelerating nonconvex statistical estimation, where the iteration-varying preconditioners promote desirable invariance properties of the trajectory with respect to the symmetry in low-rank factorization without hurting generalization.
</details>
<details>
<summary>摘要</summary>
许多科学和工程问题可以表示为估算一个低级对象（例如矩阵和张量）从不完整和可能受损的线性测量中。通过矩阵和张量分解的镜头，一种非常流行的方法是使用简单的迭代算法such as gradient descent (GD)来恢复低级因子，这些算法具有小内存和计算成本。然而，GD的收敛率与低级对象的condition number线性相关，当问题不梯化时，GD的收敛率会辐芳缓慢。这章节介绍了一种新的算法方法，称为scaled gradient descent (ScaledGD)，该方法可以在不同任务中，包括感知、稳定主成分分析和完成任务中，以Constant rate linearly converge，而不是linearly dependent on the condition number of the low-rank object。此外，ScaledGD还可以快速到达最优解，即minimax-optimal solution，从小Random initialization开始，当级数超出规定时，在存在 Gaussian noise 的情况下。总之，ScaledGD强调了适当的预conditioning在加速非 conjugate statistical estimation中的作用，iteration-varying preconditioners promote desirable invariance properties of the trajectory with respect to the symmetry in low-rank factorization without hurting generalization。
</details></li>
</ul>
<hr>
<h2 id="Manifold-augmented-Eikonal-Equations-Geodesic-Distances-and-Flows-on-Differentiable-Manifolds"><a href="#Manifold-augmented-Eikonal-Equations-Geodesic-Distances-and-Flows-on-Differentiable-Manifolds" class="headerlink" title="Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds"></a>Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06157">http://arxiv.org/abs/2310.06157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Kelshaw, Luca Magri</li>
<li>for: 这项研究旨在提供一种基于模型的方法来 parameterize distance fields和 geodesic flows on manifolds，以便在 differentiable manifolds 上进行统计分析和减少维度模型。</li>
<li>methods: 该研究使用 manifold-augmented Eikonal equation 的解来 parameterize distance fields和 geodesic flows on manifolds。</li>
<li>results: 研究发现， manifold 的geometry对 distance field 产生了影响，而 geodesic flow 可以用来获取 globally length-minimizing curves。这些结果开启了 differentiable manifolds 上的统计分析和减少维度模型的可能性。<details>
<summary>Abstract</summary>
Manifolds discovered by machine learning models provide a compact representation of the underlying data. Geodesics on these manifolds define locally length-minimising curves and provide a notion of distance, which are key for reduced-order modelling, statistical inference, and interpolation. In this work, we propose a model-based parameterisation for distance fields and geodesic flows on manifolds, exploiting solutions of a manifold-augmented Eikonal equation. We demonstrate how the geometry of the manifold impacts the distance field, and exploit the geodesic flow to obtain globally length-minimising curves directly. This work opens opportunities for statistics and reduced-order modelling on differentiable manifolds.
</details>
<details>
<summary>摘要</summary>
人工智能模型发现的 manifold 提供了数据的紧凑表示。 manifold 上的 geodesic 定义了本地最短曲线，并提供了距离的概念，这些概念是reduced-order模型、统计推断和 interpolate 等方面的关键。在这项工作中，我们提议一种基于模型的 parameterization 方法 для distance field 和 geodesic flow  на manifold，利用 manifold-augmented Eikonal equation 的解。我们示出了 manifold 的几何特性对 distance field 的影响，并利用 geodesic flow 直接获取全球最短曲线。这项工作开启了 differentiable manifold 上的统计和减少模型的可能性。
</details></li>
</ul>
<hr>
<h2 id="Latent-Diffusion-Model-for-DNA-Sequence-Generation"><a href="#Latent-Diffusion-Model-for-DNA-Sequence-Generation" class="headerlink" title="Latent Diffusion Model for DNA Sequence Generation"></a>Latent Diffusion Model for DNA Sequence Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06150">http://arxiv.org/abs/2310.06150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehui Li, Yuhao Ni, Tim August B. Huygelen, Akashaditya Das, Guoxuan Xia, Guy-Bart Stan, Yiren Zhao</li>
<li>for: 本研究旨在提出一种基于扩散模型的精灵散列模型（DiscDiff），用于静止DNA序列生成。</li>
<li>methods: 本研究使用了一种卷积神经网络（autoencoder）将扩散模型的维度嵌入到维度空间中，以便利用连续扩散模型的强大生成能力来生成扩散数据。</li>
<li>results: 本研究的DiscDiff模型能够生成具有真实DNA序列的高一致性的合成DNA序列，包括约束分布、封闭空间分布（FReD）和染色体轨迹分布。此外，本研究还提供了15种物种150000个特有前体-基因序列数据，为未来的生成模型在遗传学中提供了更多的资源。<details>
<summary>Abstract</summary>
The harnessing of machine learning, especially deep generative models, has opened up promising avenues in the field of synthetic DNA sequence generation. Whilst Generative Adversarial Networks (GANs) have gained traction for this application, they often face issues such as limited sample diversity and mode collapse. On the other hand, Diffusion Models are a promising new class of generative models that are not burdened with these problems, enabling them to reach the state-of-the-art in domains such as image generation. In light of this, we propose a novel latent diffusion model, DiscDiff, tailored for discrete DNA sequence generation. By simply embedding discrete DNA sequences into a continuous latent space using an autoencoder, we are able to leverage the powerful generative abilities of continuous diffusion models for the generation of discrete data. Additionally, we introduce Fr\'echet Reconstruction Distance (FReD) as a new metric to measure the sample quality of DNA sequence generations. Our DiscDiff model demonstrates an ability to generate synthetic DNA sequences that align closely with real DNA in terms of Motif Distribution, Latent Embedding Distribution (FReD), and Chromatin Profiles. Additionally, we contribute a comprehensive cross-species dataset of 150K unique promoter-gene sequences from 15 species, enriching resources for future generative modelling in genomics. We will make our code public upon publication.
</details>
<details>
<summary>摘要</summary>
“机器学习的应用，特别是深度生成模型，在人造DNA序列生成领域中开启了有前途的可能性。虽然生成对抗网络（GANs）在这个应用中获得了进展，但它们经常面临有限的样本多样性和模式崩溃的问题。相比之下，传播模型是一种新的生成模型，没有这些问题，因此可以在领域中实现国际级的生成。在这背景下，我们提出了一个新的潜在传播模型，DiscDiff，专门适用于碎变DNA序列生成。通过将碎变DNA序列转换为连续的潜在空间中的对抗网络，我们可以利用传播模型的强大生成能力来生成碎变数据。此外，我们引入了Fréchet重建距离（FReD）作为评估生成DNA序列质量的新指标。DiscDiff模型在关于折衣分布、隐藏嵌入分布（FReD）和染色体质量上呈现高度的一致性。此外，我们提供了15种物种150000个唯一的激活器-蛋白质序列数据，增加了未来生成模型在遗传学方面的资源。我们将代码公开发布。”
</details></li>
</ul>
<hr>
<h2 id="On-the-Correlation-between-Random-Variables-and-their-Principal-Components"><a href="#On-the-Correlation-between-Random-Variables-and-their-Principal-Components" class="headerlink" title="On the Correlation between Random Variables and their Principal Components"></a>On the Correlation between Random Variables and their Principal Components</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06139">http://arxiv.org/abs/2310.06139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zenon Gniazdowski</li>
<li>for: 本研究旨在找到Random Variables之间的相関系数，并使用线性代数方法来描述这些相关系数。</li>
<li>methods: 本研究使用了选取随机变数之间的统计量，然后使用 вектор和矩阵的概念来表述这些统计量的语言。这使得在后续步骤中可以 derivate预期的公式。</li>
<li>results: 研究发现，这个公式与因素分析中用来计算因素负载的公式相同。对于Principal Component Analysis中的主成分选择和因素分析中的因素数选择，这个公式也可以用来优化。<details>
<summary>Abstract</summary>
The article attempts to find an algebraic formula describing the correlation coefficients between random variables and the principal components representing them. As a result of the analysis, starting from selected statistics relating to individual random variables, the equivalents of these statistics relating to a set of random variables were presented in the language of linear algebra, using the concepts of vector and matrix. This made it possible, in subsequent steps, to derive the expected formula. The formula found is identical to the formula used in Factor Analysis to calculate factor loadings. The discussion showed that it is possible to apply this formula to optimize the number of principal components in Principal Component Analysis, as well as to optimize the number of factors in Factor Analysis.
</details>
<details>
<summary>摘要</summary>
文章尝试找到一个 алгебраическая方程描述Random Variables和它们的主成分之间的相关系数。经过分析，从选择的个体Random Variables的统计信息开始，使用线性代数概念 Vector和矩阵来表示这些统计信息的等价物。这使得在后续步骤中可以 derivate预期的方程。发现的方程与 фактор分析中计算因子负载的方程一样。文章还讨论了如何使用这个方程优化Principal Component Analysis中的主成分数量和Factor Analysis中的因子数量。Note: "Simplified Chinese" is a romanization of Chinese that uses a simplified set of characters and grammar rules to represent the language. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Theoretical-Analysis-of-Robust-Overfitting-for-Wide-DNNs-An-NTK-Approach"><a href="#Theoretical-Analysis-of-Robust-Overfitting-for-Wide-DNNs-An-NTK-Approach" class="headerlink" title="Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach"></a>Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06112">http://arxiv.org/abs/2310.06112</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fshp971/adv-ntk">https://github.com/fshp971/adv-ntk</a></li>
<li>paper_authors: Shaopeng Fu, Di Wang</li>
<li>for: 这篇论文主要是为了解释深度神经网络（DNN）中的对抗训练（AT）方法在Robustness方面的缺点。</li>
<li>methods: 该论文使用了神经积簇kernel（NTK）理论来扩展AT方法，并证明了一个攻击者训练的宽度DNN可以被近似为一个线性化DNN。</li>
<li>results: 该论文通过实验表明，使用Adv-NTK算法可以帮助无穷宽度DNN增强相对的Robustness，并且该结果证明了论文中的理论结论。<details>
<summary>Abstract</summary>
Adversarial training (AT) is a canonical method for enhancing the robustness of deep neural networks (DNNs). However, recent studies empirically demonstrated that it suffers from robust overfitting, i.e., a long time AT can be detrimental to the robustness of DNNs. This paper presents a theoretical explanation of robust overfitting for DNNs. Specifically, we non-trivially extend the neural tangent kernel (NTK) theory to AT and prove that an adversarially trained wide DNN can be well approximated by a linearized DNN. Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can be derived, which reveals a new AT degeneration phenomenon: a long-term AT will result in a wide DNN degenerates to that obtained without AT and thus cause robust overfitting. Based on our theoretical results, we further design a method namely Adv-NTK, the first AT algorithm for infinite-width DNNs. Experiments on real-world datasets show that Adv-NTK can help infinite-width DNNs enhance comparable robustness to that of their finite-width counterparts, which in turn justifies our theoretical findings. The code is available at https://github.com/fshp971/adv-ntk.
</details>
<details>
<summary>摘要</summary>
“对抗训练（AT）是深度神经网络（DNN）的一种标准方法，但是最近的研究表明，长期的AT可能对DNN的Robustness产生负面影响。这篇论文提供了DNN的Robust overfitting的理论解释。具体来说，我们将 neural tangent kernel（NTK）理论推广到AT，并证明了一个 adversarially trained wide DNN可以被linearized。此外，对于平方损失，我们可以 derivate closed-form AT dynamics for linearized DNN，这 revelas a new AT degeneration phenomenon：long-term AT will cause a wide DNN to degenerate into a DNN without AT, leading to robust overfitting。根据我们的理论结论，我们还设计了一种名为 Adv-NTK的AT算法，该算法可以帮助无限宽 DNN 提高相对的Robustness。实验表明，Adv-NTK可以帮助无限宽 DNN 提高与其有限宽 counterpart 的Robustness，这对我们的理论结论产生了正确的证明。代码可以在 https://github.com/fshp971/adv-ntk 中找到。”Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Grokking-as-the-Transition-from-Lazy-to-Rich-Training-Dynamics"><a href="#Grokking-as-the-Transition-from-Lazy-to-Rich-Training-Dynamics" class="headerlink" title="Grokking as the Transition from Lazy to Rich Training Dynamics"></a>Grokking as the Transition from Lazy to Rich Training Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06110">http://arxiv.org/abs/2310.06110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanishq Kumar, Blake Bordelon, Samuel J. Gershman, Cengiz Pehlevan</li>
<li>for: 该论文探讨了 Grokking 现象，即 neural network 的训练损失降低得比测试损失早得多，可能是由于 neural network 从懒散训练方式转移到了丰富的特征学习 régime。</li>
<li>methods: 作者通过使用普通的梯度下降法和二层神经网络在一个多项式回归问题上进行研究，发现 Grokking 现象不可能由现有理论解释。作者还提出了测试损失的充分统计，并在训练过程中跟踪这些统计，从而发现 Grokking 现象 arise 在神经网络首先尝试使用初始特征来适应kernel regression解决方案，然后在训练损失已经下降到低水平时发现一个泛化解决方案。</li>
<li>results: 作者发现 Grokking 现象的关键因素包括神经网络输出的速率（可以由输出参数控制）和初始特征与目标函数 $y(x)$ 的对齐度。当神经网络在初始特征学习 régime 中训练时，它会首先尝试适应kernel regression解决方案，然后在训练损失已经下降到低水平时发现一个泛化解决方案。此外，作者还发现这种延迟泛化 arise 在 dataset 大 enough，但不是太大，以致可以使神经网络泛化，但不是太早。<details>
<summary>Abstract</summary>
We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of vanilla gradient descent on a polynomial regression problem with a two layer neural network which exhibits grokking without regularization in a way that cannot be explained by existing theories. We identify sufficient statistics for the test loss of such a network, and tracking these over training reveals that grokking arises in this setting when the network first attempts to fit a kernel regression solution with its initial features, followed by late-time feature learning where a generalizing solution is identified after train loss is already low. We find that the key determinants of grokking are the rate of feature learning -- which can be controlled precisely by parameters that scale the network output -- and the alignment of the initial features with the target function $y(x)$. We argue this delayed generalization arises when (1) the top eigenvectors of the initial neural tangent kernel and the task labels $y(x)$ are misaligned, but (2) the dataset size is large enough so that it is possible for the network to generalize eventually, but not so large that train loss perfectly tracks test loss at all epochs, and (3) the network begins training in the lazy regime so does not learn features immediately. We conclude with evidence that this transition from lazy (linear model) to rich training (feature learning) can control grokking in more general settings, like on MNIST, one-layer Transformers, and student-teacher networks.
</details>
<details>
<summary>摘要</summary>
We found that the key determinants of grokking are the rate of feature learning, which can be controlled precisely by parameters that scale the network output, and the alignment of the initial features with the target function $y(x)$. We argue that delayed generalization arises when the top eigenvectors of the initial neural tangent kernel and the task labels $y(x)$ are misaligned, but the dataset size is large enough so that the network can generalize eventually, but not so large that the train loss perfectly tracks the test loss at all epochs. Additionally, the network begins training in the lazy regime, so it does not learn features immediately.We conclude that this transition from lazy (linear model) to rich training (feature learning) can control grokking in more general settings, such as on MNIST, one-layer Transformers, and student-teacher networks.
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Uncertainty-in-Deep-Learning-Classification-with-Noise-in-Discrete-Inputs-for-Risk-Based-Decision-Making"><a href="#Quantifying-Uncertainty-in-Deep-Learning-Classification-with-Noise-in-Discrete-Inputs-for-Risk-Based-Decision-Making" class="headerlink" title="Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making"></a>Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06105">http://arxiv.org/abs/2310.06105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maryam Kheirandish, Shengfan Zhang, Donald G. Catanzaro, Valeriu Crudu</li>
<li>for: 这篇论文的目的是为了提供一个数据类型为数字和分类的问题上的深度神经网络模型中的预测不确定性评估方法。</li>
<li>methods: 这篇论文使用的方法是基于 Bayesian deep learning 的方法，具体是使用 Monte Carlo dropout 和我们的提议的框架来评估预测不确定性。</li>
<li>results: 这篇论文的结果显示，我们的提议的框架可以更好地识别预测中的错误 случарес，并且比 Monte Carlo dropout 方法更能捕捉错误的情况。<details>
<summary>Abstract</summary>
The use of Deep Neural Network (DNN) models in risk-based decision-making has attracted extensive attention with broad applications in medical, finance, manufacturing, and quality control. To mitigate prediction-related risks in decision making, prediction confidence or uncertainty should be assessed alongside the overall performance of algorithms. Recent studies on Bayesian deep learning helps quantify prediction uncertainty arises from input noises and model parameters. However, the normality assumption of input noise in these models limits their applicability to problems involving categorical and discrete feature variables in tabular datasets. In this paper, we propose a mathematical framework to quantify prediction uncertainty for DNN models. The prediction uncertainty arises from errors in predictors that follow some known finite discrete distribution. We then conducted a case study using the framework to predict treatment outcome for tuberculosis patients during their course of treatment. The results demonstrate under a certain level of risk, we can identify risk-sensitive cases, which are prone to be misclassified due to error in predictors. Comparing to the Monte Carlo dropout method, our proposed framework is more aware of misclassification cases. Our proposed framework for uncertainty quantification in deep learning can support risk-based decision making in applications when discrete errors in predictors are present.
</details>
<details>
<summary>摘要</summary>
使用深度神经网络（DNN）模型在风险基础的决策中吸引了广泛的关注，应用于医疗、金融、制造和质量控制等领域。为了减少决策过程中的预测风险，需要同时评估算法的总性表现和预测uncertainty。 latest studies on Bayesian deep learning 可以量化预测uncertainty，但这些模型假设输入噪声是Normal分布，这限制了它们在具有分类和离散特征变量的表格数据集中的应用。在这篇论文中，我们提出了一个数学框架，可以量化DNN模型中的预测uncertainty。预测uncertainty来自预测器中的错误，这些错误遵循一些已知的有限离散分布。我们Then conducted a case study using the framework to predict treatment outcome for tuberculosis patients during their course of treatment. The results show that under a certain level of risk, we can identify risk-sensitive cases, which are prone to be misclassified due to error in predictors. Comparing to the Monte Carlo dropout method, our proposed framework is more aware of misclassification cases. Our proposed framework for uncertainty quantification in deep learning can support risk-based decision making in applications when discrete errors in predictors are present.
</details></li>
</ul>
<hr>
<h2 id="Transformers-and-Large-Language-Models-for-Chemistry-and-Drug-Discovery"><a href="#Transformers-and-Large-Language-Models-for-Chemistry-and-Drug-Discovery" class="headerlink" title="Transformers and Large Language Models for Chemistry and Drug Discovery"></a>Transformers and Large Language Models for Chemistry and Drug Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06083">http://arxiv.org/abs/2310.06083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andres M Bran, Philippe Schwaller</li>
<li>for: 这篇论文旨在探讨如何使用Transformer架构解决化学发现过程中的重要瓶颈问题，如retrosynthetic planning和化学空间探索。</li>
<li>methods: 这篇论文使用了Transformer架构，并将其应用于不同类型的数据，如线性化分子图、spectra、synthesis actions和人工语言。</li>
<li>results: 这篇论文描述了一种新的方法，可以通过自然语言的灵活性，解决化学问题。这种方法可以在不同的化学应用中使用，并且可以在将来的科学发现中扮演一个更重要的角色。<details>
<summary>Abstract</summary>
Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology. In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration. The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language. A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language. As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.
</details>
<details>
<summary>摘要</summary>
很多年来，语言模型在技术发展方面有了很大的进步，主要归功于Transformer架构的发明，这些架构的出现对机器学习多个领域产生了革命，包括化学和生物学。在这一章中，我们将探讨如何通过在化学和自然语言之间的相似性，使用Transformers来解决药物发现过程中的重要瓶颈，如逆 synthesis 规划和化学空间探索。这场革命从单一数据类型的模型开始，然后演进到包括其他数据类型，如分析器的spectra、合成操作和人类语言。现在，一新的趋势是利用大语言模型，让化学领域中的普遍任务得到解决，全部归功于自然语言的灵活性。我们继续探索和利用这些能力，未来machine learning在科学发现中的作用将变得更加重要。
</details></li>
</ul>
<hr>
<h2 id="Ito-Diffusion-Approximation-of-Universal-Ito-Chains-for-Sampling-Optimization-and-Boosting"><a href="#Ito-Diffusion-Approximation-of-Universal-Ito-Chains-for-Sampling-Optimization-and-Boosting" class="headerlink" title="Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting"></a>Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06081">http://arxiv.org/abs/2310.06081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aleksei Ustimenko, Aleksandr Beznosikov</li>
<li>for: 本文研究一种广泛和通用的马可夫链，即以爱因斯坦-玛丽亚偏抽象方式描述的某种随机 diffequation 的谱。</li>
<li>methods: 本文使用了 almost arbitrary 的各向异常和状态依赖的噪声，而不是通常使用的Normal和状态独立的噪声。此外，我们的链的涨落和扩散系数可以是不准确的，以涵盖广泛的应用，如某种 Stochastic Gradient Langevin Dynamics、sampling、Stochastic Gradient Descent 或 Stochastic Gradient Boosting。</li>
<li>results: 我们证明了 $W_{2}$-距离 между含义链和对应的随机 diffequation 的法律之间的上界。这些结果超越或覆盖了大多数已知的估计。此外，对某些特定情况，我们的分析是第一次。<details>
<summary>Abstract</summary>
This work considers a rather general and broad class of Markov chains, Ito chains that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and state-dependent noise instead of normal and state-independent one, as in most related papers. Moreover, our chain's drift and diffusion coefficient can be inexact to cover a wide range of applications such as Stochastic Gradient Langevin Dynamics, sampling, Stochastic Gradient Descent, or Stochastic Gradient Boosting. We prove an upper bound for $W_{2}$-distance between laws of the Ito chain and the corresponding Stochastic Differential Equation. These results improve or cover most of the known estimates. Moreover, for some particular cases, our analysis is the first.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Translation Notes:* "Markov chains" is translated as "Markov链" (Ma Ke Luo)* "Ito chains" is translated as "Itō链" (Ito Luo)* "Stochastic Differential Equation" is translated as "随机 diffe链方程" (Suī Jī Difu Luo Fang Jian)* "Wiener distance" is translated as "维纳度" (Wei Na Du)* "inexact" is translated as "不精确" (Bu Jing Ke)* "Stochastic Gradient Langevin Dynamics" is translated as "随机梯度兰格文运动" (Suī Jī Tiejian Langevin Yùndòng)* "sampling" is translated as "采样" (Cǎi Yàng)* "Stochastic Gradient Descent" is translated as "随机梯度下降" (Suī Jī Tiejian Xiào Jiàng)* "Stochastic Gradient Boosting" is translated as "随机梯度增强" (Suī Jī Tiejian Zēng Qiáng)
</details></li>
</ul>
<hr>
<h2 id="Optimal-Exploration-is-no-harder-than-Thompson-Sampling"><a href="#Optimal-Exploration-is-no-harder-than-Thompson-Sampling" class="headerlink" title="Optimal Exploration is no harder than Thompson Sampling"></a>Optimal Exploration is no harder than Thompson Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06069">http://arxiv.org/abs/2310.06069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoqi Li, Kevin Jamieson, Lalit Jain</li>
<li>for: This paper aims to solve the pure exploration linear bandit problem with high probability through noisy measurements of $x^{\top}\theta_{\ast}$.</li>
<li>methods: The paper proposes an algorithm that leverages only sampling and argmax oracles and achieves an exponential convergence rate with the optimal exponent among all possible allocations asymptotically.</li>
<li>results: The algorithm proposed in the paper can be easily implemented and performs as well empirically as existing asymptotically optimal methods.Here is the same information in Simplified Chinese:</li>
<li>for: 本 paper 目标是解决纯exploration linear bandit问题，通过各种噪声测量 $x^{\top}\theta_{\ast}$ 来寻找最优解。</li>
<li>methods: 本 paper 提出了一种基于抽象和最大值 oracle 的算法，可以在高probability下实现快速收敛率，并且可以证明这种算法在所有分配中具有最优的幂率。</li>
<li>results: 本 paper 提出的算法可以轻松实现并与现有的 asymptotically 优化方法相当。<details>
<summary>Abstract</summary>
Given a set of arms $\mathcal{Z}\subset \mathbb{R}^d$ and an unknown parameter vector $\theta_\ast\in\mathbb{R}^d$, the pure exploration linear bandit problem aims to return $\arg\max_{z\in \mathcal{Z} z^{\top}\theta_{\ast}$, with high probability through noisy measurements of $x^{\top}\theta_{\ast}$ with $x\in \mathcal{X}\subset \mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\in \mathcal{Z}$ or b) explicitly maintaining a subset of $\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same computational primitives as Thompson Sampling? We answer the question in the affirmative. We provide an algorithm that leverages only sampling and argmax oracles and achieves an exponential convergence rate, with the exponent being the optimal among all possible allocations asymptotically. In addition, we show that our algorithm can be easily implemented and performs as well empirically as existing asymptotically optimal methods.
</details>
<details>
<summary>摘要</summary>
In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same computational primitives as Thompson Sampling? We answer the question in the affirmative. We provide an algorithm that leverages only sampling and argmax oracles and achieves an exponential convergence rate, with the exponent being the optimal among all possible allocations asymptotically.Moreover, we show that our algorithm can be easily implemented and performs as well empirically as existing asymptotically optimal methods.
</details></li>
</ul>
<hr>
<h2 id="Early-Warning-via-tipping-preserving-latent-stochastic-dynamical-system-and-meta-label-correcting"><a href="#Early-Warning-via-tipping-preserving-latent-stochastic-dynamical-system-and-meta-label-correcting" class="headerlink" title="Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting"></a>Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06059">http://arxiv.org/abs/2310.06059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Zhang, Ting Gao, Jin Guo, Jinqiao Duan</li>
<li>for: 预测 эпилепсию患者的症状，以提高 их安全性和健康状况。</li>
<li>methods: 基于患者的EEG数据，提出了一种基于meta学习框架的预测方法，利用了meta标签修正方法，并通过优化 latent Stochastic differential equation(SDE) 中的信息，选择最佳的 latent 动力系统。</li>
<li>results: 通过实验 validate 了我们的方法，发现预测精度有surprisingly的增加。<details>
<summary>Abstract</summary>
Early warning for epilepsy patients is crucial for their safety and well-being, in terms of preventing or minimizing the severity of seizures. Through the patients' EEG data, we propose a meta learning framework for improving prediction on early ictal signals. To better utilize the meta label corrector method, we fuse the information from both the real data and the augmented data from the latent Stochastic differential equation(SDE). Besides, we also optimally select the latent dynamical system via distribution of transition time between real data and that from the latent SDE. In this way, the extracted tipping dynamical feature is also integrated into the meta network to better label the noisy data. To validate our method, LSTM is implemented as the baseline model. We conduct a series of experiments to predict seizure in various long-term window from 1-2 seconds input data and find surprisingly increment of prediction accuracy.
</details>
<details>
<summary>摘要</summary>
早期警告对 эпилепси patients 的安全和健康至关重要，以预防或减轻癫痫症发作的严重程度。通过患者的 EEG 数据，我们提议一种meta学框架，以提高预测早期癫痫症信号的精度。为了更好地利用meta标签修复方法，我们将真实数据和潜在数据从射频 diferencial equation（SDE）的 latent 信息 fusion。此外，我们还优化了 latent 动力系统的选择，通过transition时间分布 между real data和 latent SDE 中的数据来实现。这样，提取的折冲动力特征也被 интегрирова到 meta 网络中，以更好地标注噪音数据。为验证我们的方法，LSTM 被实现为基eline模型。我们进行了一系列实验，用1-2秒输入数据预测癫痫症，并发现了奇异的增加预测精度。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distillation-for-Anomaly-Detection"><a href="#Knowledge-Distillation-for-Anomaly-Detection" class="headerlink" title="Knowledge Distillation for Anomaly Detection"></a>Knowledge Distillation for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06047">http://arxiv.org/abs/2310.06047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HibikiJie/Multiresolution-Knowledge-Distillation-for-Anomaly-Detection">https://github.com/HibikiJie/Multiresolution-Knowledge-Distillation-for-Anomaly-Detection</a></li>
<li>paper_authors: Adrian Alan Pol, Ekaterina Govorkova, Sonja Gronroos, Nadezda Chernyavskaya, Philip Harris, Maurizio Pierini, Isobel Ojalvo, Peter Elmer</li>
<li>for: 用于压缩无监督深度学习模型，以便在有限资源的设备上部署。</li>
<li>methods: 使用知识储存法压缩无监督异常检测模型，并提出一些改进检测敏感度的技巧。</li>
<li>results: 压缩模型与原始模型的性能相似，而减少大小和内存占用。<details>
<summary>Abstract</summary>
Unsupervised deep learning techniques are widely used to identify anomalous behaviour. The performance of such methods is a product of the amount of training data and the model size. However, the size is often a limiting factor for the deployment on resource-constrained devices. We present a novel procedure based on knowledge distillation for compressing an unsupervised anomaly detection model into a supervised deployable one and we suggest a set of techniques to improve the detection sensitivity. Compressed models perform comparably to their larger counterparts while significantly reducing the size and memory footprint.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。</SYS>>无监督深度学习技术广泛用于异常行为识别。这些方法的性能与训练数据量和模型大小相乘。然而，大小往往是部署在资源受限的设备上的限制因素。我们提出了一种基于知识储存的新方法，可以压缩无监督异常检测模型成可部署的超vised模型，并提出了一些提高检测敏感度的技巧。压缩模型与其更大的对手相比，性能相似，却减少了大小和内存占用。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Decision-Theory-Safe-Autonomous-Decisions-from-Imperfect-Predictions"><a href="#Conformal-Decision-Theory-Safe-Autonomous-Decisions-from-Imperfect-Predictions" class="headerlink" title="Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions"></a>Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05921">http://arxiv.org/abs/2310.05921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Lekeufack, Anastasios N. Angelopoulos, Andrea Bajcsy, Michael I. Jordan, Jitendra Malik</li>
<li>for: 该论文旨在提供一种安全的自动决策框架，即使机器学习预测不准确。</li>
<li>methods: 该论文使用了准确预测理论，无需假设世界模型。</li>
<li>results: 实验表明，该方法在机器人运动规划、自动股票交易和机器人生产中具有实用性。<details>
<summary>Abstract</summary>
We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturing.
</details>
<details>
<summary>摘要</summary>
我们介绍了对准决策理论，一个框架用于生成安全的自动决策，即使机器学习预测不完美。这些决策的例子非常普遍，包括 robot 观察算法依赖人类预测，将自动生产调整为具有高速和低错误，以及在执行时是否信任主要政策或者转折到安全备用政策。我们的算法生成的决策是安全的，即具有可证的Statistical guarantee of low risk，不需要世界模型的任何假设，观察不必I.I.D.，甚至可以是反对的。我们的理论扩展了对准预测的结果，直接将决策calibrate，不需要建立预测集。实验展示了我们的方法在人类附近的机器人运动规划、自动股票交易和机器人生产中的 utility。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Decode-the-Surface-Code-with-a-Recurrent-Transformer-Based-Neural-Network"><a href="#Learning-to-Decode-the-Surface-Code-with-a-Recurrent-Transformer-Based-Neural-Network" class="headerlink" title="Learning to Decode the Surface Code with a Recurrent, Transformer-Based Neural Network"></a>Learning to Decode the Surface Code with a Recurrent, Transformer-Based Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05900">http://arxiv.org/abs/2310.05900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes Bausch, Andrew W Senior, Francisco J H Heras, Thomas Edlich, Alex Davies, Michael Newman, Cody Jones, Kevin Satzinger, Murphy Yuezhen Niu, Sam Blackwell, George Holland, Dvir Kafri, Juan Atalaya, Craig Gidney, Demis Hassabis, Sergio Boixo, Hartmut Neven, Pushmeet Kohli</li>
<li>for: 这个论文的目的是提高量子计算的可靠性，通过使用机器学习来解码量子错误 correction 代码。</li>
<li>methods: 这个论文使用了循环、变换器基本的神经网络，通过直接学习数据来解码表面码。</li>
<li>results: 论文的解码器在实际数据上（Google Sycamore 量子处理器）以及模拟数据上（包括干扰和误差）都有优异表现，可以覆盖距离3和5表面码，并且在训练时间25个循环后仍保持高准确率。<details>
<summary>Abstract</summary>
Quantum error-correction is a prerequisite for reliable quantum computation. Towards this goal, we present a recurrent, transformer-based neural network which learns to decode the surface code, the leading quantum error-correction code. Our decoder outperforms state-of-the-art algorithmic decoders on real-world data from Google's Sycamore quantum processor for distance 3 and 5 surface codes. On distances up to 11, the decoder maintains its advantage on simulated data with realistic noise including cross-talk, leakage, and analog readout signals, and sustains its accuracy far beyond the 25 cycles it was trained on. Our work illustrates the ability of machine learning to go beyond human-designed algorithms by learning from data directly, highlighting machine learning as a strong contender for decoding in quantum computers.
</details>
<details>
<summary>摘要</summary>
量子错误纠正是可靠量子计算的必要前提。为达到这个目标，我们提出了一种循环、转换器基于神经网络，可以学习解码表面码，这是量子错误纠正代码的领先代码。我们的解码器在Google的Sycamore量子处理器上的真实数据上表现出优于当前最佳算法解码器，在距离3和5表面码上出现了优异表现。在距离11上，我们的解码器在实际噪音，包括交叠、泄漏和分析读取信号的 simulate 数据上维持了其优势，并保持了其精度远远超出了它被训练的25次。我们的工作表明了机器学习可以超越人类设计的算法，通过直接学习数据，机器学习成为量子计算中的强有力竞争者。
</details></li>
</ul>
<hr>
<h2 id="A-Generalization-Bound-of-Deep-Neural-Networks-for-Dependent-Data"><a href="#A-Generalization-Bound-of-Deep-Neural-Networks-for-Dependent-Data" class="headerlink" title="A Generalization Bound of Deep Neural Networks for Dependent Data"></a>A Generalization Bound of Deep Neural Networks for Dependent Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05892">http://arxiv.org/abs/2310.05892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/umd-huang-lab/neural-net-generalization-via-tensor">https://github.com/umd-huang-lab/neural-net-generalization-via-tensor</a></li>
<li>paper_authors: Quan Huu Do, Binh T. Nguyen, Lam Si Tung Ho</li>
<li>for: 这个研究是为了提供非站勤$\phi$-混合数据上的对抗学习网络对应。</li>
<li>methods: 本研究使用了对抗学习网络，并提出了一个新的一致性矩阵bound。</li>
<li>results: 研究发现，这个一致性矩阵bound可以对非站勤$\phi$-混合数据进行预测，并且比旧有的 bound 更为精确。<details>
<summary>Abstract</summary>
Existing generalization bounds for deep neural networks require data to be independent and identically distributed (iid). This assumption may not hold in real-life applications such as evolutionary biology, infectious disease epidemiology, and stock price prediction. This work establishes a generalization bound of feed-forward neural networks for non-stationary $\phi$-mixing data.
</details>
<details>
<summary>摘要</summary>
现有的总体化约束要求深度神经网络数据必须是独立并且相同分布（iid）。这个假设可能不成立在实际应用中，如生物进化、感染病毒流行病学和股票价格预测。这项工作建立了非站ARY $\phi$-混合数据的含积总体化约束。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Approach-to-Predicting-Single-Event-Upsets"><a href="#A-Machine-Learning-Approach-to-Predicting-Single-Event-Upsets" class="headerlink" title="A Machine Learning Approach to Predicting Single Event Upsets"></a>A Machine Learning Approach to Predicting Single Event Upsets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05878">http://arxiv.org/abs/2310.05878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/architg1/CREMER">https://github.com/architg1/CREMER</a></li>
<li>paper_authors: Archit Gupta, Chong Yock Eng, Deon Lim Meng Wee, Rashna Analia Ahmed, See Min Sim</li>
<li>for: 预测单个事件异常 (SEU) 的发生，以提高半导体设备的可靠性。</li>
<li>methods: 使用机器学习技术，只使用位置数据预测 SEU 发生。</li>
<li>results: 提高半导体设备的可靠性，创造更安全的数字环境。<details>
<summary>Abstract</summary>
A single event upset (SEU) is a critical soft error that occurs in semiconductor devices on exposure to ionising particles from space environments. SEUs cause bit flips in the memory component of semiconductors. This creates a multitude of safety hazards as stored information becomes less reliable. Currently, SEUs are only detected several hours after their occurrence. CREMER, the model presented in this paper, predicts SEUs in advance using machine learning. CREMER uses only positional data to predict SEU occurrence, making it robust, inexpensive and scalable. Upon implementation, the improved reliability of memory devices will create a digitally safer environment onboard space vehicles.
</details>
<details>
<summary>摘要</summary>
一个单一事件冲击（SEU）是半导体设备中critical soft error的一种重要问题，它由宇宙射线粒子引起，导致内存组件中的比特跳变。这会导致存储的信息变得更加不可靠，带来多种安全风险。目前，SEU的发生只能在several hours后被探测出来。本文中提出的CREMER模型使用机器学习技术预测SEU发生，只使用位置数据，因此具有robust、便宜和可扩展的特点。在实施后，内存设备的可靠性会得到改善，从而在空间 vehicles上创造出一个更加数字安全的环境。Note: "宇宙射线粒子" in the text refers to ionising particles from space environments.
</details></li>
</ul>
<hr>
<h2 id="Bio-inspired-computational-memory-model-of-the-Hippocampus-an-approach-to-a-neuromorphic-spike-based-Content-Addressable-Memory"><a href="#Bio-inspired-computational-memory-model-of-the-Hippocampus-an-approach-to-a-neuromorphic-spike-based-Content-Addressable-Memory" class="headerlink" title="Bio-inspired computational memory model of the Hippocampus: an approach to a neuromorphic spike-based Content-Addressable Memory"></a>Bio-inspired computational memory model of the Hippocampus: an approach to a neuromorphic spike-based Content-Addressable Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05868">http://arxiv.org/abs/2310.05868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Casanueva-Morato, Alvaro Ayuso-Martinez, Juan P. Dominguez-Morales, Angel Jimenez-Fernandez, Gabriel Jimenez-Moreno</li>
<li>for: 这篇论文目的是开发一种基于海马 CA3 区域的生物体现学习系统，能够学习、忘记和回忆非正式的记忆 fragment。</li>
<li>methods: 该模型使用脉冲神经网络（SNN）和SpiNNaker 硬件平台实现，并进行了功能、压力和实用性测试。</li>
<li>results: 该模型可以学习、忘记和回忆非正式的记忆 fragment，并且在不同的压力和环境下能够正常工作。这是首次实现了一个完全可工作的生物体现学习系统，将为未来的更复杂的neuromorphic系统开拓新的可能性。<details>
<summary>Abstract</summary>
The brain has computational capabilities that surpass those of modern systems, being able to solve complex problems efficiently in a simple way. Neuromorphic engineering aims to mimic biology in order to develop new systems capable of incorporating such capabilities. Bio-inspired learning systems continue to be a challenge that must be solved, and much work needs to be done in this regard. Among all brain regions, the hippocampus stands out as an autoassociative short-term memory with the capacity to learn and recall memories from any fragment of them. These characteristics make the hippocampus an ideal candidate for developing bio-inspired learning systems that, in addition, resemble content-addressable memories. Therefore, in this work we propose a bio-inspired spiking content-addressable memory model based on the CA3 region of the hippocampus with the ability to learn, forget and recall memories, both orthogonal and non-orthogonal, from any fragment of them. The model was implemented on the SpiNNaker hardware platform using Spiking Neural Networks. A set of experiments based on functional, stress and applicability tests were performed to demonstrate its correct functioning. This work presents the first hardware implementation of a fully-functional bio-inspired spiking hippocampal content-addressable memory model, paving the way for the development of future more complex neuromorphic systems.
</details>
<details>
<summary>摘要</summary>
脑有计算能力，超过现代系统，能够解决复杂问题，使用简单的方式。神经科工程尝试模仿生物，以开发新的系统，拥有这种能力。生物启发式学习系统仍然是一个挑战，需要进一步的研究。脑中的梨膜区（CA3）是一种自动相关短期记忆，具有学习和记忆任何段落的能力。这些特点使得梨膜区成为开发生物启发式学习系统的理想选择。因此，在这项工作中，我们提出了基于CA3区的生物启发式脉冲记忆模型，具有学习、忘记和记忆任何段落的能力。该模型在SpiNNaker硬件平台上使用脉冲神经网络进行实现。我们对模型进行了功能、压力和实用性测试，以证明其正常工作。这项工作展示了首次实现了完全可用的生物启发式脉冲梨膜区内存模型，开创了未来更复杂的神经omorphic系统的发展之路。
</details></li>
</ul>
<hr>
<h2 id="DSAC-T-Distributional-Soft-Actor-Critic-with-Three-Refinements"><a href="#DSAC-T-Distributional-Soft-Actor-Critic-with-Three-Refinements" class="headerlink" title="DSAC-T: Distributional Soft Actor-Critic with Three Refinements"></a>DSAC-T: Distributional Soft Actor-Critic with Three Refinements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05858">http://arxiv.org/abs/2310.05858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingliang-duan/dsac-t">https://github.com/jingliang-duan/dsac-t</a></li>
<li>paper_authors: Jingliang Duan, Wenxuan Wang, Liming Xiao, Jiaxin Gao, Shengbo Eben Li</li>
<li>for: 提高模型自适应RL方法的性能，解决常见的过估问题。</li>
<li>methods: 使用分布式软actor-critic算法（DSAC），并进行了三种改进：批处理梯度调整、双值分布学习和 variance-based target return clipping。</li>
<li>results: 在多种环境中，DSAC-T超过了多种主流模型自适应RL算法，包括SAC、TD3、DDPG、TRPO和PPO，而且保证了高稳定性的学习过程和不同奖励缩放下的相似性。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has proven to be highly effective in tackling complex decision-making and control tasks. However, prevalent model-free RL methods often face severe performance degradation due to the well-known overestimation issue. In response to this problem, we recently introduced an off-policy RL algorithm, called distributional soft actor-critic (DSAC or DSAC-v1), which can effectively improve the value estimation accuracy by learning a continuous Gaussian value distribution. Nonetheless, standard DSAC has its own shortcomings, including occasionally unstable learning processes and needs for task-specific reward scaling, which may hinder its overall performance and adaptability in some special tasks. This paper further introduces three important refinements to standard DSAC in order to address these shortcomings. These refinements consist of critic gradient adjusting, twin value distribution learning, and variance-based target return clipping. The modified RL algorithm is named as DSAC with three refinements (DSAC-T or DSAC-v2), and its performances are systematically evaluated on a diverse set of benchmark tasks. Without any task-specific hyperparameter tuning, DSAC-T surpasses a lot of mainstream model-free RL algorithms, including SAC, TD3, DDPG, TRPO, and PPO, in all tested environments. Additionally, DSAC-T, unlike its standard version, ensures a highly stable learning process and delivers similar performance across varying reward scales.
</details>
<details>
<summary>摘要</summary>
“强化学习（RL）已经证明可以很好地解决复杂的决策和控制任务。然而，广泛使用的无策法RL方法经常会遭遇估计问题，导致性能下降。为了解决这个问题，我们最近提出了一种偏离策略RL算法，称为分布型软actor-批评（DSAC或DSAC-v1），可以有效地提高价值估计准确性。然而，标准DSAC有一些缺点，包括 occasionally 不稳定的学习过程和需要任务特定的奖励滤波，这可能会限制其总体性能和适应性。这篇文章进一步介绍了三种重要的DSAC改进，包括评价函数梯度调整、双值分布学习和归一化目标返回截卷。改进后的RL算法被称为DSAC-T或DSAC-v2，其性能在多种环境中进行系统性评估。无需任务特定的超参数调整，DSAC-T比许多主流无策法RL算法，包括SAC、TD3、DDPG、TRPO和PPO，在所有测试环境中表现出色。此外，DSAC-T不同于标准版本，可以保证学习过程非常稳定，并在不同的奖励档次下提供相似的性能。”
</details></li>
</ul>
<hr>
<h2 id="Improved-Communication-Efficiency-in-Federated-Natural-Policy-Gradient-via-ADMM-based-Gradient-Updates"><a href="#Improved-Communication-Efficiency-in-Federated-Natural-Policy-Gradient-via-ADMM-based-Gradient-Updates" class="headerlink" title="Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates"></a>Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19807">http://arxiv.org/abs/2310.19807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangchen Lan, Han Wang, James Anderson, Christopher Brinton, Vaneet Aggarwal</li>
<li>for: 这 paper 旨在解决 Federated reinforcement learning (FedRL) 中高度通信开销的问题，尤其是在 natural policy gradient (NPG) 方法中，以提高training效率。</li>
<li>methods: 该 paper 提出了 FedNPG-ADMM 框架，通过 alternating direction method of multipliers (ADMM) 方法来近似全局 NPG 方向，从而提高了training efficiency。</li>
<li>results: 该 paper  theoretically 表明，使用 ADMM-based gradient updates 可以将 communication complexity 降低至 ${O}({d})$，其中 $d$ 是模型参数的数量。此外，paper 还证明了 FedNPG-ADMM 可以保持和标准 FedNPG 相同的 convergence rate。通过在 MuJoCo 环境中评估该 algorithm，paper 还证明了 FedNPG-ADMM 可以保持 reward performance，并且当Agent 数量增加时，其 convergence rate 会提高。<details>
<summary>Abstract</summary>
Federated reinforcement learning (FedRL) enables agents to collaboratively train a global policy without sharing their individual data. However, high communication overhead remains a critical bottleneck, particularly for natural policy gradient (NPG) methods, which are second-order. To address this issue, we propose the FedNPG-ADMM framework, which leverages the alternating direction method of multipliers (ADMM) to approximate global NPG directions efficiently. We theoretically demonstrate that using ADMM-based gradient updates reduces communication complexity from ${O}({d^{2})$ to ${O}({d})$ at each iteration, where $d$ is the number of model parameters. Furthermore, we show that achieving an $\epsilon$-error stationary convergence requires ${O}(\frac{1}{(1-\gamma)^{2}{\epsilon})$ iterations for discount factor $\gamma$, demonstrating that FedNPG-ADMM maintains the same convergence rate as the standard FedNPG. Through evaluation of the proposed algorithms in MuJoCo environments, we demonstrate that FedNPG-ADMM maintains the reward performance of standard FedNPG, and that its convergence rate improves when the number of federated agents increases.
</details>
<details>
<summary>摘要</summary>
federated reinforcement learning (FedRL) 允许代理共同训练全局策略，不需要分享个人数据。然而，交通开销仍然是critical bottleneck，特别是用natural policy gradient (NPG) 方法，这些方法是second-order。为解决这个问题，我们提出了FedNPG-ADMM框架，它利用了alternating direction method of multipliers (ADMM)来高效地计算全局NPG方向。我们 teorically 表明，使用 ADMM-based Gradient更新可以将交通复杂度从 $O(d^2)$ 降低到 $O(d)$ at each iteration，where $d$ 是模型参数的数量。此外，我们还证明了在 $\epsilon$-error stationary convergence 下，FedNPG-ADMM 需要 ${O}(\frac{1}{(1-\gamma)^{2}{\epsilon})$ 迭代，这与标准 FedNPG 的迭代速率相同。通过在 MuJoCo 环境中评估提议的算法，我们表明了FedNPG-ADMM 可以保持标准 FedNPG 的奖励性能，并且当多个联合代理增加时，其迭代速率会提高。
</details></li>
</ul>
<hr>
<h2 id="Robust-Angular-Synchronization-via-Directed-Graph-Neural-Networks"><a href="#Robust-Angular-Synchronization-via-Directed-Graph-Neural-Networks" class="headerlink" title="Robust Angular Synchronization via Directed Graph Neural Networks"></a>Robust Angular Synchronization via Directed Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05842">http://arxiv.org/abs/2310.05842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixuan He, Gesine Reinert, David Wipf, Mihai Cucuringu</li>
<li>for:  angular synchronization problem and its heterogeneous extension (sensor network localization, phase retrieval, and distributed clock synchronization)</li>
<li>methods:  directed graph neural networks and new loss functions</li>
<li>results:  competitive and often superior performance against a comprehensive set of baselines, validating the robustness of GNNSync even at high noise levels.<details>
<summary>Abstract</summary>
The angular synchronization problem aims to accurately estimate (up to a constant additive phase) a set of unknown angles $\theta_1, \dots, \theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets $\theta_i-\theta_j \;\mbox{mod} \; 2\pi.$ Applications include, for example, sensor network localization, phase retrieval, and distributed clock synchronization. An extension of the problem to the heterogeneous setting (dubbed $k$-synchronization) is to estimate $k$ groups of angles simultaneously, given noisy observations (with unknown group assignment) from each group. Existing methods for angular synchronization usually perform poorly in high-noise regimes, which are common in applications. In this paper, we leverage neural networks for the angular synchronization problem, and its heterogeneous extension, by proposing GNNSync, a theoretically-grounded end-to-end trainable framework using directed graph neural networks. In addition, new loss functions are devised to encode synchronization objectives. Experimental results on extensive data sets demonstrate that GNNSync attains competitive, and often superior, performance against a comprehensive set of baselines for the angular synchronization problem and its extension, validating the robustness of GNNSync even at high noise levels.
</details>
<details>
<summary>摘要</summary>
“angular synchronization problem”targets to accurately estimate（up to a constant additive phase）a set of unknown angles $\theta_1, \dots, \theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets $\theta_i-\theta_j \;\mbox{mod} \; 2\pi$. Applications include sensor network localization, phase retrieval, and distributed clock synchronization. An extension of the problem to the heterogeneous setting（dubbed $k$-synchronization）is to estimate $k$ groups of angles simultaneously, given noisy observations（with unknown group assignment）from each group. Existing methods for angular synchronization usually perform poorly in high-noise regimes, which are common in applications. In this paper, we leverage neural networks for the angular synchronization problem and its heterogeneous extension by proposing GNNSync, a theoretically-grounded end-to-end trainable framework using directed graph neural networks. In addition, new loss functions are devised to encode synchronization objectives. Experimental results on extensive data sets demonstrate that GNNSync attains competitive, and often superior, performance against a comprehensive set of baselines for the angular synchronization problem and its extension, validating the robustness of GNNSync even at high noise levels.
</details></li>
</ul>
<hr>
<h2 id="A-Bias-Variance-Covariance-Decomposition-of-Kernel-Scores-for-Generative-Models"><a href="#A-Bias-Variance-Covariance-Decomposition-of-Kernel-Scores-for-Generative-Models" class="headerlink" title="A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models"></a>A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05833">http://arxiv.org/abs/2310.05833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian G. Gruber, Florian Buettner</li>
<li>for: 这篇论文的目的是为了提供一种评估生成模型的泛化性和不确定性的理论框架。</li>
<li>methods: 该论文使用了对kernel scores的偏差-弹性-协方差分解，并提出了不偏的和一致的估计器，只需要生成的样本而不需要下游模型。</li>
<li>results: 该论文的应用是评估扩散模型的泛化评估和发现了少数群体的极化现象，以及验证了干扰和预测卷积 entropy 作为生成模型的不确定性度量。<details>
<summary>Abstract</summary>
Generative models, like large language models, are becoming increasingly relevant in our daily lives, yet a theoretical framework to assess their generalization behavior and uncertainty does not exist. Particularly, the problem of uncertainty estimation is commonly solved in an ad-hoc manner and task dependent. For example, natural language approaches cannot be transferred to image generation. In this paper we introduce the first bias-variance-covariance decomposition for kernel scores and their associated entropy. We propose unbiased and consistent estimators for each quantity which only require generated samples but not the underlying model itself. As an application, we offer a generalization evaluation of diffusion models and discover how mode collapse of minority groups is a contrary phenomenon to overfitting. Further, we demonstrate that variance and predictive kernel entropy are viable measures of uncertainty for image, audio, and language generation. Specifically, our approach for uncertainty estimation is more predictive of performance on CoQA and TriviaQA question answering datasets than existing baselines and can also be applied to closed-source models.
</details>
<details>
<summary>摘要</summary>
大量语言模型在我们日常生活中变得越来越重要，然而一个有效的理论框架来评估它们的泛化行为和不确定性并没有出现。特别是不确定性估计问题通常采用做出的方式和任务相关。例如，自然语言方法无法被转移到图像生成。在这篇论文中，我们介绍了首个偏差-变量- covariance 分解 для核分数和它们相关的熵。我们提议不偏和一致的估计器，只需要生成的样本而不需要下游模型本身。作为应用，我们对扩散模型进行总体评估，发现扩散的小组聚合是对权重过拟合的反应。此外，我们发现了变量和预测核熵是图像、音频和语言生成中的不确定性度量。 Specifically，我们的不确定性估计方法在CoQA和TriviaQA问答数据集上的性能预测比现有基elines高，并且可以应用于关闭源模型。
</details></li>
</ul>
<hr>
<h2 id="Pre-trained-Spatial-Priors-on-Multichannel-NMF-for-Music-Source-Separation"><a href="#Pre-trained-Spatial-Priors-on-Multichannel-NMF-for-Music-Source-Separation" class="headerlink" title="Pre-trained Spatial Priors on Multichannel NMF for Music Source Separation"></a>Pre-trained Spatial Priors on Multichannel NMF for Music Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05821">http://arxiv.org/abs/2310.05821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Cabanas-Molero, Antonio J. Munoz-Montoro, Julio Carabias-Orti, Pedro Vera-Candeas</li>
<li>for: 这个论文提出了一种基于录音设置信息的声音来源分离方法，可以应用于现有的室内乐录音设置。</li>
<li>methods: 该方法使用 solo 段来训练空间混合筛选器，以捕捉室内回声和扬声器响应的信息。然后将这个预训练过的筛选器integrated into a multichannel non-negative matrix factorization 方法，以更好地捕捉不同声音来源的方差。</li>
<li>results: 实验表明，该提出的框架可以更好地分离声音来源，比传统的 MNMF 方法提高性能。<details>
<summary>Abstract</summary>
This paper presents a novel approach to sound source separation that leverages spatial information obtained during the recording setup. Our method trains a spatial mixing filter using solo passages to capture information about the room impulse response and transducer response at each sensor location. This pre-trained filter is then integrated into a multichannel non-negative matrix factorization (MNMF) scheme to better capture the variances of different sound sources. The recording setup used in our experiments is the typical setup for orchestra recordings, with a main microphone and a close "cardioid" or "supercardioid" microphone for each section of the orchestra. This makes the proposed method applicable to many existing recordings. Experiments on polyphonic ensembles demonstrate the effectiveness of the proposed framework in separating individual sound sources, improving performance compared to conventional MNMF methods.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的声音源分离方法，利用录制过程中获得的空间信息。我们的方法使用独奏段来训练一个空间混合 filters，以捕捉室内响应和传播器响应在每个感知器位置上的信息。这个预训练过的滤波器然后被 интеGRATED INTO a multichannel non-negative matrix factorization (MNMF) 方案，以更好地捕捉不同声音源的方差。我们的实验使用了典型的乐团录制设置，即主 Mikrofon 和每个乐器部分的 "cardioid" 或 "supercardioid" Mikrofon。这使得我们的方法可以应用于许多现有的录音。实验表明，我们的框架可以更有效地分离声音源，与传统的 MNMF 方法相比，对多重演奏体示出了更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Sharing-Information-Between-Machine-Tools-to-Improve-Surface-Finish-Forecasting"><a href="#Sharing-Information-Between-Machine-Tools-to-Improve-Surface-Finish-Forecasting" class="headerlink" title="Sharing Information Between Machine Tools to Improve Surface Finish Forecasting"></a>Sharing Information Between Machine Tools to Improve Surface Finish Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05807">http://arxiv.org/abs/2310.05807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel R. Clarkson, Lawrence A. Bull, Tina A. Dardeno, Chandula T. Wickramarachchi, Elizabeth J. Cross, Timothy J. Rogers, Keith Worden, Nikolaos Dervilis, Aidan J. Hughes</li>
<li>for: 预测机器制造过程中表面质量</li>
<li>methods:  bayesian hierarchical model、bayesian linear regression</li>
<li>results: 提高预测精度和不确定性评估In Simplified Chinese text:</li>
<li>for: 用于预测机器制造过程中的表面质量</li>
<li>methods: 使用 bayesian hierarchical model 和 bayesian linear regression</li>
<li>results: 提高预测精度和不确定性评估<details>
<summary>Abstract</summary>
At present, most surface-quality prediction methods can only perform single-task prediction which results in under-utilised datasets, repetitive work and increased experimental costs. To counter this, the authors propose a Bayesian hierarchical model to predict surface-roughness measurements for a turning machining process. The hierarchical model is compared to multiple independent Bayesian linear regression models to showcase the benefits of partial pooling in a machining setting with respect to prediction accuracy and uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Here's the text in Simplified Chinese:当前，大多数表面质量预测方法只能进行单任务预测，这会导致数据被占用不足，重复工作和实验成本增加。为了解决这个问题，作者提议了一种折衣概率模型，用于预测转动加工过程中表面粗糙度测量值。这个模型与多个独立的折衣线性回归模型进行比较，以示出部分汇集在机床设备中的优点，包括预测精度和不确定性评估的提高。
</details></li>
</ul>
<hr>
<h2 id="Boosted-Control-Functions"><a href="#Boosted-Control-Functions" class="headerlink" title="Boosted Control Functions"></a>Boosted Control Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05805">http://arxiv.org/abs/2310.05805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zszszszsz/.config">https://github.com/zszszszsz/.config</a></li>
<li>paper_authors: Nicola Gnecco, Jonas Peters, Sebastian Engelke, Niklas Pfister</li>
<li>for: 这篇研究旨在bridging the gap between existing prediction methods and the presence of hidden confounding, especially when the training and testing data are different.</li>
<li>methods: 本研究使用了distribution generalization from machine learning和simultaneous equation models and control function from econometrics，并提出了一新的同时方程模型（SIMDG）来描述资料生成过程下的分布差异。</li>
<li>results: 研究发现了一个强制条件（boosted control function，BCF），可以在不同的训练和测试数据下预测成功，并且提供了必要和充分的条件来识别BCF。<details>
<summary>Abstract</summary>
Modern machine learning methods and the availability of large-scale data opened the door to accurately predict target quantities from large sets of covariates. However, existing prediction methods can perform poorly when the training and testing data are different, especially in the presence of hidden confounding. While hidden confounding is well studied for causal effect estimation (e.g., instrumental variables), this is not the case for prediction tasks. This work aims to bridge this gap by addressing predictions under different training and testing distributions in the presence of unobserved confounding. In particular, we establish a novel connection between the field of distribution generalization from machine learning, and simultaneous equation models and control function from econometrics. Central to our contribution are simultaneous equation models for distribution generalization (SIMDGs) which describe the data-generating process under a set of distributional shifts. Within this framework, we propose a strong notion of invariance for a predictive model and compare it with existing (weaker) versions. Building on the control function approach from instrumental variable regression, we propose the boosted control function (BCF) as a target of inference and prove its ability to successfully predict even in intervened versions of the underlying SIMDG. We provide necessary and sufficient conditions for identifying the BCF and show that it is worst-case optimal. We introduce the ControlTwicing algorithm to estimate the BCF and analyze its predictive performance on simulated and real world data.
</details>
<details>
<summary>摘要</summary>
现代机器学习方法和大规模数据的可用性打开了预测目标量的准确预测的大门。然而，现有的预测方法在训练和测试数据不同时可能表现不佳，特别是在隐藏束缚的情况下。隐藏束缚在 causal effect estimation 中已经得到了广泛的研究（例如，用工具变量），但是这并不是预测任务的情况。本研究旨在bridging这个差距，通过面对不同训练和测试分布下的预测 task 中隐藏束缚的问题。特别是，我们建立了一种 novel connection  между机器学习中的分布泛化和 econometrics 中的同时方程模型和控制函数。我们的贡献包括在这种框架下提出的同时方程模型 для分布泛化（SIMDGs），这些模型描述了数据生成过程中的分布性变化。在这个框架下，我们提出了一种强版均衡性的目标函数，并与现有（弱版）目标函数进行比较。基于控制函数方法，我们提出了增强控制函数（BCF）作为预测目标，并证明其能够在对 SIMDG 进行 intervened 后仍能成功预测。我们还提供了 necessary and sufficient conditions  для Identifying BCF，并证明它是最差情况下的优化目标。 finally，我们介绍了 ControlTwicing 算法来估计 BCF，并分析了它在 simulated 和实际数据上的预测性能。
</details></li>
</ul>
<hr>
<h2 id="An-operator-preconditioning-perspective-on-training-in-physics-informed-machine-learning"><a href="#An-operator-preconditioning-perspective-on-training-in-physics-informed-machine-learning" class="headerlink" title="An operator preconditioning perspective on training in physics-informed machine learning"></a>An operator preconditioning perspective on training in physics-informed machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05801">http://arxiv.org/abs/2310.05801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim De Ryck, Florent Bonnet, Siddhartha Mishra, Emmanuel de Bézenac</li>
<li>for:  investigate the behavior of gradient descent algorithms in physics-informed machine learning methods like PINNs</li>
<li>methods:  employ both rigorous mathematical analysis and empirical evaluations to investigate various strategies for preconditioning a critical differential operator</li>
<li>results:  the difficulty in training these models is closely related to the conditioning of a specific differential operator, and preconditioning this operator is crucial for improving training<details>
<summary>Abstract</summary>
In this paper, we investigate the behavior of gradient descent algorithms in physics-informed machine learning methods like PINNs, which minimize residuals connected to partial differential equations (PDEs). Our key result is that the difficulty in training these models is closely related to the conditioning of a specific differential operator. This operator, in turn, is associated to the Hermitian square of the differential operator of the underlying PDE. If this operator is ill-conditioned, it results in slow or infeasible training. Therefore, preconditioning this operator is crucial. We employ both rigorous mathematical analysis and empirical evaluations to investigate various strategies, explaining how they better condition this critical operator, and consequently improve training.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了梯度下降算法在物理学知识Machine learning方法中的行为，如PINNs，它们用来最小化连接到部分偏微分方程（PDEs）的差异。我们的关键结果表明，训练这些模型的困难直接与一个特定的导数器的条件相关。这个导数器，则是PDE的导数器的 Hermitian平方的一个特殊情况。如果这个导数器是不良条件的，它会导致训练慢或不可能进行。因此，预conditioning这个关键导数器是关键。我们使用了严格的数学分析和实验评估来研究不同的策略，解释它们如何改善这个关键导数器的条件，并因此提高训练。
</details></li>
</ul>
<hr>
<h2 id="The-First-Cadenza-Signal-Processing-Challenge-Improving-Music-for-Those-With-a-Hearing-Loss"><a href="#The-First-Cadenza-Signal-Processing-Challenge-Improving-Music-for-Those-With-a-Hearing-Loss" class="headerlink" title="The First Cadenza Signal Processing Challenge: Improving Music for Those With a Hearing Loss"></a>The First Cadenza Signal Processing Challenge: Improving Music for Those With a Hearing Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05799">http://arxiv.org/abs/2310.05799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/claritychallenge/clarity/tree/main/recipes/cad1/task1">https://github.com/claritychallenge/clarity/tree/main/recipes/cad1/task1</a></li>
<li>paper_authors: Gerardo Roa Dabike, Scott Bannister, Jennifer Firth, Simone Graetzer, Rebecca Vos, Michael A. Akeroyd, Jon Barker, Trevor J. Cox, Bruno Fazenda, Alinka Greasley, William Whitmer</li>
<li>for: 提高音乐质量 для听力受损人群</li>
<li>methods: 使用信号处理挑战和个性化混音&#x2F;分离技术</li>
<li>results: 提高音乐质量，使用HAAQI指数对象评估和人类评审者对subjective评估<details>
<summary>Abstract</summary>
The Cadenza project aims to improve the audio quality of music for those who have a hearing loss. This is being done through a series of signal processing challenges, to foster better and more inclusive technologies. In the first round, two common listening scenarios are considered: listening to music over headphones, and with a hearing aid in a car. The first scenario is cast as a demixing-remixing problem, where the music is decomposed into vocals, bass, drums and other components. These can then be intelligently remixed in a personalized way, to increase the audio quality for a person who has a hearing loss. In the second scenario, music is coming from car loudspeakers, and the music has to be enhanced to overcome the masking effect of the car noise. This is done by taking into account the music, the hearing ability of the listener, the hearing aid and the speed of the car. The audio quality of the submissions will be evaluated using the Hearing Aid Audio Quality Index (HAAQI) for objective assessment and by a panel of people with hearing loss for subjective evaluation.
</details>
<details>
<summary>摘要</summary>
《干扰计划》旨在提高音乐质量，以帮助有听力问题的人。这是通过一系列的信号处理挑战，促进更加包容的科技。在首轮中，考虑了两个常见的听音情况：用耳机听音乐，以及在车里使用听力器。第一个情况是将音乐转化为男女声、低音、鼓等部分，然后以人性化的方式重新混合，以提高听损人的音乐质量。第二个情况是音乐来自车里的 loudspeakers，需要利用音乐、听力问题、听力器和车速来增强音乐，以扩除车声的遮蔽效应。音乐质量的评价使用《听力器音乐质量指数》（HAAQI）进行 объектив评估，并由有听力问题的人士进行主观评价。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Hybrid-Oversampling-and-Intelligent-Undersampling-for-Imbalanced-Big-Data-Classification"><a href="#Efficient-Hybrid-Oversampling-and-Intelligent-Undersampling-for-Imbalanced-Big-Data-Classification" class="headerlink" title="Efficient Hybrid Oversampling and Intelligent Undersampling for Imbalanced Big Data Classification"></a>Efficient Hybrid Oversampling and Intelligent Undersampling for Imbalanced Big Data Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05789">http://arxiv.org/abs/2310.05789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carla Vairetti, José Luis Assadi, Sebastián Maldonado</li>
<li>for:  solves the issue of imbalanced classification in real-world applications</li>
<li>methods:  combines intelligent undersampling and oversampling using a MapReduce framework</li>
<li>results:  outperforms alternative resampling techniques for small- and medium-sized datasets, achieves positive results on large datasets with reduced running times.Here’s the full translation of the paper’s abstract in Simplified Chinese:</li>
<li>for:  solves the issue of imbalanced classification in real-world applications</li>
<li>methods:  combines intelligent undersampling and oversampling using a MapReduce framework</li>
<li>results:  outperforms alternative resampling techniques for small- and medium-sized datasets, achieves positive results on large datasets with reduced running times.I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Imbalanced classification is a well-known challenge faced by many real-world applications. This issue occurs when the distribution of the target variable is skewed, leading to a prediction bias toward the majority class. With the arrival of the Big Data era, there is a pressing need for efficient solutions to solve this problem. In this work, we present a novel resampling method called SMOTENN that combines intelligent undersampling and oversampling using a MapReduce framework. Both procedures are performed on the same pass over the data, conferring efficiency to the technique. The SMOTENN method is complemented with an efficient implementation of the neighborhoods related to the minority samples. Our experimental results show the virtues of this approach, outperforming alternative resampling techniques for small- and medium-sized datasets while achieving positive results on large datasets with reduced running times.
</details>
<details>
<summary>摘要</summary>
不均衡分类是现实世界中许多应用程序的挑战之一。这种问题出现在目标变量的分布偏斜时，导致预测偏向大多数类。在大数据时代 arrives，有一项压力需要解决这个问题。在这种工作中，我们提出了一种新的抽样方法called SMOTENN，它将智能下抽样和上抽样与 MapReduce 框架结合在一起。两种过程都在数据上进行了同一次读取，从而提高了方法的效率。 SMOTENN 方法还包括有效地实现少数类邻居的方法。我们的实验结果表明，这种方法在小到中型数据集上表现出色，而且在大数据集上具有减少运行时间的正面效果。
</details></li>
</ul>
<hr>
<h2 id="Why-Should-This-Article-Be-Deleted-Transparent-Stance-Detection-in-Multilingual-Wikipedia-Editor-Discussions"><a href="#Why-Should-This-Article-Be-Deleted-Transparent-Stance-Detection-in-Multilingual-Wikipedia-Editor-Discussions" class="headerlink" title="Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions"></a>Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05779">http://arxiv.org/abs/2310.05779</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/copenlu/wiki-stance">https://github.com/copenlu/wiki-stance</a></li>
<li>paper_authors: Lucie-Aimée Kaffee, Arnav Arora, Isabelle Augenstein</li>
<li>For: The paper aims to improve the transparency of content moderation on online platforms, specifically on Wikipedia, by constructing a novel multilingual dataset of editor discussions and their reasoning.* Methods: The paper uses a machine learning approach to predict the stance and reason (content moderation policy) of editors for each edit decision, adding transparency to the decision-making process.* Results: The paper demonstrates that stance and corresponding reason (policy) can be predicted jointly with a high degree of accuracy, providing a more transparent approach to content moderation.Here are the three key information points in Simplified Chinese text:* For: 论文目的是提高在线平台上的内容审核透明度，具体来说是在Wikipedia上进行编辑者讨论和决策的透明度。* Methods: 论文使用机器学习方法预测编辑者决策中的态度和理由（内容审核政策），以提高决策过程的透明度。* Results: 论文表明，态度和相应的理由（政策）可以通过高精度预测的方法相互关联，从而提供更透明的内容审核方法。<details>
<summary>Abstract</summary>
The moderation of content on online platforms is usually non-transparent. On Wikipedia, however, this discussion is carried out publicly and the editors are encouraged to use the content moderation policies as explanations for making moderation decisions. Currently, only a few comments explicitly mention those policies -- 20% of the English ones, but as few as 2% of the German and Turkish comments. To aid in this process of understanding how content is moderated, we construct a novel multilingual dataset of Wikipedia editor discussions along with their reasoning in three languages. The dataset contains the stances of the editors (keep, delete, merge, comment), along with the stated reason, and a content moderation policy, for each edit decision. We demonstrate that stance and corresponding reason (policy) can be predicted jointly with a high degree of accuracy, adding transparency to the decision-making process. We release both our joint prediction models and the multilingual content moderation dataset for further research on automated transparent content moderation.
</details>
<details>
<summary>摘要</summary>
在线平台内容Moderation通常是不透明的。然而，在Wikipedia上，这个讨论被公开进行，编辑被鼓励使用内容Moderation政策作为决策的解释。目前，只有英语评论中有20%是明确提到政策，德语和土耳其语评论中则只有2%。为了帮助理解内容Moderation的过程，我们构建了一个多语言数据集，包括Wikipedia编辑讨论、理由和内容Moderation政策。我们示示了editors的立场和相应的理由（政策）可以并行预测，增加了决策过程的透明度。我们发布了联合预测模型和多语言内容Moderation数据集，以便进一步研究自动透明内容Moderation。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Models-Meet-Visualizations-Challenges-and-Opportunities"><a href="#Foundation-Models-Meet-Visualizations-Challenges-and-Opportunities" class="headerlink" title="Foundation Models Meet Visualizations: Challenges and Opportunities"></a>Foundation Models Meet Visualizations: Challenges and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05771">http://arxiv.org/abs/2310.05771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weikai Yang, Mengchen Liu, Zheng Wang, Shixia Liu</li>
<li>for: This paper explores the intersection of visualization techniques and foundation models like BERT and GPT, and how they can be used to improve transparency, explainability, fairness, and robustness in AI systems.</li>
<li>methods: The paper divides the intersections of visualization techniques and foundation models into two main areas: visualizations for foundation models (VIS4FM) and foundation models for visualizations (FM4VIS).</li>
<li>results: The paper highlights the challenges and opportunities that arise from the confluence of foundation models and visualizations, and provides a starting point for continued exploration in this promising avenue.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文探讨了基础模型如BERT和GPT与视觉技术的交叉，以提高人工智能系统的透明度、解释性、公平性和稳定性。</li>
<li>methods: 论文将这些交叉分为两个主要领域：用于基础模型的视觉（VIS4FM）和基础模型用于视觉的发展（FM4VIS）。</li>
<li>results: 论文描述了这些交叉所带来的挑战和机遇，并提供了这个领域的开始点 для进一步的探索。<details>
<summary>Abstract</summary>
Recent studies have indicated that foundation models, such as BERT and GPT, excel in adapting to a variety of downstream tasks. This adaptability has established them as the dominant force in building artificial intelligence (AI) systems. As visualization techniques intersect with these models, a new research paradigm emerges. This paper divides these intersections into two main areas: visualizations for foundation models (VIS4FM) and foundation models for visualizations (FM4VIS). In VIS4FM, we explore the primary role of visualizations in understanding, refining, and evaluating these intricate models. This addresses the pressing need for transparency, explainability, fairness, and robustness. Conversely, within FM4VIS, we highlight how foundation models can be utilized to advance the visualization field itself. The confluence of foundation models and visualizations holds great promise, but it also comes with its own set of challenges. By highlighting these challenges and the growing opportunities, this paper seeks to provide a starting point for continued exploration in this promising avenue.
</details>
<details>
<summary>摘要</summary>
现代研究表明，基础模型，如BERT和GPT，在适应多种下游任务方面表现出色。这种适应能力使其成为人工智能系统建设的主导力量。在这些模型与视觉化技术交叉点的研究中，一个新的研究模式出现。这篇论文将这些交叉点分为两个主要领域：用于基础模型的视觉化（VIS4FM）和基础模型为视觉化的应用（FM4VIS）。在 VIS4FM 中，我们探索了视觉化在理解、修改和评估这些复杂模型的 primacy 角色。这种需求包括透明度、解释性、公平性和稳定性。相反，在 FM4VIS 中，我们强调了基础模型如何推动视觉化领域的进步。这两个领域的交叉点具有极大的推动力，但也存在一些挑战。通过强调这些挑战和快速发展的机遇，这篇论文希望为这一领域的进一步探索提供一个开始。
</details></li>
</ul>
<hr>
<h2 id="LCOT-Linear-circular-optimal-transport"><a href="#LCOT-Linear-circular-optimal-transport" class="headerlink" title="LCOT: Linear circular optimal transport"></a>LCOT: Linear circular optimal transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06002">http://arxiv.org/abs/2310.06002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rocio Diaz Martin, Ivan Medri, Yikun Bai, Xinran Liu, Kangbai Yan, Gustavo K. Rohde, Soheil Kolouri</li>
<li>for: 这篇论文主要关注于圆形概率分布，并提出了一新的计算效率高的度量方法，即线性圆形最佳运输（LCOT）。</li>
<li>methods: 该论文引入了一个新的计算效率高的度量方法，即LCOT，并提供了一个可靠的线性映射，使得可以将机器学习（ML）算法应用到圆形概率分布上，并且让度量方法与ML算法之间的转换非常容易。</li>
<li>results: 论文通过一系列的数据实验示出了LCOT的效能，并显示了它在学习圆形概率分布的表现比较高于传统的圆形最佳运输（COT）度量方法。<details>
<summary>Abstract</summary>
The optimal transport problem for measures supported on non-Euclidean spaces has recently gained ample interest in diverse applications involving representation learning. In this paper, we focus on circular probability measures, i.e., probability measures supported on the unit circle, and introduce a new computationally efficient metric for these measures, denoted as Linear Circular Optimal Transport (LCOT). The proposed metric comes with an explicit linear embedding that allows one to apply Machine Learning (ML) algorithms to the embedded measures and seamlessly modify the underlying metric for the ML algorithm to LCOT. We show that the proposed metric is rooted in the Circular Optimal Transport (COT) and can be considered the linearization of the COT metric with respect to a fixed reference measure. We provide a theoretical analysis of the proposed metric and derive the computational complexities for pairwise comparison of circular probability measures. Lastly, through a set of numerical experiments, we demonstrate the benefits of LCOT in learning representations of circular measures.
</details>
<details>
<summary>摘要</summary>
最近几年，非欧几何空间上的最优运输问题已经吸引了多种应用，其中包括表示学习。在这篇论文中，我们将关注圆形概率度量，即圆周上的概率度量，并提出一种新的计算高效的度量，称为线性圆形最优运输（LCOT）。我们的度量包括一个显式的线性映射，使得可以通过对扩展到Machine Learning（ML）算法的度量进行应用，并且可以顺利地修改下面的度量为LCOT。我们证明了我们的度量基于圆形最优运输（COT）度量，并且可以视为对固定参照度量的线性化。我们提供了对度量的理论分析，并计算了对圆形概率度量的对比的计算复杂度。最后，通过一系列的数值实验，我们证明了LCOT在学习圆形度量的表示方面的好处。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Correct-and-Smooth-for-Semi-Supervised-Learning"><a href="#Nonlinear-Correct-and-Smooth-for-Semi-Supervised-Learning" class="headerlink" title="Nonlinear Correct and Smooth for Semi-Supervised Learning"></a>Nonlinear Correct and Smooth for Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05757">http://arxiv.org/abs/2310.05757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhang Shao, Xiuwen Liu</li>
<li>for: 本研究针对Graph-based semi-supervised learning (GSSL) 进行了改进，以提高预测性能。</li>
<li>methods: 本研究使用了 Label Propagation (LP) 和 Graph Neural Networks (GNNs) 等方法，并将它们组合以提高表现。</li>
<li>results: 系统评估显示，本研究的方法可以在六个常用的数据集上取得了remarkable的平均提升率，较基本预测方法提升率高出13.71%，并且较现有的后处理方法提升率高出2.16%。<details>
<summary>Abstract</summary>
Graph-based semi-supervised learning (GSSL) has been used successfully in various applications. Existing methods leverage the graph structure and labeled samples for classification. Label Propagation (LP) and Graph Neural Networks (GNNs) both iteratively pass messages on graphs, where LP propagates node labels through edges and GNN aggregates node features from the neighborhood. Recently, combining LP and GNN has led to improved performance. However, utilizing labels and features jointly in higher-order graphs has not been explored. Therefore, we propose Nonlinear Correct and Smooth (NLCS), which improves the existing post-processing approach by incorporating non-linearity and higher-order representation into the residual propagation to handle intricate node relationships effectively. Systematic evaluations show that our method achieves remarkable average improvements of 13.71% over base prediction and 2.16% over the state-of-the-art post-processing method on six commonly used datasets. Comparisons and analyses show our method effectively utilizes labels and features jointly in higher-order graphs to resolve challenging graph relationships.
</details>
<details>
<summary>摘要</summary>
GRAPH-BASED SEMI-SUPERVISED LEARNING (GSSL) 已经成功应用于多个领域。现有方法利用图结构和标注样本进行分类。标签推广（LP）和图神经网络（GNNs）都是在图上进行迭代传递消息的方法，其中LP通过边传递节点标签，GNN从邻居聚合节点特征。最近，将LP和GNN结合使用已经导致了提高性能。然而，在更高阶图上同时利用标签和特征还没有被探索。因此，我们提出了非线性稳定（NLCS）方法，它通过在剩余传播中添加非线性和高阶表示来有效地处理图中复杂的节点关系。系统性评估显示，我们的方法在六个常用的数据集上取得了显著的平均提升率为13.71%，与基础预测相比，和state-of-the-art post-processing方法相比，分别提高了2.16%。比较和分析表明，我们的方法能够有效地在更高阶图上同时利用标签和特征进行分类。
</details></li>
</ul>
<hr>
<h2 id="Deep-Concept-Removal"><a href="#Deep-Concept-Removal" class="headerlink" title="Deep Concept Removal"></a>Deep Concept Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05755">http://arxiv.org/abs/2310.05755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aman432/Spam-Classifier">https://github.com/aman432/Spam-Classifier</a></li>
<li>paper_authors: Yegor Klochkov, Jean-Francois Ton, Ruocheng Guo, Yang Liu, Hang Li</li>
<li>for: 本研究旨在深度神经网络中解决概念除去问题，以学习不含特定概念（如性别等）的表示。</li>
<li>methods: 我们提出了一种基于对概念集的对抗线性分类器的新方法，该方法可以帮助移除目标特征而不影响模型性能。我们在不同层次的网络中采用对抗探测类ifier，有效地解决了概念杂糜和OOD泛化问题。</li>
<li>results: 我们在一些流行的分布式 robust optimization（DRO）benchmark上进行了评估，以及OOD泛化任务。结果表明，我们的方法可以有效地除去概念，同时保持模型性能。<details>
<summary>Abstract</summary>
We address the problem of concept removal in deep neural networks, aiming to learn representations that do not encode certain specified concepts (e.g., gender etc.) We propose a novel method based on adversarial linear classifiers trained on a concept dataset, which helps to remove the targeted attribute while maintaining model performance. Our approach Deep Concept Removal incorporates adversarial probing classifiers at various layers of the network, effectively addressing concept entanglement and improving out-of-distribution generalization. We also introduce an implicit gradient-based technique to tackle the challenges associated with adversarial training using linear classifiers. We evaluate the ability to remove a concept on a set of popular distributionally robust optimization (DRO) benchmarks with spurious correlations, as well as out-of-distribution (OOD) generalization tasks.
</details>
<details>
<summary>摘要</summary>
我们关注深度神经网络中的概念除除问题，即学习不包含特定指定的概念（如性别等）的表示。我们提出了一种基于对概念集合的 adversarial 线性分类器的方法，可以帮助除除目标特征而保持模型性能。我们的方法深度概念除除包括对网络各层的 adversarial 探测器，有效地解决概念杂糜和外围泛化问题。此外，我们还介绍了一种基于偏导数的技术来解决对 adversarial 训练使用线性分类器的挑战。我们在一些流行的分布robust优化（DRO）benchmark上进行了评估，以及外围泛化任务。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Shape-Distances-on-Neural-Representations-with-Limited-Samples"><a href="#Estimating-Shape-Distances-on-Neural-Representations-with-Limited-Samples" class="headerlink" title="Estimating Shape Distances on Neural Representations with Limited Samples"></a>Estimating Shape Distances on Neural Representations with Limited Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05742">http://arxiv.org/abs/2310.05742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dean A. Pospisil, Brett W. Larsen, Sarah E. Harvey, Alex H. Williams</li>
<li>for: 本研究旨在提供高维网络表示之间的几何相似性测量的一种有效方法，并且对这些方法进行了系统的分析和评估。</li>
<li>methods: 本研究使用了标准估计器，以及一种新的方法——方差调整的方法 OF moments  estimator，以确定高维网络表示之间的几何相似性。</li>
<li>results: 研究发现，标准估计器在高维特征空间中存在困难，而新引入的方法 OF moments  estimator 能够在实验和神经网络数据上达到更高的性能，特别是在高维设置下。<details>
<summary>Abstract</summary>
Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of shape distance$\unicode{x2014}$a measure of representational dissimilarity proposed by Williams et al. (2021). These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Thus, we lay the foundation for a rigorous statistical theory for high-dimensional shape analysis, and we contribute a new estimation method that is well-suited to practical scientific settings.
</details>
<details>
<summary>摘要</summary>
To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Our findings lay the foundation for a rigorous statistical theory for high-dimensional shape analysis and contribute a new estimation method well-suited to practical scientific settings.
</details></li>
</ul>
<hr>
<h2 id="Post-hoc-Bias-Scoring-Is-Optimal-For-Fair-Classification"><a href="#Post-hoc-Bias-Scoring-Is-Optimal-For-Fair-Classification" class="headerlink" title="Post-hoc Bias Scoring Is Optimal For Fair Classification"></a>Post-hoc Bias Scoring Is Optimal For Fair Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05725">http://arxiv.org/abs/2310.05725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlong Chen, Yegor Klochkov, Yang Liu</li>
<li>for: 本文目标是研究一种基于分组公平约束的二分类问题的解决方案，包括人口均衡（DP）、平等机会（EOp）和平等投票机会（EO）等。</li>
<li>methods: 本文提出了一种基于 Bayes 优化的修改规则，通过对每个实例计算一个新的偏见指标（bias score），并将这些指标应用于修改规则来实现分组公平。修改规则可以是单个阈值或二元数组，具体取决于所使用的公平约束。</li>
<li>results: 本文通过使用三个数据集（Adult、COMPAS 和 CelebA）进行实验，显示了与内部处理和后处理方法相比，该方法可以实现高精度和分组公平。此外，该方法不需要在推断时访问敏感特征。<details>
<summary>Abstract</summary>
We consider a binary classification problem under group fairness constraints, which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or Equalized Odds (EO). We propose an explicit characterization of Bayes optimal classifier under the fairness constraints, which turns out to be a simple modification rule of the unconstrained classifier. Namely, we introduce a novel instance-level measure of bias, which we call bias score, and the modification rule is a simple linear rule on top of the finite amount of bias scores. Based on this characterization, we develop a post-hoc approach that allows us to adapt to fairness constraints while maintaining high accuracy. In the case of DP and EOp constraints, the modification rule is thresholding a single bias score, while in the case of EO constraints we are required to fit a linear modification rule with 2 parameters. The method can also be applied for composite group-fairness criteria, such as ones involving several sensitive attributes. We achieve competitive or better performance compared to both in-processing and post-processing methods across three datasets: Adult, COMPAS, and CelebA. Unlike most post-processing methods, we do not require access to sensitive attributes during the inference time.
</details>
<details>
<summary>摘要</summary>
我们考虑了一个二分类问题，其中需要满足一些群体公平约束，可能是人口比例（DP）、机会平等（EOp）或机会几率（EO）。我们提出了一个bayes最优分类器的显式化 caracterization，这 turns out to be a simple modification rule of the unconstrained classifier。我们引入了一个新的实例级别偏见度量，称为偏见得分，并且这个修改规则是一个单个偏见得分的阈值处理。基于这个characterization，我们开发了一种后处方法，可以在维护高准确率的同时适应公平约束。在DP和EOp约束下，修改规则是对偏见得分进行阈值处理，而在EO约束下，我们需要适应一个线性修改规则 WITH 2个参数。此方法还可以应用于复杂的群体公平标准，例如包括多个敏感特征。我们在三个 dataset（Adult、COMPAS和CelebA）上达到了竞争或更好的性能，与大多数后处方法不同的是，我们在推理时不需要访问敏感特征。
</details></li>
</ul>
<hr>
<h2 id="Planning-to-Go-Out-of-Distribution-in-Offline-to-Online-Reinforcement-Learning"><a href="#Planning-to-Go-Out-of-Distribution-in-Offline-to-Online-Reinforcement-Learning" class="headerlink" title="Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning"></a>Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05723">http://arxiv.org/abs/2310.05723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trevor McInroe, Stefano V. Albrecht, Amos Storkey</li>
<li>for: 在减少在线交互次数的情况下，找到最佳政策。</li>
<li>methods: 使用在线搜索和规划算法，以最大化在线数据收集的利益。</li>
<li>results: PTGOOD算法可以在减少在线交互次数的情况下，提高代理返回和找到最佳政策，并且可以避免许多基线算法在不同环境中的低效 converges。<details>
<summary>Abstract</summary>
Offline pretraining with a static dataset followed by online fine-tuning (offline-to-online, or OtO) is a paradigm that is well matched to a real-world RL deployment process: in few real settings would one deploy an offline policy with no test runs and tuning. In this scenario, we aim to find the best-performing policy within a limited budget of online interactions. Previous work in the OtO setting has focused on correcting for bias introduced by the policy-constraint mechanisms of offline RL algorithms. Such constraints keep the learned policy close to the behavior policy that collected the dataset, but this unnecessarily limits policy performance if the behavior policy is far from optimal. Instead, we forgo policy constraints and frame OtO RL as an exploration problem: we must maximize the benefit of the online data-collection. We study major online RL exploration paradigms, adapting them to work well with the OtO setting. These adapted methods contribute several strong baselines. Also, we introduce an algorithm for planning to go out of distribution (PTGOOD), which targets online exploration in relatively high-reward regions of the state-action space unlikely to be visited by the behavior policy. By leveraging concepts from the Conditional Entropy Bottleneck, PTGOOD encourages data collected online to provide new information relevant to improving the final deployment policy. In that way the limited interaction budget is used effectively. We show that PTGOOD significantly improves agent returns during online fine-tuning and finds the optimal policy in as few as 10k online steps in Walker and in as few as 50k in complex control tasks like Humanoid. Also, we find that PTGOOD avoids the suboptimal policy convergence that many of our baselines exhibit in several environments.
</details>
<details>
<summary>摘要</summary>
假设我们有一个偏向于实际世界的强化学习（RL）部署过程：在几个实际场景中，我们不会直接部署一个没有测试和调整的策略。在这种情况下，我们想找到最佳的策略，而且在有限的在线互动次数内完成。先前的工作在OtO设定中集中在修正在线RL算法中的偏见问题上。这些约束保持学习策略与数据收集过程中的行为策略相互关联，但这会无需lessly限制策略性能，如果行为策略远离优化。因此，我们不采用策略约束，而是视为探索问题，我们需要在在线数据收集中最大化收益。我们研究了在线RL探索方法，并将其适应到OtO设定中。这些适应方法提供了多个强大的基线。此外，我们介绍了一种计划去偏现（PTGOOD）算法，该算法target在高奖励区域的状态动作空间中进行在线探索。通过利用Conditional Entropy Bottleneck的概念，PTGOOD鼓励在线数据收集提供新的有用信息，以改进最终部署策略。因此，我们可以有效地使用有限的互动次数。我们显示，PTGOOD在在线细化中显著提高了代理返回，并在Walker和复杂控制任务中在10k和50k在线步骤内找到优化策略。此外，我们发现PTGOOD可以避免许多我们的基eline在多个环境中展现的差异性。
</details></li>
</ul>
<hr>
<h2 id="Transformer-Fusion-with-Optimal-Transport"><a href="#Transformer-Fusion-with-Optimal-Transport" class="headerlink" title="Transformer Fusion with Optimal Transport"></a>Transformer Fusion with Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05719">http://arxiv.org/abs/2310.05719</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Yahnnosh/Exploring-Model-Fusion-with-Optimal-Transport-on-Transformers">https://github.com/Yahnnosh/Exploring-Model-Fusion-with-Optimal-Transport-on-Transformers</a></li>
<li>paper_authors: Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, Sidak Pal Singh</li>
<li>for: 这 paper 的目的是探讨 transformer 网络的合并技术，以提高模型的性能。</li>
<li>methods: 这 paper 使用 Optimal Transport 算法来软对接 transformer 网络的不同组件，以实现层Alignment。 authors 还提出了一种抽象层Alignment方法，可以普适应用于不同的架构。</li>
<li>results:  experiments 表明，这 paper 的方法可以提高 transformer 网络的性能，并且可以让模型具有更好的泛化能力。 authors 还发现了一些有趣的现象，例如软对接在 transformer 网络中的重要作用。<details>
<summary>Abstract</summary>
Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. In this paper, we present a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures -- in principle -- and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way for compression of Transformers. The proposed approach is evaluated on both image classification tasks via Vision Transformer and natural language modeling tasks using BERT. Our approach consistently outperforms vanilla fusion, and, after a surprisingly short finetuning, also outperforms the individual converged parent models. In our analysis, we uncover intriguing insights about the significant role of soft alignment in the case of Transformers. Our results showcase the potential of fusing multiple Transformers, thus compounding their expertise, in the budding paradigm of model fusion and recombination.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文。merge multiple independently trained neural networks to combine their capabilities. Previous attempts were limited to fully connected, convolutional, and residual networks. In this paper, we present a systematic approach for fusing two or more transformer-based networks using optimal transport to align the various architectural components. We provide an abstraction for layer alignment that can generalize to any architecture and apply it to key ingredients of Transformers such as multi-head self-attention, layer normalization, and residual connections. We also discuss how to handle them through various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way for compression of Transformers. The proposed approach is evaluated on both image classification tasks using Vision Transformer and natural language modeling tasks using BERT. Our approach consistently outperforms vanilla fusion and, after a surprisingly short finetuning, also outperforms the individual converged parent models. In our analysis, we uncover intriguing insights about the significant role of soft alignment in the case of Transformers. Our results showcase the potential of fusing multiple Transformers, thus compounding their expertise, in the emerging paradigm of model fusion and recombination.</SYS>以下是简化中文版本：融合多个独立训练的神经网络，以合并它们的能力。过去的尝试都是限制在几何网络、卷积网络和差分网络上。在这篇论文中，我们提出了一种系统的方法，使用最优运输来软对齐多个转换器基础网络的各种建筑 ком成分。我们提供了一个抽象层对齐概念，可以泛化到任何建筑，并应用于转换器的关键组成部分，如多头自我注意、层Normalization和差分连接。我们还讨论了如何通过不同的截止方法处理它们。此外，我们的方法允许模型的不同大小（不同大小的融合），提供了一种新的高效的压缩方法。我们的方法在图像分类任务上使用视Transformer和自然语言处理任务上使用BERT进行评估，我们的方法一致性超过了普通融合，并在短暂的训练后也超过了父模型的单独整合。在我们的分析中，我们发现了关于转换器中软对齐的各种惊喜的发现。我们的结果显示，可以将多个转换器融合在一起，从而汇集它们的专长，在模型融合和重新组合的新时代中发挥作用。
</details></li>
</ul>
<hr>
<h2 id="Imitator-Learning-Achieve-Out-of-the-Box-Imitation-Ability-in-Variable-Environments"><a href="#Imitator-Learning-Achieve-Out-of-the-Box-Imitation-Ability-in-Variable-Environments" class="headerlink" title="Imitator Learning: Achieve Out-of-the-Box Imitation Ability in Variable Environments"></a>Imitator Learning: Achieve Out-of-the-Box Imitation Ability in Variable Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05712">http://arxiv.org/abs/2310.05712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiong-Hui Chen, Junyin Ye, Hang Zhao, Yi-Chen Li, Haoran Shi, Yu-Yan Xu, Zhihao Ye, Si-Hang Yang, Anqi Huang, Kai Xu, Zongzhang Zhang, Yang Yu</li>
<li>for: 本文旨在提出一种新的imitator learning（ItorL）方法，以便在很少示例的情况下，快速重建不同任务的imitator模块，并适应未预期的环境变化。</li>
<li>methods: 本文提出了一种基于一个专家示例的Demo-Attention Actor-Critic（DAAC）方法，将imitator学习纳入了一种强化学习框架，以Regularize策略的行为在意外情况下。此外，为了自主建立imitator策略，我们设计了一个示例基于注意力架构，可以有效地输出imiter动作，适应不同的状态。</li>
<li>results: 我们在一个新的导航benchmark和一个机器人环境中测试了DAAC方法，与之前的imitator方法相比，DAAC方法在 seen和未seen任务上都有大幅提高，具体来说，DAAC方法在seen任务上提高了24.3%，在未seen任务上提高了110.8%。<details>
<summary>Abstract</summary>
Imitation learning (IL) enables agents to mimic expert behaviors. Most previous IL techniques focus on precisely imitating one policy through mass demonstrations. However, in many applications, what humans require is the ability to perform various tasks directly through a few demonstrations of corresponding tasks, where the agent would meet many unexpected changes when deployed. In this scenario, the agent is expected to not only imitate the demonstration but also adapt to unforeseen environmental changes.   This motivates us to propose a new topic called imitator learning (ItorL), which aims to derive an imitator module that can on-the-fly reconstruct the imitation policies based on very limited expert demonstrations for different unseen tasks, without any extra adjustment. In this work, we focus on imitator learning based on only one expert demonstration. To solve ItorL, we propose Demo-Attention Actor-Critic (DAAC), which integrates IL into a reinforcement-learning paradigm that can regularize policies' behaviors in unexpected situations. Besides, for autonomous imitation policy building, we design a demonstration-based attention architecture for imitator policy that can effectively output imitated actions by adaptively tracing the suitable states in demonstrations. We develop a new navigation benchmark and a robot environment for \topic~and show that DAAC~outperforms previous imitation methods \textit{with large margins} both on seen and unseen tasks.
</details>
<details>
<summary>摘要</summary>
复制学习（IL）允许代理人模仿专家的行为。大多数前一些IL技术都是通过大量示例来精准地复制一个策略。然而，在许多应用场景中，人们需要代理人能够直接完成多个任务，而不需要大量的示例。在这种情况下，代理人需要不仅模仿示例，还需要适应不可预期的环境变化。这种情况 Motivates us to propose a new topic called imitator learning（ItorL）， which aims to derive an imitator module that can on-the-fly重建模仿策略 based on very limited expert demonstrations for different unseen tasks, without any extra adjustment. In this work, we focus on imitator learning based on only one expert demonstration. To solve ItorL, we propose Demo-Attention Actor-Critic（DAAC）， which integrates IL into a reinforcement-learning paradigm that can regularize policies' behaviors in unexpected situations. Besides, for autonomous imitation policy building, we design a demonstration-based attention architecture for imitator policy that can effectively output imitated actions by adaptively tracing the suitable states in demonstrations. We develop a new navigation benchmark and a robot environment for \topic~and show that DAAC~outperforms previous imitation methods \textit{with large margins} both on seen and unseen tasks.
</details></li>
</ul>
<hr>
<h2 id="Protecting-Sensitive-Data-through-Federated-Co-Training"><a href="#Protecting-Sensitive-Data-through-Federated-Co-Training" class="headerlink" title="Protecting Sensitive Data through Federated Co-Training"></a>Protecting Sensitive Data through Federated Co-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05696">http://arxiv.org/abs/2310.05696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Abourayya, Jens Kleesiek, Kanishka Rao, Erman Ayday, Bharat Rao, Geoff Webb, Michael Kamp</li>
<li>for: 保护敏感数据，避免公开地 revelas 本地训练数据。</li>
<li>methods: 使用联合学习方法，将本地训练的参数集成到一个共识模型中，以实现模型的训练。</li>
<li>results: 比较 federated learning 和分布式滤波两种方法， federated co-training 方法可以达到更高的隐私保护和模型质量。<details>
<summary>Abstract</summary>
In many critical applications, sensitive data is inherently distributed. Federated learning trains a model collaboratively by aggregating the parameters of locally trained models. This avoids exposing sensitive local data. It is possible, though, to infer upon the sensitive data from the shared model parameters. At the same time, many types of machine learning models do not lend themselves to parameter aggregation, such as decision trees, or rule ensembles. It has been observed that in many applications, in particular healthcare, large unlabeled datasets are publicly available. They can be used to exchange information between clients by distributed distillation, i.e., co-regularizing local training via the discrepancy between the soft predictions of each local client on the unlabeled dataset. This, however, still discloses private information and restricts the types of models to those trainable via gradient-based methods. We propose to go one step further and use a form of federated co-training, where local hard labels on the public unlabeled datasets are shared and aggregated into a consensus label. This consensus label can be used for local training by any supervised machine learning model. We show that this federated co-training approach achieves a model quality comparable to both federated learning and distributed distillation on a set of benchmark datasets and real-world medical datasets. It improves privacy over both approaches, protecting against common membership inference attacks to the highest degree. Furthermore, we show that federated co-training can collaboratively train interpretable models, such as decision trees and rule ensembles, achieving a model quality comparable to centralized training.
</details>
<details>
<summary>摘要</summary>
许多关键应用中敏感数据是自然地分布式。联邦学习通过合并本地训练模型的参数来培训模型，这样可以避免曝光敏感本地数据。然而，可以通过共享模型参数来推断敏感数据。此外，许多机器学习模型无法参数综合，如决策树或规则集。在许多应用中，尤其是医疗领域，大量的未标注数据公开可用。通过分布式蒸馏，即客户端之间通过未标注数据的差异来协调本地训练。这样仍然披露了私人信息，并限制了可以使用的模型类型。我们提议进一步使用联邦合作训练，其中本地硬标签在公共未标注数据上进行分布式合并，生成一个共识标签。这个共识标签可以用于本地训练任何超级vised机器学习模型。我们显示，这种联邦合作训练方法可以与联邦学习和分布式蒸馏相比，在一组benchmark数据集和真实医疗数据集上达到类似的模型质量。同时，它提高隐私性，保护 против最常见的会员推断攻击。此外，我们还显示，联邦合作训练可以共同训练可读性强的模型，如决策树和规则集，与中央训练相比。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Reinforcement-Learning-for-Temporal-Pattern-Prediction"><a href="#Hierarchical-Reinforcement-Learning-for-Temporal-Pattern-Prediction" class="headerlink" title="Hierarchical Reinforcement Learning for Temporal Pattern Prediction"></a>Hierarchical Reinforcement Learning for Temporal Pattern Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05695">http://arxiv.org/abs/2310.05695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faith Johnson, Kristin Dana</li>
<li>for: 这个论文探讨了使用层次强化学习（HRL）来解决时间序列预测任务。</li>
<li>methods: 作者使用了深度学习和HRL来开发一个用于预测股票价格时间序列的股票机器人，以及一个基于首人视频的车辆机器人来预测转向角。</li>
<li>results: 在两个领域中，作者发现了一种类型的HRL，即封顶强化学习，可以提供更高的训练速度和稳定性以及预测精度，而这一成功归功于网络层次结构中引入的时间和空间抽象。<details>
<summary>Abstract</summary>
In this work, we explore the use of hierarchical reinforcement learning (HRL) for the task of temporal sequence prediction. Using a combination of deep learning and HRL, we develop a stock agent to predict temporal price sequences from historical stock price data and a vehicle agent to predict steering angles from first person, dash cam images. Our results in both domains indicate that a type of HRL, called feudal reinforcement learning, provides significant improvements to training speed and stability and prediction accuracy over standard RL. A key component to this success is the multi-resolution structure that introduces both temporal and spatial abstraction into the network hierarchy.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们探索使用层次强制学习（HRL）来解决时间序列预测问题。通过将深度学习和HRL相结合，我们开发了一个股票代理来预测历史股票价格数据中的时间价格序列，以及一个车辆代理来预测来自首人、摄像头图像中的推理角度。我们在两个领域中的结果表明，一种称为“封顶强制学习”的HRL方法可以提供标准RL方法的训练速度和稳定性以及预测精度的显著改进。关键的一点是将多尺度结构引入网络层次结构，这种结构具有时间和空间抽象的双重优势。
</details></li>
</ul>
<hr>
<h2 id="Multi-timestep-models-for-Model-based-Reinforcement-Learning"><a href="#Multi-timestep-models-for-Model-based-Reinforcement-Learning" class="headerlink" title="Multi-timestep models for Model-based Reinforcement Learning"></a>Multi-timestep models for Model-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05672">http://arxiv.org/abs/2310.05672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelhakim Benechehab, Giuseppe Paolo, Albert Thomas, Maurizio Filippone, Balázs Kégl</li>
<li>for: This paper aims to improve the performance of model-based reinforcement learning (MBRL) algorithms by using a multi-timestep objective to train one-step models.</li>
<li>methods: The authors use a weighted sum of loss functions at various future horizons as their objective, with exponentially decaying weights, to improve the long-horizon performance of their models.</li>
<li>results: The authors find that their multi-timestep models outperform or match standard one-step models in both pure batch reinforcement learning (RL) and iterated batch RL scenarios, particularly in noisy environments.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是提高基于模型的学习（MBRL）算法的性能，通过使用多个时间步骤的目标来训练一步模型。</li>
<li>methods: 作者们使用多个时间步骤的损失函数权重和衰减来提高模型的长期性能。</li>
<li>results: 作者们发现，他们的多个时间步骤模型在纯批量学习（RL）和迭代批量RL场景中都能够超过或与标准一步模型匹配，特别在噪音环境中表现出色， highlighting the potential of their approach in real-world applications。<details>
<summary>Abstract</summary>
In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered environment, highlighting the potential of our approach in real-world applications.
</details>
<details>
<summary>摘要</summary>
在基于模型的强化学习（MBRL）中，大多数算法都是通过从一步动力学模型学习数据上的一步预测错误来预测 trajectory。这种方法的一个挑战是预测误差的积累作用，随着 trajectory 的长度增长。在这篇论文中，我们解决这个问题 by using a multi-timestep objective to train one-step models。我们的目标是一个 weighted sum of a loss function（例如 negative log-likelihood）at various future horizons。我们探索和测试了不同的Weight profile。我们发现，使用恒速衰减的Weight leads to models that significantly improve the long-horizon R2 score。这种改进特别明显在噪音环境中， highlighting the potential of our approach in real-world applications。In addition, we used a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, and found that our multi-timestep models outperformed or matched standard one-step models. This was especially evident in a noisy variant of the considered environment.
</details></li>
</ul>
<hr>
<h2 id="LARA-A-Light-and-Anti-overfitting-Retraining-Approach-for-Unsupervised-Anomaly-Detection"><a href="#LARA-A-Light-and-Anti-overfitting-Retraining-Approach-for-Unsupervised-Anomaly-Detection" class="headerlink" title="LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection"></a>LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05668">http://arxiv.org/abs/2310.05668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feiyi Chen, Zhen Qing, Yingying Zhang, Shuiguang Deng, Yi Xiao, Guansong Pang, Qingsong Wen</li>
<li>for: 这个研究旨在提出一种Light and Anti-overfitting Retraining Approach (LARA)，用于深度Variational Autoencoder (VAEs) 时间序列异常检测方法中。</li>
<li>methods: 本研究使用了一个新的Retraining process，它可以快速地调整模型，并且避免过滤。此外，本研究还提出了一个叫做ruminate block的新方法，可以利用历史数据而不需要储存它们。</li>
<li>results: 本研究的实验结果显示，可以使用43个时间槽的新分布数据进行重训，却可以与现有的异常检测模型相比，并且显示出较低的过滤频率。此外，本研究还证明了LARA模型的过程调整 overhead 轻量级。<details>
<summary>Abstract</summary>
Most of current anomaly detection models assume that the normal pattern remains same all the time. However, the normal patterns of Web services change dramatically and frequently. The model trained on old-distribution data is outdated after such changes. Retraining the whole model every time is expensive. Besides, at the beginning of normal pattern changes, there is not enough observation data from the new distribution. Retraining a large neural network model with limited data is vulnerable to overfitting. Thus, we propose a Light and Anti-overfitting Retraining Approach (LARA) for deep variational auto-encoder based time series anomaly detection methods (VAEs). This work aims to make three novel contributions: 1) the retraining process is formulated as a convex problem and can converge at a fast rate as well as prevent overfitting; 2) designing a ruminate block, which leverages the historical data without the need to store them; 3) mathematically proving that when fine-tuning the latent vector and reconstructed data, the linear formations can achieve the least adjusting errors between the ground truths and the fine-tuned ones.   Moreover, we have performed many experiments to verify that retraining LARA with even 43 time slots of data from new distribution can result in its competitive F1 Score in comparison with the state-of-the-art anomaly detection models trained with sufficient data. Besides, we verify its light overhead.
</details>
<details>
<summary>摘要</summary>
现有的异常检测模型大多假设常规模式一直不变。然而，Web服务中常规模式会频繁变化，训练过去的模型会变得异常。每次 retraining 整个模型都是昂贵的。此外，在常规模式变化的开始时，新分布中的数据不够，使用有限的数据重新训练大型神经网络模型容易过拟合。因此，我们提出了一种轻量级、避免过拟合的重新训练方法（LARA），用于深度变量自动编码器基于时间序列异常检测方法（VAEs）。本工作的三个新贡献如下：1. 重新训练过程被形式化为一个凸问题，可以快速 converge 并避免过拟合。2. 设计了一个留存块，可以利用历史数据而无需存储。3. 数学上证明，当微调 latent vector 和重构数据时，线性形式可以实现最小调整误差 между 真实值和微调后的值。此外，我们进行了多个实验，证明在使用43个时间槽的新分布数据重新训练LARA后，其竞争性F1分数与state-of-the-art异常检测模型相比较高。同时，我们还证明了其轻量级。
</details></li>
</ul>
<hr>
<h2 id="Binary-Classification-with-Confidence-Difference"><a href="#Binary-Classification-with-Confidence-Difference" class="headerlink" title="Binary Classification with Confidence Difference"></a>Binary Classification with Confidence Difference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05632">http://arxiv.org/abs/2310.05632</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wwangwitsel/ConfDiff">https://github.com/wwangwitsel/ConfDiff</a></li>
<li>paper_authors: Wei Wang, Lei Feng, Yuchen Jiang, Gang Niu, Min-Ling Zhang, Masashi Sugiyama</li>
<li>for: 本研究旨在利用信度差（Confidence Difference，简称ConfDiff）来进行Binary分类，而不需要每个训练样本的点击标签。</li>
<li>methods: 我们提出了一种风险一致的方法来解决这个问题，并证明了这个方法的整体趋势和减震性。</li>
<li>results: 我们在 benchmark 数据集和一个实际应用中的推荐系统数据集上进行了广泛的实验，并证明了我们的提议的有效性。<details>
<summary>Abstract</summary>
Recently, learning with soft labels has been shown to achieve better performance than learning with hard labels in terms of model generalization, calibration, and robustness. However, collecting pointwise labeling confidence for all training examples can be challenging and time-consuming in real-world scenarios. This paper delves into a novel weakly supervised binary classification problem called confidence-difference (ConfDiff) classification. Instead of pointwise labeling confidence, we are given only unlabeled data pairs with confidence difference that specifies the difference in the probabilities of being positive. We propose a risk-consistent approach to tackle this problem and show that the estimation error bound achieves the optimal convergence rate. We also introduce a risk correction approach to mitigate overfitting problems, whose consistency and convergence rate are also proven. Extensive experiments on benchmark data sets and a real-world recommender system data set validate the effectiveness of our proposed approaches in exploiting the supervision information of the confidence difference.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Cost-sensitive-probabilistic-predictions-for-support-vector-machines"><a href="#Cost-sensitive-probabilistic-predictions-for-support-vector-machines" class="headerlink" title="Cost-sensitive probabilistic predictions for support vector machines"></a>Cost-sensitive probabilistic predictions for support vector machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05997">http://arxiv.org/abs/2310.05997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandra Benítez-Peña, Rafael Blanquero, Emilio Carrizosa, Pepa Ramírez-Cobo</li>
<li>for: 这种方法是为了生成SVM模型中的概率输出，并且能够处理不均衡数据集，以及使用参数优化过程中生成的有价值信息来提高模型的性能。</li>
<li>methods: 这种方法使用了成本敏感的SVM模型，并将其嵌入到协同 ensemble 方法中，使用bootstrapEstimates来估计概率。</li>
<li>results: 数据测试表明，这种方法在各种数据集上比基准方法有着优异的性能。<details>
<summary>Abstract</summary>
Support vector machines (SVMs) are widely used and constitute one of the best examined and used machine learning models for two-class classification. Classification in SVM is based on a score procedure, yielding a deterministic classification rule, which can be transformed into a probabilistic rule (as implemented in off-the-shelf SVM libraries), but is not probabilistic in nature. On the other hand, the tuning of the regularization parameters in SVM is known to imply a high computational effort and generates pieces of information that are not fully exploited, not being used to build a probabilistic classification rule. In this paper we propose a novel approach to generate probabilistic outputs for the SVM. The new method has the following three properties. First, it is designed to be cost-sensitive, and thus the different importance of sensitivity (or true positive rate, TPR) and specificity (true negative rate, TNR) is readily accommodated in the model. As a result, the model can deal with imbalanced datasets which are common in operational business problems as churn prediction or credit scoring. Second, the SVM is embedded in an ensemble method to improve its performance, making use of the valuable information generated in the parameters tuning process. Finally, the probabilities estimation is done via bootstrap estimates, avoiding the use of parametric models as competing approaches. Numerical tests on a wide range of datasets show the advantages of our approach over benchmark procedures.
</details>
<details>
<summary>摘要</summary>
支持向量机（SVM）是广泛使用的机器学习模型之一，是二类分类中最好的考试和使用的模型之一。在SVM中的分类基于得分过程，得到了决定性的分类规则，可以转换为概率性的分类规则（如在各种SVM库中实现的），但是不是概率性的。然而，SVM的常量参数优化知识具有高计算成本和生成不完全利用的信息，不会建立概率分类规则。在这篇论文中，我们提出了一种新的方法，以生成SVM的概率输出。这种方法具有以下三个特点：首先，它是成本敏感的，可以 readily 折衔不均衡的数据集，这些数据集在运营商业问题中很常见，如脱退预测和信用评分。第二，SVM被嵌入到集成方法中，以提高其性能，利用参数优化过程中生成的有价值信息。最后，概率估计通过 bootstrap 估计进行，避免使用参数模型作为竞争方法。数值测试在各种数据集上表明了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="On-Prediction-Modelers-and-Decision-Makers-Why-Fairness-Requires-More-Than-a-Fair-Prediction-Model"><a href="#On-Prediction-Modelers-and-Decision-Makers-Why-Fairness-Requires-More-Than-a-Fair-Prediction-Model" class="headerlink" title="On Prediction-Modelers and Decision-Makers: Why Fairness Requires More Than a Fair Prediction Model"></a>On Prediction-Modelers and Decision-Makers: Why Fairness Requires More Than a Fair Prediction Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05598">http://arxiv.org/abs/2310.05598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teresa Scantamburlo, Joachim Baumann, Christoph Heitz</li>
<li>for: 本文旨在阐述在预测基于决策中的公平性问题，并提出一个框架来帮助实现公平性。</li>
<li>methods: 本文使用了概念分离技术，将预测和决策分为两个独立的步骤，以便更好地理解和实现公平性。</li>
<li>results: 本文提出了一个框架，可以帮助在预测基于决策中实现公平性，并提出了一些实现公平性的策略和方法。<details>
<summary>Abstract</summary>
An implicit ambiguity in the field of prediction-based decision-making regards the relation between the concepts of prediction and decision. Much of the literature in the field tends to blur the boundaries between the two concepts and often simply speaks of 'fair prediction.' In this paper, we point out that a differentiation of these concepts is helpful when implementing algorithmic fairness. Even if fairness properties are related to the features of the used prediction model, what is more properly called 'fair' or 'unfair' is a decision system, not a prediction model. This is because fairness is about the consequences on human lives, created by a decision, not by a prediction. We clarify the distinction between the concepts of prediction and decision and show the different ways in which these two elements influence the final fairness properties of a prediction-based decision system. In addition to exploring this relationship conceptually and practically, we propose a framework that enables a better understanding and reasoning of the conceptual logic of creating fairness in prediction-based decision-making. In our framework, we specify different roles, namely the 'prediction-modeler' and the 'decision-maker,' and the information required from each of them for being able to implement fairness of the system. Our framework allows for deriving distinct responsibilities for both roles and discussing some insights related to ethical and legal requirements. Our contribution is twofold. First, we shift the focus from abstract algorithmic fairness to context-dependent decision-making, recognizing diverse actors with unique objectives and independent actions. Second, we provide a conceptual framework that can help structure prediction-based decision problems with respect to fairness issues, identify responsibilities, and implement fairness governance mechanisms in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translation into Simplified Chinese一个隐式的歧义在预测基础决策领域是预测和决策的关系。大多数 литературе在这个领域通常会混同这两个概念，并只是说的'公平预测'。在这篇论文中，我们指出了这两个概念之间的分化是有助于实施算法公平的。即使公平性特性与预测模型的特性相关，但是真正是'公平'或'不公平'的是决策系统，不是预测模型。这是因为公平是关于人类生活的后果，而不是预测的结果。我们清楚地区分了预测和决策的概念，并显示了这两个元素在最终公平性质量上的不同影响。除了探讨这种关系的概念和实践方面，我们提出了一个框架，允许更好地理解和理解预测基础决策中的公平创造机制。在我们的框架中，我们详细定义了不同角色，包括'预测模型者'和'决策者'，以及它们所需的信息，以便实现预测基础决策系统的公平。我们的框架允许 derive出不同的责任，并讨论一些与伦理和法律要求相关的洞察。我们的贡献是两重的。首先，我们将焦点从抽象的算法公平转移到了 Context-dependent 决策，认可多种演员有独特的目标和独立行动。其次，我们提供了一个概念框架，可以帮助结构预测基础决策问题，识别责任，并在实际场景中实施公平管理机制。
</details></li>
</ul>
<hr>
<h2 id="ODEFormer-Symbolic-Regression-of-Dynamical-Systems-with-Transformers"><a href="#ODEFormer-Symbolic-Regression-of-Dynamical-Systems-with-Transformers" class="headerlink" title="ODEFormer: Symbolic Regression of Dynamical Systems with Transformers"></a>ODEFormer: Symbolic Regression of Dynamical Systems with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05573">http://arxiv.org/abs/2310.05573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdascoli/odeformer">https://github.com/sdascoli/odeformer</a></li>
<li>paper_authors: Stéphane d’Ascoli, Sören Becker, Alexander Mathis, Philippe Schwaller, Niki Kilbertus</li>
<li>for: 描述一种可以从单个解曲线观测数据中推断多维常微方程系统的符号形式传播模型（ODEFormer）。</li>
<li>methods: 使用变换器来推断多维常微方程系统的符号形式。</li>
<li>results: ODEFormer在两个数据集上（Strogatz和ODEBench）表现出色，在干扰和不规则观测数据中 Displaying substantially improved robustness and faster inference compared to existing methods。<details>
<summary>Abstract</summary>
We introduce ODEFormer, the first transformer able to infer multidimensional ordinary differential equation (ODE) systems in symbolic form from the observation of a single solution trajectory. We perform extensive evaluations on two datasets: (i) the existing "Strogatz" dataset featuring two-dimensional systems; (ii) ODEBench, a collection of one- to four-dimensional systems that we carefully curated from the literature to provide a more holistic benchmark. ODEFormer consistently outperforms existing methods while displaying substantially improved robustness to noisy and irregularly sampled observations, as well as faster inference. We release our code, model and benchmark dataset publicly.
</details>
<details>
<summary>摘要</summary>
我们介绍ODEFormer，首个能够从单一解析轨迹观测中推导多维常微方程系统的transformer。我们在两个数据集上进行了广泛的评估：（一）现有的“Strogatz”数据集，这是一个二维系统的数据集；（二）ODEBench，我们从文献中精心范选了一些一至四维系统，以提供更加全面的benchmark。ODEFormer在数据集上一般性高，而且在噪音和不规则采样观测下具有substantially提高的Robustness，以及更快的推导速度。我们将代码、模型和数据集公开发布。
</details></li>
</ul>
<hr>
<h2 id="A-New-Transformation-Approach-for-Uplift-Modeling-with-Binary-Outcome"><a href="#A-New-Transformation-Approach-for-Uplift-Modeling-with-Binary-Outcome" class="headerlink" title="A New Transformation Approach for Uplift Modeling with Binary Outcome"></a>A New Transformation Approach for Uplift Modeling with Binary Outcome</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05549">http://arxiv.org/abs/2310.05549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Li, Jiang Tian, Xiaojia Xiang</li>
<li>for: 这篇论文是关于如何实现更好的客户预测和精确的标的定义，以提高营销效果。</li>
<li>methods: 本论文使用的方法是一种新的变数方法，可以将原始的对象指标转换为一个新的变数，以便更好地预测客户的反应。</li>
<li>results: 实验结果显示，新的变数方法可以优化客户预测和标的定义，提高营销效果。此外，这种方法还可以轻松地应用在实际应用中。<details>
<summary>Abstract</summary>
Uplift modeling has been used effectively in fields such as marketing and customer retention, to target those customers who are more likely to respond due to the campaign or treatment. Essentially, it is a machine learning technique that predicts the gain from performing some action with respect to not taking it. A popular class of uplift models is the transformation approach that redefines the target variable with the original treatment indicator. These transformation approaches only need to train and predict the difference in outcomes directly. The main drawback of these approaches is that in general it does not use the information in the treatment indicator beyond the construction of the transformed outcome and usually is not efficient. In this paper, we design a novel transformed outcome for the case of the binary target variable and unlock the full value of the samples with zero outcome. From a practical perspective, our new approach is flexible and easy to use. Experimental results on synthetic and real-world datasets obviously show that our new approach outperforms the traditional one. At present, our new approach has already been applied to precision marketing in a China nation-wide financial holdings group.
</details>
<details>
<summary>摘要</summary>
《升级模型》在市场营销和客户保持方面得到了有效应用，以针对那些响应更高的客户进行投入。概括来说，它是一种机器学习技术，预测对不进行处理的结果所带来的提升。一种受欢迎的类型的升级模型是转换方法，它重新定义目标变量与原始治理器指标之间的关系。这些转换方法只需要训练和预测直接的差异。但是，这些方法通常不使用治理器指标中的信息，除了构建转换后的结果外。在这篇论文中，我们设计了一种新的转换结果，用于二分类目标变量的情况。我们的新方法可以充分利用零结果样本的信息，从而提高准确率。实验结果表明，我们的新方法在synthetic和实际数据集上明显超过传统方法。在一家中国国家范围内的金融控股集团中，我们的新方法已经应用于精准营销。（Note: Please note that the translation is provided as-is, and may not be perfect or completely idiomatic. However, it should be sufficient to convey the general meaning of the text.)
</details></li>
</ul>
<hr>
<h2 id="NetTiSA-Extended-IP-Flow-with-Time-series-Features-for-Universal-Bandwidth-constrained-High-speed-Network-Traffic-Classification"><a href="#NetTiSA-Extended-IP-Flow-with-Time-series-Features-for-Universal-Bandwidth-constrained-High-speed-Network-Traffic-Classification" class="headerlink" title="NetTiSA: Extended IP Flow with Time-series Features for Universal Bandwidth-constrained High-speed Network Traffic Classification"></a>NetTiSA: Extended IP Flow with Time-series Features for Universal Bandwidth-constrained High-speed Network Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05530">http://arxiv.org/abs/2310.05530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koumajos/classification_by_nettisa_flow">https://github.com/koumajos/classification_by_nettisa_flow</a></li>
<li>paper_authors: Josef Koumar, Karel Hynek, Jaroslav Pešek, Tomáš Čejka</li>
<li>for: 这篇论文旨在提出一种基于流量记录的网络流量监测方法，以便在各种网络基础设施上部署，包括承载数百万人的大型IPS网络。</li>
<li>methods: 该方法基于流量记录的时间序列分析，提出了一种新的扩展IP流记录（NetTiSA），并对25种网络类型任务进行了广泛的测试，以证明NetTiSA的广泛适用性和高可用性。</li>
<li>results: 测试结果表明，NetTiSA可以高度精准地分类网络流量，并且在计算流量扩展时对性能的影响较小。此外，NetTiSA可以在100Gbps级别的高速ISP网络上进行实际部署，因此可以提供广泛的网络安全保护。<details>
<summary>Abstract</summary>
Network traffic monitoring based on IP Flows is a standard monitoring approach that can be deployed to various network infrastructures, even the large IPS-based networks connecting millions of people. Since flow records traditionally contain only limited information (addresses, transport ports, and amount of exchanged data), they are also commonly extended for additional features that enable network traffic analysis with high accuracy. Nevertheless, the flow extensions are often too large or hard to compute, which limits their deployment only to smaller-sized networks. This paper proposes a novel extended IP flow called NetTiSA (Network Time Series Analysed), which is based on the analysis of the time series of packet sizes. By thoroughly testing 25 different network classification tasks, we show the broad applicability and high usability of NetTiSA, which often outperforms the best-performing related works. For practical deployment, we also consider the sizes of flows extended for NetTiSA and evaluate the performance impacts of its computation in the flow exporter. The novel feature set proved universal and deployable to high-speed ISP networks with 100\,Gbps lines; thus, it enables accurate and widespread network security protection.
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese:网络流量监测基于IP流是标准监测方法，可以部署到不同的网络基础设施，包括连接百万人的IPS网络。由于流记录通常只包含地址、传输端口和交换的数据量，因此它们常被扩展以获得高精度的网络流量分析。然而，扩展流量通常是太大或计算过程太复杂，因此只能在较小的网络上进行部署。本文提出了一种基于时间序列分析的增强IP流，称为NetTiSA（网络时间序列分析）。通过对25种不同的网络分类任务进行严格测试，我们表明NetTiSA的广泛适用性和高可用性。此外，我们还考虑了NetTiSA扩展流量的大小以及计算流程的性能影响。 results show that the novel feature set is universal and deployable to high-speed ISP networks with 100 Gbps lines, enabling accurate and widespread network security protection.Note: Simplified Chinese is a romanization of Chinese, it is not a direct translation of the original text. The translation is based on the pronunciation of the characters, and it may not be exactly the same as the original text.
</details></li>
</ul>
<hr>
<h2 id="A-novel-Network-Science-Algorithm-for-Improving-Triage-of-Patients"><a href="#A-novel-Network-Science-Algorithm-for-Improving-Triage-of-Patients" class="headerlink" title="A novel Network Science Algorithm for Improving Triage of Patients"></a>A novel Network Science Algorithm for Improving Triage of Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05996">http://arxiv.org/abs/2310.05996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pietro Hiram Guzzi, Annamaria De Filippo, Pierangelo Veltri</li>
<li>for: This paper aims to develop a novel algorithm for triaging patients based on the analysis of patient data, with the goal of improving the efficiency, accuracy, and consistency of patient prioritization.</li>
<li>methods: The algorithm is based on rigorous preprocessing and feature engineering of a comprehensive data set containing relevant patient information, such as vital signs, symptoms, and medical history.</li>
<li>results: The experimental results demonstrate that the algorithm achieved high accuracy and performance, outperforming traditional triage methods.<details>
<summary>Abstract</summary>
Patient triage plays a crucial role in healthcare, ensuring timely and appropriate care based on the urgency of patient conditions. Traditional triage methods heavily rely on human judgment, which can be subjective and prone to errors. Recently, a growing interest has been in leveraging artificial intelligence (AI) to develop algorithms for triaging patients. This paper presents the development of a novel algorithm for triaging patients. It is based on the analysis of patient data to produce decisions regarding their prioritization. The algorithm was trained on a comprehensive data set containing relevant patient information, such as vital signs, symptoms, and medical history. The algorithm was designed to accurately classify patients into triage categories through rigorous preprocessing and feature engineering. Experimental results demonstrate that our algorithm achieved high accuracy and performance, outperforming traditional triage methods. By incorporating computer science into the triage process, healthcare professionals can benefit from improved efficiency, accuracy, and consistency, prioritizing patients effectively and optimizing resource allocation. Although further research is needed to address challenges such as biases in training data and model interpretability, the development of AI-based algorithms for triaging patients shows great promise in enhancing healthcare delivery and patient outcomes.
</details>
<details>
<summary>摘要</summary>
医疗患者分类占据了医疗业中关键的地位，确保患者得到了时间适当的和适合的护理，根据患者的病情严重程度。传统的分类方法依赖于人类的判断，这可能是主观的和容易出错的。在最近的几年里，人们对使用人工智能（AI）开发患者分类算法表示了增加的兴趣。本文描述了一种基于患者数据分析的新的患者分类算法的开发。该算法通过对病人数据进行严格的预处理和特征工程来生成准确的患者分类结果。实验结果表明，我们的算法可以准确地将患者分为不同的分类 катего里，并且表现出了高度的准确率和性能，超过传统的分类方法。通过将计算机科学引入分类过程，医疗专业人员可以从而获得更高效、准确和一致的患者分类结果，优先级化患者，最大化资源的分配。虽然还需要进一步的研究，例如训练数据中存在的偏见和模型解释性等问题，但AI在患者分类中的应用显示了极大的潜力，以改善医疗服务和患者结果。
</details></li>
</ul>
<hr>
<h2 id="Projecting-infinite-time-series-graphs-to-finite-marginal-graphs-using-number-theory"><a href="#Projecting-infinite-time-series-graphs-to-finite-marginal-graphs-using-number-theory" class="headerlink" title="Projecting infinite time series graphs to finite marginal graphs using number theory"></a>Projecting infinite time series graphs to finite marginal graphs using number theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05526">http://arxiv.org/abs/2310.05526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Gerhardus, Jonas Wahl, Sofia Faltenbacher, Urmi Ninad, Jakob Runge</li>
<li>for: 本文是用于推广和应用 causal-graphical-model 框架的方法和应用工作的一种新方法。</li>
<li>methods: 本文提出了一种方法，可以将无穷时间序列图表示为 marginal graphical models，以解决在无穷图中的 $m$-separation 问题。</li>
<li>results: 本文提出了一种算法，可以将无穷时间序列图 projection 到 marginal graphical models，并证明这些 marginal graphs 可以用于 causal discovery 和 causal effect estimation。<details>
<summary>Abstract</summary>
In recent years, a growing number of method and application works have adapted and applied the causal-graphical-model framework to time series data. Many of these works employ time-resolved causal graphs that extend infinitely into the past and future and whose edges are repetitive in time, thereby reflecting the assumption of stationary causal relationships. However, most results and algorithms from the causal-graphical-model framework are not designed for infinite graphs. In this work, we develop a method for projecting infinite time series graphs with repetitive edges to marginal graphical models on a finite time window. These finite marginal graphs provide the answers to $m$-separation queries with respect to the infinite graph, a task that was previously unresolved. Moreover, we argue that these marginal graphs are useful for causal discovery and causal effect estimation in time series, effectively enabling to apply results developed for finite graphs to the infinite graphs. The projection procedure relies on finding common ancestors in the to-be-projected graph and is, by itself, not new. However, the projection procedure has not yet been algorithmically implemented for time series graphs since in these infinite graphs there can be infinite sets of paths that might give rise to common ancestors. We solve the search over these possibly infinite sets of paths by an intriguing combination of path-finding techniques for finite directed graphs and solution theory for linear Diophantine equations. By providing an algorithm that carries out the projection, our paper makes an important step towards a theoretically-grounded and method-agnostic generalization of a range of causal inference methods and results to time series.
</details>
<details>
<summary>摘要</summary>
近年来，一些方法和应用工作已经适应和应用了 causal-graphical-model 框架到时间序列数据。许多这些工作使用时间分解的 causal 图，其延伸到过去和未来无穷，并且图的边重复在时间上，表明了预设的站立 causal 关系。然而，大多数结果和算法从 causal-graphical-model 框架不适用于无穷图。在这种工作中，我们开发了一种方法，将无穷时间序列图的 repetitive 边投影到固定时间窗口内的 marginal 图形式。这些 marginal 图可以回答 $m$-separation 查询，对于无穷图来说，是以前未解决的问题。此外，我们认为这些 marginal 图对 causal 发现和 causal 效应估计在时间序列中都是有用的，因此可以将 finite 图上的结果应用到无穷图上。投影过程基于在要投影的图中寻找共同祖先的搜索，并不是新的。然而，在时间序列图上执行这种投影过程具有挑战，因为可能存在无穷多个路径，导致共同祖先。我们解决这个问题，通过一种独特的将 finite 图上的路径找到与无穷图相匹配的方法，并使用解决线性Diophantine方程的解辑理论。我们的论文提供了一种可以执行投影的算法，这个步骤对于在时间序列中应用 causal 推理方法和结果进行 theoretically-grounded 和方法-agnostic 的总体化做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="WeatherGNN-Exploiting-Complicated-Relationships-in-Numerical-Weather-Prediction-Bias-Correction"><a href="#WeatherGNN-Exploiting-Complicated-Relationships-in-Numerical-Weather-Prediction-Bias-Correction" class="headerlink" title="WeatherGNN: Exploiting Complicated Relationships in Numerical Weather Prediction Bias Correction"></a>WeatherGNN: Exploiting Complicated Relationships in Numerical Weather Prediction Bias Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05517">http://arxiv.org/abs/2310.05517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/water-wbq/WeatherGNN">https://github.com/water-wbq/WeatherGNN</a></li>
<li>paper_authors: Binqing Wu, Weiqi Chen, Wengwei Wang, Bingqing Peng, Liang Sun, Ling Chen</li>
<li>for:  corrected numerical weather prediction (NWP) bias</li>
<li>methods:  Graph Neural Networks (GNN) and factor-wise GNN, fast hierarchical GNN</li>
<li>results:  superior performance compared to other state-of-the-art (SOTA) methods, with an average improvement of 40.50% on RMSE compared to the original NWP.Here is the Chinese translation:</li>
<li>for:  corrected numerical weather prediction (NWP) 误差</li>
<li>methods:  Graph Neural Networks (GNN) 和分量 wise GNN, 快速层次 GNN</li>
<li>results:  与其他状态首选 (SOTA) 方法相比，平均提高40.50%的RMSE 相对于原始 NWP.<details>
<summary>Abstract</summary>
Numerical weather prediction (NWP) may be inaccurate or biased due to incomplete atmospheric physical processes, insufficient spatial-temporal resolution, and inherent uncertainty of weather. Previous studies have attempted to correct biases by using handcrafted features and domain knowledge, or by applying general machine learning models naively. They do not fully explore the complicated meteorologic interactions and spatial dependencies in the atmosphere dynamically, which limits their applicability in NWP bias-correction. Specifically, weather factors interact with each other in complex ways, and these interactions can vary regionally. In addition, the interactions between weather factors are further complicated by the spatial dependencies between regions, which are influenced by varied terrain and atmospheric motions. To address these issues, we propose WeatherGNN, an NWP bias-correction method that utilizes Graph Neural Networks (GNN) to learn meteorologic and geographic relationships in a unified framework. Our approach includes a factor-wise GNN that captures meteorological interactions within each grid (a specific location) adaptively, and a fast hierarchical GNN that captures spatial dependencies between grids dynamically. Notably, the fast hierarchical GNN achieves linear complexity with respect to the number of grids, enhancing model efficiency and scalability. Our experimental results on two real-world datasets demonstrate the superiority of WeatherGNN in comparison with other SOTA methods, with an average improvement of 40.50\% on RMSE compared to the original NWP.
</details>
<details>
<summary>摘要</summary>
numerical 天气预测（NWP）可能存在偏差或偏见，原因包括大气物理过程的缺失、时空分解不够细致，以及天气预测的内在不确定性。先前的研究已经尝试使用手工设计的特征和领域知识来纠正偏差，或者直接使用通用机器学习模型。但这些方法并未充分探索大气中复杂的物理互动和空间依赖关系，限制了它们在NWP偏差纠正中的应用。具体来说，天气因素之间存在复杂的互动，这些互动可能因地域而异，而且这些互动还受到不同的地形和大气动力的影响。为解决这些问题，我们提出了WeatherGNN，一种基于图神经网络（GNN）的NWP偏差纠正方法。我们的方法包括一个因素独立的GNN，可以在每个网格（具体位置）中适应地捕捉大气物理互动，以及一个快速的层次GNN，可以在不同网格之间快速捕捉空间依赖关系。吸引注意的是，快速的层次GNN在网格数量 linear 复杂度上具有优化，从而提高模型的效率和扩展性。我们的实验结果表明，WeatherGNN在两个真实世界数据集上的表现胜过其他SOTA方法，具有40.50%的RMSE提升。
</details></li>
</ul>
<hr>
<h2 id="A-Neural-Tangent-Kernel-View-on-Federated-Averaging-for-Deep-Linear-Neural-Network"><a href="#A-Neural-Tangent-Kernel-View-on-Federated-Averaging-for-Deep-Linear-Neural-Network" class="headerlink" title="A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network"></a>A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05495">http://arxiv.org/abs/2310.05495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Liu, Dazhi Zhan, Wei Tao, Xin Ma, Yu Pan, Yu Ding, Zhisong Pan</li>
<li>for: 这篇论文的目的是提供 FedAvg 在训练神经网络时的全球收敛性保证。</li>
<li>methods: 这篇论文使用 NTK 理论来研究 FedAvg 在训练神经网络时的收敛性。</li>
<li>results: 这篇论文提供了 FedAvg 在训练深度线性神经网络时的全球收敛性保证，并且通过实验验证了理论结论。<details>
<summary>Abstract</summary>
Federated averaging (FedAvg) is a widely employed paradigm for collaboratively training models from distributed clients without sharing data. Nowadays, the neural network has achieved remarkable success due to its extraordinary performance, which makes it a preferred choice as the model in FedAvg. However, the optimization problem of the neural network is often non-convex even non-smooth. Furthermore, FedAvg always involves multiple clients and local updates, which results in an inaccurate updating direction. These properties bring difficulties in analyzing the convergence of FedAvg in training neural networks. Recently, neural tangent kernel (NTK) theory has been proposed towards understanding the convergence of first-order methods in tackling the non-convex problem of neural networks. The deep linear neural network is a classical model in theoretical subject due to its simple formulation. Nevertheless, there exists no theoretical result for the convergence of FedAvg in training the deep linear neural network. By applying NTK theory, we make a further step to provide the first theoretical guarantee for the global convergence of FedAvg in training deep linear neural networks. Specifically, we prove FedAvg converges to the global minimum at a linear rate $\mathcal{O}\big((1-\eta K /N)^t\big)$, where $t$ is the number of iterations, $\eta$ is the learning rate, $N$ is the number of clients and $K$ is the number of local updates. Finally, experimental evaluations on two benchmark datasets are conducted to empirically validate the correctness of our theoretical findings.
</details>
<details>
<summary>摘要</summary>
《联合平均（FedAvg）》是一种广泛使用的方法，用于在分布式客户端上共同训练模型而无需分享数据。现在，神经网络已经取得了很大的成功，使得它成为了FedAvg中的首选模型。然而，神经网络的优化问题经常是非凸的，甚至是不满足的。此外，FedAvg总是包括多个客户端和本地更新，这会导致不准确的更新方向。这些特性使得分析FedAvg在训练神经网络的 converges 变得更加困难。近年来，神经积极核（NTK）理论被提出，用于理解在非凸神经网络中第一个方法的converges。深度线性神经网络是神经网络理论中的经典模型，然而，关于FedAvg在训练深度线性神经网络的converges的理论结果并未存在。通过应用NTK理论，我们做出了一个进一步的步骤，提供了对FedAvg在训练深度线性神经网络的全球最佳化的首次理论保证。具体来说，我们证明FedAvg会在$(1-\eta K/N)^t$的线性速率下收敛到全球最小值，其中$t$是迭代次数，$\eta$是学习率，$N$是客户端的数量，$K$是本地更新的数量。最后，我们在两个标准数据集上进行了实验评估，以验证我们的理论发现的正确性。
</details></li>
</ul>
<hr>
<h2 id="Integration-free-Training-for-Spatio-temporal-Multimodal-Covariate-Deep-Kernel-Point-Processes"><a href="#Integration-free-Training-for-Spatio-temporal-Multimodal-Covariate-Deep-Kernel-Point-Processes" class="headerlink" title="Integration-free Training for Spatio-temporal Multimodal Covariate Deep Kernel Point Processes"></a>Integration-free Training for Spatio-temporal Multimodal Covariate Deep Kernel Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05485">http://arxiv.org/abs/2310.05485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixuan Zhang, Quyu Kong, Feng Zhou</li>
<li>for: 本研究提出了一种新的深度空间时间点 процесс模型（深度混合点过程），即DKMPP，该模型利用多modal的covariate信息。</li>
<li>methods: DKMPP使用一种更 flexible的深度kernel来模型事件和covariate数据之间的复杂关系，从而提高模型的表达能力。</li>
<li>results: 我们的实验表明，DKMPP和其相应的分数基 estimator在基eline模型之上表现出优异，展示了将covariate信息、深度kernel和分数基 estimator相结合的优势。<details>
<summary>Abstract</summary>
In this study, we propose a novel deep spatio-temporal point process model, Deep Kernel Mixture Point Processes (DKMPP), that incorporates multimodal covariate information. DKMPP is an enhanced version of Deep Mixture Point Processes (DMPP), which uses a more flexible deep kernel to model complex relationships between events and covariate data, improving the model's expressiveness. To address the intractable training procedure of DKMPP due to the non-integrable deep kernel, we utilize an integration-free method based on score matching, and further improve efficiency by adopting a scalable denoising score matching method. Our experiments demonstrate that DKMPP and its corresponding score-based estimators outperform baseline models, showcasing the advantages of incorporating covariate information, utilizing a deep kernel, and employing score-based estimators.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一种新的深度空间时间点过程模型，深度混合点过程（DKMPP），该模型利用多Modal covariate信息。DKMPP是DMPP的改进版本，它使用更 flexible的深度核函数来模型事件和 covariate数据之间的复杂关系，提高模型的表达力。为了解决DKMPP的训练过程中的非可 интегриble深度核函数问题，我们使用了不需要 интеграción的得分匹配方法，并通过采用扩展的净化得分匹配方法来提高效率。我们的实验表明，DKMPP和其相应的得分基估计器在比例模型和事件时间点过程模型方面具有优势， demonstrating the benefits of incorporating covariate information, using a deep kernel, and employing score-based estimators.Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Vibroacoustic-Frequency-Response-Prediction-with-Query-based-Operator-Networks"><a href="#Vibroacoustic-Frequency-Response-Prediction-with-Query-based-Operator-Networks" class="headerlink" title="Vibroacoustic Frequency Response Prediction with Query-based Operator Networks"></a>Vibroacoustic Frequency Response Prediction with Query-based Operator Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05469">http://arxiv.org/abs/2310.05469</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecker-lab/FQ-Operator">https://github.com/ecker-lab/FQ-Operator</a></li>
<li>paper_authors: Jan van Delden, Julius Schultz, Christopher Blech, Sabine C. Langer, Timo Lüddecke</li>
<li>for: 本研究旨在提高机械结构如飞机、汽车和房屋等的震动声波传播的理解，以确保其用户的健康和舒适性。</li>
<li>methods: 本研究使用数据驱动模型来加速 numerical simulation，以便进行设计优化、不确定性评估和设计空间探索等任务。特别是，我们提出了一种新的频率查询运算符模型，该模型可以将板体几何特征映射到频率响应函数。</li>
<li>results: 我们在一个包含12,000个板体几何特征的全面性 benchmark 上评估了我们的方法，并发现它比 DeepONets、Fourier Neural Operators 和传统神经网络架构更高效。<details>
<summary>Abstract</summary>
Understanding vibroacoustic wave propagation in mechanical structures like airplanes, cars and houses is crucial to ensure health and comfort of their users. To analyze such systems, designers and engineers primarily consider the dynamic response in the frequency domain, which is computed through expensive numerical simulations like the finite element method. In contrast, data-driven surrogate models offer the promise of speeding up these simulations, thereby facilitating tasks like design optimization, uncertainty quantification, and design space exploration. We present a structured benchmark for a representative vibroacoustic problem: Predicting the frequency response for vibrating plates with varying forms of beadings. The benchmark features a total of 12,000 plate geometries with an associated numerical solution and introduces evaluation metrics to quantify the prediction quality. To address the frequency response prediction task, we propose a novel frequency query operator model, which is trained to map plate geometries to frequency response functions. By integrating principles from operator learning and implicit models for shape encoding, our approach effectively addresses the prediction of resonance peaks of frequency responses. We evaluate the method on our vibrating-plates benchmark and find that it outperforms DeepONets, Fourier Neural Operators and more traditional neural network architectures. The code and dataset are available from https://eckerlab.org/code/delden2023_plate.
</details>
<details>
<summary>摘要</summary>
In this study, we present a structured benchmark for a representative vibroacoustic problem: predicting the frequency response of vibrating plates with varying forms of beadings. The benchmark features a total of 12,000 plate geometries with associated numerical solutions and introduces evaluation metrics to quantify prediction quality. To address the frequency response prediction task, we propose a novel frequency query operator model, which is trained to map plate geometries to frequency response functions. By integrating principles from operator learning and implicit models for shape encoding, our approach effectively predicts the resonance peaks of frequency responses.We evaluate our method on our vibrating-plates benchmark and find that it outperforms DeepONets, Fourier Neural Operators, and more traditional neural network architectures. The code and dataset are available at <https://eckerlab.org/code/delden2023_plate>.
</details></li>
</ul>
<hr>
<h2 id="ExIFFI-and-EIF-Interpretability-and-Enhanced-Generalizability-to-Extend-the-Extended-Isolation-Forest"><a href="#ExIFFI-and-EIF-Interpretability-and-Enhanced-Generalizability-to-Extend-the-Extended-Isolation-Forest" class="headerlink" title="ExIFFI and EIF+: Interpretability and Enhanced Generalizability to Extend the Extended Isolation Forest"></a>ExIFFI and EIF+: Interpretability and Enhanced Generalizability to Extend the Extended Isolation Forest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05468">http://arxiv.org/abs/2310.05468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alessioarcudi/exiffi">https://github.com/alessioarcudi/exiffi</a></li>
<li>paper_authors: Alessio Arcudi, Davide Frizzo, Chiara Masiero, Gian Antonio Susto</li>
<li>for: 本研究旨在提出一种可解释的异常检测方法，以帮助用户更好地理解模型的预测结果并进行根本分析。</li>
<li>methods: 本研究使用了一种加强版的扩展隔离林（EIF），并提出了一种新的可解释方法ExIFFI，该方法通过特征排名来提供异常检测结果的解释。</li>
<li>results: 实验结果显示，ExIFFI在异常检测和特征选择方面具有较高的效果和可解释性。此外，研究还提供了一些实际数据集的评估结果，以便进一步研究和复现。<details>
<summary>Abstract</summary>
Anomaly detection, an essential unsupervised machine learning task, involves identifying unusual behaviors within complex datasets and systems. While Machine Learning algorithms and decision support systems (DSSs) offer effective solutions for this task, simply pinpointing anomalies often falls short in real-world applications. Users of these systems often require insight into the underlying reasons behind predictions to facilitate Root Cause Analysis and foster trust in the model. However, due to the unsupervised nature of anomaly detection, creating interpretable tools is challenging. This work introduces EIF+, an enhanced variant of Extended Isolation Forest (EIF), designed to enhance generalization capabilities. Additionally, we present ExIFFI, a novel approach that equips Extended Isolation Forest with interpretability features, specifically feature rankings. Experimental results provide a comprehensive comparative analysis of Isolation-based approaches for Anomaly Detection, including synthetic and real dataset evaluations that demonstrate ExIFFI's effectiveness in providing explanations. We also illustrate how ExIFFI serves as a valid feature selection technique in unsupervised settings. To facilitate further research and reproducibility, we also provide open-source code to replicate the results.
</details>
<details>
<summary>摘要</summary>
异常检测是机器学习中的一项不supervised任务，它的目的是在复杂的数据和系统中找到不寻常的行为。而机器学习算法和决策支持系统（DSS）可以提供有效的解决方案，但仅仅找到异常点不足以应对实际应用中的需求。用户需要对模型预测的根本原因进行分析，以便进行根本分析和增加信任。然而，由于异常检测的无supervised性，创建可解释的工具是困难的。本工作提出了EIF+，一个优化的扩展隔离林（EIF）的变体，旨在增强其一般化能力。此外，我们还提出了ExIFFI，一个新的方法，它将扩展隔离林与可解释特性结合起来。ExIFFI在实验中与其他隔离基于方法进行比较分析，包括 sintetic 和实际数据评估，以显示ExIFFI在提供解释方面的效果。我们还证明了ExIFFI可以作为无supervised设定下的特性选择技术。为便进一步研究和重现，我们还提供了开源代码，以便重现结果。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Convolutional-Explorer-Helps-Understand-1D-CNN’s-Learning-Behavior-in-Time-Series-Classification-from-Frequency-Domain"><a href="#Temporal-Convolutional-Explorer-Helps-Understand-1D-CNN’s-Learning-Behavior-in-Time-Series-Classification-from-Frequency-Domain" class="headerlink" title="Temporal Convolutional Explorer Helps Understand 1D-CNN’s Learning Behavior in Time Series Classification from Frequency Domain"></a>Temporal Convolutional Explorer Helps Understand 1D-CNN’s Learning Behavior in Time Series Classification from Frequency Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05467">http://arxiv.org/abs/2310.05467</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jrzhang33/tce">https://github.com/jrzhang33/tce</a></li>
<li>paper_authors: Junru Zhang, Lang Feng, Yang He, Yuhan Wu, Yabo Dong</li>
<li>for: 提高一维卷积神经网络（1D-CNN）在时间序列分类任务中的表现，并解释它们在应用中的不desirable outcome。</li>
<li>methods: 提出了一种Temporal Convolutional Explorer（TCE）来从频谱角度 empirically explore 1D-CNN 的学习行为。</li>
<li>results: 通过对 widely-used UCR、UEA 和 UCI 测试集进行了广泛的实验，显示了以下三点：1) TCE 对 1D-CNN 的学习行为提供了深入的理解; 2) 我们的 regulatory framework 可以在现有的 1D-CNN 中实现更好的表现，具有更少的存储和计算开销。<details>
<summary>Abstract</summary>
While one-dimensional convolutional neural networks (1D-CNNs) have been empirically proven effective in time series classification tasks, we find that there remain undesirable outcomes that could arise in their application, motivating us to further investigate and understand their underlying mechanisms. In this work, we propose a Temporal Convolutional Explorer (TCE) to empirically explore the learning behavior of 1D-CNNs from the perspective of the frequency domain. Our TCE analysis highlights that deeper 1D-CNNs tend to distract the focus from the low-frequency components leading to the accuracy degradation phenomenon, and the disturbing convolution is the driving factor. Then, we leverage our findings to the practical application and propose a regulatory framework, which can easily be integrated into existing 1D-CNNs. It aims to rectify the suboptimal learning behavior by enabling the network to selectively bypass the specified disturbing convolutions. Finally, through comprehensive experiments on widely-used UCR, UEA, and UCI benchmarks, we demonstrate that 1) TCE's insight into 1D-CNN's learning behavior; 2) our regulatory framework enables state-of-the-art 1D-CNNs to get improved performances with less consumption of memory and computational overhead.
</details>
<details>
<summary>摘要</summary>
一维数据列表（1D-CNN）已经在时间序列分类任务中被证明有效，但我们发现在其应用中可能出现不жела的结果，这使我们更深入研究和理解它们的下面机制。在这种工作中，我们提出了时间卷积探索器（TCE）来从频谱频率角度 empirically 探索 1D-CNN 的学习行为。我们的 TCE 分析表明，深度 1D-CNN 会干扰低频组件，导致精度下降现象，并且干扰卷积是驱动因素。然后，我们利用我们的发现来实际应用中，并提出了一种监管框架，可以轻松地integrated into  existing 1D-CNNs。它的目的是通过选择ively bypass  specify 干扰卷积来纠正不佳的学习行为，从而提高 state-of-the-art 1D-CNNs 的性能，同时减少内存和计算负担。最后，通过对 UCR、UEA 和 UCI 测试集进行了广泛的实验，我们证明了以下两点：1) TCE 对 1D-CNN 的学习行为提供了深入的理解; 2) 我们的监管框架可以使 state-of-the-art 1D-CNNs 获得更好的性能，同时减少内存和计算负担。
</details></li>
</ul>
<hr>
<h2 id="Reward-Consistent-Dynamics-Models-are-Strongly-Generalizable-for-Offline-Reinforcement-Learning"><a href="#Reward-Consistent-Dynamics-Models-are-Strongly-Generalizable-for-Offline-Reinforcement-Learning" class="headerlink" title="Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning"></a>Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05422">http://arxiv.org/abs/2310.05422</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sw-packages/a498e1142fb23106c12b054225864aab1156087a5ab634a1d88227024ecb1626">https://github.com/sw-packages/a498e1142fb23106c12b054225864aab1156087a5ab634a1d88227024ecb1626</a></li>
<li>paper_authors: Fan-Ming Luo, Tian Xu, Xingchen Cao, Yang Yu<br>for:* 这种研究旨在提高offline reinforcement learning的精度和可行性。methods:* 研究人员提出了一种名为”动力奖励”的隐藏因素，它在不同的过程中保持一致，从而提高了模型的泛化能力。results:* 在synthetic任务上，MOREC具有强大的泛化能力，可以 surprisngly回归一些远见过程。* 在21个offline任务上，MOREC超越了之前的最佳性能，升幅分别为4.6%和25.9%。* MOREC是第一种可以在6个D4RL任务和3个NeoRL任务中达到95%以上在线RL性能的方法。<details>
<summary>Abstract</summary>
Learning a precise dynamics model can be crucial for offline reinforcement learning, which, unfortunately, has been found to be quite challenging. Dynamics models that are learned by fitting historical transitions often struggle to generalize to unseen transitions. In this study, we identify a hidden but pivotal factor termed dynamics reward that remains consistent across transitions, offering a pathway to better generalization. Therefore, we propose the idea of reward-consistent dynamics models: any trajectory generated by the dynamics model should maximize the dynamics reward derived from the data. We implement this idea as the MOREC (Model-based Offline reinforcement learning with Reward Consistency) method, which can be seamlessly integrated into previous offline model-based reinforcement learning (MBRL) methods. MOREC learns a generalizable dynamics reward function from offline data, which is subsequently employed as a transition filter in any offline MBRL method: when generating transitions, the dynamics model generates a batch of transitions and selects the one with the highest dynamics reward value. On a synthetic task, we visualize that MOREC has a strong generalization ability and can surprisingly recover some distant unseen transitions. On 21 offline tasks in D4RL and NeoRL benchmarks, MOREC improves the previous state-of-the-art performance by a significant margin, i.e., 4.6% on D4RL tasks and 25.9% on NeoRL tasks. Notably, MOREC is the first method that can achieve above 95% online RL performance in 6 out of 12 D4RL tasks and 3 out of 9 NeoRL tasks.
</details>
<details>
<summary>摘要</summary>
学习准确的动力学模型可能是关键的，尤其是在线上学习中。然而，很遗憾的是，通过历史转移来学习的动力学模型往往难以泛化到未经看过的转移。在这项研究中，我们发现了一个隐藏的但是重要的因素，即动力奖励（dynamics reward），该因素在转移中保持一致。因此，我们提出了奖励一致的动力学模型（MOREC），即任何由动力学模型生成的转移都应该 Maximize the dynamics reward derived from the data。我们实现了这个想法，并将其与前期的Offline Model-based Reinforcement Learning（MBRL）方法相结合。MOREC可以从历史数据中学习一个通用的动力奖励函数，然后将其用作历史数据中的转移筛选器。当生成转移时，动力模型会生成一批转移，并选择具有最高动力奖励值的转移。在一个 synthetic task 上，我们可见地发现，MOREC具有强大的泛化能力，可以 surprisingly 回归一些远程未经看过的转移。在 D4RL 和 NeoRL benchmark 上的 21 个 Offline task 上，MOREC 提高了之前的状态核心性能，即 4.6% 在 D4RL 任务上和 25.9% 在 NeoRL 任务上。特别是，MOREC 是第一个可以达到上述 95% 在线RL 性能的 6 个 D4RL 任务和 3 个 NeoRL 任务。
</details></li>
</ul>
<hr>
<h2 id="On-sparse-regression-Lp-regularization-and-automated-model-discovery"><a href="#On-sparse-regression-Lp-regularization-and-automated-model-discovery" class="headerlink" title="On sparse regression, Lp-regularization, and automated model discovery"></a>On sparse regression, Lp-regularization, and automated model discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06872">http://arxiv.org/abs/2310.06872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy A. McCulloch, Skyler R. St. Pierre, Kevin Linka, Ellen Kuhl</li>
<li>For:  automatic model discovery and induce sparsity in nonlinear regression for material modeling* Methods:  hybrid approach combining regularization and physical constraints, Lp regularization, constitutive neural networks, L2, L1, L0 regularization* Results:  discovery of interpretable models and physically meaningful parameters, demonstration of Lp regularized constitutive neural networks’ ability to simultaneously discover both interpretability and predictability, and potential applications in generative material design and discovery of new materials with user-defined properties.<details>
<summary>Abstract</summary>
Sparse regression and feature extraction are the cornerstones of knowledge discovery from massive data. Their goal is to discover interpretable and predictive models that provide simple relationships among scientific variables. While the statistical tools for model discovery are well established in the context of linear regression, their generalization to nonlinear regression in material modeling is highly problem-specific and insufficiently understood. Here we explore the potential of neural networks for automatic model discovery and induce sparsity by a hybrid approach that combines two strategies: regularization and physical constraints. We integrate the concept of Lp regularization for subset selection with constitutive neural networks that leverage our domain knowledge in kinematics and thermodynamics. We train our networks with both, synthetic and real data, and perform several thousand discovery runs to infer common guidelines and trends: L2 regularization or ridge regression is unsuitable for model discovery; L1 regularization or lasso promotes sparsity, but induces strong bias; only L0 regularization allows us to transparently fine-tune the trade-off between interpretability and predictability, simplicity and accuracy, and bias and variance. With these insights, we demonstrate that Lp regularized constitutive neural networks can simultaneously discover both, interpretable models and physically meaningful parameters. We anticipate that our findings will generalize to alternative discovery techniques such as sparse and symbolic regression, and to other domains such as biology, chemistry, or medicine. Our ability to automatically discover material models from data could have tremendous applications in generative material design and open new opportunities to manipulate matter, alter properties of existing materials, and discover new materials with user-defined properties.
</details>
<details>
<summary>摘要</summary>
匿密回归和特征提取是知识发现大数据的基石。它们的目标是从科学变量之间找到可解释性强的预测模型，提供简单的关系。虽然 Linear 回归的统计工具已经在 context 中得到了良好的定制，但在非线性回归方面，它们在材料模型中的普遍性和不够了解。我们在这里探索使用神经网络自动发现模型的潜力，并通过混合两种策略来实现匿密性：规则化和物理约束。我们将 Lp 规则化用于子集选择与物理神经网络结合，并将其训练于both synthetic 和实际数据。我们进行了数千次发现运行，以推导出一些常见的指南和趋势：L2 规则化或ridge regression 不适合模型发现; L1 规则化或lasso 会导致匿密性，但会带来强烈的偏见;只有 L0 规则化可以透明地调整 interpretability 和预测性、简单性和准确性、偏见和偏差的负荷。通过这些发现，我们证明了 Lp 规则化的 constitutive 神经网络可以同时发现可解释性模型和物理意义的参数。我们预计这些发现将普遍到其他发现技术，如稀疏和符号回归，并在生物、化学、医学等领域得到应用。我们的自动发现材料模型技术可能会在生成材料设计中具有巨大的应用，开启新的材料性能控制和物质性能改变的可能性，以及发现新的材料。
</details></li>
</ul>
<hr>
<h2 id="Entropy-MCMC-Sampling-from-Flat-Basins-with-Ease"><a href="#Entropy-MCMC-Sampling-from-Flat-Basins-with-Ease" class="headerlink" title="Entropy-MCMC: Sampling from Flat Basins with Ease"></a>Entropy-MCMC: Sampling from Flat Basins with Ease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05401">http://arxiv.org/abs/2310.05401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bolian Li, Ruqi Zhang</li>
<li>for: 这个论文的目的是提出一种偏置采样方法，以优化深度学习模型的 posterior 采样。</li>
<li>methods: 该方法基于一个辅助变量，使 MCMC 采样器偏向平坦区域，从而提高采样效率和准确性。</li>
<li>results: 实验结果表明，该方法可以成功采样到深度学习模型的平坦区域，并在多个 bencmarks 上表现出色，包括分类、准确性和异常检测等。<details>
<summary>Abstract</summary>
Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our method and further show that it converges faster than several existing flatness-aware methods in the strongly convex setting. Empirical results demonstrate that our method can successfully sample from flat basins of the posterior, and outperforms all compared baselines on multiple benchmarks including classification, calibration, and out-of-distribution detection.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Find-Your-Optimal-Assignments-On-the-fly-A-Holistic-Framework-for-Clustered-Federated-Learning"><a href="#Find-Your-Optimal-Assignments-On-the-fly-A-Holistic-Framework-for-Clustered-Federated-Learning" class="headerlink" title="Find Your Optimal Assignments On-the-fly: A Holistic Framework for Clustered Federated Learning"></a>Find Your Optimal Assignments On-the-fly: A Holistic Framework for Clustered Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05397">http://arxiv.org/abs/2310.05397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongxin Guo, Xiaoying Tang, Tao Lin</li>
<li>for: 这个论文旨在探讨现有的分布式机器学习方法中，如何处理客户端数据不同性，以提高模型在所有客户端上的表现。</li>
<li>methods: 该论文使用了聚类技术来解决客户端数据不同性的问题，并提出了一种四层框架，称为HCFL，以涵盖和扩展现有的方法。</li>
<li>results: 该论文通过广泛的数值评估表明，使用提出的聚类方法可以提高模型在客户端数据不同性下的表现，并且提出了进一步改进的聚类方法。<details>
<summary>Abstract</summary>
Federated Learning (FL) is an emerging distributed machine learning approach that preserves client privacy by storing data on edge devices. However, data heterogeneity among clients presents challenges in training models that perform well on all local distributions. Recent studies have proposed clustering as a solution to tackle client heterogeneity in FL by grouping clients with distribution shifts into different clusters. However, the diverse learning frameworks used in current clustered FL methods make it challenging to integrate various clustered FL methods, gather their benefits, and make further improvements.   To this end, this paper presents a comprehensive investigation into current clustered FL methods and proposes a four-tier framework, namely HCFL, to encompass and extend existing approaches. Based on the HCFL, we identify the remaining challenges associated with current clustering methods in each tier and propose an enhanced clustering method called HCFL+ to address these challenges. Through extensive numerical evaluations, we showcase the effectiveness of our clustering framework and the improved components. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种emerging distributed machine learningapproach that preserves client privacy by storing data on edge devices. However, data heterogeneity among clients presents challenges in training models that perform well on all local distributions. Recent studies have proposed clustering as a solution to tackle client heterogeneity in FL by grouping clients with distribution shifts into different clusters. However, the diverse learning frameworks used in current clustered FL methods make it challenging to integrate various clustered FL methods, gather their benefits, and make further improvements.   To this end, this paper presents a comprehensive investigation into current clustered FL methods and proposes a four-tier framework, namely HCFL, to encompass and extend existing approaches. Based on the HCFL, we identify the remaining challenges associated with current clustering methods in each tier and propose an enhanced clustering method called HCFL+ to address these challenges. Through extensive numerical evaluations, we showcase the effectiveness of our clustering framework and the improved components. Our code will be publicly available.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Robust-Image-Watermarking-based-on-Cross-Attention-and-Invariant-Domain-Learning"><a href="#Robust-Image-Watermarking-based-on-Cross-Attention-and-Invariant-Domain-Learning" class="headerlink" title="Robust Image Watermarking based on Cross-Attention and Invariant Domain Learning"></a>Robust Image Watermarking based on Cross-Attention and Invariant Domain Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05395">http://arxiv.org/abs/2310.05395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnibh Dasgupta, Xin Zhong</li>
<li>for: 这 paper 是为了提出一种robust image watermarking方法，用于嵌入和提取 watermark within a cover image，并且使用深度学习approaches增强总结和鲁棒性。</li>
<li>methods: 这 paper 使用了 convolution 和 concatenation 来实现 watermark embedding，同时也integrate了可能的 augmentation 进行训练。</li>
<li>results: 这 paper 提出了 two novel 和 significannot advancements：first, 使用 multi-head cross attention mechanism 来实现 watermark embedding，以便在 cover image 和 watermark之间进行信息交换，并且identify semantically suitable embedding locations。second, 提出了 learning an invariant domain representation 来捕捉 both semantic 和 noise-invariant information concerning the watermark，这对于提高 image watermarking technique 是非常有价值的。<details>
<summary>Abstract</summary>
Image watermarking involves embedding and extracting watermarks within a cover image, with deep learning approaches emerging to bolster generalization and robustness. Predominantly, current methods employ convolution and concatenation for watermark embedding, while also integrating conceivable augmentation in the training process. This paper explores a robust image watermarking methodology by harnessing cross-attention and invariant domain learning, marking two novel, significant advancements. First, we design a watermark embedding technique utilizing a multi-head cross attention mechanism, enabling information exchange between the cover image and watermark to identify semantically suitable embedding locations. Second, we advocate for learning an invariant domain representation that encapsulates both semantic and noise-invariant information concerning the watermark, shedding light on promising avenues for enhancing image watermarking techniques.
</details>
<details>
<summary>摘要</summary>
Image watermarking 图像水印技术 involves embedding and extracting watermarks within a cover image, with deep learning approaches emerging to enhance generalization and robustness. Predominantly, current methods use convolution and concatenation for watermark embedding, while also incorporating possible augmentation in the training process. This paper explores a robust image watermarking methodology by harnessing cross-attention and invariant domain learning, introducing two novel, significant advancements. First, we design a watermark embedding technique utilizing a multi-head cross attention mechanism, enabling information exchange between the cover image and watermark to identify semantically suitable embedding locations. Second, we advocate for learning an invariant domain representation that encapsulates both semantic and noise-invariant information concerning the watermark, shedding light on promising avenues for enhancing image watermarking techniques.Here's the translation breakdown:Image watermarking 图像水印技术 (watermarking technique)involves embedding and extracting watermarks within a cover image, 图像 (cover image)with deep learning approaches emerging to enhance generalization and robustness. 使用深度学习方法提高泛化和鲁棒性。Predominantly, current methods use convolution and concatenation for watermark embedding, 当今主要方法使用卷积和 concatenation 进行水印嵌入。while also incorporating possible augmentation in the training process. 同时在训练过程中也包含可能的增强。This paper explores a robust image watermarking methodology by harnessing cross-attention and invariant domain learning, 本文探讨了一种基于对比注意力和不变域学习的图像水印方法。marking two novel, significant advancements. 标志着两个新、重要的进步。First, we design a watermark embedding technique utilizing a multi-head cross attention mechanism, 首先，我们设计了一种基于多头对比注意力机制的水印嵌入技术。enabling information exchange between the cover image and watermark to identify semantically suitable embedding locations. 使得图像和水印之间进行信息交换，以便在意义上适当的嵌入位置。Second, we advocate for learning an invariant domain representation that encapsulates both semantic and noise-invariant information concerning the watermark, 第二，我们提倡学习一种不变域表示，包含水印中的 semantic 和噪音不变信息。shedding light on promising avenues for enhancing image watermarking techniques. 探讨了图像水印技术的可能的提高方向。
</details></li>
</ul>
<hr>
<h2 id="Equation-Discovery-with-Bayesian-Spike-and-Slab-Priors-and-Efficient-Kernels"><a href="#Equation-Discovery-with-Bayesian-Spike-and-Slab-Priors-and-Efficient-Kernels" class="headerlink" title="Equation Discovery with Bayesian Spike-and-Slab Priors and Efficient Kernels"></a>Equation Discovery with Bayesian Spike-and-Slab Priors and Efficient Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05387">http://arxiv.org/abs/2310.05387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Da Long, Wei W. Xing, Aditi S. Krishnapriyan, Robert M. Kirby, Shandian Zhe, Michael W. Mahoney</li>
<li>For: The paper is written for discovering governing equations from data, which is important in many scientific and engineering applications.* Methods: The paper proposes a novel equation discovery method based on Kernel learning and Bayesian Spike-and-Slab priors (KBASS), which combines kernel regression with a Bayesian spike-and-slab prior for effective operator selection and uncertainty quantification.* Results: The paper shows the significant advantages of KBASS on a list of benchmark ODE and PDE discovery tasks, demonstrating its ability to overcome data sparsity and noise issues, as well as provide uncertainty quantification.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了发现数据中的权导方程，这对科学和工程应用来说非常重要。* Methods: 这篇论文提出了一种基于kernel学习和抽象积分架的方法（KBASS），它将kernel regression与抽象积分架相结合，以实现有效的运算选择和uncertainty评估。* Results: 论文在一系列的benchmark ODE和PDE发现任务上显示了KBASS的显著优势，证明了它在数据稀缺和噪声问题上的可行性，并且可以提供uncertainty评估。<details>
<summary>Abstract</summary>
Discovering governing equations from data is important to many scientific and engineering applications. Despite promising successes, existing methods are still challenged by data sparsity as well as noise issues, both of which are ubiquitous in practice. Moreover, state-of-the-art methods lack uncertainty quantification and/or are costly in training. To overcome these limitations, we propose a novel equation discovery method based on Kernel learning and BAyesian Spike-and-Slab priors (KBASS). We use kernel regression to estimate the target function, which is flexible, expressive, and more robust to data sparsity and noises. We combine it with a Bayesian spike-and-slab prior -- an ideal Bayesian sparse distribution -- for effective operator selection and uncertainty quantification. We develop an expectation propagation expectation-maximization (EP-EM) algorithm for efficient posterior inference and function estimation. To overcome the computational challenge of kernel regression, we place the function values on a mesh and induce a Kronecker product construction, and we use tensor algebra methods to enable efficient computation and optimization. We show the significant advantages of KBASS on a list of benchmark ODE and PDE discovery tasks.
</details>
<details>
<summary>摘要</summary>
发现管理方程式从数据中是科学和工程应用中非常重要的。虽然现有方法已经取得了很大的成功，但是它们仍然面临着数据稀缺和噪声问题，这两个问题在实践中却非常普遍。此外，现有的方法缺乏uncertainty量化和/或训练成本高。为了解决这些限制，我们提出了一种基于kernel学习和权重积分干扰（KBASS）的方程发现方法。我们使用kernel回归来估计目标函数，这种方法非常灵活、表达力强和更加抗抗数据稀缺和噪声。我们将其与一种bayesian积分干扰（BAYESIAN SPIKE-AND-SLAB）的干扰分布结合起来，以实现有效的运算选择和uncertainty量化。我们开发了一种期望传播期望最大化（EP-EM）算法，以便高效地进行 posterior推理和函数估计。为了解决kernel回归的计算挑战，我们将函数值放在一个网格上，并使用kronecker产品结构，以及tensor代数方法来实现高效的计算和优化。我们在一系列的benchmark ODE和PDE发现任务上展示了KBASS的显著优势。
</details></li>
</ul>
<hr>
<h2 id="Augmented-Embeddings-for-Custom-Retrievals"><a href="#Augmented-Embeddings-for-Custom-Retrievals" class="headerlink" title="Augmented Embeddings for Custom Retrievals"></a>Augmented Embeddings for Custom Retrievals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05380">http://arxiv.org/abs/2310.05380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Khatry, Yasharth Bajpai, Priyanshu Gupta, Sumit Gulwani, Ashish Tiwari</li>
<li>For: 这个论文主要针对的是如何使用 dense retrieval 技术来提高异类、严格的检索效果，以满足现代大语言模型（LLM）的推荐任务。* Methods: 该论文提出了一种名为 Adapted Dense Retrieval 的机制，它可以将预训练的卷积扩展学习到特定任务中，以提高异类、严格的检索效果。* Results: 论文通过实验证明，Adapted Dense Retrieval  Mechanism可以与现有的基于预训练矩阵的基线方法相比，在异类、严格的检索任务中提高检索效果。<details>
<summary>Abstract</summary>
Information retrieval involves selecting artifacts from a corpus that are most relevant to a given search query. The flavor of retrieval typically used in classical applications can be termed as homogeneous and relaxed, where queries and corpus elements are both natural language (NL) utterances (homogeneous) and the goal is to pick most relevant elements from the corpus in the Top-K, where K is large, such as 10, 25, 50 or even 100 (relaxed). Recently, retrieval is being used extensively in preparing prompts for large language models (LLMs) to enable LLMs to perform targeted tasks. These new applications of retrieval are often heterogeneous and strict -- the queries and the corpus contain different kinds of entities, such as NL and code, and there is a need for improving retrieval at Top-K for small values of K, such as K=1 or 3 or 5. Current dense retrieval techniques based on pretrained embeddings provide a general-purpose and powerful approach for retrieval, but they are oblivious to task-specific notions of similarity of heterogeneous artifacts. We introduce Adapted Dense Retrieval, a mechanism to transform embeddings to enable improved task-specific, heterogeneous and strict retrieval. Adapted Dense Retrieval works by learning a low-rank residual adaptation of the pretrained black-box embedding. We empirically validate our approach by showing improvements over the state-of-the-art general-purpose embeddings-based baseline.
</details>
<details>
<summary>摘要</summary>
信息检索通常包括从质量很高的文档库中选择最相关的元素，以满足给定的搜索查询。经典应用中的检索通常采用同质和松散的方式，其中查询和文档元素都是自然语言（NL）句子（同质），并且目标是从文档库中选择最相关的元素，其中K是大的，例如10、25、50或甚至100（松散）。在最近几年，检索已经在准备提示 для大型自然语言模型（LLM）中得到广泛的应用。这些新的应用程序通常是不同类型的Entity的混合和严格的，查询和文档元素不同，需要改进Top-K中的检索。当前的某些检索技术基于预训练的嵌入可以提供一种通用和强大的方法，但它们对特定任务的相似性无法考虑不同类型的文件。我们介绍了适应的检索，一种将嵌入转换成以便改进特定任务、不同类型的文件和严格的检索。适应的检索通过学习一个低级别的剩余适应来实现。我们通过对比与现有的通用嵌入基eline来验证我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Studies-for-Efficient-Parameter-Search-and-Parallelism-for-Large-Language-Model-Pre-training"><a href="#Scaling-Studies-for-Efficient-Parameter-Search-and-Parallelism-for-Large-Language-Model-Pre-training" class="headerlink" title="Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training"></a>Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05350">http://arxiv.org/abs/2310.05350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Benington, Leo Phan, Chris Pierre Paul, Evan Shoemaker, Priyanka Ranade, Torstein Collett, Grant Hodgson Perez, Christopher Krieger</li>
<li>for: 这个论文主要针对AI加速器处理能力和内存限制的问题，旨在探讨如何在可接受时间内执行机器学习任务（如训练和推理）。</li>
<li>methods: 这篇论文使用了分布式算法和电路优化技术来进行多节点环境中的模型扩展，提高模型训练和预处理的效率，并尝试将更多参数存储在有限的资源中。</li>
<li>results: 研究项目中对5个encoder-decoder LLMS进行了并行和分布式机器学习算法开发，并进行了细化的研究以量化三种ML并行方法（包括Microsoft DeepSpeed Zero Redundancy Optimizer（ZeRO）阶段）的关系。<details>
<summary>Abstract</summary>
AI accelerator processing capabilities and memory constraints largely dictate the scale in which machine learning workloads (e.g., training and inference) can be executed within a desirable time frame. Training a state of the art, transformer-based model today requires use of GPU-accelerated high performance computers with high-speed interconnects. As datasets and models continue to increase in size, computational requirements and memory demands for AI also continue to grow. These challenges have inspired the development of distributed algorithm and circuit-based optimization techniques that enable the ability to progressively scale models in multi-node environments, efficiently minimize neural network cost functions for faster convergence, and store more parameters into a set number of available resources. In our research project, we focus on parallel and distributed machine learning algorithm development, specifically for optimizing the data processing and pre-training of a set of 5 encoder-decoder LLMs, ranging from 580 million parameters to 13 billion parameters. We performed a fine-grained study to quantify the relationships between three ML parallelism methods, specifically exploring Microsoft DeepSpeed Zero Redundancy Optimizer (ZeRO) stages.
</details>
<details>
<summary>摘要</summary>
人工智能加速器处理能力和内存限制 largely dictate 机器学习任务（例如训练和推理）可以在理想时间内执行的规模。今天，使用 GPU 加速的高性能计算机和高速 интер连接来训练现代变换器基于模型。随着数据集和模型的大小不断增长，人工智能的计算要求和内存需求也在不断增长。这些挑战激发了分布式算法和绕组件优化技术的开发，以实现在多节点环境中逐渐扩大模型，高效地减少神经网络成本函数，并将更多参数存储在可用资源中。在我们的研究项目中，我们专注于并行和分布式机器学习算法开发，具体来说是优化数据处理和前期训练5个Encoder-Decoder LLMS的集合，该集合包括580亿参数到1300亿参数。我们进行了细化的研究，以量化三种机器学习并行方法之间的关系，具体来说是Microsoft DeepSpeed Zero Redundancy Optimizer（ZeRO）阶段。
</details></li>
</ul>
<hr>
<h2 id="DiffCPS-Diffusion-Model-based-Constrained-Policy-Search-for-Offline-Reinforcement-Learning"><a href="#DiffCPS-Diffusion-Model-based-Constrained-Policy-Search-for-Offline-Reinforcement-Learning" class="headerlink" title="DiffCPS: Diffusion Model based Constrained Policy Search for Offline Reinforcement Learning"></a>DiffCPS: Diffusion Model based Constrained Policy Search for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05333">http://arxiv.org/abs/2310.05333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felix-thu/DiffCPS">https://github.com/felix-thu/DiffCPS</a></li>
<li>paper_authors: Longxiang He, Linrui Zhang, Junbo Tan, Xueqian Wang</li>
<li>For: 解决 offline 强化学习中的受限策略搜索问题，提出一种基于Diffusion模型的受限策略搜索方法（DiffCPS），以高度表达能力替代先前的AWR方法。* Methods: 利用Diffusion模型的动作分布来消除受限策略搜索中的策略分布约束，然后使用Diffusion模型中的证据下界（ELBO）来近似KL约束。* Results: 在D4RL数据集上进行了广泛的实验，证明DiffCPS可以 дости得更好或至少相当于传统AWR基eline以及近期的Diffusion模型基eline。代码可以在 $\href{<a target="_blank" rel="noopener" href="https://github.com/felix-thu/DiffCPS%7D%7Bhttps://github.com/felix-thu/DiffCPS%7D$">https://github.com/felix-thu/DiffCPS}{https://github.com/felix-thu/DiffCPS}$</a> 上获取。<details>
<summary>Abstract</summary>
Constrained policy search (CPS) is a fundamental problem in offline reinforcement learning, which is generally solved by advantage weighted regression (AWR). However, previous methods may still encounter out-of-distribution actions due to the limited expressivity of Gaussian-based policies. On the other hand, directly applying the state-of-the-art models with distribution expression capabilities (i.e., diffusion models) in the AWR framework is insufficient since AWR requires exact policy probability densities, which is intractable in diffusion models. In this paper, we propose a novel approach called $\textbf{Diffusion Model based Constrained Policy Search (DiffCPS)}$, which tackles the diffusion-based constrained policy search without resorting to AWR. The theoretical analysis reveals our key insights by leveraging the action distribution of the diffusion model to eliminate the policy distribution constraint in the CPS and then utilizing the Evidence Lower Bound (ELBO) of diffusion-based policy to approximate the KL constraint. Consequently, DiffCPS admits the high expressivity of diffusion models while circumventing the cumbersome density calculation brought by AWR. Extensive experimental results based on the D4RL benchmark demonstrate the efficacy of our approach. We empirically show that DiffCPS achieves better or at least competitive performance compared to traditional AWR-based baselines as well as recent diffusion-based offline RL methods. The code is now available at $\href{https://github.com/felix-thu/DiffCPS}{https://github.com/felix-thu/DiffCPS}$.
</details>
<details>
<summary>摘要</summary>
“干预策搜索”（Constrained Policy Search，简称CPS）是机器学习中的基本问题，通常通过优先预测（Advantage Weighted Regression，简称AWR）解决。然而，先前的方法可能仍会遇到对不同的动作的不合理的行为，因为运用 Gaussian-based 政策的有限表达能力。另一方面，直接将现场的先进模型（i.e., 传播模型）应用在 AWR 框架中是不足的，因为 AWR 需要精确的政策概率密度，传播模型中的概率密度是无法求解的。在本文中，我们提出了一个新的方法，called “传播模型基于的干预策搜索”（Diffusion Model based Constrained Policy Search，简称DiffCPS）。我们的研究表明，DiffCPS 可以在干预策搜索中消除政策概率密度的限制，并且使用传播模型中的动作分布来估计 KL 函数。因此，DiffCPS 可以充分利用传播模型的表达能力，而不需要耗费时间 Calculate 政策概率密度。我们的实验结果显示，DiffCPS 可以对 D4RL benchmark 进行了广泛的测试，并且与传统 AWR 基础的基elines 和最近的传播模型基础的 offline RL 方法相比，获得了更好的性能。我们的代码现在可以在 $\href{https://github.com/felix-thu/DiffCPS}{https://github.com/felix-thu/DiffCPS}$ 上获取。”
</details></li>
</ul>
<hr>
<h2 id="Unlearning-with-Fisher-Masking"><a href="#Unlearning-with-Fisher-Masking" class="headerlink" title="Unlearning with Fisher Masking"></a>Unlearning with Fisher Masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05331">http://arxiv.org/abs/2310.05331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shivank21/Unlearning-with-Fisher-Masking">https://github.com/shivank21/Unlearning-with-Fisher-Masking</a></li>
<li>paper_authors: Yufang Liu, Changzhi Sun, Yuanbin Wu, Aimin Zhou</li>
<li>for:  Machine unlearning aims to revoke some training data after learning in response to requests from users, model developers, and administrators.</li>
<li>methods:  The proposed method uses a new masking strategy tailored to unlearning based on Fisher information.</li>
<li>results:  The proposed method can unlearn almost completely while maintaining most of the performance on the remain data, and exhibits stronger stability compared to other unlearning baselines.Here’s the full text in Simplified Chinese:</li>
<li>for: 机器学习推理批处理强制请求下的数据恢复</li>
<li>methods: 基于Fisher信息的新遮盖策略</li>
<li>results: 可以减少大量数据，保持大多数数据的表现，并且比其他基线方法更稳定<details>
<summary>Abstract</summary>
Machine unlearning aims to revoke some training data after learning in response to requests from users, model developers, and administrators. Most previous methods are based on direct fine-tuning, which may neither remove data completely nor retain full performances on the remain data. In this work, we find that, by first masking some important parameters before fine-tuning, the performances of unlearning could be significantly improved. We propose a new masking strategy tailored to unlearning based on Fisher information. Experiments on various datasets and network structures show the effectiveness of the method: without any fine-tuning, the proposed Fisher masking could unlearn almost completely while maintaining most of the performance on the remain data. It also exhibits stronger stability compared to other unlearning baselines
</details>
<details>
<summary>摘要</summary>
机器学习卷回目标是在学习后根据用户、模型开发者和管理员的请求，撤销一部分训练数据。现有的大多数方法都基于直接细化，这可能并不会完全 removes 数据，也不会保留剩下数据的全部性能。在这种工作中，我们发现，先对一些重要参数进行遮盖，然后进行细化，可以有效提高卷回的性能。我们提出了针对卷回的新的遮盖策略，基于信息理解。对各种数据集和网络结构进行实验，我们发现，无需任何细化，我们的提议的遮盖策略可以几乎完全卷回数据，同时保留大部分剩下数据的性能。它还比其他卷回基线强制稳定。
</details></li>
</ul>
<hr>
<h2 id="Provable-Compositional-Generalization-for-Object-Centric-Learning"><a href="#Provable-Compositional-Generalization-for-Object-Centric-Learning" class="headerlink" title="Provable Compositional Generalization for Object-Centric Learning"></a>Provable Compositional Generalization for Object-Centric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05327">http://arxiv.org/abs/2310.05327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thaddäus Wiedemer, Jack Brady, Alexander Panfilov, Attila Juhos, Matthias Bethge, Wieland Brendel</li>
<li>for:  bridging the gap between human and machine perception</li>
<li>methods:  learning object-centric representations, using autoencoders with structural assumptions and enforcing encoder-decoder consistency</li>
<li>results:  provable compositional generalization of object-centric representations through identifiability theory, validated through experiments on synthetic image data.Here’s the full text in Simplified Chinese:</li>
<li>for:  bridging the gap between人类和机器视觉</li>
<li>methods: 通过学习对象中心表示，使用具有结构假设的自动编码器和强制编码器-解码器一致性，实现可靠的 композиitional generalization</li>
<li>results: 通过identifiability理论，证明对象中心表示可以可靠地推广到新的组合结构，并通过synthetic图像数据实验证明了这一结论。<details>
<summary>Abstract</summary>
Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical or empirical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data.
</details>
<details>
<summary>摘要</summary>
学习概念的总结，使机器和人类视觉之间的差异越来越小，是核心的问题。一种广泛的尝试是学习对象中心的表示，这些表示被推测可以实现 композиitional generalization。然而，是否这种推测是正确的，还没有一个明确的理论或实际理解。在这项工作中，我们通过标识理论来研究对象中心表示是否可以 garantuee compositional generalization。我们证明了满足核心假设的 autoencoder 将学习对象中心表示，并且可以确定性地推导 compositional generalization。我们验证了我们的理论结论，并通过实验 validate 我们的假设在生成的图像数据上。
</details></li>
</ul>
<hr>
<h2 id="Increasing-Entropy-to-Boost-Policy-Gradient-Performance-on-Personalization-Tasks"><a href="#Increasing-Entropy-to-Boost-Policy-Gradient-Performance-on-Personalization-Tasks" class="headerlink" title="Increasing Entropy to Boost Policy Gradient Performance on Personalization Tasks"></a>Increasing Entropy to Boost Policy Gradient Performance on Personalization Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05324">http://arxiv.org/abs/2310.05324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/acstarnes/wain23-policy-regularization">https://github.com/acstarnes/wain23-policy-regularization</a></li>
<li>paper_authors: Andrew Starnes, Anton Dereventsov, Clayton Webster</li>
<li>For: 本研究考虑了使用强化学习agent中的政策梯度优化的迁移抑制效果。* Methods: 本文使用了不同的$\varphi$-差分和最大均值差Distance来增强Policy的优化目标函数，以促进Policy的多样性。* Results: 数值实验表明，通过使用多样性促进策略 régularization，可以提高各种个性化任务的性能，而且不会 sacrificing accuracy。<details>
<summary>Abstract</summary>
In this effort, we consider the impact of regularization on the diversity of actions taken by policies generated from reinforcement learning agents trained using a policy gradient. Policy gradient agents are prone to entropy collapse, which means certain actions are seldomly, if ever, selected. We augment the optimization objective function for the policy with terms constructed from various $\varphi$-divergences and Maximum Mean Discrepancy which encourages current policies to follow different state visitation and/or action choice distribution than previously computed policies. We provide numerical experiments using MNIST, CIFAR10, and Spotify datasets. The results demonstrate the advantage of diversity-promoting policy regularization and that its use on gradient-based approaches have significantly improved performance on a variety of personalization tasks. Furthermore, numerical evidence is given to show that policy regularization increases performance without losing accuracy.
</details>
<details>
<summary>摘要</summary>
“在这个努力中，我们考虑了规则化对Policy生成的多样性的影响。Policy梯度学习代理人容易出现Entropy塌塌，这意味着某些动作很少或者从未被选择。我们将优化优化目标函数中的Policy加入了不同的状态访问和/或动作选择分布的梯度的不同$\varphi$-多样性和Maximum Mean Discrepancy。我们在MNIST、CIFAR10和Spotify数据集上进行了数值实验，结果表明了促进多样性的策略REG regularization的优势，并且在各种个性化任务中显著提高了性能。此外，我们还提供了数值证明，证明政策REG regularization可以不失准确性地提高表现。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/cs.LG_2023_10_09/" data-id="clombedvg00pws088927x2zyf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/eess.IV_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T09:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/eess.IV_2023_10_09/">eess.IV - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Empirical-Evaluation-of-the-Segment-Anything-Model-SAM-for-Brain-Tumor-Segmentation"><a href="#Empirical-Evaluation-of-the-Segment-Anything-Model-SAM-for-Brain-Tumor-Segmentation" class="headerlink" title="Empirical Evaluation of the Segment Anything Model (SAM) for Brain Tumor Segmentation"></a>Empirical Evaluation of the Segment Anything Model (SAM) for Brain Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06162">http://arxiv.org/abs/2310.06162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Peivandi, Jason Zhang, Michael Lu, Dongxiao Zhu, Zhifeng Kou</li>
<li>for: 本研究旨在提高基于Segment Anything Model（SAM）的脑肿划分精度。</li>
<li>methods: 本研究使用了传输学习和Decathlon脑肿数据集来强化SAM的面掩码解码器。其中，对四维数据进行了三维封装，并使用了随机旋转和弹性变形来增加训练数据的大小。</li>
<li>results: 对比预训练的SAM和nnUNetv2，改进后的SAM在脑肿划分精度方面显示了显著提高，而nnUNetv2在总划分精度方面赢得了比较高的分数。然而，改进后的SAM在挑战性较高的案例中表现更为稳定，尤其是在 Hausdorff 距离95%的情况下。<details>
<summary>Abstract</summary>
Brain tumor segmentation presents a formidable challenge in the field of Medical Image Segmentation. While deep-learning models have been useful, human expert segmentation remains the most accurate method. The recently released Segment Anything Model (SAM) has opened up the opportunity to apply foundation models to this difficult task. However, SAM was primarily trained on diverse natural images. This makes applying SAM to biomedical segmentation, such as brain tumors with less defined boundaries, challenging. In this paper, we enhanced SAM's mask decoder using transfer learning with the Decathlon brain tumor dataset. We developed three methods to encapsulate the four-dimensional data into three dimensions for SAM. An on-the-fly data augmentation approach has been used with a combination of rotations and elastic deformations to increase the size of the training dataset. Two key metrics: the Dice Similarity Coefficient (DSC) and the Hausdorff Distance 95th Percentile (HD95), have been applied to assess the performance of our segmentation models. These metrics provided valuable insights into the quality of the segmentation results. In our evaluation, we compared this improved model to two benchmarks: the pretrained SAM and the widely used model, nnUNetv2. We find that the improved SAM shows considerable improvement over the pretrained SAM, while nnUNetv2 outperformed the improved SAM in terms of overall segmentation accuracy. Nevertheless, the improved SAM demonstrated slightly more consistent results than nnUNetv2, especially on challenging cases that can lead to larger Hausdorff distances. In the future, more advanced techniques can be applied in order to further improve the performance of SAM on brain tumor segmentation.
</details>
<details>
<summary>摘要</summary>
脑肿分割是医学图像分割领域中的一大挑战。深度学习模型已经在此领域中发挥了作用，但是人工专家分割仍然是最准确的方法。最近发布的Segment Anything Model（SAM）已经开创了应用基础模型在这个难题上的可能性。然而，SAM主要在多样的自然图像上进行训练，这使得将SAM应用于生物医学分割，如脑肿诊断，变得更加困难。在这篇论文中，我们提高了SAM的面 máscara解码器使用基于Transfer Learning的Decathlon脑肿数据集。我们开发出了三种方法来封装四维数据到三维数据中，以便在SAM上进行分割。我们采用了在线数据增强策略，结合旋转和弹性变形来增加训练集的大小。我们使用了Dice相似度系数（DSC）和 Hausdorff距离95%（HD95）两个关键指标来评估我们的分割模型的性能。这两个指标为我们提供了有价值的分割结果评估方法。在我们的评估中，我们比较了我们改进的SAM模型与预训练的SAM模型以及广泛使用的nnUNetv2模型。我们发现，改进后的SAM模型在脑肿分割任务中显著提高了性能，而nnUNetv2模型在整体分割精度方面超过了改进后的SAM模型。然而，改进后的SAM模型在挑战性较高的案例中表现更为一致，尤其是在可能导致更大的 Hausdorff 距离的情况下。未来，我们可以采用更高级的技术来进一步提高SAM模型在脑肿分割任务中的性能。
</details></li>
</ul>
<hr>
<h2 id="Dipole-Spread-Function-Engineering-for-6D-Super-Resolution-Microscopy"><a href="#Dipole-Spread-Function-Engineering-for-6D-Super-Resolution-Microscopy" class="headerlink" title="Dipole-Spread Function Engineering for 6D Super-Resolution Microscopy"></a>Dipole-Spread Function Engineering for 6D Super-Resolution Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05810">http://arxiv.org/abs/2310.05810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingting Wu, Matthew D. Lew</li>
<li>for: 这个论文的目的是探讨fluorescent molecules的六个维度超分辨单分子orientation-localization微scopic镜像技术（SMOLM）。</li>
<li>methods: 这篇论文详细介绍了fluorescent диполи的形成图像理论，以及如何通过相位和极化调制来改变镜像形成的dipole spread function（DSF）。它还描述了一些设计这些调制的方法，以及最新的技术，包括双螺旋、四肢、圆形和DeepSTORM3D学习点精度函数（PSF）。</li>
<li>results: 论文还详细介绍了一些实际应用，包括生物学应用，以及未来技术的发展和挑战。<details>
<summary>Abstract</summary>
Fluorescent molecules are versatile nanoscale emitters that enable detailed observations of biophysical processes with nanoscale resolution. Because they are well-approximated as electric dipoles, imaging systems can be designed to visualize their 3D positions and 3D orientations, so-called dipole-spread function (DSF) engineering, for 6D super-resolution single-molecule orientation-localization microscopy (SMOLM). We review fundamental image-formation theory for fluorescent di-poles, as well as how phase and polarization modulation can be used to change the image of a dipole emitter produced by a microscope, called its DSF. We describe several methods for designing these modulations for optimum performance, as well as compare recently developed techniques, including the double-helix, tetrapod, crescent, and DeepSTORM3D learned point-spread functions (PSFs), in addition to the tri-spot, vortex, pixOL, raPol, CHIDO, and MVR DSFs. We also cover common imaging system designs and techniques for implementing engineered DSFs. Finally, we discuss recent biological applications of 6D SMOLM and future challenges for pushing the capabilities and utility of the technology.
</details>
<details>
<summary>摘要</summary>
fluorescent分子是一种 versatile nanoscale发射器，可以允许详细地观察生物物理过程，resolution nanoscale.因为它们可以被视为电动 polarization dipole， therefore imaging system can be designed to visualize their 3D positions and 3D orientations, so-called dipole-spread function (DSF) engineering, for 6D super-resolution single-molecule orientation-localization microscopy (SMOLM).我们将评论基本的图像形成理论 для fluorescent di-poles，以及如何使用阶段和 polarization 模ulation change the image of a dipole emitter produced by a microscope, called its DSF。我们将描述一些设计这些模ulation的方法，以及最近开发的技术，包括double-helix, tetrapod, crescent, and DeepSTORM3D learned point-spread functions (PSFs), in addition to the tri-spot, vortex, pixOL, raPol, CHIDO, and MVR DSFs。我们还将讨论一些通用的 imaging system designs and techniques for implementing engineered DSFs。最后，我们将讨论最近的生物应用和未来挑战，以推动技术的能力和实用性。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Predictive-Coding-of-Intra-Prediction-Modes"><a href="#Efficient-Predictive-Coding-of-Intra-Prediction-Modes" class="headerlink" title="Efficient Predictive Coding of Intra Prediction Modes"></a>Efficient Predictive Coding of Intra Prediction Modes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05623">http://arxiv.org/abs/2310.05623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Reuzé, Wassim Hamidouche, Pierrick Philippe, Olivier Déforges</li>
<li>for: 提高HEVC标准和JEM编码器的压缩效率，特别是在Intra块的压缩中。</li>
<li>methods: 提出了一种基于Contextual information的专门编码方案，包括预测、分 grouped 和编码三个步骤，每个步骤都通过引入新元素（标签、测试和编码）进行了改进。使用遗传算法来最优化编码方案，以实现最高的编码效率。</li>
<li>results: 在HEVC标准下，我们的方法可以实现显著的比特率减少，同时保持JEM编码器的编码效率，这些结果表明了我们的方法在压缩效率方面的潜在提升。<details>
<summary>Abstract</summary>
The high efficiency video coding (HEVC) standard and the joint exploration model (JEM) codec incorporate 35 and 67 intra prediction modes (IPMs) respectively, which are essential for efficient compression of Intra coded blocks. These IPMs are transmitted to the decoder through a coding scheme. In our paper, we present an innovative approach to construct a dedicated coding scheme for IPM based on contextual information. This approach comprises three key steps: prediction, clustering, and coding, each of which has been enhanced by introducing new elements, namely, labels for prediction, tests for clustering, and codes for coding. In this context, we have proposed a method that utilizes a genetic algorithm to minimize the rate cost, aiming to derive the most efficient coding scheme while leveraging the available labels, tests, and codes. The resulting coding scheme, expressed as a binary tree, achieves the highest coding efficiency for a given level of complexity. In our experimental evaluation under the HEVC standard, we observed significant bitrate gains while maintaining coding efficiency under the JEM codec. These results demonstrate the potential of our approach to improve compression efficiency, particularly under the HEVC standard, while preserving the coding efficiency of the JEM codec.
</details>
<details>
<summary>摘要</summary>
高效视频编码（HEVC）标准和联合探索模型（JEM）编码器共有35和67内部预测模式（IPM），这些IPM是为高效压缩内部块的必需组成部分。这些IPM通过编码方案传输到解码器。在我们的论文中，我们提出了一种创新的方法，基于上下文信息来构建专门的编码方案。这种方法包括三个关键步骤：预测、聚类和编码，每一步都通过引入新的元素来增强，例如标签 для预测、测试 для聚类和编码。在这个上下文中，我们提出了一种使用遗传算法来最小化比特成本，以 derivate最高效的编码方案，同时利用可用的标签、测试和编码。结果表明，该编码方案，表示为二进制树，在给定的复杂度下实现了最高的编码效率。在我们的实验中，使用HEVC标准，我们观察到了显著的比特率减少，同时保持JEM编码器的编码效率。这些结果表明了我们的方法的潜在提高压缩效率，特别是在HEVC标准下，而且不会削弱JEM编码器的编码效率。
</details></li>
</ul>
<hr>
<h2 id="Longitudinal-Volumetric-Study-for-the-Progression-of-Alzheimer’s-Disease-from-Structural-MR-Images"><a href="#Longitudinal-Volumetric-Study-for-the-Progression-of-Alzheimer’s-Disease-from-Structural-MR-Images" class="headerlink" title="Longitudinal Volumetric Study for the Progression of Alzheimer’s Disease from Structural MR Images"></a>Longitudinal Volumetric Study for the Progression of Alzheimer’s Disease from Structural MR Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05558">http://arxiv.org/abs/2310.05558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prayas Sanyal, Srinjay Mukherjee, Arkapravo Das, Anindya Sen</li>
<li>for: This paper aims to survey imaging biomarkers corresponding to the progression of Alzheimer’s Disease (AD).</li>
<li>methods: The pipeline implemented includes modern pre-processing techniques such as spatial image registration, skull stripping, and inhomogeneity correction. The segmentation of tissue classes is done using an unsupervised learning approach based on intensity histogram information.</li>
<li>results: The study found that the structural change in the form of volumes of cerebrospinal fluid (CSF), grey matter (GM), and white matter (WM) can be used to track the progression of Alzheimer’s Disease (AD). The segmented features provide insights such as atrophy, increase or intolerable shifting of GM, WM and CSF, which can help in future research for automated analysis of Alzheimer’s detection with clinical domain explainability.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是探讨阿尔茨heimer病（AD）的进行诊断标志。</li>
<li>methods: 该 pipeline 使用了现代预处理技术，包括空间尺寸调整、脑骨除除和不均衡纠正。 segmentation 使用了无监督学习方法，基于Intensity histogram信息。</li>
<li>results: 研究发现，CSF、GM和WM的体积变化可以跟踪阿尔茨heimer病（AD）的进行。 segmented 特征提供了衰竭、增加或不具适应的GM、WM和CSF的信息，可以帮助未来研究自动化阿尔茨heimer 检测，并提供临床领域可解释的解释。<details>
<summary>Abstract</summary>
Alzheimer's Disease (AD) is primarily an irreversible neurodegenerative disorder affecting millions of individuals today. The prognosis of the disease solely depends on treating symptoms as they arise and proper caregiving, as there are no current medical preventative treatments. For this purpose, early detection of the disease at its most premature state is of paramount importance. This work aims to survey imaging biomarkers corresponding to the progression of Alzheimer's Disease (AD). A longitudinal study of structural MR images was performed for given temporal test subjects selected randomly from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The pipeline implemented includes modern pre-processing techniques such as spatial image registration, skull stripping, and inhomogeneity correction. The temporal data across multiple visits spanning several years helped identify the structural change in the form of volumes of cerebrospinal fluid (CSF), grey matter (GM), and white matter (WM) as the patients progressed further into the disease. Tissue classes are segmented using an unsupervised learning approach using intensity histogram information. The segmented features thus extracted provide insights such as atrophy, increase or intolerable shifting of GM, WM and CSF and should help in future research for automated analysis of Alzheimer's detection with clinical domain explainability.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病 (AD) 是一种主要是不可逆的脑组织衰退病种，影响了数百万人今天。这种病的诊断和治疗几乎完全依赖于病人的症状和照顾，没有现有的医学预防性治疗。因此，早期发现病种的症状非常重要。本研究的目的是对阿尔茨海默病的发展进行快照。使用ADNI数据库中随机选择的测试对象，我们实施了一种 longitudinal 的 MR 成像数据集，并应用现代的预处理技术，包括空间尺寸对齐、脑骨剥除和不均匀性 corrections。通过多个访问的时间跨度，我们发现了病人的结构变化，包括脑液（CSF）、灰 mater（GM）和白 matter（WM）的体积。使用无监督学习方法，我们对尺寸信息进行分类，并提取了相应的特征，如衰退、灰 mater 和 WM 的增加或不可接受的移动。这些特征提供了关于阿尔茨海默病的早期诊断和自动分析的Future研究中的解释。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/eess.IV_2023_10_09/" data-id="clombee1b0168s088586jdvte" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/eess.SP_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T08:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/eess.SP_2023_10_09/">eess.SP - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Extended-Reality-via-Cooperative-NOMA-in-Hybrid-Cloud-Mobile-Edge-Computing-Networks"><a href="#Extended-Reality-via-Cooperative-NOMA-in-Hybrid-Cloud-Mobile-Edge-Computing-Networks" class="headerlink" title="Extended Reality via Cooperative NOMA in Hybrid Cloud&#x2F;Mobile-Edge Computing Networks"></a>Extended Reality via Cooperative NOMA in Hybrid Cloud&#x2F;Mobile-Edge Computing Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06874">http://arxiv.org/abs/2310.06874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert-Jeron Reifert, Hayssam Dahrouj, Aydin Sezgin</li>
<li>for: 这篇论文旨在解决未来的扩展现实（XR）应用程序中的资源消耗性任务问题，通过融合中央云（CC）、边缘计算（EC）和无人机（UAV）的能力，以提高XR应用程序的质量体验。</li>
<li>methods: 该论文提出了一种基于协同非对称多接入（Co-NOMA）的无人机协助混合云&#x2F;移动边计算架构，以提高XR设备的质量体验。它还提出了一个权衡系统吞吐率和公平性的最大化问题，以确定计算和通信资源的分配和链接选择策略。</li>
<li>results: 该论文通过实验结果表明，提出的算法可以最大化系统吞吐率，同时保证系统公平性，并且在实际网络约束下（如能源消耗和延迟）下实现分布式实现。<details>
<summary>Abstract</summary>
Extended reality (XR) applications often perform resource-intensive tasks, which are computed remotely, a process that prioritizes the latency criticality aspect. To this end, this paper shows that through leveraging the power of the central cloud (CC), the close proximity of edge computers (ECs), and the flexibility of uncrewed aerial vehicles (UAVs), a UAV-aided hybrid cloud/mobile-edge computing architecture promises to handle the intricate requirements of future XR applications. In this context, this paper distinguishes between two types of XR devices, namely, strong and weak devices. The paper then introduces a cooperative non-orthogonal multiple access (Co-NOMA) scheme, pairing strong and weak devices, so as to aid the XR devices quality-of-user experience by intelligently selecting either the direct or the relay links toward the weak XR devices. A sum logarithmic-rate maximization problem is, thus, formulated so as to jointly determine the computation and communication resources, and link-selection strategy as a means to strike a trade-off between the system throughput and fairness. Subject to realistic network constraints, e.g., power consumption and delay, the optimization problem is then solved iteratively via discrete relaxations, successive-convex approximation, and fractional programming, an approach which can be implemented in a distributed fashion across the network. Simulation results validate the proposed algorithms performance in terms of log-rate maximization, delay-sensitivity, scalability, and runtime performance. The practical distributed Co-NOMA implementation is particularly shown to offer appreciable benefits over traditional multiple access and NOMA methods, highlighting its applicability in decentralized XR systems.
</details>
<details>
<summary>摘要</summary>
现实扩展（XR）应用程序通常执行资源密集的任务，这些任务通常在远程计算，以优先级顺序处理。为了实现这一目标，这篇论文提出了一种通过中央云（CC）、边缘计算（EC）和无人机（UAV）的 гибрид云/边缘计算架构来处理未来XR应用程序的复杂需求。在这个上下文中，这篇论文将XR设备分为两类：强设备和弱设备。论文然后引入了合作非对称多访问（Co-NOMA）方案，将强设备和弱设备相互协作，以提高XR设备用户体验质量。为了提高系统吞吐量和公平性，论文提出了一个总日志arithmic-rate最大化问题，以联合确定计算和通信资源，以及链接选择策略。充分考虑了现实网络约束，例如电力消耗和延迟，优化问题可以通过抽象relaxation、Successive-Convex Approximation和分数程序来解决。实际应用中，这种分布式Co-NOMA实现可以提供较高的日志率最大化、延迟敏感度、可扩展性和运行时性能。
</details></li>
</ul>
<hr>
<h2 id="Decomposition-Based-Interference-Management-Framework-for-Local-6G-Networks"><a href="#Decomposition-Based-Interference-Management-Framework-for-Local-6G-Networks" class="headerlink" title="Decomposition Based Interference Management Framework for Local 6G Networks"></a>Decomposition Based Interference Management Framework for Local 6G Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05809">http://arxiv.org/abs/2310.05809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samitha Gunarathne, Thushan Sivalingam, Nurul Huda Mahmood, Nandana Rajatheva, Matti Latva-Aho</li>
<li>for: 本研究旨在提出一种智能干扰管理框架，用于 garantía de calidad de servicio （QoS）的 ultra-reliable low latency communications （URLLC）应用。</li>
<li>methods: 提议的算法包括了先进的信号预处理技术——empirical mode decomposition（EMD），然后使用序列-到-一个变换器算法进行预测每个分解成分的干扰电平。预测后，使用预测结果来估算未来信号干扰比例，并将资源分配以 garantía高可靠性。最后，基于预测的干扰信号，进行干扰抑制方案的研究。</li>
<li>results: 对于两种基准算法，提议的序列-到-一个变换器模型显示了其 robustness 性。与基准方案相比，提议方案可以降低平均квадратиче差误差值（RMSE）值，最高降低55%。<details>
<summary>Abstract</summary>
Managing inter-cell interference is among the major challenges in a wireless network, more so when strict quality of service needs to be guaranteed such as in ultra-reliable low latency communications (URLLC) applications. This study introduces a novel intelligent interference management framework for a local 6G network that allocates resources based on interference prediction. The proposed algorithm involves an advanced signal pre-processing technique known as empirical mode decomposition followed by prediction of each decomposed component using the sequence-to-one transformer algorithm. The predicted interference power is then used to estimate future signal-to-interference plus noise ratio, and subsequently allocate resources to guarantee the high reliability required by URLLC applications. Finally, an interference cancellation scheme is explored based on the predicted interference signal with the transformer model. The proposed sequence-to-one transformer model exhibits its robustness for interference prediction. The proposed scheme is numerically evaluated against two baseline algorithms, and is found that the root mean squared error is reduced by up to 55% over a baseline scheme.
</details>
<details>
<summary>摘要</summary>
管理间细胞干扰是无线网络中的一个主要挑战，尤其是在需要保证严格的服务质量，如在超低延迟低功率通信（URLLC）应用中。本研究提出了一种新的智能干扰管理框架，用于本地6G网络资源分配。该算法包括一种高级的信号预处理技术known as empirical mode decomposition，然后使用序列到一转换器算法预测每个分解成分。预测的干扰功率然后用于估算未来信号干扰 plus noise ratio，并在保证URLLC应用所需的高可靠性的情况下分配资源。最后，基于预测的干扰信号，探讨了一种干扰抵消方案，使用转换器模型。提出的序列到一转换器模型在干扰预测中展现了其强健性。与两个基线算法进行比较，研究发现，使用该方案可以将根mean squared error降低到55%以下。
</details></li>
</ul>
<hr>
<h2 id="Computation-Limited-Signals-A-Channel-Capacity-Regime-Constrained-by-Computational-Complexity"><a href="#Computation-Limited-Signals-A-Channel-Capacity-Regime-Constrained-by-Computational-Complexity" class="headerlink" title="Computation-Limited Signals: A Channel Capacity Regime Constrained by Computational Complexity"></a>Computation-Limited Signals: A Channel Capacity Regime Constrained by Computational Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05794">http://arxiv.org/abs/2310.05794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saulo Queiroz, João P. Vilela, Edmundo Monteiro</li>
<li>for: 这篇论文探讨了计算限制（comp-limited）信号，即通信容量 régime中的计算时间复杂度开销是关键约束，而不是功率或带宽。</li>
<li>methods: 作者提出了一种新的数学框架，基于信息理论和计算复杂度的概念，以 relate 容量和时间复杂度。特别是，作者定义了一个名为算法容量的指标，表示在一个符号中模式化的比特数和通信符号转换所需的最低时间复杂度之间的比例。</li>
<li>results: 作者通过设置此指标为函数Channel资源，分类了一个给定的信号设计是comp-limited的。作者还提供了一个使用例子，表明无线OFDM传输器是comp-limited， Unless the lower-bound计算复杂度 of N-point DFT问题为 $\Omega(N)$，这是计算机科学中的一个开放问题。<details>
<summary>Abstract</summary>
In this letter, we introduce the computational-limited (comp-limited) signals, a communication capacity regime in which the signal time computational complexity overhead is the key constraint -- rather than power or bandwidth -- to the overall communication capacity. To relate capacity and time complexity, we propose a novel mathematical framework that builds on concepts of information theory and computational complexity. In particular, the algorithmic capacity stands for the ratio between the upper-bound number of bits modulated in a symbol and the lower-bound time complexity required to turn these bits into a communication symbol. By setting this ratio as function of the channel resources, we classify a given signal design as comp-limited if its algorithmic capacity nullifies as the channel resources grow. As a use-case, we show that an uncoded OFDM transmitter is comp-limited unless the lower-bound computational complexity of the N-point DFT problem verifies as $\Omega(N)$, which remains an open challenge in theoretical computer science.
</details>
<details>
<summary>摘要</summary>
文中，我们介绍了计算限制（comp-limited）信号，它是通信容量 Régime 中的一个条件，其中信号时间计算复杂度成本是主要的限制因素，而不是功率或带宽。为了将容量和时间复杂度相关联，我们提出了一个新的数学框架，基于信息理论和计算复杂度。具体来说，算法容量表示每个符号中模ulated的最高位数与转化这些位数为通信符号所需的最低时间复杂度之比。通过将这个比率设置为通道资源函数，我们可以将一个给定的信号设计分类为comp-limited。作为一个使用情况，我们显示了一个未编码的OFDM发送器是comp-limited， Unless the lower-bound computational complexity of the N-point DFT problem verifies as $\Omega(N)$, which remains an open challenge in theoretical computer science.Note: "计算限制" (comp-limited) is a term used to describe a communication system where the computational complexity of the signal processing is the primary limiting factor, rather than power or bandwidth.
</details></li>
</ul>
<hr>
<h2 id="Physical-Layer-Security-in-a-Private-5G-Network-for-Industrial-and-Mobility-Application"><a href="#Physical-Layer-Security-in-a-Private-5G-Network-for-Industrial-and-Mobility-Application" class="headerlink" title="Physical Layer Security in a Private 5G Network for Industrial and Mobility Application"></a>Physical Layer Security in a Private 5G Network for Industrial and Mobility Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05525">http://arxiv.org/abs/2310.05525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivraj Hanumant Gonde, Christoph Frisch, Svetoslav Duhovnikov, Martin Kubisch, Thomas Meyerhoff, Dominic Schupke</li>
<li>for: This paper is written for organizations that operate Private 5G networks in industrial environments, particularly those that require secure communication between devices.</li>
<li>methods: The paper uses Physical Layer Key Generation (PLKG) to generate a symmetric secret key between two nodes in the presence of a potential passive eavesdropper.</li>
<li>results: The paper demonstrates the establishment of a long-term symmetric key between an aerial vehicle and IT infrastructure in a manufacturing environment, using the radio interface of the Private 5G network.<details>
<summary>Abstract</summary>
Cellular communication technologies such as 5G are deployed on a large scale around the world. Compared to other communication technologies such as WiFi, Bluetooth, or Ultra Wideband, the 5G communication standard describes support for a large variety of use cases, e.g., Internet of Things, vehicular, industrial, and campus-wide communications. An organization can operate a Private 5G network to provide connectivity to devices in their manufacturing environment. Physical Layer Key Generation (PLKG) is a method to generate a symmetric secret on two nodes despite the presence of a potential passive eavesdropper. To the best of our knowledge, this work is one of the first to implement PLKG in a real Private 5G network. Therefore, it highlights the possibility of integrating PLKG in the communication technology highly relevant for industrial applications. This paper exemplifies the establishment of a long-term symmetric key between an aerial vehicle and IT infrastructure both located in a manufacturing environment and communicating via the radio interface of the Private 5G network.
</details>
<details>
<summary>摘要</summary>
fifth-generation 无线通信技术（5G）在全球范围内大规模部署。相比其他通信技术，如 WiFi、蓝牙或超宽带，5G 通信标准支持各种使用场景，如物联网、交通、工业和校园通信。组织可以运行专用5G网络，以提供制造环境中设备的连接性。物理层密钥生成（PLKG）是一种生成两个节点之间的同步密钥，即使存在可能的潜在窃听者。根据我们所知，这是首次在实际专用5G网络中实现PLKG。因此，它高亮了在工业应用中集成PLKG的可能性。这篇论文示例了在制造环境中的空中车和信息基础设施之间通过专用5G网络的广播 интер脑界面建立长期同步密钥。
</details></li>
</ul>
<hr>
<h2 id="MEDUSA-Scalable-Biometric-Sensing-in-the-Wild-through-Distributed-MIMO-Radars"><a href="#MEDUSA-Scalable-Biometric-Sensing-in-the-Wild-through-Distributed-MIMO-Radars" class="headerlink" title="MEDUSA: Scalable Biometric Sensing in the Wild through Distributed MIMO Radars"></a>MEDUSA: Scalable Biometric Sensing in the Wild through Distributed MIMO Radars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05507">http://arxiv.org/abs/2310.05507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilong Li, Ramanujan K Sheshadri, Karthik Sundaresan, Eugene Chai, Suman Banerjee</li>
<li>for: 这个研究旨在开发一个基于激光的生命 Parameter 监测系统，以提供不断的无接触式生命 Parameter 监测和医疗应用。</li>
<li>methods: 这个系统使用了一种新的快速宽频UWB激光系统，具有自适应和可调的子网络。系统利用了分布式MIMO网络的多标的优点，以提供在实际世界中的生命 Parameter 监测。</li>
<li>results: 这个研究获得了20%的平均提升，相比于使用商业激光感知器的现有系统。这证明了MEDUSA的空间多标优点，包括目标和环境动态的监测在 familier和未知内部环境中。<details>
<summary>Abstract</summary>
Radar-based techniques for detecting vital signs have shown promise for continuous contactless vital sign sensing and healthcare applications. However, real-world indoor environments face significant challenges for existing vital sign monitoring systems. These include signal blockage in non-line-of-sight (NLOS) situations, movement of human subjects, and alterations in location and orientation. Additionally, these existing systems failed to address the challenge of tracking multiple targets simultaneously. To overcome these challenges, we present MEDUSA, a novel coherent ultra-wideband (UWB) based distributed multiple-input multiple-output (MIMO) radar system, especially it allows users to customize and disperse the $16 \times 16$ into sub-arrays. MEDUSA takes advantage of the diversity benefits of distributed yet wirelessly synchronized MIMO arrays to enable robust vital sign monitoring in real-world and daily living environments where human targets are moving and surrounded by obstacles. We've developed a scalable, self-supervised contrastive learning model which integrates seamlessly with our hardware platform. Each attention weight within the model corresponds to a specific antenna pair of Tx and Rx. The model proficiently recovers accurate vital sign waveforms by decomposing and correlating the mixed received signals, including comprising human motion, mobility, noise, and vital signs. Through extensive evaluations involving 21 participants and over 200 hours of collected data (3.75 TB in total, with 1.89 TB for static subjects and 1.86 TB for moving subjects), MEDUSA's performance has been validated, showing an average gain of 20% compared to existing systems employing COTS radar sensors. This demonstrates MEDUSA's spatial diversity gain for real-world vital sign monitoring, encompassing target and environmental dynamics in familiar and unfamiliar indoor environments.
</details>
<details>
<summary>摘要</summary>
采用雷达技术探测生命 Parameters 已经展示了不间断无接触的生命参数监测和医疗应用的搭建。然而，现实世界室内环境对现有生命参数监测系统带来了重大挑战。这些挑战包括雷达信号屏蔽（NLOS）情况下的信号干扰、人体活动的移动和位置和方向的变化。此外，现有系统无法同时跟踪多个目标。为了解决这些挑战，我们提出了MEDUSA，一种新的干扰频率ultra-wideband（UWB）基于分布式多输入多输出（MIMO）雷达系统。MEDUSA利用分布式 yet wirelessly synchronized MIMO数组的多样性优势，以实现robust生命参数监测在现实生活环境中， где人类目标在移动并围绕障碍物。我们开发了一种可扩展的自适应强化学习模型，该模型与我们的硬件平台集成了良好。每个注意力量在模型中对应于特定的天线对（Tx和Rx）。模型能够高效地提取生命参数波形，通过分解和相关处理混合接收信号，包括人体运动、 mobilicity、噪声和生命参数。经过了21名参与者和超过200小时的数据收集（总共3.75TB，其中1.89TB为静止目标和1.86TB为移动目标），MEDUSA的性能已经被验证，显示与现有系统使用商业雷达传感器相比，MEDUSA具有20%的平均提升。这表明MEDUSA在实际世界中具有空间多样性增强，包括目标和环境动态在 familiarn和未知室内环境中。
</details></li>
</ul>
<hr>
<h2 id="Affine-Frequency-Division-Multiplexing-With-Index-Modulation"><a href="#Affine-Frequency-Division-Multiplexing-With-Index-Modulation" class="headerlink" title="Affine Frequency Division Multiplexing With Index Modulation"></a>Affine Frequency Division Multiplexing With Index Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05475">http://arxiv.org/abs/2310.05475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwei Tao, Miaowen Wen, Yao Ge, Jun Li</li>
<li>for: 这个论文是为了研究一种基于振荡信号的多Provider frequency division multiplexing（AFDM）系统，并提出一种基于AFDM系统的索引编码（IM）方案。</li>
<li>methods: 该论文使用了AFDM系统的框架，并在DAF频域中使用了活动状态来实现索引编码。具体来说， authors将分割DAF域中的子符号，并考虑了本地化和分布式策略。</li>
<li>results: 该论文通过closed-form的极限紧张upper bound来证明IM方案的性能，并通过计算机实验证明了该方案的优越性。results show that index bits have stronger diversity protection than modulated bits even when the full diversity condition of AFDM is not satisfied.<details>
<summary>Abstract</summary>
Affine frequency division multiplexing (AFDM) is a new multicarrier technique based on chirp signals tailored for high-mobility communications, which can achieve full diversity. In this paper, we propose an index modulation (IM) scheme based on the framework of AFDM systems, named AFDM-IM. In the proposed AFDM-IM scheme, the information bits are carried by the activation state of the subsymbols in discrete affine Fourier (DAF) domain in addition to the conventional constellation symbols. To efficiently perform IM, we divide the subsymbols in DAF domain into several groups and consider both the localized and distributed strategies. An asymptotically tight upper bound on the average bit error rate (BER) of the maximum-likelihood detection in the existence of channel estimation errors is derived in closed-form. Computer simulations are carried out to evaluate the performance of the proposed AFDM-IM scheme, whose results corroborate its superiority over the benchmark schemes in the linear time-varying channels. We also evaluate the BER performance of the index and modulated bits for the AFDM-IM scheme with and without satisfying the full diversity condition of AFDM. The results show that the index bits have a stronger diversity protection than the modulated bits even when the full diversity condition of AFDM is not satisfied.
</details>
<details>
<summary>摘要</summary>
“Affine频率分多普通方式”（AFDM）是一种基于滑动信号的新多个 carriers 技术，适用于高移动通信，可以实现全多态性。在这篇论文中，我们提出了一个基于 AFDM 系统框架的指标修征（IM）方案，称为 AFDM-IM。在我们的提案中，信息位元被传递到 AFDM 系统中的几个批次中，并且在这些批次中使用传统的折衣符号。为了有效地实现 IM，我们在 DAF 领域中分割 subsymbols 成多个群体，并考虑了本地化和分散的两种策略。我们 derive 了一个对应于最大可能性探测的对应几何率（BER）的封闭式上界，并将其与 computer simulations 进行评估。结果显示，我们的 AFDM-IM 方案在线性时间变化频率对应于更高的性能。我们还评估了 AFDM-IM 方案中的指标位元和修征位元的 BER 性能，并发现指标位元在 AFDM 的全多态性不满足时仍然具有更强的多态保护。
</details></li>
</ul>
<hr>
<h2 id="Waveform-Design-for-MIMO-OFDM-Integrated-Sensing-and-Communication-System-An-Information-Theoretical-Approach"><a href="#Waveform-Design-for-MIMO-OFDM-Integrated-Sensing-and-Communication-System-An-Information-Theoretical-Approach" class="headerlink" title="Waveform Design for MIMO-OFDM Integrated Sensing and Communication System: An Information Theoretical Approach"></a>Waveform Design for MIMO-OFDM Integrated Sensing and Communication System: An Information Theoretical Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05444">http://arxiv.org/abs/2310.05444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqing Wei, Jinghui Piao, Xin Yuan, Huici Wu, J. Andrew Zhang, Zhiyong Feng, Lin Wang, Ping Zhang</li>
<li>for: 这篇论文主要探讨了integration sensing and communication（ISAC）系统中波形设计的问题，以及在5G-A和6G移动通信系统中ISAC技术的应用。</li>
<li>methods: 本论文使用了信息论中的统一性能指标，即相互信息（MI），来度量多普逻盘ISAC系统中的感知和通信性能。然后，提出了最优波形设计方案，以最大化感知MI、通信MI和权衡感知和通信MI的加权和。</li>
<li>results: 优化结果通过Monte Carlo伪陷 simulations进行验证。本研究提供了有效的封闭式表达式，使得MIMO-OFDM ISAC系统能够实现平衡的感知和通信性能。<details>
<summary>Abstract</summary>
Integrated sensing and communication (ISAC) is regarded as the enabling technology in the future 5th-Generation-Advanced (5G-A) and 6th-Generation (6G) mobile communication system. ISAC waveform design is critical in ISAC system. However, the difference of the performance metrics between sensing and communication brings challenges for the ISAC waveform design. This paper applies the unified performance metrics in information theory, namely mutual information (MI), to measure the communication and sensing performance in multicarrier ISAC system. In multi-input multi-output orthogonal frequency division multiplexing (MIMO-OFDM) ISAC system, we first derive the sensing and communication MI with subcarrier correlation and spatial correlation. Then, we propose optimal waveform designs for maximizing the sensing MI, communication MI and the weighted sum of sensing and communication MI, respectively. The optimization results are validated by Monte Carlo simulations. Our work provides effective closed-form expressions for waveform design, enabling the realization of MIMO-OFDM ISAC system with balanced performance in communication and sensing.
</details>
<details>
<summary>摘要</summary>
Integrated sensing and communication (ISAC) 被视为未来 fifth-generation advanced (5G-A) 和 sixth-generation (6G) 移动通信系统的关键技术。 ISAC 波形设计是 ISAC 系统的关键。然而，传感和通信性能的不同会对 ISAC 波形设计带来挑战。本文使用信息理论中的共聚性指标（MI）来度量传感和通信性能。在多个输入多个输出的orthogonal frequency division multiplexing (MIMO-OFDM) ISAC 系统中，我们首先计算传感和通信 MI 的相互关系。然后，我们提出了最佳波形设计，以最大化传感 MI、通信 MI 和权重总和传感和通信 MI。我们的工作提供了有效的关闭式表达式，使得 MIMO-OFDM ISAC 系统可以实现平衡的传感和通信性能。Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="A-Stochastic-Particle-Variational-Bayesian-Inference-Inspired-Deep-Unfolding-Network-for-Non-Convex-Parameter-Estimation"><a href="#A-Stochastic-Particle-Variational-Bayesian-Inference-Inspired-Deep-Unfolding-Network-for-Non-Convex-Parameter-Estimation" class="headerlink" title="A Stochastic Particle Variational Bayesian Inference Inspired Deep-Unfolding Network for Non-Convex Parameter Estimation"></a>A Stochastic Particle Variational Bayesian Inference Inspired Deep-Unfolding Network for Non-Convex Parameter Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05382">http://arxiv.org/abs/2310.05382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixiang Hu, An Liu, Minjian Zhao</li>
<li>for: 这个研究旨在提供一个高维度非对称参数估计的方法，以应对未来无线网络中的普遍感知服务需求。</li>
<li>methods: 本研究提出了一个平行数位粒子统计量 bayesian inference（PSPVBI）算法，并将其融合到深度 unfolding 网络中（DU），以提高算法的速度和精度。</li>
<li>results: 实验结果显示，LPSPVBI 算法在无线感知应用中的参数估计比现有方法高精度。<details>
<summary>Abstract</summary>
Future wireless networks are envisioned to provide ubiquitous sensing services, which also gives rise to a substantial demand for high-dimensional non-convex parameter estimation, i.e., the associated likelihood function is non-convex and contains numerous local optima. Variational Bayesian inference (VBI) provides a powerful tool for modeling complex estimation problems and reasoning with prior information, but poses a long-standing challenge on computing intractable posteriori distributions. Most existing variational methods generally rely on assumptions about specific distribution families to derive closed-form solutions, and are difficult to apply in high-dimensional, non-convex scenarios. Given these challenges, firstly, we propose a parallel stochastic particle variational Bayesian inference (PSPVBI) algorithm. Thanks to innovations such as particle approximation, additional updates of particle positions, and parallel stochastic successive convex approximation (PSSCA), PSPVBI can flexibly drive particles to fit the posteriori distribution with acceptable complexity, yielding high-precision estimates of the target parameters. Furthermore, additional speedup can be obtained by deep-unfolding (DU) the PSPVBI algorithm. Specifically, superior hyperparameters are learned to dramatically reduce the number of algorithmic iterations. In this PSPVBI-induced Deep-Unfolding Networks, some techniques related to gradient computation, data sub-sampling, differentiable sampling, and generalization ability are also employed to facilitate the practical deployment. Finally, we apply the LPSPVBI to solve several important parameter estimation problems in wireless sensing scenarios. Simulations indicate that the LPSPVBI algorithm outperforms existing solutions.
</details>
<details>
<summary>摘要</summary>
将来的无线网络将提供 ubique 感知服务，导致高维非拟合参数估计的巨大需求，即相关的可能函数是非拟合的和含有多个局部最优点。基本 Bayesian 推理 (VB) 提供了模拟复杂估计问题和使用先验信息进行理据处理的强大工具，但计算不可靠的后验分布却成为了长期挑战。大多数现有的变量方法通常假设特定的分布家族，从而得到关闭式解决方案，而在高维、非拟合情况下困难应用。为解决这些挑战，我们首先提出了并行随机粒子变量 Bayesian 推理（PSPVBI）算法。因为增加了粒子方法、粒子位置更新和并行随机Successive Convex Approximation（PSSCA）等创新，PSPVBI可以灵活地使粒子适应 posteriori 分布，得到高精度的参数估计。此外，通过深度 unfolding（DU）的技术，我们可以进一步提高算法的速度。具体来说，我们通过学习超过参数，减少算法迭代数量，实现了在 PSPVBI 中的深度 unfolding。在 PSPVBI induced Deep-Unfolding Networks 中，我们还使用了一些相关的梯度计算、数据子抽样、可导采样和通用能力等技术，以便实际应用。最后，我们通过 LPSPVBI 算法解决了无线感知场景中的一些重要参数估计问题。 simulation 结果表明，LPSPVBI 算法在现有解决方案中具有优势。
</details></li>
</ul>
<hr>
<h2 id="Distortion-Aware-Phase-Retrieval-Receiver-for-High-Order-QAM-Transmission-with-Carrierless-Intensity-Only-Measurements"><a href="#Distortion-Aware-Phase-Retrieval-Receiver-for-High-Order-QAM-Transmission-with-Carrierless-Intensity-Only-Measurements" class="headerlink" title="Distortion-Aware Phase Retrieval Receiver for High-Order QAM Transmission with Carrierless Intensity-Only Measurements"></a>Distortion-Aware Phase Retrieval Receiver for High-Order QAM Transmission with Carrierless Intensity-Only Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05314">http://arxiv.org/abs/2310.05314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanzi Huang, Haoshuo Chen, Qi Gao, Yetian Huang, Nicolas K. Fontaine, Mikael Mazur, Lauren Dallachiesa, Roland Ryf, Zhengxuan Li, Yingxiong Song</li>
<li>for:  investigate high-order quadrature amplitude modulation (QAM) signals transmission with carrierless and intensity-only measurements, and improve precision of phase retrieval (PR) algorithm.</li>
<li>methods:  propose distortion-aware PR scheme with training and reconstruction stages, estimate and emulate distortion caused by channel impairments, improve agreement between estimated and measured amplitudes.</li>
<li>results:  experimentally demonstrate 50-GBaud 16QAM and 32QAM signals transmission over 40km and 80km SSMF spans, achieve BERs below 6.25% HD-FEC and 25% SD-FEC thresholds, and achieve post-FEC data rate of up to 140 Gb&#x2F;s with optimal pilot symbol ratio of 20%.<details>
<summary>Abstract</summary>
We experimentally investigate transmitting high-order quadrature amplitude modulation (QAM) signals with carrierless and intensity-only measurements with phase retrieval (PR) receiving techniques. The intensity errors during measurement, including noise and distortions, are found to be a limiting factor for the precise convergence of the PR algorithm. To improve the PR reconstruction accuracy, we propose a distortion-aware PR scheme comprising both training and reconstruction stages. By estimating and emulating the distortion caused by various channel impairments, the proposed scheme enables enhanced agreement between the estimated and measured amplitudes throughout the PR iteration, thus resulting in improved reconstruction performance to support high-order QAM transmission. With the aid of proposed techniques, we experimentally demonstrate 50-GBaud 16QAM and 32QAM signals transmitting through a standard single-mode optical fiber (SSMF) span of 40 and 80 km, and achieve bit error rates (BERs) below the 6.25% hard decision (HD)-forward error correction (FEC) and 25% soft decision (SD)-FEC thresholds for the two modulation formats, respectively. By tuning the pilot symbol ratio and applying concatenated coding, we also demonstrate that a post-FEC data rate of up to 140 Gb/s can be achieved for both distances at an optimal pilot symbol ratio of 20%.
</details>
<details>
<summary>摘要</summary>
我们实验性地研究了在无载波和强度仅测量下传输高阶 quadrature amplitude modulation（QAM）信号。测量过程中的强度错误，包括噪声和扭曲，被发现是精度恢复 алгоритм的限制因素。为了提高恢复精度，我们提议了一种考虑到频率响应的扭曲恢复方案，包括训练和重建两个阶段。通过估算和模拟各种通道缺陷所引起的扭曲，该方案可以在PR迭代过程中实现更好的吻合，从而提高恢复性能，以支持高阶QAM传输。通过我们的技术，我们实验性地在标准单模光纤（SSMF） span 40和80公里上传输了50Gbps 16QAM和32QAM信号，并在这两种模ulation format中达到了Below the 6.25% hard decision（HD）forward error correction（FEC）和25% soft decision（SD）FEC的下限。通过调整示例符号比例和 concatenated coding，我们还示出了在这两个距离上达到140Gb/s的后FEC数据速率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/eess.SP_2023_10_09/" data-id="clombee3i019ps08892st1aqo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/cs.SD_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T15:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/cs.SD_2023_10_08/">cs.SD - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="VITS-based-Singing-Voice-Conversion-System-with-DSPGAN-post-processing-for-SVCC2023"><a href="#VITS-based-Singing-Voice-Conversion-System-with-DSPGAN-post-processing-for-SVCC2023" class="headerlink" title="VITS-based Singing Voice Conversion System with DSPGAN post-processing for SVCC2023"></a>VITS-based Singing Voice Conversion System with DSPGAN post-processing for SVCC2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05118">http://arxiv.org/abs/2310.05118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiquan Zhou, Meng Chen, Yi Lei, Jihua Zhu, Weifeng Zhao</li>
<li>for: 这项研究的目的是为SVCC2023提供一个系统，以便在 singing voice conversion 领域实现高质量的音频转换。</li>
<li>methods: 该系统包括三个模块：特征提取器、声音转换器和后处理器。特征提取器使用 HuBERT 模型提取 singing voice 中的 F0 轨迹和 speaker-independent 语言内容。声音转换器使用 target speaker 的声音特征、F0 和语言内容来生成目标speaker 的波形。此外，为了进一步提高音质，我们还使用了一个精度调整的 DSPGAN  vocoder。</li>
<li>results: 在 official challenge 结果中，我们的系统在 cross-domain 任务中表现出色，得分第1和第2位，分别在自然性和相似性两个指标上。此外，我们还进行了一些缓解分析，以证明我们的系统设计的有效性。<details>
<summary>Abstract</summary>
This paper presents the T02 team's system for the Singing Voice Conversion Challenge 2023 (SVCC2023). Our system entails a VITS-based SVC model, incorporating three modules: a feature extractor, a voice converter, and a post-processor. Specifically, the feature extractor provides F0 contours and extracts speaker-independent linguistic content from the input singing voice by leveraging a HuBERT model. The voice converter is employed to recompose the speaker timbre, F0, and linguistic content to generate the waveform of the target speaker. Besides, to further improve the audio quality, a fine-tuned DSPGAN vocoder is introduced to re-synthesise the waveform. Given the limited target speaker data, we utilize a two-stage training strategy to adapt the base model to the target speaker. During model adaptation, several tricks, such as data augmentation and joint training with auxiliary singer data, are involved. Official challenge results show that our system achieves superior performance, especially in the cross-domain task, ranking 1st and 2nd in naturalness and similarity, respectively. Further ablation justifies the effectiveness of our system design.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Partial-Rank-Similarity-Minimization-Method-for-Quality-MOS-Prediction-of-Unseen-Speech-Synthesis-Systems-in-Zero-Shot-and-Semi-supervised-setting"><a href="#Partial-Rank-Similarity-Minimization-Method-for-Quality-MOS-Prediction-of-Unseen-Speech-Synthesis-Systems-in-Zero-Shot-and-Semi-supervised-setting" class="headerlink" title="Partial Rank Similarity Minimization Method for Quality MOS Prediction of Unseen Speech Synthesis Systems in Zero-Shot and Semi-supervised setting"></a>Partial Rank Similarity Minimization Method for Quality MOS Prediction of Unseen Speech Synthesis Systems in Zero-Shot and Semi-supervised setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05078">http://arxiv.org/abs/2310.05078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nii-yamagishilab/partial_rank_similarity">https://github.com/nii-yamagishilab/partial_rank_similarity</a></li>
<li>paper_authors: Hemant Yadav, Erica Cooper, Junichi Yamagishi, Sunayana Sitaram, Rajiv Ratn Shah</li>
<li>for: 这项研究旨在提出一种新的质量 mean opinion score（MOS）预测函数，用于评估未经见过的语音合成系统的质量。</li>
<li>methods: 该函数measure相对位置的相似性，而不是实际的MOS值，通过测量partial rank similarity（PRS）而不是L1损失函数。</li>
<li>results: 实验表明，PRS在零shot和半supervised设定下表现出色，与真实值更高度相关，而MSE和linear correlation coefficient metric可能不适用于评估MOS预测模型。<details>
<summary>Abstract</summary>
This paper introduces a novel objective function for quality mean opinion score (MOS) prediction of unseen speech synthesis systems. The proposed function measures the similarity of relative positions of predicted MOS values, in a mini-batch, rather than the actual MOS values. That is the partial rank similarity is measured (PRS) rather than the individual MOS values as with the L1 loss. Our experiments on out-of-domain speech synthesis systems demonstrate that the PRS outperforms L1 loss in zero-shot and semi-supervised settings, exhibiting stronger correlation with ground truth. These findings highlight the importance of considering rank order, as done by PRS, when training MOS prediction models. We also argue that mean squared error and linear correlation coefficient metrics may be unreliable for evaluating MOS prediction models. In conclusion, PRS-trained models provide a robust framework for evaluating speech quality and offer insights for developing high-quality speech synthesis systems. Code and models are available at github.com/nii-yamagishilab/partial_rank_similarity/
</details>
<details>
<summary>摘要</summary>
这份论文介绍了一种新的评价函数，用于预测未看过的语音合成系统的质量 mean opinion score（MOS）。提出的函数测量在一个小批次中预测的MOS值相对位置的相似性，而不是实际的MOS值。即使使用了partial rank similarity（PRS）而不是L1损失，我们的实验表明，PRS在零批次和半指导学习 Setting 中表现更好，与基准数据 exhibit stronger correlation。这些发现反映了考虑 rank order 的重要性，当训练 MOS 预测模型时。我们还认为 mean squared error 和 linear correlation coefficient  metrics 可能不可靠地评价 MOS 预测模型。 conclusion，PRS 训练的模型提供了一种robust的 speech quality 评价框架，并且为开发高质量语音合成系统提供了意见。代码和模型可以在github.com/nii-yamagishilab/partial_rank_similarity/ 找到。
</details></li>
</ul>
<hr>
<h2 id="SALT-Distinguishable-Speaker-Anonymization-Through-Latent-Space-Transformation"><a href="#SALT-Distinguishable-Speaker-Anonymization-Through-Latent-Space-Transformation" class="headerlink" title="SALT: Distinguishable Speaker Anonymization Through Latent Space Transformation"></a>SALT: Distinguishable Speaker Anonymization Through Latent Space Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05051">http://arxiv.org/abs/2310.05051</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bakerbunker/salt">https://github.com/bakerbunker/salt</a></li>
<li>paper_authors: Yuanjun Lv, Jixun Yao, Peikun Chen, Hongbin Zhou, Heng Lu, Lei Xie</li>
<li>for: 隐藏发音人的身份，保持语音质量和可理解性。</li>
<li>methods: 基于隐藏空间转换的发音人匿名系统（SALT），包括自主学习特征提取器和随机抽取多个发音人和其权重，并通过 interpolate 实现发音人匿名。同时，我们还 explore 了扩展方法以提高假发音人的多样性。</li>
<li>results: 在 Voice Privacy Challenge 数据集上，我们的系统实现了最佳的匿名度指标，同时保持语音质量和可理解性。<details>
<summary>Abstract</summary>
Speaker anonymization aims to conceal a speaker's identity without degrading speech quality and intelligibility. Most speaker anonymization systems disentangle the speaker representation from the original speech and achieve anonymization by averaging or modifying the speaker representation. However, the anonymized speech is subject to reduction in pseudo speaker distinctiveness, speech quality and intelligibility for out-of-distribution speaker. To solve this issue, we propose SALT, a Speaker Anonymization system based on Latent space Transformation. Specifically, we extract latent features by a self-supervised feature extractor and randomly sample multiple speakers and their weights, and then interpolate the latent vectors to achieve speaker anonymization. Meanwhile, we explore the extrapolation method to further extend the diversity of pseudo speakers. Experiments on Voice Privacy Challenge dataset show our system achieves a state-of-the-art distinctiveness metric while preserving speech quality and intelligibility. Our code and demo is availible at https://github.com/BakerBunker/SALT .
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PromptSpeaker-Speaker-Generation-Based-on-Text-Descriptions"><a href="#PromptSpeaker-Speaker-Generation-Based-on-Text-Descriptions" class="headerlink" title="PromptSpeaker: Speaker Generation Based on Text Descriptions"></a>PromptSpeaker: Speaker Generation Based on Text Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05001">http://arxiv.org/abs/2310.05001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongmao Zhang, Guanghou Liu, Yi Lei, Yunlin Chen, Hao Yin, Lei Xie, Zhifei Li</li>
<li>for: 这项研究旨在实现文本描述基于的发音人生成（text-guided speaker generation），即通过文本描述控制发音人生成过程。</li>
<li>methods: 该研究提出了一种名为PromptSpeaker的文本指导发音人生成系统，该系统包括提取器、零批量VITS和Glow模型。提取器预测基于文本描述的含义表示，并从这个分布中采样以获取 semantic representation。Glow模型将含义表示转换成发音人表示，而零批量VITS最后将发音人表示转换成真实的发音。</li>
<li>results: 研究证明PromptSpeaker可以生成与训练集外的新发音人，并且synthetic speaker voice具有相对合理的主观匹配质量。<details>
<summary>Abstract</summary>
Recently, text-guided content generation has received extensive attention. In this work, we explore the possibility of text description-based speaker generation, i.e., using text prompts to control the speaker generation process. Specifically, we propose PromptSpeaker, a text-guided speaker generation system. PromptSpeaker consists of a prompt encoder, a zero-shot VITS, and a Glow model, where the prompt encoder predicts a prior distribution based on the text description and samples from this distribution to obtain a semantic representation. The Glow model subsequently converts the semantic representation into a speaker representation, and the zero-shot VITS finally synthesizes the speaker's voice based on the speaker representation. We verify that PromptSpeaker can generate speakers new from the training set by objective metrics, and the synthetic speaker voice has reasonable subjective matching quality with the speaker prompt.
</details>
<details>
<summary>摘要</summary>
近些时间，文本指导内容生成已经受到了广泛关注。在这项工作中，我们探索了使用文本描述来控制发音生成过程的可能性。具体来说，我们提出了PromptSpeaker，一种文本指导的发音生成系统。PromptSpeaker包括一个描述符编码器、一个零拟合VITS和一个Glow模型，其中描述符编码器根据文本描述预测一个优先分布，并从这个分布中采样以获取一个semantic表示。Glow模型然后将semantic表示转化为发音表示，零拟合VITS最后将发音表示转化为声音。我们证明了PromptSpeaker可以新生成不同于训练集的发音，并且synthetic声音具有合理的主观匹配质量与发音描述。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/cs.SD_2023_10_08/" data-id="clombedxo00wjs0888jyx9noq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/cs.CV_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T13:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/cs.CV_2023_10_08/">cs.CV - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Progressive-Neural-Compression-for-Adaptive-Image-Offloading-under-Timing-Constraints"><a href="#Progressive-Neural-Compression-for-Adaptive-Image-Offloading-under-Timing-Constraints" class="headerlink" title="Progressive Neural Compression for Adaptive Image Offloading under Timing Constraints"></a>Progressive Neural Compression for Adaptive Image Offloading under Timing Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05306">http://arxiv.org/abs/2310.05306</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rickywrq/Progressive-Neural-Compression">https://github.com/rickywrq/Progressive-Neural-Compression</a></li>
<li>paper_authors: Ruiqi Wang, Hanyang Liu, Jiaming Qiu, Moran Xu, Roch Guerin, Chenyang Lu</li>
<li>for: 这篇论文旨在提出一种适应性的进步神经压缩方法，以提高机器学习应用程序在边缘服务器上的推论性能，并且在网络带宽不稳定的情况下进行图像卸载。</li>
<li>methods: 本篇论文使用了进步神经压缩（PNC）方法，并使用了多目标减少自适应器来训练对应的图像压缩策略，以便根据可用带宽进行图像卸载。</li>
<li>results: 相比于现有的神经压缩方法和传统压缩方法，PNC方法可以提高机器学习应用程序的推论性能，并且可以适应网络带宽不稳定的情况。<details>
<summary>Abstract</summary>
IoT devices are increasingly the source of data for machine learning (ML) applications running on edge servers. Data transmissions from devices to servers are often over local wireless networks whose bandwidth is not just limited but, more importantly, variable. Furthermore, in cyber-physical systems interacting with the physical environment, image offloading is also commonly subject to timing constraints. It is, therefore, important to develop an adaptive approach that maximizes the inference performance of ML applications under timing constraints and the resource constraints of IoT devices. In this paper, we use image classification as our target application and propose progressive neural compression (PNC) as an efficient solution to this problem. Although neural compression has been used to compress images for different ML applications, existing solutions often produce fixed-size outputs that are unsuitable for timing-constrained offloading over variable bandwidth. To address this limitation, we train a multi-objective rateless autoencoder that optimizes for multiple compression rates via stochastic taildrop to create a compression solution that produces features ordered according to their importance to inference performance. Features are then transmitted in that order based on available bandwidth, with classification ultimately performed using the (sub)set of features received by the deadline. We demonstrate the benefits of PNC over state-of-the-art neural compression approaches and traditional compression methods on a testbed comprising an IoT device and an edge server connected over a wireless network with varying bandwidth.
</details>
<details>
<summary>摘要</summary>
互联网物品（IoT）正在日益成为机器学习（ML）应用程序的数据来源。从设备到服务器的数据传输通常是通过本地无线网络，带宽是有限的，更重要的是可变的。在融合物理环境的应用中，图像传输也会受到时间限制。因此，发展一个适应的方法可以在时间限制和互联网设备的资源限制下，提高机器学习应用的推论性能。在这篇文章中，我们使用图像分类作为我们的目标应用，并提出进步的神经压缩（PNC）方法来解决这个问题。尽管神经压缩已经用于不同的机器学习应用中图像压缩，但现有的解决方案通常会生成固定大小的出力，这不适合时间限制下的图像传输。为解决这个限制，我们训练了多个目标自适应的减少顶峰网络，以便在不同的压缩率下选择适合的压缩率，并生成按照推论性能的重要性排序的特征。这些特征会根据可用带宽进行传输，并最终通过在时间限制下接收的（子）集特征进行分类。我们在一个包括IoT设备和边缘服务器之间的无线网络上进行了实验，评估PNC方法与现有的神经压缩方法和传统压缩方法的比较。
</details></li>
</ul>
<hr>
<h2 id="GestSync-Determining-who-is-speaking-without-a-talking-head"><a href="#GestSync-Determining-who-is-speaking-without-a-talking-head" class="headerlink" title="GestSync: Determining who is speaking without a talking head"></a>GestSync: Determining who is speaking without a talking head</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05304">http://arxiv.org/abs/2310.05304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sindhu-Hegde/gestsync">https://github.com/Sindhu-Hegde/gestsync</a></li>
<li>paper_authors: Sindhu B Hegde, Andrew Zisserman</li>
<li>For: The paper is written for determining if a person’s gestures are correlated with their speech or not, and exploring the use of self-supervised learning for this task.* Methods: The paper introduces a dual-encoder model for the task of Gesture-Sync, and compares the performance of different input representations, including RGB frames, keypoint images, and keypoint vectors.* Results: The paper shows that the model can be trained using self-supervised learning alone, and evaluates its performance on the LRS3 dataset. Additionally, the paper demonstrates applications of Gesture-Sync for audio-visual synchronisation and determining who is the speaker in a crowd without seeing their faces.<details>
<summary>Abstract</summary>
In this paper we introduce a new synchronisation task, Gesture-Sync: determining if a person's gestures are correlated with their speech or not. In comparison to Lip-Sync, Gesture-Sync is far more challenging as there is a far looser relationship between the voice and body movement than there is between voice and lip motion. We introduce a dual-encoder model for this task, and compare a number of input representations including RGB frames, keypoint images, and keypoint vectors, assessing their performance and advantages. We show that the model can be trained using self-supervised learning alone, and evaluate its performance on the LRS3 dataset. Finally, we demonstrate applications of Gesture-Sync for audio-visual synchronisation, and in determining who is the speaker in a crowd, without seeing their faces. The code, datasets and pre-trained models can be found at: \url{https://www.robots.ox.ac.uk/~vgg/research/gestsync}.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们引入了一个新的同步任务，即Gesture-Sync：判断一个人的手势与其说话是否相关。与Lip-Sync相比，Gesture-Sync更加具有挑战性，因为voice和身体运动之间的关系比lip motion和语音之间的关系更加松散。我们介绍了一种双encoder模型来解决这个任务，并比较了不同的输入表示，包括RGB帧、关键点图像和关键点向量，评估其性能和优势。我们表明该模型可以通过自我超视了学习来训练，并评估其性能在LRS3 dataset上。最后，我们展示了Gesture-Sync在audio-visual同步和推测人声的场景中的应用，以及不看到人脸时推测说话人的场景。相关代码、数据集和预训练模型可以在以下链接中找到：\url{https://www.robots.ox.ac.uk/~vgg/research/gestsync}.
</details></li>
</ul>
<hr>
<h2 id="Image-Compression-and-Decompression-Framework-Based-on-Latent-Diffusion-Model-for-Breast-Mammography"><a href="#Image-Compression-and-Decompression-Framework-Based-on-Latent-Diffusion-Model-for-Breast-Mammography" class="headerlink" title="Image Compression and Decompression Framework Based on Latent Diffusion Model for Breast Mammography"></a>Image Compression and Decompression Framework Based on Latent Diffusion Model for Breast Mammography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05299">http://arxiv.org/abs/2310.05299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neogeoss/EMBED_Mammo_Models">https://github.com/neogeoss/EMBED_Mammo_Models</a></li>
<li>paper_authors: InChan Hwang, MinJae Woo</li>
<li>for: 这个研究旨在开发一个新的医疗图像压缩和解压缩框架，利用隐藏扩散模型（LDM）。LDM比过去的浊度扩散概率模型（DDPM）有更好的图像质量和较少的计算资源，在图像解压缩过程中。</li>
<li>methods: 这个研究使用了隐藏扩散模型（LDM）和Torchvision进行图像扩大，并考虑了医疗图像数据的应用。</li>
<li>results: 实验结果显示，这种方法比传统文件压缩算法更好，并且训练使用解压缩档案的 convolutional neural network（CNN）模型与使用原始图像档案训练的模型相比，表现相似。此外，这种方法还可以将医疗图像数据压缩到较小的大小，以便在医疗设备中储存。研究的影响范围包括医疗图像压缩中的噪声reduction和替代复杂波纹基于损失less压缩算法。<details>
<summary>Abstract</summary>
This research presents a novel framework for the compression and decompression of medical images utilizing the Latent Diffusion Model (LDM). The LDM represents advancement over the denoising diffusion probabilistic model (DDPM) with a potential to yield superior image quality while requiring fewer computational resources in the image decompression process. A possible application of LDM and Torchvision for image upscaling has been explored using medical image data, serving as an alternative to traditional image compression and decompression algorithms. The experimental outcomes demonstrate that this approach surpasses a conventional file compression algorithm, and convolutional neural network (CNN) models trained with decompressed files perform comparably to those trained with original image files. This approach also significantly reduces dataset size so that it can be distributed with a smaller size, and medical images take up much less space in medical devices. The research implications extend to noise reduction in lossy compression algorithms and substitute for complex wavelet-based lossless algorithms.
</details>
<details>
<summary>摘要</summary>
这项研究提出了一种新的压缩和解压缩医疗图像框架，利用Latent Diffusion Model（LDM）。LDM比denoising diffusion probabilistic model（DDPM）有更多的进步，可以提供更高质量的图像，同时需要更少的计算资源进行图像解压缩。研究人员还探索了使用LDM和Torchvision进行图像缩放，作为传统图像压缩和解压缩算法的替代方案。实验结果表明，这种方法比传统文件压缩算法更好，并且使用解压缩文件训练的 convolutional neural network（CNN）模型与原始图像文件训练的模型性能相似。此外，这种方法还可以减少数据集大小，使得它可以更加容易地分布，医疗设备中的医疗图像占用的空间也更加小。研究的影响扩展到了lossy压缩算法中的噪声减少和lossless压缩算法中的复杂波лет特征。
</details></li>
</ul>
<hr>
<h2 id="MSight-An-Edge-Cloud-Infrastructure-based-Perception-System-for-Connected-Automated-Vehicles"><a href="#MSight-An-Edge-Cloud-Infrastructure-based-Perception-System-for-Connected-Automated-Vehicles" class="headerlink" title="MSight: An Edge-Cloud Infrastructure-based Perception System for Connected Automated Vehicles"></a>MSight: An Edge-Cloud Infrastructure-based Perception System for Connected Automated Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05290">http://arxiv.org/abs/2310.05290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rusheng Zhang, Depu Meng, Shengyin Shen, Zhengxia Zou, Houqiang Li, Henry X. Liu</li>
<li>for: 这篇论文是为了探讨Connected Automated Vehicle（CAV）应用中的道路边缘感知技术。</li>
<li>methods: 这篇论文使用了路侧感知系统MSight，实现了实时车辆检测、定位、追踪和短期路径预测。</li>
<li>results: 评估结果显示MSight系统能够维持车道精度，并且具有几乎 zero latency，这表明了这个系统在CAV安全性和效率方面的应用潜力。<details>
<summary>Abstract</summary>
As vehicular communication and networking technologies continue to advance, infrastructure-based roadside perception emerges as a pivotal tool for connected automated vehicle (CAV) applications. Due to their elevated positioning, roadside sensors, including cameras and lidars, often enjoy unobstructed views with diminished object occlusion. This provides them a distinct advantage over onboard perception, enabling more robust and accurate detection of road objects. This paper presents MSight, a cutting-edge roadside perception system specifically designed for CAVs. MSight offers real-time vehicle detection, localization, tracking, and short-term trajectory prediction. Evaluations underscore the system's capability to uphold lane-level accuracy with minimal latency, revealing a range of potential applications to enhance CAV safety and efficiency. Presently, MSight operates 24/7 at a two-lane roundabout in the City of Ann Arbor, Michigan.
</details>
<details>
<summary>摘要</summary>
“自动驾驶汽车（CAV）技术继续发展，基础设施上的路面感知emerges as a pivotal tool。由于路面感知器的高位置，包括摄像头和激光测距仪，通常有较好的视野和降低物体遮挡，这提供了它们与车辆上的感知相比，更加稳定和准确地检测道路上的物体。本文介绍了MSight，一个特有的路面感知系统，专门设计供CAV使用。MSight在实时提供车辆检测、定位、追踪和短期预测。评估结果显示MSight可以保持车道精度，并具有最小延迟。这些应用可能增加CAV的安全和效率。目前，MSight在美国密歇根州安那堤市的一个二轮圆环上运行24小时。”
</details></li>
</ul>
<hr>
<h2 id="The-Emergence-of-Reproducibility-and-Consistency-in-Diffusion-Models"><a href="#The-Emergence-of-Reproducibility-and-Consistency-in-Diffusion-Models" class="headerlink" title="The Emergence of Reproducibility and Consistency in Diffusion Models"></a>The Emergence of Reproducibility and Consistency in Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05264">http://arxiv.org/abs/2310.05264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Liyue Shen, Qing Qu</li>
<li>for: 这个论文的目的是探索Diffusion模型中的一种常见现象，即“一致性模型重复性”。</li>
<li>methods: 作者采用了大量实验和分析方法，包括使用不同的模型架构和训练策略，来研究Diffusion模型的一致性模型重复性。</li>
<li>results: 研究发现，Diffusion模型在不同的训练 regime中都具有一定的一致性模型重复性，其中包括“记忆化 режим”和“泛化 режим”。此外，作者还发现这种一致性模型重复性可以在多种Diffusion模型的变种中找到，例如Conditional Diffusion模型、用于解决反向问题的Diffusion模型以及精度调整后的Diffusion模型。<details>
<summary>Abstract</summary>
Recently, diffusion models have emerged as powerful deep generative models, showcasing cutting-edge performance across various applications such as image generation, solving inverse problems, and text-to-image synthesis. These models generate new data (e.g., images) by transforming random noise inputs through a reverse diffusion process. In this work, we uncover a distinct and prevalent phenomenon within diffusion models in contrast to most other generative models, which we refer to as ``consistent model reproducibility''. To elaborate, our extensive experiments have consistently shown that when starting with the same initial noise input and sampling with a deterministic solver, diffusion models tend to produce nearly identical output content. This consistency holds true regardless of the choices of model architectures and training procedures. Additionally, our research has unveiled that this exceptional model reproducibility manifests in two distinct training regimes: (i) ``memorization regime,'' characterized by a significantly overparameterized model which attains reproducibility mainly by memorizing the training data; (ii) ``generalization regime,'' in which the model is trained on an extensive dataset, and its reproducibility emerges with the model's generalization capabilities. Our analysis provides theoretical justification for the model reproducibility in ``memorization regime''. Moreover, our research reveals that this valuable property generalizes to many variants of diffusion models, including conditional diffusion models, diffusion models for solving inverse problems, and fine-tuned diffusion models. A deeper understanding of this phenomenon has the potential to yield more interpretable and controllable data generative processes based on diffusion models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Structure-Preserving-Instance-Segmentation-via-Skeleton-Aware-Distance-Transform"><a href="#Structure-Preserving-Instance-Segmentation-via-Skeleton-Aware-Distance-Transform" class="headerlink" title="Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform"></a>Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05262">http://arxiv.org/abs/2310.05262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zudi Lin, Donglai Wei, Aarush Gupta, Xingyu Liu, Deqing Sun, Hanspeter Pfister</li>
<li>for:  Histopathology image segmentation</li>
<li>methods:  Skeleton-aware distance transform (SDT) combining object skeleton and distance transform</li>
<li>results:  State-of-the-art performance in histopathology image segmentation<details>
<summary>Abstract</summary>
Objects with complex structures pose significant challenges to existing instance segmentation methods that rely on boundary or affinity maps, which are vulnerable to small errors around contacting pixels that cause noticeable connectivity change. While the distance transform (DT) makes instance interiors and boundaries more distinguishable, it tends to overlook the intra-object connectivity for instances with varying width and result in over-segmentation. To address these challenges, we propose a skeleton-aware distance transform (SDT) that combines the merits of object skeleton in preserving connectivity and DT in modeling geometric arrangement to represent instances with arbitrary structures. Comprehensive experiments on histopathology image segmentation demonstrate that SDT achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
对于具有复杂结构的对象，现有的实例分割方法，例如基于边界或相互关系图的方法，会面临 significiant 挑战。这些方法容易受到小范围内的像素错误所致的连接性变化的影响，从而导致分割不准确。而距离变换（DT）可以使实例的内部和边界更加可识别，但是它通常会忽略内部对象的连接性，导致不同宽度的实例进行过分割。为解决这些挑战，我们提议使用skeleton-aware distance transform（SDT），这种方法结合了对象骨架的优点，以保持连接性，并且利用距离变换来模型几何关系，以更好地表示具有自由结构的实例。我们对历史病理图像分割进行了广泛的实验，结果表明，SDT可以达到当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="SCANet-Scene-Complexity-Aware-Network-for-Weakly-Supervised-Video-Moment-Retrieval"><a href="#SCANet-Scene-Complexity-Aware-Network-for-Weakly-Supervised-Video-Moment-Retrieval" class="headerlink" title="SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval"></a>SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05241">http://arxiv.org/abs/2310.05241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunjae Yoon, Gwanhyeong Koo, Dahyun Kim, Chang D. Yoo</li>
<li>for: 本研究旨在提高视频oment Retrieval（VMR）系统的精度和效率，通过在多个视频中检索具有相同语言查询的时刻点。</li>
<li>methods: 本研究提出了一种新的Scene Complexity Aware Network（SCANet），该网络能够评估多个视频中场景的复杂性，并根据场景的复杂性生成适应性的提案。</li>
<li>results: 实验结果表明，使用SCANet网络可以在三个检索标准（Charades-STA、ActivityNet、TVR）上达到状态级性能，并且 demonstarted the effectiveness of incorporating scene complexity in VMR systems.<details>
<summary>Abstract</summary>
Video moment retrieval aims to localize moments in video corresponding to a given language query. To avoid the expensive cost of annotating the temporal moments, weakly-supervised VMR (wsVMR) systems have been studied. For such systems, generating a number of proposals as moment candidates and then selecting the most appropriate proposal has been a popular approach. These proposals are assumed to contain many distinguishable scenes in a video as candidates. However, existing proposals of wsVMR systems do not respect the varying numbers of scenes in each video, where the proposals are heuristically determined irrespective of the video. We argue that the retrieval system should be able to counter the complexities caused by varying numbers of scenes in each video. To this end, we present a novel concept of a retrieval system referred to as Scene Complexity Aware Network (SCANet), which measures the `scene complexity' of multiple scenes in each video and generates adaptive proposals responding to variable complexities of scenes in each video. Experimental results on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR) achieve state-of-the-art performances and demonstrate the effectiveness of incorporating the scene complexity.
</details>
<details>
<summary>摘要</summary>
视频瞬间检索目标是将视频中对应给给定语言查询的时刻点进行本地化。为了避免对时间点的标注成本昂贵，弱监督视频检索（wsVMR）系统得到了研究。这些系统通常采取生成多个提议作为时刻点候选者，然后选择最合适的提议。这些提议假设视频中包含许多可识别的场景。然而，现有的wsVMR系统提议不尊重每个视频中场景的数量，这些提议是基于视频的规则随意确定的。我们认为检索系统应该能够对每个视频中场景的复杂性进行应对。为此，我们提出了一种新的检索系统，即场景复杂度意识网络（SCANet），它在多个视频中场景的复杂性测量并生成适应场景复杂性的提议。实验结果在三个检索标准 benchmark（i.e., Charades-STA、ActivityNet、TVR）上达到了状态的最佳性能，并证明了包含场景复杂度的 incorporation 的效iveness。
</details></li>
</ul>
<hr>
<h2 id="Latent-Diffusion-Model-for-Medical-Image-Standardization-and-Enhancement"><a href="#Latent-Diffusion-Model-for-Medical-Image-Standardization-and-Enhancement" class="headerlink" title="Latent Diffusion Model for Medical Image Standardization and Enhancement"></a>Latent Diffusion Model for Medical Image Standardization and Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05237">http://arxiv.org/abs/2310.05237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Selim, Jie Zhang, Faraneh Fathi, Michael A. Brooks, Ge Wang, Guoqiang Yu, Jin Chen</li>
<li>for: 这篇论文的目的是提出一个新的数据构造模型，以对 computed tomography (CT) 图像进行标准化，以提高医学研究中的可比性和精确性。</li>
<li>methods: 这篇论文使用了一个名为 DiffusionCT 的新型数据构造模型，该模型在 latent space 中将不同的 CT 图像转换为标准化的图像，以提高医学研究中的可比性和精确性。</li>
<li>results: 这篇论文的实验结果显示，DiffusionCT 可以对 CT 图像进行高品质的标准化，并且可以降低 SPAD 图像中的噪声，进一步验证了 DiffusionCT 的有效性。<details>
<summary>Abstract</summary>
Computed tomography (CT) serves as an effective tool for lung cancer screening, diagnosis, treatment, and prognosis, providing a rich source of features to quantify temporal and spatial tumor changes. Nonetheless, the diversity of CT scanners and customized acquisition protocols can introduce significant inconsistencies in texture features, even when assessing the same patient. This variability poses a fundamental challenge for subsequent research that relies on consistent image features. Existing CT image standardization models predominantly utilize GAN-based supervised or semi-supervised learning, but their performance remains limited. We present DiffusionCT, an innovative score-based DDPM model that operates in the latent space to transform disparate non-standard distributions into a standardized form. The architecture comprises a U-Net-based encoder-decoder, augmented by a DDPM model integrated at the bottleneck position. First, the encoder-decoder is trained independently, without embedding DDPM, to capture the latent representation of the input data. Second, the latent DDPM model is trained while keeping the encoder-decoder parameters fixed. Finally, the decoder uses the transformed latent representation to generate a standardized CT image, providing a more consistent basis for downstream analysis. Empirical tests on patient CT images indicate notable improvements in image standardization using DiffusionCT. Additionally, the model significantly reduces image noise in SPAD images, further validating the effectiveness of DiffusionCT for advanced imaging tasks.
</details>
<details>
<summary>摘要</summary>
computed tomography (CT) serve as an effective tool for lung cancer screening, diagnosis, treatment, and prognosis, providing a rich source of features to quantify temporal and spatial tumor changes. However, the diversity of CT scanners and customized acquisition protocols can introduce significant inconsistencies in texture features, even when assessing the same patient. This variability poses a fundamental challenge for subsequent research that relies on consistent image features. Existing CT image standardization models predominantly utilize GAN-based supervised or semi-supervised learning, but their performance remains limited. We present DiffusionCT, an innovative score-based DDPM model that operates in the latent space to transform disparate non-standard distributions into a standardized form. The architecture comprises a U-Net-based encoder-decoder, augmented by a DDPM model integrated at the bottleneck position. First, the encoder-decoder is trained independently, without embedding DDPM, to capture the latent representation of the input data. Second, the latent DDPM model is trained while keeping the encoder-decoder parameters fixed. Finally, the decoder uses the transformed latent representation to generate a standardized CT image, providing a more consistent basis for downstream analysis. Empirical tests on patient CT images indicate notable improvements in image standardization using DiffusionCT. Additionally, the model significantly reduces image noise in SPAD images, further validating the effectiveness of DiffusionCT for advanced imaging tasks.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Cross-Dataset-Performance-of-Distracted-Driving-Detection-With-Score-Softmax-Classifier"><a href="#Enhancing-Cross-Dataset-Performance-of-Distracted-Driving-Detection-With-Score-Softmax-Classifier" class="headerlink" title="Enhancing Cross-Dataset Performance of Distracted Driving Detection With Score-Softmax Classifier"></a>Enhancing Cross-Dataset Performance of Distracted Driving Detection With Score-Softmax Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05202">http://arxiv.org/abs/2310.05202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/congduan-hnu/ssoftmax">https://github.com/congduan-hnu/ssoftmax</a></li>
<li>paper_authors: Cong Duan, Zixuan Liu, Jiahao Xia, Minghai Zhang, Jiacai Liao, Libo Cao</li>
<li>for: 这个研究旨在提高车上司机的实时监控，以预测分心、疲劳和潜在危险。</li>
<li>methods: 我们引入了Score-Softmax分类器，以解决跨数据集短cut learning问题，并且利用人类评价模式设计了二维超级监管矩阵。</li>
<li>results: 我们的研究表明，Score-Softmax分类器可以提高跨数据集表现，并且比传统方法更好地结合多个数据集。<details>
<summary>Abstract</summary>
Deep neural networks enable real-time monitoring of in-vehicle driver, facilitating the timely prediction of distractions, fatigue, and potential hazards. This technology is now integral to intelligent transportation systems. Recent research has exposed unreliable cross-dataset end-to-end driver behavior recognition due to overfitting, often referred to as ``shortcut learning", resulting from limited data samples. In this paper, we introduce the Score-Softmax classifier, which addresses this issue by enhancing inter-class independence and Intra-class uncertainty. Motivated by human rating patterns, we designed a two-dimensional supervisory matrix based on marginal Gaussian distributions to train the classifier. Gaussian distributions help amplify intra-class uncertainty while ensuring the Score-Softmax classifier learns accurate knowledge. Furthermore, leveraging the summation of independent Gaussian distributed random variables, we introduced a multi-channel information fusion method. This strategy effectively resolves the multi-information fusion challenge for the Score-Softmax classifier. Concurrently, we substantiate the necessity of transfer learning and multi-dataset combination. We conducted cross-dataset experiments using the SFD, AUCDD-V1, and 100-Driver datasets, demonstrating that Score-Softmax improves cross-dataset performance without modifying the model architecture. This provides a new approach for enhancing neural network generalization. Additionally, our information fusion approach outperforms traditional methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Discriminative-Multi-Modal-Learning-with-Large-Scale-Pre-Trained-Models"><a href="#Improving-Discriminative-Multi-Modal-Learning-with-Large-Scale-Pre-Trained-Models" class="headerlink" title="Improving Discriminative Multi-Modal Learning with Large-Scale Pre-Trained Models"></a>Improving Discriminative Multi-Modal Learning with Large-Scale Pre-Trained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05193">http://arxiv.org/abs/2310.05193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenzhuang Du, Yue Zhao, Chonghua Liao, Jiacheng You, Jie Fu, Hang Zhao</li>
<li>for: 这种研究旨在更好地利用大规模预训练的uni-modal模型，以提高多模态学习的表现。</li>
<li>methods: 这种方法使用预训练的uni-modal模型，并将其作为初始模型进行多模态联合训练，以增强模式之间的适应性。</li>
<li>results: 研究表明，这种方法可以提高多模态模型的总表现，特别是在一些任务中，even when fine-tuned with only uni-modal data。<details>
<summary>Abstract</summary>
This paper investigates how to better leverage large-scale pre-trained uni-modal models to further enhance discriminative multi-modal learning. Even when fine-tuned with only uni-modal data, these models can outperform previous multi-modal models in certain tasks. It's clear that their incorporation into multi-modal learning would significantly improve performance. However, multi-modal learning with these models still suffers from insufficient learning of uni-modal features, which weakens the resulting multi-modal model's generalization ability. While fine-tuning uni-modal models separately and then aggregating their predictions is straightforward, it doesn't allow for adequate adaptation between modalities, also leading to sub-optimal results. To this end, we introduce Multi-Modal Low-Rank Adaptation learning (MMLoRA). By freezing the weights of uni-modal fine-tuned models, adding extra trainable rank decomposition matrices to them, and subsequently performing multi-modal joint training, our method enhances adaptation between modalities and boosts overall performance. We demonstrate the effectiveness of MMLoRA on three dataset categories: audio-visual (e.g., AVE, Kinetics-Sound, CREMA-D), vision-language (e.g., MM-IMDB, UPMC Food101), and RGB-Optical Flow (UCF101).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HOD-A-Benchmark-Dataset-for-Harmful-Object-Detection"><a href="#HOD-A-Benchmark-Dataset-for-Harmful-Object-Detection" class="headerlink" title="HOD: A Benchmark Dataset for Harmful Object Detection"></a>HOD: A Benchmark Dataset for Harmful Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05192">http://arxiv.org/abs/2310.05192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/poori-nuna/hod-benchmark-dataset">https://github.com/poori-nuna/hod-benchmark-dataset</a></li>
<li>paper_authors: Eungyeom Ha, Heemook Kim, Sung Chul Hong, Dongbin Na<br>for: 这个论文的目标是开发自动识别危险内容的系统，以防止在在线服务平台上传播危险内容。methods: 这个研究使用了最新的计算机视觉技术，包括使用最新的对象检测架构和大量的数据集来训练模型。results: 研究人员通过实验表明，使用提议的数据集和方法可以准确地检测在线服务平台上的危险内容，并且可以在实时应用中提供有效的识别结果。<details>
<summary>Abstract</summary>
Recent multi-media data such as images and videos have been rapidly spread out on various online services such as social network services (SNS). With the explosive growth of online media services, the number of image content that may harm users is also growing exponentially. Thus, most recent online platforms such as Facebook and Instagram have adopted content filtering systems to prevent the prevalence of harmful content and reduce the possible risk of adverse effects on users. Unfortunately, computer vision research on detecting harmful content has not yet attracted attention enough. Users of each platform still manually click the report button to recognize patterns of harmful content they dislike when exposed to harmful content. However, the problem with manual reporting is that users are already exposed to harmful content. To address these issues, our research goal in this work is to develop automatic harmful object detection systems for online services. We present a new benchmark dataset for harmful object detection. Unlike most related studies focusing on a small subset of object categories, our dataset addresses various categories. Specifically, our proposed dataset contains more than 10,000 images across 6 categories that might be harmful, consisting of not only normal cases but also hard cases that are difficult to detect. Moreover, we have conducted extensive experiments to evaluate the effectiveness of our proposed dataset. We have utilized the recently proposed state-of-the-art (SOTA) object detection architectures and demonstrated our proposed dataset can be greatly useful for the real-time harmful object detection task. The whole source codes and datasets are publicly accessible at https://github.com/poori-nuna/HOD-Benchmark-Dataset.
</details>
<details>
<summary>摘要</summary>
近年来多媒体数据如图片和视频在不同的在线服务平台上迅速扩散，如社交媒体服务（SNS）。随着在线媒体服务的快速发展，具有可能伤害用户的图像内容的数量也在增长 exponentially。因此，现代在线平台如Facebook和Instagram已经采用内容筛选系统来防止有害内容的普及和减少可能的用户伤害的风险。然而，计算机视觉研究检测有害内容还没有吸引到够多的关注。用户们仍然通过手动报告按钮来认识他们看到的有害内容。然而，手动报告的问题在于用户已经曝露在有害内容中。为解决这些问题，我们在这项工作中的研究目标是开发自动检测有害对象系统。我们提出了一个新的比较 dataset，与大多数相关研究一样，我们的 dataset 覆盖了多个对象类别，并且包含了超过 10,000 个图像，这些图像包括不只是正常情况，还有一些困难检测的情况。此外，我们进行了广泛的实验，以评估我们提出的 dataset 的有效性。我们使用了最新的 state-of-the-art 对象检测架构，并证明了我们的 dataset 可以在实时有害对象检测任务中具有很高的有用性。整个源代码和数据集都可以在 <https://github.com/poori-nuna/HOD-Benchmark-Dataset> 上公开访问。
</details></li>
</ul>
<hr>
<h2 id="AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition"><a href="#AANet-Aggregation-and-Alignment-Network-with-Semi-hard-Positive-Sample-Mining-for-Hierarchical-Place-Recognition" class="headerlink" title="AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition"></a>AANet: Aggregation and Alignment Network with Semi-hard Positive Sample Mining for Hierarchical Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05184">http://arxiv.org/abs/2310.05184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Lu-Feng/AANet">https://github.com/Lu-Feng/AANet</a></li>
<li>paper_authors: Feng Lu, Lijun Zhang, Shuting Dong, Baifan Chen, Chun Yuan<br>for:* 这种paper是为了提出一种高效的视觉场景识别方法，用于Robotics中的位置定位。methods:* 该方法使用了两个阶段的 hierarchical two-stage VPR 方法，首先使用全局特征进行全局搜索，然后使用本地特征进行再排序。* 该方法还提出了一种 Dynamically Aligning Local Features (DALF) 算法，用于在空间约束下对本地特征进行对齐。results:* 对四个常用的 VPR 数据集进行了广泛的实验，结果显示，提出的 AANet 可以比一些现有的状态作准的方法更高效，同时占用的时间更少。<details>
<summary>Abstract</summary>
Visual place recognition (VPR) is one of the research hotspots in robotics, which uses visual information to locate robots. Recently, the hierarchical two-stage VPR methods have become popular in this field due to the trade-off between accuracy and efficiency. These methods retrieve the top-k candidate images using the global features in the first stage, then re-rank the candidates by matching the local features in the second stage. However, they usually require additional algorithms (e.g. RANSAC) for geometric consistency verification in re-ranking, which is time-consuming. Here we propose a Dynamically Aligning Local Features (DALF) algorithm to align the local features under spatial constraints. It is significantly more efficient than the methods that need geometric consistency verification. We present a unified network capable of extracting global features for retrieving candidates via an aggregation module and aligning local features for re-ranking via the DALF alignment module. We call this network AANet. Meanwhile, many works use the simplest positive samples in triplet for weakly supervised training, which limits the ability of the network to recognize harder positive pairs. To address this issue, we propose a Semi-hard Positive Sample Mining (ShPSM) strategy to select appropriate hard positive images for training more robust VPR networks. Extensive experiments on four benchmark VPR datasets show that the proposed AANet can outperform several state-of-the-art methods with less time consumption. The code is released at https://github.com/Lu-Feng/AANet.
</details>
<details>
<summary>摘要</summary>
Visual地位识别（VPR）是机器人学研究的热点之一，它利用视觉信息来定位机器人。近年来，层次分解两个阶段VPR方法在这个领域得到了广泛应用，因为它们可以平衡精度和效率。这些方法首先使用全局特征来检索top-k候选图像，然后在第二阶段使用本地特征进行重新排序。然而，它们通常需要额外的算法（例如RANSAC）来验证几何一致性，这会占用大量时间。我们提出了一种 Dinamically Aligning Local Features（DALF）算法，可以在空间约束下对本地特征进行对齐。与需要几何一致性验证的方法相比，它更高效。我们提出了一种能够提取全局特征并对本地特征进行对齐的网络，我们称之为AANet。另外，许多工作使用最简单的三个图像作为弱有监督训练的正例，这限制了网络的识别能力。为了解决这个问题，我们提出了一种 Semi-hard Positive Sample Mining（ShPSM）策略，可以选择适当的困难正例图像进行训练更加 Robust VPR 网络。我们在四个常用的VPR数据集上进行了广泛的实验，结果显示，我们的AANet可以超过一些状态OF-the-art方法，并且占用更少的时间。代码可以在https://github.com/Lu-Feng/AANet上下载。
</details></li>
</ul>
<hr>
<h2 id="ITRE-Low-light-Image-Enhancement-Based-on-Illumination-Transmission-Ratio-Estimation"><a href="#ITRE-Low-light-Image-Enhancement-Based-on-Illumination-Transmission-Ratio-Estimation" class="headerlink" title="ITRE: Low-light Image Enhancement Based on Illumination Transmission Ratio Estimation"></a>ITRE: Low-light Image Enhancement Based on Illumination Transmission Ratio Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05158">http://arxiv.org/abs/2310.05158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Yihong Wang, Tong Liu, Xiubao Sui, Qian Chen</li>
<li>for: 提高低光照图像的品质</li>
<li>methods: 使用Retinex方法，包括分色域聚类、初始照明传输矩阵计算、基础模型生成和终端检查等步骤，以避免噪声、 artifacts 和过度曝光</li>
<li>results: 对比state-of-the-art方法，本方法在降低噪声、避免 artifacts、控制曝光水平方面具有优越的表现<details>
<summary>Abstract</summary>
Noise, artifacts, and over-exposure are significant challenges in the field of low-light image enhancement. Existing methods often struggle to address these issues simultaneously. In this paper, we propose a novel Retinex-based method, called ITRE, which suppresses noise and artifacts from the origin of the model, prevents over-exposure throughout the enhancement process. Specifically, we assume that there must exist a pixel which is least disturbed by low light within pixels of same color. First, clustering the pixels on the RGB color space to find the Illumination Transmission Ratio (ITR) matrix of the whole image, which determines that noise is not over-amplified easily. Next, we consider ITR of the image as the initial illumination transmission map to construct a base model for refined transmission map, which prevents artifacts. Additionally, we design an over-exposure module that captures the fundamental characteristics of pixel over-exposure and seamlessly integrate it into the base model. Finally, there is a possibility of weak enhancement when inter-class distance of pixels with same color is too small. To counteract this, we design a Robust-Guard module that safeguards the robustness of the image enhancement process. Extensive experiments demonstrate the effectiveness of our approach in suppressing noise, preventing artifacts, and controlling over-exposure level simultaneously. Our method performs superiority in qualitative and quantitative performance evaluations by comparing with state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
噪声、artefacts和过度曝光是低光照图像增强领域中的主要挑战。现有方法 oftentimes 难以同时解决这些问题。在这篇论文中，我们提出了一种新的Retinex基于的方法，称为ITRE，该方法可以在图像增强过程中对噪声和artefacts进行控制，同时避免过度曝光。具体来说，我们假设在图像中存在一个最少受到低光照的像素，我们可以通过RGB色彩空间的聚类来找到整个图像的照明传输矩阵（ITR），该矩阵确定了噪声不易过度增强。接着，我们将ITR矩阵作为图像的初始照明传输地图，并将其用于构建基本模型，以避免artefacts。此外，我们还设计了一个过度曝光模块，该模块可以融合到基本模型中，以捕捉图像过度曝光的基本特征。最后，当相同色彩的像素间距离过小时，可能会出现弱化效果。为了解决这个问题，我们设计了一个Robust-Guard模块，以保证图像增强过程的稳定性。广泛的实验表明，我们的方法可以同时控制噪声、artefacts和过度曝光水平，并且在质量和量化性能评价中表现出优于状态艺术方法。
</details></li>
</ul>
<hr>
<h2 id="LocoNeRF-A-NeRF-based-Approach-for-Local-Structure-from-Motion-for-Precise-Localization"><a href="#LocoNeRF-A-NeRF-based-Approach-for-Local-Structure-from-Motion-for-Precise-Localization" class="headerlink" title="LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization"></a>LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05134">http://arxiv.org/abs/2310.05134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Artem Nenashev, Mikhail Kurenkov, Andrei Potapov, Iana Zhura, Maksim Katerishich, Dzmitry Tsetserukou</li>
<li>for: 提高视觉定位精度， addresses the limitations of global SfM and the challenges of local SfM.</li>
<li>methods: 使用Neural Radiance Fields (NeRF) instead of image databases for storage, and sampling reference images around the prior query position for further improvements.</li>
<li>results: 比ground truth有0.068米的准确性，但是数据库大小减少至160MB，比COLMAP的400MB有所降低。 Additionally, the ablation study shows the impact of using reference images from the NeRF reconstruction.<details>
<summary>Abstract</summary>
Visual localization is a critical task in mobile robotics, and researchers are continuously developing new approaches to enhance its efficiency. In this article, we propose a novel approach to improve the accuracy of visual localization using Structure from Motion (SfM) techniques. We highlight the limitations of global SfM, which suffers from high latency, and the challenges of local SfM, which requires large image databases for accurate reconstruction. To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as opposed to image databases, to cut down on the space required for storage. We suggest that sampling reference images around the prior query position can lead to further improvements. We evaluate the accuracy of our proposed method against ground truth obtained using LIDAR and Advanced Lidar Odometry and Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM with COLMAP in the conducted experiments. Our proposed method achieves an accuracy of 0.068 meters compared to the ground truth, which is slightly lower than the most advanced method COLMAP, which has an accuracy of 0.022 meters. However, the size of the database required for COLMAP is 400 megabytes, whereas the size of our NeRF model is only 160 megabytes. Finally, we perform an ablation study to assess the impact of using reference images from the NeRF reconstruction.
</details>
<details>
<summary>摘要</summary>
“视觉本地化是移动 роботика中的一项关键任务，研究人员不断开发新的方法来提高其效率。在这篇文章中，我们提出一种新的方法来提高视觉本地化的准确性，使用结构从运动（SfM）技术。我们指出了全球SfM的局限性，即高延迟，以及本地SfM的挑战，即需要大量图像数据库以实现准确重建。为了解决这些问题，我们提议使用神经辐射场（NeRF），而不是图像数据库，来减少存储空间。我们建议在提前查询位置附近采样参考图像可以获得进一步改进。我们对我们提议的方法与实际数据进行评估，并与使用COLMAP的本地SfM进行比较。我们的方法准确性为0.068米，轻微低于最先进的方法COLMAP准确性（0.022米）。然而，COLMAP需要400兆字节的数据库，而我们的NeRF模型只需160兆字节。最后，我们进行了一个ablation研究，以评估使用NeRF重建中的参考图像的影响。”
</details></li>
</ul>
<hr>
<h2 id="Geometry-Aware-Field-to-field-Transformations-for-3D-Semantic-Segmentation"><a href="#Geometry-Aware-Field-to-field-Transformations-for-3D-Semantic-Segmentation" class="headerlink" title="Geometry Aware Field-to-field Transformations for 3D Semantic Segmentation"></a>Geometry Aware Field-to-field Transformations for 3D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05133">http://arxiv.org/abs/2310.05133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/USTCPCS/CVPR2018_attention">https://github.com/USTCPCS/CVPR2018_attention</a></li>
<li>paper_authors: Dominik Hollidt, Clinton Wang, Polina Golland, Marc Pollefeys</li>
<li>for: 实现3D内容Semantic Segmentation仅基于2D监控，使用神经辉照场（NeRF）。</li>
<li>methods: 通过获取表面点云的特征，实现了几何簇的储存，实现了3D理解。通过伪类型自动编码学习，实现了几何簇的数据储存。</li>
<li>results: 获得了几何簇的储存，并且可以实现几何簇的3D理解。<details>
<summary>Abstract</summary>
We present a novel approach to perform 3D semantic segmentation solely from 2D supervision by leveraging Neural Radiance Fields (NeRFs). By extracting features along a surface point cloud, we achieve a compact representation of the scene which is sample-efficient and conducive to 3D reasoning. Learning this feature space in an unsupervised manner via masked autoencoding enables few-shot segmentation. Our method is agnostic to the scene parameterization, working on scenes fit with any type of NeRF.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，可以通过使用神经辐射场（NeRF）来完成3Dsemantic segmentation，仅基于2D监督。我们通过对表面点云中提取特征来获得一个减少的表示形式，该形式是效率的样本和适合3D理解。通过在掩码自动编码中学习这个特征空间，我们实现了几个shot分类。我们的方法对于场景参数化是无关的，可以应用于任何类型的NeRF场景。Note: "神经辐射场" (NeRF) is a Chinese term that refers to Neural Radiance Fields.
</details></li>
</ul>
<hr>
<h2 id="Bidirectional-Knowledge-Reconfiguration-for-Lightweight-Point-Cloud-Analysis"><a href="#Bidirectional-Knowledge-Reconfiguration-for-Lightweight-Point-Cloud-Analysis" class="headerlink" title="Bidirectional Knowledge Reconfiguration for Lightweight Point Cloud Analysis"></a>Bidirectional Knowledge Reconfiguration for Lightweight Point Cloud Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05125">http://arxiv.org/abs/2310.05125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peipei Li, Xing Cui, Yibo Hu, Man Zhang, Ting Yao, Tao Mei</li>
<li>for: 本研究旨在提高点云分析的计算机系统过载问题，使其可以在移动或边缘设备上应用。</li>
<li>methods: 本文提出了特征缩减技术来降低点云模型的计算负担。 Specifically, we propose bidirectional knowledge reconfiguration (BKR) to distill informative contextual knowledge from the teacher to the student.</li>
<li>results: 我们的方法在shape classification、part segmentation和semantic segmentation benchmarks上表现出了超越性和优势，demonstrating the universality and superiority of our method.<details>
<summary>Abstract</summary>
Point cloud analysis faces computational system overhead, limiting its application on mobile or edge devices. Directly employing small models may result in a significant drop in performance since it is difficult for a small model to adequately capture local structure and global shape information simultaneously, which are essential clues for point cloud analysis. This paper explores feature distillation for lightweight point cloud models. To mitigate the semantic gap between the lightweight student and the cumbersome teacher, we propose bidirectional knowledge reconfiguration (BKR) to distill informative contextual knowledge from the teacher to the student. Specifically, a top-down knowledge reconfiguration and a bottom-up knowledge reconfiguration are developed to inherit diverse local structure information and consistent global shape knowledge from the teacher, respectively. However, due to the farthest point sampling in most point cloud models, the intermediate features between teacher and student are misaligned, deteriorating the feature distillation performance. To eliminate it, we propose a feature mover's distance (FMD) loss based on optimal transportation, which can measure the distance between unordered point cloud features effectively. Extensive experiments conducted on shape classification, part segmentation, and semantic segmentation benchmarks demonstrate the universality and superiority of our method.
</details>
<details>
<summary>摘要</summary>
点云分析面临计算系统开销限制其在移动或边缘设备上应用。直接采用小型模型可能会导致显著性能下降，因为小型模型很难同时捕捉点云中的本地结构和全局形态信息，这些信息是点云分析的关键决定因素。本文探讨了降简点云模型的技术。为了减少教师和学生之间的Semantic gap，我们提出了双向知识重新配置（BKR），将教师知识中的有用Contextual information遗传给学生。具体来说，我们开发了从教师到学生的顶部知识重新配置和从学生到教师的底部知识重新配置，以继承教师的多样化本地结构信息和一致的全局形态知识。然而，由于多数点云模型中的远点抽样，学生和教师之间的中间特征不对Alignment，这会降低feature distillation的性能。为了解决这个问题，我们提出了基于最优运输的特征移动距离（FMD）损失，可以有效度量不同点云特征之间的距离。我们对shape classification、部分 segmentation和semantic segmentation benchmark进行了广泛的实验，结果表明我们的方法在 universality 和优势性方面具有出色的表现。
</details></li>
</ul>
<hr>
<h2 id="Cross-domain-Robust-Deepfake-Bias-Expansion-Network-for-Face-Forgery-Detection"><a href="#Cross-domain-Robust-Deepfake-Bias-Expansion-Network-for-Face-Forgery-Detection" class="headerlink" title="Cross-domain Robust Deepfake Bias Expansion Network for Face Forgery Detection"></a>Cross-domain Robust Deepfake Bias Expansion Network for Face Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05124">http://arxiv.org/abs/2310.05124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weihua Liu, Lin Li, Chaochao Lin, Said Boumaraf<br>for: 这篇论文旨在提高人脸假制检测的安全性，尤其是面对深度推假技术的威胁。methods: 本论文提出了一种解决方案——跨频率坚定偏差扩展网络（BENet），通过使用自动编码器重建输入face，保持真实face的均衡性，同时选择性地增强假face与其原始样本之间的差异。results: 对比于现有方法，BENet在内部和跨频率测试中表现出色，能够有效地检测深度推假face。此外，BENet还 incorporates 一种矩阵注意力（LSA）模块，可以更好地捕捉异常的假制特征。<details>
<summary>Abstract</summary>
The rapid advancement of deepfake technologies raises significant concerns about the security of face recognition systems. While existing methods leverage the clues left by deepfake techniques for face forgery detection, malicious users may intentionally manipulate forged faces to obscure the traces of deepfake clues and thereby deceive detection tools. Meanwhile, attaining cross-domain robustness for data-based methods poses a challenge due to potential gaps in the training data, which may not encompass samples from all relevant domains. Therefore, in this paper, we introduce a solution - a Cross-Domain Robust Bias Expansion Network (BENet) - designed to enhance face forgery detection. BENet employs an auto-encoder to reconstruct input faces, maintaining the invariance of real faces while selectively enhancing the difference between reconstructed fake faces and their original counterparts. This enhanced bias forms a robust foundation upon which dependable forgery detection can be built. To optimize the reconstruction results in BENet, we employ a bias expansion loss infused with contrastive concepts to attain the aforementioned objective. In addition, to further heighten the amplification of forged clues, BENet incorporates a Latent-Space Attention (LSA) module. This LSA module effectively captures variances in latent features between the auto-encoder's encoder and decoder, placing emphasis on inconsistent forgery-related information. Furthermore, BENet incorporates a cross-domain detector with a threshold to determine whether the sample belongs to a known distribution. The correction of classification results through the cross-domain detector enables BENet to defend against unknown deepfake attacks from cross-domain. Extensive experiments demonstrate the superiority of BENet compared with state-of-the-art methods in intra-database and cross-database evaluations.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The rapid advancement of deepfake technologies raises significant concerns about the security of face recognition systems. While existing methods leverage the clues left by deepfake techniques for face forgery detection, malicious users may intentionally manipulate forged faces to obscure the traces of deepfake clues and thereby deceive detection tools. Meanwhile, attaining cross-domain robustness for data-based methods poses a challenge due to potential gaps in the training data, which may not encompass samples from all relevant domains. Therefore, in this paper, we introduce a solution - a Cross-Domain Robust Bias Expansion Network (BENet) - designed to enhance face forgery detection. BENet employs an auto-encoder to reconstruct input faces, maintaining the invariance of real faces while selectively enhancing the difference between reconstructed fake faces and their original counterparts. This enhanced bias forms a robust foundation upon which dependable forgery detection can be built. To optimize the reconstruction results in BENet, we employ a bias expansion loss infused with contrastive concepts to attain the aforementioned objective. In addition, to further heighten the amplification of forged clues, BENet incorporates a Latent-Space Attention (LSA) module. This LSA module effectively captures variances in latent features between the auto-encoder's encoder and decoder, placing emphasis on inconsistent forgery-related information. Furthermore, BENet incorporates a cross-domain detector with a threshold to determine whether the sample belongs to a known distribution. The correction of classification results through the cross-domain detector enables BENet to defend against unknown deepfake attacks from cross-domain. Extensive experiments demonstrate the superiority of BENet compared with state-of-the-art methods in intra-database and cross-database evaluations." into Simplified Chinese.Here's the translation:“深刻的深伪技术的快速发展，对人脸识别系统的安全带来重要的忧虑。现有的方法可以利用深伪技术留下的伪证据来检测伪证，但是黑客可能会故意修改伪证，以隐藏深伪的迹象，并且欺骗检测工具。同时，为了实现跨领域Robustness，资料基础方法面临着潜在的领域差异问题，这些训练数据可能不包括所有 relevance 的领域。因此，在这篇论文中，我们提出了一个解决方案——跨领域Robust Bias Expansion Network（BENet），用于增强伪证检测。BENet 使用 auto-encoder 重建输入的脸部，保持真实脸部的不变性，同时选择性地强化伪证的重建和原始对比。这个强化的偏见形成了可靠的基础，以便建立可靠的伪证检测。为了优化 BENet 中的重建结果，我们使用了偏见扩展损失，融合了相对概念，以达到这个目标。此外，BENet 还包括一个 Latent-Space Attention（LSA）模组，这个模组可以有效地捕捉 auto-encoder 的Encoder 和 Decoder 之间的潜在特征差异，将注意力集中在伪证相关的不一致信息上。此外，BENet 还包括一个跨领域检测器，以决定样本是否属于已知分布。通过跨领域检测器进行样本的修正，使得 BENet 能够防止不知道的深伪攻击。实验结果显示，BENet 与现有的方法相比，在内部资料和跨部资料评估中表现出色。”
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Multi-Domain-Knowledge-Networks-for-Chest-X-ray-Report-Generation"><a href="#Dynamic-Multi-Domain-Knowledge-Networks-for-Chest-X-ray-Report-Generation" class="headerlink" title="Dynamic Multi-Domain Knowledge Networks for Chest X-ray Report Generation"></a>Dynamic Multi-Domain Knowledge Networks for Chest X-ray Report Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05119">http://arxiv.org/abs/2310.05119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weihua Liu, Youyuan Xue, Chaochao Lin, Said Boumaraf</li>
<li>for: This paper aims to address the challenges of automatically generating radiology diagnostic reports, particularly the imbalance in data distribution between normal and abnormal samples, by proposing a Dynamic Multi-Domain Knowledge (DMDK) network.</li>
<li>methods: The proposed DMDK network consists of four modules: Chest Feature Extractor (CFE), Dynamic Knowledge Extractor (DKE), Specific Knowledge Extractor (SKE), and Multi-knowledge Integrator (MKI) module. The network utilizes dynamic disease topic labels, domain-specific dynamic knowledge graphs, and multi-knowledge integration to mitigate data biases and enhance interpretability.</li>
<li>results: The proposed method was extensively evaluated on two widely used datasets (IU X-Ray and MIMIC-CXR) and achieved state-of-the-art performance in all evaluation metrics, outperforming previous models.<details>
<summary>Abstract</summary>
The automated generation of radiology diagnostic reports helps radiologists make timely and accurate diagnostic decisions while also enhancing clinical diagnostic efficiency. However, the significant imbalance in the distribution of data between normal and abnormal samples (including visual and textual biases) poses significant challenges for a data-driven task like automatically generating diagnostic radiology reports. Therefore, we propose a Dynamic Multi-Domain Knowledge(DMDK) network for radiology diagnostic report generation. The DMDK network consists of four modules: Chest Feature Extractor(CFE), Dynamic Knowledge Extractor(DKE), Specific Knowledge Extractor(SKE), and Multi-knowledge Integrator(MKI) module. Specifically, the CFE module is primarily responsible for extracting the unprocessed visual medical features of the images. The DKE module is responsible for extracting dynamic disease topic labels from the retrieved radiology diagnostic reports. We then fuse the dynamic disease topic labels with the original visual features of the images to highlight the abnormal regions in the original visual features to alleviate the visual data bias problem. The SKE module expands upon the conventional static knowledge graph to mitigate textual data biases and amplify the interpretability capabilities of the model via domain-specific dynamic knowledge graphs. The MKI distills all the knowledge and generates the final diagnostic radiology report. We performed extensive experiments on two widely used datasets, IU X-Ray and MIMIC-CXR. The experimental results demonstrate the effectiveness of our method, with all evaluation metrics outperforming previous state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
自动生成 radiology 诊断报告可以帮助 radiologist 更快、更准确地作出诊断决策，同时提高临床诊断效率。然而，数据分布的巨大偏度问题（包括视觉和文本偏见）对于数据驱动的任务如自动生成 radiology 诊断报告来说是一个挑战。因此，我们提出了动态多Domain知识（DMDK）网络，用于 radiology 诊断报告生成。DMDK 网络由四个模块组成：胸部特征提取器（CFE）、动态知识提取器（DKE）、特定知识提取器（SKE）和多知识 интегратор（MKI）模块。具体来说，CFE 模块主要负责从未处理的医学影像中提取视觉特征。DKE 模块负责从检索到的 radiology 诊断报告中提取动态疾病话题标签。我们将这些动态疾病话题标签与原始视觉特征相结合，以减少视觉数据偏见问题。SKE 模块在传统的静止知识图中增强了文本数据偏见问题，并通过域pecific的动态知识图来提高模型的解释能力。MKI 模块将所有的知识integrirated，并生成最终的 radiology 诊断报告。我们在 IU X-Ray 和 MIMIC-CXR 两个广泛使用的数据集上进行了广泛的实验，结果表明我们的方法效果很高，所有评价指标都高于之前的状态对照模型。
</details></li>
</ul>
<hr>
<h2 id="Lightweight-In-Context-Tuning-for-Multimodal-Unified-Models"><a href="#Lightweight-In-Context-Tuning-for-Multimodal-Unified-Models" class="headerlink" title="Lightweight In-Context Tuning for Multimodal Unified Models"></a>Lightweight In-Context Tuning for Multimodal Unified Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05109">http://arxiv.org/abs/2310.05109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Chen, Shuai Zhang, Boran Han, Jiaya Jia</li>
<li>For: The paper aims to address the challenges of in-context learning (ICL) in multimodal tasks, specifically the difficulty of extrapolating from contextual examples to perform ICL as more modalities are added.* Methods: The proposed solution is called MultiModal In-conteXt Tuning (M$^2$IXT), a lightweight module that incorporates an expandable context window to incorporate various labeled examples of multiple modalities. The module can be prepended to various multimodal unified models and trained via a mixed-tasks strategy to enable rapid few-shot adaption on multiple tasks and datasets.* Results: The paper shows that M$^2$IXT can significantly boost the few-shot ICL performance (e.g., 18% relative increase for OFA) and achieve state-of-the-art results across various tasks, including visual question answering, image captioning, visual grounding, and visual entailment, while being considerably small in terms of model parameters.<details>
<summary>Abstract</summary>
In-context learning (ICL) involves reasoning from given contextual examples. As more modalities comes, this procedure is becoming more challenging as the interleaved input modalities convolutes the understanding process. This is exemplified by the observation that multimodal models often struggle to effectively extrapolate from contextual examples to perform ICL. To address these challenges, we introduce MultiModal In-conteXt Tuning (M$^2$IXT), a lightweight module to enhance the ICL capabilities of multimodal unified models. The proposed M$^2$IXT module perceives an expandable context window to incorporate various labeled examples of multiple modalities (e.g., text, image, and coordinates). It can be prepended to various multimodal unified models (e.g., OFA, Unival, LLaVA) of different architectures and trained via a mixed-tasks strategy to enable rapid few-shot adaption on multiple tasks and datasets. When tuned on as little as 50K multimodal data, M$^2$IXT can boost the few-shot ICL performance significantly (e.g., 18\% relative increase for OFA), and obtained state-of-the-art results across an array of tasks including visual question answering, image captioning, visual grounding, and visual entailment, while being considerably small in terms of model parameters (e.g., $\sim$$20\times$ smaller than Flamingo or MMICL), highlighting the flexibility and effectiveness of M$^2$IXT as a multimodal in-context learner.
</details>
<details>
<summary>摘要</summary>
宽 Context 学习（ICL） involve 从Contextual例子进行推理。随着更多modalities来临，这个过程变得更加挑战，因为交错的输入modalities会混淆理解过程。这被示示于多modal模型在 Contextual例子上进行ICL时的表现不佳。为了解决这些挑战，我们介绍 MultiModal In-conteXt Tuning（M$^2$IXT）模块，用于提高多modal unified模型的ICL能力。该模块可以预pend于多modal unified模型（例如OFA、Unival、LLaVA）的不同架构上，并通过混合任务策略进行训练，以实现快速少量数据适应。当与50000个多modal数据进行训练时，M$^2$IXT可以显著提高ICL性能（例如18%相对提高），并在视觉问答、图像描述、视觉固定、视觉包容等任务上达到了状态之册的结果，而且模型参数很小（例如$\sim$$20\times$ smaller than Flamingo或MMICL），这说明M$^2$IXT具有多modal Contextual学习的灵活性和效果。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Representations-through-Heterogeneous-Self-Supervised-Learning"><a href="#Enhancing-Representations-through-Heterogeneous-Self-Supervised-Learning" class="headerlink" title="Enhancing Representations through Heterogeneous Self-Supervised Learning"></a>Enhancing Representations through Heterogeneous Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05108">http://arxiv.org/abs/2310.05108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhong-Yu Li, Bo-Wen Yin, Shanghua Gao, Yongxiang Liu, Li Liu, Ming-Ming Cheng</li>
<li>for: 提高自我超vised学习中基础模型的表示质量，通过在基础模型上添加不同架构的辅助头来增强表示能力。</li>
<li>methods: 提出了一种基于不同架构的辅助头的自我超vised学习方法（HSSL），通过让基础模型学习辅助头的不同架构来增强表示能力，而不需要Structural changes。</li>
<li>results: 通过多种不同的基础模型和辅助头组合，对多个下游任务（包括图像分类、 semantic segmentation、instance segmentation、object detection）进行了实验，并发现了基础模型的表示质量随着辅助头的架构差异增加而提高。此外，还提出了一种快速找到最适合基础模型学习的辅助头的搜索策略和一些简单 yet effective的方法来扩大模型差异。<details>
<summary>Abstract</summary>
Incorporating heterogeneous representations from different architectures has facilitated various vision tasks, e.g., some hybrid networks combine transformers and convolutions. However, complementarity between such heterogeneous architectures has not been well exploited in self-supervised learning. Thus, we propose Heterogeneous Self-Supervised Learning (HSSL), which enforces a base model to learn from an auxiliary head whose architecture is heterogeneous from the base model. In this process, HSSL endows the base model with new characteristics in a representation learning way without structural changes. To comprehensively understand the HSSL, we conduct experiments on various heterogeneous pairs containing a base model and an auxiliary head. We discover that the representation quality of the base model moves up as their architecture discrepancy grows. This observation motivates us to propose a search strategy that quickly determines the most suitable auxiliary head for a specific base model to learn and several simple but effective methods to enlarge the model discrepancy. The HSSL is compatible with various self-supervised methods, achieving superior performances on various downstream tasks, including image classification, semantic segmentation, instance segmentation, and object detection. Our source code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
将不同架构的表示结合在一起已经提高了许多视觉任务的性能，例如混合网络将转换器和卷积结合使用。然而，这些不同架构之间的补做性未得到了自我超vised学习中的充分利用。因此，我们提出了多样化自我超vised学习（HSSL），它要求基本模型从auxiliary头中学习，auxiliary头的架构与基本模型不同。在这个过程中，HSSL使得基本模型学习新的特征，不需要结构性改变。为了全面了解HSSL，我们在不同的 heterogeneous对中进行了实验，发现当基本模型和auxiliary头的架构差异增大时，基本模型的表示质量会提高。这一观察使我们提出了一种快速找到最适合基本模型学习的auxiliary头的搜索策略，以及一些简单 yet effective的方法来扩大模型差异。HSSL可以与多种自我超vised方法结合使用，在多种下游任务上达到了更高的性能，包括图像分类、 semantic segmentation、实例 segmentation和对象检测。我们将代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="OV-PARTS-Towards-Open-Vocabulary-Part-Segmentation"><a href="#OV-PARTS-Towards-Open-Vocabulary-Part-Segmentation" class="headerlink" title="OV-PARTS: Towards Open-Vocabulary Part Segmentation"></a>OV-PARTS: Towards Open-Vocabulary Part Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05107">http://arxiv.org/abs/2310.05107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openrobotlab/ov_parts">https://github.com/openrobotlab/ov_parts</a></li>
<li>paper_authors: Meng Wei, Xiaoyu Yue, Wenwei Zhang, Shu Kong, Xihui Liu, Jiangmiao Pang</li>
<li>for: 本研究旨在提出一个开 vocabulary part segmentation（OV-PARTS）benchmark，以探索在实世界中具有多元定义的部分构成的挑战。</li>
<li>methods: 本研究使用了两个公开可用的数据集：Pascal-Part-116和ADE20K-Part-234，并提出了三个特定任务：通用零基准部分分类、跨数据集部分分类和少量基准部分分类，以探索模型对于不同定义的部分的数据分类能力。</li>
<li>results: 本研究通过实验分析了两种现有的物件水平OVSS方法的适用性，并提供了一个精确的数据集和代码，以便未来研究者可以在OV-PARTS领域进行更多的探索和创新。<details>
<summary>Abstract</summary>
Segmenting and recognizing diverse object parts is a crucial ability in applications spanning various computer vision and robotic tasks. While significant progress has been made in object-level Open-Vocabulary Semantic Segmentation (OVSS), i.e., segmenting objects with arbitrary text, the corresponding part-level research poses additional challenges. Firstly, part segmentation inherently involves intricate boundaries, while limited annotated data compounds the challenge. Secondly, part segmentation introduces an open granularity challenge due to the diverse and often ambiguous definitions of parts in the open world. Furthermore, the large-scale vision and language models, which play a key role in the open vocabulary setting, struggle to recognize parts as effectively as objects. To comprehensively investigate and tackle these challenges, we propose an Open-Vocabulary Part Segmentation (OV-PARTS) benchmark. OV-PARTS includes refined versions of two publicly available datasets: Pascal-Part-116 and ADE20K-Part-234. And it covers three specific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part Segmentation, and Few-Shot Part Segmentation, providing insights into analogical reasoning, open granularity and few-shot adapting abilities of models. Moreover, we analyze and adapt two prevailing paradigms of existing object-level OVSS methods for OV-PARTS. Extensive experimental analysis is conducted to inspire future research in leveraging foundational models for OV-PARTS. The code and dataset are available at https://github.com/OpenRobotLab/OV_PARTS.
</details>
<details>
<summary>摘要</summary>
Segmenting and recognizing diverse object parts is a crucial ability in various computer vision and robotic tasks. Although significant progress has been made in object-level Open-Vocabulary Semantic Segmentation (OVSS), segmenting objects with arbitrary text, the corresponding part-level research poses additional challenges. Firstly, part segmentation involves intricate boundaries, and limited annotated data makes it more challenging. Secondly, part segmentation introduces an open granularity challenge due to the diverse and often ambiguous definitions of parts in the open world. Moreover, large-scale vision and language models, which play a key role in the open vocabulary setting, struggle to recognize parts as effectively as objects.To comprehensively investigate and tackle these challenges, we propose an Open-Vocabulary Part Segmentation (OV-PARTS) benchmark. OV-PARTS includes refined versions of two publicly available datasets: Pascal-Part-116 and ADE20K-Part-234. It covers three specific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part Segmentation, and Few-Shot Part Segmentation, providing insights into analogical reasoning, open granularity, and few-shot adapting abilities of models. Moreover, we analyze and adapt two prevailing paradigms of existing object-level OVSS methods for OV-PARTS. Extensive experimental analysis is conducted to inspire future research in leveraging foundational models for OV-PARTS. The code and dataset are available at https://github.com/OpenRobotLab/OV_PARTS.
</details></li>
</ul>
<hr>
<h2 id="Cross-head-mutual-Mean-Teaching-for-semi-supervised-medical-image-segmentation"><a href="#Cross-head-mutual-Mean-Teaching-for-semi-supervised-medical-image-segmentation" class="headerlink" title="Cross-head mutual Mean-Teaching for semi-supervised medical image segmentation"></a>Cross-head mutual Mean-Teaching for semi-supervised medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05082">http://arxiv.org/abs/2310.05082</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leesoon1984/cmmt-net">https://github.com/leesoon1984/cmmt-net</a></li>
<li>paper_authors: Wei Li, Ruifeng Bian, Wenyi Zhao, Weijin Xu, Huihua Yang</li>
<li>for: 提高 semi-supervised medical image segmentation 的精度和一致性</li>
<li>methods: 提出了一种新的 Cross-head mutual mean-teaching Network (CMMT-Net)，包括 teacher-student 师生网络和 pseudo label 生成等技术，以提高自教学和一致学习的性能</li>
<li>results: 实验结果显示，CMMT-Net 在三个公共可用的数据集上取得了前所未有的提高，在不同的 semi-supervised enario中均表现出色<details>
<summary>Abstract</summary>
Semi-supervised medical image segmentation (SSMIS) has witnessed substantial advancements by leveraging limited labeled data and abundant unlabeled data. Nevertheless, existing state-of-the-art (SOTA) methods encounter challenges in accurately predicting labels for the unlabeled data, giving rise to disruptive noise during training and susceptibility to erroneous information overfitting. Moreover, applying perturbations to inaccurate predictions further reduces consistent learning. To address these concerns, we propose a novel Cross-head mutual mean-teaching Network (CMMT-Net) incorporated strong-weak data augmentation, thereby benefitting both self-training and consistency learning. Specifically, our CMMT-Net consists of both teacher-student peer networks with a share encoder and dual slightly different decoders, and the pseudo labels generated by one mean teacher head are adopted to supervise the other student branch to achieve a mutual consistency. Furthermore, we propose mutual virtual adversarial training (MVAT) to smooth the decision boundary and enhance feature representations. To diversify the consistency training samples, we employ Cross-Set CutMix strategy, which also helps address distribution mismatch issues. Notably, CMMT-Net simultaneously implements data, feature, and network perturbations, amplifying model diversity and generalization performance. Experimental results on three publicly available datasets indicate that our approach yields remarkable improvements over previous SOTA methods across various semi-supervised scenarios. Code and logs will be available at https://github.com/Leesoon1984/CMMT-Net.
</details>
<details>
<summary>摘要</summary>
semi-supervised医学图像分割（SSMIS）在过去几年中取得了重大进步，通过利用有限的标注数据和庞大的无标注数据。然而，现有的状态之 искусственный智能（SOTA）方法在准确地预测无标注数据的标签时遇到了挑战，从而导致训练过程中的干扰和模型过拟合。此外，在应用扰动后，模型的学习不稳定。为解决这些问题，我们提出了一种新的交叉头同义启发网络（CMMT-Net），具有强弱数据增强和一致学习。具体来说，我们的CMMT-Net包括一个共享encoder和两个不同的decoder，其中一个用于生成 pseudo标签，另一个用于自我训练。此外，我们还提出了一种mutual virtual adversarial training（MVAT），以缓解决决策边界的问题，并提高特征表示。为了多样化一致学习样本，我们采用了 Cross-Set CutMix策略，这也有助于解决分布匹配问题。值得注意的是，CMMT-Net同时实现了数据、特征和网络扰动，从而扩大模型多样性和总体性能。我们的方法在三个公开的数据集上进行了实验，并取得了前所未有的提高。代码和日志将在https://github.com/Leesoon1984/CMMT-Net上提供。
</details></li>
</ul>
<hr>
<h2 id="Language-driven-Open-Vocabulary-Keypoint-Detection-for-Animal-Body-and-Face"><a href="#Language-driven-Open-Vocabulary-Keypoint-Detection-for-Animal-Body-and-Face" class="headerlink" title="Language-driven Open-Vocabulary Keypoint Detection for Animal Body and Face"></a>Language-driven Open-Vocabulary Keypoint Detection for Animal Body and Face</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05056">http://arxiv.org/abs/2310.05056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Kaipeng Zhang, Lumin Xu, Shenqi Lai, Wenqi Shao, Nanning Zheng, Ping Luo, Yu Qiao</li>
<li>for: 提出开放词汇键点检测（OVKD）任务，以用文本提示来ocalize任意种类的键点。</li>
<li>methods: 提出Open-Vocabulary Keypoint Detection with Semantic-feature Matching（KDSM）方法，利用视觉和语言模型来利用文本和视觉之间的关系，从而实现键点检测。</li>
<li>results: 实验表明，我们提出的组件带来了显著性能提升，而我们的总方法在OVKD中达到了非常出色的结果，甚至在零例学习方式下超过了现状卷积检测方法。<details>
<summary>Abstract</summary>
Current approaches for image-based keypoint detection on animal (including human) body and face are limited to specific keypoints and species. We address the limitation by proposing the Open-Vocabulary Keypoint Detection (OVKD) task. It aims to use text prompts to localize arbitrary keypoints of any species. To accomplish this objective, we propose Open-Vocabulary Keypoint Detection with Semantic-feature Matching (KDSM), which utilizes both vision and language models to harness the relationship between text and vision and thus achieve keypoint detection through associating text prompt with relevant keypoint features. Additionally, KDSM integrates domain distribution matrix matching and some special designs to reinforce the relationship between language and vision, thereby improving the model's generalizability and performance. Extensive experiments show that our proposed components bring significant performance improvements, and our overall method achieves impressive results in OVKD. Remarkably, our method outperforms the state-of-the-art few-shot keypoint detection methods using a zero-shot fashion. We will make the source code publicly accessible.
</details>
<details>
<summary>摘要</summary>
当前对人体和面部图像中的关键点检测方法受限于特定关键点和种类。我们解决这一限制，提出开放词汇关键点检测（OVKD）任务。该任务的目标是使用文本提示来确定任意种类的关键点。为 достичь这一目标，我们提出了开放词汇关键点检测与semantic特征匹配（KDSM）方法，该方法利用视觉和语言模型来利用视觉和文本之间的关系，从而通过文本提示与相关的关键点特征相匹配来实现关键点检测。此外，KDSM还 integrates域名分布矩阵匹配和一些特殊的设计，以强化语言和视觉之间的关系，从而提高模型的普适性和性能。我们的实验表明，我们提出的组件带来了显著的性能提升，而我们的总方法在OVKD中实现了卓越的成绩，并且在零shot模式下超越了现有的状态对抗方法。我们将将源代码公开访问。
</details></li>
</ul>
<hr>
<h2 id="FairTune-Optimizing-Parameter-Efficient-Fine-Tuning-for-Fairness-in-Medical-Image-Analysis"><a href="#FairTune-Optimizing-Parameter-Efficient-Fine-Tuning-for-Fairness-in-Medical-Image-Analysis" class="headerlink" title="FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis"></a>FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05055">http://arxiv.org/abs/2310.05055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raman Dutt, Ondrej Bohdal, Sotirios A. Tsaftaris, Timothy Hospedales</li>
<li>for: 这个论文目标是提高机器学习模型的鲁棒性和公平性，特别是在具有伦理敏感性的应用领域，如医学诊断。</li>
<li>methods: 这篇论文使用了两级优化方法来解决公平学习问题，即在验证集上优化学习策略以确保公平性，并在更新参数时考虑公平性。</li>
<li>results: 论文的实验结果表明，使用 FairTune 框架可以提高医学图像 datasets 上的公平性。<details>
<summary>Abstract</summary>
Training models with robust group fairness properties is crucial in ethically sensitive application areas such as medical diagnosis. Despite the growing body of work aiming to minimise demographic bias in AI, this problem remains challenging. A key reason for this challenge is the fairness generalisation gap: High-capacity deep learning models can fit all training data nearly perfectly, and thus also exhibit perfect fairness during training. In this case, bias emerges only during testing when generalisation performance differs across subgroups. This motivates us to take a bi-level optimisation perspective on fair learning: Optimising the learning strategy based on validation fairness. Specifically, we consider the highly effective workflow of adapting pre-trained models to downstream medical imaging tasks using parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between updating more parameters, enabling a better fit to the task of interest vs. fewer parameters, potentially reducing the generalisation gap. To manage this tradeoff, we propose FairTune, a framework to optimise the choice of PEFT parameters with respect to fairness. We demonstrate empirically that FairTune leads to improved fairness on a range of medical imaging datasets.
</details>
<details>
<summary>摘要</summary>
训练模型具有坚固的群体公平性质iels是在伦理敏感的应用领域，如医疗诊断，是非常重要的。despite the growing body of work aiming to minimize demographic bias in AI, this problem remains challenging. A key reason for this challenge is the fairness generalization gap: high-capacity deep learning models can fit all training data nearly perfectly, and thus also exhibit perfect fairness during training. In this case, bias emerges only during testing when generalization performance differs across subgroups. This motivates us to take a bi-level optimization perspective on fair learning: optimizing the learning strategy based on validation fairness. Specifically, we consider the highly effective workflow of adapting pre-trained models to downstream medical imaging tasks using parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between updating more parameters, enabling a better fit to the task of interest vs. fewer parameters, potentially reducing the generalization gap. To manage this tradeoff, we propose FairTune, a framework to optimize the choice of PEFT parameters with respect to fairness. We demonstrate empirically that FairTune leads to improved fairness on a range of medical imaging datasets.Here's the text with some additional information about the translation:I used the Google Translate API to translate the text into Simplified Chinese. The translation is in the "Simplified Chinese" language setting, which is the standard writing system used in mainland China.Please note that the translation may not be perfect, and there may be some nuances or cultural references that are lost in translation. Additionally, the translation may not be suitable for all audiences, especially in formal or professional settings. It's always a good idea to double-check the translation with a native speaker or a professional translator to ensure accuracy and cultural appropriateness.
</details></li>
</ul>
<hr>
<h2 id="Low-Resolution-Self-Attention-for-Semantic-Segmentation"><a href="#Low-Resolution-Self-Attention-for-Semantic-Segmentation" class="headerlink" title="Low-Resolution Self-Attention for Semantic Segmentation"></a>Low-Resolution Self-Attention for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05026">http://arxiv.org/abs/2310.05026</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuhuan-wu/LRFormer">https://github.com/yuhuan-wu/LRFormer</a></li>
<li>paper_authors: Yu-Huan Wu, Shi-Chen Zhang, Yun Liu, Le Zhang, Xin Zhan, Daquan Zhou, Jiashi Feng, Ming-Ming Cheng, Liangli Zhen<br>for:LRFormer is designed for semantic segmentation tasks, specifically to improve the efficiency of vision transformers while maintaining performance.methods:LRFormer uses a Low-Resolution Self-Attention (LRSA) mechanism to capture global context at a reduced computational cost, along with 3x3 depth-wise convolutions to capture fine details in the high-resolution space.results:LRFormer outperforms state-of-the-art models on the ADE20K, COCO-Stuff, and Cityscapes datasets.<details>
<summary>Abstract</summary>
Semantic segmentation tasks naturally require high-resolution information for pixel-wise segmentation and global context information for class prediction. While existing vision transformers demonstrate promising performance, they often utilize high resolution context modeling, resulting in a computational bottleneck. In this work, we challenge conventional wisdom and introduce the Low-Resolution Self-Attention (LRSA) mechanism to capture global context at a significantly reduced computational cost. Our approach involves computing self-attention in a fixed low-resolution space regardless of the input image's resolution, with additional 3x3 depth-wise convolutions to capture fine details in the high-resolution space. We demonstrate the effectiveness of our LRSA approach by building the LRFormer, a vision transformer with an encoder-decoder structure. Extensive experiments on the ADE20K, COCO-Stuff, and Cityscapes datasets demonstrate that LRFormer outperforms state-of-the-art models. The code will be made available at https://github.com/yuhuan-wu/LRFormer.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化字符的中文。</SYS>>semantic segmentation任务自然需要高分辨率信息进行像素精度分割和全局上下文信息进行类型预测。而现有的视觉变换器经常使用高分辨率上下文模型，导致计算扰乱。在这项工作中，我们挑战传统的观点，并引入低分辨率自我关注（LRSA）机制，以 capture全局上下文，但是计算成本明显减少。我们的方法是在固定的低分辨率空间内计算自我关注，并在高分辨率空间内进行3x3深度感知 convolution来捕捉细节。我们建立了LRFormer，一个具有encoder-decoder结构的视觉变换器。我们的实验表明，LRFormer在ADE20K、COCO-Stuff和Cityscapes datasets上表现出色，超过了当前最佳模型。代码将在https://github.com/yuhuan-wu/LRFormer上提供。
</details></li>
</ul>
<hr>
<h2 id="Single-Stage-Warped-Cloth-Learning-and-Semantic-Contextual-Attention-Feature-Fusion-for-Virtual-TryOn"><a href="#Single-Stage-Warped-Cloth-Learning-and-Semantic-Contextual-Attention-Feature-Fusion-for-Virtual-TryOn" class="headerlink" title="Single Stage Warped Cloth Learning and Semantic-Contextual Attention Feature Fusion for Virtual TryOn"></a>Single Stage Warped Cloth Learning and Semantic-Contextual Attention Feature Fusion for Virtual TryOn</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05024">http://arxiv.org/abs/2310.05024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanhita Pathak, Vinay Kaushik, Brejesh Lall</li>
<li>for: 提供一种基于图像的虚拟尝试服装系统，使得用户可以在图像上尝试不同的服装。</li>
<li>methods: 提posed a novel single-stage framework that implicitly learns garment warping and body synthesis from target pose keypoints, using a semantic-contextual fusion attention module and a lightweight linear attention framework to address misalignment and artifacts.</li>
<li>results: 比较 existed methods有更高的效率和质量，提供更可靠和真实的虚拟尝试体验。<details>
<summary>Abstract</summary>
Image-based virtual try-on aims to fit an in-shop garment onto a clothed person image. Garment warping, which aligns the target garment with the corresponding body parts in the person image, is a crucial step in achieving this goal. Existing methods often use multi-stage frameworks to handle clothes warping, person body synthesis and tryon generation separately or rely on noisy intermediate parser-based labels. We propose a novel single-stage framework that implicitly learns the same without explicit multi-stage learning. Our approach utilizes a novel semantic-contextual fusion attention module for garment-person feature fusion, enabling efficient and realistic cloth warping and body synthesis from target pose keypoints. By introducing a lightweight linear attention framework that attends to garment regions and fuses multiple sampled flow fields, we also address misalignment and artifacts present in previous methods. To achieve simultaneous learning of warped garment and try-on results, we introduce a Warped Cloth Learning Module. WCLM uses segmented warped garments as ground truth, operating within a single-stage paradigm. Our proposed approach significantly improves the quality and efficiency of virtual try-on methods, providing users with a more reliable and realistic virtual try-on experience. We evaluate our method on the VITON dataset and demonstrate its state-of-the-art performance in terms of both qualitative and quantitative metrics.
</details>
<details>
<summary>摘要</summary>
文本翻译：图像基于的虚拟试穿目标是将店内衣服适应到披衣人像中的不同姿势。衣服扭曲是实现这个目标的关键步骤，但现有方法frequently使用多个阶段框架来处理衣服扭曲、人体身体合成和试穿生成。我们提出了一种新的单阶段框架，不需要显式的多阶段学习。我们的方法利用了一种新的 semantics-contextual fusion attention模块来拼接衣服和人体特征，从而实现高效和真实的衣服扭曲和人体合成。我们还引入了一种轻量级的线性注意机制，用于衣服区域的注意和多个抽取流场的融合，以解决先前方法中的偏移和零散。为了同时学习扭曲衣服和试穿结果，我们引入了扭曲衣服学习模块（WCLM）。WCLM使用分割后的扭曲衣服作为真实数据，在单阶段框架中进行学习。我们的提议方法可以大幅提高虚拟试穿方法的质量和效率，为用户提供更可靠和更真实的虚拟试穿体验。我们在VITON数据集上进行了评估，并证明了我们的方法在质量和量化指标上具有当前领域的state-of-the-art性。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Abnormal-Health-Conditions-in-Smart-Home-Using-a-Drone"><a href="#Detecting-Abnormal-Health-Conditions-in-Smart-Home-Using-a-Drone" class="headerlink" title="Detecting Abnormal Health Conditions in Smart Home Using a Drone"></a>Detecting Abnormal Health Conditions in Smart Home Using a Drone</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05012">http://arxiv.org/abs/2310.05012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pronob Kumar Barman</li>
<li>for: 本研究旨在开发一种智能化的跌倒检测系统，以帮助年轻和老年人独立生活。</li>
<li>methods: 该系统使用视觉基于的跌倒监测，通过图像或视频分割和物体检测方法，以实现跌倒的识别。</li>
<li>results: 研究结果表明，该系统可以准确地识别跌倒物体，准确率为0.9948。<details>
<summary>Abstract</summary>
Nowadays, detecting aberrant health issues is a difficult process. Falling, especially among the elderly, is a severe concern worldwide. Falls can result in deadly consequences, including unconsciousness, internal bleeding, and often times, death. A practical and optimal, smart approach of detecting falling is currently a concern. The use of vision-based fall monitoring is becoming more common among scientists as it enables senior citizens and those with other health conditions to live independently. For tracking, surveillance, and rescue, unmanned aerial vehicles use video or image segmentation and object detection methods. The Tello drone is equipped with a camera and with this device we determined normal and abnormal behaviors among our participants. The autonomous falling objects are classified using a convolutional neural network (CNN) classifier. The results demonstrate that the systems can identify falling objects with a precision of 0.9948.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Augmentation-through-Pseudolabels-in-Automatic-Region-Based-Coronary-Artery-Segmentation-for-Disease-Diagnosis"><a href="#Data-Augmentation-through-Pseudolabels-in-Automatic-Region-Based-Coronary-Artery-Segmentation-for-Disease-Diagnosis" class="headerlink" title="Data Augmentation through Pseudolabels in Automatic Region Based Coronary Artery Segmentation for Disease Diagnosis"></a>Data Augmentation through Pseudolabels in Automatic Region Based Coronary Artery Segmentation for Disease Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05990">http://arxiv.org/abs/2310.05990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandesh Pokhrel, Sanjay Bhandari, Eduard Vazquez, Yash Raj Shrestha, Binod Bhattarai</li>
<li>for: 诊断心血管疾病（CAD）的准确性和效率有很大提高的需求，但现有的诊断方法往往困难和资源占用。在这种情况下，分割arteries的技术成为一种帮助临床专业人员作出准确诊断的工具。</li>
<li>methods: 本研究使用 pseudolabels 作为数据增强技术，以提高基eline Yolo 模型的性能。</li>
<li>results: 在验证集中，使用 pseudolabels 增强基eline Yolo 模型的 F1 分数提高了 9%，在测试集中提高了 3%。<details>
<summary>Abstract</summary>
Coronary Artery Diseases(CADs) though preventable are one of the leading causes of death and disability. Diagnosis of these diseases is often difficult and resource intensive. Segmentation of arteries in angiographic images has evolved as a tool for assistance, helping clinicians in making accurate diagnosis. However, due to the limited amount of data and the difficulty in curating a dataset, the task of segmentation has proven challenging. In this study, we introduce the idea of using pseudolabels as a data augmentation technique to improve the performance of the baseline Yolo model. This method increases the F1 score of the baseline by 9% in the validation dataset and by 3% in the test dataset.
</details>
<details>
<summary>摘要</summary>
心血管疾病(CAD) 虽可预防，但它是死亡和残疾的主要原因之一。诊断这种疾病的困难和资源占用。artery segmentation in angiographic images has evolved as a tool for assistance, helping clinicians make accurate diagnoses. However, due to limited data and difficulty in curating a dataset, the task of segmentation has proven challenging. In this study, we introduce the idea of using pseudolabels as a data augmentation technique to improve the performance of the baseline Yolo model. This method increases the F1 score of the baseline by 9% in the validation dataset and by 3% in the test dataset.Note: "心血管" (xīn xuè màn) is a shortened form of "心血管疾病" (xīn xuè màn jì bìng), which means "cardiovascular disease" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Building-an-Open-Vocabulary-Video-CLIP-Model-with-Better-Architectures-Optimization-and-Data"><a href="#Building-an-Open-Vocabulary-Video-CLIP-Model-with-Better-Architectures-Optimization-and-Data" class="headerlink" title="Building an Open-Vocabulary Video CLIP Model with Better Architectures, Optimization and Data"></a>Building an Open-Vocabulary Video CLIP Model with Better Architectures, Optimization and Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05010">http://arxiv.org/abs/2310.05010</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wengzejia1/open-vclip">https://github.com/wengzejia1/open-vclip</a></li>
<li>paper_authors: Zuxuan Wu, Zejia Weng, Wujian Peng, Xitong Yang, Ang Li, Larry S. Davis, Yu-Gang Jiang<br>for:* The paper aims to adapt Contrastive Language-Image Pretraining (CLIP) for zero-shot video recognition, with the goal of identifying novel actions and events in videos.methods:* The proposed method, called Open-VCLIP++, modifies CLIP to capture spatial-temporal relationships in videos, and leverages a technique called Interpolated Weight Optimization to improve generalization.* The method also utilizes large language models to produce fine-grained video descriptions, which are aligned with video features to facilitate a better transfer of CLIP to the video domain.results:* The proposed method achieves zero-shot accuracy scores of 88.1%, 58.7%, and 81.2% on UCF, HMDB, and Kinetics-600 datasets respectively, outperforming the best-performing alternative methods by 8.5%, 8.2%, and 12.3%.* The method also delivers competitive video-to-text and text-to-video retrieval performance on the MSR-VTT video-text retrieval dataset, with substantially less fine-tuning data compared to other methods.<details>
<summary>Abstract</summary>
Despite significant results achieved by Contrastive Language-Image Pretraining (CLIP) in zero-shot image recognition, limited effort has been made exploring its potential for zero-shot video recognition. This paper presents Open-VCLIP++, a simple yet effective framework that adapts CLIP to a strong zero-shot video classifier, capable of identifying novel actions and events during testing. Open-VCLIP++ minimally modifies CLIP to capture spatial-temporal relationships in videos, thereby creating a specialized video classifier while striving for generalization. We formally demonstrate that training Open-VCLIP++ is tantamount to continual learning with zero historical data. To address this problem, we introduce Interpolated Weight Optimization, a technique that leverages the advantages of weight interpolation during both training and testing. Furthermore, we build upon large language models to produce fine-grained video descriptions. These detailed descriptions are further aligned with video features, facilitating a better transfer of CLIP to the video domain. Our approach is evaluated on three widely used action recognition datasets, following a variety of zero-shot evaluation protocols. The results demonstrate that our method surpasses existing state-of-the-art techniques by significant margins. Specifically, we achieve zero-shot accuracy scores of 88.1%, 58.7%, and 81.2% on UCF, HMDB, and Kinetics-600 datasets respectively, outpacing the best-performing alternative methods by 8.5%, 8.2%, and 12.3%. We also evaluate our approach on the MSR-VTT video-text retrieval dataset, where it delivers competitive video-to-text and text-to-video retrieval performance, while utilizing substantially less fine-tuning data compared to other methods. Code is released at https://github.com/wengzejia1/Open-VCLIP.
</details>
<details>
<summary>摘要</summary>
尽管尝试语言图像预训练（CLIP）在零shot图像识别中取得了显著的成果，但是对其在零shot视频识别方面的潜在能力尚未得到了充分的探讨。本文提出了Open-VCLIP++框架，这是一种简单而有效的方法，可以将CLIP adapted into a strong zero-shot video classifier，能够在测试中识别新的动作和事件。Open-VCLIP++只需要微小地修改CLIP，以捕捉视频中的空间-时间关系，从而创造一个特циализирован的视频分类器，同时尽量保持泛化能力。我们正式证明，在训练Open-VCLIP++时， Equivalent to continual learning with zero historical data。为解决这个问题，我们提出了 interpolated weight optimization 技术，该技术利用在训练和测试中 weight interpolation 的优势。此外，我们基于大型自然语言模型来生成细腻的视频描述，这些描述与视频特征进行了更好的匹配，以便更好地将CLIP转移到视频领域。我们的方法在三个常用的动作识别数据集上进行了评估，采用了多种零shot评估协议。结果表明，我们的方法在 UCF、HMDB 和 Kinetics-600 数据集上的零shot精度分别达到了 88.1%、58.7% 和 81.2%，与最佳替代方法相比提高了8.5%、8.2% 和 12.3%。我们还在 MSR-VTT 视频-文本检索数据集上评估了我们的方法，其在视频-文本和文本-视频检索中表现竞争力强，而且使用的微型 fine-tuning 数据量相比其他方法更少。代码可以在 <https://github.com/wengzejia1/Open-VCLIP> 上下载。
</details></li>
</ul>
<hr>
<h2 id="Symmetrical-Linguistic-Feature-Distillation-with-CLIP-for-Scene-Text-Recognition"><a href="#Symmetrical-Linguistic-Feature-Distillation-with-CLIP-for-Scene-Text-Recognition" class="headerlink" title="Symmetrical Linguistic Feature Distillation with CLIP for Scene Text Recognition"></a>Symmetrical Linguistic Feature Distillation with CLIP for Scene Text Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04999">http://arxiv.org/abs/2310.04999</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wzx99/clipocr">https://github.com/wzx99/clipocr</a></li>
<li>paper_authors: Zixiao Wang, Hongtao Xie, Yuxin Wang, Jianjun Xu, Boqiang Zhang, Yongdong Zhang</li>
<li>for: 这种研究旨在探讨CLIP模型在场景文本识别（STR）领域的潜力，并提出了一种新的对称语言特征泵化框架（CLIP-OCR），用于利用CLIP模型中的视觉和语言知识。</li>
<li>methods: 该研究提出了一种对称特征泵化策略（SDS），该策略可以更好地捕捉CLIP模型中的语言知识。具体来说，通过将CLIP图像encoder与反转的CLIP文本encoder串联起来，建立了一个对称结构，该结构包括一个图像到文本的特征流，这个特征流包括了视觉和语言信息。</li>
<li>results: 实验结果表明，CLIP-OCR可以在六个流行的STR benchmark上达到93.8%的平均准确率。<details>
<summary>Abstract</summary>
In this paper, we explore the potential of the Contrastive Language-Image Pretraining (CLIP) model in scene text recognition (STR), and establish a novel Symmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to leverage both visual and linguistic knowledge in CLIP. Different from previous CLIP-based methods mainly considering feature generalization on visual encoding, we propose a symmetrical distillation strategy (SDS) that further captures the linguistic knowledge in the CLIP text encoder. By cascading the CLIP image encoder with the reversed CLIP text encoder, a symmetrical structure is built with an image-to-text feature flow that covers not only visual but also linguistic information for distillation.Benefiting from the natural alignment in CLIP, such guidance flow provides a progressive optimization objective from vision to language, which can supervise the STR feature forwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss (LCL) is proposed to enhance the linguistic capability by considering second-order statistics during the optimization. Overall, CLIP-OCR is the first to design a smooth transition between image and text for the STR task.Extensive experiments demonstrate the effectiveness of CLIP-OCR with 93.8% average accuracy on six popular STR benchmarks.Code will be available at https://github.com/wzx99/CLIPOCR.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探索了CLIP模型在文本识别（STR）领域的潜力，并提出了一种新的对称语言特征精炼框架（CLIP-OCR），用于利用CLIP模型的视觉和语言知识。与前者CLIP基于方法主要关注视觉编码特征泛化，我们提议一种对称精炼策略（SDS），进一步捕捉CLIP文本编码器中的语言知识。通过将CLIP图像编码器与反转CLIP文本编码器串联起来，建立了一个对称结构，涵盖了图像和文本之间的视觉和语言信息 для精炼。由于CLIP自然的对称性，这种导向流提供了一个逐层优化目标，从视觉到语言，可以超visually guided feature forwarding process layer by layer.此外，我们还提出了一种新的语言一致损失（LCL），以提高语言能力，通过考虑第二阶段统计信息进行优化。总的来说，CLIP-OCR是STR任务中首次设计了图像和文本之间的平滑过渡。我们的实验结果显示，CLIP-OCR在六个流行的STR benchmark上获得了93.8%的平均准确率。代码将在https://github.com/wzx99/CLIPOCR上公开。
</details></li>
</ul>
<hr>
<h2 id="SemST-Semantically-Consistent-Multi-Scale-Image-Translation-via-Structure-Texture-Alignment"><a href="#SemST-Semantically-Consistent-Multi-Scale-Image-Translation-via-Structure-Texture-Alignment" class="headerlink" title="SemST: Semantically Consistent Multi-Scale Image Translation via Structure-Texture Alignment"></a>SemST: Semantically Consistent Multi-Scale Image Translation via Structure-Texture Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04995">http://arxiv.org/abs/2310.04995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganning Zhao, Wenhui Cui, Suya You, C. -C. Jay Kuo</li>
<li>for: 本研究旨在提出一种能够维护semantic consistency的无监督图像到图像（I2I）翻译方法，以 Addressing the challenge of content discrepancy in I2I translation.</li>
<li>methods: 本方法使用对比学习和最大化输入和输出之间的共聚信息，以降低semantic distortion。另外，一种多尺度方法也是引入，以提高翻译性能。</li>
<li>results: 实验表明，本方法能够有效地减少semantic distortion，并 achieve state-of-the-art performance。此外，在域适应（DA）中应用SemST也被证明可以作为semantic segmentation任务的有利预处理。<details>
<summary>Abstract</summary>
Unsupervised image-to-image (I2I) translation learns cross-domain image mapping that transfers input from the source domain to output in the target domain while preserving its semantics. One challenge is that different semantic statistics in source and target domains result in content discrepancy known as semantic distortion. To address this problem, a novel I2I method that maintains semantic consistency in translation is proposed and named SemST in this work. SemST reduces semantic distortion by employing contrastive learning and aligning the structural and textural properties of input and output by maximizing their mutual information. Furthermore, a multi-scale approach is introduced to enhance translation performance, thereby enabling the applicability of SemST to domain adaptation in high-resolution images. Experiments show that SemST effectively mitigates semantic distortion and achieves state-of-the-art performance. Also, the application of SemST to domain adaptation (DA) is explored. It is demonstrated by preliminary experiments that SemST can be utilized as a beneficial pre-training for the semantic segmentation task.
</details>
<details>
<summary>摘要</summary>
<<SYS>>设想系统：源领域到目标领域的无监督图像译换学习映射输入源领域的图像到目标领域的图像，保持 Semantics 的含义。一个挑战是源领域和目标领域的含义统计不同，导致内容偏差，即semantic distortion。为解决这个问题，本文提出了一种新的I2I方法，命名为SemST，该方法保持了Semantics的一致性。SemST通过对输入和输出的结构和文本特征进行匹配，使得它们之间的信息共同性最大化，从而减少semantic distortion。此外，本文还提出了一种多尺度方法，以提高译换性能，使得SemST可以应用于高分辨率图像的领域适应。实验表明，SemST可以有效地减少semantic distortion，并达到领域顶尖性能。此外，本文还探讨了SemST的应用于领域适应（DA），并通过初步实验表明，SemST可以作为semantic segmentation任务的有利预训练。
</details></li>
</ul>
<hr>
<h2 id="VisionFM-a-Multi-Modal-Multi-Task-Vision-Foundation-Model-for-Generalist-Ophthalmic-Artificial-Intelligence"><a href="#VisionFM-a-Multi-Modal-Multi-Task-Vision-Foundation-Model-for-Generalist-Ophthalmic-Artificial-Intelligence" class="headerlink" title="VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for Generalist Ophthalmic Artificial Intelligence"></a>VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for Generalist Ophthalmic Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04992">http://arxiv.org/abs/2310.04992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianing Qiu, Jian Wu, Hao Wei, Peilun Shi, Minqing Zhang, Yunyun Sun, Lin Li, Hanruo Liu, Hongyi Liu, Simeng Hou, Yuyang Zhao, Xuehui Shi, Junfang Xian, Xiaoxia Qu, Sirui Zhu, Lijie Pan, Xiaoniao Chen, Xiaojia Zhang, Shuai Jiang, Kebing Wang, Chenlong Yang, Mingqiang Chen, Sujie Fan, Jianhua Hu, Aiguo Lv, Hui Miao, Li Guo, Shujun Zhang, Cheng Pei, Xiaojuan Fan, Jianqin Lei, Ting Wei, Junguo Duan, Chun Liu, Xiaobo Xia, Siqi Xiong, Junhong Li, Benny Lo, Yih Chung Tham, Tien Yin Wong, Ningli Wang, Wu Yuan</li>
<li>for: 这个论文是为了开发一个基于340万张眼科图像的基础模型，用于推动多种眼科人工智能应用程序。</li>
<li>methods: 该模型使用了340万张眼科图像，涵盖了各种眼科疾病、图像设备和人口类型，进行预训练。</li>
<li>results: 模型在12种常见眼科疾病的诊断中与专业医生的合作诊断性能相当或更高，并在新的大规模眼科疾病诊断数据集和检测数据集上表现出优异性能。<details>
<summary>Abstract</summary>
We present VisionFM, a foundation model pre-trained with 3.4 million ophthalmic images from 560,457 individuals, covering a broad range of ophthalmic diseases, modalities, imaging devices, and demography. After pre-training, VisionFM provides a foundation to foster multiple ophthalmic artificial intelligence (AI) applications, such as disease screening and diagnosis, disease prognosis, subclassification of disease phenotype, and systemic biomarker and disease prediction, with each application enhanced with expert-level intelligence and accuracy. The generalist intelligence of VisionFM outperformed ophthalmologists with basic and intermediate levels in jointly diagnosing 12 common ophthalmic diseases. Evaluated on a new large-scale ophthalmic disease diagnosis benchmark database, as well as a new large-scale segmentation and detection benchmark database, VisionFM outperformed strong baseline deep neural networks. The ophthalmic image representations learned by VisionFM exhibited noteworthy explainability, and demonstrated strong generalizability to new ophthalmic modalities, disease spectrum, and imaging devices. As a foundation model, VisionFM has a large capacity to learn from diverse ophthalmic imaging data and disparate datasets. To be commensurate with this capacity, in addition to the real data used for pre-training, we also generated and leveraged synthetic ophthalmic imaging data. Experimental results revealed that synthetic data that passed visual Turing tests, can also enhance the representation learning capability of VisionFM, leading to substantial performance gains on downstream ophthalmic AI tasks. Beyond the ophthalmic AI applications developed, validated, and demonstrated in this work, substantial further applications can be achieved in an efficient and cost-effective manner using VisionFM as the foundation.
</details>
<details>
<summary>摘要</summary>
我们介绍VisionFM，一个基础模型，通过340万张眼科图像和560457名个人数据进行预训练，覆盖了广泛的眼科疾病、模式、成像设备和人口学。预训练后，VisionFM提供了一个基础，以推动多种眼科人工智能应用程序，如疾病检测和诊断、疾病诊断、疾病类型分 subclassification和系统生物标志和疾病预测，每个应用程序都受到专家水平的智能和准确性的提高。VisionFM的通用智能超过了基本和中级水平的眼科医生，在共同诊断12种常见眼科疾病方面表现出色。在一个新的大规模眼科疾病诊断benchmark数据集和一个新的大规模分割和检测benchmark数据集上进行评估，VisionFM表现出色，并超越了强基线深度神经网络。眼科图像学习的VisionFM表现出了值得注意的解释性，并在新的眼科模式、疾病谱和成像设备上表现出了强大的普适性。作为基础模型，VisionFM具有大量学习眼科成像数据和多种数据集的能力。为了与这种能力相符，我们不仅使用了实际数据进行预训练，还生成并利用了合理的 synthetic眼科成像数据。实验结果表明，通过visual Turing测试，合理的synthetic数据也可以提高VisionFM的表征学习能力，导致下游眼科人工智能任务的性能提高。除了在本工作中开发、验证和示例的眼科人工智能应用程序外，VisionFM可以在高效和cost-effective的方式实现更多的应用。
</details></li>
</ul>
<hr>
<h2 id="Video-Teller-Enhancing-Cross-Modal-Generation-with-Fusion-and-Decoupling"><a href="#Video-Teller-Enhancing-Cross-Modal-Generation-with-Fusion-and-Decoupling" class="headerlink" title="Video-Teller: Enhancing Cross-Modal Generation with Fusion and Decoupling"></a>Video-Teller: Enhancing Cross-Modal Generation with Fusion and Decoupling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04991">http://arxiv.org/abs/2310.04991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haogeng Liu, Qihang Fan, Tingkai Liu, Linjie Yang, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang</li>
<li>for: 这篇论文旨在提出一种视频语言基础模型，以便进行视频描述生成任务。</li>
<li>methods: 该模型使用多模态融合和精细的模态对齐来显著提高视频描述生成的效果。它利用冻结预训练的视觉和语言模块，并在描述生成过程中使用大型自然语言模型来生成 concise 和 elaborate 的视频描述。</li>
<li>results: 实验结果表明，该模型可以准确地理解视频内容，并生成 coherent 和精细的语言描述。 fine-grained 模态对齐目标可以提高模型的能力（4% 提高 CIDEr 分数在 MSR-VTT），仅需训练参数增加 13%，并在推理过程中不增加额外成本。<details>
<summary>Abstract</summary>
This paper proposes Video-Teller, a video-language foundation model that leverages multi-modal fusion and fine-grained modality alignment to significantly enhance the video-to-text generation task. Video-Teller boosts the training efficiency by utilizing frozen pretrained vision and language modules. It capitalizes on the robust linguistic capabilities of large language models, enabling the generation of both concise and elaborate video descriptions. To effectively integrate visual and auditory information, Video-Teller builds upon the image-based BLIP-2 model and introduces a cascaded Q-Former which fuses information across frames and ASR texts. To better guide video summarization, we introduce a fine-grained modality alignment objective, where the cascaded Q-Former's output embedding is trained to align with the caption/summary embedding created by a pretrained text auto-encoder. Experimental results demonstrate the efficacy of our proposed video-language foundation model in accurately comprehending videos and generating coherent and precise language descriptions. It is worth noting that the fine-grained alignment enhances the model's capabilities (4% improvement of CIDEr score on MSR-VTT) with only 13% extra parameters in training and zero additional cost in inference.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文提出了 Video-Teller，一种基于视频-语言基础模型，通过多modal融合和精细模态对接来备受提高视频到文本生成任务的能力。Video-Teller 通过冻结预训练的视觉和语言模块来提高训练效率。它利用大语言模型的 Robust 语言功能，以便生成 Both concise 和 elaborate 的视频描述。为了有效地 инте格ри视觉和听觉信息，Video-Teller 基于 BLIP-2 模型，并引入一个卷积扩展器，将信息融合到帧和 ASR 文本之间。为了更好地引导视频摘要，我们引入了精细模态对接目标，使得卷积扩展器的输出嵌入与预训练文本自动编码器创建的 Caption/Summary 嵌入进行对接。实验结果表明我们提出的视频语言基础模型在准确理解视频并生成准确和精细的语言描述方面具有很高的能力。值得注意的是，精细对接对模型的能力提高带来了4%的 CIDEr 分数提高（在 MSR-VTT 上），只需要在训练中增加13%的参数，并在推理时间中没有额外的成本。
</details></li>
</ul>
<hr>
<h2 id="Compositional-Semantics-for-Open-Vocabulary-Spatio-semantic-Representations"><a href="#Compositional-Semantics-for-Open-Vocabulary-Spatio-semantic-Representations" class="headerlink" title="Compositional Semantics for Open Vocabulary Spatio-semantic Representations"></a>Compositional Semantics for Open Vocabulary Spatio-semantic Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04981">http://arxiv.org/abs/2310.04981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Karlsson, Francisco Lepe-Salazar, Kazuya Takeda</li>
<li>for: 本研究旨在实现无需人工指令的通用移动机器人，通过大语言模型（LLM）和感知语言模型（VLM）来实现常识世界知识和理性计划。</li>
<li>methods: 本研究提出了幽领结构嵌入〈z<em>〉，实现可询问的空间 semantics 表示。通过数学证明和实验验证，〈z</em>〉可以在任意集Z中找到最佳中心，并且可以由梯度下降优化从视觉外观和单词描述中学习。</li>
<li>results: 实验结果显示，〈z<em>〉可以表示到100个高维度嵌入中的10个 semantics，并且可以提高非关连的开 vocabulary segmentation性能。使用CLIP和SBERT嵌入空间的实验结果显示，一个简单的 dense VLM可以在COCO-Stuff dataset上学习〈z</em>〉，实现181个相互关联的 semantics 的构成。<details>
<summary>Abstract</summary>
General-purpose mobile robots need to complete tasks without exact human instructions. Large language models (LLMs) is a promising direction for realizing commonsense world knowledge and reasoning-based planning. Vision-language models (VLMs) transform environment percepts into vision-language semantics interpretable by LLMs. However, completing complex tasks often requires reasoning about information beyond what is currently perceived. We propose latent compositional semantic embeddings z* as a principled learning-based knowledge representation for queryable spatio-semantic memories. We mathematically prove that z* can always be found, and the optimal z* is the centroid for any set Z. We derive a probabilistic bound for estimating separability of related and unrelated semantics. We prove that z* is discoverable by iterative optimization by gradient descent from visual appearance and singular descriptions. We experimentally verify our findings on four embedding spaces incl. CLIP and SBERT. Our results show that z* can represent up to 10 semantics encoded by SBERT, and up to 100 semantics for ideal uniformly distributed high-dimensional embeddings. We demonstrate that a simple dense VLM trained on the COCO-Stuff dataset can learn z* for 181 overlapping semantics by 42.23 mIoU, while improving conventional non-overlapping open-vocabulary segmentation performance by +3.48 mIoU compared with a popular SOTA model.
</details>
<details>
<summary>摘要</summary>
通用移动机器人需要完成任务无需准确的人类指令。大型语言模型（LLM）是实现通用世界知识和理由预测的可能性的方向。视觉语言模型（VLM）将环境感知转化为可解释的视觉语言 semantics，但完成复杂任务通常需要对现有信息之外的信息进行推理。我们提议使用幽默的 composer semantic embedding z* 作为可学习基于知识表示的原则。我们 математичеamente 证明 z* 总是可以找到，并且最佳 z* 是 Z 集合中的中心。我们 derive 一个 probabilistic bound 用于估计相关和不相关 semantics 之间的分化程度。我们证明 z* 可以通过迭代的梯度下降从视觉特征和 singular descriptions 进行学习。我们在 CLIP 和 SBERT 等四个 embedding space 上进行实验，并证明 z* 可以表示 SBERT 中的 10 个 semantics，以及高维 embedding 中的 100 个 semantics。我们还示出了一个简单的 dense VLM 在 COCO-Stuff 数据集上可以通过 z* 学习 181 个 overlap semantics，而且提高了非 overlap segmentation 性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-Many-to-Many-Mapping-for-Unpaired-Real-World-Image-Super-resolution-and-Downscaling"><a href="#Learning-Many-to-Many-Mapping-for-Unpaired-Real-World-Image-Super-resolution-and-Downscaling" class="headerlink" title="Learning Many-to-Many Mapping for Unpaired Real-World Image Super-resolution and Downscaling"></a>Learning Many-to-Many Mapping for Unpaired Real-World Image Super-resolution and Downscaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04964">http://arxiv.org/abs/2310.04964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanjie Sun, Zhenzhong Chen</li>
<li>for: 这篇论文旨在提出一种不需要对比的单图超解析（SISR）方法，用于处理真实世界中的图像，因为现有的大多数不监督的实世界SISR方法采用了两个阶段训练策略，首先将高分辨率图像转换成低分辨率图像，然后在监督下训练超解析模型。</li>
<li>methods: 该方法提出了一种名为SDFlow的图像下采样和超解析模型，该模型同时学习了 bidirectional 多对多 mapping  между实世界低分辨率图像和高分辨率图像，无需对比。SDFlow 通过分离图像内容和降解信息在幂空间中，使得低分辨率图像和高分辨率图像的内容信息分布在共同的幂空间中匹配。</li>
<li>results: 实验结果表明，SDFlow 可以生成多个真实和可见的低分辨率图像和高分辨率图像，并且能够Quantitatively and qualitatively improve the performance of real-world image super-resolution.<details>
<summary>Abstract</summary>
Learning based single image super-resolution (SISR) for real-world images has been an active research topic yet a challenging task, due to the lack of paired low-resolution (LR) and high-resolution (HR) training images. Most of the existing unsupervised real-world SISR methods adopt a two-stage training strategy by synthesizing realistic LR images from their HR counterparts first, then training the super-resolution (SR) models in a supervised manner. However, the training of image degradation and SR models in this strategy are separate, ignoring the inherent mutual dependency between downscaling and its inverse upscaling process. Additionally, the ill-posed nature of image degradation is not fully considered. In this paper, we propose an image downscaling and SR model dubbed as SDFlow, which simultaneously learns a bidirectional many-to-many mapping between real-world LR and HR images unsupervisedly. The main idea of SDFlow is to decouple image content and degradation information in the latent space, where content information distribution of LR and HR images is matched in a common latent space. Degradation information of the LR images and the high-frequency information of the HR images are fitted to an easy-to-sample conditional distribution. Experimental results on real-world image SR datasets indicate that SDFlow can generate diverse realistic LR and SR images both quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
学习基于单个图像超分辨 (SISR) 对实际世界图像进行研究是一个活跃的研究话题，但是是一个具有挑战性的任务，因为缺乏匹配的低分辨率 (LR) 和高分辨率 (HR) 训练图像。大多数现有的无监督实际世界 SISR 方法采用了两个阶段训练策略，先将实际LR图像Synthesize into HR counterparts，然后在监督性训练SR模型。但是，训练图像减退和其 inverse upscaling 过程中的相互依赖关系未被考虑，同时不完全考虑图像减退的不定性。在这篇论文中，我们提出了一种名为SDFlow的图像减退和SR模型，可以同时学习实际LR和HR图像之间的 bidirectional many-to-many 映射，无需监督。主要思想是在幂空间中分离图像内容和减退信息，LR图像的内容信息和HR图像的高频信息在幂空间中匹配。LR图像的减退信息和HR图像的高频信息被 fitted 到一个易于样本的 conditional distribution。实验结果表明，SDFlow可以生成多样化的实际LR和SR图像，并且具有较高的量化和质量指标。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/cs.CV_2023_10_08/" data-id="clombedt400iys088217jemgd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/cs.AI_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T12:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/cs.AI_2023_10_08/">cs.AI - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Optimizing-Solution-Samplers-for-Combinatorial-Problems-The-Landscape-of-Policy-Gradient-Methods"><a href="#Optimizing-Solution-Samplers-for-Combinatorial-Problems-The-Landscape-of-Policy-Gradient-Methods" class="headerlink" title="Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods"></a>Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05309">http://arxiv.org/abs/2310.05309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Constantine Caramanis, Dimitris Fotakis, Alkis Kalavasis, Vasilis Kontonis, Christos Tzamos</li>
<li>for: 这 paper 的目的是提供一种新的理论框架，用于分析 Deep Neural Networks 和 Reinforcement Learning 方法在解决复杂的 combinatorial 问题时的效果。</li>
<li>methods: 这 paper 使用的方法包括使用 Deep Neural Network 作为解决方案生成器，并通过 gradient-based 方法（例如策略梯度）进行训练，以获得更好的解决方案分布。</li>
<li>results: 本 paper 的主要贡献是提供了一个答案，证明 Deep Neural Networks 和 Reinforcement Learning 方法可以有效地解决 combinatorial 问题，包括 Max- 和 Min-Cut、Max-$k$-CSP、最大权重双向匹配和旅行商问题。此外，这 paper 还介绍了一种新的规范过程，用于改进 vanilla gradient descent，并提供了理论和实验证明，这种方法可以解决消失梯度问题和避免坏的站点点。<details>
<summary>Abstract</summary>
Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions. In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. As a byproduct of our analysis we introduce a novel regularization process over vanilla gradient descent and provide theoretical and experimental evidence that it helps address vanishing-gradient issues and escape bad stationary points.
</details>
<details>
<summary>摘要</summary>
深度神经网络和强化学习方法在解决复杂的 combinatorial 问题方面有广泛的实践经验。在这些方法中，深度神经网络被用作解决生成器，然后通过梯度基本方法（例如策略梯度）进行训练，以逐渐获得更好的解决分布。在这种工作中，我们提出了一个新的理论框架来分析这些方法的效果。我们问题是否存在一些生成模型，满足以下条件：（i）能够生成约等价优的解决方案；（ii） Parameters 的数量是输入数据的线性函数；（iii）优化Landscaper 是柔和的，不含有优化点。我们的主要贡献是给出了一个积极的答案。我们的结果适用于一类复杂的 combinatorial 问题，包括最大批量和最小批量问题、最大-$k$-CSP、最大负载双向匹配和旅行商问题。作为我们的分析的侧重点，我们还提出了一种新的 Regularization 过程，并通过理论和实验证明，它可以帮助解决混合梯度问题和避免坏的站点点。
</details></li>
</ul>
<hr>
<h2 id="Tailoring-Self-Attention-for-Graph-via-Rooted-Subtrees"><a href="#Tailoring-Self-Attention-for-Graph-via-Rooted-Subtrees" class="headerlink" title="Tailoring Self-Attention for Graph via Rooted Subtrees"></a>Tailoring Self-Attention for Graph via Rooted Subtrees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05296">http://arxiv.org/abs/2310.05296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lumia-group/subtree-attention">https://github.com/lumia-group/subtree-attention</a></li>
<li>paper_authors: Siyuan Huang, Yunchong Song, Jiayue Zhou, Zhouhan Lin</li>
<li>for: 本文旨在提出一种新的多跳图注意机制，以解决现有图注意机制中的局部注意力和全局注意力的缺陷。</li>
<li>methods: 本文提出了一种名为Subtree Attention（STA）的新型多跳图注意机制，具有跨度更强的能力捕捉长距离信息和细腻的地方信息。STA还提供了一种有理证据的修正方法，以保证STA在极端情况下可以近似于全局注意力。</li>
<li>results: 对于十个节点分类 dataset，STA-based模型表现出色，超越现有的图Transformers和主流 GNNs。<details>
<summary>Abstract</summary>
Attention mechanisms have made significant strides in graph learning, yet they still exhibit notable limitations: local attention faces challenges in capturing long-range information due to the inherent problems of the message-passing scheme, while global attention cannot reflect the hierarchical neighborhood structure and fails to capture fine-grained local information. In this paper, we propose a novel multi-hop graph attention mechanism, named Subtree Attention (STA), to address the aforementioned issues. STA seamlessly bridges the fully-attentional structure and the rooted subtree, with theoretical proof that STA approximates the global attention under extreme settings. By allowing direct computation of attention weights among multi-hop neighbors, STA mitigates the inherent problems in existing graph attention mechanisms. Further we devise an efficient form for STA by employing kernelized softmax, which yields a linear time complexity. Our resulting GNN architecture, the STAGNN, presents a simple yet performant STA-based graph neural network leveraging a hop-aware attention strategy. Comprehensive evaluations on ten node classification datasets demonstrate that STA-based models outperform existing graph transformers and mainstream GNNs. The code is available at https://github.com/LUMIA-Group/SubTree-Attention.
</details>
<details>
<summary>摘要</summary>
注意机制在图学习中已经取得了重要进展，但它们仍然存在显著的限制：当地注意力不能够捕捉远程信息，因为消息传递方案的内在问题，而全局注意力则不能够反映层次结构和细化的地方信息。在这篇论文中，我们提出了一种新的多趟图注意机制，名为子树注意（STA），以解决以上问题。STA可以准确地计算多趟邻居之间的注意力权重，并且有理论证明，STA可以在极端情况下近似于全局注意。我们还提出了一种高效的STA实现方式，通过使用核函数软max，实现了线性时间复杂度。我们的结果是一种简单又高性能的STA-基于GNN，称为STAGNN，它利用跳跃注意策略来实现多趟图注意。我们对十个节点分类 datasets进行了广泛的评估，发现STA-based模型比现有的图transformer和主流GNN都有更好的性能。代码可以在https://github.com/LUMIA-Group/SubTree-Attention中下载。
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Error-Modeling-for-Search-Relevance-Data-Annotation-Tasks"><a href="#Generalizable-Error-Modeling-for-Search-Relevance-Data-Annotation-Tasks" class="headerlink" title="Generalizable Error Modeling for Search Relevance Data Annotation Tasks"></a>Generalizable Error Modeling for Search Relevance Data Annotation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05286">http://arxiv.org/abs/2310.05286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heinrich Peters, Alireza Hashemi, James Rae</li>
<li>for: 这篇论文旨在提高机器学习和人工智能系统的质量，具体来说是针对搜索 relevance 标注任务进行预测错误模型的建立和评估。</li>
<li>methods: 该论文使用了一种预测错误模型，并在三个产业级 ML 应用（音乐流媒体、视频流媒体、移动应用）中进行了实践。</li>
<li>results: 论文显示了预测错误模型可以在不同应用中具有moderate的模型性能（AUC&#x3D;0.65-0.75），并且该模型在不同应用之间具有良好的泛化性。此外，论文还提供了模型解释分析，以便理解预测错误的主要驱动因素。最后，论文还证明了这种模型在审核中的有用性，可以提高数据标注过程中的效率和质量。<details>
<summary>Abstract</summary>
Human data annotation is critical in shaping the quality of machine learning (ML) and artificial intelligence (AI) systems. One significant challenge in this context is posed by annotation errors, as their effects can degrade the performance of ML models. This paper presents a predictive error model trained to detect potential errors in search relevance annotation tasks for three industry-scale ML applications (music streaming, video streaming, and mobile apps) and assesses its potential to enhance the quality and efficiency of the data annotation process. Drawing on real-world data from an extensive search relevance annotation program, we illustrate that errors can be predicted with moderate model performance (AUC=0.65-0.75) and that model performance generalizes well across applications (i.e., a global, task-agnostic model performs on par with task-specific models). We present model explainability analyses to identify which types of features are the main drivers of predictive performance. Additionally, we demonstrate the usefulness of the model in the context of auditing, where prioritizing tasks with high predicted error probabilities considerably increases the amount of corrected annotation errors (e.g., 40% efficiency gains for the music streaming application). These results underscore that automated error detection models can yield considerable improvements in the efficiency and quality of data annotation processes. Thus, our findings reveal critical insights into effective error management in the data annotation process, thereby contributing to the broader field of human-in-the-loop ML.
</details>
<details>
<summary>摘要</summary>
人工数据标注是机器学习（ML）和人工智能（AI）系统的关键因素。一个重要的挑战在这个上是标注错误，因为它们可以降低ML模型的性能。本文介绍了一个预测错误模型，用于检测搜索相关性标注任务中的可能错误，并评估其在三个产业级ML应用（音乐流媒体、视频流媒体和移动应用）中的可能性。基于广泛的搜索相关性标注计划的实际数据，我们示出了预测错误的能力（AUC=0.65-0.75），并证明模型性能可以通过应用之间进行泛化。我们还提供了模型解释分析，以确定预测性能的主要驱动因素。此外，我们还证明了模型在审核中的用途，可以减少标注错误的效率（例如，音乐流媒体应用中的40%效率提升）。这些结果证明了自动错误检测模型可以提供显著改善数据标注过程的效率和质量。因此，我们的发现对人类在ML过程中的循环提供了重要的洞察，并贡献到更广泛的人类-在-loop ML领域。
</details></li>
</ul>
<hr>
<h2 id="Are-Personalized-Stochastic-Parrots-More-Dangerous-Evaluating-Persona-Biases-in-Dialogue-Systems"><a href="#Are-Personalized-Stochastic-Parrots-More-Dangerous-Evaluating-Persona-Biases-in-Dialogue-Systems" class="headerlink" title="Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems"></a>Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05280">http://arxiv.org/abs/2310.05280</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uclanlp/persona-biases">https://github.com/uclanlp/persona-biases</a></li>
<li>paper_authors: Yixin Wan, Jieyu Zhao, Aman Chadha, Nanyun Peng, Kai-Wei Chang</li>
<li>for: 这项研究旨在探讨对话系统中使用人物模拟的风险，以及这些风险如何影响对话系统的性能和用户体验。</li>
<li>methods: 本研究使用UNIVERSALPERSONA数据集，对四种不同的对话系统进行了比较，并采用了五种评价指标来评估对话系统中人物模拟的偏见。</li>
<li>results: 研究发现，使用人物模拟在对话系统中存在许多偏见，包括不够尊重和不当的回应，这些偏见可能会对用户造成困惑和不良影响。<details>
<summary>Abstract</summary>
Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. We define generic personas to represent demographic groups, such as "an Asian person", whereas specific personas may take the form of specific popular Asian names like "Yumi". While the adoption of personas enriches user experiences by making dialogue systems more engaging and approachable, it also casts a shadow of potential risk by exacerbating social biases within model responses, thereby causing societal harm through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of dialogue models' harmful behaviors contingent upon the personas they adopt. We categorize persona biases into biases in harmful expression and harmful agreement, and establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to investigate persona biases by experimenting with UNIVERSALPERSONA, a systematically constructed persona dataset encompassing various types of both generic and specific model personas. Through benchmarking on four different models -- including Blender, ChatGPT, Alpaca, and Vicuna -- our study uncovers significant persona biases in dialogue systems. Our findings also underscore the pressing need to revisit the use of personas in dialogue agents to ensure safe application.
</details>
<details>
<summary>摘要</summary>
现代大语言模型可以遵循自由式指令，包括模仿 generic或特定民族人物的对话。我们定义了一些通用的人物类型来表示民族组成部分，例如“一个亚洲人”，而特定的人物可能是具体的受欢迎的亚洲名字“玉米”。虽然采用人物可以增加对话系统的互动性和可接近性，但也可能扩大社会偏见在模型响应中，从而对社会造成伤害。在这篇论文中，我们系统地研究了“人物偏见”，定义为对话模型的危险行为与人物相关的敏感性。我们分类人物偏见为表达偏见和同意偏见，并设计了全面的评价框架来测试人物偏见的五个方面：不礼貌、继续恶势力、尊敬、刻板印象同意和恶势力同意。此外，我们还提出了使用 UNIVERSALPERSONA 系统构建的人物数据集，包括各种通用和特定的模型人物。通过对四种不同的模型（包括 Blender、ChatGPT、Alpaca 和 Vicuna）进行比较，我们的研究发现了对话系统中的人物偏见。我们的发现也警示了对人物的使用以确保安全应用的需要。
</details></li>
</ul>
<hr>
<h2 id="Measuring-reasoning-capabilities-of-ChatGPT"><a href="#Measuring-reasoning-capabilities-of-ChatGPT" class="headerlink" title="Measuring reasoning capabilities of ChatGPT"></a>Measuring reasoning capabilities of ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05993">http://arxiv.org/abs/2310.05993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Groza<br>for: 这个论文的目的是量化 chatGPT 在逻辑任务中生成的逻辑错误。methods: 作者使用了 chatGPT 解决 144 个逻辑题目，并使用 Prover9 和 Mace4 来验证解决方案。results: 作者发现 chatGPT 只能正确解决 7% 的题目，而 BARD 则可以正确解决 5% 的题目。此外，作者还发现 chatGPT 生成的解决方案中包含了 67 种逻辑错误，平均每个逻辑任务中包含 7 种错误。<details>
<summary>Abstract</summary>
I shall quantify the logical faults generated by ChatGPT when applied to reasoning tasks. For experiments, I use the 144 puzzles from the library \url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\cite{groza:fol}. The library contains puzzles of various types, including arithmetic puzzles, logical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling puzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct solutions for these puzzles were checked using the theorem prover Prover9~\cite{mccune2005release} and the finite models finder Mace4~\cite{mccune2003mace4} based on human-modelling in Equational First Order Logic. A first output of this study is the benchmark of 100 logical puzzles. For this dataset ChatGPT provided both correct answer and justification for 7\% only. %, while BARD for 5\%. Since the dataset seems challenging, the researchers are invited to test the dataset on more advanced or tuned models than ChatGPT3.5 with more crafted prompts. A second output is the classification of reasoning faults conveyed by ChatGPT. This classification forms a basis for a taxonomy of reasoning faults generated by large language models. I have identified 67 such logical faults, among which: inconsistencies, implication does not hold, unsupported claim, lack of commonsense, wrong justification. The 100 solutions generated by ChatGPT contain 698 logical faults. That is on average, 7 fallacies for each reasoning task. A third ouput is the annotated answers of the ChatGPT with the corresponding logical faults. Each wrong statement within the ChatGPT answer was manually annotated, aiming to quantify the amount of faulty text generated by the language model. On average, 26.03\% from the generated text was a logical fault.
</details>
<details>
<summary>摘要</summary>
A second output is the classification of reasoning faults conveyed by ChatGPT. This classification forms a basis for a taxonomy of reasoning faults generated by large language models. I have identified 67 such logical faults, including inconsistencies, implications that do not hold, unsupported claims, lack of common sense, and wrong justifications. The 100 solutions generated by ChatGPT contain 698 logical faults, averaging 7 fallacies for each reasoning task.A third output is the annotated answers of ChatGPT with the corresponding logical faults. Each wrong statement within the ChatGPT answer was manually annotated to quantify the amount of faulty text generated by the language model. On average, 26.03% of the generated text was found to contain logical faults.Note:[1] Groza, F. (2009). Puzzles for Logical Reasoning. Retrieved from <https://users.utcluj.ro/~agroza/puzzles/maloga>[2] McCune, A. (2005). Prover9: A System for Automatic Theorem Proving. Retrieved from <https://www.cs.umd.edu/projects/prover9/>[3] McCune, A. (2003). Mace4: A System for Automatic Finite Model Generation. Retrieved from <https://www.cs.umd.edu/projects/mace4/>
</details></li>
</ul>
<hr>
<h2 id="Transforming-Pixels-into-a-Masterpiece-AI-Powered-Art-Restoration-using-a-Novel-Distributed-Denoising-CNN-DDCNN"><a href="#Transforming-Pixels-into-a-Masterpiece-AI-Powered-Art-Restoration-using-a-Novel-Distributed-Denoising-CNN-DDCNN" class="headerlink" title="Transforming Pixels into a Masterpiece: AI-Powered Art Restoration using a Novel Distributed Denoising CNN (DDCNN)"></a>Transforming Pixels into a Masterpiece: AI-Powered Art Restoration using a Novel Distributed Denoising CNN (DDCNN)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05270">http://arxiv.org/abs/2310.05270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sankar B., Mukil Saravanan, Kalaivanan Kumar, Siri Dubbaka</li>
<li>For:  restore deteriorated artworks accurately and efficiently* Methods: 使用深度学习和计算机视觉技术，创造一种基于DDCNN的混合模型，可以根据不同的损害程度和类型进行自适应 restauration* Results: 实验表明，该方法可以有效地纠正损害，并保持细节的精度，提高了艺术品的 restauration 质量，比传统方法有更大的超越<details>
<summary>Abstract</summary>
Art restoration is crucial for preserving cultural heritage, but traditional methods have limitations in faithfully reproducing original artworks while addressing issues like fading, staining, and damage. We present an innovative approach using deep learning, specifically Convolutional Neural Networks (CNNs), and Computer Vision techniques to revolutionize art restoration. We start by creating a diverse dataset of deteriorated art images with various distortions and degradation levels. This dataset trains a Distributed Denoising CNN (DDCNN) to remove distortions while preserving intricate details. Our method is adaptable to different distortion types and levels, making it suitable for various deteriorated artworks, including paintings, sketches, and photographs. Extensive experiments demonstrate our approach's efficiency and effectiveness compared to other Denoising CNN models. We achieve a substantial reduction in distortion, transforming deteriorated artworks into masterpieces. Quantitative evaluations confirm our method's superiority over traditional techniques, reshaping the art restoration field and preserving cultural heritage. In summary, our paper introduces an AI-powered solution that combines Computer Vision and deep learning with DDCNN to restore artworks accurately, overcoming limitations and paving the way for future advancements in art restoration.
</details>
<details>
<summary>摘要</summary>
艺术修复是保护文化遗产的关键，但传统方法有限制，无法准确地复制原始艺术作品，同时解决抹涂、损坏等问题。我们提出了一种创新的方法，使用深度学习技术和计算机视觉技术，以推动艺术修复领域的 револю变。我们开始创建一个多样化的褪色艺术图像数据集，用于训练分布式滤清神经网络（DDCNN），以除掉抹涂而保留细节。我们的方法适用于不同类型和水平的抹涂，可以应用于不同的艺术作品，包括画作、素描和照片。我们的实验证明，我们的方法可以减少抹涂，将褪色艺术作品转化为名画。量化评估表明，我们的方法比传统方法更高效，重新定义艺术修复领域，并为未来的艺术修复领域提供了新的发展方向。总之，我们的论文介绍了一种通过计算机视觉和深度学习技术，使用 DDCNN 恢复艺术作品的准确方法，超越传统技术，开拓了未来艺术修复领域的新途径。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-A-Cutting-Edge-Survey-of-the-Latest-Advancements-and-Applications"><a href="#Federated-Learning-A-Cutting-Edge-Survey-of-the-Latest-Advancements-and-Applications" class="headerlink" title="Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications"></a>Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05269">http://arxiv.org/abs/2310.05269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Azim Akhtarshenas, Mohammad Ali Vahedifar, Navid Ayoobi, Behrouz Maham, Tohid Alizadeh, Sina Ebrahimi</li>
<li>for: 这份论文主要是为了探讨联盟学习（Federated Learning，FL）在机器学习系统中实现隐私安全性的可能性和挑战。</li>
<li>methods: 这份论文使用了分布式机器学习（Distributed Machine Learning）和封包技术来实现联盟学习，并且进行了评估和比较现有的FL应用，以评估其效率、精度和隐私保护。</li>
<li>results: 这份论文发现了联盟学习可以实现隐私安全性和成本效益，并且发现了一些未解决的问题和挑战，例如资料权益和安全性、资料分布和资料隐私保护等。<details>
<summary>Abstract</summary>
In the realm of machine learning (ML) systems featuring client-host connections, the enhancement of privacy security can be effectively achieved through federated learning (FL) as a secure distributed ML methodology. FL effectively integrates cloud infrastructure to transfer ML models onto edge servers using blockchain technology. Through this mechanism, it guarantees the streamlined processing and data storage requirements of both centralized and decentralized systems, with an emphasis on scalability, privacy considerations, and cost-effective communication. In current FL implementations, data owners locally train their models, and subsequently upload the outcomes in the form of weights, gradients, and parameters to the cloud for overall model aggregation. This innovation obviates the necessity of engaging Internet of Things (IoT) clients and participants to communicate raw and potentially confidential data directly with a cloud center. This not only reduces the costs associated with communication networks but also enhances the protection of private data. This survey conducts an analysis and comparison of recent FL applications, aiming to assess their efficiency, accuracy, and privacy protection. However, in light of the complex and evolving nature of FL, it becomes evident that additional research is imperative to address lingering knowledge gaps and effectively confront the forthcoming challenges in this field. In this study, we categorize recent literature into the following clusters: privacy protection, resource allocation, case study analysis, and applications. Furthermore, at the end of each section, we tabulate the open areas and future directions presented in the referenced literature, affording researchers and scholars an insightful view of the evolution of the field.
</details>
<details>
<summary>摘要</summary>
在机器学习（ML）系统中，通过联邦学习（FL）可以有效提高隐私安全性。FL可以将云基础设施与边缘服务器集成，通过块链技术实现模型传输。这种机制可以保证中央化和分布式系统之间的流畅处理和数据存储要求，同时强调扩展性、隐私考虑因素和效率沟通。现在的FL实现中，数据所有者在本地训练模型，然后将结果上传到云中进行总模型聚合。这种创新使得无需将互联网物联网（IoT）客户端和参与者直接与云中心进行明文和潜在敏感数据的直接交流，从而降低了通信网络成本并提高了隐私数据的保护。本文对现有的FL应用进行分析和比较，以评估其效率、准确率和隐私保护。然而，随着FL的复杂和不断演化，显然需要进一步的研究，以解决仍存的知识漏洞并有效地应对未来的挑战。在这个研究中，我们将 recens literature into以下类别：隐私保护、资源分配、案例分析和应用。此外，文章结尾附加了每个部分的开放领域和未来方向，为研究人员和学者提供了深入的视野，了解领域的演化。
</details></li>
</ul>
<hr>
<h2 id="A-Knowledge-Graph-Based-Search-Engine-for-Robustly-Finding-Doctors-and-Locations-in-the-Healthcare-Domain"><a href="#A-Knowledge-Graph-Based-Search-Engine-for-Robustly-Finding-Doctors-and-Locations-in-the-Healthcare-Domain" class="headerlink" title="A Knowledge Graph-Based Search Engine for Robustly Finding Doctors and Locations in the Healthcare Domain"></a>A Knowledge Graph-Based Search Engine for Robustly Finding Doctors and Locations in the Healthcare Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05258">http://arxiv.org/abs/2310.05258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayank Kejriwal, Hamid Haidarian, Min-Hsueh Chiu, Andy Xiang, Deep Shrestha, Faizan Javed</li>
<li>for: 这篇论文是为了解决医疗领域患者找寻医生和位置的搜索问题而写的。</li>
<li>methods: 该论文使用知识图（KG）来结合 semi-structured 数据的感知模型、自然语言处理技术和结构化查询语言 like SPARQL 和 Cypher 来提供强大的搜索引擎体系。</li>
<li>results:  Early results 表明，该方法可以对复杂查询提供明显更高的覆盖率，无需降低质量。<details>
<summary>Abstract</summary>
Efficiently finding doctors and locations is an important search problem for patients in the healthcare domain, for which traditional information retrieval methods tend not to work optimally. In the last ten years, knowledge graphs (KGs) have emerged as a powerful way to combine the benefits of gleaning insights from semi-structured data using semantic modeling, natural language processing techniques like information extraction, and robust querying using structured query languages like SPARQL and Cypher. In this short paper, we present a KG-based search engine architecture for robustly finding doctors and locations in the healthcare domain. Early results demonstrate that our approach can lead to significantly higher coverage for complex queries without degrading quality.
</details>
<details>
<summary>摘要</summary>
Traditional information retrieval methods tend not to work optimally for efficiently finding doctors and locations in the healthcare domain. In the last ten years, knowledge graphs (KGs) have emerged as a powerful way to combine the benefits of gleaning insights from semi-structured data using semantic modeling, natural language processing techniques like information extraction, and robust querying using structured query languages like SPARQL and Cypher. In this short paper, we present a KG-based search engine architecture for robustly finding doctors and locations in the healthcare domain. Early results demonstrate that our approach can lead to significantly higher coverage for complex queries without degrading quality.Here's the text in Traditional Chinese:传统的资讯搜寻方法在医疗领域中不太能够有效率地找到医生和位置。过去十年，知识图表（KGs）已经emerged as a powerful way to combine the benefits of gleaning insights from semi-structured data using semantic modeling, natural language processing techniques like information extraction, and robust querying using structured query languages like SPARQL and Cypher。在这篇短篇论文中，我们呈现了一个基于KG的搜索引擎架构，用于在医疗领域中强健地找到医生和位置。初步结果显示，我们的方法可以导致复杂的查询得到更高的覆盖率，而不会降低品质。
</details></li>
</ul>
<hr>
<h2 id="Persis-A-Persian-Font-Recognition-Pipeline-Using-Convolutional-Neural-Networks"><a href="#Persis-A-Persian-Font-Recognition-Pipeline-Using-Convolutional-Neural-Networks" class="headerlink" title="Persis: A Persian Font Recognition Pipeline Using Convolutional Neural Networks"></a>Persis: A Persian Font Recognition Pipeline Using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05255">http://arxiv.org/abs/2310.05255</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mehrdad-dev/persis">https://github.com/mehrdad-dev/persis</a></li>
<li>paper_authors: Mehrdad Mohammadian, Neda Maleki, Tobias Olsson, Fredrik Ahlgren</li>
<li>for: 这篇论文是为了解决视觉字体识别（VFR）系统中的波斯字体识别问题。</li>
<li>methods: 该论文使用卷积神经网络（CNN）来解决这个问题，并使用了新的公共可用数据集来训练模型。</li>
<li>results: 根据论文的结果，提出的管道可以达到78.0%的顶部准确率，89.1%的IDPL-PFOD数据集准确率，以及94.5%的KAFD数据集准确率。  Additionally, the average time spent in the entire pipeline for one sample of the proposed datasets is 0.54 seconds and 0.017 seconds for CPU and GPU, respectively.<details>
<summary>Abstract</summary>
What happens if we encounter a suitable font for our design work but do not know its name? Visual Font Recognition (VFR) systems are used to identify the font typeface in an image. These systems can assist graphic designers in identifying fonts used in images. A VFR system also aids in improving the speed and accuracy of Optical Character Recognition (OCR) systems. In this paper, we introduce the first publicly available datasets in the field of Persian font recognition and employ Convolutional Neural Networks (CNN) to address this problem. The results show that the proposed pipeline obtained 78.0% top-1 accuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the KAFD dataset. Furthermore, the average time spent in the entire pipeline for one sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU, respectively. We conclude that CNN methods can be used to recognize Persian fonts without the need for additional pre-processing steps such as feature extraction, binarization, normalization, etc.
</details>
<details>
<summary>摘要</summary>
如果我们在设计工作中遇到一种适合的字体，但是不知道它的名称，可以使用视觉字体识别（VFR）系统来识别字体类型。这些系统可以帮助图形设计师在图像中识别字体。VFR 系统还可以提高光学字符识别（OCR）系统的速度和准确性。在这篇论文中，我们介绍了字体识别领域的第一个公共可用数据集，并使用卷积神经网络（CNN）解决这个问题。结果显示，我们的提案的管道取得了78.0%的顶部一准确率，89.1%的IDPL-PFOD数据集和94.5%的KAFD数据集。此外，我们的整个管道中对一个样本的平均时间为0.54秒和0.017秒，分别是CPU和GPU上的。我们 conclude 的是，CNN 方法可以用来识别波斯字体，不需要额外的预处理步骤，如特征提取、二进制化、Normalization等。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Claim-Verification-via-Knowledge-Grounded-Reasoning-with-Large-Language-Models"><a href="#Explainable-Claim-Verification-via-Knowledge-Grounded-Reasoning-with-Large-Language-Models" class="headerlink" title="Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models"></a>Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05253">http://arxiv.org/abs/2310.05253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wang2226/folk">https://github.com/wang2226/folk</a></li>
<li>paper_authors: Haoran Wang, Kai Shu</li>
<li>for: 验证宣称的可靠性，对抗虚假信息的扩散。</li>
<li>methods: 使用First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning，无需人工标注数据，可以验证复杂的宣称，并生成可读的解释。</li>
<li>results: 在三个不同的数据集上，FOLK 已经超越强基eline，并且可以提供清晰的解释，帮助人工验证者更好地理解模型的决策过程。<details>
<summary>Abstract</summary>
Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>研究人员认为，确认说法的重要作用在抵御谎言中扮演着关键角色。 although existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on expensive human-annotated data. In addition, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning, which can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available.
</details></li>
</ul>
<hr>
<h2 id="In-Context-Convergence-of-Transformers"><a href="#In-Context-Convergence-of-Transformers" class="headerlink" title="In-Context Convergence of Transformers"></a>In-Context Convergence of Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05249">http://arxiv.org/abs/2310.05249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Huang, Yuan Cheng, Yingbin Liang</li>
<li>for: 这个论文研究了一层转换器在梯度下降训练下的学习动力学，以便在不需要参数调整的情况下解决未看过的任务。</li>
<li>methods: 该论文使用了梯度下降训练方法，并对一层转换器的软max注意力进行研究。</li>
<li>results: 研究发现，对于具有平衡或不平衡特征的数据，转换器在梯度下降训练下可以在不同阶段达到近Zero预测错误的finite-time收敛保证。<details>
<summary>Abstract</summary>
Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map. More notably, for data with imbalanced features, we show that the learning dynamics take a stage-wise convergence process, where the transformer first converges to a near-zero prediction error for the query tokens of dominant features, and then converges later to a near-zero prediction error for the query tokens of under-represented features, respectively via one and four training phases. Our proof features new techniques for analyzing the competing strengths of two types of attention weights, the change of which determines different training phases.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>现代机器学习中，变换器最近对许多领域进行了革命性的改变，其中一个吸引人的发现是它们在未经参数调整的情况下可以解决未看过的任务，这也激发了最近的理论研究，旨在理解变换器的在场景学习机制。然而，这些研究仅专注于线性变换器。在这项工作中，我们首先研究了一层变换器，通过梯度下降来学习线性函数类型。我们考虑了一种结构化数据模型，其中每个token是随机选择的特征向量集中的一个元素。对于具有平衡特征的数据，我们证明了在 finite-time 内 convergence guarantee，并且预测错误几乎为零。而对于具有不平衡特征的数据，我们显示了一个stage-wise convergence进程，变换器首先对查询符号的主要特征 converge 到 near-zero prediction error，然后在后四个训练阶段 convergence 到 under-represented 特征上的查询符号的 near-zero prediction error。我们的证明利用了一些新的分析技术，以确定不同训练阶段中的关键因素。
</details></li>
</ul>
<hr>
<h2 id="ChatRadio-Valuer-A-Chat-Large-Language-Model-for-Generalizable-Radiology-Report-Generation-Based-on-Multi-institution-and-Multi-system-Data"><a href="#ChatRadio-Valuer-A-Chat-Large-Language-Model-for-Generalizable-Radiology-Report-Generation-Based-on-Multi-institution-and-Multi-system-Data" class="headerlink" title="ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data"></a>ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05242">http://arxiv.org/abs/2310.05242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyang Zhong, Wei Zhao, Yutong Zhang, Yi Pan, Peixin Dong, Zuowei Jiang, Xiaoyan Kui, Youlan Shang, Li Yang, Yaonai Wei, Longtao Yang, Hao Chen, Huan Zhao, Yuxiao Liu, Ning Zhu, Yiwei Li, Yisong Wang, Jiaqi Yao, Jiaqi Wang, Ying Zeng, Lei He, Chao Zheng, Zhixue Zhang, Ming Li, Zhengliang Liu, Haixing Dai, Zihao Wu, Lu Zhang, Shu Zhang, Xiaoyan Cai, Xintao Hu, Shijie Zhao, Xi Jiang, Xin Zhang, Xiang Li, Dajiang Zhu, Lei Guo, Dinggang Shen, Junwei Han, Tianming Liu, Jun Liu, Tuo Zhang</li>
<li>For: 这个研究旨在解决医疗影像分析中的报告生成问题，以实现诊断过程中的量化分析。* Methods: 这个研究使用了大型自然语言模型（LLM），发展了一个适应器“ChatRadio-Valuer”，以自动生成医疗影像报告。* Results: 研究结果显示，ChatRadio-Valuer在医疗影像报告中诊断疾病的能力高于现有的模型，特别是与ChatGPT和GPT-4等模型相比。<details>
<summary>Abstract</summary>
Radiology report generation, as a key step in medical image analysis, is critical to the quantitative analysis of clinically informed decision-making levels. However, complex and diverse radiology reports with cross-source heterogeneity pose a huge generalizability challenge to the current methods under massive data volume, mainly because the style and normativity of radiology reports are obviously distinctive among institutions, body regions inspected and radiologists. Recently, the advent of large language models (LLM) offers great potential for recognizing signs of health conditions. To resolve the above problem, we collaborate with the Second Xiangya Hospital in China and propose ChatRadio-Valuer based on the LLM, a tailored model for automatic radiology report generation that learns generalizable representations and provides a basis pattern for model adaptation in sophisticated analysts' cases. Specifically, ChatRadio-Valuer is trained based on the radiology reports from a single institution by means of supervised fine-tuning, and then adapted to disease diagnosis tasks for human multi-system evaluation (i.e., chest, abdomen, muscle-skeleton, head, and maxillofacial $\&$ neck) from six different institutions in clinical-level events. The clinical dataset utilized in this study encompasses a remarkable total of \textbf{332,673} observations. From the comprehensive results on engineering indicators, clinical efficacy and deployment cost metrics, it can be shown that ChatRadio-Valuer consistently outperforms state-of-the-art models, especially ChatGPT (GPT-3.5-Turbo) and GPT-4 et al., in terms of the diseases diagnosis from radiology reports. ChatRadio-Valuer provides an effective avenue to boost model generalization performance and alleviate the annotation workload of experts to enable the promotion of clinical AI applications in radiology reports.
</details>
<details>
<summary>摘要</summary>
医学影像分析中的 radiology 报告生成是医疗决策中的关键步骤，但是复杂和多样的 radiology 报告带有跨源差异性，对当前方法来说是一个巨大普适性挑战。这是因为 radiology 报告的风格和标准性在不同机构、身体区域和 radiologist 之间存在显著差异。然而，最近的大语言模型（LLM）的出现带来了识别健康状况的潜在可能性。为解决这个问题，我们与中国第二医学院合作，并提出了基于 LLM 的 ChatRadio-Valuer 自动 radiology 报告生成模型，该模型学习普适表示和提供基本模式 для模型适应复杂分析员的情况。具体来说，ChatRadio-Valuer 通过单机构的 radiology 报告进行监督微调训练，然后在多个机构的疾病诊断任务中进行人类多系统评估。这些临床数据的总量为 \textbf{332,673} 个观察。根据工程指标、临床效果和部署成本度量，可以看出，ChatRadio-Valuer 在疾病诊断方面与现有模型，特别是 ChatGPT（GPT-3.5-Turbo）和 GPT-4 等模型，表现出色，尤其是在 radiology 报告中诊断疾病。ChatRadio-Valuer 为临床 AI 应用提供了一个有效的通路，以提高模型普适性性和减轻专家的标注工作负担，以便推动临床 AI 应用的普及。
</details></li>
</ul>
<hr>
<h2 id="MindfulDiary-Harnessing-Large-Language-Model-to-Support-Psychiatric-Patients’-Journaling"><a href="#MindfulDiary-Harnessing-Large-Language-Model-to-Support-Psychiatric-Patients’-Journaling" class="headerlink" title="MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients’ Journaling"></a>MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients’ Journaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05231">http://arxiv.org/abs/2310.05231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taewan Kim, Seolyeong Bae, Hyun Ah Kim, Su-woo Lee, Hwajung Hong, Chanmo Yang, Young-Ho Kim</li>
<li>for: 帮助心理病人每天记录经验，并帮助心理医生更好地理解患者的思想和日常情境。</li>
<li>methods: 使用大语言模型（LLM）和移动应用程序，实现了患者每天的自由对话记录，并遵循专业指导方针。</li>
<li>results: 经四周的场景研究，发现 MindfulDiary 可以帮助患者日常记录更详细和系统化，同时帮助心理医生更好地理解患者的思想和日常情境，有助于提高心理医疗效果。<details>
<summary>Abstract</summary>
In the mental health domain, Large Language Models (LLMs) offer promising new opportunities, though their inherent complexity and low controllability have raised questions about their suitability in clinical settings. We present MindfulDiary, a mobile journaling app incorporating an LLM to help psychiatric patients document daily experiences through conversation. Designed in collaboration with mental health professionals (MHPs), MindfulDiary takes a state-based approach to safely comply with the experts' guidelines while carrying on free-form conversations. Through a four-week field study involving 28 patients with major depressive disorder and five psychiatrists, we found that MindfulDiary supported patients in consistently enriching their daily records and helped psychiatrists better empathize with their patients through an understanding of their thoughts and daily contexts. Drawing on these findings, we discuss the implications of leveraging LLMs in the mental health domain, bridging the technical feasibility and their integration into clinical settings.
</details>
<details>
<summary>摘要</summary>
在心理健康领域，大型自然语言模型（LLM）提供了新的机遇，但其内置的复杂性和控制性问题引起了许多关于其在临床设置中适用性的问题。我们介绍了一款名为 MindfulDiary的移动日记应用程序，该应用程序通过与心理医生（MHP）合作，使用 LLM 帮助心理病人每天记录他们的经验。我们在28名主观抑郁症患者和5名心理医生参与的四周实验中发现，MindfulDiary 可以帮助患者日常记录更加详细，并帮助心理医生更好地理解他们的患者的思想和日常背景。根据这些发现，我们讨论了在心理健康领域利用 LLM 的意义，把技术可行性和其在临床设置中的集成相结合。
</details></li>
</ul>
<hr>
<h2 id="Physics-aware-Machine-Learning-Revolutionizes-Scientific-Paradigm-for-Machine-Learning-and-Process-based-Hydrology"><a href="#Physics-aware-Machine-Learning-Revolutionizes-Scientific-Paradigm-for-Machine-Learning-and-Process-based-Hydrology" class="headerlink" title="Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology"></a>Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05227">http://arxiv.org/abs/2310.05227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingsong Xu, Yilei Shi, Jonathan Bamber, Ye Tuo, Ralf Ludwig, Xiao Xiang Zhu<br>for:physics-aware machine learning (PaML) is introduced as a transformative approach to overcome the barrier between hydrology and machine learning, and to revolutionize both fields.methods:the review includes a comprehensive analysis of existing PaML methodologies that integrate prior physical knowledge or physics-based modeling into machine learning, including physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning.results:the review highlights the most promising and challenging directions for different objectives and PaML methods in hydrology, including rainfall-runoff hydrological processes and hydrodynamic processes. Additionally, a new PaML-based hydrology platform, termed HydroPML, is released as a foundation for hydrological applications, which enhances the explainability and causality of machine learning and lays the groundwork for the digital water cycle’s realization.<details>
<summary>Abstract</summary>
Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypotheses, accelerating insights from big data and fostering scientific discoveries. We first conduct a systematic review of hydrology in PaML, including rainfall-runoff hydrological processes and hydrodynamic processes, and highlight the most promising and challenging directions for different objectives and PaML methods. Finally, a new PaML-based hydrology platform, termed HydroPML, is released as a foundation for hydrological applications. HydroPML enhances the explainability and causality of ML and lays the groundwork for the digital water cycle's realization. The HydroPML platform is publicly available at https://hydropml.github.io/.
</details>
<details>
<summary>摘要</summary>
Accurate hydrological understanding和水ecycle prediction是管理水资源的科学和社会挑战中的关键，尤其是在人类活动导致的气候变化的影响下。现有的评论主要集中在机器学习（ML）的发展中，但是有一个明确的分界线：水文和ML为两个不同的思维框架。我们介绍了一种将物理知识integrated into ML的新方法，即物理意识ML（PaML），以超越这一障碍并重塑两个领域。我们对PaML方法进行了系统性的分析，包括物理数据驱动ML、物理信息驱动ML、物理嵌入ML和物理意识混合学习。PaML方法可以加速大数据的学习和探索，并促进科学发现。我们首先对PaML在水文领域进行了系统性的评论，包括雨水径流过程和 hidrodynamic过程，并将不同目标和PaML方法中最有前途和挑战的方向 highlighted。最后，我们发布了一个基于PaML的水文平台，称为HydroPML，以提高ML的解释性和因果关系，并为数字水ecycle的实现奠定基础。HydroPML平台publicly available at <https://hydropml.github.io/>。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Semiotics-Networks-Representing-Awareness"><a href="#Interpretable-Semiotics-Networks-Representing-Awareness" class="headerlink" title="Interpretable Semiotics Networks Representing Awareness"></a>Interpretable Semiotics Networks Representing Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05212">http://arxiv.org/abs/2310.05212</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Kupeev, Eyal Nitcany</li>
<li>for: 这个论文描述了一种计算模型，用于跟踪和模拟人类对物体的感知和communication中的表达。</li>
<li>methods: 该模型包括两个关键组件（’observed’和’seen’），与计算机视觉术语(‘encoding’和’decoding’)相关。这些元素结合形成了 semiotic networks，用于模拟人类对物体的感知和communication中的意识。</li>
<li>results: 作者在多个实验中证明了这个模型的可见性，并且在小训练数据集上，该模型的复合网络超过了单独的分类网络的性能。未来的工作将利用这个模型，以更好地理解人类communication和个人表达。<details>
<summary>Abstract</summary>
Humans perceive objects daily and communicate their perceptions using various channels. Here, we describe a computational model that track and simulate objects' perception, and their representations as they pass in communication.   We describe two key components of our internal representation ('observed' and 'seen') and relate them to familiar computer vision terms (encoding and decoding). These elements joined together to form semiotic networks, which simulate awareness in object perception and human communication.   Nowadays, most neural networks are uninterpretable. On the other hand, our model is free from this disadvantages. We performed several experiments and demonstrated the visibility of our model.   We describe how our network may be used as preprocessing unit to any classification network. In our experiments the compound network overperforms in average the classification network at datasets with small training data.   Future work would leverage our model to gain better understanding of human communications and personal representations.
</details>
<details>
<summary>摘要</summary>
人们日常接触物体，通过不同的渠道传达自己的感知。我们描述了一种计算模型，可以跟踪和模拟物体的感知和表达，以及它们在交流中的表现。我们描述了两个关键组成部分('观察'和'看到')，与 familar computer vision terms（编码和解码）相关。这些元素结合形成了 semiotic networks，可以模拟人类对物体感知和communication的意识。现在，大多数神经网络都是不可解释的。然而，我们的模型免受这些缺点。我们进行了多个实验，并证明了我们的模型的可见性。我们描述了如何使用我们的网络作为任何分类网络的预处理单元，并在实验中发现了compound network在小训练数据集上的超越性。未来的工作将利用我们的模型，更好地理解人类通信和个人表示。
</details></li>
</ul>
<hr>
<h2 id="TILFA-A-Unified-Framework-for-Text-Image-and-Layout-Fusion-in-Argument-Mining"><a href="#TILFA-A-Unified-Framework-for-Text-Image-and-Layout-Fusion-in-Argument-Mining" class="headerlink" title="TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining"></a>TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05210">http://arxiv.org/abs/2310.05210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/tilfa">https://github.com/hkust-knowcomp/tilfa</a></li>
<li>paper_authors: Qing Zong, Zhaowei Wang, Baixuan Xu, Tianshi Zheng, Haochen Shi, Weiqi Wang, Yangqiu Song, Ginny Y. Wong, Simon See</li>
<li>for: 本研究旨在分析作者的立场（Argument Mining）</li>
<li>methods: 该研究使用了一种新的框架—TILFA（约文本、图像和布局融合框架），可以处理混合数据（文本和图像），并且可以理解文本以及检测图像中的光学字符和布局细节</li>
<li>results: 该模型在Argumentative Stance Classification子 зада务中显著超过了现有的基eline，为知识共享（KnowComp）团队赢得了第一名<details>
<summary>Abstract</summary>
A main goal of Argument Mining (AM) is to analyze an author's stance. Unlike previous AM datasets focusing only on text, the shared task at the 10th Workshop on Argument Mining introduces a dataset including both text and images. Importantly, these images contain both visual elements and optical characters. Our new framework, TILFA (A Unified Framework for Text, Image, and Layout Fusion in Argument Mining), is designed to handle this mixed data. It excels at not only understanding text but also detecting optical characters and recognizing layout details in images. Our model significantly outperforms existing baselines, earning our team, KnowComp, the 1st place in the leaderboard of Argumentative Stance Classification subtask in this shared task.
</details>
<details>
<summary>摘要</summary>
主要目标之一的Argument Mining（AM）是分析作者的态度。与过去的AM数据集仅专注于文本的情况不同，这个共同任务在10个Argument Mining工作坊中引入了包括文本和图像的数据集。重要的是，这些图像包含视觉元素和光学字符。我们的新框架TILFA（文本、图像和布局融合在Argument Mining中的一体化框架）针对这种混合数据进行处理。它不仅能够理解文本，还能探测光学字符和图像中的布局细节。我们的模型在Argumentative Stance Classification子任务中显著超越了现有的基线，让我们的团队 KnowComp 在领导板块中获得第一名。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Laws-of-RoPE-based-Extrapolation"><a href="#Scaling-Laws-of-RoPE-based-Extrapolation" class="headerlink" title="Scaling Laws of RoPE-based Extrapolation"></a>Scaling Laws of RoPE-based Extrapolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05209">http://arxiv.org/abs/2310.05209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/OpenLMLab/scaling-rope">https://github.com/OpenLMLab/scaling-rope</a></li>
<li>paper_authors: Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, Dahua Lin</li>
<li>for: 本文主要研究了基于Rotary Position Embedding（RoPE）的大型自然语言模型（LLM）的推断能力。</li>
<li>methods: 本文提出了一种基于RoPE的推断方法，包括修改RoPE的基数和提供长文本练习。</li>
<li>results: 本文在16K训练长度下，通过调整RoPE的基数和练习文本长度，实现了在1000000上下文长度内的推断。同时，本文还提出了一种periodic perspective下的扩展法则，以描述推断性能与基数和练习文本长度之间的关系。<details>
<summary>Abstract</summary>
The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \textbf{\textit{Scaling Laws of RoPE-based Extrapolation}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \textbf{\textit{critical dimension for extrapolation}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.
</details>
<details>
<summary>摘要</summary>
Currently, the ability of Large Language Models (LLMs) to extrapolate is a topic of great interest. The mainstream approach to improving extrapolation with LLMs is to modify Rotary Position Embedding (RoPE) by replacing the rotary base of $\theta_n={10000}^{-2n/d}$ with a larger value and providing longer fine-tuning text. In this study, we find that fine-tuning a RoPE-based LLM with a smaller or larger base in the pre-training context length can significantly enhance its extrapolation performance. We then propose the \textbf{\textit{Scaling Laws of RoPE-based Extrapolation}, a unified framework from a periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \textbf{\textit{critical dimension for extrapolation}. Furthermore, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.Note:* "Rotary Position Embedding" (RoPE) is translated as "旋转位嵌入" (Fánzàng wèi yù) in Simplified Chinese.* "Large Language Models" (LLMs) is translated as "大型语言模型" (dàxí yǔyán módelǐ) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Zero-shot-Coordination-Capability-with-Behavior-Preferring-Partners"><a href="#Quantifying-Zero-shot-Coordination-Capability-with-Behavior-Preferring-Partners" class="headerlink" title="Quantifying Zero-shot Coordination Capability with Behavior Preferring Partners"></a>Quantifying Zero-shot Coordination Capability with Behavior Preferring Partners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05208">http://arxiv.org/abs/2310.05208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xihuai Wang, Shao Zhang, Wenhao Zhang, Wentao Dong, Jingxiao Chen, Ying Wen, Weinan Zhang</li>
<li>for: 评估 Zero-shot coordination（ZSC）能力的可靠、全面和高效的评估方法。</li>
<li>methods: 提出了一种基于“理想完整”评估伙伴的评估方法，包括构建“完整”评估伙伴和Multi-dimensional度量指标BR-Prox。</li>
<li>results: 使用提出的评估方法重新评估了强大的ZSC方法在Overcooked环境中的性能，结果显示一些最常用的布局下，不同ZSC方法的性能差异不明显。此外，评估的ZSC方法需要生成更多和更高性能的训练伙伴。<details>
<summary>Abstract</summary>
Zero-shot coordination (ZSC) is a new challenge focusing on generalizing learned coordination skills to unseen partners. Existing methods train the ego agent with partners from pre-trained or evolving populations. The agent's ZSC capability is typically evaluated with a few evaluation partners, including human and agent, and reported by mean returns. Current evaluation methods for ZSC capability still need to improve in constructing diverse evaluation partners and comprehensively measuring the ZSC capability. We aim to create a reliable, comprehensive, and efficient evaluation method for ZSC capability. We formally define the ideal 'diversity-complete' evaluation partners and propose the best response (BR) diversity, which is the population diversity of the BRs to the partners, to approximate the ideal evaluation partners. We propose an evaluation workflow including 'diversity-complete' evaluation partners construction and a multi-dimensional metric, the Best Response Proximity (BR-Prox) metric. BR-Prox quantifies the ZSC capability as the performance similarity to each evaluation partner's approximate best response, demonstrating generalization capability and improvement potential. We re-evaluate strong ZSC methods in the Overcooked environment using the proposed evaluation workflow. Surprisingly, the results in some of the most used layouts fail to distinguish the performance of different ZSC methods. Moreover, the evaluated ZSC methods must produce more diverse and high-performing training partners. Our proposed evaluation workflow calls for a change in how we efficiently evaluate ZSC methods as a supplement to human evaluation.
</details>
<details>
<summary>摘要</summary>
Zero-shot coordination (ZSC) 是一个新的挑战，旨在将已经学习的协调技能应用到未见过的伙伴上。现有的方法将自己作为主体Agent训练的伙伴来自预先训练或进化的人类和机器人 population。主体Agent的 ZSC 能力通常是通过一些评估伙伴，包括人类和机器人，并由平均回应报告。现有的评估方法 для ZSC 能力仍然需要改进，以建立多样化的评估伙伴和全面地衡量 ZSC 能力。我们希望创建一个可靠、全面和高效的评估方法。我们正式定义了理想的 '多样化完整' 评估伙伴，并提出了最佳回应多样性（BR 多样性），它是评估伙伴的 Population 多样性的最佳回应。我们提出了一个评估工作流程，包括 '多样化完整' 评估伙伴的建构和多维度度量，即最佳回应距离度量（BR-Prox）。BR-Prox 量化 ZSC 能力为对每个评估伙伴的近似最佳回应的性能相似度，显示了扩展性和改进潜力。我们在 Overcooked 环境中重新评估了强大 ZSC 方法，结果显示，在一些最常用的布局中，不能区分不同 ZSC 方法的表现。此外，评估 ZSC 方法的伙伴必须生成更多和更高性能的训练伙伴。我们的提出的评估工作流程将对 ZSC 方法的评估作为补充 human 评估。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Facial-Action-Unit-Detection-Through-Jointly-Learning-Facial-Landmark-Detection-and-Domain-Separation-and-Reconstruction"><a href="#Boosting-Facial-Action-Unit-Detection-Through-Jointly-Learning-Facial-Landmark-Detection-and-Domain-Separation-and-Reconstruction" class="headerlink" title="Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction"></a>Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05207">http://arxiv.org/abs/2310.05207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqiao Shang, Li Yu</li>
<li>for: 这篇研究旨在提出一个新的 facial action unit (AU) 检测框架，以便在无标的面部图像中进行supervised检测。</li>
<li>methods: 这篇研究使用多任务学习，将AU领域分类和重建、面部标志检测共享同structural facial extraction模组的 Parameters。此外，提出了一个基于对照学习的新Feature alignment方案，加入了四个中途supervisors，以促进特征重建过程。</li>
<li>results: 实验结果显示，该方法在两个benchmark上具有较高的精度和稳定性，较之前所有方法有所提高。<details>
<summary>Abstract</summary>
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GEAR-A-GPU-Centric-Experience-Replay-System-for-Large-Reinforcement-Learning-Models"><a href="#GEAR-A-GPU-Centric-Experience-Replay-System-for-Large-Reinforcement-Learning-Models" class="headerlink" title="GEAR: A GPU-Centric Experience Replay System for Large Reinforcement Learning Models"></a>GEAR: A GPU-Centric Experience Replay System for Large Reinforcement Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05205">http://arxiv.org/abs/2310.05205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bigrl-team/gear">https://github.com/bigrl-team/gear</a></li>
<li>paper_authors: Hanjing Wang, Man-Kit Sit, Congjie He, Ying Wen, Weinan Zhang, Jun Wang, Yaodong Yang, Luo Mai</li>
<li>for: 这篇论文旨在开发一种分布式、GPU-中心的经验回忆系统（GEAR），用于执行扩展的强化学习（RL），并使用大 sequences 模型（如 transformers）。</li>
<li>methods: GEAR 使用了一种优化的内存管理策略，使得 GPU 服务器的内存资源（包括主机内存和设备内存）可以有效地管理经验数据。此外，它还实现了分布式的 GPU 设备来快速执行不同的经验选择策略，从而缓解计算瓶颈。 GEAR 还使用了 GPU 加速器来收集经验数据，并使用零复制访问主机内存和远程指定内存访问来提高通信效率。</li>
<li>results: 根据集群实验结果，GEAR 可以与 Reverb 相比，在训练 state-of-the-art 大 RL 模型时达到6倍的性能水平。<details>
<summary>Abstract</summary>
This paper introduces a distributed, GPU-centric experience replay system, GEAR, designed to perform scalable reinforcement learning (RL) with large sequence models (such as transformers). With such models, existing systems such as Reverb face considerable bottlenecks in memory, computation, and communication. GEAR, however, optimizes memory efficiency by enabling the memory resources on GPU servers (including host memory and device memory) to manage trajectory data. Furthermore, it facilitates decentralized GPU devices to expedite various trajectory selection strategies, circumventing computational bottlenecks. GEAR is equipped with GPU kernels capable of collecting trajectories using zero-copy access to host memory, along with remote-directed-memory access over InfiniBand, improving communication efficiency. Cluster experiments have shown that GEAR can achieve performance levels up to 6x greater than Reverb when training state-of-the-art large RL models. GEAR is open-sourced at https://github.com/bigrl-team/gear.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了一种分布式、GPU中心的经验回放系统GEAR，用于执行可扩展的 reinforcement learning（RL），并且可以使用大型序列模型（如转换器）。现有的系统如Reverb，在内存、计算和通信方面都会遇到严重的瓶颈。然而，GEAR通过启用 GPU 服务器上的内存资源（包括主机内存和设备内存）来管理轨迹数据，从而提高了内存效率。此外，它还可以让分布式的 GPU 设备优先级化不同的轨迹选择策略，以避免计算瓶颈。GEAR 具有可收集轨迹的 GPU kernels，使用零复制访问主机内存，以及通过 InfiniBand 进行远程指定内存访问，以提高通信效率。在分布式集群实验中，GEAR 可以与 Reverb 训练state-of-the-art 大型 RL 模型时， achieve 性能水平高达 6 倍。GEAR 开源在 <https://github.com/bigrl-team/gear>。
</details></li>
</ul>
<hr>
<h2 id="GMMFormer-Gaussian-Mixture-Model-based-Transformer-for-Efficient-Partially-Relevant-Video-Retrieval"><a href="#GMMFormer-Gaussian-Mixture-Model-based-Transformer-for-Efficient-Partially-Relevant-Video-Retrieval" class="headerlink" title="GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient Partially Relevant Video Retrieval"></a>GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient Partially Relevant Video Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05195">http://arxiv.org/abs/2310.05195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia</li>
<li>For: This paper is written for partially relevant video retrieval (PRVR), which aims to find untrimmed videos containing pertinent moments in a database.* Methods: The paper proposes a novel method called GMMFormer, which models clip representations implicitly using a Gaussian-Mixture-Model (GMM) and Transformer architecture. The method incorporates Gaussian-Mixture-Model constraints during frame interactions to focus each frame on its adjacent frames, generating representations that contain multi-scale clip information.* Results: The paper demonstrates the superiority and efficiency of GMMFormer through extensive experiments on three large-scale video datasets (TVR, ActivityNet Captions, and Charades-STA). The results show that GMMFormer outperforms existing PRVR methods and achieves better efficiency by reducing the storage overhead and improving the embedding space.<details>
<summary>Abstract</summary>
Given a text query, partially relevant video retrieval (PRVR) seeks to find untrimmed videos containing pertinent moments in a database. For PRVR, clip modeling is essential to capture the partial relationship between texts and videos. Current PRVR methods adopt scanning-based clip construction to achieve explicit clip modeling, which is information-redundant and requires a large storage overhead. To solve the efficiency problem of PRVR methods, this paper proposes GMMFormer, a \textbf{G}aussian-\textbf{M}ixture-\textbf{M}odel based Trans\textbf{former} which models clip representations implicitly. During frame interactions, we incorporate Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames instead of the whole video. Then generated representations will contain multi-scale clip information, achieving implicit clip modeling. In addition, PRVR methods ignore semantic differences between text queries relevant to the same video, leading to a sparse embedding space. We propose a query diverse loss to distinguish these text queries, making the embedding space more intensive and contain more semantic information. Extensive experiments on three large-scale video datasets (\ie, TVR, ActivityNet Captions, and Charades-STA) demonstrate the superiority and efficiency of GMMFormer.
</details>
<details>
<summary>摘要</summary>
During frame interactions, we incorporate Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames instead of the whole video. This allows generated representations to contain multi-scale clip information, achieving implicit clip modeling. Additionally, PRVR methods ignore semantic differences between text queries relevant to the same video, leading to a sparse embedding space. We propose a query diverse loss to distinguish these text queries, making the embedding space more intense and contain more semantic information.Extensive experiments on three large-scale video datasets (TVR, ActivityNet Captions, and Charades-STA) demonstrate the superiority and efficiency of GMMFormer.
</details></li>
</ul>
<hr>
<h2 id="Factuality-Challenges-in-the-Era-of-Large-Language-Models"><a href="#Factuality-Challenges-in-the-Era-of-Large-Language-Models" class="headerlink" title="Factuality Challenges in the Era of Large Language Models"></a>Factuality Challenges in the Era of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05189">http://arxiv.org/abs/2310.05189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, Giovanni Zagni</li>
<li>for: 本研究旨在探讨Generative AI技术的发展以及其对社会的影响，尤其是LLMs技术的潜在的威胁和风险。</li>
<li>methods: 本研究采用了文献综述和讨论的方法，检视了现有的LLMs技术和其应用场景，并分析了这些技术的潜在的威胁和风险。</li>
<li>results: 本研究发现了一些LLMs技术的潜在威胁和风险，包括生成假信息和假 profiles，以及恶意利用这些技术来欺诈用户。同时，本研究还提出了一些可能的解决方案，如实施技术审核和评估机制，提高用户的AI理解水平，以及进行更多的研究和规范。<details>
<summary>Abstract</summary>
The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as "hallucinations." Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to shed light on navigating various aspects of veracity in the era of generative AI.
</details>
<details>
<summary>摘要</summary>
LLM（大语言模型）技术的出现，如OpenAI的ChatGPT、Microsoft的Bing Chat以及Google的Bard，吸引了广泛的公众关注。这些极其有用、自然 звуча的工具表现出了对自然语言生成的重要进步，但它们往往会生成错误、误导性的内容，通常被称为“幻见”。此外，LLM可能会被恶用于黑客活动，如大规模生成假 pero Credible-sounding内容和 Profile。这对社会带来了误导用户的风险，以及假信息的扩散。为了面对这些挑战，我们需要从事实核查、法规改革以及人工智能文化培训等方面来解决这些问题。我们希望通过识别风险、危机点以及可行的解决方案，为在生成AI时的真实性提供指南。
</details></li>
</ul>
<hr>
<h2 id="Evolutionary-Retrosynthetic-Route-Planning"><a href="#Evolutionary-Retrosynthetic-Route-Planning" class="headerlink" title="Evolutionary Retrosynthetic Route Planning"></a>Evolutionary Retrosynthetic Route Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05186">http://arxiv.org/abs/2310.05186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Zhang, Hao Hao, Xiao He, Shuanhu Gao, Aimin Zhou</li>
<li>for: 本研究目的是提出一种基于进化算法的多步反Synthesis路径规划方法，以解决现有的反Synthesis问题。</li>
<li>methods: 该方法首先将反Synthesis问题转化为优化问题，定义搜索空间和操作。此外，为提高搜索效率， parallel 策略被实现。</li>
<li>results: 对四种产品的实验结果表明，相比较 Monte Carlo tree search 算法，EA 可以Significantly 减少单步模型的调用数（均减少53.9%），搜索三个解决方案的时间减少83.9%，并同时提高可行搜索路径的数量（增加5倍）。<details>
<summary>Abstract</summary>
Molecular retrosynthesis is a significant and complex problem in the field of chemistry, however, traditional manual synthesis methods not only need well-trained experts but also are time-consuming. With the development of big data and machine learning, artificial intelligence (AI) based retrosynthesis is attracting more attention and is becoming a valuable tool for molecular retrosynthesis. At present, Monte Carlo tree search is a mainstream search framework employed to address this problem. Nevertheless, its search efficiency is compromised by its large search space. Therefore, we propose a novel approach for retrosynthetic route planning based on evolutionary optimization, marking the first use of Evolutionary Algorithm (EA) in the field of multi-step retrosynthesis. The proposed method involves modeling the retrosynthetic problem into an optimization problem, defining the search space and operators. Additionally, to improve the search efficiency, a parallel strategy is implemented. The new approach is applied to four case products, and is compared with Monte Carlo tree search. The experimental results show that, in comparison to the Monte Carlo tree search algorithm, EA significantly reduces the number of calling single-step model by an average of 53.9%. The time required to search three solutions decreased by an average of 83.9%, and the number of feasible search routes increases by 5 times.
</details>
<details>
<summary>摘要</summary>
分子逆synthesis是化学领域中的一个重要和复杂问题，但传统的手动合成方法不仅需要高水平的专业人员，还需要很长的时间。随着大数据和机器学习的发展，人工智能（AI）基于的逆synthesis在这一问题上吸引了更多的注意力，成为化学领域的一种有价值的工具。目前，蒙特卡洛tree搜索是逆synthesis搜索框架的主流，但它的搜索效率受到搜索空间的限制。因此，我们提出了一种基于进化优化的新方法，标志着多步逆synthesis中Evolutionary Algorithm（EA）的首次应用。该方法包括将逆synthesis问题转化为优化问题，定义搜索空间和运算符。此外，为了提高搜索效率，并行策略被实现。新方法在四种case продуkttest中应用，并与蒙特卡洛tree搜索进行比较。实验结果表明，相比蒙特卡洛tree搜索算法，EA可以平均减少单步模型的呼び出数量53.9%，搜索三个解决方案所需的时间减少83.9%，并同时提高可行搜索路径的数量5倍。
</details></li>
</ul>
<hr>
<h2 id="Text2NKG-Fine-Grained-N-ary-Relation-Extraction-for-N-ary-relational-Knowledge-Graph-Construction"><a href="#Text2NKG-Fine-Grained-N-ary-Relation-Extraction-for-N-ary-relational-Knowledge-Graph-Construction" class="headerlink" title="Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction"></a>Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05185">http://arxiv.org/abs/2310.05185</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lhrlab/text2nkg">https://github.com/lhrlab/text2nkg</a></li>
<li>paper_authors: Haoran Luo, Haihong E, Yuhao Yang, Tianyu Yao, Yikai Guo, Zichen Tang, Wentai Zhang, Kaiyang Wan, Shiyao Peng, Meina Song, Wei Lin</li>
<li>for: 这篇论文旨在构建基于文本的n-ary关系知识图（NKG），以便更好地表达现实世界中的多元关系。</li>
<li>methods: 本文提出了一种新的细化n-ary关系抽取方法，使用 span-tuple classification 和 heteo-ordered merging 技术来实现不同的n-ary关系抽取。</li>
<li>results: 实验结果表明，Text2NKG 比前一代模型提高了 nearly 20% 的 $F_1$ 分数在 Hyper-relational schema 中的细化n-ary关系抽取任务上。<details>
<summary>Abstract</summary>
Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications. However, the construction of NKGs still significantly relies on manual labor, and n-ary relation extraction still remains at a course-grained level, which is always in a single schema and fixed arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. Experimental results demonstrate that Text2NKG outperforms the previous state-of-the-art model by nearly 20\% points in the $F_1$ scores on the fine-grained n-ary relation extraction benchmark in the hyper-relational schema. Our code and datasets are publicly available.
</details>
<details>
<summary>摘要</summary>
traditional binary relational facts beyond, n-ary relational knowledge graphs (NKGs) comprised of n-ary relational facts containing more than two entities, closer to real-world facts with broader applications. However, the construction of NKGs still significantly relies on manual labor, and n-ary relation extraction still remains at a course-grained level, which is always in a single schema and fixed arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. Experimental results demonstrate that Text2NKG outperforms the previous state-of-the-art model by nearly 20\% points in the $F_1$ scores on the fine-grained n-ary relation extraction benchmark in the hyper-relational schema. Our code and datasets are publicly available.Here's the breakdown of the translation:* "traditional binary relational facts" becomes "传统二元关系知识"* "n-ary relational knowledge graphs" becomes "n-ary关系知识图"* "n-ary relational facts" becomes "n-ary关系事实"* "broader applications" becomes "更广泛的应用"* "manual labor" becomes "手动劳动"* "course-grained level" becomes "粗粒度层"* "single schema" becomes "单一 schema"* "fixed arity of entities" becomes " fixes 实体数量"* "Text2NKG" becomes "文本到 NKG"* "span-tuple classification" becomes " span-tuple 分类"* "hetero-ordered merging" becomes "异质顺序合并"* "fine-grained n-ary relation extraction" becomes "细化 n-ary 关系提取"* "n-ary relation extraction benchmark" becomes "n-ary 关系提取指标"* "hyper-relational schema" becomes "超过关系 schema"* "event-based schema" becomes "事件基于 schema"* "role-based schema" becomes "角色基于 schema"* "hypergraph-based schema" becomes "超graph基于 schema"* "high flexibility and practicality" becomes "高灵活性和实用性"* "previous state-of-the-art model" becomes "前一代模型"* "nearly 20\% points" becomes "约 20\% 的点数"Note that the translation is in Simplified Chinese, which is the most widely used variety of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Large-Language-Models-to-Expedite-the-Development-of-Smart-Contracts"><a href="#Optimizing-Large-Language-Models-to-Expedite-the-Development-of-Smart-Contracts" class="headerlink" title="Optimizing Large Language Models to Expedite the Development of Smart Contracts"></a>Optimizing Large Language Models to Expedite the Development of Smart Contracts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05178">http://arxiv.org/abs/2310.05178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nii Osae Osae Dade, Margaret Lartey-Quaye, Emmanuel Teye-Kofi Odonkor, Paul Ammah</li>
<li>For: The paper aims to help developers build decentralized applications (dApps) on blockchain networks by introducing MazzumaGPT, a large language model that can generate smart contract code and improve development productivity.* Methods: The paper uses a large language model called MazzumaGPT, which is optimized for generating smart contract code. The model is fine-tuned and evaluated for functional correctness.* Results: The paper reports on the performance of MazzumaGPT in generating smart contract code and improving development productivity. The results show that the model can generate correct code and improve development efficiency. However, the paper also acknowledges some limitations and broader impacts of the research.Here is the same information in Simplified Chinese:* For: 本研究旨在帮助开发者在区块链网络上建立分布式应用程序（dApps），通过引入MazzumaGPT大语言模型，生成智能合约代码并提高开发效率。* Methods: 本研究使用MazzumaGPT大语言模型，该模型是为生成智能合约代码优化。模型进行了精度调整和功能正确性评估。* Results: 本研究报告MazzumaGPT模型在生成智能合约代码和提高开发效率方面的性能。结果显示，模型可以生成正确的代码并提高开发效率，但也存在一些限制和更广泛的影响。<details>
<summary>Abstract</summary>
Programming has always been at the heart of technological innovation in the 21st century. With the advent of blockchain technologies and the proliferation of web3 paradigms of decentralised applications, smart contracts have been very instrumental in enabling developers to build applications that reside on decentralised blockchains. Despite the huge interest and potential of smart contracts, there is still a significant knowledge and skill gap that developers need to cross in order to build web3 applications. In light of this, we introduce MazzumaGPT, a large language model that has been optimised to generate smart contract code and aid developers to scaffold development and improve productivity. As part of this research, we outline the optimisation and fine-tuning parameters, evaluate the model's performance on functional correctness and address the limitations and broader impacts of our research.
</details>
<details>
<summary>摘要</summary>
Programming 一直是现代科技创新的核心在21世纪。随着区块链技术的出现和分布式应用程序的普及，智能合约帮助开发者建立在分布式区块链上的应用程序。虽然智能合约具有巨大的潜在利益和潜力，但开发者仍然需要跨越一定的知识和技能差距来构建Web3应用程序。为了解决这个问题，我们介绍MazzumaGPT，一个优化的大语言模型，可以生成智能合约代码，帮助开发者快速构建和改进开发。在这项研究中，我们详细介绍优化和细调参数，评估模型的性能并讨论我们的研究的局限性和更广泛的影响。
</details></li>
</ul>
<hr>
<h2 id="GSLB-The-Graph-Structure-Learning-Benchmark"><a href="#GSLB-The-Graph-Structure-Learning-Benchmark" class="headerlink" title="GSLB: The Graph Structure Learning Benchmark"></a>GSLB: The Graph Structure Learning Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05174">http://arxiv.org/abs/2310.05174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gsl-benchmark/gslb">https://github.com/gsl-benchmark/gslb</a></li>
<li>paper_authors: Zhixun Li, Liang Wang, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xiangxin Zhou, Qiang Liu, Shu Wu, Liang Wang, Jeffrey Xu Yu</li>
<li>for: 本研究的目的是为Graph Structure Learning (GSL)提供一个系统的分析和评估，以便更好地理解GSL在不同情况下的表现。</li>
<li>methods: 本研究使用了20种不同的图 dataset和16种不同的 GSL 算法，并进行了系统的性能分析和比较。</li>
<li>results: 研究发现，GSL 在 node-level 和 graph-level 任务中表现出色，并且在鲁棒学习和模型复杂度方面也有出色的表现。<details>
<summary>Abstract</summary>
Graph Structure Learning (GSL) has recently garnered considerable attention due to its ability to optimize both the parameters of Graph Neural Networks (GNNs) and the computation graph structure simultaneously. Despite the proliferation of GSL methods developed in recent years, there is no standard experimental setting or fair comparison for performance evaluation, which creates a great obstacle to understanding the progress in this field. To fill this gap, we systematically analyze the performance of GSL in different scenarios and develop a comprehensive Graph Structure Learning Benchmark (GSLB) curated from 20 diverse graph datasets and 16 distinct GSL algorithms. Specifically, GSLB systematically investigates the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. We comprehensively evaluate state-of-the-art GSL algorithms in node- and graph-level tasks, and analyze their performance in robust learning and model complexity. Further, to facilitate reproducible research, we have developed an easy-to-use library for training, evaluating, and visualizing different GSL methods. Empirical results of our extensive experiments demonstrate the ability of GSL and reveal its potential benefits on various downstream tasks, offering insights and opportunities for future research. The code of GSLB is available at: https://github.com/GSL-Benchmark/GSLB.
</details>
<details>
<summary>摘要</summary>
“几年前，Graph Structure Learning（GSL）已经吸引了很多注意，因为它可以同时优化Graph Neural Networks（GNNs）的参数和计算图структура。不过，过去几年发展的GSL方法中，没有一个通用的实验设置或公平的比较方法，这导致了理解这个领域的进步受到了很大的阻碍。为了填补这个空白，我们系统地分析了GSL在不同的场景下的表现，并开发了一个全面的Graph Structure Learning Benchmark（GSLB），收集了20个多标的图数据和16种不同的GSL算法。具体来说，GSLB系统地探讨了GSL的特点在三个维度上：有效性、韧性和复杂度。我们对现今的State-of-the-art GSL算法进行了node-和graph-水平的任务，并分析了它们在Robust Learning和模型复杂度上的表现。此外，为了促进可重现性的研究，我们开发了一个容易使用的库，可以用于训练、评估和显示不同的GSL方法。我们的广泛的实验结果显示了GSL的能力，并给出了不同下游任务的可能性和未来研究的方向。GSLB的代码可以在：https://github.com/GSL-Benchmark/GSLB中找到。”
</details></li>
</ul>
<hr>
<h2 id="Multi-Ship-Tracking-by-Robust-Similarity-metric"><a href="#Multi-Ship-Tracking-by-Robust-Similarity-metric" class="headerlink" title="Multi-Ship Tracking by Robust Similarity metric"></a>Multi-Ship Tracking by Robust Similarity metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05171">http://arxiv.org/abs/2310.05171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyu Zhao, Gongming Wei, Yang Xiao, Xianglei Xing</li>
<li>for: 提高多船跟踪（MST）技术的应用于海上情况意识和自动船 Navigation System 的发展。</li>
<li>methods: 通过在多目标跟踪（MOT）算法中使用最小几何形态的拟合来提高跟踪性能。</li>
<li>results: 通过将TIoU metricintegrated into state-of-the-art object tracking frameworks, such as DeepSort and ByteTrack, achieving improvements in tracking performance.<details>
<summary>Abstract</summary>
Multi-ship tracking (MST) as a core technology has been proven to be applied to situational awareness at sea and the development of a navigational system for autonomous ships. Despite impressive tracking outcomes achieved by multi-object tracking (MOT) algorithms for pedestrian and vehicle datasets, these models and techniques exhibit poor performance when applied to ship datasets. Intersection of Union (IoU) is the most popular metric for computing similarity used in object tracking. The low frame rates and severe image shake caused by wave turbulence in ship datasets often result in minimal, or even zero, Intersection of Union (IoU) between the predicted and detected bounding boxes. This issue contributes to frequent identity switches of tracked objects, undermining the tracking performance. In this paper, we address the weaknesses of IoU by incorporating the smallest convex shapes that enclose both the predicted and detected bounding boxes. The calculation of the tracking version of IoU (TIoU) metric considers not only the size of the overlapping area between the detection bounding box and the prediction box, but also the similarity of their shapes. Through the integration of the TIoU into state-of-the-art object tracking frameworks, such as DeepSort and ByteTrack, we consistently achieve improvements in the tracking performance of these frameworks.
</details>
<details>
<summary>摘要</summary>
多船跟踪（MST）作为核心技术已被应用于海上情况意识和自动船 Navigation System 的开发。 despite impressive tracking outcomes achieved by multi-object tracking（MOT）算法for pedestrian and vehicle datasets， these models and techniques exhibit poor performance when applied to ship datasets。 Intersection of Union（IoU）是计算相似性的最受欢迎度量， However， the low frame rates and severe image shake caused by wave turbulence in ship datasets often result in minimal, or even zero, Intersection of Union（IoU）between the predicted and detected bounding boxes。 This issue contributes to frequent identity switches of tracked objects, undermining the tracking performance。 In this paper， we address the weaknesses of IoU by incorporating the smallest convex shapes that enclose both the predicted and detected bounding boxes。 The calculation of the tracking version of IoU（TIoU）metric considers not only the size of the overlapping area between the detection bounding box and the prediction box， but also the similarity of their shapes。 Through the integration of the TIoU into state-of-the-art object tracking frameworks， such as DeepSort and ByteTrack， we consistently achieve improvements in the tracking performance of these frameworks。
</details></li>
</ul>
<hr>
<h2 id="DeepQTest-Testing-Autonomous-Driving-Systems-with-Reinforcement-Learning-and-Real-world-Weather-Data"><a href="#DeepQTest-Testing-Autonomous-Driving-Systems-with-Reinforcement-Learning-and-Real-world-Weather-Data" class="headerlink" title="DeepQTest: Testing Autonomous Driving Systems with Reinforcement Learning and Real-world Weather Data"></a>DeepQTest: Testing Autonomous Driving Systems with Reinforcement Learning and Real-world Weather Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05170">http://arxiv.org/abs/2310.05170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simula-complex/deepqtest">https://github.com/simula-complex/deepqtest</a></li>
<li>paper_authors: Chengjie Lu, Tao Yue, Man Zhang, Shaukat Ali</li>
<li>for: 这个论文的目的是提出一种基于强化学习的自动驾驶系统测试方法，以确保自动驾驶系统的安全性。</li>
<li>methods: 这种测试方法使用强化学习的深度Q学习算法，以学习环境配置，并采用了三种安全和舒适度量来构建奖励函数。</li>
<li>results: 对于三个比较基线，深度Q测试表现出显著更高的效果，能够更好地激发自动驾驶系统的异常行为，并确保测试场景的现实性。<details>
<summary>Abstract</summary>
Autonomous driving systems (ADSs) are capable of sensing the environment and making driving decisions autonomously. These systems are safety-critical, and testing them is one of the important approaches to ensure their safety. However, due to the inherent complexity of ADSs and the high dimensionality of their operating environment, the number of possible test scenarios for ADSs is infinite. Besides, the operating environment of ADSs is dynamic, continuously evolving, and full of uncertainties, which requires a testing approach adaptive to the environment. In addition, existing ADS testing techniques have limited effectiveness in ensuring the realism of test scenarios, especially the realism of weather conditions and their changes over time. Recently, reinforcement learning (RL) has demonstrated great potential in addressing challenging problems, especially those requiring constant adaptations to dynamic environments. To this end, we present DeepQTest, a novel ADS testing approach that uses RL to learn environment configurations with a high chance of revealing abnormal ADS behaviors. Specifically, DeepQTest employs Deep Q-Learning and adopts three safety and comfort measures to construct the reward functions. To ensure the realism of generated scenarios, DeepQTest defines a set of realistic constraints and introduces real-world weather conditions into the simulated environment. We employed three comparison baselines, i.e., random, greedy, and a state-of-the-art RL-based approach DeepCOllision, for evaluating DeepQTest on an industrial-scale ADS. Evaluation results show that DeepQTest demonstrated significantly better effectiveness in terms of generating scenarios leading to collisions and ensuring scenario realism compared with the baselines. In addition, among the three reward functions implemented in DeepQTest, Time-To-Collision is recommended as the best design according to our study.
</details>
<details>
<summary>摘要</summary>
自动驾驶系统（ADS）具有感知环境和做出自主驾驶决策的能力。这些系统的安全性非常重要，测试是确保其安全的重要方法。然而，由于ADS的内在复杂性和操作环境的高维度，测试场景的数量是无限的。此外，ADS的操作环境是动态不断变化的，充满不确定性，需要适应环境的测试方法。此外，现有的ADS测试技术对测试场景的真实性具有有限的效果，特别是天气变化和时间的变化。最近，人工智能学习（RL）已经在解决复杂问题方面表现出了极大的潜力。为此，我们提出了 DeepQTest，一种基于RL学习环境配置，以高概率暴露ADS异常行为的测试方法。具体来说，DeepQTest使用深度Q学习并采用了三种安全和舒适度量来定义奖励函数。为保证生成的场景的真实性，DeepQTest定义了一组真实的约束和将实际天气条件引入模拟环境中。我们对ADS进行了三种比较基准，即随机、积极和当前State-of-the-art RL基于approach DeepCOllision，以评估DeepQTest的效果。评估结果表明，DeepQTest与基准相比显著地提高了导致碰撞的场景生成和场景真实性的效果。此外，我们对DeepQTest中实现的三种奖励函数进行了研究，并确定了时间到碰撞为最佳设计。
</details></li>
</ul>
<hr>
<h2 id="Hieros-Hierarchical-Imagination-on-Structured-State-Space-Sequence-World-Models"><a href="#Hieros-Hierarchical-Imagination-on-Structured-State-Space-Sequence-World-Models" class="headerlink" title="Hieros: Hierarchical Imagination on Structured State Space Sequence World Models"></a>Hieros: Hierarchical Imagination on Structured State Space Sequence World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05167">http://arxiv.org/abs/2310.05167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snagnar/hieros">https://github.com/snagnar/hieros</a></li>
<li>paper_authors: Paul Mattes, Rainer Schlosser, Ralf Herbrich</li>
<li>for: 本研究旨在提高现代深度强化学习（DRL）算法的样本效率。</li>
<li>methods: 我们提出了一种层次策略（Hieros），该策略使用了S5层来学习时间抽象的世界表示，并在幂 espacio 中预测下一个世界状态。</li>
<li>results: 我们的方法在Atari 100k Benchmark上的平均和中位数正常化人工分数中超过了现有的状态势。此外，我们的提出的世界模型能够准确预测复杂的动力学。此外，我们还发现了Hieros在探索方面的优势。<details>
<summary>Abstract</summary>
One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models.   We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.
</details>
<details>
<summary>摘要</summary>
一个现代深度奖励学习（DRL）算法的主要挑战是样本效率。许多方法尝试通过在幻想中训练代理人，从而消除直接环境互动的需要。然而，这些方法经常受到幻想准确性、探索能力或运行效率的限制。我们提出了 Hieros，一种层次策略，该策略在幻想中预测时间抽象的世界表示和轨迹，并在多个时间尺度上进行幻想。Heros使用基于 S5 层的世界模型，该模型在训练和环境互动过程中并行地预测下一个世界状态。由于 S5 层的特殊性，我们的方法可以并行地训练和在幻想中预测下一个世界状态。这使得我们的方法比 RNN 类世界模型更高效，并且比 Transformer 类世界模型更高效。我们表明，我们的方法在 Atari 100k 测试集上的平均和中位数normalized human score比 state of the art 高，并且我们提出的世界模型能够准确预测复杂的动力学。此外，我们还证明 Hieros 在探索方面表现出优于现有的方法。
</details></li>
</ul>
<hr>
<h2 id="MenatQA-A-New-Dataset-for-Testing-the-Temporal-Comprehension-and-Reasoning-Abilities-of-Large-Language-Models"><a href="#MenatQA-A-New-Dataset-for-Testing-the-Temporal-Comprehension-and-Reasoning-Abilities-of-Large-Language-Models" class="headerlink" title="MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models"></a>MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05157">http://arxiv.org/abs/2310.05157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weiyifan1023/MenatQA">https://github.com/weiyifan1023/MenatQA</a></li>
<li>paper_authors: Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, Kang Liu</li>
<li>for: This paper aims to evaluate the time comprehension and reasoning abilities of large language models (LLMs) and investigate potential improvement strategies.</li>
<li>methods: The paper constructs a benchmark task called Multiple Sensitive Factors Time QA (MenatQA) that tests LLMs’ performance on three temporal factors (scope factor, order factor, counterfactual factor) with a total of 2,853 samples.</li>
<li>results: Most LLMs fall behind smaller temporal reasoning models in terms of performance on the MenatQA task, particularly in handling temporal biases and utilizing external information. The paper also explores potential improvement strategies such as devising specific prompts and leveraging external tools.<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown nearly saturated performance on many natural language processing (NLP) tasks. As a result, it is natural for people to believe that LLMs have also mastered abilities such as time understanding and reasoning. However, research on the temporal sensitivity of LLMs has been insufficiently emphasized. To fill this gap, this paper constructs Multiple Sensitive Factors Time QA (MenatQA), which encompasses three temporal factors (scope factor, order factor, counterfactual factor) with total 2,853 samples for evaluating the time comprehension and reasoning abilities of LLMs. This paper tests current mainstream LLMs with different parameter sizes, ranging from billions to hundreds of billions. The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors. In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions. Furthermore, this paper undertakes a preliminary investigation into potential improvement strategies by devising specific prompts and leveraging external tools. These approaches serve as valuable baselines or references for future research endeavors.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在自然语言处理（NLP）任务上已经显示出几乎满足性的性能。因此，人们对于 LLM 的时间理解和推理能力的拥有有了误解。然而，对于 LLN 的时间敏感性的研究却得不到充分的关注。为了填补这个空白，这篇文章建立了多重敏感因素时间问答（MenatQA），包括三个时间因素（范围因素、次序因素、Counterfactual因素），共有2,853个样本，用于评估 LLM 的时间理解和推理能力。这篇文章测试了现代主流 LLM 的不同参数大小，从十亿到百亿。结果显示，大多数 LLM 落后于不同程度的时间推理模型。具体来说，LLM 对于时间偏见具有重要的敏感性，并且对于时间提供的问题中的时间信息依赖很大。此外，这篇文章进行了初步的改进策略研究，包括设计特定的提示和使用外部工具。这些方法可以作为未来研究的基础或参考。
</details></li>
</ul>
<hr>
<h2 id="Toolink-Linking-Toolkit-Creation-and-Using-through-Chain-of-Solving-on-Open-Source-Model"><a href="#Toolink-Linking-Toolkit-Creation-and-Using-through-Chain-of-Solving-on-Open-Source-Model" class="headerlink" title="Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model"></a>Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05155">http://arxiv.org/abs/2310.05155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiancheng0/toolink">https://github.com/qiancheng0/toolink</a></li>
<li>paper_authors: Cheng Qian, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu</li>
<li>for: The paper aims to develop a comprehensive framework for task-solving using tool-based chain-of-solving (CoS) approach, with the goal of leveraging smaller, open-sourced models for adaptability.</li>
<li>methods: The proposed framework, called Toolink, creates a toolkit and integrates planning and calling of tools through a CoS approach. The authors validate the efficacy of Toolink on ChatGPT and curate a CoS dataset (CoS-GPT) for task-solving. They finetune the LLaMA-7B model to create LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities.</li>
<li>results: The evaluation on diverse tasks from BIG-bench shows that LLaMA-CoS matches the CoS ability of ChatGPT while surpassing the chain-of-thought approach in performance. The study also demonstrates the generalization of LLaMA-CoS to unseen tasks and its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation on diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses the chain-of-thought approach. Further studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios. All codes and data are released.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:大型语言模型（LLMs）已经展示出具有优异的进步，但它们的关闭源代码和高推论成本导致它们的适应性有限，需要一个有效的方法来应用小型开源模型。在这篇文章中，我们介绍Toolink，一个完整的框架，通过链式解决（CoS）方法来实现任务解决。我们首先验证Toolink在ChatGPT上的有效性，然后创建CoS-GPT dataset，并调整LLaMA-7B模型。它将实现LLaMA-CoS，一个开源模型，拥有进步的工具规划和工具呼叫能力。我们从BIG-bench中的多个任务进行评估，发现LLaMA-CoS的CoS能力与ChatGPT相似，并且其表现超过链式思维方法。此外，我们还进行了进一步的研究，证明LLaMA-CoS具有对未见任务的普遍性和在不同的工具集上的可行性，这证明了它在实际情况中的可靠性。所有代码和数据都是公开发布。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-LLM-as-a-System-of-Multiple-Expert-Agents-An-Approach-to-solve-the-Abstraction-and-Reasoning-Corpus-ARC-Challenge"><a href="#Large-Language-Model-LLM-as-a-System-of-Multiple-Expert-Agents-An-Approach-to-solve-the-Abstraction-and-Reasoning-Corpus-ARC-Challenge" class="headerlink" title="Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge"></a>Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05146">http://arxiv.org/abs/2310.05146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tanchongmin/arc-challenge">https://github.com/tanchongmin/arc-challenge</a></li>
<li>paper_authors: John Chong Min Tan, Mehul Motani</li>
<li>for: 解决Abstraction and Reasoning Corpus(ARC)挑战，使用大型自然语言模型(LLM)作为多个专家系统。</li>
<li>methods: 使用LLM的灵活性，通过零shot、几shot、上下文固定的提示，让LLM解决多种新任务。首先将输入图像转化为多种适合的文本抽象空间，然后利用LLM的协同力量，Derive输入-输出关系，并将其映射到动作形式的工作程序，类似于Voyager &#x2F; Ghost in MineCraft。 Additionally, use iterative environmental feedback to guide LLMs to solve the task.</li>
<li>results: 使用提posed方法解决111个训练集问题中的50个(45%)，只需三个抽象空间 - 网格、对象和像素。我们认为，通过添加更多抽象空间和学习动作，我们将能够解决更多问题。<details>
<summary>Abstract</summary>
We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge using Large Language Models (LLMs) as a system of multiple expert agents. Using the flexibility of LLMs to be prompted to do various novel tasks using zero-shot, few-shot, context-grounded prompting, we explore the feasibility of using LLMs to solve the ARC Challenge. We firstly convert the input image into multiple suitable text-based abstraction spaces. We then utilise the associative power of LLMs to derive the input-output relationship and map this to actions in the form of a working program, similar to Voyager / Ghost in the MineCraft. In addition, we use iterative environmental feedback in order to guide LLMs to solve the task. Our proposed approach achieves 50 solves out of 111 training set problems (45%) with just three abstraction spaces - grid, object and pixel - and we believe that with more abstraction spaces and learnable actions, we will be able to solve more.
</details>
<details>
<summary>摘要</summary>
我们尝试使用大型自然语言模型（LLM）解决抽象和逻辑 Corpora（ARC）挑战，以多个专家代理系统的形式进行解决。通过使用 LLM 的灵活性，我们可以使其响应各种新任务，使用零上下文、几上下文、上下文固定的提示，探索使用 LLM 解决 ARC 挑战的可能性。首先，我们将输入图像转换为多个适合的文本基于抽象空间。然后，我们利用 LLMS 的协同力来推导输入-输出关系，并将其映射到作为工作程序的动作，类似于 Voyager / Ghost 在 MineCraft 中。此外，我们使用迭代环境反馈，以引导 LLMS 解决任务。我们的提议方法已经实现了 50 个训练集问题（45%）的解决，只使用了三个抽象空间 - 网格、对象和像素 - 并我们认为，通过添加更多的抽象空间和学习动作，我们将能够解决更多的问题。
</details></li>
</ul>
<hr>
<h2 id="NeuralFastLAS-Fast-Logic-Based-Learning-from-Raw-Data"><a href="#NeuralFastLAS-Fast-Logic-Based-Learning-from-Raw-Data" class="headerlink" title="NeuralFastLAS: Fast Logic-Based Learning from Raw Data"></a>NeuralFastLAS: Fast Logic-Based Learning from Raw Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05145">http://arxiv.org/abs/2310.05145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Theo Charalambous, Yaniv Aspis, Alessandra Russo</li>
<li>for: 本研究旨在提出一种可扩展和高效的综合方法，即NeuralFastLAS，用于同时训练神经网络和符号学习器。</li>
<li>methods: NeuralFastLAS使用一种新的约束优化技术，通过学习一个 posterior distribution 来提高训练稳定性。</li>
<li>results: 实验结果表明，NeuralFastLAS可以在数学和逻辑任务中达到状态革命级别的准确率，训练时间比其他同时训练神经网络和符号学习器的方法快到两个数量级。<details>
<summary>Abstract</summary>
Symbolic rule learners generate interpretable solutions, however they require the input to be encoded symbolically. Neuro-symbolic approaches overcome this issue by mapping raw data to latent symbolic concepts using a neural network. Training the neural and symbolic components jointly is difficult, due to slow and unstable learning, hence many existing systems rely on hand-engineered rules to train the network. We introduce NeuralFastLAS, a scalable and fast end-to-end approach that trains a neural network jointly with a symbolic learner. For a given task, NeuralFastLAS computes a relevant set of rules, proved to contain an optimal symbolic solution, trains a neural network using these rules, and finally finds an optimal symbolic solution to the task while taking network predictions into account. A key novelty of our approach is learning a posterior distribution on rules while training the neural network to improve stability during training. We provide theoretical results for a sufficient condition on network training to guarantee correctness of the final solution. Experimental results demonstrate that NeuralFastLAS is able to achieve state-of-the-art accuracy in arithmetic and logical tasks, with a training time that is up to two orders of magnitude faster than other jointly trained neuro-symbolic methods.
</details>
<details>
<summary>摘要</summary>
symbolic rule learners 可以生成可读解释的解决方案，但是它们需要输入数据被编码成符号形式。 neural-symbolic 方法可以将原始数据映射到隐藏的符号概念上使用神经网络，从而解决这个问题。 然而，在培aujointly trained neural and symbolic components 的问题上，存在慢速和不稳定的学习问题，因此许多现有系统通常采用手工设计规则来训练网络。我们介绍NeuralFastLAS，一种可扩展和快速的终端方法，可以同时训练神经网络和符号学习器。对于给定任务，NeuralFastLAS 可以计算一个相关的规则集，证明其中包含最优的符号解决方案，使用这些规则来训练神经网络，并最终找到一个包含神经网络预测的最优符号解决方案。我们的方法的一个新特点是在培aujointly trained neural and symbolic components 时，学习一个 posterior distribution  sobre rules 以提高培aujoint training 的稳定性。我们提供了理论结果，证明在网络训练时满足某些条件下，可以保证最终解决方案的正确性。实验结果表明，NeuralFastLAS 能够在数学和逻辑任务中达到领先的准确率，并且培aujoint training 时间比其他同时训练的神经网络和符号学习器方法快到两个数量级。
</details></li>
</ul>
<hr>
<h2 id="ZooPFL-Exploring-Black-box-Foundation-Models-for-Personalized-Federated-Learning"><a href="#ZooPFL-Exploring-Black-box-Foundation-Models-for-Personalized-Federated-Learning" class="headerlink" title="ZooPFL: Exploring Black-box Foundation Models for Personalized Federated Learning"></a>ZooPFL: Exploring Black-box Foundation Models for Personalized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05143">http://arxiv.org/abs/2310.05143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/personalizedfl">https://github.com/microsoft/personalizedfl</a></li>
<li>paper_authors: Wang Lu, Hao Yu, Jindong Wang, Damien Teney, Haohan Wang, Yiqiang Chen, Qiang Yang, Xing Xie, Xiangyang Ji</li>
<li>for: 这篇论文旨在解决个性化 Federated Learning (FL) 中资源有限的问题，包括数据、计算和通信成本，以及访问模型的限制。</li>
<li>methods: 该论文提出了一种名为 ZOOPFL 的方法，使用零阶优化解决分布偏移问题，并使用简单 yet effective 的线性投影进行个性化。此外，它还使用输入修复来投影预测值。</li>
<li>results: 广泛的实验表明，ZOOPFL 可以有效地应用于黑盒基模型上的 FL 任务，并且可以提高个性化的精度。<details>
<summary>Abstract</summary>
When personalized federated learning (FL) meets large foundation models, new challenges arise from various limitations in resources. In addition to typical limitations such as data, computation, and communication costs, access to the models is also often limited. This paper endeavors to solve both the challenges of limited resources and personalization. i.e., distribution shifts between clients. To do so, we propose a method named ZOOPFL that uses Zeroth-Order Optimization for Personalized Federated Learning. ZOOPFL avoids direct interference with the foundation models and instead learns to adapt its inputs through zeroth-order optimization. In addition, we employ simple yet effective linear projections to remap its predictions for personalization. To reduce the computation costs and enhance personalization, we propose input surgery to incorporate an auto-encoder with low-dimensional and client-specific embeddings. We provide theoretical support for ZOOPFL to analyze its convergence. Extensive empirical experiments on computer vision and natural language processing tasks using popular foundation models demonstrate its effectiveness for FL on black-box foundation models.
</details>
<details>
<summary>摘要</summary>
当个性化联合学习（FL）遇到大规模基础模型时，新的挑战出现，包括不同限制的资源。除了典型的限制，如数据、计算和通信成本外，对模型的访问也经常受限。这篇论文旨在解决限制资源和个性化的两个挑战。即分布shift between客户端。为此，我们提出了一种方法名为ZOOPFL，它使用零阶优化进行个性化联合学习。ZOOPFL避免直接干扰基础模型，而是通过零阶优化学习适应输入。此外，我们使用简单 yet有效的线性映射来重新映射其预测。为了减少计算成本并提高个性化，我们提议输入手术，其中包括一个低维度的自动encoder和客户端特定的嵌入。我们提供了对ZOOPFL的理论支持，以分析其相对稳定性。我们对计算机视觉和自然语言处理任务使用了流行的基础模型进行了广泛的实验，以证明ZOOPFL在黑盒基础模型上的有效性。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Power-of-Large-Language-Models-for-Empathetic-Response-Generation-Empirical-Investigations-and-Improvements"><a href="#Harnessing-the-Power-of-Large-Language-Models-for-Empathetic-Response-Generation-Empirical-Investigations-and-Improvements" class="headerlink" title="Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements"></a>Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05140">http://arxiv.org/abs/2310.05140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yushan Qian, Wei-Nan Zhang, Ting Liu</li>
<li>for: 这个论文主要研究了大语言模型（LLMs）在建立和谐社会关系中的应用效果，以及如何使用LLMs提高对话的同理能力。</li>
<li>methods: 本文提出了三种改进方法，包括semantically similar in-context learning、two-stage interactive generation和知识库的组合。</li>
<li>results: 广泛的实验表明，LLMs可以在我们提出的方法的帮助下显著提高对话的同理能力，并在自动和人类评价中达到了领先水平。此外，我们还探讨了GPT-4可以模拟人类评价者的可能性。<details>
<summary>Abstract</summary>
Empathetic dialogue is an indispensable part of building harmonious social relationships and contributes to the development of a helpful AI. Previous approaches are mainly based on fine small-scale language models. With the advent of ChatGPT, the application effect of large language models (LLMs) in this field has attracted great attention. This work empirically investigates the performance of LLMs in generating empathetic responses and proposes three improvement methods of semantically similar in-context learning, two-stage interactive generation, and combination with the knowledge base. Extensive experiments show that LLMs can significantly benefit from our proposed methods and is able to achieve state-of-the-art performance in both automatic and human evaluations. Additionally, we explore the possibility of GPT-4 simulating human evaluators.
</details>
<details>
<summary>摘要</summary>
帮助AI的发展，对话是不可或缺的一部分。以前的方法主要基于细致语言模型。随着ChatGPT的出现，大语言模型（LLMs）在这一领域的应用效果吸引了广泛的关注。本研究employs three improvement methods of semantically similar in-context learning, two-stage interactive generation, and combination with the knowledge base to investigate the performance of LLMs in generating empathetic responses. Our extensive experiments show that LLMs can significantly benefit from our proposed methods and achieve state-of-the-art performance in both automatic and human evaluations. In addition, we explore the possibility of GPT-4 simulating human evaluators.
</details></li>
</ul>
<hr>
<h2 id="Maximizing-Utilitarian-and-Egalitarian-Welfare-of-Fractional-Hedonic-Games-on-Tree-like-Graphs"><a href="#Maximizing-Utilitarian-and-Egalitarian-Welfare-of-Fractional-Hedonic-Games-on-Tree-like-Graphs" class="headerlink" title="Maximizing Utilitarian and Egalitarian Welfare of Fractional Hedonic Games on Tree-like Graphs"></a>Maximizing Utilitarian and Egalitarian Welfare of Fractional Hedonic Games on Tree-like Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05139">http://arxiv.org/abs/2310.05139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tesshu Hanaka, Airi Ikeyama, Hirotaka Ono</li>
<li>for:  Fractional hedonic games are coalition formation games where a player’s utility is determined by the average value they assign to the members of their coalition.</li>
<li>methods:  The paper presents (pseudo)polynomial-time algorithms to compute welfare-maximizing partitions in fractional hedonic games on tree-like graphs, including two types of social welfare measures: utilitarian and egalitarian.</li>
<li>results:  The paper provides a hardness result, demonstrating that the pseudopolynomial-time solvability is the best possible under the assumption P$\neq$NP.<details>
<summary>Abstract</summary>
Fractional hedonic games are coalition formation games where a player's utility is determined by the average value they assign to the members of their coalition. These games are a variation of graph hedonic games, which are a class of coalition formation games that can be succinctly represented. Due to their applicability in network clustering and their relationship to graph hedonic games, fractional hedonic games have been extensively studied from various perspectives. However, finding welfare-maximizing partitions in fractional hedonic games is a challenging task due to the nonlinearity of utilities. In fact, it has been proven to be NP-hard and can be solved in polynomial time only for a limited number of graph classes, such as trees. This paper presents (pseudo)polynomial-time algorithms to compute welfare-maximizing partitions in fractional hedonic games on tree-like graphs. We consider two types of social welfare measures: utilitarian and egalitarian. Tree-like graphs refer to graphs with bounded treewidth and block graphs. A hardness result is provided, demonstrating that the pseudopolynomial-time solvability is the best possible under the assumption P$\neq$NP.
</details>
<details>
<summary>摘要</summary>
幂数 Hedonic 游戏是一种协会成员选择游戏，其中玩家的产生 utility 取决于他们所在协会的平均价值。这种游戏是图 Hedonic 游戏的一种变种，可以简洁地表示。由于它们在网络划分和图 Hedonic 游戏之间的关系，幂数 Hedonic 游戏已经得到了广泛的研究。然而，在幂数 Hedonic 游戏中找到最大启用分 partitions 是一项困难的任务，因为价值函数是非线性的。事实上，已经证明了这是 NP-hard 问题，只有在一些图类型，如树，可以在多项时间内解决。本文提出了（假）多项时间算法来计算幂数 Hedonic 游戏中的最大启用分 partitions。我们考虑了两种社会利益度量：utilitarian 和 egalitarian。树状图指的是具有固定树宽度的图和块图。我们还提供了一个困难性结果，证明了 pseudopolynomial-time 可行性是最佳的，即 P ≠ NP 的假设下。
</details></li>
</ul>
<hr>
<h2 id="InstructDET-Diversifying-Referring-Object-Detection-with-Generalized-Instructions"><a href="#InstructDET-Diversifying-Referring-Object-Detection-with-Generalized-Instructions" class="headerlink" title="InstructDET: Diversifying Referring Object Detection with Generalized Instructions"></a>InstructDET: Diversifying Referring Object Detection with Generalized Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05136">http://arxiv.org/abs/2310.05136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jyfenggogo/instructdet">https://github.com/jyfenggogo/instructdet</a></li>
<li>paper_authors: Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song</li>
<li>for: 本文提出了一种数据驱动的对象检测方法（InstructDET），用于基于用户指令（referring expressions，REC）进行对象检测。</li>
<li>methods: 本文使用了基于用户指令的数据驱动方法，并利用了新的视觉语言模型（VLM）和大语言模型（LLM）来生成指令和对象 bounding boxes（bbxs）。</li>
<li>results: 本文通过使用 InstructDET 方法和自制的 InDET  dataset，实现了在标准 REC  dataset 和 InDET 测试集上超越现有方法的对象检测性能。<details>
<summary>Abstract</summary>
We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are from foundation models. Our InDET is developed from existing REC datasets and object detection datasets, with the expanding potential that any image with object bbxs can be incorporated through using our InstructDET method. By using our InDET dataset, we show that a conventional ROD model surpasses existing methods on standard REC datasets and our InDET test set. Our data-centric method InstructDET, with automatic data expansion by leveraging foundation models, directs a promising field that ROD can be greatly diversified to execute common object detection instructions.
</details>
<details>
<summary>摘要</summary>
我们提出了InstructDET，一种数据驱动的引用物体检测（ROD）方法，它基于用户指令来定位目标对象。而我们所利用的指令不仅来自引用表达（REC），还包括各种用户意图相关的对象检测指令。对于一张图像，我们生成了庞大的指令和对象 bounding box（bbxs），每个指令和对应的bbxs组成一个训练数据对。为了涵盖通用的检测表达，我们利用了趋势感知模型（VLM）和大语言模型（LLM），通过文本提示和对象bbxs来引导生成指令，这些基础模型的泛化效果可以生成人类化表达（例如，描述对象属性、类别和关系）。我们称之为InDET，它包含图像、bbxs和通用指令，这些指令来自基础模型。我们的InDET是基于现有REC dataset和对象检测dataset的扩展，可以通过我们的InstructDET方法将任何图像 WITH object bbxsintegrated。通过使用InDET数据集，我们示出了一个标准ROD模型在标准REC dataset和InDET测试集上的表现比普通方法更高。我们的数据驱动方法InstructDET，通过基于基础模型的自动扩展，指明了一个可能的场景，ROD可以通过各种常见的检测指令执行。
</details></li>
</ul>
<hr>
<h2 id="Are-Emily-and-Greg-Still-More-Employable-than-Lakisha-and-Jamal-Investigating-Algorithmic-Hiring-Bias-in-the-Era-of-ChatGPT"><a href="#Are-Emily-and-Greg-Still-More-Employable-than-Lakisha-and-Jamal-Investigating-Algorithmic-Hiring-Bias-in-the-Era-of-ChatGPT" class="headerlink" title="Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT"></a>Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05135">http://arxiv.org/abs/2310.05135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshaj Kumar Veldanda, Fabian Grob, Shailja Thakur, Hammond Pearce, Benjamin Tan, Ramesh Karri, Siddharth Garg</li>
<li>for: 这个研究探讨了大语言模型（LLMs）在算法招聘中的应用，特别是将简历与职业类别相匹配。</li>
<li>methods: 研究使用了场景实验来评估大语言模型对保护属性的偏见（如性别、种族和生育状况）的影响。</li>
<li>results: 研究发现，LLMs在不同的种族和性别下表现一致，但在孕期状况和政治倾向上存在偏见。使用了开源的LLMs进行对比输入解码来探讨可能的偏见源。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit applicability across numerous tasks. One domain of interest is their use in algorithmic hiring, specifically in matching resumes with job categories. Yet, this introduces issues of bias on protected attributes like gender, race and maternity status. The seminal work of Bertrand & Mullainathan (2003) set the gold-standard for identifying hiring bias via field experiments where the response rate for identical resumes that differ only in protected attributes, e.g., racially suggestive names such as Emily or Lakisha, is compared. We replicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and Llama) to evaluate bias (or lack thereof) on gender, race, maternity status, pregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1) matching resumes to job categories; and (2) summarizing resumes with employment relevant information. Overall, LLMs are robust across race and gender. They differ in their performance on pregnancy status and political affiliation. We use contrastive input decoding on open-source LLMs to uncover potential sources of bias.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT-3.5、Bard和Claude在多个任务中表现出色。一个有趣的领域是它们在算法招聘中的应用，特别是在匹配简历与职业类别之间。然而，这会引入保护特征如性别、种族和生育状况等的偏见。Bertrand & Mullainathan（2003）的著名研究设置了标准 для识别招聘偏见，通过在实验室中对同样的简历进行比较，以确定它们是否具有保护特征。我们在当今最高级的LLMs（GPT-3.5、Bard、Claude和Llama）上重复了这个实验，以评估它们对gender、种族、生育状况、怀孕状况和政治信仰等保护特征的偏见。我们在两个任务上评估LLMs：（1）匹配简历与职业类别之间；和（2）摘要简历中有关雇佣信息。总的来说，LLMs在gender和种族方面都很稳定，但在怀孕状况和政治信仰方面存在差异。我们使用开源LLMs的对比输入解码来探测可能的偏见源。
</details></li>
</ul>
<hr>
<h2 id="ed-cec-improving-rare-word-recognition-using-asr-postprocessing-based-on-error-detection-and-context-aware-error-correction"><a href="#ed-cec-improving-rare-word-recognition-using-asr-postprocessing-based-on-error-detection-and-context-aware-error-correction" class="headerlink" title="ed-cec: improving rare word recognition using asr postprocessing based on error detection and context-aware error correction"></a>ed-cec: improving rare word recognition using asr postprocessing based on error detection and context-aware error correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05129">http://arxiv.org/abs/2310.05129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajun He, Zekun Yang, Tomoki Toda</li>
<li>for: 提高自然语言处理（NLP）任务中罕见词的识别精度，以优化下游任务 such as 关键词检测、意图检测和文本概要生成。</li>
<li>methods: 提出了一种基于错误检测和上下文相关知识的ASR后处理方法，通过针对预测出的错误位置进行优化decoding过程，最大化精度while minimizing unnecessary computations。此外，我们还利用罕见词名单提供额外的上下文知识，以便更好地 corrected罕见词。</li>
<li>results: 在五个数据集上实验表明，我们的提议方法可以比前一些方法更好地降低单词错误率（WER），同时保持一定的推理速度，并且在不同的ASR系统上表现出良好的鲁棒性。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) systems often encounter difficulties in accurately recognizing rare words, leading to errors that can have a negative impact on downstream tasks such as keyword spotting, intent detection, and text summarization. To address this challenge, we present a novel ASR postprocessing method that focuses on improving the recognition of rare words through error detection and context-aware error correction. Our method optimizes the decoding process by targeting only the predicted error positions, minimizing unnecessary computations. Moreover, we leverage a rare word list to provide additional contextual knowledge, enabling the model to better correct rare words. Experimental results across five datasets demonstrate that our proposed method achieves significantly lower word error rates (WERs) than previous approaches while maintaining a reasonable inference speed. Furthermore, our approach exhibits promising robustness across different ASR systems.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统经常遇到罕见词汇识别错误，导致下游任务如关键词检测、意图检测和文本概要 SUMMARIZATION 中的错误。为解决这个挑战，我们提出了一种新的 ASR 后处理方法，旨在提高罕见词汇识别的准确率。我们的方法优化了解码过程，只targeting 预测出的错误位置，最小化无用的计算。此外，我们利用罕见词表来提供额外的contextual knowledge，使模型更好地更正罕见词汇。实验结果 across five datasets 表明，我们提出的方法可以在word error rate（WER）下达到 significanly 更高的准确率，同时保持合理的推理速度。此外，我们的方法在不同的 ASR 系统上也展现出了良好的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Instances-and-Labels-Hierarchy-aware-Joint-Supervised-Contrastive-Learning-for-Hierarchical-Multi-Label-Text-Classification"><a href="#Instances-and-Labels-Hierarchy-aware-Joint-Supervised-Contrastive-Learning-for-Hierarchical-Multi-Label-Text-Classification" class="headerlink" title="Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification"></a>Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05128">http://arxiv.org/abs/2310.05128</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simonucl/HJCL">https://github.com/simonucl/HJCL</a></li>
<li>paper_authors: Simon Chi Lok U, Jie He, Víctor Gutiérrez-Basulto, Jeff Z. Pan</li>
<li>for: 这个研究的目的是解决多个标签分类中的多个标签间的关联性问题。</li>
<li>methods: 这个研究使用了对生成的标签类别进行对照学习，以将文本和标签嵌入更加接近。</li>
<li>results: 实验结果显示，HJCL可以实现了优异的结果，并且显示了对于多个标签分类的效果。<details>
<summary>Abstract</summary>
Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the contrastive learning objective. Extensive experiments on four multi-path HMTC datasets demonstrate that HJCL achieves promising results and the effectiveness of Contrastive Learning on HMTC.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese: Hierarchical Multi-label Text Classification (HMTC) targets 使用标签层次结构在多标签分类中。现有的 HMTC 方法面临着通过使用半supervised contrastive learning 来违规输出空间的问题，这会使文本和标签嵌入更加紧密。然而，生成样本通常会引入噪声，因为它们忽略了同一个批处理中的相似样本之间的相关性。一种解决这个问题的方法是使用supervised contrastive learning，但它在 HMTC 中尚未得到充分发挥。为了 bridge 这两种方法之间的差异，我们提出了 Hierarchy-aware Joint Supervised Contrastive Learning (HJCL) 方法。特别是，我们使用了 both instance-wise 和 label-wise contrastive learning 技术，并且细心地构造批处理来满足对做对的 contrastive learning 目标。广泛的实验表明，HJCL 在四个多路 HMTC 数据集上达到了可塑性和 HMTC 中的对比学习的效果。
</details></li>
</ul>
<hr>
<h2 id="UReader-Universal-OCR-free-Visually-situated-Language-Understanding-with-Multimodal-Large-Language-Model"><a href="#UReader-Universal-OCR-free-Visually-situated-Language-Understanding-with-Multimodal-Large-Language-Model" class="headerlink" title="UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model"></a>UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05126">http://arxiv.org/abs/2310.05126</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lukeforeveryoung/ureader">https://github.com/lukeforeveryoung/ureader</a></li>
<li>paper_authors: Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Alex Lin, Fei Huang</li>
<li>for: 这个研究旨在提出一个universal OCR-free visually-situated language understanding模型，以便在文档、表格、图表、自然图像和网页 screenshot 等多种类型的视觉文本中进行语言理解。</li>
<li>methods: 本研究使用 Multimodal Large Language Model (MLLM)，并将其训练为可以进行多种类型的视觉文本理解任务，包括文档、表格、图表、自然图像和网页 screenshot 等。此外，研究者还将两个辅助任务添加到模型中，以增强模型的视觉文本和 semantics 理解能力。</li>
<li>results: 根据研究结果，这个单一模型可以在8个不同类型的视觉文本理解任务中实现state-of-the-art的性能，不需要进行下游训练。此外，研究者还发现这个模型可以对高分辨率的图像进行有效的处理，并且可以快速地处理大量的视觉文本。<details>
<summary>Abstract</summary>
Text is ubiquitous in our visual world, conveying crucial information, such as in documents, websites, and everyday photographs. In this work, we propose UReader, a first exploration of universal OCR-free visually-situated language understanding based on the Multimodal Large Language Model (MLLM). By leveraging the shallow text recognition ability of the MLLM, we only finetuned 1.2% parameters and the training cost is much lower than previous work following domain-specific pretraining and finetuning paradigms. Concretely, UReader is jointly finetuned on a wide range of Visually-situated Language Understanding tasks via a unified instruction format. To enhance the visual text and semantic understanding, we further apply two auxiliary tasks with the same format, namely text reading and key points generation tasks. We design a shape-adaptive cropping module before the encoder-decoder architecture of MLLM to leverage the frozen low-resolution vision encoder for processing high-resolution images. Without downstream finetuning, our single model achieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated language understanding tasks, across 5 domains: documents, tables, charts, natural images, and webpage screenshots. Codes and instruction-tuning datasets will be released.
</details>
<details>
<summary>摘要</summary>
文本在我们的视觉世界中 ubique, 传递重要信息，如文档、网站和日常照片。在这项工作中，我们提出了 UReader，一种首次探索的无需 OCR 的通用视觉语言理解基于多模态大语言模型（MLLM）。我们利用 MLLM 的浅文本认知能力，只需要 finetune 1.2% 的参数，训练成本远低于先前的领域特定预训练和 fine-tuning 方法。具体来说，UReader 是通过一种统一的指令格式进行联合训练多种视觉语言理解任务。为了增强视觉文本和 semantics 理解，我们还应用了两个辅助任务，即文本读取和关键点生成任务。我们设计了适应形式的截取模块，以便使用冻结的低分辨率视觉Encoder 处理高分辨率图像。无需下游训练，我们的单个模型在 8 个视觉语言理解任务中 achievement state-of-the-art OCR-free 性能，覆盖 5 个领域：文档、表格、图表、自然图像和网页截屏。我们将代码和 instrucion-tuning 数据集发布。
</details></li>
</ul>
<hr>
<h2 id="Distribution-Based-Trajectory-Clustering"><a href="#Distribution-Based-Trajectory-Clustering" class="headerlink" title="Distribution-Based Trajectory Clustering"></a>Distribution-Based Trajectory Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05123">http://arxiv.org/abs/2310.05123</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IsolationKernel/TIDKC">https://github.com/IsolationKernel/TIDKC</a></li>
<li>paper_authors: Zi Jing Wang, Ye Zhu, Kai Ming Ting</li>
<li>for:  trajectory clustering, 探索 trajectory 数据中的共同模式</li>
<li>methods: 使用 Isolation Distributional Kernel (IDK) 作为主要工具，以实现 trajectory 相似度测量和归类</li>
<li>results: 比较传统和深度学习基于距离度量的方法，IDK 能够更好地捕捉 trajectory 中复杂的结构，并且提供了更高效和稳定的归类性能。<details>
<summary>Abstract</summary>
Trajectory clustering enables the discovery of common patterns in trajectory data. Current methods of trajectory clustering rely on a distance measure between two points in order to measure the dissimilarity between two trajectories. The distance measures employed have two challenges: high computational cost and low fidelity. Independent of the distance measure employed, existing clustering algorithms have another challenge: either effectiveness issues or high time complexity. In this paper, we propose to use a recent Isolation Distributional Kernel (IDK) as the main tool to meet all three challenges. The new IDK-based clustering algorithm, called TIDKC, makes full use of the distributional kernel for trajectory similarity measuring and clustering. TIDKC identifies non-linearly separable clusters with irregular shapes and varied densities in linear time. It does not rely on random initialisation and is robust to outliers. An extensive evaluation on 7 large real-world trajectory datasets confirms that IDK is more effective in capturing complex structures in trajectories than traditional and deep learning-based distance measures. Furthermore, the proposed TIDKC has superior clustering performance and efficiency to existing trajectory clustering algorithms.
</details>
<details>
<summary>摘要</summary>
trajectory clustering可以揭示行程数据中的共同模式。现有的行程 clustering方法都基于两点之间的距离度量来衡量行程之间的不同。现有的距离度量面临两个挑战：高计算成本和低准确性。独立于选择的距离度量，现有的归类算法又面临另一个挑战：效果不佳或高时间复杂度。在本文中，我们提议使用最近的隔离分布 kernel（IDK）作为主要工具，以解决这三个挑战。我们称之为 TIDKC 归类算法。 TIDKC 利用分布 kernel 来衡量行程之间的相似度，并且可以快速地找到非线性分割的弯曲形状和不规则的分布。它不需要随机初始化，并且对异常值有较高的Robustness。我们对 7 个大的实际行程数据集进行了广泛的评估，发现 IDK 可以更好地捕捉行程中的复杂结构，比传统和深度学习基于的距离度量更有效。此外，我们的提议的 TIDKC 归类算法也比现有的行程归类算法有更高的归类性和效率。
</details></li>
</ul>
<hr>
<h2 id="Breaking-Down-Word-Semantics-from-Pre-trained-Language-Models-through-Layer-wise-Dimension-Selection"><a href="#Breaking-Down-Word-Semantics-from-Pre-trained-Language-Models-through-Layer-wise-Dimension-Selection" class="headerlink" title="Breaking Down Word Semantics from Pre-trained Language Models through Layer-wise Dimension Selection"></a>Breaking Down Word Semantics from Pre-trained Language Models through Layer-wise Dimension Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05115">http://arxiv.org/abs/2310.05115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nayoung Choi</li>
<li>for: 本研究旨在分析BERT中各层中的不同语言知识，以及分离含义的不同方面。</li>
<li>methods: 本研究使用了一种 binary mask 技术，将中间输出规范化到不同层，以便分离含义。</li>
<li>results: 实验结果表明，通过层划分信息可以提高表达效果，而分离含义更进一步提高表达效果。<details>
<summary>Abstract</summary>
Contextual word embeddings obtained from pre-trained language model (PLM) have proven effective for various natural language processing tasks at the word level. However, interpreting the hidden aspects within embeddings, such as syntax and semantics, remains challenging. Disentangled representation learning has emerged as a promising approach, which separates specific aspects into distinct embeddings. Furthermore, different linguistic knowledge is believed to be stored in different layers of PLM. This paper aims to disentangle semantic sense from BERT by applying a binary mask to middle outputs across the layers, without updating pre-trained parameters. The disentangled embeddings are evaluated through binary classification to determine if the target word in two different sentences has the same meaning. Experiments with cased BERT$_{\texttt{base}$ show that leveraging layer-wise information is effective and disentangling semantic sense further improve performance.
</details>
<details>
<summary>摘要</summary>
Contextual word embeddings obtained from pre-trained language model (PLM) have proven effective for various natural language processing tasks at the word level. However, interpreting the hidden aspects within embeddings, such as syntax and semantics, remains challenging. Disentangled representation learning has emerged as a promising approach, which separates specific aspects into distinct embeddings. Furthermore, different linguistic knowledge is believed to be stored in different layers of PLM. This paper aims to disentangle semantic sense from BERT by applying a binary mask to middle outputs across the layers, without updating pre-trained parameters. The disentangled embeddings are evaluated through binary classification to determine if the target word in two different sentences has the same meaning. Experiments with cased BERT$_{\texttt{base}$ show that leveraging layer-wise information is effective and disentangling semantic sense further improves performance.Here's the translation in Traditional Chinese:Contextual word embeddings obtained from pre-trained language model (PLM) have proven effective for various natural language processing tasks at the word level. However, interpreting the hidden aspects within embeddings, such as syntax and semantics, remains challenging. Disentangled representation learning has emerged as a promising approach, which separates specific aspects into distinct embeddings. Furthermore, different linguistic knowledge is believed to be stored in different layers of PLM. This paper aims to disentangle semantic sense from BERT by applying a binary mask to middle outputs across the layers, without updating pre-trained parameters. The disentangled embeddings are evaluated through binary classification to determine if the target word in two different sentences has the same meaning. Experiments with cased BERT$_{\texttt{base}$ show that leveraging layer-wise information is effective and disentangling semantic sense further improves performance.
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Detection-of-Machine-Generated-Codes"><a href="#Zero-Shot-Detection-of-Machine-Generated-Codes" class="headerlink" title="Zero-Shot Detection of Machine-Generated Codes"></a>Zero-Shot Detection of Machine-Generated Codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05103">http://arxiv.org/abs/2310.05103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baoguangsheng/fast-detect-gpt">https://github.com/baoguangsheng/fast-detect-gpt</a></li>
<li>paper_authors: Xianjun Yang, Kexun Zhang, Haifeng Chen, Linda Petzold, William Yang Wang, Wei Cheng</li>
<li>for: 本研究旨在提出一种不需要训练的方法，用于检测 LLMS 生成的代码，以避免这些代码的不当使用而带来的风险。</li>
<li>methods: 我们修改了之前的零批文本检测方法 DetectGPT（Mitchell et al., 2023），使用一个代理白盒模型来估算最右侧的字符的概率，以便识别由语言模型生成的代码片断。</li>
<li>results: 我们通过对 CodeContest 和 APPS 数据集的 python 代码进行了广泛的实验，并demonstrated 我们的方法可以在 text-davinci-003、GPT-3.5 和 GPT-4 模型上达到领先的检测结果。此外，我们的方法还能够抗 Reynolds 攻击和通用化到 Java 代码。<details>
<summary>Abstract</summary>
This work proposes a training-free approach for the detection of LLMs-generated codes, mitigating the risks associated with their indiscriminate usage. To the best of our knowledge, our research is the first to investigate zero-shot detection techniques applied to code generated by advanced black-box LLMs like ChatGPT. Firstly, we find that existing training-based or zero-shot text detectors are ineffective in detecting code, likely due to the unique statistical properties found in code structures. We then modify the previous zero-shot text detection method, DetectGPT (Mitchell et al., 2023) by utilizing a surrogate white-box model to estimate the probability of the rightmost tokens, allowing us to identify code snippets generated by language models. Through extensive experiments conducted on the python codes of the CodeContest and APPS dataset, our approach demonstrates its effectiveness by achieving state-of-the-art detection results on text-davinci-003, GPT-3.5, and GPT-4 models. Moreover, our method exhibits robustness against revision attacks and generalizes well to Java codes. We also find that the smaller code language model like PolyCoder-160M performs as a universal code detector, outperforming the billion-scale counterpart. The codes will be available at https://github.com/ Xianjun-Yang/Code_detection.git
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种不需要训练的方法，用于检测 LLMs 生成的代码，从而降低这些代码的不当使用所带来的风险。据我们所知，我们的研究是首次应用零shot 检测技术于高级黑盒 LLMs 如 ChatGPT 生成的代码中。我们发现，现有的训练基于或零shot 文本检测器都不能有效地检测代码，可能是因为代码结构的独特统计特性。我们然后对之前的零shot 文本检测方法 DetectGPT（Mitchell et al., 2023）进行修改，通过利用代理白盒模型来估计右侧的最后几个字符的概率，从而识别 LLMs 生成的代码片断。经过对 Python 代码 dataset CodeContest 和 APPS 进行了广泛的实验，我们的方法在 text-davinci-003、GPT-3.5 和 GPT-4 模型上达到了最佳检测结果。此外，我们的方法还能够抗 revision 攻击，并在 Java 代码上显示良好的泛化性。我们还发现，较小的代码语言模型 PolyCoder-160M 可以作为一个通用的代码检测器，超过了一个百亿级模型的性能。代码将在 <https://github.com/Xianjun-Yang/Code_detection.git> 上提供。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-DRL-Based-Adaptive-Region-of-Interest-for-Delay-sensitive-Telemedicine-Applications"><a href="#Intelligent-DRL-Based-Adaptive-Region-of-Interest-for-Delay-sensitive-Telemedicine-Applications" class="headerlink" title="Intelligent DRL-Based Adaptive Region of Interest for Delay-sensitive Telemedicine Applications"></a>Intelligent DRL-Based Adaptive Region of Interest for Delay-sensitive Telemedicine Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05099">http://arxiv.org/abs/2310.05099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdulrahman Soliman, Amr Mohamed, Elias Yaacoub, Nikhil V. Navkar, Aiman Erbad</li>
<li>for: 本研究旨在提高 телемедицина应用的效率和质量，尤其是在 COVID-19 大流行后。</li>
<li>methods: 本研究使用 Deep Reinforcement Learning（DRL）模型，智能调整 ROI 大小和非 ROI 质量，以适应网络带宽变化。</li>
<li>results: 比较结果表明，DRL 模型可以降低延迟率 by 13%，并保持总质量在可接受范围内。这些发现对 телемедицина应用有很大的价值提升。<details>
<summary>Abstract</summary>
Telemedicine applications have recently received substantial potential and interest, especially after the COVID-19 pandemic. Remote experience will help people get their complex surgery done or transfer knowledge to local surgeons, without the need to travel abroad. Even with breakthrough improvements in internet speeds, the delay in video streaming is still a hurdle in telemedicine applications. This imposes using image compression and region of interest (ROI) techniques to reduce the data size and transmission needs. This paper proposes a Deep Reinforcement Learning (DRL) model that intelligently adapts the ROI size and non-ROI quality depending on the estimated throughput. The delay and structural similarity index measure (SSIM) comparison are used to assess the DRL model. The comparison findings and the practical application reveal that DRL is capable of reducing the delay by 13% and keeping the overall quality in an acceptable range. Since the latency has been significantly reduced, these findings are a valuable enhancement to telemedicine applications.
</details>
<details>
<summary>摘要</summary>
随着 télémedicine 应用的潜在和兴趣的不断增长，尤其是在 COVID-19 大流行之后。远程经验可以帮助人们完成复杂的手术或传输知识到地方外科医生，无需出国。尽管互联网速度有了 significative 的改善，但视频流程延迟仍然是 телеmedicine 应用的一大障碍。为了解决这个问题，这篇论文提出了一种基于深度强化学习（DRL）模型，该模型可以智能调整 ROI 大小和非 ROI 质量，以适应估算的吞吐量。延迟和结构相似度指数（SSIM）比较是用于评估 DRL 模型的。对比结果和实际应用显示，DRL 可以降低延迟约 13%，并保持总质量在可接受范围内。由于延迟得到了重要的减少，这些发现对 télémedicine 应用是有价值的改进。
</details></li>
</ul>
<hr>
<h2 id="How-Reliable-Are-AI-Generated-Text-Detectors-An-Assessment-Framework-Using-Evasive-Soft-Prompts"><a href="#How-Reliable-Are-AI-Generated-Text-Detectors-An-Assessment-Framework-Using-Evasive-Soft-Prompts" class="headerlink" title="How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts"></a>How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05095">http://arxiv.org/abs/2310.05095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tharindu Kumarage, Paras Sheth, Raha Moraffah, Joshua Garland, Huan Liu</li>
<li>for: 本研究旨在评估高性能探测器的可靠性，以响应AI生成文本的滥用问题。</li>
<li>methods: 我们提出了一种新的应对方法，即通过调整PLM的软提示来导致PLM生成”人类化”的文本，以诱导探测器做出错误判断。我们在两步中实现了universal逃脱提示：首先，我们为特定PLM设计了逃脱软提示，然后通过软提示的传输性来将学习到的逃脱软提示传递到另一个PLM上。</li>
<li>results: 我们通过多种PLM在不同写作任务中进行了广泛的实验，并评估了逃脱软提示的效果。结果表明，逃脱软提示能够成功地诱导探测器做出错误判断，并且可以在不同的PLM和写作任务中实现高度的可重复性和稳定性。<details>
<summary>Abstract</summary>
In recent years, there has been a rapid proliferation of AI-generated text, primarily driven by the release of powerful pre-trained language models (PLMs). To address the issue of misuse associated with AI-generated text, various high-performing detectors have been developed, including the OpenAI detector and the Stanford DetectGPT. In our study, we ask how reliable these detectors are. We answer the question by designing a novel approach that can prompt any PLM to generate text that evades these high-performing detectors. The proposed approach suggests a universal evasive prompt, a novel type of soft prompt, which guides PLMs in producing "human-like" text that can mislead the detectors. The novel universal evasive prompt is achieved in two steps: First, we create an evasive soft prompt tailored to a specific PLM through prompt tuning; and then, we leverage the transferability of soft prompts to transfer the learned evasive soft prompt from one PLM to another. Employing multiple PLMs in various writing tasks, we conduct extensive experiments to evaluate the efficacy of the evasive soft prompts in their evasion of state-of-the-art detectors.
</details>
<details>
<summary>摘要</summary>
近年来，人工智能生成文本的迅速扩散，主要受到强大预训练语言模型（PLM）的释放所驱动。为了解决人工智能生成文本的违规问题，许多高性能的检测器被开发出来，包括OpenAI检测器和斯坦福DetectGPT。在我们的研究中，我们问到这些检测器的可靠性。我们回答这个问题，我们设计了一种新的方法，可以让任何PLM生成文本，以逃脱这些高性能的检测器。我们的方法建议一种通用逃脱提示，一种新的软提示，可以导引PLM生成“人类化”的文本，使检测器受到误导。我们的新通用逃脱提示包括两个步骤：首先，我们通过提示调整制定一个逃脱软提示，适应特定PLM；然后，我们利用软提示的传输性，将学习的逃脱软提示从一个PLM传递到另一个PLM。通过多种PLM在不同的写作任务中使用，我们进行了广泛的实验来评估逃脱软提示的有效性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Generalizable-Agents-via-Saliency-Guided-Features-Decorrelation"><a href="#Learning-Generalizable-Agents-via-Saliency-Guided-Features-Decorrelation" class="headerlink" title="Learning Generalizable Agents via Saliency-Guided Features Decorrelation"></a>Learning Generalizable Agents via Saliency-Guided Features Decorrelation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05086">http://arxiv.org/abs/2310.05086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sili Huang, Yanchao Sun, Jifeng Hu, Siyuan Guo, Hechang Chen, Yi Chang, Lichao Sun, Bo Yang</li>
<li>for: 实现在视觉基于学习（Reinforcement Learning，RL）中agent能够通过环境变化域对环境变化的应对。</li>
<li>methods: 我们提出了Saliency-Guided Features Decorrelation（SGFD），它包括两个核心技术：Random Fourier Functions（RFF）和Saliency map。 RFF用于估计高维度像像的复杂非线性相关，而Saliency map则用于识别变化的特征。 SGFD透过样本重新权重的方式，以降低相关于变化特征的估计相关性，实现特征decorrelation。</li>
<li>results: 我们的实验结果显示，SGFD可以在广泛的试验环境中实现很好的通过率，并在处理任务不相关的变化和任务相关的变化方面具有明显的改善。<details>
<summary>Abstract</summary>
In visual-based Reinforcement Learning (RL), agents often struggle to generalize well to environmental variations in the state space that were not observed during training. The variations can arise in both task-irrelevant features, such as background noise, and task-relevant features, such as robot configurations, that are related to the optimal decisions. To achieve generalization in both situations, agents are required to accurately understand the impact of changed features on the decisions, i.e., establishing the true associations between changed features and decisions in the policy model. However, due to the inherent correlations among features in the state space, the associations between features and decisions become entangled, making it difficult for the policy to distinguish them. To this end, we propose Saliency-Guided Features Decorrelation (SGFD) to eliminate these correlations through sample reweighting. Concretely, SGFD consists of two core techniques: Random Fourier Functions (RFF) and the saliency map. RFF is utilized to estimate the complex non-linear correlations in high-dimensional images, while the saliency map is designed to identify the changed features. Under the guidance of the saliency map, SGFD employs sample reweighting to minimize the estimated correlations related to changed features, thereby achieving decorrelation in visual RL tasks. Our experimental results demonstrate that SGFD can generalize well on a wide range of test environments and significantly outperforms state-of-the-art methods in handling both task-irrelevant variations and task-relevant variations.
</details>
<details>
<summary>摘要</summary>
在视觉基于的回归学习（RL）中，代理人经常难以通过训练不包括的环境变化来泛化良好。这些变化可能来自任务不相关的特征，如背景噪音，也可能来自任务相关的特征，如机器人配置，都与优化的决策相关。为了在这两种情况下实现泛化，代理人需要准确地理解变化特征对决策的影响，即在政策模型中建立真实的关联。然而，由于状态空间中特征之间的自然相关性，这些关联变得杂乱不清晰，使得政策很难分辨它们。为此，我们提出了吸引力引导特征分解（SGFD），通过样本重新权重来消除这些相关性。SGFD包括两种核心技术：Random Fourier Functions（RFF）和Saliency Map。RFF用于估计高维图像中复杂非线性相关性，而Saliency Map则用于标识变化特征。在Saliency Map的引导下，SGFD通过样本重新权重来减少相关性，从而实现特征分解。我们的实验结果表明，SGFD可以在各种测试环境上广泛泛化，并在处理任务不相关的变化和任务相关的变化方面显著超越当前的方法。
</details></li>
</ul>
<hr>
<h2 id="FLatS-Principled-Out-of-Distribution-Detection-with-Feature-Based-Likelihood-Ratio-Score"><a href="#FLatS-Principled-Out-of-Distribution-Detection-with-Feature-Based-Likelihood-Ratio-Score" class="headerlink" title="FLatS: Principled Out-of-Distribution Detection with Feature-Based Likelihood Ratio Score"></a>FLatS: Principled Out-of-Distribution Detection with Feature-Based Likelihood Ratio Score</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05083">http://arxiv.org/abs/2310.05083</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linhaowei1/flats">https://github.com/linhaowei1/flats</a></li>
<li>paper_authors: Haowei Lin, Yuntian Gu</li>
<li>for: 本文旨在提出一种理论支持的外围样本检测方法，用于帮助NLPT模型在实际应用中更好地识别外围样本。</li>
<li>methods: 本文提出的方法基于likelihood比率的思想，通过对外围分布$\mathcal P_{\textit{out}$和内围分布$\mathcal P_{\textit{in}$的比较，来评估测试样本$\boldsymbol{x}$的”外围性”。而现有的SOTA方法，如Maha和KNN，只计算内围分布$p_{\textit{in}(\boldsymbol{x})$，因此是不优的。</li>
<li>results: 实验表明，提出的FLatS方法可以在 популяр的 benchmark 上建立新的SOTA。此外，FLatS 还可以增强其他OOD检测方法，通过包含外围分布 $p_{\textit{out}(\boldsymbol{x})$ 的估计。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) instances is crucial for NLP models in practical applications. Although numerous OOD detection methods exist, most of them are empirical. Backed by theoretical analysis, this paper advocates for the measurement of the "OOD-ness" of a test case $\boldsymbol{x}$ through the likelihood ratio between out-distribution $\mathcal P_{\textit{out}$ and in-distribution $\mathcal P_{\textit{in}$. We argue that the state-of-the-art (SOTA) feature-based OOD detection methods, such as Maha and KNN, are suboptimal since they only estimate in-distribution density $p_{\textit{in}(\boldsymbol{x})$. To address this issue, we propose FLatS, a principled solution for OOD detection based on likelihood ratio. Moreover, we demonstrate that FLatS can serve as a general framework capable of enhancing other OOD detection methods by incorporating out-distribution density $p_{\textit{out}(\boldsymbol{x})$ estimation. Experiments show that FLatS establishes a new SOTA on popular benchmarks. Our code is publicly available at https://github.com/linhaowei1/FLatS.
</details>
<details>
<summary>摘要</summary>
检测外部分布（OOD）实例是NLTP模型在实际应用中的关键。虽然有许多OOD检测方法存在，但大多数都是经验的。本文通过理论分析，提出测量测试 caso $\boldsymbol{x}$ 的 "OOD-ness" 通过likelihood比率计算，即在外部分布 $\mathcal P_{\textit{out}$ 和内部分布 $\mathcal P_{\textit{in}$ 之间的比较。我们认为现有的SOTA feature-based OOD检测方法，如Maha和KNN，是不佳的，因为它们只估计内部分布 $p_{\textit{in}(\boldsymbol{x})$。为解决这一问题，我们提出了FLatS，一种理解的OOD检测方法，基于likelihood比率。此外，我们还证明FLatS可以增强其他OOD检测方法，通过包含外部分布 $p_{\textit{out}(\boldsymbol{x})$ 估计。实验表明，FLatS在 популяр的benchmark上建立了新的SOTA。我们的代码在https://github.com/linhaowei1/FLatS上公开。
</details></li>
</ul>
<hr>
<h2 id="“A-Nova-Eletricidade-Aplicacoes-Riscos-e-Tendencias-da-IA-Moderna-–-“The-New-Electricity”-Applications-Risks-and-Trends-in-Current-AI"><a href="#“A-Nova-Eletricidade-Aplicacoes-Riscos-e-Tendencias-da-IA-Moderna-–-“The-New-Electricity”-Applications-Risks-and-Trends-in-Current-AI" class="headerlink" title="“A Nova Eletricidade: Aplicações, Riscos e Tendências da IA Moderna – “The New Electricity”: Applications, Risks, and Trends in Current AI"></a>“A Nova Eletricidade: Aplicações, Riscos e Tendências da IA Moderna – “The New Electricity”: Applications, Risks, and Trends in Current AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18324">http://arxiv.org/abs/2310.18324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana L. C. Bazzan, Anderson R. Tavares, André G. Pereira, Cláudio R. Jung, Jacob Scharcanski, Joel Luis Carbonera, Luís C. Lamb, Mariana Recamonde-Mendoza, Thiago L. T. da Silveira, Viviane Moreira<br>for:* The paper is written to provide an overview of the ever-evolving landscape of Artificial Intelligence (AI) and its applications in various sectors of the economy, impacting society and humanity.methods:* The paper analyzes the risks that come with rapid technological progress and future trends in AI, as well as the potential for AI to become a general-purpose technology like electricity.results:* The paper explores the transformative impact of AI on society, with the potential to revolutionize sectors of the economy and impact humanity in the same way that electricity did in the 19th and 20th centuries.<details>
<summary>Abstract</summary>
The thought-provoking analogy between AI and electricity, made by computer scientist and entrepreneur Andrew Ng, summarizes the deep transformation that recent advances in Artificial Intelligence (AI) have triggered in the world. This chapter presents an overview of the ever-evolving landscape of AI, written in Portuguese. With no intent to exhaust the subject, we explore the AI applications that are redefining sectors of the economy, impacting society and humanity. We analyze the risks that may come along with rapid technological progress and future trends in AI, an area that is on the path to becoming a general-purpose technology, just like electricity, which revolutionized society in the 19th and 20th centuries.   A provocativa compara\c{c}\~ao entre IA e eletricidade, feita pelo cientista da computa\c{c}\~ao e empreendedor Andrew Ng, resume a profunda transforma\c{c}\~ao que os recentes avan\c{c}os em Intelig\^encia Artificial (IA) t\^em desencadeado no mundo. Este cap\'itulo apresenta uma vis\~ao geral pela paisagem em constante evolu\c{c}\~ao da IA. Sem pretens\~oes de exaurir o assunto, exploramos as aplica\c{c}\~oes que est\~ao redefinindo setores da economia, impactando a sociedade e a humanidade. Analisamos os riscos que acompanham o r\'apido progresso tecnol\'ogico e as tend\^encias futuras da IA, \'area que trilha o caminho para se tornar uma tecnologia de prop\'osito geral, assim como a eletricidade, que revolucionou a sociedade dos s\'eculos XIX e XX.
</details>
<details>
<summary>摘要</summary>
思想提出的人工智能和电力相似性比喻，由计算机科学家和企业家安드鲁·涅（Andrew Ng）提出，概括了由最近的人工智能技术进步所Trigger的深刻变革。本章介绍了人工智能领域的不断发展，无意尝试涵盖所有方面。我们探讨人工智能在经济、社会和人类生活中的应用，以及可能随着技术进步而出现的风险。我们还分析了人工智能的未来趋势，该领域正在踏上成为一种通用技术的道路，类似于电力在19世纪和20世纪所 triggers 的社会革命。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="DialCoT-Meets-PPO-Decomposing-and-Exploring-Reasoning-Paths-in-Smaller-Language-Models"><a href="#DialCoT-Meets-PPO-Decomposing-and-Exploring-Reasoning-Paths-in-Smaller-Language-Models" class="headerlink" title="DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models"></a>DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05074">http://arxiv.org/abs/2310.05074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hccngu/dialcot">https://github.com/hccngu/dialcot</a></li>
<li>paper_authors: Chengcheng Han, Xiaowei Du, Che Zhang, Yixin Lian, Xiang Li, Ming Gao, Baoyuan Wang</li>
<li>for: 提高小语言模型（SLM）的逻辑能力</li>
<li>methods: 对 reasoning 任务进行对话指导，并使用 proximal policy optimization（PPO）算法优化逻辑路径选择</li>
<li>results: 在四个算术逻辑 dataset 上实现了显著性能提升，比前一代竞争者更好<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. To address this limitation, we introduce Dialogue-guided Chain-of-Thought (DialCoT) which employs a dialogue format to generate intermediate reasoning steps, guiding the model toward the final answer. Additionally, we optimize the model's reasoning path selection using the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Our method offers several advantages compared to previous approaches. Firstly, we transform the process of solving complex reasoning questions by breaking them down into a series of simpler sub-questions, significantly reducing the task difficulty and making it more suitable for SLMs. Secondly, we optimize the model's reasoning path selection through the PPO algorithm. We conduct comprehensive experiments on four arithmetic reasoning datasets, demonstrating that our method achieves significant performance improvements compared to state-of-the-art competitors.
</details>
<details>
<summary>摘要</summary>
大脑语言模型（LLM）的逻辑能力可以通过链条思维（CoT）提示来提高，但是当应用于少于100亿参数的小语言模型（SLM）时，CoT的效果减弱或者甚至有害。为了解决这个局限性，我们提出了对话引导链条思维（DialCoT），它使用对话格式生成中间逻辑步骤，导引模型到答案。此外，我们使用距离策略优化（PPO）算法优化模型的逻辑路径选择，进一步提高其逻辑能力。我们的方法具有以下优势：首先，我们将复杂的逻辑问题转化为一系列更加简单的子问题，从而大大减轻任务难度，使SLM更适合处理。其次，我们通过PPO算法优化模型的逻辑路径选择，从而提高模型的逻辑能力。我们对四个数学逻辑数据集进行了广泛的实验，显示我们的方法与当前的竞争对手相比有显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Video-CSR-Complex-Video-Digest-Creation-for-Visual-Language-Models"><a href="#Video-CSR-Complex-Video-Digest-Creation-for-Visual-Language-Models" class="headerlink" title="Video-CSR: Complex Video Digest Creation for Visual-Language Models"></a>Video-CSR: Complex Video Digest Creation for Visual-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05060">http://arxiv.org/abs/2310.05060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingkai Liu, Yunzhe Tao, Haogeng Liu, Qihang Fan, Ding Zhou, Huaibo Huang, Ran He, Hongxia Yang</li>
<li>for: 这个论文是用来评估视频语言模型的captioning、摘要和检索能力的新任务和人工标注数据集。</li>
<li>methods: 该数据集包含4.8万个YouTube视频clip，每个clip长度在20-60秒之间，覆盖了各种主题和兴趣。每个视频clip都有5个独立的标注caption（1句）和摘要（3-10句）。</li>
<li>results: 给任意选择的视频和其相应的ASR信息，我们评估视频语言模型在caption和摘要生成任务中，以及基于caption和摘要的检索任务中的表现。此外，我们还进行了评估不同的现有评价 metric的alignment with human preferences，并提出了一个基eline模型，以便作为Video-CSR任务的参考点。<details>
<summary>Abstract</summary>
We present a novel task and human annotated dataset for evaluating the ability for visual-language models to generate captions and summaries for real-world video clips, which we call Video-CSR (Captioning, Summarization and Retrieval). The dataset contains 4.8K YouTube video clips of 20-60 seconds in duration and covers a wide range of topics and interests. Each video clip corresponds to 5 independently annotated captions (1 sentence) and summaries (3-10 sentences). Given any video selected from the dataset and its corresponding ASR information, we evaluate visual-language models on either caption or summary generation that is grounded in both the visual and auditory content of the video. Additionally, models are also evaluated on caption- and summary-based retrieval tasks, where the summary-based retrieval task requires the identification of a target video given excerpts of a corresponding summary. Given the novel nature of the paragraph-length video summarization task, we perform extensive comparative analyses of different existing evaluation metrics and their alignment with human preferences. Finally, we propose a foundation model with competitive generation and retrieval capabilities that serves as a baseline for the Video-CSR task. We aim for Video-CSR to serve as a useful evaluation set in the age of large language models and complex multi-modal tasks.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新的任务和人标注数据集，用于评估视觉语言模型对真实视频片段的captioning、摘要和搜索能力，我们称之为Video-CSR（captioning、摘要和搜索）。该数据集包含4.8万个YouTube视频片段，每个片段长度在20-60秒之间，覆盖了广泛的主题和兴趣。每个视频片段对应着5个独立 annotated captions（1句）和摘要（3-10句）。给任意选择的视频和其相应的ASR信息，我们评估视觉语言模型在caption或摘要生成 tasks中的能力，这些任务都基于视频的视觉和听音内容。此外，我们还评估模型在caption-和摘要基于搜索任务中的能力，其中摘要基于搜索任务需要根据视频摘要的剪辑来标识目标视频。由于文章长度视频摘要任务的新性，我们进行了详细的比较分析不同的评估指标和其与人类偏好的对齐。最后，我们提出了一个基础模型，具有竞争力强的生成和搜索能力，作为Video-CSR任务的基线模型。我们希望Video-CSR能够在大型语言模型和复杂多Modal任务时代发挥作用。
</details></li>
</ul>
<hr>
<h2 id="Learning-Separable-Hidden-Unit-Contributions-for-Speaker-Adaptive-Lip-Reading"><a href="#Learning-Separable-Hidden-Unit-Contributions-for-Speaker-Adaptive-Lip-Reading" class="headerlink" title="Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading"></a>Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05058">http://arxiv.org/abs/2310.05058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songtao Luo, Shuang Yang, Shiguang Shan, Xilin Chen</li>
<li>for: 本文提出了一种基于两个观察点的新方法，用于 speaker adaptation  lip reading，目的是提高 lip reading 的精度和稳定性。</li>
<li>methods: 本文使用了 shallow 和 deep 层，将 speaker 的特征分别处理为 two different targets，以便自动学习 separable hidden unit contributions。在 shallow 层中，引入 speaker-adaptive features 来增强 speech content 相关的特征；在 deep 层中，引入 speaker-adaptive features 来抑制 speech content 不相关的噪音。</li>
<li>results: 本文的方法在不同设置下进行了广泛的分析和比较，并 consistently 超过了现有方法的性能。此外，本文还发布了一个新的测试集 CAS-VSR-S68h，以进一步评估在只有几个 speaker 的情况下，但涵盖了大量和多样化的 speech content 的情况下的性能。<details>
<summary>Abstract</summary>
In this paper, we propose a novel method for speaker adaptation in lip reading, motivated by two observations. Firstly, a speaker's own characteristics can always be portrayed well by his/her few facial images or even a single image with shallow networks, while the fine-grained dynamic features associated with speech content expressed by the talking face always need deep sequential networks to represent accurately. Therefore, we treat the shallow and deep layers differently for speaker adaptive lip reading. Secondly, we observe that a speaker's unique characteristics ( e.g. prominent oral cavity and mandible) have varied effects on lip reading performance for different words and pronunciations, necessitating adaptive enhancement or suppression of the features for robust lip reading. Based on these two observations, we propose to take advantage of the speaker's own characteristics to automatically learn separable hidden unit contributions with different targets for shallow layers and deep layers respectively. For shallow layers where features related to the speaker's characteristics are stronger than the speech content related features, we introduce speaker-adaptive features to learn for enhancing the speech content features. For deep layers where both the speaker's features and the speech content features are all expressed well, we introduce the speaker-adaptive features to learn for suppressing the speech content irrelevant noise for robust lip reading. Our approach consistently outperforms existing methods, as confirmed by comprehensive analysis and comparison across different settings. Besides the evaluation on the popular LRW-ID and GRID datasets, we also release a new dataset for evaluation, CAS-VSR-S68h, to further assess the performance in an extreme setting where just a few speakers are available but the speech content covers a large and diversified range.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的lip reading speaker adaptation方法，基于两个观察结论。首先，一个说话人的自己特征可以通过他/她的一些脸部图像或者even a single image with shallow networks来表示得非常好，而言语内容表达在说话脸上的细腻动态特征则需要深度顺序网络来表示准确。因此，我们将浅层和深层处理 differently。其次，我们发现说话人的独特特征（例如嘴巴和下颌）对不同的话语和发音有不同的影响，需要根据不同的话语和发音进行适应增强或减弱这些特征以实现Robust lip reading。基于这两个观察结论，我们提出了利用说话人自己的特征自动学习可分离的隐藏单元贡献，其中浅层的特征与话语内容相关的特征更强，我们引入说话人特征学习以增强话语内容相关的特征。深层的特征则是说话人特征和话语内容相关的特征都很好地表示，我们引入说话人特征学习以减弱话语内容无关的噪音以实现Robust lip reading。我们的方法在不同的设置下 consistently outperform了现有方法，经过了全面的分析和比较。除了在popular LRW-ID和GRID dataset上进行评估外，我们还发布了一个新的测试集，CAS-VSR-S68h，以进一步评估在只有几个说话人的情况下，但是说话内容覆盖了广泛而多样化的情况下的性能。
</details></li>
</ul>
<hr>
<h2 id="FP3O-Enabling-Proximal-Policy-Optimization-in-Multi-Agent-Cooperation-with-Parameter-Sharing-Versatility"><a href="#FP3O-Enabling-Proximal-Policy-Optimization-in-Multi-Agent-Cooperation-with-Parameter-Sharing-Versatility" class="headerlink" title="FP3O: Enabling Proximal Policy Optimization in Multi-Agent Cooperation with Parameter-Sharing Versatility"></a>FP3O: Enabling Proximal Policy Optimization in Multi-Agent Cooperation with Parameter-Sharing Versatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05053">http://arxiv.org/abs/2310.05053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lang Feng, Dong Xing, Junru Zhang, Gang Pan</li>
<li>for: 提高多代理人PPO算法的合作多代理人学习（MARL）理论保证性。</li>
<li>methods: 基于全管道思想，实现多平行优化管道，通过不同的等价分解方法表示代理人之间的连接。</li>
<li>results: FP3O算法在多代理人MuJoCo和StarCraftII任务上表现出色，超过了其他强基eline，并在不同的参数共享配置下展现了强大的可变性。<details>
<summary>Abstract</summary>
Existing multi-agent PPO algorithms lack compatibility with different types of parameter sharing when extending the theoretical guarantee of PPO to cooperative multi-agent reinforcement learning (MARL). In this paper, we propose a novel and versatile multi-agent PPO algorithm for cooperative MARL to overcome this limitation. Our approach is achieved upon the proposed full-pipeline paradigm, which establishes multiple parallel optimization pipelines by employing various equivalent decompositions of the advantage function. This procedure successfully formulates the interconnections among agents in a more general manner, i.e., the interconnections among pipelines, making it compatible with diverse types of parameter sharing. We provide a solid theoretical foundation for policy improvement and subsequently develop a practical algorithm called Full-Pipeline PPO (FP3O) by several approximations. Empirical evaluations on Multi-Agent MuJoCo and StarCraftII tasks demonstrate that FP3O outperforms other strong baselines and exhibits remarkable versatility across various parameter-sharing configurations.
</details>
<details>
<summary>摘要</summary>
现有的多代理PPO算法缺乏扩展 тео리тиче guarantee of PPO to cooperative multi-agent reinforcement learning (MARL) 中的兼容性。在这篇文章中，我们提出了一种新的和灵活的多代理PPO算法，以超越这些限制。我们的方法基于我们提出的全管道 paradigm，该 paradigm在利用多种等价 decompositions of the advantage function 中实现多个并行的优化管道。这个过程成功地表达了多个代理之间的连接，即多个管道之间的连接，使其与不同类型的参数共享兼容。我们提供了强有力的理论基础，以便策略提高，并在后续开发了一种实用的FP3O算法。在Multi-Agent MuJoCo和StarCraftII任务上的实验评估中，FP3O的表现超过了其他强大的基准，并且在不同的参数共享配置下展现出了remarkable的灵活性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Intra-and-Inter-Cell-Differences-for-Accurate-Battery-Lifespan-Prediction-across-Diverse-Conditions"><a href="#Learning-Intra-and-Inter-Cell-Differences-for-Accurate-Battery-Lifespan-Prediction-across-Diverse-Conditions" class="headerlink" title="Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions"></a>Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05052">http://arxiv.org/abs/2310.05052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhang, Yuqi Li, Shun Zheng, Ziheng Lu, Xiaofan Gui, Wei Xu, Jiang Bian</li>
<li>for: 预测电池寿命的研究具有实际应用价值，尤其是在电池研发中。现有的数据驱动模型大多依靠特定电池的早期电学信号来预测它的寿命。然而，这些模型受限于特定腐食条件，这不仅限制了它们的模型能力，还使其在不同条件下预测腐食的效果减退。因此，这些模型经常错过了可以从另一些条件下的历史数据中获得的全部 beneficial。</li>
<li>methods: 我们引入了一种方法，可以考虑target电池和参照电池之间的差异，无论它们的材料和腐食条件如何。通过这种差异，我们不仅扩大了特征空间，而且开辟了一个通用的电池寿命预测框架。我们的模型结合了inter-和intra-cell差异，在多种条件下表现出了极高的效率和准确率，使用了所有可用的数据集。</li>
<li>results: 我们的方法可以充分利用older电池的数据，使 newer电池可以借鉴过去的电池的经验。这种方法不仅拓宽了电池数据利用策略，还为未来的电池管理系统提供了智能化的基础。<details>
<summary>Abstract</summary>
Battery life prediction holds significant practical value for battery research and development. Currently, many data-driven models rely on early electrical signals from specific target batteries to predict their lifespan. A common shortfall is that most existing methods are developed based on specific aging conditions, which not only limits their model's capability but also diminishes their effectiveness in predicting degradation under varied conditions. As a result, these models often miss out on fully benefiting from the rich historical data available under other conditions. Here, to address above, we introduce an approach that explicitly captures differences between electrical signals of a target battery and a reference battery, irrespective of their materials and aging conditions, to forecast the target battery life. Through this inter-cell difference, we not only enhance the feature space but also pave the way for a universal battery life prediction framework. Remarkably, our model that combines the inter- and intra-cell differences shines across diverse conditions, standing out in its efficiency and accuracy using all accessible datasets. An essential application of our approach is its capability to leverage data from older batteries effectively, enabling newer batteries to capitalize on insights gained from past batteries. This work not only enriches the battery data utilization strategy but also sets the stage for smarter battery management system in the future.
</details>
<details>
<summary>摘要</summary>
预测电池寿命具有重要的实践价值，对电池研发具有重要的意义。目前，许多数据驱动模型依靠特定目标电池早期的电学信号来预测它们的寿命。然而，大多数现有方法都是基于特定腐蚀条件下开发的，这不仅限制了他们的模型能力，而且降低了它们在不同条件下预测腐蚀的效果。因此，这些模型经常会错过利用可用的历史数据来预测腐蚀情况。在这里，我们引入了一种方法，可以跨电池和参照电池之间的差异来预测目标电池寿命。通过这种差异，我们不仅扩大了特征空间，而且开创了一个通用的电池寿命预测框架。另外，我们的模型结合了差异和内部差异，在多种条件下表现出色，高效精准地使用所有可用的数据集。这种方法不仅可以有效地利用较老的电池数据，还可以为未来的电池管理系统提供智能化的基础。
</details></li>
</ul>
<hr>
<h2 id="From-Text-to-Tactic-Evaluating-LLMs-Playing-the-Game-of-Avalon"><a href="#From-Text-to-Tactic-Evaluating-LLMs-Playing-the-Game-of-Avalon" class="headerlink" title="From Text to Tactic: Evaluating LLMs Playing the Game of Avalon"></a>From Text to Tactic: Evaluating LLMs Playing the Game of Avalon</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05036">http://arxiv.org/abs/2310.05036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jonathanmli/avalon-llm">https://github.com/jonathanmli/avalon-llm</a></li>
<li>paper_authors: Jonathan Light, Min Cai, Sheng Shen, Ziniu Hu</li>
<li>for: 这篇论文探讨了大语言模型代理人（LLM）在游戏《抵抗avalon》中的潜力。</li>
<li>methods: 作者们使用了一个名为AvalonBench的游戏环境，以评估多代理LML模型。这个环境包括avalon游戏环境、基于规则的bot对手和ReAct风格的LML代理人。</li>
<li>results: 作者们的评估结果显示，使用AvalonBench评估LML模型时存在明显的能力差距。例如，使用ChatGPT扮演善良角色时，与基于规则的bot对手扮演邪恶角色的情况下，win rate为22.2%，而使用基于规则的bot扮演善良角色时，win rate为38.2%。<details>
<summary>Abstract</summary>
In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots playing evil, while good-role bot achieves 38.2% win rate in the same setting. We envision AvalonBench could be a good test-bed for developing more advanced LLMs (with self-playing) and agent frameworks that can effectively model the layered complexities of such game environments.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨了大语言模型代理人（LLM）在游戏《巨大叛逆：阿瓦隆》中的潜力。游戏中的玩家不仅需要根据不断发展的游戏阶段进行了解的决策，还需要与其他玩家进行交流，包括谎言、推理和谈判。这些特点使得阿瓦隆成为了研究LLM代理人决策和语言处理能力的有力的测试场景。为了促进这种研究，我们介绍了阿瓦隆Bench，一个包含以下三个重要组成部分的游戏环境：（1）阿瓦隆游戏环境，（2）基于规则的 bot 作为基准对手，以及（3）ReAct 风格的 LLM 代理人，每个角色都有适应的提示。我们的评估结果表明，与基于规则的 bot 作为邪恶对手进行比较，ChatGPT 扮演善良角色时的胜率为 22.2%，而基于规则的 bot 扮演善良角色时的胜率为 38.2%。我们认为阿瓦隆Bench 可以成为 LLM 的发展和自适应代理人框架的试验场景。
</details></li>
</ul>
<hr>
<h2 id="Self-Convinced-Prompting-Few-Shot-Question-Answering-with-Repeated-Introspection"><a href="#Self-Convinced-Prompting-Few-Shot-Question-Answering-with-Repeated-Introspection" class="headerlink" title="Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection"></a>Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05035">http://arxiv.org/abs/2310.05035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodi Zhang, Min Cai, Xinhe Zhang, Chen Jason Zhang, Rui Mao, Kaishun Wu</li>
<li>for: 提高大语言模型（LLM）的复杂理解和具有技巧使用能力</li>
<li>methods: 使用提前训练的语言模型、询问、检查和修改步骤</li>
<li>results: 实验结果 validate Self-Convince 框架的有效性，与基准值进行比较获得了显著提高<details>
<summary>Abstract</summary>
While large language models (LLMs) such as ChatGPT and PaLM have demonstrated remarkable performance in various language understanding and generation tasks, their capabilities in complex reasoning and intricate knowledge utilization still fall short of human-level proficiency. Recent studies have established the effectiveness of prompts in steering LLMs towards generating desired outputs. Building on these insights, we introduce a novel framework that harnesses the potential of large-scale pre-trained language models, to iteratively enhance performance of the LLMs. Our framework incorporates three components: \textit{Normal CoT}, a \textit{Convincer}, and an \textit{Answerer}. It processes the output of a typical few-shot chain-of-thought prompt, assesses the correctness of the response, scrutinizes the answer, refines the reasoning, and ultimately produces a new solution. Experimental results on the 7 datasets of miscellaneous problems validate the efficacy of the Self-Convince framework, achieving substantial improvements compared to the baselines. This study contributes to the burgeoning body of research focused on integrating pre-trained language models with tailored prompts and iterative refinement processes to augment their performance in complex tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如ChatGPT和PaLM在不同的语言理解和生成任务中表现出色，但它们在复杂的推理和细节知识利用方面仍然落后人类水平。现在的研究显示了干预提示的效果，可以导引LLM生成所需的出力。基于这些见解，我们提出了一个新的框架，叫做Self-Convince框架。这个框架包含三个 ком成分： Normal CoT、Convincer 和 Answerer。它在一般几步链接思维提示的出力中进行处理，评估回应的正确性，探究答案，删除错误的推理，最终生成一个新的解决方案。实验结果显示，Self-Convince框架在7个多元问题的数据集上实现了重要的改善，较基于点的表现有所提高。这项研究将大型预训语言模型与定制提示和迭代改进过程相结合，以增强它们在复杂任务中的表现。
</details></li>
</ul>
<hr>
<h2 id="Counter-Turing-Test-CT-2-AI-Generated-Text-Detection-is-Not-as-Easy-as-You-May-Think-–-Introducing-AI-Detectability-Index"><a href="#Counter-Turing-Test-CT-2-AI-Generated-Text-Detection-is-Not-as-Easy-as-You-May-Think-–-Introducing-AI-Detectability-Index" class="headerlink" title="Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index"></a>Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think – Introducing AI Detectability Index</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05030">http://arxiv.org/abs/2310.05030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megha Chakraborty, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Krish Sharma, Niyar R Barman, Chandan Gupta, Shreya Gautam, Tanay Kumar, Vinija Jain, Aman Chadha, Amit P. Sheth, Amitava Das</li>
<li>for: 这篇论文主要旨在评估当前的AI生成文本检测技术的robustness，以及评估不同大小的自然语言处理模型（LLMs）在生成文本检测中的可探测性。</li>
<li>methods: 这篇论文提出了Counter Turing Test（CT^2）作为一个完整的评估AI生成文本检测技术的标准 benchark。它们还提出了一个名为AI Detectability Index（ADI）的指标，用于评估不同大小的LLMs在生成文本检测中的可探测性。</li>
<li>results: 这篇论文的实验结果表明，现有的AI生成文本检测技术在面对CT^2的测试中具有较弱的可探测性。此外，研究发现大型LLMs具有较高的AI Detectability Index（ADI），这意味着它们在生成文本检测中更难被检测。<details>
<summary>Abstract</summary>
With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that 'If a work's traditional elements of authorship were produced by a machine, the work lacks human authorship and the Office will not register it'. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT^2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a higher ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making.
</details>
<details>
<summary>摘要</summary>
随着智能ChatGPT的出现，人工智能生成文本的风险和后果增加了致命地。为了解决人工智能生成文本的所有权归属问题，美国版权办公室发表了一份声明，表示如果文本中的传统元素的作者是机器制造出来的，那么文本就缺乏人类作者，因此不会注册。此外，美国和欧盟政府最近已经起草了关于人工智能的规制框架的初步提案。随着生成AI的焦点逐渐吸引到研究领域，AI生成文本检测（AGTD）已经成为研究的热点，一些初步的方法已经被提出，然后又有人提出了绕过检测的技术。本文介绍了Counter Turing Test（CT^2），一种包含了检测AGTD技术的多种方法的benchmark。我们的实验结果明显地表明，现有的AGTD方法在审查中表现极其脆弱。在政策制定的过程中，评估人工智能生成内容的可检测性是非常重要的。因此，我们提出了人工智能可检测指数（ADI），以评估和排名15种当代LLMs的可检测性水平。我们通过实验证明，大型LLMs tend to have higher ADI， indicating that they are less detectable compared to smaller LLMs。我们认为ADI具有重要的价值，可以作为NLP社区中的工具，并且可能在AI相关的政策制定中扮演重要的角色。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Large-Language-Models-as-Zero-shot-Relation-Extractors"><a href="#Revisiting-Large-Language-Models-as-Zero-shot-Relation-Extractors" class="headerlink" title="Revisiting Large Language Models as Zero-shot Relation Extractors"></a>Revisiting Large Language Models as Zero-shot Relation Extractors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05028">http://arxiv.org/abs/2310.05028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guozheng Li, Peng Wang, Wenjun Ke</li>
<li>for: 这个论文主要研究了使用大语言模型（LLM）进行零shot关系EXTRACTION（RE）。</li>
<li>methods: 本研究使用了Chain-of-thought（CoT）技术和summarize-and-ask（\textsc{SumAsk}）提示法来提高零shot RE的性能。</li>
<li>results: 研究发现，\textsc{SumAsk}可以Consistently和Significantly提高LLMs在不同的模型大小、benchmark和设置下的性能。此外，零shot提示与ChatGPT比较或超过了零shot和完全监督方法的性能。LLMs也能够Handle Challenge none-of-the-above（NoTA）关系，但关系性能差异较大。<details>
<summary>Abstract</summary>
Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the following findings: (i) \textsc{SumAsk} consistently and significantly improves LLMs performance on different model sizes, benchmarks and settings; (ii) Zero-shot prompting with ChatGPT achieves competitive or superior results compared with zero-shot and fully supervised methods; (iii) LLMs deliver promising performance in extracting overlapping relations; (iv) The performance varies greatly regarding different relations. Different from small language models, LLMs are effective in handling challenge none-of-the-above (NoTA) relation.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CN关系提取（RE）总是需要一定量的标注或未标注数据，即使在零shot设定下。现代语言模型（LLM）可以轻松地在新任务上进行升级，只需要一个自然语言提示，这提供了提取关系从文本中的可能性。本研究将关注使用LLM，如ChatGPT，作为零shot关系提取器的研究。一方面，我们分析了现有RE提示的缺点，并尝试使用最新的提示技术，如链条思维（CoT），来改进零shot RE。我们提出了摘要并问 (\textsc{SumAsk})提示，一种简单的提示，使用LLM recursively将RE输入转换为有效的问答（QA）格式。另一方面，我们对多个benchmark和设置进行了广泛的实验，以研究LLM在零shot RE中的能力。我们得到以下发现：(i) \textsc{SumAsk}在不同的模型大小、benchmark和设置上具有一致性和显著性，提高LLM的表现。(ii) 采用ChatGPT的零shot提示可以与零shot和完全监督方法相比，在不同的任务和设置上达到竞争或更高的性能。(iii) LLM在抽象关系提取方面表现出色，特别是在抽象关系上。(iv) 不同的关系之间的表现差异较大，而LLM在处理抽象关系方面表现更佳。与小语言模型相比，LLM在处理抽象关系方面表现更出色。
</details></li>
</ul>
<hr>
<h2 id="Fully-Spiking-Neural-Network-for-Legged-Robots"><a href="#Fully-Spiking-Neural-Network-for-Legged-Robots" class="headerlink" title="Fully Spiking Neural Network for Legged Robots"></a>Fully Spiking Neural Network for Legged Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05022">http://arxiv.org/abs/2310.05022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyang Jiang, Qiang Zhang, Jingkai Sun, Renjing Xu</li>
<li>For: The paper aims to improve the performance of legged robots using a novel Spiking Neural Network (SNN) to process body perception signals, achieving better speed and energy consumption, and improved biological interpretability.* Methods: The paper employs a SNN to process legged robots’ perception signals, which offers improved biological interpretability and natural advantages in inference speed and energy consumption compared to traditional artificial neural networks.* Results: The paper achieves outstanding results across a range of simulated terrains, demonstrating the effectiveness of SNN in legged robots.<details>
<summary>Abstract</summary>
In recent years, legged robots based on deep reinforcement learning have made remarkable progress. Quadruped robots have demonstrated the ability to complete challenging tasks in complex environments and have been deployed in real-world scenarios to assist humans. Simultaneously, bipedal and humanoid robots have achieved breakthroughs in various demanding tasks. Current reinforcement learning methods can utilize diverse robot bodies and historical information to perform actions. However, prior research has not emphasized the speed and energy consumption of network inference, as well as the biological significance of the neural networks themselves. Most of the networks employed are traditional artificial neural networks that utilize multilayer perceptrons (MLP). In this paper, we successfully apply a novel Spiking Neural Network (SNN) to process legged robots, achieving outstanding results across a range of simulated terrains. SNN holds a natural advantage over traditional neural networks in terms of inference speed and energy consumption, and their pulse-form processing of body perception signals offers improved biological interpretability. To the best of our knowledge, this is the first work to implement SNN in legged robots.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Compresso-Structured-Pruning-with-Collaborative-Prompting-Learns-Compact-Large-Language-Models"><a href="#Compresso-Structured-Pruning-with-Collaborative-Prompting-Learns-Compact-Large-Language-Models" class="headerlink" title="Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models"></a>Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05015">http://arxiv.org/abs/2310.05015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/moonlit">https://github.com/microsoft/moonlit</a></li>
<li>paper_authors: Song Guo, Jiahang Xu, Li Lyna Zhang, Mao Yang</li>
<li>for: 这篇研究的目的是为了提高大型语言模型（LLM）的部署，特别是在具有限制的硬件资源的环境下。</li>
<li>methods: 这篇研究使用了一种新的架构，叫做Compresso，它通过与LLM的协作，在训练过程中学习最佳的剪辑决策。Compresso使用了LoRA技术来实现$L_0$规律，并在调询过程中引入了协同提示，以增强整体性能。</li>
<li>results: 根据实验结果，Compresso可以将LLaMA-7B剪辑到5.4B，保持原始性能，甚至在阅读理解测试中超过LLaMA-7B的表现。Compresso比一项基eline的一项单一剪辑方法（one-shot pruning）有更高的表现，在不同的组合比例下，可以达到2.21%, 11.43%, 7.04%, 4.81%更高的分数在 Commonsense Reasoning、Reading Comprehension、MMLU和BBH测试中。<details>
<summary>Abstract</summary>
Despite the remarkable success of Large Language Models (LLMs), the massive size poses significant deployment challenges, particularly on resource-constrained hardware. While existing LLM compression methods focus on quantization, pruning remains relatively unexplored due to the high cost of training-based approaches and data collection challenges. One-shot pruning methods, although cost-effective and data-free, have become dominant in LLM pruning, but lead to performance decline under the structured pruning setting. In this work, we introduce a new paradigm for structurally pruning LLMs, called Compresso. Our approach, through the collaboration of the proposed resource-efficient pruning algorithm and the LLM itself, learns optimal pruning decisions during the training process. Compresso addresses the challenges of expensive training costs and data collection by incorporating Low-Rank Adaptation (LoRA) into the $L_0$ regularization during the instruction tuning process. Then, we further augment the pruning algorithm by introducing a collaborative prompt that fosters collaboration between the LLM and the pruning algorithm, significantly boosting the overall performance. To this end, Compresso prunes LLaMA-7B to 5.4B, maintaining original performance and even surpassing LLaMA-7B in reading comprehension by 2.62%. Extensive experiments demonstrate that Compresso significantly outperforms one-shot pruning baselines across various sparsity ratios, achieving up to 2.21%, 11.43%, 7.04%, and 4.81% higher scores on the commonsense reasoning, reading comprehension, MMLU, and BBH benchmarks, respectively.
</details>
<details>
<summary>摘要</summary>
尽管大型自然语言模型（LLM）已经取得了非常出色的成功，但是它们的巨大大小却对资源有限的硬件 pose 了部署的挑战。现有的 LLM 压缩方法主要集中在量化上，而采用 Training-based 方法和数据收集的压缩方法却受到了高成本和数据收集的挑战。一shot 压缩方法，尽管成本低廉且不需数据，但在结构化压缩设定下会导致性能下降。在这种情况下，我们提出了一种新的 LL 模型结构压缩方法，called Compresso。我们的方法通过与 LL 模型的协作，在训练过程中学习最佳压缩决策。Compresso 通过综合利用 LoRA 技术和 $L_0$ 正则化来解决训练成本和数据收集的挑战。此外，我们还在压缩算法中引入了协同提示，使 LL 模型和压缩算法之间的合作更加紧密，从而提高总性能。因此，Compresso 可以压缩 LLaMA-7B 到 5.4B，保持原有性能，甚至在阅读理解任务上超越 LLaMA-7B 的表现。我们的实验表明，Compresso significantly 高于一shot 压缩基准在不同的稀疏比例上，达到了2.21%、11.43%、7.04%和4.81%的提升。
</details></li>
</ul>
<hr>
<h2 id="The-Reinforce-Policy-Gradient-Algorithm-Revisited"><a href="#The-Reinforce-Policy-Gradient-Algorithm-Revisited" class="headerlink" title="The Reinforce Policy Gradient Algorithm Revisited"></a>The Reinforce Policy Gradient Algorithm Revisited</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05000">http://arxiv.org/abs/2310.05000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shalabh Bhatnagar</li>
<li>for: 本文提出了一种改进版本的强化策略梯度算法，用于处理无穷状态和动作空间的系统。</li>
<li>methods: 本文使用一种随机搜索方法来估计策略梯度，而不需要Regularity conditions。</li>
<li>results: 本文证明了这种新算法的收敛性，并且在无穷状态和动作空间中的系统上实现了高效的收敛性。In English:</li>
<li>for: The paper proposes an improved version of the reinforcement policy gradient algorithm for systems with infinite state and action spaces.</li>
<li>methods: The paper uses a random search method to estimate the policy gradient, without requiring regularity conditions.</li>
<li>results: The paper proves the convergence of the new algorithm and demonstrates its effectiveness in systems with infinite state and action spaces.<details>
<summary>Abstract</summary>
We revisit the Reinforce policy gradient algorithm from the literature. Note that this algorithm typically works with cost returns obtained over random length episodes obtained from either termination upon reaching a goal state (as with episodic tasks) or from instants of visit to a prescribed recurrent state (in the case of continuing tasks). We propose a major enhancement to the basic algorithm. We estimate the policy gradient using a function measurement over a perturbed parameter by appealing to a class of random search approaches. This has advantages in the case of systems with infinite state and action spaces as it relax some of the regularity requirements that would otherwise be needed for proving convergence of the Reinforce algorithm. Nonetheless, we observe that even though we estimate the gradient of the performance objective using the performance objective itself (and not via the sample gradient), the algorithm converges to a neighborhood of a local minimum. We also provide a proof of convergence for this new algorithm.
</details>
<details>
<summary>摘要</summary>
我们回顾到文献中的增强策略梯度算法。该算法通常与成本返回得到随机长度集所得到的集成任务（如果结束）或从定义状态访问中抽出的循环状态（在续行任务中）。我们提出了一个主要优化，使用功能测量在干扰参数上估计策略梯度，通过一类随机搜寻方法。这具有利陵系统拥有无限州和动作空间的情况下，可以缓和一些常量需求，从而让增强算法的证明 converges。然而，我们观察到，即使使用表现目标自身估计策略梯度（而不是采样梯度），算法仍会趋向一个地方最小值的邻近。我们也提供了该新算法的充分性证明。
</details></li>
</ul>
<hr>
<h2 id="Distantly-Supervised-Joint-Entity-and-Relation-Extraction-with-Noise-Robust-Learning"><a href="#Distantly-Supervised-Joint-Entity-and-Relation-Extraction-with-Noise-Robust-Learning" class="headerlink" title="Distantly-Supervised Joint Entity and Relation Extraction with Noise-Robust Learning"></a>Distantly-Supervised Joint Entity and Relation Extraction with Noise-Robust Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04994">http://arxiv.org/abs/2310.04994</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yul091/denrl">https://github.com/yul091/denrl</a></li>
<li>paper_authors: Yufei Li, Xiao Yu, Yanghong Guo, Yanchi Liu, Haifeng Chen, Cong Liu</li>
<li>for: 这个论文主要用于解决使用远程标注数据进行entity和关系抽象的问题，即使面临着噪声标注的问题。</li>
<li>methods: 该论文提出了一种新的噪声鲁棒方法，包括在序列标注模型中预训练GPT-2，以及使用一种新的噪声鲁棒学习框架，包括一个新的损失函数，惩罚与重要关系模式和实体关系依赖性不一致。</li>
<li>results: 实验结果显示，该方法可以在两个数据集上达到现有状态的 arts 方法的同等或更高的 JOINT 抽象性和噪声减少效果。<details>
<summary>Abstract</summary>
Joint entity and relation extraction is a process that identifies entity pairs and their relations using a single model. We focus on the problem of training these models on distantly-labeled data, which is generated by aligning entity mentions in a text corpus with their corresponding entity and relation types in a knowledge base. One key challenge here is the presence of noisy labels, which arises from both entity and relation annotations, and significantly impair the effectiveness of supervised learning applications. However, existing research primarily addresses only one type of noise, thereby limiting the effectiveness of noise reduction. To fill this gap, we introduce a new noise-robust approach, that 1)~incorporates a pre-trained GPT-2 into a sequence tagging scheme for simultaneous entity and relation detection, and 2)~employs a noise-robust learning framework which includes a new loss function that penalizes inconsistency with both significant relation patterns and entity-relation dependencies, as well as a self-adaptive learning step that iteratively selects and trains on high-quality instances. Experiments on two datasets show that our method outperforms the existing state-of-the-art methods in both joint extraction performance and noise reduction effect.
</details>
<details>
<summary>摘要</summary>
共同实体和关系抽取是一个过程，它通过单一模型标识实体对和其关系。我们关注在训练这些模型的远程标注数据上的问题，这些数据是通过文本库中的实体提及与知识库中的实体和关系类型的对应进行对齐的。一个关键挑战是噪声标注，它来自实体和关系注释，并对监督学习应用产生重要影响。然而，现有研究主要只处理一种噪声，因此限制了噪声减少的效iveness。为了填补这个空白，我们介绍了一种新的噪声Robust Approach，它包括以下两个部分：1. 使用预训练的 GPT-2 在序列标记方案中同时检测实体和关系，以提高实体和关系的同时检测能力。2. 使用一种噪声Robust的学习框架，包括一种新的损失函数，该损失函数考虑实体和关系之间的依赖关系和重要关系模式，以及一种自适应学习步骤，该步骤在高质量实例上进行逐步选择和训练。我们在两个数据集上进行了实验，结果表明，我们的方法在同时检测性能和噪声减少效果方面都超过了现有状态的方法。
</details></li>
</ul>
<hr>
<h2 id="The-Troubling-Emergence-of-Hallucination-in-Large-Language-Models-–-An-Extensive-Definition-Quantification-and-Prescriptive-Remediations"><a href="#The-Troubling-Emergence-of-Hallucination-in-Large-Language-Models-–-An-Extensive-Definition-Quantification-and-Prescriptive-Remediations" class="headerlink" title="The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations"></a>The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04988">http://arxiv.org/abs/2310.04988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das</li>
<li>for: 本研究旨在提供一种细化的幻觉分类方法，以及对幻觉的减轻策略。</li>
<li>methods: 本研究使用了15种当代大语言模型生成75,000个样本，并对其进行了人工标注。此外，本研究还提出了一个幻觉敏感指数（HVI），用于评估和排序不同的大语言模型在生成幻觉方面的敏感度。</li>
<li>results: 本研究对幻觉进行了细化分类，并提出了两种减轻幻觉的方法。<details>
<summary>Abstract</summary>
The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.
</details>
<details>
<summary>摘要</summary>
最近的大自然语言模型（LLM）的进步得到了广泛的赞誉，但同时也出现了一种问题，即幻觉。幻觉的出现引起了一定的担忧，因为它可能会对语言处理 tasks 产生负面影响。然而，关于幻觉的细分分类和相应的缓解方法尚未得到了足够的重视。为了解决这个问题，我们提出了一种细化的幻觉分类方法，以及一些缓解方法。我们将幻觉分为两类：（i）实际幻觉（FM）和（ii）银色幻觉（SL）。这两类幻觉进一步分为内在和外在两类，并且分为三级幻觉的严重程度：（i）轻度、（ii）中度和（iii）警示级。此外，我们还将幻觉分为六种类型：（i）字符混淆、（ii）数字幻觉、（iii）生成的 GOLEM、（iv）虚拟之声、（v）地理错误和（vi）时间包袋。此外，我们还创建了一个名为 HallucInation eLiciTation（HILT）的公共可用数据集，包含75000个样本，由15种当代LLM生成，以及人工标注的相应类别。最后，我们提出了一种用于评估和排名 LLM 的幻觉抵触指数（HVI）。我们认为 HVI 对 NLP 社区拥有广泛的价值，并可能被用作 AI 相关的政策制定的工具。为了缓解幻觉，我们提出了两种解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-new-economic-and-financial-theory-of-money"><a href="#A-new-economic-and-financial-theory-of-money" class="headerlink" title="A new economic and financial theory of money"></a>A new economic and financial theory of money</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04986">http://arxiv.org/abs/2310.04986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael E. Glinsky, Sharon Sievert</li>
<li>for: This paper aims to reformulate economic and financial theory to include electronic currencies, and to develop a new view of electronic currency as a transactional equity associated with tangible assets.</li>
<li>methods: The paper uses macroeconomic theory and the fundamental equation of monetary policy to value electronic currencies, and employs multi time scale models to capture true risk. The decision-making process is approached using deep reinforcement learning, generative pretrained transformers, and other methods of artificial intelligence.</li>
<li>results: The paper develops a new view of electronic currency management firms as entities responsible for coordinated monetary and fiscal policies of a substantial sub-economy, and proposes a system response function and DRL&#x2F;GPT&#x2F;AI-based active nonlinear control to stabilize unstable equilibriums in the sub-economy.<details>
<summary>Abstract</summary>
This paper fundamentally reformulates economic and financial theory to include electronic currencies. The valuation of the electronic currencies will be based on macroeconomic theory and the fundamental equation of monetary policy, not the microeconomic theory of discounted cash flows. The view of electronic currency as a transactional equity associated with tangible assets of a sub-economy will be developed, in contrast to the view of stock as an equity associated mostly with intangible assets of a sub-economy. The view will be developed of the electronic currency management firm as an entity responsible for coordinated monetary (electronic currency supply and value stabilization) and fiscal (investment and operational) policies of a substantial (for liquidity of the electronic currency) sub-economy. The risk model used in the valuations and the decision-making will not be the ubiquitous, yet inappropriate, exponential risk model that leads to discount rates, but will be multi time scale models that capture the true risk. The decision-making will be approached from the perspective of true systems control based on a system response function given by the multi scale risk model and system controllers that utilize the Deep Reinforcement Learning, Generative Pretrained Transformers, and other methods of Artificial Intelligence (DRL/GPT/AI). Finally, the sub-economy will be viewed as a nonlinear complex physical system with both stable equilibriums that are associated with short-term exploitation, and unstable equilibriums that need to be stabilized with active nonlinear control based on the multi scale system response functions and DRL/GPT/AI.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparative-Analysis-of-Transfer-Learning-in-Deep-Learning-Text-to-Speech-Models-on-a-Few-Shot-Low-Resource-Customized-Dataset"><a href="#Comparative-Analysis-of-Transfer-Learning-in-Deep-Learning-Text-to-Speech-Models-on-a-Few-Shot-Low-Resource-Customized-Dataset" class="headerlink" title="Comparative Analysis of Transfer Learning in Deep Learning Text-to-Speech Models on a Few-Shot, Low-Resource, Customized Dataset"></a>Comparative Analysis of Transfer Learning in Deep Learning Text-to-Speech Models on a Few-Shot, Low-Resource, Customized Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04982">http://arxiv.org/abs/2310.04982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Liu</li>
<li>for: 本研究旨在提高 Text-to-Speech（TTS）synthesis 的质量，使用深度学习，但现代 TTS 模型需要大量数据。因此，本研究强调使用传输学习，特别是几何学习、少量数据和自定义数据集。</li>
<li>methods: 本研究使用了现代 TTS 模型的传输学习能力，进行了系统技术分析，并对几何学习模型进行了实验分析。</li>
<li>results: 研究发现，传输学习可以大幅提高 TTS 模型在紧张数据集上的表现，并且可以找到适合特定数据集的优化模型。这种模型可以在数据稀缺时提供高质量的语音输出。<details>
<summary>Abstract</summary>
Text-to-Speech (TTS) synthesis using deep learning relies on voice quality. Modern TTS models are advanced, but they need large amount of data. Given the growing computational complexity of these models and the scarcity of large, high-quality datasets, this research focuses on transfer learning, especially on few-shot, low-resource, and customized datasets. In this research, "low-resource" specifically refers to situations where there are limited amounts of training data, such as a small number of audio recordings and corresponding transcriptions for a particular language or dialect. This thesis, is rooted in the pressing need to find TTS models that require less training time, fewer data samples, yet yield high-quality voice output. The research evaluates TTS state-of-the-art model transfer learning capabilities through a thorough technical analysis. It then conducts a hands-on experimental analysis to compare models' performance in a constrained dataset. This study investigates the efficacy of modern TTS systems with transfer learning on specialized datasets and a model that balances training efficiency and synthesis quality. Initial hypotheses suggest that transfer learning could significantly improve TTS models' performance on compact datasets, and an optimal model may exist for such unique conditions. This thesis predicts a rise in transfer learning in TTS as data scarcity increases. In the future, custom TTS applications will favour models optimized for specific datasets over generic, data-intensive ones.
</details>
<details>
<summary>摘要</summary>
TEXT-TO-SPEECH（TTS）合成使用深度学习取决于声音质量。现代TTS模型非常先进，但它们需要大量数据。随着这些模型的计算复杂度的增加和数据的罕见性，这些研究将注重传输学习，特别是几个数据集的传输学习。在这种情况下，“低资源”指的是有限的训练数据，例如一小number of audio recording和相应的转录 для一种语言或方言。这个研究是根据找到需要 fewer training time和数据amples的TTS模型的强需求。研究通过深入技术分析评估TTS模型的传输学习能力，然后通过实验分析比较模型在紧张数据集中的表现。这个研究investigatesmodern TTS系统在特殊数据集上的传输学习能力，并预测未来custom TTS应用程序将偏好特定数据集上优化的模型。Note: Simplified Chinese is used here as the translation target, as it is more widely used in mainland China and is the standard form of Chinese used in many online applications. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="MULTISCRIPT-Multimodal-Script-Learning-for-Supporting-Open-Domain-Everyday-Tasks"><a href="#MULTISCRIPT-Multimodal-Script-Learning-for-Supporting-Open-Domain-Everyday-Tasks" class="headerlink" title="MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks"></a>MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04965">http://arxiv.org/abs/2310.04965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyuan Qi, Minqian Liu, Ying Shen, Zhiyang Xu, Lifu Huang</li>
<li>for: 提高AI虚拟助手完成日常任务的自动生成脚本能力，特别是对于不熟悉的任务。</li>
<li>methods: 基于多模态视频和文本描述，提出了两个新任务：多模态脚本生成和后续步骤预测。两个任务的输入都是目标任务名和一段完成目标任务的视频示例，输出包括（1）基于视频示例的结构化文本描述，和（2）基于视频示例的后续步骤文本描述。</li>
<li>results: 提出了两种基于大语言模型知识的多模态生成框架，并在MultiScript挑战 задании上实现了显著提高。<details>
<summary>Abstract</summary>
Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning rely heavily on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge -- MultiScript, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for the subsequent step, respectively. Built from WikiHow, MultiScript covers multimodal scripts in videos and text descriptions for over 6,655 human everyday tasks across 19 diverse domains. To establish baseline performance on MultiScript, we propose two knowledge-guided multimodal generative frameworks that incorporate the task-related knowledge prompted from large language models such as Vicuna. Experimental results show that our proposed approaches significantly improve over the competitive baselines.
</details>
<details>
<summary>摘要</summary>
现代AI虚拟助手需要自动生成脚本（即文本描述的顺序步骤）从视频示例中，并根据示例视频进行逻辑推理来导引人类完成日常任务，特别是不熟悉的任务。然而，现有的生成脚本学习方法都是基于结构化的前置步骤（文本和/或图像），或者只能在特定领域中进行学习，这导致了与实际用户场景的差距。为了解决这些限制，我们提出了一个新的比赛挑战——MultiScript，包括两个新任务：（1）多媒体脚本生成和（2）后续步骤预测。对于两个任务，输入都是目标任务名和一段完成目标任务的视频示例，并且期望的输出是（1）基于示例视频的结构化文本描述，和（2）一个基于示例视频的文本描述。MultiScript由WikiHow建立，覆盖了视频和文本描述的多媒体脚本 для人类日常任务的19个不同领域，涵盖了6,655个任务。为了确定MultiScript的基准性能，我们提议两种基于大型自然语言模型（如Vicuna）的知识导向多媒体生成框架，实验结果表明，我们的提议方法具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="LLM4VV-Developing-LLM-Driven-Testsuite-for-Compiler-Validation"><a href="#LLM4VV-Developing-LLM-Driven-Testsuite-for-Compiler-Validation" class="headerlink" title="LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation"></a>LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04963">http://arxiv.org/abs/2310.04963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Munley, Aaron Jarmusch, Sunita Chandrasekaran</li>
<li>for: This paper explores the capability of state-of-the-art large language models (LLMs) to automatically generate tests and validate compiler implementations of a directive-based programming paradigm, OpenACC.</li>
<li>methods: The paper employs various prompt engineering techniques, including code templates, retrieval-augmented generation (RAG) with code templates, expressive prompts using RAG with code templates, one-shot examples, and RAG with one-shot examples.</li>
<li>results: The paper investigates the outcome of LLMs-generated tests and analyzes the capabilities of the latest LLMs for code generation.<details>
<summary>Abstract</summary>
Large language models (LLMs) are a new and powerful tool for a wide span of applications involving natural language and demonstrate impressive code generation abilities. In this paper, we explore the capabilitity of state-of-the-art LLMs, including closed-source options like OpenAI GPT-4 and open-source alternatives like Meta AI Codellama, to automatically generate tests and use these tests to validate and verify compiler implementations of a directive-based programming paradigm, OpenACC. Our approach entails exploring various prompt engineering techniques including a code template, retrieval-augmented generation (RAG) with code template, expressive prompt using RAG with code template, one-shot example, and RAG with one-shot example. This paper focusses on (a) exploring the capabilities of the latest LLMs for code generation, (b) investigating prompt and fine tuning methods, and (c) analyzing the outcome of LLMs generated tests
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）是一种新的和强大的工具，可以应用于许多自然语言相关的应用程序。在这篇论文中，我们探讨了当前领先的LLM，包括OpenAI GPT-4和Meta AI Codellama等closed-source选择，以及open-source的选择，用于自动生成测试，并使用这些测试来验证和验证编译器实现的指令式编程方法OpenACC。我们的方法包括使用代码模板、代码检索增强生成（RAG）、表达式提示、一shot示例和RAG与一shot示例等多种提示工程技术。本文主要关注以下三点：1. 探讨最新的LLM代码生成能力2. 探讨提示和精度调整方法3. 分析LLM生成的测试结果
</details></li>
</ul>
<hr>
<h2 id="Safe-Deep-Policy-Adaptation"><a href="#Safe-Deep-Policy-Adaptation" class="headerlink" title="Safe Deep Policy Adaptation"></a>Safe Deep Policy Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08602">http://arxiv.org/abs/2310.08602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenli Xiao, Tairan He, John Dolan, Guanya Shi</li>
<li>for: 本研究旨在开发一种能够快速适应动态不确定环境的自主 робоット控制框架，同时保证安全性和稳定性。</li>
<li>methods: 本研究使用了policy adaptation基于再归折衔学习（RL），并提出了一种安全防止（Safety Filter）来保证实际世界中的安全性。</li>
<li>results: 实验结果显示，SafeDPA在三个不同的环境中（倒挠杆、Safety Gym和RC Car）具有出色的安全性和任务性能，与现有的基准值进行比较，SafeDPA在不可见干扰的实际世界中展现出了300%的安全率提升。<details>
<summary>Abstract</summary>
A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors and extra perturbations. Comprehensive experiments on (1) classic control problems (Inverted Pendulum), (2) simulation benchmarks (Safety Gym), and (3) a real-world agile robotics platform (RC Car) demonstrate great superiority of SafeDPA in both safety and task performance, over state-of-the-art baselines. Particularly, SafeDPA demonstrates notable generalizability, achieving a 300% increase in safety rate compared to the baselines, under unseen disturbances in real-world experiments.
</details>
<details>
<summary>摘要</summary>
“一个重要目标是实现自主机器人快速适应动态和不确定环境。类型的适应控制和安全控制可以提供稳定性和安全保证，但是仅对特定系统类型有效。相比之下，基于征得学习（RL）的政策适应则提供了多样性和普遍性，但是产生了安全和可靠性挑战。我们提出了SafeDPA，一个新的RL和控制框架，同时解决政策适应和安全征得学习的问题。SafeDPA在实验中同时学习适应政策和动力学模型，预测环境配置，并将几何数据进行精确化。我们引入了基于控制障碍函数（CBF）的安全筛选器，以保证在真实世界中的安全运行。我们提供了理论上的安全保证，并证明SafeDPA对学习错误和额外干扰具有Robustness。实验结果显示，SafeDPA在三个不同的应用中具有优秀的安全性和任务性能，比基准设定更高。特别是，SafeDPA在未见到的干扰下 demonstrate了特别的多样性，在真实世界中获得300%的安全率提升。”
</details></li>
</ul>
<hr>
<h2 id="CodeTransOcean-A-Comprehensive-Multilingual-Benchmark-for-Code-Translation"><a href="#CodeTransOcean-A-Comprehensive-Multilingual-Benchmark-for-Code-Translation" class="headerlink" title="CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation"></a>CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04951">http://arxiv.org/abs/2310.04951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weixiangyan/codetransocean">https://github.com/weixiangyan/codetransocean</a></li>
<li>paper_authors: Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, Wen Wang</li>
<li>for: 这个研究旨在提高代码翻译的质量和维护效率，并满足实际应用中的多元化需求。</li>
<li>methods: 这个研究使用了人工神经网络翻译模型，探索了多种程式语言之间的翻译，包括具有多种程式语言的复杂混合翻译。</li>
<li>results: 研究发现，这些多种程式语言翻译方法可以提高低资源语言的翻译质量和高资源语言的培训效率。此外，研究还提出了一个新的评估指标Debugging Success Rate@K，用于评估翻译后的程式码可行性。<details>
<summary>Abstract</summary>
Recent code translation techniques exploit neural machine translation models to translate source code from one programming language to another to satisfy production compatibility or to improve efficiency of codebase maintenance. Most existing code translation datasets only focus on a single pair of popular programming languages. To advance research on code translation and meet diverse requirements of real-world applications, we construct CodeTransOcean, a large-scale comprehensive benchmark that supports the largest variety of programming languages for code translation. CodeTransOcean consists of three novel multilingual datasets, namely, MultilingualTrans supporting translations between multiple popular programming languages, NicheTrans for translating between niche programming languages and popular ones, and LLMTrans for evaluating executability of translated code by large language models (LLMs). CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for translating deep learning code across different frameworks. We develop multilingual modeling approaches for code translation and demonstrate their great potential in improving the translation quality of both low-resource and high-resource language pairs and boosting the training efficiency. We also propose a novel evaluation metric Debugging Success Rate@K for program-level code translation. Last but not least, we evaluate LLM ChatGPT on our datasets and investigate its potential for fuzzy execution predictions. We build baselines for CodeTransOcean and analyze challenges of code translation for guiding future research. The CodeTransOcean datasets and code are publicly available at https://github.com/WeixiangYAN/CodeTransOcean.
</details>
<details>
<summary>摘要</summary>
现代代码翻译技术利用神经机器翻译模型将源代码从一种编程语言翻译到另一种编程语言，以满足生产兼容性或改善代码维护效率。现有大多数代码翻译数据集只关注单个受欢迎的编程语言对。为了推动代码翻译研究和满足实际应用的多样化需求，我们构建了CodeTransOcean，一个大规模、完整的benchmark，支持最多的编程语言对进行代码翻译。CodeTransOcean包括三个新的多语言数据集：MultilingualTrans、NicheTrans和LLMTrans。MultilingualTrans支持多种受欢迎编程语言之间的翻译，NicheTrans用于将特殊编程语言与受欢迎语言之间翻译，LLMTrans用于通过大语言模型（LLMs）评估翻译后代码的执行可能性。CodeTransOcean还包括一个跨框架数据集DLTrans，用于跨不同框架深度学习代码翻译。我们开发了多语言模型方法，并证明它们在低资源语言对和高资源语言对翻译质量提高和训练效率提高。我们还提出了一个新的评价指标Debugging Success Rate@K，用于评估翻译后代码的可调试性。最后，我们评估了LLM ChatGPT在我们的数据集上的性能，并调查其可能性于软件执行预测。我们建立了CodeTransOcean的基准，并分析了代码翻译的挑战，以帮助未来研究。CodeTransOcean数据集和代码可以在https://github.com/WeixiangYAN/CodeTransOcean上下载。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/cs.AI_2023_10_08/" data-id="clombedoy0059s088c2j7c6xx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/cs.CL_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T11:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/cs.CL_2023_10_08/">cs.CL - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Visual-Storytelling-with-Question-Answer-Plans"><a href="#Visual-Storytelling-with-Question-Answer-Plans" class="headerlink" title="Visual Storytelling with Question-Answer Plans"></a>Visual Storytelling with Question-Answer Plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05295">http://arxiv.org/abs/2310.05295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danyang Liu, Mirella Lapata, Frank Keller</li>
<li>for: 本研究旨在生成吸引人的故事，从图像序列中提取有趣的视觉表达。</li>
<li>methods: 该模型将图像序列转化为可进行语言模型解释的视觉预фикс，并使用问题对话对话来选择关键的视觉概念并决定如何将它们组织成一个故事。</li>
<li>results: 自动和人工评估结果表明，蓝图基本模型可以生成更加有趣、有логи、自然的故事，比baseline和现有系统更高效。<details>
<summary>Abstract</summary>
Visual storytelling aims to generate compelling narratives from image sequences. Existing models often focus on enhancing the representation of the image sequence, e.g., with external knowledge sources or advanced graph structures. Despite recent progress, the stories are often repetitive, illogical, and lacking in detail. To mitigate these issues, we present a novel framework which integrates visual representations with pretrained language models and planning. Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative. Automatic and human evaluation on the VIST benchmark (Huang et al., 2016) demonstrates that blueprint-based models generate stories that are more coherent, interesting, and natural compared to competitive baselines and state-of-the-art systems.
</details>
<details>
<summary>摘要</summary>
Visual storytelling 目标是从图像序列中生成吸引人的故事。现有的模型通常会将注意力集中在图像序列的表现方面，例如通过与外部知识源或高级graph structures整合。despite recent progress, stories are often repetitive, illogical, and lacking in detail. To address these issues, we propose a novel framework that integrates visual representations with pre-trained language models and planning. Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings that language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative. Automatic and human evaluation on the VIST benchmark (Huang et al., 2016) shows that blueprint-based models generate stories that are more coherent, interesting, and natural compared to competitive baselines and state-of-the-art systems.
</details></li>
</ul>
<hr>
<h2 id="Hi-Guys-or-Hi-Folks-Benchmarking-Gender-Neutral-Machine-Translation-with-the-GeNTE-Corpus"><a href="#Hi-Guys-or-Hi-Folks-Benchmarking-Gender-Neutral-Machine-Translation-with-the-GeNTE-Corpus" class="headerlink" title="Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus"></a>Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05294">http://arxiv.org/abs/2310.05294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlt-mt/fbk-neutr-eval">https://github.com/hlt-mt/fbk-neutr-eval</a></li>
<li>paper_authors: Andrea Piergentili, Beatrice Savoldi, Dennis Fucci, Matteo Negri, Luisa Bentivogli</li>
<li>for:  Addressing the lack of inclusive language in machine translation, particularly in grammatical gender languages.</li>
<li>methods:  Proposing a dedicated benchmark and exploring automated evaluation methods for gender-neutral translation from English to Italian, including a natural, bilingual test set (GeNTE) and a reference-free evaluation approach.</li>
<li>results:  A new, more inclusive approach to machine translation that challenges traditional binary gender assumptions and provides a more accurate assessment of gender-neutral translation.<details>
<summary>Abstract</summary>
Gender inequality is embedded in our communication practices and perpetuated in translation technologies. This becomes particularly apparent when translating into grammatical gender languages, where machine translation (MT) often defaults to masculine and stereotypical representations by making undue binary gender assumptions. Our work addresses the rising demand for inclusive language by focusing head-on on gender-neutral translation from English to Italian. We start from the essentials: proposing a dedicated benchmark and exploring automated evaluation methods. First, we introduce GeNTE, a natural, bilingual test set for gender-neutral translation, whose creation was informed by a survey on the perception and use of neutral language. Based on GeNTE, we then overview existing reference-based evaluation approaches, highlight their limits, and propose a reference-free method more suitable to assess gender-neutral translation.
</details>
<details>
<summary>摘要</summary>
gender inequality 在我们的沟通习惯中存在并在翻译技术中被延续。这种情况特别在翻译到 grammatical gender 语言时变得明显，MT 常 defaults to  masculine 和标准化的表达，从而做出了不当的男性假设。我们的工作解决了包容性语言的增长需求，专注于从英语到意大利语的gender-neutral 翻译。我们从基础开始：提议一个专门的标准和探索自动评估方法。首先，我们介绍了 GeNTE，一个自然、双语测试集 для gender-neutral 翻译，其创建受到了对中性语言的感知和使用的调查。然后，我们概述了现有的参照基础评估方法， highlight  их的局限性，并提出了不需要参照的方法，更适合评估 gender-neutral 翻译。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Pre-Trained-Language-Models-with-Sentence-Position-Embeddings-for-Rhetorical-Roles-Recognition-in-Legal-Opinions"><a href="#Enhancing-Pre-Trained-Language-Models-with-Sentence-Position-Embeddings-for-Rhetorical-Roles-Recognition-in-Legal-Opinions" class="headerlink" title="Enhancing Pre-Trained Language Models with Sentence Position Embeddings for Rhetorical Roles Recognition in Legal Opinions"></a>Enhancing Pre-Trained Language Models with Sentence Position Embeddings for Rhetorical Roles Recognition in Legal Opinions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05276">http://arxiv.org/abs/2310.05276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anas Belfathi, Nicolas Hernandez, Laura Monceaux</li>
<li>For: 这个研究论文是为了提出一种基于预训练语言模型（PLM）和句子位置信息的新型自动预测辩论角色的模型建模方法。* Methods: 该方法使用了一个简单的模型结构，并使用了LegalEval@SemEval2023 competition上的注释 corpora进行训练。在这个 corpus 中，它们使用了一些特定的预处理技术来提高模型的性能。* Results: 研究人员发现，他们的方法比使用复杂的层次模型在全局上的方法更加简单，具有更低的计算成本。此外，他们还发现，通过在本地上增加更多的注意力，以及将句子位置信息纳入模型中，可以进一步提高结果。<details>
<summary>Abstract</summary>
The legal domain is a vast and complex field that involves a considerable amount of text analysis, including laws, legal arguments, and legal opinions. Legal practitioners must analyze these texts to understand legal cases, research legal precedents, and prepare legal documents. The size of legal opinions continues to grow, making it increasingly challenging to develop a model that can accurately predict the rhetorical roles of legal opinions given their complexity and diversity. In this research paper, we propose a novel model architecture for automatically predicting rhetorical roles using pre-trained language models (PLMs) enhanced with knowledge of sentence position information within a document. Based on an annotated corpus from the LegalEval@SemEval2023 competition, we demonstrate that our approach requires fewer parameters, resulting in lower computational costs when compared to complex architectures employing a hierarchical model in a global-context, yet it achieves great performance. Moreover, we show that adding more attention to a hierarchical model based only on BERT in the local-context, along with incorporating sentence position information, enhances the results.
</details>
<details>
<summary>摘要</summary>
法律领域是一个庞大复杂的领域，涉及到大量的文本分析，包括法律、法律论据和法律意见。法律实践者需要分析这些文本，以理解法律案例，研究法律先例，并准备法律文书。随着法律意见的大小不断增加，以至于开发一个可以准确预测法律意见的模型变得愈加挑战。在这篇研究论文中，我们提出一种新的模型建立方法，使用预训练语言模型（PLMs），并在文本中添加句子位置信息，以自动预测法律意见的文化角色。基于LegalEval@SemEval2023比赛获得的标注词汇集，我们示示了我们的方法需要 fewer parameters，相比较复杂的结构，计算成本更低，同时可以达到高效的表现。此外，我们还证明了在地方上添加更多注意力，以及基于BERT的层次模型，可以提高结果。
</details></li>
</ul>
<hr>
<h2 id="XLS-R-fine-tuning-on-noisy-word-boundaries-for-unsupervised-speech-segmentation-into-words"><a href="#XLS-R-fine-tuning-on-noisy-word-boundaries-for-unsupervised-speech-segmentation-into-words" class="headerlink" title="XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words"></a>XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05235">http://arxiv.org/abs/2310.05235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Algayres, Pablo Diego-Simon, Benoit Sagot, Emmanuel Dupoux<br>for: 这个论文的目的是提高无文本支持的语音分割任务的性能。methods: 这个论文使用了最新的自我超vised speech模型，通过精度调整来快速适应新任务，即使在资源匮乏的情况下。它们引入了 semi-supervised learning的想法，使用 XLS-R 模型预测语音分割系统生成的字Boundary。results: 这个论文的方法可以一直提高每种系统的性能，并在五种语言 corpora 上设置了新的状态态�idents，平均提高了130%的 F1 分数。此外，这个系统还可以在无seen语言中进行零shot分割。<details>
<summary>Abstract</summary>
Due to the absence of explicit word boundaries in the speech stream, the task of segmenting spoken sentences into word units without text supervision is particularly challenging. In this work, we leverage the most recent self-supervised speech models that have proved to quickly adapt to new tasks through fine-tuning, even in low resource conditions. Taking inspiration from semi-supervised learning, we fine-tune an XLS-R model to predict word boundaries themselves produced by top-tier speech segmentation systems: DPDP, VG-HuBERT, GradSeg and DP-Parse. Once XLS-R is fine-tuned, it is used to infer new word boundary labels that are used in turn for another fine-tuning step. Our method consistently improves the performance of each system and sets a new state-of-the-art that is, on average 130% higher than the previous one as measured by the F1 score on correctly discovered word tokens on five corpora featuring different languages. Finally, our system can segment speech from languages unseen during fine-tuning in a zero-shot fashion.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Spoken-Language-Model-based-on-continuous-word-sized-audio-tokens"><a href="#Generative-Spoken-Language-Model-based-on-continuous-word-sized-audio-tokens" class="headerlink" title="Generative Spoken Language Model based on continuous word-sized audio tokens"></a>Generative Spoken Language Model based on continuous word-sized audio tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05224">http://arxiv.org/abs/2310.05224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Algayres, Yossi Adi, Tu Anh Nguyen, Jade Copet, Gabriel Synnaeve, Benoit Sagot, Emmanuel Dupoux</li>
<li>for: 该论文旨在提出一种基于word-size连续值音频嵌入的生成语言模型（GSLM），以便生成多样化和表达力强的语言输出。</li>
<li>methods: 该模型使用了 Lexical Embedding 函数取代 lookup 表格，权重损失函数被替换为对比损失函数，以及多omial 采样被替换为 k-NN 采样。</li>
<li>results: 该模型的表现与基于分组单元 GSLMs 相当，自动度量器和人工评价都表示生成质量高，并且具有五倍的内存效率优势。此外，模型中的嵌入before和after Lexical Embedder 具有phonetics和semantics的可读性。<details>
<summary>Abstract</summary>
In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)在 NLP 中，文本语言模型 based on words or subwords 知道会比其字符基本的对手表现更好。然而，在语音社区中，标准输入的语音LMs 是20ms或40ms短于一个音素的分 discrete units。 drawing inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.
</details></li>
</ul>
<hr>
<h2 id="Probing-Language-Models-from-A-Human-Behavioral-Perspective"><a href="#Probing-Language-Models-from-A-Human-Behavioral-Perspective" class="headerlink" title="Probing Language Models from A Human Behavioral Perspective"></a>Probing Language Models from A Human Behavioral Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05216">http://arxiv.org/abs/2310.05216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xintong Wang, Xiaoyu Li, Xingshan Li, Chris Biemann</li>
<li>for: This paper aims to provide a better understanding of how large language models (LLMs) work and how they make predictions.</li>
<li>methods: The authors use eye-tracking measures to correlate with the values produced by LLMs and compare them to those of recurrent neural network-based language models (RNN-LMs). They also analyze the functions of self-attention and gate mechanisms in LLMs.</li>
<li>results: The study finds that LLMs exhibit a distinct prediction pattern compared to RNN-LMs, with a peak in memorization and linguistic knowledge encoding as the number of feed-forward network (FFN) layers increases, followed by a pivot to comprehension capacity. The self-attention mechanisms are found to be distributed across multiple heads, and the gate mechanisms control the flow of information, with some gates promoting and others eliminating information.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as dominant foundational models in modern NLP. However, the understanding of their prediction process and internal mechanisms, such as feed-forward networks and multi-head self-attention, remains largely unexplored. In this study, we probe LLMs from a human behavioral perspective, correlating values from LLMs with eye-tracking measures, which are widely recognized as meaningful indicators of reading patterns. Our findings reveal that LLMs exhibit a prediction pattern distinct from that of RNN-based LMs. Moreover, with the escalation of FFN layers, the capacity for memorization and linguistic knowledge encoding also surges until it peaks, subsequently pivoting to focus on comprehension capacity. The functions of self-attention are distributed across multiple heads. Lastly, we scrutinize the gate mechanisms, finding that they control the flow of information, with some gates promoting, while others eliminating information.
</details>
<details>
<summary>摘要</summary>
Note:* "Large Language Models" (LLMs) 是现代 NLP 中最具代表性的基础模型，但它们的预测过程和内部机制仍然尚未得到充分的研究。* "feed-forward networks" (FFNs) 是 LLMs 的一种基本结构，它们在预测过程中发挥着重要的作用。* "multi-head self-attention" 是 LLMs 中的一种自注意机制，它可以帮助模型更好地理解语言结构和含义。* "eye-tracking measures" 是一种广泛用于研究人类阅读习惯的方法，它可以反映人们在阅读过程中的注意力和理解程度。* "gate mechanisms" 是 LLMs 中的一种控制信息流动的机制，它可以帮助模型更好地过滤不必要的信息并保留有用信息。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Voice-Conversion-Models-with-Large-Scale-Speech-and-Singing-Data-The-T13-Systems-for-the-Singing-Voice-Conversion-Challenge-2023"><a href="#A-Comparative-Study-of-Voice-Conversion-Models-with-Large-Scale-Speech-and-Singing-Data-The-T13-Systems-for-the-Singing-Voice-Conversion-Challenge-2023" class="headerlink" title="A Comparative Study of Voice Conversion Models with Large-Scale Speech and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge 2023"></a>A Comparative Study of Voice Conversion Models with Large-Scale Speech and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05203">http://arxiv.org/abs/2310.05203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryuichi Yamamoto, Reo Yoneyama, Lester Phillip Violeta, Wen-Chin Huang, Tomoki Toda</li>
<li>for: 这个论文targets the singing voice conversion challenge (SVCC) 2023, with a recognition-synthesis approach using self-supervised learning-based representation.</li>
<li>methods: 该方法首先使用公共可用的大规模750小时的语音和唱歌数据进行扩散基于的任意到任意语音转换模型的训练，然后对每个目标唱歌者&#x2F;说话者进行微调。</li>
<li>results: 大规模的听力测试显示，我们的T13系统在SVCC 2023中获得了竞争力强的自然性和说话者相似性，这表明了我们的方法在跨频道SVC中的泛化能力。<details>
<summary>Abstract</summary>
This paper presents our systems (denoted as T13) for the singing voice conversion challenge (SVCC) 2023. For both in-domain and cross-domain English singing voice conversion (SVC) tasks (Task 1 and Task 2), we adopt a recognition-synthesis approach with self-supervised learning-based representation. To achieve data-efficient SVC with a limited amount of target singer/speaker's data (150 to 160 utterances for SVCC 2023), we first train a diffusion-based any-to-any voice conversion model using publicly available large-scale 750 hours of speech and singing data. Then, we finetune the model for each target singer/speaker of Task 1 and Task 2. Large-scale listening tests conducted by SVCC 2023 show that our T13 system achieves competitive naturalness and speaker similarity for the harder cross-domain SVC (Task 2), which implies the generalization ability of our proposed method. Our objective evaluation results show that using large datasets is particularly beneficial for cross-domain SVC.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍我们的系统（简称为T13）在2023年歌唱voice conversions挑战（SVCC）中的应用。对于英语歌唱voice conversions（SVC）的内域和跨域任务（任务1和任务2），我们采用了认知-合成方法，使用自我超vised学习基于表示。为了实现数据精efficient的SVC，我们首先使用公共可用的大规模750小时的说话和唱歌数据来训练一个扩散-based any-to-anyvoice conversions模型。然后，我们对每个目标歌手/说话人进行了微调。SVCC 2023年的大规模听力测试显示，我们的T13系统在跨域SVC（任务2）中实现了竞争性的自然和说话人相似性，这表明了我们提出的方法的泛化能力。我们的目标评价结果表明，使用大量数据对跨域SVC是非常有利的。
</details></li>
</ul>
<hr>
<h2 id="Loose-lips-sink-ships-Mitigating-Length-Bias-in-Reinforcement-Learning-from-Human-Feedback"><a href="#Loose-lips-sink-ships-Mitigating-Length-Bias-in-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback"></a>Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05199">http://arxiv.org/abs/2310.05199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang</li>
<li>for: 这篇论文的目的是如何使用人类反馈来改善大型自然语言模型，使其更好地适应人类和社会价值。</li>
<li>methods: 这篇论文使用了Product-of-Experts（PoE）技术，将奖励模型分为两部分：主要专家关注人类意图，而偏见专家则targets the identification and capture of length bias。另外，为了进一步提高偏见的学习，我们导入了扰动 INTO the bias-focused expert, disrupting the flow of semantic information。</li>
<li>results: 实验结果显示，我们的方法可以改善语言模型的性能，不受序列长度的影响。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs. In this paper, we propose an innovative solution, applying the Product-of-Experts (PoE) technique to separate reward modeling from the influence of sequence length. In our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. To further enhance the learning of bias, we introduce perturbations into the bias-focused expert, disrupting the flow of semantic information. Experimental results validate the effectiveness of our approach, indicating that language model performance is improved, irrespective of sequence length.
</details>
<details>
<summary>摘要</summary>
大language模型可以通过人类反馈来进行强化学习，这种反馈可以帮助模型与人类和社会价值观念相Alignment。为了学习奖励模型，需要一个大量的人类反馈，然后使用这个奖励模型来精化语言模型。然而，我们发现奖励模型经常会寻找短cut的缺点，假设人类更喜欢 longer responses。这种Length bias会导致模型偏好 longer outputs，但这并不意味着这些输出中含有更多的有用信息。在这篇论文中，我们提出了一种创新的解决方案，通过Product-of-Experts（PoE）技术分离奖励模型和序列长度的影响。在我们的框架中，主专家专注于理解人类意图，而偏好专家则targets the identification and capture of length bias。为了进一步增强偏好的学习，我们引入了对偏好专家中的干扰，使得 semantic information的流动被中断。实验结果证明了我们的方法的有效性，表明语言模型的性能不受序列长度的限制。
</details></li>
</ul>
<hr>
<h2 id="FABRIC-Automated-Scoring-and-Feedback-Generation-for-Essays"><a href="#FABRIC-Automated-Scoring-and-Feedback-Generation-for-Essays" class="headerlink" title="FABRIC: Automated Scoring and Feedback Generation for Essays"></a>FABRIC: Automated Scoring and Feedback Generation for Essays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05191">http://arxiv.org/abs/2310.05191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Hyunseung Lim, Yoonsu Kim, Tak Yeon Lee, Hwajung Hong, Juho Kim, So-Yeon Ahn, Alice Oh</li>
<li>for: 这个论文是为了提供一种自动生成英语写作评分的工具，以帮助学生和教师在写作课程中更好地评分和反馈写作。</li>
<li>methods: 该论文使用了一种管道模型，包括DREsS、CASE和EssayCoT三部分。DREsS是一个基于标准的写作评分数据集，CASE是一种伪造策略，可以提高模型的准确率。EssayCoT是一种写作思维推荐策略，可以根据模型预测的分数提供更好的反馈。</li>
<li>results: 论文表明，使用新的数据集DREsS和伪造策略CASE可以提高模型的准确率，并且使用EssayCoT可以提供更好的反馈。论文还表明，学生和教师对新的评分和反馈表示满意，评分和反馈的帮助程度也得到了提升。<details>
<summary>Abstract</summary>
Automated essay scoring (AES) provides a useful tool for students and instructors in writing classes by generating essay scores in real-time. However, previous AES models do not provide more specific rubric-based scores nor feedback on how to improve the essays, which can be even more important than the overall scores for learning. We present FABRIC, a pipeline to help students and instructors in English writing classes by automatically generating 1) the overall scores, 2) specific rubric-based scores, and 3) detailed feedback on how to improve the essays. Under the guidance of English education experts, we chose the rubrics for the specific scores as content, organization, and language. The first component of the FABRIC pipeline is DREsS, a real-world Dataset for Rubric-based Essay Scoring (DREsS). The second component is CASE, a Corruption-based Augmentation Strategy for Essays, with which we can improve the accuracy of the baseline model by 45.44%. The third component is EssayCoT, the Essay Chain-of-Thought prompting strategy which uses scores predicted from the AES model to generate better feedback. We evaluate the effectiveness of the new dataset DREsS and the augmentation strategy CASE quantitatively and show significant improvements over the models trained with existing datasets. We evaluate the feedback generated by EssayCoT with English education experts to show significant improvements in the helpfulness of the feedback across all rubrics. Lastly, we evaluate the FABRIC pipeline with students in a college English writing class who rated the generated scores and feedback with an average of 6 on the Likert scale from 1 to 7.
</details>
<details>
<summary>摘要</summary>
自动化文章评分（AES）为学生和教师写作课程提供了一个有用的工具，可以在实时生成文章评分。然而，先前的AES模型并不提供更加特定的评分标准和提高文章的细节反馈，这些反馈可能对学习更加重要。我们介绍了FABRIC管道，帮助学生和教师英语写作课程，可以自动生成1）总评分，2）特定评分标准，以及3）提高文章的细节反馈。在英语教育专家的指导下，我们选择了评分标准的内容、组织和语言。FABRIC管道的第一个组成部分是DREsS，一个用于评分标准的实际数据集（DREsS）。第二个组成部分是CASE，一种对文章进行恶意增强策略，可以提高基线模型的准确率45.44%。第三个组成部分是EssayCoT，文章链条思维提醒策略，使用AES模型预测的分数来生成更好的反馈。我们评估了新的数据集DREsS和增强策略CASE的效果，并显示了与现有数据集训练的模型相比有显著提高。我们评估EssayCoT生成的反馈与英语教育专家相比，并显示了所有评分标准上的有用性提高。最后，我们评估了FABRIC管道与大学英语写作课程学生的反馈，学生对生成的分数和反馈给出了7分的满分评价。
</details></li>
</ul>
<hr>
<h2 id="Do-Large-Language-Models-Know-about-Facts"><a href="#Do-Large-Language-Models-Know-about-Facts" class="headerlink" title="Do Large Language Models Know about Facts?"></a>Do Large Language Models Know about Facts?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05177">http://arxiv.org/abs/2310.05177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, Zhijiang Guo</li>
<li>for: 这paper的目的是评估大型自然语言处理模型（LLMs）中的事实知识，以及这些模型是否可以具备真实知识和抵御黑客攻击。</li>
<li>methods: 这paper使用了一个名为Pinocchio的benchmark，包含20000个多样化的事实问题，以评估LLMs中的事实知识。</li>
<li>results: 经过extensive的实验研究发现，现有的LLMs仍然缺乏事实知识，并且存在多种假相关性。<details>
<summary>Abstract</summary>
Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to comprehensively evaluate the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs are able to compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes will be publicly available.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）在最近几年中带来了一系列的性能提升。这些模型在预训练和调教过程中获得的 фактиче知识可以在多种下游任务中使用，如问答和语言生成。不同于传统的知识库（KB），LLM中的知识不是显式存储的，而是通过模型参数的方式隐式存储。由模型生成的内容经常会具有误差或不准确，因为模型可能会 incorrectly induce 或者随着时间的推移而变得过时。为了全面评估 LLM 中的知识范围和深度，我们设计了 Pinocchio  benchmark。Pinocchio 包含 20,000 个多样化的 фактиче问题，这些问题来自不同的来源、时间线、领域、地区和语言。此外，我们还 investigate 了 LLM 是否能够组合多个 фактиче知识、 temporally 更新知识、理解多个知识之间的关系、察看微妙的知识差异以及抵御骚扰示例。我们对不同大小和类型的 LLM 进行了广泛的实验，发现现有 LLM 仍然缺乏知识和受到多种假 correlate 的影响。我们认为这是人工智能实现可信worthy 的核心瓶颈。Pinocchio 数据集和我们的代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="On-the-Zero-Shot-Generalization-of-Machine-Generated-Text-Detectors"><a href="#On-the-Zero-Shot-Generalization-of-Machine-Generated-Text-Detectors" class="headerlink" title="On the Zero-Shot Generalization of Machine-Generated Text Detectors"></a>On the Zero-Shot Generalization of Machine-Generated Text Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05165">http://arxiv.org/abs/2310.05165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Pu, Jingyu Zhang, Xiaochuang Han, Yulia Tsvetkov, Tianxing He</li>
<li>for: 本研究的目的是检测机器生成的文本，以确定新生成器输出的真实性。</li>
<li>methods: 本研究使用了许多大语言模型生成的数据，并使用神经网络检测器来检测机器生成的文本。</li>
<li>results: 研究发现，使用中等大小的语言模型生成的数据来训练检测器，可以在其他大型模型上实现零基础泛化。这表明，可以通过将中等大小模型的数据作为基础，建立可靠的机器生成文本检测器。<details>
<summary>Abstract</summary>
The rampant proliferation of large language models, fluent enough to generate text indistinguishable from human-written language, gives unprecedented importance to the detection of machine-generated text. This work is motivated by an important research question: How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from each generator and test its performance on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models.
</details>
<details>
<summary>摘要</summary>
大量的语言模型的蔓延，使得机器生成文本的检测成为了不可或缺的任务。这项工作受到一个重要的研究问题的推动：新生成器输出的机器生成文本检测器如何表现？我们开始sBy collecting generation data from a wide range of LLMs, and training neural detectors on data from each generator, we test the performance of these detectors on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models.Here's the translation breakdown:* 大量 (dà liàng) - large amount* 语言模型 (yǔ yán módel) - language model* 蔓延 (shū yì) - rampant proliferation* 机器生成文本 (jī shì zhì yì wén tǐ) - machine-generated text* 检测 (jiǎn dòu) - detection* 新生成器 (xīn shēng chéng qì) - new generator* 输出 (xū chū) - output* 机器生成文本检测器 (jī shì zhì yì wén tǐ jiàn dòu qì) - machine-generated text detector* none of the detectors can generalize to all generators (zhè yī xiàng qù zhè yī xiàng qù) - none of the detectors can generalize to all generators* medium-size LLM (zhōng xiǎo yǔ yán módel) - medium-size language model* zero-shot generalize (zhè yī xiàng qù) - zero-shot generalize* ensemble (jiān) - ensemble* training data (liào xīng xīng) - training data* robust (dòu lì) - robust* detectors (jiàn dòu qì) - detectors
</details></li>
</ul>
<hr>
<h2 id="An-Investigation-of-LLMs’-Inefficacy-in-Understanding-Converse-Relations"><a href="#An-Investigation-of-LLMs’-Inefficacy-in-Understanding-Converse-Relations" class="headerlink" title="An Investigation of LLMs’ Inefficacy in Understanding Converse Relations"></a>An Investigation of LLMs’ Inefficacy in Understanding Converse Relations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05163">http://arxiv.org/abs/2310.05163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/3b-group/convre">https://github.com/3b-group/convre</a></li>
<li>paper_authors: Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, Yuanjun Laili</li>
<li>for: 本文 investigate LLMs 是否真的理解正式语言的结构化 semantics，通过一个特殊情况——抽象 binary relation。</li>
<li>methods: 本文 introduce 一个新的 benchmark ConvRE，该 benchmark 包含 17 关系和 1240 个 triple 从受欢迎的知识 Graph completion 数据集中提取出来。本 benchmark 包含 two 个任务：Re2Text 和 Text2Re，它们是通过多选问答来评估 LLMs 对关系和相关文本的匹配能力。</li>
<li>results: 经过实验表明，LLMs 经常采用短cut 学习，并且在我们的 proposed benchmark 上仍然遇到挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs' ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Recurrent-Neural-Language-Models-as-Probabilistic-Finite-state-Automata"><a href="#Recurrent-Neural-Language-Models-as-Probabilistic-Finite-state-Automata" class="headerlink" title="Recurrent Neural Language Models as Probabilistic Finite-state Automata"></a>Recurrent Neural Language Models as Probabilistic Finite-state Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05161">http://arxiv.org/abs/2310.05161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anej Svete, Ryan Cotterell</li>
<li>for: 本文研究语言模型（LM）的表示能力和限制，通过使用已知的ormalism来准确地描述LM的能力和限制。</li>
<li>methods: 本文使用了回归神经网络（RNN）LM来研究LM可以表示哪些概率分布。</li>
<li>results: 研究结果表明，简单的RNN可以表示一个子集的概率分布，而且需要至少有 $\Omega\left(N |\Sigma|\right)$ 神经元来表示一个任意决定性 finite-state LM。<details>
<summary>Abstract</summary>
Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the representational capacity of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages -- rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requires $\Omega\left(N |\Sigma|\right)$ neurons. These results present a first step towards characterizing the classes of distributions RNN LMs can represent and thus help us understand their capabilities and limitations.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:研究语言模型（LM）使用已知的形式主义，可以准确地描述它们的能力和局限性。先前的工作已经研究了基于回归神经网络（RNN）的语言模型的表示能力，但是LM不是形式语言的描述，而是一种字符串上的概率分布。在这项工作中，我们研究了RNN可以表示哪些类型的概率分布，这使得我们可以更直接地说明它们的能力。我们显示了简单的RNN等价于一个子集的概率金字塔自动机，因此它们可以模型一 subset of概率分布可以由金字塔自动机表示。此外，我们研究了表示finite-state LM的RNN空间复杂度。我们显示了，要表示一个任意deterministic finite-state LM，需要$\Omega\left(N |\Sigma|\right)$ neuron。这些结果为我们帮助理解RNN可以表示哪些类型的概率分布，并且帮助我们理解它们的能力和局限性。
</details></li>
</ul>
<hr>
<h2 id="From-Data-to-Dialogue-Leveraging-the-Structure-of-Knowledge-Graphs-for-Conversational-Exploratory-Search"><a href="#From-Data-to-Dialogue-Leveraging-the-Structure-of-Knowledge-Graphs-for-Conversational-Exploratory-Search" class="headerlink" title="From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for Conversational Exploratory Search"></a>From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for Conversational Exploratory Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05150">http://arxiv.org/abs/2310.05150</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sebischair/kg-conv-exploratory-search">https://github.com/sebischair/kg-conv-exploratory-search</a></li>
<li>paper_authors: Phillip Schneider, Nils Rehtanz, Kristiina Jokinen, Florian Matthes</li>
<li>for: 这篇研究旨在探索新闻文章中的探索搜寻，以实现对话式搜寻和知识库的融合，从而将结构化和无结构化资料搜寻融合在一起。</li>
<li>methods: 本研究使用了对话式搜寻系统和知识库来支持探索搜寻，并透过自然语言问题来询问新闻文章中的相关资讯。</li>
<li>results: 根据54名参与者的用户研究，这种基于知识库的对话式搜寻系统被证明是有效的，并且提供了开发这类系统的设计假设。<details>
<summary>Abstract</summary>
Exploratory search is an open-ended information retrieval process that aims at discovering knowledge about a topic or domain rather than searching for a specific answer or piece of information. Conversational interfaces are particularly suitable for supporting exploratory search, allowing users to refine queries and examine search results through interactive dialogues. In addition to conversational search interfaces, knowledge graphs are also useful in supporting information exploration due to their rich semantic representation of data items. In this study, we demonstrate the synergistic effects of combining knowledge graphs and conversational interfaces for exploratory search, bridging the gap between structured and unstructured information retrieval. To this end, we propose a knowledge-driven dialogue system for exploring news articles by asking natural language questions and using the graph structure to navigate between related topics. Based on a user study with 54 participants, we empirically evaluate the effectiveness of the graph-based exploratory search and discuss design implications for developing such systems.
</details>
<details>
<summary>摘要</summary>
<SYS>探索搜寻是一种开放式搜寻过程，旨在探索一个主题或领域中的知识而不是寻找具体的答案或信息。对话式 интерфей斯特别适合支持探索搜寻，允许用户通过交互对话来细化查询和检视搜寻结果。此外，知识图也非常有用于支持信息探索，因为它们可以提供丰富的Semantic Representation的数据项。在这项研究中，我们证明了结合知识图和对话式 интерфей斯可以减少结构化和无结构化搜寻之间的差距，并提供一种基于知识的对话系统来探索新闻文章。基于54名参与者的用户研究，我们Empirically评估了图structure-based探索搜寻的效果，并讨论了开发这类系统的设计方面。</SYS>Here's a word-for-word translation of the text into Simplified Chinese:<SYS>探索搜寻是一种开放式搜寻过程，旨在探索一个主题或领域中的知识而不是寻找具体的答案或信息。对话式 интерфей斯特别适合支持探索搜寻，允许用户通过交互对话来细化查询和检视搜寻结果。此外，知识图也非常有用于支持信息探索，因为它们可以提供丰富的Semantic Representation的数据项。在这项研究中，我们证明了结合知识图和对话式 интерфей斯可以减少结构化和无结构化搜寻之间的差距，并提供一种基于知识的对话系统来探索新闻文章。基于54名参与者的用户研究，我们Empirically评估了图structure-based探索搜寻的效果，并讨论了开发这类系统的设计方面。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Retrieval-Generation-Synergy-Augmented-Large-Language-Models"><a href="#Retrieval-Generation-Synergy-Augmented-Large-Language-Models" class="headerlink" title="Retrieval-Generation Synergy Augmented Large Language Models"></a>Retrieval-Generation Synergy Augmented Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05149">http://arxiv.org/abs/2310.05149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin</li>
<li>for: 提高大型自然语言模型的理解能力和多步逻辑能力</li>
<li>methods: 融合任务相关文档和大型自然语言模型，通过反射-生成协作机制，利用参数化和非参数化知识，找到正确的逻辑路径</li>
<li>results: 在四个问答任务上，经验结果表明我们的方法可以显著提高大型自然语言模型的逻辑能力，并超越先前的基eline。<details>
<summary>Abstract</summary>
Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.
</details>
<details>
<summary>摘要</summary>
大型语言模型，通过与任务相关的文档的协同工作，已经在知识型任务中表现出了惊人的表现。然而，现有的方法主要分为两类：一是从外部知识库中检索，另一是利用大型语言模型生成文档。我们提出了一种迭代检索生成协同框架，不仅能充分利用参数化和非参数化知识，而且能够通过检索生成互动，找到正确的逻辑路径，这对于需要多步逻辑的任务非常重要。我们在四个问答dataset上进行了实验，包括单步QA和多步QA任务。实验结果表明，我们的方法可以显著提高大型语言模型的逻辑能力，并超越先前的基elines。
</details></li>
</ul>
<hr>
<h2 id="Fast-DetectGPT-Efficient-Zero-Shot-Detection-of-Machine-Generated-Text-via-Conditional-Probability-Curvature"><a href="#Fast-DetectGPT-Efficient-Zero-Shot-Detection-of-Machine-Generated-Text-via-Conditional-Probability-Curvature" class="headerlink" title="Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature"></a>Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05130">http://arxiv.org/abs/2310.05130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baoguangsheng/fast-detect-gpt">https://github.com/baoguangsheng/fast-detect-gpt</a></li>
<li>paper_authors: Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang<br>for: 本研究旨在分别区别机器生成和人类撰写的内容，以建立可信赖的人工智能系统。methods: 本研究使用了 conditional probability curvature 来显示机器学习模型和人类之间的差异。results: Fast-DetectGPT 比 DetectGPT 更高效，可以在不同的数据集、来源模型和测试环境下提高检测效能，并且可以实现340倍的速度提升。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present Fast-DetectGPT, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only outperforms DetectGPT in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Document-level-Event-Argument-Extraction-with-Contextual-Clues-and-Role-Relevance"><a href="#Enhancing-Document-level-Event-Argument-Extraction-with-Contextual-Clues-and-Role-Relevance" class="headerlink" title="Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance"></a>Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05991">http://arxiv.org/abs/2310.05991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LWL-cpu/SCPRG-master">https://github.com/LWL-cpu/SCPRG-master</a></li>
<li>paper_authors: Wanlong Liu, Shaohuan Cheng, Dingyi Zeng, Hong Qu</li>
<li>for: 这个论文主要针对的是文档级事件抽象EXTRACTION中的新挑战，即输入长度大、跨句理解。</li>
<li>methods: 我们提出了一种基于Span-trigger-based Contextual Pooling和 latent Role Guidance的SCPRG模型，包括两个新的有效模块，即 Span-Trigger-based Contextual Pooling(STCP)和 Role-based Latent Information Guidance (RLIG)。</li>
<li>results: 我们的SCPRG模型在两个公共数据集上进行了比较，与之前的状态态方法相比，提高了1.13和2.64的F1分数。<details>
<summary>Abstract</summary>
Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial points: a) non-argument contextual clue information; b) the relevance among argument roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling and latent Role Guidance) model, which contains two novel and effective modules for the above problem. The Span-Trigger-based Contextual Pooling(STCP) adaptively selects and aggregates the information of non-argument clue words based on the context attention weights of specific argument-trigger pairs from pre-trained model. The Role-based Latent Information Guidance (RLIG) module constructs latent role representations, makes them interact through role-interactive encoding to capture semantic relevance, and merges them into candidate arguments. Both STCP and RLIG introduce no more than 1% new parameters compared with the base model and can be easily applied to other event extraction models, which are compact and transplantable. Experiments on two public datasets show that our SCPRG outperforms previous state-of-the-art methods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents respectively. Further analyses illustrate the interpretability of our model.
</details>
<details>
<summary>摘要</summary>
文档级事件语义抽象带来新的挑战，包括长输入和跨句 inference，与句子级对应的模型不同。然而，大多数前作 FoCUS 在事件触发器和候选参与者之间的关系，忽略了两点： a) 非参与者上下文提示信息; b) 参与者角色之间的相关性。在这篇论文中，我们提出了一种SCPRG（Span-trigger-based Contextual Pooling and latent Role Guidance）模型，其包含两个新的有效模块。Span-Trigger-based Contextual Pooling（STCP）模块根据特定参与者-触发器对的上下文注意力权重自适应地选择和聚合非参与者提示词的信息。Role-based Latent Information Guidance（RLIG）模块构建了latent角色表示，使其互相交互编码，捕捉 semantic relevance，并将其与候选参与者结合。STCP和RLIG模块新增 Parameters 不超过1%，可以与基础模型一起使用，并且可以轻松应用于其他事件抽象模型。我们在两个公共数据集上进行了实验，结果显示，我们的SCPRG模型在RAMS和WikiEvents上的F1分别提高1.13和2.64。进一步的分析表明了我们模型的可读性。
</details></li>
</ul>
<hr>
<h2 id="CARLG-Leveraging-Contextual-Clues-and-Role-Correlations-for-Improving-Document-level-Event-Argument-Extraction"><a href="#CARLG-Leveraging-Contextual-Clues-and-Role-Correlations-for-Improving-Document-level-Event-Argument-Extraction" class="headerlink" title="CARLG: Leveraging Contextual Clues and Role Correlations for Improving Document-level Event Argument Extraction"></a>CARLG: Leveraging Contextual Clues and Role Correlations for Improving Document-level Event Argument Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05116">http://arxiv.org/abs/2310.05116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanlong Liu, Wenyu Chen, Dingyi Zeng, Li Zhou, Hong Qu</li>
<li>for: 提高文档级事件抽象EXTRACTION的精度。</li>
<li>methods: 提出了一种基于CONTEXTUAL CLUES和ROLE correlation的CARLG模型，包括CONTEXTUAL CLUES Aggregation（CCA）模块和ROLE-based Latent Information Guidance（RLIG）模块，利用上下文注意力权重和角色相互作用编码，从而提高文档级EXTRACTION的精度。</li>
<li>results: 在RAMS、WikiEvents和MLEE datasets上进行了广泛的实验，并证明了CARLG模型的超越性，与之前的状态艺术方法相比，提高了1.26倍、1.22倍和1.98倍的F1分数，同时降低了推理时间 by 31%。<details>
<summary>Abstract</summary>
Document-level event argument extraction (EAE) is a crucial but challenging subtask in information extraction. Most existing approaches focus on the interaction between arguments and event triggers, ignoring two critical points: the information of contextual clues and the semantic correlations among argument roles. In this paper, we propose the CARLG model, which consists of two modules: Contextual Clues Aggregation (CCA) and Role-based Latent Information Guidance (RLIG), effectively leveraging contextual clues and role correlations for improving document-level EAE. The CCA module adaptively captures and integrates contextual clues by utilizing context attention weights from a pre-trained encoder. The RLIG module captures semantic correlations through role-interactive encoding and provides valuable information guidance with latent role representation. Notably, our CCA and RLIG modules are compact, transplantable and efficient, which introduce no more than 1% new parameters and can be easily equipped on other span-base methods with significant performance boost. Extensive experiments on the RAMS, WikiEvents, and MLEE datasets demonstrate the superiority of the proposed CARLG model. It outperforms previous state-of-the-art approaches by 1.26 F1, 1.22 F1, and 1.98 F1, respectively, while reducing the inference time by 31%. Furthermore, we provide detailed experimental analyses based on the performance gains and illustrate the interpretability of our model.
</details>
<details>
<summary>摘要</summary>
文档级事件参数提取（EAE）是信息提取中的关键但是挑战性任务。现有大多数方法强调事件触发器和参数之间的交互，忽略了两个关键点：文档背景信息和参数角色之间的 semantics 相关性。在这篇论文中，我们提出了 CARLG 模型，它由两个模块组成：文档背景信息汇集（CCA）和角色相关信息引导（RLIG）。CCA 模块可以适应地捕捉和 инте integrate 文档背景信息，并通过使用上下文注意力权重从预训练的 encoder 获得上下文注意力权重。RLIG 模块通过角色交互编码来捕捉参数角色之间的 semantics 相关性，并提供有价值的信息引导，使用潜在角色表示。各自CCA和RLIG模块都是紧凑、可移植和高效的，其新增参数不超过 1%，可以轻松地在其他基于宽度的方法上采用，并且可以提高性能。我们在 RAMS、WikiEvents 和 MLEE 数据集上进行了广泛的实验，并证明了我们的 CARLG 模型在这些数据集上的超越性。它与前一个状态的方法相比，提高了 1.26 F1、1.22 F1 和 1.98 F1，同时降低了推理时间 31%。此外，我们还提供了详细的实验分析，以及模型的可读性。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Large-Language-Models-with-Augmented-Instructions-for-Fine-grained-Information-Extraction"><a href="#Benchmarking-Large-Language-Models-with-Augmented-Instructions-for-Fine-grained-Information-Extraction" class="headerlink" title="Benchmarking Large Language Models with Augmented Instructions for Fine-grained Information Extraction"></a>Benchmarking Large Language Models with Augmented Instructions for Fine-grained Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05092">http://arxiv.org/abs/2310.05092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Gao, Huan Zhao, Yice Zhang, Wei Wang, Changlong Yu, Ruifeng Xu</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）在自然语言处理中的信息提取 task 中的应用。</li>
<li>methods: 本研究使用了精细化的信息提取标准 benchmark 数据集，并采用了加强的提取规则和输出格式来适应 LLMS 的能力。</li>
<li>results: 我们的研究发现，使用encoder-decoder模型（特别是 T5 和 FLAN-T5）可以在不同的信息类型中具有普适性，而 ChatGPT 则在新任务形态中具有更高的适应性。我们的结果还表明，模型缩放不是决定性的性能因素，architecture、数据多样性和学习技术也具有重要的作用。这项研究为 LLMS 在信息提取中的更加细化和多样化应用提供了道路。<details>
<summary>Abstract</summary>
Information Extraction (IE) is an essential task in Natural Language Processing. Traditional methods have relied on coarse-grained extraction with simple instructions. However, with the emergence of Large Language Models (LLMs), there is a need to adapt IE techniques to leverage the capabilities of these models. This paper introduces a fine-grained IE benchmark dataset tailored for LLMs, employing augmented instructions for each information type, which includes task descriptions, extraction rules, output formats, and examples. Through extensive evaluations, we observe that encoder-decoder models, particularly T5 and FLAN-T5, perform well in generalizing to unseen information types, while ChatGPT exhibits greater adaptability to new task forms. Our results also indicate that performance is not solely dictated by model scale, and highlight the significance of architecture, data diversity, and learning techniques. This work paves the way for a more refined and versatile utilization of LLMs in Information Extraction.
</details>
<details>
<summary>摘要</summary>
信息提取（IE）是自然语言处理中的一项重要任务。传统方法通常采用粗粒度提取，使用简单的指令。然而，随着大语言模型（LLM）的出现，需要对IE技术进行适应。本文介绍了一个适合LLM的细致提取数据集，使用了增强的指令集，包括任务描述、提取规则、输出格式和示例。经过广泛的评估，我们发现使用encoder-decoder模型，特别是T5和FLAN-T5，在未经见情报类型上进行泛化性能良好，而ChatGPT在新任务形式上表现出更大的适应性。我们的结果还表明，性能不 solely 受模型规模的限制，也受到体系、数据多样性和学习技巧的影响。这项工作为LLM在信息提取中更加细致和多样化的使用开出了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Argument-Structure-Extraction-with-Efficient-Leverage-of-Contextual-Information"><a href="#Enhancing-Argument-Structure-Extraction-with-Efficient-Leverage-of-Contextual-Information" class="headerlink" title="Enhancing Argument Structure Extraction with Efficient Leverage of Contextual Information"></a>Enhancing Argument Structure Extraction with Efficient Leverage of Contextual Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05073">http://arxiv.org/abs/2310.05073</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luoxiaoheics/ecase">https://github.com/luoxiaoheics/ecase</a></li>
<li>paper_authors: Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Jie Zhou, Yue Zhang</li>
<li>for: 本研究旨在提高对文档中Arguments的结构分析性能。</li>
<li>methods: 我们提出了一种高效的上下文感知ASE模型（ECASE），利用上下文信息来增强模型的表达能力和训练数据。具体来说，我们引入了序列注意力模块和距离权重相似损失函数，以便聚合上下文信息和 argumentative 信息。此外，我们还随机屏蔽了文档中的讨论标识符和句子，以降低模型对特定单词或 menos informative 句子的依赖。</li>
<li>results: 我们在五个不同领域的五个数据集上进行了实验，并确认了我们的模型在这些数据集上的状态知识表现。此外，我们还进行了减少模块的研究，以证明每个模块在我们的模型中的效果。<details>
<summary>Abstract</summary>
Argument structure extraction (ASE) aims to identify the discourse structure of arguments within documents. Previous research has demonstrated that contextual information is crucial for developing an effective ASE model. However, we observe that merely concatenating sentences in a contextual window does not fully utilize contextual information and can sometimes lead to excessive attention on less informative sentences. To tackle this challenge, we propose an Efficient Context-aware ASE model (ECASE) that fully exploits contextual information by enhancing modeling capacity and augmenting training data. Specifically, we introduce a sequence-attention module and distance-weighted similarity loss to aggregate contextual information and argumentative information. Additionally, we augment the training data by randomly masking discourse markers and sentences, which reduces the model's reliance on specific words or less informative sentences. Our experiments on five datasets from various domains demonstrate that our model achieves state-of-the-art performance. Furthermore, ablation studies confirm the effectiveness of each module in our model.
</details>
<details>
<summary>摘要</summary>
Argument structure extraction (ASE) targets to identify the discourse structure of arguments within documents. Previous research has shown that contextual information is crucial for developing an effective ASE model. However, we find that simply concatenating sentences in a contextual window does not fully utilize contextual information and can sometimes lead to excessive attention on less informative sentences. To address this challenge, we propose an Efficient Context-aware ASE model (ECASE) that fully exploits contextual information by enhancing modeling capacity and augmenting training data. Specifically, we introduce a sequence-attention module and distance-weighted similarity loss to aggregate contextual information and argumentative information. Additionally, we augment the training data by randomly masking discourse markers and sentences, which reduces the model's reliance on specific words or less informative sentences. Our experiments on five datasets from various domains demonstrate that our model achieves state-of-the-art performance. Furthermore, ablation studies confirm the effectiveness of each module in our model.Here's the word-for-word translation:Argument structure extraction (ASE) targets to identify the discourse structure of arguments within documents. Previous research has shown that contextual information is crucial for developing an effective ASE model. However, we find that simply concatenating sentences in a contextual window does not fully utilize contextual information and can sometimes lead to excessive attention on less informative sentences. To address this challenge, we propose an Efficient Context-aware ASE model (ECASE) that fully exploits contextual information by enhancing modeling capacity and augmenting training data. Specifically, we introduce a sequence-attention module and distance-weighted similarity loss to aggregate contextual information and argumentative information. Additionally, we augment the training data by randomly masking discourse markers and sentences, which reduces the model's reliance on specific words or less informative sentences. Our experiments on five datasets from various domains demonstrate that our model achieves state-of-the-art performance. Furthermore, ablation studies confirm the effectiveness of each module in our model.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Multilingual-Encoder-Potential-Boosting-Zero-Shot-Performance-via-Probability-Calibration"><a href="#Unleashing-the-Multilingual-Encoder-Potential-Boosting-Zero-Shot-Performance-via-Probability-Calibration" class="headerlink" title="Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration"></a>Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05069">http://arxiv.org/abs/2310.05069</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ercong21/calibration">https://github.com/ercong21/calibration</a></li>
<li>paper_authors: Ercong Nie, Helmut Schmid, Hinrich Schütze</li>
<li>for: 这个论文主要针对Zero-shot和少量示例情景下的多语言任务和语言探测问题。</li>
<li>methods: 这个论文使用预训练多语言encoder模型，通过重写输入示例为cloze风格的问题，直接完成多语言任务或语言探测。这种方法不需要更新模型参数。但是，模型偏好预测频繁出现的标签词，导致性能有限制。为了解决这个问题，这个论文提出了一种简单的准确化方法，并与其他现有技术进行比较。</li>
<li>results: 这个论文使用准确化技术与预训练多语言encoder模型结合，在多种任务中实现了显著性能提升。<details>
<summary>Abstract</summary>
Pretrained multilingual encoder models can directly perform zero-shot multilingual tasks or linguistic probing by reformulating the input examples into cloze-style prompts. This is accomplished by predicting the probabilities of the label words at the masked token position, without requiring any updates to the model parameters. However, the performance of this method is limited by the model's bias toward predicting label words which frequently occurred during the pretraining. These words typically receive high probabilities. To address this issue, we combine the models with calibration techniques which modify the probabilities of label words predicted by the models. We first validate the effectiveness of a proposed simple calibration method together with other existing techniques on monolingual encoders in both zero- and few-shot scenarios. We subsequently employ these calibration techniques on multilingual encoders, resulting in substantial performance improvements across a wide range of tasks.
</details>
<details>
<summary>摘要</summary>
预训练多语言encoder模型可以直接执行零shot多语言任务或语言探测，通过重写输入示例为cloze样式提示。这是通过预测掩码Token位置的标签词概率，不需要更新模型参数。然而，这种方法的性能受到模型对预测常见的标签词的偏好的限制。这些词通常会 Receive高概率预测。为解决这个问题，我们将模型与加拟定技术相结合， modify模型预测标签词的概率。我们首先验证提议的简单加拟定方法，以及其他现有的技术在单语言encoder上的效果。然后，我们在多语言encoder上使用这些加拟定技术， resulting in 广泛任务中的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Guideline-Learning-for-In-context-Information-Extraction"><a href="#Guideline-Learning-for-In-context-Information-Extraction" class="headerlink" title="Guideline Learning for In-context Information Extraction"></a>Guideline Learning for In-context Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05066">http://arxiv.org/abs/2310.05066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoxu Pang, Yixuan Cao, Qiang Ding, Ping Luo</li>
<li>for: 提高嵌入式学习（ICL）中的信息提取性能（IE）。</li>
<li>methods: 提出指南学习（GL）框架，在学习阶段自动生成指南，在推断阶段根据错误案例选择有助于ICL的指南。同时，提出基于自我一致性的活动学习方法，提高GL的效率。</li>
<li>results: 在事件提取和关系提取任务上，GL可以显著提高嵌入式IE的性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a self-consistency-based active learning method to enhance the efficiency of GL. Experiments on event extraction and relation extraction show that GL can significantly improve the performance of in-context IE.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="sign-mt-Real-Time-Multilingual-Sign-Language-Translation-Application"><a href="#sign-mt-Real-Time-Multilingual-Sign-Language-Translation-Application" class="headerlink" title="sign.mt: Real-Time Multilingual Sign Language Translation Application"></a>sign.mt: Real-Time Multilingual Sign Language Translation Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05064">http://arxiv.org/abs/2310.05064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Moryossef</li>
<li>for: 这个研究旨在为听语和手语之间的交流问题提供解决方案，实现语言通信的协调。</li>
<li>methods: 这个开源应用程序使用了现代的开源模型，包括对话语言模型和手语识别模型，以提供即时多语言对话的转换。</li>
<li>results: 这个应用程序可以实现即时多语言对话的转换，并且提供了自定义的真实人工手语演示，以激发用户参与和满意度。<details>
<summary>Abstract</summary>
This demo paper presents sign.mt, an open-source application pioneering real-time multilingual bi-directional translation between spoken and signed languages. Harnessing state-of-the-art open-source models, this tool aims to address the communication divide between the hearing and the deaf, facilitating seamless translation in both spoken-to-signed and signed-to-spoken translation directions.   Promising reliable and unrestricted communication, sign.mt offers offline functionality, crucial in areas with limited internet connectivity. It further enhances user engagement by offering customizable photo-realistic sign language avatars, thereby encouraging a more personalized and authentic user experience.   Licensed under CC BY-NC-SA 4.0, sign.mt signifies an important stride towards open, inclusive communication. The app can be used, and modified for personal and academic uses, and even supports a translation API, fostering integration into a wider range of applications. However, it is by no means a finished product.   We invite the NLP community to contribute towards the evolution of sign.mt. Whether it be the integration of more refined models, the development of innovative pipelines, or user experience improvements, your contributions can propel this project to new heights. Available at https://sign.mt, it stands as a testament to what we can achieve together, as we strive to make communication accessible to all.
</details>
<details>
<summary>摘要</summary>
这个示例文章介绍了一个开源应用程序，即sign.mt，它实现了实时多语言对话转化，包括口头语言和手语两种语言之间的对话转化。使用现有的开源模型，这工具计划解决听力和耳语之间的沟通差异，为听力和耳语之间的对话提供流畅的翻译。  sign.mt 提供了可靠和无限制的沟通，并且在网络连接性较差的地区具有离线功能。它还提高了用户参与度，通过提供可定制的真实手语人物，使用户感受到更个性化和原始的用户体验。  根据 CC BY-NC-SA 4.0 许可证，sign.mt 表示开放、包容的沟通的重要一步。这个应用程序可以用于个人和学术用途，甚至支持翻译 API，以便更广泛地应用。尽管不是一款完整的产品，但我们邀请 NLP 社区参与 sign.mt 的演进。你的贡献可以使这个项目走向更高的峰点，包括更加精准的模型集成、创新的管道开发和用户体验改进等。可以在 <https://sign.mt> 上获取更多信息。
</details></li>
</ul>
<hr>
<h2 id="BRAINTEASER-Lateral-Thinking-Puzzles-for-Large-Language-Models"><a href="#BRAINTEASER-Lateral-Thinking-Puzzles-for-Large-Language-Models" class="headerlink" title="BRAINTEASER: Lateral Thinking Puzzles for Large Language Models"></a>BRAINTEASER: Lateral Thinking Puzzles for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05057">http://arxiv.org/abs/2310.05057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati</li>
<li>for: 该论文旨在检验语义理解模型是否具备倾向性思维能力，以及模型是否能够扭转默认知的关系。</li>
<li>methods: 该论文使用了多选问答任务，以检验模型的倾向性思维能力。其中，模型需要从多个选项中选择正确答案，而不是直接回答问题。</li>
<li>results: 研究发现，当前的语义理解模型在倾向性思维任务中表现不佳，与人类表现的 gap 较大。此外，模型在不同的倾向性思维任务中的表现也异常。<details>
<summary>Abstract</summary>
The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.
</details>
<details>
<summary>摘要</summary>
成功的语言模型使得自然语言处理（NLP）社区受到了关注，把注意力转移到需要间接和复杂的理解的任务上。虽然垂直思维任务在某种程度上受到了普遍的关注，但是水平思维拼图得到了少量的关注。为了填补这个差距，我们设计了Brainteaser：一种多选问答任务，旨在测试模型的水平思维能力和脱离默认的共同理解。我们采用了三步过程来创建第一个水平思维标准 benchmark：数据收集、distractor生成和对抗示例生成，共计1,100个高质量注释的拼图。为了评估模型的水平思维一致性，我们对Brainteaser的问题进行了semantic和contextual重建。我们的实验表明，当模型面临水平思维任务时，与人类的表现存在显著的差距，此差距甚至在对抗格式的一致性上受到了进一步的扩大。我们将所有的代码和数据公开，以便激励开发和评估水平思维模型的工作。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Power-of-ChatGPT-in-Fake-News-An-In-Depth-Exploration-in-Generation-Detection-and-Explanation"><a href="#Harnessing-the-Power-of-ChatGPT-in-Fake-News-An-In-Depth-Exploration-in-Generation-Detection-and-Explanation" class="headerlink" title="Harnessing the Power of ChatGPT in Fake News: An In-Depth Exploration in Generation, Detection and Explanation"></a>Harnessing the Power of ChatGPT in Fake News: An In-Depth Exploration in Generation, Detection and Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05046">http://arxiv.org/abs/2310.05046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Huang, Lichao Sun</li>
<li>For: The paper aims to explore ChatGPT’s proficiency in generating, explaining, and detecting fake news.* Methods: The paper employs four prompt methods to generate fake news samples and obtains nine features to characterize fake news based on ChatGPT’s explanations. It also examines ChatGPT’s capacity to identify fake news and proposes a reason-aware prompt method to improve its performance.* Results: The paper demonstrates that ChatGPT shows commendable performance in detecting fake news, but there is still room for improvement. It also explores the potential extra information that could bolster its effectiveness in detecting fake news.Here are the three key points in Simplified Chinese text:* For: 本研究旨在探讨ChatGPT在生成、解释和检测假新闻方面的能力。* Methods: 本研究使用四种提示方法生成假新闻样本，并通过自我评估和人类评估来证明这些样本的质量。同时，我们从ChatGPT的解释中获取了九个特征来Characterize假新闻，并分析这些特征在多个公共数据集中的分布。* Results: 我们的实验表明，ChatGPT在检测假新闻方面表现了可嘉的表现，但仍有改进的空间。我们还探讨了可能会增强其检测假新闻效果的额外信息。<details>
<summary>Abstract</summary>
The rampant spread of fake news has adversely affected society, resulting in extensive research on curbing its spread. As a notable milestone in large language models (LLMs), ChatGPT has gained significant attention due to its exceptional natural language processing capabilities. In this study, we present a thorough exploration of ChatGPT's proficiency in generating, explaining, and detecting fake news as follows. Generation -- We employ four prompt methods to generate fake news samples and prove the high quality of these samples through both self-assessment and human evaluation. Explanation -- We obtain nine features to characterize fake news based on ChatGPT's explanations and analyze the distribution of these factors across multiple public datasets. Detection -- We examine ChatGPT's capacity to identify fake news. We explore its detection consistency and then propose a reason-aware prompt method to improve its performance. Although our experiments demonstrate that ChatGPT shows commendable performance in detecting fake news, there is still room for its improvement. Consequently, we further probe into the potential extra information that could bolster its effectiveness in detecting fake news.
</details>
<details>
<summary>摘要</summary>
《假新闻的普遍传播对社会造成了不良影响，导致了各方对其散布的研究。作为大型自然语言模型（LLM）的一项重要里程碑，ChatGPT在自然语言处理方面表现出了突出的能力。本研究中，我们对ChatGPT的能力进行了全面探索，具体来说是：生成、解释和检测假新闻。生成——我们使用四种提示方法生成假新闻样本，并通过自我评估和人类评估来证明这些样本的质量。解释——我们从ChatGPT的解释中提取了九个特征来 caracterize假新闻，并分析这些特征在多个公共数据集中的分布。检测——我们检查ChatGPT是否能够识别假新闻。我们首先检查其检测的一致性，然后提出了基于理由的提示方法来提高其性能。虽然我们的实验表明ChatGPT在检测假新闻方面表现出了良好的表现，但还有一些可以提高其效果的空间。因此，我们进一步探索可能会增强其检测假新闻的效iveness的额外信息。
</details></li>
</ul>
<hr>
<h2 id="Walking-Down-the-Memory-Maze-Beyond-Context-Limit-through-Interactive-Reading"><a href="#Walking-Down-the-Memory-Maze-Beyond-Context-Limit-through-Interactive-Reading" class="headerlink" title="Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"></a>Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05029">http://arxiv.org/abs/2310.05029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Howard Chen, Ramakanth Pasunuru, Jason Weston, Asli Celikyilmaz</li>
<li>for: 这篇论文的目的是提出一种新的长文理解方法，以解决现有的自注意机制受限的问题。</li>
<li>methods: 该方法基于论文自动浏览器，首先将长文处理成摘要节点树，然后根据查询提交，通过 iterative prompting 方式，论文模型在树上寻找相关信息，并在获得足够信息后提供答案。</li>
<li>results: 与基eline方法相比，该方法在长文问答任务上表现出色，并且可以增强解释性，通过在浏览过程中高亮相关的文本段落。<details>
<summary>Abstract</summary>
Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query.
</details>
<details>
<summary>摘要</summary>
We propose an alternative approach that treats the LLM as an interactive agent, allowing it to decide how to read the text through iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. When receiving a query, the model navigates this tree to search for relevant information and responds once it has gathered sufficient information.On long-text question answering tasks, our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. Additionally, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text, pinpointing the relevant text segments related to the query.
</details></li>
</ul>
<hr>
<h2 id="Synslator-An-Interactive-Machine-Translation-Tool-with-Online-Learning"><a href="#Synslator-An-Interactive-Machine-Translation-Tool-with-Online-Learning" class="headerlink" title="Synslator: An Interactive Machine Translation Tool with Online Learning"></a>Synslator: An Interactive Machine Translation Tool with Online Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05025">http://arxiv.org/abs/2310.05025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayi Wang, Ke Wang, Fengming Zhou, Chengyu Wang, Zhiyong Fu, Zeyu Feng, Yu Zhao, Yuqi Zhang</li>
<li>for: 这篇论文旨在描述一种名为Synslator的计算机助记翻译工具，该工具不仅支持互动翻译（IMT），而且可以在线学习并使用实时翻译记忆。</li>
<li>methods: 该工具使用两种不同的神经翻译模型来处理翻译记忆，以适应不同的部署环境。此外，系统还使用语言模型来提高互动模式下的翻译流畅性。</li>
<li>results: 我们经过评估，确认了在线学习过程中的翻译模型的有效性，并发现使用Synslator的互动功能可以提高翻译效率13%。更多细节可以参考：<a target="_blank" rel="noopener" href="https://youtu.be/K0vRsb2lTt8%E3%80%82">https://youtu.be/K0vRsb2lTt8。</a><details>
<summary>Abstract</summary>
Interactive machine translation (IMT) has emerged as a progression of the computer-aided translation paradigm, where the machine translation system and the human translator collaborate to produce high-quality translations. This paper introduces Synslator, a user-friendly computer-aided translation (CAT) tool that not only supports IMT, but is adept at online learning with real-time translation memories. To accommodate various deployment environments for CAT services, Synslator integrates two different neural translation models to handle translation memories for online learning. Additionally, the system employs a language model to enhance the fluency of translations in an interactive mode. In evaluation, we have confirmed the effectiveness of online learning through the translation models, and have observed a 13% increase in post-editing efficiency with the interactive functionalities of Synslator. A tutorial video is available at:https://youtu.be/K0vRsb2lTt8.
</details>
<details>
<summary>摘要</summary>
协助式机器翻译（IMT）已经成为计算机辅助翻译模式的进化，在这种模式下，机器翻译系统和人类翻译员共同努力以生成高质量翻译。这篇文章介绍了Synslator，一款用户友好的计算机辅助翻译（CAT）工具，不仅支持IMT，而且在线学习 WITH 实时翻译记忆。为满足不同的CAT服务部署环境，Synslator integrate了两种不同的神经翻译模型来处理翻译记忆。此外，系统还使用语言模型来提高交互模式下的翻译流畅性。经评估，我们已经确认了在线学习通过翻译模型的效iveness，并观察到了Synslator的交互功能可以提高翻译效率13%。有关教程视频，请参考：https://youtu.be/K0vRsb2lTt8。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Quantum-Classical-Machine-Learning-for-Sentiment-Analysis"><a href="#Hybrid-Quantum-Classical-Machine-Learning-for-Sentiment-Analysis" class="headerlink" title="Hybrid Quantum-Classical Machine Learning for Sentiment Analysis"></a>Hybrid Quantum-Classical Machine Learning for Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10672">http://arxiv.org/abs/2310.10672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abu Kaisar Mohammad Masum, Anshul Maurya, Dhruthi Sridhar Murthy, Pratibha, Naveed Mahmud</li>
<li>for: 本研究旨在探讨量子计算和经典机器学习的合作在自然语言处理中的可能性，尤其是对大规模数据集中表达的人类情感和意见的情感分析。</li>
<li>methods: 本研究提出了一种混合量子-经典机器学习算法的方法ología，包括量子kernel方法和量子径波变换-基于的分类器，并与经典维度减少技术 such as PCA和Haar wavelet transform进行了集成。</li>
<li>results: 实验结果表明，在减少数据维度后，量子基于的混合算法的性能是稳定和更好于经典方法。<details>
<summary>Abstract</summary>
The collaboration between quantum computing and classical machine learning offers potential advantages in natural language processing, particularly in the sentiment analysis of human emotions and opinions expressed in large-scale datasets. In this work, we propose a methodology for sentiment analysis using hybrid quantum-classical machine learning algorithms. We investigate quantum kernel approaches and variational quantum circuit-based classifiers and integrate them with classical dimension reduction techniques such as PCA and Haar wavelet transform. The proposed methodology is evaluated using two distinct datasets, based on English and Bengali languages. Experimental results show that after dimensionality reduction of the data, performance of the quantum-based hybrid algorithms were consistent and better than classical methods.
</details>
<details>
<summary>摘要</summary>
合作 между量子计算和类别机器学习可以在自然语言处理中提供potential的优势，特别是在大规模数据集中检测人们的情感和意见。在这个工作中，我们提议了一种基于量子-类别机器学习算法的情感分析方法。我们研究了量子kernel方法和量子征值回归-基于分类器，并将其与经典维度减少技术相结合，如PCA和Haar波lets变换。我们对两个不同的数据集进行了实验，一个是英语数据集，另一个是孟加拉语数据集。实验结果表明，在减少数据维度后，量子-基于 hybrid 算法的性能是一致的和更好于经典方法。
</details></li>
</ul>
<hr>
<h2 id="WikiIns-A-High-Quality-Dataset-for-Controlled-Text-Editing-by-Natural-Language-Instruction"><a href="#WikiIns-A-High-Quality-Dataset-for-Controlled-Text-Editing-by-Natural-Language-Instruction" class="headerlink" title="WikiIns: A High-Quality Dataset for Controlled Text Editing by Natural Language Instruction"></a>WikiIns: A High-Quality Dataset for Controlled Text Editing by Natural Language Instruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05009">http://arxiv.org/abs/2310.05009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/casparswift/wikiins">https://github.com/casparswift/wikiins</a></li>
<li>paper_authors: Xiang Chen, Zheng Li, Xiaojun Wan</li>
<li>for: 本研究targets the problem of controlled text editing by natural language instruction.</li>
<li>methods: 研究者使用了Wikipedia编辑历史数据库，通过批处理和人工纠正来提高数据集的质量，并提出了自动生成大规模“银”训练集的方法。</li>
<li>results: 研究者通过对WikiIns dataset进行分析和实验，得到了一些有价值的结论和编辑INTENTION分析结果。<details>
<summary>Abstract</summary>
Text editing, i.e., the process of modifying or manipulating text, is a crucial step in human writing process. In this paper, we study the problem of controlled text editing by natural language instruction. According to a given instruction that conveys the edit intention and necessary information, an original draft text is required to be revised into a target text. Existing automatically constructed datasets for this task are limited because they do not have informative natural language instruction. The informativeness requires the information contained in the instruction to be enough to produce the revised text. To address this limitation, we build and release WikiIns, a high-quality controlled text editing dataset with improved informativeness. We first preprocess the Wikipedia edit history database to extract the raw data (WikiIns-Raw). Then we crowdsource high-quality validation and test sets, as well as a small-scale training set (WikiIns-Gold). With the high-quality annotated dataset, we further propose automatic approaches to generate a large-scale ``silver'' training set (WikiIns-Silver). Finally, we provide some insightful analysis on our WikiIns dataset, including the evaluation results and the edit intention analysis. Our analysis and the experiment results on WikiIns may assist the ongoing research on text editing. The dataset, source code and annotation guideline are available at https://github.com/CasparSwift/WikiIns.
</details>
<details>
<summary>摘要</summary>
文本编辑，即对文本进行修改或 manipulate 的过程，是人类写作过程中的关键步骤。在这篇论文中，我们研究了基于自然语言指令的控制文本编辑问题。根据一个拥有修改意图和必要信息的自然语言指令，需要将原始稿件文本修改为目标文本。现有的自动生成的这类数据集有限，因为它们没有具有信息的自然语言指令。为了解决这个限制，我们建立了和发布了高质量的控制文本编辑数据集 WikiIns，其中包括改进的信息含量。我们首先从 Wikipedia 编辑历史数据库中提取原始数据（WikiIns-Raw），然后通过人工审核和测试集，以及一小规模的训练集（WikiIns-Gold）来生成高质量验证集。然后，我们提出了一些自动生成大规模“银”训练集（WikiIns-Silver）的方法。最后，我们提供了一些有价值的分析和实验结果，包括我们的 WikiIns 数据集的评价结果和修改意图分析。我们的分析和实验结果可能会帮助当前的文本编辑研究。我们的数据集、源代码和注释指南可以在 GitHub 上找到：https://github.com/CasparSwift/WikiIns。
</details></li>
</ul>
<hr>
<h2 id="MinPrompt-Graph-based-Minimal-Prompt-Data-Augmentation-for-Few-shot-Question-Answering"><a href="#MinPrompt-Graph-based-Minimal-Prompt-Data-Augmentation-for-Few-shot-Question-Answering" class="headerlink" title="MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering"></a>MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05007">http://arxiv.org/abs/2310.05007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiusi Chen, Jyun-Yu Jiang, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Wei Wang</li>
<li>for: 提高机器问答系统的满意度，使其在几个训练样本不足的情况下达到良好的结果。</li>
<li>methods: 提出了一种基于approximate graph算法和无监督问题生成的最小数据扩充框架，可以有效地提高open-domain QA任务中的精度。</li>
<li>results: 经验result表明，MinPrompt能够与基eline相比或者更好地实现精度，在不同的benchmark datasets上提高F-1分数的提升达27.5%。<details>
<summary>Abstract</summary>
Few-shot question answering (QA) aims at achieving satisfactory results on machine question answering when only a few training samples are available. Recent advances mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present MinPrompt, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that MinPrompt is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing improvements in F-1 scores by up to 27.5%.
</details>
<details>
<summary>摘要</summary>
几个示例问答（QA）目标在机器问答中实现满意的结果，只需要几个训练样本。现代进步主要依靠大型自然语言模型（LLM）的力量和特定设置的精细调整。虽然预训练阶段已经把LLM们具备了强大的推理能力，但LLM们仍需要调整以适应特定领域以达到最佳结果。在这篇论文中，我们提议选择最有用的数据进行调整，从而提高调整过程的效率，同时保持比较或更好的准确率在开放领域QA任务中。我们提出了一个名为MinPrompt的最小数据扩展框架，基于approximate graph算法和无监督问题生成。我们将原始文本转换成图结构，建立不同事实句子之间的连接，然后应用图算法选择最小的句子集，以覆盖raw文本中的最多信息。我们然后根据选择的句子集生成QA对，并在选择的句子上训练模型，从而获得最终模型。实验结果表明，MinPrompt可以与基准相比或更好的达到准确率，提高F-1分数的提升达27.5%。
</details></li>
</ul>
<hr>
<h2 id="Self-Knowledge-Guided-Retrieval-Augmentation-for-Large-Language-Models"><a href="#Self-Knowledge-Guided-Retrieval-Augmentation-for-Large-Language-Models" class="headerlink" title="Self-Knowledge Guided Retrieval Augmentation for Large Language Models"></a>Self-Knowledge Guided Retrieval Augmentation for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05002">http://arxiv.org/abs/2310.05002</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THUNLP-MT/SKR">https://github.com/THUNLP-MT/SKR</a></li>
<li>paper_authors: Yile Wang, Peng Li, Maosong Sun, Yang Liu</li>
<li>for: 提高大语言模型（LLM）的性能，不需要任务特定的精度调整。</li>
<li>methods: 使用自我认知指导的检索增强（SKR）方法，让 LLM 能够识别自己所知道和所不知道，并适应新问题。</li>
<li>results: SKR 在多个数据集上表现出色，比 chain-of-thought 和完整检索基本方法高效，使用 InstructGPT 或 ChatGPT 进行评估。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer non-parametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally. To better make use of both internal knowledge and external world knowledge, we investigate eliciting the model's ability to recognize what they know and do not know (which is also called self-knowledge) and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions. We evaluate SKR on multiple datasets and demonstrate that it outperforms chain-of-thought based and fully retrieval-based methods by using either InstructGPT or ChatGPT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TopicAdapt-An-Inter-Corpora-Topics-Adaptation-Approach"><a href="#TopicAdapt-An-Inter-Corpora-Topics-Adaptation-Approach" class="headerlink" title="TopicAdapt- An Inter-Corpora Topics Adaptation Approach"></a>TopicAdapt- An Inter-Corpora Topics Adaptation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04978">http://arxiv.org/abs/2310.04978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pritom Saha Akash, Trisha Das, Kevin Chen-Chuan Chang</li>
<li>for: 本研究提出了一种基于神经网络的话题模型，用于改进话题模型在实际场景中的表现。</li>
<li>methods: 本研究使用了一种基于神经网络的话题模型，可以从相关的源корpus中挖掘有用的话题，同时还可以在目标корpus中找到缺失的话题。</li>
<li>results: 实验结果表明，提出的话题模型在多个不同领域的数据集上具有较高的表现，比对state-of-the-art话题模型更好。<details>
<summary>Abstract</summary>
Topic models are popular statistical tools for detecting latent semantic topics in a text corpus. They have been utilized in various applications across different fields. However, traditional topic models have some limitations, including insensitivity to user guidance, sensitivity to the amount and quality of data, and the inability to adapt learned topics from one corpus to another. To address these challenges, this paper proposes a neural topic model, TopicAdapt, that can adapt relevant topics from a related source corpus and also discover new topics in a target corpus that are absent in the source corpus. The proposed model offers a promising approach to improve topic modeling performance in practical scenarios. Experiments over multiple datasets from diverse domains show the superiority of the proposed model against the state-of-the-art topic models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-the-Usage-of-Chinese-Pinyin-in-Pretraining"><a href="#Exploring-the-Usage-of-Chinese-Pinyin-in-Pretraining" class="headerlink" title="Exploring the Usage of Chinese Pinyin in Pretraining"></a>Exploring the Usage of Chinese Pinyin in Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04960">http://arxiv.org/abs/2310.04960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baojun Wang, Kun Xu, Lifeng Shang</li>
<li>for: 这篇论文主要是为了提高中文语音识别错误稳定性。</li>
<li>methods: 这篇论文使用了多种预训练方法，包括使用字符和拼音并行预训练，以增强错误识别的稳定性。</li>
<li>results: 实验结果表明，这种新预训练方法可以提高中文语音识别模型的稳定性，并且在公共错误纠正数据集上达到了最高的表现。<details>
<summary>Abstract</summary>
Unlike alphabetic languages, Chinese spelling and pronunciation are different. Both characters and pinyin take an important role in Chinese language understanding. In Chinese NLP tasks, we almost adopt characters or words as model input, and few works study how to use pinyin. However, pinyin is essential in many scenarios, such as error correction and fault tolerance for ASR-introduced errors. Most of these errors are caused by the same or similar pronunciation words, and we refer to this type of error as SSP(the same or similar pronunciation) errors for short. In this work, we explore various ways of using pinyin in pretraining models and propose a new pretraining method called PmBERT. Our method uses characters and pinyin in parallel for pretraining. Through delicate pretraining tasks, the characters and pinyin representation are fused, which can enhance the error tolerance for SSP errors. We do comprehensive experiments and ablation tests to explore what makes a robust phonetic enhanced Chinese language model. The experimental results on both the constructed noise-added dataset and the public error-correction dataset demonstrate that our model is more robust compared to SOTA models.
</details>
<details>
<summary>摘要</summary>
不同的字母语言和中文拼写、发音之间存在差异。中文NLU任务中，大多数作品是直接使用字符或词作为模型输入，而忽略了拼音。然而，拼音在许多场景中具有重要性，如错误纠正和ASR引入错误的稳定性。大多数这些错误是由同或相似的发音单词引起的，我们称这种错误为SSP（同或相似的发音）错误。在这种工作中，我们探索了使用拼音的不同方法，并提出了一种新的预训练方法called PmBERT。我们的方法在平行预训练中使用字符和拼音，通过细腻的预训练任务，字符和拼音表示被融合，从而提高了SSP错误的承受能力。我们进行了广泛的实验和割除测试，以探索使一个强大的中文语言模型具有哪些特点。实验结果表明，我们的模型在constructed noise-added dataset和公共错误纠正dataset上比SOTA模型更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Towards-Better-Chain-of-Thought-Prompting-Strategies-A-Survey"><a href="#Towards-Better-Chain-of-Thought-Prompting-Strategies-A-Survey" class="headerlink" title="Towards Better Chain-of-Thought Prompting Strategies: A Survey"></a>Towards Better Chain-of-Thought Prompting Strategies: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04959">http://arxiv.org/abs/2310.04959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, Jiajun Chen</li>
<li>for: 本文旨在探讨Chain-of-Thought（CoT）提示Strategy的效果，并系统地分析其关键因素以及如何更好地应用于不同应用场景。</li>
<li>methods: 本文通过审查广泛的当前研究，提供了系统的和全面的分析，涵盖了CoT提示的各种因素的影响，以及如何更好地应用其在不同应用场景。</li>
<li>results: 本文提出了一些挑战和未来发展方向，以帮助读者更好地理解和应用CoT提示。<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT), a step-wise and coherent reasoning chain, shows its impressive strength when used as a prompting strategy for large language models (LLM). Recent years, the prominent effect of CoT prompting has attracted emerging research. However, there still lacks of a systematic summary about key factors of CoT prompting and comprehensive guide for prompts utilizing. For a deeper understanding about CoT prompting, we survey on a wide range of current research, presenting a systematic and comprehensive analysis on several factors that may influence the effect of CoT prompting, and introduce how to better apply it in different applications under these discussions. We further analyze the challenges and propose some future directions about CoT prompting. This survey could provide an overall reference on related research.
</details>
<details>
<summary>摘要</summary>
Chain-of-Thought（CoT），一种逐步逻辑推理链，在大语言模型（LLM）中作为提示策略显示出了惊人的力量。近年来，CoT提示的明显效果吸引了学术界的关注。然而，当前还缺乏一个系统化的总结和完整的指南，用于解释CoT提示的关键因素和如何更好地应用它们。为了深入了解CoT提示，我们在广泛的当前研究中进行了系统化和完整的分析，并对各种因素的影响进行了分析，以及如何在不同应用中更好地使用它们。我们还分析了挑战和提出了未来的发展方向。这种调查可以为相关研究提供一个总体参考。Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Domain-Knowledge-Graph-Construction-Via-A-Simple-Checker"><a href="#Domain-Knowledge-Graph-Construction-Via-A-Simple-Checker" class="headerlink" title="Domain Knowledge Graph Construction Via A Simple Checker"></a>Domain Knowledge Graph Construction Via A Simple Checker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04949">http://arxiv.org/abs/2310.04949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueling Zeng, Li-C. Wang</li>
<li>for: 这项研究的目的是为Semiconductor chip设计公司提供一种基于语言模型的知识图构建方法，以满足公司的两个重要考虑因素：保密性和可扩展性。</li>
<li>methods: 本文提出了一种oracle-checker方法，利用GPT3.5的力量来解决知识图构建问题。该方法包括一个验证过程，用于检查域专家的背景知识是否已经满足了构建知识图的需求。</li>
<li>results: 本文使用RISC-V无权ISA规范为例，解释了关键想法和讨论了实践中的 oracle-checker方法的可能性。<details>
<summary>Abstract</summary>
With the availability of large language models, there is a growing interest for semiconductor chip design companies to leverage the technologies. For those companies, deployment of a new methodology must include two important considerations: confidentiality and scalability. In this context, this work tackles the problem of knowledge graph construction from hardware-design domain texts. We propose an oracle-checker scheme to leverage the power of GPT3.5 and demonstrate that the essence of the problem is in distillation of domain expert's background knowledge. Using RISC-V unprivileged ISA specification as an example, we explain key ideas and discuss practicality of our proposed oracle-checker approach.
</details>
<details>
<summary>摘要</summary>
现在大型语言模型成为可用的，半导体封包设计公司开始关注这些技术的应用。为这些公司而办理新方法时，需要考虑两个重要因素：保密和可扩展性。在这个上下文中，本文解决半导体设计领域文本知识图构建的问题。我们提议使用GPT3.5的力量，并证明知识的核心问题在封包专家背景知识的精炼中。使用RISC-V不具有特权ISA规范为例，我们介绍关键想法并讨论我们的 oracle-checker方法的实用性。
</details></li>
</ul>
<hr>
<h2 id="TEMPO-Prompt-based-Generative-Pre-trained-Transformer-for-Time-Series-Forecasting"><a href="#TEMPO-Prompt-based-Generative-Pre-trained-Transformer-for-Time-Series-Forecasting" class="headerlink" title="TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting"></a>TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04948">http://arxiv.org/abs/2310.04948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, Yan Liu</li>
<li>for: 本研究旨在开发一种新的时间序列表示学习框架，以提高时间序列预测的准确性。</li>
<li>methods: 该框架基于两个关键的强制性理念：（一）分解复杂的时间序列任务中的趋势、季度和差异部分的交互作用；以及（二）通过选择性的提示来促进非站点时间序列的分布适应。</li>
<li>results: 对多个时间序列benchmark datasets进行实验，TEMPO模型表现出了与现有方法相比的显著性能提升，不仅在标准的指导学习 Setting中，而且在未经见过数据集和多模式输入的情况下也能够获得出色的表现。这一结果表明TEMPO具有成为基础模型构建框架的潜力。<details>
<summary>Abstract</summary>
The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-stationary time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on a number of time series benchmark datasets. This performance gain is observed not only in standard supervised learning settings but also in scenarios involving previously unseen datasets as well as in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.
</details>
<details>
<summary>摘要</summary>
过去一个 décennie  witnessed significant advances in time series modeling with deep learning. Although the best-performing architectures vary greatly across applications and domains, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance by training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements.In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposing the complex interaction between trend, seasonal, and residual components; and (ii) introducing selection-based prompts to facilitate distribution adaptation in non-stationary time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on a number of time series benchmark datasets. This performance gain is observed not only in standard supervised learning settings but also in scenarios involving previously unseen datasets as well as in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/cs.CL_2023_10_08/" data-id="clombedqx00c0s0888uj6fkra" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/08/cs.LG_2023_10_08/" class="article-date">
  <time datetime="2023-10-08T10:00:00.000Z" itemprop="datePublished">2023-10-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/08/cs.LG_2023_10_08/">cs.LG - 2023-10-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Adversarial-Attacks-on-Combinatorial-Multi-Armed-Bandits"><a href="#Adversarial-Attacks-on-Combinatorial-Multi-Armed-Bandits" class="headerlink" title="Adversarial Attacks on Combinatorial Multi-Armed Bandits"></a>Adversarial Attacks on Combinatorial Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05308">http://arxiv.org/abs/2310.05308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishab Balasubramanian, Jiawei Li, Prasad Tadepalli, Huazheng Wang, Qingyun Wu, Haoyu Zhao</li>
<li>for: 这 paper 研究了对 Combinatorial Multi-armed Bandits (CMAB) 的奖伪攻击。</li>
<li>methods: 这 paper 提供了一个 suficient 和 necessary condition for the attackability of CMAB, 以及一个攻击算法 для可攻击的 CMAB 实例。</li>
<li>results: 这 paper 发现了一个意外的事实，即攻击 CMAB 实例的可能性还取决于敌方知道或不知道该实例的环境。这意味着在实际应用中，对 CMAB 的攻击非常困难，并且无法找到一个通用的攻击策略。这paper 通过实验 validate 了这些理论发现。<details>
<summary>Abstract</summary>
We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, which depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanning tree, cascading bandits for online ranking, and online shortest path.
</details>
<details>
<summary>摘要</summary>
我们研究了对 combinatorial multi-armed bandit (CMAB) 的奖励毒攻击。我们首先提供了 CMAB 的攻击可行性必要和 suficient condition，这取决于 CMAB 实例中的奖励分布和结果分布。此外，我们还设计了一种攻击算法 для可攻击 CMAB 实例。与先前对多重抓拍机器人的理解不同，我们发现了一个意外的事实，即 CMAB 实例的攻击可行性还取决于敌方知道或不知道 CMAB 实例的情况。这一发现表明了在实践中对 CMAB 进行攻击是困难的，并且没有一个通用的攻击策略可以应用于任何 CMAB 实例，因为环境多数是不知道的。我们验证了我们的理论发现Result via 广泛的实验，包括probabilistic maximum covering problem、online minimum spanning tree、cascading bandits for online ranking和online shortest path。
</details></li>
</ul>
<hr>
<h2 id="Successive-Data-Injection-in-Conditional-Quantum-GAN-Applied-to-Time-Series-Anomaly-Detection"><a href="#Successive-Data-Injection-in-Conditional-Quantum-GAN-Applied-to-Time-Series-Anomaly-Detection" class="headerlink" title="Successive Data Injection in Conditional Quantum GAN Applied to Time Series Anomaly Detection"></a>Successive Data Injection in Conditional Quantum GAN Applied to Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05307">http://arxiv.org/abs/2310.05307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Kalfon, Soumaya Cherkaoui, Jean-Frédéric Laprade, Ola Ahmad, Shengrui Wang</li>
<li>for: 这个论文主要针对的是如何使用量子生成器网络（QGAN）进行异常检测，尤其是在通信网络中采集的时间序列数据上。</li>
<li>methods: 这篇论文提出了一种新的高维编码方法，named Successive Data Injection（SuDaI），以便在量子状态中扩展更大的数据空间，从而适应更高维的时间序列数据。</li>
<li>results: 该方法可以在高维时间序列数据上进行异常检测，并且可以在其他类型的高维时间序列数据上应用，因此开 up了多个应用领域。<details>
<summary>Abstract</summary>
Classical GAN architectures have shown interesting results for solving anomaly detection problems in general and for time series anomalies in particular, such as those arising in communication networks. In recent years, several quantum GAN architectures have been proposed in the literature. When detecting anomalies in time series using QGANs, huge challenges arise due to the limited number of qubits compared to the size of the data. To address these challenges, we propose a new high-dimensional encoding approach, named Successive Data Injection (SuDaI). In this approach, we explore a larger portion of the quantum state than that in the conventional angle encoding, the method used predominantly in the literature, through repeated data injections into the quantum state. SuDaI encoding allows us to adapt the QGAN for anomaly detection with network data of a much higher dimensionality than with the existing known QGANs implementations. In addition, SuDaI encoding applies to other types of high-dimensional time series and can be used in contexts beyond anomaly detection and QGANs, opening up therefore multiple fields of application.
</details>
<details>
<summary>摘要</summary>
传统的GAN架构在检测异常问题上有诸多有趣的结果，特别是在通信网络中出现的时间序列异常问题。在过去几年，一些量子GAN架构在 литературе中被提出。在使用QGAN检测时间序列异常时，面临着很大的挑战，主要是因为量子状态的限制比数据集大得多。为解决这些挑战，我们提出了一种新的高维码编码方法，名为Successive Data Injection（SuDaI）。在SuDaI编码中，我们可以更好地探索量子状态的更大部分，而不是在文献中主要使用的角度编码方法。这使得我们可以通过重复数据注入到量子状态来适应QGAN检测高维时间序列数据的问题。此外，SuDaI编码还适用于其他类型的高维时间序列和不同的应用场景，因此开放了多个应用领域。
</details></li>
</ul>
<hr>
<h2 id="Clustering-Three-Way-Data-with-Outliers"><a href="#Clustering-Three-Way-Data-with-Outliers" class="headerlink" title="Clustering Three-Way Data with Outliers"></a>Clustering Three-Way Data with Outliers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05288">http://arxiv.org/abs/2310.05288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharine M. Clark, Paul D. McNicholas</li>
<li>for:  clustering matrix-variate normal data with outliers</li>
<li>methods: 使用分布subset log-likelihoods， extends OCLUST algorithm to matrix-variate normal data，使用迭代方法检测和剔除异常点</li>
<li>results: 可以有效地检测和剔除matrix-variate normal data中的异常点<details>
<summary>Abstract</summary>
Matrix-variate distributions are a recent addition to the model-based clustering field, thereby making it possible to analyze data in matrix form with complex structure such as images and time series. Due to its recent appearance, there is limited literature on matrix-variate data, with even less on dealing with outliers in these models. An approach for clustering matrix-variate normal data with outliers is discussed. The approach, which uses the distribution of subset log-likelihoods, extends the OCLUST algorithm to matrix-variate normal data and uses an iterative approach to detect and trim outliers.
</details>
<details>
<summary>摘要</summary>
矩阵变量分布是现代模型基 clustering 领域的新添加，可以处理矩阵数据形式的复杂结构，如图像和时间序列。由于其新的出现，关于矩阵变量数据的文献非常有限，甚至更少关于处理异常值在这些模型中。一种用于矩阵变量正态数据 clustering 和异常值排除的方法被讨论。该方法基于分布subset log-likelihood的分布，对矩阵变量数据进行了扩展，并使用迭代法排除异常值。
</details></li>
</ul>
<hr>
<h2 id="Learning-force-laws-in-many-body-systems"><a href="#Learning-force-laws-in-many-body-systems" class="headerlink" title="Learning force laws in many-body systems"></a>Learning force laws in many-body systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05273">http://arxiv.org/abs/2310.05273</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyu54/many-body-force-infer">https://github.com/wyu54/many-body-force-infer</a></li>
<li>paper_authors: Wentao Yu, Eslam Abdelaleem, Ilya Nemenman, Justin C. Burton</li>
<li>for: The paper is written to demonstrate a machine learning (ML) approach for discovering force laws in dusty plasma experiments.</li>
<li>methods: The paper uses 3D particle trajectories to train an ML model that incorporates physical intuition to infer the effective non-reciprocal forces between particles, accounting for inherent symmetries and non-identical particles.</li>
<li>results: The model accurately learns the force laws and extracts each particle’s mass and charge, with an accuracy of R^2 &gt; 0.99, indicating new physics in dusty plasma beyond the resolution of current theories and demonstrating the potential of ML-powered approaches for guiding new routes of scientific discovery in many-body systems.Here’s the same information in Simplified Chinese text:</li>
<li>for: 该文章用于演示一种基于机器学习（ML）的方法，用于在尘晶体实验中发现力法律。</li>
<li>methods: 该文章使用3D particulate轨迹来训练一个ML模型，该模型具有物理直觉，以推导粒子之间的有效非对称力，并考虑粒子之间的自旋Symmetry和不同的粒子。</li>
<li>results: 模型具有R^2&gt;0.99的准确性，表明尘晶体中存在跟当前理论不同的新物理现象，并证明ML能力可以导引科学发现的新路径。<details>
<summary>Abstract</summary>
Scientific laws describing natural systems may be more complex than our intuition can handle, and thus how we discover laws must change. Machine learning (ML) models can analyze large quantities of data, but their structure should match the underlying physical constraints to provide useful insight. Here we demonstrate a ML approach that incorporates such physical intuition to infer force laws in dusty plasma experiments. Trained on 3D particle trajectories, the model accounts for inherent symmetries and non-identical particles, accurately learns the effective non-reciprocal forces between particles, and extracts each particle's mass and charge. The model's accuracy (R^2 > 0.99) points to new physics in dusty plasma beyond the resolution of current theories and demonstrates how ML-powered approaches can guide new routes of scientific discovery in many-body systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Simplifying-GNN-Performance-with-Low-Rank-Kernel-Models"><a href="#Simplifying-GNN-Performance-with-Low-Rank-Kernel-Models" class="headerlink" title="Simplifying GNN Performance with Low Rank Kernel Models"></a>Simplifying GNN Performance with Low Rank Kernel Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05250">http://arxiv.org/abs/2310.05250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucianoavinas/lowrank-gnn-kernels">https://github.com/lucianoavinas/lowrank-gnn-kernels</a></li>
<li>paper_authors: Luciano Vinas, Arash A. Amini</li>
<li>for: 本研究 revisits recent spectral GNN approaches to semi-supervised node classification (SSNC), 提出许多现代GNN架构可能过度设计。</li>
<li>methods: 研究使用非 Parametric estimation 技术在 spectral 频谱中应用，代替许多深度学习引用 GNN 设计。这些传统技术适用于各种图类型，达到了许多常见 SSNC benchmark 的状态 arts 性能。</li>
<li>results: 研究表明，近期 GNN 方法的性能改进部分归功于评估方法的转变。此外，对 GNN  спектраль滤波技术的各种超参数进行了ablative study。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/lucianoAvinas/lowrank-gnn-kernels">https://github.com/lucianoAvinas/lowrank-gnn-kernels</a> 找到。<details>
<summary>Abstract</summary>
We revisit recent spectral GNN approaches to semi-supervised node classification (SSNC). We posit that many of the current GNN architectures may be over-engineered. Instead, simpler, traditional methods from nonparametric estimation, applied in the spectral domain, could replace many deep-learning inspired GNN designs. These conventional techniques appear to be well suited for a variety of graph types reaching state-of-the-art performance on many of the common SSNC benchmarks. Additionally, we show that recent performance improvements in GNN approaches may be partially attributed to shifts in evaluation conventions. Lastly, an ablative study is conducted on the various hyperparameters associated with GNN spectral filtering techniques. Code available at: https://github.com/lucianoAvinas/lowrank-gnn-kernels
</details>
<details>
<summary>摘要</summary>
我们回到最近的спектルールGraph Neural Network（GNN）方法，用于半supervised node classification（SSNC）。我们认为许多现有的GNN架构可能是过工程。相反，更简单的传统方法，应用于spectral domain，可以取代许多深度学习灵感的GNN设计。这些传统技术适用于多种граф型，可以达到多数常见的SSNC benchmark中的state-of-the-art表现。此外，我们显示出最近GNN方法的性能提升部分可以归因于评估惯例的变化。最后，我们进行了GNNspectral filtering技术各种参数的ablative study。code可以在以下github上取得：https://github.com/lucianoAvinas/lowrank-gnn-kernels。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Kernel-Flexibility-via-Learning-Asymmetric-Locally-Adaptive-Kernels"><a href="#Enhancing-Kernel-Flexibility-via-Learning-Asymmetric-Locally-Adaptive-Kernels" class="headerlink" title="Enhancing Kernel Flexibility via Learning Asymmetric Locally-Adaptive Kernels"></a>Enhancing Kernel Flexibility via Learning Asymmetric Locally-Adaptive Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05236">http://arxiv.org/abs/2310.05236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hefansjtu/labrbf_kernel">https://github.com/hefansjtu/labrbf_kernel</a></li>
<li>paper_authors: Fan He, Mingzhen He, Lei Shi, Xiaolin Huang, Johan A. K. Suykens</li>
<li>for: 这篇论文的目的是提高基域学习的灵活性，通过使用可训练的本地适应宽度（LAB）来增强径basis函数（RBF）kernels。</li>
<li>methods: 这篇论文提出了一种新的非对称基域函数（Asymmetric Kernel Ridge Regression，AKRR）框架，并引入了一种循环基域学习算法来训练可训练的本地适应宽度。</li>
<li>results: 实验结果表明，提出的方法可以在实际 dataset 上达到remarkable的性能，比 Nystr&quot;om approximation-based algorithms 更具有扩展性，并且在基域学习方法中显示出较高的准确率，甚至超过了 residual neural networks。<details>
<summary>Abstract</summary>
The lack of sufficient flexibility is the key bottleneck of kernel-based learning that relies on manually designed, pre-given, and non-trainable kernels. To enhance kernel flexibility, this paper introduces the concept of Locally-Adaptive-Bandwidths (LAB) as trainable parameters to enhance the Radial Basis Function (RBF) kernel, giving rise to the LAB RBF kernel. The parameters in LAB RBF kernels are data-dependent, and its number can increase with the dataset, allowing for better adaptation to diverse data patterns and enhancing the flexibility of the learned function. This newfound flexibility also brings challenges, particularly with regards to asymmetry and the need for an efficient learning algorithm. To address these challenges, this paper for the first time establishes an asymmetric kernel ridge regression framework and introduces an iterative kernel learning algorithm. This novel approach not only reduces the demand for extensive support data but also significantly improves generalization by training bandwidths on the available training data. Experimental results on real datasets underscore the remarkable performance of the proposed algorithm, showcasing its superior capability in handling large-scale datasets compared to Nystr\"om approximation-based algorithms. Moreover, it demonstrates a significant improvement in regression accuracy over existing kernel-based learning methods and even surpasses residual neural networks.
</details>
<details>
<summary>摘要</summary>
文中提到的主要瓶须是基于手动设计、预给定、不可学习的kernels的学习系统的缺乏足够的灵活性。为了增强kernel的灵活性，这篇论文引入了Locally-Adaptive-Bandwidths（LAB）作为可学习参数，从而改进了基于卷积函数（RBF）kernel，得到LAB RBF kernel。这些参数随着数据的变化而变化，数量可以随着数据集的增加而增加，以适应多样化的数据模式，从而提高学习的灵活性。然而，这种新的灵活性也带来了挑战，特别是偏 asymmetry和有效的学习算法的需求。为了解决这些挑战，这篇论文首次提出了一种偏 asymmetric kernel ridge regression框架，并引入了一种迭代式 kernel learning算法。这种新的方法不仅可以减少了大量的支持数据，还可以很好地适应不同的数据模式，从而提高了泛化性。实验结果表明，提议的算法在实际数据上表现出色，比 Nystr\"om Approximation-based algorithms更好地处理大规模数据，并且超过了基于kernel的学习方法和 residual neural networks 的准确率。
</details></li>
</ul>
<hr>
<h2 id="Global-Convergence-of-Policy-Gradient-Methods-in-Reinforcement-Learning-Games-and-Control"><a href="#Global-Convergence-of-Policy-Gradient-Methods-in-Reinforcement-Learning-Games-and-Control" class="headerlink" title="Global Convergence of Policy Gradient Methods in Reinforcement Learning, Games and Control"></a>Global Convergence of Policy Gradient Methods in Reinforcement Learning, Games and Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05230">http://arxiv.org/abs/2310.05230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shicong Cen, Yuejie Chi</li>
<li>for: 政策梯度方法，用于Sequential Decision Making中寻找政策优化。</li>
<li>methods: 使用首选信息来最大化价值函数。</li>
<li>results: 最近的进展包括对政策梯度方法的全球最优性保证，以及对重要问题参数的finite-time收敛率。<details>
<summary>Abstract</summary>
Policy gradient methods, where one searches for the policy of interest by maximizing the value functions using first-order information, become increasingly popular for sequential decision making in reinforcement learning, games, and control. Guaranteeing the global optimality of policy gradient methods, however, is highly nontrivial due to nonconcavity of the value functions. In this exposition, we highlight recent progresses in understanding and developing policy gradient methods with global convergence guarantees, putting an emphasis on their finite-time convergence rates with regard to salient problem parameters.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accelerating-Machine-Learning-Primitives-on-Commodity-Hardware"><a href="#Accelerating-Machine-Learning-Primitives-on-Commodity-Hardware" class="headerlink" title="Accelerating Machine Learning Primitives on Commodity Hardware"></a>Accelerating Machine Learning Primitives on Commodity Hardware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05218">http://arxiv.org/abs/2310.05218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Snytsar</li>
<li>for: 这篇论文是用于探讨深度神经网络（DNN）中的滑动窗口卷积技术，并对其进行了广泛的研究和评估。</li>
<li>methods: 本论文使用了滑动窗口卷积技术来提高深度神经网络的训练和推理效率，并对其进行了广泛的研究和评估。</li>
<li>results: 研究结果表明，使用滑动窗口卷积技术可以减少内存占用和提高计算效率，并在CPU和专门设计的硬件加速器上实现显著的速度提升。这种技术可能会推动AI在低功耗和低内存设备上的广泛应用，无需特殊硬件。<details>
<summary>Abstract</summary>
Sliding Window Sum algorithms have been successfully used for training and inference of Deep Neural Networks. We have shown before how both pooling and convolution 1-D primitives could be expressed as sliding sums and evaluated by the compute kernels with a shared structure. In this paper, we present an extensive study of the Sliding Window convolution technique as a more efficient alternative to the commonly used General Matrix Multiplication (GEMM) based convolution in Deep Neural Networks (DNNs). The Sliding Window technique addresses the memory bloating problem and demonstrates a significant speedup in 2-D convolution. We explore the performance of this technique on a range of implementations, including custom kernels for specific filter sizes. Our results suggest that the Sliding Window computation kernels can outperform GEMM-based convolution on a CPU and even on dedicated hardware accelerators. This could promote a wider adoption of AI on low-power and low-memory devices without the need for specialized hardware. We also discuss the compatibility of model compression methods and optimized network architectures with the Sliding Window technique, encouraging further research in these areas.
</details>
<details>
<summary>摘要</summary>
Sliding Window Sum算法已经成功地应用于深度神经网络的训练和推理。我们之前已经证明了抽象和卷积1-D primitives可以表示为滑动和计算缓存的共享结构。在这篇论文中，我们进行了广泛的滑动窗口卷积技术的研究，作为深度神经网络中常用的普通矩阵乘法（GEMM）基于卷积的更有效的替代方案。滑动窗口技术解决了内存膨胀问题，并在2-D卷积中显示出了明显的速度提升。我们对不同的实现进行了探索，包括特定的缓存器大小的自定义kernels。我们的结果表明，滑动窗口计算kernels可以在CPU和专门的硬件加速器上超越GEMM-基于卷积。这可能会推动AI在低功耗和低内存设备上的更广泛应用，无需特殊硬件。我们还讨论了模型压缩方法和优化网络架构与滑动窗口技术的相容性，鼓励进一步的研究在这些领域。
</details></li>
</ul>
<hr>
<h2 id="Towards-Optimizing-with-Large-Language-Models"><a href="#Towards-Optimizing-with-Large-Language-Models" class="headerlink" title="Towards Optimizing with Large Language Models"></a>Towards Optimizing with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05204">http://arxiv.org/abs/2310.05204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Pei-Fu Guo, Ying-Hsuan Chen, Yun-Da Tsai, Shou-De Lin</li>
<li>for: 这个研究是为了评估LLMs在不同任务和数据大小下的优化能力。</li>
<li>methods: 这个研究使用了交互式提示法， LLMS需要在每个优化步骤中从过去生成的解决方案中生成新的解决方案，然后评估这些新的解决方案的值。研究者还引入了三种综合评估任务性能的指标，这些指标适用于评估LLM在各种优化任务中的表现，并且对测试样本的变化更加敏感。</li>
<li>results: 研究发现，当处理小样本时，LLMs表现出强大的优化能力，但是对数据大小和值的影响表明需要进一步研究LLM在优化任务中的表现。<details>
<summary>Abstract</summary>
In this work, we conduct an assessment of the optimization capabilities of LLMs across various tasks and data sizes. Each of these tasks corresponds to unique optimization domains, and LLMs are required to execute these tasks with interactive prompting. That is, in each optimization step, the LLM generates new solutions from the past generated solutions with their values, and then the new solutions are evaluated and considered in the next optimization step. Additionally, we introduce three distinct metrics for a comprehensive assessment of task performance from various perspectives. These metrics offer the advantage of being applicable for evaluating LLM performance across a broad spectrum of optimization tasks and are less sensitive to variations in test samples. By applying these metrics, we observe that LLMs exhibit strong optimization capabilities when dealing with small-sized samples. However, their performance is significantly influenced by factors like data size and values, underscoring the importance of further research in the domain of optimization tasks for LLMs.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们进行了 LLMS 的优化能力评估，涵盖了多种任务和数据大小。每个任务都对应于唯一的优化领域，LLMS 需要在交互式提示下执行这些任务。即在每次优化步骤中，LLMS 从过去生成的解决方案和其值中生成新的解决方案，然后评估并考虑这些新的解决方案。此外，我们引入了三种特征metric来全面评估任务性能从多个角度。这些 metric 可以用于评估 LLMS 在各种优化任务上的性能，并且对测试样本的变化更加敏感。通过应用这些 metric，我们发现 LLMS 在小样本Size 下表现出色，但是它们的表现受到数据大小和值的影响，这 highlights 了进一步研究 LLMS 在优化任务领域的必要性。
</details></li>
</ul>
<hr>
<h2 id="Lifelong-Learning-for-Fog-Load-Balancing-A-Transfer-Learning-Approach"><a href="#Lifelong-Learning-for-Fog-Load-Balancing-A-Transfer-Learning-Approach" class="headerlink" title="Lifelong Learning for Fog Load Balancing: A Transfer Learning Approach"></a>Lifelong Learning for Fog Load Balancing: A Transfer Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05187">http://arxiv.org/abs/2310.05187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maad Ebrahim, Abdelhakim Senhaji Hafid, Mohamed Riduan Abid</li>
<li>for: 本文旨在提出一种基于Reinforcement Learning（RL）的fog computing环境中的负载均衡（LB）策略，以提高系统性能。</li>
<li>methods: 本文使用privacy-aware RL agents来优化 fog computing环境中的负载均衡，并提出一种生命周期学习框架，使用 Transfer Learning（TL）来减少训练成本和适应环境变化。</li>
<li>results: 本文的实验结果显示，使用TL可以大幅减少RL agents的训练时间和失败概率，并在不同的环境下保持鲁棒性。<details>
<summary>Abstract</summary>
Fog computing emerged as a promising paradigm to address the challenges of processing and managing data generated by the Internet of Things (IoT). Load balancing (LB) plays a crucial role in Fog computing environments to optimize the overall system performance. It requires efficient resource allocation to improve resource utilization, minimize latency, and enhance the quality of service for end-users. In this work, we improve the performance of privacy-aware Reinforcement Learning (RL) agents that optimize the execution delay of IoT applications by minimizing the waiting delay. To maintain privacy, these agents optimize the waiting delay by minimizing the change in the number of queued requests in the whole system, i.e., without explicitly observing the actual number of requests that are queued in each Fog node nor observing the compute resource capabilities of those nodes. Besides improving the performance of these agents, we propose in this paper a lifelong learning framework for these agents, where lightweight inference models are used during deployment to minimize action delay and only retrained in case of significant environmental changes. To improve the performance, minimize the training cost, and adapt the agents to those changes, we explore the application of Transfer Learning (TL). TL transfers the knowledge acquired from a source domain and applies it to a target domain, enabling the reuse of learned policies and experiences. TL can be also used to pre-train the agent in simulation before fine-tuning it in the real environment; this significantly reduces failure probability compared to learning from scratch in the real environment. To our knowledge, there are no existing efforts in the literature that use TL to address lifelong learning for RL-based Fog LB; this is one of the main obstacles in deploying RL LB solutions in Fog systems.
</details>
<details>
<summary>摘要</summary>
FOG计算技术 emerged as a promising paradigm to address the challenges of processing and managing data generated by the Internet of Things (IoT). Load balancing (LB) plays a crucial role in FOG computing environments to optimize the overall system performance. It requires efficient resource allocation to improve resource utilization, minimize latency, and enhance the quality of service for end-users. In this work, we improve the performance of privacy-aware Reinforcement Learning (RL) agents that optimize the execution delay of IoT applications by minimizing the waiting delay. To maintain privacy, these agents optimize the waiting delay by minimizing the change in the number of queued requests in the whole system, i.e., without explicitly observing the actual number of requests that are queued in each FOG node nor observing the compute resource capabilities of those nodes. Besides improving the performance of these agents, we propose in this paper a lifelong learning framework for these agents, where lightweight inference models are used during deployment to minimize action delay and only retrained in case of significant environmental changes. To improve the performance, minimize the training cost, and adapt the agents to those changes, we explore the application of Transfer Learning (TL). TL transfers the knowledge acquired from a source domain and applies it to a target domain, enabling the reuse of learned policies and experiences. TL can be also used to pre-train the agent in simulation before fine-tuning it in the real environment; this significantly reduces failure probability compared to learning from scratch in the real environment. To our knowledge, there are no existing efforts in the literature that use TL to address lifelong learning for RL-based Fog LB; this is one of the main obstacles in deploying RL LB solutions in FOG systems.
</details></li>
</ul>
<hr>
<h2 id="Unified-speech-and-gesture-synthesis-using-flow-matching"><a href="#Unified-speech-and-gesture-synthesis-using-flow-matching" class="headerlink" title="Unified speech and gesture synthesis using flow matching"></a>Unified speech and gesture synthesis using flow matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05181">http://arxiv.org/abs/2310.05181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivam Mehta, Ruibo Tu, Simon Alexanderson, Jonas Beskow, Éva Székely, Gustav Eje Henter</li>
<li>for: 这篇论文旨在描述一种新的多Modal合成方法，可以同时生成语音和手势动作。</li>
<li>methods: 该方法使用优化交通流行为匹配（OT-CFM）来联合生成语音和手势动作，而且比前一代更简单，具有更小的内存占用量，同时能够捕捉语音和手势的联合分布，从而生成两个模态的动作。</li>
<li>results: 该方法在论文中被证明可以生成更自然的语音和更人工的手势动作，并且在单模和多模测试中也表现出更高的合理性。<details>
<summary>Abstract</summary>
As text-to-speech technologies achieve remarkable naturalness in read-aloud tasks, there is growing interest in multimodal synthesis of verbal and non-verbal communicative behaviour, such as spontaneous speech and associated body gestures. This paper presents a novel, unified architecture for jointly synthesising speech acoustics and skeleton-based 3D gesture motion from text, trained using optimal-transport conditional flow matching (OT-CFM). The proposed architecture is simpler than the previous state of the art, has a smaller memory footprint, and can capture the joint distribution of speech and gestures, generating both modalities together in one single process. The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before. Uni- and multimodal subjective tests demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to existing benchmarks.
</details>
<details>
<summary>摘要</summary>
As text-to-speech technologies achieve remarkable naturalness in read-aloud tasks, there is growing interest in multimodal synthesis of verbal and non-verbal communicative behavior, such as spontaneous speech and associated body gestures. This paper presents a novel, unified architecture for jointly synthesizing speech acoustics and skeleton-based 3D gesture motion from text, trained using optimal-transport conditional flow matching (OT-CFM). The proposed architecture is simpler than the previous state of the art, has a smaller memory footprint, and can capture the joint distribution of speech and gestures, generating both modalities together in one single process. The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before. Uni- and multimodal subjective tests demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to existing benchmarks.Here's the translation breakdown:* As text-to-speech technologies achieve remarkable naturalness in read-aloud tasks: 文本读取技术已经达到了很高的自然性水平。* there is growing interest in multimodal synthesis of verbal and non-verbal communicative behavior: 人们对于涉及语音和非语音通信行为的多模态合成表示越来越大的兴趣。* such as spontaneous speech and associated body gestures: 例如，自然的语音和相关的身体姿势。* This paper presents a novel, unified architecture for jointly synthesizing speech acoustics and skeleton-based 3D gesture motion from text: 本文提出了一种新的、统一的架构，用于从文本中同时合成语音和基于骨架的3D手势动作。* trained using optimal-transport conditional flow matching (OT-CFM): 使用优化交通Conditional Flow匹配（OT-CFM）进行训练。* The proposed architecture is simpler than the previous state of the art, has a smaller memory footprint, and can capture the joint distribution of speech and gestures: 提出的架构比前一代更简单，占用更少的内存空间，并能够捕捉语音和手势的共同分布。* generating both modalities together in one single process: 一起生成两种Modalities。* The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before: 新的训练方法可以在更少的步骤（网络评估）中实现更高质量的合成。* Uni- and multimodal subjective tests demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to existing benchmarks: 单模态和多模态主观测试表明，提出的方法可以提高语音自然性、姿势人类化和交叉模态适应性，相比exist的 referential。
</details></li>
</ul>
<hr>
<h2 id="Distributional-Reinforcement-Learning-with-Online-Risk-awareness-Adaption"><a href="#Distributional-Reinforcement-Learning-with-Online-Risk-awareness-Adaption" class="headerlink" title="Distributional Reinforcement Learning with Online Risk-awareness Adaption"></a>Distributional Reinforcement Learning with Online Risk-awareness Adaption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05179">http://arxiv.org/abs/2310.05179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yupeng Wu, Wenjie Huang</li>
<li>for: 本研究旨在提出一种新的分布式RL框架，以快速适应不确定环境中的不同风险水平，以提高RL在安全关键环境中的可靠优化策略。</li>
<li>methods: 该框架基于分布式RL的基础上，通过在线解决一个总变量最小化问题， dynamically选择适度的epistemic风险水平，以满足安全性和稳定性的要求。这里使用了一种 Follow-The-Leader 类型的搜索算法，以及一种特殊修改的损失函数，以实现在线选择风险水平。</li>
<li>results: 对多种任务进行比较，研究发现，DRL-ORA方法在面对不确定环境中表现出色，超过了基于固定风险水平或手动适应风险水平的方法。此外，研究还发现，DRL-ORA方法可以轻松地与多种RL算法结合使用，不需要进行大量的修改。<details>
<summary>Abstract</summary>
The use of reinforcement learning (RL) in practical applications requires considering sub-optimal outcomes, which depend on the agent's familiarity with the uncertain environment. Dynamically adjusting the level of epistemic risk over the course of learning can tactically achieve reliable optimal policy in safety-critical environments and tackle the sub-optimality of a static risk level. In this work, we introduce a novel framework, Distributional RL with Online Risk Adaption (DRL-ORA), which can quantify the aleatory and epistemic uncertainties compositely and dynamically select the epistemic risk levels via solving a total variation minimization problem online. The risk level selection can be efficiently achieved through grid search using a Follow-The-Leader type algorithm, and its offline oracle is related to "satisficing measure" (in the decision analysis community) under a special modification of the loss function. We show multiple classes of tasks where DRL-ORA outperforms existing methods that rely on either a fixed risk level or manually predetermined risk level adaption. Given the simplicity of our modifications, we believe the framework can be easily incorporated into most RL algorithm variants.
</details>
<details>
<summary>摘要</summary>
使用强化学习（RL）在实际应用中需要考虑不理想的结果，这些结果取决于智能体对不确定环境的熟悉程度。随着学习过程中的时间推移， dynamically 调整epistemic 风险水平可以策略性实现可靠的优化策略并解决固定风险水平的不优势。在这项工作中，我们介绍了一种新的框架，分布式RLwith Online Risk Adaptation（DRL-ORA），它可以compositely 量化 aleatory 和 epistemic uncertainties，并在线 solves  total variation minimization problem 来动态选择 epistemic 风险水平。风险水平选择可以高效地通过格子搜索使用 Follow-The-Leader 类型算法进行，其 Offline oracle 与 "满意度量"（在决策分析社区）相关，只是在特定的损失函数修改下进行修改。我们证明了多种任务上，DRL-ORA 可以超过现有的固定风险水平或手动适应风险水平的方法。 compte tenu de la simplicité de nos modifications, nous croyons que le framework peut être facilement intégré dans la plupart des variantes d'algorithme RL.
</details></li>
</ul>
<hr>
<h2 id="Outlier-Weighed-Layerwise-Sparsity-OWL-A-Missing-Secret-Sauce-for-Pruning-LLMs-to-High-Sparsity"><a href="#Outlier-Weighed-Layerwise-Sparsity-OWL-A-Missing-Secret-Sauce-for-Pruning-LLMs-to-High-Sparsity" class="headerlink" title="Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity"></a>Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05175">http://arxiv.org/abs/2310.05175</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luuyin/owl">https://github.com/luuyin/owl</a></li>
<li>paper_authors: Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, Shiwei Liu<br>for:* The paper aims to improve the practical deployment of large language models (LLMs) by applying traditional network pruning techniques.methods:* The paper introduces a novel LLM pruning methodology called Outlier Weighed Layerwise sparsity (OWL), which incorporates non-uniform layerwise sparsity ratios tailored for LLM pruning.results:* The paper demonstrates the distinct advantages offered by OWL over previous methods, achieving a remarkable performance gain of 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, compared to the state-of-the-art Wanda and SparseGPT.<details>
<summary>Abstract</summary>
Large Language Models (LLMs), renowned for their remarkable performance, present a challenge due to their colossal model size when it comes to practical deployment. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters can be pruned in one-shot without hurting performance. Building upon insights gained from pre-LLM models, prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields substantially improved results. To elucidate the underlying reasons for this disparity, we conduct a comprehensive analysis of the distribution of token features within LLMs. In doing so, we discover a strong correlation with the emergence of outliers, defined as features exhibiting significantly greater magnitudes compared to their counterparts in feature dimensions. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios specifically designed for LLM pruning, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is directly proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, our approach exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）因其出色的表现而带来挑战，尤其是在实际应用时遇到模型的巨大大小问题。为了解决这个问题，努力对 LLM 应用传统网络剔除技术，发现可以剔除一个巨大的数据量而无需影响表现。建立在先前的模型中获得的见解上，现有的 LLM 剔除策略一般遵循以剔除所有层的内容的方式，但这与视觉模型中的非均匀层剔除倾向相比，通常会带来更好的结果。为了了解 LLM 中对于剔除的影响，我们进行了一个全面的分析，发现 LLM 中的内容特征分布和异常值的出现有很强的联系。根据这个发现，我们提出了一种新的 LLM 剔除方法，称为非均匀层剔除（OWL）。OWL 的剔除比率与异常值的比率直接相比，从而使得层剔除与异常值的分布更好地匹配。我们对 LLMA-V1 家族和 OPT 进行了实验，覆盖了多个测试benchmark，结果显示我们的方法在剔除率高于 70% 时表现出色，较前者的状态顶峰方法（Wanda和SparseGPT）提高了61.22和6.80的混淆度。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Ability-of-PINNs-To-Solve-Burgers’-PDE-Near-Finite-Time-BlowUp"><a href="#Investigating-the-Ability-of-PINNs-To-Solve-Burgers’-PDE-Near-Finite-Time-BlowUp" class="headerlink" title="Investigating the Ability of PINNs To Solve Burgers’ PDE Near Finite-Time BlowUp"></a>Investigating the Ability of PINNs To Solve Burgers’ PDE Near Finite-Time BlowUp</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05169">http://arxiv.org/abs/2310.05169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dibyakanti Kumar, Anirbit Mukherjee</li>
<li>for: 这个论文旨在investigating the stability of Physics Informed Neural Networks (PINNs) in solving partial differential equations (PDEs) with finite-time blow-ups.</li>
<li>methods: 作者使用了泛化 bound的方法来研究PINNs的稳定性，并通过实验证明了这些 bound 与 neurally found surrogate 的 $\ell_2$-distance有直接的相关性。</li>
<li>results: 研究发现，PINNs 可以准确地探测 finite-time blow-ups，并且可以提供与真实解的 $\ell_2$-distance的评估。<details>
<summary>Abstract</summary>
Physics Informed Neural Networks (PINNs) have been achieving ever newer feats of solving complicated PDEs numerically while offering an attractive trade-off between accuracy and speed of inference. A particularly challenging aspect of PDEs is that there exist simple PDEs which can evolve into singular solutions in finite time starting from smooth initial conditions. In recent times some striking experiments have suggested that PINNs might be good at even detecting such finite-time blow-ups. In this work, we embark on a program to investigate this stability of PINNs from a rigorous theoretical viewpoint. Firstly, we derive generalization bounds for PINNs for Burgers' PDE, in arbitrary dimensions, under conditions that allow for a finite-time blow-up. Then we demonstrate via experiments that our bounds are significantly correlated to the $\ell_2$-distance of the neurally found surrogate from the true blow-up solution, when computed on sequences of PDEs that are getting increasingly close to a blow-up.
</details>
<details>
<summary>摘要</summary>
物理学 Informed Neural Networks (PINNs) 在解决复杂的偏微分方程（PDEs）方面已经取得了不断更新的成就，同时提供了吸引人的准确率和推理速度之间的折衔。特别是，PDEs 中存在一些简单的 PDEs，可以在有限时间内从流体初始条件演化成精炼解。在最近的实验中，有些突出的实验结果表明，PINNs 可能会检测到这种有限时间爆炸。在这项工作中，我们开始了一项研究，以探讨 PINNs 在理论上的稳定性。首先，我们 derive了 PINNs 对于布尔格 PDE 的泛化上限，在任意维度下，以条件 Allowing for finite-time blow-up。然后，我们通过实验表明，我们的上限与 neurally 发现的代理模型在计算 PDE 序列中的 $\ell_2$ 距离是高度相关的，当 PDE 序列在爆炸解 approached 时。
</details></li>
</ul>
<hr>
<h2 id="A-Corrected-Expected-Improvement-Acquisition-Function-Under-Noisy-Observations"><a href="#A-Corrected-Expected-Improvement-Acquisition-Function-Under-Noisy-Observations" class="headerlink" title="A Corrected Expected Improvement Acquisition Function Under Noisy Observations"></a>A Corrected Expected Improvement Acquisition Function Under Noisy Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05166">http://arxiv.org/abs/2310.05166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/han678/correctednoisyei">https://github.com/han678/correctednoisyei</a></li>
<li>paper_authors: Han Zhou, Xingchen Ma, Matthew B Blaschko</li>
<li>for: 该论文主要针对的是 Bayesian 优化中的难题，即在含有噪声的观测中使用预期改进（EI）策略。</li>
<li>methods: 该论文提出了一种基于 Gaussian Process 模型的 EI 策略修正方法，该方法可以考虑噪声的影响，并提供一个包容更多情况的 acquisition function。</li>
<li>results: 该论文通过 theoretically 和实验来证明，该修正方法可以提高 EI 策略在含有噪声的情况下的性能，并且可以在黑盒优化和神经网络模型压缩等问题中提供更好的解决方案。<details>
<summary>Abstract</summary>
Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless settings. We show that our method achieves a sublinear convergence rate on the cumulative regret bound under heteroscedastic observation noise. Our empirical results demonstrate that our proposed acquisition function can outperform EI in the presence of noisy observations on benchmark functions for black-box optimization, as well as on parameter search for neural network model compression.
</details>
<details>
<summary>摘要</summary>
纯粹最大化期望提升（EI）是搜索优化中最广泛使用的政策之一，主要是因为它的简单性和能够处理噪音观测的能力。具体来说，提升函数经常使用 posterior mean 作为噪音观测下的最佳启发式。然而，启发式解释中往往忽略了启发式解释中的不确定性信息。为了解决这个限制，我们提议修改 EI，通过在 GP 模型中提供的决定矩阵信息来改进它的关闭形式表达。这个购买函数在噪音观测下特有化，我们认为这个函数应该取代普通的噪音观测下的表达。我们的改进购买函数在各种不同的观测环境下都具有良好的通用性。我们证明了我们的方法在各种不同的观测环境下都能够实现下降的 regret 级别。我们的实验结果表明，我们的提议的购买函数在噪音观测下可以超越 EI 在黑obox 优化和神经网络模型压缩中的性能。
</details></li>
</ul>
<hr>
<h2 id="Transferable-Availability-Poisoning-Attacks"><a href="#Transferable-Availability-Poisoning-Attacks" class="headerlink" title="Transferable Availability Poisoning Attacks"></a>Transferable Availability Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05141">http://arxiv.org/abs/2310.05141</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/trustmlrg/transpoison">https://github.com/trustmlrg/transpoison</a></li>
<li>paper_authors: Yiyong Liu, Michael Backes, Xiao Zhang</li>
<li>for: 这个论文旨在攻击机器学习模型的可用性，特别是针对模型在训练数据上的性能。</li>
<li>methods: 这个论文使用了攻击者采用不同的学习算法和攻击策略来降低模型的总测试精度。</li>
<li>results: 论文表明，如果攻击者使用不同的学习方法来攻击模型， тоThen the effectiveness of prior poisoning attacks will be significantly decreased. In addition, the authors propose a transferable poisoning attack that can produce poisoned samples with improved transferability across different learners and even paradigms. Through extensive experiments on benchmark image datasets, the authors show that their transferable poisoning attack can produce poisoned samples with significantly improved transferability.<details>
<summary>Abstract</summary>
We consider availability data poisoning attacks, where an adversary aims to degrade the overall test accuracy of a machine learning model by crafting small perturbations to its training data. Existing poisoning strategies can achieve the attack goal but assume the victim to employ the same learning method as what the adversary uses to mount the attack. In this paper, we argue that this assumption is strong, since the victim may choose any learning algorithm to train the model as long as it can achieve some targeted performance on clean data. Empirically, we observe a large decrease in the effectiveness of prior poisoning attacks if the victim uses a different learning paradigm to train the model and show marked differences in frequency-level characteristics between perturbations generated with respect to different learners and attack methods. To enhance the attack transferability, we propose Transferable Poisoning, which generates high-frequency poisoning perturbations by alternately leveraging the gradient information with two specific algorithms selected from supervised and unsupervised contrastive learning paradigms. Through extensive experiments on benchmark image datasets, we show that our transferable poisoning attack can produce poisoned samples with significantly improved transferability, not only applicable to the two learners used to devise the attack but also for learning algorithms and even paradigms beyond.
</details>
<details>
<summary>摘要</summary>
我们考虑了数据毒化攻击，敌人想要降低机器学习模型的总测试准确率，通过对训练数据进行小幅度的修改。现有的攻击策略可以实现攻击目标，但假设攻击者使用的学习方法与受害者使用的学习方法一样。在这篇论文中，我们认为这是一个强大的假设，因为受害者可以选择任何学习算法来训练模型，只要它可以在干净数据上达到一定的性能目标。我们在实验中观察到，如果受害者使用不同的学习方法来训练模型，攻击效果会减弱很多。为了提高攻击的传送性，我们提议了可传递的毒化攻击，通过交替使用两种特定的算法来生成高频毒化干扰。我们通过对标准图像集进行广泛的实验，证明了我们的可传递毒化攻击可以生成高质量的毒化样本，不仅适用于我们用于制定攻击的两种学习算法，还可以应用于其他学习算法和学习方法。
</details></li>
</ul>
<hr>
<h2 id="How-Graph-Neural-Networks-Learn-Lessons-from-Training-Dynamics-in-Function-Space"><a href="#How-Graph-Neural-Networks-Learn-Lessons-from-Training-Dynamics-in-Function-Space" class="headerlink" title="How Graph Neural Networks Learn: Lessons from Training Dynamics in Function Space"></a>How Graph Neural Networks Learn: Lessons from Training Dynamics in Function Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05105">http://arxiv.org/abs/2310.05105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxiao Yang, Qitian Wu, David Wipf, Ruoyu Sun, Junchi Yan</li>
<li>for: 本研究的目的是探讨深度学习模型在更加可读性的方式下进行学习行为。特别是对于图神经网络（GNNs），研究者们已经做出了很多进步，但是还没有充分了解GNNs在优化过程中是否会学习愿景函数。</li>
<li>methods: 研究者们使用了分析框架来研究GNNs的学习动态，并发现GNNs的训练过程可以被重新描述为一种更加熟悉的标签传播框架。此外，研究者们还提出了一种简化并实现GNNs的学习动态的方法，以提高其效率和可读性。</li>
<li>results: 研究者们发现GNNs在不同类型的图上的学习动态，包括同构图和hetrophylic graph，都具有某种程度的相互关联性。此外，研究者们还发现GNNs可以在不同类型的图上学习出高效的函数，并且这些函数可以在新的图上进行泛化。<details>
<summary>Abstract</summary>
A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.
</details>
<details>
<summary>摘要</summary>
deep learning中的一个长期目标是将黑盒模型的学习行为更加解释性地表示。对于图 neural network (GNN)，已经有了许多进步，但是它们是否在优化过程中学习所需的函数仍然不清楚。为了填补这一重要的空白，我们通过过参数化的方法研究 GNN 的学习动态在函数空间。具体来说，我们发现 GNN 的训练过程可以重新划为一种更加熟悉的标签传播框架，这与图适应偏好相关。从这个角度，我们提供了成功泛化和异谱图处理的解释，这与观察相符。在实践中，我们通过减少学习动态和实现来提出一种简洁的半监督学习算法，具有传统算法的效率和现代 GNN 的效果。
</details></li>
</ul>
<hr>
<h2 id="Asymmetrically-Decentralized-Federated-Learning"><a href="#Asymmetrically-Decentralized-Federated-Learning" class="headerlink" title="Asymmetrically Decentralized Federated Learning"></a>Asymmetrically Decentralized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05093">http://arxiv.org/abs/2310.05093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinglun Li, Miao Zhang, Nan Yin, Quanjun Yin, Li Shen</li>
<li>For: This paper aims to address the communication burden and privacy concerns associated with centralized servers in Federated Learning (FL) by proposing a Decentralized Federated Learning (DFL) algorithm based on asymmetric topologies and the Push-Sum protocol.* Methods: The proposed DFedSGPSM algorithm combines the Sharpness Aware Minimization (SAM) optimizer and local momentum to improve algorithm performance and alleviate local heterogeneous overfitting in FL. The SAM optimizer employs gradient perturbations to generate locally flat models and searches for models with uniformly low loss values, while the local momentum accelerates the optimization process.* Results: The paper demonstrates the superior performance of the proposed DFedSGPSM algorithm compared to state-of-the-art optimizers through extensive experiments on the MNIST, CIFAR10, and CIFAR100 datasets. The theoretical analysis also proves that the algorithm achieves a convergence rate of $\mathcal{O}(\frac{1}{\sqrt{T})$ in a non-convex smooth setting under mild assumptions, and that better topological connectivity achieves tighter upper bounds.<details>
<summary>Abstract</summary>
To address the communication burden and privacy concerns associated with the centralized server in Federated Learning (FL), Decentralized Federated Learning (DFL) has emerged, which discards the server with a peer-to-peer (P2P) communication framework. However, most existing DFL algorithms are based on symmetric topologies, such as ring and grid topologies, which can easily lead to deadlocks and are susceptible to the impact of network link quality in practice. To address these issues, this paper proposes the DFedSGPSM algorithm, which is based on asymmetric topologies and utilizes the Push-Sum protocol to effectively solve consensus optimization problems. To further improve algorithm performance and alleviate local heterogeneous overfitting in Federated Learning (FL), our algorithm combines the Sharpness Aware Minimization (SAM) optimizer and local momentum. The SAM optimizer employs gradient perturbations to generate locally flat models and searches for models with uniformly low loss values, mitigating local heterogeneous overfitting. The local momentum accelerates the optimization process of the SAM optimizer. Theoretical analysis proves that DFedSGPSM achieves a convergence rate of $\mathcal{O}(\frac{1}{\sqrt{T})$ in a non-convex smooth setting under mild assumptions. This analysis also reveals that better topological connectivity achieves tighter upper bounds. Empirically, extensive experiments are conducted on the MNIST, CIFAR10, and CIFAR100 datasets, demonstrating the superior performance of our algorithm compared to state-of-the-art optimizers.
</details>
<details>
<summary>摘要</summary>
为了解决联合服务器在联合学习（FL）中的通信负担和隐私问题，协同联合学习（DFL）已经出现，它抛弃了服务器，使用幂等（P2P）通信框架。然而，大多数现有的DFL算法都基于对称网络 topology，如环和格 topology，这些 topology 可以轻松导致堵塞和因网络连接质量的影响。为解决这些问题，本文提出了DFedSGPSM算法，它基于非对称网络 topology 和Push-Sum协议来有效地解决共识优化问题。为了进一步改进算法性能并避免本地不同类型的过拟合，我们的算法结合了Sharpness Aware Minimization（SAM）优化器和本地冲击。SAM优化器使用梯度偏移来生成本地平滑模型，并在搜索模型的损失值为uniformly low时搜索模型。本地冲击加速了SAM优化器的优化过程。理论分析表明，DFedSGPSM算法在非对称光滑设定下 achieve 收敛速率为 $\mathcal{O}(\frac{1}{\sqrt{T})$ ，其中 T 是迭代次数。此分析还表明，更好的网络连接性可以实现更紧的Upper bound。实验表明，我们的算法在 MNIST、CIFAR10 和 CIFAR100 数据集上进行了广泛的实验，与现状的优化器相比，它的性能显著更高。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Block-based-Quantisation-What-is-Important-for-Sub-8-bit-LLM-Inference"><a href="#Revisiting-Block-based-Quantisation-What-is-Important-for-Sub-8-bit-LLM-Inference" class="headerlink" title="Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?"></a>Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05079">http://arxiv.org/abs/2310.05079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chengzhang-98/llm-mixed-q">https://github.com/chengzhang-98/llm-mixed-q</a></li>
<li>paper_authors: Cheng Zhang, Jianyi Cheng, Ilia Shumailov, George A. Constantinides, Yiren Zhao</li>
<li>for: 这个论文的目的是解决大型自然语言模型（LLM）的扩展和缩放问题，以降低计算和存储资源的成本。</li>
<li>methods: 这篇论文使用了统计学和学习Property的分析，发现LLM层的瓶颈在于数值扩展偏移。以此为基础，他们提出了块量化方法，可以有效地减少数值扩展偏移，从计算路径的视角来看。</li>
<li>results: 根据论文的结果，使用了6位减法的量化LLM可以达到$19\times$高的数学密度和$5\times$的存储密度，比浮点32基eline高$2.5\times$的数学密度和$1.2\times$的存储密度。此外，他们还分享了在下游任务上实现 nearly-lossless 4位LLM的技巧，包括活动和重量分布的不一致、优化的精度练习策略以及LLM的统计性质中的更低的量化精度。<details>
<summary>Abstract</summary>
The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has merged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the LLM layer and attribute the bottleneck of LLM quantisation to numerical scaling offsets. To address this, we adapt block quantisations for LLMs, a family of methods that share scaling factors across packed numbers. Block quantisations efficiently reduce the numerical scaling offsets solely from an arithmetic perspective, without additional treatments in the computational path. Our nearly-lossless quantised 6-bit LLMs achieve a $19\times$ higher arithmetic density and $5\times$ memory density than the float32 baseline, surpassing the prior art 8-bit quantisation by $2.5\times$ in arithmetic density and $1.2\times$ in memory density, without requiring any data calibration or re-training. We also share our insights into sub-8-bit LLM quantisation, including the mismatch between activation and weight distributions, optimal fine-tuning strategies, and a lower quantisation granularity inherent in the statistical properties of LLMs. The latter two tricks enable nearly-lossless 4-bit LLMs on downstream tasks. Our code is open-sourced.
</details>
<details>
<summary>摘要</summary>
Large language models (LLMs) 的推理需要巨量的计算和存储资源。为了降低这些成本，量化已成为一种有前途的解决方案，但现有的 LLM 量化主要集中在8位。在这个工作中，我们研究了 LLM 层的统计和学习特性，并归因 LLM 量化的瓶颈到数字扩大偏移。为解决这个问题，我们采用了块量化技术，这种技术在压缩数据时共享扩大因子。块量化可以高效地减少数字扩大偏移，不需要额外处理在计算路径上。我们的6位量化 LLM 可以达到浮点32基eline的19倍的数学密度和5倍的存储密度，比前一代8位量化的数学密度和存储密度高出2.5倍和1.2倍。此外，我们还分享了在下游任务上实现 nearly-lossless 4位 LLMS 的技巧，包括活动和重量分布不匹配、优化 fine-tuning 策略和 LLMS 的统计性质中的更低的量化精度。这两个技巧使得我们可以在下游任务上实现 nearly-lossless 4位 LLMS。我们的代码已经开源。
</details></li>
</ul>
<hr>
<h2 id="FedFed-Feature-Distillation-against-Data-Heterogeneity-in-Federated-Learning"><a href="#FedFed-Feature-Distillation-against-Data-Heterogeneity-in-Federated-Learning" class="headerlink" title="FedFed: Feature Distillation against Data Heterogeneity in Federated Learning"></a>FedFed: Feature Distillation against Data Heterogeneity in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05077">http://arxiv.org/abs/2310.05077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visitworld123/fedfed">https://github.com/visitworld123/fedfed</a></li>
<li>paper_authors: Zhiqin Yang, Yonggang Zhang, Yu Zheng, Xinmei Tian, Hao Peng, Tongliang Liu, Bo Han</li>
<li>for: 这篇论文旨在解决联合学习（Federated Learning，FL）面临的数据不一致问题，即客户端数据的分布差异。</li>
<li>methods: 该论文提出了一种新的方法 called Federated Feature Distillation（FedFed），它将数据分为性能敏感特征（大量对模型性能的贡献）和性能鲁棒特征（对模型性能有限度贡献）。性能敏感特征被全局共享，以减轻数据不一致问题，而性能鲁棒特征被保留在本地。客户端可以使用本地和共享数据来训练模型。</li>
<li>results: 实验表明，FedFed 可以提高模型性能。<details>
<summary>Abstract</summary>
Federated learning (FL) typically faces data heterogeneity, i.e., distribution shifting among clients. Sharing clients' information has shown great potentiality in mitigating data heterogeneity, yet incurs a dilemma in preserving privacy and promoting model performance. To alleviate the dilemma, we raise a fundamental question: \textit{Is it possible to share partial features in the data to tackle data heterogeneity?} In this work, we give an affirmative answer to this question by proposing a novel approach called {\textbf{Fed}erated \textbf{Fe}ature \textbf{d}istillation} (FedFed). Specifically, FedFed partitions data into performance-sensitive features (i.e., greatly contributing to model performance) and performance-robust features (i.e., limitedly contributing to model performance). The performance-sensitive features are globally shared to mitigate data heterogeneity, while the performance-robust features are kept locally. FedFed enables clients to train models over local and shared data. Comprehensive experiments demonstrate the efficacy of FedFed in promoting model performance.
</details>
<details>
<summary>摘要</summary>
通常，联合学习（FL）会面临数据不一致性问题，即客户端数据的分布差异。如果分享客户端信息，可以减轻数据不一致性问题，但是会降低隐私和提高模型性能。为了解决这个矛盾，我们提出了一个基本问题：“是否可以分享数据中的部分特征来解决数据不一致性问题？”在这个工作中，我们给出了一个答案，并提出了一种新的方法 called“联邦特征分离”（FedFed）。具体来说，FedFed将数据分为对性能敏感的特征（即对模型性能具有重要作用）和对性能稳定的特征（即对模型性能具有有限作用）。对性能敏感的特征进行全局分享，以减轻数据不一致性问题，而对性能稳定的特征则保留在本地。FedFed允许客户端通过本地和共享数据来训练模型。我们进行了广泛的实验，并证明了FedFed的效果。
</details></li>
</ul>
<hr>
<h2 id="Towards-Scalable-Wireless-Federated-Learning-Challenges-and-Solutions"><a href="#Towards-Scalable-Wireless-Federated-Learning-Challenges-and-Solutions" class="headerlink" title="Towards Scalable Wireless Federated Learning: Challenges and Solutions"></a>Towards Scalable Wireless Federated Learning: Challenges and Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05076">http://arxiv.org/abs/2310.05076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Zhou, Yuanming Shi, Haibo Zhou, Jingjing Wang, Liqun Fu, Yang Yang</li>
<li>for: 本研究旨在探讨在无线网络中实现可信批处理的分布式机器学习（ Federated Learning，FL）的挑战和解决方案。</li>
<li>methods: 本文提出了两个方面的解决方案：一是通过任务 oriented 模型聚合来提高无线通信缓存性，二是通过计算效率优化来提高资源分配的算法可扩展性。</li>
<li>results: 本文提出了三种任务 oriented 学习算法来提高计算效率，并指出了一些需要进一步研究的问题。<details>
<summary>Abstract</summary>
The explosive growth of smart devices (e.g., mobile phones, vehicles, drones) with sensing, communication, and computation capabilities gives rise to an unprecedented amount of data. The generated massive data together with the rapid advancement of machine learning (ML) techniques spark a variety of intelligent applications. To distill intelligence for supporting these applications, federated learning (FL) emerges as an effective distributed ML framework, given its potential to enable privacy-preserving model training at the network edge. In this article, we discuss the challenges and solutions of achieving scalable wireless FL from the perspectives of both network design and resource orchestration. For network design, we discuss how task-oriented model aggregation affects the performance of wireless FL, followed by proposing effective wireless techniques to enhance the communication scalability via reducing the model aggregation distortion and improving the device participation. For resource orchestration, we identify the limitations of the existing optimization-based algorithms and propose three task-oriented learning algorithms to enhance the algorithmic scalability via achieving computation-efficient resource allocation for wireless FL. We highlight several potential research issues that deserve further study.
</details>
<details>
<summary>摘要</summary>
随着智能设备（如移动电话、汽车、无人机）的激增，生成了历史上无 precedent的数据量。这些大量数据，加上机器学习（ML）技术的快速发展，使得各种智能应用得以实现。为了提取智能，聚合式学习（FL）作为一种有效的分布式ML框架，在网络边缘实现隐私保护的模型训练。本文从网络设计和资源调度两个角度出发，探讨了无线FL的挑战和解决方案。从网络设计角度来看，我们讨论了任务导向的模型聚合如何影响无线FL的性能，然后提出了有效的无线技术来增强通信可扩展性，例如减少模型聚合误差和提高设备参与度。从资源调度角度来看，我们发现现有的优化算法有限制，因此提出了三种任务导向的学习算法，以实现计算效率的资源分配，从而提高无线FL的算法可扩展性。我们还指出了一些需要进一步研究的问题。
</details></li>
</ul>
<hr>
<h2 id="Robust-GBDT-A-Novel-Gradient-Boosting-Model-for-Noise-Robust-Classification"><a href="#Robust-GBDT-A-Novel-Gradient-Boosting-Model-for-Noise-Robust-Classification" class="headerlink" title="Robust-GBDT: A Novel Gradient Boosting Model for Noise-Robust Classification"></a>Robust-GBDT: A Novel Gradient Boosting Model for Noise-Robust Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05067">http://arxiv.org/abs/2310.05067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luojiaqimath/robust-gbdt">https://github.com/luojiaqimath/robust-gbdt</a></li>
<li>paper_authors: Jiaqi Luo, Yuedong Quan, Shixin Xu</li>
<li>for: 这个研究是为了提出一种可以处理标签杂音的高效搜寻算法，并且可以处理多类分类任务。</li>
<li>methods: 这个研究使用了进步的Gradient Boosting Decision Trees（GBDT）框架，并且引入了一些robust loss functions，以抵消标签杂音的影响。</li>
<li>results: 这个研究发现，使用Robust-GBDT模型可以获得更准确的预测结果，并且可以更好地处理杂音和类别偏心的问题。<details>
<summary>Abstract</summary>
Robust boosting algorithms have emerged as alternative solutions to traditional boosting techniques for addressing label noise in classification tasks. However, these methods have predominantly focused on binary classification, limiting their applicability to multi-class tasks. Furthermore, they encounter challenges with imbalanced datasets, missing values, and computational efficiency. In this paper, we establish that the loss function employed in advanced Gradient Boosting Decision Trees (GBDT), particularly Newton's method-based GBDT, need not necessarily exhibit global convexity. Instead, the loss function only requires convexity within a specific region. Consequently, these GBDT models can leverage the benefits of nonconvex robust loss functions, making them resilient to noise. Building upon this theoretical insight, we introduce a new noise-robust boosting model called Robust-GBDT, which seamlessly integrates the advanced GBDT framework with robust losses. Additionally, we enhance the existing robust loss functions and introduce a novel robust loss function, Robust Focal Loss, designed to address class imbalance. As a result, Robust-GBDT generates more accurate predictions, significantly enhancing its generalization capabilities, especially in scenarios marked by label noise and class imbalance. Furthermore, Robust-GBDT is user-friendly and can easily integrate existing open-source code, enabling it to effectively handle complex datasets while improving computational efficiency. Numerous experiments confirm the superiority of Robust-GBDT over other noise-robust methods.
</details>
<details>
<summary>摘要</summary>
强健的搜索算法已经出现为 tradicional boosting 技术的替代方案，以解决分类任务中的标签噪声问题。然而，这些方法主要集中在 binary 分类中，因此其可用性不高于多类任务。此外，它们还遇到了不均衡数据集、缺失值和计算效率的挑战。在这篇论文中，我们证明了 GBDT 模型使用的损失函数不必必须具有全局凸性。相反，损失函数只需要在特定区域内具有凸性。因此，这些 GBDT 模型可以利用不凸的 robust 损失函数，使其具有抗噪声的能力。基于这一理论发现，我们引入了一种新的噪声Robust GBDT 模型，该模型通过结合高级 GBDT 框架和robust损失函数来实现。此外，我们还改进了现有的robust损失函数，并引入了一种新的 Robust Focal Loss 函数，用于解决类异常现象。这使得 Robust-GBDT 可以生成更加准确的预测结果，从而提高其泛化能力，特别是在标签噪声和类异常的情况下。此外，Robust-GBDT 易于使用，可以轻松地 интеGRATE现有的开源代码，使其可以有效地处理复杂的数据集，同时提高计算效率。许多实验证明了 Robust-GBDT 的优越性。
</details></li>
</ul>
<hr>
<h2 id="Pushing-the-Limits-of-Pre-training-for-Time-Series-Forecasting-in-the-CloudOps-Domain"><a href="#Pushing-the-Limits-of-Pre-training-for-Time-Series-Forecasting-in-the-CloudOps-Domain" class="headerlink" title="Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain"></a>Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05063">http://arxiv.org/abs/2310.05063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerald Woo, Chenghao Liu, Akshat Kumar, Doyen Sahoo</li>
<li>for: 这篇论文旨在提供大规模时间序列预测数据集，以便进一步研究预测模型的预训练和扩展。</li>
<li>methods: 本研究使用云端操作（CloudOps）领域的三个大规模时间序列预测数据集，其中最大的数据集有比利они个观测值，以便进一步研究预训练和扩展时间序列模型。</li>
<li>results: 研究发现，这种预训练方法可以在大规模时间序列预测 task 上 achieve 27% 的错误减少，并且可以与 класиical 学习基eline 相比。<details>
<summary>Abstract</summary>
Time series has been left behind in the era of pre-training and transfer learning. While research in the fields of natural language processing and computer vision are enjoying progressively larger datasets to train massive models, the most popular time series datasets consist of only tens of thousands of time steps, limiting our ability to study the effectiveness of pre-training and scaling. Recent studies have also cast doubt on the need for expressive models and scale. To alleviate these issues, we introduce three large-scale time series forecasting datasets from the cloud operations (CloudOps) domain, the largest having billions of observations, enabling further study into pre-training and scaling of time series models. We build the empirical groundwork for studying pre-training and scaling of time series models and pave the way for future research by identifying a promising candidate architecture. We show that it is a strong zero-shot baseline and benefits from further scaling, both in model and dataset size. Accompanying these datasets and results is a suite of comprehensive benchmark results comparing classical and deep learning baselines to our pre-trained method - achieving a 27% reduction in error on the largest dataset. Code and datasets will be released.
</details>
<details>
<summary>摘要</summary>
时间序列已经被搁置在预训练和传输学习的时代之外。而自然语言处理和计算机视觉领域的研究正在拥有越来越大的数据集来训练庞大模型，而时间序列数据集仅有几万个时间步，限制了我们研究预训练和扩大的能力。最近的研究还把需要表达力强大的模型和扩大的疑问抛弃了出来。为了解决这些问题，我们介绍了三个大规模时间序列预测数据集，来自云操作（CloudOps）领域，最大数据集有数十亿个观察结果，使我们能够进一步研究预训练和扩大时间序列模型的效果。我们建立了时间序列模型预训练和扩大的基础实验，并证明了我们的方法是一个强大的零个shot基线，并且在模型和数据集大小增加时能够减少错误率。我们附加了这些数据集和结果，以及对классиical和深度学习基eline的比较，实现了最大数据集上的27%错误减少。代码和数据集将被公布。
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-in-Contextual-Second-Price-Pay-Per-Click-Auctions"><a href="#Online-Learning-in-Contextual-Second-Price-Pay-Per-Click-Auctions" class="headerlink" title="Online Learning in Contextual Second-Price Pay-Per-Click Auctions"></a>Online Learning in Contextual Second-Price Pay-Per-Click Auctions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05047">http://arxiv.org/abs/2310.05047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengxiao Zhang, Haipeng Luo</li>
<li>for: 本文研究在上下文敏感的Pay-Per-Click拍卖中进行在线学习，每轮都会收到一些上下文和一组广告，并需要对广告的点击率进行估计，以进行第二价拍卖。learner的目标是尽可能减少她的后悔，定义为她的总收益与一个假设的拍卖策略的差。</li>
<li>methods: 我们首先证明了在$T$轮后，learner可以达到$\sqrt{T}$的后悔，并且这是不可避免的，因为我们的算法和多重投机问题类似。然后，我们引用了最近的上下文敏感投机算法的进步，开发了两种实用的上下文拍卖算法：第一种使用对数权重方案和正方差误差，保持了同样的$\sqrt{T}$后悔 bound，而第二种通过简单的ε-胆策略将问题降到在线回归问题，尽管它的后悔 bound更差。</li>
<li>results: 我们在一个synthetic数据上进行了实验，并证明了我们的算法在实际应用中的有效性和优异性。<details>
<summary>Abstract</summary>
We study online learning in contextual pay-per-click auctions where at each of the $T$ rounds, the learner receives some context along with a set of ads and needs to make an estimate on their click-through rate (CTR) in order to run a second-price pay-per-click auction. The learner's goal is to minimize her regret, defined as the gap between her total revenue and that of an oracle strategy that always makes perfect CTR predictions. We first show that $\sqrt{T}$-regret is obtainable via a computationally inefficient algorithm and that it is unavoidable since our algorithm is no easier than the classical multi-armed bandit problem. A by-product of our results is a $\sqrt{T}$-regret bound for the simpler non-contextual setting, improving upon a recent work of [Feng et al., 2023] by removing the inverse CTR dependency that could be arbitrarily large. Then, borrowing ideas from recent advances on efficient contextual bandit algorithms, we develop two practically efficient contextual auction algorithms: the first one uses the exponential weight scheme with optimistic square errors and maintains the same $\sqrt{T}$-regret bound, while the second one reduces the problem to online regression via a simple epsilon-greedy strategy, albeit with a worse regret bound. Finally, we conduct experiments on a synthetic dataset to showcase the effectiveness and superior performance of our algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究在上下文中的线上学习，在每个 $T$ 轮中，学习者接收一些上下文以及一组广告，并需要对各个广告的点击率（CTR）进行估计，以进行第二价格支付每击广告。学习者的目标是尽量减少她的恨觉，定义为她的总收入与一个假设总是正确地预测 CTR 的 oracle 策略的差距。我们首先证明了 $\sqrt{T}$-恨觉是可以实现的，并且这是不可避免的，因为我们的算法与经典多重武器问题相同。我们的结果还提供了 $\sqrt{T}$-恨觉 bound  для更加简单的非上下文化设定，超过最近的 [Feng et al., 2023] 的研究成果，并将 inverse CTR 依赖项消除。然后，我们借鉴了最近的上下文策略算法的进步，开发了两种实用的上下文拍卖算法：第一种使用几何质数分配方案和乐观方差 Error，保持了同样的 $\sqrt{T}$-恨觉 bound，而第二种将问题降到在线回归问题，通过简单的ε-赫赫策略，尽管它的恨觉 bound 较差。最后，我们在一个 sintetic 数据集上进行了实验，以展示我们的算法的效果和优于性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-Based-Cross-Layer-Design-in-Terahertz-Mesh-Backhaul-Networks"><a href="#Deep-Reinforcement-Learning-Based-Cross-Layer-Design-in-Terahertz-Mesh-Backhaul-Networks" class="headerlink" title="Deep Reinforcement Learning Based Cross-Layer Design in Terahertz Mesh Backhaul Networks"></a>Deep Reinforcement Learning Based Cross-Layer Design in Terahertz Mesh Backhaul Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05034">http://arxiv.org/abs/2310.05034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifeng Hu, Chong Han, Xudong Wang</li>
<li>for: 这个论文是为了解决teraHertz（THz）网络中的跨层路由和长期资源分配问题，以提高未来无线后门系统的可扩展性和可靠性。</li>
<li>methods: 这个论文使用了深度强化学习（DRL）技术，实现跨层设计，包括路由对应和资源分配。在DRL方法中，使用了多任务结构，协助实现能源和子阵列的有效使用。此外，这个方法还使用了层次架构，实现每个基站的特定资源分配和学习知识传递。</li>
<li>results:  simulations 表明，DEFLECT routing 比 minimal hop-count metric 消耗更少的资源，并且不会导致包库损失和第二层延迟。此外，DEFLECT DRL 方法可以在1秒内从破损链路上复原资源有效地。<details>
<summary>Abstract</summary>
Supporting ultra-high data rates and flexible reconfigurability, Terahertz (THz) mesh networks are attractive for next-generation wireless backhaul systems that empower the integrated access and backhaul (IAB). In THz mesh backhaul networks, the efficient cross-layer routing and long-term resource allocation is yet an open problem due to dynamic traffic demands as well as possible link failures caused by the high directivity and high non-line-of-sight (NLoS) path loss of THz spectrum. In addition, unpredictable data traffic and the mixed integer programming property with the NP-hard nature further challenge the effective routing and long-term resource allocation design. In this paper, a deep reinforcement learning (DRL) based cross-layer design in THz mesh backhaul networks (DEFLECT) is proposed, by considering dynamic traffic demands and possible sudden link failures. In DEFLECT, a heuristic routing metric is first devised to facilitate resource efficiency (RE) enhancement regarding energy and sub-array usages. Furthermore, a DRL based resource allocation algorithm is developed to realize long-term RE maximization and fast recovery from broken links. Specifically in the DRL method, the exploited multi-task structure cooperatively benefits joint power and sub-array allocation. Additionally, the leveraged hierarchical architecture realizes tailored resource allocation for each base station and learned knowledge transfer for fast recovery. Simulation results show that DEFLECT routing consumes less resource, compared to the minimal hop-count metric. Moreover, unlike conventional DRL methods causing packet loss and second-level latency, DEFLECT DRL realizes the long-term RE maximization with no packet loss and millisecond-level latency, and recovers resource-efficient backhaul from broken links within 1s.
</details>
<details>
<summary>摘要</summary>
支持超高数据速率和灵活可重新配置，tera兆Hz（THz）网络是下一代无线备用系统的吸引力，它们可以强化集成访问和备用（IAB）。在THz网络中，有效的交叉层路由和长期资源分配仍然是一个开放的问题，因为动态的流量需求以及可能的链接故障，这些链接故障是由THz频谱的高直达性和高非直视线（NLoS）损失引起的。此外，不可预测的数据流量和混合整数编程性，以及NP困难的性质，进一步挑战了有效的路由和长期资源分配设计。在这篇论文中，一种基于深度学习（DRL）的交叉层设计方法（DEFLECT）被提出，该方法考虑了动态的流量需求和可能的突然链接故障。在DEFLECT中，一种帮助提高资源效率（RE）的启发式路由度量被开发，以便更好地利用能量和子频谱资源。此外，一种基于DRL的资源分配算法被开发，以实现长期RE最大化和快速从破断链接恢复。在DRL方法中，通过合作的多任务结构，对于每个基站的共享资源进行了优化。此外，通过利用层次结构，实现了个性化的资源分配和学习知识传递，以便快速恢复资源。 simulation结果表明，DEFLECT路由占用了较少的资源，相比于最小跳数 metric。此外，与传统DRL方法不同，DEFLECT DRL实现了长期RE最大化，无 packet loss和毫秒级延迟，并在1秒内从破断链接恢复资源。
</details></li>
</ul>
<hr>
<h2 id="Compressed-online-Sinkhorn"><a href="#Compressed-online-Sinkhorn" class="headerlink" title="Compressed online Sinkhorn"></a>Compressed online Sinkhorn</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05019">http://arxiv.org/abs/2310.05019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengpei Wang, Clarice Poon, Tony Shardlow<br>for:This paper focuses on the use of optimal transport (OT) distances and the Sinkhorn algorithm for large-scale data processing.methods:The paper revisits the online Sinkhorn algorithm introduced by Mensch and Peyr&#39;e in 2020, and improves the convergence analysis with a faster rate under certain parameter choices. Additionally, the paper proposes a compressed online Sinkhorn algorithm that combines measure compression techniques with the online Sinkhorn algorithm.results:The paper provides numerical results to verify the sharpness of the improved convergence rate, as well as practical numerical gains and theoretical guarantees on the efficiency of the compressed online Sinkhorn algorithm.<details>
<summary>Abstract</summary>
The use of optimal transport (OT) distances, and in particular entropic-regularised OT distances, is an increasingly popular evaluation metric in many areas of machine learning and data science. Their use has largely been driven by the availability of efficient algorithms such as the Sinkhorn algorithm. One of the drawbacks of the Sinkhorn algorithm for large-scale data processing is that it is a two-phase method, where one first draws a large stream of data from the probability distributions, before applying the Sinkhorn algorithm to the discrete probability measures. More recently, there have been several works developing stochastic versions of Sinkhorn that directly handle continuous streams of data. In this work, we revisit the recently introduced online Sinkhorn algorithm of [Mensch and Peyr\'e, 2020]. Our contributions are twofold: We improve the convergence analysis for the online Sinkhorn algorithm, the new rate that we obtain is faster than the previous rate under certain parameter choices. We also present numerical results to verify the sharpness of our result. Secondly, we propose the compressed online Sinkhorn algorithm which combines measure compression techniques with the online Sinkhorn algorithm. We provide numerical experiments to show practical numerical gains, as well as theoretical guarantees on the efficiency of our approach.
</details>
<details>
<summary>摘要</summary>
使用最优运输距离（OT）和特别是减 entropy 规范化OT距离作为评价指标，在机器学习和数据科学中越来越受欢迎。其使用主要受到高效算法如沟道算法的支持。然而，沟道算法在大规模数据处理中有一个缺点，即需要先从概率分布中筛选出大量数据，然后应用沟道算法来处理离散概率度量。最近，有几篇论文开发了直接处理连续流数据的Stochastic Sinkhorn算法。在这篇文章中，我们回顾了2020年Mensch和Peyr\'e提出的在线沟道算法。我们的贡献有两点：首先，我们提高了在线沟道算法的收敛分析，新的速率比旧速率在某些参数选择下更快。其次，我们提出了压缩在线沟道算法，该算法结合了度量压缩技术和在线沟道算法。我们提供了数据实验来证明我们的方法具有实际数值优势，以及理论保证。
</details></li>
</ul>
<hr>
<h2 id="Human-in-the-loop-The-future-of-Machine-Learning-in-Automated-Electron-Microscopy"><a href="#Human-in-the-loop-The-future-of-Machine-Learning-in-Automated-Electron-Microscopy" class="headerlink" title="Human-in-the-loop: The future of Machine Learning in Automated Electron Microscopy"></a>Human-in-the-loop: The future of Machine Learning in Automated Electron Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05018">http://arxiv.org/abs/2310.05018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergei V. Kalinin, Yongtao Liu, Arpan Biswas, Gerd Duscher, Utkarsh Pratiush, Kevin Roccapriore, Maxim Ziatdinov, Rama Vasudevan</li>
<li>for: 这篇论文主要是为了介绍机器学习技术在电子顾问中的应用，以及如何通过人工智能自动化实验来提高实验效率和准确性。</li>
<li>methods: 该论文使用的方法包括机器学习算法和APIs，用于实时分析和控制微scopes的数据和操作。</li>
<li>results: 该论文提出了一种新的实验方法，称为人类在循环（hAE），其中人类操作员监督实验的进行，并通过调整机器学习算法的策略来引导实验向特定目标进行。<details>
<summary>Abstract</summary>
Machine learning methods are progressively gaining acceptance in the electron microscopy community for de-noising, semantic segmentation, and dimensionality reduction of data post-acquisition. The introduction of the APIs by major instrument manufacturers now allows the deployment of ML workflows in microscopes, not only for data analytics but also for real-time decision-making and feedback for microscope operation. However, the number of use cases for real-time ML remains remarkably small. Here, we discuss some considerations in designing ML-based active experiments and pose that the likely strategy for the next several years will be human-in-the-loop automated experiments (hAE). In this paradigm, the ML learning agent directly controls beam position and image and spectroscopy acquisition functions, and human operator monitors experiment progression in real- and feature space of the system and tunes the policies of the ML agent to steer the experiment towards specific objectives.
</details>
<details>
<summary>摘要</summary>
Here, we discuss some considerations for designing machine learning-based active experiments and suggest that the most likely approach for the next few years will be human-in-the-loop automated experiments (hAE). In this paradigm, the machine learning agent directly controls the beam position and image and spectroscopy acquisition functions, while the human operator monitors the experiment's progress in real- and feature space and adjusts the policies of the machine learning agent to steer the experiment towards specific objectives.
</details></li>
</ul>
<hr>
<h2 id="Prompt-augmented-Temporal-Point-Process-for-Streaming-Event-Sequence"><a href="#Prompt-augmented-Temporal-Point-Process-for-Streaming-Event-Sequence" class="headerlink" title="Prompt-augmented Temporal Point Process for Streaming Event Sequence"></a>Prompt-augmented Temporal Point Process for Streaming Event Sequence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04993">http://arxiv.org/abs/2310.04993</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanyanSann/PromptTPP">https://github.com/yanyanSann/PromptTPP</a></li>
<li>paper_authors: Siqiao Xue, Yan Wang, Zhixuan Chu, Xiaoming Shi, Caigao Jiang, Hongyan Hao, Gangwei Jiang, Xiaoyun Feng, James Y. Zhang, Jun Zhou</li>
<li>for: 本研究旨在 Addressing the challenge of continuous monitoring of Neural Temporal Point Processes (TPPs) for streaming event sequences, while ensuring privacy and memory constraints.</li>
<li>methods: 我们提出了一种简单 yet effective 框架 PromptTPP，它将基础 TPP 与一个 continuous-time retrieval prompt pool 集成，以便随着时间流动而学习流行事件序列。</li>
<li>results: 我们在三个实际用户行为数据集上展示了 PromptTPP 的优秀性能，并且在 Privacy 和 Memory 约束下实现了 Continual Learning。<details>
<summary>Abstract</summary>
Neural Temporal Point Processes (TPPs) are the prevalent paradigm for modeling continuous-time event sequences, such as user activities on the web and financial transactions. In real-world applications, event data is typically received in a \emph{streaming} manner, where the distribution of patterns may shift over time. Additionally, \emph{privacy and memory constraints} are commonly observed in practical scenarios, further compounding the challenges. Therefore, the continuous monitoring of a TPP to learn the streaming event sequence is an important yet under-explored problem. Our work paper addresses this challenge by adopting Continual Learning (CL), which makes the model capable of continuously learning a sequence of tasks without catastrophic forgetting under realistic constraints. Correspondingly, we propose a simple yet effective framework, PromptTPP\footnote{Our code is available at {\small \url{ https://github.com/yanyanSann/PromptTPP}}, by integrating the base TPP with a continuous-time retrieval prompt pool. The prompts, small learnable parameters, are stored in a memory space and jointly optimized with the base TPP, ensuring that the model learns event streams sequentially without buffering past examples or task-specific attributes. We present a novel and realistic experimental setup for modeling event streams, where PromptTPP consistently achieves state-of-the-art performance across three real user behavior datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Waveformer-for-modelling-dynamical-systems"><a href="#Waveformer-for-modelling-dynamical-systems" class="headerlink" title="Waveformer for modelling dynamical systems"></a>Waveformer for modelling dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04990">http://arxiv.org/abs/2310.04990</a></li>
<li>repo_url: None</li>
<li>paper_authors: N Navaneeth, Souvik Chakraborty</li>
<li>for: 学习解析方程的解 solutions</li>
<li>methods: 使用wavelet变换和transformers来捕捉解的空间多尺度行为和远距离动态</li>
<li>results: 在四个数学示例中，waveformer可以准确地学习解析方程的解，并在推算区域中表现出优于现有状态 искусственный智能算法，具体来说，waveformer可以在推算区域中准确预测解的动态行为，并且其性能在推算区域中比现有算法更高出至少一个数量级。<details>
<summary>Abstract</summary>
Neural operators have gained recognition as potent tools for learning solutions of a family of partial differential equations. The state-of-the-art neural operators excel at approximating the functional relationship between input functions and the solution space, potentially reducing computational costs and enabling real-time applications. However, they often fall short when tackling time-dependent problems, particularly in delivering accurate long-term predictions. In this work, we propose "waveformer", a novel operator learning approach for learning solutions of dynamical systems. The proposed waveformer exploits wavelet transform to capture the spatial multi-scale behavior of the solution field and transformers for capturing the long horizon dynamics. We present four numerical examples involving Burgers's equation, KS-equation, Allen Cahn equation, and Navier Stokes equation to illustrate the efficacy of the proposed approach. Results obtained indicate the capability of the proposed waveformer in learning the solution operator and show that the proposed Waveformer can learn the solution operator with high accuracy, outperforming existing state-of-the-art operator learning algorithms by up to an order, with its advantage particularly visible in the extrapolation region
</details>
<details>
<summary>摘要</summary>
Neural operators have gained recognition as powerful tools for learning solutions of a family of partial differential equations. The state-of-the-art neural operators excel at approximating the functional relationship between input functions and the solution space, potentially reducing computational costs and enabling real-time applications. However, they often fall short when tackling time-dependent problems, particularly in delivering accurate long-term predictions. In this work, we propose "waveformer", a novel operator learning approach for learning solutions of dynamical systems. The proposed waveformer exploits wavelet transform to capture the spatial multi-scale behavior of the solution field and transformers for capturing the long horizon dynamics. We present four numerical examples involving Burgers's equation, KS-equation, Allen Cahn equation, and Navier Stokes equation to illustrate the efficacy of the proposed approach. Results obtained indicate the capability of the proposed waveformer in learning the solution operator and show that the proposed Waveformer can learn the solution operator with high accuracy, outperforming existing state-of-the-art operator learning algorithms by up to an order, with its advantage particularly visible in the extrapolation region.Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Data-centric-Graph-Learning-A-Survey"><a href="#Data-centric-Graph-Learning-A-Survey" class="headerlink" title="Data-centric Graph Learning: A Survey"></a>Data-centric Graph Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04987">http://arxiv.org/abs/2310.04987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Yang, Deyu Bo, Jixi Liu, Yufei Peng, Boyu Chen, Haoran Dai, Ao Sun, Yue Yu, Yixin Xiao, Qi Zhang, Chunchen Wang, Yuxin Guo, Chuan Shi</li>
<li>for: 本文旨在探讨如何在深度学习时更好地处理图数据，以提高图模型的能力。</li>
<li>methods: 本文使用数据中心的方法，包括修改图数据的方法，以提高图模型的性能。</li>
<li>results: 本文提出了一种基于图学习管道的新分类法，并分析了图数据中的一些潜在问题，以及如何在数据中心的方法下解决这些问题。<details>
<summary>Abstract</summary>
The history of artificial intelligence (AI) has witnessed the significant impact of high-quality data on various deep learning models, such as ImageNet for AlexNet and ResNet. Recently, instead of designing more complex neural architectures as model-centric approaches, the attention of AI community has shifted to data-centric ones, which focuses on better processing data to strengthen the ability of neural models. Graph learning, which operates on ubiquitous topological data, also plays an important role in the era of deep learning. In this survey, we comprehensively review graph learning approaches from the data-centric perspective, and aim to answer two crucial questions: (1) when to modify graph data and (2) how to modify graph data to unlock the potential of various graph models. Accordingly, we propose a novel taxonomy based on the stages in the graph learning pipeline, and highlight the processing methods for different data structures in the graph data, i.e., topology, feature and label. Furthermore, we analyze some potential problems embedded in graph data and discuss how to solve them in a data-centric manner. Finally, we provide some promising future directions for data-centric graph learning.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）的历史见证了高质量数据对各种深度学习模型的重要影响，如ImageNet对AlexNet和ResNet。在最近，AI社区的注意力转移到了数据中心的方法，而不是设计更复杂的神经网络模型。图学习，它在深度学习时代处理普遍的 topological 数据，也扮演着重要的角色。在本综述中，我们从数据中心的角度全面回顾图学习方法，并试图回答两个关键问题：（1）何时修改图数据，以及（2）如何修改图数据以解锁不同图模型的潜力。因此，我们提出了一种新的分类方法，基于图学习管道中的阶段，并高亮了不同数据结构在图数据中的处理方法，即 topological、特征和标签。此外，我们分析了图数据中的一些可能的问题，并讨论了如何在数据中心的方法下解决这些问题。最后，我们提出了一些未来的可能性，以推动数据中心的图学习发展。
</details></li>
</ul>
<hr>
<h2 id="Model-adapted-Fourier-sampling-for-generative-compressed-sensing"><a href="#Model-adapted-Fourier-sampling-for-generative-compressed-sensing" class="headerlink" title="Model-adapted Fourier sampling for generative compressed sensing"></a>Model-adapted Fourier sampling for generative compressed sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04984">http://arxiv.org/abs/2310.04984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaron Berk, Simone Brugiapaglia, Yaniv Plan, Matthew Scott, Xia Sheng, Ozgur Yilmaz</li>
<li>for: 研究卷积感知抽象，即从固定单位矩阵随机抽取测量矩阵，DFT为重要特殊情况。</li>
<li>methods: 构建基于模型的采样策略，以提高采样复杂性为$\textit{O}(kd|\boldsymbol{\alpha}|_{2}^{2})$。这由两个步骤组成：首先发展新的非均匀随机采样分布的理论回归保证，然后优化采样分布以最小化采样次数。</li>
<li>results: 提出了一种适用于自然信号类型的采样复杂性，该采样复杂性可以在低卷积频率下实现高准确性。此外，对于代表采样方案进行了实验 validate。<details>
<summary>Abstract</summary>
We study generative compressed sensing when the measurement matrix is randomly subsampled from a unitary matrix (with the DFT as an important special case). It was recently shown that $\textit{O}(kdn\| \boldsymbol{\alpha}\|_{\infty}^{2})$ uniformly random Fourier measurements are sufficient to recover signals in the range of a neural network $G:\mathbb{R}^k \to \mathbb{R}^n$ of depth $d$, where each component of the so-called local coherence vector $\boldsymbol{\alpha}$ quantifies the alignment of a corresponding Fourier vector with the range of $G$. We construct a model-adapted sampling strategy with an improved sample complexity of $\textit{O}(kd\| \boldsymbol{\alpha}\|_{2}^{2})$ measurements. This is enabled by: (1) new theoretical recovery guarantees that we develop for nonuniformly random sampling distributions and then (2) optimizing the sampling distribution to minimize the number of measurements needed for these guarantees. This development offers a sample complexity applicable to natural signal classes, which are often almost maximally coherent with low Fourier frequencies. Finally, we consider a surrogate sampling scheme, and validate its performance in recovery experiments using the CelebA dataset.
</details>
<details>
<summary>摘要</summary>
我们研究生成式压缩感知（Generative Compressed Sensing），当测量矩阵随机抽取 Unitary 矩阵（DFT 为重要特殊情况）时。最近研究表明，$kdn\| \mathbf{\alpha}\|_{\infty}^{2}$ 随机 Fourier 测量可以重建信号，其中 $\mathbf{\alpha}$ 是所谓的本地协同向量，其中每个分量表示对应的 Fourier 向量与 $G$ 函数（$G:\mathbb{R}^k \to \mathbb{R}^n$）的谱的对应。我们构建了适应模型的采样策略，其sample complexity 为 $kd\| \mathbf{\alpha}\|_{2}^{2}$ 测量。这是由以下两个步骤实现的：首先，我们开发了非均匀随机采样分布的新理论恢复保证；其次，我们优化采样分布，以最小化需要的测量数量。这一发展可以应用于自然的信号类型，其通常是高频率下几乎最大的协同。最后，我们考虑了代理采样方案，并通过 CelebA 数据集的实验验证其性能。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Robustness-of-Multi-modal-Contrastive-Learning-to-Distribution-Shift"><a href="#Understanding-the-Robustness-of-Multi-modal-Contrastive-Learning-to-Distribution-Shift" class="headerlink" title="Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift"></a>Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04971">http://arxiv.org/abs/2310.04971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihao Xue, Siddharth Joshi, Dang Nguyen, Baharan Mirzasoleiman</li>
<li>for: 本文研究了multimodal contrastive learning（MMCL）方法在不同频谱上的表达学习，尤其是CLIP的成功。</li>
<li>methods: 本文使用了rigorous分析方法，探讨了MMCL的robustness机制，发现了两种机制：intra-class contrasting和inter-class feature sharing。</li>
<li>results: 本文的 teorical findings和实验结果表明，rich captions和annotating different types of details可以提高模型的robustness和zero-shot classification accuracy under distribution shift。<details>
<summary>Abstract</summary>
Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable representations is not understood. In this work, we rigorously analyze this problem and uncover two mechanisms behind MMCL's robustness: \emph{intra-class contrasting}, which allows the model to learn features with a high variance, and \emph{inter-class feature sharing}, where annotated details in one class help learning other classes better. Both mechanisms prevent spurious features that are over-represented in the training data to overshadow the generalizable core features. This yields superior zero-shot classification accuracy under distribution shift. Furthermore, we theoretically demonstrate the benefits of using rich captions on robustness and explore the effect of annotating different types of details in the captions. We validate our theoretical findings through experiments, including a well-designed synthetic experiment and an experiment involving training CLIP on MS COCO and evaluating the model on variations of shifted ImageNet.
</details>
<details>
<summary>摘要</summary>
近期，多模态对照学习（MMCL）方法，如CLIP，在学习对分布变化robust的表示方面取得了非常成功。尽管在实际中取得了成功，但是这种机制的底层原理尚未了解。在这项工作中，我们仔细分析了这个问题，并揭示了MMCL的 robustness的两种机制：内类对照（intra-class contrasting），允许模型学习具有高方差的特征，以及间类特征共享（inter-class feature sharing），其中一个类中的注解细节帮助学习其他类。这两种机制使得模型不会由训练数据中的假样特征所掩蔽。这 führt zu einer superior zero-shot classification accuracy under distribution shift. 此外，我们也 theoretically demonstrate了使用丰富的描述对robustness带来的好处，并 explore了不同类型的描述在描述中的效果。我们 validate our theoretical findings through experiments, including a well-designed synthetic experiment and an experiment involving training CLIP on MS COCO and evaluating the model on variations of shifted ImageNet.
</details></li>
</ul>
<hr>
<h2 id="Improved-Active-Learning-via-Dependent-Leverage-Score-Sampling"><a href="#Improved-Active-Learning-via-Dependent-Leverage-Score-Sampling" class="headerlink" title="Improved Active Learning via Dependent Leverage Score Sampling"></a>Improved Active Learning via Dependent Leverage Score Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04966">http://arxiv.org/abs/2310.04966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atsushi Shimizu, Xiaoou Cheng, Christopher Musco, Jonathan Weare</li>
<li>for: 这 paper 的目的是提出一种改进的活动学习方法，用于在agnostic（对抗噪声） Setting 中提高学习效果。</li>
<li>methods: 这 paper 使用了marginal leverage score sampling 和 non-independent sampling策略，以提高 espacial coverage 和减少样本数量。具体来说，这 paper 提出了一种基于 pivotal sampling 算法的方法，并在 parametric PDEs 和 uncertainty quantification 中进行了测试。</li>
<li>results: 相比于独立 sampling，这 paper 的方法可以减少到达给定准确率的样本数量，提高了效率。此外，paper 还提供了两个理论结论：一是任何非独立 leveragescore sampling 方法，如果它符合弱一侧 $\ell_{\infty}$ 独立性条件，可以活动学习 $d$ 维线性函数，只需要 $O(d\log d)$ 个样本。这一结论扩展了 recient work on matrix Chernoff bounds under $\ell_{\infty}$ independence，并可能对其他 sampling 策略进行分析。二是，对于重要的多项式回归问题，我们的 pivotal 方法可以获得 $O(d)$ 个样本的 bound。<details>
<summary>Abstract</summary>
We show how to obtain improved active learning methods in the agnostic (adversarial noise) setting by combining marginal leverage score sampling with non-independent sampling strategies that promote spatial coverage. In particular, we propose an easily implemented method based on the pivotal sampling algorithm, which we test on problems motivated by learning-based methods for parametric PDEs and uncertainty quantification. In comparison to independent sampling, our method reduces the number of samples needed to reach a given target accuracy by up to $50\%$. We support our findings with two theoretical results. First, we show that any non-independent leverage score sampling method that obeys a weak one-sided $\ell_{\infty}$ independence condition (which includes pivotal sampling) can actively learn $d$ dimensional linear functions with $O(d\log d)$ samples, matching independent sampling. This result extends recent work on matrix Chernoff bounds under $\ell_{\infty}$ independence, and may be of interest for analyzing other sampling strategies beyond pivotal sampling. Second, we show that, for the important case of polynomial regression, our pivotal method obtains an improved bound of $O(d)$ samples.
</details>
<details>
<summary>摘要</summary>
我们展示了如何在agnostic（反对抗噪）设定中获得改进的活动学习方法，通过融合margin leverage score抽样和非独立抽样策略以提高空间覆盖率。具体而言，我们提出了一个容易实现的方法，基于pivotal抽样算法，并在parametric PDEs和 uncertainty quantification中的问题上进行测试。与独立抽样相比，我们的方法可以降低到 дости���了一定精度的样本数量，低于独立抽样的50%。我们支持我们的结果通过两个理论成果：首先，我们显示任何非独立leverage score抽样方法，只要满足弱一边 $\ell_{\infty}$ 独立性条件（包括pivotal抽样），可以活动地学习 $d$ 维Linear function，只需要 $O(d\log d)$ 样本，与独立抽样相同。这个结果推进了最近matrix Chernoff bounds under $\ell_{\infty}$ 独立性的研究，并可能适用于分析其他抽样策略。其次，我们显示，在重要的多项 regression问题上，我们的pivotal方法可以获得 $O(d)$ 样本的改进 bound。
</details></li>
</ul>
<hr>
<h2 id="Towards-Explainable-Machine-Learning-The-Effectiveness-of-Reservoir-Computing-in-Wireless-Receive-Processing"><a href="#Towards-Explainable-Machine-Learning-The-Effectiveness-of-Reservoir-Computing-in-Wireless-Receive-Processing" class="headerlink" title="Towards Explainable Machine Learning: The Effectiveness of Reservoir Computing in Wireless Receive Processing"></a>Towards Explainable Machine Learning: The Effectiveness of Reservoir Computing in Wireless Receive Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04956">http://arxiv.org/abs/2310.04956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Jere, Karim Said, Lizhong Zheng, Lingjia Liu<br>for: This paper aims to improve the performance of channel equalization in wireless communications using a learning-based technique called Reservoir Computing (RC) and provide a first principles-based understanding of its operation.methods: The paper uses an echo state network (ESN) as a channel equalizer and incorporates available domain knowledge in the form of wireless channel statistics into the weights of the ESN model. This optimized initialization of the model weights leads to improved receive processing&#x2F;symbol detection performance.results: The paper shows improved performance in receive processing&#x2F;symbol detection through simulations, demonstrating the effectiveness of the proposed approach. This is a first step towards explainable machine learning (XML) and assigning practical model interpretability that can be utilized to improve performance and enhance detection reliability.<details>
<summary>Abstract</summary>
Deep learning has seen a rapid adoption in a variety of wireless communications applications, including at the physical layer. While it has delivered impressive performance in tasks such as channel equalization and receive processing/symbol detection, it leaves much to be desired when it comes to explaining this superior performance. In this work, we investigate the specific task of channel equalization by applying a popular learning-based technique known as Reservoir Computing (RC), which has shown superior performance compared to conventional methods and other learning-based approaches. Specifically, we apply the echo state network (ESN) as a channel equalizer and provide a first principles-based signal processing understanding of its operation. With this groundwork, we incorporate the available domain knowledge in the form of the statistics of the wireless channel directly into the weights of the ESN model. This paves the way for optimized initialization of the ESN model weights, which are traditionally untrained and randomly initialized. Finally, we show the improvement in receive processing/symbol detection performance with this optimized initialization through simulations. This is a first step towards explainable machine learning (XML) and assigning practical model interpretability that can be utilized together with the available domain knowledge to improve performance and enhance detection reliability.
</details>
<details>
<summary>摘要</summary>
深度学习在无线通信应用中得到了迅速的推广，包括物理层。尽管它在channel等化和接收处理/符号检测等任务中表现出色，但它在解释这种超越性表现的问题上留下了很多不满。在这项工作中，我们调查了通道等化的特定任务，通过应用 популяр的学习基于技术——储池计算（RC），该技术在其他学习基于方法和其他学习基于技术上表现出优异。具体来说，我们使用echo state网络（ESN）作为通道等化器，并提供了基于信号处理的基本原理的操作理解。通过这种基础，我们将可用的频率频道知识直接 integrate到ESN模型的权重中。这种方法可以为ESN模型的初始化提供优化，传统上是Random initialization的。最后，我们通过 simulations 表明了增强接收处理/符号检测性能的改进。这是对机器学习（XML）的第一步，它可以让模型解释性得到实践应用，并与可用的频率频道知识结合使用，以提高性能并增强检测可靠性。
</details></li>
</ul>
<hr>
<h2 id="Information-Theoretic-Bounds-on-The-Removal-of-Attribute-Specific-Bias-From-Neural-Networks"><a href="#Information-Theoretic-Bounds-on-The-Removal-of-Attribute-Specific-Bias-From-Neural-Networks" class="headerlink" title="Information-Theoretic Bounds on The Removal of Attribute-Specific Bias From Neural Networks"></a>Information-Theoretic Bounds on The Removal of Attribute-Specific Bias From Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04955">http://arxiv.org/abs/2310.04955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhi Li, Mahyar Khayatkhoei, Jiageng Zhu, Hanchen Xie, Mohamed E. Hussein, Wael AbdAlmageed</li>
<li>for: 本研究旨在探讨避免基于保护特征（如种族、性别、年龄）的神经网络预测中的偏见问题。</li>
<li>methods: 本研究使用了一些有前途的偏见除除法，但它们的局限性尚未得到充分探讨。</li>
<li>results: 研究发现，当数据集中存在强烈的偏见时，现有的偏见除除法只能在数据集中的偏见较弱时提供有效的性能。这些结论告诉我们在小型数据集中使用这些方法可能不够有效，并促使开发能够在强烈偏见情况下提供有效的方法。<details>
<summary>Abstract</summary>
Ensuring a neural network is not relying on protected attributes (e.g., race, sex, age) for predictions is crucial in advancing fair and trustworthy AI. While several promising methods for removing attribute bias in neural networks have been proposed, their limitations remain under-explored. In this work, we mathematically and empirically reveal an important limitation of attribute bias removal methods in presence of strong bias. Specifically, we derive a general non-vacuous information-theoretical upper bound on the performance of any attribute bias removal method in terms of the bias strength. We provide extensive experiments on synthetic, image, and census datasets to verify the theoretical bound and its consequences in practice. Our findings show that existing attribute bias removal methods are effective only when the inherent bias in the dataset is relatively weak, thus cautioning against the use of these methods in smaller datasets where strong attribute bias can occur, and advocating the need for methods that can overcome this limitation.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:保持神经网络不依赖保护属性（例如种族、性别、年龄）的预测是推进公正和可信的人工智能的关键。虽然一些有前途的属性偏见除除法已经被提出，但它们的限制尚未得到充分探讨。在这种工作中，我们数学和实验上 revela了属性偏见除除法的一个重要限制：即偏见强度的影响。我们 derivates一个普遍的非虚空的信息理论上限，用于衡量任何属性偏见除除法的性能。我们在 sintetic、图像和人口普查数据集上进行了广泛的实验，以验证理论上的上限和实际情况中的后果。我们的发现表明，现有的属性偏见除除法方法只有在数据集中的偏见强度较弱时才能够有效，因此对于小型数据集而言，存在强度偏见的情况下使用这些方法可能不太可靠，而需要开发能够超越这种限制的方法。
</details></li>
</ul>
<hr>
<h2 id="A-framework-to-generate-sparsity-inducing-regularizers-for-enhanced-low-rank-matrix-completion"><a href="#A-framework-to-generate-sparsity-inducing-regularizers-for-enhanced-low-rank-matrix-completion" class="headerlink" title="A framework to generate sparsity-inducing regularizers for enhanced low-rank matrix completion"></a>A framework to generate sparsity-inducing regularizers for enhanced low-rank matrix completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04954">http://arxiv.org/abs/2310.04954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Yong Wang, Hing Cheung So</li>
<li>for: 提出了一种架构，用于生成具有关闭式距离算子的SIR，并应用于矩阵完成低级别矩阵。</li>
<li>methods: 使用了半quadratic优化方法生成相关的regularizers，并使用了alternating direction method of multipliers（ADMM）开发算法。</li>
<li>results: 对约数据进行了extensive numerical experiments，并证明了方法的效果性和Runtime的优势。<details>
<summary>Abstract</summary>
Applying half-quadratic optimization to loss functions can yield the corresponding regularizers, while these regularizers are usually not sparsity-inducing regularizers (SIRs). To solve this problem, we devise a framework to generate an SIR with closed-form proximity operator. Besides, we specify our framework using several commonly-used loss functions, and produce the corresponding SIRs, which are then adopted as nonconvex rank surrogates for low-rank matrix completion. Furthermore, algorithms based on the alternating direction method of multipliers are developed. Extensive numerical results show the effectiveness of our methods in terms of recovery performance and runtime.
</details>
<details>
<summary>摘要</summary>
使半quadratice优化方法应用于损失函数可以得到相应的正则化项，但这些正则化项通常不是简洁化正则化项（SIR）。为解决这个问题，我们提出了一个框架，可以生成具有关闭形式距离运算器的SIR。此外，我们在多种通常使用的损失函数上Specify我们的框架，并生成相应的SIR，这些SIR然后被作为非对称矩阵完成的低级rankSurrogate采用。此外，我们还开发了基于多重方向分数法的算法。广泛的numerical实验表明我们的方法在完成性和运行时间方面具有良好的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/08/cs.LG_2023_10_08/" data-id="clombedve00pqs08876ay97al" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/18/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="page-number" href="/page/18/">18</a><span class="page-number current">19</span><a class="page-number" href="/page/20/">20</a><a class="page-number" href="/page/21/">21</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/20/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">113</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">63</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
