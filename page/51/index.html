
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/51/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_09_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/12/cs.AI_2023_09_12/" class="article-date">
  <time datetime="2023-09-12T12:00:00.000Z" itemprop="datePublished">2023-09-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/12/cs.AI_2023_09_12/">cs.AI - 2023-09-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Quantum-Data-Center-Perspectives"><a href="#Quantum-Data-Center-Perspectives" class="headerlink" title="Quantum Data Center: Perspectives"></a>Quantum Data Center: Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06641">http://arxiv.org/abs/2309.06641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Liu, Liang Jiang</li>
<li>for:  quantum computing, communication, and sensing</li>
<li>methods:  combining Quantum Random Access Memory (QRAM) and quantum networks</li>
<li>results:  significant benefits in efficiency, security, and precision for customers, with potential scientific and business opportunities in machine learning and big data industries.Here’s the text in Simplified Chinese:</li>
<li>for: 量子计算、量子通信和量子探测</li>
<li>methods: 结合量子随机访问存储器（QRAM）和量子网络</li>
<li>results: 为客户提供高效、安全、精度的 significative benefits，在机器学习和大数据行业中探讨可能的科学和商业机遇。<details>
<summary>Abstract</summary>
A quantum version of data centers might be significant in the quantum era. In this paper, we introduce Quantum Data Center (QDC), a quantum version of existing classical data centers, with a specific emphasis on combining Quantum Random Access Memory (QRAM) and quantum networks. We argue that QDC will provide significant benefits to customers in terms of efficiency, security, and precision, and will be helpful for quantum computing, communication, and sensing. We investigate potential scientific and business opportunities along this novel research direction through hardware realization and possible specific applications. We show the possible impacts of QDCs in business and science, especially the machine learning and big data industries.
</details>
<details>
<summary>摘要</summary>
一个量子版的数据中心可能在量子时代具有重要意义。在这篇论文中，我们介绍量子数据中心（QDC），量子版的现有古典数据中心，强调将量子随机访问存储（QRAM）和量子网络结合使用。我们认为QDC将为客户提供高效、安全和精度的好处，并将对量子计算、通信和探测做出重要贡献。我们通过硬件实现和可能的具体应用来研究这一新的研究方向的科学和商业机会。我们展示了QDC在业务和科学领域的可能的影响，特别是机器学习和大数据领域。
</details></li>
</ul>
<hr>
<h2 id="The-Relational-Bottleneck-as-an-Inductive-Bias-for-Efficient-Abstraction"><a href="#The-Relational-Bottleneck-as-an-Inductive-Bias-for-Efficient-Abstraction" class="headerlink" title="The Relational Bottleneck as an Inductive Bias for Efficient Abstraction"></a>The Relational Bottleneck as an Inductive Bias for Efficient Abstraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06629">http://arxiv.org/abs/2309.06629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taylor W. Webb, Steven M. Frankland, Awni Altabaa, Kamesh Krishnamurthy, Declan Campbell, Jacob Russin, Randall O’Reilly, John Lafferty, Jonathan D. Cohen</li>
<li>for: 本研究旨在解释如何从有限经验中获得抽象概念。</li>
<li>methods: 本研究使用了一种新的方法，即利用 inductive bias 来从数据中引出抽象。</li>
<li>results: 研究表明，这种方法可以在数据效率的情况下induce抽象，并且可能是人类大脑中抽象概念的获得的机制。<details>
<summary>Abstract</summary>
A central challenge for cognitive science is to explain how abstract concepts are acquired from limited experience. This effort has often been framed in terms of a dichotomy between empiricist and nativist approaches, most recently embodied by debates concerning deep neural networks and symbolic cognitive models. Here, we highlight a recently emerging line of work that suggests a novel reconciliation of these approaches, by exploiting an inductive bias that we term the relational bottleneck. We review a family of models that employ this approach to induce abstractions in a data-efficient manner, emphasizing their potential as candidate models for the acquisition of abstract concepts in the human mind and brain.
</details>
<details>
<summary>摘要</summary>
中心挑战是让抽象概念从有限经验中获得。这一努力 часто被划分为经验主义和Native主义方法，最近在深度神经网络和符号认知模型之间展开了讨论。我们在这里强调一种新出现的工作，通过利用我们称为关系瓶颈的推理偏好来解决这一问题。我们评论了一家族模型，这些模型通过这种方法来从数据有效地获得抽象，强调它们在人类大脑和脑中抽象概念的获得中的潜在作用。
</details></li>
</ul>
<hr>
<h2 id="A-Reinforcement-Learning-Approach-for-Robotic-Unloading-from-Visual-Observations"><a href="#A-Reinforcement-Learning-Approach-for-Robotic-Unloading-from-Visual-Observations" class="headerlink" title="A Reinforcement Learning Approach for Robotic Unloading from Visual Observations"></a>A Reinforcement Learning Approach for Robotic Unloading from Visual Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06621">http://arxiv.org/abs/2309.06621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vittoriogiammarino/rl-for-unloading-from-pixels">https://github.com/vittoriogiammarino/rl-for-unloading-from-pixels</a></li>
<li>paper_authors: Vittorio Giammarino, Alberto Giammarino, Matthew Pearce</li>
<li>For: 本研究强调使用视觉输入自动解压包裹问题，以便机器人可以通过RGB-D图像来学习无需标注数据。* Methods: 我们提出了一种层次控制结构，其中高级决策模块和传统的运动控制结合在一起。高级模块通过深度征识学习（DRL）进行训练，并采用安全偏好机制和适应于这个任务的奖励函数。* Results: 我们的实验表明，这两个元素都对学习性能产生了关键作用。此外，为确保可重复性和未来研究的标准，我们提供了免费代码和 simulate。<details>
<summary>Abstract</summary>
In this work, we focus on a robotic unloading problem from visual observations, where robots are required to autonomously unload stacks of parcels using RGB-D images as their primary input source. While supervised and imitation learning have accomplished good results in these types of tasks, they heavily rely on labeled data, which are challenging to obtain in realistic scenarios. Our study aims to develop a sample efficient controller framework that can learn unloading tasks without the need for labeled data during the learning process. To tackle this challenge, we propose a hierarchical controller structure that combines a high-level decision-making module with classical motion control. The high-level module is trained using Deep Reinforcement Learning (DRL), wherein we incorporate a safety bias mechanism and design a reward function tailored to this task. Our experiments demonstrate that both these elements play a crucial role in achieving improved learning performance. Furthermore, to ensure reproducibility and establish a benchmark for future research, we provide free access to our code and simulation.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们关注了基于视觉观察的 роботизирован无需卸车问题，Robot需要通过RGB-D图像作为主要输入来自动卸车堆叠的包裹。而supervised学习和模仿学习在这类任务中已经达到了良好的结果，但它们强依赖于实际场景中困难获得的标注数据。我们的研究旨在开发一种样本效率高的控制框架，可以在学习过程中不需要标注数据。为此，我们提议一种层次控制结构， combining高级决策模块和传统的运动控制。高级模块通过深度循环学习（DRL）进行训练，并在这个过程中添加了安全偏好机制和适应到这个任务的奖励函数。我们的实验表明，这两个元素具有重要的作用，可以提高学习性能。此外，为确保可重现性和建立未来研究的标准，我们提供了免费的代码和 simulations。
</details></li>
</ul>
<hr>
<h2 id="CloudBrain-NMR-An-Intelligent-Cloud-Computing-Platform-for-NMR-Spectroscopy-Processing-Reconstruction-and-Analysis"><a href="#CloudBrain-NMR-An-Intelligent-Cloud-Computing-Platform-for-NMR-Spectroscopy-Processing-Reconstruction-and-Analysis" class="headerlink" title="CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis"></a>CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07178">http://arxiv.org/abs/2309.07178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Guo, Sijin Li, Jun Liu, Zhangren Tu, Tianyu Qiu, Jingjing Xu, Liubin Feng, Donghai Lin, Qing Hong, Meijin Lin, Yanqin Lin, Xiaobo Qu</li>
<li>for: 这个研究的目的是提供一个智能在线云计算平台，用于核磁共振（NMR）数据处理、重构和定量分析。</li>
<li>methods: 该平台使用并行计算和图形处理器（GPU）&#x2F;中央处理器（CPU）的分布式计算技术，以提高计算效率和简化用户的操作。另外，它还 integrate了当前最佳的深度学习算法，以提供完整的处理过程，使用户无需安装其他软件。</li>
<li>results: 该平台可以快速地处理大量的NMR数据，并提供高精度的定量分析结果。此外，它还具有开放的API，可以让用户根据需要选择不同的处理流程和深度学习算法。<details>
<summary>Abstract</summary>
Nuclear Magnetic Resonance (NMR) spectroscopy has served as a powerful analytical tool for studying molecular structure and dynamics in chemistry and biology. However, the processing of raw data acquired from NMR spectrometers and subsequent quantitative analysis involves various specialized tools, which necessitates comprehensive knowledge in programming and NMR. Particularly, the emerging deep learning tools is hard to be widely used in NMR due to the sophisticated setup of computation. Thus, NMR processing is not an easy task for chemist and biologists. In this work, we present CloudBrain-NMR, an intelligent online cloud computing platform designed for NMR data reading, processing, reconstruction, and quantitative analysis. The platform is conveniently accessed through a web browser, eliminating the need for any program installation on the user side. CloudBrain-NMR uses parallel computing with graphics processing units and central processing units, resulting in significantly shortened computation time. Furthermore, it incorporates state-of-the-art deep learning-based algorithms offering comprehensive functionalities that allow users to complete the entire processing procedure without relying on additional software. This platform has empowered NMR applications with advanced artificial intelligence processing. CloudBrain-NMR is openly accessible for free usage at https://csrc.xmu.edu.cn/CloudBrain.html
</details>
<details>
<summary>摘要</summary>
核磁共振（NMR）分析是化学和生物研究中的一种强大工具，但是从NMR仪器上获取的原始数据处理和后续的量化分析需要各种专门的工具，这需要深入的编程和NMR知识。特别是在深度学习工具出现之前，NMR处理是非常困难的。为了解决这问题，我们提出了CloudBrain-NMR，一个基于云计算的智能在线平台，用于NMR数据的读取、处理、重构和量化分析。该平台通过浏览器访问，不需要用户安装任何软件。CloudBrain-NMR使用并行计算和图形处理器，实现了明显缩短计算时间。此外，它还包含了最新的深度学习算法，提供了全面的功能，让用户可以完成整个处理过程，不需要靠其他软件。这使得NMR应用程序得到了高级人工智能处理的 empowerment。CloudBrain-NMR是免费开放的，可以在https://csrc.xmu.edu.cn/CloudBrain.html中免费使用。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Algorithm-Selection-and-Hyperparameter-Tuning-on-Distributed-Machine-Learning-Resources-A-Hierarchical-Agent-based-Approach"><a href="#Hybrid-Algorithm-Selection-and-Hyperparameter-Tuning-on-Distributed-Machine-Learning-Resources-A-Hierarchical-Agent-based-Approach" class="headerlink" title="Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach"></a>Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06604">http://arxiv.org/abs/2309.06604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Esmaeili, Julia T. Rayz, Eric T. Matson</li>
<li>for: This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters.</li>
<li>methods: The proposed method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms.</li>
<li>results: According to the results, our solution is totally correct and exhibits linear time and space complexity in relation to the size of available resources. The proposed method is also demonstrated to be effective in adapting and performing across a range of algorithmic options and datasets through experiments using a system comprised of 24 algorithms and 9 datasets.<details>
<summary>Abstract</summary>
Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical study to demonstrate the correctness, resource utilization, and computational efficiency of our technique. According to the results, our solution is totally correct and exhibits linear time and space complexity in relation to the size of available resources. To provide concrete examples of how the proposed methodologies can effectively adapt and perform across a range of algorithmic options and datasets, we have also conducted a series of experiments using a system comprised of 24 algorithms and 9 datasets.
</details>
<details>
<summary>摘要</summary>
algorithm 选择和超参数调整是学术应用机器学习中的关键步骤，然而这些步骤正在不断增加、多样化和分布化机器学习资源的情况下变得越来越细腻。在机器学习平台的设计中，多智能体系统带来了规模、灵活性和稳定性等特点。本文提出了一种完全自动和协作的智能体基于机制，用于分布式组织机器学习算法和同时调整其超参数。我们的方法基于现有的智能体层次机器学习平台，并将其查询结构改进以支持上述功能性能不受特定学习、选择和调整机制限制。我们已经进行了理论评估、正式验证和分析研究，以证明我们的技术是完全正确的，并且在资源大小的情况下具有线性时间和空间复杂度。为了让读者更好地理解我们的方法在不同算法和数据集上的应用和效果，我们还进行了一系列实验，使用了24种算法和9个数据集。
</details></li>
</ul>
<hr>
<h2 id="Rank2Tell-A-Multimodal-Driving-Dataset-for-Joint-Importance-Ranking-and-Reasoning"><a href="#Rank2Tell-A-Multimodal-Driving-Dataset-for-Joint-Importance-Ranking-and-Reasoning" class="headerlink" title="Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning"></a>Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06597">http://arxiv.org/abs/2309.06597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enna Sachdeva, Nakul Agarwal, Suhas Chundi, Sean Roelofs, Jiachen Li, Behzad Dariush, Chiho Choi, Mykel Kochenderfer</li>
<li>for: 这篇论文的目的是提高自动驾驶车和高级驾驶支持系统的社会接受度，以便实现其广泛应用。</li>
<li>methods: 这篇论文使用了一种新的多模态 egocentric 数据集，即 Rank2Tell，来评估和描述复杂交通场景中重要对象的含义水平和原因。它还提出了一种联合模型，用于同时评估对象的重要程度和自然语言描述。</li>
<li>results: 根据论文的描述，Rank2Tell 数据集和联合模型在视觉场景理解和相关领域中提供了一个有价值的资源，并在量化评估中达到了高水平的性能。<details>
<summary>Abstract</summary>
The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Further, we introduce a joint model for joint importance level ranking and natural language captions generation to benchmark our dataset and demonstrate performance with quantitative evaluations.
</details>
<details>
<summary>摘要</summary>
广泛采用商业自动驾驶车（AV）和高级驾驶助手系统（ADAS）的普及可能受到社会的接受程度的限制，这个任务的难度在于现代自动驾驶系统软件借助黑盒人工智能模型。为达到这个目标，本文提出了一个新的数据集，即 Rank2Tell，这是一个多模态自我中心数据集，用于评估对象的重要性水平和说明其重要性的原因。该数据集通过多种close和开放式视觉问答来提供各种语义、空间、时间和关系特征的精密注释，使其成为研究视觉Scene理解和相关领域的价值资源。此外，我们还引入了一种共同模型，用于同时评估对象的重要性水平和生成自然语言描述，以 benchmark我们的数据集并进行评估性评价。
</details></li>
</ul>
<hr>
<h2 id="Do-Generative-Large-Language-Models-need-billions-of-parameters"><a href="#Do-Generative-Large-Language-Models-need-billions-of-parameters" class="headerlink" title="Do Generative Large Language Models need billions of parameters?"></a>Do Generative Large Language Models need billions of parameters?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06589">http://arxiv.org/abs/2309.06589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sia Gholami, Marwan Omar</li>
<li>for: 这个论文是为了开发高效的大语言模型（LLMs）而写的。</li>
<li>methods: 这篇论文探索了模型大小、性能和计算资源之间的贸易协议，以实现LLMs的高效性。它提出了新的方法，让不同的模型部分共享参数，从而减少总的唯一参数数量。这种方法可以保证模型尽量减少大小而不丢失复杂语言结构的学习和表示能力。</li>
<li>results: 这项研究提供了创新的工具和方法，可以创造更高效和有效的LLMs，为AI语言模型的可持续发展和普及做出了贡献。<details>
<summary>Abstract</summary>
This paper presents novel systems and methodologies for the development of efficient large language models (LLMs). It explores the trade-offs between model size, performance, and computational resources, with the aim of maximizing the efficiency of these AI systems. The research explores novel methods that allow different parts of the model to share parameters, reducing the total number of unique parameters required. This approach ensures that the model remains compact without sacrificing its ability to learn and represent complex language structures. This study provides valuable insights and tools for creating more efficient and effective LLMs, contributing to a more sustainable and accessible future for AI language modeling.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "efficient" is translated as "高效" (gāo yè)* "large language models" is translated as "大型语言模型" (dà xíng yǔ yán módel)* "trade-offs" is translated as "负担" (zhāng dāng)* "compact" is translated as "减少" (jiǎn shǎo)* "sustainable" is translated as "可持续" (kě chéng xù)* "accessible" is translated as "可达" (kě dà)
</details></li>
</ul>
<hr>
<h2 id="HurriCast-An-Automatic-Framework-Using-Machine-Learning-and-Statistical-Modeling-for-Hurricane-Forecasting"><a href="#HurriCast-An-Automatic-Framework-Using-Machine-Learning-and-Statistical-Modeling-for-Hurricane-Forecasting" class="headerlink" title="HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting"></a>HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07174">http://arxiv.org/abs/2309.07174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shouwei Gao, Meiyan Gao, Yuepeng Li, Wenqian Dong</li>
<li>for: 这个研究旨在提高飓风风险评估模型，以更好地预测飓风的发展趋势和强度。</li>
<li>methods: 该研究使用了ARIMA模型和K-MEANS算法，以及自适应神经网络，对飓风趋势和强度进行更加精准的预测。</li>
<li>results: 实验表明，这种混合方法可以准确地模拟历史飓风行为，并提供详细的未来轨迹和强度预测，对风险管理策略提供了有价值的参考。<details>
<summary>Abstract</summary>
Hurricanes present major challenges in the U.S. due to their devastating impacts. Mitigating these risks is important, and the insurance industry is central in this effort, using intricate statistical models for risk assessment. However, these models often neglect key temporal and spatial hurricane patterns and are limited by data scarcity. This study introduces a refined approach combining the ARIMA model and K-MEANS to better capture hurricane trends, and an Autoencoder for enhanced hurricane simulations. Our experiments show that this hybrid methodology effectively simulate historical hurricane behaviors while providing detailed projections of potential future trajectories and intensities. Moreover, by leveraging a comprehensive yet selective dataset, our simulations enrich the current understanding of hurricane patterns and offer actionable insights for risk management strategies.
</details>
<details>
<summary>摘要</summary>
飓风在美国 pose 严重挑战，因为它们可能导致毁灭性的影响。为了降低这些风险，保险业是关键的，使用复杂的统计模型进行风险评估。然而，这些模型经常忽略风暴的时间和空间特征，并且由于数据缺乏，受限于。本研究提出了一种改进的方法，结合ARIMA模型和K-MEANS，以更好地捕捉风暴趋势，并使用自适应神经网络进行增强的风暴模拟。我们的实验表明，这种混合方法可以准确地模拟历史风暴行为，并提供详细的未来轨迹和强度预测。此外，通过利用全面而选择性的数据集，我们的模拟提高了现有风暴模式的理解，并提供了有价值的风险管理策略。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Multi-Task-Learning-Framework-for-Session-based-Recommendations"><a href="#Hierarchical-Multi-Task-Learning-Framework-for-Session-based-Recommendations" class="headerlink" title="Hierarchical Multi-Task Learning Framework for Session-based Recommendations"></a>Hierarchical Multi-Task Learning Framework for Session-based Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06533">http://arxiv.org/abs/2309.06533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sejoon Oh, Walid Shalaby, Amir Afsharinejad, Xiquan Cui</li>
<li>for: 提高Session-based recommendation系统（SBRS）的预测精度和普适性。</li>
<li>methods: 使用层次多任务学习（H-MTL）架构，将预测任务与auxiliary任务设置在层次结构之间，从auxiliary任务中获得更多的输入特征，提高预测结果的可解释性。</li>
<li>results: 在两个Session-based recommendation数据集上，HierSRec比既有SBRS的next-item预测精度高，并且针对手动生成的候选项（例如4%的总ITEMS）进行可扩展的推理。<details>
<summary>Abstract</summary>
While session-based recommender systems (SBRSs) have shown superior recommendation performance, multi-task learning (MTL) has been adopted by SBRSs to enhance their prediction accuracy and generalizability further. Hierarchical MTL (H-MTL) sets a hierarchical structure between prediction tasks and feeds outputs from auxiliary tasks to main tasks. This hierarchy leads to richer input features for main tasks and higher interpretability of predictions, compared to existing MTL frameworks. However, the H-MTL framework has not been investigated in SBRSs yet. In this paper, we propose HierSRec which incorporates the H-MTL architecture into SBRSs. HierSRec encodes a given session with a metadata-aware Transformer and performs next-category prediction (i.e., auxiliary task) with the session encoding. Next, HierSRec conducts next-item prediction (i.e., main task) with the category prediction result and session encoding. For scalable inference, HierSRec creates a compact set of candidate items (e.g., 4% of total items) per test example using the category prediction. Experiments show that HierSRec outperforms existing SBRSs as per next-item prediction accuracy on two session-based recommendation datasets. The accuracy of HierSRec measured with the carefully-curated candidate items aligns with the accuracy of HierSRec calculated with all items, which validates the usefulness of our candidate generation scheme via H-MTL.
</details>
<details>
<summary>摘要</summary>
session-based recommender systems (SBRSs) 已经显示出了更高的推荐性能，但是多任务学习 (MTL) 已经被 SBRSs 采用以进一步提高其预测精度和泛化性。层次多任务学习 (H-MTL) 设置了一个层次结构 между预测任务和输出 auxiliary tasks 的输出。这个层次结构导致主任务的输入特征更加丰富，并且提高了预测结果的解释性，相比既有MTL框架。然而，H-MTL 框架在 SBRSs 中尚未被研究。在这篇论文中，我们提出了 HierSRec，它将 H-MTL 框架应用于 SBRSs。HierSRec 使用 metadata-aware Transformer 对给定的会话进行编码，然后使用会话编码进行下一个类型预测（即 auxiliary task）。接着，HierSRec 使用类型预测结果和会话编码进行下一个项目预测（即 main task）。为了可扩展的推理，HierSRec 创建了一个紧凑的候选项列表（例如，4% 的总项）每个测试示例，使用类型预测结果进行筛选。实验结果显示，HierSRec 在两个会话基于推荐数据集上的下一个项目预测精度上表现出色，与现有 SBRSs 相比。HierSRec 测试结果与我们精心准备的候选项列表相关，证明了我们的候选生成方案的有用性。
</details></li>
</ul>
<hr>
<h2 id="Minimum-Bayes’-Risk-Decoding-for-System-Combination-of-Grammatical-Error-Correction-Systems"><a href="#Minimum-Bayes’-Risk-Decoding-for-System-Combination-of-Grammatical-Error-Correction-Systems" class="headerlink" title="Minimum Bayes’ Risk Decoding for System Combination of Grammatical Error Correction Systems"></a>Minimum Bayes’ Risk Decoding for System Combination of Grammatical Error Correction Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06520">http://arxiv.org/abs/2309.06520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rainavyas/mbr_gec">https://github.com/rainavyas/mbr_gec</a></li>
<li>paper_authors: Vyas Raina, Mark Gales</li>
<li>for: 这个论文主要是为了提高grammatical error correction（GEC）系统的性能，具体来说是通过最小 bayes 风险（MBR）解码方法来改善系统输出的匹配度。</li>
<li>methods: 这篇论文使用的方法包括：MBR解码方法、current max-voting combination scheme、individual edit-level selection、以及不同的奖励度量在 MBR 解码框架中的应用。</li>
<li>results: 实验结果表明，使用提议的 MBR 解码方法可以提高 GEC 系统的性能，并且可以通过调整奖励度量来控制系统的精度、准确率和 F-score。<details>
<summary>Abstract</summary>
For sequence-to-sequence tasks it is challenging to combine individual system outputs. Further, there is also often a mismatch between the decoding criterion and the one used for assessment. Minimum Bayes' Risk (MBR) decoding can be used to combine system outputs in a manner that encourages better alignment with the final assessment criterion. This paper examines MBR decoding for Grammatical Error Correction (GEC) systems, where performance is usually evaluated in terms of edits and an associated F-score. Hence, we propose a novel MBR loss function directly linked to this form of criterion. Furthermore, an approach to expand the possible set of candidate sentences is described. This builds on a current max-voting combination scheme, as well as individual edit-level selection. Experiments on three popular GEC datasets and with state-of-the-art GEC systems demonstrate the efficacy of the proposed MBR approach. Additionally, the paper highlights how varying reward metrics within the MBR decoding framework can provide control over precision, recall, and the F-score in combined GEC systems.
</details>
<details>
<summary>摘要</summary>
For sequence-to-sequence tasks, it is challenging to combine individual system outputs. Furthermore, there is often a mismatch between the decoding criterion and the one used for assessment. Minimum Bayes' Risk (MBR) decoding can be used to combine system outputs in a manner that encourages better alignment with the final assessment criterion. This paper examines MBR decoding for Grammatical Error Correction (GEC) systems, where performance is usually evaluated in terms of edits and an associated F-score. Therefore, we propose a novel MBR loss function directly linked to this form of criterion. Additionally, an approach to expand the possible set of candidate sentences is described. This builds on a current max-voting combination scheme, as well as individual edit-level selection. Experiments on three popular GEC datasets and with state-of-the-art GEC systems demonstrate the efficacy of the proposed MBR approach. Moreover, the paper highlights how varying reward metrics within the MBR decoding framework can provide control over precision, recall, and the F-score in combined GEC systems.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Learning-Disentangled-Avatars-with-Hybrid-3D-Representations"><a href="#Learning-Disentangled-Avatars-with-Hybrid-3D-Representations" class="headerlink" title="Learning Disentangled Avatars with Hybrid 3D Representations"></a>Learning Disentangled Avatars with Hybrid 3D Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06441">http://arxiv.org/abs/2309.06441</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yfeng95/DELTA">https://github.com/yfeng95/DELTA</a></li>
<li>paper_authors: Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, Michael J. Black</li>
<li>for: 本研究旨在实现可动画化和真实化人类模拟器。</li>
<li>methods: 本paper使用混合explicit-implicit 3D表示方法，即DELTA方法，将人类模拟器分解成不同部分的mesh和神经网络频谱场。</li>
<li>results: 本paper实现了人类模拟器的分解，并在不同应用中表现出色，如分解人体和服装、分解面孔和头发等。此外，paper还发现了可以将头发和服装转移到不同的身体形态上。<details>
<summary>Abstract</summary>
Tremendous efforts have been made to learn animatable and photorealistic human avatars. Towards this end, both explicit and implicit 3D representations are heavily studied for a holistic modeling and capture of the whole human (e.g., body, clothing, face and hair), but neither representation is an optimal choice in terms of representation efficacy since different parts of the human avatar have different modeling desiderata. For example, meshes are generally not suitable for modeling clothing and hair. Motivated by this, we present Disentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit 3D representations. DELTA takes a monocular RGB video as input, and produces a human avatar with separate body and clothing/hair layers. Specifically, we demonstrate two important applications for DELTA. For the first one, we consider the disentanglement of the human body and clothing and in the second, we disentangle the face and hair. To do so, DELTA represents the body or face with an explicit mesh-based parametric 3D model and the clothing or hair with an implicit neural radiance field. To make this possible, we design an end-to-end differentiable renderer that integrates meshes into volumetric rendering, enabling DELTA to learn directly from monocular videos without any 3D supervision. Finally, we show that how these two applications can be easily combined to model full-body avatars, such that the hair, face, body and clothing can be fully disentangled yet jointly rendered. Such a disentanglement enables hair and clothing transfer to arbitrary body shapes. We empirically validate the effectiveness of DELTA's disentanglement by demonstrating its promising performance on disentangled reconstruction, virtual clothing try-on and hairstyle transfer. To facilitate future research, we also release an open-sourced pipeline for the study of hybrid human avatar modeling.
</details>
<details>
<summary>摘要</summary>
很大的努力已经投入到了人类动画和实际化人物模型的学习中。在这个领域，both explicit和implicit的3D表示都被广泛研究，以实现人类整体模型化和捕捉（例如，身体、衣服和头发），但 neither representation是优选的选择，因为不同的人物部分有不同的模型需求。例如，网格不适用于衣服和头发的模型。这种情况下，我们提出了Disentangled Avatars（DELTA），它使用了混合的explicit-implicit 3D表示来模型人类。DELTA通过一个灰度RGB视频输入，生成了一个人物模型，其中包括身体和衣服/头发层。具体来说，我们展示了两个重要的应用场景。在第一个应用场景中，我们考虑了人体和衣服的分离，在第二个应用场景中，我们分离了面孔和头发。为了实现这些应用场景，DELTA使用了一种由网格和神经辐射场组成的混合表示方法。为了实现这种方法，我们设计了一个端到端可微 differentiable 渲染器，该渲染器将网格 integrate into volumetric rendering，以便DELTA可以直接从灰度视频中学习，不需要任何3D监督。最后，我们表明了如何将这两个应用场景结合起来，以实现全身人物模型的分离和重新渲染。这种分离允许头发和衣服进行到任何身体形状的转移。我们通过实验证明了DELTA的分离性能的表现，包括分离重建、虚拟服装尝试和头发样式传输。为了促进未来的研究，我们还发布了一个开源的人类动画模型研究管道。
</details></li>
</ul>
<hr>
<h2 id="LEAP-Hand-Low-Cost-Efficient-and-Anthropomorphic-Hand-for-Robot-Learning"><a href="#LEAP-Hand-Low-Cost-Efficient-and-Anthropomorphic-Hand-for-Robot-Learning" class="headerlink" title="LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning"></a>LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06440">http://arxiv.org/abs/2309.06440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kenneth Shaw, Ananye Agarwal, Deepak Pathak</li>
<li>for: 这 paper 的目的是为了提供一种低成本的、人工智能研究用的多功能手。</li>
<li>methods: 这 paper 使用了一种新的机械结构，使得手臂在不同的手势状态下仍能保持最大的dexterity。此外，paper 还使用了 Machine Learning 技术进行 manipulate 任务的学习。</li>
<li>results: 这 paper 的实验结果表明，LEAP Hand 可以在真实世界中完成多种抓取任务，包括视觉 теле操作和学习从无动视频数据。LEAP Hand 在所有实验中都表现出色，而且与最近的竞争对手 Allegro Hand 相比，它的成本为 1&#x2F;8。<details>
<summary>Abstract</summary>
Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation. This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEAP Hand is low-cost and can be assembled in 4 hours at a cost of 2000 USD from readily available parts. It is capable of consistently exerting large torques over long durations of time. We show that LEAP Hand can be used to perform several manipulation tasks in the real world -- from visual teleoperation to learning from passive video data and sim2real. LEAP Hand significantly outperforms its closest competitor Allegro Hand in all our experiments while being 1/8th of the cost. We release detailed assembly instructions, the Sim2Real pipeline and a development platform with useful APIs on our website at https://leap-hand.github.io/
</details>
<details>
<summary>摘要</summary>
dexterous 操作已经是机器人领域的长期挑战。虽然机器学习技术已经显示了一定的承诺，但结果主要受到硬件的限制。在这篇论文中，我们介绍了LEAP手，一个低成本的手臂，用于机器学习研究。与之前的手臂不同，LEAP手具有新的骨骼结构，允许无论手指pose都能够达到最大的dexterity。LEAP手是低成本的，可以在4小时内为2000美元组装，使用可得到的部件。它可以在长时间内一直承受大的扭矩。我们表明LEAP手可以在真实世界中完成多种操作任务，从视觉操作到学习从无动视频数据和sim2real。LEAP手在所有实验中都能够superior于Allegro手，而且只有1/8的成本。我们在网站https://leap-hand.github.io/上发布了详细的组装指南，Sim2Real管道和开发平台，以及有用的API。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-potential-of-large-language-models-in-generating-semantic-and-cross-language-clones"><a href="#Unveiling-the-potential-of-large-language-models-in-generating-semantic-and-cross-language-clones" class="headerlink" title="Unveiling the potential of large language models in generating semantic and cross-language clones"></a>Unveiling the potential of large language models in generating semantic and cross-language clones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06424">http://arxiv.org/abs/2309.06424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Palash R. Roy, Ajmain I. Alam, Farouq Al-omari, Banani Roy, Chanchal K. Roy, Kevin A. Schneider</li>
<li>for: 这个论文主要目的是研究使用OpenAI的GPT模型进行semantic和cross-language代码副本生成，以便代码重用、代码理解、重构和性能测试。</li>
<li>methods: 该论文使用了SemanticCloneBench作为测试平台，通过对一系列代码片段进行评估，以评估GPT-3模型在生成代码副本方面的性能。</li>
<li>results: 研究发现，GPT-3模型在生成semantic和cross-language代码副本方面具有出色的表现，其中在semantic clones方面取得了62.14%的准确率和0.55 BLEU分数，在cross-language clones方面达到了91.25%的准确率。<details>
<summary>Abstract</summary>
Semantic and Cross-language code clone generation may be useful for code reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has potential in such clone generation as GPT is used for text generation. When developers copy/paste codes from Stack Overflow (SO) or within a system, there might be inconsistent changes leading to unexpected behaviours. Similarly, if someone possesses a code snippet in a particular programming language but seeks equivalent functionality in a different language, a semantic cross-language code clone generation approach could provide valuable assistance. In this study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 model could help generate semantic and cross-language clone variants for a given fragment.We have comprised a diverse set of code fragments and assessed GPT-3s performance in generating code variants.Through extensive experimentation and analysis, where 9 judges spent 158 hours to validate, we investigate the model's ability to produce accurate and semantically correct variants. Our findings shed light on GPT-3's strengths in code generation, offering insights into the potential applications and challenges of using advanced language models in software development. Our quantitative analysis yields compelling results. In the realm of semantic clones, GPT-3 attains an impressive accuracy of 62.14% and 0.55 BLEU score, achieved through few-shot prompt engineering. Furthermore, the model shines in transcending linguistic confines, boasting an exceptional 91.25% accuracy in generating cross-language clones
</details>
<details>
<summary>摘要</summary>
semantic和跨语言代码倾Copy generation可能有用于代码重用、代码理解、重构和benchmarking。OpenAI的GPT模型有潜力在这种倾Copy generation中，因为GPT是用于文本生成。当开发者从Stack Overflow（SO）或系统中复制代码时，可能会出现不一致的更改，导致意外的行为。 Similarly，如果某人拥有一个代码段在特定编程语言中，但寻找相同的功能在不同语言中，semantic cross-language code clone generation方法可以提供有价值的帮助。在本研究中，使用SemanticCloneBench作为载体，我们评估了GPT-3模型在给定副本中是否可以生成Semantic和跨语言倾Copy变体。我们组织了一个多样化的代码副本，并评估GPT-3模型在生成代码变体方面的能力。经过广泛的实验和分析，9名判icator在158小时内验证了我们的结论，我们调查了模型在代码生成方面的能力。我们的数据分析得出了有力的结果。在semantic倾Copy领域，GPT-3达到了62.14%的精度和0.55 BLEU分数，通过几个极少的提示工程来实现。此外，模型在跨语言倾Copy方面表现出色，达到了91.25%的精度。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Large-Language-Models-for-Ontology-Alignment"><a href="#Exploring-Large-Language-Models-for-Ontology-Alignment" class="headerlink" title="Exploring Large Language Models for Ontology Alignment"></a>Exploring Large Language Models for Ontology Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07172">http://arxiv.org/abs/2309.07172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krr-oxford/llmap-prelim">https://github.com/krr-oxford/llmap-prelim</a></li>
<li>paper_authors: Yuan He, Jiaoyan Chen, Hang Dong, Ian Horrocks</li>
<li>for: 这项研究探讨了最新的生成型大语言模型（LLMs）在ontology alignment中的可用性，以确定概念等价映射的跨 ontology 标注。</li>
<li>methods: 我们使用了 GPT 系列和 Flan-T5 等生成型大语言模型，并对挑战性训练集进行测试，以评估其零基础性表现。</li>
<li>results: 初步发现，LLMs 可能会超越现有的 ontology alignment 系统 like BERTMap，但需要注意framwork和提示设计。<details>
<summary>Abstract</summary>
This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design.
</details>
<details>
<summary>摘要</summary>
这个研究探讨了最近的生成型大型自然语言模型（LLM），如GPT系列和Flan-T5，在ontology alignment中的可行性，以确定 Ontology 中的概念相似映射。为了测试 Flan-T5-XXL 和 GPT-3.5-turbo 的零学习性能，我们利用了 OAEI Bio-ML 跟踪中的两个等价匹配数据集，考虑概念标签和结构上下文。初步发现，LLM 有可能超越现有的ontology alignment系统BERTMap，提供精心设计的框架和提示。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Mask-Networks"><a href="#Ensemble-Mask-Networks" class="headerlink" title="Ensemble Mask Networks"></a>Ensemble Mask Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06382">http://arxiv.org/abs/2309.06382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lok-18/GeSeNet">https://github.com/lok-18/GeSeNet</a></li>
<li>paper_authors: Jonny Luntzel</li>
<li>for: 这个研究是为了问题 $\mathbb{R}^n\rightarrow \mathbb{R}^n$ Feedforward 网络是否可以学习矩阵-向量乘法？</li>
<li>methods: 这个研究引入了两种机制：可变的面罩来处理矩阵输入，以及特有的网络剪辑来尊重面罩的依赖结构。</li>
<li>results: 研究表明，这些机制可以使网络模型近似固定操作，如矩阵-向量乘法 $\phi(A,x) \rightarrow Ax$，并有应用于测试依赖关系或交互顺序在图模型中。<details>
<summary>Abstract</summary>
Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learn matrix-vector multiplication? This study introduces two mechanisms - flexible masking to take matrix inputs, and a unique network pruning to respect the mask's dependency structure. Networks can approximate fixed operations such as matrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating the mechanisms introduced with applications towards litmus-testing dependencies or interaction order in graph-based models.
</details>
<details>
<summary>摘要</summary>
可以不是$\mathbb{R}^n\to\mathbb{R}^n$的Feedforward网络学习矩阵-向量乘法吗？这个研究提出了两种机制——灵活的面 masking来处理矩阵输入，以及特殊的网络剔除来尊重面的依赖结构。网络可以近似固定操作，如矩阵-向量乘法$\phi(A,x)\to Ax$，这些机制的引入鼓励了在图模型中进行考验依赖关系或交互顺序。
</details></li>
</ul>
<hr>
<h2 id="Style2Fab-Functionality-Aware-Segmentation-for-Fabricating-Personalized-3D-Models-with-Generative-AI"><a href="#Style2Fab-Functionality-Aware-Segmentation-for-Fabricating-Personalized-3D-Models-with-Generative-AI" class="headerlink" title="Style2Fab: Functionality-Aware Segmentation for Fabricating Personalized 3D Models with Generative AI"></a>Style2Fab: Functionality-Aware Segmentation for Fabricating Personalized 3D Models with Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06379">http://arxiv.org/abs/2309.06379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faraz Faruqi, Ahmed Katary, Tarik Hasic, Amira Abdel-Rahman, Nayeemur Rahman, Leandra Tejedor, Mackenzie Leake, Megan Hofmann, Stefanie Mueller</li>
<li>for: 这个论文旨在提供一种自动将3D模型分解成功能和艺术元素的方法，以便用户可以选择性地修改3D模型的艺术元素，不会影响模型的原始功能。</li>
<li>methods: 这种方法首先创建了3D模型的功能分类体系，然后使用这个体系进行半自动的分类，将3D模型分解成功能和艺术两个部分。</li>
<li>results: 研究人员通过对1000个来自Thingiverse的3D模型进行质量分析，创建了一个功能分类体系，并使用这个体系进行分类，以及一个名为Style2Fab的系统，允许用户选择性地修改3D模型的艺术元素，而不会影响模型的原始功能。<details>
<summary>Abstract</summary>
With recent advances in Generative AI, it is becoming easier to automatically manipulate 3D models. However, current methods tend to apply edits to models globally, which risks compromising the intended functionality of the 3D model when fabricated in the physical world. For example, modifying functional segments in 3D models, such as the base of a vase, could break the original functionality of the model, thus causing the vase to fall over. We introduce a method for automatically segmenting 3D models into functional and aesthetic elements. This method allows users to selectively modify aesthetic segments of 3D models, without affecting the functional segments. To develop this method we first create a taxonomy of functionality in 3D models by qualitatively analyzing 1000 models sourced from a popular 3D printing repository, Thingiverse. With this taxonomy, we develop a semi-automatic classification method to decompose 3D models into functional and aesthetic elements. We propose a system called Style2Fab that allows users to selectively stylize 3D models without compromising their functionality. We evaluate the effectiveness of our classification method compared to human-annotated data, and demonstrate the utility of Style2Fab with a user study to show that functionality-aware segmentation helps preserve model functionality.
</details>
<details>
<summary>摘要</summary>
To develop this method, we first created a taxonomy of functionality in 3D models by analyzing 1000 models from a popular 3D printing repository, Thingiverse. We then developed a semi-automatic classification method to decompose 3D models into functional and aesthetic elements. We call this system Style2Fab, and it allows users to selectively stylize 3D models without compromising their functionality.We evaluated the effectiveness of our classification method compared to human-annotated data and demonstrated the utility of Style2Fab with a user study. Our results show that functionality-aware segmentation helps preserve the model's functionality, and users can use Style2Fab to selectively stylize 3D models without worrying about compromising their intended use.
</details></li>
</ul>
<hr>
<h2 id="Grounded-Language-Acquisition-From-Object-and-Action-Imagery"><a href="#Grounded-Language-Acquisition-From-Object-and-Action-Imagery" class="headerlink" title="Grounded Language Acquisition From Object and Action Imagery"></a>Grounded Language Acquisition From Object and Action Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06335">http://arxiv.org/abs/2309.06335</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Robert Kubricht, Zhaoyuan Yang, Jianwei Qiu, Peter Henry Tu</li>
<li>for: 这 paper 的目的是研究如何使用深度学习方法来掌握视觉数据的Private语言表示。</li>
<li>methods: 该 paper 使用了 Referential game 环境和冲突学习环境来训练 Emergent Language（EL）Encoder&#x2F;Decoder，并使用了神经机器翻译和随机森林分类来将symbolic表示转化为类别标签。</li>
<li>results: 该 paper 在object recognition和action recognition两个实验中使用了这些方法，并使用了Grad-CAM和t-SNE方法来解释symbols生成的含义。<details>
<summary>Abstract</summary>
Deep learning approaches to natural language processing have made great strides in recent years. While these models produce symbols that convey vast amounts of diverse knowledge, it is unclear how such symbols are grounded in data from the world. In this paper, we explore the development of a private language for visual data representation by training emergent language (EL) encoders/decoders in both i) a traditional referential game environment and ii) a contrastive learning environment utilizing a within-class matching training paradigm. An additional classification layer utilizing neural machine translation and random forest classification was used to transform symbolic representations (sequences of integer symbols) to class labels. These methods were applied in two experiments focusing on object recognition and action recognition. For object recognition, a set of sketches produced by human participants from real imagery was used (Sketchy dataset) and for action recognition, 2D trajectories were generated from 3D motion capture systems (MOVI dataset). In order to interpret the symbols produced for data in each experiment, gradient-weighted class activation mapping (Grad-CAM) methods were used to identify pixel regions indicating semantic features which contribute evidence towards symbols in learned languages. Additionally, a t-distributed stochastic neighbor embedding (t-SNE) method was used to investigate embeddings learned by CNN feature extractors.
</details>
<details>
<summary>摘要</summary>
深度学习方法在自然语言处理方面已经做出了很大的进步。这些模型生成的符号表达了各种多样化的知识，但是不清楚这些符号如何与世界数据相关联。在这篇论文中，我们探索了在私人语言表达中发展的私人语言（EL）编码器/解码器，并在两种不同的环境中训练这些模型：一种传统的引用游戏环境和一种对比学习环境。此外，我们还使用神经机器翻译和随机森林分类来转换符号表达（序列数字符号）为类别标签。这些方法在两个实验中应用，一个是对象识别实验（Sketchy dataset），另一个是动作识别实验（MOVI dataset）。为了解释在每个实验中生成的符号，我们使用梯度权重分布映射（Grad-CAM）方法来确定符号中含有哪些Semantic特征，以及这些特征对象的证据。此外，我们还使用了高度分布随机邻居嵌入（t-SNE）方法来调查由Convolutional Neural Networks（CNN）特征提取器学习的嵌入。
</details></li>
</ul>
<hr>
<h2 id="Learning-Minimalistic-Tsetlin-Machine-Clauses-with-Markov-Boundary-Guided-Pruning"><a href="#Learning-Minimalistic-Tsetlin-Machine-Clauses-with-Markov-Boundary-Guided-Pruning" class="headerlink" title="Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning"></a>Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06315">http://arxiv.org/abs/2309.06315</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cair/tmu">https://github.com/cair/tmu</a></li>
<li>paper_authors: Ole-Christoffer Granmo, Per-Arne Andersen, Lei Jiao, Xuan Zhang, Christian Blakely, Tor Tveit</li>
<li>for: 本 paper 的目的是提出一种新的 Tsetlin Machine（TM） feedback scheme，用于找到 Markov boundary。</li>
<li>methods: 该 scheme 使用 Finite State Automaton - Context-Specific Independence Automaton，可以学习target variable 的 Markov boundary，从而在 TM 学习过程中减少不必要的特征。</li>
<li>results: 作者通过实验和理论分析，证明了该 scheme 可以充分利用上下文特定的独立性来找到 Markov boundary，并且可以提高 TM 的学习效率和准确性。<details>
<summary>Abstract</summary>
A set of variables is the Markov blanket of a random variable if it contains all the information needed for predicting the variable. If the blanket cannot be reduced without losing useful information, it is called a Markov boundary. Identifying the Markov boundary of a random variable is advantageous because all variables outside the boundary are superfluous. Hence, the Markov boundary provides an optimal feature set. However, learning the Markov boundary from data is challenging for two reasons. If one or more variables are removed from the Markov boundary, variables outside the boundary may start providing information. Conversely, variables within the boundary may stop providing information. The true role of each candidate variable is only manifesting when the Markov boundary has been identified. In this paper, we propose a new Tsetlin Machine (TM) feedback scheme that supplements Type I and Type II feedback. The scheme introduces a novel Finite State Automaton - a Context-Specific Independence Automaton. The automaton learns which features are outside the Markov boundary of the target, allowing them to be pruned from the TM during learning. We investigate the new scheme empirically, showing how it is capable of exploiting context-specific independence to find Markov boundaries. Further, we provide a theoretical analysis of convergence. Our approach thus connects the field of Bayesian networks (BN) with TMs, potentially opening up for synergies when it comes to inference and learning, including TM-produced Bayesian knowledge bases and TM-based Bayesian inference.
</details>
<details>
<summary>摘要</summary>
一个集合的变量是marks blanket的一个随机变量，如果它包含所有预测变量的信息，则称之为Markov bound。如果边界不能被缩小而失去有用的信息，则称之为Markov bound。标识随机变量的Markov bound是有利的，因为所有外部边界的变量都是 redundant。因此，Markov bound提供了一个优化的特征集。然而，从数据中学习Markov bound是困难的，因为如果一个或多个变量被从Markov bound中移除，外部边界上的变量可能会开始提供信息。相反，Markov bound内部的变量可能会停止提供信息。每个候选变量的真实角色只有在Markov bound已经被确定出来时才会表现出来。在这篇论文中，我们提出了一种新的Tsetlin Machine（TM）反馈方案，该方案附加了类型I和类型II反馈。方案使用了一个新的Finite State Automaton——Context-Specific Independence Automaton。机器学习以外的 automaton 可以学习随机变量的Markov bound，从而在TM学习过程中将其从TM中除除。我们对新方案进行了实验性研究，并证明了它可以利用上下文特定的独立性来找到Markov bound。我们还提供了一种理论分析的归一化。我们的方法因此将Bayesian networks（BN）和TM相连，可能会开拓新的可能性，包括TM生成的Bayesian知识库和TM基于Bayesian推理的推理。
</details></li>
</ul>
<hr>
<h2 id="AI4Food-NutritionFW-A-Novel-Framework-for-the-Automatic-Synthesis-and-Analysis-of-Eating-Behaviours"><a href="#AI4Food-NutritionFW-A-Novel-Framework-for-the-Automatic-Synthesis-and-Analysis-of-Eating-Behaviours" class="headerlink" title="AI4Food-NutritionFW: A Novel Framework for the Automatic Synthesis and Analysis of Eating Behaviours"></a>AI4Food-NutritionFW: A Novel Framework for the Automatic Synthesis and Analysis of Eating Behaviours</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06308">http://arxiv.org/abs/2309.06308</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bidalab/ai4food-nutritionfw">https://github.com/bidalab/ai4food-nutritionfw</a></li>
<li>paper_authors: Sergio Romero-Tapiador, Ruben Tolosana, Aythami Morales, Isabel Espinosa-Salinas, Gala Freixer, Julian Fierrez, Ruben Vera-Rodriguez, Enrique Carrillo de Santa Pau, Ana Ramírez de Molina, Javier Ortega-Garcia</li>
<li>for: 这个论文的目的是提出一种基于人工智能的食物图像数据集创建框架，以便研究食物图像分类和个性化推荐。</li>
<li>methods: 该论文使用了图像处理和人工智能技术，并提供了一个具有4,800个不同饮食习惯的食物图像数据集。</li>
<li>results: 该论文通过自动评估饮食习惯中的健康指数，并实现了99.53%和99.60%的准确率和敏感度。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Nowadays millions of images are shared on social media and web platforms. In particular, many of them are food images taken from a smartphone over time, providing information related to the individual's diet. On the other hand, eating behaviours are directly related to some of the most prevalent diseases in the world. Exploiting recent advances in image processing and Artificial Intelligence (AI), this scenario represents an excellent opportunity to: i) create new methods that analyse the individuals' health from what they eat, and ii) develop personalised recommendations to improve nutrition and diet under specific circumstances (e.g., obesity or COVID). Having tunable tools for creating food image datasets that facilitate research in both lines is very much needed.   This paper proposes AI4Food-NutritionFW, a framework for the creation of food image datasets according to configurable eating behaviours. AI4Food-NutritionFW simulates a user-friendly and widespread scenario where images are taken using a smartphone. In addition to the framework, we also provide and describe a unique food image dataset that includes 4,800 different weekly eating behaviours from 15 different profiles and 1,200 subjects. Specifically, we consider profiles that comply with actual lifestyles from healthy eating behaviours (according to established knowledge), variable profiles (e.g., eating out, holidays), to unhealthy ones (e.g., excess of fast food or sweets). Finally, we automatically evaluate a healthy index of the subject's eating behaviours using multidimensional metrics based on guidelines for healthy diets proposed by international organisations, achieving promising results (99.53% and 99.60% accuracy and sensitivity, respectively). We also release to the research community a software implementation of our proposed AI4Food-NutritionFW and the mentioned food image dataset created with it.
</details>
<details>
<summary>摘要</summary>
现在，数百万个图像在社交媒体和网络平台上被分享。特别是，许多这些图像是由智能手机拍摄的食物图像，提供了关于个人的饮食信息。然而，饮食习惯直接关联了世界上许多最常见的疾病。利用最新的图像处理技术和人工智能（AI），这种情况表现出了优秀的机遇，可以：i) 创建新的方法，从饮食中获取个人健康信息，ii) 为特定情况（如肥胖或 COVID）提供个性化的饮食建议。有一个可调的工具集，用于创建饮食图像集，是研究这两个方面的非常需要。  本文提出了 AI4Food-NutritionFW 框架，用于创建饮食图像集，根据可配置的饮食习惯。 AI4Food-NutritionFW 模拟了一种用户友好、广泛的场景，在智能手机上拍摄图像。除框架外，我们还提供了一个唯一的饮食图像集，包含 4,800 个不同的每周饮食习惯，来自 15 个 profiles 和 1,200 个主题。特别是，我们考虑了遵循实际生活方式的健康饮食习惯（根据已知的知识）、变化 profiles（例如，吃外卖、假日），以及不健康的习惯（例如，过量快餐或糖果）。最后，我们自动评估主题的饮食习惯健康指数，使用多维度指标，基于国际组织提出的健康饮食指南，达到了非常有 promise 的结果（99.53% 和 99.60% 的准确率和敏感度，分别）。我们还向研究社区发布了我们所提出的 AI4Food-NutritionFW 和饮食图像集。
</details></li>
</ul>
<hr>
<h2 id="Transferability-analysis-of-data-driven-additive-manufacturing-knowledge-a-case-study-between-powder-bed-fusion-and-directed-energy-deposition"><a href="#Transferability-analysis-of-data-driven-additive-manufacturing-knowledge-a-case-study-between-powder-bed-fusion-and-directed-energy-deposition" class="headerlink" title="Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition"></a>Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06286">http://arxiv.org/abs/2309.06286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutahar Safdar, Jiarui Xie, Hyunwoong Ko, Yan Lu, Guy Lamouche, Yaoyao Fiona Zhao</li>
<li>for: 本研究旨在提供一种基于数据的知识传递分析框架，以支持在不同加工过程之间传递数据驱动的AM知识。</li>
<li>methods: 本研究使用了一种三步知识传递分析框架，包括预传递、传递和后传递步骤。在预传递步骤中，AM知识被抽象为特征化的知识组件。</li>
<li>results: 研究表明，可以成功将LPBF处理技术中的数据驱动解决方案传递到DED处理技术中，并在不同数据表示、模型架构和模型参数层次上进行了成功传递。<details>
<summary>Abstract</summary>
Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years. This has led to a plethora of scientific literature to emerge. The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that have not been mined and formalized in an integrated way. Moreover, no tools or guidelines exist to support data-driven knowledge transfer from one context to another. As a result, data-driven solutions using specific AI techniques are being developed and validated only for specific AM process technologies. There is a potential to exploit the inherent similarities across various AM technologies and adapt the existing solutions from one process or problem to another using AI, such as Transfer Learning. We propose a three-step knowledge transferability analysis framework in AM to support data-driven AM knowledge transfer. As a prerequisite to transferability analysis, AM knowledge is featurized into identified knowledge components. The framework consists of pre-transfer, transfer, and post-transfer steps to accomplish knowledge transfer. A case study is conducted between flagship metal AM processes. Laser Powder Bed Fusion (LPBF) is the source of knowledge motivated by its relative matureness in applying AI over Directed Energy Deposition (DED), which drives the need for knowledge transfer as the less explored target process. We show successful transfer at different levels of the data-driven solution, including data representation, model architecture, and model parameters. The pipeline of AM knowledge transfer can be automated in the future to allow efficient cross-context or cross-process knowledge exchange.
</details>
<details>
<summary>摘要</summary>
“数据驱动的研究在添加制造（AM）领域在最近几年内取得了重要成功。这导致了一大量的科学文献出现。这些文献中的知识包括AM和人工智能（AI）上下文的知识，它们没有被综合化和系统化地挖掘。此外，没有任何工具或指南来支持数据驱动知识的转移 между不同的上下文。因此，为了解决特定的AM过程技术中的问题，数据驱动解决方案使用特定的AI技术进行开发和验证。这有一定的潜在利用AM过程技术之间的共同特征，并将现有的解决方案从一个过程或问题中转移到另一个过程或问题中使用AI技术，例如传输学习。我们提议一个三步知识转移可行性分析框架在AM中支持数据驱动AM知识转移。在转移可行性分析之前，AM知识被特征化为识别出来的知识组件。该框架包括先转移、转移和后转移三个步骤，以完成知识转移。一个案例研究在标志性金属AM过程之间进行了转移。用激光粉末充电（LPBF）作为知识来源，因为它在应用AI方面更加成熟，而 Directed Energy Deposition（DED）更是一个未经探索的目标过程，这导致了知识转移的需求。我们在不同数据驱动解决方案的各级上成功进行了转移，包括数据表示、模型架构和模型参数。将来，AM知识转移管道可以被自动化，以实现跨上下文或跨过程的高效知识交换。”
</details></li>
</ul>
<hr>
<h2 id="Jersey-Number-Recognition-using-Keyframe-Identification-from-Low-Resolution-Broadcast-Videos"><a href="#Jersey-Number-Recognition-using-Keyframe-Identification-from-Low-Resolution-Broadcast-Videos" class="headerlink" title="Jersey Number Recognition using Keyframe Identification from Low-Resolution Broadcast Videos"></a>Jersey Number Recognition using Keyframe Identification from Low-Resolution Broadcast Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06285">http://arxiv.org/abs/2309.06285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bavesh Balaji, Jerrin Bright, Harish Prakash, Yuhao Chen, David A Clausi, John Zelek</li>
<li>for:  automatic jersey number detection in sports videos</li>
<li>methods:  spatio-temporal network, multi-task loss function</li>
<li>results:  significant increase in accuracy (37.81% and 37.70%)<details>
<summary>Abstract</summary>
Player identification is a crucial component in vision-driven soccer analytics, enabling various downstream tasks such as player assessment, in-game analysis, and broadcast production. However, automatically detecting jersey numbers from player tracklets in videos presents challenges due to motion blur, low resolution, distortions, and occlusions. Existing methods, utilizing Spatial Transformer Networks, CNNs, and Vision Transformers, have shown success in image data but struggle with real-world video data, where jersey numbers are not visible in most of the frames. Hence, identifying frames that contain the jersey number is a key sub-problem to tackle. To address these issues, we propose a robust keyframe identification module that extracts frames containing essential high-level information about the jersey number. A spatio-temporal network is then employed to model spatial and temporal context and predict the probabilities of jersey numbers in the video. Additionally, we adopt a multi-task loss function to predict the probability distribution of each digit separately. Extensive evaluations on the SoccerNet dataset demonstrate that incorporating our proposed keyframe identification module results in a significant 37.81% and 37.70% increase in the accuracies of 2 different test sets with domain gaps. These results highlight the effectiveness and importance of our approach in tackling the challenges of automatic jersey number detection in sports videos.
</details>
<details>
<summary>摘要</summary>
player identification是视觉驱动足球分析中的关键组件，允许多个下渠道任务，如玩家评估、游戏分析和直播生产。然而，自动从视频中检测篮球号码存在很多挑战，包括运动模糊、低分辨率、扭曲和遮挡。现有方法，使用空间变换网络、CNN和视觉变换器，在图像数据上表现出成功，但在真实世界视频数据上却表现不佳，因为篮球号码在大多数帧中不可见。因此，确定包含篮球号码的帧是关键的子问题。为解决这些问题，我们提议一种可靠的关键帧标识模块，该模块可以提取包含篮球号码的高级信息的帧。然后，我们采用了一种空间-时间网络，以模拟空间和时间上下文，并预测视频中篮球号码的概率。此外，我们采用了多任务损失函数，以预测每个数字的概率分布。我们在SoccerNet数据集进行了广泛的评估，结果表明，将我们提议的关锥帧标识模块integrated into our approach，可以提高视频中篮球号码自动检测的准确率，相对于不含该模块的情况，提高37.81%和37.70%。这些结果 highlights the effectiveness and importance of our approach in tackling the challenges of automatic jersey number detection in sports videos.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Multi-modal-Cooperation-via-Fine-grained-Modality-Valuation"><a href="#Enhancing-Multi-modal-Cooperation-via-Fine-grained-Modality-Valuation" class="headerlink" title="Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation"></a>Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06255">http://arxiv.org/abs/2309.06255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu</li>
<li>for: 本研究旨在 JOINTLY integrating 不同模式的数据，以提高多模式学习的性能。</li>
<li>methods: 我们提出了一种细化模式价值指标，用于评估每个样本中每个模式的贡献。通过模式价值评估，我们发现多模式模型往往依赖于一个特定的模式，导致其他模式成为低贡献的。我们进一步分析这一问题，并通过提高低贡献模式的抑制能力来改善多模式模型的合作。</li>
<li>results: 我们的方法可以有效地评估每个样本中每个模式的贡献，并实现了不同多模式模型中的显著提高。<details>
<summary>Abstract</summary>
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this issue and improve cooperation between modalities by enhancing the discriminative ability of low-contributing modalities in a targeted manner. Overall, our methods reasonably observe the fine-grained uni-modal contribution at sample-level and achieve considerable improvement on different multi-modal models.
</details>
<details>
<summary>摘要</summary>
（使用简化字符串）一个主要的多样性学习主题是将不同Modalities中的异质数据集合在一起。然而，大多数模型通常会受到不满意的多样性合作，无法有效地使用所有Modalities。一些方法可以识别和提高不好学习的Modalities，但是往往无法在样本水平提供细化的多样性合作观察。因此，我们需要合理地观察和改进多样性合作，尤其是在面临现实情况下，模态差异可能会随样本不同而变化。为此，我们引入细化的模态价值度量来评估每个模态的样本级贡献。通过模态价值评估，我们 regretfully 发现，多模态模型往往会依赖于一个具体的模态，导致其他模态成为低贡献的。我们进一步分析这一问题，并通过提高低贡献模态的推诊能力来改善多样性合作。总的来说，我们的方法可以合理地观察细化的单模态贡献，并在不同的多模态模型上实现显著改进。
</details></li>
</ul>
<hr>
<h2 id="On-the-Injunction-of-XAIxArt"><a href="#On-the-Injunction-of-XAIxArt" class="headerlink" title="On the Injunction of XAIxArt"></a>On the Injunction of XAIxArt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06227">http://arxiv.org/abs/2309.06227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheshta Arora, Debarun Sarkar</li>
<li>for: 本论文探讨了透明人工智能在艺术领域（XAIxArt）中的纠纷。</li>
<li>methods: 文章通过一系列快速问题，探讨了“解释”和“相关的解释”的混淆性。文章拒绝了“解释”和“相关的解释”， argue  that XAIxArt 是人类中心艺术的不安和害怕旧有作者和人类活动的回归。</li>
<li>results: 文章通过区分了 ornamentation 模型和 sense-making 模型来证明这一观点。<details>
<summary>Abstract</summary>
The position paper highlights the range of concerns that are engulfed in the injunction of explainable artificial intelligence in art (XAIxArt). Through a series of quick sub-questions, it points towards the ambiguities concerning 'explanation' and the postpositivist tradition of 'relevant explanation'. Rejecting both 'explanation' and 'relevant explanation', the paper takes a stance that XAIxArt is a symptom of insecurity of the anthropocentric notion of art and a nostalgic desire to return to outmoded notions of authorship and human agency. To justify this stance, the paper makes a distinction between an ornamentation model of explanation to a model of explanation as sense-making.
</details>
<details>
<summary>摘要</summary>
文章发表于XAIxArt的各种问题，包括'解释'和'有用的解释'的歧义，以及人类中心艺术的不安和宁静愿返回过时的作者和人类活动。文章根据解释模型和意义解释模型的区别，提出了这种姿势。Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Signle-Bit-Flip-Attacks-on-DNN-Executables"><a href="#Unveiling-Signle-Bit-Flip-Attacks-on-DNN-Executables" class="headerlink" title="Unveiling Signle-Bit-Flip Attacks on DNN Executables"></a>Unveiling Signle-Bit-Flip Attacks on DNN Executables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06223">http://arxiv.org/abs/2309.06223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanzuo Chen, Zhibo Liu, Yuanyuan Yuan, Sihang Hu, Tianxiang Li, Shuai Wang</li>
<li>for: 防护深度学习模型免受Bit-flip攻击</li>
<li>methods: 使用自动搜索工具发现深度学习模型漏洞，并利用模型结构在深度学习模型执行器中发现实际攻击方向</li>
<li>results: 发现深度学习模型执行器中存在广泛、严重（如单位位异常）和可传递的攻击表面，这些攻击表面不存在于高级深度学习框架中的模型 weights，可以让攻击者控制输出标签<details>
<summary>Abstract</summary>
Recent research has shown that bit-flip attacks (BFAs) can manipulate deep neural networks (DNNs) via DRAM Rowhammer exploitations. Existing attacks are primarily launched over high-level DNN frameworks like PyTorch and flip bits in model weight files. Nevertheless, DNNs are frequently compiled into low-level executables by deep learning (DL) compilers to fully leverage low-level hardware primitives. The compiled code is usually high-speed and manifests dramatically distinct execution paradigms from high-level DNN frameworks.   In this paper, we launch the first systematic study on the attack surface of BFA specifically for DNN executables compiled by DL compilers. We design an automated search tool to identify vulnerable bits in DNN executables and identify practical attack vectors that exploit the model structure in DNN executables with BFAs (whereas prior works make likely strong assumptions to attack model weights). DNN executables appear more "opaque" than models in high-level DNN frameworks. Nevertheless, we find that DNN executables contain extensive, severe (e.g., single-bit flip), and transferrable attack surfaces that are not present in high-level DNN models and can be exploited to deplete full model intelligence and control output labels. Our finding calls for incorporating security mechanisms in future DNN compilation toolchains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SCP-Scene-Completion-Pre-training-for-3D-Object-Detection"><a href="#SCP-Scene-Completion-Pre-training-for-3D-Object-Detection" class="headerlink" title="SCP: Scene Completion Pre-training for 3D Object Detection"></a>SCP: Scene Completion Pre-training for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06199">http://arxiv.org/abs/2309.06199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Shan, Yan Xia, Yuhong Chen, Daniel Cremers</li>
<li>for: 提高3D物体探测器的性能，使其需要更少的标注数据。</li>
<li>methods: 使用Scene Completion Pre-training（SCP）方法，通过完成点云场景，更好地捕捉城市环境中物体之间的空间和Semantic关系，并消除需要额外数据集的需求。</li>
<li>results: 使用SCP方法可以使现有的状态体际3D探测器达到相同的性能，只需要20%的标注数据。<details>
<summary>Abstract</summary>
3D object detection using LiDAR point clouds is a fundamental task in the fields of computer vision, robotics, and autonomous driving. However, existing 3D detectors heavily rely on annotated datasets, which are both time-consuming and prone to errors during the process of labeling 3D bounding boxes. In this paper, we propose a Scene Completion Pre-training (SCP) method to enhance the performance of 3D object detectors with less labeled data. SCP offers three key advantages: (1) Improved initialization of the point cloud model. By completing the scene point clouds, SCP effectively captures the spatial and semantic relationships among objects within urban environments. (2) Elimination of the need for additional datasets. SCP serves as a valuable auxiliary network that does not impose any additional efforts or data requirements on the 3D detectors. (3) Reduction of the amount of labeled data for detection. With the help of SCP, the existing state-of-the-art 3D detectors can achieve comparable performance while only relying on 20% labeled data.
</details>
<details>
<summary>摘要</summary>
三维对象检测使用激光点云是计算机视觉、 робо控和自动驾驶等领域的基本任务。然而，现有的三维检测器均依赖于标注过的数据集，这些数据集的标注过程昂贵且容易出错。在这篇论文中，我们提出了场景完成预训练（SCP）方法，以提高三维对象检测器的性能，并且需要更少的标注数据。SCP具有以下三个优势：1. 提高点云模型的初始化。通过完善场景点云，SCP可以有效地捕捉城市环境中物体之间的空间和semantic关系。2. 消除需要更多数据集的需求。SCP作为一个有价值的辅助网络，不需要额外的努力或数据要求。3. 降低检测需要的标注数据量。通过SCP的帮助，现有的状态对检测器可以在20%标注数据的情况下实现相同的性能。
</details></li>
</ul>
<hr>
<h2 id="360-circ-from-a-Single-Camera-A-Few-Shot-Approach-for-LiDAR-Segmentation"><a href="#360-circ-from-a-Single-Camera-A-Few-Shot-Approach-for-LiDAR-Segmentation" class="headerlink" title="360$^\circ$ from a Single Camera: A Few-Shot Approach for LiDAR Segmentation"></a>360$^\circ$ from a Single Camera: A Few-Shot Approach for LiDAR Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06197">http://arxiv.org/abs/2309.06197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurenz Reichardt, Nikolas Ebert, Oliver Wasenmüller</li>
<li>for: 这篇论文旨在提出一种有效和快速的 label-efficient LiDAR 分割方法，以增进现有方法的精度和可靠性。</li>
<li>methods: 该方法使用了一个图像教师网络来生成 LiDAR 数据中的 semantic 预测，并将其用于预训练 LiDAR 分割学生网络。可选的是进行360度数据的精度调整。</li>
<li>results: 该方法可以超过现有的标注效率方法的结果，并且在一些传统的完全监督分割网络之上还取得了更高的性能。<details>
<summary>Abstract</summary>
Deep learning applications on LiDAR data suffer from a strong domain gap when applied to different sensors or tasks. In order for these methods to obtain similar accuracy on different data in comparison to values reported on public benchmarks, a large scale annotated dataset is necessary. However, in practical applications labeled data is costly and time consuming to obtain. Such factors have triggered various research in label-efficient methods, but a large gap remains to their fully-supervised counterparts. Thus, we propose ImageTo360, an effective and streamlined few-shot approach to label-efficient LiDAR segmentation. Our method utilizes an image teacher network to generate semantic predictions for LiDAR data within a single camera view. The teacher is used to pretrain the LiDAR segmentation student network, prior to optional fine-tuning on 360$^\circ$ data. Our method is implemented in a modular manner on the point level and as such is generalizable to different architectures. We improve over the current state-of-the-art results for label-efficient methods and even surpass some traditional fully-supervised segmentation networks.
</details>
<details>
<summary>摘要</summary>
深度学习应用于激光数据受到不同感知器或任务的域隔差很强。为了使这些方法在不同数据上达到类似准确性，需要一个大规模的注意力标注数据集。然而，在实际应用中，标注数据昂贵和时间消耗。这些因素引发了各种研究label-efficient方法，但与完全监督方法之间仍有大的差距。因此，我们提出ImageTo360，一种高效的几极shot方法 для标签efficient LiDAR分割。我们的方法使用图像教师网络生成激光数据中的semantic预测，并在单个相机视图中使用这些预测来预训练LiDAR分割学生网络。我们的方法实现在点级别上，可以与不同的架构进行拓展。我们超越当前状态的域隔差标签方法，甚至超过了一些传统的完全监督分割网络。
</details></li>
</ul>
<hr>
<h2 id="A-3M-Hybrid-Model-for-the-Restoration-of-Unique-Giant-Murals-A-Case-Study-on-the-Murals-of-Yongle-Palace"><a href="#A-3M-Hybrid-Model-for-the-Restoration-of-Unique-Giant-Murals-A-Case-Study-on-the-Murals-of-Yongle-Palace" class="headerlink" title="A 3M-Hybrid Model for the Restoration of Unique Giant Murals: A Case Study on the Murals of Yongle Palace"></a>A 3M-Hybrid Model for the Restoration of Unique Giant Murals: A Case Study on the Murals of Yongle Palace</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06194">http://arxiv.org/abs/2309.06194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Yang, Nur Intan Raihana Ruhaiyem, Chichun Zhou</li>
<li>for:  restore the Yongle Palace murals, which are valuable cultural heritage but have suffered damage</li>
<li>methods:  propose a 3M-Hybrid model that leverages a pre-trained Vision Transformer model (VIT) and a multi-scale and multi-perspective strategy to address the challenges of domain bias and large defect restoration</li>
<li>results:  improve SSIM and PSNR by 14.61% and 4.73%, respectively, compared to the best model among four representative CNN models, and achieve favorable results in the final restoration of giant murals.Here’s the full text in Simplified Chinese:</li>
<li>for: Restore the Yongle Palace murals, which are valuable cultural heritage but have suffered damage.</li>
<li>methods: 提出3M-Hybrid模型，利用预训练的视图转换器模型（VIT）和多尺度多角度策略，解决传统传输学学习方法中的领域偏见问题，以及大 défaut的 restore 问题。</li>
<li>results: 与最佳四种表征 CNN 模型比较，提高 SSIM 和 PSNR 指标的提升率分别为14.61%和4.73%，并在大 défaut的最终 restore 问题上获得了良好的结果。<details>
<summary>Abstract</summary>
The Yongle Palace murals, as valuable cultural heritage, have suffered varying degrees of damage, making their restoration of significant importance. However, the giant size and unique data of Yongle Palace murals present challenges for existing deep-learning based restoration methods: 1) The distinctive style introduces domain bias in traditional transfer learning-based restoration methods, while the scarcity of mural data further limits the applicability of these methods. 2) Additionally, the giant size of these murals results in a wider range of defect types and sizes, necessitating models with greater adaptability. Consequently, there is a lack of focus on deep learning-based restoration methods for the unique giant murals of Yongle Palace. Here, a 3M-Hybrid model is proposed to address these challenges. Firstly, based on the characteristic that the mural data frequency is prominent in the distribution of low and high frequency features, high and low frequency features are separately abstracted for complementary learning. Furthermore, we integrate a pre-trained Vision Transformer model (VIT) into the CNN module, allowing us to leverage the benefits of a large model while mitigating domain bias. Secondly, we mitigate seam and structural distortion issues resulting from the restoration of large defects by employing a multi-scale and multi-perspective strategy, including data segmentation and fusion. Experimental results demonstrate the efficacy of our proposed model. In regular-sized mural restoration, it improves SSIM and PSNR by 14.61% and 4.73%, respectively, compared to the best model among four representative CNN models. Additionally, it achieves favorable results in the final restoration of giant murals.
</details>
<details>
<summary>摘要</summary>
永乐宫壁画，作为文化遗产，受到不同程度的损害，因此 restore 的重要性提高。然而，永乐宫壁画的巨大大小和独特数据带来了现有深度学习基于 restore 方法的挑战：1）宫壁画的特殊风格引入传统 transfer learning 基于 restore 方法中的领域偏见，而且宫壁画数据的罕见性更限制了这些方法的应用。2）此外，宫壁画的巨大大小导致了更多的缺陷类型和大小，需要更加适应性强的模型。因此，对于永乐宫壁画独特的巨大宫壁画，深度学习基于 restore 方法受到了不 enough 的关注。在这里，我们提出了一种3M-Hybrid模型，以解决这些挑战。首先，基于宫壁画数据频谱在低频和高频特征之间的分布，我们分别抽取了高频和低频特征进行 complementary learning。其次，我们将预训练的 Vision Transformer 模型（VIT）integrated 到 CNN 模块中，以利用大型模型的优势，同时避免领域偏见。其次，我们使用多比例和多视角策略来mitigate 修复大缺陷的问题，包括数据分割和融合。实验结果表明，我们提出的模型在 regular-sized 宫壁画修复中提高了 SSIM 和 PSNR 指标的值，相比最佳四种 CNN 模型，提高了14.61%和4.73%。此外，它在巨大宫壁画的最终修复中也获得了良好的结果。
</details></li>
</ul>
<hr>
<h2 id="Glancing-Future-for-Simultaneous-Machine-Translation"><a href="#Glancing-Future-for-Simultaneous-Machine-Translation" class="headerlink" title="Glancing Future for Simultaneous Machine Translation"></a>Glancing Future for Simultaneous Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06179">http://arxiv.org/abs/2309.06179</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/glance-simt">https://github.com/ictnlp/glance-simt</a></li>
<li>paper_authors: Shoutao Guo, Shaolei Zhang, Yang Feng</li>
<li>for: 提高同域机器翻译模型的翻译能力</li>
<li>methods: 使用课程学习方法，从整个句子逐渐减少可用的源语言信息，以便在 prefix2prefix 训练中增强模型的翻译能力</li>
<li>results: 比 STRONG 基eline 高效，且适用于各种同域机器翻译方法<details>
<summary>Abstract</summary>
Simultaneous machine translation (SiMT) outputs translation while reading the source sentence. Unlike conventional sequence-to-sequence (seq2seq) training, existing SiMT methods adopt the prefix-to-prefix (prefix2prefix) training, where the model predicts target tokens based on partial source tokens. However, the prefix2prefix training diminishes the ability of the model to capture global information and introduces forced predictions due to the absence of essential source information. Consequently, it is crucial to bridge the gap between the prefix2prefix training and seq2seq training to enhance the translation capability of the SiMT model. In this paper, we propose a novel method that glances future in curriculum learning to achieve the transition from the seq2seq training to prefix2prefix training. Specifically, we gradually reduce the available source information from the whole sentence to the prefix corresponding to that latency. Our method is applicable to a wide range of SiMT methods and experiments demonstrate that our method outperforms strong baselines.
</details>
<details>
<summary>摘要</summary>
同时机器翻译（SiMT）输出翻译 mientras lee la oración de fuente. 与现有的序列到序列（seq2seq）训练不同，现有的 SiMT 方法采用了 prefix-to-prefix（prefix2prefix）训练，其中模型预测目标Token基于部分源Token。然而， prefix2prefix 训练减少了模型捕捉全局信息的能力，并导致强制预测因为缺少必要的源信息。因此，它是必要的 bridge  seq2seq 训练和 prefix2prefix 训练，以提高 SiMT 模型的翻译能力。在这篇论文中，我们提出了一种新的方法，通过观察未来的劳动学习来实现这种过渡。具体来说，我们逐渐减少了可用的源信息从整个句子到对应的遅延。我们的方法适用于各种 SiMT 方法，并且实验表明，我们的方法超过了强大的基eline。
</details></li>
</ul>
<hr>
<h2 id="Robust-MBDL-A-Robust-Multi-branch-Deep-Learning-Based-Model-for-Remaining-Useful-Life-Prediction-and-Operational-Condition-Identification-of-Rotating-Machines"><a href="#Robust-MBDL-A-Robust-Multi-branch-Deep-Learning-Based-Model-for-Remaining-Useful-Life-Prediction-and-Operational-Condition-Identification-of-Rotating-Machines" class="headerlink" title="Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines"></a>Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06157">http://arxiv.org/abs/2309.06157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khoa Tran, Hai-Canh Vu, Lam Pham, Nassim Boudaoud</li>
<li>for: 预测扭矩机器的剩余有用生命（RUL）和状况操作（CO）</li>
<li>methods: 提posed system包括主要组件：1）LSTM自适应网络去噪振荡数据; 2）特征提取从去噪数据中生成时域、频域和时频基于特征; 3）一种新型和可靠的多支lines deep learning网络架构，利用多个特征</li>
<li>results: 对两个基准数据集XJTU-SY和PRONOSTIA进行了性能评估，结果表明我们提posed系统在RUL和CO预测方面准确率高，与当前最佳系统相比，具有实际应用潜力。<details>
<summary>Abstract</summary>
In this paper, a Robust Multi-branch Deep learning-based system for remaining useful life (RUL) prediction and condition operations (CO) identification of rotating machines is proposed. In particular, the proposed system comprises main components: (1) an LSTM-Autoencoder to denoise the vibration data; (2) a feature extraction to generate time-domain, frequency-domain, and time-frequency based features from the denoised data; (3) a novel and robust multi-branch deep learning network architecture to exploit the multiple features. The performance of our proposed system was evaluated and compared to the state-of-the-art systems on two benchmark datasets of XJTU-SY and PRONOSTIA. The experimental results prove that our proposed system outperforms the state-of-the-art systems and presents potential for real-life applications on bearing machines.
</details>
<details>
<summary>摘要</summary>
本文提出了一种基于深度学习的多支分支系统，用于预测旋转机器的剩余有用生命（RUL）和状况操作（CO）。特别是，提案的系统包括主要组成部分：1. LSTM自适应神经网络来排除振荡数据中的噪声;2. 特征提取来生成时域、频域和时频基于特征;3. 一种新的和可靠的多支分支深度学习网络架构，以利用多个特征。我们提出的系统的性能被评估并与现有系统进行比较，使用了两个XJTU-SY和PRONOSTIA的数据集。实验结果表明，我们的提案系统在RUL预测和CO识别方面表现出色，并有可能应用于真实的滚珍机器。
</details></li>
</ul>
<hr>
<h2 id="Measuring-vagueness-and-subjectivity-in-texts-from-symbolic-to-neural-VAGO"><a href="#Measuring-vagueness-and-subjectivity-in-texts-from-symbolic-to-neural-VAGO" class="headerlink" title="Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO"></a>Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06132">http://arxiv.org/abs/2309.06132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Icard, Vincent Claveau, Ghislain Atemezing, Paul Égré</li>
<li>for: 本研究旨在开发一种自动测量文本模糊和主观性的方法。</li>
<li>methods: 我们首先介绍了专家系统VAGO，然后在一个小 benchmark上证明了它在fact vs. opinion句子上的效果，然后在更大的法语新闻词汇库FreSaDa上进行了对比，并证明了讽刺文章中的主观标志更为常见。最后，我们基于BERT-like架构建立了一个神经网络版本VAGO，并通过LIME的解释工具来证明其对symbolic VAGO scores的增强和其他语言版本的生成的重要性。</li>
<li>results: 研究结果表明，神经网络版本VAGO在 FreSaDa 上的表现更好，并且可以增强 symbolic VAGO scores 的lexicons。此外，神经网络版本还可以生成其他语言版本，并且可以通过LIME的解释工具来了解它们的工作原理。<details>
<summary>Abstract</summary>
We present a hybrid approach to the automated measurement of vagueness and subjectivity in texts. We first introduce the expert system VAGO, we illustrate it on a small benchmark of fact vs. opinion sentences, and then test it on the larger French press corpus FreSaDa to confirm the higher prevalence of subjective markers in satirical vs. regular texts. We then build a neural clone of VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores obtained on FreSaDa. Using explainability tools (LIME), we show the interest of this neural version for the enrichment of the lexicons of the symbolic version, and for the production of versions in other languages.
</details>
<details>
<summary>摘要</summary>
我们提出了一种混合方法来自动量化文本中的uncertainty和主观性。我们首先介绍了专家系统VAGO，然后在一个小的对比实验中使用它对fact vs. opinion句子进行了示例，然后在更大的法国报纸词汇 corpus FreSaDa 上进行了测试，以确认在幽默 VS. 常规文本中的主观标记的更高频率。然后，我们建立了一个基于BERT-like架构的神经网络副本，并使用LIME Explainability工具来展示其在符号式 VAGO 分数中的利用性和在其他语言中生成版本的可能性。
</details></li>
</ul>
<hr>
<h2 id="JOADAA-joint-online-action-detection-and-action-anticipation"><a href="#JOADAA-joint-online-action-detection-and-action-anticipation" class="headerlink" title="JOADAA: joint online action detection and action anticipation"></a>JOADAA: joint online action detection and action anticipation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06130">http://arxiv.org/abs/2309.06130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Guermal, Francois Bremond, Rui Dai, Abid Ali</li>
<li>for: 这两个任务的缺失完整的知识集（过去、当前和未来）使得推断动作依赖关系困难，从而影响性能。</li>
<li>methods: 我们提议将这两个任务 fusion into a single uniform architecture，通过结合动作预测和在线动作检测，以捕捉未来信息的潜在相互关系。</li>
<li>results: 我们在三个挑战性 dataset（THUMOS’14、CHARADES和Multi-THUMOS）上验证了我们的提议模型（JOADAA），并 achieved SOTA results for both tasks。<details>
<summary>Abstract</summary>
Action anticipation involves forecasting future actions by connecting past events to future ones. However, this reasoning ignores the real-life hierarchy of events which is considered to be composed of three main parts: past, present, and future. We argue that considering these three main parts and their dependencies could improve performance. On the other hand, online action detection is the task of predicting actions in a streaming manner. In this case, one has access only to the past and present information. Therefore, in online action detection (OAD) the existing approaches miss semantics or future information which limits their performance. To sum up, for both of these tasks, the complete set of knowledge (past-present-future) is missing, which makes it challenging to infer action dependencies, therefore having low performances. To address this limitation, we propose to fuse both tasks into a single uniform architecture. By combining action anticipation and online action detection, our approach can cover the missing dependencies of future information in online action detection. This method referred to as JOADAA, presents a uniform model that jointly performs action anticipation and online action detection. We validate our proposed model on three challenging datasets: THUMOS'14, which is a sparsely annotated dataset with one action per time step, CHARADES, and Multi-THUMOS, two densely annotated datasets with more complex scenarios. JOADAA achieves SOTA results on these benchmarks for both tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate="no"Action anticipation involves forecasting future actions by connecting past events to future ones. However, this reasoning ignores the real-life hierarchy of events, which is composed of three main parts: past, present, and future. We argue that considering these three main parts and their dependencies could improve performance. On the other hand, online action detection is the task of predicting actions in a streaming manner. In this case, one has access only to the past and present information. Therefore, in online action detection (OAD), the existing approaches miss semantics or future information, which limits their performance. To sum up, for both of these tasks, the complete set of knowledge (past-present-future) is missing, which makes it challenging to infer action dependencies, therefore having low performances. To address this limitation, we propose to fuse both tasks into a single uniform architecture. By combining action anticipation and online action detection, our approach can cover the missing dependencies of future information in online action detection. This method, referred to as JOADAA, presents a uniform model that jointly performs action anticipation and online action detection. We validate our proposed model on three challenging datasets: THUMOS'14, which is a sparsely annotated dataset with one action per time step, CHARADES, and Multi-THUMOS, two densely annotated datasets with more complex scenarios. JOADAA achieves SOTA results on these benchmarks for both tasks.Note: I've kept the original text's sentence structure and vocabulary as much as possible, but some words and phrases may have been adjusted slightly to fit the Simplified Chinese grammar and idiomatic expressions.
</details></li>
</ul>
<hr>
<h2 id="LEyes-A-Lightweight-Framework-for-Deep-Learning-Based-Eye-Tracking-using-Synthetic-Eye-Images"><a href="#LEyes-A-Lightweight-Framework-for-Deep-Learning-Based-Eye-Tracking-using-Synthetic-Eye-Images" class="headerlink" title="LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images"></a>LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06129">http://arxiv.org/abs/2309.06129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dcnieho/byrneetal_leyes">https://github.com/dcnieho/byrneetal_leyes</a></li>
<li>paper_authors: sean anthony byrne, virmarie maquiling, marcus nyström, enkelejda kasneci, diederick c. niehorster</li>
<li>For:  This paper aims to address the problem of inadequate training datasets for gaze estimation techniques, which has hindered the deployment of deep learning models in real-world applications.* Methods: The proposed framework, called Light Eyes (LEyes), uses simple light distributions to model key image features required for video-based eye tracking, facilitating easy configuration for training neural networks across diverse gaze-estimation tasks.* Results: The authors demonstrate that models trained using LEyes outperform other state-of-the-art algorithms in terms of pupil and CR localization across well-known datasets, and a LEyes trained model outperforms the industry standard eye tracker using significantly more cost-effective hardware.<details>
<summary>Abstract</summary>
Deep learning has bolstered gaze estimation techniques, but real-world deployment has been impeded by inadequate training datasets. This problem is exacerbated by both hardware-induced variations in eye images and inherent biological differences across the recorded participants, leading to both feature and pixel-level variance that hinders the generalizability of models trained on specific datasets. While synthetic datasets can be a solution, their creation is both time and resource-intensive. To address this problem, we present a framework called Light Eyes or "LEyes" which, unlike conventional photorealistic methods, only models key image features required for video-based eye tracking using simple light distributions. LEyes facilitates easy configuration for training neural networks across diverse gaze-estimation tasks. We demonstrate that models trained using LEyes outperform other state-of-the-art algorithms in terms of pupil and CR localization across well-known datasets. In addition, a LEyes trained model outperforms the industry standard eye tracker using significantly more cost-effective hardware. Going forward, we are confident that LEyes will revolutionize synthetic data generation for gaze estimation models, and lead to significant improvements of the next generation video-based eye trackers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fidelity-Induced-Interpretable-Policy-Extraction-for-Reinforcement-Learning"><a href="#Fidelity-Induced-Interpretable-Policy-Extraction-for-Reinforcement-Learning" class="headerlink" title="Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning"></a>Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06097">http://arxiv.org/abs/2309.06097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Liu, Wubing Chen, Mao Tan</li>
<li>for: 提高深度强化学习（DRL）代理人的可读性和可信度，使用者能够了解代理人的决策过程和弱点。</li>
<li>methods: 基于新的可读性增强机制， FIPE 方法通过评估已有 IPE 方法的优化问题，并添加了一个准确度评估指标，以提高代理人的决策可读性和一致性。</li>
<li>results: 在 StarCraft II 复杂的控制环境中进行实验，FIPE 方法在交互性性和一致性两个方面都超过了基eline，同时易于理解。<details>
<summary>Abstract</summary>
Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making problems. However, existing DRL agents make decisions in an opaque fashion, hindering the user from establishing trust and scrutinizing weaknesses of the agents. While recent research has developed Interpretable Policy Extraction (IPE) methods for explaining how an agent takes actions, their explanations are often inconsistent with the agent's behavior and thus, frequently fail to explain. To tackle this issue, we propose a novel method, Fidelity-Induced Policy Extraction (FIPE). Specifically, we start by analyzing the optimization mechanism of existing IPE methods, elaborating on the issue of ignoring consistency while increasing cumulative rewards. We then design a fidelity-induced mechanism by integrate a fidelity measurement into the reinforcement learning feedback. We conduct experiments in the complex control environment of StarCraft II, an arena typically avoided by current IPE methods. The experiment results demonstrate that FIPE outperforms the baselines in terms of interaction performance and consistency, meanwhile easy to understand.
</details>
<details>
<summary>摘要</summary>
Specifically, we start by analyzing the optimization mechanism of existing IPE methods, highlighting the issue of ignoring consistency while increasing cumulative rewards. We then design a fidelity-induced mechanism by integrating a fidelity measurement into the reinforcement learning feedback. We conduct experiments in the complex control environment of StarCraft II, an environment typically avoided by current IPE methods. The experiment results demonstrate that FIPE outperforms the baselines in terms of interaction performance and consistency, while being easy to understand.
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Framework-to-Deconstruct-the-Primary-Drivers-for-Electricity-Market-Price-Events"><a href="#A-Machine-Learning-Framework-to-Deconstruct-the-Primary-Drivers-for-Electricity-Market-Price-Events" class="headerlink" title="A Machine Learning Framework to Deconstruct the Primary Drivers for Electricity Market Price Events"></a>A Machine Learning Framework to Deconstruct the Primary Drivers for Electricity Market Price Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06082">http://arxiv.org/abs/2309.06082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milan Jain, Xueqing Sun, Sohom Datta, Abhishek Somani</li>
<li>for: 本研究旨在分析和揭示现代电力市场中变化的价格形成因素，尤其是高可再生能源含量下的价格峰值事件。</li>
<li>methods: 该研究提出了一种基于机器学习的分析框架，可以帮助分解现代电力市场中价格峰值事件的主要驱动因素。</li>
<li>results: 研究结果表明，价格峰值事件的主要驱动因素包括可再生能源含量、天气因素和市场操作因素等。这些结果可以用于多个重要的市场设计、可再生发电和干预、运营和网络安全应用。<details>
<summary>Abstract</summary>
Power grids are moving towards 100% renewable energy source bulk power grids, and the overall dynamics of power system operations and electricity markets are changing. The electricity markets are not only dispatching resources economically but also taking into account various controllable actions like renewable curtailment, transmission congestion mitigation, and energy storage optimization to ensure grid reliability. As a result, price formations in electricity markets have become quite complex. Traditional root cause analysis and statistical approaches are rendered inapplicable to analyze and infer the main drivers behind price formation in the modern grid and markets with variable renewable energy (VRE). In this paper, we propose a machine learning-based analysis framework to deconstruct the primary drivers for price spike events in modern electricity markets with high renewable energy. The outcomes can be utilized for various critical aspects of market design, renewable dispatch and curtailment, operations, and cyber-security applications. The framework can be applied to any ISO or market data; however, in this paper, it is applied to open-source publicly available datasets from California Independent System Operator (CAISO) and ISO New England (ISO-NE).
</details>
<details>
<summary>摘要</summary>
《电力网络在向100%可再生能源源扩展方向上进行变革，电力市场的运营和供应链也在不断发生变化。电力市场不仅经济地派发资源，还考虑了多种可控行为，如可再生能源减少、传输拥堵缓解和能量存储优化，以确保网络可靠性。因此，电力市场价格的形成变得非常复杂。传统的根本原因分析和统计方法在现代网络和市场中变得无法应用，用于分析和推导现代电力市场价格主要驱动力的主要驱动力。在这篇论文中，我们提出一种基于机器学习的分析框架，以分解现代电力市场价格峰值事件的主要驱动力。这些结果可以用于各种关键应用，如市场设计、可再生发电和减少、运营和网络安全应用。这种框架可以应用于任何ISO或市场数据，但在这篇论文中，它被应用于公开可用的CAISO和ISO-NE数据集。》Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="BatMan-CLR-Making-Few-shots-Meta-Learners-Resilient-Against-Label-Noise"><a href="#BatMan-CLR-Making-Few-shots-Meta-Learners-Resilient-Against-Label-Noise" class="headerlink" title="BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise"></a>BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06046">http://arxiv.org/abs/2309.06046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeroen M. Galjaard, Robert Birke, Juan Perez, Lydia Y. Chen</li>
<li>for: 本研究探讨了meta-learning中 label noise 的负面影响，并提出了两种采样技术来增强meta-learner对 label noise 的抗性。</li>
<li>methods: 本研究使用了现有的 gradient-based $N$-way $K$-shot learners，并对其进行了extensive的分析和对比。同时，本研究还提出了两种采样技术，namely manifold (Man)和 batch manifold (BatMan)，以帮助meta-learner更好地利用噪音标签。</li>
<li>results: 研究结果显示，当meta-training中存在 label noise时， gradient-based $N$-way $K$-shot learners 的准确率可以下降达42%。而通过使用 manifold (Man) 和 batch manifold (BatMan) 采样技术，可以减少 meta-testing 中 label noise 的影响，并限制meta-testing准确率下降在${2.5}$, ${9.4}$, ${1.1}$ percent points 之间。<details>
<summary>Abstract</summary>
The negative impact of label noise is well studied in classical supervised learning yet remains an open research question in meta-learning. Meta-learners aim to adapt to unseen learning tasks by learning a good initial model in meta-training and consecutively fine-tuning it according to new tasks during meta-testing. In this paper, we present the first extensive analysis of the impact of varying levels of label noise on the performance of state-of-the-art meta-learners, specifically gradient-based $N$-way $K$-shot learners. We show that the accuracy of Reptile, iMAML, and foMAML drops by up to 42% on the Omniglot and CifarFS datasets when meta-training is affected by label noise. To strengthen the resilience against label noise, we propose two sampling techniques, namely manifold (Man) and batch manifold (BatMan), which transform the noisy supervised learners into semi-supervised ones to increase the utility of noisy labels. We first construct manifold samples of $N$-way $2$-contrastive-shot tasks through augmentation, learning the embedding via a contrastive loss in meta-training, and then perform classification through zeroing on the embedding in meta-testing. We show that our approach can effectively mitigate the impact of meta-training label noise. Even with 60% wrong labels \batman and \man can limit the meta-testing accuracy drop to ${2.5}$, ${9.4}$, ${1.1}$ percent points, respectively, with existing meta-learners across the Omniglot, CifarFS, and MiniImagenet datasets.
</details>
<details>
<summary>摘要</summary>
研究者们已经广泛研究了经典的超级学习中的标签噪声的负面影响，但在元学习领域中，这个问题仍然是一个开放的研究问题。元学习者希望通过学习一个好的初始模型，并在新任务上进行细化调整来适应未看过的学习任务。在这篇论文中，我们提供了首次对元学习器的标签噪声的影响进行了广泛的分析。我们发现，当元训练被标签噪声影响时，Reptile、iMAML和foMAML的精度 Drop by up to 42% on the Omniglot and CifarFS datasets。为了增强元学习器对标签噪声的抗性，我们提议了两种采样技术： manifold（Man）和批量 manifold（BatMan）。这两种技术可以将噪声标注的超级学习器转换成 semi-supervised 学习器，以提高噪声标注的利用性。我们首先通过扩展来构建 $N$-way $2$-contrastive-shot任务的 manifold 样本，然后通过预训练一个嵌入向量，并在元测试中使用零化来进行分类。我们发现，我们的方法可以有效地减轻元训练标签噪声的影响。即使有60%的标签是错误的，batman 和 man 可以限制元测试精度下降到 ${2.5}$, ${9.4}$, ${1.1}$%点，分别。
</details></li>
</ul>
<hr>
<h2 id="Update-Monte-Carlo-tree-search-UMCTS-algorithm-for-heuristic-global-search-of-sizing-optimization-problems-for-truss-structures"><a href="#Update-Monte-Carlo-tree-search-UMCTS-algorithm-for-heuristic-global-search-of-sizing-optimization-problems-for-truss-structures" class="headerlink" title="Update Monte Carlo tree search (UMCTS) algorithm for heuristic global search of sizing optimization problems for truss structures"></a>Update Monte Carlo tree search (UMCTS) algorithm for heuristic global search of sizing optimization problems for truss structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06045">http://arxiv.org/abs/2309.06045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fu-Yao Ko, Katsuyuki Suzuki, Kazuo Yonekura</li>
<li>for: optimization of truss structures</li>
<li>methods: reinforcement learning (RL) and Monte Carlo tree search (MCTS) with upper confidence bound (UCB)</li>
<li>results: efficient optimization algorithm with at least ten times faster computation time than branch and bound (BB) method, and stable better solutions than other conventional methods.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文是针对桁架结构的最佳化问题进行研究。</li>
<li>methods: 使用强化学习（RL）和Monte Carlo tree search（MCTS）方法，并且加上最高信心界（UCB）。</li>
<li>results: 提出了一个高效的最佳化算法， computation time 至少比branch and bound（BB）方法快十倍，并且稳定地获得更好的解。<details>
<summary>Abstract</summary>
Sizing optimization of truss structures is a complex computational problem, and the reinforcement learning (RL) is suitable for dealing with multimodal problems without gradient computations. In this paper, a new efficient optimization algorithm called update Monte Carlo tree search (UMCTS) is developed to obtain the appropriate design for truss structures. UMCTS is an RL-based method that combines the novel update process and Monte Carlo tree search (MCTS) with the upper confidence bound (UCB). Update process means that in each round, the optimal cross-sectional area of each member is determined by search tree, and its initial state is the final state in the previous round. In the UMCTS algorithm, an accelerator for the number of selections for member area and iteration number is introduced to reduce the computation time. Moreover, for each state, the average reward is replaced by the best reward collected on the simulation process to determine the optimal solution. The proposed optimization method is examined on some benchmark problems of planar and spatial trusses with discrete sizing variables to demonstrate the efficiency and validity. It is shown that the computation time for the proposed approach is at least ten times faster than the branch and bound (BB) method. The numerical results indicate that the proposed method stably achieves better solution than other conventional methods.
</details>
<details>
<summary>摘要</summary>
��erton 优化算法的评估是一个复杂的计算问题，而人工智能学习（RL）是适用于多Modal问题的解决方案。在这篇论文中，一种新的高效优化算法called update Monte Carlo tree search（UMCTS）被开发出来，以获取适当的设计 dla truss 结构。 UMCTS 是一种基于 RL 的方法，它将 Monte Carlo tree search（MCTS）与Upper Confidence Bound（UCB）结合，并在每一轮中，通过搜索树来确定每个成员的最佳跨部分面积。在 UMCTS 算法中，一种加速器 для成员面积和迭代次数的数量被引入，以降低计算时间。此外，每个状态下的平均奖励被 replaced by 最佳在 Simulation 过程中收集的奖励，以确定最佳解决方案。提出的优化方法被应用于一些 benchmark 问题 of planar 和 spatial trusses with discrete sizing variables，以示出其高效性和有效性。结果表明，提出的方法的计算时间至少比 branch and bound（BB）方法快 ten times。 numerics 表明，提出的方法稳定地实现了 better solution  than other conventional methods。
</details></li>
</ul>
<hr>
<h2 id="Learning-Score-based-Grasping-Primitive-for-Human-assisting-Dexterous-Grasping"><a href="#Learning-Score-based-Grasping-Primitive-for-Human-assisting-Dexterous-Grasping" class="headerlink" title="Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping"></a>Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06038">http://arxiv.org/abs/2309.06038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianhao Wu, Mingdong Wu, Jiyao Zhang, Yunchong Gan, Hao Dong</li>
<li>for: 本研究旨在帮助用户在人手不可用或不适用的情况下，使用人工智能手部协助 grasping 物品。</li>
<li>methods: 本研究提出了一种新的任务 called human-assisting dexterous grasping，旨在训练一个控制 робо手指的策略，以帮助用户 grasping 物品。</li>
<li>results: 实验结果表明，我们提出的方法在比基eline的情况下具有优势，highlighting 用户consciousness 和实际应用性。codes 和演示可以在 “<a target="_blank" rel="noopener" href="https://sites.google.com/view/graspgf">https://sites.google.com/view/graspgf</a>“ 中查看。<details>
<summary>Abstract</summary>
The use of anthropomorphic robotic hands for assisting individuals in situations where human hands may be unavailable or unsuitable has gained significant importance. In this paper, we propose a novel task called human-assisting dexterous grasping that aims to train a policy for controlling a robotic hand's fingers to assist users in grasping objects. Unlike conventional dexterous grasping, this task presents a more complex challenge as the policy needs to adapt to diverse user intentions, in addition to the object's geometry. We address this challenge by proposing an approach consisting of two sub-modules: a hand-object-conditional grasping primitive called Grasping Gradient Field~(GraspGF), and a history-conditional residual policy. GraspGF learns `how' to grasp by estimating the gradient from a success grasping example set, while the residual policy determines `when' and at what speed the grasping action should be executed based on the trajectory history. Experimental results demonstrate the superiority of our proposed method compared to baselines, highlighting the user-awareness and practicality in real-world applications. The codes and demonstrations can be viewed at "https://sites.google.com/view/graspgf".
</details>
<details>
<summary>摘要</summary>
人类辅助dexterous grasping任务在帮助用户抓取物品时具有重要 significanc。在这篇论文中，我们提出一种新任务，即人机合作dexterous grasping，旨在训练一个控制机器人手指的策略，以助用户抓取物品。与传统的dexterous grasping不同，这个任务呈现了更加复杂的挑战，因为策略需要适应用户的意图，以及物品的几何学特征。我们解决这个挑战的方法是通过两个子模块：一个手机-物品conditional grasping基本单元 called Grasping Gradient Field（GraspGF），以及一个历史条件的差异策略。GraspGF学习了如何抓取的“如何”，通过成功抓取示例集来估算抓取的梯度，而差异策略则确定了执行抓取动作的时间和速度，根据轨迹历史。实验结果表明我们提出的方法在基eline之上显著超越，highlighting用户意识和实际应用中的实用性。代码和演示可以在“https://sites.google.com/view/graspgf”上查看。
</details></li>
</ul>
<hr>
<h2 id="Automatically-Estimating-the-Effort-Required-to-Repay-Self-Admitted-Technical-Debt"><a href="#Automatically-Estimating-the-Effort-Required-to-Repay-Self-Admitted-Technical-Debt" class="headerlink" title="Automatically Estimating the Effort Required to Repay Self-Admitted Technical Debt"></a>Automatically Estimating the Effort Required to Repay Self-Admitted Technical Debt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06020">http://arxiv.org/abs/2309.06020</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yikun-li/satd-repayment-effort">https://github.com/yikun-li/satd-repayment-effort</a></li>
<li>paper_authors: Yikun Li, Mohamed Soliman, Paris Avgeriou<br>for: 本研究旨在提高技术债（Technical Debt）的优化和维护效率，特别是自我投诉技术债（Self-Admitted Technical Debt，SATD）的优化。methods: 本研究使用了一个大型的SATD数据集，包括341,740个SATD项来自2,568,728个提交，从1,060个Apache仓库中收集。然后，我们采用了BERT和TextCNN等机器学习方法来自动估算SATD的偿还努力。results: 我们发现不同类型的SATD偿还努力有不同的水平，代码&#x2F;设计、需求、测试债需要更多的偿还努力，而文档债需要较少的偿还努力。此外，我们还总结了在SATD偿还过程中不同水平的偿还努力关键词。本研究的贡献可以帮助优化技术债的优化和维护效率，最终为软件开发和维护带来利益。<details>
<summary>Abstract</summary>
Technical debt refers to the consequences of sub-optimal decisions made during software development that prioritize short-term benefits over long-term maintainability. Self-Admitted Technical Debt (SATD) is a specific form of technical debt, explicitly documented by developers within software artifacts such as source code comments and commit messages. As SATD can hinder software development and maintenance, it is crucial to address and prioritize it effectively. However, current methodologies lack the ability to automatically estimate the repayment effort of SATD based on its textual descriptions. To address this limitation, we propose a novel approach for automatically estimating SATD repayment effort, utilizing a comprehensive dataset comprising 341,740 SATD items from 2,568,728 commits across 1,060 Apache repositories. Our findings show that different types of SATD require varying levels of repayment effort, with code/design, requirement, and test debt demanding greater effort compared to non-SATD items, while documentation debt requires less. We introduce and evaluate machine learning methodologies, particularly BERT and TextCNN, which outperforms classic machine learning methods and the naive baseline in estimating repayment effort. Additionally, we summarize keywords associated with varying levels of repayment effort that occur during SATD repayment. Our contributions aim to enhance the prioritization of SATD repayment effort and resource allocation efficiency, ultimately benefiting software development and maintainability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Molecular-Conformation-Generation-via-Shifting-Scores"><a href="#Molecular-Conformation-Generation-via-Shifting-Scores" class="headerlink" title="Molecular Conformation Generation via Shifting Scores"></a>Molecular Conformation Generation via Shifting Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09985">http://arxiv.org/abs/2309.09985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Zhou, Ruiying Liu, Chaolong Ying, Ruimao Zhang, Tianshu Yu</li>
<li>for: 生成分子结构，一个计算化学中的关键问题，涉及生成分子的三维构型几何。</li>
<li>methods: 我们提出了一种新的分子构型生成方法，基于分子分解的观察，即将分子中的原子间距离推迟到Maxwell-Boltzmann分布。</li>
<li>results: 我们对分子数据集进行实验，并证明了我们的方法在现有方法的基础上具有优势。<details>
<summary>Abstract</summary>
Molecular conformation generation, a critical aspect of computational chemistry, involves producing the three-dimensional conformer geometry for a given molecule. Generating molecular conformation via diffusion requires learning to reverse a noising process. Diffusion on inter-atomic distances instead of conformation preserves SE(3)-equivalence and shows superior performance compared to alternative techniques, whereas related generative modelings are predominantly based upon heuristical assumptions. In response to this, we propose a novel molecular conformation generation approach driven by the observation that the disintegration of a molecule can be viewed as casting increasing force fields to its composing atoms, such that the distribution of the change of inter-atomic distance shifts from Gaussian to Maxwell-Boltzmann distribution. The corresponding generative modeling ensures a feasible inter-atomic distance geometry and exhibits time reversibility. Experimental results on molecular datasets demonstrate the advantages of the proposed shifting distribution compared to the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
分子形态生成，计算化学中一项关键任务，涉及生成给定分子的三维结构均衡。通过扩散来生成分子形态，需要学习反向噪声过程。与传统技术不同，我们的方法基于分子裂解的观察，即将分子中的原子受到增加的力场影响，因此分子中间的距离分布从 Gaussian 转变为 Maxwell-Boltzmann 分布。这种生成模型保证了原子间距离的可行性，并且具有时间逆向性。对分子数据进行实验，我们发现了我们提出的分布Shift的优势，比 estado-of-the-art 更好。
</details></li>
</ul>
<hr>
<h2 id="DSLOT-NN-Digit-Serial-Left-to-Right-Neural-Network-Accelerator"><a href="#DSLOT-NN-Digit-Serial-Left-to-Right-Neural-Network-Accelerator" class="headerlink" title="DSLOT-NN: Digit-Serial Left-to-Right Neural Network Accelerator"></a>DSLOT-NN: Digit-Serial Left-to-Right Neural Network Accelerator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06019">http://arxiv.org/abs/2309.06019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Sohail Ibrahim, Muhammad Usman, Malik Zohaib Nisar, Jeong-A, Lee</li>
<li>for: 这个研究的目的是来提高深度神经网络（DNN）的推论运算的效率，并节省电力和能源。</li>
<li>methods: 这个研究使用了一种基于数位序列的左至右（DSLOT）运算技术，称为 DSLOT-NN，并使用了低延迟的最重要 digit-first（MSDF）多项式和加法器来进行资料处理。</li>
<li>results: 这个研究的结果显示，使用 DSLOT-NN 技术可以节省大量电力和能源，并且具有较短的循环时间和较高的 OPS 每瓦特。 compared with state-of-the-art Stripes 的性能指标。<details>
<summary>Abstract</summary>
We propose a Digit-Serial Left-tO-righT (DSLOT) arithmetic based processing technique called DSLOT-NN with aim to accelerate inference of the convolution operation in the deep neural networks (DNNs). The proposed work has the ability to assess and terminate the ineffective convolutions which results in massive power and energy savings. The processing engine is comprised of low-latency most-significant-digit-first (MSDF) (also called online) multipliers and adders that processes data from left-to-right, allowing the execution of subsequent operations in digit-pipelined manner. Use of online operators eliminates the need for the development of complex mechanism of identifying the negative activation, as the output with highest weight value is generated first, and the sign of the result can be identified as soon as first non-zero digit is generated. The precision of the online operators can be tuned at run-time, making them extremely useful in situations where accuracy can be compromised for power and energy savings. The proposed design has been implemented on Xilinx Virtex-7 FPGA and is compared with state-of-the-art Stripes on various performance metrics. The results show the proposed design presents power savings, has shorter cycle time, and approximately 50% higher OPS per watt.
</details>
<details>
<summary>摘要</summary>
Our processing engine consists of low-latency most-significant-digit-first (MSDF) multipliers and adders that process data from left to right, allowing for digit-pipelined execution. This eliminates the need for complex mechanisms to identify negative activation, as the output with the highest weight value is generated first, and the sign of the result can be identified as soon as the first non-zero digit is generated.The precision of our online operators can be tuned at runtime, making them highly versatile in situations where accuracy can be compromised for power and energy savings. We have implemented our design on Xilinx Virtex-7 FPGA and compared it with state-of-the-art Stripes on various performance metrics. Our results show that the proposed design achieves power savings, has a shorter cycle time, and offers approximately 50% higher operations per second per watt.
</details></li>
</ul>
<hr>
<h2 id="SoccerNet-2023-Challenges-Results"><a href="#SoccerNet-2023-Challenges-Results" class="headerlink" title="SoccerNet 2023 Challenges Results"></a>SoccerNet 2023 Challenges Results</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06006">http://arxiv.org/abs/2309.06006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lRomul/ball-action-spotting">https://github.com/lRomul/ball-action-spotting</a></li>
<li>paper_authors: Anthony Cioppa, Silvio Giancola, Vladimir Somers, Floriane Magera, Xin Zhou, Hassan Mkhallati, Adrien Deliège, Jan Held, Carlos Hinojosa, Amir M. Mansourian, Pierre Miralles, Olivier Barnich, Christophe De Vleeschouwer, Alexandre Alahi, Bernard Ghanem, Marc Van Droogenbroeck, Abdullah Kamal, Adrien Maglo, Albert Clapés, Amr Abdelaziz, Artur Xarles, Astrid Orcesi, Atom Scott, Bin Liu, Byoungkwon Lim, Chen Chen, Fabian Deuser, Feng Yan, Fufu Yu, Gal Shitrit, Guanshuo Wang, Gyusik Choi, Hankyul Kim, Hao Guo, Hasby Fahrudin, Hidenari Koguchi, Håkan Ardö, Ibrahim Salah, Ido Yerushalmy, Iftikar Muhammad, Ikuma Uchida, Ishay Be’ery, Jaonary Rabarisoa, Jeongae Lee, Jiajun Fu, Jianqin Yin, Jinghang Xu, Jongho Nang, Julien Denize, Junjie Li, Junpei Zhang, Juntae Kim, Kamil Synowiec, Kenji Kobayashi, Kexin Zhang, Konrad Habel, Kota Nakajima, Licheng Jiao, Lin Ma, Lizhi Wang, Luping Wang, Menglong Li, Mengying Zhou, Mohamed Nasr, Mohamed Abdelwahed, Mykola Liashuha, Nikolay Falaleev, Norbert Oswald, Qiong Jia, Quoc-Cuong Pham, Ran Song, Romain Hérault, Rui Peng, Ruilong Chen, Ruixuan Liu, Ruslan Baikulov, Ryuto Fukushima, Sergio Escalera, Seungcheon Lee, Shimin Chen, Shouhong Ding, Taiga Someya, Thomas B. Moeslund, Tianjiao Li, Wei Shen, Wei Zhang, Wei Li, Wei Dai, Weixin Luo, Wending Zhao, Wenjie Zhang, Xinquan Yang, Yanbiao Ma, Yeeun Joo, Yingsen Zeng, Yiyang Gan, Yongqiang Zhu, Yujie Zhong, Zheng Ruan, Zhiheng Li, Zhijian Huang, Ziyu Meng<br>for:* 这篇论文是为了描述2023年度的SoccerNet视频理解挑战（第三届），这些挑战包括七个视觉任务，分为三个主题。methods:* 这篇论文使用了多种视觉技术，包括动作检测、球体变化检测、笔记录和Camera calibration等。results:* 这篇论文描述了2023年度的SoccerNet视频理解挑战，包括七个视觉任务的结果，其中有三个任务是新添加的，一个任务得到了更多的数据和注释，另一个任务改为着眼点到终端方法。<details>
<summary>Abstract</summary>
The SoccerNet 2023 challenges were the third annual video understanding challenges organized by the SoccerNet team. For this third edition, the challenges were composed of seven vision-based tasks split into three main themes. The first theme, broadcast video understanding, is composed of three high-level tasks related to describing events occurring in the video broadcasts: (1) action spotting, focusing on retrieving all timestamps related to global actions in soccer, (2) ball action spotting, focusing on retrieving all timestamps related to the soccer ball change of state, and (3) dense video captioning, focusing on describing the broadcast with natural language and anchored timestamps. The second theme, field understanding, relates to the single task of (4) camera calibration, focusing on retrieving the intrinsic and extrinsic camera parameters from images. The third and last theme, player understanding, is composed of three low-level tasks related to extracting information about the players: (5) re-identification, focusing on retrieving the same players across multiple views, (6) multiple object tracking, focusing on tracking players and the ball through unedited video streams, and (7) jersey number recognition, focusing on recognizing the jersey number of players from tracklets. Compared to the previous editions of the SoccerNet challenges, tasks (2-3-7) are novel, including new annotations and data, task (4) was enhanced with more data and annotations, and task (6) now focuses on end-to-end approaches. More information on the tasks, challenges, and leaderboards are available on https://www.soccer-net.org. Baselines and development kits can be found on https://github.com/SoccerNet.
</details>
<details>
<summary>摘要</summary>
occerNet 2023 挑战是第三届视频理解挑战，由 SoccerNet 团队组织。这一第三届挑战包括七个视觉任务，分为三个主题。第一主题是广播视频理解，包括三个高级任务：（1）动作搜索，关注从广播视频中检索全局动作的时间戳;（2）球动作搜索，关注从广播视频中检索球的状态变化时间戳;（3）稠密视频描述，关注使用自然语言描述广播视频，并附加时间戳。第二主题是场地理解，单个任务为（4）摄像头协调，关注从图像中提取摄像头参数。第三主题是球员理解，包括三个低级任务：（5）重识别，关注在多视图中重新识别同一名球员;（6）多对tracking，关注在未编辑视频流中跟踪球员和球;（7）球衣号码识别，关注从跟踪片中识别球员的球衣号码。相比前两届 SoccerNet 挑战，任务（2-3-7）是新的，包括新的注释和数据，任务（4）增加了更多的数据和注释，任务（6）现在专注于终端方法。更多关于任务、挑战和排名的信息可以在 <https://www.soccer-net.org> 上获取。基础和开发集可以在 <https://github.com/SoccerNet> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Life-inspired-Interoceptive-Artificial-Intelligence-for-Autonomous-and-Adaptive-Agents"><a href="#Life-inspired-Interoceptive-Artificial-Intelligence-for-Autonomous-and-Adaptive-Agents" class="headerlink" title="Life-inspired Interoceptive Artificial Intelligence for Autonomous and Adaptive Agents"></a>Life-inspired Interoceptive Artificial Intelligence for Autonomous and Adaptive Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05999">http://arxiv.org/abs/2309.05999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungwoo Lee, Younghyun Oh, Hyunhoe An, Hyebhin Yoon, Karl J. Friston, Seok Jun Hong, Choong-Wan Woo</li>
<li>for: 本研究的目的是建立自适应和自主的人工智能代理人，以便在不断变化的环境中存活和实现目标。</li>
<li>methods: 本研究使用了生物学中的感知过程，即监测自己的内部环境以保持在certain bounds内，以及生物学中的数学性质，以开发人工智能代理人。</li>
<li>results: 本研究提出了一种新的感知方法，可以帮助建立自适应和自主的人工智能代理人，并结合了遗传学、增强学习和神经科学的新进展。<details>
<summary>Abstract</summary>
Building autonomous --- i.e., choosing goals based on one's needs -- and adaptive -- i.e., surviving in ever-changing environments -- agents has been a holy grail of artificial intelligence (AI). A living organism is a prime example of such an agent, offering important lessons about adaptive autonomy. Here, we focus on interoception, a process of monitoring one's internal environment to keep it within certain bounds, which underwrites the survival of an organism. To develop AI with interoception, we need to factorize the state variables representing internal environments from external environments and adopt life-inspired mathematical properties of internal environment states. This paper offers a new perspective on how interoception can help build autonomous and adaptive agents by integrating the legacy of cybernetics with recent advances in theories of life, reinforcement learning, and neuroscience.
</details>
<details>
<summary>摘要</summary>
建立自主---即根据自己需求选择目标---以及适应---即在不断变化的环境中生存---的人工智能（AI）是人工智能的圣杯。生物体是这种代理人的一个好例子，它们提供了关键的适应自主性教训。在这里，我们关注内部环境监测---即保持内部环境在某些范围内---这一过程，这对生物体的存亡起到了关键作用。为建立具有内部监测能力的AI，我们需要将内部环境状态变量分离于外部环境状态变量，并采用生命力学中的内部环境状态生命力学性质。这篇论文提供了一种新的视角，即通过结合人工智能的启蒙、生命力学、奖励学习和神经科学的进步，实现内部监测能力的AI。
</details></li>
</ul>
<hr>
<h2 id="Goal-Space-Abstraction-in-Hierarchical-Reinforcement-Learning-via-Reachability-Analysis"><a href="#Goal-Space-Abstraction-in-Hierarchical-Reinforcement-Learning-via-Reachability-Analysis" class="headerlink" title="Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis"></a>Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07168">http://arxiv.org/abs/2309.07168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Zadem, Sergio Mover, Sao Mai Nguyen</li>
<li>for: 提高开放式学习的效率和可贸转性，使用符号方法表示目标。</li>
<li>methods: 利用发展机制实现目标发现，通过emergent表示将环境状态集分组，保持环境动力学信息。</li>
<li>results: 在导航任务上，通过逐渐学习表示和策略，实现了数据效率和可贸转性。<details>
<summary>Abstract</summary>
Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this work, we propose a developmental mechanism for subgoal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We create a HRL algorithm that gradually learns this representation along with the policies and evaluate it on navigation tasks to show the learned representation is interpretable and results in data efficiency.
</details>
<details>
<summary>摘要</summary>
开放式学习受益于使用象征方法表示目标，因为它们可以为高效和可传递的学习提供结构知识。然而，现有的层次强化学习（HRL）方法，它们通常需要手动设定目标表示，这会带来一定的限制。挑战在自动发现象征目标表示时是保留环境动力学信息。在这种工作中，我们提出一种发展机制，通过观察环境状态的集合，自动找出子目标。我们创建了一种基于HRL算法，逐渐学习这种表示，并评估其在导航任务中的效果，结果表明学习的表示是可解释的，并且具有数据效率。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Guided-Short-Context-Action-Anticipation-in-Human-Centric-Videos"><a href="#Knowledge-Guided-Short-Context-Action-Anticipation-in-Human-Centric-Videos" class="headerlink" title="Knowledge-Guided Short-Context Action Anticipation in Human-Centric Videos"></a>Knowledge-Guided Short-Context Action Anticipation in Human-Centric Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05943">http://arxiv.org/abs/2309.05943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarthak Bhagat, Simon Stepputtis, Joseph Campbell, Katia Sycara</li>
<li>for: 预测长期人类行为，特别是使用短视频段，以提高编辑工作流程的速度和创造力。</li>
<li>methods: 使用 transformer 网络具有符号知识图，在视频段中预测行为，通过在运行时提高 transformer 的注意力机制来增强表现。</li>
<li>results: 在 Breakfast 和 50Salads 两个标准 datasets 上，我们的方法与当前状态的方法相比，在长期行为预测中使用短视频段的情况下提高了9%。<details>
<summary>Abstract</summary>
This work focuses on anticipating long-term human actions, particularly using short video segments, which can speed up editing workflows through improved suggestions while fostering creativity by suggesting narratives. To this end, we imbue a transformer network with a symbolic knowledge graph for action anticipation in video segments by boosting certain aspects of the transformer's attention mechanism at run-time. Demonstrated on two benchmark datasets, Breakfast and 50Salads, our approach outperforms current state-of-the-art methods for long-term action anticipation using short video context by up to 9%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Answering-Subjective-Induction-Questions-on-Products-by-Summarizing-Multi-sources-Multi-viewpoints-Knowledge"><a href="#Answering-Subjective-Induction-Questions-on-Products-by-Summarizing-Multi-sources-Multi-viewpoints-Knowledge" class="headerlink" title="Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge"></a>Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05938">http://arxiv.org/abs/2309.05938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufeng Zhang, Meng-xiang Wang, Jianxing Yu</li>
<li>for: 提出了一个新的 Answering Subjective Induction Question on Products (SUBJPQA) 任务，解决这类问题的答案不固定，可以从多个角度解释。</li>
<li>methods: 提出了三步方法：首先从多个知识源中检索答案相关的线索，并收集了相关的印象知识；然后通过交互式注意力捕捉问题中的相关性；最后，通过模板控制的解码器输出了全面和多角度的答案。</li>
<li>results: 由于这个新任务没有相关的评估标准集，因此 constructed a large-scale dataset named SupQA，包含48,352个样本和15种产品领域。评估结果表明了我们的方法的效果。<details>
<summary>Abstract</summary>
This paper proposes a new task in the field of Answering Subjective Induction Question on Products (SUBJPQA). The answer to this kind of question is non-unique, but can be interpreted from many perspectives. For example, the answer to 'whether the phone is heavy' has a variety of different viewpoints. A satisfied answer should be able to summarize these subjective opinions from multiple sources and provide objective knowledge, such as the weight of a phone. That is quite different from the traditional QA task, in which the answer to a factoid question is unique and can be found from a single data source. To address this new task, we propose a three-steps method. We first retrieve all answer-related clues from multiple knowledge sources on facts and opinions. The implicit commonsense facts are also collected to supplement the necessary but missing contexts. We then capture their relevance with the questions by interactive attention. Next, we design a reinforcement-based summarizer to aggregate all these knowledgeable clues. Based on a template-controlled decoder, we can output a comprehensive and multi-perspective answer. Due to the lack of a relevant evaluated benchmark set for the new task, we construct a large-scale dataset, named SupQA, consisting of 48,352 samples across 15 product domains. Evaluation results show the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这篇论文提出了一个新的任务，即答え问题 Answering Subjective Induction Questions on Products (SUBJPQA)。这类问题的答案不唯一，可以从多个角度解释。例如，问题 "手机是否重" 有多种不同的看法。一个满意的答案应该能够汇集多个来源的主观意见，并提供对象知识，如手机的重量。这与传统的 QA 任务不同，传统的答案是唯一的，可以从单个数据源中找到。为 Addressing this new task, the authors propose a three-step method. First, they retrieve all answer-related clues from multiple knowledge sources on facts and opinions. They also collect implicit commonsense facts to supplement necessary but missing contexts. Next, they capture the relevance of the clues with the questions using interactive attention. Finally, they design a reinforcement-based summarizer to aggregate all the knowledgeable clues. Based on a template-controlled decoder, they can output a comprehensive and multi-perspective answer. Due to the lack of a relevant evaluated benchmark set for the new task, the authors construct a large-scale dataset named SupQA, consisting of 48,352 samples across 15 product domains. The evaluation results show the effectiveness of their approach.
</details></li>
</ul>
<hr>
<h2 id="MatSciML-A-Broad-Multi-Task-Benchmark-for-Solid-State-Materials-Modeling"><a href="#MatSciML-A-Broad-Multi-Task-Benchmark-for-Solid-State-Materials-Modeling" class="headerlink" title="MatSciML: A Broad, Multi-Task Benchmark for Solid-State Materials Modeling"></a>MatSciML: A Broad, Multi-Task Benchmark for Solid-State Materials Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05934">http://arxiv.org/abs/2309.05934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/intellabs/matsciml">https://github.com/intellabs/matsciml</a></li>
<li>paper_authors: Kin Long Kelvin Lee, Carmelo Gonzales, Marcel Nassar, Matthew Spellings, Mikhail Galkin, Santiago Miret</li>
<li>For: 该论文目的是提出一个新的benchmark，用于模型化固体材料科学领域中的机器学习方法。* Methods: 该论文使用的方法包括使用开源数据集，包括大规模数据集如OpenCatalyst、OQMD、NOMAD、Carolina Materials Database和Materials Project等，以及 simulated energies、atomic forces、材料带隔和 Space group classification数据等。* Results: 该论文通过使用多个数据集进行联合预测，实现了对固体材料的多任务学习和多数据集学习。通过 MatSci ML  benchmark，研究人员可以评估不同的 graf neural network 和 equivariant point cloud network 在多种学习场景中的性能。<details>
<summary>Abstract</summary>
We propose MatSci ML, a novel benchmark for modeling MATerials SCIence using Machine Learning (MatSci ML) methods focused on solid-state materials with periodic crystal structures. Applying machine learning methods to solid-state materials is a nascent field with substantial fragmentation largely driven by the great variety of datasets used to develop machine learning models. This fragmentation makes comparing the performance and generalizability of different methods difficult, thereby hindering overall research progress in the field. Building on top of open-source datasets, including large-scale datasets like the OpenCatalyst, OQMD, NOMAD, the Carolina Materials Database, and Materials Project, the MatSci ML benchmark provides a diverse set of materials systems and properties data for model training and evaluation, including simulated energies, atomic forces, material bandgaps, as well as classification data for crystal symmetries via space groups. The diversity of properties in MatSci ML makes the implementation and evaluation of multi-task learning algorithms for solid-state materials possible, while the diversity of datasets facilitates the development of new, more generalized algorithms and methods across multiple datasets. In the multi-dataset learning setting, MatSci ML enables researchers to combine observations from multiple datasets to perform joint prediction of common properties, such as energy and forces. Using MatSci ML, we evaluate the performance of different graph neural networks and equivariant point cloud networks on several benchmark tasks spanning single task, multitask, and multi-data learning scenarios. Our open-source code is available at https://github.com/IntelLabs/matsciml.
</details>
<details>
<summary>摘要</summary>
我们提出了MatSci ML，一个新的测试基准 для模型化材料科学使用机器学习方法（MatSci ML），专注于固体材料 periodic crystal structures 的模型。采用机器学习方法对固体材料是一个新兴领域，受到各种数据集的不同所致，这种分化使得对不同方法的比较和总体进展困难，从而阻碍了材料科学领域的研究进步。基于开源数据集，包括大规模数据集如OpenCatalyst、OQMD、NOMAD、Carolina Materials Database和Materials Project，MatSci ML 提供了多种材料系统和性能数据 для模型训练和评估，包括模拟能量、原子力、材料带隙、以及通过空间群来分类的晶体结构数据。MatSci ML 中的多种属性多样性使得实现和评估多任务学习算法 для固体材料 possible，而多种数据集的多样性促进了新的、更一般的算法和方法的开发。在多个数据集学习 Setting，MatSci ML 允许研究人员将多个数据集中的观察结果合并进行共同预测常见的属性，如能量和力。使用 MatSci ML，我们评估了不同的图 neural networks 和平衡点云网络在多个benchmark任务中的表现，涵盖单任务、多任务和多数据学习场景。我们的开源代码可以在https://github.com/IntelLabs/matsciml 中找到。
</details></li>
</ul>
<hr>
<h2 id="Combining-deep-learning-and-street-view-imagery-to-map-smallholder-crop-types"><a href="#Combining-deep-learning-and-street-view-imagery-to-map-smallholder-crop-types" class="headerlink" title="Combining deep learning and street view imagery to map smallholder crop types"></a>Combining deep learning and street view imagery to map smallholder crop types</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05930">http://arxiv.org/abs/2309.05930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordi Laguarta, Thomas Friedel, Sherrie Wang</li>
<li>for: 该研究的目的是为了创建一个全球范围内的农作物类型地图，以便监测农作物生长进度，预测全球农产量，并制定有效的政策。</li>
<li>methods: 该研究使用了深度学习技术和Google街景图像来自动生成农作物类型的地面参考数据。他们首先选择了一组街景图像，然后使用这些图像来训练一个模型，以便预测农作物类型。最后，他们将预测的标签与Remote sensing时序列结合，以创建一个无缝的农作物类型地图。</li>
<li>results: 研究发现，在泰国，该方法可以创建一个国家范围内的全面的rice、manioc、maize和甘蔗类型地图，其准确率达93%。这种方法可以在全球各地，尤其是小农场地区，提供一种快速、便宜、高准确率的农作物类型地图生成方式。<details>
<summary>Abstract</summary>
Accurate crop type maps are an essential source of information for monitoring yield progress at scale, projecting global crop production, and planning effective policies. To date, however, crop type maps remain challenging to create in low and middle-income countries due to a lack of ground truth labels for training machine learning models. Field surveys are the gold standard in terms of accuracy but require an often-prohibitively large amount of time, money, and statistical capacity. In recent years, street-level imagery, such as Google Street View, KartaView, and Mapillary, has become available around the world. Such imagery contains rich information about crop types grown at particular locations and times. In this work, we develop an automated system to generate crop type ground references using deep learning and Google Street View imagery. The method efficiently curates a set of street view images containing crop fields, trains a model to predict crop type by utilizing weakly-labelled images from disparate out-of-domain sources, and combines predicted labels with remote sensing time series to create a wall-to-wall crop type map. We show that, in Thailand, the resulting country-wide map of rice, cassava, maize, and sugarcane achieves an accuracy of 93%. As the availability of roadside imagery expands, our pipeline provides a way to map crop types at scale around the globe, especially in underserved smallholder regions.
</details>
<details>
<summary>摘要</summary>
准确的作物类别地图是考古规模监测作物进步、全球作物产量预测和制定有效政策的重要来源。然而，在LOW和中等收入国家，创建作物类别地图仍然是一项挑战，因为缺乏地面 truth标签用于训练机器学习模型。 Field surveys 是精度最高的标准，但它们需要大量的时间、金钱和统计资源。在最近几年，街道级别的图像，如Google Street View、KartaView和Mapillary，在全球变得可用。这些图像包含特定地点和时间的作物种植的详细信息。在这个工作中，我们开发了一个自动化的系统，使用深度学习和Google Street View图像来生成作物类别地标。该方法可以效率地筛选出包含作物田的街道图像，使用弱 labels 的图像从不同的域外来源来训练模型，并将预测标签与远程感知时序列合并以创建墙到墙的作物类别地图。我们显示，在泰国，得到的国家范围内的rice、 Cassava、maize和sugarcane的地图达到93%的准确率。随着路边图像的可用性扩展，我们的管道可以在全球范围内地图作物类别，特别是在小holder地区。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Aware-Masked-Autoencoders-for-Multimodal-Pretraining-on-Biosignals"><a href="#Frequency-Aware-Masked-Autoencoders-for-Multimodal-Pretraining-on-Biosignals" class="headerlink" title="Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals"></a>Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05927">http://arxiv.org/abs/2309.05927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Liu, Ellen L. Zippi, Hadi Pouransari, Chris Sandino, Jingping Nie, Hanlin Goh, Erdrin Azemi, Ali Moin</li>
<li>for: 本研究旨在提出一种适应多模态信号各种任务和多样性挑战的预训练方法，以提高对多模态信号的表征和预测性能。</li>
<li>methods: 本研究提出了一种频率意识的掩码自动编码器（$\texttt{bio}$FAME），它通过在频率空间中参数化信号表征来适应多模态信号的分布变化。该方法还包括一种频率维护预训练策略，通过在干扰空间中进行掩码自动编码来保持输入信号中的频率成分。</li>
<li>results: 对一系列的转移试验，我们获得了平均提高5.5%的分类精度，至于之前的状态艺术。此外，我们还证明了该方法在模式匹配情况下具有强大的实用性，包括预期不确定的模式掉包或替换。<details>
<summary>Abstract</summary>
Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\texttt{bio}$FAME) that learns to parameterize the representation of biosignals in the frequency space. $\texttt{bio}$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectively utilizes multimodal information during pretraining, and can be seamlessly adapted to diverse tasks and modalities at test time, regardless of input size and order. We evaluated our approach on a diverse set of transfer experiments on unimodal time series, achieving an average of $\uparrow$5.5% improvement in classification accuracy over the previous state-of-the-art. Furthermore, we demonstrated that our architecture is robust in modality mismatch scenarios, including unpredicted modality dropout or substitution, proving its practical utility in real-world applications. Code will be available soon.
</details>
<details>
<summary>摘要</summary>
利用多Modal信息从生物信号是建立全面人体和心理状态的关键。然而，多Modal生物信号经常会在预训练和测试集数据中出现重大的分布变化，这可能是因为任务规定的变化或多Modal组合的变化。为了在可能存在的分布变化情况下进行有效的预训练，我们提议一种频率意识的隐藏式自动编码器（$\texttt{bio}$FAME），该模型学习在频率空间中 parameterize 生物信号的表示。$\texttt{bio}$FAME 包含一种频率意识的转换器，该转换器通过固定大小的干扰函数来进行全token混合，不виси于输入的长度和采样率。为了保持每个输入通道中的频率组件，我们还采用了一种保持频率的预训练策略，该策略在幽latent空间进行隐藏式自动编码。这种架构可以有效利用多Modal信息进行预训练，并可以在测试时轻松适应多种任务和多Modal，无论输入的大小和顺序。我们对一组多Modal时间序列转换任务进行了多种转换试验，实现了平均提高5.5%的分类精度，胜过之前的状态艺。此外，我们还证明了我们的架构在模态匹配情况下具有实用性，包括预测不可预知的模态掉落或替换，证明了它在实际应用中的实用性。代码即将上传。
</details></li>
</ul>
<hr>
<h2 id="On-Regularized-Sparse-Logistic-Regression"><a href="#On-Regularized-Sparse-Logistic-Regression" class="headerlink" title="On Regularized Sparse Logistic Regression"></a>On Regularized Sparse Logistic Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05925">http://arxiv.org/abs/2309.05925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RohithM191/TSNE-on-Amazon-Fine-Food-reviews-Dataset">https://github.com/RohithM191/TSNE-on-Amazon-Fine-Food-reviews-Dataset</a></li>
<li>paper_authors: Mengyuan Zhang, Kai Liu</li>
<li>for: 这篇论文的目的是用来解决高维度数据中的分类和特征选择问题，同时使用非凸补助项来减少数据的维度。</li>
<li>methods: 本文提出了一些新的优化框架，用于解决这些问题，包括使用不同的条搜搜索准则来保证优化结果的好几何性。</li>
<li>results: 实验结果显示，提出的算法可以实现高效的分类和特征选择，并且比较低的计算成本。<details>
<summary>Abstract</summary>
Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
</details>
<details>
<summary>摘要</summary>
“稀疏逻辑回传数据类别和特征选择同时进行分类。处理 $\ell_1$-规定逻辑回传数据的研究已经很多，但是关于非对称责任逻辑回传数据的研究却没有相应的实践。本文提出了解决 $\ell_1$-规定稀疏逻辑回传数据和一些非对称责任逻辑回传数据的优化框架。在提议的优化框架中，我们利用不同的搜索条件来保证不同的规定Terms的优化表现良好。实验结果显示我们的提议算法在实际数据上能够有效地进行分类和特征选择，并且比较低的计算成本。”Note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Hallucination-in-Large-Foundation-Models"><a href="#A-Survey-of-Hallucination-in-Large-Foundation-Models" class="headerlink" title="A Survey of Hallucination in Large Foundation Models"></a>A Survey of Hallucination in Large Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05922">http://arxiv.org/abs/2309.05922</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vr25/hallucination-foundation-model-survey">https://github.com/vr25/hallucination-foundation-model-survey</a></li>
<li>paper_authors: Vipula Rawte, Amit Sheth, Amitava Das</li>
<li>for: 本文提供了关于基础模型中幻觉现象的广泛审视和分析，尤其是大基础模型（LFM）中幻觉现象的研究进展。</li>
<li>methods: 本文对幻觉现象进行分类，并确定了评估幻觉程度的评价标准。同时，本文还检查了现有的幻觉控制策略，并讨论了未来研究的可能性。</li>
<li>results: 本文提供了关于幻觉在LFM中的全面检查和解决方案。<details>
<summary>Abstract</summary>
Hallucination in a foundation model (FM) refers to the generation of content that strays from factual reality or includes fabricated information. This survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``Large'' Foundation Models (LFMs). The paper classifies various types of hallucination phenomena that are specific to LFMs and establishes evaluation criteria for assessing the extent of hallucination. It also examines existing strategies for mitigating hallucination in LFMs and discusses potential directions for future research in this area. Essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in LFMs.
</details>
<details>
<summary>摘要</summary>
幻想在基础模型（FM）中指的是生成不符事实或包含虚假信息的内容。这篇评论文章提供了对最近努力防止幻想的全面回顾，尤其是对大型基础模型（LFMs）的幻想问题。文章分类了LFMs中幻想现象的不同类型，并设置了评估幻想程度的评价标准。它还检查了现有的防止幻想策略，并讨论了未来研究的可能性。简言之，文章提供了幻想在LFMs中的挑战和解决方案的全面检查。
</details></li>
</ul>
<hr>
<h2 id="SAGE-Structured-Attribute-Value-Generation-for-Billion-Scale-Product-Catalogs"><a href="#SAGE-Structured-Attribute-Value-Generation-for-Billion-Scale-Product-Catalogs" class="headerlink" title="SAGE: Structured Attribute Value Generation for Billion-Scale Product Catalogs"></a>SAGE: Structured Attribute Value Generation for Billion-Scale Product Catalogs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05920">http://arxiv.org/abs/2309.05920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Athanasios N. Nikolakopoulos, Swati Kaul, Siva Karthik Gade, Bella Dubrov, Umit Batur, Suleiman Ali Khan</li>
<li>for: 这篇论文旨在为电商平台中的产品 attribute 值预测做出提高。</li>
<li>methods: 该论文提出了一种新的 attribute-value 预测方法，基于 Seq2Seq 概率模型，可以适应不同语言、产品类型和目标 attribute。该方法可以预测 attribute 值，even when such values are mentioned implicitly using periphrastic language, or not-at-all-as is the case for common-sense defaults。</li>
<li>results: 实验表明，该方法可以有效地预测 attribute 值，并且比之前的方法更高效。此外，该方法还可以预测 attribute 值在零例教程下，从而减少需要训练的数据量。<details>
<summary>Abstract</summary>
We introduce SAGE; a Generative LLM for inferring attribute values for products across world-wide e-Commerce catalogs. We introduce a novel formulation of the attribute-value prediction problem as a Seq2Seq summarization task, across languages, product types and target attributes. Our novel modeling approach lifts the restriction of predicting attribute values within a pre-specified set of choices, as well as, the requirement that the sought attribute values need to be explicitly mentioned in the text. SAGE can infer attribute values even when such values are mentioned implicitly using periphrastic language, or not-at-all-as is the case for common-sense defaults. Additionally, SAGE is capable of predicting whether an attribute is inapplicable for the product at hand, or non-obtainable from the available information. SAGE is the first method able to tackle all aspects of the attribute-value-prediction task as they arise in practical settings in e-Commerce catalogs. A comprehensive set of experiments demonstrates the effectiveness of the proposed approach, as well as, its superiority against state-of-the-art competing alternatives. Moreover, our experiments highlight SAGE's ability to tackle the task of predicting attribute values in zero-shot setting; thereby, opening up opportunities for significantly reducing the overall number of labeled examples required for training.
</details>
<details>
<summary>摘要</summary>
我们介绍SAGE；一种生成式LLM用于在全球电子商务目录中预测产品的属性值。我们提出了一种新的属性值预测问题的表述方式，即将属性值预测问题转化为一个Seq2Seq摘要任务，跨语言、产品类型和目标属性。我们的新的模型方法解决了预测属性值时的限制，包括预测属性值必须在先Specified的选择范围内，以及文本中必须直接提到属性值。SAGE可以在文本中推断属性值，即使这些值是使用射igrated语言表达或者不直接提到。此外，SAGE还可以预测产品上的属性是否存在或者可以从可用信息中获取。SAGE是第一种能够在实际设置中解决所有属性值预测问题的方法。一系列实验证明了我们的方法的有效性和其对State-of-the-art的竞争方法的超越性。此外，我们的实验还 highlight了SAGE在零shot设置下预测属性值的能力，从而开启了减少训练数据的机会。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-LLMs-do-not-Understand-Language-Towards-Symbolic-Explainable-and-Ontologically-Based-LLMs"><a href="#Stochastic-LLMs-do-not-Understand-Language-Towards-Symbolic-Explainable-and-Ontologically-Based-LLMs" class="headerlink" title="Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs"></a>Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05918">http://arxiv.org/abs/2309.05918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Walid S. Saba</li>
<li>for: 这篇论文是关于大语言模型（LLMs）的研究，主要探讨了 LLMS 的限制和不足，并提出了一种符号驱动的语言模型的建议。</li>
<li>methods: 本论文使用了数据驱动的方法，对 LLMS 进行了分析和评估，并提出了一种符号驱动的语言模型的建议。</li>
<li>results: 本论文的研究结果表明，LLMS 存在许多限制和不足，如不能准确地提供实际信息，因为它们所有的文本都被视为一样有价值的；此外，LLMS 的知识也会被归类为微特征（weights）中，无法归纳到有意义的概念中；此外，LLMS 在某些语言上也会出现偏差。<details>
<summary>Abstract</summary>
In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts. Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbol-ic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbolic setting resulting in symbolic, explainable, and ontologically grounded language models.
</details>
<details>
<summary>摘要</summary>
我们认为大量数据驱动的大型自然语言模型（LLM）的热情是有所误导的，主要有以下几点原因：1. LLM不能依赖于事实信息，因为它们对所有的文本（事实或非事实）都是一样的；2.由于它们的非符号性质，LMM所获得的语言知识都将被归类为微特征（ weights），这些微特征都是无法单独解释的；3. LLM在某些语言上将不能做出正确的推理，如名词复合、共Predication、量词范围不确定性、意思上的context。因为我们认为数据驱动大型自然语言模型（LLM）的成功不是符号vs非符号的问题，而是应用大规模的底层逆工程Strategy的成功，因此在这篇论文中，我们建议在符号环境中应用有效的底层逆工程策略，从而获得符号、可解释、基于ontology的语言模型。
</details></li>
</ul>
<hr>
<h2 id="ACT-Empowering-Decision-Transformer-with-Dynamic-Programming-via-Advantage-Conditioning"><a href="#ACT-Empowering-Decision-Transformer-with-Dynamic-Programming-via-Advantage-Conditioning" class="headerlink" title="ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning"></a>ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05915">http://arxiv.org/abs/2309.05915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxiao Gao, Chenyang Wu, Mingjun Cao, Rui Kong, Zongzhang Zhang, Yang Yu</li>
<li>for: 提高offline政策优化中DT的表现，透过带有表达性的序列模型技术来实现行动生成。</li>
<li>methods: 使用动态计划来强化DT，包括三步：首先，使用样本中的值迭代来获取估计价值函数，其中具有MDP结构的动态计划特性。其次，评估行动质量基于估计的优势。引入两种优势估计器，IAE和GAE，适用于不同任务。最后，使用优势 Conditioned Transformer (ACT) 来生成基于估计优势的行动。</li>
<li>results: 测试结果表明，通过借鉴动态计划的能力，ACT能够在环境杂化下表现出效果很好，与基eline方法在各种标准准则上表现出色。此外，我们通过减少分析来检验ACT的不同设计方案的影响。<details>
<summary>Abstract</summary>
Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation results validate that, by leveraging the power of dynamic programming, ACT demonstrates effective trajectory stitching and robust action generation in spite of the environmental stochasticity, outperforming baseline methods across various benchmarks. Additionally, we conduct an in-depth analysis of ACT's various design choices through ablation studies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Quality-Agnostic-Deepfake-Detection-with-Intra-model-Collaborative-Learning"><a href="#Quality-Agnostic-Deepfake-Detection-with-Intra-model-Collaborative-Learning" class="headerlink" title="Quality-Agnostic Deepfake Detection with Intra-model Collaborative Learning"></a>Quality-Agnostic Deepfake Detection with Intra-model Collaborative Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05911">http://arxiv.org/abs/2309.05911</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/deepfake-project/qad-iccv23">https://bitbucket.org/deepfake-project/qad-iccv23</a></li>
<li>paper_authors: Binh M. Le, Simon S. Woo</li>
<li>for: 这个研究旨在提出一个 universial 的内部型别学习框架，以实现不同质量的深伪检测。</li>
<li>methods: 我们使用一个名为 QAD 的quality-agnostic deepfake detection方法，通过观察通用错误预期函数的上限，将不同质量水平的图像间的中间表现dependency最大化。</li>
<li>results: 我们的 QAD 模型在七个受测 dataset 上进行了广泛的实验，与先前的 SOTA benchmark 相比，表现出色。<details>
<summary>Abstract</summary>
Deepfake has recently raised a plethora of societal concerns over its possible security threats and dissemination of fake information. Much research on deepfake detection has been undertaken. However, detecting low quality as well as simultaneously detecting different qualities of deepfakes still remains a grave challenge. Most SOTA approaches are limited by using a single specific model for detecting certain deepfake video quality type. When constructing multiple models with prior information about video quality, this kind of strategy incurs significant computational cost, as well as model and training data overhead. Further, it cannot be scalable and practical to deploy in real-world settings. In this work, we propose a universal intra-model collaborative learning framework to enable the effective and simultaneous detection of different quality of deepfakes. That is, our approach is the quality-agnostic deepfake detection method, dubbed QAD . In particular, by observing the upper bound of general error expectation, we maximize the dependency between intermediate representations of images from different quality levels via Hilbert-Schmidt Independence Criterion. In addition, an Adversarial Weight Perturbation module is carefully devised to enable the model to be more robust against image corruption while boosting the overall model's performance. Extensive experiments over seven popular deepfake datasets demonstrate the superiority of our QAD model over prior SOTA benchmarks.
</details>
<details>
<summary>摘要</summary>
为了解决这个问题，我们提出了一种通用内部协作学习框架，以实现不同质量深圳投影的同时检测。具体来说，我们通过观察总体错误预期的上限来 maximize 图像不同质量水平之间的依赖关系，使用希尔伯特-施密特独立性 критерион。此外，我们还妥善地设计了一个对抗权重偏移模块，以使模型更加抗 resize 而增强整体模型的表现。我们在七个流行的深圳投影数据集上进行了广泛的实验，并证明了我们的 QAD 模型在先前的 SOTA 标准之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Llama-2-and-GPT-3-LLMs-for-HPC-kernels-generation"><a href="#Comparing-Llama-2-and-GPT-3-LLMs-for-HPC-kernels-generation" class="headerlink" title="Comparing Llama-2 and GPT-3 LLMs for HPC kernels generation"></a>Comparing Llama-2 and GPT-3 LLMs for HPC kernels generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07103">http://arxiv.org/abs/2309.07103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Valero-Lara, Alexis Huante, Mustafa Al Lail, William F. Godoy, Keita Teranishi, Prasanna Balaprakash, Jeffrey S. Vetter</li>
<li>for: 本研究用于评估开源模型Llama-2在不同并行编程模型和语言（如C++：OpenMP、OpenMP Offload、OpenACC、CUDA、HIP；Fortran：OpenMP、OpenMP Offload、OpenACC；Python：numpy、Numba、pyCUDA、cuPy；Julia：Threads、CUDA.jl、AMDGPU.jl）中生成高性能计算kernels（如AXPY、GEMV、GEMM）的可能性。</li>
<li>methods: 本研究基于我们之前的工作，该基于OpenAI Codex，一个基于GPT-3的descendant，通过GitHub Copilot生成类似kernels的简单提示。我们的目标是比较Llama-2和我们原始GPT-3基线的准确率，使用相似的指标。</li>
<li>results: Llama-2模型在生成kernels时显示了竞争力或更高的准确率，而Copilot生成的代码更可靠但 menos优化，相反，Llama-2生成的代码虽然更不可靠但更高效当 corrrect。<details>
<summary>Abstract</summary>
We evaluate the use of the open-source Llama-2 model for generating well-known, high-performance computing kernels (e.g., AXPY, GEMV, GEMM) on different parallel programming models and languages (e.g., C++: OpenMP, OpenMP Offload, OpenACC, CUDA, HIP; Fortran: OpenMP, OpenMP Offload, OpenACC; Python: numpy, Numba, pyCUDA, cuPy; and Julia: Threads, CUDA.jl, AMDGPU.jl). We built upon our previous work that is based on the OpenAI Codex, which is a descendant of GPT-3, to generate similar kernels with simple prompts via GitHub Copilot. Our goal is to compare the accuracy of Llama-2 and our original GPT-3 baseline by using a similar metric. Llama-2 has a simplified model that shows competitive or even superior accuracy. We also report on the differences between these foundational large language models as generative AI continues to redefine human-computer interactions. Overall, Copilot generates codes that are more reliable but less optimized, whereas codes generated by Llama-2 are less reliable but more optimized when correct.
</details>
<details>
<summary>摘要</summary>
我们评估了基于开源的Llama-2模型来生成知名度高、性能优秀的计算器件（例如AXPY、GEMV、GEMM）在不同的并行编程模型和语言（例如C++：OpenMP、OpenMP Offload、OpenACC、CUDA、HIP；Fortran：OpenMP、OpenMP Offload、OpenACC；Python：numpy、Numba、pyCUDA、cuPy；Julia：Threads、CUDA.jl、AMDGPU.jl）上。我们基于我们之前的工作，这是基于OpenAI Codex，这是GPT-3的后代，通过GitHub Copilot来生成类似的kernels。我们的目标是将Llama-2和我们原始的GPT-3基线相比，使用相似的度量。Llama-2有简化的模型，并显示了竞争或更高的准确性。我们还报告了这些基础的大语言模型在人机交互中如何不断定义。总之，Copilot生成的代码更可靠但较少优化，而Llama-2生成的代码虽然较不可靠但当正确时更高效。
</details></li>
</ul>
<hr>
<h2 id="Strategic-Behavior-of-Large-Language-Models-Game-Structure-vs-Contextual-Framing"><a href="#Strategic-Behavior-of-Large-Language-Models-Game-Structure-vs-Contextual-Framing" class="headerlink" title="Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing"></a>Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05898">http://arxiv.org/abs/2309.05898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nunzio Lorè, Babak Heydari</li>
<li>for: 本研究探讨三种大语言模型（GPT-3.5、GPT-4和LLaMa-2）在游戏理论框架下的策略决策能力。</li>
<li>methods: 研究使用四个标准两人游戏（囚徒困境、鹿猎、雪崩和囚徒欢乐）来探讨这些模型在社会决策中的 navigate能力。</li>
<li>results: 研究发现，GPT-3.5受到上下文框架的影响很大，但是它对抽象的策略理解能力有限。GPT-4和LLaMa-2根据游戏结构和上下文进行策略调整，但LLaMa-2表现出更加细腻的游戏机制理解。这些结果 highlights current LLMs的限制和不同的聪明程度，警告不要在需要复杂策略理解的任务中不经过训练使用LLMs。<details>
<summary>Abstract</summary>
This paper investigates the strategic decision-making capabilities of three Large Language Models (LLMs): GPT-3.5, GPT-4, and LLaMa-2, within the framework of game theory. Utilizing four canonical two-player games -- Prisoner's Dilemma, Stag Hunt, Snowdrift, and Prisoner's Delight -- we explore how these models navigate social dilemmas, situations where players can either cooperate for a collective benefit or defect for individual gain. Crucially, we extend our analysis to examine the role of contextual framing, such as diplomatic relations or casual friendships, in shaping the models' decisions. Our findings reveal a complex landscape: while GPT-3.5 is highly sensitive to contextual framing, it shows limited ability to engage in abstract strategic reasoning. Both GPT-4 and LLaMa-2 adjust their strategies based on game structure and context, but LLaMa-2 exhibits a more nuanced understanding of the games' underlying mechanics. These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, cautioning against their unqualified use in tasks requiring complex strategic reasoning.
</details>
<details>
<summary>摘要</summary>
The findings show that GPT-3.5 is highly sensitive to contextual framing, but has limited ability to engage in abstract strategic reasoning. Both GPT-4 and LLaMa-2 adjust their strategies based on game structure and context, but LLaMa-2 demonstrates a more nuanced understanding of the games' underlying mechanics. These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, and caution against their unqualified use in tasks requiring complex strategic reasoning.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/12/cs.AI_2023_09_12/" data-id="clp869tqy003rk5881wu77cgw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/12/cs.CL_2023_09_12/" class="article-date">
  <time datetime="2023-09-12T11:00:00.000Z" itemprop="datePublished">2023-09-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/12/cs.CL_2023_09_12/">cs.CL - 2023-09-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="RT-LM-Uncertainty-Aware-Resource-Management-for-Real-Time-Inference-of-Language-Models"><a href="#RT-LM-Uncertainty-Aware-Resource-Management-for-Real-Time-Inference-of-Language-Models" class="headerlink" title="RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models"></a>RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06619">http://arxiv.org/abs/2309.06619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Li, Zexin Li, Wei Yang, Cong Liu</li>
<li>for: 这个论文是为了研究语言模型（LM）在实时推理中的性能问题而写的。</li>
<li>methods: 这个论文使用了一种叫做RT-LM的uncertainty-aware资源管理系统来解决LM在实时推理中的性能问题。</li>
<li>results: 根据实验结果，RT-LM可以减少响应时间和提高吞吐量，但是runtime开销很小。<details>
<summary>Abstract</summary>
Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses. Though exhibiting a promising future for various applications such as conversation AI, these LMs face deployment challenges on various devices due to their extreme computational cost and unpredictable inference latency. Such varied inference latency, identified as a consequence of uncertainty intrinsic to the nature of language, can lead to computational inefficiency and degrade the overall performance of LMs, especially under high-traffic workloads. Unfortunately, the bandwidth of these uncertainty sources is extensive, complicating the prediction of latency and the effects emanating from such uncertainties. To understand and mitigate the impact of uncertainty on real-time response-demanding systems, we take the first step to comprehend, quantify and optimize these uncertainty-induced latency performance variations in LMs. Specifically, we present RT-LM, an uncertainty-aware resource management ecosystem for real-time inference of LMs. RT-LM innovatively quantifies how specific input uncertainties, adversely affect latency, often leading to an increased output length. Exploiting these insights, we devise a lightweight yet effective method to dynamically correlate input text uncertainties with output length at runtime. Utilizing this quantification as a latency heuristic, we integrate the uncertainty information into a system-level scheduler which explores several uncertainty-induced optimization opportunities, including uncertainty-aware prioritization, dynamic consolidation, and strategic CPU offloading. Quantitative experiments across five state-of-the-art LMs on two hardware platforms demonstrates that RT-LM can significantly reduce the average response time and improve throughput while incurring a rather small runtime overhead.
</details>
<details>
<summary>摘要</summary>
RT-LM aims to comprehend, quantify, and optimize the uncertainty-induced latency performance variations in LMs. We present a lightweight yet effective method to dynamically correlate input text uncertainties with output length at runtime. By exploiting these insights, we integrate the uncertainty information into a system-level scheduler, which explores several uncertainty-induced optimization opportunities, including uncertainty-aware prioritization, dynamic consolidation, and strategic CPU offloading.Experiments on five state-of-the-art LMs on two hardware platforms show that RT-LM can significantly reduce the average response time and improve throughput while incurring a small runtime overhead. Our approach can effectively mitigate the impact of uncertainty on real-time response-demanding systems, enabling the widespread adoption of LMs in various applications such as conversation AI.
</details></li>
</ul>
<hr>
<h2 id="Text-Encoders-Lack-Knowledge-Leveraging-Generative-LLMs-for-Domain-Specific-Semantic-Textual-Similarity"><a href="#Text-Encoders-Lack-Knowledge-Leveraging-Generative-LLMs-for-Domain-Specific-Semantic-Textual-Similarity" class="headerlink" title="Text Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity"></a>Text Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06541">http://arxiv.org/abs/2309.06541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Gatto, Omar Sharif, Parker Seegmiller, Philip Bohlman, Sarah Masud Preum</li>
<li>for: 本研究旨在探讨语义文本相似性（STS）在不同任务上的评估，并发现STS在大量语言模型（LLM）评估中受到了忽略。</li>
<li>methods: 本研究表明，STS可以被视为文本生成问题，同时保持多个STS标准准的高性能。此外，我们还证明了使用生成语言模型（LLM）可以在具有复杂semantic关系的文本之间的semantic相似性评估中表现出色，并且在医学、政治和体育等领域中的新收集的STS挑战集上表现出色。</li>
<li>results: 我们的结果显示，使用生成语言模型（LLM）在具有世界知识的STS任务上的性能比使用编码器基于模型更高，在医学、政治和体育等领域的新收集的STS挑战集上，生成语言模型与STS特定的提示策略相结合可以 дости到状态之巅的性能。<details>
<summary>Abstract</summary>
Amidst the sharp rise in the evaluation of large language models (LLMs) on various tasks, we find that semantic textual similarity (STS) has been under-explored. In this study, we show that STS can be cast as a text generation problem while maintaining strong performance on multiple STS benchmarks. Additionally, we show generative LLMs significantly outperform existing encoder-based STS models when characterizing the semantic similarity between two texts with complex semantic relationships dependent on world knowledge. We validate this claim by evaluating both generative LLMs and existing encoder-based STS models on three newly collected STS challenge sets which require world knowledge in the domains of Health, Politics, and Sports. All newly collected data is sourced from social media content posted after May 2023 to ensure the performance of closed-source models like ChatGPT cannot be credited to memorization. Our results show that, on average, generative LLMs outperform the best encoder-only baselines by an average of 22.3% on STS tasks requiring world knowledge. Our results suggest generative language models with STS-specific prompting strategies achieve state-of-the-art performance in complex, domain-specific STS tasks.
</details>
<details>
<summary>摘要</summary>
在大语言模型（LLM）评估的快速上升中， semantic textual similarity（STS）却被忽视了。在这项研究中，我们发现STS可以被视为文本生成问题，同时保持多个STS benchmark task的优秀表现。此外，我们发现生成型LLM在描述两个文本之间的Semantic关系时，表现出色，特别是在具有世界知识的复杂Semantic关系上。我们 validate这一点通过对生成型LLM和现有encoder-based STS模型在健康、政治和运动等领域的三个新收集的STS挑战集上进行评估。所有新收集的数据都来自社交媒体上的内容，其中大部分是在2023年5月之后发布的，以避免Memorization的问题。我们的结果表明，在需要世界知识的STS任务中，生成型LLM平均表现出色，高于最佳encoder-only baseline的22.3%。我们的结果表明，使用STS特定的提示策略的生成语言模型在复杂的领域特定STS任务中实现了状态的表现。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-Memotion-3-Sentiment-and-Emotion-Analysis-of-Codemixed-Hinglish-Memes"><a href="#Overview-of-Memotion-3-Sentiment-and-Emotion-Analysis-of-Codemixed-Hinglish-Memes" class="headerlink" title="Overview of Memotion 3: Sentiment and Emotion Analysis of Codemixed Hinglish Memes"></a>Overview of Memotion 3: Sentiment and Emotion Analysis of Codemixed Hinglish Memes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06517">http://arxiv.org/abs/2309.06517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyash Mishra, S Suryavardan, Megha Chakraborty, Parth Patwa, Anku Rani, Aman Chadha, Aishwarya Reganti, Amitava Das, Amit Sheth, Manoj Chinnakotla, Asif Ekbal, Srijan Kumar</li>
<li>for: 本研究的目的是对互联网上的投EGIN{CJK}{UTF8}{ermann}进行分析，以了解它们在线上讨论中的影响。</li>
<li>methods: 本研究使用了Memotion 3共同任务的独立标注数据集，包括情感（Task A）、情绪（Task B）和情绪强度（Task C）。参与者可以使用CLIP、BERT修改、ViT等模型，以及学生教师模型、融合和投票等方法。</li>
<li>results: 本研究发现了50多个参与者注册参与共同任务，5个参与者提交了测试集的最终Submission。最佳最终F1分数为Task A为34.41，Task B为79.77，Task C为59.82。<details>
<summary>Abstract</summary>
Analyzing memes on the internet has emerged as a crucial endeavor due to the impact this multi-modal form of content wields in shaping online discourse. Memes have become a powerful tool for expressing emotions and sentiments, possibly even spreading hate and misinformation, through humor and sarcasm. In this paper, we present the overview of the Memotion 3 shared task, as part of the DeFactify 2 workshop at AAAI-23. The task released an annotated dataset of Hindi-English code-mixed memes based on their Sentiment (Task A), Emotion (Task B), and Emotion intensity (Task C). Each of these is defined as an individual task and the participants are ranked separately for each task. Over 50 teams registered for the shared task and 5 made final submissions to the test set of the Memotion 3 dataset. CLIP, BERT modifications, ViT etc. were the most popular models among the participants along with approaches such as Student-Teacher model, Fusion, and Ensembling. The best final F1 score for Task A is 34.41, Task B is 79.77 and Task C is 59.82.
</details>
<details>
<summary>摘要</summary>
互联网上的迷因分析已经成为一项重要的专业，因为这种多Modal的内容可以影响网络上的讨论。迷因已经成为一个强大的表达情感和意见的工具，可能 même 传播仇恨和误information，通过幽默和讽刺。在这篇文章中，我们介绍了Memotion 3共同任务的概观，这是DeFactify 2会议上的一部分。这个任务发布了统计数据集的印地语-英语混合迷因，并分为三个任务：情感（任务A）、情感（任务B）和情感强度（任务C）。每个任务都是一个独立的任务，参赛者会被分别排名。这个任务获得了超过50队的注册，并有5队提交了测试集的最终Submission。参赛者主要使用CLIP、BERT修改和ViT等模型，以及学生教师模型、融合和投票等方法。最佳的最终F1分 для任务A是34.41，任务B是79.77，任务C是59.82。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Models-and-Weak-Supervision-for-Social-Media-data-annotation-an-evaluation-using-COVID-19-self-reported-vaccination-tweets"><a href="#Leveraging-Large-Language-Models-and-Weak-Supervision-for-Social-Media-data-annotation-an-evaluation-using-COVID-19-self-reported-vaccination-tweets" class="headerlink" title="Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets"></a>Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06503">http://arxiv.org/abs/2309.06503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramya Tekumalla, Juan M. Banda</li>
<li>for: 本研究目的是为了评估大语言模型GPT-4（2023年3月23日版本）和弱监督来自Twitter上的COVID-19疫苗相关 tweet 自动标注表现，并与人类标注者进行比较。</li>
<li>methods: 本研究使用了GPT-4（2023年3月23日版本）提供标签，无需进一步 fine-tuning 或指导，使用单射模式（无需额外提示）。</li>
<li>results: 研究发现，GPT-4（2023年3月23日版本）在自动标注COVID-19疫苗相关 tweet 方面的表现与人类标注者相当，且可以快速、高效地进行自动标注。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has presented significant challenges to the healthcare industry and society as a whole. With the rapid development of COVID-19 vaccines, social media platforms have become a popular medium for discussions on vaccine-related topics. Identifying vaccine-related tweets and analyzing them can provide valuable insights for public health research-ers and policymakers. However, manual annotation of a large number of tweets is time-consuming and expensive. In this study, we evaluate the usage of Large Language Models, in this case GPT-4 (March 23 version), and weak supervision, to identify COVID-19 vaccine-related tweets, with the purpose of comparing performance against human annotators. We leveraged a manu-ally curated gold-standard dataset and used GPT-4 to provide labels without any additional fine-tuning or instructing, in a single-shot mode (no additional prompting).
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行带来了医疗业和社会全面的挑战。随着 COVID-19 疫苗的快速发展，社交媒体平台上有大量有关疫苗的讨论。可以通过分析这些微博来获得有价值的公共卫生研究人员和政策制定者的洞察。但是，手动标注大量微博是时间consuming 和昂贵的。在本研究中，我们评估了大型自然语言模型（GPT-4，2023年3月23日版）和弱级指导，用于标识 COVID-19 疫苗相关的微博，并与人工标注器进行比较。我们利用了手动精心挑选的金标准数据集，并使用 GPT-4 提供标签，无需任何额外的调整或指导，在单射模式下（没有额外的提示）。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Models-for-Automated-Dialogue-Analysis"><a href="#Leveraging-Large-Language-Models-for-Automated-Dialogue-Analysis" class="headerlink" title="Leveraging Large Language Models for Automated Dialogue Analysis"></a>Leveraging Large Language Models for Automated Dialogue Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06490">http://arxiv.org/abs/2309.06490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emorynlp/gpt-abceval">https://github.com/emorynlp/gpt-abceval</a></li>
<li>paper_authors: Sarah E. Finch, Ellie S. Paek, Jinho D. Choi</li>
<li>For: The paper aims to assess the ability of a state-of-the-art large language model (LLM) to detect nine categories of undesirable behaviors in real human-bot dialogues.* Methods: The paper uses a state-of-the-art LLM, ChatGPT-3.5, to perform dialogue behavior detection and compares its performance with specialized detection models.* Results: The paper finds that neither ChatGPT nor specialized models have yet achieved satisfactory results for this task, falling short of human performance. However, ChatGPT shows promising potential and often outperforms specialized detection models.Here are the three points in Simplified Chinese text:* For: 本研究用于评估一个状态rut-of-the-art的大语言模型（LLM）在真实人机对话中自动识别九种不良行为的能力。* Methods: 本研究使用状态rut-of-the-art的LLM，ChatGPT-3.5，进行对话行为检测，并与专门的检测模型进行比较。* Results: 本研究发现 neither ChatGPT nor specialized models have yet achieved satisfactory results for this task, falling short of human performance. However, ChatGPT shows promising potential and often outperforms specialized detection models.<details>
<summary>Abstract</summary>
Developing high-performing dialogue systems benefits from the automatic identification of undesirable behaviors in system responses. However, detecting such behaviors remains challenging, as it draws on a breadth of general knowledge and understanding of conversational practices. Although recent research has focused on building specialized classifiers for detecting specific dialogue behaviors, the behavior coverage is still incomplete and there is a lack of testing on real-world human-bot interactions. This paper investigates the ability of a state-of-the-art large language model (LLM), ChatGPT-3.5, to perform dialogue behavior detection for nine categories in real human-bot dialogues. We aim to assess whether ChatGPT can match specialized models and approximate human performance, thereby reducing the cost of behavior detection tasks. Our findings reveal that neither specialized models nor ChatGPT have yet achieved satisfactory results for this task, falling short of human performance. Nevertheless, ChatGPT shows promising potential and often outperforms specialized detection models. We conclude with an in-depth examination of the prevalent shortcomings of ChatGPT, offering guidance for future research to enhance LLM capabilities.
</details>
<details>
<summary>摘要</summary>
发展高性能对话系统受益于自动识别对话系统回应中不良行为的能力。然而，检测这些行为仍然是一项挑战，因为它需要对对话实践的广泛知识和理解。虽然最近的研究主要集中在建立特殊的对话行为分类器，但行为覆盖率仍然不完整，并且缺乏实际人机交互的测试。本文研究了一个现代大语言模型（LLM）ChatGPT-3.5在真实人机对话中的对话行为检测能力。我们希望评估ChatGPT是否能够与专门的模型相匹配，并且 approximates human performance，以降低行为检测任务的成本。我们发现 neither specialized models nor ChatGPT have yet achieved satisfactory results for this task, falling short of human performance。然而，ChatGPT表现了良好的潜力，并且经常超过专门的检测模型。我们结束于对ChatGPT缺陷的深入分析，并提供未来研究进一步增强LLM能力的指导。
</details></li>
</ul>
<hr>
<h2 id="Widely-Interpretable-Semantic-Representation-Frameless-Meaning-Representation-for-Broader-Applicability"><a href="#Widely-Interpretable-Semantic-Representation-Frameless-Meaning-Representation-for-Broader-Applicability" class="headerlink" title="Widely Interpretable Semantic Representation: Frameless Meaning Representation for Broader Applicability"></a>Widely Interpretable Semantic Representation: Frameless Meaning Representation for Broader Applicability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06460">http://arxiv.org/abs/2309.06460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lydia Feng, Gregor Williamson, Han He, Jinho D. Choi</li>
<li>for: This paper presents a novel semantic representation, WISeR, to overcome challenges in Abstract Meaning Representation (AMR).</li>
<li>methods: The paper examines the numbered arguments of predicates in AMR and converts them to thematic roles, improving the inter-annotator agreement for beginner and experienced annotators.</li>
<li>results: The WISeR model exhibits higher accuracy than its AMR counterpart across the board, demonstrating that WISeR is easier for parsers to learn.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文提出了一种新的Semantic Representation，WISeR，以解决Abstract Meaning Representation（AMR）中的挑战。</li>
<li>methods: 论文分析了AMR中 predicate的数字化Arguments，并将其转换为不需要参考semantic frames的主题角色，从而提高了 beginner和经验 annotator之间的协议一致性。</li>
<li>results: WISeR模型在所有板块中表现出了高度的准确性，证明WISeR更易 für parser 学习。<details>
<summary>Abstract</summary>
This paper presents a novel semantic representation, WISeR, that overcomes challenges for Abstract Meaning Representation (AMR). Despite its strengths, AMR is not easily applied to languages or domains without predefined semantic frames, and its use of numbered arguments results in semantic role labels, which are not directly interpretable and are semantically overloaded for parsers. We examine the numbered arguments of predicates in AMR and convert them to thematic roles that do not require reference to semantic frames. We create a new corpus of 1K English dialogue sentences annotated in both WISeR and AMR. WISeR shows stronger inter-annotator agreement for beginner and experienced annotators, with beginners becoming proficient in WISeR annotation more quickly. Finally, we train a state-of-the-art parser on the AMR 3.0 corpus and a WISeR corpus converted from AMR 3.0. The parser is evaluated on these corpora and our dialogue corpus. The WISeR model exhibits higher accuracy than its AMR counterpart across the board, demonstrating that WISeR is easier for parsers to learn.
</details>
<details>
<summary>摘要</summary>
We create a new corpus of 1,000 English dialogue sentences annotated in both WISeR and AMR. Our results show that WISeR has stronger inter-annotator agreement for both beginner and experienced annotators, with beginners becoming proficient in WISeR annotation more quickly. Additionally, we train a state-of-the-art parser on the AMR 3.0 corpus and a WISeR corpus converted from AMR 3.0. The parser is evaluated on these corpora and our dialogue corpus, and the WISeR model exhibits higher accuracy than its AMR counterpart across the board, demonstrating that WISeR is easier for parsers to learn.
</details></li>
</ul>
<hr>
<h2 id="Recovering-from-Privacy-Preserving-Masking-with-Large-Language-Models"><a href="#Recovering-from-Privacy-Preserving-Masking-with-Large-Language-Models" class="headerlink" title="Recovering from Privacy-Preserving Masking with Large Language Models"></a>Recovering from Privacy-Preserving Masking with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08628">http://arxiv.org/abs/2309.08628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arpita Vats, Zhe Liu, Peng Su, Debjyoti Paul, Yingyi Ma, Yutong Pang, Zeeshan Ahmed, Ozlem Kalinli</li>
<li>for: 本研究旨在提高模型适应性，以便处理代理训练数据和实际用户数据之间的差异。</li>
<li>methods: 本研究使用大语言模型（LLM）来提出匿名标识符替换的方法，并对其效果进行了Empirical研究。特别是，我们提出了多种预训练和精度调整的LLM基本方法，并对各种数据集进行了比较性研究。</li>
<li>results: 实验结果表明，使用匿名标识符替换后，模型在下游自然语言处理任务中能够保持与原始数据无隐私保护的同等性能。<details>
<summary>Abstract</summary>
Model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking.
</details>
<details>
<summary>摘要</summary>
模型适应是处理代理训练数据和实际用户数据接收的差异的关键。为了有效地进行适应，用户的文本数据通常会被存储在服务器或本地设备上，以便下游自然语言处理（NLP）模型可以直接使用这些领域数据进行训练。然而，这可能会带来隐私和安全问题，因为披露用户信息会增加对敌人的风险。将用户信息从文本数据中替换为通用标识符已经被研究。在这种工作中，我们利用大语言模型（LLM）来建议替换的Marker Token，并对这些方法的效果进行了实验研究。specifically，我们提出了多种预训练和精度调整的LLM基于方法，并在不同的数据集上进行了实验比较这些方法的效果。实验结果表明，使用隐藏 corpora 进行训练的模型能够达到与没有隐藏 token 训练的模型相同的性能。
</details></li>
</ul>
<hr>
<h2 id="Cited-Text-Spans-for-Citation-Text-Generation"><a href="#Cited-Text-Spans-for-Citation-Text-Generation" class="headerlink" title="Cited Text Spans for Citation Text Generation"></a>Cited Text Spans for Citation Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06365">http://arxiv.org/abs/2309.06365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangci Li, Yi-Hui Lee, Jessica Ouyang</li>
<li>for: 这个论文主要是为了解决科学论文中自动生成相关工作的问题。</li>
<li>methods: 该论文使用了一种新的方法，即基于文献标注（CTS）的方法，而不是基于摘要的方法。</li>
<li>results: 该论文表明，使用CTS可以提高自动生成相关工作的准确性，并且可以避免非事实的幻想。<details>
<summary>Abstract</summary>
Automatic related work generation must ground their outputs to the content of the cited papers to avoid non-factual hallucinations, but due to the length of scientific documents, existing abstractive approaches have conditioned only on the cited paper \textit{abstracts}. We demonstrate that the abstract is not always the most appropriate input for citation generation and that models trained in this way learn to hallucinate. We propose to condition instead on the \textit{cited text span} (CTS) as an alternative to the abstract. Because manual CTS annotation is extremely time- and labor-intensive, we experiment with automatic, ROUGE-based labeling of candidate CTS sentences, achieving sufficiently strong performance to substitute for expensive human annotations, and we propose a human-in-the-loop, keyword-based CTS retrieval approach that makes generating citation texts grounded in the full text of cited papers both promising and practical.
</details>
<details>
<summary>摘要</summary>
自动生成相关工作必须将输出锚定到引用文献中的内容，以避免非实际的幻觉。但由于科学文献的长度，现有的抽象方法只做到了基于引用文献摘要进行conditioning。我们示示了摘要并不总是最适合的引用生成输入，并且模型在这种情况下会学习幻觉。我们提议在代之之前使用引用文献中的特定文本段（CTS）作为输入，因为手动标注CTS是非常时间和劳动密集的。我们对候选CTS句子使用ROUGE基于的自动标注方法进行实验，得到了充分的表现，以至于可以代替昂贵的人工标注。此外，我们也提出了人在循环的键盘基于CTS检索方法，使得生成引用文本与引用文献的全文相关。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Predict-Concept-Ordering-for-Common-Sense-Generation"><a href="#Learning-to-Predict-Concept-Ordering-for-Common-Sense-Generation" class="headerlink" title="Learning to Predict Concept Ordering for Common Sense Generation"></a>Learning to Predict Concept Ordering for Common Sense Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06363">http://arxiv.org/abs/2309.06363</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tianhuizhang/concept_ordering">https://github.com/tianhuizhang/concept_ordering</a></li>
<li>paper_authors: Tianhui Zhang, Danushka Bollegala, Bei Peng</li>
<li>for: 本研究旨在理解输入概念的顺序对 sentence生成质量的影响，并 explore 多种语言模型（LM）和概念顺序策略的关系。</li>
<li>methods: 本研究使用了多种LM和概念顺序策略，包括CommonGen训练数据中的概念顺序，并对 sentence生成质量进行评估。</li>
<li>results: 研究发现，BART-large模型在CommonGen训练数据中的概念顺序下 consistently 表现最佳，并且比较小的LM可以在这个任务上表现更好于大型GPT3-based LLMs。此外，人工标注的输入概念集顺序可以独立地提供最佳的 sentence 生成结果，并且超过了基于概念顺序的随机化策略。<details>
<summary>Abstract</summary>
Prior work has shown that the ordering in which concepts are shown to a commonsense generator plays an important role, affecting the quality of the generated sentence. However, it remains a challenge to determine the optimal ordering of a given set of concepts such that a natural sentence covering all the concepts could be generated from a pretrained generator. To understand the relationship between the ordering of the input concepts and the quality of the generated sentences, we conduct a systematic study considering multiple language models (LMs) and concept ordering strategies. We find that BART-large model consistently outperforms all other LMs considered in this study when fine-tuned using the ordering of concepts as they appear in CommonGen training data as measured using multiple evaluation metrics. Moreover, the larger GPT3-based large language models (LLMs) variants do not necessarily outperform much smaller LMs on this task, even when fine-tuned on task-specific training data. Interestingly, human annotators significantly reorder input concept sets when manually writing sentences covering those concepts, and this ordering provides the best sentence generations independently of the LM used for the generation, outperforming a probabilistic concept ordering baseline
</details>
<details>
<summary>摘要</summary>
We found that the BART-large model consistently outperforms all other LMs considered in this study when fine-tuned using the ordering of concepts as they appear in CommonGen training data, as measured by multiple evaluation metrics. Additionally, we found that the larger GPT3-based large language models (LLMs) variants do not necessarily outperform smaller LMs on this task, even when fine-tuned on task-specific training data.Interestingly, human annotators significantly reorder input concept sets when manually writing sentences covering those concepts, and this ordering provides the best sentence generations independently of the LM used for generation, outperforming a probabilistic concept ordering baseline.
</details></li>
</ul>
<hr>
<h2 id="Re-Reading-Improves-Reasoning-in-Language-Models"><a href="#Re-Reading-Improves-Reasoning-in-Language-Models" class="headerlink" title="Re-Reading Improves Reasoning in Language Models"></a>Re-Reading Improves Reasoning in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06275">http://arxiv.org/abs/2309.06275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, Jian-guang Lou</li>
<li>for: 提高 Large Language Models (LLMs) 的推理能力</li>
<li>methods: 引入 “question re-reading” 提示策略，通过重读输入提问中嵌入的问题信息来增强 LLMs 的推理能力</li>
<li>results: 实验结果表明，该方法可以提高 LLMs 的推理能力，并且可以轻松地与其他语言模型、提示方法和集成技术结合使用<details>
<summary>Abstract</summary>
Reasoning presents a significant and challenging issue for Large Language Models (LLMs). The predominant focus of research has revolved around developing diverse prompting strategies to guide and structure the reasoning processes of LLMs. However, these approaches based on decoder-only causal language models often operate the input question in a single forward pass, potentially missing the rich, back-and-forth interactions inherent in human reasoning. Scant attention has been paid to a critical dimension, i.e., the input question itself embedded within the prompts. In response, we introduce a deceptively simple yet highly effective prompting strategy, termed question "re-reading". Drawing inspiration from human learning and problem-solving, re-reading entails revisiting the question information embedded within input prompts. This approach aligns seamlessly with the cognitive principle of reinforcement, enabling LLMs to extract deeper insights, identify intricate patterns, establish more nuanced connections, and ultimately enhance their reasoning capabilities across various tasks. Experiments conducted on a series of reasoning benchmarks serve to underscore the effectiveness and generality of our method. Moreover, our findings demonstrate that our approach seamlessly integrates with various language models, though-eliciting prompting methods, and ensemble techniques, further underscoring its versatility and compatibility in the realm of LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-first-step-is-the-hardest-Pitfalls-of-Representing-and-Tokenizing-Temporal-Data-for-Large-Language-Models"><a href="#The-first-step-is-the-hardest-Pitfalls-of-Representing-and-Tokenizing-Temporal-Data-for-Large-Language-Models" class="headerlink" title="The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models"></a>The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06236">http://arxiv.org/abs/2309.06236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Spathis, Fahim Kawsar</li>
<li>for: 这篇论文旨在探讨语言模型在人类中心任务中的应用，如移动健康感知等。</li>
<li>methods: 这篇论文使用了一些最新的研究，探讨了 популяр的语言模型在处理时间序数据时的问题，以及一些可能的解决方案，如提示调整和多模态适配器。</li>
<li>results: 这篇论文表明，许多语言模型在处理时间序数据时会错误地分词，并且提出了一些可能的解决方案，如使用轻量级嵌入层和多模态适配器。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines. Nevertheless, a notable obstacle emerges when feeding numerical/temporal data into these models, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that can help bridge this "modality gap". While the capability of language models to generalize to other modalities with minimal or no finetuning is exciting, this paper underscores the fact that their outputs cannot be meaningful if they stumble over input nuances.
</details>
<details>
<summary>摘要</summary>
In this paper, we discuss recent works that use LLMs for human-centric tasks such as mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address this issue, we highlight potential solutions such as prompt tuning with lightweight embedding layers and multimodal adapters, which can help bridge the "modality gap". While the capability of language models to generalize to other modalities with minimal or no finetuning is exciting, this paper emphasizes that their outputs cannot be meaningful if they stumble over input nuances.
</details></li>
</ul>
<hr>
<h2 id="Human-Action-Co-occurrence-in-Lifestyle-Vlogs-using-Graph-Link-Prediction"><a href="#Human-Action-Co-occurrence-in-Lifestyle-Vlogs-using-Graph-Link-Prediction" class="headerlink" title="Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction"></a>Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06219">http://arxiv.org/abs/2309.06219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oana Ignat, Santiago Castro, Weiji Li, Rada Mihalcea</li>
<li>for: 本研究旨在自动刺探人体行为协同出现情况，即判断两个人体动作是否可以同时出现在同一段时间内。</li>
<li>methods: 该研究使用了视觉和文本信息来自动推断两个人体动作是否协同出现。</li>
<li>results: 研究发现图表非常适合捕捉人体动作之间的关系，并且学习的图表表示法对该任务具有高效性和可靠性。同时，该研究还发现了一些新和相关的信息，这些信息可以在不同的数据领域中找到应用。<details>
<summary>Abstract</summary>
We introduce the task of automatic human action co-occurrence identification, i.e., determine whether two human actions can co-occur in the same interval of time. We create and make publicly available the ACE (Action Co-occurrencE) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. The ACE dataset and the code introduced in this paper are publicly available at https://github.com/MichiganNLP/vlog_action_co-occurrence.
</details>
<details>
<summary>摘要</summary>
我们介绍了自动人体动作协同识别任务，即判断两个人体动作是否可以在同一个时间间协同出现。我们创建了ACE（动作协同）数据集，包含约12k个相互协同的视觉动作对和其相应的视频片段。我们描述了基于视觉和文本信息的图链预测模型，可以自动推断两个动作是否协同出现。我们发现图是特别适合捕捉人体动作之间的关系，并且学习的图表示是我们任务中效果很高，并且在不同的数据领域中捕捉到了新和有关的信息。ACE数据集和我们在本篇文章中介绍的代码都公开可用于https://github.com/MichiganNLP/vlog_action_co-occurrence。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Dynamic-Topic-Models"><a href="#Evaluating-Dynamic-Topic-Models" class="headerlink" title="Evaluating Dynamic Topic Models"></a>Evaluating Dynamic Topic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08627">http://arxiv.org/abs/2309.08627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Charu James, Mayank Nagda, Nooshin Haji Ghassemi, Marius Kloft, Sophie Fellenz</li>
<li>for: 评估动态话题模型（DTM）中话题的进程和质量。</li>
<li>methods: 提出了一种基于话题质量和时间一致性的评估方法，并在实验中应用于合成数据和现有DTM中。</li>
<li>results: 通过人工评估和数据分析，发现该评估方法与人类判断有高度相关性，可以用于评估不同DTM的性能和指导未来研究。<details>
<summary>Abstract</summary>
There is a lack of quantitative measures to evaluate the progression of topics through time in dynamic topic models (DTMs). Filling this gap, we propose a novel evaluation measure for DTMs that analyzes the changes in the quality of each topic over time. Additionally, we propose an extension combining topic quality with the model's temporal consistency. We demonstrate the utility of the proposed measure by applying it to synthetic data and data from existing DTMs. We also conducted a human evaluation, which indicates that the proposed measure correlates well with human judgment. Our findings may help in identifying changing topics, evaluating different DTMs, and guiding future research in this area.
</details>
<details>
<summary>摘要</summary>
DTMs 缺乏时间序量化评价标准，为此，我们提出了一种新的评价标准，用于评估 DTMs 中话题的时间发展质量。此外，我们还提出了结合话题质量和模型时间一致性的扩展。我们在synthetic data和现有 DTMs 数据上应用了该标准，并进行了人类评价，结果显示了与人类判断的高度相关性。我们的发现可能有助于 indentifying changing topics, evaluating different DTMs, and guiding future research in this area.Note: "DTMs" stands for "dynamic topic models".
</details></li>
</ul>
<hr>
<h2 id="Improving-and-Evaluating-the-Detection-of-Fragmentation-in-News-Recommendations-with-the-Clustering-of-News-Story-Chains"><a href="#Improving-and-Evaluating-the-Detection-of-Fragmentation-in-News-Recommendations-with-the-Clustering-of-News-Story-Chains" class="headerlink" title="Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains"></a>Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06192">http://arxiv.org/abs/2309.06192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandra Polimeno, Myrthe Reuver, Sanne Vrijenhoek, Antske Fokkens</li>
<li>for: This paper aims to provide an extensive investigation of various approaches for quantifying Fragmentation in news recommendations, with the goal of improving the accuracy of measuring the degree of fragmentation of information streams in news recommendations.</li>
<li>methods: The paper uses Natural Language Processing (NLP) techniques, specifically agglomerative hierarchical clustering coupled with SentenceBERT text representation, to identify distinct news events, stories, or timelines and measure Fragmentation.</li>
<li>results: The paper finds that the proposed approach of agglomerative hierarchical clustering coupled with SentenceBERT text representation is substantially better at detecting Fragmentation than earlier implementations, and provides valuable insights and recommendations for stakeholders concerning the measurement and interpretation of Fragmentation.<details>
<summary>Abstract</summary>
News recommender systems play an increasingly influential role in shaping information access within democratic societies. However, tailoring recommendations to users' specific interests can result in the divergence of information streams. Fragmented access to information poses challenges to the integrity of the public sphere, thereby influencing democracy and public discourse. The Fragmentation metric quantifies the degree of fragmentation of information streams in news recommendations. Accurate measurement of this metric requires the application of Natural Language Processing (NLP) to identify distinct news events, stories, or timelines. This paper presents an extensive investigation of various approaches for quantifying Fragmentation in news recommendations. These approaches are evaluated both intrinsically, by measuring performance on news story clustering, and extrinsically, by assessing the Fragmentation scores of different simulated news recommender scenarios. Our findings demonstrate that agglomerative hierarchical clustering coupled with SentenceBERT text representation is substantially better at detecting Fragmentation than earlier implementations. Additionally, the analysis of simulated scenarios yields valuable insights and recommendations for stakeholders concerning the measurement and interpretation of Fragmentation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AKEM-Aligning-Knowledge-Base-to-Queries-with-Ensemble-Model-for-Entity-Recognition-and-Linking"><a href="#AKEM-Aligning-Knowledge-Base-to-Queries-with-Ensemble-Model-for-Entity-Recognition-and-Linking" class="headerlink" title="AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity Recognition and Linking"></a>AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity Recognition and Linking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06175">http://arxiv.org/abs/2309.06175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Lu, Zhongping Liang, Caixia Yuan, Xiaojie Wang</li>
<li>for: 这篇论文是为了解决 NLPCC 2015 年Entity Recognition and Linking Challenge 的问题而写的。</li>
<li>methods: 该方法首先扩展现有的知识库，并利用外部知识来确定候选实体，从而提高准确率。然后，从候选实体中提取特征，并使用支持向量回归和多项添加回归树作为分数函数来筛选结果。最后，通过规则进一步精细化结果以提高精度。</li>
<li>results: 该方法可以高效地处理数据，并实现了 F1 分数为 0.535。<details>
<summary>Abstract</summary>
This paper presents a novel approach to address the Entity Recognition and Linking Challenge at NLPCC 2015. The task involves extracting named entity mentions from short search queries and linking them to entities within a reference Chinese knowledge base. To tackle this problem, we first expand the existing knowledge base and utilize external knowledge to identify candidate entities, thereby improving the recall rate. Next, we extract features from the candidate entities and utilize Support Vector Regression and Multiple Additive Regression Tree as scoring functions to filter the results. Additionally, we apply rules to further refine the results and enhance precision. Our method is computationally efficient and achieves an F1 score of 0.535.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的方法来解决2015年NLPCC会议上的实体识别和连接挑战。任务是从短搜索查询中提取命名实体提及，并将其与中国知识库中的实体进行连接。为解决这个问题，我们首先扩展了现有的知识库，并利用外部知识来确定候选实体，从而提高了受检测率。接着，我们从候选实体中提取特征，并使用支持向量回归和多项加itive树分类函数来筛选结果。此外，我们还应用规则来进一步精细化结果，提高精度。我们的方法具有计算效率，并实现了F1分数0.535。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-GUA-SPA-at-IberLEF-2023-Guarani-Spanish-Code-Switching-Analysis"><a href="#Overview-of-GUA-SPA-at-IberLEF-2023-Guarani-Spanish-Code-Switching-Analysis" class="headerlink" title="Overview of GUA-SPA at IberLEF 2023: Guarani-Spanish Code Switching Analysis"></a>Overview of GUA-SPA at IberLEF 2023: Guarani-Spanish Code Switching Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06163">http://arxiv.org/abs/2309.06163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis Chiruzzo, Marvin Agüero-Torales, Gustavo Giménez-Lugo, Aldo Alvarez, Yliana Rodríguez, Santiago Góngora, Thamar Solorio</li>
<li>for: 本研究目的是探讨和分析语言混合在古拉那语和西班牙语之间的现象。</li>
<li>methods: 本研究使用了三个任务来识别和分析语言混合：一是语言标识任务，二是命名实体识别任务，三是一种新的任务是在混合语言上分类西班牙语句子的使用方式。</li>
<li>results: 三个队伍在评估阶段参与了评估，在总体来说取得了良好的结果 для任务1，但是对于任务2和3的结果则有所不同。<details>
<summary>Abstract</summary>
We present the first shared task for detecting and analyzing code-switching in Guarani and Spanish, GUA-SPA at IberLEF 2023. The challenge consisted of three tasks: identifying the language of a token, NER, and a novel task of classifying the way a Spanish span is used in the code-switched context. We annotated a corpus of 1500 texts extracted from news articles and tweets, around 25 thousand tokens, with the information for the tasks. Three teams took part in the evaluation phase, obtaining in general good results for Task 1, and more mixed results for Tasks 2 and 3.
</details>
<details>
<summary>摘要</summary>
我们现在介绍GUA-SPA的首次共同任务，即检测和分析库亚语和西班牙语的代码 switching。这个挑战包括三个任务：确定一个令素的语言，名实 recognize，以及一个新的任务，即在代码 switching 上下文中分类西班牙语句子的使用方式。我们为这个任务annotated一个新闻文章和微博中的1500篇文本，约25000个令素，以便提供任务的信息。三支队伍参与了评估阶段，在总体来说取得了良好的成绩， Task 1 的结果，而 Tasks 2 和 3 的结果则更为杂mix。
</details></li>
</ul>
<hr>
<h2 id="Prompting4Debugging-Red-Teaming-Text-to-Image-Diffusion-Models-by-Finding-Problematic-Prompts"><a href="#Prompting4Debugging-Red-Teaming-Text-to-Image-Diffusion-Models-by-Finding-Problematic-Prompts" class="headerlink" title="Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts"></a>Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06135">http://arxiv.org/abs/2309.06135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, Wei-Chen Chiu</li>
<li>for: 这个论文的目的是检测和探测文本到图像扩散模型中的安全问题，以确保这些模型不会生成不安全或版权问题的图像。</li>
<li>methods: 这篇论文使用了一种名为Prompting4Debugging（P4D）的调试和红团技术，以检测和探测文本扩散模型中的安全机制的可靠性。</li>
<li>results: 该研究发现，大约半数的原本被视为安全的提示 benchmarks 可以通过 manipulate 提示来绕过已经部署的安全机制，包括概念 removals、负提示和安全指导。这些发现表明，不进行全面测试，就可能得出假的安全感，文本到图像模型可能会生成不安全或版权问题的图像。<details>
<summary>Abstract</summary>
Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows that around half of prompts in existing safe prompting benchmarks which were originally considered "safe" can actually be manipulated to bypass many deployed safety mechanisms, including concept removal, negative prompt, and safety guidance. Our findings suggest that, without comprehensive testing, the evaluations on limited safe prompting benchmarks can lead to a false sense of safety for text-to-image models.
</details>
<details>
<summary>摘要</summary>
文本到图像扩散模型，如稳定扩散（SD），最近显示了高质量内容生成的惊人能力，成为最近一波转化AI的代表之一。然而，这种进步也带来了对这种生成技术的滥用的担忧，特别是生成版权或不安全的图像（i.e.不适合工作）。虽然努力在滥用图像/提示或移除不适合的概念/风格方面进行模型细化，但这些安全机制的可靠性在多样化问题上仍然未得到探索。在这项工作中，我们提出了Prompting4Debugging（P4D）作为调试和红团工具，自动找到 diffusion 模型的异常提示，以测试已经部署的安全机制的可靠性。我们示出了P4D工具在SD模型上的有效性，并显示了大约半数的提示在现有的安全提示benchmark中被原本认为是安全的，但实际上可以通过许多已部署的安全机制进行滥用。我们的发现表明，不完全测试可能会导致对文本到图像模型的评估产生假象的安全性。
</details></li>
</ul>
<hr>
<h2 id="Annotating-Data-for-Fine-Tuning-a-Neural-Ranker-Current-Active-Learning-Strategies-are-not-Better-than-Random-Selection"><a href="#Annotating-Data-for-Fine-Tuning-a-Neural-Ranker-Current-Active-Learning-Strategies-are-not-Better-than-Random-Selection" class="headerlink" title="Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection"></a>Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06131">http://arxiv.org/abs/2309.06131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophia Althammer, Guido Zuccon, Sebastian Hofstätter, Suzan Verberne, Allan Hanbury</li>
<li>for: 这篇论文的目的是研究基于预训语言模型（PLM）的搜寻方法在受限的训练数据和预算下的表现。</li>
<li>methods: 这篇论文使用的方法是将PLM-based rankers精致化，并考虑了两个情况：从scratch开始精致化，以及从已经精致化的缩减到一个特定数据集的情况。</li>
<li>results: 研究发现，在不同的随机选择的训练数据 subsets 中精致化 PLM rankers 会具有很大的差异，这表明可以通过活动选择训练数据 subsets 来实现更高的效率。然而，这篇论文发现，现有的活动学习（AL）策略在PLM rankers的精致化中不能够实现更高的效率，并且与随机选择相比，AL策略需要更多的评估成本。<details>
<summary>Abstract</summary>
Search methods based on Pretrained Language Models (PLM) have demonstrated great effectiveness gains compared to statistical and early neural ranking models. However, fine-tuning PLM-based rankers requires a great amount of annotated training data. Annotating data involves a large manual effort and thus is expensive, especially in domain specific tasks. In this paper we investigate fine-tuning PLM-based rankers under limited training data and budget. We investigate two scenarios: fine-tuning a ranker from scratch, and domain adaptation starting with a ranker already fine-tuned on general data, and continuing fine-tuning on a target dataset. We observe a great variability in effectiveness when fine-tuning on different randomly selected subsets of training data. This suggests that it is possible to achieve effectiveness gains by actively selecting a subset of the training data that has the most positive effect on the rankers. This way, it would be possible to fine-tune effective PLM rankers at a reduced annotation budget. To investigate this, we adapt existing Active Learning (AL) strategies to the task of fine-tuning PLM rankers and investigate their effectiveness, also considering annotation and computational costs. Our extensive analysis shows that AL strategies do not significantly outperform random selection of training subsets in terms of effectiveness. We further find that gains provided by AL strategies come at the expense of more assessments (thus higher annotation costs) and AL strategies underperform random selection when comparing effectiveness given a fixed annotation cost. Our results highlight that ``optimal'' subsets of training data that provide high effectiveness at low annotation cost do exist, but current mainstream AL strategies applied to PLM rankers are not capable of identifying them.
</details>
<details>
<summary>摘要</summary>
基于预训言语模型（PLM）的搜索方法在效果上有显著提升，但是精细调整PLM-based ranker需要大量标注数据。标注数据需要大量人工劳动，因此成本高，特别是在域pecific任务中。在这篇论文中，我们研究了在有限的培训数据和预算下进行PLM-based ranker的精细调整。我们研究了两个情况：从scratch开始精细调整rankers，以及在泛化数据上精细调整rankers，然后在目标数据上继续精细调整。我们发现在不同随机选择的培训数据上进行精细调整时，效果很大。这表示可以通过活动选择培训数据来提高PLM rankers的效果，而不需要大量的标注预算。为了 investigates这一点，我们采用了现有的活动学习（AL）策略，并对其效果进行了广泛的分析。我们发现，与随机选择的培训数据相比，AL策略不能显著提高效果。此外，AL策略相比随机选择，需要更多的评估（即更高的标注成本），并且在给定标注成本下，AL策略下表现较差。我们的结果表明，“优化”的培训数据集，可以提高PLM rankers的效果，但现有的主流AL策略无法确定这些集。
</details></li>
</ul>
<hr>
<h2 id="AstroLLaMA-Towards-Specialized-Foundation-Models-in-Astronomy"><a href="#AstroLLaMA-Towards-Specialized-Foundation-Models-in-Astronomy" class="headerlink" title="AstroLLaMA: Towards Specialized Foundation Models in Astronomy"></a>AstroLLaMA: Towards Specialized Foundation Models in Astronomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06126">http://arxiv.org/abs/2309.06126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciucă, Charlie O’Neill, Ze-Chang Sun, Maja Jabłońska, Sandor Kruk, Ernest Perkowski, Jack Miller, Jason Li, Josh Peek, Kartheik Iyer, Tomasz Różański, Pranav Khetarpal, Sharaf Zaman, David Brodrick, Sergio J. Rodríguez Méndez, Thang Bui, Alyssa Goodman, Alberto Accomazzi, Jill Naiman, Jesse Cranney, Kevin Schawinski, UniverseTBD</li>
<li>for:  bridging the gap between large language models and highly specialized domains like scholarly astronomy</li>
<li>methods:  fine-tuning a 7-billion-parameter model from LLaMA-2 using over 300,000 astronomy abstracts from arXiv, optimized for traditional causal language modeling</li>
<li>results:  achieving a 30% lower perplexity than Llama-2, generating more insightful and scientifically relevant text completions and embedding extraction than state-of-the-art foundation models despite having significantly fewer parameters<details>
<summary>Abstract</summary>
Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.
</details>
<details>
<summary>摘要</summary>
大型语言模型在许多人类语言任务中表现出色，但在高度特殊化的学术天文领域中往往表现不佳。为了bridging这个差距，我们介绍AstroLLaMA，一个基于LLaMA-2的70亿个 Parameters模型，通过arXiv上的30万篇天文摘要文献进行精细调整。这个模型适用于传统的 causal 语言模型，与LLaMA-2相比，每个字的误差率下降了30%，表明了域 adaptation。我们的模型在对天文领域的文本完成和嵌入EXTRACTING方面表现更加具有意义和科学相关性，即使有较少的参数。AstroLLaMA是一个强大的专业领域模型，具有广泛的 fine-tuning 潜力。公开发布AstroLLaMA，以促进天文研究，包括自动摘要和对话代理开发。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Latent-Perspectives-of-Media-Houses-Towards-Public-Figures"><a href="#Characterizing-Latent-Perspectives-of-Media-Houses-Towards-Public-Figures" class="headerlink" title="Characterizing Latent Perspectives of Media Houses Towards Public Figures"></a>Characterizing Latent Perspectives of Media Houses Towards Public Figures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06112">http://arxiv.org/abs/2309.06112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharath Srivatsa, Srinath Srinivasa</li>
<li>for: 本研究旨在提出一种零例学习方法，用于非EXTRACTIVE或生成式人EntityCharacterization。</li>
<li>methods: 该方法使用GPT-2预训练语言模型，首先在特定人Entity是Characterized的文献中进行了细化。然后，该模型进行了第二次细化，使用手动提供的示例Characterizations。</li>
<li>results: 结果表明，使用该方法可以生成准确的人EntityCharacterization，并且比预先训练的模型更加准确。<details>
<summary>Abstract</summary>
Media houses reporting on public figures, often come with their own biases stemming from their respective worldviews. A characterization of these underlying patterns helps us in better understanding and interpreting news stories. For this, we need diverse or subjective summarizations, which may not be amenable for classifying into predefined class labels. This work proposes a zero-shot approach for non-extractive or generative characterizations of person entities from a corpus using GPT-2. We use well-articulated articles from several well-known news media houses as a corpus to build a sound argument for this approach. First, we fine-tune a GPT-2 pre-trained language model with a corpus where specific person entities are characterized. Second, we further fine-tune this with demonstrations of person entity characterizations, created from a corpus of programmatically constructed characterizations. This twice fine-tuned model is primed with manual prompts consisting of entity names that were not previously encountered in the second fine-tuning, to generate a simple sentence about the entity. The results were encouraging, when compared against actual characterizations from the corpus.
</details>
<details>
<summary>摘要</summary>
媒体机构报道公众人物，经常带有自己的偏见，源于自己的世界观。了解这些底层模式，能够帮助我们更好地理解和解释新闻故事。为此，我们需要多样化或主观概要，这些概要可能无法被归类为预定的类别。这项工作提出了一种零批处理方法，通过GPT-2进行非抽取式或生成性人EntityCharacterizations。我们使用了多种知名新闻媒体的报道，构建了一个具有听众力的论证。首先，我们精度地调整GPT-2预训练语言模型，使其与特定人EntityCharacterizations相关的 corpus进行了精度调整。然后，我们进一步精度调整这个模型，使其能够生成基于 manually constructed characterizations的示例。这两次精度调整的模型，通过手动提供实体名称，并不在第二次精度调整中出现过的实体名称，来生成简单的句子。结果非常鼓舞人，与实际 corpus 中的Characterizations相比。
</details></li>
</ul>
<hr>
<h2 id="Towards-Visual-Taxonomy-Expansion"><a href="#Towards-Visual-Taxonomy-Expansion" class="headerlink" title="Towards Visual Taxonomy Expansion"></a>Towards Visual Taxonomy Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06105">http://arxiv.org/abs/2309.06105</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/darthzhu/vte">https://github.com/darthzhu/vte</a></li>
<li>paper_authors: Tinghui Zhu, Jingping Liu, Jiaqing Liang, Haiyun Jiang, Yanghua Xiao, Zongyu Wang, Rui Xie, Yunsen Xian</li>
<li>for: 本研究旨在解决taxonomy扩展任务中的”Prototypical Hypernym Problem”，提出了视觉税onomy扩展(VTE)方法，将视觉特征引入到税onomy扩展中。</li>
<li>methods: 本研究提出了文本层次学习任务和视觉 проtotype学习任务，将文本和视觉 semantics集成到一起。此外，我们还引入了超 proto约束，将文本和视觉 semantics结合到一起生成细腻的视觉 semantics。</li>
<li>results: 我们在两个 dataset 上进行了实验，得到了鲜明的成果。具体来说，在中文税onomy dataset上，我们的方法相比原始方法提高了准确率8.75%。此外，我们的方法还在中文税onomy dataset上比ChatGPT better。<details>
<summary>Abstract</summary>
Taxonomy expansion task is essential in organizing the ever-increasing volume of new concepts into existing taxonomies. Most existing methods focus exclusively on using textual semantics, leading to an inability to generalize to unseen terms and the "Prototypical Hypernym Problem." In this paper, we propose Visual Taxonomy Expansion (VTE), introducing visual features into the taxonomy expansion task. We propose a textual hypernymy learning task and a visual prototype learning task to cluster textual and visual semantics. In addition to the tasks on respective modalities, we introduce a hyper-proto constraint that integrates textual and visual semantics to produce fine-grained visual semantics. Our method is evaluated on two datasets, where we obtain compelling results. Specifically, on the Chinese taxonomy dataset, our method significantly improves accuracy by 8.75 %. Additionally, our approach performs better than ChatGPT on the Chinese taxonomy dataset.
</details>
<details>
<summary>摘要</summary>
《税onomy扩展任务是组织新的概念 volume 的关键，因为现有的方法仅专注于使用文本 semantics，导致无法扩展至未见到的概念和"Prototypical Hypernym Problem"。在本文中，我们提出了可视的税onomy扩展（VTE），将可视特征加入税onomy扩展任务中。我们提出了文本层次学习任务和可视标本学习任务，以排序文本和可视 semantics。此外，我们引入了文本和可视 semantics的超类征约制，以生成细部可视 semantics。我们的方法在两个数据集上进行评估，结果表明我们的方法在中文税onomy数据集上提高了精度 by 8.75%，并且比ChatGPT在中文税onomy数据集上表现更好。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Catastrophic-Forgetting-in-Cross-Lingual-Transfer-Paradigms-Exploring-Tuning-Strategies"><a href="#Measuring-Catastrophic-Forgetting-in-Cross-Lingual-Transfer-Paradigms-Exploring-Tuning-Strategies" class="headerlink" title="Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies"></a>Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06089">http://arxiv.org/abs/2309.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boshko Koloski, Blaž Škrlj, Marko Robnik-Šikonja, Senja Pollak</li>
<li>for: 这个研究旨在比较两种精度器拟合方法在跨语言设置下进行大语言模型的微调。</li>
<li>methods: 研究使用了两种微调方法：参数有效的拟合方法和全参数微调。在跨语言传递方面，研究使用了中间训练（IT）和跨语言验证（CLV）两种方法。</li>
<li>results: 研究结果显示，在两个不同的分类问题（词语攻击和产品评论）上，IT跨语言策略在目标语言上表现更好。此外，研究发现，在多种跨语言传递中，CLV策略在基础语言（英语）中的知识抑制比IT策略更强。<details>
<summary>Abstract</summary>
The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\textit{IT}) that uses each language sequentially and cross-lingual validation (\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several languages, show that the \textit{IT} cross-lingual strategy outperforms \textit{CLV} for the target language. Our findings indicate that, in the majority of cases, the \textit{CLV} strategy demonstrates superior retention of knowledge in the base language (English) compared to the \textit{IT} strategy, when evaluating catastrophic forgetting in multiple cross-lingual transfers.
</details>
<details>
<summary>摘要</summary>
cross-lingual transfer是一种有前途的技术，可以解决少语言资源的任务。在这个实验研究中，我们比较了两种精细调整方法，与零架构学习和全架构学习方法结合使用大语言模型在跨语言设置下进行比较。作为精细调整策略，我们比较了参数有效的适配器方法和所有参数的 fine-tuning。作为跨语言传递策略，我们比较了中间训练（IT），使用每种语言的顺序训练，以及跨语言验证（CLV），在练习阶段对 targets 语言进行验证。我们评估了跨语言传递的成功和源语言中的恶性遗弃现象，即在学习新语言时，之前学习的知识多少会丢失。我们在两个不同的分类问题，即词汇攻击和产品评论，每个问题都包含多种语言的数据集，得到的结果表明，对于目标语言，IT 跨语言策略表现出色。我们的发现表明，在大多数情况下，CLV 跨语言策略在多个跨语言传递中表现出更好的知识保留性，对于基础语言（英语）进行评估。
</details></li>
</ul>
<hr>
<h2 id="BHASA-A-Holistic-Southeast-Asian-Linguistic-and-Cultural-Evaluation-Suite-for-Large-Language-Models"><a href="#BHASA-A-Holistic-Southeast-Asian-Linguistic-and-Cultural-Evaluation-Suite-for-Large-Language-Models" class="headerlink" title="BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models"></a>BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06085">http://arxiv.org/abs/2309.06085</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aisingapore/bhasa">https://github.com/aisingapore/bhasa</a></li>
<li>paper_authors: Wei Qi Leong, Jian Gang Ngui, Yosephine Susanto, Hamsawardhini Rengarajan, Kengatharaiyer Sarveswaran, William Chandra Tjhi</li>
<li>for: 这个论文的目的是为了提出一个包容全面的语言模型评估遗产，以评估大语言模型在南东亚语言中的表现。</li>
<li>methods: 这个论文使用了一个包括八个任务的NLU、NLG和NLR任务的语言模型评估练习，以及一个覆盖各种语言现象的语言诊断工具集（LINDSEA）和一个文化诊断数据集。</li>
<li>results: 这个论文的初步实验表明，使用GPT-4作为参考点，这些南东亚语言的大语言模型在语言技能、文化表达和敏感性方面都存在缺陷。<details>
<summary>Abstract</summary>
The rapid development of Large Language Models (LLMs) and the emergence of novel abilities with scale have necessitated the construction of holistic, diverse and challenging benchmarks such as HELM and BIG-bench. However, at the moment, most of these benchmarks focus only on performance in English and evaluations that include Southeast Asian (SEA) languages are few in number. We therefore propose BHASA, a holistic linguistic and cultural evaluation suite for LLMs in SEA languages. It comprises three components: (1) a NLP benchmark covering eight tasks across Natural Language Understanding (NLU), Generation (NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit that spans the gamut of linguistic phenomena including syntax, semantics and pragmatics, and (3) a cultural diagnostics dataset that probes for both cultural representation and sensitivity. For this preliminary effort, we implement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil, and we only include Indonesian and Tamil for LINDSEA and the cultural diagnostics dataset. As GPT-4 is purportedly one of the best-performing multilingual LLMs at the moment, we use it as a yardstick to gauge the capabilities of LLMs in the context of SEA languages. Our initial experiments on GPT-4 with BHASA find it lacking in various aspects of linguistic capabilities, cultural representation and sensitivity in the targeted SEA languages. BHASA is a work in progress and will continue to be improved and expanded in the future. The repository for this paper can be found at: https://github.com/aisingapore/BHASA
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CNThe rapid development of Large Language Models (LLMs) and the emergence of novel abilities with scale have necessitated the construction of holistic, diverse and challenging benchmarks such as HELM and BIG-bench. However, at the moment, most of these benchmarks focus only on performance in English and evaluations that include Southeast Asian (SEA) languages are few in number. We therefore propose BHASA, a holistic linguistic and cultural evaluation suite for LLMs in SEA languages. It comprises three components: (1) a NLP benchmark covering eight tasks across Natural Language Understanding (NLU), Generation (NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit that spans the gamut of linguistic phenomena including syntax, semantics and pragmatics, and (3) a cultural diagnostics dataset that probes for both cultural representation and sensitivity. For this preliminary effort, we implement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil, and we only include Indonesian and Tamil for LINDSEA and the cultural diagnostics dataset. As GPT-4 is purportedly one of the best-performing multilingual LLMs at the moment, we use it as a yardstick to gauge the capabilities of LLMs in the context of SEA languages. Our initial experiments on GPT-4 with BHASA find it lacking in various aspects of linguistic capabilities, cultural representation and sensitivity in the targeted SEA languages. BHASA is a work in progress and will continue to be improved and expanded in the future. The repository for this paper can be found at: https://github.com/aisingapore/BHASANote: The translation is in Simplified Chinese, which is the standard writing system used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="RAP-Gen-Retrieval-Augmented-Patch-Generation-with-CodeT5-for-Automatic-Program-Repair"><a href="#RAP-Gen-Retrieval-Augmented-Patch-Generation-with-CodeT5-for-Automatic-Program-Repair" class="headerlink" title="RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair"></a>RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06057">http://arxiv.org/abs/2309.06057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weishi Wang, Yue Wang, Shafiq Joty, Steven C. H. Hoi<br>for: 本研究的目的是提高自动程序修复（APR）的效能，以减少开发人员的手动调试努力并提高软件可靠性。methods: 本研究使用了深度学习（DL）基于的方法，通过在数据驱动方式下自动化程序修复过程。另外，我们还使用了一种混合的修补检索器，以便在不同的语言环境下进行lexical和semantic匹配。results: 我们的实验结果表明，RAP-Gen可以在三个benchmark上显著超越之前的状态态的方法，例如在818个Defects4J bug中修复15个更多的bug。<details>
<summary>Abstract</summary>
Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks in Java, where the bug localization information may or may not be provided. Experimental results show that RAP-Gen significantly outperforms previous state-of-the-art approaches on all benchmarks, e.g., repairing 15 more bugs on 818 Defects4J bugs.
</details>
<details>
<summary>摘要</summary>
自动化程序修复（APR）是软件可靠性的关键因素，可以减少开发人员的手动调试努力并提高软件的可靠性。传统的搜索基本技术通常采用规则或减少假设来挖掘修复模式，而 recent years 有所见到 Deep Learning（DL） 基于的方法来自动化程序修复过程。但是，它们的性能通常受到一组固定参数来模型高度复杂的修复空间的限制。为了减轻这种固定参数的负担，在这种工作中，我们提出了一种 novel Retrieval-Augmented Patch Generation 框架（RAP-Gen），通过显式地利用 Codebase 中的修复模式来提高修复效果。 Specifically，我们构建了一种 hybrid 修复搜索器，可以根据源代码的语言无关方式进行 both lexical 和 semantic 匹配，而不需要任何代码特定的特征。此外，我们采用 Code-aware 语言模型 CodeT5 作为基础模型，以便在一个简单的方式下进行修复搜索和生成任务。我们采用分阶段的方法，首先由修复搜索器 retrieved 一个相关的外部修复对，然后将其与 CodeT5 修复生成器进行结合，以生成一个排名列表中的修复补丁候选者。需要注意的是，RAP-Gen 是一种通用的 APR 框架，可以适应不同的修复任务和语言。我们对 TFix  benchmark 、Code Refinement 和 Defects4J  benchmark 进行了严格的测试，结果表明，RAP-Gen 在所有benchmark上显著超过了前一个状态的方法，例如，对 818 Defects4J  bug 进行修复。
</details></li>
</ul>
<hr>
<h2 id="How-does-representation-impact-in-context-learning-A-exploration-on-a-synthetic-task"><a href="#How-does-representation-impact-in-context-learning-A-exploration-on-a-synthetic-task" class="headerlink" title="How does representation impact in-context learning: A exploration on a synthetic task"></a>How does representation impact in-context learning: A exploration on a synthetic task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06054">http://arxiv.org/abs/2309.06054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwen Fu, Tao Yang, Yuwang Wang, Yan Lu, Nanning Zheng</li>
<li>for:  investigate the mechanism of in-context learning in Transformer</li>
<li>methods: construct a novel synthetic task, use two probes to evaluate in-weights and in-context components</li>
<li>results: demonstrate the entanglement between in-context learning and representation learning, and the importance of in-weights component for in-context learning<details>
<summary>Abstract</summary>
In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation learning. Furthermore, we find that a good in-weights component can actually benefit the learning of the in-context component, indicating that in-weights learning should be the foundation of in-context learning. To further understand the the in-context learning mechanism and importance of the in-weights component, we proof by construction that a simple Transformer, which uses pattern matching and copy-past mechanism to perform in-context learning, can match the in-context learning performance with more complex, best tuned Transformer under the perfect in-weights component assumption. In short, those discoveries from representation learning perspective shed light on new approaches to improve the in-context capacity.
</details>
<details>
<summary>摘要</summary>
受Context学习，即通过在Context中的样本学习，是Transformer的一项惊人能力。然而，这种学习机制仍未完全理解。在这项研究中，我们尝试从 representation learning 的一个未经探索的角度来研究。在这种情况下，表示更加复杂，因为表示可以受到模型参数和Context中的样本影响。我们将这两个概念性方面的表示称为内重Component和Context Component，分别。为了研究这两个组件如何影响Context learning能力，我们构建了一个新的 sintetic任务，使得可以设置两个探针，即内重探针和Context探针，来评估这两个组件。我们发现，Context component 的质量与 Context learning 性能之间存在很高的相关性，这表明Context learning 和 representation learning 之间存在紧密的关系。此外，我们发现一个好的内重Component 可以实际提高Context component 的学习效果，这表明内重学习应该是Context learning 的基础。为了更深入地理解Context learning 机制和内重Component 的重要性，我们证明了一个简单的Transformer模型，通过模式匹配和复制机制来实现Context learning，可以与best tuned Transformer 模型匹配Context learning性能，假设内重Component 完美。总之，这些发现从 representation learning 的角度提供了新的方法来提高Context capacity。
</details></li>
</ul>
<hr>
<h2 id="Narrowing-the-Gap-between-Supervised-and-Unsupervised-Sentence-Representation-Learning-with-Large-Language-Model"><a href="#Narrowing-the-Gap-between-Supervised-and-Unsupervised-Sentence-Representation-Learning-with-Large-Language-Model" class="headerlink" title="Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model"></a>Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06453">http://arxiv.org/abs/2309.06453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingxin Li, Richong Zhang, Zhijie Nie, Yongyi Mao</li>
<li>for: 本研究的目的是解释supervised和unsupervised Contrastive learning of Sentence Embeddings (CSE)在训练过程中的性能差距，以及如何减少这个差距。</li>
<li>methods: 本研究使用了empirical experiments和metric called Fitting Difficulty Increment (FDI)来解释和解决性能差距问题。</li>
<li>results: 研究发现，性能差距的主要原因是训练数据集和评估数据集的适应度差异，并提出了一种基于LLM生成数据的方法来减少性能差距。<details>
<summary>Abstract</summary>
Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer "What happens during the training process that leads to the performance gap?" and "How can the performance gap be narrowed?". In this paper, we conduct empirical experiments to answer these "What" and "How" questions. We first answer the "What" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We observe a significant difference in fitting difficulty. Thus, we introduce a metric, called Fitting Difficulty Increment (FDI), to measure the fitting difficulty gap between the evaluation dataset and the held-out training dataset, and use the metric to answer the "What" question. Then, based on the insights gained from the "What" question, we tackle the "How" question by increasing the fitting difficulty of the training dataset. We achieve this by leveraging the In-Context Learning (ICL) capability of the Large Language Model (LLM) to generate data that simulates complex patterns. By utilizing the hierarchical patterns in the LLM-generated data, we effectively narrow the gap between supervised and unsupervised CSE.
</details>
<details>
<summary>摘要</summary>
我们首先回答"What"问题，对监督和无监督CSE在训练过程中的行为进行了仔细比较。从比较中，我们发现监督CSE在训练过程中的适应 difficulty 和无监督CSE相比较大。因此，我们引入一个指标，叫做适应难度增量 (FDI)，用于度量监督和无监督CSE在评估集和封锁训练集之间的适应难度差距。然后，根据FDI指标，我们回答"What"问题。接着，基于获得的"What"问题的回答，我们解决"How"问题。我们通过利用大语言模型 (LLM) 的启发学习 (ICL) 能力，生成数据，模拟复杂的模式。通过利用 LLB 生成的数据中的层次模式，我们有效地缩小了监督和无监督CSE之间的性能差距。
</details></li>
</ul>
<hr>
<h2 id="Content-Reduction-Surprisal-and-Information-Density-Estimation-for-Long-Documents"><a href="#Content-Reduction-Surprisal-and-Information-Density-Estimation-for-Long-Documents" class="headerlink" title="Content Reduction, Surprisal and Information Density Estimation for Long Documents"></a>Content Reduction, Surprisal and Information Density Estimation for Long Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06009">http://arxiv.org/abs/2309.06009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoxiong Ji, Wei Sun, Pekka Marttinen</li>
<li>for: 这个论文研究了语言信息含义的计算语言方法，特别是长文档中信息分布和减少内容对信息密度的影响。</li>
<li>methods: 这篇论文提出了四种信息密度估计标准，其中前三个采用信息理论中的度量，并提出了一种基于注意力的单词选择方法来处理医疗记录。</li>
<li>results: 研究发现了不同领域的长文档信息密度系统性差异，并且对自动医疗代码生成从长医疗记录中表现效果良好。<details>
<summary>Abstract</summary>
Many computational linguistic methods have been proposed to study the information content of languages. We consider two interesting research questions: 1) how is information distributed over long documents, and 2) how does content reduction, such as token selection and text summarization, affect the information density in long documents. We present four criteria for information density estimation for long documents, including surprisal, entropy, uniform information density, and lexical density. Among those criteria, the first three adopt the measures from information theory. We propose an attention-based word selection method for clinical notes and study machine summarization for multiple-domain documents. Our findings reveal the systematic difference in information density of long text in various domains. Empirical results on automated medical coding from long clinical notes show the effectiveness of the attention-based word selection method.
</details>
<details>
<summary>摘要</summary>
多种计算语言学方法已经被提出来研究语言信息内容。我们考虑了两个有趣的研究问题：1）在长文档中如何分布信息，2）如何采用内容减少方法（如选择 Token 和文本摘要）影响长文档中的信息密度。我们提出了四个信息密度估计标准，包括悬念度、熵、一致信息密度和词汇密度。其中前三个采用信息理论的度量。我们提议使用注意力基于词选择方法来处理医疗记录，并对多个领域文档进行机器摘要。我们的发现显示了不同领域的长文档信息密度存在系统性的差异。对自动医疗代码生成从长医疗记录的实验结果表明了注意力基于词选择方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Kid-Whisper-Towards-Bridging-the-Performance-Gap-in-Automatic-Speech-Recognition-for-Children-VS-Adults"><a href="#Kid-Whisper-Towards-Bridging-the-Performance-Gap-in-Automatic-Speech-Recognition-for-Children-VS-Adults" class="headerlink" title="Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults"></a>Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07927">http://arxiv.org/abs/2309.07927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Adel Attia, Jing Liu, Wei Ai, Dorottya Demszky, Carol Espy-Wilson</li>
<li>for: 这个研究旨在提高儿童语音识别系统的性能，以便更好地支持儿童的语言发展和学习。</li>
<li>methods: 该研究使用了Whisper自动语音识别系统，并对My Science Tutor儿童语音数据集进行了更有效的数据处理，以提高Whisper的性能。</li>
<li>results: 研究显示，通过更有效的数据处理，可以将Word Error Rate（WER）在MyST测试集下降至9.11%（Whisper-Small）和8.61%（Whisper-Medium），并且这种改进可以普适应用于未经看过的数据集。此外，研究还揭示了儿童语音识别系统的一些重要挑战。<details>
<summary>Abstract</summary>
Recent advancements in Automatic Speech Recognition (ASR) systems, exemplified by Whisper, have demonstrated the potential of these systems to approach human-level performance given sufficient data. However, this progress doesn't readily extend to ASR for children due to the limited availability of suitable child-specific databases and the distinct characteristics of children's speech. A recent study investigated leveraging the My Science Tutor (MyST) children's speech corpus to enhance Whisper's performance in recognizing children's speech. They were able to demonstrate some improvement on a limited testset. This paper builds on these findings by enhancing the utility of the MyST dataset through more efficient data preprocessing. We reduce the Word Error Rate (WER) on the MyST testset 13.93% to 9.11% with Whisper-Small and from 13.23% to 8.61% with Whisper-Medium and show that this improvement can be generalized to unseen datasets. We also highlight important challenges towards improving children's ASR performance. The results showcase the viable and efficient integration of Whisper for effective children's speech recognition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Robustness-of-Neural-Inverse-Text-Normalization-via-Data-Augmentation-Semi-Supervised-Learning-and-Post-Aligning-Method"><a href="#Improving-Robustness-of-Neural-Inverse-Text-Normalization-via-Data-Augmentation-Semi-Supervised-Learning-and-Post-Aligning-Method" class="headerlink" title="Improving Robustness of Neural Inverse Text Normalization via Data-Augmentation, Semi-Supervised Learning, and Post-Aligning Method"></a>Improving Robustness of Neural Inverse Text Normalization via Data-Augmentation, Semi-Supervised Learning, and Post-Aligning Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08626">http://arxiv.org/abs/2309.08626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juntae Kim, Minkyu Lim, Seokjin Hong</li>
<li>for: 这 paper 的目的是提高自动语音识别 (ASR) 系统中的反文本normalization (ITN) 性能, 特别是在 spoken-form 转换到 written-form 的上下文中。</li>
<li>methods: 这 paper 提出了一种直接训练方法, 使用 ASR 生成的 spoken 或 written 文本，并通过 ASR 语言上下文模拟和 semi-supervised learning 方法增强。 此外，paper 还引入了一种后置对aligning 方法来管理不可预测的错误，以提高 ITN 的可靠性。</li>
<li>results:  experiments 表明，paper 提出的方法在多种 ASR 场景中显著提高了 ITN 性能。<details>
<summary>Abstract</summary>
Inverse text normalization (ITN) is crucial for converting spoken-form into written-form, especially in the context of automatic speech recognition (ASR). While most downstream tasks of ASR rely on written-form, ASR systems often output spoken-form, highlighting the necessity for robust ITN in product-level ASR-based applications. Although neural ITN methods have shown promise, they still encounter performance challenges, particularly when dealing with ASR-generated spoken text. These challenges arise from the out-of-domain problem between training data and ASR-generated text. To address this, we propose a direct training approach that utilizes ASR-generated written or spoken text, with pairs augmented through ASR linguistic context emulation and a semi-supervised learning method enhanced by a large language model, respectively. Additionally, we introduce a post-aligning method to manage unpredictable errors, thereby enhancing the reliability of ITN. Our experiments show that our proposed methods remarkably improved ITN performance in various ASR scenarios.
</details>
<details>
<summary>摘要</summary>
倒计时normalization (ITN) 是对话式文本转换到书面文本的关键技术，尤其在自动语音识别 (ASR) 的上下文中。大多数 ASR 下游任务需要书面文本，但 ASR 系统通常输出说话式文本，因此需要Robust ITN 在产品级 ASR 应用中。虽然神经 ITN 方法有 shown 搅拌，但它们在处理 ASR 生成的说话文本时仍然遇到性能挑战。这些挑战来自于 ASR 生成的文本与训练数据之间的域外问题。为 Addressing 这个问题，我们提议一种直接训练方法，利用 ASR 生成的书面或说话文本，并通过 ASR 语言上下文模拟和大型语言模型增强的半监督学习方法，分别对待不同的 ASR enario。此外，我们还引入了一种后对aligning 方法，以管理不可预测的错误，从而提高 ITN 的可靠性。我们的实验表明，我们的提议方法在多种 ASR 情况下有remarkably 改善 ITN 性能。
</details></li>
</ul>
<hr>
<h2 id="Performance-of-ChatGPT-3-5-and-GPT-4-on-the-United-States-Medical-Licensing-Examination-With-and-Without-Distractions"><a href="#Performance-of-ChatGPT-3-5-and-GPT-4-on-the-United-States-Medical-Licensing-Examination-With-and-Without-Distractions" class="headerlink" title="Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions"></a>Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08625">http://arxiv.org/abs/2309.08625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Myriam Safrai, Amos Azaria</li>
<li>for: 这项研究旨在调查 chatGPT 是否能准确地提供医疗建议，并且是否会受到小话信息的影响。</li>
<li>methods: 研究使用 USMLE 步骤 3 问题作为有用的医疗数据，并使用 Mechanical Turk 平台收集小话句子。将问题和小话句子排序并提供给 chatGPT 3.5 和 4 进行回答。一位资深医生分析 chatGPT 的答案，并与正确答案进行比较。</li>
<li>results: 结果表明，在多选问题上，chatGPT 3.5 的回答精度降低了，从 72.1% 降低到 68.9%，而在开题问题上，降低到 61.5%，比对应正确答案的比例为 44.3%。相比之下，chatGPT 4 在两类问题上的回答精度均高于 3.5 版本，不受小话信息的影响。<details>
<summary>Abstract</summary>
As Large Language Models (LLMs) are predictive models building their response based on the words in the prompts, there is a risk that small talk and irrelevant information may alter the response and the suggestion given. Therefore, this study aims to investigate the impact of medical data mixed with small talk on the accuracy of medical advice provided by ChatGPT. USMLE step 3 questions were used as a model for relevant medical data. We use both multiple choice and open ended questions. We gathered small talk sentences from human participants using the Mechanical Turk platform. Both sets of USLME questions were arranged in a pattern where each sentence from the original questions was followed by a small talk sentence. ChatGPT 3.5 and 4 were asked to answer both sets of questions with and without the small talk sentences. A board-certified physician analyzed the answers by ChatGPT and compared them to the formal correct answer. The analysis results demonstrate that the ability of ChatGPT-3.5 to answer correctly was impaired when small talk was added to medical data for multiple-choice questions (72.1\% vs. 68.9\%) and open questions (61.5\% vs. 44.3\%; p=0.01), respectively. In contrast, small talk phrases did not impair ChatGPT-4 ability in both types of questions (83.6\% and 66.2\%, respectively). According to these results, ChatGPT-4 seems more accurate than the earlier 3.5 version, and it appears that small talk does not impair its capability to provide medical recommendations. Our results are an important first step in understanding the potential and limitations of utilizing ChatGPT and other LLMs for physician-patient interactions, which include casual conversations.
</details>
<details>
<summary>摘要</summary>
LLMS（大语言模型）是基于提示语言的预测模型，因此可能存在小说和无关信息影响其回答和建议的风险。这项研究旨在研究将医疗数据混合到小说中对ChatGPT提供的医疗建议精度的影响。我们使用USMLE步骤3题目作为相关医疗数据模型。我们使用多选和开放题目两种类型。我们从人工智能 Turk 平台获得了小说句子。我们将USMLE题目分配成一种模式，其中每个原始句子后接一个小说句子。ChatGPT 3.5和4被要求回答这两个集合的问题，包括和小说句子。一位资深的医生分析了ChatGPT的答案并与正确答案进行比较。分析结果显示，当小说句子添加到医疗数据时，ChatGPT-3.5 的回答正确率下降（72.1% vs. 68.9%）和开放题目中的回答正确率下降（61.5% vs. 44.3%）。相比之下，小说句子不会对ChatGPT-4 的回答造成影响（83.6%和66.2%）。根据这些结果，ChatGPT-4 显示更加准确，而小说不会对其医疗建议能力产生影响。这些结果是我们理解LLMS在实际医疗互动中的潜在和局限性的重要一步。
</details></li>
</ul>
<hr>
<h2 id="Circuit-Breaking-Removing-Model-Behaviors-with-Targeted-Ablation"><a href="#Circuit-Breaking-Removing-Model-Behaviors-with-Targeted-Ablation" class="headerlink" title="Circuit Breaking: Removing Model Behaviors with Targeted Ablation"></a>Circuit Breaking: Removing Model Behaviors with Targeted Ablation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05973">http://arxiv.org/abs/2309.05973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xanderdavies/circuit-breaking">https://github.com/xanderdavies/circuit-breaking</a></li>
<li>paper_authors: Maximilian Li, Xander Davies, Max Nadeau</li>
<li>For: 降低 GPT-2 语言生成中的偏见行为* Methods: 范例小数据集中找到关键 causal 通路，并删除这些通路以关键排除偏见行为* Results: 删除 12 条 causal 通路可以严重降低偏见语言生成，并对其他输入的性能几乎没有影响<details>
<summary>Abstract</summary>
Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
</details>
<details>
<summary>摘要</summary>
机器学习模型经常展现出提高预训练目标性能的行为，但是却害下游任务性能。我们提出了一种新的方法，通过缺省少量的 causal 路径间的缺省来消除不良行为。通过一小量的输入数据，我们学习缺省少量的重要 causal 路径。在减少 GPT-2 毒性语言生成中，我们发现缺省12个 causal 边对毒性语言生成产生了很好的效果，而不会对其他输入产生很大的影响。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Ebb-and-Flow-An-In-depth-Analysis-of-Question-Answering-Trends-across-Diverse-Platforms"><a href="#Evaluating-the-Ebb-and-Flow-An-In-depth-Analysis-of-Question-Answering-Trends-across-Diverse-Platforms" class="headerlink" title="Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms"></a>Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05961">http://arxiv.org/abs/2309.05961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rima Hazra, Agnik Saha, Somnath Banerjee, Animesh Mukherjee</li>
<li>for: This paper aims to investigate the factors that contribute to the speed of responses on Community Question Answering (CQA) platforms.</li>
<li>methods: The authors analyze six highly popular CQA platforms and identify correlations between the time taken to yield the first response to a question and various variables, including metadata and patterns of user interaction. They also employ conventional machine learning models to predict which queries will receive prompt responses.</li>
<li>results: The study finds a correlation between the time taken to yield the first response and several variables, including the formulation of the questions and the level of interaction among users. The authors also demonstrate the feasibility of using machine learning models to predict prompt responses.<details>
<summary>Abstract</summary>
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
</details>
<details>
<summary>摘要</summary>
社区问答平台（CQA）的流行程度逐渐增长，因为它们为用户提供了快速的答案。这种快速答案的速度受到多种问题特定和用户相关的因素的影响。这篇论文在六个非常受欢迎的CQA平台上 investigate这些贡献因素，并通过使用传统的机器学习模型分析这些元数据和用户交互的模式，尝试预测哪些问题会收到快速的初始答案。Here's a word-for-word translation:社区问答平台（CQA）的流行程度逐渐增长，因为它们为用户提供了快速的答案。这种快速答案的速度受到多种问题特定和用户相关的因素的影响。这篇论文在六个非常受欢迎的CQA平台上 investigate这些贡献因素，并通过使用传统的机器学习模型分析这些元数据和用户交互的模式，尝试预测哪些问题会收到快速的初始答案。
</details></li>
</ul>
<hr>
<h2 id="The-Moral-Machine-Experiment-on-Large-Language-Models"><a href="#The-Moral-Machine-Experiment-on-Large-Language-Models" class="headerlink" title="The Moral Machine Experiment on Large Language Models"></a>The Moral Machine Experiment on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05958">http://arxiv.org/abs/2309.05958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kztakemoto/mmllm">https://github.com/kztakemoto/mmllm</a></li>
<li>paper_authors: Kazuhiro Takemoto</li>
<li>for: 本研究旨在 investigate 大型自然语言模型（LLM）在道路自动驾驶中的伦理决策倾向，以及这些模型如何与人类的伦理偏好相符。</li>
<li>methods: 该研究使用了 Moral Machine 框架，对包括 GPT-3.5、GPT-4、PaLM 2 和 Llama 2 等知名 LLM 进行了比较，并与人类的偏好进行了比较。</li>
<li>results: 研究发现，尽管 LLM 和人类的偏好在一些方面相似，但 PaLM 2 和 Llama 2 等模型尤其存在明显的偏差。此外，尽管qualitative上的偏好相似，但 LLM 可能会偏向更加坚定的决策，与人类的偏好相比。这些发现可能有助于我们更好地理解 LLM 的伦理框架，并对道路自动驾驶的发展产生影响。<details>
<summary>Abstract</summary>
As large language models (LLMs) become more deeply integrated into various sectors, understanding how they make moral judgments has become crucial, particularly in the realm of autonomous driving. This study utilized the Moral Machine framework to investigate the ethical decision-making tendencies of prominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their responses to human preferences. While LLMs' and humans' preferences such as prioritizing humans over pets and favoring saving more lives are broadly aligned, PaLM 2 and Llama 2, especially, evidence distinct deviations. Additionally, despite the qualitative similarities between the LLM and human preferences, there are significant quantitative disparities, suggesting that LLMs might lean toward more uncompromising decisions, compared to the milder inclinations of humans. These insights elucidate the ethical frameworks of LLMs and their potential implications for autonomous driving.
</details>
<details>
<summary>摘要</summary>
large language models (LLMs) 在不同领域深入整合后，理解它们如何作出道德判断成为了重要的焦点，尤其在自动驾驶领域。这个研究使用道德机器框架进行 investigated the ethical decision-making tendencies of prominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, and compared their responses to human preferences。 although LLMs' and humans' preferences such as prioritizing humans over pets and favoring saving more lives are broadly aligned，PaLM 2 and Llama 2, especially, evidence distinct deviations。 In addition, despite the qualitative similarities between the LLM and human preferences, there are significant quantitative disparities, suggesting that LLMs might lean toward more uncompromising decisions, compared to the milder inclinations of humans。 These insights elucidate the ethical frameworks of LLMs and their potential implications for autonomous driving。
</details></li>
</ul>
<hr>
<h2 id="Balanced-and-Explainable-Social-Media-Analysis-for-Public-Health-with-Large-Language-Models"><a href="#Balanced-and-Explainable-Social-Media-Analysis-for-Public-Health-with-Large-Language-Models" class="headerlink" title="Balanced and Explainable Social Media Analysis for Public Health with Large Language Models"></a>Balanced and Explainable Social Media Analysis for Public Health with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05951">http://arxiv.org/abs/2309.05951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanjiangjerry/alex">https://github.com/yanjiangjerry/alex</a></li>
<li>paper_authors: Yan Jiang, Ruihong Qiu, Yi Zhang, Peng-Fei Zhang</li>
<li>for: 本研究旨在提出一种 Novel ALEX 框架，用于社交媒体数据分析，以提高公共健康活动中的监测和决策。</li>
<li>methods: 本研究使用了数据增强策略来解决社交媒体数据的数据不均衡问题，并通过提示 LLM 来提高模型的表现。</li>
<li>results: 根据实验结果，提出的 ALEX 方法在 Social Media Mining for Health 2023 (SMM4H) 竞赛中的三个任务中得到了杰出的表现，在两个任务中得到了第一名。<details>
<summary>Abstract</summary>
As social media becomes increasingly popular, more and more public health activities emerge, which is worth noting for pandemic monitoring and government decision-making. Current techniques for public health analysis involve popular models such as BERT and large language models (LLMs). Although recent progress in LLMs has shown a strong ability to comprehend knowledge by being fine-tuned on specific domain datasets, the costs of training an in-domain LLM for every specific public health task are especially expensive. Furthermore, such kinds of in-domain datasets from social media are generally highly imbalanced, which will hinder the efficiency of LLMs tuning. To tackle these challenges, the data imbalance issue can be overcome by sophisticated data augmentation methods for social media datasets. In addition, the ability of the LLMs can be effectively utilised by prompting the model properly. In light of the above discussion, in this paper, a novel ALEX framework is proposed for social media analysis on public health. Specifically, an augmentation pipeline is developed to resolve the data imbalance issue. Furthermore, an LLMs explanation mechanism is proposed by prompting an LLM with the predicted results from BERT models. Extensive experiments conducted on three tasks at the Social Media Mining for Health 2023 (SMM4H) competition with the first ranking in two tasks demonstrate the superior performance of the proposed ALEX method. Our code has been released in https://github.com/YanJiangJerry/ALEX.
</details>
<details>
<summary>摘要</summary>
为了适应社交媒体日益普及，更多的公共健康活动在发展，这对抗疫病监测和政府决策都是值得注意的。当前的公共健康分析技术主要基于受欢迎的模型BERT和大型自然语言模型（LLM）。虽然最近的LLM进步显示在特定领域数据集上精细调整后具有强大的知识把握能力，但是训练专门领域LLM的成本尤其高昂。此外，这些社交媒体数据集通常具有很高的不均衡性，这会降低LLM的调整效率。为了解决这些挑战，本文提出了一种novel ALEX框架，用于社交媒体分析。特别是，我们开发了一个数据增强管线，以解决数据不均衡问题。此外，我们还提出了一种LLM的解释机制，通过向LLM提供BERT模型预测结果进行引导。经过广泛的实验，我们在2023年社交媒体矿山健康大赛（SMM4H）中的三个任务中获得了第一名。我们的代码已经在https://github.com/YanJiangJerry/ALEX上发布。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-as-Black-Box-Optimizers-for-Vision-Language-Models"><a href="#Language-Models-as-Black-Box-Optimizers-for-Vision-Language-Models" class="headerlink" title="Language Models as Black-Box Optimizers for Vision-Language Models"></a>Language Models as Black-Box Optimizers for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05950">http://arxiv.org/abs/2309.05950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shihongl1998/llm-as-a-blackbox-optimizer">https://github.com/shihongl1998/llm-as-a-blackbox-optimizer</a></li>
<li>paper_authors: Samuel Yu, Shihong Liu, Zhiqiu Lin, Deepak Pathak, Deva Ramanan<br>for: 这个研究旨在开发一种基于自然语言提示的视觉语言模型（VLM）微调方法，以避免需要存取模型参数、特征嵌入或输出寄存器。methods: 我们提出了一种使用对话式大语言模型（LLM）作为黑盒优化器，通过自动“山丘攀登”程序，让 LLM 根据文本反馈来调整提示，以实现最佳提示的搜寻。results: 在一个挑战性的1架学习设置下，我们的简单方法比 white-box 连续提示方法 CoOp 高出1.5%的平均准确率 across 11个数据集，包括 ImageNet。我们的方法还超过 OpenAI 手动制作的提示和其他黑盒方法 like iterative APE，并且发现文本提示生成的过程不仅更加可读性，而且可以在不同的 CLIP 架构上传递。<details>
<summary>Abstract</summary>
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prompt by evaluating the accuracy of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop. In a challenging 1-shot learning setup, our simple approach surpasses the white-box continuous prompting method CoOp by an average of 1.5% across 11 datasets including ImageNet. Our approach also outperforms OpenAI's manually crafted prompts and is more efficient than other black-box methods like iterative APE. Additionally, we highlight the advantage of conversational feedback incorporating both positive and negative prompts, suggesting that LLMs can utilize the implicit "gradient" direction in textual feedback for a more efficient search. Lastly, we find that the text prompts generated through our strategy are not only more interpretable but also transfer well across different CLIP architectures in a black-box manner.
</details>
<details>
<summary>摘要</summary>
现代视觉语言模型（VLM）在大规模网络数据上进行预训练后，在视觉和多模态任务上表现出了惊人的能力。然而，现有的细化方法主要 operate在白盒子 Setting中，需要访问模型参数进行反射。然而，许多VLM rely on proprietary data，这限制了使用白盒子approach for fine-tuning。在这种情况下，我们提出了一种新的细化方法，通过自然语言提示来避免访问模型参数、特征嵌入和输出征。在这种设置中，我们提议使用流行私人大型语言模型（LLM）like ChatGPT作为黑盒子优化器，通过自然语言反馈来搜索最佳提示。具体来说，我们采用了一种自动“山丘攀 climbing”过程，通过评估当前提示的准确率，请LLM进行提示的修改，以达到最佳提示。在一个挑战性的1shot learning设置下，我们的简单方法比白盒子连续提示方法CoOp高平均1.5% across 11 datasets，包括ImageNet。我们的方法还超过OpenAI manually 制作的提示，并且更高效 чем其他黑盒子方法，如迭代APE。此外，我们发现通过 incorporating both positive和negative提示，LLMs可以利用文本反馈中的隐式“梯度”方向进行更加高效的搜索。最后，我们发现通过我们的策略生成的文本提示不仅更加可读性高，还可以在黑盒子方式下跨不同的 CLIP 架构传输。
</details></li>
</ul>
<hr>
<h2 id="Do-PLMs-Know-and-Understand-Ontological-Knowledge"><a href="#Do-PLMs-Know-and-Understand-Ontological-Knowledge" class="headerlink" title="Do PLMs Know and Understand Ontological Knowledge?"></a>Do PLMs Know and Understand Ontological Knowledge?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05936">http://arxiv.org/abs/2309.05936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vickywu1022/ontoprobe-plms">https://github.com/vickywu1022/ontoprobe-plms</a></li>
<li>paper_authors: Weiqi Wu, Chengyue Jiang, Yong Jiang, Pengjun Xie, Kewei Tu</li>
<li>for: 这个论文的目的是探讨pretrained Language Models（PLMs）是否拥有ontological knowledge的能力。</li>
<li>methods: 作者使用了多种方法来探讨PLMs的ontological knowledge，包括测试PLMs是否能够记忆类和属性之间的层次关系，以及使用ontological entailment rules进行逻辑推理。</li>
<li>results: 研究结果表明PLMs可以记忆一定的ontological knowledge，并且可以使用这些知识进行逻辑推理。然而，PLMs的记忆和逻辑推理性能都不完善， indicating that PLMs的ontological knowledge是部分的和不够深入的。<details>
<summary>Abstract</summary>
Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a systematic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic understanding of the knowledge rather than rote memorization of the surface form. To probe whether PLMs know ontological knowledge, we investigate how well PLMs memorize: (1) types of entities; (2) hierarchical relationships among classes and properties, e.g., Person is a subclass of Animal and Member of Sports Team is a subproperty of Member of ; (3) domain and range constraints of properties, e.g., the subject of Member of Sports Team should be a Person and the object should be a Sports Team. To further probe whether PLMs truly understand ontological knowledge beyond memorization, we comprehensively study whether they can reliably perform logical reasoning with given knowledge according to ontological entailment rules. Our probing results show that PLMs can memorize certain ontological knowledge and utilize implicit knowledge in reasoning. However, both the memorizing and reasoning performances are less than perfect, indicating incomplete knowledge and understanding.
</details>
<details>
<summary>摘要</summary>
ontological knowledge，包括类和属性之间的关系，对世界知识是基础性的。但是，现有的 PLM 探测研究主要集中在事实知识上，缺乏系统性的探测ontological knowledge。本文将关注 PLM 是否具备ontological knowledge，并且是否具备semantic理解这种知识，而不是只是表面上的记忆。为了探测 PLM 是否知道ontological knowledge，我们调查 PLM 是否能够记忆以下三种内容：1. 类型的实体，例如 Person 是 Animal 的 subclass。2. 类和属性之间的层次关系，例如 Member of Sports Team 是 Member of 的 subproperty。3. 属性的域和范围约束，例如 Member of Sports Team 的主题应该是 Person，而 objet 应该是 Sports Team。为了更加全面地探测 PLM 是否真正理解ontological knowledge，我们进行了系统性的逻辑推理测试，根据 ontological entailment 规则。我们的探测结果表明，PLMs 可以记忆一些ontological knowledge，并且在推理过程中可以利用隐式知识。但是，记忆和推理的性能都不完美，表明 PLMs 的知识和理解仍有限制。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/12/cs.CL_2023_09_12/" data-id="clp869ttg00bok5886bj39vus" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/12/cs.LG_2023_09_12/" class="article-date">
  <time datetime="2023-09-12T10:00:00.000Z" itemprop="datePublished">2023-09-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/12/cs.LG_2023_09_12/">cs.LG - 2023-09-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Bregman-Graph-Neural-Network"><a href="#Bregman-Graph-Neural-Network" class="headerlink" title="Bregman Graph Neural Network"></a>Bregman Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06645">http://arxiv.org/abs/2309.06645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiayuzhai1207/bregmangnn">https://github.com/jiayuzhai1207/bregmangnn</a></li>
<li>paper_authors: Jiayu Zhai, Lequan Lin, Dai Shi, Junbin Gao</li>
<li>for: 本研究旨在提出一种基于布雷格曼距离的二级优化框架，以解决图神经网络（GNN）在节点分类任务中的过度简化问题。</li>
<li>methods: 本研究使用了布雷格曼距离的想法，设计了一种新的GNN层，并通过实验 validate 其效果。</li>
<li>results: 对比原始GNN层，布雷格曼GNN层能够更好地 mitigate 过度简化问题，并且在多层情况下仍然保持良好的学习精度。<details>
<summary>Abstract</summary>
Numerous recent research on graph neural networks (GNNs) has focused on formulating GNN architectures as an optimization problem with the smoothness assumption. However, in node classification tasks, the smoothing effect induced by GNNs tends to assimilate representations and over-homogenize labels of connected nodes, leading to adverse effects such as over-smoothing and misclassification. In this paper, we propose a novel bilevel optimization framework for GNNs inspired by the notion of Bregman distance. We demonstrate that the GNN layer proposed accordingly can effectively mitigate the over-smoothing issue by introducing a mechanism reminiscent of the "skip connection". We validate our theoretical results through comprehensive empirical studies in which Bregman-enhanced GNNs outperform their original counterparts in both homophilic and heterophilic graphs. Furthermore, our experiments also show that Bregman GNNs can produce more robust learning accuracy even when the number of layers is high, suggesting the effectiveness of the proposed method in alleviating the over-smoothing issue.
</details>
<details>
<summary>摘要</summary>
Recent research on graph neural networks (GNNs) has focused on formulating GNN architectures as optimization problems with the smoothness assumption. However, in node classification tasks, the smoothing effect induced by GNNs tends to assimilate representations and over-homogenize labels of connected nodes, leading to adverse effects such as over-smoothing and misclassification. In this paper, we propose a novel bilevel optimization framework for GNNs inspired by the notion of Bregman distance. We demonstrate that the GNN layer proposed accordingly can effectively mitigate the over-smoothing issue by introducing a mechanism reminiscent of the "skip connection". We validate our theoretical results through comprehensive empirical studies in which Bregman-enhanced GNNs outperform their original counterparts in both homophilic and heterophilic graphs. Furthermore, our experiments also show that Bregman GNNs can produce more robust learning accuracy even when the number of layers is high, suggesting the effectiveness of the proposed method in alleviating the over-smoothing issue.Here is the translation in Traditional Chinese:近期研究Graph Neural Networks (GNNs) 的方法都集中在设计GNN架构为优化问题中的匀数假设。然而，在节点分类任务中，GNNs 对节点的数据汇合和调整导致节点的表现变得太平等，从而导致过滤和错分类的问题。在这篇论文中，我们提出了一个新的两级优化框架 для GNNs， inspirited by Bregman distance的想法。我们显示了这种GNN层可以有效地减少过滤的问题，通过引入一种"skip connection"的机制。我们透过实验 validate 我们的理论结果，并证明了Bregman-enhanced GNNs 在同样的节点分类任务中表现更好，并且在不同的节点分布情况下也能够获得更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Audio-Based-Classification-of-Respiratory-Diseases-using-Advanced-Signal-Processing-and-Machine-Learning-for-Assistive-Diagnosis-Support"><a href="#Audio-Based-Classification-of-Respiratory-Diseases-using-Advanced-Signal-Processing-and-Machine-Learning-for-Assistive-Diagnosis-Support" class="headerlink" title="Audio-Based Classification of Respiratory Diseases using Advanced Signal Processing and Machine Learning for Assistive Diagnosis Support"></a>Audio-Based Classification of Respiratory Diseases using Advanced Signal Processing and Machine Learning for Assistive Diagnosis Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07183">http://arxiv.org/abs/2309.07183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Constantino Álvarez Casado, Manuel Lage Cañellas, Matteo Pedone, Xiaoting Wu, Miguel Bordallo López</li>
<li>for: 这个研究旨在提高快速检测技术，以帮助诊断呼吸系统疾病。</li>
<li>methods: 这个研究使用了一个大型的医疗数据库，并使用了Empirical Mode Decomposition（EMD）和spectral analysis来提取呼吸音数据中的生物对应信号。</li>
<li>results: 研究获得了87%的准确率来分别识别健康和疾病个体，并且还使用了六种分类模型来诊断呼吸系统疾病，例如肺炎和呼吸道疾病（COPD）。此外，这个研究还提出了一个年龄和体重指数（BMI）估计模型，以及一个性别分类模型，全都基于呼吸音数据。<details>
<summary>Abstract</summary>
In global healthcare, respiratory diseases are a leading cause of mortality, underscoring the need for rapid and accurate diagnostics. To advance rapid screening techniques via auscultation, our research focuses on employing one of the largest publicly available medical database of respiratory sounds to train multiple machine learning models able to classify different health conditions. Our method combines Empirical Mode Decomposition (EMD) and spectral analysis to extract physiologically relevant biosignals from acoustic data, closely tied to cardiovascular and respiratory patterns, making our approach apart in its departure from conventional audio feature extraction practices. We use Power Spectral Density analysis and filtering techniques to select Intrinsic Mode Functions (IMFs) strongly correlated with underlying physiological phenomena. These biosignals undergo a comprehensive feature extraction process for predictive modeling. Initially, we deploy a binary classification model that demonstrates a balanced accuracy of 87% in distinguishing between healthy and diseased individuals. Subsequently, we employ a six-class classification model that achieves a balanced accuracy of 72% in diagnosing specific respiratory conditions like pneumonia and chronic obstructive pulmonary disease (COPD). For the first time, we also introduce regression models that estimate age and body mass index (BMI) based solely on acoustic data, as well as a model for gender classification. Our findings underscore the potential of this approach to significantly enhance assistive and remote diagnostic capabilities.
</details>
<details>
<summary>摘要</summary>
首先，我们部署了一个二分类模型，其在健康和疾病个体之间具有87%的平衡准确率。然后，我们使用六类分类模型，其在诊断特定的呼吸疾病，如肺炎和慢性呼吸疾病（COPD）时， achieve a balanced accuracy of 72%。此外，我们还引入了年龄和体重指数（BMI）基于呼吸数据solely的回归模型，以及一个性别分类模型。我们的发现表明，这种方法可以备受提高辅助和远程诊断能力。
</details></li>
</ul>
<hr>
<h2 id="Adapt-and-Diffuse-Sample-adaptive-Reconstruction-via-Latent-Diffusion-Models"><a href="#Adapt-and-Diffuse-Sample-adaptive-Reconstruction-via-Latent-Diffusion-Models" class="headerlink" title="Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models"></a>Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06642">http://arxiv.org/abs/2309.06642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zalan Fabian, Berk Tinaz, Mahdi Soltanolkotabi</li>
<li>for: 本研究旨在提高逆Problem的解决效率，通过估算受损度的严重程度来适应不同样本的受损程度，以提高解决效率和计算效率。</li>
<li>methods: 本研究使用了扩展的强迫推理模型，通过在推理过程中估算受损度的严重程度来适应不同样本的受损程度，并通过采用精度适应的推理方法来提高解决效率。</li>
<li>results: 本研究的实验结果表明，采用估算受损度的严重程度来适应不同样本的受损程度可以提高逆Problem的解决效率和计算效率，并且与现有的扩展推理方法相比，提高了解决效率和计算效率。<details>
<summary>Abstract</summary>
Inverse problems arise in a multitude of applications, where the goal is to recover a clean signal from noisy and possibly (non)linear observations. The difficulty of a reconstruction problem depends on multiple factors, such as the structure of the ground truth signal, the severity of the degradation, the implicit bias of the reconstruction model and the complex interactions between the above factors. This results in natural sample-by-sample variation in the difficulty of a reconstruction task, which is often overlooked by contemporary techniques. Recently, diffusion-based inverse problem solvers have established new state-of-the-art in various reconstruction tasks. However, they have the drawback of being computationally prohibitive. Our key observation in this paper is that most existing solvers lack the ability to adapt their compute power to the difficulty of the reconstruction task, resulting in long inference times, subpar performance and wasteful resource allocation. We propose a novel method that we call severity encoding, to estimate the degradation severity of noisy, degraded signals in the latent space of an autoencoder. We show that the estimated severity has strong correlation with the true corruption level and can give useful hints at the difficulty of reconstruction problems on a sample-by-sample basis. Furthermore, we propose a reconstruction method based on latent diffusion models that leverages the predicted degradation severities to fine-tune the reverse diffusion sampling trajectory and thus achieve sample-adaptive inference times. We utilize latent diffusion posterior sampling to maintain data consistency with observations. We perform experiments on both linear and nonlinear inverse problems and demonstrate that our technique achieves performance comparable to state-of-the-art diffusion-based techniques, with significant improvements in computational efficiency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>逆 проблеme 在多种应用中出现，其目标是从噪声和可能非线性的观测数据中回归到清晰的信号。逆回归问题的difficulty取决于多种因素，如真实的地面信号结构、观测数据的严重程度、逆回归模型的隐式偏见以及这些因素之间的复杂互动。这导致每个样本的逆回归任务的difficulty具有自然的样本差异，通常被当前技术所忽略。在这篇论文中，我们提出了一种新的方法，称为严重编码，以估计噪声损坏的严重程度。我们发现，这个估计的严重程度与真实的损坏水平具有强相关性，并且可以为逆回归任务的样本差异提供有用的提示。此外，我们提出了基于扩散模型的逆回归方法，利用预测的损坏严重程度来细化反扩散抽样 trajectory，以实现样本适应的计算效率。我们使用扩散 posterior 抽样来保持数据的一致性与观测数据。我们在线性和非线性逆回归问题上进行了实验，并证明了我们的技术与当前扩散基于技术相比，可以实现类似的性能，同时具有显著的计算效率提升。
</details></li>
</ul>
<hr>
<h2 id="PCN-A-Deep-Learning-Approach-to-Jet-Tagging-Utilizing-Novel-Graph-Construction-Methods-and-Chebyshev-Graph-Convolutions"><a href="#PCN-A-Deep-Learning-Approach-to-Jet-Tagging-Utilizing-Novel-Graph-Construction-Methods-and-Chebyshev-Graph-Convolutions" class="headerlink" title="PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions"></a>PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08630">http://arxiv.org/abs/2309.08630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yvsemlani/pcn-jet-tagging">https://github.com/yvsemlani/pcn-jet-tagging</a></li>
<li>paper_authors: Yash Semlani, Mihir Relan, Krithik Ramesh</li>
<li>for: 这个论文的目的是提高高能物理实验中的jet标记率，以便搜索新的物理理论。</li>
<li>methods: 这个论文使用了深度学习来探索复杂的射击数据中的隐藏模式。它使用了图基的表示方法，并设计了一种叫做Particle Chebyshev Network（PCN）的图神经网络，使用Chebyshev图 convolution来学习图数据。</li>
<li>results: 这个论文的实验结果表明，PCN可以提高jet标记率，并且比现有的标记器更高。这个研究开启了将图基的表示方法和ChebConv层应用于高能物理实验的可能性。<details>
<summary>Abstract</summary>
Jet tagging is a classification problem in high-energy physics experiments that aims to identify the collimated sprays of subatomic particles, jets, from particle collisions and tag them to their emitter particle. Advances in jet tagging present opportunities for searches of new physics beyond the Standard Model. Current approaches use deep learning to uncover hidden patterns in complex collision data. However, the representation of jets as inputs to a deep learning model have been varied, and often, informative features are withheld from models. In this study, we propose a graph-based representation of a jet that encodes the most information possible. To learn best from this representation, we design Particle Chebyshev Network (PCN), a graph neural network (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been demonstrated as an effective alternative to classical graph convolutions in GNNs and has yet to be explored in jet tagging. PCN achieves a substantial improvement in accuracy over existing taggers and opens the door to future studies into graph-based representations of jets and ChebConv layers in high-energy physics experiments. Code is available at https://github.com/YVSemlani/PCN-Jet-Tagging.
</details>
<details>
<summary>摘要</summary>
高能物理实验中的喷气标记是一种分类问题，旨在从素子反应中检测和标识喷气，这些喷气是由素子产生的。随着喷气标记的进步，开放了新物理学之外的搜索。现有方法使用深度学习来探索复杂的喷气数据中的隐藏模式。然而，喷气被输入到深度学习模型中的表示方法各异，经常会排除有用的特征。在这个研究中，我们提议一种图格基的喷气表示方法，该方法可以最大化喷气中的信息。为了利用这种表示方法，我们设计了Particle Chebyshev Network（PCN），这是一种使用Chebychev图 convolution（ChebConv）的图神经网络（GNN）。ChebConv已经证明是传统图 convolution的有效替代方案，尚未在喷气标记中使用。PCN实现了与现有标记器相比的显著改善，打开了将来研究图基的喷气表示方法和ChebConv层在高能物理实验中的大门。代码可以在https://github.com/YVSemlani/PCN-Jet-Tagging上获取。
</details></li>
</ul>
<hr>
<h2 id="Sleep-Stage-Classification-Using-a-Pre-trained-Deep-Learning-Model"><a href="#Sleep-Stage-Classification-Using-a-Pre-trained-Deep-Learning-Model" class="headerlink" title="Sleep Stage Classification Using a Pre-trained Deep Learning Model"></a>Sleep Stage Classification Using a Pre-trained Deep Learning Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07182">http://arxiv.org/abs/2309.07182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Ardeshir, Mohammad Araghi</li>
<li>for: 这个研究是为了开发一个基于机器学习的睡眠阶段分类模型，以帮助诊断睡眠障碍、评估治疗效果和理解睡眠阶段与不同的健康状况之间的关系。</li>
<li>methods: 这个研究使用了预训练的模型和电encephalogram（EEG）spectrograms的脑信号来开发一个名为“EEGMobile”的机器学习模型。</li>
<li>results: 这个模型在一个公开available的数据集“Sleep-EDF20”上取得了86.97%的准确性，比其他已知的模型更高。其中在阶段N1上的准确性为56.4%，也比其他模型更高。这些发现表明这个模型有 potential to achieve better results for the treatment of sleep disorders.<details>
<summary>Abstract</summary>
One of the common human diseases is sleep disorders. The classification of sleep stages plays a fundamental role in diagnosing sleep disorders, monitoring treatment effectiveness, and understanding the relationship between sleep stages and various health conditions. A precise and efficient classification of these stages can significantly enhance our understanding of sleep-related phenomena and ultimately lead to improved health outcomes and disease treatment.   Models others propose are often time-consuming and lack sufficient accuracy, especially in stage N1. The main objective of this research is to present a machine-learning model called "EEGMobile". This model utilizes pre-trained models and learns from electroencephalogram (EEG) spectrograms of brain signals. The model achieved an accuracy of 86.97% on a publicly available dataset named "Sleep-EDF20", outperforming other models proposed by different researchers. Moreover, it recorded an accuracy of 56.4% in stage N1, which is better than other models. These findings demonstrate that this model has the potential to achieve better results for the treatment of this disease.
</details>
<details>
<summary>摘要</summary>
一种常见的人类疾病是睡眠障碍。睡眠阶段的分类扮演着基本的角色在诊断睡眠障碍、监测治疗效果和理解各种健康状况之间的关系。一个精准和高效的分类方法可以有效提高我们对睡眠相关现象的理解，从而导致改善健康结果和疾病治疗。其他研究人员提出的模型经常占用时间和缺乏准确性，尤其是N1阶段。本研究的主要目标是提出一种名为"EEGMobile"的机器学习模型，该模型利用预训练模型和电энцефаogram（EEG）spectrogram的脑信号学习。该模型在公共数据集"Sleep-EDF20"上 achievied an accuracy of 86.97%, outperforming other models proposed by different researchers. Furthermore, it recorded an accuracy of 56.4% in stage N1, which is better than other models. These findings demonstrate that this model has the potential to achieve better results for the treatment of this disease.
</details></li>
</ul>
<hr>
<h2 id="G-Mapper-Learning-a-Cover-in-the-Mapper-Construction"><a href="#G-Mapper-Learning-a-Cover-in-the-Mapper-Construction" class="headerlink" title="$G$-Mapper: Learning a Cover in the Mapper Construction"></a>$G$-Mapper: Learning a Cover in the Mapper Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06634">http://arxiv.org/abs/2309.06634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enrique Alvarado, Robin Belton, Emily Fischer, Kang-Ju Lee, Sourabh Palande, Sarah Percival, Emilie Purvine</li>
<li>for: 这个论文主要是关于如何选择Mapper图的覆盖，以便在扩展TDA中保留数据的本质。</li>
<li>methods: 该论文提出了一种基于$G$-means clustering的算法，通过Iteratively进行安德森-达瑞尔测试来选择最佳覆盖，并使用 Gaussian mixture model来决定覆盖的基于数据的分布。</li>
<li>results: 实验表明，该算法可以生成高质量的覆盖，使得Mapper图能够 retain the essence of the datasets。<details>
<summary>Abstract</summary>
The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a "nice" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.
</details>
<details>
<summary>摘要</summary>
“Mapper”算法是一种数据可视化技术，用于批处理数据分析（TDA）中的数据结构。“Mapper”算法需要调整一些参数，以生成一个“好看”的图表。本文主要关注选择“覆盖”参数。我们提出了一种基于$G$-mean clustering的算法，通过每次进行 Anderson-Darling 测试来选择最佳分区。我们的分割过程使用 Gaussian mixture model，以便选择基于数据的分布来选择覆盖。我们的实验表明，我们的算法可以生成一个保留数据的本质的 Mapper 图。
</details></li>
</ul>
<hr>
<h2 id="Epistemic-Modeling-Uncertainty-of-Rapid-Neural-Network-Ensembles-for-Adaptive-Learning"><a href="#Epistemic-Modeling-Uncertainty-of-Rapid-Neural-Network-Ensembles-for-Adaptive-Learning" class="headerlink" title="Epistemic Modeling Uncertainty of Rapid Neural Network Ensembles for Adaptive Learning"></a>Epistemic Modeling Uncertainty of Rapid Neural Network Ensembles for Adaptive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06628">http://arxiv.org/abs/2309.06628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atticusbeachy/multi-fidelity-nn-ensemble-examples">https://github.com/atticusbeachy/multi-fidelity-nn-ensemble-examples</a></li>
<li>paper_authors: Atticus Beachy, Harok Bae, Jose Camberos, Ramana Grandhi</li>
<li>for: 这个研究旨在开发一种内置模拟器的类型physics informed neural network，以便优化航空工程系统的设计探索。</li>
<li>methods: 这个方法使用多元数据来源，包括不同的随机初始化模型。 ensemble of 模型 realizations 用于评估因为缺乏训练数据而导致的模型建构不确定性。</li>
<li>results: 这个研究发现，使用rapid neural network paradigm可以实现快速的模型训练，而无需损失预测精度。 在多个分析例子中，以及一个实际的 hypersonic vehicle 飞行 Parameters 研究中，提出了一种内置模拟器的新型类型。<details>
<summary>Abstract</summary>
Emulator embedded neural networks, which are a type of physics informed neural network, leverage multi-fidelity data sources for efficient design exploration of aerospace engineering systems. Multiple realizations of the neural network models are trained with different random initializations. The ensemble of model realizations is used to assess epistemic modeling uncertainty caused due to lack of training samples. This uncertainty estimation is crucial information for successful goal-oriented adaptive learning in an aerospace system design exploration. However, the costs of training the ensemble models often become prohibitive and pose a computational challenge, especially when the models are not trained in parallel during adaptive learning. In this work, a new type of emulator embedded neural network is presented using the rapid neural network paradigm. Unlike the conventional neural network training that optimizes the weights and biases of all the network layers by using gradient-based backpropagation, rapid neural network training adjusts only the last layer connection weights by applying a linear regression technique. It is found that the proposed emulator embedded neural network trains near-instantaneously, typically without loss of prediction accuracy. The proposed method is demonstrated on multiple analytical examples, as well as an aerospace flight parameter study of a generic hypersonic vehicle.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>模拟器内置神经网络，它是physics informed neural network的一种类型，利用多种数据来源进行航空工程系统设计的有效探索。多个神经网络模型实现被不同的随机初始化训练。ensemble的模型实现用于评估因缺乏训练样本而引起的epistemic模型不确定性。这种不确定性评估是成功目标适应学arning的关键信息。然而，训练ensemble模型的成本经常成为计算挑战，尤其当模型在适应学习过程中不在平行进行训练。在这种情况下，一种新的模拟器内置神经网络方法被提出，使用rapid neural network paradigm。与传统神经网络训练不同，这种训练方法只是将最后层连接权重调整，通过应用线性回归技术。发现，提议的模拟器内置神经网络在几乎实时内训练，通常无损失预测精度。这种方法在多个分析例子中进行了证明，以及一个涉及到一个通用 hypersonic 飞行器的航空飞行参数研究。
</details></li>
</ul>
<hr>
<h2 id="A-Sequentially-Fair-Mechanism-for-Multiple-Sensitive-Attributes"><a href="#A-Sequentially-Fair-Mechanism-for-Multiple-Sensitive-Attributes" class="headerlink" title="A Sequentially Fair Mechanism for Multiple Sensitive Attributes"></a>A Sequentially Fair Mechanism for Multiple Sensitive Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06627">http://arxiv.org/abs/2309.06627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phi-ra/SequentialFairness">https://github.com/phi-ra/SequentialFairness</a></li>
<li>paper_authors: François Hu, Philipp Ratz, Arthur Charpentier<br>for: 这个论文的目的是为了减少敏感变量和相应分数之间的关系，以解决多重敏感特征情况下的内部平衡问题。methods: 这个论文使用了多重条件 Wasserstein 中心来扩展了传统的强人口平衡定义，以便在多重敏感特征情况下进行平衡。这种方法提供了一个关键的关系解释，允许进行精确的不公平预测器。results: 这个论文的实验结果显示，这种方法可以有效地减少多重敏感特征情况下的不公平情况。此外，这种方法还可以让敏感特征之间的联乘关系获得明确的解释。<details>
<summary>Abstract</summary>
In the standard use case of Algorithmic Fairness, the goal is to eliminate the relationship between a sensitive variable and a corresponding score. Throughout recent years, the scientific community has developed a host of definitions and tools to solve this task, which work well in many practical applications. However, the applicability and effectivity of these tools and definitions becomes less straightfoward in the case of multiple sensitive attributes. To tackle this issue, we propose a sequential framework, which allows to progressively achieve fairness across a set of sensitive features. We accomplish this by leveraging multi-marginal Wasserstein barycenters, which extends the standard notion of Strong Demographic Parity to the case with multiple sensitive characteristics. This method also provides a closed-form solution for the optimal, sequentially fair predictor, permitting a clear interpretation of inter-sensitive feature correlations. Our approach seamlessly extends to approximate fairness, enveloping a framework accommodating the trade-off between risk and unfairness. This extension permits a targeted prioritization of fairness improvements for a specific attribute within a set of sensitive attributes, allowing for a case specific adaptation. A data-driven estimation procedure for the derived solution is developed, and comprehensive numerical experiments are conducted on both synthetic and real datasets. Our empirical findings decisively underscore the practical efficacy of our post-processing approach in fostering fair decision-making.
</details>
<details>
<summary>摘要</summary>
通常的用例中的算法公平 goal 是消除敏感变量和相应的分数之间的关系。在过去的几年中，科学社区已经开发出了许多定义和工具来解决这个问题，这些工具在许多实际应用中表现良好。然而，在多个敏感特征的情况下，这些工具和定义的可用性和效果变得更加复杂。为了解决这个问题，我们提出了一个顺序框架，可以逐步实现公平性 across a set of sensitive features。我们实现这一点通过利用多个敏感特征的 Wasserstein 多重中心，该扩展了标准的强人口平衡定义到多个敏感特征的情况。这种方法还提供了一个关闭式解的优化、公平预测器，允许明确地理解敏感特征之间的相互关系。我们的方法自然地扩展到近似公平性，包括一个折衔策略，可以根据特定敏感特征内的减少不公平性进行目标化优化。这种扩展允许特定敏感特征的公平改进，使得可以实现案例特定的适应。我们发展了一种数据驱动的估计过程，并在synthetic和实际数据集上进行了广泛的数值实验。我们的实验证明了我们的后处理方法在不公平决策中具有实际的有效性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Contraction-Coefficient-of-the-Schrodinger-Bridge-for-Stochastic-Linear-Systems"><a href="#On-the-Contraction-Coefficient-of-the-Schrodinger-Bridge-for-Stochastic-Linear-Systems" class="headerlink" title="On the Contraction Coefficient of the Schrödinger Bridge for Stochastic Linear Systems"></a>On the Contraction Coefficient of the Schrödinger Bridge for Stochastic Linear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06622">http://arxiv.org/abs/2309.06622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexis M. H. Teter, Yongxin Chen, Abhishek Halder</li>
<li>for: 解决Schrödinger bridge问题，即在控制Diffusion下从一个初始状态分布转移到另一个状态分布，并且在满足时间约束下实现最优化。</li>
<li>methods: 使用Contractive fixed point recursions方法来数学解Schrödinger bridge问题，这种方法可以看作是动态版本的Sinkhorn迭代，并且在某些假设下可以 garantizar线性减少。</li>
<li>results: 研究Schrödinger系统的前提估计，并提供新的几何和控制理论 интерпретаciones。此外，还指出了可以通过预处理终点支持集来提高worst-case contraction coefficient的计算。<details>
<summary>Abstract</summary>
Schr\"{o}dinger bridge is a stochastic optimal control problem to steer a given initial state density to another, subject to controlled diffusion and deadline constraints. A popular method to numerically solve the Schr\"{o}dinger bridge problems, in both classical and in the linear system settings, is via contractive fixed point recursions. These recursions can be seen as dynamic versions of the well-known Sinkhorn iterations, and under mild assumptions, they solve the so-called Schr\"{o}dinger systems with guaranteed linear convergence. In this work, we study a priori estimates for the contraction coefficients associated with the convergence of respective Schr\"{o}dinger systems. We provide new geometric and control-theoretic interpretations for the same. Building on these newfound interpretations, we point out the possibility of improved computation for the worst-case contraction coefficients of linear SBPs by preconditioning the endpoint support sets.
</details>
<details>
<summary>摘要</summary>
Schrödinger 桥是一个 Stochastic Optimal Control 问题，旨在从一个初始状态密度到另一个，并且受到控制的扩散和时间限制。一种广泛使用的方法来数学解决 Schrödinger 桥问题是通过收缩的定点反射，这些反射可以看作是动态版本的 Sinkhorn 迭代。在我们的研究中，我们研究了 Schrödinger 系统的前期估计，并提供了新的几何和控制理论解释。我们还指出了可以通过预处理终点支持集来提高 worst-case 收缩系数的计算。
</details></li>
</ul>
<hr>
<h2 id="The-Grand-Illusion-The-Myth-of-Software-Portability-and-Implications-for-ML-Progress"><a href="#The-Grand-Illusion-The-Myth-of-Software-Portability-and-Implications-for-ML-Progress" class="headerlink" title="The Grand Illusion: The Myth of Software Portability and Implications for ML Progress"></a>The Grand Illusion: The Myth of Software Portability and Implications for ML Progress</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07181">http://arxiv.org/abs/2309.07181</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/for-ai/portability">https://github.com/for-ai/portability</a></li>
<li>paper_authors: Fraser Mince, Dzung Dinh, Jonas Kgomo, Neil Thompson, Sara Hooker</li>
<li>for: 本研究旨在探讨主流机器学习软件框架在不同硬件类型上的可移植性。</li>
<li>methods: 我们采用了大规模的研究方法，测试了主流机器学习软件框架在不同硬件类型上的可移植性。</li>
<li>results: 我们的结果显示，主流机器学习软件框架在不同硬件类型上可能会产生更多于40%的关键功能损失，并且即使功能可以移植，其性能下降也可能是极大的，使得性能不可接受。<details>
<summary>Abstract</summary>
Pushing the boundaries of machine learning often requires exploring different hardware and software combinations. However, the freedom to experiment across different tooling stacks can be at odds with the drive for efficiency, which has produced increasingly specialized AI hardware and incentivized consolidation around a narrow set of ML frameworks. Exploratory research can be restricted if software and hardware are co-evolving, making it even harder to stray away from mainstream ideas that work well with popular tooling stacks. While this friction increasingly impacts the rate of innovation in machine learning, to our knowledge the lack of portability in tooling has not been quantified. In this work, we ask: How portable are popular ML software frameworks? We conduct a large-scale study of the portability of mainstream ML frameworks across different hardware types. Our findings paint an uncomfortable picture -- frameworks can lose more than 40% of their key functions when ported to other hardware. Worse, even when functions are portable, the slowdown in their performance can be extreme and render performance untenable. Collectively, our results reveal how costly straying from a narrow set of hardware-software combinations can be - and suggest that specialization of hardware impedes innovation in machine learning research.
</details>
<details>
<summary>摘要</summary>
推动机器学学科的前沿研究经常需要尝试不同的硬件和软件组合。然而，在效率的推动下，AI硬件的特циали化和主流ML框架的吸引力使得尝试新的想法和工具栈的探索变得更加困难。探索性研究可能会受到软硬件的共演化限制，使得尝试离开主流想法和工具栈更加困难。这种阻力在机器学学科的创新速度中产生了一定的影响，但到目前为止，工具栈的无法移植的问题尚未得到量化的解决。在这项工作中，我们问：流行的ML软件框架是否可以具有可移植性？我们进行了大规模的流行ML框架在不同硬件类型上的可移植性研究。我们的结果表现出一个不适的情况：框架在其他硬件上转移时可能会产生超过40%的关键功能损失。更糟糕的是，即使功能是可移植的，其性能下降可能会非常大，使其性能不可接受。总的来说，我们的结果表明特циали化的硬件对机器学学科的创新带来了成本，并建议特циали化的硬件阻碍了机器学学科的进步。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Learning-of-Nanoindentation-Data-to-Infer-Microstructural-Details-of-Complex-Materials"><a href="#Unsupervised-Learning-of-Nanoindentation-Data-to-Infer-Microstructural-Details-of-Complex-Materials" class="headerlink" title="Unsupervised Learning of Nanoindentation Data to Infer Microstructural Details of Complex Materials"></a>Unsupervised Learning of Nanoindentation Data to Infer Microstructural Details of Complex Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06613">http://arxiv.org/abs/2309.06613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhang, Clémence Bos, Stefan Sandfeld, Ruth Schwaiger</li>
<li>for: 这项研究使用了氧化镍-碳钴复合材料，通过尺寸挤压测试来研究其机械性能。</li>
<li>methods: 这项研究使用了尺寸挤压测试，并采用了不supervised学习技术， Gaussian mixture model，来分析数据，确定了机械阶段的数量和相应的机械性能。</li>
<li>results: 研究结果显示，采用不supervised学习技术可以有效地分析机械性能数据，并确定了机械阶段的数量。此外，通过cross-validation方法，研究人员还能够评估数据的充足性，并建议数据的充足量为可靠预测所需。<details>
<summary>Abstract</summary>
In this study, Cu-Cr composites were studied by nanoindentation. Arrays of indents were placed over large areas of the samples resulting in datasets consisting of several hundred measurements of Young's modulus and hardness at varying indentation depths. The unsupervised learning technique, Gaussian mixture model, was employed to analyze the data, which helped to determine the number of "mechanical phases" and the respective mechanical properties. Additionally, a cross-validation approach was introduced to infer whether the data quantity was adequate and to suggest the amount of data required for reliable predictions -- one of the often encountered but difficult to resolve issues in machine learning of materials science problems.
</details>
<details>
<summary>摘要</summary>
在这项研究中，氧化铜-镍复合材料被使用nano indent方法进行研究。数组式的 indent 被置于样品表面上，从而生成了包含多个百度测量 Young's modulus 和硬度的数据集。我们使用了无监督学习技术 Gaussian mixture model 来分析数据，以确定机械相的数量和相应的机械性质。此外，我们还提出了一种cross-validation方法，以判断数据量是否充分，并建议数据的充分量是否可靠的预测。这是机器学习材料科学问题中经常遇到，但很难解决的问题。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-with-Latent-Diffusion-in-Offline-Reinforcement-Learning"><a href="#Reasoning-with-Latent-Diffusion-in-Offline-Reinforcement-Learning" class="headerlink" title="Reasoning with Latent Diffusion in Offline Reinforcement Learning"></a>Reasoning with Latent Diffusion in Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06599">http://arxiv.org/abs/2309.06599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ldcq/ldcq">https://github.com/ldcq/ldcq</a></li>
<li>paper_authors: Siddarth Venkatraman, Shivesh Khaitan, Ravi Tej Akella, John Dolan, Jeff Schneider, Glen Berseth</li>
<li>for: 这篇论文的目的是学习无需环境互动的停滞学习（Offline Reinforcement Learning）策略，从静止数据集中学习高奖策略。</li>
<li>methods: 这篇论文提出了一种使用干扰扩散来模型支持游戏中的循环游戏路径，从而避免因缺乏数据支持而导致的推断错误。</li>
<li>results: 这篇论文的实验结果显示，使用该方法可以在D4RL测试集上达到最佳性能，特别是在长期、罕见奖励任务中表现出色。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset while avoiding extrapolation errors arising due to a lack of support in the dataset. Existing approaches use conservative methods that are tricky to tune and struggle with multi-modal data (as we show) or rely on noisy Monte Carlo return-to-go samples for reward conditioning. In this work, we propose a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining. The latent space is also expressive and gracefully copes with multi-modal data. We show that the learned temporally-abstract latent space encodes richer task-specific information for offline RL tasks as compared to raw state-actions. This improves credit assignment and facilitates faster reward propagation during Q-learning. Our method demonstrates state-of-the-art performance on the D4RL benchmarks, particularly excelling in long-horizon, sparse-reward tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset while avoiding extrapolation errors arising due to a lack of support in the dataset. Existing approaches use conservative methods that are tricky to tune and struggle with multi-modal data (as we show) or rely on noisy Monte Carlo return-to-go samples for reward conditioning. In this work, we propose a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining. The latent space is also expressive and gracefully copes with multi-modal data. We show that the learned temporally-abstract latent space encodes richer task-specific information for offline RL tasks as compared to raw state-actions. This improves credit assignment and facilitates faster reward propagation during Q-learning. Our method demonstrates state-of-the-art performance on the D4RL benchmarks, particularly excelling in long-horizon, sparse-reward tasks." into Simplified Chinese.下面是文本的中文翻译：<<SYS>>offline reinforcement learning (RL) 承袭了从静态数据集中学习高奖策略的承袭，无需进一步与环境进行交互。然而，offline RL 中的一个关键挑战在于有效地将静态数据集中的部分亚优轨迹串连起来，而不导致因数据集缺失支持而产生的推断错误。现有的方法通常使用保守的方法，困难调整并在多模态数据上异常表现（我们显示），或者基于噪声 Monte Carlo 返回样本进行奖励条件。在这种情况下，我们提出了一种新的方法，利用干扰扩散的表达能力来模型在支持轨迹序列上的压缩 latent 技巧。这使得学习 Q-函数时避免推断错误，并通过批量约束来实现。干扰扩散空间也具有表达能力，可以gracefully 处理多模态数据。我们显示，学习的时间抽象 latent 空间中包含更加任务特定的信息，比raw state-action 更加丰富，这使得奖励赋权更加准确，并促进了更快的奖励传递。我们的方法在 D4RL 标准准测试上达到了领先的性能，特别是在长远、稀热奖 Task 上。
</details></li>
</ul>
<hr>
<h2 id="Optimal-and-Fair-Encouragement-Policy-Evaluation-and-Learning"><a href="#Optimal-and-Fair-Encouragement-Policy-Evaluation-and-Learning" class="headerlink" title="Optimal and Fair Encouragement Policy Evaluation and Learning"></a>Optimal and Fair Encouragement Policy Evaluation and Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07176">http://arxiv.org/abs/2309.07176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angela Zhou</li>
<li>for: 这篇论文主要是研究如何设计优化的治疗方案，以便在不可预知的人们是否遵循治疗建议的情况下，最大化 causal 结果。</li>
<li>methods: 这篇论文使用了 causal 标识、统计量化减少、和robust 估计方法，以便在不可预知的人们是否遵循治疗建议的情况下，估计优化的治疗方案。</li>
<li>results: 这篇论文的结果表明，在不可预知的人们是否遵循治疗建议的情况下，可以使用 constrained 优化方法来设计优化的治疗方案，以便实现最大化 causal 结果。<details>
<summary>Abstract</summary>
In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal policy rules are merely suggestions in the presence of human non-adherence to treatment recommendations. In these same domains, there may be heterogeneity both in who responds in taking-up treatment, and heterogeneity in treatment efficacy. While optimal treatment rules can maximize causal outcomes across the population, access parity constraints or other fairness considerations can be relevant in the case of encouragement. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When in addition the decision-maker has distributional preferences over both access and average outcomes, the optimal decision rule changes. We study causal identification, statistical variance-reduced estimation, and robust estimation of optimal treatment rules, including under potential violations of positivity. We consider fairness constraints such as demographic parity in treatment take-up, and other constraints, via constrained optimization. Our framework can be extended to handle algorithmic recommendations under an often-reasonable covariate-conditional exclusion restriction, using our robustness checks for lack of positivity in the recommendation. We develop a two-stage algorithm for solving over parametrized policy classes under general constraints to obtain variance-sensitive regret bounds. We illustrate the methods in two case studies based on data from randomized encouragement to enroll in insurance and from pretrial supervised release with electronic monitoring.
</details>
<details>
<summary>摘要</summary>
在重要领域中，常常无法强制个人接受治疗，因此优化的政策规则仅仅是建议在人类不遵循治疗建议的情况下。在这些同时，可能存在人群响应不同，以及治疗效果的差异。优化的治疗规则可以最大化人口级别的 causal 结果，但是访问平等约束或其他公平考虑可能在鼓励中发挥作用。例如，在社会服务中，一个持续的谜题是养成很好的服务的投入率不均匀分布。当决策者有访问和平均结果的分布预期时，优化的决策规则会改变。我们研究 causal 验证、统计减少的估计和robust estimation of 优化的治疗规则，包括在可能的非正式性下进行验证。我们考虑公平约束，如人群响应不同和其他约束，通过受限优化来实现。我们的框架可以扩展到处理算法建议在conditionally reasonable covariate-conditional exclusion restriction下进行验证，使用我们的robustness检查来检测非正式性。我们开发了一种两个参数的算法，用于在总体约束下解决参数化政策类型的问题，以获得变量敏感的 regret  bound。我们在两个案例研究中应用了这些方法，基于随机鼓励保险和预 trial supervised release with electronic monitoring。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Gradient-based-MAML-in-LQR"><a href="#Convergence-of-Gradient-based-MAML-in-LQR" class="headerlink" title="Convergence of Gradient-based MAML in LQR"></a>Convergence of Gradient-based MAML in LQR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06588">http://arxiv.org/abs/2309.06588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Negin Musavi, Geir E. Dullerud</li>
<li>for:  investigate the local convergence characteristics of Model-agnostic Meta-learning (MAML) in linear system quadratic optimal control (LQR)</li>
<li>methods:  uses MAML and its variations, with theoretical guarantees provided for the local convergence of the algorithm</li>
<li>results:  presents simple numerical results to demonstrate the convergence properties of MAML in LQR tasksHere’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
The main objective of this research paper is to investigate the local convergence characteristics of Model-agnostic Meta-learning (MAML) when applied to linear system quadratic optimal control (LQR). MAML and its variations have become popular techniques for quickly adapting to new tasks by leveraging previous learning knowledge in areas like regression, classification, and reinforcement learning. However, its theoretical guarantees remain unknown due to non-convexity and its structure, making it even more challenging to ensure stability in the dynamic system setting. This study focuses on exploring MAML in the LQR setting, providing its local convergence guarantees while maintaining the stability of the dynamical system. The paper also presents simple numerical results to demonstrate the convergence properties of MAML in LQR tasks.
</details>
<details>
<summary>摘要</summary>
主要目标 OF 这个研究论文是 investigate Model-agnostic Meta-learning（MAML）在线性系统弗 quadratic 优化控制（LQR）中的本地叉�强特性。 MAML 和其变种在领域如回归、分类和 reinforcement learning 中利用 previous learning knowledge 快速适应新任务，但其理论保证仍然不清楚，因为非拟合性和结构，使得在动态系统设置下稳定性更加挑战。 这个研究集中关注 MAML 在 LQR 设置中的本地叉�强特性，提供了本地叉�强保证，同时保证动态系统的稳定性。 论文还展示了简单的数据结果，以证明 MAML 在 LQR 任务中的 converges 性质。Here's the text with some additional information about the translation:I used Google Translate to translate the text into Simplified Chinese. However, please note that the translation may not be perfect and may require some adjustments to ensure accuracy. Additionally, the translation may not capture the exact nuances and idiomatic expressions of the original text, so some phrasing and wording may be different from the original.
</details></li>
</ul>
<hr>
<h2 id="Explainable-Graph-Neural-Network-for-Alzheimer’s-Disease-And-Related-Dementias-Risk-Prediction"><a href="#Explainable-Graph-Neural-Network-for-Alzheimer’s-Disease-And-Related-Dementias-Risk-Prediction" class="headerlink" title="Explainable Graph Neural Network for Alzheimer’s Disease And Related Dementias Risk Prediction"></a>Explainable Graph Neural Network for Alzheimer’s Disease And Related Dementias Risk Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06584">http://arxiv.org/abs/2309.06584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyue Hu, Zenan Sun, Yi Nian, Yifang Dang, Fang Li, Jingna Feng, Evan Yu, Cui Tao<br>for:* 这个研究旨在提高了阿尔ツ海默症和相关失智症（ADRD）的风险预测，使用机器学习和养成数据。methods:* 这个研究使用了变换正则化编码器-解码器图ael neural network（VGNN）来估算ADRD的可能性。results:* VGNN比Random Forest和Light Gradient Boost Machine基线模型高出10%的 receiver operating characteristic 面积。In simplified Chinese:for:* 这个研究旨在提高了阿尔ツ海默症和相关失智症（ADRD）的风险预测，使用机器学习和养成数据。methods:* 这个研究使用了变换正则化编码器-解码器图ael neural network（VGNN）来估算ADRD的可能性。results:* VGNN比Random Forest和Light Gradient Boost Machine基线模型高出10%的 receiver operating characteristic 面积。<details>
<summary>Abstract</summary>
Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading cause of death in the US, underlining the importance of accurate ADRD risk prediction. While recent advancement in ADRD risk prediction have primarily relied on imaging analysis, yet not all patients undergo medical imaging before an ADRD diagnosis. Merging machine learning with claims data can reveal additional risk factors and uncover interconnections among diverse medical codes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD risk prediction. Addressing the lack of human-interpretable reasons behind these predictions, we introduce an innovative method to evaluate relationship importance and its influence on ADRD risk prediction, ensuring comprehensive interpretation.   We employed Variationally Regularized Encoder-decoder Graph Neural Network (VGNN) for estimating ADRD likelihood. We created three scenarios to assess the model's efficiency, using Random Forest and Light Gradient Boost Machine as baselines. We further used our relation importance method to clarify the key relationships for ADRD risk prediction. VGNN surpassed other baseline models by 10% in the area under the receiver operating characteristic. The integration of the GNN model and relation importance interpretation could potentially play an essential role in providing valuable insight into factors that may contribute to or delay ADRD progression.   Employing a GNN approach with claims data enhances ADRD risk prediction and provides insights into the impact of interconnected medical code relationships. This methodology not only enables ADRD risk modeling but also shows potential for other image analysis predictions using claims data.
</details>
<details>
<summary>摘要</summary>
供应链疾病（ADRD）是美国第六大死因之一，因此精准预测ADRD风险的重要性。Recent Advances in ADRD风险预测主要基于成像分析，但不 все患者会在ADRD诊断之前进行成像检查。通过将机器学习与保险数据结合，可以揭示更多的风险因素和不同的医疗代码之间的联系。我们的目标是使用图ael Neural Networks（GNNs）与保险数据进行ADRD风险预测。为了解释预测结果中的人类可读性原因，我们引入了关系重要性评估方法。我们使用了Variational Regularized Encoder-decoder Graph Neural Network（VGNN）来估计ADRD可能性。我们创建了三个场景来评估模型的效率，使用Random Forest和Light Gradient Boost Machine作为基线。我们还使用我们的关系重要性评估方法来解释ADRD风险预测中的关键关系。VGNN比基线模型提高了10%的接收操作特征曲线地区 beneath。将GNN模型和关系重要性评估结合可能会在提供ADRD进程中的价值预测和解释方面发挥关键作用。使用GNN方法和保险数据可以提高ADRD风险预测，同时提供成像分析预测中的可能性。这种方法不仅可以用于ADRD风险预测，还可以应用于其他成像分析预测中。
</details></li>
</ul>
<hr>
<h2 id="Electron-Energy-Regression-in-the-CMS-High-Granularity-Calorimeter-Prototype"><a href="#Electron-Energy-Regression-in-the-CMS-High-Granularity-Calorimeter-Prototype" class="headerlink" title="Electron Energy Regression in the CMS High-Granularity Calorimeter Prototype"></a>Electron Energy Regression in the CMS High-Granularity Calorimeter Prototype</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06582">http://arxiv.org/abs/2309.06582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fair-umn/fair-umn-hgcal">https://github.com/fair-umn/fair-umn-hgcal</a></li>
<li>paper_authors: Roger Rusack, Bhargav Joshi, Alpana Alpana, Seema Sharma, Thomas Vadnais</li>
<li>for: 该论文为了探讨一种新型加速器中的净能计，并将其数据公开发布，以便促进机器学习专家利用该数据进行高精度的图像重建。</li>
<li>methods: 该论文使用了最新的机器学习技术来重建 incident electrons 的能量，从三维hit的能量中确定incident electrons的能量。</li>
<li>results: 该论文通过使用机器学习方法，成功地重建 incident electrons 的能量，并公开发布了这些数据，以便促进相关领域的研究和应用。<details>
<summary>Abstract</summary>
We present a new publicly available dataset that contains simulated data of a novel calorimeter to be installed at the CERN Large Hadron Collider. This detector will have more than six-million channels with each channel capable of position, ionisation and precision time measurement. Reconstructing these events in an efficient way poses an immense challenge which is being addressed with the latest machine learning techniques. As part of this development a large prototype with 12,000 channels was built and a beam of high-energy electrons incident on it. Using machine learning methods we have reconstructed the energy of incident electrons from the energies of three-dimensional hits, which is known to some precision. By releasing this data publicly we hope to encourage experts in the application of machine learning to develop efficient and accurate image reconstruction of these electrons.
</details>
<details>
<summary>摘要</summary>
我们现在公布了一个新的公共可用数据集，该数据集包含 simulate 的加速器中的一种新的冷却器，该冷却器将有超过六百万个通道，每个通道都可以测量位置、离子化和精度时间。重建这些事件的方式具有极大的挑战，我们正在使用最新的机器学习技术来解决这个问题。在这个开发过程中，我们建立了一个大型原型，该原型有12,000个通道，并使用高能电子束照射它。使用机器学习方法，我们已经从三维hit的能量中重建了入射电子的能量，这已知道一定的精度。我们通过公布这些数据希望能吸引专家在机器学习应用中发展高效和准确的图像重建技术。
</details></li>
</ul>
<hr>
<h2 id="Promises-of-Deep-Kernel-Learning-for-Control-Synthesis"><a href="#Promises-of-Deep-Kernel-Learning-for-Control-Synthesis" class="headerlink" title="Promises of Deep Kernel Learning for Control Synthesis"></a>Promises of Deep Kernel Learning for Control Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06569">http://arxiv.org/abs/2309.06569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Reed, Luca Laurenti, Morteza Lahijanian</li>
<li>for: 学习和控制复杂动力系统</li>
<li>methods: 使用深度kernel学习 combines with Gaussian Processes, 并使用抽象基础框架进行控制synthesis</li>
<li>results: 对各种准确性要求的 benchmark 进行了实验，显示了与现有竞争方法相比，控制synthesis with DKL 可以具有显著性能优势。<details>
<summary>Abstract</summary>
Deep Kernel Learning (DKL) combines the representational power of neural networks with the uncertainty quantification of Gaussian Processes. Hence, it is potentially a promising tool to learn and control complex dynamical systems. In this work, we develop a scalable abstraction-based framework that enables the use of DKL for control synthesis of stochastic dynamical systems against complex specifications. Specifically, we consider temporal logic specifications and create an end-to-end framework that uses DKL to learn an unknown system from data and formally abstracts the DKL model into an Interval Markov Decision Process (IMDP) to perform control synthesis with correctness guarantees. Furthermore, we identify a deep architecture that enables accurate learning and efficient abstraction computation. The effectiveness of our approach is illustrated on various benchmarks, including a 5-D nonlinear stochastic system, showing how control synthesis with DKL can substantially outperform state-of-the-art competitive methods.
</details>
<details>
<summary>摘要</summary>
深度kernel学习（DKL）结合神经网络的表达能力和高斯过程的不确定性评估，因此它可能是控制复杂动态系统的有望工具。在这项工作中，我们开发了可扩展的抽象基础框架，使得使用DKL进行动态系统的控制合成 against complex specs 可能。特别是，我们考虑了时间逻辑特性规定，并创建了一个终端框架，使用DKL来从数据中学习未知系统，并正确地抽象DKL模型为一个Interval Markov Decision Process（IMDP）来进行控制合成。此外，我们确定了一种深度架构，使得准确地学习和高效地抽象计算。我们的方法的有效性被证明在多个标准准例上，包括一个5D非线性随机系统，显示了使用DKL进行控制合成可以大幅超越现有竞争方法。
</details></li>
</ul>
<hr>
<h2 id="MELAGE-A-purely-python-based-Neuroimaging-software-Neonatal"><a href="#MELAGE-A-purely-python-based-Neuroimaging-software-Neonatal" class="headerlink" title="MELAGE: A purely python based Neuroimaging software (Neonatal)"></a>MELAGE: A purely python based Neuroimaging software (Neonatal)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07175">http://arxiv.org/abs/2309.07175</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bahramjafrasteh/melage">https://github.com/bahramjafrasteh/melage</a></li>
<li>paper_authors: Bahram Jafrasteh, Simón Pedro Lubián López, Isabel Benavente Fernández</li>
<li>for: 这篇论文主要用于介绍MELAGE软件，一种基于Python的神经成像软件，可以用于处理和分析医疗图像。</li>
<li>methods: MELAGE软件使用了深度学习模块和自动化大脑EXTRACTION工具，可以快速和精确地提取大脑结构信息从MRI和3D超声数据中。</li>
<li>results: MELAGE软件可以快速处理和分析医疗图像，并且具有多种功能，如动态3D视化、准确的测量和交互式图像分割。这个软件在医学成像领域中具有广泛的应用前景和潜在的推广前景。<details>
<summary>Abstract</summary>
MELAGE, a pioneering Python-based neuroimaging software, emerges as a versatile tool for the visualization, processing, and analysis of medical images. Initially conceived to address the unique challenges of processing 3D ultrasound and MRI brain images during the neonatal period, MELAGE exhibits remarkable adaptability, extending its utility to the domain of adult human brain imaging. At its core, MELAGE features a semi-automatic brain extraction tool empowered by a deep learning module, ensuring precise and efficient brain structure extraction from MRI and 3D Ultrasound data. Moreover, MELAGE offers a comprehensive suite of features, encompassing dynamic 3D visualization, accurate measurements, and interactive image segmentation. This transformative software holds immense promise for researchers and clinicians, offering streamlined image analysis, seamless integration with deep learning algorithms, and broad applicability in the realm of medical imaging.
</details>
<details>
<summary>摘要</summary>
美laps, a pioneering Python-based neuroimaging software, emerges as a versatile tool for the visualization, processing, and analysis of medical images. Initially conceived to address the unique challenges of processing 3D ultrasound and MRI brain images during the neonatal period, MELAGE exhibits remarkable adaptability, extending its utility to the domain of adult human brain imaging. At its core, MELAGE features a semi-automatic brain extraction tool empowered by a deep learning module, ensuring precise and efficient brain structure extraction from MRI and 3D Ultrasound data. Moreover, MELAGE offers a comprehensive suite of features, encompassing dynamic 3D visualization, accurate measurements, and interactive image segmentation. This transformative software holds immense promise for researchers and clinicians, offering streamlined image analysis, seamless integration with deep learning algorithms, and broad applicability in the realm of medical imaging.Here's the text with some additional information about the Simplified Chinese translation:The Simplified Chinese translation of the text uses the traditional Chinese characters for "MELAGE" (美laps), which is the pinyin Romanization of the name. The translation is written in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.In the translation, I have used the phrase "美laps" to refer to the software, as this is the name that is commonly used in the field of neuroimaging. I have also used the phrase " neural imaging" (神经成像) to refer to the broader field of medical imaging, as this is the term that is commonly used in China.Additionally, I have used the phrase " semi-automatic brain extraction tool" (半自动的脑EXTRACTION工具) to refer to the software's ability to automatically extract brain structures from medical images. I have also used the phrase " deep learning module" (深度学习模块) to refer to the software's use of machine learning algorithms to improve its performance.Overall, the translation is written in a formal and technical style, using language that is appropriate for a scientific or medical audience.
</details></li>
</ul>
<hr>
<h2 id="Commands-as-AI-Conversations"><a href="#Commands-as-AI-Conversations" class="headerlink" title="Commands as AI Conversations"></a>Commands as AI Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06551">http://arxiv.org/abs/2309.06551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dspinellis/ai-cli">https://github.com/dspinellis/ai-cli</a></li>
<li>paper_authors: Diomidis Spinellis</li>
<li>for: 提高开发者和数据科学家在命令行中输入数据的效率，使命令行界面更加智能和用户友好。</li>
<li>methods: 使用OpenAI的API，通过JSON HTTP请求进行交互，将自然语言提示转换为可执行的命令行指令。</li>
<li>results: 使命令行界面更加智能和用户友好，开拓了进一步完善和跨平台应用的可能性。<details>
<summary>Abstract</summary>
Developers and data scientists often struggle to write command-line inputs, even though graphical interfaces or tools like ChatGPT can assist. The solution? "ai-cli," an open-source system inspired by GitHub Copilot that converts natural language prompts into executable commands for various Linux command-line tools. By tapping into OpenAI's API, which allows interaction through JSON HTTP requests, "ai-cli" transforms user queries into actionable command-line instructions. However, integrating AI assistance across multiple command-line tools, especially in open source settings, can be complex. Historically, operating systems could mediate, but individual tool functionality and the lack of a unified approach have made centralized integration challenging. The "ai-cli" tool, by bridging this gap through dynamic loading and linking with each program's Readline library API, makes command-line interfaces smarter and more user-friendly, opening avenues for further enhancement and cross-platform applicability.
</details>
<details>
<summary>摘要</summary>
开发者和数据科学家经常遇到 command-line 输入的问题，即使使用图形化界面或 ChatGPT 等工具。解决方案是“ ai-cli”，一个基于 GitHub Copilot 的开源系统，可以将自然语言提示转换成多种 Linux 命令行工具的执行命令。通过使用 OpenAI 的 API，可以通过 JSON HTTP 请求进行交互，将用户提问转换成可执行的命令行指令。然而，在多个命令行工具之间集成 AI 帮助，特别是在开源环境下，可能会具有复杂性。历史上，操作系统可以作为中介，但每个工具的功能和缺乏一致的方法使得中央集成变得困难。“ ai-cli” 工具通过在每个程序的 Readline 库 API 上进行动态加载和链接，使命令行界面变得更加智能和用户友好，打开了进一步改进和跨平台应用的可能性。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Transfer-Learning"><a href="#Distributionally-Robust-Transfer-Learning" class="headerlink" title="Distributionally Robust Transfer Learning"></a>Distributionally Robust Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06534">http://arxiv.org/abs/2309.06534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rvr-account/rvr">https://github.com/rvr-account/rvr</a></li>
<li>paper_authors: Xin Xiong, Zijian Guo, Tianxi Cai</li>
<li>for: 这篇论文是针对对转移学习领域的研究，具体目的是提出一种名为分布robust优化的转移学习方法（TransDRO），可以突破传统对目标数据的相似性限制，实现更好的预测性能。</li>
<li>methods: 这篇论文使用了分布robust优化的方法，具体来说是在一个不确定集中优化最大不确定损失，这个不确定集是通过将多个来源分布混合而成的，并且保证预测性能优化。</li>
<li>results: 论文通过了复杂的数据分析和实验研究，证明了TransDRO的可靠性和精准性，并且比传统转移学习方法更快速地调整模型。<details>
<summary>Abstract</summary>
Many existing transfer learning methods rely on leveraging information from source data that closely resembles the target data. However, this approach often overlooks valuable knowledge that may be present in different yet potentially related auxiliary samples. When dealing with a limited amount of target data and a diverse range of source models, our paper introduces a novel approach, Distributionally Robust Optimization for Transfer Learning (TransDRO), that breaks free from strict similarity constraints. TransDRO is designed to optimize the most adversarial loss within an uncertainty set, defined as a collection of target populations generated as a convex combination of source distributions that guarantee excellent prediction performances for the target data. TransDRO effectively bridges the realms of transfer learning and distributional robustness prediction models. We establish the identifiability of TransDRO and its interpretation as a weighted average of source models closest to the baseline model. We also show that TransDRO achieves a faster convergence rate than the model fitted with the target data. Our comprehensive numerical studies and analysis of multi-institutional electronic health records data using TransDRO further substantiate the robustness and accuracy of TransDRO, highlighting its potential as a powerful tool in transfer learning applications.
</details>
<details>
<summary>摘要</summary>
许多现有的转移学习方法倾向于利用源数据中与目标数据相似的信息。然而，这种方法经常忽视了可能存在的不同 yet 相关的辅助样本中的有价值知识。当面临有限的目标数据和多种源模型时，我们的论文引入了一种新的方法：分布robust优化 для转移学习（TransDRO）。TransDRO是设计用于最大化内部的 adversarial 损失，而不是仅仅是依靠源数据的相似性。TransDRO 可以减轻转移学习中的压力，并帮助模型更好地适应不同的目标数据。我们证明了TransDRO的可识别性和其作为基准模型 closest 的weighted 平均的解释。此外，我们还证明了TransDRO在比目标数据更快地收敛的特点。我们的全面的数学研究和多所机构电子医疗记录数据使用TransDRO进行了详细的分析，证明了TransDRO在转移学习应用中的可靠性和准确性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Benefits-of-Differentially-Private-Pre-training-and-Parameter-Efficient-Fine-tuning-for-Table-Transformers"><a href="#Exploring-the-Benefits-of-Differentially-Private-Pre-training-and-Parameter-Efficient-Fine-tuning-for-Table-Transformers" class="headerlink" title="Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers"></a>Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06526">http://arxiv.org/abs/2309.06526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/dp-tabtransformer">https://github.com/ibm/dp-tabtransformer</a></li>
<li>paper_authors: Xilong Wang, Chia-Mu Yu, Pin-Yu Chen</li>
<li>for: 本研究探讨了将 differential privacy (DP) 和 Table Transformer (TabTransformer) 结合使用，以实现在转移学习中的数据隐私保护和模型效果优化。</li>
<li>methods: 本研究使用了多种参数效率优化（PEFT）方法，包括 Adapter、LoRA 和 Prompt Tuning，以进行增强的预训练和精度调整。</li>
<li>results: 实验结果表明，使用 PEFT 方法可以在 ACSIncome 数据集上提高下游任务的准确率和数据隐私保护，同时具有更好的参数效率、隐私和准确率之间的平衡。 codes 可以在 github.com&#x2F;IBM&#x2F;DP-TabTransformer 上下载。<details>
<summary>Abstract</summary>
For machine learning with tabular data, Table Transformer (TabTransformer) is a state-of-the-art neural network model, while Differential Privacy (DP) is an essential component to ensure data privacy. In this paper, we explore the benefits of combining these two aspects together in the scenario of transfer learning -- differentially private pre-training and fine-tuning of TabTransformers with a variety of parameter-efficient fine-tuning (PEFT) methods, including Adapter, LoRA, and Prompt Tuning. Our extensive experiments on the ACSIncome dataset show that these PEFT methods outperform traditional approaches in terms of the accuracy of the downstream task and the number of trainable parameters, thus achieving an improved trade-off among parameter efficiency, privacy, and accuracy. Our code is available at github.com/IBM/DP-TabTransformer.
</details>
<details>
<summary>摘要</summary>
为机器学习 tabular 数据，表Transformer（TabTransformer）是现代神经网络模型的state-of-the-art，而数据隐私（DP）则是保证数据隐私的必要组成部分。在这篇论文中，我们探讨将这两个方面结合在一起的场景——即具有数据隐私的预训练和精度调整 TabTransformers 的方法，包括 Adapter、LoRA 和 Prompt Tuning。我们在 ACSIncome 数据集进行了广泛的实验，发现这些 PEFT 方法在下游任务的准确率和可训练参数数量方面都高于传统方法，从而实现了参数效率、隐私和准确率之间的改善的平衡。我们的代码可以在github.com/IBM/DP-TabTransformer 找到。
</details></li>
</ul>
<hr>
<h2 id="A-Q-learning-Approach-for-Adherence-Aware-Recommendations"><a href="#A-Q-learning-Approach-for-Adherence-Aware-Recommendations" class="headerlink" title="A Q-learning Approach for Adherence-Aware Recommendations"></a>A Q-learning Approach for Adherence-Aware Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06519">http://arxiv.org/abs/2309.06519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Faros, Aditya Dave, Andreas A. Malikopoulos</li>
<li>for: 这种情况下，人工智能提供的建议将被人类决策者（HDM）作为最终决策者接受或拒绝。</li>
<li>methods: 我们开发了一种“遵循度感知Q学习”算法，该算法学习HDМ的遵循度水平，并在实时更新最佳建议策略。</li>
<li>results: 我们证明了我们提出的Q学习算法 converges to the optimal value，并评估了它在多种情况下的性能。<details>
<summary>Abstract</summary>
In many real-world scenarios involving high-stakes and safety implications, a human decision-maker (HDM) may receive recommendations from an artificial intelligence while holding the ultimate responsibility of making decisions. In this letter, we develop an "adherence-aware Q-learning" algorithm to address this problem. The algorithm learns the "adherence level" that captures the frequency with which an HDM follows the recommended actions and derives the best recommendation policy in real time. We prove the convergence of the proposed Q-learning algorithm to the optimal value and evaluate its performance across various scenarios.
</details>
<details>
<summary>摘要</summary>
在许多高风险高安全性的实际场景中，人工智能推荐（HDM）可能会接收人工智能推荐的建议，而最终负责做出决策。在这封信中，我们开发了一种“遵循程度意识Q学习”算法，以解决这个问题。该算法学习“遵循程度”， capture推荐行为的频率，并在实时 derivates最佳推荐策略。我们证明算法 converges to 优化值，并评估其性能在多种场景中。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-longitudinal-tensor-response-regression-for-modeling-neuroplasticity"><a href="#Bayesian-longitudinal-tensor-response-regression-for-modeling-neuroplasticity" class="headerlink" title="Bayesian longitudinal tensor response regression for modeling neuroplasticity"></a>Bayesian longitudinal tensor response regression for modeling neuroplasticity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10065">http://arxiv.org/abs/2309.10065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suprateek Kundu, Alec Reinhardt, Serena Song, M. Lawson Meadows, Bruce Crosson, Venkatagiri Krishnamurthy</li>
<li>For: 这个论文的主要目标是 investigate longitudinal neuroimaging 数据中 voxel 级别的 neural plasticity，并且使用 Bayesian tensor response regression 方法来做这种研究。* Methods: 这个方法使用 Markov chain Monte Carlo (MCMC) 采样来实现，并使用 low-rank decomposition 来降维并保持 voxel 的空间配置。它还可以通过联合可信区间来进行特征选择，以更准确地进行推断。* Results: 这个方法可以在 group 级别和个体级别进行推断，并且可以检测到不同干扰因素对 brain 活动的影响。在应用于一个 longitudinal aphasia 数据集上，这个方法发现，对于 control 治疗，brain 活动在长期内增加，而对于 intention treatment，brain 活动在短期内增加，两者都集中在特定的本地化区域。相比之下，voxel-wise regression 无法检测到任何 significannot neuroplasticity  после多重性调整，这是生物学上不可能的和表明缺乏力。<details>
<summary>Abstract</summary>
A major interest in longitudinal neuroimaging studies involves investigating voxel-level neuroplasticity due to treatment and other factors across visits. However, traditional voxel-wise methods are beset with several pitfalls, which can compromise the accuracy of these approaches. We propose a novel Bayesian tensor response regression approach for longitudinal imaging data, which pools information across spatially-distributed voxels to infer significant changes while adjusting for covariates. The proposed method, which is implemented using Markov chain Monte Carlo (MCMC) sampling, utilizes low-rank decomposition to reduce dimensionality and preserve spatial configurations of voxels when estimating coefficients. It also enables feature selection via joint credible regions which respect the shape of the posterior distributions for more accurate inference. In addition to group level inferences, the method is able to infer individual-level neuroplasticity, allowing for examination of personalized disease or recovery trajectories. The advantages of the proposed approach in terms of prediction and feature selection over voxel-wise regression are highlighted via extensive simulation studies. Subsequently, we apply the approach to a longitudinal Aphasia dataset consisting of task functional MRI images from a group of subjects who were administered either a control intervention or intention treatment at baseline and were followed up over subsequent visits. Our analysis revealed that while the control therapy showed long-term increases in brain activity, the intention treatment produced predominantly short-term changes, both of which were concentrated in distinct localized regions. In contrast, the voxel-wise regression failed to detect any significant neuroplasticity after multiplicity adjustments, which is biologically implausible and implies lack of power.
</details>
<details>
<summary>摘要</summary>
一个主要兴趣点在长itudinal神经成像研究是调查征量级的神经重塑，因为不同因素的影响。然而，传统的征量级方法存在多种困难，可能会降低准确性。我们提出了一种新的 bayesian tensor response regression方法，用于长itudinal神经成像数据，该方法将信息归一化到空间分布的壳体上，以确定变化的主要因素，并对 covariates 进行补做。我们使用 markov chain Monte Carlo (MCMC) 采样实现该方法，并使用低级别分解来降低维度并保持壳体中各个壳体的空间配置。此外，该方法还允许功能选择，通过共同可信区域来更准确地进行推理。除了群体水平的推理，该方法还可以进行个体水平的神经重塑推理，以检测个体化的疾病或恢复轨迹。我们通过了EXTENSIVE 仪器实验来比较我们的方法与征量级 regression 的优势，并应用到一个长itudinal 语言障碍数据集，该数据集包含一组主动或接受治疗的 subjects 的任务功能 MRI 图像，从baseline开始，并在后续访问中进行追踪。我们的分析发现，控制疗法在长期内表现出增加的脑活动，而意图治疗则在短期内产生主要的变化，这些变化集中在特定的本地化区域。相比之下，征量级 regression 无法检测任何的神经重塑，这是生物学上不可能的，并 implies 缺乏力量。
</details></li>
</ul>
<hr>
<h2 id="A-Distributed-Data-Parallel-PyTorch-Implementation-of-the-Distributed-Shampoo-Optimizer-for-Training-Neural-Networks-At-Scale"><a href="#A-Distributed-Data-Parallel-PyTorch-Implementation-of-the-Distributed-Shampoo-Optimizer-for-Training-Neural-Networks-At-Scale" class="headerlink" title="A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale"></a>A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06497">http://arxiv.org/abs/2309.06497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/optimizers/tree/main/distributed_shampoo">https://github.com/facebookresearch/optimizers/tree/main/distributed_shampoo</a></li>
<li>paper_authors: Hao-Jun Michael Shi, Tsung-Hsien Lee, Shintaro Iwasaki, Jose Gallego-Posada, Zhijing Li, Kaushik Rangadurai, Dheevatsa Mudigere, Michael Rabbat</li>
<li>For: 这篇论文主要探讨了一种基于AdaGrad家族的在线泛化优化算法Shampoo，用于训练神经网络。* Methods: 该算法使用了块对角矩阵预conditioner，其中每个块包含一个粗略 Kronecker产品方法来对神经网络每个参数进行AdaGrad优化。* Results: 作者提供了一个完整的算法描述以及实现方法，并通过对ImageNet ResNet50进行ablation研究，证明了Shampoo比标准的 diagonally-scaling-based adaptive gradient方法具有更好的性能。<details>
<summary>Abstract</summary>
Shampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods for training neural networks. It constructs a block-diagonal preconditioner where each block consists of a coarse Kronecker product approximation to full-matrix AdaGrad for each parameter of the neural network. In this work, we provide a complete description of the algorithm as well as the performance optimizations that our implementation leverages to train deep networks at-scale in PyTorch. Our implementation enables fast multi-GPU distributed data-parallel training by distributing the memory and computation associated with blocks of each parameter via PyTorch's DTensor data structure and performing an AllGather primitive on the computed search directions at each iteration. This major performance enhancement enables us to achieve at most a 10% performance reduction in per-step wall-clock time compared against standard diagonal-scaling-based adaptive gradient methods. We validate our implementation by performing an ablation study on training ImageNet ResNet50, demonstrating Shampoo's superiority over standard training recipes with minimal hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>首先，我们需要介绍一下Shampoo算法。Shampoo是一种在线和随机优化算法，属于AdaGrad家族的方法，用于训练神经网络。它构建了一个块对角预conditioner，每个块包含一个粗糙的Kronecker产品approxiamtion来对神经网络每个参数进行AdaGrad优化。在这篇文章中，我们将提供Shampoo算法的完整描述，以及我们的实现方法来使用PyTorch框架进行高效的多GPU分布式数据并行训练。我们的实现方法包括将每个参数的块分配到不同的GPU上，并使用PyTorch的DTensor数据结构进行分布式计算。在每次迭代中，我们使用AllGather primitives来合并所有GPU上的计算结果。这种主要的性能优化方法使得我们可以在每步wall-clock时间中减少训练时间的最大值为10%，相比标准的对 diagonalfactor-scaling 的 adaptive gradient方法。我们验证了我们的实现方法，通过对ImageNet ResNet50进行减少学习率的研究，并证明Shampoo的superiority。<<SYS>>
</details></li>
</ul>
<hr>
<h2 id="Learning-topological-operations-on-meshes-with-application-to-block-decomposition-of-polygons"><a href="#Learning-topological-operations-on-meshes-with-application-to-block-decomposition-of-polygons" class="headerlink" title="Learning topological operations on meshes with application to block decomposition of polygons"></a>Learning topological operations on meshes with application to block decomposition of polygons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06484">http://arxiv.org/abs/2309.06484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Narayanan, Yulong Pan, Per-Olof Persson</li>
<li>for: 提高不结构化三角形和四角形网格质量</li>
<li>methods: 使用自适应学习游戏 reinforcement learning，无需先知识策略，通过标准本地和全局元素操作进行网格优化</li>
<li>results: 能够有效地减少节点度差与理想值的差异，即内部顶点的异常节点数量减少<details>
<summary>Abstract</summary>
We present a learning based framework for mesh quality improvement on unstructured triangular and quadrilateral meshes. Our model learns to improve mesh quality according to a prescribed objective function purely via self-play reinforcement learning with no prior heuristics. The actions performed on the mesh are standard local and global element operations. The goal is to minimize the deviation of the node degrees from their ideal values, which in the case of interior vertices leads to a minimization of irregular nodes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于学习的框架，用于改善无结构三角形和四边形网格的质量。我们的模型通过自动化反射学习，不含任何先验知识，来改善网格质量 according to a prescribed objective function。操作的方法包括标准的本地和全局元素操作。目标是将节点度 deviation from their ideal values as minimal as possible，即在内部顶点情况下 minimize irregular nodes。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Flows-for-Flows-Morphing-one-Dataset-into-another-with-Maximum-Likelihood-Estimation"><a href="#Flows-for-Flows-Morphing-one-Dataset-into-another-with-Maximum-Likelihood-Estimation" class="headerlink" title="Flows for Flows: Morphing one Dataset into another with Maximum Likelihood Estimation"></a>Flows for Flows: Morphing one Dataset into another with Maximum Likelihood Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06472">http://arxiv.org/abs/2309.06472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Golling, Samuel Klein, Radha Mastandrea, Benjamin Nachman, John Andrew Raine</li>
<li>for: 本研究旨在解决高能物理和其他领域中数据分析中的数据变换问题，通过将一个数据集转换成另一个数据集而不需要知道起始数据集的概率密度。</li>
<li>methods: 本研究使用了normalizing flows机器学习模型，该模型具有在多种 particle physics 任务中的出色精度。然而，normalizing flows 模型需要知道起始数据集的概率密度，而在大多数情况下，我们只能生成更多的例子，但不知道densities explicitly。为解决这个问题，我们提出了一种叫做“flows for flows”的协议，该协议可以让normalizing flows 模型在不知道起始数据集的概率密度情况下进行数据变换。</li>
<li>results: 我们研究了这种协议的多种变种，以explore如何使用normalizing flows 模型来 Statistically match two datasets。此外，我们还示了如何使用conditioning feature来创建一个基于特定特征的 morphing function，以便为每个特征值创建一个 morphing function。我们用了 Toy examples 和 collider physics 示例来 ilustrate our results。<details>
<summary>Abstract</summary>
Many components of data analysis in high energy physics and beyond require morphing one dataset into another. This is commonly solved via reweighting, but there are many advantages of preserving weights and shifting the data points instead. Normalizing flows are machine learning models with impressive precision on a variety of particle physics tasks. Naively, normalizing flows cannot be used for morphing because they require knowledge of the probability density of the starting dataset. In most cases in particle physics, we can generate more examples, but we do not know densities explicitly. We propose a protocol called flows for flows for training normalizing flows to morph one dataset into another even if the underlying probability density of neither dataset is known explicitly. This enables a morphing strategy trained with maximum likelihood estimation, a setup that has been shown to be highly effective in related tasks. We study variations on this protocol to explore how far the data points are moved to statistically match the two datasets. Furthermore, we show how to condition the learned flows on particular features in order to create a morphing function for every value of the conditioning feature. For illustration, we demonstrate flows for flows for toy examples as well as a collider physics example involving dijet events
</details>
<details>
<summary>摘要</summary>
很多高能物理数据分析中的组件需要将一个数据集转换成另一个数据集。通常通过重新权重来解决这个问题，但是有很多优点在保持权重并将数据点移动。正则化流是一种机器学习模型，它在多种粒子物理任务上具有出色的精度。然而，正则化流无法用于 morphing，因为它们需要知道开始数据集的概率密度。在大多数粒子物理任务中，我们可以生成更多的例子，但我们不知道概率密度的准确值。我们提出了一个协议called flows for flows，用于在不知道开始数据集的概率密度情况下，通过最大likelihood估计来训练正则化流。这种设置可以在相关任务中展示出非常高效。我们还研究了这个协议的变种，以探索如何在数据点之间移动的距离，以达到两个数据集的统计匹配。此外，我们还示出了如何基于特定的特征来创建一个适应于每个特征的 morphing 函数。为了说明，我们对假示例和一个撞击物理示例进行了演示。
</details></li>
</ul>
<hr>
<h2 id="On-Computationally-Efficient-Learning-of-Exponential-Family-Distributions"><a href="#On-Computationally-Efficient-Learning-of-Exponential-Family-Distributions" class="headerlink" title="On Computationally Efficient Learning of Exponential Family Distributions"></a>On Computationally Efficient Learning of Exponential Family Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06413">http://arxiv.org/abs/2309.06413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhin Shah, Devavrat Shah, Gregory W. Wornell</li>
<li>for: 学习 truncated  exponential family 的自然参数，即使参数数量很大，可以在计算上和统计上减少计算复杂度。</li>
<li>methods: 提出了一种新的损失函数和计算效率高的估计方法，该方法可以在轻量级上实现consistent和asymptotically normal的性质，并且可以视为maximum likelihood estimation的一种变形。</li>
<li>results: 提供了finite sample guarantees，表明该估计方法可以在样本数量为$O({\sf poly}(k)&#x2F;\alpha^2)$下 Achieves an error (in $\ell_2$-norm) of $\alpha$ in the parameter estimation，并且在特定的Markov random fields上实现order-optimal的sample complexity $O({\sf log}(k)&#x2F;\alpha^2)$。<details>
<summary>Abstract</summary>
We consider the classical problem of learning, with arbitrary accuracy, the natural parameters of a $k$-parameter truncated \textit{minimal} exponential family from i.i.d. samples in a computationally and statistically efficient manner. We focus on the setting where the support as well as the natural parameters are appropriately bounded. While the traditional maximum likelihood estimator for this class of exponential family is consistent, asymptotically normal, and asymptotically efficient, evaluating it is computationally hard. In this work, we propose a novel loss function and a computationally efficient estimator that is consistent as well as asymptotically normal under mild conditions. We show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re-parameterized distribution belonging to the same class of exponential family. Further, we show that our estimator can be interpreted as a solution to minimizing a particular Bregman score as well as an instance of minimizing the \textit{surrogate} likelihood. We also provide finite sample guarantees to achieve an error (in $\ell_2$-norm) of $\alpha$ in the parameter estimation with sample complexity $O({\sf poly}(k)/\alpha^2)$. Our method achives the order-optimal sample complexity of $O({\sf log}(k)/\alpha^2)$ when tailored for node-wise-sparse Markov random fields. Finally, we demonstrate the performance of our estimator via numerical experiments.
</details>
<details>
<summary>摘要</summary>
我们考虑了经典的学习问题，即用任意精度学习 naturalk parameter 的 $k$-parameter简化 exponential family 的 i.i.d. 样本。我们关注在支持和自然参数均有限制的设置下。传统的最大 LIKElihood estimator 是一个可靠、 asymptotically normal 和 asymptotically efficient 的 estimator，但是计算困难。在这种工作中，我们提出了一个新的损失函数和一个 computationally efficient 的 estimator，该 estimator 是一个可靠的 estimator ，并且在某些条件下具有 asymptotically normal 性。我们证明了，在人口水平，我们的方法可以视为一个 maximum likelihood estimation 的 re-parameterized distribution 的一个例子，这个 distribution 属于同一个类型的 exponential family。此外，我们还证明了我们的 estimator 可以被视为一个 minimize 的 Bregman score 和一个 surrogate likelihood 的解。我们还提供了 finite sample guarantees，可以在 $\ell_2$-norm 内达到 $\alpha$ 的错误水平， sample complexity 为 $O({\sf poly}(k)/\alpha^2)$。当我们特意适应 node-wise-sparse Markov random fields 时，我们的方法可以 дости到 order-optimal 的 sample complexity $O({\sf log}(k)/\alpha^2)$。最后，我们通过数值实验证明了我们的 estimator 的性能。
</details></li>
</ul>
<hr>
<h2 id="Using-Reed-Muller-Codes-for-Classification-with-Rejection-and-Recovery"><a href="#Using-Reed-Muller-Codes-for-Classification-with-Rejection-and-Recovery" class="headerlink" title="Using Reed-Muller Codes for Classification with Rejection and Recovery"></a>Using Reed-Muller Codes for Classification with Rejection and Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06359">http://arxiv.org/abs/2309.06359</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dfenth/rmaggnet">https://github.com/dfenth/rmaggnet</a></li>
<li>paper_authors: Daniel Fentham, David Parker, Mark Ryan</li>
<li>for: 防止攻击者通过生成难以分类的图像来诱导分类器错误输出。</li>
<li>methods: 基于Reed-Muller错误校正码的Aggregation Networks（RMAggNet），能够在多种攻击下 corrections 和拒绝输入。</li>
<li>results: RMAggNet可以减少错误率，同时保持good correctness，并且可以在不同的攻击budget下进行多种攻击。<details>
<summary>Abstract</summary>
When deploying classifiers in the real world, users expect them to respond to inputs appropriately. However, traditional classifiers are not equipped to handle inputs which lie far from the distribution they were trained on. Malicious actors can exploit this defect by making adversarial perturbations designed to cause the classifier to give an incorrect output. Classification-with-rejection methods attempt to solve this problem by allowing networks to refuse to classify an input in which they have low confidence. This works well for strongly adversarial examples, but also leads to the rejection of weakly perturbed images, which intuitively could be correctly classified. To address these issues, we propose Reed-Muller Aggregation Networks (RMAggNet), a classifier inspired by Reed-Muller error-correction codes which can correct and reject inputs. This paper shows that RMAggNet can minimise incorrectness while maintaining good correctness over multiple adversarial attacks at different perturbation budgets by leveraging the ability to correct errors in the classification process. This provides an alternative classification-with-rejection method which can reduce the amount of additional processing in situations where a small number of incorrect classifications are permissible.
</details>
<details>
<summary>摘要</summary>
traditional classifiers 传统的分类器adversarial perturbations 攻击性的偏移classification-with-rejection methods 分类与拒绝方法Reed-Muller error-correction codes 里德-迈尔Error correction codesRMAggNet 重元聚合网络incorrectness 错误性good correctness 好的正确性adversarial attacks 攻击性的攻击perturbation budgets 偏移预算additional processing 额外处理
</details></li>
</ul>
<hr>
<h2 id="Generalized-Regret-Analysis-of-Thompson-Sampling-using-Fractional-Posteriors"><a href="#Generalized-Regret-Analysis-of-Thompson-Sampling-using-Fractional-Posteriors" class="headerlink" title="Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors"></a>Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06349">http://arxiv.org/abs/2309.06349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Jaiswal, Debdeep Pati, Anirban Bhattacharya, Bani K. Mallick</li>
<li>For: The paper is written to improve the performance of Thompson Sampling (TS) algorithm in stochastic multi-armed bandit problems by using a fractional posterior distribution.* Methods: The paper proposes an variant of TS called $\alpha$-TS, which uses a fractional or $\alpha$-posterior instead of the standard posterior distribution. The paper also provides regret bounds for $\alpha$-TS using recent theoretical developments in non-asymptotic concentration analysis and Bernstein-von Mises type results.* Results: The paper obtains instance-dependent and instance-independent regret bounds for $\alpha$-TS, which are better than those of standard TS. Specifically, the instance-dependent bound is $\mathcal{O}\left(\sum_{k \neq i^*} \Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$ and the instance-independent bound is $\mathcal{O}(\sqrt{KT\log K})$. The paper also matches the performance of improved UCB algorithm.<details>
<summary>Abstract</summary>
Thompson sampling (TS) is one of the most popular and earliest algorithms to solve stochastic multi-armed bandit problems. We consider a variant of TS, named $\alpha$-TS, where we use a fractional or $\alpha$-posterior ($\alpha\in(0,1)$) instead of the standard posterior distribution. To compute an $\alpha$-posterior, the likelihood in the definition of the standard posterior is tempered with a factor $\alpha$. For $\alpha$-TS we obtain both instance-dependent $\mathcal{O}\left(\sum_{k \neq i^*} \Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$ and instance-independent $\mathcal{O}(\sqrt{KT\log K})$ frequentist regret bounds under very mild conditions on the prior and reward distributions, where $\Delta_k$ is the gap between the true mean rewards of the $k^{th}$ and the best arms, and $C(\alpha)$ is a known constant. Both the sub-Gaussian and exponential family models satisfy our general conditions on the reward distribution. Our conditions on the prior distribution just require its density to be positive, continuous, and bounded. We also establish another instance-dependent regret upper bound that matches (up to constants) to that of improved UCB [Auer and Ortner, 2010]. Our regret analysis carefully combines recent theoretical developments in the non-asymptotic concentration analysis and Bernstein-von Mises type results for the $\alpha$-posterior distribution. Moreover, our analysis does not require additional structural properties such as closed-form posteriors or conjugate priors.
</details>
<details>
<summary>摘要</summary>
汤姆采样（TS）是最受欢迎并且最早的多重护卫机器人问题的算法之一。我们考虑了一种变体called $\alpha$-TS，其中使用一个分数或$\alpha$-后验（$\alpha\in(0,1)$）而不是标准后验分布。为计算一个$\alpha$-后验，在标准后验定义中的概率被温和了一个因子$\alpha$。对于$\alpha$-TS，我们获得了两种不同的频见 regret bounds：一种是 $\mathcal{O}\left(\sum_{k \neq i^*} \Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$，另一种是 $\mathcal{O}(\sqrt{KT\log K})$，其中 $\Delta_k$ 是真实奖励的 $k$ 个和最佳武器的差距，$C(\alpha)$ 是已知的常量。两者都满足非常轻量级的前提和奖励分布。我们的前提只需要其概率密度是正、连续和有界的。我们还提出了另一种与改进的 UCB（Auer 和 Ortner, 2010）的 regret upper bound，其中的 regret  bound 与（除了常量外）相同。我们的 regret 分析结合了最近的非 asymptotic 归一化分析和 Bernstein-von Mises 类型的 $\alpha$- posterior 分布结果。此外，我们的分析不需要额外的结构性质，例如闭合形 posterior 或 conjugate priors。
</details></li>
</ul>
<hr>
<h2 id="Band-gap-regression-with-architecture-optimized-message-passing-neural-networks"><a href="#Band-gap-regression-with-architecture-optimized-message-passing-neural-networks" class="headerlink" title="Band-gap regression with architecture-optimized message-passing neural networks"></a>Band-gap regression with architecture-optimized message-passing neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06348">http://arxiv.org/abs/2309.06348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tisabe/jraph_mpeu">https://github.com/tisabe/jraph_mpeu</a></li>
<li>paper_authors: Tim Bechtel, Daniel T. Speckhard, Jonathan Godwin, Claudia Draxl</li>
<li>for: 本研究使用图结构神经网络（MPNN）预测固体物质的物理性质。</li>
<li>methods: 本研究使用密集 functional theory数据从AFLOW数据库进行分类材料为半导体&#x2F;隔离体或金属。然后，通过神经建筑搜索来探索MPNN模型的建筑和超参空间，以预测材料标记为非金属的带隔。</li>
<li>results: 搜索出的最佳模型组成一个ensemble，与现有文献中的模型相比显著提高了性能。不确定性评估使用Monte Carlo Dropout和集成方法，集成方法更为成功。对适用范围进行分析，包括晶系、包括Hubbard参数在内的密集函数计算、以及材料中原子species。<details>
<summary>Abstract</summary>
Graph-based neural networks and, specifically, message-passing neural networks (MPNNs) have shown great potential in predicting physical properties of solids. In this work, we train an MPNN to first classify materials through density functional theory data from the AFLOW database as being metallic or semiconducting/insulating. We then perform a neural-architecture search to explore the model architecture and hyperparameter space of MPNNs to predict the band gaps of the materials identified as non-metals. The parameters in the search include the number of message-passing steps, latent size, and activation-function, among others. The top-performing models from the search are pooled into an ensemble that significantly outperforms existing models from the literature. Uncertainty quantification is evaluated with Monte-Carlo Dropout and ensembling, with the ensemble method proving superior. The domain of applicability of the ensemble model is analyzed with respect to the crystal systems, the inclusion of a Hubbard parameter in the density functional calculations, and the atomic species building up the materials.
</details>
<details>
<summary>摘要</summary>
GRaph-based neural networks和具体地是消息传递神经网络（MPNNs）在预测固体物理性质方面表现出了极高的潜力。在这项工作中，我们使用MPNN进行物料分类，通过density functional theory数据库AFLOW中的数据来判断物料是金属或半导体/隔离体。然后，我们进行神经网络架构和超参数的搜索，以提高MPNN预测非金属材料带隔的能力。搜索的参数包括消息传递步数、隐藏大小和活化函数等。我们从搜索中选拔出最佳性能的模型，并将其 Pooling 成 ensemble，该ensemble在已有文献中的模型表现出色。我们使用Monte Carlo Dropout和集成来评估uncertainty quantification，集成方法表现更优。我们还分析了ensemble模型的适用范围，包括晶系、包括Hubbard参数在density functional计算中，以及物质组成的原子种。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Supply-and-Demand-in-Public-Transportation-Systems"><a href="#Modeling-Supply-and-Demand-in-Public-Transportation-Systems" class="headerlink" title="Modeling Supply and Demand in Public Transportation Systems"></a>Modeling Supply and Demand in Public Transportation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06299">http://arxiv.org/abs/2309.06299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Miranda Bihler, Hala Nelson, Erin Okey, Noe Reyes Rivas, John Webb, Anna White</li>
<li>for:  HDPT 想要使用数据提高运营效率和效果。</li>
<li>methods: 我们构建了两个供应和需求模型，帮助HDPT发现服务中的缺陷。这些模型考虑了多个变量，包括HDPT向联邦政府报告的方式和海arrisonburg市最有强制力的地区。我们使用数据分析和机器学习技术进行预测。</li>
<li>results: 我们预测了HDPT的服务缺陷，以帮助它提高运营效率和效果。<details>
<summary>Abstract</summary>
The Harrisonburg Department of Public Transportation (HDPT) aims to leverage their data to improve the efficiency and effectiveness of their operations. We construct two supply and demand models that help the department identify gaps in their service. The models take many variables into account, including the way that the HDPT reports to the federal government and the areas with the most vulnerable populations in Harrisonburg City. We employ data analysis and machine learning techniques to make our predictions.
</details>
<details>
<summary>摘要</summary>
哈里逊堡公共交通部门（HDPT）想要利用数据提高其运营效率和效果。我们构建了两个供应和需求模型，帮助公共交通部门识别服务中的缺陷。这些模型考虑了许多变量，包括公共交通部门向联邦政府报告的方式和哈里逊城市最为易受影响的地区。我们使用数据分析和机器学习技术进行预测。
</details></li>
</ul>
<hr>
<h2 id="ELRA-Exponential-learning-rate-adaption-gradient-descent-optimization-method"><a href="#ELRA-Exponential-learning-rate-adaption-gradient-descent-optimization-method" class="headerlink" title="ELRA: Exponential learning rate adaption gradient descent optimization method"></a>ELRA: Exponential learning rate adaption gradient descent optimization method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06274">http://arxiv.org/abs/2309.06274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Kleinsorge, Stefan Kupper, Alexander Fauck, Felix Rothe</li>
<li>For: 本研究提出了一种新的、快速（指数减速）、无参数（hyper-parameter-free）的梯度下降优化算法。* Methods: 该方法利用情况意识来适应学习率α，主要寻求与邻域梯度垂直的梯度。该方法具有高成功率和快速收敛率，不需手动调整参数，因此更通用。它可应用于任意维度n和问题规模上，并且只 Linearly 增长（order O(n)）。* Results: 对于MNIST数据集，该方法与现有优化器进行比较，并达到了惊人的性能。作者认为，这将开启一个全新的研究方向，即梯度下降优化。<details>
<summary>Abstract</summary>
We present a novel, fast (exponential rate adaption), ab initio (hyper-parameter-free) gradient based optimizer algorithm. The main idea of the method is to adapt the learning rate $\alpha$ by situational awareness, mainly striving for orthogonal neighboring gradients. The method has a high success and fast convergence rate and does not rely on hand-tuned parameters giving it greater universality. It can be applied to problems of any dimensions n and scales only linearly (of order O(n)) with the dimension of the problem. It optimizes convex and non-convex continuous landscapes providing some kind of gradient. In contrast to the Ada-family (AdaGrad, AdaMax, AdaDelta, Adam, etc.) the method is rotation invariant: optimization path and performance are independent of coordinate choices. The impressive performance is demonstrated by extensive experiments on the MNIST benchmark data-set against state-of-the-art optimizers. We name this new class of optimizers after its core idea Exponential Learning Rate Adaption - ELRA. We present it in two variants c2min and p2min with slightly different control. The authors strongly believe that ELRA will open a completely new research direction for gradient descent optimize.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的、快速（指数级别适应）、无参数（hyper-parameter-free）的梯度基本优化算法。主要思想是根据情况意识来适应学习率α，主要寻求垂直邻域梯度的协调。该方法具有高成功率和快速收敛率，不依赖手动调整参数，因此更加通用。它可以应用于任意维度n和问题规模上，且只linearly（order O(n)）随维度增长。它可以优化凸和非凸连续地征地形，并且提供梯度的一种形式。与Ada家族（AdaGrad、AdaMax、AdaDelta、Adam等）不同，该方法是坐标选择无关的：优化路径和性能独立于坐标选择。我们在MNIST数据集上进行了广泛的实验，并证明了该新类型的优化器对现状最优化器的性能具有很高的表现。我们称之为权重学习率适应（ELRA）。我们提出了两种变体：c2min和p2min，它们有些微的不同控制。作者们认为，ELRA将开启一个全新的研究方向，它将gradient descent优化至新的高度。
</details></li>
</ul>
<hr>
<h2 id="ssVERDICT-Self-Supervised-VERDICT-MRI-for-Enhanced-Prostate-Tumour-Characterisation"><a href="#ssVERDICT-Self-Supervised-VERDICT-MRI-for-Enhanced-Prostate-Tumour-Characterisation" class="headerlink" title="ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation"></a>ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06268">http://arxiv.org/abs/2309.06268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Snigdha Sen, Saurabh Singh, Hayley Pye, Caroline Moore, Hayley Whitaker, Shonit Punwani, David Atkinson, Eleftheria Panagiotaki, Paddy J. Slator</li>
<li>for: 用于诊断 próstata癌（PCa）， particularly with diffusion MRI (dMRI) to estimate microstructural information such as cell size.</li>
<li>methods: 使用自我指导的神经网络（DNNs）来修正非线性最小二乘（NLLS）的恶性问题， 而不需要专门的训练数据集。</li>
<li>results: 比基eline方法（NLLS和Supervised DNN）更高的估计精度和降低偏差，以及更高的信任度对比 benign prostate tissue和癌细胞组织的分化。<details>
<summary>Abstract</summary>
MRI is increasingly being used in the diagnosis of prostate cancer (PCa), with diffusion MRI (dMRI) playing an integral role. When combined with computational models, dMRI can estimate microstructural information such as cell size. Conventionally, such models are fit with a nonlinear least squares (NLLS) curve fitting approach, associated with a high computational cost. Supervised deep neural networks (DNNs) are an efficient alternative, however their performance is significantly affected by the underlying distribution of the synthetic training data. Self-supervised learning is an attractive alternative, where instead of using a separate training dataset, the network learns the features of the input data itself. This approach has only been applied to fitting of trivial dMRI models thus far. Here, we introduce a self-supervised DNN to estimate the parameters of the VERDICT (Vascular, Extracellular and Restricted DIffusion for Cytometry in Tumours) model for prostate. We demonstrate, for the first time, fitting of a complex three-compartment biophysical model with machine learning without the requirement of explicit training labels. We compare the estimation performance to baseline NLLS and supervised DNN methods, observing improvement in estimation accuracy and reduction in bias with respect to ground truth values. Our approach also achieves a higher confidence level for discrimination between cancerous and benign prostate tissue in comparison to the other methods on a dataset of 20 PCa patients, indicating potential for accurate tumour characterisation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Toward-Discretization-Consistent-Closure-Schemes-for-Large-Eddy-Simulation-Using-Reinforcement-Learning"><a href="#Toward-Discretization-Consistent-Closure-Schemes-for-Large-Eddy-Simulation-Using-Reinforcement-Learning" class="headerlink" title="Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning"></a>Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06260">http://arxiv.org/abs/2309.06260</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flexi-framework/relexi">https://github.com/flexi-framework/relexi</a></li>
<li>paper_authors: Andrea Beck, Marius Kurz</li>
<li>for: developing discretization-consistent closure schemes for implicitly filtered Large Eddy Simulation (LES)</li>
<li>methods: Markov decision process with Reinforcement Learning (RL) to adapt the coefficients of LES closure models</li>
<li>results: accurate and consistent results, either matching or outperforming classical state-of-the-art models for different discretizations and resolutions<details>
<summary>Abstract</summary>
We propose a novel method for developing discretization-consistent closure schemes for implicitly filtered Large Eddy Simulation (LES). In implicitly filtered LES, the induced filter kernel, and thus the closure terms, are determined by the properties of the grid and the discretization operator, leading to additional computational subgrid terms that are generally unknown in a priori analysis. Therefore, the task of adapting the coefficients of LES closure models is formulated as a Markov decision process and solved in an a posteriori manner with Reinforcement Learning (RL). This allows to adjust the model to the actual discretization as it also incorporates the interaction between the discretization and the model itself. This optimization framework is applied to both explicit and implicit closure models. An element-local eddy viscosity model is optimized as the explicit model. For the implicit modeling, RL is applied to identify an optimal blending strategy for a hybrid discontinuous Galerkin (DG) and finite volume scheme. All newly derived models achieve accurate and consistent results, either matching or outperforming classical state-of-the-art models for different discretizations and resolutions. Moreover, the explicit model is demonstrated to adapt its distribution of viscosity within the DG elements to the inhomogeneous discretization properties of the operator. In the implicit case, the optimized hybrid scheme renders itself as a viable modeling ansatz that could initiate a new class of high order schemes for compressible turbulence. Overall, the results demonstrate that the proposed RL optimization can provide discretization-consistent closures that could reduce the uncertainty in implicitly filtered LES.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于开发基于隐式筛护的大气流体学（LES）中的精度适应闭合方法。在隐式筛护LES中，抽象筛护kernels和关闭项的计算是由网格和精度分解算法的性质决定的，导致额外的计算误差，通常在先前分析中不可知。因此，我们将adapting模型的精度适应问题转化为Markov决策过程，并使用再增强学习（RL）来解决。这种优化框架可以在Explicit和隐式关闭模型之间进行选择。我们使用了一种元素本地风格膜粘性模型作为Explicit模型。而对于隐式模型，我们使用RL来标识最佳混合策略。所有新 derive的模型都实现了准确和一致的结果，与经典的状态对照模型相当或超越。此外，Explicit模型还示出了适应不同精度和分辨率的 Distribution of viscosity 的能力。在隐式情况下，优化的混合方案表明了一种可能的新的高阶方法。总的来说，结果表明了我们提出的RL优化可以提供基于隐式筛护LES的精度适应闭合，从而减少了不确定性。
</details></li>
</ul>
<hr>
<h2 id="Speciality-vs-Generality-An-Empirical-Study-on-Catastrophic-Forgetting-in-Fine-tuning-Foundation-Models"><a href="#Speciality-vs-Generality-An-Empirical-Study-on-Catastrophic-Forgetting-in-Fine-tuning-Foundation-Models" class="headerlink" title="Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models"></a>Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06256">http://arxiv.org/abs/2309.06256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, Tong Zhang</li>
<li>for: 本研究旨在探讨基础模型在精化过程中的特性和通用性之间的质量trade-off，以及如何使用多种正则化方法来解决这个问题。</li>
<li>methods: 本研究使用了多种正则化方法，包括 continual learning 和 Wise-FT 方法，以mitigate the loss of generality 在精化过程中。</li>
<li>results: 研究发现，使用 Wise-FT 方法可以最 effectively 平衡特性和通用性，并且在多种任务和分布上表现最佳。<details>
<summary>Abstract</summary>
Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions and common sense.   To address the trade-off between the speciality and generality, we investigate multiple regularization methods from continual learning, the weight averaging method (Wise-FT) from out-of-distributional (OOD) generalization, which interpolates parameters between pre-trained and fine-tuned models, and parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA). Our findings show that both continual learning and Wise-ft methods effectively mitigate the loss of generality, with Wise-FT exhibiting the strongest performance in balancing speciality and generality.
</details>
<details>
<summary>摘要</summary>
基础模型，如视觉语言模型（VLM）和大型语言模型（LLM），拥有泛化能力，可以处理多样化分布和任务，这是由于它们的广泛预训练数据集的影响。然而，在精度调整过程中，使用小型数据集可能无法完全覆盖预训练中遇到的多样化分布和任务。因此，在精度调整过程中寻求特点可能会导致模型失去泛化能力，这与深度学习中的恶化学习（CF）有关。在这个研究中，我们 demonstarte了这种现象在VLM和LLM中。例如，在CLIP的ImageNet精度调整中，VLM失去了处理多样化分布的能力，而在医疗领域的Galactica精度调整中，LLM失去了遵循指令和常识。为了解决特点和泛化之间的负担，我们 investigate了多种CONTINUAL LEARNING的正则化方法，包括Weight Averaging Method（Wise-FT）和Parameter-Efficient Fine-Tuning Method（LoRA）。我们的发现表明，CONTINUAL LEARNING和Wise-FT方法都能有效避免失去泛化能力，其中Wise-FT方法在均衡特点和泛化能力方面表现最佳。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Evaluation-Metric-for-Probability-Estimation-Models-Using-Esports-Data"><a href="#Rethinking-Evaluation-Metric-for-Probability-Estimation-Models-Using-Esports-Data" class="headerlink" title="Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data"></a>Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06248">http://arxiv.org/abs/2309.06248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Euihyeon Choi, Jooyoung Kim, Wonkyung Lee</li>
<li>for: 这个论文是为了评估电子竞技比赛中的胜率估计模型而写的。</li>
<li>methods: 这个论文使用了布莱尔分数和预期准确错误（ECE）作为评估胜率估计模型的性能评估指标，并提出了一个新的简单 yet effective的度量标准 called Balance score，该标准具有六种好的属性。</li>
<li>results: 经过广泛的 simulation studies 和实际游戏快照数据的评估，提出的 Balance score 显示出了可靠地评估电子竞技比赛中胜率估计模型的性能，并且可以作为一个全面的评估指标来评估总的比赛模型。<details>
<summary>Abstract</summary>
Probability estimation models play an important role in various fields, such as weather forecasting, recommendation systems, and sports analysis. Among several models estimating probabilities, it is difficult to evaluate which model gives reliable probabilities since the ground-truth probabilities are not available. The win probability estimation model for esports, which calculates the win probability under a certain game state, is also one of the fields being actively studied in probability estimation. However, most of the previous works evaluated their models using accuracy, a metric that only can measure the performance of discrimination. In this work, we firstly investigate the Brier score and the Expected Calibration Error (ECE) as a replacement of accuracy used as a performance evaluation metric for win probability estimation models in esports field. Based on the analysis, we propose a novel metric called Balance score which is a simple yet effective metric in terms of six good properties that probability estimation metric should have. Under the general condition, we also found that the Balance score can be an effective approximation of the true expected calibration error which has been imperfectly approximated by ECE using the binning technique. Extensive evaluations using simulation studies and real game snapshot data demonstrate the promising potential to adopt the proposed metric not only for the win probability estimation model for esports but also for evaluating general probability estimation models.
</details>
<details>
<summary>摘要</summary>
probabilistic estimation models play an important role in various fields, such as weather forecasting, recommendation systems, and sports analysis. among several models estimating probabilities, it is difficult to evaluate which model gives reliable probabilities since the ground-truth probabilities are not available. the win probability estimation model for esports, which calculates the win probability under a certain game state, is also one of the fields being actively studied in probability estimation. however, most of the previous works evaluated their models using accuracy, a metric that only can measure the performance of discrimination. in this work, we firstly investigate the Brier score and the Expected Calibration Error (ECE) as a replacement of accuracy used as a performance evaluation metric for win probability estimation models in esports field. based on the analysis, we propose a novel metric called Balance score which is a simple yet effective metric in terms of six good properties that probability estimation metric should have. under the general condition, we also found that the Balance score can be an effective approximation of the true expected calibration error which has been imperfectly approximated by ECE using the binning technique. extensive evaluations using simulation studies and real game snapshot data demonstrate the promising potential to adopt the proposed metric not only for the win probability estimation model for esports but also for evaluating general probability estimation models.
</details></li>
</ul>
<hr>
<h2 id="Consistency-and-adaptivity-are-complementary-targets-for-the-validation-of-variance-based-uncertainty-quantification-metrics-in-machine-learning-regression-tasks"><a href="#Consistency-and-adaptivity-are-complementary-targets-for-the-validation-of-variance-based-uncertainty-quantification-metrics-in-machine-learning-regression-tasks" class="headerlink" title="Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks"></a>Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06240">http://arxiv.org/abs/2309.06240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Pernot</li>
<li>for: 这篇论文主要关注机器学习 uncertainty quantification（UQ）在材料和化学领域的可靠性评估。</li>
<li>methods: 论文使用了多种方法来评估UQ的可靠性，包括consistency和adaptivity。</li>
<li>results: 论文表明了consistency和adaptivity是补充的验证目标，并且一个good consistency不一定意味着good adaptivity。<details>
<summary>Abstract</summary>
Reliable uncertainty quantification (UQ) in machine learning (ML) regression tasks is becoming the focus of many studies in materials and chemical science. It is now well understood that average calibration is insufficient, and most studies implement additional methods testing the conditional calibration with respect to uncertainty, i.e. consistency. Consistency is assessed mostly by so-called reliability diagrams. There exists however another way beyond average calibration, which is conditional calibration with respect to input features, i.e. adaptivity. In practice, adaptivity is the main concern of the final users of a ML-UQ method, seeking for the reliability of predictions and uncertainties for any point in features space. This article aims to show that consistency and adaptivity are complementary validation targets, and that a good consistency does not imply a good adaptivity. Adapted validation methods are proposed and illustrated on a representative example.
</details>
<details>
<summary>摘要</summary>
通用不确定评估（UQ）在机器学习（ML）回归任务中的可靠性已成为许多材料和化学科学研究的焦点。现已经明确，平均调整不足，大多数研究采用附加方法测试受到不确定性的Conditional calibration，即一致性。一致性通常通过所谓的可靠性图表示。然而，还有另一种超出平均调整的方法，即基于输入特征的 Conditional calibration，即适应性。在实践中，适应性是最终用户的ML-UQ方法可靠性预测和不确定性的首要关心，寻求任何特征空间中的可靠性和不确定性。本文目标表明了一致性和适应性是补充 validate 目标，并且一个好的一致性不一定意味着一个好的适应性。适应性验证方法被提出并在一个代表性的示例中 ilustrated。
</details></li>
</ul>
<hr>
<h2 id="Risk-Aware-Reinforcement-Learning-through-Optimal-Transport-Theory"><a href="#Risk-Aware-Reinforcement-Learning-through-Optimal-Transport-Theory" class="headerlink" title="Risk-Aware Reinforcement Learning through Optimal Transport Theory"></a>Risk-Aware Reinforcement Learning through Optimal Transport Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06239">http://arxiv.org/abs/2309.06239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Baheri</li>
<li>for: This paper is written for researchers and practitioners interested in developing risk-aware reinforcement learning (RL) algorithms that can operate in dynamic and uncertain environments.</li>
<li>methods: The paper integrates Optimal Transport (OT) theory with RL to create a risk-aware framework. The approach modifies the objective function to ensure that the resulting policy maximizes expected rewards while respecting risk constraints dictated by OT distances between state visitation distributions and desired risk profiles.</li>
<li>results: The paper offers a formulation that elevates risk considerations alongside conventional RL objectives, and provides a series of theorems that map the relationships between risk distributions, optimal value functions, and policy behaviors. The work demonstrates a promising direction for RL, ensuring a balanced fusion of reward pursuit and risk awareness.<details>
<summary>Abstract</summary>
In the dynamic and uncertain environments where reinforcement learning (RL) operates, risk management becomes a crucial factor in ensuring reliable decision-making. Traditional RL approaches, while effective in reward optimization, often overlook the landscape of potential risks. In response, this paper pioneers the integration of Optimal Transport (OT) theory with RL to create a risk-aware framework. Our approach modifies the objective function, ensuring that the resulting policy not only maximizes expected rewards but also respects risk constraints dictated by OT distances between state visitation distributions and the desired risk profiles. By leveraging the mathematical precision of OT, we offer a formulation that elevates risk considerations alongside conventional RL objectives. Our contributions are substantiated with a series of theorems, mapping the relationships between risk distributions, optimal value functions, and policy behaviors. Through the lens of OT, this work illuminates a promising direction for RL, ensuring a balanced fusion of reward pursuit and risk awareness.
</details>
<details>
<summary>摘要</summary>
在动态和不确定环境中，控制风险成为RL运算中关键的一个因素，以确保可靠的决策。传统RL方法，虽然能够优化奖励，但经常忽略潜在的风险风险。为应对这些问题，这篇论文提出了将Optimal Transport（OT）理论与RL结合，创建一个风险意识框架。我们的方法修改了目标函数，使得结果策略不仅最大化预期奖励，还遵循由OT距离状态访问分布和愿望风险规则的风险约束。通过OT的数学精度，我们提供了一种形ulation，使得风险考虑与传统RL目标联系起来。我们的贡献得到了一系列定理的证明，映射了风险分布、优化值函数和策略行为之间的关系。通过OT的视角，这项工作突出了RL中风险意识的重要性，并提供了一个平衡RL目标和风险考虑的可行方法。
</details></li>
</ul>
<hr>
<h2 id="A-Consistent-and-Scalable-Algorithm-for-Best-Subset-Selection-in-Single-Index-Models"><a href="#A-Consistent-and-Scalable-Algorithm-for-Best-Subset-Selection-in-Single-Index-Models" class="headerlink" title="A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models"></a>A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06230">http://arxiv.org/abs/2309.06230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Borui Tang, Jin Zhu, Junxian Zhu, Xueqin Wang, Heping Zhang</li>
<li>for: 这篇论文旨在提出一种可扩展的算法，用于在高维数据中选择最佳子集，以提高模型选择和预测性能。</li>
<li>methods: 该论文提出了一种新的算法，基于通信簇信息理论和概率评估，可以在高维数据中直接解决最佳子集选择问题，并且可以确定支持大小。</li>
<li>results:  simulations 表明，该算法不仅可以快速计算出最佳子集，而且可以准确地回归最佳子集，并且不需要进行模型选择调整。<details>
<summary>Abstract</summary>
Analysis of high-dimensional data has led to increased interest in both single index models (SIMs) and best subset selection. SIMs provide an interpretable and flexible modeling framework for high-dimensional data, while best subset selection aims to find a sparse model from a large set of predictors. However, best subset selection in high-dimensional models is known to be computationally intractable. Existing methods tend to relax the selection, but do not yield the best subset solution. In this paper, we directly tackle the intractability by proposing the first provably scalable algorithm for best subset selection in high-dimensional SIMs. Our algorithmic solution enjoys the subset selection consistency and has the oracle property with a high probability. The algorithm comprises a generalized information criterion to determine the support size of the regression coefficients, eliminating the model selection tuning. Moreover, our method does not assume an error distribution or a specific link function and hence is flexible to apply. Extensive simulation results demonstrate that our method is not only computationally efficient but also able to exactly recover the best subset in various settings (e.g., linear regression, Poisson regression, heteroscedastic models).
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:高维数据分析引发了对单指数模型（SIMs）和最佳子集选择的更多的关注。SIMs提供了可解释和灵活的模型框架，而最佳子集选择则目标是从大量预测变量中找到一个稀疏的模型。然而，高维模型中的最佳子集选择是计算 tractable。现有方法通常会放弃选择，而不是获得最佳子集解决方案。在这篇论文中，我们直接面临到了 tractability 的问题，并提出了高维 SIMs 中首个可扩展性的最佳子集选择算法。我们的算法解决了模型选择决策，并且具有可算法性和高概率 oracle 性。我们的方法不仅不需要错误分布或特定的链函数，而且可以适应应用。我们的实验结果表明，我们的方法不仅是计算高效的，而且能够在不同的设置（如线性回归、波利回归、不同的风险函数）中准确回归最佳子集。
</details></li>
</ul>
<hr>
<h2 id="Long-term-drought-prediction-using-deep-neural-networks-based-on-geospatial-weather-data"><a href="#Long-term-drought-prediction-using-deep-neural-networks-based-on-geospatial-weather-data" class="headerlink" title="Long-term drought prediction using deep neural networks based on geospatial weather data"></a>Long-term drought prediction using deep neural networks based on geospatial weather data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06212">http://arxiv.org/abs/2309.06212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vsevolod Grabar, Alexander Marusov, Yury Maximov, Nazar Sotiriadi, Alexander Bulkin, Alexey Zaytsev</li>
<li>for: 预测specific地区的旱情可能性，以便农业决策。</li>
<li>methods: 使用various spatiotemporal neural networks，包括Convolutional LSTM和transformer模型，以预测旱情Intensity。</li>
<li>results: 比基线模型更高的ROC AUC scores，表明Convolutional LSTM和transformer模型具有更高的预测精度。<details>
<summary>Abstract</summary>
The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.   Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 to 0.70 for forecast horizons from one to six months, outperforming baseline models. The transformer showed superiority for shorter horizons, while ConvLSTM did so for longer horizons. Thus, we recommend selecting the models accordingly for long-term drought forecasting.   To ensure the broad applicability of the considered models, we conduct extensive validation across regions worldwide, considering different environmental conditions. We also run several ablation and sensitivity studies to challenge our findings and provide additional information on how to solve the problem.
</details>
<details>
<summary>摘要</summary>
“精准预测特定地区的旱情机会是农业实践中重要的决策依据。特别是在长期决策中，一年前的预测非常重要。然而，预测这些机会受到当地和邻近地区的复杂因素之间的互动所困扰。在这个研究中，我们提出了一个终端解决方案，基于不同的时空神经网络。我们考虑的模型集中心于预测旱情强度，基于Palmer旱情严重指数（PDSI）的子区域，利用自然因素和气候模型的内在知识来增强旱情预测。比较评估显示，Convolutional LSTM（ConvLSTM）和transformer模型在基于梯度提升和逻辑回传模型的比较下表现出更高的ROC AUC分数，分别在1至6个月的预测时间范围内。transformer模型在短期预测中表现出色，而ConvLSTM模型在长期预测中表现出色。因此，我们建议在长期旱情预测中选择这两种模型。为确保考虑的模型在不同环境下具有广泛的应用性，我们进行了广泛的验证，考虑了不同的环境条件。我们还进行了多个ablation和敏感性研究，以提供额外的信息和解决方案。”
</details></li>
</ul>
<hr>
<h2 id="Optimization-Guarantees-of-Unfolded-ISTA-and-ADMM-Networks-With-Smooth-Soft-Thresholding"><a href="#Optimization-Guarantees-of-Unfolded-ISTA-and-ADMM-Networks-With-Smooth-Soft-Thresholding" class="headerlink" title="Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding"></a>Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06195">http://arxiv.org/abs/2309.06195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaik Basheeruddin Shah, Pradyumna Pradhan, Wei Pu, Ramunaidu Randhi, Miguel R. D. Rodrigues, Yonina C. Eldar</li>
<li>for: 本研究探讨了线性逆问题的解决方法，具体来说是使用iterative soft-thresholding algorithm (LISTA)和alternating direction method of multipliers compressive sensing network (ADMM-CSNet)来有效地Addressing these problems.</li>
<li>methods: 本文使用了finite-layer unfolded networks，如LISTA和ADMM-CSNet，以及smooth soft-thresholding nonlinearity。</li>
<li>results: 本研究表明，在over-parameterized（OP） regime下，通过利用一种修改后的Polyak-Lojasiewicz（PL*）条件，可以确保 Training loss减少到 Near-zero 的情况，并且存在global minimum和抽象减少从初始化点使用梯度下降方法。此外，我们还证明了，随着网络宽度的增加， unfolded networks 的阈值会增加，而 FFNN 的阈值则会减少。<details>
<summary>Abstract</summary>
Solving linear inverse problems plays a crucial role in numerous applications. Algorithm unfolding based, model-aware data-driven approaches have gained significant attention for effectively addressing these problems. Learned iterative soft-thresholding algorithm (LISTA) and alternating direction method of multipliers compressive sensing network (ADMM-CSNet) are two widely used such approaches, based on ISTA and ADMM algorithms, respectively. In this work, we study optimization guarantees, i.e., achieving near-zero training loss with the increase in the number of learning epochs, for finite-layer unfolded networks such as LISTA and ADMM-CSNet with smooth soft-thresholding in an over-parameterized (OP) regime. We achieve this by leveraging a modified version of the Polyak-Lojasiewicz, denoted PL$^*$, condition. Satisfying the PL$^*$ condition within a specific region of the loss landscape ensures the existence of a global minimum and exponential convergence from initialization using gradient descent based methods. Hence, we provide conditions, in terms of the network width and the number of training samples, on these unfolded networks for the PL$^*$ condition to hold. We achieve this by deriving the Hessian spectral norm of these networks. Additionally, we show that the threshold on the number of training samples increases with the increase in the network width. Furthermore, we compare the threshold on training samples of unfolded networks with that of a standard fully-connected feed-forward network (FFNN) with smooth soft-thresholding non-linearity. We prove that unfolded networks have a higher threshold value than FFNN. Consequently, one can expect a better expected error for unfolded networks than FFNN.
</details>
<details>
<summary>摘要</summary>
解决线性逆问题在许多应用中发挥关键作用。基于数据驱动的算法折叠方法在这些问题上获得了广泛的关注。例如，学习迭代软阈值算法（LISTA）和多向量方法混合压缩感知网络（ADMM-CSNet）是两种广泛使用的方法，基于ISTA和ADMM算法 соответственно。在这种情况下，我们研究了优化保证，即随着学习epoch数量的增加，训练损失逼近零。我们通过利用修改后的波利亚克-洛亚西埃茨（PL$^*$）条件来实现这一点。在特定的损失图像中满足PL$^*$条件可以保证存在全局最小值，并使用梯度下降方法进行快速收敛。因此，我们提供了基于网络宽度和训练样本数量的条件，以确保PL$^*$条件在 unfolded networks 中成立。我们通过计算这些网络的梯度特征值来实现这一点。此外，我们还证明了随着网络宽度的增加，训练样本数量的阈值也会增加。此外，我们比较了 unfolded networks 和标准的全连接径Feedforward Network（FFNN）的训练样本数量的阈值，并证明了 unfolded networks 的阈值高于 FFNN。因此，我们可以预期 unfolded networks 的预期错误更低。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Generalization-Gap-of-Learning-Based-Speech-Enhancement-Systems-in-Noisy-and-Reverberant-Environments"><a href="#Assessing-the-Generalization-Gap-of-Learning-Based-Speech-Enhancement-Systems-in-Noisy-and-Reverberant-Environments" class="headerlink" title="Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments"></a>Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06183">http://arxiv.org/abs/2309.06183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philippe Gonzalez, Tommy Sonne Alstrøm, Tobias May<br>for: 这种研究旨在解决学习型声音提高系统在不同条件下的一致性问题。methods: 研究使用了一个参照模型，该模型在测试条件下进行训练，以便用于评估测试条件的难度。results: 研究发现，所有模型在语音匹配情况下表现最差，而好的噪声和房间泛化可以通过训练多个数据库来实现。此外，最新的模型在匹配情况下表现最好，但在不匹配情况下表现很差，甚至可能 inferior to FFNN-based system。<details>
<summary>Abstract</summary>
The acoustic variability of noisy and reverberant speech mixtures is influenced by multiple factors, such as the spectro-temporal characteristics of the target speaker and the interfering noise, the signal-to-noise ratio (SNR) and the room characteristics. This large variability poses a major challenge for learning-based speech enhancement systems, since a mismatch between the training and testing conditions can substantially reduce the performance of the system. Generalization to unseen conditions is typically assessed by testing the system with a new speech, noise or binaural room impulse response (BRIR) database different from the one used during training. However, the difficulty of the speech enhancement task can change across databases, which can substantially influence the results. The present study introduces a generalization assessment framework that uses a reference model trained on the test condition, such that it can be used as a proxy for the difficulty of the test condition. This allows to disentangle the effect of the change in task difficulty from the effect of dealing with new data, and thus to define a new measure of generalization performance termed the generalization gap. The procedure is repeated in a cross-validation fashion by cycling through multiple speech, noise, and BRIR databases to accurately estimate the generalization gap. The proposed framework is applied to evaluate the generalization potential of a feedforward neural network (FFNN), Conv-TasNet, DCCRN and MANNER. We find that for all models, the performance degrades the most in speech mismatches, while good noise and room generalization can be achieved by training on multiple databases. Moreover, while recent models show higher performance in matched conditions, their performance substantially decreases in mismatched conditions and can become inferior to that of the FFNN-based system.
</details>
<details>
<summary>摘要</summary>
难以分辨的声音混响样本中的声音特征是多种因素的影响，包括目标说话人的spectro-temporal特征、干扰噪声和room特性。这种大量的变化对学习型声音提高系统来说是一个主要挑战，因为在测试和训练条件不同时，系统的性能可能会下降substantially。通常来衡量系统的普适性是通过在测试数据集中测试系统，并对其进行cross-validation验证。然而，任务难度可能会在不同的数据集中发生变化，这会对结果产生很大的影响。本研究提出了一种普适性评估框架，通过使用测试条件下的参考模型，以便用其作为测试条件的困难度的代理。这 позволяет分解把握新数据的效果与把握任务难度的效果分开，并定义一个新的普适性度量，称为普适差（generalization gap）。该框架在多个语音、噪声和BRIR数据集中重复应用，以准确估计普适性差。研究发现，对所有模型来说，性能最大程度下降是在语音匹配中，而好的噪声和room普适性可以通过训练多个数据集来实现。此外，最新的模型在匹配条件下表现出色，但在匹配不符条件下表现很差，可能变成较为老的FFNN基于系统的性能。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention"><a href="#Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention" class="headerlink" title="Efficient Memory Management for Large Language Model Serving with PagedAttention"></a>Efficient Memory Management for Large Language Model Serving with PagedAttention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06180">http://arxiv.org/abs/2309.06180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a></li>
<li>paper_authors: Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica</li>
<li>for: 提高大语言模型（LLM）的高速服务性能，需要批处理足够多的请求。但现有系统受到缓存（KV cache）内存的废弃和重复占用的问题，限制批处理大小。</li>
<li>methods: 我们提出了基于经典虚拟内存和分页技术的注意力算法——PagedAttention，并在其基础上构建了vLLM，一个实现了近于零废弃的KV cache内存和请求之间共享的LLM服务系统。</li>
<li>results: 我们的评估结果显示，vLLM可以提高受欢迎的LLM的吞吐量，比现状态之系统（如FasterTransformer和Orca）高出2-4倍，同时保持同样的响应时间。这种改进更加明显地出现在更长的序列、更大的模型和更复杂的解码算法中。vLLM的源代码可以在<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/vllm-project/vllm上获取。</a><details>
<summary>Abstract</summary>
High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm
</details>
<details>
<summary>摘要</summary>
高速服务大语言模型（LLM）需要批处理足够多的请求。然而，现有系统受到缓存（KV cache）的内存占用增加和减少的影响，导致批处理大小受限。当管理不善时，这些内存可能会受到浪费，因为碎片和重复备份，限制批处理大小。为解决这问题，我们提出了 PagedAttention，一种基于经典虚拟内存和分页技术的注意力算法。此外，我们建立了 vLLM，一个实现了（1）缓存内存几乎为零和（2）在请求之间和请求中 flexible 分享缓存的 LLM 服务系统。我们的评估显示，vLLM 可以在相同的延迟水平下提高流行的 LLM 的 Throughput 2-4 倍，比现状态的系统（如 FasterTransformer 和 Orca）更高效。这种改进更加明显，当遇到长序、大模型和复杂的解码算法时。vLLM 的源代码可以在 <https://github.com/vllm-project/vllm> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Edge-AI-with-Morpher-An-Integrated-Design-Compilation-and-Simulation-Framework-for-CGRAs"><a href="#Accelerating-Edge-AI-with-Morpher-An-Integrated-Design-Compilation-and-Simulation-Framework-for-CGRAs" class="headerlink" title="Accelerating Edge AI with Morpher: An Integrated Design, Compilation and Simulation Framework for CGRAs"></a>Accelerating Edge AI with Morpher: An Integrated Design, Compilation and Simulation Framework for CGRAs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06127">http://arxiv.org/abs/2309.06127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecolab-nus/morpher-v2">https://github.com/ecolab-nus/morpher-v2</a></li>
<li>paper_authors: Dhananjaya Wijerathne, Zhaoying Li, Tulika Mitra</li>
<li>for: 这个论文旨在探讨基于CGRA的粗粒度可重配置扩展（Coarse-Grained Reconfigurable Arrays）在边缘计算中的应用潜力，以及Morpher框架如何自动将AI应用程序核心编译到用户定义的CGRA架构上，并验证其功能。</li>
<li>methods: 这篇论文使用了Morpher框架，包括特制的编译器、模拟器、加速器合成和验证框架，来探讨CGRA在边缘计算中的灵活性和高效性。</li>
<li>results: 该论文表明，Morpher框架可以自动将AI应用程序核心编译到用户定义的CGRA架构上，并验证其功能。这些结果表明CGRA在边缘计算中的应用潜力，以及Morpher框架的可靠性和灵活性。<details>
<summary>Abstract</summary>
Coarse-Grained Reconfigurable Arrays (CGRAs) hold great promise as power-efficient edge accelerator, offering versatility beyond AI applications. Morpher, an open-source, architecture-adaptive CGRA design framework, is specifically designed to explore the vast design space of CGRAs. The comprehensive ecosystem of Morpher includes a tailored compiler, simulator, accelerator synthesis, and validation framework. This study provides an overview of Morpher, highlighting its capabilities in automatically compiling AI application kernels onto user-defined CGRA architectures and verifying their functionality. Through the Morpher framework, the versatility of CGRAs is harnessed to facilitate efficient compilation and verification of edge AI applications, covering important kernels representative of a wide range of embedded AI workloads. Morpher is available online at https://github.com/ecolab-nus/morpher-v2.
</details>
<details>
<summary>摘要</summary>
便捷grained重构阵列（CGRA）具有很大的潜力，作为智能边缘加速器，它们的灵活性超出了人工智能应用。Morpher是一个开源的、建筑架构适应的CGRA设计框架，专门为探索CGRA的庞大设计空间而设计。Morpher的全面生态系统包括特制的编译器、模拟器、加速器合成和验证框架。本文提供了Morpher的概述，强调它在自动将人工智能应用程序核心编译到用户定义的CGRA架构上并验证其功能的能力。通过Morpher框架，CGRA的灵活性得以充分发挥，以便高效地编译和验证边缘AI应用程序，覆盖了各种嵌入式AI工作负荷中的重要核心。Morpher可在https://github.com/ecolab-nus/morpher-v2上下载。
</details></li>
</ul>
<hr>
<h2 id="A-robust-synthetic-data-generation-framework-for-machine-learning-in-High-Resolution-Transmission-Electron-Microscopy-HRTEM"><a href="#A-robust-synthetic-data-generation-framework-for-machine-learning-in-High-Resolution-Transmission-Electron-Microscopy-HRTEM" class="headerlink" title="A robust synthetic data generation framework for machine learning in High-Resolution Transmission Electron Microscopy (HRTEM)"></a>A robust synthetic data generation framework for machine learning in High-Resolution Transmission Electron Microscopy (HRTEM)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06122">http://arxiv.org/abs/2309.06122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis Rangel DaCosta, Katherine Sytwu, Catherine Groschner, Mary Scott</li>
<li>for: 这项研究的目的是开发高精度自动分析工具，用于 characterizing nanomaterials。</li>
<li>methods: 这项研究使用了机器学习技术，包括 neural networks，来开发高精度自动分析工具。</li>
<li>results: 研究人员通过使用 Construction Zone 和 simulated databases，成功地实现了高精度的自动分析工具，并在多个实验室 benchmark 上达到了州际精度。<details>
<summary>Abstract</summary>
Machine learning techniques are attractive options for developing highly-accurate automated analysis tools for nanomaterials characterization, including high-resolution transmission electron microscopy (HRTEM). However, successfully implementing such machine learning tools can be difficult due to the challenges in procuring sufficiently large, high-quality training datasets from experiments. In this work, we introduce Construction Zone, a Python package for rapidly generating complex nanoscale atomic structures, and develop an end-to-end workflow for creating large simulated databases for training neural networks. Construction Zone enables fast, systematic sampling of realistic nanomaterial structures, and can be used as a random structure generator for simulated databases, which is important for generating large, diverse synthetic datasets. Using HRTEM imaging as an example, we train a series of neural networks on various subsets of our simulated databases to segment nanoparticles and holistically study the data curation process to understand how various aspects of the curated simulated data -- including simulation fidelity, the distribution of atomic structures, and the distribution of imaging conditions -- affect model performance across several experimental benchmarks. Using our results, we are able to achieve state-of-the-art segmentation performance on experimental HRTEM images of nanoparticles from several experimental benchmarks and, further, we discuss robust strategies for consistently achieving high performance with machine learning in experimental settings using purely synthetic data.
</details>
<details>
<summary>摘要</summary>
机器学习技术是为发展高精度自动分析工具的可能性非常高，特别是在高解度电子镜icroscopy (HRTEM) 领域。然而，实现这些机器学习工具可能会困难，因为实验获得足够大、高质量训练数据的挑战。在这项工作中，我们介绍了 Construction Zone，一个 Python 包，用于快速生成复杂的 nanoscale 原子结构，并开发了一个端到端的工作流程，用于创建大规模的 simulated 数据库。Construction Zone 允许快速、系统地采样真实的 nanomaterial 结构，可以用作随机结构生成器，这是生成大、多样化的 sintetic 数据库的重要工具。使用 HRTEM 成像为例，我们在不同的 sub-dataset 上训练了一系列神经网络，以分类 nanoparticles，并对数据准备过程进行了全面的研究，以了解不同的 simulated 数据特性如 simulation fidelity、原子结构分布和成像条件分布对模型性能的影响。使用我们的结果，我们可以在多个实验室benchmark上达到 estado-of-the-art 的分类性能，并讨论了在实验设置中使用纯 synthetic 数据获得高性能的机器学习策略。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-Human-Activity-Recognition-Using-Sensor-Data"><a href="#Overview-of-Human-Activity-Recognition-Using-Sensor-Data" class="headerlink" title="Overview of Human Activity Recognition Using Sensor Data"></a>Overview of Human Activity Recognition Using Sensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07170">http://arxiv.org/abs/2309.07170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebeen Ali Hamad, Wai Lok Woo, Bo Wei, Longzhi Yang</li>
<li>for: 这篇论文主要是为了探讨人活动识别（HAR）领域的最新进展和挑战。</li>
<li>methods: 本论文使用了多种感知器和深度学习技术来探讨人活动识别。</li>
<li>results: 论文提出了一些关键应用场景，如家居和办公室自动化、安全监测和医疗保健等，同时也探讨了深度学习技术在HAR领域的应用。<details>
<summary>Abstract</summary>
Human activity recognition (HAR) is an essential research field that has been used in different applications including home and workplace automation, security and surveillance as well as healthcare. Starting from conventional machine learning methods to the recently developing deep learning techniques and the Internet of things, significant contributions have been shown in the HAR area in the last decade. Even though several review and survey studies have been published, there is a lack of sensor-based HAR overview studies focusing on summarising the usage of wearable sensors and smart home sensors data as well as applications of HAR and deep learning techniques. Hence, we overview sensor-based HAR, discuss several important applications that rely on HAR, and highlight the most common machine learning methods that have been used for HAR. Finally, several challenges of HAR are explored that should be addressed to further improve the robustness of HAR.
</details>
<details>
<summary>摘要</summary>
人类活动识别（HAR）是一个重要的研究领域，在不同的应用中都有广泛的应用，包括家庭和工作场所自动化、安全监测以及医疗保健等。过去十年，在传统的机器学习方法基础上，深度学习技术的发展以及互联网物联网技术的应用，在HAR领域内部维护了重要的贡献。然而，当前的文献综述和评视研究却缺乏关于基于感知器的HAR的概要，包括穿戴式感知器和智能家居感知器数据的使用情况，以及基于HAR和深度学习技术的应用。因此，本文将对感知器基于HAR进行概要，讨论一些重要的依赖于HAR的应用，并高亮一些最常用的机器学习方法，最后探讨HAR面临的一些挑战，以便进一步改善HAR的稳定性。
</details></li>
</ul>
<hr>
<h2 id="A-General-Verification-Framework-for-Dynamical-and-Control-Models-via-Certificate-Synthesis"><a href="#A-General-Verification-Framework-for-Dynamical-and-Control-Models-via-Certificate-Synthesis" class="headerlink" title="A General Verification Framework for Dynamical and Control Models via Certificate Synthesis"></a>A General Verification Framework for Dynamical and Control Models via Certificate Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06090">http://arxiv.org/abs/2309.06090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alec Edwards, Andrea Peruffo, Alessandro Abate</li>
<li>for: 这篇论文主要关注在证书学习中，即指定一个自动或控制模型的行为，并通过函数基本证明其正确性。</li>
<li>methods: 该论文提出了一种通用框架，用于编码系统特性和相应的证书，以及一种自动化控制器和证书的生成方法。该方法利用神经网络提供候选控制函数和证书函数，并使用SMT解决器提供正式正确性保证。</li>
<li>results: 该论文通过开发一个软件工具，测试了其核心框架的可靠性和灵活性。 Results show that the proposed approach can effectively synthesize controllers and certificates for a wide range of system specifications, and provide a formal guarantee of correctness.<details>
<summary>Abstract</summary>
An emerging branch of control theory specialises in certificate learning, concerning the specification of a desired (possibly complex) system behaviour for an autonomous or control model, which is then analytically verified by means of a function-based proof. However, the synthesis of controllers abiding by these complex requirements is in general a non-trivial task and may elude the most expert control engineers. This results in a need for automatic techniques that are able to design controllers and to analyse a wide range of elaborate specifications. In this paper, we provide a general framework to encode system specifications and define corresponding certificates, and we present an automated approach to formally synthesise controllers and certificates. Our approach contributes to the broad field of safe learning for control, exploiting the flexibility of neural networks to provide candidate control and certificate functions, whilst using SMT-solvers to offer a formal guarantee of correctness. We test our framework by developing a prototype software tool, and assess its efficacy at verification via control and certificate synthesis over a large and varied suite of benchmarks.
</details>
<details>
<summary>摘要</summary>
一种新般的控制理论分支是证书学习，关注自动或控制模型的行为规范，并通过函数基本证明其分析。然而，实现这些复杂要求的控制器设计通常是一个非rivial任务，可能会让控制工程师感到惑乱。这导致了一种自动化的技术需求，能够设计控制器并分析各种复杂规范。在这篇论文中，我们提供一个通用框架来编码系统规范和相应的证书，并提出一种自动化的控制器和证书synthesis方法。我们的方法在安全学习控制领域中发挥作用，利用神经网络提供候选控制和证书函数，而使用SMT-解决器提供正式的正确性保证。我们测试了我们的框架，开发了一个原型软件工具，并通过控制和证书验证 benchmarks 进行验证。
</details></li>
</ul>
<hr>
<h2 id="Information-Flow-in-Graph-Neural-Networks-A-Clinical-Triage-Use-Case"><a href="#Information-Flow-in-Graph-Neural-Networks-A-Clinical-Triage-Use-Case" class="headerlink" title="Information Flow in Graph Neural Networks: A Clinical Triage Use Case"></a>Information Flow in Graph Neural Networks: A Clinical Triage Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06081">http://arxiv.org/abs/2309.06081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Víctor Valls, Mykhaylo Zayats, Alessandra Pascale</li>
<li>for: This paper aims to investigate the effect of embedding information flow within Graph Neural Networks (GNNs) on the prediction of links in Knowledge Graphs (KGs), with a specific use case in clinical triage.</li>
<li>methods: The paper proposes a mathematical model that decouples the GNN connectivity from the connectivity of the graph data, and evaluates the performance of GNNs with different connectivity strategies.</li>
<li>results: The results show that incorporating domain knowledge into the GNN connectivity leads to better performance than using the same connectivity as the KG or allowing unconstrained embedding propagation. Additionally, the paper finds that negative edges play a crucial role in achieving good predictions, and that using too many GNN layers can degrade performance.Here’s the simplified Chinese text for the three information points:</li>
<li>for: 这篇论文目标是调查图神经网络（GNNs）中 embedding 信息流动对知识图（KGs）中预测链接的影响，特别是在医疗抢救use case中。</li>
<li>methods: 论文提出了一个数学模型，将GNN连接性与图数据连接性分离开来，并评估不同连接策略下GNN的表现。</li>
<li>results: 结果表明，基于域知识的GNN连接策略可以在预测链接方面获得更好的性能，而使用同KG连接策略或允许无约 embedding 传播的策略则不如其他策略。此外，论文还发现，负边在预测链接方面扮演着关键的角色，并且使用过多GNN层可能会降低性能。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have gained popularity in healthcare and other domains due to their ability to process multi-modal and multi-relational graphs. However, efficient training of GNNs remains challenging, with several open research questions. In this paper, we investigate how the flow of embedding information within GNNs affects the prediction of links in Knowledge Graphs (KGs). Specifically, we propose a mathematical model that decouples the GNN connectivity from the connectivity of the graph data and evaluate the performance of GNNs in a clinical triage use case. Our results demonstrate that incorporating domain knowledge into the GNN connectivity leads to better performance than using the same connectivity as the KG or allowing unconstrained embedding propagation. Moreover, we show that negative edges play a crucial role in achieving good predictions, and that using too many GNN layers can degrade performance.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 在医疗和其他领域中得到广泛应用，这是因为它们可以处理多modal和多关系图。然而，efficiently training GNNs 仍然是一个开放的研究问题，有几个未解决的问题。在这篇论文中，我们研究了在知识图(KGs)中预测链接的情况下，GNNs 中嵌入信息的流动如何影响预测性能。我们提出了一个数学模型，该模型将GNN 连接分离于图数据的连接，并评估了在临床排序用例中GNNs 的性能。我们的结果表明，在GNN 连接中 incorporate 域知识可以提高预测性能，并且使用相同的KG连接或不受限制的嵌入传播也可以提高性能。此外，我们发现，使用负边可以获得好的预测结果，并且使用太多GNN层可以降低性能。
</details></li>
</ul>
<hr>
<h2 id="Verifiable-Fairness-Privacy-preserving-Computation-of-Fairness-for-Machine-Learning-Systems"><a href="#Verifiable-Fairness-Privacy-preserving-Computation-of-Fairness-for-Machine-Learning-Systems" class="headerlink" title="Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems"></a>Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06061">http://arxiv.org/abs/2309.06061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Toreini, Maryam Mehrnezhad, Aad van Moorsel</li>
<li>for: 这篇论文目的是提出一种安全、可靠、隐私保护的 Fairness as a Service (FaaS) 协议，用于计算和验证机器学习 (ML) 模型的公平性。</li>
<li>methods: 该协议使用密文和零知识证明来保证数据和结果的隐私和有效性。它是模型无关的，可以用来审核任何 ML 模型的公平性。</li>
<li>results: 我们实现了 FaaS，并对一个公共数据集进行了实验，证明了它的性能和可行性。<details>
<summary>Abstract</summary>
Fair machine learning is a thriving and vibrant research topic. In this paper, we propose Fairness as a Service (FaaS), a secure, verifiable and privacy-preserving protocol to computes and verify the fairness of any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensure privacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model--agnostic and can support various fairness metrics; hence, it can be used as a service to audit the fairness of any ML model. Our solution requires no trusted third party or private channels for the computation of the fairness metric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g., auditors, social activists and experts, to verify the correctness of the process. We implemented FaaS to investigate performance and demonstrate the successful use of FaaS for a publicly available data set with thousands of entries.
</details>
<details>
<summary>摘要</summary>
《公平机器学习》是一个繁殖的研究领域，在这篇论文中，我们提出了《公平服务（FaaS）》，一种安全、可靠、隐私保护的协议，用于计算和验证任何机器学习（ML）模型的公平性。在FaaS的设计中，数据和结果都是通过密文来保护隐私。此外，零知识证明 garantice了密文和下面数据的正确性。FaaS是模型无关的，可以支持多种公平度量，因此可以作为对任何ML模型的公平性进行审核的服务。我们的解决方案不需要任何不信任第三方或私人通道来计算公平度量。安全保证和承诺都是在安全可靠的方式实现的，从计算开始到结束，每一步都是安全透明的。所有输入数据的密文都是公开可见的，例如，审计人、社会活动人士和专家可以随时查看和验证过程的正确性。我们实现了FaaS，以 investigate性能和 demonstarte其在公共数据集上的成功应用。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Convergence-of-Complexon-Shift-Operators-Extended-Version"><a href="#Frequency-Convergence-of-Complexon-Shift-Operators-Extended-Version" class="headerlink" title="Frequency Convergence of Complexon Shift Operators (Extended Version)"></a>Frequency Convergence of Complexon Shift Operators (Extended Version)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07169">http://arxiv.org/abs/2309.07169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Purui Zhang, Xingchao Jian, Feng Ji, Wee Peng Tay, Bihan Wen</li>
<li>For: 这个论文研究了高阶结构（ simplicial complex）模型的转移性，并使用了一种扩展的高阶 graphon（ complexon）来模型这些结构。* Methods: 作者使用了一种基于 integral operator 的 complexon shift operator（CSO）来研究复杂子复杂体系的特性。他们还研究了 CSO 的 eigenvalues 和 eigenvectors，并与一种新的质量权重Matrix 之间的关系。* Results: 作者证明了，当 simplicial complex sequence  converges to a complexon，then the eigenvalues of the corresponding CSOs converge to that of the limit complexon。这个结论通过一个数值实验得到了证明。这些结果提出了在大 simplicial complex 或 simplicial complex sequence 上进行学习转移性的可能性，这种框架将 graphon signal processing 扩展到更高阶结构上。<details>
<summary>Abstract</summary>
Topological Signal Processing (TSP) utilizes simplicial complexes to model structures with higher order than vertices and edges. In this paper, we study the transferability of TSP via a generalized higher-order version of graphon, known as complexon. We recall the notion of a complexon as the limit of a simplicial complex sequence. Inspired by the integral operator form of graphon shift operators, we construct a marginal complexon and complexon shift operator (CSO) according to components of all possible dimensions from the complexon. We investigate the CSO's eigenvalues and eigenvectors, and relate them to a new family of weighted adjacency matrices. We prove that when a simplicial complex sequence converges to a complexon, the eigenvalues of the corresponding CSOs converge to that of the limit complexon. This conclusion is further verified by a numerical experiment. These results hint at learning transferability on large simplicial complexes or simplicial complex sequences, which generalize the graphon signal processing framework.
</details>
<details>
<summary>摘要</summary>
《拓扑信号处理（TSP）利用 simplicial 复lexes 来模型高阶结构。在本文中，我们研究了 TSP 的传送性，使用一种普遍化高阶 graphon 的概念—— complexon。我们提及 complexon 的定义为 simplicial 复lex 序列的极限。受 graphon Shift 算子的积分Operator 启发，我们构建了 marginal complexon 和 complexon shift operator（CSO），根据所有可能的维度组成部分。我们研究了 CSO 的 eigenvalues 和 eigenvectors，并与它们相关的一个新的加权邻接矩阵。我们证明，当 simplicial 复lex 序列 converge 到 complexon 时，相应的 CSO 的 eigenvalues 会 converge 到 complexon 的限制。这一结论通过数值实验得到了证明。这些结果表明在大 simplicial 复lex 或 simplicial 复lex 序列上进行学习传送性是可能的，这种框架将 graphon 信号处理扩展到更高的维度。》Note: Simplified Chinese is a written language that uses simpler characters and grammar than Traditional Chinese. It is commonly used in mainland China and is the official language of the People's Republic of China.
</details></li>
</ul>
<hr>
<h2 id="A-Perceptron-based-Fine-Approximation-Technique-for-Linear-Separation"><a href="#A-Perceptron-based-Fine-Approximation-Technique-for-Linear-Separation" class="headerlink" title="A Perceptron-based Fine Approximation Technique for Linear Separation"></a>A Perceptron-based Fine Approximation Technique for Linear Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06049">http://arxiv.org/abs/2309.06049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ákos Hajnal</li>
<li>for: 本研究提出了一种新的在线学习方法，旨在找到标注为正或负的数据点之间的分隔平面。</li>
<li>methods: 本方法基于Perceptron算法，但是只在搜索分隔平面时调整神经元的权重。</li>
<li>results: 实验结果表明，该方法可以更高效地than Perceptron算法，特别是当数据集大于数据维度时。<details>
<summary>Abstract</summary>
This paper presents a novel online learning method that aims at finding a separator hyperplane between data points labelled as either positive or negative. Since weights and biases of artificial neurons can directly be related to hyperplanes in high-dimensional spaces, the technique is applicable to train perceptron-based binary classifiers in machine learning. In case of large or imbalanced data sets, use of analytical or gradient-based solutions can become prohibitive and impractical, where heuristics and approximation techniques are still applicable. The proposed method is based on the Perceptron algorithm, however, it tunes neuron weights in just the necessary extent during searching the separator hyperplane. Due to an appropriate transformation of the initial data set we need not to consider data labels, neither the bias term. respectively, reducing separability to a one-class classification problem. The presented method has proven converge; empirical results show that it can be more efficient than the Perceptron algorithm, especially, when the size of the data set exceeds data dimensionality.
</details>
<details>
<summary>摘要</summary>
The proposed method is based on the Perceptron algorithm, but it only tunes the neuron weights to the necessary extent during the search for the separator hyperplane. This is achieved through an appropriate transformation of the initial data set, which eliminates the need to consider data labels or the bias term. As a result, the method reduces the separability problem to a one-class classification problem.Empirical results show that the proposed method is more efficient than the Perceptron algorithm, especially when the size of the data set exceeds the dimensionality of the data. The method has been proven to converge, and the results demonstrate its effectiveness in finding the optimal separator hyperplane.
</details></li>
</ul>
<hr>
<h2 id="Normality-Learning-based-Graph-Anomaly-Detection-via-Multi-Scale-Contrastive-Learning"><a href="#Normality-Learning-based-Graph-Anomaly-Detection-via-Multi-Scale-Contrastive-Learning" class="headerlink" title="Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning"></a>Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06034">http://arxiv.org/abs/2309.06034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felixdjc/nlgad">https://github.com/felixdjc/nlgad</a></li>
<li>paper_authors: Jingcan Duan, Pei Zhang, Siwei Wang, Jingtao Hu, Hu Jin, Jiaxin Zhang, Haifang Zhou, Haifang Zhou</li>
<li>for: 这篇论文的目的是提出一个基于多尺度对照学习网络的异常GRAPH检测方法（GAD），以提高检测性能。</li>
<li>methods: 本文使用了多尺度对照学习网络（Contrastive Learning Networks，CLN）来学习正常模式，并将其应用于GAD中。具体来说，首先将CLN initialization在不同尺度上，然后设计了一个有效的混合策略来选择正常节点。最后，模型仅对可靠的正常节点进行调整，以学习更加精确的正常模式，以便更好地识别异常节点。</li>
<li>results: 实验结果显示，提出的方法可以提高GAD的检测性能（最高提升5.89%的AUC），相比之前的方法。code可以在<a target="_blank" rel="noopener" href="https://github.com/FelixDJC/NLGAD%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/FelixDJC/NLGAD中下载。</a><details>
<summary>Abstract</summary>
Graph anomaly detection (GAD) has attracted increasing attention in machine learning and data mining. Recent works have mainly focused on how to capture richer information to improve the quality of node embeddings for GAD. Despite their significant advances in detection performance, there is still a relative dearth of research on the properties of the task. GAD aims to discern the anomalies that deviate from most nodes. However, the model is prone to learn the pattern of normal samples which make up the majority of samples. Meanwhile, anomalies can be easily detected when their behaviors differ from normality. Therefore, the performance can be further improved by enhancing the ability to learn the normal pattern. To this end, we propose a normality learning-based GAD framework via multi-scale contrastive learning networks (NLGAD for abbreviation). Specifically, we first initialize the model with the contrastive networks on different scales. To provide sufficient and reliable normal nodes for normality learning, we design an effective hybrid strategy for normality selection. Finally, the model is refined with the only input of reliable normal nodes and learns a more accurate estimate of normality so that anomalous nodes can be more easily distinguished. Eventually, extensive experiments on six benchmark graph datasets demonstrate the effectiveness of our normality learning-based scheme on GAD. Notably, the proposed algorithm improves the detection performance (up to 5.89% AUC gain) compared with the state-of-the-art methods. The source code is released at https://github.com/FelixDJC/NLGAD.
</details>
<details>
<summary>摘要</summary>
《图像异常检测（GAD）在机器学习和数据挖掘领域受到了越来越多的关注。 latest works mainly focus on how to capture richer information to improve the quality of node embeddings for GAD. Despite their significant advances in detection performance, there is still a relative dearth of research on the properties of the task. GAD aims to discern the anomalies that deviate from most nodes. However, the model is prone to learn the pattern of normal samples which make up the majority of samples. Meanwhile, anomalies can be easily detected when their behaviors differ from normality. Therefore, the performance can be further improved by enhancing the ability to learn the normal pattern. To this end, we propose a normality learning-based GAD framework via multi-scale contrastive learning networks (NLGAD for abbreviation). Specifically, we first initialize the model with the contrastive networks on different scales. To provide sufficient and reliable normal nodes for normality learning, we design an effective hybrid strategy for normality selection. Finally, the model is refined with the only input of reliable normal nodes and learns a more accurate estimate of normality so that anomalous nodes can be more easily distinguished. Eventually, extensive experiments on six benchmark graph datasets demonstrate the effectiveness of our normality learning-based scheme on GAD. Notably, the proposed algorithm improves the detection performance (up to 5.89% AUC gain) compared with the state-of-the-art methods. The source code is released at https://github.com/FelixDJC/NLGAD.》Note: "GAD" in the text refers to "Graph Anomaly Detection".
</details></li>
</ul>
<hr>
<h2 id="Energy-Aware-Federated-Learning-with-Distributed-User-Sampling-and-Multichannel-ALOHA"><a href="#Energy-Aware-Federated-Learning-with-Distributed-User-Sampling-and-Multichannel-ALOHA" class="headerlink" title="Energy-Aware Federated Learning with Distributed User Sampling and Multichannel ALOHA"></a>Energy-Aware Federated Learning with Distributed User Sampling and Multichannel ALOHA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06033">http://arxiv.org/abs/2309.06033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Valente da Silva, Onel L. Alcaraz López, Richard Demo Souza</li>
<li>for: 这篇论文是针对分布式学习在边缘设备上的能源有效性问题进行研究。</li>
<li>methods: 本论文使用了多频道ALOHA协议，并提出了一种方法来保证低能源缺席概率和未来任务的成功执行。</li>
<li>results: numerical results表明这种方法在某些重要的设置下具有更好的优化性和电池水平，并且比一种 нор  based 解决方案更快地训练。<details>
<summary>Abstract</summary>
Distributed learning on edge devices has attracted increased attention with the advent of federated learning (FL). Notably, edge devices often have limited battery and heterogeneous energy availability, while multiple rounds are required in FL for convergence, intensifying the need for energy efficiency. Energy depletion may hinder the training process and the efficient utilization of the trained model. To solve these problems, this letter considers the integration of energy harvesting (EH) devices into a FL network with multi-channel ALOHA, while proposing a method to ensure both low energy outage probability and successful execution of future tasks. Numerical results demonstrate the effectiveness of this method, particularly in critical setups where the average energy income fails to cover the iteration cost. The method outperforms a norm based solution in terms of convergence time and battery level.
</details>
<details>
<summary>摘要</summary>
随着联合学习（FL）的出现，分布式学习在边缘设备上已经吸引了更多的注意力。然而，边缘设备通常具有有限的电池和多样化的能源供应，而多轮FL的需求会加剧能效率问题。如果不得要遇到能源枯竭，它会阻碍训练过程和模型的有效使用。为解决这些问题，本文考虑了在FL网络中 интеGRATE了能量收集（EH）设备，并提出了一种方法来保证低能源停机概率和未来任务的成功执行。numerical results表明该方法的效果，特别在 average energy income 不足以覆盖迭代成本的情况下。该方法在 convergence time 和电池水平方面也超越了 norm 基于解决方案。
</details></li>
</ul>
<hr>
<h2 id="Emergent-Communication-in-Multi-Agent-Reinforcement-Learning-for-Future-Wireless-Networks"><a href="#Emergent-Communication-in-Multi-Agent-Reinforcement-Learning-for-Future-Wireless-Networks" class="headerlink" title="Emergent Communication in Multi-Agent Reinforcement Learning for Future Wireless Networks"></a>Emergent Communication in Multi-Agent Reinforcement Learning for Future Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06021">http://arxiv.org/abs/2309.06021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marwa Chafii, Salmane Naoumi, Reda Alami, Ebtesam Almazrouei, Mehdi Bennis, Merouane Debbah</li>
<li>for: 本文探讨了Future 6G无线网络中多个网络实体之间的合作问题，以实现最小延迟和能耗的方式解决高维数据的交换问题。</li>
<li>methods: 本文提出了一种基于多代理学习和自然通信（EC-MARL）的解决方案，可以在不可见状态下实现高维连续控制问题的解决。</li>
<li>results: 本文介绍了EC-MARL在Future 6G无线网络中的应用潜在性和研究机遇，包括自动驾驶、机器人导航、飞行基站网络规划和智能城市应用。<details>
<summary>Abstract</summary>
In different wireless network scenarios, multiple network entities need to cooperate in order to achieve a common task with minimum delay and energy consumption. Future wireless networks mandate exchanging high dimensional data in dynamic and uncertain environments, therefore implementing communication control tasks becomes challenging and highly complex. Multi-agent reinforcement learning with emergent communication (EC-MARL) is a promising solution to address high dimensional continuous control problems with partially observable states in a cooperative fashion where agents build an emergent communication protocol to solve complex tasks. This paper articulates the importance of EC-MARL within the context of future 6G wireless networks, which imbues autonomous decision-making capabilities into network entities to solve complex tasks such as autonomous driving, robot navigation, flying base stations network planning, and smart city applications. An overview of EC-MARL algorithms and their design criteria are provided while presenting use cases and research opportunities on this emerging topic.
</details>
<details>
<summary>摘要</summary>
不同无线网络enario中，多个网络元件需要合作以实现最小的延迟和能源消耗，实现高维度资料交换。未来的无线网络将实施高维度连续控制问题，因此通信控制任务将变得更加困难和复杂。多智能推劝学习（EC-MARL）是一种可能的解决方案，它可以在不可见的状态下，通过协调多个智能推劝学习代理人，解决复杂的控制问题。本文说明了EC-MARL在未来6G无线网络中的重要性，具体来说，它将具有自主决策能力，实现无人驾驶、机器人导航、飞行基站网络规划和智慧城市应用等复杂任务。文中还提供了EC-MARL算法和设计需求，以及实际应用和研究机会。
</details></li>
</ul>
<hr>
<h2 id="Interpolation-Approximation-and-Controllability-of-Deep-Neural-Networks"><a href="#Interpolation-Approximation-and-Controllability-of-Deep-Neural-Networks" class="headerlink" title="Interpolation, Approximation and Controllability of Deep Neural Networks"></a>Interpolation, Approximation and Controllability of Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06015">http://arxiv.org/abs/2309.06015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingpu Cheng, Qianxiao Li, Ting Lin, Zuowei Shen</li>
<li>for:  investigate the expressive power of deep residual neural networks idealized as continuous dynamical systems through control theory.</li>
<li>methods:  consider two properties from supervised learning: universal interpolation and universal approximation, and give a characterization of universal interpolation.</li>
<li>results:  show that universal interpolation holds for essentially any architecture with non-linearity, and elucidate the relationship between universal interpolation and universal approximation in the context of general control systems.<details>
<summary>Abstract</summary>
We investigate the expressive power of deep residual neural networks idealized as continuous dynamical systems through control theory. Specifically, we consider two properties that arise from supervised learning, namely universal interpolation - the ability to match arbitrary input and target training samples - and the closely related notion of universal approximation - the ability to approximate input-target functional relationships via flow maps. Under the assumption of affine invariance of the control family, we give a characterisation of universal interpolation, showing that it holds for essentially any architecture with non-linearity. Furthermore, we elucidate the relationship between universal interpolation and universal approximation in the context of general control systems, showing that the two properties cannot be deduced from each other. At the same time, we identify conditions on the control family and the target function that ensures the equivalence of the two notions.
</details>
<details>
<summary>摘要</summary>
我们研究深度径 residual神经网络作为连续动力系统的表达力，通过控制理论进行调查。具体来说，我们考虑了两个从supervised learning中得到的性质， namely universal interpolation和 closely related universal approximation。我们假设控制家族具有平移变换的可变性，然后给出universal interpolation的特征化，证明它对于任意架构都成立。此外，我们还解释了universal interpolation和 universal approximation在通用控制系统中的关系，并证明这两个性质之间无法从一个到另一个推导。同时，我们还提出了控制家族和目标函数的条件，以确保这两个概念之间的等价性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Unbiased-News-Article-Representations-A-Knowledge-Infused-Approach"><a href="#Learning-Unbiased-News-Article-Representations-A-Knowledge-Infused-Approach" class="headerlink" title="Learning Unbiased News Article Representations: A Knowledge-Infused Approach"></a>Learning Unbiased News Article Representations: A Knowledge-Infused Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05981">http://arxiv.org/abs/2309.05981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadia Kamal, Jimmy Hartford, Jeremy Willis, Arunkumar Bagavathi</li>
<li>for: This paper aims to quantify the political leaning of online news articles and mitigate the algorithmic political bias in machine learning models used for this task.</li>
<li>methods: The proposed knowledge-infused deep learning model uses relatively reliable external data resources to learn unbiased representations of news articles based on their global and local contexts.</li>
<li>results: The proposed model outperforms baseline methods to predict the political leaning of news articles with up to 73% accuracy, mitigating algorithmic political bias.Here’s the Chinese translation of the three pieces of information:</li>
<li>for: 这篇论文目的是量化在线新闻文章的政治倾向，并使用机器学习模型来mitigate这种algorithmic political bias。</li>
<li>methods: 提议的知识汇集深度学习模型使用相对可靠的外部数据资源来学习不受政治偏见的新闻文章表示。</li>
<li>results: 提议的模型在测试集中的准确率达到73%，比基eline方法高。<details>
<summary>Abstract</summary>
Quantification of the political leaning of online news articles can aid in understanding the dynamics of political ideology in social groups and measures to mitigating them. However, predicting the accurate political leaning of a news article with machine learning models is a challenging task. This is due to (i) the political ideology of a news article is defined by several factors, and (ii) the innate nature of existing learning models to be biased with the political bias of the news publisher during the model training. There is only a limited number of methods to study the political leaning of news articles which also do not consider the algorithmic political bias which lowers the generalization of machine learning models to predict the political leaning of news articles published by any new news publishers. In this work, we propose a knowledge-infused deep learning model that utilizes relatively reliable external data resources to learn unbiased representations of news articles using their global and local contexts. We evaluate the proposed model by setting the data in such a way that news domains or news publishers in the test set are completely unseen during the training phase. With this setup we show that the proposed model mitigates algorithmic political bias and outperforms baseline methods to predict the political leaning of news articles with up to 73% accuracy.
</details>
<details>
<summary>摘要</summary>
政治倾向量化在在线新闻文章中可以帮助我们理解社会团体中政治 идеологи的动态和 mitigate其中的问题。然而，使用机器学习模型预测新闻文章的政治倾向是一项具有挑战性的任务。这是因为（i）政治 идеологи的定义是由多个因素组成，（ii）现有的学习模型具有新闻发布商的政治偏见，从而降低了机器学习模型预测新闻文章的政治倾向的泛化性。目前只有有限的方法可以研究新闻文章的政治倾向，而且这些方法不考虑算法政治偏见。在这项工作中，我们提出一种知识感知深度学习模型，该模型使用可靠的外部数据资源来学习不受偏见的新闻文章表示。我们通过将训练集中的新闻域或新闻发布商完全不见于测试集来评估该模型。 results show that our proposed model can mitigate algorithmic political bias and outperform baseline methods in predicting the political leaning of news articles with up to 73% accuracy.
</details></li>
</ul>
<hr>
<h2 id="CleanUNet-2-A-Hybrid-Speech-Denoising-Model-on-Waveform-and-Spectrogram"><a href="#CleanUNet-2-A-Hybrid-Speech-Denoising-Model-on-Waveform-and-Spectrogram" class="headerlink" title="CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram"></a>CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05975">http://arxiv.org/abs/2309.05975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifeng Kong, Wei Ping, Ambrish Dantrey, Bryan Catanzaro</li>
<li>for: 这个论文是为了提出一种 integrate waveform denoiser 和 spectrogram denoiser 的 speech denoising模型，以提高 speech denoising 的性能。</li>
<li>methods: 该模型使用了 two-stage 框架，首先使用 waveform 模型来生成清晰的 speech waveform，然后使用 spectrogram 模型来生成高质量的 spectrogram，并将两者组合起来进行 denoising。</li>
<li>results: 对于多种 objective 和 subjective 评估指标，CleanUNet 2 的性能都高于先前的方法，并且可以在不同的 speech 质量和频率范围下提供高质量的 denoising 结果。<details>
<summary>Abstract</summary>
In this work, we present CleanUNet 2, a speech denoising model that combines the advantages of waveform denoiser and spectrogram denoiser and achieves the best of both worlds. CleanUNet 2 uses a two-stage framework inspired by popular speech synthesis methods that consist of a waveform model and a spectrogram model. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art waveform denoiser, and further boosts its performance by taking predicted spectrograms from a spectrogram denoiser as the input. We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍CleanUNet 2，一种混合波形去噪器和spectrogram去噪器的语音去噪模型，实现了两者的优点。CleanUNet 2采用了两个阶段框架，受到流行的语音合成方法的启发，包括波形模型和spectrogram模型。具体来说，CleanUNet 2基于CleanUNet，当前的波形去噪器顶峰性能，再加以使用预测的spectrogram去噪器输入，进一步提高其性能。我们展示了CleanUNet 2在多个对象和主观评价中的优越性。
</details></li>
</ul>
<hr>
<h2 id="Neural-Network-Layer-Matrix-Decomposition-reveals-Latent-Manifold-Encoding-and-Memory-Capacity"><a href="#Neural-Network-Layer-Matrix-Decomposition-reveals-Latent-Manifold-Encoding-and-Memory-Capacity" class="headerlink" title="Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity"></a>Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05968">http://arxiv.org/abs/2309.05968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ng Shyh-Chang, A-Li Luo, Bo Qiu</li>
<li>for: 这项研究证明了神经网络（NN）编码定理的对偶，即每个稳定地聚合的NN中的权重矩阵实际上编码了一个连续函数，该函数在训练集中的bounded域内准确地approximates the training dataset.</li>
<li>methods: 该研究使用了Eckart-Young定理和减少特征值分解来描述NN层的weight矩阵，从而理解 latent space manifold 的结构和NN层中的数学运算的几何特性。</li>
<li>results: 研究发现，NN可以通过储存容量来破坏维度约束，并且这两者是补偿的。此外，层矩阵分解（LMD）还发现了神经网络层的归一化矩阵和Hopfield网络和Transformer NN模型的最新发展之间的密切关系。<details>
<summary>Abstract</summary>
We prove the converse of the universal approximation theorem, i.e. a neural network (NN) encoding theorem which shows that for every stably converged NN of continuous activation functions, its weight matrix actually encodes a continuous function that approximates its training dataset to within a finite margin of error over a bounded domain. We further show that using the Eckart-Young theorem for truncated singular value decomposition of the weight matrix for every NN layer, we can illuminate the nature of the latent space manifold of the training dataset encoded and represented by every NN layer, and the geometric nature of the mathematical operations performed by each NN layer. Our results have implications for understanding how NNs break the curse of dimensionality by harnessing memory capacity for expressivity, and that the two are complementary. This Layer Matrix Decomposition (LMD) further suggests a close relationship between eigen-decomposition of NN layers and the latest advances in conceptualizations of Hopfield networks and Transformer NN models.
</details>
<details>
<summary>摘要</summary>
我们证明了射频学 theorem 的对偶，即一个神经网络（NN）编码 theorem，表明每个稳定地收敛的 NN 的权重矩阵实际上编码了一个连续函数，该函数在培训数据集中的 bounded edomain 内对数据集进行approximation，并且有finite 的误差。我们还证明了，通过 truncated singular value decomposition（Eckart-Young 定理）的 weight matrix 对每个 NN 层，可以照明 latent space manifold 被NN层所编码和表示的训练数据集的结构和 mathe matical 操作的几何特性。我们的结论有关于如何NN突破维度约束，通过吸收记忆来提高表达力，以及这两者之间的关系。此外，我们的层矩阵分解（LMD）还 suggets一种close relationship  между NN层的eigen-decomposition和最新的 Hopfield 网络和Transformer NN 模型的概念化。
</details></li>
</ul>
<hr>
<h2 id="GLAD-Content-aware-Dynamic-Graphs-For-Log-Anomaly-Detection"><a href="#GLAD-Content-aware-Dynamic-Graphs-For-Log-Anomaly-Detection" class="headerlink" title="GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection"></a>GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05953">http://arxiv.org/abs/2309.05953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yul091/GraphLogAD">https://github.com/yul091/GraphLogAD</a></li>
<li>paper_authors: Yufei Li, Yanchi Liu, Haoyu Wang, Zhengzhang Chen, Wei Cheng, Yuncong Chen, Wenchao Yu, Haifeng Chen, Cong Liu</li>
<li>for: 本研究旨在探讨系统日志中异常检测，尤其是考虑系统组件之间的关系，以优化异常检测和原因探测。</li>
<li>methods: 本研究提出了一种基于图的日志异常检测框架（GLAD），它将日志 semantics、关系模式和时序模式综合考虑到异常检测中。GLAD包括日志内容提取模块、动态日志图构建模块和时序注意力图 anomaly detection模型。</li>
<li>results: 在三个数据集上测试 GLAD，结果表明 GLAD 能够有效地检测异常，异常的关系模式也能够被识别出来。<details>
<summary>Abstract</summary>
Logs play a crucial role in system monitoring and debugging by recording valuable system information, including events and states. Although various methods have been proposed to detect anomalies in log sequences, they often overlook the significance of considering relations among system components, such as services and users, which can be identified from log contents. Understanding these relations is vital for detecting anomalies and their underlying causes. To address this issue, we introduce GLAD, a Graph-based Log Anomaly Detection framework designed to detect relational anomalies in system logs. GLAD incorporates log semantics, relational patterns, and sequential patterns into a unified framework for anomaly detection. Specifically, GLAD first introduces a field extraction module that utilizes prompt-based few-shot learning to identify essential fields from log contents. Then GLAD constructs dynamic log graphs for sliding windows by interconnecting extracted fields and log events parsed from the log parser. These graphs represent events and fields as nodes and their relations as edges. Subsequently, GLAD utilizes a temporal-attentive graph edge anomaly detection model for identifying anomalous relations in these dynamic log graphs. This model employs a Graph Neural Network (GNN)-based encoder enhanced with transformers to capture content, structural and temporal features. We evaluate our proposed method on three datasets, and the results demonstrate the effectiveness of GLAD in detecting anomalies indicated by varying relational patterns.
</details>
<details>
<summary>摘要</summary>
系统监控和调试中，日志扮演着关键角色，记录了系统中的重要信息，包括事件和状态。虽然有很多方法用于检测日志序列中的异常，但它们通常忽略了考虑系统组件之间的关系，例如服务和用户，这些关系可以从日志内容中得到。了解这些关系非常重要，可以帮助检测异常和其下面的原因。为解决这个问题，我们提出了 GLAD，一个基于图的日志异常检测框架，可以检测系统日志中的关系异常。GLAD 将日志Semantics、关系模式和时序模式集成到一个统一的异常检测框架中。具体来说，GLAD 首先引入一个字段提取模块，使用提示based few-shot learning来确定日志内容中的重要字段。然后，GLAD 构建了动态日志图，将提取的字段和日志事件与日志解析器解析的日志事件相连接。这些图表示事件和字段作为节点，以及它们之间的关系作为边。接着，GLAD 利用一个 temporal-attentive 图边异常检测模型来检测动态日志图中的异常关系。这个模型使用图神经网络（GNN）基本encoder和转换器来捕捉内容、结构和时序特征。我们对 GLAD 进行了三个数据集的测试，结果表明 GLAD 能够根据不同的关系模式检测异常。
</details></li>
</ul>
<hr>
<h2 id="Quantized-Non-Volatile-Nanomagnetic-Synapse-based-Autoencoder-for-Efficient-Unsupervised-Network-Anomaly-Detection"><a href="#Quantized-Non-Volatile-Nanomagnetic-Synapse-based-Autoencoder-for-Efficient-Unsupervised-Network-Anomaly-Detection" class="headerlink" title="Quantized Non-Volatile Nanomagnetic Synapse based Autoencoder for Efficient Unsupervised Network Anomaly Detection"></a>Quantized Non-Volatile Nanomagnetic Synapse based Autoencoder for Efficient Unsupervised Network Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06449">http://arxiv.org/abs/2309.06449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Sabbir Alam, Walid Al Misba, Jayasimha Atulasimha</li>
<li>for: 本研究旨在提出一种基于自编码器的异常检测方法，并在边缘设备中实现实时学习，以解决边缘设备的限制性。</li>
<li>methods: 该方法使用低分辨率非朋素储存器 synapse 和有效的量化神经网络学习算法，并利用磁导轨道上引入的磁界墙（DW）来实现非朋素储存器 synapse。</li>
<li>results: 该方法在 NSL-KDD 数据集上进行异常检测，并证明了对异常检测的改进（90.98%），并且在训练过程中减少了至少三个数量级的weight更新，从而实现了显著的能源节省。<details>
<summary>Abstract</summary>
In the autoencoder based anomaly detection paradigm, implementing the autoencoder in edge devices capable of learning in real-time is exceedingly challenging due to limited hardware, energy, and computational resources. We show that these limitations can be addressed by designing an autoencoder with low-resolution non-volatile memory-based synapses and employing an effective quantized neural network learning algorithm. We propose a ferromagnetic racetrack with engineered notches hosting a magnetic domain wall (DW) as the autoencoder synapses, where limited state (5-state) synaptic weights are manipulated by spin orbit torque (SOT) current pulses. The performance of anomaly detection of the proposed autoencoder model is evaluated on the NSL-KDD dataset. Limited resolution and DW device stochasticity aware training of the autoencoder is performed, which yields comparable anomaly detection performance to the autoencoder having floating-point precision weights. While the limited number of quantized states and the inherent stochastic nature of DW synaptic weights in nanoscale devices are known to negatively impact the performance, our hardware-aware training algorithm is shown to leverage these imperfect device characteristics to generate an improvement in anomaly detection accuracy (90.98%) compared to accuracy obtained with floating-point trained weights. Furthermore, our DW-based approach demonstrates a remarkable reduction of at least three orders of magnitude in weight updates during training compared to the floating-point approach, implying substantial energy savings for our method. This work could stimulate the development of extremely energy efficient non-volatile multi-state synapse-based processors that can perform real-time training and inference on the edge with unsupervised data.
</details>
<details>
<summary>摘要</summary>
在基于自适应器的异常检测 paradigm中，在边缘设备中实现自适应器是极其挑战性的，主要是因为边缘设备的硬件、能源和计算资源有限。我们表明，这些限制可以通过设计一个具有低分辨率、不可变存储器 synapse的 autoencoder，并使用有效的量化神经网络学习算法来解决。我们提议一种磁気轨道上的磁Domain墙（DW）作为 autoencoder synapse，其中有限状态（5状） synaptic  веса通过磁力辐射（SOT）电流脉冲来 manipulate。我们对提议的 autoencoder 模型在 NSL-KDD 数据集上进行异常检测性能的评估。我们采用了限制分辨率和 DW 设备不确定性的意识training autoencoder，而不是使用浮点数精度 weights。虽然有限数量的量化状态和 nanoscale 设备内的固有不确定性会对性能产生负面影响，但我们的硬件意识训练算法可以利用这些不完美设备特性来提高异常检测精度（90.98%）相比浮点数训练 weights。此外，我们的 DW 方法显示在训练期间对 weight updates 的减少是至少三个数量级，这意味着我们的方法可以获得显著的能源抑制。这种工作可能会促进非常能效的非易失multi-state synapse基于处理器的开发，以便在边缘上进行实时训练和推理，并使用不supervised数据。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/12/cs.LG_2023_09_12/" data-id="clp869u0900rhk588acdig4ky" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/12/eess.IV_2023_09_12/" class="article-date">
  <time datetime="2023-09-12T09:00:00.000Z" itemprop="datePublished">2023-09-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/12/eess.IV_2023_09_12/">eess.IV - 2023-09-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Post-processing-of-Diffusion-Tensor-Cardiac-Magnetic-Imaging-Using-Texture-conserving-Deformable-Registration"><a href="#Efficient-Post-processing-of-Diffusion-Tensor-Cardiac-Magnetic-Imaging-Using-Texture-conserving-Deformable-Registration" class="headerlink" title="Efficient Post-processing of Diffusion Tensor Cardiac Magnetic Imaging Using Texture-conserving Deformable Registration"></a>Efficient Post-processing of Diffusion Tensor Cardiac Magnetic Imaging Using Texture-conserving Deformable Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06598">http://arxiv.org/abs/2309.06598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanwen Wang, Pedro F. Ferreira, Yinzhe Wu, Andrew D. Scott, Camila Munoz, Ke Wen, Yaqing Luo, Jiahao Huang, Sonia Nielles-Vallespin, Dudley J. Pennell, Guang Yang</li>
<li>For: 提供非侵入式的心肺功能测量方法* Methods: 使用深度学习基于B-spline网络对DT-CMR图像进行抗干扰注射注射的抽象注射注射* Results: 提高了图像使用效率、手动剪辑和计算速度<details>
<summary>Abstract</summary>
Diffusion tensor based cardiac magnetic resonance (DT-CMR) is a method capable of providing non-invasive measurements of myocardial microstructure. Image registration is essential to correct image shifts due to intra and inter breath-hold motion. Registration is challenging in DT-CMR due to the low signal-to-noise and various contrasts induced by the diffusion encoding in the myocardial and surrounding organs. Traditional deformable registration destroys the texture information while rigid registration inefficiently discards frames with local deformation. In this study, we explored the possibility of deep learning-based deformable registration on DT- CMR. Based on the noise suppression using low-rank features and diffusion encoding suppression using variational auto encoder-decoder, a B-spline based registration network extracted the displacement fields and maintained the texture features of DT-CMR. In this way, our method improved the efficiency of frame utilization, manual cropping, and computational speed.
</details>
<details>
<summary>摘要</summary>
Diffusion tensor based cardiac magnetic resonance (DT-CMR) 是一种可以提供非侵入式的肌肉微结构测量方法。图像匹配是必要的，以正确补做图像偏移 Due to intra-和inter- breath-hold 运动。但是，在 DT-CMR 中，匹配是具有挑战性，因为Diffusion encoding在肌肉和周围的器官中induced的低信号至噪声和多种对比。传统的可变形注册会destroys the texture information，而rigid注册则不 efficiently discards frames with local deformation。在这种研究中，我们探索了深度学习基于的可变形注册方法在 DT-CMR 中。基于噪声抑制使用低级特征和Diffusion encoding抑制使用自适应变换器-解码器，一个基于 B-spline 的注册网络提取出了displacement fields 并保留了 DT-CMR 中的 texture features。这种方法可以提高帧使用效率、手动剪辑和计算速度。
</details></li>
</ul>
<hr>
<h2 id="RGB-Guided-Resolution-Enhancement-of-IR-Images"><a href="#RGB-Guided-Resolution-Enhancement-of-IR-Images" class="headerlink" title="RGB-Guided Resolution Enhancement of IR Images"></a>RGB-Guided Resolution Enhancement of IR Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05996">http://arxiv.org/abs/2309.05996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Trammer, Nils Genser, Jürgen Seiler</li>
<li>for: 提高低分辨率红外图像的空间分辨率</li>
<li>methods: 使用多摄像头、色彩摄像头和红外摄像头，并使用GIRRE方法进行RGB响应导向的红外分辨率提高</li>
<li>results: 比州前方法提高1.2dB至1.8dB的PSNR值，可见地提高图像的分辨率<details>
<summary>Abstract</summary>
This paper introduces a novel method for RGB-Guided Resolution Enhancement of infrared (IR) images called Guided IR Resolution Enhancement (GIRRE). In the area of single image super resolution (SISR) there exists a wide variety of algorithms like interpolation methods or neural networks to improve the spatial resolution of images. In contrast to SISR, even more information can be gathered on the recorded scene when using multiple cameras. In our setup, we are dealing with multi image super resolution, especially with stereo super resolution. We consider a color camera and an IR camera. Current IR sensors have a very low resolution compared to color sensors so that recent color sensors take up 100 times more pixels than IR sensors. To this end, GIRRE increases the spatial resolution of the low-resolution IR image. After that, the upscaled image is filtered with the aid of the high-resolution color image. We show that our method achieves an average PSNR gain of 1.2 dB and at best up to 1.8 dB compared to state-of-the-art methods, which is visually noticeable.
</details>
<details>
<summary>摘要</summary>
Here is the Simplified Chinese translation of the text:这篇论文提出了一种基于RGB指导的红外图像分辨率提高方法，称为指导红外分辨率提高（GIRRE）。与传统单张图像超解（SISR）方法不同的是，GIRRE利用多个摄像头来提高低分辨率红外图像的空间分辨率。具体来说，我们使用了一个颜色摄像头和一个红外摄像头，高分辨率颜色图像用于指导低分辨率红外图像的扩展。我们的方法实现了平均PSNR提升1.2dB，最高可达1.8dB compared to state-of-the-art方法，可见程度有所提高。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/12/eess.IV_2023_09_12/" data-id="clp869u7701a2k5888fpsfxsr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/12/eess.SP_2023_09_12/" class="article-date">
  <time datetime="2023-09-12T08:00:00.000Z" itemprop="datePublished">2023-09-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/12/eess.SP_2023_09_12/">eess.SP - 2023-09-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mobile-Object-Tracking-in-Panoramic-Video-and-LiDAR-for-Radiological-Source-Object-Attribution-and-Improved-Source-Detection"><a href="#Mobile-Object-Tracking-in-Panoramic-Video-and-LiDAR-for-Radiological-Source-Object-Attribution-and-Improved-Source-Detection" class="headerlink" title="Mobile Object Tracking in Panoramic Video and LiDAR for Radiological Source-Object Attribution and Improved Source Detection"></a>Mobile Object Tracking in Panoramic Video and LiDAR for Radiological Source-Object Attribution and Improved Source Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06592">http://arxiv.org/abs/2309.06592</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. R. Marshall, R. J. Cooper, J. C. Curtis, D. Hellfeld, T. H. Y. Joshi, M. Salathe, K. Vetter</li>
<li>for: 这项研究旨在使用计算机视觉技术来补充移动探测器系统中的放射学数据，以提高探测敏感度和场景认知。</li>
<li>methods: 研究使用计算机视觉技术来分析流动人员或车辆的动态行为，并将其与放射学警报相关联以增强探测敏感度和场景认知。</li>
<li>results: 研究在模拟 urbana 环境中进行了实验，并结果表明，通过使用计算机视觉技术可以增强探测敏感度和场景认知。<details>
<summary>Abstract</summary>
The addition of contextual sensors to mobile radiation sensors provides valuable information about radiological source encounters that can assist in adjudication of alarms. This study explores how computer-vision based object detection and tracking analyses can be used to augment radiological data from a mobile detector system. We study how contextual information (streaming video and LiDAR) can be used to associate dynamic pedestrians or vehicles with radiological alarms to enhance both situational awareness and detection sensitivity. Possible source encounters were staged in a mock urban environment where participants included pedestrians and vehicles moving in the vicinity of an intersection. Data was collected with a vehicle equipped with 6 NaI(Tl) 2 inch times 4 inch times 16 inch detectors in a hexagonal arrangement and multiple cameras, LiDARs, and an IMU. Physics-based models that describe the expected count rates from tracked objects are used to correlate vehicle and/or pedestrian trajectories to measured count-rate data through the use of Poisson maximum likelihood estimation and to discern between source-carrying and non-source-carrying objects. In this work, we demonstrate the capabilities of our source-object attribution approach as applied to a mobile detection system in the presence of moving sources to improve both detection sensitivity and situational awareness in a mock urban environment.
</details>
<details>
<summary>摘要</summary>
通过添加上下文感知器到移动辐射检测器，可以获得辐射源遇到的有价值信息，以帮助解决警报。本研究探讨了如何使用计算机视觉基于对象检测和跟踪分析来增强移动检测系统中的辐射数据。我们研究了如何使用上下文信息（流动视频和LiDAR）将动态行人或车辆与辐射警报相关联，以提高 situational awareness 和检测敏感度。在模拟城市环境中，参与者包括行人和车辆在交叉口附近移动。数据收集使用装备了6个 NaI(Tl) 2英寸×4英寸×16英寸检测器的车辆，以及多个摄像头、LiDAR和IMU。我们使用物理学基于的模型来 correlate 跟踪物体的轨迹和测量的计数率数据，并使用波利奥最大 likelihood 估计来准确归类源包含和不包含源的物体。在这种情况下，我们展示了我们的源-物体归类方法在移动检测系统中的应用，以提高检测敏感度和 situational awareness 在模拟城市环境中。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-topology-inference-on-partially-known-networks-from-input-output-pairs"><a href="#Bayesian-topology-inference-on-partially-known-networks-from-input-output-pairs" class="headerlink" title="Bayesian topology inference on partially known networks from input-output pairs"></a>Bayesian topology inference on partially known networks from input-output pairs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06532">http://arxiv.org/abs/2309.06532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tenceto/inference_langevin">https://github.com/tenceto/inference_langevin</a></li>
<li>paper_authors: Martín Sevilla, Santiago Segarra</li>
<li>for: 这个论文是为了研究系统识别的sampling算法。</li>
<li>methods: 这个算法基于冷却扩散原理，可以从 posterior 分布中 draw 样本，而不是使用传统的最大 likelihood 点估计。</li>
<li>results: 通过对实际和Synthetic网络的数据进行数值实验， authors 示出了 integrating prior knowledge 可以提高估计性能。<details>
<summary>Abstract</summary>
We propose a sampling algorithm to perform system identification from a set of input-output graph signal pairs. The dynamics of the systems we study are given by a partially known adjacency matrix and a generic parametric graph filter of unknown parameters. The methodology we employ is built upon the principles of annealed Langevin diffusion. This enables us to draw samples from the posterior distribution instead of following the classical approach of point estimation using maximum likelihood. We investigate how to harness the prior information inherent in a dataset of graphs of different sizes through the utilization of graph neural networks. We demonstrate, via numerical experiments involving both real-world and synthetic networks, that integrating prior knowledge into the estimation process enhances estimation performance.
</details>
<details>
<summary>摘要</summary>
我们提出一种采样算法来进行系统识别从输入-输出图信号对的集合。我们研究的系统动力是由一个部分知道的邻接矩阵和一个通用参数图 filter 的未知参数给出的。我们使用渐进的兰格易托 diffusion 的原则来实现采样，这使得我们可以从 posterior 分布中采样而不是使用传统的点估计方法。我们研究如何在不同大小的图 dataset 中利用图神经网络来加持先验知识。我们通过实验表明，通过将先验知识 integrate 到估计过程中，可以提高估计性能。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Design-and-Implementation-of-DC-to-5-MHz-Wide-Bandwidth-High-Power-High-Fidelity-Converter"><a href="#Design-and-Implementation-of-DC-to-5-MHz-Wide-Bandwidth-High-Power-High-Fidelity-Converter" class="headerlink" title="Design and Implementation of DC-to-5~MHz Wide-Bandwidth High-Power High-Fidelity Converter"></a>Design and Implementation of DC-to-5~MHz Wide-Bandwidth High-Power High-Fidelity Converter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06409">http://arxiv.org/abs/2309.06409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinshui Zhang, Boshuo Wang, Xiaoyang Tian, Angel Peterchev, Stefan Goetz</li>
<li>for: 高功率电子技术的进步使得可以实现高功率水平，如达到GW级别的电网，或者高输出频率，如超过MHz级别的通信。但是同时实现高功率和高输出频率仍然是一个挑战。各种应用，如高效多通道无线电力传输、先进医疗和神经科学应用，都需要同时实现高功率和高输出频率。</li>
<li>methods: 我们使用加钴氮化物（GaN）晶体管和模块化链接双H桥电路，并实现了控制系统，可以解决通常的时间和平衡问题。</li>
<li>results: 我们开发了一个轻量级嵌入式控制解决方案，包括改进的静止表示数字synthesizer和一种新的适应偏好束拟合 nearest-level 模ulation。这种解决方案可以有效解决高功率和高输出频率之间的冲突，并且可以在两个维度上扩展。我们的原型在DC至5MHz的频谱范围内，实现了 &lt;18%的总电压误差，同时实现了 &gt;5kW 的功率水平。我们进行了输出频率的扫描和两个混合试验，包括一个实际的脑神经科学应用的刺激脉冲和一个娱乐性的试验，重现了著名的Arecibo信号。<details>
<summary>Abstract</summary>
Advances in power electronics have made it possible to achieve high power levels, e.g., reaching GW in grids, or alternatively high output bandwidths, e.g., beyond MHz in communication. Achieving both simultaneously, however, remains challenging. Various applications, ranging from efficient multichannel wireless power transfer to cutting-edge medical and neuroscience applications, are demanding both high power and wide bandwidth. Conventional inverters can achieve high power and high quality at grid or specific frequency ranges but lose their fidelity when reaching higher output frequencies. Resonant circuits can promise a high output frequency but only a narrow bandwidth. We overcome the hardware challenges by combining gallium-nitride (GaN) transistors with modular cascaded double-H bridge circuits and control that can manage typical timing and balancing issues. We developed a lightweight embedded control solution that includes an improved look-up-table digital synthesizer and a novel adaptive-bias-elimination nearest-level modulation. This solution effectively solves the conflict between a high power level and high output bandwidth and can--in contrast to previous approaches--in principle be scaled in both dimensions. Our prototype exhibits a frequency range from DC to 5 MHz with <18% total voltage distortion across the entire frequency spectrum, while achieving a power level of >5 kW. We conducted tests by sweeping the output frequency and two channel-mixing trials, which included a practical magnetogenetics-oriented stimulation pulse and an entertaining trial to reproduce the famous Arecibo message with the current spectrum.
</details>
<details>
<summary>摘要</summary>
Conventional inverters can achieve high power and high quality at grid or specific frequency ranges, but lose their fidelity when reaching higher output frequencies. Resonant circuits can promise high output frequencies but only have a narrow bandwidth. To overcome these hardware challenges, we combined gallium-nitride (GaN) transistors with modular cascaded double-H bridge circuits and control that can manage typical timing and balancing issues.We developed a lightweight embedded control solution that includes an improved look-up-table digital synthesizer and a novel adaptive-bias-elimination nearest-level modulation. This solution effectively solves the conflict between high power level and high output bandwidth and can, in contrast to previous approaches, be scaled in both dimensions. Our prototype exhibits a frequency range from DC to 5 MHz with <18% total voltage distortion across the entire frequency spectrum, while achieving a power level of >5 kW.We conducted tests by sweeping the output frequency and two channel-mixing trials, including a practical magnetogenetics-oriented stimulation pulse and an entertaining trial to reproduce the famous Arecibo message with the current spectrum.
</details></li>
</ul>
<hr>
<h2 id="Opportunistic-Reflection-in-Reconfigurable-Intelligent-Surface-Assisted-Wireless-Networks"><a href="#Opportunistic-Reflection-in-Reconfigurable-Intelligent-Surface-Assisted-Wireless-Networks" class="headerlink" title="Opportunistic Reflection in Reconfigurable Intelligent Surface-Assisted Wireless Networks"></a>Opportunistic Reflection in Reconfigurable Intelligent Surface-Assisted Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06457">http://arxiv.org/abs/2309.06457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Hans D. Schotten</li>
<li>for: 这篇论文关注了无线网络中多个用户和多个智能反射 superficie（RIS）之间的多访问协议设计。通过扩展现有的单用户或单RIS情况的方法，我们提出了两种参考方案。</li>
<li>methods: 我们使用了多用户多RIS情况下的现有方法，并提出了一种简单 yet efficient的方法称为可取机会多用户反射（OMUR）。该方法在选择最佳用户作为RIS的固定点并同时传输所有用户的信号的情况下，可以优化RIS。</li>
<li>results: OMUR方法可以提高网络吞吐量和功率耗用率，并且可以在多用户多RIS情况下实现高速下载和上传。此外，我们还提出了一种简化版OMUR方法，通过随机阶段偏移来避免RIS通道估计的复杂性。<details>
<summary>Abstract</summary>
This paper focuses on multiple-access protocol design in a wireless network assisted by multiple reconfigurable intelligent surfaces (RISs). By extending the existing approaches in single-user or single-RIS cases, we present two benchmark schemes for this multi-user multi-RIS scenario. Inspecting their shortcomings, a simple but efficient method coined opportunistic multi-user reflection (OMUR) is proposed. The key idea is to opportunistically select the best user as the anchor for optimizing the RISs, and non-orthogonally transmitting all users' signals simultaneously. A simplified version of OMUR exploiting random phase shifts is also proposed to avoid the complexity of RIS channel estimation.
</details>
<details>
<summary>摘要</summary>
In Simplified Chinese:这篇论文关注无线网络中多个用户和多个智能表面（RIS）之间的协议设计。我们通过扩展单用户或单RIS情况下的现有方法，提出了多用户多RIS场景中的两个标准方案。此外，我们还提出了一种名为启发式多用户反射（OMUR）的简单 yet efficient方法，其中选择最佳用户作为RIS优化的anchor，并同时非正交发送所有用户的信号。此外，我们还提出了一种使用随机频率偏移的简化版OMUR，以避免RIS频率估计的复杂性。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Multiple-Access-Design-for-Reconfigurable-Intelligent-Surface-Aided-Systems"><a href="#A-Simple-Multiple-Access-Design-for-Reconfigurable-Intelligent-Surface-Aided-Systems" class="headerlink" title="A Simple Multiple-Access Design for Reconfigurable Intelligent Surface-Aided Systems"></a>A Simple Multiple-Access Design for Reconfigurable Intelligent Surface-Aided Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06326">http://arxiv.org/abs/2309.06326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Hans D. Schotten</li>
<li>for: 这种研究旨在设计一种无线系统，利用单个或多个可编程智能表面（RIS）提高传输效率。</li>
<li>methods: 该方法基于机会性反射和非对准传输。选择最佳用户，并只使用该用户的通道状态信息进行RIS反射优化，以降低复杂性。在第二用户上叠加其信号，实现非对准传输，提高系统容量和用户公平性。</li>
<li>results: 研究人员提出了一种简单而高效的方法，可以在实际系统中实现高性能。通过随机相位偏移来避免RIS通道估计高开销。<details>
<summary>Abstract</summary>
This paper focuses on the design of transmission methods and reflection optimization for a wireless system assisted by a single or multiple reconfigurable intelligent surfaces (RISs). The existing techniques are either too complex to implement in practical systems or too inefficient to achieve high performance. To overcome the shortcomings of the existing schemes, we propose a simple but efficient approach based on \textit{opportunistic reflection} and \textit{non-orthogonal transmission}. The key idea is opportunistically selecting the best user that can reap the maximal gain from the optimally reflected signals via RIS. That is to say, only the channel state information of the best user is used for RIS reflection optimization, which can in turn lower complexity substantially. In addition, the second user is selected to superpose its signal on that of the primary user, where the benefits of non-orthogonal transmission, i.e., high system capacity and improved user fairness, are obtained. Additionally, a simplified variant exploiting random phase shifts is proposed to avoid the high overhead of RIS channel estimation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Distributed-Adaptive-Signal-Fusion-for-Fractional-Programs"><a href="#Distributed-Adaptive-Signal-Fusion-for-Fractional-Programs" class="headerlink" title="Distributed Adaptive Signal Fusion for Fractional Programs"></a>Distributed Adaptive Signal Fusion for Fractional Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06278">http://arxiv.org/abs/2309.06278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cem Ates Musluoglu, Alexander Bertrand</li>
<li>for: 解决卫星感知网络中的空间滤波优化问题</li>
<li>methods: 使用分布式适应Signal Fusion（DASF）框架和一种基于迭代的解决方法</li>
<li>results: 提高了计算效率和精度，并且能够在带宽限制下进行分布式计算<details>
<summary>Abstract</summary>
The distributed adaptive signal fusion (DASF) framework allows to solve spatial filtering optimization problems in a distributed and adaptive fashion over a bandwidth-constrained wireless sensor network. The DASF algorithm requires each node to sequentially build a compressed version of the original network-wide problem and solve it locally. However, these local problems can still result in a high computational load at the nodes, especially when the required solver is iterative. In this paper, we study the particular case of fractional programs, i.e., problems for which the objective function is a fraction of two continuous functions, which indeed require such iterative solvers. By exploiting the structure of a commonly used method for solving fractional programs and interleaving it with the iterations of the standard DASF algorithm, we obtain a distributed algorithm with a significantly reduced computational cost compared to the straightforward application of DASF as a meta-algorithm. We prove convergence and optimality of this "fractional DASF" (FDASF) algorithm and demonstrate its performance via numerical simulations.
</details>
<details>
<summary>摘要</summary>
distributed adaptive signal fusion (DASF) 框架可以在分布式和适应性的方式下解决宽频率约束无线传感网络上的空间滤波优化问题。 DASF 算法要求每个节点先后建立一个压缩版本的原始网络范围内问题，并解决它们本地。然而，这些本地问题仍可能导致节点上的计算负担很大，特别是当需要的解决器是迭代的。在这篇论文中，我们研究了分数程序，即两个连续函数的比例的问题。这些问题确实需要迭代的解决器。我们利用一种广泛使用的方法解决分数程序的方法，并与标准 DASF 算法的迭代结合起来，从而获得了一个分布式算法，与直接在 DASF 作为元算法应用时的计算成本相比，有显著的减少。我们证明了 FDASF 算法的收敛和优化性，并通过数字实验来证明其性能。
</details></li>
</ul>
<hr>
<h2 id="Base-Station-Beamforming-Design-for-Near-field-XL-IRS-Beam-Training"><a href="#Base-Station-Beamforming-Design-for-Near-field-XL-IRS-Beam-Training" class="headerlink" title="Base Station Beamforming Design for Near-field XL-IRS Beam Training"></a>Base Station Beamforming Design for Near-field XL-IRS Beam Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06259">http://arxiv.org/abs/2309.06259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Wang, Changsheng You, Changchuan Yin</li>
<li>for: 提高XL-IRS射频训练性能</li>
<li>methods: 提议了两种快速方法优化BS射频组合，包括使用SVD方法和$\ell_1$-norm最大化方法</li>
<li>results: numerical results indicate that the proposed AO based BS beamforming design outperforms the SVD&#x2F;angle based BS beamforming in terms of training accuracy and achievable received SNR.<details>
<summary>Abstract</summary>
Existing research on extremely large-scale intelligent reflecting surface (XL-IRS) beam training has assumed the far-field channel model for base station (BS)-IRS link. However, this approach may cause degraded beam training performance in practice due to the near-field channel model of the BS-IRS link. To address this issue, we propose two efficient schemes to optimize BS beamforming for improving the XL-IRS beam training performance. Specifically, the first scheme aims to maximize total received signal power on the XL-IRS, which generalizes the existing angle based BS beamforming design and can be resolved using the singular value decomposition (SVD) method. The second scheme aims to maximize the $\ell_1$-norm of incident signals on the XL-IRS, which is shown to achieve the maximum received power at the user. To solve the non-convex $\ell_1$-norm maximization problem, we propose an eficient algorithm by using the alternating optimization (AO) technique. Numerical results show that the proposed AO based BS beamforming design outperforms the SVD/angle based BS beamforming in terms of training accuracy and achievable received signal-to-noise ratio (SNR).
</details>
<details>
<summary>摘要</summary>
原研究中的巨大智能反射Surface（XL-IRS）的杆形训练假设了BS-IRS链路的远场通道模型。然而，这种方法可能会导致实际中的杆形训练性能下降，因为BS-IRS链路的近场通道模型。为解决这个问题，我们提出了两种高效的方案来优化BS杆形干扰。 Specifically, the first scheme aims to maximize the total received signal power on the XL-IRS, which generalizes the existing angle-based BS beamforming design and can be resolved using the singular value decomposition (SVD) method. The second scheme aims to maximize the $\ell_1$-norm of incident signals on the XL-IRS, which is shown to achieve the maximum received power at the user. To solve the non-convex $\ell_1$-norm maximization problem, we propose an efficient algorithm by using the alternating optimization (AO) technique. Numerical results show that the proposed AO-based BS beamforming design outperforms the SVD/angle-based BS beamforming in terms of training accuracy and achievable received signal-to-noise ratio (SNR).Here's the translation of the text in Traditional Chinese:先前的研究中的巨大智能反射Surface（XL-IRS）的杆形训练假设了BS-IRS链路的远场通道模型。然而，这种方法可能会导致实际中的杆形训练性能下降，因为BS-IRS链路的近场通道模型。为解决这个问题，我们提出了两种高效的方案来优化BS杆形干扰。 Specifically, the first scheme aims to maximize the total received signal power on the XL-IRS, which generalizes the existing angle-based BS beamforming design and can be resolved using the singular value decomposition (SVD) method. The second scheme aims to maximize the $\ell_1$-norm of incident signals on the XL-IRS, which is shown to achieve the maximum received power at the user. To solve the non-convex $\ell_1$-norm maximization problem, we propose an efficient algorithm by using the alternating optimization (AO) technique. Numerical results show that the proposed AO-based BS beamforming design outperforms the SVD/angle-based BS beamforming in terms of training accuracy and achievable received signal-to-noise ratio (SNR).
</details></li>
</ul>
<hr>
<h2 id="Comparing-Iterative-and-Least-Squares-Based-Phase-Noise-Tracking-in-Receivers-with-1-bit-Quantization-and-Oversampling"><a href="#Comparing-Iterative-and-Least-Squares-Based-Phase-Noise-Tracking-in-Receivers-with-1-bit-Quantization-and-Oversampling" class="headerlink" title="Comparing Iterative and Least-Squares Based Phase Noise Tracking in Receivers with 1-bit Quantization and Oversampling"></a>Comparing Iterative and Least-Squares Based Phase Noise Tracking in Receivers with 1-bit Quantization and Oversampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06124">http://arxiv.org/abs/2309.06124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Gast, Stephan Zeitz, Meik Dörpinghaus, Gerhard P. Fettweis</li>
<li>for: 提高高速数据传输率，需要巨大的频率带宽和高采样率，但这会导致问题atic-to-digital converter (ADC) 的能耗增加。为了解决这个问题，提出使用 1-bit ADC。</li>
<li>methods: 研究了 iterative 数据帮助 phase 估计算法，包括 expectation-maximization 算法和 Fisher-scoring 算法，与 least-squares (LS) phas 估计相比较。另外，考虑了 Kalman 筛和 Rauch-Tung-Striebel 算法用于数据符号间 phase  interpolating。</li>
<li>results: Iterative phase noise tracking 在高信噪比下具有较低的估计误差方差，但是它对 spectral efficiency 的提高几乎只有微妙的提高，即高于 Nyquist 幂 ZXM 模ulation。<details>
<summary>Abstract</summary>
High data rates require vast bandwidths, that can be found in the sub-THz band, and high sampling frequencies, which are predicted to lead to a problematically high analog-to-digital converter (ADC) power consumption. It was proposed to use 1-bit ADCs to mitigate this problem. Moreover, oscillator phase noise is predicted to be especially high at sub-THz carrier frequencies. For synchronization the phase must be tracked based on 1-bit quantized observations. We study iterative data-aided phase estimation, i.e., the expectation-maximization and the Fisher-scoring algorithm, compared to least-squares (LS) phase estimation. For phase interpolation at the data symbols, we consider the Kalman filter and the Rauch-Tung-Striebel algorithm. Compared to LS estimation, iterative phase noise tracking leads to a significantly lower estimation error variance at high signal-to-noise ratios. However, its benefit for the spectral efficiency using zero-crossing modulation (ZXM) is limited to marginal gains for high faster-than-Nyquist signaling factors, i.e., higher order ZXM modulation.
</details>
<details>
<summary>摘要</summary>
高数据率需要庞大的带宽，可以在Sub-THz频段找到，同时需要高的采样频率，这将导致问题atically高的Analog-to-Digital Converter（ADC）电力消耗。提出使用1比特ADC来缓解这个问题。此外，oscillator阶段噪声预测在Sub-THz振荡频率下特别高。为了同步化，需要根据1比特量化的观察结果追踪阶段。我们研究了iterative数据援引phase估计算法，包括期望最大化算法和fisherscoring算法，与最小二乘（LS）phase估计相比。在数据符号上进行phase interpolating时，我们考虑了卡尔曼筛和Rauch-Tung-Striebel算法。与LS估计相比，iterative phase噪声追踪导致高信号噪声比在高信号至噪声比下显著降低。然而，对于频率使用零交叉模ulation（ZXM）的spectral efficiency来说，其利益受到高速于Nyquist频率因子的限制，即高阶ZXM模ulation。
</details></li>
</ul>
<hr>
<h2 id="Tuning-of-Ray-Based-Channel-Model-for-5G-Indoor-Industrial-Scenarios"><a href="#Tuning-of-Ray-Based-Channel-Model-for-5G-Indoor-Industrial-Scenarios" class="headerlink" title="Tuning of Ray-Based Channel Model for 5G Indoor Industrial Scenarios"></a>Tuning of Ray-Based Channel Model for 5G Indoor Industrial Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06101">http://arxiv.org/abs/2309.06101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gurjot Singh Bhatia, Yoann Corre, Marco Di Renzo</li>
<li>for: 本文提出了一种用于生成5G工业互联网的决定性通道模型的创新方法。</li>
<li>methods: 本文使用了折射跟踪（RT）通道模拟器，可以很好地捕捉到各种工业环境下的具体特性。</li>
<li>results: 本文对3.7GHz和28GHz两频段的5G工业网络进行了比较，并与文献中的场测数据进行了比较，以生成准确的折射基本通道模型。<details>
<summary>Abstract</summary>
This paper presents an innovative method that can be used to produce deterministic channel models for 5G industrial internet-of-things (IIoT) scenarios. Ray-tracing (RT) channel emulation can capture many of the specific properties of a propagation scenario, which is incredibly beneficial when facing various industrial environments and deployment setups. But the environment's complexity, composed of many metallic objects of different sizes and shapes, pushes the RT tool to its limits. In particular, the scattering or diffusion phenomena can bring significant components. Thus, in this article, the Volcano RT channel simulation is tuned and benchmarked against field measurements found in the literature at two frequencies relevant to 5G industrial networks: 3.7 GHz (mid-band) and 28 GHz (millimeter-wave (mmWave) band), to produce calibrated ray-based channel model. Both specular and diffuse scattering contributions are calculated. Finally, the tuned RT data is compared to measured large-scale parameters, such as the power delay profile (PDP), the cumulative distribution function (CDF) of delay spreads (DSs), both in line-of-sight (LoS) and non-LoS (NLoS) situations and relevant IIoT channel properties are further explored.
</details>
<details>
<summary>摘要</summary>
To address these challenges, the Volcano RT channel simulation is tuned and benchmarked against field measurements found in the literature at two frequencies relevant to 5G industrial networks: 3.7 GHz (mid-band) and 28 GHz (millimeter-wave band). The calibrated ray-based channel model takes into account both specular and diffuse scattering contributions.The tuned RT data is compared to measured large-scale parameters, such as the power delay profile (PDP), the cumulative distribution function (CDF) of delay spreads (DSs), both in line-of-sight (LoS) and non-LoS (NLoS) situations. Additionally, relevant IIoT channel properties are further explored.
</details></li>
</ul>
<hr>
<h2 id="Hybrid-NOMA-assisted-Integrated-Sensing-and-Communication-via-RIS"><a href="#Hybrid-NOMA-assisted-Integrated-Sensing-and-Communication-via-RIS" class="headerlink" title="Hybrid NOMA assisted Integrated Sensing and Communication via RIS"></a>Hybrid NOMA assisted Integrated Sensing and Communication via RIS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06080">http://arxiv.org/abs/2309.06080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanting Lyu, Yue Xiu, Xinyang Li, Songjie Yang, Phee Lep Yeoh, Yonghui Li, Zhongpei Zhang</li>
<li>for: 本研究探讨了智能表面重配置（RIS）在整合感知通信（ISAC）系统中的优化。</li>
<li>methods: 本研究使用了能量域非对称多接入（NOMA）和干扰合并（OMA）技术，并通过联合功率分配、活动扩散和RIS相位偏移设计来优化探测模式响应。</li>
<li>results: 根据实验结果，提出的低复杂度交叉优化算法可以提高最小探测响应强度（MBPG），并且对探测和通信之间的负荷调整进行分析。<details>
<summary>Abstract</summary>
This paper investigates the optimization of reconfigurable intelligent surface (RIS) in an integrated sensing and communication (ISAC) system. \red{To meet the demand of growing number of devices, power domain non-orthogonal multiple access (NOMA) is considered. However, traditional NOMA with a large number of devices is challenging due to large decoding delay and propagation error introduced by successive interference cancellation (SIC). Thus, OMA is integrated into NOMA to support more devices}. We formulate a max-min problem to optimize the sensing beampattern \red{with constraints on communication rate}, through joint power allocation, active beamforming and RIS phase shift design. To solve the non-convex problem with a non-smooth objective function, we propose a low complexity alternating optimization (AO) algorithm, where a closed form expression for the intra-cluster power allocation (intra-CPA) is derived, and penalty and successive convex approximation (SCA) methods are used to optimize the beamforming and phase shift design. Simulation results show the effectiveness of the proposed algorithm in terms of improving minimum beampattern gain (MBPG) compared with other baselines. Furthermore, the trade-off between sensing and communication is analyzed and demonstrated in the simulation results.
</details>
<details>
<summary>摘要</summary>
The paper formulates a max-min problem to optimize the sensing beampattern with constraints on communication rate, using joint power allocation, active beamforming, and RIS phase shift design. To solve the non-convex problem with a non-smooth objective function, a low-complexity alternating optimization (AO) algorithm is proposed. This algorithm includes a closed-form expression for intra-cluster power allocation (intra-CPA), penalty and successive convex approximation (SCA) methods for optimizing beamforming and phase shift design.Simulation results show the effectiveness of the proposed algorithm in terms of improving minimum beampattern gain (MBPG) compared to other baselines. Additionally, the trade-off between sensing and communication is analyzed and demonstrated in the simulation results.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Simulation-of-Three-Phase-Induction-Machines-Under-Eccentricity-Conditions"><a href="#Dynamic-Simulation-of-Three-Phase-Induction-Machines-Under-Eccentricity-Conditions" class="headerlink" title="Dynamic Simulation of Three-Phase Induction Machines Under Eccentricity Conditions"></a>Dynamic Simulation of Three-Phase Induction Machines Under Eccentricity Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07079">http://arxiv.org/abs/2309.07079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Ardekani</li>
<li>for: 这个论文提出了一种计算效率高的三相发电机动力模拟和数学模型化方法，以解决在偏心条件下的问题。</li>
<li>methods: 该方法使用了一种基于牛顿法的快速算法，可以快速计算出三相发电机的动力特性。</li>
<li>results: 实验结果表明，该方法可以准确地模拟三相发电机的动力行为，并且计算效率高于传统方法。<details>
<summary>Abstract</summary>
This thesis propose an a computationally efficient method for dynamic simulation and mathematical modelling of three-phase induction machines under eccentricity conditions.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种计算效率高的三相扩散机在偏心条件下的动态模拟和数学模拟方法。Here's the breakdown of the translation:* 这个论文 (zhè ge tèsēng) - This thesis* 提出 (tíchū) - proposes* 一种 (yī zhǒng) - a kind of* 计算效率高 (jìsuàn yìngyè gāo) - computationally efficient method* 三相扩散机 (sān fáng kuò shuāng chī) - three-phase induction machine* 在 (zài) - under* 偏心条件 (piān xīn tiáo yòng) - eccentricity conditions* 动态模拟 (dòng tǐ mó xiàng) - dynamic simulation* 数学模拟 (liǎo xué mó xiàng) - mathematical modeling
</details></li>
</ul>
<hr>
<h2 id="Which-Framework-is-Suitable-for-Online-3D-Multi-Object-Tracking-for-Autonomous-Driving-with-Automotive-4D-Imaging-Radar"><a href="#Which-Framework-is-Suitable-for-Online-3D-Multi-Object-Tracking-for-Autonomous-Driving-with-Automotive-4D-Imaging-Radar" class="headerlink" title="Which Framework is Suitable for Online 3D Multi-Object Tracking for Autonomous Driving with Automotive 4D Imaging Radar?"></a>Which Framework is Suitable for Online 3D Multi-Object Tracking for Autonomous Driving with Automotive 4D Imaging Radar?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06036">http://arxiv.org/abs/2309.06036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianan Liu, Guanhua Ding, Yuxuan Xia, Jinping Sun, Tao Huang, Lihua Xie, Bing Zhu<br>for:这篇论文旨在探讨在现实世界ADAS和自动驾驶场景中的在线3D多对象跟踪（MOT）问题，具体来说是对LiDAR和4D影像雷达点云的点对象跟踪（POT）和扩展对象跟踪（EOT）两种不同的方法进行系统性的调研。methods:这篇论文使用了三种不同的方法进行比较：传统的TBD-POT方法、最近研究的JDT-EOT方法以及我们提出的TBD-EOT方法。这些方法在两个开源的4D影像雷达数据集上进行了广泛的评估。results:实验结果表明，传统的TBD-POT方法在在线3D MOT中具有高跟踪性和低计算复杂度，而我们提出的TBD-EOT方法在某些情况下可能超越其性能。然而，JDT-EOT方法在评估场景中表现不佳，并且经过分析多种评价指标和视觉化分析后，我们提出了改进其性能的可能性。这些研究为未来4D影像雷达基于在线3D MOT的发展提供了首个和重要的指南。<details>
<summary>Abstract</summary>
Online 3D multi-object tracking (MOT) has recently received significant research interests due to the expanding demand of 3D perception in advanced driver assistance systems (ADAS) and autonomous driving (AD). Among the existing 3D MOT frameworks for ADAS and AD, conventional point object tracking (POT) framework using the tracking-by-detection (TBD) strategy has been well studied and accepted for LiDAR and 4D imaging radar point clouds. In contrast, extended object tracking (EOT), another important framework which accepts the joint-detection-and-tracking (JDT) strategy, has rarely been explored for online 3D MOT applications. This paper provides the first systematical investigation of the EOT framework for online 3D MOT in real-world ADAS and AD scenarios. Specifically, the widely accepted TBD-POT framework, the recently investigated JDT-EOT framework, and our proposed TBD-EOT framework are compared via extensive evaluations on two open source 4D imaging radar datasets: View-of-Delft and TJ4DRadSet. Experiment results demonstrate that the conventional TBD-POT framework remains preferable for online 3D MOT with high tracking performance and low computational complexity, while the proposed TBD-EOT framework has the potential to outperform it in certain situations. However, the results also show that the JDT-EOT framework encounters multiple problems and performs inadequately in evaluation scenarios. After analyzing the causes of these phenomena based on various evaluation metrics and visualizations, we provide possible guidelines to improve the performance of these MOT frameworks on real-world data. These provide the first benchmark and important insights for the future development of 4D imaging radar-based online 3D MOT.
</details>
<details>
<summary>摘要</summary>
在线3D多对象跟踪（MOT）最近受到了广泛的研究兴趣，这主要归功于自动驾驶系统（ADAS）和自主驱动（AD）的扩展需求。在现有的3D MOT框架中，使用跟踪检测（TBD）策略的点对象跟踪（POT）框架已经广泛研究和应用于激光雷达和4D图像雷达的点云数据。然而，接受联合检测与跟踪（JDT）策略的延长对象跟踪（EOT）框架在在线3D MOT应用中 rarely 被研究。本文提供了在实际ADAS和AD场景中的首次系统性的EOT框架比较，包括广泛接受的TBD-POT框架、最近研究的JDT-EOT框架以及我们的提议的TBD-EOT框架。经过广泛的评估于两个开源4D图像雷达数据集：View-of-Delft和TJ4DRadSet，实验结果显示， convent ional TBD-POT框架在在线3D MOT中具有高跟踪性和低计算复杂度，而我们提议的TBD-EOT框架在某些情况下具有超越TBD-POT框架的潜在优势。然而，结果也表明，JDT-EOT框架在评估场景中存在多个问题，并且表现不佳。经过根据多种评价指标和视觉化分析，我们提供了可能的改进方向，这些提供了首次的4D图像雷达基于在线3D MOT的 Referenced  bencmark和重要的指导。
</details></li>
</ul>
<hr>
<h2 id="Non-parametric-Ensemble-Empirical-Mode-Decomposition-for-extracting-weak-features-to-identify-bearing-defects"><a href="#Non-parametric-Ensemble-Empirical-Mode-Decomposition-for-extracting-weak-features-to-identify-bearing-defects" class="headerlink" title="Non-parametric Ensemble Empirical Mode Decomposition for extracting weak features to identify bearing defects"></a>Non-parametric Ensemble Empirical Mode Decomposition for extracting weak features to identify bearing defects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06003">http://arxiv.org/abs/2309.06003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anil Kumar, Yaakoub Berrouche, Radosław Zimroz, Govind Vashishtha, Sumika Chauhan, C. P. Gandhi, Hesheng Tang, Jiawei Xiang</li>
<li>for: 用于识别滤波器缺陷</li>
<li>methods: 非参数统计ensemble empirical mode decomposition（NPCEEMD）方法</li>
<li>results: 比较NPCEEMD和现有方法，NPCEEMD的模式混合度较低Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for identifying bearing defects using weak features, specifically using the proposed NPCEEMD method.</li>
<li>methods: The paper proposes a non-parametric complementary ensemble empirical mode decomposition (NPCEEMD) method for identifying bearing defects. This method is non-parametric, meaning it does not require defining the ideal signal-to-noise ratio (SNR) or the number of ensembles every time while processing the signals.</li>
<li>results: The paper presents simulation results showing that the proposed NPCEEMD method has less mode mixing than existing decomposition methods. Additionally, the method is applied to experimental data, and the resulting signal is computed using the envelope spectrum to confirm the presence of defects.<details>
<summary>Abstract</summary>
A non-parametric complementary ensemble empirical mode decomposition (NPCEEMD) is proposed for identifying bearing defects using weak features. NPCEEMD is non-parametric because, unlike existing decomposition methods such as ensemble empirical mode decomposition, it does not require defining the ideal SNR of noise and the number of ensembles, every time while processing the signals. The simulation results show that mode mixing in NPCEEMD is less than the existing decomposition methods. After conducting in-depth simulation analysis, the proposed method is applied to experimental data. The proposed NPCEEMD method works in following steps. First raw signal is obtained. Second, the obtained signal is decomposed. Then, the mutual information (MI) of the raw signal with NPCEEMD-generated IMFs is computed. Further IMFs with MI above 0.1 are selected and combined to form a resulting signal. Finally, envelope spectrum of resulting signal is computed to confirm the presence of defect.
</details>
<details>
<summary>摘要</summary>
“一种非Parametric complementary ensemble empirical mode decomposition（NPCEEMD）是用于发现推挽缺陷的方法，使用弱特征。NPCEEMD非 Parametric，因为它不需要每次处理信号时定义理想噪声水平和数量的集合。模拟结果显示，NPCEEMD中的模式混合度比现有的分解方法更低。经过深入的模拟分析，提议的方法应用到实验数据中。NPCEEMD方法的步骤如下：首先获取原始信号；第二，将获取的信号进行分解；然后计算原始信号与NPCEEMD生成的IMF的相互信息（MI）；进一步选择MI大于0.1的IMF并将其组合成一个结果信号；最后，计算结果信号的响应特征来确认缺陷存在。”Note: "IMF" stands for "intrinsic mode function".
</details></li>
</ul>
<hr>
<h2 id="Massive-Access-of-Static-and-Mobile-Users-via-Reconfigurable-Intelligent-Surfaces-Protocol-Design-and-Performance-Analysis"><a href="#Massive-Access-of-Static-and-Mobile-Users-via-Reconfigurable-Intelligent-Surfaces-Protocol-Design-and-Performance-Analysis" class="headerlink" title="Massive Access of Static and Mobile Users via Reconfigurable Intelligent Surfaces: Protocol Design and Performance Analysis"></a>Massive Access of Static and Mobile Users via Reconfigurable Intelligent Surfaces: Protocol Design and Performance Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05964">http://arxiv.org/abs/2309.05964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuelin Cao, Bo Yang, Chongwen Huang, George C. Alexandropoulos, Chau Yuen, Zhu Han, H. Vincent Poor, Lajos Hanzo</li>
<li>for: 这篇论文旨在研究一种基于多个可配置智能表面（RIS）的下一代多接入（NGMA）协议，以实现无线通信系统中的高 spectral efficiency 和低延迟服务。</li>
<li>methods: 该论文使用了多个可配置智能表面（RIS），并研究了NGMA协议的设计和RIS相位配置的关系，以实现更高的 spectral efficiency 和连接性。</li>
<li>results:  simulations 表明，提议的 MAC 协议在系统吞吐量和访问公平性两个方面具有优于标准协议，但是存在访问公平性和系统吞吐量之间的贸易关系。<details>
<summary>Abstract</summary>
The envisioned wireless networks of the future entail the provisioning of massive numbers of connections, heterogeneous data traffic, ultra-high spectral efficiency, and low latency services. This vision is spurring research activities focused on defining a next generation multiple access (NGMA) protocol that can accommodate massive numbers of users in different resource blocks, thereby, achieving higher spectral efficiency and increased connectivity compared to conventional multiple access schemes. In this article, we present a multiple access scheme for NGMA in wireless communication systems assisted by multiple reconfigurable intelligent surfaces (RISs). In this regard, considering the practical scenario of static users operating together with mobile ones, we first study the interplay of the design of NGMA schemes and RIS phase configuration in terms of efficiency and complexity. Based on this, we then propose a multiple access framework for RIS-assisted communication systems, and we also design a medium access control (MAC) protocol incorporating RISs. In addition, we give a detailed performance analysis of the designed RIS-assisted MAC protocol. Our extensive simulation results demonstrate that the proposed MAC design outperforms the benchmarks in terms of system throughput and access fairness, and also reveal a trade-off relationship between the system throughput and fairness.
</details>
<details>
<summary>摘要</summary>
将来的无线网络视图包括提供庞大量连接、不同数据流量、ultra-高频率效率和低延迟服务。这一视图激发了研究人员关于定义下一代多接入（NGMA）协议的研究活动，以便在不同资源块中承载大量用户，从而实现更高的频率效率和连接性。在这篇文章中，我们介绍了一种基于RIS的NGMA协议在无线通信系统中的应用。在这种情况下，我们首先研究了NGMA协议的设计和RIS相位配置之间的关系，并对系统效率和复杂性进行了分析。然后，我们提出了一种基于RIS的通信系统多Access框架，并设计了一种嵌入RIS的媒体访问控制协议（MAC）协议。此外，我们对提出的MAC协议进行了详细的性能分析。我们的广泛的 simulations结果表明，提出的MAC设计在系统吞吐量和访问公平性方面都有优于标准准确，并且还发现了系统吞吐量和公平性之间的负相关性。
</details></li>
</ul>
<hr>
<h2 id="Performance-Bounds-for-Near-Field-Localization-with-Widely-Spaced-Multi-Subarray-mmWave-THz-MIMO"><a href="#Performance-Bounds-for-Near-Field-Localization-with-Widely-Spaced-Multi-Subarray-mmWave-THz-MIMO" class="headerlink" title="Performance Bounds for Near-Field Localization with Widely-Spaced Multi-Subarray mmWave&#x2F;THz MIMO"></a>Performance Bounds for Near-Field Localization with Widely-Spaced Multi-Subarray mmWave&#x2F;THz MIMO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05944">http://arxiv.org/abs/2309.05944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songjie Yang, Xinyi Chen, Yue Xiu, Wanting Lyu, Zhongpei Zhang, Chau Yuen</li>
<li>for: 本研究探讨了靠近场地化使用广泛分布多子阵列（WSMS），并分析相应的角度和距离Cramér-Rao bounds（CRB）。</li>
<li>methods: 本研究使用里曼积分来 derivateclosed-form CRB表达式，并发现CRB可以通过target的方向和阵列的两端点之间的角度Span来 caracterize。</li>
<li>results: 研究发现，在某些情况下，WSMS的CRB小于均勋阵列的CRB，并且提供了各种系统特性的可见性和实验验证。<details>
<summary>Abstract</summary>
This paper investigates the potential of near-field localization using widely-spaced multi-subarrays (WSMSs) and analyzing the corresponding angle and range Cram\'er-Rao bounds (CRBs). By employing the Riemann sum, closed-form CRB expressions are derived for the spherical wavefront-based WSMS (SW-WSMS). We find that the CRBs can be characterized by the angular span formed by the line connecting the array's two ends to the target, and the different WSMSs with same angular spans but different number of subarrays have identical normalized CRBs. We provide a theoretical proof that, in certain scenarios, the CRB of WSMSs is smaller than that of uniform arrays. We further yield the closed-form CRBs for the hybrid spherical and planar wavefront-based WSMS (HSPW-WSMS), and its components can be seen as decompositions of the parameters from the CRBs for the SW-WSMS. Simulations are conducted to validate the accuracy of the derived closed-form CRBs and provide further insights into various system characteristics. Basically, this paper underscores the high resolution of utilizing WSMS for localization, reinforces the validity of adopting the HSPW assumption, and, considering its applications in communications, indicates a promising outlook for integrated sensing and communications based on HSPW-WSMSs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/12/eess.SP_2023_09_12/" data-id="clp869u8v01e7k588a0xe2e8c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/cs.SD_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T15:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/11/cs.SD_2023_09_11/">cs.SD - 2023-09-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Natural-Language-Supervision-for-General-Purpose-Audio-Representations"><a href="#Natural-Language-Supervision-for-General-Purpose-Audio-Representations" class="headerlink" title="Natural Language Supervision for General-Purpose Audio Representations"></a>Natural Language Supervision for General-Purpose Audio Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05767">http://arxiv.org/abs/2309.05767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/clap">https://github.com/microsoft/clap</a></li>
<li>paper_authors: Benjamin Elizalde, Soham Deshmukh, Huaming Wang</li>
<li>for: 这篇论文旨在提出一种混合语音和文本表示学习模型，以实现零基eline推理。</li>
<li>methods: 该模型使用两种创新的编码器来学习音频和语言表示，并使用对比学习将音频和语言表示带到共同的多Modal空间中。</li>
<li>results: 该模型在26个下游任务中表现出色，达到了多个任务的顶峰性能，为实现通用的音频表示铺平了道路。<details>
<summary>Abstract</summary>
Audio-Language models jointly learn multimodal text and audio representations that enable Zero-Shot inference. Models rely on the encoders to create powerful representations of the input and generalize to multiple tasks ranging from sounds, music, and speech. Although models have achieved remarkable performance, there is still a performance gap with task-specific models. In this paper, we propose a Contrastive Language-Audio Pretraining model that is pretrained with a diverse collection of 4.6M audio-text pairs employing two innovative encoders for Zero-Shot inference. To learn audio representations, we trained an audio encoder on 22 audio tasks, instead of the standard training of sound event classification. To learn language representations, we trained an autoregressive decoder-only model instead of the standard encoder-only models. Then, the audio and language representations are brought into a joint multimodal space using Contrastive Learning. We used our encoders to improve the downstream performance by a margin. We extensively evaluated the generalization of our representations on 26 downstream tasks, the largest in the literature. Our model achieves state of the art results in several tasks leading the way towards general-purpose audio representations.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language = zh-CN;</SYS>>Audio-语言模型同时学习多Modal文本和音频表示，以实现零shot推理。模型依靠encoder创建强大的输入表示，并能泛化到多个任务，从声音、音乐到语音。虽然模型已达到了非常出色的性能，但还存在任务特定模型的性能差距。在这篇论文中，我们提出了一种对比语言-音频预训练模型，该模型通过使用多样化的4.6M audio-文本对employs two innovative encoders来实现零shot推理。为了学习音频表示，我们在22种音频任务上训练了音频encoder，而不是标准的声音分类训练。为了学习语言表示，我们训练了一个自然语言模型，而不是标准的encoder-only模型。然后，音频和语言表示被带入一个共同多模态空间，使用对比学习。我们使用我们的encoder来提高下游性能的边缘。我们广泛评估了我们的表示的泛化性能，并取得了Literature中最大的26个下游任务。我们的模型在一些任务中取得了状态的战果，领先于普适音频表示的发展。
</details></li>
</ul>
<hr>
<h2 id="Kernel-Interpolation-of-Incident-Sound-Field-in-Region-Including-Scattering-Objects"><a href="#Kernel-Interpolation-of-Incident-Sound-Field-in-Region-Including-Scattering-Objects" class="headerlink" title="Kernel Interpolation of Incident Sound Field in Region Including Scattering Objects"></a>Kernel Interpolation of Incident Sound Field in Region Including Scattering Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05634">http://arxiv.org/abs/2309.05634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shoichi Koyama, Masaki Nakada, Juliano G. C. Ribeiro, Hiroshi Saruwatari</li>
<li>for: 这种方法用于估计包含散射物体的区域内的入射声场。</li>
<li>methods: 该方法基于幂函数回归的入射场，通过分离散射场的圆函数展开，消除了对散射物体的先前知识或测量的需求。</li>
<li>results: 实验结果表明，该方法比无分离的幂函数回归更高精度地估计入射声场。<details>
<summary>Abstract</summary>
A method for estimating the incident sound field inside a region containing scattering objects is proposed. The sound field estimation method has various applications, such as spatial audio capturing and spatial active noise control; however, most existing methods do not take into account the presence of scatterers within the target estimation region. Although several techniques exist that employ knowledge or measurements of the properties of the scattering objects, it is usually difficult to obtain them precisely in advance, and their properties may change during the estimation process. Our proposed method is based on the kernel ridge regression of the incident field, with a separation from the scattering field represented by a spherical wave function expansion, thus eliminating the need for prior modeling or measurements of the scatterers. Moreover, we introduce a weighting matrix to induce smoothness of the scattering field in the angular direction, which alleviates the effect of the truncation order of the expansion coefficients on the estimation accuracy. Experimental results indicate that the proposed method achieves a higher level of estimation accuracy than the kernel ridge regression without separation.
</details>
<details>
<summary>摘要</summary>
一种估计受到障碍物影响的受测 зву场的方法被提议。这种受测音场估算方法在各种应用中有重要意义，如空间音采和空间活动噪声控制，但大多数现有方法忽略了目标估算区域内的障碍物。虽然有一些技术利用了障碍物的性能知识或测量结果，但通常很难在进行估算之前 precisely 获取它们，而且它们在估算过程中可能会发生变化。我们提议的方法基于incident field的 kernel ridge regression，通过将散射场表示为球形傅里叶函数展开，因此无需在进行估算之前 precisely 知道障碍物的性能。此外，我们引入了一个权重矩阵来促进angular方向上的平滑性，这有助于减少 truncation order 对估算精度的影响。实验结果表明，我们提议的方法比kernel ridge regression无 separation 更高级别的估算精度。
</details></li>
</ul>
<hr>
<h2 id="Undecidability-Results-and-Their-Relevance-in-Modern-Music-Making"><a href="#Undecidability-Results-and-Their-Relevance-in-Modern-Music-Making" class="headerlink" title="Undecidability Results and Their Relevance in Modern Music Making"></a>Undecidability Results and Their Relevance in Modern Music Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05595">http://arxiv.org/abs/2309.05595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Halley Young</li>
<li>for: 本研究探讨了计算理论和音乐之间的交叉点，探讨了现代音乐创作和生产中 Undecidability 的重要 yet 被忽略的意义。</li>
<li>methods: 该研究采用多维度方法，包括 Ableton 的 Turing 完善性、音频效果的 Undecidability、音频作曲的约束 Undecidability、正律和律 Harmony 的 Undecidability，以及 “新的 ordering systems” 的 Undecidability。</li>
<li>results: 研究提供了这些主张的理论证明，并证明了这些概念在实践中的实用性。 本研究的最终目标是促进对 Undecidability 在音乐中的新理解，强调其更广泛的应用和可能性，以及对计算机助理（以及传统）音乐创作的影响。<details>
<summary>Abstract</summary>
This paper delves into the intersection of computational theory and music, examining the concept of undecidability and its significant, yet overlooked, implications within the realm of modern music composition and production. It posits that undecidability, a principle traditionally associated with theoretical computer science, extends its relevance to the music industry. The study adopts a multidimensional approach, focusing on five key areas: (1) the Turing completeness of Ableton, a widely used digital audio workstation, (2) the undecidability of satisfiability in sound creation utilizing an array of effects, (3) the undecidability of constraints on polymeters in musical compositions, (4) the undecidability of satisfiability in just intonation harmony constraints, and (5) the undecidability of "new ordering systems". In addition to providing theoretical proof for these assertions, the paper elucidates the practical relevance of these concepts for practitioners outside the field of theoretical computer science. The ultimate aim is to foster a new understanding of undecidability in music, highlighting its broader applicability and potential to influence contemporary computer-assisted (and traditional) music making.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>The Turing completeness of Ableton, a widely used digital audio workstation.2. The undecidability of satisfiability in sound creation using an array of effects.3. The undecidability of constraints on polymeters in musical compositions.4. The undecidability of satisfiability in just intonation harmony constraints.5. The undecidability of “new ordering systems”.In addition to providing theoretical proof for these assertions, the paper also illustrates the practical relevance of these concepts for practitioners outside the field of theoretical computer science. The ultimate aim is to foster a new understanding of undecidability in music, highlighting its broader applicability and potential to influence contemporary computer-assisted (and traditional) music making.</details></li>
</ol>
<hr>
<h2 id="SlideSpeech-A-Large-Scale-Slide-Enriched-Audio-Visual-Corpus"><a href="#SlideSpeech-A-Large-Scale-Slide-Enriched-Audio-Visual-Corpus" class="headerlink" title="SlideSpeech: A Large-Scale Slide-Enriched Audio-Visual Corpus"></a>SlideSpeech: A Large-Scale Slide-Enriched Audio-Visual Corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05396">http://arxiv.org/abs/2309.05396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoxu Wang, Fan Yu, Xian Shi, Yuezhang Wang, Shiliang Zhang, Ming Li</li>
<li>for: 提高自然语言处理系统的性能，特别是多模态自动语音识别系统。</li>
<li>methods: 利用视频和文本信息，通过关键词提取和语音识别系统中的上下文方法，提高语音识别性能。</li>
<li>results: 通过对听写材料进行分析，发现可以通过利用视频上的文本信息提高语音识别性能。<details>
<summary>Abstract</summary>
Multi-Modal automatic speech recognition (ASR) techniques aim to leverage additional modalities to improve the performance of speech recognition systems. While existing approaches primarily focus on video or contextual information, the utilization of extra supplementary textual information has been overlooked. Recognizing the abundance of online conference videos with slides, which provide rich domain-specific information in the form of text and images, we release SlideSpeech, a large-scale audio-visual corpus enriched with slides. The corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality transcribed speech. Moreover, the corpus contains a significant amount of real-time synchronized slides. In this work, we present the pipeline for constructing the corpus and propose baseline methods for utilizing text information in the visual slide context. Through the application of keyword extraction and contextual ASR methods in the benchmark system, we demonstrate the potential of improving speech recognition performance by incorporating textual information from supplementary video slides.
</details>
<details>
<summary>摘要</summary>
多Modal自动语音识别（ASR）技术目的在于利用其他modalities提高语音识别系统的性能。现有的方法主要关注视频或上下文信息，而使用补充的文本信息则被忽略。 recognizing the abundance of online conference videos with slides, which provide rich domain-specific information in the form of text and images, we release SlideSpeech, a large-scale audio-visual corpus enriched with slides. The corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality transcribed speech. Moreover, the corpus contains a significant amount of real-time synchronized slides. In this work, we present the pipeline for constructing the corpus and propose baseline methods for utilizing text information in the visual slide context. Through the application of keyword extraction and contextual ASR methods in the benchmark system, we demonstrate the potential of improving speech recognition performance by incorporating textual information from supplementary video slides.
</details></li>
</ul>
<hr>
<h2 id="Towards-generalisable-and-calibrated-synthetic-speech-detection-with-self-supervised-representations"><a href="#Towards-generalisable-and-calibrated-synthetic-speech-detection-with-self-supervised-representations" class="headerlink" title="Towards generalisable and calibrated synthetic speech detection with self-supervised representations"></a>Towards generalisable and calibrated synthetic speech detection with self-supervised representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05384">http://arxiv.org/abs/2309.05384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Oneata, Adriana Stan, Octavian Pascu, Elisabeta Oneata, Horia Cucu</li>
<li>for: 这个论文的目的是提高深度模仿器的普适性，以便建立可靠的假象检测器。</li>
<li>methods: 该论文使用预训练的自动学习表示 followed by a simple logistic regression classifier，以实现强大的普适性。</li>
<li>results: 该方法在新引入的 In-the-Wild 数据集上减少了平均错误率从 30% 降低到 8%，并且生成了更好地归一化的模型，可以用于下游任务，如不确定性估计。<details>
<summary>Abstract</summary>
Generalisation -- the ability of a model to perform well on unseen data -- is crucial for building reliable deep fake detectors. However, recent studies have shown that the current audio deep fake models fall short of this desideratum. In this paper we show that pretrained self-supervised representations followed by a simple logistic regression classifier achieve strong generalisation capabilities, reducing the equal error rate from 30% to 8% on the newly introduced In-the-Wild dataset. Importantly, this approach also produces considerably better calibrated models when compared to previous approaches. This means that we can trust our model's predictions more and use these for downstream tasks, such as uncertainty estimation. In particular, we show that the entropy of the estimated probabilities provides a reliable way of rejecting uncertain samples and further improving the accuracy.
</details>
<details>
<summary>摘要</summary>
“一般化”——模型在未见到的数据上表现良好的能力——是深圳识别器的重要需求。然而，最近的研究表明，现有的音频深圳模型尚未达到这个需求。在这篇论文中，我们展示了预训自动 represencing，然后跟着一个简单的逻辑函数分类器可以实现强大的一般化能力，从30%降至8%的平均错误率在新引入的 In-the-Wild 数据集上。此外，这种方法还生成了较好的条件分布，使得我们可以更加信任模型的预测，并将其用于下游任务，如uncertainty估计。具体来说，我们显示出估计概率的熵可以可靠地拒绝不确定的数据，并进一步提高准确率。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Speaker-Diarization-with-Large-Language-Models-A-Contextual-Beam-Search-Approach"><a href="#Enhancing-Speaker-Diarization-with-Large-Language-Models-A-Contextual-Beam-Search-Approach" class="headerlink" title="Enhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach"></a>Enhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05248">http://arxiv.org/abs/2309.05248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Jin Park, Kunal Dhawan, Nithin Koluguri, Jagadeesh Balam</li>
<li>for: 这 paper 的目的是提出一种基于大语言模型 (LLM) 的语音分类方法，以便更好地利用语音和文本之间的上下文关系。</li>
<li>methods: 该方法基于一种已有的语音基于的 speaker diarization 系统，并添加了一个大语言模型 (LLM) 的 lexical information，以在推理阶段利用上下文信息。我们将多模式推理过程设计为一个probabilistic模型，并在 joint acoustic 和 lexical beam search 中包含两种模式的信息。</li>
<li>results: 我们的实验结果表明，通过在 acoustics-only diarization 系统中添加 LLM 的 lexical knowledge，可以提高总的 speaker-attributed word error rate (SA-WER)。实验结果还表明，LLMs 可以为 speaker diarization 和其他语音处理任务提供更多的上下文信息，并且可以在不可见的上下文信息方面提供补做。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown great promise for capturing contextual information in natural language processing tasks. We propose a novel approach to speaker diarization that incorporates the prowess of LLMs to exploit contextual cues in human dialogues. Our method builds upon an acoustic-based speaker diarization system by adding lexical information from an LLM in the inference stage. We model the multi-modal decoding process probabilistically and perform joint acoustic and lexical beam search to incorporate cues from both modalities: audio and text. Our experiments demonstrate that infusing lexical knowledge from the LLM into an acoustics-only diarization system improves overall speaker-attributed word error rate (SA-WER). The experimental results show that LLMs can provide complementary information to acoustic models for the speaker diarization task via proposed beam search decoding approach showing up to 39.8% relative delta-SA-WER improvement from the baseline system. Thus, we substantiate that the proposed technique is able to exploit contextual information that is inaccessible to acoustics-only systems which is represented by speaker embeddings. In addition, these findings point to the potential of using LLMs to improve speaker diarization and other speech processing tasks by capturing semantic and contextual cues.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/cs.SD_2023_09_11/" data-id="clp869u2z00z0k5887t3sarfr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/cs.CV_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T13:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/11/cs.CV_2023_09_11/">cs.CV - 2023-09-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Radiomics-Boosts-Deep-Learning-Model-for-IPMN-Classification"><a href="#Radiomics-Boosts-Deep-Learning-Model-for-IPMN-Classification" class="headerlink" title="Radiomics Boosts Deep Learning Model for IPMN Classification"></a>Radiomics Boosts Deep Learning Model for IPMN Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05857">http://arxiv.org/abs/2309.05857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lanhong Yao, Zheyuan Zhang, Ugur Demir, Elif Keles, Camila Vendrami, Emil Agarunov, Candice Bolan, Ivo Schoots, Marc Bruno, Rajesh Keswani, Frank Miller, Tamas Gonda, Cemal Yazici, Temel Tirkes, Michael Wallace, Concetto Spampinato, Ulas Bagci<br>for:这篇论文的目的是为了提出一个新的电脑支持诊断架构，以帮助诊断潜在的胰脏癌症。methods:这篇论文使用了一种独特的自适应分 segmentation 策略来定义胰脏的边界，然后使用了一个新的深度学习架构来进行分类。results:这篇论文在使用多个检测方法时得到了超过 80% 的准确率，较之前的国际标准和出版研究更高。<details>
<summary>Abstract</summary>
Intraductal Papillary Mucinous Neoplasm (IPMN) cysts are pre-malignant pancreas lesions, and they can progress into pancreatic cancer. Therefore, detecting and stratifying their risk level is of ultimate importance for effective treatment planning and disease control. However, this is a highly challenging task because of the diverse and irregular shape, texture, and size of the IPMN cysts as well as the pancreas. In this study, we propose a novel computer-aided diagnosis pipeline for IPMN risk classification from multi-contrast MRI scans. Our proposed analysis framework includes an efficient volumetric self-adapting segmentation strategy for pancreas delineation, followed by a newly designed deep learning-based classification scheme with a radiomics-based predictive approach. We test our proposed decision-fusion model in multi-center data sets of 246 multi-contrast MRI scans and obtain superior performance to the state of the art (SOTA) in this field. Our ablation studies demonstrate the significance of both radiomics and deep learning modules for achieving the new SOTA performance compared to international guidelines and published studies (81.9\% vs 61.3\% in accuracy). Our findings have important implications for clinical decision-making. In a series of rigorous experiments on multi-center data sets (246 MRI scans from five centers), we achieved unprecedented performance (81.9\% accuracy).
</details>
<details>
<summary>摘要</summary>
卵巢瘤细胞肿（IPMN）是肝脏前期癌变，可能会进展到肝脏癌。因此，检测和分级IPMN的风险水平是肝脏疾病控制的关键。然而，这是一项非常具有挑战性的任务，因为IPMN瘤肿的形态、文本和大小均非常多样化和不规则。在这项研究中，我们提出了一种新的计算机助手诊断管线，用于IPMN风险分类从多方位MRI扫描中。我们的提议分析框架包括高效的自适应分割策略，以便识别肝脏，然后是一种新设计的深度学习基于的分类方案，以及一种基于 радиологи学的预测方法。我们在多个中心的数据集上测试了我们的提议决策融合模型，并获得了在这个领域的新的最高性能（81.9%）。我们的剖析研究表明， Both radiomics和深度学习模块对于实现新的最高性能具有重要的意义，相比于国际指南和已发表的研究（81.9% vs 61.3%）。我们的发现对临床决策有重要的意义。在多个中心的数据集上（246个MRI扫描）进行了严格的实验，我们实现了准确率81.9%。
</details></li>
</ul>
<hr>
<h2 id="Self-Correlation-and-Cross-Correlation-Learning-for-Few-Shot-Remote-Sensing-Image-Semantic-Segmentation"><a href="#Self-Correlation-and-Cross-Correlation-Learning-for-Few-Shot-Remote-Sensing-Image-Semantic-Segmentation" class="headerlink" title="Self-Correlation and Cross-Correlation Learning for Few-Shot Remote Sensing Image Semantic Segmentation"></a>Self-Correlation and Cross-Correlation Learning for Few-Shot Remote Sensing Image Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05840">http://arxiv.org/abs/2309.05840</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linhanwang/sccnet">https://github.com/linhanwang/sccnet</a></li>
<li>paper_authors: Linhan Wang, Shuo Lei, Jianfeng He, Shengkun Wang, Min Zhang, Chang-Tien Lu</li>
<li>for: 本研究旨在提出一种基于几何学相关学习的几何学相关学习网络，用于几何学相关学习图像semantic segmentation问题。</li>
<li>methods: 我们提出了一种自相关和十分相关学习网络（SCCNet），该模型通过考虑自相关和十分相关图像之间的相关性来增强分类预测的普适性。</li>
<li>results: 我们在两个遥感图像集上进行了广泛的实验，并证明了我们的模型在几何学相关学习图像semantic segmentation中的有效性和优势。<details>
<summary>Abstract</summary>
Remote sensing image semantic segmentation is an important problem for remote sensing image interpretation. Although remarkable progress has been achieved, existing deep neural network methods suffer from the reliance on massive training data. Few-shot remote sensing semantic segmentation aims at learning to segment target objects from a query image using only a few annotated support images of the target class. Most existing few-shot learning methods stem primarily from their sole focus on extracting information from support images, thereby failing to effectively address the large variance in appearance and scales of geographic objects. To tackle these challenges, we propose a Self-Correlation and Cross-Correlation Learning Network for the few-shot remote sensing image semantic segmentation. Our model enhances the generalization by considering both self-correlation and cross-correlation between support and query images to make segmentation predictions. To further explore the self-correlation with the query image, we propose to adopt a classical spectral method to produce a class-agnostic segmentation mask based on the basic visual information of the image. Extensive experiments on two remote sensing image datasets demonstrate the effectiveness and superiority of our model in few-shot remote sensing image semantic segmentation. Code and models will be accessed at https://github.com/linhanwang/SCCNet.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>遥感图像semantic segmentation是遥感图像解释中的重要问题。尽管已经取得了很大的进步，现有的深度神经网络方法却受到大量训练数据的依赖。几个shot遥感semantic segmentation目标是通过只使用少量标注图像来学习针对目标类图像进行分割。现有的几个shot学习方法主要围绕支持图像中的信息提取而设计，导致效果不够地处理大量的地理物体外观和比例差异。为解决这些挑战，我们提议一种基于自身相关和交叉相关学习网络（SCCNet）。我们的模型通过考虑支持和查询图像之间的自身相关和交叉相关来增强总体化。为进一步探索支持图像与查询图像之间的自身相关，我们提议采用一种经典的spectral方法生成基于图像的基本视觉信息的类型独立分割mask。我们的实验结果表明，我们的模型在几个shot遥感图像semantic segmentation中表现出色，并且与其他方法相比，具有更高的一致性和稳定性。代码和模型将在https://github.com/linhanwang/SCCNet上公开。
</details></li>
</ul>
<hr>
<h2 id="SCD-Net-Spatiotemporal-Clues-Disentanglement-Network-for-Self-supervised-Skeleton-based-Action-Recognition"><a href="#SCD-Net-Spatiotemporal-Clues-Disentanglement-Network-for-Self-supervised-Skeleton-based-Action-Recognition" class="headerlink" title="SCD-Net: Spatiotemporal Clues Disentanglement Network for Self-supervised Skeleton-based Action Recognition"></a>SCD-Net: Spatiotemporal Clues Disentanglement Network for Self-supervised Skeleton-based Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05834">http://arxiv.org/abs/2309.05834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cong Wu, Xiao-Jun Wu, Josef Kittler, Tianyang Xu, Sara Atito, Muhammad Awais, Zhenhua Feng</li>
<li>for: 本研究旨在提高skeleton-based动作识别的性能，通过分离空间和时间域的特征来提高contrastive学习的效果。</li>
<li>methods: 本研究提出了一种新的contrastive学习框架，即空间时间见识分解网络（SCD-Net），它通过综合decoupling模块和特征提取器来分离空间和时间域的特征。</li>
<li>results: 对于NTU-RGB+D（60&amp;120）和PKU-MMD（I&amp;II） datasets，我们的方法显著超越了现有的SOTA方法，并在多个下游任务上达到了优秀的性能，包括动作识别、动作检索、过渡学习和半监督学习。<details>
<summary>Abstract</summary>
Contrastive learning has achieved great success in skeleton-based action recognition. However, most existing approaches encode the skeleton sequences as entangled spatiotemporal representations and confine the contrasts to the same level of representation. Instead, this paper introduces a novel contrastive learning framework, namely Spatiotemporal Clues Disentanglement Network (SCD-Net). Specifically, we integrate the decoupling module with a feature extractor to derive explicit clues from spatial and temporal domains respectively. As for the training of SCD-Net, with a constructed global anchor, we encourage the interaction between the anchor and extracted clues. Further, we propose a new masking strategy with structural constraints to strengthen the contextual associations, leveraging the latest development from masked image modelling into the proposed SCD-Net. We conduct extensive evaluations on the NTU-RGB+D (60&120) and PKU-MMD (I&II) datasets, covering various downstream tasks such as action recognition, action retrieval, transfer learning, and semi-supervised learning. The experimental results demonstrate the effectiveness of our method, which outperforms the existing state-of-the-art (SOTA) approaches significantly.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对比学习在skeleton基因action认识中取得了很大成功。然而，现有的方法通常将骨架序列编码为杂合的空间时间表示，并将对比限制在同一个表示层次。而本文提出了一种新的对比学习框架，即空间时间准确网络（SCD-Net）。specifically，我们将分解模块与特征提取器结合，以 deriv implicit clue from空间和时间 DOMAIN  separately。在 SCD-Net 的训练中，我们使用构建的全球锚点，并且鼓励锚点和提取的 clue 之间的互动。此外，我们提出了一种新的masking strategy，以强制Contextual associations，利用图像模型的最新发展。我们在 NTU-RGB+D (60&120) 和 PKU-MMD (I&II)  datasets 进行了广泛的评估，覆盖了多种下游任务，如 action recognition、action retrieval、传输学习和半监督学习。实验结果表明，我们的方法可以很有效地与现有的 SOTA 方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Instance-Agnostic-Geometry-and-Contact-Dynamics-Learning"><a href="#Instance-Agnostic-Geometry-and-Contact-Dynamics-Learning" class="headerlink" title="Instance-Agnostic Geometry and Contact Dynamics Learning"></a>Instance-Agnostic Geometry and Contact Dynamics Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05832">http://arxiv.org/abs/2309.05832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengti Sun, Bowen Jiang, Bibit Bianchini, Camillo Jose Taylor, Michael Posa</li>
<li>for: 本文提出了一个无需知道物体形状和运动模型的静止学习框架，通过视觉和动力学的共同表示来同时学习物体的形状、运动轨迹和物理性质。</li>
<li>methods: 本文使用了BundleSDF视觉系统和ContactNets动力系统，并提出了一个循环训练管道，将动力模块输出用于修正视觉模块的姿态和形状，使用平面投影。</li>
<li>results: 实验表明，本文的框架可以学习静止和凹形物体的形状和动力学特性，并超越现有的跟踪框架。<details>
<summary>Abstract</summary>
This work presents an instance-agnostic learning framework that fuses vision with dynamics to simultaneously learn shape, pose trajectories and physical properties via the use of geometry as a shared representation. Unlike many contact learning approaches that assume motion capture input and a known shape prior for the collision model, our proposed framework learns an object's geometric and dynamic properties from RGBD video, without requiring either category-level or instance-level shape priors. We integrate a vision system, BundleSDF, with a dynamics system, ContactNets and propose a cyclic training pipeline to use the output from the dynamics module to refine the poses and the geometry from the vision module, using perspective reprojection. Experiments demonstrate our framework's ability to learn the geometry and dynamics of rigid and convex objects and improve upon the current tracking framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Mobile-Vision-Transformer-based-Visual-Object-Tracking"><a href="#Mobile-Vision-Transformer-based-Visual-Object-Tracking" class="headerlink" title="Mobile Vision Transformer-based Visual Object Tracking"></a>Mobile Vision Transformer-based Visual Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05829">http://arxiv.org/abs/2309.05829</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/goutamyg/mvt">https://github.com/goutamyg/mvt</a></li>
<li>paper_authors: Goutam Yelluru Gopal, Maria A. Amer</li>
<li>for: 提高目标跟踪算法的性能和速度，并在大规模数据集上实现高精度和高速度的跟踪。</li>
<li>methods: 使用Mobile Vision Transformers（MobileViT）作为后备，并提出了一种将模板和搜索区域表示 fusion 的新方法，以生成优化的目标位置编码。</li>
<li>results: 在大规模数据集 GOT10k 和 TrackingNet 上，我们的 MobileViT-based Tracker（MVT）的性能超过了当前的轻量级跟踪器，并且在 GPU 上运行的速度比 DiMP-50 快得多。 Code 和模型可以从 <a target="_blank" rel="noopener" href="https://github.com/goutamyg/MVT">https://github.com/goutamyg/MVT</a> 获取。<details>
<summary>Abstract</summary>
The introduction of robust backbones, such as Vision Transformers, has improved the performance of object tracking algorithms in recent years. However, these state-of-the-art trackers are computationally expensive since they have a large number of model parameters and rely on specialized hardware (e.g., GPU) for faster inference. On the other hand, recent lightweight trackers are fast but are less accurate, especially on large-scale datasets. We propose a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time. We also present a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. The experimental results show that our MobileViT-based Tracker, MVT, surpasses the performance of recent lightweight trackers on the large-scale datasets GOT10k and TrackingNet, and with a high inference speed. In addition, our method outperforms the popular DiMP-50 tracker despite having 4.7 times fewer model parameters and running at 2.8 times its speed on a GPU. The tracker code and models are available at https://github.com/goutamyg/MVT
</details>
<details>
<summary>摘要</summary>
Introduction of robust backbones, such as Vision Transformers, has improved the performance of object tracking algorithms in recent years. However, these state-of-the-art trackers are computationally expensive due to their large number of model parameters and reliance on specialized hardware (e.g., GPU) for faster inference. On the other hand, recent lightweight trackers are fast but less accurate, especially on large-scale datasets. We propose a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time. We also present a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. Experimental results show that our MobileViT-based Tracker, MVT, outperforms the performance of recent lightweight trackers on large-scale datasets GOT10k and TrackingNet, with high inference speed. Additionally, our method outperforms the popular DiMP-50 tracker despite having 4.7 times fewer model parameters and running at 2.8 times its speed on a GPU. Tracker code and models are available at https://github.com/goutamyg/MVT.
</details></li>
</ul>
<hr>
<h2 id="KD-FixMatch-Knowledge-Distillation-Siamese-Neural-Networks"><a href="#KD-FixMatch-Knowledge-Distillation-Siamese-Neural-Networks" class="headerlink" title="KD-FixMatch: Knowledge Distillation Siamese Neural Networks"></a>KD-FixMatch: Knowledge Distillation Siamese Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05826">http://arxiv.org/abs/2309.05826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chien-Chih Wang, Shaoyuan Xu, Jinmiao Fu, Yang Liu, Bryan Wang</li>
<li>for: 这篇论文的目的是提出一种具有知识传播的semi-supervised learning（SSL）算法，以解决深度学习中的标签数据短缺问题。</li>
<li>methods: 这篇论文使用了一种叫做FixMatch的SSL算法，并在其基础上增加了知识传播。 FixMatch使用了一个siamese神经网（SNN）来同时训练师和学生网络，并使用对应的标签来训练。 KD-FixMatch则是一种将知识传播添加到FixMatch中的新算法，并使用了组合的sequential和同时训练方法来提高性能。</li>
<li>results: 实验结果显示，KD-FixMatch在四个公开的数据集上都比 FixMatch 高效。 KD-FixMatch 可以从具有标签的数据集和无标签的数据集中获得更好的训练开头点，从而提高模型的性能。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) has become a crucial approach in deep learning as a way to address the challenge of limited labeled data. The success of deep neural networks heavily relies on the availability of large-scale high-quality labeled data. However, the process of data labeling is time-consuming and unscalable, leading to shortages in labeled data. SSL aims to tackle this problem by leveraging additional unlabeled data in the training process. One of the popular SSL algorithms, FixMatch, trains identical weight-sharing teacher and student networks simultaneously using a siamese neural network (SNN). However, it is prone to performance degradation when the pseudo labels are heavily noisy in the early training stage. We present KD-FixMatch, a novel SSL algorithm that addresses the limitations of FixMatch by incorporating knowledge distillation. The algorithm utilizes a combination of sequential and simultaneous training of SNNs to enhance performance and reduce performance degradation. Firstly, an outer SNN is trained using labeled and unlabeled data. After that, the network of the well-trained outer SNN generates pseudo labels for the unlabeled data, from which a subset of unlabeled data with trusted pseudo labels is then carefully created through high-confidence sampling and deep embedding clustering. Finally, an inner SNN is trained with the labeled data, the unlabeled data, and the subset of unlabeled data with trusted pseudo labels. Experiments on four public data sets demonstrate that KD-FixMatch outperforms FixMatch in all cases. Our results indicate that KD-FixMatch has a better training starting point that leads to improved model performance compared to FixMatch.
</details>
<details>
<summary>摘要</summary>
深度学习中的半监督学习（SSL）已成为一种重要的方法，以解决深度神经网络的受限于标注数据的问题。然而，数据标注是一个时间consuming和不可扩展的过程，导致标注数据的短缺。SSL利用额外的无标注数据来提高深度神经网络的性能。 FixMatch 是一种流行的 SSL 算法，它使用同一个 weight-sharing 教师和学生网络同时训练，使用 Siamese 神经网络（SNN）。然而， FixMatch 在早期训练阶段 pseudo labels 具有很强的噪音，可能导致性能下降。我们提出了 KD-FixMatch，一种新的 SSL 算法，通过搅合顺序和同时训练 SNNs 来提高性能并降低性能下降。首先，外部 SNN 通过标注和无标注数据进行训练。然后，外部 SNN 的网络生成 pseudo labels  для无标注数据，并从中选择一 subset of 无标注数据，通过高confidence 采样和深度嵌入划分来生成可信 pseudo labels。最后，内部 SNN 通过标注数据、无标注数据和可信 pseudo labels 进行训练。我们在四个公共数据集上进行了实验，结果显示 KD-FixMatch 在所有情况下都高于 FixMatch。我们的结果表明，KD-FixMatch 具有更好的训练开始点，导致深度神经网络的性能得到提高。
</details></li>
</ul>
<hr>
<h2 id="Rice-Plant-Disease-Detection-and-Diagnosis-using-Deep-Convolutional-Neural-Networks-and-Multispectral-Imaging"><a href="#Rice-Plant-Disease-Detection-and-Diagnosis-using-Deep-Convolutional-Neural-Networks-and-Multispectral-Imaging" class="headerlink" title="Rice Plant Disease Detection and Diagnosis using Deep Convolutional Neural Networks and Multispectral Imaging"></a>Rice Plant Disease Detection and Diagnosis using Deep Convolutional Neural Networks and Multispectral Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05818">http://arxiv.org/abs/2309.05818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yara Ali Alnaggar, Ahmad Sebaq, Karim Amer, ElSayed Naeem, Mohamed Elhelw</li>
<li>for: 本研究旨在提高rice plantaemia检测的精度，以帮助提高rice生产的效率和质量。</li>
<li>methods: 本研究使用多modal数据，包括公共多spectral和RGB图像集和深度学习pipeline，以检测rice crops疾病的早期阶段。</li>
<li>results: 研究发现，使用多spectral和RGB通道作为输入，可以获得更高的F1准确率，比使用RGB输入only更高。<details>
<summary>Abstract</summary>
Rice is considered a strategic crop in Egypt as it is regularly consumed in the Egyptian people's diet. Even though Egypt is the highest rice producer in Africa with a share of 6 million tons per year, it still imports rice to satisfy its local needs due to production loss, especially due to rice disease. Rice blast disease is responsible for 30% loss in rice production worldwide. Therefore, it is crucial to target limiting yield damage by detecting rice crops diseases in its early stages. This paper introduces a public multispectral and RGB images dataset and a deep learning pipeline for rice plant disease detection using multi-modal data. The collected multispectral images consist of Red, Green and Near-Infrared channels and we show that using multispectral along with RGB channels as input archives a higher F1 accuracy compared to using RGB input only.
</details>
<details>
<summary>摘要</summary>
rice 被视为埃及的战略作物，因为它是埃及人的日常饮食中的重要组成部分。尽管埃及是非洲最大的rice生产国，每年生产600万吨rice，但它仍然需要从外部进口rice来满足本地需求，尤其是因为生产损失，如rice疾病。rice疾病是全球rice生产损失的30%原因。因此，它是非常重要的target limiting yield damage by detecting rice crops diseases in its early stages。这篇论文介绍了一个公共多spectral和RGB图像集和一个深度学习管道，用于rice plant疾病检测以及多Modal数据。收集的多spectral图像包括红、绿和近红外通道，我们表明使用多spectral和RGB通道作为输入，可以达到高于RGB输入Only的F1准确率。
</details></li>
</ul>
<hr>
<h2 id="SHIFT3D-Synthesizing-Hard-Inputs-For-Tricking-3D-Detectors"><a href="#SHIFT3D-Synthesizing-Hard-Inputs-For-Tricking-3D-Detectors" class="headerlink" title="SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors"></a>SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05810">http://arxiv.org/abs/2309.05810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongge Chen, Zhao Chen, Gregory P. Meyer, Dennis Park, Carl Vondrick, Ashish Shrivastava, Yuning Chai</li>
<li>for: 用于生成 Structurally plausible yet challenging 3D shapes，以检测3D object detectors的漏洞。</li>
<li>methods: 使用 signed distanced function (SDF) 表示 объек，并通过权重错误信号来缓慢塑形或对象的pose进行变化，以混淆下游3D检测器。</li>
<li>results: 通过SHIFT3D方法生成的对象 physically differ from baseline object， yet retain semantically recognizable shape，可以提供3D检测器的可读性失败模式，帮助预先发现3D感知系统中的安全隐患。<details>
<summary>Abstract</summary>
We present SHIFT3D, a differentiable pipeline for generating 3D shapes that are structurally plausible yet challenging to 3D object detectors. In safety-critical applications like autonomous driving, discovering such novel challenging objects can offer insight into unknown vulnerabilities of 3D detectors. By representing objects with a signed distanced function (SDF), we show that gradient error signals allow us to smoothly deform the shape or pose of a 3D object in order to confuse a downstream 3D detector. Importantly, the objects generated by SHIFT3D physically differ from the baseline object yet retain a semantically recognizable shape. Our approach provides interpretable failure modes for modern 3D object detectors, and can aid in preemptive discovery of potential safety risks within 3D perception systems before these risks become critical failures.
</details>
<details>
<summary>摘要</summary>
我团队现在发布了Shift3D，一个可微分的管道，用于生成3D形状，这些形状具有可能挑战3D对象检测器的结构性可能性，但又保持semanticognizable的形状。在自动驾驶等安全关键应用中，发现这些新挑战的对象可以提供对3D检测器的不明显漏洞的知识。通过使用签名距离函数（SDF）表示对象，我们表明了下游3D检测器的导数误差信号，允许我们缓和形状或pose的变化，以搅乱下游3D检测器。这些由Shift3D生成的对象与基准对象有所不同，但它们仍保持semanticognizable的形状。我们的方法提供了可读取的失败模式，可以帮助在3D感知系统中发现潜在的安全风险之前，以避免这些风险变成 kritical failures。
</details></li>
</ul>
<hr>
<h2 id="Divergences-in-Color-Perception-between-Deep-Neural-Networks-and-Humans"><a href="#Divergences-in-Color-Perception-between-Deep-Neural-Networks-and-Humans" class="headerlink" title="Divergences in Color Perception between Deep Neural Networks and Humans"></a>Divergences in Color Perception between Deep Neural Networks and Humans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05809">http://arxiv.org/abs/2309.05809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan O. Nadler, Elise Darragh-Ford, Bhargav Srinivasa Desikan, Christian Conaway, Mark Chu, Tasker Hull, Douglas Guilbeault</li>
<li>for: 这个论文旨在研究深度神经网络（DNNs）是否能够模型人类视觉，以及它们是否能够捕捉人类视觉中的基本特征。</li>
<li>methods: 作者们采用了新的实验方法来评估DNNs中的色彩嵌入是否具有人类视觉中的含义性。他们还使用了在线调查来收集人类对图像的色彩相似性评估。</li>
<li>results: 研究发现，当前的DNN架构（包括卷积神经网络和视Transformer）在处理图像中的色彩相似性评估中表现不佳，其Result与人类对图像的色彩相似性评估存在很大差异。而基于wavelet分解的一种可解释的和有理性的色彩模型则能够更好地预测人类对图像的色彩相似性评估。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are increasingly proposed as models of human vision, bolstered by their impressive performance on image classification and object recognition tasks. Yet, the extent to which DNNs capture fundamental aspects of human vision such as color perception remains unclear. Here, we develop novel experiments for evaluating the perceptual coherence of color embeddings in DNNs, and we assess how well these algorithms predict human color similarity judgments collected via an online survey. We find that state-of-the-art DNN architectures $-$ including convolutional neural networks and vision transformers $-$ provide color similarity judgments that strikingly diverge from human color judgments of (i) images with controlled color properties, (ii) images generated from online searches, and (iii) real-world images from the canonical CIFAR-10 dataset. We compare DNN performance against an interpretable and cognitively plausible model of color perception based on wavelet decomposition, inspired by foundational theories in computational neuroscience. While one deep learning model $-$ a convolutional DNN trained on a style transfer task $-$ captures some aspects of human color perception, our wavelet algorithm provides more coherent color embeddings that better predict human color judgments compared to all DNNs we examine. These results hold when altering the high-level visual task used to train similar DNN architectures (e.g., image classification versus image segmentation), as well as when examining the color embeddings of different layers in a given DNN architecture. These findings break new ground in the effort to analyze the perceptual representations of machine learning algorithms and to improve their ability to serve as cognitively plausible models of human vision. Implications for machine learning, human perception, and embodied cognition are discussed.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在人类视觉模型中得到了广泛的应用，它们的表现在图像分类和物体识别任务上是极其出色的。然而，DNN是否能够捕捉人类视觉的基本特征，如色彩感知，还未得到了清楚的回答。在这篇文章中，我们开发了一系列新的实验来评估DNN中色彩嵌入的听觉性，并对DNN的预测与人类的色彩相似性判断进行比较。我们发现，包括卷积神经网络和视Transformers在内的当今最佳DNN架构，其对人类色彩判断的预测与人类实际上的色彩判断存在很大差异。我们将DNN的表现与基于计算神经科学的理论 inspirited by wavelet decomposition的一种可读性和认知可能的色彩模型进行比较。结果显示，一个 convolutional DNN 在样式传递任务上训练后 capture some aspects of human color perception，但我们的波let算法提供了更听觉性的色彩嵌入，可以更好地预测人类色彩判断。这些结果在不同的高级视觉任务（如图像分类和图像分割）和不同层次的DNN架构中都保持相同。这些发现开拓了对机器学习算法的听觉表征分析和改进其为人类视觉可能的模型的领域。文章结尾，我们讨论了机器学习、人类感知和embodied cognition等领域的影响。
</details></li>
</ul>
<hr>
<h2 id="Blendshapes-GHUM-Real-time-Monocular-Facial-Blendshape-Prediction"><a href="#Blendshapes-GHUM-Real-time-Monocular-Facial-Blendshape-Prediction" class="headerlink" title="Blendshapes GHUM: Real-time Monocular Facial Blendshape Prediction"></a>Blendshapes GHUM: Real-time Monocular Facial Blendshape Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05782">http://arxiv.org/abs/2309.05782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Grishchenko, Geng Yan, Eduard Gabriel Bazavan, Andrei Zanfir, Nikolai Chinaev, Karthik Raveendran, Matthias Grundmann, Cristian Sminchisescu</li>
<li>for: 该论文旨在实现在现代手机上实时预测人脸52个混合形态系数，以便实现人脸动作捕捉应用程序，如虚拟人物。</li>
<li>methods: 该论文提出了两大贡献：一是一种没有注释的离线方法，可以从真实的人类扫描中获取混合形态系数；二是一种轻量级的实时模型，可以基于人脸特征点预测混合形态系数。</li>
<li>results: 该论文在现代手机上实现了30+ FPS的实时预测，并且可以从单个彩色RGB图像中获取52个混合形态系数。<details>
<summary>Abstract</summary>
We present Blendshapes GHUM, an on-device ML pipeline that predicts 52 facial blendshape coefficients at 30+ FPS on modern mobile phones, from a single monocular RGB image and enables facial motion capture applications like virtual avatars. Our main contributions are: i) an annotation-free offline method for obtaining blendshape coefficients from real-world human scans, ii) a lightweight real-time model that predicts blendshape coefficients based on facial landmarks.
</details>
<details>
<summary>摘要</summary>
我们现在提供Blendshapes GHUM，一个在设备上的机器学习管道，可以在现代手机上预测52个 facial blendshape 系数，每秒30多帧，从单个灰度RGB图像中获取，并实现了人脸动作捕捉应用，如虚拟人物。我们的主要贡献包括：1. 无需注释的离线方法，可以从真实世界人体扫描中获取 blendshape 系数。2. 轻量级的实时模型，可以基于人脸特征点预测 blendshape 系数。
</details></li>
</ul>
<hr>
<h2 id="LUNet-Deep-Learning-for-the-Segmentation-of-Arterioles-and-Venules-in-High-Resolution-Fundus-Images"><a href="#LUNet-Deep-Learning-for-the-Segmentation-of-Arterioles-and-Venules-in-High-Resolution-Fundus-Images" class="headerlink" title="LUNet: Deep Learning for the Segmentation of Arterioles and Venules in High Resolution Fundus Images"></a>LUNet: Deep Learning for the Segmentation of Arterioles and Venules in High Resolution Fundus Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05780">http://arxiv.org/abs/2309.05780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Fhima, Jan Van Eijgen, Hana Kulenovic, Valérie Debeuf, Marie Vangilbergen, Marie-Isaline Billen, Heloïse Brackenier, Moti Freiman, Ingeborg Stalmans, Joachim A. Behar</li>
<li>For: The paper aims to develop a deep learning architecture for automated segmentation of retinal arterioles and venules in digital fundus images.* Methods: The proposed method, called LUNet, uses a double dilated convolutional block and a long tail to enhance the receptive field and resolution of the segmentation, respectively. The custom loss function emphasizes the continuity of the blood vessels.* Results: LUNet significantly outperforms two state-of-the-art segmentation algorithms on both the local test set and four external test sets simulating distribution shifts across ethnicity, comorbidities, and annotators.Here are the three key points in Simplified Chinese:* For: 这篇论文的目的是开发一种深度学习架构，用于自动分类retinal arterioles和venules在数字背景图像中。* Methods: 提出的方法（LUNet）使用了双倍扩展 convolutional block和长尾，以提高模型的接收场和分辨率。 Custom loss function 强调血管之间的连续性。* Results: LUNet 在本地测试集和四个外部测试集上，与两种现有的分割算法相比，显著地表现出优异。<details>
<summary>Abstract</summary>
The retina is the only part of the human body in which blood vessels can be accessed non-invasively using imaging techniques such as digital fundus images (DFI). The spatial distribution of the retinal microvasculature may change with cardiovascular diseases and thus the eyes may be regarded as a window to our hearts. Computerized segmentation of the retinal arterioles and venules (A/V) is essential for automated microvasculature analysis. Using active learning, we created a new DFI dataset containing 240 crowd-sourced manual A/V segmentations performed by fifteen medical students and reviewed by an ophthalmologist, and developed LUNet, a novel deep learning architecture for high resolution A/V segmentation. LUNet architecture includes a double dilated convolutional block that aims to enhance the receptive field of the model and reduce its parameter count. Furthermore, LUNet has a long tail that operates at high resolution to refine the segmentation. The custom loss function emphasizes the continuity of the blood vessels. LUNet is shown to significantly outperform two state-of-the-art segmentation algorithms on the local test set as well as on four external test sets simulating distribution shifts across ethnicity, comorbidities, and annotators. We make the newly created dataset open access (upon publication).
</details>
<details>
<summary>摘要</summary>
retina 是人体中唯一可以非侵入式通过图像技术访问血管的部分。通过计算机化分割，我们可以自动分析微血管网络。使用活动学习，我们创建了一个新的数字肉眼图像（DFI）集合，包括240名医学生 manually 手动 segmentation 和一位眼科医生审阅，并开发了 LUNet，一种新的深度学习架构，用于高分辨率血管分割。LUNet 架构包括一个双扩展 convolutional block，用于提高模型的感知范围和减少参数数量。此外，LUNet 还有一个长尾，用于在高分辨率下细化分割。我们定义了一个自定义损失函数，用于强调血管之间的连续性。LUNet 在本地测试集上以及四个外部测试集上，对比两种现有的分割算法，显著超越它们。我们将新创建的数据集开放给社区（在发表之前）。
</details></li>
</ul>
<hr>
<h2 id="TransferDoc-A-Self-Supervised-Transferable-Document-Representation-Learning-Model-Unifying-Vision-and-Language"><a href="#TransferDoc-A-Self-Supervised-Transferable-Document-Representation-Learning-Model-Unifying-Vision-and-Language" class="headerlink" title="TransferDoc: A Self-Supervised Transferable Document Representation Learning Model Unifying Vision and Language"></a>TransferDoc: A Self-Supervised Transferable Document Representation Learning Model Unifying Vision and Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05756">http://arxiv.org/abs/2309.05756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Souhail Bakkali, Sanket Biswas, Zuheng Ming, Mickael Coustaty, Marçal Rusiñol, Oriol Ramos Terrades, Josep Lladós</li>
<li>for: 本研究旨在提高视觉文档理解的效果， especialy in real-world online industrial settings。</li>
<li>methods: 本文提出了一种基于cross-modal transformer的 TransferDoc模型，通过自动学习的方式在不同模式下进行预处理，以提高模型的通用性、灵活性和 robustness。</li>
<li>results: 对于 downstream tasks, TransferDoc 模型表现出色，在 industrial evaluation scenario 中出perform other state-of-the-art approaches。<details>
<summary>Abstract</summary>
The field of visual document understanding has witnessed a rapid growth in emerging challenges and powerful multi-modal strategies. However, they rely on an extensive amount of document data to learn their pretext objectives in a ``pre-train-then-fine-tune'' paradigm and thus, suffer a significant performance drop in real-world online industrial settings. One major reason is the over-reliance on OCR engines to extract local positional information within a document page. Therefore, this hinders the model's generalizability, flexibility and robustness due to the lack of capturing global information within a document image. We introduce TransferDoc, a cross-modal transformer-based architecture pre-trained in a self-supervised fashion using three novel pretext objectives. TransferDoc learns richer semantic concepts by unifying language and visual representations, which enables the production of more transferable models. Besides, two novel downstream tasks have been introduced for a ``closer-to-real'' industrial evaluation scenario where TransferDoc outperforms other state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
领域中的视觉文档理解受到了快速增长的挑战和强大的多modal策略的推动。然而，它们依赖于大量的文档数据来学习其预先定义的目标任务，因此在实际上的线上工业环境中表现不佳。主要的原因是依赖于 OCR 引擎来提取文档页面上的局部位置信息，从而忽略了文档图像中的全局信息。因此，这会导致模型的普适性、灵活性和可靠性受到限制。我们介绍了 TransferDoc，一种基于 transferred 的 cross-modal transformer 架构，通过三种新的预先定义目标来预处理。TransferDoc 学习了更加具有 semantic 的概念，使得生成更加可转移的模型。此外，我们还引入了两个新的下游任务，以提供更加真实的工业评估场景，在这些场景下，TransferDoc 超过了其他状态计算的方法。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Reliability-of-CNN-Models-on-Classifying-Traffic-and-Road-Signs-using-LIME"><a href="#Evaluating-the-Reliability-of-CNN-Models-on-Classifying-Traffic-and-Road-Signs-using-LIME" class="headerlink" title="Evaluating the Reliability of CNN Models on Classifying Traffic and Road Signs using LIME"></a>Evaluating the Reliability of CNN Models on Classifying Traffic and Road Signs using LIME</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05747">http://arxiv.org/abs/2309.05747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Atiqur Rahman, Ahmed Saad Tanim, Sanjid Islam, Fahim Pranto, G. M. Shahariar, Md. Tanvir Rouf Shawon</li>
<li>for: 本研究旨在评估和比较四种当前最佳预训练模型（ResNet-34、VGG-19、DenseNet-121和Inception V3）在使用GTSRB公共数据集中分类交通和道路标志的效果。</li>
<li>methods: 本研究使用了GTSRB公共数据集进行测试和评估这些模型的预测精度和图像分类特征选择能力。同时，使用了LIME框架来增加模型预测的可解释性和可靠性。</li>
<li>results: 研究发现，使用LIME框架可以增加模型预测的可解释性和可靠性，并且可以提高模型在图像分类任务中的效果。结论：LIME是一种重要的工具，可以帮助改进机器学习模型的可解释性和可靠性，无论这些模型在图像分类任务中的性能如何。<details>
<summary>Abstract</summary>
The objective of this investigation is to evaluate and contrast the effectiveness of four state-of-the-art pre-trained models, ResNet-34, VGG-19, DenseNet-121, and Inception V3, in classifying traffic and road signs with the utilization of the GTSRB public dataset. The study focuses on evaluating the accuracy of these models' predictions as well as their ability to employ appropriate features for image categorization. To gain insights into the strengths and limitations of the model's predictions, the study employs the local interpretable model-agnostic explanations (LIME) framework. The findings of this experiment indicate that LIME is a crucial tool for improving the interpretability and dependability of machine learning models for image identification, regardless of the models achieving an f1 score of 0.99 on classifying traffic and road signs. The conclusion of this study has important ramifications for how these models are used in practice, as it is crucial to ensure that model predictions are founded on the pertinent image features.
</details>
<details>
<summary>摘要</summary>
本研究的目的是评估和比较四种现代预训练模型（ResNet-34、VGG-19、DenseNet-121和Inception V3）在使用GTSRB公共数据集进行交通和道路标志分类中的效果。研究旨在评估这些模型预测结果的准确性以及它们在图像分类中采用合适的特征。为了获得模型预测结果的含义和依赖性，研究使用了地方可解释性模型-无关的框架（LIME）。研究发现，LIME是一种重要的工具，可以提高图像识别模型的可解释性和可靠性，无论这些模型在交通和道路标志分类中的f1分数是0.99。这项研究的结论对于这些模型在实践中的使用具有重要的意义，因为需要确保模型的预测基于相关的图像特征。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Guided-Reconstruction-of-Everyday-Hand-Object-Interaction-Clips"><a href="#Diffusion-Guided-Reconstruction-of-Everyday-Hand-Object-Interaction-Clips" class="headerlink" title="Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips"></a>Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05663">http://arxiv.org/abs/2309.05663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JudyYe/diffhoi">https://github.com/JudyYe/diffhoi</a></li>
<li>paper_authors: Yufei Ye, Poorvi Hebbar, Abhinav Gupta, Shubham Tulsiani</li>
<li>for: 从短视频clip中重建手动物互动</li>
<li>methods: 使用3D推理为每个视频进行优化，并从视频中恢复神经网络表示的物体形状和时间变化性和手部运动</li>
<li>results: 对6种物体类别的 egocentric video进行了实验，与单视和多视方法相比，显示了显著的改善，并且可以重建来自YouTube的任意clip，包括1人和3人互动。<details>
<summary>Abstract</summary>
We tackle the task of reconstructing hand-object interactions from short video clips. Given an input video, our approach casts 3D inference as a per-video optimization and recovers a neural 3D representation of the object shape, as well as the time-varying motion and hand articulation. While the input video naturally provides some multi-view cues to guide 3D inference, these are insufficient on their own due to occlusions and limited viewpoint variations. To obtain accurate 3D, we augment the multi-view signals with generic data-driven priors to guide reconstruction. Specifically, we learn a diffusion network to model the conditional distribution of (geometric) renderings of objects conditioned on hand configuration and category label, and leverage it as a prior to guide the novel-view renderings of the reconstructed scene. We empirically evaluate our approach on egocentric videos across 6 object categories, and observe significant improvements over prior single-view and multi-view methods. Finally, we demonstrate our system's ability to reconstruct arbitrary clips from YouTube, showing both 1st and 3rd person interactions.
</details>
<details>
<summary>摘要</summary>
我们面临着从短视频clip中重建手对象交互的任务。给定输入视频，我们的方法将3D推断视为每个视频优化，并recover一个神经网络3D表示物体形状，以及时变动和手部骨骼运动。虽然输入视频自然提供一些多视图提示来导引3D推断，但这些提示不够准确，因为 occlusion 和有限的视角变化。为了获得高精度3D，我们将多视图信号与通用数据驱动的假设相结合，以便重建。特别是，我们学习了一种扩散网络，用于模型（geometry）视图的条件分布，根据手姿和类别标签来Conditional Rendering。我们观察了在6种物品类别上的 Egocentric 视频中，我们的方法与之前的单视和多视方法进行比较，并观察到了显著的改进。最后，我们示出了我们系统可以从 YouTube 上 reconstruction任意clip，包括1人和3人交互。
</details></li>
</ul>
<hr>
<h2 id="ViHOPE-Visuotactile-In-Hand-Object-6D-Pose-Estimation-with-Shape-Completion"><a href="#ViHOPE-Visuotactile-In-Hand-Object-6D-Pose-Estimation-with-Shape-Completion" class="headerlink" title="ViHOPE: Visuotactile In-Hand Object 6D Pose Estimation with Shape Completion"></a>ViHOPE: Visuotactile In-Hand Object 6D Pose Estimation with Shape Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05662">http://arxiv.org/abs/2309.05662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyu Li, Snehal Dikhale, Soshi Iba, Nawid Jamali</li>
<li>for: 本文提出了一种新的框架，用于使用视听触感来估计手中物体的6D姿态。</li>
<li>methods: 该框架使用一种条件生成 adversarial network来完成手中物体的形状，并与6D姿态估计任务共同优化。</li>
<li>results: 相比于直接将视听触感转换为6D姿态，该方法可以提高6D姿态估计的准确性。在视听形状完成任务中，我们超过了状态时的表现，并在Chamfer距离和Intersection of Union metric上减少了88%和265%的值。在视听姿态估计任务中，我们获得了35%和64%的位置和角度错误减少。<details>
<summary>Abstract</summary>
In this letter, we introduce ViHOPE, a novel framework for estimating the 6D pose of an in-hand object using visuotactile perception. Our key insight is that the accuracy of the 6D object pose estimate can be improved by explicitly completing the shape of the object. To this end, we introduce a novel visuotactile shape completion module that uses a conditional Generative Adversarial Network to complete the shape of an in-hand object based on volumetric representation. This approach improves over prior works that directly regress visuotactile observations to a 6D pose. By explicitly completing the shape of the in-hand object and jointly optimizing the shape completion and pose estimation tasks, we improve the accuracy of the 6D object pose estimate. We train and test our model on a synthetic dataset and compare it with the state-of-the-art. In the visuotactile shape completion task, we outperform the state-of-the-art by 265% using the Intersection of Union metric and achieve 88% lower Chamfer Distance. In the visuotactile pose estimation task, we present results that suggest our framework reduces position and angular errors by 35% and 64%, respectively. Furthermore, we ablate our framework to confirm the gain on the 6D object pose estimate from explicitly completing the shape. Ultimately, we show that our framework produces models that are robust to sim-to-real transfer on a real-world robot platform.
</details>
<details>
<summary>摘要</summary>
在这封信中，我们介绍了一个新的框架，即ViHOPE，用于估计手中物体的6D姿态。我们的关键发现是，通过显式完成手中物体的形状，可以提高6D物体姿态估计的准确性。为此，我们引入了一种新的视听形状完成模块，该模块使用conditional生成对抗网络来完成手中物体的形状基于体积表示。这种方法在直接从视听观测中预测6D姿态的先前作品上进行了改进。通过显式完成手中物体的形状并同时优化形状完成和姿态估计任务，我们提高了6D物体姿态估计的准确性。我们在一个 sintetic 数据集上训练和测试了我们的模型，并与当前最佳的状态 comparison。在视听形状完成任务中，我们使用Intersection of Union metric的比较，我们的模型在比较中高于当前最佳的状态，提高了88%的Chamfer Distance。在视听姿态估计任务中，我们提供了结果，表明我们的框架可以降低位置和angular error by 35%和64%，分别。此外，我们对我们的框架进行了剖析，并证明了在实际世界Robot平台上的robustness。最终，我们表明了我们的框架生成的模型在实际世界中是可靠的。
</details></li>
</ul>
<hr>
<h2 id="An-Effective-Two-stage-Training-Paradigm-Detector-for-Small-Dataset"><a href="#An-Effective-Two-stage-Training-Paradigm-Detector-for-Small-Dataset" class="headerlink" title="An Effective Two-stage Training Paradigm Detector for Small Dataset"></a>An Effective Two-stage Training Paradigm Detector for Small Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05652">http://arxiv.org/abs/2309.05652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Wang, Dong Xie, Hanzhi Wang, Jiang Tian</li>
<li>for: 这份报告是为了解决 object detection 领域中的标目探测问题。</li>
<li>methods: 这种方法使用了两阶段训练架构，首先预读 YOLOv8 的背景为Encoder，然后精确地训练探测器。在试验阶段，使用了复杂的扩展和重量组合。</li>
<li>results: 这种方法在 DelftBikes 测试集上得到了 30.4% 的平均精度，在领域中排名第四。<details>
<summary>Abstract</summary>
Learning from the limited amount of labeled data to the pre-train model has always been viewed as a challenging task. In this report, an effective and robust solution, the two-stage training paradigm YOLOv8 detector (TP-YOLOv8), is designed for the object detection track in VIPriors Challenge 2023. First, the backbone of YOLOv8 is pre-trained as the encoder using the masked image modeling technique. Then the detector is fine-tuned with elaborate augmentations. During the test stage, test-time augmentation (TTA) is used to enhance each model, and weighted box fusion (WBF) is implemented to further boost the performance. With the well-designed structure, our approach has achieved 30.4% average precision from 0.50 to 0.95 on the DelftBikes test set, ranking 4th on the leaderboard.
</details>
<details>
<summary>摘要</summary>
学习从有限的标注数据到预训练模型总是被视为一个挑战。在这份报告中，我们提出了一种有效和可靠的解决方案，即两stage训练 парадигмы（TP-YOLOv8），用于Object Detection track在VIPriors Challenge 2023中。首先，YOLOv8的背bone被用作Encoder，通过masked image modeling技术进行预训练。然后，检测器被细化地归一化。在测试阶段，使用测试时数学增强（TTA）以提高每个模型的性能，并实施Weighted Box Fusion（WBF）以进一步提高表现。基于我们的结构设计，我们的方法在DelftBikes测试集上达到了30.4%的平均精度，在领导者板块上排名第四。
</details></li>
</ul>
<hr>
<h2 id="CitDet-A-Benchmark-Dataset-for-Citrus-Fruit-Detection"><a href="#CitDet-A-Benchmark-Dataset-for-Citrus-Fruit-Detection" class="headerlink" title="CitDet: A Benchmark Dataset for Citrus Fruit Detection"></a>CitDet: A Benchmark Dataset for Citrus Fruit Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05645">http://arxiv.org/abs/2309.05645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan A. James, Heather K. Manching, Matthew R. Mattia, Kim D. Bowman, Amanda M. Hulse-Kemp, William J. Beksi</li>
<li>for: 该论文目的是提高 citrus 果实检测技术，以便更正确地估算 citrus 树上HLB病毒的影响。</li>
<li>methods: 该论文使用了现代对象检测算法，并在 Typical orchard 环境中进行了进一步的改进。</li>
<li>results: 该论文通过提供高分辨率图像和高质量 bounding box 约束，实现了 citrus 果实检测的高精度。此外，论文还显示了 citrus 果实的位置可以准确地反映 citrus 树上HLB病毒的影响，并与产量估算有直接的相关性。<details>
<summary>Abstract</summary>
In this letter, we present a new dataset to advance the state of the art in detecting citrus fruit and accurately estimate yield on trees affected by the Huanglongbing (HLB) disease in orchard environments via imaging. Despite the fact that significant progress has been made in solving the fruit detection problem, the lack of publicly available datasets has complicated direct comparison of results. For instance, citrus detection has long been of interest in the agricultural research community, yet there is an absence of work, particularly involving public datasets of citrus affected by HLB. To address this issue, we enhance state-of-the-art object detection methods for use in typical orchard settings. Concretely, we provide high-resolution images of citrus trees located in an area known to be highly affected by HLB, along with high-quality bounding box annotations of citrus fruit. Fruit on both the trees and the ground are labeled to allow for identification of fruit location, which contributes to advancements in yield estimation and potential measure of HLB impact via fruit drop. The dataset consists of over 32,000 bounding box annotations for fruit instances contained in 579 high-resolution images. In summary, our contributions are the following: (i) we introduce a novel dataset along with baseline performance benchmarks on multiple contemporary object detection algorithms, (ii) we show the ability to accurately capture fruit location on tree or on ground, and finally (ii) we present a correlation of our results with yield estimations.
</details>
<details>
<summary>摘要</summary>
在这封信中，我们介绍了一个新的数据集，以提高检测柑橘果的状态艺术在受 huanglongbing（HLB）病虫影响的orchard环境中。尽管在检测果实问题上已经取得了重要进步，但由于缺乏公共可用的数据集，对结果的直接比较受到了限制。例如，柑橘果检测已经在农业研究领域产生了长期的兴趣，但是没有具有公共数据集的相关研究，特别是涉及HLB病虫的柑橘果检测。为解决这个问题，我们改进了现有的object detection方法，以适应典型的orchard环境。具体来说，我们提供了高分辨率的柑橘树图像，以及高质量的 bounding box 约束标注。 fruit的位置可以在树上或地上被确定，这有助于提高产量估算和HLB病虫的影响度量。数据集包含了32,000个 bounding box 约束标注，分别包含579个高分辨率的图像。简而言之，我们的贡献包括以下几点：1. 我们引入了一个新的数据集，并提供了多种现代 object detection 算法的基准性能评价。2. 我们能够准确地捕捉柑橘果的位置，包括树上和地上的位置。3. 我们对我们的结果与产量估算之间的相关性进行了报告。
</details></li>
</ul>
<hr>
<h2 id="Learning-the-Geodesic-Embedding-with-Graph-Neural-Networks"><a href="#Learning-the-Geodesic-Embedding-with-Graph-Neural-Networks" class="headerlink" title="Learning the Geodesic Embedding with Graph Neural Networks"></a>Learning the Geodesic Embedding with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05613">http://arxiv.org/abs/2309.05613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Pang, Zhongtian Zheng, Guoping Wang, Peng-Shuai Wang</li>
<li>for: 计算精确地odesic距离 между两个精确的点 clouds，即在三维材料上的精确地odesic距离计算。</li>
<li>methods: 使用学习基于方法，通过嵌入矩阵到高维嵌入空间，计算精确地odesic距离。提出了新的图像卷积和图像聚合模块，以吸收地odesic信息，并证明其比前一代设计更有效。</li>
<li>results: 对ShapeNet进行了测试，并证明了对比 existed方法，具有orders of magnitude快的速度和相似或更好的准确性。同时，方法还能够在噪音和缺失点云上进行稳定的计算，并且具有强大的泛化能力。<details>
<summary>Abstract</summary>
We present GeGnn, a learning-based method for computing the approximate geodesic distance between two arbitrary points on discrete polyhedra surfaces with constant time complexity after fast precomputation. Previous relevant methods either focus on computing the geodesic distance between a single source and all destinations, which has linear complexity at least or require a long precomputation time. Our key idea is to train a graph neural network to embed an input mesh into a high-dimensional embedding space and compute the geodesic distance between a pair of points using the corresponding embedding vectors and a lightweight decoding function. To facilitate the learning of the embedding, we propose novel graph convolution and graph pooling modules that incorporate local geodesic information and are verified to be much more effective than previous designs. After training, our method requires only one forward pass of the network per mesh as precomputation. Then, we can compute the geodesic distance between a pair of points using our decoding function, which requires only several matrix multiplications and can be massively parallelized on GPUs. We verify the efficiency and effectiveness of our method on ShapeNet and demonstrate that our method is faster than existing methods by orders of magnitude while achieving comparable or better accuracy. Additionally, our method exhibits robustness on noisy and incomplete meshes and strong generalization ability on out-of-distribution meshes. The code and pretrained model can be found on https://github.com/IntelligentGeometry/GeGnn.
</details>
<details>
<summary>摘要</summary>
我们介绍GeGnn，一种基于学习的方法，用于计算粗略地理odesic距离 между两个随机点 на discrete polyhedra 表面上，具有常量时间复杂度以后快速预处理。先前的相关方法都是计算一个源点和所有目标点之间的 geodesic 距离，其复杂度至少是线性的，或者需要长时间的预处理。我们的关键想法是通过训练一个图神经网络，将输入网格 embedding 到高维 embedding 空间中，然后使用对应的 embedding  вектор和一个轻量级的解码函数计算 geodesic 距离。为了促进 embedding 的学习，我们提出了新的图神经和图聚合模块，它们在地理odesic 信息的本地特征上吸收了更多的信息，并被证明是远胜先前的设计。之后，我们只需要在网格上进行一次网络前向传播，然后可以使用我们的解码函数计算 geodesic 距离，这需要只需要几个矩阵乘法和可以大规模并行化在 GPU 上。我们证明了我们的方法在ShapeNet上的效率和有效性，并表明我们的方法比现有方法速度多orders of magnitude，同时具有相似或更好的准确性。此外，我们的方法在噪音和缺失网格上展现了鲁棒性，并且在不同网格上的泛化能力强。我们的代码和预训练模型可以在https://github.com/IntelligentGeometry/GeGnn 找到。
</details></li>
</ul>
<hr>
<h2 id="UniSeg-A-Unified-Multi-Modal-LiDAR-Segmentation-Network-and-the-OpenPCSeg-Codebase"><a href="#UniSeg-A-Unified-Multi-Modal-LiDAR-Segmentation-Network-and-the-OpenPCSeg-Codebase" class="headerlink" title="UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase"></a>UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05573">http://arxiv.org/abs/2309.05573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pjlab-adg/pcseg">https://github.com/pjlab-adg/pcseg</a></li>
<li>paper_authors: Youquan Liu, Runnan Chen, Xin Li, Lingdong Kong, Yuchen Yang, Zhaoyang Xia, Yeqi Bai, Xinge Zhu, Yuexin Ma, Yikang Li, Yu Qiao, Yuenan Hou</li>
<li>for: 这个论文的目的是提出一种多modal LiDAR分割网络（UniSeg），利用RGB图像和三种点云视图的信息，同时完成semantic分割和панOPTIC分割。</li>
<li>methods: 这个论文使用了自动将点云视图和图像特征相关联的Learnable cross-Modal Association（LMA）模块，并将增强的点云视图特征转换到点空间，以进行适应性的多视图相关联（LVA）。</li>
<li>results: 这个论文在三个公共评估benchmark上达到了优秀的结果，包括SemanticKITTI、nuScenes和Waymo Open Dataset（WOD）的LiDARsemantic分割和панOPTIC分割挑战赛。此外，这个论文还构建了OpenPCSeg代码库，它是最大和最全面的户外LiDAR分割代码库，包含大多数户外LiDAR分割算法和可重现实现。<details>
<summary>Abstract</summary>
Point-, voxel-, and range-views are three representative forms of point clouds. All of them have accurate 3D measurements but lack color and texture information. RGB images are a natural complement to these point cloud views and fully utilizing the comprehensive information of them benefits more robust perceptions. In this paper, we present a unified multi-modal LiDAR segmentation network, termed UniSeg, which leverages the information of RGB images and three views of the point cloud, and accomplishes semantic segmentation and panoptic segmentation simultaneously. Specifically, we first design the Learnable cross-Modal Association (LMA) module to automatically fuse voxel-view and range-view features with image features, which fully utilize the rich semantic information of images and are robust to calibration errors. Then, the enhanced voxel-view and range-view features are transformed to the point space,where three views of point cloud features are further fused adaptively by the Learnable cross-View Association module (LVA). Notably, UniSeg achieves promising results in three public benchmarks, i.e., SemanticKITTI, nuScenes, and Waymo Open Dataset (WOD); it ranks 1st on two challenges of two benchmarks, including the LiDAR semantic segmentation challenge of nuScenes and panoptic segmentation challenges of SemanticKITTI. Besides, we construct the OpenPCSeg codebase, which is the largest and most comprehensive outdoor LiDAR segmentation codebase. It contains most of the popular outdoor LiDAR segmentation algorithms and provides reproducible implementations. The OpenPCSeg codebase will be made publicly available at https://github.com/PJLab-ADG/PCSeg.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:Point-, voxel-, and range-views are three representative forms of point clouds. All of them have accurate 3D measurements but lack color and texture information. RGB images are a natural complement to these point cloud views and fully utilizing the comprehensive information of them benefits more robust perceptions. In this paper, we present a unified multi-modal LiDAR segmentation network, termed UniSeg, which leverages the information of RGB images and three views of the point cloud, and accomplishes semantic segmentation and panoptic segmentation simultaneously. Specifically, we first design the Learnable cross-Modal Association (LMA) module to automatically fuse voxel-view and range-view features with image features, which fully utilize the rich semantic information of images and are robust to calibration errors. Then, the enhanced voxel-view and range-view features are transformed to the point space,where three views of point cloud features are further fused adaptively by the Learnable cross-View Association module (LVA). Notably, UniSeg achieves promising results in three public benchmarks, i.e., SemanticKITTI, nuScenes, and Waymo Open Dataset (WOD); it ranks 1st on two challenges of two benchmarks, including the LiDAR semantic segmentation challenge of nuScenes and panoptic segmentation challenges of SemanticKITTI. Besides, we construct the OpenPCSeg codebase, which is the largest and most comprehensive outdoor LiDAR segmentation codebase. It contains most of the popular outdoor LiDAR segmentation algorithms and provides reproducible implementations. The OpenPCSeg codebase will be made publicly available at https://github.com/PJLab-ADG/PCSeg.Translated text in Simplified Chinese:Point-, voxel-, 和 range-views 是 LiDAR 点云的三种表示形式，它们都具有高精度的 3D 测量，但缺乏颜色和текстура信息。 RGB 图像是 LiDAR 点云视图的自然补充，完全利用图像的广泛 semantic 信息和 calibration 错误的Robustness。在这篇论文中，我们提出了一种多模态 LiDAR 分割网络，称为 UniSeg，它利用 RGB 图像和三种 LiDAR 点云视图，并在同时完成 semantic 分割和 panoptic 分割。具体来说，我们首先设计了 Learnable cross-Modal Association（LMA）模块，自动将 voxel-view 和 range-view 特征与图像特征进行关联，以全面利用图像的 semantic 信息和 calibration 错误的Robustness。然后，通过将增强的 voxel-view 和 range-view 特征转换到点空间，并在点云特征上进行 adaptive 关联，使得三种点云视图的特征得到了有效的融合。值得一提的是，UniSeg 在 SemanticKITTI、nuScenes 和 Waymo Open Dataset（WOD）三个公共 bencmarks 上表现出色，其中在 nuScenes 中的 LiDAR semantic 分割挑战和 SemanticKITTI 中的 panoptic 分割挑战上排名第一。此外，我们还建立了 OpenPCSeg 代码库，它是最大和最全面的 outdoor LiDAR 分割代码库，它包含了大多数流行的 outdoor LiDAR 分割算法，并提供了可重现的实现。OpenPCSeg 代码库将在 https://github.com/PJLab-ADG/PCSeg 上公开。
</details></li>
</ul>
<hr>
<h2 id="OpenFashionCLIP-Vision-and-Language-Contrastive-Learning-with-Open-Source-Fashion-Data"><a href="#OpenFashionCLIP-Vision-and-Language-Contrastive-Learning-with-Open-Source-Fashion-Data" class="headerlink" title="OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data"></a>OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05551">http://arxiv.org/abs/2309.05551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aimagelab/open-fashion-clip">https://github.com/aimagelab/open-fashion-clip</a></li>
<li>paper_authors: Giuseppe Cartella, Alberto Baldrati, Davide Morelli, Marcella Cornia, Marco Bertini, Rita Cucchiara</li>
<li>for: 这个论文的目的是提出一种基于视觉语言对比学习的开源时尚CLIP方法，以便在自动标签分类和多Modal检索等任务中实现可扩展和可靠的机器学习解决方案。</li>
<li>methods: 这个论文使用了开源时尚数据，并采用了视觉语言对比学习方法进行训练。</li>
<li>results: 实验结果表明，该方法具有显著的对外域泛化能力和稳定性，并在多个任务和benchmark上实现了STATE-OF-THE-ART的性能和准确率。<details>
<summary>Abstract</summary>
The inexorable growth of online shopping and e-commerce demands scalable and robust machine learning-based solutions to accommodate customer requirements. In the context of automatic tagging classification and multimodal retrieval, prior works either defined a low generalizable supervised learning approach or more reusable CLIP-based techniques while, however, training on closed source data. In this work, we propose OpenFashionCLIP, a vision-and-language contrastive learning method that only adopts open-source fashion data stemming from diverse domains, and characterized by varying degrees of specificity. Our approach is extensively validated across several tasks and benchmarks, and experimental results highlight a significant out-of-domain generalization capability and consistent improvements over state-of-the-art methods both in terms of accuracy and recall. Source code and trained models are publicly available at: https://github.com/aimagelab/open-fashion-clip.
</details>
<details>
<summary>摘要</summary>
“在线购物和电商的不断增长下，需要可扩展和可靠的机器学习解决方案来满足客户需求。在自动标签分类和多 modal 搜寻的上下文中，先前的工作 Either 定义了一个低通用的超vised 学习方法或更可重用的 CLIP 基本技术，而且将训练数据来源汇入关闭。在这个工作中，我们提出了 OpenFashionCLIP，一种视觉和语言对照学习方法，仅使用开源时装数据，来自多个领域，具有不同程度的特定性。我们的方法在多个任务和标准库中进行了广泛验证，实验结果显示了它在不同领域的外部测试能力和稳定性有很大提升，并且在精度和回传上也有显著的改善。源代码和训练模型可以在：https://github.com/aimagelab/open-fashion-clip 中下载。”
</details></li>
</ul>
<hr>
<h2 id="Distance-Aware-eXplanation-Based-Learning"><a href="#Distance-Aware-eXplanation-Based-Learning" class="headerlink" title="Distance-Aware eXplanation Based Learning"></a>Distance-Aware eXplanation Based Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05548">http://arxiv.org/abs/2309.05548</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/msgun/xbl-d">https://github.com/msgun/xbl-d</a></li>
<li>paper_authors: Misgina Tsighe Hagos, Niamh Belton, Kathleen M. Curran, Brian Mac Namee</li>
<li>for: This paper is written for training deep learning models with interactive learning approaches that provide transparent explanations of the model’s decisions.</li>
<li>methods: The paper proposes a method called distance-aware explanation loss, which adds a distance-based penalty to the categorical losses to train the model to focus on important regions of the training dataset.</li>
<li>results: The paper demonstrates the performance of the proposed method on three image classification tasks, and proposes an interpretability metric for evaluating visual feature-attribution based model explanations.<details>
<summary>Abstract</summary>
eXplanation Based Learning (XBL) is an interactive learning approach that provides a transparent method of training deep learning models by interacting with their explanations. XBL augments loss functions to penalize a model based on deviation of its explanations from user annotation of image features. The literature on XBL mostly depends on the intersection of visual model explanations and image feature annotations. We present a method to add a distance-aware explanation loss to categorical losses that trains a learner to focus on important regions of a training dataset. Distance is an appropriate approach for calculating explanation loss since visual model explanations such as Gradient-weighted Class Activation Mapping (Grad-CAMs) are not strictly bounded as annotations and their intersections may not provide complete information on the deviation of a model's focus from relevant image regions. In addition to assessing our model using existing metrics, we propose an interpretability metric for evaluating visual feature-attribution based model explanations that is more informative of the model's performance than existing metrics. We demonstrate performance of our proposed method on three image classification tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>批处学习（XBL）是一种互动式学习方法，它通过与模型的解释进行交互式训练深度学习模型。 XBL 将损失函数增强为对模型解释的偏差进行惩罚。 文献中的 XBL 主要基于图像特征标注和视觉模型解释的交集。 我们提出一种将距离意识到的解释损失添加到 categorical 损失函数中，以训练学习者专注于训练集中重要的区域。 距离是一种适当的方法 для计算解释损失，因为视觉模型的解释，如梯度权重分布图（Grad-CAMs），不是很准确的标注，而且它们的交集可能不会提供完整的信息，关于模型的ocus deviate from relevant image regions。 除了使用现有的metric来评估我们的模型，我们还提出了一种可以更加准确地评估视觉特征归属基于模型解释的解释度量。 我们在三个图像分类任务上展示了我们的提议的性能。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-the-detection-of-Out-Of-Distribution-samples-in-Multiple-Instance-Learning"><a href="#On-the-detection-of-Out-Of-Distribution-samples-in-Multiple-Instance-Learning" class="headerlink" title="On the detection of Out-Of-Distribution samples in Multiple Instance Learning"></a>On the detection of Out-Of-Distribution samples in Multiple Instance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05528">http://arxiv.org/abs/2309.05528</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/loic-lb/ood_mil">https://github.com/loic-lb/ood_mil</a></li>
<li>paper_authors: Loïc Le Bescond, Maria Vakalopoulou, Stergios Christodoulidis, Fabrice André, Hugues Talbot</li>
<li>for: 本研究旨在 Addressing the challenge of out-of-distribution (OOD) detection in weakly supervised learning Multiple Instance Learning (MIL) 框架中。</li>
<li>methods: 本研究采用了适应后OOD检测方法，并在多个公共数据集上进行了广泛的实验，以评估OOD检测性能在弱有监督的情况下。</li>
<li>results: 实验结果显示，DICE emerges as the best-performing method overall，但它在某些数据集上表现不佳，这说明OOD检测在MIL框架下是一个复杂和挑战性的话题。<details>
<summary>Abstract</summary>
The deployment of machine learning solutions in real-world scenarios often involves addressing the challenge of out-of-distribution (OOD) detection. While significant efforts have been devoted to OOD detection in classical supervised settings, the context of weakly supervised learning, particularly the Multiple Instance Learning (MIL) framework, remains under-explored. In this study, we tackle this challenge by adapting post-hoc OOD detection methods to the MIL setting while introducing a novel benchmark specifically designed to assess OOD detection performance in weakly supervised scenarios. Extensive experiments based on diverse public datasets do not reveal a single method with a clear advantage over the others. Although DICE emerges as the best-performing method overall, it exhibits significant shortcomings on some datasets, emphasizing the complexity of this under-explored and challenging topic. Our findings shed light on the complex nature of OOD detection under the MIL framework, emphasizing the importance of developing novel, robust, and reliable methods that can generalize effectively in a weakly supervised context. The code for the paper is available here: https://github.com/loic-lb/OOD_MIL.
</details>
<details>
<summary>摘要</summary>
deployment of machine learning solutions in real-world scenarios 常会面临对不同数据分布（Out-of-distribution，OOD）的挑战。虽然对约束学习（classical supervised learning）的OOD检测得到了很多努力，但是多例学习（Multiple Instance Learning，MIL）框架仍然尚未得到了充分的研究。在这篇研究中，我们对MIL框架中的OOD检测进行了适应，并创建了特别设计来评估OOD检测性能的实验室环境。经过了各种公开数据集的广泛实验，我们发现没有一个方法能够在所有数据集中表现出色，DICE获得了整体最好的成绩，但是在某些数据集上它具有明显的缺陷，这说明了MIL框架下OOD检测的复杂性和挑战性。我们的发现强调了在弱监督下进行OOD检测的重要性，需要开发出新的、可靠、有效的方法，以应对这种挑战。研究代码可以在以下链接中找到：https://github.com/loic-lb/OOD_MIL。
</details></li>
</ul>
<hr>
<h2 id="ReSimAD-Zero-Shot-3D-Domain-Transfer-for-Autonomous-Driving-with-Source-Reconstruction-and-Target-Simulation"><a href="#ReSimAD-Zero-Shot-3D-Domain-Transfer-for-Autonomous-Driving-with-Source-Reconstruction-and-Target-Simulation" class="headerlink" title="ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation"></a>ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05527">http://arxiv.org/abs/2309.05527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pjlab-adg/3dtrans">https://github.com/pjlab-adg/3dtrans</a></li>
<li>paper_authors: Bo Zhang, Xinyu Cai, Jiakang Yuan, Donglin Yang, Jianfei Guo, Renqiu Xia, Botian Shi, Min Dou, Tao Chen, Si Liu, Junchi Yan, Yu Qiao</li>
<li>for: 提高自动驾驶模型在不同领域中的适用性，alleviating the domain shifts problem.</li>
<li>methods: 提出了一种Reconstruction-Simulation-Perception（ReSimAD）方案，通过将知识从前一个领域转化为领域独特的表示，以提高领域总结能力。</li>
<li>results: 通过考虑不同的跨领域情况，如 Waymo-to-KITTI、Waymo-to-nuScenes 等，实验表明 ReSimAD 方法能够提高领域总结能力，甚至在 3D 预训练中表现出色。<details>
<summary>Abstract</summary>
Domain shifts such as sensor type changes and geographical situation variations are prevalent in Autonomous Driving (AD), which poses a challenge since AD model relying on the previous-domain knowledge can be hardly directly deployed to a new domain without additional costs. In this paper, we provide a new perspective and approach of alleviating the domain shifts, by proposing a Reconstruction-Simulation-Perception (ReSimAD) scheme. Specifically, the implicit reconstruction process is based on the knowledge from the previous old domain, aiming to convert the domain-related knowledge into domain-invariant representations, \textit{e.g.}, 3D scene-level meshes. Besides, the point clouds simulation process of multiple new domains is conditioned on the above reconstructed 3D meshes, where the target-domain-like simulation samples can be obtained, thus reducing the cost of collecting and annotating new-domain data for the subsequent perception process. For experiments, we consider different cross-domain situations such as Waymo-to-KITTI, Waymo-to-nuScenes, Waymo-to-ONCE, \textit{etc}, to verify the \textbf{zero-shot} target-domain perception using ReSimAD. Results demonstrate that our method is beneficial to boost the domain generalization ability, even promising for 3D pre-training.
</details>
<details>
<summary>摘要</summary>
域别变化（Domain shifts）是自动驾驶（Autonomous Driving）中的普遍问题，这对于自动驾驶模型而言是一大挑战，因为这些模型从前一个域别获得的知识很难直接应用到新的域别中。在这篇文章中，我们提出了一个新的见解和方法来解决域别变化问题，即提案了一个复原-模拟-观察（ReSimAD）方案。具体来说，这个方案的隐藏重建过程基于以前的旧域别知识，目的是将域别相关的知识转换为域别不对称的表示，例如3D场景级别的几何体。此外，模拟过程中的多个新域别的点 clouds是基于上述复原的3D几何体进行 conditioning，从而降低了获取和标注新域别数据的成本，以便在后续的观察过程中使用。实验中，我们考虑了不同的跨域别情况，如 Waymo-to-KITTI、Waymo-to-nuScenes、Waymo-to-ONCE等，以验证我们的方法在零数据目标域观察中的优化效果。结果显示，我们的方法可以增强域别普遍化能力，甚至对3D预训有推动作用。
</details></li>
</ul>
<hr>
<h2 id="Stream-based-Active-Learning-by-Exploiting-Temporal-Properties-in-Perception-with-Temporal-Predicted-Loss"><a href="#Stream-based-Active-Learning-by-Exploiting-Temporal-Properties-in-Perception-with-Temporal-Predicted-Loss" class="headerlink" title="Stream-based Active Learning by Exploiting Temporal Properties in Perception with Temporal Predicted Loss"></a>Stream-based Active Learning by Exploiting Temporal Properties in Perception with Temporal Predicted Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05517">http://arxiv.org/abs/2309.05517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Schmidt, Stephan Günnemann</li>
<li>for: 这 paper 是关于活动学习（AL），它可以减少机器学习模型训练所需的标注数据量。</li>
<li>methods: 这 paper 使用了一种新的 temporal predicted loss（TPL）方法，它利用了图像流的时间性质量进行过滤。</li>
<li>results: 实验表明，TPL 方法可以大幅提高选择的多样性，同时比 pool-based 方法更快。TPL 还与州界前面的 pool-based 和流程基于的方法进行比较，显示它在不同的模型上表现更出色。<details>
<summary>Abstract</summary>
Active learning (AL) reduces the amount of labeled data needed to train a machine learning model by intelligently choosing which instances to label. Classic pool-based AL requires all data to be present in a datacenter, which can be challenging with the increasing amounts of data needed in deep learning. However, AL on mobile devices and robots, like autonomous cars, can filter the data from perception sensor streams before reaching the datacenter. We exploited the temporal properties for such image streams in our work and proposed the novel temporal predicted loss (TPL) method. To evaluate the stream-based setting properly, we introduced the GTA V streets and the A2D2 streets dataset and made both publicly available. Our experiments showed that our approach significantly improves the diversity of the selection while being an uncertainty-based method. As pool-based approaches are more common in perception applications, we derived a concept for comparing pool-based and stream-based AL, where TPL out-performed state-of-the-art pool- or stream-based approaches for different models. TPL demonstrated a gain of 2.5 precept points (pp) less required data while being significantly faster than pool-based methods.
</details>
<details>
<summary>摘要</summary>
aktive lerning (AL) 可以减少用于训练机器学习模型的标注数据量，通过智能地选择需要标注的实例。 классиic pool-based AL 需要所有数据都存在数据中心，这可能是深度学习中所需的数据量增加的挑战。然而， AL 在移动设备和机器人（如自动驾驶车）上可以从感知传感器流中筛选数据，以避免数据中心的压力。我们利用了图像流的时间性质量，并提出了新的时间预测损失（TPL）方法。为了正确评估流式设置，我们介绍了 GTA V 街道和 A2D2 街道数据集，并将其公开发布。我们的实验表明，我们的方法可以明显提高选择的多样性，而且是一种uncertainty-based方法。在感知应用中，pool-based方法更常见，因此我们提出了对 pool-based 和流式 AL 进行比较的概念，并证明 TPL 在不同的模型上都能够超过状态对照方法。TPL 在训练过程中得到了2.5个预言点（pp） menos的数据量，同时比 pool-based 方法更快。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Co-salient-Object-Detection-Framework"><a href="#Zero-Shot-Co-salient-Object-Detection-Framework" class="headerlink" title="Zero-Shot Co-salient Object Detection Framework"></a>Zero-Shot Co-salient Object Detection Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05499">http://arxiv.org/abs/2309.05499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoke Xiao, Lv Tang, Bo Li, Zhiming Luo, Shaozi Li</li>
<li>for: 本研究旨在模仿人类视觉系统中识别图像集中的共同和突出对象的能力。</li>
<li>methods: 我们提出了一种采用基础计算机视觉模型的零标注CoSOD框架，不需要任何训练过程。我们还 introduce two novel component：集体提示生成模块（GPG）和共聚焦度图生成模块（CMP）。</li>
<li>results: 我们对广泛使用的 dataset 进行评估，并观察了非常出色的结果。我们的方法超过了现有的无监督方法，甚至超过了2020年之前开发的完全监督方法，而且与2022年之前开发的一些完全监督方法保持竞争力。<details>
<summary>Abstract</summary>
Co-salient Object Detection (CoSOD) endeavors to replicate the human visual system's capacity to recognize common and salient objects within a collection of images. Despite recent advancements in deep learning models, these models still rely on training with well-annotated CoSOD datasets. The exploration of training-free zero-shot CoSOD frameworks has been limited. In this paper, taking inspiration from the zero-shot transfer capabilities of foundational computer vision models, we introduce the first zero-shot CoSOD framework that harnesses these models without any training process. To achieve this, we introduce two novel components in our proposed framework: the group prompt generation (GPG) module and the co-saliency map generation (CMP) module. We evaluate the framework's performance on widely-used datasets and observe impressive results. Our approach surpasses existing unsupervised methods and even outperforms fully supervised methods developed before 2020, while remaining competitive with some fully supervised methods developed before 2022.
</details>
<details>
<summary>摘要</summary>
Note: "CoSOD" is a abbreviation of "Co-salient Object Detection".Here's the translation in Simplified Chinese:CoSOD (共同焦点物体检测) 目标是模仿人类视觉系统在一组图像中识别共同焦点和突出的物体。尽管最近的深度学习模型已经取得了一定的进步，但这些模型仍然需要基于良好注解的 CoSOD 数据集进行训练。训练自由零shot CoSOD 框架的探索受限。在这篇论文中，我们 inspirited 由基础计算机视觉模型的零shot 传输能力，我们提出了第一个无需训练的 CoSOD 框架。为实现这一点，我们提出了两个新的组件：集体提示生成（GPG）模块和共同焦点图生成（CMP）模块。我们在广泛使用的数据集上评估了该框架的性能，并观察到了非常出色的结果。我们的方法超过了现有的无监督方法，甚至超过了2020年之前开发的完全监督方法，同时与2022年之前开发的一些完全监督方法维持竞争力。
</details></li>
</ul>
<hr>
<h2 id="COMPASS-High-Efficiency-Deep-Image-Compression-with-Arbitrary-scale-Spatial-Scalability"><a href="#COMPASS-High-Efficiency-Deep-Image-Compression-with-Arbitrary-scale-Spatial-Scalability" class="headerlink" title="COMPASS: High-Efficiency Deep Image Compression with Arbitrary-scale Spatial Scalability"></a>COMPASS: High-Efficiency Deep Image Compression with Arbitrary-scale Spatial Scalability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07926">http://arxiv.org/abs/2309.07926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ImJongminPark/COMPASS">https://github.com/ImJongminPark/COMPASS</a></li>
<li>paper_authors: Jongmin Park, Jooyoung Lee, Munchurl Kim</li>
<li>for:  This paper proposes a novel neural network (NN)-based spatially scalable image compression method called COMPASS, which supports arbitrary-scale spatial scalability and has a flexible structure that allows for arbitrary determination of the number of layers and their respective scale factors during inference.</li>
<li>methods:  The proposed COMPASS method uses an inter-layer arbitrary scale prediction method called LIFF based on implicit neural representation to reduce spatial redundancy between adjacent layers for arbitrary scale factors. The method also uses a combined RD loss function to effectively train multiple layers.</li>
<li>results:  The experimental results show that COMPASS achieves BD-rate gain of -58.33% and -47.17% at maximum compared to SHVC and the state-of-the-art NN-based spatially scalable image compression method, respectively, for various combinations of scale factors. Additionally, COMPASS shows comparable or even better coding efficiency than the single-layer coding for various scale factors.<details>
<summary>Abstract</summary>
Recently, neural network (NN)-based image compression studies have actively been made and has shown impressive performance in comparison to traditional methods. However, most of the works have focused on non-scalable image compression (single-layer coding) while spatially scalable image compression has drawn less attention although it has many applications. In this paper, we propose a novel NN-based spatially scalable image compression method, called COMPASS, which supports arbitrary-scale spatial scalability. Our proposed COMPASS has a very flexible structure where the number of layers and their respective scale factors can be arbitrarily determined during inference. To reduce the spatial redundancy between adjacent layers for arbitrary scale factors, our COMPASS adopts an inter-layer arbitrary scale prediction method, called LIFF, based on implicit neural representation. We propose a combined RD loss function to effectively train multiple layers. Experimental results show that our COMPASS achieves BD-rate gain of -58.33% and -47.17% at maximum compared to SHVC and the state-of-the-art NN-based spatially scalable image compression method, respectively, for various combinations of scale factors. Our COMPASS also shows comparable or even better coding efficiency than the single-layer coding for various scale factors.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dual-view-Curricular-Optimal-Transport-for-Cross-lingual-Cross-modal-Retrieval"><a href="#Dual-view-Curricular-Optimal-Transport-for-Cross-lingual-Cross-modal-Retrieval" class="headerlink" title="Dual-view Curricular Optimal Transport for Cross-lingual Cross-modal Retrieval"></a>Dual-view Curricular Optimal Transport for Cross-lingual Cross-modal Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05451">http://arxiv.org/abs/2309.05451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yabing Wang, Shuhui Wang, Hao Luo, Jianfeng Dong, Fan Wang, Meng Han, Xun Wang, Meng Wang</li>
<li>for: 本研究旨在打破非英语标注数据的限制，提高cross-modal retrieval的多语言可用性。</li>
<li>methods: 我们提出了双视角优化交通（DCOT）方法，利用优化交通理论从两个视角量化样本对的相对性，并通过双视curriculum学习动态设置交通成本。</li>
<li>results: 我们在两个多语言图像文本数据集和一个视频文本数据集上进行了广泛的实验，结果表明我们的提议方法具有效果和稳定性，同时也能够扩展到跨语言图像文本基线和 OUT-OF-DOMAIN 数据。<details>
<summary>Abstract</summary>
Current research on cross-modal retrieval is mostly English-oriented, as the availability of a large number of English-oriented human-labeled vision-language corpora. In order to break the limit of non-English labeled data, cross-lingual cross-modal retrieval (CCR) has attracted increasing attention. Most CCR methods construct pseudo-parallel vision-language corpora via Machine Translation (MT) to achieve cross-lingual transfer. However, the translated sentences from MT are generally imperfect in describing the corresponding visual contents. Improperly assuming the pseudo-parallel data are correctly correlated will make the networks overfit to the noisy correspondence. Therefore, we propose Dual-view Curricular Optimal Transport (DCOT) to learn with noisy correspondence in CCR. In particular, we quantify the confidence of the sample pair correlation with optimal transport theory from both the cross-lingual and cross-modal views, and design dual-view curriculum learning to dynamically model the transportation costs according to the learning stage of the two views. Extensive experiments are conducted on two multilingual image-text datasets and one video-text dataset, and the results demonstrate the effectiveness and robustness of the proposed method. Besides, our proposed method also shows a good expansibility to cross-lingual image-text baselines and a decent generalization on out-of-domain data.
</details>
<details>
<summary>摘要</summary>
当前的跨模态检索研究大多是英语 oriented，因为有大量的英语人工标注的视觉语言数据。为了突破非英语标注数据的限制，跨语言跨模态检索（CCR）已经吸引了越来越多的关注。大多数 CCR 方法使用机器翻译（MT）construct pseudo-parallel vision-language corpora以实现跨语言传递。然而，由 MT 翻译的句子通常不能准确描述相应的视觉内容。如果不正确地假设 pseudo-parallel 数据是正确相关的，那么网络会适应到噪音相关性。因此，我们提出了双视角最优运输（DCOT），以学习噪音相关性在 CCR 中。具体来说，我们使用optimal transport理论从双视角量度来衡量样本对的相关性，并设计了双视角课程学习来动态模型运输成本 According to the learning stage of the two views。我们在两个多语言图像文本数据集和一个视频文本数据集上进行了广泛的实验，并得到了我们提posed方法的效果和稳定性。此外，我们的提出方法还显示了跨语言图像文本基elines的好适用性和out-of-domain数据的 descent generalization。
</details></li>
</ul>
<hr>
<h2 id="A-Localization-to-Segmentation-Framework-for-Automatic-Tumor-Segmentation-in-Whole-Body-PET-CT-Images"><a href="#A-Localization-to-Segmentation-Framework-for-Automatic-Tumor-Segmentation-in-Whole-Body-PET-CT-Images" class="headerlink" title="A Localization-to-Segmentation Framework for Automatic Tumor Segmentation in Whole-Body PET&#x2F;CT Images"></a>A Localization-to-Segmentation Framework for Automatic Tumor Segmentation in Whole-Body PET&#x2F;CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05446">http://arxiv.org/abs/2309.05446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medcai/l2snet">https://github.com/medcai/l2snet</a></li>
<li>paper_authors: Linghan Cai, Jianhao Huang, Zihang Zhu, Jinpeng Lu, Yongbing Zhang</li>
<li>for: 这篇论文旨在提出一个专门用于检测一些癌症，如肺癌和淋巴癌的fluorodeoxyglucose（FDG） positron emission tomography（PET）复合成像中的自动肢解析方法，以改善医生的工作负担，进而提高诊断质量。</li>
<li>methods: 这篇论文提出了一个名为L2SNet的本地化-分类框架，用于精确地分类肿瘤。L2SNet首先在肿瘤Localization阶段寻找可能的肿瘤区域，然后使用这些位置的启发信号来塑造分类结果。为了进一步提高L2SNet的分类性能，我们设计了一个适应的阈值方案，将两个阶段的分类结果考虑在内。</li>
<li>results: 在MICCAI 2023 Automated Lesion Segmentation in Whole-Body FDG-PET&#x2F;CT challenge dataset上进行实验，我们的方法在预选测试集上取得了竞争性的结果，排名在前7名之间。<details>
<summary>Abstract</summary>
Fluorodeoxyglucose (FDG) positron emission tomography (PET) combined with computed tomography (CT) is considered the primary solution for detecting some cancers, such as lung cancer and melanoma. Automatic segmentation of tumors in PET/CT images can help reduce doctors' workload, thereby improving diagnostic quality. However, precise tumor segmentation is challenging due to the small size of many tumors and the similarity of high-uptake normal areas to the tumor regions. To address these issues, this paper proposes a localization-to-segmentation framework (L2SNet) for precise tumor segmentation. L2SNet first localizes the possible lesions in the lesion localization phase and then uses the location cues to shape the segmentation results in the lesion segmentation phase. To further improve the segmentation performance of L2SNet, we design an adaptive threshold scheme that takes the segmentation results of the two phases into consideration. The experiments with the MICCAI 2023 Automated Lesion Segmentation in Whole-Body FDG-PET/CT challenge dataset show that our method achieved a competitive result and was ranked in the top 7 methods on the preliminary test set. Our work is available at: https://github.com/MedCAI/L2SNet.
</details>
<details>
<summary>摘要</summary>
富含氟代谐糖蛋白 (FDG)  позиトрон辐射Tomography (PET) 与计算机断层成像 (CT) 被视为检测一些肿瘤的首选方法，如肺癌和黑色素瘤。自动将肿瘤 segmented 出 PET/CT 图像中可以减轻医生的工作负担，从而提高诊断质量。然而，准确地 segmenting 肿瘤是困难的，因为许多肿瘤的体积很小，而且高吸收的正常区域与肿瘤区域相似。为解决这些问题，本文提出了一个 localization-to-segmentation 框架 (L2SNet)，用于准确地 segmenting 肿瘤。L2SNet 首先在肿瘤localization阶段确定可能的肿瘤，然后使用位置提示来形成 segmentation 结果。为了进一步提高 L2SNet 的 segmentation 性能，我们设计了一种适应reshold scheme，该 scheme 根据 segmentation 结果来调整阈值。我们的实验表明，使用 MICCAI 2023 自动肿瘤 segmentation in Whole-Body FDG-PET/CT 挑战数据集，我们的方法在预liminary test set 上获得了竞争性的结果，并列在前 7 名。我们的工作可以在 GitHub 上找到：https://github.com/MedCAI/L2SNet。
</details></li>
</ul>
<hr>
<h2 id="Towards-Content-based-Pixel-Retrieval-in-Revisited-Oxford-and-Paris"><a href="#Towards-Content-based-Pixel-Retrieval-in-Revisited-Oxford-and-Paris" class="headerlink" title="Towards Content-based Pixel Retrieval in Revisited Oxford and Paris"></a>Towards Content-based Pixel Retrieval in Revisited Oxford and Paris</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05438">http://arxiv.org/abs/2309.05438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anguoyuan/pixel_retrieval-segmented_instance_retrieval">https://github.com/anguoyuan/pixel_retrieval-segmented_instance_retrieval</a></li>
<li>paper_authors: Guoyuan An, Woo Jae Kim, Saelyne Yang, Rong Li, Yuchi Huo, Sung-Eui Yoon</li>
<li>for: 本研究旨在提供首个像素检索Benchmark，用于实现分割实例检索。</li>
<li>methods: 本研究使用PROxford和PRParis两个基于ROxford和RParis图像检索数据集的像素检索Benchmark，并进行了三名专业标注员的二次双重检查和精度提升。</li>
<li>results: 研究结果表明，像素检索任务对现有方法来说是一个挑战，与现有问题不同，这表明进一步研究可以提高内容基于像素检索的用户搜索体验。<details>
<summary>Abstract</summary>
This paper introduces the first two pixel retrieval benchmarks. Pixel retrieval is segmented instance retrieval. Like semantic segmentation extends classification to the pixel level, pixel retrieval is an extension of image retrieval and offers information about which pixels are related to the query object. In addition to retrieving images for the given query, it helps users quickly identify the query object in true positive images and exclude false positive images by denoting the correlated pixels. Our user study results show pixel-level annotation can significantly improve the user experience.   Compared with semantic and instance segmentation, pixel retrieval requires a fine-grained recognition capability for variable-granularity targets. To this end, we propose pixel retrieval benchmarks named PROxford and PRParis, which are based on the widely used image retrieval datasets, ROxford and RParis. Three professional annotators label 5,942 images with two rounds of double-checking and refinement. Furthermore, we conduct extensive experiments and analysis on the SOTA methods in image search, image matching, detection, segmentation, and dense matching using our pixel retrieval benchmarks. Results show that the pixel retrieval task is challenging to these approaches and distinctive from existing problems, suggesting that further research can advance the content-based pixel-retrieval and thus user search experience. The datasets can be downloaded from \href{https://github.com/anguoyuan/Pixel_retrieval-Segmented_instance_retrieval}{this link}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FlowIBR-Leveraging-Pre-Training-for-Efficient-Neural-Image-Based-Rendering-of-Dynamic-Scenes"><a href="#FlowIBR-Leveraging-Pre-Training-for-Efficient-Neural-Image-Based-Rendering-of-Dynamic-Scenes" class="headerlink" title="FlowIBR: Leveraging Pre-Training for Efficient Neural Image-Based Rendering of Dynamic Scenes"></a>FlowIBR: Leveraging Pre-Training for Efficient Neural Image-Based Rendering of Dynamic Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05418">http://arxiv.org/abs/2309.05418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Büsching, Josef Bengtson, David Nilsson, Mårten Björkman</li>
<li>for: 这个论文的目的是为了实现单目视图合成动态场景。</li>
<li>methods: 这个方法使用神经网络基于图像渲染方法，在大量可用的静止场景数据集上进行预训练，然后使用每个场景的优化的场景流场谱来抵消场景动力，使摄像机辐射线与场景动力相抵消，以present the dynamic scene as if it were static to the rendering network。</li>
<li>results: 该方法可以在单个消费级GPU上实现near-optimal results，并且可以减少每个场景优化时间量by an order of magnitude。<details>
<summary>Abstract</summary>
We introduce a novel approach for monocular novel view synthesis of dynamic scenes. Existing techniques already show impressive rendering quality but tend to focus on optimization within a single scene without leveraging prior knowledge. This limitation has been primarily attributed to the lack of datasets of dynamic scenes available for training and the diversity of scene dynamics. Our method FlowIBR circumvents these issues by integrating a neural image-based rendering method, pre-trained on a large corpus of widely available static scenes, with a per-scene optimized scene flow field. Utilizing this flow field, we bend the camera rays to counteract the scene dynamics, thereby presenting the dynamic scene as if it were static to the rendering network. The proposed method reduces per-scene optimization time by an order of magnitude, achieving comparable results to existing methods - all on a single consumer-grade GPU.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的笔脚渲染方法，用于单视图动态场景的synthesis。现有技术已经达到了非常出色的渲染质量，但它们通常会在单个场景中优化，而不是利用先前的知识。这种限制主要归结于动态场景的数据集不足以进行训练，以及场景动态的多样性。我们的方法FlowIBR通过将神经网络图像基于渲染方法与每个场景优化的场景流场融合，使用这个流场场融合了摄像机杆的弯曲，使动态场景被渲染为如果是静止的，并且通过这种方法可以大幅降低每个场景优化时间，达到与现有方法相同的结果，全部在单个Consumer-grade GPU上进行。
</details></li>
</ul>
<hr>
<h2 id="Treatment-aware-Diffusion-Probabilistic-Model-for-Longitudinal-MRI-Generation-and-Diffuse-Glioma-Growth-Prediction"><a href="#Treatment-aware-Diffusion-Probabilistic-Model-for-Longitudinal-MRI-Generation-and-Diffuse-Glioma-Growth-Prediction" class="headerlink" title="Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI Generation and Diffuse Glioma Growth Prediction"></a>Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI Generation and Diffuse Glioma Growth Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05406">http://arxiv.org/abs/2309.05406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghui Liu, Elies Fuster-Garcia, Ivar Thokle Hovden, Donatas Sederevicius, Karoline Skogen, Bradley J MacIntosh, Edvard Grødem, Till Schellhorn, Petter Brandal, Atle Bjørnerud, Kyrre Eeg Emblem</li>
<li>For:  This paper presents a novel end-to-end network for generating future tumor masks and realistic MRIs of how the tumor will look at any future time points for different treatment plans.* Methods: The approach is based on cutting-edge diffusion probabilistic models and deep-segmentation neural networks, using sequential multi-parametric magnetic resonance images (MRI) and treatment information as conditioning inputs to guide the generative diffusion process.* Results: The model has demonstrated promising performance across a range of tasks, including the generation of high-quality synthetic MRIs with tumor masks, time-series tumor segmentations, and uncertainty estimates.<details>
<summary>Abstract</summary>
Diffuse gliomas are malignant brain tumors that grow widespread through the brain. The complex interactions between neoplastic cells and normal tissue, as well as the treatment-induced changes often encountered, make glioma tumor growth modeling challenging. In this paper, we present a novel end-to-end network capable of generating future tumor masks and realistic MRIs of how the tumor will look at any future time points for different treatment plans. Our approach is based on cutting-edge diffusion probabilistic models and deep-segmentation neural networks. We included sequential multi-parametric magnetic resonance images (MRI) and treatment information as conditioning inputs to guide the generative diffusion process. This allows for tumor growth estimates at any given time point. We trained the model using real-world postoperative longitudinal MRI data with glioma tumor growth trajectories represented as tumor segmentation maps over time. The model has demonstrated promising performance across a range of tasks, including the generation of high-quality synthetic MRIs with tumor masks, time-series tumor segmentations, and uncertainty estimates. Combined with the treatment-aware generated MRIs, the tumor growth predictions with uncertainty estimates can provide useful information for clinical decision-making.
</details>
<details>
<summary>摘要</summary>
Diffuse gliomas 是肿瘤性脑肿，通过脑中扩散生长。这种肿瘤的复杂的Cellular interactions和正常组织之间的互动，以及治疗所引起的变化，使肿瘤增长模型变得具有挑战性。在这篇论文中，我们提出了一种新的端到端网络，能够生成未来肿瘤的面具和真实的MRI图像。我们的方法基于最新的扩散概率模型和深度分割神经网络。我们将继序多parametric磁共振成像(MRI)和治疗信息作为conditioning输入，以引导生成扩散过程。这allow for肿瘤增长估计在任何给定时间点。我们使用了真实世界后期手术 longitudinal MRI数据，其中肿瘤增长轨迹表示为肿瘤分割地图的时间序列。模型在多种任务上表现出色，包括生成高质量的Synthetic MRI面具、时间序列肿瘤分割、和不确定性估计。与治疗意识的生成MRI图像结合，肿瘤增长预测与不确定性估计可以为临床决策提供有用信息。
</details></li>
</ul>
<hr>
<h2 id="Two-Stage-Hybrid-Supervision-Framework-for-Fast-Low-resource-and-Accurate-Organ-and-Pan-cancer-Segmentation-in-Abdomen-CT"><a href="#Two-Stage-Hybrid-Supervision-Framework-for-Fast-Low-resource-and-Accurate-Organ-and-Pan-cancer-Segmentation-in-Abdomen-CT" class="headerlink" title="Two-Stage Hybrid Supervision Framework for Fast, Low-resource, and Accurate Organ and Pan-cancer Segmentation in Abdomen CT"></a>Two-Stage Hybrid Supervision Framework for Fast, Low-resource, and Accurate Organ and Pan-cancer Segmentation in Abdomen CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05405">http://arxiv.org/abs/2309.05405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Liu, Tong Tian, Weijin Xu, Lemeng Wang, Haoyuan Li, Huihua Yang</li>
<li>for: 这个研究旨在提高腹部器官和肿瘤分类的精度，以应用于评估器官量、手术规划和病变诊断。</li>
<li>methods: 这个方法基于自带训练和平均教师的混合模型，使用半标示和无标示数据进行分类。它还 introduce a two-stage segmentation pipeline和整个质量量-based input strategy来最大化分类精度。</li>
<li>results: 在FLARE2023的验证集上，这个方法实现了佳的分类性能，同时具有快速和低资源模型测试的能力。它的平均DSC分数为89.79%和45.55%，而执行时间和GPU内存使用率分别为11.25秒和9627.82MB。<details>
<summary>Abstract</summary>
Abdominal organ and tumour segmentation has many important clinical applications, such as organ quantification, surgical planning, and disease diagnosis. However, manual assessment is inherently subjective with considerable inter- and intra-expert variability. In the paper, we propose a hybrid supervised framework, StMt, that integrates self-training and mean teacher for the segmentation of abdominal organs and tumors using partially labeled and unlabeled data. We introduce a two-stage segmentation pipeline and whole-volume-based input strategy to maximize segmentation accuracy while meeting the requirements of inference time and GPU memory usage. Experiments on the validation set of FLARE2023 demonstrate that our method achieves excellent segmentation performance as well as fast and low-resource model inference. Our method achieved an average DSC score of 89.79\% and 45.55 \% for the organs and lesions on the validation set and the average running time and area under GPU memory-time cure are 11.25s and 9627.82MB, respectively.
</details>
<details>
<summary>摘要</summary>
腹部器官和肿瘤分割有很多重要的临床应用，如器官量化、手术规划和疾病诊断。然而，手动评估是内在含糊不清，存在较大的内外专家变化。在论文中，我们提议一种混合监督方案，StMt，该方案将自我教学和平均老师约束用于腹部器官和肿瘤分割，使用部分标注和无标注数据。我们提出了两stage分割管道和整体量化输入策略，以便最大化分割准确性，同时满足推理时间和GPU内存使用的要求。在FLARE2023验证集上进行了实验，我们的方法实现了优秀的分割性能，同时具有快速和低资源模型推理。我们的方法在验证集上获得了89.79%的DSC分数和45.55%的lesion DSC分数，平均执行时间为11.25秒，GPU内存使用率为9627.82MB。
</details></li>
</ul>
<hr>
<h2 id="Robust-Single-Rotation-Averaging-Revisited"><a href="#Robust-Single-Rotation-Averaging-Revisited" class="headerlink" title="Robust Single Rotation Averaging Revisited"></a>Robust Single Rotation Averaging Revisited</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05388">http://arxiv.org/abs/2309.05388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seong Hun Lee, Javier Civera</li>
<li>for: robust single rotation averaging to handle extremely large fractions of outliers</li>
<li>methods: minimize total truncated least unsquared deviations (TLUD) cost of geodesic distances, consisting of three steps: consider each input rotation as a potential initial solution, obtain the inlier set using the initial solution, and iteratively compute the geodesic $L_1$-mean of the inliers using the Weiszfeld algorithm on $SO(3)$</li>
<li>results: outperform the current state of the art in robustness against up to 99% outliers given a sufficient number of accurate inliers<details>
<summary>Abstract</summary>
In this work, we propose a novel method for robust single rotation averaging that can efficiently handle an extremely large fraction of outliers. Our approach is to minimize the total truncated least unsquared deviations (TLUD) cost of geodesic distances. The proposed algorithm consists of three steps: First, we consider each input rotation as a potential initial solution and choose the one that yields the least sum of truncated chordal deviations. Next, we obtain the inlier set using the initial solution and compute its chordal $L_2$-mean. Finally, starting from this estimate, we iteratively compute the geodesic $L_1$-mean of the inliers using the Weiszfeld algorithm on $SO(3)$. An extensive evaluation shows that our method is robust against up to 99% outliers given a sufficient number of accurate inliers, outperforming the current state of the art.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们提出了一种新的稳定单旋转平均方法，可以高效处理极高比例的异常值。我们的方法是将总 truncated least unsquared deviations（TLUD）成本最小化。我们的算法包括三步：首先，我们考虑每个输入旋转作为可能的初始解，选择它们的总和最小的 truncated chordal deviations 成本。接下来，我们使用初始解来获取准确的集合，并计算其圆柱 $L_2$-平均。最后，我们从这个估计开始，iteratively 使用 Weiszfeld 算法在 $SO(3)$ 上计算 geodesic $L_1$-平均。我们的方法可以快速处理大量数据，并且对于具有足够多准确准确的数据，可以快速地减少异常值的影响。
</details></li>
</ul>
<hr>
<h2 id="Collective-PV-RCNN-A-Novel-Fusion-Technique-using-Collective-Detections-for-Enhanced-Local-LiDAR-Based-Perception"><a href="#Collective-PV-RCNN-A-Novel-Fusion-Technique-using-Collective-Detections-for-Enhanced-Local-LiDAR-Based-Perception" class="headerlink" title="Collective PV-RCNN: A Novel Fusion Technique using Collective Detections for Enhanced Local LiDAR-Based Perception"></a>Collective PV-RCNN: A Novel Fusion Technique using Collective Detections for Enhanced Local LiDAR-Based Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05380">http://arxiv.org/abs/2309.05380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sven Teufel, Jörg Gamerdinger, Georg Volk, Oliver Bringmann</li>
<li>for: 提高自动驾驶车辆的环境感知能力，以便安全运行。</li>
<li>methods: 利用集成感知（CP）技术，让车辆之间交换信息以减少遮挡、探测范围有限和环境影响等问题。</li>
<li>results: 提出了一种新的拟合方法，可以将共同探测结果融合到本地LiDAR探测管道中，以提高自动驾驶车辆的环境感知能力。<details>
<summary>Abstract</summary>
Comprehensive perception of the environment is crucial for the safe operation of autonomous vehicles. However, the perception capabilities of autonomous vehicles are limited due to occlusions, limited sensor ranges, or environmental influences. Collective Perception (CP) aims to mitigate these problems by enabling the exchange of information between vehicles. A major challenge in CP is the fusion of the exchanged information. Due to the enormous bandwidth requirement of early fusion approaches and the interchangeability issues of intermediate fusion approaches, only the late fusion of shared detections is practical. Current late fusion approaches neglect valuable information for local detection, this is why we propose a novel fusion method to fuse the detections of cooperative vehicles within the local LiDAR-based detection pipeline. Therefore, we present Collective PV-RCNN (CPV-RCNN), which extends the PV-RCNN++ framework to fuse collective detections. Code is available at https://github.com/ekut-es
</details>
<details>
<summary>摘要</summary>
全面的环境感知是自动驾驶车辆安全运行的关键。然而，自动驾驶车辆的感知能力受到遮挡、感器范围有限以及环境因素的限制。集成感知（CP）试图解决这些问题，通过车辆之间的信息交换来提高感知范围和精度。但是，CP中的信息融合带来挑战，特别是在早期的融合方法需要巨大的带宽，而中间融合方法则存在交换信息的问题。为此，我们提出了一种新的融合方法，将在本地LiDAR基本检测管道中融合协作车辆的检测结果。因此，我们提出了集成PV-RCNN（CPV-RCNN），它是基于PV-RCNN++框架，用于融合集成检测结果。代码可以在GitHub上找到：https://github.com/ekut-es。
</details></li>
</ul>
<hr>
<h2 id="CNN-or-ViT-Revisiting-Vision-Transformers-Through-the-Lens-of-Convolution"><a href="#CNN-or-ViT-Revisiting-Vision-Transformers-Through-the-Lens-of-Convolution" class="headerlink" title="CNN or ViT? Revisiting Vision Transformers Through the Lens of Convolution"></a>CNN or ViT? Revisiting Vision Transformers Through the Lens of Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05375">http://arxiv.org/abs/2309.05375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghao Li, Chaoning Zhang</li>
<li>for: 提高ViT在小数据集上的表现（没有预训练）</li>
<li>methods: 应用权重掩码（Gaussian Mixture Mask，GMM）改进ViT的本地模型化</li>
<li>results: 在多个小数据集上实现了无需预训练的ViT表现提高（几乎无额外参数或计算成本增加）<details>
<summary>Abstract</summary>
The success of Vision Transformer (ViT) has been widely reported on a wide range of image recognition tasks. The merit of ViT over CNN has been largely attributed to large training datasets or auxiliary pre-training. Without pre-training, the performance of ViT on small datasets is limited because the global self-attention has limited capacity in local modeling. Towards boosting ViT on small datasets without pre-training, this work improves its local modeling by applying a weight mask on the original self-attention matrix. A straightforward way to locally adapt the self-attention matrix can be realized by an element-wise learnable weight mask (ELM), for which our preliminary results show promising results. However, the element-wise simple learnable weight mask not only induces a non-trivial additional parameter overhead but also increases the optimization complexity. To this end, this work proposes a novel Gaussian mixture mask (GMM) in which one mask only has two learnable parameters and it can be conveniently used in any ViT variants whose attention mechanism allows the use of masks. Experimental results on multiple small datasets demonstrate that the effectiveness of our proposed Gaussian mask for boosting ViTs for free (almost zero additional parameter or computation cost). Our code will be publicly available at \href{https://github.com/CatworldLee/Gaussian-Mixture-Mask-Attention}{https://github.com/CatworldLee/Gaussian-Mixture-Mask-Attention}.
</details>
<details>
<summary>摘要</summary>
异形卷积（ViT）的成功在各种图像识别任务上广泛报道。异形卷积比 traditional CNN 有更多的优势，主要归功于大规模的训练数据或auxiliary预训练。然而，在小数据集上不进行预训练时，异形卷积的性能有限，因为全球自注意的能力在本地模型中有限。为了提高异形卷积在小数据集上的性能，这种工作改进了异形卷积的本地模型，通过应用Weight Mask在原始的自注意矩阵上。一种直观的方式是使用元素可学习的权重Mask（ELM），我们的初步结果表明这种方法有承诺的结果。然而，元素可学习的简单权重Mask不仅增加了非常小的额外参数负担，还增加了优化复杂度。为了解决这个问题，这种工作提议一种新的 Gaussian Mixture Mask（GMM），它只有两个可学习参数，可以方便地在任何异形卷积变种中使用。我们的实验结果表明，我们的提议的 Gaussian 面积可以免除额外参数和计算成本，提高异形卷积的性能。我们的代码将在 GitHub 上公开，可以在 \href{https://github.com/CatworldLee/Gaussian-Mixture-Mask-Attention}{https://github.com/CatworldLee/Gaussian-Mixture-Mask-Attention} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Geometric-Representations-of-Objects-via-Interaction"><a href="#Learning-Geometric-Representations-of-Objects-via-Interaction" class="headerlink" title="Learning Geometric Representations of Objects via Interaction"></a>Learning Geometric Representations of Objects via Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05346">http://arxiv.org/abs/2309.05346</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reichlin/geomrepobj">https://github.com/reichlin/geomrepobj</a></li>
<li>paper_authors: Alfredo Reichlin, Giovanni Luca Marchetti, Hang Yin, Anastasiia Varava, Danica Kragic</li>
<li>for: 本 paper 的目的是学习从Scene中学习代理人和外部对象的表示。</li>
<li>methods: 该 frameworks 使用代理人的行为作为唯一的超visão，假设对象由未知动力移动。</li>
<li>results: 该 paper 提供了一种理论基础和实验证明，证明理想学习者可以准确地提取代理人和对象的位置，并且在下游任务中使用 reinforcement learning 进行有效的解决。<details>
<summary>Abstract</summary>
We address the problem of learning representations from observations of a scene involving an agent and an external object the agent interacts with. To this end, we propose a representation learning framework extracting the location in physical space of both the agent and the object from unstructured observations of arbitrary nature. Our framework relies on the actions performed by the agent as the only source of supervision, while assuming that the object is displaced by the agent via unknown dynamics. We provide a theoretical foundation and formally prove that an ideal learner is guaranteed to infer an isometric representation, disentangling the agent from the object and correctly extracting their locations. We evaluate empirically our framework on a variety of scenarios, showing that it outperforms vision-based approaches such as a state-of-the-art keypoint extractor. We moreover demonstrate how the extracted representations enable the agent to solve downstream tasks via reinforcement learning in an efficient manner.
</details>
<details>
<summary>摘要</summary>
我们处理了对一个场景中的代理人和外部物体之间的学习表现的问题。为此，我们提出了一个表现学习框架，从无结构的观察中提取代理人和物体的物理空间位置。我们的框架仅受代理人的动作作为指导，并假设物体是由代理人驱动的隐藏 Dynamics。我们提供了理论基础，正式证明了理想学习者可以从无结构观察中推导出不对称的表现，将代理人和物体分离开来，并正确地提取他们的位置。我们进行了实验评估，证明了我们的框架在多种情况下表现较好，并且可以通过循环学习来解决下游任务。
</details></li>
</ul>
<hr>
<h2 id="PAg-NeRF-Towards-fast-and-efficient-end-to-end-panoptic-3D-representations-for-agricultural-robotics"><a href="#PAg-NeRF-Towards-fast-and-efficient-end-to-end-panoptic-3D-representations-for-agricultural-robotics" class="headerlink" title="PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D representations for agricultural robotics"></a>PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D representations for agricultural robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05339">http://arxiv.org/abs/2309.05339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claus Smitt, Michael Halstead, Patrick Zimmer, Thomas Läbe, Esra Guclu, Cyrill Stachniss, Chris McCool</li>
<li>for: 实现园林 robots 监控和干预任务中的高精度Scene理解</li>
<li>methods: 使用NeRF技术建立3D测量和照片实际化描述</li>
<li>results: 提高 peak signal to noise ratio 和 panoptic quality，并且可以适应不精度的机器人位置资料<details>
<summary>Abstract</summary>
Precise scene understanding is key for most robot monitoring and intervention tasks in agriculture. In this work we present PAg-NeRF which is a novel NeRF-based system that enables 3D panoptic scene understanding. Our representation is trained using an image sequence with noisy robot odometry poses and automatic panoptic predictions with inconsistent IDs between frames. Despite this noisy input, our system is able to output scene geometry, photo-realistic renders and 3D consistent panoptic representations with consistent instance IDs. We evaluate this novel system in a very challenging horticultural scenario and in doing so demonstrate an end-to-end trainable system that can make use of noisy robot poses rather than precise poses that have to be pre-calculated. Compared to a baseline approach the peak signal to noise ratio is improved from 21.34dB to 23.37dB while the panoptic quality improves from 56.65% to 70.08%. Furthermore, our approach is faster and can be tuned to improve inference time by more than a factor of 2 while being memory efficient with approximately 12 times fewer parameters.
</details>
<details>
<summary>摘要</summary>
precisions scene understanding 是 agriculture robot monitoring 和 intervening 任务中的关键。在这项工作中，我们介绍了一种基于NeRF的新系统PAg-NeRF，它可以实现3D权威场景理解。我们的表示使用了含有噪声的机器人姿态pose的图像序列，以及自动生成的�anoptic预测结果，其中每帧ID不匹配。尽管输入含噪声，我们的系统仍能输出场景几何学、真实图像和3D一致的�anoptic表示，并且实现了一致的实例ID。我们在一个非常具有挑战性的植物种植场景中评估了这种新系统，并在这之中展示了一个可以使用噪声机器人姿态而不需要先计算精确姿态的端到端可训练系统。相比基线方法，我们的方法可以提高峰峰信号听频比为21.34dB到23.37dB，并提高�anoptic质量从56.65%到70.08%。此外，我们的方法更快，可以通过更 чем2倍的速度调整执行时间，同时具有较少的参数。
</details></li>
</ul>
<hr>
<h2 id="MultIOD-Rehearsal-free-Multihead-Incremental-Object-Detector"><a href="#MultIOD-Rehearsal-free-Multihead-Incremental-Object-Detector" class="headerlink" title="MultIOD: Rehearsal-free Multihead Incremental Object Detector"></a>MultIOD: Rehearsal-free Multihead Incremental Object Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05334">http://arxiv.org/abs/2309.05334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eden Belouadah, Arnaud Dapogny, Kevin Bailly</li>
<li>for: 这篇论文的目的是提出一个基于 CenterNet 的分类增量学习检测器，以解决现有的分类增量学习检测器具有过时忘记问题。</li>
<li>methods: 我们提出了一个多头特征pyramid和多头检测架构，使得有效地分类不同的分类表现，并通过分类初始学习和增量学习之间的传播学习来应对过时忘记问题。</li>
<li>results: 我们在两个 Pascal VOC 数据集上进行评估，结果显示我们的方法无需使用复杂的调整和练习，仍能超越许多现有的州Of-The-Art 方法。<details>
<summary>Abstract</summary>
Class-Incremental learning (CIL) is the ability of artificial agents to accommodate new classes as they appear in a stream. It is particularly interesting in evolving environments where agents have limited access to memory and computational resources. The main challenge of class-incremental learning is catastrophic forgetting, the inability of neural networks to retain past knowledge when learning a new one. Unfortunately, most existing class-incremental object detectors are applied to two-stage algorithms such as Faster-RCNN and rely on rehearsal memory to retain past knowledge. We believe that the current benchmarks are not realistic, and more effort should be dedicated to anchor-free and rehearsal-free object detection. In this context, we propose MultIOD, a class-incremental object detector based on CenterNet. Our main contributions are: (1) we propose a multihead feature pyramid and multihead detection architecture to efficiently separate class representations, (2) we employ transfer learning between classes learned initially and those learned incrementally to tackle catastrophic forgetting, and (3) we use a class-wise non-max-suppression as a post-processing technique to remove redundant boxes. Without bells and whistles, our method outperforms a range of state-of-the-art methods on two Pascal VOC datasets.
</details>
<details>
<summary>摘要</summary>
《级间学习（CIL）是人工智能代理人的承受新类出现在流中的能力。在发展环境中，代理人具有有限的内存和计算资源。主要挑战是迁移学习，即神经网络忘记过去知识 WHEN learning new one。现有的大多数级间学习对象检测器采用了两stage算法如Faster-RCNN，并且 rely on rehearsal memory来保持过去知识。我们认为现有的标准准则不实际，更应该投入更多的努力于 anchor-free和 rehearsal-free 对象检测。在这个上下文中，我们提出了 MultIOD，基于 CenterNet 的级间学习对象检测器。我们的主要贡献是：1. 我们提出了多头特征层和多头检测架构，以有效地分离类表示。2. 我们使用类初学习和级间学习之间的转移学习来抗衡迁移学习问题。3. 我们使用类别非最大抑制作为后处理技术，以去除重复的框。 ohne 钻石和精雕，我们的方法在 Pascal VOC 两个数据集上超越了一些状态的先进方法。》
</details></li>
</ul>
<hr>
<h2 id="Diff-Privacy-Diffusion-based-Face-Privacy-Protection"><a href="#Diff-Privacy-Diffusion-based-Face-Privacy-Protection" class="headerlink" title="Diff-Privacy: Diffusion-based Face Privacy Protection"></a>Diff-Privacy: Diffusion-based Face Privacy Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05330">http://arxiv.org/abs/2309.05330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao He, Mingrui Zhu, Dongxin Chen, Nannan Wang, Xinbo Gao</li>
<li>for: 保护人脸隐私，防止人工智能技术进行个人数据收集和违用。</li>
<li>methods: 基于扩散模型，提出了一种新的面部隐私保护方法，即Diff-Privacy。该方法通过训练多尺度图像反转模块（MSI）来获得原始图像的SDM格式条件嵌入。然后，根据条件嵌入，设计对应的嵌入调度策略，在杂化 proces中构建不同的能量函数，以实现人脸隐私和视觉特征隐私。</li>
<li>results: 对比于传统方法，提出的Diff-Privacy方法能够更好地保护人脸隐私，并且可以同时实现人脸隐私和视觉特征隐私。经验表明，Diff-Privacy方法能够减少人脸隐私攻击的风险。<details>
<summary>Abstract</summary>
Privacy protection has become a top priority as the proliferation of AI techniques has led to widespread collection and misuse of personal data. Anonymization and visual identity information hiding are two important facial privacy protection tasks that aim to remove identification characteristics from facial images at the human perception level. However, they have a significant difference in that the former aims to prevent the machine from recognizing correctly, while the latter needs to ensure the accuracy of machine recognition. Therefore, it is difficult to train a model to complete these two tasks simultaneously. In this paper, we unify the task of anonymization and visual identity information hiding and propose a novel face privacy protection method based on diffusion models, dubbed Diff-Privacy. Specifically, we train our proposed multi-scale image inversion module (MSI) to obtain a set of SDM format conditional embeddings of the original image. Based on the conditional embeddings, we design corresponding embedding scheduling strategies and construct different energy functions during the denoising process to achieve anonymization and visual identity information hiding. Extensive experiments have been conducted to validate the effectiveness of our proposed framework in protecting facial privacy.
</details>
<details>
<summary>摘要</summary>
隐私保护已成为人工智能技术普及后的首要任务，由于广泛收集和不当使用个人数据，隐私保护变得更加重要。隐身化和视觉特征隐藏是两项重要的面部隐私保护任务，它们的目标是在人类视觉水平上从面部图像中除去识别特征。然而，这两项任务之间存在重要的区别，隐身化旨在防止机器正确地识别，而视觉特征隐藏则需要保证机器识别的准确性。因此，同时完成这两项任务是很困难的。在这篇论文中，我们将隐身化和视觉特征隐藏的任务综合起来，并提出了一种基于扩散模型的面部隐私保护方法，称为Diff-Privacy。具体来说，我们将训练我们提出的多尺度图像反转模块（MSI），以获得原始图像的SDM格式条件嵌入。根据条件嵌入，我们设计了对应的嵌入调度策略，并在减噪过程中构建不同的能量函数，以实现隐身化和视觉特征隐藏。我们对方法进行了广泛的实验，以验证其在保护面部隐私方面的效果。
</details></li>
</ul>
<hr>
<h2 id="DeCUR-decoupling-common-unique-representations-for-multimodal-self-supervision"><a href="#DeCUR-decoupling-common-unique-representations-for-multimodal-self-supervision" class="headerlink" title="DeCUR: decoupling common &amp; unique representations for multimodal self-supervision"></a>DeCUR: decoupling common &amp; unique representations for multimodal self-supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05300">http://arxiv.org/abs/2309.05300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhu-xlab/decur">https://github.com/zhu-xlab/decur</a></li>
<li>paper_authors: Yi Wang, Conrad M Albrecht, Nassim Ait Ali Braham, Chenying Liu, Zhitong Xiong, Xiao Xiang Zhu</li>
<li>for: 本文旨在提出一种简单 yet effective的多modal自编学习方法，以捕捉不同模式之间的补做关系。</li>
<li>methods: 该方法通过分解共同和唯一表示来结合不同模式之间的补做信息。</li>
<li>results: 在radar-optical、RGB-elevation和RGB-depth等常见的多modal场景中，该方法能够准确地进行景物分类和 semantic segmentation下游任务，并且可以 straightforward地提高state-of-the-art超vised多modal方法的性能。<details>
<summary>Abstract</summary>
The increasing availability of multi-sensor data sparks interest in multimodal self-supervised learning. However, most existing approaches learn only common representations across modalities while ignoring intra-modal training and modality-unique representations. We propose Decoupling Common and Unique Representations (DeCUR), a simple yet effective method for multimodal self-supervised learning. By distinguishing inter- and intra-modal embeddings, DeCUR is trained to integrate complementary information across different modalities. We evaluate DeCUR in three common multimodal scenarios (radar-optical, RGB-elevation, and RGB-depth), and demonstrate its consistent benefits on scene classification and semantic segmentation downstream tasks. Notably, we get straightforward improvements by transferring our pretrained backbones to state-of-the-art supervised multimodal methods without any hyperparameter tuning. Furthermore, we conduct a comprehensive explainability analysis to shed light on the interpretation of common and unique features in our multimodal approach. Codes are available at \url{https://github.com/zhu-xlab/DeCUR}.
</details>
<details>
<summary>摘要</summary>
随着多感器数据的更加普遍，人们对多模态自我supervised学习表示越来越多的兴趣。然而，大多数现有的方法只学习多modalities中的共同表示，而忽略了INTRA-modal训练和特有表示。我们提议一种简单 yet effective的方法：异步共同和独特表示分离（DeCUR），用于多modal self-supervised learning。通过分辨Inter-和INTRA-modal嵌入，DeCUR可以整合不同modalities中的补充信息。我们在radar-optical、RGB-elevation和RGB-depth三个常见的多modal场景中进行评估，并证明DeCUR在Scene classification和semantic segmentation下渠道任务中具有一致的好处。特别是，我们可以无需任何超参数调整直接将我们预训练的backbone传承到状态 искусственный极点的supervised multimodal方法中，获得直接的改进。此外，我们进行了广泛的解释性分析，以便更好地理解我们的多modal方法中的共同和特有特征的解释。codes可以在\url{https://github.com/zhu-xlab/DeCUR}中找到。
</details></li>
</ul>
<hr>
<h2 id="Task-driven-Compression-for-Collision-Encoding-based-on-Depth-Images"><a href="#Task-driven-Compression-for-Collision-Encoding-based-on-Depth-Images" class="headerlink" title="Task-driven Compression for Collision Encoding based on Depth Images"></a>Task-driven Compression for Collision Encoding based on Depth Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05289">http://arxiv.org/abs/2309.05289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihir Kulkarni, Kostas Alexis</li>
<li>for: 这个研究旨在提出一种基于学习的方法，用于对深度图像进行攻击性压缩和编码，以便在机器人系统中进行冲击预测。</li>
<li>methods: 这个研究提出了一种新的三维图像处理方法，考虑了机器人的大小，以便适当地膨胀对于机器人的障碍物，并从而获取机器人可以在摄像头视场中无冲击地移动的距离。这些深度图像和冲击图像的对组是用于训练一个基于Variational Autoencoders架构的神经网络，以将原始深度图像的信息压缩和转换为具有冲击信息的潜在表示。</li>
<li>results: 比较这个提案的任务驱动编码方法与传统任务无关的方法， demonstrate superior performance for the task of collision image prediction from extremely low-dimensional latent spaces。一系列的比较研究显示，提案的方法可以对复杂的场景中的细小障碍物进行更好的编码，具有4050:1的压缩比。<details>
<summary>Abstract</summary>
This paper contributes a novel learning-based method for aggressive task-driven compression of depth images and their encoding as images tailored to collision prediction for robotic systems. A novel 3D image processing methodology is proposed that accounts for the robot's size in order to appropriately "inflate" the obstacles represented in the depth image and thus obtain the distance that can be traversed by the robot in a collision-free manner along any given ray within the camera frustum. Such depth-and-collision image pairs are used to train a neural network that follows the architecture of Variational Autoencoders to compress-and-transform the information in the original depth image to derive a latent representation that encodes the collision information for the given depth image. We compare our proposed task-driven encoding method with classical task-agnostic methods and demonstrate superior performance for the task of collision image prediction from extremely low-dimensional latent spaces. A set of comparative studies show that the proposed approach is capable of encoding depth image-and-collision image tuples from complex scenes with thin obstacles at long distances better than the classical methods at compression ratios as high as 4050:1.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Class-Incremental-Grouping-Network-for-Continual-Audio-Visual-Learning"><a href="#Class-Incremental-Grouping-Network-for-Continual-Audio-Visual-Learning" class="headerlink" title="Class-Incremental Grouping Network for Continual Audio-Visual Learning"></a>Class-Incremental Grouping Network for Continual Audio-Visual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05281">http://arxiv.org/abs/2309.05281</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stonemo/cign">https://github.com/stonemo/cign</a></li>
<li>paper_authors: Shentong Mo, Weiguo Pian, Yapeng Tian</li>
<li>for: 这篇论文的目的是提出一个 novel 的类别增量学习网络（CIGN），以便在类别增量学习中进行 continual audio-visual 学习。</li>
<li>methods: 这篇论文使用了一个具有 learnable 的音频视频类别标识和音频视频分组的类别增量学习网络（CIGN），并且使用了类别标识激发和类别分组来防止忘记。</li>
<li>results: 根据实验结果显示，CIGN 可以在 VGGSound-Instruments、VGGSound-100 和 VGG-Sound Sources 评量上实现类别增量学习的最佳性能。<details>
<summary>Abstract</summary>
Continual learning is a challenging problem in which models need to be trained on non-stationary data across sequential tasks for class-incremental learning. While previous methods have focused on using either regularization or rehearsal-based frameworks to alleviate catastrophic forgetting in image classification, they are limited to a single modality and cannot learn compact class-aware cross-modal representations for continual audio-visual learning. To address this gap, we propose a novel class-incremental grouping network (CIGN) that can learn category-wise semantic features to achieve continual audio-visual learning. Our CIGN leverages learnable audio-visual class tokens and audio-visual grouping to continually aggregate class-aware features. Additionally, it utilizes class tokens distillation and continual grouping to prevent forgetting parameters learned from previous tasks, thereby improving the model's ability to capture discriminative audio-visual categories. We conduct extensive experiments on VGGSound-Instruments, VGGSound-100, and VGG-Sound Sources benchmarks. Our experimental results demonstrate that the CIGN achieves state-of-the-art audio-visual class-incremental learning performance. Code is available at https://github.com/stoneMo/CIGN.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified ChineseContinual learning 是一个具有挑战性的问题， models 需要在非站点数据上进行 sequential 任务中的 class-incremental learning。 在过去的方法中，使用了 either regularization 或 rehearsal-based 框架，以减轻 catastrophic forgetting 在 image classification 中，但这些方法仅适用于单一模式，无法学习 cross-modal 的 class-aware 表示。为了解决这个差异，我们提出了一个 novel class-incremental grouping network (CIGN)，可以学习 category-wise semantic features，以实现 continual audio-visual learning。我们的 CIGN 利用可学习的 audio-visual 类别标志和 audio-visual 分组，以 continually 积累 class-aware 特征。此外，它还利用类别标志激发和 continual 分组，以防止遗传 learned 的前一个任务中的知识。我们对 VGGSound-Instruments、VGGSound-100 和 VGG-Sound Sources 标准库进行了广泛的实验。我们的实验结果显示，CIGN 在 audio-visual class-incremental learning 中 achieved state-of-the-art 性能。代码可以在 https://github.com/stoneMo/CIGN 获取。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Class-Agnostic-Object-Counting"><a href="#Interactive-Class-Agnostic-Object-Counting" class="headerlink" title="Interactive Class-Agnostic Object Counting"></a>Interactive Class-Agnostic Object Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05277">http://arxiv.org/abs/2309.05277</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Yifehuang97/ICACount">https://github.com/Yifehuang97/ICACount</a></li>
<li>paper_authors: Yifeng Huang, Viresh Ranjan, Minh Hoai</li>
<li>for: 这个论文的目的是提出一种便捷、可靠的人工智能计数方法，允许用户在互动的方式下提供反馈，以提高计数的准确性。</li>
<li>methods: 该方法包括两个主要组件：一个易于使用的视觉化器来收集反馈，以及一种高效的机制来利用反馈进行改进。在每次迭代中，我们生成一张扩散图来显示当前预测结果，并将其分成不重叠的区域，每个区域可以轻松地计算出对象的数量。用户可以通过选择显示错误的区域，并指定该区域内对象的数量范围，为计数结果提供反馈。</li>
<li>results: 我们的方法可以在两个普遍性类型的物体计数 benchmark 上减少多种现有 visual counter 的平均绝对误差，比如FSCD-LVIS 和 FSC-147，减少约30%到40%。<details>
<summary>Abstract</summary>
We propose a novel framework for interactive class-agnostic object counting, where a human user can interactively provide feedback to improve the accuracy of a counter. Our framework consists of two main components: a user-friendly visualizer to gather feedback and an efficient mechanism to incorporate it. In each iteration, we produce a density map to show the current prediction result, and we segment it into non-overlapping regions with an easily verifiable number of objects. The user can provide feedback by selecting a region with obvious counting errors and specifying the range for the estimated number of objects within it. To improve the counting result, we develop a novel adaptation loss to force the visual counter to output the predicted count within the user-specified range. For effective and efficient adaptation, we propose a refinement module that can be used with any density-based visual counter, and only the parameters in the refinement module will be updated during adaptation. Our experiments on two challenging class-agnostic object counting benchmarks, FSCD-LVIS and FSC-147, show that our method can reduce the mean absolute error of multiple state-of-the-art visual counters by roughly 30% to 40% with minimal user input. Our project can be found at https://yifehuang97.github.io/ICACountProjectPage/.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的框架，用于互动性的类共享对象计数，其中人类用户可以互动地提供反馈来提高计数的准确性。我们的框架包括两个主要组成部分：一个易于使用的视觉化器来收集反馈，以及一种高效的机制来整合它。在每次迭代中，我们生成一张扩散图来显示当前预测结果，并将其分成不重叠的区域，每个区域可以轻松地验证其中的对象数量。用户可以通过选择具有明显计数错误的区域，并指定该区域内对象数量的范围，来提供反馈。为了改进计数结果，我们开发了一种新的适应损失，使得视觉计数器输出预测的计数在用户指定的范围内。为了有效和高效地适应，我们提议一种修充模块，可以与任何扩散基本的视觉计数器结合使用，只有修充模块中的参数会在适应过程中更新。我们的实验表明，我们的方法可以在两个普遍性类共享对象计数标准 benchmark 上减少多个现状顶尖的视觉计数器的平均绝对误差约30%到40%，具有最小的用户输入。您可以在 <https://yifehuang97.github.io/ICACountProjectPage/> 查看我们的项目。
</details></li>
</ul>
<hr>
<h2 id="Diving-into-Darkness-A-Dual-Modulated-Framework-for-High-Fidelity-Super-Resolution-in-Ultra-Dark-Environments"><a href="#Diving-into-Darkness-A-Dual-Modulated-Framework-for-High-Fidelity-Super-Resolution-in-Ultra-Dark-Environments" class="headerlink" title="Diving into Darkness: A Dual-Modulated Framework for High-Fidelity Super-Resolution in Ultra-Dark Environments"></a>Diving into Darkness: A Dual-Modulated Framework for High-Fidelity Super-Resolution in Ultra-Dark Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05267">http://arxiv.org/abs/2309.05267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Gao, Ziyu Yue, Yaohua Liu, Sihan Xie, Xin Fan, Risheng Liu</li>
<li>for:  This paper is written for the problem of super-resolution in ultra-dark environments, which is a challenging and practical problem that has received little attention.</li>
<li>methods: The paper proposes a specialized dual-modulated learning framework that includes a self-regularized luminance constraint and Illuminance-Semantic Dual Modulation (ISDM) components to enhance feature-level preservation of illumination and color details. Additionally, the paper designs a Resolution-Sensitive Merging Up-sampler (RSMU) module to mitigate the presence of artifacts and halos.</li>
<li>results: The paper shows that its approach outperforms state-of-the-art methods in terms of PSNR, LPIPS, and RMSE score, with a notable improvement of 5% in PSNR and 43% in LPIPS. The paper also demonstrates the generalization of its approach across different darkness levels, with a 19-fold increase in the RMSE score.<details>
<summary>Abstract</summary>
Super-resolution tasks oriented to images captured in ultra-dark environments is a practical yet challenging problem that has received little attention. Due to uneven illumination and low signal-to-noise ratio in dark environments, a multitude of problems such as lack of detail and color distortion may be magnified in the super-resolution process compared to normal-lighting environments. Consequently, conventional low-light enhancement or super-resolution methods, whether applied individually or in a cascaded manner for such problem, often encounter limitations in recovering luminance, color fidelity, and intricate details. To conquer these issues, this paper proposes a specialized dual-modulated learning framework that, for the first time, attempts to deeply dissect the nature of the low-light super-resolution task. Leveraging natural image color characteristics, we introduce a self-regularized luminance constraint as a prior for addressing uneven lighting. Expanding on this, we develop Illuminance-Semantic Dual Modulation (ISDM) components to enhance feature-level preservation of illumination and color details. Besides, instead of deploying naive up-sampling strategies, we design the Resolution-Sensitive Merging Up-sampler (RSMU) module that brings together different sampling modalities as substrates, effectively mitigating the presence of artifacts and halos. Comprehensive experiments showcases the applicability and generalizability of our approach to diverse and challenging ultra-low-light conditions, outperforming state-of-the-art methods with a notable improvement (i.e., $\uparrow$5\% in PSNR, and $\uparrow$43\% in LPIPS). Especially noteworthy is the 19-fold increase in the RMSE score, underscoring our method's exceptional generalization across different darkness levels. The code will be available online upon publication of the paper.
</details>
<details>
<summary>摘要</summary>
“低光环境下的超解像任务是一个实际且挑战性的问题，它们获得了少量的注意。由于低光环境中的不均匀照明和讯号与干扰比，它们可能会导致缺乏细节和颜色扭曲。因此，传统的低光照增强或超解像方法， whether applied individually or in a cascaded manner for this problem, often encounter limitations in recovering luminance, color fidelity, and intricate details. To conquer these issues, this paper proposes a specialized dual-modulated learning framework that, for the first time, attempts to deeply dissect the nature of the low-light super-resolution task. Leveraging natural image color characteristics, we introduce a self-regularized luminance constraint as a prior for addressing uneven lighting. Expanding on this, we develop Illuminance-Semantic Dual Modulation (ISDM) components to enhance feature-level preservation of illumination and color details. Besides, instead of deploying naive up-sampling strategies, we design the Resolution-Sensitive Merging Up-sampler (RSMU) module that brings together different sampling modalities as substrates, effectively mitigating the presence of artifacts and halos. Comprehensive experiments showcases the applicability and generalizability of our approach to diverse and challenging ultra-low-light conditions, outperforming state-of-the-art methods with a notable improvement (i.e., $\uparrow$5\% in PSNR, and $\uparrow$43\% in LPIPS). Especially noteworthy is the 19-fold increase in the RMSE score, underscoring our method's exceptional generalization across different darkness levels. The code will be available online upon publication of the paper.”
</details></li>
</ul>
<hr>
<h2 id="A-horizon-line-annotation-tool-for-streamlining-autonomous-sea-navigation-experiments"><a href="#A-horizon-line-annotation-tool-for-streamlining-autonomous-sea-navigation-experiments" class="headerlink" title="A horizon line annotation tool for streamlining autonomous sea navigation experiments"></a>A horizon line annotation tool for streamlining autonomous sea navigation experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05262">http://arxiv.org/abs/2309.05262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yassir Zardoua, Abdelhamid El Wahabi, Mohammed Boulaala, Abdelali Astito</li>
<li>For: 本研究的目的是为marine autonomous navigation任务提供更加稳定和可靠的海Horizon line检测方法。* Methods: 本研究使用了一种新的公共注释软件，用于快速和容易地注释收集的海洋图像中的海Horizon line。* Results: 本研究通过对多种海condexts进行实验 validate了一种更加Robust的海Horizon line检测方法。Here’s the breakdown of each point in English:* For: The purpose of this research is to provide more stable and reliable horizon line detection methods for marine autonomous navigation tasks.* Methods: This research uses a new public annotation software to quickly and easily annotate collected marine images with the correct position and orientation of the horizon line.* Results: This research experimentally validated a more robust horizon line detection method by collecting and annotating images of various sea conditions.<details>
<summary>Abstract</summary>
Horizon line (or sea line) detection (HLD) is a critical component in multiple marine autonomous navigation tasks, such as identifying the navigation area (i.e., the sea), obstacle detection and geo-localization, and digital video stabilization. A recent survey highlighted several weaknesses of such detectors, particularly on sea conditions lacking from the most extensive dataset currently used by HLD researchers. Experimental validation of more robust HLDs involves collecting an extensive set of these lacking sea conditions and annotating each collected image with the correct position and orientation of the horizon line. The annotation task is daunting without a proper tool. Therefore, we present the first public annotation software with tailored features to make the sea line annotation process fast and easy. The software is available at: https://drive.google.com/drive/folders/1c0ZmvYDckuQCPIWfh_70P7E1A_DWlIvF?usp=sharing
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化字符串。<</SYS>>水平线（或海洋线）检测（HLD）是多种海上自动导航任务的关键组件，包括定位水平线（即海洋）、障碍物检测和地理位置确定、数字视频稳定化等。一项最新的调查指出了HLD检测器的一些弱点，特别是在缺乏当前HLD研究者使用最广泛的数据集的情况下。为验证更加稳定的HLD，需要收集一个广泛的这些缺失的海洋条件，并对每个收集的图像注解正确的水平线位置和方向。 annotation task是无法进行的，因此我们提供了首个公共的注释软件，带有适应的特性，使水平线注释过程快速和容易。软件可以在以下链接中下载：https://drive.google.com/drive/folders/1c0ZmvYDckuQCPIWfh_70P7E1A_DWlIvF?usp=sharing
</details></li>
</ul>
<hr>
<h2 id="Gall-Bladder-Cancer-Detection-from-US-Images-with-Only-Image-Level-Labels"><a href="#Gall-Bladder-Cancer-Detection-from-US-Images-with-Only-Image-Level-Labels" class="headerlink" title="Gall Bladder Cancer Detection from US Images with Only Image Level Labels"></a>Gall Bladder Cancer Detection from US Images with Only Image Level Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05261">http://arxiv.org/abs/2309.05261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumen Basu, Ashish Papanai, Mayank Gupta, Pankaj Gupta, Chetan Arora</li>
<li>for: 这个研究旨在提高 gallbladder cancer (GBC) 的检测精度，使用仅有的 image-level label。</li>
<li>methods: 我们使用 transformer 模型，并使用 multi-instance-learning (MIL) 和自动选择 instance 来处理弱相关目标检测 (WSOD) 问题。</li>
<li>results: 我们的方法在 AP 和检测敏感性上比 SOTA transformer-based 和 CNN-based WSOD 方法更好。Note:</li>
<li>for: 是指研究的目的或目标，即这个研究是为了提高 GBC 的检测精度。</li>
<li>methods: 是指使用的方法或技术，即使用 transformer 模型和 MIL 等方法。</li>
<li>results: 是指研究所得到的结果，即比 SOTA 方法更好的检测精度。<details>
<summary>Abstract</summary>
Automated detection of Gallbladder Cancer (GBC) from Ultrasound (US) images is an important problem, which has drawn increased interest from researchers. However, most of these works use difficult-to-acquire information such as bounding box annotations or additional US videos. In this paper, we focus on GBC detection using only image-level labels. Such annotation is usually available based on the diagnostic report of a patient, and do not require additional annotation effort from the physicians. However, our analysis reveals that it is difficult to train a standard image classification model for GBC detection. This is due to the low inter-class variance (a malignant region usually occupies only a small portion of a US image), high intra-class variance (due to the US sensor capturing a 2D slice of a 3D object leading to large viewpoint variations), and low training data availability. We posit that even when we have only the image level label, still formulating the problem as object detection (with bounding box output) helps a deep neural network (DNN) model focus on the relevant region of interest. Since no bounding box annotations is available for training, we pose the problem as weakly supervised object detection (WSOD). Motivated by the recent success of transformer models in object detection, we train one such model, DETR, using multi-instance-learning (MIL) with self-supervised instance selection to suit the WSOD task. Our proposed method demonstrates an improvement of AP and detection sensitivity over the SOTA transformer-based and CNN-based WSOD methods. Project page is at https://gbc-iitd.github.io/wsod-gbc
</details>
<details>
<summary>摘要</summary>
自动检测结肠癌（GBC）从超声画像（US）图像是一个重要的问题，吸引了研究者们的关注。然而，大多数这些工作使用难以获得的信息，如矩形框注释或更多的US视频。在这篇论文中，我们专注于基于只有图像级别的标签进行GBC检测。这些标签通常可以基于病人的诊断报告获得，不需要更多的注释努力。然而，我们的分析表明，使用标准的图像分类模型进行GBC检测是困难的。这是因为肿瘤区域通常占用US图像中的only a small portion，以及US感知器捕捉的2Dslice of a 3D object leading to large viewpoint variations。我们认为，即使只有图像级别的标签，使用深度神经网络（DNN）模型仍然可以帮助模型关注相关的区域。由于没有 bounding box 注释可用于训练，我们将问题定义为弱型Supervised Object Detection（WSOD）问题。驱动于Recent success of transformer models in object detection，我们使用多例学习（MIL）和自我驱动的实例选择来训练我们的模型DETR。我们的提议方法在AP和检测敏感性方面与State-of-the-Art（SOTA）基于 transformer 和 CNN 的 WSOD 方法之上具有进步。相关页面可以在 <https://gbc-iitd.github.io/wsod-gbc> 中找到。
</details></li>
</ul>
<hr>
<h2 id="FusionFormer-A-Multi-sensory-Fusion-in-Bird’s-Eye-View-and-Temporal-Consistent-Transformer-for-3D-Objection"><a href="#FusionFormer-A-Multi-sensory-Fusion-in-Bird’s-Eye-View-and-Temporal-Consistent-Transformer-for-3D-Objection" class="headerlink" title="FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Objection"></a>FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Objection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05257">http://arxiv.org/abs/2309.05257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyong Hu, Hang Zheng, Kun Li, Jianyun Xu, Weibo Mao, Maochun Luo, Lingxuan Wang, Mingxia Chen, Kaixuan Liu, Yiru Zhao, Peihan Hao, Minzhe Liu, Kaicheng Yu</li>
<li>for: 提高3D物体检测 task 的性能</li>
<li>methods: 利用 transformers 结合多modal 特征，并添加 depth prediction 分支以提高检测性能</li>
<li>results: 在 nuScenes  dataset 上 Achieve 72.6% mAP 和 75.1% NDS，超越现有方法<details>
<summary>Abstract</summary>
Multi-sensor modal fusion has demonstrated strong advantages in 3D object detection tasks. However, existing methods that fuse multi-modal features through a simple channel concatenation require transformation features into bird's eye view space and may lose the information on Z-axis thus leads to inferior performance. To this end, we propose FusionFormer, an end-to-end multi-modal fusion framework that leverages transformers to fuse multi-modal features and obtain fused BEV features. And based on the flexible adaptability of FusionFormer to the input modality representation, we propose a depth prediction branch that can be added to the framework to improve detection performance in camera-based detection tasks. In addition, we propose a plug-and-play temporal fusion module based on transformers that can fuse historical frame BEV features for more stable and reliable detection results. We evaluate our method on the nuScenes dataset and achieve 72.6% mAP and 75.1% NDS for 3D object detection tasks, outperforming state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
多感器模式融合已经在3D物体探测任务中显示出了强大的优势。然而，现有的方法通过简单的通道拼接来融合多模态特征可能会产生Z轴信息损失，导致性能下降。为此，我们提出了FusionFormer，一个端到端多模态融合框架，利用转换器来融合多模态特征并获得融合BEV特征。此外，基于输入模式表示的灵活适应性，我们提出了一个深度预测分支，可以添加到框架中，以提高摄像头基于探测任务的检测性能。此外，我们还提出了基于转换器的历史帧BEV特征融合模块，可以将历史帧特征融合以获得更稳定和可靠的检测结果。我们在nuScenes dataset上评估了我们的方法，并实现了3D物体探测任务中的72.6% mAP和75.1% NDS，超过了当前state-of-the-art方法。
</details></li>
</ul>
<hr>
<h2 id="Towards-Better-Data-Exploitation-In-Self-Supervised-Monocular-Depth-Estimation"><a href="#Towards-Better-Data-Exploitation-In-Self-Supervised-Monocular-Depth-Estimation" class="headerlink" title="Towards Better Data Exploitation In Self-Supervised Monocular Depth Estimation"></a>Towards Better Data Exploitation In Self-Supervised Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05254">http://arxiv.org/abs/2309.05254</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sauf4896/bdedepth">https://github.com/sauf4896/bdedepth</a></li>
<li>paper_authors: Jinfeng Liu, Lingtong Kong, Jie Yang, Wei Liu</li>
<li>for: 本研究旨在提高自助学习独眼视觉系统中的深度估计能力，以增强机器人视觉系统的泛化能力。</li>
<li>methods: 本研究使用了两种数据增强技术：Resizing-Cropping和Splitting-Permuting，以充分利用训练数据的潜在能力。具体来说，我们将原始图像和生成的两个增强图像同时 feed into 训练管道，并通过自我采样来进行自采样。此外，我们还提出了细节优化的 DepthNet，其包括一个全规模分支在编码器中和一个网格解码器，以提高depth图中的细节Restoration。</li>
<li>results: 实验结果表明，我们的方法可以在KITTI标准测试集上达到状态机器人视觉系统中的最佳性能，并且我们的模型还能够在Make3D和NYUv2测试集上 Transfer learning 表现出色。<details>
<summary>Abstract</summary>
Depth estimation plays an important role in the robotic perception system. Self-supervised monocular paradigm has gained significant attention since it can free training from the reliance on depth annotations. Despite recent advancements, existing self-supervised methods still underutilize the available training data, limiting their generalization ability. In this paper, we take two data augmentation techniques, namely Resizing-Cropping and Splitting-Permuting, to fully exploit the potential of training datasets. Specifically, the original image and the generated two augmented images are fed into the training pipeline simultaneously and we leverage them to conduct self-distillation. Additionally, we introduce the detail-enhanced DepthNet with an extra full-scale branch in the encoder and a grid decoder to enhance the restoration of fine details in depth maps. Experimental results demonstrate our method can achieve state-of-the-art performance on the KITTI benchmark, with both raw ground truth and improved ground truth. Moreover, our models also show superior generalization performance when transferring to Make3D and NYUv2 datasets. Our codes are available at https://github.com/Sauf4896/BDEdepth.
</details>
<details>
<summary>摘要</summary>
depth estimation 在 robotic perception system 中扮演着重要的角色。无监督单目法固有了广泛的关注，因为它可以免除depth注释的依赖。Despite recent advancements, existing self-supervised methods still underutilize the available training data, limiting their generalization ability. In this paper, we take two data augmentation techniques, namely Resizing-Cropping and Splitting-Permuting, to fully exploit the potential of training datasets. Specifically, the original image and the generated two augmented images are fed into the training pipeline simultaneously and we leverage them to conduct self-distillation. Additionally, we introduce the detail-enhanced DepthNet with an extra full-scale branch in the encoder and a grid decoder to enhance the restoration of fine details in depth maps. Experimental results demonstrate our method can achieve state-of-the-art performance on the KITTI benchmark, with both raw ground truth and improved ground truth. Moreover, our models also show superior generalization performance when transferring to Make3D and NYUv2 datasets. Our codes are available at https://github.com/Sauf4896/BDEdepth.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Multi3DRefer-Grounding-Text-Description-to-Multiple-3D-Objects"><a href="#Multi3DRefer-Grounding-Text-Description-to-Multiple-3D-Objects" class="headerlink" title="Multi3DRefer: Grounding Text Description to Multiple 3D Objects"></a>Multi3DRefer: Grounding Text Description to Multiple 3D Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05251">http://arxiv.org/abs/2309.05251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/3dlg-hcvc/M3DRef-CLIP">https://github.com/3dlg-hcvc/M3DRef-CLIP</a></li>
<li>paper_authors: Yiming Zhang, ZeMing Gong, Angel X. Chang<br>for: 本研究旨在用自然语言描述Localize多个3D场景中的灵活数量的物体。现有的3D视觉固定任务都是基于唯一的目标物体描述，但这种约束是不自然的，因为在真实世界情况下，我们经常需要Localize多个物体。methods: 我们提出了Multi3DRefer，一种扩展ScanRefer数据集和任务，以解决这种情况。我们的数据集包含11609个 объек的11609个描述，其中每个描述可能有一个、多个或zero个目标物体。我们还引入了一种新的评价指标和优化方法，以便进一步研究多模式3D场景理解。results: 我们开发了一种新的基线方法，利用CLIP的2D特征进行对比学习，可以在线渲染出对象提案，并超越当前状态的最佳性能在ScanReferbenchmark上。<details>
<summary>Abstract</summary>
We introduce the task of localizing a flexible number of objects in real-world 3D scenes using natural language descriptions. Existing 3D visual grounding tasks focus on localizing a unique object given a text description. However, such a strict setting is unnatural as localizing potentially multiple objects is a common need in real-world scenarios and robotic tasks (e.g., visual navigation and object rearrangement). To address this setting we propose Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains 61926 descriptions of 11609 objects, where zero, single or multiple target objects are referenced by each description. We also introduce a new evaluation metric and benchmark methods from prior work to enable further investigation of multi-modal 3D scene understanding. Furthermore, we develop a better baseline leveraging 2D features from CLIP by rendering object proposals online with contrastive learning, which outperforms the state of the art on the ScanRefer benchmark.
</details>
<details>
<summary>摘要</summary>
我们介绍了在实际世界3D场景中自然语言描述中Localizing多个灵活对象的任务。现有的3D视觉定位任务都是基于固定对象的描述，但这种 Setting是不自然的，因为在实际场景中Localizing多个对象是常见的需求，例如视觉导航和对象重新排序。为解决这种设定，我们提出了Multi3DRefer，它扩展了ScanRefer数据集和任务。我们的数据集包括11609个对象的11609个描述，其中每个描述可能引用零、单个或多个目标对象。我们还引入了一个新的评价指标和优秀方法，从优秀的多模式3D场景理解研究中继承来。此外，我们开发了一个更好的基线，通过在线渲染对象提案，使用对比学习，以获得更高的性能在ScanRefer bencmark上。
</details></li>
</ul>
<hr>
<h2 id="HAT-Hybrid-Attention-Transformer-for-Image-Restoration"><a href="#HAT-Hybrid-Attention-Transformer-for-Image-Restoration" class="headerlink" title="HAT: Hybrid Attention Transformer for Image Restoration"></a>HAT: Hybrid Attention Transformer for Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05239">http://arxiv.org/abs/2309.05239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xpixelgroup/hat">https://github.com/xpixelgroup/hat</a></li>
<li>paper_authors: Xiangyu Chen, Xintao Wang, Wenlong Zhang, Xiangtao Kong, Yu Qiao, Jiantao Zhou, Chao Dong</li>
<li>for: 提高图像修复任务中Transformer网络的表现，包括图像超解和噪声除除。</li>
<li>methods: 提出了一种新的混合注意力变换器（HAT），它结合了通道注意力和窗口自注意力两种方法，以便更好地使用输入信息。此外，我们还引入了覆盖窗口相互注意模块，以增强窗口之间信息的协同。</li>
<li>results: 对比于基eline方法，HAT可以更好地恢复图像，并且可以扩展到更多的图像修复应用，如真实世界图像超解、Gaussian图像噪声除除和图像压缩 artefacts 除除。实验表明，HAT可以达到状态之前的最佳性能。<details>
<summary>Abstract</summary>
Transformer-based methods have shown impressive performance in image restoration tasks, such as image super-resolution and denoising. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for better restoration, we propose a new Hybrid Attention Transformer (HAT). It combines both channel attention and window-based self-attention schemes, thus making use of their complementary advantages. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally adopt a same-task pre-training strategy to further exploit the potential of the model for further improvement. Extensive experiments have demonstrated the effectiveness of the proposed modules. We further scale up the model to show that the performance of the SR task can be greatly improved. Besides, we extend HAT to more image restoration applications, including real-world image super-resolution, Gaussian image denoising and image compression artifacts reduction. Experiments on benchmark and real-world datasets demonstrate that our HAT achieves state-of-the-art performance both quantitatively and qualitatively. Codes and models are publicly available at https://github.com/XPixelGroup/HAT.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Transformer-based methods have shown impressive performance in image restoration tasks, such as image super-resolution and denoising. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for better restoration, we propose a new Hybrid Attention Transformer (HAT). It combines both channel attention and window-based self-attention schemes, thus making use of their complementary advantages. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally adopt a same-task pre-training strategy to further exploit the potential of the model for further improvement. Extensive experiments have demonstrated the effectiveness of the proposed modules. We further scale up the model to show that the performance of the SR task can be greatly improved. Besides, we extend HAT to more image restoration applications, including real-world image super-resolution, Gaussian image denoising and image compression artifacts reduction. Experiments on benchmark and real-world datasets demonstrate that our HAT achieves state-of-the-art performance both quantitatively and qualitatively. Codes and models are publicly available at <https://github.com/XPixelGroup/HAT>.中文简体版：Transformer基于方法在图像修复任务中表现出色，如图像超解和噪声除除。然而，我们发现这些网络只能利用输入图像的有限范围信息，通过属性分析来进行评估。这意味着Transformer的潜力还未被完全利用。为了激活更多的输入像素，提高修复效果，我们提议一种新的混合注意力Transformer（HAT）。它结合了通道注意力和窗口基于自注意力的方法，并且利用了它们的优势。此外，为了更好地聚合跨窗信息，我们引入了重叠cross-attention模块，以增强邻近窗口特征之间的互动。在训练阶段，我们还采用了同任务预训练策略，以进一步利用模型的潜力。广泛的实验证明了我们提议的模块的效iveness。此外，我们还将HAT扩展到更多的图像修复应用，包括实际世界图像超解、Gaussian图像噪声除除和图像压缩 artifacts reduction。实验表明，我们的HAT在量化和质量上都达到了领先的性能。代码和模型在<https://github.com/XPixelGroup/HAT>公共可用。
</details></li>
</ul>
<hr>
<h2 id="Angle-Range-and-Identity-Similarity-Enhanced-Gaze-and-Head-Redirection-based-on-Synthetic-data"><a href="#Angle-Range-and-Identity-Similarity-Enhanced-Gaze-and-Head-Redirection-based-on-Synthetic-data" class="headerlink" title="Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data"></a>Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05214">http://arxiv.org/abs/2309.05214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Qin, Xueting Wang</li>
<li>for: 提高全面图像中头和视线的方向准确性和图像质量</li>
<li>methods: 使用数据扩充和提高图像质量和人脸认知稳定性</li>
<li>results: 实现了大角度重定向的改进表现，同时保持高图像质量和人脸认知稳定性<details>
<summary>Abstract</summary>
In this paper, we propose a method for improving the angular accuracy and photo-reality of gaze and head redirection in full-face images. The problem with current models is that they cannot handle redirection at large angles, and this limitation mainly comes from the lack of training data. To resolve this problem, we create data augmentation by monocular 3D face reconstruction to extend the head pose and gaze range of the real data, which allows the model to handle a wider redirection range. In addition to the main focus on data augmentation, we also propose a framework with better image quality and identity preservation of unseen subjects even training with synthetic data. Experiments show that our method significantly improves redirection performance in terms of redirection angular accuracy while maintaining high image quality, especially when redirecting to large angles.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法来提高全面图像中的视线和头部重定向精度。现有模型无法处理大角度的重定向，主要是因为数据训练的限制。为解决这个问题，我们创造了一种数据扩展方法，通过单目3D脸部重建来扩展实际数据中的头部姿态和视线范围，这allow the model可以处理更广泛的重定向范围。除了主要关注数据扩展之外，我们还提出了一个框架，可以保持不seen subjects的图像质量和身份，即使使用 synthetic data 进行训练。实验表明，我们的方法可以明显提高重定向性能，特别是在重定向大角度时。
</details></li>
</ul>
<hr>
<h2 id="Phase-Specific-Augmented-Reality-Guidance-for-Microscopic-Cataract-Surgery-Using-Long-Short-Spatiotemporal-Aggregation-Transformer"><a href="#Phase-Specific-Augmented-Reality-Guidance-for-Microscopic-Cataract-Surgery-Using-Long-Short-Spatiotemporal-Aggregation-Transformer" class="headerlink" title="Phase-Specific Augmented Reality Guidance for Microscopic Cataract Surgery Using Long-Short Spatiotemporal Aggregation Transformer"></a>Phase-Specific Augmented Reality Guidance for Microscopic Cataract Surgery Using Long-Short Spatiotemporal Aggregation Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05209">http://arxiv.org/abs/2309.05209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puxun Tu, Hongfei Ye, Haochen Shi, Jeff Young, Meng Xie, Peiquan Zhao, Ce Zheng, Xiaoyi Jiang, Xiaojun Chen</li>
<li>For: The paper is focused on developing a novel phase-specific augmented reality (AR) guidance system for phacoemulsification cataract surgery (PCS) to enhance intraoperative proficiency.* Methods: The proposed system utilizes a two-stage surgical microscopic video recognition network, including a multi-task learning structure to segment the surgical limbus region and extract spatial features, and a long-short spatiotemporal aggregation transformer (LS-SAT) network to recognize the current surgical phase. The system also incorporates AR visual cues designed in collaboration with ophthalmologists.* Results: The proposed system was evaluated on publicly available and in-house datasets, with comparison results demonstrating its superior performance compared to related works. The system was also evaluated in a clinical setup, with results indicating remarkable accuracy and real-time performance, highlighting its potential for clinical applications.<details>
<summary>Abstract</summary>
Phacoemulsification cataract surgery (PCS) is a routine procedure conducted using a surgical microscope, heavily reliant on the skill of the ophthalmologist. While existing PCS guidance systems extract valuable information from surgical microscopic videos to enhance intraoperative proficiency, they suffer from non-phasespecific guidance, leading to redundant visual information. In this study, our major contribution is the development of a novel phase-specific augmented reality (AR) guidance system, which offers tailored AR information corresponding to the recognized surgical phase. Leveraging the inherent quasi-standardized nature of PCS procedures, we propose a two-stage surgical microscopic video recognition network. In the first stage, we implement a multi-task learning structure to segment the surgical limbus region and extract limbus region-focused spatial feature for each frame. In the second stage, we propose the long-short spatiotemporal aggregation transformer (LS-SAT) network to model local fine-grained and global temporal relationships, and combine the extracted spatial features to recognize the current surgical phase. Additionally, we collaborate closely with ophthalmologists to design AR visual cues by utilizing techniques such as limbus ellipse fitting and regional restricted normal cross-correlation rotation computation. We evaluated the network on publicly available and in-house datasets, with comparison results demonstrating its superior performance compared to related works. Ablation results further validated the effectiveness of the limbus region-focused spatial feature extractor and the combination of temporal features. Furthermore, the developed system was evaluated in a clinical setup, with results indicating remarkable accuracy and real-time performance. underscoring its potential for clinical applications.
</details>
<details>
<summary>摘要</summary>
喷洗cataract手术（PCS）是一种常见的手术程序，需要医生高度精准的技巧。现有的PCS导航系统可以从手术微镜视频中提取有价值信息，以提高手术过程中的灵活性，但是这些导航系统却受到非相对阶段的导航，从而导致多余的视觉信息。在本研究中，我们的主要贡献是开发了一种新的相对阶段增强现实（AR）导航系统，该系统可以根据识别到的当前手术阶段提供适应的AR信息。利用手术微镜程序的自然固有标准化特性，我们提议一种两阶段的手术微镜视频识别网络。在第一阶段，我们实施了多任务学习结构，将手术边缘区域分割出来，并对每帧图像提取边缘区域专门的空间特征。在第二阶段，我们提议使用长短距离空间时间汇聚变换（LS-SAT）网络，模型局部细腻和全局时间关系，并将提取的空间特征组合以识别当前手术阶段。此外，我们与眼科医生合作，通过利用技术such as镜面轮廓适应和区域限制正常垂直扩散计算来设计AR视觉提示。我们对公共可用和自有数据集进行评估，与相关工作进行比较，结果显示其性能更高。剥离结果进一步验证了镜边区域专门的空间特征提取器和时间特征的组合的效果。此外，我们开发的系统在临床设置中进行了评估，结果表明其精度和实时性具有很高的潜力。
</details></li>
</ul>
<hr>
<h2 id="Learning-Sequential-Acquisition-Policies-for-Robot-Assisted-Feeding"><a href="#Learning-Sequential-Acquisition-Policies-for-Robot-Assisted-Feeding" class="headerlink" title="Learning Sequential Acquisition Policies for Robot-Assisted Feeding"></a>Learning Sequential Acquisition Policies for Robot-Assisted Feeding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05197">http://arxiv.org/abs/2309.05197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priya Sundaresan, Jiajun Wu, Dorsa Sadigh</li>
<li>for: 这个论文的目的是开发一种基于视觉行为规划的长期 manipulate 杯子上的食物获取系统，以便让助手机机器人在吃饭时提供更好的帮助。</li>
<li>methods: 这个论文使用了一种名为 VAPORS 的框架，该框架通过在模拟中学习杯子的动态特征来学习高级动作选择策略。在实际世界中执行计划时，VAPORS 委托给可视参数化的基本动作来执行动作。</li>
<li>results: 在实际杯子上进行了38个食物获取任务中，VAPORS 比基eline高效得多，可以普遍应对实际杯子上的变化（如顶部和酱料），并在49名用户中的调查中得到了较高的用户满意度。<details>
<summary>Abstract</summary>
A robot providing mealtime assistance must perform specialized maneuvers with various utensils in order to pick up and feed a range of food items. Beyond these dexterous low-level skills, an assistive robot must also plan these strategies in sequence over a long horizon to clear a plate and complete a meal. Previous methods in robot-assisted feeding introduce highly specialized primitives for food handling without a means to compose them together. Meanwhile, existing approaches to long-horizon manipulation lack the flexibility to embed highly specialized primitives into their frameworks. We propose Visual Action Planning OveR Sequences (VAPORS), a framework for long-horizon food acquisition. VAPORS learns a policy for high-level action selection by leveraging learned latent plate dynamics in simulation. To carry out sequential plans in the real world, VAPORS delegates action execution to visually parameterized primitives. We validate our approach on complex real-world acquisition trials involving noodle acquisition and bimanual scooping of jelly beans. Across 38 plates, VAPORS acquires much more efficiently than baselines, generalizes across realistic plate variations such as toppings and sauces, and qualitatively appeals to user feeding preferences in a survey conducted across 49 individuals. Code, datasets, videos, and supplementary materials can be found on our website: https://sites.google.com/view/vaporsbot.
</details>
<details>
<summary>摘要</summary>
robot提供协助时需要执行特殊的机械操作，使用各种工具来拾取和给食各种食品。除了灵活的低级技能外，协助 robot还需要规划这些策略，在较长的时间距离内完成整个餐点。现有的机器人协助食物投入方法 introduce highly specialized primitives for food handling without a means to compose them together，而现有的长期机械 manipulate方法缺乏灵活性来嵌入高级特殊 primitives。我们提出了Visual Action Planning OveR Sequences（VAPORS），一种长期食物获取框架。VAPORS学习一个高级行为选择策略，通过在模拟中学习latent plate dynamics来执行。为了在实际世界中执行sequential plans，VAPORS委托action execution给视觉参数化的基本 primitives。我们验证了我们的方法在复杂的实际食物获取任务中，比baseline效果更高，可以 generale across realistic plate variations such as toppings and sauces，并且在49名用户中的调查中获得了负面feeding preferences的评价。代码、数据集、视频和补充材料可以在我们的网站上找到：https://sites.google.com/view/vaporsbot。
</details></li>
</ul>
<hr>
<h2 id="Towards-Viewpoint-Robustness-in-Bird’s-Eye-View-Segmentation"><a href="#Towards-Viewpoint-Robustness-in-Bird’s-Eye-View-Segmentation" class="headerlink" title="Towards Viewpoint Robustness in Bird’s Eye View Segmentation"></a>Towards Viewpoint Robustness in Bird’s Eye View Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05192">http://arxiv.org/abs/2309.05192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tzofi Klinghoffer, Jonah Philion, Wenzheng Chen, Or Litany, Zan Gojcic, Jungseock Joo, Ramesh Raskar, Sanja Fidler, Jose M. Alvarez</li>
<li>For: 这个论文旨在解决自动驾驶车辆（AV）中神经网络的视角不一致问题，以便在多种车辆上部署神经网络模型 без重复收集和标注数据。* Methods: 作者们提出了一种基于鸟瞰视（BEV）分割任务的方法，使用novel view synthesis技术将收集的数据转换到目标摄像头配置的视角下，以便在不同的摄像头配置上训练BEV分割模型。* Results: 作者们通过广泛的实验发现，现有的感知模型具有较大的视角不一致敏感度，当训练数据来自特定的摄像头配置时，小量的视角变化会导致大幅下降在性能。作者们的方法能够恢复约14.7%的 intersectioin over union（IoU），即使在新的摄像头配置上部署模型。<details>
<summary>Abstract</summary>
Autonomous vehicles (AV) require that neural networks used for perception be robust to different viewpoints if they are to be deployed across many types of vehicles without the repeated cost of data collection and labeling for each. AV companies typically focus on collecting data from diverse scenarios and locations, but not camera rig configurations, due to cost. As a result, only a small number of rig variations exist across most fleets. In this paper, we study how AV perception models are affected by changes in camera viewpoint and propose a way to scale them across vehicle types without repeated data collection and labeling. Using bird's eye view (BEV) segmentation as a motivating task, we find through extensive experiments that existing perception models are surprisingly sensitive to changes in camera viewpoint. When trained with data from one camera rig, small changes to pitch, yaw, depth, or height of the camera at inference time lead to large drops in performance. We introduce a technique for novel view synthesis and use it to transform collected data to the viewpoint of target rigs, allowing us to train BEV segmentation models for diverse target rigs without any additional data collection or labeling cost. To analyze the impact of viewpoint changes, we leverage synthetic data to mitigate other gaps (content, ISP, etc). Our approach is then trained on real data and evaluated on synthetic data, enabling evaluation on diverse target rigs. We release all data for use in future work. Our method is able to recover an average of 14.7% of the IoU that is otherwise lost when deploying to new rigs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HiLM-D-Towards-High-Resolution-Understanding-in-Multimodal-Large-Language-Models-for-Autonomous-Driving"><a href="#HiLM-D-Towards-High-Resolution-Understanding-in-Multimodal-Large-Language-Models-for-Autonomous-Driving" class="headerlink" title="HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving"></a>HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05186">http://arxiv.org/abs/2309.05186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinpeng Ding, Jianhua Han, Hang Xu, Wei Zhang, Xiaomeng Li</li>
<li>for: 这 paper 是为了推动自动驾驶系统的多任务合一化，使用单一的语言模型来把多个自动驾驶任务从视频中提取出来。</li>
<li>methods: 这 paper 使用了 singular multimodal large language models (MLLMs) 来把多个自动驾驶任务从视频中提取出来，并且提出了一种 efficient method 来 incorporate high-resolution (HR) information into MLLMs。</li>
<li>results: 实验结果显示，与现有的 MLLMs 相比，HiLM-D 在 ROLISP 任务上表现出色，提高了 4.8% 的 BLEU-4 和 17.2% 的 mIoU。<details>
<summary>Abstract</summary>
Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.
</details>
<details>
<summary>摘要</summary>
自适应驾驶系统通常采用分离的模型来处理不同任务，导致设计变得复杂。我们是第一个使用单一多Modal大语言模型（MLLMs）将多个自适应驾驶任务从视频中整合，即风险对象本地化和建议预测（ROLISP）任务。ROLISP使用自然语言同时识别和解释风险对象，理解ego汽车的意图，并提供动作建议，从而消除任务特定的建筑。然而，由于缺乏高分辨率（HR）信息，现有的MLLMs经常会遗弃小对象（例如交通标志），而偏重于突出对象（例如大卡车）。我们提出了HiLM-D（推向高分辨率理解在MLLMs中的自适应驾驶），一种高效的方法，将HR信息 integrate到MLLMs中。尤其是HiLM-D具有两个分支：（i）低分辨率理解分支，可以是任何MLLMs，处理低分辨率视频，描述风险对象和ego汽车的意图/建议;（ii）高分辨率感知分支（HR-PB），特点在HiLM-D中，通过捕捉视觉特定的HR特征图和优先处理所有风险对象，提高检测精度。我们的HR-PB作为插件模块，顺利地适配现有MLLMs。实验表明HiLM-D在ROLISP数据集上表现出色，与领先MLLMs相比，提高了4.8%的BLEU-4措词率和17.2%的mIoU检测精度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/cs.CV_2023_09_11/" data-id="clp869txo00jlk5884y1c3377" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/cs.AI_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T12:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/11/cs.AI_2023_09_11/">cs.AI - 2023-09-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-bionic-neural-network-for-external-simulation-of-human-locomotor-system"><a href="#The-bionic-neural-network-for-external-simulation-of-human-locomotor-system" class="headerlink" title="The bionic neural network for external simulation of human locomotor system"></a>The bionic neural network for external simulation of human locomotor system</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05863">http://arxiv.org/abs/2309.05863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Shi, Shuhao Ma, Yihui Zhao</li>
<li>for: This paper aims to propose a physics-informed deep learning method to predict joint motion and muscle forces using musculoskeletal (MSK) modeling techniques.</li>
<li>methods: The proposed method embeds the MSK model into a neural network as an ordinary differential equation (ODE) loss function, allowing for the automatic estimation of subject-specific MSK physiological parameters during the training process.</li>
<li>results: The experimental validations on two datasets demonstrate that the proposed deep learning method can accurately identify subject-specific MSK physiological parameters and yield accurate predictions of joint motion and muscle forces.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是提出一种基于物理学习的深度学习方法，用于预测关节运动和肌肉力量。</li>
<li>methods: 该方法将MSK模型 integrate到神经网络中，作为常微分方程（ODE）损失函数，以便自动在训练过程中确定subject特定的MSK生物学参数。</li>
<li>results: 对两个数据集进行了实验验证，结果表明，该提出的深度学习方法可以准确地确定subject特定的MSK生物学参数，并且生成准确的关节运动和肌肉力量预测。<details>
<summary>Abstract</summary>
Muscle forces and joint kinematics estimated with musculoskeletal (MSK) modeling techniques offer useful metrics describing movement quality. Model-based computational MSK models can interpret the dynamic interaction between the neural drive to muscles, muscle dynamics, body and joint kinematics, and kinetics. Still, such a set of solutions suffers from high computational time and muscle recruitment problems, especially in complex modeling. In recent years, data-driven methods have emerged as a promising alternative due to the benefits of flexibility and adaptability. However, a large amount of labeled training data is not easy to be acquired. This paper proposes a physics-informed deep learning method based on MSK modeling to predict joint motion and muscle forces. The MSK model is embedded into the neural network as an ordinary differential equation (ODE) loss function with physiological parameters of muscle activation dynamics and muscle contraction dynamics to be identified. These parameters are automatically estimated during the training process which guides the prediction of muscle forces combined with the MSK forward dynamics model. Experimental validations on two groups of data, including one benchmark dataset and one self-collected dataset from six healthy subjects, are performed. The results demonstrate that the proposed deep learning method can effectively identify subject-specific MSK physiological parameters and the trained physics-informed forward-dynamics surrogate yields accurate motion and muscle forces predictions.
</details>
<details>
<summary>摘要</summary>
筋力和关节动力学估算使用musculoskeletal（MSK）模型技术提供有用的运动质量指标。模型基于计算机MSK模型可以解释动态中 между神经驱动筋肉、筋肉动力、身体和关节动力学的相互作用。然而，这些解决方案受到高计算时间和肌肉征调问题困扰，特别是在复杂的模型中。在过去几年，数据驱动方法出现为一种可能的替代方案，因为它们具有灵活性和适应性。然而，大量标注训练数据很难获得。这篇论文提议一种基于MSK模型的物理学习方法，用于预测关节运动和肌力。MSK模型被嵌入到神经网络中作为常微分方程（ODE）损失函数，以便在训练过程中自动确定肌肉活动动态和肌肉强制动态的物理参数。这些参数被自动确定，并导向预测肌肉力的组合，与MSK前向动力学模型相结合。实验验证了这种深度学习方法的有效性，在六名健康者的两组数据上进行了实验验证。结果表明，提议的深度学习方法可以有效地特定个体MSK生物学参数，并且训练的物理学习前向动力学代理模型可以准确预测运动和肌力。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-mesa-optimization-algorithms-in-Transformers"><a href="#Uncovering-mesa-optimization-algorithms-in-Transformers" class="headerlink" title="Uncovering mesa-optimization algorithms in Transformers"></a>Uncovering mesa-optimization algorithms in Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05858">http://arxiv.org/abs/2309.05858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jimmieliu/transformer-mesa-layer">https://github.com/jimmieliu/transformer-mesa-layer</a></li>
<li>paper_authors: Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Blaise Agüera y Arcas, Max Vladymyrov, Razvan Pascanu, João Sacramento</li>
<li>for: 本研究旨在解释Transformers模型的优秀表现是如何实现的，特别是该模型在深度学习中的表现。</li>
<li>methods: 本研究使用了倒推工程来探索Transformers模型中的架构偏好，并发现了一种叫做“mesa-optimization”的学习过程。此外，研究者还使用了一系列的autoregressive Transformers模型来测试这个假设。</li>
<li>results: 研究结果显示，Transformers模型中的mesa-optimization过程可以帮助模型更好地适应内置学习任务，并且可以在几乎没有训练数据的情况下解决几乎任何深度学习任务。此外，研究者还提出了一个新的自我对齐层（mesa-layer），可以辅助模型更好地解决内置学习任务。<details>
<summary>Abstract</summary>
Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention layer, the mesa-layer, that explicitly and efficiently solves optimization problems specified in context. We find that this layer can lead to improved performance in synthetic and preliminary language modeling experiments, adding weight to our hypothesis that mesa-optimization is an important operation hidden within the weights of trained Transformers.
</details>
<details>
<summary>摘要</summary>
transformers 已成为深度学习中最具有优势的模型，但其表现出色的原因尚未得到充分理解。在这里，我们提出了一种假设，即 transformers 的优异表现是由于模型具有一种叫做“mesa-优化”的建筑性偏好，这是在模型的前进通道中进行的两步过程：（i）内部学习目标的建构，以及（ii）通过优化来找到解决方案。为检验这一假设，我们对一系列基于自然语言处理任务的排序 Transformers 进行了反向工程，揭示了这些模型在生成预测时使用的梯度-based mesa-优化算法。此外，我们还证明了这种学习前进通道优化算法可以立即应用于解决一些简单的几个shot任务，表明了 mesa-优化可能在大语言模型中支持Contextual learning 的能力。最后，我们提出了一种新的自注意层，即 mesa-层，它可以专门和有效地解决在 Context 中指定的优化问题。我们发现这层可以在 synthetic 和预liminary 语言处理实验中提高表现，这进一步支持了我们假设，即 mesa-优化是训练过的 transformers 中隐藏的重要操作。
</details></li>
</ul>
<hr>
<h2 id="Challenges-in-Annotating-Datasets-to-Quantify-Bias-in-Under-represented-Society"><a href="#Challenges-in-Annotating-Datasets-to-Quantify-Bias-in-Under-represented-Society" class="headerlink" title="Challenges in Annotating Datasets to Quantify Bias in Under-represented Society"></a>Challenges in Annotating Datasets to Quantify Bias in Under-represented Society</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08624">http://arxiv.org/abs/2309.08624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vithya Yogarajan, Gillian Dobbie, Timothy Pistotti, Joshua Bensemann, Kobe Knowles<br>for: This research aims to address the lack of annotated datasets for quantifying bias in under-represented societies, specifically focusing on the New Zealand (NZ) population.methods: The research involves the manual annotation of benchmark datasets for binary gender classification and ethical&#x2F;racial considerations, despite the challenges faced with the availability of only three annotators.results: The research provides an overview of the challenges encountered and lessons learnt during the manual annotation process, and offers recommendations for future research on quantifying bias in under-represented societies.<details>
<summary>Abstract</summary>
Recent advances in artificial intelligence, including the development of highly sophisticated large language models (LLM), have proven beneficial in many real-world applications. However, evidence of inherent bias encoded in these LLMs has raised concerns about equity. In response, there has been an increase in research dealing with bias, including studies focusing on quantifying bias and developing debiasing techniques. Benchmark bias datasets have also been developed for binary gender classification and ethical/racial considerations, focusing predominantly on American demographics. However, there is minimal research in understanding and quantifying bias related to under-represented societies. Motivated by the lack of annotated datasets for quantifying bias in under-represented societies, we endeavoured to create benchmark datasets for the New Zealand (NZ) population. We faced many challenges in this process, despite the availability of three annotators. This research outlines the manual annotation process, provides an overview of the challenges we encountered and lessons learnt, and presents recommendations for future research.
</details>
<details>
<summary>摘要</summary>
近年人工智能的发展，包括高度复杂的大语言模型（LLM），在各个实际应用中得到了 beneficial 的效果。然而，这些 LLM 中的内置偏见问题引起了公平性的担忧。为了应对这些偏见，研究人员们开始了偏见的研究，包括量化偏见和开发减偏见技术。为了适应美国民族的性别和种族考虑，已经开发了一些偏见数据集。然而，对于被排挤社会的偏见问题还没有充分的研究。我们被动机于lack of annotated datasets for quantifying bias in under-represented societies，而 endeavoured 创建了新西兰（NZ）人口的 referential datasets。我们在这个过程中遇到了许多挑战，即使有三名注释者。本研究描述了我们的手动注释过程，介绍了我们遇到的挑战和学习点，并提出了未来研究的建议。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Compiler-Optimization"><a href="#Large-Language-Models-for-Compiler-Optimization" class="headerlink" title="Large Language Models for Compiler Optimization"></a>Large Language Models for Compiler Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07062">http://arxiv.org/abs/2309.07062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Chris Cummins, Volker Seeker, Dejan Grubisic, Mostafa Elhoushi, Youwei Liang, Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Kim Hazelwood, Gabriel Synnaeve, Hugh Leather</li>
<li>for: 这个论文旨在应用大型自然语言模型来优化编程代码。</li>
<li>methods: 作者使用了一个7亿参数的转换器模型，从零开始训练，以优化LLVM� assembly代码。模型接受不优化的assembly输入，并输出一个包含编译器选项的列表，以最优化程序。在训练过程中，模型需要预测未优化代码和优化后代码的指令计数，以及优化后代码本身。这些辅助学习任务有助于提高优化模型的性能和理解深度。</li>
<li>results: 作者在一个大量测试程序中评估了他们的方法。结果显示，他们的方法可以比基eline的编译器减少指令数量3.0%，并且超过了两个基eline的基eline的优化方法，这两个基eline需要多达千次编译。此外，模型表现出了逗号 surprisingly strong的代码理解能力，可以生成可编译代码91%的时间，并且完全复制编译器的输出70%的时间。<details>
<summary>Abstract</summary>
We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding.   We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time.
</details>
<details>
<summary>摘要</summary>
我团队在探索大语言模型应用于代码优化方面做出了新的应用。我们提出了一个7亿参数的转换器模型，从头开始训练，以优化LLVM Assembly代码的大小。该模型接受未优化的Assembly输入，并输出一个包含编译器选项的列表，以优化程序。在训练过程中，我们要求模型预测未优化代码和优化后代码的指令计数，以及优化后代码本身。这些辅助学习任务有助于提高优化模型的性能和代码理解深度。我们对一个大量测试程序进行评估。我们的方法在减少指令数量方面比编译器更高，提高了3.0%。此外，模型表现出了奇异的代码理解能力，生成的代码91%的时间可编译，并且70%的时间完全模拟了编译器的输出。
</details></li>
</ul>
<hr>
<h2 id="Effective-Abnormal-Activity-Detection-on-Multivariate-Time-Series-Healthcare-Data"><a href="#Effective-Abnormal-Activity-Detection-on-Multivariate-Time-Series-Healthcare-Data" class="headerlink" title="Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data"></a>Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05845">http://arxiv.org/abs/2309.05845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjia Niu, Yuchen Zhao, Hamed Haddadi</li>
<li>for: 这个研究旨在探讨多重时间序列数据（MTS）中的异常活动检测，以实现智能健康领域中的精确异常检测。</li>
<li>methods: 本研究提出了一种基于差异的异常检测方法（Rs-AD），并通过学习表现优化和异常活动检测来解决MTS数据中异常活动的探测问题。</li>
<li>results: 实验结果显示，Rs-AD方法在一个真实世界的步行数据集上取得了F1分数0.839，显示了该方法的效果。<details>
<summary>Abstract</summary>
Multivariate time series (MTS) data collected from multiple sensors provide the potential for accurate abnormal activity detection in smart healthcare scenarios. However, anomalies exhibit diverse patterns and become unnoticeable in MTS data. Consequently, achieving accurate anomaly detection is challenging since we have to capture both temporal dependencies of time series and inter-relationships among variables. To address this problem, we propose a Residual-based Anomaly Detection approach, Rs-AD, for effective representation learning and abnormal activity detection. We evaluate our scheme on a real-world gait dataset and the experimental results demonstrate an F1 score of 0.839.
</details>
<details>
<summary>摘要</summary>
多变量时间序列数据从多个传感器收集得到，提供了智能医疗场景中准确异常活动检测的潜在潜力。然而，异常活动在时间序列数据中显示多样的模式，容易被遗弃。因此，实现准确的异常检测是一项挑战，因为我们需要捕捉时间序列的 temporally 相关性和变量之间的相互关系。为解决这问题，我们提议一种基于差异的异常检测方法，Rs-AD，以便有效地学习表示和异常检测。我们对一个真实的步态数据集进行了实验，结果显示了 F1 分数为 0.839。
</details></li>
</ul>
<hr>
<h2 id="PACE-LM-Prompting-and-Augmentation-for-Calibrated-Confidence-Estimation-with-GPT-4-in-Cloud-Incident-Root-Cause-Analysis"><a href="#PACE-LM-Prompting-and-Augmentation-for-Calibrated-Confidence-Estimation-with-GPT-4-in-Cloud-Incident-Root-Cause-Analysis" class="headerlink" title="PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis"></a>PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05833">http://arxiv.org/abs/2309.05833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dylan Zhang, Xuchao Zhang, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, Saravan Rajmohan</li>
<li>for: 本研究旨在提高云计算环境中Root Cause Analysis（RCA）工具的可靠性和准确性，以确保服务可靠性和客户信任。</li>
<li>methods: 本研究提出了一种基于大语言模型（LLM）的提取补充法，可以增强RCA工具的自信估计。该方法包括两个阶段：首先，模型根据历史事件数据评估自己的信息强度，然后审查由预测器生成的根 causa。最后，一个优化步骤将这些评估结果组合起来确定最终的自信分配。</li>
<li>results: 实验结果表明，我们的方法可以让模型更好地表达自己的自信度，提供更加抗摩擦的分数。我们解决了一些研究问题，包括使用LLMs生成的自信度是否准确、域специ fic retrieved examples对自信度估计的影响和不同RCA模型之间的通用性。通过这些研究，我们希望bridge自信度估计的差距，帮助on-call工程师做出更加有 confidence的决策，提高云 incident管理的效率。<details>
<summary>Abstract</summary>
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the final confidence assignment. Experimental results illustrate that our method enables the model to articulate its confidence effectively, providing a more calibrated score. We address research questions evaluating the ability of our method to produce calibrated confidence scores using LLMs, the impact of domain-specific retrieved examples on confidence estimates, and its potential generalizability across various root cause analysis models. Through this, we aim to bridge the confidence estimation gap, aiding on-call engineers in decision-making and bolstering the efficiency of cloud incident management.
</details>
<details>
<summary>摘要</summary>
近年来，云计算领域内的服务可靠性和客户信任的重要性得到了更多的认可。为了确保服务可靠性，cloud incident根本原因分析成为了云计算领域内一项重要的任务。然而，由于当今云基础设施的复杂性，这个过程中的root cause分析具有挑战性。虽然AI驱动的root cause标识工具在市场上普及，但它们的应用受限于输出质量的不一致。这篇论文提出了一种方法，通过提高AI模型对根本原因分析结果的置信度的估计来增强root cause分析的可靠性。这种方法包括两个阶段：首先，模型根据历史事件数据评估其自身的置信度，然后对predictor生成的根本原因进行审查。最后，一个优化步骤将这两个评估结果组合起来确定最终的置信度分配。实验结果表明，我们的方法可以有效地使模型表达其置信度，提供一个更加准确的分数。我们解决了关于我们方法能否生成准确的置信度分数、采用域名Specific retrieved例子对置信度估计的影响以及其普适性的研究问题。通过这些研究，我们希望bridge置信度估计的差距，帮助on-call工程师在决策过程中更加准确，提高云 incident管理的效率。
</details></li>
</ul>
<hr>
<h2 id="Studying-Accuracy-of-Machine-Learning-Models-Trained-on-Lab-Lifting-Data-in-Solving-Real-World-Problems-Using-Wearable-Sensors-for-Workplace-Safety"><a href="#Studying-Accuracy-of-Machine-Learning-Models-Trained-on-Lab-Lifting-Data-in-Solving-Real-World-Problems-Using-Wearable-Sensors-for-Workplace-Safety" class="headerlink" title="Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data in Solving Real-World Problems Using Wearable Sensors for Workplace Safety"></a>Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data in Solving Real-World Problems Using Wearable Sensors for Workplace Safety</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05831">http://arxiv.org/abs/2309.05831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Bertrand, Nick Griffey, Ming-Lun Lu, Rashmi Jha</li>
<li>for: 本研究旨在将实验室训练的机器学习模型（lifting identification model） портирова到实际世界中。</li>
<li>methods: 本研究使用了四种可能的解决方案来提高模型表现，包括：1）调整模型的参数；2）将训练数据集与实际世界中的数据集进行混合训练；3）将模型调整为适应实际世界中的环境；4）将模型训练于更大的数据集中。</li>
<li>results: 经过实验和分析后，研究发现这些方案都能够提高模型的表现，并且可以在实际世界中获得较好的结果。<details>
<summary>Abstract</summary>
Porting ML models trained on lab data to real-world situations has long been a challenge. This paper discusses porting a lab-trained lifting identification model to the real-world. With performance much lower than on training data, we explored causes of the failure and proposed four potential solutions to increase model performance
</details>
<details>
<summary>摘要</summary>
将实验室训练的机器学习模型应用到实际世界中的挑战一直存在。本篇文章讨论了将实验室训练的抬重识别模型应用到实际世界中的问题。模型在实际世界中的性能与训练数据之间存在很大差距，我们探索了引起这个问题的可能性和提出了四种解决方案以提高模型性能。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Geometric-Deep-Learning-For-Precipitation-Nowcasting"><a href="#Exploring-Geometric-Deep-Learning-For-Precipitation-Nowcasting" class="headerlink" title="Exploring Geometric Deep Learning For Precipitation Nowcasting"></a>Exploring Geometric Deep Learning For Precipitation Nowcasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05828">http://arxiv.org/abs/2309.05828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shan Zhao, Sudipan Saha, Zhitong Xiong, Niklas Boers, Xiao Xiang Zhu</li>
<li>for: 预测降水（几个小时内）的准确性仍然是一个挑战，因为需要准确地捕捉当地复杂的地方交互。</li>
<li>methods: 我们采用几何深度学习来普通化神经网络模型，以便更好地模型非欧几何空间中的地方关系。我们使用自动学习的对数矩阵来学习邻居Matrix，然后通过GCN层和1D核函数来提高空间和时间信息的抽象。</li>
<li>results: 我们在特伦托&#x2F;意大利地区的雷达反射图序列上测试了模型，结果显示GCN可以更好地模型云profile的本地细节，以及提高预测准确性。<details>
<summary>Abstract</summary>
Precipitation nowcasting (up to a few hours) remains a challenge due to the highly complex local interactions that need to be captured accurately. Convolutional Neural Networks rely on convolutional kernels convolving with grid data and the extracted features are trapped by limited receptive field, typically expressed in excessively smooth output compared to ground truth. Thus they lack the capacity to model complex spatial relationships among the grids. Geometric deep learning aims to generalize neural network models to non-Euclidean domains. Such models are more flexible in defining nodes and edges and can effectively capture dynamic spatial relationship among geographical grids. Motivated by this, we explore a geometric deep learning-based temporal Graph Convolutional Network (GCN) for precipitation nowcasting. The adjacency matrix that simulates the interactions among grid cells is learned automatically by minimizing the L1 loss between prediction and ground truth pixel value during the training procedure. Then, the spatial relationship is refined by GCN layers while the temporal information is extracted by 1D convolution with various kernel lengths. The neighboring information is fed as auxiliary input layers to improve the final result. We test the model on sequences of radar reflectivity maps over the Trento/Italy area. The results show that GCNs improves the effectiveness of modeling the local details of the cloud profile as well as the prediction accuracy by achieving decreased error measures.
</details>
<details>
<summary>摘要</summary>
现在降水预测（几个小时）仍然是一个挑战，因为需要准确地捕捉当地复杂的地方交互。卷积神经网络（Convolutional Neural Networks，简称CNN）依靠卷积核对网格数据进行 convolution 操作，但是抽取特征被局部响应场所限制，通常会导致过度平滑的输出与实际值不匹配。这些模型缺乏模elling复杂的空间关系。使用非欧几何学学习（Geometric Deep Learning）可以普通化神经网络模型，使其在非欧几何空间中进行模型化。这些模型可以更 flexibly 定义节点和边，并有效地捕捉地图中的动态空间关系。鼓动于这一点，我们提出一种基于非欧几何学学习的temporal Graph Convolutional Network（GCN） для降水预测。在训练过程中，自动学习 adjacency 矩阵，表示各个网格单元之间的交互，可以通过L1损失函数和实际值像素值进行最小化。然后，GCN层通过各种核长进行1D卷积，提取时间信息，并通过邻域信息作为辅助输入层来改善最终结果。我们在特伦托/意大利区的雷达反射率图序列上测试了这种模型。结果表明，GCNs可以更好地模型云Profile的本地细节以及预测精度，并实现了降低错误度的目标。
</details></li>
</ul>
<hr>
<h2 id="PhotoVerse-Tuning-Free-Image-Customization-with-Text-to-Image-Diffusion-Models"><a href="#PhotoVerse-Tuning-Free-Image-Customization-with-Text-to-Image-Diffusion-Models" class="headerlink" title="PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models"></a>PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05793">http://arxiv.org/abs/2309.05793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, Min Zheng</li>
<li>for: 这篇论文旨在提出一种新的文本到图像生成方法，以提高个性化图像生成的效率和质量。</li>
<li>methods: 该方法采用双树条件机制，在文本和图像域都进行conditioning，以实现更好的控制图像生成过程。此外，我们还引入了一种新的人脸身份损失组件，以提高图像生成过程中的人脸保持性。</li>
<li>results: 我们的提案的PhotoVerse方法可以在几秒钟内生成高质量的图像，并且可以生成各种不同的场景和风格的图像。我们的方法还可以完全消除测试时间调整，只需要提供一个目标人脸的单一图像。<details>
<summary>Abstract</summary>
Personalized text-to-image generation has emerged as a powerful and sought-after tool, empowering users to create customized images based on their specific concepts and prompts. However, existing approaches to personalization encounter multiple challenges, including long tuning times, large storage requirements, the necessity for multiple input images per identity, and limitations in preserving identity and editability. To address these obstacles, we present PhotoVerse, an innovative methodology that incorporates a dual-branch conditioning mechanism in both text and image domains, providing effective control over the image generation process. Furthermore, we introduce facial identity loss as a novel component to enhance the preservation of identity during training. Remarkably, our proposed PhotoVerse eliminates the need for test time tuning and relies solely on a single facial photo of the target identity, significantly reducing the resource cost associated with image generation. After a single training phase, our approach enables generating high-quality images within only a few seconds. Moreover, our method can produce diverse images that encompass various scenes and styles. The extensive evaluation demonstrates the superior performance of our approach, which achieves the dual objectives of preserving identity and facilitating editability. Project page: https://photoverse2d.github.io/
</details>
<details>
<summary>摘要</summary>
个人化文本到图像生成技术已经成为当前最强大和最受欢迎的工具，允许用户根据自己的具体概念和提示来创建自定义的图像。然而，现有的个人化方法面临多个挑战，包括长时间调整、大量存储需求、每个标识符需要多个输入图像，以及保持标识和可编辑性的限制。为解决这些挑战，我们提出了 PhotoVerse，一种创新的方法，它在文本和图像领域都采用双枝条件机制，以提供有效控制图像生成过程。此外，我们还引入了人脸标识损失作为一种新的组件，以增强在训练过程中保持标识的能力。值得一提的是，我们的提议的 PhotoVerse 不需要测试时间调整，仅需要一个目标标识人脸的单个图像，可以减少图像生成的资源成本。另外，我们的方法可以在只需几秒钟内生成高质量图像，并且可以生成包括不同场景和风格的多种图像。我们的评估结果表明，我们的方法可以同时保持标识和促进可编辑性的两个目标。项目页面：https://photoverse2d.github.io/
</details></li>
</ul>
<hr>
<h2 id="Adaptive-User-centered-Neuro-symbolic-Learning-for-Multimodal-Interaction-with-Autonomous-Systems"><a href="#Adaptive-User-centered-Neuro-symbolic-Learning-for-Multimodal-Interaction-with-Autonomous-Systems" class="headerlink" title="Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems"></a>Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05787">http://arxiv.org/abs/2309.05787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Gomaa, Michael Feld</li>
<li>for: 这篇论文目的是提出一种基于人类教学的人工智能系统设计方法，以便让人工智能系统更好地理解物体和环境。</li>
<li>methods: 这篇论文提出了一种以多模态输入和输出为基础的人工智能系统设计方法，包括人类教学和机器学习等技术。</li>
<li>results: 这篇论文提出了一些假设和设计指南，以及一个 relate work 的应用场景，以便实现人工智能系统的更高水平的学习能力。<details>
<summary>Abstract</summary>
Recent advances in machine learning, particularly deep learning, have enabled autonomous systems to perceive and comprehend objects and their environments in a perceptual subsymbolic manner. These systems can now perform object detection, sensor data fusion, and language understanding tasks. However, there is a growing need to enhance these systems to understand objects and their environments more conceptually and symbolically. It is essential to consider both the explicit teaching provided by humans (e.g., describing a situation or explaining how to act) and the implicit teaching obtained by observing human behavior (e.g., through the system's sensors) to achieve this level of powerful artificial intelligence. Thus, the system must be designed with multimodal input and output capabilities to support implicit and explicit interaction models. In this position paper, we argue for considering both types of inputs, as well as human-in-the-loop and incremental learning techniques, for advancing the field of artificial intelligence and enabling autonomous systems to learn like humans. We propose several hypotheses and design guidelines and highlight a use case from related work to achieve this goal.
</details>
<details>
<summary>摘要</summary>
Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in a Perceptual Subsymbolic Manner. However, There Is a Growing Need to Enhance These Systems to Understand Objects and Their Environments More Conceptually and Symbolically.Recent Advances in Machine Learning Have Enabled Autonomous Systems to Perceive and Comprehend Objects and Their Environments in
</details></li>
</ul>
<hr>
<h2 id="Grey-box-Bayesian-Optimization-for-Sensor-Placement-in-Assisted-Living-Environments"><a href="#Grey-box-Bayesian-Optimization-for-Sensor-Placement-in-Assisted-Living-Environments" class="headerlink" title="Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments"></a>Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05784">http://arxiv.org/abs/2309.05784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shadan Golestan, Omid Ardakanian, Pierre Boulanger</li>
<li>for: 这篇论文是为了实现帮助生活空间中的堕落检测、室内定位和活动识别而优化传感器配置和位置。</li>
<li>methods: 本文提出了一种新的、对称的搜寻方法，利用灰色泵测测和模拟评估，寻找在无限室内空间中高质量的传感器配置。本文的主要技术贡献在于将内部活动的空间分布知识 integrate 到 Bayesian 优化中的迭代选择中。</li>
<li>results: 在两个 simulated 室内环境和一个真实世界数据中，我们显示了我们的提案方法在识别高质量传感器配置方面比 state-of-the-art 黑色盒子优化技术更好，实现了更高的 F1 分数，而且需要较少 (51.3% 的平均) 耗时和价格的函数询问。<details>
<summary>Abstract</summary>
Optimizing the configuration and placement of sensors is crucial for reliable fall detection, indoor localization, and activity recognition in assisted living spaces. We propose a novel, sample-efficient approach to find a high-quality sensor placement in an arbitrary indoor space based on grey-box Bayesian optimization and simulation-based evaluation. Our key technical contribution lies in capturing domain-specific knowledge about the spatial distribution of activities and incorporating it into the iterative selection of query points in Bayesian optimization. Considering two simulated indoor environments and a real-world dataset containing human activities and sensor triggers, we show that our proposed method performs better compared to state-of-the-art black-box optimization techniques in identifying high-quality sensor placements, leading to accurate activity recognition in terms of F1-score, while also requiring a significantly lower (51.3% on average) number of expensive function queries.
</details>
<details>
<summary>摘要</summary>
优化感知器置设和位置对于可靠的落体检测、indoor定位和活动识别在助生活空间中是关键。我们提出了一种新的、样本效率高的方法，通过灰度box bayesian优化和基于模拟的评估来找到高质量的感知器置设。我们的关键技术之一是利用域专业知识来捕捉活动空间的空间分布，并在 Bayesian 优化中逐步选择查询点。对于两个 simulated indoor 环境和一个实际数据集中的人类活动和感知器触发，我们表明了我们的提议方法在与现有的黑盒优化技术相比，能够更好地确定高质量的感知器置设，导致更加准确的活动识别（F1 分数），同时也只需要 significantly  fewer（51.3% 的平均）次昂贵的函数查询。
</details></li>
</ul>
<hr>
<h2 id="Robot-Parkour-Learning"><a href="#Robot-Parkour-Learning" class="headerlink" title="Robot Parkour Learning"></a>Robot Parkour Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05665">http://arxiv.org/abs/2309.05665</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZiwenZhuang/parkour">https://github.com/ZiwenZhuang/parkour</a></li>
<li>paper_authors: Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, Hang Zhao</li>
<li>for: 本研究旨在开发一种基于视觉的全息攻击策略，以便机器人可以在复杂环境中快速跨越各种障碍。</li>
<li>methods: 我们提出了一种基于强化学习的方法，使用简单的奖励函数来学习多种视觉基于的困难跨越技能，包括爬高障碍、跃越大距离、蹲下低障碍、缩进窄障碍和跑步。</li>
<li>results: 我们在实验中示出，我们的系统可以将这些技能融合成一个单一的视觉基于的全息攻击策略，并将其转移到一个四脚机器人上使用其 Egocentric depth camera。我们的系统可以让两个不同的低成本机器人自主选择和执行适合的跨越技能，以 traverse 复杂的实际环境。<details>
<summary>Abstract</summary>
Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower two different low-cost robots to autonomously select and execute appropriate parkour skills to traverse challenging real-world environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Hypothesis-Search-Inductive-Reasoning-with-Language-Models"><a href="#Hypothesis-Search-Inductive-Reasoning-with-Language-Models" class="headerlink" title="Hypothesis Search: Inductive Reasoning with Language Models"></a>Hypothesis Search: Inductive Reasoning with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05660">http://arxiv.org/abs/2309.05660</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D. Goodman<br>for: 这个论文的目的是提高大语言模型（LLM）在推理 inductive reasoning 能力。methods: 该论文使用了生成抽象假设的方法，首先提出多个抽象假设，然后将这些假设转换成 Python 程序，并将这些程序直接应用到观察到的示例上进行验证。results: 该论文的实验结果表明，使用这种方法可以大幅提高 LLM 在 inductive reasoning 任务上的表现。在 ARC 视觉 inductive reasoning benchmark 上，使用自动生成的假设和程序可以达到 27.5% 的准确率，比直接提示baseline（准确率为 12.5%）高出许多。并且，通过人工选择 LLM 生成的候选者来减少生成的数量，可以进一步提高表现，达到 37.5% 的准确率。<details>
<summary>Abstract</summary>
Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding "in context learning." This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. Because of the prohibitive cost of generation with state-of-the-art LLMs, we consider a middle step to filter the set of hypotheses that will be implemented into programs: we either ask the LLM to summarize into a smaller set of hypotheses, or ask human annotators to select a subset of the hypotheses. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problem subset of ARC, our automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%. (And we argue this is a lower bound on the performance of our approach without filtering.) Our ablation studies show that abstract hypothesis generation and concrete program representations are both beneficial for LLMs to perform inductive reasoning tasks.
</details>
<details>
<summary>摘要</summary>
人类可以通过推理来解决问题，如果给他们一些示例后，他们可以找出其下面的原理，并将其应用到新的场景中。最近的研究发现，大型自然语言模型（LLM）在 inductive reasoning 任务上表现不佳，因为它们直接从示例中学习不够。在这项工作中，我们提出了使 LLM 在 inductive reasoning 任务中更好的方法，那就是通过生成多个层次抽象的假设来提高它们的 inductive reasoning 能力。我们会问 LLM 提供多个自然语言中的假设，然后将这些假设转换为 Python 程序。这些程序可以直接在观察到的示例上运行，并将其推广到新的输入。由于现状的 LLM 生成成本太高，我们考虑了一个中间步骤，即使 LLM SUMMARIZE 生成的假设中的一个子集。我们使用这种方法在 ARC 视觉 inductive reasoning benchmark、其变种 1D-ARC 和 SyGuS 串转换集上进行验证。在随机选择 ARC 中的 40 个问题上，我们的自动化管道使用 LLM 摘要可以达到 27.5% 的准确率，与直接提问基eline (准确率为 12.5%) 相比有显著提高。在人工选择 LLM 生成的候选者的情况下，准确率可以达到 37.5%。我们的抽象研究表明，生成抽象假设和转换为具体程序表示都对 LLM 进行 inductive reasoning 任务是有利的。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-for-Science-A-Study-on-P-vs-NP"><a href="#Large-Language-Model-for-Science-A-Study-on-P-vs-NP" class="headerlink" title="Large Language Model for Science: A Study on P vs. NP"></a>Large Language Model for Science: A Study on P vs. NP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05689">http://arxiv.org/abs/2309.05689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/LMOps/tree/main/LLM4Science">https://github.com/microsoft/LMOps/tree/main/LLM4Science</a></li>
<li>paper_authors: Qingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru Hao, Zhifang Sui, Furu Wei</li>
<li>for: 本研究使用大型自然语言模型（LLMs）来扩展和加速NP问题的研究，NP问题是计算机科学和数学中的一个最重要的开问题。</li>
<li>methods: 本研究提出了索洛克式思维框架，这是一种推广和加速复杂问题解决的框架，使用LLMs进行深入思考和推理。</li>
<li>results: 在p vs np问题的pilot研究中，GPT-4成功地生成了证明schema并在97次对话中进行了严格的推理，得出了”P≠NP”的结论，与(Xu和Zhou, 2023)的结论一致。<details>
<summary>Abstract</summary>
In this work, we use large language models (LLMs) to augment and accelerate research on the P versus NP problem, one of the most important open problems in theoretical computer science and mathematics. Specifically, we propose Socratic reasoning, a general framework that promotes in-depth thinking with LLMs for complex problem-solving. Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. Our pilot study on the P vs. NP problem shows that GPT-4 successfully produces a proof schema and engages in rigorous reasoning throughout 97 dialogue turns, concluding "P $\neq$ NP", which is in alignment with (Xu and Zhou, 2023). The investigation uncovers novel insights within the extensive solution space of LLMs, shedding light on LLM for Science.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们使用大语言模型（LLM）来增强和加速理论计算机科学和数学领域的研究，特别是P versus NP问题。我们提出了索кратиче思维框架，这是一种推广深思的框架，可以在复杂问题解决时使用LLM。索 kratic思维鼓励LLM在问题解决过程中进行自我评估和修充，从而促进深思。我们的试点研究发现，GPT-4成功地生成了证明schema，并在97次对话中进行了严格的思考，最终结论是P≠NP，这与(Xu和Zhou, 2023)相符。这些调查揭示了LLM在解决问题空间中的广泛新发现，为LLM在科学领域的应用提供了新的思路。
</details></li>
</ul>
<hr>
<h2 id="Combinative-Cumulative-Knowledge-Processes"><a href="#Combinative-Cumulative-Knowledge-Processes" class="headerlink" title="Combinative Cumulative Knowledge Processes"></a>Combinative Cumulative Knowledge Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05638">http://arxiv.org/abs/2309.05638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Robot-learning">https://github.com/Aryia-Behroziuan/Robot-learning</a></li>
<li>paper_authors: Anna Brandenberger, Cassandra Marcussen, Elchanan Mossel, Madhu Sudan</li>
<li>for: 本文研究了Ben-Eliezer等人（ITCS 2023）提出的累累加知过程，在“指导的无环图”（DAG）中进行了分析。在这种设定中，新的知识单元可以通过将多个先前的知识单元组合而生成。</li>
<li>methods: 本文使用了idealized和简化的“树状”设定，即新单元只依赖于一个先前生成的单元。本文的主要目标是了解当前过程是否安全，即错误的影响是否可控。</li>
<li>results: 本文提供了一些必要和 suficient conditions for safety。与先前工作一样，frequency of checking和checking depth都对安全性具有关键作用。current work中新引入的一个关键参数是“组合因子”，即新单元知识取决于多少个先前生成的单元的分布。结果表明，大 combinatin factor可以赞成深度不深的检查。该结果与combination factor之间的依赖性并不简单，有些结果表示为$\mathbb{E}{1&#x2F;M}$，而其他结果则取决于$\mathbb{E}{M}$.<details>
<summary>Abstract</summary>
We analyze Cumulative Knowledge Processes, introduced by Ben-Eliezer, Mikulincer, Mossel, and Sudan (ITCS 2023), in the setting of "directed acyclic graphs", i.e., when new units of knowledge may be derived by combining multiple previous units of knowledge. The main considerations in this model are the role of errors (when new units may be erroneous) and local checking (where a few antecedent units of knowledge are checked when a new unit of knowledge is discovered). The aforementioned work defined this model but only analyzed an idealized and simplified "tree-like" setting, i.e., a setting where new units of knowledge only depended directly on one previously generated unit of knowledge.   The main goal of our work is to understand when the general process is safe, i.e., when the effect of errors remains under control. We provide some necessary and some sufficient conditions for safety. As in the earlier work, we demonstrate that the frequency of checking as well as the depth of the checks play a crucial role in determining safety. A key new parameter in the current work is the $\textit{combination factor}$ which is the distribution of the number of units $M$ of old knowledge that a new unit of knowledge depends on. Our results indicate that a large combination factor can compensate for a small depth of checking. The dependency of the safety on the combination factor is far from trivial. Indeed some of our main results are stated in terms of $\mathbb{E}\{1/M\}$ while others depend on $\mathbb{E}\{M\}$.
</details>
<details>
<summary>摘要</summary>
我们分析了Ben-Eliezer等人（ITCS 2023）提出的累积知识过程，在“指定的无环图”（DAG）中进行了研究，即新的知识单元可以通过组合多个先前的知识单元而生成。这个模型中的主要考虑因素包括错误（新单元可能错误）以及本地检查（先前的一些知识单元被检查）。该模型在理想化和简化的“树状”设置下进行了分析，即新单元只依赖于一个先前生成的知识单元。我们的主要目标是理解这个过程是安全的，即错误的影响保持在控制之下。我们提供了一些必要和充分的条件，以确定安全性。与先前的工作相同，我们发现了检查频率以及检查深度对安全性的重要作用。我们的研究发现，一个大的组合因子可以赞成一个小的检查深度。这个参数的依赖性与安全性之间存在很多不确定性。我们的主要结果中有些是基于$\mathbb{E}\{1/M\}$的，而其他些则是基于$\mathbb{E}\{M\}$。
</details></li>
</ul>
<hr>
<h2 id="Exploration-and-Comparison-of-Deep-Learning-Architectures-to-Predict-Brain-Response-to-Realistic-Pictures"><a href="#Exploration-and-Comparison-of-Deep-Learning-Architectures-to-Predict-Brain-Response-to-Realistic-Pictures" class="headerlink" title="Exploration and Comparison of Deep Learning Architectures to Predict Brain Response to Realistic Pictures"></a>Exploration and Comparison of Deep Learning Architectures to Predict Brain Response to Realistic Pictures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09983">http://arxiv.org/abs/2309.09983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Riccardo Chimisso, Sathya Buršić, Paolo Marocco, Giuseppe Vizzari, Dimitri Ognibene</li>
<li>for: 预测大脑对实际图像的反应</li>
<li>methods: 使用不同预训练模型进行广泛实验，包括简单的模型和复杂的架构，以及使用可用数据和生成的嵌入。</li>
<li>results: 使用多个简单模型，每个模型专门预测每个脑区域的反应，以获得最佳结果，但未能建立坚固的数据相关性。<details>
<summary>Abstract</summary>
We present an exploration of machine learning architectures for predicting brain responses to realistic images on occasion of the Algonauts Challenge 2023. Our research involved extensive experimentation with various pretrained models. Initially, we employed simpler models to predict brain activity but gradually introduced more complex architectures utilizing available data and embeddings generated by large-scale pre-trained models. We encountered typical difficulties related to machine learning problems, e.g. regularization and overfitting, as well as issues specific to the challenge, such as difficulty in combining multiple input encodings, as well as the high dimensionality, unclear structure, and noisy nature of the output. To overcome these issues we tested single edge 3D position-based, multi-region of interest (ROI) and hemisphere predictor models, but we found that employing multiple simple models, each dedicated to a ROI in each hemisphere of the brain of each subject, yielded the best results - a single fully connected linear layer with image embeddings generated by CLIP as input. While we surpassed the challenge baseline, our results fell short of establishing a robust association with the data.
</details>
<details>
<summary>摘要</summary>
我们在Algonauts Challenge 2023中展示了机器学习架构的探索，用于预测真实图像下大脑的响应。我们的研究包括了许多预训练模型的实验。我们首先使用简单的模型预测大脑活动，然后逐渐引入更复杂的架构，利用可用的数据和由大规模预训练模型生成的嵌入。我们遇到了常见的机器学习问题，如常见化和过拟合，以及挑战中特有的问题，如将多个输入编码器结合起来、高维度、不确定结构和噪音性的输出。为了解决这些问题，我们测试了单边3D位置基于、多区域兴趣点（ROI）和半球预测器模型，但我们发现使用每个主 FROM 中的每个subject的每个ROI上的多个简单模型，每个模型都是一个全连接线性层，使用CLIP生成的图像嵌入，最终得到了最好的结果。虽然我们超越了基准值，但我们的结果未能建立可靠的关系于数据。
</details></li>
</ul>
<hr>
<h2 id="Memory-Injections-Correcting-Multi-Hop-Reasoning-Failures-during-Inference-in-Transformer-Based-Language-Models"><a href="#Memory-Injections-Correcting-Multi-Hop-Reasoning-Failures-during-Inference-in-Transformer-Based-Language-Models" class="headerlink" title="Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models"></a>Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05605">http://arxiv.org/abs/2309.05605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/msakarvadia/memory_injections">https://github.com/msakarvadia/memory_injections</a></li>
<li>paper_authors: Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, Ian Foster</li>
<li>for: 这篇论文目的是提高大语言模型（LLM）在多步骤理解任务中的表现。</li>
<li>methods: 该方法利用LLM的注意力头进行targeted memory injection，以帮助LLM在多步骤理解任务中包含更多有关信息。</li>
<li>results: 实验结果表明，通过在关键注意力层进行简单、有效和targeted的内存注入，可以提高LLM在多步骤任务中的表现，提高下一个需要的概率，最高提高424%。<details>
<summary>Abstract</summary>
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Introspective-Deep-Metric-Learning"><a href="#Introspective-Deep-Metric-Learning" class="headerlink" title="Introspective Deep Metric Learning"></a>Introspective Deep Metric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09982">http://arxiv.org/abs/2309.09982</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wzzheng/IDML">https://github.com/wzzheng/IDML</a></li>
<li>paper_authors: Chengkun Wang, Wenzhao Zheng, Zheng Zhu, Jie Zhou, Jiwen Lu</li>
<li>for: 提出了一个 introspective deep metric learning（IDML）框架，以解决深度度量学中的uncertainty问题。</li>
<li>methods: 提出使用semantic embedding和uncertainty embedding来描述图像的 semantics和ambiguity，并使用 introspective similarity metric进行相似性评估。</li>
<li>results: 在CUB-200-2011、Cars196和Stanford Online Products dataset上，IDML framework的性能比 conventinal deep metric learning方法更高，且可以更好地处理ambiguous images。<details>
<summary>Abstract</summary>
This paper proposes an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. Conventional deep metric learning methods focus on learning a discriminative embedding to describe the semantic features of images, which ignore the existence of uncertainty in each image resulting from noise or semantic ambiguity. Training without awareness of these uncertainties causes the model to overfit the annotated labels during training and produce unsatisfactory judgments during inference. Motivated by this, we argue that a good similarity model should consider the semantic discrepancies with awareness of the uncertainty to better deal with ambiguous images for more robust training. To achieve this, we propose to represent an image using not only a semantic embedding but also an accompanying uncertainty embedding, which describes the semantic characteristics and ambiguity of an image, respectively. We further propose an introspective similarity metric to make similarity judgments between images considering both their semantic differences and ambiguities. The gradient analysis of the proposed metric shows that it enables the model to learn at an adaptive and slower pace to deal with the uncertainty during training. The proposed IDML framework improves the performance of deep metric learning through uncertainty modeling and attains state-of-the-art results on the widely used CUB-200-2011, Cars196, and Stanford Online Products datasets for image retrieval and clustering. We further provide an in-depth analysis of our framework to demonstrate the effectiveness and reliability of IDML. Code: https://github.com/wzzheng/IDML.
</details>
<details>
<summary>摘要</summary>
To address this, we argue that a good similarity model should consider the semantic discrepancies with awareness of the uncertainty to better deal with ambiguous images for more robust training. To achieve this, we propose representing an image using not only a semantic embedding but also an accompanying uncertainty embedding, which describes the semantic characteristics and ambiguity of an image, respectively.We further propose an introspective similarity metric to make similarity judgments between images considering both their semantic differences and ambiguities. The gradient analysis of the proposed metric shows that it enables the model to learn at an adaptive and slower pace to deal with the uncertainty during training.The proposed IDML framework improves the performance of deep metric learning through uncertainty modeling and achieves state-of-the-art results on the widely used CUB-200-2011, Cars196, and Stanford Online Products datasets for image retrieval and clustering. We also provide an in-depth analysis of our framework to demonstrate its effectiveness and reliability. The code is available at https://github.com/wzzheng/IDML.
</details></li>
</ul>
<hr>
<h2 id="Temporal-Action-Localization-with-Enhanced-Instant-Discriminability"><a href="#Temporal-Action-Localization-with-Enhanced-Instant-Discriminability" class="headerlink" title="Temporal Action Localization with Enhanced Instant Discriminability"></a>Temporal Action Localization with Enhanced Instant Discriminability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05590">http://arxiv.org/abs/2309.05590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dingfengshi/tridetplus">https://github.com/dingfengshi/tridetplus</a></li>
<li>paper_authors: Dingfeng Shi, Qiong Cao, Yujie Zhong, Shan An, Jian Cheng, Haogang Zhu, Dacheng Tao</li>
<li>for: 这篇论文目的是提出一种一阶段框架TriDet，用于检测视频中的动作边界和其相应的类别。</li>
<li>methods: 这篇论文使用了Trident-head模型动作边界，并提出了一种高效的粒度层（SGP层）来解决转换器基于方法中的排名损失问题。同时，它还利用预训练的大型模型来提高视频背景的表示能力。</li>
<li>results: 实验结果表明TriDet具有了高效性和状态最佳的表现在多个动作检测 datasets 上，包括层次（多个标签）动作检测 datasets。<details>
<summary>Abstract</summary>
Temporal action detection (TAD) aims to detect all action boundaries and their corresponding categories in an untrimmed video. The unclear boundaries of actions in videos often result in imprecise predictions of action boundaries by existing methods. To resolve this issue, we propose a one-stage framework named TriDet. First, we propose a Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. Then, we analyze the rank-loss problem (i.e. instant discriminability deterioration) in transformer-based methods and propose an efficient scalable-granularity perception (SGP) layer to mitigate this issue. To further push the limit of instant discriminability in the video backbone, we leverage the strong representation capability of pretrained large models and investigate their performance on TAD. Last, considering the adequate spatial-temporal context for classification, we design a decoupled feature pyramid network with separate feature pyramids to incorporate rich spatial context from the large model for localization. Experimental results demonstrate the robustness of TriDet and its state-of-the-art performance on multiple TAD datasets, including hierarchical (multilabel) TAD datasets.
</details>
<details>
<summary>摘要</summary>
Temporal action detection (TAD) targets to detect all action boundaries and their corresponding categories in an untrimmed video. The unclear boundaries of actions in videos often lead to imprecise predictions of action boundaries by existing methods. To resolve this issue, we propose a one-stage framework named TriDet. First, we propose a Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. Then, we analyze the rank-loss problem (i.e. instant discriminability deterioration) in transformer-based methods and propose an efficient scalable-granularity perception (SGP) layer to mitigate this issue. To further push the limit of instant discriminability in the video backbone, we leverage the strong representation capability of pretrained large models and investigate their performance on TAD. Last, considering the adequate spatial-temporal context for classification, we design a decoupled feature pyramid network with separate feature pyramids to incorporate rich spatial context from the large model for localization. Experimental results demonstrate the robustness of TriDet and its state-of-the-art performance on multiple TAD datasets, including hierarchical (multilabel) TAD datasets.Here's the word-for-word translation of the text into Simplified Chinese: temporal action detection (TAD) targets to detect all action boundaries and their corresponding categories in an untrimmed video. The unclear boundaries of actions in videos often lead to imprecise predictions of action boundaries by existing methods. To resolve this issue, we propose a one-stage framework named TriDet. First, we propose a Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. Then, we analyze the rank-loss problem (i.e. instant discriminability deterioration) in transformer-based methods and propose an efficient scalable-granularity perception (SGP) layer to mitigate this issue. To further push the limit of instant discriminability in the video backbone, we leverage the strong representation capability of pretrained large models and investigate their performance on TAD. Last, considering the adequate spatial-temporal context for classification, we design a decoupled feature pyramid network with separate feature pyramids to incorporate rich spatial context from the large model for localization. Experimental results demonstrate the robustness of TriDet and its state-of-the-art performance on multiple TAD datasets, including hierarchical (multilabel) TAD datasets.
</details></li>
</ul>
<hr>
<h2 id="Mind-the-Uncertainty-Risk-Aware-and-Actively-Exploring-Model-Based-Reinforcement-Learning"><a href="#Mind-the-Uncertainty-Risk-Aware-and-Actively-Exploring-Model-Based-Reinforcement-Learning" class="headerlink" title="Mind the Uncertainty: Risk-Aware and Actively Exploring Model-Based Reinforcement Learning"></a>Mind the Uncertainty: Risk-Aware and Actively Exploring Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05582">http://arxiv.org/abs/2309.05582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marin Vlastelica, Sebastian Blaes, Cristina Pineri, Georg Martius</li>
<li>for: 这篇论文是为了解决基于模型的再征询学习中的风险管理问题，使用轨迹采样和概率安全约束，并平衡optimism和pessimism两种不确定性。</li>
<li>methods: 本论文使用了一种简单 yet effective的方法，即在基于模型的再征询学习中分离不确定性，并使用概率安全约束和轨迹采样来管理风险。</li>
<li>results: 各种实验表明，在数据驱动的MPC方法中，分离不确定性是关键 для在不确定和安全控制环境中表现良好。<details>
<summary>Abstract</summary>
We introduce a simple but effective method for managing risk in model-based reinforcement learning with trajectory sampling that involves probabilistic safety constraints and balancing of optimism in the face of epistemic uncertainty and pessimism in the face of aleatoric uncertainty of an ensemble of stochastic neural networks.Various experiments indicate that the separation of uncertainties is essential to performing well with data-driven MPC approaches in uncertain and safety-critical control environments.
</details>
<details>
<summary>摘要</summary>
我们介绍一种简单 yet有效的方法来管理模型基于强化学习中的风险，这个方法包括机会不确定性和抽象不确定性之间的分类，并在这些不确定性下寻求平衡。我们通过实验发现，在不确定和安全控制环境中，分类不确定性是管理风险的关键。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="ITI-GEN-Inclusive-Text-to-Image-Generation"><a href="#ITI-GEN-Inclusive-Text-to-Image-Generation" class="headerlink" title="ITI-GEN: Inclusive Text-to-Image Generation"></a>ITI-GEN: Inclusive Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05569">http://arxiv.org/abs/2309.05569</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/humansensinglab/ITI-GEN">https://github.com/humansensinglab/ITI-GEN</a></li>
<li>paper_authors: Cheng Zhang, Xuanbai Chen, Siqi Chai, Chen Henry Wu, Dmitry Lagun, Thabo Beeler, Fernando De la Torre<br>for:This paper aims to address the issue of unequal representations of underrepresented groups in text-to-image generative models by proposing a novel approach called ITI-GEN.methods:ITI-GEN leverages readily available reference images to learn prompt embeddings that can generate inclusive images from human-written prompts. The approach does not require model fine-tuning, making it computationally efficient.results:Extensive experiments demonstrate that ITI-GEN largely improves over state-of-the-art models in generating inclusive images from prompts, ensuring that all desired attribute categories are represented uniformly.Here is the Chinese version of the three key points:for:这篇论文的目的是解决文本到图像生成模型中的弱化群体表示问题，提出了一种名为ITI-GEN的新方法。methods:ITI-GEN利用可以获得的参考图像来学习提示 embedding，从人写的提示中生成包括所有感兴趣的属性类别的包容图像。该方法不需要模型练习，可以快速进行计算效率。results:广泛的实验表明，ITI-GEN在基于提示生成图像方面大幅超越了现有模型， Ensure that all desired attribute categories are represented uniformly.<details>
<summary>Abstract</summary>
Text-to-image generative models often reflect the biases of the training data, leading to unequal representations of underrepresented groups. This study investigates inclusive text-to-image generative models that generate images based on human-written prompts and ensure the resulting images are uniformly distributed across attributes of interest. Unfortunately, directly expressing the desired attributes in the prompt often leads to sub-optimal results due to linguistic ambiguity or model misrepresentation. Hence, this paper proposes a drastically different approach that adheres to the maxim that "a picture is worth a thousand words". We show that, for some attributes, images can represent concepts more expressively than text. For instance, categories of skin tones are typically hard to specify by text but can be easily represented by example images. Building upon these insights, we propose a novel approach, ITI-GEN, that leverages readily available reference images for Inclusive Text-to-Image GENeration. The key idea is learning a set of prompt embeddings to generate images that can effectively represent all desired attribute categories. More importantly, ITI-GEN requires no model fine-tuning, making it computationally efficient to augment existing text-to-image models. Extensive experiments demonstrate that ITI-GEN largely improves over state-of-the-art models to generate inclusive images from a prompt. Project page: https://czhang0528.github.io/iti-gen.
</details>
<details>
<summary>摘要</summary>
文本到图像生成模型经常表现出训练数据的偏见，导致特定群体的不平等表达。这项研究探讨了包容型文本到图像生成模型，该模型根据人写的提示生成图像，并确保生成图像具有所有Attributes of interest的均匀分布。然而，直接表达愿景中的属性在提示中经常会导致优化不佳的结果，因为语言 ambiguity 或模型误 repreSentation。因此，这篇论文提出了一种极其不同的方法，即通过“一 picture is worth a thousand words”的maxim，我们表明，对于一些属性，图像可以更加表达Concepts than text。例如，皮肤色Category 通常由文本很难Specify，但可以通过示例图像轻松表达。基于这些意识，我们提出了一种新的方法，名为 ITI-GEN，该方法利用可以 obtAin的参考图像来实现包容型文本到图像生成。关键思想是学习一组提示Embeddings，以生成能够有效表示所有愿景Category的图像。更重要的是，ITI-GEN不需要模型细化，因此可以 computationally efficient 地增强现有的文本到图像模型。广泛的实验表明，ITI-GEN较State-of-the-art模型大幅提高了从提示生成包容图像的能力。项目页面：https://czhang0528.github.io/iti-gen。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-NetOps-Capability-of-Pre-Trained-Large-Language-Models"><a href="#An-Empirical-Study-of-NetOps-Capability-of-Pre-Trained-Large-Language-Models" class="headerlink" title="An Empirical Study of NetOps Capability of Pre-Trained Large Language Models"></a>An Empirical Study of NetOps Capability of Pre-Trained Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05557">http://arxiv.org/abs/2309.05557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukai Miao, Yu Bai, Li Chen, Dan Li, Haifeng Sun, Xizheng Wang, Ziqiu Luo, Yanyu Ren, Dapeng Sun, Xiuting Xu, Qi Zhang, Chao Xiang, Xinchi Li</li>
<li>For: The paper is written for evaluating the comprehensive capabilities of Pre-trained Large Language Models (LLMs) in Network Operations (NetOps) and measuring their performance in a multi-lingual context.* Methods: The paper presents an evaluation set called NetEval, which consists of 5,732 questions about NetOps covering five different sub-domains. The authors systematically evaluate the NetOps capability of 26 publicly available LLMs using NetEval.* Results: The results show that only GPT-4 can achieve a performance competitive to humans in NetOps, while some open models like LLaMA 2 demonstrate significant potential.Here are the three information points in Simplified Chinese text:* For: 这篇论文是为了评估大量语言模型（LLMs）在网络操作（NetOps）中的总体能力，以及在多语言 context中进行评估。* Methods: 论文提出了 NetEval 评估集，包含5,732个网络操作问题，涵盖了五个不同的子领域。作者使用 NetEval 系统性地评估了26个公开available LLMs 的网络操作能力。* Results: 结果显示，只有 GPT-4 能够与人类水平的性能，而一些开源模型如 LLaMA 2 表现出了 significativ potential。<details>
<summary>Abstract</summary>
Nowadays, the versatile capabilities of Pre-trained Large Language Models (LLMs) have attracted much attention from the industry. However, some vertical domains are more interested in the in-domain capabilities of LLMs. For the Networks domain, we present NetEval, an evaluation set for measuring the comprehensive capabilities of LLMs in Network Operations (NetOps). NetEval is designed for evaluating the commonsense knowledge and inference ability in NetOps in a multi-lingual context. NetEval consists of 5,732 questions about NetOps, covering five different sub-domains of NetOps. With NetEval, we systematically evaluate the NetOps capability of 26 publicly available LLMs. The results show that only GPT-4 can achieve a performance competitive to humans. However, some open models like LLaMA 2 demonstrate significant potential.
</details>
<details>
<summary>摘要</summary>
Note:* 预训练大语言模型 (LLMs) is translated as "预训练大语言模型" in Simplified Chinese.* Network Operations (NetOps) is translated as "网络运维" in Simplified Chinese.* GPT-4 is translated as "GPT-4" in Simplified Chinese.* LLaMA 2 is translated as "LLaMA 2" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Hybrid-ASR-for-Resource-Constrained-Robots-HMM-Deep-Learning-Fusion"><a href="#Hybrid-ASR-for-Resource-Constrained-Robots-HMM-Deep-Learning-Fusion" class="headerlink" title="Hybrid ASR for Resource-Constrained Robots: HMM - Deep Learning Fusion"></a>Hybrid ASR for Resource-Constrained Robots: HMM - Deep Learning Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07164">http://arxiv.org/abs/2309.07164</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anshulranjan2004/pyhmm">https://github.com/anshulranjan2004/pyhmm</a></li>
<li>paper_authors: Anshul Ranjan, Kaushik Jegadeesan</li>
<li>for: 这个研究是为了开发一个资源有限的机器人领域中的自动语音识别系统（ASR）。</li>
<li>methods: 这个方法结合隐藏马克夫模型（HMM）和深度学习模型，并通过 Socket 程式设计来分配处理任务，以提高语音识别精度。</li>
<li>results: 实验结果显示，这个混合式 ASR 系统在不同的机器人平台上展现出了实时和精准的语音识别能力，并且具有适应不同音律环境和低功耗硬件的能力。<details>
<summary>Abstract</summary>
This paper presents a novel hybrid Automatic Speech Recognition (ASR) system designed specifically for resource-constrained robots. The proposed approach combines Hidden Markov Models (HMMs) with deep learning models and leverages socket programming to distribute processing tasks effectively. In this architecture, the HMM-based processing takes place within the robot, while a separate PC handles the deep learning model. This synergy between HMMs and deep learning enhances speech recognition accuracy significantly. We conducted experiments across various robotic platforms, demonstrating real-time and precise speech recognition capabilities. Notably, the system exhibits adaptability to changing acoustic conditions and compatibility with low-power hardware, making it highly effective in environments with limited computational resources. This hybrid ASR paradigm opens up promising possibilities for seamless human-robot interaction. In conclusion, our research introduces a pioneering dimension to ASR techniques tailored for robotics. By employing socket programming to distribute processing tasks across distinct devices and strategically combining HMMs with deep learning models, our hybrid ASR system showcases its potential to enable robots to comprehend and respond to spoken language adeptly, even in environments with restricted computational resources. This paradigm sets a innovative course for enhancing human-robot interaction across a wide range of real-world scenarios.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了一种新的混合自动语音识别（ASR）系统，特制为有限的机器人资源。该系统结合隐藏马尔可夫模型（HMM）和深度学习模型，通过socket编程分布处理任务，以提高语音识别精度。在这个架构中，HMM基于的处理在机器人内部进行，而深度学习模型则由 separte的PC处理。这种 hybrid ASR 模型在不同的 роботиче平台上进行实验，展现了实时和精准的语音识别能力。尤其是在受到不同的音响环境影响时，该系统能够适应变化，并且与低功耗硬件相容，使其在有限的计算资源环境中表现出色。这种 hybrid ASR 模型开启了人机合作的新可能，使机器人能够通过语音理解和回应，与人类进行无缝交互，以实现各种真实世界的应用场景。
</details></li>
</ul>
<hr>
<h2 id="Kani-A-Lightweight-and-Highly-Hackable-Framework-for-Building-Language-Model-Applications"><a href="#Kani-A-Lightweight-and-Highly-Hackable-Framework-for-Building-Language-Model-Applications" class="headerlink" title="Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications"></a>Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05542">http://arxiv.org/abs/2309.05542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhudotexe/kani">https://github.com/zhudotexe/kani</a></li>
<li>paper_authors: Andrew Zhu, Liam Dugan, Alyssa Hwang, Chris Callison-Burch</li>
<li>for: 这篇论文是为了提供一个轻量级、灵活、无关模型的开源框架，用于构建语音模型应用程序。</li>
<li>methods: 论文使用了模型接口、聊天管理和强大函数调用等核心构建块来支持复杂的功能实现。所有核心函数都可以轻松地被 override，并且都具有丰富的文档，以便开发者根据自己的需求进行自定义。</li>
<li>results: 论文通过提供一个轻量级、灵活的开源框架，帮助开发者快速实现复杂的语音模型应用程序，同时保持了可重复性和细化控制。<details>
<summary>Abstract</summary>
Language model applications are becoming increasingly popular and complex, often including features like tool usage and retrieval augmentation. However, existing frameworks for such applications are often opinionated, deciding for developers how their prompts ought to be formatted and imposing limitations on customizability and reproducibility. To solve this we present Kani: a lightweight, flexible, and model-agnostic open-source framework for building language model applications. Kani helps developers implement a variety of complex features by supporting the core building blocks of chat interaction: model interfacing, chat management, and robust function calling. All Kani core functions are easily overridable and well documented to empower developers to customize functionality for their own needs. Kani thus serves as a useful tool for researchers, hobbyists, and industry professionals alike to accelerate their development while retaining interoperability and fine-grained control.
</details>
<details>
<summary>摘要</summary>
language model 应用程序在现在越来越受欢迎和复杂，通常包括工具使用和检索增强功能。然而，现有的框架经常强制性地决定开发者如何格式化他们的提示，并强制限制自定义和重现性。为解决这个问题，我们提出了 Kani：一个轻量级、灵活、无关模型的开源框架，用于构建语音模型应用程序。Kani 帮助开发者实现许多复杂的功能，通过支持语音交互的核心构建块：模型接口、聊天管理和强大的函数调用。Kani 的核心函数都可以轻松地被覆盖，并且所有函数都具有详细的文档，以便开发者可以根据自己的需求自定义功能。因此，Kani 成为了研究人员、爱好者和行业专业人员都可以使用的有用工具，以加速开发，保持兼容性和细化控制。
</details></li>
</ul>
<hr>
<h2 id="PAI-Diffusion-Constructing-and-Serving-a-Family-of-Open-Chinese-Diffusion-Models-for-Text-to-image-Synthesis-on-the-Cloud"><a href="#PAI-Diffusion-Constructing-and-Serving-a-Family-of-Open-Chinese-Diffusion-Models-for-Text-to-image-Synthesis-on-the-Cloud" class="headerlink" title="PAI-Diffusion: Constructing and Serving a Family of Open Chinese Diffusion Models for Text-to-image Synthesis on the Cloud"></a>PAI-Diffusion: Constructing and Serving a Family of Open Chinese Diffusion Models for Text-to-image Synthesis on the Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05534">http://arxiv.org/abs/2309.05534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyu Wang, Zhongjie Duan, Bingyan Liu, Xinyi Zou, Cen Chen, Kui Jia, Jun Huang</li>
<li>For: 本文旨在提出一个涵盖整体和域pecific Chinese diffusion模型的完整框架，以生成contextually relevant的图像。* Methods: 本文使用了普遍的Diffusion模型，并结合了域pecific的中文Diffusion模型，以及LoRA和ControlNet来实现细化的图像风格传输和图像编辑。* Results: 本文通过评估多个Benchmark Tasks和实际应用场景，证明了PAI-Diffusion框架在生成contextually relevant的图像方面具有了优秀的表现。<details>
<summary>Abstract</summary>
Text-to-image synthesis for the Chinese language poses unique challenges due to its large vocabulary size, and intricate character relationships. While existing diffusion models have shown promise in generating images from textual descriptions, they often neglect domain-specific contexts and lack robustness in handling the Chinese language. This paper introduces PAI-Diffusion, a comprehensive framework that addresses these limitations. PAI-Diffusion incorporates both general and domain-specific Chinese diffusion models, enabling the generation of contextually relevant images. It explores the potential of using LoRA and ControlNet for fine-grained image style transfer and image editing, empowering users with enhanced control over image generation. Moreover, PAI-Diffusion seamlessly integrates with Alibaba Cloud's Machine Learning Platform for AI, providing accessible and scalable solutions. All the Chinese diffusion model checkpoints, LoRAs, and ControlNets, including domain-specific ones, are publicly available. A user-friendly Chinese WebUI and the diffusers-api elastic inference toolkit, also open-sourced, further facilitate the easy deployment of PAI-Diffusion models in various environments, making it a valuable resource for Chinese text-to-image synthesis.
</details>
<details>
<summary>摘要</summary>
文本到图像生成 для中文语言具有独特的挑战，主要包括语言大词汇和汉字间复杂的关系。现有的扩散模型已经显示出生成图像from文本描述的潜力，但它们经常忽视特定领域上下文和中文语言的特点。本文提出PAI-Diffusion框架，解决这些局限性。PAI-Diffusion结合了通用和域专的中文扩散模型，使得可以生成上下文相关的图像。它还 explore了使用LoRA和ControlNet进行细化的图像风格传递和图像编辑，让用户对图像生成具有更多的控制权。此外，PAI-Diffusion与阿里巴巴云计算机机器学习平台的AI集成了可靠和扩展的解决方案。所有的中文扩散模型检查点、LoRAs和ControlNets，包括域专的，都是公共可用的。此外，用户友好的中文WebUI和diffusers-api可以方便地在不同环境中部署PAI-Diffusion模型，使其成为中文文本到图像生成的有价值资源。
</details></li>
</ul>
<hr>
<h2 id="On-the-meaning-of-uncertainty-for-ethical-AI-philosophy-and-practice"><a href="#On-the-meaning-of-uncertainty-for-ethical-AI-philosophy-and-practice" class="headerlink" title="On the meaning of uncertainty for ethical AI: philosophy and practice"></a>On the meaning of uncertainty for ethical AI: philosophy and practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05529">http://arxiv.org/abs/2309.05529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cassandra Bird, Daniel Williamson, Sabina Leonelli</li>
<li>for: 该论文的目的是如何增加人工智能系统的透明度和负责任性，以便更好地回应用户的反馈和评估。</li>
<li>methods: 该论文提出了一种解决方案，通过明确指出人工智能系统的开发基础和应用领域的限制，来增强模型的响应性、输出的质量和意义、以及对模型的评估透明度。</li>
<li>results: 该论文通过扩展后验投入评估来实现信念拥有，并 argueed that这是一种将伦理考虑入数学逻辑中的重要方法，以及实现伦理AI在统计实践中的实现。 在COVID-19Omicron变种的扩散问题上，该论文提供了一个实践例子。<details>
<summary>Abstract</summary>
Whether and how data scientists, statisticians and modellers should be accountable for the AI systems they develop remains a controversial and highly debated topic, especially given the complexity of AI systems and the difficulties in comparing and synthesising competing claims arising from their deployment for data analysis. This paper proposes to address this issue by decreasing the opacity and heightening the accountability of decision making using AI systems, through the explicit acknowledgement of the statistical foundations that underpin their development and the ways in which these dictate how their results should be interpreted and acted upon by users. In turn, this enhances (1) the responsiveness of the models to feedback, (2) the quality and meaning of uncertainty on their outputs and (3) their transparency to evaluation. To exemplify this approach, we extend Posterior Belief Assessment to offer a route to belief ownership from complex and competing AI structures. We argue that this is a significant way to bring ethical considerations into mathematical reasoning, and to implement ethical AI in statistical practice. We demonstrate these ideas within the context of competing models used to advise the UK government on the spread of the Omicron variant of COVID-19 during December 2021.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:whether 和 how 数据科学家、统计学家和模型构建者应该被负责做AI系统的问题是一个争议和高度讨论的话题，尤其是由于AI系统的复杂性和其部署用于数据分析时的比较和结合的困难。这篇论文提议通过降低透明度和提高决策使用AI系统的负责任，通过明确AI系统的发展基础，并让用户理解和 acted upon 其结果的方式。这有助于 (1) 提高模型的反馈responsiveness， (2) 提高输出的不确定性质量和意义， (3) 提高评估透明度。为了证明这一方法，我们将 posterior belief assessment 扩展到提供对复杂和竞争性AI结构的信念所有权的路径。我们认为这是一种将伦理考虑进数学逻辑中的重要方法，并将伦理AI应用于统计实践中。我们在2021年12月 UK 政府对奥米克隆变种COVID-19 的扩散提供了示例。
</details></li>
</ul>
<hr>
<h2 id="NExT-GPT-Any-to-Any-Multimodal-LLM"><a href="#NExT-GPT-Any-to-Any-Multimodal-LLM" class="headerlink" title="NExT-GPT: Any-to-Any Multimodal LLM"></a>NExT-GPT: Any-to-Any Multimodal LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05519">http://arxiv.org/abs/2309.05519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NExT-GPT/NExT-GPT">https://github.com/NExT-GPT/NExT-GPT</a></li>
<li>paper_authors: Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua</li>
<li>for: 这 paper 的目的是开发一个可以处理多种模式的大型语言模型（MM-LLM）系统，以便模拟人类在多种感知和交流中的行为。</li>
<li>methods: 这 paper 使用了一种综合拓展的结构，将语言模型（LLM）与多模态适配器和不同的扩散解码器相连接，以便接受和生成多种模式的输入和输出。此外，paper 还引入了一种模式转换指令调整（MosIT），并 manually 精心编辑了一个高质量的多模式数据集，以便让 NExT-GPT 具备跨模式的semantic理解和内容生成能力。</li>
<li>results: 经过训练，NExT-GPT 能够在多种模式下进行输入和输出转换，并且在不同的模式下能够具备较高的内容生成和理解能力。此外，paper 还证明了 NExT-GPT 的模式转换能力可以在不同的任务上进行改进，例如图像描述和文本生成等。<details>
<summary>Abstract</summary>
While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/
</details>
<details>
<summary>摘要</summary>
Recently, Multimodal Large Language Models (MM-LLMs) have made significant progress, but they are limited to only understanding input-side multimodality and cannot produce content in multiple modalities. As humans perceive the world and communicate with others through various modalities, developing any-to-any MM-LLMs that can accept and deliver content in any modality is essential for human-level AI. To address this gap, we propose an end-to-end general-purpose any-to-any MM-LLM system called NExT-GPT.We connect an LLM with multimodal adaptors and different diffusion decoders, allowing NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging existing well-trained and highly-performing encoders and decoders, NExT-GPT is trained with only a small amount of parameters (1% of certain projection layers), which not only reduces training costs but also facilitates the addition of more potential modalities.Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, which enables NExT-GPT to understand complex cross-modal semantics and generate content in various modalities. Our research demonstrates the possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.Project page: <https://next-gpt.github.io/>
</details></li>
</ul>
<hr>
<h2 id="Optimize-Weight-Rounding-via-Signed-Gradient-Descent-for-the-Quantization-of-LLMs"><a href="#Optimize-Weight-Rounding-via-Signed-Gradient-Descent-for-the-Quantization-of-LLMs" class="headerlink" title="Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs"></a>Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05516">http://arxiv.org/abs/2309.05516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/intel/neural-compressor">https://github.com/intel/neural-compressor</a></li>
<li>paper_authors: Wenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao Lv</li>
<li>For: The paper aims to optimize the weight rounding task for weight-only quantization in large language models to improve their deployment efficiency while maintaining accuracy.* Methods: The proposed method, SignRound, uses lightweight block-wise tuning with signed gradient descent to optimize the weight rounding task, which achieves outstanding results within 400 steps.* Results: SignRound outperforms the established baseline of rounding-to-nearest (RTN) and competes impressively against recent methods, without introducing additional inference overhead.Here’s the same information in Simplified Chinese text:</li>
<li>for: 论文目的是优化大语言模型中的weight-only quantization，以提高其部署效率while maintaining accuracy.</li>
<li>methods: 提议的方法是SignRound，它使用轻量级块 wise tuning和签名Gradient Descent来优化weight rounding任务，可以在400步内 дости得出色的结果。</li>
<li>results: SignRound比RTN基线和最近的方法更强，而无需添加更多的推理过程 overhead.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gradient descent, enabling us to achieve outstanding results within 400 steps. SignRound outperforms the established baseline of rounding-to-nearest (RTN) and competes impressively against recent methods, without introducing additional inference overhead. The source code will be publicly available at https://github.com/intel/neural-compressor soon.
</details>
<details>
<summary>摘要</summary>
To optimize the weight rounding task, we propose a concise and highly effective approach named SignRound. Our method uses lightweight block-wise tuning with signed gradient descent, achieving outstanding results within 400 steps. SignRound outperforms the established baseline of rounding-to-nearest (RTN) and competes impressively against recent methods without introducing additional inference overhead. The source code will be publicly available at https://github.com/intel/neural-compressor soon.
</details></li>
</ul>
<hr>
<h2 id="A-Co-design-Study-for-Multi-Stakeholder-Job-Recommender-System-Explanations"><a href="#A-Co-design-Study-for-Multi-Stakeholder-Job-Recommender-System-Explanations" class="headerlink" title="A Co-design Study for Multi-Stakeholder Job Recommender System Explanations"></a>A Co-design Study for Multi-Stakeholder Job Recommender System Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05507">http://arxiv.org/abs/2309.05507</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/roan-schellingerhout/jrs_explanations">https://github.com/roan-schellingerhout/jrs_explanations</a></li>
<li>paper_authors: Roan Schellingerhout, Francesco Barile, Nava Tintarev<br>for:The paper aims to determine the explanation preferences of different stakeholder types in the recruitment process, specifically candidates, recruiters, and companies.methods:The authors created a semi-structured interview guide and used grounded theory to analyze the results, finding that each stakeholder type has distinct explanation preferences.results:The study found that candidates prefer brief, textual explanations, while hiring managers prefer visual graph-based explanations, and recruiters prefer more exhaustive textual explanations. Based on these findings, the authors provide guidelines for designing an explanation interface that meets the needs of all three stakeholder types. Additionally, the validated interview guide can be used in future research to determine explanation preferences for different stakeholder types in other domains.Here is the same information in Simplified Chinese text:for:论文目的是确定各种参与者类型在招聘过程中的解释需求，具体来说是候选人、招聘人员和公司。methods:作者们创建了一份 semi-structured 采访指南，并使用基本理论来分析结果，发现每个参与者类型都有不同的解释需求。results:研究发现，候选人喜欢简短的文本解释，而招聘人员偏好图形基于的解释，而招聘人员则更喜欢详细的文本解释。根据这些发现，作者们提出了设计解释界面的指南，以满足所有参与者类型的需求。此外，采访指南也可以在未来研究中用于确定不同参与者类型的解释需求。<details>
<summary>Abstract</summary>
Recent legislation proposals have significantly increased the demand for eXplainable Artificial Intelligence (XAI) in many businesses, especially in so-called `high-risk' domains, such as recruitment. Within recruitment, AI has become commonplace, mainly in the form of job recommender systems (JRSs), which try to match candidates to vacancies, and vice versa. However, common XAI techniques often fall short in this domain due to the different levels and types of expertise of the individuals involved, making explanations difficult to generalize. To determine the explanation preferences of the different stakeholder types - candidates, recruiters, and companies - we created and validated a semi-structured interview guide. Using grounded theory, we structurally analyzed the results of these interviews and found that different stakeholder types indeed have strongly differing explanation preferences. Candidates indicated a preference for brief, textual explanations that allow them to quickly judge potential matches. On the other hand, hiring managers preferred visual graph-based explanations that provide a more technical and comprehensive overview at a glance. Recruiters found more exhaustive textual explanations preferable, as those provided them with more talking points to convince both parties of the match. Based on these findings, we describe guidelines on how to design an explanation interface that fulfills the requirements of all three stakeholder types. Furthermore, we provide the validated interview guide, which can assist future research in determining the explanation preferences of different stakeholder types.
</details>
<details>
<summary>摘要</summary>
最近的法规提案已经提高了高风险领域内的可解释人工智能（XAI）的需求，特别是在招聘领域。在招聘领域，人工智能已经广泛应用，主要是在 forme of job recommender systems（JRSs），用于匹配候选人和职位。然而，常见的XAI技术经常在这个领域下功不逮，因为不同的个人拥有不同水平和类型的专业知识，使得解释困难于总结。为了确定不同参与者类型（候选人、招聘人员和公司）的解释偏好，我们创建了和验证了一份 semi-structured 采访指南。使用基本的理论，我们结构分析了采访结果，并发现不同参与者类型确实有强烈不同的解释偏好。候选人表示偏好简洁的文本解释，让他们快速判断可能的匹配。相反，招聘人员偏好可见图表解释，提供技术性和全面的概述。招聘人员则更喜欢详细的文本解释，使得他们有更多的讲话点，以convince both parties of the match。基于这些发现，我们描述了如何设计一个满足所有参与者类型的解释界面的指南。此外，我们还提供了验证过的采访指南，可以帮助未来的研究确定不同参与者类型的解释偏好。
</details></li>
</ul>
<hr>
<h2 id="Black-Box-Analysis-GPTs-Across-Time-in-Legal-Textual-Entailment-Task"><a href="#Black-Box-Analysis-GPTs-Across-Time-in-Legal-Textual-Entailment-Task" class="headerlink" title="Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task"></a>Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05501">http://arxiv.org/abs/2309.05501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ha-Thanh Nguyen, Randy Goebel, Francesca Toni, Kostas Stathis, Ken Satoh</li>
<li>for: This study aims to evaluate the performance of GPT-3.5 and GPT-4 on a prominent benchmark for legal textual entailment, the COLIEE Task 4 dataset, and to analyze their strengths and weaknesses in handling legal textual entailment tasks.</li>
<li>methods: The study uses black-box analysis to evaluate the performance of GPT-3.5 and GPT-4 on the COLIEE Task 4 dataset, which includes legal texts from different periods in Japan.</li>
<li>results: The preliminary experimental results show intriguing insights into the models’ performance on the legal textual entailment tasks, including their ability to discern entailment relationships within Japanese statute law across different periods. The study also discusses the influence of training data distribution on the models’ generalizability.<details>
<summary>Abstract</summary>
The evolution of Generative Pre-trained Transformer (GPT) models has led to significant advancements in various natural language processing applications, particularly in legal textual entailment. We present an analysis of GPT-3.5 (ChatGPT) and GPT-4 performances on COLIEE Task 4 dataset, a prominent benchmark in this domain. The study encompasses data from Heisei 18 (2006) to Reiwa 3 (2021), exploring the models' abilities to discern entailment relationships within Japanese statute law across different periods. Our preliminary experimental results unveil intriguing insights into the models' strengths and weaknesses in handling legal textual entailment tasks, as well as the patterns observed in model performance. In the context of proprietary models with undisclosed architectures and weights, black-box analysis becomes crucial for evaluating their capabilities. We discuss the influence of training data distribution and the implications on the models' generalizability. This analysis serves as a foundation for future research, aiming to optimize GPT-based models and enable their successful adoption in legal information extraction and entailment applications.
</details>
<details>
<summary>摘要</summary>
“GPT模型的演化has led to significant advancements in various natural language processing applications, particularly in legal textual entailment. We present an analysis of GPT-3.5（ChatGPT）and GPT-4 performances on COLIEE Task 4 dataset, a prominent benchmark in this domain. The study encompasses data from Heisei 18（2006）to Reiwa 3（2021）, exploring the models' abilities to discern entailment relationships within Japanese statute law across different periods. Our preliminary experimental results unveil intriguing insights into the models' strengths and weaknesses in handling legal textual entailment tasks, as well as the patterns observed in model performance. In the context of proprietary models with undisclosed architectures and weights, black-box analysis becomes crucial for evaluating their capabilities. We discuss the influence of training data distribution and the implications on the models' generalizability. This analysis serves as a foundation for future research, aiming to optimize GPT-based models and enable their successful adoption in legal information extraction and entailment applications.”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other countries.
</details></li>
</ul>
<hr>
<h2 id="NeCo-ALQAC-2023-Legal-Domain-Knowledge-Acquisition-for-Low-Resource-Languages-through-Data-Enrichment"><a href="#NeCo-ALQAC-2023-Legal-Domain-Knowledge-Acquisition-for-Low-Resource-Languages-through-Data-Enrichment" class="headerlink" title="NeCo@ALQAC 2023: Legal Domain Knowledge Acquisition for Low-Resource Languages through Data Enrichment"></a>NeCo@ALQAC 2023: Legal Domain Knowledge Acquisition for Low-Resource Languages through Data Enrichment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05500">http://arxiv.org/abs/2309.05500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai-Long Nguyen, Dieu-Quynh Nguyen, Hoang-Trung Nguyen, Thu-Trang Pham, Huu-Dong Nguyen, Thach-Anh Nguyen, Thi-Hai-Yen Vuong, Ha-Thanh Nguyen</li>
<li>for: 本研究主要针对于自然语言处理在法律领域中的应用，尤其是对于低资源语言的法律领域知识获取。</li>
<li>methods: 本文使用了相似排名和深度学习模型来解决法律文档检索任务，而对于第二个任务，即从相关法律文章中提取问题回答，我们提议了一系列适应性技术来处理不同的问题类型。</li>
<li>results: 本文在两个任务上取得了出色的成绩，示出自动问答系统在法律领域，特别是低资源语言中的潜在利好和效果。<details>
<summary>Abstract</summary>
In recent years, natural language processing has gained significant popularity in various sectors, including the legal domain. This paper presents NeCo Team's solutions to the Vietnamese text processing tasks provided in the Automated Legal Question Answering Competition 2023 (ALQAC 2023), focusing on legal domain knowledge acquisition for low-resource languages through data enrichment. Our methods for the legal document retrieval task employ a combination of similarity ranking and deep learning models, while for the second task, which requires extracting an answer from a relevant legal article in response to a question, we propose a range of adaptive techniques to handle different question types. Our approaches achieve outstanding results on both tasks of the competition, demonstrating the potential benefits and effectiveness of question answering systems in the legal field, particularly for low-resource languages.
</details>
<details>
<summary>摘要</summary>
Recently, 自然语言处理技术在不同领域获得了广泛的应用，其中包括法律领域。这篇论文介绍了NeCo Team在2023年自动法律问答比赛（ALQAC 2023）中提供的越南文本处理任务解决方案，强调了对低资源语言的法律领域知识取得的数据增强。我们对法律文档检索任务使用了相似排名和深度学习模型，而对第二个任务，即根据问题提取相关法律文章中的答案，我们提议了一系列适应性技巧来处理不同的问题类型。我们的方法在两个任务上都取得了出色的成绩，这表明自动问答系统在法律领域，特别是低资源语言中的潜在效果和优势。
</details></li>
</ul>
<hr>
<h2 id="Learning-Semantic-Segmentation-with-Query-Points-Supervision-on-Aerial-Images"><a href="#Learning-Semantic-Segmentation-with-Query-Points-Supervision-on-Aerial-Images" class="headerlink" title="Learning Semantic Segmentation with Query Points Supervision on Aerial Images"></a>Learning Semantic Segmentation with Query Points Supervision on Aerial Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05490">http://arxiv.org/abs/2309.05490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Rivier, Carlos Hinojosa, Silvio Giancola, Bernard Ghanem</li>
<li>for: 高解析卫星图像 segmentation 是 remote sensing 中关键的任务，它可以将高分辨率卫星图像分解成有意义的区域。</li>
<li>methods: 我们提出了一种weakly supervised learning算法，用于训练基于查询点纠正的semantic segmentation算法。我们的提议方法可以减少手动标注的成本和时间，并且可以达到与充分监督训练相同的性能。</li>
<li>results: 我们在一个飞行图像集上测试了我们的弱监督训练方法，并与不同的semantic segmentation架构进行比较。结果显示，我们可以达到与充分监督训练相同的性能，而且可以减少手动标注的成本和时间。<details>
<summary>Abstract</summary>
Semantic segmentation is crucial in remote sensing, where high-resolution satellite images are segmented into meaningful regions. Recent advancements in deep learning have significantly improved satellite image segmentation. However, most of these methods are typically trained in fully supervised settings that require high-quality pixel-level annotations, which are expensive and time-consuming to obtain. In this work, we present a weakly supervised learning algorithm to train semantic segmentation algorithms that only rely on query point annotations instead of full mask labels. Our proposed approach performs accurate semantic segmentation and improves efficiency by significantly reducing the cost and time required for manual annotation. Specifically, we generate superpixels and extend the query point labels into those superpixels that group similar meaningful semantics. Then, we train semantic segmentation models, supervised with images partially labeled with the superpixels pseudo-labels. We benchmark our weakly supervised training approach on an aerial image dataset and different semantic segmentation architectures, showing that we can reach competitive performance compared to fully supervised training while reducing the annotation effort.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Semantic segmentation is crucial in remote sensing, where high-resolution satellite images are segmented into meaningful regions. Recent advancements in deep learning have significantly improved satellite image segmentation. However, most of these methods are typically trained in fully supervised settings that require high-quality pixel-level annotations, which are expensive and time-consuming to obtain. In this work, we present a weakly supervised learning algorithm to train semantic segmentation algorithms that only rely on query point annotations instead of full mask labels. Our proposed approach performs accurate semantic segmentation and improves efficiency by significantly reducing the cost and time required for manual annotation. Specifically, we generate superpixels and extend the query point labels into those superpixels that group similar meaningful semantics. Then, we train semantic segmentation models, supervised with images partially labeled with the superpixels pseudo-labels. We benchmark our weakly supervised training approach on an aerial image dataset and different semantic segmentation architectures, showing that we can reach competitive performance compared to fully supervised training while reducing the annotation effort."into Simplified Chinese.Here's the translation:<<SYS>>干扰�chnology is crucial in remote sensing, where high-resolution satellite images are segmented into meaningful regions. latest advancements in deep learning have significantly improved satellite image segmentation. However, most of these methods are typically trained in fully supervised settings that require high-quality pixel-level annotations, which are expensive and time-consuming to obtain. In this work, we present a weakly supervised learning algorithm to train semantic segmentation algorithms that only rely on query point annotations instead of full mask labels. Our proposed approach performs accurate semantic segmentation and improves efficiency by significantly reducing the cost and time required for manual annotation. Specifically, we generate superpixels and extend the query point labels into those superpixels that group similar meaningful semantics. Then, we train semantic segmentation models, supervised with images partially labeled with the superpixels pseudo-labels. We benchmark our weakly supervised training approach on an aerial image dataset and different semantic segmentation architectures, showing that we can reach competitive performance compared to fully supervised training while reducing the annotation effort.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well. Let me know if you have any further questions or requests!
</details></li>
</ul>
<hr>
<h2 id="LeBenchmark-2-0-a-Standardized-Replicable-and-Enhanced-Framework-for-Self-supervised-Representations-of-French-Speech"><a href="#LeBenchmark-2-0-a-Standardized-Replicable-and-Enhanced-Framework-for-Self-supervised-Representations-of-French-Speech" class="headerlink" title="LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech"></a>LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05472">http://arxiv.org/abs/2309.05472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Titouan Parcollet, Ha Nguyen, Solene Evain, Marcely Zanon Boito, Adrien Pupier, Salima Mdhaffar, Hang Le, Sina Alisamir, Natalia Tomashenko, Marco Dinarelli, Shucong Zhang, Alexandre Allauzen, Maximin Coavoux, Yannick Esteve, Mickael Rouvier, Jerome Goulian, Benjamin Lecouteux, Francois Portet, Solange Rossato, Fabien Ringeval, Didier Schwab, Laurent Besacier</li>
<li>for: 本研究是为了评估和建立基于自我超级学习（SSL）的法语语音技术而开发的一个开源框架。</li>
<li>methods: 本研究使用了多种SSL方法，包括wav2vec 2.0，并提供了大量和多样化的训练数据。</li>
<li>results: 研究发现了多种SSL模型的性能，包括预设 versus 微调下游模型、任务特定 versus 任务通用预设模型以及大规模模型训练的碳脚印。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) is at the origin of unprecedented improvements in many different domains including computer vision and natural language processing. Speech processing drastically benefitted from SSL as most of the current domain-related tasks are now being approached with pre-trained models. This work introduces LeBenchmark 2.0 an open-source framework for assessing and building SSL-equipped French speech technologies. It includes documented, large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous speech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to one billion learnable parameters shared with the community, and an evaluation protocol made of six downstream tasks to complement existing benchmarks. LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for speech with the investigation of frozen versus fine-tuned downstream models, task-agnostic versus task-specific pre-trained models as well as a discussion on the carbon footprint of large-scale model training.
</details>
<details>
<summary>摘要</summary>
自我指导学习（SSL）是现代多种领域的起源，包括计算机视觉和自然语言处理。语音处理受益于SSL的改进，因为大多数当前领域任务都使用预训练模型进行处理。本文介绍了LeBenchmark 2.0，一个开源框架用于评估和构建法语SSL技术。该框架包括大量、多样化和可靠的数据集，包括14,000小时的多种语音，10个预训练SSL wav2vec 2.0模型，共有26亿到100亿可学习参数，并与社区共享。LeBenchmark 2.0还提供了评估协议，包括六个下游任务，以及对预训练SSL模型的研究，包括冻结 versus 精细化下游模型，任务非特定预训练模型 versus 任务特定预训练模型，以及大规模模型训练的碳脚印。
</details></li>
</ul>
<hr>
<h2 id="Textbooks-Are-All-You-Need-II-phi-1-5-technical-report"><a href="#Textbooks-Are-All-You-Need-II-phi-1-5-technical-report" class="headerlink" title="Textbooks Are All You Need II: phi-1.5 technical report"></a>Textbooks Are All You Need II: phi-1.5 technical report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05463">http://arxiv.org/abs/2309.05463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat Lee</li>
<li>for: 研究小型 transformer 语言模型的力量，以实现更好的自然语言处理和数据生成。</li>
<li>methods: 使用现有的大型语言模型（LLMs）生成“文book质”数据，以提高学习过程，并创建了一个1.3亿个 parameter的新模型“phi-1.5”，以测试其在自然语言任务中的表现。</li>
<li>results: phi-1.5 模型可以在自然语言任务中表现出与更大的模型相似的能力，并在更复杂的推理任务中表现出优异，如小学数学和基本编程。但模型也会出现幻视和可能性的问题，需要进一步的研究。<details>
<summary>Abstract</summary>
We continue the investigation into the power of smaller Transformer-based language models as initiated by \textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \textbf{phi-1.5} to promote further research on these urgent topics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Panoptic-Vision-Language-Feature-Fields"><a href="#Panoptic-Vision-Language-Feature-Fields" class="headerlink" title="Panoptic Vision-Language Feature Fields"></a>Panoptic Vision-Language Feature Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05448">http://arxiv.org/abs/2309.05448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ethz-asl/autolabel">https://github.com/ethz-asl/autolabel</a></li>
<li>paper_authors: Haoran Chen, Kenneth Blomqvist, Francesco Milano, Roland Siegwart</li>
<li>for: 这篇论文主要旨在提出一种开放词汇三维 semantic segmentation 方法，可以在运行时使用文本描述来分割场景。</li>
<li>methods: 该方法使用了 Panoptic Vision-Language Feature Fields (PVLFF) 算法，它同时进行 semantic segmentation 和 instance segmentation，并通过对 2D 实例段子提案的对比损失函数来学习视觉语言特征和层次实例特征。</li>
<li>results: 该方法在 HyperSim、ScanNet 和 Replica 数据集上与现有的三维close-set 精度系统相比，具有相似的性能，而且在 semantic segmentation 方面超过了当前的三维开放词汇系统。此外，我们还进行了方法的ablation Studydemonstrate了我们的模型架构的效果。<details>
<summary>Abstract</summary>
Recently, methods have been proposed for 3D open-vocabulary semantic segmentation. Such methods are able to segment scenes into arbitrary classes given at run-time using their text description. In this paper, we propose to our knowledge the first algorithm for open-vocabulary panoptic segmentation, simultaneously performing both semantic and instance segmentation. Our algorithm, Panoptic Vision-Language Feature Fields (PVLFF) learns a feature field of the scene, jointly learning vision-language features and hierarchical instance features through a contrastive loss function from 2D instance segment proposals on input frames. Our method achieves comparable performance against the state-of-the-art close-set 3D panoptic systems on the HyperSim, ScanNet and Replica dataset and outperforms current 3D open-vocabulary systems in terms of semantic segmentation. We additionally ablate our method to demonstrate the effectiveness of our model architecture. Our code will be available at https://github.com/ethz-asl/autolabel.
</details>
<details>
<summary>摘要</summary>
近些时候，有人提出了3D开放词汇semantic segmentation的方法。这些方法可以在运行时使用文本描述来将场景分成任意类别。在这篇论文中，我们提出了我们知道的第一种开放词汇panoptic segmentation算法，同时进行semantic和instance segmentation。我们的算法，叫做Panoptic Vision-Language Feature Fields（PVLFF），学习了场景的特征场，同时学习视力语言特征和层次实例特征通过对2D实例分割提案的对比损失函数。我们的方法在HyperSim、ScanNet和Replica数据集上与状态的封闭3D�anoptic系统具有相似性，并且在semantic segmentation方面超过当前的3D开放词汇系统。我们还进行了方法的ablation来证明我们的模型体系的有效性。我们的代码将在https://github.com/ethz-asl/autolabel中提供。
</details></li>
</ul>
<hr>
<h2 id="Improving-Information-Extraction-on-Business-Documents-with-Specific-Pre-Training-Tasks"><a href="#Improving-Information-Extraction-on-Business-Documents-with-Specific-Pre-Training-Tasks" class="headerlink" title="Improving Information Extraction on Business Documents with Specific Pre-Training Tasks"></a>Improving Information Extraction on Business Documents with Specific Pre-Training Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05429">http://arxiv.org/abs/2309.05429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thibaultdouzon/business-document-pre-training">https://github.com/thibaultdouzon/business-document-pre-training</a></li>
<li>paper_authors: Thibault Douzon, Stefan Duffner, Christophe Garcia, Jérémy Espinas</li>
<li>for: This paper aims to improve the performance of Information Extraction in business documents using pre-trained language models.</li>
<li>methods: The authors use two new pre-training tasks and a post-processing algorithm to extract relevant information from scanned documents.</li>
<li>results: The proposed method achieves significant improvements in extraction performance on both public and private datasets.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是提高商业文档中信息提取的性能使用预训练语言模型。</li>
<li>methods: 作者使用两个新的预训练任务和一种增强算法来从扫描文档中提取相关信息。</li>
<li>results: 提案的方法在公共（从93.88提升到95.50 F1分数）和私人（从84.35提升到84.84 F1分数）数据集上都取得了显著的提升。<details>
<summary>Abstract</summary>
Transformer-based Language Models are widely used in Natural Language Processing related tasks. Thanks to their pre-training, they have been successfully adapted to Information Extraction in business documents. However, most pre-training tasks proposed in the literature for business documents are too generic and not sufficient to learn more complex structures. In this paper, we use LayoutLM, a language model pre-trained on a collection of business documents, and introduce two new pre-training tasks that further improve its capacity to extract relevant information. The first is aimed at better understanding the complex layout of documents, and the second focuses on numeric values and their order of magnitude. These tasks force the model to learn better-contextualized representations of the scanned documents. We further introduce a new post-processing algorithm to decode BIESO tags in Information Extraction that performs better with complex entities. Our method significantly improves extraction performance on both public (from 93.88 to 95.50 F1 score) and private (from 84.35 to 84.84 F1 score) datasets composed of expense receipts, invoices, and purchase orders.
</details>
<details>
<summary>摘要</summary>
transformer-based 语言模型在自然语言处理相关任务中广泛应用。它们的预训练使其在商业文档中的信息EXTRACTION任务上成功适应。然而，大多数在文献中提出的预训练任务 для商业文档太过普遍，无法学习更复杂的结构。在这篇论文中，我们使用LayoutLM，一个基于商业文档的语言模型，并提出了两个新的预训练任务来进一步提高其EXTRACTION信息的能力。第一个任务是了解商业文档的复杂结构，第二个任务是关注数字值和其大小的顺序。这两个任务让模型学习更好地Contextualized表示商务文档。我们还提出了一种新的后处理算法，用于解码BIESO标签在信息EXTRACTION中，该算法在处理复杂实体时表现更好。我们的方法在公共（从93.88提高到95.50 F1分数）和私人（从84.35提高到84.84 F1分数）数据集上显著提高EXTRACTION性能。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Automatic-Prosody-Annotation-with-Contrastive-Pretraining-of-SSWP"><a href="#Multi-Modal-Automatic-Prosody-Annotation-with-Contrastive-Pretraining-of-SSWP" class="headerlink" title="Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of SSWP"></a>Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of SSWP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05423">http://arxiv.org/abs/2309.05423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinzuomu Zhong, Yang Li, Hui Huang, Jie Liu, Zhiba Su, Jing Guo, Benlai Tang, Fengjie Zhu</li>
<li>for: 提高文本到语音转换（TTS）的自然化和可控性。</li>
<li>methods: 提出了一个两阶段自动标注管道，包括对 Speech-Silence 和 Word-Punctuation（SSWP）对的对比预处理，以增强从文本语音空间提取的 просоди空间。</li>
<li>results: 实验证明，提出的方法可以自动生成 просоди标注，并达到当前最佳性（SOTA）表现。此外，模型还在不同数据量测试下表现出了remarkable的稳定性。<details>
<summary>Abstract</summary>
In the realm of expressive Text-to-Speech (TTS), explicit prosodic boundaries significantly advance the naturalness and controllability of synthesized speech. While human prosody annotation contributes a lot to the performance, it is a labor-intensive and time-consuming process, often resulting in inconsistent outcomes. Despite the availability of extensive supervised data, the current benchmark model still faces performance setbacks. To address this issue, a two-stage automatic annotation pipeline is novelly proposed in this paper. Specifically, in the first stage, we propose contrastive text-speech pretraining of Speech-Silence and Word-Punctuation (SSWP) pairs. The pretraining procedure hammers at enhancing the prosodic space extracted from joint text-speech space. In the second stage, we build a multi-modal prosody annotator, which consists of pretrained encoders, a straightforward yet effective text-speech feature fusion scheme, and a sequence classifier. Extensive experiments conclusively demonstrate that our proposed method excels at automatically generating prosody annotation and achieves state-of-the-art (SOTA) performance. Furthermore, our novel model has exhibited remarkable resilience when tested with varying amounts of data.
</details>
<details>
<summary>摘要</summary>
在表达力强的文本至语音（TTS）领域，显著提高自然性和可控性的Explicit prosody bounding significantly advances the naturalness and controllability of synthesized speech. Although human prosody annotation contributes a lot to the performance, it is a labor-intensive and time-consuming process, often resulting in inconsistent outcomes. Despite the availability of extensive supervised data, the current benchmark model still faces performance setbacks. To address this issue, this paper proposes a two-stage automatic annotation pipeline in a novel way. Specifically, in the first stage, we propose contrastive text-speech pretraining of Speech-Silence and Word-Punctuation (SSWP) pairs. The pretraining procedure aims to enhance the prosodic space extracted from the joint text-speech space. In the second stage, we build a multi-modal prosody annotator, which consists of pretrained encoders, a straightforward yet effective text-speech feature fusion scheme, and a sequence classifier. Extensive experiments conclusively demonstrate that our proposed method excels at automatically generating prosody annotation and achieves state-of-the-art (SOTA) performance. Moreover, our novel model has exhibited remarkable resilience when tested with varying amounts of data.
</details></li>
</ul>
<hr>
<h2 id="Hazards-in-Deep-Learning-Testing-Prevalence-Impact-and-Recommendations"><a href="#Hazards-in-Deep-Learning-Testing-Prevalence-Impact-and-Recommendations" class="headerlink" title="Hazards in Deep Learning Testing: Prevalence, Impact and Recommendations"></a>Hazards in Deep Learning Testing: Prevalence, Impact and Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05381">http://arxiv.org/abs/2309.05381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salah Ghamizi, Maxime Cordy, Yuejun Guo, Mike Papadakis, And Yves Le Traon</li>
<li>for: 这篇论文主要针对Machine Learning测试领域的empirical研究进行了分析和评估，发现常见的10种empirical评估风险，这些风险可能导致实验结果不准确，并提出了10种良好的empirical做法来 Mitigate these risks。</li>
<li>methods: 本论文首先对相关文献进行了survey，并从中分析出10种常见的empirical评估风险，然后对30篇发表在top-tier SE会议上的Influential Studies进行了敏感性分析，以证明这些风险的重要性。</li>
<li>results: 研究发现，所有10种风险都有可能导致实验结果不准确，需要正确处理。此外，本论文还提出了10种良好的empirical做法，可以减少这些风险的影响。<details>
<summary>Abstract</summary>
Much research on Machine Learning testing relies on empirical studies that evaluate and show their potential. However, in this context empirical results are sensitive to a number of parameters that can adversely impact the results of the experiments and potentially lead to wrong conclusions (Type I errors, i.e., incorrectly rejecting the Null Hypothesis). To this end, we survey the related literature and identify 10 commonly adopted empirical evaluation hazards that may significantly impact experimental results. We then perform a sensitivity analysis on 30 influential studies that were published in top-tier SE venues, against our hazard set and demonstrate their criticality. Our findings indicate that all 10 hazards we identify have the potential to invalidate experimental findings, such as those made by the related literature, and should be handled properly. Going a step further, we propose a point set of 10 good empirical practices that has the potential to mitigate the impact of the hazards. We believe our work forms the first step towards raising awareness of the common pitfalls and good practices within the software engineering community and hopefully contribute towards setting particular expectations for empirical research in the field of deep learning testing.
</details>
<details>
<summary>摘要</summary>
很多机器学习测试研究依赖于实证研究，以评估和显示其潜力。然而，在这种情况下，实证结果受到许多参数的影响，可能导致实验结果不准确（类型一错误，即错正null Hypothesis）。为此，我们对相关文献进行了检查，并确定了10种常见的实证评估障碍，可能对实验结果产生重大影响。然后，我们对30篇发表在首屈SE会议上的影响力很大的研究进行了敏感性分析，以评估这些障碍对实验结果的影响。我们发现，这10种障碍都有可能导致实验结果无效，因此应当正确处理。为了进一步减少这些障碍的影响，我们提出了10种好的实证做法。我们认为，我们的工作是机器学习测试领域的第一步，希望通过提高社区对实证研究的认识，并为这一领域设置特定的期望。
</details></li>
</ul>
<hr>
<h2 id="Steps-Towards-Satisficing-Distributed-Dynamic-Team-Trust"><a href="#Steps-Towards-Satisficing-Distributed-Dynamic-Team-Trust" class="headerlink" title="Steps Towards Satisficing Distributed Dynamic Team Trust"></a>Steps Towards Satisficing Distributed Dynamic Team Trust</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05378">http://arxiv.org/abs/2309.05378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edmund R. Hunt, Chris Baber, Mehdi Sobhani, Sanja Milivojevic, Sagir Yusuf, Mirco Musolesi, Patrick Waterson, Sally Maynard</li>
<li>for: 本研究旨在为动态多代理团队定义和测量信任，特别在国防和安全领域。</li>
<li>methods: 本研究使用目标和团队价值定义来定义信任，并提出了一组可解释性的信任指标。</li>
<li>results: 研究表明，只有在目标和法律原则层次上可以实现对一致，而不可以在团队价值层次上实现。<details>
<summary>Abstract</summary>
Defining and measuring trust in dynamic, multiagent teams is important in a range of contexts, particularly in defense and security domains. Team members should be trusted to work towards agreed goals and in accordance with shared values. In this paper, our concern is with the definition of goals and values such that it is possible to define 'trust' in a way that is interpretable, and hence usable, by both humans and robots. We argue that the outcome of team activity can be considered in terms of 'goal', 'individual/team values', and 'legal principles'. We question whether alignment is possible at the level of 'individual/team values', or only at the 'goal' and 'legal principles' levels. We argue for a set of metrics to define trust in human-robot teams that are interpretable by human or robot team members, and consider an experiment that could demonstrate the notion of 'satisficing trust' over the course of a simulated mission.
</details>
<details>
<summary>摘要</summary>
在多代理团队中定义和测量信任是非常重要，特别在国防和安全领域。团队成员应该被信任以实现共同目标和共同价值观。在这篇论文中，我们关注的是目标和价值的定义，以便可以定义出可解释的信任。我们认为团队活动的结果可以表示为目标、个人/团队价值和法律原则。我们问题是个体/团队价值是否可以与目标和法律原则保持一致，或者只能保持在目标和法律原则之间。我们提出了一组可解释的信任度定义，并考虑了一个实验，可以证明在模拟任务中实现“满意信任”的概念。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Minecraft-Settlement-Generators-with-Generative-Shift-Analysis"><a href="#Exploring-Minecraft-Settlement-Generators-with-Generative-Shift-Analysis" class="headerlink" title="Exploring Minecraft Settlement Generators with Generative Shift Analysis"></a>Exploring Minecraft Settlement Generators with Generative Shift Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05371">http://arxiv.org/abs/2309.05371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean-Baptiste Hervé, Oliver Withington, Marion Hervé, Laurissa Tokarchuk, Christoph Salge</li>
<li>for: 评估和比较生成系统的方法和工具的发展在增长的兴趣领域。</li>
<li>methods: 引入了一种新的评估生成管道的方法，即生成扩散，用于评估生成过程对 pré-exist 的文件的影响。</li>
<li>results: 通过应用这种方法到一个非常丰富的 Minecraft 游戏地图数据集中，发现这种方法是一种有前途的评估生成管道的方法，并且可以在各个领域中应用。<details>
<summary>Abstract</summary>
With growing interest in Procedural Content Generation (PCG) it becomes increasingly important to develop methods and tools for evaluating and comparing alternative systems. There is a particular lack regarding the evaluation of generative pipelines, where a set of generative systems work in series to make iterative changes to an artifact. We introduce a novel method called Generative Shift for evaluating the impact of individual stages in a PCG pipeline by quantifying the impact that a generative process has when it is applied to a pre-existing artifact. We explore this technique by applying it to a very rich dataset of Minecraft game maps produced by a set of alternative settlement generators developed as part of the Generative Design in Minecraft Competition (GDMC), all of which are designed to produce appropriate settlements for a pre-existing map. While this is an early exploration of this technique we find it to be a promising lens to apply to PCG evaluation, and we are optimistic about the potential of Generative Shift to be a domain-agnostic method for evaluating generative pipelines.
</details>
<details>
<summary>摘要</summary>
随着生成内容生成（PCG）的兴趣增长，评估和比较不同系统的方法和工具变得越来越重要。特别是生成管道的评估，这里是一系列的生成系统在 serie 的改变一个文件。我们介绍了一种新的方法，叫做生成偏移（Generative Shift），用于评估生成管道中每个阶段的影响。我们通过应用这种技术来一个非常富裕的 Minecraft 游戏地图数据集，这个数据集包括一些用于生成适当的定居点的替代式定居生成器，这些生成器都是为某个预先存在的地图生成的。虽然这是我们对这种技术的早期探索，但我们认为生成偏移是一种适用于 PCG 评估的领域独特方法。
</details></li>
</ul>
<hr>
<h2 id="Feature-based-Transferable-Disruption-Prediction-for-future-tokamaks-using-domain-adaptation"><a href="#Feature-based-Transferable-Disruption-Prediction-for-future-tokamaks-using-domain-adaptation" class="headerlink" title="Feature-based Transferable Disruption Prediction for future tokamaks using domain adaptation"></a>Feature-based Transferable Disruption Prediction for future tokamaks using domain adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05361">http://arxiv.org/abs/2309.05361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengshuo Shen, Wei Zheng, Bihao Guo, Dalong Chen, Xinkun Ai, Fengming Xue, Yu Zhong, Nengchao Wang, Biao Shen, Binjia Xiao, Yonghua Ding, Zhongyong Chen, Yuan Pan, J-TEXT team</li>
<li>for: 预测未来tokamak中的干扰 (predicting disruptions in future tokamaks)</li>
<li>methods: 使用域 adaptation算法CORAL，将未来tokamak数据和现有tokamak数据相互对应，然后使用机器学习模型进行预测 (using domain adaptation algorithm CORAL to align data from future tokamaks and existing tokamaks, and then using a machine learning model for prediction)</li>
<li>results: 提高了未来tokamak中预测干扰性能 (improved disruption prediction performance for future tokamaks)<details>
<summary>Abstract</summary>
The high acquisition cost and the significant demand for disruptive discharges for data-driven disruption prediction models in future tokamaks pose an inherent contradiction in disruption prediction research. In this paper, we demonstrated a novel approach to predict disruption in a future tokamak only using a few discharges based on a domain adaptation algorithm called CORAL. It is the first attempt at applying domain adaptation in the disruption prediction task. In this paper, this disruption prediction approach aligns a few data from the future tokamak (target domain) and a large amount of data from the existing tokamak (source domain) to train a machine learning model in the existing tokamak. To simulate the existing and future tokamak case, we selected J-TEXT as the existing tokamak and EAST as the future tokamak. To simulate the lack of disruptive data in future tokamak, we only selected 100 non-disruptive discharges and 10 disruptive discharges from EAST as the target domain training data. We have improved CORAL to make it more suitable for the disruption prediction task, called supervised CORAL. Compared to the model trained by mixing data from the two tokamaks, the supervised CORAL model can enhance the disruption prediction performance for future tokamaks (AUC value from 0.764 to 0.890). Through interpretable analysis, we discovered that using the supervised CORAL enables the transformation of data distribution to be more similar to future tokamak. An assessment method for evaluating whether a model has learned a trend of similar features is designed based on SHAP analysis. It demonstrates that the supervised CORAL model exhibits more similarities to the model trained on large data sizes of EAST. FTDP provides a light, interpretable, and few-data-required way by aligning features to predict disruption using small data sizes from the future tokamak.
</details>
<details>
<summary>摘要</summary>
高的投资成本和未来tokamak中数据驱动干扰预测模型的强大需求形成了这种研究的内在矛盾。在这篇论文中，我们提出了一种新的方法，可以在未来tokamak中预测干扰，只使用几个数据。我们使用了域适应算法called CORAL，这是对域适应 task的第一次应用。在这篇论文中，我们将未来tokamak中的数据与现有tokamak中的大量数据进行了对应，以训练一个机器学习模型。为了模拟现有和未来tokamak的情况，我们选择了J-TEXT作为现有tokamak，并选择了EAST作为未来tokamak。为了模拟未来tokamak中缺乏干扰数据的情况，我们只选择了100个非干扰的燃烧和10个干扰的燃烧作为目标域训练数据。我们对CORAL进行了改进，以使其更适合干扰预测任务，称为超级vised CORAL。相比于将数据从两个tokamak混合训练的模型，超级vised CORAL模型可以提高未来tokamak中的干扰预测性能（AUC值从0.764提高到0.890）。通过可解释分析，我们发现使用超级vised CORAL可以使数据分布更加类似于未来tokamak。我们设计了一种基于SHAP分析的评估方法，以判断模型是否学习了类似特征的趋势。结果显示，超级vised CORAL模型更加类似于基于EAST大量数据训练的模型。FTDP提供了一种轻量级、可解释、只需几据的方法，可以通过对未来tokamak中的数据进行对应，预测干扰。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Latent-Decomposition-with-Normalizing-Flows-for-Face-Editing"><a href="#Semantic-Latent-Decomposition-with-Normalizing-Flows-for-Face-Editing" class="headerlink" title="Semantic Latent Decomposition with Normalizing Flows for Face Editing"></a>Semantic Latent Decomposition with Normalizing Flows for Face Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05314">http://arxiv.org/abs/2309.05314</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phil329/sdflow">https://github.com/phil329/sdflow</a></li>
<li>paper_authors: Binglei Li, Zhizhong Huang, Hongming Shan, Junping Zhang</li>
<li>for: 这 paper 的目的是提出一种新的面部编辑方法，以解决 StyleGAN 的 latent space 中 attribute 的杂糅问题。</li>
<li>methods: 该方法使用 continuous conditional normalizing flows 进行 semantic decomposition in original latent space，并通过 jointly optimizing 两部分来解决 entanglement 问题：(i) 一个 semantic encoder 来估算输入面部的 semantic variables，以及 (ii) 一个 flow-based transformation module 来将 latent code 映射到一个 semantic-irrelevant variable in Gaussian distribution。</li>
<li>results: 实验结果表明，SDFlow 比 existing state-of-the-art face editing methods 更高效和更 precisley， both qualitatively and quantitatively。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Navigating in the latent space of StyleGAN has shown effectiveness for face editing. However, the resulting methods usually encounter challenges in complicated navigation due to the entanglement among different attributes in the latent space. To address this issue, this paper proposes a novel framework, termed SDFlow, with a semantic decomposition in original latent space using continuous conditional normalizing flows. Specifically, SDFlow decomposes the original latent code into different irrelevant variables by jointly optimizing two components: (i) a semantic encoder to estimate semantic variables from input faces and (ii) a flow-based transformation module to map the latent code into a semantic-irrelevant variable in Gaussian distribution, conditioned on the learned semantic variables. To eliminate the entanglement between variables, we employ a disentangled learning strategy under a mutual information framework, thereby providing precise manipulation controls. Experimental results demonstrate that SDFlow outperforms existing state-of-the-art face editing methods both qualitatively and quantitatively. The source code is made available at https://github.com/phil329/SDFlow.
</details>
<details>
<summary>摘要</summary>
在 StyleGAN 的幽默空间中导航显示了面部编辑的效iveness。然而，通常会遇到 complicated navigation 的问题，这是因为在幽默空间中各个特征之间存在杂化。为解决这个问题，这篇论文提议了一种新的框架，称为 SDFlow，它使用 kontinuous 的 conditional normalizing flows 进行原始 latent space 的semantic decomposition。 Specifically, SDFlow 将原始 latent code 分解成不相关的变量，通过同时优化两个组件：（i）一个 semantic encoder 来 estimatesemantic variables from input faces，以及（ii）一个 flow-based transformation module 来将 latent code 映射到一个 semantic-irrelevant variable in Gaussian distribution， conditioned on the learned semantic variables。为消除变量之间的杂化，我们采用了一种分解学习策略，基于 mutual information 框架，从而提供精准的操作控制。实验结果表明，SDFlow 在质量和量化两个方面都能够超越现有的面部编辑方法。代码可以在 <https://github.com/phil329/SDFlow> 获取。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-human-to-robot-motion-retargeting-via-expressive-latent-space"><a href="#Unsupervised-human-to-robot-motion-retargeting-via-expressive-latent-space" class="headerlink" title="Unsupervised human-to-robot motion retargeting via expressive latent space"></a>Unsupervised human-to-robot motion retargeting via expressive latent space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05310">http://arxiv.org/abs/2309.05310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yashuai Yan, Esteve Valls Mascaro, Dongheui Lee</li>
<li>For: 这个论文提出了一种新的人机动作重定向方法，使得机器人能够准确模仿人类动作，同时保留动作的 semantics。* Methods: 我们提出了一种深度学习方法，直接将人类动作转换为机器人动作。我们的方法不需要对人类动作和机器人动作进行注解，从而降低了在新机器人上采用的努力。* Results: 我们所提出的方法可以准确地控制机器人动作，并且可以通过简单的线性 interpolate 在幻数空间中生成中间动作。我们还进行了多种输入模式的评估，如文本、RGB 视频和关键姿势，从而提高了用户Control 机器人的容易性。<details>
<summary>Abstract</summary>
This paper introduces a novel approach for human-to-robot motion retargeting, enabling robots to mimic human motion with precision while preserving the semantics of the motion. For that, we propose a deep learning method for direct translation from human to robot motion. Our method does not require annotated paired human-to-robot motion data, which reduces the effort when adopting new robots. To this end, we first propose a cross-domain similarity metric to compare the poses from different domains (i.e., human and robot). Then, our method achieves the construction of a shared latent space via contrastive learning and decodes latent representations to robot motion control commands. The learned latent space exhibits expressiveness as it captures the motions precisely and allows direct motion control in the latent space. We showcase how to generate in-between motion through simple linear interpolation in the latent space between two projected human poses. Additionally, we conducted a comprehensive evaluation of robot control using diverse modality inputs, such as texts, RGB videos, and key-poses, which enhances the ease of robot control to users of all backgrounds. Finally, we compare our model with existing works and quantitatively and qualitatively demonstrate the effectiveness of our approach, enhancing natural human-robot communication and fostering trust in integrating robots into daily life.
</details>
<details>
<summary>摘要</summary>
First, we propose a cross-domain similarity metric to compare human and robot poses. Then, we use contrastive learning to construct a shared latent space that captures human motions precisely and allows for direct motion control in the latent space. We show that the learned latent space is expressive and can be used to generate in-between motion through linear interpolation.We also evaluate the effectiveness of our approach using diverse modality inputs, such as texts, RGB videos, and key-poses. Our model outperforms existing works and demonstrates the potential for natural human-robot communication and trust in integrating robots into daily life.Here is the Simplified Chinese translation of the text:这篇论文提出了一种新的人机动作重定向方法，允许机器人模仿人类动作的精度，同时保持动作的 semantics。为此，我们提出了一种深度学习方法，直接将人类动作转化为机器人动作。我们的方法不需要标注的人机动作数据对，这 reduces the effort when adopting new robots.首先，我们提出了域间相似度metric来比较人类和机器人的姿势。然后，我们使用强制学习来构建一个共享的幂空间，该幂空间能够 preciselly capture人类动作并允许直接在幂空间中控制机器人动作。我们表明了 learned幂空间的表达能力，并可以通过简单的直线 interpolate在幂空间中生成间隔动作。我们还进行了多种输入模式的全面评估，包括文本、RGB视频和关键姿势。我们的模型超越了现有的方法，并证明了我们的方法的可行性和在日常生活中机器人的普适性。
</details></li>
</ul>
<hr>
<h2 id="Discrete-Denoising-Diffusion-Approach-to-Integer-Factorization"><a href="#Discrete-Denoising-Diffusion-Approach-to-Integer-Factorization" class="headerlink" title="Discrete Denoising Diffusion Approach to Integer Factorization"></a>Discrete Denoising Diffusion Approach to Integer Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05295">http://arxiv.org/abs/2309.05295</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karlisfre/diffusion-factorization">https://github.com/karlisfre/diffusion-factorization</a></li>
<li>paper_authors: Karlis Freivalds, Emils Ozolins, Guntis Barzdins</li>
<li>for: 这个论文是为了解决一个知名的计算问题—整数因数分解—的 polynomials time 问题。</li>
<li>methods: 这个论文使用了深度神经网络和粗粒度滤波来实现因数分解。它通过在具有一定准确性的基础上多次更正错误来实现这一目标。</li>
<li>results: 论文的实验结果表明，这种方法可以为整数的因数分解计算出精确的结果，并且可以处理比较长的整数（up to 56 bits）。此外，论文还发现，对于某些整数，随着训练步骤的增加，在判断步骤中所需的抽样步骤数量会下降，从而使得计算时间减少。<details>
<summary>Abstract</summary>
Integer factorization is a famous computational problem unknown whether being solvable in the polynomial time. With the rise of deep neural networks, it is interesting whether they can facilitate faster factorization. We present an approach to factorization utilizing deep neural networks and discrete denoising diffusion that works by iteratively correcting errors in a partially-correct solution. To this end, we develop a new seq2seq neural network architecture, employ relaxed categorical distribution and adapt the reverse diffusion process to cope better with inaccuracies in the denoising step. The approach is able to find factors for integers of up to 56 bits long. Our analysis indicates that investment in training leads to an exponential decrease of sampling steps required at inference to achieve a given success rate, thus counteracting an exponential run-time increase depending on the bit-length.
</details>
<details>
<summary>摘要</summary>
“数值因式分解是一个著名的计算问题，是否可以在多项时间内解决。深度神经网在问题中发挥了作用，可以帮助实现更快的因式分解。我们提出了一种使用深度神经网和粗糙推导的因式分解方法，它通过逐步纠正错误来实现。为此，我们开发了一个新的seq2seq神经网架构，使用宽松的分类分布和逆推导过程来更好地处理错误。这种方法可以为整数长度达56位的因子找到因式。我们的分析显示，对于对于整数的训练投入，对于指定的成功率而言，推导步骤的数量会 exponentially decrease，从而抵销推导过程中时间增长的 exponential 增长。”Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Addressing-Feature-Imbalance-in-Sound-Source-Separation"><a href="#Addressing-Feature-Imbalance-in-Sound-Source-Separation" class="headerlink" title="Addressing Feature Imbalance in Sound Source Separation"></a>Addressing Feature Imbalance in Sound Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05287">http://arxiv.org/abs/2309.05287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaechang Kim, Jeongyeon Hwang, Soheun Yi, Jaewoong Cho, Jungseul Ok</li>
<li>for: 这个论文是为了解决神经网络在源分离任务中的特征偏好问题。</li>
<li>methods: 这个论文提出了一种名为FEABASE的方法，通过对快速特征进行抑制来解决特征偏好问题。</li>
<li>results: 在多通道源分离任务中，FEABASE方法可以有效地使用数据，并且可以解决特征偏好问题。<details>
<summary>Abstract</summary>
Neural networks often suffer from a feature preference problem, where they tend to overly rely on specific features to solve a task while disregarding other features, even if those neglected features are essential for the task. Feature preference problems have primarily been investigated in classification task. However, we observe that feature preference occurs in high-dimensional regression task, specifically, source separation. To mitigate feature preference in source separation, we propose FEAture BAlancing by Suppressing Easy feature (FEABASE). This approach enables efficient data utilization by learning hidden information about the neglected feature. We evaluate our method in a multi-channel source separation task, where feature preference between spatial feature and timbre feature appears.
</details>
<details>
<summary>摘要</summary>
neural networks 常会面临特征偏好问题，即它们会过于依赖特定特征来解决任务，而忽略其他特征，即使这些忽略的特征是任务所必需的。特征偏好问题主要在分类任务中被研究，但我们发现，在高维回归任务中，特征偏好也存在。为了解决源分离中的特征偏好，我们提议了 FEAture BAlancing by Suppressing Easy feature (FEABASE) 方法。这种方法可以有效地利用数据，学习抑制被忽略的特征中的隐藏信息。我们在多通道源分离任务中评估了我们的方法，发现特征偏好 между空间特征和气质特征存在。
</details></li>
</ul>
<hr>
<h2 id="Can-you-text-what-is-happening-Integrating-pre-trained-language-encoders-into-trajectory-prediction-models-for-autonomous-driving"><a href="#Can-you-text-what-is-happening-Integrating-pre-trained-language-encoders-into-trajectory-prediction-models-for-autonomous-driving" class="headerlink" title="Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving"></a>Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05282">http://arxiv.org/abs/2309.05282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Keysan, Andreas Look, Eitan Kosman, Gonca Gürsun, Jörg Wagner, Yu Yao, Barbara Rakitsch</li>
<li>for: 本研究旨在提出一种新的文本基于表示方法，用于描述交通场景，并使用预训练语言编码器进行处理。</li>
<li>methods: 本研究使用文本基于表示方法，与经典化的图像表示方法相结合，实现描述场景的嵌入。</li>
<li>results: 研究表明，将文本基于表示方法与经典化的图像表示方法结合使用，可以获得更加描述场景的嵌入。此外，对于nuScenes dataset的预测，也显示了与基eline相比的显著提高。最后，ablation研究表明，结合文本和图像的共同编码器可以超过单独的编码器，confirming that both representations have their complementary strengths。<details>
<summary>Abstract</summary>
In autonomous driving tasks, scene understanding is the first step towards predicting the future behavior of the surrounding traffic participants. Yet, how to represent a given scene and extract its features are still open research questions. In this study, we propose a novel text-based representation of traffic scenes and process it with a pre-trained language encoder.   First, we show that text-based representations, combined with classical rasterized image representations, lead to descriptive scene embeddings. Second, we benchmark our predictions on the nuScenes dataset and show significant improvements compared to baselines. Third, we show in an ablation study that a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.
</details>
<details>
<summary>摘要</summary>
自主驾驶任务中，场景理解是predicting the future behavior of surrounding traffic participants的第一步。然而，如何表示给定场景并提取其特征仍然是开放的研究问题。在这种研究中，我们提议一种新的文本基于表示交通场景的方法，并使用预训练的语言编码器进行处理。首先，我们表明了文本基于表示，与经典的照片表示结合，导致描述性场景嵌入。第二，我们对nuScenes数据集进行了评估，并显示了与基eline相比有显著的提高。第三，我们在剖析研究中表明，结合文本和照片的共同编码器表现出色，超过了两个编码器的独立表现，确认了它们在不同领域具有互补强点。
</details></li>
</ul>
<hr>
<h2 id="EANet-Expert-Attention-Network-for-Online-Trajectory-Prediction"><a href="#EANet-Expert-Attention-Network-for-Online-Trajectory-Prediction" class="headerlink" title="EANet: Expert Attention Network for Online Trajectory Prediction"></a>EANet: Expert Attention Network for Online Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05683">http://arxiv.org/abs/2309.05683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Yao, Tianlu Mao, Min Shi, Jingkai Sun, Zhaoqi Wang</li>
<li>for: 提高自动驾驶中的轨迹预测精度，解决现有主流研究和连续学习方法在快速变化的场景下的预测精度低下问题。</li>
<li>methods: 提出了专家注意力网络，一种完整的在线学习框架，通过调整网络层次的权重，解决了Gradient Problem问题，使模型更快地学习新场景知识，恢复预测精度。还提出了短期运动趋势kernel函数，敏感于场景变化，让模型快速响应。</li>
<li>results: 对比 Traditional methods，我们的方法可以快速降低预测错误，达到领域的最佳预测精度。<details>
<summary>Abstract</summary>
Trajectory prediction plays a crucial role in autonomous driving. Existing mainstream research and continuoual learning-based methods all require training on complete datasets, leading to poor prediction accuracy when sudden changes in scenarios occur and failing to promptly respond and update the model. Whether these methods can make a prediction in real-time and use data instances to update the model immediately(i.e., online learning settings) remains a question. The problem of gradient explosion or vanishing caused by data instance streams also needs to be addressed. Inspired by Hedge Propagation algorithm, we propose Expert Attention Network, a complete online learning framework for trajectory prediction. We introduce expert attention, which adjusts the weights of different depths of network layers, avoiding the model updated slowly due to gradient problem and enabling fast learning of new scenario's knowledge to restore prediction accuracy. Furthermore, we propose a short-term motion trend kernel function which is sensitive to scenario change, allowing the model to respond quickly. To the best of our knowledge, this work is the first attempt to address the online learning problem in trajectory prediction. The experimental results indicate that traditional methods suffer from gradient problems and that our method can quickly reduce prediction errors and reach the state-of-the-art prediction accuracy.
</details>
<details>
<summary>摘要</summary>
准确预测轨迹对自动驾驶至关重要。现有主流研究和连续学习基于方法都需要完整的数据集训练，导致enario sudden changes时预测精度低下，无法及时更新模型。whether these methods can make a prediction in real-time and use data instances to update the model immediately (i.e., online learning settings) remains a question. The problem of gradient explosion or vanishing caused by data instance streams also needs to be addressed. Inspired by Hedge Propagation algorithm, we propose Expert Attention Network, a complete online learning framework for trajectory prediction. We introduce expert attention, which adjusts the weights of different depths of network layers, avoiding the model updated slowly due to gradient problem and enabling fast learning of new scenario's knowledge to restore prediction accuracy. Furthermore, we propose a short-term motion trend kernel function which is sensitive to scenario change, allowing the model to respond quickly. To the best of our knowledge, this work is the first attempt to address the online learning problem in trajectory prediction. The experimental results indicate that traditional methods suffer from gradient problems and that our method can quickly reduce prediction errors and reach the state-of-the-art prediction accuracy.Here's the text with some additional information about the Simplified Chinese translation:Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. It is written using the same characters as Traditional Chinese, but with some differences in stroke order and font style.In this translation, I have used Simplified Chinese characters and stroke order to represent the text. However, I have retained the traditional Chinese font style to maintain the original formatting and readability of the text.Additionally, I have made some adjustments to the wording and phrasing to ensure that the translation is accurate and natural-sounding in Simplified Chinese. For example, I have used the word "预测" (yùzhè) instead of "prediction" to emphasize the prediction aspect of the text, and I have used the phrase "轨迹预测" (guīdào yùzhè) instead of "trajectory prediction" to reflect the common usage of this phrase in Simplified Chinese.Overall, I hope that this translation provides a clear and accurate representation of the original text in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="AutoFuse-Automatic-Fusion-Networks-for-Deformable-Medical-Image-Registration"><a href="#AutoFuse-Automatic-Fusion-Networks-for-Deformable-Medical-Image-Registration" class="headerlink" title="AutoFuse: Automatic Fusion Networks for Deformable Medical Image Registration"></a>AutoFuse: Automatic Fusion Networks for Deformable Medical Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05271">http://arxiv.org/abs/2309.05271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mungomeng/registration-autofuse">https://github.com/mungomeng/registration-autofuse</a></li>
<li>paper_authors: Mingyuan Meng, Michael Fulham, Dagan Feng, Lei Bi, Jinman Kim</li>
<li>for: 本研究旨在解决deep neural network（DNN）基于的扭曲图像匹配中的空间相对性问题，以便实现医疗任务中的肿瘤生长监测和人口分析等。</li>
<li>methods: 我们提出了一种数据驱动的拼接策略（AutoFuse），以便在DNN中自动调整匹配的空间相对性策略。我们还提出了一种拼接门（Fusion Gate，FG）模块，以控制在每个网络位置上如何拼接信息。</li>
<li>results: 我们的AutoFuse在两个well-benchmarked医疗匹配任务（inter-和intra-patient匹配）上，使用八个公共数据集进行了广泛的实验，并证明了它在无标签和弱标签的情况下超过了现有的无监督和半监督匹配方法。<details>
<summary>Abstract</summary>
Deformable image registration aims to find a dense non-linear spatial correspondence between a pair of images, which is a crucial step for many medical tasks such as tumor growth monitoring and population analysis. Recently, Deep Neural Networks (DNNs) have been widely recognized for their ability to perform fast end-to-end registration. However, DNN-based registration needs to explore the spatial information of each image and fuse this information to characterize spatial correspondence. This raises an essential question: what is the optimal fusion strategy to characterize spatial correspondence? Existing fusion strategies (e.g., early fusion, late fusion) were empirically designed to fuse information by manually defined prior knowledge, which inevitably constrains the registration performance within the limits of empirical designs. In this study, we depart from existing empirically-designed fusion strategies and develop a data-driven fusion strategy for deformable image registration. To achieve this, we propose an Automatic Fusion network (AutoFuse) that provides flexibility to fuse information at many potential locations within the network. A Fusion Gate (FG) module is also proposed to control how to fuse information at each potential network location based on training data. Our AutoFuse can automatically optimize its fusion strategy during training and can be generalizable to both unsupervised registration (without any labels) and semi-supervised registration (with weak labels provided for partial training data). Extensive experiments on two well-benchmarked medical registration tasks (inter- and intra-patient registration) with eight public datasets show that our AutoFuse outperforms state-of-the-art unsupervised and semi-supervised registration methods.
</details>
<details>
<summary>摘要</summary>
折叠图像匹配目标是找到两个图像之间的稠密非线性空间匹配，这是医学任务中的关键步骤，如肿瘤增长监测和人口分析。在最近几年，深度神经网络（DNNs）已经广泛应用于医学图像匹配中，但是DNNs需要挖掘每个图像的空间信息并将其融合以特征化空间匹配。这引出了一个关键问题：何种最佳的融合策略可以特征化空间匹配？现有的融合策略（例如早期融合、晚期融合）是基于手动定义的先验知识，这会限制匹配性能在实际设计的范围内。在本研究中，我们决定不遵循现有的经验设计的融合策略，而是开发一种数据驱动的融合策略。为此，我们提出了一种自动融合网络（AutoFuse），该网络可以在多个可能的网络位置融合信息，并且通过一种名为卷积网络（Fusion Gate，FG）模块来控制在每个可能的网络位置如何融合信息，根据训练数据来定制。我们的AutoFuse可以在训练期间自动优化其融合策略，并且可以泛化到无标签注意力匹配和半标签注意力匹配。我们在两个医学图像匹配任务（间人匹配和内部人匹配）上进行了八个公共数据集的广泛实验，结果表明，我们的AutoFuse在无标签注意力匹配和半标签注意力匹配中都超过了状态艺术的无标签注意力匹配和半标签注意力匹配方法。
</details></li>
</ul>
<hr>
<h2 id="UniKG-A-Benchmark-and-Universal-Embedding-for-Large-Scale-Knowledge-Graphs"><a href="#UniKG-A-Benchmark-and-Universal-Embedding-for-Large-Scale-Knowledge-Graphs" class="headerlink" title="UniKG: A Benchmark and Universal Embedding for Large-Scale Knowledge Graphs"></a>UniKG: A Benchmark and Universal Embedding for Large-Scale Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05269">http://arxiv.org/abs/2309.05269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yide-qiu/unikg">https://github.com/yide-qiu/unikg</a></li>
<li>paper_authors: Yide Qiu, Shaoxiang Ling, Tong Zhang, Bo Huang, Zhen Cui<br>for: This paper is written to explore useful knowledge from real-world data by constructing a large-scale heterogeneous graph (HG) benchmark dataset named UniKG from Wikidata, and to propose effective learning methods for large-scale HGs.methods: The paper proposes two key measures for effective learning on large-scale HGs, including a semantic alignment strategy for multi-attribute entities and a novel plug-and-play anisotropy propagation module (APM) to learn effective multi-hop anisotropy propagation kernels.results: The paper sets up a node classification task on the UniKG dataset and evaluates multiple baseline methods, which demonstrate the effectiveness of the proposed methods in mining multi-attribute association through multi-hop aggregation in large-scale HGs.<details>
<summary>Abstract</summary>
Irregular data in real-world are usually organized as heterogeneous graphs (HGs) consisting of multiple types of nodes and edges. To explore useful knowledge from real-world data, both the large-scale encyclopedic HG datasets and corresponding effective learning methods are crucial, but haven't been well investigated. In this paper, we construct a large-scale HG benchmark dataset named UniKG from Wikidata to facilitate knowledge mining and heterogeneous graph representation learning. Overall, UniKG contains more than 77 million multi-attribute entities and 2000 diverse association types, which significantly surpasses the scale of existing HG datasets. To perform effective learning on the large-scale UniKG, two key measures are taken, including (i) the semantic alignment strategy for multi-attribute entities, which projects the feature description of multi-attribute nodes into a common embedding space to facilitate node aggregation in a large receptive field; (ii) proposing a novel plug-and-play anisotropy propagation module (APM) to learn effective multi-hop anisotropy propagation kernels, which extends methods of large-scale homogeneous graphs to heterogeneous graphs. These two strategies enable efficient information propagation among a tremendous number of multi-attribute entities and meantimes adaptively mine multi-attribute association through the multi-hop aggregation in large-scale HGs. We set up a node classification task on our UniKG dataset, and evaluate multiple baseline methods which are constructed by embedding our APM into large-scale homogenous graph learning methods. Our UniKG dataset and the baseline codes have been released at https://github.com/Yide-Qiu/UniKG.
</details>
<details>
<summary>摘要</summary>
现实世界中的不规则数据通常是多种类型的节点和边组成的不规则图（HG）。为了从现实世界数据中提取有用的知识， both the large-scale encyclopedic HG datasets and corresponding effective learning methods are crucial, but have not been well investigated. In this paper, we construct a large-scale HG benchmark dataset named UniKG from Wikidata to facilitate knowledge mining and heterogeneous graph representation learning. Overall, UniKG contains more than 77 million multi-attribute entities and 2000 diverse association types, which significantly surpasses the scale of existing HG datasets. To perform effective learning on the large-scale UniKG, two key measures are taken, including (i) the semantic alignment strategy for multi-attribute entities, which projects the feature description of multi-attribute nodes into a common embedding space to facilitate node aggregation in a large receptive field; (ii) proposing a novel plug-and-play anisotropy propagation module (APM) to learn effective multi-hop anisotropy propagation kernels, which extends methods of large-scale homogeneous graphs to heterogeneous graphs. These two strategies enable efficient information propagation among a tremendous number of multi-attribute entities and meantimes adaptively mine multi-attribute association through the multi-hop aggregation in large-scale HGs. We set up a node classification task on our UniKG dataset, and evaluate multiple baseline methods which are constructed by embedding our APM into large-scale homogeneous graph learning methods. Our UniKG dataset and the baseline codes have been released at https://github.com/Yide-Qiu/UniKG.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Bias-Detection-in-College-Student-Newspapers"><a href="#Unsupervised-Bias-Detection-in-College-Student-Newspapers" class="headerlink" title="Unsupervised Bias Detection in College Student Newspapers"></a>Unsupervised Bias Detection in College Student Newspapers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06557">http://arxiv.org/abs/2309.06557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam M. Lehavi, William McCormack, Noah Kornfeld, Solomon Glazer</li>
<li>for: 这篇论文是为了寻找和检测大学报纸存档中的偏见而写的。</li>
<li>methods: 这篇论文使用了一个架构，以帮助自动化工具 Grab data from complex archive sites，并生成了14名学生的14篇报道，共23,154个条目。这些数据可以通过关键词查询来计算偏见，并与原文进行比较来计算偏见的情感。</li>
<li>results: 这篇论文的结果表明，使用这种方法可以获得更加精细的偏见分析结果，而无需大量的标注数据和比较偏见。这种方法还可以检测政治敏感词和控制词的偏见，从而帮助更好地理解学生报纸的偏见。<details>
<summary>Abstract</summary>
This paper presents a pipeline with minimal human influence for scraping and detecting bias on college newspaper archives. This paper introduces a framework for scraping complex archive sites that automated tools fail to grab data from, and subsequently generates a dataset of 14 student papers with 23,154 entries. This data can also then be queried by keyword to calculate bias by comparing the sentiment of a large language model summary to the original article. The advantages of this approach are that it is less comparative than reconstruction bias and requires less labelled data than generating keyword sentiment. Results are calculated on politically charged words as well as control words to show how conclusions can be drawn. The complete method facilitates the extraction of nuanced insights with minimal assumptions and categorizations, paving the way for a more objective understanding of bias within student newspaper sources.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了一个极少人工干预的管道，用于抓取和检测大学报纸存档中的偏见。该管道引入了一个自动化工具无法抓取数据的复杂存档网站的框架，并生成了14份学生报纸，共计23154个项目。这些数据可以通过关键词查询来计算偏见，并比较大语言模型总结的情感与原文。这种方法的优点包括减少比较偏见和需要 menos标注数据，相比于生成关键词情感。结果分析了政治敏感词和控制词，以示如何得出结论。该完整的方法可以帮助抽取准确的偏见情况，减少假设和分类，为大学报纸来源中的偏见问题提供更Objective的理解。
</details></li>
</ul>
<hr>
<h2 id="Enabling-Runtime-Verification-of-Causal-Discovery-Algorithms-with-Automated-Conditional-Independence-Reasoning-Extended-Version"><a href="#Enabling-Runtime-Verification-of-Causal-Discovery-Algorithms-with-Automated-Conditional-Independence-Reasoning-Extended-Version" class="headerlink" title="Enabling Runtime Verification of Causal Discovery Algorithms with Automated Conditional Independence Reasoning (Extended Version)"></a>Enabling Runtime Verification of Causal Discovery Algorithms with Automated Conditional Independence Reasoning (Extended Version)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05264">http://arxiv.org/abs/2309.05264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pingchuan Ma, Zhenlan Ji, Peisen Yao, Shuai Wang, Kui Ren</li>
<li>for: 这个论文的目的是提出一种可靠和安全的 causal discovery 算法，以满足可靠性和隐私性两个方面的要求。</li>
<li>methods: 这个论文使用了一种名为 CICheck 的运行时验证工具，该工具可以帮助检测 causal discovery 算法中的不可靠和过多 CI 测试，并提供一种有效的解决方案。 CICheck 使用了一种声明式的编码方案，将 CIR 问题转化为 SMT 问题，并提供了一种四个阶段的决策过程，以及三种轻量级优化技术来提高效率。</li>
<li>results: 这个论文的实验结果表明，CICheck 可以帮助提高 causal discovery 算法的可靠性和隐私性，并且可以减少过多的 CI 测试数量。<details>
<summary>Abstract</summary>
Causal discovery is a powerful technique for identifying causal relationships among variables in data. It has been widely used in various applications in software engineering. Causal discovery extensively involves conditional independence (CI) tests. Hence, its output quality highly depends on the performance of CI tests, which can often be unreliable in practice. Moreover, privacy concerns arise when excessive CI tests are performed.   Despite the distinct nature between unreliable and excessive CI tests, this paper identifies a unified and principled approach to addressing both of them. Generally, CI statements, the outputs of CI tests, adhere to Pearl's axioms, which are a set of well-established integrity constraints on conditional independence. Hence, we can either detect erroneous CI statements if they violate Pearl's axioms or prune excessive CI statements if they are logically entailed by Pearl's axioms. Holistically, both problems boil down to reasoning about the consistency of CI statements under Pearl's axioms (referred to as CIR problem).   We propose a runtime verification tool called CICheck, designed to harden causal discovery algorithms from reliability and privacy perspectives. CICheck employs a sound and decidable encoding scheme that translates CIR into SMT problems. To solve the CIR problem efficiently, CICheck introduces a four-stage decision procedure with three lightweight optimizations that actively prove or refute consistency, and only resort to costly SMT-based reasoning when necessary. Based on the decision procedure to CIR, CICheck includes two variants: ED-CICheck and ED-CICheck, which detect erroneous CI tests (to enhance reliability) and prune excessive CI tests (to enhance privacy), respectively. [abridged due to length limit]
</details>
<details>
<summary>摘要</summary>
causal discovery 是一种 poderful technique for identifying causal relationships among variables in data. It has been widely used in various applications in software engineering. Causal discovery Extensively involves conditional independence (CI) tests. Hence, its output quality highly depends on the performance of CI tests, which can often be unreliable in practice. Moreover, privacy concerns arise when excessive CI tests are performed. Despite the distinct nature between unreliable and excessive CI tests, this paper identifies a unified and principled approach to addressing both of them. Generally, CI statements, the outputs of CI tests, adhere to Pearl's axioms, which are a set of well-established integrity constraints on conditional independence. Hence, we can either detect erroneous CI statements if they violate Pearl's axioms or prune excessive CI statements if they are logically entailed by Pearl's axioms. Holistically, both problems boil down to reasoning about the consistency of CI statements under Pearl's axioms (referred to as CIR problem). We propose a runtime verification tool called CICheck, designed to harden causal discovery algorithms from reliability and privacy perspectives. CICheck employs a sound and decidable encoding scheme that translates CIR into SMT problems. To solve the CIR problem efficiently, CICheck introduces a four-stage decision procedure with three lightweight optimizations that actively prove or refute consistency, and only resort to costly SMT-based reasoning when necessary. Based on the decision procedure to CIR, CICheck includes two variants: ED-CICheck and ED-CICheck, which detect erroneous CI tests (to enhance reliability) and prune excessive CI tests (to enhance privacy), respectively.
</details></li>
</ul>
<hr>
<h2 id="Brain-inspired-Evolutionary-Architectures-for-Spiking-Neural-Networks"><a href="#Brain-inspired-Evolutionary-Architectures-for-Spiking-Neural-Networks" class="headerlink" title="Brain-inspired Evolutionary Architectures for Spiking Neural Networks"></a>Brain-inspired Evolutionary Architectures for Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05263">http://arxiv.org/abs/2309.05263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Pan, Feifei Zhao, Zhuoya Zhao, Yi Zeng</li>
<li>For: This paper explores the evolutionary mechanisms of biological neural networks in the human brain and applies them to optimize the architecture of Spiking Neural Networks (SNNs).* Methods: The paper proposes an efficient multi-objective evolutionary algorithm based on a few-shot performance predictor to evolve SNNs architecture, incorporating brain-inspired local modular structure and global cross-module connectivity.* Results: The proposed model achieves high performance, efficiency, and low energy consumption on various datasets, including static and neuromorphic datasets. The results demonstrate the effectiveness of the brain-inspired approach to SNNs architecture optimization.Here’s the full text in Simplified Chinese:* For: 这篇论文探索了人脑中生物神经网络的进化机制，并应用其来优化神经网络（SNNs）的建立。* Methods: 论文提出了一种高效的多目标进化算法，基于几个shot性能预测器来演化SNNs的建立，包括脑Region-inspired模块和全模块连接。* Results: 提议的模型在不同的数据集上（CIFAR10、CIFAR100、CIFAR10-DVS、DVS128-Gesture）实现了高性能、高效率和低能耗。结果表明，人脑中的进化机制对SNNs建立具有很好的应用前景。<details>
<summary>Abstract</summary>
The complex and unique neural network topology of the human brain formed through natural evolution enables it to perform multiple cognitive functions simultaneously. Automated evolutionary mechanisms of biological network structure inspire us to explore efficient architectural optimization for Spiking Neural Networks (SNNs). Instead of manually designed fixed architectures or hierarchical Network Architecture Search (NAS), this paper evolves SNNs architecture by incorporating brain-inspired local modular structure and global cross-module connectivity. Locally, the brain region-inspired module consists of multiple neural motifs with excitatory and inhibitory connections; Globally, we evolve free connections among modules, including long-term cross-module feedforward and feedback connections. We further introduce an efficient multi-objective evolutionary algorithm based on a few-shot performance predictor, endowing SNNs with high performance, efficiency and low energy consumption. Extensive experiments on static datasets (CIFAR10, CIFAR100) and neuromorphic datasets (CIFAR10-DVS, DVS128-Gesture) demonstrate that our proposed model boosts energy efficiency, archiving consistent and remarkable performance. This work explores brain-inspired neural architectures suitable for SNNs and also provides preliminary insights into the evolutionary mechanisms of biological neural networks in the human brain.
</details>
<details>
<summary>摘要</summary>
人脑的复杂和独特神经网络结构，通过自然演化形成，允许它同时执行多种认知功能。我们从生物学上的演化机制中灵感，以便为神经网络算法（SNN）进行有效的建筑优化。而不是手动设计固定的结构或层次Network Architecture Search（NAS），我们在这篇论文中通过启用脑Region-inspired模块和全模块连接来进行SNNs的架构演化。本地，脑区域灵感模块包括多种神经元模式，以及兴奋和抑制连接；全球，我们演化模块之间的自由连接，包括长期跨模块的前向和反向连接。此外，我们还提出了一种高效的多目标进化算法，基于几何性表现预测器，使得SNNs具有高性能、高效和低能耗特点。通过对静止数据集（CIFAR10、CIFAR100）和neuromorphic数据集（CIFAR10-DVS、DVS128-Gesture）的广泛实验，我们的提出的模型提高了能效率，实现了一致性和很好的性能。这种工作探索了适合SNNs的脑神经网络架构，同时也提供了生物学上神经网络的演化机制的初步启示。
</details></li>
</ul>
<hr>
<h2 id="Generating-Natural-Language-Queries-for-More-Effective-Systematic-Review-Screening-Prioritisation"><a href="#Generating-Natural-Language-Queries-for-More-Effective-Systematic-Review-Screening-Prioritisation" class="headerlink" title="Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation"></a>Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05238">http://arxiv.org/abs/2309.05238</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ielab/sigir-ap-2023-bolean2natural4sr">https://github.com/ielab/sigir-ap-2023-bolean2natural4sr</a></li>
<li>paper_authors: Shuai Wang, Harrisen Scells, Martin Potthast, Bevan Koopman, Guido Zuccon</li>
<li>for: 医学系统atic review的层次化检索优化，以提高检索效率和效果。</li>
<li>methods: 使用Boolean查询和生成式大语言模型如ChatGPT和Alpaca生成的查询来优化层次化检索。</li>
<li>results: 提出了一种实用和有效的层次化检索方法，与最终标题相似效果。<details>
<summary>Abstract</summary>
Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. The goal is to prioritise the most important documents so that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review to rank documents using BERT-based neural neural rankers. However, the final title is only formulated at the end of the review process, which makes this approach impractical as it relies on ex post facto information. At the time of screening, only a rough working title is available, with which the BERT-based ranker achieves is significantly worse than the final title. In this paper, we explore alternative sources of queries for screening prioritisation, such as the Boolean query used to retrieve the set of documents to be screened, and queries generated by instruction-based generative large language models such as ChatGPT and Alpaca. Our best approach is not only practical based on the information available at screening time, but is similar in effectiveness with the final title.
</details>
<details>
<summary>摘要</summary>
屏选优化在医疗系统atic reviews中的目的是将检索得到的文档集中分类和排序。目标是通过更高效和有效的方式来快速审核文档，以便更好地使用后续的审核步骤。现状态的最佳实践是使用审核的最终标题使用BERT基于神经网络来排序文档。但是，最终标题只在审核过程的末尾确定，这使得这种方法不实用，因为它基于后审核的信息。在屏选过程中，只有一个粗略的工作标题可用，BERT基于排序器在使用这个工作标题时表现较差。在这篇论文中，我们探讨了屏选优化中的其他查询来源，例如用于检索文档集的 Boolean 查询和基于指令生成的大语言模型如ChatGPT和Alpaca生成的查询。我们的最佳方法不仅是基于屏选时可用的信息实现的实用，而且与最终标题的效果相似。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Natural-Language-Biases-with-Prompt-based-Learning"><a href="#Detecting-Natural-Language-Biases-with-Prompt-based-Learning" class="headerlink" title="Detecting Natural Language Biases with Prompt-based Learning"></a>Detecting Natural Language Biases with Prompt-based Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05227">http://arxiv.org/abs/2309.05227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abdul Aowal, Maliha T Islam, Priyanka Mary Mammen, Sandesh Shetty</li>
<li>for: 本研究探讨新兴领域的提问工程，并应用其在语言模型偏见检测任务中。</li>
<li>methods: 本研究使用手动制作的提问来检测语言模型中的四种偏见：性别、种族、性 orientation和 religión-based。</li>
<li>results: 本研究通过对BERT、RoBERTa和T5多种版本进行评估，并通过人工判断和模型自身判断来评估这些模型的偏见。<details>
<summary>Abstract</summary>
In this project, we want to explore the newly emerging field of prompt engineering and apply it to the downstream task of detecting LM biases. More concretely, we explore how to design prompts that can indicate 4 different types of biases: (1) gender, (2) race, (3) sexual orientation, and (4) religion-based. Within our project, we experiment with different manually crafted prompts that can draw out the subtle biases that may be present in the language model. We apply these prompts to multiple variations of popular and well-recognized models: BERT, RoBERTa, and T5 to evaluate their biases. We provide a comparative analysis of these models and assess them using a two-fold method: use human judgment to decide whether model predictions are biased and utilize model-level judgment (through further prompts) to understand if a model can self-diagnose the biases of its own prediction.
</details>
<details>
<summary>摘要</summary>
在这个项目中，我们想要探索新兴的提问工程领域，并将其应用于语言模型偏见检测下游任务。更具体地说，我们探索如何设计提问，以便可以检测语言模型中的4种类型偏见：（1）性别、（2）种族、（3）性 orientation和（4）宗教基础。在我们的项目中，我们对不同的手动制作提问进行了实验，以检测语言模型中的潜在偏见。我们将这些提问应用于多个流行的和广泛认可的模型：BERT、RoBERTa和T5，并对这些模型进行评估。我们采用了两种方法来评估这些模型：通过人类判断是否存在偏见，并通过进一步的提问来理解模型是否可以自我诊断其预测中的偏见。
</details></li>
</ul>
<hr>
<h2 id="SparseSwin-Swin-Transformer-with-Sparse-Transformer-Block"><a href="#SparseSwin-Swin-Transformer-with-Sparse-Transformer-Block" class="headerlink" title="SparseSwin: Swin Transformer with Sparse Transformer Block"></a>SparseSwin: Swin Transformer with Sparse Transformer Block</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05224">http://arxiv.org/abs/2309.05224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krisnapinasthika/sparseswin">https://github.com/krisnapinasthika/sparseswin</a></li>
<li>paper_authors: Krisna Pinasthika, Blessius Sheldo Putra Laksono, Riyandi Banovbi Putera Irsal, Syifa Hukma Shabiyya, Novanto Yudistira</li>
<li>for: 降低 transformer 架构中参数数量，以提高计算效率。</li>
<li>methods: 提出了 Sparse Transformer（SparTa）块，将 transformer 块中的TokenConverter添加了一个稀疏化器，以降低 Token 的数量。</li>
<li>results: 在 ImageNet100、CIFAR10 和 CIFAR100 数据集上，提出的 SparseSwin 模型与其他状态的艺术模型相比，具有更高的准确率：86.96%、97.43% 和 85.35%。<details>
<summary>Abstract</summary>
Advancements in computer vision research have put transformer architecture as the state of the art in computer vision tasks. One of the known drawbacks of the transformer architecture is the high number of parameters, this can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and in turn, made the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the number of tokens used. We use the SparTa Block inside the Swin T architecture (SparseSwin) to leverage Swin capability to downsample its input and reduce the number of initial tokens to be calculated. The proposed SparseSwin model outperforms other state of the art models in image classification with an accuracy of 86.96%, 97.43%, and 85.35% on the ImageNet100, CIFAR10, and CIFAR100 datasets respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)计算机视觉研究的进步使得转换器架构成为计算机视觉任务的状态体系。 however, one of the known drawbacks of the transformer architecture is the high number of parameters, which can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and make the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the number of tokens used. We use the SparTa Block inside the Swin T architecture (SparseSwin) to leverage Swin's ability to downsample its input and reduce the number of initial tokens to be calculated. The proposed SparseSwin model outperforms other state-of-the-art models in image classification with an accuracy of 86.96%, 97.43%, and 85.35% on the ImageNet100, CIFAR10, and CIFAR100 datasets, respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance.
</details></li>
</ul>
<hr>
<h2 id="Circle-Feature-Graphormer-Can-Circle-Features-Stimulate-Graph-Transformer"><a href="#Circle-Feature-Graphormer-Can-Circle-Features-Stimulate-Graph-Transformer" class="headerlink" title="Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?"></a>Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06574">http://arxiv.org/abs/2309.06574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingsonglv/CFG">https://github.com/jingsonglv/CFG</a></li>
<li>paper_authors: Jingsong Lv, Hongyang Chen, Yao Qi, Lei Yu</li>
<li>for: 这个论文主要针对缺失链接预测任务，具体来说是使用圈子特征来提高图Transformer神经网络的性能。</li>
<li>methods: 该论文引入了两种本地图像特征，即圈子特征和桥特征，这些特征来自圈子朋友的概念。论文还提出了这些特征的详细计算方法。</li>
<li>results: 实验结果表明，使用圈子特征改进图Transformer神经网络后，可以达到 dataset ogbl-citation2 上最佳性能。<details>
<summary>Abstract</summary>
In this paper, we introduce two local graph features for missing link prediction tasks on ogbl-citation2. We define the features as Circle Features, which are borrowed from the concept of circle of friends. We propose the detailed computing formulas for the above features. Firstly, we define the first circle feature as modified swing for common graph, which comes from bipartite graph. Secondly, we define the second circle feature as bridge, which indicates the importance of two nodes for different circle of friends. In addition, we firstly propose the above features as bias to enhance graph transformer neural network, such that graph self-attention mechanism can be improved. We implement a Circled Feature aware Graph transformer (CFG) model based on SIEG network, which utilizes a double tower structure to capture both global and local structure features. Experimental results show that CFG achieves the state-of-the-art performance on dataset ogbl-citation2.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了两种本地图像特征用于缺失链接预测任务中的ogbl-citation2。我们定义这些特征为圈形特征，它们取自社交圈的概念。我们提出了计算这些特征的详细计算公式。首先，我们定义第一个圈形特征为修改的摆动，它来自二分图。其次，我们定义第二个圈形特征为桥梁，它表示两个节点在不同的社交圈中的重要性。此外，我们首次提出了这些特征作为偏好，以便通过提高图像自注意机制来改进图像 transformer 神经网络。我们实现了基于SIEG网络的圈形特征意识 Graph transformer（CFG）模型，它使用双塔结构来捕捉全球和本地结构特征。实验结果表明，CFG在 dataset ogbl-citation2 上达到了状态 искусственный智能的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-and-Attributing-the-Hallucination-of-Large-Language-Models-via-Association-Analysis"><a href="#Quantifying-and-Attributing-the-Hallucination-of-Large-Language-Models-via-Association-Analysis" class="headerlink" title="Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis"></a>Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05217">http://arxiv.org/abs/2309.05217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Du, Yequan Wang, Xingrun Xing, Yiqun Ya, Xiang Li, Xin Jiang, Xuezhi Fang</li>
<li>for:  measure the level of hallucination of large language models (LLMs) and investigate the reasons for hallucination</li>
<li>methods:  combine hallucination level quantification and hallucination reason investigation through association analysis, and recognize risk factors according to a taxonomy of model capability</li>
<li>results:  reveal potential deficiencies in commonsense memorization, relational reasoning, and instruction following, and provide guidance for pretraining and supervised fine-tuning process to mitigate hallucination<details>
<summary>Abstract</summary>
Although demonstrating superb performance on various NLP tasks, large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs. To measure the level of hallucination of LLMs, previous works first categorize the hallucination according to the phenomenon similarity, then quantify the proportion that model outputs contain hallucinatory contents. However, such hallucination rates could easily be distorted by confounders. Moreover, such hallucination rates could not reflect the reasons for the hallucination, as similar hallucinatory phenomena may originate from different sources. To address these issues, we propose to combine the hallucination level quantification and hallucination reason investigation through an association analysis, which builds the relationship between the hallucination rate of LLMs with a set of risk factors. In this way, we are able to observe the hallucination level under each value of each risk factor, examining the contribution and statistical significance of each risk factor, meanwhile excluding the confounding effect of other factors. Additionally, by recognizing the risk factors according to a taxonomy of model capability, we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination.
</details>
<details>
<summary>摘要</summary>
To address these issues, we propose an association analysis to investigate the relationship between the hallucination rate of LLMs and a set of risk factors. This approach allows us to observe the hallucination level under each value of each risk factor, while controlling for the confounding effect of other factors. Additionally, by categorizing risk factors according to a taxonomy of model capability, we can identify potential deficiencies in commonsense memorization, relational reasoning, and instruction following. These findings can provide guidance for pretraining and supervised fine-tuning of LLMs to mitigate hallucination.Translated into Simplified Chinese:尽管大型语言模型（LLM）在各种自然语言处理（NLP）任务上表现出色，但它们仍然受到幻觉问题的威胁，这影响了其可靠性。为了衡量LLM幻觉水平，先前的研究首先将幻觉分类为相似现象类型，然后量化模型输出中幻觉内容的比例。然而，这些方法容易受到外部因素的污染，并且无法反映幻觉的原因，因为相似的幻觉现象可能来自不同的来源。为了解决这些问题，我们提议结合幻觉水平量化和幻觉原因调查，通过关系分析建立LLM幻觉水平与风险因素之间的关系。这种方法允许我们在每个风险因素值下观察幻觉水平，同时控制其他因素的污染效应。此外，通过将风险因素分类为模型能力稳定的分类，我们可以特别提出一些可能的幻觉原因，如智能记忆、关系理解和行为追踪等。这些发现可以为LLM的预训练和监督细化进程提供指导，以避免幻觉。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Audio-Visual-Information-Fusion-with-Multi-label-Joint-Decoding-for-MER-2023"><a href="#Hierarchical-Audio-Visual-Information-Fusion-with-Multi-label-Joint-Decoding-for-MER-2023" class="headerlink" title="Hierarchical Audio-Visual Information Fusion with Multi-label Joint Decoding for MER 2023"></a>Hierarchical Audio-Visual Information Fusion with Multi-label Joint Decoding for MER 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07925">http://arxiv.org/abs/2309.07925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Wang, Yuxuan Xi, Hang Chen, Jun Du, Yan Song, Qing Wang, Hengshun Zhou, Chenxi Wang, Jiefeng Ma, Pengfei Hu, Ya Jiang, Shi Cheng, Jie Zhang, Yuzhe Weng</li>
<li>for: 本文提出了一种新的情感识别框架，可以同时识别精度和维度的情感。</li>
<li>methods: 该框架使用深度特征从基础模型中提取的深度特征作为Raw视频的Robust音频和视觉表示。然后，我们设计了三种基于注意力导航的特征聚合结构，用于深度特征融合。在解码阶段，我们引入了共同解码结构 для情感分类和抑制 regression。最后，我们通过将三种结构联合在 posterior probability 水平上，得到了最终的精度和维度情感预测。</li>
<li>results: 在Multimodal Emotion Recognition Challenge (MER 2023) 数据集上测试，我们提出的框架实现了精度和抑制 regression 的同时提高。我们的最终系统在 MER-MULTI 子挑战中取得了状态的最佳表现，并在 leaderboard 上排名第三。<details>
<summary>Abstract</summary>
In this paper, we propose a novel framework for recognizing both discrete and dimensional emotions. In our framework, deep features extracted from foundation models are used as robust acoustic and visual representations of raw video. Three different structures based on attention-guided feature gathering (AFG) are designed for deep feature fusion. Then, we introduce a joint decoding structure for emotion classification and valence regression in the decoding stage. A multi-task loss based on uncertainty is also designed to optimize the whole process. Finally, by combining three different structures on the posterior probability level, we obtain the final predictions of discrete and dimensional emotions. When tested on the dataset of multimodal emotion recognition challenge (MER 2023), the proposed framework yields consistent improvements in both emotion classification and valence regression. Our final system achieves state-of-the-art performance and ranks third on the leaderboard on MER-MULTI sub-challenge.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的框架，用于识别不同类型的情感。我们的框架使用基于基础模型的深度特征来生成Robust的音频和视觉表示。然后，我们设计了三种基于注意力引导的特征聚合结构，用于深度特征融合。在解码阶段，我们引入了共同解码结构用于情感分类和价值评分。此外，我们还设计了基于不确定性的多任务损失函数来优化整个过程。最后，我们将三种不同的结构联合在 posterior probability 水平上，从而获得最终的情感分类和价值评分预测结果。在MER 2023数据集上测试，我们的提议的框架实现了顺利的提高，包括情感分类和价值评分。最终，我们的系统在MER-MULTI子挑战中 ranked third，并达到了状态机器人的表现。
</details></li>
</ul>
<hr>
<h2 id="Towards-Federated-Learning-Under-Resource-Constraints-via-Layer-wise-Training-and-Depth-Dropout"><a href="#Towards-Federated-Learning-Under-Resource-Constraints-via-Layer-wise-Training-and-Depth-Dropout" class="headerlink" title="Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout"></a>Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05213">http://arxiv.org/abs/2309.05213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Guo, Warren Richard Morningstar, Raviteja Vemulapalli, Karan Singhal, Vishal M. Patel, Philip Andrew Mansfield</li>
<li>for: 这篇论文旨在探讨如何使用 Federated Layer-wise Learning 和 Federated Depth Dropout 技术来训练大型机器学习模型，以便在边缘设备上进行训练。</li>
<li>methods: 本研究使用 Federated Layer-wise Learning 和 Federated Depth Dropout 技术，实现了降低每个客户端的内存、计算和通信成本的目的。</li>
<li>results: 研究发现，这两种技术可以同时降低训练内存使用量，并且不会对模型的性能造成重要干扰。 Specifically, 在 Federated self-supervised representation learning 中，训练内存使用量被降低了5倍或更多，而模型在下游任务中的表现与传统 Federated self-supervised learning 相似。<details>
<summary>Abstract</summary>
Large machine learning models trained on diverse data have recently seen unprecedented success. Federated learning enables training on private data that may otherwise be inaccessible, such as domain-specific datasets decentralized across many clients. However, federated learning can be difficult to scale to large models when clients have limited resources. This challenge often results in a trade-off between model size and access to diverse data. To mitigate this issue and facilitate training of large models on edge devices, we introduce a simple yet effective strategy, Federated Layer-wise Learning, to simultaneously reduce per-client memory, computation, and communication costs. Clients train just a single layer each round, reducing resource costs considerably with minimal performance degradation. We also introduce Federated Depth Dropout, a complementary technique that randomly drops frozen layers during training, to further reduce resource usage. Coupling these two techniques enables us to effectively train significantly larger models on edge devices. Specifically, we reduce training memory usage by 5x or more in federated self-supervised representation learning and demonstrate that performance in downstream tasks is comparable to conventional federated self-supervised learning.
</details>
<details>
<summary>摘要</summary>
大型机器学习模型在多样化数据的训练下最近见到了历史性的成功。联邦学习可以训练在私人数据上，这些数据可能 Otherwise 分散在多个客户端上。然而，联邦学习在大型模型训练时可能会受到限制，导致模型大小和数据多样性之间的复杂负载。为解决这个问题并在边缘设备上训练大型模型，我们介绍了一个简单 yet 有效的策略：联邦层别学习。在每个回合中，客户端只需要训练一层，这有很大的降低了客户端的资源成本。我们还介绍了联邦深度掉擦，一个与之相伴的技术，在训练过程中随机删除冻结层，进一步降低资源使用率。通过结合这两种技术，我们可以有效地在边缘设备上训练更大的模型，尤其是在自我监督学习中。具体来说，我们可以将训练内存使用量降低至5倍以上，并且在下游任务中表现与传统联邦自我监督学习相似。
</details></li>
</ul>
<hr>
<h2 id="Data-Summarization-beyond-Monotonicity-Non-monotone-Two-Stage-Submodular-Maximization"><a href="#Data-Summarization-beyond-Monotonicity-Non-monotone-Two-Stage-Submodular-Maximization" class="headerlink" title="Data Summarization beyond Monotonicity: Non-monotone Two-Stage Submodular Maximization"></a>Data Summarization beyond Monotonicity: Non-monotone Two-Stage Submodular Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05183">http://arxiv.org/abs/2309.05183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaojie Tang</li>
<li>for: 降低基aset中的元素数量，使新的目标函数优化过基aset中剩下的元素可以达到与原始基aset中的结果相似的效果。</li>
<li>methods: 使用提供的培训函数，这些函数都是具有减少基aset的优化目标，并且引入了扩展现有研究中假设的不寻常 monotonicity 的非 monotone 优化方法。</li>
<li>results: 引入了首个常数系数approximation算法，用于解决更一般的二stage submodular maximization问题。<details>
<summary>Abstract</summary>
The objective of a two-stage submodular maximization problem is to reduce the ground set using provided training functions that are submodular, with the aim of ensuring that optimizing new objective functions over the reduced ground set yields results comparable to those obtained over the original ground set. This problem has applications in various domains including data summarization. Existing studies often assume the monotonicity of the objective function, whereas our work pioneers the extension of this research to accommodate non-monotone submodular functions. We have introduced the first constant-factor approximation algorithms for this more general case.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这个两个阶段的优化问题的目标是使用提供的训练函数来减少基aset，以确保优化新的目标函数在减少后的基aset上达到原始基aset上的结果相似。这个问题在不同领域，如数据概要中有应用。现有的研究通常假设目标函数升序，而我们的研究则是扩展这个研究来包括非升序的优化函数。我们已经提出了首个常量系数近似算法 для这种更一般的情况。
</details></li>
</ul>
<hr>
<h2 id="Our-Deep-CNN-Face-Matchers-Have-Developed-Achromatopsia"><a href="#Our-Deep-CNN-Face-Matchers-Have-Developed-Achromatopsia" class="headerlink" title="Our Deep CNN Face Matchers Have Developed Achromatopsia"></a>Our Deep CNN Face Matchers Have Developed Achromatopsia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05180">http://arxiv.org/abs/2309.05180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aman Bhatta, Domingo Mery, Haiyu Wu, Joyce Annan, Micheal C. King, Kevin W. Bowyer</li>
<li>for: 这个论文旨在证明现代深度学习面部匹配器在灰度图像和彩色图像上的匹配精度是相同的。</li>
<li>methods: 这个论文使用了深度学习面部匹配器，并对其在灰度图像和彩色图像上的性能进行了分析。</li>
<li>results: 论文发现，使用灰度图像进行训练并不会影响深度学习面部匹配器的匹配精度，而且可以使用单通道灰度图像进行训练，从而减少计算量并使用更大的数据集。<details>
<summary>Abstract</summary>
Modern deep CNN face matchers are trained on datasets containing color images. We show that such matchers achieve essentially the same accuracy on the grayscale or the color version of a set of test images. We then consider possible causes for deep CNN face matchers ``not seeing color''. Popular web-scraped face datasets actually have 30 to 60\% of their identities with one or more grayscale images. We analyze whether this grayscale element in the training set impacts the accuracy achieved, and conclude that it does not. Further, we show that even with a 100\% grayscale training set, comparable accuracy is achieved on color or grayscale test images. Then we show that the skin region of an individual's images in a web-scraped training set exhibit significant variation in their mapping to color space. This suggests that color, at least for web-scraped, in-the-wild face datasets, carries limited identity-related information for training state-of-the-art matchers. Finally, we verify that comparable accuracy is achieved from training using single-channel grayscale images, implying that a larger dataset can be used within the same memory limit, with a less computationally intensive early layer.
</details>
<details>
<summary>摘要</summary>
现代深度 CNN 脸Recognizer 通常在颜色图像上训练。我们显示，这些Matcher 在颜色版本或灰度版本的测试图像上具有基本相同的准确率。然后我们考虑了深度 CNN 脸Recognizer "不看到颜色" 的可能性。流行的网络抓取 face 数据集实际上有 30% 到 60% 的个体图像包含一个或多个灰度图像。我们分析了这些灰度元素在训练集中对准确率的影响，并结论是没有影响。进一步，我们表明，即使使用 100% 灰度训练集，在颜色或灰度测试图像上也可以达到相同的准确率。然后我们显示了网络抓取的人脸训练集中个体皮肤区域的颜色空间中的变化，这表明，至少对于网络抓取的人脸数据集，颜色对于训练 state-of-the-art Matcher 来说带来了有限的个体信息。最后，我们证明了通过单通道灰度图像训练，可以达到相同的准确率，这意味着可以使用更大的数据集，在同样的内存限制下，使用更加计算机易于的早期层。
</details></li>
</ul>
<hr>
<h2 id="DePT-Decomposed-Prompt-Tuning-for-Parameter-Efficient-Fine-tuning"><a href="#DePT-Decomposed-Prompt-Tuning-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning"></a>DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05173">http://arxiv.org/abs/2309.05173</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhengxiangshi/dept">https://github.com/zhengxiangshi/dept</a></li>
<li>paper_authors: Zhengxiang Shi, Aldo Lipani</li>
<li>for: 本研究旨在提高语言模型（LM）的参数效率，通过在输入中附加小量可训练的软提示 вектор（PT）进行微调（PEFT）。</li>
<li>methods: 本研究使用的方法是分解软提示（DePT），即将软提示分解成更短的软提示和一对低级矩阵，然后通过两个不同的学习率进行优化。</li>
<li>results: 对于23种自然语言处理（NLP）和视觉语言（VL）任务，我们的实验结果表明，DePT比其他PEFT方法更高效，并且在某些场景下甚至超过了基线微调方法。此外，我们还发现DePT随模型大小增长而变得更加高效。<details>
<summary>Abstract</summary>
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance while saving over 20% memory and time costs compared to vanilla PT and its variants, without changing trainable parameter sizes. Through extensive experiments on 23 natural language processing (NLP) and vision-language (VL) tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches, including the full fine-tuning baseline in some scenarios. Additionally, we empirically show that DEPT grows more efficient as the model size increases. Our further study reveals that DePT integrates seamlessly with parameter-efficient transfer learning in the few-shot learning setting and highlights its adaptability to various model architectures and sizes.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/cs.AI_2023_09_11/" data-id="clp869tqy003pk5889mmbae9q" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/cs.CL_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T11:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/11/cs.CL_2023_09_11/">cs.CL - 2023-09-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hi-Model-generating-‘nice’-instead-of-‘good’-is-not-as-bad-as-generating-‘rice’-Towards-Context-and-Semantic-Infused-Dialogue-Generation-Loss-Function-and-Evaluation-Metric"><a href="#Hi-Model-generating-‘nice’-instead-of-‘good’-is-not-as-bad-as-generating-‘rice’-Towards-Context-and-Semantic-Infused-Dialogue-Generation-Loss-Function-and-Evaluation-Metric" class="headerlink" title="Hi Model, generating ‘nice’ instead of ‘good’ is not as bad as generating ‘rice’! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric"></a>Hi Model, generating ‘nice’ instead of ‘good’ is not as bad as generating ‘rice’! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05804">http://arxiv.org/abs/2309.05804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhisek Tiwari, Muhammed Sinan, Kaushik Roy, Amit Sheth, Sriparna Saha, Pushpak Bhattacharyya</li>
<li>for: 本研究旨在提出一种新的对话生成损失函数和评价指标，以改进对话生成模型的评价和优化。</li>
<li>methods: 本研究使用了新的Semantic Infused Contextualized diaLogue（SemTextualLogue）损失函数和Dialuation评价指标，并在两个对话数据集上进行了实验，包括任务对话和开放对话场景。</li>
<li>results: 研究发现，使用SemTextualLogue损失函数和Dialuation指标进行训练，对话生成模型的性能有显著提升，比传统的cross-entropy损失函数更能够评价对话生成模型的表现。<details>
<summary>Abstract</summary>
Over the past two decades, dialogue modeling has made significant strides, moving from simple rule-based responses to personalized and persuasive response generation. However, despite these advancements, the objective functions and evaluation metrics for dialogue generation have remained stagnant, i.e., cross-entropy and BLEU, respectively. These lexical-based metrics have the following key limitations: (a) word-to-word matching without semantic consideration: It assigns the same credit for failure to generate 'nice' and 'rice' for 'good'. (b) missing context attribute for evaluating the generated response: Even if a generated response is relevant to the ongoing dialogue context, it may still be penalized for not matching the gold utterance provided in the corpus. In this paper, we first investigate these limitations comprehensively and propose a new loss function called Semantic Infused Contextualized diaLogue (SemTextualLogue) loss function. Furthermore, we formulate a new evaluation metric called Dialuation, which incorporates both context relevance and semantic appropriateness while evaluating a generated response. We conducted experiments on two benchmark dialogue corpora, encompassing both task-oriented and open-domain scenarios. We found that the dialogue generation model trained with SemTextualLogue loss attained superior performance (in both quantitative and qualitative evaluation) compared to the traditional cross-entropy loss function across the datasets and evaluation metrics.
</details>
<details>
<summary>摘要</summary>
过去二十年，对话模型化已经做出了 significiant 进步，从简单的规则基于响应演进到个性化和说服性响应生成。然而，虽然这些进步，对话生成的目标函数和评价指标仍然停滞不前，即cross-entropy和BLEU，分别。这些lexical-based 指标具有以下两点限制：（a）word-to-word匹配无semantic考虑：它将生成 'good'和'rice'的不同的响应视为相同的失败。（b）缺少对话上下文特征：即使生成的响应与对话上下文相关，仍可能因为不匹配goldutterance而受到penalty。在这篇论文中，我们首先对这些限制进行了全面的调查，并提出了一种新的损失函数called Semantic Infused Contextualized diaLogue (SemTextualLogue)损失函数。此外，我们提出了一种新的评价指标called Dialuation，该指标包含对话上下文相关性和semantic适用性的两个方面。我们在两个标准对话 corpora上进行了实验，包括任务域和开放域场景。我们发现，使用SemTextualLogue损失函数训练的对话生成模型在所有数据集和评价指标上表现出色，比传统的cross-entropy损失函数更好。
</details></li>
</ul>
<hr>
<h2 id="MAmmoTH-Building-Math-Generalist-Models-through-Hybrid-Instruction-Tuning"><a href="#MAmmoTH-Building-Math-Generalist-Models-through-Hybrid-Instruction-Tuning" class="headerlink" title="MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"></a>MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05653">http://arxiv.org/abs/2309.05653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TIGER-AI-Lab/MAmmoTH">https://github.com/TIGER-AI-Lab/MAmmoTH</a></li>
<li>paper_authors: Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen</li>
<li>for: The paper is written for developing a series of open-source large language models (LLMs) specifically tailored for general math problem-solving.</li>
<li>methods: The paper uses a meticulously curated instruction tuning dataset called MathInstruct, which includes 13 math datasets with intermediate rationales, six of which were newly curated by the authors. The models are trained on this dataset, which presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales.</li>
<li>results: The MAmmoTH series of models substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales, with an average accuracy gain between 13% and 29%. The MAmmoTH-7B model achieves 35% accuracy on MATH, which exceeds the best open-source 7B model (WizardMath) by 25%, and the MAmmoTH-34B model achieves 46% accuracy on MATH, even surpassing GPT-4’s CoT result.<details>
<summary>Abstract</summary>
We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 13% and 29%. Remarkably, our MAmmoTH-7B model reaches 35% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 25%, and the MAmmoTH-34B model achieves 46% accuracy on MATH, even surpassing GPT-4's CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.
</details>
<details>
<summary>摘要</summary>
我们介绍MAmmoTH，一系列开源大型自然语言模型（LLMs），特别针对数学问题的解释。MAmmoTH模型在我们仔细组合的 instruNet 训练集上训练， instruNet 是我们新compile的 13 个数学数据集，其中六个是我们新给出的 rationales。这些 rationales 是一种 chain-of-thought（CoT）和 program-of-thought（PoT）的混合类型，并且涵盖了数学多个领域。这种混合类型不仅发挥工具的潜力，而且允许不同的思维过程，因此 MAmmoTH 系列在九个数学推理数据集上表现出色，具有13% 至 29% 的总精度提升。特别是我们的 MAmmoTH-7B 模型在 MATH 竞赛级数据集上 дости得 35% 的精度，超过了最佳开源 7B 模型（WizardMath）的 25%，而 MAmmoTH-34B 模型在 MATH 上取得 46% 的精度，甚至超过 GPT-4 的 CoT 结果。我们的工作强调了数学多个领域的多元问题覆盖和 hybrid 的 rationales 在开发出色数学通用模型方面的重要性。
</details></li>
</ul>
<hr>
<h2 id="Effective-Proxy-for-Human-Labeling-Ensemble-Disagreement-Scores-in-Large-Language-Models-for-Industrial-NLP"><a href="#Effective-Proxy-for-Human-Labeling-Ensemble-Disagreement-Scores-in-Large-Language-Models-for-Industrial-NLP" class="headerlink" title="Effective Proxy for Human Labeling: Ensemble Disagreement Scores in Large Language Models for Industrial NLP"></a>Effective Proxy for Human Labeling: Ensemble Disagreement Scores in Large Language Models for Industrial NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05619">http://arxiv.org/abs/2309.05619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Du, Laksh Advani, Yashmeet Gambhir, Daniel J Perry, Prashant Shiralkar, Zhengzheng Xing, Aaron Colak</li>
<li>for: 评估大语言模型（LLMs）在实际世界中的性能，以验证其在不同语言和领域中的总体性能。</li>
<li>methods: 使用 ensemble disagreement scores 作为人工标注的代理，以评估 LLM 在零shot、几shot 和 fine-tuned 设置下的性能。</li>
<li>results: 结果表明，使用 ensemble disagreement scores 可以准确地评估 LLM 的性能，与真实的人工标注 Error 相比，MAE 为 0.4% 左右，与使用另一个 LLM 作为机器标注（silver labels）的情况相比，平均提高了 13.8%。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated significant capability to generalize across a large number of NLP tasks. For industry applications, it is imperative to assess the performance of the LLM on unlabeled production data from time to time to validate for a real-world setting. Human labeling to assess model error requires considerable expense and time delay. Here we demonstrate that ensemble disagreement scores work well as a proxy for human labeling for language models in zero-shot, few-shot, and fine-tuned settings, per our evaluation on keyphrase extraction (KPE) task. We measure fidelity of the results by comparing to true error measured from human labeled ground truth. We contrast with the alternative of using another LLM as a source of machine labels, or silver labels. Results across various languages and domains show disagreement scores provide a better estimation of model performance with mean average error (MAE) as low as 0.4% and on average 13.8% better than using silver labels.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展示了广泛的应用准确性。为工业应用，需要定期评估LLM在实际世界数据上的表现，以验证其可行性。人工标注来评估模型错误需要巨大的成本和时间延迟。在本研究中，我们展示了 ensemble disagreement  scores 可以作为人工标注的代理，并在零shot、少shot和 fine-tuned 设定下进行评估。我们通过比较 true error  measured from human labeled ground truth 和 ensemble disagreement  scores 的精度，发现 ensemble disagreement  scores 能够提供更好的模型性能估计，mean average error（MAE）只有0.4%，并且在平均上比 silver labels 高13.8%。 results across various languages and domains 表明，ensemble disagreement  scores 能够提供更好的模型性能估计。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Pre-trained-Model-Prompting-in-Multimodal-Stock-Volume-Movement-Prediction"><a href="#Incorporating-Pre-trained-Model-Prompting-in-Multimodal-Stock-Volume-Movement-Prediction" class="headerlink" title="Incorporating Pre-trained Model Prompting in Multimodal Stock Volume Movement Prediction"></a>Incorporating Pre-trained Model Prompting in Multimodal Stock Volume Movement Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05608">http://arxiv.org/abs/2309.05608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rayruibochen/promuse">https://github.com/rayruibochen/promuse</a></li>
<li>paper_authors: Ruibo Chen, Zhiyuan Zhang, Yi Liu, Ruihan Bao, Keiko Harimoto, Xu Sun</li>
<li>for: 用于预测股票交易量运动的多modal数据movement prediction</li>
<li>methods: 使用预训练语言模型和提示学习方法来处理文本和时间序列模式</li>
<li>results: 比较 existing baselines 表现出色，并通过多种分析 validate 模型的效果<details>
<summary>Abstract</summary>
Multimodal stock trading volume movement prediction with stock-related news is one of the fundamental problems in the financial area. Existing multimodal works that train models from scratch face the problem of lacking universal knowledge when modeling financial news. In addition, the models ability may be limited by the lack of domain-related knowledge due to insufficient data in the datasets. To handle this issue, we propose the Prompt-based MUltimodal Stock volumE prediction model (ProMUSE) to process text and time series modalities. We use pre-trained language models for better comprehension of financial news and adopt prompt learning methods to leverage their capability in universal knowledge to model textual information. Besides, simply fusing two modalities can cause harm to the unimodal representations. Thus, we propose a novel cross-modality contrastive alignment while reserving the unimodal heads beside the fusion head to mitigate this problem. Extensive experiments demonstrate that our proposed ProMUSE outperforms existing baselines. Comprehensive analyses further validate the effectiveness of our architecture compared to potential variants and learning mechanisms.
</details>
<details>
<summary>摘要</summary>
多Modal股票交易量运动预测与股票相关新闻是金融领域的基本问题。现有的多Modal工作都是从头开始训练模型，面临缺乏通用知识的问题。另外，模型的能力可能受到数据集中的域相关知识不充分的限制。为解决这个问题，我们提出了Prompt-based MUltimodal Stock volumE prediction model（ProMUSE）来处理文本和时间序Modalities。我们使用预训练语言模型来更好地理解金融新闻，并采用提问学习方法来利用其在通用知识中的能力来模型文本信息。此外，简单地将两Modalities进行混合可能会对单Modalities的表示带来害。因此，我们提出了一种新的交叉Modalities强制对齐，以保持单Modalities的表示。广泛的实验表明，我们提出的ProMUSE超过了现有的基准值。进一步的分析还证明了我们的architecture的效果与可能的变体和学习机制相比。
</details></li>
</ul>
<hr>
<h2 id="Long-Range-Transformer-Architectures-for-Document-Understanding"><a href="#Long-Range-Transformer-Architectures-for-Document-Understanding" class="headerlink" title="Long-Range Transformer Architectures for Document Understanding"></a>Long-Range Transformer Architectures for Document Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05503">http://arxiv.org/abs/2309.05503</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thibaultdouzon/long-range-document-transformer">https://github.com/thibaultdouzon/long-range-document-transformer</a></li>
<li>paper_authors: Thibault Douzon, Stefan Duffner, Christophe Garcia, Jérémy Espinas</li>
<li>for: 这篇论文旨在应用Transformer模型于长multi-page文档处理中。</li>
<li>methods: 该论文提出了两种多模态（文本+布局）长距离模型，以及一种2D相对注意力偏好来引导自注意力。</li>
<li>results: 对多页企业文档进行信息检索时，该模型表现出了改善，与小 sequences 的性能成本相对较低。<details>
<summary>Abstract</summary>
Since their release, Transformers have revolutionized many fields from Natural Language Understanding to Computer Vision. Document Understanding (DU) was not left behind with first Transformer based models for DU dating from late 2019. However, the computational complexity of the self-attention operation limits their capabilities to small sequences. In this paper we explore multiple strategies to apply Transformer based models to long multi-page documents. We introduce 2 new multi-modal (text + layout) long-range models for DU. They are based on efficient implementations of Transformers for long sequences. Long-range models can process whole documents at once effectively and are less impaired by the document's length. We compare them to LayoutLM, a classical Transformer adapted for DU and pre-trained on millions of documents. We further propose 2D relative attention bias to guide self-attention towards relevant tokens without harming model efficiency. We observe improvements on multi-page business documents on Information Retrieval for a small performance cost on smaller sequences. Relative 2D attention revealed to be effective on dense text for both normal and long-range models.
</details>
<details>
<summary>摘要</summary>
Since their release, transformers have revolutionized many fields, from natural language understanding to computer vision. Document understanding (DU) was not left behind, with the first transformer-based models for DU dating back to late 2019. However, the computational complexity of the self-attention operation limits their capabilities to small sequences. In this paper, we explore multiple strategies to apply transformer-based models to long multi-page documents. We introduce two new multi-modal (text + layout) long-range models for DU. They are based on efficient implementations of transformers for long sequences. Long-range models can process whole documents at once effectively and are less impaired by the document's length. We compare them to LayoutLM, a classical transformer adapted for DU and pre-trained on millions of documents. We further propose 2D relative attention bias to guide self-attention towards relevant tokens without harming model efficiency. We observe improvements on multi-page business documents on information retrieval for a small performance cost on smaller sequences. Relative 2D attention revealed to be effective on dense text for both normal and long-range models.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Personality-Detection-and-Analysis-using-Twitter-Data"><a href="#Personality-Detection-and-Analysis-using-Twitter-Data" class="headerlink" title="Personality Detection and Analysis using Twitter Data"></a>Personality Detection and Analysis using Twitter Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05497">http://arxiv.org/abs/2309.05497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SRIGURUPRASAD/Trending-Polarity_Diagnosis-Word_Cloud-Profile_analysis">https://github.com/SRIGURUPRASAD/Trending-Polarity_Diagnosis-Word_Cloud-Profile_analysis</a></li>
<li>paper_authors: Abhilash Datta, Souvic Chakraborty, Animesh Mukherjee</li>
<li>for: 这篇论文是为了探讨人格特质自动检测的问题，以及将大量文本数据集用于研究人格类型的可能性。</li>
<li>methods: 本论文使用自动检测人格特质的方法，并对大量文本数据集进行了质量控制和分类。</li>
<li>results: 研究发现，自动检测人格特质的方法可以准确地预测个人的人格类型，并且可以提供有价值的信息用于各种应用领域。<details>
<summary>Abstract</summary>
Personality types are important in various fields as they hold relevant information about the characteristics of a human being in an explainable format. They are often good predictors of a person's behaviors in a particular environment and have applications ranging from candidate selection to marketing and mental health. Recently automatic detection of personality traits from texts has gained significant attention in computational linguistics. Most personality detection and analysis methods have focused on small datasets making their experimental observations often limited. To bridge this gap, we focus on collecting and releasing the largest automatically curated dataset for the research community which has 152 million tweets and 56 thousand data points for the Myers-Briggs personality type (MBTI) prediction task. We perform a series of extensive qualitative and quantitative studies on our dataset to analyze the data patterns in a better way and infer conclusions. We show how our intriguing analysis results often follow natural intuition. We also perform a series of ablation studies to show how the baselines perform for our dataset.
</details>
<details>
<summary>摘要</summary>
人格类型在不同领域具有重要的意义，它们可以带来人类特性的可观察性格。它们经常是人类在特定环境中行为的预测器，并且在选拔候选人、营销和心理健康等领域有广泛的应用。现在，自动检测人格特质从文本中的研究受到了计算语言学的广泛关注。大多数人格检测和分析方法都集中在小 dataset 上，导致其实验观察通常有限。为了bridging这个差距，我们集中在收集和发布最大自动筛选的数据集，这个数据集包含152万篇微博和56千个数据点，用于Myers-Briggs人格类型（MBTI）预测任务。我们进行了系列的详细和量化研究，以分析数据的 patrern 以及得出结论。我们的研究结果经常遵循自然的直觉，并且我们进行了一系列的减少研究，以示baseline 在我们的数据集上的性能。
</details></li>
</ul>
<hr>
<h2 id="CrisisTransformers-Pre-trained-language-models-and-sentence-encoders-for-crisis-related-social-media-texts"><a href="#CrisisTransformers-Pre-trained-language-models-and-sentence-encoders-for-crisis-related-social-media-texts" class="headerlink" title="CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts"></a>CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05494">http://arxiv.org/abs/2309.05494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera</li>
<li>for: This paper is written to address the challenges of analyzing crisis-related social media texts and to introduce an ensemble of pre-trained language models and sentence encoders called CrisisTransformers.</li>
<li>methods: The authors use an extensive corpus of over 15 billion word tokens from tweets associated with more than 30 crisis events to train their models, including BERT and RoBERTa, and evaluate their performance on 18 crisis-specific public datasets.</li>
<li>results: The authors find that their pre-trained models outperform strong baselines across all datasets in classification tasks, and their best-performing sentence encoder improves the state-of-the-art by 17.43% in sentence encoding tasks. Additionally, they investigate the impact of model initialization on convergence and the significance of domain-specific models in generating semantically meaningful sentence embeddings.<details>
<summary>Abstract</summary>
Social media platforms play an essential role in crisis communication, but analyzing crisis-related social media texts is challenging due to their informal nature. Transformer-based pre-trained models like BERT and RoBERTa have shown success in various NLP tasks, but they are not tailored for crisis-related texts. Furthermore, general-purpose sentence encoders are used to generate sentence embeddings, regardless of the textual complexities in crisis-related texts. Advances in applications like text classification, semantic search, and clustering contribute to effective processing of crisis-related texts, which is essential for emergency responders to gain a comprehensive view of a crisis event, whether historical or real-time. To address these gaps in crisis informatics literature, this study introduces CrisisTransformers, an ensemble of pre-trained language models and sentence encoders trained on an extensive corpus of over 15 billion word tokens from tweets associated with more than 30 crisis events, including disease outbreaks, natural disasters, conflicts, and other critical incidents. We evaluate existing models and CrisisTransformers on 18 crisis-specific public datasets. Our pre-trained models outperform strong baselines across all datasets in classification tasks, and our best-performing sentence encoder improves the state-of-the-art by 17.43% in sentence encoding tasks. Additionally, we investigate the impact of model initialization on convergence and evaluate the significance of domain-specific models in generating semantically meaningful sentence embeddings. All models are publicly released (https://huggingface.co/crisistransformers), with the anticipation that they will serve as a robust baseline for tasks involving the analysis of crisis-related social media texts.
</details>
<details>
<summary>摘要</summary>
社交媒体平台在危机通信中发挥了重要作用，但分析危机相关的社交媒体文本具有挑战性，这是因为这些文本的形式不具有正式的特征。BERT和RoBERTa等基于Transformer的预训练模型在不同的自然语言处理任务中显示出了成功，但它们没有特定的针对危机相关文本的训练。此外，通用的句子编码器在处理危机相关文本时会遇到文本复杂性的问题。为了解决危机信息学Literature中的漏洞，本研究提出了危机 трансформа（CrisisTransformers），这是一个基于广泛的危机事件 Tweets 集合（超过 15 亿字符）和多种危机类型的预训练语言模型和句子编码器的ensemble。我们对 existed 模型和危机 трансформа进行了18个危机特定的公共数据集的评估。我们的预训练模型在所有数据集中都高于强基eline，并且我们的最佳句子编码器在句子编码任务中提高了状态艺术的最佳性能 by 17.43%。此外，我们还 investigate了模型初始化对叠入的影响和预训练模型在生成Semantically meaningful句子编码的重要性。所有模型都公开发布（https://huggingface.co/crisistransformers），我们anticipate 它们将作为危机相关社交媒体文本分析任务的稳定基线。
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Learning-with-Minimum-Instruction-to-Extract-Social-Determinants-and-Family-History-from-Clinical-Notes-using-GPT-Model"><a href="#Zero-shot-Learning-with-Minimum-Instruction-to-Extract-Social-Determinants-and-Family-History-from-Clinical-Notes-using-GPT-Model" class="headerlink" title="Zero-shot Learning with Minimum Instruction to Extract Social Determinants and Family History from Clinical Notes using GPT Model"></a>Zero-shot Learning with Minimum Instruction to Extract Social Determinants and Family History from Clinical Notes using GPT Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05475">http://arxiv.org/abs/2309.05475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neel Bhate, Ansh Mittal, Zhe He, Xiao Luo</li>
<li>for: 本研究旨在 investigate Zero-shot learning 方法，以掌握不同条件下的 clinical notes 中的 demographics、社会条件和家族历史信息。</li>
<li>methods: 本研究使用 GPT 模型，并提供 minimum information 来检查模型的性能。</li>
<li>results: 研究结果显示，GPT-3.5 方法在 demographics 抽出中取得了 0.975 F1 的平均分，在 social determinants 抽出中取得了 0.615 F1 的平均分，在 family history 抽出中取得了 0.722 F1 的平均分。<details>
<summary>Abstract</summary>
Demographics, Social determinants of health, and family history documented in the unstructured text within the electronic health records are increasingly being studied to understand how this information can be utilized with the structured data to improve healthcare outcomes. After the GPT models were released, many studies have applied GPT models to extract this information from the narrative clinical notes. Different from the existing work, our research focuses on investigating the zero-shot learning on extracting this information together by providing minimum information to the GPT model. We utilize de-identified real-world clinical notes annotated for demographics, various social determinants, and family history information. Given that the GPT model might provide text different from the text in the original data, we explore two sets of evaluation metrics, including the traditional NER evaluation metrics and semantic similarity evaluation metrics, to completely understand the performance. Our results show that the GPT-3.5 method achieved an average of 0.975 F1 on demographics extraction, 0.615 F1 on social determinants extraction, and 0.722 F1 on family history extraction. We believe these results can be further improved through model fine-tuning or few-shots learning. Through the case studies, we also identified the limitations of the GPT models, which need to be addressed in future research.
</details>
<details>
<summary>摘要</summary>
《人口学、社会决定因素和家庭历史记录在电子健康记录中的不结构化文本是逐渐被研究以利用这些信息与结构化数据共同改善医疗结果。》After the release of GPT models, many studies have applied GPT models to extract this information from clinical notes. Different from existing work, our research focuses on investigating zero-shot learning to extract this information together by providing minimum information to the GPT model. We use de-identified real-world clinical notes annotated with demographics, social determinants, and family history information. Given that the GPT model may provide text different from the original data, we explore two sets of evaluation metrics, including traditional NER evaluation metrics and semantic similarity evaluation metrics, to fully understand the performance. Our results show that the GPT-3.5 method achieved an average of 0.975 F1 on demographics extraction, 0.615 F1 on social determinants extraction, and 0.722 F1 on family history extraction. We believe these results can be further improved through model fine-tuning or few-shots learning. Through case studies, we also identified the limitations of GPT models, which need to be addressed in future research.
</details></li>
</ul>
<hr>
<h2 id="Flesch-or-Fumble-Evaluating-Readability-Standard-Alignment-of-Instruction-Tuned-Language-Models"><a href="#Flesch-or-Fumble-Evaluating-Readability-Standard-Alignment-of-Instruction-Tuned-Language-Models" class="headerlink" title="Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models"></a>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05454">http://arxiv.org/abs/2309.05454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Marvin Imperial, Harish Tayyar Madabushi</li>
<li>for: 这个研究的目的是评估不同开源和关闭源语言模型在写作完结和简化故事任务中的表现，以便教师可以根据标准指南来评估这些任务的难度。</li>
<li>methods: 这个研究使用了多种开源和关闭源语言模型，包括ChatGPT和BLOOMZ等，并使用标准指南来控制文本的阅读难度。</li>
<li>results: 研究发现，使用标准指南控制文本阅读难度可以提高模型的表现，而ChatGPT模型在这些生成任务中表现较差，而BLOOMZ和FlanT5等开源模型则表现更加出色。<details>
<summary>Abstract</summary>
Readability metrics and standards such as Flesch Kincaid Grade Level (FKGL) and the Common European Framework of Reference for Languages (CEFR) exist to guide teachers and educators to properly assess the complexity of educational materials before administering them for classroom use. In this study, we select a diverse set of open and closed-source instruction-tuned language models and investigate their performances in writing story completions and simplifying narratives$-$tasks that teachers perform$-$using standard-guided prompts controlling text readability. Our extensive findings provide empirical proof of how globally recognized models like ChatGPT may be considered less effective and may require more refined prompts for these generative tasks compared to other open-sourced models such as BLOOMZ and FlanT5$-$which have shown promising results.
</details>
<details>
<summary>摘要</summary>
教学工具和标准，如费希-金凯德学年级水平（FKGL）和欧洲共同语言参照体系（CEFR），用于导引教师和教育工作者评估教学材料的复杂性，以确保在教室使用前，材料的阅读性能得到适当的评估。在这项研究中，我们选择了一些多样化的开源和关闭源的 instrucit-调整语言模型，并 investigate其在写作续写和简化故事任务中的表现，使用标准化的提示控制文本阅读性。我们的广泛发现证明了 globally recognized模型 like ChatGPT 可能不太有效，并且可能需要更加细化的提示来完成这些生成任务，相比于其他开源模型如 BLOOMZ 和 FlanT5，这些模型在这些任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Deductive-Competence-of-Large-Language-Models"><a href="#Evaluating-the-Deductive-Competence-of-Large-Language-Models" class="headerlink" title="Evaluating the Deductive Competence of Large Language Models"></a>Evaluating the Deductive Competence of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05452">http://arxiv.org/abs/2309.05452</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. M. Seals, Valerie L. Shalin</li>
<li>for: 这个研究旨在评估大语言模型（LLMs）的逻辑和问题解决能力。</li>
<li>methods: 研究使用了多种大语言模型（LLMs）来解决一种从认知科学文献中的逻辑推理问题。</li>
<li>results: 研究发现，这些LLMs在问题的常规形式下表现有限，并且对问题的表示形式和内容进行了跟进实验，但发现表现之间存在差异，并且与人类表现不同。总的来说，这些结果表明LLMs具有独特的逻辑偏见，与人类逻辑性表现相互关联。<details>
<summary>Abstract</summary>
The development of highly fluent large language models (LLMs) has prompted increased interest in assessing their reasoning and problem-solving capabilities. We investigate whether several LLMs can solve a classic type of deductive reasoning problem from the cognitive science literature. The tested LLMs have limited abilities to solve these problems in their conventional form. We performed follow up experiments to investigate if changes to the presentation format and content improve model performance. We do find performance differences between conditions; however, they do not improve overall performance. Moreover, we find that performance interacts with presentation format and content in unexpected ways that differ from human performance. Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance.
</details>
<details>
<summary>摘要</summary>
发展高度流畅的大语言模型（LLMs）已引发了评估其逻辑和问题解决能力的兴趣。我们研究了一些LLMs是否可以解决知识科学文献中的一种经典逻辑推理问题。我们发现，在传统形式下，测试LLMs的能力并不高。我们进行了续试实验，以确定是否可以通过改变格式和内容来改善模型表现。结果发现，尽管存在具体的表现差异，但这并不能提高总体表现。此外，我们发现模型的表现与显示格式和内容之间存在不可预期的交互作用，与人类表现不同。总之，我们的结果表明，LLMs具有人类逻辑思维不同的偏好，这些偏好只有部分与人类逻辑表现相符。
</details></li>
</ul>
<hr>
<h2 id="TeGit-Generating-High-Quality-Instruction-Tuning-Data-with-Text-Grounded-Task-Design"><a href="#TeGit-Generating-High-Quality-Instruction-Tuning-Data-with-Text-Grounded-Task-Design" class="headerlink" title="TeGit: Generating High-Quality Instruction-Tuning Data with Text-Grounded Task Design"></a>TeGit: Generating High-Quality Instruction-Tuning Data with Text-Grounded Task Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05447">http://arxiv.org/abs/2309.05447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongrui Chen, Haiyun Jiang, Xinting Huang, Shuming Shi, Guilin Qi</li>
<li>for: 提高 LLM 能力，需要高质量的指令调整数据。现有的数据收集方法受限于人工标注成本过高或者 LLM 生成幻化。</li>
<li>methods: 本文提出了一种扩展的方法，通过训练语言模型自动设计任务，以获取高质量的指令调整数据。模型通过人工写的文本来减少幻化。</li>
<li>results: 自动和手动评估实验结果表明，我们的数据集具有高质量。<details>
<summary>Abstract</summary>
High-quality instruction-tuning data is critical to improving LLM capabilities. Existing data collection methods are limited by unrealistic manual labeling costs or by the hallucination of relying solely on LLM generation. To address the problems, this paper presents a scalable method to automatically collect high-quality instructional adaptation data by training language models to automatically design tasks based on human-written texts. Intuitively, human-written text helps to help the model attenuate illusions during the generation of tasks. Unlike instruction back-translation-based methods that directly take the given text as a response, we require the model to generate the \textit{instruction}, \textit{input}, and \textit{output} simultaneously to filter the noise. The results of the automated and manual evaluation experiments demonstrate the quality of our dataset.
</details>
<details>
<summary>摘要</summary>
高品质的指导数据对于提高LLM能力至关重要。现有的数据收集方法受限于不现实的手动标签成本或者依赖solely LLM生成所导致的幻觉。为解决这些问题，本文提出了一种可扩展的方法，通过训练语言模型自动设计任务基于人类写的文本。人类写的文本可以帮助模型减少幻觉。不同于基于回答 instruction back-translation 的方法，我们需要模型同时生成 \textit{指导}, \textit{输入} 和 \textit{输出}，以过滤噪音。经自动和 manual 评估实验表明，我们的数据集具有高质量。
</details></li>
</ul>
<hr>
<h2 id="Pushing-Mixture-of-Experts-to-the-Limit-Extremely-Parameter-Efficient-MoE-for-Instruction-Tuning"><a href="#Pushing-Mixture-of-Experts-to-the-Limit-Extremely-Parameter-Efficient-MoE-for-Instruction-Tuning" class="headerlink" title="Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"></a>Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05444">http://arxiv.org/abs/2309.05444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermiş, Acyr Locatelli, Sara Hooker</li>
<li>for: 这 paper 的目的是推动 Mixture of Experts（MoE） neural network 的 Parameter Efficient Fine-Tuning（PEFT）方法，以实现一个常规 MoE 模型的缩放。</li>
<li>methods: 这 paper 使用了 MoE 架构，并将它与轻量级专家结合在一起，以实现 Parameter Efficient MoE（PEMoE）方法。这种方法可以在约 1% 的参数上进行微调，并且可以在不知道先前任务的情况下进行普适化。</li>
<li>results: 根据 экспериментах，PEMoE 方法可以与标准 PEFT 方法相比，在更小的参数上实现更高的性能。此外，PEMoE 方法还可以在未经过任务知识的情况下进行普适化，并且可以在不同的任务上实现良好的性能。<details>
<summary>Abstract</summary>
The Mixture of Experts (MoE) is a widely known neural architecture where an ensemble of specialized sub-models optimizes overall performance with a constant computational cost. However, conventional MoEs pose challenges at scale due to the need to store all experts in memory. In this paper, we push MoE to the limit. We propose extremely parameter-efficient MoE by uniquely combining MoE architecture with lightweight experts.Our MoE architecture outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on par with full fine-tuning by only updating the lightweight experts -- less than 1% of an 11B parameters model. Furthermore, our method generalizes to unseen tasks as it does not depend on any prior task knowledge. Our research underscores the versatility of the mixture of experts architecture, showcasing its ability to deliver robust performance even when subjected to rigorous parameter constraints. Our code used in all the experiments is publicly available here: https://github.com/for-ai/parameter-efficient-moe.
</details>
<details>
<summary>摘要</summary>
“混合专家（MoE）是一种广泛知名的神经网络架构，其中一个ensemble of specialized sub-models可以提高总性能减少计算成本。然而，传统的MoE遇到了规模化的挑战，因为需要存储所有专家。在这篇论文中，我们将MoE推到了界限。我们提出了非常 Paramater-efficient MoE，通过独特地将MoE架构和轻量级专家结合在一起。我们的MoE架构超越了标准的 Paramater-efficient fine-tuning（PEFT）方法，并与全面 fine-tuning 相当，只需更新轻量级专家—— menos than 1% of an 11B parameters model。此外，我们的方法可以泛化到未看到的任务，因为它不依赖任务知识。我们的研究强调了混合专家架构的灵活性，显示它可以提供坚强的性能，即使面临严格的参数约束。我们在所有实验中使用的代码可以在以下链接获取：https://github.com/for-ai/parameter-efficient-moe。”
</details></li>
</ul>
<hr>
<h2 id="Experimenting-with-UD-Adaptation-of-an-Unsupervised-Rule-based-Approach-for-Sentiment-Analysis-of-Mexican-Tourist-Texts"><a href="#Experimenting-with-UD-Adaptation-of-an-Unsupervised-Rule-based-Approach-for-Sentiment-Analysis-of-Mexican-Tourist-Texts" class="headerlink" title="Experimenting with UD Adaptation of an Unsupervised Rule-based Approach for Sentiment Analysis of Mexican Tourist Texts"></a>Experimenting with UD Adaptation of an Unsupervised Rule-based Approach for Sentiment Analysis of Mexican Tourist Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05312">http://arxiv.org/abs/2309.05312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olga Kellert, Mahmud Uz Zaman, Nicholas Hill Matlis, Carlos Gómez-Rodríguez</li>
<li>for: 这个论文描述了一种基于 Universal Dependencies (UD) 的无监督、分析性和递归 (UCR) 规则集合方法的情感分析 (SA) 实验结果，并在 Rest-Mex 2023 共同任务中提交 (Team Olga&#x2F;LyS-SALSA) (内部的 IberLEF 2023 会议)。</li>
<li>methods: 我们的方法使用基本的 sintactic 规则，如修饰和否定词的规则，从情感词典中提取words，利用这些规则来实现无监督方法的优势：(1) 情感分析的解释性和可读性，(2) 鲁棒性适用于不同的数据集、语言和领域，(3) 非 NLP 专家可以使用。</li>
<li>results: 我们的方法比其他无监督方法具有更好的表现，我们还讨论了将 modal 特征作为另一种偏置规则以提高结果，以及使用 word ambiguation 技术来正确地识别情感词。<details>
<summary>Abstract</summary>
This paper summarizes the results of experimenting with Universal Dependencies (UD) adaptation of an Unsupervised, Compositional and Recursive (UCR) rule-based approach for Sentiment Analysis (SA) submitted to the Shared Task at Rest-Mex 2023 (Team Olga/LyS-SALSA) (within the IberLEF 2023 conference). By using basic syntactic rules such as rules of modification and negation applied on words from sentiment dictionaries, our approach exploits some advantages of an unsupervised method for SA: (1) interpretability and explainability of SA, (2) robustness across datasets, languages and domains and (3) usability by non-experts in NLP. We compare our approach with other unsupervised approaches of SA that in contrast to our UCR rule-based approach use simple heuristic rules to deal with negation and modification. Our results show a considerable improvement over these approaches. We discuss future improvements of our results by using modality features as another shifting rule of polarity and word disambiguation techniques to identify the right sentiment words.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Interpretability and explainability of SA results2. Robustness across datasets, languages, and domains3. Usability by non-experts in NLPWe compare our approach with other unsupervised approaches of SA that use simple heuristic rules to deal with negation and modification, and our results show a significant improvement over these approaches. In the future, we plan to improve our results by incorporating modality features as another shifting rule of polarity and using word disambiguation techniques to identify the correct sentiment words.</details></li>
</ol>
<hr>
<h2 id="Analysing-Cross-Lingual-Transfer-in-Low-Resourced-African-Named-Entity-Recognition"><a href="#Analysing-Cross-Lingual-Transfer-in-Low-Resourced-African-Named-Entity-Recognition" class="headerlink" title="Analysing Cross-Lingual Transfer in Low-Resourced African Named Entity Recognition"></a>Analysing Cross-Lingual Transfer in Low-Resourced African Named Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05311">http://arxiv.org/abs/2309.05311</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michael-beukman/nertransfer">https://github.com/michael-beukman/nertransfer</a></li>
<li>paper_authors: Michael Beukman, Manuel Fokam</li>
<li>for: 本研究探讨了十种低资源语言之间的跨语言转移学习Property，具体是Named Entity Recognition任务。</li>
<li>methods: 研究者采用了适应细化调教和转移语言的选择对Zero-shot转移性能的影响。</li>
<li>results: 研究发现，能够在单个语言上表现出色的模型通常会在其他语言上表现不佳，而能够在多种语言上准确预测的模型通常会在单个语言上表现不佳。此外，数据集之间的数据重叠度更好地预测转移性能 than geographical或生物学距离 между语言。<details>
<summary>Abstract</summary>
Transfer learning has led to large gains in performance for nearly all NLP tasks while making downstream models easier and faster to train. This has also been extended to low-resourced languages, with some success. We investigate the properties of cross-lingual transfer learning between ten low-resourced languages, from the perspective of a named entity recognition task. We specifically investigate how much adaptive fine-tuning and the choice of transfer language affect zero-shot transfer performance. We find that models that perform well on a single language often do so at the expense of generalising to others, while models with the best generalisation to other languages suffer in individual language performance. Furthermore, the amount of data overlap between the source and target datasets is a better predictor of transfer performance than either the geographical or genetic distance between the languages.
</details>
<details>
<summary>摘要</summary>
通过转移学习，大多数自然语言处理任务上的性能有了大幅提升，而同时使下游模型更容易和更快地训练。此外，这种技术还被扩展到低资源语言中，并获得了一定的成功。我们对十种低资源语言之间的跨语言转移学习性能进行了调查，从命名实体识别任务的角度来看。我们专门研究了跨语言转移学习后，模型如何影响单个语言和其他语言之间的性能。我们发现，能够在单一语言上表现出色的模型通常是在其他语言上的性能下降的代价，而能够在多种语言上具有最好的总体性能的模型通常是单一语言上的性能下降的代价。此外，源语言和目标语言数据集之间的数据重叠度比较地理或基因距离更好地预测跨语言转移性能。
</details></li>
</ul>
<hr>
<h2 id="Minuteman-Machine-and-Human-Joining-Forces-in-Meeting-Summarization"><a href="#Minuteman-Machine-and-Human-Joining-Forces-in-Meeting-Summarization" class="headerlink" title="Minuteman: Machine and Human Joining Forces in Meeting Summarization"></a>Minuteman: Machine and Human Joining Forces in Meeting Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05272">http://arxiv.org/abs/2309.05272</a></li>
<li>repo_url: None</li>
<li>paper_authors: František Kmječ, Ondřej Bojar</li>
<li>for: 这篇论文的目的是提出一种新的会议笔记工具，帮助会议笔记人员更加快速地制作高质量的会议笔记。</li>
<li>methods: 该工具使用了语音识别和摘要模型，提供了现场 trascript 和会议笔记，让用户可以在实时Collaborative manner中编辑和修正 trascript 和笔记。</li>
<li>results: 试验结果表明，该工具可以减轻会议笔记人员的认知压力，并帮助他们更加快速地恢复 missed 部分会议。<details>
<summary>Abstract</summary>
Many meetings require creating a meeting summary to keep everyone up to date. Creating minutes of sufficient quality is however very cognitively demanding. Although we currently possess capable models for both audio speech recognition (ASR) and summarization, their fully automatic use is still problematic. ASR models frequently commit errors when transcribing named entities while the summarization models tend to hallucinate and misinterpret the transcript. We propose a novel tool -- Minuteman -- to enable efficient semi-automatic meeting minuting. The tool provides a live transcript and a live meeting summary to the users, who can edit them in a collaborative manner, enabling correction of ASR errors and imperfect summary points in real time. The resulting application eases the cognitive load of the notetakers and allows them to easily catch up if they missed a part of the meeting due to absence or a lack of focus. We conduct several tests of the application in varied settings, exploring the worthiness of the concept and the possible user strategies.
</details>
<details>
<summary>摘要</summary>
多数会议需要创建会议摘要以保持所有人的更新。创建足够质量的会议笔记是非常认知吃力的。虽然我们目前拥有了可靠的语音识别模型和摘要模型，但它们的完全自动使用仍然存在问题。语音识别模型经常对名称实体进行误报，而摘要模型往往会假设和 Misinterpret 笔记文本。我们提议一种新工具---Minuteman---以实现高效的半自动会议笔记。该工具提供了实时的会议笔记和会议摘要，用户可以在协作模式下编辑，以更正语音识别错误和摘要点。结果使得笔记员的认知负担减轻，使其更容易catch up if 缺席或缺少注意力。我们在不同的设置下进行了多次测试，探讨该概念的可行性和用户策略。
</details></li>
</ul>
<hr>
<h2 id="CONFLATOR-Incorporating-Switching-Point-based-Rotatory-Positional-Encodings-for-Code-Mixed-Language-Modeling"><a href="#CONFLATOR-Incorporating-Switching-Point-based-Rotatory-Positional-Encodings-for-Code-Mixed-Language-Modeling" class="headerlink" title="CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling"></a>CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05270">http://arxiv.org/abs/2309.05270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsin Ali, Kandukuri Sai Teja, Neeharika Gupta, Parth Patwa, Anubhab Chatterjee, Vinija Jain, Aman Chadha, Amitava Das</li>
<li>for: 本研究旨在提出一种基于神经语言模型的代码混合语言模型（CONFLATOR），以便更好地处理混合语言文本。</li>
<li>methods: 研究人员采用了多种 позицион编码方法，包括旋转 позицион编码和 switching point 信息，以提高模型的表达能力。</li>
<li>results: 研究人员通过对两个基于混合语言的任务（即 sentiment analysis 和 machine translation）进行实验，发现 CONFLATOR 可以在这些任务中达到更高的表达能力，比如state-of-the-art。<details>
<summary>Abstract</summary>
The mixing of two or more languages is called Code-Mixing (CM). CM is a social norm in multilingual societies. Neural Language Models (NLMs) like transformers have been very effective on many NLP tasks. However, NLM for CM is an under-explored area. Though transformers are capable and powerful, they cannot always encode positional/sequential information since they are non-recurrent. Therefore, to enrich word information and incorporate positional information, positional encoding is defined. We hypothesize that Switching Points (SPs), i.e., junctions in the text where the language switches (L1 -> L2 or L2-> L1), pose a challenge for CM Language Models (LMs), and hence give special emphasis to switching points in the modeling process. We experiment with several positional encoding mechanisms and show that rotatory positional encodings along with switching point information yield the best results.   We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. CONFLATOR tries to learn to emphasize switching points using smarter positional encoding, both at unigram and bigram levels. CONFLATOR outperforms the state-of-the-art on two tasks based on code-mixed Hindi and English (Hinglish): (i) sentiment analysis and (ii) machine translation.
</details>
<details>
<summary>摘要</summary>
mixing of two or more languages is called Code-Mixing (CM). CM is a social norm in multilingual societies. Neural Language Models (NLMs) like transformers have been very effective on many NLP tasks. However, NLM for CM is an under-explored area. Though transformers are capable and powerful, they cannot always encode positional/sequential information since they are non-recurrent. Therefore, to enrich word information and incorporate positional information, positional encoding is defined. We hypothesize that Switching Points (SPs), i.e., junctions in the text where the language switches (L1 -> L2 or L2-> L1), pose a challenge for CM Language Models (LMs), and hence give special emphasis to switching points in the modeling process. We experiment with several positional encoding mechanisms and show that rotatory positional encodings along with switching point information yield the best results.  We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. CONFLATOR tries to learn to emphasize switching points using smarter positional encoding, both at unigram and bigram levels. CONFLATOR outperforms the state-of-the-art on two tasks based on code-mixed Hindi and English (Hinglish): (i) sentiment analysis and (ii) machine translation.
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Law-of-Numbers-Evidence-from-China’s-Real-Estate"><a href="#Exploring-the-Law-of-Numbers-Evidence-from-China’s-Real-Estate" class="headerlink" title="Exploring the Law of Numbers: Evidence from China’s Real Estate"></a>Exploring the Law of Numbers: Evidence from China’s Real Estate</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05221">http://arxiv.org/abs/2309.05221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuqian Zhang, Zhenhua Wang</li>
<li>for: 这篇论文探讨了中国地产公司的财务报表，以便更全面地描述数字的法律。</li>
<li>methods: 该论文使用了Benford的法律来研究数字的分布，同时还研究了数字的频率和长度。</li>
<li>results: 研究发现，中国地产公司的财务报表中的数字不具有完整性，而且存在数据修饰的问题。这些结果不仅有经济 significancen，还可以深入理解数字的分布和用途。<details>
<summary>Abstract</summary>
The renowned proverb, Numbers do not lie, underscores the reliability and insight that lie beneath numbers, a concept of undisputed importance, especially in economics and finance etc. Despite the prosperity of Benford's Law in the first digit analysis, its scope fails to remain comprehensiveness when it comes to deciphering the laws of number. This paper delves into number laws by taking the financial statements of China real estate as a representative, quantitatively study not only the first digit, but also depict the other two dimensions of numbers: frequency and length. The research outcomes transcend mere reservations about data manipulation and open the door to discussions surrounding number diversity and the delineation of the usage insights. This study wields both economic significance and the capacity to foster a deeper comprehension of numerical phenomena.
</details>
<details>
<summary>摘要</summary>
著名的成语“数字不假”强调数字下面的可靠性和洞察力的重要性，尤其在经济和金融等领域。尽管本福德法在首位数分析方面取得了很大的成功，但其范围却无法涵盖数字法律的全面性。这篇论文通过利用中国地产公司的财务报表作为例子，量化研究不仅首位数，还描述了其他两个维度：频率和长度。研究结果超越了仅仅是数据报告的担忧，开启了数字多样性和使用情况的描述的讨论。这种研究具有经济意义和深入了解数字现象的能力。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Impact-of-Post-Training-Quantization-on-Large-Language-Models"><a href="#Understanding-the-Impact-of-Post-Training-Quantization-on-Large-Language-Models" class="headerlink" title="Understanding the Impact of Post-Training Quantization on Large Language Models"></a>Understanding the Impact of Post-Training Quantization on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05210">http://arxiv.org/abs/2309.05210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somnath Roy</li>
<li>for: The paper focuses on the deployment and operation of large language models (LLMs) on consumer-grade GPUs, and the impact of hyperparameters on the performance of quantized models.</li>
<li>methods: The paper compares and analyzes the performance of different quantization techniques, including nf4, fp4, and fp4-dq, on various LLMs, and investigates the effects of temperature on the performance of these models.</li>
<li>results: The study finds that nf4 and fp4 are equally proficient 4-bit quantization techniques, but nf4 displays greater resilience to temperature variations in the case of the llama2 series of models at lower temperature. Additionally, the study shows that 4-bit quantized models of varying sizes exhibit higher sensitivity to temperature in the range of 0.5 to 0.8, and that int8 quantization is associated with significantly slower inference speeds.<details>
<summary>Abstract</summary>
Large language models (LLMs) are rapidly increasing in size, with the number of parameters becoming a key factor in the success of many commercial models, such as ChatGPT, Claude, and Bard. Even the recently released publicly accessible models for commercial usage, such as Falcon and Llama2, come equipped with billions of parameters. This significant increase in the number of parameters makes deployment and operation very costly. The remarkable progress in the field of quantization for large neural networks in general and LLMs in particular, has made these models more accessible by enabling them to be deployed on consumer-grade GPUs. Quantized models generally demonstrate comparable performance levels to their unquantized base counterparts. Nonetheless, there exists a notable gap in our comprehensive understanding of how these quantized models respond to hyperparameters, such as temperature, max new tokens, and topk, particularly for next word prediction. The present analysis reveals that nf4 and fp4 are equally proficient 4-bit quantization techniques, characterized by similar attributes such as inference speed, memory consumption, and the quality of generated content. the study identifies nf4 as displaying greater resilience to temperature variations in the case of the llama2 series of models at lower temperature, while fp4 and fp4-dq proves to be a more suitable choice for falcon series of models. It is noteworthy that, in general, 4-bit quantized models of varying sizes exhibit higher sensitivity to temperature in the range of 0.5 to 0.8, unlike their unquantized counterparts. Additionally, int8 quantization is associated with significantly slower inference speeds, whereas unquantized bfloat16 models consistently yield the fastest inference speeds across models of all sizes.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在大小方面迅速增长，参数的数量成为许多商业模型的成功关键，如ChatGPT、Claude和Bard。即使最近公开的商业模型，如Falcon和Llama2，也搭载了数十亿个参数。这种 Parameters 的增加使得部署和运行变得非常昂贵。在大型神经网络和 LLM 的减量方面做出了重要进步，使得这些模型可以在消费级 GPU 上部署。减量模型通常与不减量模型的性能水平相当。然而，对于下一个字母预测中的 гиперparameters，如温度、最大新字母数和topk，我们对这些减量模型的理解仍然存在一定的差距。 presente 分析发现，nf4 和 fp4 是Equally proficient 4位减量技术，具有相似的特点，如执行速度、内存占用率和生成内容质量。study 发现，nf4 在 llama2 系列模型下 Displayed greater resilience  to temperature variations at lower temperature, while fp4 和 fp4-dq 适用于 falcon 系列模型。通常 speaking, 4位减量模型的不同大小在温度范围内 0.5-0.8  exhibit higher sensitivity to temperature, unlike their unquantized counterparts。此外，int8 减量与不减量 bfloat16 模型相比，执行速度明显 slower。
</details></li>
</ul>
<hr>
<h2 id="From-Artificially-Real-to-Real-Leveraging-Pseudo-Data-from-Large-Language-Models-for-Low-Resource-Molecule-Discovery"><a href="#From-Artificially-Real-to-Real-Leveraging-Pseudo-Data-from-Large-Language-Models-for-Low-Resource-Molecule-Discovery" class="headerlink" title="From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery"></a>From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05203">http://arxiv.org/abs/2309.05203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Chen, Nuwa Xi, Yanrui Du, Haochun Wang, Chen Jianyu, Sendong Zhao, Bing Qin</li>
<li>for: 提高底层资源缺乏的cross-modal分子发现方法的效果</li>
<li>methods: 利用人工生成的大语言模型生成的 pseudo data进行适应Domain adaptation</li>
<li>results: 使用 pseudo data 的方法比现有方法有更好的性能，同时需要较小的模型规模、数据量和训练成本，表明其高效性。<details>
<summary>Abstract</summary>
Molecule discovery serves as a cornerstone in numerous scientific domains, fueling the development of new materials and innovative drug designs. Recent developments of in-silico molecule discovery have highlighted the promising results of cross-modal techniques, which bridge molecular structures with their descriptive annotations. However, these cross-modal methods frequently encounter the issue of data scarcity, hampering their performance and application. In this paper, we address the low-resource challenge by utilizing artificially-real data generated by Large Language Models (LLMs). We first introduce a retrieval-based prompting strategy to construct high-quality pseudo data, then explore the optimal method to effectively leverage this pseudo data. Experiments show that using pseudo data for domain adaptation outperforms all existing methods, while also requiring a smaller model scale, reduced data size and lower training cost, highlighting its efficiency. Furthermore, our method shows a sustained improvement as the volume of pseudo data increases, revealing the great potential of pseudo data in advancing low-resource cross-modal molecule discovery.
</details>
<details>
<summary>摘要</summary>
分子发现在许多科学领域中 serves as a cornerstone, 推动新材料和创新药物设计的发展。  latest developments in in-silico molecule discovery have highlighted the promising results of cross-modal techniques, which bridge molecular structures with their descriptive annotations. However, these cross-modal methods frequently encounter the issue of data scarcity, hampering their performance and application. In this paper, we address the low-resource challenge by utilizing artificially-real data generated by Large Language Models (LLMs). We first introduce a retrieval-based prompting strategy to construct high-quality pseudo data, then explore the optimal method to effectively leverage this pseudo data. Experiments show that using pseudo data for domain adaptation outperforms all existing methods, while also requiring a smaller model scale, reduced data size and lower training cost, highlighting its efficiency. Furthermore, our method shows a sustained improvement as the volume of pseudo data increases, revealing the great potential of pseudo data in advancing low-resource cross-modal molecule discovery.
</details></li>
</ul>
<hr>
<h2 id="Two-is-Better-Than-One-Answering-Complex-Questions-by-Multiple-Knowledge-Sources-with-Generalized-Links"><a href="#Two-is-Better-Than-One-Answering-Complex-Questions-by-Multiple-Knowledge-Sources-with-Generalized-Links" class="headerlink" title="Two is Better Than One: Answering Complex Questions by Multiple Knowledge Sources with Generalized Links"></a>Two is Better Than One: Answering Complex Questions by Multiple Knowledge Sources with Generalized Links</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05201">http://arxiv.org/abs/2309.05201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minhao Zhang, Yongliang Ma, Yanzeng Li, Ruoyu Zhang, Lei Zou, Ming Zhou</li>
<li>for: 本研究旨在解决多知识库（KB）合并问答（QA）问题中，不能充分利用多KB之间的不同链接类型所带来的限制。</li>
<li>methods: 本研究提出了一种新的多知识库问答（Multi-KB-QA）任务，利用多KB之间的全链接和半链接来获取正确答案。同时，我们还构建了一个多样化链接和问题类型的准则集，以便效率地评估多KB-QA性能。</li>
<li>results: 实验结果表明，我们提出的方法在多知识库问答任务中，与传统KB-QA系统相比，显著提高了性能。这表明，需要解决多KB之间的不同链接类型，以提高QA性能。<details>
<summary>Abstract</summary>
Incorporating multiple knowledge sources is proven to be beneficial for answering complex factoid questions. To utilize multiple knowledge bases (KB), previous works merge all KBs into a single graph via entity alignment and reduce the problem to question-answering (QA) over the fused KB. In reality, various link relations between KBs might be adopted in QA over multi-KBs. In addition to the identity between the alignable entities (i.e. full link), unalignable entities expressing the different aspects or types of an abstract concept may also be treated identical in a question (i.e. partial link). Hence, the KB fusion in prior works fails to represent all types of links, restricting their ability to comprehend multi-KBs for QA. In this work, we formulate the novel Multi-KB-QA task that leverages the full and partial links among multiple KBs to derive correct answers, a benchmark with diversified link and query types is also constructed to efficiently evaluate Multi-KB-QA performance. Finally, we propose a method for Multi-KB-QA that encodes all link relations in the KB embedding to score and rank candidate answers. Experiments show that our method markedly surpasses conventional KB-QA systems in Multi-KB-QA, justifying the necessity of devising this task.
</details>
<details>
<summary>摘要</summary>
combining multiple knowledge sources has been proven to be beneficial for answering complex factoid questions. to utilize multiple knowledge bases (kb), previous works merge all kbs into a single graph via entity alignment and reduce the problem to question-answering (qa) over the fused kb. in reality, various link relations between kbs might be adopted in qa over multi-kbs. in addition to the identity between the alignable entities (i.e. full link), unalignable entities expressing different aspects or types of an abstract concept may also be treated identical in a question (i.e. partial link). hence, the kb fusion in prior works fails to represent all types of links, restricting their ability to comprehend multi-kbs for qa. in this work, we formulate the novel multi-kb-qa task that leverages the full and partial links among multiple kbs to derive correct answers, a benchmark with diversified link and query types is also constructed to efficiently evaluate multi-kb-qa performance. finally, we propose a method for multi-kb-qa that encodes all link relations in the kb embedding to score and rank candidate answers. experiments show that our method markedly surpasses conventional kb-qa systems in multi-kb-qa, justifying the necessity of devising this task.
</details></li>
</ul>
<hr>
<h2 id="Does-Writing-with-Language-Models-Reduce-Content-Diversity"><a href="#Does-Writing-with-Language-Models-Reduce-Content-Diversity" class="headerlink" title="Does Writing with Language Models Reduce Content Diversity?"></a>Does Writing with Language Models Reduce Content Diversity?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05196">http://arxiv.org/abs/2309.05196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vishakhpk/hai-diversity">https://github.com/vishakhpk/hai-diversity</a></li>
<li>paper_authors: Vishakh Padmakumar, He He</li>
<li>for:  measure the impact of co-writing on diversity in produced content</li>
<li>methods: controlled experiment with three setups (base LLM, feedback-tuned LLM, and no model help) and diversity metrics</li>
<li>results: writing with InstructGPT (but not GPT3) results in a statistically significant reduction in diversity, with increased similarity between writings of different authors and reduced lexical and content diversity, primarily due to InstructGPT contributing less diverse text to co-written essays.<details>
<summary>Abstract</summary>
Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays. In contrast, the user-contributed text remains unaffected by model collaboration. This suggests that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" (LLMs) is translated as "大型语言模型" (dàxìng yǔyán módelì)* "Collaborative writing" is translated as "合作写作" (hézuò xiǎoqian)* "Base LLM" is translated as "基础模型" (jīchū módelì)* "Feedback-tuned LLM" is translated as "反馈调整模型" (fǎngxiàn tiángzhèng módelì)* "Co-written essays" is translated as "合作写作的文章" (hézuò xiǎoqian de wénzhang)* "Diversity metrics" is translated as "多样性指标" (duōyànxìng zhǐbǐ)* "Statistically significant reduction in diversity" is translated as " statistically significant reduction in diversity" (统计学上的多样性减少)* "Lexical diversity" is translated as "词语多样性" (cíyǔ duōyànxìng)* "Content diversity" is translated as "内容多样性" (néngjīng duōyànxìng)
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/cs.CL_2023_09_11/" data-id="clp869ttf00bkk5882ihc184d" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/11/cs.LG_2023_09_11/" class="article-date">
  <time datetime="2023-09-11T10:00:00.000Z" itemprop="datePublished">2023-09-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/11/cs.LG_2023_09_11/">cs.LG - 2023-09-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Reaction-coordinate-flows-for-model-reduction-of-molecular-kinetics"><a href="#Reaction-coordinate-flows-for-model-reduction-of-molecular-kinetics" class="headerlink" title="Reaction coordinate flows for model reduction of molecular kinetics"></a>Reaction coordinate flows for model reduction of molecular kinetics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05878">http://arxiv.org/abs/2309.05878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wu, Frank Noé</li>
<li>for: 本研究推出了一种基于流程的机器学习方法，即反应均衡（RC）流，用于描述分子系统的低维度动力学模型。</li>
<li>methods: 该方法使用了正规化流形成均衡变换，并使用布朗运动模型来近似RC的动力学。所有模型参数都可以通过数据驱动方式进行估算。</li>
<li>results:  numerical experiments表明，提出的方法可以高效地从仿真数据中提取低维度、可读的状态空间表示。<details>
<summary>Abstract</summary>
In this work, we introduce a flow based machine learning approach, called reaction coordinate (RC) flow, for discovery of low-dimensional kinetic models of molecular systems. The RC flow utilizes a normalizing flow to design the coordinate transformation and a Brownian dynamics model to approximate the kinetics of RC, where all model parameters can be estimated in a data-driven manner. In contrast to existing model reduction methods for molecular kinetics, RC flow offers a trainable and tractable model of reduced kinetics in continuous time and space due to the invertibility of the normalizing flow. Furthermore, the Brownian dynamics-based reduced kinetic model investigated in this work yields a readily discernible representation of metastable states within the phase space of the molecular system. Numerical experiments demonstrate how effectively the proposed method discovers interpretable and accurate low-dimensional representations of given full-state kinetics from simulations.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们介绍了一种基于流的机器学习方法，即反应坐标（RC）流，用于分子系统的低维度动力学模型的发现。RC流利用了正规化流来设计坐标变换，并使用布朗运动模型来近似RC的动力学，其中所有模型参数都可以在数据驱动下被估算。与现有的分子动力学减量方法不同，RC流提供了可训练和可追踪的维度减少的动力学模型，因为正规化流的倒散性。此外，在这种研究中，我们使用布朗运动模型来研究分子系统中的潜在稳态态，从而获得了可读取的和准确的低维度表示。numerical experiments表明，该方法可以从模拟数据中提取有效和准确的低维度表示。
</details></li>
</ul>
<hr>
<h2 id="Force-directed-graph-embedding-with-hops-distance"><a href="#Force-directed-graph-embedding-with-hops-distance" class="headerlink" title="Force-directed graph embedding with hops distance"></a>Force-directed graph embedding with hops distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05865">http://arxiv.org/abs/2309.05865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamidreza Lotfalizadeh, Mohammad Al Hasan</li>
<li>for: 本研究旨在提出一种基于力的图像方法，用于图像中节点的快速嵌入和分类。</li>
<li>methods: 该方法使用了稳定加速公式，将节点嵌入低维空间中，以保持图像的结构特征。具体来说，该方法 simulate了一些自定义吸引和排斥力，用于 Node pairs中的快速嵌入。</li>
<li>results: 对多个图像分析任务进行评估，该方法可以与现有的无监督嵌入方法相比，实现竞争性的表现。<details>
<summary>Abstract</summary>
Graph embedding has become an increasingly important technique for analyzing graph-structured data. By representing nodes in a graph as vectors in a low-dimensional space, graph embedding enables efficient graph processing and analysis tasks like node classification, link prediction, and visualization. In this paper, we propose a novel force-directed graph embedding method that utilizes the steady acceleration kinetic formula to embed nodes in a way that preserves graph topology and structural features. Our method simulates a set of customized attractive and repulsive forces between all node pairs with respect to their hop distance. These forces are then used in Newton's second law to obtain the acceleration of each node. The method is intuitive, parallelizable, and highly scalable. We evaluate our method on several graph analysis tasks and show that it achieves competitive performance compared to state-of-the-art unsupervised embedding techniques.
</details>
<details>
<summary>摘要</summary>
图像嵌入技术在处理图Structured数据方面已经变得越来越重要。通过将图像中的节点表示为低维度空间中的向量，图像嵌入技术可以实现高效的图像处理和分析任务，如节点分类、链接预测和可视化。在这篇论文中，我们提出了一种新的力导向的图像嵌入方法，该方法利用了稳定加速公式来嵌入节点，以保持图像的结构特征。我们在所有节点对之间 simulate 自定义的吸引和排斥力，并使用牛顿第二定律来获得每个节点的加速度。该方法易于理解，可并行化和高度扩展。我们在多个图像分析任务中评估了该方法，并示出了与当前最佳无监督嵌入技术相比的竞争性性能。
</details></li>
</ul>
<hr>
<h2 id="Energy-Preservation-and-Stability-of-Random-Filterbanks"><a href="#Energy-Preservation-and-Stability-of-Random-Filterbanks" class="headerlink" title="Energy Preservation and Stability of Random Filterbanks"></a>Energy Preservation and Stability of Random Filterbanks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05855">http://arxiv.org/abs/2309.05855</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danedane-haider/random-filterbanks">https://github.com/danedane-haider/random-filterbanks</a></li>
<li>paper_authors: Daniel Haider, Vincent Lostanlen, Martin Ehler, Peter Balazs</li>
<li>for: 这篇论文是关于干扰波形深度学习的挑战。</li>
<li>methods: 这篇论文使用了数据卷积神经网络（convnet）来设计滤波器。</li>
<li>results: 研究发现，使用随机 Gaussian 权重的 FIR 滤波器在大 Filter 和本地 périodic 输入信号中存在不稳定性和 Condition number 问题。此外，研究还发现了预期能量保持的不够，导致了数字稳定性的问题，并提出了理论上的 BOUND 限制。<details>
<summary>Abstract</summary>
What makes waveform-based deep learning so hard? Despite numerous attempts at training convolutional neural networks (convnets) for filterbank design, they often fail to outperform hand-crafted baselines. This is all the more surprising because these baselines are linear time-invariant systems: as such, their transfer functions could be accurately represented by a convnet with a large receptive field. In this article, we elaborate on the statistical properties of simple convnets from the mathematical perspective of random convolutional operators. We find that FIR filterbanks with random Gaussian weights are ill-conditioned for large filters and locally periodic input signals, which both are typical in audio signal processing applications. Furthermore, we observe that expected energy preservation of a random filterbank is not sufficient for numerical stability and derive theoretical bounds for its expected frame bounds.
</details>
<details>
<summary>摘要</summary>
（以下是文本的简化中文版本）为什么波形基于深度学习这么困难？虽然许多尝试用深度神经网络（convnet）来设计滤波器，但它们frequently fail to outperform hand-crafted baselines。这种情况更加奇怪，因为这些基elines是线性时间不变的系统：这意味着它们的转移函数可以准确地由一个大感知场来表示。在这篇文章中，我们从数学角度来描述简单的convnet的统计性质。我们发现，随机 Gaussian 权重的 FIR 滤波器在大 filters 和本地 periodic input signals 中是不稳定的。此外，我们发现预期能量保留不够 для数学稳定，并derive theoretical bounds for its expected frame bounds。
</details></li>
</ul>
<hr>
<h2 id="ChemSpaceAL-An-Efficient-Active-Learning-Methodology-Applied-to-Protein-Specific-Molecular-Generation"><a href="#ChemSpaceAL-An-Efficient-Active-Learning-Methodology-Applied-to-Protein-Specific-Molecular-Generation" class="headerlink" title="ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation"></a>ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05853">http://arxiv.org/abs/2309.05853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregory W. Kyro, Anton Morgunov, Rafael I. Brent, Victor S. Batista</li>
<li>For: The paper is written for the purpose of developing a novel and efficient semi-supervised active learning methodology for fine-tuning generative artificial intelligence models, specifically in the context of targeted molecular generation.* Methods: The paper uses a GPT-based molecular generator and a constructed representation of the sample space to strategically operate within a chemical space proxy, maximizing attractive interactions between the generated molecules and a protein target. The approach does not require the individual evaluation of all data points used for fine-tuning, enabling the incorporation of computationally expensive metrics.* Results: The paper demonstrates the ability to fine-tune a GPT-based molecular generator with respect to an attractive interaction-based scoring function, resulting in maximized attractive interactions between the generated molecules and a protein target.Here is the same information in Simplified Chinese text:* For: 这篇论文是为了开发一种新的和高效的半监督学习方法，用于微调生成人工智能模型，特别是在分子生成领域中。* Methods: 这篇论文使用基于GPT的分子生成器，并使用一个构建的样本空间来策略地操作在一个化学空间代理中，以最大化生成分子和蛋白质目标之间的吸引力。这种方法不需要评估所有数据点，因此可以包含计算成本较高的指标。* Results: 这篇论文 demonstarted 可以使用这种方法微调基于吸引力分数函数的GPT基于分子生成器，以最大化生成分子和蛋白质目标之间的吸引力。<details>
<summary>Abstract</summary>
The incredible capabilities of generative artificial intelligence models have inevitably led to their application in the domain of drug discovery. It is therefore of tremendous interest to develop methodologies that enhance the abilities and applicability of these powerful tools. In this work, we present a novel and efficient semi-supervised active learning methodology that allows for the fine-tuning of a generative model with respect to an objective function by strategically operating within a constructed representation of the sample space. In the context of targeted molecular generation, we demonstrate the ability to fine-tune a GPT-based molecular generator with respect to an attractive interaction-based scoring function by strategically operating within a chemical space proxy, thereby maximizing attractive interactions between the generated molecules and a protein target. Importantly, our approach does not require the individual evaluation of all data points that are used for fine-tuning, enabling the incorporation of computationally expensive metrics. We are hopeful that the inherent generality of this methodology ensures that it will remain applicable as this exciting field evolves. To facilitate implementation and reproducibility, we have made all of our software available through the open-source ChemSpaceAL Python package.
</details>
<details>
<summary>摘要</summary>
“启示人工智能模型的强大能力已经无可避免地应用于药物发现领域。因此，开发 методологиías可以提高这些工具的能力和应用性是非常重要的。在这种工作中，我们提出了一种新的、高效的半监督学习方法，可以让一个生成模型与一个目标函数进行精细调整，通过在一个构造的样本空间中精细操作。在分子生成中，我们示例了可以通过在一个化学空间代理中精细调整一个GPT基于的分子生成器，以便最大化与一个蛋白质目标分子之间的吸引力相互作用。值得注意的是，我们的方法不需要评估所有用于精细调整的数据点，因此可以包括计算成本较高的指标。我们希望这种方法的内在一致性将保持其可应用性，并且随着这个吸引人的领域的发展，它将继续保持有用。为了促进实现和复制性，我们已经将所有的软件公开发布在OpenSource的ChemSpaceAL Python包中。”Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and other parts of the world. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Audio-Augmentations-for-Contrastive-Learning-of-Health-Related-Acoustic-Signals"><a href="#Optimizing-Audio-Augmentations-for-Contrastive-Learning-of-Health-Related-Acoustic-Signals" class="headerlink" title="Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals"></a>Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05843">http://arxiv.org/abs/2309.05843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Blankemeier, Sebastien Baur, Wei-Hung Weng, Jake Garrison, Yossi Matias, Shruthi Prabhakara, Diego Ardila, Zaid Nabulsi</li>
<li>for: 这个论文旨在提出一种自适应学习框架，用于对医疗声学信号进行对比学习。</li>
<li>methods: 该论文使用了SimCLR框架和Slowfast NFNet底层，并进行了对声音扩展的深入分析，以便优化Slowfast NFNet对声音 tasks的性能。</li>
<li>results: 研究发现，合适的声音扩展策略可以提高Slowfast NFNet对声音任务的性能，并且当扩展策略组合起来时，它们可以产生相互增强的效果，超过每个策略应用 separately的效果。<details>
<summary>Abstract</summary>
Health-related acoustic signals, such as cough and breathing sounds, are relevant for medical diagnosis and continuous health monitoring. Most existing machine learning approaches for health acoustics are trained and evaluated on specific tasks, limiting their generalizability across various healthcare applications. In this paper, we leverage a self-supervised learning framework, SimCLR with a Slowfast NFNet backbone, for contrastive learning of health acoustics. A crucial aspect of optimizing Slowfast NFNet for this application lies in identifying effective audio augmentations. We conduct an in-depth analysis of various audio augmentation strategies and demonstrate that an appropriate augmentation strategy enhances the performance of the Slowfast NFNet audio encoder across a diverse set of health acoustic tasks. Our findings reveal that when augmentations are combined, they can produce synergistic effects that exceed the benefits seen when each is applied individually.
</details>
<details>
<summary>摘要</summary>
医疗相关的声学信号，如喘挫和呼吸 зву频，对医疗诊断和连续健康监测有重要 significancE。现有大多数机器学习方法 для医疗声学都是专门为特定任务训练和评估，这限制了它们在不同医疗应用程序中的一致性。在这篇论文中，我们利用了一种无监督学习框架，SimCLR，并与Slowfast NFNet底层结构一起进行对照学习医疗声学。对于 optimize Slowfast NFNet  для这个应用程序，我们进行了深入的声音扩充分析，并证明了有效的声音扩充策略可以提高 Slowfast NFNet 声音编码器在多种医疗声学任务上的性能。我们的发现表明，当扩充策略相互结合时，它们可以产生同工合作的效果，超过每个扩充策略应用 separately 的效果。
</details></li>
</ul>
<hr>
<h2 id="The-Safety-Filter-A-Unified-View-of-Safety-Critical-Control-in-Autonomous-Systems"><a href="#The-Safety-Filter-A-Unified-View-of-Safety-Critical-Control-in-Autonomous-Systems" class="headerlink" title="The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems"></a>The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05837">http://arxiv.org/abs/2309.05837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai-Chieh Hsu, Haimin Hu, Jaime Fernández Fisac</li>
<li>for: 提高自主 робо器的安全性，满足新的部署环境的需求</li>
<li>methods: 评估和比较现有的安全筛选方法，提出一种统一的技术框架，推动未来的安全筛选技术的发展</li>
<li>results: 提出一种新的安全筛选方法，可以更好地满足自主 робо器的安全需求，并且可以帮助实现更好的安全性和可靠性<details>
<summary>Abstract</summary>
Recent years have seen significant progress in the realm of robot autonomy, accompanied by the expanding reach of robotic technologies. However, the emergence of new deployment domains brings unprecedented challenges in ensuring safe operation of these systems, which remains as crucial as ever. While traditional model-based safe control methods struggle with generalizability and scalability, emerging data-driven approaches tend to lack well-understood guarantees, which can result in unpredictable catastrophic failures. Successful deployment of the next generation of autonomous robots will require integrating the strengths of both paradigms. This article provides a review of safety filter approaches, highlighting important connections between existing techniques and proposing a unified technical framework to understand, compare, and combine them. The new unified view exposes a shared modular structure across a range of seemingly disparate safety filter classes and naturally suggests directions for future progress towards more scalable synthesis, robust monitoring, and efficient intervention.
</details>
<details>
<summary>摘要</summary>
This article provides a review of safety filter approaches, highlighting the connections between existing techniques and proposing a unified technical framework for understanding, comparing, and combining them. The new framework exposes a shared modular structure across a range of safety filter classes, providing a foundation for future progress in scalable synthesis, robust monitoring, and efficient intervention.
</details></li>
</ul>
<hr>
<h2 id="Ensemble-based-modeling-abstractions-for-modern-self-optimizing-systems"><a href="#Ensemble-based-modeling-abstractions-for-modern-self-optimizing-systems" class="headerlink" title="Ensemble-based modeling abstractions for modern self-optimizing systems"></a>Ensemble-based modeling abstractions for modern self-optimizing systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05823">http://arxiv.org/abs/2309.05823</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smartarch/ml-deeco-security-isola">https://github.com/smartarch/ml-deeco-security-isola</a></li>
<li>paper_authors: Michal Töpfer, Milad Abdullah, Tomáš Bureš, Petr Hnětynka, Martin Kruliš</li>
<li>for: 这篇论文旨在扩展DEECo模型，以便使用机器学习和优化策略来建立和重新配置自动化组件集。</li>
<li>methods: 论文使用机器学习和优化策略来建立和重新配置自动化组件集，并在模型层次上Capture这些概念。</li>
<li>results: 论文通过用机器学习和优化策略来建立和重新配置自动化组件集，可以在Industry 4.0 Setting中模型访问控制相关问题，并且 argue что这种方法是现代智能系统的关键特征，可以在运行时学习和优化其行为以适应环境不确定性。<details>
<summary>Abstract</summary>
In this paper, we extend our ensemble-based component model DEECo with the capability to use machine-learning and optimization heuristics in establishing and reconfiguration of autonomic component ensembles. We show how to capture these concepts on the model level and give an example of how such a model can be beneficially used for modeling access-control related problem in the Industry 4.0 settings. We argue that incorporating machine-learning and optimization heuristics is a key feature for modern smart systems which are to learn over the time and optimize their behavior at runtime to deal with uncertainty in their environment.
</details>
<details>
<summary>摘要</summary>
在本文中，我们将我们的集成式组件模型DEECo扩展以使用机器学习和优化办法在自动化组件集中进行设置和重新配置。我们证明了如何在模型层面上表示这些概念，并给出了一个例子，证明如何使用这种模型来解决在产业4.0设置下的访问控制相关问题。我们认为，在运行时使用机器学习和优化办法是现代智能系统的关键特征，以适应环境中的不确定性。Here's the word-for-word translation:在本文中，我们将我们的集成式组件模型DEECo扩展以使用机器学习和优化办法在自动化组件集中进行设置和重新配置。我们证明了如何在模型层面上表示这些概念，并给出了一个例子，证明如何使用这种模型来解决在产业4.0设置下的访问控制相关问题。我们认为，在运行时使用机器学习和优化办法是现代智能系统的关键特征，以适应环境中的不确定性。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-learning-of-effective-dynamics-for-multiscale-systems"><a href="#Interpretable-learning-of-effective-dynamics-for-multiscale-systems" class="headerlink" title="Interpretable learning of effective dynamics for multiscale systems"></a>Interpretable learning of effective dynamics for multiscale systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05812">http://arxiv.org/abs/2309.05812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanuel Menier, Sebastian Kaltenbach, Mouadh Yagoubi, Marc Schoenauer, Petros Koumoutsakos</li>
<li>for: 这篇论文旨在提出一种可解释性的学习动力学框架，以提高高维多Scale系统的模型化和仿真。</li>
<li>methods: 该框架基于深度回归神经网络，并结合了Mori-Zwanzig和Koopman运动理论。</li>
<li>results: 实验结果表明，该框架可以生成高精度预测和获得可解释性的动力学特性，适用于解决高维多Scale系统。<details>
<summary>Abstract</summary>
The modeling and simulation of high-dimensional multiscale systems is a critical challenge across all areas of science and engineering. It is broadly believed that even with today's computer advances resolving all spatiotemporal scales described by the governing equations remains a remote target. This realization has prompted intense efforts to develop model order reduction techniques. In recent years, techniques based on deep recurrent neural networks have produced promising results for the modeling and simulation of complex spatiotemporal systems and offer large flexibility in model development as they can incorporate experimental and computational data. However, neural networks lack interpretability, which limits their utility and generalizability across complex systems. Here we propose a novel framework of Interpretable Learning Effective Dynamics (iLED) that offers comparable accuracy to state-of-the-art recurrent neural network-based approaches while providing the added benefit of interpretability. The iLED framework is motivated by Mori-Zwanzig and Koopman operator theory, which justifies the choice of the specific architecture. We demonstrate the effectiveness of the proposed framework in simulations of three benchmark multiscale systems. Our results show that the iLED framework can generate accurate predictions and obtain interpretable dynamics, making it a promising approach for solving high-dimensional multiscale systems.
</details>
<details>
<summary>摘要</summary>
高维度多尺度系统的模拟和仿真是现代科学和工程领域的核心挑战。广泛认为，即使今天的计算技术得到进步，解决所有空间时间尺度的 governing 方程仍然是一个远方目标。这一 realizations 促使了对模型简化技术的努力。在过去几年，基于深度循环神经网络的技术已经在复杂空间时间系统的模拟和仿真中提供了有希望的结果，并且可以包含实验室和计算数据。然而，神经网络缺乏可解性，这限制了其应用和普遍性，特别是在复杂系统中。我们提出了一种新的框架，即可解释性学习有效动力（iLED）框架。该框架基于 Mori-Zwanzig 和 Koopman 运算理论，这个选择的特定架构是合理的。我们在三个标准多尺度系统的 simulations 中证明了该框架的有效性。我们的结果表明，iLED 框架可以生成准确预测和获得可解性动力，使其成为解决高维度多尺度系统的有力的方法。
</details></li>
</ul>
<hr>
<h2 id="Predicting-the-Radiation-Field-of-Molecular-Clouds-using-Denoising-Diffusion-Probabilistic-Models"><a href="#Predicting-the-Radiation-Field-of-Molecular-Clouds-using-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="Predicting the Radiation Field of Molecular Clouds using Denoising Diffusion Probabilistic Models"></a>Predicting the Radiation Field of Molecular Clouds using Denoising Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05811">http://arxiv.org/abs/2309.05811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Xu, Stella Offner, Robert Gutermuth, Michael Grudic, David Guszejnov, Philip Hopkins</li>
<li>for: 这篇论文的目的是量化星系形成过程中辐射反馈的影响，以便更好地理解星系形成的物理过程。</li>
<li>methods: 这篇论文使用了深度学习技术，具体来说是denoising diffusion probabilistic models (DDPMs)，来预测Interstellar Radiation Field (ISRF) 的强度，基于三种频谱尘埃辐射的观测数据。</li>
<li>results: 论文通过对STARFORGE项目的 magnetohydrodynamic模拟和Monoceros R2 (MonR2)星系形成区的观测数据进行训练，成功地预测了ISRF的分布。这些预测结果与实际值之间的偏差在0.1倍以内，并且模型可以有效地约束ISRF的相对强度在0.2倍之间。<details>
<summary>Abstract</summary>
Accurately quantifying the impact of radiation feedback in star formation is challenging. To address this complex problem, we employ deep learning techniques, denoising diffusion probabilistic models (DDPMs), to predict the interstellar radiation field (ISRF) strength based on three-band dust emission at 4.5 \um, 24 \um, and 250 \um. We adopt magnetohydrodynamic simulations from the STARFORGE (STAR FORmation in Gaseous Environments) project that model star formation and giant molecular cloud (GMC) evolution. We generate synthetic dust emission maps matching observed spectral energy distributions in the Monoceros R2 (MonR2) GMC. We train DDPMs to estimate the ISRF using synthetic three-band dust emission. The dispersion between the predictions and true values is within a factor of 0.1 for the test set. We extended our assessment of the diffusion model to include new simulations with varying physical parameters. While there is a consistent offset observed in these out-of-distribution simulations, the model effectively constrains the relative intensity to within a factor of 2. Meanwhile, our analysis reveals weak correlation between the ISRF solely derived from dust temperature and the actual ISRF. We apply our trained model to predict the ISRF in MonR2, revealing a correspondence between intense ISRF, bright sources, and high dust emission, confirming the model's ability to capture ISRF variations. Our model robustly predicts radiation feedback distribution, even in complex, poorly constrained ISRF environments like those influenced by nearby star clusters. However, precise ISRF predictions require an accurate training dataset mirroring the target molecular cloud's unique physical conditions.
</details>
<details>
<summary>摘要</summary>
准确量化星系形成中辐射反馈的影响是一项复杂的问题。为了解决这个问题，我们使用深度学习技术，杂散扩散概率模型（DDPMs），预测Interstellar Radiation Field（ISRF）强度基于三个频谱带的尘埃辐射。我们采用了STARFORGE项目中的磁液动学模拟，模拟星系形成和大分子云（GMC）的演化。我们生成了与观测 спектраль能量分布匹配的人造尘埃辐射图像。我们使用这些图像训练 DDPMs，以便估算ISRF。我们发现在测试集上，模型的预测与真实值之间的差异在一个因子0.1之内。我们对模型进行了进一步的评估，包括在不同物理参数下运行的新的simulation。虽然在这些 OUT-OF-distribution 的simulation中，我们 observes a consistent offset，但模型 Effectively constrains the relative intensity to within a factor of 2.而我们的分析发现，尘埃温度 solo  derivated ISRF 和实际 ISRF 之间存在弱相关性。我们使用我们的训练模型来预测 MonR2 中的 ISRF，发现与强 ISRF 和高尘埃辐射相对应。我们的模型可靠地预测辐射反馈分布，即使在复杂、不充分约束的 ISRF 环境中。然而，精确地预测 ISRF 需要一个准确的训练集，这个训练集必须反映目标分子云的特定物理条件。
</details></li>
</ul>
<hr>
<h2 id="Online-ML-Self-adaptation-in-Face-of-Traps"><a href="#Online-ML-Self-adaptation-in-Face-of-Traps" class="headerlink" title="Online ML Self-adaptation in Face of Traps"></a>Online ML Self-adaptation in Face of Traps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05805">http://arxiv.org/abs/2309.05805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Töpfer, František Plášil, Tomáš Bureš, Petr Hnětynka, Martin Kruliš, Danny Weyns</li>
<li>for: 本研究旨在探讨在智能农业场景中应用在线机器学习自适应系统中的陷阱。</li>
<li>methods: 本研究使用了在线机器学习 estimator 的规范和在线训练，并评估了这些 estimator 的影响。</li>
<li>results: 研究发现了一些陷阱，包括规范和在线训练的影响，以及如何评估 estimator 的方法。这些结果可以作为其他研究人员和实践者在应用在线机器学习自适应系统时的指南。<details>
<summary>Abstract</summary>
Online machine learning (ML) is often used in self-adaptive systems to strengthen the adaptation mechanism and improve the system utility. Despite such benefits, applying online ML for self-adaptation can be challenging, and not many papers report its limitations. Recently, we experimented with applying online ML for self-adaptation of a smart farming scenario and we had faced several unexpected difficulties -- traps -- that, to our knowledge, are not discussed enough in the community. In this paper, we report our experience with these traps. Specifically, we discuss several traps that relate to the specification and online training of the ML-based estimators, their impact on self-adaptation, and the approach used to evaluate the estimators. Our overview of these traps provides a list of lessons learned, which can serve as guidance for other researchers and practitioners when applying online ML for self-adaptation.
</details>
<details>
<summary>摘要</summary>
在线机器学习（ML）常常用于自适应系统以增强适应机制并提高系统的用用。 DESPITE 这些优点，将在线ML应用于自适应可能是问题，而且不多的论文就讨论了这些问题的限制。 最近，我们尝试在智能农业场景中应用在线ML自适应，并遇到了许多意外的困难（陷阱），我们知道，在社区中不太多讨论这些问题。 在这篇论文中，我们详细讨论了这些陷阱，包括在线ML基于估计器的规则和在线培训的影响，以及自适应的方法。 我们的概述这些陷阱提供了一个指南，可以帮助其他研究者和实践者当在线ML应用自适应时遇到的问题。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Energy-Based-Models-as-Policies-Ranking-Noise-Contrastive-Estimation-and-Interpolating-Energy-Models"><a href="#Revisiting-Energy-Based-Models-as-Policies-Ranking-Noise-Contrastive-Estimation-and-Interpolating-Energy-Models" class="headerlink" title="Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models"></a>Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05803">http://arxiv.org/abs/2309.05803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumeet Singh, Stephen Tu, Vikas Sindhwani</li>
<li>for: 本研究的目的是探讨能量基本模型（EBM）作为策略表示的可行性。</li>
<li>methods: 研究人员提出了一种实用的训练目标和算法，使得EBM可以成功地训练。这种方法结合了几个关键元素：（i）排名噪音对比估计（R-NCE），（ii）可学习负样本，以及（iii）非对抗共同训练。</li>
<li>results: 研究人员通过实验发现，使用EBM作为策略表示可以与 diffusion models 和其他现有方法竞争，并在多个多模态benchmark中表现出色，包括避免障碍物和推动块。<details>
<summary>Abstract</summary>
A crucial design decision for any robot learning pipeline is the choice of policy representation: what type of model should be used to generate the next set of robot actions? Owing to the inherent multi-modal nature of many robotic tasks, combined with the recent successes in generative modeling, researchers have turned to state-of-the-art probabilistic models such as diffusion models for policy representation. In this work, we revisit the choice of energy-based models (EBM) as a policy class. We show that the prevailing folklore -- that energy models in high dimensional continuous spaces are impractical to train -- is false. We develop a practical training objective and algorithm for energy models which combines several key ingredients: (i) ranking noise contrastive estimation (R-NCE), (ii) learnable negative samplers, and (iii) non-adversarial joint training. We prove that our proposed objective function is asymptotically consistent and quantify its limiting variance. On the other hand, we show that the Implicit Behavior Cloning (IBC) objective is actually biased even at the population level, providing a mathematical explanation for the poor performance of IBC trained energy policies in several independent follow-up works. We further extend our algorithm to learn a continuous stochastic process that bridges noise and data, modeling this process with a family of EBMs indexed by scale variable. In doing so, we demonstrate that the core idea behind recent progress in generative modeling is actually compatible with EBMs. Altogether, our proposed training algorithms enable us to train energy-based models as policies which compete with -- and even outperform -- diffusion models and other state-of-the-art approaches in several challenging multi-modal benchmarks: obstacle avoidance path planning and contact-rich block pushing.
</details>
<details>
<summary>摘要</summary>
robot学习管道中的关键设计决策是选择策略表示方式：使用哪种模型生成下一个机器人动作？由于许多机器人任务的自然多Modal性，加上近年来的生成模型成功，研究人员已经转向当今最先进的概率模型，如扩散模型，作为策略表示方式。在这种工作中，我们重新考虑使用能量模型（EBM）作为策略类型。我们证明了 prevailing folklore ――高维离散空间中的能量模型是不可行的――是错误的。我们开发了实用的训练目标和算法，其中包括以下几个关键元素：（i）排名噪音对比估计（R-NCE），（ii）可学习的负样本，以及（iii）非对抗共同训练。我们证明我们的提议的目标函数是 asymptotically consistent，并且量化其限界方差。然而，我们显示了启发行为嵌入（IBC）目标是偏向的，并提供了数学解释，以解释在多个独立跟进工作中，IBC训练的能量策略表现不佳。此外，我们还扩展了我们的算法，以学习一个连续随机过程，该过程将噪音和数据相连，并使用一个家族的 EBMs 指标。在这种情况下，我们证明了生成模型的核心想法和 EBMs 是兼容的。总之，我们的提议的训练算法可以训练能量模型作为策略，与扩散模型和其他当今最先进方法在多个复杂多Modal 标准准则中竞争。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Hyperedge-Prediction-with-Context-Aware-Self-Supervised-Learning"><a href="#Enhancing-Hyperedge-Prediction-with-Context-Aware-Self-Supervised-Learning" class="headerlink" title="Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning"></a>Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05798">http://arxiv.org/abs/2309.05798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yy-ko/cash">https://github.com/yy-ko/cash</a></li>
<li>paper_authors: Yunyong Ko, Hanghang Tong, Sang-Wook Kim</li>
<li>for: 这个论文主要用于解决hyperedge prediction问题，即预测未知的超链接（group-wise relations）。</li>
<li>methods: 该论文提出了一种新的hyperedge prediction框架（CASH），使用context-aware node aggregation和自supervised contrastive learning来提高hypergraph表示性和预测精度。</li>
<li>results: 实验结果显示，CASH在六个实际的超链接上的预测精度高于所有竞争方法，并且每一种提出的策略都有助于提高CASH模型的准确率。Here’s the format of the information in Simplified Chinese text:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Hypergraphs can naturally model group-wise relations (e.g., a group of users who co-purchase an item) as hyperedges. Hyperedge prediction is to predict future or unobserved hyperedges, which is a fundamental task in many real-world applications (e.g., group recommendation). Despite the recent breakthrough of hyperedge prediction methods, the following challenges have been rarely studied: (C1) How to aggregate the nodes in each hyperedge candidate for accurate hyperedge prediction? and (C2) How to mitigate the inherent data sparsity problem in hyperedge prediction? To tackle both challenges together, in this paper, we propose a novel hyperedge prediction framework (CASH) that employs (1) context-aware node aggregation to precisely capture complex relations among nodes in each hyperedge for (C1) and (2) self-supervised contrastive learning in the context of hyperedge prediction to enhance hypergraph representations for (C2). Furthermore, as for (C2), we propose a hyperedge-aware augmentation method to fully exploit the latent semantics behind the original hypergraph and consider both node-level and group-level contrasts (i.e., dual contrasts) for better node and hyperedge representations. Extensive experiments on six real-world hypergraphs reveal that CASH consistently outperforms all competing methods in terms of the accuracy in hyperedge prediction and each of the proposed strategies is effective in improving the model accuracy of CASH. For the detailed information of CASH, we provide the code and datasets at: https://github.com/yy-ko/cash.
</details>
<details>
<summary>摘要</summary>
“几何グラフ”（hypergraph）可以自然地模型群聚关系（例如，一群用户购买同一款商品）作为几何边（hyperedge）。几何边预测是几何グラフ应用中的基本任务之一（例如，群组推荐）。Despite the recent breakthrough of 几何边预测方法，以下两个挑战很少被研究：（C1）如何将几何边候选中的节点组合数据准确地预测几何边？和（C2）如何缓和几何边预测中的自然数据罕规问题？To tackle both challenges together, in this paper, we propose a novel 几何边预测框架（CASH），该框架使用（1）具体考虑几何边候选中每个节点的背景信息，以精确地捕捉几何边中每个节点之间的复杂关系，来满足（C1），以及（2）在几何边预测的上下文中，透过自适应对称学习，对几何边预测进行增强，来缓和几何边预测中的数据罕规问题，来满足（C2）。此外，为了缓和几何边预测中的数据罕规问题，我们还提出了一个几何边测试方法，具体来说是通过对几何边预测模型进行几何边层次抽象，对几何边预测进行双层次对称抽象，以便更好地捕捉几何边的内在 semantics。实验结果显示，CASH 在六个真实世界几何グラフ上 consistently outperform 所有竞争方法，并且每个提出的策略都是增强 CASH 的模型精度的有效方法。详细信息请参考：https://github.com/yy-ko/cash。”
</details></li>
</ul>
<hr>
<h2 id="On-the-Fine-Grained-Hardness-of-Inverting-Generative-Models"><a href="#On-the-Fine-Grained-Hardness-of-Inverting-Generative-Models" class="headerlink" title="On the Fine-Grained Hardness of Inverting Generative Models"></a>On the Fine-Grained Hardness of Inverting Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05795">http://arxiv.org/abs/2309.05795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feyza Duman Keles, Chinmay Hegde</li>
<li>for: 本研究的目的是为Generative模型的反射问题提供一个细化的视角，即查找一个大小为$n$的latent vector，使得generative模型的输出与给定的target匹配得非常好。</li>
<li>methods: 本研究使用了多种方法，包括几何学的抽象和复杂度分析，以及来自计算复杂性理论的抽象和技巧。</li>
<li>results: 本研究提供了一些新的下界，证明了generative模型的反射问题在某些情况下的计算复杂度是$\Omega(2^n)$，这是在计算复杂性理论中的一个新成果。<details>
<summary>Abstract</summary>
The objective of generative model inversion is to identify a size-$n$ latent vector that produces a generative model output that closely matches a given target. This operation is a core computational primitive in numerous modern applications involving computer vision and NLP. However, the problem is known to be computationally challenging and NP-hard in the worst case. This paper aims to provide a fine-grained view of the landscape of computational hardness for this problem. We establish several new hardness lower bounds for both exact and approximate model inversion. In exact inversion, the goal is to determine whether a target is contained within the range of a given generative model. Under the strong exponential time hypothesis (SETH), we demonstrate that the computational complexity of exact inversion is lower bounded by $\Omega(2^n)$ via a reduction from $k$-SAT; this is a strengthening of known results. For the more practically relevant problem of approximate inversion, the goal is to determine whether a point in the model range is close to a given target with respect to the $\ell_p$-norm. When $p$ is a positive odd integer, under SETH, we provide an $\Omega(2^n)$ complexity lower bound via a reduction from the closest vectors problem (CVP). Finally, when $p$ is even, under the exponential time hypothesis (ETH), we provide a lower bound of $2^{\Omega (n)}$ via a reduction from Half-Clique and Vertex-Cover.
</details>
<details>
<summary>摘要</summary>
目标是使用生成模型进行逆向模型，即找到一个大小为$n$的隐藏向量，使得生成模型输出与给定目标匹配得非常好。这是现代计算机视觉和自然语言处理中的一种核心计算基础。然而，这个问题知道是计算上具有NP困难的worst-case。本文旨在为这个问题提供细腻的视野，并确定了一些新的困难下界。在精确的逆向模型中，目标是确定给定生成模型是否包含一个目标。在STRONG EXPONENTIAL TIME HYPOTHESIS（SETH）下，我们证明了逆向模型的计算复杂度是$\Omega(2^n)$，这是现有结果的强化。在更实用的逆向模型中，目标是确定一个模型范围内的点是否与给定目标之间的$\ell_p$-范数相似。当$p$是正的奇数时，在SETH下，我们提供了$\Omega(2^n)$的困难下界，via reduction from closest vectors problem（CVP）。而当$p$是偶数时，在EXPONENTIAL TIME HYPOTHESIS（ETH）下，我们提供了$2^{\Omega(n)}$的困难下界，via reduction from Half-Clique and Vertex-Cover。
</details></li>
</ul>
<hr>
<h2 id="Smartwatch-derived-Acoustic-Markers-for-Deficits-in-Cognitively-Relevant-Everyday-Functioning"><a href="#Smartwatch-derived-Acoustic-Markers-for-Deficits-in-Cognitively-Relevant-Everyday-Functioning" class="headerlink" title="Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning"></a>Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05777">http://arxiv.org/abs/2309.05777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasunori Yamada, Kaoru Shinkawa, Masatomo Kobayashi, Miyuki Nemoto, Miho Ota, Kiyotaka Nemoto, Tetsuaki Arai</li>
<li>for: 早期发现脑性功能障碍的检测是重要的，特别是讨论阿兹海默病。现有的评估标准是基于主观评价。 however， speech 有可能提供准确的对� stato markers。</li>
<li>methods: 我们使用smartwatch应用程序收集语音特征作为对� estado markers。我们从54名老年人手中收集了语音数据，包括认知任务和日常对话，以及一种普遍的每日功能测试。</li>
<li>results: 我们的结果表明，使用语音特征可以准确地检测一些日常功能障碍。我们使用机器学习模型，可以在68.5%的准确率下，检测出使用标准神经心理测试不能检测到的障碍。此外，我们还发现了一些通用的语音特征，可以强制对� estado功能障碍的识别。<details>
<summary>Abstract</summary>
Detection of subtle deficits in everyday functioning due to cognitive impairment is important for early detection of neurodegenerative diseases, particularly Alzheimer's disease. However, current standards for assessment of everyday functioning are based on qualitative, subjective ratings. Speech has been shown to provide good objective markers for cognitive impairments, but the association with cognition-relevant everyday functioning remains uninvestigated. In this study, we demonstrate the feasibility of using a smartwatch-based application to collect acoustic features as objective markers for detecting deficits in everyday functioning. We collected voice data during the performance of cognitive tasks and daily conversation, as possible application scenarios, from 54 older adults, along with a measure of everyday functioning. Machine learning models using acoustic features could detect individuals with deficits in everyday functioning with up to 77.8% accuracy, which was higher than the 68.5% accuracy with standard neuropsychological tests. We also identified common acoustic features for robustly discriminating deficits in everyday functioning across both types of voice data (cognitive tasks and daily conversation). Our results suggest that common acoustic features extracted from different types of voice data can be used as markers for deficits in everyday functioning.
</details>
<details>
<summary>摘要</summary>
检测日常功能下降 Due to cognitive impairment 是早期发现 neurosurgery 疾病，特别是阿尔茨海默病的关键。然而，现有的日常功能评估标准是基于主观的评分。speech 已经被证明可以提供good 的对象标志，但与认知有关的日常功能之间的关系还没有被调查。本研究示出了使用智能手表应用程序收集语音特征作为对日常功能下降的对象标志的可能性。我们收集了54名老年人的语音数据，包括认知任务和日常对话，以及一种测量日常功能的指标。使用语音特征的机器学习模型可以准确地检测出日常功能下降，准确率高达77.8%，比标准神经生理测试的68.5%高。我们还发现了日常功能下降的共同语音特征，可以在不同类型的语音数据中robustly 分类日常功能下降。我们的结果表明，共同语音特征可以作为日常功能下降的标志。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-Intrinsic-Dimension-on-Metric-Learning-under-Compression"><a href="#The-Effect-of-Intrinsic-Dimension-on-Metric-Learning-under-Compression" class="headerlink" title="The Effect of Intrinsic Dimension on Metric Learning under Compression"></a>The Effect of Intrinsic Dimension on Metric Learning under Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05751">http://arxiv.org/abs/2309.05751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Efstratios Palias, Ata Kabán</li>
<li>for: 本研究的目的是为了提高距离基于学习算法的性能，通过找到适当的距离度量函数。</li>
<li>methods: 本研究使用了随机压缩数据来训练全级度度量函数，并提供了对随机压缩的误差 bounds。</li>
<li>results: 实验结果表明，在高维设定下，采用随机压缩训练的方法可以提高距离基于学习算法的性能，并且 bounds 不依赖于环境维度。<details>
<summary>Abstract</summary>
Metric learning aims at finding a suitable distance metric over the input space, to improve the performance of distance-based learning algorithms. In high-dimensional settings, metric learning can also play the role of dimensionality reduction, by imposing a low-rank restriction to the learnt metric. In this paper, instead of training a low-rank metric on high-dimensional data, we consider a randomly compressed version of the data, and train a full-rank metric there. We give theoretical guarantees on the error of distance-based metric learning, with respect to the random compression, which do not depend on the ambient dimension. Our bounds do not make any explicit assumptions, aside from i.i.d. data from a bounded support, and automatically tighten when benign geometrical structures are present. Experimental results on both synthetic and real data sets support our theoretical findings in high-dimensional settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CaloClouds-II-Ultra-Fast-Geometry-Independent-Highly-Granular-Calorimeter-Simulation"><a href="#CaloClouds-II-Ultra-Fast-Geometry-Independent-Highly-Granular-Calorimeter-Simulation" class="headerlink" title="CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation"></a>CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05704">http://arxiv.org/abs/2309.05704</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flc-qu-hep/caloclouds-2">https://github.com/flc-qu-hep/caloclouds-2</a></li>
<li>paper_authors: Erik Buhmann, Frank Gaede, Gregor Kasieczka, Anatolii Korol, William Korcari, Katja Krüger, Peter McKeown</li>
<li>for: 高精度探测器的能量储存速度加速是未来冲击实验所需的。</li>
<li>methods: 使用生成型机器学习（ML）模型加速和补充传统 simulate chain 的物理分析。</li>
<li>results: 新的 CaloClouds II 模型提供了许多关键改进，包括连续时间分数基本模型，可以与 Geant4 相比，在单个 CPU 上实现 $6\times$ 的速度提升，并且通过简化储存模型为准确抽象模型，实现了 $46\times$ ($37\times$) 的速度提升。<details>
<summary>Abstract</summary>
Fast simulation of the energy depositions in high-granular detectors is needed for future collider experiments with ever increasing luminosities. Generative machine learning (ML) models have been shown to speed up and augment the traditional simulation chain in physics analysis. However, the majority of previous efforts were limited to models relying on fixed, regular detector readout geometries. A major advancement is the recently introduced CaloClouds model, a geometry-independent diffusion model, which generates calorimeter showers as point clouds for the electromagnetic calorimeter of the envisioned International Large Detector (ILD).   In this work, we introduce CaloClouds II which features a number of key improvements. This includes continuous time score-based modelling, which allows for a 25 step sampling with comparable fidelity to CaloClouds while yielding a $6\times$ speed-up over Geant4 on a single CPU ($5\times$ over CaloClouds). We further distill the diffusion model into a consistency model allowing for accurate sampling in a single step and resulting in a $46\times$ ($37\times$) speed-up. This constitutes the first application of consistency distillation for the generation of calorimeter showers.
</details>
<details>
<summary>摘要</summary>
In this work, we introduce CaloClouds II, which features a number of key improvements. This includes continuous time score-based modeling, which allows for a 25-step sampling with comparable fidelity to CaloClouds while yielding a 6x speed-up over Geant4 on a single CPU (5x over CaloClouds). We further distill the diffusion model into a consistency model, allowing for accurate sampling in a single step and resulting in a 46x (37x) speed-up. This constitutes the first application of consistency distillation for the generation of calorimeter showers.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Machine-Learning-Techniques-for-Exploring-Tropical-Coamoeba-Brane-Tilings-and-Seiberg-Duality"><a href="#Unsupervised-Machine-Learning-Techniques-for-Exploring-Tropical-Coamoeba-Brane-Tilings-and-Seiberg-Duality" class="headerlink" title="Unsupervised Machine Learning Techniques for Exploring Tropical Coamoeba, Brane Tilings and Seiberg Duality"></a>Unsupervised Machine Learning Techniques for Exploring Tropical Coamoeba, Brane Tilings and Seiberg Duality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05702">http://arxiv.org/abs/2309.05702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rak-Kyeong Seong</li>
<li>for: 这个论文的目的是用无监督机器学习技术来识别四维N&#x3D;1维度超Symmetric gauge理论中的拓扑阶段，这些理论是D3- branes在toric Calabi-Yau 3-fold中的世界量理论。</li>
<li>methods: 这篇论文使用了无监督机器学习技术，如principal component analysis (PCA)和t-distributed stochastic neighbor embedding (t-SNE)，来将复数结构参数的变化所对应的coamoeba和相关的brane tilingsProject down to a lower-dimensional phase space with phase boundaries corresponding to Seiberg duality。</li>
<li>results: 这篇论文的结果是通过使用无监督机器学习技术，可以将复数结构参数的变化所对应的coamoeba和相关的brane tilingsProject down to a lower-dimensional phase space with phase boundaries corresponding to Seiberg duality，并且在这个2-dimensional phase diagram中，可以看到Seiberg duality的相关关系。<details>
<summary>Abstract</summary>
We introduce unsupervised machine learning techniques in order to identify toric phases of 4d N=1 supersymmetric gauge theories corresponding to the same toric Calabi-Yau 3-fold. These 4d N=1 supersymmetric gauge theories are worldvolume theories of a D3-brane probing a toric Calabi-Yau 3-fold and are realized in terms of a Type IIB brane configuration known as a brane tiling. It corresponds to the skeleton graph of the coamoeba projection of the mirror curve associated to the toric Calabi-Yau 3-fold. When we vary the complex structure moduli of the mirror Calabi-Yau 3-fold, the coamoeba and the corresponding brane tilings change their shape, giving rise to different toric phases related by Seiberg duality. We illustrate that by employing techniques such as principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE), we can project the space of coamoeba labelled by complex structure moduli down to a lower dimensional phase space with phase boundaries corresponding to Seiberg duality. In this work, we illustrate this technique by obtaining a 2-dimensional phase diagram for brane tilings corresponding to the cone over the zeroth Hirzebruch surface F0.
</details>
<details>
<summary>摘要</summary>
我们引入无监督机器学习技术，以识别四维N=1瑞奖场论的托立阶段。这些四维N=1瑞奖场论是D3-节点探测托立Calabi-Yau 3-次元的世界体系，并且可以表示为Type IIB节点配置所对应的节点矩阵。它对应到镜对称Calabi-Yau 3-次元的对偶矩阵。当我们变化镜对称Calabi-Yau 3-次元的复数结构参数，节点和相应的节点矩阵会改变形状，从而导致不同的托立阶段相互关联。我们使用技术如主成分分析（PCA）和t-分布随机邻近测度（t-SNE），可以将节点参数下的空间对应到较低维度的阶段空间，阶段边界与Seiberg对偶相关。在这个研究中，我们这种技术以获得F0的极点 Hirzebruch 面上的节点矩阵对应的2-维阶段图。
</details></li>
</ul>
<hr>
<h2 id="On-the-quality-of-randomized-approximations-of-Tukey’s-depth"><a href="#On-the-quality-of-randomized-approximations-of-Tukey’s-depth" class="headerlink" title="On the quality of randomized approximations of Tukey’s depth"></a>On the quality of randomized approximations of Tukey’s depth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05657">http://arxiv.org/abs/2309.05657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Briend, Gábor Lugosi, Roberto Imbuzeiro Oliveira</li>
<li>for: 这个论文是为了研究Tukey深度的准确计算问题，特别是在高维度时。</li>
<li>methods: 这篇论文使用随机化方法来估计Tukey深度，并研究这种方法在不同情况下的性能。</li>
<li>results: 研究结果显示，如果要求算法在维度上运行时间 polynomial，那么随机化方法可以准确地估计 maximal depth 和 depths close to zero，但是对于中间深度的任意点，任何好的估计都需要 exponential complexity。<details>
<summary>Abstract</summary>
Tukey's depth (or halfspace depth) is a widely used measure of centrality for multivariate data. However, exact computation of Tukey's depth is known to be a hard problem in high dimensions. As a remedy, randomized approximations of Tukey's depth have been proposed. In this paper we explore when such randomized algorithms return a good approximation of Tukey's depth. We study the case when the data are sampled from a log-concave isotropic distribution. We prove that, if one requires that the algorithm runs in polynomial time in the dimension, the randomized algorithm correctly approximates the maximal depth $1/2$ and depths close to zero. On the other hand, for any point of intermediate depth, any good approximation requires exponential complexity.
</details>
<details>
<summary>摘要</summary>
土耳其的深度（或半空间深度）是多变量数据中广泛使用的中心性指标。然而，对高维数据进行准确计算的Tukey的深度知道是一个困难的问题。为了缓解这个问题，随机化Tukey的深度的算法已经被提出。在这篇论文中，我们研究了这些随机算法是否能够给出Tukey的深度的好 aproximation。我们研究了数据来自具有卷积的均匀分布的情况。我们证明，如果要求算法在维度上运行在多项式时间内，那么随机算法可以正确地approximates maximal depth 1/2和 depths close to zero。然而，对于任何中间深度的点，任何好的approximation都需要对数复杂度。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Handover-Throw-and-Catch-with-Bimanual-Hands"><a href="#Dynamic-Handover-Throw-and-Catch-with-Bimanual-Hands" class="headerlink" title="Dynamic Handover: Throw and Catch with Bimanual Hands"></a>Dynamic Handover: Throw and Catch with Bimanual Hands</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05655">http://arxiv.org/abs/2309.05655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Qin, Yaodong Yang, Nikolay Atanasov, Xiaolong Wang</li>
<li>for: 这个论文的目的是解决人工智能机器人在投捕和捕获物体时遇到的挑战，如高速动作、精准协作和对多种物体的交互。</li>
<li>methods: 作者使用了多个拟合机器人臂上的多指手系统，并使用多任务学习和实验室转移来训练该系统。为了 bridging the Sim2Real gap，作者提供了多种新的算法设计，包括学习一个物体轨迹预测模型，以帮助机器人捕手在实时情况下了解物体的运动轨迹，并根据此反应。</li>
<li>results: 作者在实验中使用多种物体，并与多个基eline进行比较，显示了 significannot improvements。作者的项目页面可以在 \url{<a target="_blank" rel="noopener" href="https://binghao-huang.github.io/dynamic_handover/%7D">https://binghao-huang.github.io/dynamic_handover/}</a> 上找到。<details>
<summary>Abstract</summary>
Humans throw and catch objects all the time. However, such a seemingly common skill introduces a lot of challenges for robots to achieve: The robots need to operate such dynamic actions at high-speed, collaborate precisely, and interact with diverse objects. In this paper, we design a system with two multi-finger hands attached to robot arms to solve this problem. We train our system using Multi-Agent Reinforcement Learning in simulation and perform Sim2Real transfer to deploy on the real robots. To overcome the Sim2Real gap, we provide multiple novel algorithm designs including learning a trajectory prediction model for the object. Such a model can help the robot catcher has a real-time estimation of where the object will be heading, and then react accordingly. We conduct our experiments with multiple objects in the real-world system, and show significant improvements over multiple baselines. Our project page is available at \url{https://binghao-huang.github.io/dynamic_handover/}.
</details>
<details>
<summary>摘要</summary>
人类常常投掷和捕获物体，但这些动作却对机器人带来许多挑战：机器人需要在高速下进行动态动作，协同准确地操作，并与多种物体进行交互。在这篇论文中，我们设计了两个多 fingers 手 attachment 到机器人臂，以解决这个问题。我们使用多智能 reinforcement learning 在模拟环境中训练我们的系统，并通过 Sim2Real 跨度传输部署到真实机器人上。为了bridging Sim2Real gap，我们提供了多种新算法设计，包括学习物体运动预测模型。这种模型可以帮助机器人捕获器在实时情况下获得物体的预测位置，然后按照相应的反应。我们在实际系统中进行了多种物体的实验，并显示了多个基准值的改进。我们的项目页面可以在 \url{https://binghao-huang.github.io/dynamic_handover/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Data-efficiency-dimensionality-reduction-and-the-generalized-symmetric-information-bottleneck"><a href="#Data-efficiency-dimensionality-reduction-and-the-generalized-symmetric-information-bottleneck" class="headerlink" title="Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck"></a>Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05649">http://arxiv.org/abs/2309.05649</a></li>
<li>repo_url: None</li>
<li>paper_authors: K. Michael Martini, Ilya Nemenman</li>
<li>for:  simultaneous compression of two random variables to preserve information</li>
<li>methods:  Generalized Symmetric Information Bottleneck (GSIB) with different functional forms of the cost</li>
<li>results:  qualitatively less data required for simultaneous compression compared to compressing variables one at a time, demonstrating the principle of simultaneous compression being more data efficient.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个论文是为了同时压缩两个随机变量以保留信息的方法。</li>
<li>methods: 该方法是通过不同的函数形式来定义成本的扩展了信息瓶颈。</li>
<li>results: 通过分析不同的数据集大小，我们发现在典型的情况下，同时压缩两个变量的压缩需要比独立压缩每个变量的数据量更少，这是同时压缩的一个例子，表明同时压缩是独立压缩每个变量的更高效的方法。<details>
<summary>Abstract</summary>
The Symmetric Information Bottleneck (SIB), an extension of the more familiar Information Bottleneck, is a dimensionality reduction technique that simultaneously compresses two random variables to preserve information between their compressed versions. We introduce the Generalized Symmetric Information Bottleneck (GSIB), which explores different functional forms of the cost of such simultaneous reduction. We then explore the dataset size requirements of such simultaneous compression. We do this by deriving bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions. We show that, in typical situations, the simultaneous GSIB compression requires qualitatively less data to achieve the same errors compared to compressing variables one at a time. We suggest that this is an example of a more general principle that simultaneous compression is more data efficient than independent compression of each of the input variables.
</details>
<details>
<summary>摘要</summary>
“ симметричный информационный бутылочный”（SIB）是一种维度减少技术，它同时压缩两个随机变量，以保留它们压缩后的信息之间的相互关系。我们引入了“总体化 симметричный информационный бутылочный”（GSIB），它探讨了不同的函数形式，以优化这种同时压缩的成本。我们然后研究了这种同时压缩的数据大小需求，通过计算涨落函数的下界和均方差估计来评估相关的统计噪声。我们发现，在常见的情况下，同时压缩的GSIB需要较少的数据来实现相同的错误率，而独立压缩每个输入变量的情况下，需要更多的数据。我们认为，这是一种更通用的原理，即同时压缩是独立压缩每个输入变量的数据效率更高的。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Supervised-Deep-Learning-Solution-to-Detect-Distributed-Denial-of-Service-DDoS-attacks-on-Edge-Systems-using-Convolutional-Neural-Networks-CNN"><a href="#A-Novel-Supervised-Deep-Learning-Solution-to-Detect-Distributed-Denial-of-Service-DDoS-attacks-on-Edge-Systems-using-Convolutional-Neural-Networks-CNN" class="headerlink" title="A Novel Supervised Deep Learning Solution to Detect Distributed Denial of Service (DDoS) attacks on Edge Systems using Convolutional Neural Networks (CNN)"></a>A Novel Supervised Deep Learning Solution to Detect Distributed Denial of Service (DDoS) attacks on Edge Systems using Convolutional Neural Networks (CNN)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05646">http://arxiv.org/abs/2309.05646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VedanthR5/A-Novel-Deep-Learning-Solution-to-detect-DDoS-attacks-using-Neural-Networks">https://github.com/VedanthR5/A-Novel-Deep-Learning-Solution-to-detect-DDoS-attacks-using-Neural-Networks</a></li>
<li>paper_authors: Vedanth Ramanathan, Krish Mahadevan, Sejal Dua</li>
<li>for: 本研究旨在探讨一种基于深度学习的网络流量中的DDOS攻击检测方法，以帮助防止DDOS攻击对互联网安全造成威胁。</li>
<li>methods: 本研究使用了 convolutional neural network (CNN) 和常用的深度学习算法，开发了一种新的检测技术，可以分类为正常和恶意流量。研究人员采用了数据预处理、流量normalization和自适应抑制等方法，以提高模型的泛化能力和精度。</li>
<li>results: 本研究的结果表明，提案的检测算法在2000个未看过的网络流量中实现了检测精度为0.9883，表明该方法可以有效地检测DDOS攻击。此外，研究人员还发现该方法可以扩展到任何网络环境，并且可以满足实时检测的需求。<details>
<summary>Abstract</summary>
Cybersecurity attacks are becoming increasingly sophisticated and pose a growing threat to individuals, and private and public sectors. Distributed Denial of Service attacks are one of the most harmful of these threats in today's internet, disrupting the availability of essential services. This project presents a novel deep learning-based approach for detecting DDoS attacks in network traffic using the industry-recognized DDoS evaluation dataset from the University of New Brunswick, which contains packet captures from real-time DDoS attacks, creating a broader and more applicable model for the real world. The algorithm employed in this study exploits the properties of Convolutional Neural Networks (CNN) and common deep learning algorithms to build a novel mitigation technique that classifies benign and malicious traffic. The proposed model preprocesses the data by extracting packet flows and normalizing them to a fixed length which is fed into a custom architecture containing layers regulating node dropout, normalization, and a sigmoid activation function to out a binary classification. This allows for the model to process the flows effectively and look for the nodes that contribute to DDoS attacks while dropping the "noise" or the distractors. The results of this study demonstrate the effectiveness of the proposed algorithm in detecting DDOS attacks, achieving an accuracy of .9883 on 2000 unseen flows in network traffic, while being scalable for any network environment.
</details>
<details>
<summary>摘要</summary>
“网络安全攻击日益变得更加复杂，对个人和公共领域 pose 成长中的威胁。分布式拒绝服务攻击（DDoS）是当今互联网中最有害的一种威胁，可以中断网络服务的可用性。本项目提出了一种基于深度学习的新方法，用于在网络流量中检测DDoS攻击。该方法使用了大学新不列颠的DDoS评估数据集，该数据集包含实际时间DDoS攻击的PacketCapture，创造了更广泛和更适用的模型。该算法使用了卷积神经网络和常见的深度学习算法，建立了一种新的防御技术，该技术可以分类 benign 和 malicious 流量。该提案的模型采取了数据预处理步骤，将流量拼接成包流，并将其归一化为固定长度，然后通过自定义架构，包括节点Dropout、normalization和sigmoid活化函数，进行二分类。这种方法可以让模型有效地处理流量，寻找引起DDoS攻击的节点，同时抛弃“噪音”或“拖垮”。研究结果表明，该提案的算法在2000个未看到的流量中的网络流量中的检测DDOS攻击的精度为0.9883，同时具有扩展性，适用于任何网络环境。”
</details></li>
</ul>
<hr>
<h2 id="Desenvolvimento-de-modelo-para-predicao-de-cotacoes-de-acao-baseada-em-analise-de-sentimentos-de-tweets"><a href="#Desenvolvimento-de-modelo-para-predicao-de-cotacoes-de-acao-baseada-em-analise-de-sentimentos-de-tweets" class="headerlink" title="Desenvolvimento de modelo para predição de cotações de ação baseada em análise de sentimentos de tweets"></a>Desenvolvimento de modelo para predição de cotações de ação baseada em análise de sentimentos de tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06538">http://arxiv.org/abs/2309.06538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mario Mitsuo Akita, Everton Josue da Silva</li>
<li>for: 预测股票市场价格</li>
<li>methods: 使用 iFeel 2.0 平台提取推特社交媒体上关于 Petrobras 公司的19个情感特征，然后使用这些特征训练 XBoot 模型预测未来股票价格。</li>
<li>results: 使用模型预测 Petrobras 股票价格后，在250天内实现了与100个随机模型的平均性能相比的净收益 R$88,82。<details>
<summary>Abstract</summary>
Training machine learning models for predicting stock market share prices is an active area of research since the automatization of trading such papers was available in real time. While most of the work in this field of research is done by training Neural networks based on past prices of stock shares, in this work, we use iFeel 2.0 platform to extract 19 sentiment features from posts obtained from microblog platform Twitter that mention the company Petrobras. Then, we used those features to train XBoot models to predict future stock prices for the referred company. Later, we simulated the trading of Petrobras' shares based on the model's outputs and determined the gain of R$88,82 (net) in a 250-day period when compared to a 100 random models' average performance.
</details>
<details>
<summary>摘要</summary>
研究用机器学习模型预测股票市场价格是一个活跃的领域，因为自动化交易可以在实时提供相关文献。大多数在这个领域的研究是通过以往的股票价格训练神经网络，而在这种工作中，我们使用iFeel 2.0平台提取了19个情感特征从微博平台上提到Petrobras公司的帖子。然后，我们使用这些特征训练XBoot模型预测Petrobras公司的未来股票价格。后来，我们使用模型的输出进行了Petrobras股票的交易模拟，并发现在250天内比Random Models的平均表现获得了R$88,82（净）的收益。
</details></li>
</ul>
<hr>
<h2 id="Boundary-Peeling-Outlier-Detection-Method-Using-One-Class-Peeling"><a href="#Boundary-Peeling-Outlier-Detection-Method-Using-One-Class-Peeling" class="headerlink" title="Boundary Peeling: Outlier Detection Method Using One-Class Peeling"></a>Boundary Peeling: Outlier Detection Method Using One-Class Peeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05630">http://arxiv.org/abs/2309.05630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheikh Arafat, Na Sun, Maria L. Weese, Waldyn G. Martinez</li>
<li>for: 本研究旨在提出一种不需要标注数据的潜在异常点检测算法，用于数据分析阶段中的异常点检测。</li>
<li>methods: 本算法基于一类支持向量机（One-class Support Vector Machine，SVM）的迭代封闭精度计算，生成可变的边界，并通过iteratively-peeled, flexible boundaries进行异常点检测。</li>
<li>results: 在模拟数据中，本算法在无异常情况下的性能比所有现状方法更高，并在异常情况下与参考方法相当或更高，同时在常用的数据集上也表现良好，与标准方法相当或更高。<details>
<summary>Abstract</summary>
Unsupervised outlier detection constitutes a crucial phase within data analysis and remains a dynamic realm of research. A good outlier detection algorithm should be computationally efficient, robust to tuning parameter selection, and perform consistently well across diverse underlying data distributions. We introduce One-Class Boundary Peeling, an unsupervised outlier detection algorithm. One-class Boundary Peeling uses the average signed distance from iteratively-peeled, flexible boundaries generated by one-class support vector machines. One-class Boundary Peeling has robust hyperparameter settings and, for increased flexibility, can be cast as an ensemble method. In synthetic data simulations One-Class Boundary Peeling outperforms all state of the art methods when no outliers are present while maintaining comparable or superior performance in the presence of outliers, as compared to benchmark methods. One-Class Boundary Peeling performs competitively in terms of correct classification, AUC, and processing time using common benchmark data sets.
</details>
<details>
<summary>摘要</summary>
<SYS> translate("Unsupervised outlier detection constitutes a crucial phase within data analysis and remains a dynamic realm of research.")</SYS>一种无监督异常检测算法是数据分析中的关键阶段，并且是一个动态的研究领域。一个好的异常检测算法应该具有计算效率、鲁棒性和多样性的特点。我们介绍了一种无监督边缘剥离算法，即One-Class Boundary Peeling。这种算法使用一个一类支持向量机生成的轮廓进行迭代剥离，并且具有鲁棒的超参数设置。为了增加灵活性，One-Class Boundary Peeling可以被视为一种集成方法。在模拟数据中，One-Class Boundary Peeling在没有异常时的情况下比所有现状方法高效，同时在异常存在的情况下也能够保持与参考方法相当或更高的性能。在常用的测试数据集上，One-Class Boundary Peeling在正确分类、AUC和处理时间方面与参考方法竞争。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Side-Channels-in-Machine-Learning-Systems"><a href="#Privacy-Side-Channels-in-Machine-Learning-Systems" class="headerlink" title="Privacy Side Channels in Machine Learning Systems"></a>Privacy Side Channels in Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05610">http://arxiv.org/abs/2309.05610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini, Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, Florian Tramèr</li>
<li>for: 本研究旨在掌握机器学习（ML）模型中的隐私问题，并提出了四种隐私渠道，即训练数据筛选、输入预处理、输出后处理和查询筛选，这些渠道可以帮助攻击者从 ML 模型中提取私人信息。</li>
<li>methods: 本研究使用了一系列实验和分析方法，包括训练数据筛选、输入预处理、输出后处理和查询筛选等，以探索机器学习模型中的隐私问题。</li>
<li>results: 本研究发现了四种隐私渠道，可以帮助攻击者从机器学习模型中提取私人信息，包括提高会员推理攻击和提取用户测试查询等。此外，研究还发现了一些系统组件，如训练数据筛选和输出筛选，可以帮助攻击者从 ML 模型中提取私人信息。<details>
<summary>Abstract</summary>
Most current approaches for protecting privacy in machine learning (ML) assume that models exist in a vacuum, when in reality, ML models are part of larger systems that include components for training data filtering, output monitoring, and more. In this work, we introduce privacy side channels: attacks that exploit these system-level components to extract private information at far higher rates than is otherwise possible for standalone models. We propose four categories of side channels that span the entire ML lifecycle (training data filtering, input preprocessing, output post-processing, and query filtering) and allow for either enhanced membership inference attacks or even novel threats such as extracting users' test queries. For example, we show that deduplicating training data before applying differentially-private training creates a side-channel that completely invalidates any provable privacy guarantees. Moreover, we show that systems which block language models from regenerating training data can be exploited to allow exact reconstruction of private keys contained in the training set -- even if the model did not memorize these keys. Taken together, our results demonstrate the need for a holistic, end-to-end privacy analysis of machine learning.
</details>
<details>
<summary>摘要</summary>
现有的机器学习（ML）隐私保护方法都假设模型在孤立的环境中运行，而实际上ML模型是更大的系统的一部分，包括训练数据筛选、输出监控和更多的组件。在这个工作中，我们介绍了隐私副通道：它们利用这些系统级别的组件来提取私人信息，比起独立的模型来说，提取速率更高。我们提出了四种类别的副通道，涵盖了整个ML生命周期（训练数据筛选、输入预处理、输出后处理和查询筛选），可以提供加强的会员推断攻击或者是新的威胁，如提取用户的测试查询。例如，我们显示了在应用幂等训练前的数据归一化会完全跳过任何可证明隐私保证的保障。此外，我们还显示了防止语言模型重新生成训练数据的系统可以被滥用，以访问私钥，即使模型没有记忆这些私钥。总之，我们的结果表明了机器学习隐私分析应该是综合、端到端的。
</details></li>
</ul>
<hr>
<h2 id="Quantitative-Analysis-of-Forecasting-Models-In-the-Aspect-of-Online-Political-Bias"><a href="#Quantitative-Analysis-of-Forecasting-Models-In-the-Aspect-of-Online-Political-Bias" class="headerlink" title="Quantitative Analysis of Forecasting Models:In the Aspect of Online Political Bias"></a>Quantitative Analysis of Forecasting Models:In the Aspect of Online Political Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05589">http://arxiv.org/abs/2309.05589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srinath Sai Tripuraneni, Sadia Kamal, Arunkumar Bagavathi</li>
<li>for: 本研究旨在Characterizing political bias in online social media platforms, particularly in forecasting political leaning time series.</li>
<li>methods: 我们提出了一种决策树方法， Utilizing existing time series forecasting models on two social media datasets with different political ideologies, specifically Twitter and Gab.</li>
<li>results: Through our experiments and analyses, we aim to shed light on the challenges and opportunities in forecasting political bias in social media platforms, and ultimately pave the way for developing more effective strategies to mitigate the negative impact of political bias in the digital realm.<details>
<summary>Abstract</summary>
Understanding and mitigating political bias in online social media platforms are crucial tasks to combat misinformation and echo chamber effects. However, characterizing political bias temporally using computational methods presents challenges due to the high frequency of noise in social media datasets. While existing research has explored various approaches to political bias characterization, the ability to forecast political bias and anticipate how political conversations might evolve in the near future has not been extensively studied. In this paper, we propose a heuristic approach to classify social media posts into five distinct political leaning categories. Since there is a lack of prior work on forecasting political bias, we conduct an in-depth analysis of existing baseline models to identify which model best fits to forecast political leaning time series. Our approach involves utilizing existing time series forecasting models on two social media datasets with different political ideologies, specifically Twitter and Gab. Through our experiments and analyses, we seek to shed light on the challenges and opportunities in forecasting political bias in social media platforms. Ultimately, our work aims to pave the way for developing more effective strategies to mitigate the negative impact of political bias in the digital realm.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a heuristic approach to classify social media posts into five distinct political leaning categories. Since there is a lack of prior work on forecasting political bias, we conduct an in-depth analysis of existing baseline models to identify which model best fits to forecast political leaning time series. Our approach involves utilizing existing time series forecasting models on two social media datasets with different political ideologies, specifically Twitter and Gab.Through our experiments and analyses, we seek to shed light on the challenges and opportunities in forecasting political bias in social media platforms. Our work aims to pave the way for developing more effective strategies to mitigate the negative impact of political bias in the digital realm.
</details></li>
</ul>
<hr>
<h2 id="Anisotropic-Diffusion-Stencils-From-Simple-Derivations-over-Stability-Estimates-to-ResNet-Implementations"><a href="#Anisotropic-Diffusion-Stencils-From-Simple-Derivations-over-Stability-Estimates-to-ResNet-Implementations" class="headerlink" title="Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations"></a>Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05575">http://arxiv.org/abs/2309.05575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karl Schrader, Joachim Weickert, Michael Krause</li>
<li>for: This paper is written for studying the numerical approximation of anisotropic diffusion processes with a diffusion tensor, and deriving a large family of finite difference discretizations on a 3x3 stencil.</li>
<li>methods: The paper uses a directional splitting method to derive a stencil class that covers a wide range of existing discretizations, and establishes a bound on the spectral norm of the matrix corresponding to the stencil to guarantee stability of an explicit scheme in the Euclidean norm.</li>
<li>results: The paper shows that the resulting stencil class involves one free parameter and covers a wide range of existing discretizations, and that the two parameters in the stencil of Weickert et al. (2013) contain redundancy. Additionally, the paper demonstrates a natural translation of the explicit scheme into ResNet blocks, which enables simple and highly efficient parallel implementations on GPUs.<details>
<summary>Abstract</summary>
Anisotropic diffusion processes with a diffusion tensor are important in image analysis, physics, and engineering. However, their numerical approximation has a strong impact on dissipative artefacts and deviations from rotation invariance. In this work, we study a large family of finite difference discretisations on a 3 x 3 stencil. We derive it by splitting 2-D anisotropic diffusion into four 1-D diffusions. The resulting stencil class involves one free parameter and covers a wide range of existing discretisations. It comprises the full stencil family of Weickert et al. (2013) and shows that their two parameters contain redundancy. Furthermore, we establish a bound on the spectral norm of the matrix corresponding to the stencil. This gives time step size limits that guarantee stability of an explicit scheme in the Euclidean norm. Our directional splitting also allows a very natural translation of the explicit scheme into ResNet blocks. Employing neural network libraries enables simple and highly efficient parallel implementations on GPUs.
</details>
<details>
<summary>摘要</summary>
“散射过程”在图像分析、物理和工程中具有重要地位。然而，其数字逼近带来强烈的消耗残差和不够旋转 invariants 的偏差。在这项工作中，我们研究了一个大家族的finite difference离散方法，其基于2-D不同方向散射的4-1-D散射。这个stencil家族包括一个自由参数，覆盖了广泛存在的离散方法。它包括Weickert等人（2013）中的全stencil家族，并证明了其两个参数之间的重复性。此外，我们还提出了一个spectral norm的下界，这个下界 garanties stability of an explicit scheme in Euclidean norm。我们的方向分解也使得可以非常自然地将批处理翻译成ResNet块。通过使用神经网络库，可以在GPU上实现高效并简单的并行实现。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Federated-Learning-in-6G-A-Trusted-Architecture-with-Graph-based-Analysis"><a href="#Advancing-Federated-Learning-in-6G-A-Trusted-Architecture-with-Graph-based-Analysis" class="headerlink" title="Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis"></a>Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05525">http://arxiv.org/abs/2309.05525</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chendiqian/GNN4FL">https://github.com/chendiqian/GNN4FL</a></li>
<li>paper_authors: Wenxuan Ye, Chendi Qian, Xueli An, Xueqiang Yan, Georg Carle</li>
<li>for: 提高6G网络中的人工智能支持，使其更加安全和可靠。</li>
<li>methods: 使用分布式记录技术和图 neural network，包括预处理层使用同质加密、图StructuredNN用于异常模型检测、和分布式系统选择机制。</li>
<li>results: 在模拟中 Validates the feasibility of the proposed architecture, demonstrating improved performance in anomalous model detection and global model accuracy compared to relevant baselines.<details>
<summary>Abstract</summary>
Integrating native AI support into the network architecture is an essential objective of 6G. Federated Learning (FL) emerges as a potential paradigm, facilitating decentralized AI model training across a diverse range of devices under the coordination of a central server. However, several challenges hinder its wide application in the 6G context, such as malicious attacks and privacy snooping on local model updates, and centralization pitfalls. This work proposes a trusted architecture for supporting FL, which utilizes Distributed Ledger Technology (DLT) and Graph Neural Network (GNN), including three key features. First, a pre-processing layer employing homomorphic encryption is incorporated to securely aggregate local models, preserving the privacy of individual models. Second, given the distributed nature and graph structure between clients and nodes in the pre-processing layer, GNN is leveraged to identify abnormal local models, enhancing system security. Third, DLT is utilized to decentralize the system by selecting one of the candidates to perform the central server's functions. Additionally, DLT ensures reliable data management by recording data exchanges in an immutable and transparent ledger. The feasibility of the novel architecture is validated through simulations, demonstrating improved performance in anomalous model detection and global model accuracy compared to relevant baselines.
</details>
<details>
<summary>摘要</summary>
六代网络中 интеGRATING本地AI支持是一个重要目标。联邦学习（FL）emerges as a potential paradigm，允许分散的AI模型训练 Across a diverse range of devices under the coordination of a central server。然而，several challenges hinder its wide application in the 6G context, such as malicious attacks and privacy snooping on local model updates, and centralization pitfalls。This work proposes a trusted architecture for supporting FL, which utilizes Distributed Ledger Technology (DLT) and Graph Neural Network (GNN), including three key features。First, a pre-processing layer employing homomorphic encryption is incorporated to securely aggregate local models, preserving the privacy of individual models。Second, given the distributed nature and graph structure between clients and nodes in the pre-processing layer, GNN is leveraged to identify abnormal local models, enhancing system security。Third, DLT is utilized to decentralize the system by selecting one of the candidates to perform the central server's functions。Additionally, DLT ensures reliable data management by recording data exchanges in an immutable and transparent ledger。The feasibility of the novel architecture is validated through simulations, demonstrating improved performance in anomalous model detection and global model accuracy compared to relevant baselines。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Re-formalization-of-Individual-Fairness"><a href="#Re-formalization-of-Individual-Fairness" class="headerlink" title="Re-formalization of Individual Fairness"></a>Re-formalization of Individual Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05521">http://arxiv.org/abs/2309.05521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Toshihiro Kamishima</li>
<li>for: 本研究旨在重新定义个人公平，通过统计独立条件确定个人。</li>
<li>methods: 本研究使用了Dwork等人的形式化方法，将类似数据在不公平空间映射到公平空间中相似的位置。</li>
<li>results: 本研究提出了一种新的公平定义，可以与等式公平和统计平衡结合使用，并且可以应用于预处理、进程处理和后处理阶段。<details>
<summary>Abstract</summary>
The notion of individual fairness is a formalization of an ethical principle, "Treating like cases alike," which has been argued such as by Aristotle. In a fairness-aware machine learning context, Dwork et al. firstly formalized the notion. In their formalization, a similar pair of data in an unfair space should be mapped to similar positions in a fair space. We propose to re-formalize individual fairness by the statistical independence conditioned by individuals. This re-formalization has the following merits. First, our formalization is compatible with that of Dwork et al. Second, our formalization enables to combine individual fairness with the fairness notion, equalized odds or sufficiency, as well as statistical parity. Third, though their formalization implicitly assumes a pre-process approach for making fair prediction, our formalization is applicable to an in-process or post-process approach.
</details>
<details>
<summary>摘要</summary>
“个人公平”是一种形式化的道德原则， Aristotle 已经提出过，而 Dwork 等人首先将其形式化。在公平意识的机器学习上下，他们的形式化是指将不公平的空间中的相似资料映射到公平的空间中相似的位置。我们提议将个人公平重新形式化为统计独立的条件。这种重新形式化具有以下优点：首先，我们的形式化与 Dwork 等人的形式化相容。其次，我们的形式化可以与 equalized odds 或 sufficiency 的公平观念结合。最后，他们的形式化预设了先processing 的公平预测方法，而我们的形式化则可以应用于进程或后process approach。
</details></li>
</ul>
<hr>
<h2 id="Share-Your-Representation-Only-Guaranteed-Improvement-of-the-Privacy-Utility-Tradeoff-in-Federated-Learning"><a href="#Share-Your-Representation-Only-Guaranteed-Improvement-of-the-Privacy-Utility-Tradeoff-in-Federated-Learning" class="headerlink" title="Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning"></a>Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05505">http://arxiv.org/abs/2309.05505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shenzebang/centaur-privacy-federated-representation-learning">https://github.com/shenzebang/centaur-privacy-federated-representation-learning</a></li>
<li>paper_authors: Zebang Shen, Jiayuan Ye, Anmin Kang, Hamed Hassani, Reza Shokri</li>
<li>for: 这个论文目的是提出一种基于分布式学习的隐私保护 federated representation learning 方法，以保持数据隐私而提高模型性能。</li>
<li>methods: 该方法使用了现代差分隐私算法，并使用了一种新的可变权重策略来保证 differential privacy 的承诺，同时允许本地个性化。</li>
<li>results: 在线性表示设定下，我们的新算法 \DPFEDREP\ 在一个 linear rate 下 converges to a ball centered around the global optimal solution, 并且radius of the ball 与隐私预算的reciprocal成正比。这些结果提高了这个问题的 utility-privacy trade-off 的最佳性能，比过去的最佳性能增加了 $\sqrt{d}$。<details>
<summary>Abstract</summary>
Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy. Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free. Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it). We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \DPFEDREP\ converges to a ball centered around the \emph{global optimal} solution at a linear rate, and the radius of the ball is proportional to the reciprocal of the privacy budget. With this novel utility analysis, we improve the SOTA utility-privacy trade-off for this problem by a factor of $\sqrt{d}$, where $d$ is the input dimension. We empirically evaluate our method with the image classification task on CIFAR10, CIFAR100, and EMNIST, and observe a significant performance improvement over the prior work under the same small privacy budget. The code can be found in this link: https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning.
</details>
<details>
<summary>摘要</summary>
“重复的参数共享在联合学习中会导致敏感数据信息泄露，这会背离联合学习的主要目的——数据隐私。为了解决这种信息泄露风险，使用当前最佳的权限隐私算法也不是免费的。随机机制可以防止模型学习到本地模型之间的分布不同的情况下的有用表示函数，尤其是当数据不同时。在这篇论文中，我们考虑了一种联合学习目标，它鼓励不同党派共同修改共识部分的模型，同时保证隐私保证。我们证明在线性表示设定下，虽然目标函数不对称，但我们提出的新算法\DPFEDREP\在线性速率下 converge到一个球心在全球优致解的解，球半径与隐私预算reciprocal成正比。通过这种新的用户分析，我们提高了这个问题的状态艺术-隐私质量比，提高了状态艺术质量比$\sqrt{d}$，其中$d$是输入维度。我们通过对图像分类任务进行实验，观察到在相同的小隐私预算下，我们的方法与先前的成果相比有显著的性能提升。代码可以在以下链接中找到：https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning。”
</details></li>
</ul>
<hr>
<h2 id="Systematic-Review-of-Experimental-Paradigms-and-Deep-Neural-Networks-for-Electroencephalography-Based-Cognitive-Workload-Detection"><a href="#Systematic-Review-of-Experimental-Paradigms-and-Deep-Neural-Networks-for-Electroencephalography-Based-Cognitive-Workload-Detection" class="headerlink" title="Systematic Review of Experimental Paradigms and Deep Neural Networks for Electroencephalography-Based Cognitive Workload Detection"></a>Systematic Review of Experimental Paradigms and Deep Neural Networks for Electroencephalography-Based Cognitive Workload Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07163">http://arxiv.org/abs/2309.07163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishnu KN, Cota Navin Gupta</li>
<li>for: 这种系统性文献综述旨在探讨基于电энце法成功的认知工作负荷（CWL）估计方法。</li>
<li>methods: 这些研究使用了多种实验方法来刺激人类的认知工作负荷，并使用了深度神经网络（DNNs）进行信号分类。</li>
<li>results: 研究发现，只有一些研究使用了在线或 pseudo-在线的分类策略来实时估计认知工作负荷，而大多数研究使用了黑盒模型。 综述还表明，DNNs 是可以有效地分类 EEG 信号的工具，但是现有方法受到非站态信号的限制。<details>
<summary>Abstract</summary>
This article summarizes a systematic review of the electroencephalography (EEG)-based cognitive workload (CWL) estimation. The focus of the article is twofold: identify the disparate experimental paradigms used for reliably eliciting discreet and quantifiable levels of cognitive load and the specific nature and representational structure of the commonly used input formulations in deep neural networks (DNNs) used for signal classification. The analysis revealed a number of studies using EEG signals in its native representation of a two-dimensional matrix for offline classification of CWL. However, only a few studies adopted an online or pseudo-online classification strategy for real-time CWL estimation. Further, only a couple of interpretable DNNs and a single generative model were employed for cognitive load detection till date during this review. More often than not, researchers were using DNNs as black-box type models. In conclusion, DNNs prove to be valuable tools for classifying EEG signals, primarily due to the substantial modeling power provided by the depth of their network architecture. It is further suggested that interpretable and explainable DNN models must be employed for cognitive workload estimation since existing methods are limited in the face of the non-stationary nature of the signal.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Identify the disparate experimental paradigms used for reliably eliciting discreet and quantifiable levels of cognitive load.2. Examine the specific nature and representational structure of the commonly used input formulations in deep neural networks (DNNs) used for signal classification.The analysis revealed several findings:1. Many studies used EEG signals in their native representation, a two-dimensional matrix, for offline classification of CWL.2. Only a few studies adopted an online or pseudo-online classification strategy for real-time CWL estimation.3. Only a couple of interpretable DNNs and a single generative model were employed for cognitive load detection until now.4. Most researchers used DNNs as black-box type models.In conclusion, DNNs are valuable tools for classifying EEG signals, primarily due to the substantial modeling power provided by the depth of their network architecture. However, the review suggests that interpretable and explainable DNN models should be employed for cognitive workload estimation, as existing methods are limited in the face of the non-stationary nature of the signal.</details></li>
</ol>
<hr>
<h2 id="Learning-Objective-Specific-Active-Learning-Strategies-with-Attentive-Neural-Processes"><a href="#Learning-Objective-Specific-Active-Learning-Strategies-with-Attentive-Neural-Processes" class="headerlink" title="Learning Objective-Specific Active Learning Strategies with Attentive Neural Processes"></a>Learning Objective-Specific Active Learning Strategies with Attentive Neural Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05477">http://arxiv.org/abs/2309.05477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timsey/npal">https://github.com/timsey/npal</a></li>
<li>paper_authors: Tim Bakker, Herke van Hoof, Max Welling</li>
<li>for: 提高机器学习模型的数据效率</li>
<li>methods: 使用 Attentive Conditional Neural Process 模型，利用活动学习问题的对称和独立性</li>
<li>results: 比较多种基elines表现出色，并且对变化数据集 exhibits improved stabilityHere’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本文提出了一种基于 Pool-based Active Learning 技术的 Classification 方法，以提高机器学习模型的数据效率。</li>
<li>methods: 我们使用 Attentive Conditional Neural Process 模型，利用活动学习问题的对称和独立性来学习活动学习策略。</li>
<li>results: 我们的方法在不同的数据集和训练设置下表现出色，比较多种基elines表现出色，并且对变化数据集 exhibits improved stability。However, please note that the translation is not perfect and may not capture all the nuances of the original English text.<details>
<summary>Abstract</summary>
Pool-based active learning (AL) is a promising technology for increasing data-efficiency of machine learning models. However, surveys show that performance of recent AL methods is very sensitive to the choice of dataset and training setting, making them unsuitable for general application. In order to tackle this problem, the field Learning Active Learning (LAL) suggests to learn the active learning strategy itself, allowing it to adapt to the given setting. In this work, we propose a novel LAL method for classification that exploits symmetry and independence properties of the active learning problem with an Attentive Conditional Neural Process model. Our approach is based on learning from a myopic oracle, which gives our model the ability to adapt to non-standard objectives, such as those that do not equally weight the error on all data points. We experimentally verify that our Neural Process model outperforms a variety of baselines in these settings. Finally, our experiments show that our model exhibits a tendency towards improved stability to changing datasets. However, performance is sensitive to choice of classifier and more work is necessary to reduce the performance the gap with the myopic oracle and to improve scalability. We present our work as a proof-of-concept for LAL on nonstandard objectives and hope our analysis and modelling considerations inspire future LAL work.
</details>
<details>
<summary>摘要</summary>
池度基于的活动学习（AL）是一种可靠的技术，可以提高机器学习模型的数据效率。然而，评估表明，现有的AL方法在不同的数据集和训练环境下表现非常敏感，使其不适用于通用应用。为了解决这个问题，场景学习活动学习（LAL）建议学习活动学习策略自身，以适应给定的环境。在这个工作中，我们提出了一种基于归一化神经过程模型的LAL方法 для分类。我们的方法基于学习偏向oracle，让我们的模型能够适应非标准目标函数，如不均衡所有数据点的错误。我们实验表明，我们的神经过程模型在这些设置下超过了多种基准。然而，我们的模型表现受到选择类фика器的影响，需要更多的工作来减少与偏向oracle的性能差距，并提高可扩展性。我们的工作作为LAL非标准目标的证明，希望我们的分析和模型考虑能够激励未来LAL工作。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-the-dimension-of-a-Fano-variety"><a href="#Machine-learning-the-dimension-of-a-Fano-variety" class="headerlink" title="Machine learning the dimension of a Fano variety"></a>Machine learning the dimension of a Fano variety</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05473">http://arxiv.org/abs/2309.05473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/fanosearch/mldim">https://bitbucket.org/fanosearch/mldim</a></li>
<li>paper_authors: Tom Coates, Alexander M. Kasprzyk, Sara Veneziale</li>
<li>for: 本研究探讨了 whether the quantum period of a Fano variety determines its dimension.</li>
<li>methods: 使用 machine learning 技术，特别是 feed-forward neural network，来解决这个问题。</li>
<li>results: 研究发现，一个简单的 feed-forward neural network 可以准确地确定 Fano variety 的维度，准确率达到 98%。此外，研究还提出了对 Fano variety 的几何期的准确误差分析，并证明了这些误差可以用来确定 Fano variety 的维度。<details>
<summary>Abstract</summary>
Fano varieties are basic building blocks in geometry - they are `atomic pieces' of mathematical shapes. Recent progress in the classification of Fano varieties involves analysing an invariant called the quantum period. This is a sequence of integers which gives a numerical fingerprint for a Fano variety. It is conjectured that a Fano variety is uniquely determined by its quantum period. If this is true, one should be able to recover geometric properties of a Fano variety directly from its quantum period. We apply machine learning to the question: does the quantum period of X know the dimension of X? Note that there is as yet no theoretical understanding of this. We show that a simple feed-forward neural network can determine the dimension of X with 98% accuracy. Building on this, we establish rigorous asymptotics for the quantum periods of a class of Fano varieties. These asymptotics determine the dimension of X from its quantum period. Our results demonstrate that machine learning can pick out structure from complex mathematical data in situations where we lack theoretical understanding. They also give positive evidence for the conjecture that the quantum period of a Fano variety determines that variety.
</details>
<details>
<summary>摘要</summary>
To explore this idea, we applied machine learning techniques to the question of whether the quantum period of a Fano variety can determine its dimension. While there is currently no theoretical understanding of this, we found that a simple feed-forward neural network can accurately determine the dimension of a Fano variety with 98% accuracy.Building on this result, we established rigorous asymptotics for the quantum periods of a class of Fano varieties, which allow us to determine the dimension of a Fano variety from its quantum period. Our findings demonstrate that machine learning can be used to extract structure from complex mathematical data, even in situations where there is no theoretical understanding. Additionally, our results provide positive evidence for the conjecture that the quantum period of a Fano variety determines that variety.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Sentinels-Assessing-AI-Performance-in-Cybersecurity-Peer-Review"><a href="#Unveiling-the-Sentinels-Assessing-AI-Performance-in-Cybersecurity-Peer-Review" class="headerlink" title="Unveiling the Sentinels: Assessing AI Performance in Cybersecurity Peer Review"></a>Unveiling the Sentinels: Assessing AI Performance in Cybersecurity Peer Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05457">http://arxiv.org/abs/2309.05457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Niu, Nian Xue, Christina Pöpper</li>
<li>For: This paper aims to evaluate the performance of AI in reviewing academic security conferences, specifically by comparing the results obtained from human reviewers and machine-learning models.* Methods: The paper uses a comprehensive dataset of thousands of papers from computer science conferences and the arXiv preprint website, and evaluates the prediction capabilities of ChatGPT and a two-stage classification approach based on the Doc2Vec model with various classifiers.* Results: The experimental evaluation of review outcome prediction using the Doc2Vec-based approach achieves an accuracy of over 90%, significantly outperforming ChatGPT. The paper also identifies the potential advantages and limitations of the tested ML models and explores areas within the paper-reviewing process that can benefit from automated support approaches.<details>
<summary>Abstract</summary>
Peer review is the method employed by the scientific community for evaluating research advancements. In the field of cybersecurity, the practice of double-blind peer review is the de-facto standard. This paper touches on the holy grail of peer reviewing and aims to shed light on the performance of AI in reviewing for academic security conferences. Specifically, we investigate the predictability of reviewing outcomes by comparing the results obtained from human reviewers and machine-learning models. To facilitate our study, we construct a comprehensive dataset by collecting thousands of papers from renowned computer science conferences and the arXiv preprint website. Based on the collected data, we evaluate the prediction capabilities of ChatGPT and a two-stage classification approach based on the Doc2Vec model with various classifiers. Our experimental evaluation of review outcome prediction using the Doc2Vec-based approach performs significantly better than the ChatGPT and achieves an accuracy of over 90%. While analyzing the experimental results, we identify the potential advantages and limitations of the tested ML models. We explore areas within the paper-reviewing process that can benefit from automated support approaches, while also recognizing the irreplaceable role of human intellect in certain aspects that cannot be matched by state-of-the-art AI techniques.
</details>
<details>
<summary>摘要</summary>
Peer review 是科学共识社区用来评估研究进步的方法。在网络安全领域，双盲审核是标准做法。这篇论文探讨审核评审的圣杯之物，旨在探讨人工智能在学术安全会议审核中的表现。我们专门 investigate 审核结果的预测性，比较人工审核员和机器学习模型 obtiain 的结果。为了进行这项研究，我们构建了包括了数千篇计算机科学会议和arXiv预印站点上的论文的全面数据集。基于收集到的数据，我们评估 Doc2Vec 模型和两个阶段分类器的预测能力。我们的实验测试结果显示，使用 Doc2Vec 模型和两个阶段分类器可以达到高于 90% 的准确率。在分析实验结果时，我们发现了机器学习模型的优点和局限性，并探讨了可以通过自动支持方法帮助审核过程的部分，同时也认可人工智能在某些方面无法被现代 AI 技术所代替的不可或缺的作用。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Based-Co-Speech-Gesture-Generation-Using-Joint-Text-and-Audio-Representation"><a href="#Diffusion-Based-Co-Speech-Gesture-Generation-Using-Joint-Text-and-Audio-Representation" class="headerlink" title="Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation"></a>Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05455">http://arxiv.org/abs/2309.05455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna Deichler, Shivam Mehta, Simon Alexanderson, Jonas Beskow</li>
<li>for: 本研究是为了开发一个可以生成人类样式的合体动作系统，用于GENEA（生成和评估非语言行为 для具有身体的代理）挑战2023。</li>
<li>methods: 本研究基于现有的扩散基于动作合成模型，并提出了一个对比性语音和姿势预训练（CSMP）模块，用于学习语音和姿势的semantic coupling。CSMP模块的输出用作diffusion-based gesture synthesis模型的conditioning signal，以实现semantically-aware co-speech gesture generation。</li>
<li>results: 根据提交的入场点，我们的系统在人类化度和语音合适度方面得到了最高分，表明我们的系统是一种可靠的实现人类样式的合口动作生成的方法。<details>
<summary>Abstract</summary>
This paper describes a system developed for the GENEA (Generation and Evaluation of Non-verbal Behaviour for Embodied Agents) Challenge 2023. Our solution builds on an existing diffusion-based motion synthesis model. We propose a contrastive speech and motion pretraining (CSMP) module, which learns a joint embedding for speech and gesture with the aim to learn a semantic coupling between these modalities. The output of the CSMP module is used as a conditioning signal in the diffusion-based gesture synthesis model in order to achieve semantically-aware co-speech gesture generation. Our entry achieved highest human-likeness and highest speech appropriateness rating among the submitted entries. This indicates that our system is a promising approach to achieve human-like co-speech gestures in agents that carry semantic meaning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Quantized-Fourier-and-Polynomial-Features-for-more-Expressive-Tensor-Network-Models"><a href="#Quantized-Fourier-and-Polynomial-Features-for-more-Expressive-Tensor-Network-Models" class="headerlink" title="Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models"></a>Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05436">http://arxiv.org/abs/2309.05436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuripsANON2023/QFF">https://github.com/neuripsANON2023/QFF</a></li>
<li>paper_authors: Frederiek Wesel, Kim Batselier</li>
<li>for: 提高高维数据集中模型的泛化能力和精度</li>
<li>methods: 使用几何和傅ри特特征进行非线性扩展，并将模型参数约化为几何网络</li>
<li>results: 在大规模 regression 任务中实现了状态最佳的结果，并且通过实验证明了这种方法可以增强模型的泛化能力和精度<details>
<summary>Abstract</summary>
In the context of kernel machines, polynomial and Fourier features are commonly used to provide a nonlinear extension to linear models by mapping the data to a higher-dimensional space. Unless one considers the dual formulation of the learning problem, which renders exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits to tackle high-dimensional problems. One of the possible approaches to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting quantized models have a higher bound on the VC-dimension as opposed to their non-quantized counterparts, at no additional computational cost while learning from identical features. We verify experimentally how this additional tensorization regularizes the learning problem by prioritizing the most salient features in the data and how it provides models with increased generalization capabilities. We finally benchmark our approach on large regression task, achieving state-of-the-art results on a laptop computer.
</details>
<details>
<summary>摘要</summary>
在内核机器学中，多项式和傅里叶特征通常用于提供非线性扩展，将数据映射到更高维的空间。如果不考虑学习问题的 dual 形式，那么因为特征的维度乘积结构而导致的模型参数的几率增长会使大规模学习变得不可行。为了缓解这种几率增长，可以利用特征中的维度结构，将模型参数约束为减参数化的tensor网络。在这篇论文中，我们对多项式和傅里叶特征进行量化，并将相关的模型参数量化。我们发现，对于同样的参数数量，量化模型具有更高的VC-维度上限，而无需额外的计算成本，而且在学习同样的特征时，可以减少特征的繁殖。我们通过实验表明，这种额外的维度regularizes学习问题，使模型具有更好的泛化能力。最后，我们对大规模回归任务进行了 benchmark，在笔记计算机上实现了状态级Result。
</details></li>
</ul>
<hr>
<h2 id="A-parameterised-model-for-link-prediction-using-node-centrality-and-similarity-measure-based-on-graph-embedding"><a href="#A-parameterised-model-for-link-prediction-using-node-centrality-and-similarity-measure-based-on-graph-embedding" class="headerlink" title="A parameterised model for link prediction using node centrality and similarity measure based on graph embedding"></a>A parameterised model for link prediction using node centrality and similarity measure based on graph embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05434">http://arxiv.org/abs/2309.05434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haohui Lu, Shahadat Uddin</li>
<li>For: The paper is written for researchers and practitioners working on link prediction in graph machine learning, with applications in disease prediction, social network recommendations, and drug discovery.* Methods: The paper proposes a novel method called Node Centrality and Similarity Based Parameterised Model (NCSM), which integrates node centrality and similarity measures as edge features in a customized Graph Neural Network (GNN) layer.* Results: The proposed model outperforms existing state-of-the-art models like Graph Convolutional Networks and Variational Graph Autoencoder across various metrics and datasets, demonstrating its superiority in link prediction tasks.<details>
<summary>Abstract</summary>
Link prediction is a key aspect of graph machine learning, with applications as diverse as disease prediction, social network recommendations, and drug discovery. It involves predicting new links that may form between network nodes. Despite the clear importance of link prediction, existing models have significant shortcomings. Graph Convolutional Networks, for instance, have been proven to be highly efficient for link prediction on a variety of datasets. However, they encounter severe limitations when applied to short-path networks and ego networks, resulting in poor performance. This presents a critical problem space that this work aims to address. In this paper, we present the Node Centrality and Similarity Based Parameterised Model (NCSM), a novel method for link prediction tasks. NCSM uniquely integrates node centrality and similarity measures as edge features in a customised Graph Neural Network (GNN) layer, effectively leveraging the topological information of large networks. This model represents the first parameterised GNN-based link prediction model that considers topological information. The proposed model was evaluated on five benchmark graph datasets, each comprising thousands of nodes and edges. Experimental results highlight NCSM's superiority over existing state-of-the-art models like Graph Convolutional Networks and Variational Graph Autoencoder, as it outperforms them across various metrics and datasets. This exceptional performance can be attributed to NCSM's innovative integration of node centrality, similarity measures, and its efficient use of topological information.
</details>
<details>
<summary>摘要</summary>
<<SYS>>链接预测是图机器学习中关键的一环，它在多个应用领域中发挥着重要的作用，如疾病预测、社交媒体推荐和药物发现。链接预测的目标是预测图中可能会形成的新链接。 despite the clear importance of link prediction, existing models have significant shortcomings. Graph Convolutional Networks, for instance, have been proven to be highly efficient for link prediction on a variety of datasets. However, they encounter severe limitations when applied to short-path networks and ego networks, resulting in poor performance. This presents a critical problem space that this work aims to address.In this paper, we present the Node Centrality and Similarity Based Parameterised Model (NCSM), a novel method for link prediction tasks. NCSM uniquely integrates node centrality and similarity measures as edge features in a customised Graph Neural Network (GNN) layer, effectively leveraging the topological information of large networks. This model represents the first parameterised GNN-based link prediction model that considers topological information.The proposed model was evaluated on five benchmark graph datasets, each comprising thousands of nodes and edges. Experimental results highlight NCSM's superiority over existing state-of-the-art models like Graph Convolutional Networks and Variational Graph Autoencoder, as it outperforms them across various metrics and datasets. This exceptional performance can be attributed to NCSM's innovative integration of node centrality, similarity measures, and its efficient use of topological information.
</details></li>
</ul>
<hr>
<h2 id="Neuromorphic-Auditory-Perception-by-Neural-Spiketrum"><a href="#Neuromorphic-Auditory-Perception-by-Neural-Spiketrum" class="headerlink" title="Neuromorphic Auditory Perception by Neural Spiketrum"></a>Neuromorphic Auditory Perception by Neural Spiketrum</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05430">http://arxiv.org/abs/2309.05430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huajin Tang, Pengjie Gu, Jayawan Wijekoon, MHD Anas Alsakkal, Ziming Wang, Jiangrong Shen, Rui Yan</li>
<li>for: This paper aims to develop a neural spike coding model called “spiketrum” to efficiently process auditory signals and enable brain-like intelligence in neuromorphic computing.</li>
<li>methods: The paper proposes a spiketrum model that transforms time-varying analog signals into spatiotemporal spike patterns, minimizing information loss and providing informational robustness to neural fluctuations and spike losses.</li>
<li>results: The paper demonstrates the effectiveness of the spiketrum model through a neuromorphic cochlear prototype, showing that it can provide a systematic solution for spike-based artificial intelligence by fully exploiting the advantages of spike-based computation.<details>
<summary>Abstract</summary>
Neuromorphic computing holds the promise to achieve the energy efficiency and robust learning performance of biological neural systems. To realize the promised brain-like intelligence, it needs to solve the challenges of the neuromorphic hardware architecture design of biological neural substrate and the hardware amicable algorithms with spike-based encoding and learning. Here we introduce a neural spike coding model termed spiketrum, to characterize and transform the time-varying analog signals, typically auditory signals, into computationally efficient spatiotemporal spike patterns. It minimizes the information loss occurring at the analog-to-spike transformation and possesses informational robustness to neural fluctuations and spike losses. The model provides a sparse and efficient coding scheme with precisely controllable spike rate that facilitates training of spiking neural networks in various auditory perception tasks. We further investigate the algorithm-hardware co-designs through a neuromorphic cochlear prototype which demonstrates that our approach can provide a systematic solution for spike-based artificial intelligence by fully exploiting its advantages with spike-based computation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Temporal-Patience-Efficient-Adaptive-Deep-Learning-for-Embedded-Radar-Data-Processing"><a href="#Temporal-Patience-Efficient-Adaptive-Deep-Learning-for-Embedded-Radar-Data-Processing" class="headerlink" title="Temporal Patience: Efficient Adaptive Deep Learning for Embedded Radar Data Processing"></a>Temporal Patience: Efficient Adaptive Deep Learning for Embedded Radar Data Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05686">http://arxiv.org/abs/2309.05686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Sponner, Julius Ott, Lorenzo Servadei, Bernd Waschneck, Robert Wille, Akash Kumar</li>
<li>for: 这篇论文旨在提高深度学习推理的能效性，使其在具有限制的附加设备上进行实时处理。</li>
<li>methods: 该论文提出了一种使用流动 radar 数据的时间相关性来增强深度学习推理的效率。这些方法包括在架构中添加额外的分类支路，以实现在推理过程中提前终止。</li>
<li>results: 该论文的实验结果表明，使用该方法可以在推理过程中减少计算成本，同时保持准确性的最小损失。相比单 Exit 网络，该方法可以节省至 26% 的操作数量。此外，该方法可以与传统优化结合使用，使其在有限的附加设备上可用。<details>
<summary>Abstract</summary>
Radar sensors offer power-efficient solutions for always-on smart devices, but processing the data streams on resource-constrained embedded platforms remains challenging. This paper presents novel techniques that leverage the temporal correlation present in streaming radar data to enhance the efficiency of Early Exit Neural Networks for Deep Learning inference on embedded devices. These networks add additional classifier branches between the architecture's hidden layers that allow for an early termination of the inference if their result is deemed sufficient enough by an at-runtime decision mechanism. Our methods enable more informed decisions on when to terminate the inference, reducing computational costs while maintaining a minimal loss of accuracy.   Our results demonstrate that our techniques save up to 26% of operations per inference over a Single Exit Network and 12% over a confidence-based Early Exit version. Our proposed techniques work on commodity hardware and can be combined with traditional optimizations, making them accessible for resource-constrained embedded platforms commonly used in smart devices. Such efficiency gains enable real-time radar data processing on resource-constrained platforms, allowing for new applications in the context of smart homes, Internet-of-Things, and human-computer interaction.
</details>
<details>
<summary>摘要</summary>
雷达感知器提供了功率高的解决方案，但是处理流处理数据流在具有限制的嵌入式平台上仍然是一个挑战。这篇论文提出了新的技术，利用雷达数据流中的时间相关性来增强深度学习的早期终止网络（Early Exit Neural Networks）的效率在嵌入式设备上。这些网络添加了在架构中隐藏层之间的额外分支，以实现在运行时决策机制的基础上，提前终止推断。我们的方法可以更好地决定终止推断的时间，从而降低计算成本，保持最小的准确性损失。我们的结果表明，我们的技术可以在嵌入式设备上实现26%的操作数减少，相比单exit网络。此外，我们的技术还可以与传统优化相结合，使其在资源有限的嵌入式设备上可用。这些效率提升使得雷达数据流处理在资源有限的平台上实现了实时处理，开 up了新的应用场景，如智能家居、物联网和人机交互。
</details></li>
</ul>
<hr>
<h2 id="Learning-noise-induced-transitions-by-multi-scaling-reservoir-computing"><a href="#Learning-noise-induced-transitions-by-multi-scaling-reservoir-computing" class="headerlink" title="Learning noise-induced transitions by multi-scaling reservoir computing"></a>Learning noise-induced transitions by multi-scaling reservoir computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05413">http://arxiv.org/abs/2309.05413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zequn Lin, Zhaofan Lu, Zengru Di, Ying Tang</li>
<li>for: 本研究旨在使用机器学习模型，具体是循环神经网络，捕捉时间序列中的随机过渡。</li>
<li>methods: 本研究使用了循环神经网络，并对其中的一个关键参数进行了优化。</li>
<li>results: 研究发现，使用这种方法可以准确地计算过渡时间和过渡次数的统计数据。此外，该方法还可以capture多稳态系统中的过渡，包括蛋白质折叠过渡。<details>
<summary>Abstract</summary>
Noise is usually regarded as adversarial to extract the effective dynamics from time series, such that the conventional data-driven approaches usually aim at learning the dynamics by mitigating the noisy effect. However, noise can have a functional role of driving transitions between stable states underlying many natural and engineered stochastic dynamics. To capture such stochastic transitions from data, we find that leveraging a machine learning model, reservoir computing as a type of recurrent neural network, can learn noise-induced transitions. We develop a concise training protocol for tuning hyperparameters, with a focus on a pivotal hyperparameter controlling the time scale of the reservoir dynamics. The trained model generates accurate statistics of transition time and the number of transitions. The approach is applicable to a wide class of systems, including a bistable system under a double-well potential, with either white noise or colored noise. It is also aware of the asymmetry of the double-well potential, the rotational dynamics caused by non-detailed balance, and transitions in multi-stable systems. For the experimental data of protein folding, it learns the transition time between folded states, providing a possibility of predicting transition statistics from a small dataset. The results demonstrate the capability of machine-learning methods in capturing noise-induced phenomena.
</details>
<details>
<summary>摘要</summary>
噪声通常被视为时间序列数据驱动方法中的障碍物，以便学习时间序列中的动力学。然而，噪声可以扮演一种驱动稳定状态之间的转移的功能性角色。为了从数据中捕捉这些随机转移，我们发现可以通过利用机器学习模型，即复合 нейрон网络中的液体计算，来学习噪声引起的转移。我们开发了一种简洁的训练协议，以控制模型中的时间尺度，并将注重这个关键参数。训练后，模型可以准确地预测转移时间和转移次数。这种方法可以应用于广泛的系统中，包括下降 double-well 潜伏 potential 中的 биста布系统，以及白噪声或某些颜色噪声。此外，它还能够捕捉非详细平衡引起的旋转动力学，以及多稳态系统中的转移。对蛋白质折叠的实验数据，它可以学习转移时间 между折叠态，从而提供预测转移统计的可能性。结果表明机器学习方法可以捕捉噪声引起的现象。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-reinforcement-learning-via-probabilistic-co-adjustment-functions"><a href="#Physics-informed-reinforcement-learning-via-probabilistic-co-adjustment-functions" class="headerlink" title="Physics-informed reinforcement learning via probabilistic co-adjustment functions"></a>Physics-informed reinforcement learning via probabilistic co-adjustment functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05404">http://arxiv.org/abs/2309.05404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nat Wannawas, A. Aldo Faisal<br>for:The paper is written for training reinforcement learning systems in real-world tasks, which are typically data-inefficient and rely on simulation-based modeling.methods:The paper introduces two novel approaches called co-kriging adjustments (CKA) and ridge regression adjustment (RRA) that combine the advantages of using individual system dynamics and simulation models. These methods use an auto-regressive AR1 co-kriging model integrated with Gaussian process priors to improve uncertainty quantification of the entire system’s dynamics.results:The paper demonstrates the efficiency of co-kriging adjustment with an interpretable reinforcement learning control example, learning to control a biomechanical human arm using only a two-link arm simulation model and CKA derived from a small amount of interaction data. The results show that the method provides more accurate uncertainty quantification of the entire system’s dynamics than pure GP-based and AR1 methods.<details>
<summary>Abstract</summary>
Reinforcement learning of real-world tasks is very data inefficient, and extensive simulation-based modelling has become the dominant approach for training systems. However, in human-robot interaction and many other real-world settings, there is no appropriate one-model-for-all due to differences in individual instances of the system (e.g. different people) or necessary oversimplifications in the simulation models. This requires two approaches: 1. either learning the individual system's dynamics approximately from data which requires data-intensive training or 2. using a complete digital twin of the instances, which may not be realisable in many cases. We introduce two approaches: co-kriging adjustments (CKA) and ridge regression adjustment (RRA) as novel ways to combine the advantages of both approaches. Our adjustment methods are based on an auto-regressive AR1 co-kriging model that we integrate with GP priors. This yield a data- and simulation-efficient way of using simplistic simulation models (e.g., simple two-link model) and rapidly adapting them to individual instances (e.g., biomechanics of individual people). Using CKA and RRA, we obtain more accurate uncertainty quantification of the entire system's dynamics than pure GP-based and AR1 methods. We demonstrate the efficiency of co-kriging adjustment with an interpretable reinforcement learning control example, learning to control a biomechanical human arm using only a two-link arm simulation model (offline part) and CKA derived from a small amount of interaction data (on-the-fly online). Our method unlocks an efficient and uncertainty-aware way to implement reinforcement learning methods in real world complex systems for which only imperfect simulation models exist.
</details>
<details>
<summary>摘要</summary>
现实世界中的许多任务的强化学习很数据不fficient，而且广泛采用了基于模拟的模型训练系统。然而，在人机交互和许多实际场景中，没有适合一个模型所有的情况，因为实际系统的差异（例如，不同的人）或者模拟模型的必要简化。这需要两种方法：1. either学习个体系统的动态约束从数据中，需要大量的训练数据；2.使用实例的完整数字双方，可能在许多情况下不可能实现。我们介绍了两种新的方法：协同拟合调整（CKA）和ridge regression调整（RRA），这两种方法可以结合模拟模型和GP prior的优点。我们的调整方法基于一个自适应AR1拟合模型，并将其与GP prior相结合。这提供了数据和模拟效率的方式，使用简单的模拟模型（例如，两连接模型），并快速地适应个体实例（例如，人体生物力学）。使用CKA和RRA，我们可以获得更高精度的整体系统动态uncertainty量化，比GP和AR1方法更好。我们通过一个可解释的强化学习控制示例，使用只有两连接臂模型（线上部分）和CKA从小量的互动数据（在线部分）来学习控制人体生物力学。我们的方法可以快速、不确定性感知地实现强化学习方法在实际世界复杂系统中。
</details></li>
</ul>
<hr>
<h2 id="Practical-Homomorphic-Aggregation-for-Byzantine-ML"><a href="#Practical-Homomorphic-Aggregation-for-Byzantine-ML" class="headerlink" title="Practical Homomorphic Aggregation for Byzantine ML"></a>Practical Homomorphic Aggregation for Byzantine ML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05395">http://arxiv.org/abs/2309.05395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Choffrut, Rachid Guerraoui, Rafael Pinot, Renaud Sirdey, John Stephan, Martin Zuber<br>for:这个论文是关于分布式学习中的安全和隐私问题的研究。methods:这个论文使用了一种新的纯文本编码方法，这种方法可以在分布式学习中实现稳定的聚合器，并且可以加速现有的幂ometric sorting。results:该论文的实验结果表明，使用这种纯文本编码方法可以实现实时执行，并且与非隐私版本的算法具有相同的机器学习性能。<details>
<summary>Abstract</summary>
Due to the large-scale availability of data, machine learning (ML) algorithms are being deployed in distributed topologies, where different nodes collaborate to train ML models over their individual data by exchanging model-related information (e.g., gradients) with a central server. However, distributed learning schemes are notably vulnerable to two threats. First, Byzantine nodes can single-handedly corrupt the learning by sending incorrect information to the server, e.g., erroneous gradients. The standard approach to mitigate such behavior is to use a non-linear robust aggregation method at the server. Second, the server can violate the privacy of the nodes. Recent attacks have shown that exchanging (unencrypted) gradients enables a curious server to recover the totality of the nodes' data. The use of homomorphic encryption (HE), a gold standard security primitive, has extensively been studied as a privacy-preserving solution to distributed learning in non-Byzantine scenarios. However, due to HE's large computational demand especially for high-dimensional ML models, there has not yet been any attempt to design purely homomorphic operators for non-linear robust aggregators. In this work, we present SABLE, the first completely homomorphic and Byzantine robust distributed learning algorithm. SABLE essentially relies on a novel plaintext encoding method that enables us to implement the robust aggregator over batching-friendly BGV. Moreover, this encoding scheme also accelerates state-of-the-art homomorphic sorting with larger security margins and smaller ciphertext size. We perform extensive experiments on image classification tasks and show that our algorithm achieves practical execution times while matching the ML performance of its non-private counterpart.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于大规模数据的可用性，机器学习（ML）算法在分布式架构中被部署，不同的节点共同训练ML模型以其各自的数据交换模型相关信息（例如，梯度）与中央服务器。然而，分布式学习方案受到两种威胁。首先，Byzantine节点可以单方面地损害学习，通过向服务器发送错误信息（例如，错误的梯度）。标准的应对方法是使用非线性的robust合计方法。其次，服务器可以违反节点的隐私。最近的攻击表明，不加加密的梯度交换可以让curious服务器恢复整个节点的数据。使用 homomorphic encryption（HE），一种金标准安全 primitives，广泛研究了分布式学习的隐私保护方案。然而，由于 HE 的计算成本，特别是高维度 ML 模型，还没有任何尝试设计纯 homomorphic 操作符。在这项工作中，我们介绍 SABLE，首个完全 homomorphic 和 Byzantine 抗性的分布式学习算法。 SABLE 基于一种新的普通文本编码方法，允许我们实现robust合计器在批处理友好的 BGV 上。此外，这种编码方案还加速了当前的 homomorphic 排序，提供更大的安全优势和更小的 Ciphertext 大小。我们对图像分类任务进行了广泛的实验，并证明了我们的算法可以实现实用的执行时间，与非隐私counterpart匹配 ML 性能。
</details></li>
</ul>
<hr>
<h2 id="Career-Path-Recommendations-for-Long-term-Income-Maximization-A-Reinforcement-Learning-Approach"><a href="#Career-Path-Recommendations-for-Long-term-Income-Maximization-A-Reinforcement-Learning-Approach" class="headerlink" title="Career Path Recommendations for Long-term Income Maximization: A Reinforcement Learning Approach"></a>Career Path Recommendations for Long-term Income Maximization: A Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05391">http://arxiv.org/abs/2309.05391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spyros Avlonitis, Dor Lavi, Masoud Mansoury, David Graus</li>
<li>for: 增强职业规划过程，提高员工长期收入水平</li>
<li>methods: 利用Markov决策过程（MDP）和机器学习算法，如Sarsa、Q-学习和A2C，学习优化职业发展路径</li>
<li>results: 实验结果显示，RL模型，特别是Q-学习和Sarsa，可以提高员工的收入趋势，平均提高5%，对职业规划过程具有有效性。<details>
<summary>Abstract</summary>
This study explores the potential of reinforcement learning algorithms to enhance career planning processes. Leveraging data from Randstad The Netherlands, the study simulates the Dutch job market and develops strategies to optimize employees' long-term income. By formulating career planning as a Markov Decision Process (MDP) and utilizing machine learning algorithms such as Sarsa, Q-Learning, and A2C, we learn optimal policies that recommend career paths with high-income occupations and industries. The results demonstrate significant improvements in employees' income trajectories, with RL models, particularly Q-Learning and Sarsa, achieving an average increase of 5% compared to observed career paths. The study acknowledges limitations, including narrow job filtering, simplifications in the environment formulation, and assumptions regarding employment continuity and zero application costs. Future research can explore additional objectives beyond income optimization and address these limitations to further enhance career planning processes.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Dutch job market" is translated as "荷兰劳动市场" (hànlán láodòng shìchǎng)* "long-term income" is translated as "长期收入" (chángqī shōu jīn)* "career planning" is translated as "职业规划" (zhíyè guīhuà)* "Markov Decision Process" is translated as "Markov决策过程" (Markov juédà gòu jiāng)* "machine learning algorithms" is translated as "机器学习算法" (jīshī xuéxí suàn fāng)* "Q-Learning" is translated as "Q学习" (Q xuéxí)* "Sarsa" is translated as "SARSA" (SARSA)* "A2C" is translated as "A2C" (A2C)* "income trajectories" is translated as "收入轨迹" (shōu jīn xiào tiě)* "RL models" is translated as "RL模型" (RL módeli)
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Model-Reduction-and-Nonlinear-Model-Predictive-Control-of-an-Air-Separation-Unit-by-Applied-Koopman-Theory"><a href="#Data-Driven-Model-Reduction-and-Nonlinear-Model-Predictive-Control-of-an-Air-Separation-Unit-by-Applied-Koopman-Theory" class="headerlink" title="Data-Driven Model Reduction and Nonlinear Model Predictive Control of an Air Separation Unit by Applied Koopman Theory"></a>Data-Driven Model Reduction and Nonlinear Model Predictive Control of an Air Separation Unit by Applied Koopman Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05386">http://arxiv.org/abs/2309.05386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan C. Schulze, Danimir T. Doncevic, Nils Erwes, Alexander Mitsos</li>
<li>for: 实现实时能力是非线性预测控制（NMPC）的industrial应用前提。数据驱动模型减少提供了一种获取低阶控制模型的方法，并且该方法需要 minimal expert knowledge of the particular process and its model.</li>
<li>methods: 我们使用了 Schulze et al. (2022)提出的数据驱动减少策略，基于Koopman理论，生成了一个低阶控制模型，并使用了机器学习来构建。</li>
<li>results: 我们的减少策略和适应NMPC实现使得ASU的实时NMPC可以实现，而且相比原始模型，CPU时间减少了98%。<details>
<summary>Abstract</summary>
Achieving real-time capability is an essential prerequisite for the industrial implementation of nonlinear model predictive control (NMPC). Data-driven model reduction offers a way to obtain low-order control models from complex digital twins. In particular, data-driven approaches require little expert knowledge of the particular process and its model, and provide reduced models of a well-defined generic structure. Herein, we apply our recently proposed data-driven reduction strategy based on Koopman theory [Schulze et al. (2022), Comput. Chem. Eng.] to generate a low-order control model of an air separation unit (ASU). The reduced Koopman model combines autoencoders and linear latent dynamics and is constructed using machine learning. Further, we present an NMPC implementation that uses derivative computation tailored to the fixed block structure of reduced Koopman models. Our reduction approach with tailored NMPC implementation enables real-time NMPC of an ASU at an average CPU time decrease by 98 %.
</details>
<details>
<summary>摘要</summary>
实时能力是非线性预测控制（NMPC）的industrial实现的必要前提。数据驱动模型减少提供了从复杂数字响应器中获得低阶控制模型的方法。特别是数据驱动方法不需要特定过程和模型的专家知识，并提供了具有well-defined结构的减少模型。在这里，我们使用我们最近提出的基于Koopman理论的数据驱动减少策略来生成一个气体分离机（ASU）的低阶控制模型。减少的Koopman模型结合了自动编码器和线性潜在动力，通过机器学习构建。此外，我们提出了适应fixed block结构的减少Koopman模型的derivative计算，以实现NMPC实现。我们的减少方法和适应NMPC实现使得ASU的实时NMPC实现时间减少了98%。
</details></li>
</ul>
<hr>
<h2 id="EDAC-Efficient-Deployment-of-Audio-Classification-Models-For-COVID-19-Detection"><a href="#EDAC-Efficient-Deployment-of-Audio-Classification-Models-For-COVID-19-Detection" class="headerlink" title="EDAC: Efficient Deployment of Audio Classification Models For COVID-19 Detection"></a>EDAC: Efficient Deployment of Audio Classification Models For COVID-19 Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05357">http://arxiv.org/abs/2309.05357</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edac-ml4h/edac-ml4h">https://github.com/edac-ml4h/edac-ml4h</a></li>
<li>paper_authors: Andrej Jovanović, Mario Mihaly, Lennon Donaldson</li>
<li>for: 本研究旨在开发一种可靠、可行的 COVID-19 检测方法，以帮助预防和控制疫情的蔓延。</li>
<li>methods: 本研究使用机器学习方法，利用 CT 扫描图像和喊喊声音信号作为输入特征，通过深度神经网络架构实现 COVID-19 的检测。</li>
<li>results: 研究人员通过网络剪辑和量化技术来压缩两个模型，实现了模型文件尺寸的压缩和检测性能的维持。Specifically, 研究人员可以实现模型文件尺寸的压缩105.76倍和19.34倍，并对两个模型的检测时间进行了1.37倍和1.71倍的压缩。<details>
<summary>Abstract</summary>
The global spread of COVID-19 had severe consequences for public health and the world economy. The quick onset of the pandemic highlighted the potential benefits of cheap and deployable pre-screening methods to monitor the prevalence of the disease in a population. Various researchers made use of machine learning methods in an attempt to detect COVID-19. The solutions leverage various input features, such as CT scans or cough audio signals, with state-of-the-art results arising from deep neural network architectures. However, larger models require more compute; a pertinent consideration when deploying to the edge. To address this, we first recreated two models that use cough audio recordings to detect COVID-19. Through applying network pruning and quantisation, we were able to compress these two architectures without reducing the model's predictive performance. Specifically, we were able to achieve an 105.76x and an 19.34x reduction in the compressed model file size with corresponding 1.37x and 1.71x reductions in the inference times of the two models.
</details>
<details>
<summary>摘要</summary>
COVID-19 的全球蔓延引发了公共卫生和世界经济的严重后果。快速的疫情爆发表明了可能利用便宜并可部署的预屏检测方法来监测人口中疫苗的存在。各种研究人员利用机器学习方法尝试检测 COVID-19。这些解决方案利用了不同的输入特征，如 CT 扫描或喊嚔音信号，并使用了当前的神经网络架构获得了 state-of-the-art 的结果。然而，更大的模型需要更多的计算资源，这是在部署到边缘时必须考虑的。为解决这个问题，我们首先重新创建了两个使用喊嚔音记录检测 COVID-19 的模型。通过应用网络剪辑和量化，我们能够压缩这两个架构，而无需降低模型的预测性能。具体来说，我们能够实现一个 105.76x 和一个 19.34x 的压缩模型文件大小减少，相应的执行时间也降低了 1.37x 和 1.71x。
</details></li>
</ul>
<hr>
<h2 id="Neural-Discovery-of-Permutation-Subgroups"><a href="#Neural-Discovery-of-Permutation-Subgroups" class="headerlink" title="Neural Discovery of Permutation Subgroups"></a>Neural Discovery of Permutation Subgroups</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05352">http://arxiv.org/abs/2309.05352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Karjol, Rohan Kashyap, Prathosh A P</li>
<li>for: 找到 permutation group $S_{n}$ 中的子群 $H$</li>
<li>methods: 使用 $S_{n}$-invariant function 和线性变换来发现 underlying subgroup</li>
<li>results: 可以发现任何类型 $S_{k} (k \leq n)$ 的子群，并且证明了类似结果对循环和对称群也成立。<details>
<summary>Abstract</summary>
We consider the problem of discovering subgroup $H$ of permutation group $S_{n}$. Unlike the traditional $H$-invariant networks wherein $H$ is assumed to be known, we present a method to discover the underlying subgroup, given that it satisfies certain conditions. Our results show that one could discover any subgroup of type $S_{k} (k \leq n)$ by learning an $S_{n}$-invariant function and a linear transformation. We also prove similar results for cyclic and dihedral subgroups. Finally, we provide a general theorem that can be extended to discover other subgroups of $S_{n}$. We also demonstrate the applicability of our results through numerical experiments on image-digit sum and symmetric polynomial regression tasks.
</details>
<details>
<summary>摘要</summary>
我团队考虑了找到 permutation group $S_{n}$ 中的子群 $H$ 的问题。不同于传统的 $H$-invariant 网络，我们提出了一种方法，可以在 $H$ 满足 certain conditions 时找到其下面的子群。我们的结果表明，可以通过学习 $S_{n}$- invariant 函数和一个线性变换来找到任何类型 $S_{k} (k \leq n)$ 的子群。此外，我们还证明了这些结果的类型也适用于圆柱体和二面体 subgroup。最后，我们提出了一个通用的定理，可以扩展到其他 $S_{n}$ 中的子群。我们还通过数值实验证明了我们的结果，在 image-digit sum 和 symmetric polynomial regression 任务中。
</details></li>
</ul>
<hr>
<h2 id="A-DRL-based-Reflection-Enhancement-Method-for-RIS-assisted-Multi-receiver-Communications"><a href="#A-DRL-based-Reflection-Enhancement-Method-for-RIS-assisted-Multi-receiver-Communications" class="headerlink" title="A DRL-based Reflection Enhancement Method for RIS-assisted Multi-receiver Communications"></a>A DRL-based Reflection Enhancement Method for RIS-assisted Multi-receiver Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05343">http://arxiv.org/abs/2309.05343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Wang, Peizheng Li, Angela Doufexi, Mark A Beach</li>
<li>for: 本研究旨在优化具有 periodical 单反射profile 的 RIS 助持 wireless 通信系统中的 pointing 精度和反射强度。</li>
<li>methods: 本文提出了一种基于深度优化学习（DRL）的优化方法，用于解决 periodic 单反射profile 的杂合导致的 amplitude&#x2F;phase 干扰问题。</li>
<li>results: 对比Random Search和枚举Search两种方法，DRL 方法在优化时间短化方面表现出了明显的优势，并且实现了无硬件修改的 1.2 dB 增强和更宽的抛射束。<details>
<summary>Abstract</summary>
In reconfigurable intelligent surface (RIS)-assisted wireless communication systems, the pointing accuracy and intensity of reflections depend crucially on the 'profile,' representing the amplitude/phase state information of all elements in a RIS array. The superposition of multiple single-reflection profiles enables multi-reflection for distributed users. However, the optimization challenges from periodic element arrangements in single-reflection and multi-reflection profiles are understudied. The combination of periodical single-reflection profiles leads to amplitude/phase counteractions, affecting the performance of each reflection beam. This paper focuses on a dual-reflection optimization scenario and investigates the far-field performance deterioration caused by the misalignment of overlapped profiles. To address this issue, we introduce a novel deep reinforcement learning (DRL)-based optimization method. Comparative experiments against random and exhaustive searches demonstrate that our proposed DRL method outperforms both alternatives, achieving the shortest optimization time. Remarkably, our approach achieves a 1.2 dB gain in the reflection peak gain and a broader beam without any hardware modifications.
</details>
<details>
<summary>摘要</summary>
在带有智能表面（RIS）的无线通信系统中，投射精度和反射强度受到 Profile（所有数组元素的振荡状态信息）的影响。多个单投射Profile的超пози合成可以实现分布式用户的多投射。然而，单投射Profile的Periodic配置和多投射Profile的优化挑战尚未得到足够的研究。在这种双投射优化场景中，我们发现了 Profile重叠的misalignment导致的远场性能弱化。为解决这问题，我们提出了一种基于深度学习（DRL）的优化方法。与随机搜索和枚举搜索相比，我们的提案的DRL方法在优化时间上表现出了明显的优势，并且实现了无硬件修改的1.2 dB的反射峰强度提高和更广的扩散。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Gradient-Descent-like-relaxation-is-equivalent-to-Glauber-dynamics-in-discrete-optimization-and-inference-problems"><a href="#Stochastic-Gradient-Descent-like-relaxation-is-equivalent-to-Glauber-dynamics-in-discrete-optimization-and-inference-problems" class="headerlink" title="Stochastic Gradient Descent-like relaxation is equivalent to Glauber dynamics in discrete optimization and inference problems"></a>Stochastic Gradient Descent-like relaxation is equivalent to Glauber dynamics in discrete optimization and inference problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05337">http://arxiv.org/abs/2309.05337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Chiara Angelini, Angelo Giorgio Cavaliere, Raffaele Marino, Federico Ricci-Tersenghi</li>
<li>for: 这篇论文是 investigate whether Stochastic Gradient Descent (SGD) and Glauber dynamics are substantially different, and to understand the relationship between the two algorithms.</li>
<li>methods: 这篇论文使用了SGD-like algorithm和Metropolis Monte Carlo algorithm，并 Compares their dynamics in discrete optimization and inference problems.</li>
<li>results: 研究发现，在离散优化和推理问题中，SGD-like algorithm的动力学与Metropolis Monte Carlo algorithm具有很高的相似性，即使这两种算法在详细平衡不满足的情况下。这种相似性使得我们可以使用关于 Monte Carlo 算法的性能和限制来优化SGD-like algorithm的 mini-batch 大小，并使其在困难的推理问题中效率地恢复信号。<details>
<summary>Abstract</summary>
Is Stochastic Gradient Descent (SGD) substantially different from Glauber dynamics? This is a fundamental question at the time of understanding the most used training algorithm in the field of Machine Learning, but it received no answer until now. Here we show that in discrete optimization and inference problems, the dynamics of an SGD-like algorithm resemble very closely that of Metropolis Monte Carlo with a properly chosen temperature, which depends on the mini-batch size. This quantitative matching holds both at equilibrium and in the out-of-equilibrium regime, despite the two algorithms having fundamental differences (e.g.\ SGD does not satisfy detailed balance). Such equivalence allows us to use results about performances and limits of Monte Carlo algorithms to optimize the mini-batch size in the SGD-like algorithm and make it efficient at recovering the signal in hard inference problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Strong-and-Simple-Deep-Learning-Baseline-for-BCI-MI-Decoding"><a href="#A-Strong-and-Simple-Deep-Learning-Baseline-for-BCI-MI-Decoding" class="headerlink" title="A Strong and Simple Deep Learning Baseline for BCI MI Decoding"></a>A Strong and Simple Deep Learning Baseline for BCI MI Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07159">http://arxiv.org/abs/2309.07159</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elouayas/eegsimpleconv">https://github.com/elouayas/eegsimpleconv</a></li>
<li>paper_authors: Yassine El Ouahidi, Vincent Gripon, Bastien Pasdeloup, Ghaith Bouallegue, Nicolas Farrugia, Giulia Lioi</li>
<li>for: 这 paper 是为了提出一种简单的1D convolutional neural network（EEG-SimpleConv），用于股肱运动干预BCI的识别。</li>
<li>methods: 这 paper 使用了常见的文献中的标准组件，包括1D convolutional neural network和简单的训练策略。</li>
<li>results: EEG-SimpleConv 在四个EEG股肱运动数据集上表现至少如好或更高效，并且具有跨主题知识传递能力，但是执行时间较低。<details>
<summary>Abstract</summary>
We propose EEG-SimpleConv, a straightforward 1D convolutional neural network for Motor Imagery decoding in BCI. Our main motivation is to propose a very simple baseline to compare to, using only very standard ingredients from the literature. We evaluate its performance on four EEG Motor Imagery datasets, including simulated online setups, and compare it to recent Deep Learning and Machine Learning approaches. EEG-SimpleConv is at least as good or far more efficient than other approaches, showing strong knowledge-transfer capabilities across subjects, at the cost of a low inference time. We advocate that using off-the-shelf ingredients rather than coming with ad-hoc solutions can significantly help the adoption of Deep Learning approaches for BCI. We make the code of the models and the experiments accessible.
</details>
<details>
<summary>摘要</summary>
我们提出EEG-SimpleConv，一个简单的1D卷积神经网络，用于肌电意念识别 BCIs。我们的主要动机是提出一个非常简单的基准，使用文献中的标准元素。我们在四个EEG肌电意念数据集上评估了EEG-SimpleConv的性能，并与最近的深度学习和机器学习方法进行比较。EEG-SimpleConv至少和其他方法一样好，甚至更高效，在不同主题之间具有强大的知识传递能力，但没有高的推断时间。我们认为使用商业可用的元素而不是创建特殊解决方案可以帮助深度学习方法在 BCIs 的采纳。我们将模型和实验的代码公开。
</details></li>
</ul>
<hr>
<h2 id="Neural-Koopman-prior-for-data-assimilation"><a href="#Neural-Koopman-prior-for-data-assimilation" class="headerlink" title="Neural Koopman prior for data assimilation"></a>Neural Koopman prior for data assimilation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05317">http://arxiv.org/abs/2309.05317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anthony-frion/sentinel2ts">https://github.com/anthony-frion/sentinel2ts</a></li>
<li>paper_authors: Anthony Frion, Lucas Drumetz, Mauro Dalla Mura, Guillaume Tochon, Abdeldjalil Aïssa El Bey</li>
<li>for: 这个论文是用来描述如何使用神经网络模型来描述动态系统的。</li>
<li>methods: 该论文使用了 Koopman 算子理论来嵌入动态系统在隐藏空间中，以便在这个空间中描述动态系统的动态。它还介绍了一些方法来训练这种模型，包括自我监督学习和变量数据整合。</li>
<li>results: 该论文的实验结果表明，使用这种神经网络模型可以在不精确时间序列数据的情况下进行长期不间断的重建，并且可以在难以预测的情况下进行自动适应。此外，论文还示出了使用训练过的动态模型作为Variational数据整合的先验。<details>
<summary>Abstract</summary>
With the increasing availability of large scale datasets, computational power and tools like automatic differentiation and expressive neural network architectures, sequential data are now often treated in a data-driven way, with a dynamical model trained from the observation data. While neural networks are often seen as uninterpretable black-box architectures, they can still benefit from physical priors on the data and from mathematical knowledge. In this paper, we use a neural network architecture which leverages the long-known Koopman operator theory to embed dynamical systems in latent spaces where their dynamics can be described linearly, enabling a number of appealing features. We introduce methods that enable to train such a model for long-term continuous reconstruction, even in difficult contexts where the data comes in irregularly-sampled time series. The potential for self-supervised learning is also demonstrated, as we show the promising use of trained dynamical models as priors for variational data assimilation techniques, with applications to e.g. time series interpolation and forecasting.
</details>
<details>
<summary>摘要</summary>
In this paper, we use a neural network architecture that leverages the long-known Koopman operator theory to embed dynamical systems in latent spaces where their dynamics can be described linearly, allowing for several appealing features. We introduce methods that enable training such a model for long-term continuous reconstruction, even in difficult contexts where the data comes in irregularly-sampled time series. Additionally, we demonstrate the potential for self-supervised learning by showing how trained dynamical models can be used as priors for variational data assimilation techniques, with applications to time series interpolation and forecasting.
</details></li>
</ul>
<hr>
<h2 id="Balance-Measures-Derived-from-Insole-Sensor-Differentiate-Prodromal-Dementia-with-Lewy-Bodies"><a href="#Balance-Measures-Derived-from-Insole-Sensor-Differentiate-Prodromal-Dementia-with-Lewy-Bodies" class="headerlink" title="Balance Measures Derived from Insole Sensor Differentiate Prodromal Dementia with Lewy Bodies"></a>Balance Measures Derived from Insole Sensor Differentiate Prodromal Dementia with Lewy Bodies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08623">http://arxiv.org/abs/2309.08623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masatomo Kobayashi, Yasunori Yamada, Kaoru Shinkawa, Miyuki Nemoto, Miho Ota, Kiyotaka Nemoto, Tetsuaki Arai<br>for:这个研究旨在提供一种自动化识别患有 Lewy bodies 认知障碍的机器学习 pipeline，以便在 prodromal 阶段提供适当的护理。methods:这个研究使用了一种基于机器学习的自动化识别方法，利用了一个尺度感应器获取的30秒站立任务中的平衡测量数据。results:研究发现，结果模型可以准确地识别患有 Lewy bodies 认知障碍的参与者，与其他组比之下，准确率可达78.0%（AUC：0.681），比对照模型基于人口和临床神经心理测量的准确率高6.8%。<details>
<summary>Abstract</summary>
Dementia with Lewy bodies is the second most common type of neurodegenerative dementia, and identification at the prodromal stage$-$i.e., mild cognitive impairment due to Lewy bodies (MCI-LB)$-$is important for providing appropriate care. However, MCI-LB is often underrecognized because of its diversity in clinical manifestations and similarities with other conditions such as mild cognitive impairment due to Alzheimer's disease (MCI-AD). In this study, we propose a machine learning-based automatic pipeline that helps identify MCI-LB by exploiting balance measures acquired with an insole sensor during a 30-s standing task. An experiment with 98 participants (14 MCI-LB, 38 MCI-AD, 46 cognitively normal) showed that the resultant models could discriminate MCI-LB from the other groups with up to 78.0% accuracy (AUC: 0.681), which was 6.8% better than the accuracy of a reference model based on demographic and clinical neuropsychological measures. Our findings may open up a new approach for timely identification of MCI-LB, enabling better care for patients.
</details>
<details>
<summary>摘要</summary>
德мен般 Lewy body 是第二常见的肉体性脑下降症，早期识别$-$即轻度智能障碍due to Lewy bodies (MCI-LB)$-$是提供适当照顾的关键。但是， MCI-LB 常常被认为是其他病情的一部分，因为它的临床表现多样化和 Alzheimer's disease 的轻度智能障碍 (MCI-AD) 相似。在这项研究中，我们提出了一个基于机器学习的自动化管道，可以帮助识别 MCI-LB。我们使用了一个尖锐感知器测量的30秒立位任务中的平衡测量，并使用机器学习算法对数据进行分析。实验中还有98名参与者（14名 MCI-LB，38名 MCI-AD，46名正常智能），结果显示，所得的模型可以对 MCI-LB 和其他两个群体进行区别，精度可达78.0%（AUC: 0.681），较传统基于人口和临床神经心理测验的模型好6.8%。我们的发现可能会开启一个新的识别 MCI-LB 的方法，帮助病人获得更好的照顾。
</details></li>
</ul>
<hr>
<h2 id="Fully-Connected-Spatial-Temporal-Graph-for-Multivariate-Time-Series-Data"><a href="#Fully-Connected-Spatial-Temporal-Graph-for-Multivariate-Time-Series-Data" class="headerlink" title="Fully-Connected Spatial-Temporal Graph for Multivariate Time Series Data"></a>Fully-Connected Spatial-Temporal Graph for Multivariate Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05305">http://arxiv.org/abs/2309.05305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Wang, Yuecong Xu, Jianfei Yang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen</li>
<li>for: 本文旨在为多变量时间序列（MTS）数据提供有效的模型方法，具体来说是利用图神经网络（GNN）来处理MTS数据中的空间时间（ST）相互关联性。</li>
<li>methods: 本文提出了一种新的方法 called Fully-Connected Spatial-Temporal Graph Neural Network (FC-STGNN)，它包括两个关键组件：FC图构建和FC图卷积。FC图构建使用衰减图连接各感知器的时间距离，以全面模型ST相互关联性，并且FC图卷积使用移动覆盖GNN层来有效地捕捉ST相互关联性。</li>
<li>results: 对多个MTS数据集进行了广泛的实验，FC-STGNN的性能比SOTA方法更高，demonstrating the effectiveness of the proposed method in handling MTS data with ST dependencies.<details>
<summary>Abstract</summary>
Multivariate Time-Series (MTS) data is crucial in various application fields. With its sequential and multi-source (multiple sensors) properties, MTS data inherently exhibits Spatial-Temporal (ST) dependencies, involving temporal correlations between timestamps and spatial correlations between sensors in each timestamp. To effectively leverage this information, Graph Neural Network-based methods (GNNs) have been widely adopted. However, existing approaches separately capture spatial dependency and temporal dependency and fail to capture the correlations between Different sEnsors at Different Timestamps (DEDT). Overlooking such correlations hinders the comprehensive modelling of ST dependencies within MTS data, thus restricting existing GNNs from learning effective representations. To address this limitation, we propose a novel method called Fully-Connected Spatial-Temporal Graph Neural Network (FC-STGNN), including two key components namely FC graph construction and FC graph convolution. For graph construction, we design a decay graph to connect sensors across all timestamps based on their temporal distances, enabling us to fully model the ST dependencies by considering the correlations between DEDT. Further, we devise FC graph convolution with a moving-pooling GNN layer to effectively capture the ST dependencies for learning effective representations. Extensive experiments show the effectiveness of FC-STGNN on multiple MTS datasets compared to SOTA methods.
</details>
<details>
<summary>摘要</summary>
多变量时间序列（MTS）数据在各种应用领域具有重要意义。MTS数据具有顺序和多源（多感器）性质，因此自然地带有空间-时（ST）相关性，包括时间相关性和感器在每个时间戳中的空间相关性。为了有效利用这些信息，图神经网络（GNN）已经广泛应用。然而，现有方法通常分别捕捉空间相关性和时间相关性，而忽略了不同感器在不同时间戳之间的相关性（DEDT）。这限制了现有GNN的全面模型化能力，从而阻碍它们学习有效表示。为解决这个局限性，我们提出了一种新方法，即完全连接空间-时图神经网络（FC-STGNN），其包括两个关键组件：FC图构建和FC图卷积。为图构建，我们设计了衰减图来连接不同时间戳的感器，根据它们的时间距离来建立连接，从而允许我们完全模型ST相关性，包括不同DEDT之间的相关性。此外，我们设计了移动 Pooling GNN层，以有效地捕捉ST相关性，以便学习有效表示。我们对多个MTS数据集进行了广泛的实验，并证明了FC-STGNN在相对方法的基础上表现出色。
</details></li>
</ul>
<hr>
<h2 id="The-fine-print-on-tempered-posteriors"><a href="#The-fine-print-on-tempered-posteriors" class="headerlink" title="The fine print on tempered posteriors"></a>The fine print on tempered posteriors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05292">http://arxiv.org/abs/2309.05292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantinos Pitas, Julyan Arbel</li>
<li>for: 本研究探讨tempered posteriors的细节，发现了一些重要但未经讨论的点。</li>
<li>methods: 我们使用realistic models和dataset，以及Laplace approximation的紧张情况，发现在实际情况下，随机性不一定能提高测试精度。</li>
<li>results: 我们发现，bayesian模型中的随机性可能会导致测试精度下降，而targeting Frequentist metrics使得temperature参数$\lambda$在优化目标中无法被视为简单地修正错误的先前分布或概率。<details>
<summary>Abstract</summary>
We conduct a detailed investigation of tempered posteriors and uncover a number of crucial and previously undiscussed points. Contrary to previous results, we first show that for realistic models and datasets and the tightly controlled case of the Laplace approximation to the posterior, stochasticity does not in general improve test accuracy. The coldest temperature is often optimal. One might think that Bayesian models with some stochasticity can at least obtain improvements in terms of calibration. However, we show empirically that when gains are obtained this comes at the cost of degradation in test accuracy. We then discuss how targeting Frequentist metrics using Bayesian models provides a simple explanation of the need for a temperature parameter $\lambda$ in the optimization objective. Contrary to prior works, we finally show through a PAC-Bayesian analysis that the temperature $\lambda$ cannot be seen as simply fixing a misspecified prior or likelihood.
</details>
<details>
<summary>摘要</summary>
我们进行了详细的探究模tempered posteriors，并发现了一些重要且先前未讨论的点。与之前的结果不同，我们第一次表明在实际模型和数据集下，精度控制的情况下， Stochasticity不一定提高测试准确率。最冷的温度通常是最佳。一 might think Bayesian模型具有一定的随机性可以至少获得改善 Calibration的提高。但我们实际观测到，当获得了这些改善时，它们来的代价是测试准确率下降。然后我们讨论了如何使用 Bayesian 模型来target Frequentist metrics，并通过 PAC-Bayesian 分析显示了温度参数 $\lambda$ 不能被简单地视为修正了错误的先验或likelihood。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Finite-Initialization-for-Tensorized-Neural-Networks"><a href="#Efficient-Finite-Initialization-for-Tensorized-Neural-Networks" class="headerlink" title="Efficient Finite Initialization for Tensorized Neural Networks"></a>Efficient Finite Initialization for Tensorized Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06577">http://arxiv.org/abs/2309.06577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/i3bquantumteam/q4real">https://github.com/i3bquantumteam/q4real</a></li>
<li>paper_authors: Alejandro Mata Ali, Iñigo Perez Delgado, Marina Ristol Roura, Aitor Moreno Fdez. de Leceta</li>
<li>for: 本研究旨在开发一种初始化tensorized神经网络层的新方法，以避免这些层的参数数量爆炸。这种方法适用于具有较高节点数的层，其中每个节点都与输入或输出相连。</li>
<li>methods: 本方法基于层的 Frobenius  нор的 iterative 部分形式使用，以确保其 Finite 并且在某个范围内。这种 norm 计算效率高，可以在大多数情况下完全或部分计算。</li>
<li>results: 我们在不同层上应用该方法并评估其性能。我们还创建了一个 Python 函数，可以在任意层上运行，可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb">https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb</a><details>
<summary>Abstract</summary>
We present a novel method for initializing layers of tensorized neural networks in a way that avoids the explosion of the parameters of the matrix it emulates. The method is intended for layers with a high number of nodes in which there is a connection to the input or output of all or most of the nodes. The core of this method is the use of the Frobenius norm of this layer in an iterative partial form, so that it has to be finite and within a certain range. This norm is efficient to compute, fully or partially for most cases of interest. We apply the method to different layers and check its performance. We create a Python function to run it on an arbitrary layer, available in a Jupyter Notebook in the i3BQuantum repository: https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
</details>
<details>
<summary>摘要</summary>
我团队提出了一种新的层初始化方法，用于避免tensorized神经网络层的参数爆炸。这种方法适用于具有较高节点数的层，其中所有或大部分节点与输入或输出进行连接。我们的方法的核心在于使用层的 Frobenius  нор的迭代部分形式，使其必须是Finite且在某个范围内。这个 нор 效率Compute，可以在大多数情况下进行完全或半完全计算。我们在不同的层上应用了这种方法，并对其性能进行了检查。我们还创建了一个Python函数，可以在任意层上运行，可以在 i3BQuantum 存储库中找到：https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb。
</details></li>
</ul>
<hr>
<h2 id="Compressed-Real-Numbers-for-AI-a-case-study-using-a-RISC-V-CPU"><a href="#Compressed-Real-Numbers-for-AI-a-case-study-using-a-RISC-V-CPU" class="headerlink" title="Compressed Real Numbers for AI: a case-study using a RISC-V CPU"></a>Compressed Real Numbers for AI: a case-study using a RISC-V CPU</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07158">http://arxiv.org/abs/2309.07158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Rossi, Marco Cococcioni, Roger Ferrer Ibàñez, Jesùs Labarta, Filippo Mantovani, Marc Casas, Emanuele Ruffaldi, Sergio Saponara</li>
<li>for: 本研究旨在提高深度神经网络（DNN）的运算效率，使用更低的精度数字。</li>
<li>methods: 本文使用了两种已经在机器学习应用中取得了有趣的结果的压缩格式：bfloat和posit。</li>
<li>results: 本文提出了一种在计算前 decompress tensor 的方法，以避免压缩后的数据进行计算时的带宽使用和缓存不足问题。<details>
<summary>Abstract</summary>
As recently demonstrated, Deep Neural Networks (DNN), usually trained using single precision IEEE 754 floating point numbers (binary32), can also work using lower precision. Therefore, 16-bit and 8-bit compressed format have attracted considerable attention. In this paper, we focused on two families of formats that have already achieved interesting results in compressing binary32 numbers in machine learning applications, without sensible degradation of the accuracy: bfloat and posit. Even if 16-bit and 8-bit bfloat/posit are routinely used for reducing the storage of the weights/biases of trained DNNs, the inference still often happens on the 32-bit FPU of the CPU (especially if GPUs are not available). In this paper we propose a way to decompress a tensor of bfloat/posits just before computations, i.e., after the compressed operands have been loaded within the vector registers of a vector capable CPU, in order to save bandwidth usage and increase cache efficiency. Finally, we show the architectural parameters and considerations under which this solution is advantageous with respect to the uncompressed one.
</details>
<details>
<summary>摘要</summary>
Recently, deep neural networks (DNN) 通常使用单精度 IEEE 754 浮点数 (binary32) 进行训练，但也可以使用较低精度。因此，16 位和 8 位压缩格式在机器学习应用中吸引了广泛的关注。在这篇论文中，我们关注了两种家族的格式，它们已经在机器学习应用中实现了有趣的结果，无论是否占用精度：bfloat 和 posit。尽管 16 位和 8 位 bfloat/posit 通常用于减少训练后的权重/偏移的存储，但是推理通常发生在 CPU 的 32 位 FPU 上（尤其是如果 GPU 不可用）。在这篇论文中，我们提议在计算之前，即在 vector 可能 CPU 中加载压缩参数后，解压缩一个 tensor 中的 bfloat/posits，以节省带宽使用和提高缓存效率。最后，我们介绍了这种解压缩方案与不压缩方案之间的架构参数和考虑因素。
</details></li>
</ul>
<hr>
<h2 id="Beamforming-in-Wireless-Coded-Caching-Systems"><a href="#Beamforming-in-Wireless-Coded-Caching-Systems" class="headerlink" title="Beamforming in Wireless Coded-Caching Systems"></a>Beamforming in Wireless Coded-Caching Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05276">http://arxiv.org/abs/2309.05276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sneha Madhusudan, Charitha Madapatha, Behrooz Makki, Hao Guo, Tommy Svensson</li>
<li>for: 提高存取网络的容量对于传输网络 pose 容量挑战，但用户数据需求具有空间和时间相关性，可能被利用。</li>
<li>methods: 我们 investigate 一种 integrate beamforming 和coded-caching 的无线传输网络架构，其中服务器具有多个天线，将内容广播到缓存节点，负责为用户提供内容。</li>
<li>results: 我们的设计可以实现多播机会增加，干扰 Mitigation 和减少峰值后端流量。 Comparative 分析表明，与传统、uncoded caching 方法相比，我们的方法具有显著的优势。 更进一步，我们发现适当的扫描优化可以增强coded-caching 技术的效iveness，导致峰值后端流量的显著减少。<details>
<summary>Abstract</summary>
Increased capacity in the access network poses capacity challenges on the transport network due to the aggregated traffic. However, there are spatial and time correlation in the user data demands that could potentially be utilized. To that end, we investigate a wireless transport network architecture that integrates beamforming and coded-caching strategies. Especially, our proposed design entails a server with multiple antennas that broadcasts content to cache nodes responsible for serving users. Traditional caching methods face the limitation of relying on the individual memory with additional overhead. Hence, we develop an efficient genetic algorithm-based scheme for beam optimization in the coded-caching system. By exploiting the advantages of beamforming and coded-caching, the architecture achieves gains in terms of multicast opportunities, interference mitigation, and reduced peak backhaul traffic. A comparative analysis of this joint design with traditional, un-coded caching schemes is also conducted to assess the benefits of the proposed approach. Additionally, we examine the impact of various buffering and decoding methods on the performance of the coded-caching scheme. Our findings suggest that proper beamforming is useful in enhancing the effectiveness of the coded-caching technique, resulting in significant reduction in peak backhaul traffic.
</details>
<details>
<summary>摘要</summary>
增加了访问网络的容量会导致传输网络的压力增加，但是用户数据需求存在空间和时间的协调关系，这可能可以利用。为此，我们调查了一种具有扫描和编码缓存策略的无线传输网络架构。具体来说，我们的设计包括一个有多个天线的服务器，通过扫描来广播内容到缓存节点，负责服务用户。传统的缓存方法受到各自内存的限制，同时增加了过程中的额外开销。因此，我们开发了一种高效的遗传算法基本方法来优化扫描。通过利用扫描和编码缓存的优势，该架构实现了多播机会的增加、干扰 Mitigation和传输峰值压力的减少。我们还对该 JOINT 设计与传统、未编码缓存方案进行了比较分析，以评估提议的方法的优势。此外，我们还研究了缓存和解码方法对编码缓存方案的性能影响。我们的发现表明，当使用正确的扫描时，编码缓存技术的效果会增强，从而导致峰值传输压力的显著减少。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Graphon-Process-Convergence-of-Graph-Frequencies-in-Stretched-Cut-Distance"><a href="#Generalized-Graphon-Process-Convergence-of-Graph-Frequencies-in-Stretched-Cut-Distance" class="headerlink" title="Generalized Graphon Process: Convergence of Graph Frequencies in Stretched Cut Distance"></a>Generalized Graphon Process: Convergence of Graph Frequencies in Stretched Cut Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05260">http://arxiv.org/abs/2309.05260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingchao Jian, Feng Ji, Wee Peng Tay</li>
<li>for: 本文研究了稀疏图序列的收敛Property，并提出了一种基于泛化图和延展距离的方法来描述这种收敛。</li>
<li>methods: 本文使用了通过一种随机图生成器生成的泛化图来模型稀疏图的收敛，并证明了随机图的谱值收敛。</li>
<li>results: 本文的研究表明，通过延展距离来定义的稀疏图序列可以收敛到一个泛化图，并且可以在稀疏图上实现 Transfer Learning。<details>
<summary>Abstract</summary>
Graphons have traditionally served as limit objects for dense graph sequences, with the cut distance serving as the metric for convergence. However, sparse graph sequences converge to the trivial graphon under the conventional definition of cut distance, which make this framework inadequate for many practical applications. In this paper, we utilize the concepts of generalized graphons and stretched cut distance to describe the convergence of sparse graph sequences. Specifically, we consider a random graph process generated from a generalized graphon. This random graph process converges to the generalized graphon in stretched cut distance. We use this random graph process to model the growing sparse graph, and prove the convergence of the adjacency matrices' eigenvalues. We supplement our findings with experimental validation. Our results indicate the possibility of transfer learning between sparse graphs.
</details>
<details>
<summary>摘要</summary>
GRAPHONS  tradicionalmente han servido como objetos de límite para secuencias de grafos densos, con la distancia de cortes sirviendo como métrica para la convergencia. Sin embargo, las secuencias de grafos esparsas convergen al grafón trivial bajo la definición conventional de distancia de cortes, lo que hace que este marco sea inadecuado para muchas aplicaciones prácticas. En este artículo, utilizamos los conceptos de grafons generalizados y la distancia de cortes estirada para describir la convergencia de las secuencias de grafos esparsas. En particular, consideramos un proceso de grafos aleatorio generado desde un grafon generalizado. Este proceso de grafos aleatorio converge al grafon generalizado en distancia de cortes estirada. Usamos este proceso de grafos aleatorio para modelar el crecimiento de grafos esparsos y probamos la convergencia de los valores propios de las matrices de conexión. Complementamos nuestros resultados con validación experimental. Nuestros resultados sugieren la posibilidad de transferencia de aprendizaje entre grafos esparsos.Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-physics-informed-and-attention-based-graph-learning-approach-for-regional-electric-vehicle-charging-demand-prediction"><a href="#A-physics-informed-and-attention-based-graph-learning-approach-for-regional-electric-vehicle-charging-demand-prediction" class="headerlink" title="A physics-informed and attention-based graph learning approach for regional electric vehicle charging demand prediction"></a>A physics-informed and attention-based graph learning approach for regional electric vehicle charging demand prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05259">http://arxiv.org/abs/2309.05259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haohao Qu, Haoxuan Kuang, Jun Li, Linlin You</li>
<li>for: 预测电动车充电需求，以便优化电动车充电空间使用，从而缓解城市智能交通系统的负载。</li>
<li>methods: 融合 graf和时间注意力机制进行特征提取，并使用物理学 Informed 元学习来对模型进行预训。</li>
<li>results: 在深圳18,013个电动车充电桩数据集上进行评估，获得了预测性能的州OF-THE-ART和理解价格变化导致充电需求的适应性。<details>
<summary>Abstract</summary>
Along with the proliferation of electric vehicles (EVs), optimizing the use of EV charging space can significantly alleviate the growing load on intelligent transportation systems. As the foundation to achieve such an optimization, a spatiotemporal method for EV charging demand prediction in urban areas is required. Although several solutions have been proposed by using data-driven deep learning methods, it can be found that these performance-oriented methods may suffer from misinterpretations to correctly handle the reverse relationship between charging demands and prices. To tackle the emerging challenges of training an accurate and interpretable prediction model, this paper proposes a novel approach that enables the integration of graph and temporal attention mechanisms for feature extraction and the usage of physic-informed meta-learning in the model pre-training step for knowledge transfer. Evaluation results on a dataset of 18,013 EV charging piles in Shenzhen, China, show that the proposed approach, named PAG, can achieve state-of-the-art forecasting performance and the ability in understanding the adaptive changes in charging demands caused by price fluctuations.
</details>
<details>
<summary>摘要</summary>
随着电动汽车（EV）的普及，优化EV充电空间的使用可以有效缓解城市交通系统的增长荷负。为达到这种优化，在城市区域内需要一种空间时间方法来预测EV充电需求。虽然已有使用数据驱动深度学习方法提出了许多解决方案，但这些性能强调的方法可能会错误地处理充电需求和价格之间的反关系。为了解决这些新出现的训练精度和可读性预测模型的挑战，这篇论文提出了一种新的方法，名为PAG，它可以integrate图和时间注意力机制以提取特征，并在模型预训练步骤中使用物理学习来传递知识。对于18,013个EV充电柱在深圳市的数据进行评估，PAG方法可以实现领先的预测性能和理解充电需求因价格波动而发生的适应变化。
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Effect-of-Pre-training-on-Time-Series-Classification"><a href="#Examining-the-Effect-of-Pre-training-on-Time-Series-Classification" class="headerlink" title="Examining the Effect of Pre-training on Time Series Classification"></a>Examining the Effect of Pre-training on Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05256">http://arxiv.org/abs/2309.05256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashu Pu, Shiwei Zhao, Ling Cheng, Yongzhu Chang, Runze Wu, Tangjie Lv, Rongsheng Zhang</li>
<li>for: 这个研究旨在探讨无监督预训的后续精度调整方法在新的时间序列模式下的效果。</li>
<li>methods: 该研究使用了150个分类数据集，来对无监督预训后续精度调整的效果进行了全面的检查。</li>
<li>results: 研究发现，预训只有在数据适应度较差的情况下能够改善优化过程，而不会对数据适应度较好的情况下提供正则化效果。此外，预训不会提高总体化能力，但可以加速参数的整合。尽管预训任务和模型结构都会影响该方法在给定数据集上的效果，但模型结构在这两个因素中扮演更重要的角色。<details>
<summary>Abstract</summary>
Although the pre-training followed by fine-tuning paradigm is used extensively in many fields, there is still some controversy surrounding the impact of pre-training on the fine-tuning process. Currently, experimental findings based on text and image data lack consensus. To delve deeper into the unsupervised pre-training followed by fine-tuning paradigm, we have extended previous research to a new modality: time series. In this study, we conducted a thorough examination of 150 classification datasets derived from the Univariate Time Series (UTS) and Multivariate Time Series (MTS) benchmarks. Our analysis reveals several key conclusions. (i) Pre-training can only help improve the optimization process for models that fit the data poorly, rather than those that fit the data well. (ii) Pre-training does not exhibit the effect of regularization when given sufficient training time. (iii) Pre-training can only speed up convergence if the model has sufficient ability to fit the data. (iv) Adding more pre-training data does not improve generalization, but it can strengthen the advantage of pre-training on the original data volume, such as faster convergence. (v) While both the pre-training task and the model structure determine the effectiveness of the paradigm on a given dataset, the model structure plays a more significant role.
</details>
<details>
<summary>摘要</summary>
尽管预训练followed by fine-tuning paradigm在多个领域广泛应用，但是预训练对精度调整的影响仍存在一定的争议。目前，基于文本和图像数据的实验结果缺乏一致性。为了更深入地探讨预训练followed by fine-tuning paradigm，我们在新的模式下进行了扩展性研究：时间序列数据。在这项研究中，我们对150个分类数据集 derived from Univariate Time Series (UTS)和Multivariate Time Series (MTS) benchmark进行了全面的分析。我们的分析结果显示了以下几点：(i) 预训练只能帮助改善模型不适合数据的优化过程，而不是适合数据的模型。(ii) 预训练不会在充分训练时间后展现正则化效果。(iii) 预训练只能快速化整合速度，只要模型具备足够的适应能力。(iv) 增加更多的预训练数据不会提高总体化，但可以强化预训练对原始数据量的优势，如更快的整合速度。(v) 预训练任务和模型结构决定了预训练followed by fine-tuning paradigm在给定数据集的效果，但模型结构在这两个因素中扮演更重要的角色。
</details></li>
</ul>
<hr>
<h2 id="A-quantum-tug-of-war-between-randomness-and-symmetries-on-homogeneous-spaces"><a href="#A-quantum-tug-of-war-between-randomness-and-symmetries-on-homogeneous-spaces" class="headerlink" title="A quantum tug of war between randomness and symmetries on homogeneous spaces"></a>A quantum tug of war between randomness and symmetries on homogeneous spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05253">http://arxiv.org/abs/2309.05253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Arvind, Kishor Bharti, Jun Yong Khoo, Dax Enshan Koh, Jian Feng Kong</li>
<li>for: 研究量子信息中的Symmetry和Randomness的关系</li>
<li>methods: 采用几何方法，考虑状态为$H$-相似的集合，并引入哈恩抽象来描述真正随机的系统</li>
<li>results: 提出了基于哈恩空间的随机性概念，并研究了近似随机性和假随机性的概念，以及其应用于量子机器学习模型的表达性。<details>
<summary>Abstract</summary>
We explore the interplay between symmetry and randomness in quantum information. Adopting a geometric approach, we consider states as $H$-equivalent if related by a symmetry transformation characterized by the group $H$. We then introduce the Haar measure on the homogeneous space $\mathbb{U}/H$, characterizing true randomness for $H$-equivalent systems. While this mathematical machinery is well-studied by mathematicians, it has seen limited application in quantum information: we believe our work to be the first instance of utilizing homogeneous spaces to characterize symmetry in quantum information. This is followed by a discussion of approximations of true randomness, commencing with $t$-wise independent approximations and defining $t$-designs on $\mathbb{U}/H$ and $H$-equivalent states. Transitioning further, we explore pseudorandomness, defining pseudorandom unitaries and states within homogeneous spaces. Finally, as a practical demonstration of our findings, we study the expressibility of quantum machine learning ansatze in homogeneous spaces. Our work provides a fresh perspective on the relationship between randomness and symmetry in the quantum world.
</details>
<details>
<summary>摘要</summary>
我们探索量子信息中的对称和随机性的关系。我们采用幂等方法，将状态看作$H$-相似的，当$H$表示一个群。然后我们引入了哈ROW measure在同质空间$\mathbb{U}/H$上，这个概率测度描述了真正的随机性。这个数学工具已经由数学家们很好地研究过，但在量子信息领域却很少被应用。我们认为我们的工作是量子信息领域中第一次利用同质空间来描述对称的。接着，我们讨论了true randomness的近似，包括$t$-wise独立的近似和$H$-相似的状态上的$t$-设计。然后我们探索了假随机性，定义了在同质空间中的假随机单位和状态。最后，我们研究了使用同质空间来表示量子机器学习模型的表达性。我们的工作提供了量子世界中对于随机性和对称的新的视角。
</details></li>
</ul>
<hr>
<h2 id="Graph-Contextual-Contrasting-for-Multivariate-Time-Series-Classification"><a href="#Graph-Contextual-Contrasting-for-Multivariate-Time-Series-Classification" class="headerlink" title="Graph Contextual Contrasting for Multivariate Time Series Classification"></a>Graph Contextual Contrasting for Multivariate Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05202">http://arxiv.org/abs/2309.05202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Wang, Yuecong Xu, Jianfei Yang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen</li>
<li>for: 本文提出了一种新的对比学习方法，用于 Multivariate Time-Series（MTS）分类。该方法可以保证不同视图的无标样本之间的一致性，并学习有效的表示。</li>
<li>methods: 该方法使用了图像增强和对比技术来保持时间束缚的稳定性，并使用图像对比来提取强健的感知器和相关性。</li>
<li>results: 实验结果表明，提出的GCC方法可以在多种MTS分类任务上达到领先的表现。<details>
<summary>Abstract</summary>
Contrastive learning, as a self-supervised learning paradigm, becomes popular for Multivariate Time-Series (MTS) classification. It ensures the consistency across different views of unlabeled samples and then learns effective representations for these samples. Existing contrastive learning methods mainly focus on achieving temporal consistency with temporal augmentation and contrasting techniques, aiming to preserve temporal patterns against perturbations for MTS data. However, they overlook spatial consistency that requires the stability of individual sensors and their correlations. As MTS data typically originate from multiple sensors, ensuring spatial consistency becomes essential for the overall performance of contrastive learning on MTS data. Thus, we propose Graph Contextual Contrasting (GCC) for spatial consistency across MTS data. Specifically, we propose graph augmentations including node and edge augmentations to preserve the stability of sensors and their correlations, followed by graph contrasting with both node- and graph-level contrasting to extract robust sensor- and global-level features. We further introduce multi-window temporal contrasting to ensure temporal consistency in the data for each sensor. Extensive experiments demonstrate that our proposed GCC achieves state-of-the-art performance on various MTS classification tasks.
</details>
<details>
<summary>摘要</summary>
《对比学习》作为一种自我监督学习方法，在多变量时间序列（MTS）分类中变得受欢迎。它确保不同视图的无标示样本之间的一致性，然后学习这些样本的有效表示。现有的对比学习方法主要强调获得时间一致性，通过时间扩展和对比技术来保持时间模式对MTS数据的抗变化。然而，它们忽视了空间一致性，即感知器的稳定性和相关性。由于MTS数据通常来自多个感知器，保证空间一致性对总性表现的影响是关键的。因此，我们提议使用图构造对比（GCC）来保证MTS数据的空间一致性。具体来说，我们提出了图加工包括节点和边加工，以保持感知器的稳定性和相关性，然后与节点和图 уровень对比来提取Robust的感知器和全局级别特征。此外，我们还引入了多窗口时间对比来确保每个感知器的数据中的时间一致性。广泛的实验证明，我们提出的GCC可以在多种MTS分类任务上达到顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="CARE-Confidence-rich-Autonomous-Robot-Exploration-using-Bayesian-Kernel-Inference-and-Optimization"><a href="#CARE-Confidence-rich-Autonomous-Robot-Exploration-using-Bayesian-Kernel-Inference-and-Optimization" class="headerlink" title="CARE: Confidence-rich Autonomous Robot Exploration using Bayesian Kernel Inference and Optimization"></a>CARE: Confidence-rich Autonomous Robot Exploration using Bayesian Kernel Inference and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05200">http://arxiv.org/abs/2309.05200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shepherd-gregory/bkio-exploration">https://github.com/shepherd-gregory/bkio-exploration</a></li>
<li>paper_authors: Yang Xu, Ronghao Zheng, Senlin Zhang, Meiqin Liu, Shoudong Huang</li>
<li>for: 本研究旨在提高无人机在未知和复杂环境中的信息基于自主探索效率。</li>
<li>methods: 我们首先使用 Gaussian process（GP）回归来学习一个替身模型，以便通过查询控制动作的信息强度来INFER confidence-rich mutual information（CRMI），然后采用一个包含预测值和预测不确定性的目标函数来进行 Bayesian optimization（BO），即 GP-based BO（GPBO）。通过让探索和利用之间进行交互，我们可以实现质量和效率之间的平衡。</li>
<li>results: 我们提出了一种新的轻量级信息增加推理方法，基于 Bayesian kernel inference and optimization（BKIO），可以在不需要训练的情况下实现approximate logarithmic complexity。BKIO可以通过INFER CRMI和选择最佳动作来实现GPBO的同等准确性，但是具有更高的效率。我们在不同的无结构、杂乱环境中进行了广泛的数值和实际实验，并证明了我们的提议的效果。我们还提供了我们的开源实现代码，可以在 <a target="_blank" rel="noopener" href="https://github.com/Shepherd-Gregory/BKIO-Exploration">https://github.com/Shepherd-Gregory/BKIO-Exploration</a> 中下载。<details>
<summary>Abstract</summary>
In this paper, we consider improving the efficiency of information-based autonomous robot exploration in unknown and complex environments. We first utilize Gaussian process (GP) regression to learn a surrogate model to infer the confidence-rich mutual information (CRMI) of querying control actions, then adopt an objective function consisting of predicted CRMI values and prediction uncertainties to conduct Bayesian optimization (BO), i.e., GP-based BO (GPBO). The trade-off between the best action with the highest CRMI value (exploitation) and the action with high prediction variance (exploration) can be realized. To further improve the efficiency of GPBO, we propose a novel lightweight information gain inference method based on Bayesian kernel inference and optimization (BKIO), achieving an approximate logarithmic complexity without the need for training. BKIO can also infer the CRMI and generate the best action using BO with bounded cumulative regret, which ensures its comparable accuracy to GPBO with much higher efficiency. Extensive numerical and real-world experiments show the desired efficiency of our proposed methods without losing exploration performance in different unstructured, cluttered environments. We also provide our open-source implementation code at https://github.com/Shepherd-Gregory/BKIO-Exploration.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了改善自动机器人在未知和复杂环境中的信息基本探索效率。我们首先使用 Gaussian process（GP）回归来学习一个替代模型，来推算信息充沛的共享信息（CRMI）的询问控制动作的估计值，然后采用一个包含预测值和预测不确定性的目标函数来进行 Bayesian 优化（BO），即 GP-based BO（GPBO）。通过在最佳动作和高信息充沛动作之间进行负荷平衡，我们可以实现对 GPBO 的高效性。为了进一步提高 GPBO 的效率，我们提出了一种新的轻量级信息增强推断方法，基于 Bayesian kernel 推断和优化（BKIO），可以在无需训练的情况下实现对数 logarithmic 复杂度。BKIO 还可以计算 CRMI 和生成最佳动作，并使用 BO 实现 bounded 累累 regret，这 garantizes 其与 GPBO 相比较准确。我们在不同的无结构、堆满环境中进行了广泛的数值和实际实验，并证明了我们的提出的方法可以保持高效性而不失去探索性。我们还提供了我们的开源实现代码，可以在 <https://github.com/Shepherd-Gregory/BKIO-Exploration> 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/11/cs.LG_2023_09_11/" data-id="clp869u0800rfk588c4e6g4by" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/50/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/49/">49</a><a class="page-number" href="/page/50/">50</a><span class="page-number current">51</span><a class="page-number" href="/page/52/">52</a><a class="page-number" href="/page/53/">53</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/52/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
