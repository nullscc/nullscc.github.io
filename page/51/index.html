
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/51/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_08_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/26/cs.CV_2023_08_26/" class="article-date">
  <time datetime="2023-08-26T13:00:00.000Z" itemprop="datePublished">2023-08-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/26/cs.CV_2023_08_26/">cs.CV - 2023-08-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Joint-Modeling-of-Feature-Correspondence-and-a-Compressed-Memory-for-Video-Object-Segmentation"><a href="#Joint-Modeling-of-Feature-Correspondence-and-a-Compressed-Memory-for-Video-Object-Segmentation" class="headerlink" title="Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation"></a>Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13505">http://arxiv.org/abs/2308.13505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Zhang, Yutao Cui, Gangshan Wu, Limin Wang</li>
<li>for: 提高视频对象分割（VOS）方法的性能，解决现有方法中 dense matching  после Extracting 特征的限制，以及 pixel-wise 匹配导致的目标信息不具备全面理解。</li>
<li>methods: 提出一种 joint 模型，名为 JointFormer，具有三个元素的共同模型，包括特征、匹配和压缩存储。核心设计是 Joint Block，通过注意力的灵活性同时提取特征和传递目标信息到当前 токен和压缩存储 токен。这种方案允许进行广泛的信息传递和特征学习。</li>
<li>results: 根据 DAVIS 2017 val&#x2F;test-dev 和 YouTube-VOS 2018&#x2F;2019 val  bencmarks，我们的方法实现了新的state-of-the-art性能（89.7%和87.6%）和（87.0%和87.0%），与现有方法相比提高了很大的margin。<details>
<summary>Abstract</summary>
Current prevailing Video Object Segmentation (VOS) methods usually perform dense matching between the current and reference frames after extracting their features. One on hand, the decoupled modeling restricts the targets information propagation only at high-level feature space. On the other hand, the pixel-wise matching leads to a lack of holistic understanding of the targets. To overcome these issues, we propose a unified VOS framework, coined as JointFormer, for joint modeling the three elements of feature, correspondence, and a compressed memory. The core design is the Joint Block, utilizing the flexibility of attention to simultaneously extract feature and propagate the targets information to the current tokens and the compressed memory token. This scheme allows to perform extensive information propagation and discriminative feature learning. To incorporate the long-term temporal targets information, we also devise a customized online updating mechanism for the compressed memory token, which can prompt the information flow along the temporal dimension and thus improve the global modeling capability. Under the design, our method achieves a new state-of-art performance on DAVIS 2017 val/test-dev (89.7% and 87.6%) and YouTube-VOS 2018/2019 val (87.0% and 87.0%) benchmarks, outperforming existing works by a large margin.
</details>
<details>
<summary>摘要</summary>
当前主流的视频对象分割（VOS）方法通常是在提取特征后进行密集匹配 между当前和参考帧。一方面，分解模型限制目标信息的传播只在高级特征空间进行。另一方面，像素级匹配导致无法全面理解目标。为了解决这些问题，我们提出了一个统一的VOS框架，命名为JointFormer，用于联合模型特征、匹配和压缩存储。核心设计是Joint块，利用注意力的灵活性同时提取特征和传递目标信息到当前 токен和压缩存储 токен。这种方案允许进行广泛的信息传递和特征学习。为了包含长期时间的目标信息，我们还设计了自定义在线更新机制，以便在时间维度上流动信息，从而提高全局模型能力。根据设计，我们的方法在DAVIS 2017 val/test-dev（89.7%和87.6%）和YouTube-VOS 2018/2019 val（87.0%和87.0%）标准测试上达到了新的状态级表现，超过现有方法的表现。
</details></li>
</ul>
<hr>
<h2 id="A2Q-Accumulator-Aware-Quantization-with-Guaranteed-Overflow-Avoidance"><a href="#A2Q-Accumulator-Aware-Quantization-with-Guaranteed-Overflow-Avoidance" class="headerlink" title="A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance"></a>A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13504">http://arxiv.org/abs/2308.13504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ian Colbert, Alessandro Pappalardo, Jakoba Petri-Koenig</li>
<li>for: 训练量化神经网络（QNN）以避免在推理时使用低精度累加器时发生溢出。</li>
<li>methods: 引入了一种新的量化方法，即accumulator-aware quantization（A2Q），它基于模型量化的约束，使得在训练QNN时，模型权重的L1范围遵循累加器的位数范围。这种方法同时激发了模型权重的不结构化粒度缺失，以确保避免溢出。</li>
<li>results: A2Q可以在深度学习基于计算机视觉任务上训练QNN，而无需使用浮点数据类型，同时保持模型精度与浮点模型相似。在我们的评估中，我们考虑了A2Q在通用平台和可编程硬件上的影响。但我们主要目标是在FPGA上部署模型，因为它可以完全利用自定义累加器的位数。我们的实验表明，累加器位数对FPGA上的加速器资源利用率产生显著影响。在我们的 benchmark 中，A2Q可以在 average 上减少资源利用率达到 2.3倍，与32位累加器相比，而且保持99.2%的浮点模型精度。<details>
<summary>Abstract</summary>
We present accumulator-aware quantization (A2Q), a novel weight quantization method designed to train quantized neural networks (QNNs) to avoid overflow when using low-precision accumulators during inference. A2Q introduces a unique formulation inspired by weight normalization that constrains the L1-norm of model weights according to accumulator bit width bounds that we derive. Thus, in training QNNs for low-precision accumulation, A2Q also inherently promotes unstructured weight sparsity to guarantee overflow avoidance. We apply our method to deep learning-based computer vision tasks to show that A2Q can train QNNs for low-precision accumulators while maintaining model accuracy competitive with a floating-point baseline. In our evaluations, we consider the impact of A2Q on both general-purpose platforms and programmable hardware. However, we primarily target model deployment on FPGAs because they can be programmed to fully exploit custom accumulator bit widths. Our experimentation shows accumulator bit width significantly impacts the resource efficiency of FPGA-based accelerators. On average across our benchmarks, A2Q offers up to a 2.3x reduction in resource utilization over 32-bit accumulator counterparts with 99.2% of the floating-point model accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的量化预测方法，名为“accumulator-aware quantization”（A2Q），用于在推导过程中避免量化神经网络（QNN）的过流。A2Q将量化神经网络的模型重量与累绩器的位元数量产生联乘关系，以确保在推导过程中避免过流。因此，在对低精度累绩器进行训练时，A2Q同时具有过程简化和过程简化的功能。我们将这种方法应用于深度学习数据领域的应用，并证明A2Q可以在低精度累绩器下训练QNN，并保持与浮点数据模型的竞争力。在我们的评估中，我们考虑了在通用平台和可程式硬件上的影响，但我们主要针对FPGA进行部署，因为FPGA可以根据自己的特定累绩器位元数量进行自适应。我们的实验表明，累绩器位元数量有着显著的影响力，A2Q可以在FPGA上的加速器上提供更好的资源利用率。在我们的测试中，A2Q在32位累绩器下提供了2.3倍的资源利用率，并保持99.2%的浮点数据模型精度。
</details></li>
</ul>
<hr>
<h2 id="Eventful-Transformers-Leveraging-Temporal-Redundancy-in-Vision-Transformers"><a href="#Eventful-Transformers-Leveraging-Temporal-Redundancy-in-Vision-Transformers" class="headerlink" title="Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers"></a>Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13494">http://arxiv.org/abs/2308.13494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WISION-Lab/eventful-transformer">https://github.com/WISION-Lab/eventful-transformer</a></li>
<li>paper_authors: Matthew Dutson, Yin Li, Mohit Gupta</li>
<li>for: 这篇论文的目的是提高视觉识别任务中的Transformers模型精度，并且降低它们的computational cost。</li>
<li>methods: 这篇论文使用了一种称为”Eventful Transformers”的方法，它可以将Transformers模型转换为具有适应控制的计算成本，并且可以在runtime中进行自适应调节。</li>
<li>results: 这篇论文在大规模的视频物件检测（ImageNet VID）和动作识别（EPIC-Kitchens 100） dataset上进行了评估，发现这种方法可以实现2-4倍的计算成本优化，仅有小量的精度损失。<details>
<summary>Abstract</summary>
Vision Transformers achieve impressive accuracy across a range of visual recognition tasks. Unfortunately, their accuracy frequently comes with high computational costs. This is a particular issue in video recognition, where models are often applied repeatedly across frames or temporal chunks. In this work, we exploit temporal redundancy between subsequent inputs to reduce the cost of Transformers for video processing. We describe a method for identifying and re-processing only those tokens that have changed significantly over time. Our proposed family of models, Eventful Transformers, can be converted from existing Transformers (often without any re-training) and give adaptive control over the compute cost at runtime. We evaluate our method on large-scale datasets for video object detection (ImageNet VID) and action recognition (EPIC-Kitchens 100). Our approach leads to significant computational savings (on the order of 2-4x) with only minor reductions in accuracy.
</details>
<details>
<summary>摘要</summary>
< Lang="zh-CN" >视力转换器在视觉识别任务中表现出色，但它们的计算成本很高。特别是在视频识别中，模型经常在帧或时间块之间重复应用。在这项工作中，我们利用时间重复性来降低转换器的计算成本。我们描述了一种方法，可以在运行时控制计算成本。我们的提议的家族模型，即事件转换器，可以将现有转换器转换成适应性控制计算成本的模型，而无需再训练。我们在大规模数据集上进行了视频物体检测（ImageNet VID）和动作识别（EPIC-Kitchens 100）的评估，我们的方法可以实现大量的计算成本减少（在2-4倍之间），只有小量的减少准确率。</SYS>Note: The Simplified Chinese translation is done using the Google Translate API, which may not be perfect and may not capture all the nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="Unlocking-the-Performance-of-Proximity-Sensors-by-Utilizing-Transient-Histograms"><a href="#Unlocking-the-Performance-of-Proximity-Sensors-by-Utilizing-Transient-Histograms" class="headerlink" title="Unlocking the Performance of Proximity Sensors by Utilizing Transient Histograms"></a>Unlocking the Performance of Proximity Sensors by Utilizing Transient Histograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13473">http://arxiv.org/abs/2308.13473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carter Sifferman, Yeping Wang, Mohit Gupta, Michael Gleicher</li>
<li>for: 该论文主要用于提高close-range时间探测（ToF）距离传感器所获取的场景几何信息的准确性。</li>
<li>methods: 该论文使用了直接使用传感器捕获的转变 histogram，并使用可导渠 Rendering 管道来直接优化场景几何，以提高对观察结果的匹配。</li>
<li>results: 论文通过对八种不同视角的平面表面进行3,800次测量，并证明了其方法在大多数场景下高效性比Proprietary distance estimate基准值高一个数量级。此外，论文还示出了一种简单的机器人应用，通过使用该方法来感知机器人臂上的把握器上的距离和坡度。<details>
<summary>Abstract</summary>
We provide methods which recover planar scene geometry by utilizing the transient histograms captured by a class of close-range time-of-flight (ToF) distance sensor. A transient histogram is a one dimensional temporal waveform which encodes the arrival time of photons incident on the ToF sensor. Typically, a sensor processes the transient histogram using a proprietary algorithm to produce distance estimates, which are commonly used in several robotics applications. Our methods utilize the transient histogram directly to enable recovery of planar geometry more accurately than is possible using only proprietary distance estimates, and consistent recovery of the albedo of the planar surface, which is not possible with proprietary distance estimates alone. This is accomplished via a differentiable rendering pipeline, which simulates the transient imaging process, allowing direct optimization of scene geometry to match observations. To validate our methods, we capture 3,800 measurements of eight planar surfaces from a wide range of viewpoints, and show that our method outperforms the proprietary-distance-estimate baseline by an order of magnitude in most scenarios. We demonstrate a simple robotics application which uses our method to sense the distance to and slope of a planar surface from a sensor mounted on the end effector of a robot arm.
</details>
<details>
<summary>摘要</summary>
我们提供一种方法，可以利用 close-range time-of-flight (ToF) 距离传感器所捕获的过渡历史gram来恢复平面场景的几何结构。一个过渡历史gram是一个一维时间射频信号，其记录了在 ToF 传感器上 incident 光子的到达时间。通常，一个传感器会使用专有算法来处理过渡历史gram，以生成距离估计，这些估计在多种 робо得应用中被广泛使用。我们的方法直接利用过渡历史gram，以准确地恢复平面几何结构，并同时恢复平面表面的反射率，这两个参数不可能通过专有距离估计alone 获得。我们通过一个可微的渲染管线来实现这一点，该管线模拟了过渡成像过程，允许直接优化场景几何来匹配观测。为验证我们的方法，我们Capture 3,800个平面测量数据，来自多种视点，并显示我们的方法在大多数情况下高效性比专有距离估计baseline 提高一个数量级。我们还展示了一个简单的 робо得应用，使用我们的方法来检测 robot arm 上的末端器的距离和倾斜。
</details></li>
</ul>
<hr>
<h2 id="A-Fast-Minimization-Algorithm-for-the-Euler-Elastica-Model-Based-on-a-Bilinear-Decomposition"><a href="#A-Fast-Minimization-Algorithm-for-the-Euler-Elastica-Model-Based-on-a-Bilinear-Decomposition" class="headerlink" title="A Fast Minimization Algorithm for the Euler Elastica Model Based on a Bilinear Decomposition"></a>A Fast Minimization Algorithm for the Euler Elastica Model Based on a Bilinear Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13471">http://arxiv.org/abs/2308.13471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Liu, Baochen Sun, Xue-Cheng Tai, Qi Wang, Huibin Chang</li>
<li>for: 这个论文的目的是提出一种新的、快速、稳定的 alternating minimization（HALM）算法来解决Euler Elastica（EE）模型中的非线性和缺失问题。</li>
<li>methods: 该算法基于bilinear decomposition of the gradient of the underlying image，并且包括三个子最小化问题，每个问题可以在关闭式或快速解决器中解决。</li>
<li>results: 对比其他当前状态算法，新算法能够更快、更稳定地解决EE模型，并且在一系列数学实验中表现良好。例如，与fast operator-splitting-based Deng-Glowinski-Tai算法相比，新算法的平均运行时间只需一半。<details>
<summary>Abstract</summary>
The Euler Elastica (EE) model with surface curvature can generate artifact-free results compared with the traditional total variation regularization model in image processing. However, strong nonlinearity and singularity due to the curvature term in the EE model pose a great challenge for one to design fast and stable algorithms for the EE model. In this paper, we propose a new, fast, hybrid alternating minimization (HALM) algorithm for the EE model based on a bilinear decomposition of the gradient of the underlying image and prove the global convergence of the minimizing sequence generated by the algorithm under mild conditions. The HALM algorithm comprises three sub-minimization problems and each is either solved in the closed form or approximated by fast solvers making the new algorithm highly accurate and efficient. We also discuss the extension of the HALM strategy to deal with general curvature-based variational models, especially with a Lipschitz smooth functional of the curvature. A host of numerical experiments are conducted to show that the new algorithm produces good results with much-improved efficiency compared to other state-of-the-art algorithms for the EE model. As one of the benchmarks, we show that the average running time of the HALM algorithm is at most one-quarter of that of the fast operator-splitting-based Deng-Glowinski-Tai algorithm.
</details>
<details>
<summary>摘要</summary>
“欧拉-艾拉斯特拉（EE）模型可以实现无残留的结果，与传统的总方差整合模型相比，在图像处理中。然而，EE模型中的曲率项带来强烈的非线性和极值问题，使得设计快速稳定的算法成为一大挑战。在本文中，我们提出了一个新的、快速、混合替换几何（HALM）算法，基于图像的梯度的 bilinear 分解，并证明了混合替换过程中的数列 convergence 的 globally 稳定性。HALM 算法包括三个子替换问题，每个都可以通过关键简单的类型或快速的算法来解决，使得新算法具有高精度和高效性。此外，我们还讨论了对应各种曲率基于的可变量化模型的扩展，特别是一个 Lipschitz 平滑函数的曲率。在实验中，我们展示了新算法在许多测试案例中具有较好的效果，并且比传统的算法更高效。”
</details></li>
</ul>
<hr>
<h2 id="RestNet-Boosting-Cross-Domain-Few-Shot-Segmentation-with-Residual-Transformation-Network"><a href="#RestNet-Boosting-Cross-Domain-Few-Shot-Segmentation-with-Residual-Transformation-Network" class="headerlink" title="RestNet: Boosting Cross-Domain Few-Shot Segmentation with Residual Transformation Network"></a>RestNet: Boosting Cross-Domain Few-Shot Segmentation with Residual Transformation Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13469">http://arxiv.org/abs/2308.13469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyang Huang, Chuang Zhu, Wenkai Chen</li>
<li>for: 本文目的是提出一种新的交叉频谱几何学习模型，以实现在未知频谱上进行 semantic segmentation，并且可以在有限的注释样本基础上进行学习。</li>
<li>methods: 本文提出了一种名为RestNet的几何学习模型，该模型通过 Semantic Enhanced Anchor Transform (SEAT) 模块和 Intra-domain Residual Enhancement (IRE) 模块来实现知识传递，同时保持了内域支持查询特征信息。此外，本文还提出了一种基于 prototype fusion 的面 predicate 策略，帮助模型慢慢地学习如何分割。</li>
<li>results: 实验表明，RestNet 可以在 ISIC、Chest X-ray 和 FSS-1000 等 dataset 上 achieve state-of-the-art 性能，并且不需要额外的 fine-tuning。<details>
<summary>Abstract</summary>
Cross-domain few-shot segmentation (CD-FSS) aims to achieve semantic segmentation in previously unseen domains with a limited number of annotated samples. Although existing CD-FSS models focus on cross-domain feature transformation, relying exclusively on inter-domain knowledge transfer may lead to the loss of critical intra-domain information. To this end, we propose a novel residual transformation network (RestNet) that facilitates knowledge transfer while retaining the intra-domain support-query feature information. Specifically, we propose a Semantic Enhanced Anchor Transform (SEAT) module that maps features to a stable domain-agnostic space using advanced semantics. Additionally, an Intra-domain Residual Enhancement (IRE) module is designed to maintain the intra-domain representation of the original discriminant space in the new space. We also propose a mask prediction strategy based on prototype fusion to help the model gradually learn how to segment. Our RestNet can transfer cross-domain knowledge from both inter-domain and intra-domain without requiring additional fine-tuning. Extensive experiments on ISIC, Chest X-ray, and FSS-1000 show that our RestNet achieves state-of-the-art performance. Our code will be available soon.
</details>
<details>
<summary>摘要</summary>
specifically，我们提出了一种 Semantic Enhanced Anchor Transform (SEAT) 模块，它可以将特征映射到一个稳定的领域不依赖的空间中使用先进的 semantics。此外，我们还提出了一种 Intra-domain Residual Enhancement (IRE) 模块，它可以保持原始领域的 intra-domain 表示。我们还提出了一种 mask prediction strategy based on prototype fusion，帮助模型慢慢地学习如何分割。我们的 RestNet 可以从 both inter-domain 和 intra-domain 中传递知识，而不需要额外的 fine-tuning。我们进行了广泛的实验，结果表明我们的 RestNet 可以达到 state-of-the-art 性能。我们的代码很快就会发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/26/cs.CV_2023_08_26/" data-id="cloojsmf200h4re88boz8epd4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/26/cs.AI_2023_08_26/" class="article-date">
  <time datetime="2023-08-26T12:00:00.000Z" itemprop="datePublished">2023-08-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/26/cs.AI_2023_08_26/">cs.AI - 2023-08-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ChatGPT-as-Data-Augmentation-for-Compositional-Generalization-A-Case-Study-in-Open-Intent-Detection"><a href="#ChatGPT-as-Data-Augmentation-for-Compositional-Generalization-A-Case-Study-in-Open-Intent-Detection" class="headerlink" title="ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection"></a>ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13517">http://arxiv.org/abs/2308.13517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihao Fang, Xianzhi Li, Stephen W. Thomas, Xiaodan Zhu</li>
<li>for: 增强自然语言理解任务中的拓展性 generale</li>
<li>methods: 使用ChatGPT作为数据增强技术，提高开放意图检测任务中的组合泛化能力</li>
<li>results: 对多个benchmark进行严格评估，发现我们的方法可以明显提高模型性能，并且在开放意图检测任务中具有显著的提升效果。<details>
<summary>Abstract</summary>
Open intent detection, a crucial aspect of natural language understanding, involves the identification of previously unseen intents in user-generated text. Despite the progress made in this field, challenges persist in handling new combinations of language components, which is essential for compositional generalization. In this paper, we present a case study exploring the use of ChatGPT as a data augmentation technique to enhance compositional generalization in open intent detection tasks. We begin by discussing the limitations of existing benchmarks in evaluating this problem, highlighting the need for constructing datasets for addressing compositional generalization in open intent detection tasks. By incorporating synthetic data generated by ChatGPT into the training process, we demonstrate that our approach can effectively improve model performance. Rigorous evaluation of multiple benchmarks reveals that our method outperforms existing techniques and significantly enhances open intent detection capabilities. Our findings underscore the potential of large language models like ChatGPT for data augmentation in natural language understanding tasks.
</details>
<details>
<summary>摘要</summary>
开放意图检测是自然语言理解的重要方面，涉及到用户生成文本中未经见的意图的识别。Despite the progress made in this field, there are still challenges in handling new combinations of language components, which is crucial for compositional generalization. In this paper, we present a case study exploring the use of ChatGPT as a data augmentation technique to enhance compositional generalization in open intent detection tasks.我们开始 by discussing the limitations of existing benchmarks in evaluating this problem, highlighting the need for constructing datasets for addressing compositional generalization in open intent detection tasks. By incorporating synthetic data generated by ChatGPT into the training process, we demonstrate that our approach can effectively improve model performance. Rigorous evaluation of multiple benchmarks reveals that our method outperforms existing techniques and significantly enhances open intent detection capabilities. Our findings underscore the potential of large language models like ChatGPT for data augmentation in natural language understanding tasks.Here's the text with some minor adjustments to make it more readable in Simplified Chinese:开放意图检测是自然语言理解的重要方面，涉及到用户生成文本中未经见的意图的识别。尽管在这个领域已经做出了很多进步，但是处理新的语言组成部分的挑战仍然存在，这是重要的 Compositional generalization。在这篇论文中，我们进行了一个案例研究，探讨使用 ChatGPT 作为数据增强技术来提高开放意图检测任务中的 Compositional generalization。我们开始 by 讨论现有的 benchmar 的限制，高亮需要为开放意图检测任务构建数据集来解决 Compositional generalization 问题。通过在训练过程中添加 ChatGPT 生成的Synthetic数据，我们示出了我们的方法可以有效提高模型性能。多个 benchmar 的严格评估表明，我们的方法超过了现有的方法，并有效地提高了开放意图检测能力。我们的发现强调了大语言模型 like ChatGPT 的潜在作用在自然语言理解任务中。
</details></li>
</ul>
<hr>
<h2 id="Does-Asking-Clarifying-Questions-Increases-Confidence-in-Generated-Code-On-the-Communication-Skills-of-Large-Language-Models"><a href="#Does-Asking-Clarifying-Questions-Increases-Confidence-in-Generated-Code-On-the-Communication-Skills-of-Large-Language-Models" class="headerlink" title="Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models"></a>Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13507">http://arxiv.org/abs/2308.13507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie JW Wu</li>
<li>for: 提高大型自然语言模型（LLM）在代码生成任务中的能力</li>
<li>methods: 使用LLM生成器寻找高不确定性和低信任性问题，并向用户提问以获取反馈</li>
<li>results: 通过提高沟通技巧，提高代码生成器对代码质量的信任度<details>
<summary>Abstract</summary>
Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, we explore how to leverage better communication skills to achieve greater confidence in generated code. We propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low confidence in problem descriptions and generated code. We then ask clarifying questions to obtain responses from users for refining the code.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLM）已经对代码生成任务做出了重要改进，但是仍然存在LLM是出色的程序员和首席软件工程师之间的差距。根据观察到的首席软件工程师经常对需求和解决方案中的模糊性问题提出询问，我们认为这样的方法也应被应用到LLM的代码生成任务中。通过在生成代码之前向用户提出询问，可以帮助解决LLM在代码生成中的挑战，例如不清晰的意图规定、Computational Thinking的缺乏和不满意的代码质量。这样可以增加代码的信任度。在这个工作中，我们探索如何通过更好的沟通技巧来实现更高的代码信任度。我们提出了一个沟通中心的过程，使用LLM生成的通信器来识别问题中的高模糊性或低信任性，然后对用户提出询问以获取反馈。
</details></li>
</ul>
<hr>
<h2 id="Attending-Generalizability-in-Course-of-Deep-Fake-Detection-by-Exploring-Multi-task-Learning"><a href="#Attending-Generalizability-in-Course-of-Deep-Fake-Detection-by-Exploring-Multi-task-Learning" class="headerlink" title="Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task Learning"></a>Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13503">http://arxiv.org/abs/2308.13503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Balaji, Abhijit Das, Srijan Das, Antitza Dantcheva</li>
<li>for: 本研究探讨了多种多任务学习（MTL）技术，用于分类视频为原始或修改的混合修改场景，以提高深入Counterfeit场景中的泛化性。</li>
<li>methods: 我们使用了FaceForensics++ dataset，该 dataset包含1000个原始视频和4种不同的修改技术修改后的5000个视频。我们进行了广泛的多任务学习和对比技术的实验，这些技术在文献中已经得到了广泛的探讨。</li>
<li>results: 结果表明，我们提出的检测模型具有良好的泛化性，能够正确地检测不同修改方法的视频，比对state-of-the-art更高效。<details>
<summary>Abstract</summary>
This work explores various ways of exploring multi-task learning (MTL) techniques aimed at classifying videos as original or manipulated in cross-manipulation scenario to attend generalizability in deep fake scenario. The dataset used in our evaluation is FaceForensics++, which features 1000 original videos manipulated by four different techniques, with a total of 5000 videos. We conduct extensive experiments on multi-task learning and contrastive techniques, which are well studied in literature for their generalization benefits. It can be concluded that the proposed detection model is quite generalized, i.e., accurately detects manipulation methods not encountered during training as compared to the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
这项工作探讨了多种多任务学习（MTL）技术，用于分类视频为原始或修改的混合 manipulate enario，以提高深度假象场景中的泛化性。我们使用的数据集是 FaceForensics++, 该数据集包含 1000 个原始视频，被四种不同的技术修改，总共有 5000 个视频。我们进行了广泛的多任务学习和对比技术实验，这些技术在文献中已经得到了广泛的研究和证明了其泛化效果。可以结论，我们提出的检测模型具有良好的泛化性，即在训练中未遇到的修改方法上具有高度的检测精度，比之前的状态艺术。
</details></li>
</ul>
<hr>
<h2 id="Escaping-the-Sample-Trap-Fast-and-Accurate-Epistemic-Uncertainty-Estimation-with-Pairwise-Distance-Estimators"><a href="#Escaping-the-Sample-Trap-Fast-and-Accurate-Epistemic-Uncertainty-Estimation-with-Pairwise-Distance-Estimators" class="headerlink" title="Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators"></a>Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13498">http://arxiv.org/abs/2308.13498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Berry, David Meger</li>
<li>for: 本研究提出了一种新的方法来估计 ensemble 模型中的 epistemic uncertainty，使用 pairwise-distance estimators (PaiDEs)。</li>
<li>methods: PaiDEs 利用模型组件之间的对比距离来确定 entropy 的下界，并将这些下界作为信息基据 критериion 的估计。与现代深度学习方法不同，PaiDEs 可以在更大的空间（最多 100 $\times$）和更高的维度（最多 100 $\times$）上更快（大约 100 $\times$）和更准确地估计 epistemic uncertainty。</li>
<li>results: 通过一系列常用来评估 epistemic uncertainty 估计的实验（1D 杆形数据、Pendulum-v0、Hopper-v2、Ant-v2 和 Humanoid-v2），我们证明了 PaiDEs 在 epistemic uncertainty 估计中的优势。在每个实验 Setting 中，我们采用了 Active Learning 框架来展示 PaiDEs 的优势。<details>
<summary>Abstract</summary>
This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种新的方法来估计 ensemble 模型中的认知不确定性使用对比距离估计器（PaiDEs）。这些估计器利用对比距离来确定模型组件之间的 entropy  bound，并将这些 bound 用作信息基来的估计 criterion。与最近的深度学习方法不同，PaiDEs 可以在更大的空间（最多 100 倍）和更高维度（最多 100 倍）上更快（up to 100 倍）和更准确地估计认知不确定性。为验证我们的方法，我们进行了一系列通常用于评估认知不确定性估计的实验：1D 振荡数据、Pendulum-v0、Hopper-v2、Ant-v2 和 Humanoid-v2。对每个实验设置，我们应用了活动学习框架来展示 PaiDEs 在认知不确定性估计中的优势。
</details></li>
</ul>
<hr>
<h2 id="Open-Gaze-An-Open-Source-Implementation-Replicating-Google’s-Eye-Tracking-Paper"><a href="#Open-Gaze-An-Open-Source-Implementation-Replicating-Google’s-Eye-Tracking-Paper" class="headerlink" title="Open Gaze: An Open-Source Implementation Replicating Google’s Eye Tracking Paper"></a>Open Gaze: An Open-Source Implementation Replicating Google’s Eye Tracking Paper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13495">http://arxiv.org/abs/2308.13495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sushmanth reddy Mereddy, Jyothi Swaroop Reddy, Somnath Sharma<br>for:This paper aims to develop an open-source implementation of a smartphone-based gaze tracker that can accurately track eye movements without the need for specialized hardware.methods:The authors use machine learning techniques to develop an eye tracking solution that is native to smartphones, and they validate their approach using the MIT GazeCapture dataset.results:The authors demonstrate that their approach can accurately track eye movements during natural image observation and reading comprehension tasks, and they show that their smartphone-based gaze tracker is comparable in accuracy to state-of-the-art mobile eye trackers that are two orders of magnitude more expensive.<details>
<summary>Abstract</summary>
Eye tracking has been a pivotal tool in diverse fields such as vision research, language analysis, and usability assessment. The majority of prior investigations, however, have concentrated on expansive desktop displays employing specialized, costly eye tracking hardware that lacks scalability. Remarkably little insight exists into ocular movement patterns on smartphones, despite their widespread adoption and significant usage. In this manuscript, we present an open-source implementation of a smartphone-based gaze tracker that emulates the methodology proposed by a GooglePaper (whose source code remains proprietary). Our focus is on attaining accuracy comparable to that attained through the GooglePaper's methodology, without the necessity for supplementary hardware. Through the integration of machine learning techniques, we unveil an accurate eye tracking solution that is native to smartphones. Our approach demonstrates precision akin to the state-of-the-art mobile eye trackers, which are characterized by a cost that is two orders of magnitude higher. Leveraging the vast MIT GazeCapture dataset, which is available through registration on the dataset's website, we successfully replicate crucial findings from previous studies concerning ocular motion behavior in oculomotor tasks and saliency analyses during natural image observation. Furthermore, we emphasize the applicability of smartphone-based gaze tracking in discerning reading comprehension challenges. Our findings exhibit the inherent potential to amplify eye movement research by significant proportions, accommodating participation from thousands of subjects with explicit consent. This scalability not only fosters advancements in vision research, but also extends its benefits to domains such as accessibility enhancement and healthcare applications.
</details>
<details>
<summary>摘要</summary>
眼动跟踪技术已经在多个领域得到广泛应用，如视觉研究、语言分析和用户体验评估。然而，大多数前期研究都集中在使用特殊、昂贵的桌面显示器上进行眼动跟踪，lacking scalability。尚未得到充分的研究对于智能手机上的眼动跟踪，尽管智能手机的普及和使用率很高。在这篇文章中，我们提供了一个开源实现的智能手机基于眼动跟踪器，基于Google文献（其源代码尚未公开）的方法论。我们的注重点在于实现与Google文献的方法论相同的准确性，不需要额外的硬件。通过机器学习技术的 интеграción，我们提出了一种Native to smartphones的眼动跟踪解决方案。我们的方法与状态 искусственный智能手机眼动跟踪器相比，具有更高的准确性和可扩展性。基于MIT GazeCapture数据集，我们成功复制了先前研究中关于眼动行为在视觉任务和自然图像观看中的关键发现。此外，我们强调了智能手机基于眼动跟踪在了解阅读挑战中的应用。我们的发现表明了智能手机基于眼动跟踪的潜在潜力，可以提高眼动研究的进步，并扩展到访问ibilty enhancement和医疗应用领域。
</details></li>
</ul>
<hr>
<h2 id="Ultrafast-and-Ultralight-ConvNet-Based-Intelligent-Monitoring-System-for-Diagnosing-Early-Stage-Mpox-Anytime-and-Anywhere"><a href="#Ultrafast-and-Ultralight-ConvNet-Based-Intelligent-Monitoring-System-for-Diagnosing-Early-Stage-Mpox-Anytime-and-Anywhere" class="headerlink" title="Ultrafast-and-Ultralight ConvNet-Based Intelligent Monitoring System for Diagnosing Early-Stage Mpox Anytime and Anywhere"></a>Ultrafast-and-Ultralight ConvNet-Based Intelligent Monitoring System for Diagnosing Early-Stage Mpox Anytime and Anywhere</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13492">http://arxiv.org/abs/2308.13492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubiao Yue, Xiaoqiang Shi, Li Qin, Xinyue Zhang, Yanmei Chen, Jialong Xu, Zipei Zheng, Yujun Cao, Di Liu, Zhenzhang Li, Yang Li<br>for:The paper aims to develop a real-time diagnostic tool for monkeypox, addressing the lack of efficient diagnostic tools and the challenges of high inference speed, large parameter size, and limited diagnosis performance for early-stage monkeypox.methods:The proposed method, Fast-MpoxNet, is an ultrafast and ultralight deep learning network that integrates attention-based feature fusion and multiple auxiliary losses enhancement. It uses transfer learning and five-fold cross-validation, achieving 94.26% Accuracy on the Mpox dataset with a recall of 93.65% for early-stage monkeypox.results:Fast-MpoxNet achieves high accuracy and practicality in real-time diagnosis, with an Accuracy of 98.40% and a Practicality Score of 0.80 when adopting data augmentation. An application system named Mpox-AISM V2 was also developed for both personal computers and mobile phones, featuring ultrafast responses, offline functionality, and easy deployment. The proposed method has the potential to mitigate future monkeypox outbreaks and provide a new paradigm for developing real-time diagnostic tools in the healthcare field.<details>
<summary>Abstract</summary>
Due to the lack of more efficient diagnostic tools for monkeypox, its spread remains unchecked, presenting a formidable challenge to global health. While the high efficacy of deep learning models for monkeypox diagnosis has been demonstrated in related studies, the overlook of inference speed, the parameter size and diagnosis performance for early-stage monkeypox renders the models inapplicable in real-world settings. To address these challenges, we proposed an ultrafast and ultralight network named Fast-MpoxNet. Fast-MpoxNet possesses only 0.27M parameters and can process input images at 68 frames per second (FPS) on the CPU. To counteract the diagnostic performance limitation brought about by the small model capacity, it integrates the attention-based feature fusion module and the multiple auxiliary losses enhancement strategy for better detecting subtle image changes and optimizing weights. Using transfer learning and five-fold cross-validation, Fast-MpoxNet achieves 94.26% Accuracy on the Mpox dataset. Notably, its recall for early-stage monkeypox achieves 93.65%. By adopting data augmentation, our model's Accuracy rises to 98.40% and attains a Practicality Score (A new metric for measuring model practicality in real-time diagnosis application) of 0.80. We also developed an application system named Mpox-AISM V2 for both personal computers and mobile phones. Mpox-AISM V2 features ultrafast responses, offline functionality, and easy deployment, enabling accurate and real-time diagnosis for both the public and individuals in various real-world settings, especially in populous settings during the outbreak. Our work could potentially mitigate future monkeypox outbreak and illuminate a fresh paradigm for developing real-time diagnostic tools in the healthcare field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Optimal-Head-to-head-Autonomous-Racing-with-Curriculum-Reinforcement-Learning"><a href="#Towards-Optimal-Head-to-head-Autonomous-Racing-with-Curriculum-Reinforcement-Learning" class="headerlink" title="Towards Optimal Head-to-head Autonomous Racing with Curriculum Reinforcement Learning"></a>Towards Optimal Head-to-head Autonomous Racing with Curriculum Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13491">http://arxiv.org/abs/2308.13491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dvij Kalaria, Qin Lin, John M. Dolan</li>
<li>for: 本研究旨在提出一个头阵自动赛车环境，以便使用循环学习学习出最佳政策。</li>
<li>methods: 本研究使用了curriculum learning和安全循环学习算法，从 simpler vehicle model 逐渐转移到更加复杂的real environment，以教导循环学习代理人一个更加优化的政策。</li>
<li>results: 本研究的结果显示，使用curriculum learning和安全循环学习算法可以更加有效地将循环学习代理人训练到更加优化的政策，并且能够更加安全地进行训练。<details>
<summary>Abstract</summary>
Head-to-head autonomous racing is a challenging problem, as the vehicle needs to operate at the friction or handling limits in order to achieve minimum lap times while also actively looking for strategies to overtake/stay ahead of the opponent. In this work we propose a head-to-head racing environment for reinforcement learning which accurately models vehicle dynamics. Some previous works have tried learning a policy directly in the complex vehicle dynamics environment but have failed to learn an optimal policy. In this work, we propose a curriculum learning-based framework by transitioning from a simpler vehicle model to a more complex real environment to teach the reinforcement learning agent a policy closer to the optimal policy. We also propose a control barrier function-based safe reinforcement learning algorithm to enforce the safety of the agent in a more effective way while not compromising on optimality.
</details>
<details>
<summary>摘要</summary>
HEAD-TO-HEAD自动赛车是一个复杂的问题，车辆需要在摩擦或控制限制下运行，以实现最低圈速并同时积极寻找超越或保持领先的策略。在这项工作中，我们提出了一个真实精度模型车辆动力学环境的HEAD-TO-HEAD赛车环境。一些前一次的工作已经尝试直接在复杂的车辆动力学环境中学习策略，但未能学习优化策略。在这项工作中，我们提出了一种学习纲程学习框架，从一个更加简单的车辆模型转移到更加复杂的真实环境，以教育学习代理更近似于优化策略。我们还提出了一种基于控制障碍函数的安全学习算法，以更有效地保证代理的安全性，不会影响优化性。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Uncertainty-Localization-to-Enable-Human-in-the-loop-Analysis-of-Dynamic-Contrast-enhanced-Cardiac-MRI-Datasets"><a href="#Temporal-Uncertainty-Localization-to-Enable-Human-in-the-loop-Analysis-of-Dynamic-Contrast-enhanced-Cardiac-MRI-Datasets" class="headerlink" title="Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets"></a>Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13488">http://arxiv.org/abs/2308.13488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dilek M. Yalcinkaya, Khalid Youssef, Bobak Heydari, Orlando Simonetti, Rohan Dharmakumar, Subha Raman, Behzad Sharif</li>
<li>for: 这个论文的目的是提出一种基于深度神经网络的动态质量控制（dQC）工具，用于识别DCE-CMRI数据集分割失败的情况。</li>
<li>methods: 这个论文使用的方法包括DCE-CMRI数据分割、深度神经网络分割和人工征化约束。</li>
<li>results: 研究发现，使用提出的dQC工具可以准确地识别分割失败的情况，并且可以提高分割结果的准确率和减少分割失败的数量。<details>
<summary>Abstract</summary>
Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is a widely used modality for diagnosing myocardial blood flow (perfusion) abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300 time-resolved images of myocardial perfusion are acquired at various contrast "wash in/out" phases. Manual segmentation of myocardial contours in each time-frame of a DCE image series can be tedious and time-consuming, particularly when non-rigid motion correction has failed or is unavailable. While deep neural networks (DNNs) have shown promise for analyzing DCE-CMRI datasets, a "dynamic quality control" (dQC) technique for reliably detecting failed segmentations is lacking. Here we propose a new space-time uncertainty metric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRI datasets by validating the proposed metric on an external dataset and establishing a human-in-the-loop framework to improve the segmentation results. In the proposed approach, we referred the top 10% most uncertain segmentations as detected by our dQC tool to the human expert for refinement. This approach resulted in a significant increase in the Dice score (p<0.001) and a notable decrease in the number of images with failed segmentation (16.2% to 11.3%) whereas the alternative approach of randomly selecting the same number of segmentations for human referral did not achieve any significant improvement. Our results suggest that the proposed dQC framework has the potential to accurately identify poor-quality segmentations and may enable efficient DNN-based analysis of DCE-CMRI in a human-in-the-loop pipeline for clinical interpretation and reporting of dynamic CMRI datasets.
</details>
<details>
<summary>摘要</summary>
对 Dynamic Contrast-Enhanced (DCE) Cardiac Magnetic Resonance Imaging (CMRI) 诊断Myocardial Blood Flow (Perfusion) 异常，通常需要获取约 300 个时间分解的Myocardial Perfusion 影像，并在不同的对比“洗入/洗出”阶段进行评估。然而，手动分类Myocardial 边析在每个时间点的DCE影像系列可以是时间consuming 和耗时consuming，尤其是当非静态运动调整失败或无法使用时。深度神经网 (DNNs) 已经显示出了分析 DCE-CMRI 数据的潜力，但是一个“动态品质控制” (dQC) 技术来可靠地检测失败的分类是缺乏的。我们提出了一个新的空间时间不确定度量来作为 dQC 工具，并在一个人际loop 框架中进行改进。在我们的方法中，我们将top 10% 最不确定的分类作为 dQC 工具所检测的，并请求人工专家进行重新分类。这种方法导致了 Dice 分数的增加（p < 0.001）和失败分类的数量的下降（16.2% 到 11.3%），而对照方法，将相同数量的分类 randomly 选择进行人工参考，无法获得任何有意义的改善。我们的结果表明，我们的 dQC 框架具有可靠地检测失败分类的潜力，并且可以实现人际loop pipeline中的有效 DNN-based 分析 DCE-CMRI 数据，以便在诊断和报告动态 CMRI 数据时提供有用的资讯。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Knowledge-and-Reinforcement-Learning-for-Enhanced-Reliability-of-Language-Models"><a href="#Leveraging-Knowledge-and-Reinforcement-Learning-for-Enhanced-Reliability-of-Language-Models" class="headerlink" title="Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models"></a>Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13467">http://arxiv.org/abs/2308.13467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nancy Tyagi, Surjodeep Sarkar, Manas Gaur</li>
<li>for: 这个论文是为了提高现代语言模型（如BERT）的可靠性和准确性而写的。</li>
<li>methods: 这个论文使用了人工智能的集成学习方法，利用了ConceptNet和Wikipedia的知识图谱嵌入，以强化语言模型的可靠性和准确性。</li>
<li>results: 研究表明，使用这种知识导向集成学习方法可以提高语言模型的可靠性和准确性，在九个GLUE任务上都有出色的表现，超越了现有的最佳实现。<details>
<summary>Abstract</summary>
The Natural Language Processing(NLP) community has been using crowd sourcing techniques to create benchmark datasets such as General Language Understanding and Evaluation(GLUE) for training modern Language Models such as BERT. GLUE tasks measure the reliability scores using inter annotator metrics i.e. Cohens Kappa. However, the reliability aspect of LMs has often been overlooked. To counter this problem, we explore a knowledge-guided LM ensembling approach that leverages reinforcement learning to integrate knowledge from ConceptNet and Wikipedia as knowledge graph embeddings. This approach mimics human annotators resorting to external knowledge to compensate for information deficits in the datasets. Across nine GLUE datasets, our research shows that ensembling strengthens reliability and accuracy scores, outperforming state of the art.
</details>
<details>
<summary>摘要</summary>
natural language processing（NLP）社区已经使用人群SOURCING技术创建了通用语言理解和评估（GLUE）测试集，用于训练现代语言模型（BERT）。GLUE任务测试语言模型的可靠性使用互annotator metric，即科恩斯均度。然而，语言模型的可靠性问题经常被忽略。为解决这个问题，我们研究了一种基于知识图谱Embedding的知识导向语言模型集成方法。这种方法模拟人工注解者通过外部知识补做信息缺乏的数据集。在九个GLUE任务上，我们的研究表明，集成可以提高可靠性和准确率，超过当前最佳。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/26/cs.AI_2023_08_26/" data-id="cloojsmal002vre884vc60lcr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/26/cs.CL_2023_08_26/" class="article-date">
  <time datetime="2023-08-26T11:00:00.000Z" itemprop="datePublished">2023-08-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/26/cs.CL_2023_08_26/">cs.CL - 2023-08-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Training-and-Meta-Evaluating-Machine-Translation-Evaluation-Metrics-at-the-Paragraph-Level"><a href="#Training-and-Meta-Evaluating-Machine-Translation-Evaluation-Metrics-at-the-Paragraph-Level" class="headerlink" title="Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level"></a>Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13506">http://arxiv.org/abs/2308.13506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Deutsch, Juraj Juraska, Mara Finkelstein, and Markus Freitag</li>
<li>for: 这个研究的目的是为了评估自动译文件翻译文本的效果，并不是只是评估单句翻译。</li>
<li>methods: 这篇论文提出了一种方法，即使用现有的句子级评估数据来创建段落级数据，并将这些数据用于训练和meta-评估评价指标。</li>
<li>results: 实验结果表明，使用句子级评估指标来评估整个段落的效果与使用专门为段落级设计的指标相当有效。<details>
<summary>Abstract</summary>
As research on machine translation moves to translating text beyond the sentence level, it remains unclear how effective automatic evaluation metrics are at scoring longer translations. In this work, we first propose a method for creating paragraph-level data for training and meta-evaluating metrics from existing sentence-level data. Then, we use these new datasets to benchmark existing sentence-level metrics as well as train learned metrics at the paragraph level. Interestingly, our experimental results demonstrate that using sentence-level metrics to score entire paragraphs is equally as effective as using a metric designed to work at the paragraph level. We speculate this result can be attributed to properties of the task of reference-based evaluation as well as limitations of our datasets with respect to capturing all types of phenomena that occur in paragraph-level translations.
</details>
<details>
<summary>摘要</summary>
研究在机器翻译中延伸到文段级别的文本翻译效果，然而现有自动评价指标的有效性在长文段翻译方面仍然不清楚。在这项工作中，我们首先提出了将现有句子级数据转化为训练和Meta评价指标的方法。然后，我们使用这些新的数据集来评估现有的句子级指标以及在 paragraph 级别进行学习的指标。实验结果显示，使用句子级指标评估整个文段的效果与使用特制的 paragraph 级指标相当。我们推测这些结果可能是由评估任务的特性以及我们数据集中捕捉到的所有类型的现象所致。
</details></li>
</ul>
<hr>
<h2 id="Ngambay-French-Neural-Machine-Translation-sba-Fr"><a href="#Ngambay-French-Neural-Machine-Translation-sba-Fr" class="headerlink" title="Ngambay-French Neural Machine Translation (sba-Fr)"></a>Ngambay-French Neural Machine Translation (sba-Fr)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13497">http://arxiv.org/abs/2308.13497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Toadoum/Ngambay-French-Neural-Machine-Translation-sba_fr_v1-">https://github.com/Toadoum/Ngambay-French-Neural-Machine-Translation-sba_fr_v1-</a></li>
<li>paper_authors: Sakayo Toadoum Sari, Angela Fan, Lema Logamou Seknewna</li>
<li>for: 本研究旨在开发一个基于神经机器翻译（NMT）系统，以推动语言障碍的缓解。特别是在语言资源匮乏的情况下，NMT 系统的研发成为了一项感兴趣的话题。</li>
<li>methods: 本研究使用了三个预训练模型进行了微调，并使用了一个自适应的数据采集方法来生成训练数据。</li>
<li>results: 实验结果显示，使用 M2M100 模型可以在原始数据和原始+ sintetic 数据上达到高的 BLEU 分数。此外，公共可用的 bitext 数据集可以用于研究用途。<details>
<summary>Abstract</summary>
In Africa, and the world at large, there is an increasing focus on developing Neural Machine Translation (NMT) systems to overcome language barriers. NMT for Low-resource language is particularly compelling as it involves learning with limited labelled data. However, obtaining a well-aligned parallel corpus for low-resource languages can be challenging. The disparity between the technological advancement of a few global languages and the lack of research on NMT for local languages in Chad is striking. End-to-end NMT trials on low-resource Chad languages have not been attempted. Additionally, there is a dearth of online and well-structured data gathering for research in Natural Language Processing, unlike some African languages. However, a guided approach for data gathering can produce bitext data for many Chadian language translation pairs with well-known languages that have ample data. In this project, we created the first sba-Fr Dataset, which is a corpus of Ngambay-to-French translations, and fine-tuned three pre-trained models using this dataset. Our experiments show that the M2M100 model outperforms other models with high BLEU scores on both original and original+synthetic data. The publicly available bitext dataset can be used for research purposes.
</details>
<details>
<summary>摘要</summary>
在非洲和世界上，有越来越多的关注发展神经机器翻译（NMT）系统，以超越语言障碍。NMT для低资源语言特别有吸引力，因为它涉及到有限的标注数据学习。然而，获得低资源语言的高质量并行数据可以是挑战。非洲语言技术发展落差和中非的语言研究欠缺是惊人的。在毫不计入End-to-end NMT实验中，有些非洲语言的翻译还没有尝试。此外，在自然语言处理领域的在线和结构化数据收集也比较缺乏，与一些非洲语言不同。然而，一种引导的方法可以生成许多中非语言翻译对的数据，并且可以使用已知语言的丰富数据进行精度调整。在本项目中，我们创建了第一个sba-FrDataset，它是一个 Ngambay-to-French 翻译 corpus，并使用这个数据集进行三个预训练模型的精度调整。我们的实验表明，M2M100模型在原始和原始+ sintetic 数据上的 BLEU 分数均高于其他模型。公共可用的 bitext 数据集可以用于研究purposes。
</details></li>
</ul>
<hr>
<h2 id="Prompting-a-Large-Language-Model-to-Generate-Diverse-Motivational-Messages-A-Comparison-with-Human-Written-Messages"><a href="#Prompting-a-Large-Language-Model-to-Generate-Diverse-Motivational-Messages-A-Comparison-with-Human-Written-Messages" class="headerlink" title="Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages"></a>Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13479">http://arxiv.org/abs/2308.13479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Rhys Cox, Ashraf Abdul, Wei Tsang Ooi</li>
<li>for: 这个论文旨在探讨大语言模型（LLM）可以如何用于创作内容，以及使用特定指令可以提高LLM的创作质量。</li>
<li>methods: 该论文使用了一个以前的人群劳动任务管道，用于生成一个多样化的motivational message corpus。然后，该管道被用来生成消息，并与人群写作者和两个基线GPT-4提示进行比较。</li>
<li>results: 研究发现，使用人群劳动任务管道作为LLM提示可以使GPT-4生成更多样化的消息，比两个基线提示更好。此外，论文还讨论了由人类写作者和LLM生成的消息之间的对比。<details>
<summary>Abstract</summary>
Large language models (LLMs) are increasingly capable and prevalent, and can be used to produce creative content. The quality of content is influenced by the prompt used, with more specific prompts that incorporate examples generally producing better results. On from this, it could be seen that using instructions written for crowdsourcing tasks (that are specific and include examples to guide workers) could prove effective LLM prompts. To explore this, we used a previous crowdsourcing pipeline that gave examples to people to help them generate a collectively diverse corpus of motivational messages. We then used this same pipeline to generate messages using GPT-4, and compared the collective diversity of messages from: (1) crowd-writers, (2) GPT-4 using the pipeline, and (3 & 4) two baseline GPT-4 prompts. We found that the LLM prompts using the crowdsourcing pipeline caused GPT-4 to produce more diverse messages than the two baseline prompts. We also discuss implications from messages generated by both human writers and LLMs.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".Translation notes:* "Large language models" is translated as "大型语言模型" (dàxíng yǔyán módelǐng).* "Crowdsourcing" is translated as "人群协作" (rénqún xiézuò).* "Pipeline" is translated as "管道" (guǎndào).* "GPT-4" is translated as "GPT-4" (GPT-4).* "Baseline" is translated as "基线" (jīxiàn).* "Prompts" is translated as "提示" (tímǐ).* "Diverse" is translated as "多样的" (duōyàng de).* "Messages" is translated as "消息" (xiāoxi).Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="ARTIST-ARTificial-Intelligence-for-Simplified-Text"><a href="#ARTIST-ARTificial-Intelligence-for-Simplified-Text" class="headerlink" title="ARTIST: ARTificial Intelligence for Simplified Text"></a>ARTIST: ARTificial Intelligence for Simplified Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13458">http://arxiv.org/abs/2308.13458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/delftcrowd/artist">https://github.com/delftcrowd/artist</a></li>
<li>paper_authors: Lorenzo Corti, Jie Yang</li>
<li>for: 本研究旨在探讨自然语言处理中的文本简化问题，以提高公共信息和知识的接入性。</li>
<li>methods: 本研究使用了最新的生成人工智能技术，包括语言模型、领域和读者适应和可见化模块，以自动简化文本的语言和结构层次。</li>
<li>results: 研究发现了自动文本简化技术的优势和局限性，包括处理文化和常识知识的挑战。这些结果代表了对荷兰文本简化的首次探索，并为未来的研究和实践提供了灯光。<details>
<summary>Abstract</summary>
Complex text is a major barrier for many citizens when accessing public information and knowledge. While often done manually, Text Simplification is a key Natural Language Processing task that aims for reducing the linguistic complexity of a text while preserving the original meaning. Recent advances in Generative Artificial Intelligence (AI) have enabled automatic text simplification both on the lexical and syntactical levels. However, as applications often focus on English, little is understood about the effectiveness of Generative AI techniques on low-resource languages such as Dutch. For this reason, we carry out empirical studies to understand the benefits and limitations of applying generative technologies for text simplification and provide the following outcomes: 1) the design and implementation for a configurable text simplification pipeline that orchestrates state-of-the-art generative text simplification models, domain and reader adaptation, and visualisation modules; 2) insights and lessons learned, showing the strengths of automatic text simplification while exposing the challenges in handling cultural and commonsense knowledge. These outcomes represent a first step in the exploration of Dutch text simplification and shed light on future endeavours both for research and practice.
</details>
<details>
<summary>摘要</summary>
各种复杂的文本是公共信息和知识访问的主要障碍。虽然经常是手动完成的，但文本简化是自然语言处理任务的关键任务，旨在降低文本语言复杂性，保留原始意思。现代生成人工智能技术（AI）已经使得自动文本简化在语言和语法层次上自动进行。然而，应用常focus on英语，对低资源语言如荷兰语的应用知之甚少。为了了解生成技术在文本简化中的效果和限制，我们进行了实践研究，并提供以下结果： 1）设计和实现一个可配置的文本简化管道，该管道将state-of-the-art生成文本简化模型、领域和读者适应、视觉模块相互协调。 2）对 automatic文本简化的发现和经验，包括自动简化的优势和处理文化和常识知识的挑战。这些结果代表了对荷兰文本简化的首次探索，并照亮未来的研究和实践的前景。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/26/cs.CL_2023_08_26/" data-id="cloojsmcr009zre88b71j1m0a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/26/cs.LG_2023_08_26/" class="article-date">
  <time datetime="2023-08-26T10:00:00.000Z" itemprop="datePublished">2023-08-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/26/cs.LG_2023_08_26/">cs.LG - 2023-08-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unveiling-the-Role-of-Message-Passing-in-Dual-Privacy-Preservation-on-GNNs"><a href="#Unveiling-the-Role-of-Message-Passing-in-Dual-Privacy-Preservation-on-GNNs" class="headerlink" title="Unveiling the Role of Message Passing in Dual-Privacy Preservation on GNNs"></a>Unveiling the Role of Message Passing in Dual-Privacy Preservation on GNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13513">http://arxiv.org/abs/2308.13513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Zhao, Hui Hu, Lu Cheng</li>
<li>for: This paper aims to address the privacy leakage issue in Graph Neural Networks (GNNs) and propose a principled privacy-preserving GNN framework.</li>
<li>methods: The proposed framework consists of three major modules: Sensitive Information Obfuscation Module, Dynamic Structure Debiasing Module, and Adversarial Learning Module.</li>
<li>results: Experimental results on four benchmark datasets show that the proposed model effectively protects both node and link privacy while preserving high utility for downstream tasks such as node classification.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are powerful tools for learning representations on graphs, such as social networks. However, their vulnerability to privacy inference attacks restricts their practicality, especially in high-stake domains. To address this issue, privacy-preserving GNNs have been proposed, focusing on preserving node and/or link privacy. This work takes a step back and investigates how GNNs contribute to privacy leakage. Through theoretical analysis and simulations, we identify message passing under structural bias as the core component that allows GNNs to \textit{propagate} and \textit{amplify} privacy leakage. Building upon these findings, we propose a principled privacy-preserving GNN framework that effectively safeguards both node and link privacy, referred to as dual-privacy preservation. The framework comprises three major modules: a Sensitive Information Obfuscation Module that removes sensitive information from node embeddings, a Dynamic Structure Debiasing Module that dynamically corrects the structural bias, and an Adversarial Learning Module that optimizes the privacy-utility trade-off. Experimental results on four benchmark datasets validate the effectiveness of the proposed model in protecting both node and link privacy while preserving high utility for downstream tasks, such as node classification.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）是一种强大的图像学习工具，可以在社交网络等图像上学习表示。然而，它们的隐私泄露攻击限制了它们在高风险领域的实际应用。为解决这个问题，隐私保护GNNs被提出，重点保护节点和/或连接的隐私。本工作从另一个角度研究GNNs如何导致隐私泄露。通过理论分析和实验，我们发现了 message passing under structural bias 是 GNNs 中最重要的泄露扩散和强化因素。基于这些发现，我们提出了一种理解隐私保护的 GNN 框架，称为 dual-privacy preservation。该框架包括三个主要模块：敏感信息干扰模块、动态结构偏置修正模块和对抗学习模块。实验结果表明，提出的模型可以保护节点和连接的隐私，同时保持下游任务的高用户性。
</details></li>
</ul>
<hr>
<h2 id="TpuGraphs-A-Performance-Prediction-Dataset-on-Large-Tensor-Computational-Graphs"><a href="#TpuGraphs-A-Performance-Prediction-Dataset-on-Large-Tensor-Computational-Graphs" class="headerlink" title="TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs"></a>TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13490">http://arxiv.org/abs/2308.13490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/tpu_graphs">https://github.com/google-research-datasets/tpu_graphs</a></li>
<li>paper_authors: Phitchaya Mangpo Phothilimthana, Sami Abu-El-Haija, Kaidi Cao, Bahare Fatemi, Charith Mendis, Bryan Perozzi</li>
<li>for: 这篇论文的目的是提供一个大型计算图论文性能预测数据集，用于优化机器学习编译器和自动调整器。</li>
<li>methods: 这篇论文使用了计算图来表示机器学习工作负荷，并收集了来自开源机器学习项目的各种模型架构。</li>
<li>results: 这篇论文提出了一个大型计算图论文性能预测数据集（TPuGraphs），其中每个图表示一个主要计算任务，例如训练epoch或推理步骤。该数据集包含25倍以上的图数据，770倍以上的图 сред值大小，并且引入了新的挑战，如扩展性、训练效率和模型质量。<details>
<summary>Abstract</summary>
Precise hardware performance models play a crucial role in code optimizations. They can assist compilers in making heuristic decisions or aid autotuners in identifying the optimal configuration for a given program. For example, the autotuner for XLA, a machine learning compiler, discovered 10-20% speedup on state-of-the-art models serving substantial production traffic at Google. Although there exist a few datasets for program performance prediction, they target small sub-programs such as basic blocks or kernels. This paper introduces TpuGraphs, a performance prediction dataset on full tensor programs, represented as computational graphs, running on Tensor Processing Units (TPUs). Each graph in the dataset represents the main computation of a machine learning workload, e.g., a training epoch or an inference step. Each data sample contains a computational graph, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the dataset are collected from open-source machine learning programs, featuring popular model architectures, e.g., ResNet, EfficientNet, Mask R-CNN, and Transformer. TpuGraphs provides 25x more graphs than the largest graph property prediction dataset (with comparable graph sizes), and 770x larger graphs on average compared to existing performance prediction datasets on machine learning programs. This graph-level prediction task on large graphs introduces new challenges in learning, ranging from scalability, training efficiency, to model quality.
</details>
<details>
<summary>摘要</summary>
精准硬件性能模型在代码优化中扮演着关键的角色。它们可以帮助编译器做出优化决策，或者帮助自动调整器确定程序的最佳配置。例如，XLA的自动调整器在处理大规模生产流量时发现了10-20%的提升。虽然有一些程序性能预测数据集存在，但它们主要针对小型子程序，如基本块或kernels。这篇文章介绍了TpuGraphs，一个基于计算图的程序性能预测数据集，运行在tensor处理单元（TPU）上。每个图像在数据集中表示了机器学习工作负荷的主要计算，例如训练epoch或推理步骤。每个数据样本包含一个计算图、一个编译配置和图像的执行时间。数据集中的图像来自开源机器学习程序，包括受欢迎的模型架构，如ResNet、EfficientNet、Mask R-CNN和Transformer。TpuGraphs提供了25倍更多的图像，并770倍更大的图像平均大小，相比已有的机器学习程序性能预测数据集。这个图级预测任务中的大图学习问题 introduce了新的挑战，从扩展性、培训效率、模型质量等方面。
</details></li>
</ul>
<hr>
<h2 id="Staleness-Alleviated-Distributed-GNN-Training-via-Online-Dynamic-Embedding-Prediction"><a href="#Staleness-Alleviated-Distributed-GNN-Training-via-Online-Dynamic-Embedding-Prediction" class="headerlink" title="Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction"></a>Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.13466">http://arxiv.org/abs/2308.13466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangji Bai, Ziyang Yu, Zheng Chai, Yue Cheng, Liang Zhao</li>
<li>for: 这篇论文是为了解决Graph Neural Networks（GNNs）在大规模图上的训练中的难点，特别是难以同步多个节点的问题。</li>
<li>methods: 这篇论文使用了分布式计算来解决这个问题，并且使用了历史值推断来实现高并发性。</li>
<li>results: 这篇论文提出了一个名为SAT（Staleness-Alleviated Training）的新的分布式GNN训练框架，可以有效地减少节点嵌入缓存的旧化。实验结果显示，SAT可以实现更好的性能和训练速度在多个大规模图数据集上。<details>
<summary>Abstract</summary>
Despite the recent success of Graph Neural Networks (GNNs), it remains challenging to train GNNs on large-scale graphs due to neighbor explosions. As a remedy, distributed computing becomes a promising solution by leveraging abundant computing resources (e.g., GPU). However, the node dependency of graph data increases the difficulty of achieving high concurrency in distributed GNN training, which suffers from the massive communication overhead. To address it, Historical value approximation is deemed a promising class of distributed training techniques. It utilizes an offline memory to cache historical information (e.g., node embedding) as an affordable approximation of the exact value and achieves high concurrency. However, such benefits come at the cost of involving dated training information, leading to staleness, imprecision, and convergence issues. To overcome these challenges, this paper proposes SAT (Staleness-Alleviated Training), a novel and scalable distributed GNN training framework that reduces the embedding staleness adaptively. The key idea of SAT is to model the GNN's embedding evolution as a temporal graph and build a model upon it to predict future embedding, which effectively alleviates the staleness of the cached historical embedding. We propose an online algorithm to train the embedding predictor and the distributed GNN alternatively and further provide a convergence analysis. Empirically, we demonstrate that SAT can effectively reduce embedding staleness and thus achieve better performance and convergence speed on multiple large-scale graph datasets.
</details>
<details>
<summary>摘要</summary>
尽管 Graf Neural Networks (GNNs) 的最近成功，但是在大规模图上训练 GNNs 仍然具有挑战，主要是因为邻居爆炸。为了解决这问题，分布式计算成为了一种有前途的解决方案，利用了丰富的计算资源（例如 GPU）。然而，图数据中节点的依赖关系使得在分布式 GNN 训练中达到高并发性变得更加困难，这会导致巨大的通信开销。为了解决这个问题，历史值 aproximation 被视为一种有前途的分布式训练技术。它利用了一个缓存历史信息（例如节点嵌入）作为可以Affordable的近似值，并实现了高并发性。然而，这些利点来自于使用过时的训练信息，导致偏斜、不准确和融合问题。为了解决这些挑战，本文提出了 SAT（Staleness-Alleviated Training），一种新的和可扩展的分布式 GNN 训练框架。SAT 的关键思想是模型 GNN 的嵌入演化为一个 temporal graph，并建立一个模型来预测未来的嵌入，从而有效地减轻嵌入缓存的过时性。我们提出了一种在线算法来训练嵌入预测器和分布式 GNN  alternatively，并提供了一种融合分析。实验表明，SAT 可以有效地减轻嵌入缓存的过时性，从而实现更好的性能和融合速度在多个大规模图数据集上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/26/cs.LG_2023_08_26/" data-id="cloojsmhj00oare883b72boa8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/25/cs.CV_2023_08_25/" class="article-date">
  <time datetime="2023-08-25T13:00:00.000Z" itemprop="datePublished">2023-08-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/25/cs.CV_2023_08_25/">cs.CV - 2023-08-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ROAM-Robust-and-Object-aware-Motion-Generation-using-Neural-Pose-Descriptors"><a href="#ROAM-Robust-and-Object-aware-Motion-Generation-using-Neural-Pose-Descriptors" class="headerlink" title="ROAM: Robust and Object-aware Motion Generation using Neural Pose Descriptors"></a>ROAM: Robust and Object-aware Motion Generation using Neural Pose Descriptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12969">http://arxiv.org/abs/2308.12969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyue Zhang, Rishabh Dabral, Thomas Leimkühler, Vladislav Golyanik, Marc Habermann, Christian Theobalt</li>
<li>for: 本研究旨在解决现有自动方法对新物体的抗性和泛化问题，提高3D虚拟人物运动合成中对新物体的适应性和自然性。</li>
<li>methods: 本研究使用一个受过参数化的动作模型，并通过对物体只 datasets上学习的半定态特征表示来增强模型对新物体的抗性和泛化能力。</li>
<li>results: 通过对比当前状态的方法和用户研究，本研究得到了较好的3D虚拟人物运动和互动质量和稳定性，并且可以在未看过物体的情况下进行高质量的动作生成。<details>
<summary>Abstract</summary>
Existing automatic approaches for 3D virtual character motion synthesis supporting scene interactions do not generalise well to new objects outside training distributions, even when trained on extensive motion capture datasets with diverse objects and annotated interactions. This paper addresses this limitation and shows that robustness and generalisation to novel scene objects in 3D object-aware character synthesis can be achieved by training a motion model with as few as one reference object. We leverage an implicit feature representation trained on object-only datasets, which encodes an SE(3)-equivariant descriptor field around the object. Given an unseen object and a reference pose-object pair, we optimise for the object-aware pose that is closest in the feature space to the reference pose. Finally, we use l-NSM, i.e., our motion generation model that is trained to seamlessly transition from locomotion to object interaction with the proposed bidirectional pose blending scheme. Through comprehensive numerical comparisons to state-of-the-art methods and in a user study, we demonstrate substantial improvements in 3D virtual character motion and interaction quality and robustness to scenarios with unseen objects. Our project page is available at https://vcai.mpi-inf.mpg.de/projects/ROAM/.
</details>
<details>
<summary>摘要</summary>
现有自动化方法 для3D虚拟人物运动合成，不能很好地泛化到新物体外部训练分布，即使训练在含有多种物体和注释交互的大规模运动捕捉数据集上。这篇论文解决了这一问题，并显示了在含有新物体的场景中的3D物体意识Character Synthesis中的稳定性和泛化性可以通过训练一个运动模型，只需要一个参考物体。我们利用了一种基于物体专门采集的隐藏特征表示，这种表示在物体周围的SE(3)-等变换equivariant描述器场中编码了物体。给定一个未看过的物体和一个参考姿态-物体对，我们优化了 closest在特征空间的物体意识姿态。最后，我们使用l-NSM，即我们训练的运动生成模型，通过我们的拟合bidirectional姿态混合方案来协调转换从步行到物体交互。通过对现有方法的数字比较和用户研究，我们展示了3D虚拟人物运动和交互质量和稳定性在未看过物体场景中得到了显著改善。我们的项目页面可以在https://vcai.mpi-inf.mpg.de/projects/ROAM/上找到。
</details></li>
</ul>
<hr>
<h2 id="Scenimefy-Learning-to-Craft-Anime-Scene-via-Semi-Supervised-Image-to-Image-Translation"><a href="#Scenimefy-Learning-to-Craft-Anime-Scene-via-Semi-Supervised-Image-to-Image-Translation" class="headerlink" title="Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation"></a>Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12968">http://arxiv.org/abs/2308.12968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuxinn-j/scenimefy">https://github.com/yuxinn-j/scenimefy</a></li>
<li>paper_authors: Yuxin Jiang, Liming Jiang, Shuai Yang, Chen Change Loy</li>
<li>for: 这种研究的目的是提高动漫场景的自动高质量渲染，以解决现有的镜像匹配问题，提高图像的semantic preserve和精细特征。</li>
<li>methods: 这种方法使用了semi-supervised image-to-image翻译框架，使用了Structure-consistent pseudo paired data，并使用了segementation-guided data selection和patch-wise contrastive style loss来提高风格化和精细特征。</li>
<li>results: 对比 estado-of-the-art 基eline，这种方法在 both perceptual quality和量化性能方面表现出色，得到了更高的质量和更好的结果。<details>
<summary>Abstract</summary>
Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fine details. Besides, we contribute a high-resolution anime scene dataset to facilitate future research. Our extensive experiments demonstrate the superiority of our method over state-of-the-art baselines in terms of both perceptual quality and quantitative performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fine details. Besides, we contribute a high-resolution anime scene dataset to facilitate future research. Our extensive experiments demonstrate the superiority of our method over state-of-the-art baselines in terms of both perceptual quality and quantitative performance.Translated by Google Translate.
</details></li>
</ul>
<hr>
<h2 id="POCO-3D-Pose-and-Shape-Estimation-with-Confidence"><a href="#POCO-3D-Pose-and-Shape-Estimation-with-Confidence" class="headerlink" title="POCO: 3D Pose and Shape Estimation with Confidence"></a>POCO: 3D Pose and Shape Estimation with Confidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12965">http://arxiv.org/abs/2308.12965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, Dimitrios Tzionas<br>for: The paper is written for improving the accuracy of 3D human pose and shape estimation from images, and providing uncertainty estimates for downstream tasks.methods: The paper proposes a novel framework called POCO, which uses a Dual Conditioning Strategy (DCS) to estimate both the 3D body pose and the per-sample variance in a single feed-forward pass.results: The paper shows that training the network to reason about uncertainty helps it learn to more accurately estimate 3D pose, and demonstrates the effectiveness of the proposed method by applying it to three state-of-the-art HPS regressors and showing improvement in accuracy. Additionally, the paper demonstrates the usefulness of the uncertainty estimates for downstream tasks such as bootstrap HPS training and video pose estimation.Here’s the Chinese translation of the three information:for: 本文是为了提高图像中人体三维姿态和形状估计的准确性, 并为下游任务提供不确定性估计。methods: 本文提出了一种名为POCO的新框架，该框架使用双conditioning策略（DCS）来在单一的前向传播中估计3D人体姿态和每个样本的方差。results: 本文显示了训练网络理解不确定性可以帮助其更加准确地估计3D姿态，并通过应用到三个state-of-the-art HPS regressors上显示了改进准确性。此外，本文还示出了不确定性估计的实用性，例如通过自动划分不确定样本来进行HPS训练、视频人体姿态估计等。<details>
<summary>Abstract</summary>
The regression of 3D Human Pose and Shape (HPS) from an image is becoming increasingly accurate. This makes the results useful for downstream tasks like human action recognition or 3D graphics. Yet, no regressor is perfect, and accuracy can be affected by ambiguous image evidence or by poses and appearance that are unseen during training. Most current HPS regressors, however, do not report the confidence of their outputs, meaning that downstream tasks cannot differentiate accurate estimates from inaccurate ones. To address this, we develop POCO, a novel framework for training HPS regressors to estimate not only a 3D human body, but also their confidence, in a single feed-forward pass. Specifically, POCO estimates both the 3D body pose and a per-sample variance. The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressing uncertainty that is highly correlated to pose reconstruction quality. The POCO framework can be applied to any HPS regressor and here we evaluate it by modifying HMR, PARE, and CLIFF. In all cases, training the network to reason about uncertainty helps it learn to more accurately estimate 3D pose. While this was not our goal, the improvement is modest but consistent. Our main motivation is to provide uncertainty estimates for downstream tasks; we demonstrate this in two ways: (1) We use the confidence estimates to bootstrap HPS training. Given unlabelled image data, we take the confident estimates of a POCO-trained regressor as pseudo ground truth. Retraining with this automatically-curated data improves accuracy. (2) We exploit uncertainty in video pose estimation by automatically identifying uncertain frames (e.g. due to occlusion) and inpainting these from confident frames. Code and models will be available for research at https://poco.is.tue.mpg.de.
</details>
<details>
<summary>摘要</summary>
“三维人体姿态和形状（HPS）从图像回推的进步越来越精确，使得结果可以用于人做动作识别或3Dgraphics。然而，没有一个优秀的回推器，因为图像证据不明确或者人做动作和外表都没有在训练过程中出现过。现今大多数HPS回推器都不会报告其出力的可信度，因此下游任务无法分辨实际的估计和错误的估计。为了解决这个问题，我们开发了POCO，一个新的框架，可以在单一的从前进推 pass中预测HPS和其可信度。具体来说，POCO会预测3D人体姿态和每个样本的条件方差。我们的关键思想是通过引入双条件策略（DCS）来预测不确定性，这和姿态重建质量高度相关。POCO框架可以应用于任何HPS回推器，我们在这里评估了修改HMR、PARE和CLIFF等回推器。在所有情况下，将network培训来理解不确定性，使其更好地估计3D姿态。这并不是我们的主要目标，但是改善是微不足道，但是一致的。我们的主要动机是提供不确定性估计，我们在两种方式中示出了这个：（1）我们使用POCO-trained回推器的自信估计作为自动生成的pseudo陌生标本。将这些自信估计作为训练标本，然后重训，可以提高准确性。（2）我们利用不确定性在动作捕捉中自动识别 uncertain frames（例如由遮蔽所致），并从自信frames中填充这些frame。”
</details></li>
</ul>
<hr>
<h2 id="Dense-Text-to-Image-Generation-with-Attention-Modulation"><a href="#Dense-Text-to-Image-Generation-with-Attention-Modulation" class="headerlink" title="Dense Text-to-Image Generation with Attention Modulation"></a>Dense Text-to-Image Generation with Attention Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12964">http://arxiv.org/abs/2308.12964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naver-ai/densediffusion">https://github.com/naver-ai/densediffusion</a></li>
<li>paper_authors: Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, Jun-Yan Zhu</li>
<li>for: 实现文本描述中的具体图像Synthesize realistic images from dense captions, where each text prompt provides a detailed description for a specific image region.</li>
<li>methods: 使用预训条件为文本描述中的具体图像构成，并通过控制图像的构成来实现具体图像的生成。</li>
<li>results: 以无需训练和数据集，提高文本描述中的具体图像生成效果，并与特定构成条件下的图像生成效果相似。<details>
<summary>Abstract</summary>
Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images' layouts and the pre-trained model's intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.
</details>
<details>
<summary>摘要</summary>
existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images' layouts and the pre-trained model's intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.Here's the translation in Traditional Chinese:现有的文本至图扩散模型对于细节描述的文本提示则做不好，这些文本提示每个图像区域的详细描述。为解决这个问题，我们提出了DenseDiffusion，一种不需要训练的方法，可以将预训练的文本至图模型调整以应对这些细节描述。我们首先分析生成图像的布局和预训练模型的中间注意力图。接着，我们开发了一种注意力调节方法，可以根据布局指导物品出现在特定的区域中。不需要进一步的调整或数据，我们提高了对细节描述的图像生成性能，并且在自动和人类评估上都有改善。此外，我们可以使用特定布局条件进行训练，以获得相似的视觉效果。
</details></li>
</ul>
<hr>
<h2 id="MapPrior-Bird’s-Eye-View-Map-Layout-Estimation-with-Generative-Models"><a href="#MapPrior-Bird’s-Eye-View-Map-Layout-Estimation-with-Generative-Models" class="headerlink" title="MapPrior: Bird’s-Eye View Map Layout Estimation with Generative Models"></a>MapPrior: Bird’s-Eye View Map Layout Estimation with Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12963">http://arxiv.org/abs/2308.12963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiyue Zhu, Vlas Zyrianov, Zhijian Liu, Shenlong Wang</li>
<li>for: 提高 bird’s-eye view (BEV) 识别模型的准确性和生成性的 semantic map 布局</li>
<li>methods:  combine 传统的探测型 BEV 识别模型和学习的生成模型 для semantic map 布局</li>
<li>results: 在 nuScenes benchmark 上，MapPrior 比最强竞争对手提高 MMD 和 ECE  scores 的 camera-和 LiDAR-based BEV 识别任务中表现出色，得到了显著改善的结果。<details>
<summary>Abstract</summary>
Despite tremendous advancements in bird's-eye view (BEV) perception, existing models fall short in generating realistic and coherent semantic map layouts, and they fail to account for uncertainties arising from partial sensor information (such as occlusion or limited coverage). In this work, we introduce MapPrior, a novel BEV perception framework that combines a traditional discriminative BEV perception model with a learned generative model for semantic map layouts. Our MapPrior delivers predictions with better accuracy, realism, and uncertainty awareness. We evaluate our model on the large-scale nuScenes benchmark. At the time of submission, MapPrior outperforms the strongest competing method, with significantly improved MMD and ECE scores in camera- and LiDAR-based BEV perception.
</details>
<details>
<summary>摘要</summary>
尽管存在巨大的进步，现有的鸟瞰视（BEV）感知模型仍未能生成真实、凝重的 semantic map 布局，并且无法考虑部分感知器（如遮挡或有限覆盖）中的不确定性。在这项工作中，我们引入 MapPrior，一种新的 BEV 感知框架，该框架将传统的推理 BEV 感知模型与学习的生成模型结合在一起。我们的 MapPrior 能够提供更加准确、真实和不确定性意识的预测。我们在 nuScenes benchmark 上进行了评估，当时提交的 MapPrior 已经超过了最强竞争对手，在摄像头和 LiDAR 基于的 BEV 感知方面具有显著提高的 MMD 和 ECE 分数。
</details></li>
</ul>
<hr>
<h2 id="Motion-Guided-Masking-for-Spatiotemporal-Representation-Learning"><a href="#Motion-Guided-Masking-for-Spatiotemporal-Representation-Learning" class="headerlink" title="Motion-Guided Masking for Spatiotemporal Representation Learning"></a>Motion-Guided Masking for Spatiotemporal Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12962">http://arxiv.org/abs/2308.12962</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Fan, Jue Wang, Shuai Liao, Yi Zhu, Vimal Bhat, Hector Santos-Villalobos, Rohith MV, Xinyu Li</li>
<li>for: 这个论文主要是为了提高视频理解性，并且使用随机遮盲法来提高视频 autoencoder 的性能。</li>
<li>methods: 这个论文提出了一种新的推寄算法，即动态推寄法（Motion-guided masking，MGM），该算法利用运动向量来引导遮盲器在时间上的位置。</li>
<li>results: 在两个复杂的大规模视频测试集（Kinetics-400和Something-Something V2）上，这个方法可以与之前的状态OF-THE-ART方法相比，在视频 autoencoder 中获得最大 $1.3%$ 的提高。此外，这个方法还可以在训练EPoch数量相同的情况下，与之前的方法相比，在视频 autoencoder 中获得最大 $66%$ 的提高。最后，这个方法在下游传输学习和领域适应任务中表现出色，在 UCF101、HMDB51 和 Diving48  datasets上获得最大 $4.9%$ 的提高。<details>
<summary>Abstract</summary>
Several recent works have directly extended the image masked autoencoder (MAE) with random masking into video domain, achieving promising results. However, unlike images, both spatial and temporal information are important for video understanding. This suggests that the random masking strategy that is inherited from the image MAE is less effective for video MAE. This motivates the design of a novel masking algorithm that can more efficiently make use of video saliency. Specifically, we propose a motion-guided masking algorithm (MGM) which leverages motion vectors to guide the position of each mask over time. Crucially, these motion-based correspondences can be directly obtained from information stored in the compressed format of the video, which makes our method efficient and scalable. On two challenging large-scale video benchmarks (Kinetics-400 and Something-Something V2), we equip video MAE with our MGM and achieve up to +$1.3\%$ improvement compared to previous state-of-the-art methods. Additionally, our MGM achieves equivalent performance to previous video MAE using up to $66\%$ fewer training epochs. Lastly, we show that MGM generalizes better to downstream transfer learning and domain adaptation tasks on the UCF101, HMDB51, and Diving48 datasets, achieving up to +$4.9\%$ improvement compared to baseline methods.
</details>
<details>
<summary>摘要</summary>
On two challenging large-scale video benchmarks (Kinetics-400 and Something-Something V2), we equip video MAE with our MGM and achieve up to +$1.3\%$ improvement compared to previous state-of-the-art methods. Additionally, our MGM achieves equivalent performance to previous video MAE using up to $66\%$ fewer training epochs. Our MGM also generalizes better to downstream transfer learning and domain adaptation tasks on the UCF101, HMDB51, and Diving48 datasets, achieving up to +$4.9\%$ improvement compared to baseline methods.
</details></li>
</ul>
<hr>
<h2 id="Less-is-More-Towards-Efficient-Few-shot-3D-Semantic-Segmentation-via-Training-free-Networks"><a href="#Less-is-More-Towards-Efficient-Few-shot-3D-Semantic-Segmentation-via-Training-free-Networks" class="headerlink" title="Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks"></a>Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12961">http://arxiv.org/abs/2308.12961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangyangyang127/tfs3d">https://github.com/yangyangyang127/tfs3d</a></li>
<li>paper_authors: Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Hao Dong, Peng Gao</li>
<li>for: 提高3D分割任务中的几个shot学习效果，减少大规模数据的依赖。</li>
<li>methods: 提出了一种没有学习参数的培训自由3D分割网络（TFS3D）和其进一步改进版本TFS3D-T。TFS3D使用三角函数坐标编码提取密集表示，与之前的培训方法相比具有相似的性能。TFS3D-T通过增强几个shot查询和支持数据之间的交互，提高了前期培训的效果。</li>
<li>results: 对S3DIS和ScanNet数据集进行实验，TFS3D-T在mIoU方面提高了+6.93%和+17.96%，同时减少了培训时间 by -90%，表明TFS3D-T具有更高的效果和效率。<details>
<summary>Abstract</summary>
To reduce the reliance on large-scale datasets, recent works in 3D segmentation resort to few-shot learning. Current 3D few-shot semantic segmentation methods first pre-train the models on `seen' classes, and then evaluate their generalization performance on `unseen' classes. However, the prior pre-training stage not only introduces excessive time overhead, but also incurs a significant domain gap on `unseen' classes. To tackle these issues, we propose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, and a further training-based variant, TFS3D-T. Without any learnable parameters, TFS3D extracts dense representations by trigonometric positional encodings, and achieves comparable performance to previous training-based methods. Due to the elimination of pre-training, TFS3D can alleviate the domain gap issue and save a substantial amount of time. Building upon TFS3D, TFS3D-T only requires to train a lightweight query-support transferring attention (QUEST), which enhances the interaction between the few-shot query and support data. Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by +6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the training time by -90%, indicating superior effectiveness and efficiency.
</details>
<details>
<summary>摘要</summary>
Recent works in 3D segmentation have resorted to few-shot learning to reduce reliance on large-scale datasets. Current 3D few-shot semantic segmentation methods first pre-train the models on "seen" classes and then evaluate their generalization performance on "unseen" classes. However, the prior pre-training stage not only introduces excessive time overhead but also incurs a significant domain gap on "unseen" classes. To address these issues, we propose an efficient Training-free Few-shot 3D Segmentation network (TFS3D) and a further training-based variant (TFS3D-T). Without any learnable parameters, TFS3D extracts dense representations by trigonometric positional encodings and achieves comparable performance to previous training-based methods. Due to the elimination of pre-training, TFS3D can alleviate the domain gap issue and save a substantial amount of time. Building upon TFS3D, TFS3D-T only requires training a lightweight query-support transferring attention (QUEST), which enhances the interaction between the few-shot query and support data. Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by +6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the training time by -90%, indicating superior effectiveness and efficiency.
</details></li>
</ul>
<hr>
<h2 id="Towards-Realistic-Zero-Shot-Classification-via-Self-Structural-Semantic-Alignment"><a href="#Towards-Realistic-Zero-Shot-Classification-via-Self-Structural-Semantic-Alignment" class="headerlink" title="Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment"></a>Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12960">http://arxiv.org/abs/2308.12960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sheng-eatamath/S3A">https://github.com/sheng-eatamath/S3A</a></li>
<li>paper_authors: Sheng Zhang, Muzammal Naseer, Guangyi Chen, Zhiqiang Shen, Salman Khan, Kun Zhang, Fahad Khan</li>
<li>for: 本研究的目的是解决零 shot 分类中的开放世界问题，即没有注释但具有广泛的词汇。</li>
<li>methods: 本研究提出了 Self Structural Semantic Alignment (S^3A) 框架，它可以从无注释数据中提取结构性 semantics，并同时进行自我学习。S^3A 框架包括一种唯一的 Cluster-Vote-Prompt-Realign (CVPR) 算法，它通过轮循图像集成、选择每个集合中的图像，通过大语言模型生成权威提示，以及将图像和词汇进行结构性 semantic alignment，来提取结构性 semantics。</li>
<li>results: 对多种通用和细化的 benchmarcks 进行了广泛的实验，结果表明，S^3A 方法可以在零 shot 分类中提供较高的精度改进，相比 CLIP 的平均改进率高于 15%。<details>
<summary>Abstract</summary>
Large-scale pre-trained Vision Language Models (VLMs) have proven effective for zero-shot classification. Despite the success, most traditional VLMs-based methods are restricted by the assumption of partial source supervision or ideal vocabularies, which rarely satisfy the open-world scenario. In this paper, we aim at a more challenging setting, Realistic Zero-Shot Classification, which assumes no annotation but instead a broad vocabulary. To address this challenge, we propose the Self Structural Semantic Alignment (S^3A) framework, which extracts the structural semantic information from unlabeled data while simultaneously self-learning. Our S^3A framework adopts a unique Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR process includes iterative clustering on images, voting within each cluster to identify initial class candidates from the vocabulary, generating discriminative prompts with large language models to discern confusing candidates, and realigning images and the vocabulary as structural semantic alignment. Finally, we propose to self-learn the CLIP image encoder with both individual and structural semantic alignment through a teacher-student learning strategy. Our comprehensive experiments across various generic and fine-grained benchmarks demonstrate that the S^3A method offers substantial improvements over existing VLMs-based approaches, achieving a more than 15% accuracy improvement over CLIP on average. Our codes, models, and prompts are publicly released at https://github.com/sheng-eatamath/S3A.
</details>
<details>
<summary>摘要</summary>
大规模预训练视觉语言模型（VLM）已经证明有效于零shot分类。 despite the success， most traditional VLMs-based methods are restricted by the assumption of partial source supervision or ideal vocabularies，which rarely satisfy the open-world scenario. In this paper，we aim at a more challenging setting，Realistic Zero-Shot Classification，which assumes no annotation but instead a broad vocabulary. To address this challenge，we propose the Self Structural Semantic Alignment（S^3A）framework，which extracts the structural semantic information from unlabeled data while simultaneously self-learning. Our S^3A framework adopts a unique Cluster-Vote-Prompt-Realign（CVPR）algorithm，which iteratively groups unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR process includes iterative clustering on images，voting within each cluster to identify initial class candidates from the vocabulary，generating discriminative prompts with large language models to discern confusing candidates，and realigning images and the vocabulary as structural semantic alignment. Finally，we propose to self-learn the CLIP image encoder with both individual and structural semantic alignment through a teacher-student learning strategy. Our comprehensive experiments across various generic and fine-grained benchmarks demonstrate that the S^3A method offers substantial improvements over existing VLMs-based approaches，achieving a more than 15% accuracy improvement over CLIP on average. Our codes，models，and prompts are publicly released at https://github.com/sheng-eatamath/S3A.
</details></li>
</ul>
<hr>
<h2 id="Label-Budget-Allocation-in-Multi-Task-Learning"><a href="#Label-Budget-Allocation-in-Multi-Task-Learning" class="headerlink" title="Label Budget Allocation in Multi-Task Learning"></a>Label Budget Allocation in Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12949">http://arxiv.org/abs/2308.12949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ximeng Sun, Kihyuk Sohn, Kate Saenko, Clayton Mellina, Xiao Bian</li>
<li>for: 提高机器学习系统的性能，解决标签数据的成本问题。</li>
<li>methods: 提出了标签预算分配问题，并提出了一种适应任务的预算分配算法来解决这个问题。</li>
<li>results: 通过实验证明了我们的方法可以比其他各种常用的标签策略提高多任务学习的性能。<details>
<summary>Abstract</summary>
The cost of labeling data often limits the performance of machine learning systems. In multi-task learning, related tasks provide information to each other and improve overall performance, but the label cost can vary among tasks. How should the label budget (i.e. the amount of money spent on labeling) be allocated among different tasks to achieve optimal multi-task performance? We are the first to propose and formally define the label budget allocation problem in multi-task learning and to empirically show that different budget allocation strategies make a big difference to its performance. We propose a Task-Adaptive Budget Allocation algorithm to robustly generate the optimal budget allocation adaptive to different multi-task learning settings. Specifically, we estimate and then maximize the extent of new information obtained from the allocated budget as a proxy for multi-task learning performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy of our approach over other widely used heuristic labeling strategies.
</details>
<details>
<summary>摘要</summary>
machine learning系统的性能受数据标注成本的限制。在多任务学习中，相关任务之间交换信息，提高总性能，但标注成本可能因任务而异。如何在多任务学习中合理分配标注预算（即用于标注的费用）以达到最佳性能？我们是第一个提出并正式定义多任务学习标注预算分配问题，并通过实验证明不同预算分配策略对性能产生了很大影响。我们提出了适应任务的预算分配算法，可以在不同的多任务学习设置下生成最佳的预算分配策略。specifically，我们估算并最大化从分配预算中获得的新信息的总量，作为多任务学习性能的代理。 Pascal VOC和Taskonomy的实验表明我们的方法比其他常见的标注策略更有效。
</details></li>
</ul>
<hr>
<h2 id="Perspective-aware-Convolution-for-Monocular-3D-Object-Detection"><a href="#Perspective-aware-Convolution-for-Monocular-3D-Object-Detection" class="headerlink" title="Perspective-aware Convolution for Monocular 3D Object Detection"></a>Perspective-aware Convolution for Monocular 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12938">http://arxiv.org/abs/2308.12938</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KenYu910645/perspective-aware-convolution">https://github.com/KenYu910645/perspective-aware-convolution</a></li>
<li>paper_authors: Jia-Quan Yu, Soo-Chang Pei</li>
<li>for: 提高自动驾驶车辆中的单摄像头三维物体检测精度。</li>
<li>methods: 提出了一种新的视角意识核心层，该层可以在图像中提取长距离依赖关系，以捕捉场景的视角信息。</li>
<li>results: 在KITTI3D数据集上测试，该方法可以提高3D物体检测精度，达到了23.9%的准确率。这些结果表明了场景信息的重要性，以及网络设计中场景结构的潜在优势。<details>
<summary>Abstract</summary>
Monocular 3D object detection is a crucial and challenging task for autonomous driving vehicle, while it uses only a single camera image to infer 3D objects in the scene. To address the difficulty of predicting depth using only pictorial clue, we propose a novel perspective-aware convolutional layer that captures long-range dependencies in images. By enforcing convolutional kernels to extract features along the depth axis of every image pixel, we incorporates perspective information into network architecture. We integrate our perspective-aware convolutional layer into a 3D object detector and demonstrate improved performance on the KITTI3D dataset, achieving a 23.9\% average precision in the easy benchmark. These results underscore the importance of modeling scene clues for accurate depth inference and highlight the benefits of incorporating scene structure in network design. Our perspective-aware convolutional layer has the potential to enhance object detection accuracy by providing more precise and context-aware feature extraction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate given text into Simplified Chinese.<</SYS>>单目3D对象检测是自动驾驶车辆中的一项关键和挑战性任务，它使用单个摄像头图像来推断Scene中的3D对象。为了解决基于图像的深度预测困难，我们提出了一种新的视角意识核心层。我们要求核心层在每个图像像素上提取深度轴方向的特征，从而将视角信息integrated into网络 architecture。我们将这种视角意识核心层与3D对象检测器结合，并在KITTI3D数据集上进行了评估，实现了23.9%的准确率在易 benchmark。这些结果证明了场景 clue的重要性，并高亮了网络设计中场景结构的 incorporation 的好处。我们的视角意识核心层有可能提高对象检测精度，通过提供更加准确和上下文感知的特征提取。
</details></li>
</ul>
<hr>
<h2 id="Panoptic-Depth-Color-Map-for-Combination-of-Depth-and-Image-Segmentation"><a href="#Panoptic-Depth-Color-Map-for-Combination-of-Depth-and-Image-Segmentation" class="headerlink" title="Panoptic-Depth Color Map for Combination of Depth and Image Segmentation"></a>Panoptic-Depth Color Map for Combination of Depth and Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12937">http://arxiv.org/abs/2308.12937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Quan Yu, Soo-Chang Pei</li>
<li>for: 这篇论文旨在提出一种将图像分割和深度估计结合在一起的新方法，以提高自动驾驶场景中图像识别的精度和安全性。</li>
<li>methods: 该方法具有一个额外的深度估计分支，用于在分割网络中预测每个实例段的深度。</li>
<li>results: 在Cityscape数据集上测试，该方法能够实现高质量的分割结果，同时包含深度信息，并通过色彩地图可视化。这种方法开拓了将不同任务和网络结合起来生成更全面的图像识别结果，以提高自动驾驶车辆的安全性。<details>
<summary>Abstract</summary>
Image segmentation and depth estimation are crucial tasks in computer vision, especially in autonomous driving scenarios. Although these tasks are typically addressed separately, we propose an innovative approach to combine them in our novel deep learning network, Panoptic-DepthLab. By incorporating an additional depth estimation branch into the segmentation network, it can predict the depth of each instance segment. Evaluating on Cityscape dataset, we demonstrate the effectiveness of our method in achieving high-quality segmentation results with depth and visualize it with a color map. Our proposed method demonstrates a new possibility of combining different tasks and networks to generate a more comprehensive image recognition result to facilitate the safety of autonomous driving vehicles.
</details>
<details>
<summary>摘要</summary>
Image segmentation和深度估计是计算机视觉中关键任务，尤其在自动驾驶场景下。虽然这两个任务通常被视为独立的，但我们提出了一种创新的方法，将它们结合在一起。我们的新型深度学习网络Panoptic-DepthLab中添加了一个深度估计分支，可以预测每个实例分割结果中的深度。在Cityscape数据集上评估，我们示出了我们的方法可以实现高质量的分割结果，并通过色彩地图进行可见化。我们的提议的方法开 up了将不同任务和网络结合起来以生成更全面的图像认知结果，以便促进自动驾驶车辆的安全。
</details></li>
</ul>
<hr>
<h2 id="Towards-Realistic-Unsupervised-Fine-tuning-with-CLIP"><a href="#Towards-Realistic-Unsupervised-Fine-tuning-with-CLIP" class="headerlink" title="Towards Realistic Unsupervised Fine-tuning with CLIP"></a>Towards Realistic Unsupervised Fine-tuning with CLIP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12919">http://arxiv.org/abs/2308.12919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Liang, Lijun Sheng, Zhengbo Wang, Ran He, Tieniu Tan</li>
<li>for: 这个研究旨在应用CLIPvision-language模型进行下游有监督学习任务，并在无监督下精致化CLIP。</li>
<li>methods: 本研究提出了一个简单、高效的精致化方法，名为Universal Entropy Optimization（UEO），它利用amples的信任程度来减少信任度高的例子的 conditional entropy，并将不信任度高的例子的margin entropy提高。 UEO还包括了对CLIP的视觉分支中的通道对称变换进行优化。</li>
<li>results: 经过了15个领域和4种不同的专门知识的广泛实验，结果显示UEO的表现比基eline方法更好，具有更高的普遍化和外部调整检测能力。<details>
<summary>Abstract</summary>
The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.   To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entropy of less confident instances. Apart from optimizing the textual prompts, UEO also incorporates optimization of channel-wise affine transformations within the visual branch of CLIP. Through extensive experiments conducted across 15 domains and 4 different types of prior knowledge, we demonstrate that UEO surpasses baseline methods in terms of both generalization and out-of-distribution detection.
</details>
<details>
<summary>摘要</summary>
“视觉语言模型（VLM）的出现，如CLIP，已经引发了大量关于其应用于下游有监督学习任务的研究effort。 although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.  To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entropy of less confident instances. Apart from optimizing the textual prompts, UEO also incorporates optimization of channel-wise affine transformations within the visual branch of CLIP. Through extensive experiments conducted across 15 domains and 4 different types of prior knowledge, we demonstrate that UEO surpasses baseline methods in terms of both generalization and out-of-distribution detection.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Robot-Pose-Nowcasting-Forecast-the-Future-to-Improve-the-Present"><a href="#Robot-Pose-Nowcasting-Forecast-the-Future-to-Improve-the-Present" class="headerlink" title="Robot Pose Nowcasting: Forecast the Future to Improve the Present"></a>Robot Pose Nowcasting: Forecast the Future to Improve the Present</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12914">http://arxiv.org/abs/2308.12914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Simoni, Francesco Marchetti, Guido Borghi, Federico Becattini, Lorenzo Seidenari, Roberto Vezzani, Alberto Del Bimbo</li>
<li>for: 本研究旨在提供一种基于视觉数据的精准三维姿态估计系统，以便在工业4.0enario中安全和有效地协同工作人员和机器人。</li>
<li>methods: 该系统基于视觉数据，并通过对未来姿态进行预测来提高当前姿态估计精度。这种技术被称为“姿态预测”（pose nowcasting）。</li>
<li>results: 实验结果表明，该系统在两个不同的数据集上达到了状态艺术和实时性的国际先进水平，并在机器人和人类场景中都有较高的有效性。<details>
<summary>Abstract</summary>
In recent years, the effective and safe collaboration between humans and machines has gained significant importance, particularly in the Industry 4.0 scenario. A critical prerequisite for realizing this collaborative paradigm is precisely understanding the robot's 3D pose within its environment. Therefore, in this paper, we introduce a novel vision-based system leveraging depth data to accurately establish the 3D locations of robotic joints. Specifically, we prove the ability of the proposed system to enhance its current pose estimation accuracy by jointly learning to forecast future poses. Indeed, we introduce the concept of Pose Nowcasting, denoting the capability of a system to exploit the learned knowledge of the future to improve the estimation of the present. The experimental evaluation is conducted on two different datasets, providing state-of-the-art and real-time performance and confirming the validity of the proposed method on both the robotic and human scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SCoRD-Subject-Conditional-Relation-Detection-with-Text-Augmented-Data"><a href="#SCoRD-Subject-Conditional-Relation-Detection-with-Text-Augmented-Data" class="headerlink" title="SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data"></a>SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12910">http://arxiv.org/abs/2308.12910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyan Yang, Kushal Kafle, Zhe Lin, Scott Cohen, Zhihong Ding, Vicente Ordonez</li>
<li>for: 预测Scene中对象之间的关系，以及这些关系的位置。</li>
<li>methods: 提出了一种基于自动生成模型的Subject-Conditional Relation Detection（SCoRD）方法，可以conditioned on a subject，预测该主题与其他对象之间的关系和位置。</li>
<li>results: 在Open Images dataset上，通过创建OIv6-SCoRD benchmark，并使用文本描述来自动生成relation-object对，提高了relation-object预测的准确率和泛化能力。<details>
<summary>Abstract</summary>
We propose Subject-Conditional Relation Detection SCoRD, where conditioned on an input subject, the goal is to predict all its relations to other objects in a scene along with their locations. Based on the Open Images dataset, we propose a challenging OIv6-SCoRD benchmark such that the training and testing splits have a distribution shift in terms of the occurrence statistics of $\langle$subject, relation, object$\rangle$ triplets. To solve this problem, we propose an auto-regressive model that given a subject, it predicts its relations, objects, and object locations by casting this output as a sequence of tokens. First, we show that previous scene-graph prediction methods fail to produce as exhaustive an enumeration of relation-object pairs when conditioned on a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for our relation-object predictions compared to the 49.75% obtained by a recent scene graph detector. Then, we show improved generalization on both relation-object and object-box predictions by leveraging during training relation-object pairs obtained automatically from textual captions and for which no object-box annotations are available. Particularly, for $\langle$subject, relation, object$\rangle$ triplets for which no object locations are available during training, we are able to obtain a recall@3 of 42.59% for relation-object pairs and 32.27% for their box locations.
</details>
<details>
<summary>摘要</summary>
我们提议Subject-Conditional Relation Detection（SCoRD），其中 conditioned on输入主题，目标是预测它们与其他对象之间的关系以及它们的位置。基于Open Images dataset，我们提出了一个具有分布转移的OIv6-SCoRD benchmark，其中训练和测试分别的分布转移是对于$\langle$主题、关系、对象$\rangle$ triplets的出现统计学的。为解决这个问题，我们提议一个自动生成的模型，其中给定一个主题，它预测其关系、对象和对象位置，并将这些输出作为一个序列的token进行投影。我们首先表明，前一些场景树预测方法在这个benchmark上不能生成主题conditioned的完整的对象-关系对的列表。特别是，我们获得了主题conditioned的relation-object预测的准确率为83.8%，而一个最近的场景树探测器只得49.75%。然后，我们表明，通过在训练中使用自动获得的文本描述中的关系-对象对，我们可以提高对象-框预测和关系-对象预测的总体化能力。特别是，对于没有输入对象位置的主题conditioned的$\langle$主题、关系、对象$\rangle$ triplets，我们可以获得relation-object预测的准确率为42.59%和对象位置预测的准确率为32.27%。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Semantic-Segmentation-from-the-Perspective-of-Explicit-Class-Embeddings"><a href="#Boosting-Semantic-Segmentation-from-the-Perspective-of-Explicit-Class-Embeddings" class="headerlink" title="Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings"></a>Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12894">http://arxiv.org/abs/2308.12894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhe Liu, Chuanjian Liu, Kai Han, Quan Tang, Zengchang Qin</li>
<li>for: 本研究旨在提高 semantic segmentation 任务中类划分的精度和效率，特别是通过强化类划分的 semantics 来提高分类准确率。</li>
<li>methods: 本研究提出了 ECENet 模型，该模型通过在多个阶段图像特征之间交互来获取和增强类划分的准确性。此外，我们还提出了一种 Feature Reconstruction 模块，该模块通过组合内在和多样化分支来保证特征的多样性和重复性。</li>
<li>results: 实验结果表明，ECENet 模型在 ADE20K 数据集上比其他模型具有更高的精度和效率，并在 PASCAL-Context 数据集上达到了新的州OF-the-art 结果。<details>
<summary>Abstract</summary>
Semantic segmentation is a computer vision task that associates a label with each pixel in an image. Modern approaches tend to introduce class embeddings into semantic segmentation for deeply utilizing category semantics, and regard supervised class masks as final predictions. In this paper, we explore the mechanism of class embeddings and have an insight that more explicit and meaningful class embeddings can be generated based on class masks purposely. Following this observation, we propose ECENet, a new segmentation paradigm, in which class embeddings are obtained and enhanced explicitly during interacting with multi-stage image features. Based on this, we revisit the traditional decoding process and explore inverted information flow between segmentation masks and class embeddings. Furthermore, to ensure the discriminability and informativity of features from backbone, we propose a Feature Reconstruction module, which combines intrinsic and diverse branches together to ensure the concurrence of diversity and redundancy in features. Experiments show that our ECENet outperforms its counterparts on the ADE20K dataset with much less computational cost and achieves new state-of-the-art results on PASCAL-Context dataset. The code will be released at https://gitee.com/mindspore/models and https://github.com/Carol-lyh/ECENet.
</details>
<details>
<summary>摘要</summary>
Semantic segmentation 是一个计算机视觉任务，将每个图像像素标注为不同类别。现代方法通常会将类别嵌入引入 semantic segmentation，并将监督类 маSK 视为最终预测。在这篇论文中，我们研究了类别嵌入机制，并发现可以根据类 маSK 提取更Explicit和有意义的类别嵌入。基于这一观察，我们提出了 ECENet，一种新的 segmentation 模式，其中类别嵌入在多Stage图像特征交互中得到并加强。此外，我们重新评估传统的解码过程，并探索类 segmentation masks 和类别嵌入之间的倒推信息流。进一步，为保证特征来源的可识别度和信息充足，我们提出了一种 Feature Reconstruction 模块，它将内在和多样化分支结合起来，以保证特征的协调性和多样性。实验表明，我们的 ECENet 在 ADE20K 数据集上较其他方法具有许多计算成本的优势，并在 PASCAL-Context 数据集上达到了新的州态艺术结果。代码将在 https://gitee.com/mindspore/models 和 https://github.com/Carol-lyh/ECENet 上发布。
</details></li>
</ul>
<hr>
<h2 id="Multi-stage-feature-decorrelation-constraints-for-improving-CNN-classification-performance"><a href="#Multi-stage-feature-decorrelation-constraints-for-improving-CNN-classification-performance" class="headerlink" title="Multi-stage feature decorrelation constraints for improving CNN classification performance"></a>Multi-stage feature decorrelation constraints for improving CNN classification performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12880">http://arxiv.org/abs/2308.12880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuyu Zhu, Xuewen Zu, Chengfei Liu</li>
<li>for: 提高深度神经网络（CNN）的分类精度。</li>
<li>methods: 提出了一种多Stage Feature Decorrelation Loss（MFD Loss），用于约束前stage特征的谱相关性，从而提高CNN的分类精度。</li>
<li>results: 对多个常用的数据集和多种常用的CNN进行了实验比较和分析，证明了MFD Loss可以显著提高CNN的分类精度，并且与其他常见的损失函数结合使用也有优于单独使用Softmax损失的性能。<details>
<summary>Abstract</summary>
For the convolutional neural network (CNN) used for pattern classification, the training loss function is usually applied to the final output of the network, except for some regularization constraints on the network parameters. However, with the increasing of the number of network layers, the influence of the loss function on the network front layers gradually decreases, and the network parameters tend to fall into local optimization. At the same time, it is found that the trained network has significant information redundancy at all stages of features, which reduces the effectiveness of feature mapping at all stages and is not conducive to the change of the subsequent parameters of the network in the direction of optimality. Therefore, it is possible to obtain a more optimized solution of the network and further improve the classification accuracy of the network by designing a loss function for restraining the front stage features and eliminating the information redundancy of the front stage features .For CNN, this article proposes a multi-stage feature decorrelation loss (MFD Loss), which refines effective features and eliminates information redundancy by constraining the correlation of features at all stages. Considering that there are many layers in CNN, through experimental comparison and analysis, MFD Loss acts on multiple front layers of CNN, constrains the output features of each layer and each channel, and performs supervision training jointly with classification loss function during network training. Compared with the single Softmax Loss supervised learning, the experiments on several commonly used datasets on several typical CNNs prove that the classification performance of Softmax Loss+MFD Loss is significantly better. Meanwhile, the comparison experiments before and after the combination of MFD Loss and some other typical loss functions verify its good universality.
</details>
<details>
<summary>摘要</summary>
For the convolutional neural network (CNN) used for pattern classification, the training loss function is usually applied to the final output of the network, except for some regularization constraints on the network parameters. However, with the increase of the number of network layers, the influence of the loss function on the network front layers gradually decreases, and the network parameters tend to fall into local optimization. At the same time, it is found that the trained network has significant information redundancy at all stages of features, which reduces the effectiveness of feature mapping at all stages and is not conducive to the change of the subsequent parameters of the network in the direction of optimality. Therefore, it is possible to obtain a more optimized solution of the network and further improve the classification accuracy of the network by designing a loss function for restraining the front stage features and eliminating the information redundancy of the front stage features. For CNN, this article proposes a multi-stage feature decorrelation loss (MFD Loss), which refines effective features and eliminates information redundancy by constraining the correlation of features at all stages. Considering that there are many layers in CNN, through experimental comparison and analysis, MFD Loss acts on multiple front layers of CNN, constrains the output features of each layer and each channel, and performs supervision training jointly with classification loss function during network training. Compared with the single Softmax Loss supervised learning, the experiments on several commonly used datasets on several typical CNNs prove that the classification performance of Softmax Loss+MFD Loss is significantly better. Meanwhile, the comparison experiments before and after the combination of MFD Loss and some other typical loss functions verify its good universality.Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/25/cs.CV_2023_08_25/" data-id="cloojsmf100h0re889gxghq5t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/25/cs.AI_2023_08_25/" class="article-date">
  <time datetime="2023-08-25T12:00:00.000Z" itemprop="datePublished">2023-08-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/25/cs.AI_2023_08_25/">cs.AI - 2023-08-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NeO-360-Neural-Fields-for-Sparse-View-Synthesis-of-Outdoor-Scenes"><a href="#NeO-360-Neural-Fields-for-Sparse-View-Synthesis-of-Outdoor-Scenes" class="headerlink" title="NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes"></a>NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12967">http://arxiv.org/abs/2308.12967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zubair-irshad/NeO-360">https://github.com/zubair-irshad/NeO-360</a></li>
<li>paper_authors: Muhammad Zubair Irshad, Sergey Zakharov, Katherine Liu, Vitor Guizilini, Thomas Kollar, Adrien Gaidon, Zsolt Kira, Rares Ambrus</li>
<li>for: 本研究旨在提出一种可扩展的方法，用于从单个或几个姿态的RGB图像中 Synthesize 360度场景。</li>
<li>methods: 我们提出了一种基于神经网络的方法，称为NeO 360，它可以从单个或几个姿态的RGB图像中学习出360度场景的分布。我们的方法使用混合的图像conditional triplanar表示，可以在任何世界点上进行查询。</li>
<li>results: 我们在提出的挑战性360度无限 dataset中，称为NeRDS 360，进行了实验，并证明了NeO 360可以在新视图和新场景中进行高效的Synthesize，同时也提供了编辑和组合功能。<details>
<summary>Abstract</summary>
Recent implicit neural representations have shown great results for novel view synthesis. However, existing methods require expensive per-scene optimization from many views hence limiting their application to real-world unbounded urban settings where the objects of interest or backgrounds are observed from very few views. To mitigate this challenge, we introduce a new approach called NeO 360, Neural fields for sparse view synthesis of outdoor scenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenes from a single or a few posed RGB images. The essence of our approach is in capturing the distribution of complex real-world outdoor 3D scenes and using a hybrid image-conditional triplanar representation that can be queried from any world point. Our representation combines the best of both voxel-based and bird's-eye-view (BEV) representations and is more effective and expressive than each. NeO 360's representation allows us to learn from a large collection of unbounded 3D scenes while offering generalizability to new views and novel scenes from as few as a single image during inference. We demonstrate our approach on the proposed challenging 360{\deg} unbounded dataset, called NeRDS 360, and show that NeO 360 outperforms state-of-the-art generalizable methods for novel view synthesis while also offering editing and composition capabilities. Project page: https://zubair-irshad.github.io/projects/neo360.html
</details>
<details>
<summary>摘要</summary>
近期的隐式神经表示法已经实现了出色的新视图合成效果。然而，现有的方法具有每个场景需要贵重的多视图优化，因此在实际世界的无限范围城市场景中使用受限。为解决这个挑战，我们介绍了一种新的方法called NeO 360，它是一种普适的方法，可以从单个或几个RGB图像中重建360度场景。我们的方法的核心思想是捕捉复杂的实际 OUTDOOR 3D 场景的分布，并使用一种混合图像条件的三平面表示，可以在任何世界点上进行查询。我们的表示结合了 voxel-based 和 bird's-eye-view 表示的优点，并且在表示效果和可表达性方面比每个方法更好。NeO 360 的表示允许我们在大量的无限 3D 场景集中学习，并在新视图和新场景中进行推理，只需要几个图像。我们在 NeRDS 360 提出的挑战性 360度无限数据集上证明了 NeO 360 超过了当前最佳的通用方法，并且具有编辑和组合功能。项目页面：https://zubair-irshad.github.io/projects/neo360.html
</details></li>
</ul>
<hr>
<h2 id="DLIP-Distilling-Language-Image-Pre-training"><a href="#DLIP-Distilling-Language-Image-Pre-training" class="headerlink" title="DLIP: Distilling Language-Image Pre-training"></a>DLIP: Distilling Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12956">http://arxiv.org/abs/2308.12956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huafeng Kuang, Jie Wu, Xiawu Zheng, Ming Li, Xuefeng Xiao, Rui Wang, Min Zheng, Rongrong Ji</li>
<li>for: 提高语言图像预训练模型（VLP）的部署实际应用中的性能和效率。</li>
<li>methods: 通过知识填充来压缩VLP模型，并从不同模块的建筑特点和多模态信息传递的角度进行了多维ensional的分析和优化。</li>
<li>results: 通过实验，提出了一个简单 yet efficient的Distilling Language-Image Pre-training框架（DLIP），可以在多种跨模态任务中实现状态 искусственный智能的精度&#x2F;效率质量评价。例如，DLIP可以压缩BLIP模型1.9倍，从213M Parameters降至108M Parameters，同时保持和更好的性能。此外，DLIP可以保留95%以上的性能，使用22.4% Parameters和24.8% FLOPs，并提高执行速度2.7倍。<details>
<summary>Abstract</summary>
Vision-Language Pre-training (VLP) shows remarkable progress with the assistance of extremely heavy parameters, which challenges deployment in real applications. Knowledge distillation is well recognized as the essential procedure in model compression. However, existing knowledge distillation techniques lack an in-depth investigation and analysis of VLP, and practical guidelines for VLP-oriented distillation are still not yet explored. In this paper, we present DLIP, a simple yet efficient Distilling Language-Image Pre-training framework, through which we investigate how to distill a light VLP model. Specifically, we dissect the model distillation from multiple dimensions, such as the architecture characteristics of different modules and the information transfer of different modalities. We conduct comprehensive experiments and provide insights on distilling a light but performant VLP model. Experimental results reveal that DLIP can achieve a state-of-the-art accuracy/efficiency trade-off across diverse cross-modal tasks, e.g., image-text retrieval, image captioning and visual question answering. For example, DLIP compresses BLIP by 1.9x, from 213M to 108M parameters, while achieving comparable or better performance. Furthermore, DLIP succeeds in retaining more than 95% of the performance with 22.4% parameters and 24.8% FLOPs compared to the teacher model and accelerates inference speed by 2.7x.
</details>
<details>
<summary>摘要</summary>
《视力语言预训练（VLP）显示了惊人的进步，却面临实际应用中的部署挑战。知识填充被广泛认可为模型压缩的关键手段。然而，现有的知识填充技术尚未对VLP进行深入的研究和分析，并没有提供VLP-关注的压缩实践指南。本文提出了DLIP框架，是一个简单 yet efficient的语言图像预训练压缩框架。我们通过多维度分析模型压缩，包括不同模块的建筑特点和不同Modalities的信息传递。我们进行了广泛的实验，并提供了压缩轻量级VLP模型的深入分析和实践指南。实验结果表明，DLIP可以在多个横跨模态任务中实现状态机器的精度/效率交易，例如图像搜索、图像描述和视觉问答。例如，DLIP可以将BLIP压缩到1.9倍，从213M Parameters下降至108M Parameters，同时保持与教师模型相当或更好的性能。此外，DLIP可以保留95%以上的性能，使用22.4%的参数和24.8%的FLOPs，并提高执行速度2.7倍。
</details></li>
</ul>
<hr>
<h2 id="Low-count-Time-Series-Anomaly-Detection"><a href="#Low-count-Time-Series-Anomaly-Detection" class="headerlink" title="Low-count Time Series Anomaly Detection"></a>Low-count Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12925">http://arxiv.org/abs/2308.12925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Renz, Kurt Cutajar, Niall Twomey, Gavin K. C. Cheung, Hanting Xie</li>
<li>for: 本研究旨在Addressing the challenges of time series anomaly detection in low-count data settings, where signal-to-noise ratios are low and non-uniform performance is prevalent.</li>
<li>methods: 该研究引入了一种新的生成过程，用于创建含有低个数时间序列的异常段 benchmark datasets。该过程结合了理论和实验分析，以解释常用算法在异常段分布重叠问题上的缺陷。</li>
<li>results: 研究发现，使用异常分数平滑可以有效地提高异常检测性能。此外，该研究还 validate了该方法的实际用途性，在一个实际的零售店销售数据集上进行了验证。<details>
<summary>Abstract</summary>
Low-count time series describe sparse or intermittent events, which are prevalent in large-scale online platforms that capture and monitor diverse data types. Several distinct challenges surface when modelling low-count time series, particularly low signal-to-noise ratios (when anomaly signatures are provably undetectable), and non-uniform performance (when average metrics are not representative of local behaviour). The time series anomaly detection community currently lacks explicit tooling and processes to model and reliably detect anomalies in these settings. We address this gap by introducing a novel generative procedure for creating benchmark datasets comprising of low-count time series with anomalous segments. Via a mixture of theoretical and empirical analysis, our work explains how widely-used algorithms struggle with the distribution overlap between normal and anomalous segments. In order to mitigate this shortcoming, we then leverage our findings to demonstrate how anomaly score smoothing consistently improves performance. The practical utility of our analysis and recommendation is validated on a real-world dataset containing sales data for retail stores.
</details>
<details>
<summary>摘要</summary>
低个数时序列描述稀疏或间歇性事件，这些事件在大规模在线平台上采集和监测多种数据类型时很普遍。在模型低个数时序列时，存在一些独特的挑战，如低信号噪声比（畸变签识不可靠）和非均匀性（平均指标不能反映本地行为）。现有的时序异常检测社区没有专门的工具和过程来模型和可靠地检测这些设置中的异常。我们解决这个空白，通过引入一种新的生成过程，创建了包含低个数时序列异常段的标准数据集。我们通过理论和实验分析，解释了广泛使用的算法在分布重叠问题上的缺陷。然后，我们利用我们的发现，示出了如何使用异常分数缓解这个缺陷，提高性能。我们的分析和建议在实际的零售业务中得到了验证。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Vulnerabilities-in-ML-systems-in-terms-of-adversarial-attacks"><a href="#Evaluating-the-Vulnerabilities-in-ML-systems-in-terms-of-adversarial-attacks" class="headerlink" title="Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks"></a>Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12918">http://arxiv.org/abs/2308.12918</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Harshith, Mantej Singh Gill, Madhan Jothimani</li>
<li>for: 本研究探讨了最新的敌意攻击方法，以及它们对当前深度学习网络防御系统的影响。</li>
<li>methods: 本研究使用了Randomized和敌意示例来探讨漏洞的影响。</li>
<li>results: 研究发现，Randomized示例可能会导致漏洞的产生，而敌意示例则可能会导致漏洞的扩大。此外，研究还探讨了这些漏洞的伦理性。In English, that would be:</li>
<li>for: This research explores the latest adversarial attack methods and their impact on current deep learning cyber defense systems.</li>
<li>methods: The research uses Randomized and adversarial examples to examine the influence of vulnerabilities.</li>
<li>results: The study finds that Randomized examples may lead to the creation of vulnerabilities, while adversarial examples may exacerbate them. Additionally, the research discusses the ethical implications of these vulnerabilities.<details>
<summary>Abstract</summary>
There have been recent adversarial attacks that are difficult to find. These new adversarial attacks methods may pose challenges to current deep learning cyber defense systems and could influence the future defense of cyberattacks. The authors focus on this domain in this research paper. They explore the consequences of vulnerabilities in AI systems. This includes discussing how they might arise, differences between randomized and adversarial examples and also potential ethical implications of vulnerabilities. Moreover, it is important to train the AI systems appropriately when they are in testing phase and getting them ready for broader use.
</details>
<details>
<summary>摘要</summary>
现在有一些新的 adversarial 攻击方法，这些攻击方法可能会对当前的深度学习网络防御系统 pose 挑战。作者在这篇研究报告中关注这个领域，探讨了人工智能系统中的漏洞可能性。这包括讨论恶意攻击的可能性、随机化和 adversarial 示例之间的区别，以及漏洞的伦理问题。此外，在测试阶段，需要适当地训练 AI 系统，以便在更广泛的应用中使用。Note: "adversarial attacks" in the original text was translated as "恶意攻击" in Simplified Chinese, which is a more common term used in the field.
</details></li>
</ul>
<hr>
<h2 id="Language-as-Reality-A-Co-Creative-Storytelling-Game-Experience-in-1001-Nights-using-Generative-AI"><a href="#Language-as-Reality-A-Co-Creative-Storytelling-Game-Experience-in-1001-Nights-using-Generative-AI" class="headerlink" title="Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI"></a>Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12915">http://arxiv.org/abs/2308.12915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqian Sun, Zhouyi Li, Ke Fang, Chang Hee Lee, Ali Asadipour</li>
<li>For: The paper is written to explore the potential of using advanced AI tools like GPT-4 and Stable Diffusion to create an AI-native game that blends interactive narrative and text-to-image transformation, and to enhance the narrative game genre with AI-generated content.* Methods: The paper uses a game called “1001 Nights” as a case study to demonstrate the use of AI tools in game development. The game features a protagonist, Shahrzad, who is driven by a large language model and can realize words and stories in her world through conversation with the AI King. The player can steer the conversation towards specific keywords, which become battle equipment in the game.* Results: The paper presents the results of the second iteration of the game, which challenges the conventional border between the game world and reality through a dual perspective. The game allows the player to collaborate with AI to craft narratives and shape the game world, and explores the technical and design elements of implementing such a game.<details>
<summary>Abstract</summary>
In this paper, we present "1001 Nights", an AI-native game that allows players lead in-game reality through co-created storytelling with the character driven by large language model. The concept is inspired by Wittgenstein's idea of the limits of one's world being determined by the bounds of their language. Using advanced AI tools like GPT-4 and Stable Diffusion, the second iteration of the game enables the protagonist, Shahrzad, to realize words and stories in her world. The player can steer the conversation with the AI King towards specific keywords, which then become battle equipment in the game. This blend of interactive narrative and text-to-image transformation challenges the conventional border between the game world and reality through a dual perspective. We focus on Shahrzad, who seeks to alter her fate compared to the original folklore, and the player, who collaborates with AI to craft narratives and shape the game world. We explore the technical and design elements of implementing such a game with an objective to enhance the narrative game genre with AI-generated content and to delve into AI-native gameplay possibilities.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一款名为“1001夜”的人工智能（AI）原生游戏，该游戏使得玩家可以通过与人工智能合作创作故事来导导游戏世界。这个概念 draws inspiration from威特根штайн的思想，即语言的 bound пределяет我们的世界。使用了高级AI工具如GPT-4和Stable Diffusion，第二版游戏允许主人公 Шахرза德（Shahrzad）在她的世界中实现语言和故事。玩家可以通过对人工智能国王的对话指导语言，使得这些语言变成游戏中的武器。这种结合互动叙事和文本到图像转换的游戏模式挑战了传统游戏世界和现实之间的界限，我们在 dual perspective 中强调 Shahrazad 的自由和玩家和人工智能合作创作故事和形成游戏世界。我们探讨了在实施这种游戏时的技术和设计元素，以提高叙事游戏类型中的人工智能生成内容，并探索人工智能原生游戏的可能性。
</details></li>
</ul>
<hr>
<h2 id="CDAN-Convolutional-Dense-Attention-guided-Network-for-Low-light-Image-Enhancement"><a href="#CDAN-Convolutional-Dense-Attention-guided-Network-for-Low-light-Image-Enhancement" class="headerlink" title="CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement"></a>CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12902">http://arxiv.org/abs/2308.12902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Shakibania, Sina Raoufi, Hassan Khotanlou</li>
<li>for: 这篇论文主要针对低光照图像的改进和增强。</li>
<li>methods: 该论文提出了一种基于卷积神经网络和权重注意机制的Convolutional Dense Attention-guided Network（CDAN），用于提高低光照图像的明亮度、对比度和整体质量。</li>
<li>results: 对多个 benchmark 数据集进行测试，CDAN 表现出了明显的进步，与现有的状态艺技术相比，能够更好地处理低光照图像，并且能够有效地恢复图像中的纹理和颜色。<details>
<summary>Abstract</summary>
Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs remarkably on benchmark datasets, effectively mitigating under-exposure and proficiently restoring textures and colors in diverse low-light scenarios. This achievement underscores CDAN's potential for diverse computer vision tasks, notably enabling robust object detection and recognition in challenging low-light conditions.
</details>
<details>
<summary>摘要</summary>
低光照图像，受到不足照明的影响，具有减少清晰度、抑制颜色、降低细节等问题。低光照图像增强是计算机视觉中的关键任务，旨在通过提高亮度、对比度和总体品质来促进正确的分析和解释。本文介绍了一种新的卷积神经网络方法——卷积密集注意力引导网络（CDAN），用于提高低光照图像。CDAN结合了自适应网络架构、卷积块和密集块，并加入了注意力机制和跳过连接。这种架构确保了信息传递的高效和特征学习。此外，特定的后处理阶段进行了颜色均衡和对比度的调整。我们的方法在低光照图像增强中显示了明显的进步，与当前最佳结果相比，在多种复杂的场景中表现出了稳定和可靠的特点。我们的模型在标准 benchmark 数据集上表现出色，高效地抑制了下izada 和重新恢复了低光照图像中的纹理和颜色。这一成就表明 CDAN 在计算机视觉任务中具有广泛的潜力，特别是在低光照条件下进行稳定和准确的对象检测和识别。
</details></li>
</ul>
<hr>
<h2 id="Can-Linguistic-Knowledge-Improve-Multimodal-Alignment-in-Vision-Language-Pretraining"><a href="#Can-Linguistic-Knowledge-Improve-Multimodal-Alignment-in-Vision-Language-Pretraining" class="headerlink" title="Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?"></a>Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12898">http://arxiv.org/abs/2308.12898</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangfei-2019/snare">https://github.com/wangfei-2019/snare</a></li>
<li>paper_authors: Fei Wang, Liang Ding, Jun Rao, Ye Liu, Li Shen, Changxing Ding</li>
<li>for: 本研究旨在探讨语义知识和语法结构是否可以在视觉语言关联（VLP）中提取，以及这些语言知识如何影响或改善多模态对应。</li>
<li>methods: 我们设计了首个大规模多模态对应探测 benchmark，名为SNARE，以检测重要的语言组件，如 lexical、semantic 和 syntax 知识。我们的研究使用 five 种高级 VLP 模型进行总体分析，发现这些模型： i) 忽略复杂的语法结构，依赖内容词 для句子理解; ii) 对 Sentence 和否定逻辑的组合表示有限制; iii) 在视觉信息中找不到动作或空间关系，困难确定 triple 组合的正确性。</li>
<li>results: 我们的研究发现，VLP 模型在复杂的语法结构和 Sentence 与否定逻辑的组合中存在困难，而且在视觉信息中找不到动作或空间关系。<details>
<summary>Abstract</summary>
The multimedia community has shown a significant interest in perceiving and representing the physical world with multimodal pretrained neural network models, and among them, the visual-language pertaining (VLP) is, currently, the most captivating topic. However, there have been few endeavors dedicated to the exploration of 1) whether essential linguistic knowledge (e.g., semantics and syntax) can be extracted during VLP, and 2) how such linguistic knowledge impact or enhance the multimodal alignment. In response, here we aim to elucidate the impact of comprehensive linguistic knowledge, including semantic expression and syntactic structure, on multimodal alignment. Specifically, we design and release the SNARE, the first large-scale multimodal alignment probing benchmark, to detect the vital linguistic components, e.g., lexical, semantic, and syntax knowledge, containing four tasks: Semantic structure, Negation logic, Attribute ownership, and Relationship composition. Based on our proposed probing benchmarks, our holistic analyses of five advanced VLP models illustrate that the VLP model: i) shows insensitivity towards complex syntax structures and relies on content words for sentence comprehension; ii) demonstrates limited comprehension of combinations between sentences and negations; iii) faces challenges in determining the presence of actions or spatial relationships within visual information and struggles with verifying the correctness of triple combinations. We make our benchmark and code available at \url{https://github.com/WangFei-2019/SNARE/}.
</details>
<details>
<summary>摘要</summary>
multimedia社区对使用多模态预训练神经网络模型来感知和表示物理世界表示了广泛的兴趣，其中最吸引人的话题当属视语联系（VLP）。然而，有很少的尝试专门探讨以下两个问题：一是在VLP中是否可以提取语言基础知识（如 semantics和 syntax），二是如何使这些语言基础知识对多模态对应进行影响。为回答这些问题，我们希望通过检查包括语义表达和语法结构在内的全面语言知识的影响来解释多模态对应中的语言知识的影响。为此，我们设计并发布了首个大规模多模态对应探测 benchmark，即SNARE，以检测关键语言组件，如 lexical、semantic 和 syntax 知识。通过我们的提出的探测benchmark，我们对五种高级VLP模型进行了整体分析，发现：1. VLP模型对复杂语法结构表示不敏感，它们依赖于内容词来理解句子;2. VLP模型对 sentences和否定语言的组合表示有限制，它们很难理解这些组合的语义;3. VLP模型在视觉信息中寻找动作或空间关系的过程中遇到困难，同时它们也难以verify triple combinations的正确性。我们将我们的benchmark和代码发布在GitHub上，请参考 \url{https://github.com/WangFei-2019/SNARE/}.
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Vote-Prompting-for-Rare-Disease-Identification"><a href="#Large-Language-Models-Vote-Prompting-for-Rare-Disease-Identification" class="headerlink" title="Large Language Models Vote: Prompting for Rare Disease Identification"></a>Large Language Models Vote: Prompting for Rare Disease Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12890">http://arxiv.org/abs/2308.12890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oniani/llms-vote">https://github.com/oniani/llms-vote</a></li>
<li>paper_authors: David Oniani, Jordan Hilsman, Hang Dong, Fengyi Gao, Shiven Verma, Yanshan Wang</li>
<li>for: 该论文旨在提出一种具有灵活性的提问方法，以提高基于大语言模型（LLM）的几招学习（FSL）任务的性能。</li>
<li>methods: 该方法称为模型投票提示（MVP），它通过提交多个LLM执行同一任务，并将其结果进行多数投票来提高任务的性能。</li>
<li>results: 对一种稀有疾病识别和分类任务，MVP方法能够获得任务的改进结果，并且比单个模型 ensemble 的结果更佳。此外， authors 还发布了一个新的稀有疾病数据集，可供那些同意 MIMIC-IV 数据使用协议（DUA）的人使用。<details>
<summary>Abstract</summary>
The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases, affecting a small fraction of the population, inherently require FSL techniques due to limited data availability, though manual data collection and annotation is costly and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FSL, available to those who agreed to the MIMIC-IV Data Use Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple times, substantially increasing the time needed for manual annotation, and to address this, we assess the feasibility of using JSON for automating generative LLM evaluation.
</details>
<details>
<summary>摘要</summary>
大量生成语言模型（LLM）的出现强调了 precisionefficient的提示方法的需求。 LLM frequently applied in Few-Shot Learning（FSL）上下文中，在 minimal training data 下进行任务执行。 FSL 在许多人工智能（AI）子领域中得到普及，包括 AI for health。 rare diseases，affecting a small fraction of the population，inherently require FSL techniques due to limited data availability，although manual data collection and annotation is costly and time-consuming。在这篇论文中，我们提出 Models-Vote Prompting（MVP），一种 flexible prompting approach，用于改进 LLM 查询在 FSL 设置中的性能。 MVP 通过 prompting numerous LLMS 完成同一个任务，并 Then conducting a majority vote on the resulting outputs。这种方法可以提高任何一个模型 ensemble 中的表现，在一次性罕见疾病识别和分类任务中。我们还发布了一个新的罕见疾病数据集，可供那些同意 MIMIC-IV Data Use Agreement（DUA）。此外，在使用 MVP 时，每个模型都会被多次提示，这substantially increases the time needed for manual annotation，并且为了解决这个问题，我们评估了使用 JSON 自动生成 LLM 评估的可能性。
</details></li>
</ul>
<hr>
<h2 id="Inducing-Causal-Structure-for-Abstractive-Text-Summarization"><a href="#Inducing-Causal-Structure-for-Abstractive-Text-Summarization" class="headerlink" title="Inducing Causal Structure for Abstractive Text Summarization"></a>Inducing Causal Structure for Abstractive Text Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12888">http://arxiv.org/abs/2308.12888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lu Chen, Ruqing Zhang, Wei Huang, Wei Chen, Jiafeng Guo, Xueqi Cheng</li>
<li>for: 本研究旨在提高数据驱动抽象摘要模型的效果，通过强调 causal 关系而不是相关性。</li>
<li>methods: 我们引入了 Structural Causal Model (SCM)，假设文档和摘要中存在多个隐藏因素和非因果因素，用于捕捉文档和摘要的内容和风格。我们证明了在满足certain conditions下，我们可以通过适应训练数据来确定隐藏因素。基于这，我们提出了 Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq)，用于学习 causal 表示，以便寻求 causal 信息 для摘要生成。</li>
<li>results: 我们在两个常用的文本摘要数据集上进行了实验，结果显示了我们的方法的优势。<details>
<summary>Abstract</summary>
The mainstream of data-driven abstractive summarization models tends to explore the correlations rather than the causal relationships. Among such correlations, there can be spurious ones which suffer from the language prior learned from the training corpus and therefore undermine the overall effectiveness of the learned model. To tackle this issue, we introduce a Structural Causal Model (SCM) to induce the underlying causal structure of the summarization data. We assume several latent causal factors and non-causal factors, representing the content and style of the document and summary. Theoretically, we prove that the latent factors in our SCM can be identified by fitting the observed training data under certain conditions. On the basis of this, we propose a Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq) to learn the causal representations that can mimic the causal factors, guiding us to pursue causal information for summary generation. The key idea is to reformulate the Variational Auto-encoder (VAE) to fit the joint distribution of the document and summary variables from the training corpus. Experimental results on two widely used text summarization datasets demonstrate the advantages of our approach.
</details>
<details>
<summary>摘要</summary>
主流的数据驱动抽象摘要模型往往探索相关性而不是 causal 关系。其中的一些相关性可能受到训练集中的语言优先级影响，从而降低整体模型的效果。为解决这个问题，我们引入结构 causal 模型（SCM）来探索摘要数据的下面结构。我们假设了一些隐藏的 causal 因素和非 causal 因素，表示文档和摘要的内容和风格。理论上，我们证明了我们的 SCM 中的隐藏因素可以通过适应训练数据来被确定。基于这，我们提议一种 causality 激发 sequence-to-sequence 模型（CI-Seq2Seq）来学习 causal 表示，以便追求摘要中的 causal 信息。关键思想是将 Variational Autoencoder（VAE）改进来适应训练集中的 JOIN 分布。实验结果表明，我们的方法在两个常用的文本摘要数据集上具有优势。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/25/cs.AI_2023_08_25/" data-id="cloojsmak002tre88ck2qadz3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/25/cs.CL_2023_08_25/" class="article-date">
  <time datetime="2023-08-25T11:00:00.000Z" itemprop="datePublished">2023-08-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/25/cs.CL_2023_08_25/">cs.CL - 2023-08-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Qwen-VL-A-Frontier-Large-Vision-Language-Model-with-Versatile-Abilities"><a href="#Qwen-VL-A-Frontier-Large-Vision-Language-Model-with-Versatile-Abilities" class="headerlink" title="Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"></a>Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12966">http://arxiv.org/abs/2308.12966</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qwenlm/qwen-vl">https://github.com/qwenlm/qwen-vl</a></li>
<li>paper_authors: Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou</li>
<li>for: 这篇论文旨在描述一种大规模视语言模型系列（Qwen-VL），用于理解文本和图像。</li>
<li>methods: 该模型使用了新型的混合嵌入Space-Time Block Attention（STBAT）机制，以及一种基于自适应窗口的图像嵌入方法。</li>
<li>results: 论文表明，Qwen-VL系列模型在图像描述、问答、视觉定位等多个任务中表现出色，并且在零基础captioning、视觉或文档视觉问答等任务中具有优异表现。<details>
<summary>Abstract</summary>
We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.
</details>
<details>
<summary>摘要</summary>
我们介绍Qwen-VL系列，一组大规模的视觉语言模型，旨在理解和处理文本和图像。包括Qwen-VL和Qwen-VL-Chat两种模型，它们在图像描述、问答、视觉定位和自适应互动等任务中表现出色。评估范围涵盖零引入描述、视觉或文档视问题回答以及固定。我们显示Qwen-VL超越现有的大型视觉语言模型（LVLM）。我们介绍它们的架构、训练、能力和性能，强调它们在多媒体人工智能领域的贡献。可以在https://github.com/QwenLM/Qwen-VL获取代码、demo和模型。
</details></li>
</ul>
<hr>
<h2 id="Code-Llama-Open-Foundation-Models-for-Code"><a href="#Code-Llama-Open-Foundation-Models-for-Code" class="headerlink" title="Code Llama: Open Foundation Models for Code"></a>Code Llama: Open Foundation Models for Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12950">http://arxiv.org/abs/2308.12950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve</li>
<li>for: 这个论文是为了推出一个基于 Llama 2 的大语言模型 Code Llama，用于代码处理任务。</li>
<li>methods: 该论文使用了 Llama 2 作为基础，并通过不同的特定化和指令跟踪来提高模型的性能。</li>
<li>results: 论文表明，Code Llama 在多个代码测试 benchmark 上达到了当前开放模型的最佳性能，并且在某些情况下超过了 Llama 2 70B 的性能。<details>
<summary>Abstract</summary>
We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.
</details>
<details>
<summary>摘要</summary>
我们发布了 Code Llama，一家大型语言模型，用于程式码，基于 Llama 2 提供现代性能的开放模型，并具有填充功能、大型输入上下文支持和零代指令跟踪能力。我们提供多种版本，以覆盖广泛应用：基础模型（Code Llama）、Python 特化版本（Code Llama - Python）以及指令跟踪模型（Code Llama - Instruct），每个版本都有 7B、13B 和 34B 句子数的参数。所有模型都是在字串16k tokens的序列上训练，并在输入字串长度到100k tokens时显示改进。7B 和 13B Code Llama 和 Code Llama - Instruct 版本支持填充基于周围的内容。Code Llama 在开放模型中达到了一些程式码benchmark的州OF-THE-ART表现，其中包括 HumanEval 和 MBPP，分别得分53%和55%。特别是，Code Llama - Python 7B 超过 Llama 2 70B 的 HumanEval 和 MBPP 分数，并且我们的所有模型在 MultiPL-E 上超过每个公开的模型。我们发布 Code Llama 的授权是允许研究和商业用途的开放授权。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Document-Page-Classification-Design-Datasets-and-Challenges"><a href="#Beyond-Document-Page-Classification-Design-Datasets-and-Challenges" class="headerlink" title="Beyond Document Page Classification: Design, Datasets, and Challenges"></a>Beyond Document Page Classification: Design, Datasets, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12896">http://arxiv.org/abs/2308.12896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordy Van Landeghem, Sanket Biswas, Matthew B. Blaschko, Marie-Francine Moens</li>
<li>for: 本研究提出了将文档分类 benchmarking带到实际应用中，包括数据的性质（多通道、多页、多产业）和分类任务（多页文档、页流、文档集 classification）。</li>
<li>methods: 本研究识别了公共多页文档分类数据集的缺失，正规化了应用场景中的分类任务，并强调了完整文档表示的重要性。</li>
<li>results: 对于提posed的多页文档分类数据集进行了实验研究，发现现有的benchmarks已经失去了相关性，需要更新以评估实际中的完整文档。这也提出了更加成熟的评估方法，包括准确评估、时间-内存复杂度评估和各种实际分布转移（例如，生成vs扫描噪声、页码重构）。<details>
<summary>Abstract</summary>
This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distribution shifts (e.g., born-digital vs. scanning noise, shifting page order). Our study ends on a hopeful note by recommending concrete avenues for future improvements.}
</details>
<details>
<summary>摘要</summary>
An experimental study on proposed multi-page document classification datasets shows that current benchmarks are no longer relevant and need to be updated to evaluate complete documents as they naturally occur in practice. The study also highlights the need for more mature evaluation methodologies, including calibration evaluation, inference complexity (time-memory), and a range of realistic distribution shifts (e.g., born-digital vs. scanning noise, shifting page order). The paper concludes on a hopeful note by recommending concrete avenues for future improvements.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/25/cs.CL_2023_08_25/" data-id="cloojsmcq009xre88byy5fudi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/25/cs.LG_2023_08_25/" class="article-date">
  <time datetime="2023-08-25T10:00:00.000Z" itemprop="datePublished">2023-08-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/25/cs.LG_2023_08_25/">cs.LG - 2023-08-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NeuralClothSim-Neural-Deformation-Fields-Meet-the-Kirchhoff-Love-Thin-Shell-Theory"><a href="#NeuralClothSim-Neural-Deformation-Fields-Meet-the-Kirchhoff-Love-Thin-Shell-Theory" class="headerlink" title="NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory"></a>NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12970">http://arxiv.org/abs/2308.12970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navami Kairanda, Marc Habermann, Christian Theobalt, Vladislav Golyanik<br>for:这篇论文的目的是提出一种新的物理可能的布料模拟方法，使用薄shell理论来描述布料的表征和动态变化。methods:这篇论文使用了神经网络来学习布料的表征和动态变化，并使用了约束梯度下降来训练神经网络。results:实验结果表明，这种新的布料模拟方法可以具有高效的存储使用和可微的性，同时可以快速地实现布料的材质描述和模拟编辑。<details>
<summary>Abstract</summary>
Cloth simulation is an extensively studied problem, with a plethora of solutions available in computer graphics literature. Existing cloth simulators produce realistic cloth deformations that obey different types of boundary conditions. Nevertheless, their operational principle remains limited in several ways: They operate on explicit surface representations with a fixed spatial resolution, perform a series of discretised updates (which bounds their temporal resolution), and require comparably large amounts of storage. Moreover, back-propagating gradients through the existing solvers is often not straightforward, which poses additional challenges when integrating them into modern neural architectures. In response to the limitations mentioned above, this paper takes a fundamentally different perspective on physically-plausible cloth simulation and re-thinks this long-standing problem: We propose NeuralClothSim, i.e., a new cloth simulation approach using thin shells, in which surface evolution is encoded in neural network weights. Our memory-efficient and differentiable solver operates on a new continuous coordinate-based representation of dynamic surfaces, i.e., neural deformation fields (NDFs); it supervises NDF evolution with the rules of the non-linear Kirchhoff-Love shell theory. NDFs are adaptive in the sense that they 1) allocate their capacity to the deformation details as the latter arise during the cloth evolution and 2) allow surface state queries at arbitrary spatial and temporal resolutions without retraining. We show how to train our NeuralClothSim solver while imposing hard boundary conditions and demonstrate multiple applications, such as material interpolation and simulation editing. The experimental results highlight the effectiveness of our formulation and its potential impact.
</details>
<details>
<summary>摘要</summary>
cloth 模拟是一个广泛研究的问题，计算机图形文献中有很多解决方案。现有的布料模拟器可以生成真实的布料变形，但它们的运作原理受到一些限制：它们在固定的空间分辨率上进行显式表面表示，执行一系列精炼的更新（这限制了它们的时间分辨率），并需要相对较大的存储量。此外，通过现有的解决方案来归档梯度的操作也不直观，这会增加将它们集成到现代神经网络架构时的挑战。面对这些限制，这篇论文采用了一种新的思路来解决布料模拟问题：我们提出了基于薄shell的新布料模拟方法，即NeuralClothSim。我们的方法使用神经网络权重来编码表面的进化，并且使用约束 Kirchhoff-Love 薄shell理论来监督神经变换场（NDF）的演化。NDF是可适应的，即它们会根据布料演化中的变化分配其容量，并且允许在任何空间和时间分辨率上进行表面状态的查询无需重新训练。我们示出了如何在强制边界条件下训练我们的NeuralClothSim解决方案，并展示了多种应用，如材料 interpolate 和模拟编辑。实验结果表明了我们的方法的有效性和潜在影响。
</details></li>
</ul>
<hr>
<h2 id="NeO-360-Neural-Fields-for-Sparse-View-Synthesis-of-Outdoor-Scenes"><a href="#NeO-360-Neural-Fields-for-Sparse-View-Synthesis-of-Outdoor-Scenes" class="headerlink" title="NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes"></a>NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12967">http://arxiv.org/abs/2308.12967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zubair-irshad/NeO-360">https://github.com/zubair-irshad/NeO-360</a></li>
<li>paper_authors: Muhammad Zubair Irshad, Sergey Zakharov, Katherine Liu, Vitor Guizilini, Thomas Kollar, Adrien Gaidon, Zsolt Kira, Rares Ambrus</li>
<li>for: 这个论文旨在解决现有的视点合成方法需要费力的每个景象优化问题，以便应用于实际的无限无限的城市环境中， где对象或背景只有几个视角可见。</li>
<li>methods: 我们介绍了一种新的方法 called NeO 360，它使用神经场来实现 sparse view synthesis of outdoor scenes。该方法可以从单个或几个颜色图像中重建360度场景。</li>
<li>results: 我们的实验表明，NeO 360 可以在 NeRDS 360 提出的挑战性 datasets 上表现出色，并且在新的视角和原始场景中都能够得到高质量的结果。此外，NeO 360 还提供了编辑和组合功能。<details>
<summary>Abstract</summary>
Recent implicit neural representations have shown great results for novel view synthesis. However, existing methods require expensive per-scene optimization from many views hence limiting their application to real-world unbounded urban settings where the objects of interest or backgrounds are observed from very few views. To mitigate this challenge, we introduce a new approach called NeO 360, Neural fields for sparse view synthesis of outdoor scenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenes from a single or a few posed RGB images. The essence of our approach is in capturing the distribution of complex real-world outdoor 3D scenes and using a hybrid image-conditional triplanar representation that can be queried from any world point. Our representation combines the best of both voxel-based and bird's-eye-view (BEV) representations and is more effective and expressive than each. NeO 360's representation allows us to learn from a large collection of unbounded 3D scenes while offering generalizability to new views and novel scenes from as few as a single image during inference. We demonstrate our approach on the proposed challenging 360{\deg} unbounded dataset, called NeRDS 360, and show that NeO 360 outperforms state-of-the-art generalizable methods for novel view synthesis while also offering editing and composition capabilities. Project page: https://zubair-irshad.github.io/projects/neo360.html
</details>
<details>
<summary>摘要</summary>
最近的隐式神经表示法已经达到了对novel view synthesis的出色成绩。然而，现有的方法需要费时且费力地从多个视角优化，从而限制了它们在实际世界无限大的城市设置中的应用。为解决这个挑战，我们提出了一种新的方法called NeO 360，即神经场 для缺省视图Synthesis of outdoor scenes。NeO 360是一种通用的方法，可以从单个或几个RGB图像中重construct 360度场景。我们的方法的核心思想是捕捉复杂的实际户外3D场景的分布，并使用一种混合图像 conditioned triplanar表示，可以从任何世界点进行查询。我们的表示结合了 voxel-based和bird's-eye-view（BEV）表示的优点，并且在表示效果和表达力方面比每一种更高。NeO 360的表示允许我们从大量的无限大3D场景中学习，并在推理时对新视图和新场景进行普适化。我们在提出的challenging 360度无限大数据集，即NeRDS 360上进行了证明，并表明NeO 360在对novel view synthesis的推理中超越了当前最佳的通用方法，同时也提供了编辑和组合功能。项目页面：https://zubair-irshad.github.io/projects/neo360.html
</details></li>
</ul>
<hr>
<h2 id="Scenimefy-Learning-to-Craft-Anime-Scene-via-Semi-Supervised-Image-to-Image-Translation"><a href="#Scenimefy-Learning-to-Craft-Anime-Scene-via-Semi-Supervised-Image-to-Image-Translation" class="headerlink" title="Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation"></a>Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12968">http://arxiv.org/abs/2308.12968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuxinn-j/scenimefy">https://github.com/yuxinn-j/scenimefy</a></li>
<li>paper_authors: Yuxin Jiang, Liming Jiang, Shuai Yang, Chen Change Loy</li>
<li>for: 高质量动漫场景自实际图像渲染</li>
<li>methods: 使用结构保持 pseudo 对应数据引导学习，利用 CLIP 富有模型先验，并应用 segmentation-guided 数据选择，以提高准确性和细节。</li>
<li>results: 比前一代基eline表现出较高的both perceptual quality和量化性能。<details>
<summary>Abstract</summary>
Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fine details. Besides, we contribute a high-resolution anime scene dataset to facilitate future research. Our extensive experiments demonstrate the superiority of our method over state-of-the-art baselines in terms of both perceptual quality and quantitative performance.
</details>
<details>
<summary>摘要</summary>
自动高质量渲染动漫场景从复杂实际图像是实际上具有重要的实践价值。这些挑战包括场景复杂度、动漫风格独特性和领域域之间的数据域隔。尽管有承诺的尝试，过去的尝试仍然无法达到满意的结果，包括持续性的semantic preserve，证明性的风格化和细节。在本研究中，我们提出了Sceneimefy，一种新的半指导性图像-图像翻译框架。我们的方法利用结构一致的 pseudo paired数据来引导学习，从而简化了纯无监督的设定。 pseudo数据通过基于CLIP的semantic-constrained StyleGAN得到，并应用了 segmentation-guided data selection来获得高质量的pseudo超级vision。此外，我们还提供了一个高分辨率动漫场景集，以便未来的研究。我们的广泛的实验表明，我们的方法在比较现有基eline上方面具有superiority， both perceived quality和量化性能。
</details></li>
</ul>
<hr>
<h2 id="Dense-Text-to-Image-Generation-with-Attention-Modulation"><a href="#Dense-Text-to-Image-Generation-with-Attention-Modulation" class="headerlink" title="Dense Text-to-Image Generation with Attention Modulation"></a>Dense Text-to-Image Generation with Attention Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12964">http://arxiv.org/abs/2308.12964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naver-ai/densediffusion">https://github.com/naver-ai/densediffusion</a></li>
<li>paper_authors: Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, Jun-Yan Zhu</li>
<li>for: 处理细致的文本描述，生成真实的图像。</li>
<li>methods: 利用预训练的文本到图像模型，通过 Layout 指导对象在具体的区域出现。</li>
<li>results: 不需要再训练或数据集，可以根据文本描述提高图像生成效果，并且与具体的 Layout 条件下的模型效果相似。<details>
<summary>Abstract</summary>
Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images' layouts and the pre-trained model's intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.
</details>
<details>
<summary>摘要</summary>
现有的文本到图像扩散模型很难以生成具有细致描述的图像，每个文本提示都提供了特定图像区域的详细描述。为解决这个问题，我们提议了DenseDiffusion，一种不需要训练的方法，可以使用预训练的文本到图像模型来处理这些细致的文本提示，同时提供场景布局控制。我们首先分析生成图像的布局和预训练模型的中间注意力地图之间的关系。然后，我们开发了一种注意力调节方法，可以根据布局指导对象出现在特定区域中。不需要额外的训练或数据集，我们在给出细致文本提示时改进了图像生成性能， Regarding both automatic and human evaluation scores.此外，我们可以通过 specifically 将模型训练在场景条件下，实现类似的视觉效果。
</details></li>
</ul>
<hr>
<h2 id="DLIP-Distilling-Language-Image-Pre-training"><a href="#DLIP-Distilling-Language-Image-Pre-training" class="headerlink" title="DLIP: Distilling Language-Image Pre-training"></a>DLIP: Distilling Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12956">http://arxiv.org/abs/2308.12956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huafeng Kuang, Jie Wu, Xiawu Zheng, Ming Li, Xuefeng Xiao, Rui Wang, Min Zheng, Rongrong Ji<br>for:本研究旨在提出一种简单 yet efficient的语言图像预训练框架（DLIP），以实现快速、高效地压缩VLP模型。methods:本研究采用了多维度的模型压缩方法，包括不同模块的建筑特征和不同模式之间的信息传递。results:实验结果显示，DLIP可以在多种跨模态任务中实现最佳的精度&#x2F;效率质量比，如图文检索、图文captioning和视觉问答。例如，DLIP可以压缩BLIP模型1.9倍，从213M参数压缩到108M参数，而且与教师模型的性能相似或更好。此外，DLIP可以保留95%以上的性能，使用22.4%的参数和24.8%的FLOPs，并提高执行速度2.7倍。<details>
<summary>Abstract</summary>
Vision-Language Pre-training (VLP) shows remarkable progress with the assistance of extremely heavy parameters, which challenges deployment in real applications. Knowledge distillation is well recognized as the essential procedure in model compression. However, existing knowledge distillation techniques lack an in-depth investigation and analysis of VLP, and practical guidelines for VLP-oriented distillation are still not yet explored. In this paper, we present DLIP, a simple yet efficient Distilling Language-Image Pre-training framework, through which we investigate how to distill a light VLP model. Specifically, we dissect the model distillation from multiple dimensions, such as the architecture characteristics of different modules and the information transfer of different modalities. We conduct comprehensive experiments and provide insights on distilling a light but performant VLP model. Experimental results reveal that DLIP can achieve a state-of-the-art accuracy/efficiency trade-off across diverse cross-modal tasks, e.g., image-text retrieval, image captioning and visual question answering. For example, DLIP compresses BLIP by 1.9x, from 213M to 108M parameters, while achieving comparable or better performance. Furthermore, DLIP succeeds in retaining more than 95% of the performance with 22.4% parameters and 24.8% FLOPs compared to the teacher model and accelerates inference speed by 2.7x.
</details>
<details>
<summary>摘要</summary>
美化语言预训练（VLP）显示了惊人的进步，却面临实际应用中的部署挑战。知识储存是识别为模型压缩的关键过程。然而，现有的知识储存技术没有对VLP进行深入的研究和分析，也没有提供VLP-oriented储存的实用指南。在这篇论文中，我们提出了一个简单 yet efficient的Distilling Language-Image Pre-training框架（DLIP），以 investigate如何压缩一个轻量级VLP模型。我们从多个维度进行模型压缩，包括不同模块的建筑特征和不同Modalities之间的信息传递。我们进行了全面的实验，并提供了压缩轻量级VLP模型的深入分析。实验结果表明，DLIP可以在多个跨模态任务中实现状态机器人的精度/效率质量比，如图文检索、图文描述和视觉问答。例如，DLIP可以将BLIP压缩为1.9倍，从213M Parameters减少到108M Parameters，同时保持与教师模型的相同或更好的性能。此外，DLIP可以保留95%以上的性能，使用22.4% Parameters和24.8% FLOPs，相比教师模型快速加速执行速度2.7倍。
</details></li>
</ul>
<hr>
<h2 id="BridgeData-V2-A-Dataset-for-Robot-Learning-at-Scale"><a href="#BridgeData-V2-A-Dataset-for-Robot-Learning-at-Scale" class="headerlink" title="BridgeData V2: A Dataset for Robot Learning at Scale"></a>BridgeData V2: A Dataset for Robot Learning at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12952">http://arxiv.org/abs/2308.12952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rail-berkeley/BridgeData-V2">https://github.com/rail-berkeley/BridgeData-V2</a></li>
<li>paper_authors: Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-Estruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, Sergey Levine</li>
<li>For: The paper is written for researchers in the field of robotic manipulation, particularly those interested in scalable robot learning.* Methods: The paper uses a large and diverse dataset of robotic manipulation behaviors, called BridgeData V2, to facilitate research on scalable robot learning. The dataset contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot, and is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions.* Results: The paper reports on the results of training six state-of-the-art imitation learning and offline reinforcement learning methods on the BridgeData V2 dataset, and finds that these methods succeed on a suite of tasks requiring varying amounts of generalization. The paper also demonstrates that the performance of these methods improves with more data and higher capacity models, and that training on a greater variety of skills leads to improved generalization.<details>
<summary>Abstract</summary>
We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that training on a greater variety of skills leads to improved generalization. By publicly sharing BridgeData V2 and our pre-trained models, we aim to accelerate research in scalable robot learning methods. Project page at https://rail-berkeley.github.io/bridgedata
</details>
<details>
<summary>摘要</summary>
我们介绍 BridgeData V2，一个大型和多样化的机器人 manipulate 行为数据集，用于促进机器人学习扩展。 BridgeData V2 包含 60,096 条路径，在 24 个环境中收集到，这个数据集提供了广泛的任务和环境多样性，从而实现了在不同环境、领域和机构中实现数据可重用性。此外，这个数据集适用于广泛的开放词汇、多任务学习方法，以图像目标或自然语言指令为条件。在我们的实验中，我们将 6 种现代机器人模仿学习和离线强化学习方法训练在我们的数据集上，发现这些方法在一系列需要不同量的数据和模型容量的任务上成功。我们还证明了这些方法在更多的数据和更高的模型容量下表现更好，以及训练更多的技能将导致更好的数据可重用性。我们通过公开 BridgeData V2 和我们的预训模型，希望能够推动机器人学习方法的扩展。更多信息请参考https://rail-berkeley.github.io/bridgedata。
</details></li>
</ul>
<hr>
<h2 id="Label-Budget-Allocation-in-Multi-Task-Learning"><a href="#Label-Budget-Allocation-in-Multi-Task-Learning" class="headerlink" title="Label Budget Allocation in Multi-Task Learning"></a>Label Budget Allocation in Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12949">http://arxiv.org/abs/2308.12949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ximeng Sun, Kihyuk Sohn, Kate Saenko, Clayton Mellina, Xiao Bian</li>
<li>for: 提高机器学习系统的性能，因为标签数据的成本限制了系统的性能。</li>
<li>methods: 提出了一种名为多任务学习中的标签预算分配问题，并正式定义了这个问题。并通过实验表明，不同的预算分配策略对多任务学习的性能有很大的影响。我们提出了一种适应任务的预算分配算法，可以在不同的多任务学习设置下生成最佳的预算分配策略。</li>
<li>results: 我们的方法可以在PASCAL VOC和Taskonomy上实现优于其他常用的标签分配策略的性能。<details>
<summary>Abstract</summary>
The cost of labeling data often limits the performance of machine learning systems. In multi-task learning, related tasks provide information to each other and improve overall performance, but the label cost can vary among tasks. How should the label budget (i.e. the amount of money spent on labeling) be allocated among different tasks to achieve optimal multi-task performance? We are the first to propose and formally define the label budget allocation problem in multi-task learning and to empirically show that different budget allocation strategies make a big difference to its performance. We propose a Task-Adaptive Budget Allocation algorithm to robustly generate the optimal budget allocation adaptive to different multi-task learning settings. Specifically, we estimate and then maximize the extent of new information obtained from the allocated budget as a proxy for multi-task learning performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy of our approach over other widely used heuristic labeling strategies.
</details>
<details>
<summary>摘要</summary>
Machine learning系统的标签成本 oftentimeslimits its performance. In multi-task learning, related tasks can provide information to each other and improve overall performance, but the label cost can vary among tasks. How should the label budget (i.e. the amount of money spent on labeling) be allocated among different tasks to achieve optimal multi-task performance? We are the first to propose and formally define the label budget allocation problem in multi-task learning and to empirically show that different budget allocation strategies make a big difference to its performance. We propose a Task-Adaptive Budget Allocation algorithm to robustly generate the optimal budget allocation adaptive to different multi-task learning settings. Specifically, we estimate and then maximize the extent of new information obtained from the allocated budget as a proxy for multi-task learning performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy of our approach over other widely used heuristic labeling strategies.Here's the breakdown of the translation:* Machine learning系统 (机器学习系统) - This is the Chinese term for "machine learning system".* 标签成本 (标签成本) - This is the Chinese term for "label cost".* multi-task learning (多任务学习) - This is the Chinese term for "multi-task learning".* 不同任务 (不同任务) - This is the Chinese term for "different tasks".* 如何分配标签预算 (如何分配标签预算) - This is the Chinese term for "how to allocate the label budget".* 达到最佳多任务性能 (达到最佳多任务性能) - This is the Chinese term for "achieve optimal multi-task performance".* 我们是第一个 (我们是第一个) - This is the Chinese term for "we are the first".* 提出和正式定义标签预算分配问题 (提出和正式定义标签预算分配问题) - This is the Chinese term for "propose and formally define the label budget allocation problem".* 其实际效果 (其实际效果) - This is the Chinese term for "its practical effect".* 多任务学习设置 (多任务学习设置) - This is the Chinese term for "multi-task learning settings".* 适应任务 (适应任务) - This is the Chinese term for "adaptive to different tasks".* 新信息量 (新信息量) - This is the Chinese term for "new information quantity".* 作为多任务学习性能的代理 (作为多任务学习性能的代理) - This is the Chinese term for "as a proxy for multi-task learning performance".* 我们提议的Task-Adaptive Budget Allocation算法 (我们提议的Task-Adaptive Budget Allocation算法) - This is the Chinese term for "our proposed Task-Adaptive Budget Allocation algorithm".* 可以Robustly生成优化的标签预算分配 (可以Robustly生成优化的标签预算分配) - This is the Chinese term for "can robustly generate optimized label budget allocation".* PASCAL VOC和Taskonomy (PASCAL VOC和Taskonomy) - These are the Chinese terms for "PASCAL VOC and Taskonomy".* 实验证明 (实验证明) - This is the Chinese term for "experiments demonstrate".* 其他常用的标签分配策略 (其他常用的标签分配策略) - This is the Chinese term for "other commonly used labeling strategies".
</details></li>
</ul>
<hr>
<h2 id="Learning-Only-On-Boundaries-a-Physics-Informed-Neural-operator-for-Solving-Parametric-Partial-Differential-Equations-in-Complex-Geometries"><a href="#Learning-Only-On-Boundaries-a-Physics-Informed-Neural-operator-for-Solving-Parametric-Partial-Differential-Equations-in-Complex-Geometries" class="headerlink" title="Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries"></a>Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12939">http://arxiv.org/abs/2308.12939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwei Fang, Sifan Wang, Paris Perdikaris</li>
<li>for: 解决 Parametrized boundary value problems without labeled data.</li>
<li>methods: 使用 Physics-informed neural operator 方法，通过将 PDE 转化为 boundary integral equations (BIEs)，可以在边界上训练操作网络，而不需要大量标注数据。</li>
<li>results: 可以处理复杂的参数化几何和无穷大问题，并且比现有的 PINNs 和 neural operators 更快速。<details>
<summary>Abstract</summary>
Recently deep learning surrogates and neural operators have shown promise in solving partial differential equations (PDEs). However, they often require a large amount of training data and are limited to bounded domains. In this work, we present a novel physics-informed neural operator method to solve parametrized boundary value problems without labeled data. By reformulating the PDEs into boundary integral equations (BIEs), we can train the operator network solely on the boundary of the domain. This approach reduces the number of required sample points from $O(N^d)$ to $O(N^{d-1})$, where $d$ is the domain's dimension, leading to a significant acceleration of the training process. Additionally, our method can handle unbounded problems, which are unattainable for existing physics-informed neural networks (PINNs) and neural operators. Our numerical experiments show the effectiveness of parametrized complex geometries and unbounded problems.
</details>
<details>
<summary>摘要</summary>
最近，深度学习代理和神经操作已经在解偏微分方程（PDEs）中表现出了承诺。然而，它们经常需要大量的训练数据，并且受到固定域的限制。在这个工作中，我们提出了一种新的物理学习神经操作方法，用于解偏微分方程的参数化边值问题。我们将PDEs转化为边 интеграル方程（BIEs），因此我们可以在域的边上训练操作网络，不需要大量的标注数据。这种方法可以减少训练过程中需要的样本点数量从$O(N^d)$减少到$O(N^{d-1})$，其中$d$是域的维度，这导致训练过程的加速。此外，我们的方法还可以处理无界问题，这些问题对现有的物理学习神经网络（PINNs）和神经操作都是不可能的。我们的数学实验表明，参数化复杂的几何和无界问题的效果。
</details></li>
</ul>
<hr>
<h2 id="Low-count-Time-Series-Anomaly-Detection"><a href="#Low-count-Time-Series-Anomaly-Detection" class="headerlink" title="Low-count Time Series Anomaly Detection"></a>Low-count Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12925">http://arxiv.org/abs/2308.12925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Renz, Kurt Cutajar, Niall Twomey, Gavin K. C. Cheung, Hanting Xie</li>
<li>For: 这篇论文是为了解决低频时间序列中的异常检测问题，特别是在大规模在线平台上监测和记录多种数据类型时，遇到的几个独特挑战。* Methods: 该论文引入了一种新的生成过程，用于创建含有异常段的低频时间序列的基本数据集。通过理论和实验分析，论文解释了现有算法在这些设置下的缺陷，以及如何使用异常分数平滑化来改进性能。* Results: 该论文通过使用实际数据 validate了其分析和建议的实用性，并在一个实际的零售店销售数据集上证明了异常分数平滑化的作用。<details>
<summary>Abstract</summary>
Low-count time series describe sparse or intermittent events, which are prevalent in large-scale online platforms that capture and monitor diverse data types. Several distinct challenges surface when modelling low-count time series, particularly low signal-to-noise ratios (when anomaly signatures are provably undetectable), and non-uniform performance (when average metrics are not representative of local behaviour). The time series anomaly detection community currently lacks explicit tooling and processes to model and reliably detect anomalies in these settings. We address this gap by introducing a novel generative procedure for creating benchmark datasets comprising of low-count time series with anomalous segments. Via a mixture of theoretical and empirical analysis, our work explains how widely-used algorithms struggle with the distribution overlap between normal and anomalous segments. In order to mitigate this shortcoming, we then leverage our findings to demonstrate how anomaly score smoothing consistently improves performance. The practical utility of our analysis and recommendation is validated on a real-world dataset containing sales data for retail stores.
</details>
<details>
<summary>摘要</summary>
低频时序描述稀疏或间歇性事件，这些事件在大规模在线平台上采集和监测多种数据类型中很普遍。模型低频时序时，有几个明显的挑战，特别是低信号噪声比（畸变签识不可避免）和非均匀性（平均指标不是本地行为的代表）。时序异常检测社区目前缺乏专门的工具和过程来模型和可靠地检测异常情况。我们填补这个空白，引入了一种新的生成过程，用于创建包含低频时序异常段的 Referenz datasets。通过理论和实验分析，我们解释了广泛使用的算法在正常和畸变段之间的分布重叠问题。为了解决这个缺陷，我们then 利用我们的发现，示出了如何使用异常得分平滑来提高性能。我们的分析和建议在实际的零售店销售数据集上进行验证，证明了我们的方法的实用性。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Distributed-Multi-Agent-Reinforcement-Learning-for-EV-Charging-Network-Control"><a href="#An-Efficient-Distributed-Multi-Agent-Reinforcement-Learning-for-EV-Charging-Network-Control" class="headerlink" title="An Efficient Distributed Multi-Agent Reinforcement Learning for EV Charging Network Control"></a>An Efficient Distributed Multi-Agent Reinforcement Learning for EV Charging Network Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12921">http://arxiv.org/abs/2308.12921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Shojaeighadikolaei, Morteza Hashemi</li>
<li>For: The paper aims to develop an effective EV charging controller to mitigate the risk of transformer overload in the distribution grid, with a focus on preserving privacy for EV owners.* Methods: The authors propose a decentralized Multi-agent Reinforcement Learning (MARL) charging framework, employing the Centralized Training Decentralized Execution-Deep Deterministic Policy Gradient (CTDE-DDPG) scheme to provide valuable information to users during training while maintaining privacy during execution.* Results: The CTDE framework improves the performance of the charging network by reducing network costs, and reduces the Peak-to-Average Ratio (PAR) of the total demand, which in turn reduces the risk of transformer overload during peak hours.Here’s the Chinese translation of the three points:* For: 这篇论文目标是为了减少分布网络中变压器过载的风险，同时保持电动车所有者的隐私。* Methods: 作者提出了一种分布式多代理学习（MARL）充电控制器，使用中央训练分布执行-深度束缚策略 Gradient（CTDE-DDPG）算法，以提供训练过程中有价值信息，而执行过程中保持隐私。* Results: CTDE framwork可以提高充电网络的性能，降低网络成本，并降低总需求的峰值强度（PAR），从而减少变压器过载的风险。<details>
<summary>Abstract</summary>
The increasing trend in adopting electric vehicles (EVs) will significantly impact the residential electricity demand, which results in an increased risk of transformer overload in the distribution grid. To mitigate such risks, there are urgent needs to develop effective EV charging controllers. Currently, the majority of the EV charge controllers are based on a centralized approach for managing individual EVs or a group of EVs. In this paper, we introduce a decentralized Multi-agent Reinforcement Learning (MARL) charging framework that prioritizes the preservation of privacy for EV owners. We employ the Centralized Training Decentralized Execution-Deep Deterministic Policy Gradient (CTDE-DDPG) scheme, which provides valuable information to users during training while maintaining privacy during execution. Our results demonstrate that the CTDE framework improves the performance of the charging network by reducing the network costs. Moreover, we show that the Peak-to-Average Ratio (PAR) of the total demand is reduced, which, in turn, reduces the risk of transformer overload during the peak hours.
</details>
<details>
<summary>摘要</summary>
随着电动车（EV）的普及趋势，它将对分布网络的住宅电力需求产生重要影响，从而增加分布网络的变压器负荷风险。为了缓解这些风险，有紧迫需要开发有效的EV充电控制器。目前，大多数EV充电控制器采用中央化的方法来管理个体EV或一组EV。在这篇论文中，我们介绍了一种分布式多智能体学习（MARL）充电框架，这种框架强调保护电动车所有者的隐私。我们采用了中央训练、分布执行-深度决定策函数（CTDE-DDPG）方案，这种方案在训练过程中提供了有价值的信息，同时在执行过程中保持隐私。我们的结果表明，CTDE框架可以改善充电网络的性能，同时还可以降低总需求的峰值至平均值比（PAR），从而降低变压器负荷风险 durante las horas pico。
</details></li>
</ul>
<hr>
<h2 id="Towards-Realistic-Unsupervised-Fine-tuning-with-CLIP"><a href="#Towards-Realistic-Unsupervised-Fine-tuning-with-CLIP" class="headerlink" title="Towards Realistic Unsupervised Fine-tuning with CLIP"></a>Towards Realistic Unsupervised Fine-tuning with CLIP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12919">http://arxiv.org/abs/2308.12919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Liang, Lijun Sheng, Zhengbo Wang, Ran He, Tieniu Tan</li>
<li>for: 这个研究旨在应用CLIPvision-language模型（VLM）进行下游超vised学习任务中。</li>
<li>methods: 这篇文章提出了一个实用的、有效的 Fine-tuning方法，即Universal Entropy Optimization（UEO），它利用样本水平的信任度来紧急降低具有信任度的instances的conditional entropy，并将less confident instances的marginal entropy提高。</li>
<li>results: 这篇文章透过15个领域和4种不同的专业知识进行了广泛的实验，结果显示，UEO方法在泛化和out-of-distribution检测方面都大大超过了基eline方法。<details>
<summary>Abstract</summary>
The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.   To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entropy of less confident instances. Apart from optimizing the textual prompts, UEO also incorporates optimization of channel-wise affine transformations within the visual branch of CLIP. Through extensive experiments conducted across 15 domains and 4 different types of prior knowledge, we demonstrate that UEO surpasses baseline methods in terms of both generalization and out-of-distribution detection.
</details>
<details>
<summary>摘要</summary>
随着视力语言模型（VLM）的出现，如CLIP，研究人员努力将其应用于下游指导学习任务。 Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we explore a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data may contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entropy of less confident instances. Apart from optimizing the textual prompts, UEO also incorporates optimization of channel-wise affine transformations within the visual branch of CLIP. Through extensive experiments conducted across 15 domains and 4 different types of prior knowledge, we demonstrate that UEO surpasses baseline methods in terms of both generalization and out-of-distribution detection.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Vulnerabilities-in-ML-systems-in-terms-of-adversarial-attacks"><a href="#Evaluating-the-Vulnerabilities-in-ML-systems-in-terms-of-adversarial-attacks" class="headerlink" title="Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks"></a>Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12918">http://arxiv.org/abs/2308.12918</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Harshith, Mantej Singh Gill, Madhan Jothimani</li>
<li>for: 本研究探讨了最新的 adversarial 攻击方法，以及它们对当前深度学习网络防御系统的挑战。</li>
<li>methods: 本研究使用了various methods to explore the consequences of vulnerabilities in AI systems, including discussing how they might arise, differences between randomized and adversarial examples, and potential ethical implications.</li>
<li>results: 本研究发现了一些新的 adversarial 攻击方法，以及它们对当前防御系统的影响。同时，研究还提出了一些建议，以帮助在测试阶段 обу练 AI 系统，以便在更广泛的应用中使用。<details>
<summary>Abstract</summary>
There have been recent adversarial attacks that are difficult to find. These new adversarial attacks methods may pose challenges to current deep learning cyber defense systems and could influence the future defense of cyberattacks. The authors focus on this domain in this research paper. They explore the consequences of vulnerabilities in AI systems. This includes discussing how they might arise, differences between randomized and adversarial examples and also potential ethical implications of vulnerabilities. Moreover, it is important to train the AI systems appropriately when they are in testing phase and getting them ready for broader use.
</details>
<details>
<summary>摘要</summary>
近些时候出现了困难发现的对抗攻击。这些新的对抗攻击方法可能会对当前的深度学习网络防御系统 pose 挑战，并可能影响未来网络攻击防御。作者在这篇研究论文中关注这个领域。他们探讨了人工智能系统的漏洞的后果，包括对随机化和对抗示例的区别，以及漏洞的可能性的伦理问题。此外，在测试阶段，我们需要适当地训练AI系统，以便在更广泛的应用中准备它们。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="POLCA-Power-Oversubscription-in-LLM-Cloud-Providers"><a href="#POLCA-Power-Oversubscription-in-LLM-Cloud-Providers" class="headerlink" title="POLCA: Power Oversubscription in LLM Cloud Providers"></a>POLCA: Power Oversubscription in LLM Cloud Providers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12908">http://arxiv.org/abs/2308.12908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam, Ricardo Bianchini</li>
<li>for: 这个论文主要是针对大型自然语言模型（LLM）的创新和其多种应用场景带来的数据中心GPU的 compute capacity 需求，以及这些新工作负载对于数据中心的电力资源带来的挑战。</li>
<li>methods: 论文使用了对多种 LLM 的描述和配置的电力消耗模式进行了广泛的测量和分析，并识别了推导和训练过程中电力消耗的区别。</li>
<li>results: 论文发现，在推导和训练过程中， LLM 集群的平均和峰值电力使用率不高，并且可以通过实现更好的电力调度来提高数据中心的电力效率，增加可部署的服务器数量，并缩短部署时间。论文还提出了一个名为 POLCA 的可靠和可重复的电力调度机制，可以在 GPU 集群中实现更高的部署效率和可靠性。<details>
<summary>Abstract</summary>
Recent innovation in large language models (LLMs), and their myriad use-cases have rapidly driven up the compute capacity demand for datacenter GPUs. Several cloud providers and other enterprises have made substantial plans of growth in their datacenters to support these new workloads. One of the key bottleneck resources in datacenters is power, and given the increasing model sizes of LLMs, they are becoming increasingly power intensive. In this paper, we show that there is a significant opportunity to oversubscribe power in LLM clusters. Power oversubscription improves the power efficiency of these datacenters, allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow.   We extensively characterize the power consumption patterns of a variety of LLMs and their configurations. We identify the differences between the inference and training power consumption patterns. Based on our analysis of these LLMs, we claim that the average and peak power utilization in LLM clusters for inference should not be very high. Our deductions align with the data from production LLM clusters, revealing that inference workloads offer substantial headroom for power oversubscription. However, the stringent set of telemetry and controls that GPUs offer in a virtualized environment, makes it challenging to have a reliable and robust power oversubscription mechanism.   We propose POLCA, our framework for power oversubscription that is robust, reliable, and readily deployable for GPU clusters. Using open-source models to replicate the power patterns observed in production, we simulate POLCA and demonstrate that we can deploy 30% more servers in the same GPU cluster for inference, with minimal performance loss
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）的创新和多种应用场景，快速提高了数据中心GPU的计算能力需求。许多云提供商和企业在数据中心进行了大规模的扩展计划以支持这些新型应用。数据中心内部的一个关键瓶颈资源是电力，而随着LLM的模型Size不断增大，它们变得越来越占用电力。在这篇论文中，我们表明了数据中心LLM团群中的电力投入可以进行副作用。副作用提高了数据中心的电力效率，allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow。我们对各种LLM的多种配置进行了广泛的电力消耗特征分析。我们发现了推理和训练两种不同的电力消耗模式。根据我们对LLM的分析，我们认为LLM团群的平均和峰值电力利用率在推理任务上不应该很高。我们的结论与生产环境中LLM团群的数据相一致，表明推理任务提供了大量的副作用空间。然而，GPU在虚拟环境中提供的严格的测量和控制机制，使得实现可靠和可Robust的副作用机制变得具有挑战。我们提出了POLCA，我们的可靠可Robust的副作用框架。使用开源模型来复制生产环境中的电力模式，我们在POLCA中进行了模拟，并证明可以在同一GPU团群中部署30%更多的服务器，并且减少性能损失。
</details></li>
</ul>
<hr>
<h2 id="CDAN-Convolutional-Dense-Attention-guided-Network-for-Low-light-Image-Enhancement"><a href="#CDAN-Convolutional-Dense-Attention-guided-Network-for-Low-light-Image-Enhancement" class="headerlink" title="CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement"></a>CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12902">http://arxiv.org/abs/2308.12902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Shakibania, Sina Raoufi, Hassan Khotanlou</li>
<li>for: 增强低光照图像，解决低光照图像的降低清晰度、降低颜色强度和减少细节等问题，以便进行准确的分析和解释。</li>
<li>methods: 该文献提出了一种基于自编码器的Convolutional Dense Attention-guided Network（CDAN），包括卷积和密集块，以及注意力机制和跳过连接。这种架构确保了信息的有效传播和特征学习。</li>
<li>results: 该文献的方法在低光照图像增强 tasks中表现出了remarkable进步，与当前最佳结果相比，在多种复杂的低光照场景中表现出了robustness。<details>
<summary>Abstract</summary>
Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs remarkably on benchmark datasets, effectively mitigating under-exposure and proficiently restoring textures and colors in diverse low-light scenarios. This achievement underscores CDAN's potential for diverse computer vision tasks, notably enabling robust object detection and recognition in challenging low-light conditions.
</details>
<details>
<summary>摘要</summary>
低光照图像，受到不足照明的影响，会呈现出降低清晰度、抑制颜色、减少细节等问题。低光照图像改善是计算机视觉中的一项重要任务，旨在通过提高亮度、对比度和总体观察质量来使图像更加清晰，以便更加准确地分析和解释。本文介绍了一种新的低光照图像提升方法——卷积束注意力导航网络（CDAN）。CDAN通过综合了自适应网络、卷积块和束注意力机制的架构，确保了信息传递的高效性和特征学习。此外，特定的后处理阶段可以进一步调整颜色均衡和对比度。我们的方法在低光照图像改善任务中实现了显著的进步，在多种复杂的低光照场景中表现出了扎实的稳定性。这一成就表明CDAN在计算机视觉任务中具有广泛的应用前景，特别是在低光照条件下进行稳定的物体检测和识别。
</details></li>
</ul>
<hr>
<h2 id="Unified-Data-Management-and-Comprehensive-Performance-Evaluation-for-Urban-Spatial-Temporal-Prediction-Experiment-Analysis-Benchmark"><a href="#Unified-Data-Management-and-Comprehensive-Performance-Evaluation-for-Urban-Spatial-Temporal-Prediction-Experiment-Analysis-Benchmark" class="headerlink" title="Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]"></a>Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12899">http://arxiv.org/abs/2308.12899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/libcity/bigscity-libcity">https://github.com/libcity/bigscity-libcity</a></li>
<li>paper_authors: Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, Jingyuan Wang<br>for: 这个论文主要是为了解决城市空间时间预测领域中的数据访问和利用问题，以及深度学习模型的选择和结构设计问题。methods: 该论文提出了“原子文件”的统一存储格式，用于城市空间时间大数据的管理，并对40个多样化的数据集进行验证。此外，论文还提供了城市空间时间预测模型的技术进步概述，以及使用多种模型和数据集进行广泛的实验，建立了性能排名和研究方向。results: 该论文通过提出“原子文件”和对多种模型和数据集的实验，得出了有效地管理城市空间时间数据，指导未来的研究发展，并且可能在长期内对城市生活标准产生持续的贡献。<details>
<summary>Abstract</summary>
The field of urban spatial-temporal prediction is advancing rapidly with the development of deep learning techniques and the availability of large-scale datasets. However, challenges persist in accessing and utilizing diverse urban spatial-temporal datasets from different sources and stored in different formats, as well as determining effective model structures and components with the proliferation of deep learning models. This work addresses these challenges and provides three significant contributions. Firstly, we introduce "atomic files", a unified storage format designed for urban spatial-temporal big data, and validate its effectiveness on 40 diverse datasets, simplifying data management. Secondly, we present a comprehensive overview of technological advances in urban spatial-temporal prediction models, guiding the development of robust models. Thirdly, we conduct extensive experiments using diverse models and datasets, establishing a performance leaderboard and identifying promising research directions. Overall, this work effectively manages urban spatial-temporal data, guides future efforts, and facilitates the development of accurate and efficient urban spatial-temporal prediction models. It can potentially make long-term contributions to urban spatial-temporal data management and prediction, ultimately leading to improved urban living standards.
</details>
<details>
<summary>摘要</summary>
难 accessible 和利用不同来源和格式的城市空间时间数据的挑战 persistently in the field of urban spatial-temporal prediction. To address these challenges, this work makes three significant contributions. Firstly, we introduce "atomic files", a unified storage format designed for urban spatial-temporal big data, and validate its effectiveness on 40 diverse datasets, simplifying data management. Secondly, we present a comprehensive overview of technological advances in urban spatial-temporal prediction models, guiding the development of robust models. Thirdly, we conduct extensive experiments using diverse models and datasets, establishing a performance leaderboard and identifying promising research directions. Overall, this work effectively manages urban spatial-temporal data, guides future efforts, and facilitates the development of accurate and efficient urban spatial-temporal prediction models. It can potentially make long-term contributions to urban spatial-temporal data management and prediction, ultimately leading to improved urban living standards.Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Beyond-Document-Page-Classification-Design-Datasets-and-Challenges"><a href="#Beyond-Document-Page-Classification-Design-Datasets-and-Challenges" class="headerlink" title="Beyond Document Page Classification: Design, Datasets, and Challenges"></a>Beyond Document Page Classification: Design, Datasets, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12896">http://arxiv.org/abs/2308.12896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordy Van Landeghem, Sanket Biswas, Matthew B. Blaschko, Marie-Francine Moens</li>
<li>for: 本文提出了将文档分类测试更加接近实际应用的需求，包括测试数据的性质（多通道、多页、多业务）和分类任务的类型（多页文档、页流和文档套件分类等）。</li>
<li>methods: 本文认为现有的公共多页文档分类数据集缺乏，并正式化了应用场景中的多种分类任务，以及需要更好的文档表示。</li>
<li>results: 实验表明，现有的分类指标已经过时，需要更新以评估实际上occurs的完整文档。这也强调了评估方法的成熔和时间复杂度的重要性，以及面对实际分布的各种扩展。<details>
<summary>Abstract</summary>
This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distribution shifts (e.g., born-digital vs. scanning noise, shifting page order). Our study ends on a hopeful note by recommending concrete avenues for future improvements.}
</details>
<details>
<summary>摘要</summary>
An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. The study also calls for more mature evaluation methodologies, including calibration evaluation, inference complexity (time-memory), and a range of realistic distribution shifts (e.g., born-digital vs. scanning noise, shifting page order).The paper concludes on a hopeful note, recommending concrete avenues for future improvements.Translated into Simplified Chinese:这篇论文强调将文档分类 benchmarking 更近于实际应用场景，包括数据 ($X$) 的性质（多通道、多页、多产业）和分类任务 ($f$) 的多样性。文章指出了多页文档分类数据集的缺乏，并正式化了在应用场景中出现的不同分类任务。它还鼓励了 targets 精准的多页文档表示。一个实验研究表明，现有的分类标准数据集已经失去了现实意义，需要更新以评估完整的文档。研究还强调了需要更成熟的评估方法，包括准确评估、推理复杂度（时间内存）和多种现实的分布转移（例如，生成vs扫描噪声、页面顺序变化）。文章结束于一个希望的注意事项，建议将来的改进方向。Translated into Traditional Chinese:这篇论文强调将文档分类 benchmarking 更近于实际应用场景，包括数据 ($X$) 的性质（多通道、多页、多产业）和分类任务 ($f$) 的多样性。论文指出了多页文档分类数据集的缺乏，并正式化了在应用场景中出现的不同分类任务。它还鼓励了targets精准的多页文档表示。一个实验研究表明，现有的分类标准数据集已经失去了现实意义，需要更新以评估完整的文档。研究还强调了需要更成熟的评估方法，包括准确评估、推理复杂度（时间内存）和多种现实的分布转移（例如，生成vs扫描噪音、页面顺序变化）。论文结束于一个希望的注意事项，建议将来的改进方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/25/cs.LG_2023_08_25/" data-id="cloojsmhi00o6re886ewbcsdy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.SD_2023_08_24/" class="article-date">
  <time datetime="2023-08-24T15:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/cs.SD_2023_08_24/">cs.SD - 2023-08-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sparks-of-Large-Audio-Models-A-Survey-and-Outlook"><a href="#Sparks-of-Large-Audio-Models-A-Survey-and-Outlook" class="headerlink" title="Sparks of Large Audio Models: A Survey and Outlook"></a>Sparks of Large Audio Models: A Survey and Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12792">http://arxiv.org/abs/2308.12792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Heriberto Cuayáhuitl, Björn W. Schuller</li>
<li>for: 这项论文提供了大语言模型在音频处理领域的最新进展和挑战。</li>
<li>methods: 这项论文使用的方法包括使用大量数据的传统变换器结构，以及对这些模型的深入分析和评估。</li>
<li>results: 这项论文的结果表明，这些基础Audio Models可以在多种音频任务中表现出色，包括自动语音识别、文本读取和音乐生成等。此外，这些模型还可以 acting as universal translators，支持多种语言的多种语音任务。<details>
<summary>Abstract</summary>
This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="WavMark-Watermarking-for-Audio-Generation"><a href="#WavMark-Watermarking-for-Audio-Generation" class="headerlink" title="WavMark: Watermarking for Audio Generation"></a>WavMark: Watermarking for Audio Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12770">http://arxiv.org/abs/2308.12770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, Furu Wei</li>
<li>for: 这个论文旨在提出一种 Audio Watermarking 框架，用于防止静音识别和说话者冒充。</li>
<li>methods: 该框架使用了一种新的压缩方法，可以在1秒钟的音频片段中编码32比特的水印，并且具有强大的鲁棒性和隐蔽性。</li>
<li>results: 该框架可以在10-20秒的音频片段上实现平均的 Bit Error Rate 为0.48%，比现有的水印工具减少了2800%以上的错误率。<details>
<summary>Abstract</summary>
Recent breakthroughs in zero-shot voice synthesis have enabled imitating a speaker's voice using just a few seconds of recording while maintaining a high level of realism. Alongside its potential benefits, this powerful technology introduces notable risks, including voice fraud and speaker impersonation. Unlike the conventional approach of solely relying on passive methods for detecting synthetic data, watermarking presents a proactive and robust defence mechanism against these looming risks. This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achieve heightened robustness and expanded capacity. Utilizing 10 to 20-second audio as the host, our approach demonstrates an average Bit Error Rate (BER) of 0.48\% across ten common attacks, a remarkable reduction of over 2800\% in BER compared to the state-of-the-art watermarking tool. See https://aka.ms/wavmark for demos of our work.
</details>
<details>
<summary>摘要</summary>
最近的零上 синте声技术突破有了可以通过只需几秒钟的录音来模仿说话人的声音，同时保持高度的真实感。然而，这种强大技术也存在了一些风险，包括语音 fraud 和说话人模仿。不同于传统的仅依靠被动方法来检测合成数据，水印技术是一种积极和坚强的防御机制。本文介绍了一种创新的音频水印框架，可以在1秒钟的音频片断中编码Up to 32位的水印，人类不可见。这个水印具有强大的抗击攻击性，可以作为合成voice的标识符，并且有广泛的应用前途在音频版权保护方面。此外，这个框架具有高度的灵活性，可以将多个水印段组合以实现更高的强度和扩展的容量。使用10到20秒的音频作为主机，我们的方法在十种常见的攻击下显示了0.48%的比特错误率（BER），相比之下，现状的水印工具的BER减少了2800%以上。请参考https://aka.ms/wavmark 获取我们的工作示例。
</details></li>
</ul>
<hr>
<h2 id="Whombat-An-open-source-annotation-tool-for-machine-learning-development-in-bioacoustics"><a href="#Whombat-An-open-source-annotation-tool-for-machine-learning-development-in-bioacoustics" class="headerlink" title="Whombat: An open-source annotation tool for machine learning development in bioacoustics"></a>Whombat: An open-source annotation tool for machine learning development in bioacoustics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12688">http://arxiv.org/abs/2308.12688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Martinez Balvanera, Oisin Mac Aodha, Matthew J. Weldy, Holly Pringle, Ella Browning, Kate E. Jones</li>
<li>for: 这个论文的目的是提出一种用于自动分析生物声音记录的机器学习方法，以扩大生物多样性监测的规模。</li>
<li>methods: 这个论文使用的方法是基于机器学习的高级应用，需要一种数据驱动的方法，使用仔细标注和整理的评估和训练数据，以确保模型的准确性和可靠性。</li>
<li>results: 这个论文通过介绍一种名为Whombat的用户友好的浏览器基本接口，可以帮助用户管理声音记录和注释项目，并提供了一些视觉化、探索和注释工具，以便快速注释、审查和分享注释，并可视化和评估数据集中的机器学习预测结果。<details>
<summary>Abstract</summary></li>
</ul>
<ol>
<li>Automated analysis of bioacoustic recordings using machine learning (ML) methods has the potential to greatly scale biodiversity monitoring efforts. The use of ML for high-stakes applications, such as conservation research, demands a data-centric approach with a focus on utilizing carefully annotated and curated evaluation and training data that is relevant and representative. Creating annotated datasets of sound recordings presents a number of challenges, such as managing large collections of recordings with associated metadata, developing flexible annotation tools that can accommodate the diverse range of vocalization profiles of different organisms, and addressing the scarcity of expert annotators.   2. We present Whombat a user-friendly, browser-based interface for managing audio recordings and annotation projects, with several visualization, exploration, and annotation tools. It enables users to quickly annotate, review, and share annotations, as well as visualize and evaluate a set of machine learning predictions on a dataset. The tool facilitates an iterative workflow where user annotations and machine learning predictions feedback to enhance model performance and annotation quality.   3. We demonstrate the flexibility of Whombat by showcasing two distinct use cases: an project aimed at enhancing automated UK bat call identification at the Bat Conservation Trust (BCT), and a collaborative effort among the USDA Forest Service and Oregon State University researchers exploring bioacoustic applications and extending automated avian classification models in the Pacific Northwest, USA.   4. Whombat is a flexible tool that can effectively address the challenges of annotation for bioacoustic research. It can be used for individual and collaborative work, hosted on a shared server or accessed remotely, or run on a personal computer without the need for coding skills.</details>
<details>
<summary>摘要</summary></li>
<li>机器学习（ML）技术可以帮助自动分析生物声音记录，提高生物多样性监测的效率。在保护研究等高度重要应用中使用ML时，需要一种数据驱动的方法，强调使用高质量、精心标注和抽象的训练和评估数据。创建声音记录的标注数据集存在许多挑战，例如管理大量记录和相关元数据，开发 flexible的标注工具，以满足不同生物种类的声音profile的多样性。2. 我们介绍Whombat，一个易于使用的浏览器基本的界面，用于管理声音记录和标注项目。它具有许多可视化、探索和标注工具，让用户快速标注、审核和共享标注，以及可视化和评估数据集上机器学习预测的结果。工具支持循环的工作流程，在用户标注和机器学习预测之间进行反馈，以提高标注质量和模型性能。3. 我们示例了Whombat的灵活性，通过两个不同的应用场景：一是英国蝙蝠保护协会（BCT）的自动蝙蝠叫声识别项目，二是美国农业部和奥REGON州立大学合作的生物声音应用研究，探索生物声音应用和扩展自动鸟类分类模型。4. Whombat是一个灵活的工具，可以有效地解决生物声音研究中的标注挑战。它可以用于个人和团队工作，可以在共享服务器上Host或远程访问，或者在个人计算机上运行，无需编程技能。</details></li>
</ol>
<hr>
<h2 id="Naaloss-Rethinking-the-objective-of-speech-enhancement"><a href="#Naaloss-Rethinking-the-objective-of-speech-enhancement" class="headerlink" title="Naaloss: Rethinking the objective of speech enhancement"></a>Naaloss: Rethinking the objective of speech enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12615">http://arxiv.org/abs/2308.12615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan-Hsun Ho, En-Lun Yu, Jeih-weih Hung, Berlin Chen<br>for:* This paper aims to improve the performance of automatic speech recognition (ASR) in noisy environments by reducing the impact of processing artifacts generated by single-channel speech enhancement (SE) methods.methods:* The paper proposes a novel Noise- and Artifacts-aware loss function (NAaLoss) that considers the loss of estimation, de-artifact, and noise ignorance to improve the quality of SE.results:* Experimental results show that NAaLoss significantly improves the ASR performance of most setups while preserving the quality of SE, as demonstrated through visualizations of artifacts in waveforms and spectrograms.<details>
<summary>Abstract</summary>
Reducing noise interference is crucial for automatic speech recognition (ASR) in a real-world scenario. However, most single-channel speech enhancement (SE) generates "processing artifacts" that negatively affect ASR performance. Hence, in this study, we suggest a Noise- and Artifacts-aware loss function, NAaLoss, to ameliorate the influence of artifacts from a novel perspective. NAaLoss considers the loss of estimation, de-artifact, and noise ignorance, enabling the learned SE to individually model speech, artifacts, and noise. We examine two SE models (simple/advanced) learned with NAaLoss under various input scenarios (clean/noisy) using two configurations of the ASR system (with/without noise robustness). Experiments reveal that NAaLoss significantly improves the ASR performance of most setups while preserving the quality of SE toward perception and intelligibility. Furthermore, we visualize artifacts through waveforms and spectrograms, and explain their impact on ASR.
</details>
<details>
<summary>摘要</summary>
减少干扰是自动语音识别（ASR）在实际场景中的关键。然而，大多数单通道语音增强（SE）生成“处理残留”，这些残留会负面影响ASR性能。因此，在这项研究中，我们提议一种噪声和残留意识损失函数（NAaLoss），从新的视角来缓解噪声和残留的影响。NAaLoss考虑语音估计损失、去残留和噪声忽略，使得学习的SE可以分别模型语音、残留和噪声。我们在不同的输入场景（干扰/不干扰）和ASR系统的两种配置（带/ без噪声Robustness）中对两种SE模型（简单/高级）进行了NAaLoss学习。实验表明，NAaLoss可以在大多数设置中显著提高ASR性能，同时保持SE的质量。此外，我们通过波形和spectrogram来可见化残留，并解释它们对ASR的影响。
</details></li>
</ul>
<hr>
<h2 id="Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music"><a href="#Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music" class="headerlink" title="Emotion-Aligned Contrastive Learning Between Images and Music"></a>Emotion-Aligned Contrastive Learning Between Images and Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12610">http://arxiv.org/abs/2308.12610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanti Stewart, Tiantian Feng, Kleanthis Avramidis, Shrikanth Narayanan</li>
<li>for: 这个论文是为了 Retrieving emotionally-relevant music from image queries 而写的。</li>
<li>methods: 这篇论文使用的方法包括 learning an affective alignment between images and music audio，以及 cross-modal contrastive learning。</li>
<li>results: 该方法能够成功地对图像和音频进行对应，并且学习出的 embedding space 是可以用于cross-modal retrieval应用。<details>
<summary>Abstract</summary>
Traditional music search engines rely on retrieval methods that match natural language queries with music metadata. There have been increasing efforts to expand retrieval methods to consider the audio characteristics of music itself, using queries of various modalities including text, video, and speech. Most approaches aim to match general music semantics to the input queries, while only a few focus on affective qualities. We address the task of retrieving emotionally-relevant music from image queries by proposing a framework for learning an affective alignment between images and music audio. Our approach focuses on learning an emotion-aligned joint embedding space between images and music. This joint embedding space is learned via emotion-supervised contrastive learning, using an adapted cross-modal version of the SupCon loss. We directly evaluate the joint embeddings with cross-modal retrieval tasks (image-to-music and music-to-image) based on emotion labels. In addition, we investigate the generalizability of the learned music embeddings with automatic music tagging as a downstream task. Our experiments show that our approach successfully aligns images and music, and that the learned embedding space is effective for cross-modal retrieval applications.
</details>
<details>
<summary>摘要</summary>
传统音乐搜索引擎通常使用自然语言查询匹配音乐元数据。随着扩展 Retrieval 方法的尝试，有些方法开始考虑音乐自身的特征，使用不同模式的查询，包括文本、视频和语音。大多数方法尝试匹配通用音乐Semantics 到输入查询，只有一些关注情感质量。我们解决通过图像查询检索情感相关的音乐的任务，我们提议一种学习影响对齐图像和音乐音频的情感对齐的框架。我们的方法是学习一个情感对齐的共同嵌入空间 между图像和音乐。这个共同嵌入空间是通过情感supervised contrastive learning 学习，使用修改后的跨模态版本的 SupCon 损失函数。我们直接评估共同嵌入的 JOINT  embedding 与跨模态检索任务（图像到音乐和音乐到图像）基于情感标签。此外，我们还研究了学习得到的音乐嵌入的一致性，并在自动音乐标签设置作为下游任务进行研究。我们的实验表明，我们的方法成功地对图像和音乐进行对齐，并且学习的嵌入空间是跨模态检索应用中有效。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-noise-shaping-for-audio-coding-using-perfectly-overlapped-window"><a href="#Hybrid-noise-shaping-for-audio-coding-using-perfectly-overlapped-window" class="headerlink" title="Hybrid noise shaping for audio coding using perfectly overlapped window"></a>Hybrid noise shaping for audio coding using perfectly overlapped window</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12566">http://arxiv.org/abs/2308.12566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeongho Jo, Seungkwon Beack</li>
<li>for: 优化低比特率音频编码</li>
<li>methods: 基于模拟强制变换和变换编码刺激（TCX）的复杂LPC-基于CTNS，以及采用50%重叠窗口和切换方案提高编码效率</li>
<li>results: 对象指标和主观听测表明提出的编码框架具有优秀的低比特率音频编码性能<details>
<summary>Abstract</summary>
In recent years, audio coding technology has been standardized based on several frameworks that incorporate linear predictive coding (LPC). However, coding the transient signal using frequency-domain LP residual signals remains a challenge. To address this, temporal noise shaping (TNS) can be adapted, although it cannot be effectively operated since the estimated temporal envelope in the modified discrete cosine transform (MDCT) domain is accompanied by the time-domain aliasing (TDA) terms. In this study, we propose the modulated complex lapped transform-based coding framework integrated with transform coded excitation (TCX) and complex LPC-based TNS (CTNS). Our approach uses a 50\% overlap window and switching scheme for the CTNS to improve the coding efficiency. Additionally, an adaptive calculation of the target bits for the sub-bands using the frequency envelope information based on the quantized LPC coefficients is proposed. To minimize the quantization mismatch between both modes, an integrated quantization for real and complex values and a TDA augmentation method that compensates for the artificially generated TDA components during switching operations are proposed. The proposed coding framework shows a superior performance in both objective metrics and subjective listening tests, thereby demonstrating its low bit-rate audio coding.
</details>
<details>
<summary>摘要</summary>
Recently, 音频编码技术已经基于多个框架标准化，其中包括线性预测编码（LPC）。然而，使用频域LP residual信号编码脉冲信号仍然是一大挑战。为解决这个问题，可以采用时间噪声形成（TNS），但是由于修改后的离散余弦变换（MDCT）域中的时间尺度扰动（TDA）项，TNS无法有效地运行。在这种研究中，我们提出了基于模拟复杂lapsed transform的编码框架，并与变换编码刺激（TCX）和复杂LPC-based TNS（CTNS）结合。我们的方法使用50%的重叠窗口和切换方案来提高编码效率。此外，我们还提出了基于频率尺度信息的适应计算目标位数据的方法，以避免编码抖音。为了减少编码模式之间的量化差异，我们提出了混合量化和TDA扩展方法。这种编码框架在对象指标和主观听测试中表现出色， thereby demonstrating its low bit-rate audio coding.
</details></li>
</ul>
<hr>
<h2 id="MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario"><a href="#MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario" class="headerlink" title="MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario"></a>MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12490">http://arxiv.org/abs/2308.12490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Wen Chen, Zhou Yu, Julia Hirschberg</li>
<li>for: 这个研究旨在提出一种自动语音发音评估系统，能够在关闭和开放响应场景下工作，以满足不同学习需求和提供全面和准确的发音技能评估。</li>
<li>methods: 该系统使用多任务学习方法，包括语音识别和语音评估任务，以提高发音评估的准确性和可靠性。</li>
<li>results: 实验结果表明，该系统在关闭响应场景下的性能与之前的Kaldi-based系统相当，而在开放响应场景下的性能更加稳定和可靠。<details>
<summary>Abstract</summary>
The design of automatic speech pronunciation assessment can be categorized into closed and open response scenarios, each with strengths and limitations. A system with the ability to function in both scenarios can cater to diverse learning needs and provide a more precise and holistic assessment of pronunciation skills. In this study, we propose a Multi-task Pronunciation Assessment model called MultiPA. MultiPA provides an alternative to Kaldi-based systems in that it has simpler format requirements and better compatibility with other neural network models. Compared with previous open response systems, MultiPA provides a wider range of evaluations, encompassing assessments at both the sentence and word-level. Our experimental results show that MultiPA achieves comparable performance when working in closed response scenarios and maintains more robust performance when directly used for open responses.
</details>
<details>
<summary>摘要</summary>
文本设计自动发音评估可以分为关闭和开放响应场景，每个场景都有优点和局限性。一个能够在多种场景下运行的系统可以满足多样化的学习需求，并提供更加准确和全面的发音技能评估。本研究提出了一种名为MultiPA的多任务发音评估模型。MultiPA比Kaldi基础系统更简单，可以更好地与其他神经网络模型结合使用。相比之前的开放响应系统，MultiPA提供了更广泛的评估范围，包括句子和单词层次的评估。我们的实验结果表明，MultiPA在关闭响应场景下的性能相当，而directly用于开放响应场景时的性能更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Attention-Based-Acoustic-Feature-Fusion-Network-for-Depression-Detection"><a href="#Attention-Based-Acoustic-Feature-Fusion-Network-for-Depression-Detection" class="headerlink" title="Attention-Based Acoustic Feature Fusion Network for Depression Detection"></a>Attention-Based Acoustic Feature Fusion Network for Depression Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12478">http://arxiv.org/abs/2308.12478</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuxiaoooo/abafnet">https://github.com/xuxiaoooo/abafnet</a></li>
<li>paper_authors: Xiao Xu, Yang Wang, Xinru Wei, Fei Wang, Xizhe Zhang</li>
<li>for: 这篇论文的目的是提出一个新的听语音特征融合网络（ABAFnet），用于检测创伤后遗症（PTSD）和抑郁症（Depression）。</li>
<li>methods: 这篇论文使用了四种不同的听语音特征，通过深度学习模型进行融合，以实现多维度特征的有效融合。它还将提出一个新的权重调整模组，以提高检测性能。</li>
<li>results: 这篇论文的实验结果显示，ABAFnet 可以优于先前的方法在检测抑郁症和其子类型上。进一步的分析显示，听语音特征中的MFCC相关特征在检测speech-based抑郁症中扮演着重要的角色。<details>
<summary>Abstract</summary>
Depression, a common mental disorder, significantly influences individuals and imposes considerable societal impacts. The complexity and heterogeneity of the disorder necessitate prompt and effective detection, which nonetheless, poses a difficult challenge. This situation highlights an urgent requirement for improved detection methods. Exploiting auditory data through advanced machine learning paradigms presents promising research directions. Yet, existing techniques mainly rely on single-dimensional feature models, potentially neglecting the abundance of information hidden in various speech characteristics. To rectify this, we present the novel Attention-Based Acoustic Feature Fusion Network (ABAFnet) for depression detection. ABAFnet combines four different acoustic features into a comprehensive deep learning model, thereby effectively integrating and blending multi-tiered features. We present a novel weight adjustment module for late fusion that boosts performance by efficaciously synthesizing these features. The effectiveness of our approach is confirmed via extensive validation on two clinical speech databases, CNRAC and CS-NRAC, thereby outperforming previous methods in depression detection and subtype classification. Further in-depth analysis confirms the key role of each feature and highlights the importance of MFCCrelated features in speech-based depression detection.
</details>
<details>
<summary>摘要</summary>
抑郁症，一种常见的心理疾病，对个人和社会产生了重要的影响。但是检测抑郁症的复杂性和多样性却提出了严峻的挑战。这种情况强调了改进检测方法的需要。通过利用语音数据的高级机器学习方法可能会开拓出有前途的研究方向。然而，现有的技术主要依靠单一的特征模型，可能会忽略语音特征中的巨量信息。为了纠正这一点，我们提出了一种新的注意力基于的听音特征融合网络（ABAFnet），用于抑郁症检测。ABAFnet将四种不同的语音特征融合到一个深度学习模型中， thereby 有效地汇集和融合多级特征。我们还提出了一种新的权重调整模块，可以在晚期融合中提高性能。我们的方法在两个临床语音数据库（CNRAC和CS-NRAC）上进行了广泛验证，并表现出了在抑郁症检测和亚型分类中的出色表现。进一步的深入分析表明，每种特征都扮演着重要的角色，并且MFCC相关的特征在语音基于的抑郁症检测中具有重要的意义。
</details></li>
</ul>
<hr>
<h2 id="An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video"><a href="#An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video" class="headerlink" title="An Initial Exploration: Learning to Generate Realistic Audio for Silent Video"></a>An Initial Exploration: Learning to Generate Realistic Audio for Silent Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12408">http://arxiv.org/abs/2308.12408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Martel, Jackson Wagner</li>
<li>for: 这篇论文的目的是开发一个基于深度学习的框架，用于生成电影和其他媒体中的真实的音效。</li>
<li>methods: 这篇论文使用了多种不同的模型架构，包括深度融合CNN、扩展Wavenet CNN和变换器结构，以处理视频上下文和先前生成的音频。</li>
<li>results: 研究发现，使用 transformer 结构可以匹配视频的低频谱，但是无法生成更加复杂的波形。<details>
<summary>Abstract</summary>
Generating realistic audio effects for movies and other media is a challenging task that is accomplished today primarily through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in it's natural sequence and generates realistic audio to accompany it. Notably, we have reason to believe this is achievable due to advancements in realistic audio generation techniques conditioned on other inputs (e.g., Wavenet conditioned on text). We explore several different model architectures to accomplish this task that process both previously-generated audio and video context. These include deep-fusion CNN, dilated Wavenet CNN with visual context, and transformer-based architectures. We find that the transformer-based architecture yields the most promising results, matching low-frequencies to visual patterns effectively, but failing to generate more nuanced waveforms.
</details>
<details>
<summary>摘要</summary>
Generating realistic audio effects for movies and other media is a challenging task that is primarily accomplished today through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in its natural sequence and generates realistic audio to accompany it. Notably, we have reason to believe this is achievable due to advancements in realistic audio generation techniques conditioned on other inputs (e.g., Wavenet conditioned on text). We explore several different model architectures to accomplish this task, including deep-fusion CNN, dilated Wavenet CNN with visual context, and transformer-based architectures. We find that the transformer-based architecture yields the most promising results, matching low-frequencies to visual patterns effectively, but failing to generate more nuanced waveforms.Here's the translation in Traditional Chinese:生成电影和其他媒体中的真实音效是一个挑战性的任务，主要通过物理技术知为FOLEY艺术完成。FOLEY艺术家使用常规的物品（例如拳拳套和碎 glass）与影片同步生成吸引人的音轨。在这个工作中，我们想要开发一个基于深度学习的框架，可以观察影片的自然顺序，并生成吸引人的音轨。我们有理由相信这是可能的，因为有进步在真实音效生成技术中，特别是基于其他输入（例如 Wavenet  conditioned on text）。我们探索了不同的模型架构，以实现这个任务，包括深度融合 CNN、扩展 Wavenet CNN  WITH 视觉上下文，以及 transformer 型架构。我们发现 transformer 型架构产生了最有前途的结果，能够对低频调和视觉模式匹配得非常好，但是无法产生更加细部的波形。
</details></li>
</ul>
<hr>
<h2 id="AdVerb-Visually-Guided-Audio-Dereverberation"><a href="#AdVerb-Visually-Guided-Audio-Dereverberation" class="headerlink" title="AdVerb: Visually Guided Audio Dereverberation"></a>AdVerb: Visually Guided Audio Dereverberation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12370">http://arxiv.org/abs/2308.12370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjoy Chowdhury, Sreyan Ghosh, Subhrajyoti Dasgupta, Anton Ratnarajah, Utkarsh Tyagi, Dinesh Manocha</li>
<li>for: 这篇论文主要是为了提出一种基于视觉信号的声音去抖杂方法，以提高声音质量。</li>
<li>methods: 该方法使用了一种新的 geometry-aware cross-modal transformer 架构，利用视觉信号和声音信号之间的相互关系，生成一个复杂的理想比率幕，并将其应用于抖杂声音中，以估计清晰声音。</li>
<li>results: 该方法在三个下游任务中表现出色：语音提升、语音识别和speaker verification，与传统的声音 только和视觉只基eline上相比，有18%-82%的Relative improvement。此外，该方法在 AVSpeech 数据集上也实现了非常满意的 RT60 错误分数。<details>
<summary>Abstract</summary>
We present AdVerb, a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio. Although audio-only dereverberation is a well-studied problem, our approach incorporates the complementary visual modality to perform audio dereverberation. Given an image of the environment where the reverberated sound signal has been recorded, AdVerb employs a novel geometry-aware cross-modal transformer architecture that captures scene geometry and audio-visual cross-modal relationship to generate a complex ideal ratio mask, which, when applied to the reverberant audio predicts the clean sound. The effectiveness of our method is demonstrated through extensive quantitative and qualitative evaluations. Our approach significantly outperforms traditional audio-only and audio-visual baselines on three downstream tasks: speech enhancement, speech recognition, and speaker verification, with relative improvements in the range of 18% - 82% on the LibriSpeech test-clean set. We also achieve highly satisfactory RT60 error scores on the AVSpeech dataset.
</details>
<details>
<summary>摘要</summary>
我们提出了AdVerb，一种新的音频视频去噪框架，该框架利用视频听录中的视觉信息，以估算清晰的音频。虽然音频只的去噪是已经广泛研究的问题，但我们的方法利用了补充的视觉Modalidade，以实现音频去噪。给出了环境中录制的听录音信号的图像，AdVerb使用了一种新的场景意识geometry-aware cross-modal transformer架构，捕捉场景几何和音频视频的跨Modalidade关系，生成了复杂的理想比率层，当应用于听录音频时，可以预测清晰的声音。我们的方法的效果得到了广泛的量化和质量评估。我们的方法在三个下游任务上显著超过了传统的音频只和音频视频基eline，在LibriSpeech测试集上的改进率在18%-82%之间。我们还实现了AVSpeech数据集上的高度满意的RT60错误分数。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/cs.SD_2023_08_24/" data-id="cloojsmki00vere888i9ycgec" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.CV_2023_08_24/" class="article-date">
  <time datetime="2023-08-24T13:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/cs.CV_2023_08_24/">cs.CV - 2023-08-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="VNI-Net-Vector-Neurons-based-Rotation-Invariant-Descriptor-for-LiDAR-Place-Recognition"><a href="#VNI-Net-Vector-Neurons-based-Rotation-Invariant-Descriptor-for-LiDAR-Place-Recognition" class="headerlink" title="VNI-Net: Vector Neurons-based Rotation-Invariant Descriptor for LiDAR Place Recognition"></a>VNI-Net: Vector Neurons-based Rotation-Invariant Descriptor for LiDAR Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12870">http://arxiv.org/abs/2308.12870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gengxuan Tian, Junqiao Zhao, Yingfeng Cai, Fenglin Zhang, Wenjie Mu, Chen Ye</li>
<li>for: 提高 LiDAR 场景认知中的旋转不敏感性</li>
<li>methods: 使用 Vector Neurons Network (VNN) 实现 SO(3) 旋转不变性，提取邻近点的旋转等价特征，将低维特征映射到高维空间</li>
<li>results: 对公共数据集进行实验，与其他基准方法相比，提高了旋转不敏感性，与当前状态艺术场景认知方法几乎匹配Here’s a brief explanation of each point:* “for”: The paper aims to improve the rotation-invariance of LiDAR scene recognition.* “methods”: The proposed method uses Vector Neurons Network (VNN) to achieve SO(3) rotation invariance, and extracts rotation-equivalent features from neighboring points.* “results”: The proposed method significantly outperforms other baseline methods that consider rotation invariance, and achieves comparable results with current state-of-the-art place recognition methods that do not consider rotation issues.<details>
<summary>Abstract</summary>
LiDAR-based place recognition plays a crucial role in Simultaneous Localization and Mapping (SLAM) and LiDAR localization.   Despite the emergence of various deep learning-based and hand-crafting-based methods, rotation-induced place recognition failure remains a critical challenge.   Existing studies address this limitation through specific training strategies or network structures.   However, the former does not produce satisfactory results, while the latter focuses mainly on the reduced problem of SO(2) rotation invariance. Methods targeting SO(3) rotation invariance suffer from limitations in discrimination capability.   In this paper, we propose a new method that employs Vector Neurons Network (VNN) to achieve SO(3) rotation invariance.   We first extract rotation-equivariant features from neighboring points and map low-dimensional features to a high-dimensional space through VNN.   Afterwards, we calculate the Euclidean and Cosine distance in the rotation-equivariant feature space as rotation-invariant feature descriptors.   Finally, we aggregate the features using GeM pooling to obtain global descriptors.   To address the significant information loss when formulating rotation-invariant descriptors, we propose computing distances between features at different layers within the Euclidean space neighborhood.   This greatly improves the discriminability of the point cloud descriptors while ensuring computational efficiency.   Experimental results on public datasets show that our approach significantly outperforms other baseline methods implementing rotation invariance, while achieving comparable results with current state-of-the-art place recognition methods that do not consider rotation issues.
</details>
<details>
<summary>摘要</summary>
利用LiDAR技术实现地点识别在同时地图和地点位置确定（SLAM）中扮演关键角色。  despite the emergence of various深度学习基于和手动设计基于方法， rotate induced place recognition failure remains a critical challenge。 existing studies address this limitation through specific training strategies or network structures。  However, the former does not produce satisfactory results, while the latter focuses mainly on the reduced problem of SO(2) rotation invariance。 methods targeting SO(3) rotation invariance suffer from limitations in discrimination capability。  In this paper, we propose a new method that employs Vector Neurons Network (VNN) to achieve SO(3) rotation invariance。  we first extract rotation-equivariant features from neighboring points and map low-dimensional features to a high-dimensional space through VNN。  Afterwards, we calculate the Euclidean and Cosine distance in the rotation-equivariant feature space as rotation-invariant feature descriptors。  Finally, we aggregate the features using GeM pooling to obtain global descriptors。  To address the significant information loss when formulating rotation-invariant descriptors, we propose computing distances between features at different layers within the Euclidean space neighborhood。  This greatly improves the discriminability of the point cloud descriptors while ensuring computational efficiency。  Experimental results on public datasets show that our approach significantly outperforms other baseline methods implementing rotation invariance, while achieving comparable results with current state-of-the-art place recognition methods that do not consider rotation issues。Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and pronunciation. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="ToonTalker-Cross-Domain-Face-Reenactment"><a href="#ToonTalker-Cross-Domain-Face-Reenactment" class="headerlink" title="ToonTalker: Cross-Domain Face Reenactment"></a>ToonTalker: Cross-Domain Face Reenactment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12866">http://arxiv.org/abs/2308.12866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Gong, Yong Zhang, Xiaodong Cun, Fei Yin, Yanbo Fan, Xuan Wang, Baoyuan Wu, Yujiu Yang</li>
<li>for: 本研究旨在实现跨域人脸reenactment，即将真实视频转换为动漫图像和 vice versa。</li>
<li>methods: 我们提出了一种基于 transformer 框架的新方法，包括两个域特定的动作编码器和两个可学习的动作基准存储。我们还使用了源查询 transformer 和驱动 transformer 来将域特定动作 proyect 到共同幂 space，然后在该空间中进行动作传输。</li>
<li>results: 我们的方法在评估中表现出色，与竞争方法相比有所超越。此外，我们还提供了一个 Disney 风格的动漫数据集，以便进一步验证和应用我们的方法。<details>
<summary>Abstract</summary>
We target cross-domain face reenactment in this paper, i.e., driving a cartoon image with the video of a real person and vice versa. Recently, many works have focused on one-shot talking face generation to drive a portrait with a real video, i.e., within-domain reenactment. Straightforwardly applying those methods to cross-domain animation will cause inaccurate expression transfer, blur effects, and even apparent artifacts due to the domain shift between cartoon and real faces. Only a few works attempt to settle cross-domain face reenactment. The most related work AnimeCeleb requires constructing a dataset with pose vector and cartoon image pairs by animating 3D characters, which makes it inapplicable anymore if no paired data is available. In this paper, we propose a novel method for cross-domain reenactment without paired data. Specifically, we propose a transformer-based framework to align the motions from different domains into a common latent space where motion transfer is conducted via latent code addition. Two domain-specific motion encoders and two learnable motion base memories are used to capture domain properties. A source query transformer and a driving one are exploited to project domain-specific motion to the canonical space. The edited motion is projected back to the domain of the source with a transformer. Moreover, since no paired data is provided, we propose a novel cross-domain training scheme using data from two domains with the designed analogy constraint. Besides, we contribute a cartoon dataset in Disney style. Extensive evaluations demonstrate the superiority of our method over competing methods.
</details>
<details>
<summary>摘要</summary>
我们在这篇论文中targetcross-domain face reenactment，即将动漫图像驱动真实视频和真实视频驱动动漫图像。在最近的许多工作中，人们主要关注在一个shot中生成真实人脸，即在同一个频谱中reenactment。如果直接应用这些方法到cross-domain animation，会导致不准确的表达传递、模糊效果和甚至显式的artefacts，这是因为cartoon和真实人脸之间的频谱差异。只有一些工作尝试了cross-domain face reenactment。最相关的工作是AnimeCeleb，它需要构建一个数据集，其中包含pose vector和动漫图像对的Pair，并通过动画3D人物来生成这些对。这使得它在没有对应数据时无法应用。在这篇论文中，我们提出了一种新的方法，即使没有对应数据也可以实现cross-domain reenactment。具体来说，我们提出了一个基于transformer的框架，用于将不同频谱中的动作都尝试到一个共同的幂space中，然后通过幂码加法进行动作传递。我们使用了两个域特定的动作编码器和两个可学习的动作基准记忆来捕捉域属性。 sources query transformer和驱动一个是用于将域特定的动作 проек到共同空间，而编辑的动作则是通过transformer将其 projet回到源域。此外，由于没有提供对应数据，我们提出了一种新的跨域训练方案，使用了两个域的数据，并通过设计的相似性约束。此外，我们还贡献了一个以Disney风格为主的动漫数据集。我们的方法在多个评估中表现出色，超过了竞争方法的性能。
</details></li>
</ul>
<hr>
<h2 id="SkipcrossNets-Adaptive-Skip-cross-Fusion-for-Road-Detection"><a href="#SkipcrossNets-Adaptive-Skip-cross-Fusion-for-Road-Detection" class="headerlink" title="SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection"></a>SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12863">http://arxiv.org/abs/2308.12863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Zhang, Yan Gong, Zhiwei Li, Xin Gao, Dafeng Jin, Jun Li, Huaping Liu</li>
<li>For: This paper proposes a novel fusion architecture called SkipcrossNets for multi-modal fusion of LiDAR point clouds and camera images in autonomous driving tasks.* Methods: The SkipcrossNets architecture uses skip-cross connections to adaptively combine features from both modalities at each layer, without being bound to a specific fusion epoch. The network is divided into several blocks to reduce the complexity of feature fusion and the number of model parameters.* Results: The proposed SkipcrossNets architecture achieved a MaxF score of 96.85% on the KITTI dataset and an F1 score of 84.84% on the A2D2 dataset, with a memory requirement of only 2.33 MB and a speed of 68.24 FPS, making it viable for mobile terminals and embedded devices.<details>
<summary>Abstract</summary>
Multi-modal fusion is increasingly being used for autonomous driving tasks, as images from different modalities provide unique information for feature extraction. However, the existing two-stream networks are only fused at a specific network layer, which requires a lot of manual attempts to set up. As the CNN goes deeper, the two modal features become more and more advanced and abstract, and the fusion occurs at the feature level with a large gap, which can easily hurt the performance. In this study, we propose a novel fusion architecture called skip-cross networks (SkipcrossNets), which combines adaptively LiDAR point clouds and camera images without being bound to a certain fusion epoch. Specifically, skip-cross connects each layer to each layer in a feed-forward manner, and for each layer, the feature maps of all previous layers are used as input and its own feature maps are used as input to all subsequent layers for the other modality, enhancing feature propagation and multi-modal features fusion. This strategy facilitates selection of the most similar feature layers from two data pipelines, providing a complementary effect for sparse point cloud features during fusion processes. The network is also divided into several blocks to reduce the complexity of feature fusion and the number of model parameters. The advantages of skip-cross fusion were demonstrated through application to the KITTI and A2D2 datasets, achieving a MaxF score of 96.85% on KITTI and an F1 score of 84.84% on A2D2. The model parameters required only 2.33 MB of memory at a speed of 68.24 FPS, which could be viable for mobile terminals and embedded devices.
</details>
<details>
<summary>摘要</summary>
多Modal融合在自动驾驶任务中日益普遍应用，因为不同模式的图像提供了独特的特征提取信息。然而，现有的两派网络只是在特定网络层进行融合，需要大量的手动尝试设置。随着CNN深入，两种模式的特征变得越来越先进和抽象，融合发生在特征层级，这可能会产生性能下降。在这种研究中，我们提出了一种新的融合架构，即跳过网络（SkipcrossNets），它将雷达点云和摄像头图像在不同的层次进行 Adaptive 融合。具体来说，跳过连接每层与每层进行Feed-forward 的连接，并且为每层的特征图使用所有前一层的特征图作为输入，并将每层的特征图作为输入给后续层的另一个模式。这种策略使得在融合过程中选择最相似的特征层，提供了质点云特征的补做效果。此外，网络还被分解成多个块，以降低特征融合的复杂度和模型参数的数量。Skipcross融合的优点在应用于 KITTI 和 A2D2 数据集上得到了证明，最大评分达 96.85% 和 F1 分数达 84.84%。模型参数只需 2.33 MB 的内存和 68.24 FPS 的速度，这可能是蜂窝Terminal和嵌入式设备的可行选择。
</details></li>
</ul>
<hr>
<h2 id="Learned-Local-Attention-Maps-for-Synthesising-Vessel-Segmentations"><a href="#Learned-Local-Attention-Maps-for-Synthesising-Vessel-Segmentations" class="headerlink" title="Learned Local Attention Maps for Synthesising Vessel Segmentations"></a>Learned Local Attention Maps for Synthesising Vessel Segmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12861">http://arxiv.org/abs/2308.12861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Rodrigo Bonazzola, Haoran Dou, Yan Xia, Tianyou Wei, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>for: 这个论文是为了制作血管分割图像的IMAGING模式，以便在诊断和评估血液管的风险方面提供更好的工具。</li>
<li>methods: 这篇论文使用了一种encoder-decoder模型，通过将T1和T2磁共振图像进行编码，生成基于T2磁共振图像的血管分割图像。该模型采用了两个阶段多目标学习方法，以捕捉全局和局部特征。它使用学习的本地注意力地图，从T2磁共振图像中提取与生成圆束血管相关的信息。</li>
<li>results: 测试中，这个模型的生成的血管分割图像的 dice分数为$0.79\pm0.03$，比state-of-the-art的分割网络（如转换器U-Net和nnU-net）更高，同时使用的参数数量只是这些网络的一部分。主要的 Qualitative difference between our synthetic vessel segmentations and the comparative models was in the sharper resolution of the CoW vessel segments, especially in the posterior circulation.<details>
<summary>Abstract</summary>
Magnetic resonance angiography (MRA) is an imaging modality for visualising blood vessels. It is useful for several diagnostic applications and for assessing the risk of adverse events such as haemorrhagic stroke (resulting from the rupture of aneurysms in blood vessels). However, MRAs are not acquired routinely, hence, an approach to synthesise blood vessel segmentations from more routinely acquired MR contrasts such as T1 and T2, would be useful. We present an encoder-decoder model for synthesising segmentations of the main cerebral arteries in the circle of Willis (CoW) from only T2 MRI. We propose a two-phase multi-objective learning approach, which captures both global and local features. It uses learned local attention maps generated by dilating the segmentation labels, which forces the network to only extract information from the T2 MRI relevant to synthesising the CoW. Our synthetic vessel segmentations generated from only T2 MRI achieved a mean Dice score of $0.79 \pm 0.03$ in testing, compared to state-of-the-art segmentation networks such as transformer U-Net ($0.71 \pm 0.04$) and nnU-net($0.68 \pm 0.05$), while using only a fraction of the parameters. The main qualitative difference between our synthetic vessel segmentations and the comparative models was in the sharper resolution of the CoW vessel segments, especially in the posterior circulation.
</details>
<details>
<summary>摘要</summary>
磁共振成像（MRA）是一种成像血管的技术，可以用于诊断和评估风险，如血栓roke（由血管壁崩溃引起的血栓roke）。然而，MRA不是常见的成像方式，因此一种能够从常见的MR增强像素，如T1和T2，synthesize血管分 segmentation的方法会很有用。我们提出了一种编码器-解码器模型，可以从T2 MRI中synthesize主要脑血管的圆桌（CoW）分 segmentation。我们采用了两个阶段多目标学习方法，可以捕捉全局和局部特征。它使用学习的本地注意力图，从T2 MRI中提取与synthesize CoW相关的信息，使网络只提取T2 MRI中与synthesize CoW相关的信息。我们使用只有T2 MRI Synthesize的血管分 segmentation在测试中 achieve了 mean dice score为$0.79 \pm 0.03$，比 estado-of-the-art segmentation网络如transformer U-Net ($0.71 \pm 0.04$)和nnU-net ($0.68 \pm 0.05$) 的segmentation网络，而且只用了一小部分的参数。主要的qualitative difference между我们的synthetic vessel segmentation和相关的模型在CoW血管段的高分辨率，尤其是 posterior circulation。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Obstacle-Map-driven-Indoor-Navigation-Model-for-Robust-Obstacle-Avoidance"><a href="#Implicit-Obstacle-Map-driven-Indoor-Navigation-Model-for-Robust-Obstacle-Avoidance" class="headerlink" title="Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance"></a>Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12845">http://arxiv.org/abs/2308.12845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xwaiyy123/object-navigation">https://github.com/xwaiyy123/object-navigation</a></li>
<li>paper_authors: Wei Xie, Haobo Jiang, Shuo Gu, Jin Xie</li>
<li>for: 本研究旨在提高室内导航任务中的目标避免碰撞率，尤其是在视觉图像中缺失障碍物和可能的检测错误问题下。</li>
<li>methods: 本研究提出了一种基于历史尝试和错误经验学习的隐式障碍地图驱动的室内导航框架，以提高避免碰撞的Robustness。同时，一种基于非本地网络的目标念 памя库聚合模块是设计来利用非本地网络来描述导航过程中target semantic和target方向准确的相关性，以便在导航过程中挖掘最相关的物品准确准确。</li>
<li>results: 对于AI2-Thor和RoboTHOR的测试数据集，我们的提出方法得到了优秀的避免碰撞和导航效率。<details>
<summary>Abstract</summary>
Robust obstacle avoidance is one of the critical steps for successful goal-driven indoor navigation tasks.Due to the obstacle missing in the visual image and the possible missed detection issue, visual image-based obstacle avoidance techniques still suffer from unsatisfactory robustness. To mitigate it, in this paper, we propose a novel implicit obstacle map-driven indoor navigation framework for robust obstacle avoidance, where an implicit obstacle map is learned based on the historical trial-and-error experience rather than the visual image. In order to further improve the navigation efficiency, a non-local target memory aggregation module is designed to leverage a non-local network to model the intrinsic relationship between the target semantic and the target orientation clues during the navigation process so as to mine the most target-correlated object clues for the navigation decision. Extensive experimental results on AI2-Thor and RoboTHOR benchmarks verify the excellent obstacle avoidance and navigation efficiency of our proposed method. The core source code is available at https://github.com/xwaiyy123/object-navigation.
</details>
<details>
<summary>摘要</summary>
Robust obstacle avoidance 是indoor navigation任务中的一个关键步骤。由于视觉图像中缺失障碍物和可能的检测问题，视觉图像基于的障碍物避免技术仍然具有不满足的Robustness。为了解决这个问题，在这篇论文中，我们提出了一种基于历史尝试和错误经验学习的隐式障碍地图驱动的indoor navigation框架，以便更好地避免障碍物。为了进一步提高导航效率，我们还设计了一个非本地目标记忆聚合模块，通过非本地网络模型target semantic和导航过程中的target orientation clue之间的内在关系，以便在导航过程中挖掘最相关的目标对象指示。经验结果表明，我们提出的方法具有优秀的障碍物避免和导航效率。主要代码可以在https://github.com/xwaiyy123/object-navigation中获取。
</details></li>
</ul>
<hr>
<h2 id="EFormer-Enhanced-Transformer-towards-Semantic-Contour-Features-of-Foreground-for-Portraits-Matting"><a href="#EFormer-Enhanced-Transformer-towards-Semantic-Contour-Features-of-Foreground-for-Portraits-Matting" class="headerlink" title="EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting"></a>EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12831">http://arxiv.org/abs/2308.12831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zitao Wang, Qiguang Miao, Yue Xi</li>
<li>for: 提取完整的 semantics 和细腻的 outline</li>
<li>methods: 使用 transformers 自动注意 Mechanism，具有更大的接受场景，能够更好地捕捉人脸的长距离依赖关系和低频semantic 信息</li>
<li>results: 提高模型对人脸 outline 的准确性和完整性，并且不需要trimap<details>
<summary>Abstract</summary>
The portrait matting task aims to extract an alpha matte with complete semantics and finely-detailed contours. In comparison to CNN-based approaches, transformers with self-attention allow a larger receptive field, enabling it to better capture long-range dependencies and low-frequency semantic information of a portrait. However, the recent research shows that self-attention mechanism struggle with modeling high-frequency information and capturing fine contour details, which can lead to bias while predicting the portrait's contours. To address the problem, we propose EFormer to enhance the model's attention towards semantic and contour features. Especially the latter, which is surrounded by a large amount of high-frequency details. We build a semantic and contour detector (SCD) to accurately capture the distribution of semantic and contour features. And we further design contour-edge extraction branch and semantic extraction branch for refining contour features and complete semantic information. Finally, we fuse the two kinds of features and leverage the segmentation head to generate the predicted portrait matte. Remarkably, EFormer is an end-to-end trimap-free method and boasts a simple structure. Experiments conducted on VideoMatte240K-JPEGSD and AIM datasets demonstrate that EFormer outperforms previous portrait matte methods.
</details>
<details>
<summary>摘要</summary>
PORTRAIT MATTING TASK的目标是提取一个完整的α抑制矩阵，以捕捉人脸的完整 semantics和细腻的边缘信息。相比CNN基于的方法， transformer自注意力 Mechanism 具有更大的接受场，可以更好地捕捉人脸的长距离依赖关系和低频semantic信息。然而， latest research 表明 that self-attention mechanism 在模型高频信息和细腻边缘特征的处理方面存在困难，可能导致预测人脸的边缘偏倚。为了解决问题，我们提出 EFormer，用于增强模型的注意力 towards semantic和contour特征。特别是后者，它围绕着大量高频细节。我们建立了 semantic和contour探测器 (SCD)，以准确捕捉人脸的semantic和contour特征的分布。此外，我们还设计了 contour-edge extraction branch 和 semantic extraction branch，用于精细化contour特征和完整semantic信息。最后，我们融合两种特征，并利用 segmentation head 生成预测的人脸抑制矩阵。值得注意的是， EFormer 是一种端到端的trimap-free方法，具有简单的结构。在 VideoMatte240K-JPEGSD 和 AIM 数据集上进行的实验表明，EFormer 在人脸抑制矩阵预测方面高效。
</details></li>
</ul>
<hr>
<h2 id="Robotic-Scene-Segmentation-with-Memory-Network-for-Runtime-Surgical-Context-Inference"><a href="#Robotic-Scene-Segmentation-with-Memory-Network-for-Runtime-Surgical-Context-Inference" class="headerlink" title="Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference"></a>Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12789">http://arxiv.org/abs/2308.12789</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uva-dsa/runtime_robscene_seg_2context">https://github.com/uva-dsa/runtime_robscene_seg_2context</a></li>
<li>paper_authors: Zongyu Li, Ian Reyes, Homa Alemzadeh</li>
<li>for: 这 paper 是为了解决runtime context inference在机器助手手术中的挑战，以及提高视频数据的分割精度和时间一致性。</li>
<li>methods: 这 paper 使用了 Space Time Correspondence Network (STCN)，这是一种记忆网络，它可以进行二分分割并减少类别偏见的影响。STCN 使用了记忆银行，以使用过去的图像和分割信息，以确保分割掩模的一致性。</li>
<li>results: 实验表明，STCN 在公共可用的 JIGSAWS 数据集上表现出色，对于难以分割的对象，如针和织物，可以提高分割精度和上下文推断。此外，这 paper 还证明了在 runtime 无需妥协性能的情况下，可以同时进行分割和上下文推断。<details>
<summary>Abstract</summary>
Surgical context inference has recently garnered significant attention in robot-assisted surgery as it can facilitate workflow analysis, skill assessment, and error detection. However, runtime context inference is challenging since it requires timely and accurate detection of the interactions among the tools and objects in the surgical scene based on the segmentation of video data. On the other hand, existing state-of-the-art video segmentation methods are often biased against infrequent classes and fail to provide temporal consistency for segmented masks. This can negatively impact the context inference and accurate detection of critical states. In this study, we propose a solution to these challenges using a Space Time Correspondence Network (STCN). STCN is a memory network that performs binary segmentation and minimizes the effects of class imbalance. The use of a memory bank in STCN allows for the utilization of past image and segmentation information, thereby ensuring consistency of the masks. Our experiments using the publicly available JIGSAWS dataset demonstrate that STCN achieves superior segmentation performance for objects that are difficult to segment, such as needle and thread, and improves context inference compared to the state-of-the-art. We also demonstrate that segmentation and context inference can be performed at runtime without compromising performance.
</details>
<details>
<summary>摘要</summary>
医疗机器人助手中的手术上下文推断在最近几年内受到了广泛关注，因为它可以帮助分析工作流程、评估技能和检测错误。然而，运行时上下文推断具有挑战性，因为它需要在视频数据中检测工具和物品之间的互动，并在实时上提供准确的上下文推断。然而，现有的状态 искусственный智能方法通常对不常见的类型存在偏见，并且无法提供时间上的一致性 для分类mask。这可能会对上下文推断产生负面影响，并妨碍精准检测关键状态。在本研究中，我们提出了一种解决这些挑战的解决方案，即使用Space Time Correspondence Network（STCN）。STCN是一种记忆网络，它可以实现二分 segmentation，并最小化类别不均衡的影响。记忆银行在STCN中的使用，使得可以利用过去的图像和分类信息，以确保mask的一致性。我们使用公共可用的JIGSAWS数据集进行实验，并证明STCN可以在难以分类的对象，如针和织物，中提供superior的分类性能，并提高上下文推断的精度。此外，我们还证明可以在运行时进行分类和上下文推断，不会影响性能。
</details></li>
</ul>
<hr>
<h2 id="On-Offline-Evaluation-of-3D-Object-Detection-for-Autonomous-Driving"><a href="#On-Offline-Evaluation-of-3D-Object-Detection-for-Autonomous-Driving" class="headerlink" title="On Offline Evaluation of 3D Object Detection for Autonomous Driving"></a>On Offline Evaluation of 3D Object Detection for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12779">http://arxiv.org/abs/2308.12779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Schreier, Katrin Renz, Andreas Geiger, Kashyap Chitta</li>
<li>for: 这个论文是为了评估3D对象检测模型在自动驾驶核心任务中的性能而写的。</li>
<li>methods: 这篇论文使用了16种对象检测模型，并在CARLA simulate器上进行了广泛的实验，以评估不同检测精度指标如何影响自动驾驶性能。</li>
<li>results: 研究发现，nuScenes检测得分更高地相关于驾驶性能，而且警告了对&#96; плаanner-centric’指标的封闭依赖。<details>
<summary>Abstract</summary>
Prior work in 3D object detection evaluates models using offline metrics like average precision since closed-loop online evaluation on the downstream driving task is costly. However, it is unclear how indicative offline results are of driving performance. In this work, we perform the first empirical evaluation measuring how predictive different detection metrics are of driving performance when detectors are integrated into a full self-driving stack. We conduct extensive experiments on urban driving in the CARLA simulator using 16 object detection models. We find that the nuScenes Detection Score has a higher correlation to driving performance than the widely used average precision metric. In addition, our results call for caution on the exclusive reliance on the emerging class of `planner-centric' metrics.
</details>
<details>
<summary>摘要</summary>
先前的工作在3D对象检测中通常使用离线指标如平均准确率来评估模型。然而，不清楚这些离线结果对驱动性能的指导性。在本工作中，我们实施了首次employmetric evaluation的研究，measure如何不同的检测指标对自动驱动栈中检测器的驱动性能具有预测性。我们在CARLA simulator上进行了大规模的城市驱动实验，使用16个对象检测模型。我们发现，nuScenes Detection Score与驱动性能之间存在更高的相关性，而且我们的结果表明，应该小心对新般的` плаanner-centric'指标的归类依赖。
</details></li>
</ul>
<hr>
<h2 id="LISTER-Neighbor-Decoding-for-Length-Insensitive-Scene-Text-Recognition"><a href="#LISTER-Neighbor-Decoding-for-Length-Insensitive-Scene-Text-Recognition" class="headerlink" title="LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition"></a>LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12774">http://arxiv.org/abs/2308.12774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changxu Cheng, Peng Wang, Cheng Da, Qi Zheng, Cong Yao</li>
<li>for: 提高Scene Text Recognition（STR）的长文本识别能力和长文本推断能力。</li>
<li>methods: 提出Length-Insensitive Scene TExt Recognizer（LISTER）算法，包括Neighbor Decoder和Feature Enhancement Module两部分。Neighbor Decoder使用帮助器矩阵获取准确的字符注意力地图，不受文本长度影响。Feature Enhancement Module通过低计算成本模型长距离依赖关系，可以逐步增强特征地图进行迭代处理。</li>
<li>results: 实验表明，提出的LISTER算法在长文本识别和长文本推断方面具有显著优势，并且与之前的STATE-OF-THE-ART方法在标准STR测试集（主要是短文本）上表现相当。<details>
<summary>Abstract</summary>
The diversity in length constitutes a significant characteristic of text. Due to the long-tail distribution of text lengths, most existing methods for scene text recognition (STR) only work well on short or seen-length text, lacking the capability of recognizing longer text or performing length extrapolation. This is a crucial issue, since the lengths of the text to be recognized are usually not given in advance in real-world applications, but it has not been adequately investigated in previous works. Therefore, we propose in this paper a method called Length-Insensitive Scene TExt Recognizer (LISTER), which remedies the limitation regarding the robustness to various text lengths. Specifically, a Neighbor Decoder is proposed to obtain accurate character attention maps with the assistance of a novel neighbor matrix regardless of the text lengths. Besides, a Feature Enhancement Module is devised to model the long-range dependency with low computation cost, which is able to perform iterations with the neighbor decoder to enhance the feature map progressively. To the best of our knowledge, we are the first to achieve effective length-insensitive scene text recognition. Extensive experiments demonstrate that the proposed LISTER algorithm exhibits obvious superiority on long text recognition and the ability for length extrapolation, while comparing favourably with the previous state-of-the-art methods on standard benchmarks for STR (mainly short text).
</details>
<details>
<summary>摘要</summary>
Text 的多样性在长度方面是一个重要特征。由于文本长度的长尾分布，大多数现有的场景文本识别（STR）方法只能在短文本上工作良好，缺乏长文本或者长文本 extrapolation 的能力。这是一个关键问题，因为实际应用中文本的长度通常不会提前给出，但这一问题在前一未得到充分调查。因此，我们在这篇论文中提出了一种名为Length-Insensitive Scene TExt Recognizer（LISTER）的方法，以解决这种限制。具体来说，我们提出了一种邻居解码器，可以通过一个新的邻居矩阵获得准确的字符注意地图，不管文本的长度。此外，我们还设计了一种特征增强模块，可以通过低计算成本模elling 长距离关系，并且可以与邻居解码器进行多次迭代来进一步增强特征图。根据我们所知，我们的提出的 LISTER 算法是首次实现了长度不敏感的场景文本识别。我们的实验结果表明，LISTER 算法在长文本识别和长度推算方面具有明显的优势，同时与之前的状态lava 方法在标准 STR 标准库（主要是短文本）上比较了常。
</details></li>
</ul>
<hr>
<h2 id="IP-UNet-Intensity-Projection-UNet-Architecture-for-3D-Medical-Volume-Segmentation"><a href="#IP-UNet-Intensity-Projection-UNet-Architecture-for-3D-Medical-Volume-Segmentation" class="headerlink" title="IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation"></a>IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12761">http://arxiv.org/abs/2308.12761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nyothiri Aung, Tahar Kechadi, Liming Chen, Sahraoui Dhelim<br>for:  automatic breast calcification detectionmethods:  IP-UNet model, which performs multi-class segmentation on Intensity Projection (IP) of 3D volumetric data, and uses limited memory capability for training without losing the original 3D image resolution.results:  IP-UNet achieves similar segmentation accuracy as 3D-UNet but with much better performance, reducing training time by 70% and memory consumption by 92%.<details>
<summary>Abstract</summary>
CNNs have been widely applied for medical image analysis. However, limited memory capacity is one of the most common drawbacks of processing high-resolution 3D volumetric data. 3D volumes are usually cropped or downsized first before processing, which can result in a loss of resolution, increase class imbalance, and affect the performance of the segmentation algorithms. In this paper, we propose an end-to-end deep learning approach called IP-UNet. IP-UNet is a UNet-based model that performs multi-class segmentation on Intensity Projection (IP) of 3D volumetric data instead of the memory-consuming 3D volumes. IP-UNet uses limited memory capability for training without losing the original 3D image resolution. We compare the performance of three models in terms of segmentation accuracy and computational cost: 1) Slice-by-slice 2D segmentation of the CT scan images using a conventional 2D UNet model. 2) IP-UNet that operates on data obtained by merging the extracted Maximum Intensity Projection (MIP), Closest Vessel Projection (CVP), and Average Intensity Projection (AvgIP) representations of the source 3D volumes, then applying the UNet model on the output IP images. 3) 3D-UNet model directly reads the 3D volumes constructed from a series of CT scan images and outputs the 3D volume of the predicted segmentation. We test the performance of these methods on 3D volumetric images for automatic breast calcification detection. Experimental results show that IP-Unet can achieve similar segmentation accuracy with 3D-Unet but with much better performance. It reduces the training time by 70\% and memory consumption by 92\%.
</details>
<details>
<summary>摘要</summary>
对于医疗影像分析，广泛应用了深度学习网络（CNN）。然而，处理高分辨率3D数据时的内存容量问题是最常见的问题。通常会将3Dvolume裁剪或缩小以便处理，这会导致解析损失、增加分布不均和影像分析表现下降。在这篇论文中，我们提出了一个端到端的深度学习方法，即IP-UNet。IP-UNet是基于UNet模型，用于多类分类INTENSITY PROJECTION（IP）3D数据，而不需要内存昂贵的3Dvolume训练。我们将比较三种模型的表现，包括：1）对CT扫描影像的单面2D分类使用传统2D UNet模型。2）IP-UNet，它在提取Maximum Intensity Projection（MIP）、Closest Vessel Projection（CVP）和Average Intensity Projection（AvgIP）表现后，将UNet模型应用于输出IP图像。3）直接从CT扫描影像构建3D数据，并将预测分类结果传回3D数据。我们将这些方法评估在3D数据中自动胸腔癌斑准确性检测上。实验结果显示，IP-UNet可以与3D-UNet实现相似的分类精度，但是具有训练时间快速和内存消耗几乎减半的优点。
</details></li>
</ul>
<hr>
<h2 id="PartSeg-Few-shot-Part-Segmentation-via-Part-aware-Prompt-Learning"><a href="#PartSeg-Few-shot-Part-Segmentation-via-Part-aware-Prompt-Learning" class="headerlink" title="PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning"></a>PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12757">http://arxiv.org/abs/2308.12757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengya Han, Heliang Zheng, Chaoyue Wang, Yong Luo, Han Hu, Jing Zhang, Yonggang Wen</li>
<li>for: 本研究旨在实现少数示例部分 segmentation，即使用非常少的标注示例来分割未经见过的物体中的不同部分。</li>
<li>methods: 我们提出了一种基于多Modal学习的新方法，称为PartSeg，用于实现少数示例部分 segmentation。我们特别设计了一种可以让CLIP模型更好地理解“部分”概念的部分掌握学习方法。此外，我们在提问学习过程中建立了不同物体类别中同一部分之间的关系。</li>
<li>results: 我们在PartImageNet和Pascal$_$Part datasets上进行了广泛的实验，结果表明，我们提出的方法可以达到状态 искусственный智能的表现。<details>
<summary>Abstract</summary>
In this work, we address the task of few-shot part segmentation, which aims to segment the different parts of an unseen object using very few labeled examples. It is found that leveraging the textual space of a powerful pre-trained image-language model (such as CLIP) can be beneficial in learning visual features. Therefore, we develop a novel method termed PartSeg for few-shot part segmentation based on multimodal learning. Specifically, we design a part-aware prompt learning method to generate part-specific prompts that enable the CLIP model to better understand the concept of ``part'' and fully utilize its textual space. Furthermore, since the concept of the same part under different object categories is general, we establish relationships between these parts during the prompt learning process. We conduct extensive experiments on the PartImageNet and Pascal$\_$Part datasets, and the experimental results demonstrated that our proposed method achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们考虑了几个shot部分 segmentation任务，该任务的目标是使用非常少的标注例进行不同对象的部分分类。我们发现可以利用一个强大预训练的图像语言模型（如CLIP）的文本空间，可以有利于学习视觉特征。因此，我们开发了一种基于多Modal学习的新方法，称为PartSeg。具体来说，我们设计了一种部分意识的提问学习方法，以便CLIP模型更好地理解“部分”的概念，并充分利用其文本空间。此外，我们在提问学习过程中建立了这些部分之间的关系。我们在PartImageNet和Pascal$\_$Part datasets上进行了广泛的实验，并发现我们的提议方法可以达到状态的表现。
</details></li>
</ul>
<hr>
<h2 id="Learning-Heavily-Degraded-Prior-for-Underwater-Object-Detection"><a href="#Learning-Heavily-Degraded-Prior-for-Underwater-Object-Detection" class="headerlink" title="Learning Heavily-Degraded Prior for Underwater Object Detection"></a>Learning Heavily-Degraded Prior for Underwater Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12738">http://arxiv.org/abs/2308.12738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaodetection/learning-heavily-degraed-prior">https://github.com/xiaodetection/learning-heavily-degraed-prior</a></li>
<li>paper_authors: Chenping Fu, Xin Fan, Jiewen Xiao, Wanqi Yuan, Risheng Liu, Zhongxuan Luo</li>
<li>for: 解决水下物体检测中的质量下降问题，通过利用受损图像中的特征分布偏移来提高检测性能。</li>
<li>methods: 基于受损图像的统计观察，提出了差异特征传递模块（RFTM），通过学习受损图像和水下图像之间的映射，提高水下物体检测性能。</li>
<li>results: 对URPC2020和UODD数据集进行评估，显示 compared to CNN基于的检测器，本方法可以大幅提高水下物体检测性能，并且具有更高的速度和更少的参数。<details>
<summary>Abstract</summary>
Underwater object detection suffers from low detection performance because the distance and wavelength dependent imaging process yield evident image quality degradations such as haze-like effects, low visibility, and color distortions. Therefore, we commit to resolving the issue of underwater object detection with compounded environmental degradations. Typical approaches attempt to develop sophisticated deep architecture to generate high-quality images or features. However, these methods are only work for limited ranges because imaging factors are either unstable, too sensitive, or compounded. Unlike these approaches catering for high-quality images or features, this paper seeks transferable prior knowledge from detector-friendly images. The prior guides detectors removing degradations that interfere with detection. It is based on statistical observations that, the heavily degraded regions of detector-friendly (DFUI) and underwater images have evident feature distribution gaps while the lightly degraded regions of them overlap each other. Therefore, we propose a residual feature transference module (RFTM) to learn a mapping between deep representations of the heavily degraded patches of DFUI- and underwater- images, and make the mapping as a heavily degraded prior (HDP) for underwater detection. Since the statistical properties are independent to image content, HDP can be learned without the supervision of semantic labels and plugged into popular CNNbased feature extraction networks to improve their performance on underwater object detection. Without bells and whistles, evaluations on URPC2020 and UODD show that our methods outperform CNN-based detectors by a large margin. Our method with higher speeds and less parameters still performs better than transformer-based detectors. Our code and DFUI dataset can be found in https://github.com/xiaoDetection/Learning-Heavily-Degraed-Prior.
</details>
<details>
<summary>摘要</summary>
水下对象检测受到质量低下的影响，因为图像处理过程中的距离和波长的效果会导致明显的图像质量下降，如霾效、低可见度和颜色扭曲。因此，我们决心解决水下对象检测中的环境降低效应。常见的方法是开发复杂的深度架构来生成高质量图像或特征。然而，这些方法只能在有限的范围内工作，因为图像因素是不稳定、太敏感或复杂的。与这些方法不同，这篇论文寻求从检测友好的图像（DFUI）中提取知识。这种知识指导检测器去除影响检测的降低因素。这基于统计观察，水下和DFUI图像中的严重降低区域有明显的特征分布差异，而轻度降低区域之间重叠。因此，我们提出了差异特征传递模块（RFTM），以学习将深度表示DFUI图像中的严重降低区域和水下图像之间建立一个映射，并将这个映射作为强制质量（HDP）来进行水下检测。由于统计性质独立于图像内容，HDP可以在无监督的情况下学习，并可以插入流行的CNN基于特征提取网络来提高水下对象检测性能。我们的方法在URPC2020和UODD上进行评估，与CNN基于检测器相比，我们的方法在大幅度下表现出较好的性能。我们的方法还具有更高的速度和更少的参数，仍然可以在 transformer 基于检测器之上进行改进。我们的代码和 DFUI 数据集可以在 GitHub 上找到：https://github.com/xiaoDetection/Learning-Heavily-Degraed-Prior。
</details></li>
</ul>
<hr>
<h2 id="FastSurfer-HypVINN-Automated-sub-segmentation-of-the-hypothalamus-and-adjacent-structures-on-high-resolutional-brain-MRI"><a href="#FastSurfer-HypVINN-Automated-sub-segmentation-of-the-hypothalamus-and-adjacent-structures-on-high-resolutional-brain-MRI" class="headerlink" title="FastSurfer-HypVINN: Automated sub-segmentation of the hypothalamus and adjacent structures on high-resolutional brain MRI"></a>FastSurfer-HypVINN: Automated sub-segmentation of the hypothalamus and adjacent structures on high-resolutional brain MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12736">http://arxiv.org/abs/2308.12736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Estrada, David Kügler, Emad Bahrami, Peng Xu, Dilshad Mousa, Monique M. B. Breteler, N. Ahmad Aziz, Martin Reuter</li>
<li>for: 这个论文的目的是提供一种自动分割哈米尼肌肉的方法，以便更好地研究哈米尼肌肉的功能和结构。</li>
<li>methods: 这个方法使用了深度学习算法，并且可以处理0.8mm的是otropic T1w和T2w MR图像。</li>
<li>results: 这个方法可以具有高度的分割精度和可靠性，并且可以在多个数据集上进行扩展验证。<details>
<summary>Abstract</summary>
The hypothalamus plays a crucial role in the regulation of a broad range of physiological, behavioural, and cognitive functions. However, despite its importance, only a few small-scale neuroimaging studies have investigated its substructures, likely due to the lack of fully automated segmentation tools to address scalability and reproducibility issues of manual segmentation. While the only previous attempt to automatically sub-segment the hypothalamus with a neural network showed promise for 1.0 mm isotropic T1-weighted (T1w) MRI, there is a need for an automated tool to sub-segment also high-resolutional (HiRes) MR scans, as they are becoming widely available, and include structural detail also from multi-modal MRI. We, therefore, introduce a novel, fast, and fully automated deep learning method named HypVINN for sub-segmentation of the hypothalamus and adjacent structures on 0.8 mm isotropic T1w and T2w brain MR images that is robust to missing modalities. We extensively validate our model with respect to segmentation accuracy, generalizability, in-session test-retest reliability, and sensitivity to replicate hypothalamic volume effects (e.g. sex-differences). The proposed method exhibits high segmentation performance both for standalone T1w images as well as for T1w/T2w image pairs. Even with the additional capability to accept flexible inputs, our model matches or exceeds the performance of state-of-the-art methods with fixed inputs. We, further, demonstrate the generalizability of our method in experiments with 1.0 mm MR scans from both the Rhineland Study and the UK Biobank. Finally, HypVINN can perform the segmentation in less than a minute (GPU) and will be available in the open source FastSurfer neuroimaging software suite, offering a validated, efficient, and scalable solution for evaluating imaging-derived phenotypes of the hypothalamus.
</details>
<details>
<summary>摘要</summary>
《响应腔室中的肥厚腔室功能》的研究具有重要的意义，但由于缺乏可靠的自动分割工具，因此只有一些小规模的 нейро成像研究对其下部结构进行了调查。尽管之前一个使用神经网络自动分割肥厚腔室的尝试显示了对1.0 mm是otropic T1束成像（T1w）的承诺，但是有必要为高分辨率（HiRes）MR扫描图像提供自动分割工具，因为它们在广泛使用并包含多modal MRI结构细节。我们因此介绍了一种新的快速、自动化深度学习方法，名为 HypVINN，用于肥厚腔室和相邻结构的0.8 mm是otropic T1w和T2w大脑MR扫描图像的分割，具有对缺失模式的Robust性。我们对模型进行了广泛验证，包括分割精度、普适性、在SESSION中的重复测试可靠性和性别差异的敏感性。我们的模型在单独的T1w图像上以及T1w/T2w图像对上都 exhibits高度的分割性能。即使可以接受 flexible inputs，我们的模型与已有的方法相当或超过性能。我们进一步证明了我们的方法在1.0 mm MR扫描图像上的普适性，并在 Rheinland Study和UK Biobank中进行了实验。最后，HypVINN可以在 less than a minute（GPU）内完成分割，并将被包含在开源的 FastSurfer neuroscience imaging software suite中，提供一个验证、高效、扩展的解决方案，用于评估基于成像的肥厚腔室相关性。
</details></li>
</ul>
<hr>
<h2 id="Ground-to-Aerial-Person-Search-Benchmark-Dataset-and-Approach"><a href="#Ground-to-Aerial-Person-Search-Benchmark-Dataset-and-Approach" class="headerlink" title="Ground-to-Aerial Person Search: Benchmark Dataset and Approach"></a>Ground-to-Aerial Person Search: Benchmark Dataset and Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12712">http://arxiv.org/abs/2308.12712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yqc123456/hkd_for_person_search">https://github.com/yqc123456/hkd_for_person_search</a></li>
<li>paper_authors: Shizhou Zhang, Qingchun Yang, De Cheng, Yinghui Xing, Guoqiang Liang, Peng Wang, Yanning Zhang</li>
<li>for: 这个论文旨在构建一个大规模的人体搜索数据集，以便进行跨平台智能监测应用程序开发。</li>
<li>methods: 该论文使用了两步人体搜索方法和终端到终端人体搜索方法，并提出了一种简单 yet effective的知识储存方法，用于提高人体搜索性能。</li>
<li>results: 该论文通过对G2APS数据集和两个公共的人体搜索数据集进行分析，并提出了一种基于知识储存的人体搜索方法，实现了状态畅的性能。<details>
<summary>Abstract</summary>
In this work, we construct a large-scale dataset for Ground-to-Aerial Person Search, named G2APS, which contains 31,770 images of 260,559 annotated bounding boxes for 2,644 identities appearing in both of the UAVs and ground surveillance cameras. To our knowledge, this is the first dataset for cross-platform intelligent surveillance applications, where the UAVs could work as a powerful complement for the ground surveillance cameras. To more realistically simulate the actual cross-platform Ground-to-Aerial surveillance scenarios, the surveillance cameras are fixed about 2 meters above the ground, while the UAVs capture videos of persons at different location, with a variety of view-angles, flight attitudes and flight modes. Therefore, the dataset has the following unique characteristics: 1) drastic view-angle changes between query and gallery person images from cross-platform cameras; 2) diverse resolutions, poses and views of the person images under 9 rich real-world scenarios. On basis of the G2APS benchmark dataset, we demonstrate detailed analysis about current two-step and end-to-end person search methods, and further propose a simple yet effective knowledge distillation scheme on the head of the ReID network, which achieves state-of-the-art performances on both of the G2APS and the previous two public person search datasets, i.e., PRW and CUHK-SYSU. The dataset and source code available on \url{https://github.com/yqc123456/HKD_for_person_search}.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们构建了一个大规模的人earch数据集，名为G2APS，其包含31,770张图像和260,559个注解的矩形框，其中每个矩形框都包含2,644个人肖象出现在UAV和地面监测摄像头中。我们知道，这是首个跨平台智能监测应用程序的数据集，UAV可以作为地面监测摄像头的强力补充。为更真实地模拟实际跨平台地面-空中监测场景，地面监测摄像头 fixes在2米上，而UAV拍摄了不同位置的人肖象，并且有多种视角、飞行姿态和飞行模式。因此，该数据集具有以下独特特点：1）跨平台摄像头之间人肖象的极大视角变化；2）人肖象的多种分辨率、姿势和视野下的9种实际场景。基于G2APS标准数据集，我们对现有的两步人earch方法和端到端人earch方法进行详细分析，并提出了一种简单 yet有效的知识储存 scheme，用于在ReID网络的头部进行人earch，该方法在G2APS和以前两个公共人earch数据集上实现了状态当前性。数据集和源代码可以在 \url{https://github.com/yqc123456/HKD_for_person_search} 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Parse-Then-Place-Approach-for-Generating-Graphic-Layouts-from-Textual-Descriptions"><a href="#A-Parse-Then-Place-Approach-for-Generating-Graphic-Layouts-from-Textual-Descriptions" class="headerlink" title="A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions"></a>A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12700">http://arxiv.org/abs/2308.12700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Lin, Jiaqi Guo, Shizhao Sun, Weijiang Xu, Ting Liu, Jian-Guang Lou, Dongmei Zhang</li>
<li>for: 这个论文的目的是提出一种基于文本指导的图形设计方法，以低下设计难度。</li>
<li>methods: 这种方法包括两个阶段：解析阶段和放置阶段。解析阶段通过将文本描述转换为一种中间表示（IR），来模拟文本中的隐式约束。放置阶段使用Transformer模型生成图形。为了处理组合和不完整的约束，我们使用Transformer模型并且 специаль地设计了约束和图形的表示方式。</li>
<li>results: 我们在两个 Text-to-Layout 数据集上进行了实验，并取得了优秀的成绩。量化结果、质量分析和用户研究都证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Creating layouts is a fundamental step in graphic design. In this work, we propose to use text as the guidance to create graphic layouts, i.e., Text-to-Layout, aiming to lower the design barriers. Text-to-Layout is a challenging task, because it needs to consider the implicit, combined, and incomplete layout constraints from text, each of which has not been studied in previous work. To address this, we present a two-stage approach, named parse-then-place. The approach introduces an intermediate representation (IR) between text and layout to represent diverse layout constraints. With IR, Text-to-Layout is decomposed into a parse stage and a place stage. The parse stage takes a textual description as input and generates an IR, in which the implicit constraints from the text are transformed into explicit ones. The place stage generates layouts based on the IR. To model combined and incomplete constraints, we use a Transformer-based layout generation model and carefully design a way to represent constraints and layouts as sequences. Besides, we adopt the pretrain-then-finetune strategy to boost the performance of the layout generation model with large-scale unlabeled layouts. To evaluate our approach, we construct two Text-to-Layout datasets and conduct experiments on them. Quantitative results, qualitative analysis, and user studies demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
创建布局是图形设计的基本步骤。在这项工作中，我们提议使用文本作为布局创建的指导，即文本到布局（Text-to-Layout），以降低设计障碍。文本到布局是一项复杂的任务，因为它需要考虑文本中的隐式、共同和部分缺失的布局约束，每一种都没有在前期工作中研究过。为解决这个问题，我们提出了两个阶段方法，称之为parse-then-place。这种方法引入了一个中间表示（IR），用于将文本中的布局约束转换为Explicit的约束。在IR中，我们使用Transformer模型来生成布局。此外，我们还设计了一种方法来表示约束和布局为序列，以便处理共同和缺失的约束。此外，我们采用了预训练后finetune策略，以提高布局生成模型的性能。为评估我们的方法，我们构建了两个文本到布局数据集，并在其中进行了实验。量化结果、质量分析和用户研究都证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-Continual-Learning-Approach-for-Cross-Domain-White-Blood-Cell-Classification"><a href="#A-Continual-Learning-Approach-for-Cross-Domain-White-Blood-Cell-Classification" class="headerlink" title="A Continual Learning Approach for Cross-Domain White Blood Cell Classification"></a>A Continual Learning Approach for Cross-Domain White Blood Cell Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12679">http://arxiv.org/abs/2308.12679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ario Sadafi, Raheleh Salehi, Armin Gruber, Sayedali Shetab Boushehri, Pascal Giehr, Nassir Navab, Carsten Marr</li>
<li>for: 这个研究旨在提高白血球类别的准确性，以便诊断血液疾病。</li>
<li>methods: 本研究使用了一种叫做复习式专有学习的方法，可以逐步学习来自新数据流的知识，而不会忘记之前学习的知识。</li>
<li>results: 研究结果显示，使用复习式专有学习方法可以在不同的颜色、分辨率和类别结构下，实现白血球类别的准确分类。此外，在长期演进学习中，本方法也可以优于现有的iCaRL和EWC方法。<details>
<summary>Abstract</summary>
Accurate classification of white blood cells in peripheral blood is essential for diagnosing hematological diseases. Due to constantly evolving clinical settings, data sources, and disease classifications, it is necessary to update machine learning classification models regularly for practical real-world use. Such models significantly benefit from sequentially learning from incoming data streams without forgetting previously acquired knowledge. However, models can suffer from catastrophic forgetting, causing a drop in performance on previous tasks when fine-tuned on new data. Here, we propose a rehearsal-based continual learning approach for class incremental and domain incremental scenarios in white blood cell classification. To choose representative samples from previous tasks, we employ exemplar set selection based on the model's predictions. This involves selecting the most confident samples and the most challenging samples identified through uncertainty estimation of the model. We thoroughly evaluated our proposed approach on three white blood cell classification datasets that differ in color, resolution, and class composition, including scenarios where new domains or new classes are introduced to the model with every task. We also test a long class incremental experiment with both new domains and new classes. Our results demonstrate that our approach outperforms established baselines in continual learning, including existing iCaRL and EWC methods for classifying white blood cells in cross-domain environments.
</details>
<details>
<summary>摘要</summary>
Accurate classification of white blood cells in peripheral blood is essential for diagnosing hematological diseases. Due to constantly evolving clinical settings, data sources, and disease classifications, it is necessary to update machine learning classification models regularly for practical real-world use. Such models significantly benefit from sequentially learning from incoming data streams without forgetting previously acquired knowledge. However, models can suffer from catastrophic forgetting, causing a drop in performance on previous tasks when fine-tuned on new data. Here, we propose a rehearsal-based continual learning approach for class incremental and domain incremental scenarios in white blood cell classification. To choose representative samples from previous tasks, we employ exemplar set selection based on the model's predictions. This involves selecting the most confident samples and the most challenging samples identified through uncertainty estimation of the model. We thoroughly evaluated our proposed approach on three white blood cell classification datasets that differ in color, resolution, and class composition, including scenarios where new domains or new classes are introduced to the model with every task. We also test a long class incremental experiment with both new domains and new classes. Our results demonstrate that our approach outperforms established baselines in continual learning, including existing iCaRL and EWC methods for classifying white blood cells in cross-domain environments.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="A-Study-of-Age-and-Sex-Bias-in-Multiple-Instance-Learning-based-Classification-of-Acute-Myeloid-Leukemia-Subtypes"><a href="#A-Study-of-Age-and-Sex-Bias-in-Multiple-Instance-Learning-based-Classification-of-Acute-Myeloid-Leukemia-Subtypes" class="headerlink" title="A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes"></a>A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12675">http://arxiv.org/abs/2308.12675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ario Sadafi, Matthias Hehr, Nassir Navab, Carsten Marr<br>for: 这份研究旨在探讨急性白血病（AML）分型的精确分类是否受到年龄和性别偏好的影响，以提高临床决策和患者照顾。methods: 这份研究使用多例学习（MIL）架构，训练多个MIL模型，并评估它们在不同的性别偏好和年龄偏好下的表现。results: 研究发现，AML分型分类中受到性别和年龄偏好的影响，特别是女性患者更容易受到性别偏好的影响，而 certain age groups，例如72-86岁的患者，则受到年龄偏好的影响。确保训练数据的多元性是关键，以确保AML分型分类的可靠性和公平性，最终帮助多元化的患者人口。<details>
<summary>Abstract</summary>
Accurate classification of Acute Myeloid Leukemia (AML) subtypes is crucial for clinical decision-making and patient care. In this study, we investigate the potential presence of age and sex bias in AML subtype classification using Multiple Instance Learning (MIL) architectures. To that end, we train multiple MIL models using different levels of sex imbalance in the training set and excluding certain age groups. To assess the sex bias, we evaluate the performance of the models on male and female test sets. For age bias, models are tested against underrepresented age groups in the training data. We find a significant effect of sex and age bias on the performance of the model for AML subtype classification. Specifically, we observe that females are more likely to be affected by sex imbalance dataset and certain age groups, such as patients with 72 to 86 years of age with the RUNX1::RUNX1T1 genetic subtype, are significantly affected by an age bias present in the training data. Ensuring inclusivity in the training data is thus essential for generating reliable and equitable outcomes in AML genetic subtype classification, ultimately benefiting diverse patient populations.
</details>
<details>
<summary>摘要</summary>
《急性白细胞病（AML）分型准确分类是临床决策和患者护理中非常重要。本研究探讨AML分型准确分类中年龄和性别偏见的可能性，使用多例学习（MIL）架构。为此，我们在不同的性别占比水平和年龄组中训练多个MIL模型，并在测试集上评估模型的性别偏见和年龄偏见。结果显示，女性患者更容易受到数据集中的性别偏见的影响，而72-86岁的年龄组患者则受到训练数据中的年龄偏见的影响。因此，在训练数据中保证包容性是AML分型准确分类中不可或缺的。这有助于促进多样化患者群体的可靠和公平的结果，最终总是有利于患者的护理。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Masked-Feature-Modelling-Feature-Masking-for-the-Unsupervised-Pre-training-of-a-Graph-Attention-Network-Block-for-Bottom-up-Video-Event-Recognition"><a href="#Masked-Feature-Modelling-Feature-Masking-for-the-Unsupervised-Pre-training-of-a-Graph-Attention-Network-Block-for-Bottom-up-Video-Event-Recognition" class="headerlink" title="Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition"></a>Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12673">http://arxiv.org/abs/2308.12673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Daskalakis, Nikolaos Gkalelis, Vasileios Mezaris</li>
<li>for: 本研究旨在提出一种未监督预训练方法，用于提高视频事件识别模型的起点和总体性能。</li>
<li>methods: 本研究使用了一种已经预训练的视觉Tokenizer来重建视频中对象的遮盖特征，然后将预训练的GAT块 integrate到现有的视频事件识别架构中，以提高模型的起点和总体性能。</li>
<li>results: 实验结果表明，使用Masked Feature Modelling（MFM）方法可以提高视频事件识别性能。<details>
<summary>Abstract</summary>
In this paper, we introduce Masked Feature Modelling (MFM), a novel approach for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM utilizes a pretrained Visual Tokenizer to reconstruct masked features of objects within a video, leveraging the MiniKinetics dataset. We then incorporate the pre-trained GAT block into a state-of-the-art bottom-up supervised video-event recognition architecture, ViGAT, to improve the model's starting point and overall accuracy. Experimental evaluations on the YLI-MED dataset demonstrate the effectiveness of MFM in improving event recognition performance.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的无监督预训练方法，即Masked Feature Modelling（MFM），用于提高视频事件认知性能。MFM使用一个预训练的视觉分词器来重建视频中对象的遮盲特征，利用MiniKinetics dataset。然后，我们将预训练的GAT块 integrate到了一个现有的 bottom-up 超级视频事件识别架构ViGAT中，以提高模型的起点和总体准确率。实验评估在YLI-MED数据集上，表明MFM有效地提高事件识别性能。
</details></li>
</ul>
<hr>
<h2 id="An-All-Deep-System-for-Badminton-Game-Analysis"><a href="#An-All-Deep-System-for-Badminton-Game-Analysis" class="headerlink" title="An All Deep System for Badminton Game Analysis"></a>An All Deep System for Badminton Game Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12645">http://arxiv.org/abs/2308.12645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Yung Chou, Yu-Chun Lo, Bo-Zheng Xie, Cheng-Hung Lin, Yu-Yung Kao</li>
<li>for:  automatic detection of events within badminton match videos, especially the shuttlecock</li>
<li>methods:  modified TrackNet model and diverse data types to improve precision</li>
<li>results:  score of 0.78 out of 1.0 in the challenge<details>
<summary>Abstract</summary>
The CoachAI Badminton 2023 Track1 initiative aim to automatically detect events within badminton match videos. Detecting small objects, especially the shuttlecock, is of quite importance and demands high precision within the challenge. Such detection is crucial for tasks like hit count, hitting time, and hitting location. However, even after revising the well-regarded shuttlecock detecting model, TrackNet, our object detection models still fall short of the desired accuracy. To address this issue, we've implemented various deep learning methods to tackle the problems arising from noisy detectied data, leveraging diverse data types to improve precision. In this report, we detail the detection model modifications we've made and our approach to the 11 tasks. Notably, our system garnered a score of 0.78 out of 1.0 in the challenge.
</details>
<details>
<summary>摘要</summary>
coachai 羽毛球 2023 跟踪1 INITIATIVE 目标是自动探测羽毛球赛事视频中的事件。 特别是小 object，如羽毛球，需要高精度的探测，这是因为这些探测对于hit count、 hitting time 和 hitting location 等任务非常重要。 不过，即使修改了 widely recognized 的羽毛球探测模型 TrackNet，我们的 object detection 模型仍然没有达到所需的准确性。 为了解决这个问题，我们采用了多种深度学习方法，以提高不同数据类型的精度。 在这份报告中，我们详细介绍了我们对模型的修改和我们对11个任务的方法。 值得注意的是，我们的系统在挑战中得到了 0.78 分的成绩。
</details></li>
</ul>
<hr>
<h2 id="Tag-Based-Annotation-for-Avatar-Face-Creation"><a href="#Tag-Based-Annotation-for-Avatar-Face-Creation" class="headerlink" title="Tag-Based Annotation for Avatar Face Creation"></a>Tag-Based Annotation for Avatar Face Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12642">http://arxiv.org/abs/2308.12642</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Ngo, Daniel Phelps, Derrick Lai, Thanyared Wong, Lucas Mathias, Anish Shivamurthy, Mustafa Ajmal, Minghao Liu, James Davis</li>
<li>for: 这篇论文的目的是如何自动生成数字人物图像。</li>
<li>methods: 这篇论文使用了标签基于注释的方法来训练模型生成人物图像。</li>
<li>results: 这篇论文的结果是通过标签基于注释的方法来提高模型的预测质量和降低噪音水平。<details>
<summary>Abstract</summary>
Currently, digital avatars can be created manually using human images as reference. Systems such as Bitmoji are excellent producers of detailed avatar designs, with hundreds of choices for customization. A supervised learning model could be trained to generate avatars automatically, but the hundreds of possible options create difficulty in securing non-noisy data to train a model. As a solution, we train a model to produce avatars from human images using tag-based annotations. This method provides better annotator agreement, leading to less noisy data and higher quality model predictions. Our contribution is an application of tag-based annotation to train a model for avatar face creation. We design tags for 3 different facial facial features offered by Bitmoji, and train a model using tag-based annotation to predict the nose.
</details>
<details>
<summary>摘要</summary>
当前，数字化人物可以通过人像作为参考来手动创建。系统如Bitmoji可以生成细节rich的人物设计，具有数百个个性化选项。一个监督学习模型可以自动生成人物，但是数百个可能的选项带来难度，困难于获得不含噪声数据来训练模型。为解决这个问题，我们使用标签基本注解来训练模型生成人物脸。这种方法可以提供更好的注释协议，从而减少噪声数据和提高模型预测质量。我们的贡献是通过标签基本注解来训练模型，以生成人物脸。我们设计了Bitmoji提供的三种不同的 facial 特征标签，并使用标签基本注解来预测脸的鼻子。
</details></li>
</ul>
<hr>
<h2 id="Cross-Video-Contextual-Knowledge-Exploration-and-Exploitation-for-Ambiguity-Reduction-in-Weakly-Supervised-Temporal-Action-Localization"><a href="#Cross-Video-Contextual-Knowledge-Exploration-and-Exploitation-for-Ambiguity-Reduction-in-Weakly-Supervised-Temporal-Action-Localization" class="headerlink" title="Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization"></a>Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12609">http://arxiv.org/abs/2308.12609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songchun Zhang, Chunhui Zhao</li>
<li>for: 本研究旨在提高无rimmed视频中动作地址的准确性和效率，使用视频级标签进行weakly supervised temporal action localization。</li>
<li>methods: 我们提出了一个综合的框架，包括Robust Memory-Guided Contrastive Learning（RMGCL）模块和Global Knowledge Summarization and Aggregation（GKSA）模块，以挖掘和利用跨视频动作特征的相似性和一致性，从而提高动作特征的结构化编码，并降低分类学习中的ambiguity。</li>
<li>results: 我们的方法在THUMOS14、ActivityNet1.3和FineAction等三个 datasets上进行了广泛的实验，结果显示，我们的方法可以高效地提高无rimmed视频中动作地址的准确性和效率，并且可以与其他WSTAL方法结合使用。<details>
<summary>Abstract</summary>
Weakly supervised temporal action localization (WSTAL) aims to localize actions in untrimmed videos using video-level labels. Despite recent advances, existing approaches mainly follow a localization-by-classification pipeline, generally processing each segment individually, thereby exploiting only limited contextual information. As a result, the model will lack a comprehensive understanding (e.g. appearance and temporal structure) of various action patterns, leading to ambiguity in classification learning and temporal localization. Our work addresses this from a novel perspective, by exploring and exploiting the cross-video contextual knowledge within the dataset to recover the dataset-level semantic structure of action instances via weak labels only, thereby indirectly improving the holistic understanding of fine-grained action patterns and alleviating the aforementioned ambiguities. Specifically, an end-to-end framework is proposed, including a Robust Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge Summarization and Aggregation (GKSA) module. First, the RMGCL module explores the contrast and consistency of cross-video action features, assisting in learning more structured and compact embedding space, thus reducing ambiguity in classification learning. Further, the GKSA module is used to efficiently summarize and propagate the cross-video representative action knowledge in a learnable manner to promote holistic action patterns understanding, which in turn allows the generation of high-confidence pseudo-labels for self-learning, thus alleviating ambiguity in temporal localization. Extensive experiments on THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method outperforms the state-of-the-art methods, and can be easily plugged into other WSTAL methods.
</details>
<details>
<summary>摘要</summary>
弱类超级视频动作地标（WSTAL）目标是使用视频级标签来地标视频中的动作。 DESPITE recent advances, existing methods mainly follow a localization-by-classification pipeline, which only utilizes limited contextual information. As a result, the model may lack a comprehensive understanding (e.g., appearance and temporal structure) of various action patterns, leading to ambiguity in classification learning and temporal localization. Our work addresses this issue from a novel perspective by exploring and exploiting the cross-video contextual knowledge within the dataset to recover the dataset-level semantic structure of action instances via weak labels only, thereby indirectly improving the holistic understanding of fine-grained action patterns and alleviating the aforementioned ambiguities. Specifically, we propose an end-to-end framework that includes a Robust Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge Summarization and Aggregation (GKSA) module. First, the RMGCL module explores the contrast and consistency of cross-video action features, assisting in learning more structured and compact embedding space, thus reducing ambiguity in classification learning. Further, the GKSA module is used to efficiently summarize and propagate the cross-video representative action knowledge in a learnable manner to promote holistic action patterns understanding, which in turn allows the generation of high-confidence pseudo-labels for self-learning, thus alleviating ambiguity in temporal localization. Extensive experiments on THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method outperforms the state-of-the-art methods and can be easily plugged into other WSTAL methods.
</details></li>
</ul>
<hr>
<h2 id="HR-Pro-Point-supervised-Temporal-Action-Localization-via-Hierarchical-Reliability-Propagation"><a href="#HR-Pro-Point-supervised-Temporal-Action-Localization-via-Hierarchical-Reliability-Propagation" class="headerlink" title="HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation"></a>HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12608">http://arxiv.org/abs/2308.12608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pipixin321/hr-pro">https://github.com/pipixin321/hr-pro</a></li>
<li>paper_authors: Huaxin Zhang, Xiang Wang, Xiaohao Xu, Zhiwu Qing, Changxin Gao, Nong Sang</li>
<li>for: 本研究目的是提出一种基于可靠性协议的 temporal action localization 方法，以提高 label-efficient learning 的性能。</li>
<li>methods: 本方法包括两个可靠性感知阶段：短 clip 级可靠性学习和实例级可靠性学习，两个阶段都会利用高 confidence 的点签注入进行可靠性传播。</li>
<li>results: 通过多级可靠性感知学习，我们得到了更可靠的 confidence 分布和更准确的 temporal 边界。我们的 HR-Pro 在多个挑战性 benchmark 上达到了状态的最佳性能，包括 THUMOS14 的平均 mAP 60.3%。<details>
<summary>Abstract</summary>
Point-supervised Temporal Action Localization (PSTAL) is an emerging research direction for label-efficient learning. However, current methods mainly focus on optimizing the network either at the snippet-level or the instance-level, neglecting the inherent reliability of point annotations at both levels. In this paper, we propose a Hierarchical Reliability Propagation (HR-Pro) framework, which consists of two reliability-aware stages: Snippet-level Discrimination Learning and Instance-level Completeness Learning, both stages explore the efficient propagation of high-confidence cues in point annotations. For snippet-level learning, we introduce an online-updated memory to store reliable snippet prototypes for each class. We then employ a Reliability-aware Attention Block to capture both intra-video and inter-video dependencies of snippets, resulting in more discriminative and robust snippet representation. For instance-level learning, we propose a point-based proposal generation approach as a means of connecting snippets and instances, which produces high-confidence proposals for further optimization at the instance level. Through multi-level reliability-aware learning, we obtain more reliable confidence scores and more accurate temporal boundaries of predicted proposals. Our HR-Pro achieves state-of-the-art performance on multiple challenging benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably, our HR-Pro largely surpasses all previous point-supervised methods, and even outperforms several competitive fully supervised methods. Code will be available at https://github.com/pipixin321/HR-Pro.
</details>
<details>
<summary>摘要</summary>
《点指导时间动作Localization（PSTAL）是一个emerging研究方向，它的目标是实现标签效率学习。然而，当前方法主要集中于网络优化，忽略了点级和实例级的可靠性。在这篇论文中，我们提出了一个层次可靠性传播（HR-Pro）框架，它包括两个可靠性感知阶段：幂级可靠性学习和实例级可靠性学习。两个阶段都是利用高信任点级别的缓存来进行可靠性传播。为幂级学习，我们引入了一个在线更新的内存，用于存储每个类型的可靠性缓存。然后，我们使用一个可靠性感知块来捕捉内视频和 между视频依赖关系，从而生成更加准确和稳定的幂级表示。为实例级学习，我们提出了一种基于点的提议生成方法，用于将幂级与实例相连接，从而生成高信任度的提议。通过多级可靠性感知学习，我们得到了更加可靠的信任分数和更加准确的时间边界。我们的HR-Pro在多个挑战性的benchmark上实现了state-of-the-art性能，其中包括THUMOS14的很出色的平均精度（mAP）60.3%。值得注意的是，我们的HR-Pro大大超过了所有前期点指导方法，并且甚至超过了一些高效的完全监督方法。代码将在https://github.com/pipixin321/HR-Pro中提供。
</details></li>
</ul>
<hr>
<h2 id="PoseSync-Robust-pose-based-video-synchronization"><a href="#PoseSync-Robust-pose-based-video-synchronization" class="headerlink" title="PoseSync: Robust pose based video synchronization"></a>PoseSync: Robust pose based video synchronization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12600">http://arxiv.org/abs/2308.12600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishit Javia, Falak Shah, Shivam Dave</li>
<li>for: 这篇论文是用于提出一个端到端管道，用于基于姿势进行视频同步。</li>
<li>methods: 该管道包括对图像中人体部分进行剪辑，然后使用姿势检测器对剪辑的图像进行姿势检测，最后使用动态时间扭曲（DTW）算法对姿势关键点之间的角度&#x2F;距离度量进行比较，从而实现一个可比静态图像的姿势匹配管道。</li>
<li>results: 该管道可以帮助在多个领域，如游戏表现评估、编舞或导引运动员等，进行比较和评估人体动作。<details>
<summary>Abstract</summary>
Pose based video sychronization can have applications in multiple domains such as gameplay performance evaluation, choreography or guiding athletes. The subject's actions could be compared and evaluated against those performed by professionals side by side. In this paper, we propose an end to end pipeline for synchronizing videos based on pose. The first step crops the region where the person present in the image followed by pose detection on the cropped image. This is followed by application of Dynamic Time Warping(DTW) on angle/ distance measures between the pose keypoints leading to a scale and shift invariant pose matching pipeline.
</details>
<details>
<summary>摘要</summary>
pose基于视频同步可以在多个领域有应用，如游戏性能评估、编舞或引导运动员。将主体的动作与专业人员的动作进行比较和评估。在这篇论文中，我们提出了基于pose的视频同步管道的终端到终点解决方案。首先，将图像中人物的区域裁剪，然后进行pose检测。接着，对于裁剪后的图像，应用动态时间扩展(DTW)来计算pose关键点之间的角度/距离度量，从而实现一个可以快速匹配pose的缩放和平移不敏感管道。
</details></li>
</ul>
<hr>
<h2 id="Logic-induced-Diagnostic-Reasoning-for-Semi-supervised-Semantic-Segmentation"><a href="#Logic-induced-Diagnostic-Reasoning-for-Semi-supervised-Semantic-Segmentation" class="headerlink" title="Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation"></a>Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12595">http://arxiv.org/abs/2308.12595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Liang, Wenguan Wang, Jiaxu Miao, Yi Yang</li>
<li>for: 提高 semi-supervised semantic segmentation 的精度，使用 pseudo labeling 补做了有限的标注数据，忽略了 semantic concept 之间的关系知识。</li>
<li>methods: 提出了 LogicDiag，一种基于神经逻辑学习框架的新方法，利用 pseudo label 中的冲突，通过逻辑检查和诊断，纠正 pseudo label，从而缓解 error accumulation 问题。</li>
<li>results: 在三个标准 semi-supervised semantic segmentation 测试集上进行了广泛的实验，证明了 LogicDiag 的有效性和通用性。此外，LogicDiag 还探讨了将符号逻辑reasoning integrate 到 prevailing 的统计学、神经网络学习方法中的可能性。<details>
<summary>Abstract</summary>
Recent advances in semi-supervised semantic segmentation have been heavily reliant on pseudo labeling to compensate for limited labeled data, disregarding the valuable relational knowledge among semantic concepts. To bridge this gap, we devise LogicDiag, a brand new neural-logic semi-supervised learning framework. Our key insight is that conflicts within pseudo labels, identified through symbolic knowledge, can serve as strong yet commonly ignored learning signals. LogicDiag resolves such conflicts via reasoning with logic-induced diagnoses, enabling the recovery of (potentially) erroneous pseudo labels, ultimately alleviating the notorious error accumulation problem. We showcase the practical application of LogicDiag in the data-hungry segmentation scenario, where we formalize the structured abstraction of semantic concepts as a set of logic rules. Extensive experiments on three standard semi-supervised semantic segmentation benchmarks demonstrate the effectiveness and generality of LogicDiag. Moreover, LogicDiag highlights the promising opportunities arising from the systematic integration of symbolic reasoning into the prevalent statistical, neural learning approaches.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近期 semi-supervised semantic segmentation 领域的进步都受到了 pseudo labeling 的限制，忽视了 semantic concepts 之间的 valuabe relational knowledge。为了bridging这个 gap，我们提出 LogicDiag，一种全新的 neural-logic  semi-supervised learning 框架。我们的关键发现是，在 pseudo labels 中的 conflicts，可以作为强大 yet 常被忽略的学习信号。LogicDiag 通过逻辑检查，解决这些 conflicts，使得可以恢复 (可能) 错误的 pseudo labels，从而缓解 error accumulation 问题。我们在数据充沛的 segmentation enario 中实现了 LogicDiag，并形式化 semantic concepts 的抽象结构为逻辑规则。我们在三个标准 semi-supervised semantic segmentation benchmark 上进行了广泛的实验，证明 LogicDiag 的效果和通用性。此外，LogicDiag 还展示了将逻辑推理系统化到 prevailing statistical, neural learning approaches 中的推动力。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Learning-of-Implicit-Shape-Representation-with-Dense-Correspondence-for-Deformable-Objects"><a href="#Self-supervised-Learning-of-Implicit-Shape-Representation-with-Dense-Correspondence-for-Deformable-Objects" class="headerlink" title="Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects"></a>Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12590">http://arxiv.org/abs/2308.12590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baowen Zhang, Jiahe Li, Xiaoming Deng, Yinda Zhang, Cuixia Ma, Hongan Wang</li>
<li>for: 学习3D形状表示的精细对应方法，用于扭形物体。</li>
<li>methods: 提出了一种新的自动标注方法，使用签名距离场来学习神经隐式形状表示，不需要骨架和皮肤纹理的假设。</li>
<li>results: 实验表明，该方法可以表示大幅扭形的形状，并且可以支持文本传输和形状编辑等应用，性能竞争力强。Here is the summary in English for reference:</li>
<li>for: Learning 3D shape representation with dense correspondence for deformable objects.</li>
<li>methods: Propose a novel self-supervised approach to learn neural implicit shape representation, which does not require prior knowledge of skeleton and skinning weight.</li>
<li>results: Experimental results show that the method can represent shapes with large deformations and support applications such as texture transfer and shape editing with competitive performance.<details>
<summary>Abstract</summary>
Learning 3D shape representation with dense correspondence for deformable objects is a fundamental problem in computer vision. Existing approaches often need additional annotations of specific semantic domain, e.g., skeleton poses for human bodies or animals, which require extra annotation effort and suffer from error accumulation, and they are limited to specific domain. In this paper, we propose a novel self-supervised approach to learn neural implicit shape representation for deformable objects, which can represent shapes with a template shape and dense correspondence in 3D. Our method does not require the priors of skeleton and skinning weight, and only requires a collection of shapes represented in signed distance fields. To handle the large deformation, we constrain the learned template shape in the same latent space with the training shapes, design a new formulation of local rigid constraint that enforces rigid transformation in local region and addresses local reflection issue, and present a new hierarchical rigid constraint to reduce the ambiguity due to the joint learning of template shape and correspondences. Extensive experiments show that our model can represent shapes with large deformations. We also show that our shape representation can support two typical applications, such as texture transfer and shape editing, with competitive performance. The code and models are available at https://iscas3dv.github.io/deformshape
</details>
<details>
<summary>摘要</summary>
学习3D形状表示方法中的密集匹配问题是计算机视觉的基本问题。现有的方法经常需要特定的 semantic 领域的更多注释，例如人体或动物的skeleton 姿势，这会增加注释努力并受到错误堆积的限制，同时它们只适用于特定的领域。在这篇论文中，我们提出了一种新的自助学习方法，用于学习神经凝聚形状表示方法，可以在3D中表示形状。我们的方法不需要预先知道skeleton和皮肤粘性的积分，只需要一个包含形状的signed distance fields。为了处理大幅度的变形，我们将学习的模板形状固定在同一个隐藏空间中，并设计了一种新的本地刚性约束，以便在本地区域中强制刚性变换，解决本地反射问题。此外，我们还提出了一种新的层次刚性约束，以减少由模板形状和匹配的共同学习所导致的模糊性。广泛的实验表明我们的模型可以表示大幅度的变形。此外，我们还证明了我们的形状表示可以支持两种典型的应用，例如纹理传输和形状编辑，并且与竞争性表现。代码和模型可以在https://iscas3dv.github.io/deformshape 上获取。
</details></li>
</ul>
<hr>
<h2 id="Grounded-Entity-Landmark-Adaptive-Pre-training-for-Vision-and-Language-Navigation"><a href="#Grounded-Entity-Landmark-Adaptive-Pre-training-for-Vision-and-Language-Navigation" class="headerlink" title="Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation"></a>Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12587">http://arxiv.org/abs/2308.12587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csir1996/vln-gela">https://github.com/csir1996/vln-gela</a></li>
<li>paper_authors: Yibo Cui, Liang Xie, Yakun Zhang, Meishan Zhang, Ye Yan, Erwei Yin</li>
<li>for: 本研究的目的是解决视觉语言导航（VLN）中的跨模态Alignment问题。</li>
<li>methods: 我们提出了一种新的Grounded Entity-Landmark Adaptive（GELA）预训练方法，通过引入基于实体和Landmark的 annotated数据（GEL-R2R），并采用三种基于实体和Landmark的适应预训练目标来强制学习细致的跨模态Alignment。</li>
<li>results: 我们的GELA模型在两个下游任务上（R2R和CVDN）得到了状态级 результа们，证明了其效果和普适性。<details>
<summary>Abstract</summary>
Cross-modal alignment is one key challenge for Vision-and-Language Navigation (VLN). Most existing studies concentrate on mapping the global instruction or single sub-instruction to the corresponding trajectory. However, another critical problem of achieving fine-grained alignment at the entity level is seldom considered. To address this problem, we propose a novel Grounded Entity-Landmark Adaptive (GELA) pre-training paradigm for VLN tasks. To achieve the adaptive pre-training paradigm, we first introduce grounded entity-landmark human annotations into the Room-to-Room (R2R) dataset, named GEL-R2R. Additionally, we adopt three grounded entity-landmark adaptive pre-training objectives: 1) entity phrase prediction, 2) landmark bounding box prediction, and 3) entity-landmark semantic alignment, which explicitly supervise the learning of fine-grained cross-modal alignment between entity phrases and environment landmarks. Finally, we validate our model on two downstream benchmarks: VLN with descriptive instructions (R2R) and dialogue instructions (CVDN). The comprehensive experiments show that our GELA model achieves state-of-the-art results on both tasks, demonstrating its effectiveness and generalizability.
</details>
<details>
<summary>摘要</summary>
cross-modalAlignment是Vision-and-Language Navigation（VLN）中一个关键挑战。大多数现有研究专注于将全球指令或单一子指令映射到相应的路径上。然而，另一个重要的问题是实现细部对齐，即在实体水平上进行精确的对齐。为了解决这个问题，我们提出了一个新的Grounded Entity-Landmark Adaptive（GELA）预训方法 дляVLN任务。为了实现这个预训方法，我们首先将固有的实体-Landmark人工注释添加到Room-to-Room（R2R） dataset中，名为GEL-R2R。其次，我们采用三种固有的实体-Landmark适应预训练目标：1）实体短语预测，2）Landmark bounding box预测，和3）实体-Landmark语义对齐，这些目标直接监督学习跨模态的精确对齐。最后，我们 validate our model on two downstream benchmarks：VLN with descriptive instructions (R2R)和 dialogue instructions (CVDN)。弹性实验结果显示，我们的GELA模型在两个任务上均 achieve state-of-the-art results，证明其有效性和普遍性。
</details></li>
</ul>
<hr>
<h2 id="LORD-Leveraging-Open-Set-Recognition-with-Unknown-Data"><a href="#LORD-Leveraging-Open-Set-Recognition-with-Unknown-Data" class="headerlink" title="LORD: Leveraging Open-Set Recognition with Unknown Data"></a>LORD: Leveraging Open-Set Recognition with Unknown Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12584">http://arxiv.org/abs/2308.12584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Koch, Christian Riess, Thomas Köhler</li>
<li>for: 这个论文的目的是如何处理未知数据，以便在部署过程中更好地进行分类。</li>
<li>methods: 这篇论文使用了一种名为LORD的框架，该框架在分类器训练过程中直接模型了开放空间，并提供了一系列可靠的评估方法。</li>
<li>results: 经过对多种评估协议的测试，这篇论文表明了在未知数据中进行分类时的改进表现，并且通过使用mixup作为数据生成技术，减轻了依赖于大量和昂贵的背景数据的问题。<details>
<summary>Abstract</summary>
Handling entirely unknown data is a challenge for any deployed classifier. Classification models are typically trained on a static pre-defined dataset and are kept in the dark for the open unassigned feature space. As a result, they struggle to deal with out-of-distribution data during inference. Addressing this task on the class-level is termed open-set recognition (OSR). However, most OSR methods are inherently limited, as they train closed-set classifiers and only adapt the downstream predictions to OSR. This work presents LORD, a framework to Leverage Open-set Recognition by exploiting unknown Data. LORD explicitly models open space during classifier training and provides a systematic evaluation for such approaches. We identify three model-agnostic training strategies that exploit background data and applied them to well-established classifiers. Due to LORD's extensive evaluation protocol, we consistently demonstrate improved recognition of unknown data. The benchmarks facilitate in-depth analysis across various requirement levels. To mitigate dependency on extensive and costly background datasets, we explore mixup as an off-the-shelf data generation technique. Our experiments highlight mixup's effectiveness as a substitute for background datasets. Lightweight constraints on mixup synthesis further improve OSR performance.
</details>
<details>
<summary>摘要</summary>
处理完全未知数据是任何部署类фика器的挑战。类фика器通常是在静态预先定义的数据集上训练，因此在推理过程中难以处理外部不确定数据。为解决这个问题，我们提出了开放集 recognition（OSR）技术。然而，大多数OSR方法都受限于它们只是将关闭集类фика器 retrained，并且只是在推理过程中适应OSR。本文介绍了LORD框架，它可以利用未知数据进行开放集 recognition。LORD在类ifica器训练过程中直接模型开放空间，并提供了系统的评估方法。我们identified三种模型无关的训练策略，并应用这些策略到了已知的类ifica器上。由于LORD的广泛的评估协议，我们在不同的需求水平上 consistently 示出了未知数据的更好的识别。这些标准化的协议为我们进行了深入的分析。为了减少依赖于广泛和昂贵的背景数据集，我们探索了mixup作为一种可用的数据生成技术。我们的实验表明，mixup是一种有效的替代方案。在进一步提高OSR性能的同时，我们还提出了一些轻量级的约束来限制mixup的生成。
</details></li>
</ul>
<hr>
<h2 id="StreamMapNet-Streaming-Mapping-Network-for-Vectorized-Online-HD-Map-Construction"><a href="#StreamMapNet-Streaming-Mapping-Network-for-Vectorized-Online-HD-Map-Construction" class="headerlink" title="StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction"></a>StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12570">http://arxiv.org/abs/2308.12570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyuan Yuan, Yicheng Liu, Yue Wang, Yilun Wang, Hang Zhao</li>
<li>for: 高清晰地图是自动驾驶系统的关键 Component， StreamMapNet 提供了一种新的在线地图生成管线，可以处理长串 temporal 信息，提高了稳定性和性能。</li>
<li>methods: StreamMapNet 使用多点注意力和时间信息来建立大范围的本地高清晰地图，并且可以处理复杂的场景，如 occlusion。</li>
<li>results: StreamMapNet 在所有设置下都与现有方法进行比较，表现出色，并且可以在 $14.2$ FPS 的在线推理速度下保持稳定性和高性能。<details>
<summary>Abstract</summary>
High-Definition (HD) maps are essential for the safety of autonomous driving systems. While existing techniques employ camera images and onboard sensors to generate vectorized high-precision maps, they are constrained by their reliance on single-frame input. This approach limits their stability and performance in complex scenarios such as occlusions, largely due to the absence of temporal information. Moreover, their performance diminishes when applied to broader perception ranges. In this paper, we present StreamMapNet, a novel online mapping pipeline adept at long-sequence temporal modeling of videos. StreamMapNet employs multi-point attention and temporal information which empowers the construction of large-range local HD maps with high stability and further addresses the limitations of existing methods. Furthermore, we critically examine widely used online HD Map construction benchmark and datasets, Argoverse2 and nuScenes, revealing significant bias in the existing evaluation protocols. We propose to resplit the benchmarks according to geographical spans, promoting fair and precise evaluations. Experimental results validate that StreamMapNet significantly outperforms existing methods across all settings while maintaining an online inference speed of $14.2$ FPS.
</details>
<details>
<summary>摘要</summary>
高清定义（HD）地图是自动驾驶系统的关键。现有技术使用摄像头图像和车辆上的感知器来生成 вектор化高精度地图，但这些技术受到单帧输入的限制，导致它们在复杂的情况下表现不稳定，主要是因为缺乏时间信息。此外，它们在扩大观察范围时表现下降。在这篇论文中，我们提出了StreamMapNet，一种新的在线地图生成管道，可以长时间序列模型视频。StreamMapNet使用多点注意力和时间信息，使得在大范围本地高清定义地图的建构中具有高稳定性，并解决了现有方法的局限性。此外，我们严格检查了 Argoverse2 和 nuScenes 等在线 HD 地图建构标准和数据集，发现这些标准存在偏见。我们提议将标准按地理范围重新分割，以便更公正和精确的评估。实验结果表明，StreamMapNet 在所有设置下与现有方法进行比较，并且保持在线推理速度为14.2帧/秒。
</details></li>
</ul>
<hr>
<h2 id="NOVA-NOvel-View-Augmentation-for-Neural-Composition-of-Dynamic-Objects"><a href="#NOVA-NOvel-View-Augmentation-for-Neural-Composition-of-Dynamic-Objects" class="headerlink" title="NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects"></a>NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12560">http://arxiv.org/abs/2308.12560</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dakshitagrawal/nova">https://github.com/dakshitagrawal/nova</a></li>
<li>paper_authors: Dakshit Agrawal, Jiajie Xu, Siva Karthik Mustikovela, Ioannis Gkioulekas, Ashish Shrivastava, Yuning Chai</li>
<li>for:  trains NeRFs for photo-realistic 3D composition of dynamic objects in a static scene</li>
<li>methods:  uses a novel-view augmentation (NOVA) strategy</li>
<li>results:  reduces blending artifacts, achieves comparable PSNR without additional ground truth modalities, and provides ease, flexibility, and scalability in neural composition.<details>
<summary>Abstract</summary>
We propose a novel-view augmentation (NOVA) strategy to train NeRFs for photo-realistic 3D composition of dynamic objects in a static scene. Compared to prior work, our framework significantly reduces blending artifacts when inserting multiple dynamic objects into a 3D scene at novel views and times; achieves comparable PSNR without the need for additional ground truth modalities like optical flow; and overall provides ease, flexibility, and scalability in neural composition. Our codebase is on GitHub.
</details>
<details>
<summary>摘要</summary>
我们提出一种新视图增强策略（NOVA），用于在静止场景中使用神经网络组合动态对象的3D组合。相比之前的工作，我们的框架可以在新视图和时间插入多个动态对象时减少融合 artifacts，达到相同的PSNR，而不需要额外的真实流动模式 like 光学流体；同时提供了更容易、灵活和可扩展的神经组合。我们的代码库在 GitHub 上。
</details></li>
</ul>
<hr>
<h2 id="Hyperbolic-Audio-visual-Zero-shot-Learning"><a href="#Hyperbolic-Audio-visual-Zero-shot-Learning" class="headerlink" title="Hyperbolic Audio-visual Zero-shot Learning"></a>Hyperbolic Audio-visual Zero-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12558">http://arxiv.org/abs/2308.12558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Hong, Zeeshan Hayder, Junlin Han, Pengfei Fang, Mehrtash Harandi, Lars Petersson<br>for: 这个论文的目的是探讨采用几何学变换来实现零shot学习，以便更好地处理具有复杂层次结构的数据。methods: 该方法使用了一种新的损失函数，该损失函数将视频和音频特征在几何空间进行交叉模块对齐。此外，该方法还 explore了使用多个自适应几何 curvature来进行几何投影。results: 实验结果表明，我们的提议的几何方法在三个数据集上（VGGSound-GZSL、UCF-GZSL和ActivityNet-GZSL）实现了预测值的大约3.0%、7.0%和5.3%的提升，相对于现有的最佳方法。<details>
<summary>Abstract</summary>
Audio-visual zero-shot learning aims to classify samples consisting of a pair of corresponding audio and video sequences from classes that are not present during training. An analysis of the audio-visual data reveals a large degree of hyperbolicity, indicating the potential benefit of using a hyperbolic transformation to achieve curvature-aware geometric learning, with the aim of exploring more complex hierarchical data structures for this task. The proposed approach employs a novel loss function that incorporates cross-modality alignment between video and audio features in the hyperbolic space. Additionally, we explore the use of multiple adaptive curvatures for hyperbolic projections. The experimental results on this very challenging task demonstrate that our proposed hyperbolic approach for zero-shot learning outperforms the SOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL achieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%, respectively.
</details>
<details>
<summary>摘要</summary>
audio-visual zero-shot learning 目标是将相对应的音频和视频序列分类为在训练中不存在的类。 数据分析显示 audio-visual 数据具有大量的抽象性，表明可能通过使用抽象变换来实现曲线意识的几何学学习，以探索更复杂的层次数据结构。 我们提议的方法使用一种新的损失函数，该函数包含视频和音频特征在抽象空间的交叉模块Alignment。 此外，我们还探讨了多个自适应曲线的投影。 实验结果表明，我们的提议的抽象方法在三个数据集上（VGGSound-GZSL、UCF-GZSL 和 ActivityNet-GZSL）实现了harmonic mean（HM）提高约3.0%, 7.0%, 5.3%，分别。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Models-for-Facial-Emotion-Recognition-in-Children"><a href="#Hybrid-Models-for-Facial-Emotion-Recognition-in-Children" class="headerlink" title="Hybrid Models for Facial Emotion Recognition in Children"></a>Hybrid Models for Facial Emotion Recognition in Children</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12547">http://arxiv.org/abs/2308.12547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Zimmer, Marcos Sobral, Helio Azevedo</li>
<li>for: 这个研究旨在使用情绪识别技术来帮助儿童心理师在远程 робо扮SESSION中进行儿童治疗。</li>
<li>methods: 该研究使用了Embodied Conversational Agents（ECA）作为中间工具，以帮助专业人员与儿童进行交流，特别是对于患有注意力不足过动症（ADHD）、自闭症 спектル（ASD）或者因为战争、自然灾害或其他原因而无法进行面对面会议的儿童。 情绪识别技术作为反馈工具，能够帮助心理师更好地了解儿童的情绪状态。</li>
<li>results: 该研究首先对儿童情绪识别领域进行了文献综述，并对当前社区广泛使用的算法和数据集进行了初步的检视。然后，通过使用 dense optical flow features 技术，提高了儿童情绪识别的精度。 HybridCNNFusion 模型由一个 Convolutional Neural Network 和两个中间特征 fusion 组成，可以更好地识别儿童的情绪。最终，该研究使用了巴西儿童的数据集，并取得了初步的情绪识别结果。<details>
<summary>Abstract</summary>
This paper focuses on the use of emotion recognition techniques to assist psychologists in performing children's therapy through remotely robot operated sessions. In the field of psychology, the use of agent-mediated therapy is growing increasingly given recent advances in robotics and computer science. Specifically, the use of Embodied Conversational Agents (ECA) as an intermediary tool can help professionals connect with children who face social challenges such as Attention Deficit Hyperactivity Disorder (ADHD), Autism Spectrum Disorder (ASD) or even who are physically unavailable due to being in regions of armed conflict, natural disasters, or other circumstances. In this context, emotion recognition represents an important feedback for the psychotherapist. In this article, we initially present the result of a bibliographical research associated with emotion recognition in children. This research revealed an initial overview on algorithms and datasets widely used by the community. Then, based on the analysis carried out on the results of the bibliographical research, we used the technique of dense optical flow features to improve the ability of identifying emotions in children in uncontrolled environments. From the output of a hybrid model of Convolutional Neural Network, two intermediary features are fused before being processed by a final classifier. The proposed architecture was called HybridCNNFusion. Finally, we present the initial results achieved in the recognition of children's emotions using a dataset of Brazilian children.
</details>
<details>
<summary>摘要</summary>
In this article, we first present the results of a bibliographical research on emotion recognition in children. This research provided an initial overview of the algorithms and datasets commonly used by the community. Based on the analysis of the results, we improved the ability to identify emotions in children in uncontrolled environments using the technique of dense optical flow features. A hybrid model of Convolutional Neural Network (CNN) was used, which fused two intermediary features before being processed by a final classifier. The proposed architecture was called HybridCNNFusion.Finally, we present the initial results achieved in recognizing children's emotions using a dataset of Brazilian children.
</details></li>
</ul>
<hr>
<h2 id="Mutual-Guided-Dynamic-Network-for-Image-Fusion"><a href="#Mutual-Guided-Dynamic-Network-for-Image-Fusion" class="headerlink" title="Mutual-Guided Dynamic Network for Image Fusion"></a>Mutual-Guided Dynamic Network for Image Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12538">http://arxiv.org/abs/2308.12538</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guanys-dar/mgdn">https://github.com/guanys-dar/mgdn</a></li>
<li>paper_authors: Yuanshen Guan, Ruikang Xu, Mingde Yao, Lizhi Wang, Zhiwei Xiong<br>for:This paper proposes a novel mutual-guided dynamic network (MGDN) for image fusion, which aims to generate high-quality images from multiple inputs captured under varying conditions.methods:The proposed MGDN method utilizes a mutual-guided dynamic filter (MGDF) for adaptive feature extraction, which incorporates additional guidance from different inputs and generates spatial-variant kernels for different locations. Additionally, a parallel feature fusion (PFF) module is introduced to effectively fuse local and global information of the extracted features.results:Experimental results on five benchmark datasets demonstrate that the proposed MGDN method outperforms existing methods on four image fusion tasks, showcasing its effectiveness in preserving complementary information while filtering out irrelevant information for the fused result.<details>
<summary>Abstract</summary>
Image fusion aims to generate a high-quality image from multiple images captured under varying conditions. The key problem of this task is to preserve complementary information while filtering out irrelevant information for the fused result. However, existing methods address this problem by leveraging static convolutional neural networks (CNNs), suffering two inherent limitations during feature extraction, i.e., being unable to handle spatial-variant contents and lacking guidance from multiple inputs. In this paper, we propose a novel mutual-guided dynamic network (MGDN) for image fusion, which allows for effective information utilization across different locations and inputs. Specifically, we design a mutual-guided dynamic filter (MGDF) for adaptive feature extraction, composed of a mutual-guided cross-attention (MGCA) module and a dynamic filter predictor, where the former incorporates additional guidance from different inputs and the latter generates spatial-variant kernels for different locations. In addition, we introduce a parallel feature fusion (PFF) module to effectively fuse local and global information of the extracted features. To further reduce the redundancy among the extracted features while simultaneously preserving their shared structural information, we devise a novel loss function that combines the minimization of normalized mutual information (NMI) with an estimated gradient mask. Experimental results on five benchmark datasets demonstrate that our proposed method outperforms existing methods on four image fusion tasks. The code and model are publicly available at: https://github.com/Guanys-dar/MGDN.
</details>
<details>
<summary>摘要</summary>
图像融合目标是生成多个图像下 varying 条件下的高质量图像。该任务的关键问题是保留 complementary information 而过滤 irrelevant information 以生成融合结果。然而，现有方法通过利用静态 convolutional neural networks (CNNs) 解决这个问题，具有两个内在的限制：无法处理空间 variant 内容和缺乏多输入的指导。在这篇论文中，我们提出了一种新的 mutual-guided dynamic network (MGDN)  для图像融合，允许有效地利用不同的位置和输入中的信息。具体来说，我们设计了一种 mutual-guided cross-attention (MGCA) 模块和一种动态滤波预测器，其中前者包含不同输入的额外指导，而后者生成不同位置的空间variant 滤波器。此外，我们引入了一种 parallel feature fusion (PFF) 模块，以有效地融合本地和全局的特征信息。为了进一步减少提取的特征信息之间的重复，我们设计了一种新的损失函数，它将 normalized mutual information (NMI) 的最小化与一个估计的梯度掩码相结合。实验结果表明，我们的提出的方法在五个 benchmark 数据集上比现有方法在四个图像融合任务上表现出色。代码和模型可以在：https://github.com/Guanys-dar/MGDN 上获取。
</details></li>
</ul>
<hr>
<h2 id="HuBo-VLM-Unified-Vision-Language-Model-designed-for-HUman-roBOt-interaction-tasks"><a href="#HuBo-VLM-Unified-Vision-Language-Model-designed-for-HUman-roBOt-interaction-tasks" class="headerlink" title="HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks"></a>HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12537">http://arxiv.org/abs/2308.12537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dzcgaara/HuBo-VLM">https://github.com/dzcgaara/HuBo-VLM</a></li>
<li>paper_authors: Zichao Dong, Weikun Zhang, Xufeng Huang, Hang Ji, Xin Zhan, Junbo Chen</li>
<li>for: 这个论文旨在提出一种基于 трансформа器视觉语言模型的人机交互模型，以便帮助机器人理解人类的自然语言指令并完成相关任务。</li>
<li>methods: 该论文提出了一种基于 transformer 视觉语言模型的人机交互模型，包括对象检测和视觉定位。</li>
<li>results: EXTENSIVE EXPERIMENTS ON THE TALK2CAR BENCHMARK DEMONSTRATE THE EFFECTIVENESS OF THE PROPOSED APPROACH。<details>
<summary>Abstract</summary>
Human robot interaction is an exciting task, which aimed to guide robots following instructions from human. Since huge gap lies between human natural language and machine codes, end to end human robot interaction models is fair challenging. Further, visual information receiving from sensors of robot is also a hard language for robot to perceive. In this work, HuBo-VLM is proposed to tackle perception tasks associated with human robot interaction including object detection and visual grounding by a unified transformer based vision language model. Extensive experiments on the Talk2Car benchmark demonstrate the effectiveness of our approach. Code would be publicly available in https://github.com/dzcgaara/HuBo-VLM.
</details>
<details>
<summary>摘要</summary>
人机交互是一项有趣的任务，旨在使 робоッツ按照人类的指令行动。由于人类自然语言与机器代码之间存在巨大的差距，结束到端人机交互模型是非常困难的。此外，机器人的感知器也是一种困难的语言，对机器人来说很难理解。在这项工作中，我们提出了一种解决人机交互相关的感知任务，包括物体检测和视觉定位，的方法。我们使用了一种基于转换器的视Language模型，并进行了广泛的实验，证明了我们的方法的有效性。代码将在https://github.com/dzcgaara/HuBo-VLM上公开。
</details></li>
</ul>
<hr>
<h2 id="SCP-Spherical-Coordinate-based-Learned-Point-Cloud-Compression"><a href="#SCP-Spherical-Coordinate-based-Learned-Point-Cloud-Compression" class="headerlink" title="SCP: Spherical-Coordinate-based Learned Point Cloud Compression"></a>SCP: Spherical-Coordinate-based Learned Point Cloud Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12535">http://arxiv.org/abs/2308.12535</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luoao-kddi/SCP">https://github.com/luoao-kddi/SCP</a></li>
<li>paper_authors: Ao Luo, Linxin Song, Keisuke Nonaka, Kyohei Unno, Heming Sun, Masayuki Goto, Jiro Katto</li>
<li>for: 本研究targets learned point cloud compression, particularly for spinning LiDAR point clouds with circular shapes and azimuthal angle invariance features.</li>
<li>methods: 该方法基于Spherical-Coordinate-based learned Point cloud compression (SCP)，利用了上述特征，并提出了多级Octree来降低远区域重建误差。</li>
<li>results: 实验结果显示，SCP比前一代方法提高了29.14%的点到点PSNR BD-Rate。<details>
<summary>Abstract</summary>
In recent years, the task of learned point cloud compression has gained prominence. An important type of point cloud, the spinning LiDAR point cloud, is generated by spinning LiDAR on vehicles. This process results in numerous circular shapes and azimuthal angle invariance features within the point clouds. However, these two features have been largely overlooked by previous methodologies. In this paper, we introduce a model-agnostic method called Spherical-Coordinate-based learned Point cloud compression (SCP), designed to leverage the aforementioned features fully. Additionally, we propose a multi-level Octree for SCP to mitigate the reconstruction error for distant areas within the Spherical-coordinate-based Octree. SCP exhibits excellent universality, making it applicable to various learned point cloud compression techniques. Experimental results demonstrate that SCP surpasses previous state-of-the-art methods by up to 29.14% in point-to-point PSNR BD-Rate.
</details>
<details>
<summary>摘要</summary>
近年来，学习点云压缩任务得到了更多的关注。一种重要的点云类型是旋转雷达点云，由旋转雷达在车辆上生成。这个过程会生成很多圆形和方位角协variance特征，但这些特征在前一代方法中受到了忽略。在这篇论文中，我们介绍了一种模型无关的方法called Spherical-Coordinate-based learned Point cloud compression (SCP)，旨在利用上述特征。此外，我们提议了一种多级 Octree 来 mitigate SCP 的重建误差。SCP 具有优秀的通用性，可以应用于多种学习点云压缩技术。实验结果表明，SCP 可以比前一代方法提高点-到-点 PSNR BD-Rate 的最高提升率达29.14%。
</details></li>
</ul>
<hr>
<h2 id="Channel-and-Spatial-Relation-Propagation-Network-for-RGB-Thermal-Semantic-Segmentation"><a href="#Channel-and-Spatial-Relation-Propagation-Network-for-RGB-Thermal-Semantic-Segmentation" class="headerlink" title="Channel and Spatial Relation-Propagation Network for RGB-Thermal Semantic Segmentation"></a>Channel and Spatial Relation-Propagation Network for RGB-Thermal Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12534">http://arxiv.org/abs/2308.12534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikun Zhou, Shukun Wu, Guoqing Zhu, Hongpeng Wang, Zhenyu He</li>
<li>for: 这个论文的目的是提出一个 Channel and Spatial Relation-Propagation Network (CSRPNet)，用于RGB-T semantic segmentation，以利用两 modalities 之间的共同特征来提高 semantic segmentation 的精度。</li>
<li>methods: 这个论文使用了一个叫做 Channel and Spatial Relation-Propagation Network (CSRPNet) 的网络，它首先在通道和空间 dimension 进行了关系传递，以捕捉两 modalities 之间的共同特征。然后，它将一 modalities 的特征与另一 modalities 的入力特征进行了融合，以提高入力特征不受污染的问题。</li>
<li>results: 实验结果显示，CSRPNet 可以与现有的方法相比，在RGB-T semantic segmentation 中表现出色。<details>
<summary>Abstract</summary>
RGB-Thermal (RGB-T) semantic segmentation has shown great potential in handling low-light conditions where RGB-based segmentation is hindered by poor RGB imaging quality. The key to RGB-T semantic segmentation is to effectively leverage the complementarity nature of RGB and thermal images. Most existing algorithms fuse RGB and thermal information in feature space via concatenation, element-wise summation, or attention operations in either unidirectional enhancement or bidirectional aggregation manners. However, they usually overlook the modality gap between RGB and thermal images during feature fusion, resulting in modality-specific information from one modality contaminating the other. In this paper, we propose a Channel and Spatial Relation-Propagation Network (CSRPNet) for RGB-T semantic segmentation, which propagates only modality-shared information across different modalities and alleviates the modality-specific information contamination issue. Our CSRPNet first performs relation-propagation in channel and spatial dimensions to capture the modality-shared features from the RGB and thermal features. CSRPNet then aggregates the modality-shared features captured from one modality with the input feature from the other modality to enhance the input feature without the contamination issue. While being fused together, the enhanced RGB and thermal features will be also fed into the subsequent RGB or thermal feature extraction layers for interactive feature fusion, respectively. We also introduce a dual-path cascaded feature refinement module that aggregates multi-layer features to produce two refined features for semantic and boundary prediction. Extensive experimental results demonstrate that CSRPNet performs favorably against state-of-the-art algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SieveNet-Selecting-Point-Based-Features-for-Mesh-Networks"><a href="#SieveNet-Selecting-Point-Based-Features-for-Mesh-Networks" class="headerlink" title="SieveNet: Selecting Point-Based Features for Mesh Networks"></a>SieveNet: Selecting Point-Based Features for Mesh Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12530">http://arxiv.org/abs/2308.12530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sievenet/sievenet.github.io">https://github.com/sievenet/sievenet.github.io</a></li>
<li>paper_authors: Shengchao Yuan, Yishun Dou, Rui Shi, Bingbing Ni, Zhong Zheng</li>
<li>for: 提高3D计算机视觉和图形领域中的网格使用，解决网格的不规则结构限制现有神经网络体系中的应用。</li>
<li>methods: 提出了一种新的思路，即使用结构化网格结构和精度地理信息，从原始网格表面进行误差意识抽取点批量检测，从而兼顾规则结构和准确地理信息。</li>
<li>results: 经过广泛的实验研究，在分类和 segmentation 任务中，提出的 Sievenet 方法能够具有较高的效果和优势，不需要手动设计特征工程。<details>
<summary>Abstract</summary>
Meshes are widely used in 3D computer vision and graphics, but their irregular topology poses challenges in applying them to existing neural network architectures. Recent advances in mesh neural networks turn to remeshing and push the boundary of pioneer methods that solely take the raw meshes as input. Although the remeshing offers a regular topology that significantly facilitates the design of mesh network architectures, features extracted from such remeshed proxies may struggle to retain the underlying geometry faithfully, limiting the subsequent neural network's capacity. To address this issue, we propose SieveNet, a novel paradigm that takes into account both the regular topology and the exact geometry. Specifically, this method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. Furthermore, our method eliminates the need for hand-crafted feature engineering and can leverage off-the-shelf network architectures such as the vision transformer. Comprehensive experimental results on classification and segmentation tasks well demonstrate the effectiveness and superiority of our method.
</details>
<details>
<summary>摘要</summary>
mesh 广泛应用于3D计算机视觉和图形领域，但它们的不规则结构会对现有神经网络架构的应用带来挑战。 recent advances in mesh neural networks have turned to remeshing and pushed the boundaries of pioneering methods that only take raw meshes as input. Although remeshing provides a regular topology that significantly facilitates the design of mesh network architectures, features extracted from such remeshed proxies may struggle to retain the underlying geometry faithfully, limiting the subsequent neural network's capacity. To address this issue, we propose SieveNet, a novel paradigm that takes into account both the regular topology and the exact geometry. Specifically, this method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. Furthermore, our method eliminates the need for hand-crafted feature engineering and can leverage off-the-shelf network architectures such as the vision transformer. Comprehensive experimental results on classification and segmentation tasks well demonstrate the effectiveness and superiority of our method.Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China. The traditional Chinese form of the text would be slightly different.
</details></li>
</ul>
<hr>
<h2 id="Uniformly-Distributed-Category-Prototype-Guided-Vision-Language-Framework-for-Long-Tail-Recognition"><a href="#Uniformly-Distributed-Category-Prototype-Guided-Vision-Language-Framework-for-Long-Tail-Recognition" class="headerlink" title="Uniformly Distributed Category Prototype-Guided Vision-Language Framework for Long-Tail Recognition"></a>Uniformly Distributed Category Prototype-Guided Vision-Language Framework for Long-Tail Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12522">http://arxiv.org/abs/2308.12522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siming Fu, Xiaoxuan He, Xinpeng Ding, Yuchen Cao, Hualiang Wang</li>
<li>for: 这个研究是为了解决长尾认知 task 中的类别偏度问题，特别是当训练数据具有类别差异时，模型会受到特定类别的扭曲。</li>
<li>methods: 我们提出了一个uniformly category prototype-guided vision-language框架，通过生成一些均匀分布在球体上的类别原型，将不同类别的特征扩展到这些原型上，使得特征空间内的分布变得均匀。此外，我们还提出了一个无关文本筛选和特征增强模组，让模型忽略无关的噪音文本，更加重视关键特征信息。</li>
<li>results: 我们的方法比前一代的视觉语言方法更好地适应长尾认知任务，并且实现了类别偏度问题的解决。具体来说，我们的方法在认知精度上比前一代的方法提高了大约20%，并且在长尾类别上保持了高度的稳定性。<details>
<summary>Abstract</summary>
Recently, large-scale pre-trained vision-language models have presented benefits for alleviating class imbalance in long-tailed recognition. However, the long-tailed data distribution can corrupt the representation space, where the distance between head and tail categories is much larger than the distance between two tail categories. This uneven feature space distribution causes the model to exhibit unclear and inseparable decision boundaries on the uniformly distributed test set, which lowers its performance. To address these challenges, we propose the uniformly category prototype-guided vision-language framework to effectively mitigate feature space bias caused by data imbalance. Especially, we generate a set of category prototypes uniformly distributed on a hypersphere. Category prototype-guided mechanism for image-text matching makes the features of different classes converge to these distinct and uniformly distributed category prototypes, which maintain a uniform distribution in the feature space, and improve class boundaries. Additionally, our proposed irrelevant text filtering and attribute enhancement module allows the model to ignore irrelevant noisy text and focus more on key attribute information, thereby enhancing the robustness of our framework. In the image recognition fine-tuning stage, to address the positive bias problem of the learnable classifier, we design the class feature prototype-guided classifier, which compensates for the performance of tail classes while maintaining the performance of head classes. Our method outperforms previous vision-language methods for long-tailed learning work by a large margin and achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
近期，大规模预训练视觉语言模型已经显示出了对长尾识别问题的缓解效果。然而，长尾数据分布可以损害模型的表征空间，导致模型在uniform测试集上展示不明确和不分化的决策边界，从而降低其性能。为解决这些挑战，我们提议使用 uniformly分布的类prototype来引导视觉语言框架，以有效地消除数据不均分带来的表径空间偏见。具体来说，我们生成了一组 uniformly分布在 hypersphere 上的类prototype。这些类prototype在图像文本匹配中 acted as a guide, making the features of different classes converge to these distinct and uniformly distributed category prototypes, thereby maintaining a uniform distribution in the feature space and improving class boundaries.此外，我们还提出了不相关文本过滤和特征增强模块，使模型忽略不相关的噪音文本，更加注重关键特征信息，从而提高了我们的框架的Robustness。在图像识别细化阶段，为了解决learnable classifier的正面偏好问题，我们设计了类feature prototype-guided类ifier，这种方法可以补偿尾类的性能，同时保持头类的性能。根据我们的实验结果，我们的方法在长尾学习任务上比前一代视觉语言方法出performanced by a large margin，达到了状态的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Parameter-Efficient-Transfer-Learning-for-Remote-Sensing-Image-Text-Retrieval"><a href="#Parameter-Efficient-Transfer-Learning-for-Remote-Sensing-Image-Text-Retrieval" class="headerlink" title="Parameter-Efficient Transfer Learning for Remote Sensing Image-Text Retrieval"></a>Parameter-Efficient Transfer Learning for Remote Sensing Image-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12509">http://arxiv.org/abs/2308.12509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Yuan, Yang Zhan, Zhitong Xiong</li>
<li>for: 这研究旨在提出一种高效高可用的视言语传播学习方法，以便在实际应用中处理大量的远程感知数据。</li>
<li>methods: 该研究使用了CLIP模型作为预训练模型，并设计了一种多模态远程感知适配器以及一种混合多模态对比学习目标。此外，我们还提出了一种简单 yet有效的HMMC损失函数来解决高内模态相似性问题。</li>
<li>results: 我们的研究表明，使用PETL方法可以有效地传播视言语知识从自然领域到远程感知领域，并且可以大幅降低训练成本和环境影响。我们的模型只包含0.16M个训练参数，可以实现98.9%的参数减少，并且在 Retrieval 性能方面超过传统方法7-13%，与全 Fine-tuning 的性能相当或更好。<details>
<summary>Abstract</summary>
Vision-and-language pre-training (VLP) models have experienced a surge in popularity recently. By fine-tuning them on specific datasets, significant performance improvements have been observed in various tasks. However, full fine-tuning of VLP models not only consumes a significant amount of computational resources but also has a significant environmental impact. Moreover, as remote sensing (RS) data is constantly being updated, full fine-tuning may not be practical for real-world applications. To address this issue, in this work, we investigate the parameter-efficient transfer learning (PETL) method to effectively and efficiently transfer visual-language knowledge from the natural domain to the RS domain on the image-text retrieval task. To this end, we make the following contributions. 1) We construct a novel and sophisticated PETL framework for the RS image-text retrieval (RSITR) task, which includes the pretrained CLIP model, a multimodal remote sensing adapter, and a hybrid multi-modal contrastive (HMMC) learning objective; 2) To deal with the problem of high intra-modal similarity in RS data, we design a simple yet effective HMMC loss; 3) We provide comprehensive empirical studies for PETL-based RS image-text retrieval. Our results demonstrate that the proposed method is promising and of great potential for practical applications. 4) We benchmark extensive state-of-the-art PETL methods on the RSITR task. Our proposed model only contains 0.16M training parameters, which can achieve a parameter reduction of 98.9% compared to full fine-tuning, resulting in substantial savings in training costs. Our retrieval performance exceeds traditional methods by 7-13% and achieves comparable or better performance than full fine-tuning. This work can provide new ideas and useful insights for RS vision-language tasks.
</details>
<details>
<summary>摘要</summary>
Recently, vision-and-language pre-training (VLP) 模型在不同领域中得到了广泛的应用。通过特定数据集的精细调整，VLP模型在各种任务中表现出了显著的性能提升。然而，全量调整VLP模型不仅需要巨量的计算资源，还会对环境产生巨大的影响。此外，随着Remote Sensing（RS）数据不断更新，全量调整可能无法适应实际应用中的需求。为此，本文提出了参数有效传播学习（PETL）方法，以有效地和高效地将视觉语言知识从自然领域传播到RS领域中的图文检索任务上。为此，我们做了以下贡献：1. 我们建立了一个新的和复杂的PETL框架 дляRS图文检索任务，包括预训练的CLIP模型、多模态RS适配器和混合多模态对比（HMMC）学习目标；2. 为RS数据中高内模态相似性问题而设计了一个简单 yet effective的HMMC损失函数；3. 我们提供了RS图文检索的广泛的实验研究。我们的结果表明，我们提出的方法具有扎实的推荐和实际应用的潜在价值。4. 我们对现有的PETL方法进行了广泛的比较研究，并发现我们的提出的模型只需0.16M参数进行训练，相比涵盖所有模型，可以实现参数减少98.9%，减少训练成本。我们的检索性能高于传统方法7-13%，并且与全量调整的性能相当或更好。本文可以提供新的想法和有用的意见 дляRS视觉语言任务。
</details></li>
</ul>
<hr>
<h2 id="FFEINR-Flow-Feature-Enhanced-Implicit-Neural-Representation-for-Spatio-temporal-Super-Resolution"><a href="#FFEINR-Flow-Feature-Enhanced-Implicit-Neural-Representation-for-Spatio-temporal-Super-Resolution" class="headerlink" title="FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution"></a>FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12508">http://arxiv.org/abs/2308.12508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyue Jiao, Chongke Bi, Lu Yang</li>
<li>for: 提高流体动力学数据的空间和时间分辨率</li>
<li>methods: 基于嵌入型神经网络（FFEINR）和特征增强技术</li>
<li>results: 比较比特里利内插法更好的结果<details>
<summary>Abstract</summary>
Large-scale numerical simulations are capable of generating data up to terabytes or even petabytes. As a promising method of data reduction, super-resolution (SR) has been widely studied in the scientific visualization community. However, most of them are based on deep convolutional neural networks (CNNs) or generative adversarial networks (GANs) and the scale factor needs to be determined before constructing the network. As a result, a single training session only supports a fixed factor and has poor generalization ability. To address these problems, this paper proposes a Feature-Enhanced Implicit Neural Representation (FFEINR) for spatio-temporal super-resolution of flow field data. It can take full advantage of the implicit neural representation in terms of model structure and sampling resolution. The neural representation is based on a fully connected network with periodic activation functions, which enables us to obtain lightweight models. The learned continuous representation can decode the low-resolution flow field input data to arbitrary spatial and temporal resolutions, allowing for flexible upsampling. The training process of FFEINR is facilitated by introducing feature enhancements for the input layer, which complements the contextual information of the flow field.To demonstrate the effectiveness of the proposed method, a series of experiments are conducted on different datasets by setting different hyperparameters. The results show that FFEINR achieves significantly better results than the trilinear interpolation method.
</details>
<details>
<summary>摘要</summary>
大规模数值计算可以生成数据达到tera bytes甚至petabytes级别。作为数据压缩的承诺方法，超分辨率（SR）在科学视觉社区得到了广泛的研究。然而，大多数都基于深度卷积神经网络（CNN）或生成敌对网络（GAN），并且需要确定缩放因子之前建立网络。这意味着单个训练会话只支持固定因子，并且具有较差的泛化能力。为解决这些问题，本文提出了基于几何卷积神经网络的特征增强隐藏表示（FFEINR），用于空间时间超分辨率的流场数据。它可以完全利用隐藏表示的几何结构和采样分辨率来获得轻量级模型。学习的连续表示可以将低分辨率流场输入数据解码到任意空间和时间分辨率，以便灵活增加。为便于FFEINR的训练，我们引入了输入层的特征增强，以增强流场的上下文信息。为证明提案的效iveness，我们在不同的数据集上进行了一系列实验，并通过设置不同的超参数来评估结果。结果显示，FFEINR在比较方法中表现出了显著的优势。
</details></li>
</ul>
<hr>
<h2 id="DD-GCN-Directed-Diffusion-Graph-Convolutional-Network-for-Skeleton-based-Human-Action-Recognition"><a href="#DD-GCN-Directed-Diffusion-Graph-Convolutional-Network-for-Skeleton-based-Human-Action-Recognition" class="headerlink" title="DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition"></a>DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12501">http://arxiv.org/abs/2308.12501</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shiyin-lc/dd-gcn">https://github.com/shiyin-lc/dd-gcn</a></li>
<li>paper_authors: Chang Li, Qian Huang, Yingchi Mao</li>
<li>for: 这篇论文是为了提高skeleton-based human action recognition中的Graph Convolutional Networks（GCNs）性能而写的。</li>
<li>methods: 该论文使用了导向协同分布图（DD-GCN），它利用了建立导向分布图以实现动作模型化，并引入了活动分区策略来优化图 convolution kernels 的加权共享机制。此外，它还提出了空间时间同步编码器来嵌入同步空间时间 semantics。</li>
<li>results: 实验结果表明，该方法在三个公共数据集（NTU-RGB+D、NTU-RGB+D 120、NW-UCLA）上达到了当前最佳性能。<details>
<summary>Abstract</summary>
Graph Convolutional Networks (GCNs) have been widely used in skeleton-based human action recognition. In GCN-based methods, the spatio-temporal graph is fundamental for capturing motion patterns. However, existing approaches ignore the physical dependency and synchronized spatio-temporal correlations between joints, which limits the representation capability of GCNs. To solve these problems, we construct the directed diffusion graph for action modeling and introduce the activity partition strategy to optimize the weight sharing mechanism of graph convolution kernels. In addition, we present the spatio-temporal synchronization encoder to embed synchronized spatio-temporal semantics. Finally, we propose Directed Diffusion Graph Convolutional Network (DD-GCN) for action recognition, and the experiments on three public datasets: NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-art performance of our method.
</details>
<details>
<summary>摘要</summary>
格点图 neural network (GCN) 在人体动作识别中广泛应用。在 GCN 基本方法中，空间时间图是关键 для捕捉运动模式。然而，现有方法忽略了物理依赖性和同步空间时间相关性 между 关节，这限制了 GCN 的表示能力。为解决这些问题，我们构建了导向干扰图 для动作模型化，并引入活动分区策略来优化图像卷积核的加权共享机制。此外，我们提出了空间时间同步编码器，以嵌入同步空间时间 semantics。最后，我们提出了导向干扰图 convolutional neural network (DD-GCN)  для动作识别，并在三个公共数据集（NTU-RGB+D、NTU-RGB+D 120、NW-UCLA）上进行实验，其中表现出了当今最佳性能。
</details></li>
</ul>
<hr>
<h2 id="MOFA-A-Model-Simplification-Roadmap-for-Image-Restoration-on-Mobile-Devices"><a href="#MOFA-A-Model-Simplification-Roadmap-for-Image-Restoration-on-Mobile-Devices" class="headerlink" title="MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices"></a>MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12494">http://arxiv.org/abs/2308.12494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Chen, Ruiwen Zhen, Shuai Li, Xiaotian Li, Guanghui Wang</li>
<li>for:  restore high-quality images from degraded counterparts and improve the efficiency of image restoration models on mobile devices.</li>
<li>methods:  add more parameters to partial convolutions on FLOPs non-sensitive layers, apply partial depthwise convolution coupled with decoupling upsampling&#x2F;downsampling layers.</li>
<li>results:  decrease runtime by up to 13%, reduce the number of parameters by up to 23%, while increasing PSNR and SSIM on several image restoration datasets.Here is the text in Simplified Chinese:</li>
<li>for:  restore高品质的图像从损坏版本中，并提高移动设备上图像恢复模型的效率。</li>
<li>methods: 增加FLOPs非敏感层中的参数，应用部分深度卷积并与解解锁升降样例层。</li>
<li>results: 减少运行时间，减少参数数量，同时提高PSNR和SSIM在多个图像恢复数据集上。<details>
<summary>Abstract</summary>
Image restoration aims to restore high-quality images from degraded counterparts and has seen significant advancements through deep learning techniques. The technique has been widely applied to mobile devices for tasks such as mobile photography. Given the resource limitations on mobile devices, such as memory constraints and runtime requirements, the efficiency of models during deployment becomes paramount. Nevertheless, most previous works have primarily concentrated on analyzing the efficiency of single modules and improving them individually. This paper examines the efficiency across different layers. We propose a roadmap that can be applied to further accelerate image restoration models prior to deployment while simultaneously increasing PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). The roadmap first increases the model capacity by adding more parameters to partial convolutions on FLOPs non-sensitive layers. Then, it applies partial depthwise convolution coupled with decoupling upsampling/downsampling layers to accelerate the model speed. Extensive experiments demonstrate that our approach decreases runtime by up to 13% and reduces the number of parameters by up to 23%, while increasing PSNR and SSIM on several image restoration datasets. Source Code of our method is available at \href{https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}.
</details>
<details>
<summary>摘要</summary>
Image restoration aims to restore high-quality images from degraded counterparts and has seen significant advancements through deep learning techniques. The technique has been widely applied to mobile devices for tasks such as mobile photography. Given the resource limitations on mobile devices, such as memory constraints and runtime requirements, the efficiency of models during deployment becomes paramount. Nevertheless, most previous works have primarily concentrated on analyzing the efficiency of single modules and improving them individually. This paper examines the efficiency across different layers. We propose a roadmap that can be applied to further accelerate image restoration models prior to deployment while simultaneously increasing PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). The roadmap first increases the model capacity by adding more parameters to partial convolutions on FLOPs non-sensitive layers. Then, it applies partial depthwise convolution coupled with decoupling upsampling/downsampling layers to accelerate the model speed. Extensive experiments demonstrate that our approach decreases runtime by up to 13% and reduces the number of parameters by up to 23%, while increasing PSNR and SSIM on several image restoration datasets. 源代码我们的方法可以在 \href{https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA} 上 obtain.
</details></li>
</ul>
<hr>
<h2 id="Diffuse-Attend-and-Segment-Unsupervised-Zero-Shot-Segmentation-using-Stable-Diffusion"><a href="#Diffuse-Attend-and-Segment-Unsupervised-Zero-Shot-Segmentation-using-Stable-Diffusion" class="headerlink" title="Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion"></a>Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12469">http://arxiv.org/abs/2308.12469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, Mar Gonzalez-Franco</li>
<li>for: 实现零shot Segmentation的高质量分割mask，解决了计算机视觉领域中的基本问题。</li>
<li>methods: 利用稳定扩散模型的自我注意层，通过衡量KL差值 среди注意图来 merge them into valid segmentation masks。</li>
<li>results: 在COCO-Stuff-27上，我们的方法超过了先前的无supervised zero-shot SOTA方法，净误率提高26%， Mean IoU提高17%。<details>
<summary>Abstract</summary>
Producing quality segmentation masks for images is a fundamental problem in computer vision. Recent research has explored large-scale supervised training to enable zero-shot segmentation on virtually any image style and unsupervised training to enable segmentation without dense annotations. However, constructing a model capable of segmenting anything in a zero-shot manner without any annotations is still challenging. In this paper, we propose to utilize the self-attention layers in stable diffusion models to achieve this goal because the pre-trained stable diffusion model has learned inherent concepts of objects within its attention layers. Specifically, we introduce a simple yet effective iterative merging process based on measuring KL divergence among attention maps to merge them into valid segmentation masks. The proposed method does not require any training or language dependency to extract quality segmentation for any images. On COCO-Stuff-27, our method surpasses the prior unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17% in mean IoU.
</details>
<details>
<summary>摘要</summary>
生成高质量的图像分割 маSK是计算机视觉的基本问题。 latest research has explored large-scale supervised training to enable zero-shot segmentation on virtually any image style and unsupervised training to enable segmentation without dense annotations. However, constructing a model capable of segmenting anything in a zero-shot manner without any annotations is still challenging. In this paper, we propose to utilize the self-attention layers in stable diffusion models to achieve this goal because the pre-trained stable diffusion model has learned inherent concepts of objects within its attention layers. Specifically, we introduce a simple yet effective iterative merging process based on measuring KL divergence among attention maps to merge them into valid segmentation masks. The proposed method does not require any training or language dependency to extract quality segmentation for any images. On COCO-Stuff-27, our method surpasses the prior unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17% in mean IoU.Here's the text with traditional Chinese characters:生成高质量的图像分割mask是计算机视觉的基本问题。 latest research has explored large-scale supervised training to enable zero-shot segmentation on virtually any image style and unsupervised training to enable segmentation without dense annotations. However, constructing a model capable of segmenting anything in a zero-shot manner without any annotations is still challenging. In this paper, we propose to utilize the self-attention layers in stable diffusion models to achieve this goal because the pre-trained stable diffusion model has learned inherent concepts of objects within its attention layers. Specifically, we introduce a simple yet effective iterative merging process based on measuring KL divergence among attention maps to merge them into valid segmentation masks. The proposed method does not require any training or language dependency to extract quality segmentation for any images. On COCO-Stuff-27, our method surpasses the prior unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17% in mean IoU.
</details></li>
</ul>
<hr>
<h2 id="InverseSR-3D-Brain-MRI-Super-Resolution-Using-a-Latent-Diffusion-Model"><a href="#InverseSR-3D-Brain-MRI-Super-Resolution-Using-a-Latent-Diffusion-Model" class="headerlink" title="InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model"></a>InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12465">http://arxiv.org/abs/2308.12465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biomedai-ucsc/inversesr">https://github.com/biomedai-ucsc/inversesr</a></li>
<li>paper_authors: Jueqi Wang, Jacob Levman, Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, M. Jorge Cardoso, Razvan Marinescu</li>
<li>for: 这个论文的目的是提出一种基于深度学习的MRI超分辨（SR）方法，以提高临床MRI扫描的分辨率。</li>
<li>methods: 该方法利用一个 estado-of-the-art 3D脑生成模型（LDM），通过在 UK BioBank 上训练该模型，来提高临床MRI扫描的分辨率。</li>
<li>results: 该方法可以在多种不同的MRI SR问题中提高分辨率，并且可以在不同的设置下选择合适的方法。Here are the three points in Simplified Chinese text:</li>
<li>for: 这个论文的目的是提出一种基于深度学习的MRI超分辨（SR）方法，以提高临床MRI扫描的分辨率。</li>
<li>methods: 该方法利用一个 estado-of-the-art 3D脑生成模型（LDM），通过在 UK BioBank 上训练该模型，来提高临床MRI扫描的分辨率。</li>
<li>results: 该方法可以在多种不同的MRI SR问题中提高分辨率，并且可以在不同的设置下选择合适的方法。<details>
<summary>Abstract</summary>
High-resolution (HR) MRI scans obtained from research-grade medical centers provide precise information about imaged tissues. However, routine clinical MRI scans are typically in low-resolution (LR) and vary greatly in contrast and spatial resolution due to the adjustments of the scanning parameters to the local needs of the medical center. End-to-end deep learning methods for MRI super-resolution (SR) have been proposed, but they require re-training each time there is a shift in the input distribution. To address this issue, we propose a novel approach that leverages a state-of-the-art 3D brain generative model, the latent diffusion model (LDM) trained on UK BioBank, to increase the resolution of clinical MRI scans. The LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI. Based on the architecture of the brain LDM, we find that different methods are suitable for different settings of MRI SR, and thus propose two novel strategies: 1) for SR with more sparsity, we invert through both the decoder of the LDM and also through a deterministic Denoising Diffusion Implicit Models (DDIM), an approach we will call InverseSR(LDM); 2) for SR with less sparsity, we invert only through the LDM decoder, an approach we will call InverseSR(Decoder). These two approaches search different latent spaces in the LDM model to find the optimal latent code to map the given LR MRI into HR. The training process of the generative model is independent of the MRI under-sampling process, ensuring the generalization of our method to many MRI SR problems with different input measurements. We validate our method on over 100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that powerful priors given by LDM can be used for MRI reconstruction.
</details>
<details>
<summary>摘要</summary>
高解度（HR）MRI扫描从研究级医疗机构获取的信息非常精确。然而，日常临床MRI扫描通常是低解度（LR）的，并且因为扫描参数的调整而具有不同的对比度和空间分辨率。为解决这个问题，我们提出了一种新的方法，利用UK BioBank上训练的状态态流模型（LDM）来提高临床MRI扫描的解度。LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI。基于LDM的架构，我们发现不同的方法适用于不同的MRI SR设置，因此我们提出了两种新的策略：1）为SR with more sparsity，我们通过LDM的解码器和deterministic Denoising Diffusion Implicit Models（DDIM）进行逆变换，一种我们将称为InverseSR(LDM)；2）为SR with less sparsity，我们只通过LDM的解码器进行逆变换，一种我们将称为InverseSR(Decoder)。这两种方法在LDM模型中寻找不同的秘密空间，以找到将LR MRI映射到HR的最佳秘密代码。我们的训练过程不依赖MRI下抽样过程，因此我们的方法可以通用许多MRI SR问题。我们验证了我们的方法在IXI数据集上的100多个脑T1w MRI中。我们的方法可以证明LDM可以提供强大的PRIOR，用于MRI重建。
</details></li>
</ul>
<hr>
<h2 id="Overcoming-General-Knowledge-Loss-with-Selective-Parameter-Finetuning"><a href="#Overcoming-General-Knowledge-Loss-with-Selective-Parameter-Finetuning" class="headerlink" title="Overcoming General Knowledge Loss with Selective Parameter Finetuning"></a>Overcoming General Knowledge Loss with Selective Parameter Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12462">http://arxiv.org/abs/2308.12462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Zhang, Paul Janson, Rahaf Aljundi, Mohamed Elhoseiny</li>
<li>for: 提高基础模型的更新能力，以适应新的信息和维护原有知识。</li>
<li>methods: 本文提出了一种新的方法，通过对小部分参数进行本地修改来实现基础模型的不断更新。这种方法基于先前分析基础模型的经验，首先局部化特定层进行模型精度，然后引入重要性分数机制，以更新关键参数。</li>
<li>results: 对基础视觉语言模型进行了广泛评估，证明该方法可以在不同的持续学习任务上提高现有的持续学习方法，并将预先学习的知识减少到0.97%。<details>
<summary>Abstract</summary>
Foundation models encompass an extensive knowledge base and offer remarkable transferability. However, this knowledge becomes outdated or insufficient over time. The challenge lies in updating foundation models to accommodate novel information while retaining their original ability. In this paper, we present a novel approach to achieving continual model updates by effecting localized modifications to a small subset of parameters. Guided by insights gleaned from prior analyses of foundational models, we first localize a specific layer for model refinement and then introduce an importance scoring mechanism designed to update only the most crucial weights. Our method is exhaustively evaluated on foundational vision-language models, measuring its efficacy in both learning new information and preserving pre-established knowledge across a diverse spectrum of continual learning tasks, including Aircraft, Birdsnap CIFAR-100, CUB, Cars, and GTSRB. The results show that our method improves the existing continual learning methods by 0.5\% - 10\% on average, and reduces the loss of pre-trained knowledge from around 5\% to 0.97\%. Comprehensive ablation studies substantiate our method design, shedding light on the contributions of each component to controllably learning new knowledge and mitigating the forgetting of pre-trained knowledge.
</details>
<details>
<summary>摘要</summary>
基础模型包含广泛的知识库和卓越的跨 Transferability。然而，这些知识随着时间的推移会变得过时或不足。挑战在于更新基础模型以容纳新的信息，而不会失去原有的知识。在这篇论文中，我们提出了一种新的方法来实现不断模型更新，通过对一小部分参数进行本地化修改。以先前分析基础模型所获得的知识为指导，我们首先本地化特定层，然后引入一种重要性分配机制，以更新最重要的权重。我们的方法在基础视觉语言模型上进行了完整的评估，并测试其在多种不断学习任务上的效果，包括飞机、鸟卷CIFAR-100、CUB、汽车和GTSRB。结果表明，我们的方法与现有的不断学习方法相比，平均提高了0.5%-10%，并将先前学习的知识损失从约5%降低至0.97%。我们还进行了广泛的减少分析，以证明我们的方法设计的每一部分对于控制新知识学习和减少先前知识损失做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="ARF-Plus-Controlling-Perceptual-Factors-in-Artistic-Radiance-Fields-for-3D-Scene-Stylization"><a href="#ARF-Plus-Controlling-Perceptual-Factors-in-Artistic-Radiance-Fields-for-3D-Scene-Stylization" class="headerlink" title="ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for 3D Scene Stylization"></a>ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for 3D Scene Stylization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12452">http://arxiv.org/abs/2308.12452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhao Li, Tianhao Wu, Fangcheng Zhong, Cengiz Oztireli</li>
<li>for: 用于三维场景样式传递</li>
<li>methods: 使用3D神经辐射场进行样式传递，并提供四种控制方法：色彩保持控制、纹理尺度控制、空间选择性风格控制和深度增强控制</li>
<li>results: 通过实际数据集的量化和质量评估，表明ARF-Plus框架在三维场景样式传递中提供了有效的控制功能，并且可以同时应用多种样式效果，创造出独特和引人注目的风格效果。<details>
<summary>Abstract</summary>
The radiance fields style transfer is an emerging field that has recently gained popularity as a means of 3D scene stylization, thanks to the outstanding performance of neural radiance fields in 3D reconstruction and view synthesis. We highlight a research gap in radiance fields style transfer, the lack of sufficient perceptual controllability, motivated by the existing concept in the 2D image style transfer. In this paper, we present ARF-Plus, a 3D neural style transfer framework offering manageable control over perceptual factors, to systematically explore the perceptual controllability in 3D scene stylization. Four distinct types of controls - color preservation control, (style pattern) scale control, spatial (selective stylization area) control, and depth enhancement control - are proposed and integrated into this framework. Results from real-world datasets, both quantitative and qualitative, show that the four types of controls in our ARF-Plus framework successfully accomplish their corresponding perceptual controls when stylizing 3D scenes. These techniques work well for individual style inputs as well as for the simultaneous application of multiple styles within a scene. This unlocks a realm of limitless possibilities, allowing customized modifications of stylization effects and flexible merging of the strengths of different styles, ultimately enabling the creation of novel and eye-catching stylistic effects on 3D scenes.
</details>
<details>
<summary>摘要</summary>
《几何场景风格传输》是一个刚刚崛起的领域，感谢神经透辉场景的出色表现在3D重建和视觉合成中。我们指出了几何场景风格传输的研究漏洞，即无 suficient perceptual控制，这是基于2D图像风格传输的现有概念。在这篇论文中，我们提出了ARF-Plus，一个3D神经风格传输框架，可以系统地探索3D场景风格传输中的perceptual控制。我们提出了四种控制类型：颜色保持控制、样式模式比例控制、空间（选择性风格着色）控制和深度强化控制。这些控制被纳入到这个框架中，并在实际世界数据集上进行了评估。结果表明，ARF-Plus框架中的四种控制类型能够成功地实现对perceptual控制的管理，并且这些控制可以单独应用于个体风格输入或同时应用于场景中的多种风格。这些技术在创建个性化 modify 3D场景风格效果和自由混合不同风格的优点时，都工作良好。
</details></li>
</ul>
<hr>
<h2 id="MOFO-MOtion-FOcused-Self-Supervision-for-Video-Understanding"><a href="#MOFO-MOtion-FOcused-Self-Supervision-for-Video-Understanding" class="headerlink" title="MOFO: MOtion FOcused Self-Supervision for Video Understanding"></a>MOFO: MOtion FOcused Self-Supervision for Video Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12447">http://arxiv.org/abs/2308.12447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mona Ahmadian, Frank Guerin, Andrew Gilbert<br>for: 本研究的目的是提高视频中动作识别的性能，通过对视频中动作区域进行自我监督学习，以提高视频中动作的表征学习。methods: 我们提出了一种新的自我监督学习方法，称为MOFO（动作区域关注），它可以自动检测视频中动作区域，并使用这些区域来引导自我监督学习任务。我们使用了一种帮助器隐藏一定比例的输入序列中的掩码，并强制掩码在动作区域内部的一定比例被隐藏，而其余部分来自外部。此外，我们还在下游任务中进行了动作信息的加入，以强调动作的表征。results: 我们的研究表明，我们的动作区域关注技术可以明显提高当前最佳的自我监督学习方法（VideoMAE）的动作识别性能。我们在Epic-Kitchens verb、noun和动作分类任务上提高了2.6%、2.1%和1.3%的精度，并在Something-Something V2动作分类任务上提高了4.7%的精度。这表明，在自我监督学习中显式地编码动作是非常重要的。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) techniques have recently produced outstanding results in learning visual representations from unlabeled videos. Despite the importance of motion in supervised learning techniques for action recognition, SSL methods often do not explicitly consider motion information in videos. To address this issue, we propose MOFO (MOtion FOcused), a novel SSL method for focusing representation learning on the motion area of a video, for action recognition. MOFO automatically detects motion areas in videos and uses these to guide the self-supervision task. We use a masked autoencoder which randomly masks out a high proportion of the input sequence; we force a specified percentage of the inside of the motion area to be masked and the remainder from outside. We further incorporate motion information into the finetuning step to emphasise motion in the downstream task. We demonstrate that our motion-focused innovations can significantly boost the performance of the currently leading SSL method (VideoMAE) for action recognition. Our method improves the recent self-supervised Vision Transformer (ViT), VideoMAE, by achieving +2.6%, +2.1%, +1.3% accuracy on Epic-Kitchens verb, noun and action classification, respectively, and +4.7% accuracy on Something-Something V2 action classification. Our proposed approach significantly improves the performance of the current SSL method for action recognition, indicating the importance of explicitly encoding motion in SSL.
</details>
<details>
<summary>摘要</summary>
自顾学（SSL）技术在无标注视频中学习视觉表示方面最近取得了出色的结果。尽管动作认知中的运动信息在指导学习过程中非常重要，但SSL方法通常不直接考虑视频中的运动信息。为解决这个问题，我们提议MOFO（运动区域关注）方法，它是一种新的SSL方法，用于在视频中注意力集中在运动区域上，以提高动作认知。MOFO方法自动检测视频中的运动区域，并使用这些区域来引导自我超vision任务。我们使用一个随机屏蔽输入序列的masked autoencoder，其中高比例的输入序列会被随机屏蔽，而在运动区域内部则强制屏蔽一定比例。此外，我们还在下游任务中注入运动信息，以强调运动在下游任务中的作用。我们示出，我们的运动关注创新可以显著提高现有的SSL方法（VideoMAE）对动作认知的性能。我们的方法可以在Epic-Kitchens动词、名词和动作分类中提高VideoMAE的性能，分别提高+2.6%、+2.1%和+1.3%的精度。此外，我们还在Something-Something V2动作分类中提高了+4.7%的精度。这表明，在SSL中显式编码运动的重要性。
</details></li>
</ul>
<hr>
<h2 id="TAI-GAN-Temporally-and-Anatomically-Informed-GAN-for-early-to-late-frame-conversion-in-dynamic-cardiac-PET-motion-correction"><a href="#TAI-GAN-Temporally-and-Anatomically-Informed-GAN-for-early-to-late-frame-conversion-in-dynamic-cardiac-PET-motion-correction" class="headerlink" title="TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction"></a>TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12443">http://arxiv.org/abs/2308.12443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gxq1998/tai-gan">https://github.com/gxq1998/tai-gan</a></li>
<li>paper_authors: Xueqi Guo, Luyao Shi, Xiongchao Chen, Bo Zhou, Qiong Liu, Huidong Xie, Yi-Hwa Liu, Richard Palyo, Edward J. Miller, Albert J. Sinusas, Bruce Spottiswoode, Chi Liu, Nicha C. Dvornek</li>
<li>for: 这篇论文主要关注的是动脉心PET图像中的快速追踪器动力学和各帧分布的高变化，以及这些变化对插入动作 corrections 的影响。</li>
<li>methods: 该论文提出了一种使用生成方法处理 tracer 分布变化以帮助现有的注册方法。具体来说，我们提出了一种 Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN)，用于在早期帧中将 tracer 分布变化转换为late reference frame中的图像。</li>
<li>results: 我们在临床 $^{82}$Rb PET数据集上验证了我们的提议方法，并发现我们的 TAI-GAN 可以生成高质量的转换图像，与参照帧图像相似。 после TAI-GAN 转换，运动估计精度和临床血液流量（MBF）的量化得到了改善。<details>
<summary>Abstract</summary>
The rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of cross-frame distribution in dynamic cardiac positron emission tomography (PET) raise significant challenges for inter-frame motion correction, particularly for the early frames where conventional intensity-based image registration techniques are not applicable. Alternatively, a promising approach utilizes generative methods to handle the tracer distribution changes to assist existing registration methods. To improve frame-wise registration and parametric quantification, we propose a Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) to transform the early frames into the late reference frame using an all-to-one mapping. Specifically, a feature-wise linear modulation layer encodes channel-wise parameters generated from temporal tracer kinetics information, and rough cardiac segmentations with local shifts serve as the anatomical information. We validated our proposed method on a clinical $^{82}$Rb PET dataset and found that our TAI-GAN can produce converted early frames with high image quality, comparable to the real reference frames. After TAI-GAN conversion, motion estimation accuracy and clinical myocardial blood flow (MBF) quantification were improved compared to using the original frames. Our code is published at https://github.com/gxq1998/TAI-GAN.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of cross-frame distribution in dynamic cardiac positron emission tomography (PET) raise significant challenges for inter-frame motion correction, particularly for the early frames where conventional intensity-based image registration techniques are not applicable. Alternatively, a promising approach utilizes generative methods to handle the tracer distribution changes to assist existing registration methods. To improve frame-wise registration and parametric quantification, we propose a Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) to transform the early frames into the late reference frame using an all-to-one mapping. Specifically, a feature-wise linear modulation layer encodes channel-wise parameters generated from temporal tracer kinetics information, and rough cardiac segmentations with local shifts serve as the anatomical information. We validated our proposed method on a clinical $^{82}$Rb PET dataset and found that our TAI-GAN can produce converted early frames with high image quality, comparable to the real reference frames. After TAI-GAN conversion, motion estimation accuracy and clinical myocardial blood flow (MBF) quantification were improved compared to using the original frames. Our code is published at https://github.com/gxq1998/TAI-GAN." into Simplified Chinese.Here's the translation:<<SYS>>rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of cross-frame distribution in dynamic cardiac positron emission tomography (PET) pose significant challenges for inter-frame motion correction, especially for early frames where conventional intensity-based image registration techniques are not applicable. alternatively, a promising approach utilizes generative methods to handle tracer distribution changes to assist existing registration methods. to improve frame-wise registration and parametric quantification, we propose a Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) to transform early frames into the late reference frame using an all-to-one mapping. specifically, a feature-wise linear modulation layer encodes channel-wise parameters generated from temporal tracer kinetics information, and rough cardiac segmentations with local shifts serve as anatomical information. we validated our proposed method on a clinical $^{82}$Rb PET dataset and found that our TAI-GAN can produce converted early frames with high image quality, comparable to real reference frames. after TAI-GAN conversion, motion estimation accuracy and clinical myocardial blood flow (MBF) quantification were improved compared to using the original frames. our code is published at https://github.com/gxq1998/TAI-GAN.Note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="HNAS-reg-hierarchical-neural-architecture-search-for-deformable-medical-image-registration"><a href="#HNAS-reg-hierarchical-neural-architecture-search-for-deformable-medical-image-registration" class="headerlink" title="HNAS-reg: hierarchical neural architecture search for deformable medical image registration"></a>HNAS-reg: hierarchical neural architecture search for deformable medical image registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12440">http://arxiv.org/abs/2308.12440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiong Wu, Yong Fan</li>
<li>for: 这篇论文是为了找出最佳的深度学习模型，用于医疗影像注册。</li>
<li>methods: 这篇论文使用了一个内在的 NAS 框架 (HNAS-Reg)，包括了扩散操作搜索和网络架构搜索，以找到最佳的网络架构。具体来说，这个框架使用了一种参数化的搜索方法，以找到最佳的扩散操作和网络架构。</li>
<li>results: 实验结果显示，提议的方法可以建立一个具有更高影像注册精度和较小的模型大小的深度学习模型，比过去的影像注册方法更好。具体来说，在三个数据集上（包括 636 个 T1-调试磁共振成像（MRI）），提议的方法可以建立一个深度学习模型，并且与其他两个Unsupervised Learning-based方法相比，具有更高的影像注册精度和较小的模型大小。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have been widely used to build deep learning models for medical image registration, but manually designed network architectures are not necessarily optimal. This paper presents a hierarchical NAS framework (HNAS-Reg), consisting of both convolutional operation search and network topology search, to identify the optimal network architecture for deformable medical image registration. To mitigate the computational overhead and memory constraints, a partial channel strategy is utilized without losing optimization quality. Experiments on three datasets, consisting of 636 T1-weighted magnetic resonance images (MRIs), have demonstrated that the proposal method can build a deep learning model with improved image registration accuracy and reduced model size, compared with state-of-the-art image registration approaches, including one representative traditional approach and two unsupervised learning-based approaches.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）已经广泛用于深度学习模型的医学图像注册，但是手动设计的网络架构可能不是最佳的。这篇论文提出了一种层次 NAS 框架（HNAS-Reg），包括卷积操作搜索和网络架构搜索，以确定最佳的医学图像注册网络架构。为了减少计算负担和内存限制，该方法使用了部分通道策略，而不失去优化质量。在三个数据集上，包括 636 个 T1 束缚磁共振成像（MRI），实验表明，提议方法可以建立一个具有提高图像注册精度和减少模型大小的深度学习模型，相比之下一个代表性的传统方法和两个无监督学习方法。
</details></li>
</ul>
<hr>
<h2 id="Characterising-representation-dynamics-in-recurrent-neural-networks-for-object-recognition"><a href="#Characterising-representation-dynamics-in-recurrent-neural-networks-for-object-recognition" class="headerlink" title="Characterising representation dynamics in recurrent neural networks for object recognition"></a>Characterising representation dynamics in recurrent neural networks for object recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12435">http://arxiv.org/abs/2308.12435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sushrut Thorat, Adrien Doerig, Tim C. Kietzmann</li>
<li>for: 这种研究旨在理解Recurrent Neural Networks (RNNs) 在复杂视觉任务中的表征动态，特别是大规模视觉模型中的计算。</li>
<li>methods: 研究者使用了MiniEcoset，一个新的子集，来训练 RNNs 进行物体分类。他们还使用了“读取区”来描述计算轨迹的活动排序。</li>
<li>results: 研究者发现，在推断时，表示 continuted to evolve  после正确的分类，这表明 RNNs 没有“完成分类”的概念。此外，研究者发现，在“读取区”中，错误的表示具有较低的L2范数活动排序，并位于更加外围的位置。这种排序可以帮助错误的表示逐渐移动到正确的区域。这些发现可以普通到其他类型的 RNNs，包括理解Primates 视觉中的表征动态。<details>
<summary>Abstract</summary>
Recurrent neural networks (RNNs) have yielded promising results for both recognizing objects in challenging conditions and modeling aspects of primate vision. However, the representational dynamics of recurrent computations remain poorly understood, especially in large-scale visual models. Here, we studied such dynamics in RNNs trained for object classification on MiniEcoset, a novel subset of ecoset. We report two main insights. First, upon inference, representations continued to evolve after correct classification, suggesting a lack of the notion of being ``done with classification''. Second, focusing on ``readout zones'' as a way to characterize the activation trajectories, we observe that misclassified representations exhibit activation patterns with lower L2 norm, and are positioned more peripherally in the readout zones. Such arrangements help the misclassified representations move into the correct zones as time progresses. Our findings generalize to networks with lateral and top-down connections, and include both additive and multiplicative interactions with the bottom-up sweep. The results therefore contribute to a general understanding of RNN dynamics in naturalistic tasks. We hope that the analysis framework will aid future investigations of other types of RNNs, including understanding of representational dynamics in primate vision.
</details>
<details>
<summary>摘要</summary>
recurrent neural networks (RNNs) 已经在具有挑战性的条件下识别对象以及模型Primates的视觉方面显示了promising的结果。然而，RNNs中的表达动力学 Dynamics 仍未得到了充分的理解，特别是在大规模的视觉模型中。在这里，我们对RNNs在MiniEcoset上进行了对象分类训练。我们发现了两个主要的发现：首先，在推理时，表达还在继续进行改变，表明没有“完成分类”的概念。第二，我们将“读取区”作为表达轨迹的特征进行分析，发现了在读取区中的表达方式具有更低的L2范数，并且位于读取区的更外围位置。这种排列可以帮助错误的表达移动到正确的区域，并在时间的推移中进行改变。我们的发现涵盖了具有 Lateral 和上下 Connection 的网络，并包括了加法和乘法交互。这些结果因此对 RNN 动力学在自然任务中的一般理解做出了贡献，并且可以帮助未来对其他类型的 RNN 进行更深入的研究，包括理解primates 视觉中的表达动力学。
</details></li>
</ul>
<hr>
<h2 id="A-Spatiotemporal-Correspondence-Approach-to-Unsupervised-LiDAR-Segmentation-with-Traffic-Applications"><a href="#A-Spatiotemporal-Correspondence-Approach-to-Unsupervised-LiDAR-Segmentation-with-Traffic-Applications" class="headerlink" title="A Spatiotemporal Correspondence Approach to Unsupervised LiDAR Segmentation with Traffic Applications"></a>A Spatiotemporal Correspondence Approach to Unsupervised LiDAR Segmentation with Traffic Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12433">http://arxiv.org/abs/2308.12433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Li, Pan He, Aotian Wu, Sanjay Ranka, Anand Rangarajan</li>
<li>for: 这个研究旨在解决室外LiDAR点云Sequence中的无监督Semantic Segmentation问题，尤其是在自动驾驶和交叉基建中的多种交通情况下。</li>
<li>methods: 本研究利用Point cloud sequence的空间时间特性，并在多帧框架之间建立强大的对应关系，以提高Semantic Segmentation的精度。研究将 clustering和pseudo-label学习结合，将点 cloud分组成Semantic groups，并使用点 clouds的pseudo-spatiotemporal标签进行模型优化。</li>
<li>results: 研究在Semantic-KITTI、SemanticPOSS和FLORIDAbenchmark dataset上得到了竞争性的Semantic Segmentation性能，与许多现有的对照学习方法相比。这个通用框架可以带来LiDAR点云Sequence中的统一表现学习方法，并结合对领域知识的导入。<details>
<summary>Abstract</summary>
We address the problem of unsupervised semantic segmentation of outdoor LiDAR point clouds in diverse traffic scenarios. The key idea is to leverage the spatiotemporal nature of a dynamic point cloud sequence and introduce drastically stronger augmentation by establishing spatiotemporal correspondences across multiple frames. We dovetail clustering and pseudo-label learning in this work. Essentially, we alternate between clustering points into semantic groups and optimizing models using point-wise pseudo-spatiotemporal labels with a simple learning objective. Therefore, our method can learn discriminative features in an unsupervised learning fashion. We show promising segmentation performance on Semantic-KITTI, SemanticPOSS, and FLORIDA benchmark datasets covering scenarios in autonomous vehicle and intersection infrastructure, which is competitive when compared against many existing fully supervised learning methods. This general framework can lead to a unified representation learning approach for LiDAR point clouds incorporating domain knowledge.
</details>
<details>
<summary>摘要</summary>
我们 Addressing the problem of unsupervised semantic segmentation of outdoor LiDAR point clouds in diverse traffic scenarios. The key idea is to leverage the spatiotemporal nature of a dynamic point cloud sequence and introduce drastically stronger augmentation by establishing spatiotemporal correspondences across multiple frames. We dovetail clustering and pseudo-label learning in this work. Essentially, we alternate between clustering points into semantic groups and optimizing models using point-wise pseudo-spatiotemporal labels with a simple learning objective. Therefore, our method can learn discriminative features in an unsupervised learning fashion. We show promising segmentation performance on Semantic-KITTI, SemanticPOSS, and FLORIDA benchmark datasets covering scenarios in autonomous vehicle and intersection infrastructure, which is competitive when compared against many existing fully supervised learning methods. This general framework can lead to a unified representation learning approach for LiDAR point clouds incorporating domain knowledge.Here's the word-for-word translation:我们 Addressing outdoor LiDAR point cloud semantic segmentation problem in diverse traffic scenarios. 针对多个 traffic scenarios 中的 outdoor LiDAR point cloud semantic segmentation problem. The key idea is to leverage point cloud sequence's spatiotemporal nature and introduce stronger augmentation by establishing spatiotemporal correspondences across multiple frames. 利用 point cloud sequence 的 spatiotemporal nature 和多幅 frames 之间的匹配，提高 semantic segmentation 的精度。 We dovetail clustering and pseudo-label learning in this work. 在这个工作中，我们将 clustering 和 pseudo-label learning 相互协调使用。 Essentially, we alternate between clustering points into semantic groups and optimizing models using point-wise pseudo-spatiotemporal labels with a simple learning objective. 我们将 alternate between clustering points into semantic groups 和使用 point-wise pseudo-spatiotemporal labels 来优化模型，使用简单的 learning objective。 Therefore, our method can learn discriminative features in an unsupervised learning fashion. 因此，我们的方法可以在无监督学习中学习出 distinguishing 特征。 We show promising segmentation performance on Semantic-KITTI, SemanticPOSS, and FLORIDA benchmark datasets covering scenarios in autonomous vehicle and intersection infrastructure. 我们在 Semantic-KITTI, SemanticPOSS, 和 FLORIDA benchmark datasets 上显示出了优秀的 segmentation 性能，这些 datasets 涵盖了自动驾驶车和交叉道路基础设施的场景。 These datasets are competitive when compared against many existing fully supervised learning methods. 这些 datasets 与许多现有的完全监督学习方法相比，显示出了竞争力。 This general framework can lead to a unified representation learning approach for LiDAR point clouds incorporating domain knowledge. 这个通用的框架可以导致一种 incorporating domain knowledge 的 LiDAR point clouds 的表示学习方法。
</details></li>
</ul>
<hr>
<h2 id="An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video"><a href="#An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video" class="headerlink" title="An Initial Exploration: Learning to Generate Realistic Audio for Silent Video"></a>An Initial Exploration: Learning to Generate Realistic Audio for Silent Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12408">http://arxiv.org/abs/2308.12408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Martel, Jackson Wagner</li>
<li>for: 这个论文的目的是开发一种基于深度学习的框架，用于生成电影和其他媒体中的真实的音效。</li>
<li>methods: 这个论文使用了多种不同的模型建立，包括深度融合CNN、扩展Wavenet CNN以及Transformer结构。这些模型都将视频上下文和先前生成的音频融合在一起，以生成真实的音效。</li>
<li>results: 研究发现，使用Transformer结构可以匹配视频中的低频信号，但是无法生成更加复杂的波形。<details>
<summary>Abstract</summary>
Generating realistic audio effects for movies and other media is a challenging task that is accomplished today primarily through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in it's natural sequence and generates realistic audio to accompany it. Notably, we have reason to believe this is achievable due to advancements in realistic audio generation techniques conditioned on other inputs (e.g., Wavenet conditioned on text). We explore several different model architectures to accomplish this task that process both previously-generated audio and video context. These include deep-fusion CNN, dilated Wavenet CNN with visual context, and transformer-based architectures. We find that the transformer-based architecture yields the most promising results, matching low-frequencies to visual patterns effectively, but failing to generate more nuanced waveforms.
</details>
<details>
<summary>摘要</summary>
generate realistic audio effects for movies and other media is a challenging task that is primarily accomplished today through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in its natural sequence and generates realistic audio to accompany it. Notably, we have reason to believe this is achievable due to advancements in realistic audio generation techniques conditioned on other inputs (e.g., Wavenet conditioned on text). We explore several different model architectures to accomplish this task that process both previously-generated audio and video context. These include deep-fusion CNN, dilated Wavenet CNN with visual context, and transformer-based architectures. We find that the transformer-based architecture yields the most promising results, matching low-frequencies to visual patterns effectively, but failing to generate more nuanced waveforms.Here's the text with some notes on the translation:* "generate realistic audio effects" is translated as "生成真实的音效" (shēngjīn zhēnshí de yīngxìng), which is a more literal translation of the original English phrase.* "Foley art" is translated as "FOLEY艺术" (fōlēi yìshù), which is a direct translation of the original English phrase.* "captivating audio tracks" is translated as "吸引人的音乐轨迹" (xīhuī rén de yīngyuè guītà), which is a more idiomatic translation that conveys the idea of audio that is engaging and immersive.* "deep-learning based framework" is translated as "基于深度学习的框架" (jīyù shēngrán de kuàihù), which is a more literal translation of the original English phrase.* "low-frequencies" is translated as "低频谱" (dīfreqè), which is a more technical term that is commonly used in audio engineering.* "visual patterns" is translated as "视觉模式" (wèishì móxìng), which is a more idiomatic translation that conveys the idea of patterns that are visible and can be perceived through sight.I hope this helps! Let me know if you have any further questions or if you would like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="FG-Net-Facial-Action-Unit-Detection-with-Generalizable-Pyramidal-Features"><a href="#FG-Net-Facial-Action-Unit-Detection-with-Generalizable-Pyramidal-Features" class="headerlink" title="FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features"></a>FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12380">http://arxiv.org/abs/2308.12380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ihp-lab/fg-net">https://github.com/ihp-lab/fg-net</a></li>
<li>paper_authors: Yufeng Yin, Di Chang, Guoxian Song, Shen Sang, Tiancheng Zhi, Jing Liu, Linjie Luo, Mohammad Soleymani</li>
<li>for: 该文章目的是提出一种通用的表情动作单元检测方法，以优化对 facial expression 的 объектив分析。</li>
<li>methods: 该方法使用 StyleGAN2 模型预训练在大型和多样化的面孔图像集上，然后使用 Pyramid CNN Interpreter 检测表情动作单元。</li>
<li>results: 对于 DISFA 和 BP4D  datasets，提出的方法在跨域和同域检测中均达到了优于预先的状态艺术，同时在1000个样本上进行训练并且可以达到竞争性的性能。<details>
<summary>Abstract</summary>
Automatic detection of facial Action Units (AUs) allows for objective facial expression analysis. Due to the high cost of AU labeling and the limited size of existing benchmarks, previous AU detection methods tend to overfit the dataset, resulting in a significant performance loss when evaluated across corpora. To address this problem, we propose FG-Net for generalizable facial action unit detection. Specifically, FG-Net extracts feature maps from a StyleGAN2 model pre-trained on a large and diverse face image dataset. Then, these features are used to detect AUs with a Pyramid CNN Interpreter, making the training efficient and capturing essential local features. The proposed FG-Net achieves a strong generalization ability for heatmap-based AU detection thanks to the generalizable and semantic-rich features extracted from the pre-trained generative model. Extensive experiments are conducted to evaluate within- and cross-corpus AU detection with the widely-used DISFA and BP4D datasets. Compared with the state-of-the-art, the proposed method achieves superior cross-domain performance while maintaining competitive within-domain performance. In addition, FG-Net is data-efficient and achieves competitive performance even when trained on 1000 samples. Our code will be released at \url{https://github.com/ihp-lab/FG-Net}
</details>
<details>
<summary>摘要</summary>
自动检测人脸动作单元（AU）可以实现 объектив的人脸表达分析。由于AU标注的高成本和现有 benchmark 的有限大小，前一代AU检测方法往往会适应数据集，导致在 corpora 中表现不佳。为解决这个问题，我们提出了 FG-Net，一种通用的人脸动作单元检测方法。具体来说，FG-Net 从 StyleGAN2 模型在大量和多样的人脸图像数据集上预训练后的特征图进行检测AU。然后，这些特征图被 Pyramid CNN Interpreter 使用，以实现高效的训练和捕捉本地特征。我们提出的 FG-Net 在热图基于 AU 检测中实现了强大的总结能力，因为它可以从预训练的生成模型中提取通用和含义 Rich 的特征。我们进行了广泛的实验，以评估在 DISFA 和 BP4D 数据集上的在 corpora 和 across-corpus 中的 AU 检测性能。与当前状态的方法相比，我们的方法在跨频谱上实现了superior 的横跨频谱性能，同时保持竞争的在频谱内性能。此外，FG-Net 是数据效率的，可以在1000个样本上实现竞争性的表现。我们的代码将在 \url{https://github.com/ihp-lab/FG-Net} 上发布。
</details></li>
</ul>
<hr>
<h2 id="AdVerb-Visually-Guided-Audio-Dereverberation"><a href="#AdVerb-Visually-Guided-Audio-Dereverberation" class="headerlink" title="AdVerb: Visually Guided Audio Dereverberation"></a>AdVerb: Visually Guided Audio Dereverberation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12370">http://arxiv.org/abs/2308.12370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjoy Chowdhury, Sreyan Ghosh, Subhrajyoti Dasgupta, Anton Ratnarajah, Utkarsh Tyagi, Dinesh Manocha</li>
<li>for: 提高听起来的声音质量，使其更加清晰和可识别。</li>
<li>methods: 利用视觉特征和听起来的声音，通过一种新的几何学感知架构，捕捉场景几何和听起来的跨Modal关系，生成复杂的理想比例幕，以提高听起来的声音质量。</li>
<li>results: 比较传统的听起来只和听起来+视觉两个基elines，实现了18%-82%的提升，在LibriSpeech测试集上。同时，在AVSpeech数据集上也实现了非常满意的RT60错误分数。<details>
<summary>Abstract</summary>
We present AdVerb, a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio. Although audio-only dereverberation is a well-studied problem, our approach incorporates the complementary visual modality to perform audio dereverberation. Given an image of the environment where the reverberated sound signal has been recorded, AdVerb employs a novel geometry-aware cross-modal transformer architecture that captures scene geometry and audio-visual cross-modal relationship to generate a complex ideal ratio mask, which, when applied to the reverberant audio predicts the clean sound. The effectiveness of our method is demonstrated through extensive quantitative and qualitative evaluations. Our approach significantly outperforms traditional audio-only and audio-visual baselines on three downstream tasks: speech enhancement, speech recognition, and speaker verification, with relative improvements in the range of 18% - 82% on the LibriSpeech test-clean set. We also achieve highly satisfactory RT60 error scores on the AVSpeech dataset.
</details>
<details>
<summary>摘要</summary>
我们介绍了AdVerb，一种新的音频-视觉减振框架，该框架利用视觉信号以及干扰音频来估算清晰音频。虽然音频只的减振框架已经广泛研究过，但我们的方法具有较好的场景准确性和音频视觉跨模态关系，可以更好地进行音频减振。给出了环境中录制的干扰音频的图像，AdVerb使用了一种新的场景意识的cross-modal transformer架构，捕捉场景准确性和音频视觉跨模态关系，生成复杂的理想比例面积，当应用于干扰音频时，可以预测清晰音频。我们的方法的效果得到了广泛的量化和质量评估。与传统的音频只和音频视觉基线相比，我们的方法在三个下游任务中表现出了显著的改善，即speech enhancement、speech recognition和speaker verification，改善比例在0.18-0.82之间。此外，我们在AVSpeech dataset上也实现了高度满意的RT60错误分布。
</details></li>
</ul>
<hr>
<h2 id="Continual-Zero-Shot-Learning-through-Semantically-Guided-Generative-Random-Walks"><a href="#Continual-Zero-Shot-Learning-through-Semantically-Guided-Generative-Random-Walks" class="headerlink" title="Continual Zero-Shot Learning through Semantically Guided Generative Random Walks"></a>Continual Zero-Shot Learning through Semantically Guided Generative Random Walks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12366">http://arxiv.org/abs/2308.12366</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wx-zhang/igczsl">https://github.com/wx-zhang/igczsl</a></li>
<li>paper_authors: Wenxuan Zhang, Paul Janson, Kai Yi, Ivan Skorokhodov, Mohamed Elhoseiny</li>
<li>for: 本研究旨在模型人类在生活中不断学习和应用新知识，以及将之应用于未来任务中。</li>
<li>methods: 本研究使用生成模型，通过学习seen类的质量表示来提高对未经训练的视觉空间的生成理解。</li>
<li>results: 提出了一种基于生成模型的 continual zero-shot learning 算法，在 AWA1、AWA2、CUB 和 SUN 数据集上达到了状态之 arts 性能，比现有的 CZSL 方法高出 3-7%。<details>
<summary>Abstract</summary>
Learning novel concepts, remembering previous knowledge, and adapting it to future tasks occur simultaneously throughout a human's lifetime. To model such comprehensive abilities, continual zero-shot learning (CZSL) has recently been introduced. However, most existing methods overused unseen semantic information that may not be continually accessible in realistic settings. In this paper, we address the challenge of continual zero-shot learning where unseen information is not provided during training, by leveraging generative modeling. The heart of the generative-based methods is to learn quality representations from seen classes to improve the generative understanding of the unseen visual space. Motivated by this, we introduce generalization-bound tools and provide the first theoretical explanation for the benefits of generative modeling to CZSL tasks. Guided by the theoretical analysis, we then propose our learning algorithm that employs a novel semantically guided Generative Random Walk (GRW) loss. The GRW loss augments the training by continually encouraging the model to generate realistic and characterized samples to represent the unseen space. Our algorithm achieves state-of-the-art performance on AWA1, AWA2, CUB, and SUN datasets, surpassing existing CZSL methods by 3-7\%. The code has been made available here \url{https://github.com/wx-zhang/IGCZSL}
</details>
<details>
<summary>摘要</summary>
人类生命中，同时学习新概念，记忆过去知识，并将其应用到未来任务中发生。为模型这种全面能力，最近才提出了无限 zero-shot learning（CZSL）。然而，现有的方法往往过度利用无法在实际场景中 continually 获得的无序 semantic information。在这篇论文中，我们解决了 CZSL 任务中无法在训练中提供无序信息的挑战，通过使用生成模型。生成模型的核心是学习seen类型的高质量表示，以改善对未seen visual空间的生成理解。这些基于的概念工具，我们提供了第一个理论解释，描述了生成模型对 CZSL 任务的优势。受理论分析的指导，我们然后提出了我们的学习算法，该算法使用了一种新的semantically guided Generative Random Walk（GRW）损失函数。GRW损失函数在训练中不断地鼓励模型生成真实、特征化的样本，以表示未seen空间。我们的算法在 AWA1、AWA2、CUB 和 SUN 数据集上达到了状态机器人的性能，超过了现有的 CZSL 方法3-7\%。我们的代码已经在 GitHub 上公开，访问地址为 \url{https://github.com/wx-zhang/IGCZSL}。
</details></li>
</ul>
<hr>
<h2 id="Saliency-based-Video-Summarization-for-Face-Anti-spoofing"><a href="#Saliency-based-Video-Summarization-for-Face-Anti-spoofing" class="headerlink" title="Saliency-based Video Summarization for Face Anti-spoofing"></a>Saliency-based Video Summarization for Face Anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12364">http://arxiv.org/abs/2308.12364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Usman1021/Saliency">https://github.com/Usman1021/Saliency</a></li>
<li>paper_authors: Usman Muhammad, Mourad Oussalah, Md Ziaul Hoque, Jorma Laaksonen</li>
<li>for: 提高面部骗取检测器的性能和效率，使用视觉吸引力理论来增强深度学习模型的表现。</li>
<li>methods: 提出了一种视频概要方法，通过提取源图像的视觉吸引力信息，对每帧图像进行分解，并使用重要性映射来线性组合源图像，创建一个代表整个视频的单一图像。</li>
<li>results: 实验结果表明，该方法可以在五个面部骗取检测 datasets 上达到状态 искусственный智能的性能，并且比 tradicional 方法有更好的性能和效率。<details>
<summary>Abstract</summary>
Due to the growing availability of face anti-spoofing databases, researchers are increasingly focusing on video-based methods that use hundreds to thousands of images to assess their impact on performance. However, there is no clear consensus on the exact number of frames in a video required to improve the performance of face anti-spoofing tasks. Inspired by the visual saliency theory, we present a video summarization method for face anti-spoofing tasks that aims to enhance the performance and efficiency of deep learning models by leveraging visual saliency. In particular, saliency information is extracted from the differences between the Laplacian and Wiener filter outputs of the source images, enabling identification of the most visually salient regions within each frame. Subsequently, the source images are decomposed into base and detail layers, enhancing representation of important information. The weighting maps are then computed based on the saliency information, indicating the importance of each pixel in the image. By linearly combining the base and detail layers using the weighting maps, the method fuses the source images to create a single representative image that summarizes the entire video. The key contribution of our proposed method lies in demonstrating how visual saliency can be used as a data-centric approach to improve the performance and efficiency of face presentation attack detection models. By focusing on the most salient images or regions within the images, a more representative and diverse training set can be created, potentially leading to more effective models. To validate the method's effectiveness, a simple deep learning architecture (CNN-RNN) was used, and the experimental results showcased state-of-the-art performance on five challenging face anti-spoofing datasets.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于面对面骗降库的可用性不断增长，研究人员正在更加关注视频基于方法，使用数百到千个图像来评估其影响性。然而，没有明确的共识，关于视频中帧数所需要提高面对面骗降模型的性能。我们根据视觉吸引力理论，提出了一种面对面骗降视频 summarization方法，以提高深度学习模型的性能和效率。具体来说，该方法使用源图像的差分 Laplacian 和 Wiener 滤波器输出来提取视觉吸引力信息，并在每帧中标识最有吸引力的区域。然后，源图像被分解成基层和详细层，从而增强图像的重要信息表示。最后，根据视觉吸引力信息，计算weighting map，以指示每个像素的重要性。通过线性组合基层和详细层，方法将源图像总结为整个视频的代表图像。我们的提案的关键在于，通过使用视觉吸引力来为面对面骗降模型提高性能和效率。通过关注图像中最有吸引力的部分或区域，可以创建更加代表和多样的训练集，可能导致更有效的模型。为验证方法的效果，我们使用了一种简单的深度学习架构（CNN-RNN），并在五个面对面骗降数据集上获得了状态艺术性的实验结果。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Image-Translation-with-Label-Guidance-for-Domain-Adaptive-Semantic-Segmentation"><a href="#Diffusion-based-Image-Translation-with-Label-Guidance-for-Domain-Adaptive-Semantic-Segmentation" class="headerlink" title="Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation"></a>Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12350">http://arxiv.org/abs/2308.12350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Peng, Ping Hu, Qiuhong Ke, Jun Liu</li>
<li>for: 提高频率领域转换图像的semantic consistency</li>
<li>methods: 使用源域标签作为Explicit导航 during image translation</li>
<li>results: 比对前方法有superiority<details>
<summary>Abstract</summary>
Translating images from a source domain to a target domain for learning target models is one of the most common strategies in domain adaptive semantic segmentation (DASS). However, existing methods still struggle to preserve semantically-consistent local details between the original and translated images. In this work, we present an innovative approach that addresses this challenge by using source-domain labels as explicit guidance during image translation. Concretely, we formulate cross-domain image translation as a denoising diffusion process and utilize a novel Semantic Gradient Guidance (SGG) method to constrain the translation process, conditioning it on the pixel-wise source labels. Additionally, a Progressive Translation Learning (PTL) strategy is devised to enable the SGG method to work reliably across domains with large gaps. Extensive experiments demonstrate the superiority of our approach over state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
通常，域适应semantic segmentation（DASS）中将源域图像翻译到目标域图像是一种常见的策略。然而，现有方法仍然困难保持 semantic consistency的本地细节 между原始图像和翻译图像。在这种情况下，我们提出了一种创新的方法，通过在翻译过程中使用源域标签作为直接导航来解决这个挑战。具体来说，我们将cross-domain image translation表示为干扰扩散过程，并使用一种新的Semantic Gradient Guidance（SGG）方法来约束翻译过程，将其受到像素级source标签的控制。此外，我们还提出了一种Progressive Translation Learning（PTL）策略，以确保 SGG 方法在不同域的大差下可靠地工作。广泛的实验证明了我们的方法在现有方法之上表现出了superiority。
</details></li>
</ul>
<hr>
<h2 id="A-Generative-Approach-for-Image-Registration-of-Visible-Thermal-VT-Cancer-Faces"><a href="#A-Generative-Approach-for-Image-Registration-of-Visible-Thermal-VT-Cancer-Faces" class="headerlink" title="A Generative Approach for Image Registration of Visible-Thermal (VT) Cancer Faces"></a>A Generative Approach for Image Registration of Visible-Thermal (VT) Cancer Faces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12271">http://arxiv.org/abs/2308.12271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catherine Ordun, Alexandra Cha, Edward Raff, Sanjay Purushotham, Karen Kwok, Mason Rule, James Gulley</li>
<li>for: 这项研究旨在提高人工智能下的疼痛研究，使用可见光和热成像图像进行对比。</li>
<li>methods: 该研究使用了生成对应算法进行图像 регистра，以解决可见光和热成像图像之间的偏移问题。</li>
<li>results: 研究发现，通过对可见光和热成像图像进行REGISTERING，可以提高热成像图像质量，提高疼痛研究的效果，最高提高率达52.5%。<details>
<summary>Abstract</summary>
Since thermal imagery offers a unique modality to investigate pain, the U.S. National Institutes of Health (NIH) has collected a large and diverse set of cancer patient facial thermograms for AI-based pain research. However, differing angles from camera capture between thermal and visible sensors has led to misalignment between Visible-Thermal (VT) images. We modernize the classic computer vision task of image registration by applying and modifying a generative alignment algorithm to register VT cancer faces, without the need for a reference or alignment parameters. By registering VT faces, we demonstrate that the quality of thermal images produced in the generative AI downstream task of Visible-to-Thermal (V2T) image translation significantly improves up to 52.5\%, than without registration. Images in this paper have been approved by the NIH NCI for public dissemination.
</details>
<details>
<summary>摘要</summary>
由于热影像可以提供一种独特的方式来研究疼痛，美国国家医学研究院（NIH）已经收集了大量和多样化的癌症患者脸部热影像，用于人工智能基于痛症研究。然而，相机捕捉的角度差异导致热影像和可见感器拍摄的图像不一致，这导致了可见热图像的注册问题。我们使用和修改生成对齐算法，以无需参考或对齐参数，对热照相机拍摄的癌症脸部进行注册。通过注册热照相机拍摄，我们证明了在生成AI下渠道任务中，将可见图像翻译成热图像的质量显著提高，比无注册情况提高至52.5%。图像在本文中已经获得了NIH NCI的批准，可以公开发布。
</details></li>
</ul>
<hr>
<h2 id="MolGrapher-Graph-based-Visual-Recognition-of-Chemical-Structures"><a href="#MolGrapher-Graph-based-Visual-Recognition-of-Chemical-Structures" class="headerlink" title="MolGrapher: Graph-based Visual Recognition of Chemical Structures"></a>MolGrapher: Graph-based Visual Recognition of Chemical Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12234">http://arxiv.org/abs/2308.12234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ds4sd/molgrapher">https://github.com/ds4sd/molgrapher</a></li>
<li>paper_authors: Lucas Morin, Martin Danelljan, Maria Isabel Agea, Ahmed Nassar, Valery Weber, Ingmar Meijer, Peter Staar, Fisher Yu</li>
<li>for: 本研究旨在提高化学文献自动分析的效率，以促进新材料和药物的发现。</li>
<li>methods: 本研究使用了深度键点检测器和图学神经网络来自动识别化学结构。</li>
<li>results: 对五个数据集进行了广泛的实验，结果表明，我们的方法在大多数情况下与经典和学习基于方法相比，有显著的优异表现。<details>
<summary>Abstract</summary>
The automatic analysis of chemical literature has immense potential to accelerate the discovery of new materials and drugs. Much of the critical information in patent documents and scientific articles is contained in figures, depicting the molecule structures. However, automatically parsing the exact chemical structure is a formidable challenge, due to the amount of detailed information, the diversity of drawing styles, and the need for training data. In this work, we introduce MolGrapher to recognize chemical structures visually. First, a deep keypoint detector detects the atoms. Second, we treat all candidate atoms and bonds as nodes and put them in a graph. This construct allows a natural graph representation of the molecule. Last, we classify atom and bond nodes in the graph with a Graph Neural Network. To address the lack of real training data, we propose a synthetic data generation pipeline producing diverse and realistic results. In addition, we introduce a large-scale benchmark of annotated real molecule images, USPTO-30K, to spur research on this critical topic. Extensive experiments on five datasets show that our approach significantly outperforms classical and learning-based methods in most settings. Code, models, and datasets are available.
</details>
<details>
<summary>摘要</summary>
自动分析化学文献的潜在可能性非常大，可以加速发现新材料和药物。文献中大量关键信息都集中在图像中，其中包括分子结构。然而，自动解析图像中的具体化学结构是一项具有挑战性的任务，原因在于图像中的信息量、绘制风格的多样性以及需要训练数据。在这项工作中，我们介绍了MolGrapher，一种可视化化学结构的识别算法。首先，我们使用深度关键点检测器检测原子。其次，我们将所有候选原子和键视为图像中的节点，并将它们建立成一个图。这种构建方式允许自然地表示分子的图像。最后，我们使用图 neural network 来分类原子和键节点。因为缺乏真实的训练数据，我们提出了一个生成 sintetic 数据的管道，以生成多样化和真实的结果。此外，我们还介绍了一个大规模的注释实验室， USPTO-30K，以促进这一重要领域的研究。我们在五个数据集上进行了广泛的实验，结果显示，我们的方法在大多数情况下与 класси方法和学习型方法相比，表现出了显著的优势。代码、模型和数据集都可以获得。
</details></li>
</ul>
<hr>
<h2 id="SPPNet-A-Single-Point-Prompt-Network-for-Nuclei-Image-Segmentation"><a href="#SPPNet-A-Single-Point-Prompt-Network-for-Nuclei-Image-Segmentation" class="headerlink" title="SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation"></a>SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12231">http://arxiv.org/abs/2308.12231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xq141839/sppnet">https://github.com/xq141839/sppnet</a></li>
<li>paper_authors: Qing Xu, Wenwei Kuang, Zeyu Zhang, Xueyao Bao, Haoran Chen, Wenting Duan</li>
<li>for: 这个研究旨在提出一个单点提示网络（SPPNet），用于核仁像分类，以解决目前的模型存在大量参数和训练成本的问题。</li>
<li>methods: 这个模型使用了轻量级的投影 транс福曼（ViT）来取代原始的图像编码器，并添加了一个有效的混合层来提高图像中低层次的Semantic信息EXTRACTION。</li>
<li>results: 这个研究显示了 SPPNet 比现有的 U-shape 架构更好地运行，并且在训练过程中更快地训练。相比于目前的模型，SPPNet 的测试速度大约是 20 倍 faster，仅需要 1&#x2F;70 参数和computational cost。此外，这个模型只需要在训练和测试阶段点击一次，更适合临床应用。<details>
<summary>Abstract</summary>
Image segmentation plays an essential role in nuclei image analysis. Recently, the segment anything model has made a significant breakthrough in such tasks. However, the current model exists two major issues for cell segmentation: (1) the image encoder of the segment anything model involves a large number of parameters. Retraining or even fine-tuning the model still requires expensive computational resources. (2) in point prompt mode, points are sampled from the center of the ground truth and more than one set of points is expected to achieve reliable performance, which is not efficient for practical applications. In this paper, a single-point prompt network is proposed for nuclei image segmentation, called SPPNet. We replace the original image encoder with a lightweight vision transformer. Also, an effective convolutional block is added in parallel to extract the low-level semantic information from the image and compensate for the performance degradation due to the small image encoder. We propose a new point-sampling method based on the Gaussian kernel. The proposed model is evaluated on the MoNuSeg-2018 dataset. The result demonstrated that SPPNet outperforms existing U-shape architectures and shows faster convergence in training. Compared to the segment anything model, SPPNet shows roughly 20 times faster inference, with 1/70 parameters and computational cost. Particularly, only one set of points is required in both the training and inference phases, which is more reasonable for clinical applications. The code for our work and more technical details can be found at https://github.com/xq141839/SPPNet.
</details>
<details>
<summary>摘要</summary>
Image segmentation plays an essential role in nuclei image analysis. Recently, the segment anything model has made a significant breakthrough in such tasks. However, the current model exists two major issues for cell segmentation: (1) the image encoder of the segment anything model involves a large number of parameters. Retraining or even fine-tuning the model still requires expensive computational resources. (2) in point prompt mode, points are sampled from the center of the ground truth and more than one set of points is expected to achieve reliable performance, which is not efficient for practical applications.In this paper, a single-point prompt network is proposed for nuclei image segmentation, called SPPNet. We replace the original image encoder with a lightweight vision transformer. Also, an effective convolutional block is added in parallel to extract the low-level semantic information from the image and compensate for the performance degradation due to the small image encoder. We propose a new point-sampling method based on the Gaussian kernel. The proposed model is evaluated on the MoNuSeg-2018 dataset. The result demonstrated that SPPNet outperforms existing U-shape architectures and shows faster convergence in training. Compared to the segment anything model, SPPNet shows roughly 20 times faster inference, with 1/70 parameters and computational cost. Particularly, only one set of points is required in both the training and inference phases, which is more reasonable for clinical applications.The code for our work and more technical details can be found at <https://github.com/xq141839/SPPNet>.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/cs.CV_2023_08_24/" data-id="cloojsmf000gyre882deg6oeg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/50/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/49/">49</a><a class="page-number" href="/page/50/">50</a><span class="page-number current">51</span><a class="page-number" href="/page/52/">52</a><a class="page-number" href="/page/53/">53</a><span class="space">&hellip;</span><a class="page-number" href="/page/87/">87</a><a class="extend next" rel="next" href="/page/52/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">67</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
