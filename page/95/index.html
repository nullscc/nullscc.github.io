
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/95/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.AS_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/eess.AS_2023_07_05/" class="article-date">
  <time datetime="2023-07-05T14:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/05/eess.AS_2023_07_05/">eess.AS - 2023-07-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Online-Hybrid-CTC-Attention-End-to-End-Automatic-Speech-Recognition-Architecture"><a href="#Online-Hybrid-CTC-Attention-End-to-End-Automatic-Speech-Recognition-Architecture" class="headerlink" title="Online Hybrid CTC&#x2F;Attention End-to-End Automatic Speech Recognition Architecture"></a>Online Hybrid CTC&#x2F;Attention End-to-End Automatic Speech Recognition Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02351">http://arxiv.org/abs/2307.02351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Miao, Gaofeng Cheng, Pengyuan Zhang, Yonghong Yan</li>
<li>for: This paper proposes an online hybrid CTC&#x2F;attention end-to-end ASR architecture for real-time speech recognition.</li>
<li>methods: The proposed architecture uses stable monotonic chunk-wise attention (sMoChA) to streamline global attention, truncated CTC (T-CTC) prefix score calculation, and dynamic waiting joint decoding (DWJD) algorithm for online prediction.</li>
<li>results: Compared with the offline CTC&#x2F;attention model, the proposed online CTC&#x2F;attention model improves the real-time factor in human-computer interaction services while maintaining moderate performance. This is the first full-stack online solution for CTC&#x2F;attention end-to-end ASR architecture.<details>
<summary>Abstract</summary>
Recently, there has been increasing progress in end-to-end automatic speech recognition (ASR) architecture, which transcribes speech to text without any pre-trained alignments. One popular end-to-end approach is the hybrid Connectionist Temporal Classification (CTC) and attention (CTC/attention) based ASR architecture. However, how to deploy hybrid CTC/attention systems for online speech recognition is still a non-trivial problem. This article describes our proposed online hybrid CTC/attention end-to-end ASR architecture, which replaces all the offline components of conventional CTC/attention ASR architecture with their corresponding streaming components. Firstly, we propose stable monotonic chunk-wise attention (sMoChA) to stream the conventional global attention, and further propose monotonic truncated attention (MTA) to simplify sMoChA and solve the training-and-decoding mismatch problem of sMoChA. Secondly, we propose truncated CTC (T-CTC) prefix score to stream CTC prefix score calculation. Thirdly, we design dynamic waiting joint decoding (DWJD) algorithm to dynamically collect the predictions of CTC and attention in an online manner. Finally, we use latency-controlled bidirectional long short-term memory (LC-BLSTM) to stream the widely-used offline bidirectional encoder network. Experiments with LibriSpeech English and HKUST Mandarin tasks demonstrate that, compared with the offline CTC/attention model, our proposed online CTC/attention model improves the real time factor in human-computer interaction services and maintains its performance with moderate degradation. To the best of our knowledge, this is the first work to provide the full-stack online solution for CTC/attention end-to-end ASR architecture.
</details>
<details>
<summary>摘要</summary>
近年来，endo-to-end自动语音识别（ASR）建筑有了显著的进步，这种语音识别模型不需要预先训练的Alignment。一种popular的endo-to-end方法是hybrid Connectionist Temporal Classification（CTC）和注意（CTC/注意）基于ASR建筑。然而，如何在线部署hybrid CTC/注意系统仍然是一个非常困难的问题。这篇文章描述了我们提议的在线hybrid CTC/注意末端ASR建筑，该建筑将所有的Offline组件替换为其相应的流动组件。首先，我们提出了稳定的均衡块级注意（sMoChA），以流动 conventiomal global注意，并提出了均衡 truncated attention（MTA），以解决sMoChA的训练和解码匹配问题。其次，我们提出了truncated CTC（T-CTC）前缀分数，以流动 CTC前缀分数计算。最后，我们设计了动态等待联合解码（DWJD）算法，以在线收集CTC和注意的预测。我们还使用了时钟控制的 bidirectional long short-term memory（LC-BLSTM），以流动 widely used offline bidirectional encoder network。实验表明，相比于Offline CTC/注意模型，我们提议的在线 CTC/注意模型在人机交互服务中提高了实时因素，并保持了其性能，虽有一定的衰减。到目前为止，这是首次提供了全栈在线解决CTC/注意末端ASR建筑的工作。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Adversarial-Auto-Encoder-to-Protect-Gender-in-Voice-Biometrics"><a href="#Differentially-Private-Adversarial-Auto-Encoder-to-Protect-Gender-in-Voice-Biometrics" class="headerlink" title="Differentially Private Adversarial Auto-Encoder to Protect Gender in Voice Biometrics"></a>Differentially Private Adversarial Auto-Encoder to Protect Gender in Voice Biometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02135">http://arxiv.org/abs/2307.02135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oubaïda Chouchane, Michele Panariello, Oualid Zari, Ismet Kerenciler, Imen Chihaoui, Massimiliano Todisco, Melek Önen</li>
<li>for: 隐藏 gender 信息以保护个人隐私，同时保持 speaker 识别效果。</li>
<li>methods: 使用 Adversarial Auto-Encoder approach，对 gender 信息进行隐藏，并通过 Laplace 机制实现 differential privacy 保障。</li>
<li>results: 在 VoxCeleb 数据集上，可以成功隐藏 speaker 的 gender 信息，同时保持 speaker 识别效果，并可以根据需要调整 Laplace 噪声的强度来选择 Privacy 和 Utility 之间的平衡。<details>
<summary>Abstract</summary>
Over the last decade, the use of Automatic Speaker Verification (ASV) systems has become increasingly widespread in response to the growing need for secure and efficient identity verification methods. The voice data encompasses a wealth of personal information, which includes but is not limited to gender, age, health condition, stress levels, and geographical and socio-cultural origins. These attributes, known as soft biometrics, are private and the user may wish to keep them confidential. However, with the advancement of machine learning algorithms, soft biometrics can be inferred automatically, creating the potential for unauthorized use. As such, it is crucial to ensure the protection of these personal data that are inherent within the voice while retaining the utility of identity recognition. In this paper, we present an adversarial Auto-Encoder--based approach to hide gender-related information in speaker embeddings, while preserving their effectiveness for speaker verification. We use an adversarial procedure against a gender classifier and incorporate a layer based on the Laplace mechanism into the Auto-Encoder architecture. This layer adds Laplace noise for more robust gender concealment and ensures differential privacy guarantees during inference for the output speaker embeddings. Experiments conducted on the VoxCeleb dataset demonstrate that speaker verification tasks can be effectively carried out while concealing speaker gender and ensuring differential privacy guarantees; moreover, the intensity of the Laplace noise can be tuned to select the desired trade-off between privacy and utility.
</details>
<details>
<summary>摘要</summary>
过去十年，自动说话验证（ASV）系统的使用越来越普遍，以应对安全和高效的身份验证方法的增长需求。语音数据包含大量个人信息，包括但不限于性别、年龄、健康状况、压力水平和地域和文化背景。这些属性被称为软生物метrics，用户可能希望保持隐私。然而，通过机器学习算法的提高，软生物метrics可以被推断出来，从而创造不当使用的可能性。因此，保护这些个人数据，并在保留身份识别的同时确保隐私，是极为重要的。在这篇论文中，我们提出了一种利用对抗学习掩蔽 gender 信息的自动编码器方法，保持 speaker 嵌入的有效性，同时确保隐私。我们通过对 gender 分类器进行对抗程序，并在 Auto-Encoder 架构中添加基于 Laplace 机制的层。这层在推断过程中添加 Laplace 噪声，以确保在输出 speaker 嵌入时的隐私保障。在 VoxCeleb 数据集上进行的实验表明，可以在掩蔽 speaker 的 gender 信息和确保隐私保障的同时进行有效的 speaker 验证任务。此外，可以根据需要调整 Laplace 噪声的强度，选择适当的隐私和功能之间的平衡。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-multilingual-transfer-for-unsupervised-semantic-acoustic-word-embeddings"><a href="#Leveraging-multilingual-transfer-for-unsupervised-semantic-acoustic-word-embeddings" class="headerlink" title="Leveraging multilingual transfer for unsupervised semantic acoustic word embeddings"></a>Leveraging multilingual transfer for unsupervised semantic acoustic word embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02083">http://arxiv.org/abs/2307.02083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christiaan Jacobs, Herman Kamper</li>
<li>for: 这paper的目的是研究语音词嵌入（AWE），并在这个基础上构建语音词嵌入模型，以实现语音词的语义表示。</li>
<li>methods: 这paper使用了一种多语言基础模型，将语音词分类为不同的语音类别，然后使用这些类别来生成语音词嵌入。</li>
<li>results: 这paper的实验结果表明，使用这种多语言基础模型可以实现语音词嵌入的语义表示，并且在语音词相似性任务中表现出色。此外，这paper还实现了语音词 Query-by-Example 搜索的功能。<details>
<summary>Abstract</summary>
Acoustic word embeddings (AWEs) are fixed-dimensional vector representations of speech segments that encode phonetic content so that different realisations of the same word have similar embeddings. In this paper we explore semantic AWE modelling. These AWEs should not only capture phonetics but also the meaning of a word (similar to textual word embeddings). We consider the scenario where we only have untranscribed speech in a target language. We introduce a number of strategies leveraging a pre-trained multilingual AWE model -- a phonetic AWE model trained on labelled data from multiple languages excluding the target. Our best semantic AWE approach involves clustering word segments using the multilingual AWE model, deriving soft pseudo-word labels from the cluster centroids, and then training a Skipgram-like model on the soft vectors. In an intrinsic word similarity task measuring semantics, this multilingual transfer approach outperforms all previous semantic AWE methods. We also show -- for the first time -- that AWEs can be used for downstream semantic query-by-example search.
</details>
<details>
<summary>摘要</summary>
听音字嵌入（AWEs）是指将speech segmentFixed-dimensional vector representation的方法，以便不同的实现方式中的同一个词有相似的嵌入。在这篇论文中，我们探索 semantic AWE 模型。这些 AWEs 不仅应 capture phonetics, but also the meaning of a word, similar to textual word embeddings。我们考虑了target language only have untranscribed speech的场景。我们提出了一些使用预训练的多语言 AWE 模型（包括目标语言）的策略。我们的最佳semantic AWE方法是 clustering word segments using the multilingual AWE model, deriving soft pseudo-word labels from the cluster centroids, and then training a Skipgram-like model on the soft vectors。在内在词 Similarity task中，这种多语言传递方法超过了所有前一个semantic AWE方法。我们还表明了，AWEs可以用于下游semantic query-by-example search。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/05/eess.AS_2023_07_05/" data-id="clpahu7a201333h88fdovebs7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/cs.CV_2023_07_05/" class="article-date">
  <time datetime="2023-07-05T13:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/05/cs.CV_2023_07_05/">cs.CV - 2023-07-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Detecting-Images-Generated-by-Deep-Diffusion-Models-using-their-Local-Intrinsic-Dimensionality"><a href="#Detecting-Images-Generated-by-Deep-Diffusion-Models-using-their-Local-Intrinsic-Dimensionality" class="headerlink" title="Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality"></a>Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02347">http://arxiv.org/abs/2307.02347</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepfake-study/deepfake-multiLID">https://github.com/deepfake-study/deepfake-multiLID</a></li>
<li>paper_authors: Peter Lorenz, Ricard Durall, Janis Keuper</li>
<li>for: 本研究旨在检测Diffusion模型生成的图像，并Identify相关的生成器网络。</li>
<li>methods: 该方法使用了Local Intrinsic Dimensionality（LID），具体来说是多重Local Intrinsic Dimensionality（multiLID），原本是用于检测对抗性图像的方法。</li>
<li>results: 实验结果表明，提议的multiLID方法在许多现实场景中具有超越性，可以准确检测Diffusion模型生成的图像，并正确地标识相关的生成器网络。<details>
<summary>Abstract</summary>
Diffusion models recently have been successfully applied for the visual synthesis of strikingly realistic appearing images. This raises strong concerns about their potential for malicious purposes. In this paper, we propose using the lightweight multi Local Intrinsic Dimensionality (multiLID), which has been originally developed in context of the detection of adversarial examples, for the automatic detection of synthetic images and the identification of the according generator networks. In contrast to many existing detection approaches, which often only work for GAN-generated images, the proposed method provides close to perfect detection results in many realistic use cases. Extensive experiments on known and newly created datasets demonstrate that the proposed multiLID approach exhibits superiority in diffusion detection and model identification. Since the empirical evaluations of recent publications on the detection of generated images are often mainly focused on the "LSUN-Bedroom" dataset, we further establish a comprehensive benchmark for the detection of diffusion-generated images, including samples from several diffusion models with different image sizes.
</details>
<details>
<summary>摘要</summary>
各种扩散模型最近已经成功应用于可见的图像合成，这引发了严重的不良用途的担忧。在这篇论文中，我们提议使用轻量级的多地方本各自维度（multiLID），这是在检测对抗性例子方面原始开发的。与许多现有的检测方法不同，我们的方法可以准确检测多种扩散模型生成的图像，而不是仅仅是GAN生成的图像。我们在各种实际应用场景中进行了广泛的实验，结果显示了我们的multiLID方法在扩散检测和模型标识方面具有显著的优势。由于现有文献中对生成图像的检测的评估 oftens mainly focuses on "LSUN-Bedroom" 数据集，我们还建立了一个包括多种扩散模型和不同图像大小的扩散检测和模型标识 benchmark。
</details></li>
</ul>
<hr>
<h2 id="GAFAR-Graph-Attention-Feature-Augmentation-for-Registration-A-Fast-and-Light-weight-Point-Set-Registration-Algorithm"><a href="#GAFAR-Graph-Attention-Feature-Augmentation-for-Registration-A-Fast-and-Light-weight-Point-Set-Registration-Algorithm" class="headerlink" title="GAFAR: Graph-Attention Feature-Augmentation for Registration A Fast and Light-weight Point Set Registration Algorithm"></a>GAFAR: Graph-Attention Feature-Augmentation for Registration A Fast and Light-weight Point Set Registration Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02339">http://arxiv.org/abs/2307.02339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mordecaimalignatius/gafar">https://github.com/mordecaimalignatius/gafar</a></li>
<li>paper_authors: Ludwig Mohr, Ismail Geles, Friedrich Fraundorfer</li>
<li>for: 点云注册问题是计算机视觉中的基本问题，它有很多应用，从3D场景重建到geometry捕获和机器人等。如果有一个适当的初始注册， convential方法如ICP和其变种可以提供可靠的解决方案。但在缺乏适当初始注册和高异常率或小 overlap的情况下，传统的方法具有大量的计算成本和低精度。</li>
<li>methods: 我们提出了一种新的快速和轻量级网络架构，使用注意力机制来在推理时对点描述符进行增强，以便更好地适应特定点云的注册任务。我们使用了全连接图 both within和between点云，让网络理解点云之间的相互关系和重要性，从而使我们的方法具有对于异常值和3D扫描数据的耐性和泛化能力。</li>
<li>results: 我们测试了我们的注册算法在不同的注册和泛化任务中的性能，并提供了运行时间和资源消耗的信息。我们的代码和训练 веса可以在<a target="_blank" rel="noopener" href="https://github.com/mordecaimalignatius/GAFAR/%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/mordecaimalignatius/GAFAR/上下载。</a><details>
<summary>Abstract</summary>
Rigid registration of point clouds is a fundamental problem in computer vision with many applications from 3D scene reconstruction to geometry capture and robotics. If a suitable initial registration is available, conventional methods like ICP and its many variants can provide adequate solutions. In absence of a suitable initialization and in the presence of a high outlier rate or in the case of small overlap though the task of rigid registration still presents great challenges. The advent of deep learning in computer vision has brought new drive to research on this topic, since it provides the possibility to learn expressive feature-representations and provide one-shot estimates instead of depending on time-consuming iterations of conventional robust methods. Yet, the rotation and permutation invariant nature of point clouds poses its own challenges to deep learning, resulting in loss of performance and low generalization capability due to sensitivity to outliers and characteristics of 3D scans not present during network training. In this work, we present a novel fast and light-weight network architecture using the attention mechanism to augment point descriptors at inference time to optimally suit the registration task of the specific point clouds it is presented with. Employing a fully-connected graph both within and between point clouds lets the network reason about the importance and reliability of points for registration, making our approach robust to outliers, low overlap and unseen data. We test the performance of our registration algorithm on different registration and generalization tasks and provide information on runtime and resource consumption. The code and trained weights are available at https://github.com/mordecaimalignatius/GAFAR/.
</details>
<details>
<summary>摘要</summary>
rigid registration of point clouds 是计算机视觉中的基本问题，有很多应用从3D场景重建到geometry capture和机器人等。如果有合适的初始化注册， THEN conventional methods like ICP and its variants can provide adequate solutions。在缺乏合适的初始化和高异常率或小 overlap的情况下，rigid registration 仍然存在很大的挑战。计算机视觉领域的深度学习带来了新的动力，因为它提供了可以学习表达性的特征表示和一次估计而不需要时间consuming的迭代。然而，点云的旋转和排序不变性对深度学习 pose 自己的挑战，导致表现下降和低通用性，因为感知到3D扫描不存在于网络训练时的特征。在这项工作中，我们提出了一种新的快速和轻量级网络架构，使用注意力机制来在推理时增强点云描述符，以便最佳地适应特定的点云注册任务。通过在点云之间和点云内建立完全连接的图，网络可以理解点云中的重要性和可靠性，使我们的方法对异常点和低重 overlap 有很好的Robustness。我们对不同的注册和泛化任务进行了测试，并提供了运行时间和资源消耗的信息。网络和训练过的权重可以在https://github.com/mordecaimalignatius/GAFAR/下载。
</details></li>
</ul>
<hr>
<h2 id="Dual-Arbitrary-Scale-Super-Resolution-for-Multi-Contrast-MRI"><a href="#Dual-Arbitrary-Scale-Super-Resolution-for-Multi-Contrast-MRI" class="headerlink" title="Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI"></a>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02334">http://arxiv.org/abs/2307.02334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jmzhang79/dual-arbnet">https://github.com/jmzhang79/dual-arbnet</a></li>
<li>paper_authors: Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, Yapeng Tian</li>
<li>for: 这个研究的目的是提高医疗影像重建的品质，特别是在受限制的图像系统下，以提高医疗影像重建的精确性和可靠性。</li>
<li>methods: 这个研究使用了一种基于神经网络的多条件超解析技术，named Dual-ArbNet，可以在不固定 scales的情况下进行医疗影像重建。</li>
<li>results: 实验结果显示，这个技术可以在不同的扩展比例下进行医疗影像重建，并且在两个公共的MRI数据集上表现出优于现有的方法。<details>
<summary>Abstract</summary>
Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging (MRI) images from partial measurement is essential to medical imaging research. Benefiting from the diverse and complementary information of multi-contrast MR images in different imaging modalities, multi-contrast Super-Resolution (SR) reconstruction is promising to yield SR images with higher quality. In the medical scenario, to fully visualize the lesion, radiologists are accustomed to zooming the MR images at arbitrary scales rather than using a fixed scale, as used by most MRI SR methods. In addition, existing multi-contrast MRI SR methods often require a fixed resolution for the reference image, which makes acquiring reference images difficult and imposes limitations on arbitrary scale SR tasks. To address these issues, we proposed an implicit neural representations based dual-arbitrary multi-contrast MRI super-resolution method, called Dual-ArbNet. First, we decouple the resolution of the target and reference images by a feature encoder, enabling the network to input target and reference images at arbitrary scales. Then, an implicit fusion decoder fuses the multi-contrast features and uses an Implicit Decoding Function~(IDF) to obtain the final MRI SR results. Furthermore, we introduce a curriculum learning strategy to train our network, which improves the generalization and performance of our Dual-ArbNet. Extensive experiments in two public MRI datasets demonstrate that our method outperforms state-of-the-art approaches under different scale factors and has great potential in clinical practice.
</details>
<details>
<summary>摘要</summary>
限于成像系统，重建医用磁共振成像（MRI）图像从部分测量是医学成像研究中的关键。利用不同和补充的多比特MR图像不同成像模式的多比特超解析（SR）重建，可以获得高质量的SR图像。在医疗场景中，为了完全可见患部，放射医生通常会在自定义比例上缩放MR图像，而不是使用固定比例，这与大多数MRI SR方法不同。此外，现有的多比特MRI SR方法通常需要固定分辨率的参照图像，这会使得获得参照图像困难，并对自定义比例SR任务带来限制。为解决这些问题，我们提出了基于卷积神经网络的双自由多比特MRI超解析方法，称为双自由网络（Dual-ArbNet）。首先，我们通过特征编码器解耦目标和参照图像的分辨率，使得网络可以输入任意比例的目标和参照图像。然后，我们使用卷积神经网络将多比特特征进行融合，并使用卷积神经网络的强化学习函数（IDF）来获得最终的MRI SR结果。此外，我们还提出了训练网络的课程学习策略，以提高我们的Dual-ArbNet的通用性和性能。在两个公共MRI数据集上进行了广泛的实验，我们发现我们的方法在不同的比例因子下表现出优于当前状态的方法，并具有优秀的临床应用潜力。
</details></li>
</ul>
<hr>
<h2 id="MSViT-Dynamic-Mixed-Scale-Tokenization-for-Vision-Transformers"><a href="#MSViT-Dynamic-Mixed-Scale-Tokenization-for-Vision-Transformers" class="headerlink" title="MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers"></a>MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02321">http://arxiv.org/abs/2307.02321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jakob Drachmann Havtorn, Amelie Royer, Tijmen Blankevoort, Babak Ehteshami Bejnordi</li>
<li>for: 提高 ViT 模型的精度和可扩展性，通过自适应地选择最佳的 токен规模来减少计算量。</li>
<li>methods: 提出了一种名为 MSViT 的动态混合缩放tokenization scheme，通过一种名为 gate 的Conditional gating机制来选择每个图像区域的最佳 токен规模，以 dynamically determine the number of tokens per input。</li>
<li>results: 在图像分类和 segmentation 任务上，MSViT 能够 дости得更好的精度-复杂度质量平衡，并且在训练时间和资源上具有较小的开销。<details>
<summary>Abstract</summary>
The input tokens to Vision Transformers carry little semantic meaning as they are defined as regular equal-sized patches of the input image, regardless of its content. However, processing uniform background areas of an image should not necessitate as much compute as dense, cluttered areas. To address this issue, we propose a dynamic mixed-scale tokenization scheme for ViT, MSViT. Our method introduces a conditional gating mechanism that selects the optimal token scale for every image region, such that the number of tokens is dynamically determined per input. The proposed gating module is lightweight, agnostic to the choice of transformer backbone, and trained within a few epochs (e.g., 20 epochs on ImageNet) with little training overhead. In addition, to enhance the conditional behavior of the gate during training, we introduce a novel generalization of the batch-shaping loss. We show that our gating module is able to learn meaningful semantics despite operating locally at the coarse patch-level. We validate MSViT on the tasks of classification and segmentation where it leads to improved accuracy-complexity trade-off.
</details>
<details>
<summary>摘要</summary>
（input tokens to Vision Transformers carry little semantic meaning, as they are defined as regular equal-sized patches of the input image regardless of content; however, processing uniform background areas of an image should not require as much compute as dense, cluttered areas; to address this issue, we propose a dynamic mixed-scale tokenization scheme for ViT, MSViT, which introduces a conditional gating mechanism that selects the optimal token scale for every image region, such that the number of tokens is dynamically determined per input; the proposed gating module is lightweight, agnostic to the choice of transformer backbone, and trained within a few epochs (e.g., 20 epochs on ImageNet) with little training overhead; in addition, to enhance the conditional behavior of the gate during training, we introduce a novel generalization of the batch-shaping loss; we show that our gating module is able to learn meaningful semantics despite operating locally at the coarse patch-level; we validate MSViT on the tasks of classification and segmentation, where it leads to an improved accuracy-complexity trade-off）
</details></li>
</ul>
<hr>
<h2 id="Multi-Scale-Prototypical-Transformer-for-Whole-Slide-Image-Classification"><a href="#Multi-Scale-Prototypical-Transformer-for-Whole-Slide-Image-Classification" class="headerlink" title="Multi-Scale Prototypical Transformer for Whole Slide Image Classification"></a>Multi-Scale Prototypical Transformer for Whole Slide Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02308">http://arxiv.org/abs/2307.02308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saisai Ding, Jun Wang, Juncheng Li, Jun Shi</li>
<li>for: 这篇论文的目的是提出一种基于多例学习的数位组织学分类方法，以提高 computation pathology 中 WSI 的分类精度。</li>
<li>methods: 方法包括一个 prototypical Transformer (PT) 模组和一个多维度特征融合模组 (MFFM)。PT 模组通过融合 prototypical learning 到 Transformer 架构中，将所有例子取代为丛集几何中心，然后通过对丛集中心进行自我注意力机制来更正。MFFM 模组则是使用 MLP-Mixer 来增强丛集中心之间的信息交流。</li>
<li>results: 实验结果显示，提出的 MSPT 方法在两个公共 WSI 数据集上比较所有相比方法都高效，这表明它在 computation pathology 中具有应用潜力。<details>
<summary>Abstract</summary>
Whole slide image (WSI) classification is an essential task in computational pathology. Despite the recent advances in multiple instance learning (MIL) for WSI classification, accurate classification of WSIs remains challenging due to the extreme imbalance between the positive and negative instances in bags, and the complicated pre-processing to fuse multi-scale information of WSI. To this end, we propose a novel multi-scale prototypical Transformer (MSPT) for WSI classification, which includes a prototypical Transformer (PT) module and a multi-scale feature fusion module (MFFM). The PT is developed to reduce redundant instances in bags by integrating prototypical learning into the Transformer architecture. It substitutes all instances with cluster prototypes, which are then re-calibrated through the self-attention mechanism of the Trans-former. Thereafter, an MFFM is proposed to fuse the clustered prototypes of different scales, which employs MLP-Mixer to enhance the information communication between prototypes. The experimental results on two public WSI datasets demonstrate that the proposed MSPT outperforms all the compared algorithms, suggesting its potential applications.
</details>
<details>
<summary>摘要</summary>
整幕图像（WSI）分类是计算 PATHOLOGY 中的一项关键任务。尽管最近在多个实例学习（MIL）中进行了一些进步，但是准确地分类 WSI 仍然是一项挑战，主要因为实例卷积质量差异极大，以及综合多级信息的复杂处理。为此，我们提出了一种新的多级原型变换器（MSPT），它包括原型变换器（PT）模块和多级特征融合模块（MFFM）。PT 模块通过将实例替换为团体原型，并通过 Transformer 架构中的自注意机制来重新准确这些原型。然后，我们提出了一种 MFFM，用于融合不同级别的团体原型，并使用 MLP-Mixer 增强团体原型之间的信息交流。我们在两个公共 WSI 数据集上进行了实验，并证明了我们提出的 MSPT 在所有比较算法中表现出色， suggesting its potential applications。
</details></li>
</ul>
<hr>
<h2 id="Focusing-on-what-to-decode-and-what-to-train-Efficient-Training-with-HOI-Split-Decoders-and-Specific-Target-Guided-DeNoising"><a href="#Focusing-on-what-to-decode-and-what-to-train-Efficient-Training-with-HOI-Split-Decoders-and-Specific-Target-Guided-DeNoising" class="headerlink" title="Focusing on what to decode and what to train: Efficient Training with HOI Split Decoders and Specific Target Guided DeNoising"></a>Focusing on what to decode and what to train: Efficient Training with HOI Split Decoders and Specific Target Guided DeNoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02291">http://arxiv.org/abs/2307.02291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwen Chen, Yingcheng Wang, Keiji Yanai</li>
<li>for: 本研究旨在提高人物互动检测（HOI）任务中一Stage Transformer基于方法的精度和效率。</li>
<li>methods: 我们提出了一种新的一Stage框架（SOV），包括主体解码器、 объек解码器和动词解码器。此外，我们还提出了一种Specific Target Guided（STG）降噪策略，使用学习的物体和动词标签嵌入来帮助训练并加速训练快速。</li>
<li>results: 在一个训练集中，我们的方法（SOV-STG）在三分之一的训练集中达到了最高精度，并且在一个三分之一的训练时间内完成了训练。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/cjw2021/SOV-STG%7D">https://github.com/cjw2021/SOV-STG}</a> 上获取。<details>
<summary>Abstract</summary>
Recent one-stage transformer-based methods achieve notable gains in the Human-object Interaction Detection (HOI) task by leveraging the detection of DETR. However, the current methods redirect the detection target of the object decoder, and the box target is not explicitly separated from the query embeddings, which leads to long and hard training. Furthermore, matching the predicted HOI instances with the ground-truth is more challenging than object detection, simply adapting training strategies from the object detection makes the training more difficult. To clear the ambiguity between human and object detection and share the prediction burden, we propose a novel one-stage framework (SOV), which consists of a subject decoder, an object decoder, and a verb decoder. Moreover, we propose a novel Specific Target Guided (STG) DeNoising strategy, which leverages learnable object and verb label embeddings to guide the training and accelerates the training convergence. In addition, for the inference part, the label-specific information is directly fed into the decoders by initializing the query embeddings from the learnable label embeddings. Without additional features or prior language knowledge, our method (SOV-STG) achieves higher accuracy than the state-of-the-art method in one-third of training epochs. The code is available at \url{https://github.com/cjw2021/SOV-STG}.
</details>
<details>
<summary>摘要</summary>
近期一阶段变换器基本方法在人物交互检测（HOI）任务中获得了显著的进步，利用DETR的检测。然而，现有方法会将 объек 预测器的目标重定向，并且不分离查询嵌入中的盒子目标，这会导致训练变得更加长和更加困难。此外，与物体检测不同，HOI 预测的匹配更加具有挑战性，简单地采用物体检测的训练策略不足以解决这个问题。为了解决人物和物体检测之间的混乱，并分担预测负担，我们提出了一种新的一阶段框架（SOV），该框架包括主体预测器、物体预测器和动词预测器。此外，我们还提出了一种新的特定目标导航（STG）减噪策略，该策略利用可学习的物体和动词标签嵌入来导航训练，并加速训练的收敛。在推断部分，标签特定的信息直接传递给预测器，通过初始化查询嵌入从学习的标签嵌入开始。不需要额外的特征或先验语言知识，我们的方法（SOV-STG）在一个第三个训练翻卷中达到了state-of-the-art的高精度。代码可以在 \url{https://github.com/cjw2021/SOV-STG} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Image-Segmentation-with-Cross-Modality-Vision-Transformers"><a href="#Interactive-Image-Segmentation-with-Cross-Modality-Vision-Transformers" class="headerlink" title="Interactive Image Segmentation with Cross-Modality Vision Transformers"></a>Interactive Image Segmentation with Cross-Modality Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02280">http://arxiv.org/abs/2307.02280</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lik1996/icmformer">https://github.com/lik1996/icmformer</a></li>
<li>paper_authors: Kun Li, George Vosselman, Michael Ying Yang</li>
<li>for: 这 paper 是为了提出一种基于视觉转换器的交互式图像分割方法，以提高图像分割精度和稳定性。</li>
<li>methods: 该 paper 使用了 crossing-modality 视觉转换器，利用了多Modal 数据的相互关系，从而更好地引导学习过程。</li>
<li>results: 实验结果表明，提出的方法在多个benchmark上表现出色，与之前的state-of-the-art模型相比，具有更高的精度和稳定性。<details>
<summary>Abstract</summary>
Interactive image segmentation aims to segment the target from the background with the manual guidance, which takes as input multimodal data such as images, clicks, scribbles, and bounding boxes. Recently, vision transformers have achieved a great success in several downstream visual tasks, and a few efforts have been made to bring this powerful architecture to interactive segmentation task. However, the previous works neglect the relations between two modalities and directly mock the way of processing purely visual information with self-attentions. In this paper, we propose a simple yet effective network for click-based interactive segmentation with cross-modality vision transformers. Cross-modality transformers exploits mutual information to better guide the learning process. The experiments on several benchmarks show that the proposed method achieves superior performance in comparison to the previous state-of-the-art models. The stability of our method in term of avoiding failure cases shows its potential to be a practical annotation tool. The code and pretrained models will be released under https://github.com/lik1996/iCMFormer.
</details>
<details>
<summary>摘要</summary>
中文翻译：互动图像分割target goal是将目标分割自背景，通过手动指导，输入多Modal数据，如图像、点击、笔划和 bounding box。最近，视Transformers在多个下游视图任务中取得了很大成功，而在互动分割任务上，几乎没有尝试将这个强大的架构应用于互动分割任务。然而，之前的工作忽视了两种模式之间的关系，直接模仿了对纯视觉信息的处理方式，使用自注意力。在这篇论文中，我们提出了一种简单 yet effective的网络，用于点击基于的互动分割，cross-modality vision transformers。cross-modality transformers利用两种模式之间的共识来更好地引导学习过程。我们的实验表明，我们提出的方法在多个 benchmark上表现出色，与之前的状态对照模型相比，具有更高的稳定性和可行性。我们的代码和预训练模型将在https://github.com/lik1996/iCMFormer中发布。
</details></li>
</ul>
<hr>
<h2 id="Convolutions-Through-the-Lens-of-Tensor-Networks"><a href="#Convolutions-Through-the-Lens-of-Tensor-Networks" class="headerlink" title="Convolutions Through the Lens of Tensor Networks"></a>Convolutions Through the Lens of Tensor Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02275">http://arxiv.org/abs/2307.02275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Dangel</li>
<li>for: 这篇论文旨在探讨卷积神经网络的分析和推理方法。</li>
<li>methods: 该论文使用tensor网络（TN）来理解卷积神经网络的底层tensor乘法，并通过绘制图像和手动修改它们来实现函数变换、子tensor访问、融合等操作。</li>
<li>results: 该论文通过示例展示了各种自动梯度下降算法和流行的第二个信息近似算法的图像，并提供了基于连接环境的卷积特性变换，以便简化和加速图像评估。此外，该论文还提出了一种基于TN的高效绘制实现，可以加速KFAC变体和实现硬件高效的tensor dropout。<details>
<summary>Abstract</summary>
Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the generalization of theoretical and algorithmic ideas. We provide a new perspective onto convolutions through tensor networks (TNs) which allow reasoning about the underlying tensor multiplications by drawing diagrams, and manipulating them to perform function transformations, sub-tensor access, and fusion. We demonstrate this expressive power by deriving the diagrams of various autodiff operations and popular approximations of second-order information with full hyper-parameter support, batching, channel groups, and generalization to arbitrary convolution dimensions. Further, we provide convolution-specific transformations based on the connectivity pattern which allow to re-wire and simplify diagrams before evaluation. Finally, we probe computational performance, relying on established machinery for efficient TN contraction. Our TN implementation speeds up a recently-proposed KFAC variant up to 4.5x and enables new hardware-efficient tensor dropout for approximate backpropagation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Joint-Hierarchical-Priors-and-Adaptive-Spatial-Resolution-for-Efficient-Neural-Image-Compression"><a href="#Joint-Hierarchical-Priors-and-Adaptive-Spatial-Resolution-for-Efficient-Neural-Image-Compression" class="headerlink" title="Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression"></a>Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02273">http://arxiv.org/abs/2307.02273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Ghorbel, Wassim Hamidouche, Luce Morin</li>
<li>for: 提高神经网络图像压缩（NIC）的性能，使其与传统编码器相当或超越。</li>
<li>methods: 使用Transformer-based transform coding框架，并添加channel-wise auto-regressive prior模型以更好地捕捉全局和局部上下文。还使用一个可学习的扩展模块来提取更加压缩的latent代码。</li>
<li>results: 对比VVC参照编码器（VTM-18.0）和SwinT-ChARM神经编码器，提出的框架在质量压缩和解码复杂度之间取得了显著的质量提升。还进行了模型缩放研究和对象和主观分析，证明了我们的方法的计算效率和性能优势。<details>
<summary>Abstract</summary>
Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.
</details>
<details>
<summary>摘要</summary>
近些时候，神经图像压缩（NIC）的性能已经不断提高，达到或超过了传统编码器的状态对抗水平。尽管有了 significante进步，现有NIC方法仍然基于ConvNet来实现 entropy coding，受到当地连接和建筑偏好的限制，难以模型长距离依赖关系，导致复杂的下去性能不佳的模型和高解码延迟。为了提高 tranformer 基于 transform coding 框架的效率，我们提出了一种能更直观、有效的 channel-wise auto-regressive prior 模型，即 absolute image compression transformer（ICT）。通过 ICT，我们可以从隐藏表示中捕捉全局和局部上下文，更好地参数化归一化后的量化latent。此外，我们采用一个可学习的缩放模块，与一个 ConvNeXt 基于的预/后处理器，准确提取更加 компакт的latent代码，并在重建高质量图像时更好地恢复图像。在多个标准测试集上，我们的框架与VVC参考编码器（VTM-18.0）和神经编码器 SwinT-ChARM 进行了比较，并显示了显著改善了编码效率和解码复杂度之间的平衡。此外，我们还进行了模型缩放研究，以验证我们的方法的计算效率，并进行了一些对象和主观分析，以阐明 AICT 与 SwinT-ChARM 之间的性能差距。
</details></li>
</ul>
<hr>
<h2 id="RanPAC-Random-Projections-and-Pre-trained-Models-for-Continual-Learning"><a href="#RanPAC-Random-Projections-and-Pre-trained-Models-for-Continual-Learning" class="headerlink" title="RanPAC: Random Projections and Pre-trained Models for Continual Learning"></a>RanPAC: Random Projections and Pre-trained Models for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02251">http://arxiv.org/abs/2307.02251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark D. McDonnell, Dong Gong, Amin Parveneh, Ehsan Abbasnejad, Anton van den Hengel</li>
<li>for: 这 paper 的目的是提出一种简洁有效的 continual learning 方法，使得 pre-trained 模型可以在非站ARY 数据流中逐步学习不同的任务，而不会忘记之前学习的内容。</li>
<li>methods: 这 paper 使用了 training-free random projectors 和 class-prototype accumulation 来解决 continual learning 中的忘记问题。具体来说， authors 在 pre-trained 模型的 feature representations 和 output head 之间添加了一层冻结 Random Projection 层，以捕捉Feature之间的交互，提高了类prototype-based continual learning 的 linear separability。此外， authors 还证明了在使用 pre-trained 表示时，需要decorrelating 类prototype 以降低分布差异。</li>
<li>results:  compared to previous methods applied to pre-trained ViT-B&#x2F;16 models, authors 在 seven class-incremental benchmark datasets 上减少了最终的错误率，而不使用任何缓存记忆。specifically, authors 在这些 datasets 上减少了最终错误率的范围为 10% 到 62%。这表明， pre-trained 模型可以在简单、有效、快速的 continual learning 中实现出色的性能，而且还可以避免忘记问题。<details>
<summary>Abstract</summary>
Continual learning (CL) aims to incrementally learn different tasks (such as classification) in a non-stationary data stream without forgetting old ones. Most CL works focus on tackling catastrophic forgetting under a learning-from-scratch paradigm. However, with the increasing prominence of foundation models, pre-trained models equipped with informative representations have become available for various downstream requirements. Several CL methods based on pre-trained models have been explored, either utilizing pre-extracted features directly (which makes bridging distribution gaps challenging) or incorporating adaptors (which may be subject to forgetting). In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Random Projection layer with nonlinear activation between the pre-trained model's feature representations and output head, which captures interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based CL. We also demonstrate the importance of decorrelating the class-prototypes to reduce the distribution disparity when using pre-trained representations. These techniques prove to be effective and circumvent the problem of forgetting for both class- and domain-incremental continual learning. Compared to previous methods applied to pre-trained ViT-B/16 models, we reduce final error rates by between 10\% and 62\% on seven class-incremental benchmark datasets, despite not using any rehearsal memory. We conclude that the full potential of pre-trained models for simple, effective, and fast continual learning has not hitherto been fully tapped.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a concise and effective approach for CL with pre-trained models. Since forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Random Projection layer with nonlinear activation between the pre-trained model's feature representations and output head, which captures interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based CL. We also demonstrate the importance of decorrelating the class-prototypes to reduce the distribution disparity when using pre-trained representations. These techniques prove to be effective and circumvent the problem of forgetting for both class- and domain-incremental continual learning.Compared to previous methods applied to pre-trained ViT-B/16 models, we reduce final error rates by between 10\% and 62\% on seven class-incremental benchmark datasets, despite not using any rehearsal memory. We conclude that the full potential of pre-trained models for simple, effective, and fast continual learning has not hitherto been fully tapped.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Multiple-Instance-Learning-for-Whole-Slide-Image-Classification-A-Good-Instance-Classifier-is-All-You-Need"><a href="#Rethinking-Multiple-Instance-Learning-for-Whole-Slide-Image-Classification-A-Good-Instance-Classifier-is-All-You-Need" class="headerlink" title="Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Good Instance Classifier is All You Need"></a>Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Good Instance Classifier is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02249">http://arxiv.org/abs/2307.02249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linhao Qu, Yingfan Ma, Xiaoyuan Luo, Manning Wang, Zhijian Song</li>
<li>for: This paper focuses on developing an instance-level multiple instance learning (MIL) framework for weakly supervised whole slide image classification.</li>
<li>methods: The proposed method combines contrastive learning and prototype learning to effectively learn instance feature representation and generate accurate pseudo labels.</li>
<li>results: The proposed method achieves powerful performance on four datasets through extensive experiments and visualizations.Here’s the full answer in Simplified Chinese:</li>
<li>for: 本文目标是开发一种基于多个实例学习（MIL）的弱监督整幕图像分类方法，以提高整幕图像分类的精度和效率。</li>
<li>methods: 该方法结合对比学习和原型学习，以便有效地学习实例特征表示，并生成高精度的假标签。</li>
<li>results: 经验和视觉化分析表明，提出的方法在四个数据集上具有强大的表现。<details>
<summary>Abstract</summary>
Weakly supervised whole slide image classification is usually formulated as a multiple instance learning (MIL) problem, where each slide is treated as a bag, and the patches cut out of it are treated as instances. Existing methods either train an instance classifier through pseudo-labeling or aggregate instance features into a bag feature through attention mechanisms and then train a bag classifier, where the attention scores can be used for instance-level classification. However, the pseudo instance labels constructed by the former usually contain a lot of noise, and the attention scores constructed by the latter are not accurate enough, both of which affect their performance. In this paper, we propose an instance-level MIL framework based on contrastive learning and prototype learning to effectively accomplish both instance classification and bag classification tasks. To this end, we propose an instance-level weakly supervised contrastive learning algorithm for the first time under the MIL setting to effectively learn instance feature representation. We also propose an accurate pseudo label generation method through prototype learning. We then develop a joint training strategy for weakly supervised contrastive learning, prototype learning, and instance classifier training. Extensive experiments and visualizations on four datasets demonstrate the powerful performance of our method. Codes will be available.
</details>
<details>
<summary>摘要</summary>
多数批处图像分类通常被形式化为多个实例学习（MIL）问题，每个批处图像被当作袋子，并且从中切出的小块被当作实例。现有方法可以通过假标签或者通过注意机制将实例特征聚合到袋子特征上，然后训练袋子分类器，其中注意分数可以用于实例级分类。然而，由前者构造的假实例标签通常含有很多噪音，而由后者构造的注意分数不够准确，这两者都会影响其性能。在这篇论文中，我们提出了基于对比学习和原型学习的实例级MIL框架，以有效完成实例分类和袋子分类任务。为此，我们提出了在MIL设定下的首次实例级弱监督对准学习算法，以有效地学习实例特征表示。我们还提出了一种准确的假标签生成方法。然后，我们开发了一种共同训练策略，用于弱监督对准学习、原型学习和实例分类器训练。广泛的实验和视觉化在四个数据集上，证明了我们的方法的强大性能。代码将可用。
</details></li>
</ul>
<hr>
<h2 id="S3C-Self-Supervised-Stochastic-Classifiers-for-Few-Shot-Class-Incremental-Learning"><a href="#S3C-Self-Supervised-Stochastic-Classifiers-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="S3C: Self-Supervised Stochastic Classifiers for Few-Shot Class-Incremental Learning"></a>S3C: Self-Supervised Stochastic Classifiers for Few-Shot Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02246">http://arxiv.org/abs/2307.02246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jayatejak/s3c">https://github.com/jayatejak/s3c</a></li>
<li>paper_authors: Jayateja Kalla, Soma Biswas</li>
<li>for: 这个研究目的是提出一种可以逐步学习新类的方法，并且不会忘记之前学习过的类别。</li>
<li>methods: 这个方法使用了一种自我监督的随机分类器（S3C），它不仅可以减轻新类的欠拓范围问题，而且可以避免在增量阶段中忘记之前学习过的类别。</li>
<li>results: 在三个标准资料集上进行了广泛的评估，并且使用多个评估指标显示了这个方法的有效性。此外，还进行了两个实际的应用情况评估，包括不同新类的标签数量和基础类别的数量较少的情况，并证明了这个方法在这些情况下表现出色。<details>
<summary>Abstract</summary>
Few-shot class-incremental learning (FSCIL) aims to learn progressively about new classes with very few labeled samples, without forgetting the knowledge of already learnt classes. FSCIL suffers from two major challenges: (i) over-fitting on the new classes due to limited amount of data, (ii) catastrophically forgetting about the old classes due to unavailability of data from these classes in the incremental stages. In this work, we propose a self-supervised stochastic classifier (S3C) to counter both these challenges in FSCIL. The stochasticity of the classifier weights (or class prototypes) not only mitigates the adverse effect of absence of large number of samples of the new classes, but also the absence of samples from previously learnt classes during the incremental steps. This is complemented by the self-supervision component, which helps to learn features from the base classes which generalize well to unseen classes that are encountered in future, thus reducing catastrophic forgetting. Extensive evaluation on three benchmark datasets using multiple evaluation metrics show the effectiveness of the proposed framework. We also experiment on two additional realistic scenarios of FSCIL, namely where the number of annotated data available for each of the new classes can be different, and also where the number of base classes is much lesser, and show that the proposed S3C performs significantly better than the state-of-the-art for all these challenging scenarios.
</details>
<details>
<summary>摘要</summary>
《几个步骤类增长学习》（Few-shot Class-incremental Learning，FSCIL）的目标是逐渐学习新类，使用非常少的标注样本。然而，FSCIL面临两大挑战：（1）新类的过拟合由于数据有限，（2）在增量阶段缺乏过去类的数据，导致忘记已经学习的类。在这种情况下，我们提出了一种自编程随机分类器（S3C），以解决FSCIL中这两个挑战。S3C的权重随机性不仅减轻新类缺乏大量样本的影响，还减轻过去类缺乏样本的影响。此外，我们还添加了一个自编程组件，帮助学习基础类的特征，以便在未来遇到未经过学习的类时，减少忘记。我们对三个标准数据集进行了广泛的评估，并使用多个评价指标，结果表明我们的提案效果显著。此外，我们还在不同的实际情景下进行了进一步的实验，包括每个新类的标注数据有不同数量，以及基础类的数量远少于新类的情况，并证明了S3C在这些挑战情况下表现更好。
</details></li>
</ul>
<hr>
<h2 id="Set-Learning-for-Accurate-and-Calibrated-Models"><a href="#Set-Learning-for-Accurate-and-Calibrated-Models" class="headerlink" title="Set Learning for Accurate and Calibrated Models"></a>Set Learning for Accurate and Calibrated Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02245">http://arxiv.org/abs/2307.02245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lukasmut/oko">https://github.com/lukasmut/oko</a></li>
<li>paper_authors: Lukas Muttenthaler, Robert A. Vandermeulen, Qiuyi Zhang, Thomas Unterthiner, Klaus-Robert Müller</li>
<li>for: 本文是为了解决机器学习模型过于自信和轻量级别困难 Accounting for when applying standard empirical risk minimization.</li>
<li>methods: 本文提出了一种新的方法，即odd-$k$-out learning（OKO），它将cross-entropy error для集合而不是单个例子进行最小化，从而使模型能够捕捉数据示例之间的相关性，并达到更高的准确率和准确性，特别是在有限的训练数据和类别不均衡情况下。</li>
<li>results: 经过理论证明和广泛的实验分析，OKO方法能够更好地准确化模型，而且在使用硬标签和抛弃任何额外参数调整时，OKO方法仍然能够提供更好的准确性。<details>
<summary>Abstract</summary>
Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the trained model can be applied to single examples at inference time, without introducing significant run-time overhead or architecture changes.
</details>
<details>
<summary>摘要</summary>
模型过置信和轻度误差是机器学习中的常见问题，难以通过标准的实际风险最小化来补偿。在这项工作中，我们提出了一种新的方法，称为偶数-$k$-学习（OKO），它为集合而非单个例子 minimum 交叉熵错误。这种方法自然地让模型捕捉数据示例之间的相关性，并达到更高的准确率和评估，特别是在有限的训练数据和类别不均衡的情况下。 surprisingly，OKO  часто提供更好的评估，即使在使用硬标签和遗漏任何额外参数调整时。我们提供了理论基础，证明 OKO 自然地提供更好的评估，并进行了广泛的实验分析，证实我们的理论发现。我们强调的是，OKO 是一种通用的框架，可以轻松地适应多种情况，并且训练好的模型可以在推理时应用于单个示例，无需增加重要的运行时 overhead 或者架构变化。
</details></li>
</ul>
<hr>
<h2 id="Source-Identification-A-Self-Supervision-Task-for-Dense-Prediction"><a href="#Source-Identification-A-Self-Supervision-Task-for-Dense-Prediction" class="headerlink" title="Source Identification: A Self-Supervision Task for Dense Prediction"></a>Source Identification: A Self-Supervision Task for Dense Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02238">http://arxiv.org/abs/2307.02238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Chen, Subhradeep Kayal, Marleen de Bruijne</li>
<li>for: 这个论文旨在提出一种新的自监督任务，即源标识（SI）任务，用于提高数据驱动方法的表征学习效果。</li>
<li>methods: 该论文使用了自然图像生成方法生成了多个源图像的混合图像，然后让网络重建原始图像，以验证网络的能力。</li>
<li>results: 实验结果表明，提出的SI任务在某些 dense predictions 任务上比传统的自监督任务表现更好，包括填充、像素排序、INTENSITY SHIFT 和超分辨率等。此外，在不同类型图像混合中，使用不同病人图像的混合得到最佳效果。<details>
<summary>Abstract</summary>
The paradigm of self-supervision focuses on representation learning from raw data without the need of labor-consuming annotations, which is the main bottleneck of current data-driven methods. Self-supervision tasks are often used to pre-train a neural network with a large amount of unlabeled data and extract generic features of the dataset. The learned model is likely to contain useful information which can be transferred to the downstream main task and improve performance compared to random parameter initialization. In this paper, we propose a new self-supervision task called source identification (SI), which is inspired by the classic blind source separation problem. Synthetic images are generated by fusing multiple source images and the network's task is to reconstruct the original images, given the fused images. A proper understanding of the image content is required to successfully solve the task. We validate our method on two medical image segmentation tasks: brain tumor segmentation and white matter hyperintensities segmentation. The results show that the proposed SI task outperforms traditional self-supervision tasks for dense predictions including inpainting, pixel shuffling, intensity shift, and super-resolution. Among variations of the SI task fusing images of different types, fusing images from different patients performs best.
</details>
<details>
<summary>摘要</summary>
自我监督框架强调 Raw 数据 学习 representation 无需劳动 INTENSIVE 注解，这是当前数据驱动方法的主要瓶颈。自我监督任务经常用于预训练神经网络大量无注释数据，并提取数据集的通用特征。学习的模型可能包含有用信息，可以转移到下游主任务，提高与随机参数初始化相比的性能。在本文中，我们提出了一种新的自我监督任务，即源标识（SI），它是基于经典的盲源分离问题的。通过混合多个源图像生成synthetic图像，并让网络在混合图像中重建原始图像，需要成功解决这个任务的良好图像内容理解。我们验证了我们的方法在两个医学影像分割任务中，即脑肿瘤分割和白 matter 高敏敏分割。结果表明，提议的 SI 任务超过传统自我监督任务 dense 预测，包括填充、像素拼接、Intensity 变化和超分辨率。其中，将不同类型图像混合的 variations 中，将不同病人图像混合得到最好的性能。
</details></li>
</ul>
<hr>
<h2 id="Direct-segmentation-of-brain-white-matter-tracts-in-diffusion-MRI"><a href="#Direct-segmentation-of-brain-white-matter-tracts-in-diffusion-MRI" class="headerlink" title="Direct segmentation of brain white matter tracts in diffusion MRI"></a>Direct segmentation of brain white matter tracts in diffusion MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02223">http://arxiv.org/abs/2307.02223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamza Kebiri, Ali Gholipour, Meritxell Bach Cuadra, Davood Karimi<br>for:这篇论文旨在提供一种新的深度学习方法，用于非侵入性地从 diffusion MRI 数据中直接分割白 matter 轨迹，并且可以避免中间计算的错误。methods:这种新方法使用深度学习算法，直接从 diffusion MRI 数据中提取白 matter 轨迹的特征，不需要进行中间计算。results:实验表明，这种方法可以达到与现有方法相当的分割精度（mean Dice Similarity Coefficient 为 0.826），并且在受损数据下表现更好，可以满足许多重要的临床和科学应用。<details>
<summary>Abstract</summary>
The brain white matter consists of a set of tracts that connect distinct regions of the brain. Segmentation of these tracts is often needed for clinical and research studies. Diffusion-weighted MRI offers unique contrast to delineate these tracts. However, existing segmentation methods rely on intermediate computations such as tractography or estimation of fiber orientation density. These intermediate computations, in turn, entail complex computations that can result in unnecessary errors. Moreover, these intermediate computations often require dense multi-shell measurements that are unavailable in many clinical and research applications. As a result, current methods suffer from low accuracy and poor generalizability. Here, we propose a new deep learning method that segments these tracts directly from the diffusion MRI data, thereby sidestepping the intermediate computation errors. Our experiments show that this method can achieve segmentation accuracy that is on par with the state of the art methods (mean Dice Similarity Coefficient of 0.826). Compared with the state of the art, our method offers far superior generalizability to undersampled data that are typical of clinical studies and to data obtained with different acquisition protocols. Moreover, we propose a new method for detecting inaccurate segmentations and show that it is more accurate than standard methods that are based on estimation uncertainty quantification. The new methods can serve many critically important clinical and scientific applications that require accurate and reliable non-invasive segmentation of white matter tracts.
</details>
<details>
<summary>摘要</summary>
脑白物质由一组轨迹组成，连接脑各区域。 segmentation这些轨迹经常需要临床和研究实验室中。扩散束MRI提供了唯一的对比，以定义这些轨迹。然而，现有的分割方法依赖于中间计算，如轨迹学或纤维方向密度的估计。这些中间计算会导致 Complex errors。此外，这些中间计算通常需要密集的多层测量，这些测量在临床和研究应用中不可得。因此，当前的方法受到低准确率和poor generalizability的限制。我们提出了一种新的深度学习方法，直接从扩散MRI数据中分割白物质轨迹，从而割除中间计算的错误。我们的实验表明，这种方法可以与状态的方法（mean Dice Similarity Coefficient of 0.826）准确性相似。相比之下，我们的方法具有更高的普适性，可以在临床研究中常见的压缩数据上进行分割，以及数据获取不同的探测协议。此外，我们还提出了一种新的方法来检测不准确的分割，并证明它在标准方法基于估计不确定性量化上更为准确。这些新方法可以满足许多临床和科学应用中需要准确和可靠的非侵入式白物质轨迹分割。
</details></li>
</ul>
<hr>
<h2 id="Object-Recognition-System-on-a-Tactile-Device-for-Visually-Impaired"><a href="#Object-Recognition-System-on-a-Tactile-Device-for-Visually-Impaired" class="headerlink" title="Object Recognition System on a Tactile Device for Visually Impaired"></a>Object Recognition System on a Tactile Device for Visually Impaired</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02211">http://arxiv.org/abs/2307.02211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Souayah Abdelkader, Mokretar Kraroubi Abderrahmene, Slimane Larabi</li>
<li>for: developing a device that facilitates communication between individuals with visual impairments and their surroundings</li>
<li>methods: using an object detection model implemented on a Raspberry Pi, which is connected to a specifically designed tactile device to provide auditory feedback of object identification</li>
<li>results: successful demonstration of the device’s effectiveness in scene understanding, including static or dynamic objects and screen contents such as TVs, computers, and mobile phones.<details>
<summary>Abstract</summary>
People with visual impairments face numerous challenges when interacting with their environment. Our objective is to develop a device that facilitates communication between individuals with visual impairments and their surroundings. The device will convert visual information into auditory feedback, enabling users to understand their environment in a way that suits their sensory needs. Initially, an object detection model is selected from existing machine learning models based on its accuracy and cost considerations, including time and power consumption. The chosen model is then implemented on a Raspberry Pi, which is connected to a specifically designed tactile device. When the device is touched at a specific position, it provides an audio signal that communicates the identification of the object present in the scene at that corresponding position to the visually impaired individual. Conducted tests have demonstrated the effectiveness of this device in scene understanding, encompassing static or dynamic objects, as well as screen contents such as TVs, computers, and mobile phones.
</details>
<details>
<summary>摘要</summary>
人们 WITH visual impairments 面临着许多挑战 WHEN interacting with their environment. Our objective is to develop a device that facilitates communication between individuals with visual impairments AND their surroundings. The device will convert visual information into auditory feedback, enabling users to understand their environment in a way that suits their sensory needs.Initially, an object detection model IS selected FROM existing machine learning models based on its accuracy AND cost considerations, including time AND power consumption. The chosen model IS then implemented on a Raspberry Pi, which IS connected to a specifically designed tactile device. When the device IS touched at a specific position, it provides an audio signal that communicates the identification of the object present in the scene at that corresponding position to the visually impaired individual.Conducted tests have demonstrated the effectiveness of this device in scene understanding, encompassing static OR dynamic objects, as well as screen contents such as TVs, computers, AND mobile phones.
</details></li>
</ul>
<hr>
<h2 id="Neural-Fields-for-Interactive-Visualization-of-Statistical-Dependencies-in-3D-Simulation-Ensembles"><a href="#Neural-Fields-for-Interactive-Visualization-of-Statistical-Dependencies-in-3D-Simulation-Ensembles" class="headerlink" title="Neural Fields for Interactive Visualization of Statistical Dependencies in 3D Simulation Ensembles"></a>Neural Fields for Interactive Visualization of Statistical Dependencies in 3D Simulation Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02203">http://arxiv.org/abs/2307.02203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Farokhmanesh, Kevin Höhlein, Christoph Neuhauser, Rüdiger Westermann</li>
<li>for: 这篇论文是为了开发一种可以高效地表示和重建物理变量之间的统计依赖关系的神经网络。</li>
<li>methods: 论文使用了约束优化和缺省搜索来学习并重建非线性依赖关系。</li>
<li>results: 论文可以减少计算和存储需求，并在 GPU 加速的直接体积渲染器中实时显示所有依赖关系。<details>
<summary>Abstract</summary>
We present the first neural network that has learned to compactly represent and can efficiently reconstruct the statistical dependencies between the values of physical variables at different spatial locations in large 3D simulation ensembles. Going beyond linear dependencies, we consider mutual information as a measure of non-linear dependence. We demonstrate learning and reconstruction with a large weather forecast ensemble comprising 1000 members, each storing multiple physical variables at a 250 x 352 x 20 simulation grid. By circumventing compute-intensive statistical estimators at runtime, we demonstrate significantly reduced memory and computation requirements for reconstructing the major dependence structures. This enables embedding the estimator into a GPU-accelerated direct volume renderer and interactively visualizing all mutual dependencies for a selected domain point.
</details>
<details>
<summary>摘要</summary>
我们提出了第一个神经网络，可以压缩地表示并高效地重建物理变量之间的统计依赖关系在大型3D模拟集中。我们超越了线性依赖，使用共识信息作为非线性依赖的度量。我们在1000个天气预测集中，每个集包含多个物理变量，并在250x352x20的模拟网格上存储。通过避免在运行时计算昂贵的统计估计器，我们显著减少了内存和计算需求，从而实现嵌入估计器到GPU加速的直接体积渲染器中，并在选择的域点上交互式地Visualize所有的相互关系。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-AI-systems-under-uncertain-ground-truth-a-case-study-in-dermatology"><a href="#Evaluating-AI-systems-under-uncertain-ground-truth-a-case-study-in-dermatology" class="headerlink" title="Evaluating AI systems under uncertain ground truth: a case study in dermatology"></a>Evaluating AI systems under uncertain ground truth: a case study in dermatology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02191">http://arxiv.org/abs/2307.02191</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Stutz, Ali Taylan Cemgil, Abhijit Guha Roy, Tatiana Matejovicova, Melih Barsbey, Patricia Strachan, Mike Schaekermann, Jan Freyberg, Rajeev Rikhye, Beverly Freeman, Javier Perez Matos, Umesh Telang, Dale R. Webster, Yuan Liu, Greg S. Corrado, Yossi Matias, Pushmeet Kohli, Yun Liu, Arnaud Doucet, Alan Karthikesalingam</li>
<li>for: 这篇论文旨在探讨人工智能系统在医疗领域的评估方法，以及评估结果的不确定性的影响。</li>
<li>methods: 该论文提出了一种基于统计学模型的 annotation uncertainty 度量方法，以及一种 uncertainty-adjusted 性能评估方法。</li>
<li>results: 研究发现，使用现有的 deterministic adjudication 方法可能会严重低估实际性能，而使用统计学模型可以更好地评估 annotation uncertainty 并提供不确定性估计。<details>
<summary>Abstract</summary>
For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain. However, this is actually not the case and the ground truth may be uncertain. Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance. To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information. This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging. In contrast, we propose a framework where aggregation is done using a statistical model. Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities, representing distributions over classes in a classification setting, subject to a hyper-parameter encoding annotator reliability. Based on this model, we propose a metric for measuring annotation uncertainty and provide uncertainty-adjusted metrics for performance evaluation. We present a case study applying our framework to skin condition classification from images where annotations are provided in the form of differential diagnoses. The deterministic adjudication process called inverse rank normalization (IRN) from previous work ignores ground truth uncertainty in evaluation. Instead, we present two alternative statistical models: a probabilistic version of IRN and a Plackett-Luce-based model. We find that a large portion of the dataset exhibits significant ground truth uncertainty and standard IRN-based evaluation severely over-estimates performance without providing uncertainty estimates.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为了安全，医疗AI系统在部署前都会进行严格的评估，将预测结果与假设为确定的地面 truth进行比较。然而，实际上地面 truth 可能存在uncertainty。这种uncertainty 在标准AI模型评估中很多被忽略，可能导致未来性能的估计过高。为了避免这种情况，我们会测量地面 truth 的影响，假设地面 truth 可以分解为两个主要组成部分：注释uncertainty 和限制的观察信息的 uncertainty。这种地面 truth 的uncertainty 在评估地面 truth 时被忽略，通常通过 deterministic aggregation of annotations，例如多数投票或平均值来进行。相比之下，我们提出了一种使用统计模型进行集成注释的框架。具体来说，我们将注释集成作为 posterior inference of so-called plausibilities，表示分类 Setting 中的分布，受到 annotator reliability 的 hyperparameter encoding。基于这个模型，我们提出了一个度量注释uncertainty的metric，并提供了不确定度调整的性能评估方法。我们在皮肤状况分类从图像中的案例研究中应用了我们的框架。在 previous work 中使用的 inverse rank normalization (IRN) 的 deterministic adjudication process 忽略了地面 truth 的 uncertainty，而我们则提出了两种统计模型：一种是probabilistic version of IRN，另一种是 Plackett-Luce-based model。我们发现大量数据存在显著的地面 truth uncertainty，标准 IRN-based 评估严重过估了性能，没有提供不确定度估计。
</details></li>
</ul>
<hr>
<h2 id="DiffFlow-A-Unified-SDE-Framework-for-Score-Based-Diffusion-Models-and-Generative-Adversarial-Networks"><a href="#DiffFlow-A-Unified-SDE-Framework-for-Score-Based-Diffusion-Models-and-Generative-Adversarial-Networks" class="headerlink" title="DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and Generative Adversarial Networks"></a>DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02159">http://arxiv.org/abs/2307.02159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwei Zhang, Han Shi, Jincheng Yu, Enze Xie, Zhenguo Li</li>
<li>for: 这个论文的目的是提出一种统一的概率理论框架，用于描述逻辑推理模型和生成随机模型。</li>
<li>methods: 该论文使用的方法包括：1) 提出一种新的泛化概率方程（DiffFlow），用于描述逻辑推理模型和生成随机模型的学习动态；2) 通过调整权重来实现逻辑推理模型和生成随机模型之间的平滑过渡，保持极大似然估计的可行性。</li>
<li>results: 该论文的结果包括：1) 提出了一种新的泛化概率方程（DiffFlow），可以具有高速样本生成和高质量样本生成的优点；2) 通过调整权重来实现逻辑推理模型和生成随机模型之间的平滑过渡，保持极大似然估计的可行性。<details>
<summary>Abstract</summary>
Generative models can be categorized into two types: explicit generative models that define explicit density forms and allow exact likelihood inference, such as score-based diffusion models (SDMs) and normalizing flows; implicit generative models that directly learn a transformation from the prior to the data distribution, such as generative adversarial nets (GANs). While these two types of models have shown great success, they suffer from respective limitations that hinder them from achieving fast sampling and high sample quality simultaneously. In this paper, we propose a unified theoretic framework for SDMs and GANs. We shown that: i) the learning dynamics of both SDMs and GANs can be described as a novel SDE named Discriminator Denoising Diffusion Flow (DiffFlow) where the drift can be determined by some weighted combinations of scores of the real data and the generated data; ii) By adjusting the relative weights between different score terms, we can obtain a smooth transition between SDMs and GANs while the marginal distribution of the SDE remains invariant to the change of the weights; iii) we prove the asymptotic optimality and maximal likelihood training scheme of the DiffFlow dynamics; iv) under our unified theoretic framework, we introduce several instantiations of the DiffFLow that provide new algorithms beyond GANs and SDMs with exact likelihood inference and have potential to achieve flexible trade-off between high sample quality and fast sampling speed.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese生成模型可以分为两类：Explicit生成模型，其定义Explicit概率形式，并允许精确的风险推断，如投 diffusion模型（SDM）和生成 adversarial网（GAN）；Implicit生成模型，直接学习变换从 priors 到数据分布。而这两种类型的模型各有其限制，不能同时实现高质量样本和快速采样。在这篇论文中，我们提出一个统一的理论框架，用于SDM和GAN。我们证明：i) SDM和GAN的学习动态可以用一种新的SDE named Discriminator Denoising Diffusion Flow（DiffFlow）来描述，其拥有的拥有权重的混合的分数可以用来描述实际数据和生成数据的关系；ii) 通过调整不同的分数项的相对权重，可以在SDM和GAN之间进行缓和转换，而保持样本分布的偏微分不变；iii) 我们证明DiffFlow的动态是最优的和最大可信度训练方案；iv）在我们的统一理论框架下，我们引入了一些DiffFlow的实现，提供了新的算法，其中包括高可信度采样和快速采样，并且可以实现flexible的质量和速度之间的平衡。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Auto-Encoders-of-Merge-Trees-and-Persistence-Diagrams"><a href="#Wasserstein-Auto-Encoders-of-Merge-Trees-and-Persistence-Diagrams" class="headerlink" title="Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)"></a>Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02509">http://arxiv.org/abs/2307.02509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahieu Pont, Julien Tierny</li>
<li>for: 这个论文是为了提出一种计算框架，用于在 Wasserstein  метри空间中进行merge trees auto-encoding。</li>
<li>methods: 该方法使用 neural network  architecture，并在每层网络中Explicitly manipulate merge trees on their associated metric space，从而实现更高的准确率和可解释性。</li>
<li>results: 对于公共ensemble数据进行了广泛的实验，并证明了其计算效率，typically in the orders of minutes on average。此外，该方法还可以应用于数据压缩和维度减少等问题。<details>
<summary>Abstract</summary>
This paper presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts [65] at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding [65]. First, we apply MT-WAE to data reduction and reliably compress merge trees by concisely representing them with their coordinates in the final layer of our auto-encoder. Second, we document an application to dimensionality reduction, by exploiting the latent space of our auto-encoder, for the visual analysis of ensemble data. We illustrate the versatility of our framework by introducing two penalty terms, to help preserve in the latent space both the Wasserstein distances between merge trees, as well as their clusters. In both applications, quantitative experiments assess the relevance of our framework. Finally, we provide a C++ implementation that can be used for reproducibility.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Harmonizing-Feature-Attributions-Across-Deep-Learning-Architectures-Enhancing-Interpretability-and-Consistency"><a href="#Harmonizing-Feature-Attributions-Across-Deep-Learning-Architectures-Enhancing-Interpretability-and-Consistency" class="headerlink" title="Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency"></a>Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02150">http://arxiv.org/abs/2307.02150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abdul Kadir, Gowtham Krishna Addluri, Daniel Sonntag</li>
<li>for: 本研究旨在评估特征归属方法的普适性，以及如何使用这些方法来协调多种深度学习模型的本地解释。</li>
<li>methods: 本研究使用了多种深度学习架构，如卷积神经网络（CNN）和视transformer，以评估特征归属方法的普适性。</li>
<li>results: 研究发现，使用不同深度学习架构的模型可以共同使用特征归属方法来提供本地解释，并且这些特征可以在多种模型上被协调。这些结果表明，使用协调的特征归属方法可以提高机器学习模型的解释性和可靠性。<details>
<summary>Abstract</summary>
Ensuring the trustworthiness and interpretability of machine learning models is critical to their deployment in real-world applications. Feature attribution methods have gained significant attention, which provide local explanations of model predictions by attributing importance to individual input features. This study examines the generalization of feature attributions across various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers. We aim to assess the feasibility of utilizing a feature attribution method as a future detector and examine how these features can be harmonized across multiple models employing distinct architectures but trained on the same data distribution. By exploring this harmonization, we aim to develop a more coherent and optimistic understanding of feature attributions, enhancing the consistency of local explanations across diverse deep-learning models. Our findings highlight the potential for harmonized feature attribution methods to improve interpretability and foster trust in machine learning applications, regardless of the underlying architecture.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:确保机器学习模型的可靠性和可读性是应用中的关键。特征归因方法在不同的深度学习架构上得到了广泛的关注，例如卷积神经网络（CNN）和视觉变换器。本研究检验了不同深度学习架构之间的特征归因方法的一致性，以评估将这些特征归因方法应用于未来的检测器是否可行。我们还希望通过协调这些特征归因方法，从多个模型使用不同的架构但是在同一个数据分布上进行训练来增强当地解释的一致性。通过这种协调，我们希望能够提高机器学习应用中的解释性和信任度，无论下面的架构是什么。
</details></li>
</ul>
<hr>
<h2 id="Compound-Attention-and-Neighbor-Matching-Network-for-Multi-contrast-MRI-Super-resolution"><a href="#Compound-Attention-and-Neighbor-Matching-Network-for-Multi-contrast-MRI-Super-resolution" class="headerlink" title="Compound Attention and Neighbor Matching Network for Multi-contrast MRI Super-resolution"></a>Compound Attention and Neighbor Matching Network for Multi-contrast MRI Super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02148">http://arxiv.org/abs/2307.02148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Chen, Sirui Wu, Shuai Wang, Zhongsen Li, Jia Yang, Huifeng Yao, Xiaomeng Li, Xiaolei Song</li>
<li>for: 这个论文旨在提出一种新的多感知磁共振成像超分辨（SR）方法，用于提高多感知磁共振成像的图像质量。</li>
<li>methods: 该方法使用了自定义的compound-attention和邻居匹配机制（CANM-Net），通过同时capture spatial和channel维度的依赖关系，以及在邻居级别进行特征匹配和融合，从而实现更高的SR性能。</li>
<li>results: 实验结果表明，CANM-Net在IXI、fastMRI和实际扫描 dataset上的SR任务中表现出色，超过了现有的状态对方法。此外，对于不完美地注册的参考和下降图像的情况，CANM-Net仍然能够保持良好的性能，这说明了其在临床应用中的可靠性。<details>
<summary>Abstract</summary>
Multi-contrast magnetic resonance imaging (MRI) reflects information about human tissue from different perspectives and has many clinical applications. By utilizing the complementary information among different modalities, multi-contrast super-resolution (SR) of MRI can achieve better results than single-image super-resolution. However, existing methods of multi-contrast MRI SR have the following shortcomings that may limit their performance: First, existing methods either simply concatenate the reference and degraded features or exploit global feature-matching between them, which are unsuitable for multi-contrast MRI SR. Second, although many recent methods employ transformers to capture long-range dependencies in the spatial dimension, they neglect that self-attention in the channel dimension is also important for low-level vision tasks. To address these shortcomings, we proposed a novel network architecture with compound-attention and neighbor matching (CANM-Net) for multi-contrast MRI SR: The compound self-attention mechanism effectively captures the dependencies in both spatial and channel dimension; the neighborhood-based feature-matching modules are exploited to match degraded features and adjacent reference features and then fuse them to obtain the high-quality images. We conduct experiments of SR tasks on the IXI, fastMRI, and real-world scanning datasets. The CANM-Net outperforms state-of-the-art approaches in both retrospective and prospective experiments. Moreover, the robustness study in our work shows that the CANM-Net still achieves good performance when the reference and degraded images are imperfectly registered, proving good potential in clinical applications.
</details>
<details>
<summary>摘要</summary>
多模式磁共振成像（MRI）可以从不同的角度提取人体组织信息，有很多临床应用。通过利用不同模式之间的补充信息，多模式超分辨（SR）的MRI可以比单图SR更好。然而，现有的多模式MRI SR方法存在以下缺点，可能会限制其性能：首先，现有的方法可能会简单地将参考和降解特征 concatenate 或者利用全局特征匹配，这些方法不适合多模式MRI SR。其次，许多现代方法使用 transformers 捕捉在空间维度上的长距离依赖关系，但它们忽略了频道维度上的自我注意力对低级视觉任务的重要性。为了解决这些缺点，我们提出了一种新的网络架构，即 Compound-Attention and Neighbor Matching 网络（CANM-Net），用于多模式MRI SR：Compound self-attention 机制可以有效捕捉空间和频道维度上的依赖关系; 邻域特征匹配模块可以将降解特征和相邻参考特征匹配并融合以获得高质量图像。我们在 IXI、fastMRI 和实际扫描数据集上进行 SR 任务的实验，CANM-Net 超过了当前的状态艺术方法。此外，我们的Robustness 研究表明，CANM-Net 在参考和降解图像不完全匹配时仍能达到良好性能，证明了其在临床应用中的好可能性。
</details></li>
</ul>
<hr>
<h2 id="Prompting-Diffusion-Representations-for-Cross-Domain-Semantic-Segmentation"><a href="#Prompting-Diffusion-Representations-for-Cross-Domain-Semantic-Segmentation" class="headerlink" title="Prompting Diffusion Representations for Cross-Domain Semantic Segmentation"></a>Prompting Diffusion Representations for Cross-Domain Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02138">http://arxiv.org/abs/2307.02138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Gong, Martin Danelljan, Han Sun, Julio Delgado Mangas, Luc Van Gool</li>
<li>for: 本研究探讨了 diffusion-pretrained 表示的适用范围，以及如何使用这些表示进行 semantic segmentation 任务。</li>
<li>methods: 本研究使用了 diffusion-pretrained 模型，并 investigate 了如何使用模型的特点（即输入提示）来进一步提高其跨频变性性能。</li>
<li>results: 研究发现 diffusion-pretraining 可以很好地在新频变性环境下进行 semantic segmentation，并且可以使用 simple  yet effective 的 test-time domain adaptation 方法来进一步提高性能。<details>
<summary>Abstract</summary>
While originally designed for image generation, diffusion models have recently shown to provide excellent pretrained feature representations for semantic segmentation. Intrigued by this result, we set out to explore how well diffusion-pretrained representations generalize to new domains, a crucial ability for any representation. We find that diffusion-pretraining achieves extraordinary domain generalization results for semantic segmentation, outperforming both supervised and self-supervised backbone networks. Motivated by this, we investigate how to utilize the model's unique ability of taking an input prompt, in order to further enhance its cross-domain performance. We introduce a scene prompt and a prompt randomization strategy to help further disentangle the domain-invariant information when training the segmentation head. Moreover, we propose a simple but highly effective approach for test-time domain adaptation, based on learning a scene prompt on the target domain in an unsupervised manner. Extensive experiments conducted on four synthetic-to-real and clear-to-adverse weather benchmarks demonstrate the effectiveness of our approaches. Without resorting to any complex techniques, such as image translation, augmentation, or rare-class sampling, we set a new state-of-the-art on all benchmarks. Our implementation will be publicly available at \url{https://github.com/ETHRuiGong/PTDiffSeg}.
</details>
<details>
<summary>摘要</summary>
原本设计用于图像生成的扩散模型，最近显示出了优秀的预训练特征表示，用于Semantic segmentation。我们对这个结果感到感兴趣，于是决定了探索 diffusion-预训练的特征如何在新领域中 generalized。我们发现，diffusion-预训练可以在Semantic segmentation中获得杰出的领域泛化结果，超过了 both 监督和自监督的底层网络。这种结果使我们对模型的唯一能力——接受输入提示，以提高它在不同领域的跨领域性表示。我们引入了场景提示和提示随机策略，以帮助更好地分离领域无关的信息。此外，我们提出了一种简单 yet 高效的测试时领域适应方法，基于在目标领域上无监督学习场景提示。我们的实验结果在四个 synthetic-to-real 和 clear-to-adverse weather benchmark上进行了广泛的测试，并证明了我们的方法的有效性。无需使用复杂的技术，如图像翻译、扩展、罕见类采样，我们设置了新的状态之册在所有benchmark上。我们的实现将在 GitHub 上公开，可以在 \url{https://github.com/ETHRuiGong/PTDiffSeg} 中找到。
</details></li>
</ul>
<hr>
<h2 id="How-Deep-Neural-Networks-Learn-Compositional-Data-The-Random-Hierarchy-Model"><a href="#How-Deep-Neural-Networks-Learn-Compositional-Data-The-Random-Hierarchy-Model" class="headerlink" title="How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model"></a>How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02129">http://arxiv.org/abs/2307.02129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pcsl-epfl/hierarchy-learning">https://github.com/pcsl-epfl/hierarchy-learning</a></li>
<li>paper_authors: Leonardo Petrini, Francesco Cagnetta, Umberto M. Tomasini, Alessandro Favero, Matthieu Wyart</li>
<li>for: 这个论文的目的是研究深度卷积神经网络（CNN）在高维度数据上学习复杂任务的能力。</li>
<li>methods: 这个论文使用了深度卷积神经网络（CNN）来解决高维度数据上的学习问题。</li>
<li>results: 这个论文发现，深度卷积神经网络（CNN）可以通过建立不变表示来抗衡维度的呼噪问题，并且可以通过训练数据的增加来提高学习效果。<details>
<summary>Abstract</summary>
Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only polynomial in the input dimensionality; (ii) coincides with the training set size such that the representation of a trained network becomes invariant to exchanges of synonyms; (iii) corresponds to the number of data at which the correlations between low-level features and classes become detectable. Overall, our results indicate how deep CNNs can overcome the curse of dimensionality by building invariant representations, and provide an estimate of the number of data required to learn a task based on its hierarchically compositional structure.
</details>
<details>
<summary>摘要</summary>
学习高维任务非常困难，因为它们需要数据量的幂等于维度。然而，深度卷积神经网络（CNN）在缓解这个挑战上表现出了惊人的成功。一个流行的假设是，学习任务都具有很高的结构，并且CNN可以利用这种结构来建立低维度的数据表示。然而，有很少关于CNN需要多少训练数据，以及这个数据结构如何影响这个问题的研究。这篇论文回答了这个问题，对于一个简单的分类任务，即Random Hierarchy Model。在这个模型中，每个类都对应于$m$个同义词的高级特征组合，这些高级特征再经过$L$次循环组合而组成。我们发现，deep CNNs需要$P^*$多少训练数据来学习这个任务，其中（i）与输入维度相同的幂函数增长，即$n_c m^L$；（ii）与训练集大小相同，使得神经网络学习后的表示不受同义词的交换影响；（iii）与低级特征和类之间的相关性变得可识别。总之，我们的结果表明深度CNN可以缓解维度之恶的问题，并提供了学习任务基于层次结构的数据量需求的估计。
</details></li>
</ul>
<hr>
<h2 id="MDViT-Multi-domain-Vision-Transformer-for-Small-Medical-Image-Segmentation-Datasets"><a href="#MDViT-Multi-domain-Vision-Transformer-for-Small-Medical-Image-Segmentation-Datasets" class="headerlink" title="MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets"></a>MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02100">http://arxiv.org/abs/2307.02100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/siyi-wind/mdvit">https://github.com/siyi-wind/mdvit</a></li>
<li>paper_authors: Siyi Du, Nourhan Bayasi, Ghassan Harmarneh, Rafeef Garbi</li>
<li>for: 这篇论文旨在提高医疗影像分类（MIS）的精度和效率，以应对医疗影像的复杂和多样性。</li>
<li>methods: 本论文提出了多元领域 transformer（MDViT），具有领域适应器来缓解资料饥饿和抵消不同领域之间的负面知识转移（NKT）。此外，它还包括了互相知识传授 paradigm，将知识传授到多个领域的综合网络和辅助领域特有的分支。</li>
<li>results: 实验结果显示，MDViT 可以在 4 个皮肤病分类dataset上取得最佳性能，比起现有的算法，具有更好的分类性能和固定的模型大小，甚至在更多的领域添加时仍然保持良好的性能。<details>
<summary>Abstract</summary>
Despite its clinical utility, medical image segmentation (MIS) remains a daunting task due to images' inherent complexity and variability. Vision transformers (ViTs) have recently emerged as a promising solution to improve MIS; however, they require larger training datasets than convolutional neural networks. To overcome this obstacle, data-efficient ViTs were proposed, but they are typically trained using a single source of data, which overlooks the valuable knowledge that could be leveraged from other available datasets. Naivly combining datasets from different domains can result in negative knowledge transfer (NKT), i.e., a decrease in model performance on some domains with non-negligible inter-domain heterogeneity. In this paper, we propose MDViT, the first multi-domain ViT that includes domain adapters to mitigate data-hunger and combat NKT by adaptively exploiting knowledge in multiple small data resources (domains). Further, to enhance representation learning across domains, we integrate a mutual knowledge distillation paradigm that transfers knowledge between a universal network (spanning all the domains) and auxiliary domain-specific branches. Experiments on 4 skin lesion segmentation datasets show that MDViT outperforms state-of-the-art algorithms, with superior segmentation performance and a fixed model size, at inference time, even as more domains are added. Our code is available at https://github.com/siyi-wind/MDViT.
</details>
<details>
<summary>摘要</summary>
医学图像分割（MIS）是一项复杂和变化的任务，医学图像的自然复杂性和多样性使得MIS成为一项挑战。最近，视transformer（ViT）已经出现为MIS提供了一种有前途的解决方案，但它们需要更大的训练数据集than convolutional neural networks。为了解决这个障碍，数据高效的ViTs被提出，但它们通常是使用单个数据源进行训练，这会忽略其他可用的数据集中的有价值知识。将多个领域的数据集合并训练可能会导致非可靠的知识传递（NKT），即模型在某些领域的性能下降。在这篇论文中，我们提出了MDViT，第一个多领域ViT，包括域适应器来减少数据饿和抵消NKT。此外，为了增强跨领域表示学ARNING，我们 integration了一种互助知识传播模式，将所有领域的网络知识传递到各个域适应器中。我们的实验结果表明，MDViT在4种皮肤病变分割数据集上表现出色，性能高于状态 искусственный智能算法，即使添加更多的领域。我们的代码可以在https://github.com/siyi-wind/MDViT中下载。
</details></li>
</ul>
<hr>
<h2 id="Make-A-Long-Image-Short-Adaptive-Token-Length-for-Vision-Transformers"><a href="#Make-A-Long-Image-Short-Adaptive-Token-Length-for-Vision-Transformers" class="headerlink" title="Make A Long Image Short: Adaptive Token Length for Vision Transformers"></a>Make A Long Image Short: Adaptive Token Length for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02092">http://arxiv.org/abs/2307.02092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiqi Zhou, Yichen Zhu</li>
<li>for: 提高 ViT 模型的推理速度，使其更加快速和效率。</li>
<li>methods: 提出了一种基于 Resizable-ViT 模型和 Token-Length Assigner 的适应性词长分配策略，可以在执行时根据每个图像的需要选择最佳的词长。</li>
<li>results: 通过对多种 ViT 模型进行图像分类和动作识别等实验，证明了该方法可以有效地降低计算成本，提高推理速度。<details>
<summary>Abstract</summary>
The vision transformer is a model that breaks down each image into a sequence of tokens with a fixed length and processes them similarly to words in natural language processing. Although increasing the number of tokens typically results in better performance, it also leads to a considerable increase in computational cost. Motivated by the saying "A picture is worth a thousand words," we propose an innovative approach to accelerate the ViT model by shortening long images. Specifically, we introduce a method for adaptively assigning token length for each image at test time to accelerate inference speed. First, we train a Resizable-ViT (ReViT) model capable of processing input with diverse token lengths. Next, we extract token-length labels from ReViT that indicate the minimum number of tokens required to achieve accurate predictions. We then use these labels to train a lightweight Token-Length Assigner (TLA) that allocates the optimal token length for each image during inference. The TLA enables ReViT to process images with the minimum sufficient number of tokens, reducing token numbers in the ViT model and improving inference speed. Our approach is general and compatible with modern vision transformer architectures, significantly reducing computational costs. We verified the effectiveness of our methods on multiple representative ViT models on image classification and action recognition.
</details>
<details>
<summary>摘要</summary>
“当然”视觉 трансформа器（ViT）是一个模型，将每个图像转换为一串固定长度的单位，然后处理他们就如同自然语言处理中的字。虽然增加单位数量通常会导致性能提高，但也会导致庞大的计算成本增加。为了解决这个问题，我们提出了一个创新的方法，可以快速加速ViT模型的推理速度。我们首先将一个可调尺度的ViT（ReViT）模型训练，可以处理多种单位长度的输入。接着，我们从ReViT中提取了单位长度标签，这些标签指示了每个图像所需的最少单位数量以确保精准预测。我们然后使用这些标签进行训练一个轻量级的单位长度分配器（TLA），以将适当的单位长度分配给每个图像 during inference。TLA使得ReViT可以在测试时快速处理图像，并且将ViT模型内部的单位数量降低，进而降低计算成本。我们的方法是通用的，可以与现代的视觉 трансформа器架构相容，实现了计算成本的创下。我们在多个代表性的ViT模型上进行了证明，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Conversational-Head-Generation"><a href="#Interactive-Conversational-Head-Generation" class="headerlink" title="Interactive Conversational Head Generation"></a>Interactive Conversational Head Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02090">http://arxiv.org/abs/2307.02090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao</li>
<li>for: 本研究旨在提供一个面对面对话中的人工智能对话代理人生成的标准评估 benchmark，以促进对话中的人工智能代理人自动生成和多轮对话的能力。</li>
<li>methods: 本研究使用了独立的对话和聆听头生成任务（ViCo）和多轮对话对话任务（ViCo-X）两个新的数据集，并定义了三个新的任务：回应式聆听头生成、表达性对话头生成和对话合成。</li>
<li>results: 本研究的基本方法可以实现回应式和生动的代理人，并与真正的人类进行完整的对话。<details>
<summary>Abstract</summary>
We introduce a new conversation head generation benchmark for synthesizing behaviors of a single interlocutor in a face-to-face conversation. The capability to automatically synthesize interlocutors which can participate in long and multi-turn conversations is vital and offer benefits for various applications, including digital humans, virtual agents, and social robots. While existing research primarily focuses on talking head generation (one-way interaction), hindering the ability to create a digital human for conversation (two-way) interaction due to the absence of listening and interaction parts. In this work, we construct two datasets to address this issue, ``ViCo'' for independent talking and listening head generation tasks at the sentence level, and ``ViCo-X'', for synthesizing interlocutors in multi-turn conversational scenarios. Based on ViCo and ViCo-X, we define three novel tasks targeting the interaction modeling during the face-to-face conversation: 1) responsive listening head generation making listeners respond actively to the speaker with non-verbal signals, 2) expressive talking head generation guiding speakers to be aware of listeners' behaviors, and 3) conversational head generation to integrate the talking/listening ability in one interlocutor. Along with the datasets, we also propose corresponding baseline solutions to the three aforementioned tasks. Experimental results show that our baseline method could generate responsive and vivid agents that can collaborate with real person to fulfil the whole conversation. Project page: https://vico.solutions/.
</details>
<details>
<summary>摘要</summary>
我们介绍一个新的对话头生成标准 для生成面对面对话中的对话者行为。能够自动生成可以参与长时间和多轮对话的对话者是非常重要的，对于数字人类、虚拟代表和社交机器人等应用有益。而现有研究主要关注一个方面的对话（一个方向的交流），因此缺少对话者的听说和互动部分，这使得创建数字人类对话变得困难。在这项工作中，我们构建了两个数据集来解决这个问题：“ViCo”和“ViCo-X”。“ViCo”是独立的说话和听说头生成任务，而“ViCo-X”是用于synthesizing多轮对话的对话者。基于ViCo和ViCo-X，我们定义了三个新任务，包括：1）回应式听说头生成，使listeners能够活泼地回应说话者的非语言信号；2）表达声明性的说头生成，使说话者意识到听众的行为；以及3）对话头生成，将说话和听说能力集成到一个对话者中。同时，我们还提出了对这三个任务的基线解决方案。我们的基线方法可以生成有回应和生命力的代理人，与真正的人类一起完成对话。项目页面：https://vico.solutions/。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-on-Image-Classification-Models-FGSM-and-Patch-Attacks-and-their-Impact"><a href="#Adversarial-Attacks-on-Image-Classification-Models-FGSM-and-Patch-Attacks-and-their-Impact" class="headerlink" title="Adversarial Attacks on Image Classification Models: FGSM and Patch Attacks and their Impact"></a>Adversarial Attacks on Image Classification Models: FGSM and Patch Attacks and their Impact</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02055">http://arxiv.org/abs/2307.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaydip Sen, Subhasis Dasgupta</li>
<li>for: 本文介绍了图像分类模型遭受敌意攻击的问题。</li>
<li>methods: 本文使用了两种常见的敌意攻击方法：快速梯度签名方法（FGSM）和敌意贴图攻击。</li>
<li>results: 对三种强大预训练的图像分类模型（ResNet-34、GoogleNet和DenseNet-161）进行了攻击性评估，发现攻击后模型的分类精度减少了较多。<details>
<summary>Abstract</summary>
This chapter introduces the concept of adversarial attacks on image classification models built on convolutional neural networks (CNN). CNNs are very popular deep-learning models which are used in image classification tasks. However, very powerful and pre-trained CNN models working very accurately on image datasets for image classification tasks may perform disastrously when the networks are under adversarial attacks. In this work, two very well-known adversarial attacks are discussed and their impact on the performance of image classifiers is analyzed. These two adversarial attacks are the fast gradient sign method (FGSM) and adversarial patch attack. These attacks are launched on three powerful pre-trained image classifier architectures, ResNet-34, GoogleNet, and DenseNet-161. The classification accuracy of the models in the absence and presence of the two attacks are computed on images from the publicly accessible ImageNet dataset. The results are analyzed to evaluate the impact of the attacks on the image classification task.
</details>
<details>
<summary>摘要</summary>
本章介绍了图像分类模型 based on 卷积神经网络 (CNN) 面临的敌对攻击。CNN 是深度学习模型的非常受欢迎的一种，通常用于图像分类任务。然而，当这些网络受到敌对攻击时，它们可能会表现非常糟糕。在这种情况下，本文讨论了两种非常知名的敌对攻击方法，即快速梯度签名方法 (FGSM) 和敌对贴图攻击。这些攻击在三种强大预训练的图像分类器架构上进行，即 ResNet-34、GoogleNet 和 DenseNet-161。在图像网络上计算图像分类器在缺乏和存在这两种攻击时的分类精度。结果分析以评估攻击对图像分类任务的影响。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Imbalance-Aware-Gradient-Modulation-for-Weakly-supervised-Audio-Visual-Video-Parsing"><a href="#Multimodal-Imbalance-Aware-Gradient-Modulation-for-Weakly-supervised-Audio-Visual-Video-Parsing" class="headerlink" title="Multimodal Imbalance-Aware Gradient Modulation for Weakly-supervised Audio-Visual Video Parsing"></a>Multimodal Imbalance-Aware Gradient Modulation for Weakly-supervised Audio-Visual Video Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02041">http://arxiv.org/abs/2307.02041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Fu, Junyu Gao, Changsheng Xu</li>
<li>for: 本研究旨在解决听视联合分割任务中不同感知Modalities之间的特征学习不均衡问题。</li>
<li>methods: 本文提出了一种动态梯度修正机制，通过设计一种新的效果metric函数来度量听视特征学习不均衡问题。此外，本文还提出了一种模块化决策单元（MSDU），以更精准地测量听视特征学习不均衡问题。</li>
<li>results: 经过实验表明，提出的方法可以有效地解决听视联合分割任务中的特征学习不均衡问题。<details>
<summary>Abstract</summary>
Weakly-supervised audio-visual video parsing (WS-AVVP) aims to localize the temporal extents of audio, visual and audio-visual event instances as well as identify the corresponding event categories with only video-level category labels for training. Most previous methods pay much attention to refining the supervision for each modality or extracting fruitful cross-modality information for more reliable feature learning. None of them have noticed the imbalanced feature learning between different modalities in the task. In this paper, to balance the feature learning processes of different modalities, a dynamic gradient modulation (DGM) mechanism is explored, where a novel and effective metric function is designed to measure the imbalanced feature learning between audio and visual modalities. Furthermore, principle analysis indicates that the multimodal confusing calculation will hamper the precise measurement of multimodal imbalanced feature learning, which further weakens the effectiveness of our DGM mechanism. To cope with this issue, a modality-separated decision unit (MSDU) is designed for more precise measurement of imbalanced feature learning between audio and visual modalities. Comprehensive experiments are conducted on public benchmarks and the corresponding experimental results demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
弱监督音视频分析（WS-AVVP）目标是在没有视频类别标签培训的情况下，地时段化音、视觉和音视觉事件实例的涵义和相应的事件类别。先前的方法大多强调精细调整每个模式的监督或提取有利的跨模式信息以便更可靠的特征学习。然而，none of them 注意到了不同模式之间的特征学习不均衡问题。本文提出了一种动态梯度修正（DGM）机制，其中设计了一种新的有效度量函数，用于测量不同模式之间的特征学习不均衡。此外，理论分析表明，跨模式冲淡计算会妨碍精准测量跨模式特征学习不均衡问题，这会进一步削弱我们的DGM机制的效果。为解决这个问题，我们提出了一种模式分离决策单元（MSDU），用于更精准地测量不同模式之间的特征学习不均衡。我们在公共 benchmark上进行了广泛的实验，并结果表明了我们的提议方法的效果。
</details></li>
</ul>
<hr>
<h2 id="NMS-Threshold-matters-for-Ego4D-Moment-Queries-–-2nd-place-solution-to-the-Ego4D-Moment-Queries-Challenge-2023"><a href="#NMS-Threshold-matters-for-Ego4D-Moment-Queries-–-2nd-place-solution-to-the-Ego4D-Moment-Queries-Challenge-2023" class="headerlink" title="NMS Threshold matters for Ego4D Moment Queries – 2nd place solution to the Ego4D Moment Queries Challenge 2023"></a>NMS Threshold matters for Ego4D Moment Queries – 2nd place solution to the Ego4D Moment Queries Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02025">http://arxiv.org/abs/2307.02025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/happyharrycn/actionformer_release">https://github.com/happyharrycn/actionformer_release</a></li>
<li>paper_authors: Lin Sui, Fangzhou Mu, Yin Li</li>
<li>for: 本文是为了参加Ego4D Moment Queries Challenge 2023而提交的一篇论文。</li>
<li>methods: 本文extend了Latest ActionFormer方法，并在训练时使用改进的真实标注策略和推理时使用加工的SoftNMS。</li>
<li>results: 本文在公共领先板上排名第2，具有26.62%的均值准确率和45.69%的Recall@1x，在tIoU&#x3D;0.5的测试集上显著超越2023年挑战的强基eline。Here’s the English version of the three key information points:</li>
<li>for: This paper is a submission to the Ego4D Moment Queries Challenge 2023.</li>
<li>methods: The paper extends the latest ActionFormer method with an improved ground-truth assignment strategy during training and a refined version of SoftNMS at inference time.</li>
<li>results: The paper ranks 2nd on the public leaderboard with 26.62% average mAP and 45.69% Recall@1x at tIoU&#x3D;0.5 on the test set, significantly outperforming the strong baseline from 2023 challenge.<details>
<summary>Abstract</summary>
This report describes our submission to the Ego4D Moment Queries Challenge 2023. Our submission extends ActionFormer, a latest method for temporal action localization. Our extension combines an improved ground-truth assignment strategy during training and a refined version of SoftNMS at inference time. Our solution is ranked 2nd on the public leaderboard with 26.62% average mAP and 45.69% Recall@1x at tIoU=0.5 on the test set, significantly outperforming the strong baseline from 2023 challenge. Our code is available at https://github.com/happyharrycn/actionformer_release.
</details>
<details>
<summary>摘要</summary>
这份报告介绍我们在Ego4D动作识别挑战2023年提交的工作。我们的提交基于最新的ActionFormer方法，我们在训练时使用改进的真实标注策略，并在推理时使用精制的SoftNMS。我们的解决方案在公共排名榜上排名第2，具有26.62%的均值准确率和45.69%的回归率@1x，在测试集上显著超越2023年挑战的强基eline。我们的代码可以在https://github.com/happyharrycn/actionformer_release中找到。
</details></li>
</ul>
<hr>
<h2 id="ZJU-ReLER-Submission-for-EPIC-KITCHEN-Challenge-2023-TREK-150-Single-Object-Tracking"><a href="#ZJU-ReLER-Submission-for-EPIC-KITCHEN-Challenge-2023-TREK-150-Single-Object-Tracking" class="headerlink" title="ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking"></a>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02508">http://arxiv.org/abs/2307.02508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyou Xu, Jiahao Li, Zongxin Yang, Yi Yang, Yueting Zhuang</li>
<li>for: 这个研究旨在将类别框转换为面对面的图像标识，并使用Segment Anything Model (SAM)和Alpha-Refine进行标识储存和调节。</li>
<li>methods: 这个研究使用了Associating Objects with Transformers (AOT)框架，并将类别框转换为面对面的图像标识，使用Segment Anything Model (SAM)和Alpha-Refine进行标识储存和调节。</li>
<li>results: 这个研究获得了EPIC-KITCHENS TREK-150 Object Tracking Challenge的第一名。<details>
<summary>Abstract</summary>
The Associating Objects with Transformers (AOT) framework has exhibited exceptional performance in a wide range of complex scenarios for video object tracking and segmentation. In this study, we convert the bounding boxes to masks in reference frames with the help of the Segment Anything Model (SAM) and Alpha-Refine, and then propagate the masks to the current frame, transforming the task from Video Object Tracking (VOT) to video object segmentation (VOS). Furthermore, we introduce MSDeAOT, a variant of the AOT series that incorporates transformers at multiple feature scales. MSDeAOT efficiently propagates object masks from previous frames to the current frame using two feature scales of 16 and 8. As a testament to the effectiveness of our design, we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object Tracking Challenge.
</details>
<details>
<summary>摘要</summary>
《 associating objects with transformers（AOT）框架在各种复杂场景中表现出色，包括视频对象跟踪和分割。在这项研究中，我们将 bounding box 转换为 masks 在参照帧中使用 Segment Anything Model（SAM）和 Alpha-Refine，然后将 masks 传播到当前帧，从视频对象跟踪（VOT）任务转化为视频对象分割（VOS）任务。此外，我们还提出了 MSDeAOT，它是 AOT 系列中的一种变体，在多级特征尺度上引入 transformers。MSDeAOT 高效地在前一帧中传播对象mask，使用 16 和 8 级的特征尺度。我们在 EPIC-KITCHENS TREK-150 对象跟踪挑战中取得了第一名，这是我们设计的证明。
</details></li>
</ul>
<hr>
<h2 id="ZJU-ReLER-Submission-for-EPIC-KITCHEN-Challenge-2023-Semi-Supervised-Video-Object-Segmentation"><a href="#ZJU-ReLER-Submission-for-EPIC-KITCHEN-Challenge-2023-Semi-Supervised-Video-Object-Segmentation" class="headerlink" title="ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation"></a>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02010">http://arxiv.org/abs/2307.02010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Li, Yuanyou Xu, Zongxin Yang, Yi Yang, Yueting Zhuang</li>
<li>for: 这个论文主要针对视频对象分割任务，旨在提高对象分割精度和效率。</li>
<li>methods: 该论文提出了一种基于 transformers 的多级尺度Feature FusionModule (GPM)，可以有效地在不同尺度上进行对象 маска的传播。此外，论文还使用了高级的 GPM，以提高对小对象的检测和跟踪精度。</li>
<li>results: 根据实验结果，该论文在 EPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge 中取得了第一名。<details>
<summary>Abstract</summary>
The Associating Objects with Transformers (AOT) framework has exhibited exceptional performance in a wide range of complex scenarios for video object segmentation. In this study, we introduce MSDeAOT, a variant of the AOT series that incorporates transformers at multiple feature scales. Leveraging the hierarchical Gated Propagation Module (GPM), MSDeAOT efficiently propagates object masks from previous frames to the current frame using a feature scale with a stride of 16. Additionally, we employ GPM in a more refined feature scale with a stride of 8, leading to improved accuracy in detecting and tracking small objects. Through the implementation of test-time augmentations and model ensemble techniques, we achieve the top-ranking position in the EPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge.
</details>
<details>
<summary>摘要</summary>
“ associating objects with transformers”（AOT）框架在各种复杂场景中表现出了非常出色的表现，在本研究中，我们引入了多级嵌入式变换器（MSDeAOT），该系列嵌入了多级嵌入式变换器。通过层次阻止卷积模型（GPM），MSDeAOT可以有效地在当前帧中传递对象面积从前一帧，使用一个幅度为16的特征缩放级别。此外，我们还使用了一个更细的特征缩放级别，以提高对小对象的检测和跟踪精度。通过实施测试时数据增强和模型集成技术，我们在EPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge中获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="Remote-Sensing-Image-Change-Detection-with-Graph-Interaction"><a href="#Remote-Sensing-Image-Change-Detection-with-Graph-Interaction" class="headerlink" title="Remote Sensing Image Change Detection with Graph Interaction"></a>Remote Sensing Image Change Detection with Graph Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02007">http://arxiv.org/abs/2307.02007</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jackliu-97/bginet">https://github.com/jackliu-97/bginet</a></li>
<li>paper_authors: Chenglong Liu</li>
<li>for: 本研究旨在提出一种基于图STRUCTURE的 remote sensing 变化检测方法，以提高变化检测精度和效率。</li>
<li>methods: 该方法利用非局部操作和图STRUCTURE空间的mapping技术，实现了两幅时间图像之间的信息交互，从而提高变化检测精度。</li>
<li>results: 根据GZ CD数据集的实验结果，该方法与其他现有方法相比，显示了更高的准确率和更好的计算效率，同时也提高了整体效果。<details>
<summary>Abstract</summary>
Modern remote sensing image change detection has witnessed substantial advancements by harnessing the potent feature extraction capabilities of CNNs and Transforms.Yet,prevailing change detection techniques consistently prioritize extracting semantic features related to significant alterations,overlooking the viability of directly interacting with bitemporal image features.In this letter,we propose a bitemporal image graph Interaction network for remote sensing change detection,namely BGINet-CD. More specifically,by leveraging the concept of non-local operations and mapping the features obtained from the backbone network to the graph structure space,we propose a unified self-focus mechanism for bitemporal images.This approach enhances the information coupling between the two temporal images while effectively suppressing task-irrelevant interference,Based on a streamlined backbone architecture,namely ResNet18,our model demonstrates superior performance compared to other state-of-the-art methods (SOTA) on the GZ CD dataset. Moreover,the model exhibits an enhanced trade-off between accuracy and computational efficiency,further improving its overall effectiveness
</details>
<details>
<summary>摘要</summary>
现代远程感知图像变化检测技术已经取得了显著的进步，通过利用深度学习网络的强大特征提取能力。然而，现有的变化检测技术一直会优先把注意力集中在 semantics 相关的变化上，忽略了直接与二元时间图像特征进行交互的可能性。在本文中，我们提出了一种基于图像图грам的交互网络，称为 BGINet-CD。更具体地说，我们利用非本地操作的概念和将干扰网络生成的特征映射到图грам结构空间，提出了一种简单的自 Фокус机制。这种方法可以增强两个时间图像之间的信息coupling，同时有效地抑制任务无关的干扰。基于流畅的后ION架构，我们的模型在 GZ CD 数据集上达到了与其他现有方法（SOTA）比较的高性能。此外，我们的模型还展现出了提高精度和计算效率的权衡，进一步提高了其总效iveness。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Prototypes-for-Open-Set-Semantic-Segmentation"><a href="#Multi-Modal-Prototypes-for-Open-Set-Semantic-Segmentation" class="headerlink" title="Multi-Modal Prototypes for Open-Set Semantic Segmentation"></a>Multi-Modal Prototypes for Open-Set Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02003">http://arxiv.org/abs/2307.02003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhuan Yang, Chaofan Ma, Chen Ju, Ya Zhang, Yanfeng Wang</li>
<li>for: 本研究旨在解决semantic segmentation中对新物种的泛化问题，即在推理时通过提供视觉示例或文本描述来启用泛化。</li>
<li>methods: 本研究使用了一种统称为开集 semantic segmentation（O3S），它通过同时学习视觉和文本信息来学习已知和未知 semantics。研究pipeline包括单模强化和聚合、多模强化融合等步骤。</li>
<li>results: 在 Pascal和COCO数据集上，我们进行了广泛的实验评估框架的效果。 results表明，我们可以在只有粗粒化数据集上训练的情况下，在 Pascal-Animals 上达到了状态控制的结果。我们还进行了细致的拆分研究，以量化和质量地分析每个组件的贡献。<details>
<summary>Abstract</summary>
In semantic segmentation, adapting a visual system to novel object categories at inference time has always been both valuable and challenging. To enable such generalization, existing methods rely on either providing several support examples as visual cues or class names as textual cues. Through the development is relatively optimistic, these two lines have been studied in isolation, neglecting the complementary intrinsic of low-level visual and high-level language information. In this paper, we define a unified setting termed as open-set semantic segmentation (O3S), which aims to learn seen and unseen semantics from both visual examples and textual names. Our pipeline extracts multi-modal prototypes for segmentation task, by first single modal self-enhancement and aggregation, then multi-modal complementary fusion. To be specific, we aggregate visual features into several tokens as visual prototypes, and enhance the class name with detailed descriptions for textual prototype generation. The two modalities are then fused to generate multi-modal prototypes for final segmentation. On both \pascal and \coco datasets, we conduct extensive experiments to evaluate the framework effectiveness. State-of-the-art results are achieved even on more detailed part-segmentation, Pascal-Animals, by only training on coarse-grained datasets. Thorough ablation studies are performed to dissect each component, both quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
在 semantic segmentation 中，适应新的物体类别于推理时是非常有价值和挑战的。现有的方法通常采用提供一些支持例子作为视觉cue或者类名作为文本cue来实现这种总结。虽然这两种方法在开发中有一定的成功，但是它们在孤立的情况下研究，忽略了低级别视觉信息和高级别语言信息之间的共同内在。在这篇文章中，我们定义了一个统一的设定，称为开放集 semantic segmentation（O3S），该设定的目标是从视觉例子和文本名称中学习已知和未知 semantics。我们的管道包括以下步骤：1. 单modal自增和聚合：首先对视觉特征进行单modal自增，然后对各个模式进行聚合。2. 多modal补偿融合：将视觉特征聚合成多个标签，然后将文本名称补偿为更多的细节描述，以生成多模式的融合特征。3. 最终进行多模式融合，以实现最终的分类。在 Pascal 和 COCO 数据集上，我们进行了广泛的实验，以评估框架的效果。在 Pascal-Animals 数据集上，只需要训练在粗糙数据集上，就可以 дости得 estado-of-the-art 的结果。我们还进行了详细的拆分研究，以量化和质量地分析每个组件的作用。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Missing-Modality-Knowledge-from-Ultrasound-for-Endometriosis-Diagnosis-with-Magnetic-Resonance-Images"><a href="#Distilling-Missing-Modality-Knowledge-from-Ultrasound-for-Endometriosis-Diagnosis-with-Magnetic-Resonance-Images" class="headerlink" title="Distilling Missing Modality Knowledge from Ultrasound for Endometriosis Diagnosis with Magnetic Resonance Images"></a>Distilling Missing Modality Knowledge from Ultrasound for Endometriosis Diagnosis with Magnetic Resonance Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02000">http://arxiv.org/abs/2307.02000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Zhang, Hu Wang, David Butler, Minh-Son To, Jodie Avery, M Louise Hull, Gustavo Carneiro<br>for:  This paper aims to improve the accuracy of detecting pouch of Douglas (POD) obliteration from magnetic resonance imaging (MRI) scans, which is a common diagnostic challenge in endometriosis diagnosis.methods:  The proposed method uses a knowledge distillation training algorithm that leverages detection results from unpaired transvaginal gynecological ultrasound (TVUS) data to improve the POD obliteration detection from MRI. The algorithm pre-trains a teacher model to detect POD obliteration from TVUS data and a student model with a 3D masked auto-encoder using unlabelled pelvic 3D MRI volumes.results:  Experimental results on an endometriosis dataset containing TVUS and MRI data demonstrate the effectiveness of the proposed method in improving the POD detection accuracy from MRI.<details>
<summary>Abstract</summary>
Endometriosis is a common chronic gynecological disorder that has many characteristics, including the pouch of Douglas (POD) obliteration, which can be diagnosed using Transvaginal gynecological ultrasound (TVUS) scans and magnetic resonance imaging (MRI). TVUS and MRI are complementary non-invasive endometriosis diagnosis imaging techniques, but patients are usually not scanned using both modalities and, it is generally more challenging to detect POD obliteration from MRI than TVUS. To mitigate this classification imbalance, we propose in this paper a knowledge distillation training algorithm to improve the POD obliteration detection from MRI by leveraging the detection results from unpaired TVUS data. More specifically, our algorithm pre-trains a teacher model to detect POD obliteration from TVUS data, and it also pre-trains a student model with 3D masked auto-encoder using a large amount of unlabelled pelvic 3D MRI volumes. Next, we distill the knowledge from the teacher TVUS POD obliteration detector to train the student MRI model by minimizing a regression loss that approximates the output of the student to the teacher using unpaired TVUS and MRI data. Experimental results on our endometriosis dataset containing TVUS and MRI data demonstrate the effectiveness of our method to improve the POD detection accuracy from MRI.
</details>
<details>
<summary>摘要</summary>
Endometriosis 是一种常见的慢性室内疾病，其有多种特征之一是胞囊吟唱（POD）衰竭，可以通过跨膜室内ultrasound（TVUS）扫描和磁共振成像（MRI）进行诊断。TVUS和MRI 是非侵入性的抗癌诊断技术，但患者通常不会同时进行这两种扫描，并且从 MRI 中诊断 POD 衰竭更加困难。为了解决这种分类偏补，我们在这篇论文中提出了一种知识储备训练算法，用于从 MRI 中提高 PODS 衰竭的检测精度。具体来说，我们的算法首先在 TVUS 数据上训练一个老师模型，用于检测 PODS 衰竭。然后，我们使用一个 3D 卷积掩码自动机制学习模型，并使用大量的不标注的 pelvic 3D MRI 数据进行预训练。接着，我们将老师 TVUS POD 衰竭检测器的知识传播给学生 MRI 模型，通过使用不匹配的 TVUS 和 MRI 数据来执行知识储备训练。实验结果表明，我们的方法可以提高 MRI 中 PODS 衰竭的检测精度。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Neural-Architecture-Search-Challenges-Solutions-and-Opportunities"><a href="#Zero-Shot-Neural-Architecture-Search-Challenges-Solutions-and-Opportunities" class="headerlink" title="Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities"></a>Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01998">http://arxiv.org/abs/2307.01998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sldgroup/survey-zero-shot-nas">https://github.com/sldgroup/survey-zero-shot-nas</a></li>
<li>paper_authors: Guihong Li, Duc Hoang, Kartikeya Bhardwaj, Ming Lin, Zhangyang Wang, Radu Marculescu</li>
<li>for: 本研究旨在审查和比较当前最佳实践（SOTA）的零training neural architecture search（NAS）方法，强调硬件认知。</li>
<li>methods: 本研究使用了主流的零shot прокси，并讨论了它们的理论基础。</li>
<li>results: 经过大规模实验，本研究表明了零shot прокси在硬件认知和无硬件情况下的效果，并提出了一些可能更好的 proxy 设计方法。In English, the three key points are:</li>
<li>for: The purpose of this study is to comprehensively review and compare the state-of-the-art (SOTA) zero-shot NAS methods, with an emphasis on their hardware awareness.</li>
<li>methods: The study uses mainstream zero-shot proxies and discusses their theoretical underpinnings.</li>
<li>results: Through large-scale experiments, the study demonstrates the effectiveness of zero-shot proxies in both hardware-aware and hardware-oblivious NAS scenarios, and proposes several promising ideas for designing better proxies.<details>
<summary>Abstract</summary>
Recently, zero-shot (or training-free) Neural Architecture Search (NAS) approaches have been proposed to liberate the NAS from training requirements. The key idea behind zero-shot NAS approaches is to design proxies that predict the accuracies of the given networks without training network parameters. The proxies proposed so far are usually inspired by recent progress in theoretical deep learning and have shown great potential on several NAS benchmark datasets. This paper aims to comprehensively review and compare the state-of-the-art (SOTA) zero-shot NAS approaches, with an emphasis on their hardware awareness. To this end, we first review the mainstream zero-shot proxies and discuss their theoretical underpinnings. We then compare these zero-shot proxies through large-scale experiments and demonstrate their effectiveness in both hardware-aware and hardware-oblivious NAS scenarios. Finally, we point out several promising ideas to design better proxies. Our source code and the related paper list are available on https://github.com/SLDGroup/survey-zero-shot-nas.
</details>
<details>
<summary>摘要</summary>
最近，零批量（或训练自由）神经建筑搜索（NAS）方法被提出，以解放NAS免除训练要求。零批量NAS方法的关键思想是通过不需要训练网络参数的代理来预测网络的准确率。现有的代理大多受到最新的深度学习理论的启发，并在多个 NAS 评测数据集上显示出了极大的潜力。这篇论文的目的是对现代零批量 NAS 方法进行全面的评审和比较，强调硬件意识。为此，我们首先介绍主流的零批量代理，并讨论它们的理论基础。然后，我们通过大规模的实验进行比较，并在硬件意识和硬件无知 NAS 场景中证明这些零批量代理的有效性。最后，我们提出了一些可能的代理设计方法。我们的源代码和相关论文列表可以在 <https://github.com/SLDGroup/survey-zero-shot-nas> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Spectral-Demosaicing-with-Lightweight-Spectral-Attention-Networks"><a href="#Unsupervised-Spectral-Demosaicing-with-Lightweight-Spectral-Attention-Networks" class="headerlink" title="Unsupervised Spectral Demosaicing with Lightweight Spectral Attention Networks"></a>Unsupervised Spectral Demosaicing with Lightweight Spectral Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01990">http://arxiv.org/abs/2307.01990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Feng, Yongqiang Zhao, Seong G. Kong, Haijin Zeng</li>
<li>for: This paper presents an unsupervised deep learning-based spectral demosaicing technique for real-world hyperspectral images.</li>
<li>methods: The proposed method uses a mosaic loss function, a specific model structure, a transformation strategy, and an early stopping strategy to form a complete unsupervised spectral demosaicing framework. The spectral attention module is modified to reduce complexity and parameters.</li>
<li>results: The proposed method outperforms conventional unsupervised methods in terms of spatial distortion suppression, spectral fidelity, robustness, and computational cost, as demonstrated through extensive experiments on synthetic and real-world datasets.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文提出了一种基于深度学习的无监督 spectral demosaicing 技术，用于处理真实世界的多spectral 图像。</li>
<li>methods: 该方法使用了一种叫做 mosaic loss function 的损失函数，特定的模型结构，一种转换策略，以及一种 early stopping 策略，形成了一个完整的无监督 spectral demosaicing 框架。 spectral attention 模块被修改以降低复杂性和参数。</li>
<li>results: 该方法在 synthetic 和实际 datasets 上进行了广泛的实验，与传统的无监督方法相比，在平衡压缩、spectral 准确性、Robustness 和计算成本等方面表现出色。<details>
<summary>Abstract</summary>
This paper presents a deep learning-based spectral demosaicing technique trained in an unsupervised manner. Many existing deep learning-based techniques relying on supervised learning with synthetic images, often underperform on real-world images especially when the number of spectral bands increases. According to the characteristics of the spectral mosaic image, this paper proposes a mosaic loss function, the corresponding model structure, a transformation strategy, and an early stopping strategy, which form a complete unsupervised spectral demosaicing framework. A challenge in real-world spectral demosaicing is inconsistency between the model parameters and the computational resources of the imager. We reduce the complexity and parameters of the spectral attention module by dividing the spectral attention tensor into spectral attention matrices in the spatial dimension and spectral attention vector in the channel dimension, which is more suitable for unsupervised framework. This paper also presents Mosaic25, a real 25-band hyperspectral mosaic image dataset of various objects, illuminations, and materials for benchmarking. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method outperforms conventional unsupervised methods in terms of spatial distortion suppression, spectral fidelity, robustness, and computational cost.
</details>
<details>
<summary>摘要</summary>
A challenge in real-world spectral demosaicing is the inconsistency between the model parameters and the computational resources of the imager. To address this, the spectral attention module is simplified by dividing the spectral attention tensor into spectral attention matrices in the spatial dimension and a spectral attention vector in the channel dimension, making it more suitable for an unsupervised framework.This paper also introduces Mosaic25, a real 25-band hyperspectral mosaic image dataset of various objects, illuminations, and materials for benchmarking. Extensive experiments on synthetic and real-world datasets show that the proposed method outperforms conventional unsupervised methods in terms of spatial distortion suppression, spectral fidelity, robustness, and computational cost.
</details></li>
</ul>
<hr>
<h2 id="Task-Specific-Alignment-and-Multiple-Level-Transformer-for-Few-Shot-Action-Recognition"><a href="#Task-Specific-Alignment-and-Multiple-Level-Transformer-for-Few-Shot-Action-Recognition" class="headerlink" title="Task-Specific Alignment and Multiple Level Transformer for Few-Shot Action Recognition"></a>Task-Specific Alignment and Multiple Level Transformer for Few-Shot Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01985">http://arxiv.org/abs/2307.01985</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cofly2014/tsa-mlt">https://github.com/cofly2014/tsa-mlt</a></li>
<li>paper_authors: Fei Guo, Li Zhu, YiWang Wang</li>
<li>for: 这种论文的目的是提出一种基于Transformer的ew-shot动作识别方法，以提高动作识别的性能。</li>
<li>methods: 该方法使用了Multiple Level Transformer和Task-Specific Alignment（TSA）来提取多级特征，并使用了权重 fusion loss来将多种距离拟合。</li>
<li>results: 实验结果显示，该方法在HMDB51和UCF101数据集上达到了状态级Result，并在Kinetics和something-2-something V2数据集上获得了竞争性的结果。Here’s the full summary in Simplified Chinese:</li>
<li>for: 这种论文的目的是提出一种基于Transformer的ew-shot动作识别方法，以提高动作识别的性能。</li>
<li>methods: 该方法使用了Multiple Level Transformer和Task-Specific Alignment（TSA）来提取多级特征，并使用了权重 fusion loss来将多种距离拟合。</li>
<li>results: 实验结果显示，该方法在HMDB51和UCF101数据集上达到了状态级Result，并在Kinetics和something-2-something V2数据集上获得了竞争性的结果。<details>
<summary>Abstract</summary>
In the research field of few-shot learning, the main difference between image-based and video-based is the additional temporal dimension for videos. In recent years, many approaches for few-shot action recognition have followed the metric-based methods, especially, since some works use the Transformer to get the cross-attention feature of the videos or the enhanced prototype, and the results are competitive. However, they do not mine enough information from the Transformer because they only focus on the feature of a single level. In our paper, we have addressed this problem. We propose an end-to-end method named "Task-Specific Alignment and Multiple Level Transformer Network (TSA-MLT)". In our model, the Multiple Level Transformer focuses on the multiple-level feature of the support video and query video. Especially before Multiple Level Transformer, we use task-specific TSA to filter unimportant or misleading frames as a pre-processing. Furthermore, we adopt a fusion loss using two kinds of distance, the first is L2 sequence distance, which focuses on temporal order alignment. The second one is Optimal transport distance, which focuses on measuring the gap between the appearance and semantics of the videos. Using a simple fusion network, we fuse the two distances element-wise, then use the cross-entropy loss as our fusion loss. Extensive experiments show our method achieves state-of-the-art results on the HMDB51 and UCF101 datasets and a competitive result on the benchmark of Kinetics and something-2-something V2 datasets. Our code will be available at the URL: https://github.com/cofly2014/tsa-mlt.git
</details>
<details>
<summary>摘要</summary>
在几shot学习研究领域，图像和视频的主要区别在于视频具有额外的时间维度。在最近几年，许多几shot动作识别方法采用了度量基本方法，尤其是使用Transformer获得视频或增强原型的跨注意力特征，并且得到了竞争性的结果。然而，这些方法并不能充分利用Transformer的信息，因为它们只关注单级特征。在我们的论文中，我们解决了这个问题。我们提出了一种端到端方法，名为“任务特定对齐和多级Transformer网络”（TSA-MLT）。在我们的模型中，多级Transformer专注于支持视频和查询视频的多级特征。尤其是在多级Transformer之前，我们使用任务特定的TSA进行预处理，筛选不重要或误导性的帧。此外，我们采用了两种类型的混合损失，其中一种是L2序列距离，它专注于时间顺序对齐。另一种是优化运输距离，它专注于度量视频的外观和 semantics 之间的差异。使用简单的混合网络，我们将两种距离元素加权混合，然后使用交叉熵损失。广泛的实验显示，我们的方法在HMDB51和UCF101数据集上获得了状态独一标的结果，并在Kinetics和something-2-something V2数据集上获得了竞争性的结果。我们的代码将在以下URL上公开：https://github.com/cofly2014/tsa-mlt.git。
</details></li>
</ul>
<hr>
<h2 id="A-ChatGPT-Aided-Explainable-Framework-for-Zero-Shot-Medical-Image-Diagnosis"><a href="#A-ChatGPT-Aided-Explainable-Framework-for-Zero-Shot-Medical-Image-Diagnosis" class="headerlink" title="A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis"></a>A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01981">http://arxiv.org/abs/2307.01981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, Zuozhu Liu</li>
<li>for: 这个研究是为了提出一个零shot医疗影像分类框架，以应对实际医疗应用中的有限资料和疾病资料。</li>
<li>methods: 这个框架使用了CLIP的预训语言模型，并与ChatGPT进行整合，以提供可解释的诊断结果。具体来说，这个框架会将疾病名称输入到LLMs中，以生成更多的资讯和描述，例如疾病的征象或描述，以帮助提供更加精确和可解释的诊断结果。</li>
<li>results: 在一个私人数据集和四个公共数据集上进行了广泛的实验，结果显示了这个无需训练的零shot诊断框架的有效性和可解释性，并证明了VLMs和LLMs在医疗应用中的巨大潜力。<details>
<summary>Abstract</summary>
Zero-shot medical image classification is a critical process in real-world scenarios where we have limited access to all possible diseases or large-scale annotated data. It involves computing similarity scores between a query medical image and possible disease categories to determine the diagnostic result. Recent advances in pretrained vision-language models (VLMs) such as CLIP have shown great performance for zero-shot natural image recognition and exhibit benefits in medical applications. However, an explainable zero-shot medical image recognition framework with promising performance is yet under development. In this paper, we propose a novel CLIP-based zero-shot medical image classification framework supplemented with ChatGPT for explainable diagnosis, mimicking the diagnostic process performed by human experts. The key idea is to query large language models (LLMs) with category names to automatically generate additional cues and knowledge, such as disease symptoms or descriptions other than a single category name, to help provide more accurate and explainable diagnosis in CLIP. We further design specific prompts to enhance the quality of generated texts by ChatGPT that describe visual medical features. Extensive results on one private dataset and four public datasets along with detailed analysis demonstrate the effectiveness and explainability of our training-free zero-shot diagnosis pipeline, corroborating the great potential of VLMs and LLMs for medical applications.
</details>
<details>
<summary>摘要</summary>
Zero-shot医疗图像分类是现实场景中的一个关键过程，在受到有限的疾病或大规模标注数据的限制下进行诊断。它通过计算医疗图像和可能的疾病类别之间的相似性分数来确定诊断结果。现代的预训练视觉语言模型（VLM），如CLIP，在无需训练的情况下显示出了出色的性能，并在医学应用中获得了多方面的利用。然而，一个可解释的无需训练医疗图像诊断框架仍然在开发中。在这篇论文中，我们提出了一种基于CLIP的新的无需训练医疗图像诊断框架，并与ChatGPT进行了可解释的诊断。我们的关键想法是通过询问大语言模型（LLM）来自动生成更多的提示和知识，如疾病 симптом或描述，以帮助提供更准确和可解释的诊断。我们还设计了特定的提示，以提高生成的文本中的医学特征描述质量。我们在一个私人数据集和四个公共数据集上进行了广泛的实验和分析，并证明了我们的无需训练无需标注的诊断管道的有效性和可解释性，证明了VLM和LLM在医学应用中的潜在能力。
</details></li>
</ul>
<hr>
<h2 id="ToothSegNet-Image-Degradation-meets-Tooth-Segmentation-in-CBCT-Images"><a href="#ToothSegNet-Image-Degradation-meets-Tooth-Segmentation-in-CBCT-Images" class="headerlink" title="ToothSegNet: Image Degradation meets Tooth Segmentation in CBCT Images"></a>ToothSegNet: Image Degradation meets Tooth Segmentation in CBCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01979">http://arxiv.org/abs/2307.01979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxiang Liu, Tianxiang Hu, Yang Feng, Wanghui Ding, Zuozhu Liu</li>
<li>for: 三维牙齿模型的构建需要在计算机助动 orthodontics 中进行多种医疗治疗。</li>
<li>methods: 我们提出了一种新的框架，即 ToothSegNet，用于在训练中让分割模型适应受到影像质量问题的影响。</li>
<li>results: ToothSegNet 能够生成更精准的分割结果，并超过了现有的医学图像分割方法。<details>
<summary>Abstract</summary>
In computer-assisted orthodontics, three-dimensional tooth models are required for many medical treatments. Tooth segmentation from cone-beam computed tomography (CBCT) images is a crucial step in constructing the models. However, CBCT image quality problems such as metal artifacts and blurring caused by shooting equipment and patients' dental conditions make the segmentation difficult. In this paper, we propose ToothSegNet, a new framework which acquaints the segmentation model with generated degraded images during training. ToothSegNet merges the information of high and low quality images from the designed degradation simulation module using channel-wise cross fusion to reduce the semantic gap between encoder and decoder, and also refines the shape of tooth prediction through a structural constraint loss. Experimental results suggest that ToothSegNet produces more precise segmentation and outperforms the state-of-the-art medical image segmentation methods.
</details>
<details>
<summary>摘要</summary>
在计算机协助 ortodontics 中，三维牙齿模型是许多医疗治疗的关键。 however， CBCT 图像质量问题，如材料残余和扫描设备和病人的牙科状况所引起的模糊，使 segmentation 变得困难。 在这篇论文中，我们提出了 ToothSegNet，一个新的框架，它在训练中将 segmentation 模型告诉 generated 受损图像的信息。 ToothSegNet 通过设计的受损模拟器来汇集高和低质量图像的信息，使用通道 wise cross fusion 减少 encoder 和 decoder 之间的 semantic gap，并通过结构约束损失来精细调整牙齿预测的形态。 实验结果表明， ToothSegNet 可以生成更加精确的 segmentation，并超过了当前医疗图像 segmentation 方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Prompt-Learning-for-Product-Title-Generation-with-Extremely-Limited-Labels"><a href="#Multimodal-Prompt-Learning-for-Product-Title-Generation-with-Extremely-Limited-Labels" class="headerlink" title="Multimodal Prompt Learning for Product Title Generation with Extremely Limited Labels"></a>Multimodal Prompt Learning for Product Title Generation with Extremely Limited Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01969">http://arxiv.org/abs/2307.01969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bang Yang, Fenglin Liu, Zheng Li, Qingyu Yin, Chenyu You, Bing Yin, Yuexian Zou</li>
<li>for: 本研究旨在提出一种基于提示的方法，即多模态提示学习框架，以精准地和效率地生成对新产品有限标注数据的标题。</li>
<li>methods: 我们使用多模态提示从不同的模式提取特征和写作风格，并将其用于生成标题。我们只需要1%的下游标注数据进行训练，并且在预测时使用提示来生成满意的标题。</li>
<li>results: 我们在五种新产品类别下进行了实验和分析，并在域外和域内两种 эксперименталь设定下测试了我们的方法。结果表明，只需要1%的下游标注数据进行训练，我们的方法可以在少量标注下获得最佳几枚shot结果，甚至与完全监督方法在100%的训练数据上进行了竞争。当有完整的标注数据进行训练时，我们的方法达到了状态艺术的结果。<details>
<summary>Abstract</summary>
Generating an informative and attractive title for the product is a crucial task for e-commerce. Most existing works follow the standard multimodal natural language generation approaches, e.g., image captioning, and employ the large scale of human-labelled datasets to train desirable models. However, for novel products, especially in a different domain, there are few existing labelled data. In this paper, we propose a prompt-based approach, i.e., the Multimodal Prompt Learning framework, to accurately and efficiently generate titles for novel products with limited labels. We observe that the core challenges of novel product title generation are the understanding of novel product characteristics and the generation of titles in a novel writing style. To this end, we build a set of multimodal prompts from different modalities to preserve the corresponding characteristics and writing styles of novel products. As a result, with extremely limited labels for training, the proposed method can retrieve the multimodal prompts to generate desirable titles for novel products. The experiments and analyses are conducted on five novel product categories under both the in-domain and out-of-domain experimental settings. The results show that, with only 1% of downstream labelled data for training, our proposed approach achieves the best few-shot results and even achieves competitive results with fully-supervised methods trained on 100% of training data; With the full labelled data for training, our method achieves state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将产品的信息和描述呈现成有引起人们兴趣的标题是电商的关键任务。现有的大多数工作采用标准的多modal自然语言生成方法，如图像标题，并使用大量人工标注数据来训练愿望的模型。但是，对于新型产品，特别是在不同的领域，有限的标注数据。在这篇论文中，我们提出了一种提示基于的方法，即多modal提示学习框架，以准确和高效地生成新型产品的标题。我们发现，新型产品标题生成的核心挑战是理解新型产品特点和生成新式标题。为此，我们构建了多modal提示，从不同的Modalities中提取了相应的特征和写作风格，以保持新型产品的特点和写作风格。结果是，只需培育1%的下游标注数据，我们的提posed方法可以将多modal提示拟合到生成愉悦的标题。我们在五种新型产品类别下进行了在领域和非领域实验设定下的实验和分析。结果表明，只需培育1%的下游标注数据，我们的提posed方法可以达到少量多shot最佳 result，甚至与完全upervised方法在100%的培育数据上培育的结果竞争。与完全标注数据培育，我们的方法达到了状态的艺术 result。
</details></li>
</ul>
<hr>
<h2 id="Muti-scale-Graph-Neural-Network-with-Signed-attention-for-Social-Bot-Detection-A-Frequency-Perspective"><a href="#Muti-scale-Graph-Neural-Network-with-Signed-attention-for-Social-Bot-Detection-A-Frequency-Perspective" class="headerlink" title="Muti-scale Graph Neural Network with Signed-attention for Social Bot Detection: A Frequency Perspective"></a>Muti-scale Graph Neural Network with Signed-attention for Social Bot Detection: A Frequency Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01968">http://arxiv.org/abs/2307.01968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuhao Shi, Kai Qiao, Zhengyan Wang, Jie Yang, Baojie Song, Jian Chen, Bin Yan</li>
<li>for: This paper proposes a method for detecting social bots using a multi-scale signed-attention graph filter (MSGS) to improve the representation ability of the model and alleviate the over-smoothing problem of deep GNNs.</li>
<li>methods: The proposed MSGS method utilizes a multi-scale structure to produce representation vectors at different scales, which are then combined using a signed-attention mechanism. The method also uses a polymerization process to produce the final result.</li>
<li>results: The experimental results on real-world datasets demonstrate that the proposed MSGS method achieves better performance compared with several state-of-the-art social bot detection methods.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文提出了一种基于多尺度签名图表匹配（MSGS）的社交机器人检测方法，以提高模型表示能力和解决深度GNN的过滤问题。</li>
<li>methods: MSGS方法使用多尺度结构生成不同级别的表示向量，然后使用签名机制将其组合。最后，通过多尺度表示后的多层树结构（MLP）进行权重调整，以生成最终结果。</li>
<li>results: 实验结果表明，提出的MSGS方法在真实世界数据集上比一些现有的社交机器人检测方法更高效。<details>
<summary>Abstract</summary>
The presence of a large number of bots on social media has adverse effects. The graph neural network (GNN) can effectively leverage the social relationships between users and achieve excellent results in detecting bots. Recently, more and more GNN-based methods have been proposed for bot detection. However, the existing GNN-based bot detection methods only focus on low-frequency information and seldom consider high-frequency information, which limits the representation ability of the model. To address this issue, this paper proposes a Multi-scale with Signed-attention Graph Filter for social bot detection called MSGS. MSGS could effectively utilize both high and low-frequency information in the social graph. Specifically, MSGS utilizes a multi-scale structure to produce representation vectors at different scales. These representations are then combined using a signed-attention mechanism. Finally, multi-scale representations via MLP after polymerization to produce the final result. We analyze the frequency response and demonstrate that MSGS is a more flexible and expressive adaptive graph filter. MSGS can effectively utilize high-frequency information to alleviate the over-smoothing problem of deep GNNs. Experimental results on real-world datasets demonstrate that our method achieves better performance compared with several state-of-the-art social bot detection methods.
</details>
<details>
<summary>摘要</summary>
社交媒体上有大量的机器人存在，这会导致不良影响。社交图图 neural network（GNN）可以有效利用用户之间的社交关系，并实现出色的机器人检测结果。最近，更多的GNN基本方法被提出用于机器人检测。然而，现有的GNN基本方法通常只专注于低频信息，罕考虑高频信息，这限制了模型的表达能力。为了解决这个问题，本文提出了一种多尺度签名Graph Filter（MSGS）。MSGS可以有效利用社交图中的高频和低频信息。具体来说，MSGS使用多尺度结构生成不同级别的表示 вектор。这些表示vector然后通过签名机制相加。最后，通过多尺度MLP（多层感知器）进行混合，生成最终结果。我们分析频率响应，并证明MSGS是一种更灵活和表达力强的自适应图 filters。MSGS可以充分利用高频信息，以解决深度GNN的过滤问题。实验结果表明，我们的方法在真实世界数据上达到了与一些现有社交bot检测方法相比的更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Neural-Diffeomorphic-Flow-for-Shape-Representation-and-Generation-via-Triplane"><a href="#Hybrid-Neural-Diffeomorphic-Flow-for-Shape-Representation-and-Generation-via-Triplane" class="headerlink" title="Hybrid Neural Diffeomorphic Flow for Shape Representation and Generation via Triplane"></a>Hybrid Neural Diffeomorphic Flow for Shape Representation and Generation via Triplane</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01957">http://arxiv.org/abs/2307.01957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Han, Shanlin Sun, Xiaohui Xie</li>
<li>for: 用于3D计算机视觉中的形状表示和生成</li>
<li>methods: 使用混合神经抽象流变换（HNDF），通过嵌入式学习下的三平面特征来分解复杂的密集对准关系</li>
<li>results: 可以生成高质量和多样化的3D抽象流变换，保证形状的topological consistency，并且在医疗影像 segmentation 数据集上证明了 HNDF 的效果<details>
<summary>Abstract</summary>
Deep Implicit Functions (DIFs) have gained popularity in 3D computer vision due to their compactness and continuous representation capabilities. However, addressing dense correspondences and semantic relationships across DIF-encoded shapes remains a critical challenge, limiting their applications in texture transfer and shape analysis. Moreover, recent endeavors in 3D shape generation using DIFs often neglect correspondence and topology preservation. This paper presents HNDF (Hybrid Neural Diffeomorphic Flow), a method that implicitly learns the underlying representation and decomposes intricate dense correspondences into explicitly axis-aligned triplane features. To avoid suboptimal representations trapped in local minima, we propose hybrid supervision that captures both local and global correspondences. Unlike conventional approaches that directly generate new 3D shapes, we further explore the idea of shape generation with deformed template shape via diffeomorphic flows, where the deformation is encoded by the generated triplane features. Leveraging a pre-existing 2D diffusion model, we produce high-quality and diverse 3D diffeomorphic flows through generated triplanes features, ensuring topological consistency with the template shape. Extensive experiments on medical image organ segmentation datasets evaluate the effectiveness of HNDF in 3D shape representation and generation.
</details>
<details>
<summary>摘要</summary>
深层含义函数（DIF）在三维计算机视觉中受到广泛关注，因为它们具有高度压缩和连续表示能力。然而，在 DIF 编码的形状之间 densely 的对应和semantic 关系仍然是一个挑战，限制了它们在文本传输和形状分析方面的应用。此外，现在的3D形状生成使用 DIF  часто忽视对应和拓扑保持。这篇论文提出了 HyNDF（混合神经含义流），一种方法，可以 implicitly 学习下面的含义表示，并将含义嵌入在满足特定条件的三维形状中。为了避免局部最佳化，我们提议 hybrid 监督，捕捉本地和全局对应。与传统方法不同的是，我们不直接生成新的3D形状，而是通过含义流来生成扭变的模板形状，其中的扭变是由生成的三维特征表示。利用现有的2D扩散模型，我们可以通过生成的三维特征来生成高质量和多样化的3D扩散流，保证模板形状的拓扑一致性。EXTENSIVE EXPERIMENTS ON MEDICAL IMAGE ORGAN SEGMENTATION DATASETS EVALUATE THE EFFECTIVENESS OF HyNDF IN 3D SHAPE REPRESENTATION AND GENERATION。
</details></li>
</ul>
<hr>
<h2 id="Toward-more-frugal-models-for-functional-cerebral-networks-automatic-recognition-with-resting-state-fMRI"><a href="#Toward-more-frugal-models-for-functional-cerebral-networks-automatic-recognition-with-resting-state-fMRI" class="headerlink" title="Toward more frugal models for functional cerebral networks automatic recognition with resting-state fMRI"></a>Toward more frugal models for functional cerebral networks automatic recognition with resting-state fMRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01953">http://arxiv.org/abs/2307.01953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukman Ismaila, Pejman Rasti, Jean-Michel Lemée, David Rousseau</li>
<li>for: 这个论文是为了研究不同的编码技术，以降低神经网络模型的复杂性而不影响其性能。</li>
<li>methods: 这个论文使用了超像素图和图表示法来编码功能神经网络图像，并且使用了神经网络模型来识别患有脑肿瘤的患者的冥想状态功能网络。</li>
<li>results: 研究发现，使用图表示法可以保持神经网络模型的性能，同时降低模型的复杂性，最终可以提高识别精度。<details>
<summary>Abstract</summary>
We refer to a machine learning situation where models based on classical convolutional neural networks have shown good performance. We are investigating different encoding techniques in the form of supervoxels, then graphs to reduce the complexity of the model while tracking the loss of performance. This approach is illustrated on a recognition task of resting-state functional networks for patients with brain tumors. Graphs encoding supervoxels preserve activation characteristics of functional brain networks from images, optimize model parameters by 26 times while maintaining CNN model performance.
</details>
<details>
<summary>摘要</summary>
我们提到了一种机器学习情况，其中基于古典卷积神经网络的模型表现良好。我们正在研究不同的编码技术，包括超 voxel 和图гра夫，以降低模型复杂性的方式，同时跟踪模型性能的损失。这种方法在抑制性功能网络图像识别任务中被应用，并且使用图гра夫编码超 voxel 保持了功能脑网络的活动特征，并且优化模型参数26倍，而无需妥协模型性能。
</details></li>
</ul>
<hr>
<h2 id="A-Synthetic-Electrocardiogram-ECG-Image-Generation-Toolbox-to-Facilitate-Deep-Learning-Based-Scanned-ECG-Digitization"><a href="#A-Synthetic-Electrocardiogram-ECG-Image-Generation-Toolbox-to-Facilitate-Deep-Learning-Based-Scanned-ECG-Digitization" class="headerlink" title="A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization"></a>A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01946">http://arxiv.org/abs/2307.01946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshama Kodthalu Shivashankara, Afagh Mehri Shervedani, Reza Sameni</li>
<li>For: The paper is written for training machine learning models in algorithmic ECG diagnosis, and the authors aim to address the challenge of scarce ECG archives with reference time-series.* Methods: The authors propose a novel method for generating synthetic ECG images on standard paper-like ECG backgrounds with realistic artifacts, using digital twins and data augmentation techniques.* Results: The authors built a deep ECG image digitization model and trained it on the synthetic dataset, and the results show an average signal recovery SNR of 27$\pm$2.8,dB, demonstrating the significance of the proposed synthetic ECG image dataset for training deep learning models.<details>
<summary>Abstract</summary>
The electrocardiogram (ECG) is an accurate and widely available tool for diagnosing cardiovascular diseases. ECGs have been recorded in printed formats for decades and their digitization holds great potential for training machine learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at risk of deterioration and scanning printed ECGs alone is insufficient, as ML models require ECG time-series data. Therefore, the digitization and conversion of paper ECG archives into time-series data is of utmost importance. Deep learning models for image processing show promise in this regard. However, the scarcity of ECG archives with reference time-series is a challenge. Data augmentation techniques utilizing \textit{digital twins} present a potential solution.   We introduce a novel method for generating synthetic ECG images on standard paper-like ECG backgrounds with realistic artifacts. Distortions including handwritten text artifacts, wrinkles, creases and perspective transforms are applied to the generated images, without personally identifiable information. As a use case, we generated an ECG image dataset of 21,801 records from the 12-lead PhysioNet PTB-XL ECG time-series dataset. A deep ECG image digitization model was built and trained on the synthetic dataset, and was employed to convert the synthetic images to time-series data for evaluation. The signal-to-noise ratio (SNR) was calculated to assess the image digitization quality vs the ground truth ECG time-series. The results show an average signal recovery SNR of 27$\pm$2.8\,dB, demonstrating the significance of the proposed synthetic ECG image dataset for training deep learning models. The codebase is available as an open-access toolbox for ECG research.
</details>
<details>
<summary>摘要</summary>
《电cardiogram（ECG）是一种准确和普遍可用的诊断卡ди奥vascular疾病工具。ECG已经被记录在打印格式上了几十年，其数字化具有很大的潜在价值，用于训练机器学习（ML）模型进行算法ic ECG诊断。 Physical ECG Archives 面临着逐渐损坏的问题，而且单独扫描打印ECG 也无法满足 ML 模型的需求，因此将paper ECG Archives digitized和转换为时序数据是非常重要的。深度学习模型用于图像处理表现出了潜在的优势。然而，ECG Archives 中的参考时序数据的缺乏是一个挑战。使用 \textit{digital twins} 的数据增强技术可以解决这个问题。》我们提出了一种新的方法，用于在标准纸如ECG背景上生成 sintetic ECG 图像。这些图像包括手写文本残留、折痕、折叠和 perspective transforms，但不包含个人可 identificable 信息。作为用例，我们生成了一个 ECG 图像集合，包含PhysioNet PTB-XL ECG 时序数据集的 12 个导线。我们建立了一个深度 ECG 图像数字化模型，并在生成的 sintetic 图像上进行训练。然后，我们使用这个模型将 sintetic 图像转换为时序数据，并计算了信号噪声比（SNR），以评估图像数字化质量和参考时序数据之间的相似性。结果表明，Synthetic ECG 图像集合可以提供高质量的深度学习模型训练数据，并且可以提高 ECG 图像数字化质量。代码库将作为一个开放的工具箱，用于ECG 研究。
</details></li>
</ul>
<hr>
<h2 id="Text-Sketch-Image-Compression-at-Ultra-Low-Rates"><a href="#Text-Sketch-Image-Compression-at-Ultra-Low-Rates" class="headerlink" title="Text + Sketch: Image Compression at Ultra Low Rates"></a>Text + Sketch: Image Compression at Ultra Low Rates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01944">http://arxiv.org/abs/2307.01944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leieric/text-sketch">https://github.com/leieric/text-sketch</a></li>
<li>paper_authors: Eric Lei, Yiğit Berkay Uslu, Hamed Hassani, Shirin Saeedi Bidokhti</li>
<li>for: 研究了如何使用基于文本的生成模型进行图像压缩，以实现新的低比特率压缩方案。</li>
<li>methods: 使用预训练的基于文本的模型进行图像压缩，并与侧信息相结合以生成高质量重建图像，保持原始图像的 semantics 和空间结构。</li>
<li>results: 在非常低的比特率下，方法可以明显超越学习压缩器，在 semantic 和 perceived 稳定性方面达到更高的水平。<details>
<summary>Abstract</summary>
Recent advances in text-to-image generative models provide the ability to generate high-quality images from short text descriptions. These foundation models, when pre-trained on billion-scale datasets, are effective for various downstream tasks with little or no further training. A natural question to ask is how such models may be adapted for image compression. We investigate several techniques in which the pre-trained models can be directly used to implement compression schemes targeting novel low rate regimes. We show how text descriptions can be used in conjunction with side information to generate high-fidelity reconstructions that preserve both semantics and spatial structure of the original. We demonstrate that at very low bit-rates, our method can significantly improve upon learned compressors in terms of perceptual and semantic fidelity, despite no end-to-end training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Physics-based-Motion-Retargeting-from-Sparse-Inputs"><a href="#Physics-based-Motion-Retargeting-from-Sparse-Inputs" class="headerlink" title="Physics-based Motion Retargeting from Sparse Inputs"></a>Physics-based Motion Retargeting from Sparse Inputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01938">http://arxiv.org/abs/2307.01938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Reda, Jungdam Won, Yuting Ye, Michiel van de Panne, Alexander Winkler</li>
<li>for: 这个论文的目的是如何在虚拟世界中创建有互动性和吸引力的人物动画，并解决了用户动作追踪的技术问题。</li>
<li>methods: 这篇论文使用了回归学习来训练一个策略，以控制不同骨架结构的人物在物理模拟器中进行动作。这种方法不需要艺术家制作每个人物的动画，只需要人体动作捕捉数据来训练。</li>
<li>results: 研究人员通过使用这种方法，可以在实时中基于少量的人体感知数据来追踪用户的动作，并且可以让人物的姿势很好地与用户匹配。研究人员还进行了多种设定的研究，包括不均衡、舞蹈和体育动作等。<details>
<summary>Abstract</summary>
Avatars are important to create interactive and immersive experiences in virtual worlds. One challenge in animating these characters to mimic a user's motion is that commercial AR/VR products consist only of a headset and controllers, providing very limited sensor data of the user's pose. Another challenge is that an avatar might have a different skeleton structure than a human and the mapping between them is unclear. In this work we address both of these challenges. We introduce a method to retarget motions in real-time from sparse human sensor data to characters of various morphologies. Our method uses reinforcement learning to train a policy to control characters in a physics simulator. We only require human motion capture data for training, without relying on artist-generated animations for each avatar. This allows us to use large motion capture datasets to train general policies that can track unseen users from real and sparse data in real-time. We demonstrate the feasibility of our approach on three characters with different skeleton structure: a dinosaur, a mouse-like creature and a human. We show that the avatar poses often match the user surprisingly well, despite having no sensor information of the lower body available. We discuss and ablate the important components in our framework, specifically the kinematic retargeting step, the imitation, contact and action reward as well as our asymmetric actor-critic observations. We further explore the robustness of our method in a variety of settings including unbalancing, dancing and sports motions.
</details>
<details>
<summary>摘要</summary>
�� Avatars 是创建互动和 immerse 的虚拟世界体验的关键。一个挑战在动画这些人物，以便它们模拟用户的运动是商业 AR/VR 产品只有头盔和控制器，这些设备提供的感知数据非常有限。另一个挑战是人物可能有不同的骨架结构和人类的映射关系不清楚。在这种工作中，我们解决了这两个挑战。我们提出了一种在实时中归档人物动作的方法，从某些人类感知数据中提取人物动作。我们使用了强化学习来训练一个策略，以控制在物理模拟器中的人物。我们只需要人类动作捕捉数据进行训练，而不需要每个人物都有特定的动作捕捉。这使得我们可以使用大量的动作捕捉数据来训练通用的策略，以跟踪未看过的用户从真实的感知数据中的真实时间。我们在三个不同的骨架结构中进行了试验：恐龙、小型宠物和人类。我们发现，尽管没有lower body的感知数据可用，但是人物姿势仍然很准确地匹配了用户。我们讨论和折射了我们框架中的重要组件，特别是凯旋投射步骤、仿制、接触和动作奖励。我们进一步探讨了我们的方法在多种设置下的稳定性，包括不均衡、舞蹈和运动动作。
</details></li>
</ul>
<hr>
<h2 id="ProtoDiffusion-Classifier-Free-Diffusion-Guidance-with-Prototype-Learning"><a href="#ProtoDiffusion-Classifier-Free-Diffusion-Guidance-with-Prototype-Learning" class="headerlink" title="ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning"></a>ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01924">http://arxiv.org/abs/2307.01924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gulcin Baykal, Halil Faruk Karagoz, Taha Binhuraib, Gozde Unal</li>
<li>for: 提高生成质量和快速训练 diffusion models</li>
<li>methods:  incorporate prototype learning into diffusion models</li>
<li>results: 比基eline方法更快地达到更高的生成质量<details>
<summary>Abstract</summary>
Diffusion models are generative models that have shown significant advantages compared to other generative models in terms of higher generation quality and more stable training. However, the computational need for training diffusion models is considerably increased. In this work, we incorporate prototype learning into diffusion models to achieve high generation quality faster than the original diffusion model. Instead of randomly initialized class embeddings, we use separately learned class prototypes as the conditioning information to guide the diffusion process. We observe that our method, called ProtoDiffusion, achieves better performance in the early stages of training compared to the baseline method, signifying that using the learned prototypes shortens the training time. We demonstrate the performance of ProtoDiffusion using various datasets and experimental settings, achieving the best performance in shorter times across all settings.
</details>
<details>
<summary>摘要</summary>
Diffusion models 是一种生成模型，在其他生成模型的比较中显示出了更高的生成质量和更稳定的训练。然而， diffusion models 的计算需求在训练中增加了许多。在这项工作中，我们将 prototype learning  интеGRATED into diffusion models，以实现更高的生成质量和更快的训练。而不是使用随机初始化的类嵌入，我们使用分立学习的类表例来导引 diffusion 过程。我们发现，我们的方法（Call ProtoDiffusion）在训练的早期 stages 表现更好，表明使用学习的表例可以缩短训练时间。我们通过使用不同的数据集和实验设置来证明 ProtoDiffusion 的性能，在所有设置下都达到了最佳性能，并且在短时间内完成。
</details></li>
</ul>
<hr>
<h2 id="EANet-Enhanced-Attribute-based-RGBT-Tracker-Network"><a href="#EANet-Enhanced-Attribute-based-RGBT-Tracker-Network" class="headerlink" title="EANet: Enhanced Attribute-based RGBT Tracker Network"></a>EANet: Enhanced Attribute-based RGBT Tracker Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01893">http://arxiv.org/abs/2307.01893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abbas Türkoğlu, Erdem Akagündüz</li>
<li>for: 提高计算机视觉中跟踪对象的精度，特别是在受到 occlusion、光照变化和运动模糊等挑战时。</li>
<li>methods: 提议一种基于深度学习的图像跟踪方法，使用 RGB 和 TIR 图像的复合（RGBT），并使用特征提取器和跟踪器两部分组成。</li>
<li>results: 对 RGBT234 和 LasHeR 等 dataset进行评估，表明提议的系统在这些 dataset 上表现出色，与现有状态的 RGBT 对象跟踪器相比，具有较少的参数量。<details>
<summary>Abstract</summary>
Tracking objects can be a difficult task in computer vision, especially when faced with challenges such as occlusion, changes in lighting, and motion blur. Recent advances in deep learning have shown promise in challenging these conditions. However, most deep learning-based object trackers only use visible band (RGB) images. Thermal infrared electromagnetic waves (TIR) can provide additional information about an object, including its temperature, when faced with challenging conditions. We propose a deep learning-based image tracking approach that fuses RGB and thermal images (RGBT). The proposed model consists of two main components: a feature extractor and a tracker. The feature extractor encodes deep features from both the RGB and the TIR images. The tracker then uses these features to track the object using an enhanced attribute-based architecture. We propose a fusion of attribute-specific feature selection with an aggregation module. The proposed methods are evaluated on the RGBT234 \cite{LiCLiang2018} and LasHeR \cite{LiLasher2021} datasets, which are the most widely used RGBT object-tracking datasets in the literature. The results show that the proposed system outperforms state-of-the-art RGBT object trackers on these datasets, with a relatively smaller number of parameters.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>计算机视觉中跟踪对象可能是一项困难任务，特别是在面临遮挡、光照变化和运动模糊等挑战时。现代深度学习技术已经在这些条件下表现出了承诺。然而，大多数深度学习基于对象跟踪器只使用可见光（RGB）图像。热电磁波（TIR）可以提供更多关于对象的信息，包括其温度，在面临挑战情况下。我们提议一种基于深度学习的图像跟踪方法，将RGB和TIR图像进行融合（RGBT）。我们的方法包括两个主要组件：特征提取器和跟踪器。特征提取器将深度特征从RGB和TIR图像中提取出来。跟踪器然后使用这些特征进行跟踪对象，使用增强的属性基本架构。我们提议一种属性特征选择与聚合模块的融合。我们的方法在RGBT234和LasHeR数据集上进行评估，这两个数据集是现有RGBT对象跟踪数据集中最为常用的。结果显示，我们的系统在这些数据集上与现状最佳的RGBT对象跟踪器进行比较，具有相对较小的参数数量。
</details></li>
</ul>
<hr>
<h2 id="MaskBEV-Joint-Object-Detection-and-Footprint-Completion-for-Bird’s-eye-View-3D-Point-Clouds"><a href="#MaskBEV-Joint-Object-Detection-and-Footprint-Completion-for-Bird’s-eye-View-3D-Point-Clouds" class="headerlink" title="MaskBEV: Joint Object Detection and Footprint Completion for Bird’s-eye View 3D Point Clouds"></a>MaskBEV: Joint Object Detection and Footprint Completion for Bird’s-eye View 3D Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01864">http://arxiv.org/abs/2307.01864</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/norlab-ulaval/mask_bev">https://github.com/norlab-ulaval/mask_bev</a></li>
<li>paper_authors: William Guimont-Martin, Jean-Michel Fortin, François Pomerleau, Philippe Giguère</li>
<li>for: 本研究旨在提出一种基于bird’s-eye view(BEV)的mask-based对象检测方法，以解决现有对象检测方法中的 anchor-based或anchor-free检测器需要显式知识来工作的局限性。</li>
<li>methods: 本方法采用一种BEV实例面积预测方法，预测出Object的footprint，同时实现对象检测和面积完成在一个过程中。此外，本方法将检测问题转换为纯粹的分类问题，减少了归一化问题。</li>
<li>results: 在SemanticKITTI和KITTI datasets上，MaskBEV方法实现了比较出色的性能，并且分析了方法的优势和局限性。<details>
<summary>Abstract</summary>
Recent works in object detection in LiDAR point clouds mostly focus on predicting bounding boxes around objects. This prediction is commonly achieved using anchor-based or anchor-free detectors that predict bounding boxes, requiring significant explicit prior knowledge about the objects to work properly. To remedy these limitations, we propose MaskBEV, a bird's-eye view (BEV) mask-based object detector neural architecture. MaskBEV predicts a set of BEV instance masks that represent the footprints of detected objects. Moreover, our approach allows object detection and footprint completion in a single pass. MaskBEV also reformulates the detection problem purely in terms of classification, doing away with regression usually done to predict bounding boxes. We evaluate the performance of MaskBEV on both SemanticKITTI and KITTI datasets while analyzing the architecture advantages and limitations.
</details>
<details>
<summary>摘要</summary>
现代对LiDAR点云中物体探测的研究主要集中在预测物体 bounding box 上。这种预测通常使用 anchor-based 或 anchor-free 探测器，需要明确对物体的知识来工作。为了解决这些限制，我们提议了 MaskBEV，一种 bird's-eye view（BEV）Mask-based 物体探测神经网络架构。MaskBEV 预测了 BEV 实例Mask，表示探测到的对象的脚印。此外，我们的方法可以同时进行物体探测和脚印完成。而且，MaskBEV 做了 bounding box 的预测，而不需要进行坐标 regression。我们在 SemanticKITTI 和 KITTI 数据集上评估了 MaskBEV 的性能，同时分析了架构的优势和局限性。
</details></li>
</ul>
<hr>
<h2 id="Crossway-Diffusion-Improving-Diffusion-based-Visuomotor-Policy-via-Self-supervised-Learning"><a href="#Crossway-Diffusion-Improving-Diffusion-based-Visuomotor-Policy-via-Self-supervised-Learning" class="headerlink" title="Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning"></a>Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01849">http://arxiv.org/abs/2307.01849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Varun Belagali, Jinghuan Shang, Michael S. Ryoo</li>
<li>for: 这篇论文旨在提高基于扩散过程的视OMOTION政策学习，通过添加一个自然语言supervised learning（SSL）目标函数。</li>
<li>methods: 该方法使用扩散过程生成动作序列，并使用一个新的解码器将拓展图像像素和其他状态信息重建为初始扩散过程的中间表示。</li>
<li>results: 实验表明，在多个 simulate 和实际世界机器人任务中，权威 diffusion 政策 Learning 效果更好，特别是当示例具有不同水平时。<details>
<summary>Abstract</summary>
Sequence modeling approaches have shown promising results in robot imitation learning. Recently, diffusion models have been adopted for behavioral cloning, benefiting from their exceptional capabilities in modeling complex data distribution. In this work, we propose Crossway Diffusion, a method to enhance diffusion-based visuomotor policy learning by using an extra self-supervised learning (SSL) objective. The standard diffusion-based policy generates action sequences from random noise conditioned on visual observations and other low-dimensional states. We further extend this by introducing a new decoder that reconstructs raw image pixels (and other state information) from the intermediate representations of the reverse diffusion process, and train the model jointly using the SSL loss. Our experiments demonstrate the effectiveness of Crossway Diffusion in various simulated and real-world robot tasks, confirming its advantages over the standard diffusion-based policy. We demonstrate that such self-supervised reconstruction enables better representation for policy learning, especially when the demonstrations have different proficiencies.
</details>
<details>
<summary>摘要</summary>
序列模型方法在机器人模仿学习中已经取得了成功。最近，扩散模型在行为假设学习中得到了广泛的应用，这主要是因为它们可以模型复杂的数据分布。在这项工作中，我们提议了跨度扩散（Crossway Diffusion）方法，用于增强扩散基于视听动作政策学习的方法。标准的扩散基于政策可以根据随机噪声和视觉观察等低维度状态生成动作序列。我们进一步延伸了这个方法，通过引入一个新的解码器，将拟合过程中的中间表示转换回原始的图像像素和其他状态信息，并在模型中同时使用SSL损失来训练。我们的实验表明，跨度扩散方法在多种模拟和真实世界机器人任务中具有优势，特别是当示例具有不同水平时。我们的结果表明，自我超级vised reconstruction可以为政策学习提供更好的表示，尤其是当示例有不同的技巧水平时。
</details></li>
</ul>
<hr>
<h2 id="Grad-FEC-Unequal-Loss-Protection-of-Deep-Features-in-Collaborative-Intelligence"><a href="#Grad-FEC-Unequal-Loss-Protection-of-Deep-Features-in-Collaborative-Intelligence" class="headerlink" title="Grad-FEC: Unequal Loss Protection of Deep Features in Collaborative Intelligence"></a>Grad-FEC: Unequal Loss Protection of Deep Features in Collaborative Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01846">http://arxiv.org/abs/2307.01846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Korcan Uyanik, S. Faegheh Yeganli, Ivan V. Bajić</li>
<li>for: 提高edge设备和云端AI模型的协同智能系统在Packet Loss情况下的可靠性和Robustness。</li>
<li>methods: 使用Unequal Loss Protection（ULP）approach，包括feature importance estimator和Forward Error Correction（FEC） codes， selectively保护front-end生成的重要数据包。</li>
<li>results: 实验结果表明，提议的方法可以在Packet Loss情况下明显提高协同智能系统的可靠性和Robustness。<details>
<summary>Abstract</summary>
Collaborative intelligence (CI) involves dividing an artificial intelligence (AI) model into two parts: front-end, to be deployed on an edge device, and back-end, to be deployed in the cloud. The deep feature tensors produced by the front-end are transmitted to the cloud through a communication channel, which may be subject to packet loss. To address this issue, in this paper, we propose a novel approach to enhance the resilience of the CI system in the presence of packet loss through Unequal Loss Protection (ULP). The proposed ULP approach involves a feature importance estimator, which estimates the importance of feature packets produced by the front-end, and then selectively applies Forward Error Correction (FEC) codes to protect important packets. Experimental results demonstrate that the proposed approach can significantly improve the reliability and robustness of the CI system in the presence of packet loss.
</details>
<details>
<summary>摘要</summary>
合作智能（CI）涉及将人工智能（AI）模型分成两部分：前端，部署在边缘设备上，和后端，部署在云端。深度特征tensor生成于前端被传输到云端通过通信频道，该频道可能会出现包loss。为 Addressing this issue, in this paper, we propose a novel approach to enhance the resilience of the CI system in the presence of packet loss through Unequal Loss Protection (ULP). The proposed ULP approach involves a feature importance estimator, which estimates the importance of feature packets produced by the front-end, and then selectively applies Forward Error Correction (FEC) codes to protect important packets. Experimental results demonstrate that the proposed approach can significantly improve the reliability and robustness of the CI system in the presence of packet loss.Here's the translation in Traditional Chinese:与我们的研究相关的文章，我们提出了一种新的方法来增强在包loss情况下的合作智能系统（CI）的可靠性和韧性。这个方法通过不等损伤保护（ULP）来实现。ULP方法包括一个特征重要度估计器，这个估计器可以估计前端生成的特征包中的重要度，然后选择性地对重要的包应用Forward Error Correction（FEC）代码进行保护。实验结果显示，提案的方法可以在包loss情况下对CI系统的可靠性和韧性进行明显改善。
</details></li>
</ul>
<hr>
<h2 id="Deep-Features-for-Contactless-Fingerprint-Presentation-Attack-Detection-Can-They-Be-Generalized"><a href="#Deep-Features-for-Contactless-Fingerprint-Presentation-Attack-Detection-Can-They-Be-Generalized" class="headerlink" title="Deep Features for Contactless Fingerprint Presentation Attack Detection: Can They Be Generalized?"></a>Deep Features for Contactless Fingerprint Presentation Attack Detection: Can They Be Generalized?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01845">http://arxiv.org/abs/2307.01845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hailin Li, Raghavendra Ramachandra</li>
<li>for: 这项研究旨在比较不同预训练的 convolutional neural network (CNN) 和 vision transformer (ViT) 在掌握展示攻击方面的一致性。</li>
<li>methods: 该研究使用了公共可用的智能手机展示攻击数据集进行了广泛的实验，使用了四种不同的展示攻击工具 (PAI)。</li>
<li>results: 研究发现，使用 ResNet50 CNN 的特征提取方法可以获得最佳一致性表现。<details>
<summary>Abstract</summary>
The rapid evolution of high-end smartphones with advanced high-resolution cameras has resulted in contactless capture of fingerprint biometrics that are more reliable and suitable for verification. Similar to other biometric systems, contactless fingerprint-verification systems are vulnerable to presentation attacks. In this paper, we present a comparative study on the generalizability of seven different pre-trained Convolutional Neural Networks (CNN) and a Vision Transformer (ViT) to reliably detect presentation attacks. Extensive experiments were carried out on publicly available smartphone-based presentation attack datasets using four different Presentation Attack Instruments (PAI). The detection performance of the eighth deep feature technique was evaluated using the leave-one-out protocol to benchmark the generalization performance for unseen PAI. The obtained results indicated the best generalization performance with the ResNet50 CNN.
</details>
<details>
<summary>摘要</summary>
高端智能手机的高分辨率摄像头快速进化，导致无接触式指纹识别系统更加可靠和适用于验证。与其他生物特征系统一样，无接触指纹验证系统也容易受到披露攻击。在这篇论文中，我们进行了七种不同预训练Convolutional Neural Networks（CNN）和一种Vision Transformer（ViT）的比较研究，以确定可靠地探测披露攻击。我们在公开available的智能手机披露攻击数据集上进行了广泛的实验，使用了四种不同的Presentation Attack Instrument（PAI）。我们使用了留一个数据点协议来评估深度特征技术的泛化性表现。结果显示，ResNet50 CNN表现最佳。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Wound-Filling-Extraction-on-3D-Faces-Auto-Segmentation-and-Wound-Face-Regeneration-Approach"><a href="#Advancing-Wound-Filling-Extraction-on-3D-Faces-Auto-Segmentation-and-Wound-Face-Regeneration-Approach" class="headerlink" title="Advancing Wound Filling Extraction on 3D Faces: Auto-Segmentation and Wound Face Regeneration Approach"></a>Advancing Wound Filling Extraction on 3D Faces: Auto-Segmentation and Wound Face Regeneration Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01844">http://arxiv.org/abs/2307.01844</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simogroup/woundfilling3d">https://github.com/simogroup/woundfilling3d</a></li>
<li>paper_authors: Duong Q. Nguyen, Thinh D. Le, Phuong D. Nguyen, Nga T. K. Le, H. Nguyen-Xuan</li>
<li>for: 这个研究旨在 automatizing 3D facial wound segmentation, 以帮助医生在不同的医疗应用中进行精确的预预操作规划和优化病人结果。</li>
<li>methods: 我们提出了一个高效的方法，使用两条流Graph Convolutional Network (GCN)来自动化3D facial wound segmentation。我们使用Cir3D-FaIR dataset，并通过不同的损失函数进行实验，以解决数据不均衡的问题。</li>
<li>results: 我们的方法可以实现高精度的3D facial wound segmentation，并且比前一代方法更高的精度。我们还提出了一个改进的方法来提取3D facial wound filler，并与前一代方法进行比较。我们的方法在试验组中取得了0.9999986%的准确率。<details>
<summary>Abstract</summary>
Facial wound segmentation plays a crucial role in preoperative planning and optimizing patient outcomes in various medical applications. In this paper, we propose an efficient approach for automating 3D facial wound segmentation using a two-stream graph convolutional network. Our method leverages the Cir3D-FaIR dataset and addresses the challenge of data imbalance through extensive experimentation with different loss functions. To achieve accurate segmentation, we conducted thorough experiments and selected a high-performing model from the trained models. The selected model demonstrates exceptional segmentation performance for complex 3D facial wounds. Furthermore, based on the segmentation model, we propose an improved approach for extracting 3D facial wound fillers and compare it to the results of the previous study. Our method achieved a remarkable accuracy of 0.9999986\% on the test suite, surpassing the performance of the previous method. From this result, we use 3D printing technology to illustrate the shape of the wound filling. The outcomes of this study have significant implications for physicians involved in preoperative planning and intervention design. By automating facial wound segmentation and improving the accuracy of wound-filling extraction, our approach can assist in carefully assessing and optimizing interventions, leading to enhanced patient outcomes. Additionally, it contributes to advancing facial reconstruction techniques by utilizing machine learning and 3D bioprinting for printing skin tissue implants. Our source code is available at \url{https://github.com/SIMOGroup/WoundFilling3D}.
</details>
<details>
<summary>摘要</summary>
facial 伤口分割扮演着重要的前Operative 规划和优化患者结果的关键角色在各种医疗应用中。在这篇论文中，我们提出了一种高效的自动化3D facial 伤口分割方法，使用了两�ream Graph Convolutional Network。我们的方法利用了Cir3D-FaIR数据集，并通过不同损失函数的实验处理数据不均衡问题。为了实现准确的分割，我们进行了详细的实验，并从已训练的模型中选择了高性能的模型。选择的模型在复杂的3D facial 伤口上显示了出色的分割性能。此外，基于分割模型，我们提出了一种改进的3D facial 伤口粘质EXTRACT方法，并与之前的研究比较。我们的方法在测试集上达到了0.9999986%的准确率，超过了之前的方法的性能。从这个结果来看，我们使用3D打印技术来 Illustrate the shape of the wound filling。这些结果对于参与预Operative 规划和 intervención设计的医生非常重要，因为它可以帮助精确评估和优化 intervención，导致患者的结果进一步提高。此外，它还对于提高面部重建技术的发展起到了作用，因为它可以使用机器学习和3D生物打印技术来打印皮肤组织嵌入物。我们的源代码可以在 \url{https://github.com/SIMOGroup/WoundFilling3D} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Score-Distillation-for-Consistent-Visual-Synthesis"><a href="#Collaborative-Score-Distillation-for-Consistent-Visual-Synthesis" class="headerlink" title="Collaborative Score Distillation for Consistent Visual Synthesis"></a>Collaborative Score Distillation for Consistent Visual Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04787">http://arxiv.org/abs/2307.04787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/subin-kim-cv/CSD">https://github.com/subin-kim-cv/CSD</a></li>
<li>paper_authors: Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, Jinwoo Shin</li>
<li>for: 提高文本到图像扩散模型的多样化应用</li>
<li>methods: 基于 Stein Variational Gradient Descent (SVGD) 的 Collaborative Score Distillation (CSD) 方法</li>
<li>results: 在多个图像样本中实现一致性，并在多种任务中展示了 CSD 的效果，如修改投影图像、视频和3D场景。<details>
<summary>Abstract</summary>
Generative priors of large-scale text-to-image diffusion models enable a wide range of new generation and editing applications on diverse visual modalities. However, when adapting these priors to complex visual modalities, often represented as multiple images (e.g., video), achieving consistency across a set of images is challenging. In this paper, we address this challenge with a novel method, Collaborative Score Distillation (CSD). CSD is based on the Stein Variational Gradient Descent (SVGD). Specifically, we propose to consider multiple samples as "particles" in the SVGD update and combine their score functions to distill generative priors over a set of images synchronously. Thus, CSD facilitates seamless integration of information across 2D images, leading to a consistent visual synthesis across multiple samples. We show the effectiveness of CSD in a variety of tasks, encompassing the visual editing of panorama images, videos, and 3D scenes. Our results underline the competency of CSD as a versatile method for enhancing inter-sample consistency, thereby broadening the applicability of text-to-image diffusion models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用大规模文本到图像扩散模型的生成积分可以开拓广泛的新生成和编辑应用程序。然而，当适应复杂的视觉模式时，如多个图像（例如视频），保证图像集中的一致性是挑战。在这篇论文中，我们解决这个挑战通过一种新方法：协同分数热退（CSD）。CSD基于斯泰因变量梯度下降（SVGD）。我们建议将多个样本视为“粒子”在SVGD更新中，并将其分数函数相结合，以降生生成积分过多个图像同步。因此，CSD可以快速地融合多个图像中的信息，从而实现一致的视觉生成。我们在多种任务中展示了CSD的效果，包括修改投影图像、视频和3D场景。我们的结果表明CSD是一种多功能的方法，可以提高图像集中的一致性，从而扩展文本到图像扩散模型的应用范围。
</details></li>
</ul>
<hr>
<h2 id="EdgeFace-Efficient-Face-Recognition-Model-for-Edge-Devices"><a href="#EdgeFace-Efficient-Face-Recognition-Model-for-Edge-Devices" class="headerlink" title="EdgeFace: Efficient Face Recognition Model for Edge Devices"></a>EdgeFace: Efficient Face Recognition Model for Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01838">http://arxiv.org/abs/2307.01838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjith George, Christophe Ecabert, Hatef Otroshi Shahreza, Ketan Kotwal, Sebastien Marcel</li>
<li>for: 这篇论文是为了开发一个轻量级的面部识别网络（EdgeFace），以便在边缘设备上进行面部识别。</li>
<li>methods: 这篇论文使用了一种混合式架构，兼容了CNN和Transformer模型，以及一个低维度线性层，实现了高品质的面部识别。</li>
<li>results: 实验结果显示，EdgeFace网络不仅具有低 Computational Complexity 和储存空间，而且可以在挑战性的资料集上实现高度的面部识别精度，并且较以前的有效网络模型表现更好。<details>
<summary>Abstract</summary>
In this paper, we present EdgeFace, a lightweight and efficient face recognition network inspired by the hybrid architecture of EdgeNeXt. By effectively combining the strengths of both CNN and Transformer models, and a low rank linear layer, EdgeFace achieves excellent face recognition performance optimized for edge devices. The proposed EdgeFace network not only maintains low computational costs and compact storage, but also achieves high face recognition accuracy, making it suitable for deployment on edge devices. Extensive experiments on challenging benchmark face datasets demonstrate the effectiveness and efficiency of EdgeFace in comparison to state-of-the-art lightweight models and deep face recognition models. Our EdgeFace model with 1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B (92.67%), and IJB-C (94.85%), outperforming other efficient models with larger computational complexities. The code to replicate the experiments will be made available publicly.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了EdgeFace，一种轻量级、高效的面recognition网络， Drawing inspiration from EdgeNeXt 的混合体系。 EdgeFace 能够 effectively 结合 CNN 和 Transformer 模型的优点，以及低级线性层，实现面recognition 性能的优化。我们的EdgeFace 网络不仅具有低计算成本和压缩存储，还能够实现高精度的面recognition，适用于边缘设备上部署。我们进行了大量的实验，并证明EdgeFace 在艰难的面数据集上具有优秀的表现，比如LFW（99.73%）、IJB-B（92.67%）和IJB-C（94.85%）等。与其他高效模型相比，EdgeFace 的1.77M 参数的模型可以达到状态码的表现。我们将会在公共领域中公开我们的代码，以便读者可以重新实现我们的实验。
</details></li>
</ul>
<hr>
<h2 id="On-the-Matrix-Form-of-the-Quaternion-Fourier-Transform-and-Quaternion-Convolution"><a href="#On-the-Matrix-Form-of-the-Quaternion-Fourier-Transform-and-Quaternion-Convolution" class="headerlink" title="On the Matrix Form of the Quaternion Fourier Transform and Quaternion Convolution"></a>On the Matrix Form of the Quaternion Fourier Transform and Quaternion Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01836">http://arxiv.org/abs/2307.01836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgos Sfikas, George Retsinas</li>
<li>for: 这篇论文探讨了四元数版本的傅立叶变换和卷积运算的矩阵形式。</li>
<li>methods: 论文使用了四元数的 Representation 单位，但由于四元数的非 коммуutat性，使得在四元数域中处理矩阵时存在许多复杂性。</li>
<li>results: 论文发现了四元数傅立叶变换矩阵与标准复数逐步变换矩阵之间的关系，以及许多已知的复数域理论是否能够推广到四元数域。<details>
<summary>Abstract</summary>
We study matrix forms of quaternionic versions of the Fourier Transform and Convolution operations. Quaternions offer a powerful representation unit, however they are related to difficulties in their use that stem foremost from non-commutativity of quaternion multiplication, and due to that $\mu^2 = -1$ posseses infinite solutions in the quaternion domain. Handling of quaternionic matrices is consequently complicated in several aspects (definition of eigenstructure, determinant, etc.). Our research findings clarify the relation of the Quaternion Fourier Transform matrix to the standard (complex) Discrete Fourier Transform matrix, and the extend on which well-known complex-domain theorems extend to quaternions. We focus especially on the relation of Quaternion Fourier Transform matrices to Quaternion Circulant matrices (representing quaternionic convolution), and the eigenstructure of the latter. A proof-of-concept application that makes direct use of our theoretical results is presented, where we produce a method to bound the spectral norm of a Quaternionic Convolution.
</details>
<details>
<summary>摘要</summary>
我们研究matrix形式的四元数版本的傅立叶变换和卷积操作。四元数提供了一个强大的表示单元，但它们具有非共轭 multiplication 的特点，导致在使用中存在许多问题。其中，由于 $\mu^2 = -1$ 在四元数域中有无穷多个解，这使得四元数矩阵的处理变得复杂。我们的研究发现，四元数傅立叶变换矩阵与标准（复数）离散傅立叶变换矩阵之间存在密切的关系，以及复数域中已知的定理是否可以推广到四元数域。我们特别关注四元数傅立叶变换矩阵与四元数卷积矩阵（表示四元数卷积）之间的关系，以及它们的特征值结构。我们还提出了一种基于我们的理论成果的应用，可以直接 bound 四元数卷积的 спектраль нор。
</details></li>
</ul>
<hr>
<h2 id="SUIT-Learning-Significance-guided-Information-for-3D-Temporal-Detection"><a href="#SUIT-Learning-Significance-guided-Information-for-3D-Temporal-Detection" class="headerlink" title="SUIT: Learning Significance-guided Information for 3D Temporal Detection"></a>SUIT: Learning Significance-guided Information for 3D Temporal Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01807">http://arxiv.org/abs/2307.01807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Zhou, Jiachen Lu, Yihan Zeng, Hang Xu, Li Zhang</li>
<li>for: 提高自动驾驶和机器人三元素检测的精度，尤其是利用时间信息来提高3D感知。</li>
<li>methods: 提出了Significance-gUided Information for 3D Temporal detection（SUIT）方法，通过采样机制提取信息充沛的特征，并使用explicit geometric transformation learning技术来学习物体中心点之间的变换。</li>
<li>results: 在nuScenes和Waymo dataset上评估了SUIT方法，并证明了它不仅可以减少时间拼接的内存和计算成本，而且也可以与现有基线方法相比而言性能较高。<details>
<summary>Abstract</summary>
3D object detection from LiDAR point cloud is of critical importance for autonomous driving and robotics. While sequential point cloud has the potential to enhance 3D perception through temporal information, utilizing these temporal features effectively and efficiently remains a challenging problem. Based on the observation that the foreground information is sparsely distributed in LiDAR scenes, we believe sufficient knowledge can be provided by sparse format rather than dense maps. To this end, we propose to learn Significance-gUided Information for 3D Temporal detection (SUIT), which simplifies temporal information as sparse features for information fusion across frames. Specifically, we first introduce a significant sampling mechanism that extracts information-rich yet sparse features based on predicted object centroids. On top of that, we present an explicit geometric transformation learning technique, which learns the object-centric transformations among sparse features across frames. We evaluate our method on large-scale nuScenes and Waymo dataset, where our SUIT not only significantly reduces the memory and computation cost of temporal fusion, but also performs well over the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
三维 объек物检测从 LiDAR 点云是自动驾驶和 робо技术中的关键问题。虽然时序点云具有提高三维见解的潜在优势，但是利用这些时序特征有效和高效地处理仍然是一个挑战。基于 LiDAR 场景中前景信息稀疏的观察，我们认为可以通过稀疏格式提供足够的知识。为此，我们提出了 Significance-gUided Information for 3D Temporal detection（SUIT），它将时序信息简化为稀疏特征 для 信息融合。具体来说，我们首先引入一种重要采样机制，将 predicted object centroids 中的信息富集 yet sparse 特征提取出来。然后，我们提出了一种显式的几何变换学习技术，这种技术学习 object-centric 几何变换 among sparse features across frames。我们对大规模的 nuScenes 和 Waymo 数据集进行了评估，我们的 SUIT 不仅能够减少时间和计算成本，而且也能够与当前的基线方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Edge-aware-Multi-task-Network-for-Integrating-Quantification-Segmentation-and-Uncertainty-Prediction-of-Liver-Tumor-on-Multi-modality-Non-contrast-MRI"><a href="#Edge-aware-Multi-task-Network-for-Integrating-Quantification-Segmentation-and-Uncertainty-Prediction-of-Liver-Tumor-on-Multi-modality-Non-contrast-MRI" class="headerlink" title="Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI"></a>Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01798">http://arxiv.org/abs/2307.01798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojiao Xiao, Qinmin Hu, Guanghui Wang<br>for:This paper aims to provide a unified framework for simultaneous multi-index quantification, segmentation, and uncertainty estimation of liver tumors on multi-modality non-contrast magnetic resonance imaging (NCMRI).methods:The proposed method, called edge-aware multi-task network (EaMtNet), employs two parallel CNN encoders, Sobel filters, and a newly designed edge-aware feature aggregation module (EaFA) for feature fusion and selection. The network also uses multi-tasking to leverage prediction discrepancy for uncertainty estimation and improved segmentation and quantification performance.results:The proposed EaMtNet outperforms the state-of-the-art by a large margin, achieving a dice similarity coefficient of 90.01$\pm$1.23 and a mean absolute error of 2.72$\pm$0.58 mm for MD. The results demonstrate the potential of EaMtNet as a reliable clinical-aided tool for medical image analysis.Here is the same information in Simplified Chinese:for:这篇论文目的是提供一个统一的框架，用于同时进行多指标量化、分割和不确定性估计liver肿瘤在多Modal非对照磁共振成像（NCMRI）中。methods:提议的方法是edge-aware multi-task network（EaMtNet），它使用了两个平行的CNN编码器， Sobel滤波器和一个新设计的边缘感知特征聚合模块（EaFA）来抽取本地特征和边图，并实现了特征融合和选择。网络还使用了多任务学习，以利用预测差异来估计uncertainty和提高分割和量化性能。results:提议的EaMtNet比STATE-OF-THE-ART大幅提高，达到了dice相似度为90.01$\pm$1.23和MD的mean absolute error为2.72$\pm$0.58 mm。结果表明EaMtNet可能成为一种可靠的临床助手工具，用于医疗图像分析。<details>
<summary>Abstract</summary>
Simultaneous multi-index quantification, segmentation, and uncertainty estimation of liver tumors on multi-modality non-contrast magnetic resonance imaging (NCMRI) are crucial for accurate diagnosis. However, existing methods lack an effective mechanism for multi-modality NCMRI fusion and accurate boundary information capture, making these tasks challenging. To address these issues, this paper proposes a unified framework, namely edge-aware multi-task network (EaMtNet), to associate multi-index quantification, segmentation, and uncertainty of liver tumors on the multi-modality NCMRI. The EaMtNet employs two parallel CNN encoders and the Sobel filters to extract local features and edge maps, respectively. The newly designed edge-aware feature aggregation module (EaFA) is used for feature fusion and selection, making the network edge-aware by capturing long-range dependency between feature and edge maps. Multi-tasking leverages prediction discrepancy to estimate uncertainty and improve segmentation and quantification performance. Extensive experiments are performed on multi-modality NCMRI with 250 clinical subjects. The proposed model outperforms the state-of-the-art by a large margin, achieving a dice similarity coefficient of 90.01$\pm$1.23 and a mean absolute error of 2.72$\pm$0.58 mm for MD. The results demonstrate the potential of EaMtNet as a reliable clinical-aided tool for medical image analysis.
</details>
<details>
<summary>摘要</summary>
simultaneous 多指标评估、分割和不确定性估计liver肿瘤在多Modal非对照核磁共振成像（NCMRI）是诊断精准的关键。然而，现有方法缺乏有效的多Modal NCMRI融合机制和准确边缘信息捕获机制，使这些任务变得困难。为解决这些问题，本文提出了一个统一框架，即edge-aware multi-task network（EaMtNet），将多指标评估、分割和不确定性 associates with liver tumors on multi-modal NCMRI。EaMtNet使用两个平行的CNNEncoder和Sobel滤波器提取本地特征和边图像，并使用 newly designed edge-aware feature aggregation module（EaFA）进行特征融合和选择，使网络变得边缘意识。多任务学习利用预测差异来估计不确定性和提高分割和评估性能。对多Modal NCMRI的250个临床Subject进行了广泛的实验。提出的模型在与状态艺的比较中大幅度超越了当前最佳方法，实现了 dice similarity coefficient的90.01±1.23和mean absolute error的2.72±0.58mm for MD。结果表明EaMtNet具有可靠的临床助手工具的潜力。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/05/cs.CV_2023_07_05/" data-id="clpahu71y00fs3h8858nw6hmn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/cs.AI_2023_07_05/" class="article-date">
  <time datetime="2023-07-05T12:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/05/cs.AI_2023_07_05/">cs.AI - 2023-07-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Scaling-Laws-Do-Not-Scale"><a href="#Scaling-Laws-Do-Not-Scale" class="headerlink" title="Scaling Laws Do Not Scale"></a>Scaling Laws Do Not Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03201">http://arxiv.org/abs/2307.03201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MarkipTheMudkip/in-class-project-2">https://github.com/MarkipTheMudkip/in-class-project-2</a></li>
<li>paper_authors: Fernando Diaz, Michael Madaio</li>
<li>for: 本文探讨了人工智能（AI）模型的性能与模型设计参数之间的关系，即“扩大法律”（scaling laws）。</li>
<li>methods: 本文使用了现有的数据集和模型参数的大小来描述AI模型的性能，并分析了metric使用的困难和不稳定性。</li>
<li>results: 根据本文的分析，随着数据集的大小增加，模型的性能会随着数据集的大小增加，但是不同社区的人们可能对模型的输出质量有不同的评价标准，这可能会导致模型的性能不会在所有社区中提高。<details>
<summary>Abstract</summary>
Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or preferences not captured by (or in the worst case, at odds with) the metrics used to evaluate model performance for scaling laws. We end the paper with implications for AI scaling laws -- that models may not, in fact, continue to improve as the datasets get larger -- at least not for all people or communities impacted by those models.
</details>
<details>
<summary>摘要</summary>
As the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in the dataset is likely to increase. Each of these communities may have different values and preferences, which may not be captured by the metrics used to evaluate model performance. As a result, there is a risk that the communities represented in the dataset may have values or preferences that are not aligned with the metrics used to evaluate the performance of the AI model.We conclude the paper with implications for AI scaling laws, suggesting that models may not continue to improve as the size of the datasets increases for all people or communities impacted by the models.
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Data-Governance-as-Part-of-a-Data-Mesh-Platform-Concepts-and-Approaches"><a href="#Decentralized-Data-Governance-as-Part-of-a-Data-Mesh-Platform-Concepts-and-Approaches" class="headerlink" title="Decentralized Data Governance as Part of a Data Mesh Platform: Concepts and Approaches"></a>Decentralized Data Governance as Part of a Data Mesh Platform: Concepts and Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02357">http://arxiv.org/abs/2307.02357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arif Wider, Sumedha Verma, Atif Akhtar</li>
<li>for: 这篇论文是关于数据网格（Data Mesh）的一种社会技术方法，用于分布式数据分析数据管理。</li>
<li>methods: 该论文提出了一种自助数据基础设施平台，以实现分布式数据管理的自动化。这个平台还提供了分布式数据管理的自治机制。</li>
<li>results: 该论文提出了一种数据网格概念模型，并讨论了如何通过平台手段实现数据管理的自治。这些结论基于实际实现了一个可用的数据网格平台的经验。<details>
<summary>Abstract</summary>
Data mesh is a socio-technical approach to decentralized analytics data management. To manage this decentralization efficiently, data mesh relies on automation provided by a self-service data infrastructure platform. A key aspect of this platform is to enable decentralized data governance. Because data mesh is a young approach, there is a lack of coherence in how data mesh concepts are interpreted in the industry, and almost no work on how a data mesh platform facilitates governance. This paper presents a conceptual model of key data mesh concepts and discusses different approaches to drive governance through platform means. The insights presented are drawn from concrete experiences of implementing a fully-functional data mesh platform that can be used as a reference on how to approach data mesh platform development.
</details>
<details>
<summary>摘要</summary>
“数据网”是一种社会技术方法，用于分布式分析数据管理。为了有效地管理这种分布，数据网依赖于自助数据基础设施平台的自动化。该平台的一个关键方面是实现分布式数据管理。由于数据网是一种新的方法，因此在行业中对数据网概念的解释存在一定的不一致，并且几乎没有关于如何通过平台手段实现管理的研究。本文提出了一个概念模型，描述了不同的方法来驱动管理通过平台手段。这些发现来自实际实施了可用于参考的完整功能数据网平台的经验。
</details></li>
</ul>
<hr>
<h2 id="LLQL-Logistic-Likelihood-Q-Learning-for-Reinforcement-Learning"><a href="#LLQL-Logistic-Likelihood-Q-Learning-for-Reinforcement-Learning" class="headerlink" title="LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning"></a>LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02345">http://arxiv.org/abs/2307.02345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Outongyi Lv, Bingxin Zhou, Yu Guang Wang</li>
<li>for: 本研究主要针对在线和离线学习环境下的 Bellman 误差分布性 analyzed。</li>
<li>methods: 我们使用了 Logistic 分布和受限 Logistic 分布来描述在线和离线环境下 Bellman 误差的分布性。</li>
<li>results: 我们发现在线环境下 Bellman 误差遵循 Logistic 分布，而离线环境下 Bellman 误差遵循受限 Logistic 分布，其受限分布取决于离线数据集中的先前策略。基于这些发现，我们提出了一种基于 Logistic 最大似然函数的 $\rm LLoss$ 替代损失函数。我们还发现了在离线数据集中的奖励应该遵循特定的分布，以便实现离线目标。在我们的数据分析中，我们对 Soft-Actor-Critic 算法的两个变种在线和离线环境中进行了控制变量修正。结果证明了我们的假设，并发现 LLoss 的方差比 MSELoss 更小。<details>
<summary>Abstract</summary>
Currently, research on Reinforcement learning (RL) can be broadly classified into two categories: online RL and offline RL. Both in online and offline RL, the primary focus of research on the Bellman error lies in the optimization techniques and performance improvement, rather than exploring the inherent structural properties of the Bellman error, such as distribution characteristics. In this study, we analyze the distribution of the Bellman approximation error in both online and offline settings. We find that in the online environment, the Bellman error follows a Logistic distribution, while in the offline environment, the Bellman error follows a constrained Logistic distribution, where the constrained distribution is dependent on the prior policy in the offline data set. Based on this finding, we have improved the MSELoss which is based on the assumption that the Bellman errors follow a normal distribution, and we utilized the Logistic maximum likelihood function to construct $\rm LLoss$ as an alternative loss function. In addition, we observed that the rewards in the offline data set should follow a specific distribution, which would facilitate the achievement of offline objectives. In our numerical experiments, we performed controlled variable corrections on the loss functions of two variants of Soft-Actor-Critic in both online and offline environments. The results confirmed our hypothesis regarding the online and offline settings, we also found that the variance of LLoss is smaller than MSELoss. Our research provides valuable insights for further investigations based on the distribution of Bellman errors.
</details>
<details>
<summary>摘要</summary>
当前研究的束缚学习（RL）可以分为两类：在线RL和离线RL。在线和离线RL中，研究者们主要关注优化技术和性能提升，而不是探索 Bellman 误差的内在结构特性，如分布特性。在这项研究中，我们分析了在线和离线环境中 Bellman  aproximation 误差的分布。我们发现在在线环境中，Bellman 误差遵循 Logistic 分布，而在离线环境中，Bellman 误差遵循受限的 Logistic 分布，该分布取决于离线数据集中的先前策略。根据这一发现，我们改进了基于 assume Bellman 误差遵循正态分布的 MSELoss，并使用 Logistic 最大可能函数来构建 LLoss 作为替代损失函数。此外，我们发现在离线数据集中的奖励应遵循特定分布，这将有助于实现离线目标。在我们的数值实验中，我们对 MSELoss 和 LLoss 的损失函数进行了控制变量修正，并在在线和离线环境中对两种 Soft-Actor-Critic 的损失函数进行了控制变量修正。结果证明了我们的假设，并发现 LLoss 的方差比 MSELoss 更小。我们的研究提供了有价值的发现，供后续基于 Bellman 误差分布的研究。
</details></li>
</ul>
<hr>
<h2 id="Exploring-new-ways-Enforcing-representational-dissimilarity-to-learn-new-features-and-reduce-error-consistency"><a href="#Exploring-new-ways-Enforcing-representational-dissimilarity-to-learn-new-features-and-reduce-error-consistency" class="headerlink" title="Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency"></a>Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02516">http://arxiv.org/abs/2307.02516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tassilo Wald, Constantin Ulrich, Fabian Isensee, David Zimmerer, Gregor Koehler, Michael Baumgartner, Klaus H. Maier-Hein</li>
<li>for: 本研究旨在提高独立训练的机器学习模型的ensemble准确率，通过在不同架构层次上使用表示相似场方法来促进不同的表示之间的不同。</li>
<li>methods: 本研究使用了独立训练的多个机器学习模型，并通过提升不同层次的中间表示之间的不同来促进不同的表示之间的不同。</li>
<li>results: 研究发现，通过使用表示相似场方法来促进不同层次的中间表示之间的不同，可以提高ensemble的准确率，并且输出预测的相关性降低。<details>
<summary>Abstract</summary>
Independently trained machine learning models tend to learn similar features. Given an ensemble of independently trained models, this results in correlated predictions and common failure modes. Previous attempts focusing on decorrelation of output predictions or logits yielded mixed results, particularly due to their reduction in model accuracy caused by conflicting optimization objectives. In this paper, we propose the novel idea of utilizing methods of the representational similarity field to promote dissimilarity during training instead of measuring similarity of trained models. To this end, we promote intermediate representations to be dissimilar at different depths between architectures, with the goal of learning robust ensembles with disjoint failure modes. We show that highly dissimilar intermediate representations result in less correlated output predictions and slightly lower error consistency, resulting in higher ensemble accuracy. With this, we shine first light on the connection between intermediate representations and their impact on the output predictions.
</details>
<details>
<summary>摘要</summary>
独立训练的机器学习模型通常会学习类似的特征。给定一个独立训练的模型集合，这会导致相关的预测和共同失败模式。先前的尝试，包括对输出预测或搜索的减 correlate 的方法，得到了杂合的结果，特别是因为它们在优化目标之间矛盾。在这篇论文中，我们提出了一个新的想法，即利用表示相似场的方法来促进训练期间的不相似性。为此，我们在不同的架构之间Promote 中间表示的不同程度，以达到学习强健的集成模型，并且使得输出预测更加不相关。我们显示，高度不同的中间表示会导致输出预测更加不相关，并且错误一致率略低，从而提高集成模型的准确率。这样，我们首次照明了中间表示与其影响输出预测的关系。
</details></li>
</ul>
<hr>
<h2 id="Deep-Contract-Design-via-Discontinuous-Piecewise-Affine-Neural-Networks"><a href="#Deep-Contract-Design-via-Discontinuous-Piecewise-Affine-Neural-Networks" class="headerlink" title="Deep Contract Design via Discontinuous Piecewise Affine Neural Networks"></a>Deep Contract Design via Discontinuous Piecewise Affine Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02318">http://arxiv.org/abs/2307.02318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tonghan Wang, Paul Dütting, Dmitry Ivanov, Inbal Talgam-Cohen, David C. Parkes</li>
<li>for: 本文研究用深度学习自动设计优化合同。</li>
<li>methods: 本文使用深度网络表示主体对合同设计的预期收益函数，并引入了一种新的表示方式：离散ReLU（DeLU）网络，该网络模型主体的收益函数为散点函数，每个散点对应代理人行动。DeLU网络隐式地学习了代理人的奖励一致约束和主体的收益最大化目标，并支持并行推理每个散点上的线性 программирова或内点法解决优化合同问题。</li>
<li>results: 本文通过实验表明，可以使用一小数量的训练样本和扩展到大规模问题解决优化合同问题，并成功地估算主体的收益函数。<details>
<summary>Abstract</summary>
Contract design involves a principal who establishes contractual agreements about payments for outcomes that arise from the actions of an agent. In this paper, we initiate the study of deep learning for the automated design of optimal contracts. We formulate this as an offline learning problem, where a deep network is used to represent the principal's expected utility as a function of the design of a contract. We introduce a novel representation: the Discontinuous ReLU (DeLU) network, which models the principal's utility as a discontinuous piecewise affine function where each piece corresponds to the agent taking a particular action. DeLU networks implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal, and support parallel inference on each piece through linear programming or interior-point methods that solve for optimal contracts. We provide empirical results that demonstrate success in approximating the principal's utility function with a small number of training samples and scaling to find approximately optimal contracts on problems with a large number of actions and outcomes.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese as follows:CONTRACT 设计包括一个主体，该主体确定基于代理人的行为的支付方式。在这篇论文中，我们开始使用深度学习来自动设计优化的合同。我们将这视为一个离线学习问题，其中深度网络用于表示主体的预期用户函数，该函数与合同的设计相关。我们引入了一个新的表示方式：离散ReLU（DeLU）网络，该网络模型了主体的用户函数为离散凸函数，其中每个部分对应代理人行为的不同。DeLU网络隐式地学习closed-form表达代理人的奖励兼容性约束和主体的用户最大化目标，并支持并行推理每个部分的Linear Programming或内部点法解决优化合同。我们提供了实验结果，证明了使用小量训练样本和扩展可以相似地 aproximate主体的用户函数，并在具有大量行动和结果的问题上找到相对优化的合同。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-Adversarial-Bandit-Algorithms"><a href="#Meta-Learning-Adversarial-Bandit-Algorithms" class="headerlink" title="Meta-Learning Adversarial Bandit Algorithms"></a>Meta-Learning Adversarial Bandit Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02295">http://arxiv.org/abs/2307.02295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Khodak, Ilya Osadchiy, Keegan Harris, Maria-Florina Balcan, Kfir Y. Levy, Ron Meir, Zhiwei Steven Wu</li>
<li>for: 该论文目标在于在多个任务之间提高性能，使得任务之间存在自然的相似性度量。</li>
<li>methods: 论文使用外部学习算法同时调整内部学习算法的初始化和其他超参数，包括多重针对点投机（MAB）和带均值线性优化（BLO）两种重要的情况。</li>
<li>results: 论文表明，在MAB中，外部学习算法可以初始化和调整 Tsallis-entropy 泛化 Exp3 算法，使得任务平均误差降低。在BLO中，论文学习初始化和调整在线镜像 DESC 算法，并证明任务平均误差与动作空间依赖的度量直接相关。<details>
<summary>Abstract</summary>
We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two levels of low-dimensional hyperparameter tuning is enough to learn a sequence of affine functions of non-Lipschitz and sometimes non-convex Bregman divergences bounding the regret of OMD.
</details>
<details>
<summary>摘要</summary>
我们研究在线meta学习，使用反馈强制，以提高多个任务的性能，如果它们按照自然的相似度度量相似。作为首次针对在线内部partial-information设置下进行反馈强制的研究，我们设计了meta算法， combinest outer learner来同时调整内部学习器的初始化和其他超参数。对于多重抓拍机（MAB）和bandit线性优化（BLO）两个重要情况，我们的meta算法可以在不同的任务上实现性能提高。对于MAB，我们使用 Tsallis-entropy泛化的Exp3算法作为内部学习器，并将任务平均违假 regret的下降与Entropy of optima-in-hindsight的小化直接相关。对于BLO，我们学习了在线镜像下降（OMD）的初始化和调整，并证明任务平均违假 regret与动作空间依赖的度量相关。我们的保证基于证明，不含regularizer的follow-the-leadercombined with two levels of low-dimensional hyperparameter tuning是可以学习一个序列 Affine functions of non-Lipschitz和有时非 convex Bregman divergences，从而约束OMD的违假 regret。
</details></li>
</ul>
<hr>
<h2 id="First-Explore-then-Exploit-Meta-Learning-Intelligent-Exploration"><a href="#First-Explore-then-Exploit-Meta-Learning-Intelligent-Exploration" class="headerlink" title="First-Explore, then Exploit: Meta-Learning Intelligent Exploration"></a>First-Explore, then Exploit: Meta-Learning Intelligent Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02276">http://arxiv.org/abs/2307.02276</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/btnorman/First-Explore">https://github.com/btnorman/First-Explore</a></li>
<li>paper_authors: Ben Norman, Jeff Clune</li>
<li>for: 这篇论文的目的是解决人工智能学习控制器不具备人类级别的探索能力的问题。</li>
<li>methods: 这篇论文提出了一种新的meta-RL框架，叫做First-Explore，它包括两个策略：一个策略专门用于探索，另一个策略专门用于利用。在训练后，我们可以通过探索策略进行探索，直到所需的时候，然后使用所有在探索过程中获得的信息进行利用。这种方法可以避免在同时进行探索和利用时的冲突。</li>
<li>results: 论文表明，First-Explore可以学习人类级别的探索策略，并且在需要牺牲奖励的探索领域表现更好。First-Explore也超越了现有的标准RL和meta-RL方法。<details>
<summary>Abstract</summary>
Standard reinforcement learning (RL) agents never intelligently explore like a human (i.e. by taking into account complex domain priors and previous explorations). Even the most basic intelligent exploration strategies such as exhaustive search are only inefficiently or poorly approximated by approaches such as novelty search or intrinsic motivation, let alone more complicated strategies like learning new skills, climbing stairs, opening doors, or conducting experiments. This lack of intelligent exploration limits sample efficiency and prevents solving hard exploration domains. We argue a core barrier prohibiting many RL approaches from learning intelligent exploration is that the methods attempt to explore and exploit simultaneously, which harms both exploration and exploitation as the goals often conflict. We propose a novel meta-RL framework (First-Explore) with two policies: one policy learns to only explore and one policy learns to only exploit. Once trained, we can then explore with the explore policy, for as long as desired, and then exploit based on all the information gained during exploration. This approach avoids the conflict of trying to do both exploration and exploitation at once. We demonstrate that First-Explore can learn intelligent exploration strategies such as exhaustive search and more, and that it outperforms dominant standard RL and meta-RL approaches on domains where exploration requires sacrificing reward. First-Explore is a significant step towards creating meta-RL algorithms capable of learning human-level exploration which is essential to solve challenging unseen hard-exploration domains.
</details>
<details>
<summary>摘要</summary>
标准强化学习（RL）代理不像人类一样智能探索（即考虑复杂的领域假设和先前的探索）。 Even the most basic intelligent exploration strategies, such as exhaustive search, are only inefficiently or poorly approximated by approaches such as novelty search or intrinsic motivation, let alone more complicated strategies like learning new skills, climbing stairs, opening doors, or conducting experiments. This lack of intelligent exploration limits sample efficiency and prevents solving hard exploration domains. We argue that a core barrier preventing many RL approaches from learning intelligent exploration is that the methods attempt to explore and exploit simultaneously, which harms both exploration and exploitation as the goals often conflict.We propose a novel meta-RL framework (First-Explore) with two policies: one policy learns to only explore, and one policy learns to only exploit. Once trained, we can then explore with the explore policy for as long as desired, and then exploit based on all the information gained during exploration. This approach avoids the conflict of trying to do both exploration and exploitation at once. We demonstrate that First-Explore can learn intelligent exploration strategies such as exhaustive search and more, and that it outperforms dominant standard RL and meta-RL approaches on domains where exploration requires sacrificing reward. First-Explore is a significant step towards creating meta-RL algorithms capable of learning human-level exploration, which is essential to solve challenging unseen hard-exploration domains.
</details></li>
</ul>
<hr>
<h2 id="SVDM-Single-View-Diffusion-Model-for-Pseudo-Stereo-3D-Object-Detection"><a href="#SVDM-Single-View-Diffusion-Model-for-Pseudo-Stereo-3D-Object-Detection" class="headerlink" title="SVDM: Single-View Diffusion Model for Pseudo-Stereo 3D Object Detection"></a>SVDM: Single-View Diffusion Model for Pseudo-Stereo 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02270">http://arxiv.org/abs/2307.02270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuguang Shi</li>
<li>for: 提高LiDAR和摄像头方法之间准确性差距的3D物体探测方法</li>
<li>methods: 使用Pseudo-Stereo和单视扩散模型（SVDM），实现端到端、高效的3D探测框架</li>
<li>results: 在KITTI dataset上达到新的州OF-THE-ART性能，并且与多种静止探测器兼容<details>
<summary>Abstract</summary>
One of the key problems in 3D object detection is to reduce the accuracy gap between methods based on LiDAR sensors and those based on monocular cameras. A recently proposed framework for monocular 3D detection based on Pseudo-Stereo has received considerable attention in the community. However, so far these two problems are discovered in existing practices, including (1) monocular depth estimation and Pseudo-Stereo detector must be trained separately, (2) Difficult to be compatible with different stereo detectors and (3) the overall calculation is large, which affects the reasoning speed. In this work, we propose an end-to-end, efficient pseudo-stereo 3D detection framework by introducing a Single-View Diffusion Model (SVDM) that uses a few iterations to gradually deliver right informative pixels to the left image. SVDM allows the entire pseudo-stereo 3D detection pipeline to be trained end-to-end and can benefit from the training of stereo detectors. Afterwards, we further explore the application of SVDM in depth-free stereo 3D detection, and the final framework is compatible with most stereo detectors. Among multiple benchmarks on the KITTI dataset, we achieve new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Monocular depth estimation and Pseudo-Stereo detector must be trained separately.2. Difficulty in compatibility with different stereo detectors.3. The overall calculation is large, which affects the reasoning speed.In this work, we propose an end-to-end, efficient pseudo-stereo 3D detection framework by introducing a Single-View Diffusion Model (SVDM) that uses a few iterations to gradually deliver right informative pixels to the left image. SVDM allows the entire pseudo-stereo 3D detection pipeline to be trained end-to-end and can benefit from the training of stereo detectors. Additionally, we explore the application of SVDM in depth-free stereo 3D detection, and the final framework is compatible with most stereo detectors. Our experiments on the KITTI dataset achieve new state-of-the-art performance.</details></li>
</ol>
<hr>
<h2 id="Analyzing-Different-Expert-Opined-Strategies-to-Enhance-the-Effect-on-the-Goal-of-a-Multi-Attribute-Decision-Making-System-Using-a-Concept-of-Effort-Propagation-and-Application-in-Enhancement-of-High-School-Students’-Performance"><a href="#Analyzing-Different-Expert-Opined-Strategies-to-Enhance-the-Effect-on-the-Goal-of-a-Multi-Attribute-Decision-Making-System-Using-a-Concept-of-Effort-Propagation-and-Application-in-Enhancement-of-High-School-Students’-Performance" class="headerlink" title="Analyzing Different Expert-Opined Strategies to Enhance the Effect on the Goal of a Multi-Attribute Decision-Making System Using a Concept of Effort Propagation and Application in Enhancement of High School Students’ Performance"></a>Analyzing Different Expert-Opined Strategies to Enhance the Effect on the Goal of a Multi-Attribute Decision-Making System Using a Concept of Effort Propagation and Application in Enhancement of High School Students’ Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02254">http://arxiv.org/abs/2307.02254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suvojit Dhara, Adrijit Goswami</li>
<li>for: 本研究的目的是提出并分析多 attribute 决策问题中的两种策略，即并行和层次努力分配策略，以及传播策略。</li>
<li>methods: 本研究使用了一种称为努力传播的概念，并对两种策略进行了分析和讨论。分析表明，在一个实际的高中管理系统中，可以通过合理地分配努力来提高学生表现。</li>
<li>results: 研究发现，在一个实际的高中管理系统中，可以通过合理地分配努力来提高学生表现，并且可以达到约7%-15%的总努力传播。此外，研究还发现了一种最佳策略，可以帮助提高学生表现最大化。<details>
<summary>Abstract</summary>
In many real-world multi-attribute decision-making (MADM) problems, mining the inter-relationships and possible hierarchical structures among the factors are considered to be one of the primary tasks. But, besides that, one major task is to determine an optimal strategy to work on the factors to enhance the effect on the goal attribute. This paper proposes two such strategies, namely parallel and hierarchical effort assignment, and propagation strategies. The concept of effort propagation through a strategy is formally defined and described in the paper. Both the parallel and hierarchical strategies are divided into sub-strategies based on whether the assignment of efforts to the factors is uniform or depends upon some appropriate heuristics related to the factors in the system. The adapted and discussed heuristics are the relative significance and effort propagability of the factors. The strategies are analyzed for a real-life case study regarding Indian high school administrative factors that play an important role in enhancing students' performance. Total effort propagation of around 7%-15% to the goal is seen across the proposed strategies given a total of 1 unit of effort to the directly accessible factors of the system. A comparative analysis is adapted to determine the optimal strategy among the proposed ones to enhance student performance most effectively. The highest effort propagation achieved in the work is approximately 14.4348%. The analysis in the paper establishes the necessity of research towards the direction of effort propagation analysis in case of decision-making problems.
</details>
<details>
<summary>摘要</summary>
在许多实际多属性决策（MADM）问题中，挖掘因素之间的关系和可能的层次结构是一项重要任务。然而，除了这一点，另一个重要任务是确定一个优化策略，以便在因素上增强目标属性的效应。本文提出了两种策略，namely parallel和层次努力分配策略，以及宣传策略。在策略中，努力的传播是正式地定义和描述的。两种策略都被分为不同的子策略，根据努力分配到因素的方式是不同的。adapted和讨论的启发是因素相关的相对重要性和努力传播可能性。这些策略在印度高中学生表现的一个实际案例中进行了分析，并观察到了约7%-15%的总努力传播到目标。通过对这些策略进行比较分析，确定了最佳策略以提高学生表现的方式。在工作中，最高的努力传播率为约14.4348%。这种分析确立了努力传播分析在决策问题中的必要性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Multimodal-Approaches-for-Alzheimer’s-Disease-Detection-Using-Patient-Speech-Transcript-and-Audio-Data"><a href="#Exploring-Multimodal-Approaches-for-Alzheimer’s-Disease-Detection-Using-Patient-Speech-Transcript-and-Audio-Data" class="headerlink" title="Exploring Multimodal Approaches for Alzheimer’s Disease Detection Using Patient Speech Transcript and Audio Data"></a>Exploring Multimodal Approaches for Alzheimer’s Disease Detection Using Patient Speech Transcript and Audio Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02514">http://arxiv.org/abs/2307.02514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shui-dun/multimodal_ad">https://github.com/shui-dun/multimodal_ad</a></li>
<li>paper_authors: Hongmin Cai, Xiaoke Huang, Zhengliang Liu, Wenxiong Liao, Haixing Dai, Zihao Wu, Dajiang Zhu, Hui Ren, Quanzheng Li, Tianming Liu, Xiang Li</li>
<li>for: 这个研究旨在检测阿尔茨海默症（AD）使用患者的语音和文本数据，以提高患者的诊断和评估。</li>
<li>methods: 该研究使用预训练的语言模型和图 neural network（GNN）构建语音和文本数据中的图，提取特征以进行AD检测。此外，还使用了数据增强技术，包括同义词替换和GPT基于的增强器，以 Address the small dataset size issue。</li>
<li>results: 研究人员通过了intsensive的实验和分析，发现了对AD检测使用语音和文本数据的挑战和可能的解决方案。<details>
<summary>Abstract</summary>
Alzheimer's disease (AD) is a common form of dementia that severely impacts patient health. As AD impairs the patient's language understanding and expression ability, the speech of AD patients can serve as an indicator of this disease. This study investigates various methods for detecting AD using patients' speech and transcripts data from the DementiaBank Pitt database. The proposed approach involves pre-trained language models and Graph Neural Network (GNN) that constructs a graph from the speech transcript, and extracts features using GNN for AD detection. Data augmentation techniques, including synonym replacement, GPT-based augmenter, and so on, were used to address the small dataset size. Audio data was also introduced, and WavLM model was used to extract audio features. These features were then fused with text features using various methods. Finally, a contrastive learning approach was attempted by converting speech transcripts back to audio and using it for contrastive learning with the original audio. We conducted intensive experiments and analysis on the above methods. Our findings shed light on the challenges and potential solutions in AD detection using speech and audio data.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病 (AD) 是一种常见的脑残留病，对病人健康产生严重的影响。由于 AD 会对病人语言理解和表达能力产生影响，因此病人的语音可以作为该病的指标。这项研究探讨了使用病人语音和讲解数据进行 AD 检测的不同方法。提议的方法包括使用预训练语言模型和图内网络（GNN），构建语音讲解的图，并从图中提取特征进行 AD 检测。数据扩充技术，包括同义词替换、GPT-based augmenter 等，用于解决小数据集的问题。此外，我们还引入了音频数据，并使用 WavLM 模型提取音频特征。这些特征与文本特征进行拼接，并使用不同的方法进行融合。最后，我们尝试了一种对比学习方法，通过将语音讲解转换回音频，并与原始音频进行对比学习。我们进行了广泛的实验和分析，以探讨 AD 检测中使用语音和音频数据的挑战和可能的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Power-up-What-Can-Generative-Models-Do-for-Human-Computation-Workflows"><a href="#Power-up-What-Can-Generative-Models-Do-for-Human-Computation-Workflows" class="headerlink" title="Power-up! What Can Generative Models Do for Human Computation Workflows?"></a>Power-up! What Can Generative Models Do for Human Computation Workflows?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02243">http://arxiv.org/abs/2307.02243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Garrett Allen, Gaole He, Ujwal Gadiraju</li>
<li>for: This paper aims to explore the potential benefits of using large language models (LLMs) in crowdsourcing workflows and to identify the best ways to integrate LLMs into existing crowdsourcing design patterns.</li>
<li>methods: The paper proposes a vision for incorporating LLMs into crowdsourcing workflows, including identifying key junctures in the workflow where LLMs can add value and proposing means to augment existing design patterns for crowd work.</li>
<li>results: The paper aims to provide a comprehensive understanding of how LLMs can improve the effectiveness of crowdsourcing workflows and how such workflows can be evaluated from the perspectives of various stakeholders involved in the crowdsourcing paradigm.<details>
<summary>Abstract</summary>
We are amidst an explosion of artificial intelligence research, particularly around large language models (LLMs). These models have a range of applications across domains like medicine, finance, commonsense knowledge graphs, and crowdsourcing. Investigation into LLMs as part of crowdsourcing workflows remains an under-explored space. The crowdsourcing research community has produced a body of work investigating workflows and methods for managing complex tasks using hybrid human-AI methods. Within crowdsourcing, the role of LLMs can be envisioned as akin to a cog in a larger wheel of workflows. From an empirical standpoint, little is currently understood about how LLMs can improve the effectiveness of crowdsourcing workflows and how such workflows can be evaluated. In this work, we present a vision for exploring this gap from the perspectives of various stakeholders involved in the crowdsourcing paradigm -- the task requesters, crowd workers, platforms, and end-users. We identify junctures in typical crowdsourcing workflows at which the introduction of LLMs can play a beneficial role and propose means to augment existing design patterns for crowd work.
</details>
<details>
<summary>摘要</summary>
我们正处于人工智能研究的爆发阶段，特别是大语言模型（LLM）的研究。这些模型在医学、金融、通用常识图和众生编辑等领域有各种应用。研究在众生编辑中使用人工智能方法的工作流程还是一个未经探索的领域。众生编辑社区已经生成了一些关于工作流程和人工智能方法的研究。在众生编辑中，LLM可以视为工作流程中的一个部件。从实证角度来看，现在还未完全了解LLM如何改善众生编辑工作流程的效果，以及如何评估这些工作流程。在这项工作中，我们提出了一种视图，从众生编辑参与者的角度来explore这个黑板。我们标识了典型的众生编辑工作流程中的缺失，并提议使用人工智能技术来增强现有的设计模式。
</details></li>
</ul>
<hr>
<h2 id="MAE-DFER-Efficient-Masked-Autoencoder-for-Self-supervised-Dynamic-Facial-Expression-Recognition"><a href="#MAE-DFER-Efficient-Masked-Autoencoder-for-Self-supervised-Dynamic-Facial-Expression-Recognition" class="headerlink" title="MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition"></a>MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02227">http://arxiv.org/abs/2307.02227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sunlicai/mae-dfer">https://github.com/sunlicai/mae-dfer</a></li>
<li>paper_authors: Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao</li>
<li>for: 这个研究旨在提高智能和 sympathetic 机器的发展，推动面具识别技术的进步。</li>
<li>methods: 这篇文章提出了一种新的自我超vised学习方法（MAE-DFER），它利用大规模的自题supervised预训数据进行预训，以提高面具识别的发展。</li>
<li>results: 实验结果显示，MAE-DFER 可以与现有的监督学习方法相比，在六个数据集上具有更高的识别率（例如，DFEW 上的 UAR 提高了6.30%，MAFW 上的 UAR 提高了8.34%），并且具有较低的计算成本（约38% FLOPs）。<details>
<summary>Abstract</summary>
Dynamic facial expression recognition (DFER) is essential to the development of intelligent and empathetic machines. Prior efforts in this field mainly fall into supervised learning paradigm, which is severely restricted by the limited labeled data in existing datasets. Inspired by recent unprecedented success of masked autoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel self-supervised method which leverages large-scale self-supervised pre-training on abundant unlabeled data to largely advance the development of DFER. Since the vanilla Vision Transformer (ViT) employed in VideoMAE requires substantial computation during fine-tuning, MAE-DFER develops an efficient local-global interaction Transformer (LGI-Former) as the encoder. Moreover, in addition to the standalone appearance content reconstruction in VideoMAE, MAE-DFER also introduces explicit temporal facial motion modeling to encourage LGI-Former to excavate both static appearance and dynamic motion information. Extensive experiments on six datasets show that MAE-DFER consistently outperforms state-of-the-art supervised methods by significant margins (e.g., +6.30\% UAR on DFEW and +8.34\% UAR on MAFW), verifying that it can learn powerful dynamic facial representations via large-scale self-supervised pre-training. Besides, it has comparable or even better performance than VideoMAE, while largely reducing the computational cost (about 38\% FLOPs). We believe MAE-DFER has paved a new way for the advancement of DFER and can inspire more relevant research in this field and even other related tasks. Codes and models are publicly available at https://github.com/sunlicai/MAE-DFER.
</details>
<details>
<summary>摘要</summary>
“动态表情识别（DFER）是智能和同情机器的发展所必需的一环。先前的努力主要是基于指导学习 paradigm，它受限于现有数据集的标注数据量的严重限制。 Drawing inspiration from recent unprecedented success of masked autoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel self-supervised method that leverages large-scale self-supervised pre-training on abundant unlabeled data to significantly advance the development of DFER. Since the vanilla Vision Transformer (ViT) employed in VideoMAE requires substantial computation during fine-tuning, MAE-DFER develops an efficient local-global interaction Transformer (LGI-Former) as the encoder. Moreover, in addition to the standalone appearance content reconstruction in VideoMAE, MAE-DFER also introduces explicit temporal facial motion modeling to encourage LGI-Former to excavate both static appearance and dynamic motion information. Extensive experiments on six datasets show that MAE-DFER consistently outperforms state-of-the-art supervised methods by significant margins (e.g., +6.30\% UAR on DFEW and +8.34\% UAR on MAFW), verifying that it can learn powerful dynamic facial representations via large-scale self-supervised pre-training. Besides, it has comparable or even better performance than VideoMAE, while largely reducing the computational cost (about 38\% FLOPs). We believe MAE-DFER has paved a new way for the advancement of DFER and can inspire more relevant research in this field and even other related tasks. Codes and models are publicly available at https://github.com/sunlicai/MAE-DFER.”
</details></li>
</ul>
<hr>
<h2 id="On-the-Adversarial-Robustness-of-Generative-Autoencoders-in-the-Latent-Space"><a href="#On-the-Adversarial-Robustness-of-Generative-Autoencoders-in-the-Latent-Space" class="headerlink" title="On the Adversarial Robustness of Generative Autoencoders in the Latent Space"></a>On the Adversarial Robustness of Generative Autoencoders in the Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02202">http://arxiv.org/abs/2307.02202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingfei Lu, Badong Chen</li>
<li>for: 本研究探讨了生成自动编码器在伪设空间中的攻击Robustness。</li>
<li>methods: 本研究使用了多种攻击方法来评估生成自动编码器的攻击Robustness，包括latent space攻击和维度缩放攻击。</li>
<li>results: 研究发现，流行的生成自动编码器在伪设空间中存在漏洞，可以通过攻击来让模型生成错误的样本。此外，研究还发现了一定的负相关性 между模型的攻击Robustness和干扰级别的关系。<details>
<summary>Abstract</summary>
The generative autoencoders, such as the variational autoencoders or the adversarial autoencoders, have achieved great success in lots of real-world applications, including image generation, and signal communication.   However, little concern has been devoted to their robustness during practical deployment.   Due to the probabilistic latent structure, variational autoencoders (VAEs) may confront problems such as a mismatch between the posterior distribution of the latent and real data manifold, or discontinuity in the posterior distribution of the latent.   This leaves a back door for malicious attackers to collapse VAEs from the latent space, especially in scenarios where the encoder and decoder are used separately, such as communication and compressed sensing.   In this work, we provide the first study on the adversarial robustness of generative autoencoders in the latent space.   Specifically, we empirically demonstrate the latent vulnerability of popular generative autoencoders through attacks in the latent space.   We also evaluate the difference between variational autoencoders and their deterministic variants and observe that the latter performs better in latent robustness.   Meanwhile, we identify a potential trade-off between the adversarial robustness and the degree of the disentanglement of the latent codes.   Additionally, we also verify the feasibility of improvement for the latent robustness of VAEs through adversarial training.   In summary, we suggest concerning the adversarial latent robustness of the generative autoencoders, analyze several robustness-relative issues, and give some insights into a series of key challenges.
</details>
<details>
<summary>摘要</summary>
自然语言处理中的生成自动编码器，如变量自动编码器和反对自动编码器，在很多实际应用中取得了很大成功，包括图像生成和信号通信。然而，在实际部署中对这些生成自动编码器的Robustness没有受到充分的关注。由于生成自动编码器的概率 latent 结构，变量自动编码器（VAEs）可能会面临到真实数据拟合问题或 latent  posterior distribution 的缺continuity。这使得恶意攻击者可以通过 latent 空间的攻击，尤其是在encoder 和 decoder 在独立使用时。在这种情况下，VAEs 的 latent 空间 Robustness 会受到攻击。在这个工作中，我们提供了生成自动编码器的 latent 空间 Robustness 的首次研究。我们通过在 latent 空间进行攻击来实证性地表明了各种生成自动编码器的 latent 感受性问题。我们还发现了变量自动编码器和其束定variant 之间的差异，并观察到后者在 latent  Robustness 方面表现更好。此外，我们还发现了对 latent  Robustness 的一定负面影响，即难以同时保证 Robustness 和混合程度的高度。最后，我们验证了对 VAEs 的 latent Robustness 进行适应 adversarial 训练的可行性。总之，我们建议关注生成自动编码器的 latent 空间 Robustness，分析了一些Robustness-相关的问题，并提供了一些关键挑战的视角。
</details></li>
</ul>
<hr>
<h2 id="The-FormAI-Dataset-Generative-AI-in-Software-Security-Through-the-Lens-of-Formal-Verification"><a href="#The-FormAI-Dataset-Generative-AI-in-Software-Security-Through-the-Lens-of-Formal-Verification" class="headerlink" title="The FormAI Dataset: Generative AI in Software Security Through the Lens of Formal Verification"></a>The FormAI Dataset: Generative AI in Software Security Through the Lens of Formal Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02192">http://arxiv.org/abs/2307.02192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Norbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag, Lucas C. Cordeiro, Vasileios Mavroeidis<br>for:The paper presents the FormAI dataset, a large collection of AI-generated C programs with vulnerability classification.methods:The dataset is generated using GPT-3.5-turbo, and a dynamic zero-shot prompting technique is introduced to spawn a diverse set of programs. The dataset is labeled with vulnerabilities using a formal verification method based on ESBMC.results:The dataset contains 112,000 programs with varying levels of complexity, and each program is associated with relevant CWE numbers. The dataset is suitable for evaluating the effectiveness of static and dynamic analysis tools, and can be used to train LLMs and machine learning algorithms.<details>
<summary>Abstract</summary>
This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs with vulnerability classification. We introduce a dynamic zero-shot prompting technique, constructed to spawn a diverse set of programs utilizing Large Language Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity. Some programs handle complicated tasks such as network management, table games, or encryption, while others deal with simpler tasks like string manipulation. Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which performs model checking, abstract interpretation, constraint programming, and satisfiability modulo theories, to reason over safety/security properties in programs. This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports. This property of the dataset makes it suitable for evaluating the effectiveness of various static and dynamic analysis tools. Furthermore, we have associated the identified vulnerabilities with relevant Common Weakness Enumeration (CWE) numbers. We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program including location and function name, which makes the dataset ideal to train LLMs and machine learning algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Citation-A-Key-to-Building-Responsible-and-Accountable-Large-Language-Models"><a href="#Citation-A-Key-to-Building-Responsible-and-Accountable-Large-Language-Models" class="headerlink" title="Citation: A Key to Building Responsible and Accountable Large Language Models"></a>Citation: A Key to Building Responsible and Accountable Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02185">http://arxiv.org/abs/2307.02185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Huang, Kevin Chen-Chuan Chang</li>
<li>for: 这篇论文旨在探讨大语言模型（LLMs）的知识产权和伦理问题，并提出了一种新的解决方案，即引用系统。</li>
<li>methods: 论文提出了一种新的引用机制，以增强语言模型的内容透明度和可靠性，同时解决知识产权和伦理问题。</li>
<li>results: 论文提出了一些研究问题，以促进未来对语言模型的负责任和可靠性的研究。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify "citation" as a crucial yet missing component in LLMs, which could enhance content transparency and verifiability while addressing IP and ethical dilemmas. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the inherent potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diffusion-Models-for-Computational-Design-at-the-Example-of-Floor-Plans"><a href="#Diffusion-Models-for-Computational-Design-at-the-Example-of-Floor-Plans" class="headerlink" title="Diffusion Models for Computational Design at the Example of Floor Plans"></a>Diffusion Models for Computational Design at the Example of Floor Plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02511">http://arxiv.org/abs/2307.02511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joern Ploennigs, Markus Berger</li>
<li>For: The paper explores the capabilities of diffusion-based AI image generators for computational design in civil engineering, specifically for creating floor plans.* Methods: The paper uses diffusion models with improved semantic encoding to generate floor plans, and experiments are conducted to evaluate the validity and query performance of the generated plans.* Results: The paper shows that the proposed diffusion models can improve the validity of generated floor plans from 6% to 90%, and improve query performance for different examples. However, the paper also identifies shortcomings and future research challenges for these models.<details>
<summary>Abstract</summary>
AI Image generators based on diffusion models are widely discussed recently for their capability to create images from simple text prompts. But, for practical use in civil engineering they need to be able to create specific construction plans for given constraints. Within this paper we explore the capabilities of those diffusion-based AI generators for computational design at the example of floor plans and identify their current limitation. We explain how the diffusion-models work and propose new diffusion models with improved semantic encoding. In several experiments we show that we can improve validity of generated floor plans from 6% to 90% and query performance for different examples. We identify short comings and derive future research challenges of those models and discuss the need to combine diffusion models with building information modelling. With this we provide key insights into the current state and future directions for diffusion models in civil engineering.
</details>
<details>
<summary>摘要</summary>
《Diffusion模型基于AI图像生成器在近期内获得了广泛关注，因为它们可以从简单文本提示中生成图像。但是，在实用ivil engineering中，它们需要能够生成基于给定限制的具体的建筑计划。本文通过使用扩散模型来探讨计算设计的可能性，并在底层平面的示例中进行了详细的探讨。我们解释了扩散模型如何工作，并提出了改进 semantic encoding的新扩散模型。在多个实验中，我们显示了可以从6%提高到90%的有效性，并且对不同的示例进行了查询性能的改进。我们还识别出了这些模型的缺陷，并提出了未来研究的挑战。我们认为需要将扩散模型与建筑信息模型结合，以提供更好的设计解决方案。这些发现对于 diffusion模型在 civil engineering 中的当前状况和未来发展提供了关键的洞察。》Note: Simplified Chinese is used here, as it is more widely used in mainland China. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Safety-Shielding-under-Delayed-Observation"><a href="#Safety-Shielding-under-Delayed-Observation" class="headerlink" title="Safety Shielding under Delayed Observation"></a>Safety Shielding under Delayed Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02164">http://arxiv.org/abs/2307.02164</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/filipcano/safety-shields-delayed">https://github.com/filipcano/safety-shields-delayed</a></li>
<li>paper_authors: Filip Cano Córdoba, Alexander Palmisano, Martin Fränzle, Roderick Bloem, Bettina Könighofer</li>
<li>for: 这篇论文是为了描述一种可以在物理环境中运行的自动驾驶系统，并且能够承受延迟输入和输出信号的延迟遮盾。</li>
<li>methods: 这篇论文使用了一种名为”延迟遮盾”的技术，它可以在运行时确保自动驾驶系统的安全性，并且可以最小化遮盾对系统的干扰。</li>
<li>results: 这篇论文提出了一种新的延迟遮盾计算方法，可以在 worst-case 延迟输入信号的假设下保证安全性。此外，论文还提出了一些新的决策策略，可以最小化遮盾对系统的干扰。最后，论文还实现了延迟遮盾在实际驾驶模拟器中的首次应用。<details>
<summary>Abstract</summary>
Agents operating in physical environments need to be able to handle delays in the input and output signals since neither data transmission nor sensing or actuating the environment are instantaneous. Shields are correct-by-construction runtime enforcers that guarantee safe execution by correcting any action that may cause a violation of a formal safety specification. Besides providing safety guarantees, shields should interfere minimally with the agent. Therefore, shields should pick the safe corrective actions in such a way that future interferences are most likely minimized. Current shielding approaches do not consider possible delays in the input signals in their safety analyses. In this paper, we address this issue. We propose synthesis algorithms to compute \emph{delay-resilient shields} that guarantee safety under worst-case assumptions on the delays of the input signals. We also introduce novel heuristics for deciding between multiple corrective actions, designed to minimize future shield interferences caused by delays. As a further contribution, we present the first integration of shields in a realistic driving simulator. We implemented our delayed shields in the driving simulator \textsc{Carla}. We shield potentially unsafe autonomous driving agents in different safety-critical scenarios and show the effect of delays on the safety analysis.
</details>
<details>
<summary>摘要</summary>
In this paper, we address this issue by proposing synthesis algorithms to compute "delay-resilient shields" that guarantee safety under worst-case assumptions about input signal delays. We also introduce novel heuristics for deciding between multiple corrective actions, designed to minimize future shield interferences caused by delays. Furthermore, we present the first integration of shields in a realistic driving simulator. We implemented our delayed shields in the driving simulator \textsc{Carla} and shielded potentially unsafe autonomous driving agents in different safety-critical scenarios, demonstrating the effect of delays on the safety analysis.
</details></li>
</ul>
<hr>
<h2 id="Becoming-self-instruct-introducing-early-stopping-criteria-for-minimal-instruct-tuning"><a href="#Becoming-self-instruct-introducing-early-stopping-criteria-for-minimal-instruct-tuning" class="headerlink" title="Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning"></a>Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03692">http://arxiv.org/abs/2307.03692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Waseem AlShikh, Manhal Daaboul, Kirk Goddard, Brock Imel, Kiran Kamble, Parikshith Kulkarni, Melisa Russak</li>
<li>for: 本研究 introduce了一个名为 Instruction Following Score (IFS) 的度量，用于测量语言模型能够遵循指令的能力。</li>
<li>methods: 本研究使用了公开available的基础模型和指令模型，并 benchmarked 它们以显示出 ratio of well formatted responses to partial and full sentences 可以作为基础模型和指令模型之间的区别。此外，IFS 也可以用作初级停止条件 для instruct tuning。</li>
<li>results: 研究发现，使用 SFT 训练 7B 和 13B LLaMA 模型，模型在训练过程中较早学会遵循指令，并且进一步 fine-tuning 可能会导致基础模型的 semantics 变化。例如，我们通过auxiliary metric ObjecQA 显示出模型预测的 объекivity 在特定情况下变化，而且这些变化在 IFS 倾斜时最为显著。本研究希望通过将 instruct tuning 分解为 IFS 和 semantics 因素，开启更好控制的 instruct tuning 和设计最小化 instruct 界面 querying foundation models 的可能性。<details>
<summary>Abstract</summary>
In this paper, we introduce the Instruction Following Score (IFS), a metric that detects language models' ability to follow instructions. The metric has a dual purpose. First, IFS can be used to distinguish between base and instruct models. We benchmark publicly available base and instruct models, and show that the ratio of well formatted responses to partial and full sentences can be an effective measure between those two model classes. Secondly, the metric can be used as an early stopping criteria for instruct tuning. We compute IFS for Supervised Fine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models learn to follow instructions relatively early in the training process, and the further finetuning can result in changes in the underlying base model semantics. As an example of semantics change we show the objectivity of model predictions, as defined by an auxiliary metric ObjecQA. We show that in this particular case, semantic changes are the steepest when the IFS tends to plateau. We hope that decomposing instruct tuning into IFS and semantic factors starts a new trend in better controllable instruct tuning and opens possibilities for designing minimal instruct interfaces querying foundation models.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们引入了指令遵循分数（IFS），这是一个用于检测语言模型遵循指令的度量。IFS有两个目的：首先，它可以用于分辨基础模型和指令模型之间的差异。我们对公开可用的基础模型和指令模型进行了比较，并显示了基础模型和指令模型之间的响应格式化率的差异。其次，IFS可以用作指令训练的早期停止条件。我们对7B和13B LLama模型进行了超参训练（SFT），并计算了IFS，显示了模型在训练过程中早些地学会遵循指令，并且进一步的训练可以导致基础模型 semantics 的改变。我们作为一个例子，显示了模型预测的 объекivity，如定义的auxiliary metric ObjecQA。我们发现，在这种特定情况下，semantic changes 是指令训练过程中最大化的。我们希望通过将 instruct tuning 分解为 IFS 和semantic factors，开启更好控制的 instruct tuning 和设计最小的 instruct 界面，以便查询基础模型。
</details></li>
</ul>
<hr>
<h2 id="Towards-Open-Federated-Learning-Platforms-Survey-and-Vision-from-Technical-and-Legal-Perspectives"><a href="#Towards-Open-Federated-Learning-Platforms-Survey-and-Vision-from-Technical-and-Legal-Perspectives" class="headerlink" title="Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives"></a>Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02140">http://arxiv.org/abs/2307.02140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/morningd/model-centric-fml">https://github.com/morningd/model-centric-fml</a></li>
<li>paper_authors: Moming Duan</li>
<li>for: 提高 Federated Learning (FL) 的应用场景和数据持有者参与度，重新定义当前 FL 框架，并推广其概念到更加通用的 Open Federated Learning Platforms。</li>
<li>methods: 提出了两种相互合作框架：查询基于的 FL 和合同基于的 FL，以实现更加开放的 FL 平台。</li>
<li>results: 通过技术和法律两个视角进行全面回顾，提出了一系列有价值的主题，如模型库的可用性、法律合规性分析、模型 reuse 中的版权问题和知识产权保护等。特别是，提出了一种新的分类系统，用于整合 batch model reusing 方法中的许可证约束，并且提供了一个系统性的框架，用于 indentifying 可能的法律后果和限制。<details>
<summary>Abstract</summary>
Traditional Federated Learning (FL) follows a server-domincated cooperation paradigm which narrows the application scenarios of FL and decreases the enthusiasm of data holders to participate. To fully unleash the potential of FL, we advocate rethinking the design of current FL frameworks and extending it to a more generalized concept: Open Federated Learning Platforms. We propose two reciprocal cooperation frameworks for FL to achieve this: query-based FL and contract-based FL. In this survey, we conduct a comprehensive review of the feasibility of constructing an open FL platform from both technical and legal perspectives. We begin by reviewing the definition of FL and summarizing its inherent limitations, including server-client coupling, low model reusability, and non-public. In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-date model repositories for model querying, legal compliance analysis between different model licenses, and copyright issues and intellectual property protection in model reusing. In particular, we introduce a novel taxonomy to streamline the analysis of model license compatibility in FL studies that involve batch model reusing methods, including combination, amalgamation, distillation, and generation. This taxonomy provides a systematic framework for identifying the corresponding clauses of licenses and facilitates the identification of potential legal implications and restrictions when reusing models. Through this survey, we uncover the the current dilemmas faced by FL and advocate for the development of sustainable open FL platforms. We aim to provide guidance for establishing such platforms in the future, while identifying potential problems and challenges that need to be addressed.
</details>
<details>
<summary>摘要</summary>
传统的联合学习（FL）采用服务器主导的合作方式，这限制了FL应用场景和数据持有者参与的积极性。为了充分发挥FL的潜力，我们提倡重新设计当前FL框架，扩展其为更通用的概念：开放联合学习平台。我们提出了两种对接合作框架，即查询基于的FL和合同基于的FL。在这篇评论中，我们从技术和法律两个角度进行了全面的审查，以确定开放FL平台的可行性。我们首先介绍了FL的定义和其内置的局限性，包括服务器客户端耦合、低模型复用率和非公共。在查询基于的FL平台上，我们探讨了一系列有价值的话题，包括社区驱动的模型共享和再利用，以及模型存储库的可用性、许可证分析和版权问题。其中，我们提出了一种新的分类方法，用于对FL研究中的批量模型复用方法进行许可证兼容性分析。这种分类方法提供了一个系统化的框架，可以帮助identify potential legal implications和restrictions when reusing models。通过这篇评论，我们揭示了当前FL面临的矛盾和挑战，并提出了建立可持续的开放FL平台的建议。我们希望通过这篇评论，为未来建立这些平台提供指南，并 indentify potential problems和挑战需要解决。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Known-Reality-Exploiting-Counterfactual-Explanations-for-Medical-Research"><a href="#Beyond-Known-Reality-Exploiting-Counterfactual-Explanations-for-Medical-Research" class="headerlink" title="Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research"></a>Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02131">http://arxiv.org/abs/2307.02131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toygarr/counterfactual-explanations-for-medical-research">https://github.com/toygarr/counterfactual-explanations-for-medical-research</a></li>
<li>paper_authors: Toygar Tanyel, Serkan Ayvaz, Bilgin Keserci</li>
<li>for: 这个研究使用对比性解释来探讨医学研究中的”如果”场景，以扩展我们对现有边界的理解。Specifically, we focus on using MRI features for diagnosing pediatric posterior fossa brain tumors as a case study.</li>
<li>methods: 我们的方法利用对比性解释，提供一种新的方式来解释机器学习算法的结果。这些解释提供个性化和上下文特定的理解，帮助验证预测和解释不同情况下的变化。</li>
<li>results: 结果表明，对比性解释可以增强人工智能和解释的acceptance在医疗实践中。我们的方法保持了统计学和临床准确性，允许在不同的现实下探讨不同的肿瘤特征。 Additionally, we explore the potential use of counterfactuals for data augmentation and evaluate their feasibility as an alternative approach in medical research.<details>
<summary>Abstract</summary>
This study employs counterfactual explanations to explore "what if?" scenarios in medical research, with the aim of expanding our understanding beyond existing boundaries. Specifically, we focus on utilizing MRI features for diagnosing pediatric posterior fossa brain tumors as a case study. The field of artificial intelligence and explainability has witnessed a growing number of studies and increasing scholarly interest. However, the lack of human-friendly interpretations in explaining the outcomes of machine learning algorithms has significantly hindered the acceptance of these methods by clinicians in their clinical practice. To address this, our approach incorporates counterfactual explanations, providing a novel way to examine alternative decision-making scenarios. These explanations offer personalized and context-specific insights, enabling the validation of predictions and clarification of variations under diverse circumstances. Importantly, our approach maintains both statistical and clinical fidelity, allowing for the examination of distinct tumor features through alternative realities. Additionally, we explore the potential use of counterfactuals for data augmentation and evaluate their feasibility as an alternative approach in medical research. The results demonstrate the promising potential of counterfactual explanations to enhance trust and acceptance of AI-driven methods in clinical settings.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "counterfactual explanations" Counterfactual explanations are used to explore "what if?" scenarios in medical research, providing personalized and context-specific insights into alternative decision-making scenarios.* "MRI features" 我们使用 MRI 特征来诊断儿童后部脑肿。* "artificial intelligence and explainability" 人工智能和解释性在医学研究中受到了越来越多的研究和学术关注，但是机器学习算法的解释性不够人性化，对临床医生的acceptance带来了很大障碍。* "data augmentation" 我们还探讨了使用 counterfactual explanations 作为数据扩充的可能性，以增强 AI 驱动方法在临床实践中的acceptance。
</details></li>
</ul>
<hr>
<h2 id="DARE-Towards-Robust-Text-Explanations-in-Biomedical-and-Healthcare-Applications"><a href="#DARE-Towards-Robust-Text-Explanations-in-Biomedical-and-Healthcare-Applications" class="headerlink" title="DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications"></a>DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02094">http://arxiv.org/abs/2307.02094</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/domain-adaptive-attribution-robustness">https://github.com/ibm/domain-adaptive-attribution-robustness</a></li>
<li>paper_authors: Adam Ivankay, Mattia Rigotti, Pascal Frossard</li>
<li>for: 本研究旨在探讨深度神经网络在医学领域中的解释性问题，以及如何提高这些网络的解释性 robustness。</li>
<li>methods: 本研究使用了现有的权重robustness估计方法，并提出了一种适应域（DomainAdaptive）的权重robustness估计器（DARE），以及两种方法来 Mitigate brittleness：适应训练和FAR训练。</li>
<li>results: 本研究通过广泛的实验 validate了我们的方法，并表明了在三个Established biomedical benchmarks上，我们的方法能够提高深度神经网络的解释性 robustness。<details>
<summary>Abstract</summary>
Along with the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has seen a significant increase recently. Several methods have been introduced to provide insight into the inference process of deep neural networks. However, most of these explainability methods have been shown to be brittle in the face of adversarial perturbations of their inputs in the image and generic textual domain. In this work we show that this phenomenon extends to specific and important high stakes domains like biomedical datasets. In particular, we observe that the robustness of explanations should be characterized in terms of the accuracy of the explanation in linking a model's inputs and its decisions - faithfulness - and its relevance from the perspective of domain experts - plausibility. This is crucial to prevent explanations that are inaccurate but still look convincing in the context of the domain at hand. To this end, we show how to adapt current attribution robustness estimation methods to a given domain, so as to take into account domain-specific plausibility. This results in our DomainAdaptiveAREstimator (DARE) attribution robustness estimator, allowing us to properly characterize the domain-specific robustness of faithful explanations. Next, we provide two methods, adversarial training and FAR training, to mitigate the brittleness characterized by DARE, allowing us to train networks that display robust attributions. Finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.
</details>
<details>
<summary>摘要</summary>
alongside the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has become increasingly important recently. several methods have been proposed to provide insight into the inference process of deep neural networks. however, most of these explainability methods have been shown to be fragile in the face of adversarial perturbations of their inputs in the image and generic textual domain. in this work, we show that this phenomenon extends to specific and important high-stakes domains like biomedical datasets. in particular, we observe that the robustness of explanations should be characterized in terms of the accuracy of the explanation in linking a model's inputs and its decisions - faithfulness - and its relevance from the perspective of domain experts - plausibility. this is crucial to prevent explanations that are inaccurate but still look convincing in the context of the domain at hand. to this end, we show how to adapt current attribution robustness estimation methods to a given domain, so as to take into account domain-specific plausibility. this results in our DomainAdaptiveAREstimator (DARE) attribution robustness estimator, allowing us to properly characterize the domain-specific robustness of faithful explanations. next, we provide two methods, adversarial training and FAR training, to mitigate the brittleness characterized by DARE, allowing us to train networks that display robust attributions. finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Combating-Confirmation-Bias-A-Unified-Pseudo-Labeling-Framework-for-Entity-Alignment"><a href="#Combating-Confirmation-Bias-A-Unified-Pseudo-Labeling-Framework-for-Entity-Alignment" class="headerlink" title="Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment"></a>Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02075">http://arxiv.org/abs/2307.02075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qijie Ding, Jie Yin, Daokun Zhang, Junbin Gao</li>
<li>for: 提高实体对Alignment的准确率，排除假标签错误的影响</li>
<li>methods: 提posed a Unified Pseudo-Labeling framework for Entity Alignment (UPL-EA)，包括OT模型化和跨迭代pseudo标签准确率提高</li>
<li>results: 实验结果表明，our approach可以在有限先aligned种子的情况下 achieve competitive performance，并 theoretically supported by our analysis of pseudo-labeling errors.<details>
<summary>Abstract</summary>
Entity alignment (EA) aims at identifying equivalent entity pairs across different knowledge graphs (KGs) that refer to the same real-world identity. To systematically combat confirmation bias for pseudo-labeling-based entity alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment (UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the accuracy of entity alignment. UPL-EA consists of two complementary components: (1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as an effective means to enable more accurate determination of entity correspondences across two KGs and to mitigate the adverse impact of erroneous matches. A simple but highly effective criterion is further devised to derive pseudo-labeled entity pairs that satisfy one-to-one correspondences at each iteration. (2) The cross-iteration pseudo-label calibration operates across multiple consecutive iterations to further improve the pseudo-labeling precision rate by reducing the local pseudo-label selection variability with a theoretical guarantee. The two components are respectively designed to eliminate Type I and Type II pseudo-labeling errors identified through our analyse. The calibrated pseudo-labels are thereafter used to augment prior alignment seeds to reinforce subsequent model training for alignment inference. The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both theoretically supported and experimentally validated. The experimental results show that our approach achieves competitive performance with limited prior alignment seeds.
</details>
<details>
<summary>摘要</summary>
Entity alignment (EA) targets identifying equivalent entity pairs across different knowledge graphs (KGs) that refer to the same real-world identity. To systematically combat confirmation bias for pseudo-labeling-based entity alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment (UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the accuracy of entity alignment. UPL-EA consists of two complementary components:1. Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as an effective means to enable more accurate determination of entity correspondences across two KGs and to mitigate the adverse impact of erroneous matches. A simple but highly effective criterion is further devised to derive pseudo-labeled entity pairs that satisfy one-to-one correspondences at each iteration.2. The cross-iteration pseudo-label calibration operates across multiple consecutive iterations to further improve the pseudo-labeling precision rate by reducing the local pseudo-label selection variability with a theoretical guarantee. The two components are respectively designed to eliminate Type I and Type II pseudo-labeling errors identified through our analysis.The calibrated pseudo-labels are thereafter used to augment prior alignment seeds to reinforce subsequent model training for alignment inference. The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both theoretically supported and experimentally validated. The experimental results show that our approach achieves competitive performance with limited prior alignment seeds.
</details></li>
</ul>
<hr>
<h2 id="Performance-Modeling-of-Data-Storage-Systems-using-Generative-Models"><a href="#Performance-Modeling-of-Data-Storage-Systems-using-Generative-Models" class="headerlink" title="Performance Modeling of Data Storage Systems using Generative Models"></a>Performance Modeling of Data Storage Systems using Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02073">http://arxiv.org/abs/2307.02073</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Abdalaziz Rashid Al-Maeeni, Aziz Temirkhanov, Artem Ryzhikov, Mikhail Hushchyn</li>
<li>For: The paper is written for researchers and practitioners in the field of industrial data analysis, particularly those interested in high-precision modeling of systems and digital twins.* Methods: The paper uses machine learning-based generative models to develop models of a storage system, including probabilistic models for each storage component (HDD and SSD) and RAID schemes.* Results: The paper reports results of experiments demonstrating the accuracy of the predictions made by the models, with errors ranging from 4-10% for IOPS and 3-16% for latency, depending on the components and models used. The predictions show up to 0.99 Pearson correlation with Little’s law, which can be used for unsupervised reliability checks of the models. Additionally, the paper presents novel data sets that can be used for benchmarking regression algorithms, conditional generative models, and uncertainty estimation methods in machine learning.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究适用于产业数据分析领域的研究人员和实践者，尤其是高精度系统模型化领域和数字双。</li>
<li>methods: 本研究使用机器学习基于生成模型来开发存储系统模型，包括存储Component（HDD和SSD）和RAID schemes的概率模型。</li>
<li>results: 实验结果表明模型预测的误差在4-10%（IOPS）和3-16%（延迟）之间，具体取决于组件和模型。预测结果与小的法律之间呈0.99相关性，可以用于无监督可靠性检查。此外，本研究还提供了可用于机器学习回归算法、条件生成模型和不确定性估计方法的新数据集。<details>
<summary>Abstract</summary>
High-precision modeling of systems is one of the main areas of industrial data analysis. Models of systems, their digital twins, are used to predict their behavior under various conditions. We have developed several models of a storage system using machine learning-based generative models. The system consists of several components: hard disk drive (HDD) and solid-state drive (SSD) storage pools with different RAID schemes and cache. Each storage component is represented by a probabilistic model that describes the probability distribution of the component performance in terms of IOPS and latency, depending on their configuration and external data load parameters. The results of the experiments demonstrate the errors of 4-10 % for IOPS and 3-16 % for latency predictions depending on the components and models of the system. The predictions show up to 0.99 Pearson correlation with Little's law, which can be used for unsupervised reliability checks of the models. In addition, we present novel data sets that can be used for benchmarking regression algorithms, conditional generative models, and uncertainty estimation methods in machine learning.
</details>
<details>
<summary>摘要</summary>
高精度系统模型化是工业数据分析的一个主要领域。系统的数字双（digital twin）被用于预测它们在不同条件下的行为。我们已经开发了一些基于机器学习生成模型的存储系统模型。存储系统由多个组件组成：硬盘驱动器（HDD）和固态驱动器（SSD）存储池，具有不同的RAID方案和缓存。每个存储组件都是一个概率模型，描述了组件性能的概率分布在 terms of IOPS 和延迟，受到配置和外部数据负荷参数的影响。实验结果显示，各种模型的误差为4-10% для IOPS 和 3-16% для 延迟，具体取决于系统的组件和模型。预测结果与 Little's law 的相关性达到0.99，可以用于无监督可靠性检查。此外，我们还提供了新的数据集，可以用于机器学习回归算法、条件生成模型和不确定性估计方法的benchmarking。
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Machine-Learning-Methods-for-Data-with-High-Cardinality-Categorical-Variables"><a href="#A-Comparison-of-Machine-Learning-Methods-for-Data-with-High-Cardinality-Categorical-Variables" class="headerlink" title="A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables"></a>A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02071">http://arxiv.org/abs/2307.02071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fabsig/compare_ml_highcardinality_categorical_variables">https://github.com/fabsig/compare_ml_highcardinality_categorical_variables</a></li>
<li>paper_authors: Fabio Sigrist</li>
<li>for: 这篇论文主要是研究高 Cardinality  categorical 变量在机器学习模型中的影响。</li>
<li>methods: 论文使用了多种机器学习方法，包括树融合和深度神经网络，以及线性混合效应模型。</li>
<li>results: 研究发现，机器学习模型含有随机效应可以提高预测精度，而树融合含有随机效应的方法在高 Cardinality  categorical 变量下表现较好。<details>
<summary>Abstract</summary>
High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set, or in other words, there are few data points per level. Machine learning methods can have difficulties with high-cardinality variables. In this article, we empirically compare several versions of two of the most successful machine learning methods, tree-boosting and deep neural networks, and linear mixed effects models using multiple tabular data sets with high-cardinality categorical variables. We find that, first, machine learning models with random effects have higher prediction accuracy than their classical counterparts without random effects, and, second, tree-boosting with random effects outperforms deep neural networks with random effects.
</details>
<details>
<summary>摘要</summary>
高级别分类变量是指数据集中样本数少于变量水平的数量相对较多的变量，也就是说，每个水平有少量数据点。机器学习方法可能会对高级别分类变量表示困难。本文通过使用多个标准化数据集来比较多种最successful机器学习方法的实际性能，包括树融合和深度神经网络，以及线性混合模型。我们发现：首先，机器学习模型含随机效应比其经典版本没有随机效应的模型具有更高的预测精度；其次，树融合含随机效应的模型在对高级别分类变量进行预测时表现较好。
</details></li>
</ul>
<hr>
<h2 id="Line-Graphics-Digitization-A-Step-Towards-Full-Automation"><a href="#Line-Graphics-Digitization-A-Step-Towards-Full-Automation" class="headerlink" title="Line Graphics Digitization: A Step Towards Full Automation"></a>Line Graphics Digitization: A Step Towards Full Automation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02065">http://arxiv.org/abs/2307.02065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moured/document-graphics-digitization">https://github.com/moured/document-graphics-digitization</a></li>
<li>paper_authors: Omar Moured, Jiaming Zhang, Alina Roitberg, Thorsten Schwarz, Rainer Stiefelhagen</li>
<li>for:  This paper aims to improve the digitization of mathematical graphics, specifically statistical plots, to increase accessibility and reproducibility.</li>
<li>methods: The authors introduce the Line Graphics (LG) dataset, which includes pixel-wise annotations of 5 coarse and 10 fine-grained categories, and explore 7 state-of-the-art models to benchmark the dataset.</li>
<li>results: The dataset covers 520 images of mathematical graphics from 450 documents across different disciplines, and can support two computer vision tasks: semantic segmentation and object detection. The authors plan to make the dataset, code, and models publicly available to the community.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提高数学图表的数字化，尤其是统计图表，以提高可访问性和重现性。</li>
<li>methods: 作者们引入了Line Graphics（LG）数据集，该数据集包括5个汽车精细类别和10个细化类别的像素级别标注，并探索了7种现状模型来评估数据集。</li>
<li>results: 数据集包括450份文档中的520张数学图表，覆盖了不同领域的文献，并可以支持2种计算机视觉任务：semantic segmentation和物体检测。作者们计划将数据集、代码和模型公开给社区。<details>
<summary>Abstract</summary>
The digitization of documents allows for wider accessibility and reproducibility. While automatic digitization of document layout and text content has been a long-standing focus of research, this problem in regard to graphical elements, such as statistical plots, has been under-explored. In this paper, we introduce the task of fine-grained visual understanding of mathematical graphics and present the Line Graphics (LG) dataset, which includes pixel-wise annotations of 5 coarse and 10 fine-grained categories. Our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines. Our proposed dataset can support two different computer vision tasks, i.e., semantic segmentation and object detection. To benchmark our LG dataset, we explore 7 state-of-the-art models. To foster further research on the digitization of statistical graphs, we will make the dataset, code, and models publicly available to the community.
</details>
<details>
<summary>摘要</summary>
数字化文档的普及和可重现性得到了更广泛的访问和使用。自动化文档格式和文本内容的数字化已经是研究的长期焦点，但是关于图形元素，如统计图表，的问题尚未得到充分的研究。在这篇论文中，我们提出了细腻的数字化统计图表的视觉理解任务，并提供了Line Graphics（LG）数据集，该数据集包括精度的像素级别标注5种粗细的类别和10种细腻的类别。我们的数据集包括450份不同领域的文档中的520张数学图表。我们的提posed数据集可以支持两种不同的计算机视觉任务，即semantic segmentation和object detection。为了评估我们的LG数据集，我们探索了7个当前最佳模型。为了推动数字化统计图表领域的进一步研究，我们将在社区中公开数据集、代码和模型。
</details></li>
</ul>
<hr>
<h2 id="Emoji-Prediction-using-Transformer-Models"><a href="#Emoji-Prediction-using-Transformer-Models" class="headerlink" title="Emoji Prediction using Transformer Models"></a>Emoji Prediction using Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02054">http://arxiv.org/abs/2307.02054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mnusrat786/emoji-prediction-with-transformer">https://github.com/mnusrat786/emoji-prediction-with-transformer</a></li>
<li>paper_authors: Muhammad Osama Nusrat, Zeeshan Habib, Mehreen Alam, Saad Ahmed Jamal</li>
<li>for: 理解在社交媒体中的表情符号含义</li>
<li>methods: 使用BERTTransformer预训练语言模型进行表情符号预测</li>
<li>results: 实验结果显示，我们的方法在预测表情符号时的准确率超过75%，与当前状态的模型相比有所提高。<details>
<summary>Abstract</summary>
In recent years, the use of emojis in social media has increased dramatically, making them an important element in understanding online communication. However, predicting the meaning of emojis in a given text is a challenging task due to their ambiguous nature. In this study, we propose a transformer-based approach for emoji prediction using BERT, a widely-used pre-trained language model. We fine-tuned BERT on a large corpus of text containing both text and emojis to predict the most appropriate emoji for a given text. Our experimental results demonstrate that our approach outperforms several state-of-the-art models in predicting emojis with an accuracy of over 75 percent. This work has potential applications in natural language processing, sentiment analysis, and social media marketing.
</details>
<details>
<summary>摘要</summary>
近年来，社交媒体中emoji的使用量有很大的增长，使其成为了在线communication中重要的元素。然而，预测emoji在给定文本中的意思是一项具有挑战性的任务，这是因为emoji具有抽象的特点。在这项研究中，我们提出了基于 transformer 的方法，使用BERT，一种广泛使用的预先训练语言模型，来预测emoji。我们对一大量文本数据进行了微调，以预测文本中的最佳emoji。我们的实验结果表明，我们的方法可以超过75%的精度来预测emoji。这项工作有可能应用于自然语言处理、情感分析和社交媒体营销等领域。
</details></li>
</ul>
<hr>
<h2 id="Flowchase-a-Mobile-Application-for-Pronunciation-Training"><a href="#Flowchase-a-Mobile-Application-for-Pronunciation-Training" class="headerlink" title="Flowchase: a Mobile Application for Pronunciation Training"></a>Flowchase: a Mobile Application for Pronunciation Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02051">http://arxiv.org/abs/2307.02051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noé Tits, Zoé Broisson</li>
<li>for: 提供个性化和实时反馈给英语学习者</li>
<li>methods: 使用流处理应用程序和语音技术进行语音分 segment和分析</li>
<li>results: 提供了一种基于语音表征学习的机器学习模型，用于设计个性化的反馈Here’s the same information in Traditional Chinese:</li>
<li>for: 提供个性化和实时反馈给英语学习者</li>
<li>methods: 使用流处理应用程序和语音技术进行语音分 segment和分析</li>
<li>results: 提供了一种基于语音表征学习的机器学习模型，用于设计个性化的反馈<details>
<summary>Abstract</summary>
In this paper, we present a solution for providing personalized and instant feedback to English learners through a mobile application, called Flowchase, that is connected to a speech technology able to segment and analyze speech segmental and supra-segmental features. The speech processing pipeline receives linguistic information corresponding to an utterance to analyze along with a speech sample. After validation of the speech sample, a joint forced-alignment and phonetic recognition is performed thanks to a combination of machine learning models based on speech representation learning that provides necessary information for designing a feedback on a series of segmental and supra-segmental pronunciation aspects.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种解决方案，通过一款名为Flowchase的移动应用程序，为英语学习者提供个性化和实时反馈。该应用程序与一种可以分 segment和supra-segment 语音特征的语音技术相连接。语音处理管道接受语音词汇和语音样本，并进行验证。然后，通过一种基于语音表示学习的机器学习模型的组合，实现联合强制对齐和音名识别。这些信息对于设计 série of segmental和supra-segmental 发音方面的反馈是必要的。
</details></li>
</ul>
<hr>
<h2 id="Recommender-Systems-in-the-Era-of-Large-Language-Models-LLMs"><a href="#Recommender-Systems-in-the-Era-of-Large-Language-Models-LLMs" class="headerlink" title="Recommender Systems in the Era of Large Language Models (LLMs)"></a>Recommender Systems in the Era of Large Language Models (LLMs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02046">http://arxiv.org/abs/2307.02046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, Jiliang Tang, Qing Li</li>
<li>for: 本研究主要为了提供一个系统性的对 recenser systems中使用大型自然语言模型（LLMs）的审视，以便为相关领域的研究人员提供深入的理解。</li>
<li>methods: 本研究主要采用了以下三种方法来使用LLMs进行推荐系统的提升：（1）使用LLMs作为特征编码器来学习用户和Item的表示；（2）使用LLMs进行预训练、细化和提示来提高推荐系统的性能；以及（3）通过提高推荐系统的一致性和可靠性来提高用户体验。</li>
<li>results: 本研究发现，通过使用LLMs，可以提高推荐系统的一致性和可靠性，同时提高用户体验。此外，本研究还发现了一些未来的研究方向，包括如何更好地使用LLMs的特征来提高推荐系统的性能，以及如何在不同的推荐场景下使用LLMs来提高推荐系统的一致性和可靠性。<details>
<summary>Abstract</summary>
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, recent studies have attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems, to provide researchers in relevant fields with an in-depth understanding. Therefore, in this paper, we conduct a comprehensive review of LLM-empowered recommender systems from various aspects including Pre-training, Fine-tuning, and Prompting. More specifically, we first introduce representative methods to harness the power of LLMs (as a feature encoder) for learning representations of users and items. Then, we review recent techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss future directions in this emerging field.
</details>
<details>
<summary>摘要</summary>
随着电子商务和网络应用的发展，个性化推荐系统（RecSys）已经成为我们日常生活中的重要组成部分，提供个性化的建议，以适应用户的偏好。深度神经网络（DNNs）已经在提高RecSys方面做出了重要的进步，通过模拟用户 item 交互和 incorporating 文本侧 информацию，但 DNNs 方法仍然存在一些限制，如用户兴趣的理解和文本侧信息的捕捉困难、不同推荐场景的泛化和理解等等。同时，大型自然语言模型（LLMs），如 ChatGPT 和 GPT4，在自然语言处理（NLP）和人工智能（AI）领域中带来了革命，因为它们在语言理解和生成基本责任上表现出色，同时具有卓越的泛化和理解能力。因此，最近的研究尝试使用 LLMs 来改进 RecSys。随着这个研究方向的快速发展，有一定的急需要对 LLM-empowered RecSys 进行系统性的综述，以为相关领域的研究人员提供深入的理解。因此，在这篇论文中，我们进行了全面的 LLM-empowered RecSys 的综述，从多种方面进行了详细的介绍，包括预训练、精度调整和提示等方面。更 specifically，我们首先介绍了使用 LLMs 作为特征编码器来学习用户和商品的表示。然后，我们查看了最近在 RecSys 领域中使用 LLMs 的技术，从三个 paradigms 的角度进行了详细的介绍：预训练、精度调整和提示。最后，我们进行了全面的未来方向的讨论。
</details></li>
</ul>
<hr>
<h2 id="Fisher-Weighted-Merge-of-Contrastive-Learning-Models-in-Sequential-Recommendation"><a href="#Fisher-Weighted-Merge-of-Contrastive-Learning-Models-in-Sequential-Recommendation" class="headerlink" title="Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation"></a>Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05476">http://arxiv.org/abs/2307.05476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jung Hyun Ryu, Jaeheyoung Jeon, Jewoong Cho, Myungjoo Kang 1</li>
<li>for: 这个研究旨在应对在线平台和服务的推荐系统中的推荐探索，尤其是游戏和社交媒体等领域的发展。</li>
<li>methods: 本研究使用了对照学习方法，对于时间序列推荐的挑战进行了解决。特别是通过融合多个模型的参数，以提高推荐系统的总性能。</li>
<li>results: 经过广泛的实验证明，我们的提案方法能够实现更好的推荐性能，并且显示出在时间序列推荐领域的应用前景。<details>
<summary>Abstract</summary>
Along with the exponential growth of online platforms and services, recommendation systems have become essential for identifying relevant items based on user preferences. The domain of sequential recommendation aims to capture evolving user preferences over time. To address dynamic preference, various contrastive learning methods have been proposed to target data sparsity, a challenge in recommendation systems due to the limited user-item interactions. In this paper, we are the first to apply the Fisher-Merging method to Sequential Recommendation, addressing and resolving practical challenges associated with it. This approach ensures robust fine-tuning by merging the parameters of multiple models, resulting in improved overall performance. Through extensive experiments, we demonstrate the effectiveness of our proposed methods, highlighting their potential to advance the state-of-the-art in sequential learning and recommendation systems.
</details>
<details>
<summary>摘要</summary>
随着在线平台和服务的激增，推荐系统已成为根据用户偏好 identify 相关项目的重要工具。Sequential recommendation 领域targets evolving user preferences over time。为了Address dynamic preference, various contrastive learning methods have been proposed to address data sparsity, a challenge in recommendation systems due to limited user-item interactions. In this paper, we are the first to apply the Fisher-Merging method to Sequential Recommendation, addressing and resolving practical challenges associated with it. This approach ensures robust fine-tuning by merging the parameters of multiple models, resulting in improved overall performance. Through extensive experiments, we demonstrate the effectiveness of our proposed methods, highlighting their potential to advance the state-of-the-art in sequential learning and recommendation systems.Here's a word-for-word translation of the text into Simplified Chinese:随着在线平台和服务的激增，推荐系统已成为根据用户偏好 identify 相关项目的重要工具。Sequential recommendation 领域targets evolving user preferences over time。为了Address dynamic preference, various contrastive learning methods have been proposed to address data sparsity, a challenge in recommendation systems due to limited user-item interactions. In this paper, we are the first to apply the Fisher-Merging method to Sequential Recommendation, addressing and resolving practical challenges associated with it. This approach ensures robust fine-tuning by merging the parameters of multiple models, resulting in improved overall performance. Through extensive experiments, we demonstrate the effectiveness of our proposed methods, highlighting their potential to advance the state-of-the-art in sequential learning and recommendation systems.
</details></li>
</ul>
<hr>
<h2 id="VertiBench-Advancing-Feature-Distribution-Diversity-in-Vertical-Federated-Learning-Benchmarks"><a href="#VertiBench-Advancing-Feature-Distribution-Diversity-in-Vertical-Federated-Learning-Benchmarks" class="headerlink" title="VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks"></a>VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02040">http://arxiv.org/abs/2307.02040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaomin Wu, Junyi Hou, Bingsheng He</li>
<li>for: 提高 Vertical Federated Learning (VFL) 的研究和评估，因为 Privacy 限制，现有的公共实验室 VFL 数据集很少，这些数据集只能反映有限的特征分布。</li>
<li>methods: 本文提出了两个关键因素影响 VFL 性能：特征重要性和特征相关性，并提出了相应的评估指标和数据分割方法。</li>
<li>results: 本文对 cutting-edge VFL 算法进行了全面的评估，提供了 valuable 的研究指导，包括特征重要性和特征相关性对 VFL 性能的影响，以及实际 VFL 数据集的效果。<details>
<summary>Abstract</summary>
Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.
</details>
<details>
<summary>摘要</summary>
竖向联合学习（VFL）是一种重要的训练机器学习模型的方法，其中数据分布在不同的Feature中。然而，由于隐私限制，公共世界上存在的VFL数据集很少，这些数据集通常只表示一小部分的特征分布。现有的比赛通常会 resort to synthetic datasets，这些数据集来自于arbitrary的特征分裂，只 capture一部分的特征分布，导致评估算法性能的不准确。本文通过引入特征重要性和特征相关性两个关键因素，并提出相应的评估指标和数据分割方法来解决这些缺陷。此外，我们还引入了一个真实的VFL数据集，以解决图像-图像VFL场景中的欠缺。我们的全面评估 cutting-edge VFL 算法提供了有价值的意义，对未来在这个领域的研究做出了重要的贡献。
</details></li>
</ul>
<hr>
<h2 id="EHRSHOT-An-EHR-Benchmark-for-Few-Shot-Evaluation-of-Foundation-Models"><a href="#EHRSHOT-An-EHR-Benchmark-for-Few-Shot-Evaluation-of-Foundation-Models" class="headerlink" title="EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models"></a>EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02028">http://arxiv.org/abs/2307.02028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/som-shahlab/ehrshot-benchmark">https://github.com/som-shahlab/ehrshot-benchmark</a></li>
<li>paper_authors: Michael Wornow, Rahul Thapa, Ethan Steinberg, Jason Fries, Nigam Shah</li>
<li>For: The paper is written to address the challenges of applying machine learning (ML) in healthcare, specifically the lack of shared assets such as datasets, tasks, and models.* Methods: The paper contributes three main contributions: (1) a new dataset called EHRSHOT, which contains de-identified structured data from 6,712 patients’ electronic health records (EHRs) at Stanford Medicine; (2) the weights of a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients; and (3) 15 few-shot clinical prediction tasks to evaluate the performance of foundation models.* Results: The paper provides an end-to-end pipeline for the community to validate and build upon the performance of the clinical foundation model, and defines 15 few-shot clinical prediction tasks to evaluate the model’s ability to adapt to new tasks with limited training data.<details>
<summary>Abstract</summary>
While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, containing de-identified structured data from the electronic health records (EHRs) of 6,712 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaption. The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available at our Github repo here: https://github.com/som-shahlab/ehrshot-benchmark
</details>
<details>
<summary>摘要</summary>
在通用机器学习（ML）社区中，公共数据集、任务和模型的进步带来了很多利好，但是医疗机器学习（ML）的进步却受到了共享资产的缺乏的限制。成功的基础模型创造了新的挑战，需要访问共享预训练模型来验证性能效果。我们通过以下三个贡献来解决这些挑战：1. 我们发布了一个新的数据集，名为EHRSHOT，其包含了医学记录系统（EHR）中6,712名患者的去推定结构化数据。与MIMIC-III/IV和其他流行的EHR数据集不同，EHRSHOT是长期的，而不是仅仅局部ICU/ED病人。2. 我们发布了一个141M参数的临床基础模型，该模型在6.27M名患者的结构化EHR数据上进行了预训练。我们是第一个完全发布codified EHR数据的基础模型的一个，而不是只能处理文本数据，并且可以处理医疗记录中的丰富结构数据。我们提供了一个综合的执行管道，让社区可以验证和建立其性能。3. 我们定义了15个几个shot临床预测任务，以评估基础模型的样本效率和任务适应性。我们在Github上提供了代码重现结果，以及模型和数据（通过研究数据使用协议），请参考以下链接：https://github.com/som-shahlab/ehrshot-benchmark。
</details></li>
</ul>
<hr>
<h2 id="Generative-Adversarial-Networks-for-Dental-Patient-Identity-Protection-in-Orthodontic-Educational-Imaging"><a href="#Generative-Adversarial-Networks-for-Dental-Patient-Identity-Protection-in-Orthodontic-Educational-Imaging" class="headerlink" title="Generative Adversarial Networks for Dental Patient Identity Protection in Orthodontic Educational Imaging"></a>Generative Adversarial Networks for Dental Patient Identity Protection in Orthodontic Educational Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02019">http://arxiv.org/abs/2307.02019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingchuan Tian, Wilson Weixun Lu, Kelvin Weng Chiong Foong, Eugene Loh</li>
<li>For: This research aims to develop a novel area-preserving Generative Adversarial Networks (GAN) inversion technique for effectively de-identifying dental patient images, addressing privacy concerns while preserving key dental features.* Methods: The enhanced GAN Inversion methodology incorporates several deep learning models to provide end-to-end development guidance and practical application for image de-identification, adapting the context from one image to another while preserving dental features essential for oral diagnosis and dental education.* Results: The approach was assessed with varied facial pictures and achieved effective de-identification, maintaining the realism of important dental features, and was deemed useful for dental diagnostics and education by a panel of five clinicians. The generated images can be used to streamline the de-identification process of dental patient images, enhance efficiency in dental education, and create de-identified datasets for broader 2D image research.<details>
<summary>Abstract</summary>
Objectives: This research introduces a novel area-preserving Generative Adversarial Networks (GAN) inversion technique for effectively de-identifying dental patient images. This innovative method addresses privacy concerns while preserving key dental features, thereby generating valuable resources for dental education and research.   Methods: We enhanced the existing GAN Inversion methodology to maximize the preservation of dental characteristics within the synthesized images. A comprehensive technical framework incorporating several deep learning models was developed to provide end-to-end development guidance and practical application for image de-identification.   Results: Our approach was assessed with varied facial pictures, extensively used for diagnosing skeletal asymmetry and facial anomalies. Results demonstrated our model's ability to adapt the context from one image to another, maintaining compatibility, while preserving dental features essential for oral diagnosis and dental education. A panel of five clinicians conducted an evaluation on a set of original and GAN-processed images. The generated images achieved effective de-identification, maintaining the realism of important dental features and were deemed useful for dental diagnostics and education.   Clinical Significance: Our GAN model and the encompassing framework can streamline the de-identification process of dental patient images, enhancing efficiency in dental education. This method improves students' diagnostic capabilities by offering more exposure to orthodontic malocclusions. Furthermore, it facilitates the creation of de-identified datasets for broader 2D image research at major research institutions.
</details>
<details>
<summary>摘要</summary>
目标：本研究提出了一种新的面积保持式生成对抗网络（GAN）反转技术，用于有效地去标识牙科病人图像，同时保持牙科特征。这种创新方法解决了隐私问题，并为牙科教育和研究提供了有价值的资源。方法：我们对现有的GAN反转方法进行了改进，以最大化保留牙科特征在合成图像中。我们开发了一个包含多个深度学习模型的技术框架，以提供综合的开发指南和实践应用于图像去标识。结果：我们的方法在各种facial picture上进行了评估，广泛用于诊断骨骼偏位和脸部异常。结果表明我们的模型能够将上下文从一个图像传递到另一个图像，保持兼容性，同时保留牙科特征，这些特征对于牙科诊断和牙科教育非常重要。一组五名临床医生对一组原始图像和GAN处理后的图像进行评估。生成的图像实现了有效的去标识，保留了牙科特征的真实性，并被评估为用于牙科诊断和牙科教育中有用。临床意义：我们的GAN模型和包含它的框架可以快速减少牙科病人图像去标识的过程，提高牙科教育的效率。这种方法可以帮助学生提高诊断能力，并为更广泛的2D图像研究提供匿名化数据。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-GPT-4-and-Human-Graders-in-Evaluating-Praise-Given-to-Students-in-Synthetic-Dialogues"><a href="#Comparative-Analysis-of-GPT-4-and-Human-Graders-in-Evaluating-Praise-Given-to-Students-in-Synthetic-Dialogues" class="headerlink" title="Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise Given to Students in Synthetic Dialogues"></a>Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise Given to Students in Synthetic Dialogues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02018">http://arxiv.org/abs/2307.02018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dollaya Hirunyasiri, Danielle R. Thomas, Jionghao Lin, Kenneth R. Koedinger, Vincent Aleven</li>
<li>for: 本研究旨在评估一种使用大语言模型提供对教师表现的特点反馈方法，以提高教师表现。</li>
<li>methods: 本研究使用GPT-4生成30个对话，并采用零例链式思维和几例链式思维两种提示方法来评估GPT-4对特定褒奖要素的识别率。</li>
<li>results: GPT-4在特定褒奖要素上表现moderately well，但在识别教师的诚恳褒奖语言方面表现不佳，尤其在零例提示场景下。<details>
<summary>Abstract</summary>
Research suggests that providing specific and timely feedback to human tutors enhances their performance. However, it presents challenges due to the time-consuming nature of assessing tutor performance by human evaluators. Large language models, such as the AI-chatbot ChatGPT, hold potential for offering constructive feedback to tutors in practical settings. Nevertheless, the accuracy of AI-generated feedback remains uncertain, with scant research investigating the ability of models like ChatGPT to deliver effective feedback. In this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in a tutor-student setting. We use two different prompting approaches, the zero-shot chain of thought and the few-shot chain of thought, to identify specific components of effective praise based on five criteria. These approaches are then compared to the results of human graders for accuracy. Our goal is to assess the extent to which GPT-4 can accurately identify each praise criterion. We found that both zero-shot and few-shot chain of thought approaches yield comparable results. GPT-4 performs moderately well in identifying instances when the tutor offers specific and immediate praise. However, GPT-4 underperforms in identifying the tutor's ability to deliver sincere praise, particularly in the zero-shot prompting scenario where examples of sincere tutor praise statements were not provided. Future work will focus on enhancing prompt engineering, developing a more general tutoring rubric, and evaluating our method using real-life tutoring dialogues.
</details>
<details>
<summary>摘要</summary>
In this study, we evaluated 30 dialogues generated by GPT-4 in a tutor-student setting using two different prompting approaches: the zero-shot chain of thought and the few-shot chain of thought. We used five criteria to identify specific components of effective praise. We compared the results of human graders to assess the extent to which GPT-4 can accurately identify each praise criterion.We found that both zero-shot and few-shot chain of thought approaches yielded comparable results. GPT-4 performed moderately well in identifying instances when the tutor offers specific and immediate praise. However, GPT-4 underperformed in identifying the tutor's ability to deliver sincere praise, particularly when examples of sincere tutor praise statements were not provided in the zero-shot prompting scenario.Future work will focus on enhancing prompt engineering, developing a more general tutoring rubric, and evaluating our method using real-life tutoring dialogues.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Effectiveness-of-Large-Language-Models-in-Representing-Textual-Descriptions-of-Geometry-and-Spatial-Relations"><a href="#Evaluating-the-Effectiveness-of-Large-Language-Models-in-Representing-Textual-Descriptions-of-Geometry-and-Spatial-Relations" class="headerlink" title="Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations"></a>Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03678">http://arxiv.org/abs/2307.03678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Ji, Song Gao</li>
<li>for: 本研究旨在评估大语言模型（LLM）对几何体和其空间关系的表示能力。</li>
<li>methods: 研究使用GPT-2和BERT等大语言模型对几何体的WKT格式编码，然后将其编码 feed into类фика器和回归器来评估LLM生成的几何特征的有效性。</li>
<li>results: 实验显示，LLM生成的几何特征可以保持几何类型和捕捉一定的空间关系（准确率达73%），但还存在 numeric 值的估计和空间相关对象的检索等挑战。<details>
<summary>Abstract</summary>
This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="STS-CCL-Spatial-Temporal-Synchronous-Contextual-Contrastive-Learning-for-Urban-Traffic-Forecasting"><a href="#STS-CCL-Spatial-Temporal-Synchronous-Contextual-Contrastive-Learning-for-Urban-Traffic-Forecasting" class="headerlink" title="STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting"></a>STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02507">http://arxiv.org/abs/2307.02507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lincan Li, Kaixiang Yang, Fengji Luo, Jichao Bi</li>
<li>for: 这篇论文是为了解决大规模未标注交通数据中的复杂空间时间表示问题而写的。</li>
<li>methods: 本论文使用了高级对比学习方法，提出了一种新的空间时间同步Contextual Contrastive Learning（STS-CCL）模型。该模型首先提出了基本和强对抗方法 для空间时间图数据，并使用了一种学习型动态图视图生成器来进行自适应对抗。其次，该模型引入了空间时间同步对比模块（STS-CM），同时捕捉到良好的空间时间关系，并实现了图级对抗。为了进一步鉴别节点个体在负滤波中，该模型还设计了基于semantic特征和空间异质性的semantic Contextual Contrastive方法，实现了节点级对抗学习和负滤波。最后，该模型采用了一种困难的双向视图对抗训练方案，并将经典对抗损失扩展到一个整合目标函数，从而获得更好的性能。</li>
<li>results: 根据广泛的实验和评估表明，建立基于STS-CCL对抗学习模型的预测器，在交通预测任务中表现出了superior性能，胜过现有的交通预测标准准。此外，STS-CCL模型对大量的未标注交通数据进行了高效地学习和预测，因此非常适合具有数据缺乏问题的其他空间时间任务。<details>
<summary>Abstract</summary>
Efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains to be a challenging task. In considering of the dilemma, this work employs the advanced contrastive learning and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. Second, we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. To further discriminate node individuals in negative filtering, a Semantic Contextual Contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with negative filtering. Finally, we present a hard mutual-view contrastive training scheme and extend the classic contrastive loss to an integrated objective function, yielding better performance. Extensive experiments and evaluations demonstrate that building a predictor upon STS-CCL contrastive learning model gains superior performance than existing traffic forecasting benchmarks. The proposed STS-CCL is highly suitable for large datasets with only a few labeled data and other spatiotemporal tasks with data scarcity issue.
</details>
<details>
<summary>摘要</summary>
efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains a challenging task. To address this issue, this work proposes a novel Spatial-Temporal Synchronous Contextual Contrastive Learning (STS-CCL) model.First, we introduce advanced contrastive learning techniques for spatiotemporal graph data, including basic and strong augmentation methods that perturb the data in terms of graph structure and temporal characteristics. Additionally, we employ a learning-based dynamic graph view generator for adaptive augmentation.Second, we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to simultaneously capture decent spatial-temporal dependencies and realize graph-level contrasting. To further discriminate node individuals in negative filtering, we design a Semantic Contextual Contrastive method based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with negative filtering.Finally, we present a hard mutual-view contrastive training scheme and extend the classic contrastive loss to an integrated objective function, yielding better performance. Extensive experiments and evaluations demonstrate that building a predictor upon STS-CCL contrastive learning model gains superior performance than existing traffic forecasting benchmarks. The proposed STS-CCL is highly suitable for large datasets with only a few labeled data and other spatiotemporal tasks with data scarcity issues.
</details></li>
</ul>
<hr>
<h2 id="The-KiTS21-Challenge-Automatic-segmentation-of-kidneys-renal-tumors-and-renal-cysts-in-corticomedullary-phase-CT"><a href="#The-KiTS21-Challenge-Automatic-segmentation-of-kidneys-renal-tumors-and-renal-cysts-in-corticomedullary-phase-CT" class="headerlink" title="The KiTS21 Challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase CT"></a>The KiTS21 Challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01984">http://arxiv.org/abs/2307.01984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neheller/kits21">https://github.com/neheller/kits21</a></li>
<li>paper_authors: Nicholas Heller, Fabian Isensee, Dasha Trofimova, Resha Tejpaul, Zhongchen Zhao, Huai Chen, Lisheng Wang, Alex Golts, Daniel Khapun, Daniel Shats, Yoel Shoshan, Flora Gilboa-Solomon, Yasmeen George, Xi Yang, Jianpeng Zhang, Jing Zhang, Yong Xia, Mengran Wu, Zhiyang Liu, Ed Walczak, Sean McSweeney, Ranveer Vasdev, Chris Hornung, Rafat Solaiman, Jamee Schoephoerster, Bailey Abernathy, David Wu, Safa Abdulkadir, Ben Byun, Justice Spriggs, Griffin Struyk, Alexandra Austin, Ben Simpson, Michael Hagstrom, Sierra Virnig, John French, Nitin Venkatesh, Sarah Chan, Keenan Moore, Anna Jacobsen, Susan Austin, Mark Austin, Subodh Regmi, Nikolaos Papanikolopoulos, Christopher Weight</li>
<li>for: The paper is written for the 2021 Kidney and Kidney Tumor Segmentation Challenge (KiTS21) and the 2021 international conference on Medical Image Computing and Computer Assisted Interventions (MICCAI).</li>
<li>methods: The paper uses a novel annotation method that collects three separate annotations for each region of interest, and these annotations are performed in a fully transparent setting using a web-based annotation tool.</li>
<li>results: The top-performing teams achieved a significant improvement over the state of the art set in 2019, and this performance is shown to inch ever closer to human-level performance.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文是为2021年的肾脏和肾肿瘤分割挑战（KiTS21）和2021年的医学图像计算和助け手技术国际会议（MICCAI）而写的。</li>
<li>methods: 这篇论文使用了一种新的注释方法，收集了每个区域兴趣的三个独立注释，并使用了一个基于网络的注释工具进行完全透明的注释。</li>
<li>results: Top Performing Teams的性能得到了2019年的状态前进的显著改进，并且这种性能在人类水平附近。<details>
<summary>Abstract</summary>
This paper presents the challenge report for the 2021 Kidney and Kidney Tumor Segmentation Challenge (KiTS21) held in conjunction with the 2021 international conference on Medical Image Computing and Computer Assisted Interventions (MICCAI). KiTS21 is a sequel to its first edition in 2019, and it features a variety of innovations in how the challenge was designed, in addition to a larger dataset. A novel annotation method was used to collect three separate annotations for each region of interest, and these annotations were performed in a fully transparent setting using a web-based annotation tool. Further, the KiTS21 test set was collected from an outside institution, challenging participants to develop methods that generalize well to new populations. Nonetheless, the top-performing teams achieved a significant improvement over the state of the art set in 2019, and this performance is shown to inch ever closer to human-level performance. An in-depth meta-analysis is presented describing which methods were used and how they faired on the leaderboard, as well as the characteristics of which cases generally saw good performance, and which did not. Overall KiTS21 facilitated a significant advancement in the state of the art in kidney tumor segmentation, and provides useful insights that are applicable to the field of semantic segmentation as a whole.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Kidney and Kidney Tumor Segmentation Challenge" (KiTS21) was translated as "肾肿瘤分割挑战" (KiTS21)* "International Conference on Medical Image Computing and Computer Assisted Interventions" (MICCAI) was translated as "医学图像计算和计算助け手动 intervención国际会议" (MICCAI)* "novel annotation method" was translated as "新的注释方法"* "transparent setting" was translated as "透明的设定"* "web-based annotation tool" was translated as "基于网络的注释工具"* "outside institution" was translated as "外部机构"* "state of the art set" was translated as "现状下的最佳集"* "human-level performance" was translated as "人类水平的性能"* "detailed meta-analysis" was translated as "详细的元分析"* "insights" was translated as "思想"
</details></li>
</ul>
<hr>
<h2 id="SDXL-Improving-Latent-Diffusion-Models-for-High-Resolution-Image-Synthesis"><a href="#SDXL-Improving-Latent-Diffusion-Models-for-High-Resolution-Image-Synthesis" class="headerlink" title="SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"></a>SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01952">http://arxiv.org/abs/2307.01952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stability-ai/generative-models">https://github.com/stability-ai/generative-models</a></li>
<li>paper_authors: Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach</li>
<li>For: 本研究旨在提出一种基于潜在扩散模型的文本到图像生成模型，即SDXL。相比之前的稳定扩散模型，SDXL具有更大的UNet准备模型，主要是因为更多的注意块和更大的跨注意Context。* Methods: 本研究使用了多种新的conditioning方案，并在多个方向比例上训练了SDXL。此外，我们还提出了一种图像质量改进模型，用于通过后期image-to-image技术提高SDXL生成的图像的视觉准确性。* Results: 我们的实验结果表明，SDXL在比前一个版本的稳定扩散模型和黑盒状态的图像生成器中表现出了明显的改进，并达到了与之相当的图像质量。此外，我们还提供了模型代码和参数Weight，以便其他研究人员可以进行更多的研究和应用。<details>
<summary>Abstract</summary>
We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models
</details>
<details>
<summary>摘要</summary>
我们介绍SDXL，一个文本至图生成模型。与过去版本的稳定扩散模型相比，SDXL具有三倍大的UNet底层：模型参数的增加主要是因为更多的注意块和更大的跨注意条件。我们设计了多种新的条件定义方案，并将SDXL训练在多个比例的方向上。我们还引入了一个修正模型，用于在SDXL生成的样本中提高视觉实际性，使用后置的图像至图像技术。我们示出SDXL与过去版本的稳定扩散模型相比，有着截然改善的性能，并与黑盒类型的图像生成器的结果竞争。为了促进开放性的研究和透明度在大型模型训练和评估中，我们在https://github.com/Stability-AI/generative-models提供了代码和模型的重量。
</details></li>
</ul>
<hr>
<h2 id="A-Neural-Collapse-Perspective-on-Feature-Evolution-in-Graph-Neural-Networks"><a href="#A-Neural-Collapse-Perspective-on-Feature-Evolution-in-Graph-Neural-Networks" class="headerlink" title="A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks"></a>A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01951">http://arxiv.org/abs/2307.01951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kvignesh1420/gnn_collapse">https://github.com/kvignesh1420/gnn_collapse</a></li>
<li>paper_authors: Vignesh Kothapalli, Tom Tirer, Joan Bruna</li>
<li>for: 这篇论文主要是关于图结构数据的分类任务中，图内节点的分类。</li>
<li>methods: 该论文使用了图神经网络（GNNs），并进行了一系列的实验和理论分析，以了解图结构和特征的演化关系。</li>
<li>results: 研究发现，在图内节点分类任务中，也存在类似于“神经崩溃”（NC）现象，即在训练深度学习模型后，特征的 innerhalb-class variability 减少，类征的 class means 更加倾斜于某些对称结构。此外，研究还发现，这种现象不仅存在于图内节点分类任务中，还存在于图上的其他任务中。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the "Neural Collapse" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an "optimistic" mathematical model requires that the graphs obey a strict structural condition in order to possess a minimizer with exact collapse. Interestingly, this condition is viable also for heterophilic graphs and relates to recent empirical studies on settings with improved GNNs' generalization. Furthermore, by studying the gradient dynamics of the theoretical model, we provide reasoning for the partial collapse observed empirically. Finally, we present a study on the evolution of within- and between-class feature variability across layers of a well-trained GNN and contrast the behavior with spectral methods.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 在图structured data 上的分类任务中越来越受欢迎。然而，graph topology和特征进化在 GNNs 中的关系还不够清楚。在这篇论文中，我们将关注 node-wise 分类，通过社区探测在 Stochastic Block Model 图上进行示例，并通过 "Neural Collapse" (NC) 现象来探究特征进化。在训练深度分类器（例如图像分类） beyond the zero training error point 时，NC 表明深度特征中的 Within-class 变化减少，并且类别的中心对某些对称结构进行了Alignment。我们开始了一个实验，显示在 node-wise 分类 setting 中也存在类似的减少 Within-class 变化，但不如 instance-wise 情况那么严重。然后，我们进行了理论研究。具体来说，我们表明了在 graphs  obey 一定的 strict structural condition 下，才能存在一个减少 Within-class 变化的最优解。这种condition 适用于 heterophilic  graphs 也，与 latest empirical studies on improved GNNs 的泛化有关。此外，我们通过研究理论模型的梯度动态，提供了对 partial collapse 的解释。最后，我们展示了一个 well-trained GNN 的层次特征变化的进化，并与 spectral methods 进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Causal-Video-Summarizer-for-Video-Exploration"><a href="#Causal-Video-Summarizer-for-Video-Exploration" class="headerlink" title="Causal Video Summarizer for Video Exploration"></a>Causal Video Summarizer for Video Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01947">http://arxiv.org/abs/2307.01947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Hong Huang, Chao-Han Huck Yang, Pin-Yu Chen, Andrew Brown, Marcel Worring</li>
<li>for: 本研究旨在提高视频探索效果，通过多模态视频概要来解决传统视频概要模型具有固定概要，不符合用户特定需求的问题。</li>
<li>methods: 本研究提出了一种新的 causality-based 方法，名为 causal video summarizer (CVS)，以有效地捕捉视频和查询之间的交互信息，解决多模态视频概要任务。CVS 方法包括一个概率编码器和一个概率解码器。</li>
<li>results: 根据现有的多模态视频概要数据集的评估结果，实验结果显示，提出的方法在比较 state-of-the-art 方法时，具有+5.4% 的准确率和+4.92% 的 F 1 分数的提升。<details>
<summary>Abstract</summary>
Recently, video summarization has been proposed as a method to help video exploration. However, traditional video summarization models only generate a fixed video summary which is usually independent of user-specific needs and hence limits the effectiveness of video exploration. Multi-modal video summarization is one of the approaches utilized to address this issue. Multi-modal video summarization has a video input and a text-based query input. Hence, effective modeling of the interaction between a video input and text-based query is essential to multi-modal video summarization. In this work, a new causality-based method named Causal Video Summarizer (CVS) is proposed to effectively capture the interactive information between the video and query to tackle the task of multi-modal video summarization. The proposed method consists of a probabilistic encoder and a probabilistic decoder. Based on the evaluation of the existing multi-modal video summarization dataset, experimental results show that the proposed approach is effective with the increase of +5.4% in accuracy and +4.92% increase of F 1- score, compared with the state-of-the-art method.
</details>
<details>
<summary>摘要</summary>
最近，视频概要提出了方法，以便视频探索。然而，传统的视频概要模型只生成固定的视频概要，通常与用户特定需求独立，因此限制了视频探索的效iveness。多模式视频概要是解决这一问题的一种方法。多模式视频概要需要视频输入和文本基于查询输入。因此，模型视频输入和查询输入之间的交互信息的有效模型化是多模式视频概要的关键。在这项工作中，一种新的 causality-based 方法named Causal Video Summarizer (CVS) 被提出，以便有效地捕捉视频和查询之间的交互信息，解决多模式视频概要任务。该方法包括一个 probabilistic encoder 和一个 probabilistic decoder。根据现有的多模式视频概要数据集的评估结果，实验结果显示，提posed方法比状态之前的方法增加了 +5.4% 的准确率和 +4.92% 的 F 1- 分数。
</details></li>
</ul>
<hr>
<h2 id="Query-based-Video-Summarization-with-Pseudo-Label-Supervision"><a href="#Query-based-Video-Summarization-with-Pseudo-Label-Supervision" class="headerlink" title="Query-based Video Summarization with Pseudo Label Supervision"></a>Query-based Video Summarization with Pseudo Label Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01945">http://arxiv.org/abs/2307.01945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Hong Huang, Luka Murn, Marta Mrak, Marcel Worring</li>
<li>for: This paper is written for improving the performance of deep video summarization models by using self-supervised learning and pseudo labels to address the data sparsity challenge.</li>
<li>methods: The paper proposes using segment-level pseudo labels generated based on existing human-defined frame-level labels to pre-train a supervised deep model. It also introduces a semantics booster to generate context-aware query representations and mutual attention to capture interactive information between visual and textual modalities.</li>
<li>results: The proposed video summarization algorithm achieves state-of-the-art performance on three commonly-used video summarization benchmarks.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了通过自我指导学习和 pseudo labels 来提高深度视频概要模型的性能，解决数据稀缺问题。</li>
<li>methods: 论文提议使用基于现有人 definED 帧级标签的 segment-level pseudo labels 来预训练一个深度模型。它还引入了 semantics booster 来生成上下文感知的查询表示，以及 mutual attention 来捕捉视觉和文本模式之间的互动信息。</li>
<li>results: 提posed的视频概要算法在三个常用的视频概要 benchmark 上达到了国际最佳性能。<details>
<summary>Abstract</summary>
Existing datasets for manually labelled query-based video summarization are costly and thus small, limiting the performance of supervised deep video summarization models. Self-supervision can address the data sparsity challenge by using a pretext task and defining a method to acquire extra data with pseudo labels to pre-train a supervised deep model. In this work, we introduce segment-level pseudo labels from input videos to properly model both the relationship between a pretext task and a target task, and the implicit relationship between the pseudo label and the human-defined label. The pseudo labels are generated based on existing human-defined frame-level labels. To create more accurate query-dependent video summaries, a semantics booster is proposed to generate context-aware query representations. Furthermore, we propose mutual attention to help capture the interactive information between visual and textual modalities. Three commonly-used video summarization benchmarks are used to thoroughly validate the proposed approach. Experimental results show that the proposed video summarization algorithm achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Currently available datasets for manually labeled query-based video summarization are limited and expensive, which hinders the performance of supervised deep video summarization models. Self-supervision can address this issue by using a pretext task and acquiring extra data with pseudo labels to pre-train a supervised deep model. In this work, we introduce segment-level pseudo labels from input videos to properly model the relationship between the pretext task and the target task, as well as the implicit relationship between the pseudo label and the human-defined label. The pseudo labels are generated based on existing human-defined frame-level labels. To create more accurate query-dependent video summaries, a semantics booster is proposed to generate context-aware query representations. Additionally, we propose mutual attention to capture the interactive information between visual and textual modalities. Three commonly-used video summarization benchmarks are used to thoroughly validate the proposed approach. Experimental results show that the proposed video summarization algorithm achieves state-of-the-art performance.Here's the translation in Traditional Chinese:现有的手动标注的查询基本视频摘要数据集是有限且昂贵的，这限制了深度视频摘要模型的表现。自我超vision可以解决这个问题，使用预text任务和获取额外数据的pseudo标签进行预训深度模型。在这个工作中，我们引入input视频中的段级pseudo标签，以正确地模型预text任务和目标任务之间的关系，以及pseudo标签和人 definied标签之间的潜在关系。pseudo标签是基于现有的人definied帧级标签。为了创建更加精确的查询基本视频摘要，我们提议了semantics booster来生成上下文感知查询表现。此外，我们提议了互动注意力，以帮助捕捉视觉和文本Modalities之间的互动信息。三个通用的视频摘要 benchmark是用来充分验证我们的方法。实验结果显示，我们的 виде摘要算法实现了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Concept2Box-Joint-Geometric-Embeddings-for-Learning-Two-View-Knowledge-Graphs"><a href="#Concept2Box-Joint-Geometric-Embeddings-for-Learning-Two-View-Knowledge-Graphs" class="headerlink" title="Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs"></a>Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01933">http://arxiv.org/abs/2307.01933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Huang, Daheng Wang, Binxuan Huang, Chenwei Zhang, Jingbo Shang, Yan Liang, Zhengyang Wang, Xian Li, Christos Faloutsos, Yizhou Sun, Wei Wang</li>
<li>for: 本研究旨在嵌入大规模关系数据，以满足现实世界应用。</li>
<li>methods: 本方法使用双重几何表示，一是概念盒子表示，另一是实体向量表示。概念盒子表示学习层次结构和复杂关系，盒子体积可以表示概念粒度。实体向量表示学习与概念盒子的关系。</li>
<li>results: 实验表明，Concept2Box可以有效地嵌入两种视图的关系数据。<details>
<summary>Abstract</summary>
Knowledge graph embeddings (KGE) have been extensively studied to embed large-scale relational data for many real-world applications. Existing methods have long ignored the fact many KGs contain two fundamentally different views: high-level ontology-view concepts and fine-grained instance-view entities. They usually embed all nodes as vectors in one latent space. However, a single geometric representation fails to capture the structural differences between two views and lacks probabilistic semantics towards concepts' granularity. We propose Concept2Box, a novel approach that jointly embeds the two views of a KG using dual geometric representations. We model concepts with box embeddings, which learn the hierarchy structure and complex relations such as overlap and disjoint among them. Box volumes can be interpreted as concepts' granularity. Different from concepts, we model entities as vectors. To bridge the gap between concept box embeddings and entity vector embeddings, we propose a novel vector-to-box distance metric and learn both embeddings jointly. Experiments on both the public DBpedia KG and a newly-created industrial KG showed the effectiveness of Concept2Box.
</details>
<details>
<summary>摘要</summary>
知识图embeddings（KGE）已广泛研究以涵盖大规模关系数据的多种实际应用。现有方法长期忽视了许多知识图中的两种基本视图：高级 ontology-view 概念和细化实例-view 实体。它们通常将所有节点embed为一个笛卡尔空间中的向量。然而，单一的几何表示无法捕捉两种视图之间的结构差异，缺乏概率semantics towards 概念的粒度。我们提出了Concept2Box，一种新的方法，它在两种视图中同时使用双几何表示。我们使用盒形 embeddings来模型概念，它可以学习层次结构和复杂关系，如穿梭和分立。盒体体积可以被解释为概念的粒度。与概念不同，我们使用向量来模型实体。为了将盒形 embeddings和向量 embeddings相互关联，我们提出了一种新的向量-盒形距离度量，并同时学习两种embeddings。经过在公共DBpedia知识图和自己创建的工业知识图的实验，Concept2Box的效果得到了证明。
</details></li>
</ul>
<hr>
<h2 id="MDI-A-Flexible-Random-Forest-Based-Feature-Importance-Framework"><a href="#MDI-A-Flexible-Random-Forest-Based-Feature-Importance-Framework" class="headerlink" title="MDI+: A Flexible Random Forest-Based Feature Importance Framework"></a>MDI+: A Flexible Random Forest-Based Feature Importance Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01932">http://arxiv.org/abs/2307.01932</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csinva/imodels">https://github.com/csinva/imodels</a></li>
<li>paper_authors: Abhineet Agarwal, Ana M. Kenney, Yan Shuo Tan, Tiffany M. Tang, Bin Yu</li>
<li>for: 本研究旨在提出一种可以扩展MDI（Mean Decrease in Impurity）的特征重要性度量方法，以便在随机森林（Random Forest）模型中更好地评估特征的重要性。</li>
<li>methods: 本研究使用了MDI的解释，即每棵树中的特征$X_k$的MDI等于Linear regression中的非正则化$R^2$值。基于这个解释，我们提出了一种更加灵活的特征重要性框架 called MDI+。MDI+可以让分析者根据数据结构选择合适的Generalized Linear Models（GLMs）和度量，并且可以减少树见的偏见。</li>
<li>results: 我们通过数据驱动的 simulations 示例了MDI+在标准特征重要性度量方法（如MFE、Recursive Feature Elimination等）的比较，并发现MDI+可以更好地预测标准特征。此外，我们还应用MDI+于两个实际案例（药物响应预测和乳腺癌分型），并发现MDI+可以提取稳定的预测基因。<details>
<summary>Abstract</summary>
Mean decrease in impurity (MDI) is a popular feature importance measure for random forests (RFs). We show that the MDI for a feature $X_k$ in each tree in an RF is equivalent to the unnormalized $R^2$ value in a linear regression of the response on the collection of decision stumps that split on $X_k$. We use this interpretation to propose a flexible feature importance framework called MDI+. Specifically, MDI+ generalizes MDI by allowing the analyst to replace the linear regression model and $R^2$ metric with regularized generalized linear models (GLMs) and metrics better suited for the given data structure. Moreover, MDI+ incorporates additional features to mitigate known biases of decision trees against additive or smooth models. We further provide guidance on how practitioners can choose an appropriate GLM and metric based upon the Predictability, Computability, Stability framework for veridical data science. Extensive data-inspired simulations show that MDI+ significantly outperforms popular feature importance measures in identifying signal features. We also apply MDI+ to two real-world case studies on drug response prediction and breast cancer subtype classification. We show that MDI+ extracts well-established predictive genes with significantly greater stability compared to existing feature importance measures. All code and models are released in a full-fledged python package on Github.
</details>
<details>
<summary>摘要</summary>
《Mean Decrease in Impurity (MDI) is a popular feature importance measure for random forests (RFs). We show that the MDI for a feature $X_k$ in each tree in an RF is equivalent to the unnormalized $R^2$ value in a linear regression of the response on the collection of decision stumps that split on $X_k$. We use this interpretation to propose a flexible feature importance framework called MDI+. Specifically, MDI+ generalizes MDI by allowing the analyst to replace the linear regression model and $R^2$ metric with regularized generalized linear models (GLMs) and metrics better suited for the given data structure. Moreover, MDI+ incorporates additional features to mitigate known biases of decision trees against additive or smooth models. We further provide guidance on how practitioners can choose an appropriate GLM and metric based upon the Predictability, Computability, Stability framework for veridical data science. Extensive data-inspired simulations show that MDI+ significantly outperforms popular feature importance measures in identifying signal features. We also apply MDI+ to two real-world case studies on drug response prediction and breast cancer subtype classification. We show that MDI+ extracts well-established predictive genes with significantly greater stability compared to existing feature importance measures. All code and models are released in a full-fledged python package on Github.》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Learning-ECG-signal-features-without-backpropagation"><a href="#Learning-ECG-signal-features-without-backpropagation" class="headerlink" title="Learning ECG signal features without backpropagation"></a>Learning ECG signal features without backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01930">http://arxiv.org/abs/2307.01930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Péter Pósfay, Marcell T. Kurbucz, Péter Kovács, Antal Jakovác</li>
<li>for: 本研究旨在提出一种新的时间序列数据表示学习方法，以提高下游任务 such as 分类和预测的效果、范围和可应用性。</li>
<li>methods: 该方法基于物理学的想法，通过数据驱动的方式构建一个减少的表示，能够捕捉数据的下面结构和任务特定信息，同时仍保持INTUITIVE、可读性和可验证性。</li>
<li>results: 在ECG信号分类任务中，该方法实现了state-of-the-art表现。<details>
<summary>Abstract</summary>
Representation learning has become a crucial area of research in machine learning, as it aims to discover efficient ways of representing raw data with useful features to increase the effectiveness, scope and applicability of downstream tasks such as classification and prediction. In this paper, we propose a novel method to generate representations for time series-type data. This method relies on ideas from theoretical physics to construct a compact representation in a data-driven way, and it can capture both the underlying structure of the data and task-specific information while still remaining intuitive, interpretable and verifiable. This novel methodology aims to identify linear laws that can effectively capture a shared characteristic among samples belonging to a specific class. By subsequently utilizing these laws to generate a classifier-agnostic representation in a forward manner, they become applicable in a generalized setting. We demonstrate the effectiveness of our approach on the task of ECG signal classification, achieving state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>关键词： representational learning，时序数据，简化表示，物理理论，ECG信号分类研究领域：机器学习，数据分析摘要：本文提出了一种新的方法，用于生成时序数据的表示。这种方法基于物理理论的想法，在数据驱动的方式下构建了一个紧凑的表示，可以捕捉数据的内在结构和任务特定的信息，同时仍保持易于理解、可读性和验证性。这种新方法的目标是发现 Shared Characteristic 的 linear law，以便在特定类别中的样本之间建立共同的表示。通过后续使用这些法律生成一种类器无关的表示，它们在通用设定下可以应用。我们在 ECG 信号分类任务中实现了state-of-the-art 的性能。简化中文：研究领域：机器学习，数据分析摘要：本文提出了一种新的方法，用于生成时序数据的表示。这种方法基于物理理论的想法，可以捕捉数据的内在结构和任务特定的信息，同时仍保持易于理解、可读性和验证性。这种新方法的目标是发现 Shared Characteristic 的 linear law，以便在特定类别中的样本之间建立共同的表示。详细描述： Representational learning 已成为机器学习中的一个关键领域，旨在找到有效的表示方式，以提高下游任务的效果、范围和可应用性。本文提出了一种基于物理理论的方法，用于生成时序数据的表示。这种方法可以捕捉数据的内在结构和任务特定的信息，同时仍保持易于理解、可读性和验证性。本文的主要贡献在于提出了一种新的方法，用于生成时序数据的表示。这种方法基于物理理论的想法，可以捕捉数据的内在结构和任务特定的信息，同时仍保持易于理解、可读性和验证性。这种新方法的目标是发现 Shared Characteristic 的 linear law，以便在特定类别中的样本之间建立共同的表示。通过实验，我们在 ECG 信号分类任务中实现了state-of-the-art 的性能。这表明，我们的方法可以有效地捕捉 ECG 信号的特征，并且可以在通用设定下应用。结语：本文提出了一种基于物理理论的方法，用于生成时序数据的表示。这种方法可以捕捉数据的内在结构和任务特定的信息，同时仍保持易于理解、可读性和验证性。这种新方法的目标是发现 Shared Characteristic 的 linear law，以便在特定类别中的样本之间建立共同的表示。我们在 ECG 信号分类任务中实现了state-of-the-art 的性能，这表明我们的方法可以有效地捕捉 ECG 信号的特征。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-Generation-and-Understanding-of-Big-Code-for-AI-Assisted-Programming-A-Review"><a href="#Natural-Language-Generation-and-Understanding-of-Big-Code-for-AI-Assisted-Programming-A-Review" class="headerlink" title="Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review"></a>Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02503">http://arxiv.org/abs/2307.02503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Man Fai Wong, Shangxin Guo, Ching Nam Hang, Siu Wai Ho, Chee Wei Tan</li>
<li>for: 该论文探讨了自然语言处理（NLP）技术在程式设计任务中的应用，尤其是基于变数大型语言模型（LLMs）在大量代码（Big Code）上的训练。</li>
<li>methods: 该论文介绍了主要的LLMs，并探讨了这些模型在程式设计相关任务中的应用，包括程式码生成、程式码完成、程式码翻译、程式码精炼、程式码摘要、错误探测和复制探测。</li>
<li>results: 该论文总结了NLP技术在程式设计任务中的应用，包括GitHub Copilot和DeepMind AlphaCode等应用，并探讨了在扩展至Apple Xcode的 mobil 软件开发中的挑战和机遇。<details>
<summary>Abstract</summary>
This paper provides a comprehensive review of the literature concerning the utilization of Natural Language Processing (NLP) techniques, with a particular focus on transformer-based large language models (LLMs) trained using Big Code, within the domain of AI-assisted programming tasks. LLMs, augmented with software naturalness, have played a crucial role in facilitating AI-assisted programming applications, including code generation, code completion, code translation, code refinement, code summarization, defect detection, and clone detection. Notable examples of such applications include the GitHub Copilot powered by OpenAI's Codex and DeepMind AlphaCode. This paper presents an overview of the major LLMs and their applications in downstream tasks related to AI-assisted programming. Furthermore, it explores the challenges and opportunities associated with incorporating NLP techniques with software naturalness in these applications, with a discussion on extending AI-assisted programming capabilities to Apple's Xcode for mobile software development. This paper also presents the challenges of and opportunities for incorporating NLP techniques with software naturalness, empowering developers with advanced coding assistance and streamlining the software development process.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Natural Language Processing" (NLP) is translated as "自然语言处理" (Zìrán yǔyán gōngchá)* "Large Language Models" (LLMs) is translated as "大语言模型" (Dà yǔyán módelì)* "Big Code" is translated as "大代码" (Dà biǎo código)* "AI-assisted programming" is translated as "人工智能助成编程" (Réngōng zhìnéng bùxí biānjiāng)* "Software naturalness" is translated as "软件自然性" (Ròngjiàn zìrán xìng)* "GitHub Copilot" is translated as "GitHub 助手" (GitHub zhùshǒu)* "OpenAI's Codex" is translated as "OpenAI 的 Codex" (OpenAI de Codex)* "DeepMind AlphaCode" is translated as "DeepMind 的 AlphaCode" (DeepMind de AlphaCode)* "Xcode" is translated as "Xcode" (Xcode)
</details></li>
</ul>
<hr>
<h2 id="Robots-That-Ask-For-Help-Uncertainty-Alignment-for-Large-Language-Model-Planners"><a href="#Robots-That-Ask-For-Help-Uncertainty-Alignment-for-Large-Language-Model-Planners" class="headerlink" title="Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners"></a>Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01928">http://arxiv.org/abs/2307.01928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, Anirudha Majumdar</li>
<li>for: 本研究旨在提供一个测量和调整大型自然语言模型（LLM）基于的 планування器的不确定性框架，以便确保这些模型在实际应用中能够有效地完成任务。</li>
<li>methods: 本研究基于对称预测理论，使用对称预测来提供任务完成的 statistically guarantee，同时对于复杂多步 планування设定中的不确定性进行调整。</li>
<li>results: 实验结果显示， KnowNo 在多种 simulated 和实际 robot 设定中表现出色，可以优化效率和自主性，同时提供正式的保证。 KnowNo 可以与 LLM 直接使用，不需要模型调整，并且建议一种轻量级的不确定性模型，可以与基础模型相容。<details>
<summary>Abstract</summary>
Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）表现出广泛的有前途能力——从步骤规划到通过常识理解——可能提供 robot 中的用途，但仍然容易做出自信的预测。在这个工作中，我们提出了 KnowNo，它是一个测量和调整 LLM 基于的 плаanner 的不确定性框架，以便它们知道自己不确定的时候，并在需要时请求帮助。KnowNo 基于对照预测理论，以提供任务完成的 Statistical 保证，同时最小化人工帮助在复杂多步 плаanning 环境中。实验显示，KnowNo 在各种模拟和实际 robot 环境中表现出色，比modern baseline（可能包括集合或广泛提升）在任务完成和自主性方面表现更好，同时提供了正式保证。KnowNo 可以与 LLM 直接使用，不需要模型调整，并建议一个轻量级的不确定性模型，可以与基础模型的能力相互补充。详情请参考 <https://robot-help.github.io> website。
</details></li>
</ul>
<hr>
<h2 id="Stranding-Risk-for-Underactuated-Vessels-in-Complex-Ocean-Currents-Analysis-and-Controllers"><a href="#Stranding-Risk-for-Underactuated-Vessels-in-Complex-Ocean-Currents-Analysis-and-Controllers" class="headerlink" title="Stranding Risk for Underactuated Vessels in Complex Ocean Currents: Analysis and Controllers"></a>Stranding Risk for Underactuated Vessels in Complex Ocean Currents: Analysis and Controllers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01917">http://arxiv.org/abs/2307.01917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Doering, Marius Wiggert, Hanna Krasowski, Manan Doshi, Pierre F. J. Lermusiaux, Claire J. Tomlin</li>
<li>For: The paper aims to improve the safety of low-propulsion vessels navigating in high-risk regions by developing a feedback control policy that takes into account the uncertainty of ocean currents.* Methods: The authors use a combination of analytical and numerical methods, including the Hamilton-Jacobi Multi-Time Reachability (HJ-MTR) framework, to synthesize a feedback policy that guarantees safe operation in the presence of forecast errors.* Results: The authors demonstrate the effectiveness of their approach through large-scale simulations, showing that their method significantly improves safety over baseline methods while still achieving timely arrival at the destination.Here’s the same information in Simplified Chinese:* For: 这篇论文旨在提高低推力船舶在高风险区域的安全性，通过开发一种Feedback控制策略，考虑到海流预测错误的不确定性。* Methods: 作者使用了一种组合分析和数值方法，包括Hamilton-Jacobi Multi-Time Reachability（HJ-MTR）框架，Synthesize一种Feedback策略，在预测错误存在的情况下保证安全运行。* Results: 作者通过大规模仿真实验，证明了他们的方法在安全性和快速到达目的地之间具有显著改善，而且仍然能够保证船舶的安全运行。<details>
<summary>Abstract</summary>
Low-propulsion vessels can take advantage of powerful ocean currents to navigate towards a destination. Recent results demonstrated that vessels can reach their destination with high probability despite forecast errors. However, these results do not consider the critical aspect of safety of such vessels: because of their low propulsion which is much smaller than the magnitude of currents, they might end up in currents that inevitably push them into unsafe areas such as shallow areas, garbage patches, and shipping lanes. In this work, we first investigate the risk of stranding for free-floating vessels in the Northeast Pacific. We find that at least 5.04% would strand within 90 days. Next, we encode the unsafe sets as hard constraints into Hamilton-Jacobi Multi-Time Reachability (HJ-MTR) to synthesize a feedback policy that is equivalent to re-planning at each time step at low computational cost. While applying this policy closed-loop guarantees safe operation when the currents are known, in realistic situations only imperfect forecasts are available. We demonstrate the safety of our approach in such realistic situations empirically with large-scale simulations of a vessel navigating in high-risk regions in the Northeast Pacific. We find that applying our policy closed-loop with daily re-planning on new forecasts can ensure safety with high probability even under forecast errors that exceed the maximal propulsion. Our method significantly improves safety over the baselines and still achieves a timely arrival of the vessel at the destination.
</details>
<details>
<summary>摘要</summary>
低推力船只可以利用强大的海洋流动来导航到目的地。最新的结果表明，船只可以快速到达目的地，即使预测错误也可以达到高probability。然而，这些结果不考虑船只的安全性：由于它们的推力非常小，可能会被强大的流动推向危险区域，如浅水区、垃圾堆和航道。在这项工作中，我们首先调查了偏离船只在北太平洋的风险。我们发现至少5.04%的船只在90天内会偏离。接着，我们将危险集编码为硬制约into Hamilton-Jacobi Multi-Time Reachability（HJ-MTR），以生成一个等价于重新规划的反馈策略。当应用这种策略时，在计算成本低的情况下，可以保证船只的安全运行。然而，在实际情况下，只有不准确的预测available。我们通过大规模的 simulate船只在高风险区域的高效实际情况来证明我们的方法的安全性。我们发现，在每天重新规划新的预测基础上应用我们的策略可以保证船只的安全性，即使预测错误超过最大推力。我们的方法可以大幅提高船只的安全性，同时仍能达到快速到达目的地的目的。
</details></li>
</ul>
<hr>
<h2 id="Maximizing-Seaweed-Growth-on-Autonomous-Farms-A-Dynamic-Programming-Approach-for-Underactuated-Systems-Navigating-on-Uncertain-Ocean-Currents"><a href="#Maximizing-Seaweed-Growth-on-Autonomous-Farms-A-Dynamic-Programming-Approach-for-Underactuated-Systems-Navigating-on-Uncertain-Ocean-Currents" class="headerlink" title="Maximizing Seaweed Growth on Autonomous Farms: A Dynamic Programming Approach for Underactuated Systems Navigating on Uncertain Ocean Currents"></a>Maximizing Seaweed Growth on Autonomous Farms: A Dynamic Programming Approach for Underactuated Systems Navigating on Uncertain Ocean Currents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01916">http://arxiv.org/abs/2307.01916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Killer, Marius Wiggert, Hanna Krasowski, Manan Doshi, Pierre F. J. Lermusiaux, Claire J. Tomlin</li>
<li>for: 这个论文旨在设计一种控制器，以便在开放海洋中的大规模自主海藻养殖园中 maximizes 海藻生长。</li>
<li>methods: 这个论文使用了动态计划法来解决海藻生长控制问题，并提出了三种扩展方法来处理不准确的预测。</li>
<li>results: 实验结果表明，使用这种控制器可以实现95.8%的最佳生长水平，只使用5天的预测数据。这表明在实际情况下，使用低功率推进和优化控制可以提高浮动海藻养殖园中的海藻生长。<details>
<summary>Abstract</summary>
Seaweed biomass offers significant potential for climate mitigation, but large-scale, autonomous open-ocean farms are required to fully exploit it. Such farms typically have low propulsion and are heavily influenced by ocean currents. We want to design a controller that maximizes seaweed growth over months by taking advantage of the non-linear time-varying ocean currents for reaching high-growth regions. The complex dynamics and underactuation make this challenging even when the currents are known. This is even harder when only short-term imperfect forecasts with increasing uncertainty are available. We propose a dynamic programming-based method to efficiently solve for the optimal growth value function when true currents are known. We additionally present three extensions when as in reality only forecasts are known: (1) our methods resulting value function can be used as feedback policy to obtain the growth-optimal control for all states and times, allowing closed-loop control equivalent to re-planning at every time step hence mitigating forecast errors, (2) a feedback policy for long-term optimal growth beyond forecast horizons using seasonal average current data as terminal reward, and (3) a discounted finite-time Dynamic Programming (DP) formulation to account for increasing ocean current estimate uncertainty. We evaluate our approach through 30-day simulations of floating seaweed farms in realistic Pacific Ocean current scenarios. Our method demonstrates an achievement of 95.8% of the best possible growth using only 5-day forecasts. This confirms the feasibility of using low-power propulsion and optimal control for enhanced seaweed growth on floating farms under real-world conditions.
</details>
<details>
<summary>摘要</summary>
海藻质量具有气候缓和潜在，但需要大规模、自主的开放海洋农场来完全利用其潜力。这些农场通常具有低速度和受海洋流动影响，我们想要设计一个控制器，以最大化海藻生长在月分内的时间，通过利用不对称时间变化的海洋流动来到达高生长区域。由于流动的复杂dinamics和对控制的具体影响，这是一个具有挑战性的任务，甚至当真实流动currents是知道的时候。我们提出了一种基于动态计划的方法，以有效地解决最佳生长值函数的问题，当真实流动currents是知道的时候。我们还提出了三个扩展，包括：1. 我们的方法所得到的值函数可以作为反馈政策，以获得生长最佳控制，即使只有5天的预测。2. 一个以季节平均流动为终端赏金的负回授政策，以便在预测时间以外的长期优化生长。3. 一个折衣finite-time动态计划形式，以考虑海洋流动估计uncertainty的增加。我们通过30天的 simulate Pacific Ocean current scenarios中的浮动海藻农场来评估我们的方法。我们的方法能够实现95.8%的最佳生长，仅使用5天的预测。这证明了在现实情况下使用低功率propulsion和优化控制可以实现更好的海藻生长。
</details></li>
</ul>
<hr>
<h2 id="ClimateLearn-Benchmarking-Machine-Learning-for-Weather-and-Climate-Modeling"><a href="#ClimateLearn-Benchmarking-Machine-Learning-for-Weather-and-Climate-Modeling" class="headerlink" title="ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling"></a>ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01909">http://arxiv.org/abs/2307.01909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aditya-grover/climate-learn">https://github.com/aditya-grover/climate-learn</a></li>
<li>paper_authors: Tung Nguyen, Jason Jewik, Hritik Bansal, Prakhar Sharma, Aditya Grover</li>
<li>for: 该研究旨在提供一个大规模、开源的机器学习库，用于推进气候科学研究。</li>
<li>methods: 该库使用PyTorch进行实现，包括数据处理管道（如ERA5、CMIP6、PRISM）、现有的深度学习模型（如Transformers、ResNets）以及评估方法。</li>
<li>results: 该研究表明，该库可以帮助解决气候科学中一些核心问题，如天气预测和气候下降。同时，该库还提供了详细的文档、贡献指南和快速入门教程，以扩大访问和推动社区的成长。<details>
<summary>Abstract</summary>
Modeling weather and climate is an essential endeavor to understand the near- and long-term impacts of climate change, as well as inform technology and policymaking for adaptation and mitigation efforts. In recent years, there has been a surging interest in applying data-driven methods based on machine learning for solving core problems such as weather forecasting and climate downscaling. Despite promising results, much of this progress has been impaired due to the lack of large-scale, open-source efforts for reproducibility, resulting in the use of inconsistent or underspecified datasets, training setups, and evaluations by both domain scientists and artificial intelligence researchers. We introduce ClimateLearn, an open-source PyTorch library that vastly simplifies the training and evaluation of machine learning models for data-driven climate science. ClimateLearn consists of holistic pipelines for dataset processing (e.g., ERA5, CMIP6, PRISM), implementation of state-of-the-art deep learning models (e.g., Transformers, ResNets), and quantitative and qualitative evaluation for standard weather and climate modeling tasks. We supplement these functionalities with extensive documentation, contribution guides, and quickstart tutorials to expand access and promote community growth. We have also performed comprehensive forecasting and downscaling experiments to showcase the capabilities and key features of our library. To our knowledge, ClimateLearn is the first large-scale, open-source effort for bridging research in weather and climate modeling with modern machine learning systems. Our library is available publicly at https://github.com/aditya-grover/climate-learn.
</details>
<details>
<summary>摘要</summary>
Modeling 天气和气候是一项重要的尝试，以便理解气候变化的短期和长期影响，以及为适应和遏制尝试提供技术和政策。在过去几年中，对使用数据驱动方法基于机器学习解决气象预报和气候下降的核心问题有很大的兴趣。尽管有了明显的进步，但这些进步受到了大规模、开源的尝试的缺乏，导致用户们使用不一致或不充分的数据集、训练setup和评估方法，从而限制了模型的可重复性和可靠性。我们介绍了 ClimateLearn，一个基于 PyTorch 的开源库，可以大大简化气象学习模型的训练和评估。ClimateLearn 包括气象数据处理的整体管道（如 ERA5、CMIP6 和 PRISM），状态 искус理智慧模型的实现（如 Transformers 和 ResNets），以及量化和质量评估方法 для标准气象预报和气候模型任务。我们还提供了广泛的文档、贡献指南和快速入门教程，以扩大访问权限并推动社区的增长。我们还进行了全面的预测和下降实验，以示出我们库的能力和关键特点。到我们所知，ClimateLearn 是首个大规模、开源的气象学习模型与现代机器学习系统之间的桥梁。我们的库可以在 <https://github.com/aditya-grover/climate-learn> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Math-Agents-Computational-Infrastructure-Mathematical-Embedding-and-Genomics"><a href="#Math-Agents-Computational-Infrastructure-Mathematical-Embedding-and-Genomics" class="headerlink" title="Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics"></a>Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02502">http://arxiv.org/abs/2307.02502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Melanie Swan, Takashi Kido, Eric Roland, Renato P. dos Santos</li>
<li>for: 这项研究旨在使用数学代理人和数学嵌入来推动生成AI的进步，以及应用这些技术解决生物信息系统龄化问题。</li>
<li>methods: 该项目使用GPT基于的工作流程将文献中的方程转换为LaTeX和Python格式，并利用LLMs作为自然语言界面，提供人机AI对话和大规模AI支持的计算基础设施。</li>
<li>results: 该项目通过使用数学代理人和数学嵌入来解决生物信息系统龄化问题，并通过使用生成AI分析长期健康记录，使用SIR精度医疗模型分析 causal 关系。<details>
<summary>Abstract</summary>
The advancement in generative AI could be boosted with more accessible mathematics. Beyond human-AI chat, large language models (LLMs) are emerging in programming, algorithm discovery, and theorem proving, yet their genomics application is limited. This project introduces Math Agents and mathematical embedding as fresh entries to the "Moore's Law of Mathematics", using a GPT-based workflow to convert equations from literature into LaTeX and Python formats. While many digital equation representations exist, there's a lack of automated large-scale evaluation tools. LLMs are pivotal as linguistic user interfaces, providing natural language access for human-AI chat and formal languages for large-scale AI-assisted computational infrastructure. Given the infinite formal possibility spaces, Math Agents, which interact with math, could potentially shift us from "big data" to "big math". Math, unlike the more flexible natural language, has properties subject to proof, enabling its use beyond traditional applications like high-validation math-certified icons for AI alignment aims. This project aims to use Math Agents and mathematical embeddings to address the ageing issue in information systems biology by applying multiscalar physics mathematics to disease models and genomic data. Generative AI with episodic memory could help analyse causal relations in longitudinal health records, using SIR Precision Health models. Genomic data is suggested for addressing the unsolved Alzheimer's disease problem.
</details>
<details>
<summary>摘要</summary>
“生成AI的进步可能会受到更多的易于访问的数学支持。在大语言模型（LLM）的出现之前，程式语言和数学问题的发现、证明也在发展中。然而，这些领域的应用尚未推广到 genomics 领域。这个项目将引入“数学代理人”和“数学嵌入”作为“摩尔的法则”的新入口，使用基于 GPT 的 workflow 将文献中的公式转换为 LaTeX 和 Python 格式。处理大量数据的数学表现方式已经存在许多，但是没有自动化的大规模评估工具。LLMs 作为人类语言界面，提供了自然语言访问，并且可以帮助大规模的 AI 辅助 computional 基础设施。由于数学有许多固定的性质，可以进行证明，因此它可以在传统应用领域以外进行应用。这个项目将使用数学代理人和数学嵌入来解决生物信息系统中的年龄问题，并且运用多尺度物理数学来应用疾病模型和 genomic 数据。生成 AI  WITH  episodic 记忆可以分析循环健康记录中的 causal 关系，使用 SIR 精度健康模型。 genomic 数据被建议用于解决阿兹海默症的问题。”
</details></li>
</ul>
<hr>
<h2 id="Concept-Based-Explanations-to-Test-for-False-Causal-Relationships-Learned-by-Abusive-Language-Classifiers"><a href="#Concept-Based-Explanations-to-Test-for-False-Causal-Relationships-Learned-by-Abusive-Language-Classifiers" class="headerlink" title="Concept-Based Explanations to Test for False Causal Relationships Learned by Abusive Language Classifiers"></a>Concept-Based Explanations to Test for False Causal Relationships Learned by Abusive Language Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01900">http://arxiv.org/abs/2307.01900</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/isarnejad/global-sufficiency">https://github.com/isarnejad/global-sufficiency</a></li>
<li>paper_authors: Isar Nejadgholi, Svetlana Kiritchenko, Kathleen C. Fraser, Esma Balkır</li>
<li>for: The paper is written to address the issue of classifiers learning false causal relationships between concepts and labels, specifically the concept of negative emotions in abusive language.</li>
<li>methods: The paper uses three well-known abusive language classifiers trained on large English datasets, and assesses their accuracy on a challenge set across all decision thresholds. Additionally, the paper introduces concept-based explanation metrics to assess the influence of the concept on the labels.</li>
<li>results: The paper finds that the classifiers have learned unwanted dependencies on the concept of negative emotions, and that these dependencies can compromise classification accuracy. The paper also introduces concept-based explanation metrics to compare classifiers regarding the degree of false global sufficiency they have learned between a concept and a label.<details>
<summary>Abstract</summary>
Classifiers tend to learn a false causal relationship between an over-represented concept and a label, which can result in over-reliance on the concept and compromised classification accuracy. It is imperative to have methods in place that can compare different models and identify over-reliances on specific concepts. We consider three well-known abusive language classifiers trained on large English datasets and focus on the concept of negative emotions, which is an important signal but should not be learned as a sufficient feature for the label of abuse. Motivated by the definition of global sufficiency, we first examine the unwanted dependencies learned by the classifiers by assessing their accuracy on a challenge set across all decision thresholds. Further, recognizing that a challenge set might not always be available, we introduce concept-based explanation metrics to assess the influence of the concept on the labels. These explanations allow us to compare classifiers regarding the degree of false global sufficiency they have learned between a concept and a label.
</details>
<details>
<summary>摘要</summary>
классификаторы有很大可能学习一种假的 causal 关系 между一个过度表达的概念和一个标签，这可能导致依赖于该概念并降低分类精度。因此需要有方法来比较不同的模型并识别特定概念上的过度依赖。我们使用三种广泛使用的负面语言分类器，并将关注情绪的概念，这是一个重要的信号，但不应该被视为 sufficient feature  для标签。 motivated by the definition of global sufficiency，我们首先查看分类器学习的不必要依赖关系，并评估它们在所有决策阈值下的准确率。此外，recognizing 挑战集可能不总是可用，我们引入基于概念的解释指标，以评估概念对标签的影响。这些解释allow us to compare 分类器，以评估它们学习的假的 global sufficiency 程度。
</details></li>
</ul>
<hr>
<h2 id="KDSTM-Neural-Semi-supervised-Topic-Modeling-with-Knowledge-Distillation"><a href="#KDSTM-Neural-Semi-supervised-Topic-Modeling-with-Knowledge-Distillation" class="headerlink" title="KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation"></a>KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01878">http://arxiv.org/abs/2307.01878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Xu, Xiaoyu Jiang, Jay Desai, Bin Han, Fuqin Yan, Francis Iannacci</li>
<li>for: 文本分类任务中，精细调整预训练语言模型如BERT和GPT-3可以获得竞争性的准确率，但这两种方法均需要大量文本数据预训练。</li>
<li>methods: 我们开发了知识填充 semi-supervised topic modeling（KDSTM）方法，不需要预训练 embedding，只需几个标注文档和训练效率高。</li>
<li>results: 在多个 dataset 上，我们的方法比现有的超vised topic modeling方法在分类准确率、稳定性和效率方面表现出色，并且与弱监督文本分类方法相当。<details>
<summary>Abstract</summary>
In text classification tasks, fine tuning pretrained language models like BERT and GPT-3 yields competitive accuracy; however, both methods require pretraining on large text datasets. In contrast, general topic modeling methods possess the advantage of analyzing documents to extract meaningful patterns of words without the need of pretraining. To leverage topic modeling's unsupervised insights extraction on text classification tasks, we develop the Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires no pretrained embeddings, few labeled documents and is efficient to train, making it ideal under resource constrained settings. Across a variety of datasets, our method outperforms existing supervised topic modeling methods in classification accuracy, robustness and efficiency and achieves similar performance compare to state of the art weakly supervised text classification methods.
</details>
<details>
<summary>摘要</summary>
在文本分类任务中，使用预训练的自然语言模型如BERT和GPT-3可以获得竞争力的准确率，但两者都需要预训练大量文本数据。然而，通用主题模型方法具有分析文档以提取有意义的词语模式的优势，无需预训练。为了利用主题模型方法在文本分类任务中的无监督抽象知识提取，我们开发了知识填充半supervised主题模型（KDSTM）。KDSTM不需预训练 embedding，只需几个标注文档，训练效率高，适用于资源受限的情况。在多个数据集上，我们的方法比现有的监督主题模型方法在分类精度、稳定性和效率方面表现出色，并与州度静的弱监督文本分类方法实现相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Non-Verbal-Predicates-in-Semantic-Role-Labeling-Challenges-and-Opportunities"><a href="#Exploring-Non-Verbal-Predicates-in-Semantic-Role-Labeling-Challenges-and-Opportunities" class="headerlink" title="Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and Opportunities"></a>Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01870">http://arxiv.org/abs/2307.01870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Riccardo Orlando, Simone Conia, Roberto Navigli</li>
<li>for: 本文主要研究 semantic role labeling（SRL）领域中的非verb predicate，即名词和形容词作为 predicate 的情况。</li>
<li>methods: 作者提出了一个新的 PropBank  dataset，以推动 SRL 领域的发展，并通过实验表明了现有的标准 benchmark 不能准确反映 SRL 的当前情况，以及现有系统无法在不同 predicate 类型之间传输知识。</li>
<li>results: 作者发现了现有 benchmark 不能准确反映 SRL 的当前情况，以及现有系统无法在不同 predicate 类型之间传输知识。此外，作者还提出了一个新的 challenge set，以探讨不同语言资源是否可以促进知识传输。<details>
<summary>Abstract</summary>
Although we have witnessed impressive progress in Semantic Role Labeling (SRL), most of the research in the area is carried out assuming that the majority of predicates are verbs. Conversely, predicates can also be expressed using other parts of speech, e.g., nouns and adjectives. However, non-verbal predicates appear in the benchmarks we commonly use to measure progress in SRL less frequently than in some real-world settings -- newspaper headlines, dialogues, and tweets, among others. In this paper, we put forward a new PropBank dataset which boasts wide coverage of multiple predicate types. Thanks to it, we demonstrate empirically that standard benchmarks do not provide an accurate picture of the current situation in SRL and that state-of-the-art systems are still incapable of transferring knowledge across different predicate types. Having observed these issues, we also present a novel, manually-annotated challenge set designed to give equal importance to verbal, nominal, and adjectival predicate-argument structures. We use such dataset to investigate whether we can leverage different linguistic resources to promote knowledge transfer. In conclusion, we claim that SRL is far from "solved", and its integration with other semantic tasks might enable significant improvements in the future, especially for the long tail of non-verbal predicates, thereby facilitating further research on SRL for non-verbal predicates.
</details>
<details>
<summary>摘要</summary>
尽管我们已经目睹了Semantic Role Labeling（SRL）的印象人的进步，但大多数研究在这个领域都是基于假设大多数 predicate 是动词的。然而， predicate 可以通过其他部件表达，如名称和形容词。然而，非语言 predicate 在我们通用的测试数据集中出现得更少， чем在一些实际场景中，如新闻标题、对话和微博。在这篇论文中，我们提出了一个新的 PropBank 数据集，它具有广泛的多 predicate 类型覆盖率。因此，我们可以证明现有的标准测试数据集不能准确反映当前情况，并且现有的状态顶尖系统无法在不同 predicate 类型之间传输知识。经过观察这些问题，我们还提出了一个新的、手动标注的挑战集，用于让 verbal、名称和形容词 predicate-argument 结构具有同样的重要性。我们使用这个数据集来调查是否可以通过不同的语言资源来促进知识传输。在结论中，我们宣称 SRL 还没有"解决"，并且将它与其他 semantics 任务结合可能会导致未来的重要进步，特别是对非语言 predicate 的长尾，从而促进 SRL 的进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Self-Consuming-Generative-Models-Go-MAD"><a href="#Self-Consuming-Generative-Models-Go-MAD" class="headerlink" title="Self-Consuming Generative Models Go MAD"></a>Self-Consuming Generative Models Go MAD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01850">http://arxiv.org/abs/2307.01850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, Richard G. Baraniuk</li>
<li>for: This paper aims to investigate the properties of autophagous loops in generative AI algorithms and their impact on the quality and diversity of future generative models.</li>
<li>methods: The authors use state-of-the-art generative image models of three families to conduct an analytical and empirical analysis of autophagous loops with varying levels of fresh real data availability and bias.</li>
<li>results: The primary conclusion is that without enough fresh real data in each generation of an autophagous loop, future generative models are likely to experience a decline in quality (precision) or diversity (recall), which the authors term “Model Autophagy Disorder” (MAD).<details>
<summary>Abstract</summary>
Seismic advances in generative AI algorithms for imagery, text, and other data types has led to the temptation to use synthetic data to train next-generation models. Repeating this process creates an autophagous (self-consuming) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and in whether the samples from previous generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), making analogy to mad cow disease.
</details>
<details>
<summary>摘要</summary>
自适应式AI算法的发展在图像、文本和其他数据类型上，导致使用生成数据训练下一代模型的吸引力。重复这个过程会形成一个自食い（自危）循环，其性质未得好解。我们通过使用当今最佳的生成图像模型进行全面的分析和实验研究，对三种不同的自食い循环进行了分析，它们是根据训练数据的可用性和前一代模型的样本偏好是否受到影响。我们的主要结论是，如果每代训练数据不够新鲜，那么未来的生成模型就会无法维持其精度（精度）或多样性（重复率）。我们将这种情况称为“模型自食い病”（MAD），与疯牛病相似。
</details></li>
</ul>
<hr>
<h2 id="Embodied-Task-Planning-with-Large-Language-Models"><a href="#Embodied-Task-Planning-with-Large-Language-Models" class="headerlink" title="Embodied Task Planning with Large Language Models"></a>Embodied Task Planning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01848">http://arxiv.org/abs/2307.01848</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Gary3410/TaPA">https://github.com/Gary3410/TaPA</a></li>
<li>paper_authors: Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan</li>
<li>for: 本研究旨在将智能代理人赋予常识，以便在实际环境中完成复杂的人类指令。</li>
<li>methods: 本研究使用了大型自然语言模型（LLM），并与物理场景约束相结合，实现实际世界中的行动规划。</li>
<li>results: 实验结果显示，本研究所提出的Task Planing Agent（TaPA）框架可以在实际环境中高度成功完成复杂的任务，并且比LLaVA和GPT-3.5更高的成功率。<details>
<summary>Abstract</summary>
Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.
</details>
<details>
<summary>摘要</summary>
equiping 智能 agents with common sense 是为 robot 成功完成复杂的人类指令的关键，这些指令可能存在一般环境中。 latest large language models (LLM) 可以嵌入智能 agent 的具有 semantic knowledge ，但它们缺乏实际世界的信息，通常会生成不可行的动作序列。在这篇论文中，我们提出了一种 Task Planing Agent (TaPA) 在embodied task 中进行基于物理场景的固有计划，agent 会根据场景中存在的物品生成可执行的计划。specifically，我们首先构建了一个多Modal dataset，包括室内场景、指令和动作计划的 triplets，我们为 GPT-3.5 提供了设计的提示和场景中存在的物品列表，以便它在 generate 大量的指令和相应的计划动作。生成的数据被用于地面的 LLVM 训练。在推理时，我们使用多视图RGB图像来扩展开放词汇物体检测器，以确定场景中的物品。实验结果表明，我们的 TaPA 框架生成的计划可以在 LLLaVA 和 GPT-3.5 的基础上取得更高的成功率，这表明了embodied task 的偏置计划在一般和复杂的环境中的实用性。
</details></li>
</ul>
<hr>
<h2 id="DiT-3D-Exploring-Plain-Diffusion-Transformers-for-3D-Shape-Generation"><a href="#DiT-3D-Exploring-Plain-Diffusion-Transformers-for-3D-Shape-Generation" class="headerlink" title="DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation"></a>DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01831">http://arxiv.org/abs/2307.01831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DiT-3D/DiT-3D">https://github.com/DiT-3D/DiT-3D</a></li>
<li>paper_authors: Shentong Mo, Enze Xie, Ruihang Chu, Lewei Yao, Lanqing Hong, Matthias Nießner, Zhenguo Li</li>
<li>for: 本研究旨在探讨Diffusion Transformer（DiT）是否能够有效地生成3D形状，以前的3D扩散方法主要采用U-Net架构。</li>
<li>methods: 我们提出了一种新的Diffusion Transformer для3D形状生成，即DiT-3D，可以直接进行点云的净化过程使用普通的Transformers。DiT-3D采用了Diffusion Transformer的设计哲学，并将3D位置嵌入和补丁嵌入进行适应性地归一化输入点云。为了降低3D形状生成中自我注意的计算成本，我们在Transformer块中添加了3D窗口注意。最后，我们使用线性和净化层预测净化后的点云。</li>
<li>results: 我们的transformer架构可以高效地进行2D到3D的 fine-tuning，使得预测DiT-3D的点云预测器可以在ShapeNet上达到状态之最的性能。实验结果表明，我们提出的DiT-3D可以在高精度和多样化的3D点云生成中占据领先地位，特别是在Chamfer Distance上减少了状态之最方法的1-最近邻接精度值4.59，并将覆盖率提高3.51。<details>
<summary>Abstract</summary>
Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful effectiveness in generating high-quality 2D images. However, it is still being determined whether the Transformer architecture performs equally well in 3D shape generation, as previous 3D diffusion methods mostly adopted the U-Net architecture. To bridge this gap, we propose a novel Diffusion Transformer for 3D shape generation, namely DiT-3D, which can directly operate the denoising process on voxelized point clouds using plain Transformers. Compared to existing U-Net approaches, our DiT-3D is more scalable in model size and produces much higher quality generations. Specifically, the DiT-3D adopts the design philosophy of DiT but modifies it by incorporating 3D positional and patch embeddings to adaptively aggregate input from voxelized point clouds. To reduce the computational cost of self-attention in 3D shape generation, we incorporate 3D window attention into Transformer blocks, as the increased 3D token length resulting from the additional dimension of voxels can lead to high computation. Finally, linear and devoxelization layers are used to predict the denoised point clouds. In addition, our transformer architecture supports efficient fine-tuning from 2D to 3D, where the pre-trained DiT-2D checkpoint on ImageNet can significantly improve DiT-3D on ShapeNet. Experimental results on the ShapeNet dataset demonstrate that the proposed DiT-3D achieves state-of-the-art performance in high-fidelity and diverse 3D point cloud generation. In particular, our DiT-3D decreases the 1-Nearest Neighbor Accuracy of the state-of-the-art method by 4.59 and increases the Coverage metric by 3.51 when evaluated on Chamfer Distance.
</details>
<details>
<summary>摘要</summary>
现代扩散变换器（例如DiT）已经证明了它们在生成高质量2D图像方面的强大效果。然而，是否使用Transformer架构在3D形状生成方面表现良好，目前仍在决定中。为了bridging这个差距，我们提出了一种新的扩散变换器 для3D形状生成，即DiT-3D，它可以直接将噪声级别提取到点云中的简单变换器中进行操作。与现有的U-Net方法相比，我们的DiT-3D更加可扩展，生成质量更高。具体来说，DiT-3D采用了Diffusion Transformer的设计哲学，并在点云中添加3D位置嵌入和小块嵌入，以适应输入的自适应归一化。为了降低3D形状生成中自我注意力的计算成本，我们在Transformer块中添加了3D窗口注意力。最后，我们使用线性层和去voxel化层预测噪声级别提取结果。此外，我们的transformer架构支持高效的精度调整，从2D到3D的精度调整，可以使用预训练的DiT-2D Checkpoint在ImageNet上进行高效地调整DiT-3D在ShapeNet上的性能。实验结果表明，我们提出的DiT-3D可以在ShapeNet dataset上实现状态的艺术性和多样性3D点云生成的最佳性能。具体来说，我们的DiT-3D在1-Nearest Neighbor精度上降低了状态艺术性方法的精度值4.59，并在ChamferDistance上提高了覆盖率3.51。
</details></li>
</ul>
<hr>
<h2 id="Strictly-Low-Rank-Constraint-Optimization-–-An-Asymptotically-mathcal-O-frac-1-t-2-Method"><a href="#Strictly-Low-Rank-Constraint-Optimization-–-An-Asymptotically-mathcal-O-frac-1-t-2-Method" class="headerlink" title="Strictly Low Rank Constraint Optimization – An Asymptotically $\mathcal{O}(\frac{1}{t^2})$ Method"></a>Strictly Low Rank Constraint Optimization – An Asymptotically $\mathcal{O}(\frac{1}{t^2})$ Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14344">http://arxiv.org/abs/2307.14344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengyuan Zhang, Kai Liu</li>
<li>for: 提高鲁棒性和稀疏性</li>
<li>methods: 使用 proximal gradient descent 方法和支持集投影操作</li>
<li>results: 实现 $O(\frac{1}{t^2})$ 的收敛率，且支持集在每次更新中 monotonically shrinking，这是 momentum-based 算法中的一个新特点。<details>
<summary>Abstract</summary>
We study a class of non-convex and non-smooth problems with \textit{rank} regularization to promote sparsity in optimal solution. We propose to apply the proximal gradient descent method to solve the problem and accelerate the process with a novel support set projection operation on the singular values of the intermediate update. We show that our algorithms are able to achieve a convergence rate of $O(\frac{1}{t^2})$, which is exactly same as Nesterov's optimal convergence rate for first-order methods on smooth and convex problems. Strict sparsity can be expected and the support set of singular values during each update is monotonically shrinking, which to our best knowledge, is novel in momentum-based algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究一类非 convex 和非光滑的问题，使用 \textit{rank}  regularization 来提高优解的稀疏性。我们提议使用 proximal 梯度下降法来解决问题，并使用一种新的支持集投影操作来加速过程。我们表明我们的算法可以实现 $O(\frac{1}{t^2})$ 的收敛率，与内斯特洛夫的优化收敛率相同，这是在光滑和 convex 问题上的首个案例。在每次更新中，支持集的单值是 monotonically shrinking，这在旋转-based 算法中是新的。Here's the word-for-word translation of the text into Simplified Chinese:我们研究一类非 convex 和非光滑的问题，使用 \textit{rank} regularization 来提高优解的稀疏性。我们提议使用 proximal 梯度下降法来解决问题，并使用一种新的支持集投影操作来加速过程。我们表明我们的算法可以实现 $O(\frac{1}{t^2})$ 的收敛率，与内斯特洛夫的优化收敛率相同，这是在光滑和 convex 问题上的首个案例。在每次更新中，支持集的单值是 monotonically shrinking，这在旋转-based 算法中是新的。
</details></li>
</ul>
<hr>
<h2 id="Human-Trajectory-Forecasting-with-Explainable-Behavioral-Uncertainty"><a href="#Human-Trajectory-Forecasting-with-Explainable-Behavioral-Uncertainty" class="headerlink" title="Human Trajectory Forecasting with Explainable Behavioral Uncertainty"></a>Human Trajectory Forecasting with Explainable Behavioral Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01817">http://arxiv.org/abs/2307.01817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangbei Yue, Dinesh Manocha, He Wang</li>
<li>for: 这个论文的目的是提出一种新的人员运动预测方法，用于解释和预测人类行为，以便应用于社交机器人和自动驾驶车等领域。</li>
<li>methods: 这个方法基于 bayesian neural networks (BNNs) 和 Stochastic Differential Equation (SDE) 模型，具有strong explainability和高度预测精度。</li>
<li>results: 对于11种现有方法，这个方法可以达到50%的提升预测精度，并且在不同的环境和人群密度下能够更好地泛化。此外，这个方法还可以提供有信心的预测结果，以更好地解释人类行为的原因。<details>
<summary>Abstract</summary>
Human trajectory forecasting helps to understand and predict human behaviors, enabling applications from social robots to self-driving cars, and therefore has been heavily investigated. Most existing methods can be divided into model-free and model-based methods. Model-free methods offer superior prediction accuracy but lack explainability, while model-based methods provide explainability but cannot predict well. Combining both methodologies, we propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM, where a behavior SDE model is combined with Bayesian neural networks (BNNs). While the NNs provide superior predictive power, the SDE offers strong explainability with quantifiable uncertainty in behavior and observation. We show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy, compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to drastically different scenes with different environments and crowd densities (~ 20 times higher than the testing data). Finally, BNSP-SFM can provide predictions with confidence to better explain potential causes of behaviors. The code will be released upon acceptance.
</details>
<details>
<summary>摘要</summary>
人类轨迹预测可以理解和预测人类行为，因此得到了广泛的研究。现有的方法主要可以分为无模型和具有模型的方法。无模型方法具有更高的预测精度，但缺乏可解性，而具有模型的方法提供了可解性，但预测不准确。我们提出了一种新的 Bayesian Neural Stochastic Differential Equation 模型（BNSP-SFM），其中将行为 SDE 模型与 Bayesian neural networks（BNNs）结合在一起。而 BNNs 提供了更高的预测力，SDE 则提供了强大的解释性，同时可以量化行为和观察的uncertainty。我们表明，BNSP-SFM 可以与 11 种现有方法进行比较，并达到50%的提升预测精度。此外，BNSP-SFM 还可以更好地适应不同的环境和人群密度（大约 20 倍于测试数据）。最后，BNSP-SFM 可以提供预测结果的信任度，以更好地解释行为的可能性。代码将在接受后发布。
</details></li>
</ul>
<hr>
<h2 id="DeepFlorist-Rethinking-Deep-Neural-Networks-and-Ensemble-Learning-as-A-Meta-Classifier-For-Object-Classification"><a href="#DeepFlorist-Rethinking-Deep-Neural-Networks-and-Ensemble-Learning-as-A-Meta-Classifier-For-Object-Classification" class="headerlink" title="DeepFlorist: Rethinking Deep Neural Networks and Ensemble Learning as A Meta-Classifier For Object Classification"></a>DeepFlorist: Rethinking Deep Neural Networks and Ensemble Learning as A Meta-Classifier For Object Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01806">http://arxiv.org/abs/2307.01806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afshin Khadangi</li>
<li>for: 本文提出了一种新的学习模式，即“深度 florist”，用于使用集成学习为 flower 类别的高精度和可靠性。</li>
<li>methods: 该提案的网络架构结合了深度学习和集成方法，通过 dense convolutional neural networks (DCNNs) 和 convolutional neural networks (CNNs) 提取高级特征，并将其分类使用全连接层。</li>
<li>results: 实验结果表明，深度 florist 在标准花卉数据集上表现出色，与当前最佳方法进行比较，具有较高的准确率和可靠性。<details>
<summary>Abstract</summary>
In this paper, we propose a novel learning paradigm called "DeepFlorist" for flower classification using ensemble learning as a meta-classifier. DeepFlorist combines the power of deep learning with the robustness of ensemble methods to achieve accurate and reliable flower classification results. The proposed network architecture leverages a combination of dense convolutional and convolutional neural networks (DCNNs and CNNs) to extract high-level features from flower images, followed by a fully connected layer for classification. To enhance the performance and generalization of DeepFlorist, an ensemble learning approach is employed, incorporating multiple diverse models to improve the classification accuracy. Experimental results on benchmark flower datasets demonstrate the effectiveness of DeepFlorist, outperforming state-of-the-art methods in terms of accuracy and robustness. The proposed framework holds significant potential for automated flower recognition systems in real-world applications, enabling advancements in plant taxonomy, conservation efforts, and ecological studies.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的学习方法，即“深度 florist”，用于使用集成学习作为元类фикатор进行花类分类。深度 florist 结合了深度学习的力量和集成方法的稳定性，以实现高度准确和可靠的花类分类结果。提议的网络架构利用了 dense convolutional neural networks (DCNNs) 和 convolutional neural networks (CNNs) 来提取花图像中的高级特征，然后进行分类。为了提高深度 florist 的性能和泛化能力，我们采用了集成学习方法，并将多种多样的模型集成到一起，以提高分类精度。实验结果表明，深度 florist 在标准花图像集上表现出色，与当前最佳方法相比，具有更高的准确率和更好的泛化能力。该提案的框架在实际应用中的自动花识别系统中具有重要的潜在价值，可以推动植物分类、保护和生态学研究等领域的进步。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/05/cs.AI_2023_07_05/" data-id="clpahu6x500053h887t3u9375" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/cs.CL_2023_07_05/" class="article-date">
  <time datetime="2023-07-05T11:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/05/cs.CL_2023_07_05/">cs.CL - 2023-07-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="To-be-or-not-to-be-a-translation-reception-study-of-a-literary-text-translated-into-Dutch-and-Catalan-using-machine-translation"><a href="#To-be-or-not-to-be-a-translation-reception-study-of-a-literary-text-translated-into-Dutch-and-Catalan-using-machine-translation" class="headerlink" title="To be or not to be: a translation reception study of a literary text translated into Dutch and Catalan using machine translation"></a>To be or not to be: a translation reception study of a literary text translated into Dutch and Catalan using machine translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02358">http://arxiv.org/abs/2307.02358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana Guerberof Arenas, Antonio Toral</li>
<li>for: 这项研究探讨了一篇小说的翻译receiving情况，包括机器翻译（MT）、后期编辑（PE）和从头 rewrite（HT）三种条件下的读者反应。</li>
<li>methods: 研究使用223名参与者，他们对不同翻译条件进行评分，包括narraative Engagement、Enjoyment和翻译接受度三个维度。</li>
<li>results: 结果显示，在catalan语言下，HT条件对narraative Engagement、Enjoyment和翻译接受度得分较高，而在荷兰语言下，PE条件对Enjoyment和翻译接受度得分较高，但对原始英语版本的评分最高。研究结果表明，在读一篇小说翻译版本时，不仅翻译条件和质量对其接受度起到关键作用，而且参与者的读写习惯、读写语言和社会语言地位也具有重要作用。<details>
<summary>Abstract</summary>
This article presents the results of a study involving the reception of a fictional story by Kurt Vonnegut translated from English into Catalan and Dutch in three conditions: machine-translated (MT), post-edited (PE) and translated from scratch (HT). 223 participants were recruited who rated the reading conditions using three scales: Narrative Engagement, Enjoyment and Translation Reception. The results show that HT presented a higher engagement, enjoyment and translation reception in Catalan if compared to PE and MT. However, the Dutch readers show higher scores in PE than in both HT and MT, and the highest engagement and enjoyments scores are reported when reading the original English version. We hypothesize that when reading a fictional story in translation, not only the condition and the quality of the translations is key to understand its reception, but also the participants reading patterns, reading language, and, perhaps language status in their own societies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MuLMS-AZ-An-Argumentative-Zoning-Dataset-for-the-Materials-Science-Domain"><a href="#MuLMS-AZ-An-Argumentative-Zoning-Dataset-for-the-Materials-Science-Domain" class="headerlink" title="MuLMS-AZ: An Argumentative Zoning Dataset for the Materials Science Domain"></a>MuLMS-AZ: An Argumentative Zoning Dataset for the Materials Science Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02340">http://arxiv.org/abs/2307.02340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timo Pierre Schrader, Teresa Bürkle, Sophie Henning, Sherry Tan, Matteo Finco, Stefan Grünewald, Maira Indrikova, Felix Hildebrand, Annemarie Friedrich</li>
<li>for: 这篇论文是为了提高材料科学研究文献的处理而写的。</li>
<li>methods: 这篇论文使用了域标注方法，并采用了特殊的材料科学注解方案来标注文献。</li>
<li>results: 研究发现，使用域特定的 пре打 trains 可以获得高精度的分类结果，而且已经存在的其他领域的 AZ 类别也可以在一定程度上转移到材料科学领域中。<details>
<summary>Abstract</summary>
Scientific publications follow conventionalized rhetorical structures. Classifying the Argumentative Zone (AZ), e.g., identifying whether a sentence states a Motivation, a Result or Background information, has been proposed to improve processing of scholarly documents. In this work, we adapt and extend this idea to the domain of materials science research. We present and release a new dataset of 50 manually annotated research articles. The dataset spans seven sub-topics and is annotated with a materials-science focused multi-label annotation scheme for AZ. We detail corpus statistics and demonstrate high inter-annotator agreement. Our computational experiments show that using domain-specific pre-trained transformer-based text encoders is key to high classification performance. We also find that AZ categories from existing datasets in other domains are transferable to varying degrees.
</details>
<details>
<summary>摘要</summary>
Note: The Simplified Chinese translation is written in the standardized format used in China, which is different from the Traditional Chinese used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Utilizing-ChatGPT-Generated-Data-to-Retrieve-Depression-Symptoms-from-Social-Media"><a href="#Utilizing-ChatGPT-Generated-Data-to-Retrieve-Depression-Symptoms-from-Social-Media" class="headerlink" title="Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from Social Media"></a>Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02313">http://arxiv.org/abs/2307.02313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana-Maria Bucur</li>
<li>for: 本研究团队在eRisk Lab任务中做出了贡献，该任务是从Reddit社交媒体上检索和排序表达抑郁症状的句子。</li>
<li>methods: 我们使用ChatGPT生成了Synthetic Data，并设计了一个提示，以便生成的数据具有更高的 semantic diversity 和情感体验，同时还具有特定于Reddit的个人分享体验。我们使用cosine similarity进行 semantic search 和排序句子的相关性。</li>
<li>results: 我们的结果表明，使用专门为semantic search设计的模型的句子嵌入在这个任务中 perfoms 更好，而使用预训练在心理健康数据上的模型的嵌入则不及。此外，我们发现生成的Synthetic Data 是这个任务中特别适用的，使用原始BDI-II句子alone 的方法得到了最佳性能。<details>
<summary>Abstract</summary>
In this work, we present the contribution of the BLUE team in the eRisk Lab task on searching for symptoms of depression. The task consists of retrieving and ranking Reddit social media sentences that convey symptoms of depression from the BDI-II questionnaire. Given that synthetic data provided by LLMs have been proven to be a reliable method for augmenting data and fine-tuning downstream models, we chose to generate synthetic data using ChatGPT for each of the symptoms of the BDI-II questionnaire. We designed a prompt such that the generated data contains more richness and semantic diversity than the BDI-II responses for each question and, at the same time, contains emotional and anecdotal experiences that are specific to the more intimate way of sharing experiences on Reddit. We perform semantic search and rank the sentences' relevance to the BDI-II symptoms by cosine similarity. We used two state-of-the-art transformer-based models (MentalRoBERTa and a variant of MPNet) for embedding the social media posts, the original and generated responses of the BDI-II. Our results show that using sentence embeddings from a model designed for semantic search outperforms the approach using embeddings from a model pre-trained on mental health data. Furthermore, the generated synthetic data were proved too specific for this task, the approach simply relying on the BDI-II responses had the best performance.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了BLUE团队在eRisk Lab任务中搜索抑郁症状的贡献。这个任务的目标是从Reddit社交媒体上检索和排名符合BDI-II问卷中的抑郁症状表达。由于使用生成的数据已被证明可以增强模型和下游模型的精度，我们使用ChatGPT生成每个BDI-II问卷中的症状数据。我们设计了一个提示，以便生成的数据具有更多的 ricness和semantic diversity，同时具有特定于Reddit上更加亲切的情感和个人经历。我们使用cosine similarity对句子的相似性进行Semantic search和排名。我们使用两种现代变换器模型（MentalRoBERTa和MPNet的变种）来嵌入社交媒体文章和BDI-II的原始和生成回答。我们的结果表明，使用专门 дляsemantic search的句子嵌入模型比使用预先训练在精神健康数据上的模型来得到更高的性能。此外，我们发现生成的 sintetic data太特定于这个任务，使用直接使用BDI-II回答的方法获得最好的性能。
</details></li>
</ul>
<hr>
<h2 id="Sumformer-Universal-Approximation-for-Efficient-Transformers"><a href="#Sumformer-Universal-Approximation-for-Efficient-Transformers" class="headerlink" title="Sumformer: Universal Approximation for Efficient Transformers"></a>Sumformer: Universal Approximation for Efficient Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02301">http://arxiv.org/abs/2307.02301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silas Alberti, Niclas Dern, Laura Thesing, Gitta Kutyniok</li>
<li>for: 这 paper 是为了研究sequence-to-sequence函数的 universally approximation问题而写的。</li>
<li>methods: 这 paper 使用了一种新的 Sumformer 架构，以及对 Linformer 和 Performer 的分析。</li>
<li>results: 这 paper 提供了 universally approximation results for Linformer 和 Performer，并提供了一个新的证明，证明只需一层注意力层可以universally approximation sequence-to-sequence函数。<details>
<summary>Abstract</summary>
Natural language processing (NLP) made an impressive jump with the introduction of Transformers. ChatGPT is one of the most famous examples, changing the perception of the possibilities of AI even outside the research community. However, besides the impressive performance, the quadratic time and space complexity of Transformers with respect to sequence length pose significant limitations for handling long sequences. While efficient Transformer architectures like Linformer and Performer with linear complexity have emerged as promising solutions, their theoretical understanding remains limited. In this paper, we introduce Sumformer, a novel and simple architecture capable of universally approximating equivariant sequence-to-sequence functions. We use Sumformer to give the first universal approximation results for Linformer and Performer. Moreover, we derive a new proof for Transformers, showing that just one attention layer is sufficient for universal approximation.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）在Transformers的引入后作出了卓越的跳跃。ChatGPT是最著名的示例之一，对外部研究社区的可能性产生了深刻的影响。然而，Transformers的序列长度相对于时间和空间复杂性的平方带来了重大限制，对长序列处理有重要的限制。虽然高效的Transformers架构如Linformer和Performer已经出现了，但它们的理论理解仍然受限。在本文中，我们介绍Sumformer，一种新的简单架构，可以universally approximating equivariant sequence-to-sequence函数。我们使用Sumformer来给Linformer和Performer的首次universal approximation结果。此外，我们还 derivates一个新的证明，显示只需一层注意力层就能 universally approximation Transformers。
</details></li>
</ul>
<hr>
<h2 id="Performance-Comparison-of-Large-Language-Models-on-VNHSGE-English-Dataset-OpenAI-ChatGPT-Microsoft-Bing-Chat-and-Google-Bard"><a href="#Performance-Comparison-of-Large-Language-Models-on-VNHSGE-English-Dataset-OpenAI-ChatGPT-Microsoft-Bing-Chat-and-Google-Bard" class="headerlink" title="Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard"></a>Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02288">http://arxiv.org/abs/2307.02288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan-Quy Dao</li>
<li>for: 这个研究是为了比较三个大型自然语言处理模型（LLMs）在VNHSGE英语数据集上的性能。</li>
<li>methods: 这个研究使用了OpenAI ChatGPT、Microsoft Bing Chat（BingChat）和Google Bard三个模型进行比较。</li>
<li>results: 研究发现，BingChat的性能比ChatGPT和Bard高出92.4%和86%。这意味着BingChat可以取代ChatGPT，而ChatGPT尚未正式在越南上市。此外，BingChat、Bard和ChatGPT在英语水平都高于越南高中生。这些结果贡献到了LLMs在英语教学中的潜力的理解。<details>
<summary>Abstract</summary>
This paper presents a performance comparison of three large language models (LLMs), namely OpenAI ChatGPT, Microsoft Bing Chat (BingChat), and Google Bard, on the VNHSGE English dataset. The performance of BingChat, Bard, and ChatGPT (GPT-3.5) is 92.4\%, 86\%, and 79.2\%, respectively. The results show that BingChat is better than ChatGPT and Bard. Therefore, BingChat and Bard can replace ChatGPT while ChatGPT is not yet officially available in Vietnam. The results also indicate that BingChat, Bard and ChatGPT outperform Vietnamese students in English language proficiency. The findings of this study contribute to the understanding of the potential of LLMs in English language education. The remarkable performance of ChatGPT, BingChat, and Bard demonstrates their potential as effective tools for teaching and learning English at the high school level.
</details>
<details>
<summary>摘要</summary>
这个论文比较了三个大语言模型（LLMs）的性能，即OpenAI ChatGPT、Microsoft Bing Chat（BingChat）和Google Bard，在VNHSGE英语数据集上。这三个模型的性能分别为92.4%、86%和79.2%。结果表明，BingChat比ChatGPT和Bard更好，因此BingChat和Bard可以取代ChatGPT，而ChatGPT尚未正式在越南发布。结果还表明，BingChat、Bard和ChatGPT在英语水平超过越南学生。这些发现贡献于英语教学中LLMs的潜力理解。ChatGPT、BingChat和Bard的出色表现表明它们在高中英语教学中是有效的工具。
</details></li>
</ul>
<hr>
<h2 id="SpaceNLI-Evaluating-the-Consistency-of-Predicting-Inferences-in-Space"><a href="#SpaceNLI-Evaluating-the-Consistency-of-Predicting-Inferences-in-Space" class="headerlink" title="SpaceNLI: Evaluating the Consistency of Predicting Inferences in Space"></a>SpaceNLI: Evaluating the Consistency of Predicting Inferences in Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02269">http://arxiv.org/abs/2307.02269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kovvalsky/spacenli">https://github.com/kovvalsky/spacenli</a></li>
<li>paper_authors: Lasha Abzianidze, Joost Zwarts, Yoad Winter</li>
<li>for: fill the gap of spatial expression and reasoning in natural language inference (NLI) datasets</li>
<li>methods: semi-automatically created an NLI dataset for spatial reasoning called SpaceNLI, using curated reasoning patterns and expert annotations</li>
<li>results: SOTA NLI systems obtain moderate results on spatial NLI problems but lack consistency per inference pattern, with non-projective spatial inferences (especially those using the “between” preposition) being the most challenging ones.<details>
<summary>Abstract</summary>
While many natural language inference (NLI) datasets target certain semantic phenomena, e.g., negation, tense & aspect, monotonicity, and presupposition, to the best of our knowledge, there is no NLI dataset that involves diverse types of spatial expressions and reasoning. We fill this gap by semi-automatically creating an NLI dataset for spatial reasoning, called SpaceNLI. The data samples are automatically generated from a curated set of reasoning patterns, where the patterns are annotated with inference labels by experts. We test several SOTA NLI systems on SpaceNLI to gauge the complexity of the dataset and the system's capacity for spatial reasoning. Moreover, we introduce a Pattern Accuracy and argue that it is a more reliable and stricter measure than the accuracy for evaluating a system's performance on pattern-based generated data samples. Based on the evaluation results we find that the systems obtain moderate results on the spatial NLI problems but lack consistency per inference pattern. The results also reveal that non-projective spatial inferences (especially due to the "between" preposition) are the most challenging ones.
</details>
<details>
<summary>摘要</summary>
whilst many natural language inference (NLI) datasets target certain semantic phenomena, e.g., negation, tense & aspect, monotonicity, and presupposition, to the best of our knowledge, there is no NLI dataset that involves diverse types of spatial expressions and reasoning. We fill this gap by semi-automatically creating an NLI dataset for spatial reasoning, called SpaceNLI. The data samples are automatically generated from a curated set of reasoning patterns, where the patterns are annotated with inference labels by experts. We test several state-of-the-art NLI systems on SpaceNLI to gauge the complexity of the dataset and the system's capacity for spatial reasoning. Moreover, we introduce a Pattern Accuracy and argue that it is a more reliable and stricter measure than the accuracy for evaluating a system's performance on pattern-based generated data samples. Based on the evaluation results we find that the systems obtain moderate results on the spatial NLI problems but lack consistency per inference pattern. The results also reveal that non-projective spatial inferences (especially due to the "between" preposition) are the most challenging ones.Here's the translation in Traditional Chinese as well:而 whereas many natural language inference (NLI) datasets target certain semantic phenomena, e.g., negation, tense & aspect, monotonicity, and presupposition, to the best of our knowledge, there is no NLI dataset that involves diverse types of spatial expressions and reasoning. We fill this gap by semi-automatically creating an NLI dataset for spatial reasoning, called SpaceNLI. The data samples are automatically generated from a curated set of reasoning patterns, where the patterns are annotated with inference labels by experts. We test several state-of-the-art NLI systems on SpaceNLI to gauge the complexity of the dataset and the system's capacity for spatial reasoning. Moreover, we introduce a Pattern Accuracy and argue that it is a more reliable and stricter measure than the accuracy for evaluating a system's performance on pattern-based generated data samples. Based on the evaluation results we find that the systems obtain moderate results on the spatial NLI problems but lack consistency per inference pattern. The results also reveal that non-projective spatial inferences (especially due to the "between" preposition) are the most challenging ones.
</details></li>
</ul>
<hr>
<h2 id="Open-Source-Large-Language-Models-Outperform-Crowd-Workers-and-Approach-ChatGPT-in-Text-Annotation-Tasks"><a href="#Open-Source-Large-Language-Models-Outperform-Crowd-Workers-and-Approach-ChatGPT-in-Text-Annotation-Tasks" class="headerlink" title="Open-Source Large Language Models Outperform Crowd Workers and Approach ChatGPT in Text-Annotation Tasks"></a>Open-Source Large Language Models Outperform Crowd Workers and Approach ChatGPT in Text-Annotation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02179">http://arxiv.org/abs/2307.02179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meysam Alizadeh, Maël Kubli, Zeynab Samei, Shirin Dehghani, Juan Diego Bermeo, Maria Korobeynikova, Fabrizio Gilardi</li>
<li>for: This study aims to evaluate the performance of open-source Large Language Models (LLMs) in text annotation tasks and compare them with proprietary models like ChatGPT and human-based services such as MTurk.</li>
<li>methods: The study uses both zero-shot and few-shot approaches and different temperature parameters across a range of text annotation tasks to assess the performance of open-source LLMs.</li>
<li>results: The findings show that while ChatGPT achieves the best performance in most tasks, open-source LLMs not only outperform MTurk but also demonstrate competitive potential against ChatGPT in specific tasks.Here is the same information in Simplified Chinese text:</li>
<li>for: 这个研究旨在评估开源大语言模型（LLMs）在文本标注任务中的表现，并与专有模型如ChatGPT和人工服务如MTurk进行比较。</li>
<li>methods: 研究使用零shot和几shot方法，以及不同温度参数来评估开源LLMs的表现。</li>
<li>results: 结果显示，虽然ChatGPT在大多数任务中表现最佳，但开源LLMs不仅超越MTurk，还在特定任务中与ChatGPT竞争。<details>
<summary>Abstract</summary>
This study examines the performance of open-source Large Language Models (LLMs) in text annotation tasks and compares it with proprietary models like ChatGPT and human-based services such as MTurk. While prior research demonstrated the high performance of ChatGPT across numerous NLP tasks, open-source LLMs like HugginChat and FLAN are gaining attention for their cost-effectiveness, transparency, reproducibility, and superior data protection. We assess these models using both zero-shot and few-shot approaches and different temperature parameters across a range of text annotation tasks. Our findings show that while ChatGPT achieves the best performance in most tasks, open-source LLMs not only outperform MTurk but also demonstrate competitive potential against ChatGPT in specific tasks.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这个研究研究了开源大型自然语言模型（LLM）在文本标注任务中的性能，并与专有模型如ChatGPT和人类基于服务如MTurk进行比较。先前的研究表明了ChatGPT在多种NLP任务中的高性能，但开源LLM如HugginChat和FLAN在成本效益、透明度、复制性和数据保护方面吸引了关注。我们使用零shot和几shot方法以及不同温度参数，对多种文本标注任务进行评估。我们的发现表明，虽然ChatGPT在大多数任务中表现最好，但开源LLM不仅超越MTurk，还在特定任务中与ChatGPT竞争。
</details></li>
</ul>
<hr>
<h2 id="Generative-Job-Recommendations-with-Large-Language-Model"><a href="#Generative-Job-Recommendations-with-Large-Language-Model" class="headerlink" title="Generative Job Recommendations with Large Language Model"></a>Generative Job Recommendations with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02157">http://arxiv.org/abs/2307.02157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi Zheng, Zhaopeng Qiu, Xiao Hu, Likang Wu, Hengshu Zhu, Hui Xiong</li>
<li>For: 提供个性化和全面的就业搜索体验，通过语言模型生成具体的职位描述以满足企业和潜在雇员的需求。* Methods: 使用Supervised Fine-Tuning (SFT) STRATEGY和Proximal Policy Optimization (PPO)-based Reinforcement Learning (RL) 方法来训练语言模型生成职位描述，并使用CV和JD匹配度作为奖励模型。* Results: EXTENSIVE EXPERIMENTS ON A LARGE-SCALE REAL-WORLD DATASET 表明，我们的方法可以提供更高的准确率和更好的个性化效果，并且可以补充现有的作业推荐模型，提高搜索效果。<details>
<summary>Abstract</summary>
The rapid development of online recruitment services has encouraged the utilization of recommender systems to streamline the job seeking process. Predominantly, current job recommendations deploy either collaborative filtering or person-job matching strategies. However, these models tend to operate as "black-box" systems and lack the capacity to offer explainable guidance to job seekers. Moreover, conventional matching-based recommendation methods are limited to retrieving and ranking existing jobs in the database, restricting their potential as comprehensive career AI advisors. To this end, here we present GIRL (GeneratIve job Recommendation based on Large language models), a novel approach inspired by recent advancements in the field of Large Language Models (LLMs). We initially employ a Supervised Fine-Tuning (SFT) strategy to instruct the LLM-based generator in crafting suitable Job Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker. Moreover, we propose to train a model which can evaluate the matching degree between CVs and JDs as a reward model, and we use Proximal Policy Optimization (PPO)-based Reinforcement Learning (RL) method to further fine-tine the generator. This aligns the generator with recruiter feedback, tailoring the output to better meet employer preferences. In particular, GIRL serves as a job seeker-centric generative model, providing job suggestions without the need of a candidate set. This capability also enhances the performance of existing job recommendation models by supplementing job seeking features with generated content. With extensive experiments on a large-scale real-world dataset, we demonstrate the substantial effectiveness of our approach. We believe that GIRL introduces a paradigm-shifting approach to job recommendation systems, fostering a more personalized and comprehensive job-seeking experience.
</details>
<details>
<summary>摘要</summary>
“在线招聘服务的快速发展已经鼓励了对职业推荐服务的使用。现在大多数的职业推荐都使用了共同预测或人职匹配策略。但这些模型通常 acted as "黑盒子"系统，缺乏可靠的指导。传统的匹配基于推荐方法仅能从数据库中撷取和排名现有的职位，限制了它们的潜在。为了解决这个问题，我们现在提出了GIRL（生成式职业推荐，基于大型自然语言模型）。我们首先使用Supervised Fine-Tuning（SFT）策略，将LLM基于生成器 instrucured in crafting suitable Job Descriptions（JD）based on the Curriculum Vitae（CV）of a job seeker。此外，我们提议使用Proximal Policy Optimization（PPO）-based Reinforcement Learning（RL）方法，以更好地调整生成器。这样，生成器与招聘者反馈相互匹配，使生成器的输出更加符合雇主的需求。特别是，GIRL serves as a job seeker-centric generative model，提供了不需要候选人的职业建议。这个能力也提高了现有的职业推荐模型的表现，通过增加了对职业推荐的内容。经过了大规模的实验，我们证明了GIRL的实际效果。我们相信GIRL引入了一种新的职业推荐系统方法，创造了更加个性化和全面的职业搜寻体验。”
</details></li>
</ul>
<hr>
<h2 id="LOAF-M2L-Joint-Learning-of-Wording-and-Formatting-for-Singable-Melody-to-Lyric-Generation"><a href="#LOAF-M2L-Joint-Learning-of-Wording-and-Formatting-for-Singable-Melody-to-Lyric-Generation" class="headerlink" title="LOAF-M2L: Joint Learning of Wording and Formatting for Singable Melody-to-Lyric Generation"></a>LOAF-M2L: Joint Learning of Wording and Formatting for Singable Melody-to-Lyric Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02146">http://arxiv.org/abs/2307.02146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longshen Ou, Xichu Ma, Ye Wang</li>
<li>for:  bridges the singability gap between generated lyrics and melodies</li>
<li>methods:  jointly Learning wOrding And Formatting during Melody-to-Lyric training (LOAF-M2L)</li>
<li>results:  achieves 3.75% and 21.44% absolute accuracy gains in the outputs’ number-of-line and syllable-per-line requirements, and demonstrates 63.92% and 74.18% relative improvement of music-lyric compatibility and overall quality in the subjective evaluation.<details>
<summary>Abstract</summary>
Despite previous efforts in melody-to-lyric generation research, there is still a significant compatibility gap between generated lyrics and melodies, negatively impacting the singability of the outputs. This paper bridges the singability gap with a novel approach to generating singable lyrics by jointly Learning wOrding And Formatting during Melody-to-Lyric training (LOAF-M2L). After general-domain pretraining, our proposed model acquires length awareness first from a large text-only lyric corpus. Then, we introduce a new objective informed by musicological research on the relationship between melody and lyrics during melody-to-lyric training, which enables the model to learn the fine-grained format requirements of the melody. Our model achieves 3.75% and 21.44% absolute accuracy gains in the outputs' number-of-line and syllable-per-line requirements compared to naive fine-tuning, without sacrificing text fluency. Furthermore, our model demonstrates a 63.92% and 74.18% relative improvement of music-lyric compatibility and overall quality in the subjective evaluation, compared to the state-of-the-art melody-to-lyric generation model, highlighting the significance of formatting learning.
</details>
<details>
<summary>摘要</summary>
尽管之前的旋律到歌词生成研究已经做出了很多努力，但是还存在旋律到歌词的兼容性差距，这缺点影响了输出的唱ability。这篇论文通过一种新的approach来bridging这个兼容性差距，通过同时学习wording和 formatting来帮助模型学习合适的歌词。在通用领域预训练后，我们的提议的模型从大量的文本只 lyrics corpus中获得了长度意识。然后，我们引入了基于音乐学研究的melody和lyrics之间关系的新目标，使模型学习细腻的格式要求。我们的模型在Naive fine-tuning的基础上获得了3.75%和21.44%的绝对准确率提升，而无需牺牲文本流畅性。此外，我们的模型在主观评估中 Display 63.92%和74.18%的音乐-歌词兼容性和总质量提升，相比之前的State-of-the-art melody-to-lyric生成模型，表明了格式学习的重要性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Denoised-Abstract-Meaning-Representation-for-Grammatical-Error-Correction"><a href="#Leveraging-Denoised-Abstract-Meaning-Representation-for-Grammatical-Error-Correction" class="headerlink" title="Leveraging Denoised Abstract Meaning Representation for Grammatical Error Correction"></a>Leveraging Denoised Abstract Meaning Representation for Grammatical Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02127">http://arxiv.org/abs/2307.02127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hejing Cao, Dongyan Zhao</li>
<li>for: 这篇论文主要是为了提出一种基于AMR的语法错误修正模型，以提高语法错误修正 task 的性能。</li>
<li>methods: 作者提出了一种seq-to-seq模型，并使用了denoising方法来提高AMR的可靠性。</li>
<li>results: 实验结果表明，与一些强大的基准模型相比，AMR-GEC可以与其相比，而且可以降低训练时间32%。<details>
<summary>Abstract</summary>
Grammatical Error Correction (GEC) is the task of correcting errorful sentences into grammatically correct, semantically consistent, and coherent sentences. Popular GEC models either use large-scale synthetic corpora or use a large number of human-designed rules. The former is costly to train, while the latter requires quite a lot of human expertise. In recent years, AMR, a semantic representation framework, has been widely used by many natural language tasks due to its completeness and flexibility. A non-negligible concern is that AMRs of grammatically incorrect sentences may not be exactly reliable. In this paper, we propose the AMR-GEC, a seq-to-seq model that incorporates denoised AMR as additional knowledge. Specifically, We design a semantic aggregated GEC model and explore denoising methods to get AMRs more reliable. Experiments on the BEA-2019 shared task and the CoNLL-2014 shared task have shown that AMR-GEC performs comparably to a set of strong baselines with a large number of synthetic data. Compared with the T5 model with synthetic data, AMR-GEC can reduce the training time by 32\% while inference time is comparable. To the best of our knowledge, we are the first to incorporate AMR for grammatical error correction.
</details>
<details>
<summary>摘要</summary>
grammatical error correction (GEC) 是指 corrections 错误的句子到 grammatically 正确、semantically 一致、coherent 的句子。受欢迎的 GEC 模型 either 使用大规模的 synthetic corpora 或者使用大量的人工设计的规则。前者 costly 训练，后者需要很多的人类专家知识。在 recent 年，AMR，一种 semantic representation framework，has 广泛应用于 many natural language tasks  due to its completeness and flexibility。一个可耻的问题是 that AMRs of grammatically incorrect sentences may not be exactly reliable。在 this paper，we propose the AMR-GEC, a seq-to-seq model that incorporates denoised AMR as additional knowledge. Specifically, we design a semantic aggregated GEC model and explore denoising methods to get AMRs more reliable。 experiments on the BEA-2019 shared task and the CoNLL-2014 shared task have shown that AMR-GEC performs comparably to a set of strong baselines with a large number of synthetic data。 compared with the T5 model with synthetic data, AMR-GEC can reduce the training time by 32% while inference time is comparable。to the best of our knowledge, we are the first to incorporate AMR for grammatical error correction.
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Controllable-Transformer-Based-Lexical-Simplification"><a href="#Multilingual-Controllable-Transformer-Based-Lexical-Simplification" class="headerlink" title="Multilingual Controllable Transformer-Based Lexical Simplification"></a>Multilingual Controllable Transformer-Based Lexical Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02120">http://arxiv.org/abs/2307.02120</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kimchengsheang/mtls">https://github.com/kimchengsheang/mtls</a></li>
<li>paper_authors: Kim Cheng Sheang, Horacio Saggion</li>
<li>for: 这篇论文目标是提高文本读取和理解的Accessibility，使用控制token和预先训练的掩码语言模型来学习简化复杂词语的更简单的替换。</li>
<li>methods: 该论文提出了一种基于Transformer的多语言控制 Lexical Simplification（LS）系统，使用语言特定前缀、控制token和预先训练的掩码语言模型来学习简化复杂词语。</li>
<li>results: 论文的evaluation结果表明，该模型在三个常见的LS数据集（LexMTurk、BenchLS和NNSEval）上的表现比前一代的模型（如LSBert和ConLS）更好，并且在TSAR-2022多语言LS共享任务中的一部分检测数据集上与参与系统竞争，甚至在某些指标上超越GPT-3模型。此外，模型还在西班牙语和葡萄牙语上获得了性能提升。<details>
<summary>Abstract</summary>
Text is by far the most ubiquitous source of knowledge and information and should be made easily accessible to as many people as possible; however, texts often contain complex words that hinder reading comprehension and accessibility. Therefore, suggesting simpler alternatives for complex words without compromising meaning would help convey the information to a broader audience. This paper proposes mTLS, a multilingual controllable Transformer-based Lexical Simplification (LS) system fined-tuned with the T5 model. The novelty of this work lies in the use of language-specific prefixes, control tokens, and candidates extracted from pre-trained masked language models to learn simpler alternatives for complex words. The evaluation results on three well-known LS datasets -- LexMTurk, BenchLS, and NNSEval -- show that our model outperforms the previous state-of-the-art models like LSBert and ConLS. Moreover, further evaluation of our approach on the part of the recent TSAR-2022 multilingual LS shared-task dataset shows that our model performs competitively when compared with the participating systems for English LS and even outperforms the GPT-3 model on several metrics. Moreover, our model obtains performance gains also for Spanish and Portuguese.
</details>
<details>
<summary>摘要</summary>
文本是最常见的知识和信息来源，应该让更多人有访问权。然而，文本经常包含复杂的词语，这会降低阅读理解和访问性。因此，建议使用 simpler alternatives for complex words，不会妨碍意思的传递，可以帮助更多人理解。这篇论文提出了一种名为 mTLS 的多语言可控Transformer 基本 Lexical Simplification（LS）系统，该系统通过语言特定前缀、控制 токен和来自预训练的 masked language model 中的候选词来学习 simpler alternatives for complex words。我们的评估结果在 LexMTurk、BenchLS 和 NNSEval 三个常见 LS 数据集上表明，我们的模型比前一代模型 like LSBert 和 ConLS 更高效。此外，我们的方法在 TSAR-2022 年度多语言 LS 共同任务中的一部分进行了进一步的评估，我们的模型与英语 LS 中的参与系统竞争，并在一些指标上超越 GPT-3 模型。此外，我们的模型在西班牙语和葡萄牙语上也获得了性能提升。
</details></li>
</ul>
<hr>
<h2 id="Do-predictability-factors-towards-signing-avatars-hold-across-cultures"><a href="#Do-predictability-factors-towards-signing-avatars-hold-across-cultures" class="headerlink" title="Do predictability factors towards signing avatars hold across cultures?"></a>Do predictability factors towards signing avatars hold across cultures?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02103">http://arxiv.org/abs/2307.02103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelhadi Soudi, Manal El Hakkaoui, Kristof Van Laerhoven</li>
<li>for: 这研究旨在探讨虚拟人物技术在听力障碍人群中的可用性和应用前景，以及听力障碍人群对虚拟人物的acceptance和态度的影响因素。</li>
<li>methods: 本研究采用问卷调查方法，询问听力障碍人群对虚拟人物的态度和acceptance水平，并对听力障碍人群的技术经验、听力状况、年龄和手语流畅度进行分析，以了解各因素对态度的影响。</li>
<li>results: 研究发现，听力障碍人群对虚拟人物的态度和acceptance水平异常高，且与听力状况、技术经验和年龄有关。特别是，MSL用户对虚拟人物的态度较低，与其他研究结果相比。<details>
<summary>Abstract</summary>
Avatar technology can offer accessibility possibilities and improve the Deaf-and-Hard of Hearing sign language users access to communication, education and services, such as the healthcare system. However, sign language users acceptance of signing avatars as well as their attitudes towards them vary and depend on many factors. Furthermore, research on avatar technology is mostly done by researchers who are not Deaf. The study examines the extent to which intrinsic or extrinsic factors contribute to predict the attitude towards avatars across cultures. Intrinsic factors include the characteristics of the avatar, such as appearance, movements and facial expressions. Extrinsic factors include users technology experience, their hearing status, age and their sign language fluency. This work attempts to answer questions such as, if lower attitude ratings are related to poor technology experience with ASL users, for example, is that also true for Moroccan Sign Language (MSL) users? For the purposes of the study, we designed a questionnaire to understand MSL users attitude towards avatars. Three groups of participants were surveyed: Deaf (57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then compared with those reported in other relevant studies.
</details>
<details>
<summary>摘要</summary>
《备用人物技术可以提供更多的可用性和改善聋听人用手语讲解、教育和服务的访问，例如医疗系统。然而，手语用户对签名人物的接受度和对其的态度是多方面的和各不相同的。此外，研究人员大多是不聋听的。本研究探讨了对签名人物的态度是由内在或外在因素决定的程度。内在因素包括人物的特征，如外表、运动和表情。外在因素包括用户的技术经验、听力状况、年龄和手语流利程度。本研究试图回答问题，例如，听力不佳的ASL用户对人物的态度评分是否与MSL用户相似？为了了解MSL用户对人物的态度，我们设计了一份问naire。三个组合体的参与者被调查：聋听（57）、正常听力（20）和听力不佳（3）。我们的研究结果与其他相关研究的结果进行比较。
</details></li>
</ul>
<hr>
<h2 id="Different-Games-in-Dialogue-Combining-character-and-conversational-types-in-strategic-choice"><a href="#Different-Games-in-Dialogue-Combining-character-and-conversational-types-in-strategic-choice" class="headerlink" title="Different Games in Dialogue: Combining character and conversational types in strategic choice"></a>Different Games in Dialogue: Combining character and conversational types in strategic choice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02087">http://arxiv.org/abs/2307.02087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alafate Abulimiti</li>
<li>for:  investigating the interaction of conversational type and character types of interlocutors</li>
<li>methods: combining decision making process for selecting dialogue moves with character type and conversational type, and presenting a mathematical model to illustrate the interactions</li>
<li>results: presenting a quantitative approach to understanding the interactions between conversational type and character types in dialogue moves<details>
<summary>Abstract</summary>
In this paper, we show that investigating the interaction of conversational type (often known as language game or speech genre) with the character types of the interlocutors is worthwhile. We present a method of calculating the decision making process for selecting dialogue moves that combines character type and conversational type. We also present a mathematical model that illustrate these factors' interactions in a quantitative way.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们证明了对话类型（常称为语言游戏或语言类型）与对话参与者的人物类型之间的交互是有价值的。我们提出了一种计算对话搬运的决策过程，该过程结合了人物类型和对话类型。我们还提出了一个数学模型，用于显示这些因素之间的交互。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-multilingual-transfer-for-unsupervised-semantic-acoustic-word-embeddings"><a href="#Leveraging-multilingual-transfer-for-unsupervised-semantic-acoustic-word-embeddings" class="headerlink" title="Leveraging multilingual transfer for unsupervised semantic acoustic word embeddings"></a>Leveraging multilingual transfer for unsupervised semantic acoustic word embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02083">http://arxiv.org/abs/2307.02083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christiaan Jacobs, Herman Kamper</li>
<li>for: 这个论文主要研究了语音嵌入模型（AWE）的含义模型化，即使只有未标注的语音数据。</li>
<li>methods: 作者提出了一些使用预训练多语言AWE模型的策略，包括聚类词段使用多语言AWE模型的中心点， derivation soft pseudo-word标签，并在Skipgram-like模型中训练soft vectors。</li>
<li>results: 作者的方法在语义相似任务中表现出色，比过去的所有语义AWE方法都高。此外，这种多语言传递方法还能够用于下游语义查询例子搜索。<details>
<summary>Abstract</summary>
Acoustic word embeddings (AWEs) are fixed-dimensional vector representations of speech segments that encode phonetic content so that different realisations of the same word have similar embeddings. In this paper we explore semantic AWE modelling. These AWEs should not only capture phonetics but also the meaning of a word (similar to textual word embeddings). We consider the scenario where we only have untranscribed speech in a target language. We introduce a number of strategies leveraging a pre-trained multilingual AWE model -- a phonetic AWE model trained on labelled data from multiple languages excluding the target. Our best semantic AWE approach involves clustering word segments using the multilingual AWE model, deriving soft pseudo-word labels from the cluster centroids, and then training a Skipgram-like model on the soft vectors. In an intrinsic word similarity task measuring semantics, this multilingual transfer approach outperforms all previous semantic AWE methods. We also show -- for the first time -- that AWEs can be used for downstream semantic query-by-example search.
</details>
<details>
<summary>摘要</summary>
听音字嵌入（AWE）是一种固定维度的 вектор表示方法，用于 repre senting  speech segments 中的声音内容，以便不同的实现都有相似的嵌入。在这篇论文中，我们探讨 semantic AWE 模型。这些 AWE 不仅应 capture 声音特征，还应包含语言中词的含义（类似于文本字嵌入）。我们假设只有目标语言的无标注语音。我们提出了一些利用预训练的多语言 AWE 模型（包括目标语言）的策略。我们的最佳 semantics AWE 方法是将词段分组使用多语言 AWE 模型， deriv ing 软 Pseudo-word 标签从分组中心点，然后在 Skipgram-like 模型中训练 soft vectors。在内在词义相似任务中，这种多语言传输方法超过了所有之前的 semantics AWE 方法。我们还示出了，AWE 可以用于下游 semantic query-by-example 搜索。
</details></li>
</ul>
<hr>
<h2 id="Graph-Contrastive-Topic-Model"><a href="#Graph-Contrastive-Topic-Model" class="headerlink" title="Graph Contrastive Topic Model"></a>Graph Contrastive Topic Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02078">http://arxiv.org/abs/2307.02078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhehengluok/gctm">https://github.com/zhehengluok/gctm</a></li>
<li>paper_authors: Zheheng Luo, Lei Liu, Qianqian Xie, Sophia Ananiadou</li>
<li>for: 本研究旨在提高NTMs中的话题准确性和文档表示力，并解决采样偏袋问题。</li>
<li>methods: 我们提出了一种新的采样假设，即负样本中的词语应该具有与原型相关的 semantics。基于此假设，我们提出了一种图像对话Token模型（GCTM），它通过图像对话学习（GCL）来使用有用的正样本和负样本，以提高文档话题表示和隐藏话题的学习。</li>
<li>results: 我们在多个标准数据集上进行了实验，并证明了我们的方法可以提高话题准确性和文档表示力，并且比既有最佳方法更高。<details>
<summary>Abstract</summary>
Existing NTMs with contrastive learning suffer from the sample bias problem owing to the word frequency-based sampling strategy, which may result in false negative samples with similar semantics to the prototypes. In this paper, we aim to explore the efficient sampling strategy and contrastive learning in NTMs to address the aforementioned issue. We propose a new sampling assumption that negative samples should contain words that are semantically irrelevant to the prototype. Based on it, we propose the graph contrastive topic model (GCTM), which conducts graph contrastive learning (GCL) using informative positive and negative samples that are generated by the graph-based sampling strategy leveraging in-depth correlation and irrelevance among documents and words. In GCTM, we first model the input document as the document word bipartite graph (DWBG), and construct positive and negative word co-occurrence graphs (WCGs), encoded by graph neural networks, to express in-depth semantic correlation and irrelevance among words. Based on the DWBG and WCGs, we design the document-word information propagation (DWIP) process to perform the edge perturbation of DWBG, based on multi-hop correlations/irrelevance among documents and words. This yields the desired negative and positive samples, which will be utilized for GCL together with the prototypes to improve learning document topic representations and latent topics. We further show that GCL can be interpreted as the structured variational graph auto-encoder which maximizes the mutual information of latent topic representations of different perspectives on DWBG. Experiments on several benchmark datasets demonstrate the effectiveness of our method for topic coherence and document representation learning compared with existing SOTA methods.
</details>
<details>
<summary>摘要</summary>
现有的NTMs通过对比学习受到样本偏袋问题的影响，这可能导致假象样本与prototype的semantics相似。在这篇文章中，我们想探索NTMs中有效的采样策略和对比学习方法来解决上述问题。我们提出了一种新的采样假设，即负样本应包含与prototype的semantics不相关的词。基于这个假设，我们提出了图像对比话题模型（GCTM），它通过图像对比学习（GCL）使用了有用的负样本和prototype来提高文档主题表示和潜在主题。在GCTM中，我们首先将输入文档表示为文档词biipartite图（DWBG），然后构建负样本和正样本的word co-occurrence图（WCG），通过图像神经网络编码，表达文档和词之间的深度semantic correlation和irrelevance。基于DWBG和WCG，我们设计了文档词信息传播过程（DWIP）来进行DWBG的边刺激，基于文档和词之间的多趟相关性/不相关性。这会生成我们需要的负样本和正样本，它们将与prototype一起用于GCL来提高文档主题表示和潜在主题。我们还证明了GCL可以被视为structured variational graph auto-encoder，它最大化了不同角度的DWBG上latent topic representation之间的mutual information。在多个标准 benchmark dataset上进行了实验，我们发现我们的方法在主题准确性和文档表示学习方面比现有SOTA方法更有效。
</details></li>
</ul>
<hr>
<h2 id="Flacuna-Unleashing-the-Problem-Solving-Power-of-Vicuna-using-FLAN-Fine-Tuning"><a href="#Flacuna-Unleashing-the-Problem-Solving-Power-of-Vicuna-using-FLAN-Fine-Tuning" class="headerlink" title="Flacuna: Unleashing the Problem Solving Power of Vicuna using FLAN Fine-Tuning"></a>Flacuna: Unleashing the Problem Solving Power of Vicuna using FLAN Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02053">http://arxiv.org/abs/2307.02053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/declare-lab/flacuna">https://github.com/declare-lab/flacuna</a></li>
<li>paper_authors: Deepanway Ghosal, Yew Ken Chia, Navonil Majumder, Soujanya Poria<br>for:* The paper is focused on investigating the impact of the third factor (instruction dataset) on the performance of large language models (LLMs) that utilize encoder-decoder or decoder-only architecture.methods:* The paper uses a customized instruction dataset collection called FLANMINI, which includes a subset of the large-scale instruction dataset known as FLAN, as well as various code-related datasets and conversational datasets derived from ChatGPT&#x2F;GPT-4.* The paper fine-tunes VICUNA, a large language model based on LLAMA, on the FLAN dataset to obtain enhanced problem-solving abilities.results:* The paper shows that fine-tuning VICUNA on the FLAN dataset leads to significant improvements across numerous benchmark datasets in INSTRUCTEVAL.* The paper also introduces FLACUNA, a publicly available model that is fine-tuned VICUNA on the FLAN dataset, which demonstrates improved problem-solving abilities compared to the latest decoder-based LLMs.<details>
<summary>Abstract</summary>
Recently, the release of INSTRUCTEVAL has provided valuable insights into the performance of large language models (LLMs) that utilize encoder-decoder or decoder-only architecture. Interestingly, despite being introduced four years ago, T5-based LLMs, such as FLAN-T5, continue to outperform the latest decoder-based LLMs, such as LLAMA and VICUNA, on tasks that require general problem-solving skills. This performance discrepancy can be attributed to three key factors: (1) Pre-training data, (2) Backbone architecture, and (3) Instruction dataset. In this technical report, our main focus is on investigating the impact of the third factor by leveraging VICUNA, a large language model based on LLAMA, which has undergone fine-tuning on ChatGPT conversations. To achieve this objective, we fine-tuned VICUNA using a customized instruction dataset collection called FLANMINI. This collection includes a subset of the large-scale instruction dataset known as FLAN, as well as various code-related datasets and conversational datasets derived from ChatGPT/GPT-4. This dataset comprises a large number of tasks that demand problem-solving skills. Our experimental findings strongly indicate that the enhanced problem-solving abilities of our model, FLACUNA, are obtained through fine-tuning VICUNA on the FLAN dataset, leading to significant improvements across numerous benchmark datasets in INSTRUCTEVAL. FLACUNA is publicly available at https://huggingface.co/declare-lab/flacuna-13b-v1.0.
</details>
<details>
<summary>摘要</summary>
近期，INSTRUCTEVAL的发布提供了大语言模型（LLM）使用encoder-decoder或decoder-only架构的表现的有价值信息。有趣的是，虽然四年前出现了T5基于的LLM，如FLAN-T5，仍然在需要通用问题解决能力的任务上超过最新的decoder基于LLM，如LLAMA和VICUNA。这种表现差异可以归因于三个关键因素：（1）预训练数据，（2）后端架构，（3）指令集。在这份技术报告中，我们主要关注第三个因素，通过使用基于LLAMA的大语言模型VICUNA进行细化，以便 Investigate the impact of this factor. To achieve this goal, we fine-tuned VICUNA using a customized instruction dataset collection called FLANMINI. This collection includes a subset of the large-scale instruction dataset known as FLAN, as well as various code-related datasets and conversational datasets derived from ChatGPT/GPT-4. This dataset comprises a large number of tasks that demand problem-solving skills. Our experimental findings strongly indicate that the enhanced problem-solving abilities of our model, FLACUNA, are obtained through fine-tuning VICUNA on the FLAN dataset, leading to significant improvements across numerous benchmark datasets in INSTRUCTEVAL. FLACUNA is publicly available at https://huggingface.co/declare-lab/flacuna-13b-v1.0.
</details></li>
</ul>
<hr>
<h2 id="CAME-Confidence-guided-Adaptive-Memory-Efficient-Optimization"><a href="#CAME-Confidence-guided-Adaptive-Memory-Efficient-Optimization" class="headerlink" title="CAME: Confidence-guided Adaptive Memory Efficient Optimization"></a>CAME: Confidence-guided Adaptive Memory Efficient Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02047">http://arxiv.org/abs/2307.02047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangluo7/came">https://github.com/yangluo7/came</a></li>
<li>paper_authors: Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, Yang You</li>
<li>for: 本文主要用于研究一种能同时实现快速收敛和低内存使用的自适应优化器，以提高大语言模型的训练效率。</li>
<li>methods: 本文使用了一种自信量指导策略来降低现有的内存有效优化器的不稳定性。基于这种策略，我们提出了一种名为CAME的优化器，能同时实现快速收敛和低内存使用。</li>
<li>results: 广泛的实验表明，CAME可以在不同的NLP任务中（如BERT和GPT-2训练）实现稳定的训练和高度的性能。特别是在BERT预训练中使用大批处理（32,768）时，我们的提议的优化器可以更快地收敛并达到更高的准确率，比 Adam 优化器更高。<details>
<summary>Abstract</summary>
Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available.
</details>
<details>
<summary>摘要</summary>
优化器方法，如 Adam 和 LAMB，在大型语言模型的训练中表现出色。然而，需要适应性导致每个参数的二次积分估计需要保持，这会带来较高的额外存储开销。为解决这问题，许多快速优化器（例如 Adafactor）已经被提出，可以减少auxiliary存储使用量，但是它们通常会带来性能损失。在本文中，我们首先研究了一种 confidence-guided 策略，以减少现有的存储效率优化器的不稳定性。基于这种策略，我们提出了 CAME，可以同时实现两个目标：快速 convergence 和低存储使用量。我们的实验表明，CAME 在不同的 NLP 任务上（包括 BERT 和 GPT-2 训练）具有稳定的训练和高效的性能。特别是，对于 BERT 的预训练，我们的提出的优化器在大批处理大小为 32,768 时实现了更快的 convergence 和更高的准确率，相比 Adam 优化器。CAME 的实现已经公开可用。
</details></li>
</ul>
<hr>
<h2 id="Using-Data-Augmentations-and-VTLN-to-Reduce-Bias-in-Dutch-End-to-End-Speech-Recognition-Systems"><a href="#Using-Data-Augmentations-and-VTLN-to-Reduce-Bias-in-Dutch-End-to-End-Speech-Recognition-Systems" class="headerlink" title="Using Data Augmentations and VTLN to Reduce Bias in Dutch End-to-End Speech Recognition Systems"></a>Using Data Augmentations and VTLN to Reduce Bias in Dutch End-to-End Speech Recognition Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02009">http://arxiv.org/abs/2307.02009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvina Patel, Odette Scharenborg</li>
<li>for: 降低不同年龄和非本地语言 speaker的偏见</li>
<li>methods: 使用 state-of-the-art 速度扰动和spectral augmentation 数据增强技术，以及 Vocal Tract Length Normalization (VTLN) 来normalize 频谱差异</li>
<li>results: 组合数据增强和 VTLN 可以降低平均 WER 和偏见，对不同多种speaker group的表现提高了6.9%和3.9%。VTLN 模型在德语和中文儿童语音上也有良好的普适性<details>
<summary>Abstract</summary>
Speech technology has improved greatly for norm speakers, i.e., adult native speakers of a language without speech impediments or strong accents. However, non-norm or diverse speaker groups show a distinct performance gap with norm speakers, which we refer to as bias. In this work, we aim to reduce bias against different age groups and non-native speakers of Dutch. For an end-to-end (E2E) ASR system, we use state-of-the-art speed perturbation and spectral augmentation as data augmentation techniques and explore Vocal Tract Length Normalization (VTLN) to normalise for spectral differences due to differences in anatomy. The combination of data augmentation and VTLN reduced the average WER and bias across various diverse speaker groups by 6.9% and 3.9%, respectively. The VTLN model trained on Dutch was also effective in improving performance of Mandarin Chinese child speech, thus, showing generalisability across languages
</details>
<details>
<summary>摘要</summary>
语音技术在 норм 说话人（即成年本地语言无异常发音或强调的人）方面有很大的进步。然而，非 norm 或多样化 speaker group  display  distinct performance gap with norm speakers，我们称之为偏见。在这种工作中，我们想减少对不同年龄组和非本地说话人的偏见。为一个端到端（E2E）语音识别系统，我们使用现有的速度扰动和spectral augmentation作为数据增强技术，并探索 vocals tract length normalization（VTLN）来归一化因为身体差异而导致的spectral differences。这些技术的组合reduced the average WER和偏见 across various diverse speaker groups by 6.9% and 3.9%, respectively。VTLN模型在荷兰语上训练也有效地提高了中文儿童语音的性能，因此显示了语言通用性。
</details></li>
</ul>
<hr>
<h2 id="PULSAR-at-MEDIQA-Sum-2023-Large-Language-Models-Augmented-by-Synthetic-Dialogue-Convert-Patient-Dialogues-to-Medical-Records"><a href="#PULSAR-at-MEDIQA-Sum-2023-Large-Language-Models-Augmented-by-Synthetic-Dialogue-Convert-Patient-Dialogues-to-Medical-Records" class="headerlink" title="PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records"></a>PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02006">http://arxiv.org/abs/2307.02006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuping-wu/pulsar">https://github.com/yuping-wu/pulsar</a></li>
<li>paper_authors: Viktor Schlegel, Hao Li, Yuping Wu, Anand Subramanian, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Daniel Beck, Xiaojun Zeng, Riza Theresa Batista-Navarro, Stefan Winkler, Goran Nenadic</li>
<li>for: 本文描述了我们在 ImageClef 2023 MediQA-Sum 任务中对医生 диалогу摘要的提交系统 PULSAR。</li>
<li>methods: 该方案基于域pecific预训练，生成专门的自然语言模型，并在任务特定的自然数据上进行训练，同时使用黑盒 LLM 生成的 sintetic 数据进行数据增强。</li>
<li>results: 我们发现域pecific预训练和数据增强的证据有限，但是将语言模型扩大scale 实现了最好的性能提升。我们的方法在任务B中排名第二和第三，其中 code 可以在 <a target="_blank" rel="noopener" href="https://github.com/yuping-wu/PULSAR">https://github.com/yuping-wu/PULSAR</a> 上获取。<details>
<summary>Abstract</summary>
This paper describes PULSAR, our system submission at the ImageClef 2023 MediQA-Sum task on summarising patient-doctor dialogues into clinical records. The proposed framework relies on domain-specific pre-training, to produce a specialised language model which is trained on task-specific natural data augmented by synthetic data generated by a black-box LLM. We find limited evidence towards the efficacy of domain-specific pre-training and data augmentation, while scaling up the language model yields the best performance gains. Our approach was ranked second and third among 13 submissions on task B of the challenge. Our code is available at https://github.com/yuping-wu/PULSAR.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文描述了我们在ImageClef 2023 MediQA-Sum任务中提交的PULSAR系统，该系统使用域pecific预训练，生成特殊化语言模型，并在任务特定的自然数据上进行训练，同时使用由黑盒LLM生成的 sintetic数据进行数据增强。我们发现域pecific预训练和数据增强具有有限的效果，而scale up语言模型则能够实现最佳性能提升。我们的方法在任务B中 ranking第二和第三，排名13个提交。我们的代码可以在https://github.com/yuping-wu/PULSAR中找到。
</details></li>
</ul>
<hr>
<h2 id="Open-Domain-Hierarchical-Event-Schema-Induction-by-Incremental-Prompting-and-Verification"><a href="#Open-Domain-Hierarchical-Event-Schema-Induction-by-Incremental-Prompting-and-Verification" class="headerlink" title="Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification"></a>Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01972">http://arxiv.org/abs/2307.01972</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/raspberryice/inc-schema">https://github.com/raspberryice/inc-schema</a></li>
<li>paper_authors: Sha Li, Ruining Zhao, Manling Li, Heng Ji, Chris Callison-Burch, Jiawei Han</li>
<li>for: 本研究旨在提取事件知识Graph结构，并从大语言模型（LLM）中提取事件schema。</li>
<li>methods: 我们提出了一种新的方法，即将事件schema视为常识知识，并使用增量提示和验证方法来构建事件Graph。</li>
<li>results: 我们的方法可以生成大型和复杂的事件Graph，并且与直接使用LLM生成线性Graph相比，可以提高7.2%的时间关系和31.0%的层次关系。此外，我们的方法也可以让人类评估者在翻译事件Graph时覆盖了$\sim$10%更多的事件，并评估我们的schema得分高于前一个关闭领域的模型1.3分（在5分满分标准下）。<details>
<summary>Abstract</summary>
Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, our method can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover $\sim$10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.
</details>
<details>
<summary>摘要</summary>
Event schemas are a form of common sense about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of common sense that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, our method can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover approximately 10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.
</details></li>
</ul>
<hr>
<h2 id="Transformed-Protoform-Reconstruction"><a href="#Transformed-Protoform-Reconstruction" class="headerlink" title="Transformed Protoform Reconstruction"></a>Transformed Protoform Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01896">http://arxiv.org/abs/2307.01896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cmu-llab/acl-2023">https://github.com/cmu-llab/acl-2023</a></li>
<li>paper_authors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David Mortensen</li>
<li>for: 这个论文是为了重构拉丁语族 protoform（祖语形态）而写的。</li>
<li>methods: 这个论文使用了 RNN 基本循环网络和注意力机制来实现 protoform 重构。</li>
<li>results: 该模型在两个不同的数据集上（拉丁语族数据集和中文数据集）都达到了新的高水平，并且在多个不同的指标上比前一代模型（Meloni et al., 2021）表现出色。<details>
<summary>Abstract</summary>
Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al. (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.
</details>
<details>
<summary>摘要</summary>
protoform reconstruction是推理古代语言的 morpheme或词语在祖语言中的推理任务。 Meloni et al. (2021) 使用 RNN 基于 Encoder-Decoder 模型 WITH attention 模型实现了拉丁protoform reconstruction 的状态对。我们对其模型进行了更新，使用现代 seq2seq 模型：Transformer。我们的模型在两个不同的数据集上（Romance 数据集和 Hou 2004 中的 Chinese 数据集）表现出了更高的性能，并且在这两个数据集上进行了可能的phylogenetic signal的探索。我们的代码可以在https://github.com/cmu-llab/acl-2023 中找到。
</details></li>
</ul>
<hr>
<h2 id="ProPILE-Probing-Privacy-Leakage-in-Large-Language-Models"><a href="#ProPILE-Probing-Privacy-Leakage-in-Large-Language-Models" class="headerlink" title="ProPILE: Probing Privacy Leakage in Large Language Models"></a>ProPILE: Probing Privacy Leakage in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01881">http://arxiv.org/abs/2307.01881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, Seong Joon Oh</li>
<li>for: 这篇论文旨在帮助数据主人（data subject）了解大语言模型（LLM）中可能泄露的个人识别资料（PII）的水平。</li>
<li>methods: 这篇论文提出了一个名为ProPILE的新工具，可以让数据主人通过自己的个人资料来评估 LLM 模型中是否泄露 PII。</li>
<li>results: 这篇论文显示了 ProPILE 可以帮助数据主人评估 OPT-1.3B 模型是否泄露 PII，并且可以运用更强大的提示来评估 LLM 服务提供者自己的 PII 泄露水平。<details>
<summary>Abstract</summary>
The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.
</details>
<details>
<summary>摘要</summary>
“快速发展和广泛使用大型语言模型（LLM）已引发了重要的个人隐私泄露（PII）问题。这些模型通常是基于互联网收集的大量数据进行训练，可能包含敏感个人数据。本文介绍了一种名为ProPILE的新的探测工具，用于赋予数据主（PII的所有者）对LLM基础设施中的隐私侵犯程度进行了解和控制。ProPILE允许数据主根据自己的PII提取模型中的敏感数据，以评估PII泄露的可能性。我们在OPT-1.3B模型基于公共可用的Pile数据集上进行了应用。我们显示了如何假设的数据主可以评估其PII是否包含在Pile数据集中被泄露。此外，ProPILE还可以被LLM服务提供者使用来评估自己的PII泄露水平，并通过特定于自己模型的更强的提示来进行评估。这种工具为数据主提供了一个前所未有的控制和了解自己数据在互联网上的能力。”
</details></li>
</ul>
<hr>
<h2 id="Decoding-the-Popularity-of-TV-Series-A-Network-Analysis-Perspective"><a href="#Decoding-the-Popularity-of-TV-Series-A-Network-Analysis-Perspective" class="headerlink" title="Decoding the Popularity of TV Series: A Network Analysis Perspective"></a>Decoding the Popularity of TV Series: A Network Analysis Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05329">http://arxiv.org/abs/2307.05329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Melody Yu</li>
<li>for: 这个研究用于探究电视剧集的人物网络和IMDB评分之间的关系。</li>
<li>methods: 研究使用电视剧集的剧情中人物之间的互动关系创建人物网络，然后对每集的人物网络指标进行计算，包括节点度和图密度等。</li>
<li>results: 研究发现，电视剧集的certain network metrics和IMDB评分之间存在强相关关系。<details>
<summary>Abstract</summary>
In this paper, we analyze the character networks extracted from three popular television series and explore the relationship between a TV show episode's character network metrics and its review from IMDB. Character networks are graphs created from the plot of a TV show that represents the interactions of characters in scenes, indicating the presence of a connection between them. We calculate various network metrics for each episode, such as node degree and graph density, and use these metrics to explore the potential relationship between network metrics and TV series reviews from IMDB. Our results show that certain network metrics of character interactions in episodes have a strong correlation with the review score of TV series. Our research aims to provide more quantitative information that can help TV producers understand how to adjust the character dynamics of future episodes to appeal to their audience. By understanding the impact of character interactions on audience engagement and enjoyment, producers can make informed decisions about the development of their shows.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们分析了三部电视剧中的人物网络，探究电视剧集 episoden 的人物网络指标与IMDB的评论之间的关系。人物网络是从电视剧的剧情中提取的人物之间的互动关系图，表明了各个人物之间的连接存在。我们计算了每集的不同网络指标，如节点度和图密度，并使用这些指标来探究电视剧集的评论分数与人物网络之间的可能的关系。我们的结果表明，一些集 episoden 的人物互动网络指标与电视剧的IMDB评论分数具有强相关性。我们的研究旨在为电视制作人提供更多的量化信息，帮助他们更好地理解如何通过调整人物之间的互动，来满足观众的需求。通过了解人物互动对观众参与度和满意度的影响，制作人可以做出更有知识的决策，以提高他们的电视剧的质量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/05/cs.CL_2023_07_05/" data-id="clpahu6zk007z3h88bh6afcsg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/cs.LG_2023_07_05/" class="article-date">
  <time datetime="2023-07-05T10:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/05/cs.LG_2023_07_05/">cs.LG - 2023-07-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Machine-learning-at-the-mesoscale-a-computation-dissipation-bottleneck"><a href="#Machine-learning-at-the-mesoscale-a-computation-dissipation-bottleneck" class="headerlink" title="Machine learning at the mesoscale: a computation-dissipation bottleneck"></a>Machine learning at the mesoscale: a computation-dissipation bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02379">http://arxiv.org/abs/2307.02379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Ingrosso, Emanuele Panizon</li>
<li>for: 该论文旨在探讨物理系统中信息处理成本的兼顾问题，即性能和能getic费用之间的trade-off。</li>
<li>methods: 该论文采用了 computation-dissipation bottleneck  frameworks，通过使用实际数据集和 sintetic任务，证明了不对称互动的存在导致了性能提高。</li>
<li>results: 该论文的研究结果表明，在输入输出设备中，非平衡情况会导致信息压缩、计算输入输出和动力不逆转行为的均衡。<details>
<summary>Abstract</summary>
The cost of information processing in physical systems calls for a trade-off between performance and energetic expenditure. Here we formulate and study a computation-dissipation bottleneck in mesoscopic systems used as input-output devices. Using both real datasets and synthetic tasks, we show how non-equilibrium leads to enhanced performance. Our framework sheds light on a crucial compromise between information compression, input-output computation and dynamic irreversibility induced by non-reciprocal interactions.
</details>
<details>
<summary>摘要</summary>
信息处理成本在物理系统中需要一种权衡 между性能和能getic耗用。我们在 mesoscopic 系统中作为输入输出设备形式ulated computation-dissipation bottleneck，并使用实际数据和synthetic任务来表征非平衡导致性能增强。我们的框架揭示了一种关键的信息压缩、输入输出计算和动力不可逆性引起的妥协。
</details></li>
</ul>
<hr>
<h2 id="Continuum-Limits-of-Ollivier’s-Ricci-Curvature-on-data-clouds-pointwise-consistency-and-global-lower-bounds"><a href="#Continuum-Limits-of-Ollivier’s-Ricci-Curvature-on-data-clouds-pointwise-consistency-and-global-lower-bounds" class="headerlink" title="Continuum Limits of Ollivier’s Ricci Curvature on data clouds: pointwise consistency and global lower bounds"></a>Continuum Limits of Ollivier’s Ricci Curvature on data clouds: pointwise consistency and global lower bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02378">http://arxiv.org/abs/2307.02378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Garcia Trillos, Melanie Weber</li>
<li>for: 研究了一个低维抽象 $\mathcal{M} \subseteq \mathbb{R}^d$ 上的Random geometric graph的 curvature与抽象 $\mathcal{M}$ 的曲率之间的关系，通过维度下的连续假设。</li>
<li>methods: 使用了Ollivier的离散 Ricci curvature的连续假设，并证明了点性不变的consistency结果，以及如果 $\mathcal{M}$ 有下界为正数的 Ricci curvature，那么Random geometric graph将高概率拥有这种全球结构性。</li>
<li>results: 显示了热核辐射在图上的 contraction 性和抽象 $\mathcal{M}$ 的曲率之间的关系，以及从数据云中学习抽象 $\mathcal{M}$ 的应用。特别是，证明了consistency结果可以用来描述抽象 $\mathcal{M}$ 的内在曲率从外在曲率中。<details>
<summary>Abstract</summary>
Let $\mathcal{M} \subseteq \mathbb{R}^d$ denote a low-dimensional manifold and let $\mathcal{X}= \{ x_1, \dots, x_n \}$ be a collection of points uniformly sampled from $\mathcal{M}$. We study the relationship between the curvature of a random geometric graph built from $\mathcal{X}$ and the curvature of the manifold $\mathcal{M}$ via continuum limits of Ollivier's discrete Ricci curvature. We prove pointwise, non-asymptotic consistency results and also show that if $\mathcal{M}$ has Ricci curvature bounded from below by a positive constant, then the random geometric graph will inherit this global structural property with high probability. We discuss applications of the global discrete curvature bounds to contraction properties of heat kernels on graphs, as well as implications for manifold learning from data clouds. In particular, we show that the consistency results allow for characterizing the intrinsic curvature of a manifold from extrinsic curvature.
</details>
<details>
<summary>摘要</summary>
Let $\mathcal{M} \subseteq \mathbb{R}^d$ be a low-dimensional manifold, and let $\mathcal{X} = \{ x_1, \dots, x_n \}$ be a collection of points uniformly sampled from $\mathcal{M}$. We study the relationship between the curvature of a random geometric graph built from $\mathcal{X}$ and the curvature of the manifold $\mathcal{M}$ via continuum limits of Ollivier's discrete Ricci curvature. We prove pointwise, non-asymptotic consistency results and also show that if $\mathcal{M}$ has Ricci curvature bounded from below by a positive constant, then the random geometric graph will inherit this global structural property with high probability. We discuss applications of the global discrete curvature bounds to contraction properties of heat kernels on graphs, as well as implications for manifold learning from data clouds. In particular, we show that the consistency results allow for characterizing the intrinsic curvature of a manifold from extrinsic curvature.Here is the translation in Traditional Chinese:Let $\mathcal{M} \subseteq \mathbb{R}^d$ be a low-dimensional manifold, and let $\mathcal{X} = \{ x_1, \dots, x_n \}$ be a collection of points uniformly sampled from $\mathcal{M}$. We study the relationship between the curvature of a random geometric graph built from $\mathcal{X}$ and the curvature of the manifold $\mathcal{M}$ via continuum limits of Ollivier's discrete Ricci curvature. We prove pointwise, non-asymptotic consistency results and also show that if $\mathcal{M}$ has Ricci curvature bounded from below by a positive constant, then the random geometric graph will inherit this global structural property with high probability. We discuss applications of the global discrete curvature bounds to contraction properties of heat kernels on graphs, as well as implications for manifold learning from data clouds. In particular, we show that the consistency results allow for characterizing the intrinsic curvature of a manifold from extrinsic curvature.
</details></li>
</ul>
<hr>
<h2 id="Distance-Preserving-Machine-Learning-for-Uncertainty-Aware-Accelerator-Capacitance-Predictions"><a href="#Distance-Preserving-Machine-Learning-for-Uncertainty-Aware-Accelerator-Capacitance-Predictions" class="headerlink" title="Distance Preserving Machine Learning for Uncertainty Aware Accelerator Capacitance Predictions"></a>Distance Preserving Machine Learning for Uncertainty Aware Accelerator Capacitance Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02367">http://arxiv.org/abs/2307.02367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Goldenberg, Malachi Schram, Kishansingh Rajput, Thomas Britton, Chris Pappas, Dan Lu, Jared Walden, Majdi I. Radaideh, Sarah Cousineau, Sudarshan Harave</li>
<li>for: 这个论文的目的是提供一种可靠的机器学习模型，尤其是在安全敏感的应用中，如加速器系统。</li>
<li>methods: 这个论文使用了深度神经网络和 Gaussian process 近似技术，并比较了两种不同的特征提取器，包括单元值分解和spectral-normalized dense layer。</li>
<li>results: 这个模型可以实现较好的距离保持和内部容易预测，并且预测在正常分布中的电容量值几乎没有误差（少于1%）。<details>
<summary>Abstract</summary>
Providing accurate uncertainty estimations is essential for producing reliable machine learning models, especially in safety-critical applications such as accelerator systems. Gaussian process models are generally regarded as the gold standard method for this task, but they can struggle with large, high-dimensional datasets. Combining deep neural networks with Gaussian process approximation techniques have shown promising results, but dimensionality reduction through standard deep neural network layers is not guaranteed to maintain the distance information necessary for Gaussian process models. We build on previous work by comparing the use of the singular value decomposition against a spectral-normalized dense layer as a feature extractor for a deep neural Gaussian process approximation model and apply it to a capacitance prediction problem for the High Voltage Converter Modulators in the Oak Ridge Spallation Neutron Source. Our model shows improved distance preservation and predicts in-distribution capacitance values with less than 1% error.
</details>
<details>
<summary>摘要</summary>
提供准确的不确定性估计是机器学习模型生成可靠性的关键，特别是在安全关键应用中，如加速器系统。高斯过程模型通常被视为金标准方法，但它们可能在大量、高维度数据集上表现不佳。将深度神经网络与高斯过程approximation技术结合使用已经显示出了有希望的结果，但通过标准的深度神经网络层进行维度减少并不一定能保持高斯过程模型所需的距离信息。我们基于先前的工作，比较使用对快速特征EXTRACTOR的singular value decomposition和spectral-normalized dense layer作为深度神经网络 Gaussian process approximation模型的特征提取器，并应用于高电压转换模ulators在Oak Ridge Spallation Neutron Source中的电容量预测问题。我们的模型表现出了改善的距离保持和预测在distribution中的电容量值，误差低于1%。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Laws-Do-Not-Scale"><a href="#Scaling-Laws-Do-Not-Scale" class="headerlink" title="Scaling Laws Do Not Scale"></a>Scaling Laws Do Not Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03201">http://arxiv.org/abs/2307.03201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MarkipTheMudkip/in-class-project-2">https://github.com/MarkipTheMudkip/in-class-project-2</a></li>
<li>paper_authors: Fernando Diaz, Michael Madaio<br>for: This paper argues that the performance of large AI models may not continue to improve as datasets get larger, as different communities represented in the dataset may have values or preferences not captured by the metrics used to evaluate model performance.methods: The paper highlights the potential risks of using scaling laws to evaluate the performance of AI models, as these laws overlook the possibility that different communities may have different values or preferences.results: The paper suggests that as datasets used to train large AI models grow, the number of distinct communities included in the dataset is likely to increase, and these communities may have different values or preferences that are not captured by the metrics used to evaluate model performance.<details>
<summary>Abstract</summary>
Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or preferences not captured by (or in the worst case, at odds with) the metrics used to evaluate model performance for scaling laws. We end the paper with implications for AI scaling laws -- that models may not, in fact, continue to improve as the datasets get larger -- at least not for all people or communities impacted by those models.
</details>
<details>
<summary>摘要</summary>
近期的研究已经提出了一种力学关系，称为“扩大法律”，表明人工智能（AI）模型的性能和模型设计参数之间存在线性关系。即随着数据集大小（或模型参数等）的增加，使用该数据集训练的模型的性能会随之增加。然而，这个扩大法律关系忽视了评估模型性能的指标可能是不安定的、争议的或者不符合不同群体的评价标准。在这篇论文中，我们 argueThat as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or preferences not captured by (or in the worst case, at odds with) the metrics used to evaluate model performance for scaling laws. We end the paper with implications for AI scaling laws -- that models may not, in fact, continue to improve as the datasets get larger -- at least not for all people or communities impacted by those models.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other countries.
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Data-Governance-as-Part-of-a-Data-Mesh-Platform-Concepts-and-Approaches"><a href="#Decentralized-Data-Governance-as-Part-of-a-Data-Mesh-Platform-Concepts-and-Approaches" class="headerlink" title="Decentralized Data Governance as Part of a Data Mesh Platform: Concepts and Approaches"></a>Decentralized Data Governance as Part of a Data Mesh Platform: Concepts and Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02357">http://arxiv.org/abs/2307.02357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arif Wider, Sumedha Verma, Atif Akhtar</li>
<li>for: 本文旨在提供一个概念模型，用于解决数据网络（Data Mesh）的数据管理问题。</li>
<li>methods: 本文使用自动化技术，以提供一个自助数据基础设施平台，以便有效管理数据网络。</li>
<li>results: 本文提出了一种数据网络管理模型，并讨论了如何通过平台方式实现数据管理的自治。<details>
<summary>Abstract</summary>
Data mesh is a socio-technical approach to decentralized analytics data management. To manage this decentralization efficiently, data mesh relies on automation provided by a self-service data infrastructure platform. A key aspect of this platform is to enable decentralized data governance. Because data mesh is a young approach, there is a lack of coherence in how data mesh concepts are interpreted in the industry, and almost no work on how a data mesh platform facilitates governance. This paper presents a conceptual model of key data mesh concepts and discusses different approaches to drive governance through platform means. The insights presented are drawn from concrete experiences of implementing a fully-functional data mesh platform that can be used as a reference on how to approach data mesh platform development.
</details>
<details>
<summary>摘要</summary>
“数据网”是一种社会技术方法，用于分布式数据分析数据管理。为了有效地管理这种分布，数据网利用自助数据基础设施平台的自动化。该平台的一个关键方面是实现分布式数据管理。由于数据网是一种新的方法，因此在行业中对数据网概念的解释存在一定的不一致，而且对于平台如何通过平台手段进行管理的研究几乎缺乏。本文提出了一个概念模型，描述了不同的数据网平台管理方法，并从实际实施了一个可用的数据网平台 refer to as a reference for how to approach data mesh platform development.Here's a word-for-word translation of the text:“数据网”是一种社会技术方法，用于分布式数据分析数据管理。为了有效地管理这种分布，数据网利用自助数据基础设施平台的自动化。该平台的一个关键方面是实现分布式数据管理。由于数据网是一种新的方法，因此在行业中对数据网概念的解释存在一定的不一致，而且对于平台如何通过平台手段进行管理的研究几乎缺乏。本文提出了一个概念模型，描述了不同的数据网平台管理方法，并从实际实施了一个可用的数据网平台 refer to as a reference for how to approach data mesh platform development.
</details></li>
</ul>
<hr>
<h2 id="LLQL-Logistic-Likelihood-Q-Learning-for-Reinforcement-Learning"><a href="#LLQL-Logistic-Likelihood-Q-Learning-for-Reinforcement-Learning" class="headerlink" title="LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning"></a>LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02345">http://arxiv.org/abs/2307.02345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Outongyi Lv, Bingxin Zhou, Yu Guang Wang</li>
<li>for: 这个研究的目的是分析在线和离线RL中bellman错误的分布特性。</li>
<li>methods: 这个研究使用了分布统计学方法来分析bellman错误的分布特性，并基于这些分布特性来改进MSELoss和LLoss两种损失函数。</li>
<li>results: 研究发现在线环境中bellman错误遵循Logistic分布，而离线环境中bellman错误遵循受限的Logistic分布，这个受限分布与离线数据集中的先前策略相关。这些发现导致了改进MSELoss的假设，并使用Logistic最大 LIKElihood函数来构建LLoss作为替代损失函数。此外，研究还发现在离线数据集中，奖励应该遵循特定的分布，以便实现离线目标。在实验中，研究对Soft-Actor-Critic的两种变体在线和离线环境中进行了控制变量修正。结果证实了我们在线和离线设定中的假设，并发现LLoss的方差小于MSELoss。<details>
<summary>Abstract</summary>
Currently, research on Reinforcement learning (RL) can be broadly classified into two categories: online RL and offline RL. Both in online and offline RL, the primary focus of research on the Bellman error lies in the optimization techniques and performance improvement, rather than exploring the inherent structural properties of the Bellman error, such as distribution characteristics. In this study, we analyze the distribution of the Bellman approximation error in both online and offline settings. We find that in the online environment, the Bellman error follows a Logistic distribution, while in the offline environment, the Bellman error follows a constrained Logistic distribution, where the constrained distribution is dependent on the prior policy in the offline data set. Based on this finding, we have improved the MSELoss which is based on the assumption that the Bellman errors follow a normal distribution, and we utilized the Logistic maximum likelihood function to construct $\rm LLoss$ as an alternative loss function. In addition, we observed that the rewards in the offline data set should follow a specific distribution, which would facilitate the achievement of offline objectives. In our numerical experiments, we performed controlled variable corrections on the loss functions of two variants of Soft-Actor-Critic in both online and offline environments. The results confirmed our hypothesis regarding the online and offline settings, we also found that the variance of LLoss is smaller than MSELoss. Our research provides valuable insights for further investigations based on the distribution of Bellman errors.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:现在， reinforcement learning（RL）研究可以分为两类：在线RL和离线RL。在线和离线RL中，bellman错误的主要研究方向都是优化技术和性能提高，而不是探索bellman错误的内在结构特性，如分布特性。在这项研究中，我们分析了在线和离线设置中bellman预测错误的分布。我们发现在线环境中，bellman错误遵循Logistic分布，而在离线环境中，bellman错误遵循受限Logistic分布，其受限分布取决于离线数据集中的先前策略。根据这一发现，我们改进了基于bellman错误预测的MSE损失，并利用Logistic最大likely函数来构建LLoss作为替代损失函数。此外，我们发现离线数据集中的奖励应该遵循特定的分布，以便实现离线目标。在我们的数值实验中，我们对Soft-Actor-Critic两种变体的损失函数进行了在线和离线环境中的变量修正。结果证明了我们在线和离线设置中的假设，并发现LLoss的方差比MSE损失更小。我们的研究提供了有价值的发现，用于进一步基于bellman错误分布的调查。
</details></li>
</ul>
<hr>
<h2 id="FAM-Relative-Flatness-Aware-Minimization"><a href="#FAM-Relative-Flatness-Aware-Minimization" class="headerlink" title="FAM: Relative Flatness Aware Minimization"></a>FAM: Relative Flatness Aware Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02337">http://arxiv.org/abs/2307.02337</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kampmichael/RelativeFlatnessAndGeneralization">https://github.com/kampmichael/RelativeFlatnessAndGeneralization</a></li>
<li>paper_authors: Linara Adilova, Amr Abourayya, Jianning Li, Amin Dada, Henning Petzka, Jan Egger, Jens Kleesiek, Michael Kamp</li>
<li>for: 提高模型的泛化能力</li>
<li>methods: 使用 relative flatness 度量，并通过一个简单的正则化项来优化</li>
<li>results: 在多种应用和模型中，FAM 可以提高模型的泛化性能，并且可以在标准训练和finetuning中使用<details>
<summary>Abstract</summary>
Flatness of the loss curve around a model at hand has been shown to empirically correlate with its generalization ability. Optimizing for flatness has been proposed as early as 1994 by Hochreiter and Schmidthuber, and was followed by more recent successful sharpness-aware optimization techniques. Their widespread adoption in practice, though, is dubious because of the lack of theoretically grounded connection between flatness and generalization, in particular in light of the reparameterization curse - certain reparameterizations of a neural network change most flatness measures but do not change generalization. Recent theoretical work suggests that a particular relative flatness measure can be connected to generalization and solves the reparameterization curse. In this paper, we derive a regularizer based on this relative flatness that is easy to compute, fast, efficient, and works with arbitrary loss functions. It requires computing the Hessian only of a single layer of the network, which makes it applicable to large neural networks, and with it avoids an expensive mapping of the loss surface in the vicinity of the model. In an extensive empirical evaluation we show that this relative flatness aware minimization (FAM) improves generalization in a multitude of applications and models, both in finetuning and standard training. We make the code available at github.
</details>
<details>
<summary>摘要</summary>
几乎所有的模型都会在某些特定的输入上 exhibit 非常好的特性，但这并不意味着它们会在整体上具有更好的泛化能力。在1994年，豪克雷特和Schmid哈姆布尔首先提出了优化flatness的想法，然后是更加成功的锐度感知优化技术。尽管它们在实践中的普及度不高，因为没有理论上的基础连接flatness和泛化，尤其是在考虑到映射渐尘的问题。最近的理论工作表明，一种特定的相对flatness度量可以与泛化相关，并且解决了映射渐尘的问题。在这篇论文中，我们 derivates一种基于这种相对flatness度量的正则化器，它容易计算、快速、高效、可以与任何损失函数结合使用。它只需计算单个层的梯度，因此可以应用于大型神经网络，而且不需要在损失函数的附近进行昂贵的映射。在广泛的实验中，我们证明了这种相对flatness感知优化（FAM）可以提高多种应用和模型的泛化能力，包括finetuning和标准训练。我们在github上提供了代码。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Predictive-Latency-for-5G-A-Theoretical-and-Experimental-Analysis-Using-Network-Measurements"><a href="#Data-driven-Predictive-Latency-for-5G-A-Theoretical-and-Experimental-Analysis-Using-Network-Measurements" class="headerlink" title="Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements"></a>Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02329">http://arxiv.org/abs/2307.02329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Skocaj, Francesca Conserva, Nicol Sarcone Grande, Andrea Orsi, Davide Micheli, Giorgio Ghinamo, Simone Bizzarri, Roberto Verdone</li>
<li>for: This paper is written to analyze predictive latency within 5G networks using real-world network data.</li>
<li>methods: The paper uses an analytical formulation of user-plane latency as a Hypoexponential distribution, and conducts experimental results of probabilistic regression, anomaly detection, and predictive forecasting using Machine Learning (ML) techniques such as Bayesian Learning (BL) and Machine Learning on Graphs (GML).</li>
<li>results: The paper provides valuable insights into the efficacy of predictive algorithms in practical applications, and validates the proposed framework using data gathered from scenarios of vehicular mobility, dense-urban traffic, and social gathering events.<details>
<summary>Abstract</summary>
The advent of novel 5G services and applications with binding latency requirements and guaranteed Quality of Service (QoS) hastened the need to incorporate autonomous and proactive decision-making in network management procedures. The objective of our study is to provide a thorough analysis of predictive latency within 5G networks by utilizing real-world network data that is accessible to mobile network operators (MNOs). In particular, (i) we present an analytical formulation of the user-plane latency as a Hypoexponential distribution, which is validated by means of a comparative analysis with empirical measurements, and (ii) we conduct experimental results of probabilistic regression, anomaly detection, and predictive forecasting leveraging on emerging domains in Machine Learning (ML), such as Bayesian Learning (BL) and Machine Learning on Graphs (GML). We test our predictive framework using data gathered from scenarios of vehicular mobility, dense-urban traffic, and social gathering events. Our results provide valuable insights into the efficacy of predictive algorithms in practical applications.
</details>
<details>
<summary>摘要</summary>
五代新服务和应用程序的紧耦合延迟和质量服务（QoS）的出现加剧了网络管理过程中的自主和积极决策的需求。我们的研究目标是对5G网络中的预测延迟进行全面分析，并使用可达到移动网络运营商（MNOs）的实际网络数据进行验证。具体来说，我们：(i) 提出了用户层延迟的分布式 Hypoexponential 分布，并通过对实际测量数据进行比较分析来验证其有效性。(ii) 通过使用 emerging 领域的机器学习（ML）技术，如 bayesian 学习（BL）和机器学习在图上（GML），进行可预测性的回归、异常检测和预测预测，并对各种场景进行测试，包括交通堵塞、都市化交通和社交聚会等。我们的结果提供了实用应用中预测算法的有用信息。
</details></li>
</ul>
<hr>
<h2 id="Exploring-new-ways-Enforcing-representational-dissimilarity-to-learn-new-features-and-reduce-error-consistency"><a href="#Exploring-new-ways-Enforcing-representational-dissimilarity-to-learn-new-features-and-reduce-error-consistency" class="headerlink" title="Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency"></a>Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02516">http://arxiv.org/abs/2307.02516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tassilo Wald, Constantin Ulrich, Fabian Isensee, David Zimmerer, Gregor Koehler, Michael Baumgartner, Klaus H. Maier-Hein</li>
<li>for: 提高模型 ensemble 的准确率</li>
<li>methods: 利用 representational similarity field 方法来促进模型之间的不同性 durante 训练</li>
<li>results: 实现了更高的 ensemble 准确率，并且Output predictions 之间更加不相关，从而降低了模型 ensemble 的共同失败模式<details>
<summary>Abstract</summary>
Independently trained machine learning models tend to learn similar features. Given an ensemble of independently trained models, this results in correlated predictions and common failure modes. Previous attempts focusing on decorrelation of output predictions or logits yielded mixed results, particularly due to their reduction in model accuracy caused by conflicting optimization objectives. In this paper, we propose the novel idea of utilizing methods of the representational similarity field to promote dissimilarity during training instead of measuring similarity of trained models. To this end, we promote intermediate representations to be dissimilar at different depths between architectures, with the goal of learning robust ensembles with disjoint failure modes. We show that highly dissimilar intermediate representations result in less correlated output predictions and slightly lower error consistency, resulting in higher ensemble accuracy. With this, we shine first light on the connection between intermediate representations and their impact on the output predictions.
</details>
<details>
<summary>摘要</summary>
设置语言为简化中文。<</SYS>>独立训练的机器学习模型通常会学习类似的特征。给出一个独立训练的模型集合，这会导致相互相关的预测和共同失败模式。过去关注输出预测或搜索阶段的减 corr 的尝试已经获得了杂合的结果，特别是因为它们在优化目标之间发生了矛盾。在这篇论文中，我们提出了一种新的想法，利用表示相似场来促进训练期间的不同性。为此，我们将不同深度的建筑物 promoted 到不同的中间表示，以达到学习强大的集成模型，并且学习不同的失败模式。我们发现，在不同的中间表示下，输出预测之间的相互关系较强，并且 ensemble 精度较高。我们首次探讨了中间表示如何影响输出预测的问题。
</details></li>
</ul>
<hr>
<h2 id="LOB-Based-Deep-Learning-Models-for-Stock-Price-Trend-Prediction-A-Benchmark-Study"><a href="#LOB-Based-Deep-Learning-Models-for-Stock-Price-Trend-Prediction-A-Benchmark-Study" class="headerlink" title="LOB-Based Deep Learning Models for Stock Price Trend Prediction: A Benchmark Study"></a>LOB-Based Deep Learning Models for Stock Price Trend Prediction: A Benchmark Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01915">http://arxiv.org/abs/2308.01915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Prata, Giuseppe Masi, Leonardo Berti, Viviana Arrigoni, Andrea Coletta, Irene Cannistraci, Svitlana Vyetrenko, Paola Velardi, Novella Bartolini</li>
<li>for: 股票价格趋势预测（SPTP）基于限价书（LOB）数据的深度学习模型的可靠性和泛化性研究。</li>
<li>methods: 我们开发了一个名为LOBCAST的开源框架，它包括数据准备、深度学习模型训练、评估和收益分析。</li>
<li>results: 我们的广泛实验发现所有模型在新数据上表现糟糕，这引发了市场应用性的问题。我们的工作作为一个标准，暴露了当前方法的潜在和局限性，并提供了创新解决方案的视野。<details>
<summary>Abstract</summary>
The recent advancements in Deep Learning (DL) research have notably influenced the finance sector. We examine the robustness and generalizability of fifteen state-of-the-art DL models focusing on Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data. To carry out this study, we developed LOBCAST, an open-source framework that incorporates data preprocessing, DL model training, evaluation and profit analysis. Our extensive experiments reveal that all models exhibit a significant performance drop when exposed to new data, thereby raising questions about their real-world market applicability. Our work serves as a benchmark, illuminating the potential and the limitations of current approaches and providing insight for innovative solutions.
</details>
<details>
<summary>摘要</summary>
现代深度学习（DL）研究的进步已经很大程度上影响了金融领域。我们对十五种当前最佳DL模型进行了评估，以便对使用Limit Order Book（LOB）数据进行股票价格趋势预测（SPTP）。为了实现这项研究，我们开发了一个名为LOBCAST的开源框架，该框架包括数据处理、DL模型训练、评估和利润分析。我们的广泛实验表明，所有模型在新数据上表现出了显著的性能下降，这引发了market应用实际性的问题。我们的工作作为一个 referential，探讨了当前approach的潜力和局限性，并提供了创新解决方案的思路。
</details></li>
</ul>
<hr>
<h2 id="Deep-Contract-Design-via-Discontinuous-Piecewise-Affine-Neural-Networks"><a href="#Deep-Contract-Design-via-Discontinuous-Piecewise-Affine-Neural-Networks" class="headerlink" title="Deep Contract Design via Discontinuous Piecewise Affine Neural Networks"></a>Deep Contract Design via Discontinuous Piecewise Affine Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02318">http://arxiv.org/abs/2307.02318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tonghan Wang, Paul Dütting, Dmitry Ivanov, Inbal Talgam-Cohen, David C. Parkes</li>
<li>for: 这篇论文是为了研究深度学习在合同设计中的应用。</li>
<li>methods: 这篇论文使用了深度学习来自动设计优化的合同。它使用了一种新的表示方法——Discontinuous ReLU（DeLU）网络，来模型主体的利益函数。</li>
<li>results: 实验结果表明，使用DeLU网络可以 успеш地近似主体的利益函数，并且可以使用少量的训练样本和内部点法来解决优化问题。<details>
<summary>Abstract</summary>
Contract design involves a principal who establishes contractual agreements about payments for outcomes that arise from the actions of an agent. In this paper, we initiate the study of deep learning for the automated design of optimal contracts. We formulate this as an offline learning problem, where a deep network is used to represent the principal's expected utility as a function of the design of a contract. We introduce a novel representation: the Discontinuous ReLU (DeLU) network, which models the principal's utility as a discontinuous piecewise affine function where each piece corresponds to the agent taking a particular action. DeLU networks implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal, and support parallel inference on each piece through linear programming or interior-point methods that solve for optimal contracts. We provide empirical results that demonstrate success in approximating the principal's utility function with a small number of training samples and scaling to find approximately optimal contracts on problems with a large number of actions and outcomes.
</details>
<details>
<summary>摘要</summary>
（本文研究了深度学习在合同设计中的应用，具体来说是设计优化的合同。我们将这视为一个离线学习问题，其中深度网络用于表示主人的预期用处函数，并且引入了一种新的表示方法：离散ReLU（DeLU）网络。DeLU网络模型了主人的用处函数为离散的piecewise afine函数，每个piece对应代理人行动的不同。DeLU网络隐式学习了代理人的奖励一致约束和主人的用处最大化目标，并且支持并行推理每个piece的linear programming或内部点法解决优化合同。我们提供了实验结果，证明了使用少量训练样本和扩展可以successfully approximate主人的用处函数，并且可以扩展到解决包含大量行动和结果的问题）。
</details></li>
</ul>
<hr>
<h2 id="Sumformer-Universal-Approximation-for-Efficient-Transformers"><a href="#Sumformer-Universal-Approximation-for-Efficient-Transformers" class="headerlink" title="Sumformer: Universal Approximation for Efficient Transformers"></a>Sumformer: Universal Approximation for Efficient Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02301">http://arxiv.org/abs/2307.02301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silas Alberti, Niclas Dern, Laura Thesing, Gitta Kutyniok</li>
<li>for: 这篇论文主要针对的是提出一种新的嵌入式序列处理模型，以解决Transformer模型的时间和空间复杂度问题。</li>
<li>methods: 该论文使用了Linformer和Performer模型，并通过一种新的概念——Sumformer来实现对这些模型的 universally approximating。</li>
<li>results: 该论文通过实验和理论分析表明，Sumformer模型可以成功地实现对Linformer和Performer模型的universal approximation，并且提供了一个新的证明，证明Transformer模型只需要一层注意力层可以实现universal approximation。<details>
<summary>Abstract</summary>
Natural language processing (NLP) made an impressive jump with the introduction of Transformers. ChatGPT is one of the most famous examples, changing the perception of the possibilities of AI even outside the research community. However, besides the impressive performance, the quadratic time and space complexity of Transformers with respect to sequence length pose significant limitations for handling long sequences. While efficient Transformer architectures like Linformer and Performer with linear complexity have emerged as promising solutions, their theoretical understanding remains limited. In this paper, we introduce Sumformer, a novel and simple architecture capable of universally approximating equivariant sequence-to-sequence functions. We use Sumformer to give the first universal approximation results for Linformer and Performer. Moreover, we derive a new proof for Transformers, showing that just one attention layer is sufficient for universal approximation.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）在Transformers的出现后得到了非常出色的进步。ChatGPT是其中最著名的例子，对外部研究者的认知产生了深刻的影响。然而，Transformers的时间和空间复杂度随序列长度的增加平方速度增长，对于处理长序列存在重大的限制。虽然有效的Transformers架构如Linformer和Performer已经出现，但它们的理论理解仍然受限。在这篇论文中，我们介绍了Sumformer，一种新的简单架构，可以通用地近似Equivariant sequence-to-sequence函数。我们使用Sumformer来给Linformer和Performer提供首个通用近似结果。此外，我们还 deriv了一个新的证明，显示Transformers只需一层注意力层就能universal approximation。
</details></li>
</ul>
<hr>
<h2 id="Improving-Address-Matching-using-Siamese-Transformer-Networks"><a href="#Improving-Address-Matching-using-Siamese-Transformer-Networks" class="headerlink" title="Improving Address Matching using Siamese Transformer Networks"></a>Improving Address Matching using Siamese Transformer Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02300">http://arxiv.org/abs/2307.02300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/avduarte333/adress-matching">https://github.com/avduarte333/adress-matching</a></li>
<li>paper_authors: André V. Duarte, Arlindo L. Oliveira</li>
<li>for: 提高葡萄牙地址匹配效率，降低错误交付的风险</li>
<li>methods: 使用深度学习模型，包括bi-encoder和cross-encoder，对葡萄牙邮政地址进行嵌入表示，并进行高精度排名</li>
<li>results: 测试用例显示，模型在葡萄牙邮政地址上达到了95%以上的高精度率，并且使用GPU计算可以提高推理速度，比传统方法快4.5倍<details>
<summary>Abstract</summary>
Matching addresses is a critical task for companies and post offices involved in the processing and delivery of packages. The ramifications of incorrectly delivering a package to the wrong recipient are numerous, ranging from harm to the company's reputation to economic and environmental costs. This research introduces a deep learning-based model designed to increase the efficiency of address matching for Portuguese addresses. The model comprises two parts: (i) a bi-encoder, which is fine-tuned to create meaningful embeddings of Portuguese postal addresses, utilized to retrieve the top 10 likely matches of the un-normalized target address from a normalized database, and (ii) a cross-encoder, which is fine-tuned to accurately rerank the 10 addresses obtained by the bi-encoder. The model has been tested on a real-case scenario of Portuguese addresses and exhibits a high degree of accuracy, exceeding 95% at the door level. When utilized with GPU computations, the inference speed is about 4.5 times quicker than other traditional approaches such as BM25. An implementation of this system in a real-world scenario would substantially increase the effectiveness of the distribution process. Such an implementation is currently under investigation.
</details>
<details>
<summary>摘要</summary>
企业和邮政机构在包裹处理和交付过程中，匹配地址是一项非常重要的任务。 incorrect 地址交付可能会对公司的名誉产生负面影响，并且可能会导致经济和环境损失。这项研究推出了一种基于深度学习的地址匹配模型，用于提高葡萄牙地址的匹配效率。该模型包括两部分：1. 双编码器，通过细化葡萄牙邮政地址来生成有意义的嵌入。使用这些嵌入来从normalized数据库中检索最有可能性的10个目标地址。2. 混合编码器，通过细化10个从双编码器获取的地址来准确地排名它们。这个模型在葡萄牙地址的实际案例中进行测试，表现出了高度的准确率，超过95%的门槛。在使用GPU计算时，推理速度比传统方法 such as BM25 快得多，约为4.5倍。在实际应用场景中，这种系统的实施将有效地提高物流过程的效率。现在正在进行实际应用研究。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-Adversarial-Bandit-Algorithms"><a href="#Meta-Learning-Adversarial-Bandit-Algorithms" class="headerlink" title="Meta-Learning Adversarial Bandit Algorithms"></a>Meta-Learning Adversarial Bandit Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02295">http://arxiv.org/abs/2307.02295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Khodak, Ilya Osadchiy, Keegan Harris, Maria-Florina Balcan, Kfir Y. Levy, Ron Meir, Zhiwei Steven Wu</li>
<li>for: 本研究旨在提高多个任务的性能，假设这些任务之间存在自然的相似度测试。</li>
<li>methods: 我们设计了meta算法，用于同时调整内部学习器的初始化和其他超参数。对于多抢拍机（MAB）和bandit线性优化（BLO）两个重要情况，我们设计了外部学习器来同时调整初始化和超参数。</li>
<li>results: 我们证明了在不Regularization的follow-the-leader combinated with two levels of low-dimensional hyperparameter tuning是可以学习一个序列的Affine函数， bounding the regret of online mirror descent（OMD）。<details>
<summary>Abstract</summary>
We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two levels of low-dimensional hyperparameter tuning is enough to learn a sequence of affine functions of non-Lipschitz and sometimes non-convex Bregman divergences bounding the regret of OMD.
</details>
<details>
<summary>摘要</summary>
我们研究在线元学习，使得在多个任务之间提高性能，如果这些任务具有自然的相似度度量。作为首先针对在线内部部分信息设定下进行反恶力学学习的研究，我们设计了元学习算法，将外层学习器与内层学习器的初始化和其他超参数同时调整。对于多枪炮（MAB）和反馈函数优化（BLO）两个重要的案例，我们的元学习算法都可以达到良好的性能。对于 MAB，我们使用 Tsallis-Entropy 泛化 Exp3，并证明在任务平均误差小时，元学习器可以提高性能。对于 BLO，我们学习在线镜像下降（OMD）的初始化和调整，并证明任务平均误差与行动空间依赖的度量直接相关。我们的保证基于证明，不带权重的跟随者，加上两级低维度超参数调整，可以学习一个 sequence of affine function， bounding OMD 的误差。
</details></li>
</ul>
<hr>
<h2 id="Absorbing-Phase-Transitions-in-Artificial-Deep-Neural-Networks"><a href="#Absorbing-Phase-Transitions-in-Artificial-Deep-Neural-Networks" class="headerlink" title="Absorbing Phase Transitions in Artificial Deep Neural Networks"></a>Absorbing Phase Transitions in Artificial Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02284">http://arxiv.org/abs/2307.02284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keiichi Tamai, Tsuyoshi Okubo, Truong Vinh Truong Duy, Naotake Natori, Synge Todo</li>
<li>for: 了解抽象神经网络的行为</li>
<li>methods: 使用 mean-field theory 和 absorbing phase transitions 的研究方法</li>
<li>results: 显示了抽象神经网络的行为可以通过critical phenomena来理解，并且 Architecture 的不同会影响transition的类型。In simpler Chinese:</li>
<li>for: 了解神经网络的行为</li>
<li>methods: 使用 mean-field theory 和 absorbing phase transitions 的方法</li>
<li>results: 发现神经网络的行为可以通过critical phenomena来理解，而且不同的 Architecture 会影响transition的类型。<details>
<summary>Abstract</summary>
Theoretical understanding of the behavior of infinitely-wide neural networks has been rapidly developed for various architectures due to the celebrated mean-field theory. However, there is a lack of a clear, intuitive framework for extending our understanding to finite networks that are of more practical and realistic importance. In the present contribution, we demonstrate that the behavior of properly initialized neural networks can be understood in terms of universal critical phenomena in absorbing phase transitions. More specifically, we study the order-to-chaos transition in the fully-connected feedforward neural networks and the convolutional ones to show that (i) there is a well-defined transition from the ordered state to the chaotics state even for the finite networks, and (ii) difference in architecture is reflected in that of the universality class of the transition. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenological argument could lead us to semi-quantitative description of the signal propagation dynamics.
</details>
<details>
<summary>摘要</summary>
theoretical understanding of the behavior of infinitely-wide neural networks has been rapidly developed for various architectures due to the celebrated mean-field theory. However, there is a lack of a clear, intuitive framework for extending our understanding to finite networks that are of more practical and realistic importance. In the present contribution, we demonstrate that the behavior of properly initialized neural networks can be understood in terms of universal critical phenomena in absorbing phase transitions. More specifically, we study the order-to-chaos transition in the fully-connected feedforward neural networks and the convolutional ones to show that (i) there is a well-defined transition from the ordered state to the chaotics state even for the finite networks, and (ii) difference in architecture is reflected in that of the universality class of the transition. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenological argument could lead us to semi-quantitative description of the signal propagation dynamics.Here's the translation breakdown:*  theoretically understanding (理论理解) = 理论理解 (Simplified Chinese)*  has been rapidly developed (has been rapidly developed) = 快速发展 (Simplified Chinese)*  for various architectures (for various architectures) =  для多种架构 (Simplified Chinese)*  due to the celebrated mean-field theory (due to the celebrated mean-field theory) = 因为著名的平均场理论 (Simplified Chinese)*  However, there is a lack of (However, there is a lack of) = 然而，缺乏 (Simplified Chinese)*  a clear, intuitive framework (a clear, intuitive framework) = 一个明确、直观的框架 (Simplified Chinese)*  for extending our understanding (for extending our understanding) = 以便扩展我们的理解 (Simplified Chinese)*  to finite networks (to finite networks) = 到有限网络 (Simplified Chinese)*  that are of more practical and realistic importance (that are of more practical and realistic importance) = 更加实用和现实重要的网络 (Simplified Chinese)*  In the present contribution (In the present contribution) = 在当前的贡献中 (Simplified Chinese)*  we demonstrate (we demonstrate) = 我们展示 (Simplified Chinese)*  that the behavior (that the behavior) = 网络的行为 (Simplified Chinese)*  of properly initialized neural networks (of properly initialized neural networks) = 初始化过的神经网络的行为 (Simplified Chinese)*  can be understood (can be understood) = 可以理解 (Simplified Chinese)*  in terms of universal critical phenomena (in terms of universal critical phenomena) = 以 Kritical 现象的形式来理解 (Simplified Chinese)*  in absorbing phase transitions (in absorbing phase transitions) = 在吸收相对 equilibria 阶段中 (Simplified Chinese)*  More specifically (More specifically) = 更加细致 (Simplified Chinese)*  we study (we study) = 我们研究 (Simplified Chinese)*  the order-to-chaos transition (the order-to-chaos transition) = 从有序到无序的转变 (Simplified Chinese)*  in the fully-connected feedforward neural networks (in the fully-connected feedforward neural networks) = 在完全连接的前向神经网络中 (Simplified Chinese)*  and the convolutional ones (and the convolutional ones) = 以及卷积神经网络 (Simplified Chinese)*  to show (to show) = 以示 (Simplified Chinese)*  that (i) there is a well-defined transition (that (i) there is a well-defined transition) = 显示 (i) 存在一个明确的转变 (Simplified Chinese)*  from the ordered state (from the ordered state) = 从有序状态 (Simplified Chinese)*  to the chaotics state (to the chaotics state) = 到无序状态 (Simplified Chinese)*  even for the finite networks (even for the finite networks) = 甚至 для有限网络 (Simplified Chinese)*  and (ii) difference in architecture (and (ii) difference in architecture) = 以及 (ii) 不同的架构 (Simplified Chinese)*  is reflected in that of the universality class (is reflected in that of the universality class) = 在不同的架构中反映出的 универсалитет 类型 (Simplified Chinese)*  Remarkably (Remarkably) = 很有趣 (Simplified Chinese)*  the finite-size scaling (the finite-size scaling) = 有限大小缩放 (Simplified Chinese)*  can also be successfully applied (can also be successfully applied) = 也可以成功应用 (Simplified Chinese)*  indicating (indicating) = 表明 (Simplified Chinese)*  that (that) = 的 (Simplified Chinese)*  intuitive phenomenological argument (intuitive phenomenological argument) = 直观的现象学 argue (Simplified Chinese)*  could lead us to (could lead us to) = 可以导我们到 (Simplified Chinese)*  semi-quantitative description (semi-quantitative description) = 半量化的描述 (Simplified Chinese)*  of the signal propagation dynamics (of the signal propagation dynamics) = 信号传播动态的描述 (Simplified Chinese)
</details></li>
</ul>
<hr>
<h2 id="From-NeurODEs-to-AutoencODEs-a-mean-field-control-framework-for-width-varying-Neural-Networks"><a href="#From-NeurODEs-to-AutoencODEs-a-mean-field-control-framework-for-width-varying-Neural-Networks" class="headerlink" title="From NeurODEs to AutoencODEs: a mean-field control framework for width-varying Neural Networks"></a>From NeurODEs to AutoencODEs: a mean-field control framework for width-varying Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02279">http://arxiv.org/abs/2307.02279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cristina Cipriani, Massimo Fornasier, Alessandro Scagliotti</li>
<li>for: This paper aims to extend the mean-field control framework for continuous-time Autoencoders (AutoencODEs) to handle low Tikhonov regularization and potentially non-convex cost landscapes.</li>
<li>methods: The paper proposes a modification of the controlled field in the AutoencODE to enable the extension of the mean-field control framework, and develops a training method tailored to this specific type of Autoencoders with residual connections.</li>
<li>results: The paper shows that many of the global results obtained for high Tikhonov regularization can be recovered in regions where the loss function is locally convex, and validates the approach through numerical experiments conducted on various examples.<details>
<summary>Abstract</summary>
The connection between Residual Neural Networks (ResNets) and continuous-time control systems (known as NeurODEs) has led to a mathematical analysis of neural networks which has provided interesting results of both theoretical and practical significance. However, by construction, NeurODEs have been limited to describing constant-width layers, making them unsuitable for modeling deep learning architectures with layers of variable width. In this paper, we propose a continuous-time Autoencoder, which we call AutoencODE, based on a modification of the controlled field that drives the dynamics. This adaptation enables the extension of the mean-field control framework originally devised for conventional NeurODEs. In this setting, we tackle the case of low Tikhonov regularization, resulting in potentially non-convex cost landscapes. While the global results obtained for high Tikhonov regularization may not hold globally, we show that many of them can be recovered in regions where the loss function is locally convex. Inspired by our theoretical findings, we develop a training method tailored to this specific type of Autoencoders with residual connections, and we validate our approach through numerical experiments conducted on various examples.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将神经网络（ResNets）与连续时间控制系统（NeurODEs）的连接，导致了神经网络的数学分析，并且提供了有趣的理论和实践 significado。然而，由于NeurODEs的构造，它们只适用于描述常数宽的层，因此不适用于模型深度学习架构中的层。在这篇论文中，我们提出了一种基于修改控制场的连续时间自编码器，我们称之为AutoencODE。这种修改允许我们将mean-field控制框架原来设计 для传统的NeurODEs应用到这种新的Autoencoder中。在这个设定下，我们研究了低Tikhonov正则化的情况，导致的可能是非对称的成本地图。虽然高Tikhonov正则化的全局结果可能不会全球适用，但我们表明了在成本地图的局部几何上，许多全局结果可以被恢复。受到我们的理论发现的启发，我们开发了针对这种特殊类型的Autoencoders的训练方法，并通过对各种示例进行数学实验验证了我们的方法。
</details></li>
</ul>
<hr>
<h2 id="First-Explore-then-Exploit-Meta-Learning-Intelligent-Exploration"><a href="#First-Explore-then-Exploit-Meta-Learning-Intelligent-Exploration" class="headerlink" title="First-Explore, then Exploit: Meta-Learning Intelligent Exploration"></a>First-Explore, then Exploit: Meta-Learning Intelligent Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02276">http://arxiv.org/abs/2307.02276</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/btnorman/First-Explore">https://github.com/btnorman/First-Explore</a></li>
<li>paper_authors: Ben Norman, Jeff Clune</li>
<li>For: The paper aims to address the issue of intelligent exploration in reinforcement learning (RL) agents, which have been limited by the conflict between exploration and exploitation.* Methods: The proposed First-Explore framework consists of two policies: one for exploration and one for exploitation. The explore policy learns to explore the environment, while the exploit policy learns to exploit the learned knowledge.* Results: The paper demonstrates that First-Explore can learn intelligent exploration strategies such as exhaustive search and outperforms dominant standard RL and meta-RL approaches on domains where exploration requires sacrificing reward.<details>
<summary>Abstract</summary>
Standard reinforcement learning (RL) agents never intelligently explore like a human (i.e. by taking into account complex domain priors and previous explorations). Even the most basic intelligent exploration strategies such as exhaustive search are only inefficiently or poorly approximated by approaches such as novelty search or intrinsic motivation, let alone more complicated strategies like learning new skills, climbing stairs, opening doors, or conducting experiments. This lack of intelligent exploration limits sample efficiency and prevents solving hard exploration domains. We argue a core barrier prohibiting many RL approaches from learning intelligent exploration is that the methods attempt to explore and exploit simultaneously, which harms both exploration and exploitation as the goals often conflict. We propose a novel meta-RL framework (First-Explore) with two policies: one policy learns to only explore and one policy learns to only exploit. Once trained, we can then explore with the explore policy, for as long as desired, and then exploit based on all the information gained during exploration. This approach avoids the conflict of trying to do both exploration and exploitation at once. We demonstrate that First-Explore can learn intelligent exploration strategies such as exhaustive search and more, and that it outperforms dominant standard RL and meta-RL approaches on domains where exploration requires sacrificing reward. First-Explore is a significant step towards creating meta-RL algorithms capable of learning human-level exploration which is essential to solve challenging unseen hard-exploration domains.
</details>
<details>
<summary>摘要</summary>
标准强化学习（RL）代理没有人类智能的探索能力（即考虑复杂领域假设和前一次探索）。 même 最基本的智能探索策略，如探索所有可能性，都是通过方法如新鲜度搜索或内在动机来不够或不准确地 aproximated。这种缺乏智能探索限制了样本效率，阻碍解决困难探索领域。我们认为许多RL方法无法学习智能探索的核心障碍在于这些方法尝试同时探索和利用，这两个目标经常矛盾。我们提出了一种新的元RL框架（First-Explore），其中有两个策略：一个策略学习只探索，另一个策略学习只利用。一旦训练完成，我们可以使用探索策略，探索到心仪的时间，然后根据所获得的信息进行利用。这种方法避免了同时尝试探索和利用的矛盾。我们证明First-Explore可以学习智能探索策略，例如探索所有可能性，并且超过了主流标准RL和元RL方法在需要牺牲奖励的领域中的表现。First-Explore是创造meta-RL算法可以学习人类水平的探索的重要一步，解决了许多未解之探索领域。
</details></li>
</ul>
<hr>
<h2 id="Convolutions-Through-the-Lens-of-Tensor-Networks"><a href="#Convolutions-Through-the-Lens-of-Tensor-Networks" class="headerlink" title="Convolutions Through the Lens of Tensor Networks"></a>Convolutions Through the Lens of Tensor Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02275">http://arxiv.org/abs/2307.02275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Dangel</li>
<li>for: 这篇论文旨在探讨卷积神经网络（TN）如何用于理解卷积层。</li>
<li>methods: 该论文使用了tensor网络（TN）来理解卷积层，并通过绘制图像和执行函数转换、子tensor访问和融合来表示卷积层的含义。</li>
<li>results: 该论文通过对各种自动梯度计算和各种精度估计的图像表示，证明了TN的表达力。此外，该论文还提供了基于连接性模式的卷积特性转换，以便简化和加速图像计算。最后，该论文表明了使用TN实现的计算性能。<details>
<summary>Abstract</summary>
Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the generalization of theoretical and algorithmic ideas. We provide a new perspective onto convolutions through tensor networks (TNs) which allow reasoning about the underlying tensor multiplications by drawing diagrams, and manipulating them to perform function transformations, sub-tensor access, and fusion. We demonstrate this expressive power by deriving the diagrams of various autodiff operations and popular approximations of second-order information with full hyper-parameter support, batching, channel groups, and generalization to arbitrary convolution dimensions. Further, we provide convolution-specific transformations based on the connectivity pattern which allow to re-wire and simplify diagrams before evaluation. Finally, we probe computational performance, relying on established machinery for efficient TN contraction. Our TN implementation speeds up a recently-proposed KFAC variant up to 4.5x and enables new hardware-efficient tensor dropout for approximate backpropagation.
</details>
<details>
<summary>摘要</summary>
尽管卷积有简单的直觉，但它们在理论和算法上的总结和推广是更为复杂的。我们通过tensor网络（TN）提供了一新的视角来理解卷积，这些网络允许我们通过绘制图表和修改它们来实现函数转换、子tensor访问、混合等操作。我们通过示例逻辑来证明TN的表达力，包括自动梯度下降操作和流行的第二个信息近似算法，并支持批处理、通道组、普通的卷积维度等参数。此外，我们还提供了基于卷积连接 patrern的转换，可以重新排列和简化图表之前进行评估。最后，我们评估了TN实现的计算性能，利用现有的高效TN收缩机制。我们的TN实现可以加速一种最近提出的KFAC变种，并启用了新的硬件高效的tensor dropout来实现精确的反propagation。
</details></li>
</ul>
<hr>
<h2 id="Dynamical-Isometry-based-Rigorous-Fair-Neural-Architecture-Search"><a href="#Dynamical-Isometry-based-Rigorous-Fair-Neural-Architecture-Search" class="headerlink" title="Dynamical Isometry based Rigorous Fair Neural Architecture Search"></a>Dynamical Isometry based Rigorous Fair Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02263">http://arxiv.org/abs/2307.02263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianxiang Luo, Junyi Hu, Tianji Pang, Weihao Huang, Chuang Liu</li>
<li>for: 提高神经网络搜索的效率和可解释性，以及 garantizar la justicia en la evaluación de módulos。</li>
<li>methods: 基于动态同异ometry的新型神经网络搜索算法，使用fix point analysis方法对平均场观测随机神经网络的动态行为进行分析，并证明模块选择策略是正见的。</li>
<li>results: 通过对ImageNet分类任务进行广泛的实验，显示了使用提议方法可以在同等大小的神经网络中达到顶尖的top-1验证精度，并且demonstrated that our method can achieve better and more stable training performance without loss of generality。<details>
<summary>Abstract</summary>
Recently, the weight-sharing technique has significantly speeded up the training and evaluation procedure of neural architecture search. However, most existing weight-sharing strategies are solely based on experience or observation, which makes the searching results lack interpretability and rationality. In addition, due to the negligence of fairness, current methods are prone to make misjudgments in module evaluation. To address these problems, we propose a novel neural architecture search algorithm based on dynamical isometry. We use the fix point analysis method in the mean field theory to analyze the dynamics behavior in the steady state random neural network, and how dynamic isometry guarantees the fairness of weight-sharing based NAS. Meanwhile, we prove that our module selection strategy is rigorous fair by estimating the generalization error of all modules with well-conditioned Jacobian. Extensive experiments show that, with the same size, the architecture searched by the proposed method can achieve state-of-the-art top-1 validation accuracy on ImageNet classification. In addition, we demonstrate that our method is able to achieve better and more stable training performance without loss of generality.
</details>
<details>
<summary>摘要</summary>
最近，Weight-sharing技术在神经网络搜索中提高了训练和评估过程的速度。然而，大多数现有的Weight-sharing策略都是基于经验或观察，lack of interpretability和理性性。此外，由于对公平性的忽视，当前的方法容易做出不准确的模块评估。为解决这些问题，我们提出了一种基于动态同尺的神经网络搜索算法。我们使用了 fixes point analysis方法来分析动态同尺在平均场 teor 中的动态行为，并证明了动态同尺的权重分享可以保证公平性。此外，我们证明了我们的模块选择策略是正则公平的，可以通过Jacobian的condition number来估算模块的总体适应性。实验表明，我们的方法可以在ImageNet分类任务中 achievestate-of-the-art的顶部一 validate accuracy，并且可以在不失一般性的前提下提高训练性能。
</details></li>
</ul>
<hr>
<h2 id="Multivariate-Time-Series-Classification-A-Deep-Learning-Approach"><a href="#Multivariate-Time-Series-Classification-A-Deep-Learning-Approach" class="headerlink" title="Multivariate Time Series Classification: A Deep Learning Approach"></a>Multivariate Time Series Classification: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02253">http://arxiv.org/abs/2307.02253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/radrumond/timehetnet">https://github.com/radrumond/timehetnet</a></li>
<li>paper_authors: Mohamed Abouelnaga, Julien Vitay, Aida Farahani</li>
<li>for: 本研究探讨了不同方法和神经网络架构在时间序列分类领域中的可行性。</li>
<li>methods: 本研究使用了 Fully Convolutional Networks (FCN) 和 Long Short-Term Memory (LSTM)  дляsupervised learning，以及 Recurrent Autoencoders  дляsemisupervised learning。</li>
<li>results: 通过分析时间序列数据，研究发现不同参数的影响，并通过精度和准确率等指标评估不同方法的 diferencias，以确定适合这种问题的方法。<details>
<summary>Abstract</summary>
This paper investigates different methods and various neural network architectures applicable in the time series classification domain. The data is obtained from a fleet of gas sensors that measure and track quantities such as oxygen and sound. With the help of this data, we can detect events such as occupancy in a specific environment. At first, we analyze the time series data to understand the effect of different parameters, such as the sequence length, when training our models. These models employ Fully Convolutional Networks (FCN) and Long Short-Term Memory (LSTM) for supervised learning and Recurrent Autoencoders for semisupervised learning. Throughout this study, we spot the differences between these methods based on metrics such as precision and recall identifying which technique best suits this problem.
</details>
<details>
<summary>摘要</summary>
First, we analyze the time series data to understand the impact of different parameters, such as sequence length, when training our models. Our models use Fully Convolutional Networks (FCN) and Long Short-Term Memory (LSTM) for supervised learning, and Recurrent Autoencoders for semisupervised learning.Throughout the study, we compare the performance of these methods based on metrics such as precision and recall, and identify which technique is best suited for this problem.
</details></li>
</ul>
<hr>
<h2 id="RanPAC-Random-Projections-and-Pre-trained-Models-for-Continual-Learning"><a href="#RanPAC-Random-Projections-and-Pre-trained-Models-for-Continual-Learning" class="headerlink" title="RanPAC: Random Projections and Pre-trained Models for Continual Learning"></a>RanPAC: Random Projections and Pre-trained Models for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02251">http://arxiv.org/abs/2307.02251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark D. McDonnell, Dong Gong, Amin Parveneh, Ehsan Abbasnejad, Anton van den Hengel</li>
<li>for: 这个研究是为了解决在非站点数据流中进行逐步学习（Continual Learning，CL）时，不要忘记之前学习的知识。</li>
<li>methods: 这个研究使用了预训模型（pre-trained models），并将其应用到不同的下游需求。它们可以直接使用预训模型的特征（pre-extracted features），或者使用适材化器（adaptors）。但是，这些方法可能会导致忘记现象。因此，这个研究提出了一个简洁有效的方法，通过增加特征之间的互动，提高分类器的线性分类能力，并避免忘记现象。</li>
<li>results: 这个研究发现，透过将固定的Random Projector层加入预训模型的特征表现和出力头，可以增加特征之间的互动，提高分类器的线性分类能力，并避免忘记现象。此外，调整分类器的标本集也可以帮助避免分布差异导致的忘记现象。这些技术在七个类别增量学习 benchmark 测试中，与过去的方法相比，可以大幅降低最终的错误率，并且不需要使用任何复习内存。<details>
<summary>Abstract</summary>
Continual learning (CL) aims to incrementally learn different tasks (such as classification) in a non-stationary data stream without forgetting old ones. Most CL works focus on tackling catastrophic forgetting under a learning-from-scratch paradigm. However, with the increasing prominence of foundation models, pre-trained models equipped with informative representations have become available for various downstream requirements. Several CL methods based on pre-trained models have been explored, either utilizing pre-extracted features directly (which makes bridging distribution gaps challenging) or incorporating adaptors (which may be subject to forgetting). In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Random Projection layer with nonlinear activation between the pre-trained model's feature representations and output head, which captures interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based CL. We also demonstrate the importance of decorrelating the class-prototypes to reduce the distribution disparity when using pre-trained representations. These techniques prove to be effective and circumvent the problem of forgetting for both class- and domain-incremental continual learning. Compared to previous methods applied to pre-trained ViT-B/16 models, we reduce final error rates by between 10\% and 62\% on seven class-incremental benchmark datasets, despite not using any rehearsal memory. We conclude that the full potential of pre-trained models for simple, effective, and fast continual learning has not hitherto been fully tapped.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Random Projection layer with nonlinear activation between the pre-trained model's feature representations and output head, which captures interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based CL. We also demonstrate the importance of decorrelating the class-prototypes to reduce the distribution disparity when using pre-trained representations. These techniques prove to be effective and circumvent the problem of forgetting for both class- and domain-incremental continual learning.Compared to previous methods applied to pre-trained ViT-B/16 models, we reduce final error rates by between 10\% and 62\% on seven class-incremental benchmark datasets, despite not using any rehearsal memory. We conclude that the full potential of pre-trained models for simple, effective, and fast continual learning has not hitherto been fully tapped.
</details></li>
</ul>
<hr>
<h2 id="Set-Learning-for-Accurate-and-Calibrated-Models"><a href="#Set-Learning-for-Accurate-and-Calibrated-Models" class="headerlink" title="Set Learning for Accurate and Calibrated Models"></a>Set Learning for Accurate and Calibrated Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02245">http://arxiv.org/abs/2307.02245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lukasmut/oko">https://github.com/lukasmut/oko</a></li>
<li>paper_authors: Lukas Muttenthaler, Robert A. Vandermeulen, Qiuyi Zhang, Thomas Unterthiner, Klaus-Robert Müller</li>
<li>for: 降低机器学习模型的自信和准确性问题，提高模型的准确率和准确性，特别在有限的训练数据和类别偏挤的情况下。</li>
<li>methods: 提出了一种新的odd-$k$-out learning（OKO）方法，通过将cross-entropy error最小化为集合而不是单个示例，使模型能够捕捉数据示例之间的相关性，并提高准确率和准确性。</li>
<li>results: OKO方法可以在有限的训练数据和类别偏挤的情况下提高模型的准确率和准确性，并且可以不需要额外的调整参数，如温度Scaling。我们提供了理论支持和广泛的实验分析，证明OKO方法的有效性。<details>
<summary>Abstract</summary>
Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the trained model can be applied to single examples at inference time, without introducing significant run-time overhead or architecture changes.
</details>
<details>
<summary>摘要</summary>
MODEL 过信任和轻度预测困难在机器学习中存在，而且使用标准的empirical risk minimization方法很难准确地考虑这些问题。在这个工作中，我们提出了一种新的方法，称为奇数-$k$-out learning（OKO），该方法通过将cross-entropy errorMinimize for sets而不是单个例子。这 naturally allows the model to capture数据示例之间的相关性，并实现更高的准确率和报告率，尤其在有限的训练数据和类别不均衡情况下。尽管OKO经常提供更好的报告率，而且在使用硬标签和dropping any additional calibration parameter tuning时，我们提供了理论基础，证明OKO自然地提供更好的报告率。我们还提供了广泛的实验分析，证明我们的理论发现。我们强调OKO是一种通用的框架，可以轻松地适应多种设置，并且训练后的模型可以在推理时间应用于单个例子，无需添加显著的运行时过程 overhead或者architecture change。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Guided-Additive-Modeling-For-Supervised-Regression"><a href="#Knowledge-Guided-Additive-Modeling-For-Supervised-Regression" class="headerlink" title="Knowledge-Guided Additive Modeling For Supervised Regression"></a>Knowledge-Guided Additive Modeling For Supervised Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02229">http://arxiv.org/abs/2307.02229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yannclaes/kg-regression">https://github.com/yannclaes/kg-regression</a></li>
<li>paper_authors: Yann Claes, Vân Anh Huynh-Thu, Pierre Geurts</li>
<li>for: 本研究旨在评估混合模型在标准回归问题上的性能，并与传统机器学习方法进行比较。</li>
<li>methods: 本研究使用了混合模型，其中包括添加式地将 Parametric 物理 термин与机器学习 термин相加。 我们还研究了模型免Selection 训练方法。</li>
<li>results: 我们在 synthetic 和实际回归问题上进行了多种方法的比较，结果表明，混合模型在global performance和参数确定方面具有优势。<details>
<summary>Abstract</summary>
Learning processes by exploiting restricted domain knowledge is an important task across a plethora of scientific areas, with more and more hybrid methods combining data-driven and model-based approaches. However, while such hybrid methods have been tested in various scientific applications, they have been mostly tested on dynamical systems, with only limited study about the influence of each model component on global performance and parameter identification. In this work, we assess the performance of hybrid modeling against traditional machine learning methods on standard regression problems. We compare, on both synthetic and real regression problems, several approaches for training such hybrid models. We focus on hybrid methods that additively combine a parametric physical term with a machine learning term and investigate model-agnostic training procedures. We also introduce a new hybrid approach based on partial dependence functions. Experiments are carried out with different types of machine learning models, including tree-based models and artificial neural networks.
</details>
<details>
<summary>摘要</summary>
学习通过利用限制领域知识是科学领域中重要任务，随着更多的混合方法相继出现，这些混合方法结合数据驱动和模型基于方法。然而，虽然这些混合方法在科学应用中得到了证明，但是它们在动力系统上进行了大多数测试，对每个模型组件对全局性表现的影响尚未得到了充分的研究。在这项工作中，我们对混合模型与传统机器学习方法进行比较，在标准回归问题上进行了评估。我们比较了多种混合方法，包括添加式地将 Parametric 物理项与机器学习项相加的方法，以及模型无关的训练过程。此外，我们还介绍了一种基于 partial dependence 函数的新的混合方法。实验使用了不同类型的机器学习模型，包括树状模型和人工神经网络。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Federated-Learning-via-Amortized-Bayesian-Meta-Learning"><a href="#Personalized-Federated-Learning-via-Amortized-Bayesian-Meta-Learning" class="headerlink" title="Personalized Federated Learning via Amortized Bayesian Meta-Learning"></a>Personalized Federated Learning via Amortized Bayesian Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02222">http://arxiv.org/abs/2307.02222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyu Liu, Shaogao Lv, Dun Zeng, Zenglin Xu, Hui Wang, Yue Yu</li>
<li>for: 本研究旨在 Addressing the challenge of statistical heterogeneity in federated learning, 即让多个客户端协同学习一个全局模型，而不曝光他们私有数据。</li>
<li>methods: 本文提出了一种新的个性化联合学习方法，即\emph{FedABML}，它使用了层次变分推理来跨客户端。全局前期目标是捕捉客户端间共同内在结构的表示，然后将其转移到每个客户端的特定任务上，以便通过一些本地更新生成高度准确的客户端特定抽象 posterior。</li>
<li>results: 我们的理论分析表明，\emph{FedABML} 可以在未见数据上提供一个上下文bound，并且保证模型在未见数据上的泛化性能。此外，我们还实现了一些验证性实验，显示\emph{FedABML} 可以超越一些竞争对手。<details>
<summary>Abstract</summary>
Federated learning is a decentralized and privacy-preserving technique that enables multiple clients to collaborate with a server to learn a global model without exposing their private data. However, the presence of statistical heterogeneity among clients poses a challenge, as the global model may struggle to perform well on each client's specific task. To address this issue, we introduce a new perspective on personalized federated learning through Amortized Bayesian Meta-Learning. Specifically, we propose a novel algorithm called \emph{FedABML}, which employs hierarchical variational inference across clients. The global prior aims to capture representations of common intrinsic structures from heterogeneous clients, which can then be transferred to their respective tasks and aid in the generation of accurate client-specific approximate posteriors through a few local updates. Our theoretical analysis provides an upper bound on the average generalization error and guarantees the generalization performance on unseen data. Finally, several empirical results are implemented to demonstrate that \emph{FedABML} outperforms several competitive baselines.
</details>
<details>
<summary>摘要</summary>
“联邦学习”是一种分散式和隐私保证的技术，让多个客户端与服务器共同学习一个全球模型，不会曝露他们的私人数据。然而，客户端的统计差异对全球模型的性能产生挑战，因为全球模型可能无法很好地适应每个客户端的特定任务。为解决这个问题，我们将在个人化联邦学习中引入新的见解，通过整合泛化统计学和机器学习。 Specifically，我们提出一个名为“FedABML”的新算法，它使用客户端之间的层次统计推导，以捕捉客户端的共同内在结构表现。这些表现可以转移到每个客户端的特定任务中，并通过一些本地更新产生高精度的客户端特定概率 posteriors。我们的理论分析提供了随机数据的平均泛化错误上限，并保证模型在未见数据上的泛化性能。最后，我们进行了实验，证明了FedABML在多个竞争性基eline上表现出色。
</details></li>
</ul>
<hr>
<h2 id="On-the-Adversarial-Robustness-of-Generative-Autoencoders-in-the-Latent-Space"><a href="#On-the-Adversarial-Robustness-of-Generative-Autoencoders-in-the-Latent-Space" class="headerlink" title="On the Adversarial Robustness of Generative Autoencoders in the Latent Space"></a>On the Adversarial Robustness of Generative Autoencoders in the Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02202">http://arxiv.org/abs/2307.02202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingfei Lu, Badong Chen<br>for:This paper focuses on the adversarial robustness of generative autoencoders, specifically in the latent space.methods:The authors use various attacks in the latent space to demonstrate the vulnerability of popular generative autoencoders. They also compare the performance of variational autoencoders with their deterministic variants and observe that the latter has better latent robustness.results:The authors find that there is a trade-off between adversarial robustness and the degree of disentanglement of the latent codes. They also show that adversarial training can improve the latent robustness of VAEs.<details>
<summary>Abstract</summary>
The generative autoencoders, such as the variational autoencoders or the adversarial autoencoders, have achieved great success in lots of real-world applications, including image generation, and signal communication.   However, little concern has been devoted to their robustness during practical deployment.   Due to the probabilistic latent structure, variational autoencoders (VAEs) may confront problems such as a mismatch between the posterior distribution of the latent and real data manifold, or discontinuity in the posterior distribution of the latent.   This leaves a back door for malicious attackers to collapse VAEs from the latent space, especially in scenarios where the encoder and decoder are used separately, such as communication and compressed sensing.   In this work, we provide the first study on the adversarial robustness of generative autoencoders in the latent space.   Specifically, we empirically demonstrate the latent vulnerability of popular generative autoencoders through attacks in the latent space.   We also evaluate the difference between variational autoencoders and their deterministic variants and observe that the latter performs better in latent robustness.   Meanwhile, we identify a potential trade-off between the adversarial robustness and the degree of the disentanglement of the latent codes.   Additionally, we also verify the feasibility of improvement for the latent robustness of VAEs through adversarial training.   In summary, we suggest concerning the adversarial latent robustness of the generative autoencoders, analyze several robustness-relative issues, and give some insights into a series of key challenges.
</details>
<details>
<summary>摘要</summary>
“生成自 taught autoencoders，如variational autoencoders或adversarial autoencoders，在实际应用中取得了很大的成功，包括图像生成和信号通信。然而，对于它们在实际应用中的Robustness仍然受到了很少的关注。由于生成自 taught autoencoders的潜在阶层结构是probabilistic，因此它们可能会面临 posterior distribution of the latent和实际数据构造的不一致问题，或者 latent posterior distribution的突变。这使得黑客可以通过从latent空间攻击VAEs，特别是在encoder和decoder分开使用的情况下，如传输和压缩感知。在这个工作中，我们提供了生成自 taught autoencoders在latent空间的攻击Robustness的首次研究。我们透过实验示出了流行的生成自 taught autoencoders在latent空间的漏攻击敏感性。我们还评估了variational autoencoders和其决定性版本之间的差异，发现后者在latent robustness方面表现更好。此外，我们也发现了在提高VAEs的latent robustness方面存在一定的贸易关系。最后，我们还验证了对VAEs的latent robustness进行反向培训可以提高其Robustness。总之，我们建议关注生成自 taught autoencoders的latent Robustness，分析了一些Robustness相关的问题，并给出了一些关键挑战的见解。”
</details></li>
</ul>
<hr>
<h2 id="ChiENN-Embracing-Molecular-Chirality-with-Graph-Neural-Networks"><a href="#ChiENN-Embracing-Molecular-Chirality-with-Graph-Neural-Networks" class="headerlink" title="ChiENN: Embracing Molecular Chirality with Graph Neural Networks"></a>ChiENN: Embracing Molecular Chirality with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02198">http://arxiv.org/abs/2307.02198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gmum/chienn">https://github.com/gmum/chienn</a></li>
<li>paper_authors: Piotr Gaiński, Michał Koziarski, Jacek Tabor, Marek Śmieja</li>
<li>for: 本研究旨在使用Graph Neural Networks (GNNs)在化学分子 graph 上进行预测，并能够区分同一分子的镜像（折射体）。</li>
<li>methods: 我们提出了一种理论上正确的消息传递方案，使得 GNNs 能够具有邻居节点顺序的敏感性。我们在这个概念上将其应用于分子异旋性预测任务中，并构建了具有折射体敏感性的 Chiral Edge Neural Network (ChiENN) 层。</li>
<li>results: 我们的实验结果显示，将 ChiENN 层添加到 GNN 模型中，可以超越当前状态艺术方法在分子异旋性预测任务中的性能。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) play a fundamental role in many deep learning problems, in particular in cheminformatics. However, typical GNNs cannot capture the concept of chirality, which means they do not distinguish between the 3D graph of a chemical compound and its mirror image (enantiomer). The ability to distinguish between enantiomers is important especially in drug discovery because enantiomers can have very distinct biochemical properties. In this paper, we propose a theoretically justified message-passing scheme, which makes GNNs sensitive to the order of node neighbors. We apply that general concept in the context of molecular chirality to construct Chiral Edge Neural Network (ChiENN) layer which can be appended to any GNN model to enable chirality-awareness. Our experiments show that adding ChiENN layers to a GNN outperforms current state-of-the-art methods in chiral-sensitive molecular property prediction tasks.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在深度学习中扮演了基本角色，特别是在化学信息学中。然而， typical GNN 无法捕捉扁平性概念，这意味着它们不能分辨化学结构图和其镜像（扁平体）之间的差异。在药物发现中，能够分辨扁平体的能力是非常重要的，因为扁平体可能具有非常不同的生物化学性质。在这篇论文中，我们提出了一种理论基础的消息传递方案，该方案使 GNN 对节点邻居的顺序敏感。我们将该概念应用于分子扁平性上，并构建了 Chiral Edge Neural Network（ChiENN）层，可以让 GNN 模型具有扁平性意识。我们的实验表明，将 ChiENN 层添加到 GNN 模型后，可以超越当前状态的术法在扁平性敏感分子性质预测任务中表现。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-AI-systems-under-uncertain-ground-truth-a-case-study-in-dermatology"><a href="#Evaluating-AI-systems-under-uncertain-ground-truth-a-case-study-in-dermatology" class="headerlink" title="Evaluating AI systems under uncertain ground truth: a case study in dermatology"></a>Evaluating AI systems under uncertain ground truth: a case study in dermatology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02191">http://arxiv.org/abs/2307.02191</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Stutz, Ali Taylan Cemgil, Abhijit Guha Roy, Tatiana Matejovicova, Melih Barsbey, Patricia Strachan, Mike Schaekermann, Jan Freyberg, Rajeev Rikhye, Beverly Freeman, Javier Perez Matos, Umesh Telang, Dale R. Webster, Yuan Liu, Greg S. Corrado, Yossi Matias, Pushmeet Kohli, Yun Liu, Arnaud Doucet, Alan Karthikesalingam</li>
<li>for: 这个论文目的是提出了一种方法来评估AI模型的性能时考虑到真实的数据预期不确定性。</li>
<li>methods: 这个论文使用了一种基于统计模型的方法来汇集笔记，并提出了一种新的性能评价指标来考虑annotations uncertainty。</li>
<li>results: 研究发现，使用传统的deterministic aggregation方法时，评估结果具有很大的uncertainty，而使用提出的统计模型方法可以更好地评估模型的性能和uncertainty。<details>
<summary>Abstract</summary>
For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain. However, this is actually not the case and the ground truth may be uncertain. Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance. To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information. This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging. In contrast, we propose a framework where aggregation is done using a statistical model. Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities, representing distributions over classes in a classification setting, subject to a hyper-parameter encoding annotator reliability. Based on this model, we propose a metric for measuring annotation uncertainty and provide uncertainty-adjusted metrics for performance evaluation. We present a case study applying our framework to skin condition classification from images where annotations are provided in the form of differential diagnoses. The deterministic adjudication process called inverse rank normalization (IRN) from previous work ignores ground truth uncertainty in evaluation. Instead, we present two alternative statistical models: a probabilistic version of IRN and a Plackett-Luce-based model. We find that a large portion of the dataset exhibits significant ground truth uncertainty and standard IRN-based evaluation severely over-estimates performance without providing uncertainty estimates.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)为了保障，医疗领域中的 AI 系统在部署前都会进行严格的评估，以验证其预测结果与固定的真实值进行比较。然而，事实上，真实值并不是固定的，而是具有uncertainty。这一点很容易被忽略，但可能导致未来性能的过度估计。为了避免这种情况，我们需要考虑真实值的uncertainty。我们假设真实值的uncertainty可以分解为两个主要组成部分：注释uncertainty和内在uncertainty。注释uncertainty来自于不可靠的注释，而内在uncertainty来自于限制的观察信息。标准评估方法忽略了这些uncertainty，而是通过 deterministic aggregation（例如，多数投票或平均）来Estimating the ground truth。在这种情况下，我们提出了一种使用统计模型进行注释聚合的框架。specifically，我们将注释聚合视为 posterior inference of so-called plausibilities， representing distributions over classes in a classification setting, subject to a hyper-parameter encoding annotator reliability。基于这个模型，我们提出了一个度量注释uncertainty的metric，并提供了不确定度调整的性能评估 metric。我们在皮肤状况分类从图像中进行了一个案例研究， где注释是在 differential diagnoses 的形式提供的。previous work的 deterministic adjudication process（IRN）忽略了真实值uncertainty，而是通过 deterministic aggregation来Estimating the ground truth。在这种情况下，我们提出了两种统计模型：一种是probabilistic IRN，另一种是Plackett-Luce-based model。我们发现大量数据中存在很大的真实值uncertainty，标准 IRN-based evaluation 严重过度估计性能。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Computational-Design-at-the-Example-of-Floor-Plans"><a href="#Diffusion-Models-for-Computational-Design-at-the-Example-of-Floor-Plans" class="headerlink" title="Diffusion Models for Computational Design at the Example of Floor Plans"></a>Diffusion Models for Computational Design at the Example of Floor Plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02511">http://arxiv.org/abs/2307.02511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joern Ploennigs, Markus Berger</li>
<li>for: 这个研究旨在测试传播模型在土木工程中的应用，尤其是创建特定的建筑计划。</li>
<li>methods: 这个研究使用传播模型实现了图像生成，并提出了改进 semantic 编码的新传播模型。</li>
<li>results: 研究发现，这些传播模型可以从 6% 提高至 90% 的有效 floor plan 生成，并且在不同的例子中进行了多个实验。<details>
<summary>Abstract</summary>
AI Image generators based on diffusion models are widely discussed recently for their capability to create images from simple text prompts. But, for practical use in civil engineering they need to be able to create specific construction plans for given constraints. Within this paper we explore the capabilities of those diffusion-based AI generators for computational design at the example of floor plans and identify their current limitation. We explain how the diffusion-models work and propose new diffusion models with improved semantic encoding. In several experiments we show that we can improve validity of generated floor plans from 6% to 90% and query performance for different examples. We identify short comings and derive future research challenges of those models and discuss the need to combine diffusion models with building information modelling. With this we provide key insights into the current state and future directions for diffusion models in civil engineering.
</details>
<details>
<summary>摘要</summary>
《Diffusion模型基于AI图像生成器在近期受到广泛关注，能够从简单文本提示中生成图像。但在实际的ivil工程应用中，它们需要能够根据给定的约束创建特定的建筑计划。本文介绍了Diffusion模型在计算设计中的能力，以卷积图生成器为例，并评估了其Semantic编码的改进。我们在多个实验中示出，可以从6%提高到90%的有效性，并且提高了不同的示例的查询性能。我们还识别了这些模型的缺点，并提出了未来研究挑战。我们认为Diffusion模型和建筑信息模型的结合是未来的发展趋势。这些发现为Diffusion模型在 civil工程领域的现状和未来发展提供了关键的导向。》Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DiffFlow-A-Unified-SDE-Framework-for-Score-Based-Diffusion-Models-and-Generative-Adversarial-Networks"><a href="#DiffFlow-A-Unified-SDE-Framework-for-Score-Based-Diffusion-Models-and-Generative-Adversarial-Networks" class="headerlink" title="DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and Generative Adversarial Networks"></a>DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02159">http://arxiv.org/abs/2307.02159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwei Zhang, Han Shi, Jincheng Yu, Enze Xie, Zhenguo Li</li>
<li>for: 这个论文的目的是提出一种统一的概率理论框架，用于描述 explict 生成模型和 implicit 生成模型之间的关系。</li>
<li>methods: 该论文使用了一种名为 Discriminator Denoising Diffusion Flow (DiffFlow) 的新型 Stochastic Differential Equation (SDE)，用于描述生成模型的学习动态。</li>
<li>results: 该论文提出了一种可以在Explicit 生成模型和 implicit 生成模型之间进行满足的平衡点，并且可以通过调整权重来实现高质量样本的生成和快速样本生成。<details>
<summary>Abstract</summary>
Generative models can be categorized into two types: explicit generative models that define explicit density forms and allow exact likelihood inference, such as score-based diffusion models (SDMs) and normalizing flows; implicit generative models that directly learn a transformation from the prior to the data distribution, such as generative adversarial nets (GANs). While these two types of models have shown great success, they suffer from respective limitations that hinder them from achieving fast sampling and high sample quality simultaneously. In this paper, we propose a unified theoretic framework for SDMs and GANs. We shown that: i) the learning dynamics of both SDMs and GANs can be described as a novel SDE named Discriminator Denoising Diffusion Flow (DiffFlow) where the drift can be determined by some weighted combinations of scores of the real data and the generated data; ii) By adjusting the relative weights between different score terms, we can obtain a smooth transition between SDMs and GANs while the marginal distribution of the SDE remains invariant to the change of the weights; iii) we prove the asymptotic optimality and maximal likelihood training scheme of the DiffFlow dynamics; iv) under our unified theoretic framework, we introduce several instantiations of the DiffFLow that provide new algorithms beyond GANs and SDMs with exact likelihood inference and have potential to achieve flexible trade-off between high sample quality and fast sampling speed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>生成模型可以分为两类：Explicit生成模型，它们定义明确的概率形式，并允许准确的可能性推断，如排Diffusion模型（SDM）和Normalizing Flow；Implicit生成模型，它们直接学习数据分布与假设分布之间的变换，如生成敌方网络（GAN）。虽然这两种模型都有显著的成功，但它们受到减速采样和高质量采样的限制。在这篇论文中，我们提出一个统一的理论框架，用于SDM和GAN。我们证明了：i) SDM和GAN的学习动力可以被描述为一种名为Discriminator Denoising Diffusion Flow（DiffFlow）的新型SDE，其涨动可以由真实数据和生成数据的得分组成的权重所决定; ii) 通过调整不同得分项的相对权重，可以实现SDM和GAN之间的滑块过渡，而且采样分布的总体征不变; iii) 我们证明DiffFlow动力的极限优化和最大可能性训练方案; iv) 在我们统一的理论框架下，我们引入了多种DiffFlow实例，提供了新的算法，包括SDM和GAN中的准确可能性推断和高速采样速度。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Auto-Encoders-of-Merge-Trees-and-Persistence-Diagrams"><a href="#Wasserstein-Auto-Encoders-of-Merge-Trees-and-Persistence-Diagrams" class="headerlink" title="Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)"></a>Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02509">http://arxiv.org/abs/2307.02509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahieu Pont, Julien Tierny</li>
<li>for: 本研究提出了一种基于 Wasserstein  metric 空间的merge tree auto-encoding（MT-WAE）方法，用于提高传统自编码器的准确率和可读性。</li>
<li>methods: 本方法使用了一种新的非线性神经网络结构，将merge tree经过多层神经网络的操作，以实现更高的准确率和可读性。</li>
<li>results: 实验结果表明，MT-WAE可以快速计算merge tree，并且可以准确地压缩merge tree，同时 preserved Wasserstein 距离和 clusters。此外，本方法还可以应用于维度减少和数据分析等领域。<details>
<summary>Abstract</summary>
This paper presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts [65] at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding [65]. First, we apply MT-WAE to data reduction and reliably compress merge trees by concisely representing them with their coordinates in the final layer of our auto-encoder. Second, we document an application to dimensionality reduction, by exploiting the latent space of our auto-encoder, for the visual analysis of ensemble data. We illustrate the versatility of our framework by introducing two penalty terms, to help preserve in the latent space both the Wasserstein distances between merge trees, as well as their clusters. In both applications, quantitative experiments assess the relevance of our framework. Finally, we provide a C++ implementation that can be used for reproducibility.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Harmonizing-Feature-Attributions-Across-Deep-Learning-Architectures-Enhancing-Interpretability-and-Consistency"><a href="#Harmonizing-Feature-Attributions-Across-Deep-Learning-Architectures-Enhancing-Interpretability-and-Consistency" class="headerlink" title="Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency"></a>Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02150">http://arxiv.org/abs/2307.02150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abdul Kadir, Gowtham Krishna Addluri, Daniel Sonntag</li>
<li>for: 这 study aims to improve the interpretability and trustworthiness of machine learning models by examining the generalization of feature attributions across various deep learning architectures.</li>
<li>methods: The study uses feature attribution methods to provide local explanations of model predictions, and explores the feasibility of utilizing these methods as a future detector.</li>
<li>results: The findings suggest that harmonized feature attribution methods can improve interpretability and trust in machine learning applications, regardless of the underlying architecture.<details>
<summary>Abstract</summary>
Ensuring the trustworthiness and interpretability of machine learning models is critical to their deployment in real-world applications. Feature attribution methods have gained significant attention, which provide local explanations of model predictions by attributing importance to individual input features. This study examines the generalization of feature attributions across various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers. We aim to assess the feasibility of utilizing a feature attribution method as a future detector and examine how these features can be harmonized across multiple models employing distinct architectures but trained on the same data distribution. By exploring this harmonization, we aim to develop a more coherent and optimistic understanding of feature attributions, enhancing the consistency of local explanations across diverse deep-learning models. Our findings highlight the potential for harmonized feature attribution methods to improve interpretability and foster trust in machine learning applications, regardless of the underlying architecture.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Open-Federated-Learning-Platforms-Survey-and-Vision-from-Technical-and-Legal-Perspectives"><a href="#Towards-Open-Federated-Learning-Platforms-Survey-and-Vision-from-Technical-and-Legal-Perspectives" class="headerlink" title="Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives"></a>Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02140">http://arxiv.org/abs/2307.02140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/morningd/model-centric-fml">https://github.com/morningd/model-centric-fml</a></li>
<li>paper_authors: Moming Duan</li>
<li>for: 本文提出了一种新的 Federated Learning（FL）平台设计，即开放式 Federated Learning Platforms，以扩展FL的应用场景和提高数据持有者的参与积极性。</li>
<li>methods: 本文提出了两种对接口型FL框架的替换方案：查询型FL和合同型FL，以解决FL中的严重的服务器-客户端耦合、模型重复利用和非公共问题。</li>
<li>results: 本文通过对技术和法律领域的分析，证明了开放式FL平台的可行性和优势，并提出了一种模型license兼容分类法，以便在FL研究中更好地识别和解决模型使用权限问题。<details>
<summary>Abstract</summary>
Traditional Federated Learning (FL) follows a server-domincated cooperation paradigm which narrows the application scenarios of FL and decreases the enthusiasm of data holders to participate. To fully unleash the potential of FL, we advocate rethinking the design of current FL frameworks and extending it to a more generalized concept: Open Federated Learning Platforms. We propose two reciprocal cooperation frameworks for FL to achieve this: query-based FL and contract-based FL. In this survey, we conduct a comprehensive review of the feasibility of constructing an open FL platform from both technical and legal perspectives. We begin by reviewing the definition of FL and summarizing its inherent limitations, including server-client coupling, low model reusability, and non-public. In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-date model repositories for model querying, legal compliance analysis between different model licenses, and copyright issues and intellectual property protection in model reusing. In particular, we introduce a novel taxonomy to streamline the analysis of model license compatibility in FL studies that involve batch model reusing methods, including combination, amalgamation, distillation, and generation. This taxonomy provides a systematic framework for identifying the corresponding clauses of licenses and facilitates the identification of potential legal implications and restrictions when reusing models. Through this survey, we uncover the the current dilemmas faced by FL and advocate for the development of sustainable open FL platforms. We aim to provide guidance for establishing such platforms in the future, while identifying potential problems and challenges that need to be addressed.
</details>
<details>
<summary>摘要</summary>
传统的联合学习（FL）采用服务器主导的合作方式，这限制了FL的应用场景和数据持有者的参与积极性。为了充分发挥FL的潜力，我们提倡重新设计当前FL框架，扩展其为更通用的概念：开放联合学习平台。我们提出了两种相互合作的FL框架：查询基于的FL和合同基于的FL。在这篇评论中，我们对构建开放FL平台的技术和法律方面进行了全面的审查。我们开始介绍FL的定义和其内置的局限性，包括服务器客户端集成、低级别模型重用和非公共。在查询基于的FL平台中，我们探讨了许多有价值的话题，包括社区 empowered 的模型分享和重用平台，以及模型查询时的法律合规分析、版权问题和知识产权保护。特别是，我们提出了一种新的分类系统，用于协调FL研究中批处理模型 reuse 方法中的许可证兼容性分析，包括组合、混合、精炼和生成等方法。这种分类系统为在FL研究中复用模型时鉴别相关的许可证条款，并且可以帮助确定复用模型时的可能的法律后果和限制。通过这篇评论，我们揭示了当前FL面临的困境，并提倡开发可持续的开放FL平台。我们希望通过这篇评论，为未来建立开放FL平台提供指南，并识别可能的问题和挑战。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Differentiation-for-Hyperparameter-Tuning-the-Weighted-Graphical-Lasso"><a href="#Implicit-Differentiation-for-Hyperparameter-Tuning-the-Weighted-Graphical-Lasso" class="headerlink" title="Implicit Differentiation for Hyperparameter Tuning the Weighted Graphical Lasso"></a>Implicit Differentiation for Hyperparameter Tuning the Weighted Graphical Lasso</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02130">http://arxiv.org/abs/2307.02130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Pouliquen, Paulo Gonçalves, Mathurin Massias, Titouan Vayer</li>
<li>for: 提出一种 Framework 和算法来调整图像隐藏常量的超参数。</li>
<li>methods: 使用一种精简型搜索方法来解决一个笛卡尔级别优化问题。</li>
<li>results:  derivation of the Jacobian of the Graphical Lasso solution with respect to its regularization hyperparameters.<details>
<summary>Abstract</summary>
We provide a framework and algorithm for tuning the hyperparameters of the Graphical Lasso via a bilevel optimization problem solved with a first-order method. In particular, we derive the Jacobian of the Graphical Lasso solution with respect to its regularization hyperparameters.
</details>
<details>
<summary>摘要</summary>
我们提供了一个框架和算法，用于调整图解lasso的超参数via一个双层优化问题，解决使用首个方法。特别是，我们计算了图解lasso解的正则化超参数对它的Jacobian。Here's a breakdown of the translation:* 我们 (wǒmen) - we* 提供 (tīngyè) - provide* 框架 (kāiframe) - framework* 算法 (suānfǎ) - algorithm* 调整 (tiējian) - tune* 超参数 (chāoxiǎn) - hyperparameters* via (via) - via* 双层优化问题 (shuāngcéng yòuhuì wèn) - bilevel optimization problem* 解决 (jiějué) - solve* 使用 (fùyòu) - using* 首个方法 (shǒu gè fāng) - first-order method* 特别是 (tèbié shì) - particularly* 我们计算 (wǒmen jìsuān) - we calculate* 图解lasso解 (tújiě lasso jiě) - Graphical Lasso solution* 正则化超参数 (zhèngxíng huìxiāng) - regularization hyperparameters* 对 (duì) - on* 它 (tā) - it* Jacobian (jiābǐjian) - Jacobian
</details></li>
</ul>
<hr>
<h2 id="How-Deep-Neural-Networks-Learn-Compositional-Data-The-Random-Hierarchy-Model"><a href="#How-Deep-Neural-Networks-Learn-Compositional-Data-The-Random-Hierarchy-Model" class="headerlink" title="How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model"></a>How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02129">http://arxiv.org/abs/2307.02129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pcsl-epfl/hierarchy-learning">https://github.com/pcsl-epfl/hierarchy-learning</a></li>
<li>paper_authors: Leonardo Petrini, Francesco Cagnetta, Umberto M. Tomasini, Alessandro Favero, Matthieu Wyart</li>
<li>for: 这个论文的目的是解释深度卷积神经网络如何在高维度数据上学习普遍的任务。</li>
<li>methods: 这个论文使用的方法是使用深度卷积神经网络来学习Random Hierarchy Model，这是一个模拟真实数据的简单分类任务。</li>
<li>results: 研究发现，深度卷积神经网络需要的训练数据量（$P^*$）与高维度数据中类别的数量（$n_c$）和高级特征的数量（$m$）以及重复层数（$L$）有关，具体来说，$P^*$的增长率为$n_c m^L$,只是增长平方根。此外，研究还发现，当训练数据量够多时，深度卷积神经网络的表征将变得对于同义词替换无关，并且可以捕捉低级特征与类别之间的相关性。<details>
<summary>Abstract</summary>
Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only polynomial in the input dimensionality; (ii) coincides with the training set size such that the representation of a trained network becomes invariant to exchanges of synonyms; (iii) corresponds to the number of data at which the correlations between low-level features and classes become detectable. Overall, our results indicate how deep CNNs can overcome the curse of dimensionality by building invariant representations, and provide an estimate of the number of data required to learn a task based on its hierarchically compositional structure.
</details>
<details>
<summary>摘要</summary>
学习高维任务非常困难，因为它们需要数据量的幂等于维度。然而，深度卷积神经网络（CNN）表现出了很好的成功，即使在高维数据上。一种广泛的假设是，学习任务是高度结构化的，并且CNN可以利用这种结构来建立低维表示。然而，对于学习多少数据，还不清楚。这篇论文回答了这个问题，对于一个简单的分类任务，即Random Hierarchy Model。在这个模型中，每个分类对应$n_c$个高级特征的多个同义词组合，这些组合通过循环的过程重复$L$次。我们发现，深度CNN需要学习这个任务的数据量($P^*$)（i）在$n_c m^L$的极限上增长，这只是输入维度的多项式函数；（ii）与训练集大小相同，使得训练后神经网络的表示变得对同义词交换无关的；（iii）与低级特征和类之间的相关性变得可识别。总的来说，我们的结果表明深度CNN可以超越维度味精，并提供了学习任务基于层次结构的数据量的估算。
</details></li>
</ul>
<hr>
<h2 id="Robust-Graph-Structure-Learning-with-the-Alignment-of-Features-and-Adjacency-Matrix"><a href="#Robust-Graph-Structure-Learning-with-the-Alignment-of-Features-and-Adjacency-Matrix" class="headerlink" title="Robust Graph Structure Learning with the Alignment of Features and Adjacency Matrix"></a>Robust Graph Structure Learning with the Alignment of Features and Adjacency Matrix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02126">http://arxiv.org/abs/2307.02126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaogao Lv, Gang Wen, Shiyu Liu, Linsen Wei, Ming Li</li>
<li>for: 提高图 neural network 的 robustness，jointly 学习干净图结构和对应表示。</li>
<li>methods: 提出了一种新的准则化 graph structure learning 方法，利用特征信息和图信息的协调，基于我们 derive的节点级 Rademacher 复杂性下界。还具有减少维度的稀疏降维方法，使用低维度的节点特征来利用图结构。</li>
<li>results: 对实际图据进行了实验，表明我们提出的 GSL 方法在受到噪声影响的图结构下表现出色，超过了多种竞争对手。<details>
<summary>Abstract</summary>
To improve the robustness of graph neural networks (GNN), graph structure learning (GSL) has attracted great interest due to the pervasiveness of noise in graph data. Many approaches have been proposed for GSL to jointly learn a clean graph structure and corresponding representations. To extend the previous work, this paper proposes a novel regularized GSL approach, particularly with an alignment of feature information and graph information, which is motivated mainly by our derived lower bound of node-level Rademacher complexity for GNNs. Additionally, our proposed approach incorporates sparse dimensional reduction to leverage low-dimensional node features that are relevant to the graph structure. To evaluate the effectiveness of our approach, we conduct experiments on real-world graphs. The results demonstrate that our proposed GSL method outperforms several competitive baselines, especially in scenarios where the graph structures are heavily affected by noise. Overall, our research highlights the importance of integrating feature and graph information alignment in GSL, as inspired by our derived theoretical result, and showcases the superiority of our approach in handling noisy graph structures through comprehensive experiments on real-world datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Scale-U-Shape-MLP-for-Hyperspectral-Image-Classification"><a href="#Multi-Scale-U-Shape-MLP-for-Hyperspectral-Image-Classification" class="headerlink" title="Multi-Scale U-Shape MLP for Hyperspectral Image Classification"></a>Multi-Scale U-Shape MLP for Hyperspectral Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10186">http://arxiv.org/abs/2307.10186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moule Lin, Weipeng Jing, Donglin Di, Guangsheng Chen, Houbing Song</li>
<li>for: 该研究旨在提出一种基于多尺度U型多层感知器（MUMLP）模型，以提高 гиперспектраль图像中像素的标识率。</li>
<li>methods: 该模型由设计的多尺度渠道（MSC）块和U型多层感知器（UMLP）结构组成。 MSC将通道维度变换并混合 spectral band 特征，以生成深度水平的表示。 UMLP 由encoder-decoder结构和多层感知器层组成，能够压缩大规模参数。</li>
<li>results: 对于三个公共数据集（Pavia University、Houston 2013和Houston 2018），研究人员进行了广泛的实验，并证明了该模型可以在多种预测任务中卓越于现状顶尖方法。<details>
<summary>Abstract</summary>
Hyperspectral images have significant applications in various domains, since they register numerous semantic and spatial information in the spectral band with spatial variability of spectral signatures. Two critical challenges in identifying pixels of the hyperspectral image are respectively representing the correlated information among the local and global, as well as the abundant parameters of the model. To tackle this challenge, we propose a Multi-Scale U-shape Multi-Layer Perceptron (MUMLP) a model consisting of the designed MSC (Multi-Scale Channel) block and the UMLP (U-shape Multi-Layer Perceptron) structure. MSC transforms the channel dimension and mixes spectral band feature to embed the deep-level representation adequately. UMLP is designed by the encoder-decoder structure with multi-layer perceptron layers, which is capable of compressing the large-scale parameters. Extensive experiments are conducted to demonstrate our model can outperform state-of-the-art methods across-the-board on three wide-adopted public datasets, namely Pavia University, Houston 2013 and Houston 2018
</details>
<details>
<summary>摘要</summary>
To address these challenges, we propose a Multi-Scale U-shape Multi-Layer Perceptron (MUMLP) model, consisting of a designed Multi-Scale Channel (MSC) block and a U-shape Multi-Layer Perceptron (UMLP) structure. The MSC block transforms the channel dimension and mixes spectral band features to embed deep-level representation adequately. The UMLP structure is designed with an encoder-decoder structure and multi-layer perceptron layers, which can compress large-scale parameters.Extensive experiments demonstrate that our MUMLP model outperforms state-of-the-art methods across-the-board on three widely adopted public datasets, namely Pavia University, Houston 2013, and Houston 2018.
</details></li>
</ul>
<hr>
<h2 id="Proportional-Response-Contextual-Bandits-for-Simple-and-Cumulative-Regret-Minimization"><a href="#Proportional-Response-Contextual-Bandits-for-Simple-and-Cumulative-Regret-Minimization" class="headerlink" title="Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization"></a>Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02108">http://arxiv.org/abs/2307.02108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanath Kumar Krishnamurthy, Ruohan Zhan, Susan Athey, Emma Brunskill</li>
<li>for: 这篇 paper 的目的是提出一种 computationally efficient bandit algorithm 来实现 contextual bandit 的 optimal treatment assignment policy，并且可以适应 cumulative regret minimization 和 simple regret minimization 两种不同的目标。</li>
<li>methods: 这篇 paper 使用了一种新的 family of computationally efficient bandit algorithms，这些算法可以适应 contextual bandit 的条件下的模型错误和统计不确定性，并且可以在 continuous arm settings 中进行应用。这些算法基于 “conformal arm sets” (CASs) 的构造和依赖，CASs 提供了每个 context 中的一个包含 context-specific optimal arm 的集合，以 guaranteee 最小化 regret。</li>
<li>results: 这篇 paper 的实验结果显示了这些算法在 simple regret 和 cumulative regret 上都有优秀的表现，并且可以适应 contextual bandit 的不同条件下。此外，paper 还证明了一个 negative result，即一个 algorithm 无法同时 achiev instance-dependent simple regret guarantees 和 minimax optimal cumulative regret guarantees。<details>
<summary>Abstract</summary>
Simple regret minimization is a critical problem in learning optimal treatment assignment policies across various domains, including healthcare and e-commerce. However, it remains understudied in the contextual bandit setting. We propose a new family of computationally efficient bandit algorithms for the stochastic contextual bandit settings, with the flexibility to be adapted for cumulative regret minimization (with near-optimal minimax guarantees) and simple regret minimization (with SOTA guarantees). Furthermore, our algorithms adapt to model misspecification and extend to the continuous arm settings. These advantages come from constructing and relying on "conformal arm sets" (CASs), which provide a set of arms at every context that encompass the context-specific optimal arm with some probability across the context distribution. Our positive results on simple and cumulative regret guarantees are contrasted by a negative result, which shows that an algorithm can't achieve instance-dependent simple regret guarantees while simultaneously achieving minimax optimal cumulative regret guarantees.
</details>
<details>
<summary>摘要</summary>
<<SYS>>设置为使用简化中文（简化字）。<</SYS>>在多个领域中，包括医疗和电商，学习最佳准备分配策略是一个关键问题。然而，在上下文抽象机器人设置中，这个问题尚未得到充分研究。我们提出了一种新的计算效率高的抽象机器人算法家族，用于 Stochastic Contextual Bandit 设置，并且可以适应积累 regret 最小化（具有近似最优最小化保证）和简单 regret 最小化（具有 State-of-the-Art 保证）。此外，我们的算法可以适应模型误差和连续臂设置。这些优势来自于构造和依赖于 "conformal arm sets"（CASs），它们在每个上下文中提供一组拥有 Context-specific 优臂的arm，并且在 Context 分布中具有一定的概率。我们的正面结果表明，我们的算法可以在简单 regret 和积累 regret 两个方面提供保证，而且在模型误差和连续臂设置下也能够适应。相比之下，一个负面结果表明，无法同时实现实例特定的简单 regret 保证和最优的积累 regret 保证。
</details></li>
</ul>
<hr>
<h2 id="SoK-Privacy-Preserving-Data-Synthesis"><a href="#SoK-Privacy-Preserving-Data-Synthesis" class="headerlink" title="SoK: Privacy-Preserving Data Synthesis"></a>SoK: Privacy-Preserving Data Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02106">http://arxiv.org/abs/2307.02106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzheng Hu, Fan Wu, Qinbin Li, Yunhui Long, Gonzalo Munilla Garrido, Chang Ge, Bolin Ding, David Forsyth, Bo Li, Dawn Song</li>
<li>for: 本研究旨在提供一份概述、分析和讨论隐私保护数据分析（PPDS）领域的综述，以便回答有关PPDS方法的设计原则、分类、优缺点等问题。</li>
<li>methods: 本研究批判了两种主流PPDS方法：统计方法和深度学习（DL）基于方法。统计方法包括模型和表示方法的选择，而DL基于方法则包括不同生成模型原理。此外，我们还提供了参考表格、概括结论和开放问题。</li>
<li>results: 我们对私人图像生成任务进行了 benchmarking，并确定了DP-MERF是一种通用的方法。此外，我们还系统化了过去十年的研究成果，并提出了未来研究方向和对研究人员的呼吁。<details>
<summary>Abstract</summary>
As the prevalence of data analysis grows, safeguarding data privacy has become a paramount concern. Consequently, there has been an upsurge in the development of mechanisms aimed at privacy-preserving data analyses. However, these approaches are task-specific; designing algorithms for new tasks is a cumbersome process. As an alternative, one can create synthetic data that is (ideally) devoid of private information. This paper focuses on privacy-preserving data synthesis (PPDS) by providing a comprehensive overview, analysis, and discussion of the field. Specifically, we put forth a master recipe that unifies two prominent strands of research in PPDS: statistical methods and deep learning (DL)-based methods. Under the master recipe, we further dissect the statistical methods into choices of modeling and representation, and investigate the DL-based methods by different generative modeling principles. To consolidate our findings, we provide comprehensive reference tables, distill key takeaways, and identify open problems in the existing literature. In doing so, we aim to answer the following questions: What are the design principles behind different PPDS methods? How can we categorize these methods, and what are the advantages and disadvantages associated with each category? Can we provide guidelines for method selection in different real-world scenarios? We proceed to benchmark several prominent DL-based methods on the task of private image synthesis and conclude that DP-MERF is an all-purpose approach. Finally, upon systematizing the work over the past decade, we identify future directions and call for actions from researchers.
</details>
<details>
<summary>摘要</summary>
随着数据分析的普及，保护数据隐私已成为首要的关注点。因此，在数据分析中保持隐私的机制的开发呈现了增加趋势。然而，这些方法都是任务特定的，设计新任务的算法是一个繁琐的过程。为了解决这问题，可以创建没有隐私信息的Synthetic Data。本文关注于隐私保护数据合成（PPDS），提供了全面的概述、分析和讨论。特别是，我们提出了一种综合方法，称为“master recipe”，可以统一两个PPDS研究的主要流派：统计方法和深度学习（DL）基本方法。在master recipe下，我们进一步剖析统计方法，包括模型和表示方法的选择，并investigate DL基本模型的不同原则。为了归纳我们的发现，我们提供了完整的参考表格，概括关键点，并识别现有文献中的开放问题。因此，我们想回答以下问题：PPDS方法的设计原则是什么？如何分类这些方法，它们具有什么优势和缺点？是否可以提供实际应用场景中的方法选择指南？我们继续使用DP-MERF方法进行私人图像生成测试，并证明它是一种通用的方法。最后，我们系统化过去十年的工作，并提出未来方向和研究者的呼吁。
</details></li>
</ul>
<hr>
<h2 id="DARE-Towards-Robust-Text-Explanations-in-Biomedical-and-Healthcare-Applications"><a href="#DARE-Towards-Robust-Text-Explanations-in-Biomedical-and-Healthcare-Applications" class="headerlink" title="DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications"></a>DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02094">http://arxiv.org/abs/2307.02094</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/domain-adaptive-attribution-robustness">https://github.com/ibm/domain-adaptive-attribution-robustness</a></li>
<li>paper_authors: Adam Ivankay, Mattia Rigotti, Pascal Frossard</li>
<li>for:  This paper aims to provide a better understanding of the robustness of deep neural network explanations in the biomedical domain.</li>
<li>methods:  The paper proposes a new approach called DomainAdaptiveAREstimator (DARE) to estimate the attribution robustness of explanations in the biomedical domain. DARE takes into account domain-specific plausibility to ensure that the explanations are both accurate and relevant to the domain experts.</li>
<li>results:  The paper presents two methods, adversarial training and FAR training, to mitigate the brittleness of explanations in the biomedical domain. The proposed methods are validated through extensive experiments on three established biomedical benchmarks.<details>
<summary>Abstract</summary>
Along with the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has seen a significant increase recently. Several methods have been introduced to provide insight into the inference process of deep neural networks. However, most of these explainability methods have been shown to be brittle in the face of adversarial perturbations of their inputs in the image and generic textual domain. In this work we show that this phenomenon extends to specific and important high stakes domains like biomedical datasets. In particular, we observe that the robustness of explanations should be characterized in terms of the accuracy of the explanation in linking a model's inputs and its decisions - faithfulness - and its relevance from the perspective of domain experts - plausibility. This is crucial to prevent explanations that are inaccurate but still look convincing in the context of the domain at hand. To this end, we show how to adapt current attribution robustness estimation methods to a given domain, so as to take into account domain-specific plausibility. This results in our DomainAdaptiveAREstimator (DARE) attribution robustness estimator, allowing us to properly characterize the domain-specific robustness of faithful explanations. Next, we provide two methods, adversarial training and FAR training, to mitigate the brittleness characterized by DARE, allowing us to train networks that display robust attributions. Finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.
</details>
<details>
<summary>摘要</summary>
alongside the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has increased significantly recently. several methods have been introduced to provide insight into the inference process of deep neural networks. however, most of these explainability methods have been shown to be brittle in the face of adversarial perturbations of their inputs in the image and generic textual domain. in this work, we show that this phenomenon extends to specific and important high-stakes domains like biomedical datasets. in particular, we observe that the robustness of explanations should be characterized in terms of the accuracy of the explanation in linking a model's inputs and its decisions - faithfulness - and its relevance from the perspective of domain experts - plausibility. this is crucial to prevent explanations that are inaccurate but still look convincing in the context of the domain at hand. to this end, we show how to adapt current attribution robustness estimation methods to a given domain, so as to take into account domain-specific plausibility. this results in our domain-adaptive attribution robustness estimator (DARE) attribution robustness estimator, allowing us to properly characterize the domain-specific robustness of faithful explanations. next, we provide two methods, adversarial training and far training, to mitigate the brittleness characterized by DARE, allowing us to train networks that display robust attributions. finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Make-A-Long-Image-Short-Adaptive-Token-Length-for-Vision-Transformers"><a href="#Make-A-Long-Image-Short-Adaptive-Token-Length-for-Vision-Transformers" class="headerlink" title="Make A Long Image Short: Adaptive Token Length for Vision Transformers"></a>Make A Long Image Short: Adaptive Token Length for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02092">http://arxiv.org/abs/2307.02092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiqi Zhou, Yichen Zhu</li>
<li>for: 提高预测速度，减少计算成本</li>
<li>methods: 提出了一种适应测试时动态调整图像token长的方法，包括训练一个可变长度ViT模型和使用一个轻量级的Token长分配器（TLA）来分配最优的token长度</li>
<li>results: 实现了对多种现代视Transformer架构的减少计算成本，并在图像分类和动作识别任务上验证了方法的有效性<details>
<summary>Abstract</summary>
The vision transformer is a model that breaks down each image into a sequence of tokens with a fixed length and processes them similarly to words in natural language processing. Although increasing the number of tokens typically results in better performance, it also leads to a considerable increase in computational cost. Motivated by the saying "A picture is worth a thousand words," we propose an innovative approach to accelerate the ViT model by shortening long images. Specifically, we introduce a method for adaptively assigning token length for each image at test time to accelerate inference speed. First, we train a Resizable-ViT (ReViT) model capable of processing input with diverse token lengths. Next, we extract token-length labels from ReViT that indicate the minimum number of tokens required to achieve accurate predictions. We then use these labels to train a lightweight Token-Length Assigner (TLA) that allocates the optimal token length for each image during inference. The TLA enables ReViT to process images with the minimum sufficient number of tokens, reducing token numbers in the ViT model and improving inference speed. Our approach is general and compatible with modern vision transformer architectures, significantly reducing computational costs. We verified the effectiveness of our methods on multiple representative ViT models on image classification and action recognition.
</details>
<details>
<summary>摘要</summary>
“当代视觉转换器（ViT）模型将图像转换为一系列有 fix 长度的 токен，并对其进行语言处理的处理方式。虽然增加 токен 的数量通常会导致性能提高，但也会带来巨大的 Computational cost。为了解决这个问题，我们提出了一个创新的方法，即在评估时阶段适应地设定图像的 токен 长度。首先，我们训练了可以处理多种 токен 长度的 Resizable-ViT（ReViT）模型。接着，我们从 ReViT 中提取了 token-length 标签，这些标签指示了对于正确预测所需的最少的 токен 数量。我们然后使用这些标签进行训练一个轻量级的 Token-Length Assigner（TLA），这个 TLA 可以在评估过程中为每个图像分配最佳的 токен 长度。这个方法可以让 ReViT 在评估过程中对图像进行适当的处理，并且可以大幅降低 Computational cost。我们验证了我们的方法在多个代表性的 ViT 模型上进行图像分类和动作识别中的效果。”
</details></li>
</ul>
<hr>
<h2 id="Combating-Confirmation-Bias-A-Unified-Pseudo-Labeling-Framework-for-Entity-Alignment"><a href="#Combating-Confirmation-Bias-A-Unified-Pseudo-Labeling-Framework-for-Entity-Alignment" class="headerlink" title="Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment"></a>Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02075">http://arxiv.org/abs/2307.02075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qijie Ding, Jie Yin, Daokun Zhang, Junbin Gao</li>
<li>for: 提高实体对应性预测的准确率，抗衡假标签错误的影响</li>
<li>methods: 提出一种独特的 pseudo-labeling 框架（UPL-EA），通过精准的 Transport 模型和跨迭代 pseudo-标签准化来消除 pseudo-标签错误，提高实体对应性预测的准确率</li>
<li>results: 实验结果表明，我们的方法可以在有限的先前对应种子基础下达到竞争性的性能，并经过理论支持和实验验证，我们的方法可以减少 Type I 和 Type II pseudo-标签错误的影响<details>
<summary>Abstract</summary>
Entity alignment (EA) aims at identifying equivalent entity pairs across different knowledge graphs (KGs) that refer to the same real-world identity. To systematically combat confirmation bias for pseudo-labeling-based entity alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment (UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the accuracy of entity alignment. UPL-EA consists of two complementary components: (1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as an effective means to enable more accurate determination of entity correspondences across two KGs and to mitigate the adverse impact of erroneous matches. A simple but highly effective criterion is further devised to derive pseudo-labeled entity pairs that satisfy one-to-one correspondences at each iteration. (2) The cross-iteration pseudo-label calibration operates across multiple consecutive iterations to further improve the pseudo-labeling precision rate by reducing the local pseudo-label selection variability with a theoretical guarantee. The two components are respectively designed to eliminate Type I and Type II pseudo-labeling errors identified through our analyse. The calibrated pseudo-labels are thereafter used to augment prior alignment seeds to reinforce subsequent model training for alignment inference. The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both theoretically supported and experimentally validated. The experimental results show that our approach achieves competitive performance with limited prior alignment seeds.
</details>
<details>
<summary>摘要</summary>
Entity alignment (EA) 目标是在不同知识 graphs (KGs) 中标识同一个真实世界标识的等价实体对。为了系统地战胜假标注Error供 pseudo-labeling-based entity alignment，我们提出了一种Unified Pseudo-Labeling框架 дляEntity Alignment (UPL-EA)，该框架可以显著提高实体对应的准确率。UPL-EA包括两个补充部分：1. 基于Optimal Transport (OT)的 pseudo-labeling使用离散OT模型作为有效的方法来帮助更准确地确定两个KG中的实体对应关系，并 Mitigate the adverse impact of erroneous matches。我们还提出了一个简单 yet highly effective的标准来 derive pseudo-labeled entity pairs that satisfy one-to-one correspondences at each iteration。2. 跨迭代 pseudo-label calibration operated across multiple consecutive iterations to further improve the pseudo-labeling precision rate by reducing the local pseudo-label selection variability with a theoretical guarantee。这两个部分分别是为了消除Type I和Type II pseudo-labeling errors，这些错误被我们的分析所识别出。归一化后的 pseudo-labels 然后被用来增强后续模型训练中的对应性。我们的方法在 theoretically supported 和 experimentally validated 的情况下，可以减少 pseudo-labeling errors。实验结果显示，我们的方法在有限的先前对Alignment seeds的情况下可以达到竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="Performance-Modeling-of-Data-Storage-Systems-using-Generative-Models"><a href="#Performance-Modeling-of-Data-Storage-Systems-using-Generative-Models" class="headerlink" title="Performance Modeling of Data Storage Systems using Generative Models"></a>Performance Modeling of Data Storage Systems using Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02073">http://arxiv.org/abs/2307.02073</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Abdalaziz Rashid Al-Maeeni, Aziz Temirkhanov, Artem Ryzhikov, Mikhail Hushchyn</li>
<li>for: 这 paper 是用于高精度模型系统的研究。</li>
<li>methods: 这 paper 使用机器学习基于生成模型来构建存储系统模型。</li>
<li>results: 实验结果显示该模型可以对系统性能做出高精度预测（IOPS 和响应时间），错误率在4-10%和3-16%之间，与 Little’s law 之间呈0.99Spearman 相似性。此外，文章还提供了可用于机器学习 regression 算法、条件生成模型和不确定性估计方法的新数据集。<details>
<summary>Abstract</summary>
High-precision modeling of systems is one of the main areas of industrial data analysis. Models of systems, their digital twins, are used to predict their behavior under various conditions. We have developed several models of a storage system using machine learning-based generative models. The system consists of several components: hard disk drive (HDD) and solid-state drive (SSD) storage pools with different RAID schemes and cache. Each storage component is represented by a probabilistic model that describes the probability distribution of the component performance in terms of IOPS and latency, depending on their configuration and external data load parameters. The results of the experiments demonstrate the errors of 4-10 % for IOPS and 3-16 % for latency predictions depending on the components and models of the system. The predictions show up to 0.99 Pearson correlation with Little's law, which can be used for unsupervised reliability checks of the models. In addition, we present novel data sets that can be used for benchmarking regression algorithms, conditional generative models, and uncertainty estimation methods in machine learning.
</details>
<details>
<summary>摘要</summary>
高精度模型化系统是工业数据分析的一个主要领域。系统的模型，也称为数字响应器，用于预测它们在不同条件下的行为。我们已经开发了一些基于机器学习的生成模型，用于模型存储系统。该系统包括多个组件：硬盘驱动器（HDD）和固态驱动器（SSD）存储池，以及不同的RAID方案和缓存。每个存储组件都是由一个概率模型来描述该组件性能的可能性分布，包括IOPS和延迟时间，它们取决于组件的配置和外部数据负荷参数。实验结果显示，预测错误率为4-10% для IOPS和3-16% для延迟时间，具体取决于组件和模型。这些预测还与李тт尔定律（Little's law）之间有0.99余 correlations，可以用于无监督可靠性检查。此外，我们还提供了一些新的数据集，可以用于机器学习 regression 算法、条件生成模型和不确定性估计方法的Benchmark。
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Machine-Learning-Methods-for-Data-with-High-Cardinality-Categorical-Variables"><a href="#A-Comparison-of-Machine-Learning-Methods-for-Data-with-High-Cardinality-Categorical-Variables" class="headerlink" title="A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables"></a>A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02071">http://arxiv.org/abs/2307.02071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fabsig/compare_ml_highcardinality_categorical_variables">https://github.com/fabsig/compare_ml_highcardinality_categorical_variables</a></li>
<li>paper_authors: Fabio Sigrist</li>
<li>for: 这篇论文主要研究高Cardinality categorical variables的机器学习模型。</li>
<li>methods: 论文使用了树融合和深度神经网络两种机器学习方法，以及线性混合效应模型。</li>
<li>results: 研究发现，机器学习模型带有随机效应的版本比 классиical版本更高的预测精度。此外，树融合带有随机效应的版本也比深度神经网络带有随机效应的版本更高的预测精度。<details>
<summary>Abstract</summary>
High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set, or in other words, there are few data points per level. Machine learning methods can have difficulties with high-cardinality variables. In this article, we empirically compare several versions of two of the most successful machine learning methods, tree-boosting and deep neural networks, and linear mixed effects models using multiple tabular data sets with high-cardinality categorical variables. We find that, first, machine learning models with random effects have higher prediction accuracy than their classical counterparts without random effects, and, second, tree-boosting with random effects outperforms deep neural networks with random effects.
</details>
<details>
<summary>摘要</summary>
高级别分类变量是指数据集中每个变量有许多不同的水平，与样本大小相比，这些变量的数量很大。机器学习方法可能会遇到困难处理高级别分类变量。本文employs empirical comparisons of several versions of two of the most successful machine learning methods, tree-boosting and deep neural networks, as well as linear mixed effects models using multiple tabular data sets with high-cardinality categorical variables. Our findings show that: first, machine learning models with random effects have higher prediction accuracy than their classical counterparts without random effects; second, tree-boosting with random effects outperforms deep neural networks with random effects.
</details></li>
</ul>
<hr>
<h2 id="Universal-Rates-for-Multiclass-Learning"><a href="#Universal-Rates-for-Multiclass-Learning" class="headerlink" title="Universal Rates for Multiclass Learning"></a>Universal Rates for Multiclass Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02066">http://arxiv.org/abs/2307.02066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Machinfy/Human-Activity-Recognition-with-Smartphones">https://github.com/Machinfy/Human-Activity-Recognition-with-Smartphones</a></li>
<li>paper_authors: Steve Hanneke, Shay Moran, Qian Zhang</li>
<li>for: 这个论文是为了研究多类分类的普适率而写的。</li>
<li>methods: 这篇论文使用了 pseudo-cubes 和 DSL 树来研究多类分类的学习问题。</li>
<li>results: 这篇论文提出了一个普适率 bound，解决了 Kalavasis 等人（2022）对多类分类问题的开问。  Additionally, the paper shows that any class with an infinite Littlestone tree requires arbitrarily slow rates, while any class with a near-linear rate must have no infinite DSL tree.<details>
<summary>Abstract</summary>
We study universal rates for multiclass classification, establishing the optimal rates (up to log factors) for all hypothesis classes. This generalizes previous results on binary classification (Bousquet, Hanneke, Moran, van Handel, and Yehudayoff, 2021), and resolves an open question studied by Kalavasis, Velegkas, and Karbasi (2022) who handled the multiclass setting with a bounded number of class labels. In contrast, our result applies for any countable label space. Even for finite label space, our proofs provide a more precise bounds on the learning curves, as they do not depend on the number of labels. Specifically, we show that any class admits exponential rates if and only if it has no infinite Littlestone tree, and admits (near-)linear rates if and only if it has no infinite Daniely-Shalev-Shwartz-Littleston (DSL) tree, and otherwise requires arbitrarily slow rates. DSL trees are a new structure we define in this work, in which each node of the tree is given by a pseudo-cube of possible classifications of a given set of points. Pseudo-cubes are a structure, rooted in the work of Daniely and Shalev-Shwartz (2014), and recently shown by Brukhim, Carmon, Dinur, Moran, and Yehudayoff (2022) to characterize PAC learnability (i.e., uniform rates) for multiclass classification. We also resolve an open question of Kalavasis, Velegkas, and Karbasi (2022) regarding the equivalence of classes having infinite Graph-Littlestone (GL) trees versus infinite Natarajan-Littlestone (NL) trees, showing that they are indeed equivalent.
</details>
<details>
<summary>摘要</summary>
我们研究了 universality 的率数，确定了所有假设集合中的优化率（几乎Log因子）。这个结果总结了过去关于二分类（Bousquet, Hanneke, Moran, van Handel, 和 Yehudayoff, 2021）的研究，并解决了 Kalavasis, Velegkas, 和 Karbasi (2022) 处理多类标签的问题，他们只处理了具有bounded数量的类标签的情况。相比之下，我们的结果适用于任何可 COUNTABLE 标签空间。即使是Finite 标签空间，我们的证明还提供了更精确的学习曲线，因为它们不dependent于标签数量。我们证明，任何一个类别都可以实现指数率，如果和只有无限Littlestone树，而不是有限GL树。GL树是一种我们在这个工作中定义的新结构，每个节点都是一个可能的多个分类的pseudo-cube。pseudo-cubes是Daniely 和 Shalev-Shwartz (2014) 的工作中的一种结构，而且在 Brukhim, Carmon, Dinur, Moran, 和 Yehudayoff (2022) 的研究中被证明可以 caracterize PAC 学习（即uniform rates）多类标签分类。我们还解决了 Kalavasis, Velegkas, 和 Karbasi (2022) 关于无限GL树与无限NL树之间的等价性问题，证明它们确实是等价的。
</details></li>
</ul>
<hr>
<h2 id="Line-Graphics-Digitization-A-Step-Towards-Full-Automation"><a href="#Line-Graphics-Digitization-A-Step-Towards-Full-Automation" class="headerlink" title="Line Graphics Digitization: A Step Towards Full Automation"></a>Line Graphics Digitization: A Step Towards Full Automation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02065">http://arxiv.org/abs/2307.02065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moured/document-graphics-digitization">https://github.com/moured/document-graphics-digitization</a></li>
<li>paper_authors: Omar Moured, Jiaming Zhang, Alina Roitberg, Thorsten Schwarz, Rainer Stiefelhagen</li>
<li>for: 本研究旨在提高数字化文档的可访问性和可重现性，特别是对于数据统计图表的自动化涂抹和文本内容的研究已经是长期的焦点。</li>
<li>methods: 本文引入了细致的数学图表视觉理解任务，并提供了Line Graphics（LG）数据集，包括5种粗细类别的像素级别注解。我们的数据集包括450份来自不同领域的文档，共520张数据图像。</li>
<li>results: 我们在7种当前顶峰模型中测试了LG数据集，并发现这些模型在数据统计图表的Semantic Segmentation和Object Detection任务中的表现。为了推动数据统计图表的数字化进程，我们将会在社区内分享数据集、代码和模型。<details>
<summary>Abstract</summary>
The digitization of documents allows for wider accessibility and reproducibility. While automatic digitization of document layout and text content has been a long-standing focus of research, this problem in regard to graphical elements, such as statistical plots, has been under-explored. In this paper, we introduce the task of fine-grained visual understanding of mathematical graphics and present the Line Graphics (LG) dataset, which includes pixel-wise annotations of 5 coarse and 10 fine-grained categories. Our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines. Our proposed dataset can support two different computer vision tasks, i.e., semantic segmentation and object detection. To benchmark our LG dataset, we explore 7 state-of-the-art models. To foster further research on the digitization of statistical graphs, we will make the dataset, code, and models publicly available to the community.
</details>
<details>
<summary>摘要</summary>
digitization of documents allow for wider accessibility and reproducibility。although automatic digitization of document layout and text content has been a long-standing focus of research，this problem in regard to graphical elements，such as statistical plots，has been under-explored。in this paper，we introduce the task of fine-grained visual understanding of mathematical graphics and present the Line Graphics (LG) dataset，which includes pixel-wise annotations of 5 coarse and 10 fine-grained categories。our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines。our proposed dataset can support two different computer vision tasks，i.e., semantic segmentation and object detection。to benchmark our LG dataset，we explore 7 state-of-the-art models。to foster further research on the digitization of statistical graphs，we will make the dataset，code，and models publicly available to the community。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Facing-off-World-Model-Backbones-RNNs-Transformers-and-S4"><a href="#Facing-off-World-Model-Backbones-RNNs-Transformers-and-S4" class="headerlink" title="Facing off World Model Backbones: RNNs, Transformers, and S4"></a>Facing off World Model Backbones: RNNs, Transformers, and S4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02064">http://arxiv.org/abs/2307.02064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Deng, Junyeong Park, Sungjin Ahn</li>
<li>for: 提高模型基于学习 reinforcement learning（MBRL）代理的能力，增强代理的长期记忆。</li>
<li>methods:  explore alternative world model backbones，包括Transformers和Structured State Space Sequence（S4）模型，以提高长期记忆。</li>
<li>results: S4WM表现出优于Transformer-based world models的长期记忆能力，同时具有更高的训练效率和想象能力。这些结果铺开了开发更强的MBRL代理的道路。<details>
<summary>Abstract</summary>
World models are a fundamental component in model-based reinforcement learning (MBRL) agents. To perform temporally extended and consistent simulations of the future in partially observable environments, world models need to possess long-term memory. However, state-of-the-art MBRL agents, such as Dreamer, predominantly employ recurrent neural networks (RNNs) as their world model backbone, which have limited memory capacity. In this paper, we seek to explore alternative world model backbones for improving long-term memory. In particular, we investigate the effectiveness of Transformers and Structured State Space Sequence (S4) models, motivated by their remarkable ability to capture long-range dependencies in low-dimensional sequences and their complementary strengths. We propose S4WM, the first S4-based world model that can generate high-dimensional image sequences through latent imagination. Furthermore, we extensively compare RNN-, Transformer-, and S4-based world models across four sets of environments, which we have specifically tailored to assess crucial memory capabilities of world models, including long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning. Our findings demonstrate that S4WM outperforms Transformer-based world models in terms of long-term memory, while exhibiting greater efficiency during training and imagination. These results pave the way for the development of stronger MBRL agents.
</details>
<details>
<summary>摘要</summary>
世界模型是模型基 Reinforcement learning（MBRL）代理的重要组成部分。为在部分可见环境中进行持续时间扩展和一致的模拟未来，世界模型需要拥有长期记忆。然而，当前的MBRL代理，如梦幻，主要采用回归神经网络（RNN）作为世界模型脊梁，它们具有有限的记忆容量。在这篇论文中，我们寻找了替代的世界模型脊梁，以提高长期记忆。具体来说，我们调查了转换器和结构化状态空间序列（S4）模型，这些模型具有长距离依赖关系的捕捉能力和相互补偿的优势。我们提出了S4WM，首个基于S4模型的世界模型，可以通过幻想生成高维图像序列。此外，我们对RNN-, Transformer-, 和 S4-基于世界模型进行了广泛比较，并在我们专门为评估世界模型的重要记忆能力而设计的四组环境中进行了测试。我们的结果表明，S4WM在长期记忆方面比转换器基本世界模型高效，同时在训练和幻想过程中更高效。这些成果为开发更强的MBRL代理铺平道。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-on-Image-Classification-Models-FGSM-and-Patch-Attacks-and-their-Impact"><a href="#Adversarial-Attacks-on-Image-Classification-Models-FGSM-and-Patch-Attacks-and-their-Impact" class="headerlink" title="Adversarial Attacks on Image Classification Models: FGSM and Patch Attacks and their Impact"></a>Adversarial Attacks on Image Classification Models: FGSM and Patch Attacks and their Impact</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02055">http://arxiv.org/abs/2307.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaydip Sen, Subhasis Dasgupta</li>
<li>for: 本文介绍了对图像分类模型使用 adversarial 攻击的概念。</li>
<li>methods: 本文讨论了两种常见的 adversarial 攻击方法，即快速梯度签名法 (FGSM) 和 adversarial 贴图攻击。</li>
<li>results: 对三种强大预训练的图像分类模型（ResNet-34、GoogleNet、DenseNet-161）进行了攻击性评估，并计算了图像分类任务中模型在攻击和不攻击情况下的分类精度。<details>
<summary>Abstract</summary>
This chapter introduces the concept of adversarial attacks on image classification models built on convolutional neural networks (CNN). CNNs are very popular deep-learning models which are used in image classification tasks. However, very powerful and pre-trained CNN models working very accurately on image datasets for image classification tasks may perform disastrously when the networks are under adversarial attacks. In this work, two very well-known adversarial attacks are discussed and their impact on the performance of image classifiers is analyzed. These two adversarial attacks are the fast gradient sign method (FGSM) and adversarial patch attack. These attacks are launched on three powerful pre-trained image classifier architectures, ResNet-34, GoogleNet, and DenseNet-161. The classification accuracy of the models in the absence and presence of the two attacks are computed on images from the publicly accessible ImageNet dataset. The results are analyzed to evaluate the impact of the attacks on the image classification task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Graph-Neural-Network-based-Power-Flow-Model"><a href="#Graph-Neural-Network-based-Power-Flow-Model" class="headerlink" title="Graph Neural Network-based Power Flow Model"></a>Graph Neural Network-based Power Flow Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02049">http://arxiv.org/abs/2307.02049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingjian Tuo, Xingpeng Li, Tianxia Zhao</li>
<li>for: 这篇论文的目的是提出一种基于图神经网络（GNN）的电力流计算模型，以提高电力系统中线流计算的准确性和效率。</li>
<li>methods: 该模型使用历史电力系统数据进行训练，并使用图神经网络（GNN）模型来预测电力流结果。</li>
<li>results: 对比于传统的直流电力流计算模型和深度神经网络（DNN）、卷积神经网络（CNN）模型，该GNN模型能够提供更准确的解决方案，并且高效。<details>
<summary>Abstract</summary>
Power flow analysis plays a crucial role in examining the electricity flow within a power system network. By performing power flow calculations, the system's steady-state variables, including voltage magnitude, phase angle at each bus, active/reactive power flow across branches, can be determined. While the widely used DC power flow model offers speed and robustness, it may yield inaccurate line flow results for certain transmission lines. This issue becomes more critical when dealing with renewable energy sources such as wind farms, which are often located far from the main grid. Obtaining precise line flow results for these critical lines is vital for next operations. To address these challenges, data-driven approaches leverage historical grid profiles. In this paper, a graph neural network (GNN) model is trained using historical power system data to predict power flow outcomes. The GNN model enables rapid estimation of line flows. A comprehensive performance analysis is conducted, comparing the proposed GNN-based power flow model with the traditional DC power flow model, as well as deep neural network (DNN) and convolutional neural network (CNN). The results on test systems demonstrate that the proposed GNN-based power flow model provides more accurate solutions with high efficiency comparing to benchmark models.
</details>
<details>
<summary>摘要</summary>
电流流分析在电力系统网络中扮演着关键的角色，可以确定电力系统的稳定状态变量，包括每个总机的相位角和电压大小。虽然广泛使用的直流电流模型具有速度和可靠性，但可能导致certain transmission lines的流量结果不准确。这个问题在处理可再生能源such as wind farms时变得更加重要，这些可再生能源往往位于主网络远离的地方。为了解决这些挑战，数据驱动方法可以利用历史电力系统数据来预测电流流的结果。在这篇论文中，一种基于图神经网络（GNN）模型被训练使用历史电力系统数据来预测电流流的结果。GNN模型可以快速估算线流。我们进行了全面的性能分析，比较了提议的GNN-based电流流模型与传统的直流电流模型、深度神经网络（DNN）和卷积神经网络（CNN）模型。测试系统上的结果表明，提议的GNN-based电流流模型可以提供更加准确的解决方案，并且高效性比benchmark模型更高。
</details></li>
</ul>
<hr>
<h2 id="Fisher-Weighted-Merge-of-Contrastive-Learning-Models-in-Sequential-Recommendation"><a href="#Fisher-Weighted-Merge-of-Contrastive-Learning-Models-in-Sequential-Recommendation" class="headerlink" title="Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation"></a>Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05476">http://arxiv.org/abs/2307.05476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jung Hyun Ryu, Jaeheyoung Jeon, Jewoong Cho, Myungjoo Kang 1</li>
<li>for: 这篇论文主要针对推荐系统中的次序推荐问题，即为用户随时间的偏好进行推荐。</li>
<li>methods: 本论文使用了对照学习方法，将多个模型的参数融合，以提高推荐系统的总性能。</li>
<li>results: 经过广泛的实验，本论文显示出该方法的效果，并证明其能够提高次序推荐系统的状态前进。<details>
<summary>Abstract</summary>
Along with the exponential growth of online platforms and services, recommendation systems have become essential for identifying relevant items based on user preferences. The domain of sequential recommendation aims to capture evolving user preferences over time. To address dynamic preference, various contrastive learning methods have been proposed to target data sparsity, a challenge in recommendation systems due to the limited user-item interactions. In this paper, we are the first to apply the Fisher-Merging method to Sequential Recommendation, addressing and resolving practical challenges associated with it. This approach ensures robust fine-tuning by merging the parameters of multiple models, resulting in improved overall performance. Through extensive experiments, we demonstrate the effectiveness of our proposed methods, highlighting their potential to advance the state-of-the-art in sequential learning and recommendation systems.
</details>
<details>
<summary>摘要</summary>
随着在线平台和服务的快速增长，推荐系统已成为用户喜好的标准工具。序列推荐的领域旨在捕捉用户的时间演变的偏好。为了解决动态偏好的挑战，多种对照学习方法已经被提议用于目标数据稀缺。在这篇论文中，我们是首次将施耐德-抽取方法应用于序列推荐，解决和解决实际挑战。这种方法确保了精度的练习调整，从而提高总性性能。通过广泛的实验，我们证明了我们的提议方法的效果， highlighting their potential to advance the state-of-the-art in sequential learning and recommendation systems.
</details></li>
</ul>
<hr>
<h2 id="VertiBench-Advancing-Feature-Distribution-Diversity-in-Vertical-Federated-Learning-Benchmarks"><a href="#VertiBench-Advancing-Feature-Distribution-Diversity-in-Vertical-Federated-Learning-Benchmarks" class="headerlink" title="VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks"></a>VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02040">http://arxiv.org/abs/2307.02040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaomin Wu, Junyi Hou, Bingsheng He</li>
<li>for: 本研究はVertical Federated Learning（VFL）の性能评価に适用される公共世界データセットの欠如に対処します。</li>
<li>methods: 本研究では、Feature importanceとFeature correlationの2つの键因子を考虑し、それぞれに対応する评価指标とデータセットの分割方法を提案します。</li>
<li>results: 本研究では、State-of-the-art VFLアルゴリズムの效果的な评価を提供し、Future researchの参考になる価値ある Insightsを提供します。<details>
<summary>Abstract</summary>
Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.
</details>
<details>
<summary>摘要</summary>
纵向联合学习（VFL）是训练机器学习模型的重要方法，该方法在分布式数据上进行特征分区。然而由于隐私限制，公共世界中的VFL数据集很少，这些数据集只代表了有限的特征分布。现有的标准 benchmark 通常采用人工生成的数据集，这些数据集只反映了一部分特征分布，导致算法性能评估不准确。本文解决这些缺陷，通过介绍特征重要性和特征相关性两个关键因素，并提出相应的评价指标和数据分割方法。此外，我们还介绍了一个真实存在的VFL数据集，用于解决图像-图像VFL场景中的不足。我们对当前VFL领域最先进的算法进行了全面的评估，提供了valuable的情况参考。
</details></li>
</ul>
<hr>
<h2 id="Monte-Carlo-Sampling-without-Isoperimetry-A-Reverse-Diffusion-Approach"><a href="#Monte-Carlo-Sampling-without-Isoperimetry-A-Reverse-Diffusion-Approach" class="headerlink" title="Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach"></a>Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02037">http://arxiv.org/abs/2307.02037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xunpeng Huang, Hanze Dong, Yifan Hao, Yian Ma, Tong Zhang</li>
<li>for: 本研究探讨了 posterior sampling 的可能性，它是通过反射扩散来实现高质量数据样本的生成模型的效能的一种方法。</li>
<li>methods: 本研究使用了分解过程kernel的技术，将 score estimation 转化为了一个mean estimation问题，从而实现了一种新的 posterior sampling 算法。</li>
<li>results: 我们提供了这种算法的收敛分析，并证明了其在高维样本中的性能比传统MCMC方法更高，这是因为该算法的auxiliary distribution的一些性质可以减少误差。<details>
<summary>Abstract</summary>
The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the potentialities of posterior sampling through reverse diffusion. An examination of the sampling literature reveals that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the auxiliary distribution, the reverse diffusion process can give rise to a novel posterior sampling algorithm, which diverges from traditional gradient-based Markov Chain Monte Carlo (MCMC) methods. We provide the convergence analysis in total variation distance and demonstrate that the isoperimetric dependency of the proposed algorithm is comparatively lower than that observed in conventional MCMC techniques, which justifies the superior performance for high dimensional sampling with error tolerance. Our analytical framework offers fresh perspectives on the complexity of score estimation at various time points, as denoted by the properties of the auxiliary distribution.
</details>
<details>
<summary>摘要</summary>
现代生成模型的效果通常取决于扩散路径上的分数估计精度，尤其是扩散模型和它们能够生成高质量数据样本。这项研究探讨了反扩散 posterior 采样的可能性，通过将分数估计转换为auxiliary distribution的均值估计问题。通过估计auxiliary distribution的均值，反扩散过程可以生成一种新的 posterior 采样算法，与传统的梯度基本 Markov Chain Monte Carlo (MCMC) 方法不同。我们提供了整体变量距离的收敛分析，并证明了提案的算法的iso依赖关系比传统 MCMC 技术更低，这 justify了高维度采样中的高精度和误差容忍。我们的分析框架为 score estimation 的复杂性在不同时刻点提供了新的视角，即auxiliary distribution的属性。
</details></li>
</ul>
<hr>
<h2 id="Ranking-with-Abstention"><a href="#Ranking-with-Abstention" class="headerlink" title="Ranking with Abstention"></a>Ranking with Abstention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02035">http://arxiv.org/abs/2307.02035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anqi Mao, Mehryar Mohri, Yutao Zhong</li>
<li>for: 这个论文提出了一种新的排名概念，即learner可以在一定成本$c$的情况下决定不预测。</li>
<li>methods: 这个论文使用了一种扩展的理论分析，包括线性函数家族和带有一个隐藏层的神经网络的$H$-一致性 bound。</li>
<li>results: 实验结果表明，这种排名方法在实际应用中具有效果。<details>
<summary>Abstract</summary>
We introduce a novel framework of ranking with abstention, where the learner can abstain from making prediction at some limited cost $c$. We present a extensive theoretical analysis of this framework including a series of $H$-consistency bounds for both the family of linear functions and that of neural networks with one hidden-layer. These theoretical guarantees are the state-of-the-art consistency guarantees in the literature, which are upper bounds on the target loss estimation error of a predictor in a hypothesis set $H$, expressed in terms of the surrogate loss estimation error of that predictor. We further argue that our proposed abstention methods are important when using common equicontinuous hypothesis sets in practice. We report the results of experiments illustrating the effectiveness of ranking with abstention.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的排名框架，其中学习者可以在某些有限成本$c$的情况下退出预测。我们提供了广泛的理论分析，包括线性函数家族和带一层隐藏层神经网络的$H$-一致性上下文。这些理论保证是文献中的最佳一致性保证，它们是指定损失函数集$H$中预测器的目标损失估计错误的Upper bound，表示了预测器在损失函数集$H$中的损失估计错误。我们还 argue что我们提议的投降方法在实际中使用公共等距inuous假设集时是重要的。我们报告了实验结果，证明了排名框架的效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Automatic-Parallel-Training-via-Balanced-Memory-Workload-Optimization"><a href="#Improving-Automatic-Parallel-Training-via-Balanced-Memory-Workload-Optimization" class="headerlink" title="Improving Automatic Parallel Training via Balanced Memory Workload Optimization"></a>Improving Automatic Parallel Training via Balanced Memory Workload Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02031">http://arxiv.org/abs/2307.02031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujie Wang, Youhe Jiang, Xupeng Miao, Fangcheng Fu, Xiaonan Nie, Bin Cui</li>
<li>for:  This paper aims to improve the efficiency of training Transformer models across multiple GPUs.</li>
<li>methods:  The paper proposes a novel system framework called Galvatron-BMW, which integrates multiple parallelism dimensions and automatically identifies the most efficient hybrid parallelism strategy using a decision tree approach and dynamic programming search algorithm.</li>
<li>results:  Galvatron-BMW consistently achieves superior system throughput in automating distributed training under varying GPU memory constraints, surpassing previous approaches that rely on limited parallelism strategies.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文目的是提高多个GPU上Transformer模型的训练效率。</li>
<li>methods: 该论文提出了一种新的系统框架called Galvatron-BMW，它集成了多种并发方向并自动确定最佳混合并发策略，使用决策树方法和动态规划搜索算法。</li>
<li>results: Galvatron-BMW在不同Transformer模型的测试场景中 consistently达到了自动化分布训练的最高系统吞吐量，超过了前一些仅仅采用有限并发策略的方法。<details>
<summary>Abstract</summary>
Transformer models have emerged as the leading approach for achieving state-of-the-art performance across various application domains, serving as the foundation for advanced large-scale deep learning (DL) models. However, efficiently training these models across multiple GPUs remains a complex challenge due to the abundance of parallelism options. Existing DL systems either require manual efforts to design distributed training plans or limit parallelism combinations to a constrained search space. In this paper, we present Galvatron-BMW, a novel system framework that integrates multiple prevalent parallelism dimensions and automatically identifies the most efficient hybrid parallelism strategy. To effectively navigate this vast search space, we employ a decision tree approach for decomposition and pruning based on intuitive insights. We further utilize a dynamic programming search algorithm to derive the optimal plan. Moreover, to improve resource utilization and enhance system efficiency, we propose a bi-objective optimization workflow that focuses on workload balance. Our evaluations on different Transformer models demonstrate the capabilities of Galvatron-BMW in automating distributed training under varying GPU memory constraints. Across all tested scenarios, Galvatron-BMW consistently achieves superior system throughput, surpassing previous approaches that rely on limited parallelism strategies.
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate_text: transformer 模型已经成为实现不同应用领域的状态码模型的主流方法， serving 为高级大规模深度学习（DL）模型的基础。然而，在多个GPU上有效地训练这些模型仍然是一个复杂的挑战，因为存在丰富的并行性选择。现有的 DL 系统可以通过手动设计分布式训练计划或限制并行性组合的搜索空间。在这篇论文中，我们提出了 Galvatron-BMW 系统框架，该系统框架集成了多种流行的并行性维度，并自动确定最有效的混合并行性策略。为了有效地探索这个庞大的搜索空间，我们使用决策树方法进行分解和剔除，基于直观的理解。此外，我们还提出了一种动态搜索算法，以derive 最佳计划。此外，为了提高资源利用率和系统效率，我们提出了一种两个目标优化工作流程，该工作流程关注工作负荷均衡。我们对不同的 transformer 模型进行了不同的评估， demonstrates  Galvatron-BMW 在不同的 GPU 内存限制下自动化分布式训练的能力。在所有测试场景中，Galvatron-BMW  consistently  achieve  superior system throughput，超越了基于有限并行性策略的前一代方法。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="EHRSHOT-An-EHR-Benchmark-for-Few-Shot-Evaluation-of-Foundation-Models"><a href="#EHRSHOT-An-EHR-Benchmark-for-Few-Shot-Evaluation-of-Foundation-Models" class="headerlink" title="EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models"></a>EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02028">http://arxiv.org/abs/2307.02028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/som-shahlab/ehrshot-benchmark">https://github.com/som-shahlab/ehrshot-benchmark</a></li>
<li>paper_authors: Michael Wornow, Rahul Thapa, Ethan Steinberg, Jason Fries, Nigam Shah</li>
<li>for: 本研究的目的是提高医疗机器学习（ML）在医疗领域的进步，通过公共数据集、任务和模型的共享，但医疗领域的ML进步受到共享资产的限制。本研究通过三个贡献来解决这些挑战。</li>
<li>methods: 本研究使用了一个新的数据集，名为EHRSHOT，这是医疗记录电子档案（EHR）中的6,712名患者的去identify的结构化数据。与MIMIC-III&#x2F;IV和其他流行的EHR数据集不同，EHRSHOT是长期跟踪的，而不是仅仅是ICU&#x2F;ED patients的数据。此外，本研究还公布了一个141M参数的临床基础模型，这是一个可以处理coded EHR数据的完整模型，而不是只能处理不结构化文本的模型。</li>
<li>results: 本研究定义了15个几个shot临床预测任务，使得可以评估基础模型的样本效率和任务适应性。同时，研究者们还提供了一个可重现结果的代码，以及模型和数据集（通过研究数据使用协议获取）。<details>
<summary>Abstract</summary>
While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, containing de-identified structured data from the electronic health records (EHRs) of 6,712 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaption. The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available at our Github repo here: https://github.com/som-shahlab/ehrshot-benchmark
</details>
<details>
<summary>摘要</summary>
generale 机器学习（ML）社区得益于公共数据集、任务和模型，而医疗机器学习（ML）的进步却受到公共资产的缺乏所妨碍。成功的基本模型创造了新的挑战，需要访问共享预训练模型来验证性能 beneficiaries。我们通过以下三个贡献来解决这些挑战：1. 我们发布了一个新的数据集，EHRSHOT，包含了医疗电子病历（EHR）中6,712名患者的去掉个人信息的结构化数据。与MIMIC-III/IV和其他流行的EHR数据集不同，EHRSHOT是 longitudinal 的，而不是仅仅是ICU/ED patients。2. 我们发布了一个141M参数的临床基础模型，预训练于EHR数据中的结构化数据中的2.57M名患者。我们是一个 Among the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR。我们提供了一个端到端的管道，让社区可以验证和基于其性能。3. 我们定义了15个几个shot的临床预测任务，使得基础模型的性能在样本效率和任务适应方面进行评估。我们的代码、模型和数据集（通过研究数据用途协议获取）都可以在我们 GitHub 仓库中找到：https://github.com/som-shahlab/ehrshot-benchmark。
</details></li>
</ul>
<hr>
<h2 id="Using-Random-Effects-Machine-Learning-Algorithms-to-Identify-Vulnerability-to-Depression"><a href="#Using-Random-Effects-Machine-Learning-Algorithms-to-Identify-Vulnerability-to-Depression" class="headerlink" title="Using Random Effects Machine Learning Algorithms to Identify Vulnerability to Depression"></a>Using Random Effects Machine Learning Algorithms to Identify Vulnerability to Depression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02023">http://arxiv.org/abs/2307.02023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runa Bhaumik, Jonathan Stange</li>
<li>for: 预测青年成年人抑郁症状的诊断和预后 прогнозинг</li>
<li>methods: 使用数据驱动的机器学习方法（RE-EM树和MERF）对抑郁风险因素进行分类和识别</li>
<li>results: 结果表明，RE-EM树和MERF方法可以准确地预测青年成年人抑郁症状，并且可以确定抑郁风险因素的复杂相互作用，以及哪些因素对于预后预测最有用。<details>
<summary>Abstract</summary>
Background: Reliable prediction of clinical progression over time can improve the outcomes of depression. Little work has been done integrating various risk factors for depression, to determine the combinations of factors with the greatest utility for identifying which individuals are at the greatest risk. Method: This study demonstrates that data-driven machine learning (ML) methods such as RE-EM (Random Effects/Expectation Maximization) trees and MERF (Mixed Effects Random Forest) can be applied to reliably identify variables that have the greatest utility for classifying subgroups at greatest risk for depression. 185 young adults completed measures of depression risk, including rumination, worry, negative cognitive styles, cognitive and coping flexibilities, and negative life events, along with symptoms of depression. We trained RE-EM trees and MERF algorithms and compared them to traditional linear mixed models (LMMs) predicting depressive symptoms prospectively and concurrently with cross-validation. Results: Our results indicated that the RE-EM tree and MERF methods model complex interactions, identify subgroups of individuals and predict depression severity comparable to LMM. Further, machine learning models determined that brooding, negative life events, negative cognitive styles, and perceived control were the most relevant predictors of future depression levels. Conclusions: Random effects machine learning models have the potential for high clinical utility and can be leveraged for interventions to reduce vulnerability to depression.
</details>
<details>
<summary>摘要</summary>
背景：可靠预测临床进程的发展可以提高抑郁症的结果。然而，有少量的研究把不同的风险因素 integrate 以确定最有用的组合因素，以确定患有抑郁症的个人是否处于最高风险。方法：本研究表明，数据驱动的机器学习（ML）方法，如RE-EM（随机效应/期望最大化）树和MERF（混合效应随机森林）可以可靠地识别出抑郁症风险的最有用变量。185名年轻成人完成了抑郁风险的测量，包括催眠、担忧、消极思维、认知和处理的灵活性、以及负面生活事件，同时测量抑郁症的 симптом。我们训练了RE-EM树和MERF算法，并与传统的线性混合模型（LMM）相比，预测抑郁症的严重程度。结果：我们的结果表明，RE-EM树和MERF方法可以模型复杂的交互，分类个人为不同的子组合，并且预测抑郁症的严重程度与LMM相当。此外，机器学习模型确定了催眠、负面生活事件、消极思维和感知控制是抑郁症的最有用预测因素。结论：Random effects机器学习模型具有高临床实用性，可以用于降低抑郁症的抵触性。
</details></li>
</ul>
<hr>
<h2 id="Modular-DFR-Digital-Delayed-Feedback-Reservoir-Model-for-Enhancing-Design-Flexibility"><a href="#Modular-DFR-Digital-Delayed-Feedback-Reservoir-Model-for-Enhancing-Design-Flexibility" class="headerlink" title="Modular DFR: Digital Delayed Feedback Reservoir Model for Enhancing Design Flexibility"></a>Modular DFR: Digital Delayed Feedback Reservoir Model for Enhancing Design Flexibility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11094">http://arxiv.org/abs/2307.11094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sosei Ikeda, Hiromitsu Awano, Takashi Sato</li>
<li>for: 这个论文主要是为了提出一种全数字式延迟反馈水库系统（DFR），以便在硬件实现中使用。</li>
<li>methods: 该论文提出了一种新的模块化DFR模型，该模型可以完全在数字domain中实现，并且可以采用不同的非线性函数进行选择，从而提高准确性而减少功耗。</li>
<li>results: 该论文通过两种不同的非线性函数实现DFR，实现了功耗降低10倍和吞吐量提高5.3倍，而保持相同或更好的准确性。<details>
<summary>Abstract</summary>
A delayed feedback reservoir (DFR) is a type of reservoir computing system well-suited for hardware implementations owing to its simple structure. Most existing DFR implementations use analog circuits that require both digital-to-analog and analog-to-digital converters for interfacing. However, digital DFRs emulate analog nonlinear components in the digital domain, resulting in a lack of design flexibility and higher power consumption. In this paper, we propose a novel modular DFR model that is suitable for fully digital implementations. The proposed model reduces the number of hyperparameters and allows flexibility in the selection of the nonlinear function, which improves the accuracy while reducing the power consumption. We further present two DFR realizations with different nonlinear functions, achieving 10x power reduction and 5.3x throughput improvement while maintaining equal or better accuracy.
</details>
<details>
<summary>摘要</summary>
一种延迟反馈蓄水池（DFR）是一种适合硬件实现的计算系统，具有简单的结构。现有大多数 DFR 实现使用分析电路，需要数字到分析和分析到数字转换器进行交互。然而，数字 DFR 模拟分析电路在数字领域，导致设计灵活性偏低和功耗更高。在本文中，我们提出一种新的模块化 DFR 模型，适合完全数字实现。我们的提案减少了多个超参数，并允许非线性函数的选择，从而提高了准确性，同时降低了功耗。我们进一步采用了两种不同的非线性函数来实现 DFR，实现了功耗降低10倍和通过putthrough提高5.3倍，保持或更好的准确性。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Effectiveness-of-Large-Language-Models-in-Representing-Textual-Descriptions-of-Geometry-and-Spatial-Relations"><a href="#Evaluating-the-Effectiveness-of-Large-Language-Models-in-Representing-Textual-Descriptions-of-Geometry-and-Spatial-Relations" class="headerlink" title="Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations"></a>Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03678">http://arxiv.org/abs/2307.03678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Ji, Song Gao</li>
<li>for: 评估大语言模型（LLMs）在表示几何和其空间关系方面的能力。</li>
<li>methods: 使用GPT-2和BERT等大语言模型将文本（WKT）格式的几何编码并feed其 embeddings 到分类器和回归器进行评估效果。</li>
<li>results: LLMs-生成的embeddings可以保持几何类型和捕捉一定的空间关系（准确率达73%），但还存在估算数值和检索空间相关对象的挑战。<details>
<summary>Abstract</summary>
This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="STS-CCL-Spatial-Temporal-Synchronous-Contextual-Contrastive-Learning-for-Urban-Traffic-Forecasting"><a href="#STS-CCL-Spatial-Temporal-Synchronous-Contextual-Contrastive-Learning-for-Urban-Traffic-Forecasting" class="headerlink" title="STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting"></a>STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02507">http://arxiv.org/abs/2307.02507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lincan Li, Kaixiang Yang, Fengji Luo, Jichao Bi</li>
<li>for: 这个研究目的是为了提高大规模未标注交通数据中的复杂空间时间表现，以及对于其他缺乏数据的跨空间任务。</li>
<li>methods: 这篇论文使用了进步的对照学习和一个新的空间时间同步Contextual Contrastive Learning（STS-CCL）模型，包括对于空间时间граф数据的基本和强化增强方法，以及一个空间时间同步对照模组（STS-CM），以同时捕捉出Decent空间时间依赖关系。</li>
<li>results: 实验和评估结果显示，使用STS-CCL模型建立预测器可以对交通预测 benchmark 进行超越性的表现，并且适合具有缺乏数据的大规模跨空间任务。<details>
<summary>Abstract</summary>
Efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains to be a challenging task. In considering of the dilemma, this work employs the advanced contrastive learning and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. Second, we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. To further discriminate node individuals in negative filtering, a Semantic Contextual Contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with negative filtering. Finally, we present a hard mutual-view contrastive training scheme and extend the classic contrastive loss to an integrated objective function, yielding better performance. Extensive experiments and evaluations demonstrate that building a predictor upon STS-CCL contrastive learning model gains superior performance than existing traffic forecasting benchmarks. The proposed STS-CCL is highly suitable for large datasets with only a few labeled data and other spatiotemporal tasks with data scarcity issue.
</details>
<details>
<summary>摘要</summary>
efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains a challenging task. to address this challenge, this work employs advanced contrastive learning and proposes a novel spatial-temporal synchronous contextual contrastive learning (STS-CCL) model. first, we elaborate on the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. second, we introduce a spatial-temporal synchronous contrastive module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. to further discriminate node individuals in negative filtering, a semantic contextual contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with negative filtering. finally, we present a hard mutual-view contrastive training scheme and extend the classic contrastive loss to an integrated objective function, yielding better performance. extensive experiments and evaluations demonstrate that building a predictor upon STS-CCL contrastive learning model gains superior performance than existing traffic forecasting benchmarks. the proposed STS-CCL is highly suitable for large datasets with only a few labeled data and other spatiotemporal tasks with data scarcity issue.
</details></li>
</ul>
<hr>
<h2 id="Distilling-Missing-Modality-Knowledge-from-Ultrasound-for-Endometriosis-Diagnosis-with-Magnetic-Resonance-Images"><a href="#Distilling-Missing-Modality-Knowledge-from-Ultrasound-for-Endometriosis-Diagnosis-with-Magnetic-Resonance-Images" class="headerlink" title="Distilling Missing Modality Knowledge from Ultrasound for Endometriosis Diagnosis with Magnetic Resonance Images"></a>Distilling Missing Modality Knowledge from Ultrasound for Endometriosis Diagnosis with Magnetic Resonance Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02000">http://arxiv.org/abs/2307.02000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Zhang, Hu Wang, David Butler, Minh-Son To, Jodie Avery, M Louise Hull, Gustavo Carneiro</li>
<li>for: 提高 Magnetic Resonance Imaging (MRI) 图像中镜像腔腔积极膜（POD）探测精度，使用知识汇抽法。</li>
<li>methods: 利用不同数据集的 teacher 模型和学生模型，通过知识汇抽法进行训练，提高学生模型对 MRI 图像中 POD 探测的精度。</li>
<li>results: 实验结果表明，使用我们提出的方法可以提高 MRI 图像中 POD 探测的精度。<details>
<summary>Abstract</summary>
Endometriosis is a common chronic gynecological disorder that has many characteristics, including the pouch of Douglas (POD) obliteration, which can be diagnosed using Transvaginal gynecological ultrasound (TVUS) scans and magnetic resonance imaging (MRI). TVUS and MRI are complementary non-invasive endometriosis diagnosis imaging techniques, but patients are usually not scanned using both modalities and, it is generally more challenging to detect POD obliteration from MRI than TVUS. To mitigate this classification imbalance, we propose in this paper a knowledge distillation training algorithm to improve the POD obliteration detection from MRI by leveraging the detection results from unpaired TVUS data. More specifically, our algorithm pre-trains a teacher model to detect POD obliteration from TVUS data, and it also pre-trains a student model with 3D masked auto-encoder using a large amount of unlabelled pelvic 3D MRI volumes. Next, we distill the knowledge from the teacher TVUS POD obliteration detector to train the student MRI model by minimizing a regression loss that approximates the output of the student to the teacher using unpaired TVUS and MRI data. Experimental results on our endometriosis dataset containing TVUS and MRI data demonstrate the effectiveness of our method to improve the POD detection accuracy from MRI.
</details>
<details>
<summary>摘要</summary>
具体来说，我们的算法首先在 TVUS 数据上训练一个教师模型，用于检测 PODS 消失。然后，我们将这个教师模型与一个学生模型相结合，使用大量的未标注 pelvic 3D MRI 数据进行训练。接着，我们将教师模型中的知识传授给学生模型，使其通过对不同的 TVUS 和 MRI 数据进行无标注对应的损失函数来学习。实验结果表明，我们的方法可以提高 MRI 上 PODS 的检测精度。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Neural-Architecture-Search-Challenges-Solutions-and-Opportunities"><a href="#Zero-Shot-Neural-Architecture-Search-Challenges-Solutions-and-Opportunities" class="headerlink" title="Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities"></a>Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01998">http://arxiv.org/abs/2307.01998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sldgroup/survey-zero-shot-nas">https://github.com/sldgroup/survey-zero-shot-nas</a></li>
<li>paper_authors: Guihong Li, Duc Hoang, Kartikeya Bhardwaj, Ming Lin, Zhangyang Wang, Radu Marculescu</li>
<li>for: 本文旨在审视和比较当前最佳实践（SOTA）的零shot Neural Architecture Search（NAS）方法，强调它们在硬件上的意识。</li>
<li>methods: 本文首先介绍主流的零shot proxy，并讲解它们的理论基础。然后通过大规模的实验比较这些零shot proxy，并在硬件意识和硬件感知 NAS 场景中证明其效果。</li>
<li>results: 本文的实验结果表明，零shot NAS 方法在硬件意识和硬件感知场景中具有极高的效果，并且可以在不同的硬件背景下进行可靠的 NAS。此外，本文还提出了一些可能更好的 proxy 设计的想法。<details>
<summary>Abstract</summary>
Recently, zero-shot (or training-free) Neural Architecture Search (NAS) approaches have been proposed to liberate the NAS from training requirements. The key idea behind zero-shot NAS approaches is to design proxies that predict the accuracies of the given networks without training network parameters. The proxies proposed so far are usually inspired by recent progress in theoretical deep learning and have shown great potential on several NAS benchmark datasets. This paper aims to comprehensively review and compare the state-of-the-art (SOTA) zero-shot NAS approaches, with an emphasis on their hardware awareness. To this end, we first review the mainstream zero-shot proxies and discuss their theoretical underpinnings. We then compare these zero-shot proxies through large-scale experiments and demonstrate their effectiveness in both hardware-aware and hardware-oblivious NAS scenarios. Finally, we point out several promising ideas to design better proxies. Our source code and the related paper list are available on https://github.com/SLDGroup/survey-zero-shot-nas.
</details>
<details>
<summary>摘要</summary>
最近，零shot（或无需训练）神经建筑搜索（NAS）方法已经被提出，以解 liberate NAS 从训练要求中。零shot NAS 方法的关键想法是通过不需要训练网络参数来预测网络的准确性。已经提出的proxy都是基于现代神经网络理论的发展，在几个 NAS 比赛数据集上显示出了极高的潜力。这篇论文的目的是对当前领先的零shot NAS 方法进行全面的审视和比较，强调硬件意识。为此，我们首先介绍主流零shot proxy，并讨论它们的理论基础。然后，我们通过大规模的实验比较这些零shot proxy，并在硬件意识和硬件无知 NAS 场景中证明它们的效iveness。最后，我们提出了一些可能会设计更好的proxy的想法。我们的源代码和相关论文列表可以在https://github.com/SLDGroup/survey-zero-shot-nas上获取。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Feature-based-Deep-Reinforcement-Learning-for-Flow-Control-of-Circular-Cylinder-with-Sparse-Surface-Pressure-Sensing"><a href="#Dynamic-Feature-based-Deep-Reinforcement-Learning-for-Flow-Control-of-Circular-Cylinder-with-Sparse-Surface-Pressure-Sensing" class="headerlink" title="Dynamic Feature-based Deep Reinforcement Learning for Flow Control of Circular Cylinder with Sparse Surface Pressure Sensing"></a>Dynamic Feature-based Deep Reinforcement Learning for Flow Control of Circular Cylinder with Sparse Surface Pressure Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01995">http://arxiv.org/abs/2307.01995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiulei Wang, Lei Yan, Gang Hu, Wenli Chen, Bernd R. Noack</li>
<li>for: 这个研究旨在开发一种基于深度学习的闭Loop瓣纹控制算法，以降低瓣纹 drag 和 lift 波动，并且在感知不充分的情况下进行自适应控制。</li>
<li>methods: 该研究基于深度学习，将感知信号提升为动态特征（DF），以预测未来的流态态。 resulting DF-DRL 自动学习了响应控制器，无需动态模型。</li>
<li>results: 对比标准模型，DF-DRL 模型的瓣纹系数降低了25%。使用单个表面压力传感器，DF-DRL 可以降低瓣纹系数到状态 искусственный智能性的8%，并且减少了升力系数波动。这种方法还在更高的 Reynolds 数下表现良好，降低了瓣纹系数32.2% 和 46.55%。<details>
<summary>Abstract</summary>
This study proposes a self-learning algorithm for closed-loop cylinder wake control targeting lower drag and lower lift fluctuations with the additional challenge of sparse sensor information, taking deep reinforcement learning as the starting point. DRL performance is significantly improved by lifting the sensor signals to dynamic features (DF), which predict future flow states. The resulting dynamic feature-based DRL (DF-DRL) automatically learns a feedback control in the plant without a dynamic model. Results show that the drag coefficient of the DF-DRL model is 25% less than the vanilla model based on direct sensor feedback. More importantly, using only one surface pressure sensor, DF-DRL can reduce the drag coefficient to a state-of-the-art performance of about 8% at Re = 100 and significantly mitigate lift coefficient fluctuations. Hence, DF-DRL allows the deployment of sparse sensing of the flow without degrading the control performance. This method also shows good robustness in controlling flow under higher Reynolds numbers, which reduces the drag coefficient by 32.2% and 46.55% at Re = 500 and 1000, respectively, indicating the broad applicability of the method. Since surface pressure information is more straightforward to measure in realistic scenarios than flow velocity information, this study provides a valuable reference for experimentally designing the active flow control of a circular cylinder based on wall pressure signals, which is an essential step toward further developing intelligent control in realistic multi-input multi-output (MIMO) system.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-KiTS21-Challenge-Automatic-segmentation-of-kidneys-renal-tumors-and-renal-cysts-in-corticomedullary-phase-CT"><a href="#The-KiTS21-Challenge-Automatic-segmentation-of-kidneys-renal-tumors-and-renal-cysts-in-corticomedullary-phase-CT" class="headerlink" title="The KiTS21 Challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase CT"></a>The KiTS21 Challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01984">http://arxiv.org/abs/2307.01984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neheller/kits21">https://github.com/neheller/kits21</a></li>
<li>paper_authors: Nicholas Heller, Fabian Isensee, Dasha Trofimova, Resha Tejpaul, Zhongchen Zhao, Huai Chen, Lisheng Wang, Alex Golts, Daniel Khapun, Daniel Shats, Yoel Shoshan, Flora Gilboa-Solomon, Yasmeen George, Xi Yang, Jianpeng Zhang, Jing Zhang, Yong Xia, Mengran Wu, Zhiyang Liu, Ed Walczak, Sean McSweeney, Ranveer Vasdev, Chris Hornung, Rafat Solaiman, Jamee Schoephoerster, Bailey Abernathy, David Wu, Safa Abdulkadir, Ben Byun, Justice Spriggs, Griffin Struyk, Alexandra Austin, Ben Simpson, Michael Hagstrom, Sierra Virnig, John French, Nitin Venkatesh, Sarah Chan, Keenan Moore, Anna Jacobsen, Susan Austin, Mark Austin, Subodh Regmi, Nikolaos Papanikolopoulos, Christopher Weight</li>
<li>for: 本文是关于2021年的肾茵和肾肿瘤分割挑战（KiTS21）的挑战报告，与2021年的医疗图像计算和计算机助手外科会议（MICCAI）一起举行。</li>
<li>methods: 本挑战使用了一种新的标注方法，收集了每个区域兴趣的三个独立标注，并使用了一个基于网络的标注工具进行完全透明的标注。此外，KiTS21测试集来自外部机构，挑战参与者开发出能够通用化的方法。</li>
<li>results:  despite the challenges, the top-performing teams achieved a significant improvement over the state of the art set in 2019, and this performance is shown to inch ever closer to human-level performance. Here’s the translation in Traditional Chinese:</li>
<li>for: 本文是关于2021年的肾茵和肾肿瘤分割挑战（KiTS21）的挑战报告，与2021年的医疗图像计算和计算机助手外科会议（MICCAI）一起举行。</li>
<li>methods: 本挑战使用了一种新的标注方法，收集了每个区域兴趣的三个独立标注，并使用了一个基于网络的标注工具进行完全透明的标注。此外，KiTS21测试集来自外部机构，挑战参与者开发出能够通用化的方法。</li>
<li>results:  despite the challenges, the top-performing teams achieved a significant improvement over the state of the art set in 2019, and this performance is shown to inch ever closer to human-level performance.<details>
<summary>Abstract</summary>
This paper presents the challenge report for the 2021 Kidney and Kidney Tumor Segmentation Challenge (KiTS21) held in conjunction with the 2021 international conference on Medical Image Computing and Computer Assisted Interventions (MICCAI). KiTS21 is a sequel to its first edition in 2019, and it features a variety of innovations in how the challenge was designed, in addition to a larger dataset. A novel annotation method was used to collect three separate annotations for each region of interest, and these annotations were performed in a fully transparent setting using a web-based annotation tool. Further, the KiTS21 test set was collected from an outside institution, challenging participants to develop methods that generalize well to new populations. Nonetheless, the top-performing teams achieved a significant improvement over the state of the art set in 2019, and this performance is shown to inch ever closer to human-level performance. An in-depth meta-analysis is presented describing which methods were used and how they faired on the leaderboard, as well as the characteristics of which cases generally saw good performance, and which did not. Overall KiTS21 facilitated a significant advancement in the state of the art in kidney tumor segmentation, and provides useful insights that are applicable to the field of semantic segmentation as a whole.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了2021年的肾脏和肾肿瘤分割挑战（KiTS21）的挑战报告，该挑战在2021年的医学影像计算和计算助手外科学会（MICCAI）会议上举行。KiTS21是2019年的首届版本的续作，它在设计方面添加了许多创新，同时使用了更大的数据集。在这次挑战中，使用了一种新的注解方法，每个区域兴趣都有三个独立的注解，并在网络上使用了 transparent 的注解工具进行了注解。此外，KiTS21 测试集来自于外部机构，挑战参与者们开发出能够在新人口中广泛应用的方法。不过，最高排名的团队在2019年的状态前进set上达到了显著的改进，并且这种性能在人类水平逐渐往近。文章还提供了一个深入的meta-分析，描述了参与者们使用的方法以及其在排名表上的表现，以及特定情况下的好坏表现。总的来说，KiTS21 对肾脏瘤分割领域的状态前进做出了重要贡献，并为 semantic segmentation 领域提供了有用的指导。
</details></li>
</ul>
<hr>
<h2 id="A-ChatGPT-Aided-Explainable-Framework-for-Zero-Shot-Medical-Image-Diagnosis"><a href="#A-ChatGPT-Aided-Explainable-Framework-for-Zero-Shot-Medical-Image-Diagnosis" class="headerlink" title="A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis"></a>A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01981">http://arxiv.org/abs/2307.01981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, Zuozhu Liu</li>
<li>for: 这个研究是为了提出一个零条件医疗影像分类框架，以便在实际应用中对于有限的疾病或大规模标注数据进行医疗诊断。</li>
<li>methods: 这个研究使用了CLIP的预训练视觉语言模型，并与ChatGPT进行整合，以提供可解释的医疗诊断。在这个框架中，我们使用了分类名称来询问大型语言模型（LLMs），以生成更多的cue和知识，例如疾病 симптом或描述，帮助提供更加精确和可解释的诊断。</li>
<li>results: 我们在一个私人数据集和四个公共数据集上进行了广泛的实验，并进行了详细分析，结果显示了我们的零条件医疗影像分类框架的有效性和可解释性，证明了VLMs和LLMs在医疗应用中的巨大潜力。<details>
<summary>Abstract</summary>
Zero-shot medical image classification is a critical process in real-world scenarios where we have limited access to all possible diseases or large-scale annotated data. It involves computing similarity scores between a query medical image and possible disease categories to determine the diagnostic result. Recent advances in pretrained vision-language models (VLMs) such as CLIP have shown great performance for zero-shot natural image recognition and exhibit benefits in medical applications. However, an explainable zero-shot medical image recognition framework with promising performance is yet under development. In this paper, we propose a novel CLIP-based zero-shot medical image classification framework supplemented with ChatGPT for explainable diagnosis, mimicking the diagnostic process performed by human experts. The key idea is to query large language models (LLMs) with category names to automatically generate additional cues and knowledge, such as disease symptoms or descriptions other than a single category name, to help provide more accurate and explainable diagnosis in CLIP. We further design specific prompts to enhance the quality of generated texts by ChatGPT that describe visual medical features. Extensive results on one private dataset and four public datasets along with detailed analysis demonstrate the effectiveness and explainability of our training-free zero-shot diagnosis pipeline, corroborating the great potential of VLMs and LLMs for medical applications.
</details>
<details>
<summary>摘要</summary>
zero-shot医疗影像分类是现实世界中的关键过程，其中我们可能只有有限的疾病或大规模注释的数据。它涉及计算医疗影像和可能的疾病类别之间的相似性分数，以确定诊断结果。现代预训练视觉语言模型（VLM），如CLIP，在无需训练的情况下显示出了非常好的性能，并且在医疗应用中展现出了优势。然而，一个可解释的无需训练医疗影像分类框架仍然在开发中。在本文中，我们提出了一种基于CLIP的新的无需训练医疗影像分类框架，并与ChatGPT结合使用以提供可解释的诊断。我们的关键想法是使用类别名称来查询大型语言模型（LLM），以自动生成更多的引导和知识，如疾病 симптом或描述，以帮助提供更准确和可解释的诊断。我们还设计了特定的提示，以提高生成的文本中的可读性。我们的无需训练零shot诊断管道在一个私人数据集和四个公共数据集上进行了广泛的测试，并进行了详细的分析，结果证明了我们的训练free零shot诊断管道的有效性和可解释性，证明了VLM和LLM在医疗应用中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Algorithme-EM-regularise"><a href="#Algorithme-EM-regularise" class="headerlink" title="Algorithme EM régularisé"></a>Algorithme EM régularisé</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01955">http://arxiv.org/abs/2307.01955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Houdouin, Matthieu Jonkcheere, Frederic Pascal</li>
<li>for: 用于处理小样本大数据的 Gaussian Mixture Model (GMM) 最优化likelihood问题。</li>
<li>methods: 提出了一种受限制的EM算法，通过使用先验知识来缓解小样本大数据的问题，以确保covariance矩阵更新的正定性。</li>
<li>results: 实验表明该方法在 clustering 任务中表现良好。<details>
<summary>Abstract</summary>
Expectation-Maximization (EM) algorithm is a widely used iterative algorithm for computing maximum likelihood estimate when dealing with Gaussian Mixture Model (GMM). When the sample size is smaller than the data dimension, this could lead to a singular or poorly conditioned covariance matrix and, thus, to performance reduction. This paper presents a regularized version of the EM algorithm that efficiently uses prior knowledge to cope with a small sample size. This method aims to maximize a penalized GMM likelihood where regularized estimation may ensure positive definiteness of covariance matrix updates by shrinking the estimators towards some structured target covariance matrices. Finally, experiments on real data highlight the good performance of the proposed algorithm for clustering purposes
</details>
<details>
<summary>摘要</summary>
<<SYS>>预期最大化（EM）算法是一种广泛使用的迭代算法，用于计算 Gaussian Mixture Model（GMM）中的最大可能性。当样本大小小于数据维度时，这可能导致一个稀疏或不良条件的协方差矩阵，从而导致性能下降。这篇文章提出了一种经过规格化的 EM 算法，可以有效地利用先前知识来应对小样本大小。该方法的目标是最大化约束后 GMM likelihood，通过压缩估计器向一些结构化目标协方差矩阵偏转。最后，在实际数据上进行了 clustering 实验，并证明了该算法的良好性能。>>>
</details></li>
</ul>
<hr>
<h2 id="FEMDA-Une-methode-de-classification-robuste-et-flexible"><a href="#FEMDA-Une-methode-de-classification-robuste-et-flexible" class="headerlink" title="FEMDA: Une méthode de classification robuste et flexible"></a>FEMDA: Une méthode de classification robuste et flexible</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01954">http://arxiv.org/abs/2307.01954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Houdouin, Matthieu Jonckheere, Frederic Pascal</li>
<li>for: 本研究旨在提出一种可以承受不同标准差和独立但不同分布的样本的新分类分析技术，以替代传统的线性和 quadratic discriminant分析方法，这些方法受到非泊oluisson分布和杂乱数据的影响。</li>
<li>methods: 该技术基于每个数据点都由其自己的arbitrary Elliptically Symmetrical（ES）分布和自己的扩展参数来定义，这使得模型能够处理可能非常不同、独立但不同分布的样本。</li>
<li>results: 该技术比其他状态艺术方法更快速、简单，对于涉及到非泊oluisson分布和杂乱数据的情况具有更高的Robustness，可以更好地适应实际应用中的数据分布。<details>
<summary>Abstract</summary>
Linear and Quadratic Discriminant Analysis (LDA and QDA) are well-known classical methods but can heavily suffer from non-Gaussian distributions and/or contaminated datasets, mainly because of the underlying Gaussian assumption that is not robust. This paper studies the robustness to scale changes in the data of a new discriminant analysis technique where each data point is drawn by its own arbitrary Elliptically Symmetrical (ES) distribution and its own arbitrary scale parameter. Such a model allows for possibly very heterogeneous, independent but non-identically distributed samples. The new decision rule derived is simple, fast, and robust to scale changes in the data compared to other state-of-the-art method
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Neural-Collapse-Perspective-on-Feature-Evolution-in-Graph-Neural-Networks"><a href="#A-Neural-Collapse-Perspective-on-Feature-Evolution-in-Graph-Neural-Networks" class="headerlink" title="A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks"></a>A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01951">http://arxiv.org/abs/2307.01951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kvignesh1420/gnn_collapse">https://github.com/kvignesh1420/gnn_collapse</a></li>
<li>paper_authors: Vignesh Kothapalli, Tom Tirer, Joan Bruna</li>
<li>for: 本研究ocuses on node-wise classification tasks using graph neural networks (GNNs), and explores the interplay between graph topology and feature evolution.</li>
<li>methods: 本研究使用了community detection on stochastic block model graphs to illustrate the feature evolution, and explores the “Neural Collapse” (NC) phenomenon to understand the reduction in within-class variability.</li>
<li>results: 研究发现，在node-wise classification setting中，也有一定的减少内类差异，但不如instance-wise caso。然而，我们通过理论分析发现，这种减少内类差异的情况需要图像 obey certain strict structural conditions。此外，我们还研究了层次的feature variability evolution和spectral methods的差异。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the "Neural Collapse" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an "optimistic" mathematical model requires that the graphs obey a strict structural condition in order to possess a minimizer with exact collapse. Interestingly, this condition is viable also for heterophilic graphs and relates to recent empirical studies on settings with improved GNNs' generalization. Furthermore, by studying the gradient dynamics of the theoretical model, we provide reasoning for the partial collapse observed empirically. Finally, we present a study on the evolution of within- and between-class feature variability across layers of a well-trained GNN and contrast the behavior with spectral methods.
</details>
<details>
<summary>摘要</summary>
格 Edge 神经网络 (GNNs) 在图像数据上的分类任务中得到了广泛的应用。然而，图像结构和特征进化在 GNNs 之间的关系还不够了解。在这篇论文中，我们将注意力集中在图像分类任务上，使用社会均衡图来检测社群，并通过 "神经崩溃" (NC) 现象来探索特征进化。当训练深度分类器（例如图像分类） beyond 零训练错误点时，NC 显示出深度特征内部的同类变化减少和类别中心对某些对称结构的偏好增加。我们开始于一个实验研究，显示在节点级分类设定下，也存在类似的减少同类变化现象，但不如实例级分类情况那么严重。然后，我们进行了理论研究。我们表明，即使使用 "乐观" 的数学模型， graphs 需要遵循一种严格的结构条件，以便具有精确的崩溃。意外地，这种条件适用于异谱图也，并与最近的实际研究中的 GNNs 的泛化有关。此外，我们通过研究理论模型的梯度动力学，提供了崩溃观察到的解释。最后，我们展示了一个层次进化特征的演化过程，并与 спектраль方法相比较。
</details></li>
</ul>
<hr>
<h2 id="A-Synthetic-Electrocardiogram-ECG-Image-Generation-Toolbox-to-Facilitate-Deep-Learning-Based-Scanned-ECG-Digitization"><a href="#A-Synthetic-Electrocardiogram-ECG-Image-Generation-Toolbox-to-Facilitate-Deep-Learning-Based-Scanned-ECG-Digitization" class="headerlink" title="A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization"></a>A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01946">http://arxiv.org/abs/2307.01946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshama Kodthalu Shivashankara, Afagh Mehri Shervedani, Reza Sameni</li>
<li>for: 这个论文的目的是提出一种新的方法来生成Synthetic ECG图像，以便用于训练深度学习模型进行算法式ECG诊断。</li>
<li>methods: 该方法利用了深度学习图像处理技术，并将 PhysioNet PTB-XL ECG时间序列数据作为引用时间序列数据，通过数据扩展技术来生成Synthetic ECG图像。</li>
<li>results: 研究人员通过计算信号噪声比（SNR）来评估生成的Synthetic ECG图像质量，结果显示了平均信号恢复SNR为27$\pm$2.8dB，这说明了提出的Synthetic ECG图像集可以用于训练深度学习模型。<details>
<summary>Abstract</summary>
The electrocardiogram (ECG) is an accurate and widely available tool for diagnosing cardiovascular diseases. ECGs have been recorded in printed formats for decades and their digitization holds great potential for training machine learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at risk of deterioration and scanning printed ECGs alone is insufficient, as ML models require ECG time-series data. Therefore, the digitization and conversion of paper ECG archives into time-series data is of utmost importance. Deep learning models for image processing show promise in this regard. However, the scarcity of ECG archives with reference time-series is a challenge. Data augmentation techniques utilizing \textit{digital twins} present a potential solution.   We introduce a novel method for generating synthetic ECG images on standard paper-like ECG backgrounds with realistic artifacts. Distortions including handwritten text artifacts, wrinkles, creases and perspective transforms are applied to the generated images, without personally identifiable information. As a use case, we generated an ECG image dataset of 21,801 records from the 12-lead PhysioNet PTB-XL ECG time-series dataset. A deep ECG image digitization model was built and trained on the synthetic dataset, and was employed to convert the synthetic images to time-series data for evaluation. The signal-to-noise ratio (SNR) was calculated to assess the image digitization quality vs the ground truth ECG time-series. The results show an average signal recovery SNR of 27$\pm$2.8\,dB, demonstrating the significance of the proposed synthetic ECG image dataset for training deep learning models. The codebase is available as an open-access toolbox for ECG research.
</details>
<details>
<summary>摘要</summary>
电rokardiogram (ECG) 是一种精度很高且普遍可用的工具，用于诊断心血管疾病。ECG 已经被记录在Printed format 中decades，其数字化具有很大的潜力，用于训练机器学习（ML）模型。Physical ECG archive 面临着逐渐衰老和损坏的风险，而且将Printed ECG 纸背景上的ECG 记录scan alone 是不够的，因为ML 模型需要时间序列数据。因此，将纸背景上的ECG 记录数字化和转换为时间序列数据是非常重要的。深度学习模型 для图像处理表示了可能性。然而，获取ECG archive 中的参考时间序列数据是一个挑战。使用数据扩展技术利用“数字双胞胎”的想法可以解决这个问题。我们提出了一种新的方法，用于在标准纸背景上生成synthetic ECG 图像。这些图像包括手写文本 artifacts、折叠、皱纹和视角变换等缺失，但不包含个人可识别信息。作为用例，我们生成了21,801个纪录，来自PhysioNet PTB-XL ECG 时间序列 dataset。我们建立了一个深度ECG 图像数字化模型，并将其训练在生成的synthetic dataset上。然后，我们使用该模型将生成的synthetic图像转换为时间序列数据，并计算了信号噪声比（SNR）来评估图像数字化质量与真实ECG 时间序列数据之间的对比。结果显示，生成的ECG 图像数据的平均信号恢复SNR为27$\pm$2.8dB，这说明了我们提出的Synthetic ECG 图像dataset的重要性。我们的代码库作为一个开源工具箱，用于心血管疾病研究。
</details></li>
</ul>
<hr>
<h2 id="Text-Sketch-Image-Compression-at-Ultra-Low-Rates"><a href="#Text-Sketch-Image-Compression-at-Ultra-Low-Rates" class="headerlink" title="Text + Sketch: Image Compression at Ultra Low Rates"></a>Text + Sketch: Image Compression at Ultra Low Rates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01944">http://arxiv.org/abs/2307.01944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leieric/text-sketch">https://github.com/leieric/text-sketch</a></li>
<li>paper_authors: Eric Lei, Yiğit Berkay Uslu, Hamed Hassani, Shirin Saeedi Bidokhti</li>
<li>for: 本文旨在探讨如何使用文本描述生成高质量图像，并用于图像压缩。</li>
<li>methods: 本文使用了一些直接使用预训练模型进行图像压缩的技术，包括使用文本描述和侧信息生成高质量重建图像，以及使用预训练模型进行图像压缩。</li>
<li>results: 研究发现，使用这些技术可以在非常低的比特率下实现高度的semantic和spatial结构保持，并且在learned compressors中显著提高了感知和semantic faithfulness。<details>
<summary>Abstract</summary>
Recent advances in text-to-image generative models provide the ability to generate high-quality images from short text descriptions. These foundation models, when pre-trained on billion-scale datasets, are effective for various downstream tasks with little or no further training. A natural question to ask is how such models may be adapted for image compression. We investigate several techniques in which the pre-trained models can be directly used to implement compression schemes targeting novel low rate regimes. We show how text descriptions can be used in conjunction with side information to generate high-fidelity reconstructions that preserve both semantics and spatial structure of the original. We demonstrate that at very low bit-rates, our method can significantly improve upon learned compressors in terms of perceptual and semantic fidelity, despite no end-to-end training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Neural-Network-Based-Enrichment-of-Reproducing-Kernel-Approximation-for-Modeling-Brittle-Fracture"><a href="#A-Neural-Network-Based-Enrichment-of-Reproducing-Kernel-Approximation-for-Modeling-Brittle-Fracture" class="headerlink" title="A Neural Network-Based Enrichment of Reproducing Kernel Approximation for Modeling Brittle Fracture"></a>A Neural Network-Based Enrichment of Reproducing Kernel Approximation for Modeling Brittle Fracture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01937">http://arxiv.org/abs/2307.01937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonghyuk Baek, Jiun-Shyan Chen</li>
<li>For: The paper is written to propose an improved version of the neural network-enhanced Reproducing Kernel Particle Method (NN-RKPM) for modeling brittle fracture.* Methods: The proposed method uses a background reproducing kernel (RK) approximation defined on a coarse and uniform discretization, which is enriched by a neural network (NN) approximation under a Partition of Unity framework. The NN approximation automatically locates and inserts regularized discontinuities in the function space.* Results: The proposed method is demonstrated to be effective through a series of numerical examples involving damage propagation and branching, and the solution convergence of the proposed method is guaranteed.Here are the three points in Simplified Chinese:* For: 这篇论文是为了提出一种改进版的神经网络增强的 reproduce kernel particle method (NN-RKPM)，用于模拟脆性断裂。* Methods: 该方法使用了一个背景 reproduce kernel (RK) approximation，定义在一个粗略和均匀的离散中，并通过一个神经网络 (NN)  aproximation 下的 Partition of Unity 框架进行增强。NN  aproximation 自动在函数空间中找到和插入正规破碎。* Results: 该方法在一系列的数值例子中，包括损害传播和分支，并且解的收敛性是保证的。<details>
<summary>Abstract</summary>
Numerical modeling of localizations is a challenging task due to the evolving rough solution in which the localization paths are not predefined. Despite decades of efforts, there is a need for innovative discretization-independent computational methods to predict the evolution of localizations. In this work, an improved version of the neural network-enhanced Reproducing Kernel Particle Method (NN-RKPM) is proposed for modeling brittle fracture. In the proposed method, a background reproducing kernel (RK) approximation defined on a coarse and uniform discretization is enriched by a neural network (NN) approximation under a Partition of Unity framework. In the NN approximation, the deep neural network automatically locates and inserts regularized discontinuities in the function space. The NN-based enrichment functions are then patched together with RK approximation functions using RK as a Partition of Unity patching function. The optimum NN parameters defining the location, orientation, and displacement distribution across location together with RK approximation coefficients are obtained via the energy-based loss function minimization. To regularize the NN-RK approximation, a constraint on the spatial gradient of the parametric coordinates is imposed in the loss function. Analysis of the convergence properties shows that the solution convergence of the proposed method is guaranteed. The effectiveness of the proposed method is demonstrated by a series of numerical examples involving damage propagation and branching.
</details>
<details>
<summary>摘要</summary>
numerical modeling of localizations 是一个复杂的任务，因为本地化路径不是预定的。 DESPITE 数十年的努力，目前仍需要创新的离散独立计算方法，以预测本地化的演化。 在这种工作中，一种改进的神经网络增强的复现器kernel方法（NN-RKPM）被提议用于模拟脆弱裂解。在提议的方法中，背景的复现器kernel（RK）approximation在粗略和均匀的离散上定义，然后通过一个神经网络（NN）approximation在Partition of Unity框架下进行增强。在NNapproximation中，深度神经网络自动在函数空间中找到并插入正规化缺陷。然后，NN基于的增强函数被与RKapproximation函数用RK作为Partition of Unity patching函数相连接。通过能量基本的损失函数最小化来获取优化NN参数，其中NN参数包括位置、方向和分布的拟合。为了正则化NN-RKapproximation，在损失函数中添加了空间梯度的约束。分析表示方法的扩散性是保证的。通过一系列数字示例，包括损失传播和分支，这种方法的有效性得到证明。
</details></li>
</ul>
<hr>
<h2 id="MDI-A-Flexible-Random-Forest-Based-Feature-Importance-Framework"><a href="#MDI-A-Flexible-Random-Forest-Based-Feature-Importance-Framework" class="headerlink" title="MDI+: A Flexible Random Forest-Based Feature Importance Framework"></a>MDI+: A Flexible Random Forest-Based Feature Importance Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01932">http://arxiv.org/abs/2307.01932</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csinva/imodels">https://github.com/csinva/imodels</a></li>
<li>paper_authors: Abhineet Agarwal, Ana M. Kenney, Yan Shuo Tan, Tiffany M. Tang, Bin Yu</li>
<li>for: 本研究旨在提出一种可变的特征重要性框架，即MDI+，以提高Random Forest模型中特征的重要性评估。</li>
<li>methods: 本研究使用了Random Forest模型和Generalized Linear Models（GLMs），并提出了一种基于Predictability、Computability和Stability框架的指南，以帮助实践者选择适合的GLM和评价指标。</li>
<li>results: 实验表明，MDI+可以在识别信号特征方面表现出色，并且在实际应用中可以提取已知的预测性基因，并且比现有的特征重要性评估方法具有更高的稳定性。<details>
<summary>Abstract</summary>
Mean decrease in impurity (MDI) is a popular feature importance measure for random forests (RFs). We show that the MDI for a feature $X_k$ in each tree in an RF is equivalent to the unnormalized $R^2$ value in a linear regression of the response on the collection of decision stumps that split on $X_k$. We use this interpretation to propose a flexible feature importance framework called MDI+. Specifically, MDI+ generalizes MDI by allowing the analyst to replace the linear regression model and $R^2$ metric with regularized generalized linear models (GLMs) and metrics better suited for the given data structure. Moreover, MDI+ incorporates additional features to mitigate known biases of decision trees against additive or smooth models. We further provide guidance on how practitioners can choose an appropriate GLM and metric based upon the Predictability, Computability, Stability framework for veridical data science. Extensive data-inspired simulations show that MDI+ significantly outperforms popular feature importance measures in identifying signal features. We also apply MDI+ to two real-world case studies on drug response prediction and breast cancer subtype classification. We show that MDI+ extracts well-established predictive genes with significantly greater stability compared to existing feature importance measures. All code and models are released in a full-fledged python package on Github.
</details>
<details>
<summary>摘要</summary>
“ mean decrease in impurity (MDI) 是一个流行的特征重要度量表 для random forest (RF)。我们证明了 MDI 中的特征 $X_k$ 在每棵树中的 RF 相等于不调和的 $R^2$ 值在对应的决策探针中的线性回传模型中。我们使用这个解释来提出一个灵活的特征重要度框架called MDI+。 Specifically, MDI+ 将 MDI 扩展到让分析师可以更改线性回传模型和 $R^2$ 指标，并且包括额外的特征以减少决策树对添加或平滑模型的偏见。我们还提供适当的 GLM 和指标基于 Predictability, Computability, Stability 框架的指南。广泛的数据验证表明 MDI+ 可以对应用于特征重要度度量表示明显的提高。我们还应用 MDI+ 到了两个实际的应用案例：药物对应预测和乳癌类型分类。我们发现 MDI+ 可以提取稳定且有高预测力的遗传因素，较常用的特征重要度度量表示明显的更好。所有的代码和模型都可以在 GitHub 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Learning-ECG-signal-features-without-backpropagation"><a href="#Learning-ECG-signal-features-without-backpropagation" class="headerlink" title="Learning ECG signal features without backpropagation"></a>Learning ECG signal features without backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01930">http://arxiv.org/abs/2307.01930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Péter Pósfay, Marcell T. Kurbucz, Péter Kovács, Antal Jakovác</li>
<li>for: 这篇论文的目的是提出一种新的方法来生成时间序列数据的表示方式，以提高下游任务的效果、范围和可应用性。</li>
<li>methods: 该方法基于物理学的想法，通过数据驱动的方式构建一个减少的表示，同时能够捕捉数据的下面结构和任务特定信息，并且仍然保持易于理解、可读性和验证性。</li>
<li>results: 通过应用该方法于心跳信号分类任务，实现了状态首位表现。<details>
<summary>Abstract</summary>
Representation learning has become a crucial area of research in machine learning, as it aims to discover efficient ways of representing raw data with useful features to increase the effectiveness, scope and applicability of downstream tasks such as classification and prediction. In this paper, we propose a novel method to generate representations for time series-type data. This method relies on ideas from theoretical physics to construct a compact representation in a data-driven way, and it can capture both the underlying structure of the data and task-specific information while still remaining intuitive, interpretable and verifiable. This novel methodology aims to identify linear laws that can effectively capture a shared characteristic among samples belonging to a specific class. By subsequently utilizing these laws to generate a classifier-agnostic representation in a forward manner, they become applicable in a generalized setting. We demonstrate the effectiveness of our approach on the task of ECG signal classification, achieving state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
研究者们在机器学习领域内，尤其是在 Representation learning 方面，为了找到可以快速、高效地将原始数据转换为有用特征，以提高下游任务（如分类和预测）的效果、范围和可重用性。在这篇论文中，我们提出了一种新的方法，用于生成时间序列型数据的表示。这种方法基于物理学的想法，通过在数据驱动的方式下构建一个压缩表示，能够捕捉数据的下面结构和任务特定信息，同时仍然保持易于理解、可读性和可验证性。这种新的方法ology 目标是在特定类别中找到共同的特征，并通过这些法律生成一个批处器无关的表示，以便在总体上应用。我们在 ECG 信号分类任务中证明了我们的方法的效果，达到了领导性的表现。
</details></li>
</ul>
<hr>
<h2 id="ProtoDiffusion-Classifier-Free-Diffusion-Guidance-with-Prototype-Learning"><a href="#ProtoDiffusion-Classifier-Free-Diffusion-Guidance-with-Prototype-Learning" class="headerlink" title="ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning"></a>ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01924">http://arxiv.org/abs/2307.01924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gulcin Baykal, Halil Faruk Karagoz, Taha Binhuraib, Gozde Unal</li>
<li>for: 提高生成质量和稳定性，减少训练时间</li>
<li>methods:  integrate prototype learning into diffusion models</li>
<li>results: 在不同的数据集和实验设置下，成功实现更高的生成质量和更快的训练时间<details>
<summary>Abstract</summary>
Diffusion models are generative models that have shown significant advantages compared to other generative models in terms of higher generation quality and more stable training. However, the computational need for training diffusion models is considerably increased. In this work, we incorporate prototype learning into diffusion models to achieve high generation quality faster than the original diffusion model. Instead of randomly initialized class embeddings, we use separately learned class prototypes as the conditioning information to guide the diffusion process. We observe that our method, called ProtoDiffusion, achieves better performance in the early stages of training compared to the baseline method, signifying that using the learned prototypes shortens the training time. We demonstrate the performance of ProtoDiffusion using various datasets and experimental settings, achieving the best performance in shorter times across all settings.
</details>
<details>
<summary>摘要</summary>
Diffusion models 是一类生成模型，在生成质量和训练稳定性方面表现出了明显的优势。然而，训练 diffusion models 所需的计算资源增加了 considrably。在这种情况下，我们将 prototype learning 引入 diffusion models，以实现更高的生成质量和更快的训练速度。而不是使用随机初始化的类嵌入，我们使用分开学习的类prototype来导引diffusion过程。我们发现，我们的方法（即 ProtoDiffusion）在训练的早期阶段表现出了更好的性能，这表明使用学习的 prototype 可以缩短训练时间。我们通过不同的数据集和实验设置来证明 ProtoDiffusion 的性能，在所有设置下都达到了最佳性能，并且在更短的时间内完成。
</details></li>
</ul>
<hr>
<h2 id="ClimateLearn-Benchmarking-Machine-Learning-for-Weather-and-Climate-Modeling"><a href="#ClimateLearn-Benchmarking-Machine-Learning-for-Weather-and-Climate-Modeling" class="headerlink" title="ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling"></a>ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01909">http://arxiv.org/abs/2307.01909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aditya-grover/climate-learn">https://github.com/aditya-grover/climate-learn</a></li>
<li>paper_authors: Tung Nguyen, Jason Jewik, Hritik Bansal, Prakhar Sharma, Aditya Grover<br>for:* The paper is written to introduce an open-source PyTorch library called ClimateLearn for training and evaluating machine learning models in data-driven climate science.methods:* The library includes holistic pipelines for dataset processing, state-of-the-art deep learning models, and quantitative and qualitative evaluation for standard weather and climate modeling tasks.results:* The authors have performed comprehensive forecasting and downscaling experiments to showcase the capabilities and key features of their library, and to their knowledge, ClimateLearn is the first large-scale, open-source effort for bridging research in weather and climate modeling with modern machine learning systems.<details>
<summary>Abstract</summary>
Modeling weather and climate is an essential endeavor to understand the near- and long-term impacts of climate change, as well as inform technology and policymaking for adaptation and mitigation efforts. In recent years, there has been a surging interest in applying data-driven methods based on machine learning for solving core problems such as weather forecasting and climate downscaling. Despite promising results, much of this progress has been impaired due to the lack of large-scale, open-source efforts for reproducibility, resulting in the use of inconsistent or underspecified datasets, training setups, and evaluations by both domain scientists and artificial intelligence researchers. We introduce ClimateLearn, an open-source PyTorch library that vastly simplifies the training and evaluation of machine learning models for data-driven climate science. ClimateLearn consists of holistic pipelines for dataset processing (e.g., ERA5, CMIP6, PRISM), implementation of state-of-the-art deep learning models (e.g., Transformers, ResNets), and quantitative and qualitative evaluation for standard weather and climate modeling tasks. We supplement these functionalities with extensive documentation, contribution guides, and quickstart tutorials to expand access and promote community growth. We have also performed comprehensive forecasting and downscaling experiments to showcase the capabilities and key features of our library. To our knowledge, ClimateLearn is the first large-scale, open-source effort for bridging research in weather and climate modeling with modern machine learning systems. Our library is available publicly at https://github.com/aditya-grover/climate-learn.
</details>
<details>
<summary>摘要</summary>
模拟天气和气候是一项非常重要的努力，以便更好地理解气候变化的短期和长期影响，以及为适应和控制努力提供技术和政策。在过去几年中，有一种增长的兴趣在应用基于机器学习的数据驱动方法来解决气候科学中的核心问题，如天气预报和气候减小。然而，由于缺乏大规模、开源的努力，导致许多进步受到了限制，因为很多域科学家和人工智能研究者使用不一致或不够特定的数据集、训练setup和评估方法。我们介绍了一个名为ClimateLearn的开源PyTorch库，该库可以很大程度地简化天气预报和气候模型训练和评估的过程。ClimateLearn包括整体数据处理管道（如ERA5、CMIP6、PRISM）、现代深度学习模型（如转换器、径深网络）的实现，以及标准天气和气候模型计算任务的量化和质量评估。我们还提供了广泛的文档、贡献指南和快速入门教程，以扩大访问权限和促进社区增长。我们还执行了广泛的预测和减小实验，以示出库的能力和关键特点。到我们所知，ClimateLearn是首个大规模、开源的气候科学与现代机器学习系统之间的桥梁。我们的库可以在https://github.com/aditya-grover/climate-learn上获取。
</details></li>
</ul>
<hr>
<h2 id="Stability-Analysis-Framework-for-Particle-based-Distance-GANs-with-Wasserstein-Gradient-Flow"><a href="#Stability-Analysis-Framework-for-Particle-based-Distance-GANs-with-Wasserstein-Gradient-Flow" class="headerlink" title="Stability Analysis Framework for Particle-based Distance GANs with Wasserstein Gradient Flow"></a>Stability Analysis Framework for Particle-based Distance GANs with Wasserstein Gradient Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01879">http://arxiv.org/abs/2307.01879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuqi Chen, Yue Wu, Yang Xiang</li>
<li>For: 本研究 investigate the training process of generative networks that use particle-based distance as the objective function, such as MMD GAN, Cramér GAN, and EIEG GAN. However, these GANs often suffer from unstable training.* Methods: 我们 analyze the stability of the training process of these GANs from the perspective of probability density dynamics. We regard the discriminator $D$ as a feature transformation mapping and the generator $G$ as a random variable mapping. We use the Wasserstein gradient flow of the probability density function to perform stability analysis.* Results: 我们发现 that the training process of the discriminator is usually unstable due to the formulation of $\min_G \max_D E(G, D)$ in GANs. To address this issue, we add a stabilizing term in the discriminator loss function. We conduct experiments to validate our stability analysis and stabilizing method.<details>
<summary>Abstract</summary>
In this paper, we investigate the training process of generative networks that use a type of probability density distance named particle-based distance as the objective function, e.g. MMD GAN, Cram\'er GAN, EIEG GAN. However, these GANs often suffer from the problem of unstable training. In this paper, we analyze the stability of the training process of these GANs from the perspective of probability density dynamics. In our framework, we regard the discriminator $D$ in these GANs as a feature transformation mapping that maps high dimensional data into a feature space, while the generator $G$ maps random variables to samples that resemble real data in terms of feature space. This perspective enables us to perform stability analysis for the training of GANs using the Wasserstein gradient flow of the probability density function. We find that the training process of the discriminator is usually unstable due to the formulation of $\min_G \max_D E(G, D)$ in GANs. To address this issue, we add a stabilizing term in the discriminator loss function. We conduct experiments to validate our stability analysis and stabilizing method.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了生成网络在使用某种概率密度距离函数作为目标函数时的训练过程，例如MMD GAN、Cramér GAN、EIEG GAN。然而，这些GANs经常遇到训练不稳定的问题。在这篇论文中，我们从概率密度动力学的角度分析了这些GANs的训练过程的稳定性。我们认为权重网络$D$ acts as a feature transformation mapping that maps high-dimensional data into a feature space, while generator $G$ maps random variables to samples that resemble real data in terms of feature space.这种角度允许我们使用泊松流程来分析GANs的训练过程的稳定性。我们发现通常在GANs中的训练过程中，权重网络的训练是不稳定的，这是由于GANs中的$\min_G \max_D E(G, D)$的形式化引起的。为了解决这个问题，我们在权重网络的损失函数中添加了稳定化项。我们进行了实验来验证我们的稳定性分析和稳定化方法。
</details></li>
</ul>
<hr>
<h2 id="Fast-Private-Kernel-Density-Estimation-via-Locality-Sensitive-Quantization"><a href="#Fast-Private-Kernel-Density-Estimation-via-Locality-Sensitive-Quantization" class="headerlink" title="Fast Private Kernel Density Estimation via Locality Sensitive Quantization"></a>Fast Private Kernel Density Estimation via Locality Sensitive Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01877">http://arxiv.org/abs/2307.01877</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/talwagner/lsq">https://github.com/talwagner/lsq</a></li>
<li>paper_authors: Tal Wagner, Yonatan Naamad, Nina Mishra</li>
<li>for: efficient mechanisms for differentially private kernel density estimation (DP-KDE)</li>
<li>methods: Locality Sensitive Quantization (LSQ) framework, which leverages existing non-private KDE methods and privatizes them in a black-box manner</li>
<li>results: DP-KDE mechanisms that are fast and accurate on large datasets in both high and low dimensions, with linear time complexity in the number of dimensions $d$<details>
<summary>Abstract</summary>
We study efficient mechanisms for differentially private kernel density estimation (DP-KDE). Prior work for the Gaussian kernel described algorithms that run in time exponential in the number of dimensions $d$. This paper breaks the exponential barrier, and shows how the KDE can privately be approximated in time linear in $d$, making it feasible for high-dimensional data. We also present improved bounds for low-dimensional data.   Our results are obtained through a general framework, which we term Locality Sensitive Quantization (LSQ), for constructing private KDE mechanisms where existing KDE approximation techniques can be applied. It lets us leverage several efficient non-private KDE methods -- like Random Fourier Features, the Fast Gauss Transform, and Locality Sensitive Hashing -- and ``privatize'' them in a black-box manner. Our experiments demonstrate that our resulting DP-KDE mechanisms are fast and accurate on large datasets in both high and low dimensions.
</details>
<details>
<summary>摘要</summary>
我们研究高效的权限私钥频率概率密度估计（DP-KDE）机制。先前的工作对于 Gaussian kernel 提出了时间复杂度为对数函数($d$)的算法。这篇论文破坏了这个限制，并示出了在高维数据时间复杂度 linear 的 KDE  aproximation 机制，使得其成为可行的。我们还提供了低维数据的改进 bound。我们的结果基于一个通用的框架，我们称之为 Local Sensitive Quantization（LSQ），用于构建私钥 KDE 机制。它允许我们利用一些高效的非私钥 KDE 方法，如 Random Fourier Features、Fast Gauss Transform 和 Locality Sensitive Hashing，并将它们“黑盒”化，以实现私钥 KDE 机制。我们的实验表明，我们的 resulting DP-KDE 机制在大数据集上具有高速和高准确性。
</details></li>
</ul>
<hr>
<h2 id="Generalization-Guarantees-via-Algorithm-dependent-Rademacher-Complexity"><a href="#Generalization-Guarantees-via-Algorithm-dependent-Rademacher-Complexity" class="headerlink" title="Generalization Guarantees via Algorithm-dependent Rademacher Complexity"></a>Generalization Guarantees via Algorithm-dependent Rademacher Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02501">http://arxiv.org/abs/2307.02501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah Sachs, Tim van Erven, Liam Hodgkinson, Rajiv Khanna, Umut Simsekli</li>
<li>for: 本文旨在提供一种新的复杂度度量来控制通用化误差，用于现代机器学习算法。</li>
<li>methods: 本文使用了一种基于卷积函数的复杂度度量，并利用了这种度量的一些标准性质和各种各样的假设集合的结构，从而得到了一些新的一阶bounds。</li>
<li>results: 本文得到了一些新的一阶bounds，包括基于幂积函数的bounds和基于假设集合的稳定性的bounds。这些bounds可以扩展到连续函数空间中的函数类和压缩算法，并且比之前的方法更加简单和直观。<details>
<summary>Abstract</summary>
Algorithm- and data-dependent generalization bounds are required to explain the generalization behavior of modern machine learning algorithms. In this context, there exists information theoretic generalization bounds that involve (various forms of) mutual information, as well as bounds based on hypothesis set stability. We propose a conceptually related, but technically distinct complexity measure to control generalization error, which is the empirical Rademacher complexity of an algorithm- and data-dependent hypothesis class. Combining standard properties of Rademacher complexity with the convenient structure of this class, we are able to (i) obtain novel bounds based on the finite fractal dimension, which (a) extend previous fractal dimension-type bounds from continuous to finite hypothesis classes, and (b) avoid a mutual information term that was required in prior work; (ii) we greatly simplify the proof of a recent dimension-independent generalization bound for stochastic gradient descent; and (iii) we easily recover results for VC classes and compression schemes, similar to approaches based on conditional mutual information.
</details>
<details>
<summary>摘要</summary>
Algorithm-和数据-依赖的总结 bounds 是现代机器学习算法的总结行为的解释需要的。在这种情况下，存在信息理论性的总结 bounds，其中包括（不同形式的）相互信息，以及基于假设集的稳定性。我们提出了一个概念上相关，但技术上不同的复杂度测量来控制总结错误，即算法和数据依赖的假设集中的Empirical Rademacher complexity。通过将标准的Rademacher complexity性质与这种类型的概念结合，我们能够：（i）获得基于finite fractal dimension的新的 bounds，这些bounds（a）在前期的继承维度类型 bounds 中扩展到了有限假设类型，并（b）避免在先前的工作中需要的mutual information项;（ii）我们大大简化了最近的维度独立总结 bound for stochastic gradient descent;（iii）我们轻松地回归到VC类和压缩 schemes中的结果，与基于conditional mutual information的方法类似。
</details></li>
</ul>
<hr>
<h2 id="Approximate-Adapt-Anonymize-3A-a-Framework-for-Privacy-Preserving-Training-Data-Release-for-Machine-Learning"><a href="#Approximate-Adapt-Anonymize-3A-a-Framework-for-Privacy-Preserving-Training-Data-Release-for-Machine-Learning" class="headerlink" title="Approximate, Adapt, Anonymize (3A): a Framework for Privacy Preserving Training Data Release for Machine Learning"></a>Approximate, Adapt, Anonymize (3A): a Framework for Privacy Preserving Training Data Release for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01875">http://arxiv.org/abs/2307.01875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tamas Madl, Weijie Xu, Olivia Choudhury, Matthew Howard</li>
<li>for: 这篇论文目的是提高机器学习中的数据 Utility，同时保证 differential privacy。</li>
<li>methods: 本文提出了一个名为 3A (Approximate, Adapt, Anonymize) 的数据发布框架，以 maximize 数据 Utility，同时保证 differential privacy。</li>
<li>results: 实验结果显示，使用本文提出的方法可以实现高度的数据 Utility，并且与实际数据中的模型性能相似。 compared to state-of-the-art models, 本文的方法可以提高数据生成的分类性能。<details>
<summary>Abstract</summary>
The availability of large amounts of informative data is crucial for successful machine learning. However, in domains with sensitive information, the release of high-utility data which protects the privacy of individuals has proven challenging. Despite progress in differential privacy and generative modeling for privacy-preserving data release in the literature, only a few approaches optimize for machine learning utility: most approaches only take into account statistical metrics on the data itself and fail to explicitly preserve the loss metrics of machine learning models that are to be subsequently trained on the generated data. In this paper, we introduce a data release framework, 3A (Approximate, Adapt, Anonymize), to maximize data utility for machine learning, while preserving differential privacy. We also describe a specific implementation of this framework that leverages mixture models to approximate, kernel-inducing points to adapt, and Gaussian differential privacy to anonymize a dataset, in order to ensure that the resulting data is both privacy-preserving and high utility. We present experimental evidence showing minimal discrepancy between performance metrics of models trained on real versus privatized datasets, when evaluated on held-out real data. We also compare our results with several privacy-preserving synthetic data generation models (such as differentially private generative adversarial networks), and report significant increases in classification performance metrics compared to state-of-the-art models. These favorable comparisons show that the presented framework is a promising direction of research, increasing the utility of low-risk synthetic data release for machine learning.
</details>
<details>
<summary>摘要</summary>
“具有大量有用数据的可用性是成功机器学习的关键。然而，在包含敏感信息的领域中，发布高Utility数据以保护个人隐私是挑战。尽管在Literature中已有进步的泛化隐私和生成模型，但大多数方法只考虑数据本身的统计指标，并没有显式保持机器学习模型将要在生成数据上训练的损失指标。在这篇论文中，我们介绍了一个数据发布框架，称为3A（简化、适应、匿名），以最大化机器学习数据的有用性，同时保持泛化隐私。我们还描述了该框架的具体实现，利用混合模型简化数据，使用抽象点适应数据，并使用泛化隐私保护数据，以确保生成的数据具有隐私保护和高Utility。我们通过实验证明，在评估模型在真实数据上的性能时， Privatized 数据与实际数据的差异很小。我们还与一些隐私保护生成数据生成模型进行比较，并发现我们的结果具有显著的提高性，相比于当前的模型。这些有利的比较表明，我们提出的框架是一个有前途的研究方向，增加低风险的生成数据发布的机器学习 utility。”
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-machine-learning-framework-for-clad-characteristics-prediction-in-metal-additive-manufacturing"><a href="#A-hybrid-machine-learning-framework-for-clad-characteristics-prediction-in-metal-additive-manufacturing" class="headerlink" title="A hybrid machine learning framework for clad characteristics prediction in metal additive manufacturing"></a>A hybrid machine learning framework for clad characteristics prediction in metal additive manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01872">http://arxiv.org/abs/2307.01872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sinatayebati/cladnet-ml-for-am">https://github.com/sinatayebati/cladnet-ml-for-am</a></li>
<li>paper_authors: Sina Tayebati, Kyu Taek Cho<br>for:This paper aims to develop a hybrid approach that combines computational fluid dynamics (CFD) modeling and machine learning (ML) techniques to predict and understand the characteristics of metal additive manufacturing (MAM) printed clads.methods:The authors use a calibrated CFD model to generate a comprehensive dataset of clad characteristics, including geometry, quality, and processing parameters. They then employ two sets of processing parameters for training ML models, along with versatile ML models and reliable evaluation metrics, to create a scalable learning framework for predicting clad geometry and quality.results:The proposed hybrid approach resolves many challenges of conventional modeling methods in MAM by providing an efficient, accurate, and scalable platform for clad characteristics prediction and optimization. The authors demonstrate the effectiveness of their approach by using it to predict clad geometry and quality under different processing conditions.<details>
<summary>Abstract</summary>
During the past decade, metal additive manufacturing (MAM) has experienced significant developments and gained much attention due to its ability to fabricate complex parts, manufacture products with functionally graded materials, minimize waste, and enable low-cost customization. Despite these advantages, predicting the impact of processing parameters on the characteristics of an MAM printed clad is challenging due to the complex nature of MAM processes. Machine learning (ML) techniques can help connect the physics underlying the process and processing parameters to the clad characteristics. In this study, we introduce a hybrid approach which involves utilizing the data provided by a calibrated multi-physics computational fluid dynamic (CFD) model and experimental research for preparing the essential big dataset, and then uses a comprehensive framework consisting of various ML models to predict and understand clad characteristics. We first compile an extensive dataset by fusing experimental data into the data generated using the developed CFD model for this study. This dataset comprises critical clad characteristics, including geometrical features such as width, height, and depth, labels identifying clad quality, and processing parameters. Second, we use two sets of processing parameters for training the ML models: machine setting parameters and physics-aware parameters, along with versatile ML models and reliable evaluation metrics to create a comprehensive and scalable learning framework for predicting clad geometry and quality. This framework can serve as a basis for clad characteristics control and process optimization. The framework resolves many challenges of conventional modeling methods in MAM by solving t the issue of data scarcity using a hybrid approach and introducing an efficient, accurate, and scalable platform for clad characteristics prediction and optimization.
</details>
<details>
<summary>摘要</summary>
过去一个 décennie，金属添加itive制造（MAM）经历了重要的发展和引起了广泛关注，因为它可以制造复杂的部件，生产具有功能分布的材料的产品，最小化废弃物，并实现低成本定制。然而，预测MAM打印后皮层特性的影响因素是复杂的，因为MAM过程的自然特性。机器学习（ML）技术可以帮助将物理下面的过程和处理参数与皮层特性相连。在本研究中，我们提出了一种混合方法，利用实验室数据和CFD模型提供的数据来准备 essencial的大型数据集，然后使用包括多种ML模型的完整框架来预测和理解皮层特性。我们首先编辑了一个广泛的数据集，将实验室数据和CFD模型生成的数据融合在一起，这个数据集包括皮层特性的关键特征，如宽度、高度和深度，标签标识皮层质量，以及处理参数。第二，我们使用两组处理参数进行训练ML模型：机器设置参数和物理意识参数，以及多种可靠的ML模型和评价指标来建立一个全面、精准和可扩展的学习框架，用于预测皮层几何和质量。这个框架可以作为皮层特性控制和过程优化的基础。这个框架解决了传统模型方法在MAM中的多个挑战，例如数据不足问题，通过混合方法和引入高效、准确和可扩展的平台，以便预测皮层特性和优化过程。
</details></li>
</ul>
<hr>
<h2 id="Self-Consuming-Generative-Models-Go-MAD"><a href="#Self-Consuming-Generative-Models-Go-MAD" class="headerlink" title="Self-Consuming Generative Models Go MAD"></a>Self-Consuming Generative Models Go MAD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01850">http://arxiv.org/abs/2307.01850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, Richard G. Baraniuk</li>
<li>for: 本研究探讨了使用生成AI算法训练下一代模型时，自适应循环的特性。</li>
<li>methods: 我们使用当今最佳生成图像模型的三种家族来分析自适应循环的不同情况，包括在训练过程中是否有固定或新鲜的实际数据可用，以及模型是否受到偏见，以考虑数据质量和多样性之间的trade-off。</li>
<li>results: 我们发现，在没有充足的新鲜实际数据的情况下，自适应循环中的未来生成模型会逐渐减少精度或多样性。我们称这种情况为模型自适应疾病（MAD），与狂牛病相似。<details>
<summary>Abstract</summary>
Seismic advances in generative AI algorithms for imagery, text, and other data types has led to the temptation to use synthetic data to train next-generation models. Repeating this process creates an autophagous (self-consuming) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and in whether the samples from previous generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), making analogy to mad cow disease.
</details>
<details>
<summary>摘要</summary>
seized advances in generative AI algorithms for imagery, text, and other data types has led to the temptation to use synthetic data to train next-generation models. Repeating this process creates an autophagous (self-consuming) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and in whether the samples from previous generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), making analogy to mad cow disease.Here's the translation breakdown: seized (抓取) - advances autophagous (自食性) - loops generative (生成) - AI algorithms imagery (图像) - data types other (其他) - data typesNote that the word "mad" in the last sentence is not translated, as it is a metaphorical term and not a direct translation.
</details></li>
</ul>
<hr>
<h2 id="Crossway-Diffusion-Improving-Diffusion-based-Visuomotor-Policy-via-Self-supervised-Learning"><a href="#Crossway-Diffusion-Improving-Diffusion-based-Visuomotor-Policy-via-Self-supervised-Learning" class="headerlink" title="Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning"></a>Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01849">http://arxiv.org/abs/2307.01849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Varun Belagali, Jinghuan Shang, Michael S. Ryoo</li>
<li>for:  robot imitation learning</li>
<li>methods:  diffusion models, self-supervised learning (SSL) objective</li>
<li>results:  better representation for policy learning, especially when the demonstrations have different proficiencies.Here’s the full text in Simplified Chinese:</li>
<li>for:  robot imitation learning</li>
<li>methods:  diffusion models, self-supervised learning (SSL) objective</li>
<li>results:  更好的政策学习表示，特别是当示例具有不同水平的时候。<details>
<summary>Abstract</summary>
Sequence modeling approaches have shown promising results in robot imitation learning. Recently, diffusion models have been adopted for behavioral cloning, benefiting from their exceptional capabilities in modeling complex data distribution. In this work, we propose Crossway Diffusion, a method to enhance diffusion-based visuomotor policy learning by using an extra self-supervised learning (SSL) objective. The standard diffusion-based policy generates action sequences from random noise conditioned on visual observations and other low-dimensional states. We further extend this by introducing a new decoder that reconstructs raw image pixels (and other state information) from the intermediate representations of the reverse diffusion process, and train the model jointly using the SSL loss. Our experiments demonstrate the effectiveness of Crossway Diffusion in various simulated and real-world robot tasks, confirming its advantages over the standard diffusion-based policy. We demonstrate that such self-supervised reconstruction enables better representation for policy learning, especially when the demonstrations have different proficiencies.
</details>
<details>
<summary>摘要</summary>
sequence modeling方法在机器人模仿学习中显示了扎实的成果。最近，扩散模型在行为刻画中被采用，因为它们在处理复杂数据分布方面表现出色。在这种工作中，我们提议了跨度扩散（Crossway Diffusion），一种使用额外的自动学习（SSL）目标来增强扩散基于视 Motor 政策学习的方法。标准的扩散基于策略会根据随机噪声和视觉观察结果生成动作序列。我们进一步延伸了这种方法，通过引入一个新的解码器，将推 diffusion 过程中的中间表示重建为原始图像像素和其他状态信息，并在模型中同时使用 SSL 损失进行训练。我们的实验表明，跨度扩散在各种模拟和实际的机器人任务中具有优势，特别是当示例具有不同的技巧水平时。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Sample-Complexity-of-Neural-Network-Mixed-State-Reconstruction"><a href="#Empirical-Sample-Complexity-of-Neural-Network-Mixed-State-Reconstruction" class="headerlink" title="Empirical Sample Complexity of Neural Network Mixed State Reconstruction"></a>Empirical Sample Complexity of Neural Network Mixed State Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01840">http://arxiv.org/abs/2307.01840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haimeng Zhao, Giuseppe Carleo, Filippo Vicentini</li>
<li>for: 这个论文旨在研究量子状态重建技术，以减少实际应用中的量子极限复杂性。</li>
<li>methods: 该论文使用了不同的量子状态重建技术，包括变分减少技术，并对其进行了数值研究。</li>
<li>results: 研究发现，在温度有限的伊塞ING模型中，使用不同的量子状态重建技术可以系统地减少量子资源的需求。同时，比较了两种主要的量子 neural 状态编码，即量子扩散算符表示和正值算符测量表示，并发现它们在混合性的不同范围内表现不同。<details>
<summary>Abstract</summary>
Quantum state reconstruction using Neural Quantum States has been proposed as a viable tool to reduce quantum shot complexity in practical applications, and its advantage over competing techniques has been shown in numerical experiments focusing mainly on the noiseless case. In this work, we numerically investigate the performance of different quantum state reconstruction techniques for mixed states: the finite-temperature Ising model. We show how to systematically reduce the quantum resource requirement of the algorithms by applying variance reduction techniques. Then, we compare the two leading neural quantum state encodings of the state, namely, the Neural Density Operator and the positive operator-valued measurement representation, and illustrate their different performance as the mixedness of the target state varies. We find that certain encodings are more efficient in different regimes of mixedness and point out the need for designing more efficient encodings in terms of both classical and quantum resources.
</details>
<details>
<summary>摘要</summary>
量子状态重建使用神经量子状态已被提议为实际应用中减少量子射频复杂性的可能工具，并其优势于竞争技术在数字实验中得到了证明。在这项工作中，我们数字实验 investigate了不同量子状态重建技术的性能在杂态场景下：finite-temperature Ising模型。我们表明如何系统地减少量子资源需求的算法，并应用变差缓和技术。然后，我们比较了两种主要的神经量子状态编码方法， namely，神经激发函数和正值算符测量表示法，并示出它们在不同杂度水平下的不同性能。我们发现某些编码在不同的杂度范围内更高效，并指出了设计更高效的编码的需求，即类比和量子资源。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Score-Distillation-for-Consistent-Visual-Synthesis"><a href="#Collaborative-Score-Distillation-for-Consistent-Visual-Synthesis" class="headerlink" title="Collaborative Score Distillation for Consistent Visual Synthesis"></a>Collaborative Score Distillation for Consistent Visual Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04787">http://arxiv.org/abs/2307.04787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/subin-kim-cv/CSD">https://github.com/subin-kim-cv/CSD</a></li>
<li>paper_authors: Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, Jinwoo Shin</li>
<li>for: 提高文本到图像扩散模型的应用范围和可编辑性。</li>
<li>methods: 基于 Stein 变分Gradient Descent（SVGD）的 Collaborative Score Distillation（CSD）方法，通过考虑多个样本的分布来塑造图像集的共聚性。</li>
<li>results: 在各种任务中，如修改投影图像、视频和3D场景，CSD方法能够提高图像集之间的一致性，从而扩展文本到图像扩散模型的应用范围。<details>
<summary>Abstract</summary>
Generative priors of large-scale text-to-image diffusion models enable a wide range of new generation and editing applications on diverse visual modalities. However, when adapting these priors to complex visual modalities, often represented as multiple images (e.g., video), achieving consistency across a set of images is challenging. In this paper, we address this challenge with a novel method, Collaborative Score Distillation (CSD). CSD is based on the Stein Variational Gradient Descent (SVGD). Specifically, we propose to consider multiple samples as "particles" in the SVGD update and combine their score functions to distill generative priors over a set of images synchronously. Thus, CSD facilitates seamless integration of information across 2D images, leading to a consistent visual synthesis across multiple samples. We show the effectiveness of CSD in a variety of tasks, encompassing the visual editing of panorama images, videos, and 3D scenes. Our results underline the competency of CSD as a versatile method for enhancing inter-sample consistency, thereby broadening the applicability of text-to-image diffusion models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本转换为简化中文。<</SYS>>大规模文本到图像扩散模型的生成先验可以激发多种新的生成和编辑应用程序。然而，当应用这些先验到复杂的视觉模式时，保证一组图像之间的一致性是挑战。在这篇论文中，我们解决这个挑战方法是协同分数精灵（CSD）。CSD基于斯坦变分 Gradient Descent（SVGD）。我们建议将多个样本视为“粒子”在SVGD更新中，并将它们的分数函数组合以静止生成先验覆盖多个图像同步。因此，CSD使得多个图像之间的信息集成更加简单，从而实现了多个样本之间的视觉同步。我们在多种任务中展示了CSD的效果，包括修改广角图像、视频和3D场景。我们的结果表明CSD是一种多功能的方法，可以增强样本之间的一致性，从而扩大文本到图像扩散模型的应用范围。
</details></li>
</ul>
<hr>
<h2 id="DiT-3D-Exploring-Plain-Diffusion-Transformers-for-3D-Shape-Generation"><a href="#DiT-3D-Exploring-Plain-Diffusion-Transformers-for-3D-Shape-Generation" class="headerlink" title="DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation"></a>DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01831">http://arxiv.org/abs/2307.01831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DiT-3D/DiT-3D">https://github.com/DiT-3D/DiT-3D</a></li>
<li>paper_authors: Shentong Mo, Enze Xie, Ruihang Chu, Lewei Yao, Lanqing Hong, Matthias Nießner, Zhenguo Li</li>
<li>for: This paper is written for generating high-quality 3D point clouds using a novel Diffusion Transformer architecture, specifically designed for 3D shape generation.</li>
<li>methods: The paper proposes a novel Diffusion Transformer architecture called DiT-3D, which adapts the design philosophy of DiT but incorporates 3D positional and patch embeddings to adaptively aggregate input from voxelized point clouds. The paper also introduces 3D window attention to reduce computational cost in 3D shape generation.</li>
<li>results: The proposed DiT-3D achieves state-of-the-art performance in high-fidelity and diverse 3D point cloud generation on the ShapeNet dataset, with a 4.59 decrease in 1-Nearest Neighbor Accuracy and a 3.51 increase in Coverage metric compared to the state-of-the-art method.<details>
<summary>Abstract</summary>
Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful effectiveness in generating high-quality 2D images. However, it is still being determined whether the Transformer architecture performs equally well in 3D shape generation, as previous 3D diffusion methods mostly adopted the U-Net architecture. To bridge this gap, we propose a novel Diffusion Transformer for 3D shape generation, namely DiT-3D, which can directly operate the denoising process on voxelized point clouds using plain Transformers. Compared to existing U-Net approaches, our DiT-3D is more scalable in model size and produces much higher quality generations. Specifically, the DiT-3D adopts the design philosophy of DiT but modifies it by incorporating 3D positional and patch embeddings to adaptively aggregate input from voxelized point clouds. To reduce the computational cost of self-attention in 3D shape generation, we incorporate 3D window attention into Transformer blocks, as the increased 3D token length resulting from the additional dimension of voxels can lead to high computation. Finally, linear and devoxelization layers are used to predict the denoised point clouds. In addition, our transformer architecture supports efficient fine-tuning from 2D to 3D, where the pre-trained DiT-2D checkpoint on ImageNet can significantly improve DiT-3D on ShapeNet. Experimental results on the ShapeNet dataset demonstrate that the proposed DiT-3D achieves state-of-the-art performance in high-fidelity and diverse 3D point cloud generation. In particular, our DiT-3D decreases the 1-Nearest Neighbor Accuracy of the state-of-the-art method by 4.59 and increases the Coverage metric by 3.51 when evaluated on Chamfer Distance.
</details>
<details>
<summary>摘要</summary>
最近的扩散变换器（例如DiT）已经表现出了高质量的2D图像生成能力。然而，是否Transformer架构在3D形状生成中表现 similarly well，现在都是一个问题。因为前一些3D扩散方法主要采用U-Net架构。为了bridging这个差距，我们提出了一种新的3D扩散变换器，即DiT-3D，它可以直接对粗糙点云进行杂化处理，并使用平杂Transformers进行操作。相比现有的U-Net方法，我们的DiT-3D更加扩展性强，生成质量更高。具体来说，DiT-3D采用了Diffusion Transformer的设计哲学，但是将其修改为包括3D位置嵌入和补丁嵌入，以适应 voxelized点云的输入。为了降低3D形状生成中自我注意力的计算成本，我们引入了3D窗口注意力，并在Transformer块中应用。最后，我们使用线性和反粗糙层来预测净化后的点云。此外，我们的 transformer 架构支持高效的 fine-tuning 从2D到3D，其中预先训练的 DiT-2D  checkpoint 在 ImageNet 上可以显著提高 DiT-3D 的性能。实验结果表明，我们提出的 DiT-3D 在 ShapeNet 数据集上实现了状态可见的高精度和多样化3D点云生成。具体来说，我们的 DiT-3D 在1-Nearest Neighbor Accuracy 和 Coverage 指标上降低了state-of-the-art 方法的值，分别降低了4.59和3.51。
</details></li>
</ul>
<hr>
<h2 id="Deconstructing-Data-Reconstruction-Multiclass-Weight-Decay-and-General-Losses"><a href="#Deconstructing-Data-Reconstruction-Multiclass-Weight-Decay-and-General-Losses" class="headerlink" title="Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses"></a>Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01827">http://arxiv.org/abs/2307.01827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gon Buzaglo, Niv Haim, Gilad Yehudai, Gal Vardi, Yakir Oz, Yaniv Nikankin, Michal Irani</li>
<li>for: 这个研究的目的是探讨神经网络中训练样本的内存化现象，以及这种现象对神经网络的影响。</li>
<li>methods: 该研究使用了多层感知器和卷积神经网络进行重建训练样本的方法，并对不同的损失函数进行了探讨。</li>
<li>results: 研究发现，使用权重衰变 durante 训练可以提高神经网络的重建可能性，同时也影响了神经网络的性能。此外，研究还发现，在训练样本数量和神经网络neuron数量之间存在一定的关系。<details>
<summary>Abstract</summary>
Memorization of training data is an active research area, yet our understanding of the inner workings of neural networks is still in its infancy. Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples from multilayer perceptron binary classifiers, effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks. In this work, we extend their findings in several directions, including reconstruction from multiclass and convolutional neural networks. We derive a more general reconstruction scheme which is applicable to a wider range of loss functions such as regression losses. Moreover, we study the various factors that contribute to networks' susceptibility to such reconstruction schemes. Intriguingly, we observe that using weight decay during training increases reconstructability both in terms of quantity and quality. Additionally, we examine the influence of the number of neurons relative to the number of training samples on the reconstructability.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> neural network 的吸收训练数据是一个活跃的研究领域，然而我们对其内部工作的理解仍然处于初期阶段。最近，海沃特等人（2022）提出了一种方案，可以从多层感知器二分类网络中重建训练样本，Effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks。在这项工作中，我们将这些发现扩展到多类和卷积神经网络，并 derivate a more general reconstruction scheme 可以应用于更广泛的损失函数，如回归损失。此外，我们研究了不同因素对神经网络的重建性的影响，发现使用权重衰减 durante 训练可以提高重建性 both in terms of quantity and quality。此外，我们还研究了神经网络的 neurons 和训练样本的数量之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Structural-Balance-and-Random-Walks-on-Complex-Networks-with-Complex-Weights"><a href="#Structural-Balance-and-Random-Walks-on-Complex-Networks-with-Complex-Weights" class="headerlink" title="Structural Balance and Random Walks on Complex Networks with Complex Weights"></a>Structural Balance and Random Walks on Complex Networks with Complex Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01813">http://arxiv.org/abs/2307.01813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Tian, Renaud Lambiotte</li>
<li>for: This paper focuses on the study of complex-weighted networks, specifically investigating their structural and dynamical properties when the weight matrix is Hermitian.</li>
<li>methods: The authors use concepts from signed graphs to classify complex-weighted networks based on structural balance and explore the shared spectral properties within each type. They also apply the results to characterize the dynamics of random walks on these networks.</li>
<li>results: The paper shows that local consensus can be achieved asymptotically when the graph is structurally balanced, while global consensus will be obtained when it is strictly unbalanced. The authors also propose a spectral clustering algorithm and explore the performance of the algorithm on both synthetic and real networks.<details>
<summary>Abstract</summary>
Complex numbers define the relationship between entities in many situations. A canonical example would be the off-diagonal terms in a Hamiltonian matrix in quantum physics. Recent years have seen an increasing interest to extend the tools of network science when the weight of edges are complex numbers. Here, we focus on the case when the weight matrix is Hermitian, a reasonable assumption in many applications, and investigate both structural and dynamical properties of the complex-weighted networks. Building on concepts from signed graphs, we introduce a classification of complex-weighted networks based on the notion of structural balance, and illustrate the shared spectral properties within each type. We then apply the results to characterise the dynamics of random walks on complex-weighted networks, where local consensus can be achieved asymptotically when the graph is structurally balanced, while global consensus will be obtained when it is strictly unbalanced. Finally, we explore potential applications of our findings by generalising the notion of cut, and propose an associated spectral clustering algorithm. We also provide further characteristics of the magnetic Laplacian, associating directed networks to complex-weighted ones. The performance of the algorithm is verified on both synthetic and real networks.
</details>
<details>
<summary>摘要</summary>
复杂数字定义了实体之间的关系在许多情况下。一个典型的例子是量子物理中的哈密顿矩阵中的偏置项。过去几年，有越来越多的研究者想要扩展网络科学中的工具，当Weight of edges是复数时。我们在这里关注Hermitian矩阵的情况，这是许多应用中的合理假设。我们研究了复数权重网络的结构和动态特性，并基于签名图的概念引入了复数权重网络的分类。我们发现在不同类型的网络中，存在共同的 спектраль性质。然后，我们应用结果来描述复杂权重网络上Random walk的动态，当网络是结构均衡的时，本地协同可以在极限上 achievable，而全球协同则需要网络是严格不均衡的。最后，我们探讨了我们的发现的应用，包括通过扩展割的概念和相关的 спектраль划分算法。我们还提供了复杂 Laplacian的性能，将导向网络与复数权重网络相关联。我们的实验表明，我们的算法在 synthetic 和实际网络上都能够 достичь好的性能。
</details></li>
</ul>
<hr>
<h2 id="Capturing-Local-Temperature-Evolution-during-Additive-Manufacturing-through-Fourier-Neural-Operators"><a href="#Capturing-Local-Temperature-Evolution-during-Additive-Manufacturing-through-Fourier-Neural-Operators" class="headerlink" title="Capturing Local Temperature Evolution during Additive Manufacturing through Fourier Neural Operators"></a>Capturing Local Temperature Evolution during Additive Manufacturing through Fourier Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01804">http://arxiv.org/abs/2307.01804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangce Chen, Wenzhuo Xu, Martha Baldwin, Björn Nijhuis, Ton van den Boogaard, Noelia Grande Gutiérrez, Sneha Prabha Narra, Christopher McComb</li>
<li>for: 本研究旨在提高附加制造技术的性能，通过快速模拟热性能。</li>
<li>methods: 本文使用Fourier Neural Operator来捕捉加工过程中的本地温度演化。</li>
<li>results: 模型在numerical simulations中表现出高精度，并且可以在不同的几何体上保持通用性。In English, that would be:</li>
<li>for: The purpose of this research is to improve the performance of additive manufacturing technologies by quickly simulating thermal behavior.</li>
<li>methods: The paper uses Fourier Neural Operator to capture the local temperature evolution during the manufacturing process.</li>
<li>results: The model shows high accuracy in numerical simulations and maintains generalizability to different geometries.<details>
<summary>Abstract</summary>
High-fidelity, data-driven models that can quickly simulate thermal behavior during additive manufacturing (AM) are crucial for improving the performance of AM technologies in multiple areas, such as part design, process planning, monitoring, and control. However, the complexities of part geometries make it challenging for current models to maintain high accuracy across a wide range of geometries. Additionally, many models report a low mean square error (MSE) across the entire domain (part). However, in each time step, most areas of the domain do not experience significant changes in temperature, except for the heat-affected zones near recent depositions. Therefore, the MSE-based fidelity measurement of the models may be overestimated.   This paper presents a data-driven model that uses Fourier Neural Operator to capture the local temperature evolution during the additive manufacturing process. In addition, the authors propose to evaluate the model using the $R^2$ metric, which provides a relative measure of the model's performance compared to using mean temperature as a prediction. The model was tested on numerical simulations based on the Discontinuous Galerkin Finite Element Method for the Direct Energy Deposition process, and the results demonstrate that the model achieves high fidelity as measured by $R^2$ and maintains generalizability to geometries that were not included in the training process.
</details>
<details>
<summary>摘要</summary>
高精度、数据驱动的模型可以快速模拟附加制造过程中的热性能，这些模型在多个领域，如部件设计、过程规划、监测和控制方面，都有提高附加制造技术的表现。然而，部件的复杂 геометри Structure 使得当前的模型难以保持高精度 across 各种 geometries。此外，许多模型报告了 across 整个领域 ($part$) 的低 Mean Square Error ($MSE$)，但在每个时间步骤中，大多数领域并不经历 significannot 的温度变化，只有近 recent depositions 的热效应区域。因此，基于 $MSE$ 的模型准确性测试可能受到过度估计。本文提出了一种基于 Fourier Neural Operator 的数据驱动模型，用于捕捉附加制造过程中的本地温度演化。此外，作者们提议使用 $R^2$ 指标来评估模型的性能，$R^2$ 指标为模型的Relative 性能指标，可以与使用 Mean Temperature 作为预测的 $R^2$ 指标进行比较。模型在基于 Discontinuous Galerkin Finite Element Method 的数值 simulations 上进行测试，结果表明该模型在 $R^2$ 指标下达到了高准确性，并且可以在不包含在训练过程中的 geometry 上保持通用性。
</details></li>
</ul>
<hr>
<h2 id="Edge-aware-Multi-task-Network-for-Integrating-Quantification-Segmentation-and-Uncertainty-Prediction-of-Liver-Tumor-on-Multi-modality-Non-contrast-MRI"><a href="#Edge-aware-Multi-task-Network-for-Integrating-Quantification-Segmentation-and-Uncertainty-Prediction-of-Liver-Tumor-on-Multi-modality-Non-contrast-MRI" class="headerlink" title="Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI"></a>Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01798">http://arxiv.org/abs/2307.01798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojiao Xiao, Qinmin Hu, Guanghui Wang</li>
<li>for:  Liver tumor diagnosis and analysis</li>
<li>methods:  Multi-modality non-contrast magnetic resonance imaging (NCMRI) fusion, edge-aware feature aggregation module (EaFA), and multi-task learning</li>
<li>results:  Outperformed state-of-the-art methods with a dice similarity coefficient of 90.01$\pm$1.23 and a mean absolute error of 2.72$\pm$0.58 mm for MD.<details>
<summary>Abstract</summary>
Simultaneous multi-index quantification, segmentation, and uncertainty estimation of liver tumors on multi-modality non-contrast magnetic resonance imaging (NCMRI) are crucial for accurate diagnosis. However, existing methods lack an effective mechanism for multi-modality NCMRI fusion and accurate boundary information capture, making these tasks challenging. To address these issues, this paper proposes a unified framework, namely edge-aware multi-task network (EaMtNet), to associate multi-index quantification, segmentation, and uncertainty of liver tumors on the multi-modality NCMRI. The EaMtNet employs two parallel CNN encoders and the Sobel filters to extract local features and edge maps, respectively. The newly designed edge-aware feature aggregation module (EaFA) is used for feature fusion and selection, making the network edge-aware by capturing long-range dependency between feature and edge maps. Multi-tasking leverages prediction discrepancy to estimate uncertainty and improve segmentation and quantification performance. Extensive experiments are performed on multi-modality NCMRI with 250 clinical subjects. The proposed model outperforms the state-of-the-art by a large margin, achieving a dice similarity coefficient of 90.01$\pm$1.23 and a mean absolute error of 2.72$\pm$0.58 mm for MD. The results demonstrate the potential of EaMtNet as a reliable clinical-aided tool for medical image analysis.
</details>
<details>
<summary>摘要</summary>
simultanous多指标评估、分割和不确定度估计liver肿瘤在多Modal非contrast磁共振成像（NCMRI）中是诊断精准的关键。然而，现有方法缺乏有效的多Modal NCMRI融合机制和准确边界信息捕获机制，使这些任务变得困难。为解决这些问题，这篇论文提出了一个统一框架，即edge-aware多任务网络（EaMtNet），用于 associating multi-index评估、分割和不确定度估计liver肿瘤在多Modal NCMRI中。EaMtNet使用了两个并行的CNN Encoder和Sobel滤波器来提取本地特征和边图，分别。新设计的edge-aware特征聚合模块（EaFA）用于特征融合和选择，使网络变得edge-aware，捕捉特征和边图之间的长距离依赖关系。多任务利用预测差异来估计不确定度和提高分割和评估性能。广泛的实验在多Modal NCMRI上进行，涉及250名临床实验者。提出的模型在多Modal NCMRI中表现出色，达到了dice相似度系数90.01±1.23和平均绝对误差2.72±0.58mm for MD。结果表明EaMtNet可能成为一种可靠的临床辅助工具 для医学影像分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/05/cs.LG_2023_07_05/" data-id="clpahu74o00nt3h88857r8nr6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/eess.IV_2023_07_05/" class="article-date">
  <time datetime="2023-07-05T09:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/05/eess.IV_2023_07_05/">eess.IV - 2023-07-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dual-Arbitrary-Scale-Super-Resolution-for-Multi-Contrast-MRI"><a href="#Dual-Arbitrary-Scale-Super-Resolution-for-Multi-Contrast-MRI" class="headerlink" title="Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI"></a>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02334">http://arxiv.org/abs/2307.02334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jmzhang79/dual-arbnet">https://github.com/jmzhang79/dual-arbnet</a></li>
<li>paper_authors: Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, Yapeng Tian</li>
<li>for: 这篇论文旨在提高医疗成像领域中的磁共振成像（MRI）像素化，以提高医生在诊断和治疗时的可见度。</li>
<li>methods: 本研究使用了一种基于神经网络的多标示磁共振超解析（SR）重建方法，称为Dual-ArbNet，它可以在不同的标示模式下进行SR重建，并且可以处理不同的像素比例和分辨率。</li>
<li>results: 实验结果显示，Dual-ArbNet方法在两个公共MRI数据集上具有较高的SR性能，并且可以在不同的像素比例和分辨率下进行SR重建。此外，该方法还可以运用到临床实践中。<details>
<summary>Abstract</summary>
Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging (MRI) images from partial measurement is essential to medical imaging research. Benefiting from the diverse and complementary information of multi-contrast MR images in different imaging modalities, multi-contrast Super-Resolution (SR) reconstruction is promising to yield SR images with higher quality. In the medical scenario, to fully visualize the lesion, radiologists are accustomed to zooming the MR images at arbitrary scales rather than using a fixed scale, as used by most MRI SR methods. In addition, existing multi-contrast MRI SR methods often require a fixed resolution for the reference image, which makes acquiring reference images difficult and imposes limitations on arbitrary scale SR tasks. To address these issues, we proposed an implicit neural representations based dual-arbitrary multi-contrast MRI super-resolution method, called Dual-ArbNet. First, we decouple the resolution of the target and reference images by a feature encoder, enabling the network to input target and reference images at arbitrary scales. Then, an implicit fusion decoder fuses the multi-contrast features and uses an Implicit Decoding Function~(IDF) to obtain the final MRI SR results. Furthermore, we introduce a curriculum learning strategy to train our network, which improves the generalization and performance of our Dual-ArbNet. Extensive experiments in two public MRI datasets demonstrate that our method outperforms state-of-the-art approaches under different scale factors and has great potential in clinical practice.
</details>
<details>
<summary>摘要</summary>
限于快照系统，重建快照成像（MRI）图像从部分测量是医学成像研究中的关键。利用不同和补充的多比特MR成像模式的多比特超分辨（SR）重建可以获得更高质量的SR图像。在医疗场景下，为了全面显示肿瘤，辐射医生通常会在自定义的比例下缩放MR图像，而不是使用固定比例，这与大多数MRI SR方法不同。此外，现有的多比特MRI SR方法通常需要固定的参参图像分辨率，这使得获得参考图像困难，并对自定义比例SR任务带来限制。为解决这些问题，我们提出了基于卷积神经表示的双自由多比特MRI超分辨方法，称为Dual-ArbNet。首先，我们将目标和参考图像的分辨率解耦通过特征编码器，使网络可以输入自定义的目标和参考图像。然后，我们使用卷积叠加器将多比特特征进行卷积叠加，并使用偏函数IDF获取最终的MRI SR结果。此外，我们引入了课程学习策略来训练我们的网络，这有助于提高我们Dual-ArbNet的一般化和性能。广泛的实验在两个公共MRI数据集上表明，我们的方法在不同的比例因子下表现出色，有很好的潜在应用前景。
</details></li>
</ul>
<hr>
<h2 id="Joint-Hierarchical-Priors-and-Adaptive-Spatial-Resolution-for-Efficient-Neural-Image-Compression"><a href="#Joint-Hierarchical-Priors-and-Adaptive-Spatial-Resolution-for-Efficient-Neural-Image-Compression" class="headerlink" title="Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression"></a>Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02273">http://arxiv.org/abs/2307.02273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Ghorbel, Wassim Hamidouche, Luce Morin</li>
<li>for: 这篇论文是关于神经网络图像压缩（NIC）的研究，旨在提高NIC的性能，并且希望通过对Tranformer-based transform coding框架进行改进，以提高图像压缩的效率和质量。</li>
<li>methods: 本文使用Tranformer-based channel-wise auto-regressive prior模型来提高SwinT-ChARM的性能，并且添加了一个可学习的缩放模块来更好地提取更紧凑的缺失代码。</li>
<li>results: 实验结果表明，提出的框架可以在各种测试集上显著提高NIC的质量和效率，并且在与VVC参考编码器（VTM-18.0）和SwinT-ChARM神经编码器进行比较时，具有更好的质量和效率。<details>
<summary>Abstract</summary>
Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.
</details>
<details>
<summary>摘要</summary>
近些年，神经网络图像压缩（NIC）的性能已经逐渐提高，达到或超越传统编码器的状态元。 DESPITE 这些进步，当前的 NIC 方法仍然基于 ConvNet 来实现 entropy coding，受到本地连接性的限制，以及逐渐增加的建筑学偏好和先验，导致复杂的不够表现的模型和高解码延迟。 被Transformer 基于 transform coding 框架的 SwinT-ChARM 的效率调查所驱动，我们提议使用更直观而有效的 Tranformer 基于通道 wise auto-regressive prior 模型，以提高后者。通过我们的提议的 ICT，我们可以从 latent 表示中捕捉全局和局部上下文，更好地参数化归一化的量化 latent。此外，我们利用一个可学习的缩放模块，并在 ConvNeXt 基于的预处理/后处理器中使用它来准确地提取更紧凑的 latent 代码，并在重建更高质量的图像。对于一系列的 benchmark 数据集，我们进行了广泛的实验研究，并证明了我们的框架可以显著提高对 coding 效率和解码器复杂度的质量权衡。此外，我们还提供了模型缩放研究，以证明我们的方法的计算效率。最后，我们进行了一些对象和主观分析，以强调 AICT 与 SwinT-ChARM 之间的性能差距。
</details></li>
</ul>
<hr>
<h2 id="Direct-segmentation-of-brain-white-matter-tracts-in-diffusion-MRI"><a href="#Direct-segmentation-of-brain-white-matter-tracts-in-diffusion-MRI" class="headerlink" title="Direct segmentation of brain white matter tracts in diffusion MRI"></a>Direct segmentation of brain white matter tracts in diffusion MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02223">http://arxiv.org/abs/2307.02223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamza Kebiri, Ali Gholipour, Meritxell Bach Cuadra, Davood Karimi<br>for:白 matter  tracts 的 segmentation，即 brain 中各个区域之间的连接组织。methods:使用 deep learning 方法，直接从 diffusion MRI 数据中提取 white matter tracts。results: segmentation 精度与现有方法相当（mean Dice Similarity Coefficient 为 0.826），并且具有更高的普适性，可应用于低样本量的临床研究和不同的数据获取协议。<details>
<summary>Abstract</summary>
The brain white matter consists of a set of tracts that connect distinct regions of the brain. Segmentation of these tracts is often needed for clinical and research studies. Diffusion-weighted MRI offers unique contrast to delineate these tracts. However, existing segmentation methods rely on intermediate computations such as tractography or estimation of fiber orientation density. These intermediate computations, in turn, entail complex computations that can result in unnecessary errors. Moreover, these intermediate computations often require dense multi-shell measurements that are unavailable in many clinical and research applications. As a result, current methods suffer from low accuracy and poor generalizability. Here, we propose a new deep learning method that segments these tracts directly from the diffusion MRI data, thereby sidestepping the intermediate computation errors. Our experiments show that this method can achieve segmentation accuracy that is on par with the state of the art methods (mean Dice Similarity Coefficient of 0.826). Compared with the state of the art, our method offers far superior generalizability to undersampled data that are typical of clinical studies and to data obtained with different acquisition protocols. Moreover, we propose a new method for detecting inaccurate segmentations and show that it is more accurate than standard methods that are based on estimation uncertainty quantification. The new methods can serve many critically important clinical and scientific applications that require accurate and reliable non-invasive segmentation of white matter tracts.
</details>
<details>
<summary>摘要</summary>
脑白atter包括一组通过不同脑区域的脑 tract， segmentation 这些 tract 常需要在临床和研究实验中进行。Diffusion-weighted MRI 提供了一个唯一的对比，以定义这些 tract。然而，现有的 segmentation 方法通常需要中间计算，如 tractography 或 fibre orientation density 的估计。这些中间计算可能会导致多余的错误，并且常常需要 dense multi-shell measurements，这些 measurements 在许多临床和研究应用中不可得。因此，现有的方法受到低精度和差异化的限制。在这里，我们提出了一种新的深度学习方法，可以直接从 diffusion MRI 数据中分割 white matter tract，并且避免中间计算的错误。我们的实验表明，这种方法可以达到与现有方法相同的 segmentation 精度（mean Dice Similarity Coefficient 0.826）。相比之下，我们的方法在不同的数据采样和数据采集协议下具有更好的普适性。此外，我们还提出了一种新的方法来检测不准确的分割，并证明它比标准的方法更加准确。这些新方法可以为许多重要的临床和科学应用提供准确和可靠的非侵入式 white matter tract 的分割。
</details></li>
</ul>
<hr>
<h2 id="Compound-Attention-and-Neighbor-Matching-Network-for-Multi-contrast-MRI-Super-resolution"><a href="#Compound-Attention-and-Neighbor-Matching-Network-for-Multi-contrast-MRI-Super-resolution" class="headerlink" title="Compound Attention and Neighbor Matching Network for Multi-contrast MRI Super-resolution"></a>Compound Attention and Neighbor Matching Network for Multi-contrast MRI Super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02148">http://arxiv.org/abs/2307.02148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Chen, Sirui Wu, Shuai Wang, Zhongsen Li, Jia Yang, Huifeng Yao, Xiaomeng Li, Xiaolei Song</li>
<li>for: 这个论文旨在提出一种新的多模式磁共振成像超分辨（SR）网络架构，用于解决现有的SR方法在多模式磁共振成像中存在缺陷，如缺乏合适的参考特征和缺乏频率匹配。</li>
<li>methods: 该论文提出了一种基于自我注意力和邻居匹配的网络架构，称为CANM-Net，它使用复合自我注意力机制和邻居匹配模块来捕捉多模式磁共振成像中的相互依赖关系，并将参考特征和下落特征进行适应性匹配，以实现高质量的SR图像生成。</li>
<li>results: 该论文通过在IXI、fastMRI和实际扫描数据集上进行SR任务的实验，证明了CANM-Net在透彻和跨模式磁共振成像SR中具有优于现有方法的性能，并且在不当 registrations 的情况下仍然保持良好的表现，这表明其在临床应用中具有良好的潜力。<details>
<summary>Abstract</summary>
Multi-contrast magnetic resonance imaging (MRI) reflects information about human tissue from different perspectives and has many clinical applications. By utilizing the complementary information among different modalities, multi-contrast super-resolution (SR) of MRI can achieve better results than single-image super-resolution. However, existing methods of multi-contrast MRI SR have the following shortcomings that may limit their performance: First, existing methods either simply concatenate the reference and degraded features or exploit global feature-matching between them, which are unsuitable for multi-contrast MRI SR. Second, although many recent methods employ transformers to capture long-range dependencies in the spatial dimension, they neglect that self-attention in the channel dimension is also important for low-level vision tasks. To address these shortcomings, we proposed a novel network architecture with compound-attention and neighbor matching (CANM-Net) for multi-contrast MRI SR: The compound self-attention mechanism effectively captures the dependencies in both spatial and channel dimension; the neighborhood-based feature-matching modules are exploited to match degraded features and adjacent reference features and then fuse them to obtain the high-quality images. We conduct experiments of SR tasks on the IXI, fastMRI, and real-world scanning datasets. The CANM-Net outperforms state-of-the-art approaches in both retrospective and prospective experiments. Moreover, the robustness study in our work shows that the CANM-Net still achieves good performance when the reference and degraded images are imperfectly registered, proving good potential in clinical applications.
</details>
<details>
<summary>摘要</summary>
多模式磁共振成像（MRI）可以从不同角度获取人体组织信息，有广泛的临床应用。通过利用不同模式之间的共趋性信息，多模式超解析（SR）的MRI可以实现更好的结果，而存在的方法却有以下缺点：首先，现有方法可能会简单地 concatenate 参考和压缩特征，或者利用全局特征匹配，这些方法不适合多模式MRI SR。其次，虽然许多最新的方法使用 transformer 来捕捉空间维度的长距离依赖关系，但它们忽略了通道维度的自我注意力的重要性，这对低级视觉任务来说非常重要。为了解决这些缺点，我们提出了一种新的网络架构，即嵌入式自注意和邻居匹配网络（CANM-Net），用于多模式MRI SR：嵌入式自注意机制可以有效捕捉空间和通道维度之间的依赖关系；邻居特征匹配模块可以将压缩特征和相邻参考特征匹配并融合，以获得高质量的图像。我们在 IXI、fastMRI 和实际扫描数据集上进行 SR 任务的实验，CANM-Net 比 estado-of-the-art 方法在回顾和前瞻性实验中表现出色。此外，我们的 robustness 研究显示，CANM-Net 在参考和压缩图像不完美匹配时仍能保持良好的性能，这证明它在临床应用中具有良好的潜力。
</details></li>
</ul>
<hr>
<h2 id="A-Mini-Batch-Quasi-Newton-Proximal-Method-for-Constrained-Total-Variation-Nonlinear-Image-Reconstruction"><a href="#A-Mini-Batch-Quasi-Newton-Proximal-Method-for-Constrained-Total-Variation-Nonlinear-Image-Reconstruction" class="headerlink" title="A Mini-Batch Quasi-Newton Proximal Method for Constrained Total-Variation Nonlinear Image Reconstruction"></a>A Mini-Batch Quasi-Newton Proximal Method for Constrained Total-Variation Nonlinear Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02043">http://arxiv.org/abs/2307.02043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Hong, Thanh-an Pham, Irad Yavneh, Michael Unser</li>
<li>for: 这篇论文是关于计算成像，使用精确的物理模型来实现高质量重建的。</li>
<li>methods: 本文提出了一种基于强化随机批处理的准确非线性物理模型的计算成像方法，即mini-batch quasi-Newton proximal方法（BQNPM）。</li>
<li>results: 本文通过对三维反射问题进行实验和实际数据测试，证明BQNPM比ASPMs更快速地 converges，并且可以在计算成像中实现高质量的重建。<details>
<summary>Abstract</summary>
Over the years, computational imaging with accurate nonlinear physical models has drawn considerable interest due to its ability to achieve high-quality reconstructions. However, such nonlinear models are computationally demanding. A popular choice for solving the corresponding inverse problems is accelerated stochastic proximal methods (ASPMs), with the caveat that each iteration is expensive. To overcome this issue, we propose a mini-batch quasi-Newton proximal method (BQNPM) tailored to image-reconstruction problems with total-variation regularization. It involves an efficient approach that computes a weighted proximal mapping at a cost similar to that of the proximal mapping in ASPMs. However, BQNPM requires fewer iterations than ASPMs to converge. We assess the performance of BQNPM on three-dimensional inverse-scattering problems with linear and nonlinear physical models. Our results on simulated and real data show the effectiveness and efficiency of BQNPM,
</details>
<details>
<summary>摘要</summary>
随着时间的推移，计算成像技术已经吸引了广泛的关注，因为它可以实现高质量的重建。然而，这些非线性模型在计算上具有挑战性。一种受欢迎的解决方案是加速随机邻域方法（ASPMs），但每个迭代都是贵夫。为了解决这个问题，我们提议一种基于图像重建问题的权重贝叶斯方法（BQNPM）。这种方法具有计算贝叶斯映射的效率，但需要 fewer than ASPMs 的迭代次数才能达到 convergence。我们对三维反射问题进行了线性和非线性物理模型的测试，结果表明 BQNPM 的效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Joint-Recovery-of-T1-T2-and-Proton-Density-Maps-Using-a-Bayesian-Approach-with-Parameter-Estimation-and-Complementary-Undersampling-Patterns"><a href="#Joint-Recovery-of-T1-T2-and-Proton-Density-Maps-Using-a-Bayesian-Approach-with-Parameter-Estimation-and-Complementary-Undersampling-Patterns" class="headerlink" title="Joint Recovery of T1, T2* and Proton Density Maps Using a Bayesian Approach with Parameter Estimation and Complementary Undersampling Patterns"></a>Joint Recovery of T1, T2* and Proton Density Maps Using a Bayesian Approach with Parameter Estimation and Complementary Undersampling Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02015">http://arxiv.org/abs/2307.02015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Huang, James J. Lah, Jason W. Allen, Deqiang Qiu<br>for:  This paper aims to improve the quality of quantitative MR images recovered from undersampled measurements by incorporating the signal model of the variable-flip-angle (VFA) multi-echo 3D gradient-echo (GRE) method into the reconstruction of $T_1$, $T_2^*$, and proton density (PD) maps.methods:  The proposed approach is based on a probabilistic Bayesian formulation of the recovery problem, and uses approximate message passing with built-in parameter estimation (AMP-PE) to jointly recover distribution parameters, VFA multi-echo images, and $T_1$, $T_2^*$, and PD maps without the need for hyperparameter tuning.results:  The proposed AMP-PE approach outperforms the state-of-the-art $l1$-norm minimization approach in terms of reconstruction performance, and adopting complementary undersampling patterns across different flip angles and&#x2F;or echo times yields the best performance for $T_2^*$ and proton density mappings.<details>
<summary>Abstract</summary>
Purpose: To improve the quality of quantitative MR images recovered from undersampled measurements, we incorporate the signal model of the variable-flip-angle (VFA) multi-echo 3D gradient-echo (GRE) method into the reconstruction of $T_1$, $T_2^*$ and proton density (PD) maps. Additionally, we investigate the use of complementary undersampling patterns to determine optimal undersampling schemes for quantitative MRI.   Theory: We propose a probabilistic Bayesian formulation of the recovery problem. Our proposed approach, approximate message passing with built-in parameter estimation (AMP-PE), enables the joint recovery of distribution parameters, VFA multi-echo images, and $T_1$, $T_2^*$, and PD maps without the need for hyperparameter tuning.   Methods: We conducted both retrospective and prospective undersampling to obtain Fourier measurements using variable-density and Poisson-disk patterns. We investigated a variety of undersampling schemes, adopting complementary patterns across different flip angles and/or echo times.   Results: AMP-PE adopts a joint recovery strategy, it outperforms the state-of-the-art $l1$-norm minimization approach that follows a decoupled recovery strategy. For $T_1$ mapping, employing fixed sampling patterns across different echo times produced the best performance. Whereas for $T_2^*$ and proton density mappings, using complementary sampling patterns across different flip angles yielded the best performance.   Conclusion: AMP-PE achieves better performance by combining information from both the MR signal model and the sparse prior on VFA multi-echo images. It is equipped with automatic and adaptive parameter estimation, and works naturally with the clinical prospective undersampling scheme.
</details>
<details>
<summary>摘要</summary>
目的：提高 Undersampled 测量中的量子 MR 图像质量，我们在 reconstruction 中 incorporate 变量扭矩（VFA）多echo 3D 梯阶 echo（GRE）方法的信号模型。此外，我们还 investigate 使用 complementary 抽象样本来确定最佳的 Undersampling 方案。理论：我们提出了一种 Bayesian 形式的回归问题。我们的提议方法为 approximate message passing with built-in parameter estimation（AMP-PE），它可以同时回归分布参数、VFA multi-echo 图像和 $T_1$, $T_2^*$ 和 proton density（PD）图像，无需进行hyperparameter 调整。方法：我们在 retrospective 和 prospectively 抽象到 obtain Fourier 测量。我们 investigate 了不同的抽象方案，包括 variable-density 和 Poisson-disk 模式。结果：AMP-PE 采用了联合回归策略，其表现更好于 state-of-the-art $l1$-norm 最小化方法，后者采用了解 Coupled 回归策略。对 $T_1$ 图像，使用 fixes 抽象模式 across 不同的 echo times 得到了最佳性能。而对 $T_2^*$ 和 proton density 图像，使用 complementary 抽象模式 across 不同的扭矩 angles 得到了最佳性能。结论：AMP-PE 通过结合 MR 信号模型和 VFA multi-echo 图像的稀热先验来提高量子 MR 图像的质量。它具有自动和适应参数估计，并可以自然地与临床的前向抽象 schemes 结合。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Spectral-Demosaicing-with-Lightweight-Spectral-Attention-Networks"><a href="#Unsupervised-Spectral-Demosaicing-with-Lightweight-Spectral-Attention-Networks" class="headerlink" title="Unsupervised Spectral Demosaicing with Lightweight Spectral Attention Networks"></a>Unsupervised Spectral Demosaicing with Lightweight Spectral Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01990">http://arxiv.org/abs/2307.01990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Feng, Yongqiang Zhao, Seong G. Kong, Haijin Zeng</li>
<li>for: 这 paper 的目的是提出一种基于深度学习的无监督 spectral demosaicing 技术，以便在实际图像中进行高质量的颜色彩度恢复。</li>
<li>methods: 该 paper 使用了一种无监督学习的架构，包括提出了一种 mosaic loss function、模型结构、变换策略以及 early stopping 策略，这些组成了一个完整的无监督 spectral demosaicing 框架。</li>
<li>results: 对于实际图像，该 paper 的方法能够更好地抑制空间扭曲、保持 spectral 准确性、稳定性和计算成本，并且在 synthetic 和实际数据上进行了广泛的实验，得到了更高的性能。<details>
<summary>Abstract</summary>
This paper presents a deep learning-based spectral demosaicing technique trained in an unsupervised manner. Many existing deep learning-based techniques relying on supervised learning with synthetic images, often underperform on real-world images especially when the number of spectral bands increases. According to the characteristics of the spectral mosaic image, this paper proposes a mosaic loss function, the corresponding model structure, a transformation strategy, and an early stopping strategy, which form a complete unsupervised spectral demosaicing framework. A challenge in real-world spectral demosaicing is inconsistency between the model parameters and the computational resources of the imager. We reduce the complexity and parameters of the spectral attention module by dividing the spectral attention tensor into spectral attention matrices in the spatial dimension and spectral attention vector in the channel dimension, which is more suitable for unsupervised framework. This paper also presents Mosaic25, a real 25-band hyperspectral mosaic image dataset of various objects, illuminations, and materials for benchmarking. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method outperforms conventional unsupervised methods in terms of spatial distortion suppression, spectral fidelity, robustness, and computational cost.
</details>
<details>
<summary>摘要</summary>
One of the challenges in real-world spectral demosaicing is the inconsistency between the model parameters and the computational resources of the imager. To address this, the paper reduces the complexity and parameters of the spectral attention module by dividing the spectral attention tensor into spectral attention matrices in the spatial dimension and spectral attention vectors in the channel dimension. This is more suitable for an unsupervised framework.The paper also presents Mosaic25, a real 25-band hyperspectral mosaic image dataset of various objects, illuminations, and materials for benchmarking. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method outperforms conventional unsupervised methods in terms of spatial distortion suppression, spectral fidelity, robustness, and computational cost.Here is the Simplified Chinese translation of the text:这篇论文提出了一种基于深度学习的spectral demosaicing技术，该技术在无监督的情况下训练。现有的深度学习基于的技术 часто采用监督学习 Synthetic 图像，在实际图像中表现不佳，特别是随着频谱带数的增加。根据频谱拼接图像的特点，这篇论文提出了一种拼接损失函数、相应的模型结构、转换策略和早stopping策略，这些组成了一个完整的无监督 spectral demosaicing 框架。实际频谱拼接中的一个挑战是模型参数和捕获设备的计算资源之间的不一致。这篇论文通过将频谱注意力矩阵分割成空间维度的频谱注意力矩阵和通道维度的频谱注意力向量，来降低模型的复杂性和参数数量。这更适合无监督的框架。此外，这篇论文还提供了Mosaic25，一个实际25个频谱带的卷积合成图像数据集，包括不同的物体、照明和材料，用于对比。对于实际和 sintetic 数据集进行了广泛的实验，结果表明，提出的方法在隐藏扰乱、频谱准确、稳定性和计算成本方面都有较好的表现。
</details></li>
</ul>
<hr>
<h2 id="ToothSegNet-Image-Degradation-meets-Tooth-Segmentation-in-CBCT-Images"><a href="#ToothSegNet-Image-Degradation-meets-Tooth-Segmentation-in-CBCT-Images" class="headerlink" title="ToothSegNet: Image Degradation meets Tooth Segmentation in CBCT Images"></a>ToothSegNet: Image Degradation meets Tooth Segmentation in CBCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01979">http://arxiv.org/abs/2307.01979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxiang Liu, Tianxiang Hu, Yang Feng, Wanghui Ding, Zuozhu Liu</li>
<li>for: constructing three-dimensional tooth models in computer-assisted orthodontics</li>
<li>methods: 使用ToothSegNet框架，通过生成降低图像的信息来训练分割模型，并使用通道维度的混合来减少Encoder和Decoder之间的语义差异，以及通过结构约束损失来精细调整预测的牙齿形态</li>
<li>results: 比前一代医疗图像分割方法更高精度的牙齿分割结果<details>
<summary>Abstract</summary>
In computer-assisted orthodontics, three-dimensional tooth models are required for many medical treatments. Tooth segmentation from cone-beam computed tomography (CBCT) images is a crucial step in constructing the models. However, CBCT image quality problems such as metal artifacts and blurring caused by shooting equipment and patients' dental conditions make the segmentation difficult. In this paper, we propose ToothSegNet, a new framework which acquaints the segmentation model with generated degraded images during training. ToothSegNet merges the information of high and low quality images from the designed degradation simulation module using channel-wise cross fusion to reduce the semantic gap between encoder and decoder, and also refines the shape of tooth prediction through a structural constraint loss. Experimental results suggest that ToothSegNet produces more precise segmentation and outperforms the state-of-the-art medical image segmentation methods.
</details>
<details>
<summary>摘要</summary>
在计算机协助orthodontics中，三维牙齿模型是许多医疗治疗的关键 step。然而，CBCT图像质量问题，如机器设备和病人的牙科条件所导致的锈损和模糊，使 segmentation 变得更加困难。在这篇论文中，我们提出了 ToothSegNet，一个新的框架，通过在训练过程中对生成的受损图像进行准备，使 segmentation 模型更加熟悉受损图像的特征。ToothSegNet 通过核心混合来减少编码器和解码器之间的semantic gap，并通过结构约束损失来精细调整牙齿预测的形态。实验结果表明，ToothSegNet 可以生成更加精准的 segmentation，并超越了当前医学影像 segmentation 方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Millimeter-Wave-Reflectionless-Filters-Using-Advanced-Thin-Film-Fabrication"><a href="#Millimeter-Wave-Reflectionless-Filters-Using-Advanced-Thin-Film-Fabrication" class="headerlink" title="Millimeter-Wave Reflectionless Filters Using Advanced Thin-Film Fabrication"></a>Millimeter-Wave Reflectionless Filters Using Advanced Thin-Film Fabrication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01914">http://arxiv.org/abs/2307.01914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Morgan, Seng Loo, Tod Boyd, Miho Hunter</li>
<li>for: developing millimeter-wave, lumped-element reflectionless filters</li>
<li>methods: using advanced thin-film fabrication process with better than 2 μm feature size and integrated elements such as SiN Metal-Insulator-Metal (MIM) capacitors, bridges, and TaN Thin-Film Resistors (TFRs)</li>
<li>results: achieved higher frequency implementation than ever beforeHere’s the same information in Simplified Chinese text:</li>
<li>for: 开发毫米波、堆叠元件反射 filters</li>
<li>methods: 利用高精度薄膜制造过程，实现更高频率实现</li>
<li>results: 实现了历史上最高频率实现<details>
<summary>Abstract</summary>
We report on the development of millimeter-wave, lumped-element reflectionless filters using an advanced thin-film fabrication process. Based on previously demonstrated circuit topologies capable of achieving 50{\Omega} impedance match at all frequencies, these circuits have been implemented at higher frequencies than ever before by leveraging a thin-film process with better than 2 {\mu}m feature size and integrated elements such as SiN Metal-Insulator-Metal (MIM) capacitors, bridges, and TaN Thin-Film Resistors (TFRs).
</details>
<details>
<summary>摘要</summary>
我们报道了毫米波，积成元件反射性筛选器的发展，使用进步的薄膜制造过程。基于之前已经证明可以在所有频率上实现50Ω输Impedance匹配的电路结构，这些电路在以前没有达到过的高频范围内实现了，通过利用 better than 2μm的薄膜特性和集成元件 such as SiN Metal-Insulator-Metal (MIM) 电容器、桥和 TaN Thin-Film Resistors (TFRs)。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Deep-Learning-for-Model-Correction-in-the-Computational-Crystallography-Toolbox"><a href="#Self-Supervised-Deep-Learning-for-Model-Correction-in-the-Computational-Crystallography-Toolbox" class="headerlink" title="Self-Supervised Deep Learning for Model Correction in the Computational Crystallography Toolbox"></a>Self-Supervised Deep Learning for Model Correction in the Computational Crystallography Toolbox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01901">http://arxiv.org/abs/2307.01901</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gigantocypris/spread">https://github.com/gigantocypris/spread</a></li>
<li>paper_authors: Vidya Ganapati, Daniel Tchon, Aaron S. Brewster, Nicholas K. Sauter</li>
<li>for:  This paper aims to use the Computational Crystallography Toolbox (CCTBX) to determine the oxidation state of individual metal atoms in a macromolecule.</li>
<li>methods:  The paper uses self-supervised deep learning to correct the scientific model in CCTBX and provide uncertainty quantification.</li>
<li>results:  The paper describes the potential impact of using self-supervised deep learning to correct the scientific model in CCTBX and provide uncertainty quantification, and provides code for forward model simulation and data analysis at <a target="_blank" rel="noopener" href="https://github.com/gigantocypris/SPREAD.Here">https://github.com/gigantocypris/SPREAD.Here</a> is the text in Simplified Chinese:</li>
<li>for: 这篇论文使用Computational Crystallography Toolbox（CCTBX）确定蛋白质中金属原子的氧化状态。</li>
<li>methods: 这篇论文使用自我超vised深度学习修正CCTBX中的科学模型，并提供不确定性评估。</li>
<li>results: 这篇论文描述了使用自我超vised深度学习修正CCTBX中的科学模型的可能影响，并提供了<a target="_blank" rel="noopener" href="https://github.com/gigantocypris/SPREAD%E4%B8%AD%E7%9A%84%E4%BB%A3%E7%A0%81%E8%BF%9B%E8%A1%8C%E5%89%8D%E5%90%91%E6%A8%A1%E5%9E%8B%E4%BB%BF%E7%9C%9F%E5%92%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%82">https://github.com/gigantocypris/SPREAD中的代码进行前向模型仿真和数据分析。</a><details>
<summary>Abstract</summary>
The Computational Crystallography Toolbox (CCTBX) is open-source software that allows for processing of crystallographic data, including from serial femtosecond crystallography (SFX), for macromolecular structure determination. We aim to use the modules in CCTBX to determine the oxidation state of individual metal atoms in a macromolecule. Changes in oxidation state are reflected in small shifts of the atom's X-ray absorption edge. These energy shifts can be extracted from the diffraction images recorded in serial femtosecond crystallography, given knowledge of a forward physics model. However, as the diffraction changes only slightly due to the absorption edge shift, inaccuracies in the forward physics model make it extremely challenging to observe the oxidation state. In this work, we describe the potential impact of using self-supervised deep learning to correct the scientific model in CCTBX and provide uncertainty quantification. We provide code for forward model simulation and data analysis, built from CCTBX modules, at https://github.com/gigantocypris/SPREAD , which can be integrated with machine learning. We describe open questions in algorithm development to help spur advances through dialog between crystallographers and machine learning researchers. New methods could help elucidate charge transfer processes in many reactions, including key events in photosynthesis.
</details>
<details>
<summary>摘要</summary>
《计算 кристалagraphy工具箱（CCTBX）》是一款开源软件，用于处理晶体学数据，包括 serial femtosecond crystallography（SFX），以确定大分子结构。我们想使用 CCTBX 模块来确定杂谱中金属原子的氧化状态。氧化状态的变化会导致原子的 X-射线吸收边缘微小变化。这些能量差可以从 serial femtosecond crystallography 记录的 diffraction 图像中提取，只要知道前向物理学模型。然而，由于 diffraction 变化只是微小，因此错误在物理学模型中会导致非常困难地观察氧化状态。在这种情况下，我们描述了使用自适应深度学习来更正科学模型在 CCTBX 中的可能的影响，以及提供不确定性评估。我们在 GitHub 上提供了代码，包括 forward 物理学模型的 simulate 和数据分析，可以与机器学习结合使用。我们描述了在算法开发中的开问，以帮助推动进步，通过晶体学家和机器学习研究人员之间的对话。新的方法可以帮助解释生物化学中的电子传递过程，包括照明很重要的PhotoSynthesis 过程。
</details></li>
</ul>
<hr>
<h2 id="Grad-FEC-Unequal-Loss-Protection-of-Deep-Features-in-Collaborative-Intelligence"><a href="#Grad-FEC-Unequal-Loss-Protection-of-Deep-Features-in-Collaborative-Intelligence" class="headerlink" title="Grad-FEC: Unequal Loss Protection of Deep Features in Collaborative Intelligence"></a>Grad-FEC: Unequal Loss Protection of Deep Features in Collaborative Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01846">http://arxiv.org/abs/2307.01846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Korcan Uyanik, S. Faegheh Yeganli, Ivan V. Bajić</li>
<li>for: 提高edge设备和云端的智能合作系统的可靠性和Robustness，即Collaborative Intelligence（CI）系统。</li>
<li>methods: 提出了一种基于Unequal Loss Protection（ULP）的新方法，包括特征重要度估计器，以优先保护front-end生成的重要特征包。</li>
<li>results: 实验结果表明，提出的方法可以在 packet loss 的情况下显著提高CI系统的可靠性和Robustness。<details>
<summary>Abstract</summary>
Collaborative intelligence (CI) involves dividing an artificial intelligence (AI) model into two parts: front-end, to be deployed on an edge device, and back-end, to be deployed in the cloud. The deep feature tensors produced by the front-end are transmitted to the cloud through a communication channel, which may be subject to packet loss. To address this issue, in this paper, we propose a novel approach to enhance the resilience of the CI system in the presence of packet loss through Unequal Loss Protection (ULP). The proposed ULP approach involves a feature importance estimator, which estimates the importance of feature packets produced by the front-end, and then selectively applies Forward Error Correction (FEC) codes to protect important packets. Experimental results demonstrate that the proposed approach can significantly improve the reliability and robustness of the CI system in the presence of packet loss.
</details>
<details>
<summary>摘要</summary>
共同智能（CI）包括将人工智能（AI）模型分成两部分：前端，部署在边缘设备上，和后端，部署在云端。深度特征张量生成于前端将被传输到云端通过通信频道，该频道可能会出现包loss。为 Addressing this issue, in this paper, we propose a novel approach to enhance the resilience of the CI system in the presence of packet loss through Unequal Loss Protection (ULP). The proposed ULP approach involves a feature importance estimator, which estimates the importance of feature packets produced by the front-end, and then selectively applies Forward Error Correction (FEC) codes to protect important packets. Experimental results demonstrate that the proposed approach can significantly improve the reliability and robustness of the CI system in the presence of packet loss.Here's the breakdown of the translation:* 共同智能 (CI): Collaborative intelligence* 人工智能 (AI)：Artificial intelligence* 模型 (model): Model* 前端 (front-end): Front-end* 后端 (back-end): Back-end* 云端 (cloud): Cloud* 深度特征张量 (deep feature tensors): Deep feature tensors* 包loss (packet loss): Packet loss* 强化 (enhance): Enhance* 不平等损失保护 (ULP): Unequal loss protection* 特征重要性估计器 (feature importance estimator): Feature importance estimator* 前端生成的特征包 (feature packets produced by the front-end): Feature packets produced by the front-end* FEC (Forward Error Correction) 码：Forward error correction codes* 实验结果 (experimental results): Experimental results* 可以显著提高 (can significantly improve): Can significantly improve* 可靠性 (reliability): Reliability* Robustness: Robustness
</details></li>
</ul>
<hr>
<h2 id="Multi-Channel-Feature-Extraction-for-Virtual-Histological-Staining-of-Photon-Absorption-Remote-Sensing-Images"><a href="#Multi-Channel-Feature-Extraction-for-Virtual-Histological-Staining-of-Photon-Absorption-Remote-Sensing-Images" class="headerlink" title="Multi-Channel Feature Extraction for Virtual Histological Staining of Photon Absorption Remote Sensing Images"></a>Multi-Channel Feature Extraction for Virtual Histological Staining of Photon Absorption Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01824">http://arxiv.org/abs/2307.01824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marian Boktor, James E. D. Tweel, Benjamin R. Ecclestone, Jennifer Ai Ye, Paul Fieguth, Parsin Haji Reza<br>for: 这项研究旨在提高血液染色的效率和可靠性，以便在病理诊断中提供可靠的诊断信息，帮助病理医生在疾病分类、评估和治疗规划中做出更好的决策。methods: 该研究提出了一种基于深度学习的虚拟 histological 染色框架，使用 photon absorption remote sensing（PARS）图像进行特征提取，并使用一种变体的K-means方法来捕捉有价值的多模态信息。此外，该研究还提出了一种基于传统cycleGAN框架的多通道cycleGAN（MC-GAN）模型，以包括更多的特征。results: 实验结果表明，特定的特征组合可以超过传统通道的性能，并且可以提高虚拟染色结果与化学染色（H&amp;E）图像的吻合度。在人皮肤和mouse brain组织中应用，结果表明，选择最佳特征组合是关键，可以提高虚拟染色结果的可靠性和可视化质量。<details>
<summary>Abstract</summary>
Accurate and fast histological staining is crucial in histopathology, impacting diagnostic precision and reliability. Traditional staining methods are time-consuming and subjective, causing delays in diagnosis. Digital pathology plays a vital role in advancing and optimizing histology processes to improve efficiency and reduce turnaround times. This study introduces a novel deep learning-based framework for virtual histological staining using photon absorption remote sensing (PARS) images. By extracting features from PARS time-resolved signals using a variant of the K-means method, valuable multi-modal information is captured. The proposed multi-channel cycleGAN (MC-GAN) model expands on the traditional cycleGAN framework, allowing the inclusion of additional features. Experimental results reveal that specific combinations of features outperform the conventional channels by improving the labeling of tissue structures prior to model training. Applied to human skin and mouse brain tissue, the results underscore the significance of choosing the optimal combination of features, as it reveals a substantial visual and quantitative concurrence between the virtually stained and the gold standard chemically stained hematoxylin and eosin (H&E) images, surpassing the performance of other feature combinations. Accurate virtual staining is valuable for reliable diagnostic information, aiding pathologists in disease classification, grading, and treatment planning. This study aims to advance label-free histological imaging and opens doors for intraoperative microscopy applications.
</details>
<details>
<summary>摘要</summary>
准精准快的 Histological 染色是 Histopathology 中非常重要的，它直接影响诊断的准确性和可靠性。传统的染色方法需要较长的时间和主观的干预，导致诊断的延迟。数字化Patology 在提高和优化 Histology 过程中扮演着重要的角色，以提高效率和减少回转时间。本研究提出了一种基于深度学习的虚拟 Histological 染色方法，使用 photon absorption remote sensing（PARS）图像来提取特征。通过 variants of the K-means 方法提取 PARS 时间分解信号中的有价值多Modal 信息。提出的多通道 cycleGAN（MC-GAN）模型在传统 cycleGAN 框架上进行扩展，以包括额外的特征。实验结果表明，特定的特征组合能够超越传统渠道的表现，提高识别组织结构之前的标签。应用于人皮和 Mouse brain 组织样本，结果表明选择最佳特征组合非常重要，它可以提供较高的视觉和量化协调性，超过其他特征组合。准确的虚拟染色对诊断信息的可靠性至关重要，帮助病理学家在疾病分类、评分和治疗规划中做出更加准确的决策。本研究旨在提高无标签 Histological 成像，开启了Intraoperative 镜像应用的大门。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/05/eess.IV_2023_07_05/" data-id="clpahu7bm016w3h883hpr9j9n" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/cs.SD_2023_07_04/" class="article-date">
  <time datetime="2023-07-04T15:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/cs.SD_2023_07_04/">cs.SD - 2023-07-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Disentanglement-in-a-GAN-for-Unconditional-Speech-Synthesis"><a href="#Disentanglement-in-a-GAN-for-Unconditional-Speech-Synthesis" class="headerlink" title="Disentanglement in a GAN for Unconditional Speech Synthesis"></a>Disentanglement in a GAN for Unconditional Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01673">http://arxiv.org/abs/2307.01673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rf5/simple-asgan">https://github.com/rf5/simple-asgan</a></li>
<li>paper_authors: Matthew Baas, Herman Kamper</li>
<li>for: This paper is written for unconditional speech synthesis, specifically to learn a disentangled latent space for speech synthesis.</li>
<li>methods: The paper proposes a generative adversarial network (GAN) called AudioStyleGAN (ASGAN), which is tailored to learn a disentangled latent space for speech synthesis. The ASGAN model builds upon the StyleGAN family of image synthesis models, and it uses a modified adaptation of adaptive discriminator augmentation to successfully train the model.</li>
<li>results: The paper achieves state-of-the-art results in unconditional speech synthesis on the small-vocabulary Google Speech Commands digits dataset, and it is substantially faster than existing top-performing diffusion models. The paper also demonstrates that the ASGAN model’s latent space is disentangled, and that simple linear operations in the space can be used to perform several tasks unseen during training, such as voice conversion, speech enhancement, speaker verification, and keyword classification.<details>
<summary>Abstract</summary>
Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis. It is also substantially faster than existing top-performing diffusion models. We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training. Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks. Code, models, samples: https://github.com/RF5/simple-asgan/
</details>
<details>
<summary>摘要</summary>
可以开发一个模型，将真实的语音直接从潜在空间 sintesize，无需显式的条件？过去十年多的尝试仍然无法完成这一点，即使在小词库dataset上。为解决这个问题，我们提出了AudioStyleGAN（ASGAN）——一个类型为生成对抗网络的构成，用于无条件语音合成。ASGAN使用抽象的随机变量，将样本的噪声映射到一个分离的潜在空间中，然后将这个空间映射到一系列的语音特征，以抑制信号扩散。为了成功地训练ASGAN，我们导入了一些新的技术，包括修改适应性的检测器更新，以及在检测器更新中 probabilistically skips 检测器更新。我们将其应用于 Google Speech Commands 小词库dataset，实现了无条件语音合成的州际级结果，并且比现有的扩散模型更快。我们还证明了 ASGAN 的潜在空间是分离的：我们显示了在训练中没有看到的任务上，可以使用简单的直线运算来完成多个任务。 Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. 我们的工作表明，GANs 在无条件语音合成领域仍然非常竞争，并且可以使用分离的潜在空间来帮助泛化到未见到的任务。codes, models, samples可以在 https://github.com/RF5/simple-asgan/ 上找到。
</details></li>
</ul>
<hr>
<h2 id="Pretraining-Conformer-with-ASR-or-ASV-for-Anti-Spoofing-Countermeasure"><a href="#Pretraining-Conformer-with-ASR-or-ASV-for-Anti-Spoofing-Countermeasure" class="headerlink" title="Pretraining Conformer with ASR or ASV for Anti-Spoofing Countermeasure"></a>Pretraining Conformer with ASR or ASV for Anti-Spoofing Countermeasure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01546">http://arxiv.org/abs/2307.01546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikang Wang, Hiromitsu Nishizaki, Ming Li</li>
<li>for: 这个论文旨在提出一种基于Transformer的多级特征聚合嵌入器（MFA-Conformer）结构，用于音频反假护照（CM）。MFA-Conformer可以同时聚合全局和本地信息，从而帮助CM系统捕捉到假造的音频特征。</li>
<li>methods: 该论文提出了一种基于Conformer模型的转移学习方法，使得CM系统可以通过使用预训练的Conformer模型来增强其鲁棒性。此外，论文还提出了一种使用嵌入器融合多级特征的方法，以提高CM系统的抗误差性。</li>
<li>results: 实验结果表明，MFA-Conformer模型在清晰语音库（FAD）的清洁集上达到了0.038%的EER，远远超过了基eline。此外，转移学习方法在纯音频段上进行了有效的提升。<details>
<summary>Abstract</summary>
This paper introduces the Multi-scale Feature Aggregation Conformer (MFA-Conformer) structure for audio anti-spoofing countermeasure (CM). MFA-Conformer combines a convolutional neural networkbased on the Transformer, allowing it to aggregate global andlocal information. This may benefit the anti-spoofing CM system to capture the synthetic artifacts hidden both locally and globally. In addition, given the excellent performance of MFA Conformer on automatic speech recognition (ASR) and automatic speaker verification (ASV) tasks, we present a transfer learning method that utilizes pretrained Conformer models on ASR or ASV tasks to enhance the robustness of CM systems. The proposed method is evaluated on both Chinese and Englishs poofing detection databases. On the FAD clean set, the MFA-Conformer model pretrained on the ASR task achieves an EER of 0.038%, which dramatically outperforms the baseline. Moreover, experimental results demonstrate that proposed transfer learning method on Conformer is effective on pure speech segments after voice activity detection processing.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种基于Transformer的多级特征汇集声音防 spoofing countermeasure（MFA-Conformer）结构。MFA-Conformer结合了一个卷积神经网络，使其能够汇集全局和本地信息。这可能使防 spoofing CM 系统能够捕捉到假声音中的合成artefacts。此外，基于ASR和ASV任务的预训练Conformer模型的表现很出色，我们提出了一种在CM系统中使用这些预训练模型进行升级，以提高CM系统的Robustness。我们在中文和英文伪声检测数据库上评估了该方法。在FAD清洁集上，预训练MFA-Conformer模型在ASR任务上达到了EER值为0.038%，这在比基准值有很大的提升。此外，实验结果表明，在声音段后的语音活动检测处理后，提出的传输学习方法对Conformer是有效的。
</details></li>
</ul>
<hr>
<h2 id="Spatial-temporal-Graph-Based-Multi-channel-Speaker-Verification-With-Ad-hoc-Microphone-Arrays"><a href="#Spatial-temporal-Graph-Based-Multi-channel-Speaker-Verification-With-Ad-hoc-Microphone-Arrays" class="headerlink" title="Spatial-temporal Graph Based Multi-channel Speaker Verification With Ad-hoc Microphone Arrays"></a>Spatial-temporal Graph Based Multi-channel Speaker Verification With Ad-hoc Microphone Arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01386">http://arxiv.org/abs/2307.01386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijiang Chen, Chengdong Liang, Xiao-Lei Zhang</li>
<li>for: 这篇论文目的是提高杂音环境中的多渠道话语识别率。</li>
<li>methods: 这篇论文使用的方法包括一个特性聚合块和一个通道选择块，两者都是基于图形。特性聚合块将不同时间和通道的话者特征聚合，使用空间时间图形对于多渠道话语识别。通道选择块则排除了可能对系统造成负面影响的杂音通道。</li>
<li>results: 实验结果显示，提案的方法与六种代表性方法相比，在实验数据中提供了15.39%的相对平均错误率（EER）下降，并在模拟数据中提供了17.70%的相对平均错误率下降。此外，其性能也具有不同讯号对比度和复响时间的Robustness。<details>
<summary>Abstract</summary>
The performance of speaker verification degrades significantly in adverse acoustic environments with strong reverberation and noise. To address this issue, this paper proposes a spatial-temporal graph convolutional network (GCN) method for the multi-channel speaker verification with ad-hoc microphone arrays. It includes a feature aggregation block and a channel selection block, both of which are built on graphs. The feature aggregation block fuses speaker features among different time and channels by a spatial-temporal GCN. The graph-based channel selection block discards the noisy channels that may contribute negatively to the system. The proposed method is flexible in incorporating various kinds of graphs and prior knowledge. We compared the proposed method with six representative methods in both real-world and simulated environments.   Experimental results show that the proposed method achieves a relative equal error rate (EER) reduction of $\mathbf{15.39\%}$ lower than the strongest referenced method in the simulated datasets, and $\mathbf{17.70\%}$ lower than the latter in the real datasets. Moreover, its performance is robust across different signal-to-noise ratios and reverberation time.
</details>
<details>
<summary>摘要</summary>
“对于具有强 reverberation 和噪声的恶劣类比测试环境， speaker verification 的性能会显著下降。为了解决这个问题，这篇论文提出了一个使用多条件参数的Graph Convolutional Network（GCN）方法，来进行多条件speaker verification。这个方法包括一个网格汇整块和一个网格选择块，它们都是基于图。网格汇整块会融合不同时间和通道的Speaker feature，通过对图进行汇整。网格基于的通道选择块将潜在干扰通道排除，以避免降低系统性能。提案的方法可以灵活地应用多种图和专业知识。我们与六种代表性方法进行比较，结果显示，提案的方法在实际和模拟环境中均有15.39%和17.70%的相对平均错误率（EER）下降，并且在不同的讯号载度和复合时间下保持稳定性。”
</details></li>
</ul>
<hr>
<h2 id="Semantic-enrichment-towards-efficient-speech-representations"><a href="#Semantic-enrichment-towards-efficient-speech-representations" class="headerlink" title="Semantic enrichment towards efficient speech representations"></a>Semantic enrichment towards efficient speech representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01323">http://arxiv.org/abs/2307.01323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaëlle Laperrière, Ha Nguyen, Sahar Ghannay, Bassam Jabaian, Yannick Estève</li>
<li>for: 本研究的目的是提高 spoken language understanding 任务中的 semantic extraction，并且考虑 computation costs。</li>
<li>methods: 本研究使用 SAMU-XLSR 模型，通过特有的域内semantic enrichment来增强多语言Speech representation。同时，我们还使用 same-domain French和Italian benchmarks 来提高 low-resource language 的可移植性，以及 explore cross-domain capacities of the enriched SAMU-XLSR。</li>
<li>results: 本研究表明，通过特有的域内semantic enrichment，可以提高 spoken language understanding 任务中的 semantic extraction性能，同时也可以提高 low-resource language 的可移植性。<details>
<summary>Abstract</summary>
Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.
</details>
<details>
<summary>摘要</summary>
在过去几年，自我超级学习的发音表示 emerged 作为解决 spoken language understanding (SLU) 任务的有用替代方案。同时，基于庞大文本数据的多语言模型被引入，以编码语言不受限制的 semantics。最近，SAMU-XLSR 方法引入了使用文本模型增强多语言speech表示的方法。通过寻求在具有挑战性的 SLU 任务中更好地EXTRACT semantic information和考虑计算成本，本研究探讨了特定领域内Semantic enhancement of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task。此外，我们还展示了使用 same-domain French and Italian benchmarks 的低资源语言可移植性和跨领域 capacities of the enriched SAMU-XLSR.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/04/cs.SD_2023_07_04/" data-id="clpahu77d00vm3h887n9o569b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/cs.CV_2023_07_04/" class="article-date">
  <time datetime="2023-07-04T13:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/cs.CV_2023_07_04/">cs.CV - 2023-07-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Localized-Data-Work-as-a-Precondition-for-Data-Centric-ML-A-Case-Study-of-Full-Lifecycle-Crop-Disease-Identification-in-Ghana"><a href="#Localized-Data-Work-as-a-Precondition-for-Data-Centric-ML-A-Case-Study-of-Full-Lifecycle-Crop-Disease-Identification-in-Ghana" class="headerlink" title="Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana"></a>Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01767">http://arxiv.org/abs/2307.01767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darlington Akogo, Issah Samori, Cyril Akafia, Harriet Fiagbor, Andrews Kangah, Donald Kwame Asiedu, Kwabena Fuachie, Luis Oala</li>
<li>for: 这项研究的目的是提高农业生产力和食品安全。</li>
<li>methods: 该研究使用无人机采集数据和机器学习算法来确定作物受到的压力。</li>
<li>results: 研究组共同开发了数据、模型和应用程序，并将其提供给当地农民 via 桌面应用程序。<details>
<summary>Abstract</summary>
The Ghana Cashew Disease Identification with Artificial Intelligence (CADI AI) project demonstrates the importance of sound data work as a precondition for the delivery of useful, localized datacentric solutions for public good tasks such as agricultural productivity and food security. Drone collected data and machine learning are utilized to determine crop stressors. Data, model and the final app are developed jointly and made available to local farmers via a desktop application.
</details>
<details>
<summary>摘要</summary>
《加纳核桃疾病识别用人工智能（CADI AI）项目》表明了准确的数据工作的重要性，作为地方化数据驱动解决方案的先决条件，以提高农业生产力和食品安全。在该项目中，用扫描机采集的数据和机器学习算法来确定作物压力。数据、模型和最终应用程序都是在桌面应用程序上开发的，并且提供给当地农民使用。
</details></li>
</ul>
<hr>
<h2 id="Pretraining-is-All-You-Need-A-Multi-Atlas-Enhanced-Transformer-Framework-for-Autism-Spectrum-Disorder-Classification"><a href="#Pretraining-is-All-You-Need-A-Multi-Atlas-Enhanced-Transformer-Framework-for-Autism-Spectrum-Disorder-Classification" class="headerlink" title="Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification"></a>Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01759">http://arxiv.org/abs/2307.01759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lugges991/metaformer">https://github.com/lugges991/metaformer</a></li>
<li>paper_authors: Lucas Mahler, Qi Wang, Julius Steiglechner, Florian Birk, Samuel Heczko, Klaus Scheffler, Gabriele Lohmann</li>
<li>for: 这个研究旨在提出一个新的多几个地图扩展器架构（METAFormer），用于抑制Autism Spectrum Disorder（ASD）分类。</li>
<li>methods: 这个架构使用了休息状态功能磁共振成像资料，并使用多几个地图方法，包括AAL、CC200和DOS160地图，以进行自我超vised pretraining。</li>
<li>results: 这个研究显示，METAFormer可以在ABIDE I dataset上超过现有的州���状态表现，具有83.7%的精度和0.832的AUC分数。<details>
<summary>Abstract</summary>
Autism spectrum disorder (ASD) is a prevalent psychiatric condition characterized by atypical cognitive, emotional, and social patterns. Timely and accurate diagnosis is crucial for effective interventions and improved outcomes in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced Transformer framework, METAFormer, ASD classification. Our framework utilizes resting-state functional magnetic resonance imaging data from the ABIDE I dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer employs a multi-atlas approach, where flattened connectivity matrices from the AAL, CC200, and DOS160 atlases serve as input to the transformer encoder. Notably, we demonstrate that self-supervised pretraining, involving the reconstruction of masked values from the input, significantly enhances classification performance without the need for additional or separate training data. Through stratified cross-validation, we evaluate the proposed framework and show that it surpasses state-of-the-art performance on the ABIDE I dataset, with an average accuracy of 83.7% and an AUC-score of 0.832. The code for our framework is available at https://github.com/Lugges991/METAFormer
</details>
<details>
<summary>摘要</summary>
“对于自闭症 спектル中的诊断，时间和准确性都是非常重要的。在这个研究中，我们提出了一个新的多几个 Atlases 增强 Transformer 框架，METAFormer，用于自闭症分类。我们的框架使用了 AAL、CC200 和 DOS160 的 Atlases，将其融合为一个入口，并将其传递给 Transformer Encoder。我们还证明了，不需要额外训练数据，通过自我预 обу的方法，可以很好地增强分类性能。通过阶层验证，我们评估了我们的框架，并发现它在 ABIDE I 数据集上超过了现有的州度数据，具有83.7% 的精度和 0.832 的 AUC 得分。我们的代码可以在 GitHub 上找到：https://github.com/Lugges991/METAFormer。”
</details></li>
</ul>
<hr>
<h2 id="K-complex-Detection-Using-Fourier-Spectrum-Analysis-In-EEG"><a href="#K-complex-Detection-Using-Fourier-Spectrum-Analysis-In-EEG" class="headerlink" title="K-complex Detection Using Fourier Spectrum Analysis In EEG"></a>K-complex Detection Using Fourier Spectrum Analysis In EEG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01754">http://arxiv.org/abs/2307.01754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexey Protopopov</li>
<li>for:  automatic K-complex detection in EEG records</li>
<li>methods:  based on fast Fourier transform, not using neural networks</li>
<li>results:  comparable or superior quality to previous methods, including those using neural networks, with less computational power required.<details>
<summary>Abstract</summary>
K-complexes are an important marker of brain activity and are used both in clinical practice to perform sleep scoring, and in research. However, due to the size of electroencephalography (EEG) records, as well as the subjective nature of K-complex detection performed by somnologists, it is reasonable to automate K-complex detection. Previous works in this field of research have relied on the values of true positive rate and false positive rate to quantify the effectiveness of proposed methods, however this set of metrics may be misleading. The objective of the present research is to find a more accurate set of metrics and use them to develop a new method of K-complex detection, which would not rely on neural networks. Thus, the present article proposes two new methods for K-complex detection based on the fast Fourier transform. The results achieved demonstrated that the proposed methods offered a quality of K-complex detection that is either similar or superior to the quality of the methods demonstrated in previous works, including the methods employing neural networks, while requiring less computational power, meaning that K-complex detection does not require the use of neural networks. The proposed methods were evaluated using a new set of metrics, which is more representative of the quality of K-complex detection.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SRCD-Semantic-Reasoning-with-Compound-Domains-for-Single-Domain-Generalized-Object-Detection"><a href="#SRCD-Semantic-Reasoning-with-Compound-Domains-for-Single-Domain-Generalized-Object-Detection" class="headerlink" title="SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection"></a>SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01750">http://arxiv.org/abs/2307.01750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhijie Rao, Jingcai Guo, Luyao Tang, Yue Huang, Xinghao Ding, Song Guo</li>
<li>for: 本文提出了一种单域泛化物体检测（Single-DGOD）框架，旨在学习和维护自增强样本的 semantic 结构，以提高模型的泛化能力。</li>
<li>methods: 我们提出的 SRCD 包括两个主要组成部分： texture-based self-augmentation (TBSA) 模块和 local-global semantic reasoning (LGSR) 模块。 TBSA 模块通过自适应增强来消除影响标签的不相关特征，如光、阴影、颜色等。而 LGSR 模块则用于进一步模型实例特征之间的semantic关系，以暴露和维护内在的 semantic 结构。</li>
<li>results: 我们在多个 benchmark 上进行了广泛的实验，证明了我们提出的 SRCD 的效果。<details>
<summary>Abstract</summary>
This paper provides a novel framework for single-domain generalized object detection (i.e., Single-DGOD), where we are interested in learning and maintaining the semantic structures of self-augmented compound cross-domain samples to enhance the model's generalization ability. Different from DGOD trained on multiple source domains, Single-DGOD is far more challenging to generalize well to multiple target domains with only one single source domain. Existing methods mostly adopt a similar treatment from DGOD to learn domain-invariant features by decoupling or compressing the semantic space. However, there may have two potential limitations: 1) pseudo attribute-label correlation, due to extremely scarce single-domain data; and 2) the semantic structural information is usually ignored, i.e., we found the affinities of instance-level semantic relations in samples are crucial to model generalization. In this paper, we introduce Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. Specifically, our SRCD contains two main components, namely, the texture-based self-augmentation (TBSA) module, and the local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the effects of irrelevant attributes associated with labels, such as light, shadow, color, etc., at the image level by a light-yet-efficient self-augmentation. Moreover, LGSR is used to further model the semantic relationships on instance features to uncover and maintain the intrinsic semantic structures. Extensive experiments on multiple benchmarks demonstrate the effectiveness of the proposed SRCD.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的单域总体化对象检测框架（即Single-DGOD），我们在这种框架中学习和维护自适应增强的混合域样本的 semantic structure，以提高模型的通用能力。与多源域DGOD相比，单域DGOD更加具有挑战性，因为只有一个源域，因此模型难以通用到多个目标域。现有方法通常采用类似于DGOD的方法，即学习域无关特征，通过减少或压缩 semantic space来实现。但是，存在两个潜在的限制：1） Pseudo attribute-label correlation，由于单域数据非常稀缺; 2）模型忽略了实例水平的semantic structural information，即 samples中实例之间的semantic关系的强度是对模型泛化的关键。在本文中，我们提出了Semantic Reasoning with Compound Domains（SRCD）模型，其包括两个主要组件：texture-based self-augmentation（TBSA）模块和local-global semantic reasoning（LGSR）模块。TBSA模块通过快速和高效地自适应来消除与标签相关的不相关特征，如光、阴影、颜色等。而LGSR模块则用于进一步模型实例水平的semantic关系，以uncover和维护内在的semantic结构。我们在多个 benchmark上进行了广泛的实验，并证明了我们提出的SRCD的效果。
</details></li>
</ul>
<hr>
<h2 id="Ben-ge-Extending-BigEarthNet-with-Geographical-and-Environmental-Data"><a href="#Ben-ge-Extending-BigEarthNet-with-Geographical-and-Environmental-Data" class="headerlink" title="Ben-ge: Extending BigEarthNet with Geographical and Environmental Data"></a>Ben-ge: Extending BigEarthNet with Geographical and Environmental Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01741">http://arxiv.org/abs/2307.01741</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsg-aiml/ben-ge">https://github.com/hsg-aiml/ben-ge</a></li>
<li>paper_authors: Michael Mommert, Nicolas Kesseli, Joëlle Hanna, Linus Scheibenreif, Damian Borth, Begüm Demir</li>
<li>for: 本研究旨在探讨多模式 Earth observation 数据的分析方法，并证明 combining 不同数据模式可以提高下游任务的准确率。</li>
<li>methods: 本研究使用了 Earth observation 数据的多模式 combine，包括 patch-based 地用&#x2F;地形类别和地用&#x2F;地形分割等下游任务。</li>
<li>results: 研究表明，通过 combining 不同数据模式，可以提高下游任务的准确率，并且可以作为 Earth observation 应用的测试平台。<details>
<summary>Abstract</summary>
Deep learning methods have proven to be a powerful tool in the analysis of large amounts of complex Earth observation data. However, while Earth observation data are multi-modal in most cases, only single or few modalities are typically considered. In this work, we present the ben-ge dataset, which supplements the BigEarthNet-MM dataset by compiling freely and globally available geographical and environmental data. Based on this dataset, we showcase the value of combining different data modalities for the downstream tasks of patch-based land-use/land-cover classification and land-use/land-cover segmentation. ben-ge is freely available and expected to serve as a test bed for fully supervised and self-supervised Earth observation applications.
</details>
<details>
<summary>摘要</summary>
深度学习方法在大量复杂的地球观测数据分析中表现出了强大的功能。然而，大多数情况下的地球观测数据是多modal的，但只考虑单个或少数modalities。在这个工作中，我们提供了ben-ge数据集，该数据集收集了全球和自由可用的地理和环境数据，并基于这个数据集，我们展示了不同modalities的结合对下游任务（patch-based 土地用途/土地覆盖分类和土地用途/土地覆盖分割）的价值。ben-ge是免费可用的，预计将成为全supervised和self-supervised Earth observation应用程序的测试床。
</details></li>
</ul>
<hr>
<h2 id="Synchronous-Image-Label-Diffusion-Probability-Model-with-Application-to-Stroke-Lesion-Segmentation-on-Non-contrast-CT"><a href="#Synchronous-Image-Label-Diffusion-Probability-Model-with-Application-to-Stroke-Lesion-Segmentation-on-Non-contrast-CT" class="headerlink" title="Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT"></a>Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01740">http://arxiv.org/abs/2307.01740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianhai Zhang, Tonghua Wan, Ethan MacDonald, Bijoy Menon, Aravind Ganesh, Qiu Wu</li>
<li>for: 这个论文是为了提出一种基于Markov扩散过程的同步图像标签扩散可能性模型（SDPM），用于非对比CT扫描图像中的血栓病变 segmentation。</li>
<li>methods: 该模型基于一个隐变量模型（LVM），并引入了一个额外网络流，以获得初始噪声标签估计，以便高效地推理最终标签。通过优化指定的可变边界，训练好的模型可以对输入图像噪声给出多个标签估计。</li>
<li>results: 该模型在三个血栓病变数据集上进行测试，包括一个公共数据集和两个私人数据集，并与一些U-net和变换器基于的分割方法进行比较。结果显示，提出的SDPM模型能够达到当前最佳性能。代码公开 disponível。<details>
<summary>Abstract</summary>
Stroke lesion volume is a key radiologic measurement for assessing the prognosis of Acute Ischemic Stroke (AIS) patients, which is challenging to be automatically measured on Non-Contrast CT (NCCT) scans. Recent diffusion probabilistic models have shown potentials of being used for image segmentation. In this paper, a novel Synchronous image-label Diffusion Probability Model (SDPM) is proposed for stroke lesion segmentation on NCCT using Markov diffusion process. The proposed SDPM is fully based on a Latent Variable Model (LVM), offering a complete probabilistic elaboration. An additional net-stream, parallel with a noise prediction stream, is introduced to obtain initial noisy label estimates for efficiently inferring the final labels. By optimizing the specified variational boundaries, the trained model can infer multiple label estimates for reference given the input images with noises. The proposed model was assessed on three stroke lesion datasets including one public and two private datasets. Compared to several U-net and transformer-based segmentation methods, our proposed SDPM model is able to achieve state-of-the-art performance. The code is publicly available.
</details>
<details>
<summary>摘要</summary>
stroke lesion volume 是评估急性血栓roke（AIS）患者 prospect 的关键 radiologic 测量，在 Non-Contrast CT（NCCT）扫描中具有挑战性。  recient diffusion probabilistic 模型已经表现出了用于图像分割的潜在性。 在这篇论文中，一种新的同步图像标签Diffusion Probability Model（SDPM）被提出用于NCCT扫描中的roke lesion 分割。 SDPM 基于 Latent Variable Model（LVM），提供了完整的 probabilistic 推导。 一个额外的网络流，与噪声预测流并行，用于获取初始噪声标签估计，以高效地推导最终标签。 通过优化指定的边界，训练模型可以基于输入图像噪声提供多个标签估计。  proposed SDPM 模型在三个roke lesion 数据集中评估，包括一个公共数据集和两个私人数据集。 与一些 U-net 和 transformer 基于的 segmentation 方法相比，我们的提出的 SDPM 模型能够实现状态天表性性能。 代码公开可用。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Calibration-Bias-Without-Fixed-Attribute-Grouping-for-Improved-Fairness-in-Medical-Imaging-Analysis"><a href="#Mitigating-Calibration-Bias-Without-Fixed-Attribute-Grouping-for-Improved-Fairness-in-Medical-Imaging-Analysis" class="headerlink" title="Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis"></a>Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01738">http://arxiv.org/abs/2307.01738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changjian Shui, Justin Szeto, Raghav Mehta, Douglas L. Arnold, Tal Arbel</li>
<li>for: 这个研究是为了提高深度学习医学影像分析模型在实际医疗执行中的可靠性和准确性。</li>
<li>methods: 我们提出了一个新的二阶方法：集群专注法，它可以首先识别低准确的样本，然后将它们分为群体，最后将每个群体使用集群专注损失来改善准确偏差。</li>
<li>results: 我们的方法可以帮助控制最差表现的子群体的准确偏差，同时保持预测性能，并比最近的基elines表现更好。<details>
<summary>Abstract</summary>
Trustworthy deployment of deep learning medical imaging models into real-world clinical practice requires that they be calibrated. However, models that are well calibrated overall can still be poorly calibrated for a sub-population, potentially resulting in a clinician unwittingly making poor decisions for this group based on the recommendations of the model. Although methods have been shown to successfully mitigate biases across subgroups in terms of model accuracy, this work focuses on the open problem of mitigating calibration biases in the context of medical image analysis. Our method does not require subgroup attributes during training, permitting the flexibility to mitigate biases for different choices of sensitive attributes without re-training. To this end, we propose a novel two-stage method: Cluster-Focal to first identify poorly calibrated samples, cluster them into groups, and then introduce group-wise focal loss to improve calibration bias. We evaluate our method on skin lesion classification with the public HAM10000 dataset, and on predicting future lesional activity for multiple sclerosis (MS) patients. In addition to considering traditional sensitive attributes (e.g. age, sex) with demographic subgroups, we also consider biases among groups with different image-derived attributes, such as lesion load, which are required in medical image analysis. Our results demonstrate that our method effectively controls calibration error in the worst-performing subgroups while preserving prediction performance, and outperforming recent baselines.
</details>
<details>
<summary>摘要</summary>
信任性的深度学习医疗影像模型在实际临床实践中部署需要进行准确化。然而，即使模型在整体上具有良好的准确性，也可能对一个子population产生差异，导致医生因模型的建议而做出不良决策。虽然有方法可以在不同 subgroup 上减少偏见，但这项工作将关注医疗影像分析中的开放问题——准确性偏见的缓解。我们的方法不需要在训练过程中提供 subgroup 特征，因此可以随时缓解不同敏感特征的偏见。为此，我们提出了一种新的两阶段方法：首先使用 clustering 来identify poorly calibrated samples，然后引入 group-wise focal loss 来改善偏见偏见。我们在皮肤损害分类和多发性硬化病（MS）患者预测未来损害情况上进行了评估。此外，我们还考虑了传统敏感特征（如年龄、性别）和医疗影像分析中必需的图像特征，如肿瘤荷重。我们的结果表明，我们的方法可以控制最差 subgroup 的准确性错误，保持预测性能，并超过最近的基elines。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Computer-Vision-Models-through-Adversarial-Training-Unveiling-the-Robustness-Interpretability-Connection"><a href="#Interpretable-Computer-Vision-Models-through-Adversarial-Training-Unveiling-the-Robustness-Interpretability-Connection" class="headerlink" title="Interpretable Computer Vision Models through Adversarial Training: Unveiling the Robustness-Interpretability Connection"></a>Interpretable Computer Vision Models through Adversarial Training: Unveiling the Robustness-Interpretability Connection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02500">http://arxiv.org/abs/2307.02500</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/delyan-boychev/pytorch_trainers_interpretability">https://github.com/delyan-boychev/pytorch_trainers_interpretability</a></li>
<li>paper_authors: Delyan Boychev</li>
<li>for: 本研究旨在评估对抗训练的影响，以生成更加鲁棒的模型，即具有对抗攻击的抵抗能力。</li>
<li>methods: 本研究使用了本地特征重要性方法（SHAP和Integrated Gradients）和特征视觉技术（Representation Inversion和Class Specific Image Generation）进行了广泛的测试和分析。</li>
<li>results: 研究发现，对抗训练可以使计算机视觉模型更加 interpretable，即其学习的特征更加类似于人类的理解。此外，对抗训练的模型在对抗攻击时表现更加鲁棒，并且更加注重图像中的特定区域，以支持其预测。<details>
<summary>Abstract</summary>
With the perpetual increase of complexity of the state-of-the-art deep neural networks, it becomes a more and more challenging task to maintain their interpretability. Our work aims to evaluate the effects of adversarial training utilized to produce robust models - less vulnerable to adversarial attacks. It has been shown to make computer vision models more interpretable. Interpretability is as essential as robustness when we deploy the models to the real world. To prove the correlation between these two problems, we extensively examine the models using local feature-importance methods (SHAP, Integrated Gradients) and feature visualization techniques (Representation Inversion, Class Specific Image Generation). Standard models, compared to robust are more susceptible to adversarial attacks, and their learned representations are less meaningful to humans. Conversely, these models focus on distinctive regions of the images which support their predictions. Moreover, the features learned by the robust model are closer to the real ones.
</details>
<details>
<summary>摘要</summary>
随着现代深度神经网络的复杂性不断增加，维护它们的解释性变得越来越困难。我们的工作旨在评估针对性训练可以生成更加鲁棒的模型，以降低对抗性攻击的脆弱性。已经证明了在计算机视觉领域中，使用针对性训练可以提高模型的解释性。当我们将模型部署到实际应用中时，解释性的重要性与鲁棒性一样高。为了证明这两个问题之间的相关性，我们广泛使用本地特征重要性方法（SHAP、整合梯度）和特征视觉技术（ Representation Inversion、类特征图生成）进行检验。对比标准模型和鲁棒模型，后者更易受到抗性攻击，并且它所学习的特征更难以被人类理解。然而，这些模型强调特定的图像区域，这些区域支持它们的预测。此外，鲁棒模型学习的特征更加接近真实的特征。
</details></li>
</ul>
<hr>
<h2 id="Graph-Ensemble-Learning-Model-for-Multi-label-Skin-Lesion-Classification-using-Dermoscopy-and-Clinical-Images"><a href="#Graph-Ensemble-Learning-Model-for-Multi-label-Skin-Lesion-Classification-using-Dermoscopy-and-Clinical-Images" class="headerlink" title="Graph-Ensemble Learning Model for Multi-label Skin Lesion Classification using Dermoscopy and Clinical Images"></a>Graph-Ensemble Learning Model for Multi-label Skin Lesion Classification using Dermoscopy and Clinical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01704">http://arxiv.org/abs/2307.01704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Tang, Yang Nan, Tobias Lasser</li>
<li>for: 本研究旨在开发一种基于多模态数据的多标签分类方法，以提高皮肤病诊断的准确性。</li>
<li>methods: 该方法利用图像 convolutional neural network (GCN) 来利用多模态数据的协调关系，并通过自适应 fusion 技术将 GCN 的预测与多模态数据 fusion 模型的预测进行权重平均 fusions，以获取更高的分类精度。</li>
<li>results: 实验结果表明，提出的 Graph-Ensemble Learning Model (GELN) 可以在不同的 dataset 上提高分类性能，并在 SPC 和诊断分类方面达到领先的表现。<details>
<summary>Abstract</summary>
Many skin lesion analysis (SLA) methods recently focused on developing a multi-modal-based multi-label classification method due to two factors. The first is multi-modal data, i.e., clinical and dermoscopy images, which can provide complementary information to obtain more accurate results than single-modal data. The second one is that multi-label classification, i.e., seven-point checklist (SPC) criteria as an auxiliary classification task can not only boost the diagnostic accuracy of melanoma in the deep learning (DL) pipeline but also provide more useful functions to the clinical doctor as it is commonly used in clinical dermatologist's diagnosis. However, most methods only focus on designing a better module for multi-modal data fusion; few methods explore utilizing the label correlation between SPC and skin disease for performance improvement. This study fills the gap that introduces a Graph Convolution Network (GCN) to exploit prior co-occurrence between each category as a correlation matrix into the DL model for the multi-label classification. However, directly applying GCN degraded the performances in our experiments; we attribute this to the weak generalization ability of GCN in the scenario of insufficient statistical samples of medical data. We tackle this issue by proposing a Graph-Ensemble Learning Model (GELN) that views the prediction from GCN as complementary information of the predictions from the fusion model and adaptively fuses them by a weighted averaging scheme, which can utilize the valuable information from GCN while avoiding its negative influences as much as possible. To evaluate our method, we conduct experiments on public datasets. The results illustrate that our GELN can consistently improve the classification performance on different datasets and that the proposed method can achieve state-of-the-art performance in SPC and diagnosis classification.
</details>
<details>
<summary>摘要</summary>
多种皮肤 lesion 分析（SLA）方法最近都在努力开发一种多模态基于多标签分类方法，主要是因为两点。第一，我们有多种模态数据，例如临床和肤视图图像，这些数据可以提供补偿信息，以获得更准确的结果。第二，多标签分类可以不仅提高抑制癌症的深度学习（DL）管道的诊断精度，还可以提供更有用的功能 для临床医生，因为这种分类方法在临床 dermatologist 的诊断中广泛使用。然而，大多数方法都是关注设计更好的多模态数据融合模块，很少方法探讨利用皮病分类标准（SPC）和皮肤病的标签相关性来提高性能。本研究填补了这一空白，通过引入一个图像卷积网络（GCN），利用每个类别之间的协同关系，在 DL 模型中进行多标签分类。然而，直接应用 GCN 会下降性能，我们归因于医学数据的不充分统计样本的问题。我们解决这个问题，提出一种图像ensemble学习模型（GELN），视图GCN 的预测为补充信息，并通过一种权重平均方式，将其与融合模型的预测相乘，以利用 GCN 的有价值信息，同时避免它的负面影响。为评估我们的方法，我们在公共数据集上进行实验。结果表明，我们的 GELN 可以在不同的数据集上具有稳定的分类性能，并且可以在 SPC 和诊断分类中达到国际级的表现。
</details></li>
</ul>
<hr>
<h2 id="Augment-Features-Beyond-Color-for-Domain-Generalized-Segmentation"><a href="#Augment-Features-Beyond-Color-for-Domain-Generalized-Segmentation" class="headerlink" title="Augment Features Beyond Color for Domain Generalized Segmentation"></a>Augment Features Beyond Color for Domain Generalized Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01703">http://arxiv.org/abs/2307.01703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiyu Sun, Pavlo Melnyk, Michael Felsberg, Yang Tang</li>
<li>for: 这篇论文旨在提出一种适用于不同类型资料的通用Semantic Segmentation方法，并且不需要target资料进行训练。</li>
<li>methods: 我们的方法包括两个模组：随机图像颜色增强（RICA）和随机特征分布增强（RFDA）。RICA将图像从RGB转换为CIELAB颜色模型，并在一个感知基础上随机调整图像以提高图像质量。我们还使用CycleGAN-based生成网络将增强后的图像扩展到特征空间，以更好地丰富数据。</li>
<li>results: 我们进行了广泛的实验，结果显示我们的方法在不同的资料集上（包括Synthia、Cityscapes、BDDS和Mapillary）实现了顶尖的Semantic Segmentation性能。<details>
<summary>Abstract</summary>
Domain generalized semantic segmentation (DGSS) is an essential but highly challenging task, in which the model is trained only on source data and any target data is not available. Previous DGSS methods can be partitioned into augmentation-based and normalization-based ones. The former either introduces extra biased data or only conducts channel-wise adjustments for data augmentation, and the latter may discard beneficial visual information, both of which lead to limited performance in DGSS. Contrarily, our method performs inter-channel transformation and meanwhile evades domain-specific biases, thus diversifying data and enhancing model generalization performance. Specifically, our method consists of two modules: random image color augmentation (RICA) and random feature distribution augmentation (RFDA). RICA converts images from RGB to the CIELAB color model and randomizes color maps in a perception-based way for image enhancement purposes. We further this augmentation by extending it beyond color to feature space using a CycleGAN-based generative network, which complements RICA and further boosts generalization capability. We conduct extensive experiments, and the generalization results from the synthetic GTAV and SYNTHIA to the real Cityscapes, BDDS, and Mapillary datasets show that our method achieves state-of-the-art performance in DGSS.
</details>
<details>
<summary>摘要</summary>
领域普遍 semantic segmentation (DGSS) 是一个非常重要但也非常具有挑战性的任务，模型在训练时只有source数据可用，target数据不可用。先前的DGSS方法可以分为两种：增强型和normalization型。前者可能引入额外的偏见数据或只是执行通道 wise的调整，后者可能会弃用有利的视觉信息，这两者都导致DGSS的表现有限。相反，我们的方法会执行 между通道转换和避免域专偏见，因此可以多样化数据和提高模型的普遍性表现。我们的方法包括两个模组：随机图像颜色增强 (RICA) 和随机特征分布增强 (RFDA)。RICA 将图像从 RGB 转换为 CIELAB 颜色模型，并在感知方式下随机调整图像以增强图像表现。我们继续这个增强，通过使用 CycleGAN 基本的生成网络，该网络可以补充 RICA 并进一步提高普遍能力。我们实现了广泛的实验，并从 sintetic GTAV 和 SYNTHIA  Synthetic 资料集到 real Cityscapes、BDDS 和 Mapillary 资料集的一致性结果显示，我们的方法在 DGSS 中实现了顶尖的表现。
</details></li>
</ul>
<hr>
<h2 id="Spike-driven-Transformer"><a href="#Spike-driven-Transformer" class="headerlink" title="Spike-driven Transformer"></a>Spike-driven Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01694">http://arxiv.org/abs/2307.01694</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biclab/spike-driven-transformer">https://github.com/biclab/spike-driven-transformer</a></li>
<li>paper_authors: Man Yao, Jiakui Hu, Zhaokun Zhou, Li Yuan, Yonghong Tian, Bo Xu, Guoqi Li<br>for:The paper is written to propose a new deep learning model called Spike-driven Transformer, which incorporates the spike-driven paradigm into the Transformer architecture.methods:The proposed Spike-driven Transformer uses four unique properties: event-driven, binary spike communication, self-attention with linear complexity, and mask and addition operations.results:The Spike-driven Transformer achieves a top-1 accuracy of 77.1% on ImageNet-1K, which is the state-of-the-art result in the SNN field.<details>
<summary>Abstract</summary>
Spiking Neural Networks (SNNs) provide an energy-efficient deep learning option due to their unique spike-based event-driven (i.e., spike-driven) paradigm. In this paper, we incorporate the spike-driven paradigm into Transformer by the proposed Spike-driven Transformer with four unique properties: 1) Event-driven, no calculation is triggered when the input of Transformer is zero; 2) Binary spike communication, all matrix multiplications associated with the spike matrix can be transformed into sparse additions; 3) Self-attention with linear complexity at both token and channel dimensions; 4) The operations between spike-form Query, Key, and Value are mask and addition. Together, there are only sparse addition operations in the Spike-driven Transformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA), which exploits only mask and addition operations without any multiplication, and thus having up to $87.2\times$ lower computation energy than vanilla self-attention. Especially in SDSA, the matrix multiplication between Query, Key, and Value is designed as the mask operation. In addition, we rearrange all residual connections in the vanilla Transformer before the activation functions to ensure that all neurons transmit binary spike signals. It is shown that the Spike-driven Transformer can achieve 77.1\% top-1 accuracy on ImageNet-1K, which is the state-of-the-art result in the SNN field. The source code is available at https://github.com/BICLab/Spike-Driven-Transformer.
</details>
<details>
<summary>摘要</summary>
ospiking neural networks (SNNs) provide an energy-efficient deep learning option due to their unique spike-based event-driven (i.e., spike-driven) paradigm. In this paper, we incorporate the spike-driven paradigm into Transformer by the proposed Spike-driven Transformer with four unique properties: 1) Event-driven, no calculation is triggered when the input of Transformer is zero; 2) Binary spike communication, all matrix multiplications associated with the spike matrix can be transformed into sparse additions; 3) Self-attention with linear complexity at both token and channel dimensions; 4) The operations between spike-form Query, Key, and Value are mask and addition. Together, there are only sparse addition operations in the Spike-driven Transformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA), which exploits only mask and addition operations without any multiplication, and thus having up to $87.2\times$ lower computation energy than vanilla self-attention. Especially in SDSA, the matrix multiplication between Query, Key, and Value is designed as the mask operation. In addition, we rearrange all residual connections in the vanilla Transformer before the activation functions to ensure that all neurons transmit binary spike signals. It is shown that the Spike-driven Transformer can achieve 77.1\% top-1 accuracy on ImageNet-1K, which is the state-of-the-art result in the SNN field. The source code is available at https://github.com/BICLab/Spike-Driven-Transformer.Note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Training-Energy-Based-Models-with-Diffusion-Contrastive-Divergences"><a href="#Training-Energy-Based-Models-with-Diffusion-Contrastive-Divergences" class="headerlink" title="Training Energy-Based Models with Diffusion Contrastive Divergences"></a>Training Energy-Based Models with Diffusion Contrastive Divergences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01668">http://arxiv.org/abs/2307.01668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijian Luo, Hao Jiang, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Zhihua Zhang<br>for:This paper focuses on improving the efficiency and accuracy of Energy-Based Models (EBMs) for generative modeling, specifically addressing the trade-off between computational burden and validity in Contrastive Divergence (CD) training.methods:The authors propose a new family of Diffusion Contrastive Divergence (DCD) methods that replace the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, leading to more efficient and accurate training of EBMs.results:The proposed DCD methods outperform CD in terms of computational efficiency and accuracy, as demonstrated through extensive experiments on synthetic data modeling, high-dimensional image denoising and generation, and image generation. Specifically, the proposed DCD achieves better performance than CD on synthetic data learning and image denoising experiments, and is capable of training an EBM for generating the Celab-A $32\times 32$ dataset.<details>
<summary>Abstract</summary>
Energy-Based Models (EBMs) have been widely used for generative modeling. Contrastive Divergence (CD), a prevailing training objective for EBMs, requires sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which leads to an irreconcilable trade-off between the computational burden and the validity of the CD. Running MCMCs till convergence is computationally intensive. On the other hand, short-run MCMC brings in an extra non-negligible parameter gradient term that is difficult to handle. In this paper, we provide a general interpretation of CD, viewing it as a special instance of our proposed Diffusion Contrastive Divergence (DCD) family. By replacing the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, we propose a more efficient divergence. We show that the proposed DCDs are both more computationally efficient than the CD and are not limited to a non-negligible gradient term. We conduct intensive experiments, including both synthesis data modeling and high-dimensional image denoising and generation, to show the advantages of the proposed DCDs. On the synthetic data learning and image denoising experiments, our proposed DCD outperforms CD by a large margin. In image generation experiments, the proposed DCD is capable of training an energy-based model for generating the Celab-A $32\times 32$ dataset, which is comparable to existing EBMs.
</details>
<details>
<summary>摘要</summary>
energy-based models (EBMs) 已经广泛应用于生成模型。对比тив的游逸差（CD），一种广泛使用的EBMs 训练目标，需要从EBM中采样使用Markov Chain Monte Carlo方法（MCMC），这导致了计算束缚和CD 的有效性之间的一种不可调和的负担。在MCMC 到 converges 的过程中，计算束缚是计算昂贵的。另一方面，短跑MCMC 会带来一个难以处理的非可忽略的参数梯度项。在这篇论文中，我们提供了CD 的通用解释，视其为我们提议的噪声扩散对照法（DCD）家族的一个特例。通过将CD 中的朗格文动力换为其他EBM参数无关的扩散过程，我们提议了更有效的分化。我们表明，我们提议的DCD 比CD更高效，并且不受非可忽略的参数梯度项的限制。我们在对 synthetic data 学习和图像噪声除掉和生成等实验中进行了广泛的测试，并证明了我们的DCD 比CD 大幅提高性能。在图像生成实验中，我们的DCD 能够训练一个能够生成 Celab-A $32\times 32$ 数据集的能量基本模型，与现有EBMs 相当。
</details></li>
</ul>
<hr>
<h2 id="Sensors-and-Systems-for-Monitoring-Mental-Fatigue-A-systematic-review"><a href="#Sensors-and-Systems-for-Monitoring-Mental-Fatigue-A-systematic-review" class="headerlink" title="Sensors and Systems for Monitoring Mental Fatigue: A systematic review"></a>Sensors and Systems for Monitoring Mental Fatigue: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01666">http://arxiv.org/abs/2307.01666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prabin Sharma, Joanna C. Justus, Govinda R. Poudel<br>for: 本研究旨在提供一个批判性概念模型，描述关键实现感疲劳检测技术，并进行系统性审查 latest studies 使用生物感测器系统检测人类的疲劳状态。methods: 本研究使用系统性搜寻和评价57篇文献（N&#x3D;1082），主要使用电enzephalography（EEG）基于感应器追踪疲劳状态。results: 本研究发现EEG基于感应器可提供moderate至good的感传度来检测疲劳状态。另外，本研究发现高密度EEG感应器无法提供增加的优势。基于发现，本研究提供了一个批判性讨论，探讨将穿戴式EEG和环境感应器integragted into real-world monitoring。未来的工作需要进一步改进和适应这些技术，以便实现广泛的疲劳监控在自主和无人驾驶产业中。<details>
<summary>Abstract</summary>
Mental fatigue is a leading cause of motor vehicle accidents, medical errors, loss of workplace productivity, and student disengagements in e-learning environment. Development of sensors and systems that can reliably track mental fatigue can prevent accidents, reduce errors, and help increase workplace productivity. This review provides a critical summary of theoretical models of mental fatigue, a description of key enabling sensor technologies, and a systematic review of recent studies using biosensor-based systems for tracking mental fatigue in humans. We conducted a systematic search and review of recent literature which focused on detection and tracking of mental fatigue in humans. The search yielded 57 studies (N=1082), majority of which used electroencephalography (EEG) based sensors for tracking mental fatigue. We found that EEG-based sensors can provide a moderate to good sensitivity for fatigue detection. Notably, we found no incremental benefit of using high-density EEG sensors for application in mental fatigue detection. Given the findings, we provide a critical discussion on the integration of wearable EEG and ambient sensors in the context of achieving real-world monitoring. Future work required to advance and adapt the technologies toward widespread deployment of wearable sensors and systems for fatigue monitoring in semi-autonomous and autonomous industries is examined.
</details>
<details>
<summary>摘要</summary>
心理疲劳是主要导致机动车事故、医疗错误、工作场所产量下降和电子学习环境中学生失业的原因。开发可靠跟踪心理疲劳的感应器和系统可以预防事故、减少错误，并帮助提高工作场所产量。本文提供了心理疲劳理论模型的批判摘要、关键实现感应器技术的描述，以及在人类中使用感应器基于系统的评估。我们对最新的文献进行了系统性的搜索和评估，搜索结果共57篇论文（N=1082），大多数使用电encephalography（EEG）基于感应器跟踪心理疲劳。我们发现EEG基于感应器可以提供moderate至good的敏感性 для疲劳检测。另外，我们未发现使用高密度EEG感应器在心理疲劳检测中增加的优势。根据发现，我们提供了在实际监测中 integrating wearable EEG和 ambient sensor的批判讨论，以及将这些技术应用于自动化和半自动化业务的未来工作。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Transformers-for-On-Line-Handwritten-Signature-Verification"><a href="#Exploring-Transformers-for-On-Line-Handwritten-Signature-Verification" class="headerlink" title="Exploring Transformers for On-Line Handwritten Signature Verification"></a>Exploring Transformers for On-Line Handwritten Signature Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01663">http://arxiv.org/abs/2307.01663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pietro Melzi, Ruben Tolosana, Ruben Vera-Rodriguez, Paula Delgado-Santos, Giuseppe Stragapede, Julian Fierrez, Javier Ortega-Garcia</li>
<li>for: 这个研究旨在评估基于最新的Transformers架构的在线签名验证系统的可靠性。</li>
<li>methods: 研究人员使用四种不同的配置，其中两种使用基于Vanilla Transformer核心的Encoder，另外两种则已经在步行和活动识别任务上得到了成功。</li>
<li>results: 实验结果表明，使用Transformers架构可以提供高度可靠的在线签名验证系统。<details>
<summary>Abstract</summary>
The application of mobile biometrics as a user-friendly authentication method has increased in the last years. Recent studies have proposed novel behavioral biometric recognition systems based on Transformers, which currently outperform the state of the art in several application scenarios. On-line handwritten signature verification aims to verify the identity of subjects, based on their biometric signatures acquired using electronic devices such as tablets or smartphones. This paper investigates the suitability of architectures based on recent Transformers for on-line signature verification. In particular, four different configurations are studied, two of them rely on the Vanilla Transformer encoder, and the two others have been successfully applied to the tasks of gait and activity recognition. We evaluate the four proposed configurations according to the experimental protocol proposed in the SVC-onGoing competition. The results obtained in our experiments are promising, and promote the use of Transformers for on-line signature verification.
</details>
<details>
<summary>摘要</summary>
随着移动生物 метрик作为用户友好的验证方法的应用逐渐增加，最近的研究已经提出了基于转换器的新型行为生物метри克认证系统。在线手写签名验证目的是验证使用电子设备such as 平板或智能手机所获取的生物签名的真实性，以验证个体身份。本文研究了基于最新的转换器架构的在线手写签名验证。特别是，我们研究了四种不同的配置，其中两种使用 Vanilla Transformer 编码器，另外两种已经成功应用于步态和活动识别任务。我们按照SVC-onGoing competition的实验协议进行了测试，实验结果很俊朗，这些结果推荐使用转换器进行在线手写签名验证。
</details></li>
</ul>
<hr>
<h2 id="Task-Planning-Support-for-Arborists-and-Foresters-Comparing-Deep-Learning-Approaches-for-Tree-Inventory-and-Tree-Vitality-Assessment-Based-on-UAV-Data"><a href="#Task-Planning-Support-for-Arborists-and-Foresters-Comparing-Deep-Learning-Approaches-for-Tree-Inventory-and-Tree-Vitality-Assessment-Based-on-UAV-Data" class="headerlink" title="Task Planning Support for Arborists and Foresters: Comparing Deep Learning Approaches for Tree Inventory and Tree Vitality Assessment Based on UAV-Data"></a>Task Planning Support for Arborists and Foresters: Comparing Deep Learning Approaches for Tree Inventory and Tree Vitality Assessment Based on UAV-Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01651">http://arxiv.org/abs/2307.01651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas-Dario Troles, Richard Nieding, Sonia Simons, Ute Schmid</li>
<li>for:  This paper aims to optimize workflows and increase productivity for arborists and foresters who care for trees in urban areas and forests.</li>
<li>methods: The approach uses RGB and multispectral UAV data, as well as multispectral satellite data and soil moisture sensors, to create tree inventories and vitality assessments.</li>
<li>results: The approach generates helpful information and improves task planning for arborists and foresters, allowing them to better care for trees in urban areas and forests.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文是为了优化植物护理人员的工作流程和提高生产力而写的。</li>
<li>methods: 该方法使用RGB和多spectral UAV数据，以及多spectral卫星数据和土壤湿度传感器，创建树木目录和评估树木生长状况。</li>
<li>results: 该方法生成有用的信息，提高植物护理人员的日常任务规划，以便更好地照顾城市公园和森林中的树木。<details>
<summary>Abstract</summary>
Climate crisis and correlating prolonged, more intense periods of drought threaten tree health in cities and forests. In consequence, arborists and foresters suffer from increasing workloads and, in the best case, a consistent but often declining workforce. To optimise workflows and increase productivity, we propose a novel open-source end-to-end approach that generates helpful information and improves task planning of those who care for trees in and around cities. Our approach is based on RGB and multispectral UAV data, which is used to create tree inventories of city parks and forests and to deduce tree vitality assessments through statistical indices and Deep Learning. Due to EU restrictions regarding flying drones in urban areas, we will also use multispectral satellite data and fifteen soil moisture sensors to extend our tree vitality-related basis of data. Furthermore, Bamberg already has a georeferenced tree cadastre of around 15,000 solitary trees in the city area, which is also used to generate helpful information. All mentioned data is then joined and visualised in an interactive web application allowing arborists and foresters to generate individual and flexible evaluations, thereby improving daily task planning.
</details>
<details>
<summary>摘要</summary>
клима紧急情况和相关的长期、更加激烈的干旱期导致城市和森林树木健康受到威胁。因此，树木医生和森林管理员工作量增加，而且最好的情况下也是不断减少的工作力量。为了优化工作流程和提高生产力，我们提出了一种新的开源终端方法，通过使用RGB和多spectral UAV数据来生成有用的信息和改善树木照顾人员在城市和森林中的任务规划。我们的方法基于RGB和多spectral UAV数据，用于创建城市公园和森林的树木目录和树木质量评估通过统计指标和深度学习。由于欧盟对城市区域内飞行无人机的限制，我们还使用多spectral卫星数据和十五个土壤湿度传感器来扩展我们树木质量相关的数据基础。此外，巴姆贝格已经有精确地参考树木目录，包括城市区域内约15,000棵 solitary树木，这些数据也用于生成有用信息。所有提到的数据然后被联合并视觉化在一个交互式网页应用程序中， allowing树木医生和森林管理员在日常任务规划中提高效率。
</details></li>
</ul>
<hr>
<h2 id="In-Domain-Self-Supervised-Learning-Can-Lead-to-Improvements-in-Remote-Sensing-Image-Classification"><a href="#In-Domain-Self-Supervised-Learning-Can-Lead-to-Improvements-in-Remote-Sensing-Image-Classification" class="headerlink" title="In-Domain Self-Supervised Learning Can Lead to Improvements in Remote Sensing Image Classification"></a>In-Domain Self-Supervised Learning Can Lead to Improvements in Remote Sensing Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01645">http://arxiv.org/abs/2307.01645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivica Dimitrovski, Ivan Kitanovski, Nikola Simidjievski, Dragi Kocev</li>
<li>for: 本研究旨在探讨自监督学习（SSL）在遥感图像分类中的应用前景，尝试利用大量无标签数据来学习图像表示。</li>
<li>methods: 本研究使用Million AID数据集，通过形式ulated auxiliary tasks来生成 pseudo-标签，并使用ViT模型进行预训练和细化训练。</li>
<li>results: 实验结果显示，使用域内SSL预训练（iBOT框架和ViT模型）在14个多样化的遥感图像分类任务中表现更好，比supervised预训练使用ImageNet数据集。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) has emerged as a promising approach for remote sensing image classification due to its ability to leverage large amounts of unlabeled data. In contrast to traditional supervised learning, SSL aims to learn representations of data without the need for explicit labels. This is achieved by formulating auxiliary tasks that can be used to create pseudo-labels for the unlabeled data and learn pre-trained models. The pre-trained models can then be fine-tuned on downstream tasks such as remote sensing image scene classification. The paper analyzes the effectiveness of SSL pre-training using Million AID - a large unlabeled remote sensing dataset on various remote sensing image scene classification datasets as downstream tasks. More specifically, we evaluate the effectiveness of SSL pre-training using the iBOT framework coupled with Vision transformers (ViT) in contrast to supervised pre-training of ViT using the ImageNet dataset. The comprehensive experimental work across 14 datasets with diverse properties reveals that in-domain SSL leads to improved predictive performance of models compared to the supervised counterparts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ChildPlay-A-New-Benchmark-for-Understanding-Children’s-Gaze-Behaviour"><a href="#ChildPlay-A-New-Benchmark-for-Understanding-Children’s-Gaze-Behaviour" class="headerlink" title="ChildPlay: A New Benchmark for Understanding Children’s Gaze Behaviour"></a>ChildPlay: A New Benchmark for Understanding Children’s Gaze Behaviour</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01630">http://arxiv.org/abs/2307.01630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samy Tafasca, Anshul Gupta, Jean-Marc Odobez</li>
<li>for: 这个研究是为了预测儿童和成人之间的注视目标，以便更好地诊断developmental disorders。</li>
<li>methods: 我们提出了一个新的注视目标预测模型，利用latest geometry preserving depth inference methods和rich gaze information，以及一个新的 ChildPlay 数据集。</li>
<li>results: 我们的模型在benchmark datasets和 ChildPlay 上 achieved state of the art results，并且发现looking at faces prediction performance on children is much worse than on adults，可以通过 fine-tuning models using child gaze annotations 进一步提高。<details>
<summary>Abstract</summary>
Gaze behaviors such as eye-contact or shared attention are important markers for diagnosing developmental disorders in children. While previous studies have looked at some of these elements, the analysis is usually performed on private datasets and is restricted to lab settings. Furthermore, all publicly available gaze target prediction benchmarks mostly contain instances of adults, which makes models trained on them less applicable to scenarios with young children. In this paper, we propose the first study for predicting the gaze target of children and interacting adults. To this end, we introduce the ChildPlay dataset: a curated collection of short video clips featuring children playing and interacting with adults in uncontrolled environments (e.g. kindergarten, therapy centers, preschools etc.), which we annotate with rich gaze information. We further propose a new model for gaze target prediction that is geometrically grounded by explicitly identifying the scene parts in the 3D field of view (3DFoV) of the person, leveraging recent geometry preserving depth inference methods. Our model achieves state of the art results on benchmark datasets and ChildPlay. Furthermore, results show that looking at faces prediction performance on children is much worse than on adults, and can be significantly improved by fine-tuning models using child gaze annotations. Our dataset and models will be made publicly available.
</details>
<details>
<summary>摘要</summary>
“眼光行为如视线接触或共同注意是诊断儿童发展障碍的重要标志。过往的研究通常仅在私人数据库中进行分析，并仅限于实验室设置。而所有公开可用的眼光目标预测参考 benchmark 几乎都包含成年人，这使得在他们上训练的模型在儿童enario中变得更加不适用。在本文中，我们提出了第一个预测儿童和成年人之间的眼光目标的研究。为此，我们介绍了 ChildPlay 数据集：一个 curaated 的短视频剪辑集，该集包含儿童在不受控制的环境中玩耍和与成年人互动的短片，并将其注解为丰富的眼光信息。我们还提出了一新的眼光目标预测模型，它在Scene parts 的3D 视野（3DFoV）中Explicitly 识别场景元件，并仅仅利用最近的具有 geometry preserving 的深度推测方法。我们的模型在参考数据集和 ChildPlay 上实现了顶尖的结果，并且结果显示，对于儿童的视线预测性能与成年人相比，相对较差，并可以通过 Fine-tuning 模型使用儿童的眼光注解进行改善。我们的数据和模型将会公开可用。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Lie-Group-Symmetry-Transformations-with-Neural-Networks"><a href="#Learning-Lie-Group-Symmetry-Transformations-with-Neural-Networks" class="headerlink" title="Learning Lie Group Symmetry Transformations with Neural Networks"></a>Learning Lie Group Symmetry Transformations with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01583">http://arxiv.org/abs/2307.01583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/victoria-klein/learning-lie-group-symmetries">https://github.com/victoria-klein/learning-lie-group-symmetries</a></li>
<li>paper_authors: Alex Gabel, Victoria Klein, Riccardo Valperga, Jeroen S. W. Lamb, Kevin Webster, Rick Quax, Efstratios Gavves</li>
<li>for: 本研究旨在探讨和评估数据集中的对称性，以便更好地进行模型选择、生成模型和数据分析等。</li>
<li>methods: 本研究使用一种新的方法，可以自动找出数据集中未知的对称性，包括李群对称变换以外的其他对称变换。</li>
<li>results: 研究结果表明，该方法可以在不同的数据点 Parametrization 下成功地描述数据集的对称性，并且可以在不同的设置下进行数据分析和模型选择。<details>
<summary>Abstract</summary>
The problem of detecting and quantifying the presence of symmetries in datasets is useful for model selection, generative modeling, and data analysis, amongst others. While existing methods for hard-coding transformations in neural networks require prior knowledge of the symmetries of the task at hand, this work focuses on discovering and characterizing unknown symmetries present in the dataset, namely, Lie group symmetry transformations beyond the traditional ones usually considered in the field (rotation, scaling, and translation). Specifically, we consider a scenario in which a dataset has been transformed by a one-parameter subgroup of transformations with different parameter values for each data point. Our goal is to characterize the transformation group and the distribution of the parameter values. The results showcase the effectiveness of the approach in both these settings.
</details>
<details>
<summary>摘要</summary>
“检测和量化数据集中的对称性问题有助于选择模型、生成模型和数据分析等方面。现有的方法需要先知道任务的对称性，而这项工作则是发现和描述数据集中未知的对称性，即李群对称变换以外的其他对称性。特别是，我们考虑了一种情况，在这种情况下，数据集已经被一个一参数子群的变换所变换，每个数据点的参数值都不同。我们的目标是Characterizing the transformation group and the distribution of parameter values。结果显示了该方法在这两个设置下的效果。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Multi-scale-Approach-for-Speech-and-Dynamics-Synchrony-in-Talking-Head-Generation"><a href="#A-Comprehensive-Multi-scale-Approach-for-Speech-and-Dynamics-Synchrony-in-Talking-Head-Generation" class="headerlink" title="A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation"></a>A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03270">http://arxiv.org/abs/2307.03270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louisbearing/hmo-audio">https://github.com/louisbearing/hmo-audio</a></li>
<li>paper_authors: Louis Airale, Dominique Vaufreydaz, Xavier Alameda-Pineda</li>
<li>for: 这种研究旨在提高深度生成模型中的语音与表情动作的同步，以便生成更自然的人脸动作和语音同步。</li>
<li>methods: 该研究使用了多级音视频同步损失函数和多级权重网络来更好地处理语音和表情动作之间的短期和长期相关性。</li>
<li>results: 实验表明，使用该方法可以大幅提高人脸动作的自然性和多级音视频同步的质量，并且在不同时间尺度上都能够保持良好的同步。<details>
<summary>Abstract</summary>
Animating still face images with deep generative models using a speech input signal is an active research topic and has seen important recent progress. However, much of the effort has been put into lip syncing and rendering quality while the generation of natural head motion, let alone the audio-visual correlation between head motion and speech, has often been neglected. In this work, we propose a multi-scale audio-visual synchrony loss and a multi-scale autoregressive GAN to better handle short and long-term correlation between speech and the dynamics of the head and lips. In particular, we train a stack of syncer models on multimodal input pyramids and use these models as guidance in a multi-scale generator network to produce audio-aligned motion unfolding over diverse time scales. Our generator operates in the facial landmark domain, which is a standard low-dimensional head representation. The experiments show significant improvements over the state of the art in head motion dynamics quality and in multi-scale audio-visual synchrony both in the landmark domain and in the image domain.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用深度生成模型动画静止图像是一个活跃的研究领域，近年来有重要的进步。然而，许多努力都是 lip syncing 和图像质量的改进，而Audio-visual相关性和自然的头部运动 generation 则 часто被忽略。在这项工作中，我们提出了一种多级音频视频同步损失和多级自适应GAN，以更好地处理 speech 和头部动作之间的短期和长期相关性。具体来说，我们在 multimodal 输入PYRAMIDS 上训练了一 stack of syncer 模型，并将这些模型作为指导在一种多级生成网络中使用，以生成 audio-aligned 动作，从多个时间尺度中 unfolding。我们的生成器在 facial landmark 空间中运行，这是一个标准的低维度头部表示。实验显示，我们的方法可以在 head motion dynamics 质量和多级音频视频同步两个方面提供显著改进，并在 landmark 空间和图像空间都达到了领先水平。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-reconstruct-the-bubble-distribution-with-conductivity-maps-using-Invertible-Neural-Networks-and-Error-Diffusion"><a href="#Learning-to-reconstruct-the-bubble-distribution-with-conductivity-maps-using-Invertible-Neural-Networks-and-Error-Diffusion" class="headerlink" title="Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion"></a>Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02496">http://arxiv.org/abs/2307.02496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishant Kumar, Lukas Krause, Thomas Wondrak, Sven Eckert, Kerstin Eckert, Stefan Gumhold</li>
<li>for: 这个论文的目的是提高电解质水氢生产的环保性，并且解决在电解质水中产生的气泡对反应干扰、细胞效率和能源消耗的问题。</li>
<li>methods: 这个论文使用了外部磁场传感器测量电解质水中的磁场干扰，并解决了磁场干扰对电解质水中的气泡大小和位置的影响。</li>
<li>results: 研究表明，使用归一化神经网络（INN）可以重建电解质水中的电阻场图像，并且对于这种具有较高分辨率的电阻场图像，INN的性能远胜于提高质量。<details>
<summary>Abstract</summary>
Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles generated during the process hinder reactions, reduce cell efficiency, and increase energy consumption. Additionally, these gas bubbles cause changes in the conductivity inside the cell, resulting in corresponding variations in the induced magnetic field around the cell. Therefore, measuring these gas bubble-induced magnetic field fluctuations using external magnetic sensors and solving the inverse problem of Biot-Savart Law allows for estimating the conductivity in the cell and, thus, bubble size and location. However, determining high-resolution conductivity maps from only a few induced magnetic field measurements is an ill-posed inverse problem. To overcome this, we exploit Invertible Neural Networks (INNs) to reconstruct the conductivity field. Our qualitative results and quantitative evaluation using random error diffusion show that INN achieves far superior performance compared to Tikhonov regularization.
</details>
<details>
<summary>摘要</summary>
电解是绿色氢生产中不可或缺的过程，但在这个过程中生成的气泡会阻碍反应，降低细胞效率，并增加能源消耗。此外，这些气泡会导致细胞内的电导率发生变化，从而导致细胞外的磁场变化。因此，通过外部磁场传感器测量这些气泡引起的磁场变化，并解决反演比特-萨瓦尔律的逆问题，可以估算细胞内的电导率，并因此计算气泡的大小和位置。然而，从只有几个磁场测量获得高分辨率电导率地图是一个不定性问题。为了解决这个问题，我们利用归一化神经网络（INN）来重建电导率场。我们的资料和量化评估结果表明，INN在比特-萨瓦尔律regularization的情况下表现得更好。
</details></li>
</ul>
<hr>
<h2 id="EffSeg-Efficient-Fine-Grained-Instance-Segmentation-using-Structure-Preserving-Sparsity"><a href="#EffSeg-Efficient-Fine-Grained-Instance-Segmentation-using-Structure-Preserving-Sparsity" class="headerlink" title="EffSeg: Efficient Fine-Grained Instance Segmentation using Structure-Preserving Sparsity"></a>EffSeg: Efficient Fine-Grained Instance Segmentation using Structure-Preserving Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01545">http://arxiv.org/abs/2307.01545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cédric Picron, Tinne Tuytelaars</li>
<li>for: 提高实例分割精度和效率</li>
<li>methods: 使用Structure-Preserving Sparsity（SPS）方法，将活动特征、潜在特征和 dense 2D 索引地图分别存储，以保持2D空间结构</li>
<li>results: 与 RefineMask 相当的性能在 COCO 上，降低 FLOPs 71%，提高 FPS 29%<details>
<summary>Abstract</summary>
Many two-stage instance segmentation heads predict a coarse 28x28 mask per instance, which is insufficient to capture the fine-grained details of many objects. To address this issue, PointRend and RefineMask predict a 112x112 segmentation mask resulting in higher quality segmentations. Both methods however have limitations by either not having access to neighboring features (PointRend) or by performing computation at all spatial locations instead of sparsely (RefineMask). In this work, we propose EffSeg performing fine-grained instance segmentation in an efficient way by using our Structure-Preserving Sparsity (SPS) method based on separately storing the active features, the passive features and a dense 2D index map containing the feature indices. The goal of the index map is to preserve the 2D spatial configuration or structure between the features such that any 2D operation can still be performed. EffSeg achieves similar performance on COCO compared to RefineMask, while reducing the number of FLOPs by 71% and increasing the FPS by 29%. Code will be released.
</details>
<details>
<summary>摘要</summary>
多两个阶段实例分割头预测每个实例粗略的28x28 mask，这并不够 capture 多个 объек 的细节信息。为解决这个问题，PointRend 和 RefineMask 预测 112x112 分割mask，得到更高质量的分割结果。然而，这两种方法都有局限性，一是不能访问周围特征（PointRend），二是在所有空间位置上进行计算（RefineMask）。在这项工作中，我们提出了高效的 EffSeg，实现细化实例分割，使用我们的结构保持稀疏（SPS）方法，基于分开存储活动特征、游离特征和 dense 2D 索引地图，包含特征索引。索引地图的目的是保持 2D 空间配置或结构，以便任何 2D 操作仍可以进行。EffSeg 在 COCO 上与 RefineMask 的性能相似，而减少 FLOPs 的数量为 71%，并提高 FPS 的值为 29%。代码将被公开。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Video-Anomaly-Detection-with-Diffusion-Models-Conditioned-on-Compact-Motion-Representations"><a href="#Unsupervised-Video-Anomaly-Detection-with-Diffusion-Models-Conditioned-on-Compact-Motion-Representations" class="headerlink" title="Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations"></a>Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01533">http://arxiv.org/abs/2307.01533</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anilosmantur/conditioned_video_anomaly_diffusion">https://github.com/anilosmantur/conditioned_video_anomaly_diffusion</a></li>
<li>paper_authors: Anil Osman Tur, Nicola Dall’Asen, Cigdem Beyan, Elisa Ricci</li>
<li>for: 本研究旨在解决无监督视频异常检测（VAD）问题，即根据视频帧是否为正常或异常来分类，无需任何标签。</li>
<li>methods: 本方法使用条件扩散模型，输入数据是一个预训练网络提取的空间时间特征，条件是一个缩写视频段的运动和外观特征。</li>
<li>results: 我们的方法使用数据驱动的阈值，并将高重建错误视为异常事件。实验结果表明，我们的方法可以在两个大规模的VAD benchmark上提高异常检测性能，特别是在不同的数据集上表现更好，超过了当前state-of-the-art和基线方法。<details>
<summary>Abstract</summary>
This paper aims to address the unsupervised video anomaly detection (VAD) problem, which involves classifying each frame in a video as normal or abnormal, without any access to labels. To accomplish this, the proposed method employs conditional diffusion models, where the input data is the spatiotemporal features extracted from a pre-trained network, and the condition is the features extracted from compact motion representations that summarize a given video segment in terms of its motion and appearance. Our method utilizes a data-driven threshold and considers a high reconstruction error as an indicator of anomalous events. This study is the first to utilize compact motion representations for VAD and the experiments conducted on two large-scale VAD benchmarks demonstrate that they supply relevant information to the diffusion model, and consequently improve VAD performances w.r.t the prior art. Importantly, our method exhibits better generalization performance across different datasets, notably outperforming both the state-of-the-art and baseline methods. The code of our method is available at https://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion
</details>
<details>
<summary>摘要</summary>
本文目的是解决无监督视频异常检测（VAD）问题，即对每帧视频进行正常或异常分类，无需访问标签。为此，我们提出的方法使用条件扩散模型，其输入数据为预训练网络提取的空间时间特征，条件为视频段异常特征概括。我们的方法使用数据驱动的阈值和高重建错误作为异常事件指标。本研究是首次利用异常动作表示来实现VAD，并在两个大规模VAD标准数据集上进行了实验，证明它们为扩散模型提供有用信息，并在对比先前艺术方法时显著提高VAD性能。重要的是，我们的方法在不同数据集上表现更好的泛化性，特别是在比较先前和基eline方法时表现出优异。代码可以在https://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion上获取。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Richness-of-Learned-Compressed-Representation-of-Images-for-Semantic-Segmentation"><a href="#Exploiting-Richness-of-Learned-Compressed-Representation-of-Images-for-Semantic-Segmentation" class="headerlink" title="Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation"></a>Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01524">http://arxiv.org/abs/2307.01524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DL4Compression/Semantic_Segmentation_of_Driving_Videos_on_Learning_based_Image_Compression">https://github.com/DL4Compression/Semantic_Segmentation_of_Driving_Videos_on_Learning_based_Image_Compression</a></li>
<li>paper_authors: Ravi Kakaiya, Rakshith Sathish, Ramanathan Sethuraman, Debdoot Sheet</li>
<li>for: 这个论文目的是提出一种基于学习的压缩编码器，以减少自这些汽车与高级驾驶辅助系统（ADAS）中的数据传输网络延迟。</li>
<li>methods: 这个方法使用学习基于的压缩编码器，以减少从汽车给云端服务器的数据传输网络延迟。实验 validate 了提案的管道，并证明了学习压缩表示可以用于进行像分类 segmentation 的任务，同时实现了 $66 \times$ 的压缩因子，并保留了适用于分类的信息，而且降低了总 Compute 的 $11%$。</li>
<li>results: 这个研究的结果显示，使用学习基于的压缩编码器可以实现高效的数据传输和分类 tasks，并且可以降低从汽车至云端服务器的网络延迟。<details>
<summary>Abstract</summary>
Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the potential to radically change the way we travel. Many such vehicles currently rely on segmentation and object detection algorithms to detect and track objects around its surrounding. The data collected from the vehicles are often sent to cloud servers to facilitate continual/life-long learning of these algorithms. Considering the bandwidth constraints, the data is compressed before sending it to servers, where it is typically decompressed for training and analysis. In this work, we propose the use of a learning-based compression Codec to reduce the overhead in latency incurred for the decompression operation in the standard pipeline. We demonstrate that the learned compressed representation can also be used to perform tasks like semantic segmentation in addition to decompression to obtain the images. We experimentally validate the proposed pipeline on the Cityscapes dataset, where we achieve a compression factor up to $66 \times$ while preserving the information required to perform segmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved using decompressed images while reducing the overall compute by $11\%$.
</details>
<details>
<summary>摘要</summary>
自动驾驶车和高级驾驶助手系统（ADAS）有可能对我们的旅行方式产生重大变革。许多这些车辆目前使用分割和物体检测算法来检测和跟踪周围的对象。收集到的数据通常将被送往云服务器以便持续学习这些算法。由于带宽限制，这些数据通常会进行压缩，然后在服务器上解压缩以进行训练和分析。在这项工作中，我们提议使用学习基于的压缩编码器来减少标准管道中的延迟过程中的开销。我们示出了学习压缩表示可以同时完成压缩和减少计算的任务，例如 semantic segmentation。我们对Cityscapes dataset进行实验，并实现了最高的66倍压缩因子，保留了用于完成分割的信息，而计算总量减少了11%。
</details></li>
</ul>
<hr>
<h2 id="LPN-Language-guided-Prototypical-Network-for-few-shot-classification"><a href="#LPN-Language-guided-Prototypical-Network-for-few-shot-classification" class="headerlink" title="LPN: Language-guided Prototypical Network for few-shot classification"></a>LPN: Language-guided Prototypical Network for few-shot classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01515">http://arxiv.org/abs/2307.01515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaihui Cheng, Chule Yang</li>
<li>for: 这篇论文主要针对几何shot分类问题进行研究，旨在对新任务进行适应，并充分利用可用的数据。</li>
<li>methods: 本文提出了一个名为Language-guided Prototypical Network（LPN）的方法，它利用视觉和语言模式的共同作用，通过两个平行分支来实现这一目的。</li>
<li>results: 实验结果显示，LPN方法与现有方法相比，在 benchmark 数据集上表现竞争力强。<details>
<summary>Abstract</summary>
Few-shot classification aims to adapt to new tasks with limited labeled examples. To fully use the accessible data, recent methods explore suitable measures for the similarity between the query and support images and better high-dimensional features with meta-training and pre-training strategies. However, the potential of multi-modality information has barely been explored, which may bring promising improvement for few-shot classification. In this paper, we propose a Language-guided Prototypical Network (LPN) for few-shot classification, which leverages the complementarity of vision and language modalities via two parallel branches. Concretely, to introduce language modality with limited samples in the visual task, we leverage a pre-trained text encoder to extract class-level text features directly from class names while processing images with a conventional image encoder. Then, a language-guided decoder is introduced to obtain text features corresponding to each image by aligning class-level features with visual features. In addition, to take advantage of class-level features and prototypes, we build a refined prototypical head that generates robust prototypes in the text branch for follow-up measurement. Finally, we aggregate the visual and text logits to calibrate the deviation of a single modality. Extensive experiments demonstrate the competitiveness of LPN against state-of-the-art methods on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
《语言指导的原型网络（LPN） для少量类别分类》目标是在新任务上适应，以限量标注示例进行训练。为了充分利用可用数据，当前方法探索适合查询和支持图像之间的相似度度量和高维特征的meta-training和预训练策略。然而，多Modal信息的潜力尚未得到充分利用，这可能带来了promising的改进。在这篇论文中，我们提议一种语言指导的原型网络（LPN），该网络利用视觉和语言Modal的 complementarity，通过两个平行分支来进行图像分类。具体来说，为了在视觉任务中使用有限的语言样本，我们利用预训练的文本编码器提取类别特征直接从类名称中，然后将这些特征与图像进行处理。接着，我们引入语言指导的解码器，以获取每个图像的特有的文本特征。此外，为了利用类别特征和原型，我们构建了一个精细的原型头，该头在文本分支中生成了robust的原型。最后，我们将视觉和文本的启示拼接起来，以调整单一模式的偏差。广泛的实验表明，LPN在标准 benchmark 数据集上与当前状态OF-the-art方法竞争。
</details></li>
</ul>
<hr>
<h2 id="SelfFed-Self-supervised-Federated-Learning-for-Data-Heterogeneity-and-Label-Scarcity-in-IoMT"><a href="#SelfFed-Self-supervised-Federated-Learning-for-Data-Heterogeneity-and-Label-Scarcity-in-IoMT" class="headerlink" title="SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT"></a>SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01514">http://arxiv.org/abs/2307.01514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunder Ali Khowaja, Kapal Dev, Syed Muhammad Anwar, Marius George Linguraru</li>
<li>for: 这个研究旨在提出一个基于自适应学习的联邦学习框架，以解决联邦学习中的数据不均匀问题和标签稀缺问题。</li>
<li>methods: 我们提出了一个两阶段的方法，首先是预训练阶段，使用Swin Transformer基本Encoder在分散式的方式下进行增强模型。第二阶段是精度调整阶段，引入了对照网络和一个新的联合策略，用于在分散式的方式下训练仅有几个标签的目标任务。</li>
<li>results: 我们在公开可用的医疗影像数据集上进行实验分析，结果显示我们的提出的SelfFed框架在非相同和相同数据集上比 existed baseline perform 更好，尤其是在标签稀缺的情况下。我们的方法在非IID数据集上取得最大改进率为8.8%和4.1%。此外，我们的方法甚至在仅使用10%标签的情况下也能够超越 existed baseline。<details>
<summary>Abstract</summary>
Self-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem. We perform our experimental analysis on publicly available medical imaging datasets and show that our proposed SelfFed framework performs better when compared to existing baselines concerning non-independent and identically distributed (IID) data and label scarcity. Our method achieves a maximum improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID dataset. Further, our proposed method outperforms existing baselines even when trained on a few (10%) labeled instances.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified ChineseSelf-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem. We perform our experimental analysis on publicly available medical imaging datasets and show that our proposed SelfFed framework performs better when compared to existing baselines concerning non-independent and identically distributed (IID) data and label scarcity. Our method achieves a maximum improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID dataset. Further, our proposed method outperforms existing baselines even when trained on a few (10%) labeled instances.中文简体版：自我超级vised学习在联邦学习模式下受到了互联网医疗器件（IoMT）领域的广泛关注，因为它可以在无标签的隔离数据上进行协同学习。然而，基于自我超级vised的联邦学习策略受到了数据不一致和标签稀缺的问题的影响。在这篇论文中，我们提出了SelfFed框架，用于解决这些问题。我们的SelfFed框架包括两个阶段：首先是预训练阶段，使用SwinTransformer基于 encoder 进行增强模型化，在分布式方式下进行。这个阶段可以减轻数据不一致问题。第二个阶段是精度调整阶段，引入对比网络和一种新的聚合策略，通过限量标签数据进行定制。这个阶段可以解决标签稀缺问题。我们在公共可用的医学成像数据集上进行了实验分析，并证明了我们的SelfFed框架在非独立和同分布（IID）数据上比较出色。我们的方法在非IID数据上最大提高8.8%和4.1%的提升。此外，我们的提posed方法还可以在只有10%的标签实例上进行训练，并且在这些情况下也能够超越现有的基elines。
</details></li>
</ul>
<hr>
<h2 id="FB-OCC-3D-Occupancy-Prediction-based-on-Forward-Backward-View-Transformation"><a href="#FB-OCC-3D-Occupancy-Prediction-based-on-Forward-Backward-View-Transformation" class="headerlink" title="FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation"></a>FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01492">http://arxiv.org/abs/2307.01492</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvlabs/fb-bev">https://github.com/nvlabs/fb-bev</a></li>
<li>paper_authors: Zhiqi Li, Zhiding Yu, David Austin, Mingsheng Fang, Shiyi Lan, Jan Kautz, Jose M. Alvarez</li>
<li>for: 本研究旨在提出一个获胜解决方案来3D占用预测挑战，该挑战是CVPR 2023 Workshop on End-to-End Autonomous Driving和CVPR 23 Workshop on Vision-Centric Autonomous Driving Workshop的一部分。</li>
<li>methods: 本研究基于FB-BEV，一个前进后退投影的镜头基础预测设计。在FB-BEV的基础上，我们进一步研究了特有的设计和优化，包括共同深度-Semantic预训练、共同矩阵-BEV表示、模型缩放和有效的后处理策略。</li>
<li>results: 这些设计和优化导致在nuScenes数据集上的mIoU分数为54.19%，在挑战赛道上排名第一。代码和模型将在：<a target="_blank" rel="noopener" href="https://github.com/NVlabs/FB-BEV">https://github.com/NVlabs/FB-BEV</a> 中发布。<details>
<summary>Abstract</summary>
This technical report summarizes the winning solution for the 3D Occupancy Prediction Challenge, which is held in conjunction with the CVPR 2023 Workshop on End-to-End Autonomous Driving and CVPR 23 Workshop on Vision-Centric Autonomous Driving Workshop. Our proposed solution FB-OCC builds upon FB-BEV, a cutting-edge camera-based bird's-eye view perception design using forward-backward projection. On top of FB-BEV, we further study novel designs and optimization tailored to the 3D occupancy prediction task, including joint depth-semantic pre-training, joint voxel-BEV representation, model scaling up, and effective post-processing strategies. These designs and optimization result in a state-of-the-art mIoU score of 54.19% on the nuScenes dataset, ranking the 1st place in the challenge track. Code and models will be released at: https://github.com/NVlabs/FB-BEV.
</details>
<details>
<summary>摘要</summary>
这份技术报告介绍了在CVPR 2023 工作坊上的3D占用预测挑战赛中获胜的解决方案，该挑战赛与CVPR 23 工作坊联合举行。我们的提议方案FB-OCC基于FB-BEV，这是一种前瞻型镜头视图识别设计，使用前进和后退投影。在FB-BEV的基础之上，我们进一步研究了适应3D占用预测任务的新设计和优化策略，包括共同深度semantic预训练、共同矩阵BEV表示、模型缩放和有效 posterior 处理策略。这些设计和优化使得我们在nuScenes数据集上 achieve 54.19%的mIoU分数，在挑战赛中名列第一名。代码和模型将在：https://github.com/NVlabs/FB-BEV 中发布。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Segmentation-on-3D-Point-Clouds-with-High-Density-Variations"><a href="#Semantic-Segmentation-on-3D-Point-Clouds-with-High-Density-Variations" class="headerlink" title="Semantic Segmentation on 3D Point Clouds with High Density Variations"></a>Semantic Segmentation on 3D Point Clouds with High Density Variations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01489">http://arxiv.org/abs/2307.01489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Faulkner, Luke Haub, Simon Ratcliffe, Ian Reid, Tat-Jun Chin</li>
<li>for: 这篇论文是为了解决 LiDAR 探测应用中的大规模 3D 点云对应问题，这些点云具有广泛的区域和距离，并且具有巨大的地方密度变化。</li>
<li>methods: 这篇论文提出了一个名为 HDVNet 的新架构，这个架构包含一个嵌入的集合 Encoder-Decoder 路径，每个路径处理特定的点密度范围。限制Feature Map 之间的连接，使得 HDVNet 能够根据点密度来衡量每个特征的可靠性，例如，对于低密度物体而言，杜重高密度的特征。</li>
<li>results: 在实际的点云数据中，HDVNet 比 state-of-the-art 模型具有更高的准确性，只需使用一半的 Parameters。<details>
<summary>Abstract</summary>
LiDAR scanning for surveying applications acquire measurements over wide areas and long distances, which produces large-scale 3D point clouds with significant local density variations. While existing 3D semantic segmentation models conduct downsampling and upsampling to build robustness against varying point densities, they are less effective under the large local density variations characteristic of point clouds from surveying applications. To alleviate this weakness, we propose a novel architecture called HDVNet that contains a nested set of encoder-decoder pathways, each handling a specific point density range. Limiting the interconnections between the feature maps enables HDVNet to gauge the reliability of each feature based on the density of a point, e.g., downweighting high density features not existing in low density objects. By effectively handling input density variations, HDVNet outperforms state-of-the-art models in segmentation accuracy on real point clouds with inconsistent density, using just over half the weights.
</details>
<details>
<summary>摘要</summary>
李达尔扫描 для测量应用程序获取大面积和长距离的测量值，生成大规模的3D点云，点云中存在显著的地方密度变化。现有的3D semantic segmentation模型通过下采样和上采样来建立对点云密度变化的鲁棒性，但这些模型对大规模的点云密度变化不够有效。为解决这个弱点，我们提出了一种新的架构 called HDVNet，它包含一个嵌入式的编码器-解码器路径，每个路径处理特定的点密度范围。限制Feature map之间的连接，使得 HDVNet 可以根据点的密度来评估特征的可靠性，例如，减重高密度特征不存在低密度对象中。通过有效地处理输入密度变化，HDVNet 在实际点云中 segments 精度高于状态机制模型，使用只有一半的权重。
</details></li>
</ul>
<hr>
<h2 id="H-DenseFormer-An-Efficient-Hybrid-Densely-Connected-Transformer-for-Multimodal-Tumor-Segmentation"><a href="#H-DenseFormer-An-Efficient-Hybrid-Densely-Connected-Transformer-for-Multimodal-Tumor-Segmentation" class="headerlink" title="H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation"></a>H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01486">http://arxiv.org/abs/2307.01486</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shijun18/h-denseformer">https://github.com/shijun18/h-denseformer</a></li>
<li>paper_authors: Jun Shi, Hongyu Kan, Shulan Ruan, Ziqi Zhu, Minfan Zhao, Liang Qiao, Zhaohui Wang, Hong An, Xudong Xue</li>
<li>for: 这篇论文旨在提出一个混合式密接连接网络（H-DenseFormer）来进行肿瘤分类，以提高多modalities的医疗影像识别能力。</li>
<li>methods: 这篇论文使用了一个具有多路平行嵌入（MPE）模组，可以将多modalities的输入资料组合成多modalities的融合特征。然后，这些融合特征会被传递到不同层次的encoder中进行增强多modalities的学习表现。此外，还设计了一个轻量级的密接连接几何（DCT）封顶，以取代标准几何封顶，从而实现显著的计算量削减。</li>
<li>results: 在HECKTOR21和PI-CAI22两个公共多modalities datasets上进行了广泛的实验，结果显示，我们的提案方法在与现有的州前方法进行比较中，具有更高的识别率和更低的计算量。代码可以在<a target="_blank" rel="noopener" href="https://github.com/shijun18/H-DenseFormer%E4%B8%8A%E5%8F%96%E5%BE%97%E3%80%82">https://github.com/shijun18/H-DenseFormer上取得。</a><details>
<summary>Abstract</summary>
Recently, deep learning methods have been widely used for tumor segmentation of multimodal medical images with promising results. However, most existing methods are limited by insufficient representational ability, specific modality number and high computational complexity. In this paper, we propose a hybrid densely connected network for tumor segmentation, named H-DenseFormer, which combines the representational power of the Convolutional Neural Network (CNN) and the Transformer structures. Specifically, H-DenseFormer integrates a Transformer-based Multi-path Parallel Embedding (MPE) module that can take an arbitrary number of modalities as input to extract the fusion features from different modalities. Then, the multimodal fusion features are delivered to different levels of the encoder to enhance multimodal learning representation. Besides, we design a lightweight Densely Connected Transformer (DCT) block to replace the standard Transformer block, thus significantly reducing computational complexity. We conduct extensive experiments on two public multimodal datasets, HECKTOR21 and PI-CAI22. The experimental results show that our proposed method outperforms the existing state-of-the-art methods while having lower computational complexity. The source code is available at https://github.com/shijun18/H-DenseFormer.
</details>
<details>
<summary>摘要</summary>
近些年来，深度学习方法在多Modal医学影像肿瘤分割方面取得了可观的成果。然而，现有的方法受到表达能力不充分、特定的Modal数量和高计算复杂性的限制。在这篇论文中，我们提出了一种混合 densely connected network  для肿瘤分割，称为 H-DenseFormer，它将 CNN 和 Transformer 结构相结合。具体来说，H-DenseFormer 使用 Transformer 结构基于多Path Parallel Embedding（MPE）模块，可以将任意数量的Modalities作为输入，以提取不同Modalities中的融合特征。然后，多Modal融合特征被传递到不同层次的编码器，以增强多Modal学习表达。此外，我们设计了一种轻量级的 Densely Connected Transformer（DCT）块，以取代标准 Transformer 块，从而实现显著降低计算复杂性。我们对公共的两个多Modal 数据集进行了广泛的实验，结果表明，我们的提议方法在计算复杂性下与现有的状态态势表达方法相比，表现出色，并且可以在多Modal 数据集上达到更高的识别率。代码可以在 https://github.com/shijun18/H-DenseFormer 上找到。
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Driver-Gaze-Estimation-and-Application-in-Gaze-Behavior-Understanding"><a href="#A-Review-of-Driver-Gaze-Estimation-and-Application-in-Gaze-Behavior-Understanding" class="headerlink" title="A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding"></a>A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01470">http://arxiv.org/abs/2307.01470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Kumar Sharma, Pranamesh Chakraborty</li>
<li>for: 本研究的主要目标是对 Driver Gaze 基础知识、测算方法和应用场景进行全面的概述，以便更好地理解和应用 Driver Gaze 技术。</li>
<li>methods: 本研究主要使用 Head-mounted 和远程设置基于眼动估计的方法，并详细介绍了现有的 Driver Gaze 数据集和估计算法，包括传统机器学习和深度学习等技术。</li>
<li>results: 本研究使用 Driver Gaze 估计结果来理解 drivers 在拐弯、加速和减速时的眼动行为，以及道路投放广告结构的影响。同时，本研究也提出了现有文献的限制、挑战和未来发展方向。<details>
<summary>Abstract</summary>
Driver gaze plays an important role in different gaze-based applications such as driver attentiveness detection, visual distraction detection, gaze behavior understanding, and building driver assistance system. The main objective of this study is to perform a comprehensive summary of driver gaze fundamentals, methods to estimate driver gaze, and it's applications in real world driving scenarios. We first discuss the fundamentals related to driver gaze, involving head-mounted and remote setup based gaze estimation and the terminologies used for each of these data collection methods. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and the equipment used for such data collection. This is followed by a discussion of the algorithms used for driver gaze estimation, which primarily involves traditional machine learning and deep learning based techniques. The estimated driver gaze is then used for understanding gaze behavior while maneuvering through intersections, on-ramps, off-ramps, lane changing, and determining the effect of roadside advertising structures. Finally, we have discussed the limitations in the existing literature, challenges, and the future scope in driver gaze estimation and gaze-based applications.
</details>
<details>
<summary>摘要</summary>
Driver's gaze plays an important role in various gaze-based applications such as driver attentiveness detection, visual distraction detection, and understanding gaze behavior. The main objective of this study is to provide a comprehensive summary of driver gaze fundamentals, methods for estimating driver gaze, and its applications in real-world driving scenarios.First, we discuss the fundamentals of driver gaze, including head-mounted and remote setup-based gaze estimation, and the terminologies used for each of these data collection methods. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and equipment used for such data collection.Then, we discuss the algorithms used for driver gaze estimation, which primarily involve traditional machine learning and deep learning-based techniques. The estimated driver gaze is used for understanding gaze behavior while maneuvering through intersections, on-ramps, off-ramps, lane changing, and determining the effect of roadside advertising structures.Finally, we discuss the limitations in the existing literature, challenges, and the future scope in driver gaze estimation and gaze-based applications.
</details></li>
</ul>
<hr>
<h2 id="Generating-Animatable-3D-Cartoon-Faces-from-Single-Portraits"><a href="#Generating-Animatable-3D-Cartoon-Faces-from-Single-Portraits" class="headerlink" title="Generating Animatable 3D Cartoon Faces from Single Portraits"></a>Generating Animatable 3D Cartoon Faces from Single Portraits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01468">http://arxiv.org/abs/2307.01468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanyu Pan, Guowei Yang, Taijiang Mu, Yu-Kun Lai</li>
<li>for: 本研究旨在提供一种生成可animatable 3D漫画人脸的新方法，以满足现代虚拟现实技术的需求。</li>
<li>methods: 我们提出了一种两stage reconstruction方法，首先使用StyleGAN将输入的真实世界照片转换为 стили化漫画图像，然后通过非rigid deformation以准确地还原3D漫画人脸的细节xture。最后，我们提出了一种基于手动创建的模板和变形传递的semantic preserving face rigging方法。</li>
<li>results: 相比之前的研究，我们的方法可以更好地保持人脸的准确性、美观性和相似性标准。此外，我们还实现了在实时进行人脸动画的能力。<details>
<summary>Abstract</summary>
With the booming of virtual reality (VR) technology, there is a growing need for customized 3D avatars. However, traditional methods for 3D avatar modeling are either time-consuming or fail to retain similarity to the person being modeled. We present a novel framework to generate animatable 3D cartoon faces from a single portrait image. We first transfer an input real-world portrait to a stylized cartoon image with a StyleGAN. Then we propose a two-stage reconstruction method to recover the 3D cartoon face with detailed texture, which first makes a coarse estimation based on template models, and then refines the model by non-rigid deformation under landmark supervision. Finally, we propose a semantic preserving face rigging method based on manually created templates and deformation transfer. Compared with prior arts, qualitative and quantitative results show that our method achieves better accuracy, aesthetics, and similarity criteria. Furthermore, we demonstrate the capability of real-time facial animation of our 3D model.
</details>
<details>
<summary>摘要</summary>
随着虚拟现实（VR）技术的发展，个性化3D人物模型的需求在增长。然而，传统的3D人物模型方法 Either time-consuming or failure to retain the similarity of the person being modeled. We present a novel framework to generate animatable 3D cartoon faces from a single portrait image. We first transfer an input real-world portrait to a stylized cartoon image with a StyleGAN. Then we propose a two-stage reconstruction method to recover the 3D cartoon face with detailed texture, which first makes a coarse estimation based on template models, and then refines the model by non-rigid deformation under landmark supervision. Finally, we propose a semantic preserving face rigging method based on manually created templates and deformation transfer. Compared with prior arts, qualitative and quantitative results show that our method achieves better accuracy, aesthetics, and similarity criteria. Furthermore, we demonstrate the capability of real-time facial animation of our 3D model.Here's a breakdown of the translation:* "随着" (suī zhe) - "with the development of"* "虚拟现实" (hū zhì shèng jì) - "virtual reality"* "技术" (jì shu) - "technology"* "个性化" (ge xìng yào) - "customized"* "3D人物模型" (3D rén wù molding) - "3D avatar model"* "需求" (xū yè) - "need"* "在增长" (zài jìn cháng) - "is growing"* "传统的" (chuán tǒng de) - "traditional"* "3D人物模型方法" (3D rén wù molding fāng fá) - "3D avatar modeling methods"* "Either time-consuming or failure to retain the similarity of the person being modeled" - This phrase is translated as "或者时间耗费或失去人物相似性" (or zhī shí hòu fèi or shī qù rén wù xiàng yì)* "We present a novel framework to generate animatable 3D cartoon faces from a single portrait image" - This phrase is translated as "我们提出了一种新的框架，可以从单一的肖像图片中生成可动的3D漫画人脸" (wǒ men tī shè le yī zhōng xīn de kāng yì, cóng zhī yī zhōng shèng cháng yǐn dào yī zhōng)* "We first transfer an input real-world portrait to a stylized cartoon image with a StyleGAN" - This phrase is translated as "我们首先将输入的真实世界肖像图片转化为一个风格化的漫画人脸，使用StyleGAN" (wǒ men chū xiān shì yǐn zhī shì jīn shì, cóng zhī yī zhōng shèng cháng yǐn dào yī zhōng)* "Then we propose a two-stage reconstruction method to recover the 3D cartoon face with detailed texture" - This phrase is translated as "然后我们提出了一种两阶段重建方法，以获取3D漫画人脸的详细TEXTURE" (rán hái wǒ men tī shè le yī zhōng èr jiāng tiě jīng fāng fá, yǐn dào jīn shì sān jiāng)* "which first makes a coarse estimation based on template models, and then refines the model by non-rigid deformation under landmark supervision" - This phrase is translated as "首先基于模板模型进行大致的估计，然后通过非固定形态的扭变以下领别监督进行细化" (chū shí zhī shì yǐn zhī shì, cóng zhī yī zhōng shèng cháng yǐn dào)* "Finally, we propose a semantic preserving face rigging method based on manually created templates and deformation transfer" - This phrase is translated as "最后，我们提出了一种基于手动创建的模板和形态传递的 semantics保持的人脸动画方法" (wǒ men tī shè le yī zhōng xiān shì yǐn zhī shì, cóng zhī yī zhōng shèng cháng yǐn dào)* "Compared with prior arts, qualitative and quantitative results show that our method achieves better accuracy, aesthetics, and similarity criteria" - This phrase is translated as "与之前的艺术比较，我们的方法在准确性、艺术性和相似性标准上都达到了更高的水平" (yǔ zhī qián de yì shù bǐ kě, wǒ men de fāng fá zài jìn cháng yǐn dào zhèng zhì, xiàng yì yì shù)* "Furthermore, we demonstrate the capability of real-time facial animation of our 3D model" - This phrase is translated as "此外，我们还展示了我们的3D模型在实时动画方面的能力" (qí wài, wǒ men hái jiǎng yǐn dào)
</details></li>
</ul>
<hr>
<h2 id="Technical-Report-for-Ego4D-Long-Term-Action-Anticipation-Challenge-2023"><a href="#Technical-Report-for-Ego4D-Long-Term-Action-Anticipation-Challenge-2023" class="headerlink" title="Technical Report for Ego4D Long Term Action Anticipation Challenge 2023"></a>Technical Report for Ego4D Long Term Action Anticipation Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01467">http://arxiv.org/abs/2307.01467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatsuya Ishibashi, Kosuke Ono, Noriyuki Kugo, Yuji Sato</li>
<li>for: 预测未来动作序列</li>
<li>methods: 基于SlowFast和SlowFast-CLIP模型的ensemble，加入 Label smoothing 和动作类别（动词、名称）的约束</li>
<li>results: 超越基线性能，在公共领先板上记录第二名成绩<details>
<summary>Abstract</summary>
In this report, we describe the technical details of our approach for the Ego4D Long-Term Action Anticipation Challenge 2023. The aim of this task is to predict a sequence of future actions that will take place at an arbitrary time or later, given an input video. To accomplish this task, we introduce three improvements to the baseline model, which consists of an encoder that generates clip-level features from the video, an aggregator that integrates multiple clip-level features, and a decoder that outputs Z future actions. 1) Model ensemble of SlowFast and SlowFast-CLIP; 2) Label smoothing to relax order constraints for future actions; 3) Constraining the prediction of the action class (verb, noun) based on word co-occurrence. Our method outperformed the baseline performance and recorded as second place solution on the public leaderboard.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我们介绍了我们对Ego4D长期动作预测挑战2023年的技术细节。该任务的目标是基于输入视频预测未来动作的序列，我们在这里引入了三种改进基eline模型，即encoder生成视频clip级别特征，aggregator将多个clip级别特征集成，以及decoder输出Z个未来动作。1）SlowFast和SlowFast-CLIP模型集成；2）将未来动作顺序约束松弛为label smoothing；3）基于单词共occurrence constrain动作类别（动词、名词）预测。我们的方法比基eline表现更好，在公共排名板上录制第二名。
</details></li>
</ul>
<hr>
<h2 id="AdAM-Few-Shot-Image-Generation-via-Adaptation-Aware-Kernel-Modulation"><a href="#AdAM-Few-Shot-Image-Generation-via-Adaptation-Aware-Kernel-Modulation" class="headerlink" title="AdAM: Few-Shot Image Generation via Adaptation-Aware Kernel Modulation"></a>AdAM: Few-Shot Image Generation via Adaptation-Aware Kernel Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01465">http://arxiv.org/abs/2307.01465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunqing Zhao, Keshigeyan Chandrasegaran, Abdollahzadeh Milad, Chao Du, Tianyu Pang, Ruoteng Li, Henghui Ding, Ngai-Man Cheung</li>
<li>for: 这个论文目的是解决几个示例（例如10）训练样本的图像生成问题。</li>
<li>methods: 这个论文使用了一个已经在大规模源频道上预训练的GAN，并将其适应到目标频道。Central to recent FSIG methods are knowledge preservation criteria, which select and preserve a subset of source knowledge to the adapted model.</li>
<li>results: 我们的研究表明，许多现有的State-of-the-art（SOTA）方法，只考虑源频道，在不同的频道距离下进行适应，表现不佳。我们的第二个贡献是提出了适应aware的kernel Modulation（AdAM），用于通用的几个源-目标频道距离的图像生成。广泛的实验表明，AdAM可以在挑战性的设置下 consistently achieve SOTA performance。<details>
<summary>Abstract</summary>
Few-shot image generation (FSIG) aims to learn to generate new and diverse images given few (e.g., 10) training samples. Recent work has addressed FSIG by leveraging a GAN pre-trained on a large-scale source domain and adapting it to the target domain with few target samples. Central to recent FSIG methods are knowledge preservation criteria, which select and preserve a subset of source knowledge to the adapted model. However, a major limitation of existing methods is that their knowledge preserving criteria consider only source domain/task and fail to consider target domain/adaptation in selecting source knowledge, casting doubt on their suitability for setups of different proximity between source and target domain. Our work makes two contributions. Firstly, we revisit recent FSIG works and their experiments. We reveal that under setups which assumption of close proximity between source and target domains is relaxed, many existing state-of-the-art (SOTA) methods which consider only source domain in knowledge preserving perform no better than a baseline method. As our second contribution, we propose Adaptation-Aware kernel Modulation (AdAM) for general FSIG of different source-target domain proximity. Extensive experiments show that AdAM consistently achieves SOTA performance in FSIG, including challenging setups where source and target domains are more apart.
</details>
<details>
<summary>摘要</summary>
几个示例图像生成（FSIG）目标是通过几个（例如10）训练样本学习生成新和多样化的图像。 latest work has addressed FSIG by leveraging a pre-trained GAN on a large-scale source domain and adapting it to the target domain with few target samples. central to recent FSIG methods are knowledge preservation criteria, which select and preserve a subset of source knowledge to the adapted model. However, a major limitation of existing methods is that their knowledge preserving criteria consider only source domain/task and fail to consider target domain/adaptation in selecting source knowledge, casting doubt on their suitability for setups of different proximity between source and target domain. Our work makes two contributions. Firstly, we revisit recent FSIG works and their experiments. We reveal that under setups which assumption of close proximity between source and target domains is relaxed, many existing state-of-the-art (SOTA) methods which consider only source domain in knowledge preserving perform no better than a baseline method. As our second contribution, we propose Adaptation-Aware kernel Modulation (AdAM) for general FSIG of different source-target domain proximity. Extensive experiments show that AdAM consistently achieves SOTA performance in FSIG, including challenging setups where source and target domains are more apart.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Quality-Prediction-for-Improved-Single-Frame-and-Weighted-Sequential-Visual-Place-Recognition"><a href="#Unsupervised-Quality-Prediction-for-Improved-Single-Frame-and-Weighted-Sequential-Visual-Place-Recognition" class="headerlink" title="Unsupervised Quality Prediction for Improved Single-Frame and Weighted Sequential Visual Place Recognition"></a>Unsupervised Quality Prediction for Improved Single-Frame and Weighted Sequential Visual Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01464">http://arxiv.org/abs/2307.01464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helen Carson, Jason J. Ford, Michael Milford</li>
<li>for: 本研究旨在提高自动驾驶系统的定位和视觉定位技术的可靠性和预测性。</li>
<li>methods: 本研究使用一种新的训练自由的方法来预测定位估计的质量，并使用这些预测来偏补一种序列匹配过程，以实现更高的精度性能。</li>
<li>results: 在四个数据集和三种视觉定位技术上，我们的结合系统可以提高定位精度性能，特别是在高精度低匹配点操作点上。我们还提供了减少和分析，以分析预测系统和偏补序列匹配器的性能贡献。<details>
<summary>Abstract</summary>
While substantial progress has been made in the absolute performance of localization and Visual Place Recognition (VPR) techniques, it is becoming increasingly clear from translating these systems into applications that other capabilities like integrity and predictability are just as important, especially for safety- or operationally-critical autonomous systems. In this research we present a new, training-free approach to predicting the likely quality of localization estimates, and a novel method for using these predictions to bias a sequence-matching process to produce additional performance gains beyond that of a naive sequence matching approach. Our combined system is lightweight, runs in real-time and is agnostic to the underlying VPR technique. On extensive experiments across four datasets and three VPR techniques, we demonstrate our system improves precision performance, especially at the high-precision/low-recall operating point. We also present ablation and analysis identifying the performance contributions of the prediction and weighted sequence matching components in isolation, and the relationship between the quality of the prediction system and the benefits of the weighted sequential matcher.
</details>
<details>
<summary>摘要</summary>
“尽管当地化和视觉地标识（VPR）技术的绝对性表现已经取得了 significiant 进步，但是在将这些系统应用于实际应用中，其他特性如完整性和预测性也变得越来越重要，特别是 для安全或操作critical的自动驾驶系统。在这项研究中，我们提出了一种新的、无需训练的方法来预测当地化估计的质量，以及一种新的方法来使用这些预测来偏补一个序列匹配过程，以实现更高的性能提升。我们的整体系统轻量级、实时执行，并且对于下述VPR技术无关。在四个数据集和三种VPR技术的广泛实验中，我们证明了我们的系统可以提高精度性能，特别是在高精度/低回归点操作点。我们还提供了剥离和分析，描述预测和权重序列匹配组件在孤立情况下的性能贡献，以及预测系统质量和权重序列匹配的关系。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Practical-Collaborative-Perception-A-Framework-for-Asynchronous-and-Multi-Agent-3D-Object-Detection"><a href="#Practical-Collaborative-Perception-A-Framework-for-Asynchronous-and-Multi-Agent-3D-Object-Detection" class="headerlink" title="Practical Collaborative Perception: A Framework for Asynchronous and Multi-Agent 3D Object Detection"></a>Practical Collaborative Perception: A Framework for Asynchronous and Multi-Agent 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01462">http://arxiv.org/abs/2307.01462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minh-Quan Dao, Julie Stephany Berrio, Vincent Frémont, Mao Shan, Elwan Héry, Stewart Worrall</li>
<li>for: 提高 LiDAR-based 物体探测方法中的遮挡问题，特别是在城市交通中， где egocar 需要可靠的物体探测，以避免碰撞，而其视场受到了大量道路用户的阻挡。</li>
<li>methods: collaborative perception via Vehicle-to-Everything (V2X) communication，利用多个连接的代理机构形成完整的场景表示，并通过中间协作来解决性能和带宽之间的负担。</li>
<li>results: 我们提出了一种简单又有效的协作方法，可以在实际应用中超越先前的状态艺术方法，同时尽可能减少对单车辆检测模型的修改和假设不实的多个代理机构同步。实验结果表明，我们的协作方法可以达到98%的性能水平，只 consume相同的带宽，与先前的中间协作方法相比。<details>
<summary>Abstract</summary>
Occlusion is a major challenge for LiDAR-based object detection methods. This challenge becomes safety-critical in urban traffic where the ego vehicle must have reliable object detection to avoid collision while its field of view is severely reduced due to the obstruction posed by a large number of road users. Collaborative perception via Vehicle-to-Everything (V2X) communication, which leverages the diverse perspective thanks to the presence at multiple locations of connected agents to form a complete scene representation, is an appealing solution. State-of-the-art V2X methods resolve the performance-bandwidth tradeoff using a mid-collaboration approach where the Bird-Eye View images of point clouds are exchanged so that the bandwidth consumption is lower than communicating point clouds as in early collaboration, and the detection performance is higher than late collaboration, which fuses agents' output, thanks to a deeper interaction among connected agents. While achieving strong performance, the real-world deployment of most mid-collaboration approaches is hindered by their overly complicated architectures, involving learnable collaboration graphs and autoencoder-based compressor/ decompressor, and unrealistic assumptions about inter-agent synchronization. In this work, we devise a simple yet effective collaboration method that achieves a better bandwidth-performance tradeoff than prior state-of-the-art methods while minimizing changes made to the single-vehicle detection models and relaxing unrealistic assumptions on inter-agent synchronization. Experiments on the V2X-Sim dataset show that our collaboration method achieves 98\% of the performance of an early-collaboration method, while only consuming the equivalent bandwidth of a late-collaboration method.
</details>
<details>
<summary>摘要</summary>
干扰是LiDAR基于对象检测方法的主要挑战。在城市交通中， egovehicle 需要可靠的对象检测，以避免事故，而其视场受到了大量道路用户的干扰。 Collaborative perception via Vehicle-to-Everything (V2X) communication 是一种吸引人的解决方案，它利用了多个位置的连接代理人形成完整的场景表示，并且可以解决性能和带宽的贸易。现状的V2X方法在性能和带宽之间进行了中间协作，通过在 Bird-Eye View 图像上进行点云的交换，以避免在早期协作中的带宽浪费，并在晚期协作中进行拼接，以获得更高的检测性能。在实现优秀性能的同时，大多数中间协作方法的实际部署受到了其复杂的架构和学习可学的协作图、 autoencoder 基于的压缩器/解压缩器的假设，以及 между代理人的不realistic 同步假设。在这种工作中，我们设计了一种简单 yet 有效的协作方法，可以在带宽-性能贸易中提供更好的贸易比例，而且尽可能地避免改变单车检测模型，并放弃不realistic 的 между代理人同步假设。 V2X-Sim 数据集的实验表明，我们的协作方法可以达到98%的性能，只消耗与晚期协作相同的带宽。
</details></li>
</ul>
<hr>
<h2 id="Learning-Feature-Matching-via-Matchable-Keypoint-Assisted-Graph-Neural-Network"><a href="#Learning-Feature-Matching-via-Matchable-Keypoint-Assisted-Graph-Neural-Network" class="headerlink" title="Learning Feature Matching via Matchable Keypoint-Assisted Graph Neural Network"></a>Learning Feature Matching via Matchable Keypoint-Assisted Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01447">http://arxiv.org/abs/2307.01447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zizhuo Li, Jiayi Ma</li>
<li>for: 提高计算机视觉中的特征匹配精度和效率，特别是在相机转换、基本矩阵估计和视觉地标 tasks 上。</li>
<li>methods: 我们提出了一种快速准确的 MaKeGNN 模型，它使用了稀疏注意力机制来缺省非重复的关键点，并通过匹配关键点来导引有意义的信息传递。</li>
<li>results: 我们在相机转换、基本矩阵估计和视觉地标 tasks 上达到了领先的性能，同时减少了计算和内存复杂度。<details>
<summary>Abstract</summary>
Accurately matching local features between a pair of images is a challenging computer vision task. Previous studies typically use attention based graph neural networks (GNNs) with fully-connected graphs over keypoints within/across images for visual and geometric information reasoning. However, in the context of feature matching, considerable keypoints are non-repeatable due to occlusion and failure of the detector, and thus irrelevant for message passing. The connectivity with non-repeatable keypoints not only introduces redundancy, resulting in limited efficiency, but also interferes with the representation aggregation process, leading to limited accuracy. Targeting towards high accuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN architecture which bypasses non-repeatable keypoints and leverages matchable ones to guide compact and meaningful message passing. More specifically, our Bilateral Context-Aware Sampling Module first dynamically samples two small sets of well-distributed keypoints with high matchability scores from the image pair. Then, our Matchable Keypoint-Assisted Context Aggregation Module regards sampled informative keypoints as message bottlenecks and thus constrains each keypoint only to retrieve favorable contextual information from intra- and inter- matchable keypoints, evading the interference of irrelevant and redundant connectivity with non-repeatable ones. Furthermore, considering the potential noise in initial keypoints and sampled matchable ones, the MKACA module adopts a matchability-guided attentional aggregation operation for purer data-dependent context propagation. By these means, we achieve the state-of-the-art performance on relative camera estimation, fundamental matrix estimation, and visual localization, while significantly reducing computational and memory complexity compared to typical attentional GNNs.
</details>
<details>
<summary>摘要</summary>
通过快速匹配本地特征点，计算机视觉任务中的一个挑战是准确地匹配两个图像中的特征点。先前的研究通常使用注意力基于图像内部/ между图像的全连接图 neural networks (GNNs) 进行视觉和几何信息的推理。然而，在特征匹配任务中，许多特征点是不可重复的，这些特征点由遮挡和检测器失败所致，因此对信息传递无关。与非重复的特征点连接不仅引入纠续，导致效率有限，而且干扰归纳表达过程，从而限制准确性。为了实现高精度和效率，我们提出了MaKeGNN，一种稀缺注意力基于GNN架构。MaKeGNN通过快速匹配特征点来导引有效的信息传递。更加详细地说，我们的双边上下文感知抽取模块首先在图像对中动态选择了一小集well-distributed的高匹配分数的特征点。然后，我们的匹配点协助上下文聚合模块将这些有用的特征点作为信道瓶颈，限制每个特征点只能从匹配的内部和外部匹配点中 retrieve 有利的上下文信息，避免与非匹配的非重复连接干扰信息传递。此外，为了避免初始特征点和抽取的匹配点中的噪音，MKACA模块采用了匹配性指导的注意力聚合操作，以保证数据依赖关系的纯净传递。通过这些手段，我们实现了相对摄像头估算、基本矩阵估算和视觉地理位置估算的状态 искусственный情况，同时显著降低了通常的注意力GNNs的计算和内存复杂性。
</details></li>
</ul>
<hr>
<h2 id="Continual-Learning-in-Open-vocabulary-Classification-with-Complementary-Memory-Systems"><a href="#Continual-Learning-in-Open-vocabulary-Classification-with-Complementary-Memory-Systems" class="headerlink" title="Continual Learning in Open-vocabulary Classification with Complementary Memory Systems"></a>Continual Learning in Open-vocabulary Classification with Complementary Memory Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01430">http://arxiv.org/abs/2307.01430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Zhu, Weijie Lyu, Yao Xiao, Derek Hoiem</li>
<li>for: 这paper是为了解决开放词汇图像分类 tasks中的灵活 continual learning问题， Drawing inspiration from human cognition的 complementary learning systems.</li>
<li>methods: 我们提出了一种”tree probe”方法， Which enables fast learning from new examples with competitive accuracy to batch-trained linear models. We also propose a method to combine predictions from a CLIP zero-shot model and the exemplar-based model, using the zero-shot estimated probability that a sample’s class is within any of the exemplar classes.</li>
<li>results: Our proposed method achieves a good balance of learning speed, target task effectiveness, and zero-shot effectiveness in data incremental, class incremental, and task incremental settings, as well as ability to perform flexible inference on varying subsets of zero-shot and learned categories.<details>
<summary>Abstract</summary>
We introduce a method for flexible continual learning in open-vocabulary image classification, drawing inspiration from the complementary learning systems observed in human cognition. We propose a "tree probe" method, an adaption of lazy learning principles, which enables fast learning from new examples with competitive accuracy to batch-trained linear models. Further, we propose a method to combine predictions from a CLIP zero-shot model and the exemplar-based model, using the zero-shot estimated probability that a sample's class is within any of the exemplar classes. We test in data incremental, class incremental, and task incremental settings, as well as ability to perform flexible inference on varying subsets of zero-shot and learned categories. Our proposed method achieves a good balance of learning speed, target task effectiveness, and zero-shot effectiveness.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种可动的开放词汇图像分类方法， drawing inspiration from human cognition中的补偿学习系统。我们提出了“树触”方法，基于懒散学习原则，可以快速从新示例中学习，与批量训练的直线模型具有竞争性准确率。此外，我们提出了将CLIP零shot模型和 exemplar-based模型的预测结果组合使用，使用零shot估计概率是任何 exemplar类中的样本类别。我们在数据增量、类增量和任务增量设置下测试了我们的提议方法，以及在不同的零shot和学习类别上进行可变的测试。我们的提议方法实现了learner speed, target task effectiveness和零shot effectiveness的好 equilibrio。
</details></li>
</ul>
<hr>
<h2 id="DeepfakeBench-A-Comprehensive-Benchmark-of-Deepfake-Detection"><a href="#DeepfakeBench-A-Comprehensive-Benchmark-of-Deepfake-Detection" class="headerlink" title="DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection"></a>DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01426">http://arxiv.org/abs/2307.01426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sclbd/deepfakebench">https://github.com/sclbd/deepfakebench</a></li>
<li>paper_authors: Zhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, Baoyuan Wu</li>
<li>for: 这篇论文是为了提供一个标准化、一致的深伪检测 benchmark，以解决现有的检测模型之间的不公正比较和可能的误导结果。</li>
<li>methods: 这篇论文使用了一个统一的数据管理系统，以确保所有检测模型的输入数据都是一致的。此外，论文还提供了一个整合了现有方法的框架，以及一个标准化的评估度量和评估协议，以促进透明度和重复性。</li>
<li>results: 这篇论文提供了一个全面的检测模型评估，包括15种现有的检测方法、9个深伪数据集、多种深伪检测评估协议和分析工具，以及广泛的评估。此外，论文还提供了新的分析结果，包括不同的观点（例如，数据增强、后向）。<details>
<summary>Abstract</summary>
A critical yet frequently overlooked challenge in the field of deepfake detection is the lack of a standardized, unified, comprehensive benchmark. This issue leads to unfair performance comparisons and potentially misleading results. Specifically, there is a lack of uniformity in data processing pipelines, resulting in inconsistent data inputs for detection models. Additionally, there are noticeable differences in experimental settings, and evaluation strategies and metrics lack standardization. To fill this gap, we present the first comprehensive benchmark for deepfake detection, called DeepfakeBench, which offers three key contributions: 1) a unified data management system to ensure consistent input across all detectors, 2) an integrated framework for state-of-the-art methods implementation, and 3) standardized evaluation metrics and protocols to promote transparency and reproducibility. Featuring an extensible, modular-based codebase, DeepfakeBench contains 15 state-of-the-art detection methods, 9 deepfake datasets, a series of deepfake detection evaluation protocols and analysis tools, as well as comprehensive evaluations. Moreover, we provide new insights based on extensive analysis of these evaluations from various perspectives (e.g., data augmentations, backbones). We hope that our efforts could facilitate future research and foster innovation in this increasingly critical domain. All codes, evaluations, and analyses of our benchmark are publicly available at https://github.com/SCLBD/DeepfakeBench.
</details>
<details>
<summary>摘要</summary>
具有批评性和 часто被忽视的挑战在深度假造检测领域是缺乏标准化、一致化、完整的标准准测。这个问题导致了不公平的性能比较和可能出现误导性的结果。具体来说，存在数据处理管道的不一致性，导致检测模型的输入数据不一致。此外，实验设置存在显著差异，评价策略和标准化也缺乏。为了填补这一漏洞，我们提出了首个深度假造检测的完整准测，即深度假造准测（DeepfakeBench），它的三个重要贡献如下：1. 一个统一的数据管理系统，确保所有检测器的输入数据具有一致性。2. 一个集成的方法实现框架，包括当前领域的状态艺术方法。3. 标准化的评价指标和协议，以促进透明度和可重现性。我们的准测拥有可扩展、模块化的代码基础，包括15种当前领域的检测方法，9个深度假造数据集，深度假造检测评价协议和分析工具，以及全面的评估。此外，我们还提供了来自多个角度的新的分析结论（例如数据增强、后向传播）。我们希望我们的努力可以促进未来的研究和促进深度假造检测领域的创新。我们的所有代码、评价、分析和结论都公开可用于https://github.com/SCLBD/DeepfakeBench。
</details></li>
</ul>
<hr>
<h2 id="Consistent-Multimodal-Generation-via-A-Unified-GAN-Framework"><a href="#Consistent-Multimodal-Generation-via-A-Unified-GAN-Framework" class="headerlink" title="Consistent Multimodal Generation via A Unified GAN Framework"></a>Consistent Multimodal Generation via A Unified GAN Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01425">http://arxiv.org/abs/2307.01425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Zhu, Yijun Li, Weijie Lyu, Krishna Kumar Singh, Zhixin Shu, Soeren Pirk, Derek Hoiem</li>
<li>for: 这篇论文的目的是如何通过单一的生成模型生成多模态图像输出，包括RGB、深度和表面法向量。</li>
<li>methods: 该论文基于StyleGAN3架构，使用共享背部和模态特定分支在图像生成网络的最后层。它还提出了每个模态的准确性评估器和跨模态一致性评估器。</li>
<li>results: 在斯坦福2D3D数据集上进行实验，论文实现了真实和一致的RGB、深度和表面法向量图像生成。它还提供了一种训练recipe，可以轻松地将预训练模型应用于新领域，只需几个对应的数据对。此外，论文还评估了使用生成的RGB和深度对进行深度估计器的训练或细化。<details>
<summary>Abstract</summary>
We investigate how to generate multimodal image outputs, such as RGB, depth, and surface normals, with a single generative model. The challenge is to produce outputs that are realistic, and also consistent with each other. Our solution builds on the StyleGAN3 architecture, with a shared backbone and modality-specific branches in the last layers of the synthesis network, and we propose per-modality fidelity discriminators and a cross-modality consistency discriminator. In experiments on the Stanford2D3D dataset, we demonstrate realistic and consistent generation of RGB, depth, and normal images. We also show a training recipe to easily extend our pretrained model on a new domain, even with a few pairwise data. We further evaluate the use of synthetically generated RGB and depth pairs for training or fine-tuning depth estimators. Code will be available at https://github.com/jessemelpolio/MultimodalGAN.
</details>
<details>
<summary>摘要</summary>
我们研究如何通过单一的生成模型生成多modal的图像输出，如RGB、深度和表面法向量。挑战在于生成的输出应该是真实的，同时也需要在不同modal之间具有一致性。我们基于StyleGAN3架构，在生成网络的最后层添加共享后置和modal特定的分支，并提出每个modal的准确性评价器以及交叉modal的一致性评价器。在Stanford2D3D数据集上进行实验，我们实现了真实和一致的RGB、深度和normal图像生成。此外，我们还提供了一个训练recipe，可以轻松地在新领域上扩展我们预训练模型，只需要一些对应的对比数据。最后，我们还评估了使用生成的RGB和深度对来训练或练化深度估计器。代码将在GitHub上提供。
</details></li>
</ul>
<hr>
<h2 id="Direct-Superpoints-Matching-for-Fast-and-Robust-Point-Cloud-Registration"><a href="#Direct-Superpoints-Matching-for-Fast-and-Robust-Point-Cloud-Registration" class="headerlink" title="Direct Superpoints Matching for Fast and Robust Point Cloud Registration"></a>Direct Superpoints Matching for Fast and Robust Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01362">http://arxiv.org/abs/2307.01362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Gupta, Yiming Xie, Hanumant Singh, Huaizu Jiang</li>
<li>for: 本研究旨在提出一种简单 yet effective的方法，可以直接匹配 superpoints，以确定源点云和目标点云之间的固定变换。</li>
<li>methods: 本方法使用全局滑动层来直接匹配 superpoints，并使用这些匹配来确定点云之间的变换。</li>
<li>results: 与直接预测对应点的方法相比，我们的方法可以更准确地估计变换，并且不需要任何后处理修复。在 ModelNet 和 3DMatch 测试集上，我们的方法达到了当前最佳的结果。<details>
<summary>Abstract</summary>
Although deep neural networks endow the downsampled superpoints with discriminative feature representations, directly matching them is usually not used alone in state-of-the-art methods, mainly for two reasons. First, the correspondences are inevitably noisy, so RANSAC-like refinement is usually adopted. Such ad hoc postprocessing, however, is slow and not differentiable, which can not be jointly optimized with feature learning. Second, superpoints are sparse and thus more RANSAC iterations are needed. Existing approaches use the coarse-to-fine strategy to propagate the superpoints correspondences to the point level, which are not discriminative enough and further necessitates the postprocessing refinement. In this paper, we present a simple yet effective approach to extract correspondences by directly matching superpoints using a global softmax layer in an end-to-end manner, which are used to determine the rigid transformation between the source and target point cloud. Compared with methods that directly predict corresponding points, by leveraging the rich information from the superpoints matchings, we can obtain more accurate estimation of the transformation and effectively filter out outliers without any postprocessing refinement. As a result, our approach is not only fast, but also achieves state-of-the-art results on the challenging ModelNet and 3DMatch benchmarks. Our code and model weights will be publicly released.
</details>
<details>
<summary>摘要</summary>
尽管深度神经网络使得下采样后的超点具有特征表示能力，但直接匹配它们通常不是独立使用的方法，主要是因为两个原因。首先，匹配是不可避免的噪声，因此通常采用RANSAC-like的修正。这种随机处理不是梯度可导的，因此无法与特征学习一起共同优化。其次，超点是稀疏的，因此需要更多的RANSAC迭代。现有的方法使用粗细到细粒的策略来传播超点匹配到点级别，但这些匹配不够精细，需要进一步的后处理修正。在这篇论文中，我们提出了一种简单又有效的方法，通过直接匹配超点使用全局软max层，以END-TO-END的方式确定源点云和目标点云之间的固定变换。与直接预测对应点的方法相比，通过利用超点匹配中的丰富信息，我们可以更准确地估算变换，并有效地排除噪声，无需任何后处理修正。因此，我们的方法不仅快速，而且实现了对挑战性 ModelNet 和 3DMatch 测试集的state-of-the-art 结果。我们的代码和模型参数将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Patch-CNN-Training-data-efficient-deep-learning-for-high-fidelity-diffusion-tensor-estimation-from-minimal-diffusion-protocols"><a href="#Patch-CNN-Training-data-efficient-deep-learning-for-high-fidelity-diffusion-tensor-estimation-from-minimal-diffusion-protocols" class="headerlink" title="Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols"></a>Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01346">http://arxiv.org/abs/2307.01346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Goodwin-Allcock, Ting Gong, Robert Gray, Parashkev Nachev, Hui Zhang</li>
<li>for: 优化Diffusion Tensor Estimation（DT）从六个方向的扩散束图像（DWI）中获得更高精度的数据。</li>
<li>methods: 使用深度学习方法，主要是基于 convolutional neural networks（CNN），从只有六个方向的扩散束图像（DWI）中获得 diffusion tensor 参数。</li>
<li>results: 与传统模型适应和基于voxel-wise fully-connected neural networks（FCN）的方法相比，Patch-CNN 可以更好地优化 scalar dMRI 参数和纤维orientation estimation from six-direction DWIs，并生成更好的 tractogram。<details>
<summary>Abstract</summary>
We propose a new method, Patch-CNN, for diffusion tensor (DT) estimation from only six-direction diffusion weighted images (DWI). Deep learning-based methods have been recently proposed for dMRI parameter estimation, using either voxel-wise fully-connected neural networks (FCN) or image-wise convolutional neural networks (CNN). In the acute clinical context -- where pressure of time limits the number of imaged directions to a minimum -- existing approaches either require an infeasible number of training images volumes (image-wise CNNs), or do not estimate the fibre orientations (voxel-wise FCNs) required for tractogram estimation. To overcome these limitations, we propose Patch-CNN, a neural network with a minimal (non-voxel-wise) convolutional kernel (3$\times$3$\times$3). Compared with voxel-wise FCNs, this has the advantage of allowing the network to leverage local anatomical information. Compared with image-wise CNNs, the minimal kernel vastly reduces training data demand. Evaluated against both conventional model fitting and a voxel-wise FCN, Patch-CNN, trained with a single subject is shown to improve the estimation of both scalar dMRI parameters and fibre orientation from six-direction DWIs. The improved fibre orientation estimation is shown to produce improved tractogram.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新方法，named Patch-CNN，用于从只有六个方向的扩散束图像（DWI）中提取扩散tensor（DT）。在临床应用中，使用深度学习基于方法进行dMRI参数估计，可以使用 Either voxel-wise fully-connected neural networks (FCN) or image-wise convolutional neural networks (CNN)。然而，在临床上，由于时间压力，通常只能取得有限的图像方向，因此现有的方法可能需要很多的训练图像量（image-wise CNNs），或者不会估计束纹方向（voxel-wise FCNs）。为了解决这些限制，我们提出了Patch-CNN，一个具有最小（非voxel-wise） convolutional kernel（3x3x3）的神经网络。与voxel-wise FCNs比较，这有利于网络利用局部 анатомиче信息。与image-wise CNNs比较，最小kernel减少了训练数据的需求。我们对比 conventunal model fitting和voxel-wise FCN进行评估，发现Patch-CNN，通过使用单个subject进行训练，可以提高从六个方向DWIs中的scalar dMRI参数和束纹方向的估计。这些改进的束纹方向估计，可以生成改进的 tractogram。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Monocular-Full-body-Capture-in-World-Space-via-Sequential-Proxy-to-Motion-Learning"><a href="#Real-time-Monocular-Full-body-Capture-in-World-Space-via-Sequential-Proxy-to-Motion-Learning" class="headerlink" title="Real-time Monocular Full-body Capture in World Space via Sequential Proxy-to-Motion Learning"></a>Real-time Monocular Full-body Capture in World Space via Sequential Proxy-to-Motion Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01200">http://arxiv.org/abs/2307.01200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Zhang, Hongwen Zhang, Liangxiao Hu, Hongwei Yi, Shengping Zhang, Yebin Liu</li>
<li>for: 这个论文的目的是提出一种基于学习的单视动作捕捉系统，可以在世界空间中实时捕捉全身动作，同时保持准确性。</li>
<li>methods: 该论文使用了一种顺序的代理人-到-动作学习方案，并使用了一个代理数据集，包括2D骨架序列和3D旋转动作在世界空间中。这些代理数据允许我们建立一个基于学习的网络，并在全身动作上提供准确的超级视图指导。此外，我们还在网络中共享了身体手部上下文信息，以便更好地恢复腕姿。</li>
<li>results: 根据该论文的结果，我们实现了世界空间中的实时单视全身动作捕捉系统，并且能够保持准确性和物理可能性。此外，我们还提供了更多的视频结果，可以在我们项目页面上找到：<a target="_blank" rel="noopener" href="https://liuyebin.com/proxycap%E3%80%82">https://liuyebin.com/proxycap。</a><details>
<summary>Abstract</summary>
Learning-based approaches to monocular motion capture have recently shown promising results by learning to regress in a data-driven manner. However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space. In this work, we contribute a sequential proxy-to-motion learning scheme together with a proxy dataset of 2D skeleton sequences and 3D rotational motions in world space. Such proxy data enables us to build a learning-based network with accurate full-body supervision while also mitigating the generalization issues. For more accurate and physically plausible predictions, a contact-aware neural motion descent module is proposed in our network so that it can be aware of foot-ground contact and motion misalignment with the proxy observations. Additionally, we share the body-hand context information in our network for more compatible wrist poses recovery with the full-body model. With the proposed learning-based solution, we demonstrate the first real-time monocular full-body capture system with plausible foot-ground contact in world space. More video results can be found at our project page: https://liuyebin.com/proxycap.
</details>
<details>
<summary>摘要</summary>
现代学习方法在单视动作捕捉领域已经显示出了扎实的成果，通过数据驱动的方式来进行回归。然而，由于数据收集和网络设计的问题，现有的解决方案在实时全身捕捉中具有很大的挑战。在这项工作中，我们提出了一种顺序Proxy-to-动作学习方案，并使用了2Dskeleton序列和3D旋转动作的世界空间数据集作为代理数据。这些代理数据允许我们建立一个基于学习的网络，并在全身监督下进行准确的回归。为了提高预测的准确性和物理可能性，我们还提出了一种具有联系感的神经动作下降模块，该模块能够考虑到脚地接触和动作不一致的代理观察。此外，我们在网络中分享了身体手势信息，以便更好地恢复与全身模型兼容的手势pose。通过我们的学习型解决方案，我们实现了世界空间中实时单视全身捕捉系统，并且首次实现了准确的脚地接触。更多视频结果可以在我们项目页面上找到：https://liuyebin.com/proxycap。
</details></li>
</ul>
<hr>
<h2 id="Segment-Anything-Meets-Point-Tracking"><a href="#Segment-Anything-Meets-Point-Tracking" class="headerlink" title="Segment Anything Meets Point Tracking"></a>Segment Anything Meets Point Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01197">http://arxiv.org/abs/2307.01197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/syscv/sam-pt">https://github.com/syscv/sam-pt</a></li>
<li>paper_authors: Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu</li>
<li>for: 这篇论文旨在扩展Segment Anything Model (SAM)的能力，以便在动态视频中跟踪和分割任何东西。</li>
<li>methods: 该方法利用了稳健和稀疏点选择和卷积技术来生成Mask，并且使用了点推送技术来跟踪目标对象。</li>
<li>results: 该方法在流行的视频对象分割评价标准DAVIS、YouTube-VOS和MOSE上表现出了强大的零shot性能。相比传统的对象中心的推送策略，我们使用点推送技术来利用地方结构信息，不受对象 semantics 的限制。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) has established itself as a powerful zero-shot image segmentation model, employing interactive prompts such as points to generate masks. This paper presents SAM-PT, a method extending SAM's capability to tracking and segmenting anything in dynamic videos. SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation, demonstrating that a SAM-based segmentation tracker can yield strong zero-shot performance across popular video object segmentation benchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditional object-centric mask propagation strategies, we uniquely use point propagation to exploit local structure information that is agnostic to object semantics. We highlight the merits of point-based tracking through direct evaluation on the zero-shot open-world Unidentified Video Objects (UVO) benchmark. To further enhance our approach, we utilize K-Medoids clustering for point initialization and track both positive and negative points to clearly distinguish the target object. We also employ multiple mask decoding passes for mask refinement and devise a point re-initialization strategy to improve tracking accuracy. Our code integrates different point trackers and video segmentation benchmarks and will be released at https://github.com/SysCV/sam-pt.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 已成为一种强大的零shot图像分割模型，使用交互提示如点来生成面积。本文介绍 SAM-PT，一种扩展 SAM 的能力，以跟踪和分割视频中的任何物体。SAM-PT 利用了Robust 和稀疏的点选择和宣传技术来生成面积，实际表明了基于 SAM 的分割跟踪器可以在流行的视频对象分割benchmark中具有强大的零shot性能，包括 DAVIS、YouTube-VOS 和 MOSE。与传统的对象中心的面积宣传策略不同，我们使用点宣传来利用本地结构信息，无论对象 semantics 无关。我们通过直接评估零shot开放世界 Unidentified Video Objects (UVO) benchmark来评估点跟踪的优势。为了进一步提高我们的方法，我们使用 K-Medoids 聚类算法来初始化点并跟踪正面和负面的点，以清晰地 distinguishes 目标对象。我们还使用多个面积解码通过来进行面积纠正，并设计了点重初始化策略以提高跟踪准确性。我们的代码将包括不同的点跟踪器和视频分割benchmark，将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Data-Memorization-in-3D-Latent-Diffusion-Models-for-Medical-Image-Synthesis"><a href="#Investigating-Data-Memorization-in-3D-Latent-Diffusion-Models-for-Medical-Image-Synthesis" class="headerlink" title="Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis"></a>Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01148">http://arxiv.org/abs/2307.01148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salman Ul Hassan Dar, Arman Ghanaat, Jannik Kahmann, Isabelle Ayx, Theano Papavassiliu, Stefan O. Schoenberg, Sandy Engelhardt</li>
<li>for: 用于生成真实的医疗数据，保护患者隐私。</li>
<li>methods: 使用生成模型，利用自我指导学习方法检测数据记忆能力。</li>
<li>results: 发现模型确实记忆训练数据，需要采取缓解措施。<details>
<summary>Abstract</summary>
Generative latent diffusion models have been established as state-of-the-art in data generation. One promising application is generation of realistic synthetic medical imaging data for open data sharing without compromising patient privacy. Despite the promise, the capacity of such models to memorize sensitive patient training data and synthesize samples showing high resemblance to training data samples is relatively unexplored. Here, we assess the memorization capacity of 3D latent diffusion models on photon-counting coronary computed tomography angiography and knee magnetic resonance imaging datasets. To detect potential memorization of training samples, we utilize self-supervised models based on contrastive learning. Our results suggest that such latent diffusion models indeed memorize training data, and there is a dire need for devising strategies to mitigate memorization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过实验，我们发现了生成潜在扩散模型在数据生成中的表现。这些模型在生成真实的医疗影像数据方面具有潜在的应用，例如公开分享医疗影像数据而不会侵犯病人隐私。然而，这些模型是否能够记忆敏感的训练数据仍然是一个未知之地。在这篇文章中，我们评估了3D潜在扩散模型在光子计数 computed tomography angiography和膝骨磁共振成像数据集上的记忆能力。我们使用自动化学习的对照学习方法来检测模型是否会记忆训练数据。我们的结果显示，这些潜在扩散模型确实会记忆训练数据，因此需要发展新的策略来缓和这种记忆。
</details></li>
</ul>
<hr>
<h2 id="AVSegFormer-Audio-Visual-Segmentation-with-Transformer"><a href="#AVSegFormer-Audio-Visual-Segmentation-with-Transformer" class="headerlink" title="AVSegFormer: Audio-Visual Segmentation with Transformer"></a>AVSegFormer: Audio-Visual Segmentation with Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01146">http://arxiv.org/abs/2307.01146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vvvb-github/avsegformer">https://github.com/vvvb-github/avsegformer</a></li>
<li>paper_authors: Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, Tong Lu</li>
<li>for: 该研究旨在提出一种新的听视多模态（AVS）任务，即将视频中的声音对象定位和分割。</li>
<li>methods: 该研究提出了一种基于 transformer 架构的新方法，称为 AVSegFormer，该方法利用听 queries 和可学习 queries 在 transformer 解码器中，使网络能够选择性地注意到有兴趣的视觉特征。</li>
<li>results: extensive experiments 表明，AVSegFormer 在 AVS  benchmark 上达到了状态 искусственный智能的最佳 результаTS，并且可以在不同的视频和声音背景下表现出色。I hope that helps!<details>
<summary>Abstract</summary>
The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate that AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is available at https://github.com/vvvb-github/AVSegFormer.
</details>
<details>
<summary>摘要</summary>
通过音频和视觉的组合，多模态社区已经是长期的研究主题。最近，一个新的音频视频分割（AVS）任务被提出，旨在在给定的视频中找到并分割声音的对象。这个任务需要音频驱动像素级场景理解，对于多模态社区来说，它具有 significante挑战。在这篇论文中，我们提出了AVSegFormer，一种新的AVS任务框架，利用转换器架构。我们在转换器解码器中引入了音频问题和学习问题，使网络可以选择性地听取 interess visual特征。此外，我们还提出了一个音频视频混合器，可以动态调整视觉特征，增强相关的空间通道，并降低无关的空间通道。此外，我们还设计了一个中间mask损失，以增强解码器的监督，让网络生成更加准确的中间预测。我们的实验结果表明，AVSegFormer可以在AVS标准 benchmark上达到状态 искусственный智能的Result。代码可以在https://github.com/vvvb-github/AVSegFormer中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/04/cs.CV_2023_07_04/" data-id="clpahu71x00fo3h8829irhwc2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/cs.AI_2023_07_04/" class="article-date">
  <time datetime="2023-07-04T12:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/cs.AI_2023_07_04/">cs.AI - 2023-07-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Inner-Sentiments-of-a-Thought"><a href="#The-Inner-Sentiments-of-a-Thought" class="headerlink" title="The Inner Sentiments of a Thought"></a>The Inner Sentiments of a Thought</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01784">http://arxiv.org/abs/2307.01784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Gagne, Peter Dayan</li>
<li>for: 这 paper 探讨了Transformer-based large-scale language models (LLMs) 能够生成高度现实的文本，以及这些模型能够表达和直接或间接表达各种情感和颜色的能力。</li>
<li>methods: 作者使用了LLMs的隐藏表示来训练分布预测器，用于预测句子的最终情感分布的多个量。</li>
<li>results: 作者发现了这些分布预测器具有良好的准确性和均匀性，并且可以用于分析句子的情感 trait。例如，用于分析句子的情感 trait，例如用于分析句子的情感 trait，例如“but” conjunction 可以对话趋于极端情感的变化。此外，作者还使用了这些分布预测器来生成具有特定情感 trait 的句子。<details>
<summary>Abstract</summary>
Transformer-based large-scale language models (LLMs) are able to generate highly realistic text. They are duly able to express, and at least implicitly represent, a wide range of sentiments and color, from the obvious, such as valence and arousal to the subtle, such as determination and admiration. We provide a first exploration of these representations and how they can be used for understanding the inner sentimental workings of single sentences. We train predictors of the quantiles of the distributions of final sentiments of sentences from the hidden representations of an LLM applied to prefixes of increasing lengths. After showing that predictors of distributions of valence, determination, admiration, anxiety and annoyance are well calibrated, we provide examples of using these predictors for analyzing sentences, illustrating, for instance, how even ordinary conjunctions (e.g., "but") can dramatically alter the emotional trajectory of an utterance. We then show how to exploit the distributional predictions to generate sentences with sentiments in the tails of distributions. We discuss the implications of our results for the inner workings of thoughts, for instance for psychiatric dysfunction.
</details>
<details>
<summary>摘要</summary>
Transformer-based大型语言模型（LLMs）能够生成高度真实的文本。它们能够表达，并至少隐含表达，从明显的投情和兴奋到 SUBTLE的决心和赞誉。我们提供了首次探索这些表示方式，并如何用它们来理解单句的内心情感运作。我们使用LLM应用到预FIX的长度 prefixes 中的隐藏表示进行训练，然后预测句子的final sentiment distribution 的quantiles。我们显示了这些预测器具有良好的准确性，然后提供了使用这些预测器来分析句子的例子，例如，如何通过使用 "but" 来剧烈地改变一句话的情感轨迹。最后，我们显示了如何利用分布预测器来生成具有不同情感的句子。我们讨论了我们的结果对内心工作的影响，例如心理障碍。
</details></li>
</ul>
<hr>
<h2 id="GHOST-A-Graph-Neural-Network-Accelerator-using-Silicon-Photonics"><a href="#GHOST-A-Graph-Neural-Network-Accelerator-using-Silicon-Photonics" class="headerlink" title="GHOST: A Graph Neural Network Accelerator using Silicon Photonics"></a>GHOST: A Graph Neural Network Accelerator using Silicon Photonics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01782">http://arxiv.org/abs/2307.01782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salma Afifi, Febin Sunny, Amin Shafiee, Mahdi Nikdast, Sudeep Pasricha</li>
<li>for: 这篇论文旨在提出一种基于光学频谱的干扰加速器，用于加速基于图structured数据的图神经网络（GNNs）模型的运行。</li>
<li>methods: 该论文使用了光学频谱技术，实现了图神经网络的三个主要阶段：邻居更新、Message Passing和采样。这些阶段都是在光学频谱中实现的，以提高加速器的效率和能效性。</li>
<li>results:  simulations 表明，GHOST 比 GPU、TPU、CPU 和多种现有的 GNN 硬件加速器具有至少 10.2 倍的吞吐量和 3.8 倍的能效率。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have emerged as a powerful approach for modelling and learning from graph-structured data. Multiple fields have since benefitted enormously from the capabilities of GNNs, such as recommendation systems, social network analysis, drug discovery, and robotics. However, accelerating and efficiently processing GNNs require a unique approach that goes beyond conventional artificial neural network accelerators, due to the substantial computational and memory requirements of GNNs. The slowdown of scaling in CMOS platforms also motivates a search for alternative implementation substrates. In this paper, we present GHOST, the first silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates the costs associated with both vertex-centric and edge-centric operations. It implements separately the three main stages involved in running GNNs in the optical domain, allowing it to be used for the inference of various widely used GNN models and architectures, such as graph convolution networks and graph attention networks. Our simulation studies indicate that GHOST exhibits at least 10.2x better throughput and 3.8x better energy efficiency when compared to GPU, TPU, CPU and multiple state-of-the-art GNN hardware accelerators.
</details>
<details>
<summary>摘要</summary>
GRAPH神经网络（GNNs）已经成为模elling和学习图structured数据的强大方法。多个领域受益于GNNs的能力，如推荐系统、社交网络分析、药物发现和机器人。但是，加速和有效地处理GNNs需要一种特殊的方法，这些方法超出了传统的人工神经网络加速器的能力，因为GNNs的计算和存储需求很大。CMOS平台的慢速化也驱动了寻找代替实现基台。在这篇论文中，我们提出了GHOST，首个基于光学频谱的GNN硬件加速器。GHOST efficiently减少了顶点中心和边中心操作的成本。它在光学频谱中实现了GNNs的三个主要阶段，allowing it to be used for the inference of various widely used GNN models and architectures, such as graph convolution networks and graph attention networks。我们的Simulation studies indicate that GHOST exhibits at least 10.2 times better throughput and 3.8 times better energy efficiency compared to GPU, TPU, CPU, and multiple state-of-the-art GNN hardware accelerators.
</details></li>
</ul>
<hr>
<h2 id="Physically-Realizable-Natural-Looking-Clothing-Textures-Evade-Person-Detectors-via-3D-Modeling"><a href="#Physically-Realizable-Natural-Looking-Clothing-Textures-Evade-Person-Detectors-via-3D-Modeling" class="headerlink" title="Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling"></a>Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01778">http://arxiv.org/abs/2307.01778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WhoTHU/Adversarial_camou">https://github.com/WhoTHU/Adversarial_camou</a></li>
<li>paper_authors: Zhanhao Hu, Wenda Chu, Xiaopei Zhu, Hui Zhang, Bo Zhang, Xiaolin Hu</li>
<li>for: 这 paper 的目的是为了逃脱人体检测器，而且它们可以在多个视角下工作。</li>
<li>methods: 这 paper 使用了3D 模型来制作隐蔽的 Texture，这种技术已经在隐蔽固定物体上得到成功。然而，人体和服装都是非固定的，因此实现这种技术在物理上是非常困难的。</li>
<li>results: 这 paper 的实验结果表明，使用 AdvCaT 技术可以在多个视角下逃脱多种人体检测器，并且可以在实际世界中应用。<details>
<summary>Abstract</summary>
Recent works have proposed to craft adversarial clothes for evading person detectors, while they are either only effective at limited viewing angles or very conspicuous to humans. We aim to craft adversarial texture for clothes based on 3D modeling, an idea that has been used to craft rigid adversarial objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes are non-rigid, leading to difficulties in physical realization. In order to craft natural-looking adversarial clothes that can evade person detectors at multiple viewing angles, we propose adversarial camouflage textures (AdvCaT) that resemble one kind of the typical textures of daily clothes, camouflage textures. We leverage the Voronoi diagram and Gumbel-softmax trick to parameterize the camouflage textures and optimize the parameters via 3D modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes combining topologically plausible projection (TopoProj) and Thin Plate Spline (TPS) to narrow the gap between digital and real-world objects. We printed the developed 3D texture pieces on fabric materials and tailored them into T-shirts and trousers. Experiments show high attack success rates of these clothes against multiple detectors.
</details>
<details>
<summary>摘要</summary>
最近的研究提出了为逃脱人体检测器而制作险oso的衣服，但它们只有有限的视场效果或对人类非常明显。我们想制作基于3D模型的险oso texture для衣服，这是已经用于制作固定险oso对象，如3D打印的乌龟。与固定对象不同，人类和衣服是非固定的，这导致物理实现的困难。为了制作多视场可以逃脱人体检测器的自然looking险oso衣服，我们提出了险oso披落文（AdvCaT），它们类似于日常衣服的一种典型文化，披落文。我们利用 Voronoi 图和 Gumbel-softmax 技巧来参数化险oso披落文并优化参数。此外，我们提出了一种高效的3D矩阵增强管道，将数字和实际对象之间的差异缩小。我们打印了开发的3D texture Piece onto fabric材料，并将其制成T恤和裤子。实验表明这些衣服可以高效地逃脱多个检测器。
</details></li>
</ul>
<hr>
<h2 id="MOPO-LSI-A-User-Guide"><a href="#MOPO-LSI-A-User-Guide" class="headerlink" title="MOPO-LSI: A User Guide"></a>MOPO-LSI: A User Guide</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01719">http://arxiv.org/abs/2307.01719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Zheng, Kumar Neelotpal Shukla, Jasmine Xu, David, Wang, Michael O’Leary</li>
<li>for: 这份论文是为了介绍MOPO-LSI库的用户指南，包括问题设置、工作流程和配置参数。</li>
<li>methods: 该论文使用MOPO-LSI库来实现多目标投资策略的优化，包括问题设置、工作流程和配置参数。</li>
<li>results: 该论文提供了MOPO-LSI库的用户指南，包括问题设置、工作流程和配置参数，以帮助用户快速地采用该库进行多目标投资策略的优化。<details>
<summary>Abstract</summary>
MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
</details>
<details>
<summary>摘要</summary>
MOPO-LSI是一个开源的多目标投资组合优化库，用于可持续投资。本文件提供MOPO-LSI版本1.0的用户手册，包括问题设置、工作流程和配置参数。
</details></li>
</ul>
<hr>
<h2 id="On-the-Constrained-Time-Series-Generation-Problem"><a href="#On-the-Constrained-Time-Series-Generation-Problem" class="headerlink" title="On the Constrained Time-Series Generation Problem"></a>On the Constrained Time-Series Generation Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01717">http://arxiv.org/abs/2307.01717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Coletta, Sriram Gopalakrishan, Daniel Borrajo, Svitlana Vyetrenko</li>
<li>for: 这种论文是为了提供一种有效的时间序列生成方法，以满足实际应用中的需求，如增强机器学习算法的性能、增加罕见事件的发生率和创造更多的对称时间序列enario。</li>
<li>methods: 该论文提出了一种新的时间序列生成方法，基于受限制的优化框架，并使用一种名为“指导噪时间”的干扰模型来生成真实的时间序列。</li>
<li>results: 该论文的实验结果表明，相比于现有的方法，该方法能够更高效地生成受限制的时间序列，并且不需要重新训练，从而降低碳脚印。<details>
<summary>Abstract</summary>
Synthetic time series are often used in practical applications to augment the historical time series dataset for better performance of machine learning algorithms, amplify the occurrence of rare events, and also create counterfactual scenarios described by the time series. Distributional-similarity (which we refer to as realism) as well as the satisfaction of certain numerical constraints are common requirements in counterfactual time series scenario generation requests. For instance, the US Federal Reserve publishes synthetic market stress scenarios given by the constrained time series for financial institutions to assess their performance in hypothetical recessions. Existing approaches for generating constrained time series usually penalize training loss to enforce constraints, and reject non-conforming samples. However, these approaches would require re-training if we change constraints, and rejection sampling can be computationally expensive, or impractical for complex constraints. In this paper, we propose a novel set of methods to tackle the constrained time series generation problem and provide efficient sampling while ensuring the realism of generated time series. In particular, we frame the problem using a constrained optimization framework and then we propose a set of generative methods including ``GuidedDiffTime'', a guided diffusion model to generate realistic time series. Empirically, we evaluate our work on several datasets for financial and energy data, where incorporating constraints is critical. We show that our approaches outperform existing work both qualitatively and quantitatively. Most importantly, we show that our ``GuidedDiffTime'' model is the only solution where re-training is not necessary for new constraints, resulting in a significant carbon footprint reduction.
</details>
<details>
<summary>摘要</summary>
通常情况下，人工时间序列被用于实际应用中增强历史时间序列数据，增加罕见事件的发生，以及创建具有时间序列的对应的counterfactualenario。实际性（即 Distributional-similarity）以及满足某些数学条件是常见的需求。例如，美国联邦储金会发布基于受限时间序列的 Synthetic market stress scenarios，用于评估金融机构在假设的经济衰退中的性能。现有的时间序列生成方法通常是通过权重loss来强制满足约束，并拒绝不符合约束的样本。然而，这些方法会需要重新训练，如果变更约束，并且拒绝样本可能是计算昂贵的，或者对于复杂的约束来说是不实用。在这篇论文中，我们提出一种新的方法来解决受约束时间序列生成问题，并提供高效的采样，同时保证生成的时间序列具有实际性。具体来说，我们将问题带入受约束优化框架，然后我们提出了一些生成方法，包括一种名为“GuidedDiffTime”的导向扩散模型，用于生成实际时间序列。我们对几个金融和能源数据集进行了实验，并证明我们的方法在质量和量上都超过了现有的方法。最重要的是，我们的“GuidedDiffTime”模型不需要重新训练，因此可以避免重新训练所带来的碳脚印。
</details></li>
</ul>
<hr>
<h2 id="Distributional-Model-Equivalence-for-Risk-Sensitive-Reinforcement-Learning"><a href="#Distributional-Model-Equivalence-for-Risk-Sensitive-Reinforcement-Learning" class="headerlink" title="Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning"></a>Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01708">http://arxiv.org/abs/2307.01708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Kastner, Murat A. Erdogdu, Amir-massoud Farahmand</li>
<li>for: 学习风险敏感奖励学习模型</li>
<li>methods: 使用分布式奖励学习引入两种新的模型相等性定义，一种是通用的但是 Computationally intractable，另一种是实用的可以选择希望计划优化的风险度量。</li>
<li>results: 在Tabular和大规模实验中证明了这种框架可以增强任何模型自由风险敏感算法，并提供了多种实际应用场景。<details>
<summary>Abstract</summary>
We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.
</details>
<details>
<summary>摘要</summary>
我团队正在研究风险敏感的推荐学习问题。我们理论上显示，合适的值相等方法，可以在风险中性设置下进行优化规划，但是这些方法在风险敏感设置下不够。我们利用分布式推荐学习来引入两种新的模型相等性，一种是通用的，可以用来规划任何风险度量，但是它是不可能实现的；另一种是实用的，允许您选择想要优化的风险度量。我们 demonstarte了我们的框架可以用来增强任何模型自由风险敏感算法，并提供了表格和大规模实验来证明其能力。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-is-all-you-need-removing-the-auxiliary-data-assumption-for-membership-inference-attacks-against-synthetic-data"><a href="#Synthetic-is-all-you-need-removing-the-auxiliary-data-assumption-for-membership-inference-attacks-against-synthetic-data" class="headerlink" title="Synthetic is all you need: removing the auxiliary data assumption for membership inference attacks against synthetic data"></a>Synthetic is all you need: removing the auxiliary data assumption for membership inference attacks against synthetic data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01701">http://arxiv.org/abs/2307.01701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florent Guépin, Matthieu Meeus, Ana-Maria Cretu, Yves-Alexandre de Montjoye</li>
<li>for: 这个论文目的是评估人工数据的隐私。</li>
<li>methods: 这篇论文使用了阴影模型进行会员推断攻击，以评估人工数据的安全性。</li>
<li>results: 研究人员通过使用只有人工数据进行三种攻击场景，成功地实现了会员推断攻击，并在两个实际数据集和两个人工数据生成器上进行了测试。这些结果表明，当审核人工数据时，可以减轻对auxiliary dataset的假设，从而实现实际的攻击。<details>
<summary>Abstract</summary>
Synthetic data is emerging as the most promising solution to share individual-level data while safeguarding privacy. Membership inference attacks (MIAs), based on shadow modeling, have become the standard to evaluate the privacy of synthetic data. These attacks, however, currently assume the attacker to have access to an auxiliary dataset sampled from a similar distribution as the training dataset. This often is a very strong assumption that would make an attack unlikely to happen in practice. We here show how this assumption can be removed and how MIAs can be performed using only the synthetic data. More specifically, in three different attack scenarios using only synthetic data, our results demonstrate that MIAs are still successful, across two real-world datasets and two synthetic data generators. These results show how the strong hypothesis made when auditing synthetic data releases - access to an auxiliary dataset - can be relaxed to perform an actual attack.
</details>
<details>
<summary>摘要</summary>
现代数据是许多领域的解决方案，它可以保护个人隐私的同时，共享个人数据。模型阴影攻击（MIAs），基于阴影模型，已成为评估合成数据隐私的标准方法。然而，这些攻击假设攻击者有访问类似于训练数据的 auxiliary dataset 的权限，这是一个很强的假设，在实际情况中很难发生。我们在这里显示了如何除掉这个假设，并使用只有合成数据进行 MIAs。具体来说，我们在三种不同的攻击场景中，使用了两个真实数据集和两个合成数据生成器，结果表明，MIAs 仍然成功，不需要 auxiliary dataset。这些结果表明，在审核合成数据发布时，对于实际攻击而言，可以放宽这一假设。
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-and-Solving-Infinite-Games-with-an-ERM-Oracle"><a href="#Online-Learning-and-Solving-Infinite-Games-with-an-ERM-Oracle" class="headerlink" title="Online Learning and Solving Infinite Games with an ERM Oracle"></a>Online Learning and Solving Infinite Games with an ERM Oracle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01689">http://arxiv.org/abs/2307.01689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelos Assos, Idan Attias, Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson</li>
<li>for: 这篇论文旨在解决在线学习中的泛化误差问题，而现有的算法依赖于计算不fficient的oracle，如标准优化算法（SOA）。</li>
<li>methods: 该论文提出了基于ERM oracle的在线二分类Setting的算法，并证明其在可 realizable 设定下具有有限征逐和在agnostic 设定下具有线性增长的 regret。 regret 的 bound 基于下面两个维度：Littlestone 维度和阈值维度。</li>
<li>results: 该论文显示了在非参数化游戏中，ERM oracle可以被视为best response oracle，并提供了基于best response oracle的学习算法，可以在两个玩家zero-sum 游戏和多个玩家general-sum 游戏中达到approximate-minimax 均衡和approximate coarse correlated 均衡，只要游戏有 bounded fat-threshold 维度。<details>
<summary>Abstract</summary>
While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.   We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero-sum games and approximate coarse correlated equilibria in multi-player general-sum games, as long as the game has a bounded fat-threshold dimension. Our algorithms apply to both binary-valued and real-valued games and can be viewed as providing justification for the wide use of double oracle and multiple oracle algorithms in the practice of solving large games.
</details>
<details>
<summary>摘要</summary>
在搜索学习设定下，ERM 可以达到近似优化的泛化误差，但在在线学习设定下，算法们往往需要计算效率低的oracle，如标准优化算法（SOA）。在这种情况下，我们提出了一种凭据仅仅基于ERM oracle call的在线二分类设定算法，并证明其在 realizable 设定下有finite regret，在agnostic 设定下有sublinearly growing regret。我们将 regret 约束为Littlestone和阈值维度的下界。在非参数学习游戏中，ERM oracle可以被视为最优回应 oracle，找到对某个玩家的历史玩家的最优回应。在这个设定下，我们提供了只凭据最优回应 oracle 的学习算法，可以在二player零Sum游戏中 converge to approximate-minimax equilibria，在多player general-sum游戏中 converge to approximate coarse correlated equilibria，只要游戏有bounded fat-threshold dimension。我们的算法适用于 binary-valued 和 real-valued 游戏，可以被视为对double oracle和多个 oracle 算法在实践中的 justify。
</details></li>
</ul>
<hr>
<h2 id="Serving-Graph-Neural-Networks-With-Distributed-Fog-Servers-For-Smart-IoT-Services"><a href="#Serving-Graph-Neural-Networks-With-Distributed-Fog-Servers-For-Smart-IoT-Services" class="headerlink" title="Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services"></a>Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01684">http://arxiv.org/abs/2307.01684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liekang Zeng, Xu Chen, Peng Huang, Ke Luo, Xiaoxi Zhang, Zhi Zhou</li>
<li>for: 这个论文是为了提供一个分布式实时 Graph Neural Network (GNN) 推论框架，以便在 IoT 驱动的智能应用中提供 GNN 服务。</li>
<li>methods: 这个论文使用了轻量级的 fog computing 技术，并将 GNN 推论框架分布在多个 fog 节点上，以便更好地利用 IoT 资料来源附近的多元化和动态资源。</li>
<li>results: 这个论文的实验和案例研究显示，Fograph 可以与现有的云 computing 和 fog 部署相比，提供更高的执行速度和过程效率，最高可以达到 5.39 倍的执行速度提升和 6.84 倍的过程提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have gained growing interest in miscellaneous applications owing to their outstanding ability in extracting latent representation on graph structures. To render GNN-based service for IoT-driven smart applications, traditional model serving paradigms usually resort to the cloud by fully uploading geo-distributed input data to remote datacenters. However, our empirical measurements reveal the significant communication overhead of such cloud-based serving and highlight the profound potential in applying the emerging fog computing. To maximize the architectural benefits brought by fog computing, in this paper, we present Fograph, a novel distributed real-time GNN inference framework that leverages diverse and dynamic resources of multiple fog nodes in proximity to IoT data sources. By introducing heterogeneity-aware execution planning and GNN-specific compression techniques, Fograph tailors its design to well accommodate the unique characteristics of GNN serving in fog environments. Prototype-based evaluation and case study demonstrate that Fograph significantly outperforms the state-of-the-art cloud serving and fog deployment by up to 5.39x execution speedup and 6.84x throughput improvement.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经在不同的应用中引起了广泛的关注，因为它们在图结构上能够提取潜在的表示。为了在智能应用中提供 GNN 基于服务，传统的模型服务 paradigm 通常会将全部的地理分布的输入数据上传到远程数据中心。然而，我们的实验测量表明，这种云端服务中的通信开销很大，而fog计算可以带来极大的潜在优势。为了最大化fog计算的建筑减法，在这篇论文中，我们提出了一种名为 Fograph 的分布式实时 GNN 推理框架。通过对不同类型的 fog 节点进行hetereogeneity-aware执行规划和 GNN 特定压缩技术，Fograph 设计得特别适应 GNN 在fog环境中的服务。实验和案例研究表明，Fograph 可以与状态艺术云服务和 fog 部署相比，提高执行速度和通过put Throughput by up to 5.39 倍和 6.84 倍。
</details></li>
</ul>
<hr>
<h2 id="Learning-Discrete-Weights-and-Activations-Using-the-Local-Reparameterization-Trick"><a href="#Learning-Discrete-Weights-and-Activations-Using-the-Local-Reparameterization-Trick" class="headerlink" title="Learning Discrete Weights and Activations Using the Local Reparameterization Trick"></a>Learning Discrete Weights and Activations Using the Local Reparameterization Trick</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01683">http://arxiv.org/abs/2307.01683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Berger, Aviv Navon, Ethan Fetaya</li>
<li>for: 降低计算机视ION和机器学习中的 neural network inference 计算复杂性和内存需求</li>
<li>methods: 使用 binarization 方法，将 neural network  weights 和活动化函数 binarized，以实现更高效的计算</li>
<li>results: 实现了针对 binary activations 的网络训练，并且可以在具有低资源的设备上进行高效的计算，并且可以实现 state-of-the-art 的Results<details>
<summary>Abstract</summary>
In computer vision and machine learning, a crucial challenge is to lower the computation and memory demands for neural network inference. A commonplace solution to address this challenge is through the use of binarization. By binarizing the network weights and activations, one can significantly reduce computational complexity by substituting the computationally expensive floating operations with faster bitwise operations. This leads to a more efficient neural network inference that can be deployed on low-resource devices. In this work, we extend previous approaches that trained networks with discrete weights using the local reparameterization trick to also allow for discrete activations. The original approach optimized a distribution over the discrete weights and uses the central limit theorem to approximate the pre-activation with a continuous Gaussian distribution. Here we show that the probabilistic modeling can also allow effective training of networks with discrete activation as well. This further reduces runtime and memory footprint at inference time with state-of-the-art results for networks with binary activations.
</details>
<details>
<summary>摘要</summary>
在计算机视觉和机器学习领域，一个重要的挑战是降低神经网络推理的计算和内存占用。一种常见的解决方案是通过binarization来实现。通过将神经网络权重和活动化值binarized，可以很大减少计算复杂性，将计算昂贵的浮点运算替换为更快的位运算。这导致一个更高效的神经网络推理，可以在低资源设备上部署。在这项工作中，我们将之前的方法扩展，使得神经网络可以使用离散权重和离散活动化值进行训练。原来的方法使用了local reparameterization trick来优化分布式权重的学习，并使用中心假设定理来近似预活化的 kontinuous Gaussian Distribution。在这里，我们表明了概率模型也可以有效地训练离散活动化的神经网络。这进一步减少了执行时间和内存占用，并达到了当前最佳的结果，在神经网络中使用二进制活动化。
</details></li>
</ul>
<hr>
<h2 id="RaidEnv-Exploring-New-Challenges-in-Automated-Content-Balancing-for-Boss-Raid-Games"><a href="#RaidEnv-Exploring-New-Challenges-in-Automated-Content-Balancing-for-Boss-Raid-Games" class="headerlink" title="RaidEnv: Exploring New Challenges in Automated Content Balancing for Boss Raid Games"></a>RaidEnv: Exploring New Challenges in Automated Content Balancing for Boss Raid Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01676">http://arxiv.org/abs/2307.01676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeon-Chang Jeon, In-Chang Baek, Cheong-mok Bae, Taehwa Park, Wonsang You, Taegwan Ha, Hoyun Jung, Jinha Noh, Seungwon Oh, Kyung-Joong Kim</li>
<li>for: 这研究旨在提供一个新的游戏模拟器和两个评价指标，用于自动游戏内容均衡。</li>
<li>methods: 这研究使用人工智能技术自动调整游戏内容，并在MMORPG游戏中采用多样化和可定制的内容来测试其效果。</li>
<li>results: 这研究提出了一个新的游戏研究平台，可以扩大自动游戏均衡问题的研究范围，并提供一个真实的游戏生产管道中的框架。<details>
<summary>Abstract</summary>
The balance of game content significantly impacts the gaming experience. Unbalanced game content diminishes engagement or increases frustration because of repetitive failure. Although game designers intend to adjust the difficulty of game content, this is a repetitive, labor-intensive, and challenging process, especially for commercial-level games with extensive content. To address this issue, the game research community has explored automated game balancing using artificial intelligence (AI) techniques. However, previous studies have focused on limited game content and did not consider the importance of the generalization ability of playtesting agents when encountering content changes. In this study, we propose RaidEnv, a new game simulator that includes diverse and customizable content for the boss raid scenario in MMORPG games. Additionally, we design two benchmarks for the boss raid scenario that can aid in the practical application of game AI. These benchmarks address two open problems in automatic content balancing, and we introduce two evaluation metrics to provide guidance for AI in automatic content balancing. This novel game research platform expands the frontiers of automatic game balancing problems and offers a framework within a realistic game production pipeline.
</details>
<details>
<summary>摘要</summary>
游戏内容平衡对游戏体验产生很大影响。不均衡的游戏内容会导致玩家失望或厌烦，因为玩家需要重复失败。虽然游戏设计师希望通过调整游戏内容的Difficulty来解决这个问题，但这是一项重复、劳动 INTENSIVE 和挑战性较高的过程，特别是在商业级游戏中。为解决这个问题，游戏研究社区已经开始使用人工智能（AI）技术自动平衡游戏内容。然而，前一些研究都集中在有限的游戏内容上，并未考虑AI游戏测试者在内容变化时的总体化能力的重要性。在这项研究中，我们提出了RaidEnv，一个新的游戏模拟器，包括MMORPG游戏中的bossoid难度scenario中的多样化和可定制内容。此外，我们设计了两个备用测试基准，可以帮助在游戏AI中自动平衡内容的实践应用。这两个基准解决了游戏自动平衡问题中的两个开放问题，并我们引入了两个评价指标，为AI在自动平衡内容中提供指导。这种新的游戏研究平台扩展了自动游戏平衡问题的前iers，并提供了一个在真实游戏生产管道中可行的框架。
</details></li>
</ul>
<hr>
<h2 id="mPLUG-DocOwl-Modularized-Multimodal-Large-Language-Model-for-Document-Understanding"><a href="#mPLUG-DocOwl-Modularized-Multimodal-Large-Language-Model-for-Document-Understanding" class="headerlink" title="mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"></a>mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02499">http://arxiv.org/abs/2307.02499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/x-plug/mplug-docowl">https://github.com/x-plug/mplug-docowl</a></li>
<li>paper_authors: Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang</li>
<li>for: 这篇论文是关于免需OCR文档理解的研究，旨在提高现有多种模型的 document understanding 能力。</li>
<li>methods: 该论文使用了 mPLUG-Owl 模型，并通过自定义数据集和训练策略进行了强化和调整。</li>
<li>results: 实验结果表明，该模型在免需OCR文档理解任务上表现出色，并且在不具体 fine-tuning 的情况下也能够在多个下游任务上发挥良好的效果。<details>
<summary>Abstract</summary>
Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy. We also build an OCR-free document instruction understanding evaluation set LLMDoc to better compare models' capabilities on instruct compliance and document understanding. Experimental results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOwl generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at https://github.com/X-PLUG/mPLUG-DocOwl.
</details>
<details>
<summary>摘要</summary>
文档理解指的是自动提取、分析和理解各种数字文档中的信息，如网页。现有的多模型大型语言模型（MLLMs），包括mPLUG-Owl，在零批量情况下已经表现出了扑捉人的可能性，这表明它们可能为无需OCR的文档理解做出贡献。然而，无法在域内训练时，这些模型往往忽略细腻的OCR特征，如复杂的表格或大块文本，这些特征是无需OCR文档理解的关键。在这篇论文中，我们提出了基于mPLUG-Owl的mPLUG-DocOwl模型，用于无需OCR的文档理解。具体来说，我们首先构建了一个具有各种视觉语言理解任务的指导调教数据集。然后，我们通过将模型同时在语言只、通用视觉语言和文档指导调教数据集上进行联合训练，使模型具备更强的无需OCR文档理解能力。此外，我们还建立了一个无需OCR文档指导理解评估集LLMDoc，以更好地比较模型在指令遵从和文档理解方面的能力。实验结果表明，我们的模型在现有多modal模型中表现出色，并且无需特定的 fine-tuning，mPLUG-DocOwl在多种下游任务上具有良好的普适性。我们的代码、模型、训练数据和评估集可以在https://github.com/X-PLUG/mPLUG-DocOwl上获取。
</details></li>
</ul>
<hr>
<h2 id="SwinGNN-Rethinking-Permutation-Invariance-in-Diffusion-Models-for-Graph-Generation"><a href="#SwinGNN-Rethinking-Permutation-Invariance-in-Diffusion-Models-for-Graph-Generation" class="headerlink" title="SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation"></a>SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01646">http://arxiv.org/abs/2307.01646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiyan98/swingnn">https://github.com/qiyan98/swingnn</a></li>
<li>paper_authors: Qi Yan, Zhengyang Liang, Yang Song, Renjie Liao, Lele Wang</li>
<li>For: 本文研究了基于Permutation-equivariant networks的扩散模型，可以学习 permutation-invariant的分布。但是，与非恒等模型相比，这些 invariable 模型在学习中遇到更大的挑战，其有效的目标分布具有更多的模式，且最佳一步噪声分布是 Gaussian mixture 的分布。* Methods: 本文提出了一种非恒等扩散模型，即 $\textit{SwinGNN}$，它使用高效的 edge-to-edge 2-WL 消息传递网络，并使用 shifted window 基于 SwinTransformers 的自注意力。此外，通过系统的ablations，我们确定了一些关键的训练和采样技术，可以大幅提高生成的样本质量。* Results: 我们的 $\textit{SwinGNN}$ 在 synthetic 和实际的蛋白质和分子数据集上达到了领先的性能。我们的代码在 <a target="_blank" rel="noopener" href="https://github.com/qiyan98/SwinGNN">https://github.com/qiyan98/SwinGNN</a> 上发布。<details>
<summary>Abstract</summary>
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph generative model to a permutation-invariant one. Extensive experiments on synthetic and real-world protein and molecule datasets show that our SwinGNN achieves state-of-the-art performances. Our code is released at https://github.com/qiyan98/SwinGNN.
</details>
<details>
<summary>摘要</summary>
“基于 permutation-equivariant 网络的扩散模型可以学习 permutation-invariant 分布 для图数据。然而，相比其非 invariat 对手，我们发现这些 invariat 模型在学习中遇到更大的挑战，主要表现在以下两点：1) 其有效目标分布具有更多的模式; 2) 其最佳一步干扰得分函数是 Gaussian 混合函数的更多组件。这些分析结果为我们提供了灵感，我们提议一种非 invariat 扩散模型，即 $\textit{SwinGNN}$，该模型使用高效的 edge-to-edge 2-WL 消息传递网络，并使用 shifted window 基于 SwinTransformers 的自注意力。此外，通过系统性的ablation 研究，我们确定了一些关键的训练和采样技术，可以大幅提高生成的样本质量。最后，我们提出了一个简单的后处理技术，即随机排序生成的图，这可以证明任何图生成模型都可以变换为 permutation-invariant 模型。我们在 synthetic 和实际世界的蛋白质和分子数据上进行了广泛的实验，并证明了我们的 SwinGNN 达到了状态的最佳性能。我们的代码可以在 https://github.com/qiyan98/SwinGNN 上下载。”
</details></li>
</ul>
<hr>
<h2 id="Insert-expansions-for-Tool-enabled-Conversational-Agents"><a href="#Insert-expansions-for-Tool-enabled-Conversational-Agents" class="headerlink" title="Insert-expansions for Tool-enabled Conversational Agents"></a>Insert-expansions for Tool-enabled Conversational Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01644">http://arxiv.org/abs/2307.01644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Göldi, Roman Rietsche</li>
<li>for: 这篇论文关注大语言模型中的链条思维提示实现方法，特别是在用工具（或“插件”）在明确的思维路径中生成的 conversational agents 中使用工具。</li>
<li>methods: 我们使用 conversation analysis 来研究用户如何在 conversational agents 中提供必要的细节和纠正请求，以便实现更好的回答。</li>
<li>results: 我们通过两个实验直接比较，发现在推荐领域使用“用户为工具”方法可以获得利益。<details>
<summary>Abstract</summary>
This paper delves into an advanced implementation of Chain-of-Thought-Prompting in Large Language Models, focusing on the use of tools (or "plug-ins") within the explicit reasoning paths generated by this prompting method. We find that tool-enabled conversational agents often become sidetracked, as additional context from tools like search engines or calculators diverts from original user intents. To address this, we explore a concept wherein the user becomes the tool, providing necessary details and refining their requests. Through Conversation Analysis, we characterize this interaction as insert-expansion - an intermediary conversation designed to facilitate the preferred response. We explore possibilities arising from this 'user-as-a-tool' approach in two empirical studies using direct comparison, and find benefits in the recommendation domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Heuristic-Algorithms-for-the-Approximation-of-Mutual-Coherence"><a href="#Heuristic-Algorithms-for-the-Approximation-of-Mutual-Coherence" class="headerlink" title="Heuristic Algorithms for the Approximation of Mutual Coherence"></a>Heuristic Algorithms for the Approximation of Mutual Coherence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01639">http://arxiv.org/abs/2307.01639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregor Betz, Vera Chekan, Tamara Mchedlidze</li>
<li>for: This paper aims to accelerate the computation of mutual coherence, which is a measure of similarity between two opinions, in the context of the Wahl-O-Mat system used in Germany to help voters find candidates that align with their political preferences.</li>
<li>methods: The authors model the distribution of confirmation values as a mixture of three Gaussians and present efficient heuristics to estimate the model parameters. They also use the expected value of the distribution to approximate the mutual coherence. Some of the presented algorithms are fully polynomial-time, while others only require solving a small number of instances of the SAT model counting problem.</li>
<li>results: The authors’ best algorithm achieves an average squared error of less than 0.0035, which is considered insignificant given the efficiency of the algorithm. The accuracy is precise enough to be used in Wahl-O-Mat-like systems.<details>
<summary>Abstract</summary>
Mutual coherence is a measure of similarity between two opinions. Although the notion comes from philosophy, it is essential for a wide range of technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters to find candidates that are the closest to their political preferences. The exact computation of mutual coherence is highly time-consuming due to the iteration over all subsets of an opinion. Moreover, for every subset, an instance of the SAT model counting problem has to be solved which is known to be a hard problem in computer science. This work is the first study to accelerate this computation. We model the distribution of the so-called confirmation values as a mixture of three Gaussians and present efficient heuristics to estimate its model parameters. The mutual coherence is then approximated with the expected value of the distribution. Some of the presented algorithms are fully polynomial-time, others only require solving a small number of instances of the SAT model counting problem. The average squared error of our best algorithm lies below 0.0035 which is insignificant if the efficiency is taken into account. Furthermore, the accuracy is precise enough to be used in Wahl-O-Mat-like systems.
</details>
<details>
<summary>摘要</summary>
共同 coherence 是两个意见之间的相似度量量。这个概念来自哲学，但它对各种技术领域都是关键的，例如德国的 Wahl-O-Mat 系统。这个系统帮助选民找到最接近其政治偏好的候选人。然而，正确计算共同 coherence 是非常时间consuming，因为需要遍历所有意见集合中的所有子集，并对每个子集解决一个 SAT 模型计数问题。这个问题在计算机科学中是一个知名的困难问题。这个研究是第一个加速这种计算的研究。我们模型了confirmation value 的分布为三个 Gaussian 的混合，并提供了高效的启发法来估算其模型参数。然后，我们使用这些参数来近似共同 coherence。我们的算法中有一些是完全 polynomial-time，另一些只需解决一小数量的 SAT 模型计数问题。我们的最佳算法的平均平方误差低于 0.0035，这是可以忽略的。此外，我们的精度够高，可以用于 Wahl-O-Mat 类系统。
</details></li>
</ul>
<hr>
<h2 id="Random-Walk-on-Multiple-Networks"><a href="#Random-Walk-on-Multiple-Networks" class="headerlink" title="Random Walk on Multiple Networks"></a>Random Walk on Multiple Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01637">http://arxiv.org/abs/2307.01637</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flyingdoog/rwm">https://github.com/flyingdoog/rwm</a></li>
<li>paper_authors: Dongsheng Luo, Yuchen Bian, Yaowei Yan, Xiong Yu, Jun Huan, Xiao Liu, Xiang Zhang<br>for: 本研究旨在利用多个网络来提高实体之间的归一化和网络推荐等任务中的做出更好的推断。methods: 本研究提出了随机游走在多个网络上（RWM），可以处理多种多网络和多类实体的数据。RWM使用随机游走者在每个网络上访问节点，并计算每个节点的本地邻居关系（i.e., 节点访问概率）。在发现类似访问概率的节点时，游走者们强制合作。results: 研究人员通过分析RWM的整合性和可靠性，并提出了两种可靠性保证的优化方法。在链接预测、网络嵌入和本地社区检测等任务中，RWM表现出色，并且在实际数据上进行了广泛的 экспериментирования。<details>
<summary>Abstract</summary>
Random Walk is a basic algorithm to explore the structure of networks, which can be used in many tasks, such as local community detection and network embedding. Existing random walk methods are based on single networks that contain limited information. In contrast, real data often contain entities with different types or/and from different sources, which are comprehensive and can be better modeled by multiple networks. To take advantage of rich information in multiple networks and make better inferences on entities, in this study, we propose random walk on multiple networks, RWM. RWM is flexible and supports both multiplex networks and general multiple networks, which may form many-to-many node mappings between networks. RWM sends a random walker on each network to obtain the local proximity (i.e., node visiting probabilities) w.r.t. the starting nodes. Walkers with similar visiting probabilities reinforce each other. We theoretically analyze the convergence properties of RWM. Two approximation methods with theoretical performance guarantees are proposed for efficient computation. We apply RWM in link prediction, network embedding, and local community detection. Comprehensive experiments conducted on both synthetic and real-world datasets demonstrate the effectiveness and efficiency of RWM.
</details>
<details>
<summary>摘要</summary>
随机漫步是一种基本算法，用于探索网络结构，可以用于多种任务，如本地社区检测和网络嵌入。现有的随机漫步方法基于单个网络，它们只包含有限信息。然而，实际数据通常包含不同类型的实体或来自不同来源的实体，这些信息更加全面，可以更好地使用多个网络来模型。为了利用多个网络中的丰富信息，提高对实体的推断，我们提出了随机漫步多网络（RWM）。RWM是灵活的，支持多类多网络和通用多网络，它们可能形成多对多节点映射。RWM将在每个网络上Random walker，以获取起始节点的本地邻近性（即节点访问概率）。漫步者之间相似的访问概率强化对方。我们 theoretically 分析 RWM 的收敛性质。我们还提出了两种有理性 guarantees 的近似方法，用于有效地计算。我们在链接预测、网络嵌入和本地社区检测中应用 RWM。我们在 Synthetic 和实际数据集上进行了广泛的实验，并证明了 RWM 的效果和效率。
</details></li>
</ul>
<hr>
<h2 id="SageFormer-Series-Aware-Graph-Enhanced-Transformers-for-Multivariate-Time-Series-Forecasting"><a href="#SageFormer-Series-Aware-Graph-Enhanced-Transformers-for-Multivariate-Time-Series-Forecasting" class="headerlink" title="SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting"></a>SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01616">http://arxiv.org/abs/2307.01616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwei Zhang, Xin Wang, Yuantao Gu</li>
<li>for: 本文旨在提出一种能够有效地捕捉和模型系列之间的依赖关系的系列意识推断模型，以提高多ivariate时间序列预测的准确性。</li>
<li>methods: 本文提出了一种基于图结构的Series-aware Graph-enhanced Transformer模型，可以有效地表示多个时间序列的多样性模式，并避免系列之间的重复信息。</li>
<li>results: 经过对真实数据和 sintetic 数据的广泛实验，本文表明了SageFormer 模型在比较现有方法时的显著性能优势。<details>
<summary>Abstract</summary>
Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
多变量时间序列预测在多个领域发挥重要作用。Recent Advances in Deep Learning Methods, especially Transformers, have shown promise, but there is still a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.Here is the word-for-word translation of the text into Simplified Chinese:多变量时间序列预测在多个领域发挥重要作用。最近的深度学习方法，特别是转换器，已经展示了承诺，但还有一个差距在强调多个时间序列之间的相互关系。这篇论文介绍了SageFormer，一种基于图strucutres的Series-aware Graph-enhanced Transformer模型，旨在通过图结构来有效地捕捉和模型时间序列之间的相互关系。SageFormer解决了两个关键挑战：一是在多个时间序列之间有效地表示多样的时间模式，二是在多个时间序列之间减少重复的信息。重要的是，提议的系列意识框架可以轻松地与现有的转换器基本模型集成，从而增强其模型多个时间序列之间的相互关系。通过对真实世界和 sintetic数据集进行了广泛的实验，我们展示了SageFormer的性能超过了前一个状态的方法。
</details></li>
</ul>
<hr>
<h2 id="Overconfidence-is-a-Dangerous-Thing-Mitigating-Membership-Inference-Attacks-by-Enforcing-Less-Confident-Prediction"><a href="#Overconfidence-is-a-Dangerous-Thing-Mitigating-Membership-Inference-Attacks-by-Enforcing-Less-Confident-Prediction" class="headerlink" title="Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction"></a>Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01610">http://arxiv.org/abs/2307.01610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dependablesystemslab/mia_defense_hamp">https://github.com/dependablesystemslab/mia_defense_hamp</a></li>
<li>paper_authors: Zitao Chen, Karthik Pattabiraman</li>
<li>for: 防止机器学习模型被攻击，保护模型的训练数据隐私。</li>
<li>methods: 提出了一种防御技术，即高度积震训练框架和均衡抑制器，使模型在训练和测试样本上具有类似的预测结果，从而保护模型的隐私。</li>
<li>results: 对五个 benchmark 数据集进行了广泛的评估，并显示了HAMP 可以保持高度的准确率和强的会员隐私。与七种现有的防御技术进行比较，HAMP 在隐私利用与用途之间具有更好的质量比。<details>
<summary>Abstract</summary>
Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving high accuracy. To further reduce privacy risk, HAMP uniformly modifies all the prediction outputs to become low-confidence outputs while preserving the accuracy, which effectively obscures the differences between the prediction on members and non-members. We conduct extensive evaluation on five benchmark datasets, and show that HAMP provides consistently high accuracy and strong membership privacy. Our comparison with seven state-of-the-art defenses shows that HAMP achieves a superior privacy-utility trade off than those techniques.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）模型容易受到会员推测攻击（MIA），该攻击可以判断给定输入是否用于训练目标模型。虽然有许多防御技术，但它们通常受到有限的隐私保护、大幅下降精度和/或需要难以获得的额外数据。这个工作提出了一种防御技术，即HAMP，可以同时保障强大的会员隐私和高精度。为了 Mitigate MIAs 的不同形式，我们发现它们都利用 ML 模型对训练样本的过于自信。这种动机导致我们的设计强制模型在测试样本上预测更加不自信，从而使模型在训练和测试样本上行为相同。HAMP 包括一种新的训练框架，高 entropy 软标签和一种基于Entropy的正则化器，以限制模型的预测，并且仍然实现高精度。为了进一步减少隐私风险，HAMP 对所有预测输出进行同步修改，使其变成低自信输出，保持精度，同时减少了会员隐私风险。我们对五个 benchmark 数据集进行了广泛的评估，并表明HAMP 可以保持高精度和强大的会员隐私。与七种 state-of-the-art 防御技术进行比较，我们发现HAMP 在隐私利用和实用性之间取得了更好的平衡。
</details></li>
</ul>
<hr>
<h2 id="Bridge-the-Performance-Gap-in-Peak-hour-Series-Forecasting-The-Seq2Peak-Framework"><a href="#Bridge-the-Performance-Gap-in-Peak-hour-Series-Forecasting-The-Seq2Peak-Framework" class="headerlink" title="Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework"></a>Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01597">http://arxiv.org/abs/2307.01597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwei Zhang, Xin Wang, Jingyuan Xie, Heling Zhang, Yuantao Gu</li>
<li>For: 预测高峰时间序列 (PHSF) 是各个领域中的一项重要 yet 未经充分利用的任务。现有的深度学习模型在常规时间序列预测 (TSF) 中具有出色的表现，但在 PHSF 中却表现不佳。这可以归结于高峰时间序列的非站ARY性问题，导致直接预测更加困难于标准 TSF。* Methods: 该 paper 提出了一个名为 Seq2Peak 的新框架，用于解决 PHSF 任务。Seq2Peak 包括两个关键组成部分：一个名为 CyclicNorm 的管道，用于 Mitigate 非站ARY性问题，以及一个简单 yet effective 的可调 Parameters-free 峰值时间解码器，使用 hybrid 损失函数，将原始序列和峰值序列作为监督信号。* Results: 对于四个实际 dataset 进行了广泛的实验，证明了提posed 框架的有效性，得到了平均相对改进率为 37.7%，对于 transformer- 和非 transformer- 基于 TSF 模型。<details>
<summary>Abstract</summary>
Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in various domains. While state-of-the-art deep learning models excel in regular Time Series Forecasting (TSF), they struggle to achieve comparable results in PHSF. This can be attributed to the challenges posed by the high degree of non-stationarity in peak-hour series, which makes direct forecasting more difficult than standard TSF. Additionally, manually extracting the maximum value from regular forecasting results leads to suboptimal performance due to models minimizing the mean deficit. To address these issues, this paper presents Seq2Peak, a novel framework designed specifically for PHSF tasks, bridging the performance gap observed in TSF models. Seq2Peak offers two key components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid loss function that utilizes both the original series and peak-hour series as supervised signals. Extensive experimentation on publicly available time series datasets demonstrates the effectiveness of the proposed framework, yielding a remarkable average relative improvement of 37.7\% across four real-world datasets for both transformer- and non-transformer-based TSF models.
</details>
<details>
<summary>摘要</summary>
《峰值时间序列预测（PHSF）是许多领域中的一项重要 yet 未得到充分研究的任务。当前的深度学习模型在标准时间序列预测（TSF）中表现出色，但在 PHSF 中却表现不佳。这可以归结于峰值时间序列的非站点性问题，导致直接预测变得更加困难。此外，手动提取常规预测结果中的最大值会导致性能下降，因为模型会尝试最小化均方误差。为了解决这些问题，本文提出了 Seq2Peak 框架，这是一种特有的 PHSF 任务解决方案， bridging 标准 TSF 模型的性能差距。Seq2Peak 框架包括两个关键组件：CyclingNorm 管道来 mitigate 非站点性问题，以及一个简单 yet 高效的可学习参数无法 peak-hour 解码器，使用两种不同的超参数来学习峰值时间序列和常规时间序列。经过对公开的时间序列数据集进行了广泛的实验，显示了提议的框架的效果，其中平均相对提升率为 37.7%，对四个实际世界数据集中的两种 transformer 和非 transformer 基于 TSF 模型进行了评估。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Tuning-Pushes-Farther-Contrastive-Learning-Pulls-Closer-A-Two-Stage-Approach-to-Mitigate-Social-Biases"><a href="#Prompt-Tuning-Pushes-Farther-Contrastive-Learning-Pulls-Closer-A-Two-Stage-Approach-to-Mitigate-Social-Biases" class="headerlink" title="Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases"></a>Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01595">http://arxiv.org/abs/2307.01595</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyingji1996/CCPA">https://github.com/liyingji1996/CCPA</a></li>
<li>paper_authors: Yingji Li, Mengnan Du, Xin Wang, Ying Wang<br>for: 这个研究旨在减少预训练 corpora 中的社会偏见，并提高预训练 Language Model 的表现。methods: 本研究使用了两个阶段的方法：第一个阶段是基于 continuous prompt tuning 的数据增强方法，第二个阶段是使用对照学习抑制预训练 Model 的参数。results: 实验结果显示，CCPA 比基于 Counterfactual Data Augmentation 的方法有更好的减少社会偏见表现，并且保持了预训练 Language Model 的语言模型表现。<details>
<summary>Abstract</summary>
As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs' encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs' parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.
</details>
<details>
<summary>摘要</summary>
随着预训言语模型（PLMs）的表达能力提高，社会偏见的继承问题日益减少。大多数前一代减偏技术使用Counterfactual Data Augmentation（CDA）来填补训练集。然而，CDA只是略微修改原始集合，因此在不同的人口组中的表达距离仍然很窄。这会使减偏模型轻松地适应差异 между counterfactual pair，从而影响其减偏性能。在这篇论文中，我们提出一种基于对抗学习的两stage减偏模型，使用Continuous Prompt Augmentation（CPA）来减少PLMs的社会偏见。在第一个阶段，我们提出一种基于连续提示调整的数据增强方法，以增加不同人口组之间的表达距离。在第二个阶段，我们使用对抗学习来吸引增强后的样本对以更近的表达距离，然后细化PLMs的参数以获得减偏编码。我们的方法会导致模型增强减偏性能，通过增加训练过程的困难程度。广泛的实验表明，CCPA超过基准的减偏性能。同时，在GLUE benchmark上的实验结果表明，CCPA保留了PLMs的语言模型能力。
</details></li>
</ul>
<hr>
<h2 id="Cross-Element-Combinatorial-Selection-for-Multi-Element-Creative-in-Display-Advertising"><a href="#Cross-Element-Combinatorial-Selection-for-Multi-Element-Creative-in-Display-Advertising" class="headerlink" title="Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising"></a>Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01593">http://arxiv.org/abs/2307.01593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhang, Ping Zhang, Jian Dong, Yongkang Wang, Pengye Zhang, Bo Zhang, Xingxing Wang, Dong Wang</li>
<li>for: 提高电商广告创作效果</li>
<li>methods: 跨元素组合选择框架(CECS)</li>
<li>results: 实现了最高纪录的Offline指标分数，并且在实际业务中实现了6.02%的点击率和10.37%的营收增长<details>
<summary>Abstract</summary>
The effectiveness of ad creatives is greatly influenced by their visual appearance. Advertising platforms can generate ad creatives with different appearances by combining creative elements provided by advertisers. However, with the increasing number of ad creative elements, it becomes challenging to select a suitable combination from the countless possibilities. The industry's mainstream approach is to select individual creative elements independently, which often overlooks the importance of interaction between creative elements during the modeling process. In response, this paper proposes a Cross-Element Combinatorial Selection framework for multiple creative elements, termed CECS. In the encoder process, a cross-element interaction is adopted to dynamically adjust the expression of a single creative element based on the current candidate creatives. In the decoder process, the creative combination problem is transformed into a cascade selection problem of multiple creative elements. A pointer mechanism with a cascade design is used to model the associations among candidates. Comprehensive experiments on real-world datasets show that CECS achieved the SOTA score on offline metrics. Moreover, the CECS algorithm has been deployed in our industrial application, resulting in a significant 6.02% CTR and 10.37% GMV lift, which is beneficial to the business.
</details>
<details>
<summary>摘要</summary>
“广告创意的有效性受到它的见识性标志影响。广告平台可以通过不同的创意元素结合来生成不同的创意标志。然而，随着创意元素的数量增加，选择合适的结合成为愈来愈困难。industry的主流方法是选择个别创意元素独立，往往忽略了创意元素间的互动过程中的重要性。对此，本文提出了跨元素兼容选择框架，简称为CECS。在Encoder过程中，采用跨元素互动以静态地调整单一创意元素的表达，以满足目前候选的创意标志。在Decoder过程中，创意组合问题转化为多个创意元素之间的传递选择问题。使用一个链接机制，模型候选者之间的协调关系。实际测试结果显示，CECS取得了线上数据上的SOTA分数。此外，CECS算法已经在我们的商业应用中实现了6.02%的Click Through Rate（CTR）和10.37%的Gross Merchandise Value（GMV）提升，对业务有益。”
</details></li>
</ul>
<hr>
<h2 id="Transcribing-Educational-Videos-Using-Whisper-A-preliminary-study-on-using-AI-for-transcribing-educational-videos"><a href="#Transcribing-Educational-Videos-Using-Whisper-A-preliminary-study-on-using-AI-for-transcribing-educational-videos" class="headerlink" title="Transcribing Educational Videos Using Whisper: A preliminary study on using AI for transcribing educational videos"></a>Transcribing Educational Videos Using Whisper: A preliminary study on using AI for transcribing educational videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03200">http://arxiv.org/abs/2307.03200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashwin Rao</li>
<li>for: 这篇论文是为了探讨如何使用自动语音识别（ASR）系统来提高电子学习视频的掌握效果。</li>
<li>methods: 该论文使用了各种语音识别算法和技术来生成视频的字幕，并对25个教育视频进行了评估。</li>
<li>results: 研究发现，使用ASR系统可以减少对视频的杂音和干扰的影响，并提高视频的掌握效果。同时，还有一些开放的研究方向，如如何更好地识别教育视频中的语音、如何提高语音识别精度等。<details>
<summary>Abstract</summary>
Videos are increasingly being used for e-learning, and transcripts are vital to enhance the learning experience. The costs and delays of generating transcripts can be alleviated by automatic speech recognition (ASR) systems. In this article, we quantify the transcripts generated by whisper for 25 educational videos and identify some open avenues of research when leveraging ASR for transcribing educational videos.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="IAdet-Simplest-human-in-the-loop-object-detection"><a href="#IAdet-Simplest-human-in-the-loop-object-detection" class="headerlink" title="IAdet: Simplest human-in-the-loop object detection"></a>IAdet: Simplest human-in-the-loop object detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01582">http://arxiv.org/abs/2307.01582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/franchesoni/iadet">https://github.com/franchesoni/iadet</a></li>
<li>paper_authors: Franco Marchesoni-Acland, Gabriele Facciolo</li>
<li>for: 这个论文是为了提出一种人工智能注解策略，帮助在数据标注过程中训练模型。</li>
<li>methods: 这个策略包括三个模块：一、助け物标注；二、背景模型训练；三、活动选择下一个数据点。这个框架下开源了一个专门用于单类物体检测的工具——IAdet。</li>
<li>results: 对于PASCAL VOC数据集，IAdet工具可以将数据标注时间减少$25%$，并提供一个免费的训练模型。这些结果是基于一个故意简单的IAdet设计而得到的。因此，IAdet具有多种可以轻松改进的可能性，这为人工智能 loop对象检测系统开创了道路。<details>
<summary>Abstract</summary>
This work proposes a strategy for training models while annotating data named Intelligent Annotation (IA). IA involves three modules: (1) assisted data annotation, (2) background model training, and (3) active selection of the next datapoints. Under this framework, we open-source the IAdet tool, which is specific for single-class object detection. Additionally, we devise a method for automatically evaluating such a human-in-the-loop system. For the PASCAL VOC dataset, the IAdet tool reduces the database annotation time by $25\%$ while providing a trained model for free. These results are obtained for a deliberately very simple IAdet design. As a consequence, IAdet is susceptible to multiple easy improvements, paving the way for powerful human-in-the-loop object detection systems.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种名为智能标注（IA）的模型训练策略。IA包括三个模块：（1）助记数据标注、（2）背景模型训练和（3）活动选择下一个数据点。在这个框架下，我们开源了专门 для单类物体检测的IADE工具。此外，我们开发了一种自动评估这种人工Loop系统的方法。对于PASCAL VOC数据集，IADE工具可以将数据库标注时间减少$25\%$，并提供一个免费的训练模型。这些结果是基于一个故意非常简单的IADE设计来获得的。因此，IADE具有多个容易改进的地方，这将为人工Loop对象检测系统开额。
</details></li>
</ul>
<hr>
<h2 id="Optimal-and-Efficient-Binary-Questioning-for-Human-in-the-Loop-Annotation"><a href="#Optimal-and-Efficient-Binary-Questioning-for-Human-in-the-Loop-Annotation" class="headerlink" title="Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation"></a>Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01578">http://arxiv.org/abs/2307.01578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franco Marchesoni-Acland, Jean-Michel Morel, Josselin Kherroubi, Gabriele Facciolo</li>
<li>for: 本研究旨在解决人工纠正数据集的全面标注问题，即使有预测器可用。</li>
<li>methods: 本研究使用了一系列的优化策略和lookahead最小化代理成本函数来解决问题。</li>
<li>results: 对于 synthetic 和实际世界的数据集，提议的方法可以实现 significiant 改善（23-86%）的标注效率。In English, that would be:</li>
<li>for: The paper aims to solve the problem of fully annotating a binary classification dataset when a predictor is available.</li>
<li>methods: The paper uses a series of optimization strategies and lookahead minimization of proxy cost functions to solve the problem.</li>
<li>results: On synthetic and real-world datasets, the proposed method achieves significant improvements (23-86%) in annotation efficiency.<details>
<summary>Abstract</summary>
Even though data annotation is extremely important for interpretability, research and development of artificial intelligence solutions, most research efforts such as active learning or few-shot learning focus on the sample efficiency problem. This paper studies the neglected complementary problem of getting annotated data given a predictor. For the simple binary classification setting, we present the spectrum ranging from optimal general solutions to practical efficient methods. The problem is framed as the full annotation of a binary classification dataset with the minimal number of yes/no questions when a predictor is available. For the case of general binary questions the solution is found in coding theory, where the optimal questioning strategy is given by the Huffman encoding of the possible labelings. However, this approach is computationally intractable even for small dataset sizes. We propose an alternative practical solution based on several heuristics and lookahead minimization of proxy cost functions. The proposed solution is analysed, compared with optimal solutions and evaluated on several synthetic and real-world datasets. On these datasets, the method allows a significant improvement ($23-86\%$) in annotation efficiency.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管数据标注对人工智能研发和解释性很重要，大多数研究强调样本效率问题，这篇论文却研究受到忽略的问题——使用预测器获取标注数据。在简单的二分类设置下，我们提出了谱范围从优质通用解决方案到实用效果的方法。问题设为使用预测器完全标注二分类 dataset 的最小 yes/no 问题数。对于普通的二分类问题，解决方案基于编码理论，其中优化问题的策略是根据可能的标签编码使用 Huffman 编码。然而，这种方法对小型数据集来说是计算拥塞的。我们提出了一种实用的解决方案，基于多种准则和lookahead最小化代理成本函数。解决方案被分析、与优质解决方案进行比较，并在多个 sintetic 和实际世界数据集上评估。在这些数据集上，方法可以实现23-86%的标注效率提升。
</details></li>
</ul>
<hr>
<h2 id="Conceptual-Cognitive-Maps-Formation-with-Neural-Successor-Networks-and-Word-Embeddings"><a href="#Conceptual-Cognitive-Maps-Formation-with-Neural-Successor-Networks-and-Word-Embeddings" class="headerlink" title="Conceptual Cognitive Maps Formation with Neural Successor Networks and Word Embeddings"></a>Conceptual Cognitive Maps Formation with Neural Successor Networks and Word Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01577">http://arxiv.org/abs/2307.01577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Stoewer, Achim Schilling, Andreas Maier, Patrick Krauss</li>
<li>for: 这个论文旨在探讨人工智能中如何利用人脑中的Contextualization能力，以提高人工智能模型的表现。</li>
<li>methods: 该论文使用Successor Representation和神经网络，以及word embedding vector，构建了三个不同概念的认知地图。</li>
<li>results: 该模型能够学习两种不同的地图级别，并将新信息与相关的先前知识表示相似地分布在认知地图中。<details>
<summary>Abstract</summary>
The human brain possesses the extraordinary capability to contextualize the information it receives from our environment. The entorhinal-hippocampal plays a critical role in this function, as it is deeply engaged in memory processing and constructing cognitive maps using place and grid cells. Comprehending and leveraging this ability could significantly augment the field of artificial intelligence. The multi-scale successor representation serves as a good model for the functionality of place and grid cells and has already shown promise in this role. Here, we introduce a model that employs successor representations and neural networks, along with word embedding vectors, to construct a cognitive map of three separate concepts. The network adeptly learns two different scaled maps and situates new information in proximity to related pre-existing representations. The dispersion of information across the cognitive map varies according to its scale - either being heavily concentrated, resulting in the formation of the three concepts, or spread evenly throughout the map. We suggest that our model could potentially improve current AI models by providing multi-modal context information to any input, based on a similarity metric for the input and pre-existing knowledge representations.
</details>
<details>
<summary>摘要</summary>
人脑具有Contextualizing信息的杰出能力，即将来自环境的信息Contextualized在我们的认知中。Entorhinal-hippocampal系统在这一功能中扮演关键角色，因为它深入参与记忆处理和构建认知地图，使用Place和Grid维度。理解和利用这种能力可能会大幅提升人工智能领域。我们提出一种使用Successor表示和神经网络，以及Word Embedding向量，构建三个不同概念的认知地图。该网络能够学习两种不同的缩放级别的地图，并将新的信息与相关的先前表示相关联。信息在认知地图中的散布方式因缩放级别而异，可能是集中形成三个概念，或者在整个地图中均匀分布。我们建议，我们的模型可能可以提高当前的人工智能模型，通过为输入提供多modal的上下文信息，基于输入和先前知识表示之间的相似度 metric。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Based-Intrusion-Detection-Feature-Selection-versus-Feature-Extraction"><a href="#Machine-Learning-Based-Intrusion-Detection-Feature-Selection-versus-Feature-Extraction" class="headerlink" title="Machine Learning-Based Intrusion Detection: Feature Selection versus Feature Extraction"></a>Machine Learning-Based Intrusion Detection: Feature Selection versus Feature Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01570">http://arxiv.org/abs/2307.01570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vu-Duc Ngo, Tuan-Cuong Vuong, Thien Van Luong, Hung Tran</li>
<li>for: 这种研究旨在比较Feature Selection和Feature Extraction两种方法在网络入侵检测中的性能，以及它们在不同的数据集和分类方式下的运行时间复杂度。</li>
<li>methods: 这种研究使用了UNSW-NB15数据集和多种性能指标，如准确率、回归率、检测精度和运行时间复杂度，对Feature Selection和Feature Extraction两种方法进行了比较。</li>
<li>results: 研究发现，Feature Selection方法在多数情况下提供了更好的检测性能，同时具有较低的训练和检测时间复杂度。然而，Feature Extraction方法在某些情况下（如K很小）表现更为可靠，并且对K的变化更为敏感。<details>
<summary>Abstract</summary>
Internet of things (IoT) has been playing an important role in many sectors, such as smart cities, smart agriculture, smart healthcare, and smart manufacturing. However, IoT devices are highly vulnerable to cyber-attacks, which may result in security breaches and data leakages. To effectively prevent these attacks, a variety of machine learning-based network intrusion detection methods for IoT networks have been developed, which often rely on either feature extraction or feature selection techniques for reducing the dimension of input data before being fed into machine learning models. This aims to make the detection complexity low enough for real-time operations, which is particularly vital in any intrusion detection systems. This paper provides a comprehensive comparison between these two feature reduction methods of intrusion detection in terms of various performance metrics, namely, precision rate, recall rate, detection accuracy, as well as runtime complexity, in the presence of the modern UNSW-NB15 dataset as well as both binary and multiclass classification. For example, in general, the feature selection method not only provides better detection performance but also lower training and inference time compared to its feature extraction counterpart, especially when the number of reduced features K increases. However, the feature extraction method is much more reliable than its selection counterpart, particularly when K is very small, such as K = 4. Additionally, feature extraction is less sensitive to changing the number of reduced features K than feature selection, and this holds true for both binary and multiclass classifications. Based on this comparison, we provide a useful guideline for selecting a suitable intrusion detection type for each specific scenario, as detailed in Tab. 14 at the end of Section IV.
</details>
<details>
<summary>摘要</summary>
互联网智能化 (IoT) 在多个领域中扮演着重要角色，如智能城市、智能农业、智能医疗和智能制造。然而，IoT 设备高度易受到黑客攻击，这可能会导致安全泄露和数据泄露。为了有效防止这些攻击，一些基于机器学习的网络入侵检测方法在 IoT 网络中得到了开发，这些方法通常是基于特征抽象或特征选择技术来将输入数据的维度降低到可以被机器学习模型处理的水平。这样可以确保检测的复杂度足够低，以便在实时运行，特别是在任何入侵检测系统中。本文提供了对这两种特征减少方法的入侵检测方法在不同的性能指标下进行了比较，包括精度率、回传率、检测率和运行时间复杂度。例如，通常来说，选择特征方法不仅提供更好的检测性能，而且还比特征抽象方法来的训练和检测时间更短，尤其当K增加时。然而，抽象方法在K很小时（例如K=4）时更加可靠，而且特征选择方法比特征抽象方法更敏感于K的变化。根据这个比较，我们提供了一个实用的指南，可以帮助您在具体情况下选择适合的入侵检测类型，详细可见在表14中。
</details></li>
</ul>
<hr>
<h2 id="Scalable-variable-selection-for-two-view-learning-tasks-with-projection-operators"><a href="#Scalable-variable-selection-for-two-view-learning-tasks-with-projection-operators" class="headerlink" title="Scalable variable selection for two-view learning tasks with projection operators"></a>Scalable variable selection for two-view learning tasks with projection operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01558">http://arxiv.org/abs/2307.01558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aalto-ics-kepaco/projse">https://github.com/aalto-ics-kepaco/projse</a></li>
<li>paper_authors: Sandor Szedmak, Riikka Huusari, Tat Hong Duong Le, Juho Rousu</li>
<li>for: 提出了一种新的变量选择方法，适用于两视图设置或 vector-valued supervised learning 问题。</li>
<li>methods: 使用迭代选择高度相关于输出变量的变量，但与先前选择的变量不相关。使用投影算子和其代数来测量相关性，并可以利用 kernel 函数来表达非线性相关模型。</li>
<li>results: 通过实验 validate 了我们的方法，并在真实数据上验证了其扩展性和选择的有用性。<details>
<summary>Abstract</summary>
In this paper we propose a novel variable selection method for two-view settings, or for vector-valued supervised learning problems. Our framework is able to handle extremely large scale selection tasks, where number of data samples could be even millions. In a nutshell, our method performs variable selection by iteratively selecting variables that are highly correlated with the output variables, but which are not correlated with the previously chosen variables. To measure the correlation, our method uses the concept of projection operators and their algebra. With the projection operators the relationship, correlation, between sets of input and output variables can also be expressed by kernel functions, thus nonlinear correlation models can be exploited as well. We experimentally validate our approach, showing on both synthetic and real data its scalability and the relevance of the selected features. Keywords: Supervised variable selection, vector-valued learning, projection-valued measure, reproducing kernel Hilbert space
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的变量选择方法，适用于两视设置或vector-valued生育学问题。我们的框架可以处理巨大规模的选择任务，数据样本数可以达到十万级。总之，我们的方法通过逐步选择与输出变量高度相关的变量，但与之前选择的变量不相关的方式进行变量选择。为了度量相关性，我们使用投影算子和其代数来测量输入和输出变量之间的相关性。通过投影算子，我们可以将输入和输出变量之间的相关性表示为kernel函数，从而可以利用非线性相关模型。我们实验 validate我们的方法，并在Synthetic和实际数据上展示了其扩展性和选择的有效性。关键词：supervised变量选择，vector-valued学习，投影值度量，复制kernel希尔бер特空间。
</details></li>
</ul>
<hr>
<h2 id="Separated-RoadTopoFormer"><a href="#Separated-RoadTopoFormer" class="headerlink" title="Separated RoadTopoFormer"></a>Separated RoadTopoFormer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01557">http://arxiv.org/abs/2307.01557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingjie Lu, Yuanxian Huang, Ji Liu, Jinzhang Peng, Lu Tian, Ashish Sirasao</li>
<li>for: 本研究的目的是提高自动驾驶技术的实现，强调了驾驶场景理解的重要性。</li>
<li>methods: 本研究使用了Separated RoadTopoFormer框架，这是一个端到端的框架，可以同时检测路径中的交通元素和车道中心线，以及这些元素之间的关系。</li>
<li>results: 本研究的最终提交得分为0.445 OLS，在两个子任务和总分中都具有竞争力。<details>
<summary>Abstract</summary>
Understanding driving scenarios is crucial to realizing autonomous driving. Previous works such as map learning and BEV lane detection neglect the connection relationship between lane instances, and traffic elements detection tasks usually neglect the relationship with lane lines. To address these issues, the task is presented which includes 4 sub-tasks, the detection of traffic elements, the detection of lane centerlines, reasoning connection relationships among lanes, and reasoning assignment relationships between lanes and traffic elements. We present Separated RoadTopoFormer to tackle the issues, which is an end-to-end framework that detects lane centerline and traffic elements with reasoning relationships among them. We optimize each module separately to prevent interaction with each other and aggregate them together with few finetunes. For two detection heads, we adopted a DETR-like architecture to detect objects, and for the relationship head, we concat two instance features from front detectors and feed them to the classifier to obtain relationship probability. Our final submission achieves 0.445 OLS, which is competitive in both sub-task and combined scores.
</details>
<details>
<summary>摘要</summary>
理解驾驶场景是自动驾驶实现的关键。先前的工作，如地图学习和BEV车道检测，忽略了车道实例之间的连接关系，而交通元素检测任务通常忽略车道线的关系。为解决这些问题，我们提出了一个包含4个子任务的任务，即交通元素检测、车道中心线检测、车道间连接关系的推理和车道和交通元素之间的关系推理。我们提出了分离的路况拟合器（Separated RoadTopoFormer）来解决这些问题，它是一个端到端框架，可以同时检测车道中心线和交通元素，并推理它们之间的关系。我们对每个模块进行独立优化，以避免它们之间的交互，并将它们粗略地融合。为两个检测头，我们采用了一种类似于DETR架构来检测对象，而对于关系头，我们将前两个检测器的实例特征 concatenate 并feed 到分类器来获得关系概率。我们的最终提交得分为0.445 OLS，这在两个子任务和合并分数中都是竞争力强的。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graph-for-NLG-in-the-context-of-conversational-agents"><a href="#Knowledge-Graph-for-NLG-in-the-context-of-conversational-agents" class="headerlink" title="Knowledge Graph for NLG in the context of conversational agents"></a>Knowledge Graph for NLG in the context of conversational agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01548">http://arxiv.org/abs/2307.01548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussam Ghanem, Massinissa Atmani, Christophe Cruz</li>
<li>for: 本文提供了对知识图（KG）到文本生成的不同架构的回顾，包括图神经网络、图转换器和 seq2seq 模型。</li>
<li>methods: 本文讲解了不同架构的优势和局限性，并指出了在实际任务中选择架构的重要性。</li>
<li>results: 本文选择了基于 PLM 的 seq2seq 转换器模型来进行知识图到文本生成任务，并计划修改 PLM 上的 kg-to-text 生成数据集，以及在未来的工作中探讨情感和多语言维度。<details>
<summary>Abstract</summary>
The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness of the responses provided by a conversational agent. While generating answers during conversations consists in generating text from these KGs, it is still regarded as a challenging task that has gained significant attention in recent years. In this document, we provide a review of different architectures used for knowledge graph-to-text generation including: Graph Neural Networks, the Graph Transformer, and linearization with seq2seq models. We discuss the advantages and limitations of each architecture and conclude that the choice of architecture will depend on the specific requirements of the task at hand. We also highlight the importance of considering constraints such as execution time and model validity, particularly in the context of conversational agents. Based on these constraints and the availability of labeled data for the domains of DAVI, we choose to use seq2seq Transformer-based models (PLMs) for the Knowledge Graph-to-Text Generation task. We aim to refine benchmark datasets of kg-to-text generation on PLMs and to explore the emotional and multilingual dimensions in our future work. Overall, this review provides insights into the different approaches for knowledge graph-to-text generation and outlines future directions for research in this area.
</details>
<details>
<summary>摘要</summary>
使用知识图（KG）可以提高对话机器人的回答准确性和全面性。在生成回答时，从KG中生成文本是一项有趣且复杂的任务，在最近几年内受到了广泛关注。在本文中，我们提供了不同架构的知识图到文本生成评论，包括图神经网络、图转换器和 linearization with seq2seq 模型。我们讨论了每个架构的优点和缺点，并结论认为选择架构取决于任务的具体需求。我们还 highlight了考虑执行时间和模型有效性的重要性，特别在对话机器人的上下文中。基于这些约束和 DAVI 领域的可用标注数据，我们选择使用 Transformer 基于 seq2seq 模型（PLMs）进行知识图到文本生成任务。我们希望可以修改 PLMs 上 kg-to-text 生成的标准数据集，并在未来的工作中探讨情感和多语言维度。总的来说，本文提供了不同的知识图到文本生成方法的概述，并预示了未来这个领域的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Prompt-in-the-Classroom-to-Understand-AI-Limits-A-pilot-study"><a href="#Learning-to-Prompt-in-the-Classroom-to-Understand-AI-Limits-A-pilot-study" class="headerlink" title="Learning to Prompt in the Classroom to Understand AI Limits: A pilot study"></a>Learning to Prompt in the Classroom to Understand AI Limits: A pilot study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01540">http://arxiv.org/abs/2307.01540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emily Theophilou, Cansu Koyuturk, Mona Yavari, Sathya Bursic, Gregor Donabauer, Alessia Telari, Alessia Testa, Raffaele Boiano, Davinia Hernandez-Leo, Martin Ruskov, Davide Taibi, Alessandro Gabbiadini, Dimitri Ognibene</li>
<li>for: 本研究的目的是研究人工智能的acceptance和用途，以及如何将人工智能应用于解决社会问题。</li>
<li>methods: 本研究使用了Large Language Models（LLM）和其 derivated chatbots，如ChatGPT，来实现人工智能的自然语言处理能力。</li>
<li>results: 研究获得了显著的结果，包括学生对人工智能的评价高，对 chatGPT 的互动质量改善，对人工智能的态度变得更积极，同时更好地理解人工智能的限制。<details>
<summary>Abstract</summary>
Artificial intelligence's progress holds great promise in assisting society in addressing pressing societal issues. In particular Large Language Models (LLM) and the derived chatbots, like ChatGPT, have highly improved the natural language processing capabilities of AI systems allowing them to process an unprecedented amount of unstructured data. The consequent hype has also backfired, raising negative sentiment even after novel AI methods' surprising contributions. One of the causes, but also an important issue per se, is the rising and misleading feeling of being able to access and process any form of knowledge to solve problems in any domain with no effort or previous expertise in AI or problem domain, disregarding current LLMs limits, such as hallucinations and reasoning limits. Acknowledging AI fallibility is crucial to address the impact of dogmatic overconfidence in possibly erroneous suggestions generated by LLMs. At the same time, it can reduce fear and other negative attitudes toward AI. AI literacy interventions are necessary that allow the public to understand such LLM limits and learn how to use them in a more effective manner, i.e. learning to "prompt". With this aim, a pilot educational intervention was performed in a high school with 30 students. It involved (i) presenting high-level concepts about intelligence, AI, and LLM, (ii) an initial naive practice with ChatGPT in a non-trivial task, and finally (iii) applying currently-accepted prompting strategies. Encouraging preliminary results have been collected such as students reporting a) high appreciation of the activity, b) improved quality of the interaction with the LLM during the educational activity, c) decreased negative sentiments toward AI, d) increased understanding of limitations and specifically We aim to study factors that impact AI acceptance and to refine and repeat this activity in more controlled settings.
</details>
<details>
<summary>摘要</summary>
人工智能的进步具有巨大的承诺，可以帮助社会解决一系列的社会问题。特别是大型自然语言处理模型（LLM）和其 derivated chatbot如ChatGPT，有效提高了人工智能系统的自然语言处理能力，使其能处理前所未有的大量不结构化数据。然而，这种热情也导致了负面情绪的升温，包括误导和过度自信的问题。一个重要的原因是人们对人工智能的训练和应用方面的知识和经验不足，导致他们假设AI系统可以轻松地解决任何问题，不需要专业知识或培训。我们认为，承诺AI系统的有限性是关键，以避免“欺诈式”的过度自信和误导。同时，了解AI系统的限制可以减少对人工智能的恐惧和负面情绪。为此，我们发展了一种AI文化干预措施，以帮助公众更好地理解LLM的限制和如何更有效地使用它们。在这个研究中，我们在一所高中进行了一场教育干预，涉及到（i）介绍智能、AI和LLM的概念，（ii）初步使用ChatGPT完成一项非常困难的任务，以及（iii）应用已知的提示策略。结果表明，学生对这个活动表示高度的欢迎，并且在与LLM的交互中改善了质量，同时减少了对人工智能的负面情绪。我们计划进一步研究这些因素，以便更好地理解AI接受度的影响因素，并在更加控制的环境下重复和改进这种活动。
</details></li>
</ul>
<hr>
<h2 id="Anomaly-detection-in-image-or-latent-space-of-patch-based-auto-encoders-for-industrial-image-analysis"><a href="#Anomaly-detection-in-image-or-latent-space-of-patch-based-auto-encoders-for-industrial-image-analysis" class="headerlink" title="Anomaly detection in image or latent space of patch-based auto-encoders for industrial image analysis"></a>Anomaly detection in image or latent space of patch-based auto-encoders for industrial image analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02495">http://arxiv.org/abs/2307.02495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Pinon, Robin Trombetta, Carole Lartizien</li>
<li>for: 检测颜色图像中异常点的方法</li>
<li>methods: 基于patch-based auto-encoder构建的三种方法： errors between original image and its reconstruction, support estimation of normal image distribution in latent space, and error between original image and restored version of reconstructed image</li>
<li>results: 在MVTecAD工业图像数据库上评估和比较三种方法的性能，并与两种现有的状态前方法进行比较<details>
<summary>Abstract</summary>
We study several methods for detecting anomalies in color images, constructed on patch-based auto-encoders. Wecompare the performance of three types of methods based, first, on the error between the original image and its reconstruction,second, on the support estimation of the normal image distribution in the latent space, and third, on the error between the originalimage and a restored version of the reconstructed image. These methods are evaluated on the industrial image database MVTecADand compared to two competitive state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
我们研究了一些用于检测颜色图像异常的方法，基于patch-based自适应编码器。我们对三种类型的方法进行比较，分别是根据原始图像与其重建图像之间的错误、在隐藏空间中正常图像分布的支持估计、以及原始图像与重建后的图像之间的错误。这些方法在MVTecAD工业图像数据库上进行评估，并与两种现有的状态艺术方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Intentional-Behavior-in-Autonomous-Agents-under-Uncertainty"><a href="#Analyzing-Intentional-Behavior-in-Autonomous-Agents-under-Uncertainty" class="headerlink" title="Analyzing Intentional Behavior in Autonomous Agents under Uncertainty"></a>Analyzing Intentional Behavior in Autonomous Agents under Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01532">http://arxiv.org/abs/2307.01532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/filipcano/intentional-autonomous-agents">https://github.com/filipcano/intentional-autonomous-agents</a></li>
<li>paper_authors: Filip Cano Córdoba, Samuel Judson, Timos Antonopoulos, Katrine Bjørner, Nicholas Shoemaker, Scott J. Shapiro, Ruzica Piskac, Bettina Könighofer</li>
<li>for: 本研究旨在提供一种量化评估自动决策系统的责任性，以便在不确定环境中进行原则正的决策。</li>
<li>methods: 我们使用Markov决策过程（MDP）模型不确定环境，并使用概率模型检查来计算机器人在达到某个事件的能力。我们称这为“职责范围”。我们还使用Counterfactual reasoning来自动生成相关的场景，以提高评估的可靠性。</li>
<li>results: 我们通过一个实验示例，证明我们的方法可以区分“意图的”和“巧合的”交通事故。<details>
<summary>Abstract</summary>
Principled accountability for autonomous decision-making in uncertain environments requires distinguishing intentional outcomes from negligent designs from actual accidents. We propose analyzing the behavior of autonomous agents through a quantitative measure of the evidence of intentional behavior. We model an uncertain environment as a Markov Decision Process (MDP). For a given scenario, we rely on probabilistic model checking to compute the ability of the agent to influence reaching a certain event. We call this the scope of agency. We say that there is evidence of intentional behavior if the scope of agency is high and the decisions of the agent are close to being optimal for reaching the event. Our method applies counterfactual reasoning to automatically generate relevant scenarios that can be analyzed to increase the confidence of our assessment. In a case study, we show how our method can distinguish between 'intentional' and 'accidental' traffic collisions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用原则性的负责任度来评估自主决策在不确定环境中的决策过程，需要分辨出意图的结果和不注意的设计。我们提出了基于量化度量的意图行为分析方法。我们将不确定环境模型为马尔可夫决策过程（MDP）。对于每个场景，我们采用概率模型检查来计算机器人的影响力。我们称之为作用范围。如果作用范围高并且机器人的决策较近于最优的达成事件，我们认为有意图行为的证据。我们的方法使用Counterfactual reasoning自动生成相关的场景，以提高评估的信度。在一个案例研究中，我们示例如如何使用我们的方法分辨出“意图”和“意外”的交通事故。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Transformer-for-Autonomous-Recognition-and-Grading-of-Tomatoes-Under-Various-Lighting-Occlusion-and-Ripeness-Conditions"><a href="#Convolutional-Transformer-for-Autonomous-Recognition-and-Grading-of-Tomatoes-Under-Various-Lighting-Occlusion-and-Ripeness-Conditions" class="headerlink" title="Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions"></a>Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01530">http://arxiv.org/abs/2307.01530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asim Khan, Taimur Hassan, Muhammad Shafay, Israa Fahmy, Naoufel Werghi, Lakmal Seneviratne, Irfan Hussain<br>for:*  Tomatoes are harvested using mobile robots in real-world scenarios, but this is challenging due to factors such as occlusion and color similarity.methods:*  The proposed framework uses a convolutional transformer architecture to autonomously recognize and grade tomatoes, regardless of occlusion level, lighting conditions, and ripeness.results:*  The proposed framework outperforms existing methods by 58.14%, 65.42%, and 66.39% in terms of mean average precision scores on three different datasets.Here are the three points in Simplified Chinese text:for:*  Tomatoes 被用mobile robots在实际场景中收割，但这会受到叶子和枝条的 occlusion 和 Tomatoes 和周围植物的颜色相似性的影响。methods:*  this research 提出了一种基于 convolutional transformer 架构的自动 Tomatoes 识别和评分方法，不 regard occlusion 水平、照明条件和成熟度。results:*  this research 的提出方法在三个不同的 dataset 上比基eline 方法高出 58.14%, 65.42%, 66.39% 的mean average precision 分数。<details>
<summary>Abstract</summary>
Harvesting fully ripe tomatoes with mobile robots presents significant challenges in real-world scenarios. These challenges arise from factors such as occlusion caused by leaves and branches, as well as the color similarity between tomatoes and the surrounding foliage during the fruit development stage. The natural environment further compounds these issues with varying light conditions, viewing angles, occlusion factors, and different maturity levels. To overcome these obstacles, this research introduces a novel framework that leverages a convolutional transformer architecture to autonomously recognize and grade tomatoes, irrespective of their occlusion level, lighting conditions, and ripeness. The proposed model is trained and tested using carefully annotated images curated specifically for this purpose. The dataset is prepared under various lighting conditions, viewing perspectives, and employs different mobile camera sensors, distinguishing it from existing datasets such as Laboro Tomato and Rob2Pheno Annotated Tomato. The effectiveness of the proposed framework in handling cluttered and occluded tomato instances was evaluated using two additional public datasets, Laboro Tomato and Rob2Pheno Annotated Tomato, as benchmarks. The evaluation results across these three datasets demonstrate the exceptional performance of our proposed framework, surpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of mean average precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated Tomato, respectively. The results underscore the superiority of the proposed model in accurately detecting and delineating tomatoes compared to baseline methods and previous approaches. Specifically, the model achieves an F1-score of 80.14%, a Dice coefficient of 73.26%, and a mean IoU of 66.41% on the KUTomaData image dataset.
</details>
<details>
<summary>摘要</summary>
摘取全熟 Tomatoes 的 mobile robot 在实际应用场景中具有显著的挑战。这些挑战来自于叶子和枝条所引起的遮挡，以及 Tomatoes 和周围的植物发育阶段的颜色相似性。自然环境更进一步复杂这些问题，包括不同的照明条件、视角、遮挡因子和不同的成熟度。为了解决这些问题，这项研究提出了一种新的框架，利用卷积变换器架构来自动认识和评分 Tomatoes，不受遮挡、照明条件和成熟度的影响。该模型被训练和测试使用特别为这个目的制作的精心注释图像集。该数据集在不同的照明条件和视角下准备，并使用不同的移动摄像头感知器，与现有的数据集不同。为了评估提案的效果，我们使用了两个额外的公共数据集，即 Laboro Tomato 和 Rob2Pheno Annotated Tomato，作为标准。测试结果表明，提案的框架在处理受遮挡和受遮挡的 Tomatoes 实例方面表现出色，相比基eline方法和前一代方法，提案的模型在三个数据集上的mean average precision分数上出色，高于state-of-the-art的58.14%、65.42%和66.39%。结果表明，提案的模型在识别和定义 Tomatoes 方面具有显著的优势，具体来说，模型在 KUTomaData 图像集上达到了80.14%的F1分数、73.26%的Dice系数和66.41%的mean IoU。
</details></li>
</ul>
<hr>
<h2 id="LEAT-Towards-Robust-Deepfake-Disruption-in-Real-World-Scenarios-via-Latent-Ensemble-Attack"><a href="#LEAT-Towards-Robust-Deepfake-Disruption-in-Real-World-Scenarios-via-Latent-Ensemble-Attack" class="headerlink" title="LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via Latent Ensemble Attack"></a>LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via Latent Ensemble Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01520">http://arxiv.org/abs/2307.01520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joonkyo Shim, Hyunsoo Yoon</li>
<li>for: 防止深伪（deepfake）威胁， recent studies 使用 adversarial perturbation 来攻击深伪模型的输出。</li>
<li>methods: 我们提出了一个简单又有效的攻击方法，called Latent Ensemble ATtack (LEAT)，它攻击独立的潜在编码过程，从而生成不同于目标属性的复杂的出力图像。</li>
<li>results: 我们的方法在实际应用中具有更高的防护成功率，比之前的方法更加有效。<details>
<summary>Abstract</summary>
Deepfakes, malicious visual contents created by generative models, pose an increasingly harmful threat to society. To proactively mitigate deepfake damages, recent studies have employed adversarial perturbation to disrupt deepfake model outputs. However, previous approaches primarily focus on generating distorted outputs based on only predetermined target attributes, leading to a lack of robustness in real-world scenarios where target attributes are unknown. Additionally, the transferability of perturbations between two prominent generative models, Generative Adversarial Networks (GANs) and Diffusion Models, remains unexplored. In this paper, we emphasize the importance of target attribute-transferability and model-transferability for achieving robust deepfake disruption. To address this challenge, we propose a simple yet effective disruption method called Latent Ensemble ATtack (LEAT), which attacks the independent latent encoding process. By disrupting the latent encoding process, it generates distorted output images in subsequent generation processes, regardless of the given target attributes. This target attribute-agnostic attack ensures robust disruption even when the target attributes are unknown. Additionally, we introduce a Normalized Gradient Ensemble strategy that effectively aggregates gradients for iterative gradient attacks, enabling simultaneous attacks on various types of deepfake models, involving both GAN-based and Diffusion-based models. Moreover, we demonstrate the insufficiency of evaluating disruption quality solely based on pixel-level differences. As a result, we propose an alternative protocol for comprehensively evaluating the success of defense. Extensive experiments confirm the efficacy of our method in disrupting deepfakes in real-world scenarios, reporting a higher defense success rate compared to previous methods.
</details>
<details>
<summary>摘要</summary>
深刻复制（Deepfakes），由生成模型生成的恶意视觉内容，对社会 pose 危害性的增加。为了积极防止深刻复制害处， latest studies have employed adversarial perturbation to disrupt deepfake model outputs. However, previous approaches primarily focus on generating distorted outputs based on predetermined target attributes, leading to a lack of robustness in real-world scenarios where target attributes are unknown. Additionally, the transferability of perturbations between two prominent generative models, 生成 adversarial Networks (GANs) and Diffusion Models, remains unexplored. In this paper, we emphasize the importance of target attribute-transferability and model-transferability for achieving robust deepfake disruption. To address this challenge, we propose a simple yet effective disruption method called Latent Ensemble ATtack (LEAT), which attacks the independent latent encoding process. By disrupting the latent encoding process, it generates distorted output images in subsequent generation processes, regardless of the given target attributes. This target attribute-agnostic attack ensures robust disruption even when the target attributes are unknown. Additionally, we introduce a Normalized Gradient Ensemble strategy that effectively aggregates gradients for iterative gradient attacks, enabling simultaneous attacks on various types of deepfake models, involving both GAN-based and Diffusion-based models. Moreover, we demonstrate the insufficiency of evaluating disruption quality solely based on pixel-level differences. As a result, we propose an alternative protocol for comprehensively evaluating the success of defense. Extensive experiments confirm the efficacy of our method in disrupting deepfakes in real-world scenarios, reporting a higher defense success rate compared to previous methods.
</details></li>
</ul>
<hr>
<h2 id="Deep-Attention-Q-Network-for-Personalized-Treatment-Recommendation"><a href="#Deep-Attention-Q-Network-for-Personalized-Treatment-Recommendation" class="headerlink" title="Deep Attention Q-Network for Personalized Treatment Recommendation"></a>Deep Attention Q-Network for Personalized Treatment Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01519">http://arxiv.org/abs/2307.01519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevenmsm/rl-icu-daqn">https://github.com/stevenmsm/rl-icu-daqn</a></li>
<li>paper_authors: Simin Ma, Junghwan Lee, Nicoleta Serban, Shihao Yang</li>
<li>for: 这研究旨在提出个性化治疗建议的新方法，以提高医疗结果。</li>
<li>methods: 该研究使用深度注意力Q网络， combinig transformer架构和深度强化学习框架，fficiently incorporate all past patient observations。</li>
<li>results: 研究在实际的 septic shock 和急性低血压 cohorts 中展示了其超过当前状态艺术模型的优势。<details>
<summary>Abstract</summary>
Tailoring treatment for individual patients is crucial yet challenging in order to achieve optimal healthcare outcomes. Recent advances in reinforcement learning offer promising personalized treatment recommendations; however, they rely solely on current patient observations (vital signs, demographics) as the patient's state, which may not accurately represent the true health status of the patient. This limitation hampers policy learning and evaluation, ultimately limiting treatment effectiveness. In this study, we propose the Deep Attention Q-Network for personalized treatment recommendations, utilizing the Transformer architecture within a deep reinforcement learning framework to efficiently incorporate all past patient observations. We evaluated the model on real-world sepsis and acute hypotension cohorts, demonstrating its superiority to state-of-the-art models. The source code for our model is available at https://github.com/stevenmsm/RL-ICU-DAQN.
</details>
<details>
<summary>摘要</summary>
个人化治疗是现代医疗的关键，但同时也是非常具有挑战性，以达到最佳医疗结果。最新的增强学习技术具有个人化治疗建议的承诺，但是它们只是基于当前患者的观察（生命 Parameters、人口）来判断患者的状态，这可能并不准确地反映患者的真实健康状况。这种限制会阻碍策略学习和评估，从而限制治疗的效果。在这项研究中，我们提出了深度注意力Q网络，利用转换器架构在深度增强学习框架中高效地包含所有过去患者的观察。我们对实际世界的 septic shock 和急性低血压群体进行了评估，并证明了我们的模型的优越性。模型的源代码可以在https://github.com/stevenmsm/RL-ICU-DAQN上获取。
</details></li>
</ul>
<hr>
<h2 id="All-in-One-Multi-task-Prompting-for-Graph-Neural-Networks"><a href="#All-in-One-Multi-task-Prompting-for-Graph-Neural-Networks" class="headerlink" title="All in One: Multi-task Prompting for Graph Neural Networks"></a>All in One: Multi-task Prompting for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01504">http://arxiv.org/abs/2307.01504</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sheldonresearch/ProG">https://github.com/sheldonresearch/ProG</a></li>
<li>paper_authors: Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan</li>
<li>for: 填充预训练模型的知识空间，以提高graph任务的性能。</li>
<li>methods: 提出了一种多任务提问方法，通过统一格式、语言提问的拓展和下游任务的改进，使得自然语言处理中的提问思想可以轻松地应用于图领域。</li>
<li>results: 经过广泛的实验，结果表明该方法能够提高graph任务的性能，并且可以适应不同的任务。<details>
<summary>Abstract</summary>
Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.
</details>
<details>
<summary>摘要</summary>
近期，“预训练和精度调整”被广泛采用为许多图像任务的标准工作流程，因为它可以将通用的图像知识传递给不同应用程序，填补每个应用程序的图像缺失。然而，图像任务中的节点水平、边水平和图像水平具有很大的多样性，这使得预训练预测经常与这些多种任务不兼容。这种差距甚至可能导致特定应用程序的负面传播，从而导致Results poor。 inspirited by the prompt learning in natural language processing (NLP), which has shown significant effectiveness in leveraging prior knowledge for various NLP tasks, we investigate the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.
</details></li>
</ul>
<hr>
<h2 id="HEDI-First-Time-Clinical-Application-and-Results-of-a-Biomechanical-Evaluation-and-Visualisation-Tool-for-Incisional-Hernia-Repair"><a href="#HEDI-First-Time-Clinical-Application-and-Results-of-a-Biomechanical-Evaluation-and-Visualisation-Tool-for-Incisional-Hernia-Repair" class="headerlink" title="HEDI: First-Time Clinical Application and Results of a Biomechanical Evaluation and Visualisation Tool for Incisional Hernia Repair"></a>HEDI: First-Time Clinical Application and Results of a Biomechanical Evaluation and Visualisation Tool for Incisional Hernia Repair</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01502">http://arxiv.org/abs/2307.01502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob J. Relle, Samuel Voß, Ramesch Raschidi, Regine Nessel, Johannes Görich, Mark O. Wielpütz, Thorsten Löffler, Vincent Heuveline, Friedrich Kallinowski, Philipp D. Lösel</li>
<li>for: 治疗腹部坏死病例，减少疼痛、不适和重复手术的风险</li>
<li>methods: 使用生物力学方法，考虑腹部团附肌肉的活动、内部压力、组织弹性和腹部膨润，以提高腹部坏死病例的治疗效果</li>
<li>results: 在31名患者的临床应用中，使用HEDI工具进行预操作评估，比报告的成功率显著提高，所有患者三年后没有疼痛和坏死病例重复Here’s the English version of the three key points for reference:</li>
<li>for: Treatment of abdominal wall defects to reduce pain, discomfort, and the risk of repeated surgical repairs</li>
<li>methods: Use of biomechanical methods that consider muscle activation, intra-abdominal pressure, tissue elasticity, and abdominal wall distention to improve treatment outcomes</li>
<li>results: Significantly improved success rates in the first clinical application of HEDI in the preoperative evaluation of 31 patients, with all patients remaining pain-free and showing no hernia recurrence after three years of follow-up.<details>
<summary>Abstract</summary>
Abdominal wall defects often lead to pain, discomfort, and recurrence of incisional hernias, resulting in significant morbidity and repeated surgical repairs worldwide. Mesh repair for large hernias is usually based on the defect area with a fixed overlap, without considering biomechanical aspects such as muscle activation, intra-abdominal pressure, tissue elasticity, and abdominal wall distention. To address this issue, we present a biomechanical approach to incisional hernia repair that takes into account the unstable abdominal wall. Additionally, we introduce HEDI, a tool that uses dynamic computed tomography with Valsalva maneuver to automatically detect and assess hernia size, volume, and abdominal wall instability. Our first clinical application of HEDI in the preoperative evaluation of 31 patients shows significantly improved success rates compared to reported rates, with all patients remaining pain-free and showing no hernia recurrence after three years of follow-up.
</details>
<details>
<summary>摘要</summary>
腹壁损害常引起疼痛、不适和重复的 Hernia 复发，导致全球较高的患者病况和多次手术修复。固定 overlap 的网 repair 通常基于缺陷区域，没有考虑生物力学方面的因素，如肌肉活动、腹部压力、组织弹性和腹壁膨胀。为解决这个问题，我们提出了一种生物力学方法 для剖股 Hernia 修复，考虑到不稳定的腹壁。此外，我们还介绍了 HEDI，一种使用动态计算tomography 和 Valsalva 优化来自动检测和评估 Hernia 大小、体积和腹壁不稳定性。我们在31名患者的前Operative 评估中首次应用 HEDI，显示成功率明显高于报道率，所有患者三年跟踪后保持无疼痛，无 Hernia 复发。
</details></li>
</ul>
<hr>
<h2 id="FREEDOM-Target-Label-Source-Data-Domain-Information-Free-Multi-Source-Domain-Adaptation-for-Unsupervised-Personalization"><a href="#FREEDOM-Target-Label-Source-Data-Domain-Information-Free-Multi-Source-Domain-Adaptation-for-Unsupervised-Personalization" class="headerlink" title="FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization"></a>FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02493">http://arxiv.org/abs/2307.02493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunju Yang, Gyusang Cho, Chan-Hyun Youn</li>
<li>for: 本研究旨在提出一种实用的多源领域适应（MSDA）问题scenario，以适应部署模型到客户端的数据集。</li>
<li>methods: 本研究提出了一种新的适应问题scenario，称为Three-Free Domain Adaptation（TFDA），其中目标标签、源数据集和源领域信息（领域标签和领域数量）都不可用。为解决这种问题scenario，我们提出了一种实用的适应框架called FREEDOM。</li>
<li>results: FREEDOM可以在无源领域信息的情况下实现state-of-the-art或相当的性能，同时减少了最终模型的大小。此外，FREEDOM可以独立于源领域数量进行部署。<details>
<summary>Abstract</summary>
From a service perspective, Multi-Source Domain Adaptation (MSDA) is a promising scenario to adapt a deployed model to a client's dataset. It can provide adaptation without a target label and support the case where a source dataset is constructed from multiple domains. However, it is impractical, wherein its training heavily relies on prior domain information of the multi-source dataset -- how many domains exist and the domain label of each data sample. Moreover, MSDA requires both source and target datasets simultaneously (physically), causing storage limitations on the client device or data privacy issues by transferring client data to a server. For a more practical scenario of model adaptation from a service provider's point of view, we relax these constraints and present a novel problem scenario of Three-Free Domain Adaptation, namely TFDA, where 1) target labels, 2) source dataset, and mostly 3) source domain information (domain labels + the number of domains) are unavailable. Under the problem scenario, we propose a practical adaptation framework called FREEDOM. It leverages the power of the generative model, disentangling data into class and style aspects, where the style is defined as the class-independent information from the source data and designed with a nonparametric Bayesian approach. In the adaptation stage, FREEDOM aims to match the source class distribution with the target's under the philosophy that class distribution is consistent even if the style is different; after then, only part of the classification model is deployed as a personalized network. As a result, FREEDOM achieves state-of-the-art or comparable performance even without domain information, with reduced final model size on the target side, independent of the number of source domains.
</details>
<details>
<summary>摘要</summary>
从服务方面来看，多源领域适应（MSDA）是一个有前途的enario，可以将部署的模型适应到客户的资料集。它可以无需目标标签进行适应，并且可以处理多个领域的资料集合建构的情况。然而，MSDA的训练 heavily rely on多个领域的领域信息（每个数据标签），并且需要源和目标资料集同时存在（physically），导致客户设备限制或资料隐私问题。为了更实际的模型适应方案，我们将这些限制放宽，并提出了一个新的问题场景：三自领域适应（TFDA），其中1）目标标签、2）源资料集和3）源领域信息（领域标签和领域数量）都不可用。在这个问题场景下，我们提出了一个实用的适应框架called FREEDOM。它利用了生成模型的力量，将数据分解为类别和风格的两个方面，其中风格是源数据中的class独立信息，通过非 Parametric Bayesian方法设计。在适应阶段，FREEDOM的目标是将源类别分布与目标的类别分布相对consistent，然后仅部署一部分的分类模型为个人化网络。因此，FREEDOM可以实现state-of-the-art或相等的性能，而不需要领域信息，并且对数据集大小进行干扰。
</details></li>
</ul>
<hr>
<h2 id="A-Bibliographic-Study-on-Artificial-Intelligence-Research-Global-Panorama-and-Indian-Appearance"><a href="#A-Bibliographic-Study-on-Artificial-Intelligence-Research-Global-Panorama-and-Indian-Appearance" class="headerlink" title="A Bibliographic Study on Artificial Intelligence Research: Global Panorama and Indian Appearance"></a>A Bibliographic Study on Artificial Intelligence Research: Global Panorama and Indian Appearance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00705">http://arxiv.org/abs/2308.00705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Tiwari, Susmita Bardhan, Vikas Kumar</li>
<li>for: 本研究用科学映射方法对2015-2020年的人工智能（AI）研究进行了 bibliometric 分析，以了解AI研究的发展趋势。</li>
<li>methods: 本研究使用了Scopus数据库收集必要的数据，并对数据进行了手动和工具（OpenRefine）的数据转换，以便进行分析。</li>
<li>results: 研究发现，在2015-2020年间，开放获取和商业刊物的AI研究量相对较高，IEEE是最主要的出版商，发表了84%的最高引用文章。此外，中国和美国是AI领域的主要贡献国。研究还发现， neural networks和深度学习是AI研究中最主要的话题。最后，研究发现，不仅公共机构，私人机构也在投入AI研究。<details>
<summary>Abstract</summary>
The present study identifies and assesses the bibliographic trend in Artificial Intelligence (AI) research for the years 2015-2020 using the science mapping method of bibliometric study. The required data has been collected from the Scopus database. To make the collected data analysis-ready, essential data transformation was performed manually and with the help of a tool viz. OpenRefine. For determining the trend and performing the mapping techniques, top five open access and commercial journals of AI have been chosen based on their citescore driven ranking. The work includes 6880 articles published in the specified period for analysis. The trend is based on Country-wise publications, year-wise publications, topical terms in AI, top-cited articles, prominent authors, major institutions, involvement of industries in AI and Indian appearance. The results show that compared to open access journals; commercial journals have a higher citescore and number of articles published over the years. Additionally, IEEE is the prominent publisher which publishes 84% of the top-cited publications. Further, China and the United States are the major contributors to literature in the AI domain. The study reveals that neural networks and deep learning are the major topics included in top AI research publications. Recently, not only public institutions but also private bodies are investing their resources in AI research. The study also investigates the relative position of Indian researchers in terms of AI research. Present work helps in understanding the initial development, current stand and future direction of AI.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Country-wise publications: China and the United States are the major contributors to AI literature.2. Year-wise publications: There has been a steady increase in the number of publications over the years.3. Topical terms in AI: Neural networks and deep learning are the major topics included in top AI research publications.4. Top-cited articles: IEEE publishes 84% of the top-cited publications.5. Prominent authors: The study reveals that there are several prominent authors in the field of AI.6. Major institutions: The study shows that China and the United States are the major contributors to AI research.7. Involvement of industries in AI: Private bodies are investing their resources in AI research, in addition to public institutions.8. Indian appearance: The study investigates the relative position of Indian researchers in terms of AI research.The present work provides an understanding of the initial development, current stand, and future direction of AI research.</details></li>
</ol>
<hr>
<h2 id="Mitigating-Bias-Enhancing-Image-Classification-by-Improving-Model-Explanations"><a href="#Mitigating-Bias-Enhancing-Image-Classification-by-Improving-Model-Explanations" class="headerlink" title="Mitigating Bias: Enhancing Image Classification by Improving Model Explanations"></a>Mitigating Bias: Enhancing Image Classification by Improving Model Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01473">http://arxiv.org/abs/2307.01473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raha Ahmadi, Mohammad Javad Rajabi, Mohammad Khalooie, Mohammad Sabokrou</li>
<li>for: 提高图像分类器的主要概念理解和表示，增强模型对图像中主要元素的理解。</li>
<li>methods: 提出了一种新的方法，通过同时引导模型对前景进行注意力调控，使模型更好地捕捉图像中的主要概念。</li>
<li>results: 通过对标准数据集进行广泛的实验，证明了该方法的有效性，提高了图像分类器的准确率。<details>
<summary>Abstract</summary>
Deep learning models have demonstrated remarkable capabilities in learning complex patterns and concepts from training data. However, recent findings indicate that these models tend to rely heavily on simple and easily discernible features present in the background of images rather than the main concepts or objects they are intended to classify. This phenomenon poses a challenge to image classifiers as the crucial elements of interest in images may be overshadowed. In this paper, we propose a novel approach to address this issue and improve the learning of main concepts by image classifiers. Our central idea revolves around concurrently guiding the model's attention toward the foreground during the classification task. By emphasizing the foreground, which encapsulates the primary objects of interest, we aim to shift the focus of the model away from the dominant influence of the background. To accomplish this, we introduce a mechanism that encourages the model to allocate sufficient attention to the foreground. We investigate various strategies, including modifying the loss function or incorporating additional architectural components, to enable the classifier to effectively capture the primary concept within an image. Additionally, we explore the impact of different foreground attention mechanisms on model performance and provide insights into their effectiveness. Through extensive experimentation on benchmark datasets, we demonstrate the efficacy of our proposed approach in improving the classification accuracy of image classifiers. Our findings highlight the importance of foreground attention in enhancing model understanding and representation of the main concepts within images. The results of this study contribute to advancing the field of image classification and provide valuable insights for developing more robust and accurate deep-learning models.
</details>
<details>
<summary>摘要</summary>
深度学习模型已经展现出了学习复杂模式和概念的Remarkable能力。然而，最近的发现表明这些模型在训练数据中很可能会依赖于图像的背景中的简单和易于识别的特征，而不是主要的概念或 объек。这种情况会导致图像分类器的挑战，因为图像中的关键元素可能会被遮盖。在这篇论文中，我们提出了一种新的方法来解决这个问题，并提高图像分类器的学习。我们的中心思想是在分类任务中同时引导模型的注意力向前景方向。通过强调前景，我们希望使模型忽略背景的主导性的影响。为实现这一点，我们引入了一种机制，使得模型能够充分分配注意力于前景。我们 investigate了多种策略，包括修改损失函数或添加额外的建筑 комponents，以使模型能够有效地捕捉图像中的主要概念。此外，我们还探讨了不同的前景注意力机制对模型性能的影响，并提供了有价值的发现。通过对标准数据集进行广泛的实验，我们证明了我们提出的方法的可行性和效果。我们的发现指出了图像分类器中的前景注意力对模型理解和图像中主要概念的表示的重要性。这些发现对图像分类领域的进一步发展具有重要意义，并为开发更加稳定和准确的深度学习模型提供了有价值的发现。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Conservatism-Diffusion-Policies-in-Offline-Multi-agent-Reinforcement-Learning"><a href="#Beyond-Conservatism-Diffusion-Policies-in-Offline-Multi-agent-Reinforcement-Learning" class="headerlink" title="Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning"></a>Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01472">http://arxiv.org/abs/2307.01472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoran Li, Ling Pan, Longbo Huang</li>
<li>For: 提出了一种新的多智能体偏Diffusion Offline Multi-agent Model（DOM2），用于离线多智能体学习（MARL）。* Methods: 在政策网络中 интегри了扩散模型，并提出了一种轨迹基据增强方案，以提高政策表达力和多样性。* Results: 对多智能体particle和多智能体MuJoCo环境进行了广泛的实验，并显示了DOM2在环境变化时的稳定性和表现优于现有方法。此外，DOM2在数据效率方面也表现出了优势，可以在$20+$ times less data的情况下达到现有方法的性能水平。<details>
<summary>Abstract</summary>
We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms that rely mainly on conservatism in policy design, DOM2 enhances policy expressiveness and diversity based on diffusion. Specifically, we incorporate a diffusion model into the policy network and propose a trajectory-based data-augmentation scheme in training. These key ingredients make our algorithm more robust to environment changes and achieve significant improvements in performance, generalization and data-efficiency. Our extensive experimental results demonstrate that DOM2 outperforms existing state-of-the-art methods in multi-agent particle and multi-agent MuJoCo environments, and generalizes significantly better in shifted environments thanks to its high expressiveness and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve state-of-the-art performance with $20+$ times less data compared to existing algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的多智能体吸引模型（DOM2），用于离线多智能体学习（MARL）。与现有的策略设计依赖保守主义的算法不同，DOM2 增强策略表达性和多样性基于吸引模型。我们在策略网络中 integrate 了吸引模型，并提出了一种基于轨迹的数据增强方案。这些关键元素使得我们的算法更加鲁棒地应对环境变化，并实现了显著提高表现、泛化和数据效率。我们的广泛的实验结果表明，DOM2 在多智能体粒子和多智能体 MuJoCo 环境中表现出色，并在Shifted环境中具有更高的表现和泛化能力。此外，DOM2 还示出了更高的数据效率，可以使用 $20++$  times less data  than existing algorithms 达到相同的表现。
</details></li>
</ul>
<hr>
<h2 id="A-multilevel-framework-for-AI-governance"><a href="#A-multilevel-framework-for-AI-governance" class="headerlink" title="A multilevel framework for AI governance"></a>A multilevel framework for AI governance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03198">http://arxiv.org/abs/2307.03198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyesun Choung, Prabu David, John S. Seberger</li>
<li>for: 本研究旨在发展一种基于伦理和基本人类价值观的AI治理框架，以实现AI的潜在好荣和风险减少。</li>
<li>methods: 本研究使用多级治理方法，包括政府、企业和公民三个层次的潜在利益相互关系，以及三个维度的信任（能力、完整性和善良）。</li>
<li>results: 本研究提供了实用的洞察，可以用于进一步提高用户体验和指导AI相关公共政策。<details>
<summary>Abstract</summary>
To realize the potential benefits and mitigate potential risks of AI, it is necessary to develop a framework of governance that conforms to ethics and fundamental human values. Although several organizations have issued guidelines and ethical frameworks for trustworthy AI, without a mediating governance structure, these ethical principles will not translate into practice. In this paper, we propose a multilevel governance approach that involves three groups of interdependent stakeholders: governments, corporations, and citizens. We examine their interrelationships through dimensions of trust, such as competence, integrity, and benevolence. The levels of governance combined with the dimensions of trust in AI provide practical insights that can be used to further enhance user experiences and inform public policy related to AI.
</details>
<details>
<summary>摘要</summary>
为了实现人工智能的潜在优势并mitigate其潜在风险，需要建立一个遵循伦理和基本人类价值观的管理框架。虽然一些组织已经发布了信任worthy AI的指南和伦理体系，但无法mediating governance结构，这些伦理原则将不会在实践中传递。在这篇论文中，我们提议一种多级管理方法，其包括三个相互依赖的团队：政府、企业和公民。我们研究这些团队之间的关系通过信任的维度，如能力、完整性和好意。这些管理层结合了信任的维度，可以为AI的用户经验提供实用的洞察，同时也可以为AI相关的公共政策提供指导。
</details></li>
</ul>
<hr>
<h2 id="Causal-Reinforcement-Learning-A-Survey"><a href="#Causal-Reinforcement-Learning-A-Survey" class="headerlink" title="Causal Reinforcement Learning: A Survey"></a>Causal Reinforcement Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01452">http://arxiv.org/abs/2307.01452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang</li>
<li>for: 本文主要是为了介绍 causal reinforcement learning，一种增强现有算法的措施，通过 incorporating causal relationships 来增强知识传递效果。</li>
<li>methods: 本文首先介绍了 causality 和 reinforcement learning 的基本概念，然后解释了如何通过 causality Address core challenges in non-causal reinforcement learning。最后，文章系统地审视了现有的 causal reinforcement learning 方法，根据其 Target problems 和方法ologies 进行分类。</li>
<li>results: 本文提供了一个 comprehensive review of the literature on causal reinforcement learning，包括了现有的方法和技术，以及未来的开发方向。<details>
<summary>Abstract</summary>
Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcement learning. We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning. We categorize and systematically review existing causal reinforcement learning approaches based on their target problems and methodologies. Finally, we outline open issues and future directions in this emerging field.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化字版本。<</SYS>>根据序列决策问题下的不确定性，力量学习是一种重要的思想模式。虽然过去几十年内有很多出色的成就，但是在实际应用中仍然存在很多挑战。其中一个主要的障碍是，力量学习代理人缺乏世界的基本理解，因此必须通过多次尝试和错误互动来学习。它们还可能面临困难提供决策的解释和泛化获得的知识。然而， causality 却提供了一个明显的优势，即可以系统地形式化知识，并利用不变性来实现有效的知识传递。这导致了 causal reinforcement learning 的出现，这是一种增强现有算法的方法，通过包含 causal 关系来增强学习过程。在这篇评论中，我们系统地介绍了 causal reinforcement learning 的文献。我们首先介绍了 causality 和 reinforcement learning 的基本概念，然后解释了如何通过 causality 解决非 causal reinforcement learning 中的核心挑战。然后，我们按照目标问题和方法分类地系统地审查了现有的 causal reinforcement learning 方法。最后，我们列出了未解决的问题和未来方向。
</details></li>
</ul>
<hr>
<h2 id="A-Double-Machine-Learning-Approach-to-Combining-Experimental-and-Observational-Data"><a href="#A-Double-Machine-Learning-Approach-to-Combining-Experimental-and-Observational-Data" class="headerlink" title="A Double Machine Learning Approach to Combining Experimental and Observational Data"></a>A Double Machine Learning Approach to Combining Experimental and Observational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01449">http://arxiv.org/abs/2307.01449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Morucci, Vittorio Orlandi, Harsh Parikh, Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky</li>
<li>for:  combines experimental and observational studies to test for assumption violations and estimate treatment effects consistently</li>
<li>methods: double machine learning approach, tests for violations of external validity and ignorability under milder assumptions, semi-parametrically efficient treatment effect estimators</li>
<li>results: demonstrated in three real-world case studies, relevant for practical settings<details>
<summary>Abstract</summary>
Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
</details>
<details>
<summary>摘要</summary>
实验和观察研究经常受到有效性问题，因为假设未经测试。我们提出了一种双机器学习方法，将实验和观察研究结合起来，让实践者可以一直测试假设违反和处理效果的一致性。我们的框架测试了外在效应和忽略性的违反，假设更加轻松。只有一个假设违反时，我们提供了半 Parametric 有效的处理效果估计器。但我们的食物免费 theorem 表明，要准确地识别违反的假设，以确保可靠地估计处理效果。我们在三个实际案例中示例出了我们的方法的实用性。
</details></li>
</ul>
<hr>
<h2 id="TablEye-Seeing-small-Tables-through-the-Lens-of-Images"><a href="#TablEye-Seeing-small-Tables-through-the-Lens-of-Images" class="headerlink" title="TablEye: Seeing small Tables through the Lens of Images"></a>TablEye: Seeing small Tables through the Lens of Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02491">http://arxiv.org/abs/2307.02491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seung-eon Lee, Sang-Chul Lee</li>
<li>for: 本研究旨在探讨几shot表格学习的可能性，尤其是在表格数据中缺乏 Labeling 的情况下。</li>
<li>methods: 本研究提出了一种名为 TablEye 的框架，通过域转换来缓解表格数据中的限制，并采用了一些已经证明有效的几shot学习算法和嵌入函数。</li>
<li>results:  TablEye 在 4-shot 任务中的最高 AUC 为 0.11，在 1-shot 设置中的平均提前率为 3.17%。这表明 TablEye 在几shot表格学习中表现出色。<details>
<summary>Abstract</summary>
The exploration of few-shot tabular learning becomes imperative. Tabular data is a versatile representation that captures diverse information, yet it is not exempt from limitations, property of data and model size. Labeling extensive tabular data can be challenging, and it may not be feasible to capture every important feature. Few-shot tabular learning, however, remains relatively unexplored, primarily due to scarcity of shared information among independent datasets and the inherent ambiguity in defining boundaries within tabular data. To the best of our knowledge, no meaningful and unrestricted few-shot tabular learning techniques have been developed without imposing constraints on the dataset. In this paper, we propose an innovative framework called TablEye, which aims to overcome the limit of forming prior knowledge for tabular data by adopting domain transformation. It facilitates domain transformation by generating tabular images, which effectively conserve the intrinsic semantics of the original tabular data. This approach harnesses rigorously tested few-shot learning algorithms and embedding functions to acquire and apply prior knowledge. Leveraging shared data domains allows us to utilize this prior knowledge, originally learned from the image domain. Specifically, TablEye demonstrated a superior performance by outstripping the TabLLM in a 4-shot task with a maximum 0.11 AUC and a STUNT in a 1- shot setting, where it led on average by 3.17% accuracy.
</details>
<details>
<summary>摘要</summary>
《几招学习 tabular 数据的探索成为必要。表格数据是一种多元的表示方式，涵盖了多种信息，但它并不免受数据和模型大小的限制。对于广泛的表格数据进行标签可能是困难的，而且可能无法捕捉到所有重要的特征。然而，几招 tabular 学习仍然 relativity 未explored，主要是因为独立的数据集之间的共享信息缺乏，以及表格数据中的内在含糊性。根据我们所知，没有有意义且不受限制的几招 tabular 学习技术有 been developed，不需要对数据集做任何限制。在这篇论文中，我们提出了一个创新的框架，即 TablEye，以解决表格数据的限制。TablEye 采用域转换来强制保留表格数据的内在 semantics，并采用了已经测试过的几招学习算法和嵌入函数来获取和应用优先知识。通过共享数据域，我们可以利用这些优先知识，原本从图像领域学习而来。 TablEye 在 4 折事件中的最大 AUC 为 0.11，在 1 折事件中的 STUNT 中平均高于 TabLLM 3.17% 的精度。》
</details></li>
</ul>
<hr>
<h2 id="Garbage-in-garbage-out-Zero-shot-detection-of-crime-using-Large-Language-Models"><a href="#Garbage-in-garbage-out-Zero-shot-detection-of-crime-using-Large-Language-Models" class="headerlink" title="Garbage in, garbage out: Zero-shot detection of crime using Large Language Models"></a>Garbage in, garbage out: Zero-shot detection of crime using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06844">http://arxiv.org/abs/2307.06844</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anjsimmo/zero-shot-crime-detection">https://github.com/anjsimmo/zero-shot-crime-detection</a></li>
<li>paper_authors: Anj Simmons, Rajesh Vasa</li>
<li>for: 这个论文旨在利用大语言模型学习的通用常识来进行零极端理解犯罪，给出文本描述的视频surveillance。</li>
<li>methods: 该论文使用大语言模型进行零极端理解犯罪，但是需要手动将视频转换成高质量的文本描述。</li>
<li>results: 研究发现，当视频被手动转换成高质量的文本描述时，大语言模型可以通过零极端理解犯罪，并且性能与现有的状态时技术相当。但是，现有的自动视频到文本转换方法无法生成高质量的视频描述，导致大语言模型输出垃圾结果。<details>
<summary>Abstract</summary>
This paper proposes exploiting the common sense knowledge learned by large language models to perform zero-shot reasoning about crimes given textual descriptions of surveillance videos. We show that when video is (manually) converted to high quality textual descriptions, large language models are capable of detecting and classifying crimes with state-of-the-art performance using only zero-shot reasoning. However, existing automated video-to-text approaches are unable to generate video descriptions of sufficient quality to support reasoning (garbage video descriptions into the large language model, garbage out).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unsupervised-Feature-Learning-with-Emergent-Data-Driven-Prototypicality"><a href="#Unsupervised-Feature-Learning-with-Emergent-Data-Driven-Prototypicality" class="headerlink" title="Unsupervised Feature Learning with Emergent Data-Driven Prototypicality"></a>Unsupervised Feature Learning with Emergent Data-Driven Prototypicality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01421">http://arxiv.org/abs/2307.01421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunhui Guo, Youren Zhang, Yubei Chen, Stella X. Yu</li>
<li>for: 在一个没有标签的图像集中，我们的目标是训练一个模型，使其对每个图像映射到一个特征空间中，以便不仅 proximity 表示视觉相似性，而且图像的位置直接表示数据集中的趋势。</li>
<li>methods: 我们的关键发现是在径比空间中进行无监督特征学习，而不是在欧几里得空间中。在这个空间中，点之间的距离仍然表示图像相似性，而且我们获得了更多的容量来表示数据集中的趋势。</li>
<li>results: 我们提出了一种无监督特征学习算法，使用径比空间中的圆束排序（HACK）。HACK 首先生成了径比空间中的均匀填充的粒子，然后将每个图像分配给每个粒子。在凝固后，图像会更加典型地表示数据集。我们的特征映射器只需要在径比空间中扩散训练实例，我们发现图像会靠近起始点，确认我们的想法：无监督的趋势发现。我们展示了我们的数据驱动的趋势可以轻松地实现无监督实例选择，提高模型的普适性和对于非典型实例的鲁棒性。<details>
<summary>Abstract</summary>
Given an image set without any labels, our goal is to train a model that maps each image to a point in a feature space such that, not only proximity indicates visual similarity, but where it is located directly encodes how prototypical the image is according to the dataset.   Our key insight is to perform unsupervised feature learning in hyperbolic instead of Euclidean space, where the distance between points still reflect image similarity, and yet we gain additional capacity for representing prototypicality with the location of the point: The closer it is to the origin, the more prototypical it is. The latter property is simply emergent from optimizing the usual metric learning objective: The image similar to many training instances is best placed at the center of corresponding points in Euclidean space, but closer to the origin in hyperbolic space.   We propose an unsupervised feature learning algorithm in Hyperbolic space with sphere pACKing. HACK first generates uniformly packed particles in the Poincar\'e ball of hyperbolic space and then assigns each image uniquely to each particle. Images after congealing are regarded more typical of the dataset it belongs to. With our feature mapper simply trained to spread out training instances in hyperbolic space, we observe that images move closer to the origin with congealing, validating our idea of unsupervised prototypicality discovery. We demonstrate that our data-driven prototypicality provides an easy and superior unsupervised instance selection to reduce sample complexity, increase model generalization with atypical instances and robustness with typical ones.
</details>
<details>
<summary>摘要</summary>
We propose an unsupervised feature learning algorithm in hyperbolic space with sphere pACKing. HACK first generates uniformly packed particles in the Poincaré ball of hyperbolic space and then assigns each image uniquely to each particle. Images after congealing are regarded as more typical of the dataset they belong to. With our feature mapper simply trained to spread out training instances in hyperbolic space, we observe that images move closer to the origin with congealing, validating our idea of unsupervised prototypicality discovery.We demonstrate that our data-driven prototypicality provides an easy and superior unsupervised instance selection to reduce sample complexity, increase model generalization with atypical instances, and robustness with typical ones.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-vulnerabilities-in-SplitFed-Learning-Assessing-the-robustness-against-Data-Poisoning-Attacks"><a href="#Analyzing-the-vulnerabilities-in-SplitFed-Learning-Assessing-the-robustness-against-Data-Poisoning-Attacks" class="headerlink" title="Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks"></a>Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03197">http://arxiv.org/abs/2307.03197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aysha Thahsin Zahir Ismail, Raj Mani Shukla</li>
<li>for: 这篇论文旨在研究分布式协同机器学习（DCML）中数据欺诈攻击的影响。特别是在DCML中 Split learning（SL）和 Federated Learning（FL）的混合方法（SplitFed Learning，SFL）中。</li>
<li>methods: 本研究提出了三种新型攻击策略，包括无目标攻击、Targeted攻击和距离基于攻击。这些攻击策略的目标是降低基于DCML的分类器性能。</li>
<li>results: 经过对三种攻击策略的测试和分析，结果显示无目标和距离基于攻击对SFL中的分类器性能有更大的影响，而Targeted攻击的影响相对较小。这些结果也在两个不同的案例研究中进行了验证。<details>
<summary>Abstract</summary>
Distributed Collaborative Machine Learning (DCML) is a potential alternative to address the privacy concerns associated with centralized machine learning. The Split learning (SL) and Federated Learning (FL) are the two effective learning approaches in DCML. Recently there have been an increased interest on the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is the earliest attempt to study, analyze and present the impact of data poisoning attacks in SFL. We propose three kinds of novel attack strategies namely untargeted, targeted and distance-based attacks for SFL. All the attacks strategies aim to degrade the performance of the DCML-based classifier. We test the proposed attack strategies for two different case studies on Electrocardiogram signal classification and automatic handwritten digit recognition. A series of attack experiments were conducted by varying the percentage of malicious clients and the choice of the model split layer between the clients and the server. The results after the comprehensive analysis of attack strategies clearly convey that untargeted and distance-based poisoning attacks have greater impacts in evading the classifier outcomes compared to targeted attacks in SFL
</details>
<details>
<summary>摘要</summary>
分布式合作机器学习（DCML）是一种可能的中央机器学习隐私问题的解决方案。Split learning（SL）和联邦学习（FL）是DCML中两种有效的学习方法。在最近，关于FL和SL的混合，即SplitFed Learning（SFL）的兴趣增加。这项研究是DCML基于类别器性能下数据毒素攻击的第一个研究。我们提出了三种新的攻击策略，包括无目标、targeted和距离基于攻击，这些攻击策略的目的都是降低DCML基于类别器的性能。我们对两个不同的案例进行了电势心跳信号分类和自动手写数字识别的测试，并通过变化客户端中的Percentage of malicious clients和模型Split层来进行了一系列攻击实验。结果表明，无目标和距离基于攻击更有可能影响DCML基于类别器的性能，compared to targeted attacks。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Communicate-using-Contrastive-Learning"><a href="#Learning-to-Communicate-using-Contrastive-Learning" class="headerlink" title="Learning to Communicate using Contrastive Learning"></a>Learning to Communicate using Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01403">http://arxiv.org/abs/2307.01403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SonamSangpoLama/Music-Genre-Classification">https://github.com/SonamSangpoLama/Music-Genre-Classification</a></li>
<li>paper_authors: Yat Long Lo, Biswa Sengupta, Jakob Foerster, Michael Noukhovitch</li>
<li>for: 这篇论文的目的是提出一种基于对比学习的通信方法，以便在多智能RL中实现有效的协调。</li>
<li>methods: 该方法利用在智能代理之间交换的通信信息，并通过分析这些信息的关系来学习有效的通信方式。</li>
<li>results: 该方法在通信 essencial 环境下表现出色，比前一些工作更高效和更快速地学习。此外，该方法还能够使通信更加 симметричной，并capture环境中的全局状态信息。<details>
<summary>Abstract</summary>
Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
</details>
<details>
<summary>摘要</summary>
通信是多智能体RL中协调的有力工具。但是引入有效、通用语言是一项困难挑战，特别是在分布式设定下。在这项工作中，我们提出一种不同的视角，即在智能体之间交换的通信消息被视为环境状态的不同不完整的视图。通过对交换的消息之间关系的检查，我们提议使用对比学习来最大化交换消息序列中的相互信息。在通信必需的环境下，我们的方法在性能和学习速度两个方面都超过了前一个工作。使用质量度量和表示探测，我们表明我们的方法导致更Symmetric的通信和捕捉环境中的全局状态信息。总之，我们展示了对比学习的力量和通信消息作为编码的利用对RL中的协调有益。
</details></li>
</ul>
<hr>
<h2 id="In-depth-Analysis-On-Parallel-Processing-Patterns-for-High-Performance-Dataframes"><a href="#In-depth-Analysis-On-Parallel-Processing-Patterns-for-High-Performance-Dataframes" class="headerlink" title="In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes"></a>In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01394">http://arxiv.org/abs/2307.01394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranda Perera, Arup Kumar Sarker, Mills Staylor, Gregor von Laszewski, Kaiying Shan, Supun Kamburugamuve, Chathura Widanage, Vibhatha Abeykoon, Thejaka Amila Kanewela, Geoffrey Fox</li>
<li>for: The paper aims to improve the performance of data processing pipelines by optimizing the use of Dataframes, which are widely used in data engineering applications.</li>
<li>methods: The authors propose a cost model for evaluating parallel processing patterns for distributed Dataframe operators and evaluate the performance of their reference runtime implementation, Cylon, on the ORNL Summit supercomputer.</li>
<li>results: The authors evaluate the performance of Cylon on the ORNL Summit supercomputer and present the results, which demonstrate the potential for improving the performance of data processing pipelines using their proposed approach.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提高数据处理管道的性能，通过优化广泛使用的数据框架（DataFrame）。</li>
<li>methods: 作者提出了一个评估分布式数据框架操作的成本模型，并评估了他们的参考运行时实现（Cylon）在ORNL Summit超级计算机上的性能。</li>
<li>results: 作者在ORNL Summit超级计算机上评估了Cylon的性能，并显示了他们的提案可以提高数据处理管道的性能。<details>
<summary>Abstract</summary>
The Data Science domain has expanded monumentally in both research and industry communities during the past decade, predominantly owing to the Big Data revolution. Artificial Intelligence (AI) and Machine Learning (ML) are bringing more complexities to data engineering applications, which are now integrated into data processing pipelines to process terabytes of data. Typically, a significant amount of time is spent on data preprocessing in these pipelines, and hence improving its e fficiency directly impacts the overall pipeline performance. The community has recently embraced the concept of Dataframes as the de-facto data structure for data representation and manipulation. However, the most widely used serial Dataframes today (R, pandas) experience performance limitations while working on even moderately large data sets. We believe that there is plenty of room for improvement by taking a look at this problem from a high-performance computing point of view. In a prior publication, we presented a set of parallel processing patterns for distributed dataframe operators and the reference runtime implementation, Cylon [1]. In this paper, we are expanding on the initial concept by introducing a cost model for evaluating the said patterns. Furthermore, we evaluate the performance of Cylon on the ORNL Summit supercomputer.
</details>
<details>
<summary>摘要</summary>
“数据科学领域在过去十年内发展壮大，主要归功于大数据革命。人工智能（AI）和机器学习（ML）的应用在数据工程领域中增加了复杂性，现在已经成为数据处理管道的一部分，处理 terrabytes 的数据。通常，数据预处理过程中需要投入大量时间，因此提高预处理效率直接影响整个管道性能。社区最近广泛接受了数据帧作为数据表示和修改的标准数据结构。然而，当前最广泛使用的串行数据帧（R、pandas）在处理中规模较大的数据集时会出现性能限制。我们认为，从高性能计算的角度来看这个问题，还有很多空间提高。在先前的发表文章中，我们提出了分布式数据帧操作的并行处理模式和参考实现 Cylon 。在这篇论文中，我们将对此概念进行扩展，并提出一种成本模型来评估所提出的Pattern。此外，我们还在 ORNL Summit 超级计算机上评估了 Cylon 的性能。”
</details></li>
</ul>
<hr>
<h2 id="Depth-video-data-enabled-predictions-of-longitudinal-dairy-cow-body-weight-using-thresholding-and-Mask-R-CNN-algorithms"><a href="#Depth-video-data-enabled-predictions-of-longitudinal-dairy-cow-body-weight-using-thresholding-and-Mask-R-CNN-algorithms" class="headerlink" title="Depth video data-enabled predictions of longitudinal dairy cow body weight using thresholding and Mask R-CNN algorithms"></a>Depth video data-enabled predictions of longitudinal dairy cow body weight using thresholding and Mask R-CNN algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01383">http://arxiv.org/abs/2307.01383</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yebigithub/BW_dairy">https://github.com/yebigithub/BW_dairy</a></li>
<li>paper_authors: Ye Bi, Leticia M. Campos, Jin Wang, Haipeng Yu, Mark D. Hanigan, Gota Morota</li>
<li>for: 这个研究的目的是预测牛体重，并使用视频数据来预测。</li>
<li>methods: 研究使用了深度学习segmentation方法，包括单resholding、自适应阈值和Mask R-CNN等三种方法来 segment牛体从背景。</li>
<li>results: 研究发现，使用Mask R-CNN方法和线性混合模型可以获得最佳预测系数和平均绝对误差，即0.98和2.03%。此外，这种方法也在留三头牛掉cross-validation中表现最佳。<details>
<summary>Abstract</summary>
Monitoring cow body weight is crucial to support farm management decisions due to its direct relationship with the growth, nutritional status, and health of dairy cows. Cow body weight is a repeated trait, however, the majority of previous body weight prediction research only used data collected at a single point in time. Furthermore, the utility of deep learning-based segmentation for body weight prediction using videos remains unanswered. Therefore, the objectives of this study were to predict cow body weight from repeatedly measured video data, to compare the performance of the thresholding and Mask R-CNN deep learning approaches, to evaluate the predictive ability of body weight regression models, and to promote open science in the animal science community by releasing the source code for video-based body weight prediction. A total of 40,405 depth images and depth map files were obtained from 10 lactating Holstein cows and 2 non-lactating Jersey cows. Three approaches were investigated to segment the cow's body from the background, including single thresholding, adaptive thresholding, and Mask R-CNN. Four image-derived biometric features, such as dorsal length, abdominal width, height, and volume, were estimated from the segmented images. On average, the Mask-RCNN approach combined with a linear mixed model resulted in the best prediction coefficient of determination and mean absolute percentage error of 0.98 and 2.03%, respectively, in the forecasting cross-validation. The Mask-RCNN approach was also the best in the leave-three-cows-out cross-validation. The prediction coefficients of determination and mean absolute percentage error of the Mask-RCNN coupled with the linear mixed model were 0.90 and 4.70%, respectively. Our results suggest that deep learning-based segmentation improves the prediction performance of cow body weight from longitudinal depth video data.
</details>
<details>
<summary>摘要</summary>
监测牛体重是重要的 farm 管理决策的支持因素，因为它直接关系着牛的生长、营养状况和健康。牛体重是一个 repeating 特征，但大多数之前的体重预测研究只使用了单点时间收集的数据。此外，使用视频深度学习 segmentation 对体重预测还没有得到充分的答案。因此，本研究的目标是预测牛体重从重复测量的视频数据，比较深度学习 segmentation 和阈值分割的性能，评估体重预测模型的预测能力，并促进动物科学社区开放科学的发展。总的来说，我们获得了 10 头排 milk Holstein 牛和 2 头不排 milk Jersey 牛的 40,405 个深度图像和深度图像文件。我们 investigate 三种方法来 segment 牛的身体和背景，包括单个阈值、自适应阈值和Mask R-CNN。从 segmented 图像中，我们估算了牛身体的四个图像特征，包括背部长度、腹部宽度、高度和体积。在预测cross-validation中，Mask R-CNN 方法与直线混合模型结合得到了最好的预测 coefficient of determination 和 mean absolute percentage error，具体值分别为 0.98 和 2.03%。在离别三只牛 cross-validation 中，Mask R-CNN 方法也是最好的。预测 coefficient of determination 和 mean absolute percentage error 的值分别为 0.90 和 4.70%。我们的结果表明，使用深度学习 segmentation 可以提高牛体重预测的精度，从长itudinal depth video 数据中预测牛体重。
</details></li>
</ul>
<hr>
<h2 id="Shifting-Attention-to-Relevance-Towards-the-Uncertainty-Estimation-of-Large-Language-Models"><a href="#Shifting-Attention-to-Relevance-Towards-the-Uncertainty-Estimation-of-Large-Language-Models" class="headerlink" title="Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models"></a>Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01379">http://arxiv.org/abs/2307.01379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhaoduan/shifting-attention-to-relevance">https://github.com/jinhaoduan/shifting-attention-to-relevance</a></li>
<li>paper_authors: Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, Kaidi Xu</li>
<li>for: 该研究旨在解决自然语言生成模型（LLM）中的不确定性评估问题，即用户可以信任模型输出的问题。</li>
<li>methods: 该研究使用了自动进行反推的LLM，并研究了生成不均衡（generative inequalities）如何影响不确定性评估。</li>
<li>results: 实验结果表明，可以通过对更重要的（relevant）组件进行共同偏移注意力来解决生成不均衡导致的偏袋。这种方法被称为共同偏移注意力到更重要的组件（SAR）。<details>
<summary>Abstract</summary>
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular "off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance.
</details>
<details>
<summary>摘要</summary>
although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular "off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance.Here's the translation in Traditional Chinese:although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular "off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance.
</details></li>
</ul>
<hr>
<h2 id="A-CNN-regression-model-to-estimate-buildings-height-maps-using-Sentinel-1-SAR-and-Sentinel-2-MSI-time-series"><a href="#A-CNN-regression-model-to-estimate-buildings-height-maps-using-Sentinel-1-SAR-and-Sentinel-2-MSI-time-series" class="headerlink" title="A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series"></a>A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01378">http://arxiv.org/abs/2307.01378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritu Yadav, Andrea Nascetti, Yifang Ban</li>
<li>For: The paper is written for urban planning, infrastructure management, and environmental analysis, with a focus on accurately estimating building heights using satellite time series data.* Methods: The paper proposes a supervised Multimodal Building Height Regression Network (MBHR-Net) that uses Sentinel-1 (S1) and Sentinel-2 (S2) satellite time series data to estimate building heights at 10m spatial resolution. The model extracts meaningful features from both S1 and S2 images to learn complex spatio-temporal relationships between image patterns and building heights.* Results: The preliminary results demonstrate the effectiveness of the MBHR-Net model in accurately estimating building heights, with a Root Mean Squared Error (RMSE) of 3.73m, an Intersection over Union (IOU) of 0.95, and an R-squared (R2) score of 0.61. These results show the potential of the model for urban planning, environmental impact analysis, and other related applications.Here is the simplified Chinese text for the three key information points:* 为：这篇论文是为城市规划、基础设施管理和环境分析而写的，旨在准确估计建筑高度使用卫星时序数据。* 方法：该论文提出一种监督式多Modal Building Height Regression Network（MBHR-Net），使用卫星时序数据Sentinel-1（S1）和Sentinel-2（S2）来估计建筑高度的10米空间分辨率。MBHR-Net利用S1和S2图像中的有价值特征来学习建筑高度与图像模式之间的复杂空间时间关系。* 结果：初步结果表明MBHR-Net模型可以准确估计建筑高度，RMSE为3.73米，IOU为0.95，R2为0.61。这些结果表明MBHR-Net模型在城市规划、环境影响分析等领域有广泛的应用前景。<details>
<summary>Abstract</summary>
Accurate estimation of building heights is essential for urban planning, infrastructure management, and environmental analysis. In this study, we propose a supervised Multimodal Building Height Regression Network (MBHR-Net) for estimating building heights at 10m spatial resolution using Sentinel-1 (S1) and Sentinel-2 (S2) satellite time series. S1 provides Synthetic Aperture Radar (SAR) data that offers valuable information on building structures, while S2 provides multispectral data that is sensitive to different land cover types, vegetation phenology, and building shadows. Our MBHR-Net aims to extract meaningful features from the S1 and S2 images to learn complex spatio-temporal relationships between image patterns and building heights. The model is trained and tested in 10 cities in the Netherlands. Root Mean Squared Error (RMSE), Intersection over Union (IOU), and R-squared (R2) score metrics are used to evaluate the performance of the model. The preliminary results (3.73m RMSE, 0.95 IoU, 0.61 R2) demonstrate the effectiveness of our deep learning model in accurately estimating building heights, showcasing its potential for urban planning, environmental impact analysis, and other related applications.
</details>
<details>
<summary>摘要</summary>
准确估算建筑高度是城市规划、基础设施管理和环境分析中非常重要的。在这项研究中，我们提出了一种监督式多Modal Building Height Regression Network（MBHR-Net），用于使用Sentinel-1（S1）和Sentinel-2（S2）卫星时序序数据来估算建筑高度的10米分辨率。S1提供Synthetic Aperture Radar（SAR）数据，可以提供建筑结构信息，而S2提供多spectral数据，敏感于不同的地面覆盖类型、植被生长阶段和建筑阴影。我们的MBHR-Net通过提取S1和S2图像中有用的特征来学习图像模式和建筑高度之间的复杂空间时间关系。模型在荷兰10座城市进行训练和测试。使用Root Mean Squared Error（RMSE）、Intersection over Union（IOU）和R-squared（R2） metric来评估模型的性能。初步结果（3.73米RMSE、0.95 IoU、0.61 R2）表明我们的深度学习模型在准确地估算建筑高度方面表现出色，这有助于城市规划、环境影响分析等相关应用。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Determination-of-Safety-Requirements-for-Perception-Systems"><a href="#Efficient-Determination-of-Safety-Requirements-for-Perception-Systems" class="headerlink" title="Efficient Determination of Safety Requirements for Perception Systems"></a>Efficient Determination of Safety Requirements for Perception Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01371">http://arxiv.org/abs/2307.01371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sydney M. Katz, Anthony L. Corso, Esen Yel, Mykel J. Kochenderfer</li>
<li>for: 本研究旨在提高安全性的感知系统设计，通过将高级安全要求转化为组件级别的要求。</li>
<li>methods: 本研究使用了 Gaussian processes 和 threshold bandits 等常见黑盒估计技术，并将其结合起来开发了一种新的估计方法，称为 smoothing bandits。</li>
<li>results: 在基于视觉的飞机冲突避免问题上进行了实验，并显示了 compared to Gaussian process 和 threshold bandit 基elines，smoothing bandits 方法可以提高准确性和效率。<details>
<summary>Abstract</summary>
Perception systems operate as a subcomponent of the general autonomy stack, and perception system designers often need to optimize performance characteristics while maintaining safety with respect to the overall closed-loop system. For this reason, it is useful to distill high-level safety requirements into component-level requirements on the perception system. In this work, we focus on efficiently determining sets of safe perception system performance characteristics given a black-box simulator of the fully-integrated, closed-loop system. We combine the advantages of common black-box estimation techniques such as Gaussian processes and threshold bandits to develop a new estimation method, which we call smoothing bandits. We demonstrate our method on a vision-based aircraft collision avoidance problem and show improvements in terms of both accuracy and efficiency over the Gaussian process and threshold bandit baselines.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:感知系统作为整体自主架构的一部分，感知系统设计师需要优化性能特性，同时保持关于全关环境的安全性。为此，可以将高层级的安全需求转换为 ком成器-level的需求。在这个工作中，我们专注于使用黑盒模拟器来划出安全的感知系统性能特性集。我们结合了常见的黑盒估计技术，如 Gaussian processes 和阈值拍赌，开发了一种新的估计方法，我们称之为“缓和拍赌”。我们在基于 computer vision 的飞机回避撞击问题上进行了实验，并证明了我们的方法在精度和效率上具有改进。
</details></li>
</ul>
<hr>
<h2 id="Minimizing-Age-of-Information-for-Mobile-Edge-Computing-Systems-A-Nested-Index-Approach"><a href="#Minimizing-Age-of-Information-for-Mobile-Edge-Computing-Systems-A-Nested-Index-Approach" class="headerlink" title="Minimizing Age of Information for Mobile Edge Computing Systems: A Nested Index Approach"></a>Minimizing Age of Information for Mobile Edge Computing Systems: A Nested Index Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01366">http://arxiv.org/abs/2307.01366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Chen, Ning Yang, Meng Zhang, Jun Wang</li>
<li>for: 实现实时应用需求，减少信息新鲜度指标 Age-of-Information (AoI) 的延迟。</li>
<li>methods: 利用移动 Edge 计算 (MEC) 技术，将任务从移动设备卸载到 Edge 节点进行计算，以提高计算效率。</li>
<li>results: 提出一种基于 Restless Multi-Arm-Bandit (RMAB) 问题的嵌入式指标框架，并设计一种嵌入式指标策略，可以提供可靠性和准确性的平衡。该策略可以减少优化率差距达40%，并且随系统缩放因子增大，可以达到下界的静态优化。<details>
<summary>Abstract</summary>
Exploiting the computational heterogeneity of mobile devices and edge nodes, mobile edge computation (MEC) provides an efficient approach to achieving real-time applications that are sensitive to information freshness, by offloading tasks from mobile devices to edge nodes. We use the metric Age-of-Information (AoI) to evaluate information freshness. An efficient solution to minimize the AoI for the MEC system with multiple users is non-trivial to obtain due to the random computing time. In this paper, we consider multiple users offloading tasks to heterogeneous edge servers in a MEC system. We first reformulate the problem as a Restless Multi-Arm-Bandit (RMAB) problem and establish a hierarchical Markov Decision Process (MDP) to characterize the updating of AoI for the MEC system. Based on the hierarchical MDP, we propose a nested index framework and design a nested index policy with provably asymptotic optimality. Finally, the closed form of the nested index is obtained, which enables the performance tradeoffs between computation complexity and accuracy. Our algorithm leads to an optimality gap reduction of up to 40%, compared to benchmarks. Our algorithm asymptotically approximates the lower bound as the system scalar gets large enough.
</details>
<details>
<summary>摘要</summary>
使用移动设备和边缘节点的计算多样性，移动边计算（MEC）提供了一种高效的方法来实现快速应用程序，这些应用程序需要快速获取新信息。我们使用年龄信息新鲜度（AoI）度量来评估信息新鲜度。由于移动设备到边缘节点的任务下载是随机的，因此设计一个高效的解决方案来减少AoI的差值是非常困难的。在这篇论文中，我们考虑了多个用户将任务下载到不同类型的边缘服务器。我们首先将问题转化为一个闹鼓多臂投资（RMAB）问题，然后建立了一个层次的Markov决策过程（MDP）来描述MEC系统中AoI的更新。基于层次MDP，我们提出了一个嵌入式索引框架，并设计了一个嵌入式索引策略，其可以证明是 asymptotic 优化的。最后，我们计算出嵌入式索引的闭合形式，这使得我们可以实现计算复杂度和准确性之间的性能质量负荷。我们的算法可以减少优化缺陷至多40%，相比 benchmark。我们的算法可以在系统缩放比例很大时，近似于下界。
</details></li>
</ul>
<hr>
<h2 id="Towards-Safe-Autonomous-Driving-Policies-using-a-Neuro-Symbolic-Deep-Reinforcement-Learning-Approach"><a href="#Towards-Safe-Autonomous-Driving-Policies-using-a-Neuro-Symbolic-Deep-Reinforcement-Learning-Approach" class="headerlink" title="Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach"></a>Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01316">http://arxiv.org/abs/2307.01316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cav-research-lab/safe-reinforcement-learning-using-symbolic-logical-programming-for-autonomous-highway-driving">https://github.com/cav-research-lab/safe-reinforcement-learning-using-symbolic-logical-programming-for-autonomous-highway-driving</a></li>
<li>paper_authors: Iman Sharifi, Mustafa Yildirim, Saber Fallah</li>
<li>for: 本研究旨在开发一种能够在真实环境中学习自动驾驶策略，并确保安全性的神经 символи学学习方法（DRLSL）。</li>
<li>methods: 本研究使用了神经网络和 симвоlic first-order 逻辑组合，以学习自动驾驶策略。</li>
<li>results: 实验结果表明，DRLSL 方法可以在真实环境中学习自动驾驶策略，并且在训练和测试阶段都可以避免不安全的行为。此外，DRLSL 方法在新的驾驶场景中表现更好，比传统 DRL 方法更快速地训练并且更好地泛化。<details>
<summary>Abstract</summary>
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics (knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD dataset and demonstrated that our method successfully avoids unsafe actions during both the training and testing phases. Furthermore, our results indicate that DRLSL achieves faster convergence during training and exhibits better generalizability to new driving scenarios compared to traditional DRL methods.
</details>
<details>
<summary>摘要</summary>
Autonomous driving 环境的动态性和多样化的道路用户带来了决策的挑战。深度让资料学习（DRL）已经成为解决这个问题的流行方法。然而，现有的 DRL 解决方案主要在虚拟环境中应用，这限制了它们在实际环境中的部署。为了解决这个问题，这篇文章介绍了一种新的神经符号学习（DRLSL）方法，它结合了 DRL 的学习从经验和符号逻辑的知识驱动理解来实现在实时交互中的自动驾驶策略学习。这种创新的方法可以在实际环境中学习自动驾驶策略，同时保证安全。我们在高D数据集上实现了 DRLSL 框架，并在训练和测试阶段都避免了不安全的行为。此外，我们的结果表明，DRLSL 在训练阶段更快地 converges 和在新驾驶场景中更好地泛化。
</details></li>
</ul>
<hr>
<h2 id="Self-Tuning-PID-Control-via-a-Hybrid-Actor-Critic-Based-Neural-Structure-for-Quadcopter-Control"><a href="#Self-Tuning-PID-Control-via-a-Hybrid-Actor-Critic-Based-Neural-Structure-for-Quadcopter-Control" class="headerlink" title="Self-Tuning PID Control via a Hybrid Actor-Critic-Based Neural Structure for Quadcopter Control"></a>Self-Tuning PID Control via a Hybrid Actor-Critic-Based Neural Structure for Quadcopter Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01312">http://arxiv.org/abs/2307.01312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Sharifi, Aria Alasty<br>for: 这个研究旨在实时自适应PID控制器的设计和训练，以提高Quadrotor的态度和高度控制稳定性。methods: 这个研究使用了一个基于强迫学习的神经网络，用于自适应PID控制器的训练和决策。具体来说，这个神经网络包含了两个隐藏层和sigmoid滤波函数，并使用了Adaptive Momentum（ADAM）优化器和Back-Propagation（BP）算法进行学习。results: 研究结果显示，提案的方法在面对不确定模型参数和外部干扰时能够更加稳定和有效，并且在训练和决策过程中能够快速地适应环境变化。此外，这个方法也比常规PID控制器 WITH CONSTANT GAINS更好地表现，具体来说，它能够更好地应对Quadrotor的态度和高度控制需求。<details>
<summary>Abstract</summary>
Proportional-Integrator-Derivative (PID) controller is used in a wide range of industrial and experimental processes. There are a couple of offline methods for tuning PID gains. However, due to the uncertainty of model parameters and external disturbances, real systems such as Quadrotors need more robust and reliable PID controllers. In this research, a self-tuning PID controller using a Reinforcement-Learning-based Neural Network for attitude and altitude control of a Quadrotor has been investigated. An Incremental PID, which contains static and dynamic gains, has been considered and only the variable gains have been tuned. To tune dynamic gains, a model-free actor-critic-based hybrid neural structure was used that was able to properly tune PID gains, and also has done the best as an identifier. In both tunning and identification tasks, a Neural Network with two hidden layers and sigmoid activation functions has been learned using Adaptive Momentum (ADAM) optimizer and Back-Propagation (BP) algorithm. This method is online, able to tackle disturbance, and fast in training. In addition to robustness to mass uncertainty and wind gust disturbance, results showed that the proposed method had a better performance when compared to a PID controller with constant gains.
</details>
<details>
<summary>摘要</summary>
提出了一种基于循环优化器的自适应PID控制器，用于飞行器的拜投和高度控制。在实验中，通过使用一个基于循环优化器的神经网络来自适应PID控制器的变量参数，并且通过使用一个模型自由的actor-critic型神经网络来调整动态参数。在训练过程中，使用了适应动量优化器和反射传播算法来学习神经网络。这种方法在线、能够抗扰动、快速训练，并且在飞行器的拜投和高度控制中表现更好。此外，这种方法还能够抗质量不确定和风 Gust扰动的影响。Here is the word-for-word translation of the given text into Simplified Chinese:提出了一种基于循环优化器的自适应PID控制器，用于飞行器的拜投和高度控制。在实验中，通过使用一个基于循环优化器的神经网络来自适应PID控制器的变量参数，并且通过使用一个模型自由的actor-critic型神经网络来调整动态参数。在训练过程中，使用了适应动量优化器和反射传播算法来学习神经网络。这种方法在线、能够抗扰动、快速训练，并且在飞行器的拜投和高度控制中表现更好。
</details></li>
</ul>
<hr>
<h2 id="Reliable-AI-Does-the-Next-Generation-Require-Quantum-Computing"><a href="#Reliable-AI-Does-the-Next-Generation-Require-Quantum-Computing" class="headerlink" title="Reliable AI: Does the Next Generation Require Quantum Computing?"></a>Reliable AI: Does the Next Generation Require Quantum Computing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01301">http://arxiv.org/abs/2307.01301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aras Bacho, Holger Boche, Gitta Kutyniok</li>
<li>for: The paper explores the question of whether quantum computing is necessary for the next generation of artificial intelligence.</li>
<li>methods: The paper uses various computational models, including digital and analog computing models, to evaluate the limitations of current artificial intelligence systems and the potential benefits of quantum computing.</li>
<li>results: The paper finds that current digital computing models are limited in their ability to solve certain problems, such as optimization and deep learning, and that analog computing models may offer a way to overcome these limitations. However, even when using quantum computing models, some limitations persist.<details>
<summary>Abstract</summary>
In this survey, we aim to explore the fundamental question of whether the next generation of artificial intelligence requires quantum computing. Artificial intelligence is increasingly playing a crucial role in many aspects of our daily lives and is central to the fourth industrial revolution. It is therefore imperative that artificial intelligence is reliable and trustworthy. However, there are still many issues with reliability of artificial intelligence, such as privacy, responsibility, safety, and security, in areas such as autonomous driving, healthcare, robotics, and others. These problems can have various causes, including insufficient data, biases, and robustness problems, as well as fundamental issues such as computability problems on digital hardware. The cause of these computability problems is rooted in the fact that digital hardware is based on the computing model of the Turing machine, which is inherently discrete. Notably, our findings demonstrate that digital hardware is inherently constrained in solving problems about optimization, deep learning, or differential equations. Therefore, these limitations carry substantial implications for the field of artificial intelligence, in particular for machine learning. Furthermore, although it is well known that the quantum computer shows a quantum advantage for certain classes of problems, our findings establish that some of these limitations persist when employing quantum computing models based on the quantum circuit or the quantum Turing machine paradigm. In contrast, analog computing models, such as the Blum-Shub-Smale machine, exhibit the potential to surmount these limitations.
</details>
<details>
<summary>摘要</summary>
在这份调查中，我们想要探讨人工智能下一代是否需要量子计算。人工智能在我们日常生活中越来越重要，是四个工业革命中的核心。因此，人工智能的可靠性和信worthiness是非常重要。然而，人工智能的可靠性还存在许多问题，如隐私、责任、安全和安全性，在自动驾驶、医疗、机器人等领域。这些问题的原因可能包括不充分的数据、偏见和稳定性问题，以及基本问题如计算机问题。这些计算机问题的根源在于数字硬件基于图灵机制，这是不可避免的离散的。注意，我们的发现表明，数字硬件在解决优化、深度学习和梯度方程问题方面存在极大的限制。因此，这些限制对人工智能领域，特别是机器学习领域产生了深刻的影响。此外，虽然广泛认为量子计算在某些问题上具有优势，但我们的发现表明，使用量子计算模型 based on quantum circuit或量子图灵机制时，这些限制仍然存在。相比之下，分析计算模型，如Blum-Shub-Smale机器，表现出可以突破这些限制的潜力。
</details></li>
</ul>
<hr>
<h2 id="Pareto-Secure-Machine-Learning-PSML-Fingerprinting-and-Securing-Inference-Serving-Systems"><a href="#Pareto-Secure-Machine-Learning-PSML-Fingerprinting-and-Securing-Inference-Serving-Systems" class="headerlink" title="Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems"></a>Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01292">http://arxiv.org/abs/2307.01292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debopam Sanyal, Jui-Tse Hung, Manav Agrawal, Prahlad Jasti, Shahab Nikkhoo, Somesh Jha, Tianhao Wang, Sibin Mohan, Alexey Tumanov<br>for: 这种论文主要针对于Model-serving系统的安全性问题，具体来说是针对于模型EXTRACTION攻击的Robustness问题。methods: 该论文提出了一种新的查询减少算法，以及一种基于噪声的防御机制来对抗模型EXTRACTION攻击。results: 该论文表明，使用提出的查询减少算法和噪声防御机制可以减少模型EXTRACTION攻击的精度和准确率，同时可以保持系统的性能（goodput）在接受ABLE范围内。<details>
<summary>Abstract</summary>
Model-serving systems have become increasingly popular, especially in real-time web applications. In such systems, users send queries to the server and specify the desired performance metrics (e.g., desired accuracy, latency). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks assume a single model can be repeatedly selected for serving inference requests. Modern inference serving systems break this assumption. Thus, they cannot be directly applied to extract a victim model, as models are hidden behind a layer of abstraction exposed by the serving system. An attacker can no longer identify which model she is interacting with. To this end, we first propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accuracy scores within $1\%$ of the scores obtained when attacking a single, explicitly specified model, as well as up to $14.6\%$ gain in accuracy and up to $7.7\%$ gain in fidelity compared to the naive attack. Second, we counter the proposed attack with a noise-based defense mechanism that thwarts fingerprinting by adding noise to the specified performance metrics. The proposed defense strategy reduces the attack's accuracy and fidelity by up to $9.8\%$ and $4.8\%$, respectively (on medium-sized model extraction). Third, we show that the proposed defense induces a fundamental trade-off between the level of protection and system goodput, achieving configurable and significant victim model extraction protection while maintaining acceptable goodput ($>80\%$). We implement the proposed defense in a real system with plans to open source.
</details>
<details>
<summary>摘要</summary>
现代服务系统中的模型服务系统在实时网络应用中变得越来越流行，用户可以向服务器发送查询，并指定所需的性能指标（例如精度和响应时间）。服务器将维护一个模型集（模型zoo），并根据用户的查询来提供服务。这篇论文检查这些系统的安全性，尤其是对于模型提取攻击的Robustness。现有的黑盒攻击假设可以重复选择服务器提供的模型来进行推理请求。然而，现代推理服务系统破坏了这一假设，因此无法直接应用于提取受害者模型。攻击者无法确定她正在互动的模型。为此，我们首先提出了一种高效的询问 fingerprinting 算法，允许攻击者随时触发所需的模型。我们显示，使用我们的 fingerprinting 算法可以在 $1\%$ 的精度和准确性上达到单个、显式指定的模型攻击的精度和准确性水平，并且可以在 $14.6\%$ 的精度和 $7.7\%$ 的准确性上获得更高的提取精度和准确性。其次，我们采用噪音基的防御机制来抵御 fingerprinting，通过添加噪音到指定的性能指标来推迟攻击。我们的防御策略可以在中等模型提取 task 下 reducethe attack's accuracy和fidelity by up to $9.8\%$和$4.8\%$，respectively。最后，我们表明了我们的防御策略存在一定的质量负担和系统好put的负担，可以实现可配置的和显著的受害者模型提取保护，同时维护 Acceptable 的好put ($>80\%$).我们已经实现了我们的防御策略，计划将其开源。
</details></li>
</ul>
<hr>
<h2 id="Fighting-the-disagreement-in-Explainable-Machine-Learning-with-consensus"><a href="#Fighting-the-disagreement-in-Explainable-Machine-Learning-with-consensus" class="headerlink" title="Fighting the disagreement in Explainable Machine Learning with consensus"></a>Fighting the disagreement in Explainable Machine Learning with consensus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01288">http://arxiv.org/abs/2307.01288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Jesus Banegas-Luna, Carlos Martınez-Cortes, Horacio Perez-Sanchez</li>
<li>for: 了解机器学习模型内部工作方式</li>
<li>methods: 使用解释算法进行模型解释</li>
<li>results: 研究发现提出的函数比其他函数更公正、更一致、更准确地解释了五种机器学习模型。<details>
<summary>Abstract</summary>
Machine learning (ML) models are often valued by the accuracy of their predictions. However, in some areas of science, the inner workings of models are as relevant as their accuracy. To understand how ML models work internally, the use of interpretability algorithms is the preferred option. Unfortunately, despite the diversity of algorithms available, they often disagree in explaining a model, leading to contradictory explanations. To cope with this issue, consensus functions can be applied once the models have been explained. Nevertheless, the problem is not completely solved because the final result will depend on the selected consensus function and other factors. In this paper, six consensus functions have been evaluated for the explanation of five ML models. The models were previously trained on four synthetic datasets whose internal rules were known in advance. The models were then explained with model-agnostic local and global interpretability algorithms. Finally, consensus was calculated with six different functions, including one developed by the authors. The results demonstrated that the proposed function is fairer than the others and provides more consistent and accurate explanations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Using-BOLD-fMRI-to-Compute-the-Respiration-Volume-per-Time-RTV-and-Respiration-Variation-RV-with-Convolutional-Neural-Networks-CNN-in-the-Human-Connectome-Development-Cohort"><a href="#Using-BOLD-fMRI-to-Compute-the-Respiration-Volume-per-Time-RTV-and-Respiration-Variation-RV-with-Convolutional-Neural-Networks-CNN-in-the-Human-Connectome-Development-Cohort" class="headerlink" title="Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort"></a>Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05426">http://arxiv.org/abs/2307.05426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdoljalil Addeh, Fernando Vega, Rebecca J Williams, Ali Golestani, G. Bruce Pike, M. Ethan MacDonald</li>
<li>for: 降低fMRI研究成本、简化实验设备、减轻参与者负担。</li>
<li>methods: 使用一维度卷积神经网络模型，从休息BOLD信号中捕捉有用的特征，重建实际的呼吸周期和呼吸变化时间序列。</li>
<li>results: CNN模型能够从休息BOLD信号中捕捉有用的特征，重建实际的呼吸周期和呼吸变化时间序列。<details>
<summary>Abstract</summary>
In many fMRI studies, respiratory signals are unavailable or do not have acceptable quality. Consequently, the direct removal of low-frequency respiratory variations from BOLD signals is not possible. This study proposes a one-dimensional CNN model for reconstruction of two respiratory measures, RV and RVT. Results show that a CNN can capture informative features from resting BOLD signals and reconstruct realistic RV and RVT timeseries. It is expected that application of the proposed method will lower the cost of fMRI studies, reduce complexity, and decrease the burden on participants as they will not be required to wear a respiratory bellows.
</details>
<details>
<summary>摘要</summary>
很多fMRI研究中，呼吸信号不可用或质量不符合要求。因此，直接从BOLD信号中除掉低频呼吸变化不能进行。这项研究提议一种一维Convolutional Neural Network（CNN）模型，用于重建两个呼吸指标：RV和RVT。结果表明，CNN可以从休息BOLD信号中捕捉有用的特征，重建实际的RV和RVT时序。预计通过该方法应用，将降低fMRI研究的成本，降低复杂性，并减轻参与者的负担，他们不需要穿着呼吸膜。
</details></li>
</ul>
<hr>
<h2 id="NeuBTF-Neural-fields-for-BTF-encoding-and-transfer"><a href="#NeuBTF-Neural-fields-for-BTF-encoding-and-transfer" class="headerlink" title="NeuBTF: Neural fields for BTF encoding and transfer"></a>NeuBTF: Neural fields for BTF encoding and transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01199">http://arxiv.org/abs/2307.01199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Rodriguez-Pardo, Konstantinos Kazatzis, Jorge Lopez-Moreno, Elena Garces</li>
<li>for: 本研究提出了一种新的神经材料表示法，用于解决神经材料的固定性问题，以提高渲染效果。</li>
<li>methods: 该方法使用导航图像作为输入，用于condition神经BTF的结构特征。然后，神经BTF可以用UVs、摄像机和光照 вектор进行查询。</li>
<li>results: 该方法在多种 sintetic和捕捉的材料上实现了竞争性的压缩率，并且能够学习表示多种光学性质。<details>
<summary>Abstract</summary>
Neural material representations are becoming a popular way to represent materials for rendering. They are more expressive than analytic models and occupy less memory than tabulated BTFs. However, existing neural materials are immutable, meaning that their output for a certain query of UVs, camera, and light vector is fixed once they are trained. While this is practical when there is no need to edit the material, it can become very limiting when the fragment of the material used for training is too small or not tileable, which frequently happens when the material has been captured with a gonioreflectometer. In this paper, we propose a novel neural material representation which jointly tackles the problems of BTF compression, tiling, and extrapolation. At test time, our method uses a guidance image as input to condition the neural BTF to the structural features of this input image. Then, the neural BTF can be queried as a regular BTF using UVs, camera, and light vectors. Every component in our framework is purposefully designed to maximize BTF encoding quality at minimal parameter count and computational complexity, achieving competitive compression rates compared with previous work. We demonstrate the results of our method on a variety of synthetic and captured materials, showing its generality and capacity to learn to represent many optical properties.
</details>
<details>
<summary>摘要</summary>
神经材料表示法正在成为渲染中的受欢迎方法。它们比分析模型更加表达力，占用内存更少，但现有的神经材料都是不可变的，意味着它们在训练后的输出将 forever fixed。这在没有需要修改材料时是有用的，但在材料的 Fragment 太小或不可平铺时会变得非常局限。在这篇论文中，我们提出了一种新的神经材料表示法，该法同时解决了 BTF 压缩、瓦片和推理问题。在测试时，我们使用引导图像作为输入，通过神经 BTF 根据 UV、摄像头和光照向量进行访问。每个组件在我们的框架中都是为 maximize BTF 编码质量而设计，同时保持最低的参数计数和计算复杂度，与前一个工作相比，我们的压缩率具有竞争力。我们在多种 sintetic 和 captured 材料上展示了我们的方法的通用性和能力学习表示多种光学性质。
</details></li>
</ul>
<hr>
<h2 id="Squeezing-Large-Scale-Diffusion-Models-for-Mobile"><a href="#Squeezing-Large-Scale-Diffusion-Models-for-Mobile" class="headerlink" title="Squeezing Large-Scale Diffusion Models for Mobile"></a>Squeezing Large-Scale Diffusion Models for Mobile</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01193">http://arxiv.org/abs/2307.01193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiwoong Choi, Minkyu Kim, Daehyun Ahn, Taesu Kim, Yulhwa Kim, Dongwon Jo, Hyesung Jeon, Jae-Joon Kim, Hyungjun Kim</li>
<li>for: 本文旨在探讨如何将稳定扩散模型（Stable Diffusion）部署到移动设备上，以便实现高精度图像生成。</li>
<li>methods: 本文使用了TensorFlow Lite框架来部署稳定扩散模型到移动设备上，并解决了由限制计算资源和存储空间所带来的问题。</li>
<li>results: 研究人员通过实现Mobile Stable Diffusion来降低了512x512像素图像生成的推理时间至少于7秒，在Android设备上使用移动GPU进行推理。<details>
<summary>Abstract</summary>
The emergence of diffusion models has greatly broadened the scope of high-fidelity image synthesis, resulting in notable advancements in both practical implementation and academic research. With the active adoption of the model in various real-world applications, the need for on-device deployment has grown considerably. However, deploying large diffusion models such as Stable Diffusion with more than one billion parameters to mobile devices poses distinctive challenges due to the limited computational and memory resources, which may vary according to the device. In this paper, we present the challenges and solutions for deploying Stable Diffusion on mobile devices with TensorFlow Lite framework, which supports both iOS and Android devices. The resulting Mobile Stable Diffusion achieves the inference latency of smaller than 7 seconds for a 512x512 image generation on Android devices with mobile GPUs.
</details>
<details>
<summary>摘要</summary>
diffusion 模型的出现对高精度图像生成带来了广泛的应用场景，从实际应用到学术研究都有了很大的进步。随着模型在各种实际应用中的推广，需要将模型部署到移动设备上的需求增长了。然而，将大 diffusion 模型（如稳定扩散），它有超过一十亿参数，部署到移动设备上具有限制的计算资源和内存资源的问题。在这篇文章中，我们介绍了将 Stable Diffusion 部署到移动设备上的挑战和解决方案，使用 TensorFlow Lite 框架支持 iOS 和 Android 设备。Mobile Stable Diffusion 实现了对 Android 设备上的移动 GPU 进行512x512像素图像生成的评估时间小于 7 秒。
</details></li>
</ul>
<hr>
<h2 id="SAMAug-Point-Prompt-Augmentation-for-Segment-Anything-Model"><a href="#SAMAug-Point-Prompt-Augmentation-for-Segment-Anything-Model" class="headerlink" title="SAMAug: Point Prompt Augmentation for Segment Anything Model"></a>SAMAug: Point Prompt Augmentation for Segment Anything Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01187">http://arxiv.org/abs/2307.01187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haixing Dai, Chong Ma, Zhengliang Liu, Yiwei Li, Peng Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Dajiang Zhu, Wei Liu, Quanzheng Li, Tianming Liu, Xiang Li</li>
<li>for: 这篇论文是为了提高交互式图像分割模型（SAM）的性能而写的。</li>
<li>methods: 这篇论文提出了一种新的视觉点增强方法，称为SAMAug，用于增强SAM的分割性能。SAMAug生成了增强点提示，以提供更多的信息给SAM。</li>
<li>results: 经过测试COCO、Fundus和Chest X-ray等数据集，研究发现SAMAug可以提高SAM的分割性能，尤其是使用最大差Entropy和Saliency模型方法时。这种方法表明了视觉提示工程可以推动交互计算机视觉模型的进步。<details>
<summary>Abstract</summary>
This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information to SAM. From the initial point prompt, SAM produces the initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We evaluate four point augmentation techniques: random selection, maximum difference entropy, maximum distance, and a saliency model. Experiments on the COCO, Fundus, and Chest X-ray datasets demonstrate that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency model methods. SAMAug underscores the potential of visual prompt engineering to advance interactive computer vision models.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了SAMAug，一种新的视觉点增强方法，用于提高Segment Anything Model（SAM）的交互图像分割性能。SAMAug生成了增强后的点提示，以提供更多信息给SAM。从初始点提示开始，SAM生成了初始面积，然后我们提posed SAMAug将这些增强后的点提示与初始提示相结合，以生成增强后的分割面积。通过这种方式，SAM可以基于增强后的点提示和初始提示来生成更好的分割结果。我们评估了四种点增强技术：随机选择、最大差异熵、最大距离和聚光模型。在COCO、Fundus和Chest X-ray数据集上进行了实验，结果表明，使用最大距离和聚光模型方法时，SAMAug可以提高SAM的分割结果，特别是在远程图像分割任务上。SAMAug还证明了视觉提示工程的潜力，可以推动交互计算机视觉模型的进步。
</details></li>
</ul>
<hr>
<h2 id="PlanE-Representation-Learning-over-Planar-Graphs"><a href="#PlanE-Representation-Learning-over-Planar-Graphs" class="headerlink" title="PlanE: Representation Learning over Planar Graphs"></a>PlanE: Representation Learning over Planar Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01180">http://arxiv.org/abs/2307.01180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zzysonny/plane">https://github.com/zzysonny/plane</a></li>
<li>paper_authors: Radoslav Dimitrov, Zeyang Zhao, Ralph Abboud, İsmail İlkan Ceylan</li>
<li>for: 本研究的目的是设计一个可以实现完整Graph Isomorphism的planar graphs Representation Learning架构。</li>
<li>methods: 本研究使用了PlanE框架，其中包括一些可以学习完整Graph Isomorphism的planar graphs Representation Learning架构。</li>
<li>results: 实验结果表明，PlanE框架可以实现高效地learning complete invariants over planar graphs，并在well-known planar graph benchmarks上achieve multiple state-of-the-art results。<details>
<summary>Abstract</summary>
Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations graph invariants. On the other hand, it is well-known that graph invariants learned by these class of models are incomplete: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and Tarjan, we propose PlanE as a framework for planar representation learning. PlanE includes architectures which can learn complete invariants over planar graphs while remaining practically scalable. We empirically validate the strong performance of the resulting model architectures on well-known planar graph benchmarks, achieving multiple state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
граф neural networks是输入图形的表示学习模型的主要选择，其中的思想是通过一系列变换来计算输入图形中节点的表示，以确保学习的图函数是isoomorfism不变的，这使得学习的表示是图 invariants。然而，这类模型学习的图 invariants是不完整的：存在非isoomorfism的图对的标准图 neural networks无法分辨。这并不奇怪，因为普通图 isomorphism testing 是NP完备问题，但是在特殊的图类中，有高效的图 isomorphism testing 算法，如平面图。本文的目标是设计能够有效地学习平面图的完整 invariants 的架构。我们提出 PlanE 框架，它包括可以学习平面图中的完整 invariants 的建筑。我们验证了 PlanE 的实验性成果，在一些知名的平面图 bencmarks 上达到了多个 state-of-the-art 结果。
</details></li>
</ul>
<hr>
<h2 id="Don’t-freeze-Finetune-encoders-for-better-Self-Supervised-HAR"><a href="#Don’t-freeze-Finetune-encoders-for-better-Self-Supervised-HAR" class="headerlink" title="Don’t freeze: Finetune encoders for better Self-Supervised HAR"></a>Don’t freeze: Finetune encoders for better Self-Supervised HAR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01168">http://arxiv.org/abs/2307.01168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vitor Fortes Rey, Dominique Nshimyimana, Paul Lukowicz</li>
<li>for:  solves the labelled data availability problem in human activity recognition</li>
<li>methods:  uses pretext tasks such as reconstruction or contrastive predictive coding to learn useful representations</li>
<li>results:  substantial performance gains across pretext tasks, with the improvement inversely proportional to the amount of labelled data.<details>
<summary>Abstract</summary>
Recently self-supervised learning has been proposed in the field of human activity recognition as a solution to the labelled data availability problem. The idea being that by using pretext tasks such as reconstruction or contrastive predictive coding, useful representations can be learned that then can be used for classification. Those approaches follow the pretrain, freeze and fine-tune procedure. In this paper we will show how a simple change - not freezing the representation - leads to substantial performance gains across pretext tasks. The improvement was found in all four investigated datasets and across all four pretext tasks and is inversely proportional to amount of labelled data. Moreover the effect is present whether the pretext task is carried on the Capture24 dataset or directly in unlabelled data of the target dataset.
</details>
<details>
<summary>摘要</summary>
近些时候，自我指导学习在人体活动识别领域被提出，作为标签数据可用性问题的解决方案。这种方法通过使用预测任务，如重建或对比预测编码，来学习有用的表示。这些方法遵循“预训练、冻结并微调”的过程。在这篇论文中，我们将展示一种简单的变化，即不冻结表示，导致了重大的性能提升，并在所有四个调查dataset和所有四个预测任务中都有效。此外，这种效果是无论预测任务是在Capture24 dataset上进行还是直接在无标签数据中进行的。
</details></li>
</ul>
<hr>
<h2 id="Human-in-the-AI-loop-via-xAI-and-Active-Learning-for-Visual-Inspection"><a href="#Human-in-the-AI-loop-via-xAI-and-Active-Learning-for-Visual-Inspection" class="headerlink" title="Human in the AI loop via xAI and Active Learning for Visual Inspection"></a>Human in the AI loop via xAI and Active Learning for Visual Inspection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05508">http://arxiv.org/abs/2307.05508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jože M. Rožanec, Elias Montini, Vincenzo Cutrona, Dimitrios Papamartzivanos, Timotej Klemenčič, Blaž Fortuna, Dunja Mladenić, Entso Veliou, Thanassis Giannetsos, Christos Emmanouilidis</li>
<li>for: 这篇论文主要是关于工业革命的影响和人机合作的研究。</li>
<li>methods: 论文使用了活动学习和可解释人工智能等两个人工智能子领域，以实现人机合作。</li>
<li>results: 论文提出了人机合作在视觉检测方面的可能性，并在欧盟H2020星计划中获得了一些关于视觉检测的结果，包括人工智能、人数字双生和网络安全等方面的研究。<details>
<summary>Abstract</summary>
Industrial revolutions have historically disrupted manufacturing by introducing automation into production. Increasing automation reshapes the role of the human worker. Advances in robotics and artificial intelligence open new frontiers of human-machine collaboration. Such collaboration can be realized considering two sub-fields of artificial intelligence: active learning and explainable artificial intelligence. Active learning aims to devise strategies that help obtain data that allows machine learning algorithms to learn better. On the other hand, explainable artificial intelligence aims to make the machine learning models intelligible to the human person. The present work first describes Industry 5.0, human-machine collaboration, and state-of-the-art regarding quality inspection, emphasizing visual inspection. Then it outlines how human-machine collaboration could be realized and enhanced in visual inspection. Finally, some of the results obtained in the EU H2020 STAR project regarding visual inspection are shared, considering artificial intelligence, human digital twins, and cybersecurity.
</details>
<details>
<summary>摘要</summary>
工业革命历史上都会对制造进行重大的变革，通过引入自动化技术来提高生产效率。随着机器人和人工智能的发展，人工与机器之间的合作被打开了新的前ier。这种合作可以通过两个人工智能的子领域来实现：活动学习和可 explainable artificial intelligence。活动学习的目标是开发出用于帮助机器学习算法学习的策略，而可 explainable artificial intelligence的目标是使机器学习模型对人类更加明了。本文首先描述了第五代工业革命（Industry 5.0）、人机合作和当前在质量检查方面的状况，特别是视觉检查。然后，它详细介绍了如何通过人机合作来增强视觉检查。最后，本文分享了在欧盟H2020 STAR项目中关于视觉检查的一些结果，包括人工智能、人数字双和安全性。
</details></li>
</ul>
<hr>
<h2 id="Soft-Gripping-Specifying-for-Trustworthiness"><a href="#Soft-Gripping-Specifying-for-Trustworthiness" class="headerlink" title="Soft Gripping: Specifying for Trustworthiness"></a>Soft Gripping: Specifying for Trustworthiness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01159">http://arxiv.org/abs/2307.01159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhaminda B. Abeywickrama, Nguyen Hao Le, Greg Chance, Peter D. Winter, Arianna Manzini, Alix J. Partridge, Jonathan Ives, John Downer, Graham Deacon, Jonathan Rossiter, Kerstin Eder, Shane Windsor</li>
<li>for: 这篇论文主要用于推动软体机器人技术的广泛应用，提高软体机器人的可靠性和信任性。</li>
<li>methods: 该论文提出了对软体机器人系统的规范化需求，包括功能性和非功能性需求，如可靠性、安全性、适应性、预测性、伦理和法规要求。</li>
<li>results: 该论文提出了一个广泛的软体机器人抓取器的规范，用于快递卸载各种商品。该规范覆盖了软体机器人抓取器的功能和非功能需求，以提高软体机器人的可靠性和信任性。<details>
<summary>Abstract</summary>
Soft robotics is an emerging technology in which engineers create flexible devices for use in a variety of applications. In order to advance the wide adoption of soft robots, ensuring their trustworthiness is essential; if soft robots are not trusted, they will not be used to their full potential. In order to demonstrate trustworthiness, a specification needs to be formulated to define what is trustworthy. However, even for soft robotic grippers, which is one of the most mature areas in soft robotics, the soft robotics community has so far given very little attention to formulating specifications. In this work, we discuss the importance of developing specifications during development of soft robotic systems, and present an extensive example specification for a soft gripper for pick-and-place tasks for grocery items. The proposed specification covers both functional and non-functional requirements, such as reliability, safety, adaptability, predictability, ethics, and regulations. We also highlight the need to promote verifiability as a first-class objective in the design of a soft gripper.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Theory-of-Mind-as-Intrinsic-Motivation-for-Multi-Agent-Reinforcement-Learning"><a href="#Theory-of-Mind-as-Intrinsic-Motivation-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning"></a>Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01158">http://arxiv.org/abs/2307.01158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ini Oguntola, Joseph Campbell, Simon Stepputtis, Katia Sycara</li>
<li>for: 提高人工智能在多代理器中的社会智能能力，使其能够模拟人类的心理状态。</li>
<li>methods: 使用深度网络模型精准地表示政策，并将含义深刻的信仰ground在政策中。</li>
<li>results: 在混合合作竞争环境中实现了初步的实验成果。<details>
<summary>Abstract</summary>
The ability to model the mental states of others is crucial to human social intelligence, and can offer similar benefits to artificial agents with respect to the social dynamics induced in multi-agent settings. We present a method of grounding semantically meaningful, human-interpretable beliefs within policies modeled by deep networks. We then consider the task of 2nd-order belief prediction. We propose that ability of each agent to predict the beliefs of the other agents can be used as an intrinsic reward signal for multi-agent reinforcement learning. Finally, we present preliminary empirical results in a mixed cooperative-competitive environment.
</details>
<details>
<summary>摘要</summary>
人类社交智能中能模拟他人的心理状态是关键，可以为人工智能 Agent 在多个 Agent 设置中带来类似的 beneficial 效果。我们提出了将 semantically meaningful 和 human-interpretable 的 belief 嵌入 deep network 中的策略中。然后，我们考虑了第二阶段的 belief 预测任务。我们认为每个 Agent 可以使用它他人的 belief 预测作为多 Agent 学习 reinforcement 中的潜在奖励信号。最后，我们提供了一些初步的实验结果在混合合作-竞争环境中。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="SCITUNE-Aligning-Large-Language-Models-with-Scientific-Multimodal-Instructions"><a href="#SCITUNE-Aligning-Large-Language-Models-with-Scientific-Multimodal-Instructions" class="headerlink" title="SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions"></a>SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01139">http://arxiv.org/abs/2307.01139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lupantech/ScienceQA">https://github.com/lupantech/ScienceQA</a></li>
<li>paper_authors: Sameera Horawalavithana, Sai Munikoti, Ian Stewart, Henry Kvinge</li>
<li>for: 这个论文主要是为了提高大型语言模型（LLM）与科学领域的整合。</li>
<li>methods: 这篇论文使用了一种名为SciTune的调整框架，以提高LLM的科学多Modal指令遵循能力。</li>
<li>results: 与机器生成数据进行finetuning的模型相比，LLaMA-SciTune在科学问答 benchMark中的表现平均高于人类表现，并在多个子类中也达到了人类水平。<details>
<summary>Abstract</summary>
Instruction finetuning is a popular paradigm to align large language models (LLM) with human intent. Despite its popularity, this idea is less explored in improving the LLMs to align existing foundation models with scientific disciplines, concepts and goals. In this work, we present SciTune as a tuning framework to improve the ability of LLMs to follow scientific multimodal instructions. To test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model LLaMA-SciTune that connects a vision encoder and LLM for science-focused visual and language understanding. In comparison to the models that are finetuned with machine generated data only, LLaMA-SciTune surpasses human performance on average and in many sub-categories on the ScienceQA benchmark.
</details>
<details>
<summary>摘要</summary>
科学定uning是一种流行的思想，用于将大型语言模型（LLM）与人类意图相对应。尽管这个想法在改进现有基础模型方面得到了更多的关注，但是它在科学领域中的应用还很少。在这项工作中，我们提出了SciTune作为一种调整框架，用于改进LLM的遵循科学多模式指令的能力。为测试我们的方法ологи，我们使用了人类生成的科学指令调整数据集，并训练了一个大型多modal模型LLaMA-SciTune，该模型连接了视觉编码器和LLM，用于科学领域中的视觉和语言理解。与由机器生成数据进行Finetuning的模型相比，LLaMA-SciTune在科学问答标准 bencmark上平均和许多子类型方面超越了人类表现。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-In-context-Learning-Ability-of-Large-Language-Model-for-Biomedical-Concept-Linking"><a href="#Exploring-the-In-context-Learning-Ability-of-Large-Language-Model-for-Biomedical-Concept-Linking" class="headerlink" title="Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking"></a>Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01137">http://arxiv.org/abs/2307.01137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyong Wang, Zhenxiang Gao, Rong Xu</li>
<li>for:  This research aims to explore the effectiveness of large language models (LLMs) in biomedical concept mapping, specifically in the task of biomedical concept linking.</li>
<li>methods: The proposed approach uses a two-stage retrieve-and-rank framework that leverages in-context learning (ICL) capabilities of LLMs. The approach first embeds biomedical concepts using language models, and then uses embedding similarity to retrieve the top candidates. The contextual information of these candidates is incorporated into the prompt and processed by a large language model to re-rank the concepts.</li>
<li>results: The approach achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7% in chemical entity normalization, demonstrating competitive performance relative to supervised learning methods. Additionally, it showed a significant improvement (over 20-point absolute increase in F1 score) on an oncology matching dataset.<details>
<summary>Abstract</summary>
The biomedical field relies heavily on concept linking in various areas such as literature mining, graph alignment, information retrieval, question-answering, data, and knowledge integration. Although large language models (LLMs) have made significant strides in many natural language processing tasks, their effectiveness in biomedical concept mapping is yet to be fully explored. This research investigates a method that exploits the in-context learning (ICL) capabilities of large models for biomedical concept linking. The proposed approach adopts a two-stage retrieve-and-rank framework. Initially, biomedical concepts are embedded using language models, and then embedding similarity is utilized to retrieve the top candidates. These candidates' contextual information is subsequently incorporated into the prompt and processed by a large language model to re-rank the concepts. This approach achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7% in chemical entity normalization, exhibiting a competitive performance relative to supervised learning methods. Further, it showed a significant improvement, with an over 20-point absolute increase in F1 score on an oncology matching dataset. Extensive qualitative assessments were conducted, and the benefits and potential shortcomings of using large language models within the biomedical domain were discussed. were discussed.
</details>
<details>
<summary>摘要</summary>
生物医学领域强调概念链接在文献检索、图像对alignment、信息检索、问答系统、数据和知识 интеграции等方面发挥重要作用。虽然大型自然语言处理模型（LLM）在许多自然语言处理任务中取得了 significiant进步，但它们在生物医学概念映射方面的效果尚未得到完全探索。本研究探讨了一种利用大型模型的在场学习（ICL）能力进行生物医学概念链接的方法。该方法采用了两个阶段的 retrieve-and-rank框架。首先，生物医学概念被使用语言模型进行嵌入，然后使用嵌入相似性来 retrieve top candidates。这些候选的上下文信息然后被 incorporated 到提示中，并被一个大型语言模型处理以重新排名概念。该方法在 BC5CDR 疾病实体Normalization 和化学实体Normalization 中实现了 90% 的准确率和 94.7% 的准确率，与超级vised learning方法相当。此外，它在生物医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学�
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-vs-Google-A-Comparative-Study-of-Search-Performance-and-User-Experience"><a href="#ChatGPT-vs-Google-A-Comparative-Study-of-Search-Performance-and-User-Experience" class="headerlink" title="ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience"></a>ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01135">http://arxiv.org/abs/2307.01135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiyun Xu, Yue Feng, Hailiang Chen</li>
<li>for:  investigate the differences in user behavior when employing search engines and chatbot tools for information-seeking tasks</li>
<li>methods: randomized online experiment, ChatGPT-like tool and Google Search-like tool</li>
<li>results: ChatGPT group consistently spends less time on all tasks, ChatGPT levels user search performance across different education levels, perceived information quality and user experience are better with ChatGPT, but may also lead to overreliance and generate or replicate misinformation.Here is the same information in Simplified Chinese text:</li>
<li>for: 研究用户在使用搜索引擎和 чат机器人工具时的行为差异</li>
<li>methods: 随机在线实验，使用ChatGPT类工具和Google搜索类工具</li>
<li>results: ChatGPT组的时间投入都比较短，ChatGPT在不同教育水平上的搜索性能相似，但可能会导致过依赖和生成或复制错误信息。<details>
<summary>Abstract</summary>
The advent of ChatGPT, a large language model-powered chatbot, has prompted questions about its potential implications for traditional search engines. In this study, we investigate the differences in user behavior when employing search engines and chatbot tools for information-seeking tasks. We carry out a randomized online experiment, dividing participants into two groups: one using a ChatGPT-like tool and the other using a Google Search-like tool. Our findings reveal that the ChatGPT group consistently spends less time on all tasks, with no significant difference in overall task performance between the groups. Notably, ChatGPT levels user search performance across different education levels and excels in answering straightforward questions and providing general solutions but falls short in fact-checking tasks. Users perceive ChatGPT's responses as having higher information quality compared to Google Search, despite displaying a similar level of trust in both tools. Furthermore, participants using ChatGPT report significantly better user experiences in terms of usefulness, enjoyment, and satisfaction, while perceived ease of use remains comparable between the two tools. However, ChatGPT may also lead to overreliance and generate or replicate misinformation, yielding inconsistent results. Our study offers valuable insights for search engine management and highlights opportunities for integrating chatbot technologies into search engine designs.
</details>
<details>
<summary>摘要</summary>
《ChatGPT的出现：一项研究 traditional search engines的影响》Introduction:随着ChatGPT的出现，它使得用户对传统搜索引擎的使用方式和功能表现出了不同的需求和预期。本研究旨在探讨用户在使用搜索引擎和ChatGPT工具时的行为差异。我们采用了随机在线实验，将参与者分为两组：一组使用ChatGPT类工具，另一组使用Google搜索类工具。我们的发现表明，ChatGPT组的用户在所有任务上的时间投入相对较少，没有显著差异在总任务表现水平之间。另外，ChatGPT在简单问题和通用解决方案方面表现出色，但在事实核查任务方面表现不佳。用户对ChatGPT的回答评价为高信息质量，尽管显示类似的信任度。此外，使用ChatGPT的参与者对工具的用户体验比使用Google搜索更高，包括有用性、愉悦度和满意度，但易用性认知不异。然而，ChatGPT也可能导致过度依赖和生成或复制错误信息，导致不一致的结果。本研究对搜索引擎管理提供了有价值的发现，同时也探讨了将chatbot技术integrated into search engine designs的可能性。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Iterative-Zero-Shot-LLM-Prompting-for-Knowledge-Graph-Construction"><a href="#Iterative-Zero-Shot-LLM-Prompting-for-Knowledge-Graph-Construction" class="headerlink" title="Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction"></a>Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01128">http://arxiv.org/abs/2307.01128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pompianu, Sandro Gabriele Tiddia</li>
<li>for: 本研究旨在提出一种可扩展和灵活的知识图生成方法，以解决现有知识图生成技术的瓶颈和局限性。</li>
<li>methods: 该方法基于最新的生成大语言模型GPT-3.5，包括迭代提示策略和外部知识无关策略，以解决知识图生成过程中的主要挑战。</li>
<li>results: 实验结果表明，该方法可以有效地生成高质量的知识图，并且可以应对不同的应用场景。<details>
<summary>Abstract</summary>
In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for "guiding" the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.
</details>
<details>
<summary>摘要</summary>
在当今数字化时代，捕捉并有效地表达知识是许多实际场景中的关键。在这个 контексте，知识图表示一种可观之的工具，可以快速地检索和组织大量信息，并将其拼接成可读可写的结构。然而，知识图的生成仍然是一个挑战，经常需要大量的人工劳动和领域专业知识，从而限制了扩展性和灵活性在不同应用领域。这篇论文提出了一种创新的知识图生成方法，利用最新的生成大语言模型，如GPT-3.5，解决了知识图生成中的主要挑战。该方法通过一个包含新的迭代零shot和外部知识无关策略的管道来实现。我们的独特 manifoldapproach可能带来了科学社区的重要收益。具体来说，主要贡献可以概括为：（i）一种创新的迭代Prompt大语言模型中提取 relevante组件的策略；（ii）每个Prompt不需要提供示例，即零shot策略；（iii）可扩展的解决方案，因为采用LLMs可以避免任何外部资源或人类专业知识的需求。为评估我们提出的模型效果，我们在特定领域中进行了实验。我们宣称，我们的提案是一种可扩展和多样化的知识图生成方法，可以应用于不同的和新的上下文。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/04/cs.AI_2023_07_04/" data-id="clpahu6x500043h88bu7b003u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/cs.CL_2023_07_04/" class="article-date">
  <time datetime="2023-07-04T11:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/cs.CL_2023_07_04/">cs.CL - 2023-07-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Knowledge-Aware-Audio-Grounded-Generative-Slot-Filling-for-Limited-Annotated-Data"><a href="#Knowledge-Aware-Audio-Grounded-Generative-Slot-Filling-for-Limited-Annotated-Data" class="headerlink" title="Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data"></a>Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01764">http://arxiv.org/abs/2307.01764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/the-anonymous-bs/espnet">https://github.com/the-anonymous-bs/espnet</a></li>
<li>paper_authors: Guangzhi Sun, Chao Zhang, Ivan Vulić, Paweł Budzianowski, Philip C. Woodland</li>
<li>for: 提高 task-oriented dialogue (ToD) 系统中的 slot-filling 精度和效率，即使具有有限的标注数据。</li>
<li>methods: 提出了一种 Knowledge-Aware Audio-Grounded 框架（KA2G），通过将 text 生成任务和 audio 模式结合起来，实现了数据稀缺下的 slot-filling。KA2G 还使用了可用的外部知识（如预先定义的槽值列表）来进一步提高 slot-filling 的精度。</li>
<li>results: KA2G 在 speech-based single-turn SLURP 数据集和一个商业 ToD 系统中的 multi-turn 数据集上进行了实验，并显示了与先前作品相比，特别是在 few-shot 和 zero-shot 设置下，具有强大和一致的提升。<details>
<summary>Abstract</summary>
Manually annotating fine-grained slot-value labels for task-oriented dialogue (ToD) systems is an expensive and time-consuming endeavour. This motivates research into slot-filling methods that operate with limited amounts of labelled data. Moreover, the majority of current work on ToD is based solely on text as the input modality, neglecting the additional challenges of imperfect automatic speech recognition (ASR) when working with spoken language. In this work, we propose a Knowledge-Aware Audio-Grounded generative slot-filling framework, termed KA2G, that focuses on few-shot and zero-shot slot filling for ToD with speech input. KA2G achieves robust and data-efficient slot filling for speech-based ToD by 1) framing it as a text generation task, 2) grounding text generation additionally in the audio modality, and 3) conditioning on available external knowledge (e.g. a predefined list of possible slot values). We show that combining both modalities within the KA2G framework improves the robustness against ASR errors. Further, the knowledge-aware slot-value generator in KA2G, implemented via a pointer generator mechanism, particularly benefits few-shot and zero-shot learning. Experiments, conducted on the standard speech-based single-turn SLURP dataset and a multi-turn dataset extracted from a commercial ToD system, display strong and consistent gains over prior work, especially in few-shot and zero-shot setups.
</details>
<details>
<summary>摘要</summary>
人工标注细腻槽值标签 для任务对话（ToD）系统是一项昂贵和耗时的努力。这种情况激励了对槽 filling 方法的研究，这些方法可以采用有限量的标注数据。此外，当前大多数 ToD 研究仅基于文本输入模式，忽略了自动语音识别（ASR）的额外挑战。在这种工作中，我们提出了一个知识感知音频根据 generator 框架（KA2G），这个框架专注于几 shot 和零 shot 槽 filling  для speech-based ToD。KA2G 通过以下三种方法实现了稳定和数据效果的槽 filling：1. 将其视为文本生成任务。2. 在音频模式中进一步地固定文本生成。3. 使用可用的外部知识（如预定的槽值列表）进行条件。我们发现，在 KA2G 框架中结合两个模式可以提高对 ASR 错误的强度。此外，KA2G 中的知识感知槽值生成器，通过使用指针生成机制实现，尤其是在几 shot 和零 shot 学习中具有优势。我们在标准的speech-based single-turn SLURP 数据集和一个商业 ToD 系统提取的多turn 数据集上进行了实验，并表现出了强大和一致的提升，特别是在几 shot 和零 shot 设置下。
</details></li>
</ul>
<hr>
<h2 id="Align-With-Purpose-Optimize-Desired-Properties-in-CTC-Models-with-a-General-Plug-and-Play-Framework"><a href="#Align-With-Purpose-Optimize-Desired-Properties-in-CTC-Models-with-a-General-Plug-and-Play-Framework" class="headerlink" title="Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework"></a>Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01715">http://arxiv.org/abs/2307.01715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eliya Segev, Maya Alroy, Ronen Katsir, Noam Wies, Ayana Shenhav, Yael Ben-Oren, David Zar, Oren Tadmor, Jacob Bitterman, Amnon Shashua, Tal Rosenwein</li>
<li>for: 提高 Automatic Speech Recognition (ASR) 模型的某些特性，如采样时间和单词错误率 (WER)。</li>
<li>methods: 提出了一种通用的 Plug-and-Play 框架，可以补充 CTCP 损失函数，以便根据某些愿望的属性进行优化。</li>
<li>results: 在 ASR 领域中应用该框架，可以提高 emission time 的优化效果，最多提高 570ms，同时只有较少影响 WER 的准确率。此外，还可以提高 WER 的准确率，相比基eline模型，提高了4.5%。<details>
<summary>Abstract</summary>
Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect alignments. We apply our framework in the domain of Automatic Speech Recognition (ASR) and show its generality in terms of property selection, architectural choice, and scale of training dataset (up to 280,000 hours). To demonstrate the effectiveness of our framework, we apply it to two unrelated properties: emission time and word error rate (WER). For the former, we report an improvement of up to 570ms in latency optimization with a minor reduction in WER, and for the latter, we report a relative improvement of 4.5% WER over the baseline models. To the best of our knowledge, these applications have never been demonstrated to work on a scale of data as large as ours. Notably, our method can be implemented using only a few lines of code, and can be extended to other alignment-free loss functions and to domains other than ASR.
</details>
<details>
<summary>摘要</summary>
Connectionist Temporal Classification (CTC) 是一种广泛使用的训练监督序列到序列（seq2seq）模型的评价标准。它使得学习输入和输出序列之间的关系，称为对齐，通过排除完美对齐（导致真实的输出）的权重，以换取不完美对齐。这种二分法对于真实世界应用中的其他重要对齐特性不足。我们提出了《Align With Purpose》，一种通用的插件和替换框架，可以增强模型在 CTc 评价标准下的愿望特性。我们通过补充 CTc loss函数中的一个额外损失项来实现这一点，该项将对齐按照愿望特性进行优先级排序。我们的方法不需要对 CTc loss函数进行任何改变，可以轻松地优化多种特性，并允许对不完美对齐进行区分。我们在自动语音识别（ASR）领域应用了我们的框架，并在不同的特性、模型选择和训练数据集大小（最大达280,000小时）上进行了通用性测试。为证明我们的框架的有效性，我们在两种不相关的特性上应用了它：发射时间和单词错误率（WER）。对于前者，我们report了最多570ms的延迟优化和一定的WER降低，对于后者，我们report了相对于基eline模型的4.5% WER提升。到目前为止，这些应用都没有在这样大的数据集上进行过。值得注意的是，我们的方法只需要几行代码实现，并且可以扩展到其他对齐无法损失函数和领域。
</details></li>
</ul>
<hr>
<h2 id="Dipping-PLMs-Sauce-Bridging-Structure-and-Text-for-Effective-Knowledge-Graph-Completion-via-Conditional-Soft-Prompting"><a href="#Dipping-PLMs-Sauce-Bridging-Structure-and-Text-for-Effective-Knowledge-Graph-Completion-via-Conditional-Soft-Prompting" class="headerlink" title="Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting"></a>Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01709">http://arxiv.org/abs/2307.01709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenchens190009/csprom-kg">https://github.com/chenchens190009/csprom-kg</a></li>
<li>paper_authors: Chen Chen, Yufei Wang, Aixin Sun, Bing Li, Kwok-Yan Lam</li>
<li>for: 本研究的目的是提高知识图谱完成（KGC） task 的效果，通过维护知识图谱的结构信息和文本信息之间的平衡。</li>
<li>methods: 本研究提出了一种名为 CSProm-KG（Conditional Soft Prompts for KGC）的方法，它只是根据实体和关系表示生成的条件软提示参数进行调整。</li>
<li>results: 对三个常见的静态 KGC 测试集 WN18RR、FB15K-237 和 Wikidata5M 以及两个时间 KGC 测试集 ICEWS14 和 ICEWS05-15 进行测试，CSProm-KG 表现出色，超越了比较基eline模型。我们还进行了进一步的分析，以证明我们的提出的组件的有效性、CSProm-KG 的效率和其可变性。<details>
<summary>Abstract</summary>
Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural information and textual knowledge. CSProm-KG only tunes the parameters of Conditional Soft Prompts that are generated by the entities and relations representations. We verify the effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR, FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new state-of-the-art on these benchmarks. We conduct further analysis to show (i) the effectiveness of our proposed components, (ii) the efficiency of CSProm-KG, and (iii) the flexibility of CSProm-KG.
</details>
<details>
<summary>摘要</summary>
知识图结束 (KGC) 常常需要知识图结构和文本信息同时进行效果。先训练语言模型 (PLMs) 已经被用来学习文本信息，通常在细致调参 paradigm 中进行 KGC 任务。然而，细致调参 PLMs 经常偏重于文本信息，忽略知识图结构。为了解决这个问题，这篇论文提出了 CSProm-KG (Conditional Soft Prompts for KGC)，它保持了知识图结构和文本知识之间的平衡。CSProm-KG 只是调整基于实体和关系表示的 Conditional Soft Prompts 的参数。我们证明了 CSProm-KG 在三个流行的静态 KGC 标准测试集 WN18RR、FB15K-237 和 Wikidata5M 上表现出色，并在两个时间 KGC 标准测试集 ICEWS14 和 ICEWS05-15 上设置新的状态纪录。我们进一步分析表明（i）我们提posed的组件的效果，（ii）CSProm-KG 的效率，以及（iii）CSProm-KG 的灵活性。
</details></li>
</ul>
<hr>
<h2 id="Racial-Bias-Trends-in-the-Text-of-US-Legal-Opinions"><a href="#Racial-Bias-Trends-in-the-Text-of-US-Legal-Opinions" class="headerlink" title="Racial Bias Trends in the Text of US Legal Opinions"></a>Racial Bias Trends in the Text of US Legal Opinions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01693">http://arxiv.org/abs/2307.01693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Jinturkar</li>
<li>for: 这篇论文探讨了美国法律中的种族偏见，具体来说是法官的言论中是否存在种族偏见，以及这种偏见是否随时间和地区而变化。</li>
<li>methods: 作者使用了一种方法来测量大规模文本中的隐性种族偏见，并应用了这种方法来分析600万多个美国联邦和州法院案例文献从1860年到2009年。</li>
<li>results: 研究发现，美国法官的言论中存在强烈的种族偏见，传统的黑人名字更加与“不愉快”的词语相关，而传统的白人名字更加与“愉快”的词语相关。此外，研究还发现，在1950年之前的法律意见中没有发现更高的隐性种族偏见，nor did legal opinions from Northeastern states show greater change in racial bias over time compared to Southern states.<details>
<summary>Abstract</summary>
Although there is widespread recognition of racial bias in US law, it is unclear how such bias appears in the language of law, namely judicial opinions, and whether it varies across time period or region. Building upon approaches for measuring implicit racial bias in large-scale corpora, we approximate GloVe word embeddings for over 6 million US federal and state court cases from 1860 to 2009. We find strong evidence of racial bias across nearly all regions and time periods, as traditionally Black names are more closely associated with pre-classified "unpleasant" terms whereas traditionally White names are more closely associated with pre-classified "pleasant" terms. We also test whether legal opinions before 1950 exhibit more implicit racial bias than those after 1950, as well as whether opinions from Southern states exhibit less change in racial bias than those from Northeastern states. We do not find evidence of elevated bias in legal opinions before 1950, or evidence that legal opinions from Northeastern states show greater change in racial bias over time compared to Southern states. These results motivate further research into institutionalized racial bias.
</details>
<details>
<summary>摘要</summary>
尽管美国法律界存在普遍的种族偏见，但是未知如何在法律语言中表现出这种偏见，以及是否随时间或地区而变化。我们基于大规模文本潜在偏见测量方法，对1860年至2009年美国联邦和州法院案例600万起进行了 aproximate GloVe词嵌入。我们发现在大多数地区和时间期间，传统的黑人名字更加密切相关于预先分类的“不愉快” terms，而传统的白人名字更加密切相关于预先分类的“愉悦” terms。我们还测试了1950年之前的法律意见是否具有更高的潜在种族偏见，以及南部州法律意见是否在时间的推移中改变了种族偏见的变化。我们未能发现1950年之前的法律意见具有偏高的偏见，也未能发现南部州法律意见在时间的推移中改变种族偏见的变化。这些结果激励进一步研究 институциализи了种族偏见。
</details></li>
</ul>
<hr>
<h2 id="Robust-Hate-Speech-Detection-in-Social-Media-A-Cross-Dataset-Empirical-Evaluation"><a href="#Robust-Hate-Speech-Detection-in-Social-Media-A-Cross-Dataset-Empirical-Evaluation" class="headerlink" title="Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation"></a>Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01680">http://arxiv.org/abs/2307.01680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimosthenis Antypas, Jose Camacho-Collados</li>
<li>for: 这篇论文的目的是探讨在自然语言处理（NLP）领域中自动推测仇恨言论的研究。大多数前一 studies 都是基于社交媒体数据集，这些数据集的创建过程中含有自己的偏见，而模型从这些数据集偏见中学习。</li>
<li>methods: 在这篇论文中，我们使用了大规模的训练语言模型，并在不同的仇恨言论检测数据集上进行了精致的调整。我们还进行了许多数据集的比较，以探讨不同数据集在培训 hate speech detection 模型时的可行性。</li>
<li>results: 我们的实验结果显示，不同的数据集在培训 hate speech detection 模型时有所不同的可行性。其中，一些数据集更加普遍，可以在不同的背景下进行应用。此外，我们发现可以通过 комбінуing 不同的数据集来建立更加Robust的 hate speech detection 模型，这个 Robustness 甚至在控制data size 和比较最佳个别数据集时仍然保持。<details>
<summary>Abstract</summary>
The automatic detection of hate speech online is an active research area in NLP. Most of the studies to date are based on social media datasets that contribute to the creation of hate speech detection models trained on them. However, data creation processes contain their own biases, and models inherently learn from these dataset-specific biases. In this paper, we perform a large-scale cross-dataset comparison where we fine-tune language models on different hate speech detection datasets. This analysis shows how some datasets are more generalisable than others when used as training data. Crucially, our experiments show how combining hate speech detection datasets can contribute to the development of robust hate speech detection models. This robustness holds even when controlling by data size and compared with the best individual datasets.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）领域中自动发现仇恨言论在线是一个活跃的研究领域。大多数研究到目前为止都是基于社交媒体数据集，这些数据集在创建仇恨言论检测模型时提供了贡献。然而，数据创建过程中带有自己的偏见，模型从这些数据集特定的偏见中学习。在这篇论文中，我们进行了大规模的跨数据集比较，我们在不同的仇恨言论检测数据集上进行了精细的微调。这一分析表明了某些数据集在用于训练模型时更加通用，而且我们的实验显示，将多个仇恨言论检测数据集组合起来可以帮助建立更加鲁棒的仇恨言论检测模型。这种鲁棒性甚至在控制数据量和相比最佳单个数据集的情况下保持。
</details></li>
</ul>
<hr>
<h2 id="Disentanglement-in-a-GAN-for-Unconditional-Speech-Synthesis"><a href="#Disentanglement-in-a-GAN-for-Unconditional-Speech-Synthesis" class="headerlink" title="Disentanglement in a GAN for Unconditional Speech Synthesis"></a>Disentanglement in a GAN for Unconditional Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01673">http://arxiv.org/abs/2307.01673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rf5/simple-asgan">https://github.com/rf5/simple-asgan</a></li>
<li>paper_authors: Matthew Baas, Herman Kamper</li>
<li>for: 这个研究旨在开发一个可以直接从潜在空间生成真实语音的模型，不需要明确的条件。</li>
<li>methods: 这个模型基于StyleGAN家族的内生对称网络，可以将抽象的噪音映射到一个分离的潜在空间中，然后将这个潜在空间映射到一系列的语音特征，以消除干扰信号扩散。</li>
<li>results: 在使用Google Speech Commands数据集的小词库数据集上，ASGAN已经取得了顶尖的结果，并且比 existing 的扩散模型快得多。此外，我们还证明了 ASGAN 的潜在空间是分离的，可以使用Simple linear operations在这个空间中进行多个未见 durante 训练的任务。<details>
<summary>Abstract</summary>
Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis. It is also substantially faster than existing top-performing diffusion models. We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training. Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks. Code, models, samples: https://github.com/RF5/simple-asgan/
</details>
<details>
<summary>摘要</summary>
可以开发一个模型， direct from a latent space synthesize realistic speech without explicit conditioning?  despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space.  Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis. It is also substantially faster than existing top-performing diffusion models. We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training. Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks.Here's the translation in Traditional Chinese:可以开发一个模型， directly from a latent space synthesize realistic speech without explicit conditioning?  despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space.  Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis. It is also substantially faster than existing top-performing diffusion models. We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training. Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks.
</details></li>
</ul>
<hr>
<h2 id="Boosting-Norwegian-Automatic-Speech-Recognition"><a href="#Boosting-Norwegian-Automatic-Speech-Recognition" class="headerlink" title="Boosting Norwegian Automatic Speech Recognition"></a>Boosting Norwegian Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01672">http://arxiv.org/abs/2307.01672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier de la Rosa, Rolv-Arild Braaten, Per Egil Kummervold, Freddy Wetjen, Svein Arne Brygfjeld</li>
<li>for: 本研究为自动speech recognition（ASR）模型在挪威官方文字两种语言中提供了多个基线。</li>
<li>methods: 本研究使用不同大小和预训练方法的ASR模型在多个挪威语音dataset上进行了比较。同时，我们也测试了这些模型在之前的状态艺术模型和尘泥dataset上的性能。</li>
<li>results: 我们在挪威议会语音 corpus（NPSC）上从单词错误率（WER）17.10%下降至7.60%，模型在挪威语言中获得了5.81%的最佳状态。同时，我们还讨论了进一步改进ASR模型的挑战和解决方案。<details>
<summary>Abstract</summary>
In this paper, we present several baselines for automatic speech recognition (ASR) models for the two official written languages in Norway: Bokm{\aa}l and Nynorsk. We compare the performance of models of varying sizes and pre-training approaches on multiple Norwegian speech datasets. Additionally, we measure the performance of these models against previous state-of-the-art ASR models, as well as on out-of-domain datasets. We improve the state of the art on the Norwegian Parliamentary Speech Corpus (NPSC) from a word error rate (WER) of 17.10\% to 7.60\%, with models achieving 5.81\% for Bokm{\aa}l and 11.54\% for Nynorsk. We also discuss the challenges and potential solutions for further improving ASR models for Norwegian.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了多种基线模型 для自动语音识别（ASR）模型，用于两种官方文字语言在挪威：博克马尔和新北兰语。我们比较了不同大小和预训练方法的模型在多个挪威语音 dataset 上的性能。此外，我们还测试了这些模型与之前的状态对应 ASR 模型和不同语音集上的性能。我们在挪威国会语音集（NPSC）上提高了状态对应率从17.10% 降低至7.60%，其中模型为5.81%  для博克马尔和11.54%  для新北兰语。我们还讨论了进一步改进 ASR 模型的挑战和解决方案。
</details></li>
</ul>
<hr>
<h2 id="Unified-Conversational-Models-with-System-Initiated-Transitions-between-Chit-Chat-and-Task-Oriented-Dialogues"><a href="#Unified-Conversational-Models-with-System-Initiated-Transitions-between-Chit-Chat-and-Task-Oriented-Dialogues" class="headerlink" title="Unified Conversational Models with System-Initiated Transitions between Chit-Chat and Task-Oriented Dialogues"></a>Unified Conversational Models with System-Initiated Transitions between Chit-Chat and Task-Oriented Dialogues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01664">http://arxiv.org/abs/2307.01664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye Liu, Stefan Ultes, Wolfgang Minker, Wolfgang Maier</li>
<li>for: 这个论文的目的是研究在对话模型中实现功能目标和社交对话的联合模型，以及在对话模式之间发生转变时的启发机制。</li>
<li>methods: 这个论文使用了两种类型的对话场景，一种从社交对话逐渐转移到任务 Orientated 请求，另一种从任务 Orientated 交互开始，然后在所有请求信息都提供后转移到社交对话。作者还提出了两种有效的启发模型，一种是一个精确的批量模型，另一种是一个连续的启发模型使用自动生成的推荐embeddings。</li>
<li>results: 研究发现，连续启发模型可以在多个领域任务中实现更高的转换效果，并且可以用于指导对话模型在不同领域之间的批量转换。<details>
<summary>Abstract</summary>
Spoken dialogue systems (SDSs) have been separately developed under two different categories, task-oriented and chit-chat. The former focuses on achieving functional goals and the latter aims at creating engaging social conversations without special goals. Creating a unified conversational model that can engage in both chit-chat and task-oriented dialogue is a promising research topic in recent years. However, the potential ``initiative'' that occurs when there is a change between dialogue modes in one dialogue has rarely been explored. In this work, we investigate two kinds of dialogue scenarios, one starts from chit-chat implicitly involving task-related topics and finally switching to task-oriented requests; the other starts from task-oriented interaction and eventually changes to casual chat after all requested information is provided. We contribute two efficient prompt models which can proactively generate a transition sentence to trigger system-initiated transitions in a unified dialogue model. One is a discrete prompt model trained with two discrete tokens, the other one is a continuous prompt model using continuous prompt embeddings automatically generated by a classifier. We furthermore show that the continuous prompt model can also be used to guide the proactive transitions between particular domains in a multi-domain task-oriented setting.
</details>
<details>
<summary>摘要</summary>
干脆对话系统（SDS）已经分别开发出了两类：任务oriented和聊天。前者关注实现功能目标，而后者想创造有趣的社交对话没有特定目标。在最近几年中，创建一个综合对话模型可以在一个对话中同时进行聊天和任务oriented对话是一个有前途的研究话题。然而，在对话模式之间的变化中可能会发生的“发起”（initiative） rarely been explored。在这项工作中，我们研究了两种对话场景：一个从聊天逐渐涉及到任务相关话题，最后转换到任务oriented请求；另一个从任务oriented交互开始，最后变成了聊天。我们提出了两种高效的提示模型，可以触发系统自主发起对话模式的转换。一个是使用两个简单的Token进行训练的批示模型，另一个是使用自动生成的连续提示嵌入数据来 guideline 系统自主转换的连续提示模型。此外，我们还证明了连续提示模型可以在多个领域任务oriented Setting 中用于指导系统自主转换。
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Thought-Prompting-Elicits-Knowledge-Augmentation"><a href="#Chain-of-Thought-Prompting-Elicits-Knowledge-Augmentation" class="headerlink" title="Chain of Thought Prompting Elicits Knowledge Augmentation"></a>Chain of Thought Prompting Elicits Knowledge Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01640">http://arxiv.org/abs/2307.01640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruckbreasoning/cot-ka">https://github.com/ruckbreasoning/cot-ka</a></li>
<li>paper_authors: Dingjun Wu, Jing Zhang, Xinmei Huang</li>
<li>for: 这篇论文旨在提出一种基于链条思维（Chain-of-Thought，CoT）的知识增强深度学习方法（Knowledge-Augmented Deep Learning，KADL）。</li>
<li>methods: 这种方法使用大语言模型进行广泛预训练，然后将其作为外部知识集成到深度学习模型中。</li>
<li>results: 对于多种逻辑任务的 eleven 个公共数据集上，CoT-KA 方法比纯CoT方法和非增强方法表现出色，得到了更高的性能。<details>
<summary>Abstract</summary>
The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge. In this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments knowledge for deep learning. CoT-KA avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods. Our results demonstrate that CoT-KA outperforms both pure CoT-based methods and the non-augmented method across the majority of eleven publicly available benchmarks for various reasoning tasks.
</details>
<details>
<summary>摘要</summary>
知识增强深度学习方式指的是一种将领域知识集成到深度模型中的方法。传统方法通常采用任务特定的方法来从多种来源中收集外部知识。然而，大型语言模型已经广泛预训练，可以作为外部知识的全面来源。在这篇论文中，我们提出了基于链条思想的CoT-KA方法，用于增强深度学习。CoT-KA不需要额外的知识检索或知识推理模型，与传统增强方法不同。我们的结果表明，CoT-KA在多种公共可用的benchmark上比纯CoT方法和非增强方法表现出色，其中大多数任务的性能都高于非增强方法。
</details></li>
</ul>
<hr>
<h2 id="A-Language-Model-for-Grammatical-Error-Correction-in-L2-Russian"><a href="#A-Language-Model-for-Grammatical-Error-Correction-in-L2-Russian" class="headerlink" title="A Language Model for Grammatical Error Correction in L2 Russian"></a>A Language Model for Grammatical Error Correction in L2 Russian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01609">http://arxiv.org/abs/2307.01609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Remnev, Sergei Obiedkov, Ekaterina Rakhilina, Ivan Smirnov, Anastasia Vyrenkova</li>
<li>for:  correction of non-native (L2) writing errors in Russian language</li>
<li>methods: use of a language model trained on untagged texts of the Newspaper subcorpus of the Russian National Corpus</li>
<li>results: validation of the model’s quality against the RULEC-GEC corpusHere’s the full text in Simplified Chinese:</li>
<li>for:  correction of non-native (L2) 中文写作错误</li>
<li>methods: 使用基于新闻子集的俄语国家 corpus 上的无标文本语言模型</li>
<li>results:  validate 模型质量 against RULEC-GEC corpusI hope that helps!<details>
<summary>Abstract</summary>
Grammatical error correction is one of the fundamental tasks in Natural Language Processing. For the Russian language, most of the spellcheckers available correct typos and other simple errors with high accuracy, but often fail when faced with non-native (L2) writing, since the latter contains errors that are not typical for native speakers. In this paper, we propose a pipeline involving a language model intended for correcting errors in L2 Russian writing. The language model proposed is trained on untagged texts of the Newspaper subcorpus of the Russian National Corpus, and the quality of the model is validated against the RULEC-GEC corpus.
</details>
<details>
<summary>摘要</summary>
grammatical error correction是自然语言处理中的基本任务之一。对于俄语，大多数可用的拼写检查器能够准确地检测 typo和其他简单错误，但在面临非Native（L2）写作时，它们frequently failed，因为L2写作中含有不典型的错误。在这篇论文中，我们提议一个涉及语言模型的管道，用于 correecting L2俄语写作中的错误。我们的语言模型基于俄语日报子集（Newspaper subcorpus）上的未标注文本，并对RULEC-GEC corpus进行验证。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-the-Learning-Bias-towards-Repetition-by-Self-Contrastive-Training-for-Open-Ended-Generation"><a href="#Mitigating-the-Learning-Bias-towards-Repetition-by-Self-Contrastive-Training-for-Open-Ended-Generation" class="headerlink" title="Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation"></a>Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01542">http://arxiv.org/abs/2307.01542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-coai/selfcont">https://github.com/thu-coai/selfcont</a></li>
<li>paper_authors: Jian Guan, Minlie Huang</li>
<li>for: 提高自然语言生成 tasks 中的多样性，尤其是使用 GPT2 预训练语言模型进行开放式生成时，具有重复性的问题。</li>
<li>methods: 我们提出了一种自我对比训练方法，通过对同一模型的 premature checkpoint 的输出进行罚款，以避免重复性的过度估计。</li>
<li>results: 我们在两个 datasets 上进行了实验，发现这种方法可以有效地避免重复性，同时保持流畅性。此外，我们发现Language Models 在预测重复Token时使用更长的词语关系，可能是句子水平重复的原因。<details>
<summary>Abstract</summary>
Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops.
</details>
<details>
<summary>摘要</summary>
尽管在许多生成任务中进步很大，预训练语言模型（LM）如GPT2仍然很容易通过最大化基于解码算法来生成重复的文本。我们认为这是因为学习偏见：LM学习了简单的重复模式更快，使得它们在MLE损失函数下过度估计token级别的重复概率。我们提议使用自我对比训练来追加预训练模型的检查点，并在检查点不正确预测重复时进行惩罚，这有效地减少了重复，同时保持了流畅性在两个数据集上。此外，我们发现LM在预测重复token时使用了更长的距离，这可能是句子水平的重复循环的原因。
</details></li>
</ul>
<hr>
<h2 id="On-Evaluating-and-Mitigating-Gender-Biases-in-Multilingual-Settings"><a href="#On-Evaluating-and-Mitigating-Gender-Biases-in-Multilingual-Settings" class="headerlink" title="On Evaluating and Mitigating Gender Biases in Multilingual Settings"></a>On Evaluating and Mitigating Gender Biases in Multilingual Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01503">http://arxiv.org/abs/2307.01503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Vashishtha, Kabir Ahuja, Sunayana Sitaram</li>
<li>for: 本研究旨在 investigate the challenges of evaluating and mitigating biases in multilingual settings, especially for non-western context.</li>
<li>methods: 本研究使用 human annotations 创建了一个用于评估 gender biases 的指标，并将 existing debiasing methods 扩展到不同的印度语言。</li>
<li>results: 研究发现了 multilingual settings 中 studying social biases 的挑战，并提供了资源和 mitigation techniques 以逐步扩展到更多的语言。<details>
<summary>Abstract</summary>
While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.
</details>
<details>
<summary>摘要</summary>
tradicional，理解并消除语言模型中的性别偏见问题一直是自然语言处理领域的长期问题，但之前的研究主要集中在英语上。在这项工作中，我们探讨了在多语言设置中评估和消除偏见的挑战，以及由于英语以外的语言缺乏现有的偏见评估 benchmark和资源而导致的问题。在这篇论文中，我们首先创建了评估隐藏语言模型中的性别偏见的benchmark，通过对印度语言进行扩展DisCo以获得人工纠正。然后，我们扩展了不同的去偏见方法以工作在英语以外的语言上，并评估这些方法在我们提出的指标上的效果。总之，我们的工作揭示了在多语言设置中研究社会偏见的挑战，并提供了资源以及消除技术，以便扩展到更多的语言。
</details></li>
</ul>
<hr>
<h2 id="SCAT-Robust-Self-supervised-Contrastive-Learning-via-Adversarial-Training-for-Text-Classification"><a href="#SCAT-Robust-Self-supervised-Contrastive-Learning-via-Adversarial-Training-for-Text-Classification" class="headerlink" title="SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification"></a>SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01488">http://arxiv.org/abs/2307.01488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Wu, Dit-Yan Yeung</li>
<li>for: 防御文本攻击，提高自然语言处理（NLP）系统的鲁棒性。</li>
<li>methods: 自我标注对假数据进行随机修饰，并通过对这些修饰和其对应的对抗样本进行对比来实现对抗训练。</li>
<li>results: 可以很好地训练不含标签数据的语言模型，并且可以提高现有预训练语言模型的鲁棒性。<details>
<summary>Abstract</summary>
Despite their promising performance across various natural language processing (NLP) tasks, current NLP systems are vulnerable to textual adversarial attacks. To defend against these attacks, most existing methods apply adversarial training by incorporating adversarial examples. However, these methods have to rely on ground-truth labels to generate adversarial examples, rendering it impractical for large-scale model pre-training which is commonly used nowadays for NLP and many other tasks. In this paper, we propose a novel learning framework called SCAT (Self-supervised Contrastive Learning via Adversarial Training), which can learn robust representations without requiring labeled data. Specifically, SCAT modifies random augmentations of the data in a fully labelfree manner to generate adversarial examples. Adversarial training is achieved by minimizing the contrastive loss between the augmentations and their adversarial counterparts. We evaluate SCAT on two text classification datasets using two state-of-the-art attack schemes proposed recently. Our results show that SCAT can not only train robust language models from scratch, but it can also significantly improve the robustness of existing pre-trained language models. Moreover, to demonstrate its flexibility, we show that SCAT can also be combined with supervised adversarial training to further enhance model robustness.
</details>
<details>
<summary>摘要</summary>
尽管现有的自然语言处理（NLP）系统在不同的任务上表现出色，但它们对文本恶作剂攻击仍然易受到影响。为防止这些攻击，大多数现有的方法采用了对抗训练，但这些方法往往需要使用准确的标签来生成对抗示例，这使得大规模模型预训练成为不可能的。在这篇论文中，我们提出了一种新的学习框架，即SCAT（自主对抗学习），可以不需要标签数据来学习强化表示。具体来说，SCAT通过修改数据的随机扩展来生成对抗示例，然后通过对这些对抗示例和其对抗样本进行对抗训练来减少对抗攻击的影响。我们在两个文本分类任务上使用了两种最新的攻击方案进行评估。我们的结果显示，SCAT不仅可以从零开始训练Robust语言模型，而且还可以显著提高现有预训练语言模型的Robust性。此外，我们还证明了SCAT可以与有监督对抗训练结合使用，以进一步增强模型的Robust性。
</details></li>
</ul>
<hr>
<h2 id="CARE-MI-Chinese-Benchmark-for-Misinformation-Evaluation-in-Maternity-and-Infant-Care"><a href="#CARE-MI-Chinese-Benchmark-for-Misinformation-Evaluation-in-Maternity-and-Infant-Care" class="headerlink" title="CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care"></a>CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01458">http://arxiv.org/abs/2307.01458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meetyou-ai-lab/care-mi">https://github.com/meetyou-ai-lab/care-mi</a></li>
<li>paper_authors: Tong Xiang, Liangzhi Li, Wangyue Li, Mingbai Bai, Lu Wei, Bowen Wang, Noa Garcia<br>for: 这 paper 的目的是evaluating the misinformation generated by large language models (LLMs) in the sensitive topic of maternity and infant care, and providing a benchmark for assessing the quality of long-form generation in Chinese.methods: 该 paper 使用了一个新的 benchmark ， named CARE-MI, to evaluate the misinformation of LLMs in the maternity and infant care domain, and compared potential solutions for long-form generation evaluation.results: 该 paper 发现，current Chinese LLMs 在这个领域 still have a long way to go, and proposed a judgment model for automatically assessing the long-form output of LLMs using the benchmark questions.<details>
<summary>Abstract</summary>
The recent advances in NLP, have led to a new trend of applying LLMs to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building long-form generation evaluation benchmarks that can be transferred to other knowledge-intensive domains and low-resourced languages. Our proposed benchmark fills the gap between the extensive usage of LLMs and the lack of datasets for assessing the misinformation generated by these models. It contains 1,612 expert-checked questions, accompanied with human-selected references. Using our benchmark, we conduct extensive experiments and found that current Chinese LLMs are far from perfect in the topic of maternity and infant care. In an effort to minimize the reliance on human resources for performance evaluation, we offer a judgment model for automatically assessing the long-form output of LLMs using the benchmark questions. Moreover, we compare potential solutions for long-form generation evaluation and provide insights for building more robust and efficient automated metric.
</details>
<details>
<summary>摘要</summary>
近些年，自然语言处理（NLP）的进步，启动了应用大型自然语言模型（LLM）到实际场景的新趋势。latest LLMs在与人类交互时表现出很高的流畅性，但它们受到谎言问题的困扰，即不慎生成的false信息。这可能导致有害的后果，特别是在敏感场景中，如医疗领域。然而，前期工作很少关注了LLMs中的谎言评估，特别是在知识密集的领域和语言中。为了解决这问题，我们提出了一个benchmark，CARE-MI，用于评估LLMs中的谎言。CARE-MI包括以下两个方面：1）敏感领域，即婴儿护理领域；2）语言，即中文。我们还提供了一种创新的评估长形生成 benchmark的方法，可以转移到其他知识密集的领域和低资源语言。我们的提案填补了LLMs的广泛使用和评估谎言生成的数据差距。我们的benchmark包括1,612个专家审核的问题，以及人选的参考文献。使用我们的benchmark，我们进行了广泛的实验，发现当前的中文LLMs在婴儿护理领域还有很大的改进空间。为了减少人工资源的依赖，我们提供了一种自动评估长形输出的模型，以及对不同解决方案的比较。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Retrieval-Augmented-In-Context-Learning-for-Dialogue-State-Tracking"><a href="#Diverse-Retrieval-Augmented-In-Context-Learning-for-Dialogue-State-Tracking" class="headerlink" title="Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking"></a>Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01453">http://arxiv.org/abs/2307.01453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jlab-nlp/refpydst">https://github.com/jlab-nlp/refpydst</a></li>
<li>paper_authors: Brendan King, Jeffrey Flanigan</li>
<li>for: 提高对对话状态跟踪（DST）的表现，特别是在零和几个示例学习环境下。</li>
<li>methods: 提出了三种改进来提高在对话状态跟踪中的受Context learning，包括：将DST视为Python编程任务，明确表示语言核心引用在Python中；选择多个相关示例来提高性能；在解码阶段使用重新权重方法，考虑竞争表达形式的概率，生成更准确的对话状态预测。</li>
<li>results: 使用MultiWOZ进行评估，在零和几个示例学习环境下实现了多个多任务共同目标准确率的状态前几。<details>
<summary>Abstract</summary>
There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting (Hu et al. 2022). We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST. First, we formulate DST as a Python programming task, explicitly modeling language coreference as variable reference in Python. Second, since in-context learning depends highly on the context examples, we propose a method to retrieve a diverse set of relevant examples to improve performance. Finally, we introduce a novel re-weighting method during decoding that takes into account probabilities of competing surface forms, and produces a more accurate dialogue state prediction. We evaluate our approach using MultiWOZ and achieve state-of-the-art multi-domain joint-goal accuracy in zero and few-shot settings.
</details>
<details>
<summary>摘要</summary>
有很多人表达了对零和几个shot学习对话状态追踪（DST）的兴趣，这是因为收集和标注任务型对话的成本很高。现有研究表明，在Context中学习只需要很少数据和零参数更新，甚至在几个shot Setting下超越训练方法的性能（Hu et al. 2022）。我们提出了RefPyDST，这是一种在Context中学习DST的新方法，它具有以下三个进步：1. 我们将DST视为一种Python编程任务，直接在Python中表示语言核心语言引用。2. 由于Context学习强烈取决于上下文示例，我们提议一种方法来检索更多相关的示例，以提高性能。3. 我们提出了一种新的重新权重方法，在解码过程中考虑竞争表面形式的概率，并生成更准确的对话状态预测。我们使用MultiWOZ进行评估，并在零和几个shot Setting下实现了多个领域共同目标准确率的状态前景。
</details></li>
</ul>
<hr>
<h2 id="ReactIE-Enhancing-Chemical-Reaction-Extraction-with-Weak-Supervision"><a href="#ReactIE-Enhancing-Chemical-Reaction-Extraction-with-Weak-Supervision" class="headerlink" title="ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision"></a>ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01448">http://arxiv.org/abs/2307.01448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Zhong, Siru Ouyang, Minhao Jiang, Vivian Hu, Yizhu Jiao, Xuan Wang, Jiawei Han</li>
<li>for: 本研究旨在提供一种用于提取结构化反应信息的方法，以便化学家在实验室工作和计算机辅助药物设计等高级任务中使用。</li>
<li>methods: 本研究使用了两种弱监督方法进行预训练，利用文本中频繁出现的语言特征来识别化学反应的特征。同时，我们采用了专利记录中的 sintetic 数据作为远程监督，以把领域知识integrated到模型中。</li>
<li>results: 实验表明， ReactIE 方法可以达到显著提高，并超过所有基eline。<details>
<summary>Abstract</summary>
Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design. Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive due to the significant labor required from domain experts. Consequently, the scarcity of sufficient training data poses an obstacle to the progress of related models in this domain. In this paper, we propose ReactIE, which combines two weakly supervised approaches for pre-training. Our method utilizes frequent patterns within the text as linguistic cues to identify specific characteristics of chemical reactions. Additionally, we adopt synthetic data from patent records as distant supervision to incorporate domain knowledge into the model. Experiments demonstrate that ReactIE achieves substantial improvements and outperforms all existing baselines.
</details>
<details>
<summary>摘要</summary>
科学文献中的结构化化学反应信息对化学家进行实验室工作和高级尝试（如计算机支持药物设计）起着重要作用。然而，提取结构化反应的数据标注因为需要域专家劳动量大，因此成本高昂。这导致相关模型在这个领域进步受阻。在这篇论文中，我们提议了ReactIE，它组合了两种弱监督方法进行预训练。我们的方法利用文本中的频繁出现的语言特征作为化学反应的特征标志。此外，我们采用了专利记录中的 sintetic data作为远程监督，以把领域知识引入模型中。实验表明，ReactIE可以实现显著改进，并超过所有基elines。
</details></li>
</ul>
<hr>
<h2 id="On-Conditional-and-Compositional-Language-Model-Differentiable-Prompting"><a href="#On-Conditional-and-Compositional-Language-Model-Differentiable-Prompting" class="headerlink" title="On Conditional and Compositional Language Model Differentiable Prompting"></a>On Conditional and Compositional Language Model Differentiable Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01446">http://arxiv.org/abs/2307.01446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jpilaul/PRopS">https://github.com/jpilaul/PRopS</a></li>
<li>paper_authors: Jonathan Pilault, Can Liu, Mohit Bansal, Markus Dreyer</li>
<li>for: 本研究旨在提高预训练语言模型（PLM）的下游任务性能，通过各种提示方法来适应不同任务。</li>
<li>methods: 本研究使用了 conditional和compositional的可微分提示方法，并提出了一种新的模型——Prompt Production System（PRopS），可以将任务说明或输入元数据转化为Continuous提示，以便从PLM中获取特定任务输出。PRopS使用了基于神经网络的Production Systems模型结构，可以学习到特定提示输入模式的精细规则，从而实现compositional transfer learning和少量学习。</li>
<li>results: 对比其他PLM适应技术，PRopS在compositional generalization任务、可控摘要和多语言翻译等任务中具有优异表现，需要更少的可训练参数。<details>
<summary>Abstract</summary>
Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine-tuned models, on compositional generalization tasks, controllable summarization and multilingual translation, while needing fewer trainable parameters.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文本提示（Prompts）已经被证明是一种有效的方法，用于适应预训练语言模型（PLM）来实现下游任务的好准确性。文本提示可以表示为人工设计的单词序列或学习到的连续嵌入。在这项工作中，我们研究了强制和组合的可微分提示。我们提出了一个新的模型，即提示生产系统（PRopS），该模型可以将任务指令或输入元数据转换为可微分的提示，从而使PLM发生任务特定的输出。我们的模型采用基于我们的神经网络表述的生产系统结构，该结构允许模型学习分解规则——神经函数学习特定提示输入模式的转换，使其适用于组合转移学习和少量学习。我们对PRopS进行了广泛的实验和理论分析，并证明了其在组合总结任务、可控概要和多语言翻译方面的表现，常常超过其他PLM适应技术，而且经常超过完全精度地训练的模型。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Modeling-Tag-Prediction-based-on-Question-Tagging-Behavior-Analysis-of-CommunityQA-Platform-Users"><a href="#Modeling-Tag-Prediction-based-on-Question-Tagging-Behavior-Analysis-of-CommunityQA-Platform-Users" class="headerlink" title="Modeling Tag Prediction based on Question Tagging Behavior Analysis of CommunityQA Platform Users"></a>Modeling Tag Prediction based on Question Tagging Behavior Analysis of CommunityQA Platform Users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01420">http://arxiv.org/abs/2307.01420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuntal Kumar Pal, Michael Gamon, Nirupama Chandrasekaran, Silviu Cucerzan</li>
<li>for: 提高社区问答平台信息组织和检索效果，更快准确地回答问题，评估话题Popularity。</li>
<li>methods: 对17个StackExchange社区用户标签行为进行了系统性分析，发现了不同领域的共同特性。采用发现结果开发flexible的神经网络标签预测模型，可预测问题的流行标签和更加细化的标签。</li>
<li>results: 经过广泛的实验和性能评估，证明了模型的有效性。<details>
<summary>Abstract</summary>
In community question-answering platforms, tags play essential roles in effective information organization and retrieval, better question routing, faster response to questions, and assessment of topic popularity. Hence, automatic assistance for predicting and suggesting tags for posts is of high utility to users of such platforms. To develop better tag prediction across diverse communities and domains, we performed a thorough analysis of users' tagging behavior in 17 StackExchange communities. We found various common inherent properties of this behavior in those diverse domains. We used the findings to develop a flexible neural tag prediction architecture, which predicts both popular tags and more granular tags for each question. Our extensive experiments and obtained performance show the effectiveness of our model
</details>
<details>
<summary>摘要</summary>
在社区问答平台上，标签扮演着关键的角色，即信息组织和检索、更好的问题路由、更快的问题回答以及评估话题 популярность。因此，自动为帖子提供标签预测和建议是用户们的高Utility功能。为了在多个社区和领域中提高标签预测，我们进行了17个Stack Exchange社区用户标签行为的严格分析。我们发现了这些多样化领域中标签行为的共同特性。我们使用这些发现来开发一种灵活的神经网络标签预测架构，可以预测每个问题的流行标签以及更加细化的标签。我们的广泛的实验和表现表明我们的模型的效果。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Learning-Improves-Performance-In-Deep-Argument-Mining-Models"><a href="#Multi-Task-Learning-Improves-Performance-In-Deep-Argument-Mining-Models" class="headerlink" title="Multi-Task Learning Improves Performance In Deep Argument Mining Models"></a>Multi-Task Learning Improves Performance In Deep Argument Mining Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01401">http://arxiv.org/abs/2307.01401</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Amirhossein Farzam, Shashank Shekhar, Isaac Mehlhaff, Marco Morucci</li>
<li>for: 本研究的目的是提高对用户生成文本中的论证技巧的分析，以便进行政治和市场分析等下游任务。</li>
<li>methods: 本研究使用了现代深度学习方法，包括多任务学习，以提取和注释用户生成文本中的论证技巧。</li>
<li>results: 研究表明，不同的论证检测任务共享相似的semantic和logical结构，并且可以通过共享表示和 Parametern sharing 来提高性能。<details>
<summary>Abstract</summary>
The successful analysis of argumentative techniques from user-generated text is central to many downstream tasks such as political and market analysis. Recent argument mining tools use state-of-the-art deep learning methods to extract and annotate argumentative techniques from various online text corpora, however each task is treated as separate and different bespoke models are fine-tuned for each dataset. We show that different argument mining tasks share common semantic and logical structure by implementing a multi-task approach to argument mining that achieves better performance than state-of-the-art methods for the same problems. Our model builds a shared representation of the input text that is common to all tasks and exploits similarities between tasks in order to further boost performance via parameter-sharing. Our results are important for argument mining as they show that different tasks share substantial similarities and suggest a holistic approach to the extraction of argumentative techniques from text.
</details>
<details>
<summary>摘要</summary>
成功分析口说技巧是许多下游任务的核心，如政治和市场分析。现有的口说采矿工具使用 cutting-edge 深度学习方法提取和标注口说技巧，但每个任务都是专门训练不同的模型。我们显示出不同的口说采矿任务有共同的semantic和logical结构，通过实现多任务方法来采矿口说，可以更好地提高性能。我们的模型建立了输入文本共同的表示，并利用任务之间的相似性以进一步提高性能。我们的结果对口说采矿有重要意义，表明不同任务之间有许多相似之处，并建议一个整体的方法来从文本中提取口说技巧。
</details></li>
</ul>
<hr>
<h2 id="ALBERTI-a-Multilingual-Domain-Specific-Language-Model-for-Poetry-Analysis"><a href="#ALBERTI-a-Multilingual-Domain-Specific-Language-Model-for-Poetry-Analysis" class="headerlink" title="ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis"></a>ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01387">http://arxiv.org/abs/2307.01387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier de la Rosa, Álvaro Pérez Pozo, Salvador Ros, Elena González-Blanco</li>
<li>for: This paper is written for the analysis of poetry in a multilingual setting, specifically to address the lack of tools for automatically analyzing and scanning poems.</li>
<li>methods: The paper presents a new approach called \textsc{Alberti}, which is a multilingual pre-trained large language model for poetry. The model is trained using domain-specific pre-training (DSP) on a corpus of over 12 million verses from 12 languages.</li>
<li>results: The paper reports that \textsc{Alberti} outperforms multilingual BERT and other transformers-based models of similar sizes on two structural poetry tasks: Spanish stanza type classification and metrical pattern prediction for Spanish, English, and German. Additionally, \textsc{Alberti} achieves state-of-the-art results for German when compared to rule-based systems.<details>
<summary>Abstract</summary>
The computational analysis of poetry is limited by the scarcity of tools to automatically analyze and scan poems. In a multilingual settings, the problem is exacerbated as scansion and rhyme systems only exist for individual languages, making comparative studies very challenging and time consuming. In this work, we present \textsc{Alberti}, the first multilingual pre-trained large language model for poetry. Through domain-specific pre-training (DSP), we further trained multilingual BERT on a corpus of over 12 million verses from 12 languages. We evaluated its performance on two structural poetry tasks: Spanish stanza type classification, and metrical pattern prediction for Spanish, English and German. In both cases, \textsc{Alberti} outperforms multilingual BERT and other transformers-based models of similar sizes, and even achieves state-of-the-art results for German when compared to rule-based systems, demonstrating the feasibility and effectiveness of DSP in the poetry domain.
</details>
<details>
<summary>摘要</summary>
计算 poetry 的分析受到计算 poetry 工具的缺乏的限制。在多语言设置下，问题更加严重，因为押韵和律诗系统只存在于个别语言中，这使得比较研究非常困难和耗时。在这种工作中，我们介绍了 \textsc{Alberti}，首个用于 poetry 的多语言预训练大语言模型。通过领域特定预训练（DSP），我们进一步训练了多语言 BERT 在12种语言的超过12万句诗歌中进行预训练。我们对其表现进行评估，并在西班牙押韵类型分类和德语、英语和西班牙的 мет律 Pattern 预测任务上达到了比较好的结果，并且在对比rule-based系统的 germany 语言中达到了国际一流的结果，这表明了 DSP 在 poetry 领域的可能性和有效性。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Memory-Transformer-for-Computationally-Efficient-Simultaneous-Speech-Translation"><a href="#Implicit-Memory-Transformer-for-Computationally-Efficient-Simultaneous-Speech-Translation" class="headerlink" title="Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation"></a>Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01381">http://arxiv.org/abs/2307.01381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-starlab/implicitmemory">https://github.com/osu-starlab/implicitmemory</a></li>
<li>paper_authors: Matthew Raffel, Lizhong Chen</li>
<li>for: 这个论文的目的是提出一种新的做法来处理同时的语音翻译任务，使得翻译器可以同时接受输入语音序列，并且不需要耗费过多计算资源。</li>
<li>methods: 这个论文使用了一种新的Left Context方法，通过将上一个segment的注意力输出作为下一个segment的左 контекст来实现。这种方法可以减少计算资源的消耗，同时也可以保持模型的准确性。</li>
<li>results: 实验结果表明，使用这种Left Context方法可以在encoder前进行加速，而且与使用左 context和内存银行的方法相比，翻译质量几乎相同。<details>
<summary>Abstract</summary>
Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedup on the encoder forward pass with nearly identical translation quality when compared with the state-of-the-art approach that employs both left context and memory banks.
</details>
<details>
<summary>摘要</summary>
同时语音翻译是一项人类交流困难的沟通任务，即在流动输入语音时生成翻译。为此流处理任务，使用块处理的转换器已经实现了状态体系的最佳性能，并降低计算成本。现有的方法，包括左上下文和内存银行，尝试使信息在段之间传递，但是这些方法都是不充分的表示和过分的计算成本。在这篇论文中，我们提出了隐式记忆转换器，通过新的左上下文方法，消除了需要显式表示内存的需求。我们从上一个段的注意输出中生成左上下文，并将其包含在当前段的注意计算中的键和值中。对于 Must-C 数据集的实验结果表明，隐式记忆转换器在编码前进行速度增加，与使用左上下文和内存银行的状态体系相比，翻译质量几乎完全一致。
</details></li>
</ul>
<hr>
<h2 id="Shiftable-Context-Addressing-Training-Inference-Context-Mismatch-in-Simultaneous-Speech-Translation"><a href="#Shiftable-Context-Addressing-Training-Inference-Context-Mismatch-in-Simultaneous-Speech-Translation" class="headerlink" title="Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation"></a>Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01377">http://arxiv.org/abs/2307.01377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-starlab/shiftablecontext">https://github.com/osu-starlab/shiftablecontext</a></li>
<li>paper_authors: Matthew Raffel, Drew Penney, Lizhong Chen</li>
<li>for: 提高同时翻译的精度，解决 segment-based 处理模型在训练和推理环境中的上下文匹配问题。</li>
<li>methods: 提出了Shiftable Context scheme，通过保证训练和推理环境中segment和上下文大小的一致性，提高同时翻译的精度。</li>
<li>results: 在英语-德语、英语-法语和英语-西班牙语语对上，对Augmented Memory Transformer模型进行Shiftable Context修改后，提高了等待k值的BLEU分数平均值2.09、1.83和1.95个数值，而 computation-aware Average Lagging的影响很小。<details>
<summary>Abstract</summary>
Transformer models using segment-based processing have been an effective architecture for simultaneous speech translation. However, such models create a context mismatch between training and inference environments, hindering potential translation accuracy. We solve this issue by proposing Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. Shiftable Context is also broadly applicable to segment-based transformers for streaming tasks. Our experiments on the English-German, English-French, and English-Spanish language pairs from the MUST-C dataset demonstrate that when applied to the Augmented Memory Transformer, a state-of-the-art model for simultaneous speech translation, the proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value for the three language pairs, respectively, with a minimal impact on computation-aware Average Lagging.
</details>
<details>
<summary>摘要</summary>
使用分割基于的处理模型已经是同时交互翻译的有效架构。然而，这些模型会在训练和推理环境中创建上下文匹配问题，从而影响翻译准确性。我们解决这个问题 by proposing Shiftable Context，一种简单 yet effective的方案，确保在训练和推理过程中保持一致的分割和上下文大小，即使有部分填充的分割段 due to the streaming nature of simultaneous translation。Shiftable Context 还可以广泛应用于流处理任务中的 segment-based transformers。我们在英语-德语、英语-法语和英语-西班牙语语对的 MUST-C 数据集上进行了实验，并发现当应用到 Augmented Memory Transformer，一种现有的同时speech翻译模型时，提议的方案可以在每个 wait-k 值上得到平均提高2.09、1.83和1.95 的 BLEU 分数，并且对 computation-aware Average Lagging 产生了最小的影响。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Language-Models-are-not-Multicultural-A-Case-Study-in-Emotion"><a href="#Multilingual-Language-Models-are-not-Multicultural-A-Case-Study-in-Emotion" class="headerlink" title="Multilingual Language Models are not Multicultural: A Case Study in Emotion"></a>Multilingual Language Models are not Multicultural: A Case Study in Emotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01370">http://arxiv.org/abs/2307.01370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shreyahavaldar/multicultural_emotion">https://github.com/shreyahavaldar/multicultural_emotion</a></li>
<li>paper_authors: Shreya Havaldar, Sunny Rai, Bhumika Singhal, Langchen Liu, Sharath Chandra Guntuku, Lyle Ungar</li>
<li>for:  investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages</li>
<li>methods:  use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, and investigate the Anglocentricity of embeddings obtained from LMs and the Western norms reflected in generative LMs</li>
<li>results:  multilingual LMs do not successfully learn the culturally appropriate nuances of emotion, and possible research directions towards correcting this are highlighted<details>
<summary>Abstract</summary>
Emotions are experienced and expressed differently across the world. In order to use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, LMs must reflect this cultural variation in emotion. In this study, we investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages. We find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.
</details>
<details>
<summary>摘要</summary>
情感表达在不同的文化中存在差异。为了使用大语言模型（LM）进行多语言任务需要情感敏感，LM必须反映这种文化差异。本研究发现，2023年广泛使用的多语言LM（例如XLM-RoBERTa）的嵌入是英语中心，生成LM（例如ChatGPT）even responding to prompts in other languages ​​still reflect Western norms. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion, and we highlight possible research directions towards correcting this.Note that the translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Semantic-enrichment-towards-efficient-speech-representations"><a href="#Semantic-enrichment-towards-efficient-speech-representations" class="headerlink" title="Semantic enrichment towards efficient speech representations"></a>Semantic enrichment towards efficient speech representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01323">http://arxiv.org/abs/2307.01323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaëlle Laperrière, Ha Nguyen, Sahar Ghannay, Bassam Jabaian, Yannick Estève</li>
<li>for: 本研究旨在提高 spoken language understanding 任务中的 semantic extraction，并且考虑 computation costs。</li>
<li>methods: 本研究使用 SAMU-XLSR 模型，通过特点域 semantic enrichment 来增强 multilingual speech representation。</li>
<li>results: 研究发现，特点域 semantic enrichment 可以提高 spoken language understanding 任务中的 semantic extraction，同时还可以提高 low-resource language 的 portability。<details>
<summary>Abstract</summary>
Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.
</details>
<details>
<summary>摘要</summary>
过去几年，自我超级学习的语音表示方法在解决口语语言理解（SLU）任务上出现了丰盈的替代方案。同时，基于巨量文本数据的多种语言模型被引入，以将语言不可知变Semantics编码。最近，SAMU-XLSR方法引入了将投资 language agnostic semantics的方法，以增强多ilingual speech表示。本研究的目的是通过对特定领域的Semantic抽象来提高SAMU-XLSR模型的SLU能力，并考虑计算成本。此外，我们还展示了对 French和Italian benchmarks的同domain使用可以提高低资源语言的可移植性，并探索了增强SAMU-XLSR的跨领域能力。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Spoken-Named-Entity-Recognition-A-Cross-Lingual-Perspective"><a href="#Exploring-Spoken-Named-Entity-Recognition-A-Cross-Lingual-Perspective" class="headerlink" title="Exploring Spoken Named Entity Recognition: A Cross-Lingual Perspective"></a>Exploring Spoken Named Entity Recognition: A Cross-Lingual Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01310">http://arxiv.org/abs/2307.01310</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moncefbenaicha/spokenner">https://github.com/moncefbenaicha/spokenner</a></li>
<li>paper_authors: Moncef Benaicha, David Thulke, M. A. Tuğtekin Turan</li>
<li>for: 研究 spoken named entity recognition (NER) 的进展，以便更好地从文本数据中识别实体。</li>
<li>methods: 使用 transferred learning 技术，从荷兰语、英语和德语三种语言进行了跨语言交流学习。使用 Wav2Vec2-XLS-R 模型，并在自定义 Pseudo-annotated 数据集上进行了训练。</li>
<li>results: 结果表明，使用 End-to-End 方式的 spoken NER 比 pipeline 方式的系统表现更好，特别是从德语到荷兰语的转移学习表现出色，超过了荷兰 E2E 系统7%，超过了荷兰 pipeline 系统4%。这项研究不仅证明了跨语言转移学习在 spoken NER 中的可行性，还提示了未来的评估中需要更多的数据收集，以提高结果。<details>
<summary>Abstract</summary>
Recent advancements in Named Entity Recognition (NER) have significantly improved the identification of entities in textual data. However, spoken NER, a specialized field of spoken document retrieval, lags behind due to its limited research and scarce datasets. Moreover, cross-lingual transfer learning in spoken NER has remained unexplored. This paper utilizes transfer learning across Dutch, English, and German using pipeline and End-to-End (E2E) schemes. We employ Wav2Vec2-XLS-R models on custom pseudo-annotated datasets and investigate several architectures for the adaptability of cross-lingual systems. Our results demonstrate that End-to-End spoken NER outperforms pipeline-based alternatives over our limited annotations. Notably, transfer learning from German to Dutch surpasses the Dutch E2E system by 7% and the Dutch pipeline system by 4%. This study not only underscores the feasibility of transfer learning in spoken NER but also sets promising outcomes for future evaluations, hinting at the need for comprehensive data collection to augment the results.
</details>
<details>
<summary>摘要</summary>
近期的Named Entity Recognition（NER）技术发展有所进步，有效地识别文本数据中的实体。然而，口语NER，是特殊的口语文检 Retrieval 领域，由于研究的限制和数据的缺乏，落后于NER。此外，口语NER的语言交互转移学习还未得到探索。这篇论文利用了语言交互转移学习 across Dutch, English, and German，使用管道和End-to-End（E2E）方案。我们使用Wav2Vec2-XLS-R模型在自定义pseudo-annotated dataset上进行了训练，并 investigate了多种架构以便适应跨语言系统的适应性。我们的结果表明，End-to-End口语NER比管道方式更高效，并且跨语言转移学习从德语到荷兰语的表现比荷兰E2E系统高出7%，并高过荷兰管道系统4%。这篇研究不仅证明了口语NER中的转移学习的可能性，还提供了未来评估中的优秀结果，强调了需要大量数据收集以增强结果。
</details></li>
</ul>
<hr>
<h2 id="The-Evolution-of-Substance-Use-Coverage-in-the-Philadelphia-Inquirer"><a href="#The-Evolution-of-Substance-Use-Coverage-in-the-Philadelphia-Inquirer" class="headerlink" title="The Evolution of Substance Use Coverage in the Philadelphia Inquirer"></a>The Evolution of Substance Use Coverage in the Philadelphia Inquirer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01299">http://arxiv.org/abs/2307.01299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Layla Bouzoubaa, Ramtin Ehsani, Preetha Chatterjee, Rezvaneh Rezapour</li>
<li>for: 本研究旨在探讨媒体对非法药物使用的报道和讨论是如何发展和变化，以及这些报道对公众对毒瘾的观感、政策和公共健康OUTCOMES有什么影响。</li>
<li>methods: 本研究使用了157,476篇洛杉矶时报文章，时间跨度为10年，并从这些文章中选择了3,903篇文章，其中每篇文章至少提到一种通常被滥用的药物。</li>
<li>results: 研究发现，大麻和鸦片是报道的最多的药物类型，而幻觉药物则被更加正面地报道。相比之下，鸦片被报道的最为负面。这项研究的目的是强调媒体对毒瘾和药物使用的报道应该准确、包容，以便减少对毒瘾人士的刻板印象和恐慌。<details>
<summary>Abstract</summary>
The media's representation of illicit substance use can lead to harmful stereotypes and stigmatization for individuals struggling with addiction, ultimately influencing public perception, policy, and public health outcomes. To explore how the discourse and coverage of illicit drug use changed over time, this study analyzes 157,476 articles published in the Philadelphia Inquirer over a decade. Specifically, the study focuses on articles that mentioned at least one commonly abused substance, resulting in a sample of 3,903 articles. Our analysis shows that cannabis and narcotics are the most frequently discussed classes of drugs. Hallucinogenic drugs are portrayed more positively than other categories, whereas narcotics are portrayed the most negatively. Our research aims to highlight the need for accurate and inclusive portrayals of substance use and addiction in the media.
</details>
<details>
<summary>摘要</summary>
媒体对非法药物使用的表达可能会导致有害的 sterotype 和偏见，影响公众对添iction的看法，政策和公共健康 outcome。为了探讨媒体对非法药物使用的话语和报道如何变化过时，这项研究分析了费城纪事报上的157,476篇文章，时间段为10年。研究选择了提及常用药物的文章，共3,903篇。我们的分析表明，大麻和毒品是最常讨论的药物类型。幻觉药物在其他类型中被更正面地描述，而毒品则被最为负面地描述。我们的研究旨在强调媒体对药物使用和添iction的精准和包容的报道是必要的。
</details></li>
</ul>
<hr>
<h2 id="Trainable-Transformer-in-Transformer"><a href="#Trainable-Transformer-in-Transformer" class="headerlink" title="Trainable Transformer in Transformer"></a>Trainable Transformer in Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01189">http://arxiv.org/abs/2307.01189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhishekpanigrahi1996/transformer_in_transformer">https://github.com/abhishekpanigrahi1996/transformer_in_transformer</a></li>
<li>paper_authors: Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora</li>
<li>for: 这篇论文是为了探讨如何使用大型预训练语言模型进行在线学习（ICL），并在推理过程中 simulate 和 fine-tune 内部模型（例如 linear 或 2-layer MLP）。</li>
<li>methods: 该论文提出了一种高效的构建方法，即 Transformer in Transformer（简称 TinT），允许 transformer 模型在推理过程中 simulate 和 fine-tune 复杂的模型（例如预训练语言模型）。该方法使用了创新的近似技术，使得 TinT 模型只需要 fewer than 2 billion parameters 可以 simulate 和 fine-tune 125 million parameter transformer 模型。</li>
<li>results: 该论文通过进行综合的 end-to-end 实验 validate 了 TinT 模型的内部细化过程，并发现在不同的语言模型和下游任务上，TinT 模型可以提高性能 by 4-16% 绝对值。这些发现表明大型预训练语言模型可以执行复杂的子任务。<details>
<summary>Abstract</summary>
Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.
</details>
<details>
<summary>摘要</summary>
最近的研究归功启发式学习（ICL）在大型预训练语言模型中的能力，是因为这些模型在推理过程中隐式地模拟和精细调整内部模型（例如线性或2层MLP）。然而，这些构造需要大量内存负担，使得更复杂的内部模型的模拟变得不可行。在这项工作中，我们提出了高效的构造方案——Transformer in Transformer（简称TinT），允许 transformer 模型在推理过程中内部模拟和精细调整复杂模型（例如预训练语言模型）。具体来说，我们提出了创新的近似技术，使得 TinT 模型 fewer than 2 billion parameters 可以在单个前进 passes 中模拟和精细调整 125 million parameter transformer 模型。TinT 支持许多常见的 transformer 变体，并且其设计思想还改进了过去简单模型在 transformers 中的效率。我们通过综合实验 validate 内部精细调整过程的有效性，并发现 TinT 对于不同语言模型和下游任务的性能都有明显提升。例如，即使只有一步预算，我们发现 TinT 对于 OPT-125M 模型可以提高性能的平均差值为 4-16%。这些发现表明大型预训练语言模型可以执行复杂的子过程。为了促进进一步研究，我们附加了可重用和扩展的代码库。
</details></li>
</ul>
<hr>
<h2 id="Improving-Language-Plasticity-via-Pretraining-with-Active-Forgetting"><a href="#Improving-Language-Plasticity-via-Pretraining-with-Active-Forgetting" class="headerlink" title="Improving Language Plasticity via Pretraining with Active Forgetting"></a>Improving Language Plasticity via Pretraining with Active Forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01163">http://arxiv.org/abs/2307.01163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Chen, Kelly Marchisio, Roberta Raileanu, David Ifeoluwa Adelani, Pontus Stenetorp, Sebastian Riedel, Mikel Artetxe</li>
<li>for: 这篇论文的目的是提出一种简单的活动忘记机制，以便使PLMs可以快速适应新语言。</li>
<li>methods: 该论文使用了一种活动忘记机制，在预训练过程中每K更新一次 embedding layer，以便让PLMs能够快速学习新的 embedding。</li>
<li>results: 实验表明，使用该忘记机制可以使PLMs在语言适应过程中更快 converges，并且在具有少量数据的情况下，特别是与英语远程的语言，能够表现出更好的性能。<details>
<summary>Abstract</summary>
Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, particularly for languages that are distant from English.
</details>
<details>
<summary>摘要</summary>
现代自然语言处理（NLP）中的预训练语言模型（PLM）已成为主流模型。尽管它们在下游任务中表现出色，但是将PLM应用于新语言可能会困难，这可能限制了它们的 universality 性。先前的工作已经证明可以通过学习一个新的映射层来解决这个问题，但是这需要大量的数据和计算资源。我们提议使用活动忘记机制 durante la pretraining，以便快速地使PLM适应新语言。具体来说，在每个更新中重置 embedding layer，我们鼓励PLM在有限的更新数量内快速学习新的映射，类似于一种元学习效应。我们使用 RoBERTa 进行实验，并证明了在语言适应过程中使用我们的忘记机制可以更快地 converges，并且在数据量较少的情况下，特别是与英语较为 distant 的语言，模型的表现更出色。
</details></li>
</ul>
<hr>
<h2 id="Translating-Latin-with-Artificial-Intelligence"><a href="#Translating-Latin-with-Artificial-Intelligence" class="headerlink" title="Translating Latin with Artificial Intelligence"></a>Translating Latin with Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07520">http://arxiv.org/abs/2307.07520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sylvio R. Bistafa</li>
<li>for: 该研究旨在透过人工智能翻译技术，解决早期科学文献的可用性问题，特别是李奥纳·欧拉的作品。</li>
<li>methods: 本研究使用了两种流行的人工智能翻译算法，即Google Translate和ChatGPT，进行比较性测试，以验证它们的表现。</li>
<li>results: 测试结果表明，ChatGPT在翻译李奥纳·欧拉的1739年信件中表现出色，提供了优秀的翻译结果，这表明了ChatGPT可以作为一个有价值的翻译工具，不仅对普通的拉丁文献专家有帮助，还对特殊的拉丁文献翻译家有利。<details>
<summary>Abstract</summary>
The major hindrance in the study of earlier scientific literature is the availability of Latin translations into modern languages. This is particular true for the works of Euler who authored about 850 manuscripts and wrote a thousand letters and received back almost two thousand more. The translation of many of these manuscripts, books and letters have been published in various sources over the last two centuries, but many more have not yet appeared. Fortunately, nowadays, the artificial intelligence AI translation can be used to circumvent the challenges of translating such substantial number of texts. To validate this tool, benchmark tests have been performed to compare the performance of two popular AI translating algorithms, namely Google Translate and ChatGPT. Since it was found that ChatGPT performed better on these tests, this translating support was then used on an excerpt of a 1739 letter from Johann Bernoulli to Euler, where he notifies that he was sending to Euler the first part of his manuscript Hydraulica. The findings highlight ChatGPT as a valuable translation tool, catering not only to general Latin practitioners but also proving beneficial for specialized Latin translators.
</details>
<details>
<summary>摘要</summary>
主要阻碍古科学文献研究的问题是现代语言中的拉丁文翻译的可用性。这 particualrly true for Euler 的作品，他撰写了约850份手稿和写了1000封信件，收到了 almost 2000封回信。许多这些手稿、书籍和信件的翻译已经在过去两个世纪出版，但还有很多没有出现。幸运的是，现在可以使用人工智能 AI 翻译工具来绕过这些文献的翻译挑战。为验证这个工具，我们进行了比较两个流行的 AI 翻译算法的 benchMark 测试，结果发现 ChatGPT 的表现更好，因此选择使用这个翻译支持。在一篇1739年的Euler 写给 Bernoulli 的信件中，Bernoulli 通知他将发送给 Euler 的第一部分的液体学 manuscript。这些发现 highlight ChatGPT 作为一个有价值的翻译工具，不仅有利于一般拉丁文翻译者，还有利于专业拉丁文翻译者。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/04/cs.CL_2023_07_04/" data-id="clpahu6zi007u3h8864aoakpj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/94/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/93/">93</a><a class="page-number" href="/page/94/">94</a><span class="page-number current">95</span><a class="page-number" href="/page/96/">96</a><a class="page-number" href="/page/97/">97</a><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/96/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
