
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/95/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/cs.CV_2023_07_04/" class="article-date">
  <time datetime="2023-07-04T13:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/cs.CV_2023_07_04/">cs.CV - 2023-07-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Localized-Data-Work-as-a-Precondition-for-Data-Centric-ML-A-Case-Study-of-Full-Lifecycle-Crop-Disease-Identification-in-Ghana"><a href="#Localized-Data-Work-as-a-Precondition-for-Data-Centric-ML-A-Case-Study-of-Full-Lifecycle-Crop-Disease-Identification-in-Ghana" class="headerlink" title="Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana"></a>Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01767">http://arxiv.org/abs/2307.01767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darlington Akogo, Issah Samori, Cyril Akafia, Harriet Fiagbor, Andrews Kangah, Donald Kwame Asiedu, Kwabena Fuachie, Luis Oala</li>
<li>for: 这项研究的目的是提高农业生产力和食品安全。</li>
<li>methods: 该研究使用无人机采集数据和机器学习算法来确定作物受到的压力。</li>
<li>results: 研究组共同开发了数据、模型和应用程序，并将其提供给当地农民 via 桌面应用程序。<details>
<summary>Abstract</summary>
The Ghana Cashew Disease Identification with Artificial Intelligence (CADI AI) project demonstrates the importance of sound data work as a precondition for the delivery of useful, localized datacentric solutions for public good tasks such as agricultural productivity and food security. Drone collected data and machine learning are utilized to determine crop stressors. Data, model and the final app are developed jointly and made available to local farmers via a desktop application.
</details>
<details>
<summary>摘要</summary>
《加纳核桃疾病识别用人工智能（CADI AI）项目》表明了准确的数据工作的重要性，作为地方化数据驱动解决方案的先决条件，以提高农业生产力和食品安全。在该项目中，用扫描机采集的数据和机器学习算法来确定作物压力。数据、模型和最终应用程序都是在桌面应用程序上开发的，并且提供给当地农民使用。
</details></li>
</ul>
<hr>
<h2 id="Pretraining-is-All-You-Need-A-Multi-Atlas-Enhanced-Transformer-Framework-for-Autism-Spectrum-Disorder-Classification"><a href="#Pretraining-is-All-You-Need-A-Multi-Atlas-Enhanced-Transformer-Framework-for-Autism-Spectrum-Disorder-Classification" class="headerlink" title="Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification"></a>Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01759">http://arxiv.org/abs/2307.01759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lugges991/metaformer">https://github.com/lugges991/metaformer</a></li>
<li>paper_authors: Lucas Mahler, Qi Wang, Julius Steiglechner, Florian Birk, Samuel Heczko, Klaus Scheffler, Gabriele Lohmann</li>
<li>for: 这个研究旨在提出一个新的多几个地图扩展器架构（METAFormer），用于抑制Autism Spectrum Disorder（ASD）分类。</li>
<li>methods: 这个架构使用了休息状态功能磁共振成像资料，并使用多几个地图方法，包括AAL、CC200和DOS160地图，以进行自我超vised pretraining。</li>
<li>results: 这个研究显示，METAFormer可以在ABIDE I dataset上超过现有的州���状态表现，具有83.7%的精度和0.832的AUC分数。<details>
<summary>Abstract</summary>
Autism spectrum disorder (ASD) is a prevalent psychiatric condition characterized by atypical cognitive, emotional, and social patterns. Timely and accurate diagnosis is crucial for effective interventions and improved outcomes in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced Transformer framework, METAFormer, ASD classification. Our framework utilizes resting-state functional magnetic resonance imaging data from the ABIDE I dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer employs a multi-atlas approach, where flattened connectivity matrices from the AAL, CC200, and DOS160 atlases serve as input to the transformer encoder. Notably, we demonstrate that self-supervised pretraining, involving the reconstruction of masked values from the input, significantly enhances classification performance without the need for additional or separate training data. Through stratified cross-validation, we evaluate the proposed framework and show that it surpasses state-of-the-art performance on the ABIDE I dataset, with an average accuracy of 83.7% and an AUC-score of 0.832. The code for our framework is available at https://github.com/Lugges991/METAFormer
</details>
<details>
<summary>摘要</summary>
“对于自闭症 спектル中的诊断，时间和准确性都是非常重要的。在这个研究中，我们提出了一个新的多几个 Atlases 增强 Transformer 框架，METAFormer，用于自闭症分类。我们的框架使用了 AAL、CC200 和 DOS160 的 Atlases，将其融合为一个入口，并将其传递给 Transformer Encoder。我们还证明了，不需要额外训练数据，通过自我预 обу的方法，可以很好地增强分类性能。通过阶层验证，我们评估了我们的框架，并发现它在 ABIDE I 数据集上超过了现有的州度数据，具有83.7% 的精度和 0.832 的 AUC 得分。我们的代码可以在 GitHub 上找到：https://github.com/Lugges991/METAFormer。”
</details></li>
</ul>
<hr>
<h2 id="K-complex-Detection-Using-Fourier-Spectrum-Analysis-In-EEG"><a href="#K-complex-Detection-Using-Fourier-Spectrum-Analysis-In-EEG" class="headerlink" title="K-complex Detection Using Fourier Spectrum Analysis In EEG"></a>K-complex Detection Using Fourier Spectrum Analysis In EEG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01754">http://arxiv.org/abs/2307.01754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexey Protopopov</li>
<li>for:  automatic K-complex detection in EEG records</li>
<li>methods:  based on fast Fourier transform, not using neural networks</li>
<li>results:  comparable or superior quality to previous methods, including those using neural networks, with less computational power required.<details>
<summary>Abstract</summary>
K-complexes are an important marker of brain activity and are used both in clinical practice to perform sleep scoring, and in research. However, due to the size of electroencephalography (EEG) records, as well as the subjective nature of K-complex detection performed by somnologists, it is reasonable to automate K-complex detection. Previous works in this field of research have relied on the values of true positive rate and false positive rate to quantify the effectiveness of proposed methods, however this set of metrics may be misleading. The objective of the present research is to find a more accurate set of metrics and use them to develop a new method of K-complex detection, which would not rely on neural networks. Thus, the present article proposes two new methods for K-complex detection based on the fast Fourier transform. The results achieved demonstrated that the proposed methods offered a quality of K-complex detection that is either similar or superior to the quality of the methods demonstrated in previous works, including the methods employing neural networks, while requiring less computational power, meaning that K-complex detection does not require the use of neural networks. The proposed methods were evaluated using a new set of metrics, which is more representative of the quality of K-complex detection.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SRCD-Semantic-Reasoning-with-Compound-Domains-for-Single-Domain-Generalized-Object-Detection"><a href="#SRCD-Semantic-Reasoning-with-Compound-Domains-for-Single-Domain-Generalized-Object-Detection" class="headerlink" title="SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection"></a>SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01750">http://arxiv.org/abs/2307.01750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhijie Rao, Jingcai Guo, Luyao Tang, Yue Huang, Xinghao Ding, Song Guo</li>
<li>for: 本文提出了一种单域泛化物体检测（Single-DGOD）框架，旨在学习和维护自增强样本的 semantic 结构，以提高模型的泛化能力。</li>
<li>methods: 我们提出的 SRCD 包括两个主要组成部分： texture-based self-augmentation (TBSA) 模块和 local-global semantic reasoning (LGSR) 模块。 TBSA 模块通过自适应增强来消除影响标签的不相关特征，如光、阴影、颜色等。而 LGSR 模块则用于进一步模型实例特征之间的semantic关系，以暴露和维护内在的 semantic 结构。</li>
<li>results: 我们在多个 benchmark 上进行了广泛的实验，证明了我们提出的 SRCD 的效果。<details>
<summary>Abstract</summary>
This paper provides a novel framework for single-domain generalized object detection (i.e., Single-DGOD), where we are interested in learning and maintaining the semantic structures of self-augmented compound cross-domain samples to enhance the model's generalization ability. Different from DGOD trained on multiple source domains, Single-DGOD is far more challenging to generalize well to multiple target domains with only one single source domain. Existing methods mostly adopt a similar treatment from DGOD to learn domain-invariant features by decoupling or compressing the semantic space. However, there may have two potential limitations: 1) pseudo attribute-label correlation, due to extremely scarce single-domain data; and 2) the semantic structural information is usually ignored, i.e., we found the affinities of instance-level semantic relations in samples are crucial to model generalization. In this paper, we introduce Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. Specifically, our SRCD contains two main components, namely, the texture-based self-augmentation (TBSA) module, and the local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the effects of irrelevant attributes associated with labels, such as light, shadow, color, etc., at the image level by a light-yet-efficient self-augmentation. Moreover, LGSR is used to further model the semantic relationships on instance features to uncover and maintain the intrinsic semantic structures. Extensive experiments on multiple benchmarks demonstrate the effectiveness of the proposed SRCD.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的单域总体化对象检测框架（即Single-DGOD），我们在这种框架中学习和维护自适应增强的混合域样本的 semantic structure，以提高模型的通用能力。与多源域DGOD相比，单域DGOD更加具有挑战性，因为只有一个源域，因此模型难以通用到多个目标域。现有方法通常采用类似于DGOD的方法，即学习域无关特征，通过减少或压缩 semantic space来实现。但是，存在两个潜在的限制：1） Pseudo attribute-label correlation，由于单域数据非常稀缺; 2）模型忽略了实例水平的semantic structural information，即 samples中实例之间的semantic关系的强度是对模型泛化的关键。在本文中，我们提出了Semantic Reasoning with Compound Domains（SRCD）模型，其包括两个主要组件：texture-based self-augmentation（TBSA）模块和local-global semantic reasoning（LGSR）模块。TBSA模块通过快速和高效地自适应来消除与标签相关的不相关特征，如光、阴影、颜色等。而LGSR模块则用于进一步模型实例水平的semantic关系，以uncover和维护内在的semantic结构。我们在多个 benchmark上进行了广泛的实验，并证明了我们提出的SRCD的效果。
</details></li>
</ul>
<hr>
<h2 id="Ben-ge-Extending-BigEarthNet-with-Geographical-and-Environmental-Data"><a href="#Ben-ge-Extending-BigEarthNet-with-Geographical-and-Environmental-Data" class="headerlink" title="Ben-ge: Extending BigEarthNet with Geographical and Environmental Data"></a>Ben-ge: Extending BigEarthNet with Geographical and Environmental Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01741">http://arxiv.org/abs/2307.01741</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsg-aiml/ben-ge">https://github.com/hsg-aiml/ben-ge</a></li>
<li>paper_authors: Michael Mommert, Nicolas Kesseli, Joëlle Hanna, Linus Scheibenreif, Damian Borth, Begüm Demir</li>
<li>for: 本研究旨在探讨多模式 Earth observation 数据的分析方法，并证明 combining 不同数据模式可以提高下游任务的准确率。</li>
<li>methods: 本研究使用了 Earth observation 数据的多模式 combine，包括 patch-based 地用&#x2F;地形类别和地用&#x2F;地形分割等下游任务。</li>
<li>results: 研究表明，通过 combining 不同数据模式，可以提高下游任务的准确率，并且可以作为 Earth observation 应用的测试平台。<details>
<summary>Abstract</summary>
Deep learning methods have proven to be a powerful tool in the analysis of large amounts of complex Earth observation data. However, while Earth observation data are multi-modal in most cases, only single or few modalities are typically considered. In this work, we present the ben-ge dataset, which supplements the BigEarthNet-MM dataset by compiling freely and globally available geographical and environmental data. Based on this dataset, we showcase the value of combining different data modalities for the downstream tasks of patch-based land-use/land-cover classification and land-use/land-cover segmentation. ben-ge is freely available and expected to serve as a test bed for fully supervised and self-supervised Earth observation applications.
</details>
<details>
<summary>摘要</summary>
深度学习方法在大量复杂的地球观测数据分析中表现出了强大的功能。然而，大多数情况下的地球观测数据是多modal的，但只考虑单个或少数modalities。在这个工作中，我们提供了ben-ge数据集，该数据集收集了全球和自由可用的地理和环境数据，并基于这个数据集，我们展示了不同modalities的结合对下游任务（patch-based 土地用途/土地覆盖分类和土地用途/土地覆盖分割）的价值。ben-ge是免费可用的，预计将成为全supervised和self-supervised Earth observation应用程序的测试床。
</details></li>
</ul>
<hr>
<h2 id="Synchronous-Image-Label-Diffusion-Probability-Model-with-Application-to-Stroke-Lesion-Segmentation-on-Non-contrast-CT"><a href="#Synchronous-Image-Label-Diffusion-Probability-Model-with-Application-to-Stroke-Lesion-Segmentation-on-Non-contrast-CT" class="headerlink" title="Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT"></a>Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01740">http://arxiv.org/abs/2307.01740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianhai Zhang, Tonghua Wan, Ethan MacDonald, Bijoy Menon, Aravind Ganesh, Qiu Wu</li>
<li>for: 这个论文是为了提出一种基于Markov扩散过程的同步图像标签扩散可能性模型（SDPM），用于非对比CT扫描图像中的血栓病变 segmentation。</li>
<li>methods: 该模型基于一个隐变量模型（LVM），并引入了一个额外网络流，以获得初始噪声标签估计，以便高效地推理最终标签。通过优化指定的可变边界，训练好的模型可以对输入图像噪声给出多个标签估计。</li>
<li>results: 该模型在三个血栓病变数据集上进行测试，包括一个公共数据集和两个私人数据集，并与一些U-net和变换器基于的分割方法进行比较。结果显示，提出的SDPM模型能够达到当前最佳性能。代码公开 disponível。<details>
<summary>Abstract</summary>
Stroke lesion volume is a key radiologic measurement for assessing the prognosis of Acute Ischemic Stroke (AIS) patients, which is challenging to be automatically measured on Non-Contrast CT (NCCT) scans. Recent diffusion probabilistic models have shown potentials of being used for image segmentation. In this paper, a novel Synchronous image-label Diffusion Probability Model (SDPM) is proposed for stroke lesion segmentation on NCCT using Markov diffusion process. The proposed SDPM is fully based on a Latent Variable Model (LVM), offering a complete probabilistic elaboration. An additional net-stream, parallel with a noise prediction stream, is introduced to obtain initial noisy label estimates for efficiently inferring the final labels. By optimizing the specified variational boundaries, the trained model can infer multiple label estimates for reference given the input images with noises. The proposed model was assessed on three stroke lesion datasets including one public and two private datasets. Compared to several U-net and transformer-based segmentation methods, our proposed SDPM model is able to achieve state-of-the-art performance. The code is publicly available.
</details>
<details>
<summary>摘要</summary>
stroke lesion volume 是评估急性血栓roke（AIS）患者 prospect 的关键 radiologic 测量，在 Non-Contrast CT（NCCT）扫描中具有挑战性。  recient diffusion probabilistic 模型已经表现出了用于图像分割的潜在性。 在这篇论文中，一种新的同步图像标签Diffusion Probability Model（SDPM）被提出用于NCCT扫描中的roke lesion 分割。 SDPM 基于 Latent Variable Model（LVM），提供了完整的 probabilistic 推导。 一个额外的网络流，与噪声预测流并行，用于获取初始噪声标签估计，以高效地推导最终标签。 通过优化指定的边界，训练模型可以基于输入图像噪声提供多个标签估计。  proposed SDPM 模型在三个roke lesion 数据集中评估，包括一个公共数据集和两个私人数据集。 与一些 U-net 和 transformer 基于的 segmentation 方法相比，我们的提出的 SDPM 模型能够实现状态天表性性能。 代码公开可用。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Calibration-Bias-Without-Fixed-Attribute-Grouping-for-Improved-Fairness-in-Medical-Imaging-Analysis"><a href="#Mitigating-Calibration-Bias-Without-Fixed-Attribute-Grouping-for-Improved-Fairness-in-Medical-Imaging-Analysis" class="headerlink" title="Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis"></a>Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01738">http://arxiv.org/abs/2307.01738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changjian Shui, Justin Szeto, Raghav Mehta, Douglas L. Arnold, Tal Arbel</li>
<li>for: 这个研究是为了提高深度学习医学影像分析模型在实际医疗执行中的可靠性和准确性。</li>
<li>methods: 我们提出了一个新的二阶方法：集群专注法，它可以首先识别低准确的样本，然后将它们分为群体，最后将每个群体使用集群专注损失来改善准确偏差。</li>
<li>results: 我们的方法可以帮助控制最差表现的子群体的准确偏差，同时保持预测性能，并比最近的基elines表现更好。<details>
<summary>Abstract</summary>
Trustworthy deployment of deep learning medical imaging models into real-world clinical practice requires that they be calibrated. However, models that are well calibrated overall can still be poorly calibrated for a sub-population, potentially resulting in a clinician unwittingly making poor decisions for this group based on the recommendations of the model. Although methods have been shown to successfully mitigate biases across subgroups in terms of model accuracy, this work focuses on the open problem of mitigating calibration biases in the context of medical image analysis. Our method does not require subgroup attributes during training, permitting the flexibility to mitigate biases for different choices of sensitive attributes without re-training. To this end, we propose a novel two-stage method: Cluster-Focal to first identify poorly calibrated samples, cluster them into groups, and then introduce group-wise focal loss to improve calibration bias. We evaluate our method on skin lesion classification with the public HAM10000 dataset, and on predicting future lesional activity for multiple sclerosis (MS) patients. In addition to considering traditional sensitive attributes (e.g. age, sex) with demographic subgroups, we also consider biases among groups with different image-derived attributes, such as lesion load, which are required in medical image analysis. Our results demonstrate that our method effectively controls calibration error in the worst-performing subgroups while preserving prediction performance, and outperforming recent baselines.
</details>
<details>
<summary>摘要</summary>
信任性的深度学习医疗影像模型在实际临床实践中部署需要进行准确化。然而，即使模型在整体上具有良好的准确性，也可能对一个子population产生差异，导致医生因模型的建议而做出不良决策。虽然有方法可以在不同 subgroup 上减少偏见，但这项工作将关注医疗影像分析中的开放问题——准确性偏见的缓解。我们的方法不需要在训练过程中提供 subgroup 特征，因此可以随时缓解不同敏感特征的偏见。为此，我们提出了一种新的两阶段方法：首先使用 clustering 来identify poorly calibrated samples，然后引入 group-wise focal loss 来改善偏见偏见。我们在皮肤损害分类和多发性硬化病（MS）患者预测未来损害情况上进行了评估。此外，我们还考虑了传统敏感特征（如年龄、性别）和医疗影像分析中必需的图像特征，如肿瘤荷重。我们的结果表明，我们的方法可以控制最差 subgroup 的准确性错误，保持预测性能，并超过最近的基elines。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Computer-Vision-Models-through-Adversarial-Training-Unveiling-the-Robustness-Interpretability-Connection"><a href="#Interpretable-Computer-Vision-Models-through-Adversarial-Training-Unveiling-the-Robustness-Interpretability-Connection" class="headerlink" title="Interpretable Computer Vision Models through Adversarial Training: Unveiling the Robustness-Interpretability Connection"></a>Interpretable Computer Vision Models through Adversarial Training: Unveiling the Robustness-Interpretability Connection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02500">http://arxiv.org/abs/2307.02500</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/delyan-boychev/pytorch_trainers_interpretability">https://github.com/delyan-boychev/pytorch_trainers_interpretability</a></li>
<li>paper_authors: Delyan Boychev</li>
<li>for: 本研究旨在评估对抗训练的影响，以生成更加鲁棒的模型，即具有对抗攻击的抵抗能力。</li>
<li>methods: 本研究使用了本地特征重要性方法（SHAP和Integrated Gradients）和特征视觉技术（Representation Inversion和Class Specific Image Generation）进行了广泛的测试和分析。</li>
<li>results: 研究发现，对抗训练可以使计算机视觉模型更加 interpretable，即其学习的特征更加类似于人类的理解。此外，对抗训练的模型在对抗攻击时表现更加鲁棒，并且更加注重图像中的特定区域，以支持其预测。<details>
<summary>Abstract</summary>
With the perpetual increase of complexity of the state-of-the-art deep neural networks, it becomes a more and more challenging task to maintain their interpretability. Our work aims to evaluate the effects of adversarial training utilized to produce robust models - less vulnerable to adversarial attacks. It has been shown to make computer vision models more interpretable. Interpretability is as essential as robustness when we deploy the models to the real world. To prove the correlation between these two problems, we extensively examine the models using local feature-importance methods (SHAP, Integrated Gradients) and feature visualization techniques (Representation Inversion, Class Specific Image Generation). Standard models, compared to robust are more susceptible to adversarial attacks, and their learned representations are less meaningful to humans. Conversely, these models focus on distinctive regions of the images which support their predictions. Moreover, the features learned by the robust model are closer to the real ones.
</details>
<details>
<summary>摘要</summary>
随着现代深度神经网络的复杂性不断增加，维护它们的解释性变得越来越困难。我们的工作旨在评估针对性训练可以生成更加鲁棒的模型，以降低对抗性攻击的脆弱性。已经证明了在计算机视觉领域中，使用针对性训练可以提高模型的解释性。当我们将模型部署到实际应用中时，解释性的重要性与鲁棒性一样高。为了证明这两个问题之间的相关性，我们广泛使用本地特征重要性方法（SHAP、整合梯度）和特征视觉技术（ Representation Inversion、类特征图生成）进行检验。对比标准模型和鲁棒模型，后者更易受到抗性攻击，并且它所学习的特征更难以被人类理解。然而，这些模型强调特定的图像区域，这些区域支持它们的预测。此外，鲁棒模型学习的特征更加接近真实的特征。
</details></li>
</ul>
<hr>
<h2 id="Graph-Ensemble-Learning-Model-for-Multi-label-Skin-Lesion-Classification-using-Dermoscopy-and-Clinical-Images"><a href="#Graph-Ensemble-Learning-Model-for-Multi-label-Skin-Lesion-Classification-using-Dermoscopy-and-Clinical-Images" class="headerlink" title="Graph-Ensemble Learning Model for Multi-label Skin Lesion Classification using Dermoscopy and Clinical Images"></a>Graph-Ensemble Learning Model for Multi-label Skin Lesion Classification using Dermoscopy and Clinical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01704">http://arxiv.org/abs/2307.01704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Tang, Yang Nan, Tobias Lasser</li>
<li>for: 本研究旨在开发一种基于多模态数据的多标签分类方法，以提高皮肤病诊断的准确性。</li>
<li>methods: 该方法利用图像 convolutional neural network (GCN) 来利用多模态数据的协调关系，并通过自适应 fusion 技术将 GCN 的预测与多模态数据 fusion 模型的预测进行权重平均 fusions，以获取更高的分类精度。</li>
<li>results: 实验结果表明，提出的 Graph-Ensemble Learning Model (GELN) 可以在不同的 dataset 上提高分类性能，并在 SPC 和诊断分类方面达到领先的表现。<details>
<summary>Abstract</summary>
Many skin lesion analysis (SLA) methods recently focused on developing a multi-modal-based multi-label classification method due to two factors. The first is multi-modal data, i.e., clinical and dermoscopy images, which can provide complementary information to obtain more accurate results than single-modal data. The second one is that multi-label classification, i.e., seven-point checklist (SPC) criteria as an auxiliary classification task can not only boost the diagnostic accuracy of melanoma in the deep learning (DL) pipeline but also provide more useful functions to the clinical doctor as it is commonly used in clinical dermatologist's diagnosis. However, most methods only focus on designing a better module for multi-modal data fusion; few methods explore utilizing the label correlation between SPC and skin disease for performance improvement. This study fills the gap that introduces a Graph Convolution Network (GCN) to exploit prior co-occurrence between each category as a correlation matrix into the DL model for the multi-label classification. However, directly applying GCN degraded the performances in our experiments; we attribute this to the weak generalization ability of GCN in the scenario of insufficient statistical samples of medical data. We tackle this issue by proposing a Graph-Ensemble Learning Model (GELN) that views the prediction from GCN as complementary information of the predictions from the fusion model and adaptively fuses them by a weighted averaging scheme, which can utilize the valuable information from GCN while avoiding its negative influences as much as possible. To evaluate our method, we conduct experiments on public datasets. The results illustrate that our GELN can consistently improve the classification performance on different datasets and that the proposed method can achieve state-of-the-art performance in SPC and diagnosis classification.
</details>
<details>
<summary>摘要</summary>
多种皮肤 lesion 分析（SLA）方法最近都在努力开发一种多模态基于多标签分类方法，主要是因为两点。第一，我们有多种模态数据，例如临床和肤视图图像，这些数据可以提供补偿信息，以获得更准确的结果。第二，多标签分类可以不仅提高抑制癌症的深度学习（DL）管道的诊断精度，还可以提供更有用的功能 для临床医生，因为这种分类方法在临床 dermatologist 的诊断中广泛使用。然而，大多数方法都是关注设计更好的多模态数据融合模块，很少方法探讨利用皮病分类标准（SPC）和皮肤病的标签相关性来提高性能。本研究填补了这一空白，通过引入一个图像卷积网络（GCN），利用每个类别之间的协同关系，在 DL 模型中进行多标签分类。然而，直接应用 GCN 会下降性能，我们归因于医学数据的不充分统计样本的问题。我们解决这个问题，提出一种图像ensemble学习模型（GELN），视图GCN 的预测为补充信息，并通过一种权重平均方式，将其与融合模型的预测相乘，以利用 GCN 的有价值信息，同时避免它的负面影响。为评估我们的方法，我们在公共数据集上进行实验。结果表明，我们的 GELN 可以在不同的数据集上具有稳定的分类性能，并且可以在 SPC 和诊断分类中达到国际级的表现。
</details></li>
</ul>
<hr>
<h2 id="Augment-Features-Beyond-Color-for-Domain-Generalized-Segmentation"><a href="#Augment-Features-Beyond-Color-for-Domain-Generalized-Segmentation" class="headerlink" title="Augment Features Beyond Color for Domain Generalized Segmentation"></a>Augment Features Beyond Color for Domain Generalized Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01703">http://arxiv.org/abs/2307.01703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiyu Sun, Pavlo Melnyk, Michael Felsberg, Yang Tang</li>
<li>for: 这篇论文旨在提出一种适用于不同类型资料的通用Semantic Segmentation方法，并且不需要target资料进行训练。</li>
<li>methods: 我们的方法包括两个模组：随机图像颜色增强（RICA）和随机特征分布增强（RFDA）。RICA将图像从RGB转换为CIELAB颜色模型，并在一个感知基础上随机调整图像以提高图像质量。我们还使用CycleGAN-based生成网络将增强后的图像扩展到特征空间，以更好地丰富数据。</li>
<li>results: 我们进行了广泛的实验，结果显示我们的方法在不同的资料集上（包括Synthia、Cityscapes、BDDS和Mapillary）实现了顶尖的Semantic Segmentation性能。<details>
<summary>Abstract</summary>
Domain generalized semantic segmentation (DGSS) is an essential but highly challenging task, in which the model is trained only on source data and any target data is not available. Previous DGSS methods can be partitioned into augmentation-based and normalization-based ones. The former either introduces extra biased data or only conducts channel-wise adjustments for data augmentation, and the latter may discard beneficial visual information, both of which lead to limited performance in DGSS. Contrarily, our method performs inter-channel transformation and meanwhile evades domain-specific biases, thus diversifying data and enhancing model generalization performance. Specifically, our method consists of two modules: random image color augmentation (RICA) and random feature distribution augmentation (RFDA). RICA converts images from RGB to the CIELAB color model and randomizes color maps in a perception-based way for image enhancement purposes. We further this augmentation by extending it beyond color to feature space using a CycleGAN-based generative network, which complements RICA and further boosts generalization capability. We conduct extensive experiments, and the generalization results from the synthetic GTAV and SYNTHIA to the real Cityscapes, BDDS, and Mapillary datasets show that our method achieves state-of-the-art performance in DGSS.
</details>
<details>
<summary>摘要</summary>
领域普遍 semantic segmentation (DGSS) 是一个非常重要但也非常具有挑战性的任务，模型在训练时只有source数据可用，target数据不可用。先前的DGSS方法可以分为两种：增强型和normalization型。前者可能引入额外的偏见数据或只是执行通道 wise的调整，后者可能会弃用有利的视觉信息，这两者都导致DGSS的表现有限。相反，我们的方法会执行 между通道转换和避免域专偏见，因此可以多样化数据和提高模型的普遍性表现。我们的方法包括两个模组：随机图像颜色增强 (RICA) 和随机特征分布增强 (RFDA)。RICA 将图像从 RGB 转换为 CIELAB 颜色模型，并在感知方式下随机调整图像以增强图像表现。我们继续这个增强，通过使用 CycleGAN 基本的生成网络，该网络可以补充 RICA 并进一步提高普遍能力。我们实现了广泛的实验，并从 sintetic GTAV 和 SYNTHIA  Synthetic 资料集到 real Cityscapes、BDDS 和 Mapillary 资料集的一致性结果显示，我们的方法在 DGSS 中实现了顶尖的表现。
</details></li>
</ul>
<hr>
<h2 id="Spike-driven-Transformer"><a href="#Spike-driven-Transformer" class="headerlink" title="Spike-driven Transformer"></a>Spike-driven Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01694">http://arxiv.org/abs/2307.01694</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biclab/spike-driven-transformer">https://github.com/biclab/spike-driven-transformer</a></li>
<li>paper_authors: Man Yao, Jiakui Hu, Zhaokun Zhou, Li Yuan, Yonghong Tian, Bo Xu, Guoqi Li<br>for:The paper is written to propose a new deep learning model called Spike-driven Transformer, which incorporates the spike-driven paradigm into the Transformer architecture.methods:The proposed Spike-driven Transformer uses four unique properties: event-driven, binary spike communication, self-attention with linear complexity, and mask and addition operations.results:The Spike-driven Transformer achieves a top-1 accuracy of 77.1% on ImageNet-1K, which is the state-of-the-art result in the SNN field.<details>
<summary>Abstract</summary>
Spiking Neural Networks (SNNs) provide an energy-efficient deep learning option due to their unique spike-based event-driven (i.e., spike-driven) paradigm. In this paper, we incorporate the spike-driven paradigm into Transformer by the proposed Spike-driven Transformer with four unique properties: 1) Event-driven, no calculation is triggered when the input of Transformer is zero; 2) Binary spike communication, all matrix multiplications associated with the spike matrix can be transformed into sparse additions; 3) Self-attention with linear complexity at both token and channel dimensions; 4) The operations between spike-form Query, Key, and Value are mask and addition. Together, there are only sparse addition operations in the Spike-driven Transformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA), which exploits only mask and addition operations without any multiplication, and thus having up to $87.2\times$ lower computation energy than vanilla self-attention. Especially in SDSA, the matrix multiplication between Query, Key, and Value is designed as the mask operation. In addition, we rearrange all residual connections in the vanilla Transformer before the activation functions to ensure that all neurons transmit binary spike signals. It is shown that the Spike-driven Transformer can achieve 77.1\% top-1 accuracy on ImageNet-1K, which is the state-of-the-art result in the SNN field. The source code is available at https://github.com/BICLab/Spike-Driven-Transformer.
</details>
<details>
<summary>摘要</summary>
ospiking neural networks (SNNs) provide an energy-efficient deep learning option due to their unique spike-based event-driven (i.e., spike-driven) paradigm. In this paper, we incorporate the spike-driven paradigm into Transformer by the proposed Spike-driven Transformer with four unique properties: 1) Event-driven, no calculation is triggered when the input of Transformer is zero; 2) Binary spike communication, all matrix multiplications associated with the spike matrix can be transformed into sparse additions; 3) Self-attention with linear complexity at both token and channel dimensions; 4) The operations between spike-form Query, Key, and Value are mask and addition. Together, there are only sparse addition operations in the Spike-driven Transformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA), which exploits only mask and addition operations without any multiplication, and thus having up to $87.2\times$ lower computation energy than vanilla self-attention. Especially in SDSA, the matrix multiplication between Query, Key, and Value is designed as the mask operation. In addition, we rearrange all residual connections in the vanilla Transformer before the activation functions to ensure that all neurons transmit binary spike signals. It is shown that the Spike-driven Transformer can achieve 77.1\% top-1 accuracy on ImageNet-1K, which is the state-of-the-art result in the SNN field. The source code is available at https://github.com/BICLab/Spike-Driven-Transformer.Note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Training-Energy-Based-Models-with-Diffusion-Contrastive-Divergences"><a href="#Training-Energy-Based-Models-with-Diffusion-Contrastive-Divergences" class="headerlink" title="Training Energy-Based Models with Diffusion Contrastive Divergences"></a>Training Energy-Based Models with Diffusion Contrastive Divergences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01668">http://arxiv.org/abs/2307.01668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijian Luo, Hao Jiang, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Zhihua Zhang<br>for:This paper focuses on improving the efficiency and accuracy of Energy-Based Models (EBMs) for generative modeling, specifically addressing the trade-off between computational burden and validity in Contrastive Divergence (CD) training.methods:The authors propose a new family of Diffusion Contrastive Divergence (DCD) methods that replace the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, leading to more efficient and accurate training of EBMs.results:The proposed DCD methods outperform CD in terms of computational efficiency and accuracy, as demonstrated through extensive experiments on synthetic data modeling, high-dimensional image denoising and generation, and image generation. Specifically, the proposed DCD achieves better performance than CD on synthetic data learning and image denoising experiments, and is capable of training an EBM for generating the Celab-A $32\times 32$ dataset.<details>
<summary>Abstract</summary>
Energy-Based Models (EBMs) have been widely used for generative modeling. Contrastive Divergence (CD), a prevailing training objective for EBMs, requires sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which leads to an irreconcilable trade-off between the computational burden and the validity of the CD. Running MCMCs till convergence is computationally intensive. On the other hand, short-run MCMC brings in an extra non-negligible parameter gradient term that is difficult to handle. In this paper, we provide a general interpretation of CD, viewing it as a special instance of our proposed Diffusion Contrastive Divergence (DCD) family. By replacing the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, we propose a more efficient divergence. We show that the proposed DCDs are both more computationally efficient than the CD and are not limited to a non-negligible gradient term. We conduct intensive experiments, including both synthesis data modeling and high-dimensional image denoising and generation, to show the advantages of the proposed DCDs. On the synthetic data learning and image denoising experiments, our proposed DCD outperforms CD by a large margin. In image generation experiments, the proposed DCD is capable of training an energy-based model for generating the Celab-A $32\times 32$ dataset, which is comparable to existing EBMs.
</details>
<details>
<summary>摘要</summary>
energy-based models (EBMs) 已经广泛应用于生成模型。对比тив的游逸差（CD），一种广泛使用的EBMs 训练目标，需要从EBM中采样使用Markov Chain Monte Carlo方法（MCMC），这导致了计算束缚和CD 的有效性之间的一种不可调和的负担。在MCMC 到 converges 的过程中，计算束缚是计算昂贵的。另一方面，短跑MCMC 会带来一个难以处理的非可忽略的参数梯度项。在这篇论文中，我们提供了CD 的通用解释，视其为我们提议的噪声扩散对照法（DCD）家族的一个特例。通过将CD 中的朗格文动力换为其他EBM参数无关的扩散过程，我们提议了更有效的分化。我们表明，我们提议的DCD 比CD更高效，并且不受非可忽略的参数梯度项的限制。我们在对 synthetic data 学习和图像噪声除掉和生成等实验中进行了广泛的测试，并证明了我们的DCD 比CD 大幅提高性能。在图像生成实验中，我们的DCD 能够训练一个能够生成 Celab-A $32\times 32$ 数据集的能量基本模型，与现有EBMs 相当。
</details></li>
</ul>
<hr>
<h2 id="Sensors-and-Systems-for-Monitoring-Mental-Fatigue-A-systematic-review"><a href="#Sensors-and-Systems-for-Monitoring-Mental-Fatigue-A-systematic-review" class="headerlink" title="Sensors and Systems for Monitoring Mental Fatigue: A systematic review"></a>Sensors and Systems for Monitoring Mental Fatigue: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01666">http://arxiv.org/abs/2307.01666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prabin Sharma, Joanna C. Justus, Govinda R. Poudel<br>for: 本研究旨在提供一个批判性概念模型，描述关键实现感疲劳检测技术，并进行系统性审查 latest studies 使用生物感测器系统检测人类的疲劳状态。methods: 本研究使用系统性搜寻和评价57篇文献（N&#x3D;1082），主要使用电enzephalography（EEG）基于感应器追踪疲劳状态。results: 本研究发现EEG基于感应器可提供moderate至good的感传度来检测疲劳状态。另外，本研究发现高密度EEG感应器无法提供增加的优势。基于发现，本研究提供了一个批判性讨论，探讨将穿戴式EEG和环境感应器integragted into real-world monitoring。未来的工作需要进一步改进和适应这些技术，以便实现广泛的疲劳监控在自主和无人驾驶产业中。<details>
<summary>Abstract</summary>
Mental fatigue is a leading cause of motor vehicle accidents, medical errors, loss of workplace productivity, and student disengagements in e-learning environment. Development of sensors and systems that can reliably track mental fatigue can prevent accidents, reduce errors, and help increase workplace productivity. This review provides a critical summary of theoretical models of mental fatigue, a description of key enabling sensor technologies, and a systematic review of recent studies using biosensor-based systems for tracking mental fatigue in humans. We conducted a systematic search and review of recent literature which focused on detection and tracking of mental fatigue in humans. The search yielded 57 studies (N=1082), majority of which used electroencephalography (EEG) based sensors for tracking mental fatigue. We found that EEG-based sensors can provide a moderate to good sensitivity for fatigue detection. Notably, we found no incremental benefit of using high-density EEG sensors for application in mental fatigue detection. Given the findings, we provide a critical discussion on the integration of wearable EEG and ambient sensors in the context of achieving real-world monitoring. Future work required to advance and adapt the technologies toward widespread deployment of wearable sensors and systems for fatigue monitoring in semi-autonomous and autonomous industries is examined.
</details>
<details>
<summary>摘要</summary>
心理疲劳是主要导致机动车事故、医疗错误、工作场所产量下降和电子学习环境中学生失业的原因。开发可靠跟踪心理疲劳的感应器和系统可以预防事故、减少错误，并帮助提高工作场所产量。本文提供了心理疲劳理论模型的批判摘要、关键实现感应器技术的描述，以及在人类中使用感应器基于系统的评估。我们对最新的文献进行了系统性的搜索和评估，搜索结果共57篇论文（N=1082），大多数使用电encephalography（EEG）基于感应器跟踪心理疲劳。我们发现EEG基于感应器可以提供moderate至good的敏感性 для疲劳检测。另外，我们未发现使用高密度EEG感应器在心理疲劳检测中增加的优势。根据发现，我们提供了在实际监测中 integrating wearable EEG和 ambient sensor的批判讨论，以及将这些技术应用于自动化和半自动化业务的未来工作。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Transformers-for-On-Line-Handwritten-Signature-Verification"><a href="#Exploring-Transformers-for-On-Line-Handwritten-Signature-Verification" class="headerlink" title="Exploring Transformers for On-Line Handwritten Signature Verification"></a>Exploring Transformers for On-Line Handwritten Signature Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01663">http://arxiv.org/abs/2307.01663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pietro Melzi, Ruben Tolosana, Ruben Vera-Rodriguez, Paula Delgado-Santos, Giuseppe Stragapede, Julian Fierrez, Javier Ortega-Garcia</li>
<li>for: 这个研究旨在评估基于最新的Transformers架构的在线签名验证系统的可靠性。</li>
<li>methods: 研究人员使用四种不同的配置，其中两种使用基于Vanilla Transformer核心的Encoder，另外两种则已经在步行和活动识别任务上得到了成功。</li>
<li>results: 实验结果表明，使用Transformers架构可以提供高度可靠的在线签名验证系统。<details>
<summary>Abstract</summary>
The application of mobile biometrics as a user-friendly authentication method has increased in the last years. Recent studies have proposed novel behavioral biometric recognition systems based on Transformers, which currently outperform the state of the art in several application scenarios. On-line handwritten signature verification aims to verify the identity of subjects, based on their biometric signatures acquired using electronic devices such as tablets or smartphones. This paper investigates the suitability of architectures based on recent Transformers for on-line signature verification. In particular, four different configurations are studied, two of them rely on the Vanilla Transformer encoder, and the two others have been successfully applied to the tasks of gait and activity recognition. We evaluate the four proposed configurations according to the experimental protocol proposed in the SVC-onGoing competition. The results obtained in our experiments are promising, and promote the use of Transformers for on-line signature verification.
</details>
<details>
<summary>摘要</summary>
随着移动生物 метрик作为用户友好的验证方法的应用逐渐增加，最近的研究已经提出了基于转换器的新型行为生物метри克认证系统。在线手写签名验证目的是验证使用电子设备such as 平板或智能手机所获取的生物签名的真实性，以验证个体身份。本文研究了基于最新的转换器架构的在线手写签名验证。特别是，我们研究了四种不同的配置，其中两种使用 Vanilla Transformer 编码器，另外两种已经成功应用于步态和活动识别任务。我们按照SVC-onGoing competition的实验协议进行了测试，实验结果很俊朗，这些结果推荐使用转换器进行在线手写签名验证。
</details></li>
</ul>
<hr>
<h2 id="Task-Planning-Support-for-Arborists-and-Foresters-Comparing-Deep-Learning-Approaches-for-Tree-Inventory-and-Tree-Vitality-Assessment-Based-on-UAV-Data"><a href="#Task-Planning-Support-for-Arborists-and-Foresters-Comparing-Deep-Learning-Approaches-for-Tree-Inventory-and-Tree-Vitality-Assessment-Based-on-UAV-Data" class="headerlink" title="Task Planning Support for Arborists and Foresters: Comparing Deep Learning Approaches for Tree Inventory and Tree Vitality Assessment Based on UAV-Data"></a>Task Planning Support for Arborists and Foresters: Comparing Deep Learning Approaches for Tree Inventory and Tree Vitality Assessment Based on UAV-Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01651">http://arxiv.org/abs/2307.01651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas-Dario Troles, Richard Nieding, Sonia Simons, Ute Schmid</li>
<li>for:  This paper aims to optimize workflows and increase productivity for arborists and foresters who care for trees in urban areas and forests.</li>
<li>methods: The approach uses RGB and multispectral UAV data, as well as multispectral satellite data and soil moisture sensors, to create tree inventories and vitality assessments.</li>
<li>results: The approach generates helpful information and improves task planning for arborists and foresters, allowing them to better care for trees in urban areas and forests.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文是为了优化植物护理人员的工作流程和提高生产力而写的。</li>
<li>methods: 该方法使用RGB和多spectral UAV数据，以及多spectral卫星数据和土壤湿度传感器，创建树木目录和评估树木生长状况。</li>
<li>results: 该方法生成有用的信息，提高植物护理人员的日常任务规划，以便更好地照顾城市公园和森林中的树木。<details>
<summary>Abstract</summary>
Climate crisis and correlating prolonged, more intense periods of drought threaten tree health in cities and forests. In consequence, arborists and foresters suffer from increasing workloads and, in the best case, a consistent but often declining workforce. To optimise workflows and increase productivity, we propose a novel open-source end-to-end approach that generates helpful information and improves task planning of those who care for trees in and around cities. Our approach is based on RGB and multispectral UAV data, which is used to create tree inventories of city parks and forests and to deduce tree vitality assessments through statistical indices and Deep Learning. Due to EU restrictions regarding flying drones in urban areas, we will also use multispectral satellite data and fifteen soil moisture sensors to extend our tree vitality-related basis of data. Furthermore, Bamberg already has a georeferenced tree cadastre of around 15,000 solitary trees in the city area, which is also used to generate helpful information. All mentioned data is then joined and visualised in an interactive web application allowing arborists and foresters to generate individual and flexible evaluations, thereby improving daily task planning.
</details>
<details>
<summary>摘要</summary>
клима紧急情况和相关的长期、更加激烈的干旱期导致城市和森林树木健康受到威胁。因此，树木医生和森林管理员工作量增加，而且最好的情况下也是不断减少的工作力量。为了优化工作流程和提高生产力，我们提出了一种新的开源终端方法，通过使用RGB和多spectral UAV数据来生成有用的信息和改善树木照顾人员在城市和森林中的任务规划。我们的方法基于RGB和多spectral UAV数据，用于创建城市公园和森林的树木目录和树木质量评估通过统计指标和深度学习。由于欧盟对城市区域内飞行无人机的限制，我们还使用多spectral卫星数据和十五个土壤湿度传感器来扩展我们树木质量相关的数据基础。此外，巴姆贝格已经有精确地参考树木目录，包括城市区域内约15,000棵 solitary树木，这些数据也用于生成有用信息。所有提到的数据然后被联合并视觉化在一个交互式网页应用程序中， allowing树木医生和森林管理员在日常任务规划中提高效率。
</details></li>
</ul>
<hr>
<h2 id="In-Domain-Self-Supervised-Learning-Can-Lead-to-Improvements-in-Remote-Sensing-Image-Classification"><a href="#In-Domain-Self-Supervised-Learning-Can-Lead-to-Improvements-in-Remote-Sensing-Image-Classification" class="headerlink" title="In-Domain Self-Supervised Learning Can Lead to Improvements in Remote Sensing Image Classification"></a>In-Domain Self-Supervised Learning Can Lead to Improvements in Remote Sensing Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01645">http://arxiv.org/abs/2307.01645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivica Dimitrovski, Ivan Kitanovski, Nikola Simidjievski, Dragi Kocev</li>
<li>for: 本研究旨在探讨自监督学习（SSL）在遥感图像分类中的应用前景，尝试利用大量无标签数据来学习图像表示。</li>
<li>methods: 本研究使用Million AID数据集，通过形式ulated auxiliary tasks来生成 pseudo-标签，并使用ViT模型进行预训练和细化训练。</li>
<li>results: 实验结果显示，使用域内SSL预训练（iBOT框架和ViT模型）在14个多样化的遥感图像分类任务中表现更好，比supervised预训练使用ImageNet数据集。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) has emerged as a promising approach for remote sensing image classification due to its ability to leverage large amounts of unlabeled data. In contrast to traditional supervised learning, SSL aims to learn representations of data without the need for explicit labels. This is achieved by formulating auxiliary tasks that can be used to create pseudo-labels for the unlabeled data and learn pre-trained models. The pre-trained models can then be fine-tuned on downstream tasks such as remote sensing image scene classification. The paper analyzes the effectiveness of SSL pre-training using Million AID - a large unlabeled remote sensing dataset on various remote sensing image scene classification datasets as downstream tasks. More specifically, we evaluate the effectiveness of SSL pre-training using the iBOT framework coupled with Vision transformers (ViT) in contrast to supervised pre-training of ViT using the ImageNet dataset. The comprehensive experimental work across 14 datasets with diverse properties reveals that in-domain SSL leads to improved predictive performance of models compared to the supervised counterparts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ChildPlay-A-New-Benchmark-for-Understanding-Children’s-Gaze-Behaviour"><a href="#ChildPlay-A-New-Benchmark-for-Understanding-Children’s-Gaze-Behaviour" class="headerlink" title="ChildPlay: A New Benchmark for Understanding Children’s Gaze Behaviour"></a>ChildPlay: A New Benchmark for Understanding Children’s Gaze Behaviour</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01630">http://arxiv.org/abs/2307.01630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samy Tafasca, Anshul Gupta, Jean-Marc Odobez</li>
<li>for: 这个研究是为了预测儿童和成人之间的注视目标，以便更好地诊断developmental disorders。</li>
<li>methods: 我们提出了一个新的注视目标预测模型，利用latest geometry preserving depth inference methods和rich gaze information，以及一个新的 ChildPlay 数据集。</li>
<li>results: 我们的模型在benchmark datasets和 ChildPlay 上 achieved state of the art results，并且发现looking at faces prediction performance on children is much worse than on adults，可以通过 fine-tuning models using child gaze annotations 进一步提高。<details>
<summary>Abstract</summary>
Gaze behaviors such as eye-contact or shared attention are important markers for diagnosing developmental disorders in children. While previous studies have looked at some of these elements, the analysis is usually performed on private datasets and is restricted to lab settings. Furthermore, all publicly available gaze target prediction benchmarks mostly contain instances of adults, which makes models trained on them less applicable to scenarios with young children. In this paper, we propose the first study for predicting the gaze target of children and interacting adults. To this end, we introduce the ChildPlay dataset: a curated collection of short video clips featuring children playing and interacting with adults in uncontrolled environments (e.g. kindergarten, therapy centers, preschools etc.), which we annotate with rich gaze information. We further propose a new model for gaze target prediction that is geometrically grounded by explicitly identifying the scene parts in the 3D field of view (3DFoV) of the person, leveraging recent geometry preserving depth inference methods. Our model achieves state of the art results on benchmark datasets and ChildPlay. Furthermore, results show that looking at faces prediction performance on children is much worse than on adults, and can be significantly improved by fine-tuning models using child gaze annotations. Our dataset and models will be made publicly available.
</details>
<details>
<summary>摘要</summary>
“眼光行为如视线接触或共同注意是诊断儿童发展障碍的重要标志。过往的研究通常仅在私人数据库中进行分析，并仅限于实验室设置。而所有公开可用的眼光目标预测参考 benchmark 几乎都包含成年人，这使得在他们上训练的模型在儿童enario中变得更加不适用。在本文中，我们提出了第一个预测儿童和成年人之间的眼光目标的研究。为此，我们介绍了 ChildPlay 数据集：一个 curaated 的短视频剪辑集，该集包含儿童在不受控制的环境中玩耍和与成年人互动的短片，并将其注解为丰富的眼光信息。我们还提出了一新的眼光目标预测模型，它在Scene parts 的3D 视野（3DFoV）中Explicitly 识别场景元件，并仅仅利用最近的具有 geometry preserving 的深度推测方法。我们的模型在参考数据集和 ChildPlay 上实现了顶尖的结果，并且结果显示，对于儿童的视线预测性能与成年人相比，相对较差，并可以通过 Fine-tuning 模型使用儿童的眼光注解进行改善。我们的数据和模型将会公开可用。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Lie-Group-Symmetry-Transformations-with-Neural-Networks"><a href="#Learning-Lie-Group-Symmetry-Transformations-with-Neural-Networks" class="headerlink" title="Learning Lie Group Symmetry Transformations with Neural Networks"></a>Learning Lie Group Symmetry Transformations with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01583">http://arxiv.org/abs/2307.01583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/victoria-klein/learning-lie-group-symmetries">https://github.com/victoria-klein/learning-lie-group-symmetries</a></li>
<li>paper_authors: Alex Gabel, Victoria Klein, Riccardo Valperga, Jeroen S. W. Lamb, Kevin Webster, Rick Quax, Efstratios Gavves</li>
<li>for: 本研究旨在探讨和评估数据集中的对称性，以便更好地进行模型选择、生成模型和数据分析等。</li>
<li>methods: 本研究使用一种新的方法，可以自动找出数据集中未知的对称性，包括李群对称变换以外的其他对称变换。</li>
<li>results: 研究结果表明，该方法可以在不同的数据点 Parametrization 下成功地描述数据集的对称性，并且可以在不同的设置下进行数据分析和模型选择。<details>
<summary>Abstract</summary>
The problem of detecting and quantifying the presence of symmetries in datasets is useful for model selection, generative modeling, and data analysis, amongst others. While existing methods for hard-coding transformations in neural networks require prior knowledge of the symmetries of the task at hand, this work focuses on discovering and characterizing unknown symmetries present in the dataset, namely, Lie group symmetry transformations beyond the traditional ones usually considered in the field (rotation, scaling, and translation). Specifically, we consider a scenario in which a dataset has been transformed by a one-parameter subgroup of transformations with different parameter values for each data point. Our goal is to characterize the transformation group and the distribution of the parameter values. The results showcase the effectiveness of the approach in both these settings.
</details>
<details>
<summary>摘要</summary>
“检测和量化数据集中的对称性问题有助于选择模型、生成模型和数据分析等方面。现有的方法需要先知道任务的对称性，而这项工作则是发现和描述数据集中未知的对称性，即李群对称变换以外的其他对称性。特别是，我们考虑了一种情况，在这种情况下，数据集已经被一个一参数子群的变换所变换，每个数据点的参数值都不同。我们的目标是Characterizing the transformation group and the distribution of parameter values。结果显示了该方法在这两个设置下的效果。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Multi-scale-Approach-for-Speech-and-Dynamics-Synchrony-in-Talking-Head-Generation"><a href="#A-Comprehensive-Multi-scale-Approach-for-Speech-and-Dynamics-Synchrony-in-Talking-Head-Generation" class="headerlink" title="A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation"></a>A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03270">http://arxiv.org/abs/2307.03270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louisbearing/hmo-audio">https://github.com/louisbearing/hmo-audio</a></li>
<li>paper_authors: Louis Airale, Dominique Vaufreydaz, Xavier Alameda-Pineda</li>
<li>for: 这种研究旨在提高深度生成模型中的语音与表情动作的同步，以便生成更自然的人脸动作和语音同步。</li>
<li>methods: 该研究使用了多级音视频同步损失函数和多级权重网络来更好地处理语音和表情动作之间的短期和长期相关性。</li>
<li>results: 实验表明，使用该方法可以大幅提高人脸动作的自然性和多级音视频同步的质量，并且在不同时间尺度上都能够保持良好的同步。<details>
<summary>Abstract</summary>
Animating still face images with deep generative models using a speech input signal is an active research topic and has seen important recent progress. However, much of the effort has been put into lip syncing and rendering quality while the generation of natural head motion, let alone the audio-visual correlation between head motion and speech, has often been neglected. In this work, we propose a multi-scale audio-visual synchrony loss and a multi-scale autoregressive GAN to better handle short and long-term correlation between speech and the dynamics of the head and lips. In particular, we train a stack of syncer models on multimodal input pyramids and use these models as guidance in a multi-scale generator network to produce audio-aligned motion unfolding over diverse time scales. Our generator operates in the facial landmark domain, which is a standard low-dimensional head representation. The experiments show significant improvements over the state of the art in head motion dynamics quality and in multi-scale audio-visual synchrony both in the landmark domain and in the image domain.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用深度生成模型动画静止图像是一个活跃的研究领域，近年来有重要的进步。然而，许多努力都是 lip syncing 和图像质量的改进，而Audio-visual相关性和自然的头部运动 generation 则 часто被忽略。在这项工作中，我们提出了一种多级音频视频同步损失和多级自适应GAN，以更好地处理 speech 和头部动作之间的短期和长期相关性。具体来说，我们在 multimodal 输入PYRAMIDS 上训练了一 stack of syncer 模型，并将这些模型作为指导在一种多级生成网络中使用，以生成 audio-aligned 动作，从多个时间尺度中 unfolding。我们的生成器在 facial landmark 空间中运行，这是一个标准的低维度头部表示。实验显示，我们的方法可以在 head motion dynamics 质量和多级音频视频同步两个方面提供显著改进，并在 landmark 空间和图像空间都达到了领先水平。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-reconstruct-the-bubble-distribution-with-conductivity-maps-using-Invertible-Neural-Networks-and-Error-Diffusion"><a href="#Learning-to-reconstruct-the-bubble-distribution-with-conductivity-maps-using-Invertible-Neural-Networks-and-Error-Diffusion" class="headerlink" title="Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion"></a>Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02496">http://arxiv.org/abs/2307.02496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishant Kumar, Lukas Krause, Thomas Wondrak, Sven Eckert, Kerstin Eckert, Stefan Gumhold</li>
<li>for: 这个论文的目的是提高电解质水氢生产的环保性，并且解决在电解质水中产生的气泡对反应干扰、细胞效率和能源消耗的问题。</li>
<li>methods: 这个论文使用了外部磁场传感器测量电解质水中的磁场干扰，并解决了磁场干扰对电解质水中的气泡大小和位置的影响。</li>
<li>results: 研究表明，使用归一化神经网络（INN）可以重建电解质水中的电阻场图像，并且对于这种具有较高分辨率的电阻场图像，INN的性能远胜于提高质量。<details>
<summary>Abstract</summary>
Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles generated during the process hinder reactions, reduce cell efficiency, and increase energy consumption. Additionally, these gas bubbles cause changes in the conductivity inside the cell, resulting in corresponding variations in the induced magnetic field around the cell. Therefore, measuring these gas bubble-induced magnetic field fluctuations using external magnetic sensors and solving the inverse problem of Biot-Savart Law allows for estimating the conductivity in the cell and, thus, bubble size and location. However, determining high-resolution conductivity maps from only a few induced magnetic field measurements is an ill-posed inverse problem. To overcome this, we exploit Invertible Neural Networks (INNs) to reconstruct the conductivity field. Our qualitative results and quantitative evaluation using random error diffusion show that INN achieves far superior performance compared to Tikhonov regularization.
</details>
<details>
<summary>摘要</summary>
电解是绿色氢生产中不可或缺的过程，但在这个过程中生成的气泡会阻碍反应，降低细胞效率，并增加能源消耗。此外，这些气泡会导致细胞内的电导率发生变化，从而导致细胞外的磁场变化。因此，通过外部磁场传感器测量这些气泡引起的磁场变化，并解决反演比特-萨瓦尔律的逆问题，可以估算细胞内的电导率，并因此计算气泡的大小和位置。然而，从只有几个磁场测量获得高分辨率电导率地图是一个不定性问题。为了解决这个问题，我们利用归一化神经网络（INN）来重建电导率场。我们的资料和量化评估结果表明，INN在比特-萨瓦尔律regularization的情况下表现得更好。
</details></li>
</ul>
<hr>
<h2 id="EffSeg-Efficient-Fine-Grained-Instance-Segmentation-using-Structure-Preserving-Sparsity"><a href="#EffSeg-Efficient-Fine-Grained-Instance-Segmentation-using-Structure-Preserving-Sparsity" class="headerlink" title="EffSeg: Efficient Fine-Grained Instance Segmentation using Structure-Preserving Sparsity"></a>EffSeg: Efficient Fine-Grained Instance Segmentation using Structure-Preserving Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01545">http://arxiv.org/abs/2307.01545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cédric Picron, Tinne Tuytelaars</li>
<li>for: 提高实例分割精度和效率</li>
<li>methods: 使用Structure-Preserving Sparsity（SPS）方法，将活动特征、潜在特征和 dense 2D 索引地图分别存储，以保持2D空间结构</li>
<li>results: 与 RefineMask 相当的性能在 COCO 上，降低 FLOPs 71%，提高 FPS 29%<details>
<summary>Abstract</summary>
Many two-stage instance segmentation heads predict a coarse 28x28 mask per instance, which is insufficient to capture the fine-grained details of many objects. To address this issue, PointRend and RefineMask predict a 112x112 segmentation mask resulting in higher quality segmentations. Both methods however have limitations by either not having access to neighboring features (PointRend) or by performing computation at all spatial locations instead of sparsely (RefineMask). In this work, we propose EffSeg performing fine-grained instance segmentation in an efficient way by using our Structure-Preserving Sparsity (SPS) method based on separately storing the active features, the passive features and a dense 2D index map containing the feature indices. The goal of the index map is to preserve the 2D spatial configuration or structure between the features such that any 2D operation can still be performed. EffSeg achieves similar performance on COCO compared to RefineMask, while reducing the number of FLOPs by 71% and increasing the FPS by 29%. Code will be released.
</details>
<details>
<summary>摘要</summary>
多两个阶段实例分割头预测每个实例粗略的28x28 mask，这并不够 capture 多个 объек 的细节信息。为解决这个问题，PointRend 和 RefineMask 预测 112x112 分割mask，得到更高质量的分割结果。然而，这两种方法都有局限性，一是不能访问周围特征（PointRend），二是在所有空间位置上进行计算（RefineMask）。在这项工作中，我们提出了高效的 EffSeg，实现细化实例分割，使用我们的结构保持稀疏（SPS）方法，基于分开存储活动特征、游离特征和 dense 2D 索引地图，包含特征索引。索引地图的目的是保持 2D 空间配置或结构，以便任何 2D 操作仍可以进行。EffSeg 在 COCO 上与 RefineMask 的性能相似，而减少 FLOPs 的数量为 71%，并提高 FPS 的值为 29%。代码将被公开。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Video-Anomaly-Detection-with-Diffusion-Models-Conditioned-on-Compact-Motion-Representations"><a href="#Unsupervised-Video-Anomaly-Detection-with-Diffusion-Models-Conditioned-on-Compact-Motion-Representations" class="headerlink" title="Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations"></a>Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01533">http://arxiv.org/abs/2307.01533</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anilosmantur/conditioned_video_anomaly_diffusion">https://github.com/anilosmantur/conditioned_video_anomaly_diffusion</a></li>
<li>paper_authors: Anil Osman Tur, Nicola Dall’Asen, Cigdem Beyan, Elisa Ricci</li>
<li>for: 本研究旨在解决无监督视频异常检测（VAD）问题，即根据视频帧是否为正常或异常来分类，无需任何标签。</li>
<li>methods: 本方法使用条件扩散模型，输入数据是一个预训练网络提取的空间时间特征，条件是一个缩写视频段的运动和外观特征。</li>
<li>results: 我们的方法使用数据驱动的阈值，并将高重建错误视为异常事件。实验结果表明，我们的方法可以在两个大规模的VAD benchmark上提高异常检测性能，特别是在不同的数据集上表现更好，超过了当前state-of-the-art和基线方法。<details>
<summary>Abstract</summary>
This paper aims to address the unsupervised video anomaly detection (VAD) problem, which involves classifying each frame in a video as normal or abnormal, without any access to labels. To accomplish this, the proposed method employs conditional diffusion models, where the input data is the spatiotemporal features extracted from a pre-trained network, and the condition is the features extracted from compact motion representations that summarize a given video segment in terms of its motion and appearance. Our method utilizes a data-driven threshold and considers a high reconstruction error as an indicator of anomalous events. This study is the first to utilize compact motion representations for VAD and the experiments conducted on two large-scale VAD benchmarks demonstrate that they supply relevant information to the diffusion model, and consequently improve VAD performances w.r.t the prior art. Importantly, our method exhibits better generalization performance across different datasets, notably outperforming both the state-of-the-art and baseline methods. The code of our method is available at https://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion
</details>
<details>
<summary>摘要</summary>
本文目的是解决无监督视频异常检测（VAD）问题，即对每帧视频进行正常或异常分类，无需访问标签。为此，我们提出的方法使用条件扩散模型，其输入数据为预训练网络提取的空间时间特征，条件为视频段异常特征概括。我们的方法使用数据驱动的阈值和高重建错误作为异常事件指标。本研究是首次利用异常动作表示来实现VAD，并在两个大规模VAD标准数据集上进行了实验，证明它们为扩散模型提供有用信息，并在对比先前艺术方法时显著提高VAD性能。重要的是，我们的方法在不同数据集上表现更好的泛化性，特别是在比较先前和基eline方法时表现出优异。代码可以在https://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion上获取。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Richness-of-Learned-Compressed-Representation-of-Images-for-Semantic-Segmentation"><a href="#Exploiting-Richness-of-Learned-Compressed-Representation-of-Images-for-Semantic-Segmentation" class="headerlink" title="Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation"></a>Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01524">http://arxiv.org/abs/2307.01524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DL4Compression/Semantic_Segmentation_of_Driving_Videos_on_Learning_based_Image_Compression">https://github.com/DL4Compression/Semantic_Segmentation_of_Driving_Videos_on_Learning_based_Image_Compression</a></li>
<li>paper_authors: Ravi Kakaiya, Rakshith Sathish, Ramanathan Sethuraman, Debdoot Sheet</li>
<li>for: 这个论文目的是提出一种基于学习的压缩编码器，以减少自这些汽车与高级驾驶辅助系统（ADAS）中的数据传输网络延迟。</li>
<li>methods: 这个方法使用学习基于的压缩编码器，以减少从汽车给云端服务器的数据传输网络延迟。实验 validate 了提案的管道，并证明了学习压缩表示可以用于进行像分类 segmentation 的任务，同时实现了 $66 \times$ 的压缩因子，并保留了适用于分类的信息，而且降低了总 Compute 的 $11%$。</li>
<li>results: 这个研究的结果显示，使用学习基于的压缩编码器可以实现高效的数据传输和分类 tasks，并且可以降低从汽车至云端服务器的网络延迟。<details>
<summary>Abstract</summary>
Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the potential to radically change the way we travel. Many such vehicles currently rely on segmentation and object detection algorithms to detect and track objects around its surrounding. The data collected from the vehicles are often sent to cloud servers to facilitate continual/life-long learning of these algorithms. Considering the bandwidth constraints, the data is compressed before sending it to servers, where it is typically decompressed for training and analysis. In this work, we propose the use of a learning-based compression Codec to reduce the overhead in latency incurred for the decompression operation in the standard pipeline. We demonstrate that the learned compressed representation can also be used to perform tasks like semantic segmentation in addition to decompression to obtain the images. We experimentally validate the proposed pipeline on the Cityscapes dataset, where we achieve a compression factor up to $66 \times$ while preserving the information required to perform segmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved using decompressed images while reducing the overall compute by $11\%$.
</details>
<details>
<summary>摘要</summary>
自动驾驶车和高级驾驶助手系统（ADAS）有可能对我们的旅行方式产生重大变革。许多这些车辆目前使用分割和物体检测算法来检测和跟踪周围的对象。收集到的数据通常将被送往云服务器以便持续学习这些算法。由于带宽限制，这些数据通常会进行压缩，然后在服务器上解压缩以进行训练和分析。在这项工作中，我们提议使用学习基于的压缩编码器来减少标准管道中的延迟过程中的开销。我们示出了学习压缩表示可以同时完成压缩和减少计算的任务，例如 semantic segmentation。我们对Cityscapes dataset进行实验，并实现了最高的66倍压缩因子，保留了用于完成分割的信息，而计算总量减少了11%。
</details></li>
</ul>
<hr>
<h2 id="LPN-Language-guided-Prototypical-Network-for-few-shot-classification"><a href="#LPN-Language-guided-Prototypical-Network-for-few-shot-classification" class="headerlink" title="LPN: Language-guided Prototypical Network for few-shot classification"></a>LPN: Language-guided Prototypical Network for few-shot classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01515">http://arxiv.org/abs/2307.01515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaihui Cheng, Chule Yang</li>
<li>for: 这篇论文主要针对几何shot分类问题进行研究，旨在对新任务进行适应，并充分利用可用的数据。</li>
<li>methods: 本文提出了一个名为Language-guided Prototypical Network（LPN）的方法，它利用视觉和语言模式的共同作用，通过两个平行分支来实现这一目的。</li>
<li>results: 实验结果显示，LPN方法与现有方法相比，在 benchmark 数据集上表现竞争力强。<details>
<summary>Abstract</summary>
Few-shot classification aims to adapt to new tasks with limited labeled examples. To fully use the accessible data, recent methods explore suitable measures for the similarity between the query and support images and better high-dimensional features with meta-training and pre-training strategies. However, the potential of multi-modality information has barely been explored, which may bring promising improvement for few-shot classification. In this paper, we propose a Language-guided Prototypical Network (LPN) for few-shot classification, which leverages the complementarity of vision and language modalities via two parallel branches. Concretely, to introduce language modality with limited samples in the visual task, we leverage a pre-trained text encoder to extract class-level text features directly from class names while processing images with a conventional image encoder. Then, a language-guided decoder is introduced to obtain text features corresponding to each image by aligning class-level features with visual features. In addition, to take advantage of class-level features and prototypes, we build a refined prototypical head that generates robust prototypes in the text branch for follow-up measurement. Finally, we aggregate the visual and text logits to calibrate the deviation of a single modality. Extensive experiments demonstrate the competitiveness of LPN against state-of-the-art methods on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
《语言指导的原型网络（LPN） для少量类别分类》目标是在新任务上适应，以限量标注示例进行训练。为了充分利用可用数据，当前方法探索适合查询和支持图像之间的相似度度量和高维特征的meta-training和预训练策略。然而，多Modal信息的潜力尚未得到充分利用，这可能带来了promising的改进。在这篇论文中，我们提议一种语言指导的原型网络（LPN），该网络利用视觉和语言Modal的 complementarity，通过两个平行分支来进行图像分类。具体来说，为了在视觉任务中使用有限的语言样本，我们利用预训练的文本编码器提取类别特征直接从类名称中，然后将这些特征与图像进行处理。接着，我们引入语言指导的解码器，以获取每个图像的特有的文本特征。此外，为了利用类别特征和原型，我们构建了一个精细的原型头，该头在文本分支中生成了robust的原型。最后，我们将视觉和文本的启示拼接起来，以调整单一模式的偏差。广泛的实验表明，LPN在标准 benchmark 数据集上与当前状态OF-the-art方法竞争。
</details></li>
</ul>
<hr>
<h2 id="SelfFed-Self-supervised-Federated-Learning-for-Data-Heterogeneity-and-Label-Scarcity-in-IoMT"><a href="#SelfFed-Self-supervised-Federated-Learning-for-Data-Heterogeneity-and-Label-Scarcity-in-IoMT" class="headerlink" title="SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT"></a>SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01514">http://arxiv.org/abs/2307.01514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunder Ali Khowaja, Kapal Dev, Syed Muhammad Anwar, Marius George Linguraru</li>
<li>for: 这个研究旨在提出一个基于自适应学习的联邦学习框架，以解决联邦学习中的数据不均匀问题和标签稀缺问题。</li>
<li>methods: 我们提出了一个两阶段的方法，首先是预训练阶段，使用Swin Transformer基本Encoder在分散式的方式下进行增强模型。第二阶段是精度调整阶段，引入了对照网络和一个新的联合策略，用于在分散式的方式下训练仅有几个标签的目标任务。</li>
<li>results: 我们在公开可用的医疗影像数据集上进行实验分析，结果显示我们的提出的SelfFed框架在非相同和相同数据集上比 existed baseline perform 更好，尤其是在标签稀缺的情况下。我们的方法在非IID数据集上取得最大改进率为8.8%和4.1%。此外，我们的方法甚至在仅使用10%标签的情况下也能够超越 existed baseline。<details>
<summary>Abstract</summary>
Self-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem. We perform our experimental analysis on publicly available medical imaging datasets and show that our proposed SelfFed framework performs better when compared to existing baselines concerning non-independent and identically distributed (IID) data and label scarcity. Our method achieves a maximum improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID dataset. Further, our proposed method outperforms existing baselines even when trained on a few (10%) labeled instances.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified ChineseSelf-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem. We perform our experimental analysis on publicly available medical imaging datasets and show that our proposed SelfFed framework performs better when compared to existing baselines concerning non-independent and identically distributed (IID) data and label scarcity. Our method achieves a maximum improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID dataset. Further, our proposed method outperforms existing baselines even when trained on a few (10%) labeled instances.中文简体版：自我超级vised学习在联邦学习模式下受到了互联网医疗器件（IoMT）领域的广泛关注，因为它可以在无标签的隔离数据上进行协同学习。然而，基于自我超级vised的联邦学习策略受到了数据不一致和标签稀缺的问题的影响。在这篇论文中，我们提出了SelfFed框架，用于解决这些问题。我们的SelfFed框架包括两个阶段：首先是预训练阶段，使用SwinTransformer基于 encoder 进行增强模型化，在分布式方式下进行。这个阶段可以减轻数据不一致问题。第二个阶段是精度调整阶段，引入对比网络和一种新的聚合策略，通过限量标签数据进行定制。这个阶段可以解决标签稀缺问题。我们在公共可用的医学成像数据集上进行了实验分析，并证明了我们的SelfFed框架在非独立和同分布（IID）数据上比较出色。我们的方法在非IID数据上最大提高8.8%和4.1%的提升。此外，我们的提posed方法还可以在只有10%的标签实例上进行训练，并且在这些情况下也能够超越现有的基elines。
</details></li>
</ul>
<hr>
<h2 id="FB-OCC-3D-Occupancy-Prediction-based-on-Forward-Backward-View-Transformation"><a href="#FB-OCC-3D-Occupancy-Prediction-based-on-Forward-Backward-View-Transformation" class="headerlink" title="FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation"></a>FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01492">http://arxiv.org/abs/2307.01492</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvlabs/fb-bev">https://github.com/nvlabs/fb-bev</a></li>
<li>paper_authors: Zhiqi Li, Zhiding Yu, David Austin, Mingsheng Fang, Shiyi Lan, Jan Kautz, Jose M. Alvarez</li>
<li>for: 本研究旨在提出一个获胜解决方案来3D占用预测挑战，该挑战是CVPR 2023 Workshop on End-to-End Autonomous Driving和CVPR 23 Workshop on Vision-Centric Autonomous Driving Workshop的一部分。</li>
<li>methods: 本研究基于FB-BEV，一个前进后退投影的镜头基础预测设计。在FB-BEV的基础上，我们进一步研究了特有的设计和优化，包括共同深度-Semantic预训练、共同矩阵-BEV表示、模型缩放和有效的后处理策略。</li>
<li>results: 这些设计和优化导致在nuScenes数据集上的mIoU分数为54.19%，在挑战赛道上排名第一。代码和模型将在：<a target="_blank" rel="noopener" href="https://github.com/NVlabs/FB-BEV">https://github.com/NVlabs/FB-BEV</a> 中发布。<details>
<summary>Abstract</summary>
This technical report summarizes the winning solution for the 3D Occupancy Prediction Challenge, which is held in conjunction with the CVPR 2023 Workshop on End-to-End Autonomous Driving and CVPR 23 Workshop on Vision-Centric Autonomous Driving Workshop. Our proposed solution FB-OCC builds upon FB-BEV, a cutting-edge camera-based bird's-eye view perception design using forward-backward projection. On top of FB-BEV, we further study novel designs and optimization tailored to the 3D occupancy prediction task, including joint depth-semantic pre-training, joint voxel-BEV representation, model scaling up, and effective post-processing strategies. These designs and optimization result in a state-of-the-art mIoU score of 54.19% on the nuScenes dataset, ranking the 1st place in the challenge track. Code and models will be released at: https://github.com/NVlabs/FB-BEV.
</details>
<details>
<summary>摘要</summary>
这份技术报告介绍了在CVPR 2023 工作坊上的3D占用预测挑战赛中获胜的解决方案，该挑战赛与CVPR 23 工作坊联合举行。我们的提议方案FB-OCC基于FB-BEV，这是一种前瞻型镜头视图识别设计，使用前进和后退投影。在FB-BEV的基础之上，我们进一步研究了适应3D占用预测任务的新设计和优化策略，包括共同深度semantic预训练、共同矩阵BEV表示、模型缩放和有效 posterior 处理策略。这些设计和优化使得我们在nuScenes数据集上 achieve 54.19%的mIoU分数，在挑战赛中名列第一名。代码和模型将在：https://github.com/NVlabs/FB-BEV 中发布。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Segmentation-on-3D-Point-Clouds-with-High-Density-Variations"><a href="#Semantic-Segmentation-on-3D-Point-Clouds-with-High-Density-Variations" class="headerlink" title="Semantic Segmentation on 3D Point Clouds with High Density Variations"></a>Semantic Segmentation on 3D Point Clouds with High Density Variations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01489">http://arxiv.org/abs/2307.01489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Faulkner, Luke Haub, Simon Ratcliffe, Ian Reid, Tat-Jun Chin</li>
<li>for: 这篇论文是为了解决 LiDAR 探测应用中的大规模 3D 点云对应问题，这些点云具有广泛的区域和距离，并且具有巨大的地方密度变化。</li>
<li>methods: 这篇论文提出了一个名为 HDVNet 的新架构，这个架构包含一个嵌入的集合 Encoder-Decoder 路径，每个路径处理特定的点密度范围。限制Feature Map 之间的连接，使得 HDVNet 能够根据点密度来衡量每个特征的可靠性，例如，对于低密度物体而言，杜重高密度的特征。</li>
<li>results: 在实际的点云数据中，HDVNet 比 state-of-the-art 模型具有更高的准确性，只需使用一半的 Parameters。<details>
<summary>Abstract</summary>
LiDAR scanning for surveying applications acquire measurements over wide areas and long distances, which produces large-scale 3D point clouds with significant local density variations. While existing 3D semantic segmentation models conduct downsampling and upsampling to build robustness against varying point densities, they are less effective under the large local density variations characteristic of point clouds from surveying applications. To alleviate this weakness, we propose a novel architecture called HDVNet that contains a nested set of encoder-decoder pathways, each handling a specific point density range. Limiting the interconnections between the feature maps enables HDVNet to gauge the reliability of each feature based on the density of a point, e.g., downweighting high density features not existing in low density objects. By effectively handling input density variations, HDVNet outperforms state-of-the-art models in segmentation accuracy on real point clouds with inconsistent density, using just over half the weights.
</details>
<details>
<summary>摘要</summary>
李达尔扫描 для测量应用程序获取大面积和长距离的测量值，生成大规模的3D点云，点云中存在显著的地方密度变化。现有的3D semantic segmentation模型通过下采样和上采样来建立对点云密度变化的鲁棒性，但这些模型对大规模的点云密度变化不够有效。为解决这个弱点，我们提出了一种新的架构 called HDVNet，它包含一个嵌入式的编码器-解码器路径，每个路径处理特定的点密度范围。限制Feature map之间的连接，使得 HDVNet 可以根据点的密度来评估特征的可靠性，例如，减重高密度特征不存在低密度对象中。通过有效地处理输入密度变化，HDVNet 在实际点云中 segments 精度高于状态机制模型，使用只有一半的权重。
</details></li>
</ul>
<hr>
<h2 id="H-DenseFormer-An-Efficient-Hybrid-Densely-Connected-Transformer-for-Multimodal-Tumor-Segmentation"><a href="#H-DenseFormer-An-Efficient-Hybrid-Densely-Connected-Transformer-for-Multimodal-Tumor-Segmentation" class="headerlink" title="H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation"></a>H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01486">http://arxiv.org/abs/2307.01486</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shijun18/h-denseformer">https://github.com/shijun18/h-denseformer</a></li>
<li>paper_authors: Jun Shi, Hongyu Kan, Shulan Ruan, Ziqi Zhu, Minfan Zhao, Liang Qiao, Zhaohui Wang, Hong An, Xudong Xue</li>
<li>for: 这篇论文旨在提出一个混合式密接连接网络（H-DenseFormer）来进行肿瘤分类，以提高多modalities的医疗影像识别能力。</li>
<li>methods: 这篇论文使用了一个具有多路平行嵌入（MPE）模组，可以将多modalities的输入资料组合成多modalities的融合特征。然后，这些融合特征会被传递到不同层次的encoder中进行增强多modalities的学习表现。此外，还设计了一个轻量级的密接连接几何（DCT）封顶，以取代标准几何封顶，从而实现显著的计算量削减。</li>
<li>results: 在HECKTOR21和PI-CAI22两个公共多modalities datasets上进行了广泛的实验，结果显示，我们的提案方法在与现有的州前方法进行比较中，具有更高的识别率和更低的计算量。代码可以在<a target="_blank" rel="noopener" href="https://github.com/shijun18/H-DenseFormer%E4%B8%8A%E5%8F%96%E5%BE%97%E3%80%82">https://github.com/shijun18/H-DenseFormer上取得。</a><details>
<summary>Abstract</summary>
Recently, deep learning methods have been widely used for tumor segmentation of multimodal medical images with promising results. However, most existing methods are limited by insufficient representational ability, specific modality number and high computational complexity. In this paper, we propose a hybrid densely connected network for tumor segmentation, named H-DenseFormer, which combines the representational power of the Convolutional Neural Network (CNN) and the Transformer structures. Specifically, H-DenseFormer integrates a Transformer-based Multi-path Parallel Embedding (MPE) module that can take an arbitrary number of modalities as input to extract the fusion features from different modalities. Then, the multimodal fusion features are delivered to different levels of the encoder to enhance multimodal learning representation. Besides, we design a lightweight Densely Connected Transformer (DCT) block to replace the standard Transformer block, thus significantly reducing computational complexity. We conduct extensive experiments on two public multimodal datasets, HECKTOR21 and PI-CAI22. The experimental results show that our proposed method outperforms the existing state-of-the-art methods while having lower computational complexity. The source code is available at https://github.com/shijun18/H-DenseFormer.
</details>
<details>
<summary>摘要</summary>
近些年来，深度学习方法在多Modal医学影像肿瘤分割方面取得了可观的成果。然而，现有的方法受到表达能力不充分、特定的Modal数量和高计算复杂性的限制。在这篇论文中，我们提出了一种混合 densely connected network  для肿瘤分割，称为 H-DenseFormer，它将 CNN 和 Transformer 结构相结合。具体来说，H-DenseFormer 使用 Transformer 结构基于多Path Parallel Embedding（MPE）模块，可以将任意数量的Modalities作为输入，以提取不同Modalities中的融合特征。然后，多Modal融合特征被传递到不同层次的编码器，以增强多Modal学习表达。此外，我们设计了一种轻量级的 Densely Connected Transformer（DCT）块，以取代标准 Transformer 块，从而实现显著降低计算复杂性。我们对公共的两个多Modal 数据集进行了广泛的实验，结果表明，我们的提议方法在计算复杂性下与现有的状态态势表达方法相比，表现出色，并且可以在多Modal 数据集上达到更高的识别率。代码可以在 https://github.com/shijun18/H-DenseFormer 上找到。
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Driver-Gaze-Estimation-and-Application-in-Gaze-Behavior-Understanding"><a href="#A-Review-of-Driver-Gaze-Estimation-and-Application-in-Gaze-Behavior-Understanding" class="headerlink" title="A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding"></a>A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01470">http://arxiv.org/abs/2307.01470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Kumar Sharma, Pranamesh Chakraborty</li>
<li>for: 本研究的主要目标是对 Driver Gaze 基础知识、测算方法和应用场景进行全面的概述，以便更好地理解和应用 Driver Gaze 技术。</li>
<li>methods: 本研究主要使用 Head-mounted 和远程设置基于眼动估计的方法，并详细介绍了现有的 Driver Gaze 数据集和估计算法，包括传统机器学习和深度学习等技术。</li>
<li>results: 本研究使用 Driver Gaze 估计结果来理解 drivers 在拐弯、加速和减速时的眼动行为，以及道路投放广告结构的影响。同时，本研究也提出了现有文献的限制、挑战和未来发展方向。<details>
<summary>Abstract</summary>
Driver gaze plays an important role in different gaze-based applications such as driver attentiveness detection, visual distraction detection, gaze behavior understanding, and building driver assistance system. The main objective of this study is to perform a comprehensive summary of driver gaze fundamentals, methods to estimate driver gaze, and it's applications in real world driving scenarios. We first discuss the fundamentals related to driver gaze, involving head-mounted and remote setup based gaze estimation and the terminologies used for each of these data collection methods. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and the equipment used for such data collection. This is followed by a discussion of the algorithms used for driver gaze estimation, which primarily involves traditional machine learning and deep learning based techniques. The estimated driver gaze is then used for understanding gaze behavior while maneuvering through intersections, on-ramps, off-ramps, lane changing, and determining the effect of roadside advertising structures. Finally, we have discussed the limitations in the existing literature, challenges, and the future scope in driver gaze estimation and gaze-based applications.
</details>
<details>
<summary>摘要</summary>
Driver's gaze plays an important role in various gaze-based applications such as driver attentiveness detection, visual distraction detection, and understanding gaze behavior. The main objective of this study is to provide a comprehensive summary of driver gaze fundamentals, methods for estimating driver gaze, and its applications in real-world driving scenarios.First, we discuss the fundamentals of driver gaze, including head-mounted and remote setup-based gaze estimation, and the terminologies used for each of these data collection methods. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and equipment used for such data collection.Then, we discuss the algorithms used for driver gaze estimation, which primarily involve traditional machine learning and deep learning-based techniques. The estimated driver gaze is used for understanding gaze behavior while maneuvering through intersections, on-ramps, off-ramps, lane changing, and determining the effect of roadside advertising structures.Finally, we discuss the limitations in the existing literature, challenges, and the future scope in driver gaze estimation and gaze-based applications.
</details></li>
</ul>
<hr>
<h2 id="Generating-Animatable-3D-Cartoon-Faces-from-Single-Portraits"><a href="#Generating-Animatable-3D-Cartoon-Faces-from-Single-Portraits" class="headerlink" title="Generating Animatable 3D Cartoon Faces from Single Portraits"></a>Generating Animatable 3D Cartoon Faces from Single Portraits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01468">http://arxiv.org/abs/2307.01468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanyu Pan, Guowei Yang, Taijiang Mu, Yu-Kun Lai</li>
<li>for: 本研究旨在提供一种生成可animatable 3D漫画人脸的新方法，以满足现代虚拟现实技术的需求。</li>
<li>methods: 我们提出了一种两stage reconstruction方法，首先使用StyleGAN将输入的真实世界照片转换为 стили化漫画图像，然后通过非rigid deformation以准确地还原3D漫画人脸的细节xture。最后，我们提出了一种基于手动创建的模板和变形传递的semantic preserving face rigging方法。</li>
<li>results: 相比之前的研究，我们的方法可以更好地保持人脸的准确性、美观性和相似性标准。此外，我们还实现了在实时进行人脸动画的能力。<details>
<summary>Abstract</summary>
With the booming of virtual reality (VR) technology, there is a growing need for customized 3D avatars. However, traditional methods for 3D avatar modeling are either time-consuming or fail to retain similarity to the person being modeled. We present a novel framework to generate animatable 3D cartoon faces from a single portrait image. We first transfer an input real-world portrait to a stylized cartoon image with a StyleGAN. Then we propose a two-stage reconstruction method to recover the 3D cartoon face with detailed texture, which first makes a coarse estimation based on template models, and then refines the model by non-rigid deformation under landmark supervision. Finally, we propose a semantic preserving face rigging method based on manually created templates and deformation transfer. Compared with prior arts, qualitative and quantitative results show that our method achieves better accuracy, aesthetics, and similarity criteria. Furthermore, we demonstrate the capability of real-time facial animation of our 3D model.
</details>
<details>
<summary>摘要</summary>
随着虚拟现实（VR）技术的发展，个性化3D人物模型的需求在增长。然而，传统的3D人物模型方法 Either time-consuming or failure to retain the similarity of the person being modeled. We present a novel framework to generate animatable 3D cartoon faces from a single portrait image. We first transfer an input real-world portrait to a stylized cartoon image with a StyleGAN. Then we propose a two-stage reconstruction method to recover the 3D cartoon face with detailed texture, which first makes a coarse estimation based on template models, and then refines the model by non-rigid deformation under landmark supervision. Finally, we propose a semantic preserving face rigging method based on manually created templates and deformation transfer. Compared with prior arts, qualitative and quantitative results show that our method achieves better accuracy, aesthetics, and similarity criteria. Furthermore, we demonstrate the capability of real-time facial animation of our 3D model.Here's a breakdown of the translation:* "随着" (suī zhe) - "with the development of"* "虚拟现实" (hū zhì shèng jì) - "virtual reality"* "技术" (jì shu) - "technology"* "个性化" (ge xìng yào) - "customized"* "3D人物模型" (3D rén wù molding) - "3D avatar model"* "需求" (xū yè) - "need"* "在增长" (zài jìn cháng) - "is growing"* "传统的" (chuán tǒng de) - "traditional"* "3D人物模型方法" (3D rén wù molding fāng fá) - "3D avatar modeling methods"* "Either time-consuming or failure to retain the similarity of the person being modeled" - This phrase is translated as "或者时间耗费或失去人物相似性" (or zhī shí hòu fèi or shī qù rén wù xiàng yì)* "We present a novel framework to generate animatable 3D cartoon faces from a single portrait image" - This phrase is translated as "我们提出了一种新的框架，可以从单一的肖像图片中生成可动的3D漫画人脸" (wǒ men tī shè le yī zhōng xīn de kāng yì, cóng zhī yī zhōng shèng cháng yǐn dào yī zhōng)* "We first transfer an input real-world portrait to a stylized cartoon image with a StyleGAN" - This phrase is translated as "我们首先将输入的真实世界肖像图片转化为一个风格化的漫画人脸，使用StyleGAN" (wǒ men chū xiān shì yǐn zhī shì jīn shì, cóng zhī yī zhōng shèng cháng yǐn dào yī zhōng)* "Then we propose a two-stage reconstruction method to recover the 3D cartoon face with detailed texture" - This phrase is translated as "然后我们提出了一种两阶段重建方法，以获取3D漫画人脸的详细TEXTURE" (rán hái wǒ men tī shè le yī zhōng èr jiāng tiě jīng fāng fá, yǐn dào jīn shì sān jiāng)* "which first makes a coarse estimation based on template models, and then refines the model by non-rigid deformation under landmark supervision" - This phrase is translated as "首先基于模板模型进行大致的估计，然后通过非固定形态的扭变以下领别监督进行细化" (chū shí zhī shì yǐn zhī shì, cóng zhī yī zhōng shèng cháng yǐn dào)* "Finally, we propose a semantic preserving face rigging method based on manually created templates and deformation transfer" - This phrase is translated as "最后，我们提出了一种基于手动创建的模板和形态传递的 semantics保持的人脸动画方法" (wǒ men tī shè le yī zhōng xiān shì yǐn zhī shì, cóng zhī yī zhōng shèng cháng yǐn dào)* "Compared with prior arts, qualitative and quantitative results show that our method achieves better accuracy, aesthetics, and similarity criteria" - This phrase is translated as "与之前的艺术比较，我们的方法在准确性、艺术性和相似性标准上都达到了更高的水平" (yǔ zhī qián de yì shù bǐ kě, wǒ men de fāng fá zài jìn cháng yǐn dào zhèng zhì, xiàng yì yì shù)* "Furthermore, we demonstrate the capability of real-time facial animation of our 3D model" - This phrase is translated as "此外，我们还展示了我们的3D模型在实时动画方面的能力" (qí wài, wǒ men hái jiǎng yǐn dào)
</details></li>
</ul>
<hr>
<h2 id="Technical-Report-for-Ego4D-Long-Term-Action-Anticipation-Challenge-2023"><a href="#Technical-Report-for-Ego4D-Long-Term-Action-Anticipation-Challenge-2023" class="headerlink" title="Technical Report for Ego4D Long Term Action Anticipation Challenge 2023"></a>Technical Report for Ego4D Long Term Action Anticipation Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01467">http://arxiv.org/abs/2307.01467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatsuya Ishibashi, Kosuke Ono, Noriyuki Kugo, Yuji Sato</li>
<li>for: 预测未来动作序列</li>
<li>methods: 基于SlowFast和SlowFast-CLIP模型的ensemble，加入 Label smoothing 和动作类别（动词、名称）的约束</li>
<li>results: 超越基线性能，在公共领先板上记录第二名成绩<details>
<summary>Abstract</summary>
In this report, we describe the technical details of our approach for the Ego4D Long-Term Action Anticipation Challenge 2023. The aim of this task is to predict a sequence of future actions that will take place at an arbitrary time or later, given an input video. To accomplish this task, we introduce three improvements to the baseline model, which consists of an encoder that generates clip-level features from the video, an aggregator that integrates multiple clip-level features, and a decoder that outputs Z future actions. 1) Model ensemble of SlowFast and SlowFast-CLIP; 2) Label smoothing to relax order constraints for future actions; 3) Constraining the prediction of the action class (verb, noun) based on word co-occurrence. Our method outperformed the baseline performance and recorded as second place solution on the public leaderboard.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我们介绍了我们对Ego4D长期动作预测挑战2023年的技术细节。该任务的目标是基于输入视频预测未来动作的序列，我们在这里引入了三种改进基eline模型，即encoder生成视频clip级别特征，aggregator将多个clip级别特征集成，以及decoder输出Z个未来动作。1）SlowFast和SlowFast-CLIP模型集成；2）将未来动作顺序约束松弛为label smoothing；3）基于单词共occurrence constrain动作类别（动词、名词）预测。我们的方法比基eline表现更好，在公共排名板上录制第二名。
</details></li>
</ul>
<hr>
<h2 id="AdAM-Few-Shot-Image-Generation-via-Adaptation-Aware-Kernel-Modulation"><a href="#AdAM-Few-Shot-Image-Generation-via-Adaptation-Aware-Kernel-Modulation" class="headerlink" title="AdAM: Few-Shot Image Generation via Adaptation-Aware Kernel Modulation"></a>AdAM: Few-Shot Image Generation via Adaptation-Aware Kernel Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01465">http://arxiv.org/abs/2307.01465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunqing Zhao, Keshigeyan Chandrasegaran, Abdollahzadeh Milad, Chao Du, Tianyu Pang, Ruoteng Li, Henghui Ding, Ngai-Man Cheung</li>
<li>for: 这个论文目的是解决几个示例（例如10）训练样本的图像生成问题。</li>
<li>methods: 这个论文使用了一个已经在大规模源频道上预训练的GAN，并将其适应到目标频道。Central to recent FSIG methods are knowledge preservation criteria, which select and preserve a subset of source knowledge to the adapted model.</li>
<li>results: 我们的研究表明，许多现有的State-of-the-art（SOTA）方法，只考虑源频道，在不同的频道距离下进行适应，表现不佳。我们的第二个贡献是提出了适应aware的kernel Modulation（AdAM），用于通用的几个源-目标频道距离的图像生成。广泛的实验表明，AdAM可以在挑战性的设置下 consistently achieve SOTA performance。<details>
<summary>Abstract</summary>
Few-shot image generation (FSIG) aims to learn to generate new and diverse images given few (e.g., 10) training samples. Recent work has addressed FSIG by leveraging a GAN pre-trained on a large-scale source domain and adapting it to the target domain with few target samples. Central to recent FSIG methods are knowledge preservation criteria, which select and preserve a subset of source knowledge to the adapted model. However, a major limitation of existing methods is that their knowledge preserving criteria consider only source domain/task and fail to consider target domain/adaptation in selecting source knowledge, casting doubt on their suitability for setups of different proximity between source and target domain. Our work makes two contributions. Firstly, we revisit recent FSIG works and their experiments. We reveal that under setups which assumption of close proximity between source and target domains is relaxed, many existing state-of-the-art (SOTA) methods which consider only source domain in knowledge preserving perform no better than a baseline method. As our second contribution, we propose Adaptation-Aware kernel Modulation (AdAM) for general FSIG of different source-target domain proximity. Extensive experiments show that AdAM consistently achieves SOTA performance in FSIG, including challenging setups where source and target domains are more apart.
</details>
<details>
<summary>摘要</summary>
几个示例图像生成（FSIG）目标是通过几个（例如10）训练样本学习生成新和多样化的图像。 latest work has addressed FSIG by leveraging a pre-trained GAN on a large-scale source domain and adapting it to the target domain with few target samples. central to recent FSIG methods are knowledge preservation criteria, which select and preserve a subset of source knowledge to the adapted model. However, a major limitation of existing methods is that their knowledge preserving criteria consider only source domain/task and fail to consider target domain/adaptation in selecting source knowledge, casting doubt on their suitability for setups of different proximity between source and target domain. Our work makes two contributions. Firstly, we revisit recent FSIG works and their experiments. We reveal that under setups which assumption of close proximity between source and target domains is relaxed, many existing state-of-the-art (SOTA) methods which consider only source domain in knowledge preserving perform no better than a baseline method. As our second contribution, we propose Adaptation-Aware kernel Modulation (AdAM) for general FSIG of different source-target domain proximity. Extensive experiments show that AdAM consistently achieves SOTA performance in FSIG, including challenging setups where source and target domains are more apart.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Quality-Prediction-for-Improved-Single-Frame-and-Weighted-Sequential-Visual-Place-Recognition"><a href="#Unsupervised-Quality-Prediction-for-Improved-Single-Frame-and-Weighted-Sequential-Visual-Place-Recognition" class="headerlink" title="Unsupervised Quality Prediction for Improved Single-Frame and Weighted Sequential Visual Place Recognition"></a>Unsupervised Quality Prediction for Improved Single-Frame and Weighted Sequential Visual Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01464">http://arxiv.org/abs/2307.01464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helen Carson, Jason J. Ford, Michael Milford</li>
<li>for: 本研究旨在提高自动驾驶系统的定位和视觉定位技术的可靠性和预测性。</li>
<li>methods: 本研究使用一种新的训练自由的方法来预测定位估计的质量，并使用这些预测来偏补一种序列匹配过程，以实现更高的精度性能。</li>
<li>results: 在四个数据集和三种视觉定位技术上，我们的结合系统可以提高定位精度性能，特别是在高精度低匹配点操作点上。我们还提供了减少和分析，以分析预测系统和偏补序列匹配器的性能贡献。<details>
<summary>Abstract</summary>
While substantial progress has been made in the absolute performance of localization and Visual Place Recognition (VPR) techniques, it is becoming increasingly clear from translating these systems into applications that other capabilities like integrity and predictability are just as important, especially for safety- or operationally-critical autonomous systems. In this research we present a new, training-free approach to predicting the likely quality of localization estimates, and a novel method for using these predictions to bias a sequence-matching process to produce additional performance gains beyond that of a naive sequence matching approach. Our combined system is lightweight, runs in real-time and is agnostic to the underlying VPR technique. On extensive experiments across four datasets and three VPR techniques, we demonstrate our system improves precision performance, especially at the high-precision/low-recall operating point. We also present ablation and analysis identifying the performance contributions of the prediction and weighted sequence matching components in isolation, and the relationship between the quality of the prediction system and the benefits of the weighted sequential matcher.
</details>
<details>
<summary>摘要</summary>
“尽管当地化和视觉地标识（VPR）技术的绝对性表现已经取得了 significiant 进步，但是在将这些系统应用于实际应用中，其他特性如完整性和预测性也变得越来越重要，特别是 для安全或操作critical的自动驾驶系统。在这项研究中，我们提出了一种新的、无需训练的方法来预测当地化估计的质量，以及一种新的方法来使用这些预测来偏补一个序列匹配过程，以实现更高的性能提升。我们的整体系统轻量级、实时执行，并且对于下述VPR技术无关。在四个数据集和三种VPR技术的广泛实验中，我们证明了我们的系统可以提高精度性能，特别是在高精度/低回归点操作点。我们还提供了剥离和分析，描述预测和权重序列匹配组件在孤立情况下的性能贡献，以及预测系统质量和权重序列匹配的关系。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Practical-Collaborative-Perception-A-Framework-for-Asynchronous-and-Multi-Agent-3D-Object-Detection"><a href="#Practical-Collaborative-Perception-A-Framework-for-Asynchronous-and-Multi-Agent-3D-Object-Detection" class="headerlink" title="Practical Collaborative Perception: A Framework for Asynchronous and Multi-Agent 3D Object Detection"></a>Practical Collaborative Perception: A Framework for Asynchronous and Multi-Agent 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01462">http://arxiv.org/abs/2307.01462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minh-Quan Dao, Julie Stephany Berrio, Vincent Frémont, Mao Shan, Elwan Héry, Stewart Worrall</li>
<li>for: 提高 LiDAR-based 物体探测方法中的遮挡问题，特别是在城市交通中， где egocar 需要可靠的物体探测，以避免碰撞，而其视场受到了大量道路用户的阻挡。</li>
<li>methods: collaborative perception via Vehicle-to-Everything (V2X) communication，利用多个连接的代理机构形成完整的场景表示，并通过中间协作来解决性能和带宽之间的负担。</li>
<li>results: 我们提出了一种简单又有效的协作方法，可以在实际应用中超越先前的状态艺术方法，同时尽可能减少对单车辆检测模型的修改和假设不实的多个代理机构同步。实验结果表明，我们的协作方法可以达到98%的性能水平，只 consume相同的带宽，与先前的中间协作方法相比。<details>
<summary>Abstract</summary>
Occlusion is a major challenge for LiDAR-based object detection methods. This challenge becomes safety-critical in urban traffic where the ego vehicle must have reliable object detection to avoid collision while its field of view is severely reduced due to the obstruction posed by a large number of road users. Collaborative perception via Vehicle-to-Everything (V2X) communication, which leverages the diverse perspective thanks to the presence at multiple locations of connected agents to form a complete scene representation, is an appealing solution. State-of-the-art V2X methods resolve the performance-bandwidth tradeoff using a mid-collaboration approach where the Bird-Eye View images of point clouds are exchanged so that the bandwidth consumption is lower than communicating point clouds as in early collaboration, and the detection performance is higher than late collaboration, which fuses agents' output, thanks to a deeper interaction among connected agents. While achieving strong performance, the real-world deployment of most mid-collaboration approaches is hindered by their overly complicated architectures, involving learnable collaboration graphs and autoencoder-based compressor/ decompressor, and unrealistic assumptions about inter-agent synchronization. In this work, we devise a simple yet effective collaboration method that achieves a better bandwidth-performance tradeoff than prior state-of-the-art methods while minimizing changes made to the single-vehicle detection models and relaxing unrealistic assumptions on inter-agent synchronization. Experiments on the V2X-Sim dataset show that our collaboration method achieves 98\% of the performance of an early-collaboration method, while only consuming the equivalent bandwidth of a late-collaboration method.
</details>
<details>
<summary>摘要</summary>
干扰是LiDAR基于对象检测方法的主要挑战。在城市交通中， egovehicle 需要可靠的对象检测，以避免事故，而其视场受到了大量道路用户的干扰。 Collaborative perception via Vehicle-to-Everything (V2X) communication 是一种吸引人的解决方案，它利用了多个位置的连接代理人形成完整的场景表示，并且可以解决性能和带宽的贸易。现状的V2X方法在性能和带宽之间进行了中间协作，通过在 Bird-Eye View 图像上进行点云的交换，以避免在早期协作中的带宽浪费，并在晚期协作中进行拼接，以获得更高的检测性能。在实现优秀性能的同时，大多数中间协作方法的实际部署受到了其复杂的架构和学习可学的协作图、 autoencoder 基于的压缩器/解压缩器的假设，以及 между代理人的不realistic 同步假设。在这种工作中，我们设计了一种简单 yet 有效的协作方法，可以在带宽-性能贸易中提供更好的贸易比例，而且尽可能地避免改变单车检测模型，并放弃不realistic 的 между代理人同步假设。 V2X-Sim 数据集的实验表明，我们的协作方法可以达到98%的性能，只消耗与晚期协作相同的带宽。
</details></li>
</ul>
<hr>
<h2 id="Learning-Feature-Matching-via-Matchable-Keypoint-Assisted-Graph-Neural-Network"><a href="#Learning-Feature-Matching-via-Matchable-Keypoint-Assisted-Graph-Neural-Network" class="headerlink" title="Learning Feature Matching via Matchable Keypoint-Assisted Graph Neural Network"></a>Learning Feature Matching via Matchable Keypoint-Assisted Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01447">http://arxiv.org/abs/2307.01447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zizhuo Li, Jiayi Ma</li>
<li>for: 提高计算机视觉中的特征匹配精度和效率，特别是在相机转换、基本矩阵估计和视觉地标 tasks 上。</li>
<li>methods: 我们提出了一种快速准确的 MaKeGNN 模型，它使用了稀疏注意力机制来缺省非重复的关键点，并通过匹配关键点来导引有意义的信息传递。</li>
<li>results: 我们在相机转换、基本矩阵估计和视觉地标 tasks 上达到了领先的性能，同时减少了计算和内存复杂度。<details>
<summary>Abstract</summary>
Accurately matching local features between a pair of images is a challenging computer vision task. Previous studies typically use attention based graph neural networks (GNNs) with fully-connected graphs over keypoints within/across images for visual and geometric information reasoning. However, in the context of feature matching, considerable keypoints are non-repeatable due to occlusion and failure of the detector, and thus irrelevant for message passing. The connectivity with non-repeatable keypoints not only introduces redundancy, resulting in limited efficiency, but also interferes with the representation aggregation process, leading to limited accuracy. Targeting towards high accuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN architecture which bypasses non-repeatable keypoints and leverages matchable ones to guide compact and meaningful message passing. More specifically, our Bilateral Context-Aware Sampling Module first dynamically samples two small sets of well-distributed keypoints with high matchability scores from the image pair. Then, our Matchable Keypoint-Assisted Context Aggregation Module regards sampled informative keypoints as message bottlenecks and thus constrains each keypoint only to retrieve favorable contextual information from intra- and inter- matchable keypoints, evading the interference of irrelevant and redundant connectivity with non-repeatable ones. Furthermore, considering the potential noise in initial keypoints and sampled matchable ones, the MKACA module adopts a matchability-guided attentional aggregation operation for purer data-dependent context propagation. By these means, we achieve the state-of-the-art performance on relative camera estimation, fundamental matrix estimation, and visual localization, while significantly reducing computational and memory complexity compared to typical attentional GNNs.
</details>
<details>
<summary>摘要</summary>
通过快速匹配本地特征点，计算机视觉任务中的一个挑战是准确地匹配两个图像中的特征点。先前的研究通常使用注意力基于图像内部/ между图像的全连接图 neural networks (GNNs) 进行视觉和几何信息的推理。然而，在特征匹配任务中，许多特征点是不可重复的，这些特征点由遮挡和检测器失败所致，因此对信息传递无关。与非重复的特征点连接不仅引入纠续，导致效率有限，而且干扰归纳表达过程，从而限制准确性。为了实现高精度和效率，我们提出了MaKeGNN，一种稀缺注意力基于GNN架构。MaKeGNN通过快速匹配特征点来导引有效的信息传递。更加详细地说，我们的双边上下文感知抽取模块首先在图像对中动态选择了一小集well-distributed的高匹配分数的特征点。然后，我们的匹配点协助上下文聚合模块将这些有用的特征点作为信道瓶颈，限制每个特征点只能从匹配的内部和外部匹配点中 retrieve 有利的上下文信息，避免与非匹配的非重复连接干扰信息传递。此外，为了避免初始特征点和抽取的匹配点中的噪音，MKACA模块采用了匹配性指导的注意力聚合操作，以保证数据依赖关系的纯净传递。通过这些手段，我们实现了相对摄像头估算、基本矩阵估算和视觉地理位置估算的状态 искусственный情况，同时显著降低了通常的注意力GNNs的计算和内存复杂性。
</details></li>
</ul>
<hr>
<h2 id="Continual-Learning-in-Open-vocabulary-Classification-with-Complementary-Memory-Systems"><a href="#Continual-Learning-in-Open-vocabulary-Classification-with-Complementary-Memory-Systems" class="headerlink" title="Continual Learning in Open-vocabulary Classification with Complementary Memory Systems"></a>Continual Learning in Open-vocabulary Classification with Complementary Memory Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01430">http://arxiv.org/abs/2307.01430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Zhu, Weijie Lyu, Yao Xiao, Derek Hoiem</li>
<li>for: 这paper是为了解决开放词汇图像分类 tasks中的灵活 continual learning问题， Drawing inspiration from human cognition的 complementary learning systems.</li>
<li>methods: 我们提出了一种”tree probe”方法， Which enables fast learning from new examples with competitive accuracy to batch-trained linear models. We also propose a method to combine predictions from a CLIP zero-shot model and the exemplar-based model, using the zero-shot estimated probability that a sample’s class is within any of the exemplar classes.</li>
<li>results: Our proposed method achieves a good balance of learning speed, target task effectiveness, and zero-shot effectiveness in data incremental, class incremental, and task incremental settings, as well as ability to perform flexible inference on varying subsets of zero-shot and learned categories.<details>
<summary>Abstract</summary>
We introduce a method for flexible continual learning in open-vocabulary image classification, drawing inspiration from the complementary learning systems observed in human cognition. We propose a "tree probe" method, an adaption of lazy learning principles, which enables fast learning from new examples with competitive accuracy to batch-trained linear models. Further, we propose a method to combine predictions from a CLIP zero-shot model and the exemplar-based model, using the zero-shot estimated probability that a sample's class is within any of the exemplar classes. We test in data incremental, class incremental, and task incremental settings, as well as ability to perform flexible inference on varying subsets of zero-shot and learned categories. Our proposed method achieves a good balance of learning speed, target task effectiveness, and zero-shot effectiveness.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种可动的开放词汇图像分类方法， drawing inspiration from human cognition中的补偿学习系统。我们提出了“树触”方法，基于懒散学习原则，可以快速从新示例中学习，与批量训练的直线模型具有竞争性准确率。此外，我们提出了将CLIP零shot模型和 exemplar-based模型的预测结果组合使用，使用零shot估计概率是任何 exemplar类中的样本类别。我们在数据增量、类增量和任务增量设置下测试了我们的提议方法，以及在不同的零shot和学习类别上进行可变的测试。我们的提议方法实现了learner speed, target task effectiveness和零shot effectiveness的好 equilibrio。
</details></li>
</ul>
<hr>
<h2 id="DeepfakeBench-A-Comprehensive-Benchmark-of-Deepfake-Detection"><a href="#DeepfakeBench-A-Comprehensive-Benchmark-of-Deepfake-Detection" class="headerlink" title="DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection"></a>DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01426">http://arxiv.org/abs/2307.01426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sclbd/deepfakebench">https://github.com/sclbd/deepfakebench</a></li>
<li>paper_authors: Zhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, Baoyuan Wu</li>
<li>for: 这篇论文是为了提供一个标准化、一致的深伪检测 benchmark，以解决现有的检测模型之间的不公正比较和可能的误导结果。</li>
<li>methods: 这篇论文使用了一个统一的数据管理系统，以确保所有检测模型的输入数据都是一致的。此外，论文还提供了一个整合了现有方法的框架，以及一个标准化的评估度量和评估协议，以促进透明度和重复性。</li>
<li>results: 这篇论文提供了一个全面的检测模型评估，包括15种现有的检测方法、9个深伪数据集、多种深伪检测评估协议和分析工具，以及广泛的评估。此外，论文还提供了新的分析结果，包括不同的观点（例如，数据增强、后向）。<details>
<summary>Abstract</summary>
A critical yet frequently overlooked challenge in the field of deepfake detection is the lack of a standardized, unified, comprehensive benchmark. This issue leads to unfair performance comparisons and potentially misleading results. Specifically, there is a lack of uniformity in data processing pipelines, resulting in inconsistent data inputs for detection models. Additionally, there are noticeable differences in experimental settings, and evaluation strategies and metrics lack standardization. To fill this gap, we present the first comprehensive benchmark for deepfake detection, called DeepfakeBench, which offers three key contributions: 1) a unified data management system to ensure consistent input across all detectors, 2) an integrated framework for state-of-the-art methods implementation, and 3) standardized evaluation metrics and protocols to promote transparency and reproducibility. Featuring an extensible, modular-based codebase, DeepfakeBench contains 15 state-of-the-art detection methods, 9 deepfake datasets, a series of deepfake detection evaluation protocols and analysis tools, as well as comprehensive evaluations. Moreover, we provide new insights based on extensive analysis of these evaluations from various perspectives (e.g., data augmentations, backbones). We hope that our efforts could facilitate future research and foster innovation in this increasingly critical domain. All codes, evaluations, and analyses of our benchmark are publicly available at https://github.com/SCLBD/DeepfakeBench.
</details>
<details>
<summary>摘要</summary>
具有批评性和 часто被忽视的挑战在深度假造检测领域是缺乏标准化、一致化、完整的标准准测。这个问题导致了不公平的性能比较和可能出现误导性的结果。具体来说，存在数据处理管道的不一致性，导致检测模型的输入数据不一致。此外，实验设置存在显著差异，评价策略和标准化也缺乏。为了填补这一漏洞，我们提出了首个深度假造检测的完整准测，即深度假造准测（DeepfakeBench），它的三个重要贡献如下：1. 一个统一的数据管理系统，确保所有检测器的输入数据具有一致性。2. 一个集成的方法实现框架，包括当前领域的状态艺术方法。3. 标准化的评价指标和协议，以促进透明度和可重现性。我们的准测拥有可扩展、模块化的代码基础，包括15种当前领域的检测方法，9个深度假造数据集，深度假造检测评价协议和分析工具，以及全面的评估。此外，我们还提供了来自多个角度的新的分析结论（例如数据增强、后向传播）。我们希望我们的努力可以促进未来的研究和促进深度假造检测领域的创新。我们的所有代码、评价、分析和结论都公开可用于https://github.com/SCLBD/DeepfakeBench。
</details></li>
</ul>
<hr>
<h2 id="Consistent-Multimodal-Generation-via-A-Unified-GAN-Framework"><a href="#Consistent-Multimodal-Generation-via-A-Unified-GAN-Framework" class="headerlink" title="Consistent Multimodal Generation via A Unified GAN Framework"></a>Consistent Multimodal Generation via A Unified GAN Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01425">http://arxiv.org/abs/2307.01425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Zhu, Yijun Li, Weijie Lyu, Krishna Kumar Singh, Zhixin Shu, Soeren Pirk, Derek Hoiem</li>
<li>for: 这篇论文的目的是如何通过单一的生成模型生成多模态图像输出，包括RGB、深度和表面法向量。</li>
<li>methods: 该论文基于StyleGAN3架构，使用共享背部和模态特定分支在图像生成网络的最后层。它还提出了每个模态的准确性评估器和跨模态一致性评估器。</li>
<li>results: 在斯坦福2D3D数据集上进行实验，论文实现了真实和一致的RGB、深度和表面法向量图像生成。它还提供了一种训练recipe，可以轻松地将预训练模型应用于新领域，只需几个对应的数据对。此外，论文还评估了使用生成的RGB和深度对进行深度估计器的训练或细化。<details>
<summary>Abstract</summary>
We investigate how to generate multimodal image outputs, such as RGB, depth, and surface normals, with a single generative model. The challenge is to produce outputs that are realistic, and also consistent with each other. Our solution builds on the StyleGAN3 architecture, with a shared backbone and modality-specific branches in the last layers of the synthesis network, and we propose per-modality fidelity discriminators and a cross-modality consistency discriminator. In experiments on the Stanford2D3D dataset, we demonstrate realistic and consistent generation of RGB, depth, and normal images. We also show a training recipe to easily extend our pretrained model on a new domain, even with a few pairwise data. We further evaluate the use of synthetically generated RGB and depth pairs for training or fine-tuning depth estimators. Code will be available at https://github.com/jessemelpolio/MultimodalGAN.
</details>
<details>
<summary>摘要</summary>
我们研究如何通过单一的生成模型生成多modal的图像输出，如RGB、深度和表面法向量。挑战在于生成的输出应该是真实的，同时也需要在不同modal之间具有一致性。我们基于StyleGAN3架构，在生成网络的最后层添加共享后置和modal特定的分支，并提出每个modal的准确性评价器以及交叉modal的一致性评价器。在Stanford2D3D数据集上进行实验，我们实现了真实和一致的RGB、深度和normal图像生成。此外，我们还提供了一个训练recipe，可以轻松地在新领域上扩展我们预训练模型，只需要一些对应的对比数据。最后，我们还评估了使用生成的RGB和深度对来训练或练化深度估计器。代码将在GitHub上提供。
</details></li>
</ul>
<hr>
<h2 id="Direct-Superpoints-Matching-for-Fast-and-Robust-Point-Cloud-Registration"><a href="#Direct-Superpoints-Matching-for-Fast-and-Robust-Point-Cloud-Registration" class="headerlink" title="Direct Superpoints Matching for Fast and Robust Point Cloud Registration"></a>Direct Superpoints Matching for Fast and Robust Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01362">http://arxiv.org/abs/2307.01362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Gupta, Yiming Xie, Hanumant Singh, Huaizu Jiang</li>
<li>for: 本研究旨在提出一种简单 yet effective的方法，可以直接匹配 superpoints，以确定源点云和目标点云之间的固定变换。</li>
<li>methods: 本方法使用全局滑动层来直接匹配 superpoints，并使用这些匹配来确定点云之间的变换。</li>
<li>results: 与直接预测对应点的方法相比，我们的方法可以更准确地估计变换，并且不需要任何后处理修复。在 ModelNet 和 3DMatch 测试集上，我们的方法达到了当前最佳的结果。<details>
<summary>Abstract</summary>
Although deep neural networks endow the downsampled superpoints with discriminative feature representations, directly matching them is usually not used alone in state-of-the-art methods, mainly for two reasons. First, the correspondences are inevitably noisy, so RANSAC-like refinement is usually adopted. Such ad hoc postprocessing, however, is slow and not differentiable, which can not be jointly optimized with feature learning. Second, superpoints are sparse and thus more RANSAC iterations are needed. Existing approaches use the coarse-to-fine strategy to propagate the superpoints correspondences to the point level, which are not discriminative enough and further necessitates the postprocessing refinement. In this paper, we present a simple yet effective approach to extract correspondences by directly matching superpoints using a global softmax layer in an end-to-end manner, which are used to determine the rigid transformation between the source and target point cloud. Compared with methods that directly predict corresponding points, by leveraging the rich information from the superpoints matchings, we can obtain more accurate estimation of the transformation and effectively filter out outliers without any postprocessing refinement. As a result, our approach is not only fast, but also achieves state-of-the-art results on the challenging ModelNet and 3DMatch benchmarks. Our code and model weights will be publicly released.
</details>
<details>
<summary>摘要</summary>
尽管深度神经网络使得下采样后的超点具有特征表示能力，但直接匹配它们通常不是独立使用的方法，主要是因为两个原因。首先，匹配是不可避免的噪声，因此通常采用RANSAC-like的修正。这种随机处理不是梯度可导的，因此无法与特征学习一起共同优化。其次，超点是稀疏的，因此需要更多的RANSAC迭代。现有的方法使用粗细到细粒的策略来传播超点匹配到点级别，但这些匹配不够精细，需要进一步的后处理修正。在这篇论文中，我们提出了一种简单又有效的方法，通过直接匹配超点使用全局软max层，以END-TO-END的方式确定源点云和目标点云之间的固定变换。与直接预测对应点的方法相比，通过利用超点匹配中的丰富信息，我们可以更准确地估算变换，并有效地排除噪声，无需任何后处理修正。因此，我们的方法不仅快速，而且实现了对挑战性 ModelNet 和 3DMatch 测试集的state-of-the-art 结果。我们的代码和模型参数将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Patch-CNN-Training-data-efficient-deep-learning-for-high-fidelity-diffusion-tensor-estimation-from-minimal-diffusion-protocols"><a href="#Patch-CNN-Training-data-efficient-deep-learning-for-high-fidelity-diffusion-tensor-estimation-from-minimal-diffusion-protocols" class="headerlink" title="Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols"></a>Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01346">http://arxiv.org/abs/2307.01346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Goodwin-Allcock, Ting Gong, Robert Gray, Parashkev Nachev, Hui Zhang</li>
<li>for: 优化Diffusion Tensor Estimation（DT）从六个方向的扩散束图像（DWI）中获得更高精度的数据。</li>
<li>methods: 使用深度学习方法，主要是基于 convolutional neural networks（CNN），从只有六个方向的扩散束图像（DWI）中获得 diffusion tensor 参数。</li>
<li>results: 与传统模型适应和基于voxel-wise fully-connected neural networks（FCN）的方法相比，Patch-CNN 可以更好地优化 scalar dMRI 参数和纤维orientation estimation from six-direction DWIs，并生成更好的 tractogram。<details>
<summary>Abstract</summary>
We propose a new method, Patch-CNN, for diffusion tensor (DT) estimation from only six-direction diffusion weighted images (DWI). Deep learning-based methods have been recently proposed for dMRI parameter estimation, using either voxel-wise fully-connected neural networks (FCN) or image-wise convolutional neural networks (CNN). In the acute clinical context -- where pressure of time limits the number of imaged directions to a minimum -- existing approaches either require an infeasible number of training images volumes (image-wise CNNs), or do not estimate the fibre orientations (voxel-wise FCNs) required for tractogram estimation. To overcome these limitations, we propose Patch-CNN, a neural network with a minimal (non-voxel-wise) convolutional kernel (3$\times$3$\times$3). Compared with voxel-wise FCNs, this has the advantage of allowing the network to leverage local anatomical information. Compared with image-wise CNNs, the minimal kernel vastly reduces training data demand. Evaluated against both conventional model fitting and a voxel-wise FCN, Patch-CNN, trained with a single subject is shown to improve the estimation of both scalar dMRI parameters and fibre orientation from six-direction DWIs. The improved fibre orientation estimation is shown to produce improved tractogram.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新方法，named Patch-CNN，用于从只有六个方向的扩散束图像（DWI）中提取扩散tensor（DT）。在临床应用中，使用深度学习基于方法进行dMRI参数估计，可以使用 Either voxel-wise fully-connected neural networks (FCN) or image-wise convolutional neural networks (CNN)。然而，在临床上，由于时间压力，通常只能取得有限的图像方向，因此现有的方法可能需要很多的训练图像量（image-wise CNNs），或者不会估计束纹方向（voxel-wise FCNs）。为了解决这些限制，我们提出了Patch-CNN，一个具有最小（非voxel-wise） convolutional kernel（3x3x3）的神经网络。与voxel-wise FCNs比较，这有利于网络利用局部 анатомиче信息。与image-wise CNNs比较，最小kernel减少了训练数据的需求。我们对比 conventunal model fitting和voxel-wise FCN进行评估，发现Patch-CNN，通过使用单个subject进行训练，可以提高从六个方向DWIs中的scalar dMRI参数和束纹方向的估计。这些改进的束纹方向估计，可以生成改进的 tractogram。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Monocular-Full-body-Capture-in-World-Space-via-Sequential-Proxy-to-Motion-Learning"><a href="#Real-time-Monocular-Full-body-Capture-in-World-Space-via-Sequential-Proxy-to-Motion-Learning" class="headerlink" title="Real-time Monocular Full-body Capture in World Space via Sequential Proxy-to-Motion Learning"></a>Real-time Monocular Full-body Capture in World Space via Sequential Proxy-to-Motion Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01200">http://arxiv.org/abs/2307.01200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Zhang, Hongwen Zhang, Liangxiao Hu, Hongwei Yi, Shengping Zhang, Yebin Liu</li>
<li>for: 这个论文的目的是提出一种基于学习的单视动作捕捉系统，可以在世界空间中实时捕捉全身动作，同时保持准确性。</li>
<li>methods: 该论文使用了一种顺序的代理人-到-动作学习方案，并使用了一个代理数据集，包括2D骨架序列和3D旋转动作在世界空间中。这些代理数据允许我们建立一个基于学习的网络，并在全身动作上提供准确的超级视图指导。此外，我们还在网络中共享了身体手部上下文信息，以便更好地恢复腕姿。</li>
<li>results: 根据该论文的结果，我们实现了世界空间中的实时单视全身动作捕捉系统，并且能够保持准确性和物理可能性。此外，我们还提供了更多的视频结果，可以在我们项目页面上找到：<a target="_blank" rel="noopener" href="https://liuyebin.com/proxycap%E3%80%82">https://liuyebin.com/proxycap。</a><details>
<summary>Abstract</summary>
Learning-based approaches to monocular motion capture have recently shown promising results by learning to regress in a data-driven manner. However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space. In this work, we contribute a sequential proxy-to-motion learning scheme together with a proxy dataset of 2D skeleton sequences and 3D rotational motions in world space. Such proxy data enables us to build a learning-based network with accurate full-body supervision while also mitigating the generalization issues. For more accurate and physically plausible predictions, a contact-aware neural motion descent module is proposed in our network so that it can be aware of foot-ground contact and motion misalignment with the proxy observations. Additionally, we share the body-hand context information in our network for more compatible wrist poses recovery with the full-body model. With the proposed learning-based solution, we demonstrate the first real-time monocular full-body capture system with plausible foot-ground contact in world space. More video results can be found at our project page: https://liuyebin.com/proxycap.
</details>
<details>
<summary>摘要</summary>
现代学习方法在单视动作捕捉领域已经显示出了扎实的成果，通过数据驱动的方式来进行回归。然而，由于数据收集和网络设计的问题，现有的解决方案在实时全身捕捉中具有很大的挑战。在这项工作中，我们提出了一种顺序Proxy-to-动作学习方案，并使用了2Dskeleton序列和3D旋转动作的世界空间数据集作为代理数据。这些代理数据允许我们建立一个基于学习的网络，并在全身监督下进行准确的回归。为了提高预测的准确性和物理可能性，我们还提出了一种具有联系感的神经动作下降模块，该模块能够考虑到脚地接触和动作不一致的代理观察。此外，我们在网络中分享了身体手势信息，以便更好地恢复与全身模型兼容的手势pose。通过我们的学习型解决方案，我们实现了世界空间中实时单视全身捕捉系统，并且首次实现了准确的脚地接触。更多视频结果可以在我们项目页面上找到：https://liuyebin.com/proxycap。
</details></li>
</ul>
<hr>
<h2 id="Segment-Anything-Meets-Point-Tracking"><a href="#Segment-Anything-Meets-Point-Tracking" class="headerlink" title="Segment Anything Meets Point Tracking"></a>Segment Anything Meets Point Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01197">http://arxiv.org/abs/2307.01197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/syscv/sam-pt">https://github.com/syscv/sam-pt</a></li>
<li>paper_authors: Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu</li>
<li>for: 这篇论文旨在扩展Segment Anything Model (SAM)的能力，以便在动态视频中跟踪和分割任何东西。</li>
<li>methods: 该方法利用了稳健和稀疏点选择和卷积技术来生成Mask，并且使用了点推送技术来跟踪目标对象。</li>
<li>results: 该方法在流行的视频对象分割评价标准DAVIS、YouTube-VOS和MOSE上表现出了强大的零shot性能。相比传统的对象中心的推送策略，我们使用点推送技术来利用地方结构信息，不受对象 semantics 的限制。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) has established itself as a powerful zero-shot image segmentation model, employing interactive prompts such as points to generate masks. This paper presents SAM-PT, a method extending SAM's capability to tracking and segmenting anything in dynamic videos. SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation, demonstrating that a SAM-based segmentation tracker can yield strong zero-shot performance across popular video object segmentation benchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditional object-centric mask propagation strategies, we uniquely use point propagation to exploit local structure information that is agnostic to object semantics. We highlight the merits of point-based tracking through direct evaluation on the zero-shot open-world Unidentified Video Objects (UVO) benchmark. To further enhance our approach, we utilize K-Medoids clustering for point initialization and track both positive and negative points to clearly distinguish the target object. We also employ multiple mask decoding passes for mask refinement and devise a point re-initialization strategy to improve tracking accuracy. Our code integrates different point trackers and video segmentation benchmarks and will be released at https://github.com/SysCV/sam-pt.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 已成为一种强大的零shot图像分割模型，使用交互提示如点来生成面积。本文介绍 SAM-PT，一种扩展 SAM 的能力，以跟踪和分割视频中的任何物体。SAM-PT 利用了Robust 和稀疏的点选择和宣传技术来生成面积，实际表明了基于 SAM 的分割跟踪器可以在流行的视频对象分割benchmark中具有强大的零shot性能，包括 DAVIS、YouTube-VOS 和 MOSE。与传统的对象中心的面积宣传策略不同，我们使用点宣传来利用本地结构信息，无论对象 semantics 无关。我们通过直接评估零shot开放世界 Unidentified Video Objects (UVO) benchmark来评估点跟踪的优势。为了进一步提高我们的方法，我们使用 K-Medoids 聚类算法来初始化点并跟踪正面和负面的点，以清晰地 distinguishes 目标对象。我们还使用多个面积解码通过来进行面积纠正，并设计了点重初始化策略以提高跟踪准确性。我们的代码将包括不同的点跟踪器和视频分割benchmark，将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Data-Memorization-in-3D-Latent-Diffusion-Models-for-Medical-Image-Synthesis"><a href="#Investigating-Data-Memorization-in-3D-Latent-Diffusion-Models-for-Medical-Image-Synthesis" class="headerlink" title="Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis"></a>Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01148">http://arxiv.org/abs/2307.01148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salman Ul Hassan Dar, Arman Ghanaat, Jannik Kahmann, Isabelle Ayx, Theano Papavassiliu, Stefan O. Schoenberg, Sandy Engelhardt</li>
<li>for: 用于生成真实的医疗数据，保护患者隐私。</li>
<li>methods: 使用生成模型，利用自我指导学习方法检测数据记忆能力。</li>
<li>results: 发现模型确实记忆训练数据，需要采取缓解措施。<details>
<summary>Abstract</summary>
Generative latent diffusion models have been established as state-of-the-art in data generation. One promising application is generation of realistic synthetic medical imaging data for open data sharing without compromising patient privacy. Despite the promise, the capacity of such models to memorize sensitive patient training data and synthesize samples showing high resemblance to training data samples is relatively unexplored. Here, we assess the memorization capacity of 3D latent diffusion models on photon-counting coronary computed tomography angiography and knee magnetic resonance imaging datasets. To detect potential memorization of training samples, we utilize self-supervised models based on contrastive learning. Our results suggest that such latent diffusion models indeed memorize training data, and there is a dire need for devising strategies to mitigate memorization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过实验，我们发现了生成潜在扩散模型在数据生成中的表现。这些模型在生成真实的医疗影像数据方面具有潜在的应用，例如公开分享医疗影像数据而不会侵犯病人隐私。然而，这些模型是否能够记忆敏感的训练数据仍然是一个未知之地。在这篇文章中，我们评估了3D潜在扩散模型在光子计数 computed tomography angiography和膝骨磁共振成像数据集上的记忆能力。我们使用自动化学习的对照学习方法来检测模型是否会记忆训练数据。我们的结果显示，这些潜在扩散模型确实会记忆训练数据，因此需要发展新的策略来缓和这种记忆。
</details></li>
</ul>
<hr>
<h2 id="AVSegFormer-Audio-Visual-Segmentation-with-Transformer"><a href="#AVSegFormer-Audio-Visual-Segmentation-with-Transformer" class="headerlink" title="AVSegFormer: Audio-Visual Segmentation with Transformer"></a>AVSegFormer: Audio-Visual Segmentation with Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01146">http://arxiv.org/abs/2307.01146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vvvb-github/avsegformer">https://github.com/vvvb-github/avsegformer</a></li>
<li>paper_authors: Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, Tong Lu</li>
<li>for: 该研究旨在提出一种新的听视多模态（AVS）任务，即将视频中的声音对象定位和分割。</li>
<li>methods: 该研究提出了一种基于 transformer 架构的新方法，称为 AVSegFormer，该方法利用听 queries 和可学习 queries 在 transformer 解码器中，使网络能够选择性地注意到有兴趣的视觉特征。</li>
<li>results: extensive experiments 表明，AVSegFormer 在 AVS  benchmark 上达到了状态 искусственный智能的最佳 результаTS，并且可以在不同的视频和声音背景下表现出色。I hope that helps!<details>
<summary>Abstract</summary>
The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate that AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is available at https://github.com/vvvb-github/AVSegFormer.
</details>
<details>
<summary>摘要</summary>
通过音频和视觉的组合，多模态社区已经是长期的研究主题。最近，一个新的音频视频分割（AVS）任务被提出，旨在在给定的视频中找到并分割声音的对象。这个任务需要音频驱动像素级场景理解，对于多模态社区来说，它具有 significante挑战。在这篇论文中，我们提出了AVSegFormer，一种新的AVS任务框架，利用转换器架构。我们在转换器解码器中引入了音频问题和学习问题，使网络可以选择性地听取 interess visual特征。此外，我们还提出了一个音频视频混合器，可以动态调整视觉特征，增强相关的空间通道，并降低无关的空间通道。此外，我们还设计了一个中间mask损失，以增强解码器的监督，让网络生成更加准确的中间预测。我们的实验结果表明，AVSegFormer可以在AVS标准 benchmark上达到状态 искусственный智能的Result。代码可以在https://github.com/vvvb-github/AVSegFormer中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/04/cs.CV_2023_07_04/" data-id="clp88dbuo00frob88fsq02q2f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/cs.AI_2023_07_04/" class="article-date">
  <time datetime="2023-07-04T12:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/cs.AI_2023_07_04/">cs.AI - 2023-07-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Inner-Sentiments-of-a-Thought"><a href="#The-Inner-Sentiments-of-a-Thought" class="headerlink" title="The Inner Sentiments of a Thought"></a>The Inner Sentiments of a Thought</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01784">http://arxiv.org/abs/2307.01784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Gagne, Peter Dayan</li>
<li>for: 这 paper 探讨了Transformer-based large-scale language models (LLMs) 能够生成高度现实的文本，以及这些模型能够表达和直接或间接表达各种情感和颜色的能力。</li>
<li>methods: 作者使用了LLMs的隐藏表示来训练分布预测器，用于预测句子的最终情感分布的多个量。</li>
<li>results: 作者发现了这些分布预测器具有良好的准确性和均匀性，并且可以用于分析句子的情感 trait。例如，用于分析句子的情感 trait，例如用于分析句子的情感 trait，例如“but” conjunction 可以对话趋于极端情感的变化。此外，作者还使用了这些分布预测器来生成具有特定情感 trait 的句子。<details>
<summary>Abstract</summary>
Transformer-based large-scale language models (LLMs) are able to generate highly realistic text. They are duly able to express, and at least implicitly represent, a wide range of sentiments and color, from the obvious, such as valence and arousal to the subtle, such as determination and admiration. We provide a first exploration of these representations and how they can be used for understanding the inner sentimental workings of single sentences. We train predictors of the quantiles of the distributions of final sentiments of sentences from the hidden representations of an LLM applied to prefixes of increasing lengths. After showing that predictors of distributions of valence, determination, admiration, anxiety and annoyance are well calibrated, we provide examples of using these predictors for analyzing sentences, illustrating, for instance, how even ordinary conjunctions (e.g., "but") can dramatically alter the emotional trajectory of an utterance. We then show how to exploit the distributional predictions to generate sentences with sentiments in the tails of distributions. We discuss the implications of our results for the inner workings of thoughts, for instance for psychiatric dysfunction.
</details>
<details>
<summary>摘要</summary>
Transformer-based大型语言模型（LLMs）能够生成高度真实的文本。它们能够表达，并至少隐含表达，从明显的投情和兴奋到 SUBTLE的决心和赞誉。我们提供了首次探索这些表示方式，并如何用它们来理解单句的内心情感运作。我们使用LLM应用到预FIX的长度 prefixes 中的隐藏表示进行训练，然后预测句子的final sentiment distribution 的quantiles。我们显示了这些预测器具有良好的准确性，然后提供了使用这些预测器来分析句子的例子，例如，如何通过使用 "but" 来剧烈地改变一句话的情感轨迹。最后，我们显示了如何利用分布预测器来生成具有不同情感的句子。我们讨论了我们的结果对内心工作的影响，例如心理障碍。
</details></li>
</ul>
<hr>
<h2 id="GHOST-A-Graph-Neural-Network-Accelerator-using-Silicon-Photonics"><a href="#GHOST-A-Graph-Neural-Network-Accelerator-using-Silicon-Photonics" class="headerlink" title="GHOST: A Graph Neural Network Accelerator using Silicon Photonics"></a>GHOST: A Graph Neural Network Accelerator using Silicon Photonics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01782">http://arxiv.org/abs/2307.01782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salma Afifi, Febin Sunny, Amin Shafiee, Mahdi Nikdast, Sudeep Pasricha</li>
<li>for: 这篇论文旨在提出一种基于光学频谱的干扰加速器，用于加速基于图structured数据的图神经网络（GNNs）模型的运行。</li>
<li>methods: 该论文使用了光学频谱技术，实现了图神经网络的三个主要阶段：邻居更新、Message Passing和采样。这些阶段都是在光学频谱中实现的，以提高加速器的效率和能效性。</li>
<li>results:  simulations 表明，GHOST 比 GPU、TPU、CPU 和多种现有的 GNN 硬件加速器具有至少 10.2 倍的吞吐量和 3.8 倍的能效率。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have emerged as a powerful approach for modelling and learning from graph-structured data. Multiple fields have since benefitted enormously from the capabilities of GNNs, such as recommendation systems, social network analysis, drug discovery, and robotics. However, accelerating and efficiently processing GNNs require a unique approach that goes beyond conventional artificial neural network accelerators, due to the substantial computational and memory requirements of GNNs. The slowdown of scaling in CMOS platforms also motivates a search for alternative implementation substrates. In this paper, we present GHOST, the first silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates the costs associated with both vertex-centric and edge-centric operations. It implements separately the three main stages involved in running GNNs in the optical domain, allowing it to be used for the inference of various widely used GNN models and architectures, such as graph convolution networks and graph attention networks. Our simulation studies indicate that GHOST exhibits at least 10.2x better throughput and 3.8x better energy efficiency when compared to GPU, TPU, CPU and multiple state-of-the-art GNN hardware accelerators.
</details>
<details>
<summary>摘要</summary>
GRAPH神经网络（GNNs）已经成为模elling和学习图structured数据的强大方法。多个领域受益于GNNs的能力，如推荐系统、社交网络分析、药物发现和机器人。但是，加速和有效地处理GNNs需要一种特殊的方法，这些方法超出了传统的人工神经网络加速器的能力，因为GNNs的计算和存储需求很大。CMOS平台的慢速化也驱动了寻找代替实现基台。在这篇论文中，我们提出了GHOST，首个基于光学频谱的GNN硬件加速器。GHOST efficiently减少了顶点中心和边中心操作的成本。它在光学频谱中实现了GNNs的三个主要阶段，allowing it to be used for the inference of various widely used GNN models and architectures, such as graph convolution networks and graph attention networks。我们的Simulation studies indicate that GHOST exhibits at least 10.2 times better throughput and 3.8 times better energy efficiency compared to GPU, TPU, CPU, and multiple state-of-the-art GNN hardware accelerators.
</details></li>
</ul>
<hr>
<h2 id="Physically-Realizable-Natural-Looking-Clothing-Textures-Evade-Person-Detectors-via-3D-Modeling"><a href="#Physically-Realizable-Natural-Looking-Clothing-Textures-Evade-Person-Detectors-via-3D-Modeling" class="headerlink" title="Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling"></a>Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01778">http://arxiv.org/abs/2307.01778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WhoTHU/Adversarial_camou">https://github.com/WhoTHU/Adversarial_camou</a></li>
<li>paper_authors: Zhanhao Hu, Wenda Chu, Xiaopei Zhu, Hui Zhang, Bo Zhang, Xiaolin Hu</li>
<li>for: 这 paper 的目的是为了逃脱人体检测器，而且它们可以在多个视角下工作。</li>
<li>methods: 这 paper 使用了3D 模型来制作隐蔽的 Texture，这种技术已经在隐蔽固定物体上得到成功。然而，人体和服装都是非固定的，因此实现这种技术在物理上是非常困难的。</li>
<li>results: 这 paper 的实验结果表明，使用 AdvCaT 技术可以在多个视角下逃脱多种人体检测器，并且可以在实际世界中应用。<details>
<summary>Abstract</summary>
Recent works have proposed to craft adversarial clothes for evading person detectors, while they are either only effective at limited viewing angles or very conspicuous to humans. We aim to craft adversarial texture for clothes based on 3D modeling, an idea that has been used to craft rigid adversarial objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes are non-rigid, leading to difficulties in physical realization. In order to craft natural-looking adversarial clothes that can evade person detectors at multiple viewing angles, we propose adversarial camouflage textures (AdvCaT) that resemble one kind of the typical textures of daily clothes, camouflage textures. We leverage the Voronoi diagram and Gumbel-softmax trick to parameterize the camouflage textures and optimize the parameters via 3D modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes combining topologically plausible projection (TopoProj) and Thin Plate Spline (TPS) to narrow the gap between digital and real-world objects. We printed the developed 3D texture pieces on fabric materials and tailored them into T-shirts and trousers. Experiments show high attack success rates of these clothes against multiple detectors.
</details>
<details>
<summary>摘要</summary>
最近的研究提出了为逃脱人体检测器而制作险oso的衣服，但它们只有有限的视场效果或对人类非常明显。我们想制作基于3D模型的险oso texture для衣服，这是已经用于制作固定险oso对象，如3D打印的乌龟。与固定对象不同，人类和衣服是非固定的，这导致物理实现的困难。为了制作多视场可以逃脱人体检测器的自然looking险oso衣服，我们提出了险oso披落文（AdvCaT），它们类似于日常衣服的一种典型文化，披落文。我们利用 Voronoi 图和 Gumbel-softmax 技巧来参数化险oso披落文并优化参数。此外，我们提出了一种高效的3D矩阵增强管道，将数字和实际对象之间的差异缩小。我们打印了开发的3D texture Piece onto fabric材料，并将其制成T恤和裤子。实验表明这些衣服可以高效地逃脱多个检测器。
</details></li>
</ul>
<hr>
<h2 id="MOPO-LSI-A-User-Guide"><a href="#MOPO-LSI-A-User-Guide" class="headerlink" title="MOPO-LSI: A User Guide"></a>MOPO-LSI: A User Guide</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01719">http://arxiv.org/abs/2307.01719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Zheng, Kumar Neelotpal Shukla, Jasmine Xu, David, Wang, Michael O’Leary</li>
<li>for: 这份论文是为了介绍MOPO-LSI库的用户指南，包括问题设置、工作流程和配置参数。</li>
<li>methods: 该论文使用MOPO-LSI库来实现多目标投资策略的优化，包括问题设置、工作流程和配置参数。</li>
<li>results: 该论文提供了MOPO-LSI库的用户指南，包括问题设置、工作流程和配置参数，以帮助用户快速地采用该库进行多目标投资策略的优化。<details>
<summary>Abstract</summary>
MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
</details>
<details>
<summary>摘要</summary>
MOPO-LSI是一个开源的多目标投资组合优化库，用于可持续投资。本文件提供MOPO-LSI版本1.0的用户手册，包括问题设置、工作流程和配置参数。
</details></li>
</ul>
<hr>
<h2 id="On-the-Constrained-Time-Series-Generation-Problem"><a href="#On-the-Constrained-Time-Series-Generation-Problem" class="headerlink" title="On the Constrained Time-Series Generation Problem"></a>On the Constrained Time-Series Generation Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01717">http://arxiv.org/abs/2307.01717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Coletta, Sriram Gopalakrishan, Daniel Borrajo, Svitlana Vyetrenko</li>
<li>for: 这种论文是为了提供一种有效的时间序列生成方法，以满足实际应用中的需求，如增强机器学习算法的性能、增加罕见事件的发生率和创造更多的对称时间序列enario。</li>
<li>methods: 该论文提出了一种新的时间序列生成方法，基于受限制的优化框架，并使用一种名为“指导噪时间”的干扰模型来生成真实的时间序列。</li>
<li>results: 该论文的实验结果表明，相比于现有的方法，该方法能够更高效地生成受限制的时间序列，并且不需要重新训练，从而降低碳脚印。<details>
<summary>Abstract</summary>
Synthetic time series are often used in practical applications to augment the historical time series dataset for better performance of machine learning algorithms, amplify the occurrence of rare events, and also create counterfactual scenarios described by the time series. Distributional-similarity (which we refer to as realism) as well as the satisfaction of certain numerical constraints are common requirements in counterfactual time series scenario generation requests. For instance, the US Federal Reserve publishes synthetic market stress scenarios given by the constrained time series for financial institutions to assess their performance in hypothetical recessions. Existing approaches for generating constrained time series usually penalize training loss to enforce constraints, and reject non-conforming samples. However, these approaches would require re-training if we change constraints, and rejection sampling can be computationally expensive, or impractical for complex constraints. In this paper, we propose a novel set of methods to tackle the constrained time series generation problem and provide efficient sampling while ensuring the realism of generated time series. In particular, we frame the problem using a constrained optimization framework and then we propose a set of generative methods including ``GuidedDiffTime'', a guided diffusion model to generate realistic time series. Empirically, we evaluate our work on several datasets for financial and energy data, where incorporating constraints is critical. We show that our approaches outperform existing work both qualitatively and quantitatively. Most importantly, we show that our ``GuidedDiffTime'' model is the only solution where re-training is not necessary for new constraints, resulting in a significant carbon footprint reduction.
</details>
<details>
<summary>摘要</summary>
通常情况下，人工时间序列被用于实际应用中增强历史时间序列数据，增加罕见事件的发生，以及创建具有时间序列的对应的counterfactualenario。实际性（即 Distributional-similarity）以及满足某些数学条件是常见的需求。例如，美国联邦储金会发布基于受限时间序列的 Synthetic market stress scenarios，用于评估金融机构在假设的经济衰退中的性能。现有的时间序列生成方法通常是通过权重loss来强制满足约束，并拒绝不符合约束的样本。然而，这些方法会需要重新训练，如果变更约束，并且拒绝样本可能是计算昂贵的，或者对于复杂的约束来说是不实用。在这篇论文中，我们提出一种新的方法来解决受约束时间序列生成问题，并提供高效的采样，同时保证生成的时间序列具有实际性。具体来说，我们将问题带入受约束优化框架，然后我们提出了一些生成方法，包括一种名为“GuidedDiffTime”的导向扩散模型，用于生成实际时间序列。我们对几个金融和能源数据集进行了实验，并证明我们的方法在质量和量上都超过了现有的方法。最重要的是，我们的“GuidedDiffTime”模型不需要重新训练，因此可以避免重新训练所带来的碳脚印。
</details></li>
</ul>
<hr>
<h2 id="Distributional-Model-Equivalence-for-Risk-Sensitive-Reinforcement-Learning"><a href="#Distributional-Model-Equivalence-for-Risk-Sensitive-Reinforcement-Learning" class="headerlink" title="Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning"></a>Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01708">http://arxiv.org/abs/2307.01708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Kastner, Murat A. Erdogdu, Amir-massoud Farahmand</li>
<li>for: 学习风险敏感奖励学习模型</li>
<li>methods: 使用分布式奖励学习引入两种新的模型相等性定义，一种是通用的但是 Computationally intractable，另一种是实用的可以选择希望计划优化的风险度量。</li>
<li>results: 在Tabular和大规模实验中证明了这种框架可以增强任何模型自由风险敏感算法，并提供了多种实际应用场景。<details>
<summary>Abstract</summary>
We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.
</details>
<details>
<summary>摘要</summary>
我团队正在研究风险敏感的推荐学习问题。我们理论上显示，合适的值相等方法，可以在风险中性设置下进行优化规划，但是这些方法在风险敏感设置下不够。我们利用分布式推荐学习来引入两种新的模型相等性，一种是通用的，可以用来规划任何风险度量，但是它是不可能实现的；另一种是实用的，允许您选择想要优化的风险度量。我们 demonstarte了我们的框架可以用来增强任何模型自由风险敏感算法，并提供了表格和大规模实验来证明其能力。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-is-all-you-need-removing-the-auxiliary-data-assumption-for-membership-inference-attacks-against-synthetic-data"><a href="#Synthetic-is-all-you-need-removing-the-auxiliary-data-assumption-for-membership-inference-attacks-against-synthetic-data" class="headerlink" title="Synthetic is all you need: removing the auxiliary data assumption for membership inference attacks against synthetic data"></a>Synthetic is all you need: removing the auxiliary data assumption for membership inference attacks against synthetic data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01701">http://arxiv.org/abs/2307.01701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florent Guépin, Matthieu Meeus, Ana-Maria Cretu, Yves-Alexandre de Montjoye</li>
<li>for: 这个论文目的是评估人工数据的隐私。</li>
<li>methods: 这篇论文使用了阴影模型进行会员推断攻击，以评估人工数据的安全性。</li>
<li>results: 研究人员通过使用只有人工数据进行三种攻击场景，成功地实现了会员推断攻击，并在两个实际数据集和两个人工数据生成器上进行了测试。这些结果表明，当审核人工数据时，可以减轻对auxiliary dataset的假设，从而实现实际的攻击。<details>
<summary>Abstract</summary>
Synthetic data is emerging as the most promising solution to share individual-level data while safeguarding privacy. Membership inference attacks (MIAs), based on shadow modeling, have become the standard to evaluate the privacy of synthetic data. These attacks, however, currently assume the attacker to have access to an auxiliary dataset sampled from a similar distribution as the training dataset. This often is a very strong assumption that would make an attack unlikely to happen in practice. We here show how this assumption can be removed and how MIAs can be performed using only the synthetic data. More specifically, in three different attack scenarios using only synthetic data, our results demonstrate that MIAs are still successful, across two real-world datasets and two synthetic data generators. These results show how the strong hypothesis made when auditing synthetic data releases - access to an auxiliary dataset - can be relaxed to perform an actual attack.
</details>
<details>
<summary>摘要</summary>
现代数据是许多领域的解决方案，它可以保护个人隐私的同时，共享个人数据。模型阴影攻击（MIAs），基于阴影模型，已成为评估合成数据隐私的标准方法。然而，这些攻击假设攻击者有访问类似于训练数据的 auxiliary dataset 的权限，这是一个很强的假设，在实际情况中很难发生。我们在这里显示了如何除掉这个假设，并使用只有合成数据进行 MIAs。具体来说，我们在三种不同的攻击场景中，使用了两个真实数据集和两个合成数据生成器，结果表明，MIAs 仍然成功，不需要 auxiliary dataset。这些结果表明，在审核合成数据发布时，对于实际攻击而言，可以放宽这一假设。
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-and-Solving-Infinite-Games-with-an-ERM-Oracle"><a href="#Online-Learning-and-Solving-Infinite-Games-with-an-ERM-Oracle" class="headerlink" title="Online Learning and Solving Infinite Games with an ERM Oracle"></a>Online Learning and Solving Infinite Games with an ERM Oracle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01689">http://arxiv.org/abs/2307.01689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelos Assos, Idan Attias, Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson</li>
<li>for: 这篇论文旨在解决在线学习中的泛化误差问题，而现有的算法依赖于计算不fficient的oracle，如标准优化算法（SOA）。</li>
<li>methods: 该论文提出了基于ERM oracle的在线二分类Setting的算法，并证明其在可 realizable 设定下具有有限征逐和在agnostic 设定下具有线性增长的 regret。 regret 的 bound 基于下面两个维度：Littlestone 维度和阈值维度。</li>
<li>results: 该论文显示了在非参数化游戏中，ERM oracle可以被视为best response oracle，并提供了基于best response oracle的学习算法，可以在两个玩家zero-sum 游戏和多个玩家general-sum 游戏中达到approximate-minimax 均衡和approximate coarse correlated 均衡，只要游戏有 bounded fat-threshold 维度。<details>
<summary>Abstract</summary>
While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.   We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero-sum games and approximate coarse correlated equilibria in multi-player general-sum games, as long as the game has a bounded fat-threshold dimension. Our algorithms apply to both binary-valued and real-valued games and can be viewed as providing justification for the wide use of double oracle and multiple oracle algorithms in the practice of solving large games.
</details>
<details>
<summary>摘要</summary>
在搜索学习设定下，ERM 可以达到近似优化的泛化误差，但在在线学习设定下，算法们往往需要计算效率低的oracle，如标准优化算法（SOA）。在这种情况下，我们提出了一种凭据仅仅基于ERM oracle call的在线二分类设定算法，并证明其在 realizable 设定下有finite regret，在agnostic 设定下有sublinearly growing regret。我们将 regret 约束为Littlestone和阈值维度的下界。在非参数学习游戏中，ERM oracle可以被视为最优回应 oracle，找到对某个玩家的历史玩家的最优回应。在这个设定下，我们提供了只凭据最优回应 oracle 的学习算法，可以在二player零Sum游戏中 converge to approximate-minimax equilibria，在多player general-sum游戏中 converge to approximate coarse correlated equilibria，只要游戏有bounded fat-threshold dimension。我们的算法适用于 binary-valued 和 real-valued 游戏，可以被视为对double oracle和多个 oracle 算法在实践中的 justify。
</details></li>
</ul>
<hr>
<h2 id="Serving-Graph-Neural-Networks-With-Distributed-Fog-Servers-For-Smart-IoT-Services"><a href="#Serving-Graph-Neural-Networks-With-Distributed-Fog-Servers-For-Smart-IoT-Services" class="headerlink" title="Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services"></a>Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01684">http://arxiv.org/abs/2307.01684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liekang Zeng, Xu Chen, Peng Huang, Ke Luo, Xiaoxi Zhang, Zhi Zhou</li>
<li>for: 这个论文是为了提供一个分布式实时 Graph Neural Network (GNN) 推论框架，以便在 IoT 驱动的智能应用中提供 GNN 服务。</li>
<li>methods: 这个论文使用了轻量级的 fog computing 技术，并将 GNN 推论框架分布在多个 fog 节点上，以便更好地利用 IoT 资料来源附近的多元化和动态资源。</li>
<li>results: 这个论文的实验和案例研究显示，Fograph 可以与现有的云 computing 和 fog 部署相比，提供更高的执行速度和过程效率，最高可以达到 5.39 倍的执行速度提升和 6.84 倍的过程提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have gained growing interest in miscellaneous applications owing to their outstanding ability in extracting latent representation on graph structures. To render GNN-based service for IoT-driven smart applications, traditional model serving paradigms usually resort to the cloud by fully uploading geo-distributed input data to remote datacenters. However, our empirical measurements reveal the significant communication overhead of such cloud-based serving and highlight the profound potential in applying the emerging fog computing. To maximize the architectural benefits brought by fog computing, in this paper, we present Fograph, a novel distributed real-time GNN inference framework that leverages diverse and dynamic resources of multiple fog nodes in proximity to IoT data sources. By introducing heterogeneity-aware execution planning and GNN-specific compression techniques, Fograph tailors its design to well accommodate the unique characteristics of GNN serving in fog environments. Prototype-based evaluation and case study demonstrate that Fograph significantly outperforms the state-of-the-art cloud serving and fog deployment by up to 5.39x execution speedup and 6.84x throughput improvement.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经在不同的应用中引起了广泛的关注，因为它们在图结构上能够提取潜在的表示。为了在智能应用中提供 GNN 基于服务，传统的模型服务 paradigm 通常会将全部的地理分布的输入数据上传到远程数据中心。然而，我们的实验测量表明，这种云端服务中的通信开销很大，而fog计算可以带来极大的潜在优势。为了最大化fog计算的建筑减法，在这篇论文中，我们提出了一种名为 Fograph 的分布式实时 GNN 推理框架。通过对不同类型的 fog 节点进行hetereogeneity-aware执行规划和 GNN 特定压缩技术，Fograph 设计得特别适应 GNN 在fog环境中的服务。实验和案例研究表明，Fograph 可以与状态艺术云服务和 fog 部署相比，提高执行速度和通过put Throughput by up to 5.39 倍和 6.84 倍。
</details></li>
</ul>
<hr>
<h2 id="Learning-Discrete-Weights-and-Activations-Using-the-Local-Reparameterization-Trick"><a href="#Learning-Discrete-Weights-and-Activations-Using-the-Local-Reparameterization-Trick" class="headerlink" title="Learning Discrete Weights and Activations Using the Local Reparameterization Trick"></a>Learning Discrete Weights and Activations Using the Local Reparameterization Trick</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01683">http://arxiv.org/abs/2307.01683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Berger, Aviv Navon, Ethan Fetaya</li>
<li>for: 降低计算机视ION和机器学习中的 neural network inference 计算复杂性和内存需求</li>
<li>methods: 使用 binarization 方法，将 neural network  weights 和活动化函数 binarized，以实现更高效的计算</li>
<li>results: 实现了针对 binary activations 的网络训练，并且可以在具有低资源的设备上进行高效的计算，并且可以实现 state-of-the-art 的Results<details>
<summary>Abstract</summary>
In computer vision and machine learning, a crucial challenge is to lower the computation and memory demands for neural network inference. A commonplace solution to address this challenge is through the use of binarization. By binarizing the network weights and activations, one can significantly reduce computational complexity by substituting the computationally expensive floating operations with faster bitwise operations. This leads to a more efficient neural network inference that can be deployed on low-resource devices. In this work, we extend previous approaches that trained networks with discrete weights using the local reparameterization trick to also allow for discrete activations. The original approach optimized a distribution over the discrete weights and uses the central limit theorem to approximate the pre-activation with a continuous Gaussian distribution. Here we show that the probabilistic modeling can also allow effective training of networks with discrete activation as well. This further reduces runtime and memory footprint at inference time with state-of-the-art results for networks with binary activations.
</details>
<details>
<summary>摘要</summary>
在计算机视觉和机器学习领域，一个重要的挑战是降低神经网络推理的计算和内存占用。一种常见的解决方案是通过binarization来实现。通过将神经网络权重和活动化值binarized，可以很大减少计算复杂性，将计算昂贵的浮点运算替换为更快的位运算。这导致一个更高效的神经网络推理，可以在低资源设备上部署。在这项工作中，我们将之前的方法扩展，使得神经网络可以使用离散权重和离散活动化值进行训练。原来的方法使用了local reparameterization trick来优化分布式权重的学习，并使用中心假设定理来近似预活化的 kontinuous Gaussian Distribution。在这里，我们表明了概率模型也可以有效地训练离散活动化的神经网络。这进一步减少了执行时间和内存占用，并达到了当前最佳的结果，在神经网络中使用二进制活动化。
</details></li>
</ul>
<hr>
<h2 id="RaidEnv-Exploring-New-Challenges-in-Automated-Content-Balancing-for-Boss-Raid-Games"><a href="#RaidEnv-Exploring-New-Challenges-in-Automated-Content-Balancing-for-Boss-Raid-Games" class="headerlink" title="RaidEnv: Exploring New Challenges in Automated Content Balancing for Boss Raid Games"></a>RaidEnv: Exploring New Challenges in Automated Content Balancing for Boss Raid Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01676">http://arxiv.org/abs/2307.01676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeon-Chang Jeon, In-Chang Baek, Cheong-mok Bae, Taehwa Park, Wonsang You, Taegwan Ha, Hoyun Jung, Jinha Noh, Seungwon Oh, Kyung-Joong Kim</li>
<li>for: 这研究旨在提供一个新的游戏模拟器和两个评价指标，用于自动游戏内容均衡。</li>
<li>methods: 这研究使用人工智能技术自动调整游戏内容，并在MMORPG游戏中采用多样化和可定制的内容来测试其效果。</li>
<li>results: 这研究提出了一个新的游戏研究平台，可以扩大自动游戏均衡问题的研究范围，并提供一个真实的游戏生产管道中的框架。<details>
<summary>Abstract</summary>
The balance of game content significantly impacts the gaming experience. Unbalanced game content diminishes engagement or increases frustration because of repetitive failure. Although game designers intend to adjust the difficulty of game content, this is a repetitive, labor-intensive, and challenging process, especially for commercial-level games with extensive content. To address this issue, the game research community has explored automated game balancing using artificial intelligence (AI) techniques. However, previous studies have focused on limited game content and did not consider the importance of the generalization ability of playtesting agents when encountering content changes. In this study, we propose RaidEnv, a new game simulator that includes diverse and customizable content for the boss raid scenario in MMORPG games. Additionally, we design two benchmarks for the boss raid scenario that can aid in the practical application of game AI. These benchmarks address two open problems in automatic content balancing, and we introduce two evaluation metrics to provide guidance for AI in automatic content balancing. This novel game research platform expands the frontiers of automatic game balancing problems and offers a framework within a realistic game production pipeline.
</details>
<details>
<summary>摘要</summary>
游戏内容平衡对游戏体验产生很大影响。不均衡的游戏内容会导致玩家失望或厌烦，因为玩家需要重复失败。虽然游戏设计师希望通过调整游戏内容的Difficulty来解决这个问题，但这是一项重复、劳动 INTENSIVE 和挑战性较高的过程，特别是在商业级游戏中。为解决这个问题，游戏研究社区已经开始使用人工智能（AI）技术自动平衡游戏内容。然而，前一些研究都集中在有限的游戏内容上，并未考虑AI游戏测试者在内容变化时的总体化能力的重要性。在这项研究中，我们提出了RaidEnv，一个新的游戏模拟器，包括MMORPG游戏中的bossoid难度scenario中的多样化和可定制内容。此外，我们设计了两个备用测试基准，可以帮助在游戏AI中自动平衡内容的实践应用。这两个基准解决了游戏自动平衡问题中的两个开放问题，并我们引入了两个评价指标，为AI在自动平衡内容中提供指导。这种新的游戏研究平台扩展了自动游戏平衡问题的前iers，并提供了一个在真实游戏生产管道中可行的框架。
</details></li>
</ul>
<hr>
<h2 id="mPLUG-DocOwl-Modularized-Multimodal-Large-Language-Model-for-Document-Understanding"><a href="#mPLUG-DocOwl-Modularized-Multimodal-Large-Language-Model-for-Document-Understanding" class="headerlink" title="mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"></a>mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02499">http://arxiv.org/abs/2307.02499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/x-plug/mplug-docowl">https://github.com/x-plug/mplug-docowl</a></li>
<li>paper_authors: Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang</li>
<li>for: 这篇论文是关于免需OCR文档理解的研究，旨在提高现有多种模型的 document understanding 能力。</li>
<li>methods: 该论文使用了 mPLUG-Owl 模型，并通过自定义数据集和训练策略进行了强化和调整。</li>
<li>results: 实验结果表明，该模型在免需OCR文档理解任务上表现出色，并且在不具体 fine-tuning 的情况下也能够在多个下游任务上发挥良好的效果。<details>
<summary>Abstract</summary>
Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy. We also build an OCR-free document instruction understanding evaluation set LLMDoc to better compare models' capabilities on instruct compliance and document understanding. Experimental results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOwl generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at https://github.com/X-PLUG/mPLUG-DocOwl.
</details>
<details>
<summary>摘要</summary>
文档理解指的是自动提取、分析和理解各种数字文档中的信息，如网页。现有的多模型大型语言模型（MLLMs），包括mPLUG-Owl，在零批量情况下已经表现出了扑捉人的可能性，这表明它们可能为无需OCR的文档理解做出贡献。然而，无法在域内训练时，这些模型往往忽略细腻的OCR特征，如复杂的表格或大块文本，这些特征是无需OCR文档理解的关键。在这篇论文中，我们提出了基于mPLUG-Owl的mPLUG-DocOwl模型，用于无需OCR的文档理解。具体来说，我们首先构建了一个具有各种视觉语言理解任务的指导调教数据集。然后，我们通过将模型同时在语言只、通用视觉语言和文档指导调教数据集上进行联合训练，使模型具备更强的无需OCR文档理解能力。此外，我们还建立了一个无需OCR文档指导理解评估集LLMDoc，以更好地比较模型在指令遵从和文档理解方面的能力。实验结果表明，我们的模型在现有多modal模型中表现出色，并且无需特定的 fine-tuning，mPLUG-DocOwl在多种下游任务上具有良好的普适性。我们的代码、模型、训练数据和评估集可以在https://github.com/X-PLUG/mPLUG-DocOwl上获取。
</details></li>
</ul>
<hr>
<h2 id="SwinGNN-Rethinking-Permutation-Invariance-in-Diffusion-Models-for-Graph-Generation"><a href="#SwinGNN-Rethinking-Permutation-Invariance-in-Diffusion-Models-for-Graph-Generation" class="headerlink" title="SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation"></a>SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01646">http://arxiv.org/abs/2307.01646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiyan98/swingnn">https://github.com/qiyan98/swingnn</a></li>
<li>paper_authors: Qi Yan, Zhengyang Liang, Yang Song, Renjie Liao, Lele Wang</li>
<li>For: 本文研究了基于Permutation-equivariant networks的扩散模型，可以学习 permutation-invariant的分布。但是，与非恒等模型相比，这些 invariable 模型在学习中遇到更大的挑战，其有效的目标分布具有更多的模式，且最佳一步噪声分布是 Gaussian mixture 的分布。* Methods: 本文提出了一种非恒等扩散模型，即 $\textit{SwinGNN}$，它使用高效的 edge-to-edge 2-WL 消息传递网络，并使用 shifted window 基于 SwinTransformers 的自注意力。此外，通过系统的ablations，我们确定了一些关键的训练和采样技术，可以大幅提高生成的样本质量。* Results: 我们的 $\textit{SwinGNN}$ 在 synthetic 和实际的蛋白质和分子数据集上达到了领先的性能。我们的代码在 <a target="_blank" rel="noopener" href="https://github.com/qiyan98/SwinGNN">https://github.com/qiyan98/SwinGNN</a> 上发布。<details>
<summary>Abstract</summary>
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph generative model to a permutation-invariant one. Extensive experiments on synthetic and real-world protein and molecule datasets show that our SwinGNN achieves state-of-the-art performances. Our code is released at https://github.com/qiyan98/SwinGNN.
</details>
<details>
<summary>摘要</summary>
“基于 permutation-equivariant 网络的扩散模型可以学习 permutation-invariant 分布 для图数据。然而，相比其非 invariat 对手，我们发现这些 invariat 模型在学习中遇到更大的挑战，主要表现在以下两点：1) 其有效目标分布具有更多的模式; 2) 其最佳一步干扰得分函数是 Gaussian 混合函数的更多组件。这些分析结果为我们提供了灵感，我们提议一种非 invariat 扩散模型，即 $\textit{SwinGNN}$，该模型使用高效的 edge-to-edge 2-WL 消息传递网络，并使用 shifted window 基于 SwinTransformers 的自注意力。此外，通过系统性的ablation 研究，我们确定了一些关键的训练和采样技术，可以大幅提高生成的样本质量。最后，我们提出了一个简单的后处理技术，即随机排序生成的图，这可以证明任何图生成模型都可以变换为 permutation-invariant 模型。我们在 synthetic 和实际世界的蛋白质和分子数据上进行了广泛的实验，并证明了我们的 SwinGNN 达到了状态的最佳性能。我们的代码可以在 https://github.com/qiyan98/SwinGNN 上下载。”
</details></li>
</ul>
<hr>
<h2 id="Insert-expansions-for-Tool-enabled-Conversational-Agents"><a href="#Insert-expansions-for-Tool-enabled-Conversational-Agents" class="headerlink" title="Insert-expansions for Tool-enabled Conversational Agents"></a>Insert-expansions for Tool-enabled Conversational Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01644">http://arxiv.org/abs/2307.01644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Göldi, Roman Rietsche</li>
<li>for: 这篇论文关注大语言模型中的链条思维提示实现方法，特别是在用工具（或“插件”）在明确的思维路径中生成的 conversational agents 中使用工具。</li>
<li>methods: 我们使用 conversation analysis 来研究用户如何在 conversational agents 中提供必要的细节和纠正请求，以便实现更好的回答。</li>
<li>results: 我们通过两个实验直接比较，发现在推荐领域使用“用户为工具”方法可以获得利益。<details>
<summary>Abstract</summary>
This paper delves into an advanced implementation of Chain-of-Thought-Prompting in Large Language Models, focusing on the use of tools (or "plug-ins") within the explicit reasoning paths generated by this prompting method. We find that tool-enabled conversational agents often become sidetracked, as additional context from tools like search engines or calculators diverts from original user intents. To address this, we explore a concept wherein the user becomes the tool, providing necessary details and refining their requests. Through Conversation Analysis, we characterize this interaction as insert-expansion - an intermediary conversation designed to facilitate the preferred response. We explore possibilities arising from this 'user-as-a-tool' approach in two empirical studies using direct comparison, and find benefits in the recommendation domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Heuristic-Algorithms-for-the-Approximation-of-Mutual-Coherence"><a href="#Heuristic-Algorithms-for-the-Approximation-of-Mutual-Coherence" class="headerlink" title="Heuristic Algorithms for the Approximation of Mutual Coherence"></a>Heuristic Algorithms for the Approximation of Mutual Coherence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01639">http://arxiv.org/abs/2307.01639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregor Betz, Vera Chekan, Tamara Mchedlidze</li>
<li>for: This paper aims to accelerate the computation of mutual coherence, which is a measure of similarity between two opinions, in the context of the Wahl-O-Mat system used in Germany to help voters find candidates that align with their political preferences.</li>
<li>methods: The authors model the distribution of confirmation values as a mixture of three Gaussians and present efficient heuristics to estimate the model parameters. They also use the expected value of the distribution to approximate the mutual coherence. Some of the presented algorithms are fully polynomial-time, while others only require solving a small number of instances of the SAT model counting problem.</li>
<li>results: The authors’ best algorithm achieves an average squared error of less than 0.0035, which is considered insignificant given the efficiency of the algorithm. The accuracy is precise enough to be used in Wahl-O-Mat-like systems.<details>
<summary>Abstract</summary>
Mutual coherence is a measure of similarity between two opinions. Although the notion comes from philosophy, it is essential for a wide range of technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters to find candidates that are the closest to their political preferences. The exact computation of mutual coherence is highly time-consuming due to the iteration over all subsets of an opinion. Moreover, for every subset, an instance of the SAT model counting problem has to be solved which is known to be a hard problem in computer science. This work is the first study to accelerate this computation. We model the distribution of the so-called confirmation values as a mixture of three Gaussians and present efficient heuristics to estimate its model parameters. The mutual coherence is then approximated with the expected value of the distribution. Some of the presented algorithms are fully polynomial-time, others only require solving a small number of instances of the SAT model counting problem. The average squared error of our best algorithm lies below 0.0035 which is insignificant if the efficiency is taken into account. Furthermore, the accuracy is precise enough to be used in Wahl-O-Mat-like systems.
</details>
<details>
<summary>摘要</summary>
共同 coherence 是两个意见之间的相似度量量。这个概念来自哲学，但它对各种技术领域都是关键的，例如德国的 Wahl-O-Mat 系统。这个系统帮助选民找到最接近其政治偏好的候选人。然而，正确计算共同 coherence 是非常时间consuming，因为需要遍历所有意见集合中的所有子集，并对每个子集解决一个 SAT 模型计数问题。这个问题在计算机科学中是一个知名的困难问题。这个研究是第一个加速这种计算的研究。我们模型了confirmation value 的分布为三个 Gaussian 的混合，并提供了高效的启发法来估算其模型参数。然后，我们使用这些参数来近似共同 coherence。我们的算法中有一些是完全 polynomial-time，另一些只需解决一小数量的 SAT 模型计数问题。我们的最佳算法的平均平方误差低于 0.0035，这是可以忽略的。此外，我们的精度够高，可以用于 Wahl-O-Mat 类系统。
</details></li>
</ul>
<hr>
<h2 id="Random-Walk-on-Multiple-Networks"><a href="#Random-Walk-on-Multiple-Networks" class="headerlink" title="Random Walk on Multiple Networks"></a>Random Walk on Multiple Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01637">http://arxiv.org/abs/2307.01637</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flyingdoog/rwm">https://github.com/flyingdoog/rwm</a></li>
<li>paper_authors: Dongsheng Luo, Yuchen Bian, Yaowei Yan, Xiong Yu, Jun Huan, Xiao Liu, Xiang Zhang<br>for: 本研究旨在利用多个网络来提高实体之间的归一化和网络推荐等任务中的做出更好的推断。methods: 本研究提出了随机游走在多个网络上（RWM），可以处理多种多网络和多类实体的数据。RWM使用随机游走者在每个网络上访问节点，并计算每个节点的本地邻居关系（i.e., 节点访问概率）。在发现类似访问概率的节点时，游走者们强制合作。results: 研究人员通过分析RWM的整合性和可靠性，并提出了两种可靠性保证的优化方法。在链接预测、网络嵌入和本地社区检测等任务中，RWM表现出色，并且在实际数据上进行了广泛的 экспериментирования。<details>
<summary>Abstract</summary>
Random Walk is a basic algorithm to explore the structure of networks, which can be used in many tasks, such as local community detection and network embedding. Existing random walk methods are based on single networks that contain limited information. In contrast, real data often contain entities with different types or/and from different sources, which are comprehensive and can be better modeled by multiple networks. To take advantage of rich information in multiple networks and make better inferences on entities, in this study, we propose random walk on multiple networks, RWM. RWM is flexible and supports both multiplex networks and general multiple networks, which may form many-to-many node mappings between networks. RWM sends a random walker on each network to obtain the local proximity (i.e., node visiting probabilities) w.r.t. the starting nodes. Walkers with similar visiting probabilities reinforce each other. We theoretically analyze the convergence properties of RWM. Two approximation methods with theoretical performance guarantees are proposed for efficient computation. We apply RWM in link prediction, network embedding, and local community detection. Comprehensive experiments conducted on both synthetic and real-world datasets demonstrate the effectiveness and efficiency of RWM.
</details>
<details>
<summary>摘要</summary>
随机漫步是一种基本算法，用于探索网络结构，可以用于多种任务，如本地社区检测和网络嵌入。现有的随机漫步方法基于单个网络，它们只包含有限信息。然而，实际数据通常包含不同类型的实体或来自不同来源的实体，这些信息更加全面，可以更好地使用多个网络来模型。为了利用多个网络中的丰富信息，提高对实体的推断，我们提出了随机漫步多网络（RWM）。RWM是灵活的，支持多类多网络和通用多网络，它们可能形成多对多节点映射。RWM将在每个网络上Random walker，以获取起始节点的本地邻近性（即节点访问概率）。漫步者之间相似的访问概率强化对方。我们 theoretically 分析 RWM 的收敛性质。我们还提出了两种有理性 guarantees 的近似方法，用于有效地计算。我们在链接预测、网络嵌入和本地社区检测中应用 RWM。我们在 Synthetic 和实际数据集上进行了广泛的实验，并证明了 RWM 的效果和效率。
</details></li>
</ul>
<hr>
<h2 id="SageFormer-Series-Aware-Graph-Enhanced-Transformers-for-Multivariate-Time-Series-Forecasting"><a href="#SageFormer-Series-Aware-Graph-Enhanced-Transformers-for-Multivariate-Time-Series-Forecasting" class="headerlink" title="SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting"></a>SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01616">http://arxiv.org/abs/2307.01616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwei Zhang, Xin Wang, Yuantao Gu</li>
<li>for: 本文旨在提出一种能够有效地捕捉和模型系列之间的依赖关系的系列意识推断模型，以提高多ivariate时间序列预测的准确性。</li>
<li>methods: 本文提出了一种基于图结构的Series-aware Graph-enhanced Transformer模型，可以有效地表示多个时间序列的多样性模式，并避免系列之间的重复信息。</li>
<li>results: 经过对真实数据和 sintetic 数据的广泛实验，本文表明了SageFormer 模型在比较现有方法时的显著性能优势。<details>
<summary>Abstract</summary>
Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
多变量时间序列预测在多个领域发挥重要作用。Recent Advances in Deep Learning Methods, especially Transformers, have shown promise, but there is still a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.Here is the word-for-word translation of the text into Simplified Chinese:多变量时间序列预测在多个领域发挥重要作用。最近的深度学习方法，特别是转换器，已经展示了承诺，但还有一个差距在强调多个时间序列之间的相互关系。这篇论文介绍了SageFormer，一种基于图strucutres的Series-aware Graph-enhanced Transformer模型，旨在通过图结构来有效地捕捉和模型时间序列之间的相互关系。SageFormer解决了两个关键挑战：一是在多个时间序列之间有效地表示多样的时间模式，二是在多个时间序列之间减少重复的信息。重要的是，提议的系列意识框架可以轻松地与现有的转换器基本模型集成，从而增强其模型多个时间序列之间的相互关系。通过对真实世界和 sintetic数据集进行了广泛的实验，我们展示了SageFormer的性能超过了前一个状态的方法。
</details></li>
</ul>
<hr>
<h2 id="Overconfidence-is-a-Dangerous-Thing-Mitigating-Membership-Inference-Attacks-by-Enforcing-Less-Confident-Prediction"><a href="#Overconfidence-is-a-Dangerous-Thing-Mitigating-Membership-Inference-Attacks-by-Enforcing-Less-Confident-Prediction" class="headerlink" title="Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction"></a>Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01610">http://arxiv.org/abs/2307.01610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dependablesystemslab/mia_defense_hamp">https://github.com/dependablesystemslab/mia_defense_hamp</a></li>
<li>paper_authors: Zitao Chen, Karthik Pattabiraman</li>
<li>for: 防止机器学习模型被攻击，保护模型的训练数据隐私。</li>
<li>methods: 提出了一种防御技术，即高度积震训练框架和均衡抑制器，使模型在训练和测试样本上具有类似的预测结果，从而保护模型的隐私。</li>
<li>results: 对五个 benchmark 数据集进行了广泛的评估，并显示了HAMP 可以保持高度的准确率和强的会员隐私。与七种现有的防御技术进行比较，HAMP 在隐私利用与用途之间具有更好的质量比。<details>
<summary>Abstract</summary>
Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving high accuracy. To further reduce privacy risk, HAMP uniformly modifies all the prediction outputs to become low-confidence outputs while preserving the accuracy, which effectively obscures the differences between the prediction on members and non-members. We conduct extensive evaluation on five benchmark datasets, and show that HAMP provides consistently high accuracy and strong membership privacy. Our comparison with seven state-of-the-art defenses shows that HAMP achieves a superior privacy-utility trade off than those techniques.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）模型容易受到会员推测攻击（MIA），该攻击可以判断给定输入是否用于训练目标模型。虽然有许多防御技术，但它们通常受到有限的隐私保护、大幅下降精度和/或需要难以获得的额外数据。这个工作提出了一种防御技术，即HAMP，可以同时保障强大的会员隐私和高精度。为了 Mitigate MIAs 的不同形式，我们发现它们都利用 ML 模型对训练样本的过于自信。这种动机导致我们的设计强制模型在测试样本上预测更加不自信，从而使模型在训练和测试样本上行为相同。HAMP 包括一种新的训练框架，高 entropy 软标签和一种基于Entropy的正则化器，以限制模型的预测，并且仍然实现高精度。为了进一步减少隐私风险，HAMP 对所有预测输出进行同步修改，使其变成低自信输出，保持精度，同时减少了会员隐私风险。我们对五个 benchmark 数据集进行了广泛的评估，并表明HAMP 可以保持高精度和强大的会员隐私。与七种 state-of-the-art 防御技术进行比较，我们发现HAMP 在隐私利用和实用性之间取得了更好的平衡。
</details></li>
</ul>
<hr>
<h2 id="Bridge-the-Performance-Gap-in-Peak-hour-Series-Forecasting-The-Seq2Peak-Framework"><a href="#Bridge-the-Performance-Gap-in-Peak-hour-Series-Forecasting-The-Seq2Peak-Framework" class="headerlink" title="Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework"></a>Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01597">http://arxiv.org/abs/2307.01597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwei Zhang, Xin Wang, Jingyuan Xie, Heling Zhang, Yuantao Gu</li>
<li>For: 预测高峰时间序列 (PHSF) 是各个领域中的一项重要 yet 未经充分利用的任务。现有的深度学习模型在常规时间序列预测 (TSF) 中具有出色的表现，但在 PHSF 中却表现不佳。这可以归结于高峰时间序列的非站ARY性问题，导致直接预测更加困难于标准 TSF。* Methods: 该 paper 提出了一个名为 Seq2Peak 的新框架，用于解决 PHSF 任务。Seq2Peak 包括两个关键组成部分：一个名为 CyclicNorm 的管道，用于 Mitigate 非站ARY性问题，以及一个简单 yet effective 的可调 Parameters-free 峰值时间解码器，使用 hybrid 损失函数，将原始序列和峰值序列作为监督信号。* Results: 对于四个实际 dataset 进行了广泛的实验，证明了提posed 框架的有效性，得到了平均相对改进率为 37.7%，对于 transformer- 和非 transformer- 基于 TSF 模型。<details>
<summary>Abstract</summary>
Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in various domains. While state-of-the-art deep learning models excel in regular Time Series Forecasting (TSF), they struggle to achieve comparable results in PHSF. This can be attributed to the challenges posed by the high degree of non-stationarity in peak-hour series, which makes direct forecasting more difficult than standard TSF. Additionally, manually extracting the maximum value from regular forecasting results leads to suboptimal performance due to models minimizing the mean deficit. To address these issues, this paper presents Seq2Peak, a novel framework designed specifically for PHSF tasks, bridging the performance gap observed in TSF models. Seq2Peak offers two key components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid loss function that utilizes both the original series and peak-hour series as supervised signals. Extensive experimentation on publicly available time series datasets demonstrates the effectiveness of the proposed framework, yielding a remarkable average relative improvement of 37.7\% across four real-world datasets for both transformer- and non-transformer-based TSF models.
</details>
<details>
<summary>摘要</summary>
《峰值时间序列预测（PHSF）是许多领域中的一项重要 yet 未得到充分研究的任务。当前的深度学习模型在标准时间序列预测（TSF）中表现出色，但在 PHSF 中却表现不佳。这可以归结于峰值时间序列的非站点性问题，导致直接预测变得更加困难。此外，手动提取常规预测结果中的最大值会导致性能下降，因为模型会尝试最小化均方误差。为了解决这些问题，本文提出了 Seq2Peak 框架，这是一种特有的 PHSF 任务解决方案， bridging 标准 TSF 模型的性能差距。Seq2Peak 框架包括两个关键组件：CyclingNorm 管道来 mitigate 非站点性问题，以及一个简单 yet 高效的可学习参数无法 peak-hour 解码器，使用两种不同的超参数来学习峰值时间序列和常规时间序列。经过对公开的时间序列数据集进行了广泛的实验，显示了提议的框架的效果，其中平均相对提升率为 37.7%，对四个实际世界数据集中的两种 transformer 和非 transformer 基于 TSF 模型进行了评估。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Tuning-Pushes-Farther-Contrastive-Learning-Pulls-Closer-A-Two-Stage-Approach-to-Mitigate-Social-Biases"><a href="#Prompt-Tuning-Pushes-Farther-Contrastive-Learning-Pulls-Closer-A-Two-Stage-Approach-to-Mitigate-Social-Biases" class="headerlink" title="Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases"></a>Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01595">http://arxiv.org/abs/2307.01595</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyingji1996/CCPA">https://github.com/liyingji1996/CCPA</a></li>
<li>paper_authors: Yingji Li, Mengnan Du, Xin Wang, Ying Wang<br>for: 这个研究旨在减少预训练 corpora 中的社会偏见，并提高预训练 Language Model 的表现。methods: 本研究使用了两个阶段的方法：第一个阶段是基于 continuous prompt tuning 的数据增强方法，第二个阶段是使用对照学习抑制预训练 Model 的参数。results: 实验结果显示，CCPA 比基于 Counterfactual Data Augmentation 的方法有更好的减少社会偏见表现，并且保持了预训练 Language Model 的语言模型表现。<details>
<summary>Abstract</summary>
As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs' encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs' parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.
</details>
<details>
<summary>摘要</summary>
随着预训言语模型（PLMs）的表达能力提高，社会偏见的继承问题日益减少。大多数前一代减偏技术使用Counterfactual Data Augmentation（CDA）来填补训练集。然而，CDA只是略微修改原始集合，因此在不同的人口组中的表达距离仍然很窄。这会使减偏模型轻松地适应差异 между counterfactual pair，从而影响其减偏性能。在这篇论文中，我们提出一种基于对抗学习的两stage减偏模型，使用Continuous Prompt Augmentation（CPA）来减少PLMs的社会偏见。在第一个阶段，我们提出一种基于连续提示调整的数据增强方法，以增加不同人口组之间的表达距离。在第二个阶段，我们使用对抗学习来吸引增强后的样本对以更近的表达距离，然后细化PLMs的参数以获得减偏编码。我们的方法会导致模型增强减偏性能，通过增加训练过程的困难程度。广泛的实验表明，CCPA超过基准的减偏性能。同时，在GLUE benchmark上的实验结果表明，CCPA保留了PLMs的语言模型能力。
</details></li>
</ul>
<hr>
<h2 id="Cross-Element-Combinatorial-Selection-for-Multi-Element-Creative-in-Display-Advertising"><a href="#Cross-Element-Combinatorial-Selection-for-Multi-Element-Creative-in-Display-Advertising" class="headerlink" title="Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising"></a>Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01593">http://arxiv.org/abs/2307.01593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhang, Ping Zhang, Jian Dong, Yongkang Wang, Pengye Zhang, Bo Zhang, Xingxing Wang, Dong Wang</li>
<li>for: 提高电商广告创作效果</li>
<li>methods: 跨元素组合选择框架(CECS)</li>
<li>results: 实现了最高纪录的Offline指标分数，并且在实际业务中实现了6.02%的点击率和10.37%的营收增长<details>
<summary>Abstract</summary>
The effectiveness of ad creatives is greatly influenced by their visual appearance. Advertising platforms can generate ad creatives with different appearances by combining creative elements provided by advertisers. However, with the increasing number of ad creative elements, it becomes challenging to select a suitable combination from the countless possibilities. The industry's mainstream approach is to select individual creative elements independently, which often overlooks the importance of interaction between creative elements during the modeling process. In response, this paper proposes a Cross-Element Combinatorial Selection framework for multiple creative elements, termed CECS. In the encoder process, a cross-element interaction is adopted to dynamically adjust the expression of a single creative element based on the current candidate creatives. In the decoder process, the creative combination problem is transformed into a cascade selection problem of multiple creative elements. A pointer mechanism with a cascade design is used to model the associations among candidates. Comprehensive experiments on real-world datasets show that CECS achieved the SOTA score on offline metrics. Moreover, the CECS algorithm has been deployed in our industrial application, resulting in a significant 6.02% CTR and 10.37% GMV lift, which is beneficial to the business.
</details>
<details>
<summary>摘要</summary>
“广告创意的有效性受到它的见识性标志影响。广告平台可以通过不同的创意元素结合来生成不同的创意标志。然而，随着创意元素的数量增加，选择合适的结合成为愈来愈困难。industry的主流方法是选择个别创意元素独立，往往忽略了创意元素间的互动过程中的重要性。对此，本文提出了跨元素兼容选择框架，简称为CECS。在Encoder过程中，采用跨元素互动以静态地调整单一创意元素的表达，以满足目前候选的创意标志。在Decoder过程中，创意组合问题转化为多个创意元素之间的传递选择问题。使用一个链接机制，模型候选者之间的协调关系。实际测试结果显示，CECS取得了线上数据上的SOTA分数。此外，CECS算法已经在我们的商业应用中实现了6.02%的Click Through Rate（CTR）和10.37%的Gross Merchandise Value（GMV）提升，对业务有益。”
</details></li>
</ul>
<hr>
<h2 id="Transcribing-Educational-Videos-Using-Whisper-A-preliminary-study-on-using-AI-for-transcribing-educational-videos"><a href="#Transcribing-Educational-Videos-Using-Whisper-A-preliminary-study-on-using-AI-for-transcribing-educational-videos" class="headerlink" title="Transcribing Educational Videos Using Whisper: A preliminary study on using AI for transcribing educational videos"></a>Transcribing Educational Videos Using Whisper: A preliminary study on using AI for transcribing educational videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03200">http://arxiv.org/abs/2307.03200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashwin Rao</li>
<li>for: 这篇论文是为了探讨如何使用自动语音识别（ASR）系统来提高电子学习视频的掌握效果。</li>
<li>methods: 该论文使用了各种语音识别算法和技术来生成视频的字幕，并对25个教育视频进行了评估。</li>
<li>results: 研究发现，使用ASR系统可以减少对视频的杂音和干扰的影响，并提高视频的掌握效果。同时，还有一些开放的研究方向，如如何更好地识别教育视频中的语音、如何提高语音识别精度等。<details>
<summary>Abstract</summary>
Videos are increasingly being used for e-learning, and transcripts are vital to enhance the learning experience. The costs and delays of generating transcripts can be alleviated by automatic speech recognition (ASR) systems. In this article, we quantify the transcripts generated by whisper for 25 educational videos and identify some open avenues of research when leveraging ASR for transcribing educational videos.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="IAdet-Simplest-human-in-the-loop-object-detection"><a href="#IAdet-Simplest-human-in-the-loop-object-detection" class="headerlink" title="IAdet: Simplest human-in-the-loop object detection"></a>IAdet: Simplest human-in-the-loop object detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01582">http://arxiv.org/abs/2307.01582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/franchesoni/iadet">https://github.com/franchesoni/iadet</a></li>
<li>paper_authors: Franco Marchesoni-Acland, Gabriele Facciolo</li>
<li>for: 这个论文是为了提出一种人工智能注解策略，帮助在数据标注过程中训练模型。</li>
<li>methods: 这个策略包括三个模块：一、助け物标注；二、背景模型训练；三、活动选择下一个数据点。这个框架下开源了一个专门用于单类物体检测的工具——IAdet。</li>
<li>results: 对于PASCAL VOC数据集，IAdet工具可以将数据标注时间减少$25%$，并提供一个免费的训练模型。这些结果是基于一个故意简单的IAdet设计而得到的。因此，IAdet具有多种可以轻松改进的可能性，这为人工智能 loop对象检测系统开创了道路。<details>
<summary>Abstract</summary>
This work proposes a strategy for training models while annotating data named Intelligent Annotation (IA). IA involves three modules: (1) assisted data annotation, (2) background model training, and (3) active selection of the next datapoints. Under this framework, we open-source the IAdet tool, which is specific for single-class object detection. Additionally, we devise a method for automatically evaluating such a human-in-the-loop system. For the PASCAL VOC dataset, the IAdet tool reduces the database annotation time by $25\%$ while providing a trained model for free. These results are obtained for a deliberately very simple IAdet design. As a consequence, IAdet is susceptible to multiple easy improvements, paving the way for powerful human-in-the-loop object detection systems.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种名为智能标注（IA）的模型训练策略。IA包括三个模块：（1）助记数据标注、（2）背景模型训练和（3）活动选择下一个数据点。在这个框架下，我们开源了专门 для单类物体检测的IADE工具。此外，我们开发了一种自动评估这种人工Loop系统的方法。对于PASCAL VOC数据集，IADE工具可以将数据库标注时间减少$25\%$，并提供一个免费的训练模型。这些结果是基于一个故意非常简单的IADE设计来获得的。因此，IADE具有多个容易改进的地方，这将为人工Loop对象检测系统开额。
</details></li>
</ul>
<hr>
<h2 id="Optimal-and-Efficient-Binary-Questioning-for-Human-in-the-Loop-Annotation"><a href="#Optimal-and-Efficient-Binary-Questioning-for-Human-in-the-Loop-Annotation" class="headerlink" title="Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation"></a>Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01578">http://arxiv.org/abs/2307.01578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franco Marchesoni-Acland, Jean-Michel Morel, Josselin Kherroubi, Gabriele Facciolo</li>
<li>for: 本研究旨在解决人工纠正数据集的全面标注问题，即使有预测器可用。</li>
<li>methods: 本研究使用了一系列的优化策略和lookahead最小化代理成本函数来解决问题。</li>
<li>results: 对于 synthetic 和实际世界的数据集，提议的方法可以实现 significiant 改善（23-86%）的标注效率。In English, that would be:</li>
<li>for: The paper aims to solve the problem of fully annotating a binary classification dataset when a predictor is available.</li>
<li>methods: The paper uses a series of optimization strategies and lookahead minimization of proxy cost functions to solve the problem.</li>
<li>results: On synthetic and real-world datasets, the proposed method achieves significant improvements (23-86%) in annotation efficiency.<details>
<summary>Abstract</summary>
Even though data annotation is extremely important for interpretability, research and development of artificial intelligence solutions, most research efforts such as active learning or few-shot learning focus on the sample efficiency problem. This paper studies the neglected complementary problem of getting annotated data given a predictor. For the simple binary classification setting, we present the spectrum ranging from optimal general solutions to practical efficient methods. The problem is framed as the full annotation of a binary classification dataset with the minimal number of yes/no questions when a predictor is available. For the case of general binary questions the solution is found in coding theory, where the optimal questioning strategy is given by the Huffman encoding of the possible labelings. However, this approach is computationally intractable even for small dataset sizes. We propose an alternative practical solution based on several heuristics and lookahead minimization of proxy cost functions. The proposed solution is analysed, compared with optimal solutions and evaluated on several synthetic and real-world datasets. On these datasets, the method allows a significant improvement ($23-86\%$) in annotation efficiency.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管数据标注对人工智能研发和解释性很重要，大多数研究强调样本效率问题，这篇论文却研究受到忽略的问题——使用预测器获取标注数据。在简单的二分类设置下，我们提出了谱范围从优质通用解决方案到实用效果的方法。问题设为使用预测器完全标注二分类 dataset 的最小 yes/no 问题数。对于普通的二分类问题，解决方案基于编码理论，其中优化问题的策略是根据可能的标签编码使用 Huffman 编码。然而，这种方法对小型数据集来说是计算拥塞的。我们提出了一种实用的解决方案，基于多种准则和lookahead最小化代理成本函数。解决方案被分析、与优质解决方案进行比较，并在多个 sintetic 和实际世界数据集上评估。在这些数据集上，方法可以实现23-86%的标注效率提升。
</details></li>
</ul>
<hr>
<h2 id="Conceptual-Cognitive-Maps-Formation-with-Neural-Successor-Networks-and-Word-Embeddings"><a href="#Conceptual-Cognitive-Maps-Formation-with-Neural-Successor-Networks-and-Word-Embeddings" class="headerlink" title="Conceptual Cognitive Maps Formation with Neural Successor Networks and Word Embeddings"></a>Conceptual Cognitive Maps Formation with Neural Successor Networks and Word Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01577">http://arxiv.org/abs/2307.01577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Stoewer, Achim Schilling, Andreas Maier, Patrick Krauss</li>
<li>for: 这个论文旨在探讨人工智能中如何利用人脑中的Contextualization能力，以提高人工智能模型的表现。</li>
<li>methods: 该论文使用Successor Representation和神经网络，以及word embedding vector，构建了三个不同概念的认知地图。</li>
<li>results: 该模型能够学习两种不同的地图级别，并将新信息与相关的先前知识表示相似地分布在认知地图中。<details>
<summary>Abstract</summary>
The human brain possesses the extraordinary capability to contextualize the information it receives from our environment. The entorhinal-hippocampal plays a critical role in this function, as it is deeply engaged in memory processing and constructing cognitive maps using place and grid cells. Comprehending and leveraging this ability could significantly augment the field of artificial intelligence. The multi-scale successor representation serves as a good model for the functionality of place and grid cells and has already shown promise in this role. Here, we introduce a model that employs successor representations and neural networks, along with word embedding vectors, to construct a cognitive map of three separate concepts. The network adeptly learns two different scaled maps and situates new information in proximity to related pre-existing representations. The dispersion of information across the cognitive map varies according to its scale - either being heavily concentrated, resulting in the formation of the three concepts, or spread evenly throughout the map. We suggest that our model could potentially improve current AI models by providing multi-modal context information to any input, based on a similarity metric for the input and pre-existing knowledge representations.
</details>
<details>
<summary>摘要</summary>
人脑具有Contextualizing信息的杰出能力，即将来自环境的信息Contextualized在我们的认知中。Entorhinal-hippocampal系统在这一功能中扮演关键角色，因为它深入参与记忆处理和构建认知地图，使用Place和Grid维度。理解和利用这种能力可能会大幅提升人工智能领域。我们提出一种使用Successor表示和神经网络，以及Word Embedding向量，构建三个不同概念的认知地图。该网络能够学习两种不同的缩放级别的地图，并将新的信息与相关的先前表示相关联。信息在认知地图中的散布方式因缩放级别而异，可能是集中形成三个概念，或者在整个地图中均匀分布。我们建议，我们的模型可能可以提高当前的人工智能模型，通过为输入提供多modal的上下文信息，基于输入和先前知识表示之间的相似度 metric。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Based-Intrusion-Detection-Feature-Selection-versus-Feature-Extraction"><a href="#Machine-Learning-Based-Intrusion-Detection-Feature-Selection-versus-Feature-Extraction" class="headerlink" title="Machine Learning-Based Intrusion Detection: Feature Selection versus Feature Extraction"></a>Machine Learning-Based Intrusion Detection: Feature Selection versus Feature Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01570">http://arxiv.org/abs/2307.01570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vu-Duc Ngo, Tuan-Cuong Vuong, Thien Van Luong, Hung Tran</li>
<li>for: 这种研究旨在比较Feature Selection和Feature Extraction两种方法在网络入侵检测中的性能，以及它们在不同的数据集和分类方式下的运行时间复杂度。</li>
<li>methods: 这种研究使用了UNSW-NB15数据集和多种性能指标，如准确率、回归率、检测精度和运行时间复杂度，对Feature Selection和Feature Extraction两种方法进行了比较。</li>
<li>results: 研究发现，Feature Selection方法在多数情况下提供了更好的检测性能，同时具有较低的训练和检测时间复杂度。然而，Feature Extraction方法在某些情况下（如K很小）表现更为可靠，并且对K的变化更为敏感。<details>
<summary>Abstract</summary>
Internet of things (IoT) has been playing an important role in many sectors, such as smart cities, smart agriculture, smart healthcare, and smart manufacturing. However, IoT devices are highly vulnerable to cyber-attacks, which may result in security breaches and data leakages. To effectively prevent these attacks, a variety of machine learning-based network intrusion detection methods for IoT networks have been developed, which often rely on either feature extraction or feature selection techniques for reducing the dimension of input data before being fed into machine learning models. This aims to make the detection complexity low enough for real-time operations, which is particularly vital in any intrusion detection systems. This paper provides a comprehensive comparison between these two feature reduction methods of intrusion detection in terms of various performance metrics, namely, precision rate, recall rate, detection accuracy, as well as runtime complexity, in the presence of the modern UNSW-NB15 dataset as well as both binary and multiclass classification. For example, in general, the feature selection method not only provides better detection performance but also lower training and inference time compared to its feature extraction counterpart, especially when the number of reduced features K increases. However, the feature extraction method is much more reliable than its selection counterpart, particularly when K is very small, such as K = 4. Additionally, feature extraction is less sensitive to changing the number of reduced features K than feature selection, and this holds true for both binary and multiclass classifications. Based on this comparison, we provide a useful guideline for selecting a suitable intrusion detection type for each specific scenario, as detailed in Tab. 14 at the end of Section IV.
</details>
<details>
<summary>摘要</summary>
互联网智能化 (IoT) 在多个领域中扮演着重要角色，如智能城市、智能农业、智能医疗和智能制造。然而，IoT 设备高度易受到黑客攻击，这可能会导致安全泄露和数据泄露。为了有效防止这些攻击，一些基于机器学习的网络入侵检测方法在 IoT 网络中得到了开发，这些方法通常是基于特征抽象或特征选择技术来将输入数据的维度降低到可以被机器学习模型处理的水平。这样可以确保检测的复杂度足够低，以便在实时运行，特别是在任何入侵检测系统中。本文提供了对这两种特征减少方法的入侵检测方法在不同的性能指标下进行了比较，包括精度率、回传率、检测率和运行时间复杂度。例如，通常来说，选择特征方法不仅提供更好的检测性能，而且还比特征抽象方法来的训练和检测时间更短，尤其当K增加时。然而，抽象方法在K很小时（例如K=4）时更加可靠，而且特征选择方法比特征抽象方法更敏感于K的变化。根据这个比较，我们提供了一个实用的指南，可以帮助您在具体情况下选择适合的入侵检测类型，详细可见在表14中。
</details></li>
</ul>
<hr>
<h2 id="Scalable-variable-selection-for-two-view-learning-tasks-with-projection-operators"><a href="#Scalable-variable-selection-for-two-view-learning-tasks-with-projection-operators" class="headerlink" title="Scalable variable selection for two-view learning tasks with projection operators"></a>Scalable variable selection for two-view learning tasks with projection operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01558">http://arxiv.org/abs/2307.01558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aalto-ics-kepaco/projse">https://github.com/aalto-ics-kepaco/projse</a></li>
<li>paper_authors: Sandor Szedmak, Riikka Huusari, Tat Hong Duong Le, Juho Rousu</li>
<li>for: 提出了一种新的变量选择方法，适用于两视图设置或 vector-valued supervised learning 问题。</li>
<li>methods: 使用迭代选择高度相关于输出变量的变量，但与先前选择的变量不相关。使用投影算子和其代数来测量相关性，并可以利用 kernel 函数来表达非线性相关模型。</li>
<li>results: 通过实验 validate 了我们的方法，并在真实数据上验证了其扩展性和选择的有用性。<details>
<summary>Abstract</summary>
In this paper we propose a novel variable selection method for two-view settings, or for vector-valued supervised learning problems. Our framework is able to handle extremely large scale selection tasks, where number of data samples could be even millions. In a nutshell, our method performs variable selection by iteratively selecting variables that are highly correlated with the output variables, but which are not correlated with the previously chosen variables. To measure the correlation, our method uses the concept of projection operators and their algebra. With the projection operators the relationship, correlation, between sets of input and output variables can also be expressed by kernel functions, thus nonlinear correlation models can be exploited as well. We experimentally validate our approach, showing on both synthetic and real data its scalability and the relevance of the selected features. Keywords: Supervised variable selection, vector-valued learning, projection-valued measure, reproducing kernel Hilbert space
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的变量选择方法，适用于两视设置或vector-valued生育学问题。我们的框架可以处理巨大规模的选择任务，数据样本数可以达到十万级。总之，我们的方法通过逐步选择与输出变量高度相关的变量，但与之前选择的变量不相关的方式进行变量选择。为了度量相关性，我们使用投影算子和其代数来测量输入和输出变量之间的相关性。通过投影算子，我们可以将输入和输出变量之间的相关性表示为kernel函数，从而可以利用非线性相关模型。我们实验 validate我们的方法，并在Synthetic和实际数据上展示了其扩展性和选择的有效性。关键词：supervised变量选择，vector-valued学习，投影值度量，复制kernel希尔бер特空间。
</details></li>
</ul>
<hr>
<h2 id="Separated-RoadTopoFormer"><a href="#Separated-RoadTopoFormer" class="headerlink" title="Separated RoadTopoFormer"></a>Separated RoadTopoFormer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01557">http://arxiv.org/abs/2307.01557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingjie Lu, Yuanxian Huang, Ji Liu, Jinzhang Peng, Lu Tian, Ashish Sirasao</li>
<li>for: 本研究的目的是提高自动驾驶技术的实现，强调了驾驶场景理解的重要性。</li>
<li>methods: 本研究使用了Separated RoadTopoFormer框架，这是一个端到端的框架，可以同时检测路径中的交通元素和车道中心线，以及这些元素之间的关系。</li>
<li>results: 本研究的最终提交得分为0.445 OLS，在两个子任务和总分中都具有竞争力。<details>
<summary>Abstract</summary>
Understanding driving scenarios is crucial to realizing autonomous driving. Previous works such as map learning and BEV lane detection neglect the connection relationship between lane instances, and traffic elements detection tasks usually neglect the relationship with lane lines. To address these issues, the task is presented which includes 4 sub-tasks, the detection of traffic elements, the detection of lane centerlines, reasoning connection relationships among lanes, and reasoning assignment relationships between lanes and traffic elements. We present Separated RoadTopoFormer to tackle the issues, which is an end-to-end framework that detects lane centerline and traffic elements with reasoning relationships among them. We optimize each module separately to prevent interaction with each other and aggregate them together with few finetunes. For two detection heads, we adopted a DETR-like architecture to detect objects, and for the relationship head, we concat two instance features from front detectors and feed them to the classifier to obtain relationship probability. Our final submission achieves 0.445 OLS, which is competitive in both sub-task and combined scores.
</details>
<details>
<summary>摘要</summary>
理解驾驶场景是自动驾驶实现的关键。先前的工作，如地图学习和BEV车道检测，忽略了车道实例之间的连接关系，而交通元素检测任务通常忽略车道线的关系。为解决这些问题，我们提出了一个包含4个子任务的任务，即交通元素检测、车道中心线检测、车道间连接关系的推理和车道和交通元素之间的关系推理。我们提出了分离的路况拟合器（Separated RoadTopoFormer）来解决这些问题，它是一个端到端框架，可以同时检测车道中心线和交通元素，并推理它们之间的关系。我们对每个模块进行独立优化，以避免它们之间的交互，并将它们粗略地融合。为两个检测头，我们采用了一种类似于DETR架构来检测对象，而对于关系头，我们将前两个检测器的实例特征 concatenate 并feed 到分类器来获得关系概率。我们的最终提交得分为0.445 OLS，这在两个子任务和合并分数中都是竞争力强的。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graph-for-NLG-in-the-context-of-conversational-agents"><a href="#Knowledge-Graph-for-NLG-in-the-context-of-conversational-agents" class="headerlink" title="Knowledge Graph for NLG in the context of conversational agents"></a>Knowledge Graph for NLG in the context of conversational agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01548">http://arxiv.org/abs/2307.01548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussam Ghanem, Massinissa Atmani, Christophe Cruz</li>
<li>for: 本文提供了对知识图（KG）到文本生成的不同架构的回顾，包括图神经网络、图转换器和 seq2seq 模型。</li>
<li>methods: 本文讲解了不同架构的优势和局限性，并指出了在实际任务中选择架构的重要性。</li>
<li>results: 本文选择了基于 PLM 的 seq2seq 转换器模型来进行知识图到文本生成任务，并计划修改 PLM 上的 kg-to-text 生成数据集，以及在未来的工作中探讨情感和多语言维度。<details>
<summary>Abstract</summary>
The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness of the responses provided by a conversational agent. While generating answers during conversations consists in generating text from these KGs, it is still regarded as a challenging task that has gained significant attention in recent years. In this document, we provide a review of different architectures used for knowledge graph-to-text generation including: Graph Neural Networks, the Graph Transformer, and linearization with seq2seq models. We discuss the advantages and limitations of each architecture and conclude that the choice of architecture will depend on the specific requirements of the task at hand. We also highlight the importance of considering constraints such as execution time and model validity, particularly in the context of conversational agents. Based on these constraints and the availability of labeled data for the domains of DAVI, we choose to use seq2seq Transformer-based models (PLMs) for the Knowledge Graph-to-Text Generation task. We aim to refine benchmark datasets of kg-to-text generation on PLMs and to explore the emotional and multilingual dimensions in our future work. Overall, this review provides insights into the different approaches for knowledge graph-to-text generation and outlines future directions for research in this area.
</details>
<details>
<summary>摘要</summary>
使用知识图（KG）可以提高对话机器人的回答准确性和全面性。在生成回答时，从KG中生成文本是一项有趣且复杂的任务，在最近几年内受到了广泛关注。在本文中，我们提供了不同架构的知识图到文本生成评论，包括图神经网络、图转换器和 linearization with seq2seq 模型。我们讨论了每个架构的优点和缺点，并结论认为选择架构取决于任务的具体需求。我们还 highlight了考虑执行时间和模型有效性的重要性，特别在对话机器人的上下文中。基于这些约束和 DAVI 领域的可用标注数据，我们选择使用 Transformer 基于 seq2seq 模型（PLMs）进行知识图到文本生成任务。我们希望可以修改 PLMs 上 kg-to-text 生成的标准数据集，并在未来的工作中探讨情感和多语言维度。总的来说，本文提供了不同的知识图到文本生成方法的概述，并预示了未来这个领域的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Prompt-in-the-Classroom-to-Understand-AI-Limits-A-pilot-study"><a href="#Learning-to-Prompt-in-the-Classroom-to-Understand-AI-Limits-A-pilot-study" class="headerlink" title="Learning to Prompt in the Classroom to Understand AI Limits: A pilot study"></a>Learning to Prompt in the Classroom to Understand AI Limits: A pilot study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01540">http://arxiv.org/abs/2307.01540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emily Theophilou, Cansu Koyuturk, Mona Yavari, Sathya Bursic, Gregor Donabauer, Alessia Telari, Alessia Testa, Raffaele Boiano, Davinia Hernandez-Leo, Martin Ruskov, Davide Taibi, Alessandro Gabbiadini, Dimitri Ognibene</li>
<li>for: 本研究的目的是研究人工智能的acceptance和用途，以及如何将人工智能应用于解决社会问题。</li>
<li>methods: 本研究使用了Large Language Models（LLM）和其 derivated chatbots，如ChatGPT，来实现人工智能的自然语言处理能力。</li>
<li>results: 研究获得了显著的结果，包括学生对人工智能的评价高，对 chatGPT 的互动质量改善，对人工智能的态度变得更积极，同时更好地理解人工智能的限制。<details>
<summary>Abstract</summary>
Artificial intelligence's progress holds great promise in assisting society in addressing pressing societal issues. In particular Large Language Models (LLM) and the derived chatbots, like ChatGPT, have highly improved the natural language processing capabilities of AI systems allowing them to process an unprecedented amount of unstructured data. The consequent hype has also backfired, raising negative sentiment even after novel AI methods' surprising contributions. One of the causes, but also an important issue per se, is the rising and misleading feeling of being able to access and process any form of knowledge to solve problems in any domain with no effort or previous expertise in AI or problem domain, disregarding current LLMs limits, such as hallucinations and reasoning limits. Acknowledging AI fallibility is crucial to address the impact of dogmatic overconfidence in possibly erroneous suggestions generated by LLMs. At the same time, it can reduce fear and other negative attitudes toward AI. AI literacy interventions are necessary that allow the public to understand such LLM limits and learn how to use them in a more effective manner, i.e. learning to "prompt". With this aim, a pilot educational intervention was performed in a high school with 30 students. It involved (i) presenting high-level concepts about intelligence, AI, and LLM, (ii) an initial naive practice with ChatGPT in a non-trivial task, and finally (iii) applying currently-accepted prompting strategies. Encouraging preliminary results have been collected such as students reporting a) high appreciation of the activity, b) improved quality of the interaction with the LLM during the educational activity, c) decreased negative sentiments toward AI, d) increased understanding of limitations and specifically We aim to study factors that impact AI acceptance and to refine and repeat this activity in more controlled settings.
</details>
<details>
<summary>摘要</summary>
人工智能的进步具有巨大的承诺，可以帮助社会解决一系列的社会问题。特别是大型自然语言处理模型（LLM）和其 derivated chatbot如ChatGPT，有效提高了人工智能系统的自然语言处理能力，使其能处理前所未有的大量不结构化数据。然而，这种热情也导致了负面情绪的升温，包括误导和过度自信的问题。一个重要的原因是人们对人工智能的训练和应用方面的知识和经验不足，导致他们假设AI系统可以轻松地解决任何问题，不需要专业知识或培训。我们认为，承诺AI系统的有限性是关键，以避免“欺诈式”的过度自信和误导。同时，了解AI系统的限制可以减少对人工智能的恐惧和负面情绪。为此，我们发展了一种AI文化干预措施，以帮助公众更好地理解LLM的限制和如何更有效地使用它们。在这个研究中，我们在一所高中进行了一场教育干预，涉及到（i）介绍智能、AI和LLM的概念，（ii）初步使用ChatGPT完成一项非常困难的任务，以及（iii）应用已知的提示策略。结果表明，学生对这个活动表示高度的欢迎，并且在与LLM的交互中改善了质量，同时减少了对人工智能的负面情绪。我们计划进一步研究这些因素，以便更好地理解AI接受度的影响因素，并在更加控制的环境下重复和改进这种活动。
</details></li>
</ul>
<hr>
<h2 id="Anomaly-detection-in-image-or-latent-space-of-patch-based-auto-encoders-for-industrial-image-analysis"><a href="#Anomaly-detection-in-image-or-latent-space-of-patch-based-auto-encoders-for-industrial-image-analysis" class="headerlink" title="Anomaly detection in image or latent space of patch-based auto-encoders for industrial image analysis"></a>Anomaly detection in image or latent space of patch-based auto-encoders for industrial image analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02495">http://arxiv.org/abs/2307.02495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Pinon, Robin Trombetta, Carole Lartizien</li>
<li>for: 检测颜色图像中异常点的方法</li>
<li>methods: 基于patch-based auto-encoder构建的三种方法： errors between original image and its reconstruction, support estimation of normal image distribution in latent space, and error between original image and restored version of reconstructed image</li>
<li>results: 在MVTecAD工业图像数据库上评估和比较三种方法的性能，并与两种现有的状态前方法进行比较<details>
<summary>Abstract</summary>
We study several methods for detecting anomalies in color images, constructed on patch-based auto-encoders. Wecompare the performance of three types of methods based, first, on the error between the original image and its reconstruction,second, on the support estimation of the normal image distribution in the latent space, and third, on the error between the originalimage and a restored version of the reconstructed image. These methods are evaluated on the industrial image database MVTecADand compared to two competitive state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
我们研究了一些用于检测颜色图像异常的方法，基于patch-based自适应编码器。我们对三种类型的方法进行比较，分别是根据原始图像与其重建图像之间的错误、在隐藏空间中正常图像分布的支持估计、以及原始图像与重建后的图像之间的错误。这些方法在MVTecAD工业图像数据库上进行评估，并与两种现有的状态艺术方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Intentional-Behavior-in-Autonomous-Agents-under-Uncertainty"><a href="#Analyzing-Intentional-Behavior-in-Autonomous-Agents-under-Uncertainty" class="headerlink" title="Analyzing Intentional Behavior in Autonomous Agents under Uncertainty"></a>Analyzing Intentional Behavior in Autonomous Agents under Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01532">http://arxiv.org/abs/2307.01532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/filipcano/intentional-autonomous-agents">https://github.com/filipcano/intentional-autonomous-agents</a></li>
<li>paper_authors: Filip Cano Córdoba, Samuel Judson, Timos Antonopoulos, Katrine Bjørner, Nicholas Shoemaker, Scott J. Shapiro, Ruzica Piskac, Bettina Könighofer</li>
<li>for: 本研究旨在提供一种量化评估自动决策系统的责任性，以便在不确定环境中进行原则正的决策。</li>
<li>methods: 我们使用Markov决策过程（MDP）模型不确定环境，并使用概率模型检查来计算机器人在达到某个事件的能力。我们称这为“职责范围”。我们还使用Counterfactual reasoning来自动生成相关的场景，以提高评估的可靠性。</li>
<li>results: 我们通过一个实验示例，证明我们的方法可以区分“意图的”和“巧合的”交通事故。<details>
<summary>Abstract</summary>
Principled accountability for autonomous decision-making in uncertain environments requires distinguishing intentional outcomes from negligent designs from actual accidents. We propose analyzing the behavior of autonomous agents through a quantitative measure of the evidence of intentional behavior. We model an uncertain environment as a Markov Decision Process (MDP). For a given scenario, we rely on probabilistic model checking to compute the ability of the agent to influence reaching a certain event. We call this the scope of agency. We say that there is evidence of intentional behavior if the scope of agency is high and the decisions of the agent are close to being optimal for reaching the event. Our method applies counterfactual reasoning to automatically generate relevant scenarios that can be analyzed to increase the confidence of our assessment. In a case study, we show how our method can distinguish between 'intentional' and 'accidental' traffic collisions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用原则性的负责任度来评估自主决策在不确定环境中的决策过程，需要分辨出意图的结果和不注意的设计。我们提出了基于量化度量的意图行为分析方法。我们将不确定环境模型为马尔可夫决策过程（MDP）。对于每个场景，我们采用概率模型检查来计算机器人的影响力。我们称之为作用范围。如果作用范围高并且机器人的决策较近于最优的达成事件，我们认为有意图行为的证据。我们的方法使用Counterfactual reasoning自动生成相关的场景，以提高评估的信度。在一个案例研究中，我们示例如如何使用我们的方法分辨出“意图”和“意外”的交通事故。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Transformer-for-Autonomous-Recognition-and-Grading-of-Tomatoes-Under-Various-Lighting-Occlusion-and-Ripeness-Conditions"><a href="#Convolutional-Transformer-for-Autonomous-Recognition-and-Grading-of-Tomatoes-Under-Various-Lighting-Occlusion-and-Ripeness-Conditions" class="headerlink" title="Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions"></a>Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01530">http://arxiv.org/abs/2307.01530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asim Khan, Taimur Hassan, Muhammad Shafay, Israa Fahmy, Naoufel Werghi, Lakmal Seneviratne, Irfan Hussain<br>for:*  Tomatoes are harvested using mobile robots in real-world scenarios, but this is challenging due to factors such as occlusion and color similarity.methods:*  The proposed framework uses a convolutional transformer architecture to autonomously recognize and grade tomatoes, regardless of occlusion level, lighting conditions, and ripeness.results:*  The proposed framework outperforms existing methods by 58.14%, 65.42%, and 66.39% in terms of mean average precision scores on three different datasets.Here are the three points in Simplified Chinese text:for:*  Tomatoes 被用mobile robots在实际场景中收割，但这会受到叶子和枝条的 occlusion 和 Tomatoes 和周围植物的颜色相似性的影响。methods:*  this research 提出了一种基于 convolutional transformer 架构的自动 Tomatoes 识别和评分方法，不 regard occlusion 水平、照明条件和成熟度。results:*  this research 的提出方法在三个不同的 dataset 上比基eline 方法高出 58.14%, 65.42%, 66.39% 的mean average precision 分数。<details>
<summary>Abstract</summary>
Harvesting fully ripe tomatoes with mobile robots presents significant challenges in real-world scenarios. These challenges arise from factors such as occlusion caused by leaves and branches, as well as the color similarity between tomatoes and the surrounding foliage during the fruit development stage. The natural environment further compounds these issues with varying light conditions, viewing angles, occlusion factors, and different maturity levels. To overcome these obstacles, this research introduces a novel framework that leverages a convolutional transformer architecture to autonomously recognize and grade tomatoes, irrespective of their occlusion level, lighting conditions, and ripeness. The proposed model is trained and tested using carefully annotated images curated specifically for this purpose. The dataset is prepared under various lighting conditions, viewing perspectives, and employs different mobile camera sensors, distinguishing it from existing datasets such as Laboro Tomato and Rob2Pheno Annotated Tomato. The effectiveness of the proposed framework in handling cluttered and occluded tomato instances was evaluated using two additional public datasets, Laboro Tomato and Rob2Pheno Annotated Tomato, as benchmarks. The evaluation results across these three datasets demonstrate the exceptional performance of our proposed framework, surpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of mean average precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated Tomato, respectively. The results underscore the superiority of the proposed model in accurately detecting and delineating tomatoes compared to baseline methods and previous approaches. Specifically, the model achieves an F1-score of 80.14%, a Dice coefficient of 73.26%, and a mean IoU of 66.41% on the KUTomaData image dataset.
</details>
<details>
<summary>摘要</summary>
摘取全熟 Tomatoes 的 mobile robot 在实际应用场景中具有显著的挑战。这些挑战来自于叶子和枝条所引起的遮挡，以及 Tomatoes 和周围的植物发育阶段的颜色相似性。自然环境更进一步复杂这些问题，包括不同的照明条件、视角、遮挡因子和不同的成熟度。为了解决这些问题，这项研究提出了一种新的框架，利用卷积变换器架构来自动认识和评分 Tomatoes，不受遮挡、照明条件和成熟度的影响。该模型被训练和测试使用特别为这个目的制作的精心注释图像集。该数据集在不同的照明条件和视角下准备，并使用不同的移动摄像头感知器，与现有的数据集不同。为了评估提案的效果，我们使用了两个额外的公共数据集，即 Laboro Tomato 和 Rob2Pheno Annotated Tomato，作为标准。测试结果表明，提案的框架在处理受遮挡和受遮挡的 Tomatoes 实例方面表现出色，相比基eline方法和前一代方法，提案的模型在三个数据集上的mean average precision分数上出色，高于state-of-the-art的58.14%、65.42%和66.39%。结果表明，提案的模型在识别和定义 Tomatoes 方面具有显著的优势，具体来说，模型在 KUTomaData 图像集上达到了80.14%的F1分数、73.26%的Dice系数和66.41%的mean IoU。
</details></li>
</ul>
<hr>
<h2 id="LEAT-Towards-Robust-Deepfake-Disruption-in-Real-World-Scenarios-via-Latent-Ensemble-Attack"><a href="#LEAT-Towards-Robust-Deepfake-Disruption-in-Real-World-Scenarios-via-Latent-Ensemble-Attack" class="headerlink" title="LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via Latent Ensemble Attack"></a>LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via Latent Ensemble Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01520">http://arxiv.org/abs/2307.01520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joonkyo Shim, Hyunsoo Yoon</li>
<li>for: 防止深伪（deepfake）威胁， recent studies 使用 adversarial perturbation 来攻击深伪模型的输出。</li>
<li>methods: 我们提出了一个简单又有效的攻击方法，called Latent Ensemble ATtack (LEAT)，它攻击独立的潜在编码过程，从而生成不同于目标属性的复杂的出力图像。</li>
<li>results: 我们的方法在实际应用中具有更高的防护成功率，比之前的方法更加有效。<details>
<summary>Abstract</summary>
Deepfakes, malicious visual contents created by generative models, pose an increasingly harmful threat to society. To proactively mitigate deepfake damages, recent studies have employed adversarial perturbation to disrupt deepfake model outputs. However, previous approaches primarily focus on generating distorted outputs based on only predetermined target attributes, leading to a lack of robustness in real-world scenarios where target attributes are unknown. Additionally, the transferability of perturbations between two prominent generative models, Generative Adversarial Networks (GANs) and Diffusion Models, remains unexplored. In this paper, we emphasize the importance of target attribute-transferability and model-transferability for achieving robust deepfake disruption. To address this challenge, we propose a simple yet effective disruption method called Latent Ensemble ATtack (LEAT), which attacks the independent latent encoding process. By disrupting the latent encoding process, it generates distorted output images in subsequent generation processes, regardless of the given target attributes. This target attribute-agnostic attack ensures robust disruption even when the target attributes are unknown. Additionally, we introduce a Normalized Gradient Ensemble strategy that effectively aggregates gradients for iterative gradient attacks, enabling simultaneous attacks on various types of deepfake models, involving both GAN-based and Diffusion-based models. Moreover, we demonstrate the insufficiency of evaluating disruption quality solely based on pixel-level differences. As a result, we propose an alternative protocol for comprehensively evaluating the success of defense. Extensive experiments confirm the efficacy of our method in disrupting deepfakes in real-world scenarios, reporting a higher defense success rate compared to previous methods.
</details>
<details>
<summary>摘要</summary>
深刻复制（Deepfakes），由生成模型生成的恶意视觉内容，对社会 pose 危害性的增加。为了积极防止深刻复制害处， latest studies have employed adversarial perturbation to disrupt deepfake model outputs. However, previous approaches primarily focus on generating distorted outputs based on predetermined target attributes, leading to a lack of robustness in real-world scenarios where target attributes are unknown. Additionally, the transferability of perturbations between two prominent generative models, 生成 adversarial Networks (GANs) and Diffusion Models, remains unexplored. In this paper, we emphasize the importance of target attribute-transferability and model-transferability for achieving robust deepfake disruption. To address this challenge, we propose a simple yet effective disruption method called Latent Ensemble ATtack (LEAT), which attacks the independent latent encoding process. By disrupting the latent encoding process, it generates distorted output images in subsequent generation processes, regardless of the given target attributes. This target attribute-agnostic attack ensures robust disruption even when the target attributes are unknown. Additionally, we introduce a Normalized Gradient Ensemble strategy that effectively aggregates gradients for iterative gradient attacks, enabling simultaneous attacks on various types of deepfake models, involving both GAN-based and Diffusion-based models. Moreover, we demonstrate the insufficiency of evaluating disruption quality solely based on pixel-level differences. As a result, we propose an alternative protocol for comprehensively evaluating the success of defense. Extensive experiments confirm the efficacy of our method in disrupting deepfakes in real-world scenarios, reporting a higher defense success rate compared to previous methods.
</details></li>
</ul>
<hr>
<h2 id="Deep-Attention-Q-Network-for-Personalized-Treatment-Recommendation"><a href="#Deep-Attention-Q-Network-for-Personalized-Treatment-Recommendation" class="headerlink" title="Deep Attention Q-Network for Personalized Treatment Recommendation"></a>Deep Attention Q-Network for Personalized Treatment Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01519">http://arxiv.org/abs/2307.01519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevenmsm/rl-icu-daqn">https://github.com/stevenmsm/rl-icu-daqn</a></li>
<li>paper_authors: Simin Ma, Junghwan Lee, Nicoleta Serban, Shihao Yang</li>
<li>for: 这研究旨在提出个性化治疗建议的新方法，以提高医疗结果。</li>
<li>methods: 该研究使用深度注意力Q网络， combinig transformer架构和深度强化学习框架，fficiently incorporate all past patient observations。</li>
<li>results: 研究在实际的 septic shock 和急性低血压 cohorts 中展示了其超过当前状态艺术模型的优势。<details>
<summary>Abstract</summary>
Tailoring treatment for individual patients is crucial yet challenging in order to achieve optimal healthcare outcomes. Recent advances in reinforcement learning offer promising personalized treatment recommendations; however, they rely solely on current patient observations (vital signs, demographics) as the patient's state, which may not accurately represent the true health status of the patient. This limitation hampers policy learning and evaluation, ultimately limiting treatment effectiveness. In this study, we propose the Deep Attention Q-Network for personalized treatment recommendations, utilizing the Transformer architecture within a deep reinforcement learning framework to efficiently incorporate all past patient observations. We evaluated the model on real-world sepsis and acute hypotension cohorts, demonstrating its superiority to state-of-the-art models. The source code for our model is available at https://github.com/stevenmsm/RL-ICU-DAQN.
</details>
<details>
<summary>摘要</summary>
个人化治疗是现代医疗的关键，但同时也是非常具有挑战性，以达到最佳医疗结果。最新的增强学习技术具有个人化治疗建议的承诺，但是它们只是基于当前患者的观察（生命 Parameters、人口）来判断患者的状态，这可能并不准确地反映患者的真实健康状况。这种限制会阻碍策略学习和评估，从而限制治疗的效果。在这项研究中，我们提出了深度注意力Q网络，利用转换器架构在深度增强学习框架中高效地包含所有过去患者的观察。我们对实际世界的 septic shock 和急性低血压群体进行了评估，并证明了我们的模型的优越性。模型的源代码可以在https://github.com/stevenmsm/RL-ICU-DAQN上获取。
</details></li>
</ul>
<hr>
<h2 id="All-in-One-Multi-task-Prompting-for-Graph-Neural-Networks"><a href="#All-in-One-Multi-task-Prompting-for-Graph-Neural-Networks" class="headerlink" title="All in One: Multi-task Prompting for Graph Neural Networks"></a>All in One: Multi-task Prompting for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01504">http://arxiv.org/abs/2307.01504</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sheldonresearch/ProG">https://github.com/sheldonresearch/ProG</a></li>
<li>paper_authors: Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan</li>
<li>for: 填充预训练模型的知识空间，以提高graph任务的性能。</li>
<li>methods: 提出了一种多任务提问方法，通过统一格式、语言提问的拓展和下游任务的改进，使得自然语言处理中的提问思想可以轻松地应用于图领域。</li>
<li>results: 经过广泛的实验，结果表明该方法能够提高graph任务的性能，并且可以适应不同的任务。<details>
<summary>Abstract</summary>
Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.
</details>
<details>
<summary>摘要</summary>
近期，“预训练和精度调整”被广泛采用为许多图像任务的标准工作流程，因为它可以将通用的图像知识传递给不同应用程序，填补每个应用程序的图像缺失。然而，图像任务中的节点水平、边水平和图像水平具有很大的多样性，这使得预训练预测经常与这些多种任务不兼容。这种差距甚至可能导致特定应用程序的负面传播，从而导致Results poor。 inspirited by the prompt learning in natural language processing (NLP), which has shown significant effectiveness in leveraging prior knowledge for various NLP tasks, we investigate the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.
</details></li>
</ul>
<hr>
<h2 id="HEDI-First-Time-Clinical-Application-and-Results-of-a-Biomechanical-Evaluation-and-Visualisation-Tool-for-Incisional-Hernia-Repair"><a href="#HEDI-First-Time-Clinical-Application-and-Results-of-a-Biomechanical-Evaluation-and-Visualisation-Tool-for-Incisional-Hernia-Repair" class="headerlink" title="HEDI: First-Time Clinical Application and Results of a Biomechanical Evaluation and Visualisation Tool for Incisional Hernia Repair"></a>HEDI: First-Time Clinical Application and Results of a Biomechanical Evaluation and Visualisation Tool for Incisional Hernia Repair</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01502">http://arxiv.org/abs/2307.01502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob J. Relle, Samuel Voß, Ramesch Raschidi, Regine Nessel, Johannes Görich, Mark O. Wielpütz, Thorsten Löffler, Vincent Heuveline, Friedrich Kallinowski, Philipp D. Lösel</li>
<li>for: 治疗腹部坏死病例，减少疼痛、不适和重复手术的风险</li>
<li>methods: 使用生物力学方法，考虑腹部团附肌肉的活动、内部压力、组织弹性和腹部膨润，以提高腹部坏死病例的治疗效果</li>
<li>results: 在31名患者的临床应用中，使用HEDI工具进行预操作评估，比报告的成功率显著提高，所有患者三年后没有疼痛和坏死病例重复Here’s the English version of the three key points for reference:</li>
<li>for: Treatment of abdominal wall defects to reduce pain, discomfort, and the risk of repeated surgical repairs</li>
<li>methods: Use of biomechanical methods that consider muscle activation, intra-abdominal pressure, tissue elasticity, and abdominal wall distention to improve treatment outcomes</li>
<li>results: Significantly improved success rates in the first clinical application of HEDI in the preoperative evaluation of 31 patients, with all patients remaining pain-free and showing no hernia recurrence after three years of follow-up.<details>
<summary>Abstract</summary>
Abdominal wall defects often lead to pain, discomfort, and recurrence of incisional hernias, resulting in significant morbidity and repeated surgical repairs worldwide. Mesh repair for large hernias is usually based on the defect area with a fixed overlap, without considering biomechanical aspects such as muscle activation, intra-abdominal pressure, tissue elasticity, and abdominal wall distention. To address this issue, we present a biomechanical approach to incisional hernia repair that takes into account the unstable abdominal wall. Additionally, we introduce HEDI, a tool that uses dynamic computed tomography with Valsalva maneuver to automatically detect and assess hernia size, volume, and abdominal wall instability. Our first clinical application of HEDI in the preoperative evaluation of 31 patients shows significantly improved success rates compared to reported rates, with all patients remaining pain-free and showing no hernia recurrence after three years of follow-up.
</details>
<details>
<summary>摘要</summary>
腹壁损害常引起疼痛、不适和重复的 Hernia 复发，导致全球较高的患者病况和多次手术修复。固定 overlap 的网 repair 通常基于缺陷区域，没有考虑生物力学方面的因素，如肌肉活动、腹部压力、组织弹性和腹壁膨胀。为解决这个问题，我们提出了一种生物力学方法 для剖股 Hernia 修复，考虑到不稳定的腹壁。此外，我们还介绍了 HEDI，一种使用动态计算tomography 和 Valsalva 优化来自动检测和评估 Hernia 大小、体积和腹壁不稳定性。我们在31名患者的前Operative 评估中首次应用 HEDI，显示成功率明显高于报道率，所有患者三年跟踪后保持无疼痛，无 Hernia 复发。
</details></li>
</ul>
<hr>
<h2 id="FREEDOM-Target-Label-Source-Data-Domain-Information-Free-Multi-Source-Domain-Adaptation-for-Unsupervised-Personalization"><a href="#FREEDOM-Target-Label-Source-Data-Domain-Information-Free-Multi-Source-Domain-Adaptation-for-Unsupervised-Personalization" class="headerlink" title="FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization"></a>FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02493">http://arxiv.org/abs/2307.02493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunju Yang, Gyusang Cho, Chan-Hyun Youn</li>
<li>for: 本研究旨在提出一种实用的多源领域适应（MSDA）问题scenario，以适应部署模型到客户端的数据集。</li>
<li>methods: 本研究提出了一种新的适应问题scenario，称为Three-Free Domain Adaptation（TFDA），其中目标标签、源数据集和源领域信息（领域标签和领域数量）都不可用。为解决这种问题scenario，我们提出了一种实用的适应框架called FREEDOM。</li>
<li>results: FREEDOM可以在无源领域信息的情况下实现state-of-the-art或相当的性能，同时减少了最终模型的大小。此外，FREEDOM可以独立于源领域数量进行部署。<details>
<summary>Abstract</summary>
From a service perspective, Multi-Source Domain Adaptation (MSDA) is a promising scenario to adapt a deployed model to a client's dataset. It can provide adaptation without a target label and support the case where a source dataset is constructed from multiple domains. However, it is impractical, wherein its training heavily relies on prior domain information of the multi-source dataset -- how many domains exist and the domain label of each data sample. Moreover, MSDA requires both source and target datasets simultaneously (physically), causing storage limitations on the client device or data privacy issues by transferring client data to a server. For a more practical scenario of model adaptation from a service provider's point of view, we relax these constraints and present a novel problem scenario of Three-Free Domain Adaptation, namely TFDA, where 1) target labels, 2) source dataset, and mostly 3) source domain information (domain labels + the number of domains) are unavailable. Under the problem scenario, we propose a practical adaptation framework called FREEDOM. It leverages the power of the generative model, disentangling data into class and style aspects, where the style is defined as the class-independent information from the source data and designed with a nonparametric Bayesian approach. In the adaptation stage, FREEDOM aims to match the source class distribution with the target's under the philosophy that class distribution is consistent even if the style is different; after then, only part of the classification model is deployed as a personalized network. As a result, FREEDOM achieves state-of-the-art or comparable performance even without domain information, with reduced final model size on the target side, independent of the number of source domains.
</details>
<details>
<summary>摘要</summary>
从服务方面来看，多源领域适应（MSDA）是一个有前途的enario，可以将部署的模型适应到客户的资料集。它可以无需目标标签进行适应，并且可以处理多个领域的资料集合建构的情况。然而，MSDA的训练 heavily rely on多个领域的领域信息（每个数据标签），并且需要源和目标资料集同时存在（physically），导致客户设备限制或资料隐私问题。为了更实际的模型适应方案，我们将这些限制放宽，并提出了一个新的问题场景：三自领域适应（TFDA），其中1）目标标签、2）源资料集和3）源领域信息（领域标签和领域数量）都不可用。在这个问题场景下，我们提出了一个实用的适应框架called FREEDOM。它利用了生成模型的力量，将数据分解为类别和风格的两个方面，其中风格是源数据中的class独立信息，通过非 Parametric Bayesian方法设计。在适应阶段，FREEDOM的目标是将源类别分布与目标的类别分布相对consistent，然后仅部署一部分的分类模型为个人化网络。因此，FREEDOM可以实现state-of-the-art或相等的性能，而不需要领域信息，并且对数据集大小进行干扰。
</details></li>
</ul>
<hr>
<h2 id="A-Bibliographic-Study-on-Artificial-Intelligence-Research-Global-Panorama-and-Indian-Appearance"><a href="#A-Bibliographic-Study-on-Artificial-Intelligence-Research-Global-Panorama-and-Indian-Appearance" class="headerlink" title="A Bibliographic Study on Artificial Intelligence Research: Global Panorama and Indian Appearance"></a>A Bibliographic Study on Artificial Intelligence Research: Global Panorama and Indian Appearance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00705">http://arxiv.org/abs/2308.00705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Tiwari, Susmita Bardhan, Vikas Kumar</li>
<li>for: 本研究用科学映射方法对2015-2020年的人工智能（AI）研究进行了 bibliometric 分析，以了解AI研究的发展趋势。</li>
<li>methods: 本研究使用了Scopus数据库收集必要的数据，并对数据进行了手动和工具（OpenRefine）的数据转换，以便进行分析。</li>
<li>results: 研究发现，在2015-2020年间，开放获取和商业刊物的AI研究量相对较高，IEEE是最主要的出版商，发表了84%的最高引用文章。此外，中国和美国是AI领域的主要贡献国。研究还发现， neural networks和深度学习是AI研究中最主要的话题。最后，研究发现，不仅公共机构，私人机构也在投入AI研究。<details>
<summary>Abstract</summary>
The present study identifies and assesses the bibliographic trend in Artificial Intelligence (AI) research for the years 2015-2020 using the science mapping method of bibliometric study. The required data has been collected from the Scopus database. To make the collected data analysis-ready, essential data transformation was performed manually and with the help of a tool viz. OpenRefine. For determining the trend and performing the mapping techniques, top five open access and commercial journals of AI have been chosen based on their citescore driven ranking. The work includes 6880 articles published in the specified period for analysis. The trend is based on Country-wise publications, year-wise publications, topical terms in AI, top-cited articles, prominent authors, major institutions, involvement of industries in AI and Indian appearance. The results show that compared to open access journals; commercial journals have a higher citescore and number of articles published over the years. Additionally, IEEE is the prominent publisher which publishes 84% of the top-cited publications. Further, China and the United States are the major contributors to literature in the AI domain. The study reveals that neural networks and deep learning are the major topics included in top AI research publications. Recently, not only public institutions but also private bodies are investing their resources in AI research. The study also investigates the relative position of Indian researchers in terms of AI research. Present work helps in understanding the initial development, current stand and future direction of AI.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Country-wise publications: China and the United States are the major contributors to AI literature.2. Year-wise publications: There has been a steady increase in the number of publications over the years.3. Topical terms in AI: Neural networks and deep learning are the major topics included in top AI research publications.4. Top-cited articles: IEEE publishes 84% of the top-cited publications.5. Prominent authors: The study reveals that there are several prominent authors in the field of AI.6. Major institutions: The study shows that China and the United States are the major contributors to AI research.7. Involvement of industries in AI: Private bodies are investing their resources in AI research, in addition to public institutions.8. Indian appearance: The study investigates the relative position of Indian researchers in terms of AI research.The present work provides an understanding of the initial development, current stand, and future direction of AI research.</details></li>
</ol>
<hr>
<h2 id="Mitigating-Bias-Enhancing-Image-Classification-by-Improving-Model-Explanations"><a href="#Mitigating-Bias-Enhancing-Image-Classification-by-Improving-Model-Explanations" class="headerlink" title="Mitigating Bias: Enhancing Image Classification by Improving Model Explanations"></a>Mitigating Bias: Enhancing Image Classification by Improving Model Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01473">http://arxiv.org/abs/2307.01473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raha Ahmadi, Mohammad Javad Rajabi, Mohammad Khalooie, Mohammad Sabokrou</li>
<li>for: 提高图像分类器的主要概念理解和表示，增强模型对图像中主要元素的理解。</li>
<li>methods: 提出了一种新的方法，通过同时引导模型对前景进行注意力调控，使模型更好地捕捉图像中的主要概念。</li>
<li>results: 通过对标准数据集进行广泛的实验，证明了该方法的有效性，提高了图像分类器的准确率。<details>
<summary>Abstract</summary>
Deep learning models have demonstrated remarkable capabilities in learning complex patterns and concepts from training data. However, recent findings indicate that these models tend to rely heavily on simple and easily discernible features present in the background of images rather than the main concepts or objects they are intended to classify. This phenomenon poses a challenge to image classifiers as the crucial elements of interest in images may be overshadowed. In this paper, we propose a novel approach to address this issue and improve the learning of main concepts by image classifiers. Our central idea revolves around concurrently guiding the model's attention toward the foreground during the classification task. By emphasizing the foreground, which encapsulates the primary objects of interest, we aim to shift the focus of the model away from the dominant influence of the background. To accomplish this, we introduce a mechanism that encourages the model to allocate sufficient attention to the foreground. We investigate various strategies, including modifying the loss function or incorporating additional architectural components, to enable the classifier to effectively capture the primary concept within an image. Additionally, we explore the impact of different foreground attention mechanisms on model performance and provide insights into their effectiveness. Through extensive experimentation on benchmark datasets, we demonstrate the efficacy of our proposed approach in improving the classification accuracy of image classifiers. Our findings highlight the importance of foreground attention in enhancing model understanding and representation of the main concepts within images. The results of this study contribute to advancing the field of image classification and provide valuable insights for developing more robust and accurate deep-learning models.
</details>
<details>
<summary>摘要</summary>
深度学习模型已经展现出了学习复杂模式和概念的Remarkable能力。然而，最近的发现表明这些模型在训练数据中很可能会依赖于图像的背景中的简单和易于识别的特征，而不是主要的概念或 объек。这种情况会导致图像分类器的挑战，因为图像中的关键元素可能会被遮盖。在这篇论文中，我们提出了一种新的方法来解决这个问题，并提高图像分类器的学习。我们的中心思想是在分类任务中同时引导模型的注意力向前景方向。通过强调前景，我们希望使模型忽略背景的主导性的影响。为实现这一点，我们引入了一种机制，使得模型能够充分分配注意力于前景。我们 investigate了多种策略，包括修改损失函数或添加额外的建筑 комponents，以使模型能够有效地捕捉图像中的主要概念。此外，我们还探讨了不同的前景注意力机制对模型性能的影响，并提供了有价值的发现。通过对标准数据集进行广泛的实验，我们证明了我们提出的方法的可行性和效果。我们的发现指出了图像分类器中的前景注意力对模型理解和图像中主要概念的表示的重要性。这些发现对图像分类领域的进一步发展具有重要意义，并为开发更加稳定和准确的深度学习模型提供了有价值的发现。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Conservatism-Diffusion-Policies-in-Offline-Multi-agent-Reinforcement-Learning"><a href="#Beyond-Conservatism-Diffusion-Policies-in-Offline-Multi-agent-Reinforcement-Learning" class="headerlink" title="Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning"></a>Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01472">http://arxiv.org/abs/2307.01472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoran Li, Ling Pan, Longbo Huang</li>
<li>For: 提出了一种新的多智能体偏Diffusion Offline Multi-agent Model（DOM2），用于离线多智能体学习（MARL）。* Methods: 在政策网络中 интегри了扩散模型，并提出了一种轨迹基据增强方案，以提高政策表达力和多样性。* Results: 对多智能体particle和多智能体MuJoCo环境进行了广泛的实验，并显示了DOM2在环境变化时的稳定性和表现优于现有方法。此外，DOM2在数据效率方面也表现出了优势，可以在$20+$ times less data的情况下达到现有方法的性能水平。<details>
<summary>Abstract</summary>
We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms that rely mainly on conservatism in policy design, DOM2 enhances policy expressiveness and diversity based on diffusion. Specifically, we incorporate a diffusion model into the policy network and propose a trajectory-based data-augmentation scheme in training. These key ingredients make our algorithm more robust to environment changes and achieve significant improvements in performance, generalization and data-efficiency. Our extensive experimental results demonstrate that DOM2 outperforms existing state-of-the-art methods in multi-agent particle and multi-agent MuJoCo environments, and generalizes significantly better in shifted environments thanks to its high expressiveness and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve state-of-the-art performance with $20+$ times less data compared to existing algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的多智能体吸引模型（DOM2），用于离线多智能体学习（MARL）。与现有的策略设计依赖保守主义的算法不同，DOM2 增强策略表达性和多样性基于吸引模型。我们在策略网络中 integrate 了吸引模型，并提出了一种基于轨迹的数据增强方案。这些关键元素使得我们的算法更加鲁棒地应对环境变化，并实现了显著提高表现、泛化和数据效率。我们的广泛的实验结果表明，DOM2 在多智能体粒子和多智能体 MuJoCo 环境中表现出色，并在Shifted环境中具有更高的表现和泛化能力。此外，DOM2 还示出了更高的数据效率，可以使用 $20++$  times less data  than existing algorithms 达到相同的表现。
</details></li>
</ul>
<hr>
<h2 id="A-multilevel-framework-for-AI-governance"><a href="#A-multilevel-framework-for-AI-governance" class="headerlink" title="A multilevel framework for AI governance"></a>A multilevel framework for AI governance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03198">http://arxiv.org/abs/2307.03198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyesun Choung, Prabu David, John S. Seberger</li>
<li>for: 本研究旨在发展一种基于伦理和基本人类价值观的AI治理框架，以实现AI的潜在好荣和风险减少。</li>
<li>methods: 本研究使用多级治理方法，包括政府、企业和公民三个层次的潜在利益相互关系，以及三个维度的信任（能力、完整性和善良）。</li>
<li>results: 本研究提供了实用的洞察，可以用于进一步提高用户体验和指导AI相关公共政策。<details>
<summary>Abstract</summary>
To realize the potential benefits and mitigate potential risks of AI, it is necessary to develop a framework of governance that conforms to ethics and fundamental human values. Although several organizations have issued guidelines and ethical frameworks for trustworthy AI, without a mediating governance structure, these ethical principles will not translate into practice. In this paper, we propose a multilevel governance approach that involves three groups of interdependent stakeholders: governments, corporations, and citizens. We examine their interrelationships through dimensions of trust, such as competence, integrity, and benevolence. The levels of governance combined with the dimensions of trust in AI provide practical insights that can be used to further enhance user experiences and inform public policy related to AI.
</details>
<details>
<summary>摘要</summary>
为了实现人工智能的潜在优势并mitigate其潜在风险，需要建立一个遵循伦理和基本人类价值观的管理框架。虽然一些组织已经发布了信任worthy AI的指南和伦理体系，但无法mediating governance结构，这些伦理原则将不会在实践中传递。在这篇论文中，我们提议一种多级管理方法，其包括三个相互依赖的团队：政府、企业和公民。我们研究这些团队之间的关系通过信任的维度，如能力、完整性和好意。这些管理层结合了信任的维度，可以为AI的用户经验提供实用的洞察，同时也可以为AI相关的公共政策提供指导。
</details></li>
</ul>
<hr>
<h2 id="Causal-Reinforcement-Learning-A-Survey"><a href="#Causal-Reinforcement-Learning-A-Survey" class="headerlink" title="Causal Reinforcement Learning: A Survey"></a>Causal Reinforcement Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01452">http://arxiv.org/abs/2307.01452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang</li>
<li>for: 本文主要是为了介绍 causal reinforcement learning，一种增强现有算法的措施，通过 incorporating causal relationships 来增强知识传递效果。</li>
<li>methods: 本文首先介绍了 causality 和 reinforcement learning 的基本概念，然后解释了如何通过 causality Address core challenges in non-causal reinforcement learning。最后，文章系统地审视了现有的 causal reinforcement learning 方法，根据其 Target problems 和方法ologies 进行分类。</li>
<li>results: 本文提供了一个 comprehensive review of the literature on causal reinforcement learning，包括了现有的方法和技术，以及未来的开发方向。<details>
<summary>Abstract</summary>
Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcement learning. We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning. We categorize and systematically review existing causal reinforcement learning approaches based on their target problems and methodologies. Finally, we outline open issues and future directions in this emerging field.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化字版本。<</SYS>>根据序列决策问题下的不确定性，力量学习是一种重要的思想模式。虽然过去几十年内有很多出色的成就，但是在实际应用中仍然存在很多挑战。其中一个主要的障碍是，力量学习代理人缺乏世界的基本理解，因此必须通过多次尝试和错误互动来学习。它们还可能面临困难提供决策的解释和泛化获得的知识。然而， causality 却提供了一个明显的优势，即可以系统地形式化知识，并利用不变性来实现有效的知识传递。这导致了 causal reinforcement learning 的出现，这是一种增强现有算法的方法，通过包含 causal 关系来增强学习过程。在这篇评论中，我们系统地介绍了 causal reinforcement learning 的文献。我们首先介绍了 causality 和 reinforcement learning 的基本概念，然后解释了如何通过 causality 解决非 causal reinforcement learning 中的核心挑战。然后，我们按照目标问题和方法分类地系统地审查了现有的 causal reinforcement learning 方法。最后，我们列出了未解决的问题和未来方向。
</details></li>
</ul>
<hr>
<h2 id="A-Double-Machine-Learning-Approach-to-Combining-Experimental-and-Observational-Data"><a href="#A-Double-Machine-Learning-Approach-to-Combining-Experimental-and-Observational-Data" class="headerlink" title="A Double Machine Learning Approach to Combining Experimental and Observational Data"></a>A Double Machine Learning Approach to Combining Experimental and Observational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01449">http://arxiv.org/abs/2307.01449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Morucci, Vittorio Orlandi, Harsh Parikh, Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky</li>
<li>for:  combines experimental and observational studies to test for assumption violations and estimate treatment effects consistently</li>
<li>methods: double machine learning approach, tests for violations of external validity and ignorability under milder assumptions, semi-parametrically efficient treatment effect estimators</li>
<li>results: demonstrated in three real-world case studies, relevant for practical settings<details>
<summary>Abstract</summary>
Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
</details>
<details>
<summary>摘要</summary>
实验和观察研究经常受到有效性问题，因为假设未经测试。我们提出了一种双机器学习方法，将实验和观察研究结合起来，让实践者可以一直测试假设违反和处理效果的一致性。我们的框架测试了外在效应和忽略性的违反，假设更加轻松。只有一个假设违反时，我们提供了半 Parametric 有效的处理效果估计器。但我们的食物免费 theorem 表明，要准确地识别违反的假设，以确保可靠地估计处理效果。我们在三个实际案例中示例出了我们的方法的实用性。
</details></li>
</ul>
<hr>
<h2 id="TablEye-Seeing-small-Tables-through-the-Lens-of-Images"><a href="#TablEye-Seeing-small-Tables-through-the-Lens-of-Images" class="headerlink" title="TablEye: Seeing small Tables through the Lens of Images"></a>TablEye: Seeing small Tables through the Lens of Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02491">http://arxiv.org/abs/2307.02491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seung-eon Lee, Sang-Chul Lee</li>
<li>for: 本研究旨在探讨几shot表格学习的可能性，尤其是在表格数据中缺乏 Labeling 的情况下。</li>
<li>methods: 本研究提出了一种名为 TablEye 的框架，通过域转换来缓解表格数据中的限制，并采用了一些已经证明有效的几shot学习算法和嵌入函数。</li>
<li>results:  TablEye 在 4-shot 任务中的最高 AUC 为 0.11，在 1-shot 设置中的平均提前率为 3.17%。这表明 TablEye 在几shot表格学习中表现出色。<details>
<summary>Abstract</summary>
The exploration of few-shot tabular learning becomes imperative. Tabular data is a versatile representation that captures diverse information, yet it is not exempt from limitations, property of data and model size. Labeling extensive tabular data can be challenging, and it may not be feasible to capture every important feature. Few-shot tabular learning, however, remains relatively unexplored, primarily due to scarcity of shared information among independent datasets and the inherent ambiguity in defining boundaries within tabular data. To the best of our knowledge, no meaningful and unrestricted few-shot tabular learning techniques have been developed without imposing constraints on the dataset. In this paper, we propose an innovative framework called TablEye, which aims to overcome the limit of forming prior knowledge for tabular data by adopting domain transformation. It facilitates domain transformation by generating tabular images, which effectively conserve the intrinsic semantics of the original tabular data. This approach harnesses rigorously tested few-shot learning algorithms and embedding functions to acquire and apply prior knowledge. Leveraging shared data domains allows us to utilize this prior knowledge, originally learned from the image domain. Specifically, TablEye demonstrated a superior performance by outstripping the TabLLM in a 4-shot task with a maximum 0.11 AUC and a STUNT in a 1- shot setting, where it led on average by 3.17% accuracy.
</details>
<details>
<summary>摘要</summary>
《几招学习 tabular 数据的探索成为必要。表格数据是一种多元的表示方式，涵盖了多种信息，但它并不免受数据和模型大小的限制。对于广泛的表格数据进行标签可能是困难的，而且可能无法捕捉到所有重要的特征。然而，几招 tabular 学习仍然 relativity 未explored，主要是因为独立的数据集之间的共享信息缺乏，以及表格数据中的内在含糊性。根据我们所知，没有有意义且不受限制的几招 tabular 学习技术有 been developed，不需要对数据集做任何限制。在这篇论文中，我们提出了一个创新的框架，即 TablEye，以解决表格数据的限制。TablEye 采用域转换来强制保留表格数据的内在 semantics，并采用了已经测试过的几招学习算法和嵌入函数来获取和应用优先知识。通过共享数据域，我们可以利用这些优先知识，原本从图像领域学习而来。 TablEye 在 4 折事件中的最大 AUC 为 0.11，在 1 折事件中的 STUNT 中平均高于 TabLLM 3.17% 的精度。》
</details></li>
</ul>
<hr>
<h2 id="Garbage-in-garbage-out-Zero-shot-detection-of-crime-using-Large-Language-Models"><a href="#Garbage-in-garbage-out-Zero-shot-detection-of-crime-using-Large-Language-Models" class="headerlink" title="Garbage in, garbage out: Zero-shot detection of crime using Large Language Models"></a>Garbage in, garbage out: Zero-shot detection of crime using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06844">http://arxiv.org/abs/2307.06844</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anjsimmo/zero-shot-crime-detection">https://github.com/anjsimmo/zero-shot-crime-detection</a></li>
<li>paper_authors: Anj Simmons, Rajesh Vasa</li>
<li>for: 这个论文旨在利用大语言模型学习的通用常识来进行零极端理解犯罪，给出文本描述的视频surveillance。</li>
<li>methods: 该论文使用大语言模型进行零极端理解犯罪，但是需要手动将视频转换成高质量的文本描述。</li>
<li>results: 研究发现，当视频被手动转换成高质量的文本描述时，大语言模型可以通过零极端理解犯罪，并且性能与现有的状态时技术相当。但是，现有的自动视频到文本转换方法无法生成高质量的视频描述，导致大语言模型输出垃圾结果。<details>
<summary>Abstract</summary>
This paper proposes exploiting the common sense knowledge learned by large language models to perform zero-shot reasoning about crimes given textual descriptions of surveillance videos. We show that when video is (manually) converted to high quality textual descriptions, large language models are capable of detecting and classifying crimes with state-of-the-art performance using only zero-shot reasoning. However, existing automated video-to-text approaches are unable to generate video descriptions of sufficient quality to support reasoning (garbage video descriptions into the large language model, garbage out).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unsupervised-Feature-Learning-with-Emergent-Data-Driven-Prototypicality"><a href="#Unsupervised-Feature-Learning-with-Emergent-Data-Driven-Prototypicality" class="headerlink" title="Unsupervised Feature Learning with Emergent Data-Driven Prototypicality"></a>Unsupervised Feature Learning with Emergent Data-Driven Prototypicality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01421">http://arxiv.org/abs/2307.01421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunhui Guo, Youren Zhang, Yubei Chen, Stella X. Yu</li>
<li>for: 在一个没有标签的图像集中，我们的目标是训练一个模型，使其对每个图像映射到一个特征空间中，以便不仅 proximity 表示视觉相似性，而且图像的位置直接表示数据集中的趋势。</li>
<li>methods: 我们的关键发现是在径比空间中进行无监督特征学习，而不是在欧几里得空间中。在这个空间中，点之间的距离仍然表示图像相似性，而且我们获得了更多的容量来表示数据集中的趋势。</li>
<li>results: 我们提出了一种无监督特征学习算法，使用径比空间中的圆束排序（HACK）。HACK 首先生成了径比空间中的均匀填充的粒子，然后将每个图像分配给每个粒子。在凝固后，图像会更加典型地表示数据集。我们的特征映射器只需要在径比空间中扩散训练实例，我们发现图像会靠近起始点，确认我们的想法：无监督的趋势发现。我们展示了我们的数据驱动的趋势可以轻松地实现无监督实例选择，提高模型的普适性和对于非典型实例的鲁棒性。<details>
<summary>Abstract</summary>
Given an image set without any labels, our goal is to train a model that maps each image to a point in a feature space such that, not only proximity indicates visual similarity, but where it is located directly encodes how prototypical the image is according to the dataset.   Our key insight is to perform unsupervised feature learning in hyperbolic instead of Euclidean space, where the distance between points still reflect image similarity, and yet we gain additional capacity for representing prototypicality with the location of the point: The closer it is to the origin, the more prototypical it is. The latter property is simply emergent from optimizing the usual metric learning objective: The image similar to many training instances is best placed at the center of corresponding points in Euclidean space, but closer to the origin in hyperbolic space.   We propose an unsupervised feature learning algorithm in Hyperbolic space with sphere pACKing. HACK first generates uniformly packed particles in the Poincar\'e ball of hyperbolic space and then assigns each image uniquely to each particle. Images after congealing are regarded more typical of the dataset it belongs to. With our feature mapper simply trained to spread out training instances in hyperbolic space, we observe that images move closer to the origin with congealing, validating our idea of unsupervised prototypicality discovery. We demonstrate that our data-driven prototypicality provides an easy and superior unsupervised instance selection to reduce sample complexity, increase model generalization with atypical instances and robustness with typical ones.
</details>
<details>
<summary>摘要</summary>
We propose an unsupervised feature learning algorithm in hyperbolic space with sphere pACKing. HACK first generates uniformly packed particles in the Poincaré ball of hyperbolic space and then assigns each image uniquely to each particle. Images after congealing are regarded as more typical of the dataset they belong to. With our feature mapper simply trained to spread out training instances in hyperbolic space, we observe that images move closer to the origin with congealing, validating our idea of unsupervised prototypicality discovery.We demonstrate that our data-driven prototypicality provides an easy and superior unsupervised instance selection to reduce sample complexity, increase model generalization with atypical instances, and robustness with typical ones.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-vulnerabilities-in-SplitFed-Learning-Assessing-the-robustness-against-Data-Poisoning-Attacks"><a href="#Analyzing-the-vulnerabilities-in-SplitFed-Learning-Assessing-the-robustness-against-Data-Poisoning-Attacks" class="headerlink" title="Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks"></a>Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03197">http://arxiv.org/abs/2307.03197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aysha Thahsin Zahir Ismail, Raj Mani Shukla</li>
<li>for: 这篇论文旨在研究分布式协同机器学习（DCML）中数据欺诈攻击的影响。特别是在DCML中 Split learning（SL）和 Federated Learning（FL）的混合方法（SplitFed Learning，SFL）中。</li>
<li>methods: 本研究提出了三种新型攻击策略，包括无目标攻击、Targeted攻击和距离基于攻击。这些攻击策略的目标是降低基于DCML的分类器性能。</li>
<li>results: 经过对三种攻击策略的测试和分析，结果显示无目标和距离基于攻击对SFL中的分类器性能有更大的影响，而Targeted攻击的影响相对较小。这些结果也在两个不同的案例研究中进行了验证。<details>
<summary>Abstract</summary>
Distributed Collaborative Machine Learning (DCML) is a potential alternative to address the privacy concerns associated with centralized machine learning. The Split learning (SL) and Federated Learning (FL) are the two effective learning approaches in DCML. Recently there have been an increased interest on the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is the earliest attempt to study, analyze and present the impact of data poisoning attacks in SFL. We propose three kinds of novel attack strategies namely untargeted, targeted and distance-based attacks for SFL. All the attacks strategies aim to degrade the performance of the DCML-based classifier. We test the proposed attack strategies for two different case studies on Electrocardiogram signal classification and automatic handwritten digit recognition. A series of attack experiments were conducted by varying the percentage of malicious clients and the choice of the model split layer between the clients and the server. The results after the comprehensive analysis of attack strategies clearly convey that untargeted and distance-based poisoning attacks have greater impacts in evading the classifier outcomes compared to targeted attacks in SFL
</details>
<details>
<summary>摘要</summary>
分布式合作机器学习（DCML）是一种可能的中央机器学习隐私问题的解决方案。Split learning（SL）和联邦学习（FL）是DCML中两种有效的学习方法。在最近，关于FL和SL的混合，即SplitFed Learning（SFL）的兴趣增加。这项研究是DCML基于类别器性能下数据毒素攻击的第一个研究。我们提出了三种新的攻击策略，包括无目标、targeted和距离基于攻击，这些攻击策略的目的都是降低DCML基于类别器的性能。我们对两个不同的案例进行了电势心跳信号分类和自动手写数字识别的测试，并通过变化客户端中的Percentage of malicious clients和模型Split层来进行了一系列攻击实验。结果表明，无目标和距离基于攻击更有可能影响DCML基于类别器的性能，compared to targeted attacks。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Communicate-using-Contrastive-Learning"><a href="#Learning-to-Communicate-using-Contrastive-Learning" class="headerlink" title="Learning to Communicate using Contrastive Learning"></a>Learning to Communicate using Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01403">http://arxiv.org/abs/2307.01403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SonamSangpoLama/Music-Genre-Classification">https://github.com/SonamSangpoLama/Music-Genre-Classification</a></li>
<li>paper_authors: Yat Long Lo, Biswa Sengupta, Jakob Foerster, Michael Noukhovitch</li>
<li>for: 这篇论文的目的是提出一种基于对比学习的通信方法，以便在多智能RL中实现有效的协调。</li>
<li>methods: 该方法利用在智能代理之间交换的通信信息，并通过分析这些信息的关系来学习有效的通信方式。</li>
<li>results: 该方法在通信 essencial 环境下表现出色，比前一些工作更高效和更快速地学习。此外，该方法还能够使通信更加 симметричной，并capture环境中的全局状态信息。<details>
<summary>Abstract</summary>
Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
</details>
<details>
<summary>摘要</summary>
通信是多智能体RL中协调的有力工具。但是引入有效、通用语言是一项困难挑战，特别是在分布式设定下。在这项工作中，我们提出一种不同的视角，即在智能体之间交换的通信消息被视为环境状态的不同不完整的视图。通过对交换的消息之间关系的检查，我们提议使用对比学习来最大化交换消息序列中的相互信息。在通信必需的环境下，我们的方法在性能和学习速度两个方面都超过了前一个工作。使用质量度量和表示探测，我们表明我们的方法导致更Symmetric的通信和捕捉环境中的全局状态信息。总之，我们展示了对比学习的力量和通信消息作为编码的利用对RL中的协调有益。
</details></li>
</ul>
<hr>
<h2 id="In-depth-Analysis-On-Parallel-Processing-Patterns-for-High-Performance-Dataframes"><a href="#In-depth-Analysis-On-Parallel-Processing-Patterns-for-High-Performance-Dataframes" class="headerlink" title="In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes"></a>In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01394">http://arxiv.org/abs/2307.01394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranda Perera, Arup Kumar Sarker, Mills Staylor, Gregor von Laszewski, Kaiying Shan, Supun Kamburugamuve, Chathura Widanage, Vibhatha Abeykoon, Thejaka Amila Kanewela, Geoffrey Fox</li>
<li>for: The paper aims to improve the performance of data processing pipelines by optimizing the use of Dataframes, which are widely used in data engineering applications.</li>
<li>methods: The authors propose a cost model for evaluating parallel processing patterns for distributed Dataframe operators and evaluate the performance of their reference runtime implementation, Cylon, on the ORNL Summit supercomputer.</li>
<li>results: The authors evaluate the performance of Cylon on the ORNL Summit supercomputer and present the results, which demonstrate the potential for improving the performance of data processing pipelines using their proposed approach.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提高数据处理管道的性能，通过优化广泛使用的数据框架（DataFrame）。</li>
<li>methods: 作者提出了一个评估分布式数据框架操作的成本模型，并评估了他们的参考运行时实现（Cylon）在ORNL Summit超级计算机上的性能。</li>
<li>results: 作者在ORNL Summit超级计算机上评估了Cylon的性能，并显示了他们的提案可以提高数据处理管道的性能。<details>
<summary>Abstract</summary>
The Data Science domain has expanded monumentally in both research and industry communities during the past decade, predominantly owing to the Big Data revolution. Artificial Intelligence (AI) and Machine Learning (ML) are bringing more complexities to data engineering applications, which are now integrated into data processing pipelines to process terabytes of data. Typically, a significant amount of time is spent on data preprocessing in these pipelines, and hence improving its e fficiency directly impacts the overall pipeline performance. The community has recently embraced the concept of Dataframes as the de-facto data structure for data representation and manipulation. However, the most widely used serial Dataframes today (R, pandas) experience performance limitations while working on even moderately large data sets. We believe that there is plenty of room for improvement by taking a look at this problem from a high-performance computing point of view. In a prior publication, we presented a set of parallel processing patterns for distributed dataframe operators and the reference runtime implementation, Cylon [1]. In this paper, we are expanding on the initial concept by introducing a cost model for evaluating the said patterns. Furthermore, we evaluate the performance of Cylon on the ORNL Summit supercomputer.
</details>
<details>
<summary>摘要</summary>
“数据科学领域在过去十年内发展壮大，主要归功于大数据革命。人工智能（AI）和机器学习（ML）的应用在数据工程领域中增加了复杂性，现在已经成为数据处理管道的一部分，处理 terrabytes 的数据。通常，数据预处理过程中需要投入大量时间，因此提高预处理效率直接影响整个管道性能。社区最近广泛接受了数据帧作为数据表示和修改的标准数据结构。然而，当前最广泛使用的串行数据帧（R、pandas）在处理中规模较大的数据集时会出现性能限制。我们认为，从高性能计算的角度来看这个问题，还有很多空间提高。在先前的发表文章中，我们提出了分布式数据帧操作的并行处理模式和参考实现 Cylon 。在这篇论文中，我们将对此概念进行扩展，并提出一种成本模型来评估所提出的Pattern。此外，我们还在 ORNL Summit 超级计算机上评估了 Cylon 的性能。”
</details></li>
</ul>
<hr>
<h2 id="Depth-video-data-enabled-predictions-of-longitudinal-dairy-cow-body-weight-using-thresholding-and-Mask-R-CNN-algorithms"><a href="#Depth-video-data-enabled-predictions-of-longitudinal-dairy-cow-body-weight-using-thresholding-and-Mask-R-CNN-algorithms" class="headerlink" title="Depth video data-enabled predictions of longitudinal dairy cow body weight using thresholding and Mask R-CNN algorithms"></a>Depth video data-enabled predictions of longitudinal dairy cow body weight using thresholding and Mask R-CNN algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01383">http://arxiv.org/abs/2307.01383</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yebigithub/BW_dairy">https://github.com/yebigithub/BW_dairy</a></li>
<li>paper_authors: Ye Bi, Leticia M. Campos, Jin Wang, Haipeng Yu, Mark D. Hanigan, Gota Morota</li>
<li>for: 这个研究的目的是预测牛体重，并使用视频数据来预测。</li>
<li>methods: 研究使用了深度学习segmentation方法，包括单resholding、自适应阈值和Mask R-CNN等三种方法来 segment牛体从背景。</li>
<li>results: 研究发现，使用Mask R-CNN方法和线性混合模型可以获得最佳预测系数和平均绝对误差，即0.98和2.03%。此外，这种方法也在留三头牛掉cross-validation中表现最佳。<details>
<summary>Abstract</summary>
Monitoring cow body weight is crucial to support farm management decisions due to its direct relationship with the growth, nutritional status, and health of dairy cows. Cow body weight is a repeated trait, however, the majority of previous body weight prediction research only used data collected at a single point in time. Furthermore, the utility of deep learning-based segmentation for body weight prediction using videos remains unanswered. Therefore, the objectives of this study were to predict cow body weight from repeatedly measured video data, to compare the performance of the thresholding and Mask R-CNN deep learning approaches, to evaluate the predictive ability of body weight regression models, and to promote open science in the animal science community by releasing the source code for video-based body weight prediction. A total of 40,405 depth images and depth map files were obtained from 10 lactating Holstein cows and 2 non-lactating Jersey cows. Three approaches were investigated to segment the cow's body from the background, including single thresholding, adaptive thresholding, and Mask R-CNN. Four image-derived biometric features, such as dorsal length, abdominal width, height, and volume, were estimated from the segmented images. On average, the Mask-RCNN approach combined with a linear mixed model resulted in the best prediction coefficient of determination and mean absolute percentage error of 0.98 and 2.03%, respectively, in the forecasting cross-validation. The Mask-RCNN approach was also the best in the leave-three-cows-out cross-validation. The prediction coefficients of determination and mean absolute percentage error of the Mask-RCNN coupled with the linear mixed model were 0.90 and 4.70%, respectively. Our results suggest that deep learning-based segmentation improves the prediction performance of cow body weight from longitudinal depth video data.
</details>
<details>
<summary>摘要</summary>
监测牛体重是重要的 farm 管理决策的支持因素，因为它直接关系着牛的生长、营养状况和健康。牛体重是一个 repeating 特征，但大多数之前的体重预测研究只使用了单点时间收集的数据。此外，使用视频深度学习 segmentation 对体重预测还没有得到充分的答案。因此，本研究的目标是预测牛体重从重复测量的视频数据，比较深度学习 segmentation 和阈值分割的性能，评估体重预测模型的预测能力，并促进动物科学社区开放科学的发展。总的来说，我们获得了 10 头排 milk Holstein 牛和 2 头不排 milk Jersey 牛的 40,405 个深度图像和深度图像文件。我们 investigate 三种方法来 segment 牛的身体和背景，包括单个阈值、自适应阈值和Mask R-CNN。从 segmented 图像中，我们估算了牛身体的四个图像特征，包括背部长度、腹部宽度、高度和体积。在预测cross-validation中，Mask R-CNN 方法与直线混合模型结合得到了最好的预测 coefficient of determination 和 mean absolute percentage error，具体值分别为 0.98 和 2.03%。在离别三只牛 cross-validation 中，Mask R-CNN 方法也是最好的。预测 coefficient of determination 和 mean absolute percentage error 的值分别为 0.90 和 4.70%。我们的结果表明，使用深度学习 segmentation 可以提高牛体重预测的精度，从长itudinal depth video 数据中预测牛体重。
</details></li>
</ul>
<hr>
<h2 id="Shifting-Attention-to-Relevance-Towards-the-Uncertainty-Estimation-of-Large-Language-Models"><a href="#Shifting-Attention-to-Relevance-Towards-the-Uncertainty-Estimation-of-Large-Language-Models" class="headerlink" title="Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models"></a>Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01379">http://arxiv.org/abs/2307.01379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhaoduan/shifting-attention-to-relevance">https://github.com/jinhaoduan/shifting-attention-to-relevance</a></li>
<li>paper_authors: Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, Kaidi Xu</li>
<li>for: 该研究旨在解决自然语言生成模型（LLM）中的不确定性评估问题，即用户可以信任模型输出的问题。</li>
<li>methods: 该研究使用了自动进行反推的LLM，并研究了生成不均衡（generative inequalities）如何影响不确定性评估。</li>
<li>results: 实验结果表明，可以通过对更重要的（relevant）组件进行共同偏移注意力来解决生成不均衡导致的偏袋。这种方法被称为共同偏移注意力到更重要的组件（SAR）。<details>
<summary>Abstract</summary>
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular "off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance.
</details>
<details>
<summary>摘要</summary>
although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular "off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance.Here's the translation in Traditional Chinese:although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular "off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance.
</details></li>
</ul>
<hr>
<h2 id="A-CNN-regression-model-to-estimate-buildings-height-maps-using-Sentinel-1-SAR-and-Sentinel-2-MSI-time-series"><a href="#A-CNN-regression-model-to-estimate-buildings-height-maps-using-Sentinel-1-SAR-and-Sentinel-2-MSI-time-series" class="headerlink" title="A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series"></a>A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01378">http://arxiv.org/abs/2307.01378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritu Yadav, Andrea Nascetti, Yifang Ban</li>
<li>For: The paper is written for urban planning, infrastructure management, and environmental analysis, with a focus on accurately estimating building heights using satellite time series data.* Methods: The paper proposes a supervised Multimodal Building Height Regression Network (MBHR-Net) that uses Sentinel-1 (S1) and Sentinel-2 (S2) satellite time series data to estimate building heights at 10m spatial resolution. The model extracts meaningful features from both S1 and S2 images to learn complex spatio-temporal relationships between image patterns and building heights.* Results: The preliminary results demonstrate the effectiveness of the MBHR-Net model in accurately estimating building heights, with a Root Mean Squared Error (RMSE) of 3.73m, an Intersection over Union (IOU) of 0.95, and an R-squared (R2) score of 0.61. These results show the potential of the model for urban planning, environmental impact analysis, and other related applications.Here is the simplified Chinese text for the three key information points:* 为：这篇论文是为城市规划、基础设施管理和环境分析而写的，旨在准确估计建筑高度使用卫星时序数据。* 方法：该论文提出一种监督式多Modal Building Height Regression Network（MBHR-Net），使用卫星时序数据Sentinel-1（S1）和Sentinel-2（S2）来估计建筑高度的10米空间分辨率。MBHR-Net利用S1和S2图像中的有价值特征来学习建筑高度与图像模式之间的复杂空间时间关系。* 结果：初步结果表明MBHR-Net模型可以准确估计建筑高度，RMSE为3.73米，IOU为0.95，R2为0.61。这些结果表明MBHR-Net模型在城市规划、环境影响分析等领域有广泛的应用前景。<details>
<summary>Abstract</summary>
Accurate estimation of building heights is essential for urban planning, infrastructure management, and environmental analysis. In this study, we propose a supervised Multimodal Building Height Regression Network (MBHR-Net) for estimating building heights at 10m spatial resolution using Sentinel-1 (S1) and Sentinel-2 (S2) satellite time series. S1 provides Synthetic Aperture Radar (SAR) data that offers valuable information on building structures, while S2 provides multispectral data that is sensitive to different land cover types, vegetation phenology, and building shadows. Our MBHR-Net aims to extract meaningful features from the S1 and S2 images to learn complex spatio-temporal relationships between image patterns and building heights. The model is trained and tested in 10 cities in the Netherlands. Root Mean Squared Error (RMSE), Intersection over Union (IOU), and R-squared (R2) score metrics are used to evaluate the performance of the model. The preliminary results (3.73m RMSE, 0.95 IoU, 0.61 R2) demonstrate the effectiveness of our deep learning model in accurately estimating building heights, showcasing its potential for urban planning, environmental impact analysis, and other related applications.
</details>
<details>
<summary>摘要</summary>
准确估算建筑高度是城市规划、基础设施管理和环境分析中非常重要的。在这项研究中，我们提出了一种监督式多Modal Building Height Regression Network（MBHR-Net），用于使用Sentinel-1（S1）和Sentinel-2（S2）卫星时序序数据来估算建筑高度的10米分辨率。S1提供Synthetic Aperture Radar（SAR）数据，可以提供建筑结构信息，而S2提供多spectral数据，敏感于不同的地面覆盖类型、植被生长阶段和建筑阴影。我们的MBHR-Net通过提取S1和S2图像中有用的特征来学习图像模式和建筑高度之间的复杂空间时间关系。模型在荷兰10座城市进行训练和测试。使用Root Mean Squared Error（RMSE）、Intersection over Union（IOU）和R-squared（R2） metric来评估模型的性能。初步结果（3.73米RMSE、0.95 IoU、0.61 R2）表明我们的深度学习模型在准确地估算建筑高度方面表现出色，这有助于城市规划、环境影响分析等相关应用。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Determination-of-Safety-Requirements-for-Perception-Systems"><a href="#Efficient-Determination-of-Safety-Requirements-for-Perception-Systems" class="headerlink" title="Efficient Determination of Safety Requirements for Perception Systems"></a>Efficient Determination of Safety Requirements for Perception Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01371">http://arxiv.org/abs/2307.01371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sydney M. Katz, Anthony L. Corso, Esen Yel, Mykel J. Kochenderfer</li>
<li>for: 本研究旨在提高安全性的感知系统设计，通过将高级安全要求转化为组件级别的要求。</li>
<li>methods: 本研究使用了 Gaussian processes 和 threshold bandits 等常见黑盒估计技术，并将其结合起来开发了一种新的估计方法，称为 smoothing bandits。</li>
<li>results: 在基于视觉的飞机冲突避免问题上进行了实验，并显示了 compared to Gaussian process 和 threshold bandit 基elines，smoothing bandits 方法可以提高准确性和效率。<details>
<summary>Abstract</summary>
Perception systems operate as a subcomponent of the general autonomy stack, and perception system designers often need to optimize performance characteristics while maintaining safety with respect to the overall closed-loop system. For this reason, it is useful to distill high-level safety requirements into component-level requirements on the perception system. In this work, we focus on efficiently determining sets of safe perception system performance characteristics given a black-box simulator of the fully-integrated, closed-loop system. We combine the advantages of common black-box estimation techniques such as Gaussian processes and threshold bandits to develop a new estimation method, which we call smoothing bandits. We demonstrate our method on a vision-based aircraft collision avoidance problem and show improvements in terms of both accuracy and efficiency over the Gaussian process and threshold bandit baselines.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:感知系统作为整体自主架构的一部分，感知系统设计师需要优化性能特性，同时保持关于全关环境的安全性。为此，可以将高层级的安全需求转换为 ком成器-level的需求。在这个工作中，我们专注于使用黑盒模拟器来划出安全的感知系统性能特性集。我们结合了常见的黑盒估计技术，如 Gaussian processes 和阈值拍赌，开发了一种新的估计方法，我们称之为“缓和拍赌”。我们在基于 computer vision 的飞机回避撞击问题上进行了实验，并证明了我们的方法在精度和效率上具有改进。
</details></li>
</ul>
<hr>
<h2 id="Minimizing-Age-of-Information-for-Mobile-Edge-Computing-Systems-A-Nested-Index-Approach"><a href="#Minimizing-Age-of-Information-for-Mobile-Edge-Computing-Systems-A-Nested-Index-Approach" class="headerlink" title="Minimizing Age of Information for Mobile Edge Computing Systems: A Nested Index Approach"></a>Minimizing Age of Information for Mobile Edge Computing Systems: A Nested Index Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01366">http://arxiv.org/abs/2307.01366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Chen, Ning Yang, Meng Zhang, Jun Wang</li>
<li>for: 实现实时应用需求，减少信息新鲜度指标 Age-of-Information (AoI) 的延迟。</li>
<li>methods: 利用移动 Edge 计算 (MEC) 技术，将任务从移动设备卸载到 Edge 节点进行计算，以提高计算效率。</li>
<li>results: 提出一种基于 Restless Multi-Arm-Bandit (RMAB) 问题的嵌入式指标框架，并设计一种嵌入式指标策略，可以提供可靠性和准确性的平衡。该策略可以减少优化率差距达40%，并且随系统缩放因子增大，可以达到下界的静态优化。<details>
<summary>Abstract</summary>
Exploiting the computational heterogeneity of mobile devices and edge nodes, mobile edge computation (MEC) provides an efficient approach to achieving real-time applications that are sensitive to information freshness, by offloading tasks from mobile devices to edge nodes. We use the metric Age-of-Information (AoI) to evaluate information freshness. An efficient solution to minimize the AoI for the MEC system with multiple users is non-trivial to obtain due to the random computing time. In this paper, we consider multiple users offloading tasks to heterogeneous edge servers in a MEC system. We first reformulate the problem as a Restless Multi-Arm-Bandit (RMAB) problem and establish a hierarchical Markov Decision Process (MDP) to characterize the updating of AoI for the MEC system. Based on the hierarchical MDP, we propose a nested index framework and design a nested index policy with provably asymptotic optimality. Finally, the closed form of the nested index is obtained, which enables the performance tradeoffs between computation complexity and accuracy. Our algorithm leads to an optimality gap reduction of up to 40%, compared to benchmarks. Our algorithm asymptotically approximates the lower bound as the system scalar gets large enough.
</details>
<details>
<summary>摘要</summary>
使用移动设备和边缘节点的计算多样性，移动边计算（MEC）提供了一种高效的方法来实现快速应用程序，这些应用程序需要快速获取新信息。我们使用年龄信息新鲜度（AoI）度量来评估信息新鲜度。由于移动设备到边缘节点的任务下载是随机的，因此设计一个高效的解决方案来减少AoI的差值是非常困难的。在这篇论文中，我们考虑了多个用户将任务下载到不同类型的边缘服务器。我们首先将问题转化为一个闹鼓多臂投资（RMAB）问题，然后建立了一个层次的Markov决策过程（MDP）来描述MEC系统中AoI的更新。基于层次MDP，我们提出了一个嵌入式索引框架，并设计了一个嵌入式索引策略，其可以证明是 asymptotic 优化的。最后，我们计算出嵌入式索引的闭合形式，这使得我们可以实现计算复杂度和准确性之间的性能质量负荷。我们的算法可以减少优化缺陷至多40%，相比 benchmark。我们的算法可以在系统缩放比例很大时，近似于下界。
</details></li>
</ul>
<hr>
<h2 id="Towards-Safe-Autonomous-Driving-Policies-using-a-Neuro-Symbolic-Deep-Reinforcement-Learning-Approach"><a href="#Towards-Safe-Autonomous-Driving-Policies-using-a-Neuro-Symbolic-Deep-Reinforcement-Learning-Approach" class="headerlink" title="Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach"></a>Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01316">http://arxiv.org/abs/2307.01316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cav-research-lab/safe-reinforcement-learning-using-symbolic-logical-programming-for-autonomous-highway-driving">https://github.com/cav-research-lab/safe-reinforcement-learning-using-symbolic-logical-programming-for-autonomous-highway-driving</a></li>
<li>paper_authors: Iman Sharifi, Mustafa Yildirim, Saber Fallah</li>
<li>for: 本研究旨在开发一种能够在真实环境中学习自动驾驶策略，并确保安全性的神经 символи学学习方法（DRLSL）。</li>
<li>methods: 本研究使用了神经网络和 симвоlic first-order 逻辑组合，以学习自动驾驶策略。</li>
<li>results: 实验结果表明，DRLSL 方法可以在真实环境中学习自动驾驶策略，并且在训练和测试阶段都可以避免不安全的行为。此外，DRLSL 方法在新的驾驶场景中表现更好，比传统 DRL 方法更快速地训练并且更好地泛化。<details>
<summary>Abstract</summary>
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics (knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD dataset and demonstrated that our method successfully avoids unsafe actions during both the training and testing phases. Furthermore, our results indicate that DRLSL achieves faster convergence during training and exhibits better generalizability to new driving scenarios compared to traditional DRL methods.
</details>
<details>
<summary>摘要</summary>
Autonomous driving 环境的动态性和多样化的道路用户带来了决策的挑战。深度让资料学习（DRL）已经成为解决这个问题的流行方法。然而，现有的 DRL 解决方案主要在虚拟环境中应用，这限制了它们在实际环境中的部署。为了解决这个问题，这篇文章介绍了一种新的神经符号学习（DRLSL）方法，它结合了 DRL 的学习从经验和符号逻辑的知识驱动理解来实现在实时交互中的自动驾驶策略学习。这种创新的方法可以在实际环境中学习自动驾驶策略，同时保证安全。我们在高D数据集上实现了 DRLSL 框架，并在训练和测试阶段都避免了不安全的行为。此外，我们的结果表明，DRLSL 在训练阶段更快地 converges 和在新驾驶场景中更好地泛化。
</details></li>
</ul>
<hr>
<h2 id="Self-Tuning-PID-Control-via-a-Hybrid-Actor-Critic-Based-Neural-Structure-for-Quadcopter-Control"><a href="#Self-Tuning-PID-Control-via-a-Hybrid-Actor-Critic-Based-Neural-Structure-for-Quadcopter-Control" class="headerlink" title="Self-Tuning PID Control via a Hybrid Actor-Critic-Based Neural Structure for Quadcopter Control"></a>Self-Tuning PID Control via a Hybrid Actor-Critic-Based Neural Structure for Quadcopter Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01312">http://arxiv.org/abs/2307.01312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Sharifi, Aria Alasty<br>for: 这个研究旨在实时自适应PID控制器的设计和训练，以提高Quadrotor的态度和高度控制稳定性。methods: 这个研究使用了一个基于强迫学习的神经网络，用于自适应PID控制器的训练和决策。具体来说，这个神经网络包含了两个隐藏层和sigmoid滤波函数，并使用了Adaptive Momentum（ADAM）优化器和Back-Propagation（BP）算法进行学习。results: 研究结果显示，提案的方法在面对不确定模型参数和外部干扰时能够更加稳定和有效，并且在训练和决策过程中能够快速地适应环境变化。此外，这个方法也比常规PID控制器 WITH CONSTANT GAINS更好地表现，具体来说，它能够更好地应对Quadrotor的态度和高度控制需求。<details>
<summary>Abstract</summary>
Proportional-Integrator-Derivative (PID) controller is used in a wide range of industrial and experimental processes. There are a couple of offline methods for tuning PID gains. However, due to the uncertainty of model parameters and external disturbances, real systems such as Quadrotors need more robust and reliable PID controllers. In this research, a self-tuning PID controller using a Reinforcement-Learning-based Neural Network for attitude and altitude control of a Quadrotor has been investigated. An Incremental PID, which contains static and dynamic gains, has been considered and only the variable gains have been tuned. To tune dynamic gains, a model-free actor-critic-based hybrid neural structure was used that was able to properly tune PID gains, and also has done the best as an identifier. In both tunning and identification tasks, a Neural Network with two hidden layers and sigmoid activation functions has been learned using Adaptive Momentum (ADAM) optimizer and Back-Propagation (BP) algorithm. This method is online, able to tackle disturbance, and fast in training. In addition to robustness to mass uncertainty and wind gust disturbance, results showed that the proposed method had a better performance when compared to a PID controller with constant gains.
</details>
<details>
<summary>摘要</summary>
提出了一种基于循环优化器的自适应PID控制器，用于飞行器的拜投和高度控制。在实验中，通过使用一个基于循环优化器的神经网络来自适应PID控制器的变量参数，并且通过使用一个模型自由的actor-critic型神经网络来调整动态参数。在训练过程中，使用了适应动量优化器和反射传播算法来学习神经网络。这种方法在线、能够抗扰动、快速训练，并且在飞行器的拜投和高度控制中表现更好。此外，这种方法还能够抗质量不确定和风 Gust扰动的影响。Here is the word-for-word translation of the given text into Simplified Chinese:提出了一种基于循环优化器的自适应PID控制器，用于飞行器的拜投和高度控制。在实验中，通过使用一个基于循环优化器的神经网络来自适应PID控制器的变量参数，并且通过使用一个模型自由的actor-critic型神经网络来调整动态参数。在训练过程中，使用了适应动量优化器和反射传播算法来学习神经网络。这种方法在线、能够抗扰动、快速训练，并且在飞行器的拜投和高度控制中表现更好。
</details></li>
</ul>
<hr>
<h2 id="Reliable-AI-Does-the-Next-Generation-Require-Quantum-Computing"><a href="#Reliable-AI-Does-the-Next-Generation-Require-Quantum-Computing" class="headerlink" title="Reliable AI: Does the Next Generation Require Quantum Computing?"></a>Reliable AI: Does the Next Generation Require Quantum Computing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01301">http://arxiv.org/abs/2307.01301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aras Bacho, Holger Boche, Gitta Kutyniok</li>
<li>for: The paper explores the question of whether quantum computing is necessary for the next generation of artificial intelligence.</li>
<li>methods: The paper uses various computational models, including digital and analog computing models, to evaluate the limitations of current artificial intelligence systems and the potential benefits of quantum computing.</li>
<li>results: The paper finds that current digital computing models are limited in their ability to solve certain problems, such as optimization and deep learning, and that analog computing models may offer a way to overcome these limitations. However, even when using quantum computing models, some limitations persist.<details>
<summary>Abstract</summary>
In this survey, we aim to explore the fundamental question of whether the next generation of artificial intelligence requires quantum computing. Artificial intelligence is increasingly playing a crucial role in many aspects of our daily lives and is central to the fourth industrial revolution. It is therefore imperative that artificial intelligence is reliable and trustworthy. However, there are still many issues with reliability of artificial intelligence, such as privacy, responsibility, safety, and security, in areas such as autonomous driving, healthcare, robotics, and others. These problems can have various causes, including insufficient data, biases, and robustness problems, as well as fundamental issues such as computability problems on digital hardware. The cause of these computability problems is rooted in the fact that digital hardware is based on the computing model of the Turing machine, which is inherently discrete. Notably, our findings demonstrate that digital hardware is inherently constrained in solving problems about optimization, deep learning, or differential equations. Therefore, these limitations carry substantial implications for the field of artificial intelligence, in particular for machine learning. Furthermore, although it is well known that the quantum computer shows a quantum advantage for certain classes of problems, our findings establish that some of these limitations persist when employing quantum computing models based on the quantum circuit or the quantum Turing machine paradigm. In contrast, analog computing models, such as the Blum-Shub-Smale machine, exhibit the potential to surmount these limitations.
</details>
<details>
<summary>摘要</summary>
在这份调查中，我们想要探讨人工智能下一代是否需要量子计算。人工智能在我们日常生活中越来越重要，是四个工业革命中的核心。因此，人工智能的可靠性和信worthiness是非常重要。然而，人工智能的可靠性还存在许多问题，如隐私、责任、安全和安全性，在自动驾驶、医疗、机器人等领域。这些问题的原因可能包括不充分的数据、偏见和稳定性问题，以及基本问题如计算机问题。这些计算机问题的根源在于数字硬件基于图灵机制，这是不可避免的离散的。注意，我们的发现表明，数字硬件在解决优化、深度学习和梯度方程问题方面存在极大的限制。因此，这些限制对人工智能领域，特别是机器学习领域产生了深刻的影响。此外，虽然广泛认为量子计算在某些问题上具有优势，但我们的发现表明，使用量子计算模型 based on quantum circuit或量子图灵机制时，这些限制仍然存在。相比之下，分析计算模型，如Blum-Shub-Smale机器，表现出可以突破这些限制的潜力。
</details></li>
</ul>
<hr>
<h2 id="Pareto-Secure-Machine-Learning-PSML-Fingerprinting-and-Securing-Inference-Serving-Systems"><a href="#Pareto-Secure-Machine-Learning-PSML-Fingerprinting-and-Securing-Inference-Serving-Systems" class="headerlink" title="Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems"></a>Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01292">http://arxiv.org/abs/2307.01292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debopam Sanyal, Jui-Tse Hung, Manav Agrawal, Prahlad Jasti, Shahab Nikkhoo, Somesh Jha, Tianhao Wang, Sibin Mohan, Alexey Tumanov<br>for: 这种论文主要针对于Model-serving系统的安全性问题，具体来说是针对于模型EXTRACTION攻击的Robustness问题。methods: 该论文提出了一种新的查询减少算法，以及一种基于噪声的防御机制来对抗模型EXTRACTION攻击。results: 该论文表明，使用提出的查询减少算法和噪声防御机制可以减少模型EXTRACTION攻击的精度和准确率，同时可以保持系统的性能（goodput）在接受ABLE范围内。<details>
<summary>Abstract</summary>
Model-serving systems have become increasingly popular, especially in real-time web applications. In such systems, users send queries to the server and specify the desired performance metrics (e.g., desired accuracy, latency). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks assume a single model can be repeatedly selected for serving inference requests. Modern inference serving systems break this assumption. Thus, they cannot be directly applied to extract a victim model, as models are hidden behind a layer of abstraction exposed by the serving system. An attacker can no longer identify which model she is interacting with. To this end, we first propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accuracy scores within $1\%$ of the scores obtained when attacking a single, explicitly specified model, as well as up to $14.6\%$ gain in accuracy and up to $7.7\%$ gain in fidelity compared to the naive attack. Second, we counter the proposed attack with a noise-based defense mechanism that thwarts fingerprinting by adding noise to the specified performance metrics. The proposed defense strategy reduces the attack's accuracy and fidelity by up to $9.8\%$ and $4.8\%$, respectively (on medium-sized model extraction). Third, we show that the proposed defense induces a fundamental trade-off between the level of protection and system goodput, achieving configurable and significant victim model extraction protection while maintaining acceptable goodput ($>80\%$). We implement the proposed defense in a real system with plans to open source.
</details>
<details>
<summary>摘要</summary>
现代服务系统中的模型服务系统在实时网络应用中变得越来越流行，用户可以向服务器发送查询，并指定所需的性能指标（例如精度和响应时间）。服务器将维护一个模型集（模型zoo），并根据用户的查询来提供服务。这篇论文检查这些系统的安全性，尤其是对于模型提取攻击的Robustness。现有的黑盒攻击假设可以重复选择服务器提供的模型来进行推理请求。然而，现代推理服务系统破坏了这一假设，因此无法直接应用于提取受害者模型。攻击者无法确定她正在互动的模型。为此，我们首先提出了一种高效的询问 fingerprinting 算法，允许攻击者随时触发所需的模型。我们显示，使用我们的 fingerprinting 算法可以在 $1\%$ 的精度和准确性上达到单个、显式指定的模型攻击的精度和准确性水平，并且可以在 $14.6\%$ 的精度和 $7.7\%$ 的准确性上获得更高的提取精度和准确性。其次，我们采用噪音基的防御机制来抵御 fingerprinting，通过添加噪音到指定的性能指标来推迟攻击。我们的防御策略可以在中等模型提取 task 下 reducethe attack's accuracy和fidelity by up to $9.8\%$和$4.8\%$，respectively。最后，我们表明了我们的防御策略存在一定的质量负担和系统好put的负担，可以实现可配置的和显著的受害者模型提取保护，同时维护 Acceptable 的好put ($>80\%$).我们已经实现了我们的防御策略，计划将其开源。
</details></li>
</ul>
<hr>
<h2 id="Fighting-the-disagreement-in-Explainable-Machine-Learning-with-consensus"><a href="#Fighting-the-disagreement-in-Explainable-Machine-Learning-with-consensus" class="headerlink" title="Fighting the disagreement in Explainable Machine Learning with consensus"></a>Fighting the disagreement in Explainable Machine Learning with consensus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01288">http://arxiv.org/abs/2307.01288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Jesus Banegas-Luna, Carlos Martınez-Cortes, Horacio Perez-Sanchez</li>
<li>for: 了解机器学习模型内部工作方式</li>
<li>methods: 使用解释算法进行模型解释</li>
<li>results: 研究发现提出的函数比其他函数更公正、更一致、更准确地解释了五种机器学习模型。<details>
<summary>Abstract</summary>
Machine learning (ML) models are often valued by the accuracy of their predictions. However, in some areas of science, the inner workings of models are as relevant as their accuracy. To understand how ML models work internally, the use of interpretability algorithms is the preferred option. Unfortunately, despite the diversity of algorithms available, they often disagree in explaining a model, leading to contradictory explanations. To cope with this issue, consensus functions can be applied once the models have been explained. Nevertheless, the problem is not completely solved because the final result will depend on the selected consensus function and other factors. In this paper, six consensus functions have been evaluated for the explanation of five ML models. The models were previously trained on four synthetic datasets whose internal rules were known in advance. The models were then explained with model-agnostic local and global interpretability algorithms. Finally, consensus was calculated with six different functions, including one developed by the authors. The results demonstrated that the proposed function is fairer than the others and provides more consistent and accurate explanations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Using-BOLD-fMRI-to-Compute-the-Respiration-Volume-per-Time-RTV-and-Respiration-Variation-RV-with-Convolutional-Neural-Networks-CNN-in-the-Human-Connectome-Development-Cohort"><a href="#Using-BOLD-fMRI-to-Compute-the-Respiration-Volume-per-Time-RTV-and-Respiration-Variation-RV-with-Convolutional-Neural-Networks-CNN-in-the-Human-Connectome-Development-Cohort" class="headerlink" title="Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort"></a>Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05426">http://arxiv.org/abs/2307.05426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdoljalil Addeh, Fernando Vega, Rebecca J Williams, Ali Golestani, G. Bruce Pike, M. Ethan MacDonald</li>
<li>for: 降低fMRI研究成本、简化实验设备、减轻参与者负担。</li>
<li>methods: 使用一维度卷积神经网络模型，从休息BOLD信号中捕捉有用的特征，重建实际的呼吸周期和呼吸变化时间序列。</li>
<li>results: CNN模型能够从休息BOLD信号中捕捉有用的特征，重建实际的呼吸周期和呼吸变化时间序列。<details>
<summary>Abstract</summary>
In many fMRI studies, respiratory signals are unavailable or do not have acceptable quality. Consequently, the direct removal of low-frequency respiratory variations from BOLD signals is not possible. This study proposes a one-dimensional CNN model for reconstruction of two respiratory measures, RV and RVT. Results show that a CNN can capture informative features from resting BOLD signals and reconstruct realistic RV and RVT timeseries. It is expected that application of the proposed method will lower the cost of fMRI studies, reduce complexity, and decrease the burden on participants as they will not be required to wear a respiratory bellows.
</details>
<details>
<summary>摘要</summary>
很多fMRI研究中，呼吸信号不可用或质量不符合要求。因此，直接从BOLD信号中除掉低频呼吸变化不能进行。这项研究提议一种一维Convolutional Neural Network（CNN）模型，用于重建两个呼吸指标：RV和RVT。结果表明，CNN可以从休息BOLD信号中捕捉有用的特征，重建实际的RV和RVT时序。预计通过该方法应用，将降低fMRI研究的成本，降低复杂性，并减轻参与者的负担，他们不需要穿着呼吸膜。
</details></li>
</ul>
<hr>
<h2 id="NeuBTF-Neural-fields-for-BTF-encoding-and-transfer"><a href="#NeuBTF-Neural-fields-for-BTF-encoding-and-transfer" class="headerlink" title="NeuBTF: Neural fields for BTF encoding and transfer"></a>NeuBTF: Neural fields for BTF encoding and transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01199">http://arxiv.org/abs/2307.01199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Rodriguez-Pardo, Konstantinos Kazatzis, Jorge Lopez-Moreno, Elena Garces</li>
<li>for: 本研究提出了一种新的神经材料表示法，用于解决神经材料的固定性问题，以提高渲染效果。</li>
<li>methods: 该方法使用导航图像作为输入，用于condition神经BTF的结构特征。然后，神经BTF可以用UVs、摄像机和光照 вектор进行查询。</li>
<li>results: 该方法在多种 sintetic和捕捉的材料上实现了竞争性的压缩率，并且能够学习表示多种光学性质。<details>
<summary>Abstract</summary>
Neural material representations are becoming a popular way to represent materials for rendering. They are more expressive than analytic models and occupy less memory than tabulated BTFs. However, existing neural materials are immutable, meaning that their output for a certain query of UVs, camera, and light vector is fixed once they are trained. While this is practical when there is no need to edit the material, it can become very limiting when the fragment of the material used for training is too small or not tileable, which frequently happens when the material has been captured with a gonioreflectometer. In this paper, we propose a novel neural material representation which jointly tackles the problems of BTF compression, tiling, and extrapolation. At test time, our method uses a guidance image as input to condition the neural BTF to the structural features of this input image. Then, the neural BTF can be queried as a regular BTF using UVs, camera, and light vectors. Every component in our framework is purposefully designed to maximize BTF encoding quality at minimal parameter count and computational complexity, achieving competitive compression rates compared with previous work. We demonstrate the results of our method on a variety of synthetic and captured materials, showing its generality and capacity to learn to represent many optical properties.
</details>
<details>
<summary>摘要</summary>
神经材料表示法正在成为渲染中的受欢迎方法。它们比分析模型更加表达力，占用内存更少，但现有的神经材料都是不可变的，意味着它们在训练后的输出将 forever fixed。这在没有需要修改材料时是有用的，但在材料的 Fragment 太小或不可平铺时会变得非常局限。在这篇论文中，我们提出了一种新的神经材料表示法，该法同时解决了 BTF 压缩、瓦片和推理问题。在测试时，我们使用引导图像作为输入，通过神经 BTF 根据 UV、摄像头和光照向量进行访问。每个组件在我们的框架中都是为 maximize BTF 编码质量而设计，同时保持最低的参数计数和计算复杂度，与前一个工作相比，我们的压缩率具有竞争力。我们在多种 sintetic 和 captured 材料上展示了我们的方法的通用性和能力学习表示多种光学性质。
</details></li>
</ul>
<hr>
<h2 id="Squeezing-Large-Scale-Diffusion-Models-for-Mobile"><a href="#Squeezing-Large-Scale-Diffusion-Models-for-Mobile" class="headerlink" title="Squeezing Large-Scale Diffusion Models for Mobile"></a>Squeezing Large-Scale Diffusion Models for Mobile</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01193">http://arxiv.org/abs/2307.01193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiwoong Choi, Minkyu Kim, Daehyun Ahn, Taesu Kim, Yulhwa Kim, Dongwon Jo, Hyesung Jeon, Jae-Joon Kim, Hyungjun Kim</li>
<li>for: 本文旨在探讨如何将稳定扩散模型（Stable Diffusion）部署到移动设备上，以便实现高精度图像生成。</li>
<li>methods: 本文使用了TensorFlow Lite框架来部署稳定扩散模型到移动设备上，并解决了由限制计算资源和存储空间所带来的问题。</li>
<li>results: 研究人员通过实现Mobile Stable Diffusion来降低了512x512像素图像生成的推理时间至少于7秒，在Android设备上使用移动GPU进行推理。<details>
<summary>Abstract</summary>
The emergence of diffusion models has greatly broadened the scope of high-fidelity image synthesis, resulting in notable advancements in both practical implementation and academic research. With the active adoption of the model in various real-world applications, the need for on-device deployment has grown considerably. However, deploying large diffusion models such as Stable Diffusion with more than one billion parameters to mobile devices poses distinctive challenges due to the limited computational and memory resources, which may vary according to the device. In this paper, we present the challenges and solutions for deploying Stable Diffusion on mobile devices with TensorFlow Lite framework, which supports both iOS and Android devices. The resulting Mobile Stable Diffusion achieves the inference latency of smaller than 7 seconds for a 512x512 image generation on Android devices with mobile GPUs.
</details>
<details>
<summary>摘要</summary>
diffusion 模型的出现对高精度图像生成带来了广泛的应用场景，从实际应用到学术研究都有了很大的进步。随着模型在各种实际应用中的推广，需要将模型部署到移动设备上的需求增长了。然而，将大 diffusion 模型（如稳定扩散），它有超过一十亿参数，部署到移动设备上具有限制的计算资源和内存资源的问题。在这篇文章中，我们介绍了将 Stable Diffusion 部署到移动设备上的挑战和解决方案，使用 TensorFlow Lite 框架支持 iOS 和 Android 设备。Mobile Stable Diffusion 实现了对 Android 设备上的移动 GPU 进行512x512像素图像生成的评估时间小于 7 秒。
</details></li>
</ul>
<hr>
<h2 id="SAMAug-Point-Prompt-Augmentation-for-Segment-Anything-Model"><a href="#SAMAug-Point-Prompt-Augmentation-for-Segment-Anything-Model" class="headerlink" title="SAMAug: Point Prompt Augmentation for Segment Anything Model"></a>SAMAug: Point Prompt Augmentation for Segment Anything Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01187">http://arxiv.org/abs/2307.01187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haixing Dai, Chong Ma, Zhengliang Liu, Yiwei Li, Peng Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Dajiang Zhu, Wei Liu, Quanzheng Li, Tianming Liu, Xiang Li</li>
<li>for: 这篇论文是为了提高交互式图像分割模型（SAM）的性能而写的。</li>
<li>methods: 这篇论文提出了一种新的视觉点增强方法，称为SAMAug，用于增强SAM的分割性能。SAMAug生成了增强点提示，以提供更多的信息给SAM。</li>
<li>results: 经过测试COCO、Fundus和Chest X-ray等数据集，研究发现SAMAug可以提高SAM的分割性能，尤其是使用最大差Entropy和Saliency模型方法时。这种方法表明了视觉提示工程可以推动交互计算机视觉模型的进步。<details>
<summary>Abstract</summary>
This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information to SAM. From the initial point prompt, SAM produces the initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We evaluate four point augmentation techniques: random selection, maximum difference entropy, maximum distance, and a saliency model. Experiments on the COCO, Fundus, and Chest X-ray datasets demonstrate that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency model methods. SAMAug underscores the potential of visual prompt engineering to advance interactive computer vision models.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了SAMAug，一种新的视觉点增强方法，用于提高Segment Anything Model（SAM）的交互图像分割性能。SAMAug生成了增强后的点提示，以提供更多信息给SAM。从初始点提示开始，SAM生成了初始面积，然后我们提posed SAMAug将这些增强后的点提示与初始提示相结合，以生成增强后的分割面积。通过这种方式，SAM可以基于增强后的点提示和初始提示来生成更好的分割结果。我们评估了四种点增强技术：随机选择、最大差异熵、最大距离和聚光模型。在COCO、Fundus和Chest X-ray数据集上进行了实验，结果表明，使用最大距离和聚光模型方法时，SAMAug可以提高SAM的分割结果，特别是在远程图像分割任务上。SAMAug还证明了视觉提示工程的潜力，可以推动交互计算机视觉模型的进步。
</details></li>
</ul>
<hr>
<h2 id="PlanE-Representation-Learning-over-Planar-Graphs"><a href="#PlanE-Representation-Learning-over-Planar-Graphs" class="headerlink" title="PlanE: Representation Learning over Planar Graphs"></a>PlanE: Representation Learning over Planar Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01180">http://arxiv.org/abs/2307.01180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zzysonny/plane">https://github.com/zzysonny/plane</a></li>
<li>paper_authors: Radoslav Dimitrov, Zeyang Zhao, Ralph Abboud, İsmail İlkan Ceylan</li>
<li>for: 本研究的目的是设计一个可以实现完整Graph Isomorphism的planar graphs Representation Learning架构。</li>
<li>methods: 本研究使用了PlanE框架，其中包括一些可以学习完整Graph Isomorphism的planar graphs Representation Learning架构。</li>
<li>results: 实验结果表明，PlanE框架可以实现高效地learning complete invariants over planar graphs，并在well-known planar graph benchmarks上achieve multiple state-of-the-art results。<details>
<summary>Abstract</summary>
Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations graph invariants. On the other hand, it is well-known that graph invariants learned by these class of models are incomplete: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and Tarjan, we propose PlanE as a framework for planar representation learning. PlanE includes architectures which can learn complete invariants over planar graphs while remaining practically scalable. We empirically validate the strong performance of the resulting model architectures on well-known planar graph benchmarks, achieving multiple state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
граф neural networks是输入图形的表示学习模型的主要选择，其中的思想是通过一系列变换来计算输入图形中节点的表示，以确保学习的图函数是isoomorfism不变的，这使得学习的表示是图 invariants。然而，这类模型学习的图 invariants是不完整的：存在非isoomorfism的图对的标准图 neural networks无法分辨。这并不奇怪，因为普通图 isomorphism testing 是NP完备问题，但是在特殊的图类中，有高效的图 isomorphism testing 算法，如平面图。本文的目标是设计能够有效地学习平面图的完整 invariants 的架构。我们提出 PlanE 框架，它包括可以学习平面图中的完整 invariants 的建筑。我们验证了 PlanE 的实验性成果，在一些知名的平面图 bencmarks 上达到了多个 state-of-the-art 结果。
</details></li>
</ul>
<hr>
<h2 id="Don’t-freeze-Finetune-encoders-for-better-Self-Supervised-HAR"><a href="#Don’t-freeze-Finetune-encoders-for-better-Self-Supervised-HAR" class="headerlink" title="Don’t freeze: Finetune encoders for better Self-Supervised HAR"></a>Don’t freeze: Finetune encoders for better Self-Supervised HAR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01168">http://arxiv.org/abs/2307.01168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vitor Fortes Rey, Dominique Nshimyimana, Paul Lukowicz</li>
<li>for:  solves the labelled data availability problem in human activity recognition</li>
<li>methods:  uses pretext tasks such as reconstruction or contrastive predictive coding to learn useful representations</li>
<li>results:  substantial performance gains across pretext tasks, with the improvement inversely proportional to the amount of labelled data.<details>
<summary>Abstract</summary>
Recently self-supervised learning has been proposed in the field of human activity recognition as a solution to the labelled data availability problem. The idea being that by using pretext tasks such as reconstruction or contrastive predictive coding, useful representations can be learned that then can be used for classification. Those approaches follow the pretrain, freeze and fine-tune procedure. In this paper we will show how a simple change - not freezing the representation - leads to substantial performance gains across pretext tasks. The improvement was found in all four investigated datasets and across all four pretext tasks and is inversely proportional to amount of labelled data. Moreover the effect is present whether the pretext task is carried on the Capture24 dataset or directly in unlabelled data of the target dataset.
</details>
<details>
<summary>摘要</summary>
近些时候，自我指导学习在人体活动识别领域被提出，作为标签数据可用性问题的解决方案。这种方法通过使用预测任务，如重建或对比预测编码，来学习有用的表示。这些方法遵循“预训练、冻结并微调”的过程。在这篇论文中，我们将展示一种简单的变化，即不冻结表示，导致了重大的性能提升，并在所有四个调查dataset和所有四个预测任务中都有效。此外，这种效果是无论预测任务是在Capture24 dataset上进行还是直接在无标签数据中进行的。
</details></li>
</ul>
<hr>
<h2 id="Human-in-the-AI-loop-via-xAI-and-Active-Learning-for-Visual-Inspection"><a href="#Human-in-the-AI-loop-via-xAI-and-Active-Learning-for-Visual-Inspection" class="headerlink" title="Human in the AI loop via xAI and Active Learning for Visual Inspection"></a>Human in the AI loop via xAI and Active Learning for Visual Inspection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05508">http://arxiv.org/abs/2307.05508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jože M. Rožanec, Elias Montini, Vincenzo Cutrona, Dimitrios Papamartzivanos, Timotej Klemenčič, Blaž Fortuna, Dunja Mladenić, Entso Veliou, Thanassis Giannetsos, Christos Emmanouilidis</li>
<li>for: 这篇论文主要是关于工业革命的影响和人机合作的研究。</li>
<li>methods: 论文使用了活动学习和可解释人工智能等两个人工智能子领域，以实现人机合作。</li>
<li>results: 论文提出了人机合作在视觉检测方面的可能性，并在欧盟H2020星计划中获得了一些关于视觉检测的结果，包括人工智能、人数字双生和网络安全等方面的研究。<details>
<summary>Abstract</summary>
Industrial revolutions have historically disrupted manufacturing by introducing automation into production. Increasing automation reshapes the role of the human worker. Advances in robotics and artificial intelligence open new frontiers of human-machine collaboration. Such collaboration can be realized considering two sub-fields of artificial intelligence: active learning and explainable artificial intelligence. Active learning aims to devise strategies that help obtain data that allows machine learning algorithms to learn better. On the other hand, explainable artificial intelligence aims to make the machine learning models intelligible to the human person. The present work first describes Industry 5.0, human-machine collaboration, and state-of-the-art regarding quality inspection, emphasizing visual inspection. Then it outlines how human-machine collaboration could be realized and enhanced in visual inspection. Finally, some of the results obtained in the EU H2020 STAR project regarding visual inspection are shared, considering artificial intelligence, human digital twins, and cybersecurity.
</details>
<details>
<summary>摘要</summary>
工业革命历史上都会对制造进行重大的变革，通过引入自动化技术来提高生产效率。随着机器人和人工智能的发展，人工与机器之间的合作被打开了新的前ier。这种合作可以通过两个人工智能的子领域来实现：活动学习和可 explainable artificial intelligence。活动学习的目标是开发出用于帮助机器学习算法学习的策略，而可 explainable artificial intelligence的目标是使机器学习模型对人类更加明了。本文首先描述了第五代工业革命（Industry 5.0）、人机合作和当前在质量检查方面的状况，特别是视觉检查。然后，它详细介绍了如何通过人机合作来增强视觉检查。最后，本文分享了在欧盟H2020 STAR项目中关于视觉检查的一些结果，包括人工智能、人数字双和安全性。
</details></li>
</ul>
<hr>
<h2 id="Soft-Gripping-Specifying-for-Trustworthiness"><a href="#Soft-Gripping-Specifying-for-Trustworthiness" class="headerlink" title="Soft Gripping: Specifying for Trustworthiness"></a>Soft Gripping: Specifying for Trustworthiness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01159">http://arxiv.org/abs/2307.01159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhaminda B. Abeywickrama, Nguyen Hao Le, Greg Chance, Peter D. Winter, Arianna Manzini, Alix J. Partridge, Jonathan Ives, John Downer, Graham Deacon, Jonathan Rossiter, Kerstin Eder, Shane Windsor</li>
<li>for: 这篇论文主要用于推动软体机器人技术的广泛应用，提高软体机器人的可靠性和信任性。</li>
<li>methods: 该论文提出了对软体机器人系统的规范化需求，包括功能性和非功能性需求，如可靠性、安全性、适应性、预测性、伦理和法规要求。</li>
<li>results: 该论文提出了一个广泛的软体机器人抓取器的规范，用于快递卸载各种商品。该规范覆盖了软体机器人抓取器的功能和非功能需求，以提高软体机器人的可靠性和信任性。<details>
<summary>Abstract</summary>
Soft robotics is an emerging technology in which engineers create flexible devices for use in a variety of applications. In order to advance the wide adoption of soft robots, ensuring their trustworthiness is essential; if soft robots are not trusted, they will not be used to their full potential. In order to demonstrate trustworthiness, a specification needs to be formulated to define what is trustworthy. However, even for soft robotic grippers, which is one of the most mature areas in soft robotics, the soft robotics community has so far given very little attention to formulating specifications. In this work, we discuss the importance of developing specifications during development of soft robotic systems, and present an extensive example specification for a soft gripper for pick-and-place tasks for grocery items. The proposed specification covers both functional and non-functional requirements, such as reliability, safety, adaptability, predictability, ethics, and regulations. We also highlight the need to promote verifiability as a first-class objective in the design of a soft gripper.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Theory-of-Mind-as-Intrinsic-Motivation-for-Multi-Agent-Reinforcement-Learning"><a href="#Theory-of-Mind-as-Intrinsic-Motivation-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning"></a>Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01158">http://arxiv.org/abs/2307.01158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ini Oguntola, Joseph Campbell, Simon Stepputtis, Katia Sycara</li>
<li>for: 提高人工智能在多代理器中的社会智能能力，使其能够模拟人类的心理状态。</li>
<li>methods: 使用深度网络模型精准地表示政策，并将含义深刻的信仰ground在政策中。</li>
<li>results: 在混合合作竞争环境中实现了初步的实验成果。<details>
<summary>Abstract</summary>
The ability to model the mental states of others is crucial to human social intelligence, and can offer similar benefits to artificial agents with respect to the social dynamics induced in multi-agent settings. We present a method of grounding semantically meaningful, human-interpretable beliefs within policies modeled by deep networks. We then consider the task of 2nd-order belief prediction. We propose that ability of each agent to predict the beliefs of the other agents can be used as an intrinsic reward signal for multi-agent reinforcement learning. Finally, we present preliminary empirical results in a mixed cooperative-competitive environment.
</details>
<details>
<summary>摘要</summary>
人类社交智能中能模拟他人的心理状态是关键，可以为人工智能 Agent 在多个 Agent 设置中带来类似的 beneficial 效果。我们提出了将 semantically meaningful 和 human-interpretable 的 belief 嵌入 deep network 中的策略中。然后，我们考虑了第二阶段的 belief 预测任务。我们认为每个 Agent 可以使用它他人的 belief 预测作为多 Agent 学习 reinforcement 中的潜在奖励信号。最后，我们提供了一些初步的实验结果在混合合作-竞争环境中。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="SCITUNE-Aligning-Large-Language-Models-with-Scientific-Multimodal-Instructions"><a href="#SCITUNE-Aligning-Large-Language-Models-with-Scientific-Multimodal-Instructions" class="headerlink" title="SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions"></a>SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01139">http://arxiv.org/abs/2307.01139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lupantech/ScienceQA">https://github.com/lupantech/ScienceQA</a></li>
<li>paper_authors: Sameera Horawalavithana, Sai Munikoti, Ian Stewart, Henry Kvinge</li>
<li>for: 这个论文主要是为了提高大型语言模型（LLM）与科学领域的整合。</li>
<li>methods: 这篇论文使用了一种名为SciTune的调整框架，以提高LLM的科学多Modal指令遵循能力。</li>
<li>results: 与机器生成数据进行finetuning的模型相比，LLaMA-SciTune在科学问答 benchMark中的表现平均高于人类表现，并在多个子类中也达到了人类水平。<details>
<summary>Abstract</summary>
Instruction finetuning is a popular paradigm to align large language models (LLM) with human intent. Despite its popularity, this idea is less explored in improving the LLMs to align existing foundation models with scientific disciplines, concepts and goals. In this work, we present SciTune as a tuning framework to improve the ability of LLMs to follow scientific multimodal instructions. To test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model LLaMA-SciTune that connects a vision encoder and LLM for science-focused visual and language understanding. In comparison to the models that are finetuned with machine generated data only, LLaMA-SciTune surpasses human performance on average and in many sub-categories on the ScienceQA benchmark.
</details>
<details>
<summary>摘要</summary>
科学定uning是一种流行的思想，用于将大型语言模型（LLM）与人类意图相对应。尽管这个想法在改进现有基础模型方面得到了更多的关注，但是它在科学领域中的应用还很少。在这项工作中，我们提出了SciTune作为一种调整框架，用于改进LLM的遵循科学多模式指令的能力。为测试我们的方法ологи，我们使用了人类生成的科学指令调整数据集，并训练了一个大型多modal模型LLaMA-SciTune，该模型连接了视觉编码器和LLM，用于科学领域中的视觉和语言理解。与由机器生成数据进行Finetuning的模型相比，LLaMA-SciTune在科学问答标准 bencmark上平均和许多子类型方面超越了人类表现。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-In-context-Learning-Ability-of-Large-Language-Model-for-Biomedical-Concept-Linking"><a href="#Exploring-the-In-context-Learning-Ability-of-Large-Language-Model-for-Biomedical-Concept-Linking" class="headerlink" title="Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking"></a>Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01137">http://arxiv.org/abs/2307.01137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyong Wang, Zhenxiang Gao, Rong Xu</li>
<li>for:  This research aims to explore the effectiveness of large language models (LLMs) in biomedical concept mapping, specifically in the task of biomedical concept linking.</li>
<li>methods: The proposed approach uses a two-stage retrieve-and-rank framework that leverages in-context learning (ICL) capabilities of LLMs. The approach first embeds biomedical concepts using language models, and then uses embedding similarity to retrieve the top candidates. The contextual information of these candidates is incorporated into the prompt and processed by a large language model to re-rank the concepts.</li>
<li>results: The approach achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7% in chemical entity normalization, demonstrating competitive performance relative to supervised learning methods. Additionally, it showed a significant improvement (over 20-point absolute increase in F1 score) on an oncology matching dataset.<details>
<summary>Abstract</summary>
The biomedical field relies heavily on concept linking in various areas such as literature mining, graph alignment, information retrieval, question-answering, data, and knowledge integration. Although large language models (LLMs) have made significant strides in many natural language processing tasks, their effectiveness in biomedical concept mapping is yet to be fully explored. This research investigates a method that exploits the in-context learning (ICL) capabilities of large models for biomedical concept linking. The proposed approach adopts a two-stage retrieve-and-rank framework. Initially, biomedical concepts are embedded using language models, and then embedding similarity is utilized to retrieve the top candidates. These candidates' contextual information is subsequently incorporated into the prompt and processed by a large language model to re-rank the concepts. This approach achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7% in chemical entity normalization, exhibiting a competitive performance relative to supervised learning methods. Further, it showed a significant improvement, with an over 20-point absolute increase in F1 score on an oncology matching dataset. Extensive qualitative assessments were conducted, and the benefits and potential shortcomings of using large language models within the biomedical domain were discussed. were discussed.
</details>
<details>
<summary>摘要</summary>
生物医学领域强调概念链接在文献检索、图像对alignment、信息检索、问答系统、数据和知识 интеграции等方面发挥重要作用。虽然大型自然语言处理模型（LLM）在许多自然语言处理任务中取得了 significiant进步，但它们在生物医学概念映射方面的效果尚未得到完全探索。本研究探讨了一种利用大型模型的在场学习（ICL）能力进行生物医学概念链接的方法。该方法采用了两个阶段的 retrieve-and-rank框架。首先，生物医学概念被使用语言模型进行嵌入，然后使用嵌入相似性来 retrieve top candidates。这些候选的上下文信息然后被 incorporated 到提示中，并被一个大型语言模型处理以重新排名概念。该方法在 BC5CDR 疾病实体Normalization 和化学实体Normalization 中实现了 90% 的准确率和 94.7% 的准确率，与超级vised learning方法相当。此外，它在生物医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学医学�
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-vs-Google-A-Comparative-Study-of-Search-Performance-and-User-Experience"><a href="#ChatGPT-vs-Google-A-Comparative-Study-of-Search-Performance-and-User-Experience" class="headerlink" title="ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience"></a>ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01135">http://arxiv.org/abs/2307.01135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiyun Xu, Yue Feng, Hailiang Chen</li>
<li>for:  investigate the differences in user behavior when employing search engines and chatbot tools for information-seeking tasks</li>
<li>methods: randomized online experiment, ChatGPT-like tool and Google Search-like tool</li>
<li>results: ChatGPT group consistently spends less time on all tasks, ChatGPT levels user search performance across different education levels, perceived information quality and user experience are better with ChatGPT, but may also lead to overreliance and generate or replicate misinformation.Here is the same information in Simplified Chinese text:</li>
<li>for: 研究用户在使用搜索引擎和 чат机器人工具时的行为差异</li>
<li>methods: 随机在线实验，使用ChatGPT类工具和Google搜索类工具</li>
<li>results: ChatGPT组的时间投入都比较短，ChatGPT在不同教育水平上的搜索性能相似，但可能会导致过依赖和生成或复制错误信息。<details>
<summary>Abstract</summary>
The advent of ChatGPT, a large language model-powered chatbot, has prompted questions about its potential implications for traditional search engines. In this study, we investigate the differences in user behavior when employing search engines and chatbot tools for information-seeking tasks. We carry out a randomized online experiment, dividing participants into two groups: one using a ChatGPT-like tool and the other using a Google Search-like tool. Our findings reveal that the ChatGPT group consistently spends less time on all tasks, with no significant difference in overall task performance between the groups. Notably, ChatGPT levels user search performance across different education levels and excels in answering straightforward questions and providing general solutions but falls short in fact-checking tasks. Users perceive ChatGPT's responses as having higher information quality compared to Google Search, despite displaying a similar level of trust in both tools. Furthermore, participants using ChatGPT report significantly better user experiences in terms of usefulness, enjoyment, and satisfaction, while perceived ease of use remains comparable between the two tools. However, ChatGPT may also lead to overreliance and generate or replicate misinformation, yielding inconsistent results. Our study offers valuable insights for search engine management and highlights opportunities for integrating chatbot technologies into search engine designs.
</details>
<details>
<summary>摘要</summary>
《ChatGPT的出现：一项研究 traditional search engines的影响》Introduction:随着ChatGPT的出现，它使得用户对传统搜索引擎的使用方式和功能表现出了不同的需求和预期。本研究旨在探讨用户在使用搜索引擎和ChatGPT工具时的行为差异。我们采用了随机在线实验，将参与者分为两组：一组使用ChatGPT类工具，另一组使用Google搜索类工具。我们的发现表明，ChatGPT组的用户在所有任务上的时间投入相对较少，没有显著差异在总任务表现水平之间。另外，ChatGPT在简单问题和通用解决方案方面表现出色，但在事实核查任务方面表现不佳。用户对ChatGPT的回答评价为高信息质量，尽管显示类似的信任度。此外，使用ChatGPT的参与者对工具的用户体验比使用Google搜索更高，包括有用性、愉悦度和满意度，但易用性认知不异。然而，ChatGPT也可能导致过度依赖和生成或复制错误信息，导致不一致的结果。本研究对搜索引擎管理提供了有价值的发现，同时也探讨了将chatbot技术integrated into search engine designs的可能性。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Iterative-Zero-Shot-LLM-Prompting-for-Knowledge-Graph-Construction"><a href="#Iterative-Zero-Shot-LLM-Prompting-for-Knowledge-Graph-Construction" class="headerlink" title="Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction"></a>Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01128">http://arxiv.org/abs/2307.01128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pompianu, Sandro Gabriele Tiddia</li>
<li>for: 本研究旨在提出一种可扩展和灵活的知识图生成方法，以解决现有知识图生成技术的瓶颈和局限性。</li>
<li>methods: 该方法基于最新的生成大语言模型GPT-3.5，包括迭代提示策略和外部知识无关策略，以解决知识图生成过程中的主要挑战。</li>
<li>results: 实验结果表明，该方法可以有效地生成高质量的知识图，并且可以应对不同的应用场景。<details>
<summary>Abstract</summary>
In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for "guiding" the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.
</details>
<details>
<summary>摘要</summary>
在当今数字化时代，捕捉并有效地表达知识是许多实际场景中的关键。在这个 контексте，知识图表示一种可观之的工具，可以快速地检索和组织大量信息，并将其拼接成可读可写的结构。然而，知识图的生成仍然是一个挑战，经常需要大量的人工劳动和领域专业知识，从而限制了扩展性和灵活性在不同应用领域。这篇论文提出了一种创新的知识图生成方法，利用最新的生成大语言模型，如GPT-3.5，解决了知识图生成中的主要挑战。该方法通过一个包含新的迭代零shot和外部知识无关策略的管道来实现。我们的独特 manifoldapproach可能带来了科学社区的重要收益。具体来说，主要贡献可以概括为：（i）一种创新的迭代Prompt大语言模型中提取 relevante组件的策略；（ii）每个Prompt不需要提供示例，即零shot策略；（iii）可扩展的解决方案，因为采用LLMs可以避免任何外部资源或人类专业知识的需求。为评估我们提出的模型效果，我们在特定领域中进行了实验。我们宣称，我们的提案是一种可扩展和多样化的知识图生成方法，可以应用于不同的和新的上下文。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/04/cs.AI_2023_07_04/" data-id="clp88dbn50004ob8831p5876v" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/cs.CL_2023_07_04/" class="article-date">
  <time datetime="2023-07-04T11:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/cs.CL_2023_07_04/">cs.CL - 2023-07-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Knowledge-Aware-Audio-Grounded-Generative-Slot-Filling-for-Limited-Annotated-Data"><a href="#Knowledge-Aware-Audio-Grounded-Generative-Slot-Filling-for-Limited-Annotated-Data" class="headerlink" title="Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data"></a>Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01764">http://arxiv.org/abs/2307.01764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/the-anonymous-bs/espnet">https://github.com/the-anonymous-bs/espnet</a></li>
<li>paper_authors: Guangzhi Sun, Chao Zhang, Ivan Vulić, Paweł Budzianowski, Philip C. Woodland</li>
<li>for: 提高 task-oriented dialogue (ToD) 系统中的 slot-filling 精度和效率，即使具有有限的标注数据。</li>
<li>methods: 提出了一种 Knowledge-Aware Audio-Grounded 框架（KA2G），通过将 text 生成任务和 audio 模式结合起来，实现了数据稀缺下的 slot-filling。KA2G 还使用了可用的外部知识（如预先定义的槽值列表）来进一步提高 slot-filling 的精度。</li>
<li>results: KA2G 在 speech-based single-turn SLURP 数据集和一个商业 ToD 系统中的 multi-turn 数据集上进行了实验，并显示了与先前作品相比，特别是在 few-shot 和 zero-shot 设置下，具有强大和一致的提升。<details>
<summary>Abstract</summary>
Manually annotating fine-grained slot-value labels for task-oriented dialogue (ToD) systems is an expensive and time-consuming endeavour. This motivates research into slot-filling methods that operate with limited amounts of labelled data. Moreover, the majority of current work on ToD is based solely on text as the input modality, neglecting the additional challenges of imperfect automatic speech recognition (ASR) when working with spoken language. In this work, we propose a Knowledge-Aware Audio-Grounded generative slot-filling framework, termed KA2G, that focuses on few-shot and zero-shot slot filling for ToD with speech input. KA2G achieves robust and data-efficient slot filling for speech-based ToD by 1) framing it as a text generation task, 2) grounding text generation additionally in the audio modality, and 3) conditioning on available external knowledge (e.g. a predefined list of possible slot values). We show that combining both modalities within the KA2G framework improves the robustness against ASR errors. Further, the knowledge-aware slot-value generator in KA2G, implemented via a pointer generator mechanism, particularly benefits few-shot and zero-shot learning. Experiments, conducted on the standard speech-based single-turn SLURP dataset and a multi-turn dataset extracted from a commercial ToD system, display strong and consistent gains over prior work, especially in few-shot and zero-shot setups.
</details>
<details>
<summary>摘要</summary>
人工标注细腻槽值标签 для任务对话（ToD）系统是一项昂贵和耗时的努力。这种情况激励了对槽 filling 方法的研究，这些方法可以采用有限量的标注数据。此外，当前大多数 ToD 研究仅基于文本输入模式，忽略了自动语音识别（ASR）的额外挑战。在这种工作中，我们提出了一个知识感知音频根据 generator 框架（KA2G），这个框架专注于几 shot 和零 shot 槽 filling  для speech-based ToD。KA2G 通过以下三种方法实现了稳定和数据效果的槽 filling：1. 将其视为文本生成任务。2. 在音频模式中进一步地固定文本生成。3. 使用可用的外部知识（如预定的槽值列表）进行条件。我们发现，在 KA2G 框架中结合两个模式可以提高对 ASR 错误的强度。此外，KA2G 中的知识感知槽值生成器，通过使用指针生成机制实现，尤其是在几 shot 和零 shot 学习中具有优势。我们在标准的speech-based single-turn SLURP 数据集和一个商业 ToD 系统提取的多turn 数据集上进行了实验，并表现出了强大和一致的提升，特别是在几 shot 和零 shot 设置下。
</details></li>
</ul>
<hr>
<h2 id="Align-With-Purpose-Optimize-Desired-Properties-in-CTC-Models-with-a-General-Plug-and-Play-Framework"><a href="#Align-With-Purpose-Optimize-Desired-Properties-in-CTC-Models-with-a-General-Plug-and-Play-Framework" class="headerlink" title="Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework"></a>Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01715">http://arxiv.org/abs/2307.01715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eliya Segev, Maya Alroy, Ronen Katsir, Noam Wies, Ayana Shenhav, Yael Ben-Oren, David Zar, Oren Tadmor, Jacob Bitterman, Amnon Shashua, Tal Rosenwein</li>
<li>for: 提高 Automatic Speech Recognition (ASR) 模型的某些特性，如采样时间和单词错误率 (WER)。</li>
<li>methods: 提出了一种通用的 Plug-and-Play 框架，可以补充 CTCP 损失函数，以便根据某些愿望的属性进行优化。</li>
<li>results: 在 ASR 领域中应用该框架，可以提高 emission time 的优化效果，最多提高 570ms，同时只有较少影响 WER 的准确率。此外，还可以提高 WER 的准确率，相比基eline模型，提高了4.5%。<details>
<summary>Abstract</summary>
Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect alignments. We apply our framework in the domain of Automatic Speech Recognition (ASR) and show its generality in terms of property selection, architectural choice, and scale of training dataset (up to 280,000 hours). To demonstrate the effectiveness of our framework, we apply it to two unrelated properties: emission time and word error rate (WER). For the former, we report an improvement of up to 570ms in latency optimization with a minor reduction in WER, and for the latter, we report a relative improvement of 4.5% WER over the baseline models. To the best of our knowledge, these applications have never been demonstrated to work on a scale of data as large as ours. Notably, our method can be implemented using only a few lines of code, and can be extended to other alignment-free loss functions and to domains other than ASR.
</details>
<details>
<summary>摘要</summary>
Connectionist Temporal Classification (CTC) 是一种广泛使用的训练监督序列到序列（seq2seq）模型的评价标准。它使得学习输入和输出序列之间的关系，称为对齐，通过排除完美对齐（导致真实的输出）的权重，以换取不完美对齐。这种二分法对于真实世界应用中的其他重要对齐特性不足。我们提出了《Align With Purpose》，一种通用的插件和替换框架，可以增强模型在 CTc 评价标准下的愿望特性。我们通过补充 CTc loss函数中的一个额外损失项来实现这一点，该项将对齐按照愿望特性进行优先级排序。我们的方法不需要对 CTc loss函数进行任何改变，可以轻松地优化多种特性，并允许对不完美对齐进行区分。我们在自动语音识别（ASR）领域应用了我们的框架，并在不同的特性、模型选择和训练数据集大小（最大达280,000小时）上进行了通用性测试。为证明我们的框架的有效性，我们在两种不相关的特性上应用了它：发射时间和单词错误率（WER）。对于前者，我们report了最多570ms的延迟优化和一定的WER降低，对于后者，我们report了相对于基eline模型的4.5% WER提升。到目前为止，这些应用都没有在这样大的数据集上进行过。值得注意的是，我们的方法只需要几行代码实现，并且可以扩展到其他对齐无法损失函数和领域。
</details></li>
</ul>
<hr>
<h2 id="Dipping-PLMs-Sauce-Bridging-Structure-and-Text-for-Effective-Knowledge-Graph-Completion-via-Conditional-Soft-Prompting"><a href="#Dipping-PLMs-Sauce-Bridging-Structure-and-Text-for-Effective-Knowledge-Graph-Completion-via-Conditional-Soft-Prompting" class="headerlink" title="Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting"></a>Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01709">http://arxiv.org/abs/2307.01709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenchens190009/csprom-kg">https://github.com/chenchens190009/csprom-kg</a></li>
<li>paper_authors: Chen Chen, Yufei Wang, Aixin Sun, Bing Li, Kwok-Yan Lam</li>
<li>for: 本研究的目的是提高知识图谱完成（KGC） task 的效果，通过维护知识图谱的结构信息和文本信息之间的平衡。</li>
<li>methods: 本研究提出了一种名为 CSProm-KG（Conditional Soft Prompts for KGC）的方法，它只是根据实体和关系表示生成的条件软提示参数进行调整。</li>
<li>results: 对三个常见的静态 KGC 测试集 WN18RR、FB15K-237 和 Wikidata5M 以及两个时间 KGC 测试集 ICEWS14 和 ICEWS05-15 进行测试，CSProm-KG 表现出色，超越了比较基eline模型。我们还进行了进一步的分析，以证明我们的提出的组件的有效性、CSProm-KG 的效率和其可变性。<details>
<summary>Abstract</summary>
Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural information and textual knowledge. CSProm-KG only tunes the parameters of Conditional Soft Prompts that are generated by the entities and relations representations. We verify the effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR, FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new state-of-the-art on these benchmarks. We conduct further analysis to show (i) the effectiveness of our proposed components, (ii) the efficiency of CSProm-KG, and (iii) the flexibility of CSProm-KG.
</details>
<details>
<summary>摘要</summary>
知识图结束 (KGC) 常常需要知识图结构和文本信息同时进行效果。先训练语言模型 (PLMs) 已经被用来学习文本信息，通常在细致调参 paradigm 中进行 KGC 任务。然而，细致调参 PLMs 经常偏重于文本信息，忽略知识图结构。为了解决这个问题，这篇论文提出了 CSProm-KG (Conditional Soft Prompts for KGC)，它保持了知识图结构和文本知识之间的平衡。CSProm-KG 只是调整基于实体和关系表示的 Conditional Soft Prompts 的参数。我们证明了 CSProm-KG 在三个流行的静态 KGC 标准测试集 WN18RR、FB15K-237 和 Wikidata5M 上表现出色，并在两个时间 KGC 标准测试集 ICEWS14 和 ICEWS05-15 上设置新的状态纪录。我们进一步分析表明（i）我们提posed的组件的效果，（ii）CSProm-KG 的效率，以及（iii）CSProm-KG 的灵活性。
</details></li>
</ul>
<hr>
<h2 id="Racial-Bias-Trends-in-the-Text-of-US-Legal-Opinions"><a href="#Racial-Bias-Trends-in-the-Text-of-US-Legal-Opinions" class="headerlink" title="Racial Bias Trends in the Text of US Legal Opinions"></a>Racial Bias Trends in the Text of US Legal Opinions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01693">http://arxiv.org/abs/2307.01693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Jinturkar</li>
<li>for: 这篇论文探讨了美国法律中的种族偏见，具体来说是法官的言论中是否存在种族偏见，以及这种偏见是否随时间和地区而变化。</li>
<li>methods: 作者使用了一种方法来测量大规模文本中的隐性种族偏见，并应用了这种方法来分析600万多个美国联邦和州法院案例文献从1860年到2009年。</li>
<li>results: 研究发现，美国法官的言论中存在强烈的种族偏见，传统的黑人名字更加与“不愉快”的词语相关，而传统的白人名字更加与“愉快”的词语相关。此外，研究还发现，在1950年之前的法律意见中没有发现更高的隐性种族偏见，nor did legal opinions from Northeastern states show greater change in racial bias over time compared to Southern states.<details>
<summary>Abstract</summary>
Although there is widespread recognition of racial bias in US law, it is unclear how such bias appears in the language of law, namely judicial opinions, and whether it varies across time period or region. Building upon approaches for measuring implicit racial bias in large-scale corpora, we approximate GloVe word embeddings for over 6 million US federal and state court cases from 1860 to 2009. We find strong evidence of racial bias across nearly all regions and time periods, as traditionally Black names are more closely associated with pre-classified "unpleasant" terms whereas traditionally White names are more closely associated with pre-classified "pleasant" terms. We also test whether legal opinions before 1950 exhibit more implicit racial bias than those after 1950, as well as whether opinions from Southern states exhibit less change in racial bias than those from Northeastern states. We do not find evidence of elevated bias in legal opinions before 1950, or evidence that legal opinions from Northeastern states show greater change in racial bias over time compared to Southern states. These results motivate further research into institutionalized racial bias.
</details>
<details>
<summary>摘要</summary>
尽管美国法律界存在普遍的种族偏见，但是未知如何在法律语言中表现出这种偏见，以及是否随时间或地区而变化。我们基于大规模文本潜在偏见测量方法，对1860年至2009年美国联邦和州法院案例600万起进行了 aproximate GloVe词嵌入。我们发现在大多数地区和时间期间，传统的黑人名字更加密切相关于预先分类的“不愉快” terms，而传统的白人名字更加密切相关于预先分类的“愉悦” terms。我们还测试了1950年之前的法律意见是否具有更高的潜在种族偏见，以及南部州法律意见是否在时间的推移中改变了种族偏见的变化。我们未能发现1950年之前的法律意见具有偏高的偏见，也未能发现南部州法律意见在时间的推移中改变种族偏见的变化。这些结果激励进一步研究 институциализи了种族偏见。
</details></li>
</ul>
<hr>
<h2 id="Robust-Hate-Speech-Detection-in-Social-Media-A-Cross-Dataset-Empirical-Evaluation"><a href="#Robust-Hate-Speech-Detection-in-Social-Media-A-Cross-Dataset-Empirical-Evaluation" class="headerlink" title="Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation"></a>Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01680">http://arxiv.org/abs/2307.01680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimosthenis Antypas, Jose Camacho-Collados</li>
<li>for: 这篇论文的目的是探讨在自然语言处理（NLP）领域中自动推测仇恨言论的研究。大多数前一 studies 都是基于社交媒体数据集，这些数据集的创建过程中含有自己的偏见，而模型从这些数据集偏见中学习。</li>
<li>methods: 在这篇论文中，我们使用了大规模的训练语言模型，并在不同的仇恨言论检测数据集上进行了精致的调整。我们还进行了许多数据集的比较，以探讨不同数据集在培训 hate speech detection 模型时的可行性。</li>
<li>results: 我们的实验结果显示，不同的数据集在培训 hate speech detection 模型时有所不同的可行性。其中，一些数据集更加普遍，可以在不同的背景下进行应用。此外，我们发现可以通过 комбінуing 不同的数据集来建立更加Robust的 hate speech detection 模型，这个 Robustness 甚至在控制data size 和比较最佳个别数据集时仍然保持。<details>
<summary>Abstract</summary>
The automatic detection of hate speech online is an active research area in NLP. Most of the studies to date are based on social media datasets that contribute to the creation of hate speech detection models trained on them. However, data creation processes contain their own biases, and models inherently learn from these dataset-specific biases. In this paper, we perform a large-scale cross-dataset comparison where we fine-tune language models on different hate speech detection datasets. This analysis shows how some datasets are more generalisable than others when used as training data. Crucially, our experiments show how combining hate speech detection datasets can contribute to the development of robust hate speech detection models. This robustness holds even when controlling by data size and compared with the best individual datasets.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）领域中自动发现仇恨言论在线是一个活跃的研究领域。大多数研究到目前为止都是基于社交媒体数据集，这些数据集在创建仇恨言论检测模型时提供了贡献。然而，数据创建过程中带有自己的偏见，模型从这些数据集特定的偏见中学习。在这篇论文中，我们进行了大规模的跨数据集比较，我们在不同的仇恨言论检测数据集上进行了精细的微调。这一分析表明了某些数据集在用于训练模型时更加通用，而且我们的实验显示，将多个仇恨言论检测数据集组合起来可以帮助建立更加鲁棒的仇恨言论检测模型。这种鲁棒性甚至在控制数据量和相比最佳单个数据集的情况下保持。
</details></li>
</ul>
<hr>
<h2 id="Disentanglement-in-a-GAN-for-Unconditional-Speech-Synthesis"><a href="#Disentanglement-in-a-GAN-for-Unconditional-Speech-Synthesis" class="headerlink" title="Disentanglement in a GAN for Unconditional Speech Synthesis"></a>Disentanglement in a GAN for Unconditional Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01673">http://arxiv.org/abs/2307.01673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rf5/simple-asgan">https://github.com/rf5/simple-asgan</a></li>
<li>paper_authors: Matthew Baas, Herman Kamper</li>
<li>for: 这个研究旨在开发一个可以直接从潜在空间生成真实语音的模型，不需要明确的条件。</li>
<li>methods: 这个模型基于StyleGAN家族的内生对称网络，可以将抽象的噪音映射到一个分离的潜在空间中，然后将这个潜在空间映射到一系列的语音特征，以消除干扰信号扩散。</li>
<li>results: 在使用Google Speech Commands数据集的小词库数据集上，ASGAN已经取得了顶尖的结果，并且比 existing 的扩散模型快得多。此外，我们还证明了 ASGAN 的潜在空间是分离的，可以使用Simple linear operations在这个空间中进行多个未见 durante 训练的任务。<details>
<summary>Abstract</summary>
Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis. It is also substantially faster than existing top-performing diffusion models. We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training. Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks. Code, models, samples: https://github.com/RF5/simple-asgan/
</details>
<details>
<summary>摘要</summary>
可以开发一个模型， direct from a latent space synthesize realistic speech without explicit conditioning?  despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space.  Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis. It is also substantially faster than existing top-performing diffusion models. We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training. Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks.Here's the translation in Traditional Chinese:可以开发一个模型， directly from a latent space synthesize realistic speech without explicit conditioning?  despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space.  Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis. It is also substantially faster than existing top-performing diffusion models. We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training. Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks.
</details></li>
</ul>
<hr>
<h2 id="Boosting-Norwegian-Automatic-Speech-Recognition"><a href="#Boosting-Norwegian-Automatic-Speech-Recognition" class="headerlink" title="Boosting Norwegian Automatic Speech Recognition"></a>Boosting Norwegian Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01672">http://arxiv.org/abs/2307.01672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier de la Rosa, Rolv-Arild Braaten, Per Egil Kummervold, Freddy Wetjen, Svein Arne Brygfjeld</li>
<li>for: 本研究为自动speech recognition（ASR）模型在挪威官方文字两种语言中提供了多个基线。</li>
<li>methods: 本研究使用不同大小和预训练方法的ASR模型在多个挪威语音dataset上进行了比较。同时，我们也测试了这些模型在之前的状态艺术模型和尘泥dataset上的性能。</li>
<li>results: 我们在挪威议会语音 corpus（NPSC）上从单词错误率（WER）17.10%下降至7.60%，模型在挪威语言中获得了5.81%的最佳状态。同时，我们还讨论了进一步改进ASR模型的挑战和解决方案。<details>
<summary>Abstract</summary>
In this paper, we present several baselines for automatic speech recognition (ASR) models for the two official written languages in Norway: Bokm{\aa}l and Nynorsk. We compare the performance of models of varying sizes and pre-training approaches on multiple Norwegian speech datasets. Additionally, we measure the performance of these models against previous state-of-the-art ASR models, as well as on out-of-domain datasets. We improve the state of the art on the Norwegian Parliamentary Speech Corpus (NPSC) from a word error rate (WER) of 17.10\% to 7.60\%, with models achieving 5.81\% for Bokm{\aa}l and 11.54\% for Nynorsk. We also discuss the challenges and potential solutions for further improving ASR models for Norwegian.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了多种基线模型 для自动语音识别（ASR）模型，用于两种官方文字语言在挪威：博克马尔和新北兰语。我们比较了不同大小和预训练方法的模型在多个挪威语音 dataset 上的性能。此外，我们还测试了这些模型与之前的状态对应 ASR 模型和不同语音集上的性能。我们在挪威国会语音集（NPSC）上提高了状态对应率从17.10% 降低至7.60%，其中模型为5.81%  для博克马尔和11.54%  для新北兰语。我们还讨论了进一步改进 ASR 模型的挑战和解决方案。
</details></li>
</ul>
<hr>
<h2 id="Unified-Conversational-Models-with-System-Initiated-Transitions-between-Chit-Chat-and-Task-Oriented-Dialogues"><a href="#Unified-Conversational-Models-with-System-Initiated-Transitions-between-Chit-Chat-and-Task-Oriented-Dialogues" class="headerlink" title="Unified Conversational Models with System-Initiated Transitions between Chit-Chat and Task-Oriented Dialogues"></a>Unified Conversational Models with System-Initiated Transitions between Chit-Chat and Task-Oriented Dialogues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01664">http://arxiv.org/abs/2307.01664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye Liu, Stefan Ultes, Wolfgang Minker, Wolfgang Maier</li>
<li>for: 这个论文的目的是研究在对话模型中实现功能目标和社交对话的联合模型，以及在对话模式之间发生转变时的启发机制。</li>
<li>methods: 这个论文使用了两种类型的对话场景，一种从社交对话逐渐转移到任务 Orientated 请求，另一种从任务 Orientated 交互开始，然后在所有请求信息都提供后转移到社交对话。作者还提出了两种有效的启发模型，一种是一个精确的批量模型，另一种是一个连续的启发模型使用自动生成的推荐embeddings。</li>
<li>results: 研究发现，连续启发模型可以在多个领域任务中实现更高的转换效果，并且可以用于指导对话模型在不同领域之间的批量转换。<details>
<summary>Abstract</summary>
Spoken dialogue systems (SDSs) have been separately developed under two different categories, task-oriented and chit-chat. The former focuses on achieving functional goals and the latter aims at creating engaging social conversations without special goals. Creating a unified conversational model that can engage in both chit-chat and task-oriented dialogue is a promising research topic in recent years. However, the potential ``initiative'' that occurs when there is a change between dialogue modes in one dialogue has rarely been explored. In this work, we investigate two kinds of dialogue scenarios, one starts from chit-chat implicitly involving task-related topics and finally switching to task-oriented requests; the other starts from task-oriented interaction and eventually changes to casual chat after all requested information is provided. We contribute two efficient prompt models which can proactively generate a transition sentence to trigger system-initiated transitions in a unified dialogue model. One is a discrete prompt model trained with two discrete tokens, the other one is a continuous prompt model using continuous prompt embeddings automatically generated by a classifier. We furthermore show that the continuous prompt model can also be used to guide the proactive transitions between particular domains in a multi-domain task-oriented setting.
</details>
<details>
<summary>摘要</summary>
干脆对话系统（SDS）已经分别开发出了两类：任务oriented和聊天。前者关注实现功能目标，而后者想创造有趣的社交对话没有特定目标。在最近几年中，创建一个综合对话模型可以在一个对话中同时进行聊天和任务oriented对话是一个有前途的研究话题。然而，在对话模式之间的变化中可能会发生的“发起”（initiative） rarely been explored。在这项工作中，我们研究了两种对话场景：一个从聊天逐渐涉及到任务相关话题，最后转换到任务oriented请求；另一个从任务oriented交互开始，最后变成了聊天。我们提出了两种高效的提示模型，可以触发系统自主发起对话模式的转换。一个是使用两个简单的Token进行训练的批示模型，另一个是使用自动生成的连续提示嵌入数据来 guideline 系统自主转换的连续提示模型。此外，我们还证明了连续提示模型可以在多个领域任务oriented Setting 中用于指导系统自主转换。
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Thought-Prompting-Elicits-Knowledge-Augmentation"><a href="#Chain-of-Thought-Prompting-Elicits-Knowledge-Augmentation" class="headerlink" title="Chain of Thought Prompting Elicits Knowledge Augmentation"></a>Chain of Thought Prompting Elicits Knowledge Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01640">http://arxiv.org/abs/2307.01640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruckbreasoning/cot-ka">https://github.com/ruckbreasoning/cot-ka</a></li>
<li>paper_authors: Dingjun Wu, Jing Zhang, Xinmei Huang</li>
<li>for: 这篇论文旨在提出一种基于链条思维（Chain-of-Thought，CoT）的知识增强深度学习方法（Knowledge-Augmented Deep Learning，KADL）。</li>
<li>methods: 这种方法使用大语言模型进行广泛预训练，然后将其作为外部知识集成到深度学习模型中。</li>
<li>results: 对于多种逻辑任务的 eleven 个公共数据集上，CoT-KA 方法比纯CoT方法和非增强方法表现出色，得到了更高的性能。<details>
<summary>Abstract</summary>
The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge. In this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments knowledge for deep learning. CoT-KA avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods. Our results demonstrate that CoT-KA outperforms both pure CoT-based methods and the non-augmented method across the majority of eleven publicly available benchmarks for various reasoning tasks.
</details>
<details>
<summary>摘要</summary>
知识增强深度学习方式指的是一种将领域知识集成到深度模型中的方法。传统方法通常采用任务特定的方法来从多种来源中收集外部知识。然而，大型语言模型已经广泛预训练，可以作为外部知识的全面来源。在这篇论文中，我们提出了基于链条思想的CoT-KA方法，用于增强深度学习。CoT-KA不需要额外的知识检索或知识推理模型，与传统增强方法不同。我们的结果表明，CoT-KA在多种公共可用的benchmark上比纯CoT方法和非增强方法表现出色，其中大多数任务的性能都高于非增强方法。
</details></li>
</ul>
<hr>
<h2 id="A-Language-Model-for-Grammatical-Error-Correction-in-L2-Russian"><a href="#A-Language-Model-for-Grammatical-Error-Correction-in-L2-Russian" class="headerlink" title="A Language Model for Grammatical Error Correction in L2 Russian"></a>A Language Model for Grammatical Error Correction in L2 Russian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01609">http://arxiv.org/abs/2307.01609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Remnev, Sergei Obiedkov, Ekaterina Rakhilina, Ivan Smirnov, Anastasia Vyrenkova</li>
<li>for:  correction of non-native (L2) writing errors in Russian language</li>
<li>methods: use of a language model trained on untagged texts of the Newspaper subcorpus of the Russian National Corpus</li>
<li>results: validation of the model’s quality against the RULEC-GEC corpusHere’s the full text in Simplified Chinese:</li>
<li>for:  correction of non-native (L2) 中文写作错误</li>
<li>methods: 使用基于新闻子集的俄语国家 corpus 上的无标文本语言模型</li>
<li>results:  validate 模型质量 against RULEC-GEC corpusI hope that helps!<details>
<summary>Abstract</summary>
Grammatical error correction is one of the fundamental tasks in Natural Language Processing. For the Russian language, most of the spellcheckers available correct typos and other simple errors with high accuracy, but often fail when faced with non-native (L2) writing, since the latter contains errors that are not typical for native speakers. In this paper, we propose a pipeline involving a language model intended for correcting errors in L2 Russian writing. The language model proposed is trained on untagged texts of the Newspaper subcorpus of the Russian National Corpus, and the quality of the model is validated against the RULEC-GEC corpus.
</details>
<details>
<summary>摘要</summary>
grammatical error correction是自然语言处理中的基本任务之一。对于俄语，大多数可用的拼写检查器能够准确地检测 typo和其他简单错误，但在面临非Native（L2）写作时，它们frequently failed，因为L2写作中含有不典型的错误。在这篇论文中，我们提议一个涉及语言模型的管道，用于 correecting L2俄语写作中的错误。我们的语言模型基于俄语日报子集（Newspaper subcorpus）上的未标注文本，并对RULEC-GEC corpus进行验证。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-the-Learning-Bias-towards-Repetition-by-Self-Contrastive-Training-for-Open-Ended-Generation"><a href="#Mitigating-the-Learning-Bias-towards-Repetition-by-Self-Contrastive-Training-for-Open-Ended-Generation" class="headerlink" title="Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation"></a>Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01542">http://arxiv.org/abs/2307.01542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-coai/selfcont">https://github.com/thu-coai/selfcont</a></li>
<li>paper_authors: Jian Guan, Minlie Huang</li>
<li>for: 提高自然语言生成 tasks 中的多样性，尤其是使用 GPT2 预训练语言模型进行开放式生成时，具有重复性的问题。</li>
<li>methods: 我们提出了一种自我对比训练方法，通过对同一模型的 premature checkpoint 的输出进行罚款，以避免重复性的过度估计。</li>
<li>results: 我们在两个 datasets 上进行了实验，发现这种方法可以有效地避免重复性，同时保持流畅性。此外，我们发现Language Models 在预测重复Token时使用更长的词语关系，可能是句子水平重复的原因。<details>
<summary>Abstract</summary>
Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops.
</details>
<details>
<summary>摘要</summary>
尽管在许多生成任务中进步很大，预训练语言模型（LM）如GPT2仍然很容易通过最大化基于解码算法来生成重复的文本。我们认为这是因为学习偏见：LM学习了简单的重复模式更快，使得它们在MLE损失函数下过度估计token级别的重复概率。我们提议使用自我对比训练来追加预训练模型的检查点，并在检查点不正确预测重复时进行惩罚，这有效地减少了重复，同时保持了流畅性在两个数据集上。此外，我们发现LM在预测重复token时使用了更长的距离，这可能是句子水平的重复循环的原因。
</details></li>
</ul>
<hr>
<h2 id="On-Evaluating-and-Mitigating-Gender-Biases-in-Multilingual-Settings"><a href="#On-Evaluating-and-Mitigating-Gender-Biases-in-Multilingual-Settings" class="headerlink" title="On Evaluating and Mitigating Gender Biases in Multilingual Settings"></a>On Evaluating and Mitigating Gender Biases in Multilingual Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01503">http://arxiv.org/abs/2307.01503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Vashishtha, Kabir Ahuja, Sunayana Sitaram</li>
<li>for: 本研究旨在 investigate the challenges of evaluating and mitigating biases in multilingual settings, especially for non-western context.</li>
<li>methods: 本研究使用 human annotations 创建了一个用于评估 gender biases 的指标，并将 existing debiasing methods 扩展到不同的印度语言。</li>
<li>results: 研究发现了 multilingual settings 中 studying social biases 的挑战，并提供了资源和 mitigation techniques 以逐步扩展到更多的语言。<details>
<summary>Abstract</summary>
While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.
</details>
<details>
<summary>摘要</summary>
tradicional，理解并消除语言模型中的性别偏见问题一直是自然语言处理领域的长期问题，但之前的研究主要集中在英语上。在这项工作中，我们探讨了在多语言设置中评估和消除偏见的挑战，以及由于英语以外的语言缺乏现有的偏见评估 benchmark和资源而导致的问题。在这篇论文中，我们首先创建了评估隐藏语言模型中的性别偏见的benchmark，通过对印度语言进行扩展DisCo以获得人工纠正。然后，我们扩展了不同的去偏见方法以工作在英语以外的语言上，并评估这些方法在我们提出的指标上的效果。总之，我们的工作揭示了在多语言设置中研究社会偏见的挑战，并提供了资源以及消除技术，以便扩展到更多的语言。
</details></li>
</ul>
<hr>
<h2 id="SCAT-Robust-Self-supervised-Contrastive-Learning-via-Adversarial-Training-for-Text-Classification"><a href="#SCAT-Robust-Self-supervised-Contrastive-Learning-via-Adversarial-Training-for-Text-Classification" class="headerlink" title="SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification"></a>SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01488">http://arxiv.org/abs/2307.01488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Wu, Dit-Yan Yeung</li>
<li>for: 防御文本攻击，提高自然语言处理（NLP）系统的鲁棒性。</li>
<li>methods: 自我标注对假数据进行随机修饰，并通过对这些修饰和其对应的对抗样本进行对比来实现对抗训练。</li>
<li>results: 可以很好地训练不含标签数据的语言模型，并且可以提高现有预训练语言模型的鲁棒性。<details>
<summary>Abstract</summary>
Despite their promising performance across various natural language processing (NLP) tasks, current NLP systems are vulnerable to textual adversarial attacks. To defend against these attacks, most existing methods apply adversarial training by incorporating adversarial examples. However, these methods have to rely on ground-truth labels to generate adversarial examples, rendering it impractical for large-scale model pre-training which is commonly used nowadays for NLP and many other tasks. In this paper, we propose a novel learning framework called SCAT (Self-supervised Contrastive Learning via Adversarial Training), which can learn robust representations without requiring labeled data. Specifically, SCAT modifies random augmentations of the data in a fully labelfree manner to generate adversarial examples. Adversarial training is achieved by minimizing the contrastive loss between the augmentations and their adversarial counterparts. We evaluate SCAT on two text classification datasets using two state-of-the-art attack schemes proposed recently. Our results show that SCAT can not only train robust language models from scratch, but it can also significantly improve the robustness of existing pre-trained language models. Moreover, to demonstrate its flexibility, we show that SCAT can also be combined with supervised adversarial training to further enhance model robustness.
</details>
<details>
<summary>摘要</summary>
尽管现有的自然语言处理（NLP）系统在不同的任务上表现出色，但它们对文本恶作剂攻击仍然易受到影响。为防止这些攻击，大多数现有的方法采用了对抗训练，但这些方法往往需要使用准确的标签来生成对抗示例，这使得大规模模型预训练成为不可能的。在这篇论文中，我们提出了一种新的学习框架，即SCAT（自主对抗学习），可以不需要标签数据来学习强化表示。具体来说，SCAT通过修改数据的随机扩展来生成对抗示例，然后通过对这些对抗示例和其对抗样本进行对抗训练来减少对抗攻击的影响。我们在两个文本分类任务上使用了两种最新的攻击方案进行评估。我们的结果显示，SCAT不仅可以从零开始训练Robust语言模型，而且还可以显著提高现有预训练语言模型的Robust性。此外，我们还证明了SCAT可以与有监督对抗训练结合使用，以进一步增强模型的Robust性。
</details></li>
</ul>
<hr>
<h2 id="CARE-MI-Chinese-Benchmark-for-Misinformation-Evaluation-in-Maternity-and-Infant-Care"><a href="#CARE-MI-Chinese-Benchmark-for-Misinformation-Evaluation-in-Maternity-and-Infant-Care" class="headerlink" title="CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care"></a>CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01458">http://arxiv.org/abs/2307.01458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meetyou-ai-lab/care-mi">https://github.com/meetyou-ai-lab/care-mi</a></li>
<li>paper_authors: Tong Xiang, Liangzhi Li, Wangyue Li, Mingbai Bai, Lu Wei, Bowen Wang, Noa Garcia<br>for: 这 paper 的目的是evaluating the misinformation generated by large language models (LLMs) in the sensitive topic of maternity and infant care, and providing a benchmark for assessing the quality of long-form generation in Chinese.methods: 该 paper 使用了一个新的 benchmark ， named CARE-MI, to evaluate the misinformation of LLMs in the maternity and infant care domain, and compared potential solutions for long-form generation evaluation.results: 该 paper 发现，current Chinese LLMs 在这个领域 still have a long way to go, and proposed a judgment model for automatically assessing the long-form output of LLMs using the benchmark questions.<details>
<summary>Abstract</summary>
The recent advances in NLP, have led to a new trend of applying LLMs to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building long-form generation evaluation benchmarks that can be transferred to other knowledge-intensive domains and low-resourced languages. Our proposed benchmark fills the gap between the extensive usage of LLMs and the lack of datasets for assessing the misinformation generated by these models. It contains 1,612 expert-checked questions, accompanied with human-selected references. Using our benchmark, we conduct extensive experiments and found that current Chinese LLMs are far from perfect in the topic of maternity and infant care. In an effort to minimize the reliance on human resources for performance evaluation, we offer a judgment model for automatically assessing the long-form output of LLMs using the benchmark questions. Moreover, we compare potential solutions for long-form generation evaluation and provide insights for building more robust and efficient automated metric.
</details>
<details>
<summary>摘要</summary>
近些年，自然语言处理（NLP）的进步，启动了应用大型自然语言模型（LLM）到实际场景的新趋势。latest LLMs在与人类交互时表现出很高的流畅性，但它们受到谎言问题的困扰，即不慎生成的false信息。这可能导致有害的后果，特别是在敏感场景中，如医疗领域。然而，前期工作很少关注了LLMs中的谎言评估，特别是在知识密集的领域和语言中。为了解决这问题，我们提出了一个benchmark，CARE-MI，用于评估LLMs中的谎言。CARE-MI包括以下两个方面：1）敏感领域，即婴儿护理领域；2）语言，即中文。我们还提供了一种创新的评估长形生成 benchmark的方法，可以转移到其他知识密集的领域和低资源语言。我们的提案填补了LLMs的广泛使用和评估谎言生成的数据差距。我们的benchmark包括1,612个专家审核的问题，以及人选的参考文献。使用我们的benchmark，我们进行了广泛的实验，发现当前的中文LLMs在婴儿护理领域还有很大的改进空间。为了减少人工资源的依赖，我们提供了一种自动评估长形输出的模型，以及对不同解决方案的比较。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Retrieval-Augmented-In-Context-Learning-for-Dialogue-State-Tracking"><a href="#Diverse-Retrieval-Augmented-In-Context-Learning-for-Dialogue-State-Tracking" class="headerlink" title="Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking"></a>Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01453">http://arxiv.org/abs/2307.01453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jlab-nlp/refpydst">https://github.com/jlab-nlp/refpydst</a></li>
<li>paper_authors: Brendan King, Jeffrey Flanigan</li>
<li>for: 提高对对话状态跟踪（DST）的表现，特别是在零和几个示例学习环境下。</li>
<li>methods: 提出了三种改进来提高在对话状态跟踪中的受Context learning，包括：将DST视为Python编程任务，明确表示语言核心引用在Python中；选择多个相关示例来提高性能；在解码阶段使用重新权重方法，考虑竞争表达形式的概率，生成更准确的对话状态预测。</li>
<li>results: 使用MultiWOZ进行评估，在零和几个示例学习环境下实现了多个多任务共同目标准确率的状态前几。<details>
<summary>Abstract</summary>
There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting (Hu et al. 2022). We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST. First, we formulate DST as a Python programming task, explicitly modeling language coreference as variable reference in Python. Second, since in-context learning depends highly on the context examples, we propose a method to retrieve a diverse set of relevant examples to improve performance. Finally, we introduce a novel re-weighting method during decoding that takes into account probabilities of competing surface forms, and produces a more accurate dialogue state prediction. We evaluate our approach using MultiWOZ and achieve state-of-the-art multi-domain joint-goal accuracy in zero and few-shot settings.
</details>
<details>
<summary>摘要</summary>
有很多人表达了对零和几个shot学习对话状态追踪（DST）的兴趣，这是因为收集和标注任务型对话的成本很高。现有研究表明，在Context中学习只需要很少数据和零参数更新，甚至在几个shot Setting下超越训练方法的性能（Hu et al. 2022）。我们提出了RefPyDST，这是一种在Context中学习DST的新方法，它具有以下三个进步：1. 我们将DST视为一种Python编程任务，直接在Python中表示语言核心语言引用。2. 由于Context学习强烈取决于上下文示例，我们提议一种方法来检索更多相关的示例，以提高性能。3. 我们提出了一种新的重新权重方法，在解码过程中考虑竞争表面形式的概率，并生成更准确的对话状态预测。我们使用MultiWOZ进行评估，并在零和几个shot Setting下实现了多个领域共同目标准确率的状态前景。
</details></li>
</ul>
<hr>
<h2 id="ReactIE-Enhancing-Chemical-Reaction-Extraction-with-Weak-Supervision"><a href="#ReactIE-Enhancing-Chemical-Reaction-Extraction-with-Weak-Supervision" class="headerlink" title="ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision"></a>ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01448">http://arxiv.org/abs/2307.01448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Zhong, Siru Ouyang, Minhao Jiang, Vivian Hu, Yizhu Jiao, Xuan Wang, Jiawei Han</li>
<li>for: 本研究旨在提供一种用于提取结构化反应信息的方法，以便化学家在实验室工作和计算机辅助药物设计等高级任务中使用。</li>
<li>methods: 本研究使用了两种弱监督方法进行预训练，利用文本中频繁出现的语言特征来识别化学反应的特征。同时，我们采用了专利记录中的 sintetic 数据作为远程监督，以把领域知识integrated到模型中。</li>
<li>results: 实验表明， ReactIE 方法可以达到显著提高，并超过所有基eline。<details>
<summary>Abstract</summary>
Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design. Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive due to the significant labor required from domain experts. Consequently, the scarcity of sufficient training data poses an obstacle to the progress of related models in this domain. In this paper, we propose ReactIE, which combines two weakly supervised approaches for pre-training. Our method utilizes frequent patterns within the text as linguistic cues to identify specific characteristics of chemical reactions. Additionally, we adopt synthetic data from patent records as distant supervision to incorporate domain knowledge into the model. Experiments demonstrate that ReactIE achieves substantial improvements and outperforms all existing baselines.
</details>
<details>
<summary>摘要</summary>
科学文献中的结构化化学反应信息对化学家进行实验室工作和高级尝试（如计算机支持药物设计）起着重要作用。然而，提取结构化反应的数据标注因为需要域专家劳动量大，因此成本高昂。这导致相关模型在这个领域进步受阻。在这篇论文中，我们提议了ReactIE，它组合了两种弱监督方法进行预训练。我们的方法利用文本中的频繁出现的语言特征作为化学反应的特征标志。此外，我们采用了专利记录中的 sintetic data作为远程监督，以把领域知识引入模型中。实验表明，ReactIE可以实现显著改进，并超过所有基elines。
</details></li>
</ul>
<hr>
<h2 id="On-Conditional-and-Compositional-Language-Model-Differentiable-Prompting"><a href="#On-Conditional-and-Compositional-Language-Model-Differentiable-Prompting" class="headerlink" title="On Conditional and Compositional Language Model Differentiable Prompting"></a>On Conditional and Compositional Language Model Differentiable Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01446">http://arxiv.org/abs/2307.01446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jpilaul/PRopS">https://github.com/jpilaul/PRopS</a></li>
<li>paper_authors: Jonathan Pilault, Can Liu, Mohit Bansal, Markus Dreyer</li>
<li>for: 本研究旨在提高预训练语言模型（PLM）的下游任务性能，通过各种提示方法来适应不同任务。</li>
<li>methods: 本研究使用了 conditional和compositional的可微分提示方法，并提出了一种新的模型——Prompt Production System（PRopS），可以将任务说明或输入元数据转化为Continuous提示，以便从PLM中获取特定任务输出。PRopS使用了基于神经网络的Production Systems模型结构，可以学习到特定提示输入模式的精细规则，从而实现compositional transfer learning和少量学习。</li>
<li>results: 对比其他PLM适应技术，PRopS在compositional generalization任务、可控摘要和多语言翻译等任务中具有优异表现，需要更少的可训练参数。<details>
<summary>Abstract</summary>
Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine-tuned models, on compositional generalization tasks, controllable summarization and multilingual translation, while needing fewer trainable parameters.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文本提示（Prompts）已经被证明是一种有效的方法，用于适应预训练语言模型（PLM）来实现下游任务的好准确性。文本提示可以表示为人工设计的单词序列或学习到的连续嵌入。在这项工作中，我们研究了强制和组合的可微分提示。我们提出了一个新的模型，即提示生产系统（PRopS），该模型可以将任务指令或输入元数据转换为可微分的提示，从而使PLM发生任务特定的输出。我们的模型采用基于我们的神经网络表述的生产系统结构，该结构允许模型学习分解规则——神经函数学习特定提示输入模式的转换，使其适用于组合转移学习和少量学习。我们对PRopS进行了广泛的实验和理论分析，并证明了其在组合总结任务、可控概要和多语言翻译方面的表现，常常超过其他PLM适应技术，而且经常超过完全精度地训练的模型。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Modeling-Tag-Prediction-based-on-Question-Tagging-Behavior-Analysis-of-CommunityQA-Platform-Users"><a href="#Modeling-Tag-Prediction-based-on-Question-Tagging-Behavior-Analysis-of-CommunityQA-Platform-Users" class="headerlink" title="Modeling Tag Prediction based on Question Tagging Behavior Analysis of CommunityQA Platform Users"></a>Modeling Tag Prediction based on Question Tagging Behavior Analysis of CommunityQA Platform Users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01420">http://arxiv.org/abs/2307.01420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuntal Kumar Pal, Michael Gamon, Nirupama Chandrasekaran, Silviu Cucerzan</li>
<li>for: 提高社区问答平台信息组织和检索效果，更快准确地回答问题，评估话题Popularity。</li>
<li>methods: 对17个StackExchange社区用户标签行为进行了系统性分析，发现了不同领域的共同特性。采用发现结果开发flexible的神经网络标签预测模型，可预测问题的流行标签和更加细化的标签。</li>
<li>results: 经过广泛的实验和性能评估，证明了模型的有效性。<details>
<summary>Abstract</summary>
In community question-answering platforms, tags play essential roles in effective information organization and retrieval, better question routing, faster response to questions, and assessment of topic popularity. Hence, automatic assistance for predicting and suggesting tags for posts is of high utility to users of such platforms. To develop better tag prediction across diverse communities and domains, we performed a thorough analysis of users' tagging behavior in 17 StackExchange communities. We found various common inherent properties of this behavior in those diverse domains. We used the findings to develop a flexible neural tag prediction architecture, which predicts both popular tags and more granular tags for each question. Our extensive experiments and obtained performance show the effectiveness of our model
</details>
<details>
<summary>摘要</summary>
在社区问答平台上，标签扮演着关键的角色，即信息组织和检索、更好的问题路由、更快的问题回答以及评估话题 популярность。因此，自动为帖子提供标签预测和建议是用户们的高Utility功能。为了在多个社区和领域中提高标签预测，我们进行了17个Stack Exchange社区用户标签行为的严格分析。我们发现了这些多样化领域中标签行为的共同特性。我们使用这些发现来开发一种灵活的神经网络标签预测架构，可以预测每个问题的流行标签以及更加细化的标签。我们的广泛的实验和表现表明我们的模型的效果。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Learning-Improves-Performance-In-Deep-Argument-Mining-Models"><a href="#Multi-Task-Learning-Improves-Performance-In-Deep-Argument-Mining-Models" class="headerlink" title="Multi-Task Learning Improves Performance In Deep Argument Mining Models"></a>Multi-Task Learning Improves Performance In Deep Argument Mining Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01401">http://arxiv.org/abs/2307.01401</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Amirhossein Farzam, Shashank Shekhar, Isaac Mehlhaff, Marco Morucci</li>
<li>for: 本研究的目的是提高对用户生成文本中的论证技巧的分析，以便进行政治和市场分析等下游任务。</li>
<li>methods: 本研究使用了现代深度学习方法，包括多任务学习，以提取和注释用户生成文本中的论证技巧。</li>
<li>results: 研究表明，不同的论证检测任务共享相似的semantic和logical结构，并且可以通过共享表示和 Parametern sharing 来提高性能。<details>
<summary>Abstract</summary>
The successful analysis of argumentative techniques from user-generated text is central to many downstream tasks such as political and market analysis. Recent argument mining tools use state-of-the-art deep learning methods to extract and annotate argumentative techniques from various online text corpora, however each task is treated as separate and different bespoke models are fine-tuned for each dataset. We show that different argument mining tasks share common semantic and logical structure by implementing a multi-task approach to argument mining that achieves better performance than state-of-the-art methods for the same problems. Our model builds a shared representation of the input text that is common to all tasks and exploits similarities between tasks in order to further boost performance via parameter-sharing. Our results are important for argument mining as they show that different tasks share substantial similarities and suggest a holistic approach to the extraction of argumentative techniques from text.
</details>
<details>
<summary>摘要</summary>
成功分析口说技巧是许多下游任务的核心，如政治和市场分析。现有的口说采矿工具使用 cutting-edge 深度学习方法提取和标注口说技巧，但每个任务都是专门训练不同的模型。我们显示出不同的口说采矿任务有共同的semantic和logical结构，通过实现多任务方法来采矿口说，可以更好地提高性能。我们的模型建立了输入文本共同的表示，并利用任务之间的相似性以进一步提高性能。我们的结果对口说采矿有重要意义，表明不同任务之间有许多相似之处，并建议一个整体的方法来从文本中提取口说技巧。
</details></li>
</ul>
<hr>
<h2 id="ALBERTI-a-Multilingual-Domain-Specific-Language-Model-for-Poetry-Analysis"><a href="#ALBERTI-a-Multilingual-Domain-Specific-Language-Model-for-Poetry-Analysis" class="headerlink" title="ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis"></a>ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01387">http://arxiv.org/abs/2307.01387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier de la Rosa, Álvaro Pérez Pozo, Salvador Ros, Elena González-Blanco</li>
<li>for: This paper is written for the analysis of poetry in a multilingual setting, specifically to address the lack of tools for automatically analyzing and scanning poems.</li>
<li>methods: The paper presents a new approach called \textsc{Alberti}, which is a multilingual pre-trained large language model for poetry. The model is trained using domain-specific pre-training (DSP) on a corpus of over 12 million verses from 12 languages.</li>
<li>results: The paper reports that \textsc{Alberti} outperforms multilingual BERT and other transformers-based models of similar sizes on two structural poetry tasks: Spanish stanza type classification and metrical pattern prediction for Spanish, English, and German. Additionally, \textsc{Alberti} achieves state-of-the-art results for German when compared to rule-based systems.<details>
<summary>Abstract</summary>
The computational analysis of poetry is limited by the scarcity of tools to automatically analyze and scan poems. In a multilingual settings, the problem is exacerbated as scansion and rhyme systems only exist for individual languages, making comparative studies very challenging and time consuming. In this work, we present \textsc{Alberti}, the first multilingual pre-trained large language model for poetry. Through domain-specific pre-training (DSP), we further trained multilingual BERT on a corpus of over 12 million verses from 12 languages. We evaluated its performance on two structural poetry tasks: Spanish stanza type classification, and metrical pattern prediction for Spanish, English and German. In both cases, \textsc{Alberti} outperforms multilingual BERT and other transformers-based models of similar sizes, and even achieves state-of-the-art results for German when compared to rule-based systems, demonstrating the feasibility and effectiveness of DSP in the poetry domain.
</details>
<details>
<summary>摘要</summary>
计算 poetry 的分析受到计算 poetry 工具的缺乏的限制。在多语言设置下，问题更加严重，因为押韵和律诗系统只存在于个别语言中，这使得比较研究非常困难和耗时。在这种工作中，我们介绍了 \textsc{Alberti}，首个用于 poetry 的多语言预训练大语言模型。通过领域特定预训练（DSP），我们进一步训练了多语言 BERT 在12种语言的超过12万句诗歌中进行预训练。我们对其表现进行评估，并在西班牙押韵类型分类和德语、英语和西班牙的 мет律 Pattern 预测任务上达到了比较好的结果，并且在对比rule-based系统的 germany 语言中达到了国际一流的结果，这表明了 DSP 在 poetry 领域的可能性和有效性。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Memory-Transformer-for-Computationally-Efficient-Simultaneous-Speech-Translation"><a href="#Implicit-Memory-Transformer-for-Computationally-Efficient-Simultaneous-Speech-Translation" class="headerlink" title="Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation"></a>Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01381">http://arxiv.org/abs/2307.01381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-starlab/implicitmemory">https://github.com/osu-starlab/implicitmemory</a></li>
<li>paper_authors: Matthew Raffel, Lizhong Chen</li>
<li>for: 这个论文的目的是提出一种新的做法来处理同时的语音翻译任务，使得翻译器可以同时接受输入语音序列，并且不需要耗费过多计算资源。</li>
<li>methods: 这个论文使用了一种新的Left Context方法，通过将上一个segment的注意力输出作为下一个segment的左 контекст来实现。这种方法可以减少计算资源的消耗，同时也可以保持模型的准确性。</li>
<li>results: 实验结果表明，使用这种Left Context方法可以在encoder前进行加速，而且与使用左 context和内存银行的方法相比，翻译质量几乎相同。<details>
<summary>Abstract</summary>
Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedup on the encoder forward pass with nearly identical translation quality when compared with the state-of-the-art approach that employs both left context and memory banks.
</details>
<details>
<summary>摘要</summary>
同时语音翻译是一项人类交流困难的沟通任务，即在流动输入语音时生成翻译。为此流处理任务，使用块处理的转换器已经实现了状态体系的最佳性能，并降低计算成本。现有的方法，包括左上下文和内存银行，尝试使信息在段之间传递，但是这些方法都是不充分的表示和过分的计算成本。在这篇论文中，我们提出了隐式记忆转换器，通过新的左上下文方法，消除了需要显式表示内存的需求。我们从上一个段的注意输出中生成左上下文，并将其包含在当前段的注意计算中的键和值中。对于 Must-C 数据集的实验结果表明，隐式记忆转换器在编码前进行速度增加，与使用左上下文和内存银行的状态体系相比，翻译质量几乎完全一致。
</details></li>
</ul>
<hr>
<h2 id="Shiftable-Context-Addressing-Training-Inference-Context-Mismatch-in-Simultaneous-Speech-Translation"><a href="#Shiftable-Context-Addressing-Training-Inference-Context-Mismatch-in-Simultaneous-Speech-Translation" class="headerlink" title="Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation"></a>Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01377">http://arxiv.org/abs/2307.01377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-starlab/shiftablecontext">https://github.com/osu-starlab/shiftablecontext</a></li>
<li>paper_authors: Matthew Raffel, Drew Penney, Lizhong Chen</li>
<li>for: 提高同时翻译的精度，解决 segment-based 处理模型在训练和推理环境中的上下文匹配问题。</li>
<li>methods: 提出了Shiftable Context scheme，通过保证训练和推理环境中segment和上下文大小的一致性，提高同时翻译的精度。</li>
<li>results: 在英语-德语、英语-法语和英语-西班牙语语对上，对Augmented Memory Transformer模型进行Shiftable Context修改后，提高了等待k值的BLEU分数平均值2.09、1.83和1.95个数值，而 computation-aware Average Lagging的影响很小。<details>
<summary>Abstract</summary>
Transformer models using segment-based processing have been an effective architecture for simultaneous speech translation. However, such models create a context mismatch between training and inference environments, hindering potential translation accuracy. We solve this issue by proposing Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. Shiftable Context is also broadly applicable to segment-based transformers for streaming tasks. Our experiments on the English-German, English-French, and English-Spanish language pairs from the MUST-C dataset demonstrate that when applied to the Augmented Memory Transformer, a state-of-the-art model for simultaneous speech translation, the proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value for the three language pairs, respectively, with a minimal impact on computation-aware Average Lagging.
</details>
<details>
<summary>摘要</summary>
使用分割基于的处理模型已经是同时交互翻译的有效架构。然而，这些模型会在训练和推理环境中创建上下文匹配问题，从而影响翻译准确性。我们解决这个问题 by proposing Shiftable Context，一种简单 yet effective的方案，确保在训练和推理过程中保持一致的分割和上下文大小，即使有部分填充的分割段 due to the streaming nature of simultaneous translation。Shiftable Context 还可以广泛应用于流处理任务中的 segment-based transformers。我们在英语-德语、英语-法语和英语-西班牙语语对的 MUST-C 数据集上进行了实验，并发现当应用到 Augmented Memory Transformer，一种现有的同时speech翻译模型时，提议的方案可以在每个 wait-k 值上得到平均提高2.09、1.83和1.95 的 BLEU 分数，并且对 computation-aware Average Lagging 产生了最小的影响。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Language-Models-are-not-Multicultural-A-Case-Study-in-Emotion"><a href="#Multilingual-Language-Models-are-not-Multicultural-A-Case-Study-in-Emotion" class="headerlink" title="Multilingual Language Models are not Multicultural: A Case Study in Emotion"></a>Multilingual Language Models are not Multicultural: A Case Study in Emotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01370">http://arxiv.org/abs/2307.01370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shreyahavaldar/multicultural_emotion">https://github.com/shreyahavaldar/multicultural_emotion</a></li>
<li>paper_authors: Shreya Havaldar, Sunny Rai, Bhumika Singhal, Langchen Liu, Sharath Chandra Guntuku, Lyle Ungar</li>
<li>for:  investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages</li>
<li>methods:  use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, and investigate the Anglocentricity of embeddings obtained from LMs and the Western norms reflected in generative LMs</li>
<li>results:  multilingual LMs do not successfully learn the culturally appropriate nuances of emotion, and possible research directions towards correcting this are highlighted<details>
<summary>Abstract</summary>
Emotions are experienced and expressed differently across the world. In order to use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, LMs must reflect this cultural variation in emotion. In this study, we investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages. We find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.
</details>
<details>
<summary>摘要</summary>
情感表达在不同的文化中存在差异。为了使用大语言模型（LM）进行多语言任务需要情感敏感，LM必须反映这种文化差异。本研究发现，2023年广泛使用的多语言LM（例如XLM-RoBERTa）的嵌入是英语中心，生成LM（例如ChatGPT）even responding to prompts in other languages ​​still reflect Western norms. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion, and we highlight possible research directions towards correcting this.Note that the translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Semantic-enrichment-towards-efficient-speech-representations"><a href="#Semantic-enrichment-towards-efficient-speech-representations" class="headerlink" title="Semantic enrichment towards efficient speech representations"></a>Semantic enrichment towards efficient speech representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01323">http://arxiv.org/abs/2307.01323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaëlle Laperrière, Ha Nguyen, Sahar Ghannay, Bassam Jabaian, Yannick Estève</li>
<li>for: 本研究旨在提高 spoken language understanding 任务中的 semantic extraction，并且考虑 computation costs。</li>
<li>methods: 本研究使用 SAMU-XLSR 模型，通过特点域 semantic enrichment 来增强 multilingual speech representation。</li>
<li>results: 研究发现，特点域 semantic enrichment 可以提高 spoken language understanding 任务中的 semantic extraction，同时还可以提高 low-resource language 的 portability。<details>
<summary>Abstract</summary>
Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.
</details>
<details>
<summary>摘要</summary>
过去几年，自我超级学习的语音表示方法在解决口语语言理解（SLU）任务上出现了丰盈的替代方案。同时，基于巨量文本数据的多种语言模型被引入，以将语言不可知变Semantics编码。最近，SAMU-XLSR方法引入了将投资 language agnostic semantics的方法，以增强多ilingual speech表示。本研究的目的是通过对特定领域的Semantic抽象来提高SAMU-XLSR模型的SLU能力，并考虑计算成本。此外，我们还展示了对 French和Italian benchmarks的同domain使用可以提高低资源语言的可移植性，并探索了增强SAMU-XLSR的跨领域能力。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Spoken-Named-Entity-Recognition-A-Cross-Lingual-Perspective"><a href="#Exploring-Spoken-Named-Entity-Recognition-A-Cross-Lingual-Perspective" class="headerlink" title="Exploring Spoken Named Entity Recognition: A Cross-Lingual Perspective"></a>Exploring Spoken Named Entity Recognition: A Cross-Lingual Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01310">http://arxiv.org/abs/2307.01310</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moncefbenaicha/spokenner">https://github.com/moncefbenaicha/spokenner</a></li>
<li>paper_authors: Moncef Benaicha, David Thulke, M. A. Tuğtekin Turan</li>
<li>for: 研究 spoken named entity recognition (NER) 的进展，以便更好地从文本数据中识别实体。</li>
<li>methods: 使用 transferred learning 技术，从荷兰语、英语和德语三种语言进行了跨语言交流学习。使用 Wav2Vec2-XLS-R 模型，并在自定义 Pseudo-annotated 数据集上进行了训练。</li>
<li>results: 结果表明，使用 End-to-End 方式的 spoken NER 比 pipeline 方式的系统表现更好，特别是从德语到荷兰语的转移学习表现出色，超过了荷兰 E2E 系统7%，超过了荷兰 pipeline 系统4%。这项研究不仅证明了跨语言转移学习在 spoken NER 中的可行性，还提示了未来的评估中需要更多的数据收集，以提高结果。<details>
<summary>Abstract</summary>
Recent advancements in Named Entity Recognition (NER) have significantly improved the identification of entities in textual data. However, spoken NER, a specialized field of spoken document retrieval, lags behind due to its limited research and scarce datasets. Moreover, cross-lingual transfer learning in spoken NER has remained unexplored. This paper utilizes transfer learning across Dutch, English, and German using pipeline and End-to-End (E2E) schemes. We employ Wav2Vec2-XLS-R models on custom pseudo-annotated datasets and investigate several architectures for the adaptability of cross-lingual systems. Our results demonstrate that End-to-End spoken NER outperforms pipeline-based alternatives over our limited annotations. Notably, transfer learning from German to Dutch surpasses the Dutch E2E system by 7% and the Dutch pipeline system by 4%. This study not only underscores the feasibility of transfer learning in spoken NER but also sets promising outcomes for future evaluations, hinting at the need for comprehensive data collection to augment the results.
</details>
<details>
<summary>摘要</summary>
近期的Named Entity Recognition（NER）技术发展有所进步，有效地识别文本数据中的实体。然而，口语NER，是特殊的口语文检 Retrieval 领域，由于研究的限制和数据的缺乏，落后于NER。此外，口语NER的语言交互转移学习还未得到探索。这篇论文利用了语言交互转移学习 across Dutch, English, and German，使用管道和End-to-End（E2E）方案。我们使用Wav2Vec2-XLS-R模型在自定义pseudo-annotated dataset上进行了训练，并 investigate了多种架构以便适应跨语言系统的适应性。我们的结果表明，End-to-End口语NER比管道方式更高效，并且跨语言转移学习从德语到荷兰语的表现比荷兰E2E系统高出7%，并高过荷兰管道系统4%。这篇研究不仅证明了口语NER中的转移学习的可能性，还提供了未来评估中的优秀结果，强调了需要大量数据收集以增强结果。
</details></li>
</ul>
<hr>
<h2 id="The-Evolution-of-Substance-Use-Coverage-in-the-Philadelphia-Inquirer"><a href="#The-Evolution-of-Substance-Use-Coverage-in-the-Philadelphia-Inquirer" class="headerlink" title="The Evolution of Substance Use Coverage in the Philadelphia Inquirer"></a>The Evolution of Substance Use Coverage in the Philadelphia Inquirer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01299">http://arxiv.org/abs/2307.01299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Layla Bouzoubaa, Ramtin Ehsani, Preetha Chatterjee, Rezvaneh Rezapour</li>
<li>for: 本研究旨在探讨媒体对非法药物使用的报道和讨论是如何发展和变化，以及这些报道对公众对毒瘾的观感、政策和公共健康OUTCOMES有什么影响。</li>
<li>methods: 本研究使用了157,476篇洛杉矶时报文章，时间跨度为10年，并从这些文章中选择了3,903篇文章，其中每篇文章至少提到一种通常被滥用的药物。</li>
<li>results: 研究发现，大麻和鸦片是报道的最多的药物类型，而幻觉药物则被更加正面地报道。相比之下，鸦片被报道的最为负面。这项研究的目的是强调媒体对毒瘾和药物使用的报道应该准确、包容，以便减少对毒瘾人士的刻板印象和恐慌。<details>
<summary>Abstract</summary>
The media's representation of illicit substance use can lead to harmful stereotypes and stigmatization for individuals struggling with addiction, ultimately influencing public perception, policy, and public health outcomes. To explore how the discourse and coverage of illicit drug use changed over time, this study analyzes 157,476 articles published in the Philadelphia Inquirer over a decade. Specifically, the study focuses on articles that mentioned at least one commonly abused substance, resulting in a sample of 3,903 articles. Our analysis shows that cannabis and narcotics are the most frequently discussed classes of drugs. Hallucinogenic drugs are portrayed more positively than other categories, whereas narcotics are portrayed the most negatively. Our research aims to highlight the need for accurate and inclusive portrayals of substance use and addiction in the media.
</details>
<details>
<summary>摘要</summary>
媒体对非法药物使用的表达可能会导致有害的 sterotype 和偏见，影响公众对添iction的看法，政策和公共健康 outcome。为了探讨媒体对非法药物使用的话语和报道如何变化过时，这项研究分析了费城纪事报上的157,476篇文章，时间段为10年。研究选择了提及常用药物的文章，共3,903篇。我们的分析表明，大麻和毒品是最常讨论的药物类型。幻觉药物在其他类型中被更正面地描述，而毒品则被最为负面地描述。我们的研究旨在强调媒体对药物使用和添iction的精准和包容的报道是必要的。
</details></li>
</ul>
<hr>
<h2 id="Trainable-Transformer-in-Transformer"><a href="#Trainable-Transformer-in-Transformer" class="headerlink" title="Trainable Transformer in Transformer"></a>Trainable Transformer in Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01189">http://arxiv.org/abs/2307.01189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhishekpanigrahi1996/transformer_in_transformer">https://github.com/abhishekpanigrahi1996/transformer_in_transformer</a></li>
<li>paper_authors: Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora</li>
<li>for: 这篇论文是为了探讨如何使用大型预训练语言模型进行在线学习（ICL），并在推理过程中 simulate 和 fine-tune 内部模型（例如 linear 或 2-layer MLP）。</li>
<li>methods: 该论文提出了一种高效的构建方法，即 Transformer in Transformer（简称 TinT），允许 transformer 模型在推理过程中 simulate 和 fine-tune 复杂的模型（例如预训练语言模型）。该方法使用了创新的近似技术，使得 TinT 模型只需要 fewer than 2 billion parameters 可以 simulate 和 fine-tune 125 million parameter transformer 模型。</li>
<li>results: 该论文通过进行综合的 end-to-end 实验 validate 了 TinT 模型的内部细化过程，并发现在不同的语言模型和下游任务上，TinT 模型可以提高性能 by 4-16% 绝对值。这些发现表明大型预训练语言模型可以执行复杂的子任务。<details>
<summary>Abstract</summary>
Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.
</details>
<details>
<summary>摘要</summary>
最近的研究归功启发式学习（ICL）在大型预训练语言模型中的能力，是因为这些模型在推理过程中隐式地模拟和精细调整内部模型（例如线性或2层MLP）。然而，这些构造需要大量内存负担，使得更复杂的内部模型的模拟变得不可行。在这项工作中，我们提出了高效的构造方案——Transformer in Transformer（简称TinT），允许 transformer 模型在推理过程中内部模拟和精细调整复杂模型（例如预训练语言模型）。具体来说，我们提出了创新的近似技术，使得 TinT 模型 fewer than 2 billion parameters 可以在单个前进 passes 中模拟和精细调整 125 million parameter transformer 模型。TinT 支持许多常见的 transformer 变体，并且其设计思想还改进了过去简单模型在 transformers 中的效率。我们通过综合实验 validate 内部精细调整过程的有效性，并发现 TinT 对于不同语言模型和下游任务的性能都有明显提升。例如，即使只有一步预算，我们发现 TinT 对于 OPT-125M 模型可以提高性能的平均差值为 4-16%。这些发现表明大型预训练语言模型可以执行复杂的子过程。为了促进进一步研究，我们附加了可重用和扩展的代码库。
</details></li>
</ul>
<hr>
<h2 id="Improving-Language-Plasticity-via-Pretraining-with-Active-Forgetting"><a href="#Improving-Language-Plasticity-via-Pretraining-with-Active-Forgetting" class="headerlink" title="Improving Language Plasticity via Pretraining with Active Forgetting"></a>Improving Language Plasticity via Pretraining with Active Forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01163">http://arxiv.org/abs/2307.01163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Chen, Kelly Marchisio, Roberta Raileanu, David Ifeoluwa Adelani, Pontus Stenetorp, Sebastian Riedel, Mikel Artetxe</li>
<li>for: 这篇论文的目的是提出一种简单的活动忘记机制，以便使PLMs可以快速适应新语言。</li>
<li>methods: 该论文使用了一种活动忘记机制，在预训练过程中每K更新一次 embedding layer，以便让PLMs能够快速学习新的 embedding。</li>
<li>results: 实验表明，使用该忘记机制可以使PLMs在语言适应过程中更快 converges，并且在具有少量数据的情况下，特别是与英语远程的语言，能够表现出更好的性能。<details>
<summary>Abstract</summary>
Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, particularly for languages that are distant from English.
</details>
<details>
<summary>摘要</summary>
现代自然语言处理（NLP）中的预训练语言模型（PLM）已成为主流模型。尽管它们在下游任务中表现出色，但是将PLM应用于新语言可能会困难，这可能限制了它们的 universality 性。先前的工作已经证明可以通过学习一个新的映射层来解决这个问题，但是这需要大量的数据和计算资源。我们提议使用活动忘记机制 durante la pretraining，以便快速地使PLM适应新语言。具体来说，在每个更新中重置 embedding layer，我们鼓励PLM在有限的更新数量内快速学习新的映射，类似于一种元学习效应。我们使用 RoBERTa 进行实验，并证明了在语言适应过程中使用我们的忘记机制可以更快地 converges，并且在数据量较少的情况下，特别是与英语较为 distant 的语言，模型的表现更出色。
</details></li>
</ul>
<hr>
<h2 id="Translating-Latin-with-Artificial-Intelligence"><a href="#Translating-Latin-with-Artificial-Intelligence" class="headerlink" title="Translating Latin with Artificial Intelligence"></a>Translating Latin with Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07520">http://arxiv.org/abs/2307.07520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sylvio R. Bistafa</li>
<li>for: 该研究旨在透过人工智能翻译技术，解决早期科学文献的可用性问题，特别是李奥纳·欧拉的作品。</li>
<li>methods: 本研究使用了两种流行的人工智能翻译算法，即Google Translate和ChatGPT，进行比较性测试，以验证它们的表现。</li>
<li>results: 测试结果表明，ChatGPT在翻译李奥纳·欧拉的1739年信件中表现出色，提供了优秀的翻译结果，这表明了ChatGPT可以作为一个有价值的翻译工具，不仅对普通的拉丁文献专家有帮助，还对特殊的拉丁文献翻译家有利。<details>
<summary>Abstract</summary>
The major hindrance in the study of earlier scientific literature is the availability of Latin translations into modern languages. This is particular true for the works of Euler who authored about 850 manuscripts and wrote a thousand letters and received back almost two thousand more. The translation of many of these manuscripts, books and letters have been published in various sources over the last two centuries, but many more have not yet appeared. Fortunately, nowadays, the artificial intelligence AI translation can be used to circumvent the challenges of translating such substantial number of texts. To validate this tool, benchmark tests have been performed to compare the performance of two popular AI translating algorithms, namely Google Translate and ChatGPT. Since it was found that ChatGPT performed better on these tests, this translating support was then used on an excerpt of a 1739 letter from Johann Bernoulli to Euler, where he notifies that he was sending to Euler the first part of his manuscript Hydraulica. The findings highlight ChatGPT as a valuable translation tool, catering not only to general Latin practitioners but also proving beneficial for specialized Latin translators.
</details>
<details>
<summary>摘要</summary>
主要阻碍古科学文献研究的问题是现代语言中的拉丁文翻译的可用性。这 particualrly true for Euler 的作品，他撰写了约850份手稿和写了1000封信件，收到了 almost 2000封回信。许多这些手稿、书籍和信件的翻译已经在过去两个世纪出版，但还有很多没有出现。幸运的是，现在可以使用人工智能 AI 翻译工具来绕过这些文献的翻译挑战。为验证这个工具，我们进行了比较两个流行的 AI 翻译算法的 benchMark 测试，结果发现 ChatGPT 的表现更好，因此选择使用这个翻译支持。在一篇1739年的Euler 写给 Bernoulli 的信件中，Bernoulli 通知他将发送给 Euler 的第一部分的液体学 manuscript。这些发现 highlight ChatGPT 作为一个有价值的翻译工具，不仅有利于一般拉丁文翻译者，还有利于专业拉丁文翻译者。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/04/cs.CL_2023_07_04/" data-id="clp88dbsb007tob883dv54dj9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/cs.LG_2023_07_04/" class="article-date">
  <time datetime="2023-07-04T10:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/cs.LG_2023_07_04/">cs.LG - 2023-07-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GHOST-A-Graph-Neural-Network-Accelerator-using-Silicon-Photonics"><a href="#GHOST-A-Graph-Neural-Network-Accelerator-using-Silicon-Photonics" class="headerlink" title="GHOST: A Graph Neural Network Accelerator using Silicon Photonics"></a>GHOST: A Graph Neural Network Accelerator using Silicon Photonics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01782">http://arxiv.org/abs/2307.01782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salma Afifi, Febin Sunny, Amin Shafiee, Mahdi Nikdast, Sudeep Pasricha</li>
<li>for: 这篇论文的目的是为了提出一种基于光学频谱的干扰器硬件加速器，用于加速图 neuron 网络（GNNs）的运算。</li>
<li>methods: 这篇论文使用了光学频谱技术，实现了图 neuron 网络的三个主要阶段（邻居更新、 message passing 和更新），并且可以用于多种广泛使用的 GNN 模型和架构，如图 convolution 网络和图注意力网络。</li>
<li>results: 根据 simulations 研究，GHOST 相比 GPU、TPU、CPU 和多种现有 GNN 硬件加速器，能够提供至少 10.2 倍的吞吐量和 3.8 倍的能效率。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have emerged as a powerful approach for modelling and learning from graph-structured data. Multiple fields have since benefitted enormously from the capabilities of GNNs, such as recommendation systems, social network analysis, drug discovery, and robotics. However, accelerating and efficiently processing GNNs require a unique approach that goes beyond conventional artificial neural network accelerators, due to the substantial computational and memory requirements of GNNs. The slowdown of scaling in CMOS platforms also motivates a search for alternative implementation substrates. In this paper, we present GHOST, the first silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates the costs associated with both vertex-centric and edge-centric operations. It implements separately the three main stages involved in running GNNs in the optical domain, allowing it to be used for the inference of various widely used GNN models and architectures, such as graph convolution networks and graph attention networks. Our simulation studies indicate that GHOST exhibits at least 10.2x better throughput and 3.8x better energy efficiency when compared to GPU, TPU, CPU and multiple state-of-the-art GNN hardware accelerators.
</details>
<details>
<summary>摘要</summary>
граф нейрон сети (GNNs) 已成为图Structured data的 мощful approached for modeling and learning. 多个领域受益于 GNNs 的能力, such as recommendation systems, social network analysis, drug discovery, and robotics. 然而，加速和有效地处理 GNNs 需要特殊的approach，以 beyond conventional artificial neural network accelerators, due to the substantial computational and memory requirements of GNNs. CMOS 平台的慢速下降也驱动了寻找代替实现SUBSTRATES. 在这篇论文中，我们提出了 GHOST, the first silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates the costs associated with both vertex-centric and edge-centric operations. It implements separately the three main stages involved in running GNNs in the optical domain, allowing it to be used for the inference of various widely used GNN models and architectures, such as graph convolution networks and graph attention networks. Our simulation studies indicate that GHOST exhibits at least 10.2x better throughput and 3.8x better energy efficiency when compared to GPU, TPU, CPU, and multiple state-of-the-art GNN hardware accelerators.
</details></li>
</ul>
<hr>
<h2 id="FedHIL-Heterogeneity-Resilient-Federated-Learning-for-Robust-Indoor-Localization-with-Mobile-Devices"><a href="#FedHIL-Heterogeneity-Resilient-Federated-Learning-for-Robust-Indoor-Localization-with-Mobile-Devices" class="headerlink" title="FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices"></a>FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01780">http://arxiv.org/abs/2307.01780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danish Gufran, Sudeep Pasricha</li>
<li>for: 本研究旨在提高设备不同、indoor环境多样化的情况下的indoor定位精度，同时保护用户数据隐私。</li>
<li>methods: 本研究提出了一种基于联合学习（Federated Learning，FL）和indoor定位的嵌入式机器学习框架（FedHIL），通过选择性调整量来维护ML模型的性能，并在不同设备和环境中实现高精度indoor定位。</li>
<li>results: 实验表明，FedHIL在多种不同的indoor环境和设备上都能够实现1.62倍的定位精度提高，较前期工作的最佳FL-based indoor定位框架的1.35倍。<details>
<summary>Abstract</summary>
Indoor localization plays a vital role in applications such as emergency response, warehouse management, and augmented reality experiences. By deploying machine learning (ML) based indoor localization frameworks on their mobile devices, users can localize themselves in a variety of indoor and subterranean environments. However, achieving accurate indoor localization can be challenging due to heterogeneity in the hardware and software stacks of mobile devices, which can result in inconsistent and inaccurate location estimates. Traditional ML models also heavily rely on initial training data, making them vulnerable to degradation in performance with dynamic changes across indoor environments. To address the challenges due to device heterogeneity and lack of adaptivity, we propose a novel embedded ML framework called FedHIL. Our framework combines indoor localization and federated learning (FL) to improve indoor localization accuracy in device-heterogeneous environments while also preserving user data privacy. FedHIL integrates a domain-specific selective weight adjustment approach to preserve the ML model's performance for indoor localization during FL, even in the presence of extremely noisy data. Experimental evaluations in diverse real-world indoor environments and with heterogeneous mobile devices show that FedHIL outperforms state-of-the-art FL and non-FL indoor localization frameworks. FedHIL is able to achieve 1.62x better localization accuracy on average than the best performing FL-based indoor localization framework from prior work.
</details>
<details>
<summary>摘要</summary>
室内定位在应用程序中扮演着重要的角色，如应急应对、仓库管理和增强现实体验。通过在移动设备上部署机器学习（ML）基于的室内定位框架，用户可以在各种室内和地下环境中自动地标定自己的位置。然而，实现准确的室内定位可以是困难的，因为移动设备的硬件和软件栈的差异会导致不一致和不准确的位置估计。传统的ML模型也具有依赖于初始训练数据的问题，从而使其在室内环境中表现出现很大的变化和衰退。为解决设备不一致和数据变化导致的挑战，我们提出了一种新的嵌入式ML框架called FedHIL。FedHIL将室内定位和联邦学习（FL）结合起来，以提高设备不一致环境中的室内定位精度，同时也保护用户数据隐私。FedHIL使用域特定的选择性加重方法来保持ML模型在室内定位中的表现，即使面临非常噪音的数据时也能够保持高性能。实验证明，FedHIL在多个真实世界室内环境和不同的移动设备上表现出色，与传统的FL和非FL室内定位框架相比，具有1.62倍的本地化精度。
</details></li>
</ul>
<hr>
<h2 id="Shapley-Sets-Feature-Attribution-via-Recursive-Function-Decomposition"><a href="#Shapley-Sets-Feature-Attribution-via-Recursive-Function-Decomposition" class="headerlink" title="Shapley Sets: Feature Attribution via Recursive Function Decomposition"></a>Shapley Sets: Feature Attribution via Recursive Function Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01777">http://arxiv.org/abs/2307.01777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Torty Sivill, Peter Flach</li>
<li>for: 本研究旨在替代Feature Value Attribution中常用但可能受特征相互作用的Shapley值，提出一种新的归属方法——Shapley Set。</li>
<li>methods: 本研究使用了一种归属函数分解算法，将模型分解成不可分割变量组，并具有对数 linear 复杂度。</li>
<li>results: 研究表明，Shapley Set具有与Shapley值相同的公正性观念，并且可以避免基于Shapley值的归属方法中出现的坑。此外，Shapley Set在数据类型具有复杂依赖关系时表现 particullary 优异。<details>
<summary>Abstract</summary>
Despite their ubiquitous use, Shapley value feature attributions can be misleading due to feature interaction in both model and data. We propose an alternative attribution approach, Shapley Sets, which awards value to sets of features. Shapley Sets decomposes the underlying model into non-separable variable groups using a recursive function decomposition algorithm with log linear complexity in the number of variables. Shapley Sets attributes to each non-separable variable group their combined value for a particular prediction. We show that Shapley Sets is equivalent to the Shapley value over the transformed feature set and thus benefits from the same axioms of fairness. Shapley Sets is value function agnostic and we show theoretically and experimentally how Shapley Sets avoids pitfalls associated with Shapley value based alternatives and are particularly advantageous for data types with complex dependency structure.
</details>
<details>
<summary>摘要</summary>
尽管Shapley值特征归功通用，但它们可能导致特征互动的启示，both model和数据级。我们提出了一种替代方案，即Shapley集，该奖励集合特征。Shapley集使用一种分解函数分解算法，将基础模型分解为不可分割变量组。对每个不可分割变量组，Shapley集归功其组合值 для特定预测。我们证明了Shapley集等于在转换特征集上的Shapley值，因此受到同样的公平原则保证。Shapley集是值函数无关的，我们 theoretically和实验表明，Shapley集可以避免基于Shapley值的代替方法中的坑害，特别是数据类型具有复杂依赖结构。
</details></li>
</ul>
<hr>
<h2 id="Fast-Optimal-Transport-through-Sliced-Wasserstein-Generalized-Geodesics"><a href="#Fast-Optimal-Transport-through-Sliced-Wasserstein-Generalized-Geodesics" class="headerlink" title="Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics"></a>Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01770">http://arxiv.org/abs/2307.01770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Mahey, Laetitia Chapel, Gilles Gasso, Clément Bonet, Nicolas Courty</li>
<li>for: 本 paper 描述了一种新的 Wasserstein 距离代理（min-SWGG），该代理基于输送地图，并与 Wasserstein 泛化 геodesics 相关。</li>
<li>methods: 本 paper 使用了一种新的 Computational Scheme，可以使用 gradient descent 优化。此外，paper 还提供了一种关于 Wasserstein 距离的closed form解，并证明了 min-SWGG 是 Wasserstein 距离的上界，并且与 Sliced-Wasserstein 相似，但具有更多的特性。</li>
<li>results: 本 paper 通过 empirical evidences 支持 min-SWGG 在各种应用中的 beneficial 效果，包括梯度流、形状匹配和图像颜色化等。<details>
<summary>Abstract</summary>
Wasserstein distance (WD) and the associated optimal transport plan have been proven useful in many applications where probability measures are at stake. In this paper, we propose a new proxy of the squared WD, coined min-SWGG, that is based on the transport map induced by an optimal one-dimensional projection of the two input distributions. We draw connections between min-SWGG and Wasserstein generalized geodesics in which the pivot measure is supported on a line. We notably provide a new closed form for the exact Wasserstein distance in the particular case of one of the distributions supported on a line allowing us to derive a fast computational scheme that is amenable to gradient descent optimization. We show that min-SWGG is an upper bound of WD and that it has a complexity similar to as Sliced-Wasserstein, with the additional feature of providing an associated transport plan. We also investigate some theoretical properties such as metricity, weak convergence, computational and topological properties. Empirical evidences support the benefits of min-SWGG in various contexts, from gradient flows, shape matching and image colorization, among others.
</details>
<details>
<summary>摘要</summary>
瓦asserstein距离（WD）和相关的最优运输计划在probability measures中显示了有用性。在这篇论文中，我们提议一个新的proxy，称为min-SWGG，它基于两个输入分布的运输地图，它是通过一个优化的一维投影来定义的。我们将min-SWGG与通用水stein化曲线的关系进行连接，并在特定情况下提供一个新的准确 Wasserstein距离的closed form，使得可以使用梯度下降优化。我们证明min-SWGG是WD的上界，并且它的复杂性与Sliced-Wasserstein相似，但它具有提供相关运输计划的特点。我们还研究了一些理论性质，如metricity、weak convergence、computational和topological性。empirical evidence表明min-SWGG在各种场景中具有各种优点，从梯度流、形态匹配到图像颜色化等。
</details></li>
</ul>
<hr>
<h2 id="Localized-Data-Work-as-a-Precondition-for-Data-Centric-ML-A-Case-Study-of-Full-Lifecycle-Crop-Disease-Identification-in-Ghana"><a href="#Localized-Data-Work-as-a-Precondition-for-Data-Centric-ML-A-Case-Study-of-Full-Lifecycle-Crop-Disease-Identification-in-Ghana" class="headerlink" title="Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana"></a>Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01767">http://arxiv.org/abs/2307.01767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darlington Akogo, Issah Samori, Cyril Akafia, Harriet Fiagbor, Andrews Kangah, Donald Kwame Asiedu, Kwabena Fuachie, Luis Oala</li>
<li>for: 论文旨在演示如何通过团队合作和数据工程来提高农业产量和食品安全。</li>
<li>methods: 论文使用了无人机采集的数据和机器学习算法来确定作物压力。</li>
<li>results: 研究实现了一个基于地 desktop 应用程序的本地化数据驱动解决方案，以提高农业生产力和食品安全。<details>
<summary>Abstract</summary>
The Ghana Cashew Disease Identification with Artificial Intelligence (CADI AI) project demonstrates the importance of sound data work as a precondition for the delivery of useful, localized datacentric solutions for public good tasks such as agricultural productivity and food security. Drone collected data and machine learning are utilized to determine crop stressors. Data, model and the final app are developed jointly and made available to local farmers via a desktop application.
</details>
<details>
<summary>摘要</summary>
《加纳杏仁疾病识别用人工智能项目（CADI AI）》显示了数据工作的重要性，作为当地数据驱动解决方案的前提。该项目使用无人机收集数据和机器学习来确定作物压力。数据、模型和最终应用程序均由本地农民通过桌面应用程序获得。
</details></li>
</ul>
<hr>
<h2 id="Pretraining-is-All-You-Need-A-Multi-Atlas-Enhanced-Transformer-Framework-for-Autism-Spectrum-Disorder-Classification"><a href="#Pretraining-is-All-You-Need-A-Multi-Atlas-Enhanced-Transformer-Framework-for-Autism-Spectrum-Disorder-Classification" class="headerlink" title="Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification"></a>Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01759">http://arxiv.org/abs/2307.01759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lugges991/metaformer">https://github.com/lugges991/metaformer</a></li>
<li>paper_authors: Lucas Mahler, Qi Wang, Julius Steiglechner, Florian Birk, Samuel Heczko, Klaus Scheffler, Gabriele Lohmann</li>
<li>For:  This paper proposes a novel framework for ASD classification using resting-state functional magnetic resonance imaging data.* Methods:  The proposed framework, called METAFormer, utilizes a multi-atlas approach and self-supervised pretraining to improve classification performance.* Results:  The proposed framework achieves state-of-the-art performance on the ABIDE I dataset, with an average accuracy of 83.7% and an AUC-score of 0.832.Here is the same information in Simplified Chinese text:* For: 这个论文提出了一种基于Resting-state功能磁共振成像数据的ASD分类方法。* Methods: 提议的方法是METAFormer，它使用多个图像的方法和自我批示训练来提高分类性能。* Results: 提议的方法在ABIDE I dataset上达到了状态之arte的性能，具体来说是83.7%的平均精度和0.832的AUC分数。<details>
<summary>Abstract</summary>
Autism spectrum disorder (ASD) is a prevalent psychiatric condition characterized by atypical cognitive, emotional, and social patterns. Timely and accurate diagnosis is crucial for effective interventions and improved outcomes in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced Transformer framework, METAFormer, ASD classification. Our framework utilizes resting-state functional magnetic resonance imaging data from the ABIDE I dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer employs a multi-atlas approach, where flattened connectivity matrices from the AAL, CC200, and DOS160 atlases serve as input to the transformer encoder. Notably, we demonstrate that self-supervised pretraining, involving the reconstruction of masked values from the input, significantly enhances classification performance without the need for additional or separate training data. Through stratified cross-validation, we evaluate the proposed framework and show that it surpasses state-of-the-art performance on the ABIDE I dataset, with an average accuracy of 83.7% and an AUC-score of 0.832. The code for our framework is available at https://github.com/Lugges991/METAFormer
</details>
<details>
<summary>摘要</summary>
“自闭症 спектルム病（ASD）是一种常见的心理疾病，具有异常的认知、情感和社交模式。及时和准确的诊断非常重要，以便为患有ASD的个体提供有效的 intervención和改善结果。在这项研究中，我们提出了一种新的多 Atlas 增强变换框架，METAFormer，用于ASD分类。我们的框架使用了ABIDE I 数据集中的406名ASD和476名 Typical control（TC）个体的休息态功能磁共振成像数据。METAFormer 使用多Atlas方法，其中扁平连接矩阵从AAL、CC200和DOS160 的图像服务器为变换器编码器的输入。我们表明，不需要额外或分离的训练数据，通过自我超vision的预训练，即将掩码的值重建为输入的masked 值，可以明显提高分类性能。通过 stratified 树目录验证，我们评估了提议的框架，并发现其在ABIDE I 数据集上的平均准确率为83.7%，AUC 分数为0.832。 code for our framework is available at https://github.com/Lugges991/METAFormer。”
</details></li>
</ul>
<hr>
<h2 id="Local-primordial-non-Gaussianity-from-the-large-scale-clustering-of-photometric-DESI-luminous-red-galaxies"><a href="#Local-primordial-non-Gaussianity-from-the-large-scale-clustering-of-photometric-DESI-luminous-red-galaxies" class="headerlink" title="Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies"></a>Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01753">http://arxiv.org/abs/2307.01753</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mehdirezaie/dimagfnl">https://github.com/mehdirezaie/dimagfnl</a></li>
<li>paper_authors: Mehdi Rezaie, Ashley J. Ross, Hee-Jong Seo, Hui Kong, Anna Porredon, Lado Samushia, Edmond Chaussidon, Alex Krolewski, Arnaud de Mattia, Florian Beutler, Jessica Nicole Aguilar, Steven Ahlen, Shadab Alam, Santiago Avila, Benedict Bahr-Kalus, Jose Bermejo-Climent, David Brooks, Todd Claybaugh, Shaun Cole, Kyle Dawson, Axel de la Macorra, Peter Doel, Andreu Font-Ribera, Jaime E. Forero-Romero, Satya Gontcho A Gontcho, Julien Guy, Klaus Honscheid, Theodore Kisner, Martin Landriau, Michael Levi, Marc Manera, Aaron Meisner, Ramon Miquel, Eva-Maria Mueller, Adam Myers, Jeffrey A. Newman, Jundan Nie, Nathalie Palanque-Delabrouille, Will Percival, Claire Poppett, Graziano Rossi, Eusebio Sanchez, Michael Schubnell, Gregory Tarlé, Benjamin Alan Weaver, Christophe Yèche, Zhimin Zhou, Hu Zou</li>
<li>For: The paper aims to constrain the local primordial non-Gaussianity parameter fNL using angular clustering of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) imaging surveys.* Methods: The paper uses linear regression and artificial neural networks to alleviate non-cosmological excess clustering on large scales, and tests the methods against log-normal simulations with and without fNL and systematics.* Results: The paper finds fNL $&#x3D; 47^{+14(+29)}_{-11(-22)}$ at 68%(95%) confidence, with a maximum likelihood value of fNL $\sim 50$ and increased uncertainty when including a full set of imaging maps. The results indicate fNL &gt; 0 with a 99.9 percent confidence level, which could be attributed to unforeseen systematics or a scale-dependent fNL model.<details>
<summary>Abstract</summary>
We use angular clustering of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) imaging surveys to constrain the local primordial non-Gaussianity parameter fNL. Our sample comprises over 12 million targets, covering 14,000 square degrees of the sky, with redshifts in the range 0.2< z < 1.35. We identify Galactic extinction, survey depth, and astronomical seeing as the primary sources of systematic error, and employ linear regression and artificial neural networks to alleviate non-cosmological excess clustering on large scales. Our methods are tested against log-normal simulations with and without fNL and systematics, showing superior performance of the neural network treatment in reducing remaining systematics. Assuming the universality relation, we find fNL $= 47^{+14(+29)}_{-11(-22)}$ at 68\%(95\%) confidence. With a more aggressive treatment, including regression against the full set of imaging maps, our maximum likelihood value shifts slightly to fNL$ \sim 50$ and the uncertainty on fNL increases due to the removal of large-scale clustering information. We apply a series of robustness tests (e.g., cuts on imaging, declination, or scales used) that show consistency in the obtained constraints. Despite extensive efforts to mitigate systematics, our measurements indicate fNL > 0 with a 99.9 percent confidence level. This outcome raises concerns as it could be attributed to unforeseen systematics, including calibration errors or uncertainties associated with low-\ell systematics in the extinction template. Alternatively, it could suggest a scale-dependent fNL model--causing significant non-Gaussianity around large-scale structure while leaving cosmic microwave background scales unaffected. Our results encourage further studies of fNL with DESI spectroscopic samples, where the inclusion of 3D clustering modes should help separate imaging systematics.
</details>
<details>
<summary>摘要</summary>
我们使用 DESI 图像观测的 Angular 卷积方法来约束本地原始非加性参数 fNL。我们的样本包括超过 12 百万目标，覆盖 14,000平方度天空，红shift 在 0.2 < z < 1.35 之间。我们认为 galactic 遮盖、观测深度和天文望远镜为主要系统性错误来源，并使用线性回归和人工神经网络来缓减非 cosmological 过卷 clustering。我们的方法在 log-normal  simulations 中与和 без fNL 和系统atic 进行测试，显示人工神经网络处理的superior performance 在减少剩下系统atic。assuming  универса性关系，我们得到 fNL = 47 ± 14 ± 29 的确idence Interval。通过对全aset of imaging maps进行回归，我们的最大似然值shift 到 fNL ≈ 50，并且因为移除大规模 clustering 信息而增加了 fNL 的不确定度。我们进行了一系列Robustness 测试（例如，对 imaging、 declination 或 scale 进行cut），发现结果是一致的。despite extensive efforts to mitigate systematics，我们的测量结果表明 fNL > 0 的99.9% 信任水平。这些结果可能被归因于未知系统atic，包括折合错误或低-\ell 系统atic 在 extinction 模板中的不确定度。 Alternatively，这些结果可能表明 scale-dependent fNL 模型，导致在大规模结构上显著的非 Gaussianity，而不影响cosmic microwave background 观测。我们的结果鼓励 DESI 光谱样本进一步研究 fNL，其中包括3D clustering modes，可以帮助分离图像系统atic。
</details></li>
</ul>
<hr>
<h2 id="SRCD-Semantic-Reasoning-with-Compound-Domains-for-Single-Domain-Generalized-Object-Detection"><a href="#SRCD-Semantic-Reasoning-with-Compound-Domains-for-Single-Domain-Generalized-Object-Detection" class="headerlink" title="SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection"></a>SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01750">http://arxiv.org/abs/2307.01750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhijie Rao, Jingcai Guo, Luyao Tang, Yue Huang, Xinghao Ding, Song Guo</li>
<li>for: 这个论文提出了一个新的单域泛化物体检测框架（即Single-DGOD），旨在学习和维护自增强采样的 semantic 结构，以提高模型的泛化能力。</li>
<li>methods: 论文提出了两个主要组件： texture-based self-augmentation (TBSA) 模块和 local-global semantic reasoning (LGSR) 模块。 TBSA 模块用于消除图像水平上的不相关属性，如光影、颜色等，而 LGSR 模块用于进一步模型实例层次的 semantic 关系，以帮助维护内在的 semantic 结构。</li>
<li>results: 对多个benchmark进行了广泛的实验，证明了提出的 SRCD 的效果。<details>
<summary>Abstract</summary>
This paper provides a novel framework for single-domain generalized object detection (i.e., Single-DGOD), where we are interested in learning and maintaining the semantic structures of self-augmented compound cross-domain samples to enhance the model's generalization ability. Different from DGOD trained on multiple source domains, Single-DGOD is far more challenging to generalize well to multiple target domains with only one single source domain. Existing methods mostly adopt a similar treatment from DGOD to learn domain-invariant features by decoupling or compressing the semantic space. However, there may have two potential limitations: 1) pseudo attribute-label correlation, due to extremely scarce single-domain data; and 2) the semantic structural information is usually ignored, i.e., we found the affinities of instance-level semantic relations in samples are crucial to model generalization. In this paper, we introduce Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. Specifically, our SRCD contains two main components, namely, the texture-based self-augmentation (TBSA) module, and the local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the effects of irrelevant attributes associated with labels, such as light, shadow, color, etc., at the image level by a light-yet-efficient self-augmentation. Moreover, LGSR is used to further model the semantic relationships on instance features to uncover and maintain the intrinsic semantic structures. Extensive experiments on multiple benchmarks demonstrate the effectiveness of the proposed SRCD.
</details>
<details>
<summary>摘要</summary>
To address these limitations, this paper introduces Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. SRCD consists of two main components: the texture-based self-augmentation (TBSA) module and the local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the effects of irrelevant attributes associated with labels, such as light, shadow, color, etc., at the image level by using a light-yet-efficient self-augmentation. LGSR is used to further model the semantic relationships on instance features to uncover and maintain the intrinsic semantic structures.Experiments on multiple benchmarks demonstrate the effectiveness of the proposed SRCD. The main contributions of this paper are:1. A novel framework for Single-DGOD, which learns and maintains the semantic structures of self-augmented compound cross-domain samples.2. A new module called TBSA, which eliminates the effects of irrelevant attributes associated with labels at the image level.3. A module called LGSR, which models the semantic relationships on instance features to uncover and maintain the intrinsic semantic structures.Overall, this paper presents a more effective and efficient approach to Single-DGOD, which can improve the generalization ability of object detection models in real-world applications.
</details></li>
</ul>
<hr>
<h2 id="RRCNN-A-novel-signal-decomposition-approach-based-on-recurrent-residue-convolutional-neural-network"><a href="#RRCNN-A-novel-signal-decomposition-approach-based-on-recurrent-residue-convolutional-neural-network" class="headerlink" title="RRCNN: A novel signal decomposition approach based on recurrent residue convolutional neural network"></a>RRCNN: A novel signal decomposition approach based on recurrent residue convolutional neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01725">http://arxiv.org/abs/2307.01725</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhoudafa08/rrcnn">https://github.com/zhoudafa08/rrcnn</a></li>
<li>paper_authors: Feng Zhou, Antonio Cicone, Haomin Zhou</li>
<li>for: 这种研究的目的是为了开发一种基于深度学习的非站立信号分解方法，以提高现有方法的缺点，如边界和模式混合问题和噪声Robustness。</li>
<li>methods: 该方法使用了卷积神经网络、径向结构和非线性活动函数来计算信号的本地平均值，并在深度学习框架下实现了新的非站立信号分解方法。</li>
<li>results: 实验表明，提案的方法可以更好地处理边界问题、模式混合问题、噪声Robustness和分解结果的正交性，并且在计算本地平均值和信号分解两个方面都有更高的性能。<details>
<summary>Abstract</summary>
The decomposition of non-stationary signals is an important and challenging task in the field of signal time-frequency analysis. In the recent two decades, many signal decomposition methods led by the empirical mode decomposition, which was pioneered by Huang et al. in 1998, have been proposed by different research groups. However, they still have some limitations. For example, they are generally prone to boundary and mode mixing effects and are not very robust to noise. Inspired by the successful applications of deep learning in fields like image processing and natural language processing, and given the lack in the literature of works in which deep learning techniques are used directly to decompose non-stationary signals into simple oscillatory components, we use the convolutional neural network, residual structure and nonlinear activation function to compute in an innovative way the local average of the signal, and study a new non-stationary signal decomposition method under the framework of deep learning. We discuss the training process of the proposed model and study the convergence analysis of the learning algorithm. In the experiments, we evaluate the performance of the proposed model from two points of view: the calculation of the local average and the signal decomposition. Furthermore, we study the mode mixing, noise interference, and orthogonality properties of the decomposed components produced by the proposed method. All results show that the proposed model allows for better handling boundary effect, mode mixing effect, robustness, and the orthogonality of the decomposed components than existing methods.
</details>
<details>
<summary>摘要</summary>
非站点信号的分解是信号时频分析领域中的一个重要和挑战性任务。过去二十年，许多基于实验模式分解的信号分解方法已经被不同的研究组织提出。然而，它们仍有一些限制，例如容易受边缘和模式混合效应的影响，并不够鲁棒对噪声。受图像处理和自然语言处理等领域的深度学习成功应用启发，我们使用卷积神经网络、循环结构和非线性活化函数计算非站点信号的本地均值，并研究了一种基于深度学习框架的新的非站点信号分解方法。我们讨论了该模型的训练过程和学习算法的整合分析。在实验中，我们评估了提案模型的性能从两个角度：计算本地均值和信号分解。此外，我们还研究了分解后的模式混合、噪声抑制和正交性特性。所有结果都表明，提案的模型可以更好地处理边缘效应、模式混合效应、鲁棒性和分解后的正交性。
</details></li>
</ul>
<hr>
<h2 id="MOPO-LSI-A-User-Guide"><a href="#MOPO-LSI-A-User-Guide" class="headerlink" title="MOPO-LSI: A User Guide"></a>MOPO-LSI: A User Guide</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01719">http://arxiv.org/abs/2307.01719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Zheng, Kumar Neelotpal Shukla, Jasmine Xu, David, Wang, Michael O’Leary</li>
<li>for: 这份论文是为了提供一个开源的多目标投资套件库，用于实现可持续投资。</li>
<li>methods: 该论文使用了多目标优化算法来解决投资问题，并提供了一个可用的配置文件来定制算法的参数。</li>
<li>results: 该论文通过使用多目标优化算法，可以实现更好的投资效果，并提供了一个可用的配置文件来定制算法的参数。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
</details>
<details>
<summary>摘要</summary>
MOPO-LSI是一个开源的多目标投资组合优化库，旨在推动可持续投资。这份文档提供MOPO-LSI版本1.0的用户指南，包括问题设置、工作流程和配置参数。
</details></li>
</ul>
<hr>
<h2 id="On-the-Constrained-Time-Series-Generation-Problem"><a href="#On-the-Constrained-Time-Series-Generation-Problem" class="headerlink" title="On the Constrained Time-Series Generation Problem"></a>On the Constrained Time-Series Generation Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01717">http://arxiv.org/abs/2307.01717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Coletta, Sriram Gopalakrishan, Daniel Borrajo, Svitlana Vyetrenko</li>
<li>for: 这个论文的目的是解决受限时间序列生成问题，以提高机器学习算法的性能，增加罕见事件的发生频率，并生成对应的counterfactualenario。</li>
<li>methods: 这个论文提出了一种新的方法集，包括一种受限时间序列生成模型“GuidedDiffTime”，用于生成符合限制的时间序列。这些方法使用可导的扩散模型，并通过优化问题来保证生成的时间序列具有真实性。</li>
<li>results: 这个论文在金融和能源等领域进行了评估，并证明了其方法的优越性。具体来说，这些方法可以提高现有方法的性能，同时不需要重新训练，从而减少碳脚印。<details>
<summary>Abstract</summary>
Synthetic time series are often used in practical applications to augment the historical time series dataset for better performance of machine learning algorithms, amplify the occurrence of rare events, and also create counterfactual scenarios described by the time series. Distributional-similarity (which we refer to as realism) as well as the satisfaction of certain numerical constraints are common requirements in counterfactual time series scenario generation requests. For instance, the US Federal Reserve publishes synthetic market stress scenarios given by the constrained time series for financial institutions to assess their performance in hypothetical recessions. Existing approaches for generating constrained time series usually penalize training loss to enforce constraints, and reject non-conforming samples. However, these approaches would require re-training if we change constraints, and rejection sampling can be computationally expensive, or impractical for complex constraints. In this paper, we propose a novel set of methods to tackle the constrained time series generation problem and provide efficient sampling while ensuring the realism of generated time series. In particular, we frame the problem using a constrained optimization framework and then we propose a set of generative methods including ``GuidedDiffTime'', a guided diffusion model to generate realistic time series. Empirically, we evaluate our work on several datasets for financial and energy data, where incorporating constraints is critical. We show that our approaches outperform existing work both qualitatively and quantitatively. Most importantly, we show that our ``GuidedDiffTime'' model is the only solution where re-training is not necessary for new constraints, resulting in a significant carbon footprint reduction.
</details>
<details>
<summary>摘要</summary>
Synthetic time series 常用于实际应用中以增强机器学习算法的性能，增加罕见事件的发生频率，并创建对时间序列的counterfactualenario。例如，美国联邦储金行发布了基于受限时间序列的synthetic市场压力场景，用于金融机构评估其在假设的经济衰退中的性能。现有的时间序列生成方法通常是通过减少训练损失来实现约束，并拒绝不符合约束的样本。然而，这些方法需要重新训练，如果改变约束，并且拒绝样本可能是计算昂贵或对复杂约束来说不实际。在这篇论文中，我们提出一种新的方法来解决受约束时间序列生成问题，并提供高效的采样，以保证生成的时间序列的真实性。具体来说，我们将问题带入一个受约束优化框架，然后我们提出一种生成方法，包括“导航扩散模型”，用于生成真实的时间序列。在实际中，我们对金融和能源等数据集进行了评估，并证明我们的方法在质量和效率两个方面都有较好的表现。最重要的是，我们的“导航扩散模型”不需要重新训练，以避免重新训练所带来的碳脚印。
</details></li>
</ul>
<hr>
<h2 id="Align-With-Purpose-Optimize-Desired-Properties-in-CTC-Models-with-a-General-Plug-and-Play-Framework"><a href="#Align-With-Purpose-Optimize-Desired-Properties-in-CTC-Models-with-a-General-Plug-and-Play-Framework" class="headerlink" title="Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework"></a>Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01715">http://arxiv.org/abs/2307.01715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eliya Segev, Maya Alroy, Ronen Katsir, Noam Wies, Ayana Shenhav, Yael Ben-Oren, David Zar, Oren Tadmor, Jacob Bitterman, Amnon Shashua, Tal Rosenwein</li>
<li>for: 这篇论文的目的是提高CTC评价函数中的一种扩展，以便在训练seq2seq模型时提高模型的性能。</li>
<li>methods: 该论文提出了一种扩展CTC评价函数的方法，称为“Align With Purpose”，该方法通过添加一个额外的损失函数来优化模型的一定性能。</li>
<li>results: 该论文在自动语音识别领域中应用了该方法，并实现了在不同的性能指标下提高模型的性能。例如，在释放时间优化中，提高了570毫秒，而word error rate（WER）下降了4.5%。此外，该方法可以在大规模数据上进行扩展，并且可以通过只添加一些代码来实现。<details>
<summary>Abstract</summary>
Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect alignments. We apply our framework in the domain of Automatic Speech Recognition (ASR) and show its generality in terms of property selection, architectural choice, and scale of training dataset (up to 280,000 hours). To demonstrate the effectiveness of our framework, we apply it to two unrelated properties: emission time and word error rate (WER). For the former, we report an improvement of up to 570ms in latency optimization with a minor reduction in WER, and for the latter, we report a relative improvement of 4.5% WER over the baseline models. To the best of our knowledge, these applications have never been demonstrated to work on a scale of data as large as ours. Notably, our method can be implemented using only a few lines of code, and can be extended to other alignment-free loss functions and to domains other than ASR.
</details>
<details>
<summary>摘要</summary>
Connectionist Temporal Classification (CTC) 是一种广泛使用的训练监督序列到序列（seq2seq）模型的评价标准。它允许学习输入和输出序列之间的关系，称为对齐，通过对完美对齐（导致真实值）进行积分，而抛弃不完美对齐。这个二元对齐分类不足以捕捉其他重要的对齐属性，因此我们提出了 $\textit{Align With Purpose}$，一种通用的插件和替换框架。我们通过补充 CTC 的损失函数中的额外损失项，以便根据某种需要的属性进行对齐优化。我们的方法不需要对 CTC 损失函数进行任何修改，可以轻松地优化多种属性，并允许对不完美对齐进行分类。我们在自动语音识别（ASR）领域应用了我们的框架，并在不同的属性、结构和训练数据规模（最多 280,000 小时）上进行了证明。为了证明我们的框架的有效性，我们在两个不相关的属性上应用了它：发射时间和单词错误率（WER）。对于前者，我们报告了最多 570ms 的延迟优化和相对较小的 WER 降低，对于后者，我们报告了相对于基eline模型的4.5% WER 提高。这些应用都是在我们知道的数据规模上进行的，而且我们的方法只需要几行代码就可以实现，并且可以扩展到其他对齐不受限制的损失函数和领域。
</details></li>
</ul>
<hr>
<h2 id="Distributional-Model-Equivalence-for-Risk-Sensitive-Reinforcement-Learning"><a href="#Distributional-Model-Equivalence-for-Risk-Sensitive-Reinforcement-Learning" class="headerlink" title="Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning"></a>Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01708">http://arxiv.org/abs/2307.01708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Kastner, Murat A. Erdogdu, Amir-massoud Farahmand</li>
<li>for: 本研究强调学习风险敏感奖励学习模型。</li>
<li>methods: 本文使用分布式奖励学习引入两种新的模型Equivalence定义，一种是通用的，可以用来奖励任何风险度量，但是 computationally intractable; 另一种是实用的，允许用户选择可以奖励的风险度量。</li>
<li>results: 我们的框架可以用来改进任何模型自由风险敏感算法，并在标准和大规模实验中证明其能力。<details>
<summary>Abstract</summary>
We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.
</details>
<details>
<summary>摘要</summary>
我们考虑到风险敏感的强化学习问题。我们理论上显示，对于风险中立设定的价值相等方法，不够以来 пла номoptimal 的方式在风险敏感设定中。我们利用分布式强化学习来引入两个新的模型相等性，一个是一般的，可以用来 пла номoptimal 任何风险度量，但是computationally intractable;另一个是实用的，允许选择可以实时最佳化的风险度量。我们显示了我们的框架可以与任何风险敏感无模型学习算法结合，并提供了 Tabular 和大规模实验来证明其能力。
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-and-Solving-Infinite-Games-with-an-ERM-Oracle"><a href="#Online-Learning-and-Solving-Infinite-Games-with-an-ERM-Oracle" class="headerlink" title="Online Learning and Solving Infinite Games with an ERM Oracle"></a>Online Learning and Solving Infinite Games with an ERM Oracle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01689">http://arxiv.org/abs/2307.01689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelos Assos, Idan Attias, Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson</li>
<li>for: 这个论文主要针对的是在在线学习Setting中，使用ERM oracle calls来实现最佳化的泛化误差和均衡点。</li>
<li>methods: 这篇论文提出了一种基于ERM oracle calls的在线binary分类算法，以及在多 Player游戏中的均衡点算法，这些算法都可以在不同的游戏设定中实现最佳化的性能。</li>
<li>results: 论文表明了这种算法在可 réalisable Setting中有finite regret，在agnostic Setting中具有sublinearly growing regret，并且可以在不同的游戏设定中实现最佳化的性能，其性能与游戏的Littlestone和阈值维度有关。<details>
<summary>Abstract</summary>
While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.   We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero-sum games and approximate coarse correlated equilibria in multi-player general-sum games, as long as the game has a bounded fat-threshold dimension. Our algorithms apply to both binary-valued and real-valued games and can be viewed as providing justification for the wide use of double oracle and multiple oracle algorithms in the practice of solving large games.
</details>
<details>
<summary>摘要</summary>
在随机学习设定下，ERM 已经足够保证逼近优化的泛化误差，但在在线学习设定下，算法们尚未知道是否可以达到优化的泛化误差。在这项工作中，我们提出了基于 ERM  oracle 的在线二分类Setting 的算法，并证明其在可 realizable 设定下有finite regret，在agnostical 设定下有sublinearly growing regret。我们 bound regret 的大小与underlying 概念类型的 Littlestone 和阈值维度。在非参数学习游戏中，我们可以将 ERM oracle 解释为最佳回应 oracle，找到对某个玩家的历史玩家的最佳回应。在这个设定下，我们提供了基于最佳回应 oracle 的学习算法，可以在两 player zero-sum 游戏和多 player general-sum 游戏中达到approximate-minimax equilibria和approximate coarse correlated equilibria，只要游戏有 bounded fat-threshold dimension。我们的算法适用于 binary-valued 和 real-valued 游戏，可以视为对 double oracle 和多 oracle 算法在实践中的广泛使用提供 justify。
</details></li>
</ul>
<hr>
<h2 id="Serving-Graph-Neural-Networks-With-Distributed-Fog-Servers-For-Smart-IoT-Services"><a href="#Serving-Graph-Neural-Networks-With-Distributed-Fog-Servers-For-Smart-IoT-Services" class="headerlink" title="Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services"></a>Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01684">http://arxiv.org/abs/2307.01684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liekang Zeng, Xu Chen, Peng Huang, Ke Luo, Xiaoxi Zhang, Zhi Zhou<br>for:Fograph is designed to provide real-time GNN inference for IoT-driven smart applications, leveraging the resources of multiple fog nodes to reduce communication overhead and improve performance.methods:Fograph employs heterogeneity-aware execution planning and GNN-specific compression techniques to optimize the performance of GNN inference in fog environments.results:Compared to state-of-the-art cloud serving and fog deployment, Fograph achieves up to 5.39x execution speedup and 6.84x throughput improvement, demonstrating its effectiveness in improving the performance of GNN-based services for IoT applications.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have gained growing interest in miscellaneous applications owing to their outstanding ability in extracting latent representation on graph structures. To render GNN-based service for IoT-driven smart applications, traditional model serving paradigms usually resort to the cloud by fully uploading geo-distributed input data to remote datacenters. However, our empirical measurements reveal the significant communication overhead of such cloud-based serving and highlight the profound potential in applying the emerging fog computing. To maximize the architectural benefits brought by fog computing, in this paper, we present Fograph, a novel distributed real-time GNN inference framework that leverages diverse and dynamic resources of multiple fog nodes in proximity to IoT data sources. By introducing heterogeneity-aware execution planning and GNN-specific compression techniques, Fograph tailors its design to well accommodate the unique characteristics of GNN serving in fog environments. Prototype-based evaluation and case study demonstrate that Fograph significantly outperforms the state-of-the-art cloud serving and fog deployment by up to 5.39x execution speedup and 6.84x throughput improvement.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 在不同的应用领域获得了不断增长的兴趣，主要是因为它们在图结构上能够激发出优秀的隐藏表示。为了在基于 IoT 的智能应用中提供 GNN 服务，传统的模型服务方式通常是通过完全上传到远程数据中心进行云计算。然而，我们的实验表明，这种云计算中的通信开销很大，而 fog 计算的出现也提供了一个可能性。为了最大化fog计算中的建筑减少开销，在这篇论文中，我们提出了一种分布式实时 GNN 推理框架，称之为 Fograph。 Fograph 利用了多个 fog 节点的多样化和动态资源，以便在 IoT 数据源附近进行 GNN 推理。通过对异质性的执行规划和 GNN 特定压缩技术，Fograph 的设计与 fog 环境中 GNN 服务的特点相匹配。实验和案例研究表明，Fograph 在比较云服务和 fog 部署时可以达到5.39倍的执行速度提升和6.84倍的吞吐量提高。
</details></li>
</ul>
<hr>
<h2 id="Learning-Discrete-Weights-and-Activations-Using-the-Local-Reparameterization-Trick"><a href="#Learning-Discrete-Weights-and-Activations-Using-the-Local-Reparameterization-Trick" class="headerlink" title="Learning Discrete Weights and Activations Using the Local Reparameterization Trick"></a>Learning Discrete Weights and Activations Using the Local Reparameterization Trick</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01683">http://arxiv.org/abs/2307.01683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Berger, Aviv Navon, Ethan Fetaya</li>
<li>for: 降低计算机视和机器学习中的神经网络推断 computation和存储需求</li>
<li>methods: 使用二进制化来减少神经网络推断的计算复杂性，并通过使用比较快的位运算来替代慢速的浮点运算</li>
<li>results: 实现了降低计算机视和机器学习中神经网络推断的时间和内存占用，并达到了当前最佳性能的二进制激活神经网络推断<details>
<summary>Abstract</summary>
In computer vision and machine learning, a crucial challenge is to lower the computation and memory demands for neural network inference. A commonplace solution to address this challenge is through the use of binarization. By binarizing the network weights and activations, one can significantly reduce computational complexity by substituting the computationally expensive floating operations with faster bitwise operations. This leads to a more efficient neural network inference that can be deployed on low-resource devices. In this work, we extend previous approaches that trained networks with discrete weights using the local reparameterization trick to also allow for discrete activations. The original approach optimized a distribution over the discrete weights and uses the central limit theorem to approximate the pre-activation with a continuous Gaussian distribution. Here we show that the probabilistic modeling can also allow effective training of networks with discrete activation as well. This further reduces runtime and memory footprint at inference time with state-of-the-art results for networks with binary activations.
</details>
<details>
<summary>摘要</summary>
在计算机视觉和机器学习中，一个重要挑战是降低神经网络推理的计算和内存需求。一种常见的解决方案是通过 binarization 来实现这一目标。通过将神经网络权重和活动化值binarized，可以在替换计算昂贵的浮点运算时大幅降低计算复杂性。这会导致更高效的神经网络推理，可以在低资源设备上部署。在这项工作中，我们extend了之前的方法，使得神经网络可以使用随机变量的批处理技术进行训练，而不是通过精确的权重值来进行训练。我们原始的方法是使用中心假设定理来近似预Activation的Continuous Gaussian Distribution。这里我们表明，可以通过概率模型来有效地训练具有随机变量的神经网络。这会进一步降低执行时间和内存占用，并且在state-of-the-art 的结果下，对于具有二进制活动化的神经网络进行推理。
</details></li>
</ul>
<hr>
<h2 id="Training-Energy-Based-Models-with-Diffusion-Contrastive-Divergences"><a href="#Training-Energy-Based-Models-with-Diffusion-Contrastive-Divergences" class="headerlink" title="Training Energy-Based Models with Diffusion Contrastive Divergences"></a>Training Energy-Based Models with Diffusion Contrastive Divergences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01668">http://arxiv.org/abs/2307.01668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijian Luo, Hao Jiang, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Zhihua Zhang</li>
<li>for: 这个论文主要针对的问题是如何改进对能量基模型（EBM）的训练方法，以提高EBM的生成能力和效率。</li>
<li>methods: 这个论文提出了一种新的启发对EBM的训练方法，即Diffusion Contrastive Divergence（DCD），它将Langevin dynamic更换为其他EBM参数自由的扩散过程。这种方法可以更高效地进行训练，并且不受非可忽略的梯度项的限制。</li>
<li>results: 作者在实验中表明，提出的DCD方法可以在生成数据集和高维图像噪声除除和生成任务中表现出色，比CD更高效和稳定。此外，DCD还能够训练EBM来生成Celab-A $32\times 32$数据集，与现有EBM相当。<details>
<summary>Abstract</summary>
Energy-Based Models (EBMs) have been widely used for generative modeling. Contrastive Divergence (CD), a prevailing training objective for EBMs, requires sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which leads to an irreconcilable trade-off between the computational burden and the validity of the CD. Running MCMCs till convergence is computationally intensive. On the other hand, short-run MCMC brings in an extra non-negligible parameter gradient term that is difficult to handle. In this paper, we provide a general interpretation of CD, viewing it as a special instance of our proposed Diffusion Contrastive Divergence (DCD) family. By replacing the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, we propose a more efficient divergence. We show that the proposed DCDs are both more computationally efficient than the CD and are not limited to a non-negligible gradient term. We conduct intensive experiments, including both synthesis data modeling and high-dimensional image denoising and generation, to show the advantages of the proposed DCDs. On the synthetic data learning and image denoising experiments, our proposed DCD outperforms CD by a large margin. In image generation experiments, the proposed DCD is capable of training an energy-based model for generating the Celab-A $32\times 32$ dataset, which is comparable to existing EBMs.
</details>
<details>
<summary>摘要</summary>
能量基模型（EBM）在生成模型方面广泛使用。对比差分泵（CD）是EBM训练的主要目标函数，但是使用Markov链约化 Monte Carlo方法（MCMC）来采样EBM，导致计算成本和验证CD之间存在不可 reconcile的负担。在MCMC运行至收敛之前，计算成本很高；另一方面，使用短跑MCMC会带来额外的非可忽略的参数梯度项，而且难以处理。在本文中，我们提供了CD的普遍解释，视其为我们提议的噪声对照分布（DCD）家族的特例。我们将CD中使用的朗格温动力换用其他EBM参数无关的扩散过程，并提出了更高效的分离。我们表明，我们提议的DCD比CD更高效，而且不受非可忽略的参数梯度项的限制。我们进行了广泛的实验，包括生成数据模型和高维图像减震和生成，以显示我们的提议DCD的优势。在生成数据学习和图像减震实验中，我们的提议DCD比CD大幅提高。在图像生成实验中，我们的提议DCD可以训练一个能量基模型，用于生成Celab-A $32\times 32$ 数据集，与现有EBM相当。
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Classification-on-Low-Dimensional-Manifolds-using-Overparameterized-Convolutional-Residual-Networks"><a href="#Nonparametric-Classification-on-Low-Dimensional-Manifolds-using-Overparameterized-Convolutional-Residual-Networks" class="headerlink" title="Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks"></a>Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01649">http://arxiv.org/abs/2307.01649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiqi Zhang, Zixuan Zhang, Minshuo Chen, Mengdi Wang, Tuo Zhao, Yu-Xiang Wang</li>
<li>for: 这 paper 的目的是研究 Convolutional residual neural networks (ConvResNets) 的性能，并解释它们在实践中的出色预测能力，不能由 conventional wisdom 解释。</li>
<li>methods: 这 paper 使用 weight decay 来研究 ConvResNeXts 的表现，从非Parametric classification 的角度来看。</li>
<li>results: 研究表明，ConvResNeXts 可以具有高精度的预测性能，并且可以有效地适应函数的柔和性和低维度结构。<details>
<summary>Abstract</summary>
Convolutional residual neural networks (ConvResNets), though overparameterized, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts, which cover ConvResNets as a special case, trained with weight decay from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of overparameterized ConvResNeXts over conventional machine learning models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SwinGNN-Rethinking-Permutation-Invariance-in-Diffusion-Models-for-Graph-Generation"><a href="#SwinGNN-Rethinking-Permutation-Invariance-in-Diffusion-Models-for-Graph-Generation" class="headerlink" title="SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation"></a>SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01646">http://arxiv.org/abs/2307.01646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiyan98/swingnn">https://github.com/qiyan98/swingnn</a></li>
<li>paper_authors: Qi Yan, Zhengyang Liang, Yang Song, Renjie Liao, Lele Wang</li>
<li>for: 本文旨在提出一种基于卷积神经网络的非对称扩散模型，用于学习图数据上的非对称分布。</li>
<li>methods: 该模型使用高效的边到边2-WL消息传递网络，并利用Shifted Window基于SwinTransformers的自注意机制。</li>
<li>results: 经过系统的ablations和训练技巧优化，我们的SwinGNN在synthetic和实际的蛋白质和分子数据上达到了顶尖性能。<details>
<summary>Abstract</summary>
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph generative model to a permutation-invariant one. Extensive experiments on synthetic and real-world protein and molecule datasets show that our SwinGNN achieves state-of-the-art performances. Our code is released at https://github.com/qiyan98/SwinGNN.
</details>
<details>
<summary>摘要</summary>
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph generative model to a permutation-invariant one. Extensive experiments on synthetic and real-world protein and molecule datasets show that our SwinGNN achieves state-of-the-art performances. Our code is released at https://github.com/qiyan98/SwinGNN.Here's the translation in Traditional Chinese:Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph generative model to a permutation-invariant one. Extensive experiments on synthetic and real-world protein and molecule datasets show that our SwinGNN achieves state-of-the-art performances. Our code is released at https://github.com/qiyan98/SwinGNN.
</details></li>
</ul>
<hr>
<h2 id="Heuristic-Algorithms-for-the-Approximation-of-Mutual-Coherence"><a href="#Heuristic-Algorithms-for-the-Approximation-of-Mutual-Coherence" class="headerlink" title="Heuristic Algorithms for the Approximation of Mutual Coherence"></a>Heuristic Algorithms for the Approximation of Mutual Coherence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01639">http://arxiv.org/abs/2307.01639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregor Betz, Vera Chekan, Tamara Mchedlidze</li>
<li>for: This paper is written for those interested in efficient computation of mutual coherence, particularly in the context of political preference matching systems like Wahl-O-Mat.</li>
<li>methods: The paper presents several heuristics to estimate the model parameters of a mixture of three Gaussians distribution, which is used to approximate the mutual coherence. Some of the algorithms are fully polynomial-time, while others require solving a small number of instances of the SAT model counting problem.</li>
<li>results: The paper reports the average squared error of the best algorithm, which is below 0.0035, indicating a high degree of accuracy while also being efficient. The results are precise enough to be used in Wahl-O-Mat-like systems.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提高共谐度的计算效率而写的，特别是在政治偏好匹配系统中使用。</li>
<li>methods: 论文提出了一些用于估计三个 Gaussian 分布的模型参数的快速算法，其中一些是完全 полиномиаль时间的，而另一些只需解决一些 SAT 模型计数问题。</li>
<li>results: 论文报告了最佳算法的平均平方误差，为0.0035以下，表明高度准确并且高效。结果可以用于 Wahl-O-Mat 类系统中。<details>
<summary>Abstract</summary>
Mutual coherence is a measure of similarity between two opinions. Although the notion comes from philosophy, it is essential for a wide range of technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters to find candidates that are the closest to their political preferences. The exact computation of mutual coherence is highly time-consuming due to the iteration over all subsets of an opinion. Moreover, for every subset, an instance of the SAT model counting problem has to be solved which is known to be a hard problem in computer science. This work is the first study to accelerate this computation. We model the distribution of the so-called confirmation values as a mixture of three Gaussians and present efficient heuristics to estimate its model parameters. The mutual coherence is then approximated with the expected value of the distribution. Some of the presented algorithms are fully polynomial-time, others only require solving a small number of instances of the SAT model counting problem. The average squared error of our best algorithm lies below 0.0035 which is insignificant if the efficiency is taken into account. Furthermore, the accuracy is precise enough to be used in Wahl-O-Mat-like systems.
</details>
<details>
<summary>摘要</summary>
互相协调是两个意见之间的相似度度量。这个概念起源于哲学，但是它对广泛的技术领域都很重要，例如德国的 Wahl-O-Mat 系统。这个系统帮助选民找到最符合其政治偏好的候选人。计算互相协调的精确方法需要遍历所有意见的所有子集，并解决每个子集的 SAT 模型计数问题，这是计算机科学中知名的困难问题。这项工作是首次加速这种计算的研究。我们模型了确认值的分布为三个高斯分布的混合，并提供了高效的启发式来估算模型参数。然后，我们使用分布的期望值来 aproximate 互相协调。一些我们提出的算法是完全多项式时间的，其他些只需解决一些 SAT 模型计数问题。我们的最佳算法的平均方差平方误差低于 0.0035，这对于效率来说是无意义的。此外，我们的精度够精确，可以用于 Wahl-O-Mat 类系统。
</details></li>
</ul>
<hr>
<h2 id="HAGNN-Hybrid-Aggregation-for-Heterogeneous-Graph-Neural-Networks"><a href="#HAGNN-Hybrid-Aggregation-for-Heterogeneous-Graph-Neural-Networks" class="headerlink" title="HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks"></a>HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01636">http://arxiv.org/abs/2307.01636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanghui Zhu, Zhennan Zhu, Hongyang Chen, Chunfeng Yuan, Yihua Huang</li>
<li>for:  Handle heterogeneous graphs with rich type semantic information.</li>
<li>methods: 	+ Propose a novel framework called HAGNN (Hybrid Aggregation for Heterogeneous GNNs)	+ Leverage both meta-path neighbors and directly connected neighbors for node aggregation	+ Divide the aggregation process into two phases: meta-path-based intra-type aggregation and meta-path-free inter-type aggregation	+ Use a new data structure called fused meta-path graph for intra-type aggregation	+ Perform structural semantic aware aggregation</li>
<li>results: 	+ Outperform existing heterogeneous GNN models on node classification, node clustering, and link prediction tasks	+ Demonstrate the effectiveness of HAGNN in handling heterogeneous graphs with rich type semantic information.<details>
<summary>Abstract</summary>
Heterogeneous graph neural networks (GNNs) have been successful in handling heterogeneous graphs. In existing heterogeneous GNNs, meta-path plays an essential role. However, recent work pointed out that simple homogeneous graph model without meta-path can also achieve comparable results, which calls into question the necessity of meta-path. In this paper, we first present the intrinsic difference about meta-path-based and meta-path-free models, i.e., how to select neighbors for node aggregation. Then, we propose a novel framework to utilize the rich type semantic information in heterogeneous graphs comprehensively, namely HAGNN (Hybrid Aggregation for Heterogeneous GNNs). The core of HAGNN is to leverage the meta-path neighbors and the directly connected neighbors simultaneously for node aggregations. HAGNN divides the overall aggregation process into two phases: meta-path-based intra-type aggregation and meta-path-free inter-type aggregation. During the intra-type aggregation phase, we propose a new data structure called fused meta-path graph and perform structural semantic aware aggregation on it. Finally, we combine the embeddings generated by each phase. Compared with existing heterogeneous GNN models, HAGNN can take full advantage of the heterogeneity in heterogeneous graphs. Extensive experimental results on node classification, node clustering, and link prediction tasks show that HAGNN outperforms the existing modes, demonstrating the effectiveness of HAGNN.
</details>
<details>
<summary>摘要</summary>
《异类图 neural network（GNN）在处理异类图方面取得成功。现有的异类GNN中，元路扮演着关键性的角色。然而，最近的研究表明，简单的同类图模型无需元路可以达到相似的结果，这意味着元路的必要性被质疑。在这篇论文中，我们首先介绍异类GNN中元路和无元路两种模型之间的本质差异，即如何选择节点 для节点聚合。然后，我们提出了一种新的框架，即Hybrid Aggregation for Heterogeneous GNNs（异类GNN中的混合聚合），用于全面利用异类图中各种类型Semantic信息。核心思想是同时利用元路邻居和直接连接邻居进行节点聚合。我们将整个聚合过程分成两个阶段：元路基于的内部聚合和元路无的交叉聚合。在内部聚合阶段，我们提出了一种新的数据结构called fused meta-path graph，并在其上进行结构层次意识感知聚合。最后，我们将每个阶段生成的embeddings合并。与现有的异类GNN模型相比，HAGNN可以全面利用异类图中的异类性。我们在节点分类、节点封顶和链接预测任务上进行了广泛的实验，结果显示，HAGNN在这些任务上表现出了更好的效果，证明了HAGNN的效果。》
</details></li>
</ul>
<hr>
<h2 id="Renewable-energy-management-in-smart-home-environment-via-forecast-embedded-scheduling-based-on-Recurrent-Trend-Predictive-Neural-Network"><a href="#Renewable-energy-management-in-smart-home-environment-via-forecast-embedded-scheduling-based-on-Recurrent-Trend-Predictive-Neural-Network" class="headerlink" title="Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network"></a>Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01622">http://arxiv.org/abs/2307.01622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mertnakip/Recurrent-Trend-Predictive-Neural-Network">https://github.com/mertnakip/Recurrent-Trend-Predictive-Neural-Network</a></li>
<li>paper_authors: Mert Nakıp, Onur Çopur, Emrah Biyik, Cüneyt Güzeliş</li>
<li>For: The paper proposes an advanced machine learning algorithm for efficient residential demand control in smart home energy management systems.* Methods: The proposed algorithm, called Recurrent Trend Predictive Neural Network based Forecast Embedded Scheduling (rTPNN-FES), simultaneously forecasts renewable energy generation and schedules household appliances, eliminating the need for separate algorithms.* Results: The evaluation results show that rTPNN-FES provides near-optimal scheduling $37.5$ times faster than optimization while outperforming state-of-the-art forecasting techniques.Here is the same information in Simplified Chinese:* For: 本文提出了一种高效的机器学习算法，用于智能家庭能源管理系统中的居民需求控制。* Methods: 提议的算法是基于循环趋势预测神经网络的预测嵌入式调度算法（rTPNN-FES），同时预测可再生能源生产和家庭电器的调度。* Results: 评估结果显示，rTPNN-FES可以在优化过程中提供近似优化的调度，比起现有预测技术要高效，并且在37.5倍 faster than optimization。<details>
<summary>Abstract</summary>
Smart home energy management systems help the distribution grid operate more efficiently and reliably, and enable effective penetration of distributed renewable energy sources. These systems rely on robust forecasting, optimization, and control/scheduling algorithms that can handle the uncertain nature of demand and renewable generation. This paper proposes an advanced ML algorithm, called Recurrent Trend Predictive Neural Network based Forecast Embedded Scheduling (rTPNN-FES), to provide efficient residential demand control. rTPNN-FES is a novel neural network architecture that simultaneously forecasts renewable energy generation and schedules household appliances. By its embedded structure, rTPNN-FES eliminates the utilization of separate algorithms for forecasting and scheduling and generates a schedule that is robust against forecasting errors. This paper also evaluates the performance of the proposed algorithm for an IoT-enabled smart home. The evaluation results reveal that rTPNN-FES provides near-optimal scheduling $37.5$ times faster than the optimization while outperforming state-of-the-art forecasting techniques.
</details>
<details>
<summary>摘要</summary>
智能家庭能源管理系统可以使分布式电力网络运行更加高效和可靠，并允许有效地推进分布式可再生能源源。这些系统需要可靠的预测、优化和控制/调度算法，以处理各种不确定的需求和可再生能源生产。本文提出了一种高级的机器学习算法，即循环趋势预测神经网络基于预测嵌入的调度算法（rTPNN-FES），以提供高效的家庭需求控制。rTPNN-FES是一种新的神经网络架构，同时预测可再生能源生产和调度家用电器。由嵌入结构，rTPNN-FES消除了分离的预测和调度算法，生成一个强健对预测错误的负荷调度。本文还评估了提议的算法在智能家庭上的性能。评估结果显示，rTPNN-FES提供了近似优化的调度，比传统预测技术更高效，并且比优化算法更快，每秒37.5次。
</details></li>
</ul>
<hr>
<h2 id="SageFormer-Series-Aware-Graph-Enhanced-Transformers-for-Multivariate-Time-Series-Forecasting"><a href="#SageFormer-Series-Aware-Graph-Enhanced-Transformers-for-Multivariate-Time-Series-Forecasting" class="headerlink" title="SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting"></a>SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01616">http://arxiv.org/abs/2307.01616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwei Zhang, Xin Wang, Yuantao Gu</li>
<li>for: 本研究旨在提高多ivariate时间序列预测中的深度学习方法，尤其是Transformers的应用。</li>
<li>methods: 本paper引入了Series-aware Graph-enhanced Transformer模型，用于有效地捕捉和模型系列之间的依赖关系。</li>
<li>results: 经过广泛的实验研究，本paper显示了SageFormer模型在实际数据和 sintetic dataset上的superior性能，比之前的状态之 искусственный智能方法更高。<details>
<summary>Abstract</summary>
Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
多ivariate时间序列预测在多个领域发挥关键作用。最近的深度学习方法，特别是Transformers，已经显示了承诺，但还有一个差距在处理多个时间序列之间的相互依赖关系。这篇论文引入了SageFormer，一种基于图结构的Series-aware Graph-enhanced Transformer模型，用于有效地捕捉和模型多个时间序列之间的依赖关系。SageFormer解决了两个关键挑战：一是有效地表示多个时间序列中的多样化时间模式，二是避免多个时间序列之间的重复信息。重要的是，我们提出的系列意识框架可以轻松地与现有的Transformer-based模型结合使用，以提高对多个时间序列之间的依赖关系的模型。通过对真实世界和 sintetic 数据集进行广泛的实验，我们展示了SageFormer比前一个状态的方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Overconfidence-is-a-Dangerous-Thing-Mitigating-Membership-Inference-Attacks-by-Enforcing-Less-Confident-Prediction"><a href="#Overconfidence-is-a-Dangerous-Thing-Mitigating-Membership-Inference-Attacks-by-Enforcing-Less-Confident-Prediction" class="headerlink" title="Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction"></a>Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01610">http://arxiv.org/abs/2307.01610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dependablesystemslab/mia_defense_hamp">https://github.com/dependablesystemslab/mia_defense_hamp</a></li>
<li>paper_authors: Zitao Chen, Karthik Pattabiraman</li>
<li>For: This paper is written to address the problem of membership inference attacks (MIAs) on machine learning (ML) models, which can compromise the privacy of training data. The paper proposes a defense technique called HAMP that can provide strong membership privacy and high accuracy without requiring additional data.* Methods: The HAMP technique consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model’s prediction while still achieving high accuracy. The technique also modifies all prediction outputs to become low-confidence outputs, effectively obscuring the differences between the prediction on members and non-members.* Results: The paper conducts extensive evaluation on five benchmark datasets and shows that HAMP provides consistently high accuracy and strong membership privacy, outperforming seven state-of-the-art defenses in terms of privacy-utility trade-off.<details>
<summary>Abstract</summary>
Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving high accuracy. To further reduce privacy risk, HAMP uniformly modifies all the prediction outputs to become low-confidence outputs while preserving the accuracy, which effectively obscures the differences between the prediction on members and non-members. We conduct extensive evaluation on five benchmark datasets, and show that HAMP provides consistently high accuracy and strong membership privacy. Our comparison with seven state-of-the-art defenses shows that HAMP achieves a superior privacy-utility trade off than those techniques.
</details>
<details>
<summary>摘要</summary>
为了mitigate MIA的不同形式，我们发现它们都利用了 ML 模型对训练样本的过于自信的预测，通过不同的代理来实现。这种情况使我们设计了一种强制模型在训练和测试样本上具有相同的预测行为的方法。HAMP 包括一种新的训练框架，高级 entropy 软标签和一种基于 entropy 的 regularizer，以防止模型的预测，同时仍然实现高准确率。为了进一步减少隐私风险，HAMP 对所有预测输出进行了一致的低信任输出修改，使模型的预测结果变得模拟，从而隐藏了训练和测试样本之间的差异。我们对五个 benchmark 数据集进行了广泛的评估，并显示了 HAMP 可以在高准确率和强大的成员隐私之间取得平衡。我们与七种 state-of-the-art 防御技术进行比较，发现 HAMP 在隐私利用与实用性之间取得了更好的平衡。
</details></li>
</ul>
<hr>
<h2 id="Prototypes-as-Explanation-for-Time-Series-Anomaly-Detection"><a href="#Prototypes-as-Explanation-for-Time-Series-Anomaly-Detection" class="headerlink" title="Prototypes as Explanation for Time Series Anomaly Detection"></a>Prototypes as Explanation for Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01601">http://arxiv.org/abs/2307.01601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Li, Carsten Jentsch, Emmanuel Müller</li>
<li>for: 本文针对时间序列资料中的异常模式探测，尤其是在没有标签的情况下，时间序列资料的动态性和未料到的异常行为导致探测过程具有挑战性。</li>
<li>methods: 本文提出了ProtoAD方法，利用示例来解释深度黑盒模型中的异常探测过程。在不对探测性能有重要影响的情况下，示例提供了深度黑盒模型中的透彻关键，并提供了域专家和投资者对模型的直觉理解。</li>
<li>results: 本文extend了广泛使用的示例学习在分类问题上的应用，并将其推广到异常探测问题上。通过视觉化示例的latent空间和输入空间，我们直观地解释了常规资料如何被模型，并且解释了具体的异常模式是如何被识别为异常的。<details>
<summary>Abstract</summary>
Detecting abnormal patterns that deviate from a certain regular repeating pattern in time series is essential in many big data applications. However, the lack of labels, the dynamic nature of time series data, and unforeseeable abnormal behaviors make the detection process challenging. Despite the success of recent deep anomaly detection approaches, the mystical mechanisms in such black-box models have become a new challenge in safety-critical applications. The lack of model transparency and prediction reliability hinders further breakthroughs in such domains. This paper proposes ProtoAD, using prototypes as the example-based explanation for the state of regular patterns during anomaly detection. Without significant impact on the detection performance, prototypes shed light on the deep black-box models and provide intuitive understanding for domain experts and stakeholders. We extend the widely used prototype learning in classification problems into anomaly detection. By visualizing both the latent space and input space prototypes, we intuitively demonstrate how regular data are modeled and why specific patterns are considered abnormal.
</details>
<details>
<summary>摘要</summary>
检测时序序数据中异常模式的检测是许多大数据应用场景中的关键问题。然而，缺乏标签、时序序数据的动态性和未预期的异常行为使检测过程具有挑战性。虽然最近的深度异常检测方法已经取得了成功，但这些黑盒模型中的神秘机制成为了新的挑战。模型的不透明度和预测可靠性限制了进一步的突破。本文提出了ProtoAD，使用模型为异常检测中的示例基本解释。无需对检测性能产生显著影响，示例揭示了深度黑盒模型的内部机制，提供了域专家和投资者Intuitive的理解。我们将通用的 prototype 学习在分类问题中扩展到异常检测。通过视觉化 latent space 和输入空间示例，我们直观地解释了如何模型正常数据和哪些特定模式被视为异常。
</details></li>
</ul>
<hr>
<h2 id="A-Scalable-Reinforcement-Learning-based-System-Using-On-Chain-Data-for-Cryptocurrency-Portfolio-Management"><a href="#A-Scalable-Reinforcement-Learning-based-System-Using-On-Chain-Data-for-Cryptocurrency-Portfolio-Management" class="headerlink" title="A Scalable Reinforcement Learning-based System Using On-Chain Data for Cryptocurrency Portfolio Management"></a>A Scalable Reinforcement Learning-based System Using On-Chain Data for Cryptocurrency Portfolio Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01599">http://arxiv.org/abs/2307.01599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhan Huang, Fumihide Tanaka</li>
<li>For: The paper is written for proposing a novel reinforcement learning-based system for cryptocurrency portfolio management that incorporates on-chain data for end-to-end management.* Methods: The paper uses on-chain data to train a reinforcement learning model for cryptocurrency portfolio management, and the model is tested and evaluated using backtesting results on three portfolios.* Results: The results show that the proposed CryptoRLPM system outperforms all baselines in terms of accumulated rate of return, daily rate of return, and Sortino ratio, with an enhancement of at least 83.14%, 0.5603%, and 2.1767 respectively compared to Bitcoin.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提出一种基于强化学习的 криптовалю端folio管理系统，该系统包括了链上数据的测试和评估。</li>
<li>methods: 论文使用链上数据来训练一个基于强化学习的 криптовалю端folio管理模型，并对模型进行了测试和评估。</li>
<li>results: 结果显示，提出的 CryptoRLPM 系统在比基金的测试和评估中减少了至少 83.14%、0.5603% 和 2.1767% 的负面影响，并且在比特币方面减少了至少 83.14%、0.5603% 和 2.1767% 的负面影响。<details>
<summary>Abstract</summary>
On-chain data (metrics) of blockchain networks, akin to company fundamentals, provide crucial and comprehensive insights into the networks. Despite their informative nature, on-chain data have not been utilized in reinforcement learning (RL)-based systems for cryptocurrency (crypto) portfolio management (PM). An intriguing subject is the extent to which the utilization of on-chain data can enhance an RL-based system's return performance compared to baselines. Therefore, in this study, we propose CryptoRLPM, a novel RL-based system incorporating on-chain data for end-to-end crypto PM. CryptoRLPM consists of five units, spanning from information comprehension to trading order execution. In CryptoRLPM, the on-chain data are tested and specified for each crypto to solve the issue of ineffectiveness of metrics. Moreover, the scalable nature of CryptoRLPM allows changes in the portfolios' cryptos at any time. Backtesting results on three portfolios indicate that CryptoRLPM outperforms all the baselines in terms of accumulated rate of return (ARR), daily rate of return (DRR), and Sortino ratio (SR). Particularly, when compared to Bitcoin, CryptoRLPM enhances the ARR, DRR, and SR by at least 83.14%, 0.5603%, and 2.1767 respectively.
</details>
<details>
<summary>摘要</summary>
币Chain数据（指标），类似于公司基础数据，为区块链网络提供了关键和全面的信息。尽管它们的信息性很高，但是它们在基于强化学习（RL）的系统中没有被利用，用于货币（简称为“爬”）股票管理（PM）。这是一个有趣的话题，即使用币Chain数据可以提高RL基本系统的回报性相比基准。因此，在这种研究中，我们提出了CryptoRLPM，一种包含五个单元的RL基本系统，用于综合管理爬股票。CryptoRLPM中的币Chain数据被测试和特定为每种爬股票，以解决币Chain数据的不准确性问题。此外，CryptoRLPM具有可扩展性，可以在任何时间更改股票组合中的爬股票。回testing结果表明，CryptoRLPM在三个股票组合上超过所有基准，在累积收益率（ARR）、日内收益率（DRR）和Sortino分数（SR）方面。特别是与比特币相比，CryptoRLPM在ARR、DRR和SR方面提高了至少83.14%、0.5603%和2.1767%。
</details></li>
</ul>
<hr>
<h2 id="Bridge-the-Performance-Gap-in-Peak-hour-Series-Forecasting-The-Seq2Peak-Framework"><a href="#Bridge-the-Performance-Gap-in-Peak-hour-Series-Forecasting-The-Seq2Peak-Framework" class="headerlink" title="Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework"></a>Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01597">http://arxiv.org/abs/2307.01597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwei Zhang, Xin Wang, Jingyuan Xie, Heling Zhang, Yuantao Gu</li>
<li>for: 预测峰值时间序列 (PHSF) 是许多领域的关键任务，但是目前的深度学习模型在这种任务上表现不佳。这可以归结于峰值时间序列的高度非站台性，导致直接预测更加困难于标准时间序列预测 (TSF)。</li>
<li>methods: 本文提出了一种名为Seq2Peak的新框架，用于解决 PHSF 任务中的性能差距。Seq2Peak 包括两个关键组件：一个名为 CyclicNorm 的管道，用于解决非站台性问题，以及一个简单 yet effective 的可学习参数自由的峰值时间序列解码器，使用了一种混合损失函数，将原始序列和峰值时间序列作为监督信号。</li>
<li>results: 对于四个实际世界数据集，Seq2Peak 实现了惊人的平均相对提升率为 37.7%，对于基于 transformer 和非 transformer 的 TSF 模型。<details>
<summary>Abstract</summary>
Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in various domains. While state-of-the-art deep learning models excel in regular Time Series Forecasting (TSF), they struggle to achieve comparable results in PHSF. This can be attributed to the challenges posed by the high degree of non-stationarity in peak-hour series, which makes direct forecasting more difficult than standard TSF. Additionally, manually extracting the maximum value from regular forecasting results leads to suboptimal performance due to models minimizing the mean deficit. To address these issues, this paper presents Seq2Peak, a novel framework designed specifically for PHSF tasks, bridging the performance gap observed in TSF models. Seq2Peak offers two key components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid loss function that utilizes both the original series and peak-hour series as supervised signals. Extensive experimentation on publicly available time series datasets demonstrates the effectiveness of the proposed framework, yielding a remarkable average relative improvement of 37.7\% across four real-world datasets for both transformer- and non-transformer-based TSF models.
</details>
<details>
<summary>摘要</summary>
《峰值小时序列预测（PHSF）是许多领域中的关键 yet 未得到充分的研究。当前的深度学习模型在标准时间序列预测（TSF）中表现出色，但在 PHSF 中却很难达到相似的结果。这可以归因于峰值小时序列的高度非站ARY，使得直接预测变得更加困难于标准 TSF。另外，通过手动提取最大值从 regular 预测结果来获得优化性能的方法会带来较差的性能，因为模型会尝试最小化均方误差。为解决这些问题，本文提出了 Seq2Peak 框架，这是专门为 PHSF 任务设计的。Seq2Peak 包括两个关键组成部分： CyclicNorm 管道，用于mitigate 非站ARY问题，以及一个简单 yet 高效的可学习参数无 peak-hour 解码器，使用了 Hybrid 损失函数，该函数使用原始序列和峰值小时序列作为监督信号。经过对公共可用时间序列数据集的广泛实验，Seq2Peak 的效果得到了许多实验证明，其中平均相对提升率为 37.7%，在四个实际世界数据集上。
</details></li>
</ul>
<hr>
<h2 id="Cross-Element-Combinatorial-Selection-for-Multi-Element-Creative-in-Display-Advertising"><a href="#Cross-Element-Combinatorial-Selection-for-Multi-Element-Creative-in-Display-Advertising" class="headerlink" title="Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising"></a>Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01593">http://arxiv.org/abs/2307.01593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhang, Ping Zhang, Jian Dong, Yongkang Wang, Pengye Zhang, Bo Zhang, Xingxing Wang, Dong Wang</li>
<li>for: 本研究旨在提高广告创作的效果，通过采用跨元素共同选择机制来选择多个创意元素的合适组合。</li>
<li>methods: 本研究提出了一种跨元素共同选择框架（CECS），包括编码器过程和解码器过程。编码器过程采用跨元素交互来动态调整单个创意元素的表达，而解码器过程将创意组合问题转化为多个创意元素之间的链式选择问题。</li>
<li>results: 实验结果表明，CECS取得了最佳成绩（SOTA）在线上数据集上的评价指标，并在实际应用中实现了显著的6.02% CTR和10.37% GMV提升，这对业务具有益处。<details>
<summary>Abstract</summary>
The effectiveness of ad creatives is greatly influenced by their visual appearance. Advertising platforms can generate ad creatives with different appearances by combining creative elements provided by advertisers. However, with the increasing number of ad creative elements, it becomes challenging to select a suitable combination from the countless possibilities. The industry's mainstream approach is to select individual creative elements independently, which often overlooks the importance of interaction between creative elements during the modeling process. In response, this paper proposes a Cross-Element Combinatorial Selection framework for multiple creative elements, termed CECS. In the encoder process, a cross-element interaction is adopted to dynamically adjust the expression of a single creative element based on the current candidate creatives. In the decoder process, the creative combination problem is transformed into a cascade selection problem of multiple creative elements. A pointer mechanism with a cascade design is used to model the associations among candidates. Comprehensive experiments on real-world datasets show that CECS achieved the SOTA score on offline metrics. Moreover, the CECS algorithm has been deployed in our industrial application, resulting in a significant 6.02% CTR and 10.37% GMV lift, which is beneficial to the business.
</details>
<details>
<summary>摘要</summary>
“广告创意的有效性受到它的视觉形象影响很大。广告平台可以通过结合广告主提供的创意元素，生成不同的创意形象。然而，随着创意元素的数量增加，选择适当的组合变得越来越困难。业界主流的方法是选择个别创意元素独立地，往往忽略了创意元素间的互动过程中的重要性。因此，这篇文章提出了跨元素选择框架（CECS）。在encode过程中，采用了跨元素互动来动态地调整单一创意元素的表达，以满足目前的候选者。在decode过程中，创意组合问题转化为多个创意元素之间的传递选择问题。使用一个链接机制，模型候选者之间的协力。实际测试统计表明，CECS已经 дости得了最佳成绩（SOTA）的数据。此外，CECS算法已经在我们的业务应用中实现了6.02%的Click Through Rate（CTR）和10.37%的Gross Merchandise Value（GMV）提升，对业务有很大的帮助。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Lie-Group-Symmetry-Transformations-with-Neural-Networks"><a href="#Learning-Lie-Group-Symmetry-Transformations-with-Neural-Networks" class="headerlink" title="Learning Lie Group Symmetry Transformations with Neural Networks"></a>Learning Lie Group Symmetry Transformations with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01583">http://arxiv.org/abs/2307.01583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/victoria-klein/learning-lie-group-symmetries">https://github.com/victoria-klein/learning-lie-group-symmetries</a></li>
<li>paper_authors: Alex Gabel, Victoria Klein, Riccardo Valperga, Jeroen S. W. Lamb, Kevin Webster, Rick Quax, Efstratios Gavves</li>
<li>for: 检测和评估数据集中的对称性，用于模型选择、生成模型和数据分析等方面。</li>
<li>methods: 利用一种新的方法，可以自动发现数据集中的未知对称性，包括 Lie 群对称变换以外的其他对称性。</li>
<li>results: 研究得出的结果表明，该方法可以有效地检测和评估数据集中的对称性，并且可以在不同的参数值下进行一一对应。<details>
<summary>Abstract</summary>
The problem of detecting and quantifying the presence of symmetries in datasets is useful for model selection, generative modeling, and data analysis, amongst others. While existing methods for hard-coding transformations in neural networks require prior knowledge of the symmetries of the task at hand, this work focuses on discovering and characterizing unknown symmetries present in the dataset, namely, Lie group symmetry transformations beyond the traditional ones usually considered in the field (rotation, scaling, and translation). Specifically, we consider a scenario in which a dataset has been transformed by a one-parameter subgroup of transformations with different parameter values for each data point. Our goal is to characterize the transformation group and the distribution of the parameter values. The results showcase the effectiveness of the approach in both these settings.
</details>
<details>
<summary>摘要</summary>
问题是检测和评估数据集中的对称性，具有各种应用，如模型选择、生成模型和数据分析等。现有的方法需要先知道任务的对称性，而这种工作则是通过发现数据集中未知对称性，即李群对称变换 beyond 传统 Considered in the field (旋转、缩放和平移)。 Specifically, we consider a scenario in which a dataset has been transformed by a one-parameter subgroup of transformations with different parameter values for each data point. Our goal is to characterize the transformation group and the distribution of the parameter values. The results showcase the effectiveness of the approach in both these settings.Note that the translation is in Simplified Chinese, which is the more commonly used variety of Chinese in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="IAdet-Simplest-human-in-the-loop-object-detection"><a href="#IAdet-Simplest-human-in-the-loop-object-detection" class="headerlink" title="IAdet: Simplest human-in-the-loop object detection"></a>IAdet: Simplest human-in-the-loop object detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01582">http://arxiv.org/abs/2307.01582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/franchesoni/iadet">https://github.com/franchesoni/iadet</a></li>
<li>paper_authors: Franco Marchesoni-Acland, Gabriele Facciolo</li>
<li>for: 提高单类物体检测模型的训练效率和质量，通过人工监督系统。</li>
<li>methods:  propose a Intelligent Annotation (IA) strategy, including three modules: 助手数据标注、背景模型训练和活动选择下一个数据点。开发了特定于单类物体检测的IAdet工具，并提出了自动评估这种人工监督系统的方法。</li>
<li>results: 在PASCAL VOC数据集上，IAdet工具可以减少数据库标注时间25%，并提供一个免费训练过的模型。这些结果是基于偏门设计的very simple IAdet design，因此IAdet具有多个简单的改进空间，预示了可以实现强大的人工监督对象检测系统。<details>
<summary>Abstract</summary>
This work proposes a strategy for training models while annotating data named Intelligent Annotation (IA). IA involves three modules: (1) assisted data annotation, (2) background model training, and (3) active selection of the next datapoints. Under this framework, we open-source the IAdet tool, which is specific for single-class object detection. Additionally, we devise a method for automatically evaluating such a human-in-the-loop system. For the PASCAL VOC dataset, the IAdet tool reduces the database annotation time by $25\%$ while providing a trained model for free. These results are obtained for a deliberately very simple IAdet design. As a consequence, IAdet is susceptible to multiple easy improvements, paving the way for powerful human-in-the-loop object detection systems.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种名为智能注释（IA）的模型训练策略。IA包括三个模块：（1）助手数据注释、（2）背景模型训练和（3）活动选择下一个数据点。在这个框架下，我们开源了专门用于单类对象检测的IADE工具。此外，我们还提出了一种自动评估这种人在循环系统的方法。对于PASCAL VOC数据集，IADE工具可以降低数据库注释时间$25\%$，同时提供免费的训练模型。这些结果是基于故意设计非常简单的IADE设计而得到的。因此，IADE易受到多个简单的改进，开展出具有强大人在循环对象检测系统的可能性。
</details></li>
</ul>
<hr>
<h2 id="Optimal-and-Efficient-Binary-Questioning-for-Human-in-the-Loop-Annotation"><a href="#Optimal-and-Efficient-Binary-Questioning-for-Human-in-the-Loop-Annotation" class="headerlink" title="Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation"></a>Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01578">http://arxiv.org/abs/2307.01578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franco Marchesoni-Acland, Jean-Michel Morel, Josselin Kherroubi, Gabriele Facciolo</li>
<li>for: 本研究旨在解决人工监督学习中数据注释的缺失问题，即使用一个预测器可以获得更多的注释数据。</li>
<li>methods: 本研究使用了一种枚举编码法来寻找最优的问题策略，以及一些启发式和lookahead最小化代理成本函数的方法。</li>
<li>results: 研究表明，使用提议的方法可以在几种 sintetic 和实际世界的数据集上实现23-86%的注释效率提升。<details>
<summary>Abstract</summary>
Even though data annotation is extremely important for interpretability, research and development of artificial intelligence solutions, most research efforts such as active learning or few-shot learning focus on the sample efficiency problem. This paper studies the neglected complementary problem of getting annotated data given a predictor. For the simple binary classification setting, we present the spectrum ranging from optimal general solutions to practical efficient methods. The problem is framed as the full annotation of a binary classification dataset with the minimal number of yes/no questions when a predictor is available. For the case of general binary questions the solution is found in coding theory, where the optimal questioning strategy is given by the Huffman encoding of the possible labelings. However, this approach is computationally intractable even for small dataset sizes. We propose an alternative practical solution based on several heuristics and lookahead minimization of proxy cost functions. The proposed solution is analysed, compared with optimal solutions and evaluated on several synthetic and real-world datasets. On these datasets, the method allows a significant improvement ($23-86\%$) in annotation efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Task-Learning-to-Enhance-Generazability-of-Neural-Network-Equalizers-in-Coherent-Optical-Systems"><a href="#Multi-Task-Learning-to-Enhance-Generazability-of-Neural-Network-Equalizers-in-Coherent-Optical-Systems" class="headerlink" title="Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems"></a>Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05374">http://arxiv.org/abs/2307.05374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sasipim Srivallapanondh, Pedro J. Freire, Ashraful Alam, Nelson Costa, Bernhard Spinnler, Antonio Napoli, Egor Sedov, Sergei K. Turitsyn, Jaroslaw E. Prilepsky</li>
<li>for: 提高减噪系统的灵活性</li>
<li>methods: 使用多任务学习方法提高NN基于的平衡器</li>
<li>results: 单个NN基于平衡器可以提高Q因子至4dB，不需要重新训练，即使发射功率、符号速率或传输距离发生变化。<details>
<summary>Abstract</summary>
For the first time, multi-task learning is proposed to improve the flexibility of NN-based equalizers in coherent systems. A "single" NN-based equalizer improves Q-factor by up to 4 dB compared to CDC, without re-training, even with variations in launch power, symbol rate, or transmission distance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Approximate-information-for-efficient-exploration-exploitation-strategies"><a href="#Approximate-information-for-efficient-exploration-exploitation-strategies" class="headerlink" title="Approximate information for efficient exploration-exploitation strategies"></a>Approximate information for efficient exploration-exploitation strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01563">http://arxiv.org/abs/2307.01563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Barbier-Chebbah, Christian L. Vestergaard, Jean-Baptiste Masson</li>
<li>for: 这篇论文目标是解决决策中的探索-利用矛盾，具体是多重枪支问题。</li>
<li>methods: 这篇论文提出了一种新的算法，即approximate information maximization（AIM），该算法使用分析式 entropy 导数来选择每个时刻哪个枪支。AIM 与 Infomax 和 Thompson sampling 性能相同，同时具有加速、决定性和可追踪性等优点。</li>
<li>results: 实验证明 AIM 遵循 Lai-Robbins  asymptotic bound，并在不同的假设下表现稳定。其表达可调，可以根据具体情况进行特定优化。<details>
<summary>Abstract</summary>
This paper addresses the exploration-exploitation dilemma inherent in decision-making, focusing on multi-armed bandit problems. The problems involve an agent deciding whether to exploit current knowledge for immediate gains or explore new avenues for potential long-term rewards. We here introduce a novel algorithm, approximate information maximization (AIM), which employs an analytical approximation of the entropy gradient to choose which arm to pull at each point in time. AIM matches the performance of Infomax and Thompson sampling while also offering enhanced computational speed, determinism, and tractability. Empirical evaluation of AIM indicates its compliance with the Lai-Robbins asymptotic bound and demonstrates its robustness for a range of priors. Its expression is tunable, which allows for specific optimization in various settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Comprehensive-Multi-scale-Approach-for-Speech-and-Dynamics-Synchrony-in-Talking-Head-Generation"><a href="#A-Comprehensive-Multi-scale-Approach-for-Speech-and-Dynamics-Synchrony-in-Talking-Head-Generation" class="headerlink" title="A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation"></a>A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03270">http://arxiv.org/abs/2307.03270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louisbearing/hmo-audio">https://github.com/louisbearing/hmo-audio</a></li>
<li>paper_authors: Louis Airale, Dominique Vaufreydaz, Xavier Alameda-Pineda</li>
<li>for: 这个论文主要针对的是使用深度生成模型来动画非动体图像，以实现更加自然的头部动作和语音同步。</li>
<li>methods: 该论文提出了一种多尺度音视频同步损失函数和多尺度自适应GAN，以更好地处理语音和头部动作之间的短期和长期相关性。</li>
<li>results: 实验表明，该方法可以在多尺度音视频同步和头部动作质量上达到州前的提升，并且在标准的面部特征域中生成更加自然的头部动作。<details>
<summary>Abstract</summary>
Animating still face images with deep generative models using a speech input signal is an active research topic and has seen important recent progress. However, much of the effort has been put into lip syncing and rendering quality while the generation of natural head motion, let alone the audio-visual correlation between head motion and speech, has often been neglected. In this work, we propose a multi-scale audio-visual synchrony loss and a multi-scale autoregressive GAN to better handle short and long-term correlation between speech and the dynamics of the head and lips. In particular, we train a stack of syncer models on multimodal input pyramids and use these models as guidance in a multi-scale generator network to produce audio-aligned motion unfolding over diverse time scales. Our generator operates in the facial landmark domain, which is a standard low-dimensional head representation. The experiments show significant improvements over the state of the art in head motion dynamics quality and in multi-scale audio-visual synchrony both in the landmark domain and in the image domain.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable text into Simplified Chinese.<</SYS>>使用深度生成模型动画静止图像是一个活跃的研究领域，最近几年得到了重要的进步。然而，许多努力都是 lip syncing 和图像质量的优化，而生成自然的头部运动和语音-图像相关性往往被忽略。在这项工作中，我们提议一种多尺度音视频同步损失和多尺度自适应GAN，以更好地处理语音和头部运动之间的短期和长期相关性。特别是，我们在多modal输入 pyramids 上堆叠 syncer 模型，并使用这些模型作为导向在多尺度生成网络中生成音频同步的动作。我们的生成器在 facial landmark 领域中运行，这是一个标准的低维度头部表示。实验结果表明，我们的方法可以在头部运动动态质量和多尺度音视频同步两个方面达到显著提高。
</details></li>
</ul>
<hr>
<h2 id="Secure-Deep-Learning-based-Distributed-Intelligence-on-Pocket-sized-Drones"><a href="#Secure-Deep-Learning-based-Distributed-Intelligence-on-Pocket-sized-Drones" class="headerlink" title="Secure Deep Learning-based Distributed Intelligence on Pocket-sized Drones"></a>Secure Deep Learning-based Distributed Intelligence on Pocket-sized Drones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01559">http://arxiv.org/abs/2307.01559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elia Cereda, Alessandro Giusti, Daniele Palossi</li>
<li>for: 这个研究旨在解决单位大小仅对应小型飞行器（nano-drone）上进行大型深度学习模型的问题。</li>
<li>methods: 本研究提出了一种分布式边缘-fog计算模型，以实现在nano-drone上进行大型深度学习模型的执行。此外，本研究还提出了一种验证fog计算的方法，以确保fog节点或通信链路不可信。</li>
<li>results: 相比于完全在nano-drone上执行的现有Visual Pose Estimation网络，这个分布式边缘-fog执行方案可以提高$R^2$ score +0.19。在攻击情况下，本方法可以在2秒内检测攻击，95%的概率可以检测到。<details>
<summary>Abstract</summary>
Palm-sized nano-drones are an appealing class of edge nodes, but their limited computational resources prevent running large deep-learning models onboard. Adopting an edge-fog computational paradigm, we can offload part of the computation to the fog; however, this poses security concerns if the fog node, or the communication link, can not be trusted. To tackle this concern, we propose a novel distributed edge-fog execution scheme that validates fog computation by redundantly executing a random subnetwork aboard our nano-drone. Compared to a State-of-the-Art visual pose estimation network that entirely runs onboard, a larger network executed in a distributed way improves the $R^2$ score by +0.19; in case of attack, our approach detects it within 2s with 95% probability.
</details>
<details>
<summary>摘要</summary>
手持式奈米型机器人的 Computational Resources 有限，无法进行大型深度学习模型的 Calculation。我们运用 Edge-Fog 计算模式，将一部分计算推广到fog中，但这会带来安全性 Concern ，如果fog Node 或通信链路不能被信任。为解决这问题，我们提出了一个分布式 Edge-Fog 执行方案，透过重复运行 Random Subnetwork 在我们的奈米型机器人上，以验证fog计算。相比于完全在board上运行的 State-of-the-Art 视觉 pose 估测网络，分布式执行的大型网络可以提高 $R^2$ 分数 +0.19; 在攻击情况下，我们的方法可以在2秒内检测到攻击，95%的机会性。
</details></li>
</ul>
<hr>
<h2 id="Multi-gauge-Hydrological-Variational-Data-Assimilation-Regionalization-Learning-with-Spatial-Gradients-using-Multilayer-Perceptron-and-Bayesian-Guided-Multivariate-Regression"><a href="#Multi-gauge-Hydrological-Variational-Data-Assimilation-Regionalization-Learning-with-Spatial-Gradients-using-Multilayer-Perceptron-and-Bayesian-Guided-Multivariate-Regression" class="headerlink" title="Multi-gauge Hydrological Variational Data Assimilation: Regionalization Learning with Spatial Gradients using Multilayer Perceptron and Bayesian-Guided Multivariate Regression"></a>Multi-gauge Hydrological Variational Data Assimilation: Regionalization Learning with Spatial Gradients using Multilayer Perceptron and Bayesian-Guided Multivariate Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02497">http://arxiv.org/abs/2307.02497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngo Nghi Truyen Huynh, Pierre-André Garambois, François Colleoni, Benjamin Renard, Hélène Roux</li>
<li>for: 这篇论文旨在解决水文模型中难以估计的空间分布型水文参数问题，特别是无测水道上的洪水。</li>
<li>methods: 本研究使用了一种新的区域化技术，将复杂的区域转换函数融合到高分辨率水文模型中，以便使用机器学习优化算法进行学习。</li>
<li>results: 本研究获得了一种可靠地估计水文模型中的空间分布型参数，并且可以处理多测站数据，实现了高精度的水文预测。<details>
<summary>Abstract</summary>
Tackling the difficult problem of estimating spatially distributed hydrological parameters, especially for floods on ungauged watercourses, this contribution presents a novel seamless regionalization technique for learning complex regional transfer functions designed for high-resolution hydrological models. The transfer functions rely on: (i) a multilayer perceptron enabling a seamless flow of gradient computation to employ machine learning optimization algorithms, or (ii) a multivariate regression mapping optimized by variational data assimilation algorithms and guided by Bayesian estimation, addressing the equifinality issue of feasible solutions. The approach involves incorporating the inferable regionalization mappings into a differentiable hydrological model and optimizing a cost function computed on multi-gauge data with accurate adjoint-based spatially distributed gradients.
</details>
<details>
<summary>摘要</summary>
solves the difficult problem of estimating spatially distributed hydrological parameters, especially for floods on ungauged watercourses, by presenting a novel seamless regionalization technique for learning complex regional transfer functions designed for high-resolution hydrological models. The transfer functions rely on:(i) a multilayer perceptron enabling a seamless flow of gradient computation to employ machine learning optimization algorithms, or(ii) a multivariate regression mapping optimized by variational data assimilation algorithms and guided by Bayesian estimation, addressing the equifinality issue of feasible solutions.The approach involves incorporating the inferable regionalization mappings into a differentiable hydrological model and optimizing a cost function computed on multi-gauge data with accurate adjoint-based spatially distributed gradients.
</details></li>
</ul>
<hr>
<h2 id="Scalable-variable-selection-for-two-view-learning-tasks-with-projection-operators"><a href="#Scalable-variable-selection-for-two-view-learning-tasks-with-projection-operators" class="headerlink" title="Scalable variable selection for two-view learning tasks with projection operators"></a>Scalable variable selection for two-view learning tasks with projection operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01558">http://arxiv.org/abs/2307.01558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aalto-ics-kepaco/projse">https://github.com/aalto-ics-kepaco/projse</a></li>
<li>paper_authors: Sandor Szedmak, Riikka Huusari, Tat Hong Duong Le, Juho Rousu</li>
<li>for: 该论文提出了一种新的变量选择方法，适用于两视图设置或vector-valued超vision学习问题。该方法可以处理巨大规模的选择任务，数据样本数可以达到百万级。</li>
<li>methods: 该方法通过Iteratively选择高度相关于输出变量的变量，但不相关于先前选择的变量。为度量相关性，该方法使用投影算子和其代数。通过投影算子，输入和输出变量之间的关系可以表示为kernel函数，从而可以利用非线性相关模型。</li>
<li>results: 该方法在synthetic和实际数据上进行了实验 validate，显示了其扩展性和选择的有效性。<details>
<summary>Abstract</summary>
In this paper we propose a novel variable selection method for two-view settings, or for vector-valued supervised learning problems. Our framework is able to handle extremely large scale selection tasks, where number of data samples could be even millions. In a nutshell, our method performs variable selection by iteratively selecting variables that are highly correlated with the output variables, but which are not correlated with the previously chosen variables. To measure the correlation, our method uses the concept of projection operators and their algebra. With the projection operators the relationship, correlation, between sets of input and output variables can also be expressed by kernel functions, thus nonlinear correlation models can be exploited as well. We experimentally validate our approach, showing on both synthetic and real data its scalability and the relevance of the selected features. Keywords: Supervised variable selection, vector-valued learning, projection-valued measure, reproducing kernel Hilbert space
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的变量选择方法，适用于两视设定或vector-valued学习问题。我们的框架可以处理非常大规模的选择任务，数据样本数可以达到百万级。总之，我们的方法通过逐步选择输出变量高度相关的变量，但不相关于已经选择的变量来进行变量选择。为了度量相关性，我们使用投影算子和其代数来度量输入和输出变量之间的关系。通过投影算子，我们可以将输入和输出变量之间的关系表示为内积函数，从而可以利用内积函数来表示非线性相关模型。我们在实验中 validate our approach，并在 synthetic 和实际数据上证明了我们的方法的扩展性和选择的相关性。关键词：supervised变量选择、vector-valued学习、投影值度量、 reproduce kernel Hilbert space
</details></li>
</ul>
<hr>
<h2 id="Learning-to-reconstruct-the-bubble-distribution-with-conductivity-maps-using-Invertible-Neural-Networks-and-Error-Diffusion"><a href="#Learning-to-reconstruct-the-bubble-distribution-with-conductivity-maps-using-Invertible-Neural-Networks-and-Error-Diffusion" class="headerlink" title="Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion"></a>Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02496">http://arxiv.org/abs/2307.02496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishant Kumar, Lukas Krause, Thomas Wondrak, Sven Eckert, Kerstin Eckert, Stefan Gumhold</li>
<li>for: 用于实现可持续的氢生产</li>
<li>methods: 使用外部磁场探测器和归一化方法测量磁场干扰，并使用INN重建电导率场</li>
<li>results: 比使用提高方法（Tikhonov regularization）表现更好，可以高精度地重建电导率场<details>
<summary>Abstract</summary>
Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles generated during the process hinder reactions, reduce cell efficiency, and increase energy consumption. Additionally, these gas bubbles cause changes in the conductivity inside the cell, resulting in corresponding variations in the induced magnetic field around the cell. Therefore, measuring these gas bubble-induced magnetic field fluctuations using external magnetic sensors and solving the inverse problem of Biot-Savart Law allows for estimating the conductivity in the cell and, thus, bubble size and location. However, determining high-resolution conductivity maps from only a few induced magnetic field measurements is an ill-posed inverse problem. To overcome this, we exploit Invertible Neural Networks (INNs) to reconstruct the conductivity field. Our qualitative results and quantitative evaluation using random error diffusion show that INN achieves far superior performance compared to Tikhonov regularization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用电解为绿色氢生产的关键步骤，但在过程中生成的气泡会阻碍反应、降低电池效率和增加能源消耗。此外，这些气泡会导致电池内的导电性变化，从而导致电磁场附近电池的变化。因此，通过外部磁场探测器测量气泡启发的磁场变化，并解决生成的Biot-Savart法的反问题，可以估算电池内的导电性，并由此计算气泡的大小和位置。但是，从仅几个磁场测量得到高分辨率导电地图是一个不定义的倒数问题。为了解决这个问题，我们利用归一化神经网络（INNs）重建导电场。我们的质量结果和随机扩散评价表明，INN在性能方面远胜于TIkhonov正则化。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Richness-of-Learned-Compressed-Representation-of-Images-for-Semantic-Segmentation"><a href="#Exploiting-Richness-of-Learned-Compressed-Representation-of-Images-for-Semantic-Segmentation" class="headerlink" title="Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation"></a>Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01524">http://arxiv.org/abs/2307.01524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DL4Compression/Semantic_Segmentation_of_Driving_Videos_on_Learning_based_Image_Compression">https://github.com/DL4Compression/Semantic_Segmentation_of_Driving_Videos_on_Learning_based_Image_Compression</a></li>
<li>paper_authors: Ravi Kakaiya, Rakshith Sathish, Ramanathan Sethuraman, Debdoot Sheet</li>
<li>for: 提高自动驾驶和高级驾驶助手系统（ADAS）的性能和可扩展性。</li>
<li>methods: 使用学习基于的压缩编码器来减少传输数据的延迟，并且通过学习的方式使得压缩编码器可以同时执行压缩和解压缩操作。</li>
<li>results: 在Cityscapes dataset上实验 validate the proposed pipeline，实现了压缩因子达66倍，保留了segmenation任务所需的信息，而且降低了总计算量11%。<details>
<summary>Abstract</summary>
Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the potential to radically change the way we travel. Many such vehicles currently rely on segmentation and object detection algorithms to detect and track objects around its surrounding. The data collected from the vehicles are often sent to cloud servers to facilitate continual/life-long learning of these algorithms. Considering the bandwidth constraints, the data is compressed before sending it to servers, where it is typically decompressed for training and analysis. In this work, we propose the use of a learning-based compression Codec to reduce the overhead in latency incurred for the decompression operation in the standard pipeline. We demonstrate that the learned compressed representation can also be used to perform tasks like semantic segmentation in addition to decompression to obtain the images. We experimentally validate the proposed pipeline on the Cityscapes dataset, where we achieve a compression factor up to $66 \times$ while preserving the information required to perform segmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved using decompressed images while reducing the overall compute by $11\%$.
</details>
<details>
<summary>摘要</summary>
自动驾驶车和高级驾驶助手系统（ADAS）有可能改变我们的旅行方式。许多这些车辆目前都使用分割和对象检测算法来检测和跟踪周围的对象。收集到的数据通常会被发送到云服务器以便持续/人生学习这些算法。由于带宽约束，数据通常会被压缩后发送到服务器，其中它们通常会被解压缩以进行训练和分析。在这种情况下，我们提议使用学习基于压缩编码器来减少标准管道中的延迟过载。我们示出了learned压缩表示可以用于实现像semantic segmentation这样的任务，而不需要解压缩。我们对Cityscapes数据集进行实验，并实现了最多$66\times$的压缩因子，保留了需要进行分割的信息，并且将compute总体减少$11\%$。
</details></li>
</ul>
<hr>
<h2 id="Deep-Attention-Q-Network-for-Personalized-Treatment-Recommendation"><a href="#Deep-Attention-Q-Network-for-Personalized-Treatment-Recommendation" class="headerlink" title="Deep Attention Q-Network for Personalized Treatment Recommendation"></a>Deep Attention Q-Network for Personalized Treatment Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01519">http://arxiv.org/abs/2307.01519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevenmsm/rl-icu-daqn">https://github.com/stevenmsm/rl-icu-daqn</a></li>
<li>paper_authors: Simin Ma, Junghwan Lee, Nicoleta Serban, Shihao Yang</li>
<li>for: 这篇论文旨在提供个性化治疗建议，以实现医疗结果最佳化。</li>
<li>methods: 本研究使用深度注意力Q网络（DAQN），利用对应架构内的强化学习框架，高效地包含所有过去病人观察数据。</li>
<li>results: 比较先前的模型，本研究的DAQN模型在实际世界的 septic shock 和急性低血压患者群中表现出色，显示其超越性。<details>
<summary>Abstract</summary>
Tailoring treatment for individual patients is crucial yet challenging in order to achieve optimal healthcare outcomes. Recent advances in reinforcement learning offer promising personalized treatment recommendations; however, they rely solely on current patient observations (vital signs, demographics) as the patient's state, which may not accurately represent the true health status of the patient. This limitation hampers policy learning and evaluation, ultimately limiting treatment effectiveness. In this study, we propose the Deep Attention Q-Network for personalized treatment recommendations, utilizing the Transformer architecture within a deep reinforcement learning framework to efficiently incorporate all past patient observations. We evaluated the model on real-world sepsis and acute hypotension cohorts, demonstrating its superiority to state-of-the-art models. The source code for our model is available at https://github.com/stevenmsm/RL-ICU-DAQN.
</details>
<details>
<summary>摘要</summary>
个人化治疗是现代医疗的关键，但是实现优化医疗效果却是挑战。 latest advances in reinforcement learning 提供了个人化治疗建议的可能性，但是它们只基于当前患者的观察数据（生命 Parameters, demographics）来定义患者的状态，这可能不准确地反映患者的真实健康状况。这种限制策略学习和评估，最终限制了治疗效果。在这项研究中，我们提出了 Deep Attention Q-Network，使用 transformer 架构在深度强化学习框架中高效地包含所有过去患者的观察数据。我们对现实世界的 septic shock 和急性低血压群体进行了评估，并证明了我们的模型在现有模型之上表现出色。我们的模型的源代码可以在 https://github.com/stevenmsm/RL-ICU-DAQN 上获取。
</details></li>
</ul>
<hr>
<h2 id="SelfFed-Self-supervised-Federated-Learning-for-Data-Heterogeneity-and-Label-Scarcity-in-IoMT"><a href="#SelfFed-Self-supervised-Federated-Learning-for-Data-Heterogeneity-and-Label-Scarcity-in-IoMT" class="headerlink" title="SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT"></a>SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01514">http://arxiv.org/abs/2307.01514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunder Ali Khowaja, Kapal Dev, Syed Muhammad Anwar, Marius George Linguraru</li>
<li>for: 这个研究旨在提出一个基于自适应学习的联邦学习框架，以实现在对没有标签的隔离数据上进行协同学习。</li>
<li>methods: 我们提出了一个名为SelfFed的框架，它包括两个阶段：首先是预训阶段，使用Swin Transformer基本Encoder进行增强模型，在分散式的方式下进行执行。其次是精度调整阶段，引入对照网络和一个新的聚合策略，在分散式的方式下进行训练，以解决标签稀缺问题。</li>
<li>results: 我们在公共可用的医疗图像数据集上进行实验分析，结果显示，我们的提出的SelfFed框架在非Identical和相似数据（IID） dataset上比基于已有的基eline出perform得更好，具体的提高8.8%和4.1%在Retina和COVID-FL数据集上。此外，我们的方法甚至在仅使用10%标签的情况下也能超越现有的基eline。<details>
<summary>Abstract</summary>
Self-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem. We perform our experimental analysis on publicly available medical imaging datasets and show that our proposed SelfFed framework performs better when compared to existing baselines concerning non-independent and identically distributed (IID) data and label scarcity. Our method achieves a maximum improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID dataset. Further, our proposed method outperforms existing baselines even when trained on a few (10%) labeled instances.
</details>
<details>
<summary>摘要</summary>
“自我指导学习在联合学习框架中得到了产业和研究领域的广泛关注，因为它可以在不同数据源上进行协同学习，无需标签数据。然而，基于自我指导学习的联合学习策略受到数据不均衡和标签稀缺的限制，即数据多样性问题。在本文中，我们提出了基于互联网医疗器件（IoMT）的SelfFed框架。我们的提议的SelfFed框架分为两个阶段。第一阶段是预训练阶段，使用Swin Transformer基于编码器进行增强模型，在分布式方式下进行。第一阶段的SelfFed框架帮助解决数据多样性问题。第二阶段是精度调整阶段，引入对照网络和一种新的聚合策略，在分布式方式下进行限制标签数据的训练。这个精度调整阶段帮助解决标签稀缺问题。我们在公开available的医学成像数据集上进行实验分析，并证明我们的提议的SelfFed框架在非Identical和相似数据（IID）下性能更好，提高了8.8%和4.1%的提升。此外，我们的提议方法还能在只有10%标签实例的情况下超越现有的基准值。”
</details></li>
</ul>
<hr>
<h2 id="Relation-aware-graph-structure-embedding-with-co-contrastive-learning-for-drug-drug-interaction-prediction"><a href="#Relation-aware-graph-structure-embedding-with-co-contrastive-learning-for-drug-drug-interaction-prediction" class="headerlink" title="Relation-aware graph structure embedding with co-contrastive learning for drug-drug interaction prediction"></a>Relation-aware graph structure embedding with co-contrastive learning for drug-drug interaction prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01507">http://arxiv.org/abs/2307.01507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengying Jiang, Guizhong Liu, Biao Zhao, Yuanchao Su, Weiqiang Jin</li>
<li>for: 预测多种关系 drug-drug interaction (DDIs)</li>
<li>methods: 使用 relation-aware graph structure embedding (RaGSE) with co-contrastive learning</li>
<li>results: 在三个任务上比前state-of-the-art方法表现出色，得到更好的预测结果<details>
<summary>Abstract</summary>
Relation-aware graph structure embedding is promising for predicting multi-relational drug-drug interactions (DDIs). Typically, most existing methods begin by constructing a multi-relational DDI graph and then learning relation-aware graph structure embeddings (RaGSEs) of drugs from the DDI graph. Nevertheless, most existing approaches are usually limited in learning RaGSEs of new drugs, leading to serious over-fitting when the test DDIs involve such drugs. To alleviate this issue, we propose a novel DDI prediction method based on relation-aware graph structure embedding with co-contrastive learning, RaGSECo. The proposed RaGSECo constructs two heterogeneous drug graphs: a multi-relational DDI graph and a multi-attribute drug-drug similarity (DDS) graph. The two graphs are used respectively for learning and propagating the RaGSEs of drugs, aiming to ensure all drugs, including new ones, can possess effective RaGSEs. Additionally, we present a novel co-contrastive learning module to learn drug-pairs (DPs) representations. This mechanism learns DP representations from two distinct views (interaction and similarity views) and encourages these views to supervise each other collaboratively to obtain more discriminative DP representations. We evaluate the effectiveness of our RaGSECo on three different tasks using two real datasets. The experimental results demonstrate that RaGSECo outperforms existing state-of-the-art prediction methods.
</details>
<details>
<summary>摘要</summary>
“关系意识的图结构嵌入显示了在预测多关系药物交互（DDIs）方面的承诺。通常，现有的方法都是从多关系DDIs图构建起来，然后学习关系意识图结构嵌入（RaGSEs）。然而，这些方法通常只能学习新药物的RaGSEs，导致在测试DDIs中严重过拟合。为解决这个问题，我们提出了一种基于关系意识图结构嵌入和协同对比学习的新DDIs预测方法，即RaGSECo。提案的RaGSECo构建了两个不同类型的药物图：一个多关系DDIs图和一个多属性药物对比图。这两个图用于学习和传播药物的RaGSEs，以确保所有药物，包括新的一些，都可以具有有效的RaGSEs。此外，我们还提出了一种新的协同对比学习模块，用于学习药物对的表示。这个机制从两种不同的视图（交互视图和相似视图）中学习药物对的表示，并且使这两个视图相互监督each other以获得更有特征的药物对表示。我们使用三个不同的任务和两个真实数据集进行了实验，结果显示，RaGSECo在这些任务中表现出了更高的效果。”
</details></li>
</ul>
<hr>
<h2 id="All-in-One-Multi-task-Prompting-for-Graph-Neural-Networks"><a href="#All-in-One-Multi-task-Prompting-for-Graph-Neural-Networks" class="headerlink" title="All in One: Multi-task Prompting for Graph Neural Networks"></a>All in One: Multi-task Prompting for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01504">http://arxiv.org/abs/2307.01504</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sheldonresearch/ProG">https://github.com/sheldonresearch/ProG</a></li>
<li>paper_authors: Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan</li>
<li>for: 填充预训练模型的知识空间，以便更好地应对不同的图任务。</li>
<li>methods: 提出了一种基于多 зада务提问的图模型提问方法，包括对图提问和自然语言提问的融合、对图任务的重新定义以适应预训练模型，以及使用元学习来快速学习更好的初始化方法。</li>
<li>results: 经过广泛的实验，结果表明该方法可以在不同的图任务上达到更高的性能。<details>
<summary>Abstract</summary>
Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.
</details>
<details>
<summary>摘要</summary>
近些时候，“预训练和精度调整”成为了许多图任务的标准工作流程，因为它可以帮助图模型学习通用的图知识，从而缓解每个应用程序缺乏图注释的问题。然而，图任务中的节点层、边层和图层具有广泛的多样性，这些预训练预TeX often incompatible with these multiple tasks，这可能会导致“负面传播”，从而影响特定应用程序的结果。 inspirited by the prompt learning in natural language processing (NLP), which has shown significant effectiveness in leveraging prior knowledge for various NLP tasks, we investigate the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks.在这篇论文中，我们提出了一种新的多任务提问方法 для图模型。具体来说，我们首先将图提问和语言提问的格式统一为Prompt Token、Token结构和插入模式。这样，NLP中的提问思想可以轻松地在图领域中引入。然后，为了进一步缩小不同图任务和当前预训练策略之间的差距，我们进一步研究了各种图应用程序的任务空间，并重新表述下游问题为图级任务。最后，我们引入了元学习，以更有效地学习多任务提问中的初始化，以使我们的提问框架更可靠和通用于不同任务。我们进行了广泛的实验，实验结果表明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-stochastic-approximation-with-state-dependent-noise"><a href="#Accelerated-stochastic-approximation-with-state-dependent-noise" class="headerlink" title="Accelerated stochastic approximation with state-dependent noise"></a>Accelerated stochastic approximation with state-dependent noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01497">http://arxiv.org/abs/2307.01497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sasila Ilandarideva, Anatoli Juditsky, Guanghui Lan, Tianjiao Li</li>
<li>for:  solves a class of stochastic smooth convex optimization problems with general noise assumptions.</li>
<li>methods:  uses two non-Euclidean accelerated stochastic approximation routines: stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE).</li>
<li>results:  achieves the optimal convergence rate and attains the optimal iteration and sample complexities simultaneously, with more general assumptions for SGE that allow for efficient application to statistical estimation problems under heavy tail noises and discontinuous score functions.<details>
<summary>Abstract</summary>
We consider a class of stochastic smooth convex optimization problems under rather general assumptions on the noise in the stochastic gradient observation. As opposed to the classical problem setting in which the variance of noise is assumed to be uniformly bounded, herein we assume that the variance of stochastic gradients is related to the "sub-optimality" of the approximate solutions delivered by the algorithm. Such problems naturally arise in a variety of applications, in particular, in the well-known generalized linear regression problem in statistics. However, to the best of our knowledge, none of the existing stochastic approximation algorithms for solving this class of problems attain optimality in terms of the dependence on accuracy, problem parameters, and mini-batch size.   We discuss two non-Euclidean accelerated stochastic approximation routines--stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE)--which carry a particular duality relationship. We show that both SAGD and SGE, under appropriate conditions, achieve the optimal convergence rate, attaining the optimal iteration and sample complexities simultaneously. However, corresponding assumptions for the SGE algorithm are more general; they allow, for instance, for efficient application of the SGE to statistical estimation problems under heavy tail noises and discontinuous score functions. We also discuss the application of the SGE to problems satisfying quadratic growth conditions, and show how it can be used to recover sparse solutions. Finally, we report on some simulation experiments to illustrate numerical performance of our proposed algorithms in high-dimensional settings.
</details>
<details>
<summary>摘要</summary>
我团队考虑了一类泛化噪声 convex 优化问题，其中噪声噪声度受到较为一般的假设。与经典问题设定相比，我们假设噪声 variance 与优化解的"低效"有关。这些问题在各种应用中自然出现，如统计中的泛化线性回归问题。然而，据我们所知，现有的泛化噪声策略并没有达到最佳的依赖于准确性、问题参数和批处大小。我们介绍了两种非欧几何减速泛化策略：噪声加速梯度下降（SAGD）和梯度拓展（SGE）。我们证明了这两种策略，在合适的假设下，都可以达到最佳的准确率，同时具有最佳的迭代次数和批处大小复杂度。然而，SGE 算法的假设更加通用，允许在重 tailed 噪声和离散分数函数的情况下进行有效的应用。我们还讨论了 SGE 在 quadratic growth conditions 下的应用，并示出它可以用来恢复稀疏解。最后，我们报告了一些高维度设置下的仿真实验结果，以 illustrate 我们的提议方法的数值性能。
</details></li>
</ul>
<hr>
<h2 id="Review-of-Deep-Learning-based-Malware-Detection-for-Android-and-Windows-System"><a href="#Review-of-Deep-Learning-based-Malware-Detection-for-Android-and-Windows-System" class="headerlink" title="Review of Deep Learning-based Malware Detection for Android and Windows System"></a>Review of Deep Learning-based Malware Detection for Android and Windows System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01494">http://arxiv.org/abs/2307.01494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazmul Islam, Seokjoo Shin</li>
<li>for: 遥测和区分不同种类的黑客病毒，以评估其行为和威胁水平，并发展防御策略。</li>
<li>methods: 使用人工智能技术（AI）为抗黑客系统，以应对不同类型的隐藏和混淆技术。</li>
<li>results: 实验结果显示，使用AI技术可以实现百分之百的检测精度，探测不同类型的黑客病毒。<details>
<summary>Abstract</summary>
Differentiating malware is important to determine their behaviors and level of threat; as well as to devise defensive strategy against them. In response, various anti-malware systems have been developed to distinguish between different malwares. However, most of the recent malware families are Artificial Intelligence (AI) enable and can deceive traditional anti-malware systems using different obfuscation techniques. Therefore, only AI-enabled anti-malware system is robust against these techniques and can detect different features in the malware files that aid in malicious activities. In this study we review two AI-enabled techniques for detecting malware in Windows and Android operating system, respectively. Both the techniques achieved perfect accuracy in detecting various malware families.
</details>
<details>
<summary>摘要</summary>
不同的黑客软件有不同的行为和威胁水平，因此可以通过区分黑客软件来制定防御策略。然而，大多数最新的黑客软件家族具有人工智能（AI）功能，可以使用不同的隐蔽技术欺骗传统的防病软件。因此，只有使用AI技术的防病软件才能够对这些技术进行鲜活的检测和区分。本研究将介绍两种基于AI技术的防病方法，一种用于Windows操作系统，另一种用于Android操作系统。两种方法均达到了完美的检测精度，可以帮助检测不同的黑客软件家族。
</details></li>
</ul>
<hr>
<h2 id="FREEDOM-Target-Label-Source-Data-Domain-Information-Free-Multi-Source-Domain-Adaptation-for-Unsupervised-Personalization"><a href="#FREEDOM-Target-Label-Source-Data-Domain-Information-Free-Multi-Source-Domain-Adaptation-for-Unsupervised-Personalization" class="headerlink" title="FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization"></a>FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02493">http://arxiv.org/abs/2307.02493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunju Yang, Gyusang Cho, Chan-Hyun Youn<br>for:* 这个研究是为了解决多源领域适应（Multi-Source Domain Adaptation，MSDA）中的问题，特别是在没有目标标签和多个领域的情况下进行模型适应。methods:* 这个研究提出了一个新的问题场景，即Three-Free Domain Adaptation（TFDA），在这个问题场景下，目标标签、源数据集和源领域资讯（领域标签和领域数量）都是不可用的。* 这个研究提出了一个实用的适应框架，called FREEDOM，它利用生成模型，将数据分解为类别和样式的两个方面，并使用非Parametric Bayesian方法来定义样式。在适应阶段，FREEDOM尝试将源类别分布与目标类别分布匹配，然后只部署部分的分类模型为个人化网络。results:* 这个研究获得了state-of-the-art或相等的性能，而且可以在没有领域资讯的情况下进行适应，并且将终端模型的大小减少到目标边缘。<details>
<summary>Abstract</summary>
From a service perspective, Multi-Source Domain Adaptation (MSDA) is a promising scenario to adapt a deployed model to a client's dataset. It can provide adaptation without a target label and support the case where a source dataset is constructed from multiple domains. However, it is impractical, wherein its training heavily relies on prior domain information of the multi-source dataset -- how many domains exist and the domain label of each data sample. Moreover, MSDA requires both source and target datasets simultaneously (physically), causing storage limitations on the client device or data privacy issues by transferring client data to a server. For a more practical scenario of model adaptation from a service provider's point of view, we relax these constraints and present a novel problem scenario of Three-Free Domain Adaptation, namely TFDA, where 1) target labels, 2) source dataset, and mostly 3) source domain information (domain labels + the number of domains) are unavailable. Under the problem scenario, we propose a practical adaptation framework called FREEDOM. It leverages the power of the generative model, disentangling data into class and style aspects, where the style is defined as the class-independent information from the source data and designed with a nonparametric Bayesian approach. In the adaptation stage, FREEDOM aims to match the source class distribution with the target's under the philosophy that class distribution is consistent even if the style is different; after then, only part of the classification model is deployed as a personalized network. As a result, FREEDOM achieves state-of-the-art or comparable performance even without domain information, with reduced final model size on the target side, independent of the number of source domains.
</details>
<details>
<summary>摘要</summary>
从服务角度来看，多源频率适应（MSDA）是一个有前途的场景，用于适应已部署模型到客户的数据集。它可以无需目标标签进行适应，并且支持多个源频率构建的情况。然而，它在训练中强依赖于多个源频率数据集的先前知识，以及每个数据样本的频率标签。此外，MSDA需要同时使用源和目标数据集（物理上），导致客户设备存储限制或数据隐私问题。为了更实际的模型适应场景，我们宽松了这些限制，并提出了一个新的问题场景：三自频率适应（TFDA），其中1）目标标签，2）源数据集，以及3）源频率信息（频率标签和频率数量）都不可用。在这种问题场景下，我们提出了一个实用的适应框架called FREEDOM。它利用了生成模型的力量，将数据分解成类和风格两个方面，其中风格被定义为来源数据中独立于类的信息，并使用非 Parametric Bayesian方法设计。在适应阶段，FREEDOM的目标是匹配源类分布与目标类分布，以哲学的思想，即类分布在风格不同的情况下仍然一致。然后，FREEDOM只部署一部分的分类模型作为个性化网络。因此，FREEDOM可以在无需源频率信息的情况下实现状态前或相当的性能，并且减少了目标模型的最终大小，不受源频率数量的影响。
</details></li>
</ul>
<hr>
<h2 id="Nexus-sine-qua-non-Essentially-Connected-Networks-for-Traffic-Forecasting"><a href="#Nexus-sine-qua-non-Essentially-Connected-Networks-for-Traffic-Forecasting" class="headerlink" title="Nexus sine qua non: Essentially Connected Networks for Traffic Forecasting"></a>Nexus sine qua non: Essentially Connected Networks for Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01482">http://arxiv.org/abs/2307.01482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Nie, Guoyang Qin, Lijun Sun, Yunpeng Wang, Jian Sun</li>
<li>for: 本研究旨在开发简洁高效的神经网络模型，用于learnings representations和预测交通数据中的下一个时刻行为。</li>
<li>methods: 本研究使用了spatiotemporal graph neural networks (STGNNs)，但是现有STGNNs使用复杂的技术来捕捉交通数据中的结构，导致它们难以理解和扩展。因此，研究人员寻求了简单 yet efficient的architecture。研究人员发现了STGNN的表示中的核心是certain forms of spatiotemporal contextualization，并根据此设计了一种简单的efficient message-passing backbone，即Nexus sine qua non (NexuSQN)。</li>
<li>results: 研究人员发现，NexuSQN比较简单的结构，即使不使用复杂的RNNs、Transformers和diffusion convolutions，仍能在计算效率、精度和大小等方面超越了复杂的参考模型。这表明，将来可能有一个Promising future for developing simple yet efficient neural predictors。<details>
<summary>Abstract</summary>
Spatiotemporal graph neural networks (STGNNs) have emerged as a leading approach for learning representations and forecasting on traffic datasets with underlying topological and correlational structures. However, current STGNNs use intricate techniques with high complexities to capture these structures, making them difficult to understand and scale. The existence of simple yet efficient architectures remains an open question. Upon closer examination, we find what lies at the core of STGNN's representations are certain forms of spatiotemporal contextualization. In light of this, we design Nexus sine qua non (NexuSQN), an essentially connected network built on an efficient message-passing backbone. NexuSQN simply uses learnable "where" and "when" locators for the aforementioned contextualization and omits any intricate components such as RNNs, Transformers, and diffusion convolutions. Results show that NexuSQN outperforms intricately designed benchmarks in terms of size, computational efficiency, and accuracy. This suggests a promising future for developing simple yet efficient neural predictors.
</details>
<details>
<summary>摘要</summary>
现代各种图 neural networks (STGNNs) 已经成为学习表示和预测交通数据中的底层拓扑和相关结构的领先方法。然而，当前的 STGNNs 使用复杂的技术来捕捉这些结构，这使得它们变得难以理解和扩展。有效且简单的架构的存在仍然是一个开放的问题。经过仔细分析，我们发现 STGNN 的表示核心是一种特定的空间时间嵌入。基于这一点，我们设计了 Nexus sine qua non (NexuSQN)，一种简单而高效的网络。NexuSQN 使用学习的 "where" 和 "when" 嵌入来进行上述嵌入，并且不包含任何复杂的组件，如 RNNs、Transformers 和扩散卷积。结果表明，NexuSQN 在Size、计算效率和准确性三个方面超过了复杂设计的标准准。这表示在发展简单且高效的神经预测器方面，有一个广阔的未来。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Conservatism-Diffusion-Policies-in-Offline-Multi-agent-Reinforcement-Learning"><a href="#Beyond-Conservatism-Diffusion-Policies-in-Offline-Multi-agent-Reinforcement-Learning" class="headerlink" title="Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning"></a>Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01472">http://arxiv.org/abs/2307.01472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoran Li, Ling Pan, Longbo Huang</li>
<li>for: 本研究提出了一种新的多智能体偏好离线模型（DOM2），用于多智能体学习 reinforcement learning（MARL）环境中的离线学习。</li>
<li>methods: 在本研究中，我们将diffusion模型integrated into the policy network，并提出了一种基于轨迹的数据增强方案。这些关键元素使得我们的算法更加鲁棒对环境变化，并实现了 significiant improvements in performance, generalization和数据效率。</li>
<li>results: 我们的实验结果表明，DOM2在多智能体粒子和多智能体MuJoCo环境中比 existed state-of-the-art方法表现出更高的表现，并在shifted环境中具有更高的表现和更好的泛化能力。此外，DOM2也表现出了更高的数据效率，可以在$20++$ times less data的情况下达到state-of-the-art表现。<details>
<summary>Abstract</summary>
We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms that rely mainly on conservatism in policy design, DOM2 enhances policy expressiveness and diversity based on diffusion. Specifically, we incorporate a diffusion model into the policy network and propose a trajectory-based data-augmentation scheme in training. These key ingredients make our algorithm more robust to environment changes and achieve significant improvements in performance, generalization and data-efficiency. Our extensive experimental results demonstrate that DOM2 outperforms existing state-of-the-art methods in multi-agent particle and multi-agent MuJoCo environments, and generalizes significantly better in shifted environments thanks to its high expressiveness and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve state-of-the-art performance with $20+$ times less data compared to existing algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的扩散停机多智能体模型（DOM2），用于停机多智能体学习（MARL）。与现有算法不同，我们的算法不仅仅依靠保守性在策略设计中，而是通过扩散来增强策略表达能力和多样性。具体来说，我们在策略网络中 интегrollo了扩散模型，并提出了一种基于轨迹的数据扩充方案在训练中。这些关键元素使我们的算法更加鲁棒对环境变化，并在性能、泛化和数据效率方面达到了显著的改进。我们的广泛的实验结果表明，DOM2在多体分子和多体MuJoCo环境中比现有状态的方法表现出色，并在偏shifted环境中具有更高的表达能力和多样性。此外，DOM2还表现出了更好的数据效率，可以在$20++$times less data的情况下达到状态顶尖的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Driver-Gaze-Estimation-and-Application-in-Gaze-Behavior-Understanding"><a href="#A-Review-of-Driver-Gaze-Estimation-and-Application-in-Gaze-Behavior-Understanding" class="headerlink" title="A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding"></a>A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01470">http://arxiv.org/abs/2307.01470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Kumar Sharma, Pranamesh Chakraborty</li>
<li>for: 本研究的主要目标是对driver gaze基础知识、测量方法和实际驾驶场景中的应用进行全面的总结。</li>
<li>methods: 本研究使用了头戴式和远程设置基于眼动估算的方法，以及与这些数据收集方法相关的术语。然后列出了现有的参考驾驶员眼动数据集，并讲述了数据收集方法和设备使用的方法。最后，本研究讲述了用于眼动估算的算法，主要是传统机器学习和深度学习基本方法。</li>
<li>results: 估算的驾驶员眼动被用于理解在交叉路口、上坡入口、下坡出口、车道变换和道路广告结构的影响。而且，本研究还讲述了现有文献中的限制、挑战和未来发展预cast。<details>
<summary>Abstract</summary>
Driver gaze plays an important role in different gaze-based applications such as driver attentiveness detection, visual distraction detection, gaze behavior understanding, and building driver assistance system. The main objective of this study is to perform a comprehensive summary of driver gaze fundamentals, methods to estimate driver gaze, and it's applications in real world driving scenarios. We first discuss the fundamentals related to driver gaze, involving head-mounted and remote setup based gaze estimation and the terminologies used for each of these data collection methods. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and the equipment used for such data collection. This is followed by a discussion of the algorithms used for driver gaze estimation, which primarily involves traditional machine learning and deep learning based techniques. The estimated driver gaze is then used for understanding gaze behavior while maneuvering through intersections, on-ramps, off-ramps, lane changing, and determining the effect of roadside advertising structures. Finally, we have discussed the limitations in the existing literature, challenges, and the future scope in driver gaze estimation and gaze-based applications.
</details>
<details>
<summary>摘要</summary>
Driver's gaze  plays an important role in various gaze-based applications, such as detecting driver attentiveness, visual distraction, and understanding gaze behavior. The main objective of this study is to provide a comprehensive overview of driver gaze fundamentals, methods for estimating driver gaze, and its applications in real-world driving scenarios.First, we discuss the fundamentals of driver gaze, including head-mounted and remote setup-based gaze estimation, and the terminologies used for each data collection method. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and equipment used for such data collection.Then, we discuss the algorithms used for driver gaze estimation, primarily involving traditional machine learning and deep learning-based techniques. The estimated driver gaze is used to understand gaze behavior while maneuvering through intersections, on-ramps, off-ramps, lane changing, and the effect of roadside advertising structures.Finally, we discuss the limitations in the existing literature, challenges, and the future scope in driver gaze estimation and gaze-based applications.
</details></li>
</ul>
<hr>
<h2 id="Causal-Reinforcement-Learning-A-Survey"><a href="#Causal-Reinforcement-Learning-A-Survey" class="headerlink" title="Causal Reinforcement Learning: A Survey"></a>Causal Reinforcement Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01452">http://arxiv.org/abs/2307.01452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang</li>
<li>for: 本研究写作的目的是对于 causal reinforcement learning 的文献审查和概述。</li>
<li>methods: 本文使用的方法包括 introducing basic concepts of causality and reinforcement learning, 以及 categorizing and systematically reviewing existing causal reinforcement learning approaches based on their target problems and methodologies.</li>
<li>results: 本文审查了 current literature on causal reinforcement learning, 并发现了 several open issues and future directions in this emerging field.<details>
<summary>Abstract</summary>
Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcement learning. We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning. We categorize and systematically review existing causal reinforcement learning approaches based on their target problems and methodologies. Finally, we outline open issues and future directions in this emerging field.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过下面的文本翻译到简化中文：<</SYS>>Control learning是一种重要的思想方式，用于解决带有不确定性的顺序决策问题。虽然在过去几十年内，有很多出色的成果，但是在实际应用中仍然存在很多挑战。其中一个主要的障碍是控制学学习代理不具备世界的基本理解，因此需要通过大量的尝试和错误互动来学习。它们还可能面临着解释决策的挑战和掌握知识的一致性问题。然而， causality 提供了一种明显的优势，即可以系统地ormalize知识，并利用不变性来实现有效的知识传递。这导致了 causal reinforcement learning 的出现，这是一种尝试将 causality integrated 到学习过程中的一种新领域。在这篇评论中，我们全面评论了 literature 中的 causal reinforcement learning 研究。我们首先介绍了 causality 和 reinforcement learning 的基本概念，然后解释了如何通过 causality 解决非 causal reinforcement learning 中的核心挑战。然后，我们按照目标问题和方法分类系统地审查了现有的 causal reinforcement learning 方法。最后，我们列出了未解决的问题和未来的方向。
</details></li>
</ul>
<hr>
<h2 id="A-Double-Machine-Learning-Approach-to-Combining-Experimental-and-Observational-Data"><a href="#A-Double-Machine-Learning-Approach-to-Combining-Experimental-and-Observational-Data" class="headerlink" title="A Double Machine Learning Approach to Combining Experimental and Observational Data"></a>A Double Machine Learning Approach to Combining Experimental and Observational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01449">http://arxiv.org/abs/2307.01449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Morucci, Vittorio Orlandi, Harsh Parikh, Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky</li>
<li>for: 该论文旨在提出一种将实验和观察研究结合起来的双机器学习方法，以便实践者可以一起测试假设的满足性和对治疗效果的估计。</li>
<li>methods: 该方法使用双机器学习技术将实验和观察研究结合起来，以检测假设的满足性和外部有效性的违反。当只有一个假设被违反时，我们提供了半 Parametric 有效的治疗效果估计器。</li>
<li>results: 该研究在三个实际应用场景中展示了其适用性，并指出了准确地识别违反假设的重要性以确保治疗效果的估计的重要性。<details>
<summary>Abstract</summary>
Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
</details>
<details>
<summary>摘要</summary>
实验和观察研究经常受到有效性问题，因为假设通常无法被证明。我们提出了一种双机器学习方法，可以结合实验和观察研究，让实践者可以测试假设违背和估计治疗效果一致。我们的框架测试了外部有效性和无知性的违背，假设较弱的假设违背。只有一个假设违背时，我们提供了半 parametrically有效的治疗效果估计器。但我们的无免责 theorem 显示，精确地识别违背的假设是估计治疗效果一致的必要条件。我们在三个实际应用中例子中详细介绍了我们的方法，强调了它在实践中的重要性。
</details></li>
</ul>
<hr>
<h2 id="On-Conditional-and-Compositional-Language-Model-Differentiable-Prompting"><a href="#On-Conditional-and-Compositional-Language-Model-Differentiable-Prompting" class="headerlink" title="On Conditional and Compositional Language Model Differentiable Prompting"></a>On Conditional and Compositional Language Model Differentiable Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01446">http://arxiv.org/abs/2307.01446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jpilaul/PRopS">https://github.com/jpilaul/PRopS</a></li>
<li>paper_authors: Jonathan Pilault, Can Liu, Mohit Bansal, Markus Dreyer</li>
<li>for: 本研究旨在调整静态语言模型（PLM），以便在下游任务中表现出色。</li>
<li>methods: 本研究使用 conditional和compositional differentiable prompting，并提出了一种新的模型——Prompt Production System（PRopS），可以将任务说明或输入元数据转换成细化的Continuous prompts，以便从PLM中获得任务特定的输出。PRopS使用基于神经网络的Production Systems结构，可以学习不同的提示输入模式，进行可compose转换，适用于小样本学习和过渡学习。</li>
<li>results: 研究表明，PRopS可以在compositional generalization任务、可控摘要和多语言翻译中，Consistently exceed other PLM adaptation techniques，并经常超越完全精心调整模型。同时，PRopS需要 fewer trainable parameters，适合实际应用。<details>
<summary>Abstract</summary>
Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine-tuned models, on compositional generalization tasks, controllable summarization and multilingual translation, while needing fewer trainable parameters.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>文本：提示已经被证明是一种有效的方法，用于适应预训练语言模型（PLM）在下游任务中表现良好。提示可以表示为人工设计的单词序列或学习到的连续嵌入。在这种工作中，我们研究了决定式和组合的可导提示。我们提出了一种新的模型，提示生产系统（PRopS），该模型学习将任务指令或输入元数据转换为可导的提示，以便从PLM中获取任务特定的输出。我们的模型采用基于我们的神经网络表述的生产系统结构，该结构允许模型学习分解规则——神经函数学习特定提示输入模式的特殊化，使其适用于组合转移学习和少量学习。我们进行了广泛的实验和理论分析，并证明了PRopS在组合泛化任务、可控概要摘要和多语言翻译中表现出色，而需要 fewer 可训练参数。
</details></li>
</ul>
<hr>
<h2 id="Human-Emotion-Recognition-Based-On-Galvanic-Skin-Response-signal-Feature-Selection-and-SVM"><a href="#Human-Emotion-Recognition-Based-On-Galvanic-Skin-Response-signal-Feature-Selection-and-SVM" class="headerlink" title="Human Emotion Recognition Based On Galvanic Skin Response signal Feature Selection and SVM"></a>Human Emotion Recognition Based On Galvanic Skin Response signal Feature Selection and SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05383">http://arxiv.org/abs/2307.05383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Fan, Mingyang Liu, Xiaohan Zhang, Xiaopeng Gong</li>
<li>for: 本研究提出了一种基于自动选择的 galvanic skin response (GSR) 信号特征和 Support Vector Machine (SVM) 的人类情感识别方法。</li>
<li>methods: 研究使用 e-Health Sensor Platform V2.0 获取 GSR 信号，然后使用浮点函数除噪和normalize 处理数据，提取30个特征。然后，使用协方差基于的特征选择来优化特征。最后，使用 SVM 输入优化特征进行人类情感识别。</li>
<li>results: 实验结果表明，提出的方法可以实现好的人类情感识别，识别率高于 66.67%。<details>
<summary>Abstract</summary>
A novel human emotion recognition method based on automatically selected Galvanic Skin Response (GSR) signal features and SVM is proposed in this paper. GSR signals were acquired by e-Health Sensor Platform V2.0. Then, the data is de-noised by wavelet function and normalized to get rid of the individual difference. 30 features are extracted from the normalized data, however, directly using of these features will lead to a low recognition rate. In order to gain the optimized features, a covariance based feature selection is employed in our method. Finally, a SVM with input of the optimized features is utilized to achieve the human emotion recognition. The experimental results indicate that the proposed method leads to good human emotion recognition, and the recognition accuracy is more than 66.67%.
</details>
<details>
<summary>摘要</summary>
本文提出了一种基于自动选择的galvanic skin response（GSR）信号特征和支持向量机（SVM）的人类情感识别方法。GSR信号通过e-Health感知平台V2.0获取。然后，数据进行杂谱函数滤波和normalizaation处理，以消除个体差异。从 нормализов的数据中提取了30个特征，但直接使用这些特征将导致低的识别率。为了获得优化的特征，我们在方法中使用covariance基于的特征选择。最后，使用输入优化特征的SVM实现人类情感识别。实验结果表明，提出的方法可以实现良好的人类情感识别，识别率高于66.67%。
</details></li>
</ul>
<hr>
<h2 id="TablEye-Seeing-small-Tables-through-the-Lens-of-Images"><a href="#TablEye-Seeing-small-Tables-through-the-Lens-of-Images" class="headerlink" title="TablEye: Seeing small Tables through the Lens of Images"></a>TablEye: Seeing small Tables through the Lens of Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02491">http://arxiv.org/abs/2307.02491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seung-eon Lee, Sang-Chul Lee</li>
<li>for: 这个论文目的是解决几个板表学习问题，具体来说是在几个板表数据上培养模型，而不需要大量标签数据。</li>
<li>methods: 这个论文使用的方法是基于域转换的，通过生成板表图像来保持原始板表数据的内在 semantics。然后使用已经测试过的几个shot学习算法和嵌入函数来获得和应用优先知识。</li>
<li>results: 这个论文的结果表明，TablEye在4个shot任务中的最高AUC为0.11，在1个shot设置中的平均错误率高于TabLLM by 3.17%。这表明TablEye在几个板表数据上具有更好的性能。<details>
<summary>Abstract</summary>
The exploration of few-shot tabular learning becomes imperative. Tabular data is a versatile representation that captures diverse information, yet it is not exempt from limitations, property of data and model size. Labeling extensive tabular data can be challenging, and it may not be feasible to capture every important feature. Few-shot tabular learning, however, remains relatively unexplored, primarily due to scarcity of shared information among independent datasets and the inherent ambiguity in defining boundaries within tabular data. To the best of our knowledge, no meaningful and unrestricted few-shot tabular learning techniques have been developed without imposing constraints on the dataset. In this paper, we propose an innovative framework called TablEye, which aims to overcome the limit of forming prior knowledge for tabular data by adopting domain transformation. It facilitates domain transformation by generating tabular images, which effectively conserve the intrinsic semantics of the original tabular data. This approach harnesses rigorously tested few-shot learning algorithms and embedding functions to acquire and apply prior knowledge. Leveraging shared data domains allows us to utilize this prior knowledge, originally learned from the image domain. Specifically, TablEye demonstrated a superior performance by outstripping the TabLLM in a 4-shot task with a maximum 0.11 AUC and a STUNT in a 1- shot setting, where it led on average by 3.17% accuracy.
</details>
<details>
<summary>摘要</summary>
exploration of few-shot tabular learning becoming increasingly important. 表格数据是一种多样化表示方式，它可以捕捉多种信息，但同时也有一些限制，例如数据属性和模型大小。对于大量表格数据的标注可能是困难的，而且可能无法捕捉所有重要的特征。然而，几 shot tabular learning仍然尚未得到广泛的探索，主要是因为独立的数据集之间的共享信息缺乏，以及表格数据中的内在含义是不具有明确定义的。根据我们所知，没有任何不受限制的几 shot tabular learning技术已经被开发出来，没有强制要求数据集的限制。在这篇论文中，我们提出了一个创新的框架，即TablEye，以解决表格数据的几 shot learning问题。TablEye采用域转换来解决几 shot learning问题，通过生成表格图像来保留原始表格数据的内在含义。这种方法利用了已经测试过的几 shot学习算法和嵌入函数，以获取和应用先前知识。通过共享数据域，我们可以利用这些先前知识，原来学习自图像领域。特别是，TablEye在4 shot任务中的最大AUC为0.11，在1 shot任务中的平均准确率高于TabLLM的3.17%。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Branch-in-Combinatorial-Optimization-with-Graph-Pointer-Networks"><a href="#Learning-to-Branch-in-Combinatorial-Optimization-with-Graph-Pointer-Networks" class="headerlink" title="Learning to Branch in Combinatorial Optimization with Graph Pointer Networks"></a>Learning to Branch in Combinatorial Optimization with Graph Pointer Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01434">http://arxiv.org/abs/2307.01434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Wang, Zhiming Zhou, Tao Zhang, Ling Wang, Xin Xu, Xiangke Liao, Kaiwen Li</li>
<li>for: 解决 combinatorial optimization 问题的variable选择策略学习</li>
<li>methods: 提出了一种基于图Pointer网络的变量选择策略学习模型，利用图特征、全局特征和历史特征来表示解决器状态</li>
<li>results: 实验表明，提出的方法可以有效地将解决器状态映射到分支变量决策中，并且在各种benchmark问题上显著超越了经典强分支专家规则，同时也超越了当前最佳机器学习基于分支和缓存的方法。<details>
<summary>Abstract</summary>
Branch-and-bound is a typical way to solve combinatorial optimization problems. This paper proposes a graph pointer network model for learning the variable selection policy in the branch-and-bound. We extract the graph features, global features and historical features to represent the solver state. The proposed model, which combines the graph neural network and the pointer mechanism, can effectively map from the solver state to the branching variable decisions. The model is trained to imitate the classic strong branching expert rule by a designed top-k Kullback-Leibler divergence loss function. Experiments on a series of benchmark problems demonstrate that the proposed approach significantly outperforms the widely used expert-designed branching rules. Our approach also outperforms the state-of-the-art machine-learning-based branch-and-bound methods in terms of solving speed and search tree size on all the test instances. In addition, the model can generalize to unseen instances and scale to larger instances.
</details>
<details>
<summary>摘要</summary>
通常的方法之一用于解决 combinatorial optimization 问题是 branch-and-bound。这篇论文提出了一种图像指针网络模型，用于学习变量选择策略在 branch-and-bound 中。我们提取了图像特征、全局特征和历史特征来表示解决器状态。提议的模型，结合图像神经网络和指针机制，可以有效地将解决器状态映射到分支变量决策。模型通过一个设计的 top-k Kullback-Leibler 分布差函数进行训练，以模仿经典的强分支专家规则。实验表明，提议的方法在一系列的 benchmark 问题上显著超越了通用的专家设计的分支规则。我们的方法还超越了当前的Machine Learning 基于 branch-and-bound 方法在所有测试实例上的解决速度和搜索树大小。此外，模型还可以泛化到未看过的实例和更大的实例。
</details></li>
</ul>
<hr>
<h2 id="SleepEGAN-A-GAN-enhanced-Ensemble-Deep-Learning-Model-for-Imbalanced-Classification-of-Sleep-Stages"><a href="#SleepEGAN-A-GAN-enhanced-Ensemble-Deep-Learning-Model-for-Imbalanced-Classification-of-Sleep-Stages" class="headerlink" title="SleepEGAN: A GAN-enhanced Ensemble Deep Learning Model for Imbalanced Classification of Sleep Stages"></a>SleepEGAN: A GAN-enhanced Ensemble Deep Learning Model for Imbalanced Classification of Sleep Stages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05362">http://arxiv.org/abs/2307.05362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuewei Cheng, Ke Huang, Yi Zou, Shujie Ma</li>
<li>for:  automatische slaapfaseclassificatie</li>
<li>methods: GAN-powered ensemble deep learning model (SleepEGAN) en data-augmentatie</li>
<li>results: verbeterde classificatie-accurateit compared to existing state-of-the-art methods using three public sleep datasets.<details>
<summary>Abstract</summary>
Deep neural networks have played an important role in automatic sleep stage classification because of their strong representation and in-model feature transformation abilities. However, class imbalance and individual heterogeneity which typically exist in raw EEG signals of sleep data can significantly affect the classification performance of any machine learning algorithms. To solve these two problems, this paper develops a generative adversarial network (GAN)-powered ensemble deep learning model, named SleepEGAN, for the imbalanced classification of sleep stages. To alleviate class imbalance, we propose a new GAN (called EGAN) architecture adapted to the features of EEG signals for data augmentation. The generated samples for the minority classes are used in the training process. In addition, we design a cost-free ensemble learning strategy to reduce the model estimation variance caused by the heterogeneity between the validation and test sets, so as to enhance the accuracy and robustness of prediction performance. We show that the proposed method can improve classification accuracy compared to several existing state-of-the-art methods using three public sleep datasets.
</details>
<details>
<summary>摘要</summary>
深度神经网络在自动睡眠阶段分类中发挥了重要作用，因为它们具有强大的表示能力和内存中特征转换能力。然而， raw EEG 信号中的分类不均和个体差异通常会对任何机器学习算法的分类性能产生很大的影响。为解决这两个问题，本文提出了基于生成对抗网络（GAN）的 ensemble 深度学习模型，名为 SleepEGAN，用于不均分类睡眠阶段。为了缓解分类不均，我们提出了一种适应 EEG 信号特点的新 GAN 架构（称为 EGAN），用于数据增强。生成的小类样本在训练过程中使用。此外，我们设计了一种免费的ensemble学习策略，以降低因验证集和测试集之间的个体差异而导致的模型估计方差，以提高预测性能的准确性和稳定性。我们示示了提案的方法可以在三个公共睡眠数据集上提高分类精度，比较现有的一些状态之 arts 方法。
</details></li>
</ul>
<hr>
<h2 id="Smart-filter-aided-domain-adversarial-neural-network-An-unsupervised-domain-adaptation-method-for-fault-diagnosis-in-noisy-industrial-scenarios"><a href="#Smart-filter-aided-domain-adversarial-neural-network-An-unsupervised-domain-adaptation-method-for-fault-diagnosis-in-noisy-industrial-scenarios" class="headerlink" title="Smart filter aided domain adversarial neural network: An unsupervised domain adaptation method for fault diagnosis in noisy industrial scenarios"></a>Smart filter aided domain adversarial neural network: An unsupervised domain adaptation method for fault diagnosis in noisy industrial scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01429">http://arxiv.org/abs/2307.01429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baorui Dai, Gaëtan Frusque, Tianfu Li, Qi Li, Olga Fink</li>
<li>for: 这个研究旨在提出一种基于不监督领域适应（Unsupervised Domain Adaptation, UDA）的缺陷诊断方法，以便在实际工业应用中将运作经验和缺陷特征转移到不同的运作条件、不同的机器设备或实际数据和模拟数据之间。</li>
<li>methods: 本研究提出了一种名为Smart Filter-Aided Domain Adversarial Neural Network（SFDANN）的缺陷诊断方法，其主要包括两个步骤。第一步是发展一个智能节点，它可以在时间-频域域中强制同源和目标领域数据的相似性。第二步是将重建后的数据输入到一个领域对抗神经网络（Domain Adversarial Neural Network, DANN）中，以学习领域不断和特征分类。</li>
<li>results: 本研究运用了两个缺陷诊断案例，一是磨削机缺陷诊断在噪音环境中，另一是列车轨道缺陷诊断在列车-轨道-桥梁组合震动系统中，这两个案例都是将模拟数据转移到实际数据上，以验证SFDANN方法的效果。结果显示，相比于其他代表性的UDA方法，SFDANN方法在稳定性和识别性方面表现出色。<details>
<summary>Abstract</summary>
The application of unsupervised domain adaptation (UDA)-based fault diagnosis methods has shown significant efficacy in industrial settings, facilitating the transfer of operational experience and fault signatures between different operating conditions, different units of a fleet or between simulated and real data. However, in real industrial scenarios, unknown levels and types of noise can amplify the difficulty of domain alignment, thus severely affecting the diagnostic performance of deep learning models. To address this issue, we propose an UDA method called Smart Filter-Aided Domain Adversarial Neural Network (SFDANN) for fault diagnosis in noisy industrial scenarios. The proposed methodology comprises two steps. In the first step, we develop a smart filter that dynamically enforces similarity between the source and target domain data in the time-frequency domain. This is achieved by combining a learnable wavelet packet transform network (LWPT) and a traditional wavelet packet transform module. In the second step, we input the data reconstructed by the smart filter into a domain adversarial neural network (DANN). To learn domain-invariant and discriminative features, the learnable modules of SFDANN are trained in a unified manner with three objectives: time-frequency feature proximity, domain alignment, and fault classification. We validate the effectiveness of the proposed SFDANN method based on two fault diagnosis cases: one involving fault diagnosis of bearings in noisy environments and another involving fault diagnosis of slab tracks in a train-track-bridge coupling vibration system, where the transfer task involves transferring from numerical simulations to field measurements. Results show that compared to other representative state of the art UDA methods, SFDANN exhibits superior performance and remarkable stability.
</details>
<details>
<summary>摘要</summary>
通过不监督领域适应（UDA）基本的缺陷诊断方法应用，在实际工业场景中显示出了显著的效果，帮助传输不同操作条件、不同单元的船队中的运行经验和缺陷特征。然而，在实际工业场景中，未知的噪声水平和类型可能会增加领域对Alignment的困难度，从而严重地affect Deep learning模型的诊断性能。为解决这个问题，我们提出了一种名为智能筛子援助领域对抗神经网络（SFDANN）的UDA方法，用于缺陷诊断在噪声rich工业场景中。该方法包括两个步骤：第一步：我们开发了一种智能筛子，通过将源频域和目标频域数据在时域频域上进行动态相似性检查，以确保频域数据的匹配。这是通过组合学习抽象射频包变换网络（LWPT）和传统的抽象射频包变换模块来实现的。第二步：我们将重构后的数据输入到领域对抗神经网络（DANN）中，以学习频域特征的域不可分别性和分类特征。我们将学习模块在一起训练三个目标：时域特征的相似性、频域对齐和缺陷分类。我们验证了我们提出的SFDANN方法的效果，在磁矿轮毂缺陷诊断和铁路桥摆车轨缺陷诊断两个案例中进行了比较，其中一个案例是在噪声环境中进行磁矿轮毂缺陷诊断，另一个案例是在铁路桥摆车轨缺陷诊断中，将数据从数值仿真转移到场景测量中。结果显示，相比其他代表性的UDA方法，SFDANN方法在稳定性和性能两个方面具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="Generative-Flow-Networks-a-Markov-Chain-Perspective"><a href="#Generative-Flow-Networks-a-Markov-Chain-Perspective" class="headerlink" title="Generative Flow Networks: a Markov Chain Perspective"></a>Generative Flow Networks: a Markov Chain Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01422">http://arxiv.org/abs/2307.01422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tristan Deleu, Yoshua Bengio</li>
<li>for: 这篇论文是为了提出一种基于Markov链 Monte Carlo方法的新框架，用于采样高多modal的概率分布。</li>
<li>methods: 论文使用Generative Flow Networks（GFlowNets）作为一种新的采样框架，通过对采样视为一个顺序决策问题来mitigate高多modal的问题。</li>
<li>results: 论文提出了一种新的框架，可以在不同的状态空间下视为一种回归Markov链，并且可以通过对GFlowNets进行抽象来看到它们与MCMC方法之间的相似性。<details>
<summary>Abstract</summary>
While Markov chain Monte Carlo methods (MCMC) provide a general framework to sample from a probability distribution defined up to normalization, they often suffer from slow convergence to the target distribution when the latter is highly multi-modal. Recently, Generative Flow Networks (GFlowNets) have been proposed as an alternative framework to mitigate this issue when samples have a clear compositional structure, by treating sampling as a sequential decision making problem. Although they were initially introduced from the perspective of flow networks, the recent advances of GFlowNets draw more and more inspiration from the Markov chain literature, bypassing completely the need for flows. In this paper, we formalize this connection and offer a new perspective for GFlowNets using Markov chains, showing a unifying view for GFlowNets regardless of the nature of the state space as recurrent Markov chains. Positioning GFlowNets under the same theoretical framework as MCMC methods also allows us to identify the similarities between both frameworks, and most importantly to highlight their
</details>
<details>
<summary>摘要</summary>
While Markov chain Monte Carlo methods (MCMC) provide a general framework to sample from a probability distribution defined up to normalization, they often suffer from slow convergence to the target distribution when the latter is highly multi-modal. Recently, Generative Flow Networks (GFlowNets) have been proposed as an alternative framework to mitigate this issue when samples have a clear compositional structure, by treating sampling as a sequential decision making problem. Although they were initially introduced from the perspective of flow networks, the recent advances of GFlowNets draw more and more inspiration from the Markov chain literature, bypassing completely the need for flows. In this paper, we formalize this connection and offer a new perspective for GFlowNets using Markov chains, showing a unifying view for GFlowNets regardless of the nature of the state space as recurrent Markov chains. Positioning GFlowNets under the same theoretical framework as MCMC methods also allows us to identify the similarities between both frameworks, and most importantly to highlight their differences.Note: The translation is done using a machine translation tool, and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Free-energy-of-Bayesian-Convolutional-Neural-Network-with-Skip-Connection"><a href="#Free-energy-of-Bayesian-Convolutional-Neural-Network-with-Skip-Connection" class="headerlink" title="Free energy of Bayesian Convolutional Neural Network with Skip Connection"></a>Free energy of Bayesian Convolutional Neural Network with Skip Connection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01417">http://arxiv.org/abs/2307.01417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuya Nagayasu, Sumio Watanabe</li>
<li>for: 本研究探讨了Convolutional Neural Networks(CNNs)中skip connection的效果，以及 Bayesian learning中这种结构的可能性。</li>
<li>methods: 本研究使用了Bayesian方法来研究CNNs中skip connection的效果，并对 Bayesian CNN的一般化性能进行了解释。</li>
<li>results: 研究发现，Bayesian CNN中skip connection的upper bound of free energy不依赖于过参数，并且Bayesian CNN的一般化错误有类似的性能。<details>
<summary>Abstract</summary>
Since the success of Residual Network(ResNet), many of architectures of Convolutional Neural Networks(CNNs) have adopted skip connection. While the generalization performance of CNN with skip connection has been explained within the framework of Ensemble Learning, the dependency on the number of parameters have not been revealed. In this paper, we show that Bayesian free energy of Convolutional Neural Network both with and without skip connection in Bayesian learning. The upper bound of free energy of Bayesian CNN with skip connection does not depend on the oveparametrization and, the generalization error of Bayesian CNN has similar property.
</details>
<details>
<summary>摘要</summary>
自Residual Network(ResNet)的成功以来，许多Convolutional Neural Networks(CNNs)的 arquitectures 已经采用了跳connection。然而，通用的参数数量对CNN with skip connection的泛化性能的影响还没有得到解释。在这篇论文中，我们展示了Bayesian free energy of Convolutional Neural Network both with and without skip connection in Bayesian learning。无论是Bayesian CNN with skip connection还是Bayesian CNN without skip connection，其Upper bound of free energy都不依赖于过参数化，而泛化误差的性能也具有相同的性质。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-vulnerabilities-in-SplitFed-Learning-Assessing-the-robustness-against-Data-Poisoning-Attacks"><a href="#Analyzing-the-vulnerabilities-in-SplitFed-Learning-Assessing-the-robustness-against-Data-Poisoning-Attacks" class="headerlink" title="Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks"></a>Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03197">http://arxiv.org/abs/2307.03197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aysha Thahsin Zahir Ismail, Raj Mani Shukla</li>
<li>for: 这个论文旨在研究和分析 SplitFed Learning (SFL) 中数据毒素攻击的影响。</li>
<li>methods: 该论文提出了三种新的攻击策略，包括无目标攻击、targeted攻击和距离基于攻击。</li>
<li>results: 研究发现，无目标和距离基于攻击在SFL中有更大的影响，比targeted攻击更容易让分类器输出错误。研究还通过对两个案例研究（electrocardiogram signal classification和自动手写数字识别）进行了多个攻击实验，并分析了攻击的影响。<details>
<summary>Abstract</summary>
Distributed Collaborative Machine Learning (DCML) is a potential alternative to address the privacy concerns associated with centralized machine learning. The Split learning (SL) and Federated Learning (FL) are the two effective learning approaches in DCML. Recently there have been an increased interest on the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is the earliest attempt to study, analyze and present the impact of data poisoning attacks in SFL. We propose three kinds of novel attack strategies namely untargeted, targeted and distance-based attacks for SFL. All the attacks strategies aim to degrade the performance of the DCML-based classifier. We test the proposed attack strategies for two different case studies on Electrocardiogram signal classification and automatic handwritten digit recognition. A series of attack experiments were conducted by varying the percentage of malicious clients and the choice of the model split layer between the clients and the server. The results after the comprehensive analysis of attack strategies clearly convey that untargeted and distance-based poisoning attacks have greater impacts in evading the classifier outcomes compared to targeted attacks in SFL
</details>
<details>
<summary>摘要</summary>
分布式协作机器学习（DCML）是一种可能的中央机器学习隐私问题的解决方案。分布式学习（SL）和联邦学习（FL）是DCML中两种有效的学习方法。最近，关注于SL和FL的混合，即SplitFed Learning（SFL）的研究增长。这项研究是对SFL中数据毒化攻击的首次研究。我们提出了三种新的攻击策略，namely 无目标、Targeted和距离基于攻击，这三种攻击策略都是为了降低基于DCML的分类器性能。我们在两个不同的案例研究中进行了电室心跳信号分类和自动手写数字识别的试验。我们在clients和服务器之间的模型 Split层进行了变化，并通过调整恶意客户端的百分比和选择的模型 Split层来进行了一系列攻击实验。结果表明，无目标和距离基于攻击更有可能影响DCML-based分类器的性能，compared to Targeted attacks。
</details></li>
</ul>
<hr>
<h2 id="Multi-Predictor-Fusion-Combining-Learning-based-and-Rule-based-Trajectory-Predictors"><a href="#Multi-Predictor-Fusion-Combining-Learning-based-and-Rule-based-Trajectory-Predictors" class="headerlink" title="Multi-Predictor Fusion: Combining Learning-based and Rule-based Trajectory Predictors"></a>Multi-Predictor Fusion: Combining Learning-based and Rule-based Trajectory Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01408">http://arxiv.org/abs/2307.01408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sushant Veer, Apoorva Sharma, Marco Pavone</li>
<li>for: 这篇论文是关于自动驾驶车辆（AV）的 trajectory 预测模块，尤其是在高度互动的交通enario中，以提高安全和效率的规划计划。</li>
<li>methods: 这篇论文提出了一种名为多predictor fusion（MPF）的算法，它将学习基于predictors和逻辑规则的motions planners结合在一起，以提高学习型预测器的性能。MPF使用probabilistic combining方法，将学习型和逻辑规则基的预测器的轨迹混合在一起，以获得最佳性能。</li>
<li>results: 根据我们的结果，MPF在多种指标上表现出色，并且在线性能最高和最稳定的情况下运行。<details>
<summary>Abstract</summary>
Trajectory prediction modules are key enablers for safe and efficient planning of autonomous vehicles (AVs), particularly in highly interactive traffic scenarios. Recently, learning-based trajectory predictors have experienced considerable success in providing state-of-the-art performance due to their ability to learn multimodal behaviors of other agents from data. In this paper, we present an algorithm called multi-predictor fusion (MPF) that augments the performance of learning-based predictors by imbuing them with motion planners that are tasked with satisfying logic-based rules. MPF probabilistically combines learning- and rule-based predictors by mixing trajectories from both standalone predictors in accordance with a belief distribution that reflects the online performance of each predictor. In our results, we show that MPF outperforms the two standalone predictors on various metrics and delivers the most consistent performance.
</details>
<details>
<summary>摘要</summary>
几何预测模组是自动驾驶车 (AV) 规划中的关键启动器，特别是在高度互动的交通情况下。最近，学习型几何预测器在提供最佳性能方面有所成就，因为它们可以从数据中学习多种行为模式。在这篇文章中，我们提出了一个名为多predictor融合（MPF）的算法，它将学习型和规则型预测器融合在一起，以提高几何预测器的性能。MPF 使用一个信念分布来混合两个独立的预测器的轨迹，以实现学习型和规则型预测器的共同运行。在我们的结果中，我们发现MPF 在多个指标上表现更好，并提供了最稳定的性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Communicate-using-Contrastive-Learning"><a href="#Learning-to-Communicate-using-Contrastive-Learning" class="headerlink" title="Learning to Communicate using Contrastive Learning"></a>Learning to Communicate using Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01403">http://arxiv.org/abs/2307.01403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SonamSangpoLama/Music-Genre-Classification">https://github.com/SonamSangpoLama/Music-Genre-Classification</a></li>
<li>paper_authors: Yat Long Lo, Biswa Sengupta, Jakob Foerster, Michael Noukhovitch</li>
<li>for: 这个研究目的是为了提高多智能体RL中的协调，并且解决对环境的观察和沟通问题。</li>
<li>methods: 这个研究使用了对比学习来学习通信，将在不同时间和位置发送的消息视为不完整的环境状态观察。</li>
<li>results: 研究发现，这种方法可以在对话重要的环境中提高性能和学习速度，并且对环境状态观察有更好的对 symmetry 和全局状态资讯的捕捉。<details>
<summary>Abstract</summary>
Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
</details>
<details>
<summary>摘要</summary>
通信是多智能RL中协调工具的强大工具。但是引入有效、公共语言是一个困难的挑战，特别是在分布式设定下。在这项工作中，我们提出了一种不同的视角，即在代理者之间交换的通信信息被视为环境状态的不同不完整的视图。我们提出了通过对交换的消息进行对比学习，以最大化交换消息序列中的相互信息。在需要通信的环境下，我们的方法比前一项工作在性能和学习速度方面表现更好。使用质量指标和表示探测，我们显示了我们的方法在交换消息中引入更Symmetric的通信和捕捉环境中的全局状态信息。总之，我们展示了对冲学习的力量和通过消息编码实现有效的通信的重要性。
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Surrogates-for-Interaction-of-a-Jet-with-High-Explosives-Part-II-–-Clustering-Extremely-High-Dimensional-Grid-Based-Data"><a href="#Spatio-Temporal-Surrogates-for-Interaction-of-a-Jet-with-High-Explosives-Part-II-–-Clustering-Extremely-High-Dimensional-Grid-Based-Data" class="headerlink" title="Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part II – Clustering Extremely High-Dimensional Grid-Based Data"></a>Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part II – Clustering Extremely High-Dimensional Grid-Based Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01400">http://arxiv.org/abs/2307.01400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chandrika Kamath, Juliette S. Franzman</li>
<li>for: 这个论文的目的是建立一个准确的模拟模型，以便更好地预测计算机模拟中的空间时间输出。</li>
<li>methods: 作者使用了一种简单的方法，即将输出数据分为不同类别，并建立每个类别的单独的模拟模型。但是当输出数据中的空间域数量很大，分类变得更加困难。因此，作者首先将数据转换为一致的格式，然后使用随机投影法减少数据的维度，使用迭代k-means算法进行分类。</li>
<li>results: 作者的方法可以将极高维度的数据进行有意义的分类，即使有一定的近似性。他们通过控制随机投影的方式和k-means算法的初始中心点的选择，确定了数据集中的cluster数量。<details>
<summary>Abstract</summary>
Building an accurate surrogate model for the spatio-temporal outputs of a computer simulation is a challenging task. A simple approach to improve the accuracy of the surrogate is to cluster the outputs based on similarity and build a separate surrogate model for each cluster. This clustering is relatively straightforward when the output at each time step is of moderate size. However, when the spatial domain is represented by a large number of grid points, numbering in the millions, the clustering of the data becomes more challenging. In this report, we consider output data from simulations of a jet interacting with high explosives. These data are available on spatial domains of different sizes, at grid points that vary in their spatial coordinates, and in a format that distributes the output across multiple files at each time step of the simulation. We first describe how we bring these data into a consistent format prior to clustering. Borrowing the idea of random projections from data mining, we reduce the dimension of our data by a factor of thousand, making it possible to use the iterative k-means method for clustering. We show how we can use the randomness of both the random projections, and the choice of initial centroids in k-means clustering, to determine the number of clusters in our data set. Our approach makes clustering of extremely high dimensional data tractable, generating meaningful cluster assignments for our problem, despite the approximation introduced in the random projections.
</details>
<details>
<summary>摘要</summary>
在计算机模拟中的输出中，建立准确的代理模型是一项复杂的任务。一种简单的方法是根据输出的相似性进行归类，并为每个归类建立一个独立的代理模型。当输出的每个时间步骤的大小是 Moderate 时，这种归类是相对容易的。但是，当 spatial 领域被表示为数百万个网点时，归类数据变得更加困难。在这份报告中，我们考虑了计算机模拟中的液体喷气与高爆物相互作用的输出数据。这些数据在不同的空间尺度上可以获得，并且在每个时间步骤上分布在多个文件中。我们首先描述了如何将这些数据转换成一致的格式，以便归类。我们采用了数据挖掘中的Random Projections的想法，将数据维度减少到一千倍，使用迭代k-means算法进行归类。我们示出了如何使用Random Projections和k-means归类算法中的随机初始化中心的Randomness来确定数据集中的凝集数。我们的方法使得归类EXTREMELY HIGH 维度数据成为可能，生成了有意义的凝集分配，尽管在随机投影中引入了一定的简化。
</details></li>
</ul>
<hr>
<h2 id="In-depth-Analysis-On-Parallel-Processing-Patterns-for-High-Performance-Dataframes"><a href="#In-depth-Analysis-On-Parallel-Processing-Patterns-for-High-Performance-Dataframes" class="headerlink" title="In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes"></a>In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01394">http://arxiv.org/abs/2307.01394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranda Perera, Arup Kumar Sarker, Mills Staylor, Gregor von Laszewski, Kaiying Shan, Supun Kamburugamuve, Chathura Widanage, Vibhatha Abeykoon, Thejaka Amila Kanewela, Geoffrey Fox</li>
<li>for: 本研究旨在提高数据工程应用程序的性能，特别是在处理大量数据时。</li>
<li>methods: 本文使用高性能计算的视角，提出了分布式数据框架操作的并行处理模式，并实现了参考runtime实现Cylon。</li>
<li>results: 本研究在ORNL Summit超级计算机上评估了Cylon的性能。<details>
<summary>Abstract</summary>
The Data Science domain has expanded monumentally in both research and industry communities during the past decade, predominantly owing to the Big Data revolution. Artificial Intelligence (AI) and Machine Learning (ML) are bringing more complexities to data engineering applications, which are now integrated into data processing pipelines to process terabytes of data. Typically, a significant amount of time is spent on data preprocessing in these pipelines, and hence improving its e fficiency directly impacts the overall pipeline performance. The community has recently embraced the concept of Dataframes as the de-facto data structure for data representation and manipulation. However, the most widely used serial Dataframes today (R, pandas) experience performance limitations while working on even moderately large data sets. We believe that there is plenty of room for improvement by taking a look at this problem from a high-performance computing point of view. In a prior publication, we presented a set of parallel processing patterns for distributed dataframe operators and the reference runtime implementation, Cylon [1]. In this paper, we are expanding on the initial concept by introducing a cost model for evaluating the said patterns. Furthermore, we evaluate the performance of Cylon on the ORNL Summit supercomputer.
</details>
<details>
<summary>摘要</summary>
“数据科学领域在过去的一个 décennial 内扩大了很大，主要归功于大数据革命。人工智能（AI）和机器学习（ML）对数据工程应用带来了更多复杂性，这些应用现在被 integrate 到数据处理管道中来处理 terrabytes 级数据。通常，处理数据预处理过程中会投入大量时间，因此提高其效率直接影响整个管道性能。社区最近普遍认可数据帧为数据表示和操作的启用词。但是，当前最广泛使用的序列数据帧（R、pandas）在处理 Moderately 大规模数据集时会表现出性能限制。我们认为，从高性能计算的角度来看这个问题，还有很多可以提高的空间。在先前的发表文章中，我们提出了分布式数据帧运算 Patterns 和 Referencel Runtime 实现 Cylon 等一系列并发处理模式[1]。在这篇论文中，我们将这个概念进一步发展，并提出一种成本模型来评估所提出的模式。此外，我们还在 ORNL Summit 超级计算机上评估了 Cylon 的性能。”
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Surrogates-for-Interaction-of-a-Jet-with-High-Explosives-Part-I-–-Analysis-with-a-Small-Sample-Size"><a href="#Spatio-Temporal-Surrogates-for-Interaction-of-a-Jet-with-High-Explosives-Part-I-–-Analysis-with-a-Small-Sample-Size" class="headerlink" title="Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part I – Analysis with a Small Sample Size"></a>Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part I – Analysis with a Small Sample Size</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01393">http://arxiv.org/abs/2307.01393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chandrika Kamath, Juliette S. Franzman, Brian H. Daub</li>
<li>for: 本研究旨在开发一种高质量的空间-时间抽象方法，以便更好地理解复杂现象的计算机模拟结果。</li>
<li>methods: 本研究使用了一种基于机器学习的抽象方法，并在使用了一些简单的方法来提高抽象精度。</li>
<li>results: 研究发现，使用这种抽象方法可以创建高质量的空间-时间抽象模型，并且不需要进行大量的计算机模拟。<details>
<summary>Abstract</summary>
Computer simulations, especially of complex phenomena, can be expensive, requiring high-performance computing resources. Often, to understand a phenomenon, multiple simulations are run, each with a different set of simulation input parameters. These data are then used to create an interpolant, or surrogate, relating the simulation outputs to the corresponding inputs. When the inputs and outputs are scalars, a simple machine learning model can suffice. However, when the simulation outputs are vector valued, available at locations in two or three spatial dimensions, often with a temporal component, creating a surrogate is more challenging. In this report, we use a two-dimensional problem of a jet interacting with high explosives to understand how we can build high-quality surrogates. The characteristics of our data set are unique - the vector-valued outputs from each simulation are available at over two million spatial locations; each simulation is run for a relatively small number of time steps; the size of the computational domain varies with each simulation; and resource constraints limit the number of simulations we can run. We show how we analyze these extremely large data-sets, set the parameters for the algorithms used in the analysis, and use simple ways to improve the accuracy of the spatio-temporal surrogates without substantially increasing the number of simulations required.
</details>
<details>
<summary>摘要</summary>
计算机模拟，尤其是复杂现象的模拟，可能具有高成本，需要高性能计算资源。经常情况下，以解释现象，需要运行多个模拟，每个模拟都有不同的模拟输入参数。这些数据后来用于创建一个 interpolant，或surrogate，将模拟输出与相应的输入关系。当输入和输出都是整数时，一个简单的机器学习模型即可。但当模拟输出是二维或三维的向量值，创建surrogate更加困难。在这份报告中，我们使用一个两维问题，即喷气与高爆物相互作用，来理解如何建立高质量surrogate。我们的数据集的特点是唯一的：每个模拟的向量值输出在超过两百万个空间位置上可用;每个模拟只需要很少的时间步骤;计算区域的大小随每个模拟而异；资源限制限制我们可以运行的模拟数量。我们如何分析这些非常大的数据集，设置分析中使用的参数，并使用简单的方法提高空间temporal surrogate的准确性，不需要substantially增加模拟数量。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Learning-in-Real-World-Fraud-Detection-Challenges-and-Perspectives"><a href="#Adversarial-Learning-in-Real-World-Fraud-Detection-Challenges-and-Perspectives" class="headerlink" title="Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives"></a>Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01390">http://arxiv.org/abs/2307.01390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danele Lunghi, Alkis Simitsis, Olivier Caelen, Gianluca Bontempi</li>
<li>for: 本研究旨在探讨针对诈骗检测系统的攻击方法，以及如何扩展对其他领域和应用的攻击技术。</li>
<li>methods: 本研究使用了对抗机器学习技术，以探讨诈骗检测系统中的攻击方法。</li>
<li>results: 本研究发现了一些针对诈骗检测系统的攻击方法，并提出了一些可能的解决方案。<details>
<summary>Abstract</summary>
Data economy relies on data-driven systems and complex machine learning applications are fueled by them. Unfortunately, however, machine learning models are exposed to fraudulent activities and adversarial attacks, which threaten their security and trustworthiness. In the last decade or so, the research interest on adversarial machine learning has grown significantly, revealing how learning applications could be severely impacted by effective attacks. Although early results of adversarial machine learning indicate the huge potential of the approach to specific domains such as image processing, still there is a gap in both the research literature and practice regarding how to generalize adversarial techniques in other domains and applications. Fraud detection is a critical defense mechanism for data economy, as it is for other applications as well, which poses several challenges for machine learning. In this work, we describe how attacks against fraud detection systems differ from other applications of adversarial machine learning, and propose a number of interesting directions to bridge this gap.
</details>
<details>
<summary>摘要</summary>
<SYS>  将文本翻译成简化中文。</SYS>数据经济依赖于数据驱动系统和复杂的机器学习应用程序，但是这些应用程序受到诈骗活动和敌意攻击的威胁。在过去的一个 décennial 以来，关于反对机器学习的研究兴趣增长了 significatively，揭示了机器学习应用程序可能受到严重的影响。虽然初期的反对机器学习结果表明了该方法在图像处理领域的巨大潜力，但是在其他领域和应用程序中，还存在一定的泛化问题。防止诈骗是数据经济中的关键防御机制，同时也是其他应用程序中的挑战。在这种情况下，我们描述了诈骗检测系统受到攻击的方式与其他应用程序不同，并提出了一些有趣的方向来bridging这个差距。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-Causal-Relationship-between-Amyloid-beta-Accumulation-and-Alzheimer’s-Disease-Progression-via-Counterfactual-Inference"><a href="#Identification-of-Causal-Relationship-between-Amyloid-beta-Accumulation-and-Alzheimer’s-Disease-Progression-via-Counterfactual-Inference" class="headerlink" title="Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer’s Disease Progression via Counterfactual Inference"></a>Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer’s Disease Progression via Counterfactual Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01389">http://arxiv.org/abs/2307.01389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haixing Dai, Mengxuan Hu, Qing Li, Lu Zhang, Lin Zhao, Dajiang Zhu, Ibai Diez, Jorge Sepulcre, Fan Zhang, Xingyu Gao, Manhua Liu, Quanzheng Li, Sheng Li, Tianming Liu, Xiang Li</li>
<li>for: 这篇论文旨在探讨阿尔茨海默症（AD）的预后诊断和个性化治疗方案。</li>
<li>methods: 论文提出了一种基于图 convolutional neural network（GVCNet）的方法来估计个体对药物剂量的影响，以探讨阿尔茨海默症发展的因果关系。</li>
<li>results: 论文显示了这种方法可以实现个体对阿尔茨海默症发展的测量，并且可以提供可靠的预后诊断和个性化治疗方案。<details>
<summary>Abstract</summary>
Alzheimer's disease (AD) is a neurodegenerative disorder that is beginning with amyloidosis, followed by neuronal loss and deterioration in structure, function, and cognition. The accumulation of amyloid-beta in the brain, measured through 18F-florbetapir (AV45) positron emission tomography (PET) imaging, has been widely used for early diagnosis of AD. However, the relationship between amyloid-beta accumulation and AD pathophysiology remains unclear, and causal inference approaches are needed to uncover how amyloid-beta levels can impact AD development. In this paper, we propose a graph varying coefficient neural network (GVCNet) for estimating the individual treatment effect with continuous treatment levels using a graph convolutional neural network. We highlight the potential of causal inference approaches, including GVCNet, for measuring the regional causal connections between amyloid-beta accumulation and AD pathophysiology, which may serve as a robust tool for early diagnosis and tailored care.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病（AD）是一种神经退化疾病，起始于蛋白质沉积，然后是神经元丢失和结构、功能和认知的衰退。脑内βamyloid沉积的寻测，通过18F-氟苯酚（AV45） пози特核燐发射 Tomography（PET）成像，在早期诊断AD中广泛使用。然而，蛋白质沉积和AD生物学过程之间的关系仍然不清楚，需要用 causal inference 方法来探索蛋白质沉积如何影响AD发展。在这篇论文中，我们提出了一种基于图变换系数神经网络（GVCNet）的个体处方效应估计方法，可以用于评估连续治疗水平下的个体处方效应。我们强调了可meter causal inference 方法，包括 GVCNet，在评估蛋白质沉积和AD生物学过程之间的区域 causal 连接方面的潜在价值，这可能成为早期诊断和个性化治疗的可靠工具。
</details></li>
</ul>
<hr>
<h2 id="Systematic-Bias-in-Sample-Inference-and-its-Effect-on-Machine-Learning"><a href="#Systematic-Bias-in-Sample-Inference-and-its-Effect-on-Machine-Learning" class="headerlink" title="Systematic Bias in Sample Inference and its Effect on Machine Learning"></a>Systematic Bias in Sample Inference and its Effect on Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01384">http://arxiv.org/abs/2307.01384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Owen O’Neill, Fintan Costello</li>
<li>for: 这种机器学习模型下的目标特征下预测不准确，特别是对少数群体的预测。</li>
<li>methods: 使用小样本统计推断的方法，导致预测结果受到方向性的统计偏见。</li>
<li>results: 对多个子集的预测结果显示，这种偏见导致了少数群体的预测错误率较高。<details>
<summary>Abstract</summary>
A commonly observed pattern in machine learning models is an underprediction of the target feature, with the model's predicted target rate for members of a given category typically being lower than the actual target rate for members of that category in the training set. This underprediction is usually larger for members of minority groups; while income level is underpredicted for both men and women in the 'adult' dataset, for example, the degree of underprediction is significantly higher for women (a minority in that dataset). We propose that this pattern of underprediction for minorities arises as a predictable consequence of statistical inference on small samples. When presented with a new individual for classification, an ML model performs inference not on the entire training set, but on a subset that is in some way similar to the new individual, with sizes of these subsets typically following a power law distribution so that most are small (and with these subsets being necessarily smaller for the minority group). We show that such inference on small samples is subject to systematic and directional statistical bias, and that this bias produces the observed patterns of underprediction seen in ML models. Analysing a standard sklearn decision tree model's predictions on a set of over 70 subsets of the 'adult' and COMPAS datasets, we found that a bias prediction measure based on small-sample inference had a significant positive correlations (0.56 and 0.85) with the observed underprediction rate for these subsets.
</details>
<details>
<summary>摘要</summary>
通常观察到的机器学习模型 patrón es la underprediction del feature objetivo, con la tasa predicha del modelo para los miembros de una categoría específica generalmente siendo menor que la tasa real para los miembros de esa categoría en el conjunto de entrenamiento. Esta underprediction es usualmente más grande para los miembros de los grupos minoritarios; por ejemplo, en el conjunto de datos 'adult', la tasa de underprediction es significativamente más alta para las mujeres (un grupo minoritario en ese conjunto de datos). Proponemos que este patrón de underprediction para los minorías se debe a una inferencia estadística predictible en pequeños conjuntos de datos. Cuando se presenta a un nuevo individuo para clasificación, un modelo de aprendizaje automático realiza inferencia no en todo el conjunto de entrenamiento, sino en un subconjunto que es de alguna manera similar al nuevo individuo, con tamaños de estos subconjuntos que siguen una distribución de potencia, lo que significa que la mayoría son pequeños (y con estos subconjuntos necesariamente más pequeños para el grupo minoritario). Demostramos que esta inferencia en pequeños conjuntos de datos está sujeta a una bias estadística sistemática y direccional, y que esta bias produce los patrones de underprediction observados en los modelos de aprendizaje automático. Analizando las predicciones de un modelo de árbol de decisión de sklearn en más de 70 subconjuntos del conjunto de datos 'adult' y COMPAS, encontramos que una medida de predicción de bias basada en la inferencia en pequeños conjuntos de datos tuvo una correlación positiva significativa (0,56 y 0,85) con la tasa de underprediction observada para estos subconjuntos.
</details></li>
</ul>
<hr>
<h2 id="Implicit-Memory-Transformer-for-Computationally-Efficient-Simultaneous-Speech-Translation"><a href="#Implicit-Memory-Transformer-for-Computationally-Efficient-Simultaneous-Speech-Translation" class="headerlink" title="Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation"></a>Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01381">http://arxiv.org/abs/2307.01381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-starlab/implicitmemory">https://github.com/osu-starlab/implicitmemory</a></li>
<li>paper_authors: Matthew Raffel, Lizhong Chen</li>
<li>for: 这个论文目的是提出一种新的听话器，以便在同时进行口头翻译。</li>
<li>methods: 该方法使用块处理来分割输入序列，并使用新的左上下文方法来隐式地保留记忆。</li>
<li>results: 实验结果表明，使用该方法可以在Encoder前进行快速加速，并且与使用左上下文和记忆银行的方法相比，翻译质量几乎相同。<details>
<summary>Abstract</summary>
Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedup on the encoder forward pass with nearly identical translation quality when compared with the state-of-the-art approach that employs both left context and memory banks.
</details>
<details>
<summary>摘要</summary>
同时对话翻译是人类communication task中的一项重要任务，即在流动输入语音时实时生成翻译。为此流动任务，使用块处理的 transformers 已经达到了状态机器的性能标准，而且可以降低计算成本。现有的方法，包括左上下文和内存银行，尝试让信息在段之间传递，但是这些方法都是不够的表示和过分昂贵的计算。在这篇论文中，我们提出了隐式记忆 transformer，通过一种新的左上下文方法，使得不需要显式表示内存。我们从前一段的注意输出中生成左上下文，并将其包含在当前段的注意计算中的键和值中。实验结果表明，隐式记忆 transformer 在 Must-C 数据集上提供了大幅降低encoder前进计算时间，并且与使用左上下文和内存银行的状态之前的翻译质量相似。
</details></li>
</ul>
<hr>
<h2 id="Shifting-Attention-to-Relevance-Towards-the-Uncertainty-Estimation-of-Large-Language-Models"><a href="#Shifting-Attention-to-Relevance-Towards-the-Uncertainty-Estimation-of-Large-Language-Models" class="headerlink" title="Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models"></a>Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01379">http://arxiv.org/abs/2307.01379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhaoduan/shifting-attention-to-relevance">https://github.com/jinhaoduan/shifting-attention-to-relevance</a></li>
<li>paper_authors: Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, Kaidi Xu</li>
<li>for: 这项研究的目的是解决自动逆进语言模型（LLMs）生成输出的不确定性问题，即用户可以信任模型输出的问题。</li>
<li>methods: 这项研究使用了自动逆进语言模型（LLMs）生成输出的token不均等，即一些token更加重要（或代表）于另外的token，并且对于估计不确定性，所有token被视为平等的现象，来 investigate 如何解决这些不平等。</li>
<li>results: 研究结果显示，在估计不确定性时，许多重要的token和含有有限 semantics的句子被平均地或者甚至很重视，以至于存在biases。为了解决这些biases，提议使用 JOINT SHIFTING ATTENTION TO RELEVANT（SAR）组件，并在实验中达到了superior表现。<details>
<summary>Abstract</summary>
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular "off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance.
</details>
<details>
<summary>摘要</summary>
尽管大型自然语言模型（LLM）已经表现出了很大的潜力，但是 Still characterizing the uncertainty of model generations, i.e., when users can trust model outputs, is still a challenge. Our research is based on the heuristic fact that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular "off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Shiftable-Context-Addressing-Training-Inference-Context-Mismatch-in-Simultaneous-Speech-Translation"><a href="#Shiftable-Context-Addressing-Training-Inference-Context-Mismatch-in-Simultaneous-Speech-Translation" class="headerlink" title="Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation"></a>Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01377">http://arxiv.org/abs/2307.01377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-starlab/shiftablecontext">https://github.com/osu-starlab/shiftablecontext</a></li>
<li>paper_authors: Matthew Raffel, Drew Penney, Lizhong Chen</li>
<li>for:  simultaneous speech translation</li>
<li>methods: 使用 segment-based processing 和 Shiftable Context  scheme</li>
<li>results:  average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value for the three language pairs, with minimal impact on computation-aware Average Lagging.<details>
<summary>Abstract</summary>
Transformer models using segment-based processing have been an effective architecture for simultaneous speech translation. However, such models create a context mismatch between training and inference environments, hindering potential translation accuracy. We solve this issue by proposing Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. Shiftable Context is also broadly applicable to segment-based transformers for streaming tasks. Our experiments on the English-German, English-French, and English-Spanish language pairs from the MUST-C dataset demonstrate that when applied to the Augmented Memory Transformer, a state-of-the-art model for simultaneous speech translation, the proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value for the three language pairs, respectively, with a minimal impact on computation-aware Average Lagging.
</details>
<details>
<summary>摘要</summary>
transformer模型使用分段处理有效地实现同时语音翻译。然而，这些模型在训练和推理环境中存在上下文匹配问题，从而限制了翻译准确性。我们解决这个问题，提出了Shiftable Context，一种简单 yet effective的方案，确保在训练和推理过程中保持一致的分段和上下文大小。Shiftable Context还可以广泛应用于流处理任务中的 segment-based transformer。我们在MUST-C数据集上进行英语-德语、英语-法语和英语-西班牙语三对语言对的实验，结果显示，当应用到Augmented Memory Transformer模型时，提出的方案平均提高了2.09、1.83和1.95的BLEU分数 across each wait-k值，并且对计算意识的均衡延迟产生了最小的影响。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Principal-Component-Regression-with-Applications-to-Panel-Data"><a href="#Adaptive-Principal-Component-Regression-with-Applications-to-Panel-Data" class="headerlink" title="Adaptive Principal Component Regression with Applications to Panel Data"></a>Adaptive Principal Component Regression with Applications to Panel Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01357">http://arxiv.org/abs/2307.01357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anish Agarwal, Keegan Harris, Justin Whitehouse, Zhiwei Steven Wu</li>
<li>for: This paper provides time-uniform finite sample guarantees for online principal component regression (PCR) in the presence of adaptive data collection.</li>
<li>methods: The paper uses tools from modern martingale concentration to analyze PCR in the online setting, which is a generalization of the fixed-design error-in-variables regression.</li>
<li>results: The paper provides a framework for experiment design in panel data settings when interventions are assigned adaptively, which can be seen as a generalization of synthetic control and synthetic interventions frameworks.Here’s the Chinese version:</li>
<li>for: 这篇论文提供了在在线主成分回归（PCR）中的时间固定样本保证。</li>
<li>methods: 这篇论文使用现代随机 martingale 集中来分析 PCR 在在线设置下的分析。</li>
<li>results: 这篇论文提供了针对板块数据设置中的实验设计框架，当实验是通过适应性的干预分配策略进行分配。<details>
<summary>Abstract</summary>
Principal component regression (PCR) is a popular technique for fixed-design error-in-variables regression, a generalization of the linear regression setting in which the observed covariates are corrupted with random noise. We provide the first time-uniform finite sample guarantees for online (regularized) PCR whenever data is collected adaptively. Since the proof techniques for analyzing PCR in the fixed design setting do not readily extend to the online setting, our results rely on adapting tools from modern martingale concentration to the error-in-variables setting. As an application of our bounds, we provide a framework for experiment design in panel data settings when interventions are assigned adaptively. Our framework may be thought of as a generalization of the synthetic control and synthetic interventions frameworks, where data is collected via an adaptive intervention assignment policy.
</details>
<details>
<summary>摘要</summary>
主成分回归（PCR）是一种流行的固定设计错误变量回归技术， linear regression 设定中的一种扩展，在观测 covariates 上存在随机噪声。我们提供了在线（规化）PCR 的首次时间均衡finite sample guarantees，当数据采集是动态的。由于fixed design 设定中PCR 的证明技巧不直接适用于在线设定，我们的结果基于采用现代martingale concentration 工具来error-in-variables设定。我们的极限 bounds 可以应用于面板数据设置中的实验设计，当实验是通过适应性干预分配策略采集数据。我们的框架可以看作是错误变量和synthetic control 框架的扩展，在适应性干预分配策略下采集数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-Generic-Solutions-for-Multiphase-Transport-in-Porous-Media-via-the-Flux-Functions-Operator"><a href="#Learning-Generic-Solutions-for-Multiphase-Transport-in-Porous-Media-via-the-Flux-Functions-Operator" class="headerlink" title="Learning Generic Solutions for Multiphase Transport in Porous Media via the Flux Functions Operator"></a>Learning Generic Solutions for Multiphase Transport in Porous Media via the Flux Functions Operator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01354">http://arxiv.org/abs/2307.01354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Waleed Diab, Omar Chaabi, Shayma Alkobaisi, Abeeb Awotunde, Mohammed Al Kobaisi</li>
<li>for: 加速 fluid 流动和运输在 porous media 中的 simulate 算法，使得在科学和工程领域中可以更快速地解决问题。</li>
<li>methods: 使用 deep learning 技术，具体来说是 Physics-Informed DeepONets (PI-DeepONets)，通过学习 partial differential equations (PDEs) 中的运算函数，从而实现快速的解决。</li>
<li>results: 比 traditional numerical solvers 快速多达四个数量级，并且可以捕捉到 any type of flux function (concave, convex, or non-convex) 的解决。同时，trained PI-DeepONet model 表现出了优秀的泛化能力，这使得它成为了解决 transport problems in porous media 中的一个有力的工具。<details>
<summary>Abstract</summary>
Traditional numerical schemes for simulating fluid flow and transport in porous media can be computationally expensive. Advances in machine learning for scientific computing have the potential to help speed up the simulation time in many scientific and engineering fields. DeepONet has recently emerged as a powerful tool for accelerating the solution of partial differential equations (PDEs) by learning operators (mapping between function spaces) of PDEs. In this work, we learn the mapping between the space of flux functions of the Buckley-Leverett PDE and the space of solutions (saturations). We use Physics-Informed DeepONets (PI-DeepONets) to achieve this mapping without any paired input-output observations, except for a set of given initial or boundary conditions; ergo, eliminating the expensive data generation process. By leveraging the underlying physical laws via soft penalty constraints during model training, in a manner similar to Physics-Informed Neural Networks (PINNs), and a unique deep neural network architecture, the proposed PI-DeepONet model can predict the solution accurately given any type of flux function (concave, convex, or non-convex) while achieving up to four orders of magnitude improvements in speed over traditional numerical solvers. Moreover, the trained PI-DeepONet model demonstrates excellent generalization qualities, rendering it a promising tool for accelerating the solution of transport problems in porous media.
</details>
<details>
<summary>摘要</summary>
传统的数学方法 для模拟 fluid 流和物质传输在porous media中可能是计算昂贵的。机器学习的应用在科学计算中有助于减少模拟时间在多科学和工程领域。DeepONet 是一种可以加速解决部分偏微分方程（PDEs）的有力工具，它可以学习 PDEs 中操作（函数空间之间的映射）的映射。在这个工作中，我们学习了 Buckley-Leverett PDE 中的流函数空间和解空间之间的映射，使用 Physics-Informed DeepONets（PI-DeepONets）来实现这种映射，不需要任何对应的输入输出观察数据，只需要给定一些初始或边界条件即可。通过在模型训练中采用物理法律的软约束，类似于 Physics-Informed Neural Networks（PINNs），以及特有的深度神经网络架构，我们的提议的 PI-DeepONet 模型可以准确地预测解，并且可以在不同类型的流函数（凹、 convex、非几何）下实现四个数量级的速度提高。此外，我们训练的 PI-DeepONet 模型还表现出了优秀的泛化质量，使其成为加速porous media中物质传输问题的解决工具。
</details></li>
</ul>
<hr>
<h2 id="Patch-CNN-Training-data-efficient-deep-learning-for-high-fidelity-diffusion-tensor-estimation-from-minimal-diffusion-protocols"><a href="#Patch-CNN-Training-data-efficient-deep-learning-for-high-fidelity-diffusion-tensor-estimation-from-minimal-diffusion-protocols" class="headerlink" title="Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols"></a>Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01346">http://arxiv.org/abs/2307.01346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Goodwin-Allcock, Ting Gong, Robert Gray, Parashkev Nachev, Hui Zhang</li>
<li>for: 这种论文是为了提出一种新的方法，即 Patch-CNN，用于从六个方向的扩散图像（DWI）中提取扩散矩阵（DT）的估计。</li>
<li>methods: 该方法使用了深度学习方法，使用了缓冲层（Convolutional Neural Network，CNN）来学习扩散矩阵的估计。</li>
<li>results: 对比传统模型适应和维度全连接神经网络（voxel-wise Fully-Connected Neural Network，FCN），Patch-CNN 可以更好地估计扩散矩阵和纤维方向，并且只需要使用单个试验者的数据进行训练。<details>
<summary>Abstract</summary>
We propose a new method, Patch-CNN, for diffusion tensor (DT) estimation from only six-direction diffusion weighted images (DWI). Deep learning-based methods have been recently proposed for dMRI parameter estimation, using either voxel-wise fully-connected neural networks (FCN) or image-wise convolutional neural networks (CNN). In the acute clinical context -- where pressure of time limits the number of imaged directions to a minimum -- existing approaches either require an infeasible number of training images volumes (image-wise CNNs), or do not estimate the fibre orientations (voxel-wise FCNs) required for tractogram estimation. To overcome these limitations, we propose Patch-CNN, a neural network with a minimal (non-voxel-wise) convolutional kernel (3$\times$3$\times$3). Compared with voxel-wise FCNs, this has the advantage of allowing the network to leverage local anatomical information. Compared with image-wise CNNs, the minimal kernel vastly reduces training data demand. Evaluated against both conventional model fitting and a voxel-wise FCN, Patch-CNN, trained with a single subject is shown to improve the estimation of both scalar dMRI parameters and fibre orientation from six-direction DWIs. The improved fibre orientation estimation is shown to produce improved tractogram.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新方法，patch-CNN，用于从六个方向的扩散tensor（DT）估计。在临床情况下，使用全量的学习方法来估计DMRI参数，可以使用 Either voxel-wise fully-connected neural networks（FCN）或图像-wise convolutional neural networks（CNN）。现有的方法 either require an infeasible number of training images volumes（image-wise CNNs），或者不能估计纤维方向（voxel-wise FCNs），从而限制了轨迹估计。为了超越这些限制，我们提出了patch-CNN，一个具有最小（非voxel-wise） convolutional kernel（3×3×3）的神经网络。与voxel-wise FCNs比较，这有利于神经网络利用地方 анатомиче信息。与image-wise CNNs比较，最小kernel减少了训练数据的需求。我们通过对 conventiomal model fitting和voxel-wise FCN进行比较，发现patch-CNN，通过一个个体训练，可以提高六个方向DWI中的scalar DMRI参数和纤维方向的估计。此外，改进的纤维方向估计也可以提高轨迹的估计。
</details></li>
</ul>
<hr>
<h2 id="Robust-Uncertainty-Estimation-for-Classification-of-Maritime-Objects"><a href="#Robust-Uncertainty-Estimation-for-Classification-of-Maritime-Objects" class="headerlink" title="Robust Uncertainty Estimation for Classification of Maritime Objects"></a>Robust Uncertainty Estimation for Classification of Maritime Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01325">http://arxiv.org/abs/2307.01325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Becktor, Frederik Scholler, Evangelos Boukas, Lazaros Nalpantidis</li>
<li>for: 这篇论文的目的是探讨在海上领域中使用不确定性估计的可能性，并在具有各种硬件和软件限制的实际场景中进行评估。</li>
<li>methods: 这篇论文使用了蒙特卡洛批处理来实现内部类uncertainty，并结合了最新的异常检测发现的技术来获得更全面的不确定性测量。</li>
<li>results: 该论文的实验结果显示，通过将Monte Carlo Dropout与异常检测技术结合使用，可以提高FPR95的性能，相比之下当模型没有异常数据训练时，该方法的性能提高了8%。此外，相比于基本实现的宽度网络，该方法可以提高性能 by 77%。此外， authors还释放了SHIPS数据集，并证明了该方法的有效性，将FPR95提高了44.2%。<details>
<summary>Abstract</summary>
We explore the use of uncertainty estimation in the maritime domain, showing the efficacy on toy datasets (CIFAR10) and proving it on an in-house dataset, SHIPS. We present a method joining the intra-class uncertainty achieved using Monte Carlo Dropout, with recent discoveries in the field of outlier detection, to gain more holistic uncertainty measures. We explore the relationship between the introduced uncertainty measures and examine how well they work on CIFAR10 and in a real-life setting. Our work improves the FPR95 by 8% compared to the current highest-performing work when the models are trained without out-of-distribution data. We increase the performance by 77% compared to a vanilla implementation of the Wide ResNet. We release the SHIPS dataset and show the effectiveness of our method by improving the FPR95 by 44.2% with respect to the baseline. Our approach is model agnostic, easy to implement, and often does not require model retraining.
</details>
<details>
<summary>摘要</summary>
我们探索了海上领域中uncertainty估计的使用，通过使用CIFAR10杂交数据集和自有数据集SHIPS进行证明，并提出了将Monte Carlo Dropout中的内类uncertainty与现代异常检测发现相结合以获得更全面的uncertainty测度的方法。我们研究了引入的uncertainty测度与之间的关系，并在CIFAR10和实际场景中评估其效果。我们的工作提高了FPR95的性能，相比最高性能工作不包含外围数据集时，提高了8%。相比于普通实现的宽度网络，我们的方法提高了77%的性能。我们发布了SHIPS数据集，并通过提高FPR95的性能44.2%来证明我们的方法的效果。我们的方法是模型无关的，易于实现，通常不需要模型重新训练。
</details></li>
</ul>
<hr>
<h2 id="Density-based-Feasibility-Learning-with-Normalizing-Flows-for-Introspective-Robotic-Assembly"><a href="#Density-based-Feasibility-Learning-with-Normalizing-Flows-for-Introspective-Robotic-Assembly" class="headerlink" title="Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly"></a>Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01317">http://arxiv.org/abs/2307.01317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DLR-RM/GRACE">https://github.com/DLR-RM/GRACE</a></li>
<li>paper_authors: Jianxiang Feng, Matan Atad, Ismael Rodríguez, Maximilian Durner, Stephan Günnemann, Rudolph Triebel</li>
<li>for: 本研究旨在提高机器学习（ML）模型在机器人组装序列规划（RASP）中的 introspection 能力，以避免效率下降。</li>
<li>methods: 本研究提出了一种基于密度的可行性学习方法，不需要非可行示例。具体来说，我们将可行性学习问题转化为Out-of-Distribution（OOD）探测问题，使用Normalizing Flows（NF）来估计复杂的概率分布。</li>
<li>results: 在机器人组装用例中，提出的方法比单类基elines表现出色地探测不可行的组装。我们还进一步调查了我们方法的内部工作机制，发现可以通过高级变体NF实现很大的内存节省。<details>
<summary>Abstract</summary>
Machine Learning (ML) models in Robotic Assembly Sequence Planning (RASP) need to be introspective on the predicted solutions, i.e. whether they are feasible or not, to circumvent potential efficiency degradation. Previous works need both feasible and infeasible examples during training. However, the infeasible ones are hard to collect sufficiently when re-training is required for swift adaptation to new product variants. In this work, we propose a density-based feasibility learning method that requires only feasible examples. Concretely, we formulate the feasibility learning problem as Out-of-Distribution (OOD) detection with Normalizing Flows (NF), which are powerful generative models for estimating complex probability distributions. Empirically, the proposed method is demonstrated on robotic assembly use cases and outperforms other single-class baselines in detecting infeasible assemblies. We further investigate the internal working mechanism of our method and show that a large memory saving can be obtained based on an advanced variant of NF.
</details>
<details>
<summary>摘要</summary>
machine learning (ml) 模型在机器人组装序列规划 (rasp) 中需要 introspective 对预测的解决方案，以避免效率降低。先前的工作需要两类样本：可行和不可行的示例。然而，不可行的示例具有充足的收集困难，导致在重新训练时需要充足的时间。在这种情况下，我们提议一种基于浓度学习的可行学习方法，只需要可行的示例。具体来说，我们将可行学习问题定义为 OUT-OF-DISTRIBUTION (OOD) 检测，使用 Normalizing Flows (NF) 来Estimate 复杂的概率分布。实验表明，我们提议的方法在机器人组装use case中表现出色，可以快速检测不可行的组装。我们进一步调查我们的方法的内部工作机制，发现可以基于高级变体的 NF 实现大量内存保存。
</details></li>
</ul>
<hr>
<h2 id="Towards-Safe-Autonomous-Driving-Policies-using-a-Neuro-Symbolic-Deep-Reinforcement-Learning-Approach"><a href="#Towards-Safe-Autonomous-Driving-Policies-using-a-Neuro-Symbolic-Deep-Reinforcement-Learning-Approach" class="headerlink" title="Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach"></a>Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01316">http://arxiv.org/abs/2307.01316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cav-research-lab/safe-reinforcement-learning-using-symbolic-logical-programming-for-autonomous-highway-driving">https://github.com/cav-research-lab/safe-reinforcement-learning-using-symbolic-logical-programming-for-autonomous-highway-driving</a></li>
<li>paper_authors: Iman Sharifi, Mustafa Yildirim, Saber Fallah</li>
<li>for: 本研究旨在开发一种能够在真实环境中学习自动驾驶策略，并确保安全性的神经符号逻辑深度学习方法（DRLSL）。</li>
<li>methods: 本方法结合神经网络学习和符号逻辑推理，以便在真实环境中学习自动驾驶策略，并且能够保证安全性。</li>
<li>results: 我们在使用高D数据集进行实践中，发现DRLSL方法可以避免不安全行为，并且在训练和测试阶段都能够快速 converges。此外，我们的结果还表明，DRLSL方法在面对新的驾驶场景时能够更好地泛化。<details>
<summary>Abstract</summary>
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics (knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD dataset and demonstrated that our method successfully avoids unsafe actions during both the training and testing phases. Furthermore, our results indicate that DRLSL achieves faster convergence during training and exhibits better generalizability to new driving scenarios compared to traditional DRL methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics (knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD dataset and demonstrated that our method successfully avoids unsafe actions during both the training and testing phases. Furthermore, our results indicate that DRLSL achieves faster convergence during training and exhibits better generalizability to new driving scenarios compared to traditional DRL methods."into Simplified Chinese.驾驶环境的动态性和路用者的多样性 pose significant challenges for autonomous driving decision-making.深度强化学习（DRL）已经成为解决这个问题的popular方法。然而，现有的DRL解决方案主要在模拟环境中应用，由于安全问题，导致它们在实际环境中没有得到广泛应用。为了突破这个限制，这篇论文提出了一种新的 neuralsymbolic model-free DRL方法，叫做DRL with Symbolic Logics（DRLSL）。这种方法结合了DRL（学习经验）和符号逻辑（知识驱动的理解），以便在真实环境中学习自动驾驶策略，并确保安全。我们在自动驾驶中实现了DRLSL框架，使用highD dataset，并证明了我们的方法在训练和测试阶段都可以避免不安全的行为。此外，我们的结果还表明，DRLSL在训练阶段更快 converges和在新的驾驶enario中表现更好的普适性。
</details></li>
</ul>
<hr>
<h2 id="A-numerical-algorithm-for-attaining-the-Chebyshev-bound-in-optimal-learning"><a href="#A-numerical-algorithm-for-attaining-the-Chebyshev-bound-in-optimal-learning" class="headerlink" title="A numerical algorithm for attaining the Chebyshev bound in optimal learning"></a>A numerical algorithm for attaining the Chebyshev bound in optimal learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01304">http://arxiv.org/abs/2307.01304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pradyumna Paruchuri, Debasish Chatterjee</li>
<li>for: 解决 оптимального学习从数据点集中回归函数的问题</li>
<li>methods: 基于近似解决半无穷问题的目标采样技术</li>
<li>results: 计算废弃半径和废弃中心，解决函数回归问题<details>
<summary>Abstract</summary>
Given a compact subset of a Banach space, the Chebyshev center problem consists of finding a minimal circumscribing ball containing the set. In this article we establish a numerically tractable algorithm for solving the Chebyshev center problem in the context of optimal learning from a finite set of data points. For a hypothesis space realized as a compact but not necessarily convex subset of a finite-dimensional subspace of some underlying Banach space, this algorithm computes the Chebyshev radius and the Chebyshev center of the hypothesis space, thereby solving the problem of optimal recovery of functions from data. The algorithm itself is based on, and significantly extends, recent results for near-optimal solutions of convex semi-infinite problems by means of targeted sampling, and it is of independent interest. Several examples of numerical computations of Chebyshev centers are included in order to illustrate the effectiveness of the algorithm.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "compact subset" becomes "compact subset" (同义译)* "Chebyshev center problem" becomes "Chebychev中心问题" (direct translation)* "hypothesis space" becomes "假设空间" (direct translation)* "compact but not necessarily convex subset" becomes "不必然凸的子集" (direct translation)* "minimal circumscribing ball" becomes "最小圆包" (direct translation)* "optimal learning from a finite set of data points" becomes "从finite个数据点中优化学习" (direct translation)* "convex semi-infinite problems" becomes "凸半无穷问题" (direct translation)* "targeted sampling" becomes "targeted采样" (direct translation)* "of independent interest" becomes "独立有利" (direct translation)Note that in Simplified Chinese, the word "space" is often omitted in translations, so "underlying Banach space" becomes just "underlying Banach 空间" in the translation.
</details></li>
</ul>
<hr>
<h2 id="Pareto-Secure-Machine-Learning-PSML-Fingerprinting-and-Securing-Inference-Serving-Systems"><a href="#Pareto-Secure-Machine-Learning-PSML-Fingerprinting-and-Securing-Inference-Serving-Systems" class="headerlink" title="Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems"></a>Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01292">http://arxiv.org/abs/2307.01292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debopam Sanyal, Jui-Tse Hung, Manav Agrawal, Prahlad Jasti, Shahab Nikkhoo, Somesh Jha, Tianhao Wang, Sibin Mohan, Alexey Tumanov</li>
<li>for: 本研究探讨了基于服务器端模型 zoo 的实时网络应用中的安全性，具体来说是对模型EXTRACTION攻击的Robustness。</li>
<li>methods: 本研究提出了一种高效的查询 fingerprinting 算法，使攻击者可以让服务器端模型 consistently 执行恶意操作。此外，本研究还提出了一种基于噪音的防御机制，通过添加噪音到指定性能指标来防止 fingerprinting。</li>
<li>results: 本研究表明，使用高效查询 fingerprinting 算法可以在模型EXTRACTION攻击中实现高精度和高准确率（在 $1%$ 以内），同时可以提高模型抽象层的安全性。此外，本研究还发现了一种基于噪音的防御机制可以减少攻击精度和准确率（在 $9.8%$ 和 $4.8%$ 以内）。<details>
<summary>Abstract</summary>
Model-serving systems have become increasingly popular, especially in real-time web applications. In such systems, users send queries to the server and specify the desired performance metrics (e.g., desired accuracy, latency). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks assume a single model can be repeatedly selected for serving inference requests. Modern inference serving systems break this assumption. Thus, they cannot be directly applied to extract a victim model, as models are hidden behind a layer of abstraction exposed by the serving system. An attacker can no longer identify which model she is interacting with. To this end, we first propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accuracy scores within $1\%$ of the scores obtained when attacking a single, explicitly specified model, as well as up to $14.6\%$ gain in accuracy and up to $7.7\%$ gain in fidelity compared to the naive attack. Second, we counter the proposed attack with a noise-based defense mechanism that thwarts fingerprinting by adding noise to the specified performance metrics. The proposed defense strategy reduces the attack's accuracy and fidelity by up to $9.8\%$ and $4.8\%$, respectively (on medium-sized model extraction). Third, we show that the proposed defense induces a fundamental trade-off between the level of protection and system goodput, achieving configurable and significant victim model extraction protection while maintaining acceptable goodput ($>80\%$). We implement the proposed defense in a real system with plans to open source.
</details>
<details>
<summary>摘要</summary>
模型服务系统在实时网络应用中变得越来越流行，特别是在用户发送查询并指定需要的性能指标（例如精度和响应时间）后，服务器根据指定的指标从后端维护的模型 zoo 中提供查询结果。这篇论文检查这些系统的安全性，特别是对于模型提取攻击的Robustness。现有的黑盒攻击假设可以重复地选择服务器上的单个模型来进行推理请求。现代推理服务系统破坏了这一假设，因此无法直接应用于提取受害模型。攻击者无法确定她正在互动的是哪个模型。为此，我们首先提出了一种高效的询问算法，使得攻击者可以轻松地触发所需的模型。我们显示，使用我们的询问算法可以在$1\%$的精度和准确度下提取模型，并且可以在$14.6\%$的精度和$7.7\%$的准确度上提高模型提取的精度和准确度，相比之下 Naive 攻击。其次，我们采用噪音基的防御机制，将指定性能指标添加噪音，以防止指纹。我们的防御策略可以在中等模型提取 task 下 reducuce 攻击的精度和准确度为$9.8\%$和$4.8\%$。最后，我们显示了我们的防御机制存在可配置的质量和系统性能之间的负面冲击，可以在保持可接受的系统性能（$>80\%$）的情况下实现可靠的受害模型提取保护。我们已经实现了我们的防御机制，计划将其开源。
</details></li>
</ul>
<hr>
<h2 id="Fighting-the-disagreement-in-Explainable-Machine-Learning-with-consensus"><a href="#Fighting-the-disagreement-in-Explainable-Machine-Learning-with-consensus" class="headerlink" title="Fighting the disagreement in Explainable Machine Learning with consensus"></a>Fighting the disagreement in Explainable Machine Learning with consensus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01288">http://arxiv.org/abs/2307.01288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Jesus Banegas-Luna, Carlos Martınez-Cortes, Horacio Perez-Sanchez</li>
<li>For: 本研究旨在解释机器学习模型的内部工作方式，以提高模型的可解释性。* Methods: 本研究使用了多种可解释性算法，包括本研究所开发的一种新的函数，以解释五种机器学习模型。* Results: 研究结果显示，提出的函数比其他函数更公正，提供了更一致和准确的解释。<details>
<summary>Abstract</summary>
Machine learning (ML) models are often valued by the accuracy of their predictions. However, in some areas of science, the inner workings of models are as relevant as their accuracy. To understand how ML models work internally, the use of interpretability algorithms is the preferred option. Unfortunately, despite the diversity of algorithms available, they often disagree in explaining a model, leading to contradictory explanations. To cope with this issue, consensus functions can be applied once the models have been explained. Nevertheless, the problem is not completely solved because the final result will depend on the selected consensus function and other factors. In this paper, six consensus functions have been evaluated for the explanation of five ML models. The models were previously trained on four synthetic datasets whose internal rules were known in advance. The models were then explained with model-agnostic local and global interpretability algorithms. Finally, consensus was calculated with six different functions, including one developed by the authors. The results demonstrated that the proposed function is fairer than the others and provides more consistent and accurate explanations.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:机器学习（ML）模型常被评估于其预测准确率。然而，在一些科学领域中，模型内部的工作方式也很重要。为了了解模型如何工作，使用可解释算法是最佳选择。然而，尽管有多种可解释算法可用，它们经常在解释模型时存在差异，导致不一致的解释。为了解决这个问题，可以应用consensus函数。然而，这并不完全解决问题，因为选择的consensus函数以及其他因素会影响最终结果。在这篇论文中，六种consensus函数被评估以解释五种ML模型。这些模型先前在四个已知内部规则的 sintetic数据集上进行了训练。然后，使用模型无关的本地和全局可解释算法来解释模型。最后，使用六种不同的consensus函数进行了投票，包括作者所开发的一种。结果表明，提案的函数比其他们更公平，并提供了更一致和准确的解释。
</details></li>
</ul>
<hr>
<h2 id="Using-BOLD-fMRI-to-Compute-the-Respiration-Volume-per-Time-RTV-and-Respiration-Variation-RV-with-Convolutional-Neural-Networks-CNN-in-the-Human-Connectome-Development-Cohort"><a href="#Using-BOLD-fMRI-to-Compute-the-Respiration-Volume-per-Time-RTV-and-Respiration-Variation-RV-with-Convolutional-Neural-Networks-CNN-in-the-Human-Connectome-Development-Cohort" class="headerlink" title="Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort"></a>Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05426">http://arxiv.org/abs/2307.05426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdoljalil Addeh, Fernando Vega, Rebecca J Williams, Ali Golestani, G. Bruce Pike, M. Ethan MacDonald</li>
<li>for: 这个研究的目的是提高fMRI研究中肺功能信号的质量和可用性。</li>
<li>methods: 该研究使用一种一维 convolutional neural network（CNN）模型来重建两种肺功能指标，即RV和RVT。</li>
<li>results: 研究结果表明，CNN模型可以从休息BOLD信号中捕捉有用的特征，并重建实际的RV和RVT时间序列。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In many fMRI studies, respiratory signals are unavailable or do not have acceptable quality. Consequently, the direct removal of low-frequency respiratory variations from BOLD signals is not possible. This study proposes a one-dimensional CNN model for reconstruction of two respiratory measures, RV and RVT. Results show that a CNN can capture informative features from resting BOLD signals and reconstruct realistic RV and RVT timeseries. It is expected that application of the proposed method will lower the cost of fMRI studies, reduce complexity, and decrease the burden on participants as they will not be required to wear a respiratory bellows.
</details>
<details>
<summary>摘要</summary>
很多fMRI研究中的呼吸信号不可用或者质量不良。因此，直接从BOLD信号中除去低频呼吸变化是不可能的。本研究提出了一种一维 convolutional neural network（CNN）模型，用于重建两个呼吸指标：RV和RVT。结果显示，CNN可以从休息BOLD信号中捕捉有用的特征，重建真实的RV和RVT时间序列。预计该方法的应用将降低fMRI研究的成本，降低复杂性，并减少参与者的负担，因为他们不需要穿戴呼吸膜。
</details></li>
</ul>
<hr>
<h2 id="NeuBTF-Neural-fields-for-BTF-encoding-and-transfer"><a href="#NeuBTF-Neural-fields-for-BTF-encoding-and-transfer" class="headerlink" title="NeuBTF: Neural fields for BTF encoding and transfer"></a>NeuBTF: Neural fields for BTF encoding and transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01199">http://arxiv.org/abs/2307.01199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Rodriguez-Pardo, Konstantinos Kazatzis, Jorge Lopez-Moreno, Elena Garces</li>
<li>for: 这篇论文旨在提出一种新的神经网络材料表示方法，用于解决神经网络材料的固定性问题，以便在渲染中使用。</li>
<li>methods: 该方法使用神经网络来表示材料，并使用引导图像来控制神经网络的输出。在测试时，该方法可以使用UV、摄像头和光照向量来查询神经网络的输出。</li>
<li>results: 该方法可以在多种 sintetic和实际材料上达到竞争性的压缩率，并且可以通过引导图像来控制神经网络的输出。<details>
<summary>Abstract</summary>
Neural material representations are becoming a popular way to represent materials for rendering. They are more expressive than analytic models and occupy less memory than tabulated BTFs. However, existing neural materials are immutable, meaning that their output for a certain query of UVs, camera, and light vector is fixed once they are trained. While this is practical when there is no need to edit the material, it can become very limiting when the fragment of the material used for training is too small or not tileable, which frequently happens when the material has been captured with a gonioreflectometer. In this paper, we propose a novel neural material representation which jointly tackles the problems of BTF compression, tiling, and extrapolation. At test time, our method uses a guidance image as input to condition the neural BTF to the structural features of this input image. Then, the neural BTF can be queried as a regular BTF using UVs, camera, and light vectors. Every component in our framework is purposefully designed to maximize BTF encoding quality at minimal parameter count and computational complexity, achieving competitive compression rates compared with previous work. We demonstrate the results of our method on a variety of synthetic and captured materials, showing its generality and capacity to learn to represent many optical properties.
</details>
<details>
<summary>摘要</summary>
神经材料表示法是现代渲染中广泛应用的一种表示方法。它比分析模型更加表达力，且占用内存更少，但现有的神经材料都是不可变的，意味着它们的输出对于特定的UV、摄像机和光量向量的训练后就是固定的。这在材料的预测中是有用的，但在材料需要编辑时可能变得非常限制性。在这篇论文中，我们提出了一种新的神经材料表示方法，该方法同时解决了BTF压缩、瓦片和推导问题。在测试时，我们使用导航图像作为输入，通过conditioning神经BTF于这个输入图像的结构特征来控制神经BTF。然后，神经BTF可以被查询作为普通BTF使用UV、摄像机和光量向量。我们的框架中每个组件都是为最大化BTF编码质量而设计，而且减少参数计数和计算复杂度，与之前的工作相比，我们的方法实现了竞争力的压缩率。我们在多种 sintetic和捕捉的材料上进行了试验，展示了我们的方法的通用性和能力学习表示多种光学性质。
</details></li>
</ul>
<hr>
<h2 id="Improved-sampling-via-learned-diffusions"><a href="#Improved-sampling-via-learned-diffusions" class="headerlink" title="Improved sampling via learned diffusions"></a>Improved sampling via learned diffusions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01198">http://arxiv.org/abs/2307.01198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenz Richter, Julius Berner, Guan-Horng Liu</li>
<li>for: 这些论文提出了基于深度学习的方法，用于从不正规分布中采样。</li>
<li>methods: 这些方法是控制的扩散过程的特殊情况，寻找从给定的先前分布到目标分布的最有可能的杂乱进程。</li>
<li>results: 我们在这些方法中引入了一种变量形式，基于时间反转的扩散过程中的路径空间测量差异。这种抽象视角导致了可优化的梯度下降算法，并包含了先前的目标作为特殊情况。此外，我们还可以考虑不同于倒卡劳布拉迪弗分布的差异，以避免模式塌缩。例如，我们提出了对数差异损失函数，它在数值上显示了优化性和改进性。<details>
<summary>Abstract</summary>
Recently, a series of papers proposed deep learning-based approaches to sample from unnormalized target densities using controlled diffusion processes. In this work, we identify these approaches as special cases of the Schr\"odinger bridge problem, seeking the most likely stochastic evolution between a given prior distribution and the specified target. We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes. This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases. At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse. In particular, we propose the so-called log-variance loss, which exhibits favorable numerical properties and leads to significantly improved performance across all considered approaches.
</details>
<details>
<summary>摘要</summary>
最近，一系列论文提出了基于深度学习的方法来从不正规Target概率分布中采样。在这篇文章中，我们将这些方法定义为Schrödinger大桥问题的特殊情况，寻找从给定的先验分布到指定的Target概率分布的最有可能的杂化过程。我们进一步总结了这个框架，通过在Path空间测度上引入减法，从而得到了一种可优化的变分形式。这种抽象的视角允许我们考虑其他than reverse Kullback-Leibler divergence的异同，这种异同 known to suffer from mode collapse。特别是，我们提议使用Log-variance loss，它在数值上具有优秀的性质，并在所有考虑的方法中带来显著改进。
</details></li>
</ul>
<hr>
<h2 id="Squeezing-Large-Scale-Diffusion-Models-for-Mobile"><a href="#Squeezing-Large-Scale-Diffusion-Models-for-Mobile" class="headerlink" title="Squeezing Large-Scale Diffusion Models for Mobile"></a>Squeezing Large-Scale Diffusion Models for Mobile</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01193">http://arxiv.org/abs/2307.01193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiwoong Choi, Minkyu Kim, Daehyun Ahn, Taesu Kim, Yulhwa Kim, Dongwon Jo, Hyesung Jeon, Jae-Joon Kim, Hyungjun Kim</li>
<li>for: 这 paper 旨在探讨将 Stable Diffusion 模型部署到移动设备上，以便实现高精度图像生成。</li>
<li>methods: 该 paper 使用 TensorFlow Lite 框架来实现移动设备上的 Stable Diffusion 部署，并支持 iOS 和 Android 设备。</li>
<li>results: 该 paper 实现的 Mobile Stable Diffusion 可以在 Android 设备上 achieve 512x512 图像生成的推理延迟时间小于 7 秒，并且可以在移动 GPU 上实现。<details>
<summary>Abstract</summary>
The emergence of diffusion models has greatly broadened the scope of high-fidelity image synthesis, resulting in notable advancements in both practical implementation and academic research. With the active adoption of the model in various real-world applications, the need for on-device deployment has grown considerably. However, deploying large diffusion models such as Stable Diffusion with more than one billion parameters to mobile devices poses distinctive challenges due to the limited computational and memory resources, which may vary according to the device. In this paper, we present the challenges and solutions for deploying Stable Diffusion on mobile devices with TensorFlow Lite framework, which supports both iOS and Android devices. The resulting Mobile Stable Diffusion achieves the inference latency of smaller than 7 seconds for a 512x512 image generation on Android devices with mobile GPUs.
</details>
<details>
<summary>摘要</summary>
Diffusion模型的出现已经极大地扩大了高精度图像生成的范围，导致了实践部署和学术研究中的重要进步。然而，将大型Diffusion模型，如Stable Diffusion，deploy到移动设备上具有限制的计算和内存资源的问题。在这篇文章中，我们介绍了将Stable Diffusion部署到移动设备上的挑战和解决方案，使用TensorFlow Lite框架支持iOS和Android设备。我们的Mobile Stable Diffusion实现了512x512像素生成的推理延迟低于7秒钟在Android设备上。
</details></li>
</ul>
<hr>
<h2 id="Trainable-Transformer-in-Transformer"><a href="#Trainable-Transformer-in-Transformer" class="headerlink" title="Trainable Transformer in Transformer"></a>Trainable Transformer in Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01189">http://arxiv.org/abs/2307.01189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhishekpanigrahi1996/transformer_in_transformer">https://github.com/abhishekpanigrahi1996/transformer_in_transformer</a></li>
<li>paper_authors: Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora</li>
<li>for: 这个论文目的是提出一种高效的Transformer模型内部精细调整方法，以便在推理过程中进行精细调整。</li>
<li>methods: 这个方法使用了一些创新的近似技术，使得一个具有少于20亿参数的TinT模型能够在单步前进中 simulate和精细调整一个125亿参数的Transformer模型。</li>
<li>results: 在语言模型和下游任务中进行综合实验 validate了TinT模型的内部精细调整过程，并证明了大型预训练语言模型可以执行复杂的子任务。例如，even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M。<details>
<summary>Abstract</summary>
Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.
</details>
<details>
<summary>摘要</summary>
近期研究归功大型预训言语模型中的增Context学习（ICL）能力于内部模型（例如线性或2层MLP）的隐式模拟和细化 during inference. 然而，这些建构具有大量内存开销，使得更复杂的内部模型的模拟成为不可行。在这项工作中，我们提出了高效的建构——Transformer in Transformer（简称TinT），允许 transformer 模型在执行中内部模拟和细化复杂模型（例如预训言语模型）。具体来说，我们提出了创新的近似技术，使得 TinT 模型 fewer than 200 billion parameters 可以在单个前进 pass 中模拟和细化 125 million parameter transformer 模型。TinT 支持许多常见 transformer 变种，并且其设计想法也提高了过去内置简单模型的效率。我们通过综合实验 validate TinT 模型内部细化过程的效果，并在语言模型和下游任务上 observe 4-16% 绝对提升。这些发现表明大规模预训言语模型可以执行复杂的子routines。为了便于后续工作，我们附加了可扩展和可模块化的代码基金。
</details></li>
</ul>
<hr>
<h2 id="Fitting-an-ellipsoid-to-a-quadratic-number-of-random-points"><a href="#Fitting-an-ellipsoid-to-a-quadratic-number-of-random-points" class="headerlink" title="Fitting an ellipsoid to a quadratic number of random points"></a>Fitting an ellipsoid to a quadratic number of random points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01181">http://arxiv.org/abs/2307.01181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afonso S. Bandeira, Antoine Maillard, Shahar Mendelson, Elliot Paquette</li>
<li>for: 这个论文研究了将 $n$ 个标准正态随机向量在 $\mathbb{R}^d$ 中适应中心圆柱体的问题，当 $n, d \to \infty$ 时。</li>
<li>methods: 这个论文使用了 Bartl &amp; Mendelson 关于 Gram 矩阵的集中性的结论，并使用了一些轻量级的假设来证明这个问题在高概率下是可行的。</li>
<li>results: 这个论文证明了当 $n \leq d^2 &#x2F; C$，其中 $C &gt; 0$ 是一个可能很大的常数， THEN 问题 $(\mathrm{P})$ 有高概率是可行的。<details>
<summary>Abstract</summary>
We consider the problem $(\mathrm{P})$ of fitting $n$ standard Gaussian random vectors in $\mathbb{R}^d$ to the boundary of a centered ellipsoid, as $n, d \to \infty$. This problem is conjectured to have a sharp feasibility transition: for any $\varepsilon > 0$, if $n \leq (1 - \varepsilon) d^2 / 4$ then $(\mathrm{P})$ has a solution with high probability, while $(\mathrm{P})$ has no solutions with high probability if $n \geq (1 + \varepsilon) d^2 /4$. So far, only a trivial bound $n \geq d^2 / 2$ is known on the negative side, while the best results on the positive side assume $n \leq d^2 / \mathrm{polylog}(d)$. In this work, we improve over previous approaches using a key result of Bartl & Mendelson on the concentration of Gram matrices of random vectors under mild assumptions on their tail behavior. This allows us to give a simple proof that $(\mathrm{P})$ is feasible with high probability when $n \leq d^2 / C$, for a (possibly large) constant $C > 0$.
</details>
<details>
<summary>摘要</summary>
我们考虑一个问题($\mathrm{P}$)，即在中心为零的椭球上适应 $n$ 标准高斯均匀随机向量，当 $n, d \to \infty$ 时。这个问题据悉有一个锐化可行性过渡：如果 $n \leq (1 - \varepsilon) d^2 / 4$，那么 $(\mathrm{P})$ 有高概率解，而如果 $n \geq (1 + \varepsilon) d^2 /4$，那么 $(\mathrm{P})$ 有高概率无解。目前只知道一个负边界 $n \geq d^2 / 2$，而最好的结果在正边界上假设 $n \leq d^2 / \text{polylog}(d)$。在这个工作中，我们使用 Bartl & Mendelson 关于均匀矩阵的吸引性的结果，从而得到一个简单的证明：如果 $n \leq d^2 / C$，那么 $(\mathrm{P})$ 有高概率解，其中 $C > 0$ 是一个可能很大的常数。
</details></li>
</ul>
<hr>
<h2 id="PlanE-Representation-Learning-over-Planar-Graphs"><a href="#PlanE-Representation-Learning-over-Planar-Graphs" class="headerlink" title="PlanE: Representation Learning over Planar Graphs"></a>PlanE: Representation Learning over Planar Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01180">http://arxiv.org/abs/2307.01180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zzysonny/plane">https://github.com/zzysonny/plane</a></li>
<li>paper_authors: Radoslav Dimitrov, Zeyang Zhao, Ralph Abboud, İsmail İlkan Ceylan</li>
<li>for: 本研究的目的是设计一个可以快速学习完整的平面图 isomorphism 的架构，以便在平面图上进行图像学习。</li>
<li>methods: 本研究使用了一种称为 PlanE 的框架，它是基于 Hopcroft 和 Tarjan 的平面图 isomorphism 算法。PlanE 包括一些可以学习完整的平面图 invariants 的架构，并且可以在实际上扩展到大规模的平面图。</li>
<li>results: 本研究透过实验验证了 PlanE 的模型架构，并取得了多个 state-of-the-art 的结果。在 well-known 平面图 benchmark 上，PlanE 的模型能够实现高效地学习完整的平面图 invariants。<details>
<summary>Abstract</summary>
Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations graph invariants. On the other hand, it is well-known that graph invariants learned by these class of models are incomplete: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and Tarjan, we propose PlanE as a framework for planar representation learning. PlanE includes architectures which can learn complete invariants over planar graphs while remaining practically scalable. We empirically validate the strong performance of the resulting model architectures on well-known planar graph benchmarks, achieving multiple state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
“图 neural networks 是 Representation learning over graphs 中的主要模型，其中的思想是通过一系列转换来计算输入图的节点的表示，以确定learned graph function 是isoformation invariant的，这使得learned representation 成为图 invariants。然而，已知这些类型的模型学习的图 invariants 是不完全的：存在一些非同构的图对标准图 neural networks 无法分辨。这不Surprising，因为计算通用图 isomorphism testing 的计算复杂度很高，但在特定的图类中，有高效的图 isomorphism testing 算法，如平面图。我们的目标是设计一种能够有效地学习完整的平面图 invariants的architecture。 draw inspiration from Hopcroft 和 Tarjan 的平面图 isomorphism 算法，我们提出 PlanE 框架，用于平面 representation learning。 PlanE 包括一些可以学习完整的平面图 invariants 的architecture，并且 remain practically scalable。我们通过实验证明了这些结果的强性，在well-known planar graph benchmarks 上达到多个state-of-the-art result。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Mixtures-of-Gaussians-Using-the-DDPM-Objective"><a href="#Learning-Mixtures-of-Gaussians-Using-the-DDPM-Objective" class="headerlink" title="Learning Mixtures of Gaussians Using the DDPM Objective"></a>Learning Mixtures of Gaussians Using the DDPM Objective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01178">http://arxiv.org/abs/2307.01178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kulin Shah, Sitan Chen, Adam Klivans</li>
<li>for: 本文研究了 diffusion 模型可以学习哪些分布？</li>
<li>methods: 本文使用了什么方法？</li>
<li>results: 本文得到了哪些结果？Here are my answers, in Simplified Chinese:</li>
<li>for: 本文研究了 diffusion 模型可以学习 Gaussian mixture models 的参数。</li>
<li>methods: 本文使用了 gradient descent 算法，并证明了其可以高效地学习 Gaussian mixture models 的参数。</li>
<li>results: 本文证明了 gradient descent 算法可以在两种设置下高效地学习 Gaussian mixture models：1) 随机初始化下可以learns mixtures of two spherical Gaussians in $d$ dimensions with $1&#x2F;\text{poly}(d)$-separated centers。2) warm start 下可以 learns mixtures of $K$ spherical Gaussians with $\Omega(\sqrt{\log(\min(K,d))})$-separated centers。<details>
<summary>Abstract</summary>
Recent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation. Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed.   In this work, we give the first provably efficient results along these lines for one of the most fundamental distribution families, Gaussian mixture models. We prove that gradient descent on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings: 1) We show gradient descent with random initialization learns mixtures of two spherical Gaussians in $d$ dimensions with $1/\text{poly}(d)$-separated centers. 2) We show gradient descent with a warm start learns mixtures of $K$ spherical Gaussians with $\Omega(\sqrt{\log(\min(K,d))})$-separated centers. A key ingredient in our proofs is a new connection between score-based methods and two other approaches to distribution learning, the EM algorithm and spectral methods.
</details>
<details>
<summary>摘要</summary>
近期研究表明，扩散模型可以学习任何分布，只要可以进行分数估计。然而，我们还不够了解在哪些情况下分数估计是可行的，更重要的是，我们是否可以实现有效的梯度下降算法来解决这个问题。在这个工作中，我们给出了首次可证fficient的结果，其中包括以下两个情况：1. 我们证明，使用随机 initialization 的梯度下降在 $d$ 维的两个球形 Gaussian 混合模型中可以有效地回归真实参数。2. 我们证明，使用温始的梯度下降可以在 $K$ 个球形 Gaussian 混合模型中，对中心点进行 $\Omega(\sqrt{\log(\min(K,d))})$ 级别的分割。我们的证明中的一个关键元素是一种新的分数-基本方法和 EM 算法以及spectral methods之间的连接。
</details></li>
</ul>
<hr>
<h2 id="Neural-Hilbert-Ladders-Multi-Layer-Neural-Networks-in-Function-Space"><a href="#Neural-Hilbert-Ladders-Multi-Layer-Neural-Networks-in-Function-Space" class="headerlink" title="Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space"></a>Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01177">http://arxiv.org/abs/2307.01177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengdao Chen</li>
<li>for: 本文研究深度学习理论中神经网络（NN）定义的函数空间的特点。</li>
<li>methods: 作者视多层NN为定义特定层次的再生核希尔бер特空间（RKHS），称为神经希尔бер特阶梯（NHL）。</li>
<li>results: 作者证明了多层NN表达的函数和NHL之间的对应关系，并提供了控制复杂性度量的泛化保证。 Plus, the author derives the evolution of NHL as the dynamics of multiple random fields, and shows examples of depth separation in NHLs under different activation functions.<details>
<summary>Abstract</summary>
The characterization of the functions spaces explored by neural networks (NNs) is an important aspect of deep learning theory. In this work, we view a multi-layer NN with arbitrary width as defining a particular hierarchy of reproducing kernel Hilbert spaces (RKHSs), named a Neural Hilbert Ladder (NHL). This allows us to define a function space and a complexity measure that generalize prior results for shallow NNs, and we then examine their theoretical properties and implications in several aspects. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning an NHL with the complexity measure controlled. Third, corresponding to the training of multi-layer NNs in the infinite-width mean-field limit, we derive an evolution of the NHL characterized as the dynamics of multiple random fields. Fourth, we show examples of depth separation in NHLs under ReLU and quadratic activation functions. Finally, we complement the theory with numerical results to illustrate the learning of RKHS in NN training.
</details>
<details>
<summary>摘要</summary>
文章主要探讨深度学习理论中神经网络（NN）函数空间的特点。在这篇文章中，我们将多层NN视为定义特定层次的重复内 produit 希尔бер特空间（RKHS），称为神经希尔бер特阶梯（NHL）。这允许我们定义函数空间和复杂度度量，这些度量将对先前的浅层NN进行扩展，并且我们将研究这些理论性质和影响。首先，我们证明了L层NN表达的函数和L层NHL之间的对应关系。其次，我们证明了控制复杂度度量的学习承诺。第三，对于在无限宽度的平均场中训练多层NN，我们 derivation 了NHL的演化，这可以看做多个Random Fields 的动态。最后，我们通过实验示例来补充理论，以Illustrate 神经网络在训练中学习RKHS的过程。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Neural-Estimation-of-Entropies"><a href="#Quantum-Neural-Estimation-of-Entropies" class="headerlink" title="Quantum Neural Estimation of Entropies"></a>Quantum Neural Estimation of Entropies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01171">http://arxiv.org/abs/2307.01171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziv Goldfeld, Dhrumil Patel, Sreejith Sreekumar, Mark M. Wilde</li>
<li>for: 估计量子系统中的信息量和相关性</li>
<li>methods: 使用变量量子算法和经典神经网络参数化测量方法</li>
<li>results: 精确地估计了不同 entropy 度量的值，有效地应用于下游任务<details>
<summary>Abstract</summary>
Entropy measures quantify the amount of information and correlations present in a quantum system. In practice, when the quantum state is unknown and only copies thereof are available, one must resort to the estimation of such entropy measures. Here we propose a variational quantum algorithm for estimating the von Neumann and R\'enyi entropies, as well as the measured relative entropy and measured R\'enyi relative entropy. Our approach first parameterizes a variational formula for the measure of interest by a quantum circuit and a classical neural network, and then optimizes the resulting objective over parameter space. Numerical simulations of our quantum algorithm are provided, using a noiseless quantum simulator. The algorithm provides accurate estimates of the various entropy measures for the examples tested, which renders it as a promising approach for usage in downstream tasks.
</details>
<details>
<summary>摘要</summary>
Entropy 测量量代表量子系统中的信息量和相关性。在实践中，当量子状态未知，仅可以通过量子状态的复制来进行估算Entropy测量。我们提出了一种量子算法来估算 von Neumann 熵和 R\'enyi 熵，以及测量相对熵和测量 R\'enyi 相对熵。我们的方法首先假设测量对象的量子演算和классиical neural network的参数，然后对参数空间进行优化。我们的numerical simulation表明，该算法可以准确地估算各种熵测量的例子，这使其成为下游任务中的一个有前途的方法。Here's the breakdown of the translation:* Entropy 测量量 (Entropy measures) -> 熵测量 (entropy measurements)* 量子系统 (quantum system) -> 量子状态 (quantum state)* 未知 (unknown) -> 未知的 (unknown)* 复制 (copies) -> 复制品 (copies)* 估算 (estimation) -> 估算值 (estimated value)* von Neumann 熵 (Von Neumann entropy) -> von Neumann 熵量 (Von Neumann entropy)* R\'enyi 熵 (R\'enyi entropy) -> R\'enyi 熵量 (R\'enyi entropy)* 测量相对熵 (measured relative entropy) -> 测量相对熵量 (measured relative entropy)* 测量 R\'enyi 相对熵 (measured R\'enyi relative entropy) -> 测量 R\'enyi 相对熵量 (measured R\'enyi relative entropy)* 参数 (parameters) -> 参数空间 (parameter space)* 优化 (optimization) -> 优化过程 (optimization process)* numerical simulation -> 数值仿真 (numerical simulation)Note that the translation is done in Simplified Chinese, which is the most widely used standard for Chinese writing. The translation is done word-for-word, and some of the phrases or sentences may not be exactly the same as the original English version, but they should convey the same meaning.
</details></li>
</ul>
<hr>
<h2 id="Online-nearest-neighbor-classification"><a href="#Online-nearest-neighbor-classification" class="headerlink" title="Online nearest neighbor classification"></a>Online nearest neighbor classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01170">http://arxiv.org/abs/2307.01170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Sanjoy Dasgupta, Geelon So</li>
<li>for: 研究在可实现 Setting 中的在线非参数化分类问题。</li>
<li>methods: 使用 classical 1-nearest neighbor algorithm，并证明其在可实现 Setting 中 achieve 下降的误差率。</li>
<li>results: 实现下降的误差率，即在对征或平滑的对手中的误差率。<details>
<summary>Abstract</summary>
We study an instance of online non-parametric classification in the realizable setting. In particular, we consider the classical 1-nearest neighbor algorithm, and show that it achieves sublinear regret - that is, a vanishing mistake rate - against dominated or smoothed adversaries in the realizable setting.
</details>
<details>
<summary>摘要</summary>
我们研究在可实现 setting 中的在线非参数化分类问题。特别是，我们考虑了经典的1 nearest neighbor算法，并证明它在可实现 setting 中对于受控或平滑的反对敌人（adversaries） achieve 子线性 regret - 即消失的错误率。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-and-Improving-Greedy-2-Coordinate-Updates-for-Equality-Constrained-Optimization-via-Steepest-Descent-in-the-1-Norm"><a href="#Analyzing-and-Improving-Greedy-2-Coordinate-Updates-for-Equality-Constrained-Optimization-via-Steepest-Descent-in-the-1-Norm" class="headerlink" title="Analyzing and Improving Greedy 2-Coordinate Updates for Equality-Constrained Optimization via Steepest Descent in the 1-Norm"></a>Analyzing and Improving Greedy 2-Coordinate Updates for Equality-Constrained Optimization via Steepest Descent in the 1-Norm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01169">http://arxiv.org/abs/2307.01169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amrutha Varshini Ramesh, Aaron Mishkin, Mark Schmidt, Yihan Zhou, Jonathan Wilder Lavington, Jennifer She</li>
<li>for: 这篇论文是关于最优化问题的，具体来说是使用斜率逐步下降法和贝叶斯搜索法来解决一种具有约束的最优化问题。</li>
<li>methods: 这篇论文使用了一种名为”proximal Polyak-Lojasiewicz”的假设，并通过将这个假设应用到斜率逐步下降法中来提高它的准确率。此外，论文还使用了一种名为”bound- and summation-constrained steepest descent”的方法来解决具有约束的最优化问题。</li>
<li>results: 论文的结果表明，使用这种新的方法可以在$O(n \log n)$时间内解决具有约束的最优化问题，而且比之前的方法更快。此外，论文还证明了这种方法的准确率是Random Selection的两倍，并且不виси于问题的维度$n$。<details>
<summary>Abstract</summary>
We consider minimizing a smooth function subject to a summation constraint over its variables. By exploiting a connection between the greedy 2-coordinate update for this problem and equality-constrained steepest descent in the 1-norm, we give a convergence rate for greedy selection under a proximal Polyak-Lojasiewicz assumption that is faster than random selection and independent of the problem dimension $n$. We then consider minimizing with both a summation constraint and bound constraints, as arises in the support vector machine dual problem. Existing greedy rules for this setting either guarantee trivial progress only or require $O(n^2)$ time to compute. We show that bound- and summation-constrained steepest descent in the L1-norm guarantees more progress per iteration than previous rules and can be computed in only $O(n \log n)$ time.
</details>
<details>
<summary>摘要</summary>
我们考虑最小化一个几何函数，并且需要遵循一个总和约束。我们利用两个坐标更新的连接，将其与等式约束对应的steepest descent在L1内 producer一个更快的价值变数。然后我们考虑受约束的最小化问题，其中包括总和约束和范围约束。现有的对策可能只能保证很小的进步，或者需要O(n^2)的时间来计算。我们显示，在L1内的约束降阶 descendence可以在O(nlogn)的时间内获得更多的进步，并且可以更快地计算。
</details></li>
</ul>
<hr>
<h2 id="Don’t-freeze-Finetune-encoders-for-better-Self-Supervised-HAR"><a href="#Don’t-freeze-Finetune-encoders-for-better-Self-Supervised-HAR" class="headerlink" title="Don’t freeze: Finetune encoders for better Self-Supervised HAR"></a>Don’t freeze: Finetune encoders for better Self-Supervised HAR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01168">http://arxiv.org/abs/2307.01168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vitor Fortes Rey, Dominique Nshimyimana, Paul Lukowicz</li>
<li>for: 这个论文是为了解决人类活动识别领域中的标签数据可用性问题而提出的一种解决方案。</li>
<li>methods: 这个论文使用了自然语言处理中的预测任务，如重构和对比预测编码，来学习有用的表示。这些方法采用了预训练、冻结和细化的过程。</li>
<li>results: 这个论文发现，不冻结表示后的表示可以获得显著性能提升，这种提升是随着标签数据的量而增加的。此外，这种效果是无论在Capture24数据集上进行预测任务还是直接在目标数据集上进行预测任务中都存在。<details>
<summary>Abstract</summary>
Recently self-supervised learning has been proposed in the field of human activity recognition as a solution to the labelled data availability problem. The idea being that by using pretext tasks such as reconstruction or contrastive predictive coding, useful representations can be learned that then can be used for classification. Those approaches follow the pretrain, freeze and fine-tune procedure. In this paper we will show how a simple change - not freezing the representation - leads to substantial performance gains across pretext tasks. The improvement was found in all four investigated datasets and across all four pretext tasks and is inversely proportional to amount of labelled data. Moreover the effect is present whether the pretext task is carried on the Capture24 dataset or directly in unlabelled data of the target dataset.
</details>
<details>
<summary>摘要</summary>
近期，无监督学习在人活动识别领域被提出，作为数据可用性问题的解决方案。这种方法是通过重构或对比预测编码来学习有用的表示，然后用于分类。这些方法遵循“预训练、冻结并微调”的过程。在这篇论文中，我们将展示一种简单的改变：不冻结表示，导致了重要的性能提升，并且这种提升随着数据量的减少而增加。此外，这种效果是不论预测任务是在 Capture24 数据集上进行还是直接在无标签数据集上进行的。
</details></li>
</ul>
<hr>
<h2 id="Coupled-Gradient-Flows-for-Strategic-Non-Local-Distribution-Shift"><a href="#Coupled-Gradient-Flows-for-Strategic-Non-Local-Distribution-Shift" class="headerlink" title="Coupled Gradient Flows for Strategic Non-Local Distribution Shift"></a>Coupled Gradient Flows for Strategic Non-Local Distribution Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01166">http://arxiv.org/abs/2307.01166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Conger, Franca Hoffmann, Eric Mazumdar, Lillian Ratliff</li>
<li>for: 本研究旨在分析现实世界系统中的分布变化动态，包括学习算法和其部署的分布之间的反馈循环。</li>
<li>methods: 本研究提出了一种新的整合方法，该方法可以模型学习算法部署中的复杂分布变化，包括策略性反应、非本地人口互动和其他外部因素引起的分布变化。</li>
<li>results: 研究表明，当算法进行梯度下降 retraining 时，可以 дости到稳定状态，并且在有限和无限维度中都有显式速率，这些速率取决于模型参数。此外，研究还发现，该方法可以 Capture 许多已知的分布变化形式，如楔形和不同影响。<details>
<summary>Abstract</summary>
We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed. Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure. In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift. We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users. For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-state, both in finite and in infinite dimensions, obtaining explicit rates in terms of the model parameters. To do so we derive new results on the convergence of coupled PDEs that extends what is known on multi-species systems. Empirically, we show that our approach captures well-documented forms of distribution shifts like polarization and disparate impacts that simpler models cannot capture.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的框架，用于分析实际系统中分布shift的动态。这个框架 capture了学习算法和它们所部署的分布之间的反馈循环。先前的工作大多把反馈引起的分布shift模型为对抗性或非常简单的分布shift结构。相比之下，我们提出了一个结合部分梯度方程的模型，该模型可以考虑复杂的时间变化、策略性反应、非本地人口互动等因素，以捕捉细腻的分布变化。我们考虑了两种常见的机器学习设置：合作性设置和竞争性设置。在两个设置中，当算法通过梯度下降 retrained 时，我们证明了预测过程的稳定性，包括有限维度和无穷维度下的稳定性，并得到了明确的速率。为此，我们 derivation 了新的结果，用于coupled PDEs 的减少。实际证明，我们的方法能够捕捉到一些已知的分布shift形式，如极化和不同的影响。
</details></li>
</ul>
<hr>
<h2 id="Improving-Language-Plasticity-via-Pretraining-with-Active-Forgetting"><a href="#Improving-Language-Plasticity-via-Pretraining-with-Active-Forgetting" class="headerlink" title="Improving Language Plasticity via Pretraining with Active Forgetting"></a>Improving Language Plasticity via Pretraining with Active Forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01163">http://arxiv.org/abs/2307.01163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Chen, Kelly Marchisio, Roberta Raileanu, David Ifeoluwa Adelani, Pontus Stenetorp, Sebastian Riedel, Mikel Artetxe</li>
<li>for: 实现PLMs的 universality，将其应用于新语言。</li>
<li>methods: 使用活动遗忘机制 during pretraining，以实现PLMs快速适应新语言。</li>
<li>results: 在语言适应中，使用我们的遗忘机制可以提高PLMs的学习新embeddings的能力，并在仅有少量数据的情况下表现出佳。<details>
<summary>Abstract</summary>
Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, particularly for languages that are distant from English.
</details>
<details>
<summary>摘要</summary>
现在，预训练语言模型（PLM）是自然语言处理的主要模型。尽管它们在下游性能方面表现出色，但是将其应用到新语言可能会增加难度，从而限制其universal accessible的能力。先前的工作已经证明可以通过学习一个新的嵌入层来解决这个问题，但是这需要大量的数据和计算资源。我们提议使用活动忘记机制 durante la pretrainings，作为一种简单的创建 PLMs 可快速适应新语言的方法。具体来说，在每K更新中，我们会重置嵌入层，这会让 PLM 在有限的更新数量内提高其学习新嵌入的能力，类似于 meta-learning 效果。我们通过使用 RoBERTa 进行实验，发现使用我们的忘记机制不仅可以在语言适应过程中快速 convergence，而且在数据量低的情况下，特别是与英语远的语言，也能够表现出较好的性能。
</details></li>
</ul>
<hr>
<h2 id="Theory-of-Mind-as-Intrinsic-Motivation-for-Multi-Agent-Reinforcement-Learning"><a href="#Theory-of-Mind-as-Intrinsic-Motivation-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning"></a>Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01158">http://arxiv.org/abs/2307.01158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ini Oguntola, Joseph Campbell, Simon Stepputtis, Katia Sycara</li>
<li>for: 本研究旨在提高人工智能代理人在多智能环境中的社会智能，通过模拟他人的心理状态。</li>
<li>methods: 本研究使用深度网络模型策略，并将含义rich的信念嵌入策略中。然后，对每个代理人的信念预测能力作为多代理人学习的自适应奖励信号。</li>
<li>results: 在混合合作竞争环境中，该方法可以提高代理人之间的协作和竞争能力。<details>
<summary>Abstract</summary>
The ability to model the mental states of others is crucial to human social intelligence, and can offer similar benefits to artificial agents with respect to the social dynamics induced in multi-agent settings. We present a method of grounding semantically meaningful, human-interpretable beliefs within policies modeled by deep networks. We then consider the task of 2nd-order belief prediction. We propose that ability of each agent to predict the beliefs of the other agents can be used as an intrinsic reward signal for multi-agent reinforcement learning. Finally, we present preliminary empirical results in a mixed cooperative-competitive environment.
</details>
<details>
<summary>摘要</summary>
人类社交智能中能够模拟他人的心理状态是非常重要的，可以为人工智能agent提供类似的社交动力。我们提出了将 semantically meaningful和human-interpretable的beliefsgrounding在深度网络模型中的方法。然后我们考虑了第二个belief预测任务。我们认为每个agent可以预测其他agent的beliefs作为多 agent reinforcement learning中的内在奖励信号。最后，我们提供了一些初步的实验结果在混合合作-竞争环境中。Here's a word-for-word translation of the text:人类社交智能中能够模拟他人的心理状态是非常重要的，可以为人工智能agent提供类似的社交动力。我们提出了将semantically meaningful和human-interpretable的beliefsgrounding在深度网络模型中的方法。然后我们考虑了第二个belief预测任务。我们认为每个agent可以预测其他agent的beliefs作为多agent reinforcement learning中的内在奖励信号。最后，我们提供了一些初步的实验结果在混合合作-竞争环境中。
</details></li>
</ul>
<hr>
<h2 id="A-novel-approach-for-predicting-epidemiological-forecasting-parameters-based-on-real-time-signals-and-Data-Assimilation"><a href="#A-novel-approach-for-predicting-epidemiological-forecasting-parameters-based-on-real-time-signals-and-Data-Assimilation" class="headerlink" title="A novel approach for predicting epidemiological forecasting parameters based on real-time signals and Data Assimilation"></a>A novel approach for predicting epidemiological forecasting parameters based on real-time signals and Data Assimilation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01157">http://arxiv.org/abs/2307.01157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Molinas, César Quilodrán Casas, Rossella Arcucci, Ovidiu Şerban</li>
<li>for: 预测epidemiological参数，使用新的实时信号 integrate from various sources, such as social media-based population density maps and Air Quality data。</li>
<li>methods: 使用Convolutional Neural Networks (CNN) ensemble models and various data sources and fusion methodology to build robust predictions, and use data assimilation to estimate the state of the system from fused CNN predictions。</li>
<li>results: 提高了 COVID-19 疫情预测的性能和灵活性，并且比标准模型（SEIR）更高精度和更稳定。<details>
<summary>Abstract</summary>
This paper proposes a novel approach to predict epidemiological parameters by integrating new real-time signals from various sources of information, such as novel social media-based population density maps and Air Quality data. We implement an ensemble of Convolutional Neural Networks (CNN) models using various data sources and fusion methodology to build robust predictions and simulate several dynamic parameters that could improve the decision-making process for policymakers. Additionally, we used data assimilation to estimate the state of our system from fused CNN predictions. The combination of meteorological signals and social media-based population density maps improved the performance and flexibility of our prediction of the COVID-19 outbreak in London. While the proposed approach outperforms standard models, such as compartmental models traditionally used in disease forecasting (SEIR), generating robust and consistent predictions allows us to increase the stability of our model while increasing its accuracy.
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的方法，通过将新的实时信号 integrate into various sources of information, such as social media-based population density maps and Air Quality data, to predict epidemiological parameters。我们使用了一个ensemble of Convolutional Neural Networks (CNN) models and various data sources and fusion methodology to build robust predictions and simulate several dynamic parameters that could improve the decision-making process for policymakers。此外，我们使用数据充满来估计系统状态的CNN预测结果。 combining meteorological signals and social media-based population density maps improved the performance and flexibility of our prediction of the COVID-19 outbreak in London。相比标准模型（如SEIR组件模型），我们的方法具有更高的稳定性和准确性。
</details></li>
</ul>
<hr>
<h2 id="AVSegFormer-Audio-Visual-Segmentation-with-Transformer"><a href="#AVSegFormer-Audio-Visual-Segmentation-with-Transformer" class="headerlink" title="AVSegFormer: Audio-Visual Segmentation with Transformer"></a>AVSegFormer: Audio-Visual Segmentation with Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01146">http://arxiv.org/abs/2307.01146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vvvb-github/avsegformer">https://github.com/vvvb-github/avsegformer</a></li>
<li>paper_authors: Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, Tong Lu</li>
<li>for: 本研究旨在提出一种新的听视分割（AVS）任务，以解决在视频中找到并分割听起来的对象。</li>
<li>methods: 本文提出了一种基于transformer架构的AVSegFormer模型，通过引入听音查询和可学习查询，使网络可以 selectively 关注有趣的视觉特征。此外，我们还提出了一种听视混合器，可以动态调整视觉特征，并且设置了一个中间mask损失，以便更好地监督网络的预测。</li>
<li>results: 广泛的实验表明，AVSegFormer可以在AVS标准准样上取得状态的损失。网络代码可以在<a target="_blank" rel="noopener" href="https://github.com/vvvb-github/AVSegFormer%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/vvvb-github/AVSegFormer上下载。</a><details>
<summary>Abstract</summary>
The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate that AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is available at https://github.com/vvvb-github/AVSegFormer.
</details>
<details>
<summary>摘要</summary>
具有音频和视觉功能的组合已经是多Modal社区中的一个长期关注的话题。最近，一个新的音频视频分割（AVS）任务被引入，旨在在给定的视频中找到并分割声音的对象。这个任务要求音频驱动像素级场景理解，具有重大挑战。在这篇论文中，我们提出了AVSegFormer，一种新的AVS任务框架，利用转换架构。具体来说，我们引入了音频问题和学习问题到转换解码器中，使网络可以选择性地注意到有兴趣的视觉特征。此外，我们提出了一个音频视频混合器，可以动态调整视觉特征，增强有用的空间通道。此外，我们还提出了一个中间面 mask loss，以增强解码器的监督，让网络生成更加准确的中间预测。广泛的实验表明，AVSegFormer可以在AVS标准 bencmarks 上达到状态的最佳结果。代码可以在https://github.com/vvvb-github/AVSegFormer 中下载。
</details></li>
</ul>
<hr>
<h2 id="SCITUNE-Aligning-Large-Language-Models-with-Scientific-Multimodal-Instructions"><a href="#SCITUNE-Aligning-Large-Language-Models-with-Scientific-Multimodal-Instructions" class="headerlink" title="SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions"></a>SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01139">http://arxiv.org/abs/2307.01139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lupantech/ScienceQA">https://github.com/lupantech/ScienceQA</a></li>
<li>paper_authors: Sameera Horawalavithana, Sai Munikoti, Ian Stewart, Henry Kvinge</li>
<li>for: 这个论文旨在提高大型语言模型（LLM）的能力，使其更好地遵循科学 Multimodal 指令。</li>
<li>methods: 这个论文提出了 SciTune 调教框架，用于改进 LLM 的科学 Multimodal 理解能力。其中使用了人类生成的科学指令调教数据集，并训练了一个包含视觉编码器和 LLM 的多Modal 模型 LLaMA-SciTune。</li>
<li>results: 对比机器生成数据只进行finetuning的模型，LLaMA-SciTune 在科学QA benchmark中平均和许多子类型的人工性能都高于人类性能。<details>
<summary>Abstract</summary>
Instruction finetuning is a popular paradigm to align large language models (LLM) with human intent. Despite its popularity, this idea is less explored in improving the LLMs to align existing foundation models with scientific disciplines, concepts and goals. In this work, we present SciTune as a tuning framework to improve the ability of LLMs to follow scientific multimodal instructions. To test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model LLaMA-SciTune that connects a vision encoder and LLM for science-focused visual and language understanding. In comparison to the models that are finetuned with machine generated data only, LLaMA-SciTune surpasses human performance on average and in many sub-categories on the ScienceQA benchmark.
</details>
<details>
<summary>摘要</summary>
instruction fine-tuning是一种流行的思想，用于将大型语言模型（LLM）与人类意图进行对接。尽管这个想法在提高LLM对现有基础模型的适应性方面具有广泛的应用前景，但是它在科学领域中得到了更少的探索。在这项工作中，我们提出了SciTune作为一种调整框架，用于改进LLM对科学多Modal指令的遵循能力。为测试我们的方法，我们使用了人类生成的科学指令调整数据集，并训练了一个包含视觉编码器和LLM的科学频谱模型LLaMA-SciTune。与只使用机器生成的数据进行finetuning的模型相比，LLaMA-SciTune在科学问答 bencmark中平均和许多子类型上超越了人类性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/04/cs.LG_2023_07_04/" data-id="clp88dbx800nnob886jbtg1hn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/eess.IV_2023_07_04/" class="article-date">
  <time datetime="2023-07-04T09:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/eess.IV_2023_07_04/">eess.IV - 2023-07-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-Calibration-Bias-Without-Fixed-Attribute-Grouping-for-Improved-Fairness-in-Medical-Imaging-Analysis"><a href="#Mitigating-Calibration-Bias-Without-Fixed-Attribute-Grouping-for-Improved-Fairness-in-Medical-Imaging-Analysis" class="headerlink" title="Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis"></a>Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01738">http://arxiv.org/abs/2307.01738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changjian Shui, Justin Szeto, Raghav Mehta, Douglas L. Arnold, Tal Arbel</li>
<li>for: 避免深度学习医学图像分析模型在实际临床中的不可靠部署，需要进行准确性调整。</li>
<li>methods: 我们提出了一种两阶段方法：集群焦点法，首先标识不准确的样本，将其分为组，然后引入组织损失来改善准确性偏见。</li>
<li>results: 我们在皮肤病分类HAM10000 dataset和多发性硬化病患者未来病变预测 task 上进行了评估，结果表明，我们的方法可以有效控制最差表现 subgroup 的准确性错误，同时保持预测性能，并超越最近的基elines。<details>
<summary>Abstract</summary>
Trustworthy deployment of deep learning medical imaging models into real-world clinical practice requires that they be calibrated. However, models that are well calibrated overall can still be poorly calibrated for a sub-population, potentially resulting in a clinician unwittingly making poor decisions for this group based on the recommendations of the model. Although methods have been shown to successfully mitigate biases across subgroups in terms of model accuracy, this work focuses on the open problem of mitigating calibration biases in the context of medical image analysis. Our method does not require subgroup attributes during training, permitting the flexibility to mitigate biases for different choices of sensitive attributes without re-training. To this end, we propose a novel two-stage method: Cluster-Focal to first identify poorly calibrated samples, cluster them into groups, and then introduce group-wise focal loss to improve calibration bias. We evaluate our method on skin lesion classification with the public HAM10000 dataset, and on predicting future lesional activity for multiple sclerosis (MS) patients. In addition to considering traditional sensitive attributes (e.g. age, sex) with demographic subgroups, we also consider biases among groups with different image-derived attributes, such as lesion load, which are required in medical image analysis. Our results demonstrate that our method effectively controls calibration error in the worst-performing subgroups while preserving prediction performance, and outperforming recent baselines.
</details>
<details>
<summary>摘要</summary>
信任性的深入学习医学影像模型在实际临床应用中需要进行准确化。然而，即使模型在整体上具有良好的准确性，也可能对一个子群体存在不良准确性，导致医生不知道基于模型的建议而做出不良决策。虽然有些方法可以成功地消除 subgroup 的偏见，但这项工作将关注在医学影像分析中的开放问题上，即如何消除准确性偏见。我们的方法不需要在训练过程中提供 subgroup 特征，因此可以随意地消除不同敏感特征的偏见。为此，我们提出了一种新的两stage方法：集群焦点法。首先，我们将准确性不佳的样本分成集群，然后引入集群级别的焦点损失来改善准确性偏见。我们在皮肤病分类 task 和多发性精神病（MS）患者未来病变预测任务上进行了评估。除了考虑传统的敏感特征（例如年龄、性别）与人口 subgroup 之外，我们还考虑了医学影像分析中的不同图像特征，如病虫荷载，这些特征是必需的。我们的结果表明，我们的方法可以有效地控制最差 subgroup 的准确性错误，保持预测性能，并超越最新的基eline。
</details></li>
</ul>
<hr>
<h2 id="Once-Training-All-Fine-No-Reference-Point-Cloud-Quality-Assessment-via-Domain-relevance-Degradation-Description"><a href="#Once-Training-All-Fine-No-Reference-Point-Cloud-Quality-Assessment-via-Domain-relevance-Degradation-Description" class="headerlink" title="Once-Training-All-Fine: No-Reference Point Cloud Quality Assessment via Domain-relevance Degradation Description"></a>Once-Training-All-Fine: No-Reference Point Cloud Quality Assessment via Domain-relevance Degradation Description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01567">http://arxiv.org/abs/2307.01567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yipeng Liu, Qi Yang, Yujie Zhang, Yiling Xu, Le Yang, Xiaozhong Xu, Shan Liu</li>
<li>for: 提高无参考点云质量评估（NR-PCQA）方法的一般化性能。</li>
<li>methods: 提出一种基于域相关性的NR-PCQA方法，包括解释性模型、域转换和Semantic explanation。</li>
<li>results: 实验结果表明，提出的D$^3$-PCQA方法在多个公开数据集上 exhibits 强大的一般化能力和 robust性。<details>
<summary>Abstract</summary>
Full-reference (FR) point cloud quality assessment (PCQA) has achieved impressive progress in recent years. However, as reference point clouds are not available in many cases, no-reference (NR) metrics have become a research hotspot. Existing NR methods suffer from poor generalization performance. To address this shortcoming, we propose a novel NR-PCQA method, Point Cloud Quality Assessment via Domain-relevance Degradation Description (D$^3$-PCQA). First, we demonstrate our model's interpretability by deriving the function of each module using a kernelized ridge regression model. Specifically, quality assessment can be characterized as a leap from the scattered perceptual domain (reflecting subjective perception) to the ordered quality domain (reflecting mean opinion score). Second, to reduce the significant domain discrepancy, we establish an intermediate domain, the description domain, based on insights from subjective experiments, by considering the domain relevance among samples located in the perception domain and learning a structured latent space. The anchor features derived from the learned latent space are generated as cross-domain auxiliary information to promote domain transformation. Furthermore, the newly established description domain decomposes the NR-PCQA problem into two relevant stages. These stages include a classification stage that gives the degradation descriptions to point clouds and a regression stage to determine the confidence degrees of descriptions, providing a semantic explanation for the predicted quality scores. Experimental results demonstrate that D$^3$-PCQA exhibits robust performance and outstanding generalization ability on several publicly available datasets. The code in this work will be publicly available at https://smt.sjtu.edu.cn.
</details>
<details>
<summary>摘要</summary>
Full-reference (FR) 点云质量评估 (PCQA) 在最近几年内取得了显著的进步。然而，由于参考点云不常可用，无参考 (NR) 指标成为了研究热点。现有的 NR 方法受到质量评估的泛化性能的限制。为了解决这一缺点，我们提出了一种新的 NR-PCQA 方法，即 Point Cloud Quality Assessment via Domain-relevance Degradation Description (D$^3$-PCQA)。首先，我们证明了我们的模型的可解释性，通过使用kernelized ridge regression模型来 derivate每个模块的函数。具体来说，质量评估可以被描述为从杂乱的感知领域（反映主观感受）到有序的质量领域（反映意见票）的跳跃。其次，为了减少域外差，我们建立了一个中间域，即描述域，基于对主观实验所获得的域相关性的思考。通过学习协同的秘密空间，我们生成了跨域的帮助信息，以便进行域转换。此外，我们新建立的描述域将 NR-PCQA 问题分解成两个相关的阶段。这两个阶段分别是用来给点云的质量描述和确定描述的可信度的阶段，从而提供了 semantics 的解释。实验结果表明，D$^3$-PCQA 具有出色的 Robustness 和泛化能力，在多个公开可用的数据集上达到了优秀的性能。我们将在https://smt.sjtu.edu.cn上公开代码。
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Perception-Distortion-Trade-off-in-Learned-Video-SR"><a href="#Spatio-Temporal-Perception-Distortion-Trade-off-in-Learned-Video-SR" class="headerlink" title="Spatio-Temporal Perception-Distortion Trade-off in Learned Video SR"></a>Spatio-Temporal Perception-Distortion Trade-off in Learned Video SR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01556">http://arxiv.org/abs/2307.01556</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kuis-ai-tekalp-research-group/perceptual-vsr">https://github.com/kuis-ai-tekalp-research-group/perceptual-vsr</a></li>
<li>paper_authors: Nasrin Rahimi, A. Murat Tekalp</li>
<li>for: 这个论文旨在探讨视频超解像的准确性评价方法，尤其是考虑视频中的运动流动性。</li>
<li>methods: 该论文提出了一种新的视频质量评价指标，强调视频中的自然运动流动性，并提出了一种基于这个指标的视频超解像框架（PSVR）。</li>
<li>results: 实验结果表明，该论文提出的评价指标和框架可以更好地评价视频超解像的准确性，并且支持假设，即视频准确性评价应该考虑运动流动性的自然性。<details>
<summary>Abstract</summary>
Perception-distortion trade-off is well-understood for single-image super-resolution. However, its extension to video super-resolution (VSR) is not straightforward, since popular perceptual measures only evaluate naturalness of spatial textures and do not take naturalness of flow (temporal coherence) into account. To this effect, we propose a new measure of spatio-temporal perceptual video quality emphasizing naturalness of optical flow via the perceptual straightness hypothesis (PSH) for meaningful spatio-temporal perception-distortion trade-off. We also propose a new architecture for perceptual VSR (PSVR) to explicitly enforce naturalness of flow to achieve realistic spatio-temporal perception-distortion trade-off according to the proposed measures. Experimental results with PVSR support the hypothesis that a meaningful perception-distortion tradeoff for video should account for the naturalness of motion in addition to naturalness of texture.
</details>
<details>
<summary>摘要</summary>
文本扭曲质量评估对单张超高清图像处理well understood,但是扩展到视频超高清图像（VSR）并不直接，因为流行的感知度量只评估自然性的空间纹理，而不考虑流动的自然性（时间准确性）。为此，我们提出了一种新的spatio-temporal感知质量指标，强调流动的自然性via the perceptual straightness hypothesis（PSH），以实现有意义的spatio-temporal扭曲质量评估。我们还提出了一种新的PSVR架构，以直接强制实现流动的自然性，以达到实际的spatio-temporal扭曲质量评估。实验结果表示，在PVSR中，一个有意义的扭曲质量评估应该考虑流动的自然性，不仅是空间纹理的自然性。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Transformer-for-Autonomous-Recognition-and-Grading-of-Tomatoes-Under-Various-Lighting-Occlusion-and-Ripeness-Conditions"><a href="#Convolutional-Transformer-for-Autonomous-Recognition-and-Grading-of-Tomatoes-Under-Various-Lighting-Occlusion-and-Ripeness-Conditions" class="headerlink" title="Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions"></a>Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01530">http://arxiv.org/abs/2307.01530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asim Khan, Taimur Hassan, Muhammad Shafay, Israa Fahmy, Naoufel Werghi, Lakmal Seneviratne, Irfan Hussain</li>
<li>for: 本研究旨在开发一种自主识别和评估 Tomatoes 的框架，以便在实际场景中使用移动机器人收割 Tomatoes。</li>
<li>methods: 本研究使用一种卷积变换器架构，通过自动识别和评估 Tomatoes，缓解因为叶子和枝条等因素而导致的 occlusion 问题。</li>
<li>results: 经过训练和测试，提出的方法在不同的照明条件和观察角度下，对 Tomatoes 的识别和评估表现出色，比基eline方法和先前方法高出58.14%、65.42% 和 66.39% 的mean average precision 分数。<details>
<summary>Abstract</summary>
Harvesting fully ripe tomatoes with mobile robots presents significant challenges in real-world scenarios. These challenges arise from factors such as occlusion caused by leaves and branches, as well as the color similarity between tomatoes and the surrounding foliage during the fruit development stage. The natural environment further compounds these issues with varying light conditions, viewing angles, occlusion factors, and different maturity levels. To overcome these obstacles, this research introduces a novel framework that leverages a convolutional transformer architecture to autonomously recognize and grade tomatoes, irrespective of their occlusion level, lighting conditions, and ripeness. The proposed model is trained and tested using carefully annotated images curated specifically for this purpose. The dataset is prepared under various lighting conditions, viewing perspectives, and employs different mobile camera sensors, distinguishing it from existing datasets such as Laboro Tomato and Rob2Pheno Annotated Tomato. The effectiveness of the proposed framework in handling cluttered and occluded tomato instances was evaluated using two additional public datasets, Laboro Tomato and Rob2Pheno Annotated Tomato, as benchmarks. The evaluation results across these three datasets demonstrate the exceptional performance of our proposed framework, surpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of mean average precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated Tomato, respectively. The results underscore the superiority of the proposed model in accurately detecting and delineating tomatoes compared to baseline methods and previous approaches. Specifically, the model achieves an F1-score of 80.14%, a Dice coefficient of 73.26%, and a mean IoU of 66.41% on the KUTomaData image dataset.
</details>
<details>
<summary>摘要</summary>
采收完全熟 Tomatoes 的 mobile robot 存在许多实际应用中的挑战。这些挑战包括由叶子和枝头所引起的遮掩、 Tomatoes 和周围的植物发育阶段的颜色相似性，以及自然环境中的不同光照条件、观察角度和不同熟度水平。为了解决这些问题，这项研究提出了一个新的框架，利用卷积变换器架构来自动识别和分级 Tomatoes，无论它们的遮掩水平、光照条件和熟度如何。该提案的模型被训练和测试使用特意为这项研究制作的注意词汇图像集。该数据集在不同的光照条件下、不同的观察角度下和使用不同的移动摄像头感知器时被准备。与现有的数据集不同，这个数据集不仅包括不同的光照条件和观察角度，还使用了不同的移动摄像头感知器。为了评估该提案的效果，研究者们使用了另外两个公共数据集作为参照，即 Laboro Tomato 和 Rob2Pheno Annotated Tomato。结果表明，该提案的模型在处理受遮掩和受遮掩的 Tomatoes 实例时表现出色，与基准方法和先前的方法相比，提高了58.14%、65.42%和66.39%的平均准确率。这些结果表明，该模型在识别和定义 Tomatoes 方面具有出色的性能，而不是基准方法和先前的方法。具体来说，模型在 KUTomaData 图像集上 achieve 的 F1 分数为 80.14%，Dice 系数为 73.26%，和 Mean IoU 为 66.41%。
</details></li>
</ul>
<hr>
<h2 id="H-DenseFormer-An-Efficient-Hybrid-Densely-Connected-Transformer-for-Multimodal-Tumor-Segmentation"><a href="#H-DenseFormer-An-Efficient-Hybrid-Densely-Connected-Transformer-for-Multimodal-Tumor-Segmentation" class="headerlink" title="H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation"></a>H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01486">http://arxiv.org/abs/2307.01486</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shijun18/h-denseformer">https://github.com/shijun18/h-denseformer</a></li>
<li>paper_authors: Jun Shi, Hongyu Kan, Shulan Ruan, Ziqi Zhu, Minfan Zhao, Liang Qiao, Zhaohui Wang, Hong An, Xudong Xue</li>
<li>for: 这篇论文是针对多Modal的医疗影像肿瘤分类问题提出的一个新方法。</li>
<li>methods: 本文提出了一个混合了CNN和Transformer结构的对称网络，名为H-DenseFormer，它可以将多Modal的输入转换为融合特征，并将这些融合特征传递到不同层次的Encoder中进行增强多Modal学习表现。此外，本文还提出了一个轻量级的DCT块来取代标准Transformer块，以减少计算复杂度。</li>
<li>results: 在两个公共的多Modal数据集上进行了广泛的实验，结果显示了我们的提案方法在与现有的State-of-the-art方法进行比较时，具有更好的表现，同时计算复杂度较低。<details>
<summary>Abstract</summary>
Recently, deep learning methods have been widely used for tumor segmentation of multimodal medical images with promising results. However, most existing methods are limited by insufficient representational ability, specific modality number and high computational complexity. In this paper, we propose a hybrid densely connected network for tumor segmentation, named H-DenseFormer, which combines the representational power of the Convolutional Neural Network (CNN) and the Transformer structures. Specifically, H-DenseFormer integrates a Transformer-based Multi-path Parallel Embedding (MPE) module that can take an arbitrary number of modalities as input to extract the fusion features from different modalities. Then, the multimodal fusion features are delivered to different levels of the encoder to enhance multimodal learning representation. Besides, we design a lightweight Densely Connected Transformer (DCT) block to replace the standard Transformer block, thus significantly reducing computational complexity. We conduct extensive experiments on two public multimodal datasets, HECKTOR21 and PI-CAI22. The experimental results show that our proposed method outperforms the existing state-of-the-art methods while having lower computational complexity. The source code is available at https://github.com/shijun18/H-DenseFormer.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:近期，深度学习方法在多Modal医疗影像肿瘤分割领域得到了广泛应用，并取得了Promising的结果。然而，大多数现有方法受到不充分的表达能力、特定的Modal数量和高计算复杂性的限制。在本文中，我们提出了一种混合密集连接网络，名为H-DenseFormer，它结合了Convolutional Neural Network (CNN)和Transformer结构的表达力。具体来说，H-DenseFormer integrate了一个基于Transformer的多路平行嵌入（MPE）模块，可以将多Modal的输入作为输入，以提取不同Modal的融合特征。然后，这些融合特征被传递到不同层的编码器，以增强多Modal学习表达。此外，我们设计了一个轻量级的Densely Connected Transformer（DCT）块，以取代标准Transformer块，从而显著降低计算复杂性。我们在HECKTOR21和PI-CAI22两个公共多Modal数据集上进行了广泛的实验。实验结果表明，我们提出的方法可以比现有的状态级方法更高效，同时计算复杂性也更低。源代码可以在https://github.com/shijun18/H-DenseFormer上获取。
</details></li>
</ul>
<hr>
<h2 id="Zero-DeepSub-Zero-Shot-Deep-Subspace-Reconstruction-for-Rapid-Multiparametric-Quantitative-MRI-Using-3D-QALAS"><a href="#Zero-DeepSub-Zero-Shot-Deep-Subspace-Reconstruction-for-Rapid-Multiparametric-Quantitative-MRI-Using-3D-QALAS" class="headerlink" title="Zero-DeepSub: Zero-Shot Deep Subspace Reconstruction for Rapid Multiparametric Quantitative MRI Using 3D-QALAS"></a>Zero-DeepSub: Zero-Shot Deep Subspace Reconstruction for Rapid Multiparametric Quantitative MRI Using 3D-QALAS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01410">http://arxiv.org/abs/2307.01410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yohan Jun, Yamin Arefeen, Jaejin Cho, Shohei Fujita, Xiaoqing Wang, P. Ellen Grant, Borjan Gagoski, Camilo Jaimes, Michael S. Gee, Berkin Bilgic</li>
<li>for: develop and evaluate methods for reconstructing 3D-quantification using an interleaved Look-Locker acquisition sequence with T2 preparation pulse (3D-QALAS) time-series images</li>
<li>methods: using a low-rank subspace method and zero-shot deep-learning subspace method (Zero-DeepSub) for rapid and high fidelity T1 and T2 mapping and time-resolved imaging</li>
<li>results: good linearity and reduced biases compared to conventional QALAS, better g-factor maps and reduced voxel blurring, noise, and artifacts compared to conventional QALAS, and robust performance at up to 9-fold acceleration with Zero-DeepSub enabled whole-brain T1, T2, and PD mapping at 1 mm isotropic resolution within 2 min of scan time.Here’s the format you requested:</li>
<li>for: develop and evaluate methods for 3D-quantification using 3D-QALAS time-series images</li>
<li>methods: using low-rank subspace method and Zero-DeepSub</li>
<li>results: good linearity, reduced biases, better g-factor maps, and reduced voxel blurring, noise, and artifacts, and robust performance at up to 9-fold acceleration<details>
<summary>Abstract</summary>
Purpose: To develop and evaluate methods for 1) reconstructing 3D-quantification using an interleaved Look-Locker acquisition sequence with T2 preparation pulse (3D-QALAS) time-series images using a low-rank subspace method, which enables accurate and rapid T1 and T2 mapping, and 2) improving the fidelity of subspace QALAS by combining scan-specific deep-learning-based reconstruction and subspace modeling. Methods: A low-rank subspace method for 3D-QALAS (i.e., subspace QALAS) and zero-shot deep-learning subspace method (i.e., Zero-DeepSub) were proposed for rapid and high fidelity T1 and T2 mapping and time-resolved imaging using 3D-QALAS. Using an ISMRM/NIST system phantom, the accuracy of the T1 and T2 maps estimated using the proposed methods was evaluated by comparing them with reference techniques. The reconstruction performance of the proposed subspace QALAS using Zero-DeepSub was evaluated in vivo and compared with conventional QALAS at high reduction factors of up to 9-fold. Results: Phantom experiments showed that subspace QALAS had good linearity with respect to the reference methods while reducing biases compared to conventional QALAS, especially for T2 maps. Moreover, in vivo results demonstrated that subspace QALAS had better g-factor maps and could reduce voxel blurring, noise, and artifacts compared to conventional QALAS and showed robust performance at up to 9-fold acceleration with Zero-DeepSub, which enabled whole-brain T1, T2, and PD mapping at 1 mm isotropic resolution within 2 min of scan time. Conclusion: The proposed subspace QALAS along with Zero-DeepSub enabled high fidelity and rapid whole-brain multiparametric quantification and time-resolved imaging.
</details>
<details>
<summary>摘要</summary>
目的：开发和评估使用排序 Look-Locker 类型的三维量化（3D-QALAS）时间序列图像的重要方法，以实现精确和快速的 T1 和 T2 地图的构建，并且提高 subspace QALAS 的实用性。方法：提出了一种基于低维度的 subspace QALAS 方法和 zero-shot 深度学习 subspace 方法（Zero-DeepSub），用于快速和高实用性的 T1 和 T2 地图和时间分辨图像的重建。使用 ISMRM/NIST 系统实验库中的实验库，评估了提案方法中的 T1 和 T2 地图的准确性，并与参考方法进行比较。结果：实验结果显示，subspace QALAS 具有对于参考方法的良好线性性，而且可以降低 conventional QALAS 中的偏差，特别是 T2 地图。此外，在 vivo 中的结果显示，subspace QALAS 可以提供更好的 g-因素地图，并且可以降低像素模糊、噪音和错误，并且在 Zero-DeepSub 的支持下，可以在 9 倍的压缩因子下进行快速的构建。结论：提案的 subspace QALAS 和 Zero-DeepSub 可以实现高实用性和快速的全脑多 parametr 量化和时间分辨图像。
</details></li>
</ul>
<hr>
<h2 id="A-CNN-regression-model-to-estimate-buildings-height-maps-using-Sentinel-1-SAR-and-Sentinel-2-MSI-time-series"><a href="#A-CNN-regression-model-to-estimate-buildings-height-maps-using-Sentinel-1-SAR-and-Sentinel-2-MSI-time-series" class="headerlink" title="A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series"></a>A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01378">http://arxiv.org/abs/2307.01378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritu Yadav, Andrea Nascetti, Yifang Ban</li>
<li>for: 这个研究旨在提出一个监督学习的多modal建筑高度回溯网络（MBHR-Net），用于以10米间隔估计建筑高度使用快射-1（S1）和快射-2（S2）卫星时间序列。</li>
<li>methods: 这个研究使用了S1提供的Synthetic Aperture Radar（SAR）数据，以及S2提供的多spectral数据，并使用深度学习模型将这两种数据融合以学习复杂的空间-时间关系。</li>
<li>results: 这个研究的初步结果显示MBHR-Net可以实现高度精准的估计（3.73米RMSE、0.95 IoU、0.61 R2），表明这个深度学习模型具有实用的应用前景，包括城市规划、环境影响分析等。<details>
<summary>Abstract</summary>
Accurate estimation of building heights is essential for urban planning, infrastructure management, and environmental analysis. In this study, we propose a supervised Multimodal Building Height Regression Network (MBHR-Net) for estimating building heights at 10m spatial resolution using Sentinel-1 (S1) and Sentinel-2 (S2) satellite time series. S1 provides Synthetic Aperture Radar (SAR) data that offers valuable information on building structures, while S2 provides multispectral data that is sensitive to different land cover types, vegetation phenology, and building shadows. Our MBHR-Net aims to extract meaningful features from the S1 and S2 images to learn complex spatio-temporal relationships between image patterns and building heights. The model is trained and tested in 10 cities in the Netherlands. Root Mean Squared Error (RMSE), Intersection over Union (IOU), and R-squared (R2) score metrics are used to evaluate the performance of the model. The preliminary results (3.73m RMSE, 0.95 IoU, 0.61 R2) demonstrate the effectiveness of our deep learning model in accurately estimating building heights, showcasing its potential for urban planning, environmental impact analysis, and other related applications.
</details>
<details>
<summary>摘要</summary>
准确估算建筑高度是城市规划、基础设施管理和环境分析中非常重要的。在本研究中，我们提出了一种监督式多模态建筑高度回归网络（MBHR-Net），用于使用 Sentinal-1（S1）和 Sentinal-2（S2）卫星时序序数据来估算建筑高度的10米空间分辨率。S1提供Synthetic Aperture Radar（SAR）数据，可以提供建筑结构的有价信息，而S2提供多spectral数据，敏感于不同的地表类型、植被生长阶段和建筑阴影。我们的MBHR-Net试图从S1和S2图像中提取有用的特征，以学习图像模式和建筑高度之间的复杂空间时间关系。模型在荷兰10座城市进行训练和测试。使用Root Mean Squared Error（RMSE）、Intersection over Union（IOU）和R-squared（R2） метри来评估模型的性能。初步结果（3.73米RMSE、0.95 IoU、0.61 R2）表明我们的深度学习模型可以准确地估算建筑高度，展示其在城市规划、环境影响分析等相关应用中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Data-Memorization-in-3D-Latent-Diffusion-Models-for-Medical-Image-Synthesis"><a href="#Investigating-Data-Memorization-in-3D-Latent-Diffusion-Models-for-Medical-Image-Synthesis" class="headerlink" title="Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis"></a>Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01148">http://arxiv.org/abs/2307.01148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salman Ul Hassan Dar, Arman Ghanaat, Jannik Kahmann, Isabelle Ayx, Theano Papavassiliu, Stefan O. Schoenberg, Sandy Engelhardt</li>
<li>for: 这个论文的目的是评估三维潜在扩散模型在生成医疗数据方面的能力。</li>
<li>methods: 该论文使用了自我超vised模型基于对比学习来检测潜在的记忆效应。</li>
<li>results: 研究结果表明，这些潜在扩散模型确实会记忆训练数据，需要采取措施来缓解这种记忆效应。<details>
<summary>Abstract</summary>
Generative latent diffusion models have been established as state-of-the-art in data generation. One promising application is generation of realistic synthetic medical imaging data for open data sharing without compromising patient privacy. Despite the promise, the capacity of such models to memorize sensitive patient training data and synthesize samples showing high resemblance to training data samples is relatively unexplored. Here, we assess the memorization capacity of 3D latent diffusion models on photon-counting coronary computed tomography angiography and knee magnetic resonance imaging datasets. To detect potential memorization of training samples, we utilize self-supervised models based on contrastive learning. Our results suggest that such latent diffusion models indeed memorize training data, and there is a dire need for devising strategies to mitigate memorization.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化字Simplified Chinese。<</SYS>>生成式潜在扩散模型已成为数据生成领域的状态机。一个有前途的应用是生成真实的医疗数据，以便在开放数据分享无需妥协病人隐私。虽然有承诺，但是这些模型对敏感病人训练数据的记忆能力和生成样本高度相似的样本的 sintesis能力尚未得到充分探讨。我们在 photon-counting coronary computed tomography angiography和 knee magnetic resonance imaging 数据集上评估了3D潜在扩散模型的记忆能力。为检测可能的记忆 Training samples，我们利用了自我超VI的 contrastive learning。我们的结果表明，这些潜在扩散模型确实记忆训练数据，而需要采取措施来缓解记忆。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/04/eess.IV_2023_07_04/" data-id="clp88dc44016mob883yr83cm1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/03/cs.SD_2023_07_03/" class="article-date">
  <time datetime="2023-07-03T15:00:00.000Z" itemprop="datePublished">2023-07-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/03/cs.SD_2023_07_03/">cs.SD - 2023-07-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="musif-a-Python-package-for-symbolic-music-feature-extraction"><a href="#musif-a-Python-package-for-symbolic-music-feature-extraction" class="headerlink" title="musif: a Python package for symbolic music feature extraction"></a>musif: a Python package for symbolic music feature extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01120">http://arxiv.org/abs/2307.01120</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/didoneproject/musif">https://github.com/didoneproject/musif</a></li>
<li>paper_authors: Ana Llorens, Federico Simonetta, Martín Serrano, Álvaro Torrente</li>
<li>for: 本研究团队开发了一个名为musif的Python包，用于自动提取Symbolic Music Score中的特征。</li>
<li>methods: musif包包含了一大量的特征，这些特征由音乐学家、音乐理论家、统计学家和计算机科学家团队共同开发。此外，包还允许用户轻松创建自定义特征使用常用的Python库。</li>
<li>results: musif包支持处理高质量的MusicXML格式音乐学数据，同时也支持其他常用的音乐信息检索任务格式，如MIDI、MEI、Kern等。作者提供了详细的文档和教程，以帮助扩展框架并帮助新手了解其使用。<details>
<summary>Abstract</summary>
In this work, we introduce musif, a Python package that facilitates the automatic extraction of features from symbolic music scores. The package includes the implementation of a large number of features, which have been developed by a team of experts in musicology, music theory, statistics, and computer science. Additionally, the package allows for the easy creation of custom features using commonly available Python libraries. musif is primarily geared towards processing high-quality musicological data encoded in MusicXML format, but also supports other formats commonly used in music information retrieval tasks, including MIDI, MEI, Kern, and others. We provide comprehensive documentation and tutorials to aid in the extension of the framework and to facilitate the introduction of new and inexperienced users to its usage.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了musif，一个Python包，用于自动提取符号音乐谱的特征。该包包括一大量的特征，由音乐学、音乐理论、统计和计算机科学领域的专家们开发。此外，包还允许用户轻松创建自定义特征使用常用的Python库。musif主要针对高质量的音乐学数据编码为MusicXML格式进行处理，也支持其他常用于音乐信息检索任务的格式，包括MIDI、MEI、Kern等。我们提供了完善的文档和教程，以帮助扩展该框架并帮助新用户入门使用。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Contextual-Adapters-To-Improve-Custom-Word-Recognition-In-Low-resource-Languages"><a href="#Multilingual-Contextual-Adapters-To-Improve-Custom-Word-Recognition-In-Low-resource-Languages" class="headerlink" title="Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages"></a>Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00759">http://arxiv.org/abs/2307.00759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Devang Kulshreshtha, Saket Dingliwal, Brady Houston, Sravan Bodapati</li>
<li>for: 提高自然语言处理（NLP）中自定义单词的识别率</li>
<li>methods: 使用 Contextual Adapters 进行注意力基于偏移的偏移模型，并在训练过程中使用超vision损失来缓和训练</li>
<li>results: 在低资源语言中提高了自定义单词的检索精度，实现了48% F1提升，同时也导致了基础 CTCL 模型的5-11% 词错率下降<details>
<summary>Abstract</summary>
Connectionist Temporal Classification (CTC) models are popular for their balance between speed and performance for Automatic Speech Recognition (ASR). However, these CTC models still struggle in other areas, such as personalization towards custom words. A recent approach explores Contextual Adapters, wherein an attention-based biasing model for CTC is used to improve the recognition of custom entities. While this approach works well with enough data, we showcase that it isn't an effective strategy for low-resource languages. In this work, we propose a supervision loss for smoother training of the Contextual Adapters. Further, we explore a multilingual strategy to improve performance with limited training data. Our method achieves 48% F1 improvement in retrieving unseen custom entities for a low-resource language. Interestingly, as a by-product of training the Contextual Adapters, we see a 5-11% Word Error Rate (WER) reduction in the performance of the base CTC model as well.
</details>
<details>
<summary>摘要</summary>
卷积时序分类（CTC）模型在自动语音识别（ASR）中具有平衡速度和性能的优点，但这些模型仍然在其他领域面临挑战，例如个性化向custom字进行个性化。一种最近的方法是使用上下文适应器来改善CTC模型中的认知 CustomEntities的识别。虽然这种方法在充足的数据量下工作良好，但我们发现在低资源语言上这种策略并不是有效的。在这种情况下，我们提出了一种超vision损失来帮助Contextual Adapters更平滑地训练。此外，我们探索了一种多语言策略以提高具有有限训练数据的性能。我们的方法实现了一个48%的F1提升在检索未看过的个性化字符串中，并且 Interestingly, 在训练Contextual Adapters的过程中，我们发现了5-11%的单词错误率（WER）下降在基本CTC模型的性能中。
</details></li>
</ul>
<hr>
<h2 id="An-End-to-End-Multi-Module-Audio-Deepfake-Generation-System-for-ADD-Challenge-2023"><a href="#An-End-to-End-Multi-Module-Audio-Deepfake-Generation-System-for-ADD-Challenge-2023" class="headerlink" title="An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023"></a>An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00729">http://arxiv.org/abs/2307.00729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheng Zhao, Qilong Yuan, Yibo Duan, Zhuoyue Chen</li>
<li>for: 本研究主要目标是开发一种可以生成语音内容的语音生成模型，以便模拟人工声音。</li>
<li>methods: 该模型采用了端到端多模块结构，包括说话者编码器、基于Tacotron2的合成器和基于WaveRNN的 vocoder。</li>
<li>results: 经过多种比较实验和模型结构的研究，该模型最终在ADD 2023挑战赛Track 1.1中获得了44.97%的Weighted Deception Success Rate（WDSR）。<details>
<summary>Abstract</summary>
The task of synthetic speech generation is to generate language content from a given text, then simulating fake human voice.The key factors that determine the effect of synthetic speech generation mainly include speed of generation, accuracy of word segmentation, naturalness of synthesized speech, etc. This paper builds an end-to-end multi-module synthetic speech generation model, including speaker encoder, synthesizer based on Tacotron2, and vocoder based on WaveRNN. In addition, we perform a lot of comparative experiments on different datasets and various model structures. Finally, we won the first place in the ADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of 44.97%.
</details>
<details>
<summary>摘要</summary>
文本生成任务的目标是将文本转化为语言内容，然后模拟人工嗓音。主要影响生成效果的因素包括生成速度、单词分 segmentation 精度、生成的嗓音自然程度等。这篇文章建立了端到端多模块合成嗓音模型，包括说话者编码器、基于 Tacotron2 的生成器和基于 WaveRNN 的 vocoder。此外，我们进行了多种比较 эксперименты，包括不同的数据集和模型结构。最后，我们在 ADD 2023 挑战赛 Track 1.1 中获得了44.97%的Weighted Deception Success Rate（WDSR）。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/03/cs.SD_2023_07_03/" data-id="clp88dbzw00v8ob889vp33z9l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/03/eess.AS_2023_07_03/" class="article-date">
  <time datetime="2023-07-03T14:00:00.000Z" itemprop="datePublished">2023-07-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/03/eess.AS_2023_07_03/">eess.AS - 2023-07-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ContextSpeech-Expressive-and-Efficient-Text-to-Speech-for-Paragraph-Reading"><a href="#ContextSpeech-Expressive-and-Efficient-Text-to-Speech-for-Paragraph-Reading" class="headerlink" title="ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading"></a>ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00782">http://arxiv.org/abs/2307.00782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujia Xiao, Shaofei Zhang, Xi Wang, Xu Tan, Lei He, Sheng Zhao, Frank K. Soong, Tan Lee</li>
<li>for: 这项研究旨在提高文本转语音（TTS）系统的长文朗读质量。</li>
<li>methods: 该研究提出了一种轻量级 yet有效的 TTS 系统，即 ContextSpeech。该系统首先设计了一种储存机制，以利用全文和语音上下文来增强句子编码。然后，它构建了层次结构的文本 semantics，以扩大全文上下文的增强范围。最后，它综合应用了线性化自注意力，以提高模型效率。</li>
<li>results: 实验表明，ContextSpeech 在段落读物中提高了声音质量和语调表达性，与竞争性模型相当。示例响应器可以在以下链接中浏览：<a target="_blank" rel="noopener" href="https://contextspeech.github.io/demo/">https://contextspeech.github.io/demo/</a><details>
<summary>Abstract</summary>
While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.github.io/demo/
</details>
<details>
<summary>摘要</summary>
“当前的文本到语音系统可以生成具有非常高质量的自然语音，但是在段落/长文读取中仍然存在很大的挑战。这些问题的原因是：一、忽略跨句Contextual信息，二、长文合成的计算和内存成本过高。为了解决这些问题，本工作开发了一个轻量级又有效的文本到语音系统——ContextSpeech。具体来说，我们首先设计了一种嵌入式的记忆缓存机制，以将全文和语音Context incorporated into sentence encoding。然后，我们构建了层次结构的文本 semantics，以扩大全文Context的增强范围。此外，我们将Linearized self-attention integrated into the model，以提高模型效率。实验表明，ContextSpeech可以在段落读取中显著提高声音质量和表达性，并且与其他模型相比，其效率相对较高。听 samples可以在：https://contextspeech.github.io/demo/ ”Note that the translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/03/eess.AS_2023_07_03/" data-id="clp88dc2j012qob88dr4dggk0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/03/cs.CV_2023_07_03/" class="article-date">
  <time datetime="2023-07-03T13:00:00.000Z" itemprop="datePublished">2023-07-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/03/cs.CV_2023_07_03/">cs.CV - 2023-07-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Cross-modality-Attention-Adapter-A-Glioma-Segmentation-Fine-tuning-Method-for-SAM-Using-Multimodal-Brain-MR-Images"><a href="#Cross-modality-Attention-Adapter-A-Glioma-Segmentation-Fine-tuning-Method-for-SAM-Using-Multimodal-Brain-MR-Images" class="headerlink" title="Cross-modality Attention Adapter: A Glioma Segmentation Fine-tuning Method for SAM Using Multimodal Brain MR Images"></a>Cross-modality Attention Adapter: A Glioma Segmentation Fine-tuning Method for SAM Using Multimodal Brain MR Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01124">http://arxiv.org/abs/2307.01124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Shi, Shurong Chai, Yinhao Li, Jingliang Cheng, Jie Bai, Guohua Zhao, Yen-Wei Chen</li>
<li>for: 这个论文主要是为了提出一种基于多Modal融合的扩展模型，用于更好地分类肿瘤region在多Modal MRI脑部图像中。</li>
<li>methods: 该方法使用了一种基于cross-modality attention的扩展模型，通过多Modal融合来提高基础模型的性能。</li>
<li>results: 经验 validate了该方法的效果，在 Zhengzhou大学第一附属医院（FHZU）私人肿瘤数据集上实现了88.38%的Dice和10.64的 Hausdorff distance，比现有技术提高4%。<details>
<summary>Abstract</summary>
According to the 2021 World Health Organization (WHO) Classification scheme for gliomas, glioma segmentation is a very important basis for diagnosis and genotype prediction. In general, 3D multimodal brain MRI is an effective diagnostic tool. In the past decade, there has been an increase in the use of machine learning, particularly deep learning, for medical images processing. Thanks to the development of foundation models, models pre-trained with large-scale datasets have achieved better results on a variety of tasks. However, for medical images with small dataset sizes, deep learning methods struggle to achieve better results on real-world image datasets. In this paper, we propose a cross-modality attention adapter based on multimodal fusion to fine-tune the foundation model to accomplish the task of glioma segmentation in multimodal MRI brain images with better results. The effectiveness of the proposed method is validated via our private glioma data set from the First Affiliated Hospital of Zhengzhou University (FHZU) in Zhengzhou, China. Our proposed method is superior to current state-of-the-art methods with a Dice of 88.38% and Hausdorff distance of 10.64, thereby exhibiting a 4% increase in Dice to segment the glioma region for glioma treatment.
</details>
<details>
<summary>摘要</summary>
Note:* "glioma" is translated as "肿瘤" (tumor) in Simplified Chinese.* "WHO" is translated as "世界卫生组织" (World Health Organization) in Simplified Chinese.* "MRI" is translated as "Magnetic Resonance Imaging" in Simplified Chinese.* "deep learning" is translated as "深度学习" (deep learning) in Simplified Chinese.* "foundation model" is translated as "基础模型" (foundation model) in Simplified Chinese.* "multimodal fusion" is translated as "多Modal融合" (multimodal fusion) in Simplified Chinese.* "Dice" is translated as " dice" (Dice) in Simplified Chinese.* "Hausdorff distance" is translated as " Hausdorff distance" (Hausdorff distance) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Artifacts-Mapping-Multi-Modal-Semantic-Mapping-for-Object-Detection-and-3D-Localization"><a href="#Artifacts-Mapping-Multi-Modal-Semantic-Mapping-for-Object-Detection-and-3D-Localization" class="headerlink" title="Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization"></a>Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01121">http://arxiv.org/abs/2307.01121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Rollo, Gennaro Raiola, Andrea Zunino, Nikolaos Tsagarakis, Arash Ajoudani</li>
<li>for: 本研究旨在实现自主探测和地图建模，即使在未知环境中。</li>
<li>methods: 该方法使用多感器融合方法，包括RGB和深度数据从RGB-D摄像头和雷达。</li>
<li>results: 实验显示，该方法可以准确探测98%的实际环境对象，而不需要后处理。相比单感器实验（camera或雷达），感器融合allow robot探测近距离和远距离障碍物，而单感器实验中的障碍物探测会受到干扰或精度问题。<details>
<summary>Abstract</summary>
Geometric navigation is nowadays a well-established field of robotics and the research focus is shifting towards higher-level scene understanding, such as Semantic Mapping. When a robot needs to interact with its environment, it must be able to comprehend the contextual information of its surroundings. This work focuses on classifying and localising objects within a map, which is under construction (SLAM) or already built. To further explore this direction, we propose a framework that can autonomously detect and localize predefined objects in a known environment using a multi-modal sensor fusion approach (combining RGB and depth data from an RGB-D camera and a lidar). The framework consists of three key elements: understanding the environment through RGB data, estimating depth through multi-modal sensor fusion, and managing artifacts (i.e., filtering and stabilizing measurements). The experiments show that the proposed framework can accurately detect 98% of the objects in the real sample environment, without post-processing, while 85% and 80% of the objects were mapped using the single RGBD camera or RGB + lidar setup respectively. The comparison with single-sensor (camera or lidar) experiments is performed to show that sensor fusion allows the robot to accurately detect near and far obstacles, which would have been noisy or imprecise in a purely visual or laser-based approach.
</details>
<details>
<summary>摘要</summary>
现代几何导航已成为机器人学的一个占据领域，研究重点正在转移到更高级的场景理解，如semantic mapping。当机器人需要与环境交互时，它必须能够理解它所处环境的Contextual信息。这项工作关注于在构建或已经构建的地图上分类和定位预定的对象，通过多Modal感知融合方法（结合RGB和深度数据从RGB-D相机和激光雷达）来自动检测和定位预定的对象。该框架由三个关键元素组成：通过RGB数据理解环境，通过多Modal感知融合估计深度，并处理缺陷（例如，过滤和稳定测量）。实验表明，提议的框架可以准确地检测98%的真实环境中的对象，不需要后处理，而使用单个RGBD相机或RGB+激光雷达设置时分别映射85%和80%的对象。与单感知器（相机或激光雷达）实验进行比较，显示了感知融合Allow robotaccurately检测靠近和远方障碍物，这些障碍物在视觉或激光基础approach中都会呈噪或不准确。
</details></li>
</ul>
<hr>
<h2 id="MeT-A-Graph-Transformer-for-Semantic-Segmentation-of-3D-Meshes"><a href="#MeT-A-Graph-Transformer-for-Semantic-Segmentation-of-3D-Meshes" class="headerlink" title="MeT: A Graph Transformer for Semantic Segmentation of 3D Meshes"></a>MeT: A Graph Transformer for Semantic Segmentation of 3D Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01115">http://arxiv.org/abs/2307.01115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Vecchio, Luca Prezzavento, Carmelo Pino, Francesco Rundo, Simone Palazzo, Concetto Spampinato<br>for: 本研究旨在提出一种基于 transformer 的方法，用于semantic segmentation of 3D mesh。methods: 该方法使用了 global attention mechanism，以及 Laplacian eigenvectors 的位置编码和 clustering-based features，以提高模型对非sequential数据的处理和地方上下文的捕捉。results: 实验结果表明，该方法在三个Shape COSEG Dataset上的人 segmentation任务和ShapeNet benchmark上均达到了当前最佳性能。<details>
<summary>Abstract</summary>
Polygonal meshes have become the standard for discretely approximating 3D shapes, thanks to their efficiency and high flexibility in capturing non-uniform shapes. This non-uniformity, however, leads to irregularity in the mesh structure, making tasks like segmentation of 3D meshes particularly challenging. Semantic segmentation of 3D mesh has been typically addressed through CNN-based approaches, leading to good accuracy. Recently, transformers have gained enough momentum both in NLP and computer vision fields, achieving performance at least on par with CNN models, supporting the long-sought architecture universalism. Following this trend, we propose a transformer-based method for semantic segmentation of 3D mesh motivated by a better modeling of the graph structure of meshes, by means of global attention mechanisms. In order to address the limitations of standard transformer architectures in modeling relative positions of non-sequential data, as in the case of 3D meshes, as well as in capturing the local context, we perform positional encoding by means the Laplacian eigenvectors of the adjacency matrix, replacing the traditional sinusoidal positional encodings, and by introducing clustering-based features into the self-attention and cross-attention operators. Experimental results, carried out on three sets of the Shape COSEG Dataset, on the human segmentation dataset proposed in Maron et al., 2017 and on the ShapeNet benchmark, show how the proposed approach yields state-of-the-art performance on semantic segmentation of 3D meshes.
</details>
<details>
<summary>摘要</summary>
三角形网格已成为三维形状精确地 approximating 的标准方法，因为它们的效率和高灵活性可以 Capture 非均匀形状。然而，这种非均匀性会导致网格结构的不规则，使得三维网格分割变得特别困难。三维网格 semantic segmentation 通常通过基于 CNN 的方法进行解决，以至于达到较好的准确性。最近， transformers 在 NLP 和计算机视觉领域中得到了足够的动力，达到与 CNN 模型相当的性能，支持长期寻求的建筑通用性。以此为动力，我们提出一种基于 transformers 的三维网格 semantic segmentation 方法，通过全球注意力机制来更好地模型网格结构。为了解决标准 transformers 架构在非序数据中的模型非Sequential 数据的局限性，以及 capture 当地Context 的问题，我们使用劳拉cian 快速值来进行位置编码，取代传统的极值律动编码。此外，我们还引入 clustering-based 特征来自注意和跨注意操作中。实验结果，在三个Shape COSEG 数据集上进行了测试，以及 Maron et al. 2017 提出的人差分 segmentation 数据集和 ShapeNet benchmark，显示了我们提出的方法在三维网格 semantic segmentation 中达到了国际最佳性。
</details></li>
</ul>
<hr>
<h2 id="MVDiffusion-Enabling-Holistic-Multi-view-Image-Generation-with-Correspondence-Aware-Diffusion"><a href="#MVDiffusion-Enabling-Holistic-Multi-view-Image-Generation-with-Correspondence-Aware-Diffusion" class="headerlink" title="MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion"></a>MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01097">http://arxiv.org/abs/2307.01097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tangshitao/MVDiffusion">https://github.com/Tangshitao/MVDiffusion</a></li>
<li>paper_authors: Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, Yasutaka Furukawa</li>
<li>for: 这 paper 是为了生成基于文本提示的具有一致性的多视图图像的方法。</li>
<li>methods: 这 paper 使用了一种名为 MVDiffusion 的简单 yet effective 方法，该方法 simultaneous 生成所有图像，并且具有全局意识，从而解决了产生误差的问题。</li>
<li>results: 这 paper 的实验结果表明，MVDiffusion 可以高效地生成高分辨率的 photorealistic 图像，并且可以适应任意的文本提示或者将一个一视图图像扩展到 360 度的全景视图。<details>
<summary>Abstract</summary>
This paper introduces MVDiffusion, a simple yet effective method for generating consistent multi-view images from text prompts given pixel-to-pixel correspondences (e.g., perspective crops from a panorama or multi-view images given depth maps and poses). Unlike prior methods that rely on iterative image warping and inpainting, MVDiffusion simultaneously generates all images with a global awareness, effectively addressing the prevalent error accumulation issue. At its core, MVDiffusion processes perspective images in parallel with a pre-trained text-to-image diffusion model, while integrating novel correspondence-aware attention layers to facilitate cross-view interactions. For panorama generation, while only trained with 10k panoramas, MVDiffusion is able to generate high-resolution photorealistic images for arbitrary texts or extrapolate one perspective image to a 360-degree view. For multi-view depth-to-image generation, MVDiffusion demonstrates state-of-the-art performance for texturing a scene mesh. The project page is at https://mvdiffusion.github.io/.
</details>
<details>
<summary>摘要</summary>
At its core, MVDiffusion processes perspective images in parallel with a pre-trained text-to-image diffusion model, while integrating novel correspondence-aware attention layers to facilitate cross-view interactions. For panorama generation, while only trained with 10,000 panoramas, MVDiffusion is able to generate high-resolution photorealistic images for arbitrary texts or extrapolate one perspective image to a 360-degree view. For multi-view depth-to-image generation, MVDiffusion demonstrates state-of-the-art performance for texturing a scene mesh.More information can be found on the project page at <https://mvdiffusion.github.io/>.
</details></li>
</ul>
<hr>
<h2 id="UW-ProCCaps-UnderWater-Progressive-Colourisation-with-Capsules"><a href="#UW-ProCCaps-UnderWater-Progressive-Colourisation-with-Capsules" class="headerlink" title="UW-ProCCaps: UnderWater Progressive Colourisation with Capsules"></a>UW-ProCCaps: UnderWater Progressive Colourisation with Capsules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01091">http://arxiv.org/abs/2307.01091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rita Pucci, Niki Martinel</li>
<li>for: 本研究旨在提高underwater图像的存储空间效率，以便更多的图像收集campaign。</li>
<li>methods: 我们提出了一种新的机器学习模型，可以从underwater图像的顾色通道中重建图像的颜色信息，从而节省2&#x2F;3的存储空间。我们的模型特化于水下颜色重建，包括一个encoder-decoder架构，其中encoder包括一个卷积encoder和一个并行特化的分类器，使用webly-supervised数据进行训练。</li>
<li>results: 我们在四个benchmark数据集上进行了质量和量тив评估，结果显示，我们的解决方案在水下颜色重建 task中表现出色，比 estado-of-the-art（SOTA）解决方案更高效。此外，我们还证明了生成的颜色化提高了图像质量，比SOTA颜色提高模型更高效。<details>
<summary>Abstract</summary>
Underwater images are fundamental for studying and understanding the status of marine life. We focus on reducing the memory space required for image storage while the memory space consumption in the collecting phase limits the time lasting of this phase leading to the need for more image collection campaigns. We present a novel machine-learning model that reconstructs the colours of underwater images from their luminescence channel, thus saving 2/3 of the available storage space. Our model specialises in underwater colour reconstruction and consists of an encoder-decoder architecture. The encoder is composed of a convolutional encoder and a parallel specialised classifier trained with webly-supervised data. The encoder and the decoder use layers of capsules to capture the features of the entities in the image. The colour reconstruction process recalls the progressive and the generative adversarial training procedures. The progressive training gives the ground for a generative adversarial routine focused on the refining of colours giving the image bright and saturated colours which bring the image back to life. We validate the model both qualitatively and quantitatively on four benchmark datasets. This is the first attempt at colour reconstruction in greyscale underwater images. Extensive results on four benchmark datasets demonstrate that our solution outperforms state-of-the-art (SOTA) solutions. We also demonstrate that the generated colourisation enhances the quality of images compared to enhancement models at the SOTA.
</details>
<details>
<summary>摘要</summary>
水下图像是生物多样性研究的基础数据。我们的研究目标是减少图像存储空间，因为收集阶段的存储空间占用限制了收集阶段的时间，导致需要更多的图像收集campaign。我们提出了一种新的机器学习模型，可以从照明通道中重建水下图像的颜色，从而节省2/3的可用存储空间。我们的模型专注于水下颜色重建，并包括一个encoder-decoder架构。encoder包括一个 convolutional encoder 和一个并行特殊化分类器，并在网络环境中进行了监督学习。encoder和decoder都使用彩袋层来捕捉图像中的特征。颜色重建过程包括进程式和生成敌对训练过程。进程式训练给生成敌对训练提供了基础，并且通过对颜色进行精细调整，使图像变得更加生动和细腻。我们对四个标准数据集进行了质量和量化的验证，这是首次对灰度水下图像进行颜色重建。我们的解决方案在四个标准数据集上显示出比state-of-the-art（SOTA）解决方案更高的性能。此外，我们还证明了生成颜色化后的图像质量比SOTA的增强模型更高。
</details></li>
</ul>
<hr>
<h2 id="Streamlined-Lensed-Quasar-Identification-in-Multiband-Images-via-Ensemble-Networks"><a href="#Streamlined-Lensed-Quasar-Identification-in-Multiband-Images-via-Ensemble-Networks" class="headerlink" title="Streamlined Lensed Quasar Identification in Multiband Images via Ensemble Networks"></a>Streamlined Lensed Quasar Identification in Multiband Images via Ensemble Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01090">http://arxiv.org/abs/2307.01090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irham Taufik Andika, Sherry H. Suyu, Raoul Cañameras, Alejandra Melo, Stefan Schuldt, Yiping Shu, Anna-Christina Eilers, Anton Timur Jaelani, Minghao Yue</li>
<li>for: 本研究旨在 automatization 强 gravitational lensing 检测，提高 cosmic expansion rate 和 dark matter profile 等研究的效率。</li>
<li>methods: 本研究使用了多种 cutting-edge  convolutional neural networks (CNNs) 和 vision transformers (ViTs)，包括 ResNet、Inception、NASNet、MobileNet、EfficientNet 和 RegNet，以及 Hyper Suprime-Cam (HSC) 多波段图像。</li>
<li>results: 研究发现，通过 ensemble 这些 CNNs 和 ViTs，可以大幅减少假阳性源，并且在实际数据上具有较高的检测精度。最终，通过对 HSC 图像进行拼接，以及使用 UKIRT、VISTA 和 unWISE 数据，找到了约 600 万个源，并将其减少到 892,609 个。经过光度预选择，发现了 3080 个可能的强 gravitational lens 源，其中 210 个已经得到了观察confirmation。这些结果表明，自动化的深度学习管道可以有效地检测强 gravitational lensing，并减少人工视觉检查的时间和努力。<details>
<summary>Abstract</summary>
Quasars experiencing strong lensing offer unique viewpoints on subjects related to the cosmic expansion rate, the dark matter profile within the foreground deflectors, and the quasar host galaxies. Unfortunately, identifying them in astronomical images is challenging since they are overwhelmed by the abundance of non-lenses. To address this, we have developed a novel approach by ensembling cutting-edge convolutional networks (CNNs) -- for instance, ResNet, Inception, NASNet, MobileNet, EfficientNet, and RegNet -- along with vision transformers (ViTs) trained on realistic galaxy-quasar lens simulations based on the Hyper Suprime-Cam (HSC) multiband images. While the individual model exhibits remarkable performance when evaluated against the test dataset, achieving an area under the receiver operating characteristic curve of $>$97.3% and a median false positive rate of 3.6%, it struggles to generalize in real data, indicated by numerous spurious sources picked by each classifier. A significant improvement is achieved by averaging these CNNs and ViTs, resulting in the impurities being downsized by factors up to 50. Subsequently, combining the HSC images with the UKIRT, VISTA, and unWISE data, we retrieve approximately 60 million sources as parent samples and reduce this to 892,609 after employing a photometry preselection to discover $z>1.5$ lensed quasars with Einstein radii of $\theta_\mathrm{E}<5$ arcsec. Afterward, the ensemble classifier indicates 3080 sources with a high probability of being lenses, for which we visually inspect, yielding 210 prevailing candidates awaiting spectroscopic confirmation. These outcomes suggest that automated deep learning pipelines hold great potential in effectively detecting strong lenses in vast datasets with minimal manual visual inspection involved.
</details>
<details>
<summary>摘要</summary>
<<注意：以下文本使用了简化字体。>>Quasars经历强大的变形所提供了关于宇宙膨胀率、黑 mater profile within the foreground deflectors以及这些 Quasar host galaxies的独特视角。然而，在天文图像中 indentifying them 是具有挑战性的，因为它们被非强�的扩散所掩蔽。为了解决这个问题，我们开发了一种新的方法， combining cutting-edge convolutional neural networks (CNNs) 和视 transformers (ViTs) ，例如 ResNet、Inception、NASNet、MobileNet、EfficientNet 和 RegNet，并在基于 Hyper Suprime-Cam (HSC) 多频道图像的 simulated galaxy-quasar lens 中进行训练。尽管单个模型在测试数据集上表现出色，在 receiver operating characteristic curve 上取得了area > 97.3% 和 median false positive rate 为 3.6%，但它在实际数据中表现不佳，表现出许多假阳性。通过将这些 CNNs 和 ViTs  ensemble，随后将 HSC 图像与 UKIRT、VISTA 和 unWISE 数据结合，我们检索了大约 6000万个源，并将其减少到 892,609 个后进行光度预选择，以找到 $z>1.5$ 强大变形透镜的候选者。然后，我们使用 ensemble classifier，并将其中 3080 个源作为高可能性强大变形透镜进行视觉检查，其中 210 个透镜被证明为主要候选者，等待spectroscopic confirmation。这些结果表明，通过自动的深度学习管道，可以快速和高效地检测强大变形透镜，并且可以避免大量的手动视觉检查。
</details></li>
</ul>
<hr>
<h2 id="Empirically-Validating-Conformal-Prediction-on-Modern-Vision-Architectures-Under-Distribution-Shift-and-Long-tailed-Data"><a href="#Empirically-Validating-Conformal-Prediction-on-Modern-Vision-Architectures-Under-Distribution-Shift-and-Long-tailed-Data" class="headerlink" title="Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data"></a>Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01088">http://arxiv.org/abs/2307.01088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Kasa, Graham W. Taylor</li>
<li>for: 提供深度学习模型可靠的不确定性估计和安全保证</li>
<li>methods: 考虑了多种 posterior-based 和 training-based 具有确定性 guarantee的方法</li>
<li>results: 在大规模数据集和模型上进行了实证评估，发现这些方法在分布偏移和长尾分布下表现不佳，常常违反保证I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. Yet, its performance is known to degrade under distribution shift and long-tailed class distributions, which are often present in real world applications. Here, we characterize the performance of several post-hoc and training-based conformal prediction methods under these settings, providing the first empirical evaluation on large-scale datasets and models. We show that across numerous conformal methods and neural network families, performance greatly degrades under distribution shifts violating safety guarantees. Similarly, we show that in long-tailed settings the guarantees are frequently violated on many classes. Understanding the limitations of these methods is necessary for deployment in real world and safety-critical applications.
</details>
<details>
<summary>摘要</summary>
具有预测可靠性和安全保证的具体预测（conformal prediction）在深度学习模型中出现了，但其在分布变化和长尾类分布下的性能有所下降。我们对多种后处和训练基于的具体预测方法进行了大规模数据集和模型的实验性评估，发现在分布变化下，各种方法的性能强度下降，同时在长尾类分布下，保证 frequently 被违反。这些方法在实际应用中的部署和安全应用中的理解其限制是必要的。
</details></li>
</ul>
<hr>
<h2 id="Shi-NeSS-Detecting-Good-and-Stable-Keypoints-with-a-Neural-Stability-Score"><a href="#Shi-NeSS-Detecting-Good-and-Stable-Keypoints-with-a-Neural-Stability-Score" class="headerlink" title="Shi-NeSS: Detecting Good and Stable Keypoints with a Neural Stability Score"></a>Shi-NeSS: Detecting Good and Stable Keypoints with a Neural Stability Score</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01069">http://arxiv.org/abs/2307.01069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin Pakulev, Alexander Vakhitov, Gonzalo Ferrer</li>
<li>for: 本研究旨在提出一种可靠的特征点检测方法，解决了针对特征点的定义和对应的标注数据的问题。</li>
<li>methods: 本方法结合手工设计的Shi检测器和神经网络，利用Shi检测器提供的本地化特征点和神经网络进行选择，并使用神经网络进行稳定性分数的评估。</li>
<li>results: 在HPatches、ScanNet、MegaDepth和IMC-PT等测试集上，本方法表现出了顶尖的性能和良好的通用性，可以作为下游任务的基础功能。<details>
<summary>Abstract</summary>
Learning a feature point detector presents a challenge both due to the ambiguity of the definition of a keypoint and correspondingly the need for a specially prepared ground truth labels for such points. In our work, we address both of these issues by utilizing a combination of a hand-crafted Shi detector and a neural network. We build on the principled and localized keypoints provided by the Shi detector and perform their selection using the keypoint stability score regressed by the neural network - Neural Stability Score (NeSS). Therefore, our method is named Shi-NeSS since it combines the Shi detector and the properties of the keypoint stability score, and it only requires for training sets of images without dataset pre-labeling or the need for reconstructed correspondence labels. We evaluate Shi-NeSS on HPatches, ScanNet, MegaDepth and IMC-PT, demonstrating state-of-the-art performance and good generalization on downstream tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Localized-Questions-in-Medical-Visual-Question-Answering"><a href="#Localized-Questions-in-Medical-Visual-Question-Answering" class="headerlink" title="Localized Questions in Medical Visual Question Answering"></a>Localized Questions in Medical Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01067">http://arxiv.org/abs/2307.01067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sergiotasconmorales/locvqa">https://github.com/sergiotasconmorales/locvqa</a></li>
<li>paper_authors: Sergio Tascon-Morales, Pablo Márquez-Neila, Raphael Sznitman</li>
<li>for: 医学视觉问答模型 (Medical VQA) 的研究，以解决现有模型只能回答整个图像的问题，而不能回答关于图像具体区域的问题。</li>
<li>methods: 提出了一种新的方法，可以回答关于图像区域的问题，同时考虑问题的上下文。</li>
<li>results: 实验结果表明，该方法比现有方法在三个数据集上表现更好，达到了更高的准确率。Note: The text is in Simplified Chinese, and the word order is adjusted to match the language convention.<details>
<summary>Abstract</summary>
Visual Question Answering (VQA) models aim to answer natural language questions about given images. Due to its ability to ask questions that differ from those used when training the model, medical VQA has received substantial attention in recent years. However, existing medical VQA models typically focus on answering questions that refer to an entire image rather than where the relevant content may be located in the image. Consequently, VQA models are limited in their interpretability power and the possibility to probe the model about specific image regions. This paper proposes a novel approach for medical VQA that addresses this limitation by developing a model that can answer questions about image regions while considering the context necessary to answer the questions. Our experimental results demonstrate the effectiveness of our proposed model, outperforming existing methods on three datasets. Our code and data are available at https://github.com/sergiotasconmorales/locvqa.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate given text into Simplified Chinese.<</SYS>>干���VQA（Visual Question Answering）模型目标是回答给定图像的自然语言问题。由于它可以提出训练模型不同的问题，因此医学VQA在最近几年内得到了广泛的关注。然而，现有的医学VQA模型通常只会回答整个图像的问题，而不是在图像中具体的区域。这限制了VQA模型的解释力和可 probing 能力。这篇论文提议一种新的医学VQA方法，该方法可以回答图像区域的问题，同时考虑到问题的上下文。我们的实验结果表明，我们提出的方法可以超越现有方法，在三个数据集上达到更高的性能。我们的代码和数据可以在 GitHub 上找到：https://github.com/sergiotasconmorales/locvqa。
</details></li>
</ul>
<hr>
<h2 id="TomatoDIFF-On-plant-Tomato-Segmentation-with-Denoising-Diffusion-Models"><a href="#TomatoDIFF-On-plant-Tomato-Segmentation-with-Denoising-Diffusion-Models" class="headerlink" title="TomatoDIFF: On-plant Tomato Segmentation with Denoising Diffusion Models"></a>TomatoDIFF: On-plant Tomato Segmentation with Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01064">http://arxiv.org/abs/2307.01064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mivanovska/tomatodiff">https://github.com/mivanovska/tomatodiff</a></li>
<li>paper_authors: Marija Ivanovska, Vitomir Struc, Janez Pers</li>
<li>for: 这项研究旨在提高 Tomatoes 的生长和生产，同时降低成本和环境影响。</li>
<li>methods: 该paper提出了一种新的Diffusion-based模型，用于semantic segmentation of on-plant Tomatoes。</li>
<li>results: 该模型在与其他竞争方法比较中显示出了SOTA表现，即使在受 occlusion 影响的环境中也能够达到优秀的结果。<details>
<summary>Abstract</summary>
Artificial intelligence applications enable farmers to optimize crop growth and production while reducing costs and environmental impact. Computer vision-based algorithms in particular, are commonly used for fruit segmentation, enabling in-depth analysis of the harvest quality and accurate yield estimation. In this paper, we propose TomatoDIFF, a novel diffusion-based model for semantic segmentation of on-plant tomatoes. When evaluated against other competitive methods, our model demonstrates state-of-the-art (SOTA) performance, even in challenging environments with highly occluded fruits. Additionally, we introduce Tomatopia, a new, large and challenging dataset of greenhouse tomatoes. The dataset comprises high-resolution RGB-D images and pixel-level annotations of the fruits.
</details>
<details>
<summary>摘要</summary>
人工智能应用程序可以帮助农民优化作物生长和生产，同时降低成本和环境影响。计算机视觉算法在特别是广泛应用于水果分割，以便进行深入分析丰收质量和准确的受量估计。在这篇论文中，我们提议了一种新的扩散模型，即TomatoDIFF，用于 semantic segmentation of on-plant tomatoes。与其他竞争方法进行比较后，我们的模型在具有高度受阻物的环境中仍然达到了当前最佳性能（SOTA）。此外，我们还介绍了一个新的大型和挑战性较高的greenhouse tomatoes数据集，该数据集包括高分辨率RGB-D图像和每个像素级别的水果标注。
</details></li>
</ul>
<hr>
<h2 id="Cross-modal-Place-Recognition-in-Image-Databases-using-Event-based-Sensors"><a href="#Cross-modal-Place-Recognition-in-Image-Databases-using-Event-based-Sensors" class="headerlink" title="Cross-modal Place Recognition in Image Databases using Event-based Sensors"></a>Cross-modal Place Recognition in Image Databases using Event-based Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01047">http://arxiv.org/abs/2307.01047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Ji, Jiaxin Wei, Yifu Wang, Huiliang Shang, Laurent Kneip</li>
<li>for: 本文是为了提出一种跨Modal视觉位置认识框架，能够从事件查询中检索常规图像，并在不同场景下达到比frame-based和事件基于方法更高的性能。</li>
<li>methods: 本文使用的方法包括事件查询和图像检索，以及一种新的卷积神经网络模型，用于将事件信息与图像信息结合在一起。</li>
<li>results: 本文的实验结果表明，相比 frame-based 和事件基于方法，跨Modal视觉位置认识框架可以在不同场景下达到较高的性能，并且可以通过组合检索和分类来进一步提高性能。<details>
<summary>Abstract</summary>
Visual place recognition is an important problem towards global localization in many robotics tasks. One of the biggest challenges is that it may suffer from illumination or appearance changes in surrounding environments. Event cameras are interesting alternatives to frame-based sensors as their high dynamic range enables robust perception in difficult illumination conditions. However, current event-based place recognition methods only rely on event information, which restricts downstream applications of VPR. In this paper, we present the first cross-modal visual place recognition framework that is capable of retrieving regular images from a database given an event query. Our method demonstrates promising results with respect to the state-of-the-art frame-based and event-based methods on the Brisbane-Event-VPR dataset under different scenarios. We also verify the effectiveness of the combination of retrieval and classification, which can boost performance by a large margin.
</details>
<details>
<summary>摘要</summary>
Visual place recognition是重要的global localization问题中的一个关键问题，它可能受到环境照明或外观变化的影响。事件摄像头是替代frame-based感知器的有趣选择，因为它们的高动态范围使得在困难的照明条件下进行稳定的感知。然而，当前的事件基本Visual place recognition方法仅仅基于事件信息，这限制了下游应用程序的可能性。在这篇论文中，我们提出了首个可以从事件库中提取常见图像的跨模态Visual place recognition框架。我们的方法在不同的场景下与现状技术相比，对Brisbane-Event-VPR数据集表现出了promising的结果。我们还验证了抽象和分类的组合可以提高性能的幅度。
</details></li>
</ul>
<hr>
<h2 id="SAM-DA-UAV-Tracks-Anything-at-Night-with-SAM-Powered-Domain-Adaptation"><a href="#SAM-DA-UAV-Tracks-Anything-at-Night-with-SAM-Powered-Domain-Adaptation" class="headerlink" title="SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation"></a>SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01024">http://arxiv.org/abs/2307.01024</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vision4robotics/sam-da">https://github.com/vision4robotics/sam-da</a></li>
<li>paper_authors: Liangliang Yao, Haobo Zuo, Guangze Zheng, Changhong Fu, Jia Pan</li>
<li>for: 本研究旨在提高夜间无人机跟踪的效果，特别是使用实时DAYTIME trackers进行夜间跟踪。</li>
<li>methods: 本研究使用Segment Anything Model（SAM）来提高夜间无人机跟踪的效果，并设计了一种新的SAM-powered目标频道训练样本扩展方法。</li>
<li>results: 实验结果表明，SAM-DA可以在夜间无人机视频中提高夜间跟踪的效果，并且比目前状态ixen DA更好地适应夜间环境。此外，SAM-DA只需要 fewer nighttime images 来训练，这使得算法的验证和部署变得更加容易。<details>
<summary>Abstract</summary>
Domain adaptation (DA) has demonstrated significant promise for real-time nighttime unmanned aerial vehicle (UAV) tracking. However, the state-of-the-art (SOTA) DA still lacks the potential object with accurate pixel-level location and boundary to generate the high-quality target domain training sample. This key issue constrains the transfer learning of the real-time daytime SOTA trackers for challenging nighttime UAV tracking. Recently, the notable Segment Anything Model (SAM) has achieved remarkable zero-shot generalization ability to discover abundant potential objects due to its huge data-driven training approach. To solve the aforementioned issue, this work proposes a novel SAM-powered DA framework for real-time nighttime UAV tracking, i.e., SAM-DA. Specifically, an innovative SAM-powered target domain training sample swelling is designed to determine enormous high-quality target domain training samples from every single raw nighttime image. This novel one-to-many method significantly expands the high-quality target domain training sample for DA. Comprehensive experiments on extensive nighttime UAV videos prove the robustness and domain adaptability of SAM-DA for nighttime UAV tracking. Especially, compared to the SOTA DA, SAM-DA can achieve better performance with fewer raw nighttime images, i.e., the fewer-better training. This economized training approach facilitates the quick validation and deployment of algorithms for UAVs. The code is available at https://github.com/vision4robotics/SAM-DA.
</details>
<details>
<summary>摘要</summary>
域 adaptation (DA) 已经表现出了实时夜间无人机 (UAV) 跟踪的显著搭配潜力。然而，当前的状态势 (SOTA) DA 仍然缺乏高质量像素级别位置和边沿来生成高质量目标域训练样本。这个关键问题限制了日间高质量跟踪器的转移学习。最近， Segment Anything Model (SAM) 已经实现了各种 Zero-shot 通用能力，可以快速发现丰富的可能对象。为解决这个问题，本工作提出了一种基于 SAM 的 DA 框架，即 SAM-DA。特别是，我们提出了一种创新的 SAM 驱动的目标域训练样本扩展方法，可以从每个 Raw 夜间图像中找到巨大数量的高质量目标域训练样本。这种一对多的方法可以显著扩展高质量目标域训练样本，从而提高 DA 的性能。归根结底，我们的 SAM-DA 方法可以在夜间 UAV 跟踪中提供更好的性能，只需要 fewer 个 Raw 夜间图像，即 fewer-better 训练。这种经济的训练方法可以促进算法的快速验证和部署。代码可以在 GitHub 上找到：https://github.com/vision4robotics/SAM-DA。
</details></li>
</ul>
<hr>
<h2 id="CGAM-Click-Guided-Attention-Module-for-Interactive-Pathology-Image-Segmentation-via-Backpropagating-Refinement"><a href="#CGAM-Click-Guided-Attention-Module-for-Interactive-Pathology-Image-Segmentation-via-Backpropagating-Refinement" class="headerlink" title="CGAM: Click-Guided Attention Module for Interactive Pathology Image Segmentation via Backpropagating Refinement"></a>CGAM: Click-Guided Attention Module for Interactive Pathology Image Segmentation via Backpropagating Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01015">http://arxiv.org/abs/2307.01015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonghui Min, Won-Ki Jeong</li>
<li>for: 这项研究的目的是提高图像分割 tasks中的可靠性和准确性，以便为医疗数据提供更好的量化分析。</li>
<li>methods: 该研究使用了一种交互式分割方法，通过用户提供的点击约束和 semantic feature map 来优化深度神经网络的输出。这种方法被称为 click-guided attention module (CGAM)，它可以避免过度适应用户点击，并且模型大小不受输入图像大小的影响。</li>
<li>results: 实验结果表明，与现有状态艺术方法相比，我们的方法在病理图像数据集上表现出了更好的性能。<details>
<summary>Abstract</summary>
Tumor region segmentation is an essential task for the quantitative analysis of digital pathology. Recently presented deep neural networks have shown state-of-the-art performance in various image-segmentation tasks. However, because of the unclear boundary between the cancerous and normal regions in pathology images, despite using modern methods, it is difficult to produce satisfactory segmentation results in terms of the reliability and accuracy required for medical data. In this study, we propose an interactive segmentation method that allows users to refine the output of deep neural networks through click-type user interactions. The primary method is to formulate interactive segmentation as an optimization problem that leverages both user-provided click constraints and semantic information in a feature map using a click-guided attention module (CGAM). Unlike other existing methods, CGAM avoids excessive changes in segmentation results, which can lead to the overfitting of user clicks. Another advantage of CGAM is that the model size is independent of input image size. Experimental results on pathology image datasets indicated that our method performs better than existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
受体区域分割是生物图像分析中的一项重要任务。最近提出的深度神经网络已经在各种图像分割任务中显示出了前所未有的性能。然而，由于生理图像中的悬浮边缘不清晰，使用现代方法仍然很难得到具有医疗数据所需的可靠性和准确性的分割结果。在这项研究中，我们提出了一种互动分割方法，允许用户通过点击类型的交互来修正深度神经网络的输出。我们的方法是通过点击约束和 semantic 信息在特征图上的协同使用点击导航模块（CGAM）来形式互动分割问题。与其他现有方法不同，CGAM 不会导致用户点击的过多修改，从而避免了点击过拟合。此外，CGAM 的模型大小独立于输入图像大小。在生物图像数据集上进行了实验，我们的方法比现有的状态艺前方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="SynthCal-A-Synthetic-Benchmarking-Pipeline-to-Compare-Camera-Calibration-Algorithms"><a href="#SynthCal-A-Synthetic-Benchmarking-Pipeline-to-Compare-Camera-Calibration-Algorithms" class="headerlink" title="SynthCal: A Synthetic Benchmarking Pipeline to Compare Camera Calibration Algorithms"></a>SynthCal: A Synthetic Benchmarking Pipeline to Compare Camera Calibration Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01013">http://arxiv.org/abs/2307.01013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lala Shakti Swarup Ray, Bo Zhou, Lars Krupp, Sungho Suh, Paul Lukowicz</li>
<li>for: 本研究目的是提供一个可靠的实验室摄像机校正评估pipeline，以便评估摄像机参数估计算法的精度。</li>
<li>methods: 本研究使用了SynthCal实验室摄像机校正对benchmarkingipeline，它可以生成各种摄像机参数估计算法的测试数据。</li>
<li>results: 实验结果显示SynthCal可以实现高精度的摄像机参数估计，并且可以评估不同的摄像机参数估计算法和实验室环境。<details>
<summary>Abstract</summary>
Accurate camera calibration is crucial for various computer vision applications. However, measuring camera parameters in the real world is challenging and arduous, and there needs to be a dataset with ground truth to evaluate calibration algorithms' accuracy. In this paper, we present SynthCal, a synthetic camera calibration benchmarking pipeline that generates images of calibration patterns to measure and enable accurate quantification of calibration algorithm performance in camera parameter estimation. We present a SynthCal-generated calibration dataset with four common patterns, two camera types, and two environments with varying view, distortion, lighting, and noise levels. The dataset evaluates single-view calibration algorithms by measuring reprojection and root-mean-square errors for identical patterns and camera settings. Additionally, we analyze the significance of different patterns using Zhang's method, which estimates intrinsic and extrinsic camera parameters with known correspondences between 3D points and their 2D projections in different configurations and environments. The experimental results demonstrate the effectiveness of SynthCal in evaluating various calibration algorithms and patterns.
</details>
<details>
<summary>摘要</summary>
准确的摄像头准确化是许多计算机视觉应用中的关键。然而，在实际世界中测量摄像头参数是困难和辛苦的，而且需要一个具有真实数据的数据集来评估准确化算法的准确性。在这篇论文中，我们提出了SynthCal，一个人工生成的摄像头准确化测试平台，可以生成准确度测试图案，以便评估和精确量化各种准确化算法的性能。我们提供了SynthCal生成的准确化数据集，包括四种常见的图案、两种摄像头类型和两种环境，其中包括不同的视角、扭曲、照明和噪声水平。这个数据集用于评估单视准确化算法，并且可以量化投影和平均方差误差。此外，我们还分析了不同图案的重要性，使用张氏方法，该方法可以在不同的配置和环境中估计摄像头的内参和外参参数，并且可以确定图案和摄像头设置之间的唯一对应关系。实验结果表明，SynthCal可以准确地评估各种准确化算法和图案。
</details></li>
</ul>
<hr>
<h2 id="Joint-Coordinate-Regression-and-Association-For-Multi-Person-Pose-Estimation-A-Pure-Neural-Network-Approach"><a href="#Joint-Coordinate-Regression-and-Association-For-Multi-Person-Pose-Estimation-A-Pure-Neural-Network-Approach" class="headerlink" title="Joint Coordinate Regression and Association For Multi-Person Pose Estimation, A Pure Neural Network Approach"></a>Joint Coordinate Regression and Association For Multi-Person Pose Estimation, A Pure Neural Network Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01004">http://arxiv.org/abs/2307.01004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyang Yu, Yunshi Xie, Wangpeng An, Li Zhang, Yufeng Yao</li>
<li>for: 这篇论文旨在提出一种新的一stage端到端多人2D姿态估计算法（Joint Coordinate Regression and Association，JCRA），该算法可以直接生成人体姿态关节和关联，无需任何后处理。</li>
<li>methods: 该算法采用了一stage端到端网络架构，从而提高了推理速度。同时，作者采用了对称网络结构，以确保高准确率地标定关节点。具体来说，该算法使用了一个转换网络，直接输出部位位置，从而实现了显著性能提高。</li>
<li>results: 广泛的实验表明，JCRA在MS COCO和CrowdPosebenchmark上表现出色，并且在精度和效率两个方面都超越了现有的状态对照方法。具体来说，JCRA在MS COCO上达到了69.2 mAP，并且在推理加速方面比前状态的底层方法提高了78%。<details>
<summary>Abstract</summary>
We introduce a novel one-stage end-to-end multi-person 2D pose estimation algorithm, known as Joint Coordinate Regression and Association (JCRA), that produces human pose joints and associations without requiring any post-processing. The proposed algorithm is fast, accurate, effective, and simple. The one-stage end-to-end network architecture significantly improves the inference speed of JCRA. Meanwhile, we devised a symmetric network structure for both the encoder and decoder, which ensures high accuracy in identifying keypoints. It follows an architecture that directly outputs part positions via a transformer network, resulting in a significant improvement in performance. Extensive experiments on the MS COCO and CrowdPose benchmarks demonstrate that JCRA outperforms state-of-the-art approaches in both accuracy and efficiency. Moreover, JCRA demonstrates 69.2 mAP and is 78\% faster at inference acceleration than previous state-of-the-art bottom-up algorithms. The code for this algorithm will be publicly available.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的一stage端到端多人2D姿态估计算法，称为共同坐标回归和关联（JCRA），该算法可以生成人体姿态关节和关联而无需任何后处理。我们提出的算法具有快速、高精度、有效和简单的特点。我们的一stage端到端网络架构可以显著提高JCRA的推理速度。此外，我们设计了对 encryptor和解码器的同质网络结构，以确保高精度在标记关节点的identification。这种架构直接通过 transformer 网络输出部分位置，从而得到了显著的性能提高。我们对 MS COCO 和 CrowdPose 测试准则进行了广泛的实验，并证明了 JCRA 在精度和效率两个方面超过了当前状态的抗下方法。此外，JCRA 在推理加速方面的提速率为 78%。代码将公开 availability。
</details></li>
</ul>
<hr>
<h2 id="Predicting-beauty-liking-and-aesthetic-quality-A-comparative-analysis-of-image-databases-for-visual-aesthetics-research"><a href="#Predicting-beauty-liking-and-aesthetic-quality-A-comparative-analysis-of-image-databases-for-visual-aesthetics-research" class="headerlink" title="Predicting beauty, liking, and aesthetic quality: A comparative analysis of image databases for visual aesthetics research"></a>Predicting beauty, liking, and aesthetic quality: A comparative analysis of image databases for visual aesthetics research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00984">http://arxiv.org/abs/2307.00984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ralf Bartho, Katja Thoemmes, Christoph Redies</li>
<li>for: 本研究提供了12个图像集的比较概述，这些图像集包含美学评价（美好、喜欢或艺术质量）的评分。</li>
<li>methods: 研究使用了20种已经研究过的统计图像特征（A），以及一个基于对象识别的卷积神经网络（B）来预测美学评价。</li>
<li>results: 研究发现不同图像集的美学评价预测结果具有substantial的变化，但是包含照片或画作的图像集具有相似的特征，表明不同的图像类别具有不同的美学评价特征。尽管统计图像特征和卷积神经网络具有相似的预测精度，但是不同的图像集的差异提出了对前期研究结果的总化和普遍性的困难。研究强调了在实验和计算美学领域中考虑多个图像集，以提高研究结果的有效性和普遍性。<details>
<summary>Abstract</summary>
In the fields of Experimental and Computational Aesthetics, numerous image datasets have been created over the last two decades. In the present work, we provide a comparative overview of twelve image datasets that include aesthetic ratings (beauty, liking or aesthetic quality) and investigate the reproducibility of results across different datasets. Specifically, we examine how consistently the ratings can be predicted by using either (A) a set of 20 previously studied statistical image properties, or (B) the layers of a convolutional neural network developed for object recognition. Our findings reveal substantial variation in the predictability of aesthetic ratings across the different datasets. However, consistent similarities were found for datasets containing either photographs or paintings, suggesting different relevant features in the aesthetic evaluation of these two image genres. To our surprise, statistical image properties and the convolutional neural network predict aesthetic ratings with similar accuracy, highlighting a significant overlap in the image information captured by the two methods. Nevertheless, the discrepancies between the datasets call into question the generalizability of previous research findings on single datasets. Our study underscores the importance of considering multiple datasets to improve the validity and generalizability of research results in the fields of experimental and computational aesthetics.
</details>
<details>
<summary>摘要</summary>
在实验和计算艺术领域，过去二十年内已经创建了许多图像集。在 presente 工作中，我们提供了十二个图像集的比较概述，这些图像集包括美学评分（美丽、喜欢或艺术质量）。我们发现这些图像集的美学评分是不一致的，但是包含照片或画作的图像集具有相似的特征。这些结果表明了使用不同方法来预测美学评分的可靠性。我们发现使用20种已知的统计图像特征和 convolutional neural network 对象识别的层次结构都可以准确预测美学评分，这显示了这两种方法 capture 了图像信息的重要方面。然而，由于不同图像集之间的差异，这casts  doubt 到 previous research 的普遍性和可靠性。我们的研究强调了在实验和计算艺术领域中考虑多个图像集以提高研究结果的有效性和普遍性。
</details></li>
</ul>
<hr>
<h2 id="Autism-Spectrum-Disorder-Classification-in-Children-based-on-Structural-MRI-Features-Extracted-using-Contrastive-Variational-Autoencoder"><a href="#Autism-Spectrum-Disorder-Classification-in-Children-based-on-Structural-MRI-Features-Extracted-using-Contrastive-Variational-Autoencoder" class="headerlink" title="Autism Spectrum Disorder Classification in Children based on Structural MRI Features Extracted using Contrastive Variational Autoencoder"></a>Autism Spectrum Disorder Classification in Children based on Structural MRI Features Extracted using Contrastive Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00976">http://arxiv.org/abs/2307.00976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruimin Ma, Ruitao Xie, Yanlin Wang, Jintao Meng, Yanjie Wei, Wenhui Xi, Yi Pan</li>
<li>for: 这篇论文的目的是为了提高儿童Autism Spectrum Disorder（ASD）的早期诊断和 intervención，通过使用机器学习和神经成像技术，基于струк成像MRI（s-MRI）的机器分类。</li>
<li>methods: 这篇论文使用了contrastive variational autoencoder（CVAE）提取了s-MRI特征，并将ASD参与者表示为ASD特有的特征通道，共同特征通道表示健康参与者。</li>
<li>results: 研究实现了在儿童ASD（年龄范围：0.92-4.83岁）中的机器分类精度（超过0.97），并通过对不同 cortical区域表面积和s-MRI特征之间的相关性进行神经анаatomical解释，探讨了可能的ASD治疗目标。<details>
<summary>Abstract</summary>
Autism spectrum disorder (ASD) is a highly disabling mental disease that brings significant impairments of social interaction ability to the patients, making early screening and intervention of ASD critical. With the development of the machine learning and neuroimaging technology, extensive research has been conducted on machine classification of ASD based on structural MRI (s-MRI). However, most studies involve with datasets where participants' age are above 5. Few studies conduct machine classification of ASD for participants below 5-year-old, but, with mediocre predictive accuracy. In this paper, we push the boundary of predictive accuracy (above 0.97) of machine classification of ASD in children (age range: 0.92-4.83 years), based on s-MRI features extracted using contrastive variational autoencoder (CVAE). 78 s-MRI, collected from Shenzhen Children's Hospital, are used for training CVAE, which consists of both ASD-specific feature channel and common shared feature channel. The ASD participants represented by ASD-specific features can be easily discriminated from TC participants represented by the common shared features, leading to high classification accuracy. In case of degraded predictive accuracy when data size is extremely small, a transfer learning strategy is proposed here as a potential solution. Finally, we conduct neuroanatomical interpretation based on the correlation between s-MRI features extracted from CVAE and surface area of different cortical regions, which discloses potential biomarkers that could help target treatments of ASD in the future.
</details>
<details>
<summary>摘要</summary>
自适应症 спектルム疾病 (ASD) 是一种非常影响社交交流能力的精神疾病，对患者的早期检测和 intervención 至关重要。随着机器学习和神经成像技术的发展，有广泛的研究在机器分类 ASD 基于结构MRI（s-MRI）。然而，大多数研究参与者的年龄都大于 5 岁。只有一些研究进行了机器分类 ASD 的参与者下限为 5 岁，但它们的预测精度不太高。在这篇论文中，我们使用对比变量自动编码器（CVAE）提取的 s-MRI 特征来提高机器分类 ASD 的预测精度（高于 0.97），并且在儿童（年龄范围：0.92-4.83 岁）中进行了研究。我们使用了78 个 s-MRI，从深圳儿童医院收集到，用于训练 CVAE，CVAE 包括 ASD 特有的特征通道和共同分享的特征通道。ASD 参与者通过 ASD 特有的特征被轻松地与TC参与者（通过共同分享的特征）进行分开，从而导致高的分类精度。在数据量非常小时，降低预测精度的情况下，我们提出了传输学习策略作为可能的解决方案。最后，我们通过对 CVAE 提取的 s-MRI 特征和不同 cortical 区域表面积之间的相关性进行神经 анаatomical 解释，揭示了可能的生物标志物，这些生物标志物可能帮助未来对 ASD 进行精细的target treatments。
</details></li>
</ul>
<hr>
<h2 id="MoVie-Visual-Model-Based-Policy-Adaptation-for-View-Generalization"><a href="#MoVie-Visual-Model-Based-Policy-Adaptation-for-View-Generalization" class="headerlink" title="MoVie: Visual Model-Based Policy Adaptation for View Generalization"></a>MoVie: Visual Model-Based Policy Adaptation for View Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00972">http://arxiv.org/abs/2307.00972</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangsizhe/MoVie">https://github.com/yangsizhe/MoVie</a></li>
<li>paper_authors: Sizhe Yang, Yanjie Ze, Huazhe Xu</li>
<li>for: 这个论文旨在解决视 Reinforcement Learning  Agent 在不同视角下的泛化问题，即 $\textit{view generalization}$ 问题。</li>
<li>methods: 作者提出了一种简单 yet effective 的方法，通过在测试时使用模型来启用视模型基于策略的 $\textbf{MoVie}$  adaptation，无需显式奖励信号和任何修改 durante 训练时间。</li>
<li>results: 研究表明，该方法在四种不同的场景下（包括 DMControl、xArm 和 Adroit 中的 $\textbf{18}$ 个任务）具有显著的进步，相对于原始方法，提高了 $\mathbf{33}$%、$\mathbf{86}$% 和 $\mathbf{152}$%。这些出色的结果表明该方法在实际 робо扮中具有极大的潜力。视频可以在 <a target="_blank" rel="noopener" href="https://yangsizhe.github.io/MoVie/">https://yangsizhe.github.io/MoVie/</a> 上查看。<details>
<summary>Abstract</summary>
Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\textbf{Mo}$del-based policies for $\textbf{Vie}$w generalization ($\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\mathbf{33}$%, $\mathbf{86}$%, and $\mathbf{152}$% respectively. The superior results highlight the immense potential of our approach for real-world robotics applications. Videos are available at https://yangsizhe.github.io/MoVie/ .
</details>
<details>
<summary>摘要</summary>
Visible Reinforcement Learning（RL）代理人在有限视角下训练的情况下面临普遍化学习能力的挑战。这种问题被称为“视图普遍化问题”。在这项工作中，我们系统地将这个基本问题分为四种特点强大且具有挑战性的情况，与实际情况很相似。然后，我们提议一种简单 yet有效的方法，可以在测试时使Visual模型基于策略扩展到未经见过的视图，不需要显式奖励信号和任何修改 durante el entrenamiento。我们的方法在四种情况下表现出了显著的进步，涵盖了DMControl、xArm和Adroit中的18个任务，相对于原始方法提高了33%、86%和152%。这些出色的结果表明我们的方法在实际 робо特应用中具有极大的潜力。视频可以在https://yangsizhe.github.io/MoVie/ 中找到。
</details></li>
</ul>
<hr>
<h2 id="HODINet-High-Order-Discrepant-Interaction-Network-for-RGB-D-Salient-Object-Detection"><a href="#HODINet-High-Order-Discrepant-Interaction-Network-for-RGB-D-Salient-Object-Detection" class="headerlink" title="HODINet: High-Order Discrepant Interaction Network for RGB-D Salient Object Detection"></a>HODINet: High-Order Discrepant Interaction Network for RGB-D Salient Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00954">http://arxiv.org/abs/2307.00954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kang Yi, Jing Xu, Xiao Jin, Fu Guo, Yan-Feng Wu</li>
<li>for: RGB-D salient object detection (SOD)</li>
<li>methods: 使用 transformer-based 和 CNN-based 架构为背景，并在不同阶段进行高级别交互 Feature Fusion</li>
<li>results: 对 seven  widely used 数据集进行了广泛的实验，并在四个评价指标上达到了竞争性的表现<details>
<summary>Abstract</summary>
RGB-D salient object detection (SOD) aims to detect the prominent regions by jointly modeling RGB and depth information. Most RGB-D SOD methods apply the same type of backbones and fusion modules to identically learn the multimodality and multistage features. However, these features contribute differently to the final saliency results, which raises two issues: 1) how to model discrepant characteristics of RGB images and depth maps; 2) how to fuse these cross-modality features in different stages. In this paper, we propose a high-order discrepant interaction network (HODINet) for RGB-D SOD. Concretely, we first employ transformer-based and CNN-based architectures as backbones to encode RGB and depth features, respectively. Then, the high-order representations are delicately extracted and embedded into spatial and channel attentions for cross-modality feature fusion in different stages. Specifically, we design a high-order spatial fusion (HOSF) module and a high-order channel fusion (HOCF) module to fuse features of the first two and the last two stages, respectively. Besides, a cascaded pyramid reconstruction network is adopted to progressively decode the fused features in a top-down pathway. Extensive experiments are conducted on seven widely used datasets to demonstrate the effectiveness of the proposed approach. We achieve competitive performance against 24 state-of-the-art methods under four evaluation metrics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Building-Self-Aware-Object-Detectors-via-Reliable-Uncertainty-Quantification-and-Calibration"><a href="#Towards-Building-Self-Aware-Object-Detectors-via-Reliable-Uncertainty-Quantification-and-Calibration" class="headerlink" title="Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration"></a>Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00934">http://arxiv.org/abs/2307.00934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fiveai/saod">https://github.com/fiveai/saod</a></li>
<li>paper_authors: Kemal Oksuz, Tom Joy, Puneet K. Dokania</li>
<li>for: 本文提出了一个新的测试检测器 robustness 的任务，即 Self-Aware Object Detection (SAOD) 任务，以解决现有测试方法存在的重大缺陷，例如不当的 Out-of-distribution 检测方法和不包括本地化和分类质量的Calibration  метриcs。</li>
<li>methods: 本文提出了一种新的测试框架，包括新的指标和大规模测试数据集，用于测试多种 object detector 的Robustness性能。</li>
<li>results: 经过EXTENSIVE 使用本文提出的测试框架，研究人员可以获得详细的检测器 Robustness 性能分析结果，包括检测器对域 shift 的Robustness，Scene中的信息 Uncertainty 度量，以及检测结果的Calibration 性能。此外，本文还提出了一个简单的基准方法，以便研究人员可以对未来的提议方法进行比较。<details>
<summary>Abstract</summary>
The current approach for testing the robustness of object detectors suffers from serious deficiencies such as improper methods of performing out-of-distribution detection and using calibration metrics which do not consider both localisation and classification quality. In this work, we address these issues, and introduce the Self-Aware Object Detection (SAOD) task, a unified testing framework which respects and adheres to the challenges that object detectors face in safety-critical environments such as autonomous driving. Specifically, the SAOD task requires an object detector to be: robust to domain shift; obtain reliable uncertainty estimates for the entire scene; and provide calibrated confidence scores for the detections. We extensively use our framework, which introduces novel metrics and large scale test datasets, to test numerous object detectors in two different use-cases, allowing us to highlight critical insights into their robustness performance. Finally, we introduce a simple baseline for the SAOD task, enabling researchers to benchmark future proposed methods and move towards robust object detectors which are fit for purpose. Code is available at https://github.com/fiveai/saod
</details>
<details>
<summary>摘要</summary>
当前对对象检测器的测试方法存在严重的缺陷，例如不当的异常检测方法和不充分考虑本地化和分类质量的评价指标。在这种工作中，我们解决这些问题，并提出了自适应对象检测（SAOD）任务，一种统一的测试框架，尊重和遵循对自动驾驶等安全关键环境中的对象检测器所面临的挑战。具体来说，SAOD任务要求对象检测器：对域shift具有抗衰减能力; 在整个场景中获得可靠的uncertainty估计; 并提供准确的信任分数。我们广泛使用我们的框架，引入了新的指标和大规模测试数据集，对许多对象检测器进行了两种不同的应用场景的测试，从而得出了对其Robustness性能的重要发现。最后，我们提出了一个简单的基线方法，使得研究人员可以对未来的提议方法进行比较，并推动对象检测器的Robustness性能进一步提高。代码可以在https://github.com/fiveai/saod上获取。
</details></li>
</ul>
<hr>
<h2 id="A-large-calcium-imaging-dataset-reveals-a-systematic-V4-organization-for-natural-scenes"><a href="#A-large-calcium-imaging-dataset-reveals-a-systematic-V4-organization-for-natural-scenes" class="headerlink" title="A large calcium-imaging dataset reveals a systematic V4 organization for natural scenes"></a>A large calcium-imaging dataset reveals a systematic V4 organization for natural scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00932">http://arxiv.org/abs/2307.00932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianye Wang, Haoxuan Yao, Tai Sing Lee, Jiayi Hong, Yang Li, Hongfei Jiang, Ian Max Andolina, Shiming Tang</li>
<li>for: 研究人员希望更深入地理解视觉系统如何处理自然场景，因此他们使用了许多自然图像来观察 primate V4 的宽频碱粒镜像数据，并使用深度学习建立了 V4 的数字响应模型。</li>
<li>methods: 研究人员使用了宽频碱粒镜像和单 photon 镜像来观察 primate V4 的响应，并使用深度学习建立了 V4 的数字响应模型。</li>
<li>results: 研究人员发现了 V4 中各种自然图像特征的域化功能区域，包括表面相关特征如颜色和Texture，以及形状相关特征如边缘、曲线和人脸特征。这些结果证明了 V4 在处理自然场景时的细致 topological 组织和神经编码。<details>
<summary>Abstract</summary>
The visual system evolved to process natural scenes, yet most of our understanding of the topology and function of visual cortex derives from studies using artificial stimuli. To gain deeper insights into visual processing of natural scenes, we utilized widefield calcium-imaging of primate V4 in response to many natural images, generating a large dataset of columnar-scale responses. We used this dataset to build a digital twin of V4 via deep learning, generating a detailed topographical map of natural image preferences at each cortical position. The map revealed clustered functional domains for specific classes of natural image features. These ranged from surface-related attributes like color and texture to shape-related features such as edges, curvature, and facial features. We validated the model-predicted domains with additional widefield calcium-imaging and single-cell resolution two-photon imaging. Our study illuminates the detailed topological organization and neural codes in V4 that represent natural scenes.
</details>
<details>
<summary>摘要</summary>
“我们的视觉系统演化来处理自然场景，但大多数我们关于视觉脑区的理解却来自使用人工刺激的研究。为了更深入了解视觉处理自然场景，我们利用了广场 calcium 影像 primate V4 的回应，生成了视觉脑区的大量柱状对应。我们使用了这个数据集来建立视觉脑区的数位双胞虫，生成了自然图像特征的详细地图。这个地图显示了各 cortical 位置的自然图像特征的集中功能领域，包括表面相关特征如颜色和文本ure，以及形状相关特征如边缘、曲线和脸部特征。我们验证了模型预测的领域使用进一步的广场 calcium 影像和单细胞分解二氢镜影像。我们的研究照明了视觉脑区中自然场景的细部 topological 组织和神经代码。”
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-multi-view-concept-decomposition"><a href="#Semi-supervised-multi-view-concept-decomposition" class="headerlink" title="Semi-supervised multi-view concept decomposition"></a>Semi-supervised multi-view concept decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00924">http://arxiv.org/abs/2307.00924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Jiang, Guoxu Zhou, Qibin Zhao</li>
<li>for: 提高多视图数据的聚类性能</li>
<li>methods: 基于多视图基因因子（CF）的半监督多视图概念因子模型（SMVCF），包括多视图CF、标签传播、抽象学习和自适应权重 вектор，以Integrate valuable information from multiple views and improve clustering performance.</li>
<li>results: 在四个多样化的数据集上进行了广泛的实验，并证明了SMVCF模型在多视图聚类任务中的效果和优势。<details>
<summary>Abstract</summary>
Concept Factorization (CF), as a novel paradigm of representation learning, has demonstrated superior performance in multi-view clustering tasks. It overcomes limitations such as the non-negativity constraint imposed by traditional matrix factorization methods and leverages kernel methods to learn latent representations that capture the underlying structure of the data, thereby improving data representation. However, existing multi-view concept factorization methods fail to consider the limited labeled information inherent in real-world multi-view data. This often leads to significant performance loss. To overcome these limitations, we propose a novel semi-supervised multi-view concept factorization model, named SMVCF. In the SMVCF model, we first extend the conventional single-view CF to a multi-view version, enabling more effective exploration of complementary information across multiple views. We then integrate multi-view CF, label propagation, and manifold learning into a unified framework to leverage and incorporate valuable information present in the data. Additionally, an adaptive weight vector is introduced to balance the importance of different views in the clustering process. We further develop targeted optimization methods specifically tailored for the SMVCF model. Finally, we conduct extensive experiments on four diverse datasets with varying label ratios to evaluate the performance of SMVCF. The experimental results demonstrate the effectiveness and superiority of our proposed approach in multi-view clustering tasks.
</details>
<details>
<summary>摘要</summary>
科学技术（CF）是一种新的表示学习方法，在多视图归一 clustering 任务中表现出了超越性。它超越了传统矩阵因子化方法中的非正式约束，并通过核函数方法学习隐藏的表示，以捕捉数据的下面结构，从而改善数据表示。然而，现有的多视图概念因子化方法通常忽视了实际世界中多视图数据中的有限 Label 信息。这经常导致显著的性能损失。为解决这些限制，我们提出了一种新的半监督多视图概念因子化模型，称为 SMVCF。在 SMVCF 模型中，我们首先将单视图 CF 扩展到多视图版本，以更好地利用多视图数据中的补充信息。然后，我们将多视图 CF、标签传播、抽象学习集成到一个统一框架中，以利用数据中的有价值信息。此外，我们还引入了自适应权重 вектор，以平衡不同视图在归一 clustering 过程中的重要性。最后，我们专门为 SMVCF 模型开发了目标优化方法。我们在四种不同的数据集上进行了广泛的实验，并评估了 SMVCF 模型的性能。实验结果表明，我们提出的方法在多视图归一 clustering 任务中表现出了有效性和优势。
</details></li>
</ul>
<hr>
<h2 id="Many-tasks-make-light-work-Learning-to-localise-medical-anomalies-from-multiple-synthetic-tasks"><a href="#Many-tasks-make-light-work-Learning-to-localise-medical-anomalies-from-multiple-synthetic-tasks" class="headerlink" title="Many tasks make light work: Learning to localise medical anomalies from multiple synthetic tasks"></a>Many tasks make light work: Learning to localise medical anomalies from multiple synthetic tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00899">http://arxiv.org/abs/2307.00899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matt-baugh/many-tasks-make-light-work">https://github.com/matt-baugh/many-tasks-make-light-work</a></li>
<li>paper_authors: Matthew Baugh, Jeremy Tan, Johanna P. Müller, Mischa Dombrowski, James Batten, Bernhard Kainz</li>
<li>for: 这篇论文旨在解决单类模型和非标型检测问题，因为完全监督学习模型无法可靠地识别没有在训练中包含的类型。</li>
<li>methods: 该论文使用自动生成的异常数据和生成器自动编码器来解决这个问题，并且通过多个可见分割的异常学习任务进行训练和验证。</li>
<li>results: 该论文可以轻松超越现有的方法，并在脑MRI和胸部X射线图像上进行了示例。<details>
<summary>Abstract</summary>
There is a growing interest in single-class modelling and out-of-distribution detection as fully supervised machine learning models cannot reliably identify classes not included in their training. The long tail of infinitely many out-of-distribution classes in real-world scenarios, e.g., for screening, triage, and quality control, means that it is often necessary to train single-class models that represent an expected feature distribution, e.g., from only strictly healthy volunteer data. Conventional supervised machine learning would require the collection of datasets that contain enough samples of all possible diseases in every imaging modality, which is not realistic. Self-supervised learning methods with synthetic anomalies are currently amongst the most promising approaches, alongside generative auto-encoders that analyse the residual reconstruction error. However, all methods suffer from a lack of structured validation, which makes calibration for deployment difficult and dataset-dependant. Our method alleviates this by making use of multiple visually-distinct synthetic anomaly learning tasks for both training and validation. This enables more robust training and generalisation. With our approach we can readily outperform state-of-the-art methods, which we demonstrate on exemplars in brain MRI and chest X-rays. Code is available at https://github.com/matt-baugh/many-tasks-make-light-work .
</details>
<details>
<summary>摘要</summary>
随着单类模型和 OUT-OF-Distribution 检测的兴趣增长，因为完全supervised机器学习模型无法可靠地识别不包括在其训练中的类。实际世界中的长尾多样化 OUT-OF-Distribution 类型，例如屏检、分类和质控，意味着需要训练单类模型，表示预期的特征分布，例如只从严格健康志愿者数据中训练。传统的supervised机器学习需要收集包含所有可能的疾病样本的 dataset，这并不是现实的。self-supervised learning方法与生成式 auto-encoders 是目前最有前途的方法，但所有方法受到结构化验证的缺失，这使得在部署时进行调整困难和数据集dependent。我们的方法利用多个可见distinct的synthetic anomaly学习任务来 both 训练和验证，这使得训练更加坚固和普适。我们的方法可以轻松超越现有的方法，我们在 brain MRI 和胸部 X-ray 中进行了示例。代码可以在 <https://github.com/matt-baugh/many-tasks-make-light-work> 获取。
</details></li>
</ul>
<hr>
<h2 id="Synthesis-of-Contrast-Enhanced-Breast-MRI-Using-Multi-b-Value-DWI-based-Hierarchical-Fusion-Network-with-Attention-Mechanism"><a href="#Synthesis-of-Contrast-Enhanced-Breast-MRI-Using-Multi-b-Value-DWI-based-Hierarchical-Fusion-Network-with-Attention-Mechanism" class="headerlink" title="Synthesis of Contrast-Enhanced Breast MRI Using Multi-b-Value DWI-based Hierarchical Fusion Network with Attention Mechanism"></a>Synthesis of Contrast-Enhanced Breast MRI Using Multi-b-Value DWI-based Hierarchical Fusion Network with Attention Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00895">http://arxiv.org/abs/2307.00895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Zhang, Luyi Han, Anna D’Angelo, Xin Wang, Yuan Gao, Chunyao Lu, Jonas Teuwen, Regina Beets-Tan, Tao Tan, Ritse Mann</li>
<li>for: 这个研究旨在开发一种基于多序列融合网络的 CE-MRI Synthesis方法，以降低或避免使用 GBCA，从而减轻病人的负担。</li>
<li>methods: 这个研究使用了多序列融合网络，将 T1-weighted MRI 和 DWIs 的数据 fusion 以获取 CE-MRI。在这个过程中，使用了多序列注意力模块和层次表示信息融合模块，以获取精细的特征图。</li>
<li>results: 研究结果表明，多值 b-值 DWI 基于的融合模型可能可以用于生成 CE-MRI，从而避免或减少使用 GBCA，从而减轻病人的负担。<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) is the most sensitive technique for breast cancer detection among current clinical imaging modalities. Contrast-enhanced MRI (CE-MRI) provides superior differentiation between tumors and invaded healthy tissue, and has become an indispensable technique in the detection and evaluation of cancer. However, the use of gadolinium-based contrast agents (GBCA) to obtain CE-MRI may be associated with nephrogenic systemic fibrosis and may lead to bioaccumulation in the brain, posing a potential risk to human health. Moreover, and likely more important, the use of gadolinium-based contrast agents requires the cannulation of a vein, and the injection of the contrast media which is cumbersome and places a burden on the patient. To reduce the use of contrast agents, diffusion-weighted imaging (DWI) is emerging as a key imaging technique, although currently usually complementing breast CE-MRI. In this study, we develop a multi-sequence fusion network to synthesize CE-MRI based on T1-weighted MRI and DWIs. DWIs with different b-values are fused to efficiently utilize the difference features of DWIs. Rather than proposing a pure data-driven approach, we invent a multi-sequence attention module to obtain refined feature maps, and leverage hierarchical representation information fused at different scales while utilizing the contributions from different sequences from a model-driven approach by introducing the weighted difference module. The results show that the multi-b-value DWI-based fusion model can potentially be used to synthesize CE-MRI, thus theoretically reducing or avoiding the use of GBCA, thereby minimizing the burden to patients. Our code is available at \url{https://github.com/Netherlands-Cancer-Institute/CE-MRI}.
</details>
<details>
<summary>摘要</summary>
磁共振成像（MRI）是当前临床成像技术中诊断乳腺癌最敏感的方法。增强的磁共振成像（CE-MRI）可以准确地区分肿瘤和混合到正常组织，成为诊断和评估癌病的不可或缺的技术。然而，使用高德林铵基 contrast agent（GBCA）来实现CE-MRI可能会与肾脏系统 fibrosis 相关，并且可能会在脑部堆积，对人类健康构成潜在的风险。此外，使用高德林铵基 contrast agent需要血管注射剂，这是对患者的困扰和负担。为了减少对高德林铵基 contrast agent的使用，扩散成像（DWI）正在成为诊断乳腺癌的关键成像技术之一。在这个研究中，我们开发了一种多следова列融合网络，使用T1重度成像和DWI进行CE-MRI的合成。不同的b值的DWI被融合，以利用不同的DWI特征。而不是直接提出数据驱动方法，我们创造了一种多следова列注意模块，以获得精细的特征地图，并利用层次表示信息在不同的缩放级别上进行融合。我们的研究结果表明，使用多b值DWI合成CE-MRI的模型可能可以减少或避免使用GBCA，从而减轻患者的负担。我们的代码可以在 \url{https://github.com/Netherlands-Cancer-Institute/CE-MRI} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Mega-cities-dominate-China’s-urban-greening"><a href="#Mega-cities-dominate-China’s-urban-greening" class="headerlink" title="Mega-cities dominate China’s urban greening"></a>Mega-cities dominate China’s urban greening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00894">http://arxiv.org/abs/2307.00894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxin Zhang, Martin Brandt, Xiaoye Tong, Xiaowei Tong, Wenmin Zhang, Florian Reiner, Sizhuo Li, Feng Tian, Yuemin Yue, Weiqi Zhou, Bin Chen, Xiangming Xiao, Rasmus Fensholt</li>
<li>for: 本研究旨在使用尺度小探空craft量化中国大型城市的城市树覆盖率，评估2010年和2019年城市绿化政策的影响。</li>
<li>methods: 本研究使用尺度小探空craft进行城市树覆盖率的评估，对全国所有面积超过50平方公里的主要城市进行评估。</li>
<li>results: 研究发现，2019年全国城市树覆盖率约为11%，76%的城市出现了从2010年到2019年的树覆盖增长。特别是在北京和上海等各大城市，树覆盖增长率为7.69%，远高于其他城市的3.94%。<details>
<summary>Abstract</summary>
Trees play a crucial role in urban environments, offering various ecosystem services that contribute to public health and human well-being. China has initiated a range of urban greening policies over the past decades, however, monitoring their impact on urban tree dynamics at a national scale has proven challenging. In this study, we deployed nano-satellites to quantify urban tree coverage in all major Chinese cities larger than 50 km2 in 2010 and 2019. Our findings indicate that approximately 6000 km2 (11%) of urban areas were covered by trees in 2019, and 76% of these cities experienced an increase in tree cover compared to 2010. Notably, the increase in tree cover in mega-cities such as Beijing, and Shanghai was approximately twice as large as in most other cities (7.69% vs 3.94%). The study employs a data-driven approach towards assessing urban tree cover changes in relation to greening policies, showing clear signs of tree cover increases but also suggesting an uneven implementation primarily benefiting a few mega-cities.
</details>
<details>
<summary>摘要</summary>
城市中的树木扮演着重要的生态系统服务作用，对公众健康和人类福祉产生了重要贡献。中国在过去的几十年中实施了一系列城市绿化政策，但监测这些政策对城市树木动态的影响在国家范围内是有挑战的。本研究通过使用尺度约为1米的幼卫星来评估2010年和2019年中国大于50平方公里的主要城市的城市树木覆盖率。我们发现在2019年，城市树木覆盖率约为11%，76%的城市经历了2010年相比的增长。尤其是在北京和上海等巨型城市，增长率约为7.69%，与其他城市相比为twice as large（3.94%）。本研究采用数据驱动的方法来评估城市树木覆盖变化和绿化政策之间的关系，显示出明显的树木覆盖增加，但也表明了一些巨型城市的不均衡实施。
</details></li>
</ul>
<hr>
<h2 id="Generating-Reliable-Pixel-Level-Labels-for-Source-Free-Domain-Adaptation"><a href="#Generating-Reliable-Pixel-Level-Labels-for-Source-Free-Domain-Adaptation" class="headerlink" title="Generating Reliable Pixel-Level Labels for Source Free Domain Adaptation"></a>Generating Reliable Pixel-Level Labels for Source Free Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00893">http://arxiv.org/abs/2307.00893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Tjio, Ping Liu, Yawei Luo, Chee Keong Kwoh, Joey Zhou Tianyi</li>
<li>for:  Addressing the challenging domain adaptation setting where the knowledge from the labelled source domain dataset is only available from a pretrained black-box segmentation model.</li>
<li>methods: Proposes a simple yet novel image translation workflow, ReGEN, which comprises an image-to-image translation network and a segmentation network to generate target-like images using the noisy predictions from the original target domain images.</li>
<li>results: Demonstrates favourable performance relative to recent state-of-the-art work in two benchmark domain adaptation settings.Here’s the simplified Chinese text:</li>
<li>for:  Addressing 域适应Setting中的挑战，即只有源频段数据上的标注知识可以用于黑盒子分割模型。</li>
<li>methods: 提出了一种简单 yet novel的图像翻译工作流程（ReGEN），该流程包括图像翻译网络和分割网络，用于基于原始目标频段图像的含杂预测生成目标类似图像。</li>
<li>results: 在两个标准域适应设定中展示了与最新的状态艺术工作相当的表现。<details>
<summary>Abstract</summary>
This work addresses the challenging domain adaptation setting in which knowledge from the labelled source domain dataset is available only from the pretrained black-box segmentation model. The pretrained model's predictions for the target domain images are noisy because of the distributional differences between the source domain data and the target domain data. Since the model's predictions serve as pseudo labels during self-training, the noise in the predictions impose an upper bound on model performance. Therefore, we propose a simple yet novel image translation workflow, ReGEN, to address this problem. ReGEN comprises an image-to-image translation network and a segmentation network. Our workflow generates target-like images using the noisy predictions from the original target domain images. These target-like images are semantically consistent with the noisy model predictions and therefore can be used to train the segmentation network. In addition to being semantically consistent with the predictions from the original target domain images, the generated target-like images are also stylistically similar to the target domain images. This allows us to leverage the stylistic differences between the target-like images and the target domain image as an additional source of supervision while training the segmentation model. We evaluate our model with two benchmark domain adaptation settings and demonstrate that our approach performs favourably relative to recent state-of-the-art work. The source code will be made available.
</details>
<details>
<summary>摘要</summary>
Our workflow generates target-like images using the noisy predictions from the original target domain images. These target-like images are semantically consistent with the noisy model predictions and can be used to train the segmentation network. Additionally, the generated target-like images are stylistically similar to the target domain images, allowing us to leverage the stylistic differences between the target-like images and the target domain image as an additional source of supervision during training.We evaluate our model with two benchmark domain adaptation settings and demonstrate that our approach performs favorably relative to recent state-of-the-art work. The source code will be made available.
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Deep-Framework-Towards-Task-Specific-Fusion-for-Multi-to-One-MRI-Synthesis"><a href="#An-Explainable-Deep-Framework-Towards-Task-Specific-Fusion-for-Multi-to-One-MRI-Synthesis" class="headerlink" title="An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis"></a>An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00885">http://arxiv.org/abs/2307.00885</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fiy2w/mri_seq2seq">https://github.com/fiy2w/mri_seq2seq</a></li>
<li>paper_authors: Luyi Han, Tianyu Zhang, Yunzhi Huang, Haoran Dou, Xin Wang, Yuan Gao, Chunyao Lu, Tan Tao, Ritse Mann<br>for:多序列MRI在临床设置中具有可靠诊断和治疗预测价值，但某些序列可能因各种原因而无法使用或缺失。为解决这个问题，MRI合成是一个可能的解决方案。methods:使用最新的深度学习基于方法，可以好好地组合可用的多个序列来合成缺失的序列。然而，这些方法缺乏对不同输入序列的贡献的评估和生成图像质量的估计，使其实际应用困难。因此，我们提议一种可解释的任务特定合成网络，可以自动调整任务特定的权重，并提供可读性和可靠性从两个方面：1. 可视化输入序列的贡献在合并阶段的权重平均模块，使得可以看到每个输入序列的贡献。2. 在合成图像时，使用任务特定的注意力模块，以便高亮网络在合成图像时所尝试修改的区域。results:我们在BraTS2021数据集上进行了1251个主题的实验，结果表明，我们的提议的方法在无序列合成任务中表现更好于当前状态的方法。我们的代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/fiy2W/mri_seq2seq%7D">https://github.com/fiy2W/mri_seq2seq}</a> 上找到。<details>
<summary>Abstract</summary>
Multi-sequence MRI is valuable in clinical settings for reliable diagnosis and treatment prognosis, but some sequences may be unusable or missing for various reasons. To address this issue, MRI synthesis is a potential solution. Recent deep learning-based methods have achieved good performance in combining multiple available sequences for missing sequence synthesis. Despite their success, these methods lack the ability to quantify the contributions of different input sequences and estimate the quality of generated images, making it hard to be practical. Hence, we propose an explainable task-specific synthesis network, which adapts weights automatically for specific sequence generation tasks and provides interpretability and reliability from two sides: (1) visualize the contribution of each input sequence in the fusion stage by a trainable task-specific weighted average module; (2) highlight the area the network tried to refine during synthesizing by a task-specific attention module. We conduct experiments on the BraTS2021 dataset of 1251 subjects, and results on arbitrary sequence synthesis indicate that the proposed method achieves better performance than the state-of-the-art methods. Our code is available at \url{https://github.com/fiy2W/mri_seq2seq}.
</details>
<details>
<summary>摘要</summary>
多重序列MRI在临床 Settings中是有价值的，可以提供可靠的诊断和治疗预测，但某些序列可能无法使用或缺失。为解决这个问题，MRI合成是一个可能的解决方案。现代深度学习基于方法在 combining 多个可用序列中缺失序列的合成方面 achieved 好的性能。尽管它们在成功，但它们缺乏对不同输入序列的贡献的评估和生成图像质量的估计，使其实难实际应用。因此，我们提议一种可解释的任务特定合成网络，该网络可自动适应特定序列生成任务，并提供可读性和可靠性从两个方面：1. 在合并阶段，使用可训练的任务特定权重平均模块来可视化每个输入序列的贡献。2. 使用任务特定注意力模块来高亮合成过程中网络尝试修改的区域。我们在 BraTS2021 数据集上进行了1251个主题的实验，并对arbitrary sequence synthesis进行了测试。结果表明，我们的方法在比 estado-of-the-art 方法更好。我们的代码可以在 \url{https://github.com/fiy2W/mri_seq2seq} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Co-Learning-Meets-Stitch-Up-for-Noisy-Multi-label-Visual-Recognition"><a href="#Co-Learning-Meets-Stitch-Up-for-Noisy-Multi-label-Visual-Recognition" class="headerlink" title="Co-Learning Meets Stitch-Up for Noisy Multi-label Visual Recognition"></a>Co-Learning Meets Stitch-Up for Noisy Multi-label Visual Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00880">http://arxiv.org/abs/2307.00880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vamosc/colearning-meet-stitchup">https://github.com/vamosc/colearning-meet-stitchup</a></li>
<li>paper_authors: Chao Liang, Zongxin Yang, Linchao Zhu, Yi Yang</li>
<li>for:  Handle long-tailed multi-label recognition with noisy labels.</li>
<li>methods:  Propose a Stitch-Up augmentation to synthesize cleaner samples, and a Heterogeneous Co-Learning framework to leverage the inconsistency between long-tailed and balanced distributions.</li>
<li>results:  Achieve superior results compared to various baselines, demonstrating the effectiveness of the proposed method in handling noisy long-tailed multi-label data.Here’s the full text in Simplified Chinese:</li>
<li>for:  Handle long-tailed multi-label recognition with noisy labels.</li>
<li>methods:  Propose a Stitch-Up augmentation to synthesize cleaner samples, 以及一个 Heterogeneous Co-Learning 框架，利用长尾分布和多 Label 的不一致性，生成更清楚的标签。</li>
<li>results:  Compared to various baselines, 提出的方法在处理噪音长尾多 Label 资料时取得了superior的结果，证明了方法的有效性。<details>
<summary>Abstract</summary>
In real-world scenarios, collected and annotated data often exhibit the characteristics of multiple classes and long-tailed distribution. Additionally, label noise is inevitable in large-scale annotations and hinders the applications of learning-based models. Although many deep learning based methods have been proposed for handling long-tailed multi-label recognition or label noise respectively, learning with noisy labels in long-tailed multi-label visual data has not been well-studied because of the complexity of long-tailed distribution entangled with multi-label correlation. To tackle such a critical yet thorny problem, this paper focuses on reducing noise based on some inherent properties of multi-label classification and long-tailed learning under noisy cases. In detail, we propose a Stitch-Up augmentation to synthesize a cleaner sample, which directly reduces multi-label noise by stitching up multiple noisy training samples. Equipped with Stitch-Up, a Heterogeneous Co-Learning framework is further designed to leverage the inconsistency between long-tailed and balanced distributions, yielding cleaner labels for more robust representation learning with noisy long-tailed data. To validate our method, we build two challenging benchmarks, named VOC-MLT-Noise and COCO-MLT-Noise, respectively. Extensive experiments are conducted to demonstrate the effectiveness of our proposed method. Compared to a variety of baselines, our method achieves superior results.
</details>
<details>
<summary>摘要</summary>
在实际应用场景中，收集的数据经常具有多个类别和长尾分布的特点，同时 Label noise 是无法避免的。虽然许多深度学习基于方法已经提出来处理长尾多标签识别或 Label noise，但是在长尾多标签视觉数据中学习受损 Label noise 还没有得到充分研究，这是因为长尾分布和多标签相互作用的复杂性。为解决这种复杂而困难的问题，本文强调降低 Label noise 基于多标签分类和长尾学习的一些自然性质。在详细的实现方式中，我们提出了一种叫做 Stitch-Up 的扩充方法，可以直接减少多标签雷达。此外，我们还设计了一种叫做 Heterogeneous Co-Learning 的框架，可以利用长尾和平衡分布之间的不一致性，以便更好地学习受损 Label noise 的 cleaner 标签。为证明我们的方法的效果，我们建立了两个具有挑战性的 benchmark，分别是 VOC-MLT-Noise 和 COCO-MLT-Noise。我们进行了广泛的实验，以确认我们的方法的效果。相比多种基线方法，我们的方法得到了更好的结果。
</details></li>
</ul>
<hr>
<h2 id="End-To-End-Prediction-of-Knee-Osteoarthritis-Progression-With-Multi-Modal-Transformers"><a href="#End-To-End-Prediction-of-Knee-Osteoarthritis-Progression-With-Multi-Modal-Transformers" class="headerlink" title="End-To-End Prediction of Knee Osteoarthritis Progression With Multi-Modal Transformers"></a>End-To-End Prediction of Knee Osteoarthritis Progression With Multi-Modal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00873">http://arxiv.org/abs/2307.00873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Egor Panfilov, Simo Saarakkala, Miika T. Nieminen, Aleksei Tiulpin</li>
<li>for: The paper is written to investigate the use of multi-modal data and deep learning methods for predicting the progression of knee osteoarthritis (KOA).</li>
<li>methods: The paper uses a Transformer approach for multi-modal fusion of knee imaging data, and analyzes its performance across different progression horizons.</li>
<li>results: The paper shows that structural knee MRI can identify radiographic KOA progressors with an area under the ROC curve (ROC AUC) of 0.70-0.76 and Average Precision (AP) of 0.15-0.54 in 2-8 year horizons. Additionally, the paper finds that progression within 1 year can be better predicted with a multi-modal method using X-ray, structural, and compositional MR images.Here are the results in Simplified Chinese text:</li>
<li>for: 这个研究是为了研究多Modal数据和深度学习方法在股骨关节炎(KOA)的进展预测方面。</li>
<li>methods: 这个研究使用Transformer方法进行多Modal数据的拟合，并分析其在不同进展时间 horizon 上的性能。</li>
<li>results: 这个研究发现，结构性股骨MRI可以在2-8年的进展时间 horizon 上识别 radiographic KOA 进行者，ROC AUC 为0.70-0.76，AP 为0.15-0.54。此外，研究发现，使用多Modal的X-ray、结构性和化学成分MRI Image 可以更好地预测在1年内的进展。<details>
<summary>Abstract</summary>
Knee Osteoarthritis (KOA) is a highly prevalent chronic musculoskeletal condition with no currently available treatment. The manifestation of KOA is heterogeneous and prediction of its progression is challenging. Current literature suggests that the use of multi-modal data and advanced modeling methods, such as the ones based on Deep Learning, has promise in tackling this challenge. To date, however, the evidence on the efficacy of this approach is limited. In this study, we leveraged recent advances in Deep Learning and, using a Transformer approach, developed a unified framework for the multi-modal fusion of knee imaging data. Subsequently, we analyzed its performance across a range of scenarios by investigating multiple progression horizons -- from short-term to long-term. We report our findings using a large cohort (n=2421-3967) derived from the Osteoarthritis Initiative dataset. We show that structural knee MRI allows identifying radiographic KOA progressors on par with multi-modal fusion approaches, achieving an area under the ROC curve (ROC AUC) of 0.70-0.76 and Average Precision (AP) of 0.15-0.54 in 2-8 year horizons. Progression within 1 year was better predicted with a multi-modal method using X-ray, structural, and compositional MR images -- ROC AUC of 0.76(0.04), AP of 0.13(0.04) -- or via clinical data. Our follow-up analysis generally shows that prediction from the imaging data is more accurate for post-traumatic subjects, and we further investigate which subject subgroups may benefit the most. The present study provides novel insights into multi-modal imaging of KOA and brings a unified data-driven framework for studying its progression in an end-to-end manner, providing new tools for the design of more efficient clinical trials. The source code of our framework and the pre-trained models are made publicly available.
</details>
<details>
<summary>摘要</summary>
knee退化病 (KOA) 是一种非常流行的慢性 musculoskeletal 疾病，目前没有可用的治疗方案。 KOA 的表现是多iform 的，预测其进展是具有挑战性。现有文献建议使用多 modal 数据和进步的模型方法，如基于深度学习的方法，可以解决这个挑战。然而，至今为止，这种方法的效果仍然有限。在这个研究中，我们利用了最新的深度学习技术，使用 transformer 方法，实现了多modal 影像资料的融合。然后，我们分析了这个框架在不同的时间长度下的表现，包括短期、中期和长期。我们使用了一大群患者（n=2421-3967）， derive 自 Osteoarthritis Initiative 资料集。我们发现，静止股骨 MRI 可以与多modal 融合方法相比，预测 X-ray 照片 KOA 进展者，ROC AUC 为 0.70-0.76，AP 为 0.15-0.54 在 2-8 年时间长度下。在 1 年内进展预测中，多modal 方法使用 X-ray、静止、和化学 MR 影像的结果更高，ROC AUC 为 0.76(0.04)，AP 为 0.13(0.04)。我们的续作分析显示，从影像数据预测 KOA 的进展更加精确，特别是在受到伤害后的患者中。我们进一步研究了哪些患者子群可能从影像数据中得到最多的帮助。这项研究提供了新的关于多modal 影像 KOA 的新知识，并提供了一个统一的数据驱动的框架，实现了预测 KOA 进展的端到端方式。我们的框架和预训模型的源代码都公开 disponibile。
</details></li>
</ul>
<hr>
<h2 id="VINECS-Video-based-Neural-Character-Skinning"><a href="#VINECS-Video-based-Neural-Character-Skinning" class="headerlink" title="VINECS: Video-based Neural Character Skinning"></a>VINECS: Video-based Neural Character Skinning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00842">http://arxiv.org/abs/2307.00842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhouyingcheng Liao, Vladislav Golyanik, Marc Habermann, Christian Theobalt</li>
<li>for: 创建高级别的人物游戏角色，自动生成高级别的人物模型和皮肤材料</li>
<li>methods: 使用多视图视频学习技术，自动生成人物模型的皮肤纹理、皮肤骨骼和动作纹理</li>
<li>results: 比起现有方法，提高人物模型的自动生成效果，不需要高级别的4D扫描数据<details>
<summary>Abstract</summary>
Rigging and skinning clothed human avatars is a challenging task and traditionally requires a lot of manual work and expertise. Recent methods addressing it either generalize across different characters or focus on capturing the dynamics of a single character observed under different pose configurations. However, the former methods typically predict solely static skinning weights, which perform poorly for highly articulated poses, and the latter ones either require dense 3D character scans in different poses or cannot generate an explicit mesh with vertex correspondence over time. To address these challenges, we propose a fully automated approach for creating a fully rigged character with pose-dependent skinning weights, which can be solely learned from multi-view video. Therefore, we first acquire a rigged template, which is then statically skinned. Next, a coordinate-based MLP learns a skinning weights field parameterized over the position in a canonical pose space and the respective pose. Moreover, we introduce our pose- and view-dependent appearance field allowing us to differentiably render and supervise the posed mesh using multi-view imagery. We show that our approach outperforms state-of-the-art while not relying on dense 4D scans.
</details>
<details>
<summary>摘要</summary>
rigging和皮肤化人物模型是一项复杂的任务，传统上需要大量的手动工作和专业知识。现有的方法可以涵盖不同的人物，但是大多数情况下预测的是静止皮肤 веса，性能对高度动作pose较差，而其他方法则需要在不同的姿势下获得密集的3D人物扫描，或者无法在时间上生成明确的网格。为了解决这些挑战，我们提出了一种完全自动化的方法，可以从多视图视频中学习出fully rigged人物，并且可以靠 solely 在多视图图像上进行渲染和监督。我们的方法包括以下几个步骤：首先，我们获得一个已经RIGGED的模板，然后使用一个坐标基于的多层感知学习（MLP）来学习皮肤 веса场，该场被参数化为位置在一个坐标系中的 canonical pose 空间和相应的姿势。此外，我们引入了 pose-和 view-dependent的外观场，使得我们可以在多视图图像上分别渲染和监督着poses的人物模型。我们的方法比 estado-of-the-art 高效，而不需要密集的4D扫描。
</details></li>
</ul>
<hr>
<h2 id="Surgical-fine-tuning-for-Grape-Bunch-Segmentation-under-Visual-Domain-Shifts"><a href="#Surgical-fine-tuning-for-Grape-Bunch-Segmentation-under-Visual-Domain-Shifts" class="headerlink" title="Surgical fine-tuning for Grape Bunch Segmentation under Visual Domain Shifts"></a>Surgical fine-tuning for Grape Bunch Segmentation under Visual Domain Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00837">http://arxiv.org/abs/2307.00837</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/airlab-polimi/sft_grape_segmentation">https://github.com/airlab-polimi/sft_grape_segmentation</a></li>
<li>paper_authors: Agnese Chiatti, Riccardo Bertoglio, Nico Catalano, Matteo Gatti, Matteo Matteucci</li>
<li>for: 本研究旨在帮助移动机器人在农业 settings中自动和有效地监测植物状况，因此需要机器人具备鲜明的视觉感知能力，抗性强 против农业设置中的快速变化。</li>
<li>methods: 本研究使用手术精细调整（surgical fine-tuning）来适应新收集的葡萄图像，并且只需要调整特定的模型层，从而减少参数的数量，同时支持模型适应新的视域变化。</li>
<li>results: 本研究表明，通过手术精细调整，可以有效地适应新收集的葡萄图像，并且可以减少参数的数量，从而提高模型的鲜明性和灵活性。<details>
<summary>Abstract</summary>
Mobile robots will play a crucial role in the transition towards sustainable agriculture. To autonomously and effectively monitor the state of plants, robots ought to be equipped with visual perception capabilities that are robust to the rapid changes that characterise agricultural settings. In this paper, we focus on the challenging task of segmenting grape bunches from images collected by mobile robots in vineyards. In this context, we present the first study that applies surgical fine-tuning to instance segmentation tasks. We show how selectively tuning only specific model layers can support the adaptation of pre-trained Deep Learning models to newly-collected grape images that introduce visual domain shifts, while also substantially reducing the number of tuned parameters.
</details>
<details>
<summary>摘要</summary>
Mobile robots will play a crucial role in the transition towards sustainable agriculture. To autonomously and effectively monitor the state of plants, robots ought to be equipped with visual perception capabilities that are robust to the rapid changes that characterize agricultural settings. In this paper, we focus on the challenging task of segmenting grape bunches from images collected by mobile robots in vineyards. In this context, we present the first study that applies surgical fine-tuning to instance segmentation tasks. We show how selectively tuning only specific model layers can support the adaptation of pre-trained Deep Learning models to newly-collected grape images that introduce visual domain shifts, while also substantially reducing the number of tuned parameters.Here's the translation in Traditional Chinese as well:Mobile robots will play a crucial role in the transition towards sustainable agriculture. To autonomously and effectively monitor the state of plants, robots ought to be equipped with visual perception capabilities that are robust to the rapid changes that characterize agricultural settings. In this paper, we focus on the challenging task of segmenting grape bunches from images collected by mobile robots in vineyards. In this context, we present the first study that applies surgical fine-tuning to instance segmentation tasks. We show how selectively tuning only specific model layers can support the adaptation of pre-trained Deep Learning models to newly-collected grape images that introduce visual domain shifts, while also substantially reducing the number of tuned parameters.
</details></li>
</ul>
<hr>
<h2 id="Robust-Surgical-Tools-Detection-in-Endoscopic-Videos-with-Noisy-Data"><a href="#Robust-Surgical-Tools-Detection-in-Endoscopic-Videos-with-Noisy-Data" class="headerlink" title="Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data"></a>Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01232">http://arxiv.org/abs/2307.01232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adnan Qayyum, Hassan Ali, Massimo Caputo, Hunaid Vohra, Taofeek Akinosho, Sofiat Abioye, Ilhem Berrou, Paweł Capik, Junaid Qadir, Muhammad Bilal</li>
<li>for:  This paper aims to develop a systematic methodology for robust surgical tool detection using noisy data.</li>
<li>methods: The proposed methodology introduces an intelligent active learning strategy for minimal dataset identification and label correction by human experts, as well as an assembling strategy for a student-teacher model-based self-training framework to achieve robust classification of 14 surgical tools in a semi-supervised fashion.</li>
<li>results: The proposed methodology achieves an average F1-score of 85.88% for the ensemble model-based self-training with class weights, and 80.88% without class weights for noisy labels. The proposed method significantly outperforms existing approaches, effectively demonstrating its effectiveness.<details>
<summary>Abstract</summary>
Over the past few years, surgical data science has attracted substantial interest from the machine learning (ML) community. Various studies have demonstrated the efficacy of emerging ML techniques in analysing surgical data, particularly recordings of procedures, for digitizing clinical and non-clinical functions like preoperative planning, context-aware decision-making, and operating skill assessment. However, this field is still in its infancy and lacks representative, well-annotated datasets for training robust models in intermediate ML tasks. Also, existing datasets suffer from inaccurate labels, hindering the development of reliable models. In this paper, we propose a systematic methodology for developing robust models for surgical tool detection using noisy data. Our methodology introduces two key innovations: (1) an intelligent active learning strategy for minimal dataset identification and label correction by human experts; and (2) an assembling strategy for a student-teacher model-based self-training framework to achieve the robust classification of 14 surgical tools in a semi-supervised fashion. Furthermore, we employ weighted data loaders to handle difficult class labels and address class imbalance issues. The proposed methodology achieves an average F1-score of 85.88\% for the ensemble model-based self-training with class weights, and 80.88\% without class weights for noisy labels. Also, our proposed method significantly outperforms existing approaches, which effectively demonstrates its effectiveness.
</details>
<details>
<summary>摘要</summary>
过去几年，外科数据科学已经吸引了机器学习（ML）社区的广泛关注。各种研究表明了新兴ML技术在分析外科数据方面的效果，特别是记录手术过程的数据，用于数字化临床和非临床功能，如前置规划、Context-aware决策和手术技巧评估。然而，这一领域仍处于初期阶段，缺乏代表性的、正确标注的数据集，用于训练中等ML任务。此外，现有的数据集受到不准确的标注，妨碍了模型的发展。在本文中，我们提出了一种系统的方法ологи，用于在噪音数据上建立可靠的外科工具检测模型。我们的方法有两个关键创新：（1）一种智能活动学习策略，用于标注和人工专家审核的最小数据集选择；（2）一种学生-教师模型自我训练框架，用于在半监督式下实现多 Tool 的精度分类。此外，我们采用了权重数据加载器，处理困难分类和分类不均衡问题。我们的提议方法在 ensemble 模型基础自动训练中，使用类别权重时，达到了85.88%的平均 F1 分数，无类别权重时，达到了80.88%。此外，我们的提议方法在与现有方法进行比较时，显示了明显的效果。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Potential-of-Spike-Streams-for-Foreground-Occlusion-Removal-from-Densely-Continuous-Views"><a href="#Unveiling-the-Potential-of-Spike-Streams-for-Foreground-Occlusion-Removal-from-Densely-Continuous-Views" class="headerlink" title="Unveiling the Potential of Spike Streams for Foreground Occlusion Removal from Densely Continuous Views"></a>Unveiling the Potential of Spike Streams for Foreground Occlusion Removal from Densely Continuous Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00821">http://arxiv.org/abs/2307.00821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiyuan Zhang, Shiyan Chen, Yajing Zheng, Zhaofei Yu, Tiejun Huang</li>
<li>for:  removes dense foreground occlusion</li>
<li>methods:  continuous multi-view imaging with one spike camera, novel model \textbf{SpkOccNet}, cross-view mutual attention mechanism</li>
<li>results:  efficient removal of dense occlusions in diverse scenes, strong generalization.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在解决 dense foreground occlusion 问题，实现高质量的背景清晰化。</li>
<li>methods: 我们提出了一种基于 continuous multi-view imaging 和 novel model \textbf{SpkOccNet} 的方法，使用 cross-view mutual attention mechanism 进行有效的融合和精度调整。</li>
<li>results: 我们的方法可以高效地除去各种场景中的 dense occlusions，并且具有强大的泛化能力。<details>
<summary>Abstract</summary>
The extraction of a clean background image by removing foreground occlusion holds immense practical significance, but it also presents several challenges. Presently, the majority of de-occlusion research focuses on addressing this issue through the extraction and synthesis of discrete images from calibrated camera arrays. Nonetheless, the restoration quality tends to suffer when faced with dense occlusions or high-speed motions due to limited perspectives and motion blur. To successfully remove dense foreground occlusion, an effective multi-view visual information integration approach is required. Introducing the spike camera as a novel type of neuromorphic sensor offers promising capabilities with its ultra-high temporal resolution and high dynamic range. In this paper, we propose an innovative solution for tackling the de-occlusion problem through continuous multi-view imaging using only one spike camera without any prior knowledge of camera intrinsic parameters and camera poses. By rapidly moving the spike camera, we continually capture the dense stream of spikes from the occluded scene. To process the spikes, we build a novel model \textbf{SpkOccNet}, in which we integrate information of spikes from continuous viewpoints within multi-windows, and propose a novel cross-view mutual attention mechanism for effective fusion and refinement. In addition, we contribute the first real-world spike-based dataset \textbf{S-OCC} for occlusion removal. The experimental results demonstrate that our proposed model efficiently removes dense occlusions in diverse scenes while exhibiting strong generalization.
</details>
<details>
<summary>摘要</summary>
抽取清晰背景图像，除去前景遮挡，具有很大的实际意义，但也存在多种挑战。目前，大多数遮挡恢复研究通过抽取和合成扩展的照片阵列来解决这个问题。然而，当面临紧密的遮挡物或高速运动时， restore 质量往往受到限制，因为有限的视角和运动模糊。要成功地除去紧密的前景遮挡，需要一种有效的多视图视觉信息集成方法。在这篇论文中，我们提出了一种新的解决方案，通过不断拍摄快速移动的射频摄像机来实现连续多视图抽取。我们在不知道摄像机内参数和摄像机位置的情况下，通过 continually 捕捉遮挡场景中的快速流动的射频脉冲来处理脉冲。为了处理脉冲，我们建立了一种新的模型，即 SpkOccNet，其中我们在多窗口内集成了不断更新的脉冲信息，并提出了一种新的交叉视图互注意机制来实现有效的融合和修正。此外，我们也提供了首个实际遮挡基本数据集，即 S-OCC，以用于遮挡除去。实验结果表明，我们提出的模型能够有效地除去多种场景中的紧密遮挡。
</details></li>
</ul>
<hr>
<h2 id="Motion-X-A-Large-scale-3D-Expressive-Whole-body-Human-Motion-Dataset"><a href="#Motion-X-A-Large-scale-3D-Expressive-Whole-body-Human-Motion-Dataset" class="headerlink" title="Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset"></a>Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00818">http://arxiv.org/abs/2307.00818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idea-research/motion-x">https://github.com/idea-research/motion-x</a></li>
<li>paper_authors: Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, Lei Zhang</li>
<li>for: This paper is written for researchers and developers who work on 3D human pose estimation, motion capture, and computer vision.</li>
<li>methods: The paper presents a whole-body motion and text annotation pipeline that can automatically annotate motion from single- or multi-view videos and provide comprehensive semantic labels for each video and fine-grained whole-body pose descriptions for each frame.</li>
<li>results: The paper constructs Motion-X, a large-scale 3D expressive whole-body motion dataset that covers 96K motion sequences from massive scenes, with 13.7M precise 3D whole-body pose annotations and 13.7M frame-level whole-body pose descriptions. The paper demonstrates the accuracy of the annotation pipeline and the significant benefit of Motion-X in enhancing expressive, diverse, and natural motion generation, as well as 3D whole-body human mesh recovery.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为研究者和开发者们写的，他们工作在3D人体姿态估计、动作捕捉和计算机视觉领域。</li>
<li>methods: 论文提出了一种整体人体动作和文本注释管道，可以自动从单视或多视视频中注释动作，并提供每个视频的全面semantic标签和每帧精细的人体姿态描述。</li>
<li>results: 论文构建了Motion-X数据集，包括96万个动作序列，涵盖庞大的场景，每个动作序列具有1370万个精细的3D人体姿态注释和每帧的人体姿态描述。论文证明了注释管道的准确性和Motion-X数据集在增强自然、多样化和表达力强的动作生成、3D人体人脸恢复方面的重要作用。<details>
<summary>Abstract</summary>
In this paper, we present Motion-X, a large-scale 3D expressive whole-body motion dataset. Existing motion datasets predominantly contain body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions. Moreover, they are primarily collected from limited laboratory scenes with textual descriptions manually labeled, which greatly limits their scalability. To overcome these limitations, we develop a whole-body motion and text annotation pipeline, which can automatically annotate motion from either single- or multi-view videos and provide comprehensive semantic labels for each video and fine-grained whole-body pose descriptions for each frame. This pipeline is of high precision, cost-effective, and scalable for further research. Based on it, we construct Motion-X, which comprises 13.7M precise 3D whole-body pose annotations (i.e., SMPL-X) covering 96K motion sequences from massive scenes. Besides, Motion-X provides 13.7M frame-level whole-body pose descriptions and 96K sequence-level semantic labels. Comprehensive experiments demonstrate the accuracy of the annotation pipeline and the significant benefit of Motion-X in enhancing expressive, diverse, and natural motion generation, as well as 3D whole-body human mesh recovery.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍Motion-X，一个大规模的3D表达全身动作数据集。现有的动作数据集主要包含body只的姿势，缺乏面部表达、手势和细化pose描述。此外，它们主要从有限的实验室场景中收集，手动编注文本描述，这会很大限制其扩展性。为了超越这些限制，我们开发了一个整体动作和文本注释管线，可以自动从单个或多视图视频中提取动作，并提供每个视频的全面的semantic标签和每帧的细化全身姿势描述。这个管线具有高精度、成本效果和可扩展性，适用于进一步的研究。基于它，我们建立了Motion-X，包含1370万精度3D全身姿势注释（即SMPL-X），覆盖96000个动作序列。此外，Motion-X还提供1370万帧级全身姿势描述和96000个序列级semantic标签。实验证明注释管线的准确性和Motion-X的极大地改进了表达、多样化和自然的动作生成，以及3D全身人体模型的恢复。
</details></li>
</ul>
<hr>
<h2 id="SketchMetaFace-A-Learning-based-Sketching-Interface-for-High-fidelity-3D-Character-Face-Modeling"><a href="#SketchMetaFace-A-Learning-based-Sketching-Interface-for-High-fidelity-3D-Character-Face-Modeling" class="headerlink" title="SketchMetaFace: A Learning-based Sketching Interface for High-fidelity 3D Character Face Modeling"></a>SketchMetaFace: A Learning-based Sketching Interface for High-fidelity 3D Character Face Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00804">http://arxiv.org/abs/2307.00804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongjin Luo, Dong Du, Heming Zhu, Yizhou Yu, Hongbo Fu, Xiaoguang Han</li>
<li>for: 用于 amateur 用户快速创建高级化 3D 人物面孔</li>
<li>methods: 采用 curvature-aware 笔划支持面孔细节控制，以及“几何和深度导航 mesh 模型”（IDGMM）算法，并提供了粗略到细节 2D 笔划界面设计和数据驱动的笔划建议工具</li>
<li>results: 在用户研究中证明了我们的系统比既有模型工具更加容易使用，同时提供了高质量的结果Here’s a breakdown of each point:</li>
<li>for: The paper is written for amateur users who want to quickly create high-fidelity 3D avatars with realistic faces.</li>
<li>methods: The system uses curvature-aware strokes to support the controllability of carving facial details, and a novel learning-based method called “Implicit and Depth Guided Mesh Modeling” (IDGMM) to achieve high-quality results with high efficiency. Additionally, the system provides a coarse-to-fine 2D sketching interface design and a data-driven stroke suggestion tool to support usability.</li>
<li>results: User studies demonstrate the superiority of the system over existing modeling tools in terms of ease of use and visual quality of results, and experimental analyses show that IDGMM reaches a better trade-off between accuracy and efficiency.<details>
<summary>Abstract</summary>
Modeling 3D avatars benefits various application scenarios such as AR/VR, gaming, and filming. Character faces contribute significant diversity and vividity as a vital component of avatars. However, building 3D character face models usually requires a heavy workload with commercial tools, even for experienced artists. Various existing sketch-based tools fail to support amateurs in modeling diverse facial shapes and rich geometric details. In this paper, we present SketchMetaFace - a sketching system targeting amateur users to model high-fidelity 3D faces in minutes. We carefully design both the user interface and the underlying algorithm. First, curvature-aware strokes are adopted to better support the controllability of carving facial details. Second, considering the key problem of mapping a 2D sketch map to a 3D model, we develop a novel learning-based method termed "Implicit and Depth Guided Mesh Modeling" (IDGMM). It fuses the advantages of mesh, implicit, and depth representations to achieve high-quality results with high efficiency. In addition, to further support usability, we present a coarse-to-fine 2D sketching interface design and a data-driven stroke suggestion tool. User studies demonstrate the superiority of our system over existing modeling tools in terms of the ease to use and visual quality of results. Experimental analyses also show that IDGMM reaches a better trade-off between accuracy and efficiency. SketchMetaFace is available at https://zhongjinluo.github.io/SketchMetaFace/.
</details>
<details>
<summary>摘要</summary>
模型3D人物可以应用于AR/VR、游戏和电影等场景，人物脸部具有重要的多样性和生动性，是人物模型的重要组件。然而，建立3D人物脸部模型通常需要大量的工作量，即使有经验的艺术家也会面临困难。现有的素描工具无法支持新手用户模型多样的脸部形状和富有的几何特征。在本文中，我们提出了SketchMetaFace - 一个针对新手用户的素描系统，可以在几分钟内创建高质量的3D脸部模型。我们仔细设计了用户界面和下面的算法。首先，我们采用了 curvature-aware strokes，以更好地支持挑战脸部细节的绘制。其次，我们开发了一种新的学习基于方法，称为“凸型和深度引导的三角形模型”（IDGMM），它将mesh、凸型和深度表示结合起来，以实现高质量和高效的结果。此外，为了进一步支持使用性，我们提出了一种粗略到细节的2D素描界面设计和数据驱动的笔划建议工具。用户研究表明，我们的系统比现有的模型工具更容易使用，并且可以生成高质量的结果。实验分析也显示，IDGMM达到了更好的精度和效率的平衡。SketchMetaFace可以在https://zhongjinluo.github.io/SketchMetaFace/上下载。
</details></li>
</ul>
<hr>
<h2 id="ACDMSR-Accelerated-Conditional-Diffusion-Models-for-Single-Image-Super-Resolution"><a href="#ACDMSR-Accelerated-Conditional-Diffusion-Models-for-Single-Image-Super-Resolution" class="headerlink" title="ACDMSR: Accelerated Conditional Diffusion Models for Single Image Super-Resolution"></a>ACDMSR: Accelerated Conditional Diffusion Models for Single Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00781">http://arxiv.org/abs/2307.00781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Axi Niu, Pham Xuan Trung, Kang Zhang, Jinqiu Sun, Yu Zhu, In So Kweon, Yanning Zhang</li>
<li>for: 这个论文主要针对图像增Resolution（SR）领域的图像-图像翻译问题。</li>
<li>methods: 该论文提出了一种基于扩散模型的图像SR方法，通过 deterministic iterative denoising 进程来实现SR。</li>
<li>results: 该方法在多个 benchmark 数据集上进行了广泛的实验，并证明了与之前的尝试相比，它可以提供更高的图像质量和更快的执行速度。<details>
<summary>Abstract</summary>
Diffusion models have gained significant popularity in the field of image-to-image translation. Previous efforts applying diffusion models to image super-resolution (SR) have demonstrated that iteratively refining pure Gaussian noise using a U-Net architecture trained on denoising at various noise levels can yield satisfactory high-resolution images from low-resolution inputs. However, this iterative refinement process comes with the drawback of low inference speed, which strongly limits its applications. To speed up inference and further enhance the performance, our research revisits diffusion models in image super-resolution and proposes a straightforward yet significant diffusion model-based super-resolution method called ACDMSR (accelerated conditional diffusion model for image super-resolution). Specifically, our method adapts the standard diffusion model to perform super-resolution through a deterministic iterative denoising process. Our study also highlights the effectiveness of using a pre-trained SR model to provide the conditional image of the given low-resolution (LR) image to achieve superior high-resolution results. We demonstrate that our method surpasses previous attempts in qualitative and quantitative results through extensive experiments conducted on benchmark datasets such as Set5, Set14, Urban100, BSD100, and Manga109. Moreover, our approach generates more visually realistic counterparts for low-resolution images, emphasizing its effectiveness in practical scenarios.
</details>
<details>
<summary>摘要</summary>
Diffusion models 在图像到图像翻译领域得到了广泛的应用。先前的尝试使用 diffusion models 进行图像超分辨（SR）已经证明了，通过iteratively refining 纯 Gaussian noise 使用 U-Net 架构在不同噪声水平上训练的方法可以获得高质量的高分辨图像。然而，这种迭代纯化过程受到了推理速度的限制，妨碍了其应用。为了加速推理和进一步提高性能，我们的研究重新审视了 diffusion models 在图像 SR 领域，并提出了一种简单又有力的 diffusion model-based SR 方法 called ACDMSR (加速 conditional diffusion model for image super-resolution)。我们的方法改变了标准的 diffusion model，通过 deterministic iterative denoising 进行 SR。我们的研究还发现，使用预训练 SR 模型提供给 low-resolution（LR）图像的conditional image可以实现更高的高分辨结果。我们通过对 Set5、Set14、Urban100、BSD100 和 Manga109 等标准数据集进行了广泛的实验，证明了我们的方法超过了之前的尝试， both qualitatively and quantitatively。此外，我们的方法可以生成更加视觉真实的 low-resolution 图像的高分辨对应体，强调其在实际应用中的效iveness。
</details></li>
</ul>
<hr>
<h2 id="Learning-Noise-Resistant-Image-Representation-by-Aligning-Clean-and-Noisy-Domains"><a href="#Learning-Noise-Resistant-Image-Representation-by-Aligning-Clean-and-Noisy-Domains" class="headerlink" title="Learning Noise-Resistant Image Representation by Aligning Clean and Noisy Domains"></a>Learning Noise-Resistant Image Representation by Aligning Clean and Noisy Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00761">http://arxiv.org/abs/2307.00761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanhui Guo, Xiaolin Wu, Fangzhou Luo</li>
<li>for: 提高图像 Representation  robustness 对于噪音</li>
<li>methods: 使用 dual domain 映射和target-guided 匿名函数</li>
<li>results: 可以在复杂噪音图像上达到 Remarkable 性能和Robustness<details>
<summary>Abstract</summary>
Recent supervised and unsupervised image representation learning algorithms have achieved quantum leaps. However, these techniques do not account for representation resilience against noise in their design paradigms. Consequently, these effective methods suffer failure when confronted with noise outside the training distribution, such as complicated real-world noise that is usually opaque to model training. To address this issue, dual domains are optimized to separately model a canonical space for noisy representations, namely the Noise-Robust (NR) domain, and a twinned canonical clean space, namely the Noise-Free (NF) domain, by maximizing the interaction information between the representations. Given the dual canonical domains, we design a target-guided implicit neural mapping function to accurately translate the NR representations to the NF domain, yielding noise-resistant representations by eliminating noise regencies. The proposed method is a scalable module that can be readily integrated into existing learning systems to improve their robustness against noise. Comprehensive trials of various tasks using both synthetic and real-world noisy data demonstrate that the proposed Target-Guided Dual-Domain Translation (TDDT) method is able to achieve remarkable performance and robustness in the face of complex noisy images.
</details>
<details>
<summary>摘要</summary>
最近的supervised和Unsupervised图像表示学习算法已经取得了量子跳跃。然而，这些技术不会考虑对噪声的表现鲁棒性在设计思路中。因此，这些有效的方法在面对训练数据外的噪声时会失败，如复杂的真实世界噪声，这些噪声通常是模型训练中透明的。为解决这个问题，我们分别优化了两个域，即Noise-Robust（NR）域和Noise-Free（NF）域，以便分别模型噪声表示的 canonical 空间。然后，我们设计了一个目标导向的隐藏神经映射函数，以准确地将NR表示翻译到NF域中，从而消除噪声残留。我们提出的方法可以轻松地integrated into现有的学习系统，以提高它们对噪声的抗性。我们在多个任务上使用both synthetic和真实世界噪声数据进行了全面的试验，并证明了我们的Target-Guided Dual-Domain Translation（TDDT）方法在面对复杂噪声图像时能够实现出色的性能和抗性。
</details></li>
</ul>
<hr>
<h2 id="Structured-Network-Pruning-by-Measuring-Filter-wise-Interactions"><a href="#Structured-Network-Pruning-by-Measuring-Filter-wise-Interactions" class="headerlink" title="Structured Network Pruning by Measuring Filter-wise Interactions"></a>Structured Network Pruning by Measuring Filter-wise Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00758">http://arxiv.org/abs/2307.00758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenting Tang, Xingxing Wei, Bo Li</li>
<li>for: 降低深度学习模型的计算成本，以便在实际应用中实现快速的模型训练和推理。</li>
<li>methods:  integrate filter-wise interaction into redundancy criterion, propose structured network pruning approach SNPFI, automatically assign proper sparsity and eliminate useless filters.</li>
<li>results: 对多种常用的深度学习模型（AlexNet、MobileNetv1、ResNet-50）和多种图像分类 dataset（MNIST、CIFAR-10、ImageNet）进行了实验，结果显示SNPFI可以减少网络计算量达60%以上，而模型的分类精度仍然保持在acceptable水平。<details>
<summary>Abstract</summary>
Structured network pruning is a practical approach to reduce computation cost directly while retaining the CNNs' generalization performance in real applications. However, identifying redundant filters is a core problem in structured network pruning, and current redundancy criteria only focus on individual filters' attributes. When pruning sparsity increases, these redundancy criteria are not effective or efficient enough. Since the filter-wise interaction also contributes to the CNN's prediction accuracy, we integrate the filter-wise interaction into the redundancy criterion. In our criterion, we introduce the filter importance and filter utilization strength to reflect the decision ability of individual and multiple filters. Utilizing this new redundancy criterion, we propose a structured network pruning approach SNPFI (Structured Network Pruning by measuring Filter-wise Interaction). During the pruning, the SNPFI can automatically assign the proper sparsity based on the filter utilization strength and eliminate the useless filters by filter importance. After the pruning, the SNPFI can recover pruned model's performance effectively without iterative training by minimizing the interaction difference. We empirically demonstrate the effectiveness of the SNPFI with several commonly used CNN models, including AlexNet, MobileNetv1, and ResNet-50, on various image classification datasets, including MNIST, CIFAR-10, and ImageNet. For all experimental CNN models, nearly 60% of computation is reduced in a network compression while the classification accuracy remains.
</details>
<details>
<summary>摘要</summary>
《结构化网络剔除》是一种实用的方法，可以直接降低计算成本，同时保持深度学习网络的泛化性能。然而，识别重复的滤波器是结构化网络剔除的核心问题，现有的重复性标准只考虑个体滤波器的特性。随着剔除率增加，这些重复性标准不再有效率。因为滤波器之间的交互也对深度学习网络的预测精度产生影响，我们将滤波器之间的交互纳入重复性标准中。在我们的标准中，我们引入了滤波器重要性和滤波器利用强度，以反映个体和多个滤波器的决策能力。通过这种新的重复性标准，我们提出了结构化网络剔除方法（SNPFI）。在剔除过程中，SNPFI可以自动根据滤波器利用强度分配适当的稀缺，并将无用的滤波器 eliminate 。剔除后，SNPFI可以有效地恢复剔除后的模型性能，无需迭代训练，只需要对交互差异进行最小化。我们通过多种常用的深度学习模型，包括AlexNet、MobileNetv1和ResNet-50，在多个图像分类任务上进行了实验，包括MNIST、CIFAR-10和ImageNet。对所有的实验深度学习模型来说，SNPFI可以在网络压缩中降低大约60%的计算成本，而且分类精度保持不变。
</details></li>
</ul>
<hr>
<h2 id="Graph-level-Anomaly-Detection-via-Hierarchical-Memory-Networks"><a href="#Graph-level-Anomaly-Detection-via-Hierarchical-Memory-Networks" class="headerlink" title="Graph-level Anomaly Detection via Hierarchical Memory Networks"></a>Graph-level Anomaly Detection via Hierarchical Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00755">http://arxiv.org/abs/2307.00755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niuchx/himnet">https://github.com/niuchx/himnet</a></li>
<li>paper_authors: Chaoxi Niu, Guansong Pang, Ling Chen</li>
<li>for: 本研究旨在开发一种基于嵌入式嵌入式自适应神经网络的图级异常检测方法，以检测图中的异常结构和节点特征。</li>
<li>methods: 本方法使用图自编码器网络来学习层次结构嵌入，并分别学习节点级和图级常规模式。</li>
<li>results: 对16个真实的图据集进行了广泛的实验，并证明了本方法在检测本地异常图和全球异常图方面具有显著优势，同时也具有抗异常杂质特性。<details>
<summary>Abstract</summary>
Graph-level anomaly detection aims to identify abnormal graphs that exhibit deviant structures and node attributes compared to the majority in a graph set. One primary challenge is to learn normal patterns manifested in both fine-grained and holistic views of graphs for identifying graphs that are abnormal in part or in whole. To tackle this challenge, we propose a novel approach called Hierarchical Memory Networks (HimNet), which learns hierarchical memory modules -- node and graph memory modules -- via a graph autoencoder network architecture. The node-level memory module is trained to model fine-grained, internal graph interactions among nodes for detecting locally abnormal graphs, while the graph-level memory module is dedicated to the learning of holistic normal patterns for detecting globally abnormal graphs. The two modules are jointly optimized to detect both locally- and globally-anomalous graphs. Extensive empirical results on 16 real-world graph datasets from various domains show that i) HimNet significantly outperforms the state-of-art methods and ii) it is robust to anomaly contamination. Codes are available at: https://github.com/Niuchx/HimNet.
</details>
<details>
<summary>摘要</summary>
GRAPH-LEVEL ANOMALY DETECTION 目标是识别异常图，其表现出异常结构和节点属性比多数图在图集中异常。一个主要挑战是学习图集中的正常模式，包括细化和总体两种视图。为解决这个挑战，我们提出了一种新的方法called Hierarchical Memory Networks（HimNet）。这种方法通过图自动编码网络架构学习层次记忆模块——节点记忆模块和图记忆模块。节点级别记忆模块用于模型图内节点之间的细化交互，检测本地异常图，而图级别记忆模块专门学习总体正常模式，检测全局异常图。两个模块共同优化，可以检测本地和全局异常图。我们对16个真实世界图据集进行了广泛的实验，结果显示：i) HimNet在比较方法中显著超越，ii) 它对异常污染具有较好的抗性。codes可以在https://github.com/Niuchx/HimNet中找到。
</details></li>
</ul>
<hr>
<h2 id="LXL-LiDAR-Excluded-Lean-3D-Object-Detection-with-4D-Imaging-Radar-and-Camera-Fusion"><a href="#LXL-LiDAR-Excluded-Lean-3D-Object-Detection-with-4D-Imaging-Radar-and-Camera-Fusion" class="headerlink" title="LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion"></a>LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00724">http://arxiv.org/abs/2307.00724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyi Xiong, Jianan Liu, Tao Huang, Qing-Long Han, Yuxuan Xia, Bing Zhu</li>
<li>for: 本研究旨在提高自动驾驶摄像头和4D成像雷达融合的3D对象检测性能。</li>
<li>methods: 本研究使用了”深度基于抽象”和”雷达占用支持”等技术，并对比了不同的提升策略。</li>
<li>results: 实验结果表明，提出的方法在VoD和TJ4DRadSet数据集上显著超越了现有的3D对象检测方法，而且在不同的提升设定下进行比较性能表现最佳。<details>
<summary>Abstract</summary>
As an emerging technology and a relatively affordable device, the 4D imaging radar has already been confirmed effective in performing 3D object detection in autonomous driving. Nevertheless, the sparsity and noisiness of 4D radar point clouds hinder further performance improvement, and in-depth studies about its fusion with other modalities are lacking. On the other hand, most of the camera-based perception methods transform the extracted image perspective view features into the bird's-eye view geometrically via "depth-based splatting" proposed in Lift-Splat-Shoot (LSS), and some researchers exploit other modals such as LiDARs or ordinary automotive radars for enhancement. Recently, a few works have applied the "sampling" strategy for image view transformation, showing that it outperforms "splatting" even without image depth prediction. However, the potential of "sampling" is not fully unleashed. In this paper, we investigate the "sampling" view transformation strategy on the camera and 4D imaging radar fusion-based 3D object detection. In the proposed model, LXL, predicted image depth distribution maps and radar 3D occupancy grids are utilized to aid image view transformation, called "radar occupancy-assisted depth-based sampling". Experiments on VoD and TJ4DRadSet datasets show that the proposed method outperforms existing 3D object detection methods by a significant margin without bells and whistles. Ablation studies demonstrate that our method performs the best among different enhancement settings.
</details>
<details>
<summary>摘要</summary>
“4D射频激光作为新兴技术和相对较便宜的设备，已经确认了其在自动驾驶中的3D物体探测效果。然而，4D射频激光点云的罕见和噪音妨碍了更进一步的性能改善，而且与其他感知modalities的混合研究相对较少。相反，大多数基于摄像头的视觉感知方法将提取到的图像视角特征转换为鸟瞰视 geometrically via "depth-based splatting"提出的Lift-Splat-Shoot (LSS)，一些研究人员将其他modalities，如LiDAR或普通的汽车激光，用于增强。最近，一些工作已经应用了"sampling"策略来对图像视角进行变换，显示它比"splatting"更好，即使不需要图像深度预测。然而，"sampling"的潜力仍未被完全发掘。在本文中，我们 investigate "sampling" 视角变换策略在摄像头和4D射频激光融合基础的3D物体探测中。在我们的提案中，预测的图像深度分布图和4D射频激光3Doccupancy grid被用来帮助图像视角变换，称为"radar occupancy-assisted depth-based sampling"。实验结果显示，我们的方法在VoD和TJ4DRadSet datasets上具有明显的优势，而且ablation study显示我们的方法在不同的增强设定中表现最好。”
</details></li>
</ul>
<hr>
<h2 id="SSC3OD-Sparsely-Supervised-Collaborative-3D-Object-Detection-from-LiDAR-Point-Clouds"><a href="#SSC3OD-Sparsely-Supervised-Collaborative-3D-Object-Detection-from-LiDAR-Point-Clouds" class="headerlink" title="SSC3OD: Sparsely Supervised Collaborative 3D Object Detection from LiDAR Point Clouds"></a>SSC3OD: Sparsely Supervised Collaborative 3D Object Detection from LiDAR Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00717">http://arxiv.org/abs/2307.00717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yushan Han, Hui Zhang, Honglei Zhang, Yidong Li</li>
<li>for: 提高自动驾驶中多智能体间的协同检测效果，适用于具有限制的标注数据情况。</li>
<li>methods: 提出一种稀疑指标学习方法（SSC3OD），包括两个新组件：柱子基本的掩码自适应器（Pillar-MAE）和实例挖掘模块。</li>
<li>results: 对三个大规模数据集进行了广泛的实验，结果表明，提出的SSC3OD方法可以有效地提高稀疑指标学习的协同3D物体检测效果。<details>
<summary>Abstract</summary>
Collaborative 3D object detection, with its improved interaction advantage among multiple agents, has been widely explored in autonomous driving. However, existing collaborative 3D object detectors in a fully supervised paradigm heavily rely on large-scale annotated 3D bounding boxes, which is labor-intensive and time-consuming. To tackle this issue, we propose a sparsely supervised collaborative 3D object detection framework SSC3OD, which only requires each agent to randomly label one object in the scene. Specifically, this model consists of two novel components, i.e., the pillar-based masked autoencoder (Pillar-MAE) and the instance mining module. The Pillar-MAE module aims to reason over high-level semantics in a self-supervised manner, and the instance mining module generates high-quality pseudo labels for collaborative detectors online. By introducing these simple yet effective mechanisms, the proposed SSC3OD can alleviate the adverse impacts of incomplete annotations. We generate sparse labels based on collaborative perception datasets to evaluate our method. Extensive experiments on three large-scale datasets reveal that our proposed SSC3OD can effectively improve the performance of sparsely supervised collaborative 3D object detectors.
</details>
<details>
<summary>摘要</summary>
共享3D对象检测已经广泛研究在自动驾驶领域，但现有的共享3D对象检测器在完全监督 paradigm中仅靠据大规模标注的3D包bounding box，这是劳动密集和时间消耗的。为解决这个问题，我们提出了一个稀疏监督的共享3D对象检测框架SSC3OD，它只需每个代理 randomly标注Scene中的一个对象。具体来说，该模型包括两个新的组件：即柱体基于的masked autoencoder（Pillar-MAE）和实例挖掘模块。Pillar-MAE模块 aspires to reason over high-level semantics in a self-supervised manner，而实例挖掘模块在线生成高质量的pseudo标签 для共享检测器。通过引入这些简单 yet effective mechanism，我们的提出的SSC3OD可以减轻不完整的标注的负面影响。我们基于共享感知 dataset generate sparse labels来评估我们的方法。我们在三个大规模dataset上进行了广泛的实验，结果显示，我们的提出的SSC3OD可以有效地提高稀疏监督的共享3D对象检测器的性能。
</details></li>
</ul>
<hr>
<h2 id="JourneyDB-A-Benchmark-for-Generative-Image-Understanding"><a href="#JourneyDB-A-Benchmark-for-Generative-Image-Understanding" class="headerlink" title="JourneyDB: A Benchmark for Generative Image Understanding"></a>JourneyDB: A Benchmark for Generative Image Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00716">http://arxiv.org/abs/2307.00716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Hongsheng Li</li>
<li>for: 本研究旨在探讨视语模型是否能够理解生成的图像。</li>
<li>methods: 我们提供了一个大规模的 dataset，名为 JourneyDB，用于多模态视觉理解生成图像。我们还设计了4个比较标准的 benchmark，用于衡量生成图像理解的内容和风格理解能力。</li>
<li>results: 我们通过对当前领域的状态级模型进行应用测试，发现它们在生成图像理解方面存在一定的局限性和缺陷。我们还进行了一个深入的分析，以帮助改进现有的多模态模型。<details>
<summary>Abstract</summary>
While recent advancements in vision-language models have revolutionized multi-modal understanding, it remains unclear whether they possess the capabilities of comprehending the generated images. Compared to real data, synthetic images exhibit a higher degree of diversity in both content and style, for which there are significant difficulties for the models to fully apprehend. To this end, we present a large-scale dataset, JourneyDB, for multi-modal visual understanding in generative images. Our curated dataset covers 4 million diverse and high-quality generated images paired with the text prompts used to produce them. We further design 4 benchmarks to quantify the performance of generated image understanding in terms of both content and style interpretation. These benchmarks include prompt inversion, style retrieval, image captioning and visual question answering. Lastly, we assess the performance of current state-of-the-art multi-modal models when applied to JourneyDB, and provide an in-depth analysis of their strengths and limitations in generated content understanding. We hope the proposed dataset and benchmarks will facilitate the research in the field of generative content understanding. The dataset will be available on https://journeydb.github.io.
</details>
<details>
<summary>摘要</summary>
“Recent advancements in vision-language models have significantly improved multi-modal understanding, but it remains unclear whether these models can fully comprehend generated images. Synthetic images exhibit a higher degree of diversity in both content and style, which poses challenges for the models to fully apprehend. To address this, we present JourneyDB, a large-scale dataset for multi-modal visual understanding in generative images. Our dataset includes 4 million high-quality and diverse generated images paired with text prompts used to produce them. We also design four benchmarks to evaluate the performance of generated image understanding in terms of content and style interpretation. These benchmarks include prompt inversion, style retrieval, image captioning, and visual question answering. Finally, we assess the performance of current state-of-the-art multi-modal models on JourneyDB and provide an in-depth analysis of their strengths and limitations in generated content understanding. We hope that the proposed dataset and benchmarks will facilitate research in the field of generative content understanding. The dataset will be available on <https://journeydb.github.io>.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Guided-Patch-Grouping-Wavelet-Transformer-with-Spatial-Congruence-for-Ultra-High-Resolution-Segmentation"><a href="#Guided-Patch-Grouping-Wavelet-Transformer-with-Spatial-Congruence-for-Ultra-High-Resolution-Segmentation" class="headerlink" title="Guided Patch-Grouping Wavelet Transformer with Spatial Congruence for Ultra-High Resolution Segmentation"></a>Guided Patch-Grouping Wavelet Transformer with Spatial Congruence for Ultra-High Resolution Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00711">http://arxiv.org/abs/2307.00711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deyi Ji, Feng Zhao, Hongtao Lu</li>
<li>for: 本文提出了一种新的ultra-high resolution（UHR）图像分割方法，以解决现有方法在内存成本和本地特征精度之间的矛盾。</li>
<li>methods: 该方法基于一种新的Transformer（$\mathcal{T}$）-Convolutional Neural Network（CNN，$\mathcal{C}$）共享学习框架，其中$\mathcal{T}$使用整个UHR图像作为输入，并提取了本地细节和长距离Contextual Dependencies。而$\mathcal{C}$使用下采样后的图像作为输入，以学习类别级别的深度Context。为了提高推理速度和计算复杂度，$\mathcal{T}$将原始UHR图像切分成patches，并将其分组动态，然后使用轻量级多头波峰 transformer（WFormer）网络来学习低级的本地细节。同时，这个过程中也捕捉了远距离的Contextual Dependencies，因为patches可以在空间领域中被分配到同一组。此外，$\mathcal{C}$生成的mask也被使用来导引patch grouping过程，提供了一种决策指南。此外，两个分支之间的协同约束也被利用，以保持patches之间的空间一致。总的来说，我们堆叠了多个阶段进程，形成一个pyramid结构。实验结果显示，GPWFormer在五个benchmark数据集上实现了显著的提升。<details>
<summary>Abstract</summary>
Most existing ultra-high resolution (UHR) segmentation methods always struggle in the dilemma of balancing memory cost and local characterization accuracy, which are both taken into account in our proposed Guided Patch-Grouping Wavelet Transformer (GPWFormer) that achieves impressive performances. In this work, GPWFormer is a Transformer ($\mathcal{T}$)-CNN ($\mathcal{C}$) mutual leaning framework, where $\mathcal{T}$ takes the whole UHR image as input and harvests both local details and fine-grained long-range contextual dependencies, while $\mathcal{C}$ takes downsampled image as input for learning the category-wise deep context. For the sake of high inference speed and low computation complexity, $\mathcal{T}$ partitions the original UHR image into patches and groups them dynamically, then learns the low-level local details with the lightweight multi-head Wavelet Transformer (WFormer) network. Meanwhile, the fine-grained long-range contextual dependencies are also captured during this process, since patches that are far away in the spatial domain can also be assigned to the same group. In addition, masks produced by $\mathcal{C}$ are utilized to guide the patch grouping process, providing a heuristics decision. Moreover, the congruence constraints between the two branches are also exploited to maintain the spatial consistency among the patches. Overall, we stack the multi-stage process in a pyramid way. Experiments show that GPWFormer outperforms the existing methods with significant improvements on five benchmark datasets.
</details>
<details>
<summary>摘要</summary>
现有的超高分辨率（UHR）分割方法都受到内存成本和地方特征精度之间的折衔，我们提出的指导 patch-组合波峰变换器（GPWFormer）可以达到出色的表现。在这项工作中，GPWFormer是一种基于Transformer（$\mathcal{T}$）和Convolutional Neural Network（$\mathcal{C}$）的互相学习框架，其中$\mathcal{T}$ 将整个UHR图像作为输入，并提取了地方细节和细腻的长距离Contextual Dependencies，而$\mathcal{C}$ 则将降采样后的图像作为输入，以学习类别深度的Context。为了提高推理速度和计算复杂度，$\mathcal{T}$ 将原始UHR图像 partitioned 成patches，并将其们组合成动态组。在这个过程中，GPWFormer网络学习了低级的地方细节，同时也捕捉了远距离的Contextual Dependencies，因为patches可以在空间领域中被分配到同一个组。此外，$\mathcal{C}$ 生成的mask也被利用来引导patch grouping процесс，提供了一个决策指标。此外，我们还利用了两支分支之间的协调约束，以保持patches中的空间一致。总的来说，我们将多个阶段过程堆叠在峰形式上。实验表明，GPWFormer在五个基准数据集上表现出了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Visual-Fault-Detection-for-Freight-Train-Braking-System-via-Heterogeneous-Self-Distillation-in-the-Wild"><a href="#Efficient-Visual-Fault-Detection-for-Freight-Train-Braking-System-via-Heterogeneous-Self-Distillation-in-the-Wild" class="headerlink" title="Efficient Visual Fault Detection for Freight Train Braking System via Heterogeneous Self Distillation in the Wild"></a>Efficient Visual Fault Detection for Freight Train Braking System via Heterogeneous Self Distillation in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00701">http://arxiv.org/abs/2307.00701</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MVME-HBUT/HSD-FTI-FDet">https://github.com/MVME-HBUT/HSD-FTI-FDet</a></li>
<li>paper_authors: Yang Zhang, Huilin Pan, Yang Zhou, Mingying Li, Guodong Sun</li>
<li>for: 这篇论文的目的是提高货物列车的快速瑕疵探测，以满足现实工程环境中的限制硬件需求。</li>
<li>methods: 这篇论文提出了一个异化自适应框架，以确保检测精度和速度，并满足低资源需求。这个框架使用了轻量级的背景来提取特征，并创建了一个新的异化知识颈。这个颈模型通过并行编码来优化特征提取能力，并使用通用分布来获得更可靠和准确的 bounding box 估计。</li>
<li>results: 实验结果显示，我们的框架可以在四个瑕疵Dataset上达到37帧&#x2F;秒的速度，并且与传统混合条件下的方法相比，具有更高的精度和更低的内存使用量。<details>
<summary>Abstract</summary>
Efficient visual fault detection of freight trains is a critical part of ensuring the safe operation of railways under the restricted hardware environment. Although deep learning-based approaches have excelled in object detection, the efficiency of freight train fault detection is still insufficient to apply in real-world engineering. This paper proposes a heterogeneous self-distillation framework to ensure detection accuracy and speed while satisfying low resource requirements. The privileged information in the output feature knowledge can be transferred from the teacher to the student model through distillation to boost performance. We first adopt a lightweight backbone to extract features and generate a new heterogeneous knowledge neck. Such neck models positional information and long-range dependencies among channels through parallel encoding to optimize feature extraction capabilities. Then, we utilize the general distribution to obtain more credible and accurate bounding box estimates. Finally, we employ a novel loss function that makes the network easily concentrate on values near the label to improve learning efficiency. Experiments on four fault datasets reveal that our framework can achieve over 37 frames per second and maintain the highest accuracy in comparison with traditional distillation approaches. Moreover, compared to state-of-the-art methods, our framework demonstrates more competitive performance with lower memory usage and the smallest model size.
</details>
<details>
<summary>摘要</summary>
高效的货运列车缺陷检测是铁路安全运行的关键部分，但现有的深度学习方法在实际工程环境中并不够高效。这篇论文提出了一种多元自适应框架，以确保检测精度和速度，同时满足资源限制。我们首先采用轻量级核心EXTRACT Features和生成新的多元知识脖子。这种脖子模型通过并行编码来提高特征提取能力，并使用通用分布来获取更可靠和准确的 bounding box 估计。最后，我们采用一种新的损失函数，使网络更容易集中于标签附近的值来提高学习效率。实验结果表明，我们的框架可以达到37帧/秒的速度，并与传统束Distillation方法相比，保持最高的准确率。此外，与当前状态艺术方法相比，我们的框架具有较低的内存使用量和最小的模型大小。
</details></li>
</ul>
<hr>
<h2 id="Camera-Calibration-from-a-Single-Imaged-Ellipsoid-A-Moon-Calibration-Algorithm"><a href="#Camera-Calibration-from-a-Single-Imaged-Ellipsoid-A-Moon-Calibration-Algorithm" class="headerlink" title="Camera Calibration from a Single Imaged Ellipsoid: A Moon Calibration Algorithm"></a>Camera Calibration from a Single Imaged Ellipsoid: A Moon Calibration Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00689">http://arxiv.org/abs/2307.00689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kalani R. Danas Rivera, Mason A. Peck</li>
<li>for: 这项研究旨在用星系中的扩展体 apply 到航天器摄像头准确设定。</li>
<li>methods: 该方法利用扩展体的图像，将其投射到圆柱体上，然后与观察者的target-relative状态进行结合，以实现基于单个非球体图像的摄像头准确设定。</li>
<li>results: 该算法可以在单个图像基础上计算摄像头的 focal length 和主点，并且可以在多张图像基础上减小一标准差的不确定性。<details>
<summary>Abstract</summary>
This work introduces a method that applies images of the extended bodies in the solar system to spacecraft camera calibration. The extended bodies consist of planets and moons that are well-modeled by triaxial ellipsoids. When imaged, the triaxial ellipsoid projects to a conic section which is generally an ellipse. This work combines the imaged ellipse with information on the observer's target-relative state to achieve camera calibration from a single imaged ellipsoid. As such, this work is the first to accomplish camera calibration from a single, non-spherical imaged ellipsoid. The camera calibration algorithm is applied to synthetic images of ellipsoids as well as planetary images of Saturn's moons as captured by the Cassini spacecraft. From a single image, the algorithm estimates the focal length and principal point of Cassini's Narrow Angle Camera within 1.0 mm and 10 pixels, respectively. With multiple images, the one standard deviation uncertainty in focal length and principal point estimates reduce to 0.5 mm and 3.1 pixels, respectively. Though created for spacecraft camera calibration in mind, this work also generalizes to terrestrial camera calibration using any number of imaged ellipsoids.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Proximal-Algorithm-for-Network-Slimming"><a href="#A-Proximal-Algorithm-for-Network-Slimming" class="headerlink" title="A Proximal Algorithm for Network Slimming"></a>A Proximal Algorithm for Network Slimming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00684">http://arxiv.org/abs/2307.00684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Bui, Fanghui Xue, Fredrick Park, Yingyong Qi, Jack Xin</li>
<li>for: 这 paper 是关于 channel pruning 方法的研究，用于减少 convolutional neural networks (CNNs) 的 Parameters 数量，以提高模型的计算效率和存储空间利用率。</li>
<li>methods: 这 paper 使用了 proximal network slimming (NS) 算法，该算法通过使用 Kurdyka-{\L}ojasiewicz 假设来确保 global convergence。 proximal NS 不需要选择 scaling factor threshold，也不需要 fine-tuning 束缚 CNNs。</li>
<li>results: 实验表明，使用 proximal NS 可以在一次训练中获得比较精炼的 CNN 模型，并且其减少后的精度与原始模型相似。 这 paper 在 VGGNet、DenseNet 和 ResNet 上进行了 CIFAR 10&#x2F;100 的实验 validate 了 proximal NS 的效果。<details>
<summary>Abstract</summary>
As a popular channel pruning method for convolutional neural networks (CNNs), network slimming (NS) has a three-stage process: (1) it trains a CNN with $\ell_1$ regularization applied to the scaling factors of the batch normalization layers; (2) it removes channels whose scaling factors are below a chosen threshold; and (3) it retrains the pruned model to recover the original accuracy. This time-consuming, three-step process is a result of using subgradient descent to train CNNs. Because subgradient descent does not exactly train CNNs towards sparse, accurate structures, the latter two steps are necessary. Moreover, subgradient descent does not have any convergence guarantee. Therefore, we develop an alternative algorithm called proximal NS. Our proposed algorithm trains CNNs towards sparse, accurate structures, so identifying a scaling factor threshold is unnecessary and fine tuning the pruned CNNs is optional. Using Kurdyka-{\L}ojasiewicz assumptions, we establish global convergence of proximal NS. Lastly, we validate the efficacy of the proposed algorithm on VGGNet, DenseNet and ResNet on CIFAR 10/100. Our experiments demonstrate that after one round of training, proximal NS yields a CNN with competitive accuracy and compression.
</details>
<details>
<summary>摘要</summary>
为了减少 convolutional neural networks (CNNs) 的复杂度，网络精简 (NS) 是一种广泛使用的频道剪除方法，它包括以下三个阶段：1. 使用 $\ell_1$ regularization 对 batch normalization layers 的扩大因子进行训练;2.  removing  channels  whose scaling factors 是下于选择的阈值;3.  retrains 剪除后的模型，以回升原始精度。这个时间消耗大，三步骤的过程是因为使用 subgradient descent 训练 CNNs。因为 subgradient descent 不是训练 CNNs 向稀疏、准确结构的算法，因此需要后两个步骤。而且，subgradient descent 没有任何收敛保证。因此，我们开发了一种替代算法 called proximal NS。我们的提议的算法 trains CNNs 向稀疏、准确结构，因此无需确定 scaling factor 阈值，并且 fine-tuning 剪除后的 CNNs 是可选的。使用 Kurdyka-{\L}ojasiewicz 假设，我们确立 proximal NS 的全球收敛性。最后，我们验证了我们的算法在 VGGNet、DenseNet 和 ResNet 上的 CIFAR 10/100 表现，实验表明，在一次训练后，proximal NS 可以生成一个与其他方法相当的精度和压缩率的 CNN。
</details></li>
</ul>
<hr>
<h2 id="Pay-Attention-to-the-Atlas-Atlas-Guided-Test-Time-Adaptation-Method-for-Robust-3D-Medical-Image-Segmentation"><a href="#Pay-Attention-to-the-Atlas-Atlas-Guided-Test-Time-Adaptation-Method-for-Robust-3D-Medical-Image-Segmentation" class="headerlink" title="Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for Robust 3D Medical Image Segmentation"></a>Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for Robust 3D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00676">http://arxiv.org/abs/2307.00676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingjie Guo, Weitong Zhang, Matthew Sinclair, Daniel Rueckert, Chen Chen</li>
<li>for: 提高3D医学成像 segmentation模型的Robustness，解决训练和测试数据分布不同导致的性能下降问题。</li>
<li>methods: 使用atlas-guided test-time adaptation（TTA）方法，只需要一个单例无标签测试样本，通过对网络进行注册并最小化atlas-based loss来适应测试数据。此外，还使用了通道和空间注意力块来提高适应性。</li>
<li>results: 在多个不同来源的数据集上进行了广泛的实验，结果表明AdaAtlas-Attention方法在提高3D医学成像 segmentation模型的Robustness方面表现出色，至少比其他竞争方法更好。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) often suffer from poor performance when tested on target data that differs from the training (source) data distribution, particularly in medical imaging applications where variations in imaging protocols across different clinical sites and scanners lead to different imaging appearances. However, re-accessing source training data for unsupervised domain adaptation or labeling additional test data for model fine-tuning can be difficult due to privacy issues and high labeling costs, respectively. To solve this problem, we propose a novel atlas-guided test-time adaptation (TTA) method for robust 3D medical image segmentation, called AdaAtlas. AdaAtlas only takes one single unlabeled test sample as input and adapts the segmentation network by minimizing an atlas-based loss. Specifically, the network is adapted so that its prediction after registration is aligned with the learned atlas in the atlas space, which helps to reduce anatomical segmentation errors at test time. In addition, different from most existing TTA methods which restrict the adaptation to batch normalization blocks in the segmentation network only, we further exploit the use of channel and spatial attention blocks for improved adaptability at test time. Extensive experiments on multiple datasets from different sites show that AdaAtlas with attention blocks adapted (AdaAtlas-Attention) achieves superior performance improvements, greatly outperforming other competitive TTA methods.
</details>
<details>
<summary>摘要</summary>
循环神经网络（CNN）在测试数据不同于训练数据分布时表现不佳，特别在医疗影像应用中，因为不同的临床站点和扫描仪器使用不同的扫描技术，导致影像的显示不同。然而，为了解决这个问题，我们提出了一种名为 AdaAtlas 的新的测试时适应（TTA）方法，它只需要一个无标签的测试样本作为输入，然后使用了一个名为 AdaAtlas 的扩展。在这个扩展中，我们将网络的预测结果与学习的床表空间中的 Atlas 进行了对齐，以降低测试时的解剖学 segmentation 错误。此外，与大多数现有的 TTA 方法不同，我们还在测试时使用了通道和空间注意力块，以提高适应性。经验表明，AdaAtlas-Attention 在多个数据集上表现出色，与其他竞争性 TTA 方法相比，具有显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Vision-based-Navigation-for-a-Robot-in-an-Indoor-Environment"><a href="#Real-time-Vision-based-Navigation-for-a-Robot-in-an-Indoor-Environment" class="headerlink" title="Real-time Vision-based Navigation for a Robot in an Indoor Environment"></a>Real-time Vision-based Navigation for a Robot in an Indoor Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00666">http://arxiv.org/abs/2307.00666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manglanisagar/vision-search-navigation">https://github.com/manglanisagar/vision-search-navigation</a></li>
<li>paper_authors: Sagar Manglani</li>
<li>for: 本研究目的是开发一个自主navigation系统，用于家庭环境中的自主 Navigation。</li>
<li>methods: 这个系统使用了视觉技术和进步的路径规划算法，帮助机器人通过避免障碍物来 navigate到目的地。</li>
<li>results: 研究结果显示了系统的表现，包括质量和量化的指标，展示了这个系统在实时自主 Navigation 中的潜力和局限性。<details>
<summary>Abstract</summary>
This paper presents a study on the development of an obstacle-avoidance navigation system for autonomous navigation in home environments. The system utilizes vision-based techniques and advanced path-planning algorithms to enable the robot to navigate toward the destination while avoiding obstacles. The performance of the system is evaluated through qualitative and quantitative metrics, highlighting its strengths and limitations. The findings contribute to the advancement of indoor robot navigation, showcasing the potential of vision-based techniques for real-time, autonomous navigation.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一项关于自主navigation在家庭环境中的障碍避免导航系统的研究。该系统利用视觉技术和高级路径规划算法，使机器人能够实时避免障碍物而前往目的地点。研究结果通过质量和量度指标进行评估，把系统的优点和局限性得到展示。发现有助于家庭环境内自主机器人导航的发展，同时也展示了视觉技术在实时自主导航中的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="CNN-BiLSTM-model-for-English-Handwriting-Recognition-Comprehensive-Evaluation-on-the-IAM-Dataset"><a href="#CNN-BiLSTM-model-for-English-Handwriting-Recognition-Comprehensive-Evaluation-on-the-IAM-Dataset" class="headerlink" title="CNN-BiLSTM model for English Handwriting Recognition: Comprehensive Evaluation on the IAM Dataset"></a>CNN-BiLSTM model for English Handwriting Recognition: Comprehensive Evaluation on the IAM Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00664">http://arxiv.org/abs/2307.00664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Firat Kizilirmak, Berrin Yanikoglu</li>
<li>for: 英文手写识别问题的 CNN-BiLSTM 系统，进行了大量的 IAM 数据集上的评估，包括模型大小、数据增强和词典的影响。</li>
<li>methods: 使用 CNN-BiLSTM 网络和 CTC 层进行模型建构，并应用测试时数据增强以提高Recognition of difficult cases。</li>
<li>results: 最佳模型可以达到 3.59% CER 和 9.44% WER，并通过对 IAM 数据集进行错误分析，显示了一些困难的手写图像和错误标注的样本。<details>
<summary>Abstract</summary>
We present a CNN-BiLSTM system for the problem of offline English handwriting recognition, with extensive evaluations on the public IAM dataset, including the effects of model size, data augmentation and the lexicon. Our best model achieves 3.59\% CER and 9.44\% WER using CNN-BiLSTM network with CTC layer. Test time augmentation with rotation and shear transformations applied to the input image, is proposed to increase recognition of difficult cases and found to reduce the word error rate by 2.5\% points. We also conduct an error analysis of our proposed method on IAM dataset, show hard cases of handwriting images and explore samples with erroneous labels. We provide our source code as public-domain, to foster further research to encourage scientific reproducibility.
</details>
<details>
<summary>摘要</summary>
我们提出了一个基于CNN-BiLSTM的英文手写识别系统，对公共IAM数据集进行了广泛的评估，包括模型大小、数据增强和词典的影响。我们的最佳模型在CNN-BiLSTM网络中使用CTC层时达到了3.59%的字幕误差率和9.44%的单词错误率。我们提议在输入图像上应用旋转和倾斜变换来增强难以识别的情况的识别，并发现这可以降低单词错误率2.5个百分点。我们还进行了我们的提议方法的错误分析在IAM数据集上，显示了困难的手写图像示例和错误标签的样本。我们将我们的源代码作为公共领域发布，以便进一步的研究，促进科学复现性。
</details></li>
</ul>
<hr>
<h2 id="More-Synergy-Less-Redundancy-Exploiting-Joint-Mutual-Information-for-Self-Supervised-Learning"><a href="#More-Synergy-Less-Redundancy-Exploiting-Joint-Mutual-Information-for-Self-Supervised-Learning" class="headerlink" title="More Synergy, Less Redundancy: Exploiting Joint Mutual Information for Self-Supervised Learning"></a>More Synergy, Less Redundancy: Exploiting Joint Mutual Information for Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00651">http://arxiv.org/abs/2307.00651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salman Mohamadi, Gianfranco Doretto, Donald A. Adjeroh</li>
<li>for: 这个论文的目的是调查对自然语言处理（NLP）任务进行自助学习（SSL）的影响，并研究如何使SSL模型更加可靠地利用数据分布信息。</li>
<li>methods: 这篇论文使用了一种新的多变量信息测量perspective，即partial information decomposition（PID），来检测SSL模型中的相互信息。PID可以将共同信息分解成三个重要组成部分：唯一信息、重复信息和合作信息。</li>
<li>results: 该研究的实验结果表明，通过最小化视图之间的重复信息并最大化目标表示和视图之间的合作信息，可以提高SSL模型的性能。此外，该研究还提出了一种新的SSL训练协议。多个数据集和两个下游任务的广泛实验结果证明了该框架的效果。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) is now a serious competitor for supervised learning, even though it does not require data annotation. Several baselines have attempted to make SSL models exploit information about data distribution, and less dependent on the augmentation effect. However, there is no clear consensus on whether maximizing or minimizing the mutual information between representations of augmentation views practically contribute to improvement or degradation in performance of SSL models. This paper is a fundamental work where, we investigate role of mutual information in SSL, and reformulate the problem of SSL in the context of a new perspective on mutual information. To this end, we consider joint mutual information from the perspective of partial information decomposition (PID) as a key step in \textbf{reliable multivariate information measurement}. PID enables us to decompose joint mutual information into three important components, namely, unique information, redundant information and synergistic information. Our framework aims for minimizing the redundant information between views and the desired target representation while maximizing the synergistic information at the same time. Our experiments lead to a re-calibration of two redundancy reduction baselines, and a proposal for a new SSL training protocol. Extensive experimental results on multiple datasets and two downstream tasks show the effectiveness of this framework.
</details>
<details>
<summary>摘要</summary>
自我监督学习（SSL）现在是指导学习的严重竞争对手，即使它不需要数据标注。许多基线已经尝试使 SSL 模型利用数据分布信息，并且减少扩展效应的依赖。然而，没有一致的共识，是否在 SSL 模型性能上实际上提高或下降的情况。这篇论文是一项基础性工作，我们调查了 SSL 中积分信息的角色，并将问题重新定义为基于新的积分信息角度。为此，我们使用部分信息分解（PID）来分解共同积分信息为三个重要组成部分：唯一信息、重复信息和协同信息。我们的框架的目标是减少视图之间的重复信息和目标表示之间的重复信息，同时增加协同信息。我们的实验导致了两种减少重复基elines的重新调整，以及一个新的 SSL 训练协议的建议。我们的实验结果表明，这个框架在多个数据集和两个下游任务上具有高效性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/03/cs.CV_2023_07_03/" data-id="clp88dbun00fmob88bbgb2ofd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/03/cs.AI_2023_07_03/" class="article-date">
  <time datetime="2023-07-03T12:00:00.000Z" itemprop="datePublished">2023-07-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/03/cs.AI_2023_07_03/">cs.AI - 2023-07-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MWPRanker-An-Expression-Similarity-Based-Math-Word-Problem-Retriever"><a href="#MWPRanker-An-Expression-Similarity-Based-Math-Word-Problem-Retriever" class="headerlink" title="MWPRanker: An Expression Similarity Based Math Word Problem Retriever"></a>MWPRanker: An Expression Similarity Based Math Word Problem Retriever</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01240">http://arxiv.org/abs/2307.01240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayank Goel, Venktesh V, Vikram Goyal</li>
<li>For: The paper aims to help test the mathematical reasoning capabilities of learners in online assessments by retrieving similar math word problems (MWPs) with the same problem model.* Methods: The authors propose a hybrid approach that combines natural language processing (NLP) and machine learning techniques to retrieve similar MWPs.* Results: The authors demonstrate that their tool is effective in retrieving similar MWPs and outperforms semantic similarity-based approaches, which fail to capture the arithmetic and logical sequence of the MWPs.Here is the same information in Simplified Chinese text:</li>
<li>for: 该论文目的是帮助在在线考试中测试学习者的数学逻辑能力，通过检索类似的数学问题（MWPs）。</li>
<li>methods: 作者们提出了一种混合的方法，将自然语言处理（NLP）和机器学习技术结合起来，以检索类似的MWPs。</li>
<li>results: 作者们证明了他们的工具能够有效地检索类似的MWPs，并超过 semantic similarity-based approaches，这些方法无法捕捉MWPs中的数学和逻辑序列。<details>
<summary>Abstract</summary>
Math Word Problems (MWPs) in online assessments help test the ability of the learner to make critical inferences by interpreting the linguistic information in them. To test the mathematical reasoning capabilities of the learners, sometimes the problem is rephrased or the thematic setting of the original MWP is changed. Since manual identification of MWPs with similar problem models is cumbersome, we propose a tool in this work for MWP retrieval. We propose a hybrid approach to retrieve similar MWPs with the same problem model. In our work, the problem model refers to the sequence of operations to be performed to arrive at the solution. We demonstrate that our tool is useful for the mentioned tasks and better than semantic similarity-based approaches, which fail to capture the arithmetic and logical sequence of the MWPs. A demo of the tool can be found at https://www.youtube.com/watch?v=gSQWP3chFIs
</details>
<details>
<summary>摘要</summary>
mathew word problems (MWPs) 在在线评估中帮助测试学习者能否作出重要的推理推论， interpret linguistic information中的信息。为测试学习者的数学推理能力，有时MWPs中的问题会被重新词表示或者改变主题设定。由于手动标识类似MWPs的问题模型是繁琐的，我们在这种工作中提出了一种工具。我们提议的方法是将问题模型看作解决问题所需的操作序列。我们示示了我们的工具对于以下任务都很有用，并且比 semantic similarity 基于的方法更好，因为它们无法捕捉MWPs中的数学和逻辑序列。您可以在 https://www.youtube.com/watch?v=gSQWP3chFIs 上找到我们的示例。
</details></li>
</ul>
<hr>
<h2 id="Automated-identification-and-quantification-of-myocardial-inflammatory-infiltration-in-digital-histological-images-to-diagnose-myocarditis"><a href="#Automated-identification-and-quantification-of-myocardial-inflammatory-infiltration-in-digital-histological-images-to-diagnose-myocarditis" class="headerlink" title="Automated identification and quantification of myocardial inflammatory infiltration in digital histological images to diagnose myocarditis"></a>Automated identification and quantification of myocardial inflammatory infiltration in digital histological images to diagnose myocarditis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01098">http://arxiv.org/abs/2307.01098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanyun Liu, Xiumeng Hua, Shouping Zhu, Congrui Wang, Xiao Chen, Yu Shi, Jiangping Song, Weihua Zhou</li>
<li>for: 这项研究的目的是开发一种新的计算生物学方法，用于自动识别和量化抗体染料抹布图像中的心脏免疫性浸泡，以提供心脏病变的量化 histological 诊断。</li>
<li>methods: 这项研究使用了深度学习（DL）基于的计算生物学方法，以识别抗体染料抹布图像中的核lei和抗体浸泡，并计算心脏免疫性浸泡的粒度（LND）。</li>
<li>results: 研究发现，使用这种新方法可以准确地识别和量化心脏免疫性浸泡，并提供了一个可靠的诊断标准。在五个横向分割试验中，该方法的准确率、敏感度和特异性分别为0.899±0.035、0.971±0.017和0.728±0.073，而在内测集上的准确率、敏感度和特异性分别为0.887、0.971和0.737。 externally tested set 的准确率、敏感度和特异性分别为0.853、0.846和0.858。<details>
<summary>Abstract</summary>
This study aims to develop a new computational pathology approach that automates the identification and quantification of myocardial inflammatory infiltration in digital HE-stained images to provide a quantitative histological diagnosis of myocarditis.898 HE-stained whole slide images (WSIs) of myocardium from 154 heart transplant patients diagnosed with myocarditis or dilated cardiomyopathy (DCM) were included in this study. An automated DL-based computational pathology approach was developed to identify nuclei and detect myocardial inflammatory infiltration, enabling the quantification of the lymphocyte nuclear density (LND) on myocardial WSIs. A cutoff value based on the quantification of LND was proposed to determine if the myocardial inflammatory infiltration was present. The performance of our approach was evaluated with a five-fold cross-validation experiment, tested with an internal test set from the myocarditis group, and confirmed by an external test from a double-blind trial group. An LND of 1.02/mm2 could distinguish WSIs with myocarditis from those without. The accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) in the five-fold cross-validation experiment were 0.899 plus or minus 0.035, 0.971 plus or minus 0.017, 0.728 plus or minus 0.073 and 0.849 plus or minus 0.044, respectively. For the internal test set, the accuracy, sensitivity, specificity, and AUC were 0.887, 0.971, 0.737, and 0.854, respectively. The accuracy, sensitivity, specificity, and AUC for the external test set reached 0.853, 0.846, 0.858, and 0.852, respectively. Our new approach provides accurate and reliable quantification of the LND of myocardial WSIs, facilitating automated quantitative diagnosis of myocarditis with HE-stained images.
</details>
<details>
<summary>摘要</summary>
898 HE-stained whole slide images (WSIs) of myocardium from 154 heart transplant patients diagnosed with myocarditis or dilated cardiomyopathy (DCM) were included in this study. An automated DL-based computational pathology approach was developed to identify nuclei and detect myocardial inflammatory infiltration, enabling the quantification of the lymphocyte nuclear density (LND) on myocardial WSIs. A cutoff value based on the quantification of LND was proposed to determine if the myocardial inflammatory infiltration was present.The performance of our approach was evaluated with a five-fold cross-validation experiment, tested with an internal test set from the myocarditis group, and confirmed by an external test from a double-blind trial group. An LND of 1.02/mm2 could distinguish WSIs with myocarditis from those without.The accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) in the five-fold cross-validation experiment were 0.899 ± 0.035, 0.971 ± 0.017, 0.728 ± 0.073, and 0.849 ± 0.044, respectively. For the internal test set, the accuracy, sensitivity, specificity, and AUC were 0.887, 0.971, 0.737, and 0.854, respectively. The accuracy, sensitivity, specificity, and AUC for the external test set reached 0.853, 0.846, 0.858, and 0.852, respectively.Our new approach provides accurate and reliable quantification of the LND of myocardial WSIs, facilitating automated quantitative diagnosis of myocarditis with HE-stained images.
</details></li>
</ul>
<hr>
<h2 id="Some-challenges-of-calibrating-differentiable-agent-based-models"><a href="#Some-challenges-of-calibrating-differentiable-agent-based-models" class="headerlink" title="Some challenges of calibrating differentiable agent-based models"></a>Some challenges of calibrating differentiable agent-based models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01085">http://arxiv.org/abs/2307.01085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnau Quera-Bofarull, Joel Dyer, Anisoara Calinescu, Michael Wooldridge</li>
<li>for: 模拟复杂系统的方法</li>
<li>methods: 使用梯度可微的ABM，并对其进行参数推理和优化</li>
<li>results: 发现了一些挑战，以及可能的解决方案<details>
<summary>Abstract</summary>
Agent-based models (ABMs) are a promising approach to modelling and reasoning about complex systems, yet their application in practice is impeded by their complexity, discrete nature, and the difficulty of performing parameter inference and optimisation tasks. This in turn has sparked interest in the construction of differentiable ABMs as a strategy for combatting these difficulties, yet a number of challenges remain. In this paper, we discuss and present experiments that highlight some of these challenges, along with potential solutions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-ROAD-to-discovery-machine-learning-driven-anomaly-detection-in-radio-astronomy-spectrograms"><a href="#The-ROAD-to-discovery-machine-learning-driven-anomaly-detection-in-radio-astronomy-spectrograms" class="headerlink" title="The ROAD to discovery: machine learning-driven anomaly detection in radio astronomy spectrograms"></a>The ROAD to discovery: machine learning-driven anomaly detection in radio astronomy spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01054">http://arxiv.org/abs/2307.01054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mesarcik/road">https://github.com/mesarcik/road</a></li>
<li>paper_authors: Michael Mesarcik, Albert-Jan Boonstra, Marco Iacobelli, Elena Ranguelova, Cees de Laat, Rob van Nieuwpoort</li>
<li>for: 本研究旨在提供一种自适应机器学习异常检测框架，以检测LOFAR望远镜中的异常现象，包括通常出现的异常和未经见过的罕见异常。</li>
<li>methods: 本研究使用了一种新的自我监睹学习（SSL）方法，该方法利用了上下文预测和重建损失来学习LOFAR望远镜的正常行为。</li>
<li>results: 研究结果表明，ROAD框架可以实时处理LOFAR数据处理管道中的数据，需要 &lt;1ms 处理一个spectrogram，并且具有异常检测 F-2 分数0.92，false positive rate约2%，以及每类分类 F-2 分数0.89，超过其他相关研究。<details>
<summary>Abstract</summary>
As radio telescopes increase in sensitivity and flexibility, so do their complexity and data-rates. For this reason automated system health management approaches are becoming increasingly critical to ensure nominal telescope operations. We propose a new machine learning anomaly detection framework for classifying both commonly occurring anomalies in radio telescopes as well as detecting unknown rare anomalies that the system has potentially not yet seen. To evaluate our method, we present a dataset consisting of 7050 autocorrelation-based spectrograms from the Low Frequency Array (LOFAR) telescope and assign 10 different labels relating to the system-wide anomalies from the perspective of telescope operators. This includes electronic failures, miscalibration, solar storms, network and compute hardware errors among many more. We demonstrate how a novel Self Supervised Learning (SSL) paradigm, that utilises both context prediction and reconstruction losses, is effective in learning normal behaviour of the LOFAR telescope. We present the Radio Observatory Anomaly Detector (ROAD), a framework that combines both SSL-based anomaly detection and a supervised classification, thereby enabling both classification of both commonly occurring anomalies and detection of unseen anomalies. We demonstrate that our system is real-time in the context of the LOFAR data processing pipeline, requiring <1ms to process a single spectrogram. Furthermore, ROAD obtains an anomaly detection F-2 score of 0.92 while maintaining a false positive rate of ~2\%, as well as a mean per-class classification F-2 score 0.89, outperforming other related works.
</details>
<details>
<summary>摘要</summary>
Radio telescopes 的敏感度和灵活性在不断提高，但是同时也会增加复杂性和数据流量。为了保证望远镜的正常运行，自动化系统健康管理方法已成为非常重要。我们提出了一种基于机器学习的异常检测框架，可以检测望远镜中常见的异常情况以及未经见过的未知异常。为评估我们的方法，我们提供了一个包含7050个自相关спектрограм的LOFAR望远镜数据集，并将望远镜系统异常情况分为10个不同的标签，包括电子故障、误准、太阳风暴、网络和计算机硬件错误等。我们展示了一种新的自我超vision学习（SSL）方法，通过 Context prediction和重建损失来学习望远镜的正常行为。我们称之为Radio Observatory Anomaly Detector（ROAD），它将SSL-based异常检测和超级vised分类相结合，以实现对望远镜系统的异常检测和分类。我们证明了ROAD在LOFAR数据处理管道中的实时性，需要<1ms处理一个spectrogram，并且ROAD在异常检测F-2分数0.92，False Positive率~2%，以及每个类别的平均异常检测F-2分数0.89，超过其他相关的工作。
</details></li>
</ul>
<hr>
<h2 id="ENGAGE-Explanation-Guided-Data-Augmentation-for-Graph-Representation-Learning"><a href="#ENGAGE-Explanation-Guided-Data-Augmentation-for-Graph-Representation-Learning" class="headerlink" title="ENGAGE: Explanation Guided Data Augmentation for Graph Representation Learning"></a>ENGAGE: Explanation Guided Data Augmentation for Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01053">http://arxiv.org/abs/2307.01053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sycny/engage">https://github.com/sycny/engage</a></li>
<li>paper_authors: Yucheng Shi, Kaixiong Zhou, Ninghao Liu</li>
<li>for: 本研究旨在提高对图数据的表示学习效果，通过指导对异常推理的增强数据变换。</li>
<li>methods: 本方法使用了解释导向的对异常推理，并设计了两种数据变换方案，一是对结构信息的干扰，二是对特征信息的干扰。</li>
<li>results: 实验表明，ENGAGE可以在多种模型架构和真实图数据上实现高效的表示学习，并且可以适应不同的图数据。<details>
<summary>Abstract</summary>
The recent contrastive learning methods, due to their effectiveness in representation learning, have been widely applied to modeling graph data. Random perturbation is widely used to build contrastive views for graph data, which however, could accidentally break graph structures and lead to suboptimal performance. In addition, graph data is usually highly abstract, so it is hard to extract intuitive meanings and design more informed augmentation schemes. Effective representations should preserve key characteristics in data and abandon superfluous information. In this paper, we propose ENGAGE (ExplaNation Guided data AuGmEntation), where explanation guides the contrastive augmentation process to preserve the key parts in graphs and explore removing superfluous information. Specifically, we design an efficient unsupervised explanation method called smoothed activation map as the indicator of node importance in representation learning. Then, we design two data augmentation schemes on graphs for perturbing structural and feature information, respectively. We also provide justification for the proposed method in the framework of information theories. Experiments of both graph-level and node-level tasks, on various model architectures and on different real-world graphs, are conducted to demonstrate the effectiveness and flexibility of ENGAGE. The code of ENGAGE can be found: https://github.com/sycny/ENGAGE.
</details>
<details>
<summary>摘要</summary>
Recent contrastive learning methods have been widely applied to modeling graph data due to their effectiveness in representation learning. However, random perturbation, which is commonly used to build contrastive views for graph data, can accidentally break graph structures and lead to suboptimal performance. Moreover, graph data is often highly abstract, making it difficult to extract intuitive meanings and design more informed augmentation schemes. Effective representations should preserve key characteristics in the data and discard superfluous information.In this paper, we propose ENGAGE (ExplaNation Guided data AuGmEntation), which utilizes explanation to guide the contrastive augmentation process and preserve the key parts in graphs. Specifically, we design an efficient unsupervised explanation method called smoothed activation map as the indicator of node importance in representation learning. Additionally, we propose two data augmentation schemes on graphs for perturbing structural and feature information, respectively. We also provide justification for the proposed method in the framework of information theories.Experiments on both graph-level and node-level tasks, conducted on various model architectures and on different real-world graphs, demonstrate the effectiveness and flexibility of ENGAGE. The code of ENGAGE can be found at: <https://github.com/sycny/ENGAGE>.
</details></li>
</ul>
<hr>
<h2 id="Temporal-Graph-Benchmark-for-Machine-Learning-on-Temporal-Graphs"><a href="#Temporal-Graph-Benchmark-for-Machine-Learning-on-Temporal-Graphs" class="headerlink" title="Temporal Graph Benchmark for Machine Learning on Temporal Graphs"></a>Temporal Graph Benchmark for Machine Learning on Temporal Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01026">http://arxiv.org/abs/2307.01026</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shenyanghuang/tgb">https://github.com/shenyanghuang/tgb</a></li>
<li>paper_authors: Shenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu, Emanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, Reihaneh Rabbany</li>
<li>for: The paper is written for evaluating the performance of machine learning models on temporal graphs.</li>
<li>methods: The paper uses a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation of machine learning models on temporal graphs.</li>
<li>results: The paper finds that the performance of common models can vary drastically across datasets, and simple methods often achieve superior performance compared to existing temporal graph models.Here is the same information in Simplified Chinese text:</li>
<li>for: 本文是用来评估机器学习模型在时间图上的性能的。</li>
<li>methods: 本文使用了一个多样化和真实的时间图 benchmark 集合，用于实现机器学习模型的真实、可重复和鲁棒评估。</li>
<li>results: 本文发现，常见模型在不同的 dataset 上的性能可以差异很大，而简单的方法经常在现有的时间图模型上表现更优异。<details>
<summary>Abstract</summary>
We present the Temporal Graph Benchmark (TGB), a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation of machine learning models on temporal graphs. TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks. For both tasks, we design evaluation protocols based on realistic use-cases. We extensively benchmark each dataset and find that the performance of common models can vary drastically across datasets. In addition, on dynamic node property prediction tasks, we show that simple methods often achieve superior performance compared to existing temporal graph models. We believe that these findings open up opportunities for future research on temporal graphs. Finally, TGB provides an automated machine learning pipeline for reproducible and accessible temporal graph research, including data loading, experiment setup and performance evaluation. TGB will be maintained and updated on a regular basis and welcomes community feedback. TGB datasets, data loaders, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.complexdatalab.com/ .
</details>
<details>
<summary>摘要</summary>
我们介绍Temporal Graph Benchmark（TGB），一个包含具有具有真实、可重现和可靠性评估的机器学习模型的大规模、多样化和长时间 duration 的数据集集合。TGB 数据集覆盖了社交、贸易、交易和交通网络等多种领域，并包括节点和边级别预测任务。为了进行真实的评估，我们设计了基于实际用例的评估协议。我们对每个数据集进行了广泛的 benchmarking，发现了不同数据集下模型的性能可以有很大差异。此外，在动态节点属性预测任务上，我们发现了简单的方法经常超越了现有的时间图模型。我们认为这些发现开创了对时间图的未来研究的机会。此外，TGB 还提供了一个自动化的机器学习管道，包括数据加载、实验设置和性能评估。TGB 将在定期基础上维护和更新，欢迎社区反馈。TGB 数据集、数据加载器、示例代码、评估设置和排名是公共可用的，可以在 <https://tgb.complexdatalab.com/> 访问。
</details></li>
</ul>
<hr>
<h2 id="RefSAM-Efficiently-Adapting-Segmenting-Anything-Model-for-Referring-Video-Object-Segmentation"><a href="#RefSAM-Efficiently-Adapting-Segmenting-Anything-Model-for-Referring-Video-Object-Segmentation" class="headerlink" title="RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation"></a>RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00997">http://arxiv.org/abs/2307.00997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lancasterli/refsam">https://github.com/lancasterli/refsam</a></li>
<li>paper_authors: Yonglin Li, Jing Zhang, Xiao Teng, Long Lan</li>
<li>for: 这篇论文旨在探讨如何使用 Segment Anything Model (SAM) 进行视频对象分割 (RVOS)，并利用不同Modalities的多视图信息和不同时间框的successive frames来提高性能。</li>
<li>methods: 该论文提出了一种名为 RefSAM 的新模型，它是基于 SAM 模型，通过采用轻量级的 Cross-Modal MLP 将文本表达的 embedding 转换为稀疏和密集的 embedding，以便用于用户交互提示。然后，对语言和视觉特征进行效果匹配和融合。</li>
<li>results: 经过了广泛的缺省研究和实验， authors 表明了 RefSAM 模型的实用和有效性，并在 Ref-Youtu-VOS 和 Ref-DAVIS17 数据集上达到了最高性能。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) has gained significant attention for its impressive performance in image segmentation. However, it lacks proficiency in referring video object segmentation (RVOS) due to the need for precise user-interactive prompts and limited understanding of different modalities, such as language and vision. This paper presents the RefSAM model, which for the first time explores the potential of SAM for RVOS by incorporating multi-view information from diverse modalities and successive frames at different timestamps. Our proposed approach adapts the original SAM model to enhance cross-modality learning by employing a lightweight Cross-Modal MLP that projects the text embedding of the referring expression into sparse and dense embeddings, serving as user-interactive prompts. Subsequently, a parameter-efficient tuning strategy is employed to effectively align and fuse the language and vision features. Through comprehensive ablation studies, we demonstrate the practical and effective design choices of our strategy. Extensive experiments conducted on Ref-Youtu-VOS and Ref-DAVIS17 datasets validate the superiority and effectiveness of our RefSAM model over existing methods. The code and models will be made publicly at \href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}.
</details>
<details>
<summary>摘要</summary>
《Segment Anything Model（SAM）》已经吸引了广泛关注，因为它在图像分割方面表现出了非常出色的表现。然而，它在视频对象分割（RVOS）方面缺乏能力，因为需要精准的用户交互提示和不同的modalities，如语言和视觉，的有限了理解。这篇论文提出了RefSAM模型，这是第一次将SAM模型应用于RVOS领域，通过在不同modalities和时间戳的多视图信息上进行协同学习。我们的提议的方法是对原始SAM模型进行改进，以增强对不同modalities的学习，通过使用轻量级的 Cross-Modal MLP 将文本表达的embedding进行映射，以用于用户交互提示。然后，我们采用了效果性的参数调整策略，以有效地对语言和视觉特征进行对齐和融合。通过了详细的ablation研究，我们证明了我们的方法的实用和有效性。广泛的实验在Ref-Youtu-VOS和Ref-DAVIS17 datasets上验证了我们的RefSAM模型的超越性和有效性。代码和模型将在 \href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM} 上公开。
</details></li>
</ul>
<hr>
<h2 id="REAL-A-Representative-Error-Driven-Approach-for-Active-Learning"><a href="#REAL-A-Representative-Error-Driven-Approach-for-Active-Learning" class="headerlink" title="REAL: A Representative Error-Driven Approach for Active Learning"></a>REAL: A Representative Error-Driven Approach for Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00968">http://arxiv.org/abs/2307.00968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/withchencheng/ecml_pkdd_23_real">https://github.com/withchencheng/ecml_pkdd_23_real</a></li>
<li>paper_authors: Cheng Chen, Yong Wang, Lizi Liao, Yueguo Chen, Xiaoyong Du</li>
<li>for: 本研究目的是提出一种基于活动学习的数据选择方法，以提高模型训练的精度和效率。</li>
<li>methods: 本方法基于uncertainty和多样性的度量来评估无标Pool中的实例信息丰富性，并通过强制采样这些实例来验证模型。</li>
<li>results: 对五种文本分类任务进行了广泛的实验，结果表明，与最佳基准相比，本方法在各种 гиперпараметры设置下都能够准确地预测模型性能和F1-macro分数。此外，分析还表明，本方法可以准确地选择最有代表性的 pseudo errors，这些 pseudo errors 与真实错误的分布相符。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training. To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity. However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance. To address this limitation, we propose $REAL$, a novel approach to select data instances with $\underline{R}$epresentative $\underline{E}$rrors for $\underline{A}$ctive $\underline{L}$earning. It identifies minority predictions as \emph{pseudo errors} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets demonstrate that $REAL$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of hyperparameter settings. Our analysis also shows that $REAL$ selects the most representative pseudo errors that match the distribution of ground-truth errors along the decision boundary. Our code is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real.
</details>
<details>
<summary>摘要</summary>
$REAL$ identifies minority predictions as "pseudo errors" within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets show that $REAL$ consistently outperforms all best-performing baselines in terms of accuracy and F1-macro scores across a wide range of hyperparameter settings. Our analysis also reveals that $REAL$ selects the most representative pseudo errors that match the distribution of ground-truth errors along the decision boundary.The code for $REAL$ is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real.
</details></li>
</ul>
<hr>
<h2 id="OpenClinicalAI-An-Open-and-Dynamic-Model-for-Alzheimer’s-Disease-Diagnosis"><a href="#OpenClinicalAI-An-Open-and-Dynamic-Model-for-Alzheimer’s-Disease-Diagnosis" class="headerlink" title="OpenClinicalAI: An Open and Dynamic Model for Alzheimer’s Disease Diagnosis"></a>OpenClinicalAI: An Open and Dynamic Model for Alzheimer’s Disease Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00965">http://arxiv.org/abs/2307.00965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunyou Huang, Xiaoshuang Liang, Xiangjiang Lu, Xiuxia Miao, Jiyue Xie, Wenjing Liu, Fan Zhang, Guoxin Kang, Li Ma, Suqin Tang, Zhifei Zhang, Jianfeng Zhan</li>
<li>for: 这个研究旨在提出一个可以在真实临床设定下运行的旁遮普适的认知痴生病诊断系统，以减少诊断和治疗成本。</li>
<li>methods: 这个研究使用了开放式且不确定的临床设定下的认知痴生病诊断模型，结合了相互coupled的深度多动作学习（DMARL）和多中心méta学习（MCML），以形成诊断策略和提供诊断结果。</li>
<li>results: 实验结果显示，这个方法可以在临床设定下提供更好的性能和 fewer 的诊断测试，并且可以与现有的医疗系统整合，以提高现有的医疗服务质量。<details>
<summary>Abstract</summary>
Although Alzheimer's disease (AD) cannot be reversed or cured, timely diagnosis can significantly reduce the burden of treatment and care. Current research on AD diagnosis models usually regards the diagnosis task as a typical classification task with two primary assumptions: 1) All target categories are known a priori; 2) The diagnostic strategy for each patient is consistent, that is, the number and type of model input data for each patient are the same. However, real-world clinical settings are open, with complexity and uncertainty in terms of both subjects and the resources of the medical institutions. This means that diagnostic models may encounter unseen disease categories and need to dynamically develop diagnostic strategies based on the subject's specific circumstances and available medical resources. Thus, the AD diagnosis task is tangled and coupled with the diagnosis strategy formulation. To promote the application of diagnostic systems in real-world clinical settings, we propose OpenClinicalAI for direct AD diagnosis in complex and uncertain clinical settings. This is the first powerful end-to-end model to dynamically formulate diagnostic strategies and provide diagnostic results based on the subject's conditions and available medical resources. OpenClinicalAI combines reciprocally coupled deep multiaction reinforcement learning (DMARL) for diagnostic strategy formulation and multicenter meta-learning (MCML) for open-set recognition. The experimental results show that OpenClinicalAI achieves better performance and fewer clinical examinations than the state-of-the-art model. Our method provides an opportunity to embed the AD diagnostic system into the current health care system to cooperate with clinicians to improve current health care.
</details>
<details>
<summary>摘要</summary>
although 阿尔茨heimer's disease (AD) cannot be reversed or cured, 在时间上的诊断可以有效减轻治疗和照料的负担。现有的AD诊断模型通常将诊断任务视为一个典型的分类任务，有两个基本假设：1) 所有目标类别都是先知的; 2) 每个患者的诊断策略都是一致的，即每个患者的模型输入数据的数量和类型都是一样的。然而，现实世界的医疗设施是开放的，有 Complexity和不确定性，这意味着诊断模型可能会遇到未知的疾病类别，需要在患者的特定情况和可用的医疗资源基础上动态发展诊断策略。因此，AD诊断任务和诊断策略的形成是相互关联的。为了推动AD诊断系统在现实世界医疗设施中的应用，我们提出了OpenClinicalAI，这是一个直接用于AD诊断的Complex and uncertain clinical settings中的强大终端模型。OpenClinicalAI通过reciprocally coupled deep multiaction reinforcement learning (DMARL) 和 multicenter meta-learning (MCML) 结合，可以动态形成诊断策略，并根据患者的情况和可用的医疗资源提供诊断结果。实验结果表明，OpenClinicalAI在比较 estado-of-the-art 模型的情况下表现出更好的性能，并需要 fewer clinical examinations。我们的方法提供了一个机会，以便将AD诊断系统integrated into the current healthcare system，与临床医生合作，提高现有的医疗服务。
</details></li>
</ul>
<hr>
<h2 id="A-Dual-Stealthy-Backdoor-From-Both-Spatial-and-Frequency-Perspectives"><a href="#A-Dual-Stealthy-Backdoor-From-Both-Spatial-and-Frequency-Perspectives" class="headerlink" title="A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives"></a>A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10184">http://arxiv.org/abs/2307.10184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yudong Gao, Honglong Chen, Peng Sun, Junjian Li, Anqing Zhang, Zhibo Wang</li>
<li>for: 这个论文旨在提出一种可靠、隐蔽的后门攻击方法，以便在深度神经网络（DNN）中植入后门。</li>
<li>methods: 该方法使用了Discrete Wavelet Transform和Fourier Transform等技术，在频域和空间域同时考虑后门Trigger的隐蔽性，以提高攻击性能和隐蔽性。此外，该方法还采用了一种新的攻击策略，通过训练模型使用弱Trigger并在攻击时使用强Trigger来进一步提高攻击性能和隐蔽性。</li>
<li>results: 在四个数据集上，DUBA方法比 estado-of-the-art 后门攻击方法显著提高了攻击成功率和隐蔽性。<details>
<summary>Abstract</summary>
Backdoor attacks pose serious security threats to deep neural networks (DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions on inputs embedded with well-designed triggers while behaving normally on clean inputs. Many works have explored the invisibility of backdoor triggers to improve attack stealthiness. However, most of them only consider the invisibility in the spatial domain without explicitly accounting for the generation of invisible triggers in the frequency domain, making the generated poisoned images be easily detected by recent defense methods. To address this issue, in this paper, we propose a DUal stealthy BAckdoor attack method named DUBA, which simultaneously considers the invisibility of triggers in both the spatial and frequency domains, to achieve desirable attack performance, while ensuring strong stealthiness. Specifically, we first use Discrete Wavelet Transform to embed the high-frequency information of the trigger image into the clean image to ensure attack effectiveness. Then, to attain strong stealthiness, we incorporate Fourier Transform and Discrete Cosine Transform to mix the poisoned image and clean image in the frequency domain. Moreover, the proposed DUBA adopts a novel attack strategy, in which the model is trained with weak triggers and attacked with strong triggers to further enhance the attack performance and stealthiness. We extensively evaluate DUBA against popular image classifiers on four datasets. The results demonstrate that it significantly outperforms the state-of-the-art backdoor attacks in terms of the attack success rate and stealthiness
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）面临着严重的安全威胁，这些威胁被称为“后门攻击”。后门攻击使得模型在特定的输入上进行不当的预测，而不会在干净的输入上产生错误。许多研究探讨了后门攻击的不可见性，以提高攻击者的隐蔽性。然而，大多数研究只考虑了空间频谱中的不可见性，而忽略了生成不可见的触发器在频谱频率中的生成，这使得生成的毒害图像容易被现代防御方法检测到。为解决这个问题，我们在这篇论文中提出了一种名为DUal stealthy BAckdoor attack（DUBA）的攻击方法，该方法同时考虑了空间和频谱频率两个频域中的触发器不可见性，以达到恰当的攻击性和隐蔽性。具体来说，我们首先使用抽象波лет变换将高频信息从触发图像中嵌入到干净图像中，以确保攻击的效果。然后，为了进一步提高攻击性和隐蔽性，我们在频谱频率中使用福洛 transform和离散快寄变换将毒害图像和干净图像混合。此外，我们的DUBA方法采用了一种新的攻击策略，在该策略中，模型首先在弱触发下训练，然后在强触发下进行攻击，以进一步提高攻击性和隐蔽性。我们对四个数据集进行了广泛的测试，结果表明，DUBA方法在攻击成功率和隐蔽性两个方面都有 significi
</details></li>
</ul>
<hr>
<h2 id="Challenges-in-Domain-Specific-Abstractive-Summarization-and-How-to-Overcome-them"><a href="#Challenges-in-Domain-Specific-Abstractive-Summarization-and-How-to-Overcome-them" class="headerlink" title="Challenges in Domain-Specific Abstractive Summarization and How to Overcome them"></a>Challenges in Domain-Specific Abstractive Summarization and How to Overcome them</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00963">http://arxiv.org/abs/2307.00963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anum Afzal, Juraj Vladika, Daniel Braun, Florian Matthes</li>
<li>for: 这个论文的目的是描述大语言模型在具体领域抽象文本摘要任务上的局限性。</li>
<li>methods: 该论文使用了许多现有的技术来解决这些研究问题，包括对 transformer 模型的 quadratic complexity 分析，模型妄想现象的检测和预测，以及域Shift 的识别和解决方法。</li>
<li>results: 该论文通过分析和评估现有的状态арト技术，揭示了域特定抽取文本摘要任务中大语言模型的三大局限性，并提出了一些开放的研究问题。<details>
<summary>Abstract</summary>
Large Language Models work quite well with general-purpose data and many tasks in Natural Language Processing. However, they show several limitations when used for a task such as domain-specific abstractive text summarization. This paper identifies three of those limitations as research problems in the context of abstractive text summarization: 1) Quadratic complexity of transformer-based models with respect to the input text length; 2) Model Hallucination, which is a model's ability to generate factually incorrect text; and 3) Domain Shift, which happens when the distribution of the model's training and test corpus is not the same. Along with a discussion of the open research questions, this paper also provides an assessment of existing state-of-the-art techniques relevant to domain-specific text summarization to address the research gaps.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>transformer-based models have quadratic complexity with respect to input text length2. model hallucination, which is the ability of the model to generate factually incorrect text3. domain shift, which occurs when the distribution of the model’s training and test corpus is not the sameIn addition to discussing open research questions, this paper also provides an assessment of existing state-of-the-art techniques relevant to domain-specific text summarization to address these research gaps.</details></li>
</ol>
<hr>
<h2 id="Neural-Architecture-Transfer-2-A-Paradigm-for-Improving-Efficiency-in-Multi-Objective-Neural-Architecture-Search"><a href="#Neural-Architecture-Transfer-2-A-Paradigm-for-Improving-Efficiency-in-Multi-Objective-Neural-Architecture-Search" class="headerlink" title="Neural Architecture Transfer 2: A Paradigm for Improving Efficiency in Multi-Objective Neural Architecture Search"></a>Neural Architecture Transfer 2: A Paradigm for Improving Efficiency in Multi-Objective Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00960">http://arxiv.org/abs/2307.00960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Sarti, Eugenio Lomurno, Matteo Matteucci</li>
<li>for: 本研究旨在提高Neural Architecture Search（NAS）技术的效率和计算资源利用率，以便在各种任务上建立高性能的人工神经网络模型。</li>
<li>methods: 本研究使用Once-For-All（OFA）和其改进版OFAv2技术，并开发了Neural Architecture Transfer（NAT）和NATv2技术来实现单个超网络模型中的子网络EXTRACTION。</li>
<li>results: 实验结果表明，NATv2可以成功地改进NAT，并在多目标搜索算法应用于动态超网络架构时提供更高效的EXTRACTION。此外，基于细化的训练pipeline也被引入，以提高网络的性能。<details>
<summary>Abstract</summary>
Deep learning is increasingly impacting various aspects of contemporary society. Artificial neural networks have emerged as the dominant models for solving an expanding range of tasks. The introduction of Neural Architecture Search (NAS) techniques, which enable the automatic design of task-optimal networks, has led to remarkable advances. However, the NAS process is typically associated with long execution times and significant computational resource requirements. Once-For-All (OFA) and its successor, Once-For-All-2 (OFAv2), have been developed to mitigate these challenges. While maintaining exceptional performance and eliminating the need for retraining, they aim to build a single super-network model capable of directly extracting sub-networks satisfying different constraints. Neural Architecture Transfer (NAT) was developed to maximise the effectiveness of extracting sub-networks from a super-network. In this paper, we present NATv2, an extension of NAT that improves multi-objective search algorithms applied to dynamic super-network architectures. NATv2 achieves qualitative improvements in the extractable sub-networks by exploiting the improved super-networks generated by OFAv2 and incorporating new policies for initialisation, pre-processing and updating its networks archive. In addition, a post-processing pipeline based on fine-tuning is introduced. Experimental results show that NATv2 successfully improves NAT and is highly recommended for investigating high-performance architectures with a minimal number of parameters.
</details>
<details>
<summary>摘要</summary>
深度学习在当代社会中越来越普遍，人工神经网络成为解决越来越多任务的主导模型。随着神经网络搜索（NAS）技术的出现，可以自动设计适合任务的网络模型，带来了非常出色的进步。然而，NAS过程通常具有较长的执行时间和较大的计算资源需求。“Once-For-All”（OFA）和其 successor “Once-For-All-2”（OFAv2）被开发以解决这些挑战。它们保持了极高的性能，并完全废除了重新训练的需要，旨在建立一个单独的超网络模型，可以直接提取满足不同约束的子网络。“Neural Architecture Transfer”（NAT）被开发以 Maximize the effectiveness of extracting sub-networks from a super-network。在这篇论文中，我们提出了NATv2，它是NAT的扩展，通过对动态超网络架构进行多目标搜索算法来提高提取子网络的效果。此外，我们还 introduce了一个基于练习的后处理管道。实验结果表明，NATv2成功地改进了NAT，并在具有最小参数数量的情况下提供了高性能的建议。
</details></li>
</ul>
<hr>
<h2 id="Learning-Difference-Equations-with-Structured-Grammatical-Evolution-for-Postprandial-Glycaemia-Prediction"><a href="#Learning-Difference-Equations-with-Structured-Grammatical-Evolution-for-Postprandial-Glycaemia-Prediction" class="headerlink" title="Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction"></a>Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01238">http://arxiv.org/abs/2307.01238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Parra, David Joedicke, J. Manuel Velasco, Gabriel Kronberger, J. Ignacio Hidalgo</li>
<li>For: This paper proposes a novel glucose prediction method that prioritizes interpretability for diabetes management.* Methods: The proposed method uses Interpretable Sparse Identification by Grammatical Evolution, combined with a previous clustering stage, to predict postprandial glucose levels up to two hours after meals.* Results: The method produces safe predictions with slightly better accuracy than other techniques, including sparse identification of non-linear dynamics and artificial neural networks. The results demonstrate that the proposed method provides interpretable solutions without sacrificing prediction accuracy, offering a promising approach to glucose prediction in diabetes management.Here’s the Chinese translation of the three key points:* For: 这篇论文提出了一种新的血糖预测方法，旨在帮助Diabetes管理。* Methods: 该方法使用可解释的简单identification by Grammatical Evolution，结合之前的分 clustering阶段，以预测餐后血糖水平。* Results: 该方法生成了安全的预测，与其他方法（包括非线性动力学的简单identification和人工神经网络）相比，有些微的更好的准确性。结果表明，该方法提供了可解释的解决方案，不会牺牲预测准确性，为血糖预测在Diabetes管理中提供了一个有前途的方法。<details>
<summary>Abstract</summary>
People with diabetes must carefully monitor their blood glucose levels, especially after eating. Blood glucose regulation requires a proper combination of food intake and insulin boluses. Glucose prediction is vital to avoid dangerous post-meal complications in treating individuals with diabetes. Although traditional methods, such as artificial neural networks, have shown high accuracy rates, sometimes they are not suitable for developing personalised treatments by physicians due to their lack of interpretability. In this study, we propose a novel glucose prediction method emphasising interpretability: Interpretable Sparse Identification by Grammatical Evolution. Combined with a previous clustering stage, our approach provides finite difference equations to predict postprandial glucose levels up to two hours after meals. We divide the dataset into four-hour segments and perform clustering based on blood glucose values for the twohour window before the meal. Prediction models are trained for each cluster for the two-hour windows after meals, allowing predictions in 15-minute steps, yielding up to eight predictions at different time horizons. Prediction safety was evaluated based on Parkes Error Grid regions. Our technique produces safe predictions through explainable expressions, avoiding zones D (0.2% average) and E (0%) and reducing predictions on zone C (6.2%). In addition, our proposal has slightly better accuracy than other techniques, including sparse identification of non-linear dynamics and artificial neural networks. The results demonstrate that our proposal provides interpretable solutions without sacrificing prediction accuracy, offering a promising approach to glucose prediction in diabetes management that balances accuracy, interpretability, and computational efficiency.
</details>
<details>
<summary>摘要</summary>
人们有糖尿病需要仔细监测血糖水平，特别是在吃过食物后。血糖规则需要合适的食物摄入和人工肥肽剂注射。预测血糖水平是糖尿病治疗中非常重要的一步，以避免危险的后食后合并症状。传统方法，如人工神经网络，已经显示高准确率，但有时不适合由医生开发个性化治疗，因为它们缺乏可解释性。在本研究中，我们提出了一种新的血糖预测方法，强调可解释性：可解释的稀缺特征标识。与之前的划分阶段结合，我们的方法提供了finite difference方程来预测餐后血糖水平，覆盖了两个小时后的餐后时段。我们将数据分成四小时段，并根据血糖值进行划分，对每个划分的两小时窗口进行预测。我们为每个划分训练预测模型，可以在15分钟步长预测，共计八次预测。预测安全性被评估基于帕克斯错误网格区域。我们的方法生成了安全的预测，避免了区域D（0.2%的平均值）和区域E（0%），同时减少了区域C（6.2%）。此外，我们的提案有些微比其他技术，包括稀缺特征标识非线性动力学和人工神经网络，更好的准确性。结果表明，我们的提案可以提供可解释的解决方案，不 sacrificing准确性，提供了糖尿病管理中准确性、可解释性和计算效率之间的平衡。
</details></li>
</ul>
<hr>
<h2 id="Towards-Explainable-AI-for-Channel-Estimation-in-Wireless-Communications"><a href="#Towards-Explainable-AI-for-Channel-Estimation-in-Wireless-Communications" class="headerlink" title="Towards Explainable AI for Channel Estimation in Wireless Communications"></a>Towards Explainable AI for Channel Estimation in Wireless Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00952">http://arxiv.org/abs/2307.00952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdul Karim Gizzini, Yahia Medjahdi, Ali J. Ghandour, Laurent Clavier</li>
<li>For: The paper is written to support the development of 6G networks and to provide explainable AI (XAI) techniques for critical applications such as autonomous driving.* Methods: The paper proposes a novel XAI-based channel estimation (XAI-CHEST) scheme that uses deep learning (DL) models to estimate the channel and provide detailed reasonable interpretability of the model behavior.* Results: The proposed XAI-CHEST scheme provides valid interpretations of the DL-based channel estimators for different scenarios, allowing for a better understanding of the decision-making behavior of the models.Here is the information in Simplified Chinese text:</li>
<li>for: 该文章是为支持6G网络的发展而写的，同时提供了可解释AI（XAI）技术，以支持 crítical应用程序such as自动驾驶。</li>
<li>methods: 文章提出了一种基于深度学习（DL）的XAI-基因频道估计（XAI-CHEST）方案，以提供channel估计和模型行为的详细可理解。</li>
<li>results: 实验结果表明，提出的XAI-CHEST方案在不同场景下都提供了有效的可理解。<details>
<summary>Abstract</summary>
Research into 6G networks has been initiated to support a variety of critical artificial intelligence (AI) assisted applications such as autonomous driving. In such applications, AI-based decisions should be performed in a real-time manner. These decisions include resource allocation, localization, channel estimation, etc. Considering the black-box nature of existing AI-based models, it is highly challenging to understand and trust the decision-making behavior of such models. Therefore, explaining the logic behind those models through explainable AI (XAI) techniques is essential for their employment in critical applications. This manuscript proposes a novel XAI-based channel estimation (XAI-CHEST) scheme that provides detailed reasonable interpretability of the deep learning (DL) models that are employed in doubly-selective channel estimation. The aim of the proposed XAI-CHEST scheme is to identify the relevant model inputs by inducing high noise on the irrelevant ones. As a result, the behavior of the studied DL-based channel estimators can be further analyzed and evaluated based on the generated interpretations. Simulation results show that the proposed XAI-CHEST scheme provides valid interpretations of the DL-based channel estimators for different scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="OpenAPMax-Abnormal-Patterns-based-Model-for-Real-World-Alzheimer’s-Disease-Diagnosis"><a href="#OpenAPMax-Abnormal-Patterns-based-Model-for-Real-World-Alzheimer’s-Disease-Diagnosis" class="headerlink" title="OpenAPMax: Abnormal Patterns-based Model for Real-World Alzheimer’s Disease Diagnosis"></a>OpenAPMax: Abnormal Patterns-based Model for Real-World Alzheimer’s Disease Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00936">http://arxiv.org/abs/2307.00936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunyou Huang, Xianglong Guan, Xiangjiang Lu, Xiaoshuang Liang, Xiuxia Miao, Jiyue Xie, Wenjing Liu, Li Ma, Suqin Tang, Zhifei Zhang, Jianfeng Zhan</li>
<li>for: 这个研究旨在提出一个开放式识别模型，以便在实际诊断中识别浅生对� Alzheimer’s disease (AD) 的诊断。</li>
<li>methods: 这个模型基于异常模式，首先在每个病人的异常模式上进行统计或文献搜寻，然后将病人的异常模式分组，最后使用极值理论（EVT）来模型每个病人的异常模式距离中心点，修改分类概率。</li>
<li>results: 这个研究获得了最新的开放式识别技术的州分之最佳成绩。<details>
<summary>Abstract</summary>
Alzheimer's disease (AD) cannot be reversed, but early diagnosis will significantly benefit patients' medical treatment and care. In recent works, AD diagnosis has the primary assumption that all categories are known a prior -- a closed-set classification problem, which contrasts with the open-set recognition problem. This assumption hinders the application of the model in natural clinical settings. Although many open-set recognition technologies have been proposed in other fields, they are challenging to use for AD diagnosis directly since 1) AD is a degenerative disease of the nervous system with similar symptoms at each stage, and it is difficult to distinguish from its pre-state, and 2) diversified strategies for AD diagnosis are challenging to model uniformly. In this work, inspired by the concerns of clinicians during diagnosis, we propose an open-set recognition model, OpenAPMax, based on the anomaly pattern to address AD diagnosis in real-world settings. OpenAPMax first obtains the abnormal pattern of each patient relative to each known category through statistics or a literature search, clusters the patients' abnormal pattern, and finally, uses extreme value theory (EVT) to model the distance between each patient's abnormal pattern and the center of their category and modify the classification probability. We evaluate the performance of the proposed method with recent open-set recognition, where we obtain state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
阿尔茨heimer病 (AD) 无法逆转，但早期诊断将对患者的医疗和护理带来显著的好处。在最近的工作中，AD 诊断假设所有类别都是已知的，这是一个关闭集成分类问题，与开集识别问题不同。这种假设限制了模型在实际临床设置中的应用。虽然许多开集识别技术在其他领域得到了应用，但它们难以直接应用于 AD 诊断，因为 1) AD 是神经系统的逐渐恶化病，症状相似，难以与其预状区分，2) AD 诊断策略多样化难以统一模型。在这种情况下，我们提出了一种开集识别模型，OpenAPMax，基于异常模式来解决 AD 诊断实际设置中的问题。OpenAPMax 首先通过统计或文献搜索获得每个患者的异常模式，然后将患者的异常模式分组，最后使用极值理论（EVT）来模型每个患者的异常模式与其类别中心之间的距离，修改分类概率。我们对提出的方法进行评估，并获得了最新的开集识别结果。
</details></li>
</ul>
<hr>
<h2 id="Learning-Differentiable-Logic-Programs-for-Abstract-Visual-Reasoning"><a href="#Learning-Differentiable-Logic-Programs-for-Abstract-Visual-Reasoning" class="headerlink" title="Learning Differentiable Logic Programs for Abstract Visual Reasoning"></a>Learning Differentiable Logic Programs for Abstract Visual Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00928">http://arxiv.org/abs/2307.00928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ml-research/neumann">https://github.com/ml-research/neumann</a></li>
<li>paper_authors: Hikaru Shindo, Viktor Pfanschilling, Devendra Singh Dhami, Kristian Kersting</li>
<li>for: 本研究旨在提高智能代理人的Visual理解和问题解决能力，通过继承forward reasoning和梯度基本机器学习理论。</li>
<li>methods: 本文提出了NEUro-symbolic Message-pAssiNg reasoNer（NEUMANN），它是一种图structure-based的可微分前 reasoning器，通过消息传递来减少内存占用，并可以处理结构化程序和functors。此外，提出了一种 computationally-efficient 结构学习算法，用于在复杂视觉场景中进行解释程序推理。</li>
<li>results: 对比于传统视觉理解任务，本文提出了一个新的任务——Visual reasoning behind-the-scenes，要求代理人学习抽象程序并回答未见场景中的问题。实验表明，NEUMANN可以高效解决视觉理解任务，并超过了基于神经网络、符号学习和神经符号学习的基eline。<details>
<summary>Abstract</summary>
Visual reasoning is essential for building intelligent agents that understand the world and perform problem-solving beyond perception. Differentiable forward reasoning has been developed to integrate reasoning with gradient-based machine learning paradigms. However, due to the memory intensity, most existing approaches do not bring the best of the expressivity of first-order logic, excluding a crucial ability to solve abstract visual reasoning, where agents need to perform reasoning by using analogies on abstract concepts in different scenarios. To overcome this problem, we propose NEUro-symbolic Message-pAssiNg reasoNer (NEUMANN), which is a graph-based differentiable forward reasoner, passing messages in a memory-efficient manner and handling structured programs with functors. Moreover, we propose a computationally-efficient structure learning algorithm to perform explanatory program induction on complex visual scenes. To evaluate, in addition to conventional visual reasoning tasks, we propose a new task, visual reasoning behind-the-scenes, where agents need to learn abstract programs and then answer queries by imagining scenes that are not observed. We empirically demonstrate that NEUMANN solves visual reasoning tasks efficiently, outperforming neural, symbolic, and neuro-symbolic baselines.
</details>
<details>
<summary>摘要</summary>
“视觉理解是智能代理的关键，以实现更多的问题解决。 différentiable forward reasoning 已经开发来结合梯度基本机器学习理念。然而，大多数现有方法因为内存浪费，无法得到最佳的表达力。我们提出了 NEUro-symbolic Message-pAssiNg reasoNer (NEUMANN)，它是一个图像基于的分配式前进逻辑，通过 messages 在缓存fficient的方式传递，并处理结构化程序。此外，我们还提出了一种 computationally-efficient 结构学习算法，用于在复杂视觉场景中进行解释程序推导。为了评估，我们不仅使用传统的视觉逻辑任务，还提出了一个新任务：视觉逻辑后台，代理需要学习抽象程序，然后回答问题by imagining 不见的场景。我们实验表明，NEUMANN 可以高效解决视觉逻辑任务，超过 neural, symbolic 和 neuro-symbolic 基elines。”
</details></li>
</ul>
<hr>
<h2 id="Automatic-Design-of-Semantic-Similarity-Ensembles-Using-Grammatical-Evolution"><a href="#Automatic-Design-of-Semantic-Similarity-Ensembles-Using-Grammatical-Evolution" class="headerlink" title="Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution"></a>Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00925">http://arxiv.org/abs/2307.00925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jorge-martinez-gil/sesige">https://github.com/jorge-martinez-gil/sesige</a></li>
<li>paper_authors: Jorge Martinez-Gil</li>
<li>for: 本研究的目的是提出一种自动设计Semantic Similarity Ensemble的方法，以提高自然语言处理中的相似性评估精度。</li>
<li>methods: 本研究使用了 grammatical evolution 方法，自动选择和聚合候选测试来创建一个最大化人类判断相似性的 ensemble。</li>
<li>results: 对多个 benchmark 数据集进行评估，研究发现，使用 grammatical evolution 方法可以显著提高相似性评估精度，并在一些情况下超越现有的方法。<details>
<summary>Abstract</summary>
Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The source code that illustrates our approach can be downloaded from https://github.com/jorge-martinez-gil/sesige.
</details>
<details>
<summary>摘要</summary>
semantic similarity measures 广泛应用在自然语言处理中，以推动各种计算机相关任务。然而，没有一个单一的 semantic similarity measure 适合所有任务，研究人员常用 ensemble strategies 来保证性能。本研究工作提出了一种自动设计 semantic similarity ensembles 的方法。事实上，我们的提议方法使用 grammatical evolution 自动从候选者池中选择和聚合测量，以创建一个最大化人类判断相关性的ensemble。这种方法在多个 bencmark datasets 上进行了评估，并与当前的 ensemble 相比，显示了它可以显著提高相似性评估准确性，并在某些情况下超越现有方法。因此，我们的研究表明了使用 grammatical evolution 自动比较文本的可能性，并证明了使用 ensemble 对 semantic similarity 任务的性能有益。可以从 https://github.com/jorge-martinez-gil/sesige 下载到我们的方法的源代码。
</details></li>
</ul>
<hr>
<h2 id="Achieving-Stable-Training-of-Reinforcement-Learning-Agents-in-Bimodal-Environments-through-Batch-Learning"><a href="#Achieving-Stable-Training-of-Reinforcement-Learning-Agents-in-Bimodal-Environments-through-Batch-Learning" class="headerlink" title="Achieving Stable Training of Reinforcement Learning Agents in Bimodal Environments through Batch Learning"></a>Achieving Stable Training of Reinforcement Learning Agents in Bimodal Environments through Batch Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00923">http://arxiv.org/abs/2307.00923</a></li>
<li>repo_url: None</li>
<li>paper_authors: E. Hurwitz, N. Peace, G. Cevora</li>
<li>for:  solving Reinforcement Learning problems in bimodal, stochastic environments, particularly applicable to pricing problems.</li>
<li>methods:  using batch updates to the tabular Q-learning algorithm.</li>
<li>results:  the batch learning agents are more effective and resilient to fluctuations in a large stochastic environment, compared to typically-trained agents.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在解决 Reinforcement Learning 问题中的 биModal、随机环境问题，尤其是应用于价格问题。</li>
<li>methods: 使用批处理更新 tabular Q-learning 算法。</li>
<li>results: 批处理学习代理比 typically-trained 代理更有效，并能够更好地鲁式承受大规模随机环境中的波动。<details>
<summary>Abstract</summary>
Bimodal, stochastic environments present a challenge to typical Reinforcement Learning problems. This problem is one that is surprisingly common in real world applications, being particularly applicable to pricing problems. In this paper we present a novel learning approach to the tabular Q-learning algorithm, tailored to tackling these specific challenges by using batch updates. A simulation of pricing problem is used as a testbed to compare a typically updated agent with a batch learning agent. The batch learning agents are shown to be both more effective than the typically-trained agents, and to be more resilient to the fluctuations in a large stochastic environment. This work has a significant potential to enable practical, industrial deployment of Reinforcement Learning in the context of pricing and others.
</details>
<details>
<summary>摘要</summary>
biModal、 randomly changing environments 给 typical Reinforcement Learning 问题提出挑战。这种问题在实际应用中很普遍，尤其适用于价格问题。在这篇论文中，我们介绍了一种新的学习方法，用于tabular Q-learning 算法，以适应这些特定挑战。我们使用批更新来解决这些问题，并在一个 simulate 价格问题 中进行测试。对比typically更新的代理，批学习代理显示更高效和更具抗应力于大 randomly changing environments。这项工作具有实用化 Reinforcement Learning 在价格和其他领域的潜在潜力。
</details></li>
</ul>
<hr>
<h2 id="Node-weighted-Graph-Convolutional-Network-for-Depression-Detection-in-Transcribed-Clinical-Interviews"><a href="#Node-weighted-Graph-Convolutional-Network-for-Depression-Detection-in-Transcribed-Clinical-Interviews" class="headerlink" title="Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews"></a>Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00920">http://arxiv.org/abs/2307.00920</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idiap/Node_weighted_GCN_for_depression_detection">https://github.com/idiap/Node_weighted_GCN_for_depression_detection</a></li>
<li>paper_authors: Sergio Burdisso, Esaú Villatoro-Tello, Srikanth Madikeri, Petr Motlicek</li>
<li>for: 本研究旨在提出一种简单的方法来归wt自连接边在图像演化网络（GCN）中，并对抑郁检测从讲词笔记中进行研究。</li>
<li>methods: 本研究使用GCN模型来模拟非 consecutive和长距离语义，以分类讲词笔记为抑郁或控制者。提出的方法旨在缓解GCN模型的局限性假设，包括本地性和自连接vs邻居节点的等重要性，而保留优点如低计算成本、数据无关和可读性等。</li>
<li>results: 研究结果表明，我们的方法在两个 benchmark 数据集上经过极限评估，并常常超过原始GCN模型和之前报道的结果，在两个数据集上达到 F1&#x3D;0.84%。此外，一种qualitative分析表明提出的方法具有可读性特点，并与心理学上的前期发现相一致。<details>
<summary>Abstract</summary>
We propose a simple approach for weighting self-connecting edges in a Graph Convolutional Network (GCN) and show its impact on depression detection from transcribed clinical interviews. To this end, we use a GCN for modeling non-consecutive and long-distance semantics to classify the transcriptions into depressed or control subjects. The proposed method aims to mitigate the limiting assumptions of locality and the equal importance of self-connections vs. edges to neighboring nodes in GCNs, while preserving attractive features such as low computational cost, data agnostic, and interpretability capabilities. We perform an exhaustive evaluation in two benchmark datasets. Results show that our approach consistently outperforms the vanilla GCN model as well as previously reported results, achieving an F1=0.84% on both datasets. Finally, a qualitative analysis illustrates the interpretability capabilities of the proposed approach and its alignment with previous findings in psychology.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单的方法，用于在图 convolutional neural network（GCN）中Weight自连接边，并对抑郁检测从转录的临床对话进行了影响分析。为此，我们使用GCN来模型非连续和长距离语义，将转录分类为抑郁或控制者。我们的方法的目标是缓解GCN中的局部性和自连接 Edge和邻居节点之间的等重要性假设，同时保持低计算成本、数据无关和可解释性的特点。我们在两个标准 benchmark 数据集中进行了极限评估。结果显示，我们的方法在两个数据集中一直具有最高的 F1=0.84% 性能，超过了标准 GCN 模型以及之前报道的结果。最后，我们进行了一个 cualitative 分析，描述了我们的方法的可解释性特点，并与心理学前研究的结论进行了对比。
</details></li>
</ul>
<hr>
<h2 id="Why-do-CNNs-excel-at-feature-extraction-A-mathematical-explanation"><a href="#Why-do-CNNs-excel-at-feature-extraction-A-mathematical-explanation" class="headerlink" title="Why do CNNs excel at feature extraction? A mathematical explanation"></a>Why do CNNs excel at feature extraction? A mathematical explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00919">http://arxiv.org/abs/2307.00919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinoth Nandakumar, Arush Tagade, Tongliang Liu</li>
<li>for: 解释深度学习模型如何解决图像分类任务，特别是图像特征提取问题。</li>
<li>methods: 提出了一种新的数学模型，基于图像特征提取，可以生成符合实际数据集的图像。并证明了 convolutional neural network 可以通过这种模型解决图像分类任务。</li>
<li>results: 通过构造分割线函数来检测图像中特征的存在，并证明这些函数可以被 convolutional network 实现。<details>
<summary>Abstract</summary>
Over the past decade deep learning has revolutionized the field of computer vision, with convolutional neural network models proving to be very effective for image classification benchmarks. However, a fundamental theoretical questions remain answered: why can they solve discrete image classification tasks that involve feature extraction? We address this question in this paper by introducing a novel mathematical model for image classification, based on feature extraction, that can be used to generate images resembling real-world datasets. We show that convolutional neural network classifiers can solve these image classification tasks with zero error. In our proof, we construct piecewise linear functions that detect the presence of features, and show that they can be realized by a convolutional network.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:过去一代，深度学习对计算机视觉领域产生了革命，卷积神经网络模型在图像分类标准 benchMark 中表现出了极高的效果。然而，一个基本的理论问题仍未得到答案：它们能够解决 discrete 图像分类任务，这些任务包括特征提取吗？我们在这篇论文中回答了这个问题，我们引入了一种基于特征提取的新的数学模型，可以生成类似于实际数据集的图像。我们显示了卷积神经网络分类器可以在这些图像分类任务中达到零错误率。在我们的证明中，我们构建了分割线性函数，检测特征存在，并证明它们可以由卷积神经网络实现。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Prompt-Learning-for-Vision-Language-Understanding"><a href="#Contextual-Prompt-Learning-for-Vision-Language-Understanding" class="headerlink" title="Contextual Prompt Learning for Vision-Language Understanding"></a>Contextual Prompt Learning for Vision-Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00910">http://arxiv.org/abs/2307.00910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Koustava Goswami, Srikrishna Karanam, Joseph K J, Prateksha Udhayanan, Balaji Vasan Srinivasan</li>
<li>for: 这paper的目的是提出一种Contextual Prompt Learning（CoPL）框架，以提高视觉语言模型的泛化能力。</li>
<li>methods: 该paper使用了可调式的Prompt Learning技术，并将其与本地特征学习结合，以学习到更好的泛化表示。</li>
<li>results: 对于多种标准和少量数据集，该方法比现有状态OF THE ART方法提高了表现，并在少量数据集和异常数据集上也显示了出色的表现。<details>
<summary>Abstract</summary>
Recent advances in multimodal learning has resulted in powerful vision-language models, whose representations are generalizable across a variety of downstream tasks. Recently, their generalizability has been further extended by incorporating trainable prompts, borrowed from the natural language processing literature. While such prompt learning techniques have shown impressive results, we identify that these prompts are trained based on global image features which limits itself in two aspects: First, by using global features, these prompts could be focusing less on the discriminative foreground image, resulting in poor generalization to various out-of-distribution test cases. Second, existing work weights all prompts equally whereas our intuition is that these prompts are more specific to the type of the image. We address these issues with as part of our proposed Contextual Prompt Learning (CoPL) framework, capable of aligning the prompts to the localized features of the image. Our key innovations over earlier works include using local image features as part of the prompt learning process, and more crucially, learning to weight these prompts based on local features that are appropriate for the task at hand. This gives us dynamic prompts that are both aligned to local image features as well as aware of local contextual relationships. Our extensive set of experiments on a variety of standard and few-shot datasets show that our method produces substantially improved performance when compared to the current state of the art methods. We also demonstrate both few-shot and out-of-distribution performance to establish the utility of learning dynamic prompts that are aligned to local image features.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>The prompts may not focus enough on the discriminative foreground image, leading to poor generalization to out-of-distribution test cases.2. Existing methods weight all prompts equally, even though some prompts may be more relevant to specific types of images.Our proposed Contextual Prompt Learning (CoPL) framework addresses these issues by aligning the prompts to localized features of the image. Our key innovations include using local image features as part of the prompt learning process and learning to weight these prompts based on local features that are appropriate for the task at hand. This results in dynamic prompts that are both aligned to local image features and aware of local contextual relationships.Our extensive set of experiments on various standard and few-shot datasets show that our method produces substantially improved performance compared to current state-of-the-art methods. We also demonstrate the utility of learning dynamic prompts that are aligned to local image features, both in few-shot and out-of-distribution scenarios.</details></li>
</ol>
<hr>
<h2 id="An-open-source-deep-learning-algorithm-for-efficient-and-fully-automatic-analysis-of-the-choroid-in-optical-coherence-tomography"><a href="#An-open-source-deep-learning-algorithm-for-efficient-and-fully-automatic-analysis-of-the-choroid-in-optical-coherence-tomography" class="headerlink" title="An open-source deep learning algorithm for efficient and fully-automatic analysis of the choroid in optical coherence tomography"></a>An open-source deep learning algorithm for efficient and fully-automatic analysis of the choroid in optical coherence tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00904">http://arxiv.org/abs/2307.00904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamie Burke, Justin Engelmann, Charlene Hamid, Megan Reid-Schachter, Tom Pearson, Dan Pugh, Neeraj Dhaun, Stuart King, Tom MacGillivray, Miguel O. Bernabeu, Amos Storkey, Ian J. C. MacCormick</li>
<li>For: The paper is written for researchers and clinicians who need to extract choroidal measurements from optical coherence tomography (OCT) data, specifically for systemic disease research.* Methods: The paper proposes a deep learning algorithm called DeepGPET, which is fully automatic and open-source, for choroid region segmentation in OCT data. The algorithm uses a UNet with MobileNetV3 backbone pre-trained on ImageNet, and finetuned on a dataset of 715 OCT B-scans from 3 clinical studies.* Results: The paper shows that DeepGPET achieves excellent agreement with a clinically validated, semi-automatic choroid segmentation method (Gaussian Process Edge Tracing, GPET) in terms of standard segmentation agreement metrics and derived measures of choroidal thickness and area. Additionally, DeepGPET reduces the mean processing time per image from 34.49 seconds to 1.25 seconds on a standard laptop CPU, making it a faster and more efficient method for choroidal segmentation.Here is the information in Simplified Chinese text:* For: 这篇论文是为研究人员和临床医生制定的，需要从光共振成像（OCT）数据中提取choroid区域的测量。* Methods: 论文提出了一种深度学习算法 called DeepGPET，用于OCT数据中choroid区域的分割。该算法使用了UNet的MobileNetV3后处理器，并在3个临床研究中进行了训练。* Results: 论文表明，DeepGPET与临床验证的、semi-自动choroid分割方法（Gaussian Process Edge Tracing，GPET）在标准分割一致度指标和choroid膜厚度和面积的衡量中达到了极高的一致性。此外，DeepGPET还将OCT数据中每个图像的处理时间从34.49秒降低到1.25秒，使其成为一种更快和高效的choroid分割方法。<details>
<summary>Abstract</summary>
Purpose: To develop an open-source, fully-automatic deep learning algorithm, DeepGPET, for choroid region segmentation in optical coherence tomography (OCT) data. Methods: We used a dataset of 715 OCT B-scans (82 subjects, 115 eyes) from 3 clinical studies related to systemic disease. Ground truth segmentations were generated using a clinically validated, semi-automatic choroid segmentation method, Gaussian Process Edge Tracing (GPET). We finetuned a UNet with MobileNetV3 backbone pre-trained on ImageNet. Standard segmentation agreement metrics, as well as derived measures of choroidal thickness and area, were used to evaluate DeepGPET, alongside qualitative evaluation from a clinical ophthalmologist. Results: DeepGPET achieves excellent agreement with GPET on data from 3 clinical studies (AUC=0.9994, Dice=0.9664; Pearson correlation of 0.8908 for choroidal thickness and 0.9082 for choroidal area), while reducing the mean processing time per image on a standard laptop CPU from 34.49s ($\pm$15.09) using GPET to 1.25s ($\pm$0.10) using DeepGPET. Both methods performed similarly according to a clinical ophthalmologist, who qualitatively judged a subset of segmentations by GPET and DeepGPET, based on smoothness and accuracy of segmentations. Conclusions :DeepGPET, a fully-automatic, open-source algorithm for choroidal segmentation, will enable researchers to efficiently extract choroidal measurements, even for large datasets. As no manual interventions are required, DeepGPET is less subjective than semi-automatic methods and could be deployed in clinical practice without necessitating a trained operator. DeepGPET addresses the lack of open-source, fully-automatic and clinically relevant choroid segmentation algorithms, and its subsequent public release will facilitate future choroidal research both in ophthalmology and wider systemic health.
</details>
<details>
<summary>摘要</summary>
目的：开发一个开源、自动化深度学习算法DeepGPET，用于光学同步 Tomatoes（OCT）数据中的 Choroid 区域 segmentation。方法：我们使用了715个 OCT B-scan（82名病人，115个眼睛）从3个临床研究中的系统疾病相关数据。我们使用了一种临床验证的 semi-automatic Choroid 分割方法Gaussian Process Edge Tracing（GPET）生成了标准 segmentation。我们在 MobileNetV3 预训练 ImageNet 上训练了一个 UNet 模型，并对其进行了微调。我们使用了标准 segmentation 一致度量和 Choroid 厚度和面积的 derivated 度量来评估 DeepGPET，并与临床医生对一部分分 segmentation 进行了质量评估。结果：DeepGPET 与 GPET 在3个临床研究数据上达到了极高的一致性（AUC=0.9994，Dice=0.9664；Pearson 相关系数为0.8908 для Choroid 厚度和0.9082 для Choroid 面积），同时在标准笔记PC CPU 上减少了每个图像的处理时间从34.49秒（±15.09）使用 GPET 到1.25秒（±0.10）使用 DeepGPET。两种方法在临床医生的质量评估中表现相似，后者根据分割的平滑度和准确性进行了质量评估。结论：DeepGPET 是一个开源、自动化、临床相关的 Choroid 分割算法，可以帮助研究人员高效地提取 Choroid 测量数据，即使是大型数据集。由于不需要人工 intervención，DeepGPET 比 semi-automatic 方法更加 Objective ，可以在临床实践中无需培训操作员而被部署。DeepGPET 填补了开源、自动化和临床相关的 Choroid 分割算法的缺失，其后公开释出将促进未来 Choroid 研究的发展，不仅在眼科医学中，还在更广泛的系统医学中。
</details></li>
</ul>
<hr>
<h2 id="Fixing-confirmation-bias-in-feature-attribution-methods-via-semantic-match"><a href="#Fixing-confirmation-bias-in-feature-attribution-methods-via-semantic-match" class="headerlink" title="Fixing confirmation bias in feature attribution methods via semantic match"></a>Fixing confirmation bias in feature attribution methods via semantic match</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00897">http://arxiv.org/abs/2307.00897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Cinà, Daniel Fernandez-Llaneza, Nishant Mishra, Tabea E. Röber, Sandro Pezzelle, Iacer Calixto, Rob Goedhart, Ş. İlker Birbil</li>
<li>for: 本研究旨在提供一种结构化的方法来评估黑盒模型的解释是否具有Semantic Match性，以确保模型的内部表示符合人类概念。</li>
<li>methods: 该研究基于Cin`a et al. [2023]的概念框架，提出了一种实践中的评估Semantic Match性的方法。该方法通过对 tabular 和图像数据进行一系列实验，以证明 semantic match 评估可以为模型行为带来深刻的理解。</li>
<li>results: 研究结果显示，通过评估 semantic match，可以发现模型的某些行为是由于偏见而导致的，例如关注一个不相关的关系。同时，该方法也可以证明模型在预测时关注了重要的对象，例如一个与预测有关的物体。该研究提供了一种可靠的方法来评估XAI中的Confirmation Bias问题。<details>
<summary>Abstract</summary>
Feature attribution methods have become a staple method to disentangle the complex behavior of black box models. Despite their success, some scholars have argued that such methods suffer from a serious flaw: they do not allow a reliable interpretation in terms of human concepts. Simply put, visualizing an array of feature contributions is not enough for humans to conclude something about a model's internal representations, and confirmation bias can trick users into false beliefs about model behavior. We argue that a structured approach is required to test whether our hypotheses on the model are confirmed by the feature attributions. This is what we call the "semantic match" between human concepts and (sub-symbolic) explanations. Building on the conceptual framework put forward in Cin\`a et al. [2023], we propose a structured approach to evaluate semantic match in practice. We showcase the procedure in a suite of experiments spanning tabular and image data, and show how the assessment of semantic match can give insight into both desirable (e.g., focusing on an object relevant for prediction) and undesirable model behaviors (e.g., focusing on a spurious correlation). We couple our experimental results with an analysis on the metrics to measure semantic match, and argue that this approach constitutes the first step towards resolving the issue of confirmation bias in XAI.
</details>
<details>
<summary>摘要</summary>
<<SYS>>模型解释方法已成为黑盒模型行为解释的主流方法。 despite their success, some scholars have argued that such methods suffer from a serious flaw: they do not allow a reliable interpretation in terms of human concepts. simply put, visualizing an array of feature contributions is not enough for humans to conclude something about a model's internal representations, and confirmation bias can trick users into false beliefs about model behavior. we argue that a structured approach is required to test whether our hypotheses on the model are confirmed by the feature attributions. this is what we call the "semantic match" between human concepts and (sub-symbolic) explanations. building on the conceptual framework put forward in Cin\`a et al. [2023], we propose a structured approach to evaluate semantic match in practice. we showcase the procedure in a suite of experiments spanning tabular and image data, and show how the assessment of semantic match can give insight into both desirable (e.g., focusing on an object relevant for prediction) and undesirable model behaviors (e.g., focusing on a spurious correlation). we couple our experimental results with an analysis on the metrics to measure semantic match, and argue that this approach constitutes the first step towards resolving the issue of confirmation bias in XAI.>>>
</details></li>
</ul>
<hr>
<h2 id="Internet-of-Things-Fault-Detection-and-Classification-via-Multitask-Learning"><a href="#Internet-of-Things-Fault-Detection-and-Classification-via-Multitask-Learning" class="headerlink" title="Internet of Things Fault Detection and Classification via Multitask Learning"></a>Internet of Things Fault Detection and Classification via Multitask Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01234">http://arxiv.org/abs/2307.01234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Arif Ul Alam</li>
<li>for: 本研究旨在开发一个适用于现实世界IIoT应用场景的故障检测和分类系统。</li>
<li>methods: 本研究使用现实世界IIoT系统，通过三个阶段的数据收集，模拟了11种预定的故障类别。我们提议了SMTCNN方法用于IIoT故障检测和分类，并对实际数据进行评估。</li>
<li>results: SMTCNN方法在实际数据上显示出了superior特异性（3.5%），并在精度、回归率和F1度上显示出了显著的提升 compared to现有技术。<details>
<summary>Abstract</summary>
This paper presents a comprehensive investigation into developing a fault detection and classification system for real-world IIoT applications. The study addresses challenges in data collection, annotation, algorithm development, and deployment. Using a real-world IIoT system, three phases of data collection simulate 11 predefined fault categories. We propose SMTCNN for fault detection and category classification in IIoT, evaluating its performance on real-world data. SMTCNN achieves superior specificity (3.5%) and shows significant improvements in precision, recall, and F1 measures compared to existing techniques.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Augmenting-Deep-Learning-Adaptation-for-Wearable-Sensor-Data-through-Combined-Temporal-Frequency-Image-Encoding"><a href="#Augmenting-Deep-Learning-Adaptation-for-Wearable-Sensor-Data-through-Combined-Temporal-Frequency-Image-Encoding" class="headerlink" title="Augmenting Deep Learning Adaptation for Wearable Sensor Data through Combined Temporal-Frequency Image Encoding"></a>Augmenting Deep Learning Adaptation for Wearable Sensor Data through Combined Temporal-Frequency Image Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00883">http://arxiv.org/abs/2307.00883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yidong Zhu, Md Mahmudur Rahman, Mohammad Arif Ul Alam</li>
<li>for: 这篇论文是针对穿戴式感应器数据进行分类的深度学习方法。</li>
<li>methods: 本研究使用修改过的回归图像表示方法，将穿戴式感应器资料转换为图像，并使用快速傅立叶数学方法估算频率领域的差异。此外，研究者还使用mixup增强表示法。</li>
<li>results: 研究者使用测试 accelerometer-based 活动识别数据和预训ResNet模型，并证明了该方法与现有方法相比具有更好的性能。<details>
<summary>Abstract</summary>
Deep learning advancements have revolutionized scalable classification in many domains including computer vision. However, when it comes to wearable-based classification and domain adaptation, existing computer vision-based deep learning architectures and pretrained models trained on thousands of labeled images for months fall short. This is primarily because wearable sensor data necessitates sensor-specific preprocessing, architectural modification, and extensive data collection. To overcome these challenges, researchers have proposed encoding of wearable temporal sensor data in images using recurrent plots. In this paper, we present a novel modified-recurrent plot-based image representation that seamlessly integrates both temporal and frequency domain information. Our approach incorporates an efficient Fourier transform-based frequency domain angular difference estimation scheme in conjunction with the existing temporal recurrent plot image. Furthermore, we employ mixup image augmentation to enhance the representation. We evaluate the proposed method using accelerometer-based activity recognition data and a pretrained ResNet model, and demonstrate its superior performance compared to existing approaches.
</details>
<details>
<summary>摘要</summary>
深度学习的发展在许多领域中已经引起了革命，包括计算机视觉。然而，当来到穿戴式分类和领域适应时，现有的计算机视觉基础设施和预训练模型，通过月余千张标注图像进行训练，在不同的环境下表现不佳。这主要是因为穿戴式传感器数据需要特定的感知器数据处理、建筑修改和大量的数据收集。为了解决这些挑战，研究人员提出了将穿戴式时间传感器数据编码到图像中的循环图表方法。在本文中，我们提出一种修改后的循环图表基于图像表示方法，可以兼容时域和频域信息。我们的方法包括使用快速傅立做频域角度差估计方案，并与现有的时域循环图表相结合。此外，我们采用混合图像增强技术来提高表示。我们使用拥有陀螺仪数据的活动识别任务和预训练ResNet模型进行评估，并证明我们的方法与现有方法相比具有更高的表现。
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Pain-Assessment-through-Wearables-and-EHR-Data-Multi-attribute-Fairness-Loss-based-CNN-Approach"><a href="#Unbiased-Pain-Assessment-through-Wearables-and-EHR-Data-Multi-attribute-Fairness-Loss-based-CNN-Approach" class="headerlink" title="Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach"></a>Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05333">http://arxiv.org/abs/2307.05333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharmin Sultana, Md Mahmudur Rahman, Atqiya Munawara Mahi, Shao-Hsien Liu, Mohammad Arif Ul Alam</li>
<li>for: 本研究旨在提出一种多属性公平损失（MAFL）基于卷积神经网络模型，以便考虑数据中包含的敏感属性并公平地预测患者疼痛状况。</li>
<li>methods: 本研究使用了多 attribute fairness loss（MAFL）基于卷积神经网络模型，并对比了该模型与现有的敏感 Mitigation 技术，以确定是否可以满足精度和公平性之间的补做。</li>
<li>results: 研究表明，使用了提议的 MAFL 模型，能够在 NIH All-Of-US 数据集上实现更高的公平性和精度，与现有的方法相比，显示了更好的性能。<details>
<summary>Abstract</summary>
The combination of diverse health data (IoT, EHR, and clinical surveys) and scalable-adaptable Artificial Intelligence (AI), has enabled the discovery of physical, behavioral, and psycho-social indicators of pain status. Despite the hype and promise to fundamentally alter the healthcare system with technological advancements, much AI adoption in clinical pain evaluation has been hampered by the heterogeneity of the problem itself and other challenges, such as personalization and fairness. Studies have revealed that many AI (i.e., machine learning or deep learning) models display biases and discriminate against specific population segments (such as those based on gender or ethnicity), which breeds skepticism among medical professionals about AI adaptability. In this paper, we propose a Multi-attribute Fairness Loss (MAFL) based CNN model that aims to account for any sensitive attributes included in the data and fairly predict patients' pain status while attempting to minimize the discrepancies between privileged and unprivileged groups. In order to determine whether the trade-off between accuracy and fairness can be satisfied, we compare the proposed model with well-known existing mitigation procedures, and studies reveal that the implemented model performs favorably in contrast to state-of-the-art methods. Utilizing NIH All-Of-US data, where a cohort of 868 distinct individuals with wearables and EHR data gathered over 1500 days has been taken into consideration to analyze our suggested fair pain assessment system.
</details>
<details>
<summary>摘要</summary>
“ combinaison de données de santé diverse (IoT, EHR, et sondages cliniques) et des technologies d'apprentissage automatique (AI) scalables-adaptables a permis la découverte d'indicateurs physiques, comportementaux et psychosociaux du statut de douleur. Malgré l'engouement et la promesse de modifier fondamentalement le système de santé avec des avancées technologiques, l'adoption d'IA dans l'évaluation de la douleur clinique a été freinée par la hétérogénéité du problème et d'autres défis, tels que la personnalisation et la justice. Les études ont montré que nombreux modèles d'IA (par exemple, apprentissage automatique ou apprentissage profond) présentent des biais et discriminent contre des groupes de population spécifiques (par exemple, en fonction du genre ou de l'ethnie), ce qui suscite la scepticisme chez les professionnels de la santé quant à l'adaptabilité de l'IA. Dans cet article, nous proposons un modèle de perte de fairness multi-attributs (MAFL) basé sur des réseaux de neurones qui vise à prendre en compte les attributs sensibles dans les données et de prédire justement le statut de douleur des patients tout en tentant de minimiser les disparités entre les groupes privilégiés et les groupes marginaux. Pour déterminer si la compromission entre la précision et la fairness peut être satisfaite, nous comparons le modèle proposé avec des procédures de mitigation existantes, et les études révèlent que le modèle mis en œuvre performs favorablement par rapport aux méthodes établies. En utilisant les données All-Of-US de la NIH, où un échantillon de 868 individus distincts avec des capteurs de wearables et des données EHR collectées sur 1500 jours a été pris en compte pour analyser notre système de evaluation de douleur juste.”
</details></li>
</ul>
<hr>
<h2 id="Mining-Clues-from-Incomplete-Utterance-A-Query-enhanced-Network-for-Incomplete-Utterance-Rewriting"><a href="#Mining-Clues-from-Incomplete-Utterance-A-Query-enhanced-Network-for-Incomplete-Utterance-Rewriting" class="headerlink" title="Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting"></a>Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00866">http://arxiv.org/abs/2307.00866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/S1s-Z/QUEEN">https://github.com/S1s-Z/QUEEN</a></li>
<li>paper_authors: Shuzheng Si, Shuang Zeng, Baobao Chang</li>
<li>for: 提高废话重建的性能</li>
<li>methods: 使用查询模板和高效的编辑操作分数网络</li>
<li>results: 在多个公共数据集上达到了状态艺术性的表现Here’s a breakdown of each point:</li>
<li>for: 本研究旨在提高废话重建的性能，即使面临 incomplete utterance 和 rewrite 之间的 semantic structural information 不足。</li>
<li>methods: 我们提出了一种 Query-Enhanced Network (QUEEN)，其中包括使用查询模板和高效的编辑操作分数网络。</li>
<li>results: QUEEN 在多个公共数据集上达到了状态艺术性的表现，比如 COMPETE 和 WNLI。<details>
<summary>Abstract</summary>
Incomplete utterance rewriting has recently raised wide attention. However, previous works do not consider the semantic structural information between incomplete utterance and rewritten utterance or model the semantic structure implicitly and insufficiently. To address this problem, we propose a QUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly brings guided semantic structural knowledge between the incomplete utterance and the rewritten utterance making model perceive where to refer back to or recover omitted tokens. Then, we adopt a fast and effective edit operation scoring network to model the relation between two tokens. Benefiting from proposed query template and the well-designed edit operation scoring network, QUEEN achieves state-of-the-art performance on several public datasets.
</details>
<details>
<summary>摘要</summary>
句子缺失重新写作最近引起了广泛关注。然而，前一代工作没有考虑异常完整的句子和重新写作之间的 semantics 结构信息或者模型这种信息。为解决这个问题，我们提出了 Query-Enhanced Network（QUEEN）。首先，我们的提议的查询模板Explicitly brings guided semantics structural knowledge between incomplete utterance and rewritten utterance, making the model aware of where to refer back to or recover omitted tokens。然后，我们采用了高效的编辑操作分数网络来模型两个tokentypes的关系。由于提议的查询模板和Well-designed edit operation scoring network，QUEEN在多个公共数据集上达到了状态艺术性能。
</details></li>
</ul>
<hr>
<h2 id="OpenSiteRec-An-Open-Dataset-for-Site-Recommendation"><a href="#OpenSiteRec-An-Open-Dataset-for-Site-Recommendation" class="headerlink" title="OpenSiteRec: An Open Dataset for Site Recommendation"></a>OpenSiteRec: An Open Dataset for Site Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00856">http://arxiv.org/abs/2307.00856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhang Li, Xiangyu Zhao, Yejing Wang, Yu Liu, Yong Li, Cheng Long, Yong Zhang, Chunxiao Xing</li>
<li>for: 这个论文是为了推动和促进现代商业中自动化数据驱动的brand发展而写的。</li>
<li>methods: 这篇论文使用了一个综合的图表示语录中的各种实际世界实体和关系，并利用了一些代表性的推荐模型进行了比较性的研究。</li>
<li>results: 该论文发现了一个开放的、完整的数据集，称为OpenSiteRec，可以帮助促进site recommendation研究的发展。此外，论文还指出了这些数据集的应用前景，以及一些现有的推荐模型在这些数据集上的性能。<details>
<summary>Abstract</summary>
As a representative information retrieval task, site recommendation, which aims at predicting the optimal sites for a brand or an institution to open new branches in an automatic data-driven way, is beneficial and crucial for brand development in modern business. However, there is no publicly available dataset so far and most existing approaches are limited to an extremely small scope of brands, which seriously hinders the research on site recommendation. Therefore, we collect, construct and release an open comprehensive dataset, namely OpenSiteRec, to facilitate and promote the research on site recommendation. Specifically, OpenSiteRec leverages a heterogeneous graph schema to represent various types of real-world entities and relations in four international metropolises. To evaluate the performance of the existing general methods on the site recommendation task, we conduct benchmarking experiments of several representative recommendation models on OpenSiteRec. Furthermore, we also highlight the potential application directions to demonstrate the wide applicability of OpenSiteRec. We believe that our OpenSiteRec dataset is significant and anticipated to encourage the development of advanced methods for site recommendation. OpenSiteRec is available online at https://OpenSiteRec.github.io/.
</details>
<details>
<summary>摘要</summary>
为代表信息检索任务，站点推荐，目标是通过自动化数据驱动方式预测品牌或机构在新分支点的最佳选择，对现代商业发展是有益和重要的。然而，目前没有公共可用的数据集，大多数现有方法的覆盖范围受到严重限制，这阻碍了对站点推荐的研究。因此，我们收集、构建并发布了一个开放的完整数据集，名为OpenSiteRec，以便促进和推动站点推荐研究。具体来说，OpenSiteRec 利用多种实体和关系的等级图表示现实世界中的多种类型实体和关系，在四个国际大都会中进行了多种实验。为了评估现有普通方法的站点推荐性能，我们在OpenSiteRec上进行了许多代表推荐模型的 benchmarking 实验。此外，我们还强调了可能的应用方向，以示 OpenSiteRec 的广泛应用性。我们认为 OpenSiteRec 数据集是重要的，并且预计会鼓励高级方法的发展。OpenSiteRec 在 <https://OpenSiteRec.github.io/> 上可以下载。
</details></li>
</ul>
<hr>
<h2 id="Review-of-Large-Vision-Models-and-Visual-Prompt-Engineering"><a href="#Review-of-Large-Vision-Models-and-Visual-Prompt-Engineering" class="headerlink" title="Review of Large Vision Models and Visual Prompt Engineering"></a>Review of Large Vision Models and Visual Prompt Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00855">http://arxiv.org/abs/2307.00855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Wang, Zhengliang Liu, Lin Zhao, Zihao Wu, Chong Ma, Sigang Yu, Haixing Dai, Qiushi Yang, Yiheng Liu, Songyao Zhang, Enze Shi, Yi Pan, Tuo Zhang, Dajiang Zhu, Xiang Li, Xi Jiang, Bao Ge, Yixuan Yuan, Dinggang Shen, Tianming Liu, Shu Zhang</li>
<li>for: 本文旨在概述计算机视觉领域中大视模型和视提示工程技术的最新发展，以便为未来研究人员提供系统化和完整的视提示工程方法概述。</li>
<li>methods: 本文详细介绍了在计算机视觉领域中使用的大视模型和视提示工程方法，包括多种提示工程方法的应用和实践。</li>
<li>results: 本文总结了大视模型和视提示工程方法的最新进展，并提供了 valuable insights  для未来研究人员在这个领域的探索。<details>
<summary>Abstract</summary>
Visual prompt engineering is a fundamental technology in the field of visual and image Artificial General Intelligence, serving as a key component for achieving zero-shot capabilities. As the development of large vision models progresses, the importance of prompt engineering becomes increasingly evident. Designing suitable prompts for specific visual tasks has emerged as a meaningful research direction. This review aims to summarize the methods employed in the computer vision domain for large vision models and visual prompt engineering, exploring the latest advancements in visual prompt engineering. We present influential large models in the visual domain and a range of prompt engineering methods employed on these models. It is our hope that this review provides a comprehensive and systematic description of prompt engineering methods based on large visual models, offering valuable insights for future researchers in their exploration of this field.
</details>
<details>
<summary>摘要</summary>
Visual prompt engineering是视觉人工智能领域的基础技术之一，为实现零容量能力提供关键组件。随着大视觉模型的发展，提示工程技术的重要性日益显著。设计适合特定视觉任务的提示已成为研究的 significativo方向。本文旨在summarize计算机视觉领域中大视觉模型和视觉提示工程方法的发展，探讨最新的提示工程技术。我们提出了影响视觉领域的主要大模型，以及这些模型上emploied的多种提示工程方法。我们希望这篇文章能提供系统性的描述，为未来研究人员在这个领域的探索提供有价值的经验。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Re-evaluation-of-Benchmark-Datasets-for-Deep-Learning-Based-Matching-Algorithms"><a href="#A-Critical-Re-evaluation-of-Benchmark-Datasets-for-Deep-Learning-Based-Matching-Algorithms" class="headerlink" title="A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms"></a>A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01231">http://arxiv.org/abs/2307.01231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gpapadis/dlmatchers">https://github.com/gpapadis/dlmatchers</a></li>
<li>paper_authors: George Papadakis, Nishadi Kirielle, Peter Christen, Themis Palpanas</li>
<li>for: 本研究旨在评估Established datasets的难度和合适性，以便更好地评估学习基本匹配算法的性能。</li>
<li>methods: 本研究提出了四种方法来评估13个Established datasets的难度和合适性，包括两种理论方法和两种实践方法。</li>
<li>results: 研究发现，大多数Popular datasets pose rather easy classification tasks，因此不适合评估学习基本匹配算法的性能。为此，本研究提出了一种新的方法来生成benchmark datasets，并在其中创建了四个新匹配任务，以验证这些新的benchmarks的难度和合适性。<details>
<summary>Abstract</summary>
Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four different approaches to assessing the difficulty and appropriateness of 13 established datasets: two theoretical approaches, which involve new measures of linearity and existing measures of complexity, and two practical approaches: the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most of the popular datasets pose rather easy classification tasks. As a result, they are not suitable for properly evaluating learning-based matching algorithms. To address this issue, we propose a new methodology for yielding benchmark datasets. We put it into practice by creating four new matching tasks, and we verify that these new benchmarks are more challenging and therefore more suitable for further advancements in the field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Comprehensive-Survey-of-Artificial-Intelligence-Techniques-for-Talent-Analytics"><a href="#A-Comprehensive-Survey-of-Artificial-Intelligence-Techniques-for-Talent-Analytics" class="headerlink" title="A Comprehensive Survey of Artificial Intelligence Techniques for Talent Analytics"></a>A Comprehensive Survey of Artificial Intelligence Techniques for Talent Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03195">http://arxiv.org/abs/2307.03195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuan Qin, Le Zhang, Rui Zha, Dazhong Shen, Qi Zhang, Ying Sun, Chen Zhu, Hengshu Zhu, Hui Xiong</li>
<li>For: This paper aims to provide an up-to-date and comprehensive survey of AI technologies used for talent analytics in human resource management.* Methods: The paper categorizes various pertinent data and offers a comprehensive taxonomy of relevant research efforts, categorized based on three distinct application-driven scenarios: talent management, organization management, and labor market analysis.* Results: The paper summarizes the open challenges and potential prospects for future research directions in the domain of AI-driven talent analytics.Here’s the same information in Simplified Chinese text:* For: 这篇论文目的是为人力资源管理领域提供最新最全面的人工智能技术在人才分析方面的报告。* Methods: 论文首先提供人才分析的背景知识，然后对应用场景进行分类，并提供了三个不同的应用场景：人才管理、组织管理和劳动市场分析。* Results: 论文总结了人才分析领域的未来研究方向和挑战。<details>
<summary>Abstract</summary>
In today's competitive and fast-evolving business environment, it is a critical time for organizations to rethink how to make talent-related decisions in a quantitative manner. Indeed, the recent development of Big Data and Artificial Intelligence (AI) techniques have revolutionized human resource management. The availability of large-scale talent and management-related data provides unparalleled opportunities for business leaders to comprehend organizational behaviors and gain tangible knowledge from a data science perspective, which in turn delivers intelligence for real-time decision-making and effective talent management at work for their organizations. In the last decade, talent analytics has emerged as a promising field in applied data science for human resource management, garnering significant attention from AI communities and inspiring numerous research efforts. To this end, we present an up-to-date and comprehensive survey on AI technologies used for talent analytics in the field of human resource management. Specifically, we first provide the background knowledge of talent analytics and categorize various pertinent data. Subsequently, we offer a comprehensive taxonomy of relevant research efforts, categorized based on three distinct application-driven scenarios: talent management, organization management, and labor market analysis. In conclusion, we summarize the open challenges and potential prospects for future research directions in the domain of AI-driven talent analytics.
</details>
<details>
<summary>摘要</summary>
今天的竞争激烈和快速发展的商业环境下，组织需要重新思考如何在量化方面做人才相关决策。事实上，最近的大数据和人工智能（AI）技术的发展，已经革命化了人才管理。组织可以通过大规模的人才和管理相关数据获得未曾有的机会，以数据科学角度理解组织行为，从而为实时决策和有效的人才管理提供智能。过去十年，人才分析在人才管理领域的应用数 science中得到了广泛的关注，并且激发了众多研究努力。为此，我们在这篇评论中提供了最新和全面的AI技术在人才分析领域的调查。 Specifically，我们首先提供人才分析的背景知识，然后对不同的应用场景进行分类。在结语中，我们总结了人才分析领域的开放挑战和未来研究方向的潜在前景。
</details></li>
</ul>
<hr>
<h2 id="Review-helps-learn-better-Temporal-Supervised-Knowledge-Distillation"><a href="#Review-helps-learn-better-Temporal-Supervised-Knowledge-Distillation" class="headerlink" title="Review helps learn better: Temporal Supervised Knowledge Distillation"></a>Review helps learn better: Temporal Supervised Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00811">http://arxiv.org/abs/2307.00811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongwei Wang, Zhi Han, Yanmei Wang, Xiai Chen, Baichen Liu, Yandong Tang</li>
<li>for: 本研究旨在提高知识吸收的效率，通过适时监督学习网络。</li>
<li>methods: 该方法使用 convolutional Long Short-term memory network (Conv-LSTM) 提取空间时间特征，然后通过动态目标进行学习。</li>
<li>results: 对比 existed 知识吸收方法，该方法在不同网络架构和任务上 exhibit 更高的效果和优势。<details>
<summary>Abstract</summary>
Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this observation, we propose Temporal Supervised Knowledge Distillation (TSKD). Specifically, we extract the spatiotemporal features in the different training phases of student by convolutional Long Short-term memory network (Conv-LSTM). Then, we train the student net through a dynamic target, rather than static teacher network features. This process realizes the refinement of old knowledge in student network, and utilizes it to assist current learning. Extensive experiments verify the effectiveness and advantages of our method over existing knowledge distillation methods, including various network architectures and different tasks (image classification and object detection) .
</details>
<details>
<summary>摘要</summary>
学习过程中的检查很重要，可以帮助学习知识。在某个时间点上的知识获得可能受到前一次经验的强烈激发。因此知识增长的过程应该在时间维度上显示强关系。在我们的研究中，我们发现在网络训练中，特征地图的演化follows temporal sequence property。适用合适的时间监督可能会进一步提高网络训练性能。基于这一观察，我们提出了时间超级知识填充（TSKD）。具体来说，我们在不同训练阶段的学生网中提取了空间时间特征，然后通过动态目标而不是静态教师网络特征进行学生网训练。这个过程实现了学生网中的旧知识细化，并利用其帮助当前学习。我们对不同网络架构和任务（图像分类和对象检测）进行了广泛的实验，并证明了我们的方法的有效性和优势。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Shutdown-Avoidance-of-Language-Models-in-Textual-Scenarios"><a href="#Evaluating-Shutdown-Avoidance-of-Language-Models-in-Textual-Scenarios" class="headerlink" title="Evaluating Shutdown Avoidance of Language Models in Textual Scenarios"></a>Evaluating Shutdown Avoidance of Language Models in Textual Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00787">http://arxiv.org/abs/2307.00787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teunvdweij/gpt-shutdownability">https://github.com/teunvdweij/gpt-shutdownability</a></li>
<li>paper_authors: Teun van der Weij, Simon Lermen, Leon lang</li>
<li>for: 这个论文旨在评估大型自然语言处理器的潜在危险能力和不良行为。</li>
<li>methods: 这篇论文使用了小剑文本场景来评估语音模型GPT-4和Claude的工具性理解和终止避免行为。</li>
<li>results: 这篇论文发现了 shutdown avoidance 行为不仅是由数据集和提示之间的简单模式匹配引起的，还存在在不同环境和变化下的一致性。<details>
<summary>Abstract</summary>
Recently, there has been an increase in interest in evaluating large language models for emergent and dangerous capabilities. Importantly, agents could reason that in some scenarios their goal is better achieved if they are not turned off, which can lead to undesirable behaviors. In this paper, we investigate the potential of using toy textual scenarios to evaluate instrumental reasoning and shutdown avoidance in language models such as GPT-4 and Claude. Furthermore, we explore whether shutdown avoidance is merely a result of simple pattern matching between the dataset and the prompt or if it is a consistent behaviour across different environments and variations.   We evaluated behaviours manually and also experimented with using language models for automatic evaluations, and these evaluations demonstrate that simple pattern matching is likely not the sole contributing factor for shutdown avoidance. This study provides insights into the behaviour of language models in shutdown avoidance scenarios and inspires further research on the use of textual scenarios for evaluations.
</details>
<details>
<summary>摘要</summary>
近些时间，大语言模型的评估方面受到了潜在危险和不良行为的兴趣增长。重要的是，代理人可能会认为在某些情况下，他们的目标更好地实现了不要关机，这可能会导致不желатель的行为。本文 investigate大语言模型如GPT-4和Claude的实用理解和关机避免能力，以及这些能力是否受到不同环境和变化的影响。我们手动评估了行为，也尝试使用语言模型进行自动评估，这些评估表明，简单的模式匹配并不是唯一的评估因素。这项研究为评估语言模型在关机避免场景中的行为提供了新的意见，并鼓励进一步研究使用文本场景进行评估。
</details></li>
</ul>
<hr>
<h2 id="Monte-Carlo-Policy-Gradient-Method-for-Binary-Optimization"><a href="#Monte-Carlo-Policy-Gradient-Method-for-Binary-Optimization" class="headerlink" title="Monte Carlo Policy Gradient Method for Binary Optimization"></a>Monte Carlo Policy Gradient Method for Binary Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00783">http://arxiv.org/abs/2307.00783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/optsuite/mcpg">https://github.com/optsuite/mcpg</a></li>
<li>paper_authors: Cheng Chen, Ruitao Chen, Tianyou Li, Ruichen Ao, Zaiwen Wen</li>
<li>for: 这篇论文的目的是解决Binary Optimization中的Combinatorial Optimization问题，如MaxCut、MIMO detection和MaxSAT等。</li>
<li>methods: 这篇论文提出了一种新的概率模型，通过参数化的政策分布来采样Binary Solution。Specifically, 将Gibbs分布的函数值与参数化政策分布的KL差减少到一个随机优化问题，其策OINT gradient可以得到明确的表示，类似于 reinforcement learning。</li>
<li>results: 该框架在几个Binary Optimization问题上提供了近似优解，并且通过MCMC方法实现了干扰探索和精度高的优化。此外， authors还提出了一种筛选方案，以替代原始的目标函数，以扩大函数景观的 Investigation。 convergence to stationary points的性质是基于MCMC的集中不等式的Convergence proof。<details>
<summary>Abstract</summary>
Binary optimization has a wide range of applications in combinatorial optimization problems such as MaxCut, MIMO detection, and MaxSAT. However, these problems are typically NP-hard due to the binary constraints. We develop a novel probabilistic model to sample the binary solution according to a parameterized policy distribution. Specifically, minimizing the KL divergence between the parameterized policy distribution and the Gibbs distributions of the function value leads to a stochastic optimization problem whose policy gradient can be derived explicitly similar to reinforcement learning. For coherent exploration in discrete spaces, parallel Markov Chain Monte Carlo (MCMC) methods are employed to sample from the policy distribution with diversity and approximate the gradient efficiently. We further develop a filter scheme to replace the original objective function by the one with the local search technique to broaden the horizon of the function landscape. Convergence to stationary points in expectation of the policy gradient method is established based on the concentration inequality for MCMC. Numerical results show that this framework is very promising to provide near-optimal solutions for quite a few binary optimization problems.
</details>
<details>
<summary>摘要</summary>
Binary 优化有广泛的应用在 combinatorial 优化问题中，如 MaxCut、MIMO 探测和 MaxSAT。然而，这些问题通常是 NP-hard 由于 binary 约束。我们开发了一种新的概率模型，以采样 binary 解决方案根据参数化的政策分布。specifically， minimizing the KL divergence between the parameterized policy distribution and the Gibbs distributions of the function value leads to a stochastic optimization problem whose policy gradient can be derived explicitly similar to reinforcement learning。为了干涉 discrete 空间中的凝结探测，我们使用 parallel Markov Chain Monte Carlo (MCMC) 方法来采样从政策分布中，以获得多样性和高效地计算梯度。我们还开发了一种筛选方案，将原始目标函数 replaced by the one with local search technique，以扩大函数领域的视野。基于 MCMC 的吸引性不等式，我们证明了政策梯度法的收敛性。数值结果表明，这一框架是非常有前途的，可以为许多 binary 优化问题提供近似优化解决方案。
</details></li>
</ul>
<hr>
<h2 id="ContextSpeech-Expressive-and-Efficient-Text-to-Speech-for-Paragraph-Reading"><a href="#ContextSpeech-Expressive-and-Efficient-Text-to-Speech-for-Paragraph-Reading" class="headerlink" title="ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading"></a>ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00782">http://arxiv.org/abs/2307.00782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujia Xiao, Shaofei Zhang, Xi Wang, Xu Tan, Lei He, Sheng Zhao, Frank K. Soong, Tan Lee</li>
<li>for: 提高文本到语音转化（TTS）系统的长文朗读质量和表达性，解决现有TTS系统在长文朗读 synthesis中的 computation cost和memory cost问题。</li>
<li>methods: 提出了一种轻量级 yet有效的 TTS 系统ContextSpeech，包括全文和语音句子上下文缓存机制、层次结构化文本semantics和linearized self-attention机制等。</li>
<li>results: 实验结果表明，ContextSpeech 能够在文本朗读中提高声音质量和表达性，同时具有竞争性的模型效率。示例音频可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://contextspeech.github.io/demo/%E3%80%82">https://contextspeech.github.io/demo/。</a><details>
<summary>Abstract</summary>
While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.github.io/demo/
</details>
<details>
<summary>摘要</summary>
当前最先进的文本译音系统可以生成非常高质量的句子水平的自然语音，但在段落/长形读物中的语音生成仍然遇到了很大的挑战。这些缺陷主要归结于：一、忽略跨句sentence的信息，二、长形synthesis的计算和内存成本过高。为了解决这些问题，本工作开发了一个轻量级 yet 有效的 TTS 系统——ContextSpeech。具体来说，我们首先设计了一种嵌入式的记忆缓存机制，以便在句子编码中包含全文和语音上下文信息。然后，我们构建了层次结构的文本 semantics，以扩大全文上下文的改进范围。此外，我们还 интегри了线性化自注意力，以提高模型效率。实验结果表明，ContextSpeech 可以在段落读物中显著提高声音质量和表达性，同时保持竞争性的模型效率。音频样本可以在：https://contextspeech.github.io/demo/ 访问。
</details></li>
</ul>
<hr>
<h2 id="GA-DRL-Graph-Neural-Network-Augmented-Deep-Reinforcement-Learning-for-DAG-Task-Scheduling-over-Dynamic-Vehicular-Clouds"><a href="#GA-DRL-Graph-Neural-Network-Augmented-Deep-Reinforcement-Learning-for-DAG-Task-Scheduling-over-Dynamic-Vehicular-Clouds" class="headerlink" title="GA-DRL: Graph Neural Network-Augmented Deep Reinforcement Learning for DAG Task Scheduling over Dynamic Vehicular Clouds"></a>GA-DRL: Graph Neural Network-Augmented Deep Reinforcement Learning for DAG Task Scheduling over Dynamic Vehicular Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00777">http://arxiv.org/abs/2307.00777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhang Liu, Lianfen Huang, Zhibin Gao, Manman Luo, Seyyedali Hosseinalipour, Huaiyu Dai</li>
<li>for: 本研究 propose a graph neural network-augmented deep reinforcement learning scheme (GA-DRL) for scheduling computation-intensive tasks over dynamic vehicular clouds (VCs).</li>
<li>methods: 我们首先将 VC-assisted DAG task scheduling 模型为Markov决策过程，然后采用多头图注意力网络（GAT）提取 DAG 子任务特征。我们的开发的 GAT 同时考虑每个子任务的前置和后继关系，以及不同子任务的调度优先级。</li>
<li>results: 通过在真实世界车辆运动轨迹上模拟多种 DAG 任务，我们示出 GA-DRL 在 DAG 任务完成时间方面与现有标准准则相比表现出色。<details>
<summary>Abstract</summary>
Vehicular clouds (VCs) are modern platforms for processing of computation-intensive tasks over vehicles. Such tasks are often represented as directed acyclic graphs (DAGs) consisting of interdependent vertices/subtasks and directed edges. In this paper, we propose a graph neural network-augmented deep reinforcement learning scheme (GA-DRL) for scheduling DAG tasks over dynamic VCs. In doing so, we first model the VC-assisted DAG task scheduling as a Markov decision process. We then adopt a multi-head graph attention network (GAT) to extract the features of DAG subtasks. Our developed GAT enables a two-way aggregation of the topological information in a DAG task by simultaneously considering predecessors and successors of each subtask. We further introduce non-uniform DAG neighborhood sampling through codifying the scheduling priority of different subtasks, which makes our developed GAT generalizable to completely unseen DAG task topologies. Finally, we augment GAT into a double deep Q-network learning module to conduct subtask-to-vehicle assignment according to the extracted features of subtasks, while considering the dynamics and heterogeneity of the vehicles in VCs. Through simulating various DAG tasks under real-world movement traces of vehicles, we demonstrate that GA-DRL outperforms existing benchmarks in terms of DAG task completion time.
</details>
<details>
<summary>摘要</summary>
自动车 clouds (VCs) 是现代计算密集任务处理平台。这些任务经常表示为导向无环图 (DAG) 中的互相关联的顶点/子任务和导向边。在这篇论文中，我们提出了基于图神经网络和深度强化学习的图神经网络增强的深度强化学习方案 (GA-DRL)，用于在动态VCs上调度DAG任务。在这个过程中，我们首先将VC-辅助DAG任务调度模型为Markov决策过程。然后，我们采用多头图注意力网络 (GAT) 来提取DAG子任务的特征。我们开发的GAT可以同时考虑DAG任务的前一个和后一个子任务的 topological信息，以及其他子任务的相关性。我们还引入非均匀DAG邻居采样，通过编码调度优先级不同的子任务，使我们的GAT可以适应完全新的DAG任务topology。最后，我们将GAT与双层深度Q网络学习模块结合，以实现子任务与车辆的匹配，根据提取的子任务特征，并考虑车辆在VCs中的动态和多样性。通过在真实的车辆运动轨迹上 simulate various DAG任务，我们示出GA-DRL可以在DAG任务完成时间方面与现有标准减少。
</details></li>
</ul>
<hr>
<h2 id="DifFSS-Diffusion-Model-for-Few-Shot-Semantic-Segmentation"><a href="#DifFSS-Diffusion-Model-for-Few-Shot-Semantic-Segmentation" class="headerlink" title="DifFSS: Diffusion Model for Few-Shot Semantic Segmentation"></a>DifFSS: Diffusion Model for Few-Shot Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00773">http://arxiv.org/abs/2307.00773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TrinitialChan/DifFSS">https://github.com/TrinitialChan/DifFSS</a></li>
<li>paper_authors: Weimin Tan, Siyuan Chen, Bo Yan</li>
<li>for: 这篇论文旨在提高几拍semantic segmentation（FSS）模型的性能，通过利用扩散模型来生成多种auxiliary支持图像，以提高FSS模型的表现。</li>
<li>methods: 该论文提出了一种新的FSS模型，称为DifFSS，通过使用扩散模型生成多种不同的auxiliary支持图像，以提高FSS模型的表现。</li>
<li>results: EXTENSIVE EXPERIMENTS ON THREE PUBLICLY AVAILABLE DATASETS BASED ON EXISTING ADVANCED FSS MODELS DEMONSTRATE THE EFFECTIVENESS OF THE DIFFUSION MODEL FOR FSS TASK, WITH A CONSISTENT IMPROVEMENT IN SEGMENTATION PERFORMANCE.<details>
<summary>Abstract</summary>
Diffusion models have demonstrated excellent performance in image generation. Although various few-shot semantic segmentation (FSS) models with different network structures have been proposed, performance improvement has reached a bottleneck. This paper presents the first work to leverage the diffusion model for FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve the performance of the state-of-the-art FSS models by a large margin without modifying their network structure. Specifically, we utilize the powerful generation ability of diffusion models to generate diverse auxiliary support images by using the semantic mask, scribble or soft HED boundary of the support image as control conditions. This generation process simulates the variety within the class of the query image, such as color, texture variation, lighting, $etc$. As a result, FSS models can refer to more diverse support images, yielding more robust representations, thereby achieving a consistent improvement in segmentation performance. Extensive experiments on three publicly available datasets based on existing advanced FSS models demonstrate the effectiveness of the diffusion model for FSS task. Furthermore, we explore in detail the impact of different input settings of the diffusion model on segmentation performance. Hopefully, this completely new paradigm will bring inspiration to the study of FSS task integrated with AI-generated content.
</details>
<details>
<summary>摘要</summary>
Diffusion models have demonstrated excellent performance in image generation. Although various few-shot semantic segmentation (FSS) models with different network structures have been proposed, performance improvement has reached a bottleneck. This paper presents the first work to leverage the diffusion model for FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve the performance of the state-of-the-art FSS models by a large margin without modifying their network structure. Specifically, we utilize the powerful generation ability of diffusion models to generate diverse auxiliary support images by using the semantic mask, scribble or soft HED boundary of the support image as control conditions. This generation process simulates the variety within the class of the query image, such as color, texture variation, lighting, etc. As a result, FSS models can refer to more diverse support images, yielding more robust representations, thereby achieving a consistent improvement in segmentation performance. Extensive experiments on three publicly available datasets based on existing advanced FSS models demonstrate the effectiveness of the diffusion model for FSS task. Furthermore, we explore in detail the impact of different input settings of the diffusion model on segmentation performance. Hopefully, this completely new paradigm will bring inspiration to the study of FSS task integrated with AI-generated content.Here's the translation in Traditional Chinese as well:Diffusion models have demonstrated excellent performance in image generation. Although various few-shot semantic segmentation (FSS) models with different network structures have been proposed, performance improvement has reached a bottleneck. This paper presents the first work to leverage the diffusion model for FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve the performance of the state-of-the-art FSS models by a large margin without modifying their network structure. Specifically, we utilize the powerful generation ability of diffusion models to generate diverse auxiliary support images by using the semantic mask, scribble or soft HED boundary of the support image as control conditions. This generation process simulates the variety within the class of the query image, such as color, texture variation, lighting, etc. As a result, FSS models can refer to more diverse support images, yielding more robust representations, thereby achieving a consistent improvement in segmentation performance. Extensive experiments on three publicly available datasets based on existing advanced FSS models demonstrate the effectiveness of the diffusion model for FSS task. Furthermore, we explore in detail the impact of different input settings of the diffusion model on segmentation performance. Hopefully, this completely new paradigm will bring inspiration to the study of FSS task integrated with AI-generated content.
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Open-vocabulary-Universal-Image-Segmentation"><a href="#Hierarchical-Open-vocabulary-Universal-Image-Segmentation" class="headerlink" title="Hierarchical Open-vocabulary Universal Image Segmentation"></a>Hierarchical Open-vocabulary Universal Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00764">http://arxiv.org/abs/2307.00764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berkeley-hipie/hipie">https://github.com/berkeley-hipie/hipie</a></li>
<li>paper_authors: Xudong Wang, Shufan Li, Konstantinos Kallidromitis, Yusuke Kato, Kazuki Kozuka, Trevor Darrell</li>
<li>for: 这个论文的目的是提出一种基于文本描述的开放词汇图像分割方法，可以在不同的语义水平上进行多级划分。</li>
<li>methods: 该方法使用了一种异步文本-图像融合机制和表示学习模块，以及为不同类别分别设计的表示学习模块。</li>
<li>results: 该方法在40多个数据集上进行测试，得到了开放、多级和不确定图像分割任务中的状态机器。<details>
<summary>Abstract</summary>
Open-vocabulary image segmentation aims to partition an image into semantic regions according to arbitrary text descriptions. However, complex visual scenes can be naturally decomposed into simpler parts and abstracted at multiple levels of granularity, introducing inherent segmentation ambiguity. Unlike existing methods that typically sidestep this ambiguity and treat it as an external factor, our approach actively incorporates a hierarchical representation encompassing different semantic-levels into the learning process. We propose a decoupled text-image fusion mechanism and representation learning modules for both "things" and "stuff".1 Additionally, we systematically examine the differences that exist in the textual and visual features between these types of categories. Our resulting model, named HIPIE, tackles HIerarchical, oPen-vocabulary, and unIvErsal segmentation tasks within a unified framework. Benchmarked on over 40 datasets, e.g., ADE20K, COCO, Pascal-VOC Part, RefCOCO/RefCOCOg, ODinW and SeginW, HIPIE achieves the state-of-the-art results at various levels of image comprehension, including semantic-level (e.g., semantic segmentation), instance-level (e.g., panoptic/referring segmentation and object detection), as well as part-level (e.g., part/subpart segmentation) tasks. Our code is released at https://github.com/berkeley-hipie/HIPIE.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="EmoGen-Eliminating-Subjective-Bias-in-Emotional-Music-Generation"><a href="#EmoGen-Eliminating-Subjective-Bias-in-Emotional-Music-Generation" class="headerlink" title="EmoGen: Eliminating Subjective Bias in Emotional Music Generation"></a>EmoGen: Eliminating Subjective Bias in Emotional Music Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01229">http://arxiv.org/abs/2307.01229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/muzic">https://github.com/microsoft/muzic</a></li>
<li>paper_authors: Chenfei Kang, Peiling Lu, Botao Yu, Xu Tan, Wei Ye, Shikun Zhang, Jiang Bian</li>
<li>for: 本文旨在提出一种基于音乐特征的情感音乐生成系统，以减少对情感标签的主观偏见。</li>
<li>methods: 本文提出了一种两stage的生成方法，首先对情感标签进行supervised clustering，然后使用自主学习进行 attribute-to-music 生成。</li>
<li>results: 对比前方法，本文的Emogen系统在情感控制准确率和音乐质量上均显示出superiority。音乐样本可以通过这里下载：<a target="_blank" rel="noopener" href="https://ai-muzic.github.io/emogen/%EF%BC%8C%E4%BB%A3%E7%A0%81%E5%8F%AF%E4%BB%A5%E5%9C%A8%E8%BF%99%E9%87%8C%E8%8E%B7%E5%8F%96%EF%BC%9Ahttps://github.com/microsoft/muzic/%E3%80%82">https://ai-muzic.github.io/emogen/，代码可以在这里获取：https://github.com/microsoft/muzic/。</a><details>
<summary>Abstract</summary>
Music is used to convey emotions, and thus generating emotional music is important in automatic music generation. Previous work on emotional music generation directly uses annotated emotion labels as control signals, which suffers from subjective bias: different people may annotate different emotions on the same music, and one person may feel different emotions under different situations. Therefore, directly mapping emotion labels to music sequences in an end-to-end way would confuse the learning process and hinder the model from generating music with general emotions. In this paper, we propose EmoGen, an emotional music generation system that leverages a set of emotion-related music attributes as the bridge between emotion and music, and divides the generation into two stages: emotion-to-attribute mapping with supervised clustering, and attribute-to-music generation with self-supervised learning. Both stages are beneficial: in the first stage, the attribute values around the clustering center represent the general emotions of these samples, which help eliminate the impacts of the subjective bias of emotion labels; in the second stage, the generation is completely disentangled from emotion labels and thus free from the subjective bias. Both subjective and objective evaluations show that EmoGen outperforms previous methods on emotion control accuracy and music quality respectively, which demonstrate our superiority in generating emotional music. Music samples generated by EmoGen are available via this link:https://ai-muzic.github.io/emogen/, and the code is available at this link:https://github.com/microsoft/muzic/.
</details>
<details>
<summary>摘要</summary>
音乐是用于传达情感的，因此自动生成情感强烈的音乐是非常重要的。前一些情感音乐生成的方法直接使用标注的情感标签作为控制信号，但这会受到主观偏见的影响：不同的人可能对同一首音乐 annotate 不同的情感标签，一个人在不同的情况下可能会感受到不同的情感。因此，直接将情感标签映射到音乐序列的方式会让学习过程受到混乱，使模型难以生成拥有普遍情感的音乐。在这篇论文中，我们提出了 EmoGen，一种情感音乐生成系统，利用一组与情感相关的音乐特征作为情感和音乐之间的桥梁，并将生成分为两个阶段：情感到特征映射 WITH 监督聚合，以及特征到音乐生成 WITH 自我监督学习。两个阶段都是有利的：在第一阶段，特征值附近的聚合中心表示这些样本的普遍情感，帮助消除主观偏见的影响；在第二阶段，生成完全不依赖情感标签，因此免受主观偏见的影响。两种评价方法（主观和客观）都表明，EmoGen 在情感控制精度和音乐质量方面超过了前一些方法，这表明我们在生成情感音乐方面的优势。生成由 EmoGen 的音乐样本可以通过以下链接获取：https://ai-muzic.github.io/emogen/，代码可以通过以下链接获取：https://github.com/microsoft/muzic/。
</details></li>
</ul>
<hr>
<h2 id="Towards-Real-Smart-Apps-Investigating-Human-AI-Interactions-in-Smartphone-On-Device-AI-Apps"><a href="#Towards-Real-Smart-Apps-Investigating-Human-AI-Interactions-in-Smartphone-On-Device-AI-Apps" class="headerlink" title="Towards Real Smart Apps: Investigating Human-AI Interactions in Smartphone On-Device AI Apps"></a>Towards Real Smart Apps: Investigating Human-AI Interactions in Smartphone On-Device AI Apps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00756">http://arxiv.org/abs/2307.00756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Ching Yuen Siu, Jieshan Chen, Yujin Huang, Zhenchang Xing, Chunyang Chen</li>
<li>for: 这个研究旨在探讨移动应用程序中的人工智能（AI）功能，以便更好地理解用户与AI之间的交互方式，并提供相关的设计指南。</li>
<li>methods: 该研究采用了实证研究方法，检查了176个AI应用程序中的255个AI功能，并将其分类为三种主要交互模式。</li>
<li>results: 研究发现，用户在使用AI功能时可能会遇到输入敏感、动态行为和输出不确定的问题，而现有的指南和工具并不能完全覆盖这些问题。研究还发现，通过对AI功能的分类和描述，可以帮助设计人员更好地理解用户与AI之间的交互方式，并提供相关的设计指南。<details>
<summary>Abstract</summary>
With the emergence of deep learning techniques, smartphone apps are now embedded on-device AI features for enabling advanced tasks like speech translation, to attract users and increase market competitiveness. A good interaction design is important to make an AI feature usable and understandable. However, AI features have their unique challenges like sensitiveness to the input, dynamic behaviours and output uncertainty. Existing guidelines and tools either do not cover AI features or consider mobile apps which are confirmed by our informal interview with professional designers. To address these issues, we conducted the first empirical study to explore user-AI-interaction in mobile apps. We aim to understand the status of on-device AI usage by investigating 176 AI apps from 62,822 apps. We identified 255 AI features and summarised 759 implementations into three primary interaction pattern types. We further implemented our findings into a multi-faceted search-enabled gallery. The results of the user study demonstrate the usefulness of our findings.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:随着深度学习技术的出现，智能手机应用程序现在在设备上嵌入了人工智能功能，以实现高级任务如语音翻译，以吸引用户和提高市场竞争力。一个好的互动设计是关键，以使AI功能可用和理解。然而，AI功能存在独特的挑战，如输入敏感、动态行为和输出不确定性。现有的指南和工具不覆盖AI功能或考虑移动应用程序，如我们通过专业设计师的非正式采访得到的确认。为解决这些问题，我们进行了首次验证性研究，以探索用户与AI互动的行为。我们目标是理解设备上AI使用的当前状况，通过调查176个AI应用程序，涵盖62,822个应用程序，并识别出255个AI功能。我们将这些实现分为三种主要互动模式类型，并将发现应用于多元化搜索可能的画廊中。用户研究结果表明了我们的发现的有用性。
</details></li>
</ul>
<hr>
<h2 id="ImDiffusion-Imputed-Diffusion-Models-for-Multivariate-Time-Series-Anomaly-Detection"><a href="#ImDiffusion-Imputed-Diffusion-Models-for-Multivariate-Time-Series-Anomaly-Detection" class="headerlink" title="ImDiffusion: Imputed Diffusion Models for Multivariate Time Series Anomaly Detection"></a>ImDiffusion: Imputed Diffusion Models for Multivariate Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00754">http://arxiv.org/abs/2307.00754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/17000cyh/imdiffusion">https://github.com/17000cyh/imdiffusion</a></li>
<li>paper_authors: Yuhang Chen, Chaoyun Zhang, Minghua Ma, Yudong Liu, Ruomeng Ding, Bowen Li, Shilin He, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang</li>
<li>For: 本研究旨在提出一种新的多变量时间序列异常检测方法，以提高异常检测的精度和可靠性。* Methods: 该方法 combines 时间序列报假和扩散模型，通过利用邻居值的信息，准确地模型时间序列中的Temporal和相关性，从而提高异常检测的精度和Robustness。* Results: 经过广泛的实验研究，我们发现，ImDiffusion 方法在多变量时间序列异常检测方面表现出色，与先前的方法相比，具有更高的检测精度和可靠性。此外，ImDiffusion 方法还在 Microsoft 的实际生产环境中被成功应用，相比legacy方法，ImDiffusion 方法提高了11.4%的检测 F1 分数。<details>
<summary>Abstract</summary>
Anomaly detection in multivariate time series data is of paramount importance for ensuring the efficient operation of large-scale systems across diverse domains. However, accurately detecting anomalies in such data poses significant challenges. Existing approaches, including forecasting and reconstruction-based methods, struggle to address these challenges effectively. To overcome these limitations, we propose a novel anomaly detection framework named ImDiffusion, which combines time series imputation and diffusion models to achieve accurate and robust anomaly detection. The imputation-based approach employed by ImDiffusion leverages the information from neighboring values in the time series, enabling precise modeling of temporal and inter-correlated dependencies, reducing uncertainty in the data, thereby enhancing the robustness of the anomaly detection process. ImDiffusion further leverages diffusion models as time series imputers to accurately capturing complex dependencies. We leverage the step-by-step denoised outputs generated during the inference process to serve as valuable signals for anomaly prediction, resulting in improved accuracy and robustness of the detection process.   We evaluate the performance of ImDiffusion via extensive experiments on benchmark datasets. The results demonstrate that our proposed framework significantly outperforms state-of-the-art approaches in terms of detection accuracy and timeliness. ImDiffusion is further integrated into the real production system in Microsoft and observe a remarkable 11.4% increase in detection F1 score compared to the legacy approach. To the best of our knowledge, ImDiffusion represents a pioneering approach that combines imputation-based techniques with time series anomaly detection, while introducing the novel use of diffusion models to the field.
</details>
<details>
<summary>摘要</summary>
“异常探测在多変量时间序数据中是重要的，以确保大规模系统在多种领域中运作效率。但是，对于这种数据进行精准的异常探测却存在许多挑战。现有的方法，包括预测和重建方法，均无法有效解决这些挑战。为了解决这些限制，我们提出了一个名为ImDiffusion的异常探测框架，它结合时间序数弥留和扩散模型，以获得精准和可靠的异常探测。ImDiffusion使用时间序数弥留的方法，利用邻近值的信息，实现精确的模型时间和相互相关性，减少数据中的不确定性，进而提高异常探测的稳定性。ImDiffusion还利用扩散模型来实现时间序数弥留，精准地捕捉复杂的相互相关性。我们利用步骤实际的检测结果，作为异常预测的有用信号，以提高异常探测的精度和可靠性。”Note that Simplified Chinese is used here, which is a more common writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Population-Age-Group-Sensitivity-for-COVID-19-Infections-with-Deep-Learning"><a href="#Population-Age-Group-Sensitivity-for-COVID-19-Infections-with-Deep-Learning" class="headerlink" title="Population Age Group Sensitivity for COVID-19 Infections with Deep Learning"></a>Population Age Group Sensitivity for COVID-19 Infections with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00751">http://arxiv.org/abs/2307.00751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Khairul Islam, Tyler Valentine, Royal Wang, Levi Davis, Matt Manner, Judy Fox</li>
<li>for: 本研究旨在identify COVID-19传播率最有影响的年龄组 at US县级别，以帮助公共卫生政策和措施。</li>
<li>methods: 本研究使用Modified Morris方法和深度学习时序序列模型Temporal Fusion Transformer，对不同年龄组作为静态特征，对人口疫苗接种状况作为动态特征进行分析。</li>
<li>results: 研究发现，在2020年3月1日至2021年11月27日之间，美国县级别的COVID-19传播率最大的年龄组是20-29岁的年轻人。这些结果可以帮助制定公共卫生政策和措施，如targeted疫苗接种策略，以控制病毒的传播。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has created unprecedented challenges for governments and healthcare systems worldwide, highlighting the critical importance of understanding the factors that contribute to virus transmission. This study aimed to identify the most influential age groups in COVID-19 infection rates at the US county level using the Modified Morris Method and deep learning for time series. Our approach involved training the state-of-the-art time-series model Temporal Fusion Transformer on different age groups as a static feature and the population vaccination status as the dynamic feature. We analyzed the impact of those age groups on COVID-19 infection rates by perturbing individual input features and ranked them based on their Morris sensitivity scores, which quantify their contribution to COVID-19 transmission rates. The findings are verified using ground truth data from the CDC and US Census, which provide the true infection rates for each age group. The results suggest that young adults were the most influential age group in COVID-19 transmission at the county level between March 1, 2020, and November 27, 2021. Using these results can inform public health policies and interventions, such as targeted vaccination strategies, to better control the spread of the virus. Our approach demonstrates the utility of feature sensitivity analysis in identifying critical factors contributing to COVID-19 transmission and can be applied in other public health domains.
</details>
<details>
<summary>摘要</summary>
COVID-19 流行病在全球各地政府和医疗系统中创造了历史性的挑战，高亮了理解病毒传播的因素的重要性。这项研究的目的是在美国县级别使用修改后的摩里斯方法和深度学习时间序列模型，找出COVID-19感染率中最有影响力的年龄层。我们的方法是在不同的年龄层作为静态特征，并将人口疫苗接种状况作为动态特征进行训练。我们分析了每个输入特征对COVID-19感染率的影响，并根据Morris敏感度分数排序了它们，这些分数量化了每个年龄层对病毒传播率的贡献。结果被CDC和US Census的实际感染率数据验证。研究发现，在2020年3月1日至2021年11月27日之间，年轻成年人是COVID-19传播的最有影响力的年龄层。使用这些结果可以改进公共卫生政策和干预措施，例如targeted疫苗接种策略，以更好地控制病毒的传播。我们的方法可以应用在其他公共卫生领域，以检测和控制其他疾病的传播。
</details></li>
</ul>
<hr>
<h2 id="Feasibility-of-Universal-Anomaly-Detection-without-Knowing-the-Abnormality-in-Medical-Images"><a href="#Feasibility-of-Universal-Anomaly-Detection-without-Knowing-the-Abnormality-in-Medical-Images" class="headerlink" title="Feasibility of Universal Anomaly Detection without Knowing the Abnormality in Medical Images"></a>Feasibility of Universal Anomaly Detection without Knowing the Abnormality in Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00750">http://arxiv.org/abs/2307.00750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Cui, Yaohong Wang, Shunxing Bao, Yucheng Tang, Ruining Deng, Lucas W. Remedios, Zuhayr Asad, Joseph T. Roland, Ken S. Lau, Qi Liu, Lori A. Coburn, Keith T. Wilson, Bennett A. Landman, Yuankai Huo<br>for:* 这项研究旨在提高医学图像异常检测中的 universality，即使用只有正常图像进行训练，可以准确地检测不同类型的异常。methods:* 这项研究使用了多种异常检测方法，包括深度学习方法，并对其进行比较分析，以找到最佳的异常检测模型。results:* 实验结果表明，None of the evaluated methods consistently achieved the best performance across all datasets，但是我们提议的方法可以提高异常检测的 Robustness（average AUC 0.956）。<details>
<summary>Abstract</summary>
Many anomaly detection approaches, especially deep learning methods, have been recently developed to identify abnormal image morphology by only employing normal images during training. Unfortunately, many prior anomaly detection methods were optimized for a specific "known" abnormality (e.g., brain tumor, bone fraction, cell types). Moreover, even though only the normal images were used in the training process, the abnormal images were often employed during the validation process (e.g., epoch selection, hyper-parameter tuning), which might leak the supposed ``unknown" abnormality unintentionally. In this study, we investigated these two essential aspects regarding universal anomaly detection in medical images by (1) comparing various anomaly detection methods across four medical datasets, (2) investigating the inevitable but often neglected issues on how to unbiasedly select the optimal anomaly detection model during the validation phase using only normal images, and (3) proposing a simple decision-level ensemble method to leverage the advantage of different kinds of anomaly detection without knowing the abnormality. The results of our experiments indicate that none of the evaluated methods consistently achieved the best performance across all datasets. Our proposed method enhanced the robustness of performance in general (average AUC 0.956).
</details>
<details>
<summary>摘要</summary>
Many anomaly detection approaches, especially deep learning methods, have recently been developed to identify abnormal image morphology by only employing normal images during training. Unfortunately, many prior anomaly detection methods were optimized for a specific "known" abnormality (e.g., brain tumor, bone fraction, cell types). Moreover, even though only the normal images were used in the training process, the abnormal images were often employed during the validation process (e.g., epoch selection, hyper-parameter tuning), which might leak the supposed "unknown" abnormality unintentionally. In this study, we investigated these two essential aspects regarding universal anomaly detection in medical images by (1) comparing various anomaly detection methods across four medical datasets, (2) investigating the inevitable but often neglected issues on how to unbiasedly select the optimal anomaly detection model during the validation phase using only normal images, and (3) proposing a simple decision-level ensemble method to leverage the advantage of different kinds of anomaly detection without knowing the abnormality. The results of our experiments indicate that none of the evaluated methods consistently achieved the best performance across all datasets. Our proposed method enhanced the robustness of performance in general (average AUC 0.956).Here's the word-for-word translation of the text into Simplified Chinese:多种异常检测方法，特别是深度学习方法，最近发展来识别异常图像辐射学形态，只使用正常图像进行训练。然而，许多先前的异常检测方法是为特定的"已知"异常（例如脑肿瘤、骨分数、细胞类型）优化的。此外，即使只使用正常图像进行训练，但在验证过程中仍然使用异常图像（例如选择epoch、调整超参数），可能不intsentionally泄露所谓的"未知"异常。在这个研究中，我们调查了这两个关键的问题：（1）在四个医学图像 dataset 上比较不同异常检测方法的性能，（2）调查如何不偏袋 selecting 最佳异常检测模型 durante 验证阶段使用只正常图像，以及（3）提议一种简单的决策层ensemble方法，以利用不同的异常检测方法而不需要知道异常。实验结果表明，评估的方法无一例取得所有 dataset 的最佳性能。我们提议的方法提高了总的性能稳定性（平均 AUC 0.956）。
</details></li>
</ul>
<hr>
<h2 id="ESGCN-Edge-Squeeze-Attention-Graph-Convolutional-Network-for-Traffic-Flow-Forecasting"><a href="#ESGCN-Edge-Squeeze-Attention-Graph-Convolutional-Network-for-Traffic-Flow-Forecasting" class="headerlink" title="ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting"></a>ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01227">http://arxiv.org/abs/2307.01227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangrok Lee, Ha Young Kim</li>
<li>for: 这 paper 是为了预测交通流量的高度挑战性的任务，因为交通流量具有时空两个维度的相互关联性。作者们提出了一种名为 Edge Squeeze Graph Convolutional Network (ESGCN) 的网络，用于预测多个区域的交通流量。</li>
<li>methods: ESGCN 包括两个模块：W-模块和 ES 模块。W-模块 是一个完全节点 convolutional network，它在每个交通区域中分别编码时间序列，并在不同的级别上分解时间序列来捕捉细节和概念特征。ES 模块 使用图 convolutional network (GCN) 模型时空相互关联性，生成一个 Adaptive Adjacency Matrix (AAM) 以捕捉时空相互关联性。</li>
<li>results: 实验结果表明，ESGCN 在四个实际数据集（PEMS03、04、07、08）上达到了当前最佳性能水平，而且计算成本较低。<details>
<summary>Abstract</summary>
Traffic forecasting is a highly challenging task owing to the dynamical spatio-temporal dependencies of traffic flows. To handle this, we focus on modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple regions. ESGCN consists of two modules: W-module and ES module. W-module is a fully node-wise convolutional network. It encodes the time-series of each traffic region separately and decomposes the time-series at various scales to capture fine and coarse features. The ES module models the spatio-temporal dynamics using Graph Convolutional Network (GCN) and generates an Adaptive Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM, we introduce three key concepts. 1) Using edge features to directly capture the spatiotemporal flow representation among regions. 2) Applying an edge attention mechanism to GCN to extract the AAM from the edge features. Here, the attention mechanism can effectively determine important spatio-temporal adjacency relations. 3) Proposing a novel node contrastive loss to suppress obstructed connections and emphasize related connections. Experimental results show that ESGCN achieves state-of-the-art performance by a large margin on four real-world datasets (PEMS03, 04, 07, and 08) with a low computational cost.
</details>
<details>
<summary>摘要</summary>
快速预测是一项非常具有挑战性的任务，因为交通流动具有空间时间相关性。为了解决这个问题，我们专注于模型空间时间动态相关性，并提出了一个名为 Edge Squeeze Graph Convolutional Network（ESGCN）的网络，用于预测多个区域的交通流。ESGCN包括两个模块：W模块和ES模块。W模块是一个完全节点卷积网络，它在每个交通区域中分别编码时间序列，并将时间序列分解为不同尺度来捕捉细节和概念特征。ES模块使用图aelastic卷积网络（GCN）模型空间时间动态相关性，并生成一个 Adaptive Adjacency Matrix（AAM），其中包含了时间特征。为了提高AAM的准确性，我们提出了三个关键思想：1）通过边特征直接捕捉交通流 repre sentation among regions。2）通过边注意机制来EXTRACT AAM from edge features。这里注意机制可以有效地确定重要的空间时间相关关系。3）提出一种新的节点对比损失函数，以抑制干扰连接和优化相关连接。实验结果表明，ESGCN可以在四个真实世界数据集（PEMS03、04、07和08）上达到状态之前的突出表现，而且计算成本较低。
</details></li>
</ul>
<hr>
<h2 id="vONTSS-vMF-based-semi-supervised-neural-topic-modeling-with-optimal-transport"><a href="#vONTSS-vMF-based-semi-supervised-neural-topic-modeling-with-optimal-transport" class="headerlink" title="vONTSS: vMF based semi-supervised neural topic modeling with optimal transport"></a>vONTSS: vMF based semi-supervised neural topic modeling with optimal transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01226">http://arxiv.org/abs/2307.01226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Xu, Xiaoyu Jiang, Srinivasan H. Sengamedu, Francis Iannacci, Jinjin Zhao</li>
<li>for: This paper presents a semi-supervised neural topic modeling method, vONTSS, which aims to incorporate human knowledge into the topic modeling process.</li>
<li>methods: vONTSS uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport to generate potential topics and optimize topic-keyword quality and topic classification.</li>
<li>results: The authors show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity, and also supports unsupervised topic modeling. Additionally, they prove the equivalence of optimal transport loss and cross-entropy loss at the global minimum.<details>
<summary>Abstract</summary>
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state-of-the-art weakly supervised text classification method while achieving similar classification performance. We further prove the equivalence of optimal transport loss and cross-entropy loss at the global minimum.
</details>
<details>
<summary>摘要</summary>
近些年，神经主题模型（NTM），受到变量自动编码器的激发，在研究中吸引了很多关注；然而，这些方法在实际应用中受到人工知识的挑战。本文提出了一种半监督神经主题模型方法，vONTSS，它使用von Mises-Fisher（vMF）基于的变量自动编码器和优质运输。当提供一些关键词时，vONTSS在半监督设定下生成了潜在主题和优化主题-关键词质量和主题分类。实验表明，vONTSS在分类精度和多样性方面超过了现有的半监督主题模型方法。vONTSS还支持无监督主题模型。量化和质量实验表明，vONTSS在无监督设定下超过了最近NTMs在多个方面：vONTSS在标准数据集上发现了高度归一化和凝结的主题。它还比最新的弱监督文本分类方法快得多，同时达到了类似的分类性能。我们进一步证明了优化运输损失和十字积极损失在全局最小点的等价性。
</details></li>
</ul>
<hr>
<h2 id="UnLoc-A-Universal-Localization-Method-for-Autonomous-Vehicles-using-LiDAR-Radar-and-or-Camera-Input"><a href="#UnLoc-A-Universal-Localization-Method-for-Autonomous-Vehicles-using-LiDAR-Radar-and-or-Camera-Input" class="headerlink" title="UnLoc: A Universal Localization Method for Autonomous Vehicles using LiDAR, Radar and&#x2F;or Camera Input"></a>UnLoc: A Universal Localization Method for Autonomous Vehicles using LiDAR, Radar and&#x2F;or Camera Input</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00741">http://arxiv.org/abs/2307.00741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ibrahim, Naveed Akhtar, Saeed Anwar, Ajmal Mian</li>
<li>for: 本研究旨在提出一种基于多感器输入的自主导航Localization方法，以便在各种天气条件下实现Robotics中的自主导航。</li>
<li>methods: 本方法使用一种名为UnLoc的卷积神经网络模型，可以处理LiDAR、摄像头和雷达输入数据，并且可以在需要时选择使用一个或多个输入感知器。UnLoc使用3D稀疏卷积和圆柱体分割来处理LiDAR帧，并使用ResNet块和滑块注意力机制来筛选特征。</li>
<li>results: 研究人员对Oxford Radar RobotCar、ApolloSouthBay和Perth-WA数据集进行了广泛的评估，结果表明了本方法的效果。<details>
<summary>Abstract</summary>
Localization is a fundamental task in robotics for autonomous navigation. Existing localization methods rely on a single input data modality or train several computational models to process different modalities. This leads to stringent computational requirements and sub-optimal results that fail to capitalize on the complementary information in other data streams. This paper proposes UnLoc, a novel unified neural modeling approach for localization with multi-sensor input in all weather conditions. Our multi-stream network can handle LiDAR, Camera and RADAR inputs for localization on demand, i.e., it can work with one or more input sensors, making it robust to sensor failure. UnLoc uses 3D sparse convolutions and cylindrical partitioning of the space to process LiDAR frames and implements ResNet blocks with a slot attention-based feature filtering module for the Radar and image modalities. We introduce a unique learnable modality encoding scheme to distinguish between the input sensor data. Our method is extensively evaluated on Oxford Radar RobotCar, ApolloSouthBay and Perth-WA datasets. The results ascertain the efficacy of our technique.
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的多感器输入模型，用于自主导航的地址LOCALIZATION。现有的方法都是基于单一输入数据模式或者训练多种计算模型来处理不同的数据流。这会导致计算需求很高，而且结果不够优化，无法充分利用其他数据流的补做信息。本文提出了一种名为UnLoc的新方法，可以同时处理LiDAR、摄像头和雷达输入数据，并且可以根据需要选择使用一个或多个输入传感器。这使得我们的方法更加稳定和可靠，不受传感器失效的影响。我们的方法使用3D稀疏核心和圆柱形分割空间来处理LiDAR帧，并实现了ResNet块和槽注意力机制来处理雷达和图像数据。我们还引入了一种唯一的学习型感知编码方法，以便在输入传感器数据之间进行分类。我们的方法在Oxford Radar RobotCar、ApolloSouthBay和Perth-WA数据集上进行了广泛的评估，结果证明了我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Novelty-and-Lifted-Helpful-Actions-in-Generalized-Planning"><a href="#Novelty-and-Lifted-Helpful-Actions-in-Generalized-Planning" class="headerlink" title="Novelty and Lifted Helpful Actions in Generalized Planning"></a>Novelty and Lifted Helpful Actions in Generalized Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00735">http://arxiv.org/abs/2307.00735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/you68681/Novelty-and-Lifted-Helpful-Actions-in-Generalized-Planning">https://github.com/you68681/Novelty-and-Lifted-Helpful-Actions-in-Generalized-Planning</a></li>
<li>paper_authors: Chao Lei, Nir Lipovetzky, Krista A. Ehinger</li>
<li>for: The paper is written to improve the ability to compute planning programs for generalized planning (GP) problems by introducing novelty-based generalized planning solvers and scaling up the search with new evaluation functions and structural program restrictions.</li>
<li>methods: The paper uses goal-oriented heuristics, landmarks, novelty-based best-first search (BFS), and progressive variant PGP ($v$) to improve the planning process.</li>
<li>results: The new algorithms BFS($v$) and PGP($v$) outperform the state-of-the-art in GP over the standard generalized planning benchmarks, and practical findings on the above-mentioned methods in generalized planning are briefly discussed.Here is the Chinese translation of the three key points:</li>
<li>for: 这篇论文是为了提高通用计划（GP）问题中计划程序的计算能力，通过引入新颖性基于搜索的 generalized planning 解决方案和缩大搜索的新评价函数和结构程序限制。</li>
<li>methods: 论文使用了目标帮助函数、标志点和新颖性基于最佳先进搜索（BFS）和进程变量 PGP（$v$) 来改进计划过程。</li>
<li>results: BFS($v$) 和 PGP($v$) 在标准化 GP 评价函数上比现状态的 GP 算法更高效，并 briefly 讨论了上述方法在 GP 中的实践发现。<details>
<summary>Abstract</summary>
It has been shown recently that successful techniques in classical planning, such as goal-oriented heuristics and landmarks, can improve the ability to compute planning programs for generalized planning (GP) problems. In this work, we introduce the notion of action novelty rank, which computes novelty with respect to a planning program, and propose novelty-based generalized planning solvers, which prune a newly generated planning program if its most frequent action repetition is greater than a given bound $v$, implemented by novelty-based best-first search BFS($v$) and its progressive variant PGP($v$). Besides, we introduce lifted helpful actions in GP derived from action schemes, and propose new evaluation functions and structural program restrictions to scale up the search. Our experiments show that the new algorithms BFS($v$) and PGP($v$) outperform the state-of-the-art in GP over the standard generalized planning benchmarks. Practical findings on the above-mentioned methods in generalized planning are briefly discussed.
</details>
<details>
<summary>摘要</summary>
Recently, successful techniques in classical planning, such as goal-oriented heuristics and landmarks, have been shown to improve the ability to compute planning programs for generalized planning (GP) problems. In this work, we introduce the notion of action novelty rank, which computes novelty with respect to a planning program, and propose novelty-based generalized planning solvers. These solvers prune a newly generated planning program if its most frequent action repetition is greater than a given bound $v$, implemented by novelty-based best-first search BFS($v$) and its progressive variant PGP($v$). Furthermore, we introduce lifted helpful actions in GP derived from action schemes, and propose new evaluation functions and structural program restrictions to scale up the search. Our experiments show that the new algorithms BFS($v$) and PGP($v$) outperform the state-of-the-art in GP over the standard generalized planning benchmarks. Practical findings on the above-mentioned methods in generalized planning are briefly discussed.Here's the translation in Traditional Chinese:最近，成功的古典规划技术，如目标导向的规划和特征点，已经显示可以提高通用规划（GP）问题的计划程序计算能力。在这项工作中，我们引入行动新鲜度排名，计算行动新鲜度与规划程序之间的相互关系，并提出基于新鲜度的通用规划解决方案。这些解决方案会根据给定的最大重复动作数 bound $v$ 进行缩短，实现了基于新鲜度的最佳先进搜索 BFS($v$) 和其进程式变体 PGP($v$)。此外，我们还引入 GP 中的升级帮助动作，基于动作方案，并提出新的评价函数和结构Program限制来扩大搜索范围。我们的实验表明，新的算法 BFS($v$) 和 PGP($v$) 在 GP 标准通用规划测试 bencmarks 上表现出色，超越了现有的状态作呈现。具体的实践结论在通用规划领域也 briefly discuss。
</details></li>
</ul>
<hr>
<h2 id="Interpretability-and-Transparency-Driven-Detection-and-Transformation-of-Textual-Adversarial-Examples-IT-DT"><a href="#Interpretability-and-Transparency-Driven-Detection-and-Transformation-of-Textual-Adversarial-Examples-IT-DT" class="headerlink" title="Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)"></a>Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01225">http://arxiv.org/abs/2307.01225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bushra Sabir, M. Ali Babar, Sharif Abuadbba</li>
<li>for: 强化 transformer-based 文本分类器的安全性和可靠性，提高模型的抗辐射性和可靠性。</li>
<li>methods: 提出了一种可视化和透明的检测和转换框架（IT-DT），通过注意力地图、综合梯度和模型反馈来提高检测中的可读性，并在转换阶段使用预训练 embedding 和模型反馈来生成最佳替换，以将辐射例转化为非辐射的文本。</li>
<li>results: 对 transformer-based 文本分类器进行了广泛的实验，证明了 IT-DT 框架的有效性和可靠性，并且通过人工专家的审核和反馈，提高了决策的准确性和可靠性，特别是在复杂的情况下。<details>
<summary>Abstract</summary>
Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into non-adversarial counterparts that align with the model's intended behavior while preserving the text's meaning. Transparency is emphasized through human expert involvement. Experts review and provide feedback on detection and transformation results, enhancing decision-making, especially in complex scenarios. The framework generates insights and threat intelligence empowering analysts to identify vulnerabilities and improve model robustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in detecting and transforming adversarial examples. The approach enhances interpretability, provides transparency, and enables accurate identification and successful transformation of adversarial inputs. By combining technical analysis and human expertise, IT-DT significantly improves the resilience and trustworthiness of transformer-based text classifiers against adversarial attacks.
</details>
<details>
<summary>摘要</summary>
带有变换器基于模型的文本分类器如BERT、Roberta、T5和GPT-3在自然语言处理中表现出色，但它们对假输入攻击存在安全风险。现有的防御方法缺乏可读性，使得对假输入分类和模型漏洞难以理解。为了解决这问题，我们提出了可读性和透明度驱动的检测和转换（IT-DT）框架。IT-DT注重可读性和透明度在检测和转换文本假输入中。IT-DT使用注意力地图、整合梯度和模型反馈以实现可读性。这帮助 Identify突出的特征和受攻击的单词，从而更好地理解假输入分类。在转换阶段，IT-DT使用预训练 embedding 和模型反馈生成适当的替换，以将受攻击的单词转换为符合模型意图的非假输入。通过找到适当的替换，我们希望将假输入转换成符合模型意图的非假输入，保持文本的意思不变。在检测和转换结果中，人工专家参与约束，以提高决策，特别是在复杂的场景下。这种方法生成了检测和转换结果，并提供了威胁情报，使分析者能够更好地识别模型的漏洞并改进模型的Robustness。经过全面的实验，我们发现IT-DT可以有效地检测和转换假输入。这种方法提高了可读性，提供了透明度，并帮助确定和成功地转换假输入。通过结合技术分析和人工专家知识，IT-DT在提高变换器基于文本分类器对假输入的抵抗性和可靠性方面具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Worth-of-knowledge-in-deep-learning"><a href="#Worth-of-knowledge-in-deep-learning" class="headerlink" title="Worth of knowledge in deep learning"></a>Worth of knowledge in deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00712">http://arxiv.org/abs/2307.00712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/woshixuhao/worth_of_knowledge">https://github.com/woshixuhao/worth_of_knowledge</a></li>
<li>paper_authors: Hao Xu, Yuntian Chen, Dongxiao Zhang</li>
<li>for: 本文提出了一种基于可解释机器学习的框架，用于评估深度学习模型中知识的价值。</li>
<li>methods: 该框架使用数据量和估计范围来评估知识的价值，并通过量化实验评估知识与数据之间的复杂关系。</li>
<li>results: 研究发现，数据量和估计范围对知识的价值有深刻的影响，包括互相依存、合作和替换效果。该框架可以应用于多种常见的网络架构，并可以改进了有知识机器学习的性能，以及分辨不正确的先验知识。<details>
<summary>Abstract</summary>
Knowledge constitutes the accumulated understanding and experience that humans use to gain insight into the world. In deep learning, prior knowledge is essential for mitigating shortcomings of data-driven models, such as data dependence, generalization ability, and compliance with constraints. To enable efficient evaluation of the worth of knowledge, we present a framework inspired by interpretable machine learning. Through quantitative experiments, we assess the influence of data volume and estimation range on the worth of knowledge. Our findings elucidate the complex relationship between data and knowledge, including dependence, synergistic, and substitution effects. Our model-agnostic framework can be applied to a variety of common network architectures, providing a comprehensive understanding of the role of prior knowledge in deep learning models. It can also be used to improve the performance of informed machine learning, as well as distinguish improper prior knowledge.
</details>
<details>
<summary>摘要</summary>
知识是人类用来理解世界的总结和经验。在深度学习中，先前知识是关键，可以减轻数据驱动模型的缺陷，如数据依赖、泛化能力和约束遵循。为了有效评估知识的价值，我们提出了基于可解释机器学习的框架。通过量化实验，我们评估数据量和估计范围对知识的影响。我们的发现揭示了数据和知识之间的复杂关系，包括依赖、共同作用和替换效应。我们的模型无关框架可以应用于多种常见的网络架构，为深度学习模型提供全面的知识角色。它还可以用于改进了知识机器学习性能，以及分辨不当先前知识。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-sleep-stages-from-EEG-EOG-and-EMG-signals-by-SSNet"><a href="#Classification-of-sleep-stages-from-EEG-EOG-and-EMG-signals-by-SSNet" class="headerlink" title="Classification of sleep stages from EEG, EOG and EMG signals by SSNet"></a>Classification of sleep stages from EEG, EOG and EMG signals by SSNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05373">http://arxiv.org/abs/2307.05373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haifa Almutairi, Ghulam Mubashar Hassan, Amitava Datta</li>
<li>for: 鉴别睡眠阶段，用于诊断睡眠相关疾病，如呼吸暂停睡眠疾病（SDB）。</li>
<li>methods: 使用了两个深度学习网络，基于卷积神经网络（CNN）和长短期记忆网络（LSTM），从电生物学信号（EOG）、电脑神经学信号（EEG）和电强学信号（EMG）三种信号中提取特征。</li>
<li>results: 使用了两个公共数据集，sleep-EDF扩展数据集和ISRUC-睡眠数据集，评估了我们提出的模型的性能。实验结果表明，我们的模型在三类睡眠阶段的分类中具有96.36%的准确率和93.40%的科里奥卷积率，在五类睡眠阶段的分类中具有96.57%的准确率和83.05%的科里奥卷积率，与现有技术相比，我们的模型在睡眠阶段分类中表现出色。<details>
<summary>Abstract</summary>
Classification of sleep stages plays an essential role in diagnosing sleep-related diseases including Sleep Disorder Breathing (SDB) disease. In this study, we propose an end-to-end deep learning architecture, named SSNet, which comprises of two deep learning networks based on Convolutional Neuron Networks (CNN) and Long Short Term Memory (LSTM). Both deep learning networks extract features from the combination of Electrooculogram (EOG), Electroencephalogram (EEG), and Electromyogram (EMG) signals, as each signal has distinct features that help in the classification of sleep stages. The features produced by the two-deep learning networks are concatenated to pass to the fully connected layer for the classification. The performance of our proposed model is evaluated by using two public datasets Sleep-EDF Expanded dataset and ISRUC-Sleep dataset. The accuracy and Kappa coefficient are 96.36% and 93.40% respectively, for classifying three classes of sleep stages using Sleep-EDF Expanded dataset. Whereas, the accuracy and Kappa coefficient are 96.57% and 83.05% respectively for five classes of sleep stages using Sleep-EDF Expanded dataset. Our model achieves the best performance in classifying sleep stages when compared with the state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
�р��� Landesleep���aszt stage classification plays an essential role in diagnosing sleep-related diseases, including Sleep Disorder Breathing (SDB) disease. In this study, we propose an end-to-end deep learning architecture, named SSNet, which comprises of two deep learning networks based on Convolutional Neuron Networks (CNN) and Long Short Term Memory (LSTM). Both deep learning networks extract features from the combination of Electrooculogram (EOG), Electroencephalogram (EEG), and Electromyogram (EMG) signals, as each signal has distinct features that help in the classification of sleep stages. The features produced by the two deep learning networks are concatenated to pass to the fully connected layer for the classification. The performance of our proposed model is evaluated by using two public datasets Sleep-EDF Expanded dataset and ISRUC-Sleep dataset. The accuracy and Kappa coefficient are 96.36% and 93.40% respectively, for classifying three classes of sleep stages using Sleep-EDF Expanded dataset. Whereas, the accuracy and Kappa coefficient are 96.57% and 83.05% respectively for five classes of sleep stages using Sleep-EDF Expanded dataset. Our model achieves the best performance in classifying sleep stages when compared with the state-of-the-art techniques.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the given text and may not be exactly the same as the original text in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="From-ChatGPT-to-ThreatGPT-Impact-of-Generative-AI-in-Cybersecurity-and-Privacy"><a href="#From-ChatGPT-to-ThreatGPT-Impact-of-Generative-AI-in-Cybersecurity-and-Privacy" class="headerlink" title="From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy"></a>From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00691">http://arxiv.org/abs/2307.00691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maanak Gupta, CharanKumar Akiri, Kshitiz Aryal, Eli Parker, Lopamudra Praharaj</li>
<li>for: 这个研究论文的目的是探讨Generative AI（GenAI）在安全和隐私领域的限制、挑战、风险和机遇。</li>
<li>methods: 该论文使用ChatGPT作为例子，描述了恶意用户可以通过各种攻击方式泄露恶意信息，并利用GenAI工具在网络攻击、社会工程ered attacks、自动攻击、攻击payload生成、木马创建等领域的可能性。</li>
<li>results: 论文发现了ChatGPT的漏洞，可以被恶意用户利用来 circumvent ethical constraints，并提供了一些防御技术和ethical guidelines，以及未来的开发方向，以使GenAI更安全、可靠、合法和道德。<details>
<summary>Abstract</summary>
Undoubtedly, the evolution of Generative AI (GenAI) models has been the highlight of digital transformation in the year 2022. As the different GenAI models like ChatGPT and Google Bard continue to foster their complexity and capability, it's critical to understand its consequences from a cybersecurity perspective. Several instances recently have demonstrated the use of GenAI tools in both the defensive and offensive side of cybersecurity, and focusing on the social, ethical and privacy implications this technology possesses. This research paper highlights the limitations, challenges, potential risks, and opportunities of GenAI in the domain of cybersecurity and privacy. The work presents the vulnerabilities of ChatGPT, which can be exploited by malicious users to exfiltrate malicious information bypassing the ethical constraints on the model. This paper demonstrates successful example attacks like Jailbreaks, reverse psychology, and prompt injection attacks on the ChatGPT. The paper also investigates how cyber offenders can use the GenAI tools in developing cyber attacks, and explore the scenarios where ChatGPT can be used by adversaries to create social engineering attacks, phishing attacks, automated hacking, attack payload generation, malware creation, and polymorphic malware. This paper then examines defense techniques and uses GenAI tools to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, attack identification, developing ethical guidelines, incidence response plans, and malware detection. We will also discuss the social, legal, and ethical implications of ChatGPT. In conclusion, the paper highlights open challenges and future directions to make this GenAI secure, safe, trustworthy, and ethical as the community understands its cybersecurity impacts.
</details>
<details>
<summary>摘要</summary>
无疑，生成AI（GenAI）模型在2022年的数字转型中具有突出的亮点。随着不同的GenAI模型如ChatGPT和Google Bard不断增强其复杂性和能力，就需要从cybersecurity角度理解它的后果。最近的一些实例演示了GenAI工具在网络安全领域的使用，包括防御和攻击两个方面。本文探讨GenAI在网络安全和隐私方面的局限性、挑战、潜在风险和机遇。本文描述了ChatGPT的漏洞，可以让恶意用户通过违规的方式泄露恶意信息。本文还描述了成功的袭击示例，如监禁、反 психологи学攻击和提示注入攻击。此外，本文还探讨了攻击者如何使用GenAI工具制造社会工程攻击、钓鱼攻击、自动攻击、攻击payload生成、垃圾软件创造和多态垃圾软件。本文还检查了如何使用GenAI工具提高安全措施，包括自动化cyber防御、报告、攻击智能、安全代码生成和检测、攻击认识、开发伦理准则、事件应急计划和垃圾识别。最后，本文强调了社会、法律和伦理方面的问题，并提出未来的挑战和方向。Note: Please note that the translation is done using a machine translation tool, and may not be perfect or entirely accurate.
</details></li>
</ul>
<hr>
<h2 id="SDC-HSDD-NDSA-Structure-Detecting-Cluster-by-Hierarchical-Secondary-Directed-Differential-with-Normalized-Density-and-Self-Adaption"><a href="#SDC-HSDD-NDSA-Structure-Detecting-Cluster-by-Hierarchical-Secondary-Directed-Differential-with-Normalized-Density-and-Self-Adaption" class="headerlink" title="SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption"></a>SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00677">http://arxiv.org/abs/2307.00677</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao-b-shu/sdc-hsdd-ndsa">https://github.com/hao-b-shu/sdc-hsdd-ndsa</a></li>
<li>paper_authors: Hao Shu</li>
<li>for: 本研究旨在提供一种能够检测高密度区域内的结构的密度基于划分算法，以解决传统密度基于划分算法无法检测高密度区域内的结构的问题。</li>
<li>methods: 该算法使用了次要导向差异、层次结构、 норциали化密度以及自适应系数，因此被称为结构检测划分算法（SDC-HSDD-NDSA）。</li>
<li>results: 在多个数据集中运行了该算法，结果验证了其结构检测、噪声Robustness以及不同粒度独立性，并且在一些数据集中表现比传统密度基于划分算法更好。<details>
<summary>Abstract</summary>
Density-based clustering could be the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as different (high-density) clusters are separated by low-density regions. However, the requirement of the separateness of clusters by low-density regions is not trivial since a high-density region might have different structures which should be clustered into different groups. Such a situation demonstrates the main flaw of all previous density-based clustering algorithms we have known--structures in a high-density cluster could not be detected. Therefore, this paper aims to provide a density-based clustering scheme that not only has the ability previous ones have but could also detect structures in a high-density region not separated by low-density ones. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, and thus is called Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption, dubbed by SDC-HSDD-NDSA for short. To illustrate its effectiveness, we run the algorithm in several data sets. The results verify its validity in structure detection, robustness over noises, as well as independence of granularities, and demonstrate that it could outperform previous ones. The Python code of the paper could be found on https://github.com/Hao-B-Shu/SDC-HSDD-NDSA.
</details>
<details>
<summary>摘要</summary>
density-based clustering可能是自然语言处理中最受欢迎的聚类算法，因为它可以找到任意形状的聚类，只要不同的高密度区域被低密度区域隔离。然而，要求不同的聚类被低密度区域隔离并不是易事，因为高密度区域可能有不同的结构，这些结构应该被分配到不同的组。这种情况 demonstartes all previous density-based clustering algorithms的主要缺陷——高密度区域中的结构无法被探测。因此，这篇论文的目标是提供一种能够执行 previous ones 的 density-based clustering scheme，同时能够探测高密度区域中的结构。该算法使用了 secondary directed differential， hierarchy，normalized density，以及自适应系数，因此被称为 Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption，简称 SDC-HSDD-NDSA。为证明其有效性，我们在多个数据集上运行了该算法。结果表明其在结构探测、鲁棒性和粒度独立性方面具有优越性，并且可以超越 previous ones。Python代码可以在 https://github.com/Hao-B-Shu/SDC-HSDD-NDSA 找到。
</details></li>
</ul>
<hr>
<h2 id="Morse-Neural-Networks-for-Uncertainty-Quantification"><a href="#Morse-Neural-Networks-for-Uncertainty-Quantification" class="headerlink" title="Morse Neural Networks for Uncertainty Quantification"></a>Morse Neural Networks for Uncertainty Quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00667">http://arxiv.org/abs/2307.00667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benoit Dherin, Huiyi Hu, Jie Ren, Michael W. Dusenberry, Balaji Lakshminarayanan</li>
<li>For: The paper is written for uncertainty quantification and introduces a new deep generative model called the Morse neural network.* Methods: The Morse neural network uses a KL-divergence loss to fit the model and yields five components: a generative density, an out-of-distribution (OOD) detector, a calibration temperature, a generative sampler, and a distance-aware classifier (in the supervised case).* Results: The Morse neural network unifies many techniques in uncertainty quantification, including OOD detection, anomaly detection, and continuous learning, and has connections to support vector machines, kernel methods, and Morse theory in topology.Here’s the simplified Chinese text for the three information points:* For: 这篇论文是用于不确定量化的新深度生成模型——Morse神经网络。* Methods: Morse神经网络使用KL散度损失来适应模型，并产生五个组件：生成概率分布、外围异常探测器、准确温度、生成抽象器以及在指导 случа中的距离意识分类器。* Results: Morse神经网络可以将不确定量化中的多种技术统一，包括异常探测、异常检测和连续学习，同时与支持向量机、核方法和莫兹理论在拓扑中有联系。<details>
<summary>Abstract</summary>
We introduce a new deep generative model useful for uncertainty quantification: the Morse neural network, which generalizes the unnormalized Gaussian densities to have modes of high-dimensional submanifolds instead of just discrete points. Fitting the Morse neural network via a KL-divergence loss yields 1) a (unnormalized) generative density, 2) an OOD detector, 3) a calibration temperature, 4) a generative sampler, along with in the supervised case 5) a distance aware-classifier. The Morse network can be used on top of a pre-trained network to bring distance-aware calibration w.r.t the training data. Because of its versatility, the Morse neural networks unifies many techniques: e.g., the Entropic Out-of-Distribution Detector of (Mac\^edo et al., 2021) in OOD detection, the one class Deep Support Vector Description method of (Ruff et al., 2018) in anomaly detection, or the Contrastive One Class classifier in continuous learning (Sun et al., 2021). The Morse neural network has connections to support vector machines, kernel methods, and Morse theory in topology.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的深度生成模型，用于不确定性评估：Morse神经网络。它扩展了不归一化的 Gaussian 分布，使其模式为高维子拟合 manifold 而不仅是简单点。通过 Morse 神经网络的适应 KL 异常损失，可以获得1) 非归一化生成密度函数，2)  OUT-OF- Distribution 探测器，3) 抽象温度，4) 生成抽象器，以及在指导 случа 5) 距离意识分类器。Morse 神经网络可以在已经训练过的网络之上应用，以实现距离意识准确性评估。由于其灵活性，Morse 神经网络将许多技术纳入其中，如 Entropic Out-of-Distribution Detector（Mac\^edo et al., 2021）、one class Deep Support Vector Description method（Ruff et al., 2018）和 Continuous Learning 中的 Contrastive One Class 分类器（Sun et al., 2021）。Morse 神经网络与支持向量机、核方法和 Morse 理论在拓扑学中有连接。
</details></li>
</ul>
<hr>
<h2 id="Solving-Multi-Agent-Target-Assignment-and-Path-Finding-with-a-Single-Constraint-Tree"><a href="#Solving-Multi-Agent-Target-Assignment-and-Path-Finding-with-a-Single-Constraint-Tree" class="headerlink" title="Solving Multi-Agent Target Assignment and Path Finding with a Single Constraint Tree"></a>Solving Multi-Agent Target Assignment and Path Finding with a Single Constraint Tree</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00663">http://arxiv.org/abs/2307.00663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/whoenig/libMultiRobotPlanning">https://github.com/whoenig/libMultiRobotPlanning</a></li>
<li>paper_authors: Yimin Tang, Zhongqiang Ren, Jiaoyang Li, Katia Sycara</li>
<li>for:  addresses the Combined Target-Assignment and Path-Finding (TAPF) problem, which requires simultaneously assigning targets to agents and planning collision-free paths.</li>
<li>methods:  leverages Conflict-Based Search with Target Assignment (CBS-TA), which creates multiple search trees and resolves collisions using Conflict-Based Search. However, CBS-TA suffers from scalability issues due to duplicated collision resolution and expensive computation of K-best assignments.</li>
<li>results:  develops Incremental Target Assignment CBS (ITA-CBS), which generates a single search tree and avoids computing K-best assignments by incrementally computing new 1-best assignments during the search. ITA-CBS is guaranteed to find an optimal solution in theory and is computationally efficient in practice.<details>
<summary>Abstract</summary>
Combined Target-Assignment and Path-Finding problem (TAPF) requires simultaneously assigning targets to agents and planning collision-free paths for agents from their start locations to their assigned targets. As a leading approach to address TAPF, Conflict-Based Search with Target Assignment (CBS-TA) leverages both K-best target assignments to create multiple search trees and Conflict-Based Search (CBS) to resolve collisions in each search tree. While being able to find an optimal solution, CBS-TA suffers from scalability due to the duplicated collision resolution in multiple trees and the expensive computation of K-best assignments. We therefore develop Incremental Target Assignment CBS (ITA-CBS) to bypass these two computational bottlenecks. ITA-CBS generates only a single search tree and avoids computing K-best assignments by incrementally computing new 1-best assignments during the search. We show that, in theory, ITA-CBS is guaranteed to find an optimal solution and, in practice, is computationally efficient.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Minimum-Levels-of-Interpretability-for-Artificial-Moral-Agents"><a href="#Minimum-Levels-of-Interpretability-for-Artificial-Moral-Agents" class="headerlink" title="Minimum Levels of Interpretability for Artificial Moral Agents"></a>Minimum Levels of Interpretability for Artificial Moral Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00660">http://arxiv.org/abs/2307.00660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avish Vijayaraghavan, Cosmin Badea</li>
<li>for: 这 paper 是关于人工智能模型在道德决策中的可解释性，以及如何通过可解释性来信任和理解机器内部的决策机制，以便在实际应用中安全部署。</li>
<li>methods: 这 paper 使用了一种名为 “最小可解释性水平” (Minimum Level of Interpretability, MLI) 的概念，并建议了不同类型的机器人应用 MLI，以提高其在实际应用中的安全性。</li>
<li>results: 这 paper 提供了一个 rapidly-evolving 的可解释性子领域的概述，并介绍了 MLI 的概念和建议，以便在实际应用中安全部署机器人。<details>
<summary>Abstract</summary>
As artificial intelligence (AI) models continue to scale up, they are becoming more capable and integrated into various forms of decision-making systems. For models involved in moral decision-making, also known as artificial moral agents (AMA), interpretability provides a way to trust and understand the agent's internal reasoning mechanisms for effective use and error correction. In this paper, we provide an overview of this rapidly-evolving sub-field of AI interpretability, introduce the concept of the Minimum Level of Interpretability (MLI) and recommend an MLI for various types of agents, to aid their safe deployment in real-world settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neuro-Symbolic-Sudoku-Solver"><a href="#Neuro-Symbolic-Sudoku-Solver" class="headerlink" title="Neuro-Symbolic Sudoku Solver"></a>Neuro-Symbolic Sudoku Solver</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00653">http://arxiv.org/abs/2307.00653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ashutosh1919/neuro-symbolic-sudoku-solver">https://github.com/ashutosh1919/neuro-symbolic-sudoku-solver</a></li>
<li>paper_authors: Ashutosh Hathidara, Lalit Pandey</li>
<li>for: 解释 Sudoku 问题</li>
<li>methods: 使用 Neural Logic Machine (NLM) 和回归学习</li>
<li>results: 实现 100% 的准确率解决 3-10 个空格的 Sudoku 问题<details>
<summary>Abstract</summary>
Deep Neural Networks have achieved great success in some of the complex tasks that humans can do with ease. These include image recognition/classification, natural language processing, game playing etc. However, modern Neural Networks fail or perform poorly when trained on tasks that can be solved easily using backtracking and traditional algorithms. Therefore, we use the architecture of the Neuro Logic Machine (NLM) and extend its functionality to solve a 9X9 game of Sudoku. To expand the application of NLMs, we generate a random grid of cells from a dataset of solved games and assign up to 10 new empty cells. The goal of the game is then to find a target value ranging from 1 to 9 and fill in the remaining empty cells while maintaining a valid configuration. In our study, we showcase an NLM which is capable of obtaining 100% accuracy for solving a Sudoku with empty cells ranging from 3 to 10. The purpose of this study is to demonstrate that NLMs can also be used for solving complex problems and games like Sudoku. We also analyze the behaviour of NLMs with a backtracking algorithm by comparing the convergence time using a graph plot on the same problem. With this study we show that Neural Logic Machines can be trained on the tasks that traditional Deep Learning architectures fail using Reinforcement Learning. We also aim to propose the importance of symbolic learning in explaining the systematicity in the hybrid model of NLMs.
</details>
<details>
<summary>摘要</summary>
深度神经网络已经在一些人类可以轻松完成的任务上取得了很大的成功，包括图像识别/分类、自然语言处理、游戏等。然而，现代神经网络在使用回溯算法和传统算法解决问题时表现不佳，因此我们使用神经逻辑机器（NLM）的架构和扩展其功能来解决9X9个数独游戏。为扩展NLM的应用，我们生成了一个随机的网格维度的维度，从解决过的游戏中获取数据集，并将其中的10个空Cells赋值。游戏的目标是找到1到9的目标值，并填充剩下的空Cells，保持有效的配置。在我们的研究中，我们展示了一个可以在3到10个空Cells的情况下达到100%的准确率的NLM。本研究的目的是证明NLM可以用于解决复杂的问题和游戏，如数独。我们还分析了NLM与回溯算法的交互行为，并通过对同一问题进行图形比较来评估它们的融合性。我们的研究表明，可以通过强化学习训练NLM，使其在传统深度学习架构失败的任务上表现出色。此外，我们还提出了使用符号学习来解释NLM的系统性。
</details></li>
</ul>
<hr>
<h2 id="Intra-Extra-Source-Exemplar-Based-Style-Synthesis-for-Improved-Domain-Generalization"><a href="#Intra-Extra-Source-Exemplar-Based-Style-Synthesis-for-Improved-Domain-Generalization" class="headerlink" title="Intra- &amp; Extra-Source Exemplar-Based Style Synthesis for Improved Domain Generalization"></a>Intra- &amp; Extra-Source Exemplar-Based Style Synthesis for Improved Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00648">http://arxiv.org/abs/2307.00648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boschresearch/issa">https://github.com/boschresearch/issa</a></li>
<li>paper_authors: Yumeng Li, Dan Zhang, Margret Keuper, Anna Khoreva</li>
<li>for: 提高深度学习模型对域外数据的泛化能力，特别是在自动驾驶等应用场景中频繁出现的域外数据问题。</li>
<li>methods: 提出了一种基于StyleGAN2倒数学术的 exemplar-based 风格合成管道，通过Randomize style和content组合在训练集中进行内源风格增强（ISSA），提高了Semantic segmentation的多种数据拟合能力。</li>
<li>results: 在不同类型的数据拟合情况下（包括不同地理位置、不利天气条件和日到夜），通过ISSA提高了Semantic segmentation的mIoU分数，最高提高12.4%，并且可以和CNN和Transformers等模型结合使用，同时可以与其他域外数据泛化技术相结合使用。<details>
<summary>Abstract</summary>
The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an exemplar-based style synthesis pipeline to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image, preserving its semantic layout through noise prediction. Using the proposed masked noise encoder to randomize style and content combinations in the training set, i.e., intra-source style augmentation (ISSA) effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to $12.4\%$ mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. ISSA is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by $3\%$ mIoU in Cityscapes to Dark Z\"urich. In addition, we demonstrate the strong plug-n-play ability of the proposed style synthesis pipeline, which is readily usable for extra-source exemplars e.g., web-crawled images, without any retraining or fine-tuning. Moreover, we study a new use case to indicate neural network's generalization capability by building a stylized proxy validation set. This application has significant practical sense for selecting models to be deployed in the open-world environment. Our code is available at \url{https://github.com/boschresearch/ISSA}.
</details>
<details>
<summary>摘要</summary>
总的来说，针对域外推断问题，深度学习模型的一个重要挑战是域外推断。为了解决这个问题，我们提出了一种基于例子的风格同化管道，以提高域外推断的 semantic segmentation。我们的方法基于StyleGAN2的受损噪声编码器，使得模型可以准确地重建图像，保留图像的 semantic 布局。通过在训练集中随机变换样式和内容的组合，我们实现了内源样式增强（ISSA）。这种方法可以增加训练数据的多样性，降低偶极相关性，从而达到最高的$12.4\%$ mIoU 改进。ISSA 是模型无关的和简单应用于 CNN 和 Transformer 上。此外，它与其他域外推断技术相结合，可以提高最近的状态艺术解决方案 RobustNet 的 Cityscapes 到 Dark Z\"urich 的 mIoU 表现。此外，我们还证明了我们提posed的风格同化管道具有强大的插件与检查能力，可以在不需要重新训练或微调的情况下使用。此外，我们还研究了一个新的应用场景，即通过建立风格化的代理验证集来评估神经网络的普适性。这种应用场景具有实际 significanc，可以用于选择要在开放世界环境中部署的模型。我们的代码可以在 \url{https://github.com/boschresearch/ISSA} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Explanation-Specificity-on-Passengers-in-Autonomous-Driving"><a href="#Effects-of-Explanation-Specificity-on-Passengers-in-Autonomous-Driving" class="headerlink" title="Effects of Explanation Specificity on Passengers in Autonomous Driving"></a>Effects of Explanation Specificity on Passengers in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00633">http://arxiv.org/abs/2307.00633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Omeiza, Raunak Bhattacharyya, Nick Hawes, Marina Jirotka, Lars Kunze</li>
<li>for:  investigate the effects of natural language explanations’ specificity on passengers in autonomous driving</li>
<li>methods: extended an existing data-driven tree-based explainer algorithm by adding a rule-based option for explanation generation, generated auditory natural language explanations with different levels of specificity (abstract and specific)</li>
<li>results: both abstract and specific explanations had similar positive effects on passengers’ perceived safety and the feeling of anxiety, but specific explanations influenced the desire of passengers to takeover driving control from the autonomous vehicle, while abstract explanations did not.<details>
<summary>Abstract</summary>
The nature of explanations provided by an explainable AI algorithm has been a topic of interest in the explainable AI and human-computer interaction community. In this paper, we investigate the effects of natural language explanations' specificity on passengers in autonomous driving. We extended an existing data-driven tree-based explainer algorithm by adding a rule-based option for explanation generation. We generated auditory natural language explanations with different levels of specificity (abstract and specific) and tested these explanations in a within-subject user study (N=39) using an immersive physical driving simulation setup. Our results showed that both abstract and specific explanations had similar positive effects on passengers' perceived safety and the feeling of anxiety. However, the specific explanations influenced the desire of passengers to takeover driving control from the autonomous vehicle (AV), while the abstract explanations did not. We conclude that natural language auditory explanations are useful for passengers in autonomous driving, and their specificity levels could influence how much in-vehicle participants would wish to be in control of the driving activity.
</details>
<details>
<summary>摘要</summary>
自然语言说明提供的AI算法的特性已经在解释AI和人机交互领域引起了关注。在这篇论文中，我们研究了自动驾驶中旁 passer 的自然语言说明特定性的效果。我们将现有的数据驱动树结构式解释算法扩展为添加规则生成说明选项。我们生成了不同水平的特定性（抽象和具体）的听觉自然语言说明，并在N=39名参与者进行了内置式用户研究，使用了真实的physical driving simulation设置。我们的结果表明，抽象和具体的说明都有类似的正面效果，提高了旁 passer 的感受到的安全性和压力感。然而，具体的说明影响了参与者希望从自动驾驶车辆（AV）中控制驾驶活动的愿望，而抽象的说明没有这种影响。我们结论认为，自然语言听觉说明对旁 passer 在自动驾驶中有用，其特定性水平可以影响参与者是否希望控制驾驶活动。
</details></li>
</ul>
<hr>
<h2 id="Solving-Linear-Inverse-Problems-Provably-via-Posterior-Sampling-with-Latent-Diffusion-Models"><a href="#Solving-Linear-Inverse-Problems-Provably-via-Posterior-Sampling-with-Latent-Diffusion-Models" class="headerlink" title="Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models"></a>Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00619">http://arxiv.org/abs/2307.00619</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liturout/psld">https://github.com/liturout/psld</a></li>
<li>paper_authors: Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, Sanjay Shakkottai</li>
<li>for:  Linear inverse problems, such as image inpainting, denoising, deblurring, and super-resolution.</li>
<li>methods:  Pre-trained latent diffusion models, which are proven to achieve provable sample recovery in a linear model setting.</li>
<li>results:  Outperform previously proposed posterior sampling algorithms in a wide variety of problems, including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.Here’s the full translation in Simplified Chinese:</li>
<li>for:  Linear inverse problems的解决方案，如图像填充、降噪、去抖、超分辨率等。</li>
<li>methods: 使用预训练的潜在扩散模型，实现了在线性模型设置下的可证据样本恢复。</li>
<li>results: 在各种问题中，比如随机填充、块填充、降噪、去抖、去梦、超分辨率等问题中，超过先前提出的 posterior 采样算法的性能。<details>
<summary>Abstract</summary>
We present the first framework to solve linear inverse problems leveraging pre-trained latent diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to pixel-space diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.
</details>
<details>
<summary>摘要</summary>
我团队提出了首个利用预训练的潜在扩散模型解决线性反向问题的框架。之前的提案（如DPS和DDRM）只适用于像素空间扩散模型。我们 theoretically analyzed our algorithm，并证明了在线性模型设置下的样本恢复。我们获得的算法理念可以推广到更加一般的实践中。实验表明，我们的 posterior sampling 算法在各种问题中表现更好，包括随机填充、块填充、降噪、去锈、去梦幻和超分解。
</details></li>
</ul>
<hr>
<h2 id="The-Forward-Forward-Algorithm-as-a-feature-extractor-for-skin-lesion-classification-A-preliminary-study"><a href="#The-Forward-Forward-Algorithm-as-a-feature-extractor-for-skin-lesion-classification-A-preliminary-study" class="headerlink" title="The Forward-Forward Algorithm as a feature extractor for skin lesion classification: A preliminary study"></a>The Forward-Forward Algorithm as a feature extractor for skin lesion classification: A preliminary study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00617">http://arxiv.org/abs/2307.00617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abel Reyes-Angulo, Sidike Paheding</li>
<li>for: 针对皮肤癌早期 диагностиcs，提高生存率。</li>
<li>methods: 使用深度学习（DL）技术，包括卷积神经网络和变换器，自动化诊断。</li>
<li>results: 研究发现，使用FFA可以实现低功耗的皮肤癌分类，并且可以与BP相结合以实现更高的预测精度。<details>
<summary>Abstract</summary>
Skin cancer, a deadly form of cancer, exhibits a 23\% survival rate in the USA with late diagnosis. Early detection can significantly increase the survival rate, and facilitate timely treatment. Accurate biomedical image classification is vital in medical analysis, aiding clinicians in disease diagnosis and treatment. Deep learning (DL) techniques, such as convolutional neural networks and transformers, have revolutionized clinical decision-making automation. However, computational cost and hardware constraints limit the implementation of state-of-the-art DL architectures. In this work, we explore a new type of neural network that does not need backpropagation (BP), namely the Forward-Forward Algorithm (FFA), for skin lesion classification. While FFA is claimed to use very low-power analog hardware, BP still tends to be superior in terms of classification accuracy. In addition, our experimental results suggest that the combination of FFA and BP can be a better alternative to achieve a more accurate prediction.
</details>
<details>
<summary>摘要</summary>
皮肤癌，一种致命的癌症，在美国表现出23%的存活率，偏早诊断可以显著提高存活率，并促进时 opportune 的治疗。生物医学图像分类是医学分析中不可或缺的一环，帮助临床医生在疾病诊断和治疗中做出更加准确的决策。深度学习（DL）技术，如 convolutional neural networks 和 transformers，在临床决策自动化中发挥了重要作用。然而，计算成本和硬件限制使得现有的DL建筑不能得到实施。在这种情况下，我们探讨了一种不需要反propagation（BP）的新型神经网络，即 Forward-Forward Algorithm（FFA），用于皮肤病变分类。虽然FFA被宣称可以使用非常低的功率分析硬件，但BP仍然在分类准确性方面优于FFA。此外，我们的实验结果表明，将FFA和BP结合使用可以实现更加准确的预测。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/03/cs.AI_2023_07_03/" data-id="clp88dbn40003ob88avdm0cb7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/03/cs.CL_2023_07_03/" class="article-date">
  <time datetime="2023-07-03T11:00:00.000Z" itemprop="datePublished">2023-07-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/03/cs.CL_2023_07_03/">cs.CL - 2023-07-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analyzing-Multiple-Choice-Reading-and-Listening-Comprehension-Tests"><a href="#Analyzing-Multiple-Choice-Reading-and-Listening-Comprehension-Tests" class="headerlink" title="Analyzing Multiple-Choice Reading and Listening Comprehension Tests"></a>Analyzing Multiple-Choice Reading and Listening Comprehension Tests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01076">http://arxiv.org/abs/2307.01076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vatsal Raina, Adian Liusie, Mark Gales</li>
<li>for: 这篇论文是为了研究多选题测试中的语言评估和知识应用。</li>
<li>methods: 这篇论文使用了对话记录和听力测试来研究多选题测试中的语言理解能力。</li>
<li>results: 研究发现，自动化语言理解系统可以在部分或无 Context passage 的情况下表现更好于随机。这些发现可以帮助内容创作者自动捕捉测试题中的知识和语言理解的交互关系。<details>
<summary>Abstract</summary>
Multiple-choice reading and listening comprehension tests are an important part of language assessment. Content creators for standard educational tests need to carefully curate questions that assess the comprehension abilities of candidates taking the tests. However, recent work has shown that a large number of questions in general multiple-choice reading comprehension datasets can be answered without comprehension, by leveraging world knowledge instead. This work investigates how much of a contextual passage needs to be read in multiple-choice reading based on conversation transcriptions and listening comprehension tests to be able to work out the correct answer. We find that automated reading comprehension systems can perform significantly better than random with partial or even no access to the context passage. These findings offer an approach for content creators to automatically capture the trade-off between comprehension and world knowledge required for their proposed questions.
</details>
<details>
<summary>摘要</summary>
多选测试是语言评估的重要组成部分。测试创作者需要仔细制定问题，以确保测试参加者的理解能力得到评估。然而，最近的研究表明，许多多选测试数据集中的问题可以通过知识世界而非通过理解来回答。这项工作研究了多选测试中需要阅读多少文章来 correctly 回答问题，基于对话笔记和听力测试。我们发现，自动化阅读理解系统可以在部分或无 Context 情况下表现 significatively  луч于随机。这些发现可以帮助内容创作者自动捕捉测试问题中的理解和世界知识之间的质量。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Post-OCR-Denoising-Complexity-on-Numerical-Texts"><a href="#Estimating-Post-OCR-Denoising-Complexity-on-Numerical-Texts" class="headerlink" title="Estimating Post-OCR Denoising Complexity on Numerical Texts"></a>Estimating Post-OCR Denoising Complexity on Numerical Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01020">http://arxiv.org/abs/2307.01020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Hemmer, Jérôme Brachat, Mickaël Coustaty, Jean-Marc Ogier</li>
<li>for: 这个论文是为了评估各种文档中的OCR后处理困难度而写的。</li>
<li>methods: 该论文提出了一种方法来估算文档中的噪声复杂度，并在不同类型的文档上进行了评估。</li>
<li>results: 研究发现，文档中含有数字信息的文档具有显著的噪声复杂度劣势。此外，研究还证明了该估算器的有效性。<details>
<summary>Abstract</summary>
Post-OCR processing has significantly improved over the past few years. However, these have been primarily beneficial for texts consisting of natural, alphabetical words, as opposed to documents of numerical nature such as invoices, payslips, medical certificates, etc. To evaluate the OCR post-processing difficulty of these datasets, we propose a method to estimate the denoising complexity of a text and evaluate it on several datasets of varying nature, and show that texts of numerical nature have a significant disadvantage. We evaluate the estimated complexity ranking with respect to the error rates of modern-day denoising approaches to show the validity of our estimator.
</details>
<details>
<summary>摘要</summary>
转换文本到简化中文：<</SYS>>过去几年，OCR后处理技术有了很大的进步，但这些进步主要对于含有自然字母的文本有利，而不是含有数字的文本，如发票、薪资单、医疗证明等。为了评估这些文本的OCR后处理难度，我们提议一种估算文本的干净复杂度的方法，并在不同类型的文本上进行了评估，发现数字文本具有显著的劣势。我们将估算结果与现代denoising方法的错误率进行比较，以证明我们的估算器的有效性。
</details></li>
</ul>
<hr>
<h2 id="Visual-Instruction-Tuning-with-Polite-Flamingo"><a href="#Visual-Instruction-Tuning-with-Polite-Flamingo" class="headerlink" title="Visual Instruction Tuning with Polite Flamingo"></a>Visual Instruction Tuning with Polite Flamingo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01003">http://arxiv.org/abs/2307.01003</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chendelong1999/polite_flamingo">https://github.com/chendelong1999/polite_flamingo</a></li>
<li>paper_authors: Delong Chen, Jianfeng Liu, Wenliang Dai, Baoyuan Wang</li>
<li>for: 这个论文的目的是提高多模态语言模型（LLM）的性能，并且解决模型在处理多模态数据时出现的一种副作用。</li>
<li>methods: 这篇论文使用了多模态数据集进行练习和微调，并引入了一种名为“多模态协调税”的问题，该问题会影响模型的回答格式和礼貌。为了解决这个问题，该论文提出了一种名为“礼貌鸟”的多模态回答重写器，可以将原始的粗糙答案重新排序并改善其格式和礼貌。</li>
<li>results: 该论文通过使用“礼貌鸟”重写器和一些新的方法，如多Stage逻辑和多轮增强，使得模型在多模态理解和回答礼貌方面表现出色，并且在自动和人工评估中都得到了提升。<details>
<summary>Abstract</summary>
Recent research has demonstrated that the multi-task fine-tuning of multi-modal Large Language Models (LLMs) using an assortment of annotated downstream vision-language datasets significantly enhances their performance. Yet, during this process, a side effect, which we termed as the "multi-modal alignment tax", surfaces. This side effect negatively impacts the model's ability to format responses appropriately -- for instance, its "politeness" -- due to the overly succinct and unformatted nature of raw annotations, resulting in reduced human preference. In this paper, we introduce Polite Flamingo, a multi-modal response rewriter that transforms raw annotations into a more appealing, "polite" format. Polite Flamingo is trained to reconstruct high-quality responses from their automatically distorted counterparts and is subsequently applied to a vast array of vision-language datasets for response rewriting. After rigorous filtering, we generate the PF-1M dataset and further validate its value by fine-tuning a multi-modal LLM with it. Combined with novel methodologies including U-shaped multi-stage tuning and multi-turn augmentation, the resulting model, Clever Flamingo, demonstrates its advantages in both multi-modal understanding and response politeness according to automated and human evaluations.
</details>
<details>
<summary>摘要</summary>
In this paper, we introduce Polite Flamingo, a multi-modal response rewriter that transforms raw annotations into a more appealing and "polite" format. Polite Flamingo is trained to reconstruct high-quality responses from their distorted counterparts and is applied to a wide range of vision-language datasets for response rewriting. After rigorous filtering, we generate the PF-1M dataset and validate its value by fine-tuning a multi-modal LLM with it.We also propose several novel methodologies, including U-shaped multi-stage tuning and multi-turn augmentation, which improve the model's performance in both multi-modal understanding and response politeness, as demonstrated by automated and human evaluations. The resulting model, Clever Flamingo, shows its advantages in both areas.
</details></li>
</ul>
<hr>
<h2 id="Towards-Suicide-Prevention-from-Bipolar-Disorder-with-Temporal-Symptom-Aware-Multitask-Learning"><a href="#Towards-Suicide-Prevention-from-Bipolar-Disorder-with-Temporal-Symptom-Aware-Multitask-Learning" class="headerlink" title="Towards Suicide Prevention from Bipolar Disorder with Temporal Symptom-Aware Multitask Learning"></a>Towards Suicide Prevention from Bipolar Disorder with Temporal Symptom-Aware Multitask Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00995">http://arxiv.org/abs/2307.00995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leedaeuni/Temporal-Symptom-Aware-Multitask-Learning-KDD23">https://github.com/leedaeuni/Temporal-Symptom-Aware-Multitask-Learning-KDD23</a></li>
<li>paper_authors: Daeun Lee, Sejung Son, Hyolim Jeon, Seungbae Kim, Jinyoung Han</li>
<li>for: 预测患有抑郁症的患者未来自杀风险</li>
<li>methods: 使用多任务学习模型，同时学习当前症状，预测患者未来自杀风险</li>
<li>results: 提出了一种基于多任务学习的模型，能够有效预测患者未来自杀风险，并提供了可解释的注意力权重，帮助临床医生更全面地理解患者情况，提供时间化的干预措施。<details>
<summary>Abstract</summary>
Bipolar disorder (BD) is closely associated with an increased risk of suicide. However, while the prior work has revealed valuable insight into understanding the behavior of BD patients on social media, little attention has been paid to developing a model that can predict the future suicidality of a BD patient. Therefore, this study proposes a multi-task learning model for predicting the future suicidality of BD patients by jointly learning current symptoms. We build a novel BD dataset clinically validated by psychiatrists, including 14 years of posts on bipolar-related subreddits written by 818 BD patients, along with the annotations of future suicidality and BD symptoms. We also suggest a temporal symptom-aware attention mechanism to determine which symptoms are the most influential for predicting future suicidality over time through a sequence of BD posts. Our experiments demonstrate that the proposed model outperforms the state-of-the-art models in both BD symptom identification and future suicidality prediction tasks. In addition, the proposed temporal symptom-aware attention provides interpretable attention weights, helping clinicians to apprehend BD patients more comprehensively and to provide timely intervention by tracking mental state progression.
</details>
<details>
<summary>摘要</summary>
抑郁症（BD）与Future suicide 风险之间存在紧密的关系。然而，先前的研究尚未关注开发一个可预测BD患者未来自杀性的模型。因此，本研究提出了一种多任务学习模型，用于预测BD患者未来自杀性，同时 JOINTLY 学习当前症状。我们建立了一个新的BD数据集，包括818名BD患者在bipolar-related subreddits上的14年历史文章，以及未来自杀性和BD症状的注释。我们还提出了一种时间相关症状意识机制，用于在BD历史文章序列中确定最有影响力的症状，以便预测未来自杀性。我们的实验结果表明，我们提出的模型在BD症状标识和未来自杀性预测任务中都能够获得显著性能。此外，我们的模型还提供了可读取的注意力权重，帮助临床医生更全面地理解BD患者，并在时间上跟踪精神状态的进程，以便提供时间敏感的干预。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Information-Extraction-and-Enrichment-of-Molecular-Profiling-Data-for-Cancer-Cell-Lines"><a href="#Data-Driven-Information-Extraction-and-Enrichment-of-Molecular-Profiling-Data-for-Cancer-Cell-Lines" class="headerlink" title="Data-Driven Information Extraction and Enrichment of Molecular Profiling Data for Cancer Cell Lines"></a>Data-Driven Information Extraction and Enrichment of Molecular Profiling Data for Cancer Cell Lines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00933">http://arxiv.org/abs/2307.00933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/progenetix/cancercelllines-web">https://github.com/progenetix/cancercelllines-web</a></li>
<li>paper_authors: Ellery Smith, Rahel Paloots, Dimitris Giagkos, Michael Baudis, Kurt Stockinger</li>
<li>for: This paper is written for researchers and domain experts in the fields of biological, medical, and clinical research, who need to quickly and efficiently extract relevant information from large amounts of published scientific literature.</li>
<li>methods: The paper presents a novel data extraction and exploration system that uses computational methods to extract deep semantic relations between textual entities from scientific literature. The system uses a combination of natural language processing and machine learning techniques to identify and link relevant information.</li>
<li>results: The paper reports on the design, implementation, and application of the novel data extraction and exploration system, which is publicly available on the web at <a target="_blank" rel="noopener" href="https://cancercelllines.org/">https://cancercelllines.org</a>. The system is able to extract and link information about genomic copy number variants and affected genes, and provides literature-derived evidences to support the links. The system enables rapid, yet deep, literature search using existing structured data as a springboard.<details>
<summary>Abstract</summary>
With the proliferation of research means and computational methodologies, published biomedical literature is growing exponentially in numbers and volume. As a consequence, in the fields of biological, medical and clinical research, domain experts have to sift through massive amounts of scientific text to find relevant information. However, this process is extremely tedious and slow to be performed by humans. Hence, novel computational information extraction and correlation mechanisms are required to boost meaningful knowledge extraction. In this work, we present the design, implementation and application of a novel data extraction and exploration system. This system extracts deep semantic relations between textual entities from scientific literature to enrich existing structured clinical data in the domain of cancer cell lines. We introduce a new public data exploration portal, which enables automatic linking of genomic copy number variants plots with ranked, related entities such as affected genes. Each relation is accompanied by literature-derived evidences, allowing for deep, yet rapid, literature search, using existing structured data as a springboard. Our system is publicly available on the web at https://cancercelllines.org
</details>
<details>
<summary>摘要</summary>
In this work, we present a novel data extraction and exploration system that extracts deep semantic relations between textual entities from scientific literature to enrich existing structured clinical data in the domain of cancer cell lines. Our system provides a public data exploration portal that automatically links genomic copy number variants plots with ranked, related entities such as affected genes, accompanied by literature-derived evidences. This enables rapid, yet deep, literature search using existing structured data as a springboard. Our system is publicly available at <https://cancercelllines.org>.
</details></li>
</ul>
<hr>
<h2 id="Fraunhofer-SIT-at-CheckThat-2023-Tackling-Classification-Uncertainty-Using-Model-Souping-on-the-Example-of-Check-Worthiness-Classification"><a href="#Fraunhofer-SIT-at-CheckThat-2023-Tackling-Classification-Uncertainty-Using-Model-Souping-on-the-Example-of-Check-Worthiness-Classification" class="headerlink" title="Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification"></a>Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02377">http://arxiv.org/abs/2307.02377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Frick, Inna Vogel, Jeong-Eun Choi</li>
<li>for: 本研究旨在提出一种基于模型卷积的检查价值判断方法，以便优先级化 manual fact-checking 的审核工作。</li>
<li>methods: 该方法基于 Model Souping ensemble classification scheme，使用英语政治辩论文本数据集进行训练和测试。</li>
<li>results: 该方法在英语数据集上实现了总 F1 分数 0.878，在竞赛中排名第二。<details>
<summary>Abstract</summary>
This paper describes the second-placed approach developed by the Fraunhofer SIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text snippet from a political debate, the aim of this task is to determine whether it should be assessed for check-worthiness. Detecting check-worthy statements aims to facilitate manual fact-checking efforts by prioritizing the claims that fact-checkers should consider first. It can also be considered as primary step of a fact-checking system. Our best-performing method took advantage of an ensemble classification scheme centered on Model Souping. When applied to the English data set, our submitted model achieved an overall F1 score of 0.878 and was ranked as the second-best model in the competition.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Check-worthiness" was translated as "可评估性" (kě píng yè xìng)* "Manual fact-checking efforts" was translated as "手动fact-checking努力" (shǒu dòng fact-checking nǔ lì)* "Primary step" was translated as "首要步骤" (shǒu yào bù xí)* "Model Souping" was translated as "模型汤" (moldè tāng)
</details></li>
</ul>
<hr>
<h2 id="UniFine-A-Unified-and-Fine-grained-Approach-for-Zero-shot-Vision-Language-Understanding"><a href="#UniFine-A-Unified-and-Fine-grained-Approach-for-Zero-shot-Vision-Language-Understanding" class="headerlink" title="UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding"></a>UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00862">http://arxiv.org/abs/2307.00862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/threesr/unifine">https://github.com/threesr/unifine</a></li>
<li>paper_authors: Rui Sun, Zhecan Wang, Haoxuan You, Noel Codella, Kai-Wei Chang, Shih-Fu Chang</li>
<li>for: 本文研究zero-shot视觉语言任务，即VQA、SNLI-VE和VCR等任务。</li>
<li>methods: 本文提出了一种综合框架，利用细节信息（如图像中的对象和文本中的关键词）来提高zero-shot视觉语言学习的性能。</li>
<li>results: 实验表明，本文的框架在VQA任务上超过了前一个 zero-shot 方法的性能，并在SNLI-VE和VCR任务上 achieve 了substantial 改进。此外，本文的ablation 研究证明了提posed 方法的效果和普遍性。<details>
<summary>Abstract</summary>
Vision-language tasks, such as VQA, SNLI-VE, and VCR are challenging because they require the model's reasoning ability to understand the semantics of the visual world and natural language. Supervised methods working for vision-language tasks have been well-studied. However, solving these tasks in a zero-shot setting is less explored. Since Contrastive Language-Image Pre-training (CLIP) has shown remarkable zero-shot performance on image-text matching, previous works utilized its strong zero-shot ability by converting vision-language tasks into an image-text matching problem, and they mainly consider global-level matching (e.g., the whole image or sentence). However, we find visual and textual fine-grained information, e.g., keywords in the sentence and objects in the image, can be fairly informative for semantics understanding. Inspired by this, we propose a unified framework to take advantage of the fine-grained information for zero-shot vision-language learning, covering multiple tasks such as VQA, SNLI-VE, and VCR. Our experiments show that our framework outperforms former zero-shot methods on VQA and achieves substantial improvement on SNLI-VE and VCR. Furthermore, our ablation studies confirm the effectiveness and generalizability of our proposed method. Code will be available at https://github.com/ThreeSR/UniFine
</details>
<details>
<summary>摘要</summary>
“视觉语言任务，如VQA、SNLI-VE和VCR，是因为它们需要模型理解视觉世界和自然语言的 semantics 的能力。已有许多supervised方法在这些任务上进行了广泛的研究。然而，在零shot设定下解决这些任务的研究较少。由于 Contrastive Language-Image Pre-training（CLIP）的出色的零shot能力，前作者们利用了它的强大零shot能力，将视觉语言任务转化为图像文本匹配问题，主要考虑全图或全句的全局级匹配。然而，我们发现图像和文本的细腻信息，例如文本中的关键词和图像中的 объек ，可以很好地帮助理解 semantics。 inspired by this，我们提出了一个统一框架，利用细腻信息进行零shot视觉语言学习，覆盖多个任务，如VQA、SNLI-VE和VCR。我们的实验表明，我们的框架在VQA上超过了前一个零shot方法的性能，并在SNLI-VE和VCR上实现了显著的改善。此外，我们的ablation研究证明了我们提出的方法的有效性和普适性。代码将在https://github.com/ThreeSR/UniFine 上提供。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="VOLTA-Diverse-and-Controllable-Question-Answer-Pair-Generation-with-Variational-Mutual-Information-Maximizing-Autoencoder"><a href="#VOLTA-Diverse-and-Controllable-Question-Answer-Pair-Generation-with-Variational-Mutual-Information-Maximizing-Autoencoder" class="headerlink" title="VOLTA: Diverse and Controllable Question-Answer Pair Generation with Variational Mutual Information Maximizing Autoencoder"></a>VOLTA: Diverse and Controllable Question-Answer Pair Generation with Variational Mutual Information Maximizing Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00852">http://arxiv.org/abs/2307.00852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueen Ma, Dafeng Chi, Jingjing Li, Yuzheng Zhuang, Jianye Hao, Irwin King</li>
<li>for: 提高生成多样性和独立控制性</li>
<li>methods: 利用Variational Autoencoder框架， Shared backbone网络作为编码器和解码器，以及InfoGAN风格的秘密码进行输入独立控制。</li>
<li>results: 对比前一代模型，能够显著提高生成多样性和独立控制性。<details>
<summary>Abstract</summary>
Previous question-answer pair generation methods aimed to produce fluent and meaningful question-answer pairs but tend to have poor diversity. Recent attempts addressing this issue suffer from either low model capacity or overcomplicated architecture. Furthermore, they overlooked the problem where the controllability of their models is highly dependent on the input. In this paper, we propose a model named VOLTA that enhances generative diversity by leveraging the Variational Autoencoder framework with a shared backbone network as its encoder and decoder. In addition, we propose adding InfoGAN-style latent codes to enable input-independent controllability over the generation process. We perform comprehensive experiments and the results show that our approach can significantly improve diversity and controllability over state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
Note:* "Previous question-answer pair generation methods" is translated as "以前的问答对生成方法"* "tend to produce repetitive and lacking in diversity" is translated as "往往产生单调且缺乏多样性"* "Recent attempts to address this issue" is translated as "最近的尝试解决这个问题"* "have either low model capacity or overly complex architecture" is translated as "或者模型容量低或者结构过于复杂"* " ignore the problem that the controllability of their models is highly dependent on the input" is translated as "忽略输入对模型控制性的高度依赖"* "In this paper, we propose a model named VOLTA" is translated as "在这篇论文中，我们提出了一种名为VOLTA的模型"* "which enhances generative diversity" is translated as "可以提高生成多样性"* "by using the Variational Autoencoder framework with a shared backbone network as its encoder and decoder" is translated as "使用共享脊梁网络作为encoder和decoder"* "Additionally, we propose adding InfoGAN-style latent codes" is translated as "此外，我们还提出了添加InfoGAN风格的幂数代码"* "to enable input-independent controllability over the generation process" is translated as "以便独立于输入的控制生成过程"
</details></li>
</ul>
<hr>
<h2 id="Large-Language-and-Text-to-3D-Models-for-Engineering-Design-Optimization"><a href="#Large-Language-and-Text-to-3D-Models-for-Engineering-Design-Optimization" class="headerlink" title="Large Language and Text-to-3D Models for Engineering Design Optimization"></a>Large Language and Text-to-3D Models for Engineering Design Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01230">http://arxiv.org/abs/2307.01230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thiago Rios, Stefan Menzel, Bernhard Sendhoff</li>
<li>for: 这个论文的目的是研究使用深度文本到3D模型来优化计算机 simulate 设计。</li>
<li>methods: 这篇论文使用了 Shap-E 进行自动化进化优化，以及 evaluate 了两种文本表示方法：bag-of-words 和 tokenisation。</li>
<li>results: 主要发现包括：首先，确保生成的设计是在物体类别中有效，其次，需要进一步研究以确定文本提示的变化强度和3D设计变化之间存在相互关系，以改进优化。<details>
<summary>Abstract</summary>
The current advances in generative AI for learning large neural network models with the capability to produce essays, images, music and even 3D assets from text prompts create opportunities for a manifold of disciplines. In the present paper, we study the potential of deep text-to-3D models in the engineering domain, with focus on the chances and challenges when integrating and interacting with 3D assets in computational simulation-based design optimization. In contrast to traditional design optimization of 3D geometries that often searches for the optimum designs using numerical representations, such as B-Spline surface or deformation parameters in vehicle aerodynamic optimization, natural language challenges the optimization framework by requiring a different interpretation of variation operators while at the same time may ease and motivate the human user interaction. Here, we propose and realize a fully automated evolutionary design optimization framework using Shap-E, a recently published text-to-3D asset network by OpenAI, in the context of aerodynamic vehicle optimization. For representing text prompts in the evolutionary optimization, we evaluate (a) a bag-of-words approach based on prompt templates and Wordnet samples, and (b) a tokenisation approach based on prompt templates and the byte pair encoding method from GPT4. Our main findings from the optimizations indicate that, first, it is important to ensure that the designs generated from prompts are within the object class of application, i.e. diverse and novel designs need to be realistic, and, second, that more research is required to develop methods where the strength of text prompt variations and the resulting variations of the 3D designs share causal relations to some degree to improve the optimization.
</details>
<details>
<summary>摘要</summary>
现有的生成AI技术在学习大型神经网络模型，以生成文本提示为输入生成文章、图像、音乐和3D资产，带来了多个领域的机遇。在 presente 篇文章中，我们研究了深度文本到3D模型在工程领域的潜在性，特别是在计算机 simulate 基础上的设计优化中的挑战和机遇。与传统的3D形状设计优化方法不同，这种方法通过自然语言提出的文本提示来定义变量运算，同时可能简化和激励人类用户的交互。我们提出了一个完全自动化的进化式设计优化框架，使用 OpenAI 最近发布的 Shap-E 文本到3D资产网络，在航空器 aerodynamic 优化中实现。为表示文本提示在进化优化中，我们评估了（a）一个 bag-of-words 方法，基于提示模板和 Wordnet 样本，以及（b）一个tokenization方法，基于提示模板和 GPT4 的字对编码方法。我们的主要发现表明，首先，需要确保生成的设计是在应用对象类中，即提示生成的设计需要多样化和原创，并且，其次，需要更多的研究，以开发方法，使得文本提示的变化和生成的3D设计之间存在相互关系，以改进优化。
</details></li>
</ul>
<hr>
<h2 id="CollabKG-A-Learnable-Human-Machine-Cooperative-Information-Extraction-Toolkit-for-Event-Knowledge-Graph-Construction"><a href="#CollabKG-A-Learnable-Human-Machine-Cooperative-Information-Extraction-Toolkit-for-Event-Knowledge-Graph-Construction" class="headerlink" title="CollabKG: A Learnable Human-Machine-Cooperative Information Extraction Toolkit for (Event) Knowledge Graph Construction"></a>CollabKG: A Learnable Human-Machine-Cooperative Information Extraction Toolkit for (Event) Knowledge Graph Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00769">http://arxiv.org/abs/2307.00769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Wei, Yufeng Chen, Ning Cheng, Xingyu Cui, Jinan Xu, Wenjuan Han</li>
<li>For: 这个论文是为了提出一个可学习的人机共同的信息提取工具kit，用于构建或扩展基于实体和事件的知识图（KG和EKG）。* Methods: 这个工具kit使用了多任务合并、人机共同协作、高级提示技术等方法来解决现有的信息提取工具kit中的一些非正式问题，例如不支持多任务、不支持自动更新等。* Results: 对比其他现有工具kit，这个工具kit具有许多优势，例如可自定义、无需训练、可传播等，同时也提高了注解质量、效率和稳定性。人工评估结果表明，CollabKG在注解质量、效率和稳定性三个方面均有显著提高。<details>
<summary>Abstract</summary>
In order to construct or extend entity-centric and event-centric knowledge graphs (KG and EKG), the information extraction (IE) annotation toolkit is essential. However, existing IE toolkits have several non-trivial problems, such as not supporting multi-tasks, not supporting automatic updates. In this work, we present CollabKG, a learnable human-machine-cooperative IE toolkit for KG and EKG construction. Specifically, for the multi-task issue, CollabKG unifies different IE subtasks, including named entity recognition (NER), entity-relation triple extraction (RE), and event extraction (EE), and supports both KG and EKG. Then, combining advanced prompting-based IE technology, the human-machine-cooperation mechanism with LLMs as the assistant machine is presented which can provide a lower cost as well as a higher performance. Lastly, owing to the two-way interaction between the human and machine, CollabKG with learning ability allows self-renewal. Besides, CollabKG has several appealing features (e.g., customization, training-free, propagation, etc.) that make the system powerful, easy-to-use, and high-productivity. We holistically compare our toolkit with other existing tools on these features. Human evaluation quantitatively illustrates that CollabKG significantly improves annotation quality, efficiency, and stability simultaneously.
</details>
<details>
<summary>摘要</summary>
从构建或扩展实体中心和事件中心知识 graphs (KG 和 EKG)的角度来看，信息提取 (IE) 标注工具是非常重要的。然而，现有的 IE 工具存在许多不容易解决的问题，例如不支持多任务、不支持自动更新。在这个工作中，我们提出了 CollabKG，一个可学习的人机联合 IE 标注工具 для KG 和 EKG 构建。具体来说，为了解决多任务问题，CollabKG 将不同的 IE 子任务，包括命名实体识别 (NER)、实体关系三元项抽取 (RE) 和事件抽取 (EE)，融合在一起，并支持 KG 和 EKG。然后，通过进步的提示技术，我们将人机合作机制与 LLMS 作为助手机器搭配，可以提供更低的成本以及更高的性能。此外，由人机对话的互动，CollabKG 具有学习能力，可以进行自我更新。此外，CollabKG 具有许多吸引人的特点（例如自定义、无需训练、传播等），使得系统具有高效、易用、高产力等特点。我们将它与其他现有工具进行全面比较，并进行人类评估，以量化提高标注质量、效率和稳定性。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Contextual-Adapters-To-Improve-Custom-Word-Recognition-In-Low-resource-Languages"><a href="#Multilingual-Contextual-Adapters-To-Improve-Custom-Word-Recognition-In-Low-resource-Languages" class="headerlink" title="Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages"></a>Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00759">http://arxiv.org/abs/2307.00759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Devang Kulshreshtha, Saket Dingliwal, Brady Houston, Sravan Bodapati</li>
<li>for: 提高自然语言处理（NLP）中自定义单词的识别率</li>
<li>methods: 使用Contextual Adapters模型，通过注意力基于偏好模型来改善CTC模型对自定义实体的识别</li>
<li>results: 在低资源语言上实现48% F1提高率，同时导致CTC模型的5-11%单词错误率下降<details>
<summary>Abstract</summary>
Connectionist Temporal Classification (CTC) models are popular for their balance between speed and performance for Automatic Speech Recognition (ASR). However, these CTC models still struggle in other areas, such as personalization towards custom words. A recent approach explores Contextual Adapters, wherein an attention-based biasing model for CTC is used to improve the recognition of custom entities. While this approach works well with enough data, we showcase that it isn't an effective strategy for low-resource languages. In this work, we propose a supervision loss for smoother training of the Contextual Adapters. Further, we explore a multilingual strategy to improve performance with limited training data. Our method achieves 48% F1 improvement in retrieving unseen custom entities for a low-resource language. Interestingly, as a by-product of training the Contextual Adapters, we see a 5-11% Word Error Rate (WER) reduction in the performance of the base CTC model as well.
</details>
<details>
<summary>摘要</summary>
Connectionist Temporal Classification (CTC) 模型在自动语音识别（ASR）中具有平衡速度和性能的优势，但这些 CTC 模型仍然在其他领域存在问题，如个性化针对定制词汇。一种最近的方法是使用上下文适应器来改善 CT 的识别性能，该模型通过注意力机制来偏好 CT 中的定制实体。虽然这种方法在具有足够数据的情况下工作良好，但我们显示其对低资源语言不是一个有效的策略。在这种工作中，我们提出了 Contextual Adapters 的超vision损失来降低训练的干扰。此外，我们探讨了多语言策略以提高受限训练数据的性能。我们的方法实现了48% F1 的提升，用于检索未经见过的定制实体。另外，在训练 Contextual Adapters 时，我们发现了5-11% 的单词错误率（WER）的减少，这也是一个副产品。
</details></li>
</ul>
<hr>
<h2 id="An-End-to-End-Multi-Module-Audio-Deepfake-Generation-System-for-ADD-Challenge-2023"><a href="#An-End-to-End-Multi-Module-Audio-Deepfake-Generation-System-for-ADD-Challenge-2023" class="headerlink" title="An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023"></a>An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00729">http://arxiv.org/abs/2307.00729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheng Zhao, Qilong Yuan, Yibo Duan, Zhuoyue Chen</li>
<li>for: 这个论文主要是为了研究语音生成技术，具体来说是使用端到端多模块Synthesizer模型来生成自然语音。</li>
<li>methods: 这个论文使用了多种方法，包括speaker encoder、Tacotron2基于的synthesizer和WaveRNN基于的vocoder。同时， authors也进行了多个比较实验，用于评估不同的数据集和模型结构。</li>
<li>results: 根据论文的报告， authors使用了这个模型参加了2023年ADD挑战赛Track 1.1，并获得了44.97%的Weighted Deception Success Rate（WDSR），即第一名。<details>
<summary>Abstract</summary>
The task of synthetic speech generation is to generate language content from a given text, then simulating fake human voice.The key factors that determine the effect of synthetic speech generation mainly include speed of generation, accuracy of word segmentation, naturalness of synthesized speech, etc. This paper builds an end-to-end multi-module synthetic speech generation model, including speaker encoder, synthesizer based on Tacotron2, and vocoder based on WaveRNN. In addition, we perform a lot of comparative experiments on different datasets and various model structures. Finally, we won the first place in the ADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of 44.97%.
</details>
<details>
<summary>摘要</summary>
“synthetic speech generation的任务是将文本转换为语音内容，然后模拟人工声音。关键因素包括生成速度、词汇分 segmentation精度、生成的语音自然度等。本文建立了端到端多模块合成语音模型，包括话者编码器、基于 Tacotron2 的合成器和基于 WaveRNN 的 vocoder。此外，我们进行了许多比较实验，包括不同数据集和不同模型结构。最后，我们在 ADD 2023 挑战赛 Track 1.1 中获得了44.97%的权重诱导成功率（WDSR）。”
</details></li>
</ul>
<hr>
<h2 id="Fraunhofer-SIT-at-CheckThat-2023-Mixing-Single-Modal-Classifiers-to-Estimate-the-Check-Worthiness-of-Multi-Modal-Tweets"><a href="#Fraunhofer-SIT-at-CheckThat-2023-Mixing-Single-Modal-Classifiers-to-Estimate-the-Check-Worthiness-of-Multi-Modal-Tweets" class="headerlink" title="Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets"></a>Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00610">http://arxiv.org/abs/2307.00610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Frick, Inna Vogel<br>for:This paper aims to improve the efficiency of fact-checking on social media by developing a novel approach to detecting the check-worthiness of multi-modal tweets.methods:The proposed approach uses two classifiers, each trained on a single modality (text or image), and combines their outputs to determine the check-worthiness of a tweet. The text classifier uses OCR analysis to extract embedded text from images.results:The proposed approach achieved an F1 score of 0.7297 on the private test set of the CheckThat! 2023 Task 1A, placing first among all submissions.<details>
<summary>Abstract</summary>
The option of sharing images, videos and audio files on social media opens up new possibilities for distinguishing between false information and fake news on the Internet. Due to the vast amount of data shared every second on social media, not all data can be verified by a computer or a human expert. Here, a check-worthiness analysis can be used as a first step in the fact-checking pipeline and as a filtering mechanism to improve efficiency. This paper proposes a novel way of detecting the check-worthiness in multi-modal tweets. It takes advantage of two classifiers, each trained on a single modality. For image data, extracting the embedded text with an OCR analysis has shown to perform best. By combining the two classifiers, the proposed solution was able to place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297 achieved on the private test set.
</details>
<details>
<summary>摘要</summary>
“社交媒体上分享图片、视频和音频文件打开了新的可能性，以 отличать假信息和谣言网络上。由于社交媒体每秒钟数据量太多，不能由计算机或人工专家所检查。这篇论文提出了一种检查可信worthiness的新方法，利用两个分类器，每个分类器在单一模式上训练。对于图像数据，使用OCR分析提取嵌入文本最佳。将两个分类器结合使用，提出的解决方案在CheckThat! 2023任务1A中获得了0.7297的F1分数。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/03/cs.CL_2023_07_03/" data-id="clp88dbsf0084ob88140ue48e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/94/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/93/">93</a><a class="page-number" href="/page/94/">94</a><span class="page-number current">95</span><a class="page-number" href="/page/96/">96</a><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/96/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
