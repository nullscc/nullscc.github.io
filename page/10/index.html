
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/10/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/eess.IV_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/06/eess.IV_2023_08_06/">eess.IV - 2023-08-06 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FourLLIE-Boosting-Low-Light-Image-Enhancement-by-Fourier-Frequency-Information"><a href="#FourLLIE-Boosting-Low-Light-Image-Enhancement-by-Fourier-Frequency-Information" class="headerlink" title="FourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information"></a>FourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03033">http://arxiv.org/abs/2308.03033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangchx67/fourllie">https://github.com/wangchx67/fourllie</a></li>
<li>paper_authors: Chenxi Wang, Hongjun Wu, Zhi Jin</li>
<li>for: 提高低光照图像的亮度和细节</li>
<li>methods: 基于快执行 fourier 变换，利用振荡频率信息和地图信息，实现低光照图像的进一步优化</li>
<li>results: 与当前最佳方法进行比较，实现了更高的亮度和细节精度，同时具有较好的模型效率<details>
<summary>Abstract</summary>
Recently, Fourier frequency information has attracted much attention in Low-Light Image Enhancement (LLIE). Some researchers noticed that, in the Fourier space, the lightness degradation mainly exists in the amplitude component and the rest exists in the phase component. By incorporating both the Fourier frequency and the spatial information, these researchers proposed remarkable solutions for LLIE. In this work, we further explore the positive correlation between the magnitude of amplitude and the magnitude of lightness, which can be effectively leveraged to improve the lightness of low-light images in the Fourier space. Moreover, we find that the Fourier transform can extract the global information of the image, and does not introduce massive neural network parameters like Multi-Layer Perceptrons (MLPs) or Transformer. To this end, a two-stage Fourier-based LLIE network (FourLLIE) is proposed. In the first stage, we improve the lightness of low-light images by estimating the amplitude transform map in the Fourier space. In the second stage, we introduce the Signal-to-Noise-Ratio (SNR) map to provide the prior for integrating the global Fourier frequency and the local spatial information, which recovers image details in the spatial space. With this ingenious design, FourLLIE outperforms the existing state-of-the-art (SOTA) LLIE methods on four representative datasets while maintaining good model efficiency.
</details>
<details>
<summary>摘要</summary>
最近，傅里叶频率信息在低光照图像增强（LLIE）中吸引了很多关注。一些研究人员注意到，在傅里叶空间中，亮度下降主要存在于振荡Component中，而剩下的存在于相位Component中。通过汇合傅里叶频率和空间信息，这些研究人员提出了非常出色的解决方案。在这项工作中，我们进一步探索了振荡幅度与亮度幅度之间的正相关关系，可以有效地提高低光照图像的亮度在傅里叶空间中。此外，我们发现傅里叶变换可以提取图像的全局信息，而不需要大量的神经网络参数，比如多层感知器（MLP）或转换器。基于这种创新的设计，我们提出了一种两个阶段的傅里叶基于LLIE网络（FourLLIE）。在第一阶段，我们使用傅里叶变换Map来提高低光照图像的亮度。在第二阶段，我们引入信噪比Map，以提供优化全局傅里叶频率和本地空间信息的优化约束，从而恢复图像的细节在空间空间中。与现有的状态的 искусственный智能（SOTA）LLIE方法相比，FourLLIE在四个代表性的数据集上达到了更高的性能，而且保持了好的模型效率。
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Spike-based-Image-Restoration-under-General-Illumination"><a href="#Recurrent-Spike-based-Image-Restoration-under-General-Illumination" class="headerlink" title="Recurrent Spike-based Image Restoration under General Illumination"></a>Recurrent Spike-based Image Restoration under General Illumination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03018">http://arxiv.org/abs/2308.03018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bit-vision/rsir">https://github.com/bit-vision/rsir</a></li>
<li>paper_authors: Lin Zhu, Yunlong Zheng, Mengyue Geng, Lizhi Wang, Hua Huang</li>
<li>for: 提高雨天或晚上场景下的激活图像重建速度</li>
<li>methods: 基于物理模型建立噪声分布，并设计了适应性激光变换模块、回归时间特征融合模块和频率域激光噪声除净模块</li>
<li>results: 对实际 dataset进行了广泛的实验，并在不同照明条件下达到了效果的重建图像Here’s a breakdown of each line:</li>
<li>for: 这篇论文的目的是提高雨天或晚上场景下的激活图像重建速度。</li>
<li>methods: 这篇论文使用了基于物理模型建立噪声分布，并设计了适应性激光变换模块、回归时间特征融合模块和频率域激光噪声除净模块来实现图像重建。</li>
<li>results: 这篇论文对实际 dataset进行了广泛的实验，并在不同照明条件下达到了效果的重建图像。<details>
<summary>Abstract</summary>
Spike camera is a new type of bio-inspired vision sensor that records light intensity in the form of a spike array with high temporal resolution (20,000 Hz). This new paradigm of vision sensor offers significant advantages for many vision tasks such as high speed image reconstruction. However, existing spike-based approaches typically assume that the scenes are with sufficient light intensity, which is usually unavailable in many real-world scenarios such as rainy days or dusk scenes. To unlock more spike-based application scenarios, we propose a Recurrent Spike-based Image Restoration (RSIR) network, which is the first work towards restoring clear images from spike arrays under general illumination. Specifically, to accurately describe the noise distribution under different illuminations, we build a physical-based spike noise model according to the sampling process of the spike camera. Based on the noise model, we design our RSIR network which consists of an adaptive spike transformation module, a recurrent temporal feature fusion module, and a frequency-based spike denoising module. Our RSIR can process the spike array in a recursive manner to ensure that the spike temporal information is well utilized. In the training process, we generate the simulated spike data based on our noise model to train our network. Extensive experiments on real-world datasets with different illuminations demonstrate the effectiveness of the proposed network. The code and dataset are released at https://github.com/BIT-Vision/RSIR.
</details>
<details>
<summary>摘要</summary>
新型生物启发式视觉传感器“穿孔相机”记录了光Intensity的形式为高时间分辨率（20,000 Hz）的脉冲数组。这种新的视觉传感器模式具有许多视觉任务的优势，如高速图像重建。然而，现有的脉冲基本approaches通常假设场景中有足够的光INTENSITY，这通常不存在在真实世界中的雨天或晚上场景。为了解锁更多的脉冲基本应用场景，我们提议了一种基于脉冲的图像修复网络（RSIR），这是首次对脉冲数组进行图像修复。 Specifically, we build a physical-based spike noise model according to the sampling process of the spike camera to accurately describe the noise distribution under different illuminations. Based on the noise model, we design our RSIR network, which consists of an adaptive spike transformation module, a recurrent temporal feature fusion module, and a frequency-based spike denoising module. Our RSIR can process the spike array in a recursive manner to ensure that the spike temporal information is well utilized. In the training process, we generate the simulated spike data based on our noise model to train our network. Extensive experiments on real-world datasets with different illuminations demonstrate the effectiveness of the proposed network.  The code and dataset are released at https://github.com/BIT-Vision/RSIR.
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Vision-Transformers-for-Pixel-Level-Identification-of-Structural-Components-and-Damage"><a href="#High-Resolution-Vision-Transformers-for-Pixel-Level-Identification-of-Structural-Components-and-Damage" class="headerlink" title="High-Resolution Vision Transformers for Pixel-Level Identification of Structural Components and Damage"></a>High-Resolution Vision Transformers for Pixel-Level Identification of Structural Components and Damage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03006">http://arxiv.org/abs/2308.03006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kareem Eltouny, Seyedomid Sajedi, Xiao Liang</li>
<li>for: 这个研究是为了提高桥梁检查的效率和可靠性，使用无人机和人工智能技术来快速和安全地进行视觉检查。</li>
<li>methods: 该研究使用了基于视Transformer和 Laplacian pyramids scaling networks的semantic segmentation网络，来高效地分析高分辨率的视觉检查图像。</li>
<li>results: 研究结果表明，该方法可以高效地分析大量的高分辨率视觉检查图像，同时保持了地方细节和全局 semantics信息，不会对计算效率造成影响。<details>
<summary>Abstract</summary>
Visual inspection is predominantly used to evaluate the state of civil structures, but recent developments in unmanned aerial vehicles (UAVs) and artificial intelligence have increased the speed, safety, and reliability of the inspection process. In this study, we develop a semantic segmentation network based on vision transformers and Laplacian pyramids scaling networks for efficiently parsing high-resolution visual inspection images. The massive amounts of collected high-resolution images during inspections can slow down the investigation efforts. And while there have been extensive studies dedicated to the use of deep learning models for damage segmentation, processing high-resolution visual data can pose major computational difficulties. Traditionally, images are either uniformly downsampled or partitioned to cope with computational demands. However, the input is at risk of losing local fine details, such as thin cracks, or global contextual information. Inspired by super-resolution architectures, our vision transformer model learns to resize high-resolution images and masks to retain both the valuable local features and the global semantics without sacrificing computational efficiency. The proposed framework has been evaluated through comprehensive experiments on a dataset of bridge inspection report images using multiple metrics for pixel-wise materials detection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用视觉检查来评估公共建筑结构，但是最新的无人机（UAV）和人工智能技术已经提高了检查过程的速度、安全性和可靠性。在这项研究中，我们开发了基于视transformer和Laplacian pyramids scaling网络的semantic segmentation网络，用于高效地分解高分辨率视检图像。收集的大量高分辨率图像可能会拖slow down调查工作，而且过去对深度学习模型用于损害分 segmentation的研究非常广泛。但是处理高分辨率视数据可以带来巨大的计算困难。传统上，图像会被uniform downsample或分割，以降低计算成本，但是输入可能会产生本地细小损害，例如细裂，或者全局Contextual信息。受到超分辨architecture的启发，我们的视transformer模型学习了resize高分辨率图像和mask，以保留valuable的本地特征和全局semantics，不会 sacrificing计算效率。我们提出的框架已经在bridge检查报告图像集上进行了完整的实验，并使用多个 метри来进行像素精度检测。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-segmentation-of-intracranial-aneurysms-using-a-3D-focal-modulation-UNet"><a href="#Weakly-supervised-segmentation-of-intracranial-aneurysms-using-a-3D-focal-modulation-UNet" class="headerlink" title="Weakly supervised segmentation of intracranial aneurysms using a 3D focal modulation UNet"></a>Weakly supervised segmentation of intracranial aneurysms using a 3D focal modulation UNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03001">http://arxiv.org/abs/2308.03001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Rasoulian, Soorena Salari, Yiming Xiao<br>for: 这 paper 的目的是提供一种高精度的自动化UIA诊断方法，以便改善Current assessment based on 2D manual measures of aneurysms on 3D MRA的评估方法。methods: 该 paper 使用了一种名为 FocalSegNet 的新型3D焦点调制UNet，以及一种名为 CRF 的后处理技术，来实现高精度的 UIA 分 segmentation。results: 该 paper 的实验结果表明，提案的算法比 state-of-the-art 3D UNet 和 Swin-UNETR 更高精度，并且 demonstarted the superiority of the proposed FocalSegNet 和 focal modulation 的 beneficial effect on the task。<details>
<summary>Abstract</summary>
Accurate identification and quantification of unruptured intracranial aneurysms (UIAs) are essential for the risk assessment and treatment decisions of this cerebrovascular disorder. Current assessment based on 2D manual measures of aneurysms on 3D magnetic resonance angiography (MRA) is sub-optimal and time-consuming. Automatic 3D measures can significantly benefit the clinical workflow and treatment outcomes. However, one major issue in medical image segmentation is the need for large well-annotated data, which can be expensive to obtain. Techniques that mitigate the requirement, such as weakly supervised learning with coarse labels are highly desirable. In this paper, we leverage coarse labels of UIAs from time-of-flight MRAs to obtain refined UIAs segmentation using a novel 3D focal modulation UNet, called FocalSegNet and conditional random field (CRF) postprocessing, with a Dice score of 0.68 and 95% Hausdorff distance of 0.95 mm. We evaluated the performance of the proposed algorithms against the state-of-the-art 3D UNet and Swin-UNETR, and demonstrated the superiority of the proposed FocalSegNet and the benefit of focal modulation for the task.
</details>
<details>
<summary>摘要</summary>
correctly 识别和量化脑血管疾病（UIAs）的精度是诊断和治疗决策的关键。现有的评估方法基于2D手动测量的感知器件图像（MRA）是下pecific和耗时consuming。自动化3D测量可以帮助优化诊断和治疗结果。然而，医疗图像分割的一个主要问题是需要大量的良好标注数据，这可以是expensive to obtain。使用弱有supervised learning with coarse labels可以减少这个问题。在这篇论文中，我们利用时间飞行扫描产生的UIAs粗略标签来获得改进的UIAs分割，使用一种新的3D焦点修饰UNet，called FocalSegNet，并与条件Random field（CRF）后处理，得到了0.68的Dice分数和0.95 mm的95% Hausdorff距离。我们评估了提议的算法与现有的3D UNet和Swin-UNETR的性能，并证明了提议的FocalSegNet的优越性和焦点修饰的好处。
</details></li>
</ul>
<hr>
<h2 id="DermoSegDiff-A-Boundary-aware-Segmentation-Diffusion-Model-for-Skin-Lesion-Delineation"><a href="#DermoSegDiff-A-Boundary-aware-Segmentation-Diffusion-Model-for-Skin-Lesion-Delineation" class="headerlink" title="DermoSegDiff: A Boundary-aware Segmentation Diffusion Model for Skin Lesion Delineation"></a>DermoSegDiff: A Boundary-aware Segmentation Diffusion Model for Skin Lesion Delineation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02959">http://arxiv.org/abs/2308.02959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mindflow-institue/dermosegdiff">https://github.com/mindflow-institue/dermosegdiff</a></li>
<li>paper_authors: Afshin Bozorgpour, Yousef Sadegheih, Amirhossein Kazerouni, Reza Azad, Dorit Merhof</li>
<li>for: 静脉皮肤病诊断 early detection 和准确诊断</li>
<li>methods: 利用 Diffusion Probabilistic Models (DDPMs) 进行静脉皮肤病诊断, 并在学习过程中加入边缘信息</li>
<li>results: 对多个皮肤分割数据集进行实验，显示 DermoSegDiff 的效果和泛化能力都较为出色，超过了现有的 CNN、transformer 和 diffusion-based 方法<details>
<summary>Abstract</summary>
Skin lesion segmentation plays a critical role in the early detection and accurate diagnosis of dermatological conditions. Denoising Diffusion Probabilistic Models (DDPMs) have recently gained attention for their exceptional image-generation capabilities. Building on these advancements, we propose DermoSegDiff, a novel framework for skin lesion segmentation that incorporates boundary information during the learning process. Our approach introduces a novel loss function that prioritizes the boundaries during training, gradually reducing the significance of other regions. We also introduce a novel U-Net-based denoising network that proficiently integrates noise and semantic information inside the network. Experimental results on multiple skin segmentation datasets demonstrate the superiority of DermoSegDiff over existing CNN, transformer, and diffusion-based approaches, showcasing its effectiveness and generalization in various scenarios. The implementation is publicly accessible on \href{https://github.com/mindflow-institue/dermosegdiff}{GitHub}
</details>
<details>
<summary>摘要</summary>
皮肤损害分割在诊断皮肤疾病的早期阶段发挥了关键作用。近年来，Diffusion Probabilistic Models（DDPMs）在图像生成方面受到了广泛关注。我们基于这些进步，提出了DermoSegDiff，一种新的皮肤损害分割框架。我们的方法引入了一个新的损失函数，在训练过程中优先级化边界信息，逐渐减少其他区域的重要性。我们还引入了一种基于U-Net的杂音级别网络，可以高效地 интеGRATE噪音和semantic信息内网络。多个皮肤分割数据集的实验结果表明，DermoSegDiff在不同的场景下对现有的CNN、transformer和Diffusion-based方法优于，demonstrating its effectiveness and generalization。实现可以在 \href{https://github.com/mindflow-institue/dermosegdiff}{GitHub} 上获取。
</details></li>
</ul>
<hr>
<h2 id="MomentaMorph-Unsupervised-Spatial-Temporal-Registration-with-Momenta-Shooting-and-Correction"><a href="#MomentaMorph-Unsupervised-Spatial-Temporal-Registration-with-Momenta-Shooting-and-Correction" class="headerlink" title="MomentaMorph: Unsupervised Spatial-Temporal Registration with Momenta, Shooting, and Correction"></a>MomentaMorph: Unsupervised Spatial-Temporal Registration with Momenta, Shooting, and Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02949">http://arxiv.org/abs/2308.02949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhangxing Bian, Shuwen Wei, Yihao Liu, Junyu Chen, Jiachen Zhuo, Fangxu Xing, Jonghye Woo, Aaron Carass, Jerry L. Prince</li>
<li>for: 用于估计大量运动和复杂征 patrerns 中的� MR 影像中的运动</li>
<li>methods: 基于 Lie 代数和 Lie 组 principls 的 “势量、射击、修正” 框架，以快速尝试到真正的 optima，并确保收敛到真正的 optima</li>
<li>results: 在 synthetic 数据集和实际 3D tMRI 数据集上，方法能够高效地估计精准、密集、 diffeomorphic 2D&#x2F;3D 运动场I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Tagged magnetic resonance imaging (tMRI) has been employed for decades to measure the motion of tissue undergoing deformation. However, registration-based motion estimation from tMRI is difficult due to the periodic patterns in these images, particularly when the motion is large. With a larger motion the registration approach gets trapped in a local optima, leading to motion estimation errors. We introduce a novel "momenta, shooting, and correction" framework for Lagrangian motion estimation in the presence of repetitive patterns and large motion. This framework, grounded in Lie algebra and Lie group principles, accumulates momenta in the tangent vector space and employs exponential mapping in the diffeomorphic space for rapid approximation towards true optima, circumventing local optima. A subsequent correction step ensures convergence to true optima. The results on a 2D synthetic dataset and a real 3D tMRI dataset demonstrate our method's efficiency in estimating accurate, dense, and diffeomorphic 2D/3D motion fields amidst large motion and repetitive patterns.
</details>
<details>
<summary>摘要</summary>
带标记的核磁共振成像（tMRI）已经在数十年中用于测量软组织中的运动。然而，基于匹配的运动估计从tMRI中很难进行注册，尤其是当运动较大时。当运动较大时，匹配方法会被困在地方最优点中，导致运动估计错误。我们介绍了一种新的“动量、射击和修正”框架，用于在具有循环Patterns和大运动时进行劳动动量估计。这个框架基于李 álgebra和李群原理，在 Tangent vector space中积累动量，使用 экспоненциаль映射在 diffeomorphic 空间中快速 Approximate towards true optima， circumventing local optima。后续的修正步骤确保了 converges to true optima。Synthetic dataset和实际的3D tMRI dataset results demonstrate our method's efficiency in estimating accurate, dense, and diffeomorphic 2D/3D motion fields amidst large motion and repetitive patterns.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/06/eess.IV_2023_08_06/" data-id="clly4xtg500envl88c6924ky6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/cs.LG_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/05/cs.LG_2023_08_05/">cs.LG - 2023-08-05 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Edge-of-stability-echo-state-networks"><a href="#Edge-of-stability-echo-state-networks" class="headerlink" title="Edge of stability echo state networks"></a>Edge of stability echo state networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02902">http://arxiv.org/abs/2308.02902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Ceni, Claudio Gallicchio</li>
<li>for: 这个论文提出了一种新的储存计算（Reservoir Computing，RC）架构，称为Edge of Stability Echo State Network（ES$^2$N）。</li>
<li>methods: 该模型基于定义储存层为非线性储存（如标准ESN）和线性储存（实现正交变换）的concat。 我们提供了整个数学分析，证明ES2N map的Jacobian的全谱特征在一个可控的圆锥形范围内，并利用这个性质来证明ES$^2$N的前向动力系统在设计的边缘混乱 режиobe动。</li>
<li>results: 我们的实验分析显示，新引入的储存模型可以达到理论上的最大短期记忆容量。同时，相比标准ESN，ES$^2$N具有更好的记忆和非线性之间的融合，以及在推理非线性模型方面的显著改进。<details>
<summary>Abstract</summary>
In this paper, we propose a new Reservoir Computing (RC) architecture, called the Edge of Stability Echo State Network (ES$^2$N). The introduced ES$^2$N model is based on defining the reservoir layer as a convex combination of a nonlinear reservoir (as in the standard ESN), and a linear reservoir that implements an orthogonal transformation. We provide a thorough mathematical analysis of the introduced model, proving that the whole eigenspectrum of the Jacobian of the ES2N map can be contained in an annular neighbourhood of a complex circle of controllable radius, and exploit this property to demonstrate that the ES$^2$N's forward dynamics evolves close to the edge-of-chaos regime by design. Remarkably, our experimental analysis shows that the newly introduced reservoir model is able to reach the theoretical maximum short-term memory capacity. At the same time, in comparison to standard ESN, ES$^2$N is shown to offer a favorable trade-off between memory and nonlinearity, as well as a significant improvement of performance in autoregressive nonlinear modeling.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的抽象计算（Reservoir Computing，RC）架构，称为边缘稳定声纳网络（ES$^2$N）。我们的ES$^2$N模型基于定义储存层为非线性储存（如标准ESN）和线性储存实现正交变换的混合体。我们对引入的模型进行了深入的数学分析，证明整个特征值谱可以在控制的圆盘内含义，并利用这个性质来证明ES$^2$N的前向动力系统在设计上靠近边缘混乱 режи宜。在实验中，我们发现新引入的储存模型能够达到理论上的最大短期记忆容量。同时，相比标准ESN，ES$^2$N表现出了更好的记忆与非线性之间的质量协议，以及在抽象非线性模型中显著的性能改善。
</details></li>
</ul>
<hr>
<h2 id="Textual-Data-Mining-for-Financial-Fraud-Detection-A-Deep-Learning-Approach"><a href="#Textual-Data-Mining-for-Financial-Fraud-Detection-A-Deep-Learning-Approach" class="headerlink" title="Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach"></a>Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03800">http://arxiv.org/abs/2308.03800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuru Li</li>
<li>for: 这个研究旨在使用深度学习方法进行自然语言处理（NLP）Binary分类任务，以分析金融诈骗文本。</li>
<li>methods: 我使用了多种神经网络模型，包括多层权重层、vanilla RNN、LSTM和GRU来进行文本分类任务。</li>
<li>results: 我的结果表明，使用这些多种神经网络模型可以提高金融诈骗检测的准确率，这些结果对于金融诈骗检测有着重要的意义，并为业界实践者、监管机构和研究人员提供有价值的洞察。<details>
<summary>Abstract</summary>
In this report, I present a deep learning approach to conduct a natural language processing (hereafter NLP) binary classification task for analyzing financial-fraud texts. First, I searched for regulatory announcements and enforcement bulletins from HKEX news to define fraudulent companies and to extract their MD&A reports before I organized the sentences from the reports with labels and reporting time. My methodology involved different kinds of neural network models, including Multilayer Perceptrons with Embedding layers, vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) for the text classification task. By utilizing this diverse set of models, I aim to perform a comprehensive comparison of their accuracy in detecting financial fraud. My results bring significant implications for financial fraud detection as this work contributes to the growing body of research at the intersection of deep learning, NLP, and finance, providing valuable insights for industry practitioners, regulators, and researchers in the pursuit of more robust and effective fraud detection methodologies.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我使用深度学习方法来进行自然语言处理（以下简称 NLP）的二分类任务，以分析金融诈骗文本。首先，我从香港证券交易所新闻中搜索了规范公告和执行通知，以定义诈骗公司并提取其财务报告。然后，我将报告中的句子分为标签和时间排序。我的方法包括多层感知器、普通逻辑神经网络、长短期记忆网络（LSTM）和闭合逻辑Unit（GRU）等不同类型的神经网络模型，用于文本分类任务。通过利用这些多样化的模型，我希望能够对金融诈骗检测的准确率进行全面的比较。我的结果对金融诈骗检测具有重要的意义，这项工作将加入深度学习、NLP和金融之间的交叉领域的研究中，为业内专业人士、监管部门和研究人员提供价值的发现。
</details></li>
</ul>
<hr>
<h2 id="Elucidate-Gender-Fairness-in-Singing-Voice-Transcription"><a href="#Elucidate-Gender-Fairness-in-Singing-Voice-Transcription" class="headerlink" title="Elucidate Gender Fairness in Singing Voice Transcription"></a>Elucidate Gender Fairness in Singing Voice Transcription</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02898">http://arxiv.org/abs/2308.02898</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guxm2021/svt_speechbrain">https://github.com/guxm2021/svt_speechbrain</a></li>
<li>paper_authors: Xiangming Gu, Wei Zeng, Ye Wang</li>
<li>for:  investigate the performance disparity in singing voice transcription (SVT) between males and females, and propose a method to address the gender bias.</li>
<li>methods: use an attribute predictor to predict gender labels and adversarially train the SVT system to enforce the gender-invariance of acoustic representations, conditionally align acoustic representations between demographic groups by feeding note events to the attribute predictor.</li>
<li>results: significant reduction of gender bias (up to more than 50%) with negligible degradation of overall SVT performance, on both in-domain and out-of-domain singing data, offering a better fairness-utility trade-off.<details>
<summary>Abstract</summary>
It is widely known that males and females typically possess different sound characteristics when singing, such as timbre and pitch, but it has never been explored whether these gender-based characteristics lead to a performance disparity in singing voice transcription (SVT), whose target includes pitch. Such a disparity could cause fairness issues and severely affect the user experience of downstream SVT applications. Motivated by this, we first demonstrate the female superiority of SVT systems, which is observed across different models and datasets. We find that different pitch distributions, rather than gender data imbalance, contribute to this disparity. To address this issue, we propose using an attribute predictor to predict gender labels and adversarially training the SVT system to enforce the gender-invariance of acoustic representations. Leveraging the prior knowledge that pitch distributions may contribute to the gender bias, we propose conditionally aligning acoustic representations between demographic groups by feeding note events to the attribute predictor. Empirical experiments on multiple benchmark SVT datasets show that our method significantly reduces gender bias (up to more than 50%) with negligible degradation of overall SVT performance, on both in-domain and out-of-domain singing data, thus offering a better fairness-utility trade-off.
</details>
<details>
<summary>摘要</summary>
广泛知道，男女在唱歌时通常具有不同的音色特征，如音 timbre 和音高，但这些 gender-based 特征是否会导致唱歌voice transcription（SVT）中的性别偏袋问题？如果存在这种偏袋问题，那么这将导致 fairness 问题并且严重地影响下游 SVT 应用程序的用户体验。为了解决这个问题，我们首先示出了女性 SVT 系统的优势，这种优势可以在不同的模型和数据集上被观察到。我们发现，不同的投射分布，而不是性别数据不均衡，是导致这种偏袋问题的主要原因。为了解决这个问题，我们提议使用一个 attribute predictor 来预测性别标签，并在 SVT 系统中进行对 gender-invariance 的 adversarial 训练。利用投射分布可能会导致性别偏袋的知识，我们提议通过将 note events 传递给 attribute predictor，来Conditional 地将音频表示同步。我们的方法在多个标准 SVT 数据集上进行了实验，结果显示，我们的方法可以减少性别偏袋（达到50%以上），同时不会影响 SVT 总性能，从而提供了更好的 fairness-utility 交易。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-Gaussian-process-model-for-Euler-Bernoulli-beam-elements"><a href="#Physics-informed-Gaussian-process-model-for-Euler-Bernoulli-beam-elements" class="headerlink" title="Physics-informed Gaussian process model for Euler-Bernoulli beam elements"></a>Physics-informed Gaussian process model for Euler-Bernoulli beam elements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02894">http://arxiv.org/abs/2308.02894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gledson Rodrigo Tondo, Sebastian Rau, Igor Kavrakov, Guido Morgenthal</li>
<li>for: 这个论文是用于开发一种基于物理学习的、多输出泛化过程的机器学习模型，用于估计结构的弯矩稳定性。</li>
<li>methods: 这个模型使用了欧拉-伯涅瓦梁式方程，并通过适当的数据集来训练。</li>
<li>results: 模型可以用来描述结构的弯矩稳定性，进行 interpolate 和 probabilistic 推断，并在结构健康监测中使用 Mahalanobis 距离来评估结构系统中可能的损害的位置和范围。<details>
<summary>Abstract</summary>
A physics-informed machine learning model, in the form of a multi-output Gaussian process, is formulated using the Euler-Bernoulli beam equation. Given appropriate datasets, the model can be used to regress the analytical value of the structure's bending stiffness, interpolate responses, and make probabilistic inferences on latent physical quantities. The developed model is applied on a numerically simulated cantilever beam, where the regressed bending stiffness is evaluated and the influence measurement noise on the prediction quality is investigated. Further, the regressed probabilistic stiffness distribution is used in a structural health monitoring context, where the Mahalanobis distance is employed to reason about the possible location and extent of damage in the structural system. To validate the developed framework, an experiment is conducted and measured heterogeneous datasets are used to update the assumed analytical structural model.
</details>
<details>
<summary>摘要</summary>
一种physics-informed机器学习模型，具体来说是一种多输出 Gaussian process，基于Euler-Bernoulli梁式方程。给定合适的数据集，该模型可以用来回归分析结构的弯矩稳定性， interpolate 响应，以及进行 probabilistic 推断 Physical quantity 的存在。该模型在 numerically simulated  cantilever beam 上进行应用，其中推断的弯矩稳定性被评估，并investigate 测量噪声对预测质量的影响。此外，推断的 probabilistic 弯矩分布被用于结构健康监测上，通过 Mahalanobis distance 来了解结构系统中可能的损害位置和范围。为验证开发的框架，进行了实验，并使用测量的 hetereogeneous 数据集来更新假设的分析结构模型。
</details></li>
</ul>
<hr>
<h2 id="Secure-Deep-JSCC-Against-Multiple-Eavesdroppers"><a href="#Secure-Deep-JSCC-Against-Multiple-Eavesdroppers" class="headerlink" title="Secure Deep-JSCC Against Multiple Eavesdroppers"></a>Secure Deep-JSCC Against Multiple Eavesdroppers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02892">http://arxiv.org/abs/2308.02892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyyed Amirhossein Ameli Kalkhoran, Mehdi Letafati, Ecenaz Erdemir, Babak Hossein Khalaj, Hamid Behroozi, Deniz Gündüz</li>
<li>for: 这个研究旨在提出一个基于深度学习的综合式通信安全方法，以保护传输过程中的私人资讯免受多名听者的窃取和探索。</li>
<li>methods: 这个方法使用深度学习的综合式模型，实现了一个数据驱动的安全通信方案，不需要对数据分布进行假设。</li>
<li>results: 实验结果显示，这个方法可以降低听者的攻击精度，对于不同的测试渠道（Rayleigh fading、Nakagami-m、AWGN）进行了评估。<details>
<summary>Abstract</summary>
In this paper, a generalization of deep learning-aided joint source channel coding (Deep-JSCC) approach to secure communications is studied. We propose an end-to-end (E2E) learning-based approach for secure communication against multiple eavesdroppers over complex-valued fading channels. Both scenarios of colluding and non-colluding eavesdroppers are studied. For the colluding strategy, eavesdroppers share their logits to collaboratively infer private attributes based on ensemble learning method, while for the non-colluding setup they act alone. The goal is to prevent eavesdroppers from inferring private (sensitive) information about the transmitted images, while delivering the images to a legitimate receiver with minimum distortion. By generalizing the ideas of privacy funnel and wiretap channel coding, the trade-off between the image recovery at the legitimate node and the information leakage to the eavesdroppers is characterized. To solve this secrecy funnel framework, we implement deep neural networks (DNNs) to realize a data-driven secure communication scheme, without relying on a specific data distribution. Simulations over CIFAR-10 dataset verifies the secrecy-utility trade-off. Adversarial accuracy of eavesdroppers are also studied over Rayleigh fading, Nakagami-m, and AWGN channels to verify the generalization of the proposed scheme. Our experiments show that employing the proposed secure neural encoding can decrease the adversarial accuracy by 28%.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了深度学习帮助的同时源渠道编码（Deep-JSCC）方法的扩展，以确保通信的安全性。我们提议一种终端到终端（E2E）学习基于的安全通信方法，用于对多个伪装者进行安全通信。我们研究了协作和不协作的情况下的伪装者。在协作情况下，伪装者共享其логиits来共同推理私有特征，而在不协作情况下，他们 acted alone。我们的目标是防止伪装者推理传输的图像中的私有（敏感）信息，同时将图像传输到合法接收器，并最小化干扰。通过扩展隐私管道和窃听渠道编码的想法，我们研究了图像恢复和伪装者信息泄露之间的质量负担。为解决这个隐私管道框架，我们使用深度神经网络（DNNs）来实现数据驱动的安全通信方案，不需要固定数据分布。我们的实验结果表明，通过使用我们的安全神经编码，可以降低伪装者的攻击精度，减少了28%。我们还对伪装者的攻击精度进行了随机抽样和AWGN渠道的研究，以验证我们的方案的普适性。
</details></li>
</ul>
<hr>
<h2 id="Private-Federated-Learning-with-Dynamic-Power-Control-via-Non-Coherent-Over-the-Air-Computation"><a href="#Private-Federated-Learning-with-Dynamic-Power-Control-via-Non-Coherent-Over-the-Air-Computation" class="headerlink" title="Private Federated Learning with Dynamic Power Control via Non-Coherent Over-the-Air Computation"></a>Private Federated Learning with Dynamic Power Control via Non-Coherent Over-the-Air Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02881">http://arxiv.org/abs/2308.02881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anbang Zhang, Shuaishuai Guo, Shuai Liu</li>
<li>for: 提高 Federated Learning（FL）中模型保护和性能提高的方法</li>
<li>methods: 基于动态功率控制的Over-the-Air Computation（AirComp）方案</li>
<li>results: 可以 Mitigate 时间同步错误、通道抑降和噪声的影响，并提供了理论上的整合证明。<details>
<summary>Abstract</summary>
To further preserve model weight privacy and improve model performance in Federated Learning (FL), FL via Over-the-Air Computation (AirComp) scheme based on dynamic power control is proposed. The edge devices (EDs) transmit the signs of local stochastic gradients by activating two adjacent orthogonal frequency division multi-plexing (OFDM) subcarriers, and majority votes (MVs) at the edge server (ES) are obtained by exploiting the energy accumulation on the subcarriers. Then, we propose a dynamic power control algorithm to further offset the biased aggregation of the MV aggregation values. We show that the whole scheme can mitigate the impact of the time synchronization error, channel fading and noise. The theoretical convergence proof of the scheme is re-derived.
</details>
<details>
<summary>摘要</summary>
为了进一步保护模型权重私钥和改进 Federated Learning（FL）的性能，我们提出了基于动态功率控制的 Federated Learning via Over-the-Air Computation（AirComp）方案。 Edge devices（ED）通过活动两个邻近的正交频分多普逊（OFDM）子频，将本地随机梯度签名发送到 Edge server（ES），然后通过利用频分的能量积累实现多数投票（MV）。然后，我们提出了一种动态功率控制算法，以更正偏好的MV汇聚值的偏好。我们证明了整个方案可以减轻时间同步错误、通道抑降和噪声的影响。我们重新证明了方案的理论收敛证明。
</details></li>
</ul>
<hr>
<h2 id="Meta-learning-in-healthcare-A-survey"><a href="#Meta-learning-in-healthcare-A-survey" class="headerlink" title="Meta-learning in healthcare: A survey"></a>Meta-learning in healthcare: A survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02877">http://arxiv.org/abs/2308.02877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Rafiei, Ronald Moore, Sina Jahromi, Farshid Hajati, Rishikesan Kamaleswaran</li>
<li>for: This paper aims to explore the applications of meta-learning in the healthcare domain and provide insights into how it can address critical healthcare challenges.</li>
<li>methods: The paper discusses the theoretical foundations and pivotal methods of meta-learning, including multi&#x2F;single-task learning and many&#x2F;few-shot learning.</li>
<li>results: The paper surveys various studies that have applied meta-learning in the healthcare domain and highlights the current challenges in meta-learning research, as well as potential solutions and future perspectives.<details>
<summary>Abstract</summary>
As a subset of machine learning, meta-learning, or learning to learn, aims at improving the model's capabilities by employing prior knowledge and experience. A meta-learning paradigm can appropriately tackle the conventional challenges of traditional learning approaches, such as insufficient number of samples, domain shifts, and generalization. These unique characteristics position meta-learning as a suitable choice for developing influential solutions in various healthcare contexts, where the available data is often insufficient, and the data collection methodologies are different. This survey discusses meta-learning broad applications in the healthcare domain to provide insight into how and where it can address critical healthcare challenges. We first describe the theoretical foundations and pivotal methods of meta-learning. We then divide the employed meta-learning approaches in the healthcare domain into two main categories of multi/single-task learning and many/few-shot learning and survey the studies. Finally, we highlight the current challenges in meta-learning research, discuss the potential solutions and provide future perspectives on meta-learning in healthcare.
</details>
<details>
<summary>摘要</summary>
为一种机器学习子领域，meta-learning，或学习学习，旨在提高模型的能力，通过使用先前知识和经验。meta-learning概念可以有效地解决传统学习方法的常见挑战，如样本不够、领域变化和泛化。这些特点使得meta-learning成为在医疗领域开发影响力强的解决方案的适用场景。本文首先介绍了meta-learning的理论基础和关键方法，然后将在医疗领域使用的meta-learning方法分为两个主要类别：多/单任务学习和多/少射学习，并对相关研究进行概述。最后，我们描述了当前meta-learning研究中的挑战，讨论了可能的解决方案，并提供了未来meta-learning在医疗领域的前景。
</details></li>
</ul>
<hr>
<h2 id="Data-Based-Design-of-Multi-Model-Inferential-Sensors"><a href="#Data-Based-Design-of-Multi-Model-Inferential-Sensors" class="headerlink" title="Data-Based Design of Multi-Model Inferential Sensors"></a>Data-Based Design of Multi-Model Inferential Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02872">http://arxiv.org/abs/2308.02872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Mojto, Karol Lubušký, Miroslav Fikar, Radoslav Paulen</li>
<li>for: 这篇论文关注软感知设计问题，旨在提高软感知仪器的预测性能。</li>
<li>methods: 本文提出了两种新的软感知设计方法，以提高软感知仪器的预测性能，并且可以维护其线性结构。</li>
<li>results: 对于一个实际的燃气油氢化单元，比较了多种单一模型软感知仪器和当前 referential 软感知仪器，结果表明了新方法的显著提高。<details>
<summary>Abstract</summary>
This paper deals with the problem of inferential (soft) sensor design. The nonlinear character of industrial processes is usually the main limitation to designing simple linear inferential sensors with sufficient accuracy. In order to increase the inferential sensor predictive performance and yet to maintain its linear structure, multi-model inferential sensors represent a straightforward option. In this contribution, we propose two novel approaches for the design of multi-model inferential sensors aiming to mitigate some drawbacks of the state-of-the-art approaches. For a demonstration of the developed techniques, we design inferential sensors for a Vacuum Gasoil Hydrogenation unit, which is a real-world petrochemical refinery unit. The performance of the multi-model inferential sensor is compared against various single-model inferential sensors and the current (referential) inferential sensor used in the refinery. The results show substantial improvements over the state-of-the-art design techniques for single-/multi-model inferential sensors.
</details>
<details>
<summary>摘要</summary>
In this study, we propose two new methods for designing multi-model inferential sensors. We demonstrate the effectiveness of these methods using a real-world petrochemical refinery unit, the vacuum gasoil hydrogenation unit. The performance of the multi-model inferential sensor is compared to various single-model inferential sensors and the current referential inferential sensor used in the refinery. The results show significant improvements over existing design techniques for single-/multi-model inferential sensors.The key contributions of this paper are:1. Two novel approaches for designing multi-model inferential sensors that mitigate some drawbacks of state-of-the-art techniques.2. A demonstration of the effectiveness of the proposed methods using a real-world petrochemical refinery unit.3. Comparison of the performance of the multi-model inferential sensor with various single-model inferential sensors and the current referential inferential sensor used in the refinery.The rest of the paper is organized as follows: Section 2 reviews the related work on soft sensors and multi-model inferential sensors. Section 3 describes the proposed methods for designing multi-model inferential sensors. Section 4 presents the case study of the vacuum gasoil hydrogenation unit. Section 5 compares the performance of the multi-model inferential sensor with other approaches. Finally, Section 6 concludes the paper and highlights future research directions.
</details></li>
</ul>
<hr>
<h2 id="NP-SemiSeg-When-Neural-Processes-meet-Semi-Supervised-Semantic-Segmentation"><a href="#NP-SemiSeg-When-Neural-Processes-meet-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="NP-SemiSeg: When Neural Processes meet Semi-Supervised Semantic Segmentation"></a>NP-SemiSeg: When Neural Processes meet Semi-Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02866">http://arxiv.org/abs/2308.02866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jianf-wang/np-semiseg">https://github.com/jianf-wang/np-semiseg</a></li>
<li>paper_authors: Jianfeng Wang, Daniela Massiceti, Xiaolin Hu, Vladimir Pavlovic, Thomas Lukasiewicz</li>
<li>for: 这种方法用于 assigning pixel-wise labels to unlabeled images at training time, 有助于减少标注成本和提高 segmentation 的准确率。</li>
<li>methods: 该方法使用 neural processes (NPs) for uncertainty quantification, and adapts NPs to semi-supervised semantic segmentation, resulting in a new model called NP-SemiSeg.</li>
<li>results: 实验结果表明，NP-SemiSeg 在 PASCAL VOC 2012 和 Cityscapes 上的公共benchmark上，在不同的训练设置下，具有显著的效果。<details>
<summary>Abstract</summary>
Semi-supervised semantic segmentation involves assigning pixel-wise labels to unlabeled images at training time. This is useful in a wide range of real-world applications where collecting pixel-wise labels is not feasible in time or cost. Current approaches to semi-supervised semantic segmentation work by predicting pseudo-labels for each pixel from a class-wise probability distribution output by a model. If the predicted probability distribution is incorrect, however, this leads to poor segmentation results, which can have knock-on consequences in safety critical systems, like medical images or self-driving cars. It is, therefore, important to understand what a model does not know, which is mainly achieved by uncertainty quantification. Recently, neural processes (NPs) have been explored in semi-supervised image classification, and they have been a computationally efficient and effective method for uncertainty quantification. In this work, we move one step forward by adapting NPs to semi-supervised semantic segmentation, resulting in a new model called NP-SemiSeg. We experimentally evaluated NP-SemiSeg on the public benchmarks PASCAL VOC 2012 and Cityscapes, with different training settings, and the results verify its effectiveness.
</details>
<details>
<summary>摘要</summary>
semi-supervised semantic segmentation是将不带标签的图像分类为不同类别的过程。这有很多实际应用场景，例如医疗图像或自动驾驶车辆，因为收集标签是不可能或者成本高昂。现有的方法是通过模型输出类别概率分布来预测每个像素的pseudo标签。如果预测结果不正确，则会导致 segmentation 结果差，这可能会对安全关键系统产生影响，例如医疗图像或自动驾驶车辆。因此，理解模型不知道的内容非常重要。最近，神经过程（NP）在半supervised图像分类中被探索，它们是一种计算效率高且有效的不确定量化方法。在这种工作中，我们将NP应用于半supervised semantic segmentation，得到了一种新的模型called NP-SemiSeg。我们对NP-SemiSeg进行了不同的训练设置，并在公共测试集PASCAL VOC 2012和Cityscapes上进行了实验，结果证明了它的有效性。
</details></li>
</ul>
<hr>
<h2 id="Generative-Adversarial-Networks-for-Stain-Normalisation-in-Histopathology"><a href="#Generative-Adversarial-Networks-for-Stain-Normalisation-in-Histopathology" class="headerlink" title="Generative Adversarial Networks for Stain Normalisation in Histopathology"></a>Generative Adversarial Networks for Stain Normalisation in Histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02851">http://arxiv.org/abs/2308.02851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Breen, Kieran Zucker, Katie Allen, Nishant Ravikumar, Nicolas M. Orsi</li>
<li>for: 提高临床诊断的准确率和效率，透过人工智能技术的发展。</li>
<li>methods: 主要是使用生成对抗网络（GANs）等技术进行染料标准化。</li>
<li>results: GAN-based methods typically outperform non-generative approaches, but the best method for stain normalization is still an ongoing field of study and depends on the specific scenario and performance metrics.<details>
<summary>Abstract</summary>
The rapid growth of digital pathology in recent years has provided an ideal opportunity for the development of artificial intelligence-based tools to improve the accuracy and efficiency of clinical diagnoses. One of the significant roadblocks to current research is the high level of visual variability across digital pathology images, causing models to generalise poorly to unseen data. Stain normalisation aims to standardise the visual profile of digital pathology images without changing the structural content of the images. In this chapter, we explore different techniques which have been used for stain normalisation in digital pathology, with a focus on approaches which utilise generative adversarial networks (GANs). Typically, GAN-based methods outperform non-generative approaches but at the cost of much greater computational requirements. However, it is not clear which method is best for stain normalisation in general, with different GAN and non-GAN approaches outperforming each other in different scenarios and according to different performance metrics. This is an ongoing field of study as researchers aim to identify a method which efficiently and effectively normalises pathology images to make AI models more robust and generalisable.
</details>
<details>
<summary>摘要</summary>
随着数字病理学的快速发展，提供了一个理想的机会，用于发展基于人工智能技术的工具，以提高临床诊断的准确性和效率。然而，一个 significante roadblock 是数字病理图像之间的视觉变化，导致模型很难泛化到未看过的数据。颜色标准化目的是标准化数字病理图像的视觉特征，而不改变图像的结构内容。在这章中，我们探讨了不同的技术，用于数字病理颜色标准化，特别是使用生成对抗网络（GAN）。通常，GAN基本方法在不同的场景下都会超越非生成方法，但是计算需求很高。然而，没有一个方法是最佳的，不同的 GAN 和非 GAN 方法在不同的场景和性能指标下都会出perform。这是一个持续的研究领域，研究人员希望能够找到一种能够有效地和效率地标准化病理图像的方法，以使AI模型更加可靠和泛化。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Positive-Homogeneous-Functions-with-Scale-Invariant-Neural-Networks"><a href="#Approximating-Positive-Homogeneous-Functions-with-Scale-Invariant-Neural-Networks" class="headerlink" title="Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks"></a>Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02836">http://arxiv.org/abs/2308.02836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Bamberger, Reinhard Heckel, Felix Krahmer</li>
<li>for:  investigated the possibility of solving linear inverse problems with $ReLu$ networks</li>
<li>methods:  used positive homogeneity and absence of bias terms in the network architecture</li>
<li>results:  showed that $ReLu$-networks with two hidden layers can achieve approximate recovery with arbitrary precision and sparsity level, and extended the results to a wider class of recovery problems and general positive homogeneous functions.<details>
<summary>Abstract</summary>
We investigate to what extent it is possible to solve linear inverse problems with $ReLu$ networks. Due to the scaling invariance arising from the linearity, an optimal reconstruction function $f$ for such a problem is positive homogeneous, i.e., satisfies $f(\lambda x) = \lambda f(x)$ for all non-negative $\lambda$. In a $ReLu$ network, this condition translates to considering networks without bias terms. We first consider recovery of sparse vectors from few linear measurements. We prove that $ReLu$- networks with only one hidden layer cannot even recover $1$-sparse vectors, not even approximately, and regardless of the width of the network. However, with two hidden layers, approximate recovery with arbitrary precision and arbitrary sparsity level $s$ is possible in a stable way. We then extend our results to a wider class of recovery problems including low-rank matrix recovery and phase retrieval. Furthermore, we also consider the approximation of general positive homogeneous functions with neural networks. Extending previous work, we establish new results explaining under which conditions such functions can be approximated with neural networks. Our results also shed some light on the seeming contradiction between previous works showing that neural networks for inverse problems typically have very large Lipschitz constants, but still perform very well also for adversarial noise. Namely, the error bounds in our expressivity results include a combination of a small constant term and a term that is linear in the noise level, indicating that robustness issues may occur only for very small noise levels.
</details>
<details>
<summary>摘要</summary>
我们研究可以使用 $ReLu$ 网络解决线性逆问题的可能性。由于线性的扩展对称性，一个好的复原函数 $f$ 的选择会是正Homogeneous，即满足 $f(\lambda x) = \lambda f(x)$ 的所有非负 $\lambda$。在 $ReLu$ 网络中，这个条件可以翻译为不包含偏好项。我们首先考虑从几个线性量测中回复簇节点。我们证明了 $ReLu$ 网络只有一个隐藏层时无法复原 $1$-簇节点，不管网络宽度如何。但是，具有两个隐藏层的 $ReLu$ 网络可以在稳定的方式下复原任意精度和簇节点数量 $s$。我们随后将结果扩展到更加广泛的复原问题，包括低维矩阵复原和相位回复。此外，我们还考虑了一般正Homogeneous函数的逼近，并建立了新的结果，说明在哪些情况下，这些函数可以透过神经网络逼近。我们的结果还照明了对于过去的研究所提出的对立之处，即神经网络 для反对数学问题通常具有非常大的Lipschitz常数，但是还是能够非常好地运行也在阶梯误差下。这意味着可能在非常小的误差水平下，发生了Robustness问题。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Financial-Index-Tracking"><a href="#Reinforcement-Learning-for-Financial-Index-Tracking" class="headerlink" title="Reinforcement Learning for Financial Index Tracking"></a>Reinforcement Learning for Financial Index Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02820">http://arxiv.org/abs/2308.02820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dppalomar/sparseindextracking">https://github.com/dppalomar/sparseindextracking</a></li>
<li>paper_authors: Xianhua Peng, Chenyin Gong, Xue Dong He</li>
<li>for: 本研究旨在提出一种精细时间架构的财务指数追踪问题解决方案，以满足返回基本追踪误差和价值基本追踪误差两种不同的追踪目标。</li>
<li>methods: 本研究使用了离散时间无限远景动态模型，并使用了Banach固定点迭代法解决端口重新平衡方程。此外，本研究还提出了一种基于深度强化学习（RL）方法的解决方案，以解决数据有限性问题。</li>
<li>results: 实验结果表明，提出的方法可以超过标准方法在追踪准确性和赚利率方面表现出色，并且可以通过具有策略的现金投入或抽取来实现额外的收益。<details>
<summary>Abstract</summary>
We propose the first discrete-time infinite-horizon dynamic formulation of the financial index tracking problem under both return-based tracking error and value-based tracking error. The formulation overcomes the limitations of existing models by incorporating the intertemporal dynamics of market information variables not limited to prices, allowing exact calculation of transaction costs, accounting for the tradeoff between overall tracking error and transaction costs, allowing effective use of data in a long time period, etc. The formulation also allows novel decision variables of cash injection or withdraw. We propose to solve the portfolio rebalancing equation using a Banach fixed point iteration, which allows to accurately calculate the transaction costs specified as nonlinear functions of trading volumes in practice. We propose an extension of deep reinforcement learning (RL) method to solve the dynamic formulation. Our RL method resolves the issue of data limitation resulting from the availability of a single sample path of financial data by a novel training scheme. A comprehensive empirical study based on a 17-year-long testing set demonstrates that the proposed method outperforms a benchmark method in terms of tracking accuracy and has the potential for earning extra profit through cash withdraw strategy.
</details>
<details>
<summary>摘要</summary>
我们提出了首个精细时间无限远景动态模型，用于跟踪金融指数问题，包括返点基本跟踪错误和价值基本跟踪错误。该模型超越了现有模型的局限性，因为它包含市场信息变量的时间动态，允许精确计算交易成本，考虑跟踪错误和交易成本之间的贸易做，使用长时间期间的数据，等等。该模型还允许新的决策变量：资金注入或撤回。我们提出使用巴нах固定点迭代法解决portfolio重新平衡方程，可以准确计算交易成本，实际上是非线性函数的交易量。我们还提出了RL方法的扩展，用于解决动态模型。我们的RL方法可以在单个财务数据样本路径上解决数据有限制的问题，通过一种新的训练方案。我们的实验表明，我们的方法在17年的测试集上表现出色，超过了参考方法的跟踪精度，并且有可能通过资金撤回策略获得额外利润。
</details></li>
</ul>
<hr>
<h2 id="A-generative-model-for-surrogates-of-spatial-temporal-wildfire-nowcasting"><a href="#A-generative-model-for-surrogates-of-spatial-temporal-wildfire-nowcasting" class="headerlink" title="A generative model for surrogates of spatial-temporal wildfire nowcasting"></a>A generative model for surrogates of spatial-temporal wildfire nowcasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02810">http://arxiv.org/abs/2308.02810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sibo Cheng, Yike Guo, Rossella Arcucci</li>
<li>for: 预测野火发展 (predicting wildfire development)</li>
<li>methods: 使用三维vector-量化类似自适应器 (using three-dimensional Vector-Quantized Variational Autoencoders)</li>
<li>results: 成功生成了可见度和结构的野火燃烧区域 (successfully generated coherent and structured wildfire burned areas)<details>
<summary>Abstract</summary>
Recent increase in wildfires worldwide has led to the need for real-time fire nowcasting. Physics-driven models, such as cellular automata and computational fluid dynamics can provide high-fidelity fire spread simulations but they are computationally expensive and time-consuming. Much effort has been put into developing machine learning models for fire prediction. However, these models are often region-specific and require a substantial quantity of simulation data for training purpose. This results in a significant amount of computational effort for different ecoregions. In this work, a generative model is proposed using a three-dimensional Vector-Quantized Variational Autoencoders to generate spatial-temporal sequences of unseen wildfire burned areas in a given ecoregion. The model is tested in the ecoregion of a recent massive wildfire event in California, known as the Chimney fire. Numerical results show that the model succeed in generating coherent and structured fire scenarios, taking into account the impact from geophysical variables, such as vegetation and slope. Generated data are also used to train a surrogate model for predicting wildfire dissemination, which has been tested on both simulation data and the real Chimney fire event.
</details>
<details>
<summary>摘要</summary>
全球各地的野火增加的趋势，使得实时野火预测变得越来越重要。物理驱动的模型，如细胞自动机和计算流体力学，可以提供高精度的野火快速扩散模拟，但它们 Computationally expensive and time-consuming。大量的努力被投入到了机器学习模型的开发中，以预测野火。然而，这些模型通常是地域特定的，需要大量的模拟数据来训练。这会导致不同的生态区域需要大量的计算劳动。在这种情况下，本文提出了一种生成模型，使用三维向量量化自适应机制来生成未经见过的野火烧区Sequence。该模型在加利福尼亚州的一个大规模野火事件中进行了测试，称为奇尼火。numerical results show that the model successfully generated coherent and structured fire scenarios, taking into account the impact of geophysical variables such as vegetation and slope.Generated data were also used to train a surrogate model for predicting wildfire spread, which was tested on both simulation data and the real Chimney fire event.
</details></li>
</ul>
<hr>
<h2 id="MiAMix-Enhancing-Image-Classification-through-a-Multi-stage-Augmented-Mixed-Sample-Data-Augmentation-Method"><a href="#MiAMix-Enhancing-Image-Classification-through-a-Multi-stage-Augmented-Mixed-Sample-Data-Augmentation-Method" class="headerlink" title="MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixed Sample Data Augmentation Method"></a>MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixed Sample Data Augmentation Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02804">http://arxiv.org/abs/2308.02804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Liang, Youzhi Liang, Jianguo Jia</li>
<li>for: 提高深度学习模型的泛化性和性能，使其在多种计算机视觉任务中具有更好的普适性和稳定性。</li>
<li>methods: 提出了一种新的混合方法called MiAMix，它是基于混合框架的多阶段扩展混合方法，通过多种多样化的混合方法同时进行混合，并通过随机选择混合掩码的混合方法来提高混合方法。</li>
<li>results: 通过四个图像标准benchmark进行了全面的评估，并与当前状态的混合样本数据增强技术进行比较，demonstrated that MiAMix可以提高性能而无需增加计算负担。<details>
<summary>Abstract</summary>
Despite substantial progress in the field of deep learning, overfitting persists as a critical challenge, and data augmentation has emerged as a particularly promising approach due to its capacity to enhance model generalization in various computer vision tasks. While various strategies have been proposed, Mixed Sample Data Augmentation (MSDA) has shown great potential for enhancing model performance and generalization. We introduce a novel mixup method called MiAMix, which stands for Multi-stage Augmented Mixup. MiAMix integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods. Recent methods utilize saliency information and the MiAMix is designed for computational efficiency as well, reducing additional overhead and offering easy integration into existing training pipelines. We comprehensively evaluate MiaMix using four image benchmarks and pitting it against current state-of-the-art mixed sample data augmentation techniques to demonstrate that MIAMix improves performance without heavy computational overhead.
</details>
<details>
<summary>摘要</summary>
尽管深度学习领域已经取得了重大进步，但过拟合仍然是一个 kritical 挑战，而数据扩充被认为是一种有效的方法来提高模型的通用性。在不同的策略中，混合样本数据扩充（MSDA）被认为是提高模型性能和通用性的有效方法。我们介绍了一种新的mixup方法，称为MiAMix，它是多stage混合的augmentation方法。MiAMix将图像扩充integrated into the mixup框架，并同时使用多种多样化的混合方法，通过随机选择混合面积的混合方法来提高混合方法。现有的方法使用了saliency信息，而MiAMix是为计算效率而设计的，减少了额外的负担和提供了与现有训练管道的集成。我们对MiaMix进行了四个图像标准 benchMark 的完整评估，并与当前状态的混合样本数据扩充技术进行了比较，以示MiAMix可以提高性能而不带重大计算负担。
</details></li>
</ul>
<hr>
<h2 id="OBESEYE-Interpretable-Diet-Recommender-for-Obesity-Management-using-Machine-Learning-and-Explainable-AI"><a href="#OBESEYE-Interpretable-Diet-Recommender-for-Obesity-Management-using-Machine-Learning-and-Explainable-AI" class="headerlink" title="OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine Learning and Explainable AI"></a>OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine Learning and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02796">http://arxiv.org/abs/2308.02796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mrinmoy Roy, Srabonti Das, Anica Tasnim Protity</li>
<li>For: The paper aims to provide a novel machine learning-based system to predict the amount of nutrients an individual requires for being healthy, with a focus on patients with comorbidities.* Methods: The paper uses various machine learning algorithms, including linear regression, support vector machine (SVM), decision tree, random forest, XGBoost, and LightGBM, to predict fluid, carbohydrate, protein, and fat consumption.* Results: The paper achieves high accuracy with low root mean square error (RMSE) using linear regression in fluid prediction, random forest in carbohydrate prediction, and LightGBM in protein and fat prediction. The system, called OBESEYE, is the only one of its kind to consider comorbidities and physical conditions when recommending diets.Here’s the simplified Chinese text for the three main points:* For: 这篇论文目的是提供一种基于机器学习的健康饮食计划，特别是为患有多种疾病的患者。* Methods: 论文使用了不同的机器学习算法，包括线性回归、支持向量机（SVM）、决策树、Random Forest、XGBoost和LightGBM等，来预测 fluid、碳水化合物、蛋白质和脂肪的消耗。* Results: 论文在 fluid 预测中使用线性回归得到了高精度低根平方差（RMSE），在碳水化合物预测中使用Random Forest 得到了高精度，在蛋白质和脂肪预测中使用LightGBM 得到了高精度。OBESEYE 系统是唯一考虑了患者的COMorbidities和物理状况的饮食建议系统。<details>
<summary>Abstract</summary>
Obesity, the leading cause of many non-communicable diseases, occurs mainly for eating more than our body requirements and lack of proper activity. So, being healthy requires heathy diet plans, especially for patients with comorbidities. But it is difficult to figure out the exact quantity of each nutrient because nutrients requirement varies based on physical and disease conditions. In our study we proposed a novel machine learning based system to predict the amount of nutrients one individual requires for being healthy. We applied different machine learning algorithms: linear regression, support vector machine (SVM), decision tree, random forest, XGBoost, LightGBM on fluid and 3 other major micronutrients: carbohydrate, protein, fat consumption prediction. We achieved high accuracy with low root mean square error (RMSE) by using linear regression in fluid prediction, random forest in carbohydrate prediction and LightGBM in protein and fat prediction. We believe our diet recommender system, OBESEYE, is the only of its kind which recommends diet with the consideration of comorbidities and physical conditions and promote encouragement to get rid of obesity.
</details>
<details>
<summary>摘要</summary>
肥胖是多种非传染疾病的主要原因，主要由于食物摄入量超过身体需求，以及不足的适当活动。因此，保持健康需要健康的饮食计划，特别是 для患有多种疾病的患者。然而，确定每个营养素的准确量是困难的，因为营养素需求因 físical 和疾病状况而异。在我们的研究中，我们提出了一种基于机器学习的系统，可以预测个人需要的营养素量。我们应用了不同的机器学习算法：线性回归、支持向量机(SVM)、决策树、随机森林、XGBoost、LightGBM 等，对流体和三种主要微量粮类：碳水化合物、蛋白质、脂肪摄入预测。我们得到了高精度低根平方差(RMSE)的结果，通过线性回归在流体预测中，随机森林在碳水化合物预测中，LightGBM在蛋白质和脂肪预测中。我们认为我们的饮食建议系统“OBESEYE”是目前唯一一个考虑了慢性疾病和物理状况，并且激励人们做减肥的健康饮食建议系统。
</details></li>
</ul>
<hr>
<h2 id="OrcoDCS-An-IoT-Edge-Orchestrated-Online-Deep-Compressed-Sensing-Framework"><a href="#OrcoDCS-An-IoT-Edge-Orchestrated-Online-Deep-Compressed-Sensing-Framework" class="headerlink" title="OrcoDCS: An IoT-Edge Orchestrated Online Deep Compressed Sensing Framework"></a>OrcoDCS: An IoT-Edge Orchestrated Online Deep Compressed Sensing Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05757">http://arxiv.org/abs/2308.05757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Wei Ching, Chirag Gupta, Zi Huang, Liting Hu</li>
<li>for: 这个研究是为了提出一个可以灵活地适应不同感知任务和环境变化的协调式深度压缩数据聚合（CDA）框架，以提高这些应用的性能和可扩展性。</li>
<li>methods: 这个框架使用了专门设计的对称启发器，并与边缘设备进行协调，以实现在无线感知网络（WSN）上的线上执行和训练。</li>
<li>results: 这个研究显示，在训练时间和灵活性方面，OrcoDCS比前一代的深度压缩数据聚合（DCDA）要好，并且在进一步应用中实现了更高的性能。<details>
<summary>Abstract</summary>
Compressed data aggregation (CDA) over wireless sensor networks (WSNs) is task-specific and subject to environmental changes. However, the existing compressed data aggregation (CDA) frameworks (e.g., compressed sensing-based data aggregation, deep learning(DL)-based data aggregation) do not possess the flexibility and adaptivity required to handle distinct sensing tasks and environmental changes. Additionally, they do not consider the performance of follow-up IoT data-driven deep learning (DL)-based applications. To address these shortcomings, we propose OrcoDCS, an IoT-Edge orchestrated online deep compressed sensing framework that offers high flexibility and adaptability to distinct IoT device groups and their sensing tasks, as well as high performance for follow-up applications. The novelty of our work is the design and deployment of IoT-Edge orchestrated online training framework over WSNs by leveraging an specially-designed asymmetric autoencoder, which can largely reduce the encoding overhead and improve the reconstruction performance and robustness. We show analytically and empirically that OrcoDCS outperforms the state-of-the-art DCDA on training time, significantly improves flexibility and adaptability when distinct reconstruction tasks are given, and achieves higher performance for follow-up applications.
</details>
<details>
<summary>摘要</summary>
压缩数据聚合（CDA）在无线传感网络（WSN）上是任务特定和环境变化受限的。然而，现有的CDA框架（如扩lapsed sensing基于数据聚合、深度学习（DL）基于数据聚合）不具备适应性和灵活性，无法处理不同感知任务和环境变化。另外，它们不考虑ollow-up IoT数据驱动深度学习（DL）应用的性能。为了解决这些缺点，我们提议OrcoDCS，一个基于IoT-Edge协调在线深度压缩探测框架，具有高适应性和灵活性，以及高性能 дляollow-up应用。我们利用特制的非对称自动encoder，可以大幅减少编码负担，提高重建性能和稳定性。我们通过分析和实验证明，OrcoDCS在训练时间、适应性和灵活性方面都超过了现有的DCDA。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-Contrastive-Regression-for-Estimation-of-Eye-Gaze"><a href="#Semi-supervised-Contrastive-Regression-for-Estimation-of-Eye-Gaze" class="headerlink" title="Semi-supervised Contrastive Regression for Estimation of Eye Gaze"></a>Semi-supervised Contrastive Regression for Estimation of Eye Gaze</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02784">http://arxiv.org/abs/2308.02784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somsukla Maiti, Akshansh Gupta</li>
<li>for: 这篇论文的目的是发展一个 semi-supervised contrastive learning 框架，以便估算人们的 gaze 方向。</li>
<li>methods: 本论文使用 appearance-based deep learning 模型来进行 gaze 估算，并提出了一新的对称损失函数，从中对 similary 的图像进行对比，以提高对 gaze 方向的估算精度。</li>
<li>results: 本论文的实验结果显示，这个 semi-supervised contrastive learning 框架能够从小量 annotated gaze 数据中学习一个通用的 gaze 估算模型，并且与一些state-of-the-art contrastive learning 技术相比，表现更好。<details>
<summary>Abstract</summary>
With the escalated demand of human-machine interfaces for intelligent systems, development of gaze controlled system have become a necessity. Gaze, being the non-intrusive form of human interaction, is one of the best suited approach. Appearance based deep learning models are the most widely used for gaze estimation. But the performance of these models is entirely influenced by the size of labeled gaze dataset and in effect affects generalization in performance. This paper aims to develop a semi-supervised contrastive learning framework for estimation of gaze direction. With a small labeled gaze dataset, the framework is able to find a generalized solution even for unseen face images. In this paper, we have proposed a new contrastive loss paradigm that maximizes the similarity agreement between similar images and at the same time reduces the redundancy in embedding representations. Our contrastive regression framework shows good performance in comparison to several state of the art contrastive learning techniques used for gaze estimation.
</details>
<details>
<summary>摘要</summary>
“随着人机界面智能系统的需求增加，视线控制系统的开发已成为必填。视线是非侵入式人际互动的一种最佳方法。深度学习模型基于 appearances 是目前最受欢迎的视线估计方法。但是这些模型的性能受到labelled gaze dataset的大小影响，从而影响其一般化性能。本文提出了一个半监督对称学习框架，以便透过小量labelled gaze dataset来找到一个通用的解决方案，包括未见过的脸像。本文提出了一个新的对称损失函数，它将相似的图像 Similarity 提高，并同时将嵌入表现的统计复杂度降低。我们的对称回传框架在与多个现有的对称学习技术相比之下，表现良好。”
</details></li>
</ul>
<hr>
<h2 id="Dataopsy-Scalable-and-Fluid-Visual-Exploration-using-Aggregate-Query-Sculpting"><a href="#Dataopsy-Scalable-and-Fluid-Visual-Exploration-using-Aggregate-Query-Sculpting" class="headerlink" title="Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting"></a>Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02764">http://arxiv.org/abs/2308.02764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Naimul Hoque, Niklas Elmqvist<br>for:* 这个论文是为了描述一种faceted visual query技术，帮助用户在大规模多维数据中进行查询和浏览。methods:* 这种查询技术使用的方法包括PIVOT、PARTITION、PEEK、PILE、PROJECT和PRUNE等六个步骤，用于Progressive exploration of the dataset。results:* 通过使用这种查询技术，用户可以在大规模多维数据中进行有效的查询和浏览，并且可以逐步缩小数据集，以便更好地理解数据的结构和特征。<details>
<summary>Abstract</summary>
We present aggregate query sculpting (AQS), a faceted visual query technique for large-scale multidimensional data. As a "born scalable" query technique, AQS starts visualization with a single visual mark representing an aggregation of the entire dataset. The user can then progressively explore the dataset through a sequence of operations abbreviated as P6: pivot (facet an aggregate based on an attribute), partition (lay out a facet in space), peek (see inside a subset using an aggregate visual representation), pile (merge two or more subsets), project (extracting a subset into a new substrate), and prune (discard an aggregate not currently of interest). We validate AQS with Dataopsy, a prototype implementation of AQS that has been designed for fluid interaction on desktop and touch-based mobile devices. We demonstrate AQS and Dataopsy using two case studies and three application examples.
</details>
<details>
<summary>摘要</summary>
我们介绍了统计查询雕刻（AQS），一种适合大规模多维数据的faceted visual查询技术。作为一种“生成可扩展”的查询技术，AQS开始可视化的方法是透过单一的可视示表示数据集的总聚合。用户可以逐步探索数据集通过一系列操作缩写为P6：折冲（基于特征对资料聚合进行分割）、分区（在空间中排列分割）、侦错（查看子集使用聚合图表示）、堆叠（合并两个或更多的子集）、专案（将子集转换为新基板）、剪除（不再关注的聚合）。我们验证了AQS和Dataopsy，一个实现AQS的试验版本，在桌面和触控式移动设备上进行流过交互。我们透过两个案例和三个应用例子来示范AQS和Dataopsy。
</details></li>
</ul>
<hr>
<h2 id="Neural-Collapse-in-the-Intermediate-Hidden-Layers-of-Classification-Neural-Networks"><a href="#Neural-Collapse-in-the-Intermediate-Hidden-Layers-of-Classification-Neural-Networks" class="headerlink" title="Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks"></a>Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02760">http://arxiv.org/abs/2308.02760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Parker, Emre Onal, Anton Stengel, Jake Intrater</li>
<li>for: 这paper探讨了分类神经网络中间埋点层中的Neural Collapse（NC）现象的emergence。</li>
<li>methods: 这paper使用了多种网络架构、活化函数和数据集来研究NC现象在不同层次的emergence。</li>
<li>results: 研究发现，NC现象在大多数中间埋点层中出现，其度Of collapse与层次深度正相关。此外，研究还发现，大多数减少类内方差的改进发生在神经网络的浅层，而angular separation between class means随层次深度增加。<details>
<summary>Abstract</summary>
Neural Collapse (NC) gives a precise description of the representations of classes in the final hidden layer of classification neural networks. This description provides insights into how these networks learn features and generalize well when trained past zero training error. However, to date, (NC) has only been studied in the final layer of these networks. In the present paper, we provide the first comprehensive empirical analysis of the emergence of (NC) in the intermediate hidden layers of these classifiers. We examine a variety of network architectures, activations, and datasets, and demonstrate that some degree of (NC) emerges in most of the intermediate hidden layers of the network, where the degree of collapse in any given layer is typically positively correlated with the depth of that layer in the neural network. Moreover, we remark that: (1) almost all of the reduction in intra-class variance in the samples occurs in the shallower layers of the networks, (2) the angular separation between class means increases consistently with hidden layer depth, and (3) simple datasets require only the shallower layers of the networks to fully learn them, whereas more difficult ones require the entire network. Ultimately, these results provide granular insights into the structural propagation of features through classification neural networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="WeldMon-A-Cost-effective-Ultrasonic-Welding-Machine-Condition-Monitoring-System"><a href="#WeldMon-A-Cost-effective-Ultrasonic-Welding-Machine-Condition-Monitoring-System" class="headerlink" title="WeldMon: A Cost-effective Ultrasonic Welding Machine Condition Monitoring System"></a>WeldMon: A Cost-effective Ultrasonic Welding Machine Condition Monitoring System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05756">http://arxiv.org/abs/2308.05756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beitong Tian, Kuan-Chieh Lu, Ahmadreza Eslaminia, Yaohui Wang, Chenhui Shao, Klara Nahrstedt</li>
<li>for: 这个论文是为了提出一个可靠、高性能且Cost-effective的工具状态监控系统，以提高鋳接过程中的质量控制。</li>
<li>methods: 这个系统使用自定义的数据收集系统和资料分析管道，实现实时分析。标本检测算法结合自动生成的特征和手工设计的特征，在条件分类任务中达到了95.8%的预设准确率（相比前一代方法的92.5%）。数据增强方法可以减少概念变化问题，提高工具状态分类精度8.3%。所有算法都在本地运行，仅需385毫秒过程数据。</li>
<li>results: 我们部署了WeldMon和一个商业系统在实际的鋳接机上，进行了全面的比较。我们发现，WeldMon可以提供高性能、可靠且Cost-effective的工具状态监控系统。<details>
<summary>Abstract</summary>
Ultrasonic welding machines play a critical role in the lithium battery industry, facilitating the bonding of batteries with conductors. Ensuring high-quality welding is vital, making tool condition monitoring systems essential for early-stage quality control. However, existing monitoring methods face challenges in cost, downtime, and adaptability. In this paper, we present WeldMon, an affordable ultrasonic welding machine condition monitoring system that utilizes a custom data acquisition system and a data analysis pipeline designed for real-time analysis. Our classification algorithm combines auto-generated features and hand-crafted features, achieving superior cross-validation accuracy (95.8% on average over all testing tasks) compared to the state-of-the-art method (92.5%) in condition classification tasks. Our data augmentation approach alleviates the concept drift problem, enhancing tool condition classification accuracy by 8.3%. All algorithms run locally, requiring only 385 milliseconds to process data for each welding cycle. We deploy WeldMon and a commercial system on an actual ultrasonic welding machine, performing a comprehensive comparison. Our findings highlight the potential for developing cost-effective, high-performance, and reliable tool condition monitoring systems.
</details>
<details>
<summary>摘要</summary>
服务器焊接机在锂离子电池业中发挥关键作用，帮助焊接电池与导电器。保证高质量焊接是关键，因此工具状态监测系统成为了早期质量控制的必备工具。然而，现有监测方法存在成本高、下机时间长和适应性差的问题。本文提出了WeldMon，一种可靠、高性能、成本下降的焊接机状态监测系统。WeldMon使用自定义数据获取系统和实时分析管道，并将自动生成的特征和手动设计的特征结合使用，实现了95.8%的验证精度（相对于状态艺术法92.5%）。我们的数据扩展方法可以减轻概念飘移问题，提高工具状态分类精度8.3%。所有算法都运行在本地，仅需385毫秒处理数据每个焊接周期。我们将WeldMon和一个商业系统部署在实际的服务器焊接机上，进行了完整的比较。我们的发现表明，可以开发出成本下降、高性能、可靠的工具状态监测系统。
</details></li>
</ul>
<hr>
<h2 id="DaMSTF-Domain-Adversarial-Learning-Enhanced-Meta-Self-Training-for-Domain-Adaptation"><a href="#DaMSTF-Domain-Adversarial-Learning-Enhanced-Meta-Self-Training-for-Domain-Adaptation" class="headerlink" title="DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation"></a>DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02753">http://arxiv.org/abs/2308.02753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglong Lu, Zhen Huang, Yunxiang Zhao, Zhiliang Tian, Yang Liu, Dongsheng Li</li>
<li>for: 这篇论文的目的是提出一个新的自我训练框架 для领域适应（Domain Adaptation），以解决预测错误导致的标签噪音问题。</li>
<li>methods: 这篇论文使用了自我训练的方法，将模型的预测作为目标领域中的伪标签，通过这些伪标签进行自我训练。此外，论文还提出了一个名为Domain adversarial learning enhanced Self-Training Framework（DaMSTF）的新自我训练框架，具有以下三个特点：一、使用meta-learning估算伪标签的重要性，以降低标签噪音并保留困难的例子；二、设计了一个meta constructor来建立meta-验证集，以保证meta-learning模组的有效性；三、使用领域抗击学来初始化神经网络，以解决meta-learning模组的训练导向变差问题。</li>
<li>results: 论文通过理论和实验证明了DaMSTF的有效性。在跨领域情感分类任务上，DaMSTF比BERT提高了近4%的性能。<details>
<summary>Abstract</summary>
Self-training emerges as an important research line on domain adaptation. By taking the model's prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model. Although these strategies effectively reduce the label noise, they are prone to miss the hard examples. In this paper, we propose a new self-training framework for domain adaptation, namely Domain adversarial learning enhanced Self-Training Framework (DaMSTF). Firstly, DaMSTF involves meta-learning to estimate the importance of each pseudo instance, so as to simultaneously reduce the label noise and preserve hard examples. Secondly, we design a meta constructor for constructing the meta-validation set, which guarantees the effectiveness of the meta-learning module by improving the quality of the meta-validation set. Thirdly, we find that the meta-learning module suffers from the training guidance vanishment and tends to converge to an inferior optimal. To this end, we employ domain adversarial learning as a heuristic neural network initialization method, which can help the meta-learning module converge to a better optimal. Theoretically and experimentally, we demonstrate the effectiveness of the proposed DaMSTF. On the cross-domain sentiment classification task, DaMSTF improves the performance of BERT with an average of nearly 4%.
</details>
<details>
<summary>摘要</summary>
自适应预测为域适应研究的重要线索。通过将模型的预测作为目标域无标签数据的 Pseudo 标签，自适应启动模型以 Pseudo 实例作为目标域中的训练数据。然而，预测错误（标签噪音）对自适应表现成为一个挑战。以前的方法仅使用可靠 Pseudo 实例来重新训练模型，即 Pseudo 实例具有高预测信任度。虽然这些策略有效减少标签噪音，但容易过损难例。在这篇论文中，我们提出了一种新的自适应框架 для域适应，即域适应学习强化自适应框架（DaMSTF）。首先，DaMSTF 通过元学习来估计每个 Pseudo 实例的重要性，以同时减少标签噪音并保留难例。其次，我们设计了元构建模块，用于构建元验证集，以保证元学习模块的有效性。最后，我们发现元学习模块受训练指导消失的问题，容易 converges 到一个差的优化点。为此，我们采用域适应学习作为一种启发函数初始化方法，可以帮助元学习模块 converge 到一个更好的优化点。理论和实验表明，我们提出的 DaMSTF 具有较高的效果。在跨域情感分类任务上，DaMSTF 可以提高 BERT 的性能，均提高约 4%。
</details></li>
</ul>
<hr>
<h2 id="NeRFs-The-Search-for-the-Best-3D-Representation"><a href="#NeRFs-The-Search-for-the-Best-3D-Representation" class="headerlink" title="NeRFs: The Search for the Best 3D Representation"></a>NeRFs: The Search for the Best 3D Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02751">http://arxiv.org/abs/2308.02751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravi Ramamoorthi</li>
<li>for: 这篇论文描述了NeRF表示法的应用和发展，以及三十年来寻找最佳视图合成和相关问题的3D表示方法的漫长历程。</li>
<li>methods: NeRF表示法使用神经网络来描述场景的连续体，包括视点依赖的光泽和体积密度。</li>
<li>results: NeRF表示法已成为计算机图形和视觉领域的标准表示方法，广泛应用于视图合成和图像基于渲染等问题，并且有 thousands of 篇论文进行扩展和建立。<details>
<summary>Abstract</summary>
Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe new developments in terms of NeRF representations and make some observations and insights regarding the future of 3D representations.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>神经辐射场或NeRFs已成为视觉合成或基于图像渲染等问题的表示方法选择，以及计算机视觉领域中许多其他应用程序的首选方法。它们的核心在于描述了一种新的3D场景或3D几何表示方法。而不是mesh、投影图、多平面图像或者VOXEL网格，NeRF代表场景为一个连续的Volume，通过问题 neural network 获得了视觉依赖的辐射光照和体积密度。NeRF表示法已经广泛应用，每年有 thousands of 篇论文扩展或基于它，多个作者和网站提供了概述和评论，以及许多工业应用和创业公司。在这篇文章中，我们 briefly 评论了NeRF表示法，并描述了三十年来为视觉合成和相关问题寻找最佳3D表示方法的历程， culminating 在NeRF论文中。然后，我们描述了新的NeRF表示法和一些关于未来3D表示方法的见解和发现。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-On-chip-Heterogeneity-of-Versal-Architecture-for-GNN-Inference-Acceleration"><a href="#Exploiting-On-chip-Heterogeneity-of-Versal-Architecture-for-GNN-Inference-Acceleration" class="headerlink" title="Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration"></a>Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02749">http://arxiv.org/abs/2308.02749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Chen, Pavan Manjunath, Sasindu Wijeratne, Bingyi Zhang, Viktor Prasanna</li>
<li>for: 该 paper 是为了提高 Graph Neural Network (GNN) 的推理速度而设计的。</li>
<li>methods: 该 paper 使用 AMD Versal ACAP 架构的特性进行 GNN 推理加速。具体来说，它使用 Programmable Logic (PL) 和 AI Engine (AIE) 两种不同的计算模块来实现 sparse 和 dense 操作的快速执行。</li>
<li>results: 该 paper 的实现在 VCK5000 ACAP 平台上表现出了较好的性能，与 CPU、GPU、ACAP 和其他自定义 GNN 加速器相比，实现了显著的均值运行时速度提升（162.42x、17.01x、9.90x 和 27.23x）。此外，对于 Graph Convolutional Network (GCN) 推理，该approach 在同一 ACAP 设备上实现了3.9-96.7x的速度提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have revolutionized many Machine Learning (ML) applications, such as social network analysis, bioinformatics, etc. GNN inference can be accelerated by exploiting data sparsity in the input graph, vertex features, and intermediate data in GNN computations. For dynamic sparsity exploitation, we leverage the heterogeneous computing capabilities of AMD Versal ACAP architecture to accelerate GNN inference. We develop a custom hardware module that executes the sparse primitives of the computation kernel on the Programmable Logic (PL) and efficiently computes the dense primitives using the AI Engine (AIE). To exploit data sparsity during inference, we devise a runtime kernel mapping strategy that dynamically assigns computation tasks to the PL and AIE based on data sparsity. Our implementation on the VCK5000 ACAP platform leads to superior performance compared with the state-of-the-art implementations on CPU, GPU, ACAP, and other custom GNN accelerators. Compared with these implementations, we achieve significant average runtime speedup across various models and datasets of 162.42x, 17.01x, 9.90x, and 27.23x, respectively. Furthermore, for Graph Convolutional Network (GCN) inference, our approach leads to a speedup of 3.9-96.7x compared to designs using PL only on the same ACAP device.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）已经革命化了许多机器学习（ML）应用，如社交网络分析和生物信息学等。GNN推理可以通过利用输入图数据稀疏性来加速。为了实现动态稀疏性利用，我们利用AMD Versal ACAP架构的多样化计算能力来加速GNN推理。我们开发了一个自定义硬件模块，该模块在Programmable Logic（PL）上执行稀疏计算 primitives，并使用AI Engine（AIE）来高效计算 dense primitives。为了在推理过程中利用数据稀疏性，我们提出了一种运行时kernel映射策略，该策略在PL和AIE之间动态分配计算任务基于数据稀疏性。我们在VCK5000 ACAP平台上实现了比之前的状态泰器实现在CPU、GPU、ACAP和其他自定义GNN加速器上的性能。相比这些实现，我们实现了平均运行时速度提升值为162.42倍、17.01倍、9.90倍和27.23倍，分别。此外，对于图卷积网络（GCN）推理，我们的方法实现了PL只的设计相比3.9-96.7倍的加速。
</details></li>
</ul>
<hr>
<h2 id="SABRE-Robust-Bayesian-Peer-to-Peer-Federated-Learning"><a href="#SABRE-Robust-Bayesian-Peer-to-Peer-Federated-Learning" class="headerlink" title="SABRE: Robust Bayesian Peer-to-Peer Federated Learning"></a>SABRE: Robust Bayesian Peer-to-Peer Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02747">http://arxiv.org/abs/2308.02747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nasimeh Heydaribeni, Ruisi Zhang, Tara Javidi, Cristina Nita-Rotaru, Farinaz Koushanfar</li>
<li>for: 提出了一种新的 Federated Learning 框架，即 SABRE，以提高 robustness。</li>
<li>methods: 使用了一种新的权重聚合方法，可以在非标准分布Setting下工作，并且不需要benign节点多于恶意节点。</li>
<li>results: 在一些 benchmark 数据上进行了证明和评估，并证明了 SABRE 在各种恶意攻击下的Robustness。<details>
<summary>Abstract</summary>
We introduce SABRE, a novel framework for robust variational Bayesian peer-to-peer federated learning. We analyze the robustness of the known variational Bayesian peer-to-peer federated learning framework (BayP2PFL) against poisoning attacks and subsequently show that BayP2PFL is not robust against those attacks. The new SABRE aggregation methodology is then devised to overcome the limitations of the existing frameworks. SABRE works well in non-IID settings, does not require the majority of the benign nodes over the compromised ones, and even outperforms the baseline algorithm in benign settings. We theoretically prove the robustness of our algorithm against data / model poisoning attacks in a decentralized linear regression setting. Proof-of-Concept evaluations on benchmark data from image classification demonstrate the superiority of SABRE over the existing frameworks under various poisoning attacks.
</details>
<details>
<summary>摘要</summary>
我们介绍SABRE，一个新的 Federated Learning框架，可以防护Against poisoning攻击。我们分析了已知的Variational Bayesian Peer-to-Peer Federated Learning框架（BayP2PFL）对于毒素攻击的不敏感性，并证明BayP2PFL不具有抗毒素能力。我们随后设计了一个新的SABRE聚合方法，以解决现有框架的局限性。SABRE在非ID Setting下表现良好，不需要大多数善意节点 greater than 损坏节点，甚至在正常设定下超越了基准算法。我们对Decentralized Linear Regression Setting中的数据/模型毒素攻击进行了理论上的证明，并在benchmark数据上进行了Proof-of-Concept评估，证明SABRE在不同的毒素攻击下的superiority。
</details></li>
</ul>
<hr>
<h2 id="Meta-Tsallis-Entropy-Minimization-A-New-Self-Training-Approach-for-Domain-Adaptation-on-Text-Classification"><a href="#Meta-Tsallis-Entropy-Minimization-A-New-Self-Training-Approach-for-Domain-Adaptation-on-Text-Classification" class="headerlink" title="Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification"></a>Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02746">http://arxiv.org/abs/2308.02746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglong Lu, Zhen Huang, Zhiliang Tian, Yunxiang Zhao, Xuanyu Fei, Dongsheng Li</li>
<li>for: 本文旨在提高文本分类模型的适应性 across 多个频道，有广泛的应用。</li>
<li>methods: 本文提出了一种基于自适应学习的方法，即使用自适应学习生成pseudo例，并在源频道和目标频道中进行分类。</li>
<li>results: 实验表明， compared to traditional self-training methods, MTEM可以提高BERT模型的适应性，平均提高4%的精度。<details>
<summary>Abstract</summary>
Text classification is a fundamental task for natural language processing, and adapting text classification models across domains has broad applications. Self-training generates pseudo-examples from the model's predictions and iteratively trains on the pseudo-examples, i.e., minimizes the loss on the source domain and the Gibbs entropy on the target domain. However, Gibbs entropy is sensitive to prediction errors, and thus, self-training tends to fail when the domain shift is large. In this paper, we propose Meta-Tsallis Entropy minimization (MTEM), which applies a meta-learning algorithm to optimize the instance adaptive Tsallis entropy on the target domain. To reduce the computation cost of MTEM, we propose an approximation technique to approximate the Second-order derivation involved in the meta-learning. To efficiently generate pseudo labels, we propose an annealing sampling mechanism for exploring the model's prediction probability. Theoretically, we prove the convergence of the meta-learning algorithm in MTEM and analyze the effectiveness of MTEM in achieving domain adaptation. Experimentally, MTEM improves the adaptation performance of BERT with an average of 4 percent on the benchmark dataset.
</details>
<details>
<summary>摘要</summary>
文本分类是自然语言处理中的基本任务，并且在不同领域中适应文本分类模型有广泛的应用。自我训练会生成 pseudo-example 从模型预测中，并在这些 pseudo-example 上进行逐步训练，即在源领域中减少损失，并在目标领域中减少 Gibbs  entropy。然而，Gibbs entropy 受到预测错误的影响，因此自我训练在大量领域变换时常常失败。在这篇论文中，我们提出了 Meta-Tsallis Entropy 优化（MTEM），它使用元学习算法来优化目标领域中的实例适应 Tsallis  entropy。为了减少 MTEM 的计算成本，我们提出了一种近似技术来近似元学习中的第二阶导数。同时，我们还提出了一种缓和抽象采样机制，以便快速生成 pseudo 标签。理论上，我们证明了 MTEM 中元学习算法的收敛性，并分析了 MTEM 在适应性方面的效果。实验表明，MTEM 可以在标准数据集上提高 BERT 的适应性表现，平均提高了4%。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Schedule-in-Non-Stationary-Wireless-Networks-With-Unknown-Statistics"><a href="#Learning-to-Schedule-in-Non-Stationary-Wireless-Networks-With-Unknown-Statistics" class="headerlink" title="Learning to Schedule in Non-Stationary Wireless Networks With Unknown Statistics"></a>Learning to Schedule in Non-Stationary Wireless Networks With Unknown Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02734">http://arxiv.org/abs/2308.02734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang Minh Nguyen, Eytan Modiano</li>
<li>for: 研究 wireless network 中的吞吐率最优化策略，满足通信系统中部分可见性和时间变化的假设。</li>
<li>methods: 提出了一种基于 Max-Weight 策略的 MW-UCB 算法，利用 Sliding-Window Upper-Confidence Bound 学习无线通信中的通道统计数据，并且可以在非站立性下实现throughput最优。</li>
<li>results: 经过 simulations 验证，MW-UCB 算法可以在具有部分可见性和时间变化的无线网络中实现高效的吞吐率最优化。<details>
<summary>Abstract</summary>
The emergence of large-scale wireless networks with partially-observable and time-varying dynamics has imposed new challenges on the design of optimal control policies. This paper studies efficient scheduling algorithms for wireless networks subject to generalized interference constraint, where mean arrival and mean service rates are unknown and non-stationary. This model exemplifies realistic edge devices' characteristics of wireless communication in modern networks. We propose a novel algorithm termed MW-UCB for generalized wireless network scheduling, which is based on the Max-Weight policy and leverages the Sliding-Window Upper-Confidence Bound to learn the channels' statistics under non-stationarity. MW-UCB is provably throughput-optimal under mild assumptions on the variability of mean service rates. Specifically, as long as the total variation in mean service rates over any time period grows sub-linearly in time, we show that MW-UCB can achieve the stability region arbitrarily close to the stability region of the class of policies with full knowledge of the channel statistics. Extensive simulations validate our theoretical results and demonstrate the favorable performance of MW-UCB.
</details>
<details>
<summary>摘要</summary>
现代无线网络中的大规模无线网络具有部分可见和时间变化的动态特性，对优化控制策略的设计带来了新的挑战。本文研究了基于通用干扰约束的无线网络占用策略的有效计划算法。这种模型体现了现代无线通信网络中Edge设备的准确特性。我们提出了一种名为MW-UCB的新算法，它基于Max-Weight策略，并利用Sliding-Window Upper-Confidence Bound来学习不确定的通道统计。MW-UCB可以在不确定的服务率的情况下保证通信吞吐量的最大化，并且可以在服务率的变化范围内保持稳定。我们证明MW-UCB在一定程度上是可以达到稳定区的最优策略，并且可以在服务率的变化范围内保持稳定。我们的实验结果 validate我们的理论结论，并证明MW-UCB在实际应用中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Personalization-of-Stress-Mobile-Sensing-using-Self-Supervised-Learning"><a href="#Personalization-of-Stress-Mobile-Sensing-using-Self-Supervised-Learning" class="headerlink" title="Personalization of Stress Mobile Sensing using Self-Supervised Learning"></a>Personalization of Stress Mobile Sensing using Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02731">http://arxiv.org/abs/2308.02731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Islam, Peter Washington</li>
<li>for: 这研究旨在开发一种个性化的压力预测方法，可以使用小数据量的标签来个性化学习。</li>
<li>methods: 研究人员使用了一种自动学习技术，即一维度的卷积神经网络（CNN），通过自动学习来学习每个用户的基线生物信号模式，从而实现个性化学习。</li>
<li>results: 研究人员发现，使用自动学习前training方法可以学习出高性能的压力预测模型，仅需使用少量的标签数据。这种个性化学习方法可以帮助实现精准健康系统，这些系统可以根据每个用户的特点进行个性化预测和提供推荐。<details>
<summary>Abstract</summary>
Stress is widely recognized as a major contributor to a variety of health issues. Stress prediction using biosignal data recorded by wearables is a key area of study in mobile sensing research because real-time stress prediction can enable digital interventions to immediately react at the onset of stress, helping to avoid many psychological and physiological symptoms such as heart rhythm irregularities. Electrodermal activity (EDA) is often used to measure stress. However, major challenges with the prediction of stress using machine learning include the subjectivity and sparseness of the labels, a large feature space, relatively few labels, and a complex nonlinear and subjective relationship between the features and outcomes. To tackle these issues, we examine the use of model personalization: training a separate stress prediction model for each user. To allow the neural network to learn the temporal dynamics of each individual's baseline biosignal patterns, thus enabling personalization with very few labels, we pre-train a 1-dimensional convolutional neural network (CNN) using self-supervised learning (SSL). We evaluate our method using the Wearable Stress and Affect prediction (WESAD) dataset. We fine-tune the pre-trained networks to the stress prediction task and compare against equivalent models without any self-supervised pre-training. We discover that embeddings learned using our pre-training method outperform supervised baselines with significantly fewer labeled data points: the models trained with SSL require less than 30% of the labels to reach equivalent performance without personalized SSL. This personalized learning method can enable precision health systems which are tailored to each subject and require few annotations by the end user, thus allowing for the mobile sensing of increasingly complex, heterogeneous, and subjective outcomes such as stress.
</details>
<details>
<summary>摘要</summary>
stress是广泛认可的健康问题的重要 contribuens。预测stress使用记录在佩戴式设备中的生物信号数据是移动感知研究中关键的预测领域，因为实时预测stress可以让数字干预出现在压力开始时，以避免心跳rhythm irregularities和许多心理和生理学症状。电导活动（EDA）经常用来测量压力。然而，机器学习预测压力的主要挑战包括标签的主观性和稀缺性，大的特征空间，相对少的标签，以及复杂的非线性和主观关系 между特征和结果。为解决这些问题，我们研究了个性化预测：为每个用户训练一个压力预测模型。为让神经网络学习每个人的基线生物信号模式的时间 dynamics，因此启用个性化预测，我们使用自动学习（SSL）预训练一个1维度卷积神经网络（CNN）。我们使用WESAD数据集进行评估我们的方法。我们精细调整预训练后的网络来完成压力预测任务，并与没有自动学习预训练的相同模型进行比较。我们发现，使用我们的预训练方法学习的嵌入超越了无预训练基线的表现，只需要少于30%的标签数据点来达到相同的性能。这种个性化学习方法可以启用手持式健康系统，这些系统可以根据每个用户而定制，并且只需要少量标签由用户提供，以便在移动感知中评估越来越复杂、多元和主观的结果。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Programmatic-Policies-with-Actor-Critic-Algorithms-and-ReLU-Networks"><a href="#Synthesizing-Programmatic-Policies-with-Actor-Critic-Algorithms-and-ReLU-Networks" class="headerlink" title="Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks"></a>Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02729">http://arxiv.org/abs/2308.02729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spyros Orfanos, Levi H. S. Lelis</li>
<li>for: 本研究旨在探讨Programmatically Interpretable Reinforcement Learning（PIRL）算法是否需要专门的PIRL算法，以及如何使用actor-critic算法直接从 neural network 中提取政策。</li>
<li>methods: 本研究使用 actor-critic 算法，以及一种将ReLU神经网络翻译成 if-then-else 结构、线性变换和PID操作的方法，来将政策编码在程序中。</li>
<li>results: 实验结果表明，使用此翻译方法可以学习短而有效的政策，并且与PIRL算法 synthesize 的政策相比，译制后的政策在许多控制问题上表现至少兼容，有时甚至超越。<details>
<summary>Abstract</summary>
Programmatically Interpretable Reinforcement Learning (PIRL) encodes policies in human-readable computer programs. Novel algorithms were recently introduced with the goal of handling the lack of gradient signal to guide the search in the space of programmatic policies. Most of such PIRL algorithms first train a neural policy that is used as an oracle to guide the search in the programmatic space. In this paper, we show that such PIRL-specific algorithms are not needed, depending on the language used to encode the programmatic policies. This is because one can use actor-critic algorithms to directly obtain a programmatic policy. We use a connection between ReLU neural networks and oblique decision trees to translate the policy learned with actor-critic algorithms into programmatic policies. This translation from ReLU networks allows us to synthesize policies encoded in programs with if-then-else structures, linear transformations of the input values, and PID operations. Empirical results on several control problems show that this translation approach is capable of learning short and effective policies. Moreover, the translated policies are at least competitive and often far superior to the policies PIRL algorithms synthesize.
</details>
<details>
<summary>摘要</summary>
Programmatically Interpretable Reinforcement Learning (PIRL) 编码策略为人类可读性计算机程序。最近，为了解决策略搜索空间中缺失梯度信号的问题，新的算法得到了开发。大多数PIRL算法首先使用神经网络作为尝试导航搜索空间的oracle。在这篇论文中，我们表明PIRL特有的算法不是必要的，具体来说，取决于编码策略的语言。因为可以直接使用actor-critic算法获得程序编码策略。我们使用ReLU神经网络和倾斜决策树的连接将actor-critic算法学习的策略翻译成程序编码策略。这种翻译方法可以synthesize编码策略中的if-then-else结构、输入值线性变换和PID操作。我们在几个控制问题上进行了实验，结果表明这种翻译方法可以学习短而有效的策略。此外，翻译出来的策略与PIRL算法生成的策略相比，通常更加竞争力强。
</details></li>
</ul>
<hr>
<h2 id="Towards-Improving-Harmonic-Sensitivity-and-Prediction-Stability-for-Singing-Melody-Extraction"><a href="#Towards-Improving-Harmonic-Sensitivity-and-Prediction-Stability-for-Singing-Melody-Extraction" class="headerlink" title="Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction"></a>Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02723">http://arxiv.org/abs/2308.02723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smoothken/kknet">https://github.com/smoothken/kknet</a></li>
<li>paper_authors: Keren Shao, Ke Chen, Taylor Berg-Kirkpatrick, Shlomo Dubnov</li>
<li>for: 这 paper 是为了提高 singing melody extraction 的性能而写的。</li>
<li>methods: 这 paper 使用了 input feature modification 和 training objective modification 两种方法来提高模型的性能。</li>
<li>results: 实验结果表明，提posed modifications 对 singing melody extraction 是有 empirical effectiveness 的。Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 这 paper 是为了提高 singing melody extraction 的性能而写的。</li>
<li>methods: 这 paper 使用了 input feature modification 和 training objective modification 两种方法来提高模型的性能。</li>
<li>results: 实验结果表明，提posed modifications 对 singing melody extraction 是有 empirical effectiveness 的。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.
</details>
<details>
<summary>摘要</summary>
在深度学习研究中，许多旋律提取模型通过重新设计神经网络架构来提高性能。在这篇论文中，我们提出了输入特征修改和训练目标修改，基于两个假设。首先，音频数据中的干扰在频谱图中快速衰减。为了增强模型对尾部干扰的敏感性，我们使用离散ζ变换修改合并频率和周期性（CFP）表示。其次，歌唱和非歌唱段的时间非常短暂是非常罕见的。为保证更稳定的旋律轮廓，我们设计了可导的损失函数，避免模型预测这些段落。我们应用这些修改于多个模型，包括MSNet、FTANet以及我们新引入的PianoNet，这是基于钢琴谱写网络的修改。我们的实验结果表明，我们的修改是实际有效的对歌唱旋律提取。
</details></li>
</ul>
<hr>
<h2 id="Fluid-Property-Prediction-Leveraging-AI-and-Robotics"><a href="#Fluid-Property-Prediction-Leveraging-AI-and-Robotics" class="headerlink" title="Fluid Property Prediction Leveraging AI and Robotics"></a>Fluid Property Prediction Leveraging AI and Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02715">http://arxiv.org/abs/2308.02715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baratilab/vid2visc">https://github.com/baratilab/vid2visc</a></li>
<li>paper_authors: Jong Hoon Park, Gauri Pramod Dalwankar, Alison Bartsch, Abraham George, Amir Barati Farimani</li>
<li>for: 这篇论文是为了探讨如何使用视觉信息来推断流体的性质，以便在自动化流体处理系统中更好地控制流体的行为。</li>
<li>methods: 这篇论文使用了3D卷积自适应神经网络来学习不同的流体振荡模式在视频中的表征。这些表征然后被用来从视频中视觉地推断流体的动态粘度。</li>
<li>results: 研究发现，使用这种视觉方法可以准确地推断流体的动态粘度，并且比传统方法更快速和更加精准。<details>
<summary>Abstract</summary>
Inferring liquid properties from vision is a challenging task due to the complex nature of fluids, both in behavior and detection. Nevertheless, the ability to infer their properties directly from visual information is highly valuable for autonomous fluid handling systems, as cameras are readily available. Moreover, predicting fluid properties purely from vision can accelerate the process of fluid characterization saving considerable time and effort in various experimental environments. In this work, we present a purely vision-based approach to estimate viscosity, leveraging the fact that the behavior of the fluid oscillations is directly related to the viscosity. Specifically, we utilize a 3D convolutional autoencoder to learn latent representations of different fluid-oscillating patterns present in videos. We leverage this latent representation to visually infer the category of fluid or the dynamics viscosity of fluid from video.
</details>
<details>
<summary>摘要</summary>
<<SYS Translate="yes">推断流体属性从视觉是一项复杂的任务，主要因为流体的行为和检测都具有复杂的特性。然而，从视觉信息直接推断流体属性的能力具有高度的价值，因为摄像头是 readily available。此外，仅基于视觉预测流体属性可以大幅缩短各种实验室环境中的测试时间和努力。在这项工作中，我们提出了一种完全基于视觉的方法，使用3D卷积自适应神经网络来学习不同的流体振荡模式在视频中的含义。我们利用这种含义来从视频中可视化地推断流体的类别或流体的动态粘性。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Effect-of-Sparse-Recovery-on-the-Quality-of-Image-Superresolution"><a href="#Exploring-the-Effect-of-Sparse-Recovery-on-the-Quality-of-Image-Superresolution" class="headerlink" title="Exploring the Effect of Sparse Recovery on the Quality of Image Superresolution"></a>Exploring the Effect of Sparse Recovery on the Quality of Image Superresolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02714">http://arxiv.org/abs/2308.02714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Castro</li>
<li>for: 该论文旨在研究用字典学习进行图像超解像，通过学习高分辨率和低分辨率图像对应的对应的patch对来学习一对联动的字典，以便使用这些字典来从低分辨率输入图像中恢复高分辨率图像patch。</li>
<li>methods: 该论文使用了字典学习技术，通过学习高分辨率和低分辨率图像对应的对应的patch对来学习一对联动的字典，并使用这些字典来从低分辨率输入图像中恢复高分辨率图像patch。</li>
<li>results: 该论文通过实验研究了不同的简单恢复算法对图像超解像质量的影响，并提出了最佳的简单恢复算法选择方法。<details>
<summary>Abstract</summary>
Dictionary learning can be used for image superresolution by learning a pair of coupled dictionaries of image patches from high-resolution and low-resolution image pairs such that the corresponding pairs share the same sparse vector when represented by the coupled dictionaries. These dictionaries then can be used to to reconstruct the corresponding high-resolution patches from low-resolution input images based on sparse recovery. The idea is to recover the shared sparse vector using the low-resolution dictionary and then multiply it by the high-resolution dictionary to recover the corresponding high-resolution image patch. In this work, we study the effect of the sparse recovery algorithm that we use on the quality of the reconstructed images. We offer empirical experiments to search for the best sparse recovery algorithm that can be used for this purpose.
</details>
<details>
<summary>摘要</summary>
《字典学习可以用于图像超分辨by学习一对相互关联的字典，这些字典分别表示高分辨率和低分辨率图像对应的图像 patches，以便在这些字典中共享相同的稀疏 вектор。这些字典可以用来从低分辨率输入图像中重建对应的高分辨率图像 patch，基于稀疏恢复。我们的研究将关注使用的稀疏恢复算法对重建图像质量的影响。我们将进行实验寻找最佳的稀疏恢复算法。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Scalable-Computation-of-Causal-Bounds"><a href="#Scalable-Computation-of-Causal-Bounds" class="headerlink" title="Scalable Computation of Causal Bounds"></a>Scalable Computation of Causal Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02709">http://arxiv.org/abs/2308.02709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Madhumitha Shridharan, Garud Iyengar</li>
<li>for:  Computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold.</li>
<li>methods:  Significantly prune a linear programming (LP) formulation to compute bounds, allowing for larger causal inference problems compared to existing techniques. Extend the pruning methodology to fractional LPs for additional observations.</li>
<li>results:  Significant runtime improvement compared to benchmarks in experiments, and high-quality bounds produced by an efficient greedy heuristic that scales to larger problems.<details>
<summary>Abstract</summary>
We consider the problem of computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold. Existing non-parametric approaches for computing such bounds use linear programming (LP) formulations that quickly become intractable for existing solvers because the size of the LP grows exponentially in the number of edges in the causal graph. We show that this LP can be significantly pruned, allowing us to compute bounds for significantly larger causal inference problems compared to existing techniques. This pruning procedure allows us to compute bounds in closed form for a special class of problems, including a well-studied family of problems where multiple confounded treatments influence an outcome. We extend our pruning methodology to fractional LPs which compute bounds for causal queries which incorporate additional observations about the unit. We show that our methods provide significant runtime improvement compared to benchmarks in experiments and extend our results to the finite data setting. For causal inference without additional observations, we propose an efficient greedy heuristic that produces high quality bounds, and scales to problems that are several orders of magnitude larger than those for which the pruned LP can be solved.
</details>
<details>
<summary>摘要</summary>
我团队考虑了计算 causal 查询 bounds 的问题，这里有不观察到的假设变量和离散型观察变量。我们显示出现非参数方法的 LP 表示可以快速减少，使得我们可以计算 bounds  для更大的 causal inference 问题。我们扩展了我们的减少方法来计算 fractional LPs，这些 LPs 计算 bounds  для包含额外观察的 causal queries。我们在实验中显示了我们的方法可以提供重要的时间改进 compared to 参考值。此外，我们还提出了一种高质量 bounds 的生成策略，该策略可以扩展到许多个数量级更大的问题。Here's the translation of the text into Traditional Chinese:我团队考虑过 Computing causal queries bounds 的问题，这里有不观察到的假设变量和离散型观察变量。我们显示出现非参数方法的 LP 表示可以快速减少，使得我们可以计算 bounds  для更大的 causal inference 问题。我们扩展了我们的减少方法来计算 fractional LPs，这些 LPs 计算 bounds  для包含额外观察的 causal queries。我们在实验中显示了我们的方法可以提供重要的时间改进 compared to 参考值。此外，我们还提出了一种高质量 bounds 的生成策略，这策略可以扩展到许多个数量级更大的问题。
</details></li>
</ul>
<hr>
<h2 id="FPR-Estimation-for-Fraud-Detection-in-the-Presence-of-Class-Conditional-Label-Noise"><a href="#FPR-Estimation-for-Fraud-Detection-in-the-Presence-of-Class-Conditional-Label-Noise" class="headerlink" title="FPR Estimation for Fraud Detection in the Presence of Class-Conditional Label Noise"></a>FPR Estimation for Fraud Detection in the Presence of Class-Conditional Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02695">http://arxiv.org/abs/2308.02695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jtittelfitz/fpr-estimation">https://github.com/jtittelfitz/fpr-estimation</a></li>
<li>paper_authors: Justin Tittelfitz</li>
<li>for: 本研究旨在估计二分类模型中存在标签噪声（label noise）时的假阳性率（FPR）和正确率（TPR）。</li>
<li>methods: 本研究使用了一种新的方法，即使用模型直接清洁验证数据，以避免因验证数据中的噪声而导致的假阳性率和正确率的估计偏差。</li>
<li>results: 研究发现，使用现有的方法可能会导致假阳性率和正确率的估计偏差，而使用新的方法可以更好地估计这两个指标。<details>
<summary>Abstract</summary>
We consider the problem of estimating the false-/ true-positive-rate (FPR/TPR) for a binary classification model when there are incorrect labels (label noise) in the validation set. Our motivating application is fraud prevention where accurate estimates of FPR are critical to preserving the experience for good customers, and where label noise is highly asymmetric. Existing methods seek to minimize the total error in the cleaning process - to avoid cleaning examples that are not noise, and to ensure cleaning of examples that are. This is an important measure of accuracy but insufficient to guarantee good estimates of the true FPR or TPR for a model, and we show that using the model to directly clean its own validation data leads to underestimates even if total error is low. This indicates a need for researchers to pursue methods that not only reduce total error but also seek to de-correlate cleaning error with model scores.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Explainable-Deep-Learning-based-Solar-Flare-Prediction-with-post-hoc-Attention-for-Operational-Forecasting"><a href="#Explainable-Deep-Learning-based-Solar-Flare-Prediction-with-post-hoc-Attention-for-Operational-Forecasting" class="headerlink" title="Explainable Deep Learning-based Solar Flare Prediction with post hoc Attention for Operational Forecasting"></a>Explainable Deep Learning-based Solar Flare Prediction with post hoc Attention for Operational Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02682">http://arxiv.org/abs/2308.02682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/gsudmlab/explainingfulldisk">https://bitbucket.org/gsudmlab/explainingfulldisk</a></li>
<li>paper_authors: Chetraj Pandey, Rafal A. Angryk, Manolis K. Georgoulis, Berkay Aydin</li>
<li>for: 预测日冕大地震（solar flare）的深度学习模型，以提高现有的预测方法的准确性和可靠性。</li>
<li>methods: 使用每小时的全盘线对视图磁图图像，采用二分类预测模式预测日冕大地震发生的可能性。采用自定义数据增强和样本权重来解决类偏置问题。使用真正的技能统计量和赫迪克技能分数作为评估指标。</li>
<li>results: 研究发现，全盘预测日冕大地震能够准确地 lokate和预测近日冕的大地震，这是操作预测中的关键特征。模型达到了平均的TSS&#x3D;0.51$\pm$0.05和HSS&#x3D;0.38$\pm$0.08水平，并且发现这些模型可以从全盘磁图中学习出明显的活跃区域特征。<details>
<summary>Abstract</summary>
This paper presents a post hoc analysis of a deep learning-based full-disk solar flare prediction model. We used hourly full-disk line-of-sight magnetogram images and selected binary prediction mode to predict the occurrence of $\geq$M1.0-class flares within 24 hours. We leveraged custom data augmentation and sample weighting to counter the inherent class-imbalance problem and used true skill statistic and Heidke skill score as evaluation metrics. Recent advancements in gradient-based attention methods allow us to interpret models by sending gradient signals to assign the burden of the decision on the input features. We interpret our model using three post hoc attention methods: (i) Guided Gradient-weighted Class Activation Mapping, (ii) Deep Shapley Additive Explanations, and (iii) Integrated Gradients. Our analysis shows that full-disk predictions of solar flares align with characteristics related to the active regions. The key findings of this study are: (1) We demonstrate that our full disk model can tangibly locate and predict near-limb solar flares, which is a critical feature for operational flare forecasting, (2) Our candidate model achieves an average TSS=0.51$\pm$0.05 and HSS=0.38$\pm$0.08, and (3) Our evaluation suggests that these models can learn conspicuous features corresponding to active regions from full-disk magnetograms.
</details>
<details>
<summary>摘要</summary>
Recent advancements in gradient-based attention methods allow us to interpret models by sending gradient signals to assign the burden of the decision on the input features. We interpret our model using three post hoc attention methods:1. Guided Gradient-weighted Class Activation Mapping: 使用梯度信号将决策担当分配到输入特征上。2. Deep Shapley Additive Explanations: 使用深度谱分解方法计算出每个特征对预测结果的贡献。3. Integrated Gradients: 使用梯度 интеграル方法计算出输入特征对预测结果的贡献。Our analysis shows that full-disk predictions of solar flares align with characteristics related to the active regions. The key findings of this study are:1. We demonstrate that our full disk model can tangibly locate and predict near-limb solar flares, which is a critical feature for operational flare forecasting.2. Our candidate model achieves an average TSS=0.51$\pm$0.05 and HSS=0.38$\pm$0.08.3. Our evaluation suggests that these models can learn conspicuous features corresponding to active regions from full-disk magnetograms.
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Change-of-Variable-Formulas-for-Generative-Modeling"><a href="#A-Review-of-Change-of-Variable-Formulas-for-Generative-Modeling" class="headerlink" title="A Review of Change of Variable Formulas for Generative Modeling"></a>A Review of Change of Variable Formulas for Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02652">http://arxiv.org/abs/2308.02652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ullrich Köthe</li>
<li>for: 本研究旨在总结Change-of-variables（CoV）方程的各种应用和 derivation，并从编码器&#x2F;解码器架构的视角出发，收集28种CoV方程，探讨各种方法之间的关系，强调文献中不一定够清楚地表达的重要区别，并发现未来研究中的潜在漏洞。</li>
<li>methods: 本研究使用了变量替换方程来简化复杂的概率分布，并通过学习的变换来实现 tractable Jacobian determinant。</li>
<li>results: 本研究收集了28种Change-of-variables方程，并通过对这些方程的系统性分析，探讨各种方法之间的关系，强调文献中不一定够清楚地表达的重要区别，并发现未来研究中的潜在漏洞。<details>
<summary>Abstract</summary>
Change-of-variables (CoV) formulas allow to reduce complicated probability densities to simpler ones by a learned transformation with tractable Jacobian determinant. They are thus powerful tools for maximum-likelihood learning, Bayesian inference, outlier detection, model selection, etc. CoV formulas have been derived for a large variety of model types, but this information is scattered over many separate works. We present a systematic treatment from the unifying perspective of encoder/decoder architectures, which collects 28 CoV formulas in a single place, reveals interesting relationships between seemingly diverse methods, emphasizes important distinctions that are not always clear in the literature, and identifies surprising gaps for future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>变量更改（CoV）公式可以将复杂的概率密度降低到更简单的密度中，通过学习的变换，其Jacobian determinant是可追踪的。因此，它们是最大 likelihood 学习、 bayesian 推理、异常检测、模型选择等方面的强大工具。CoV 公式已经 derivated  для许多模型类型，但这些信息分散在多个不同的论文中。我们在encoder/decoder 架构的统一视角下提供了一个系统性的处理方法，收集了28种CoV 公式，在一个地方汇总了这些信息，揭示了文献中的感兴趣关系，强调了文献中不一定明确的重要区别，并发现了未来研究中的潜在空白。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="ReCLIP-Refine-Contrastive-Language-Image-Pre-Training-with-Source-Free-Domain-Adaptation"><a href="#ReCLIP-Refine-Contrastive-Language-Image-Pre-Training-with-Source-Free-Domain-Adaptation" class="headerlink" title="ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation"></a>ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03793">http://arxiv.org/abs/2308.03793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Hu, Ke Zhang, Lu Xia, Albert Chen, Jiajia Luo, Yuyin Sun, Ken Wang, Nan Qiao, Xiao Zeng, Min Sun, Cheng-Hao Kuo, Ram Nevatia</li>
<li>for: 提高CLIP模型在下游目标领域的性能，即使没有源数据或目标标注数据。</li>
<li>methods: 提出了一种源自由领域适应方法，通过减轻视文对象 embedding的偏差和使用杂模相关自动标注来实现。</li>
<li>results: 经过广泛的实验，ReCLIP方法可以将CLIP模型的平均错误率从30.17%降低至25.06%在22个图像分类 benchmark上。<details>
<summary>Abstract</summary>
Large-scale Pre-Training Vision-Language Model such as CLIP has demonstrated outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1 accuracy on ImageNet without seeing any example, which leads to potential benefits to many tasks that have no labeled data. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, the first source-free domain adaptation method for vision-language models, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels, and then deploys cross-modality self-training with the pseudo labels, to update visual and text encoders, refine labels and reduce domain gaps and misalignments iteratively. With extensive experiments, we demonstrate ReCLIP reduces the average error rate of CLIP from 30.17% to 25.06% on 22 image classification benchmarks.
</details>
<details>
<summary>摘要</summary>
大规模预训练视语模型，如CLIP，在零shot分类任务上表现出色，例如在ImageNet上取得76.3%的顶峰准确率无需见过任何示例，这可能导致多种任务 ohne 标注数据的潜在优势。然而，在应用CLIP到下游目标领域时，视图和文本频谱差和跨模态不一致可能会严重影响模型性能。为了解决这些挑战，我们提议ReCLIP，首先学习抑制视图和文本嵌入的投影空间，然后通过跨模态自适应学习，使用假标签，更新视图和文本编码器，细化标签，并逐步减少频谱差和不一致。经过广泛实验，我们表明ReCLIP可以将CLIP的平均错误率从30.17%降低至25.06%在22个图像分类任务上。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Topology-Cosmological-Parameter-Estimation-from-the-Large-scale-Structure"><a href="#Learning-from-Topology-Cosmological-Parameter-Estimation-from-the-Large-scale-Structure" class="headerlink" title="Learning from Topology: Cosmological Parameter Estimation from the Large-scale Structure"></a>Learning from Topology: Cosmological Parameter Estimation from the Large-scale Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02636">http://arxiv.org/abs/2308.02636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacky H. T. Yip, Adam Rouhiainen, Gary Shiu</li>
<li>for: 研究宇宙大规模结构的 cosmological 参数</li>
<li>methods: 使用神经网络模型从 persistency 图像中提取 cosmological 参数</li>
<li>results: 模型可以准确地估算 cosmological 参数，质量比传统 Bayesian 推理方法更高<details>
<summary>Abstract</summary>
The topology of the large-scale structure of the universe contains valuable information on the underlying cosmological parameters. While persistent homology can extract this topological information, the optimal method for parameter estimation from the tool remains an open question. To address this, we propose a neural network model to map persistence images to cosmological parameters. Through a parameter recovery test, we demonstrate that our model makes accurate and precise estimates, considerably outperforming conventional Bayesian inference approaches.
</details>
<details>
<summary>摘要</summary>
宇宙大规模结构的Topology含有价值的 cosmological参数信息。 persistent homology 可以提取这些 topological 信息，但是最佳的方法 для参数估计仍然是一个开放的问题。为解决这个问题，我们提议一种神经网络模型，将 persistency 图像映射到 cosmological 参数。通过参数恢复测试，我们证明我们的模型可以做出准确和精确的估计，明显超过了传统的 Bayesian 推理方法。Note: "persistent homology" is translated as " persistency" in Simplified Chinese, which is a common abbreviation used in the field of topological data analysis.
</details></li>
</ul>
<hr>
<h2 id="MM-Vet-Evaluating-Large-Multimodal-Models-for-Integrated-Capabilities"><a href="#MM-Vet-Evaluating-Large-Multimodal-Models-for-Integrated-Capabilities" class="headerlink" title="MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"></a>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02490">http://arxiv.org/abs/2308.02490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweihao/mm-vet">https://github.com/yuweihao/mm-vet</a></li>
<li>paper_authors: Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang</li>
<li>For: The paper proposes an evaluation benchmark for large multimodal models (LMMs) to evaluate their ability to integrate different core vision-language (VL) capabilities and solve complicated multimodal tasks.* Methods: The paper presents MM-Vet, a benchmark that defines 6 core VL capabilities and examines 16 integrations of interest derived from capability combination. The paper also proposes an LLM-based evaluator for open-ended outputs, which enables the evaluation across different question types and answer styles.* Results: The paper evaluates representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models. The results show that the proposed evaluator can provide a unified scoring metric for open-ended outputs, and the MM-Vet benchmark can help identify the strengths and weaknesses of different LMM systems.<details>
<summary>Abstract</summary>
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models. Code and data are available at https://github.com/yuweihao/MM-Vet.
</details>
<details>
<summary>摘要</summary>
我们提出了 MM-Vet，一个评估板准 benchmark，用于测试大型多Modal模型（LMM）在复杂多Modal任务上的能力。现代LMM在不同的任务上表现出了各种感人的能力，如解释黑板上的数学问题，理解新闻图片中的事件和名人，以及解释视觉笑话。由于模型的快速发展，评估板准的开发受到了挑战。问题包括：(1)如何系统地结构化和评估复杂多Modal任务；(2)如何设计评估指标，可以在问题和答案类型之间具有一致性；以及(3)如何为模型提供更多的材料，而不仅仅是一个简单的性能排名。为此，我们提出了 MM-Vet，基于了核心视语言（VL）能力的总结。MM-Vet定义了6个核心VL能力，并评估了这些能力之间的16种 интеграción。为评估指标，我们提议了基于LLM的评估器，可以对开放式输出进行评估，并且可以在不同的问题类型和答案风格之间具有一致性。我们对代表性的LMM进行了 MM-Vet的评估，提供了不同模型系统和模型的能力的见解。代码和数据可以在 https://github.com/yuweihao/MM-Vet 上获取。
</details></li>
</ul>
<hr>
<h2 id="Generation-of-Realistic-Synthetic-Raw-Radar-Data-for-Automated-Driving-Applications-using-Generative-Adversarial-Networks"><a href="#Generation-of-Realistic-Synthetic-Raw-Radar-Data-for-Automated-Driving-Applications-using-Generative-Adversarial-Networks" class="headerlink" title="Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks"></a>Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02632">http://arxiv.org/abs/2308.02632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo C. Fidelis, Fabio Reway, Herick Y. S. Ribeiro, Pietro L. Campos, Werner Huber, Christian Icking, Lester A. Faria, Torsten Schön</li>
<li>for: 这个论文是为了模拟雷达数据的研究而写的。</li>
<li>methods: 这篇论文使用了生成对抗网络（GAN）来生成雷达数据。</li>
<li>results: 这个方法可以生成16个同时发射的雷达射频信号，并且可以用于进一步开发雷达数据处理算法（例如滤波和封装）。这可以增加数据的扩展和增强，例如生成不可能或安全关键的场景的数据，以便进行数据 augmentation。<details>
<summary>Abstract</summary>
The main approaches for simulating FMCW radar are based on ray tracing, which is usually computationally intensive and do not account for background noise. This work proposes a faster method for FMCW radar simulation capable of generating synthetic raw radar data using generative adversarial networks (GAN). The code and pre-trained weights are open-source and available on GitHub. This method generates 16 simultaneous chirps, which allows the generated data to be used for the further development of algorithms for processing radar data (filtering and clustering). This can increase the potential for data augmentation, e.g., by generating data in non-existent or safety-critical scenarios that are not reproducible in real life. In this work, the GAN was trained with radar measurements of a motorcycle and used to generate synthetic raw radar data of a motorcycle traveling in a straight line. For generating this data, the distance of the motorcycle and Gaussian noise are used as input to the neural network. The synthetic generated radar chirps were evaluated using the Frechet Inception Distance (FID). Then, the Range-Azimuth (RA) map is calculated twice: first, based on synthetic data using this GAN and, second, based on real data. Based on these RA maps, an algorithm with adaptive threshold and edge detection is used for object detection. The results have shown that the data is realistic in terms of coherent radar reflections of the motorcycle and background noise based on the comparison of chirps, the RA maps and the object detection results. Thus, the proposed method in this work has shown to minimize the simulation-to-reality gap for the generation of radar data.
</details>
<details>
<summary>摘要</summary>
主要方法 для模拟FMCW雷达是基于射线追踪，通常是计算昂贵的并不考虑背景噪声。这项工作提出了一种更快的FMCW雷达模拟方法，可以生成基于生成对抗网络（GAN）的合成雷达数据。代码和预训练 весов在GitHub上公开可用。这种方法生成了16个同时发射的毫声信号，这使得生成的数据可以用于进一步开发雷达数据处理算法（过滤和归一）。这可以增加数据增强的潜在性，例如通过生成不可能或安全关键的场景中的数据，以便在实际生产中不可能重现。在这项工作中，GAN被训练使用雷达测量数据，用于生成雷达数据的 sintetic raw radar chirps。为生成这些数据，雷达车辆的距离和高斯噪声作为神经网络的输入。生成的雷达毫声信号被评估使用Frechet InceptionDistance（FID）。然后，雷达距离-方位（RA）图被计算两次：首先，基于生成数据使用这个GAN，然后，基于实际数据。基于这些RA图，一种适应阈值和边检测算法用于物体检测。结果表明，生成的数据具有准确的干扰雷达反射和背景噪声，基于毫声信号、RA图和物体检测结果的比较。因此，该提出的方法在本工作中已经成功地减小了模拟到实际的差距。
</details></li>
</ul>
<hr>
<h2 id="BlindSage-Label-Inference-Attacks-against-Node-level-Vertical-Federated-Graph-Neural-Networks"><a href="#BlindSage-Label-Inference-Attacks-against-Node-level-Vertical-Federated-Graph-Neural-Networks" class="headerlink" title="BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks"></a>BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02465">http://arxiv.org/abs/2308.02465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Arazzi, Mauro Conti, Stefanos Koffas, Marina Krcek, Antonino Nocera, Stjepan Picek, Jing Xu</li>
<li>for: 这篇论文的主要目的是研究 vertical federated learning（VFL）中的标签推理攻击，特别是在没有背景知识的情况下进行攻击。</li>
<li>methods: 本文使用了 zero-background knowledge 策略来进行攻击，具体是使用 Graph Neural Networks（GNNs）作为目标模型，并在 node classification 任务上进行了实验。</li>
<li>results: 本文的 proposed attack，BlindSage，在实验中实现了 nearly 100% 的准确率，甚至在攻击者没有任何背景知识的情况下仍能实现准确率高于 85%。此外，本文发现了一些常见的防御措施不能对抗本攻击，而且这些防御措施会对模型的表现造成负面影响。<details>
<summary>Abstract</summary>
Federated learning enables collaborative training of machine learning models by keeping the raw data of the involved workers private. One of its main objectives is to improve the models' privacy, security, and scalability. Vertical Federated Learning (VFL) offers an efficient cross-silo setting where a few parties collaboratively train a model without sharing the same features. In such a scenario, classification labels are commonly considered sensitive information held exclusively by one (active) party, while other (passive) parties use only their local information. Recent works have uncovered important flaws of VFL, leading to possible label inference attacks under the assumption that the attacker has some, even limited, background knowledge on the relation between labels and data. In this work, we are the first (to the best of our knowledge) to investigate label inference attacks on VFL using a zero-background knowledge strategy. To concretely formulate our proposal, we focus on Graph Neural Networks (GNNs) as a target model for the underlying VFL. In particular, we refer to node classification tasks, which are widely studied, and GNNs have shown promising results. Our proposed attack, BlindSage, provides impressive results in the experiments, achieving nearly 100% accuracy in most cases. Even when the attacker has no information about the used architecture or the number of classes, the accuracy remained above 85% in most instances. Finally, we observe that well-known defenses cannot mitigate our attack without affecting the model's performance on the main classification task.
</details>
<details>
<summary>摘要</summary>
合作学习（Federated Learning）可以保持参与者的原始数据加密，以提高模型的隐私、安全性和可扩展性。垂直联合学习（VFL）提供了跨存储设置，在不共享同一个特征的情况下，几个党合作训练模型。在这种场景下，分类标签通常被视为活动党拥有的敏感信息，而其他投降党仅使用本地信息。现有研究曝光了VFL的重要漏洞，可能导致标签推理攻击，假设攻击者具有一定的背景知识关于标签和数据之间的关系。在这种情况下，我们是第一个（到我们知道的最佳）investigate VFL中的标签推理攻击，使用零背景知识策略。为了具体实现我们的提议，我们将Graph Neural Networks（GNNs）作为目标模型，特别是节点分类任务，这是广泛研究的领域，GNNs在这些任务上表现出色。我们提出的攻击方法，BlindSage，在实验中提供了很好的结果，在大多数情况下达到了nearly 100%的准确率。即使攻击者没有关于使用的架构或分类数量的任何信息，我们的攻击仍然在大多数情况下保持了超过85%的准确率。最后，我们发现了一些常见的防御无法防止我们的攻击，而不会影响模型在主要分类任务上的性能。
</details></li>
</ul>
<hr>
<h2 id="Universal-Approximation-of-Linear-Time-Invariant-LTI-Systems-through-RNNs-Power-of-Randomness-in-Reservoir-Computing"><a href="#Universal-Approximation-of-Linear-Time-Invariant-LTI-Systems-through-RNNs-Power-of-Randomness-in-Reservoir-Computing" class="headerlink" title="Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing"></a>Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02464">http://arxiv.org/abs/2308.02464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Jere, Lizhong Zheng, Karim Said, Lingjia Liu<br>for:RC is used to overcome the issues of vanishing and exploding gradients in standard RNN training, and it has demonstrated superior empirical performance in various fields. However, the theoretical grounding for this observed performance has not been fully developed.methods:The paper uses a signal processing interpretation of RC to universally approximate a general LTI system. The optimal probability distribution function for generating the recurrent weights of the underlying RNN of the RC is derived, and extensive numerical evaluations are provided to validate the optimality of the derived distribution.results:The paper shows that RC can universally approximate a general LTI system, and provides a clear signal processing-based model interpretability of RC. The derived optimal probability distribution function for the recurrent weights of the RC is found to be effective in simulating the LTI system. The work provides a complete analytical characterization of the untrained recurrent weights, which is important for explainable machine learning applications where training samples are limited.<details>
<summary>Abstract</summary>
Recurrent neural networks (RNNs) are known to be universal approximators of dynamic systems under fairly mild and general assumptions, making them good tools to process temporal information. However, RNNs usually suffer from the issues of vanishing and exploding gradients in the standard RNN training. Reservoir computing (RC), a special RNN where the recurrent weights are randomized and left untrained, has been introduced to overcome these issues and has demonstrated superior empirical performance in fields as diverse as natural language processing and wireless communications especially in scenarios where training samples are extremely limited. On the contrary, the theoretical grounding to support this observed performance has not been fully developed at the same pace. In this work, we show that RNNs can provide universal approximation of linear time-invariant (LTI) systems. Specifically, we show that RC can universally approximate a general LTI system. We present a clear signal processing interpretation of RC and utilize this understanding in the problem of simulating a generic LTI system through RC. Under this setup, we analytically characterize the optimal probability distribution function for generating the recurrent weights of the underlying RNN of the RC. We provide extensive numerical evaluations to validate the optimality of the derived optimum distribution of the recurrent weights of the RC for the LTI system simulation problem. Our work results in clear signal processing-based model interpretability of RC and provides theoretical explanation for the power of randomness in setting instead of training RC's recurrent weights. It further provides a complete optimum analytical characterization for the untrained recurrent weights, marking an important step towards explainable machine learning (XML) which is extremely important for applications where training samples are limited.
</details>
<details>
<summary>摘要</summary>
循环神经网络（RNN）是已知的通用函数近似器，可以处理时间信息。然而，标准的RNN训练过程中通常会出现消失和扩散梯度的问题。宽泛计算（RC）是一种特殊的RNN，其循环权重随机并未训练，可以解决这些问题，并在自然语言处理和无线通信等领域展现出优于标准RNN的实际表现。然而，对于RC的理论基础的发展并没有与实际表现相同的速度。在这个工作中，我们证明了RNN可以 universally approximate linear time-invariant（LTI）系统。具体来说，我们证明了RC可以 universally approximate任何LTI系统。我们提供了一个清晰的信号处理解释，用于理解RC的工作原理，并使用这种理解来解决一个通用LTI系统的模拟问题。在这个设置下，我们分析性地 caracterize了RC的优化梯度的概率分布。我们进行了广泛的数值评估，以验证我们所 derivated的RC的优化梯度概率分布的优化性。我们的工作具有以下优点：1. 我们提供了一个信号处理基于的RC模型解释，具有明确的数学基础。2. 我们提供了一个可靠的数值评估，以验证RC的优化梯度概率分布的优化性。3. 我们的结论可以用于解释Randomness在RC中的作用，并且提供了一个完整的优化梯度概率分布的分析。4. 我们的工作对于Explainable Machine Learning（XML）的发展具有重要意义，特别是在训练样本数量有限的情况下。总之，我们的工作提供了一个信号处理基于的RC模型解释，并且提供了一个可靠的数值评估，以验证RC的优化梯度概率分布的优化性。此外，我们的结论可以用于解释Randomness在RC中的作用，并且对于XML的发展具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Accurate-Reduced-Order-Modeling-of-a-MOOSE-based-Additive-Manufacturing-Model-with-Operator-Learning"><a href="#Fast-and-Accurate-Reduced-Order-Modeling-of-a-MOOSE-based-Additive-Manufacturing-Model-with-Operator-Learning" class="headerlink" title="Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning"></a>Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02462">http://arxiv.org/abs/2308.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</li>
<li>for: 本研究目的是为了提高additive manufacturing（AM）过程中的物料特性，通过 manipulate 生产过程参数以实现特定的物料特性。</li>
<li>methods: 本研究使用了Operator Learning（OL）方法，通过学习减少了过程变量的 differential equation 家族，以建立快速和准确的减少模型（ROM）。</li>
<li>results: 研究发现，OL方法可以与传统的深度神经网络（DNN）相比，在预测scalar模型响应时提供了相似的性能，且在精度和泛化性方面甚至超越DNN。DNN基于的ROM具有最快的训练时间，但是所有的ROM都比原始的MOOSE模型快速，且仍提供了准确的预测。FNO在预测时间序数据方面具有较小的平均预测误差，但是DeepONet具有较大的变化。不同于DNN，FNO和DeepONet都可以无需维度减少技术来预测时间序数据。<details>
<summary>Abstract</summary>
One predominant challenge in additive manufacturing (AM) is to achieve specific material properties by manipulating manufacturing process parameters during the runtime. Such manipulation tends to increase the computational load imposed on existing simulation tools employed in AM. The goal of the present work is to construct a fast and accurate reduced-order model (ROM) for an AM model developed within the Multiphysics Object-Oriented Simulation Environment (MOOSE) framework, ultimately reducing the time/cost of AM control and optimization processes. Our adoption of the operator learning (OL) approach enabled us to learn a family of differential equations produced by altering process variables in the laser's Gaussian point heat source. More specifically, we used the Fourier neural operator (FNO) and deep operator network (DeepONet) to develop ROMs for time-dependent responses. Furthermore, we benchmarked the performance of these OL methods against a conventional deep neural network (DNN)-based ROM. Ultimately, we found that OL methods offer comparable performance and, in terms of accuracy and generalizability, even outperform DNN at predicting scalar model responses. The DNN-based ROM afforded the fastest training time. Furthermore, all the ROMs were faster than the original MOOSE model yet still provided accurate predictions. FNO had a smaller mean prediction error than DeepONet, with a larger variance for time-dependent responses. Unlike DNN, both FNO and DeepONet were able to simulate time series data without the need for dimensionality reduction techniques. The present work can help facilitate the AM optimization process by enabling faster execution of simulation tools while still preserving evaluation accuracy.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在添加制造（AM）是在运行时控制和优化过程中实现特定材料性能。这种控制通常会增加现有的Simulation工具在AM中的计算负担。目标是在MOOSE框架中开发一个快速和准确的减少维度模型（ROM），以降低AM控制和优化过程的时间/成本。我们采用了运算学（OL）方法，通过修改过程变量来学习激光的 Gaussian点热源生成的家族 diffeqential equations。我们使用了Fourier neural operator（FNO）和深度运算网络（DeepONet）来开发ROMs，并对这些OL方法与传统的深度神经网络（DNN）基于ROM进行比较。结果表明，OL方法可以与DNN相比，具有相同的性能和精度，并且在预测批量响应方面更高一点。DNN基于ROM培训时间最快，但所有ROM都比原始MOOSE模型更快，并且仍然提供了准确的预测。FNO的平均预测误差较小， DeepONet在时间相对应的响应中有较大的变化。不同于DNN，FNO和DeepONet都可以不需要维度减少技术来预测时间序列数据。现有的工作可以帮助加快AM优化过程中的Simulation工具执行，保持评估准确性。
</details></li>
</ul>
<hr>
<h2 id="Nonprehensile-Planar-Manipulation-through-Reinforcement-Learning-with-Multimodal-Categorical-Exploration"><a href="#Nonprehensile-Planar-Manipulation-through-Reinforcement-Learning-with-Multimodal-Categorical-Exploration" class="headerlink" title="Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration"></a>Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02459">http://arxiv.org/abs/2308.02459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar</li>
<li>for: 开发能够实现dexterous nonprehensile manipulation的 робот控制器，如pushing an object on a table。</li>
<li>methods: 使用Reinforcement Learning（RL）框架，并提出了多模态探索方法，以处理非线性和不确定性。</li>
<li>results: 实现了高精度、非平滑曲线和复杂运动的推动政策，并且可以承受外部干扰和观测噪声，同时也可以在多个推动器情况下进行缩放。<details>
<summary>Abstract</summary>
Developing robot controllers capable of achieving dexterous nonprehensile manipulation, such as pushing an object on a table, is challenging. The underactuated and hybrid-dynamics nature of the problem, further complicated by the uncertainty resulting from the frictional interactions, requires sophisticated control behaviors. Reinforcement Learning (RL) is a powerful framework for developing such robot controllers. However, previous RL literature addressing the nonprehensile pushing task achieves low accuracy, non-smooth trajectories, and only simple motions, i.e. without rotation of the manipulated object. We conjecture that previously used unimodal exploration strategies fail to capture the inherent hybrid-dynamics of the task, arising from the different possible contact interaction modes between the robot and the object, such as sticking, sliding, and separation. In this work, we propose a multimodal exploration approach through categorical distributions, which enables us to train planar pushing RL policies for arbitrary starting and target object poses, i.e. positions and orientations, and with improved accuracy. We show that the learned policies are robust to external disturbances and observation noise, and scale to tasks with multiple pushers. Furthermore, we validate the transferability of the learned policies, trained entirely in simulation, to a physical robot hardware using the KUKA iiwa robot arm. See our supplemental video: https://youtu.be/vTdva1mgrk4.
</details>
<details>
<summary>摘要</summary>
开发能够实现灵活无握 manipulate robot控制器，如表面上推pushing一个物体，是一项挑战。由于控制器的下降启动和混合动力学性质，以及由摩擦产生的不确定性，需要复杂的控制行为。 reinforcement learning (RL) 是一个强大的框架 для开发这类 robot控制器。然而，过去RL文献中对非握持推动任务的精度、非精炸曲线和简单运动（即不含旋转）很低。我们 conjecture  previous 使用单模态探索策略 failed  to capture 任务的内在混合动力学性质， arise  from 不同的接触交互方式 between the robot and the object， such as sticking, sliding, and separation.在这种工作中，我们提议使用多模态探索方法，通过 categorical distributions，可以训练平面推动 RL 政策，对于任意开始和目标物体姿态（位置和 orientations），并具有改进的精度。我们显示了学习的策略对于外部干扰和观察噪声具有Robustness，并可扩展到多个推动者任务。此外，我们验证了学习策略，完全在 simulator 中训练，在物理 Kuka iiwa 机械臂上运行。请参考我们的补充视频：https://youtu.be/vTdva1mgrk4.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-and-Propagation-in-Accelerated-MRI-Reconstruction"><a href="#Uncertainty-Estimation-and-Propagation-in-Accelerated-MRI-Reconstruction" class="headerlink" title="Uncertainty Estimation and Propagation in Accelerated MRI Reconstruction"></a>Uncertainty Estimation and Propagation in Accelerated MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02631">http://arxiv.org/abs/2308.02631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paulkogni/mr-recon-uq">https://github.com/paulkogni/mr-recon-uq</a></li>
<li>paper_authors: Paul Fischer, Thomas Küstner, Christian F. Baumgartner</li>
<li>for: 这篇论文是关于基于深度学习的MRI重建技术的研究，尤其是在高速 Settings下 achievable 的重建质量。</li>
<li>methods: 该论文提出了一种新的 probabilistic reconstruction technique (PHiRec)，基于conditional hierarchical variational autoencoders的想法。</li>
<li>results: 该方法可以生成高质量的重建结果，同时也可以提供substantially better calibrated的uncertainty quantification，以及可以协调到下游 segmentation 任务中的uncertainty estimate。<details>
<summary>Abstract</summary>
MRI reconstruction techniques based on deep learning have led to unprecedented reconstruction quality especially in highly accelerated settings. However, deep learning techniques are also known to fail unexpectedly and hallucinate structures. This is particularly problematic if reconstructions are directly used for downstream tasks such as real-time treatment guidance or automated extraction of clinical paramters (e.g. via segmentation). Well-calibrated uncertainty quantification will be a key ingredient for safe use of this technology in clinical practice. In this paper we propose a novel probabilistic reconstruction technique (PHiRec) building on the idea of conditional hierarchical variational autoencoders. We demonstrate that our proposed method produces high-quality reconstructions as well as uncertainty quantification that is substantially better calibrated than several strong baselines. We furthermore demonstrate how uncertainties arising in the MR econstruction can be propagated to a downstream segmentation task, and show that PHiRec also allows well-calibrated estimation of segmentation uncertainties that originated in the MR reconstruction process.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Modelling-of-Levy-Area-for-High-Order-SDE-Simulation"><a href="#Generative-Modelling-of-Levy-Area-for-High-Order-SDE-Simulation" class="headerlink" title="Generative Modelling of Lévy Area for High Order SDE Simulation"></a>Generative Modelling of Lévy Area for High Order SDE Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02452">http://arxiv.org/abs/2308.02452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andraž Jelinčič, Jiajie Tao, William F. Turner, Thomas Cass, James Foster, Hao Ni</li>
<li>for: 这篇论文是为了提出一种基于深度学习的模型，用于生成精确的莱比零区 conditional on  Брау内幂增量。</li>
<li>methods: 这种模型使用了一种专门设计的 GNN-inspired 架构，以保证输出分布和 conditioning 变量之间的正确依赖关系。同时，使用了一种基于特征函数的数学原理的 discriminator。</li>
<li>results: 对于 4 维 Брау内幂，这种模型 exhibits state-of-the-art 性能 across 多个维度，并且在数学金融中的 log-Heston 模型中进行了一个数值实验，证明了高质量的 synthetic 莱比零区可以导致高阶弱 convergence 和 variance reduction when using multilevel Monte Carlo (MLMC)。<details>
<summary>Abstract</summary>
It is well known that, when numerically simulating solutions to SDEs, achieving a strong convergence rate better than O(\sqrt{h}) (where h is the step size) requires the use of certain iterated integrals of Brownian motion, commonly referred to as its "L\'{e}vy areas". However, these stochastic integrals are difficult to simulate due to their non-Gaussian nature and for a d-dimensional Brownian motion with d > 2, no fast almost-exact sampling algorithm is known.   In this paper, we propose L\'{e}vyGAN, a deep-learning-based model for generating approximate samples of L\'{e}vy area conditional on a Brownian increment. Due to our "Bridge-flipping" operation, the output samples match all joint and conditional odd moments exactly. Our generator employs a tailored GNN-inspired architecture, which enforces the correct dependency structure between the output distribution and the conditioning variable. Furthermore, we incorporate a mathematically principled characteristic-function based discriminator. Lastly, we introduce a novel training mechanism termed "Chen-training", which circumvents the need for expensive-to-generate training data-sets. This new training procedure is underpinned by our two main theoretical results.   For 4-dimensional Brownian motion, we show that L\'{e}vyGAN exhibits state-of-the-art performance across several metrics which measure both the joint and marginal distributions. We conclude with a numerical experiment on the log-Heston model, a popular SDE in mathematical finance, demonstrating that high-quality synthetic L\'{e}vy area can lead to high order weak convergence and variance reduction when using multilevel Monte Carlo (MLMC).
</details>
<details>
<summary>摘要</summary>
它已经广泛知道，当数值实现解决涨落方程时，以较好的减法速率（即幂函数）为依据，需要使用某些迭代积分的布朗运动，通常称为其"Lévy区域"。然而，这些随机积分具有非高斯性质，而且为高维布朗运动（d > 2），没有快速准确样本生成算法。在这篇论文中，我们提出了LévyGAN，一种基于深度学习的模型，用于生成 conditional Lévy area 的相似样本。由于我们的 "桥跃" 操作，输出样本满足所有的共同偶极值和条件偶极值。我们的生成器采用了特制的 GNN-inspired 架构，以保证输出分布和条件变量之间的正确依赖关系。此外，我们采用了基于特征函数的数学原理的批量分类器。最后，我们介绍了一种新的训练机制，称为 "Chen-training"，它使得不需要生成昂贵的训练数据集。这种新的训练过程基于我们的两个主要理论结论。对于四维布朗运动，我们表明了LévyGAN在多个维度上的性能都达到了状态机器人的水平。我们结束于一个对柯本方程（一种流行的数学金融方程）的数字实验，展示了高质量的 synthetic Lévy area 可以导致高阶弱整合和变量减少，当使用多层 Monte Carlo（MLMC）时。
</details></li>
</ul>
<hr>
<h2 id="Pruning-a-neural-network-using-Bayesian-inference"><a href="#Pruning-a-neural-network-using-Bayesian-inference" class="headerlink" title="Pruning a neural network using Bayesian inference"></a>Pruning a neural network using Bayesian inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02451">http://arxiv.org/abs/2308.02451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunil Mathew, Daniel B. Rowe</li>
<li>for: 这个研究论文的目的是提出一种基于 Bayesian 推理的神经网络减少技术，以降低大神经网络的计算和存储占用。</li>
<li>methods: 该方法使用 Bayesian 推理，将神经网络的 posterior 概率分布计算到减少前和减少后，然后利用这些概率导向 iterative 减少。</li>
<li>results: 经过对多个 benchmarck 进行全面评估，我们表明了我们的方法可以实现 жела的稀有性，同时保持竞争性的准确率。<details>
<summary>Abstract</summary>
Neural network pruning is a highly effective technique aimed at reducing the computational and memory demands of large neural networks. In this research paper, we present a novel approach to pruning neural networks utilizing Bayesian inference, which can seamlessly integrate into the training procedure. Our proposed method leverages the posterior probabilities of the neural network prior to and following pruning, enabling the calculation of Bayes factors. The calculated Bayes factors guide the iterative pruning. Through comprehensive evaluations conducted on multiple benchmarks, we demonstrate that our method achieves desired levels of sparsity while maintaining competitive accuracy.
</details>
<details>
<summary>摘要</summary>
大脑网络剪辑是一种非常有效的技术，用于减少大脑网络的计算和内存占用。在这篇研究报告中，我们提出了一种基于 bayesian 推理的 neural network 剪辑方法，可以轻松地 integrate 到训练过程中。我们的提议方法利用 neural network 之前和之后剪辑 posterior 概率，以计算 bayes 因子。计算出的 bayes 因子导引了迭代剪辑。我们在多个 benchmark 上进行了广泛的评估，并证明了我们的方法可以达到所需的稀疏程度，同时保持竞争性的准确率。
</details></li>
</ul>
<hr>
<h2 id="From-Military-to-Healthcare-Adopting-and-Expanding-Ethical-Principles-for-Generative-Artificial-Intelligence"><a href="#From-Military-to-Healthcare-Adopting-and-Expanding-Ethical-Principles-for-Generative-Artificial-Intelligence" class="headerlink" title="From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence"></a>From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02448">http://arxiv.org/abs/2308.02448</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Oniani, Jordan Hilsman, Yifan Peng, COL, Ronald K. Poropatich, COL Jeremy C. Pamplin, LTC Gary L. Legault, Yanshan Wang</li>
<li>For: The paper aims to propose ethical principles for the use of generative AI in healthcare, with the goal of addressing ethical dilemmas and challenges in the integration of this technology in the medical field.* Methods: The paper uses a framework called GREAT PLEA, which stands for Governance, Reliability, Equity, Accountability, Traceability, Privacy, Lawfulness, Empathy, and Autonomy, to guide the development of ethical principles for generative AI in healthcare.* Results: The paper proposes a set of ethical principles for generative AI in healthcare, with the goal of proactively addressing the ethical dilemmas and challenges posed by the integration of this technology in the medical field. These principles include governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy.<details>
<summary>Abstract</summary>
In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about its application in healthcare, mainly due to concerns about transparency and related issues. Meanwhile, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. However, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI. In this paper, we propose GREAT PLEA ethical principles, encompassing governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim to proactively address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare.
</details>
<details>
<summary>摘要</summary>
在2020年，美国国防部官方公布了一组伦理原则，用于导引人工智能技术在未来战场上的使用。尽管在不同的情况下，军人和医疗人员之间有 stark 的不同，但是在面临生命改变的情况下，他们都需要快速做出决策。战场上的战士常常面临生命改变的情况，需要快速做出决策。医疗人员在医疗环境中也经常面临快速变化的情况，如在急诊室或在治疗生命危险的情况下。新兴的生成型人工智能技术在计算能力的提高和医疗数据的增加下，将健康卫生领域 revolutionized。 recent years, this technology has attracted significant attention from the research community, leading to debates about its application in healthcare, primarily due to concerns about transparency and related issues. However, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. Despite this, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI.在这篇论文中，我们提出了 GREAT PLEA 伦理原则，包括政府、可靠性、公平性、责任、可追溯性、隐私、法律合法性、 Empathy 和自主权，用于生成型人工智能技术在健康卫生领域中的应用。我们希望通过这些原则，active address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Preferential-Attached-kNN-Graph-with-Distribution-Awareness"><a href="#Adaptive-Preferential-Attached-kNN-Graph-with-Distribution-Awareness" class="headerlink" title="Adaptive Preferential Attached kNN Graph with Distribution-Awareness"></a>Adaptive Preferential Attached kNN Graph with Distribution-Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02442">http://arxiv.org/abs/2308.02442</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/4alexmin/knnsotas">https://github.com/4alexmin/knnsotas</a></li>
<li>paper_authors: Shaojie Min, Ji Liu</li>
<li>for: 提高机器学习任务中的总体性能和精度，特别是在具有复杂分布的实际数据上。</li>
<li>methods: 基于分布情况的 adaptive-k 技术，在建构图时采用分布信息作为一体。</li>
<li>results: 在多个实际数据集上进行了严格的评估，并证明了 paNNG 在不同场景下的适应性和效果优于现有算法。<details>
<summary>Abstract</summary>
Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks due to their simplicity and effectiveness. However, as factual data often inherit complex distributions, the conventional kNN graph's reliance on a unified k-value can hinder its performance. A crucial factor behind this challenge is the presence of ambiguous samples along decision boundaries that are inevitably more prone to incorrect classifications. To address the situation, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which adopts distribution-aware adaptive-k into graph construction. By incorporating distribution information as a cohesive entity, paNNG can significantly improve performance on ambiguous samples by "pulling" them towards their original classes and hence enhance overall generalization capability. Through rigorous evaluations on diverse datasets, paNNG outperforms state-of-the-art algorithms, showcasing its adaptability and efficacy across various real-world scenarios.
</details>
<details>
<summary>摘要</summary>
基于图的kNN算法在机器学习任务中广泛受欢迎，因为它们简单易用而且有效。然而，由于实际数据经常具有复杂的分布，传统的kNN图中对一个固定的k值的依赖可能会降低其表现。这个挑战的关键原因在于具有抽象样本的异常高错误率，这些样本通常位于分类边界附近。为解决这个问题，我们提出了Preferential Attached k-Nearest Neighbors Graph（paNNG），该算法在图struc图构建中采用了分布情况的敏感性。通过将分布信息作为一个整体纳入图构建，paNNG可以在抽象样本上提高表现，“拖”这些样本向其原来的类别，从而提高总的泛化能力。经过严谨的评估，paNNG在多种实际场景中舒适地超越了当前的状态árt算法，示出了它的适应性和效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/05/cs.LG_2023_08_05/" data-id="clly4xtds006dvl8805vs7ky9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/cs.SD_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/05/cs.SD_2023_08_05/">cs.SD - 2023-08-05 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ApproBiVT-Lead-ASR-Models-to-Generalize-Better-Using-Approximated-Bias-Variance-Tradeoff-Guided-Early-Stopping-and-Checkpoint-Averaging"><a href="#ApproBiVT-Lead-ASR-Models-to-Generalize-Better-Using-Approximated-Bias-Variance-Tradeoff-Guided-Early-Stopping-and-Checkpoint-Averaging" class="headerlink" title="ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging"></a>ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02870">http://arxiv.org/abs/2308.02870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fangyuan Wang, Ming Hao, Yuhai Shi, Bo Xu</li>
<li>for: 提高自动语音识别（ASR）模型的性能，通过重新评估和更新早期停止和多个检查点的策略，以降低模型的总泛化误差。</li>
<li>methods: 使用适应停止和多个检查点的策略，通过评估训练损失和验证损失来评估模型的偏差和异谱，并通过Approximated Bias-Variance Tradeoff（ApproBiVT）来导引这些策略。</li>
<li>results: 在使用高级ASR模型时，该策略可以提供2.5%-3.7%和3.1%-4.6%的CER减少在AISHELL-1和AISHELL-2上。<details>
<summary>Abstract</summary>
The conventional recipe for Automatic Speech Recognition (ASR) models is to 1) train multiple checkpoints on a training set while relying on a validation set to prevent overfitting using early stopping and 2) average several last checkpoints or that of the lowest validation losses to obtain the final model. In this paper, we rethink and update the early stopping and checkpoint averaging from the perspective of the bias-variance tradeoff. Theoretically, the bias and variance represent the fitness and variability of a model and the tradeoff of them determines the overall generalization error. But, it's impractical to evaluate them precisely. As an alternative, we take the training loss and validation loss as proxies of bias and variance and guide the early stopping and checkpoint averaging using their tradeoff, namely an Approximated Bias-Variance Tradeoff (ApproBiVT). When evaluating with advanced ASR models, our recipe provides 2.5%-3.7% and 3.1%-4.6% CER reduction on the AISHELL-1 and AISHELL-2, respectively.
</details>
<details>
<summary>摘要</summary>
传统的自动语音识别（ASR）模型的制作方法是：1）在训练集上训练多个检查点，使用验证集来防止过拟合，并2）使用多个最后的检查点或验证损失最低的一个来获取最终模型。在这篇论文中，我们重新思考了早期停止和检查点平均的方法，从偏差-差异质量的角度来更新。在理论上，偏差和差异表示模型的适应度和多样性，它们之间的质量交换决定总的泛化误差。但是，不可能准确地评估它们。因此，我们使用训练损失和验证损失作为偏差和差异的代理，并通过它们之间的质量交换来导引早期停止和检查点平均，即 Approximated Bias-Variance Tradeoff（ApproBiVT）。在使用高级ASR模型进行评估时，我们的制作方法可以提供2.5%-3.7%和3.1%-4.6%的CER减少在AISHELL-1和AISHELL-2上。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Exploration-of-Joint-training-for-Singing-Voice-Synthesis"><a href="#A-Systematic-Exploration-of-Joint-training-for-Singing-Voice-Synthesis" class="headerlink" title="A Systematic Exploration of Joint-training for Singing Voice Synthesis"></a>A Systematic Exploration of Joint-training for Singing Voice Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02867">http://arxiv.org/abs/2308.02867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuning Wu, Yifeng Yu, Jiatong Shi, Tao Qian, Qin Jin</li>
<li>for: 提高 singing voice synthesis（SVS）系统的性能和可读性。</li>
<li>methods: 通过对音响模型和 vocoder 进行共同训练，提高 SVS 系统的 JOINT 训练性能。</li>
<li>results: 通过实验表明，我们的 JOINT 训练策略在不同数据集上具有更稳定的性能提升，同时也提高了整个框架的可读性。<details>
<summary>Abstract</summary>
There has been a growing interest in using end-to-end acoustic models for singing voice synthesis (SVS). Typically, these models require an additional vocoder to transform the generated acoustic features into the final waveform. However, since the acoustic model and the vocoder are not jointly optimized, a gap can exist between the two models, leading to suboptimal performance. Although a similar problem has been addressed in the TTS systems by joint-training or by replacing acoustic features with a latent representation, adopting corresponding approaches to SVS is not an easy task. How to improve the joint-training of SVS systems has not been well explored. In this paper, we conduct a systematic investigation of how to better perform a joint-training of an acoustic model and a vocoder for SVS. We carry out extensive experiments and demonstrate that our joint-training strategy outperforms baselines, achieving more stable performance across different datasets while also increasing the interpretability of the entire framework.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>现在，使用端到端音声模型进行唱歌voice synthesis（SVS）已经有越来越多的兴趣。通常，这些模型需要一个额外的 vocoder 将生成的音声特征转换成最终波形。然而，由于音声模型和 vocoder 并没有同时优化，因此两个模型之间可能存在一个差距，导致表现不佳。虽然在 TTS 系统中Addressing 类似问题的方法已经被研究，但在 SVS 中采用相应的方法并不是一件容易的事情。如何改进 SVS 系统的 JOINT 训练方法尚未得到了充分的探索。在这篇论文中，我们进行了系统的调查，探讨了如何更好地进行 SVS 系统的 JOINT 训练。我们进行了广泛的实验，并证明了我们的 JOINT 训练策略在不同的数据集上表现更稳定，同时也提高了整个框架的可解释性。
</details></li>
</ul>
<hr>
<h2 id="Bootstrapping-Contrastive-Learning-Enhanced-Music-Cold-Start-Matching"><a href="#Bootstrapping-Contrastive-Learning-Enhanced-Music-Cold-Start-Matching" class="headerlink" title="Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching"></a>Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02844">http://arxiv.org/abs/2308.02844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinping Zhao, Ying Zhang, Qiang Xiao, Yuming Ren, Yingchun Yang</li>
<li>for: 这个论文主要针对的是音乐冷启动匹配任务，即给出一个冷启动歌曲请求，然后快速推送该歌曲到相关歌曲的听众中以进行温身。但是，关于这个任务的研究非常有限，因此本文将music冷启动匹配问题进行详细定义并提供一种方案。</li>
<li>methods: 在线下训练中，我们尝试通过歌曲内容特征来学习高质量的歌曲表示。然而，我们发现监督信号通常遵循力学律分布，导致表示学习受到扭曲。为解决这个问题，我们提出了一种新的对比学习方法 named Bootstrapping Contrastive Learning (BCL)，通过对比激活来强制高质量表示学习。</li>
<li>results: 在线下数据集和在线系统上进行了广泛的实验，并证明了我们的方法的有效性和高效性。目前，我们已经将其部署到NetEase Cloud Music上，影响了百万用户。代码将在未来发布。<details>
<summary>Abstract</summary>
We study a particular matching task we call Music Cold-Start Matching. In short, given a cold-start song request, we expect to retrieve songs with similar audiences and then fastly push the cold-start song to the audiences of the retrieved songs to warm up it. However, there are hardly any studies done on this task. Therefore, in this paper, we will formalize the problem of Music Cold-Start Matching detailedly and give a scheme. During the offline training, we attempt to learn high-quality song representations based on song content features. But, we find supervision signals typically follow power-law distribution causing skewed representation learning. To address this issue, we propose a novel contrastive learning paradigm named Bootstrapping Contrastive Learning (BCL) to enhance the quality of learned representations by exerting contrastive regularization. During the online serving, to locate the target audiences more accurately, we propose Clustering-based Audience Targeting (CAT) that clusters audience representations to acquire a few cluster centroids and then locate the target audiences by measuring the relevance between the audience representations and the cluster centroids. Extensive experiments on the offline dataset and online system demonstrate the effectiveness and efficiency of our method. Currently, we have deployed it on NetEase Cloud Music, affecting millions of users. Code will be released in the future.
</details>
<details>
<summary>摘要</summary>
我们研究一个名为音乐冷启始匹配的特定任务。简而言之，给定一个冷启始歌曲请求，我们希望可以检索到与其相似的听众，然后快速推送冷启始歌曲到检索到的听众中，以便让其热身。然而，目前 hardly any studies have been conducted on this task.因此，在这篇论文中，我们将 Music Cold-Start Matching 问题进行详细化 formalization，并提出一种方案。在线上训练中，我们尝试学习高质量的歌曲表示，基于歌曲内容特征。然而，我们发现超级vision signals  Typically follow a power-law distribution, causing skewed representation learning. To address this issue, we propose a novel contrastive learning paradigm named Bootstrapping Contrastive Learning (BCL) to enhance the quality of learned representations by exerting contrastive regularization.在在线服务中，我们提出一种名为 Clustering-based Audience Targeting (CAT) 的方法，通过对听众表示进行 clustering，获得一些集中点，然后通过测量听众表示和集中点之间的相似性，准确地定位目标听众。我们对做了大量的实验，证明了我们的方法的有效性和高效性。目前，我们已经将其部署到 NetEase Cloud Music，影响了数百万用户。代码将在未来发布。
</details></li>
</ul>
<hr>
<h2 id="Self-Distillation-Network-with-Ensemble-Prototypes-Learning-Robust-Speaker-Representations-without-Supervision"><a href="#Self-Distillation-Network-with-Ensemble-Prototypes-Learning-Robust-Speaker-Representations-without-Supervision" class="headerlink" title="Self-Distillation Network with Ensemble Prototypes: Learning Robust Speaker Representations without Supervision"></a>Self-Distillation Network with Ensemble Prototypes: Learning Robust Speaker Representations without Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02774">http://arxiv.org/abs/2308.02774</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/3D-Speaker">https://github.com/alibaba-damo-academy/3D-Speaker</a></li>
<li>paper_authors: Yafeng Chen, Siqi Zheng, Qian Chen</li>
<li>for: 本研究旨在无需使用说话人标签进行说话人验证系统的自我学习和鲁棒化。</li>
<li>methods: 本文提出了一种高效的Self-Distillation网络 Ensemble Prototypes（SDEP），用于实现无监督的说话人表示学习。</li>
<li>results: 在VoxCeleb数据集上进行了多种实验，并证明了SDEP框架在说话人验证中具有最高精度。SDEP在Voxceleb1 speaker verification评价标准上达到了新的最佳性能（即错误率1.94%、1.99%和3.77%），不使用任何说话人标签 durante el entrenamiento。<details>
<summary>Abstract</summary>
Training speaker-discriminative and robust speaker verification systems without speaker labels is still challenging and worthwhile to explore. Previous studies have noted a substantial performance disparity between self-supervised and fully supervised approaches. In this paper, we propose an effective Self-Distillation network with Ensemble Prototypes (SDEP) to facilitate self-supervised speaker representation learning. A range of experiments conducted on the VoxCeleb datasets demonstrate the superiority of the SDEP framework in speaker verification. SDEP achieves a new SOTA on Voxceleb1 speaker verification evaluation benchmark ( i.e., equal error rate 1.94\%, 1.99\%, and 3.77\% for trial Vox1-O, Vox1-E and Vox1-H , respectively), discarding any speaker labels in the training phase. Code will be publicly available at https://github.com/alibaba-damo-academy/3D-Speaker.
</details>
<details>
<summary>摘要</summary>
<SYS>使用无标签的语音训练SpeakerVerification系统仍然是一项挑战性的任务，值得进行探索。过去的研究表明，自动学习和完全监督方法之间存在很大的性能差距。本文提出了一种高效的Self-Distillation网络加ensemble Prototypes（SDEP），用于实现无标签语音表示学习。在VoxCeleb dataset上进行了多种实验，得到了SDEP框架在Speaker Verification中的新最佳性能（即Voxceleb1的误差率为1.94%、1.99%和3.77%），不使用任何语音标签 during training phase。代码将在https://github.com/alibaba-damo-academy/3D-Speaker上公开。</SYS>Note: "SDEP" stands for "Self-Distillation network with Ensemble Prototypes".
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/05/cs.SD_2023_08_05/" data-id="clly4xtem009ivl889kpg9zlq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/eess.IV_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/05/eess.IV_2023_08_05/">eess.IV - 2023-08-05 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Flashlight-Search-Medial-Axis-A-Pixel-Free-Pore-Network-Extraction-Algorithm"><a href="#Flashlight-Search-Medial-Axis-A-Pixel-Free-Pore-Network-Extraction-Algorithm" class="headerlink" title="Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm"></a>Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10990">http://arxiv.org/abs/2308.10990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Liu, Tao Zhang, Shuyu Sun</li>
<li>for: 该研究旨在提出一种无像素的维持精度的脉径网络EXTRACTION方法，以便在大规模的含材质媒体中实现流体流动的研究。</li>
<li>methods: 该方法基于探灯搜索中心轴（FSMA）算法，在维持精度的情况下大幅降低计算复杂性，可以应用于多种不同的含材质媒体和流体流动情况。</li>
<li>results: 实验结果表明，FSMA算法可以准确地找到脉径网络，无论含材质媒体的结构如何，也无论脉径和喉径中心的位置如何。此外，该算法还可以检测死端脉径，这对于多相流动在含材质媒体中的研究非常重要。<details>
<summary>Abstract</summary>
Pore-network models (PNMs) have become an important tool in the study of fluid flow in porous media over the last few decades, and the accuracy of their results highly depends on the extraction of pore networks. Traditional methods of pore-network extraction are based on pixels and require images with high quality. Here, a pixel-free method called the flashlight search medial axis (FSMA) algorithm is proposed for pore-network extraction in a continuous space. The search domain in a two-dimensional space is a line, whereas a surface domain is searched in a three-dimensional scenario. Thus, the FSMA algorithm follows the dimensionality reduction idea; the medial axis can be identified using only a few points instead of calculating every point in the void space. In this way, computational complexity of this method is greatly reduced compared to that of traditional pixel-based extraction methods, thus enabling large-scale pore-network extraction. Based on cases featuring two- and three-dimensional porous media, the FSMA algorithm performs well regardless of the topological structure of the pore network or the positions of the pore and throat centers. This algorithm can also be used to examine both closed- and open-boundary cases. Finally, the FSMA algorithm can search dead-end pores, which is of great significance in the study of multiphase flow in porous media.
</details>
<details>
<summary>摘要</summary>
PORE-NETWORK MODELS (PNMs) 已经在过去几十年内成为论 fluid flow 在孔隙媒体的研究工具，而实际结果的准确性很大程度上取决于破孔网络的提取。传统的破孔网络提取方法基于像素，需要高质量的图像。在这里，一种没有像素的方法 called  flashlight search medial axis (FSMA) 算法是用于破孔网络提取的。在两维空间中，搜索域是一条直线，而在三维情况下，搜索域是一个表面。因此，FSMA 算法遵循缩放空间的想法，通过仅在缺空间中标识中点而不是计算所有点。这样，FSMA 算法的计算复杂性被大幅降低，从而实现大规模的破孔网络提取。无论破孔网络的结构是二维还是三维，FSMA 算法都能够正确地提取破孔网络。此外，该算法还可以处理封闭边界和开放边界两种情况。最后，FSMA 算法还可以搜索死绕孔，这对多相流在孔隙媒体的研究非常重要。
</details></li>
</ul>
<hr>
<h2 id="Landmark-Detection-using-Transformer-Toward-Robot-assisted-Nasal-Airway-Intubation"><a href="#Landmark-Detection-using-Transformer-Toward-Robot-assisted-Nasal-Airway-Intubation" class="headerlink" title="Landmark Detection using Transformer Toward Robot-assisted Nasal Airway Intubation"></a>Landmark Detection using Transformer Toward Robot-assisted Nasal Airway Intubation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02845">http://arxiv.org/abs/2308.02845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/conorlth/airway_intubation_landmarks_detection">https://github.com/conorlth/airway_intubation_landmarks_detection</a></li>
<li>paper_authors: Tianhang Liu, Hechen Li, Long Bai, Yanan Wu, An Wang, Mobarakol Islam, Hongliang Ren</li>
<li>for: 该研究旨在提供一种高精度的自动标记检测方法，用于机器人协助的鼻腔插管。</li>
<li>methods: 该方法基于变换器（DeTR），并采用了可变DeTR和语义对齐匹配模块来检测鼻腔中的两个重要标记（鼻孔和肺膜）。</li>
<li>results: 实验结果表明，该方法可以具有竞争力的检测精度。<details>
<summary>Abstract</summary>
Robot-assisted airway intubation application needs high accuracy in locating targets and organs. Two vital landmarks, nostrils and glottis, can be detected during the intubation to accommodate the stages of nasal intubation. Automated landmark detection can provide accurate localization and quantitative evaluation. The Detection Transformer (DeTR) leads object detectors to a new paradigm with long-range dependence. However, current DeTR requires long iterations to converge, and does not perform well in detecting small objects. This paper proposes a transformer-based landmark detection solution with deformable DeTR and the semantic-aligned-matching module for detecting landmarks in robot-assisted intubation. The semantics aligner can effectively align the semantics of object queries and image features in the same embedding space using the most discriminative features. To evaluate the performance of our solution, we utilize a publicly accessible glottis dataset and automatically annotate a nostril detection dataset. The experimental results demonstrate our competitive performance in detection accuracy. Our code is publicly accessible.
</details>
<details>
<summary>摘要</summary>
机器人协助气管插管应用需要高精度在目标和器官的位置检测。气管插管过程中可以检测到两个重要的特征点，即鼻孔和软颈椎。自动化特征点检测可以提供高精度的位置定位和量化评估。 however， current DeTR requires long iterations to converge, and does not perform well in detecting small objects. This paper proposes a transformer-based landmark detection solution with deformable DeTR and the semantic-aligned-matching module for detecting landmarks in robot-assisted intubation. The semantics aligner can effectively align the semantics of object queries and image features in the same embedding space using the most discriminative features. To evaluate the performance of our solution, we utilize a publicly accessible glottis dataset and automatically annotate a nostril detection dataset. The experimental results demonstrate our competitive performance in detection accuracy. Our code is publicly accessible.Here's the translation in Traditional Chinese:机器人协助气管插管应用需要高精度在目标和器官的位置检测。气管插管过程中可以检测到两个重要的特征点，即鼻孔和软颈椎。自动化特征点检测可以提供高精度的位置定位和量化评估。然而， current DeTR需要长迭代才能融合，并不能够检测小型物体。这篇论文提出了一个基于 transformer 的特征点检测解决方案，该解决方案包括扭转 DeTR 和对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩�
</details></li>
</ul>
<hr>
<h2 id="Non-line-of-sight-reconstruction-via-structure-sparsity-regularization"><a href="#Non-line-of-sight-reconstruction-via-structure-sparsity-regularization" class="headerlink" title="Non-line-of-sight reconstruction via structure sparsity regularization"></a>Non-line-of-sight reconstruction via structure sparsity regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02782">http://arxiv.org/abs/2308.02782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duolan Huang, Quan Chen, Zhun Wei, Rui Chen</li>
<li>for: 本研究旨在提高非线视场（NLOS）成像质量，使其能够应用于自动驾驶、机器人视觉、医疗成像、安全监测等领域。</li>
<li>methods: 本研究使用了结构稀热（SS）正则化方法，通过利用方向光束变换（DLCT）模型中的核心矩阵来捕捉方向性的隐藏信息，从而提高NLOS成像的稀热性。</li>
<li>results: 经过实验和synthetic数据的评估，提出的方法可以在短时间和低SNR情况下提供高质量的NLOS成像，并且超过了现有的重建算法，特别是在封闭物体探测方面表现出色。<details>
<summary>Abstract</summary>
Non-line-of-sight (NLOS) imaging allows for the imaging of objects around a corner, which enables potential applications in various fields such as autonomous driving, robotic vision, medical imaging, security monitoring, etc. However, the quality of reconstruction is challenged by low signal-noise-ratio (SNR) measurements. In this study, we present a regularization method, referred to as structure sparsity (SS) regularization, for denoising in NLOS reconstruction. By exploiting the prior knowledge of structure sparseness, we incorporate nuclear norm penalization into the cost function of directional light-cone transform (DLCT) model for NLOS imaging system. This incorporation effectively integrates the neighborhood information associated with the directional albedo, thereby facilitating the denoising process. Subsequently, the reconstruction is achieved by optimizing a directional albedo model with SS regularization using fast iterative shrinkage-thresholding algorithm. Notably, the robust reconstruction of occluded objects is observed. Through comprehensive evaluations conducted on both synthetic and experimental datasets, we demonstrate that the proposed approach yields high-quality reconstructions, surpassing the state-of-the-art reconstruction algorithms, especially in scenarios involving short exposure and low SNR measurements.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dual-Degradation-Inspired-Deep-Unfolding-Network-for-Low-Light-Image-Enhancement"><a href="#Dual-Degradation-Inspired-Deep-Unfolding-Network-for-Low-Light-Image-Enhancement" class="headerlink" title="Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement"></a>Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02776">http://arxiv.org/abs/2308.02776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huake Wang, Xingsong Hou, Xiaoyang Yan<br>for:这篇论文的目的是提出一种基于深度恢复模型的低光照图像改进方法，以解决现有的低光照图像恢复方法往往强调黑盒网络的增强性能，而忽略了图像恢复模型的物理意义。methods:该论文提出了一种基于双层降低模型的深度 unfolding 网络（DASUNet），其中包括了构建双层降低模型（DDM），以便显式地模拟低光照图像的劣化机制。DDM 学习了两个不同的图像假设，通过考虑颜色和灰度空间中的降低特点来进行适应。为使提议方案可行，我们设计了一种交叉优化解决方案，并将其拓展到一个具体的深度网络中，以形成 DASUNet。results:对多个流行的低光照图像dataset进行了广泛的实验，并证明了 DASUNet 比 canon 状态的低光照图像恢复方法更有效。我们将源代码和预训练模型公开发布。<details>
<summary>Abstract</summary>
Although low-light image enhancement has achieved great stride based on deep enhancement models, most of them mainly stress on enhancement performance via an elaborated black-box network and rarely explore the physical significance of enhancement models. Towards this issue, we propose a Dual degrAdation-inSpired deep Unfolding network, termed DASUNet, for low-light image enhancement. Specifically, we construct a dual degradation model (DDM) to explicitly simulate the deterioration mechanism of low-light images. It learns two distinct image priors via considering degradation specificity between luminance and chrominance spaces. To make the proposed scheme tractable, we design an alternating optimization solution to solve the proposed DDM. Further, the designed solution is unfolded into a specified deep network, imitating the iteration updating rules, to form DASUNet. Local and long-range information are obtained by prior modeling module (PMM), inheriting the advantages of convolution and Transformer, to enhance the representation capability of dual degradation priors. Additionally, a space aggregation module (SAM) is presented to boost the interaction of two degradation models. Extensive experiments on multiple popular low-light image datasets validate the effectiveness of DASUNet compared to canonical state-of-the-art low-light image enhancement methods. Our source code and pretrained model will be publicly available.
</details>
<details>
<summary>摘要</summary>
尽管深度修剪模型在低光照图像改善方面已经取得了很大的进步，但大多数模型都强调了修剪性能的提高，而很少探讨修剪模型的物理意义。为了解决这个问题，我们提出了一种名为DASUNet的深度 unfolding 网络，用于低光照图像改善。具体来说，我们构建了一个双层降低模型（DDM），以显式地模拟低光照图像的衰化机制。DDM 学习了两个不同的图像假设，通过考虑颜色和灰度空间之间的特定降低特征来进行适应。为了使我们的方案可行，我们设计了一种交叉优化解决方案，并将其 unfolding 成一个具体的深度网络，以形成DASUNet。PMM 模块（假设模型）在维护本地和长距离信息的同时，继承了 convolution 和 Transformer 的优点，以提高修剪两个假设的表示能力。此外，我们还提出了一种空间聚合模块（SAM），以提高两个降低模型之间的交互。我们在多个流行的低光照图像数据集上进行了广泛的实验，并证明了 DASUNet 的效果比 canonical 状态的低光照图像修剪方法更高。我们将源代码和预训练模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="Incorporation-of-Eye-Tracking-and-Gaze-Feedback-to-Characterize-and-Improve-Radiologist-Search-Patterns-of-Chest-X-rays-A-Randomized-Controlled-Clinical-Trial"><a href="#Incorporation-of-Eye-Tracking-and-Gaze-Feedback-to-Characterize-and-Improve-Radiologist-Search-Patterns-of-Chest-X-rays-A-Randomized-Controlled-Clinical-Trial" class="headerlink" title="Incorporation of Eye-Tracking and Gaze Feedback to Characterize and Improve Radiologist Search Patterns of Chest X-rays: A Randomized Controlled Clinical Trial"></a>Incorporation of Eye-Tracking and Gaze Feedback to Characterize and Improve Radiologist Search Patterns of Chest X-rays: A Randomized Controlled Clinical Trial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06280">http://arxiv.org/abs/2308.06280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carolina Ramirez-Tamayo, Syed Hasib Akhter Faruqui, Stanford Martinez, Angel Brisco, Nicholas Czarnek, Adel Alaeddini, Jeffrey R. Mock, Edward J. Golob, Kal L. Clark</li>
<li>for:  This study aimed to improve the accuracy of radiologists in detecting suspicious pulmonary nodules.</li>
<li>methods: The study used eye-tracking technology to analyze radiologists’ search patterns and provided automated feedback to the intervention group.</li>
<li>results: The intervention group showed a 38.89% absolute improvement in detecting suspicious-for-cancer nodules compared to the control group, with improvement observed in all four training sessions.Here’s the text in Simplified Chinese:</li>
<li>for: 这项研究旨在提高胸部肿瘤检测中的医生准确率。</li>
<li>methods: 该研究使用眼动跟踪技术分析医生的搜寻模式，并对参与实验组提供自动反馈。</li>
<li>results: 参与实验组对可疑肿瘤的检测精度有38.89%的绝对提升，比控制组的改善（5.56%）显著（p值&#x3D;0.006），并在四个训练会议中持续改善（p值&#x3D;0.0001）。<details>
<summary>Abstract</summary>
Diagnostic errors in radiology often occur due to incomplete visual assessments by radiologists, despite their knowledge of predicting disease classes. This insufficiency is possibly linked to the absence of required training in search patterns. Additionally, radiologists lack consistent feedback on their visual search patterns, relying on ad-hoc strategies and peer input to minimize errors and enhance efficiency, leading to suboptimal patterns and potential false negatives. This study aimed to use eye-tracking technology to analyze radiologist search patterns, quantify performance using established metrics, and assess the impact of an automated feedback-driven educational framework on detection accuracy. Ten residents participated in a controlled trial focused on detecting suspicious pulmonary nodules. They were divided into an intervention group (received automated feedback) and a control group. Results showed that the intervention group exhibited a 38.89% absolute improvement in detecting suspicious-for-cancer nodules, surpassing the control group's improvement (5.56%, p-value=0.006). Improvement was more rapid over the four training sessions (p-value=0.0001). However, other metrics such as speed, search pattern heterogeneity, distractions, and coverage did not show significant changes. In conclusion, implementing an automated feedback-driven educational framework improved radiologist accuracy in detecting suspicious nodules. The study underscores the potential of such systems in enhancing diagnostic performance and reducing errors. Further research and broader implementation are needed to consolidate these promising results and develop effective training strategies for radiologists, ultimately benefiting patient outcomes.
</details>
<details>
<summary>摘要</summary>
radiologists  oftentimes make diagnostic errors due to inadequate visual assessments, despite their knowledge of predicting disease classes. This deficiency may be linked to the lack of required training in search patterns. Additionally, radiologists lack consistent feedback on their visual search patterns, relying on ad-hoc strategies and peer input to minimize errors and enhance efficiency, leading to suboptimal patterns and potential false negatives. This study aimed to use eye-tracking technology to analyze radiologist search patterns, quantify performance using established metrics, and assess the impact of an automated feedback-driven educational framework on detection accuracy. Ten residents participated in a controlled trial focused on detecting suspicious pulmonary nodules. They were divided into an intervention group (received automated feedback) and a control group. Results showed that the intervention group exhibited a 38.89% absolute improvement in detecting suspicious-for-cancer nodules, surpassing the control group's improvement (5.56%, p-value=0.006). Improvement was more rapid over the four training sessions (p-value=0.0001). However, other metrics such as speed, search pattern heterogeneity, distractions, and coverage did not show significant changes. In conclusion, implementing an automated feedback-driven educational framework improved radiologist accuracy in detecting suspicious nodules. The study underscores the potential of such systems in enhancing diagnostic performance and reducing errors. Further research and broader implementation are needed to consolidate these promising results and develop effective training strategies for radiologists, ultimately benefiting patient outcomes.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/05/eess.IV_2023_08_05/" data-id="clly4xtg500elvl88ekw7e9p3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/cs.LG_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/04/cs.LG_2023_08_04/">cs.LG - 2023-08-04 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-Cell-Tracking-with-a-Time-Symmetric-Deep-Learning-Approach"><a href="#Enhancing-Cell-Tracking-with-a-Time-Symmetric-Deep-Learning-Approach" class="headerlink" title="Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach"></a>Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03887">http://arxiv.org/abs/2308.03887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gergely Szabó, Paolo Bonaiuti, Andrea Ciliberto, András Horváth</li>
<li>for: 生物学实验中跟踪细胞的动态行为</li>
<li>methods: 基于深度学习的方法，不受连续帧的限制，仅基于细胞的空间时间邻域</li>
<li>results: 能够处理大量视频帧，并且可以学习细胞的运动模式无需任何先前假设<details>
<summary>Abstract</summary>
The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated through multiple biologically motivated validation strategies and compared against several state-of-the-art cell tracking methods.
</details>
<details>
<summary>摘要</summary>
live cells 的准确跟踪使用视频微scopy记录仍然是流行的state-of-the-art image processing基于对象跟踪方法中的挑战。在过去几年中，一些现有的和新的应用程序尝试 integrating deep learning基础框架来实现这项任务，但大多数它们仍然受限于连续帧基础或其他前提，这会阻碍总体学习。为解决这个问题，我们努力开发了一种新的深度学习基础的跟踪方法，这种方法假设Cells可以根据其空间-时间 neighborhood来跟踪，不需要 consecutive frame。这种方法还有一个利点，即 predictor 可以通过完全学习 cells 的运动模式而不需要任何先前假设，并且可以处理大量视频帧。我们通过多种生物学上驱动的验证方法来证明方法的效果，并与一些state-of-the-art cell tracking方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Learning-Optimal-Admission-Control-in-Partially-Observable-Queueing-Networks"><a href="#Learning-Optimal-Admission-Control-in-Partially-Observable-Queueing-Networks" class="headerlink" title="Learning Optimal Admission Control in Partially Observable Queueing Networks"></a>Learning Optimal Admission Control in Partially Observable Queueing Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02391">http://arxiv.org/abs/2308.02391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonatha Anselmi, Bruno Gaujal, Louis-Sébastien Rebuffi</li>
<li>for: 这个论文目的是学习一种有效的权限控制策略，以优化在部分可见的队列网络中的排队控制。</li>
<li>methods: 这个论文使用了一种基于POMDP的强化学习算法，并且使用了Norton的等价定理和生成-死亡过程的有效强化学习算法来实现。</li>
<li>results: 这个论文得到了一个只依赖于最大任务数 $S$ 的减少，而不是依赖于任务系统的径长，从而实现了高效的排队控制。<details>
<summary>Abstract</summary>
We present an efficient reinforcement learning algorithm that learns the optimal admission control policy in a partially observable queueing network. Specifically, only the arrival and departure times from the network are observable, and optimality refers to the average holding/rejection cost in infinite horizon.   While reinforcement learning in Partially Observable Markov Decision Processes (POMDP) is prohibitively expensive in general, we show that our algorithm has a regret that only depends sub-linearly on the maximal number of jobs in the network, $S$. In particular, in contrast with existing regret analyses, our regret bound does not depend on the diameter of the underlying Markov Decision Process (MDP), which in most queueing systems is at least exponential in $S$.   The novelty of our approach is to leverage Norton's equivalent theorem for closed product-form queueing networks and an efficient reinforcement learning algorithm for MDPs with the structure of birth-and-death processes.
</details>
<details>
<summary>摘要</summary>
我们提出了一个高效的增强学习算法，用于在部分可观察queueing网络中找到最佳接受控制策略。具体来说，只有网络的到达和离开时间可观察，并且将 Optimal 定义为无限时间平均保持/拒绝成本。而在部分可观察Markov决策过程（POMDP）中，增强学习通常是不可能高效的，但我们显示我们的算法仅对最大作业数量($S$)有较低的干扰。具体来说，我们的干扰 bound 不过依赖 Underlying Markov Decision Process（MDP）的尺度，这在大多数队列系统中是至少对数尺度的 $S$。我们的新的方法是利用Norton的等效定理，以及高效的增强学习算法 для birth-and-death 过程结构的 MDP。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Survival-Analysis-in-Healthcare-with-Federated-Survival-Forests-A-Comparative-Study-on-Heart-Failure-and-Breast-Cancer-Genomics"><a href="#Scaling-Survival-Analysis-in-Healthcare-with-Federated-Survival-Forests-A-Comparative-Study-on-Heart-Failure-and-Breast-Cancer-Genomics" class="headerlink" title="Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics"></a>Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02382">http://arxiv.org/abs/2308.02382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Archetti, Francesca Ieva, Matteo Matteucci</li>
<li>for: This paper is written for the purpose of developing a federated learning algorithm for survival analysis, called FedSurF++, which can handle incomplete, censored, and distributed survival data while preserving user privacy.</li>
<li>methods: The FedSurF++ algorithm uses a federated ensemble method that constructs random survival forests in heterogeneous federations, and investigates several new tree sampling methods from client forests.</li>
<li>results: The paper shows that FedSurF++ achieves comparable performance to existing methods while requiring only a single communication round to complete, and presents results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private.<details>
<summary>Abstract</summary>
Survival analysis is a fundamental tool in medicine, modeling the time until an event of interest occurs in a population. However, in real-world applications, survival data are often incomplete, censored, distributed, and confidential, especially in healthcare settings where privacy is critical. The scarcity of data can severely limit the scalability of survival models to distributed applications that rely on large data pools. Federated learning is a promising technique that enables machine learning models to be trained on multiple datasets without compromising user privacy, making it particularly well-suited for addressing the challenges of survival data and large-scale survival applications. Despite significant developments in federated learning for classification and regression, many directions remain unexplored in the context of survival analysis. In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method constructs random survival forests in heterogeneous federations. Specifically, we investigate several new tree sampling methods from client forests and compare the results with state-of-the-art survival models based on neural networks. The key advantage of FedSurF++ is its ability to achieve comparable performance to existing methods while requiring only a single communication round to complete. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private. We also present results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. Our results underscore the potential of FedSurF++ to improve the scalability and effectiveness of survival analysis in distributed settings while preserving user privacy.
</details>
<details>
<summary>摘要</summary>
生存分析是医学中的基本工具，用于模型 populate 中的事件发生的时间。然而，在实际应用中，生存数据通常是不完整、审核、分布和保密的，特别是在医疗设置中，隐私是非常重要。数据的稀缺性可能会使生存模型在分布式应用中的扩展性受到严重限制。联邦学习是一种有 Promise 的技术，它允许机器学习模型在多个数据集上进行训练，而不需要违反用户隐私。因此，它在生存数据和大规模生存应用中具有潜在的优势。 despite ， many  direction  in the context of survival analysis remains unexplored in federated learning.In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method constructs random survival forests in heterogeneous federations. Specifically, we investigate several new tree sampling methods from client forests and compare the results with state-of-the-art survival models based on neural networks. The key advantage of FedSurF++ is its ability to achieve comparable performance to existing methods while requiring only a single communication round to complete. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private. We also present results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. Our results underscore the potential of FedSurF++ to improve the scalability and effectiveness of survival analysis in distributed settings while preserving user privacy.
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Web-and-Knowledge-Graphs-for-Automated-Impact-Investing-Scoring"><a href="#Harnessing-the-Web-and-Knowledge-Graphs-for-Automated-Impact-Investing-Scoring" class="headerlink" title="Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring"></a>Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02622">http://arxiv.org/abs/2308.02622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingzhi Hu, Daniel Daza, Laurens Swinkels, Kristina Ūsaitė, Robbert-Jan ‘t Hoen, Paul Groth</li>
<li>for: 该研究旨在自动化SDG框架创建过程，提高了SDG批判和分析的效率和准确性。</li>
<li>methods: 该研究提出了一种数据驱动的方法，包括收集和筛选不同网络源和知识图文本数据，然后使用这些数据进行分类，预测公司与SDG的对应度。</li>
<li>results: 实验结果显示，该模型可以准确预测SDG分数，微平均F1分数为0.89，证明该方案的有效性。此外，该研究还提出了一种可以让人类使用的模型解释方法，以便更好地理解和使用模型预测结果。<details>
<summary>Abstract</summary>
The Sustainable Development Goals (SDGs) were introduced by the United Nations in order to encourage policies and activities that help guarantee human prosperity and sustainability. SDG frameworks produced in the finance industry are designed to provide scores that indicate how well a company aligns with each of the 17 SDGs. This scoring enables a consistent assessment of investments that have the potential of building an inclusive and sustainable economy. As a result of the high quality and reliability required by such frameworks, the process of creating and maintaining them is time-consuming and requires extensive domain expertise. In this work, we describe a data-driven system that seeks to automate the process of creating an SDG framework. First, we propose a novel method for collecting and filtering a dataset of texts from different web sources and a knowledge graph relevant to a set of companies. We then implement and deploy classifiers trained with this data for predicting scores of alignment with SDGs for a given company. Our results indicate that our best performing model can accurately predict SDG scores with a micro average F1 score of 0.89, demonstrating the effectiveness of the proposed solution. We further describe how the integration of the models for its use by humans can be facilitated by providing explanations in the form of data relevant to a predicted score. We find that our proposed solution enables access to a large amount of information that analysts would normally not be able to process, resulting in an accurate prediction of SDG scores at a fraction of the cost.
</details>
<details>
<summary>摘要</summary>
《可持续发展目标（SDG）》由联合国引入，以促进政策和活动，确保人类发展和可持续。 SDG 框架在金融业中生成，用于提供对每个 SDG 的分数，以衡量公司是否与它们相align。这些分数允许对投资进行一致的评估，以建立包容和可持续的经济。由于需要高质量和可靠性，创建和维护 SDG 框架的过程是时间consuming 和需要广泛领域专业知识。在这种情况下，我们描述了一个数据驱动的系统，用于自动化 SDG 框架的创建过程。我们首先提出了一种新的方法，收集和筛选来自不同网络源和知识图库相关的公司文本数据集。然后，我们实施和部署基于这些数据的分类器，以预测公司与 SDG 的Alignment 分数。我们的结果表明，我们的最佳表现模型可以准确预测 SDG 分数，μicro 平均 F1 分数为 0.89，证明了我们的解决方案的有效性。我们还描述了如何将模型与人类使用者集成，通过提供预测分数的数据可视化来提供解释。我们发现，我们的提议的解决方案可以访问大量信息， analysts  normally 不能处理，并在较低的成本下实现准确的 SDG 分数预测。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Method-for-Predicting-Traffic-Signal-Timing-from-Probe-Vehicle-Data"><a href="#A-Machine-Learning-Method-for-Predicting-Traffic-Signal-Timing-from-Probe-Vehicle-Data" class="headerlink" title="A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data"></a>A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02370">http://arxiv.org/abs/2308.02370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juliette Ugirumurera, Joseph Severino, Erik A. Bensen, Qichao Wang, Jane Macfarlane</li>
<li>for: 本研究使用机器学习技术来估算交通信号时间信息从车辆探测数据中。</li>
<li>methods: 我们使用极限梯度提升（XGBoost）模型来估算信号周期长度，并使用神经网络模型来确定每个阶段的红灯时间。</li>
<li>results: 我们的结果显示估算周期长度的错误在0.56秒之间，而红灯时间预测的平均错误为7.2秒。<details>
<summary>Abstract</summary>
Traffic signals play an important role in transportation by enabling traffic flow management, and ensuring safety at intersections. In addition, knowing the traffic signal phase and timing data can allow optimal vehicle routing for time and energy efficiency, eco-driving, and the accurate simulation of signalized road networks. In this paper, we present a machine learning (ML) method for estimating traffic signal timing information from vehicle probe data. To the authors best knowledge, very few works have presented ML techniques for determining traffic signal timing parameters from vehicle probe data. In this work, we develop an Extreme Gradient Boosting (XGBoost) model to estimate signal cycle lengths and a neural network model to determine the corresponding red times per phase from probe data. The green times are then be derived from the cycle length and red times. Our results show an error of less than 0.56 sec for cycle length, and red times predictions within 7.2 sec error on average.
</details>
<details>
<summary>摘要</summary>
交通信号机制环境中，交通信号控制对交通流控制和安全性具有重要作用。此外，了解交通信号阶段和时间数据可以帮助车辆进行优化的路径规划，以提高时间和能源效率，eco- driving，以及准确地模拟信号化道路网络。本文提出了一种机器学习（ML）方法，用于从车辆探测数据中估算交通信号时间信息。作者知道的研究 Works 中，很少有使用机器学习技术来确定交通信号时间参数从车辆探测数据。本文开发了极大幂boosting（XGBoost）模型来估算信号阶段长度，并使用神经网络模型来确定每个阶段的红灯时间。绿灯时间则可以从阶段长度和红灯时间中 derivation。我们的结果显示，ecycle length 的预测错误在0.56秒左右，而红灯时间的预测错误在7.2秒左右。
</details></li>
</ul>
<hr>
<h2 id="Color-Image-Recovery-Using-Generalized-Matrix-Completion-over-Higher-Order-Finite-Dimensional-Algebra"><a href="#Color-Image-Recovery-Using-Generalized-Matrix-Completion-over-Higher-Order-Finite-Dimensional-Algebra" class="headerlink" title="Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra"></a>Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02621">http://arxiv.org/abs/2308.02621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Liao, Zhuang Guo, Qi Gao, Yan Wang, Fajun Yu, Qifeng Zhao, Stephen Johh Maybank</li>
<li>for: 填充颜色图像中缺失数据的高精度图像完成</li>
<li>methods: 基于扩展的高阶矩阵模型，包括像素 neighborgood 扩展策略来描述地方像素约束</li>
<li>results: 对于各种算法进行了广泛的实验，并与公共可用的图像进行了比较，结果显示，我们的扩展矩阵完成模型和相应的算法与其低阶矩阵和传统矩阵对手相比，性能很高。<details>
<summary>Abstract</summary>
To improve the accuracy of color image completion with missing entries, we present a recovery method based on generalized higher-order scalars. We extend the traditional second-order matrix model to a more comprehensive higher-order matrix equivalent, called the "t-matrix" model, which incorporates a pixel neighborhood expansion strategy to characterize the local pixel constraints. This "t-matrix" model is then used to extend some commonly used matrix and tensor completion algorithms to their higher-order versions. We perform extensive experiments on various algorithms using simulated data and algorithms on simulated data and publicly available images and compare their performance. The results show that our generalized matrix completion model and the corresponding algorithm compare favorably with their lower-order tensor and conventional matrix counterparts.
</details>
<details>
<summary>摘要</summary>
为提高颜色图像完成缺失项的准确性，我们提出一种基于泛化高阶约束的恢复方法。我们将传统的第二阶矩阵模型扩展到更加全面的高阶矩阵等价物，称之为“t-矩阵”模型，该模型通过描述当地像素约束的像素邻域扩展策略。这个“t-矩阵”模型后来用于扩展一些通常使用的矩阵和张量完成算法到其高阶版本。我们在各种算法上进行了广泛的实验，使用模拟数据和公共可用的图像，并比较了其性能。结果显示，我们的泛化约束模型和相应的算法与其低阶张量和传统矩阵counterparts相比，表现良好。
</details></li>
</ul>
<hr>
<h2 id="Intensity-free-Integral-based-Learning-of-Marked-Temporal-Point-Processes"><a href="#Intensity-free-Integral-based-Learning-of-Marked-Temporal-Point-Processes" class="headerlink" title="Intensity-free Integral-based Learning of Marked Temporal Point Processes"></a>Intensity-free Integral-based Learning of Marked Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02360">http://arxiv.org/abs/2308.02360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stepinsilence/ifib">https://github.com/stepinsilence/ifib</a></li>
<li>paper_authors: Sishun Liu, Ke Deng, Xiuzhen Zhang, Yongli Ren</li>
<li>for: 这个论文的目的是为了开发一个高精度的数值点事件模型，以应对发生在多维数值空间中的数值事件。</li>
<li>methods: 这个论文使用了一个名为IFIB的解决方案，它是一种不使用强度函数的方法，它直接模型了条件共同PDF $p^{*}(m,t)$，并且可以实现高精度的数值点事件模型。</li>
<li>results: 这个论文的实验结果显示，IFIB可以实现高精度的数值点事件模型，并且在实际应用中具有较好的性能。另外，这个论文还提供了一个可用的代码库，供其他研究者使用。<details>
<summary>Abstract</summary>
In the marked temporal point processes (MTPP), a core problem is to parameterize the conditional joint PDF (probability distribution function) $p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history. The majority of existing studies predefine intensity functions. Their utility is challenged by specifying the intensity function's proper form, which is critical to balance expressiveness and processing efficiency. Recently, there are studies moving away from predefining the intensity function -- one models $p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal point processes (TPPs), which do not consider marks. This study aims to develop high-fidelity $p^*(m,t)$ for discrete events where the event marks are either categorical or numeric in a multi-dimensional continuous space. We propose a solution framework IFIB (\underline{I}ntensity-\underline{f}ree \underline{I}ntegral-\underline{b}ased process) that models conditional joint PDF $p^*(m,t)$ directly without intensity functions. It remarkably simplifies the process to compel the essential mathematical restrictions. We show the desired properties of IFIB and the superior experimental results of IFIB on real-world and synthetic datasets. The code is available at \url{https://github.com/StepinSilence/IFIB}.
</details>
<details>
<summary>摘要</summary>
在标记时间点过程中（MTPP），核心问题是参数化 conditional joint PDF（概率分布函数）$p^*(m,t)$， conditioned on the history，其中 $m$ 表示事件标记， $t$ 表示事件间隔时间。大多数现有研究都是先定义INTENSITY函数。然而，这些INTENSITY函数的合适形式是关键，需要平衡表达能力和处理效率。在最近几年，有一些研究尝试离开先定义INTENSITY函数，其中一种是将 $p^*(t)$ 和 $p^*(m)$ 分别模型，另一种是关注时间点过程（TPP），不考虑标记。本研究旨在开发高精度的 $p^*(m,t)$  для精确的事件时间点，其中事件标记可以是 categorical 或 numeric，并且在多维连续空间中。我们提出了一种解决方案框架 IFIB（INTENSITY-free INTEGRAL-based process），它直接模型 conditional joint PDF $p^*(m,t)$ 无需INTENSITY函数。这有效简化了过程，使得mathematical restrictions强制性地减少。我们显示了 IFIB 的愿望性质和实验结果，并提供了实验结果。代码可以在 <https://github.com/StepinSilence/IFIB> 上获取。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-for-GTFS-From-Words-to-Information"><a href="#ChatGPT-for-GTFS-From-Words-to-Information" class="headerlink" title="ChatGPT for GTFS: From Words to Information"></a>ChatGPT for GTFS: From Words to Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02618">http://arxiv.org/abs/2308.02618</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/utel-uiuc/gtfs_llm">https://github.com/utel-uiuc/gtfs_llm</a></li>
<li>paper_authors: Saipraneeth Devunuri, Shirin Qiam, Lewis Lehe</li>
<li>For: The paper aims to explore the ability of large language models (LLMs) to retrieve information from the General Transit Feed Specification (GTFS) using natural language instructions.* Methods: The paper uses the ChatGPT model (GPT-3.5) to test its understanding of the GTFS specification and to perform information extraction from a filtered GTFS feed with 4 routes. The paper compares zero-shot and program synthesis methods for information retrieval.* Results: The paper finds that program synthesis achieves higher accuracy (~90% for simple questions and ~40% for complex questions) than zero-shot methods for information retrieval from GTFS using natural language instructions.<details>
<summary>Abstract</summary>
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
</details>
<details>
<summary>摘要</summary>
通用交通Feed规范（GTFS）是公共交通数据的发布标准，这种标准是表格数据，信息分布在不同文件中，因此需要专门的工具或包装来获取信息。同时，使用大型自然语言模型（LLM）来检索文本和信息的使用也在增长。本研究的想法是看看目前广泛采用的LLM（ChatGPT）能否使用自然语言指令来从GTFS中提取信息。我们首先测试了GPT-3.5是否理解GTFS规范。GPT-3.5回答了我们的多项选择题（MCQ）77% correctly。接下来，我们让LLM从过滤后的GTFS feed中提取信息。为了获取信息，我们比较了零shot和程序合成。程序合成更好，实现了~90%的简单问题的准确率和~40%的复杂问题的准确率。
</details></li>
</ul>
<hr>
<h2 id="Multi-attacks-Many-images-the-same-adversarial-attack-to-many-target-labels"><a href="#Multi-attacks-Many-images-the-same-adversarial-attack-to-many-target-labels" class="headerlink" title="Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels"></a>Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03792">http://arxiv.org/abs/2308.03792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanislavfort/multi-attacks">https://github.com/stanislavfort/multi-attacks</a></li>
<li>paper_authors: Stanislav Fort</li>
<li>for: 这个论文旨在描述一种可以同时对多个图像进行攻击的攻击方法。</li>
<li>methods: 这个论文使用了一种称为”多重攻击”的方法，可以对多个图像进行攻击，并且可以在不同的目标类上进行攻击。</li>
<li>results: 论文表明，使用这种多重攻击方法可以对数百个图像进行攻击，并且可以在不同的图像和目标类上进行攻击。此外，论文还发现了一些相关的结果，如图像的高信任区域数量是$\mathcal{O}(100)$以上，这会带来一些问题 для防御策略。<details>
<summary>Abstract</summary>
We show that we can easily design a single adversarial perturbation $P$ that changes the class of $n$ images $X_1,X_2,\dots,X_n$ from their original, unperturbed classes $c_1, c_2,\dots,c_n$ to desired (not necessarily all the same) classes $c^*_1,c^*_2,\dots,c^*_n$ for up to hundreds of images and target classes at once. We call these \textit{multi-attacks}. Characterizing the maximum $n$ we can achieve under different conditions such as image resolution, we estimate the number of regions of high class confidence around a particular image in the space of pixels to be around $10^{\mathcal{O}(100)}$, posing a significant problem for exhaustive defense strategies. We show several immediate consequences of this: adversarial attacks that change the resulting class based on their intensity, and scale-independent adversarial examples. To demonstrate the redundancy and richness of class decision boundaries in the pixel space, we look for its two-dimensional sections that trace images and spell words using particular classes. We also show that ensembling reduces susceptibility to multi-attacks, and that classifiers trained on random labels are more susceptible. Our code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
我们显示出可以轻松设计一个单一敌对偏移$P$，使$n$个图像$X_1,X_2,\dots,X_n$的原始、未偏变的类别变更为Target类别$c_1,c_2,\dots,c_n$的欲要（可能不是所有的类别都是一样的）类别$c^*_1,c^*_2,\dots,c^*_n$，称之为“多元攻击”。我们估计在不同的图像分辨率下，可以达到大量的$n$，并且考虑到像素空间中高度信任类别的区域数量约为$10^{\mathcal{O}(100)}$，这会对对抗策略造成严重的问题。我们显示了一些立即的后果：对于图像的数量和Target类别的变化，以及对于图像的缩放和转换的类别攻击。为了证明像素空间中类别决策boundary的丰富和紧张，我们寻找了图像和字串之间的二维部分，并证明了折衣组合可以对抗多元攻击，而且随机 labels 训练的分类器更加易受到攻击。我们的代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Adapting-to-Change-Robust-Counterfactual-Explanations-in-Dynamic-Data-Landscapes"><a href="#Adapting-to-Change-Robust-Counterfactual-Explanations-in-Dynamic-Data-Landscapes" class="headerlink" title="Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes"></a>Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02353">http://arxiv.org/abs/2308.02353</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bardhprenkaj/hansel">https://github.com/bardhprenkaj/hansel</a></li>
<li>paper_authors: Bardh Prenkaj, Mario Villaizan-Vallelado, Tobias Leemann, Gjergji Kasneci</li>
<li>for: This paper presents a novel semi-supervised method for counterfactual explanation generation, called Dynamic GRAph Counterfactual Explainer (DyGRACE).</li>
<li>methods: DyGRACE uses two graph autoencoders (GAEs) to learn the representation of each class in a binary classification scenario, and optimizes a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximizing the factual autoencoder’s reconstruction error.</li>
<li>results: DyGRACE is effective in identifying counterfactuals and can act as a drift detector, identifying distributional drift based on differences in reconstruction errors between iterations. It avoids reliance on the oracle’s predictions in successive iterations, thereby increasing the efficiency of counterfactual discovery.<details>
<summary>Abstract</summary>
We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE) methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leverages initial knowledge about the data distribution to search for valid counterfactuals while avoiding using information from potentially outdated decision functions in subsequent time steps. Employing two graph autoencoders (GAEs), DyGRACE learns the representation of each class in a binary classification scenario. The GAEs minimise the reconstruction error between the original graph and its learned representation during training. The method involves (i) optimising a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximising the factual autoencoder's reconstruction error, (ii) minimising the counterfactual autoencoder's error, and (iii) maximising the similarity between the factual and counterfactual graphs. This semi-supervised approach is independent of an underlying black-box oracle. A logistic regression model is trained on a set of graph pairs to learn weights that aid in finding counterfactuals. At inference, for each unseen graph, the logistic regressor identifies the best counterfactual candidate using these learned weights, while the GAEs can be iteratively updated to represent the continual adaptation of the learned graph representation over iterations. DyGRACE is quite effective and can act as a drift detector, identifying distributional drift based on differences in reconstruction errors between iterations. It avoids reliance on the oracle's predictions in successive iterations, thereby increasing the efficiency of counterfactual discovery. DyGRACE, with its capacity for contrastive learning and drift detection, will offer new avenues for semi-supervised learning and explanation generation.
</details>
<details>
<summary>摘要</summary>
我们介绍一种新的半监督式グラフカウンターファクタルエクスプレイナー（GCE）方法，即动态GRAPHカウンターファクタルエクスプレイナー（DyGRACE）。它利用初始知识来搜寻有效的假设，而不需要在后续时间步骤中使用可能已过时的决策函数。使用两个图自动生成器（GAE），DyGRACE学习图像中的每个类别表现。在训练过程中，GAE将图像和其学习的表现之间的差异降到最小。方法包括：(i) 优化一个 Parametric Density Function（实现为逻辑回归函数），以确定假设，最大化实际自动生成器的重建错误。(ii) 降低假设自动生成器的错误。(iii) 将实际和假设图像之间的相似度最大化。这个半监督式方法不需要背景黑盒模型，可以独立进行假设搜寻。在推断过程中，一个逻辑回归模型将被训练，以学习对图像对的权重，并且在每次推断过程中选择最佳的假设候选者。在迭代过程中，GAEs可以逐步更新，以反映适应学习的图像表现。DyGRACE能够实现对照学习和解释生成，并且可以检测分布迁移，根据不同的重建错误值进行推断。这些特点使得DyGRACE能够提高假设搜寻的效率，并且具有跨时间的内存和适应能力。
</details></li>
</ul>
<hr>
<h2 id="RobustMQ-Benchmarking-Robustness-of-Quantized-Models"><a href="#RobustMQ-Benchmarking-Robustness-of-Quantized-Models" class="headerlink" title="RobustMQ: Benchmarking Robustness of Quantized Models"></a>RobustMQ: Benchmarking Robustness of Quantized Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02350">http://arxiv.org/abs/2308.02350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang Guo, Xianglong Liu</li>
<li>for: 评估量化神经网络模型的可靠性和抗噪性。</li>
<li>methods: 使用了多种噪音（攻击性噪音、自然噪音和系统噪音）对量化神经网络模型进行了全面的评估。</li>
<li>results: 研究发现，量化模型对攻击性噪音具有更高的抗噪性，但对自然噪音和系统噪音更容易受损。增加量化比特宽度会导致对攻击性噪音的抗噪性下降，对自然噪音和系统噪音的抗噪性增加。等类型的噪音对量化模型的影响不同。<details>
<summary>Abstract</summary>
Quantization has emerged as an essential technique for deploying deep neural networks (DNNs) on devices with limited resources. However, quantized models exhibit vulnerabilities when exposed to various noises in real-world applications. Despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. The comprehensive evaluation results empirically provide valuable insights into the robustness of quantized models in various scenarios, for example: (1) quantized models exhibit higher adversarial robustness than their floating-point counterparts, but are more vulnerable to natural corruptions and systematic noises; (2) in general, increasing the quantization bit-width results in a decrease in adversarial robustness, an increase in natural robustness, and an increase in systematic robustness; (3) among corruption methods, \textit{impulse noise} and \textit{glass blur} are the most harmful to quantized models, while \textit{brightness} has the least impact; (4) among systematic noises, the \textit{nearest neighbor interpolation} has the highest impact, while bilinear interpolation, cubic interpolation, and area interpolation are the three least harmful. Our research contributes to advancing the robust quantization of models and their deployment in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
量化技术已成为深度神经网络（DNNs）部署在有限资源设备的重要手段。然而，量化模型在实际应用中受到各种噪声的影响，这些噪声包括攻击性噪声、自然损害和系统性噪声。despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. The comprehensive evaluation results empirically provide valuable insights into the robustness of quantized models in various scenarios, for example:1. 量化模型对攻击性噪声比浮点模型更高，但对自然损害和系统性噪声更容易受到影响。2. 在不同的量化比特宽度下，对攻击性噪声的影响随着量化比特宽度的增加而逐渐减少，对自然损害和系统性噪声的影响则随着量化比特宽度的增加而逐渐增加。3. 对量化模型的噪声方法，抖擦噪声和玻璃噪声是最有害的两种，而亮度噪声对量化模型的影响最小。4. 对系统性噪声，最近的邻居 interpolate 是最有害的一种，而 bilinear interpolate、cubic interpolate 和 area interpolate 是最弱的三种。我们的研究为深度神经网络的可靠量化和实际应用做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Vehicles-Control-Collision-Avoidance-using-Federated-Deep-Reinforcement-Learning"><a href="#Vehicles-Control-Collision-Avoidance-using-Federated-Deep-Reinforcement-Learning" class="headerlink" title="Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning"></a>Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02614">http://arxiv.org/abs/2308.02614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Badr Ben Elallid, Amine Abouaomar, Nabil Benamar, Abdellatif Kobbane</li>
<li>for: 运输管理和安全性问题在都市化的人口增加和车辆量增加的情况下变得非常重要。这篇论文探讨了在碰撞避免方面使用智能控制系统的发展，并且利用联合深度循环学习（FDRL）技术。</li>
<li>methods: 这篇论文使用了两种模型：地方模型（DDPG）和联合模型（FDDPG），并进行了比较分析，以决定它们在碰撞避免方面的效果。</li>
<li>results: 结果显示，使用FDDPG算法可以更好地控制车辆，避免碰撞。尤其是，FDDPG-based algorithm在减少旅行延迟和提高平均速度方面表现出了明显的改善。<details>
<summary>Abstract</summary>
In the face of growing urban populations and the escalating number of vehicles on the roads, managing transportation efficiently and ensuring safety have become critical challenges. To tackle these issues, the development of intelligent control systems for vehicles is paramount. This paper presents a comprehensive study on vehicle control for collision avoidance, leveraging the power of Federated Deep Reinforcement Learning (FDRL) techniques. Our main goal is to minimize travel delays and enhance the average speed of vehicles while prioritizing safety and preserving data privacy. To accomplish this, we conducted a comparative analysis between the local model, Deep Deterministic Policy Gradient (DDPG), and the global model, Federated Deep Deterministic Policy Gradient (FDDPG), to determine their effectiveness in optimizing vehicle control for collision avoidance. The results obtained indicate that the FDDPG algorithm outperforms DDPG in terms of effectively controlling vehicles and preventing collisions. Significantly, the FDDPG-based algorithm demonstrates substantial reductions in travel delays and notable improvements in average speed compared to the DDPG algorithm.
</details>
<details>
<summary>摘要</summary>
面对城市人口增长和交通量不断增加的问题，有效地管理交通和确保安全已成为核心挑战。为此，开发智能控制系统 для车辆是极其重要的。本文通过详细的研究，探讨了采用联邦深度强化学习（FDRL）技术来控制车辆，以最小化旅行延迟和提高车辆的平均速度，同时保持数据隐私。为此，我们进行了本地模型（DDPG）和全球模型（FDDPG）的比较分析，以确定它们在避免碰撞方面的效果。结果表明，FDDPG算法在控制车辆和避免碰撞方面表现较好，并且在旅行延迟和车辆平均速度方面具有显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Neural-Networks-with-more-flexible-memory-better-predictions-than-rough-volatility"><a href="#Recurrent-Neural-Networks-with-more-flexible-memory-better-predictions-than-rough-volatility" class="headerlink" title="Recurrent Neural Networks with more flexible memory: better predictions than rough volatility"></a>Recurrent Neural Networks with more flexible memory: better predictions than rough volatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08550">http://arxiv.org/abs/2308.08550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Damien Challet, Vincent Ragel</li>
<li>for: 这篇论文旨在扩展循环神经网络，以应对具有长 памя于或高度不均匀的时间步骤。</li>
<li>methods: 这篇论文使用了多个灵活的时间尺度，以提高循环神经网络的能力，并与标准LSTM进行比较。</li>
<li>results: 相比标准LSTM，扩展LSTM需要训练更少的epoch，并且预测资产波动性的模型体系性高于20%。<details>
<summary>Abstract</summary>
We extend recurrent neural networks to include several flexible timescales for each dimension of their output, which mechanically improves their abilities to account for processes with long memory or with highly disparate time scales. We compare the ability of vanilla and extended long short term memory networks (LSTMs) to predict asset price volatility, known to have a long memory. Generally, the number of epochs needed to train extended LSTMs is divided by two, while the variation of validation and test losses among models with the same hyperparameters is much smaller. We also show that the model with the smallest validation loss systemically outperforms rough volatility predictions by about 20% when trained and tested on a dataset with multiple time series.
</details>
<details>
<summary>摘要</summary>
我们扩展回传神经网络，以包括每个输出维度的多个灵活时间尺度，以机械提高其能力处理具有长期记忆或高度不同时间尺度的过程。我们比较了净体和扩展的长期快短时间记忆网络（LSTM）的能力预测资产波动性，知道具有长期记忆。通常，训练扩展LSTM需要的轮数比vanilla LSTM要少半，并且模型之间的验证和测试损失的变化相对较小。我们还显示，具有最小验证损失的模型系统地超过了使用多个时间序列的预测波动性预测值的20%。
</details></li>
</ul>
<hr>
<h2 id="Stability-and-Generalization-of-Hypergraph-Collaborative-Networks"><a href="#Stability-and-Generalization-of-Hypergraph-Collaborative-Networks" class="headerlink" title="Stability and Generalization of Hypergraph Collaborative Networks"></a>Stability and Generalization of Hypergraph Collaborative Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02347">http://arxiv.org/abs/2308.02347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Ng, Hanrui Wu, Andy Yip</li>
<li>For: 本研究旨在确保hypergraph collaborative networks的核心层的算法稳定性，并提供一般化保证。* Methods: 本文使用hypergraph collaborative networks，并通过对它们的核心层进行分析，提供了一般化保证。* Results: 实验结果表明，通过合适地调整数据和hypergraph filters的缩放，可以实现uniform的学习过程稳定性。<details>
<summary>Abstract</summary>
Graph neural networks have been shown to be very effective in utilizing pairwise relationships across samples. Recently, there have been several successful proposals to generalize graph neural networks to hypergraph neural networks to exploit more complex relationships. In particular, the hypergraph collaborative networks yield superior results compared to other hypergraph neural networks for various semi-supervised learning tasks. The collaborative network can provide high quality vertex embeddings and hyperedge embeddings together by formulating them as a joint optimization problem and by using their consistency in reconstructing the given hypergraph. In this paper, we aim to establish the algorithmic stability of the core layer of the collaborative network and provide generalization guarantees. The analysis sheds light on the design of hypergraph filters in collaborative networks, for instance, how the data and hypergraph filters should be scaled to achieve uniform stability of the learning process. Some experimental results on real-world datasets are presented to illustrate the theory.
</details>
<details>
<summary>摘要</summary>
graph neural networks 有 shown 能够很 effectively 利用 sample 对的 pairwise 关系。 最近， 有 several successful proposals 将 graph neural networks 扩展到 hypergraph neural networks，以利用更复杂的关系。特别是， hypergraph collaborative networks 在 various semi-supervised learning tasks 中 yield superior results compared to other hypergraph neural networks。 collaborative network 可以提供 high quality vertex embeddings 和 hyperedge embeddings，通过 formulating them as a joint optimization problem 和使用 their consistency in reconstructing the given hypergraph。在这篇 paper 中，我们 aim to establish the algorithmic stability of the core layer of the collaborative network and provide generalization guarantees。analysis  shed light on the design of hypergraph filters in collaborative networks, such as how the data and hypergraph filters should be scaled to achieve uniform stability of the learning process。some experimental results on real-world datasets are presented to illustrate the theory。
</details></li>
</ul>
<hr>
<h2 id="Learning-Networks-from-Gaussian-Graphical-Models-and-Gaussian-Free-Fields"><a href="#Learning-Networks-from-Gaussian-Graphical-Models-and-Gaussian-Free-Fields" class="headerlink" title="Learning Networks from Gaussian Graphical Models and Gaussian Free Fields"></a>Learning Networks from Gaussian Graphical Models and Gaussian Free Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02344">http://arxiv.org/abs/2308.02344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhro Ghosh, Soumendu Sundar Mukherjee, Hoang-Son Tran, Ujan Gangopadhyay</li>
<li>for: 估计weighted网络的结构</li>
<li>methods: 基于重复测量Gaussian Graphical Model（GGM）的方法</li>
<li>results: 提出一种新的估计器，可以从GGM的复杂概率特性中提取有用信息，并且可以提供具体的回归保证和样本复杂度下界。特别是，在 Erdos-Renyi 随机网络上，我们证明了在样本大小 $n$ 足够大时，网络结构可以recovery With high probability.<details>
<summary>Abstract</summary>
We investigate the problem of estimating the structure of a weighted network from repeated measurements of a Gaussian Graphical Model (GGM) on the network. In this vein, we consider GGMs whose covariance structures align with the geometry of the weighted network on which they are based. Such GGMs have been of longstanding interest in statistical physics, and are referred to as the Gaussian Free Field (GFF). In recent years, they have attracted considerable interest in the machine learning and theoretical computer science. In this work, we propose a novel estimator for the weighted network (equivalently, its Laplacian) from repeated measurements of a GFF on the network, based on the Fourier analytic properties of the Gaussian distribution. In this pursuit, our approach exploits complex-valued statistics constructed from observed data, that are of interest on their own right. We demonstrate the effectiveness of our estimator with concrete recovery guarantees and bounds on the required sample complexity. In particular, we show that the proposed statistic achieves the parametric rate of estimation for fixed network size. In the setting of networks growing with sample size, our results show that for Erdos-Renyi random graphs $G(d,p)$ above the connectivity threshold, we demonstrate that network recovery takes place with high probability as soon as the sample size $n$ satisfies $n \gg d^4 \log d \cdot p^{-2}$.
</details>
<details>
<summary>摘要</summary>
我们研究如何从重复观测 Gaussian Graphical Model (GGM) 中 estimate 网络的结构。在这个意境下，我们考虑 GGM 的协调结构与网络的重量相对应。这些 GGM 在统计物理学中有很长的历史，通常被称为 Gaussian Free Field (GFF)。在最近几年中，它们在机器学习和理论计算机科学中受到了很大的关注。在这个工作中，我们提出了一个新的网络重量Estimator，基于网络上重复观测 GFF 的 Fourier分析特性。我们的方法利用观测数据中的复数统计，具有自己的科学价值。我们显示了这个统计的效果，并提供了具体的回溯保证和sample complexity bound。尤其是，我们证明了这个统计在固定网络大小下具有参数率的估计率。在探索网络规模 grow 于样本大小的情况下，我们的结果显示，当样本大小 $n$ 满足 $n \gg d^4 \log d \cdot p^{-2}$ 时，网络重建很有可能会在高可信度下发生。Note: "Simplified Chinese" is a romanization of Chinese that uses a simplified set of characters and grammar rules to represent the language. It is commonly used in mainland China and Singapore, and is one of the two official languages of the People's Republic of China.
</details></li>
</ul>
<hr>
<h2 id="RAHNet-Retrieval-Augmented-Hybrid-Network-for-Long-tailed-Graph-Classification"><a href="#RAHNet-Retrieval-Augmented-Hybrid-Network-for-Long-tailed-Graph-Classification" class="headerlink" title="RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification"></a>RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02335">http://arxiv.org/abs/2308.02335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyang Mao, Wei Ju, Yifang Qin, Xiao Luo, Ming Zhang</li>
<li>for: 提高图像分类 tasks 中的泛化能力，尤其是在长尾类分布的real-world数据中。</li>
<li>methods: 提出了一种名为 Retrieval Augmented Hybrid Network (RAHNet) 的新框架，用于同时学习一个 Robust 特征提取器和一个不偏袋化的分类器，并在特征提取器和分类器之间进行分离学习。在特征提取器训练阶段，我们开发了一个图像检索模块，用于搜索与tail类相关的图像，以直接增强tail类之间的内部多样性。在分类器细化阶段，我们使用了两种重量规regularization技术，即Max-norm和weight decay，以均衡分类器的重量。</li>
<li>results: 在各种流行的benchmark上进行了实验，并证明了我们的方法在state-of-the-artapproaches中的优越性。<details>
<summary>Abstract</summary>
Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant graphs that directly enrich the intra-class diversity for the tail classes. Moreover, we innovatively optimize a category-centered supervised contrastive loss to obtain discriminative representations, which is more suitable for long-tailed scenarios. In the classifier fine-tuning stage, we balance the classifier weights with two weight regularization techniques, i.e., Max-norm and weight decay. Experiments on various popular benchmarks verify the superiority of the proposed method against state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
“图像分类是现实世界多媒体应用中的关键任务，图像可以表示各种媒体数据类型，如图像、视频和社交网络。先前的尝试都是在平衡的情况下应用图像神经网络（GNNs），但实际数据通常会出现长尾分布，导致使用GNNs时对尾类的偏袋和有限的泛化能力。现有的方法主要集中在模型训练时重新平衡不同类别，但这会失去新知识的导入和头类的性能。为解决这些缺点，我们提出了一种新的框架，即Retrieval Augmented Hybrid Network（RAHNet），它可以同时学习一个强健的特征提取器和一个不偏袋的分类器。在特征提取器训练阶段，我们开发了一个图像检索模块，以找到适当的图像来增强尾类的内部多样性。此外，我们还创新地优化了一种类型中心的超级vised对比损失，以获得适合长尾情况的表示，”Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Interoperable-synthetic-health-data-with-SyntHIR-to-enable-the-development-of-CDSS-tools"><a href="#Interoperable-synthetic-health-data-with-SyntHIR-to-enable-the-development-of-CDSS-tools" class="headerlink" title="Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools"></a>Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02613">http://arxiv.org/abs/2308.02613</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/potter-coder89/synthir">https://github.com/potter-coder89/synthir</a></li>
<li>paper_authors: Pavitra Chauhan, Mohsen Gamal Saad Askar, Bjørn Fjukstad, Lars Ailo Bongo, Edvard Pedersen<br>for:这个论文旨在提出一种基于机器学习的临床决策支持系统（CDSS）的开发方法，使用高质量的患者日志和医疗注册来生成 synthetic EHR 数据，并在临床工作流程中实现 CDSS 工具的开发和测试。methods:这个论文使用的方法包括使用 FHIR 标准实现数据互操作性，使用 Gretel 框架生成 synthetic 数据，使用 Microsoft Azure FHIR 服务器作为基于 FHIR 的 EHR 系统，以及使用 SMART on FHIR 框架实现工具可重用性。results:论文通过开发一个基于机器学习的 CDSS 工具，使用 Norwegian Patient Register (NPR) 和 Norwegian Patient Prescriptions (NorPD) 数据进行开发，并在 SyntHIR 系统上测试和评估该工具。结果表明，SyntHIR 提供了一个通用的 CDSS 工具开发框架，可以使用 synthetic FHIR 数据进行测试和评估，并且可以在临床 setting 中实现。但是，synthetic 数据质量的问题还需要进一步改进。代码可以在 GitHub 上获取：<a target="_blank" rel="noopener" href="https://github.com/potter-coder89/SyntHIR.git%E3%80%82">https://github.com/potter-coder89/SyntHIR.git。</a><details>
<summary>Abstract</summary>
There is a great opportunity to use high-quality patient journals and health registers to develop machine learning-based Clinical Decision Support Systems (CDSS). To implement a CDSS tool in a clinical workflow, there is a need to integrate, validate and test this tool on the Electronic Health Record (EHR) systems used to store and manage patient data. However, it is often not possible to get the necessary access to an EHR system due to legal compliance. We propose an architecture for generating and using synthetic EHR data for CDSS tool development. The architecture is implemented in a system called SyntHIR. The SyntHIR system uses the Fast Healthcare Interoperability Resources (FHIR) standards for data interoperability, the Gretel framework for generating synthetic data, the Microsoft Azure FHIR server as the FHIR-based EHR system and SMART on FHIR framework for tool transportability. We demonstrate the usefulness of SyntHIR by developing a machine learning-based CDSS tool using data from the Norwegian Patient Register (NPR) and Norwegian Patient Prescriptions (NorPD). We demonstrate the development of the tool on the SyntHIR system and then lift it to the Open DIPS environment. In conclusion, SyntHIR provides a generic architecture for CDSS tool development using synthetic FHIR data and a testing environment before implementing it in a clinical setting. However, there is scope for improvement in terms of the quality of the synthetic data generated. The code is open source and available at https://github.com/potter-coder89/SyntHIR.git.
</details>
<details>
<summary>摘要</summary>
“有一大机会使用高质量的患者日记和医疗注册来开发基于机器学习的临床决策支持系统（CDSS）。为实现CDSS工具在临床工作流程中的应用，需要将这个工具与电子医疗记录（EHR）系统集成、验证和测试。然而，由于法律合规的问题，通常无法获得EHR系统的必要访问权。我们提出了一种使用生成的Synthetic EHR数据来开发CDSS工具的建筑方案。该建筑方案在一个名为SyntHIR的系统中实现，该系统使用Fast Healthcare Interoperability Resources（FHIR）标准来实现数据互操作，使用Gretel框架生成synthetic数据，使用Microsoft Azure FHIR服务器作为FHIR基于EHR系统，并使用SMART on FHIR框架来提供工具可重用性。我们通过使用挪威患者注册（NPR）和挪威药品订单（NorPD）的数据开发了一个基于机器学习的CDSS工具，并在SyntHIR系统上测试了该工具。最后，我们将工具提取到Open DIPS环境中。总之，SyntHIR提供了一个通用的CDSS工具开发基于Synthetic FHIR数据的测试环境，但是可以进一步提高生成的synthetic数据质量。代码可以在https://github.com/potter-coder89/SyntHIR.git中获取。”
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-spike-detection-in-deep-brain-stimulation-surgery"><a href="#Deep-learning-for-spike-detection-in-deep-brain-stimulation-surgery" class="headerlink" title="Deep learning for spike detection in deep brain stimulation surgery"></a>Deep learning for spike detection in deep brain stimulation surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05755">http://arxiv.org/abs/2308.05755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arkadiusz Nowacki, Ewelina Kołpa, Mateusz Szychiewicz, Konrad Ciecierski</li>
<li>for: 这个论文是为了描述一种基于深度学习的神经活动记录分析方法，用于深 bran stimulation（DBS） neurosurgery 中的 neuronal activity 识别。</li>
<li>methods: 该方法使用了一种卷积神经网络（CNN）来分析神经活动记录，并在不同的时间窗口中进行识别。</li>
<li>results: 实验结果表明，该方法可以达到最高的准确率（98.98%）和受器操作特征曲线的面积（AUC）的最高值（0.9898），而无需进行数据预处理。<details>
<summary>Abstract</summary>
Deep brain stimulation (DBS) is a neurosurgical procedure successfully used to treat conditions such as Parkinson's disease. Electrostimulation, carried out by implanting electrodes into an identified focus in the brain, makes it possible to reduce the symptoms of the disease significantly. In this paper, a method for analyzing recordings of neuronal activity acquired during DBS neurosurgery using deep learning is presented. We tested using a convolutional neural network (CNN) for this purpose. Based on the time window, the classifier assesses whether neuronal activity (spike) is present. The maximum accuracy value for the classifier was 98.98%, and the area under the receiver operating characteristic curve (AUC) was 0.9898. The method made it possible to obtain a classification without using data preprocessing.
</details>
<details>
<summary>摘要</summary>
深度脑刺激（DBS）是一种 neurosurgical 程序，已经成功地治疗了 Parkinson's disease 等疾病。通过在脑中implanting 电极，可以减轻疾病的 симптом。在这篇论文中，我们提出了使用深度学习分析 DBS  neurosurgery 中记录的 neuronal 活动的方法。我们测试了 convolutional neural network（CNN）来完成这个任务。根据时间窗口，分类器评估 neuronal 活动（脉冲）是否存在。最大准确率值为 98.98%，准确率下接收操作特征曲线（AUC）值为 0.9898。这种方法可以不使用数据预处理来获得分类。
</details></li>
</ul>
<hr>
<h2 id="A-stochastic-optimization-approach-to-train-non-linear-neural-networks-with-a-higher-order-variation-regularization"><a href="#A-stochastic-optimization-approach-to-train-non-linear-neural-networks-with-a-higher-order-variation-regularization" class="headerlink" title="A stochastic optimization approach to train non-linear neural networks with a higher-order variation regularization"></a>A stochastic optimization approach to train non-linear neural networks with a higher-order variation regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02293">http://arxiv.org/abs/2308.02293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oknakfm/hovr">https://github.com/oknakfm/hovr</a></li>
<li>paper_authors: Akifumi Okuno</li>
<li>For: This paper aims to address the issue of overfitting in highly expressive parametric models, such as deep neural networks, by introducing a new regularization term called $(k,q)$th order variation regularization ($(k,q)$-VR).* Methods: The paper proposes a stochastic optimization algorithm that can efficiently train general models with the $(k,q)$-VR term without conducting explicit numerical integration. The algorithm is based on stochastic gradient descent and automatic differentiation, and can be applied to the training of deep neural networks with arbitrary structure.* Results: The paper demonstrates that the neural networks trained with the $(k,q)$-VR terms are more “resilient” than those with the conventional parameter regularization, and the proposed algorithm can also be extended to the physics-informed training of neural networks (PINNs).<details>
<summary>Abstract</summary>
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $(k,q)$th order variation regularization ($(k,q)$-VR), which is defined as the $q$th-powered integral of the absolute $k$th order derivative of the parametric models to be trained; penalizing the $(k,q)$-VR is expected to yield a smoother function, which is expected to avoid overfitting. Particularly, $(k,q)$-VR encompasses the conventional (general-order) total variation with $q=1$. While the $(k,q)$-VR terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $(k,q)$-VR without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradient descent algorithm and automatic differentiation. Our numerical experiments demonstrate that the neural networks trained with the $(k,q)$-VR terms are more ``resilient'' than those with the conventional parameter regularization. The proposed algorithm also can be extended to the physics-informed training of neural networks (PINNs).
</details>
<details>
<summary>摘要</summary>
“而高度表达力的 parametric 模型，如深度神经网络，具有模型复杂概念的优势。然而，训练这些非线性模型时存在高风险的过拟合。为解决这个问题，本研究考虑了 $(k,q)$ 项变化规则（$(k,q)$-VR），即将要训练的 parametric 模型的 $q$ 阶幂化积分 absolute $k$ 阶差分。penalizing $(k,q)$-VR 会导致更平滑的函数，以避免过拟合。特别是，$(k,q)$-VR 包括普通（总阶）变量的 $q=1$。而 $(k,q)$-VR 应用于普通 parametric 模型时 computationally intractable due to integration，本研究提供了一种可efficiently 训练通用模型的随机优化算法。这种方法可以应用于深度神经网络的训练，并且可以通过简单的随机梯度下降算法和自动导数来实现。我们的numerical experiments表明，使用 $(k,q)$-VR 训练的神经网络比使用传统参数正则化更为“坚固”。此外，这种算法还可以扩展到物理学信息训练神经网络（PINNs）。”
</details></li>
</ul>
<hr>
<h2 id="Frustratingly-Easy-Model-Generalization-by-Dummy-Risk-Minimization"><a href="#Frustratingly-Easy-Model-Generalization-by-Dummy-Risk-Minimization" class="headerlink" title="Frustratingly Easy Model Generalization by Dummy Risk Minimization"></a>Frustratingly Easy Model Generalization by Dummy Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02287">http://arxiv.org/abs/2308.02287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juncheng Wang, Jindong Wang, Xixu Hu, Shujun Wang, Xing Xie</li>
<li>for: 提高机器学习模型的泛化能力</li>
<li>methods: 使用拟合风险最小化（Dummy Risk Minimization，DuRM）技术，即通过扩大输出логи特征来提高模型的泛化能力</li>
<li>results: DuRM可以在多个任务上提高表现，包括传统的分类、Semantic segmentation、out-of-distribution泛化、对抗训练和长尾识别等，并且可以与现有的泛化技术相结合使用。<details>
<summary>Abstract</summary>
Empirical risk minimization (ERM) is a fundamental machine learning paradigm. However, its generalization ability is limited in various tasks. In this paper, we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general technique to improve the generalization of ERM. DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent. Moreover, we validate the efficacy of DuRM on both theoretical and empirical analysis. Theoretically, we show that DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima. Empirically, we conduct evaluations of DuRM across different datasets, modalities, and network architectures on diverse tasks, including conventional classification, semantic segmentation, out-of-distribution generalization, adverserial training, and long-tailed recognition. Results demonstrate that DuRM could consistently improve the performance under all tasks with an almost free lunch manner. Furthermore, we show that DuRM is compatible with existing generalization techniques and we discuss possible limitations. We hope that DuRM could trigger new interest in the fundamental research on risk minimization.
</details>
<details>
<summary>摘要</summary>
empirical risk minimization (ERM) 是机器学习的一种基本思想。然而，其泛化能力在各种任务上有限。在这篇论文中，我们提出了干扰risk minimization（DuRM），一种极其简单和普遍适用的技术，以提高ERM的泛化能力。DuRM的实现非常简单：只需扩大输出logits的维度，然后使用标准的梯度下降优化。我们在理论和实验两方面 validate DuRM的有效性。在理论上，我们表明DuRM可以提高模型的泛化能力，通过观察更好的平坦的本地极小值。在实验上，我们对不同的数据集、模式和网络架构进行了多种任务的评估，包括传统的分类、semantic segmentation、out-of-distribution泛化、对抗训练和长尾识别。结果表明，DuRM可以在所有任务上提高性能，几乎没有免费的午餐。此外，我们还证明了DuRM与现有的泛化技术相容，并讨论了可能的限制。我们希望DuRM可以触发新的研究于风险最小化的基础。
</details></li>
</ul>
<hr>
<h2 id="DIVERSIFY-A-General-Framework-for-Time-Series-Out-of-distribution-Detection-and-Generalization"><a href="#DIVERSIFY-A-General-Framework-for-Time-Series-Out-of-distribution-Detection-and-Generalization" class="headerlink" title="DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization"></a>DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02282">http://arxiv.org/abs/2308.02282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, Xiangyang Ji, Qiang Yang, Xing Xie<br>for: This paper aims to address the challenges of out-of-distribution (OOD) detection and generalization on time series data, which is non-stationary and has dynamic distributions.methods: The proposed method, DIVERSIFY, is an iterative framework that uses adversarial training to obtain the “worst-case” latent distribution scenario, and then reduces the gap between these latent distributions. DIVERSIFY combines existing OOD detection methods with outputs of models for detection and utilizes outputs for classification.results: Extensive experiments on seven datasets with different OOD settings show that DIVERSIFY learns more generalized features and significantly outperforms other baselines. Theoretical insights also support the effectiveness of DIVERSIFY.<details>
<summary>Abstract</summary>
Time series remains one of the most challenging modalities in machine learning research. The out-of-distribution (OOD) detection and generalization on time series tend to suffer due to its non-stationary property, i.e., the distribution changes over time. The dynamic distributions inside time series pose great challenges to existing algorithms to identify invariant distributions since they mainly focus on the scenario where the domain information is given as prior knowledge. In this paper, we attempt to exploit subdomains within a whole dataset to counteract issues induced by non-stationary for generalized representation learning. We propose DIVERSIFY, a general framework, for OOD detection and generalization on dynamic distributions of time series. DIVERSIFY takes an iterative process: it first obtains the "worst-case" latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We implement DIVERSIFY via combining existing OOD detection methods according to either extracted features or outputs of models for detection while we also directly utilize outputs for classification. In addition, theoretical insights illustrate that DIVERSIFY is theoretically supported. Extensive experiments are conducted on seven datasets with different OOD settings across gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition. Qualitative and quantitative results demonstrate that DIVERSIFY learns more generalized features and significantly outperforms other baselines.
</details>
<details>
<summary>摘要</summary>
时序序列仍然是机器学习研究中最为困难的模式之一。非站点性（OOD）检测和泛化在时序序列上通常受到非站点性的影响，即时序序列的分布随着时间的变化。时序序列中的动态分布对现有算法提供了很大挑战，因为它们主要假设有域信息作为先验知识。在这篇论文中，我们尝试利用时序序列中的子领域来缓解由非站点性引起的问题，以实现泛化学习。我们提出了DIVERSIFY，一种通用框架，用于OOD检测和泛化动态分布的时序序列。DIVERSIFY采用了迭代过程：首先通过对恶性学习获得“最差”的幂本分布场景，然后减少这些幂本分布之间的差距。我们通过结合现有OOD检测方法来实现DIVERSIFY，并直接利用模型输出进行分类。此外，理论分析表明DIVERSIFY是理论上支持的。我们对七个不同的数据集进行了广泛的实验，包括手势识别、语音命令识别、着装压力和情感识别以及基于传感器的人体活动识别。结果表明DIVERSIFY学习了更泛化的特征，并显著超过了其他基elines。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Proximal-Gradient-Method-for-Convex-Optimization"><a href="#Adaptive-Proximal-Gradient-Method-for-Convex-Optimization" class="headerlink" title="Adaptive Proximal Gradient Method for Convex Optimization"></a>Adaptive Proximal Gradient Method for Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02261">http://arxiv.org/abs/2308.02261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yura Malitsky, Konstantin Mishchenko</li>
<li>for: 本文研究了两种基本的首阶算法在凸优化中，即梯度下降（GD）和 proximal梯度方法（ProxGD）。我们的注意点是使这两种算法完全适应тив，利用凸函数的地方几何信息。</li>
<li>methods: 我们提出了基于观察到的梯度差的自适应GD和ProxGD版本，无需额外计算成本。此外，我们证明了我们的方法在只假设本地 lipschitz 的梯度下 converges。</li>
<li>results: 我们的方法可以使用更大的步长 than those initially suggested in [MM20]。<details>
<summary>Abstract</summary>
In this paper, we explore two fundamental first-order algorithms in convex optimization, namely, gradient descent (GD) and proximal gradient method (ProxGD). Our focus is on making these algorithms entirely adaptive by leveraging local curvature information of smooth functions. We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs. Moreover, we prove convergence of our methods assuming only local Lipschitzness of the gradient. In addition, the proposed versions allow for even larger stepsizes than those initially suggested in [MM20].
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了两种基本的首阶算法在凸优化中，即梯度下降（GD）和贝克斯 gradient 方法（ProxGD）。我们的关注点是使这些算法完全适应ive，利用当地凸函数的曲率信息。我们提出了基于观察到的梯度差的自适应GD和ProxGD版本，无需额外计算成本。此外，我们证明了我们的方法在本地lipchitz continuous的梯度下 converges。此外，我们的方法还允许更大的步长than those initially suggested in [MM20].
</details></li>
</ul>
<hr>
<h2 id="Finding-Tori-Self-supervised-Learning-for-Analyzing-Korean-Folk-Song"><a href="#Finding-Tori-Self-supervised-Learning-for-Analyzing-Korean-Folk-Song" class="headerlink" title="Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song"></a>Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02249">http://arxiv.org/abs/2308.02249</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danbinaerinhan/finding-tori">https://github.com/danbinaerinhan/finding-tori</a></li>
<li>paper_authors: Danbinaerin Han, Rafael Caro Repetto, Dasaem Jeong</li>
<li>for: 这个论文是对韩国民族歌曲录音数据集进行计算分析的，该数据集包含约700小时的民歌，录制于1980-90年代。</li>
<li>methods: 作者使用自动超vision学习和卷积神经网络，通过抽象报表来解决录音中的挑战。</li>
<li>results: 实验结果表明，作者的方法可以更好地捕捉韩国民歌中的护卷特征，比传统的抑制历史更加精准。通过这种方法，作者可以对现有学术中的音乐讨论在实际录音中进行实质性的探讨。<details>
<summary>Abstract</summary>
In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, the dataset provides several challenges. To address this challenge, we utilized self-supervised learning with convolutional neural network based on pitch contour, then analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. The experimental result shows that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种计算方法对韩国传统歌曲场记录数据集进行分析，该数据集约为700小时，录制于1980-90年代。由于大多数歌曲由非专业音乐家演唱，没有伴奏，因此该数据集具有许多挑战。为 Addressing this challenge, we utilized self-supervised learning with convolutional neural networks based on pitch contours, and analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. Our experimental results show that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.Here's the breakdown of the translation:* 韩国传统歌曲 (Korean traditional folk songs) -> 韩国传统歌曲 (Simplified Chinese)* 场记录数据集 (field recording dataset) -> 场记录数据集 (Simplified Chinese)* 约为700小时 (approximately 700 hours) -> 约为700小时 (Simplified Chinese)* 录制于1980-90年代 (recorded in the 1980s-1990s) -> 录制于1980-90年代 (Simplified Chinese)* 非专业音乐家 (non-expert musicians) -> 非专业音乐家 (Simplified Chinese)* 没有伴奏 (no accompaniment) -> 没有伴奏 (Simplified Chinese)* 计算方法 (computational method) -> 计算方法 (Simplified Chinese)* 自动学习 (self-supervised learning) -> 自动学习 (Simplified Chinese)* 基于折衣 (based on pitch contours) -> 基于折衣 (Simplified Chinese)* tori (a classification system) -> tori (Simplified Chinese)* 定义为特定的音阶、装饰音和idiomatic melodic contour -> 定义为特定的音阶、装饰音和idiomatic melodic contour (Simplified Chinese)* 使用我们的方法可以更好地捕捉折衣的特点 -> 使用我们的方法可以更好地捕捉折衣的特点 (Simplified Chinese)* 比传统折衣 histogram 更好 -> 比传统折衣 histogram 更好 (Simplified Chinese)* 使用我们的方法 -> 使用我们的方法 (Simplified Chinese)* 我们已经使用这些方法 -> 我们已经使用这些方法 (Simplified Chinese)* 对现有的音乐学讨论进行实际应用 -> 对现有的音乐学讨论进行实际应用 (Simplified Chinese)* 探讨了韩国传统歌曲中的音乐讨论 -> 探讨了韩国传统歌曲中的音乐讨论 (Simplified Chinese)
</details></li>
</ul>
<hr>
<h2 id="Deep-neural-networks-from-the-perspective-of-ergodic-theory"><a href="#Deep-neural-networks-from-the-perspective-of-ergodic-theory" class="headerlink" title="Deep neural networks from the perspective of ergodic theory"></a>Deep neural networks from the perspective of ergodic theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03888">http://arxiv.org/abs/2308.03888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang</li>
<li>for: 这个论文旨在解释深度神经网络的设计是如何变成一种更加科学的过程，而不是一种艺术。</li>
<li>methods: 这个论文使用了时间演化观的思想，将神经网络看作是一个动力系统的时间演化，每层对应于一个时间实例。</li>
<li>results: 这个论文表明，一些可能看起来神秘的规则，可以被解释为启发。<details>
<summary>Abstract</summary>
The design of deep neural networks remains somewhat of an art rather than precise science. By tentatively adopting ergodic theory considerations on top of viewing the network as the time evolution of a dynamical system, with each layer corresponding to a temporal instance, we show that some rules of thumb, which might otherwise appear mysterious, can be attributed heuristics.
</details>
<details>
<summary>摘要</summary>
神经网络设计仍然很有创造性，更像是一种艺术而非精确科学。通过尝试将ergodic theory应用于视网膜上，视网膜为时间演化的动力系统，每层对应一个时间实例，我们显示了一些可能看起来神秘的规则，实际上可以归结为优化策略。
</details></li>
</ul>
<hr>
<h2 id="Self-Normalizing-Neural-Network-Enabling-One-Shot-Transfer-Learning-for-Modeling-EDFA-Wavelength-Dependent-Gain"><a href="#Self-Normalizing-Neural-Network-Enabling-One-Shot-Transfer-Learning-for-Modeling-EDFA-Wavelength-Dependent-Gain" class="headerlink" title="Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain"></a>Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02233">http://arxiv.org/abs/2308.02233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agastya Raj, Zehao Wang, Frank Slyne, Tingjun Chen, Dan Kilper, Marco Ruffini</li>
<li>for: 该论文旨在提出一种基于 semi-supervised, self-normalizing neural networks 的多芯片 EDFA 波长依赖性的模型化框架，以实现一次转移学习。</li>
<li>methods: 该论文使用 semi-supervised, self-normalizing neural networks 来模型多芯片 EDFA 的波长依赖性，并实现了一次转移学习。</li>
<li>results: 实验结果表明，该模型在 Open Ireland 和 COSMOS 测试平台上的 22 个 EDFA 中具有高精度的转移学习能力，即使操作在不同的芯片类型上。<details>
<summary>Abstract</summary>
We present a novel ML framework for modeling the wavelength-dependent gain of multiple EDFAs, based on semi-supervised, self-normalizing neural networks, enabling one-shot transfer learning. Our experiments on 22 EDFAs in Open Ireland and COSMOS testbeds show high-accuracy transfer-learning even when operated across different amplifier types.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的机器学习框架，用于模型多个电子发射激光扩展器（EDFA）的波长依赖性收益，基于半监督自适应神经网络。我们的实验表明，这种框架可以在不同类型的扩展器上实现高精度的传输学习，并且可以在22个EDFA上进行一次转移学习。
</details></li>
</ul>
<hr>
<h2 id="Likelihood-ratio-based-confidence-intervals-for-neural-networks"><a href="#Likelihood-ratio-based-confidence-intervals-for-neural-networks" class="headerlink" title="Likelihood-ratio-based confidence intervals for neural networks"></a>Likelihood-ratio-based confidence intervals for neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02221">http://arxiv.org/abs/2308.02221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/laurenssluyterman/likelihood_ratio_intervals">https://github.com/laurenssluyterman/likelihood_ratio_intervals</a></li>
<li>paper_authors: Laurens Sluijterman, Eric Cator, Tom Heskes</li>
<li>for: 这个论文是为了建立一种基于likelihood ratio的方法来计算神经网络的信心 интерval。</li>
<li>methods: 这个方法使用了likelihood ratio的思想，可以建立不对称的信心 интерval，并且自动包含了训练时间、网络架构、训练技巧等因素。</li>
<li>results: 这个方法可以在对于医学预测或天文物理等领域，提供一个可靠的未知度估计，并且显示出这种方法在某些情况下可能已经有经济效益。<details>
<summary>Abstract</summary>
This paper introduces a first implementation of a novel likelihood-ratio-based approach for constructing confidence intervals for neural networks. Our method, called DeepLR, offers several qualitative advantages: most notably, the ability to construct asymmetric intervals that expand in regions with a limited amount of data, and the inherent incorporation of factors such as the amount of training time, network architecture, and regularization techniques. While acknowledging that the current implementation of the method is prohibitively expensive for many deep-learning applications, the high cost may already be justified in specific fields like medical predictions or astrophysics, where a reliable uncertainty estimate for a single prediction is essential. This work highlights the significant potential of a likelihood-ratio-based uncertainty estimate and establishes a promising avenue for future research.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文介绍了一种基于likelihood-ratio的神经网络置信范围的首次实现方法，称为DeepLR。我们的方法具有许多优点：能够构建不均匀的置信范围，在数据有限的地方扩展，同时自动包含训练时间、网络架构和正则化技术等因素。虽然当前实现可能对许多深度学习应用程序来说过于昂贵，但在医学预测或天文物理等领域，准确地估计单个预测结果的不确定性可能已经被 justify。这篇文章探讨了基于likelihood-ratio的置信范围的可能性，并开启了未来研究的新途径。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Driven-Multi-Agent-Reinforcement-Learning-for-Computation-Offloading-in-Cybertwin-Enabled-Internet-of-Vehicles"><a href="#Knowledge-Driven-Multi-Agent-Reinforcement-Learning-for-Computation-Offloading-in-Cybertwin-Enabled-Internet-of-Vehicles" class="headerlink" title="Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles"></a>Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02603">http://arxiv.org/abs/2308.02603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijin Sun, Xiao Yang, Nan Cheng, Xiucheng Wang, Changle Li</li>
<li>for: 提高 cybertwin-enabled IoV 中任务卸载延迟</li>
<li>methods: 使用知识驱动多代理人学习（KMARL）方法，利用域知识加入图 neural networks，实现选择最佳卸载选项</li>
<li>results: 比较其他方法，KMARL 表现更高的奖励和更好的扩展性，受到域知识的整合帮助<details>
<summary>Abstract</summary>
By offloading computation-intensive tasks of vehicles to roadside units (RSUs), mobile edge computing (MEC) in the Internet of Vehicles (IoV) can relieve the onboard computation burden. However, existing model-based task offloading methods suffer from heavy computational complexity with the increase of vehicles and data-driven methods lack interpretability. To address these challenges, in this paper, we propose a knowledge-driven multi-agent reinforcement learning (KMARL) approach to reduce the latency of task offloading in cybertwin-enabled IoV. Specifically, in the considered scenario, the cybertwin serves as a communication agent for each vehicle to exchange information and make offloading decisions in the virtual space. To reduce the latency of task offloading, a KMARL approach is proposed to select the optimal offloading option for each vehicle, where graph neural networks are employed by leveraging domain knowledge concerning graph-structure communication topology and permutation invariance into neural networks. Numerical results show that our proposed KMARL yields higher rewards and demonstrates improved scalability compared with other methods, benefitting from the integration of domain knowledge.
</details>
<details>
<summary>摘要</summary>
通过异步计算任务转移到路边单元（RSU），移动边缘计算（MEC）在互联网机器人（IoV）中可以减轻车辆上计算负担。然而，现有的模型基于任务转移方法受到增加车辆和数据驱动方法的计算复杂性的影响。为解决这些挑战，在这篇论文中，我们提出了知识驱动多智能体强化学习（KMARL）方法，以减少异步任务转移的延迟。具体来说，在考虑的场景中， cybertwin 作为每辆车辆的通信代理，在虚拟空间中交换信息并做出转移决策。通过使用图神经网络，我们利用了域知识，包括图structure 通信topology和 permutation 不变性，从而提高了强化学习的精度和扩展性。 numerically 的结果表明，我们提出的 KMARL 方法可以获得更高的奖励，并且在其他方法相比，具有更好的扩展性，受益于域知识的集成。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Spanish-Clinical-Language-Models"><a href="#A-Survey-of-Spanish-Clinical-Language-Models" class="headerlink" title="A Survey of Spanish Clinical Language Models"></a>A Survey of Spanish Clinical Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02199">http://arxiv.org/abs/2308.02199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillem García Subies, Álvaro Barbero Jiménez, Paloma Martínez Fernández</li>
<li>for: 这项研究专注于使用语言模型解决西班牙语医疗领域任务。</li>
<li>methods: 研究人员回顾了17个词库，主要集中在医疗任务上，然后列出了最有影响力的西班牙语语言模型和医疗语言模型。研究人员还对这些模型进行了严格的比较，用于找出最佳performing的模型，总共超过3000个模型进行了微调。</li>
<li>results: 研究人员对一些可访问的 corpora 进行了测试，并将结果公开发布，以便由独立团队重复或在未来对新的西班牙语医疗语言模型进行挑战。<details>
<summary>Abstract</summary>
This survey focuses in encoder Language Models for solving tasks in the clinical domain in the Spanish language. We review the contributions of 17 corpora focused mainly in clinical tasks, then list the most relevant Spanish Language Models and Spanish Clinical Language models. We perform a thorough comparison of these models by benchmarking them over a curated subset of the available corpora, in order to find the best-performing ones; in total more than 3000 models were fine-tuned for this study. All the tested corpora and the best models are made publically available in an accessible way, so that the results can be reproduced by independent teams or challenged in the future when new Spanish Clinical Language models are created.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字符" in Chinese.Please note that the translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="AutoML4ETC-Automated-Neural-Architecture-Search-for-Real-World-Encrypted-Traffic-Classification"><a href="#AutoML4ETC-Automated-Neural-Architecture-Search-for-Real-World-Encrypted-Traffic-Classification" class="headerlink" title="AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification"></a>AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02182">http://arxiv.org/abs/2308.02182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/orangeuw/automl4etc">https://github.com/orangeuw/automl4etc</a></li>
<li>paper_authors: Navid Malekghaini, Elham Akbari, Mohammad A. Salahuddin, Noura Limam, Raouf Boutaba, Bertrand Mathieu, Stephanie Moteau, Stephane Tuffin</li>
<li>for: 这个研究是为了提出一个自动设计高性能的神经网络模型，用于实时隐私化网络流量分类。</li>
<li>methods: 这个研究使用了自动机器学习（AutoML）技术，定义了一个特定设计的搜寻空间，并运用不同的搜寻策略来寻找最佳的神经网络模型。</li>
<li>results: 研究发现，使用AutoML4ETC可以自动设计高性能的神经网络模型，并且比现有的隐私化网络流量分类模型更加精确和轻量级。<details>
<summary>Abstract</summary>
Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark datasets and real-world TLS and QUIC traffic collected from the Orange mobile network. In addition to being more accurate, AutoML4ETC's architectures are significantly more efficient and lighter in terms of the number of parameters. Finally, we make AutoML4ETC publicly available for future research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Scaling-Clinical-Trial-Matching-Using-Large-Language-Models-A-Case-Study-in-Oncology"><a href="#Scaling-Clinical-Trial-Matching-Using-Large-Language-Models-A-Case-Study-in-Oncology" class="headerlink" title="Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology"></a>Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02180">http://arxiv.org/abs/2308.02180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cliff Wong, Sheng Zhang, Yu Gu, Christine Moung, Jacob Abel, Naoto Usuyama, Roshanthi Weerasinghe, Brian Piening, Tristan Naumann, Carlo Bifulco, Hoifung Poon</li>
<li>For: The paper is written for scaling clinical trial matching using large language models (LLMs) in the field of oncology.* Methods: The paper uses a systematic study approach with cutting-edge LLMs such as GPT-4 to structure eligibility criteria of clinical trials and extract complex matching logic.* Results: The initial findings show that LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop, but there are still areas for improvement such as context limitation and accuracy.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了扩大临床试验匹配使用大型自然语言模型（LLMs）的应用，主要是在肿瘤领域。* Methods: 该论文使用系统性的研究方法，使用最新的GPT-4等 LLMS来结构临床试验资格标准和提取复杂匹配逻辑。* Results: 初步发现结果表明，LLMs已经比前一代强大基elinesubstantially better，可能用作人工协作的初步解决方案，但还有一些需要进一步改进的方向，如上下文限制和准确性。<details>
<summary>Abstract</summary>
Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records.
</details>
<details>
<summary>摘要</summary>
临床试验匹配是医疗卫生系统中一个关键的过程，但在实践中却受到极多的不结构化数据和不可扩展的手动处理的困扰。在这篇论文中，我们进行了系统性的研究，使用大型自然语言模型（LLMs）来扩大临床试验匹配的规模。我们的研究基于一个目前在大型美国医疗网络中测试的临床试验匹配系统。初步的结果很有前途：直接使用最新的GPT-4等 cutting-edge LLMs，可以立即结构化临床试验报名标准和提取复杂的匹配逻辑（例如，嵌入 AND/OR/NOT 结构）。虽然还有一定的改进空间，但LLMs已经明显超过了先前的强基线，并可能作为人工干预的准备解决方案。我们的研究还揭示了应用LLMs到终端临床试验匹配中的一些重要成长点，如Context limitation和准确率，特别是从患者 longitudinal 医疗记录中提取patient信息。
</details></li>
</ul>
<hr>
<h2 id="High-Accuracy-Prediction-of-Metal-Insulator-Metal-Metasurface-with-Deep-Learning"><a href="#High-Accuracy-Prediction-of-Metal-Insulator-Metal-Metasurface-with-Deep-Learning" class="headerlink" title="High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning"></a>High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04450">http://arxiv.org/abs/2308.04450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaizhu Liu, Hsiang-Chen Chui, Changsen Sun, Xue Han</li>
<li>for: 本研究旨在提出一种基于深度学习的电磁软件计算结果预测方法，以提高计算效率和准确性。</li>
<li>methods: 本研究使用了ResNets-10模型进行预测плазмон喷流表 parameters的方法，并采用了k-fold cross-validation和小学习率的两个阶段训练。</li>
<li>results: 根据实验结果，对铝、金、银金属-隔体-铁的预测损失值分别为-48.45、-46.47和-35.54，表明提出的网络可以取代传统电磁计算方法，并且训练过程只需要少于1,100个迭代。<details>
<summary>Abstract</summary>
Deep learning prediction of electromagnetic software calculation results has been a widely discussed issue in recent years. But the prediction accuracy was still one of the challenges to be solved. In this work, we proposed that the ResNets-10 model was used for predicting plasmonic metasurface S11 parameters. The two-stage training was performed by the k-fold cross-validation and small learning rate. After the training was completed, the prediction loss for aluminum, gold, and silver metal-insulator-metal metasurfaces was -48.45, -46.47, and -35.54, respectively. Due to the ultralow error value, the proposed network can replace the traditional electromagnetic computing method for calculation within a certain structural range. Besides, this network can finish the training process less than 1,100 epochs. This means that the network training process can effectively lower the design process time. The ResNets-10 model we proposed can also be used to design meta-diffractive devices and biosensors, thereby reducing the time required for the calculation process. The ultralow error of the network indicates that this work contributes to the development of future artificial intelligence electromagnetic computing software.
</details>
<details>
<summary>摘要</summary>
Recently, deep learning prediction of electromagnetic software calculation results has been a widely discussed issue. However, prediction accuracy was still a challenge to be solved. In this work, we proposed using the ResNets-10 model to predict plasmonic metasurface S11 parameters. We performed two-stage training with k-fold cross-validation and small learning rate. After training, the prediction loss for aluminum, gold, and silver metal-insulator-metal metasurfaces was -48.45, -46.47, and -35.54, respectively. Due to the ultralow error value, the proposed network can replace traditional electromagnetic computing methods for calculation within a certain structural range. Additionally, this network can complete the training process in less than 1,100 epochs, effectively lowering the design process time. The ResNets-10 model we proposed can also be used to design meta-diffractive devices and biosensors, reducing the calculation process time. The ultralow error of the network indicates that this work contributes to the development of future artificial intelligence electromagnetic computing software.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-probabilistic-models-enhance-variational-autoencoder-for-crystal-structure-generative-modeling"><a href="#Diffusion-probabilistic-models-enhance-variational-autoencoder-for-crystal-structure-generative-modeling" class="headerlink" title="Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling"></a>Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02165">http://arxiv.org/abs/2308.02165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teerachote Pakornchote, Natthaphon Choomphon-anomakhun, Sorrjit Arrerut, Chayanon Atthapak, Sakarn Khamkaeo, Thiparat Chotibut, Thiti Bovornratanaraks</li>
<li>for: 生成真实的晶体结构，保持晶体对称性</li>
<li>methods: 使用新的扩散概率模型（DP模型）对原子坐标进行减噪，而不是采用标准的分数匹配方法</li>
<li>results: 能够生成和重建晶体结构，质量与原始CDVAE相似，而且与 relaxed 结构计算得到的碳结构更加接近ground state，能量差值平均为68.1 meV&#x2F;atom 下降，表明DP-CDVAE模型能够更好地代表晶体结构的ground state配置。<details>
<summary>Abstract</summary>
The crystal diffusion variational autoencoder (CDVAE) is a machine learning model that leverages score matching to generate realistic crystal structures that preserve crystal symmetry. In this study, we leverage novel diffusion probabilistic (DP) models to denoise atomic coordinates rather than adopting the standard score matching approach in CDVAE. Our proposed DP-CDVAE model can reconstruct and generate crystal structures whose qualities are statistically comparable to those of the original CDVAE. Furthermore, notably, when comparing the carbon structures generated by the DP-CDVAE model with relaxed structures obtained from density functional theory calculations, we find that the DP-CDVAE generated structures are remarkably closer to their respective ground states. The energy differences between these structures and the true ground states are, on average, 68.1 meV/atom lower than those generated by the original CDVAE. This significant improvement in the energy accuracy highlights the effectiveness of the DP-CDVAE model in generating crystal structures that better represent their ground-state configurations.
</details>
<details>
<summary>摘要</summary>
“单晶扩散条件自适应器”（CDVAE）是一种机器学习模型，利用得分匹配来生成具有实验室同调的晶体结构。在这个研究中，我们使用新的扩散概率模型（DP）来降噪原子坐标而不是采用CDVAE的标准得分匹配方法。我们称之为DP-CDVAE模型。这个模型可以重建和生成具有同等质量的晶体结构，并且在比较碳原子结构的情况下，DP-CDVAE模型生成的结构与 relaxation 计算得到的结构更加接近真实的基体状态。这些结构的能量差异与真实基体状态相比，平均降低了68.1 meV/atom。这显示DP-CDVAE模型具有更好的基体状态表现，并且能够更好地生成具有实验室同调的晶体结构。
</details></li>
</ul>
<hr>
<h2 id="Speaker-Diarization-of-Scripted-Audiovisual-Content"><a href="#Speaker-Diarization-of-Scripted-Audiovisual-Content" class="headerlink" title="Speaker Diarization of Scripted Audiovisual Content"></a>Speaker Diarization of Scripted Audiovisual Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02160">http://arxiv.org/abs/2308.02160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yogesh Virkar, Brian Thompson, Rohit Paturi, Sundararajan Srinivasan, Marcello Federico</li>
<li>for: 这篇论文主要是为了提高媒体本地化行业中的语音识别技术，具体来说是使用制作过程中使用的脚本来提高电视节目中的 speaker diarization 任务。</li>
<li>methods: 这篇论文提出了一种新的 semi-supervised 方法，通过使用制作过程中的脚本来提取 pseudo-labeled 数据，以提高 speaker diarization 任务的准确率。</li>
<li>results: 在测试集上，这种方法与两个无监督基线模型进行比较，实现了51.7% 的提升。<details>
<summary>Abstract</summary>
The media localization industry usually requires a verbatim script of the final film or TV production in order to create subtitles or dubbing scripts in a foreign language. In particular, the verbatim script (i.e. as-broadcast script) must be structured into a sequence of dialogue lines each including time codes, speaker name and transcript. Current speech recognition technology alleviates the transcription step. However, state-of-the-art speaker diarization models still fall short on TV shows for two main reasons: (i) their inability to track a large number of speakers, (ii) their low accuracy in detecting frequent speaker changes. To mitigate this problem, we present a novel approach to leverage production scripts used during the shooting process, to extract pseudo-labeled data for the speaker diarization task. We propose a novel semi-supervised approach and demonstrate improvements of 51.7% relative to two unsupervised baseline models on our metrics on a 66 show test set.
</details>
<details>
<summary>摘要</summary>
媒体地化业务通常需要最终电影或电视制作的字幕或配音脚本的 verbatim 脚本，以便在外语中创建字幕或配音脚本。特别是 verbatim 脚本（即播放版本）必须以时间码、说话人名和对话内容的结构组织。当前的语音识别技术使得转录步骤得以alleviates。然而，当前的话者分类模型仍然在电视节目中存在两个主要问题：（i）它们无法跟踪大量的说话人，（ii）它们在说话人变化频繁时的准确率低。为解决这个问题，我们提出了一种利用摄制过程中使用的制作脚本，提取 pseudo-labeled 数据来进行说话人分类任务。我们提出了一种新的半超vised方法，并在我们的测试集上实现了51.7%的相对提升，比两个无监督基线模型更高。
</details></li>
</ul>
<hr>
<h2 id="Improved-Order-Analysis-and-Design-of-Exponential-Integrator-for-Diffusion-Models-Sampling"><a href="#Improved-Order-Analysis-and-Design-of-Exponential-Integrator-for-Diffusion-Models-Sampling" class="headerlink" title="Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling"></a>Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02157">http://arxiv.org/abs/2308.02157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinsheng Zhang, Jiaming Song, Yongxin Chen</li>
<li>for: 提高 diffusion models (DMs) 的抽象速度，使其能够更快速地进行抽象。</li>
<li>methods: 利用高级别的减法积分器 (EI)，并通过重新设计高级别减法积分器来满足所有顺序条件，从而提高抽象质量和稳定性。</li>
<li>results: 通过 theoretically 和实际应用，提出了一种改进的减法积分器（RES），可以提高抽象质量和稳定性，并且在实际应用中可以减少数值缺陷和提高 FID 值。例如，在 ImageNet 扩散模型中，通过将单步 DPM-Solver++ 替换为 ORDER-satisfied RES solver，可以降低数值缺陷的比例为 25.2%，并提高 FID 值为 25.4%。<details>
<summary>Abstract</summary>
Efficient differential equation solvers have significantly reduced the sampling time of diffusion models (DMs) while retaining high sampling quality. Among these solvers, exponential integrators (EI) have gained prominence by demonstrating state-of-the-art performance. However, existing high-order EI-based sampling algorithms rely on degenerate EI solvers, resulting in inferior error bounds and reduced accuracy in contrast to the theoretically anticipated results under optimal settings. This situation makes the sampling quality extremely vulnerable to seemingly innocuous design choices such as timestep schedules. For example, an inefficient timestep scheduler might necessitate twice the number of steps to achieve a quality comparable to that obtained through carefully optimized timesteps. To address this issue, we reevaluate the design of high-order differential solvers for DMs. Through a thorough order analysis, we reveal that the degeneration of existing high-order EI solvers can be attributed to the absence of essential order conditions. By reformulating the differential equations in DMs and capitalizing on the theory of exponential integrators, we propose refined EI solvers that fulfill all the order conditions, which we designate as Refined Exponential Solver (RES). Utilizing these improved solvers, RES exhibits more favorable error bounds theoretically and achieves superior sampling efficiency and stability in practical applications. For instance, a simple switch from the single-step DPM-Solver++ to our order-satisfied RES solver when Number of Function Evaluations (NFE) $=9$, results in a reduction of numerical defects by $25.2\%$ and FID improvement of $25.4\%$ (16.77 vs 12.51) on a pre-trained ImageNet diffusion model.
</details>
<details>
<summary>摘要</summary>
高效的差分方程解析器在扩散模型（DM）中减少了采样时间，同时保持高质量的采样。其中，对数Integrators（EI）已经成为了状态之一，但现有的高阶EI基本样式依赖于弱化的EI解决方案，从而导致了较差的误差 bound和降低的准确性，与理论预期的结果不符。这种情况使得采样质量极易受到 seems innocuous的设计选择，如时间步骤调度。例如，使用不优化的时间步骤调度可能需要两倍的步骤数量以达到相同的质量。为解决这一问题，我们重新评估了高阶差分解析器的设计。通过系统的顺序分析，我们发现现有高阶EI解决方案的弱化可以归结于缺乏关键的顺序条件。我们根据扩散模型的差分方程和快速Integrators的理论，提出了改进的REFined Exponential Solver（RES）。我们的改进的解析器可以满足所有顺序条件，并且在实际应用中表现出较好的采样效率和稳定性。例如，将单步DPM-Solver++ switched to我们的顺序满足RES解析器，当Number of Function Evaluations（NFE）=9时，可以降低数值缺陷的比例为25.2%，并提高FID的改进率（16.77 vs 12.51）。
</details></li>
</ul>
<hr>
<h2 id="Optimization-on-Pareto-sets-On-a-theory-of-multi-objective-optimization"><a href="#Optimization-on-Pareto-sets-On-a-theory-of-multi-objective-optimization" class="headerlink" title="Optimization on Pareto sets: On a theory of multi-objective optimization"></a>Optimization on Pareto sets: On a theory of multi-objective optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02145">http://arxiv.org/abs/2308.02145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Roy, Geelon So, Yi-An Ma</li>
<li>for: 多目标优化中，一个单一决策 вектор需要寻找许多目标之间的最佳变数平衡。这些解答被称为Pareto优化解答，它们是对任何一个目标进行改善都需要在另一个目标上付出的决策 vector。但是，Pareto优化解答的集合可能很大，因此我们进一步考虑一个更实际 significanse的Pareto受限优化问题，其中的目标是将一个偏好函数对应到Pareto集。</li>
<li>methods: 我们调查了本地方法来解决这个受限优化问题，这个问题存在两个特点：（i）参数集是隐式定义的，（ii）通常是非凸非光滑的。我们定义了优化和稳定性的概念，并提供了一个Algorithm，其中的最后迭代速率为$O(K^{-1&#x2F;2})$，对于具有强式凹陷和Lipschitz光滑的目标而言。</li>
<li>results: 我们的研究表明，当目标是强式凹陷和Lipschitz光滑的时候，我们的方法具有最后迭代速率$O(K^{-1&#x2F;2})$，即在最后一迭代时，数据的变化速率为$O(K^{-1&#x2F;2})$。这表明我们的方法在解决Pareto受限优化问题时具有高效率和稳定性。<details>
<summary>Abstract</summary>
In multi-objective optimization, a single decision vector must balance the trade-offs between many objectives. Solutions achieving an optimal trade-off are said to be Pareto optimal: these are decision vectors for which improving any one objective must come at a cost to another. But as the set of Pareto optimal vectors can be very large, we further consider a more practically significant Pareto-constrained optimization problem, where the goal is to optimize a preference function constrained to the Pareto set.   We investigate local methods for solving this constrained optimization problem, which poses significant challenges because the constraint set is (i) implicitly defined, and (ii) generally non-convex and non-smooth, even when the objectives are. We define notions of optimality and stationarity, and provide an algorithm with a last-iterate convergence rate of $O(K^{-1/2})$ to stationarity when the objectives are strongly convex and Lipschitz smooth.
</details>
<details>
<summary>摘要</summary>
在多目标优化中，单个决策 вектор必须平衡多个目标之间的贸易offs。solutions达到优化的贸易offs是say Pareto优化的：这些决策 вектор在改进任何一个目标时，必须付出另一个目标的代价。但是Pareto优化集可能很大，因此我们进一步考虑一个更实际 significannot的 Pareto受限优化问题，其中的目标是通过对Pareto集进行优化。我们研究了本地方法来解决这个受限优化问题，这个问题具有以下两个特点：（i） constraint set是通过某种方式implcitly定义的，（ii）通常是非拥有凸形和光滑的。我们定义了优化和稳定性的概念，并提供了一个算法，其last-iterate convergence rate为 $O(K^{-1/2})$ 当目标函数是强转化和Lipschitz平滑的时候。
</details></li>
</ul>
<hr>
<h2 id="Event-based-Dynamic-Graph-Representation-Learning-for-Patent-Application-Trend-Prediction"><a href="#Event-based-Dynamic-Graph-Representation-Learning-for-Patent-Application-Trend-Prediction" class="headerlink" title="Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction"></a>Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09780">http://arxiv.org/abs/2308.09780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Zou, Le Yu, Leilei Sun, Bowen Du, Deqing Wang, Fuzhen Zhuang</li>
<li>for: 预测公司将在下一时期申请哪些专利，以估计其发展策略和找到前期伙伴或竞争对手。</li>
<li>methods: 我们提出了一种基于事件驱动图学习框架的专利申请趋势预测方法，利用公司和专利分类码的启动表示和历史记忆，以及 hierarchical message passing mechanism 来捕捉专利分类码的 semantic proximities。</li>
<li>results: 我们的方法在实际数据上进行了多种实验，并emonstrated 其效果 under various experimental conditions，并且探索了方法在学习分类码 semantics 和跟踪公司技术发展轨迹的能力。<details>
<summary>Abstract</summary>
Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by updating their representations along the hierarchical taxonomy. Finally, the patent application trend is predicted by aggregating the representations of the target company and classification codes from static, dynamic, and hierarchical perspectives. Experiments on real-world data demonstrate the effectiveness of our approach under various experimental conditions, and also reveal the abilities of our method in learning semantics of classification codes and tracking technology developing trajectories of companies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by updating their representations along the hierarchical taxonomy. Finally, the patent application trend is predicted by aggregating the representations of the target company and classification codes from static, dynamic, and hierarchical perspectives. Experiments on real-world data demonstrate the effectiveness of our approach under various experimental conditions, and also reveal the abilities of our method in learning semantics of classification codes and tracking technology developing trajectories of companies."中文翻译：准确预测公司将在下一时间段申请哪种专利，可以为其发展策略提供指导，并在前置的时间内发现可能的合作伙伴或竞争对手。虽然这个问题非常重要，但在前期研究中 rarely studied due to the challenges in modeling companies' continuously evolving preferences and capturing the semantic correlations of classification codes。为了填补这一空白，我们提出了一种基于事件的动态图学学习框架，用于预测专利申请趋势。具体来说，我们的方法基于公司和专利分类代码的记忆表示。当观察到新专利时，相关公司和专利分类代码的表示将根据历史记忆和当前编码的消息进行更新。此外，我们还提供了一种层次消息传递机制，以捕捉专利分类代码的semantic proximity。最后，我们通过 static、动态和层次视角的表示集成来预测专利申请趋势。实验结果表明，我们的方法在不同的实验条件下具有效果，并能够学习分类代码的 semantics和跟踪公司技术发展轨迹。
</details></li>
</ul>
<hr>
<h2 id="Learning-the-solution-operator-of-two-dimensional-incompressible-Navier-Stokes-equations-using-physics-aware-convolutional-neural-networks"><a href="#Learning-the-solution-operator-of-two-dimensional-incompressible-Navier-Stokes-equations-using-physics-aware-convolutional-neural-networks" class="headerlink" title="Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks"></a>Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02137">http://arxiv.org/abs/2308.02137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viktor Grimm, Alexander Heinlein, Axel Klawonn</li>
<li>for: 本研究旨在解决physics-inclusive机器学习技术中geometry的局限性问题，提出一种能够在不同的geometry中学习稳态 Navier-Stokes方程的解 approximate solutions的方法。</li>
<li>methods: 本研究使用了一种基于U-Net-like CNN和finite difference方法的combined方法，并与数据基于方法进行比较。</li>
<li>results: 研究结果表明，physics-aware CNN可以在不同的geometry中学习稳态 Navier-Stokes方程的解 approximate solutions，并且可以与数据基于方法相结合以提高性能。<details>
<summary>Abstract</summary>
In recent years, the concept of introducing physics to machine learning has become widely popular. Most physics-inclusive ML-techniques however are still limited to a single geometry or a set of parametrizable geometries. Thus, there remains the need to train a new model for a new geometry, even if it is only slightly modified. With this work we introduce a technique with which it is possible to learn approximate solutions to the steady-state Navier--Stokes equations in varying geometries without the need of parametrization. This technique is based on a combination of a U-Net-like CNN and well established discretization methods from the field of the finite difference method.The results of our physics-aware CNN are compared to a state-of-the-art data-based approach. Additionally, it is also shown how our approach performs when combined with the data-based approach.
</details>
<details>
<summary>摘要</summary>
近年来，将物理学引入机器学习的概念得到了广泛的推广。然而，大多数物理包含的机器学习技术仍然受限于单个几何或一组可 parametrize 的几何。因此，在新的几何上训练新的模型仍然是必要的。我们在这里介绍一种可以在不同几何中学习稳态奈特-斯托克方程的估计解的技术。这种技术基于一种组合了 U-Net 类 CNN 和已确立的精度方法的finite difference方法。我们对我们的物理意识 CNN 的结果进行了与当前最佳数据驱动方法的比较，同时还展示了我们的方法与数据驱动方法的组合效果。
</details></li>
</ul>
<hr>
<h2 id="Can-Attention-Be-Used-to-Explain-EHR-Based-Mortality-Prediction-Tasks-A-Case-Study-on-Hemorrhagic-Stroke"><a href="#Can-Attention-Be-Used-to-Explain-EHR-Based-Mortality-Prediction-Tasks-A-Case-Study-on-Hemorrhagic-Stroke" class="headerlink" title="Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke"></a>Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05110">http://arxiv.org/abs/2308.05110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhang Feng, Jiayi Yuan, Forhan Bin Emdad, Karim Hanna, Xia Hu, Zhe He</li>
<li>for: 预测中风死亡风险的早期预测</li>
<li>methods: 使用一种新的解释性听力基于变换器模型，以提高预测模型的准确性和可读性</li>
<li>results: 研究表明，这种解释性听力基于变换器模型可以提高预测模型的准确性和可读性，并且可以提供有用的特征重要性信息。<details>
<summary>Abstract</summary>
Stroke is a significant cause of mortality and morbidity, necessitating early predictive strategies to minimize risks. Traditional methods for evaluating patients, such as Acute Physiology and Chronic Health Evaluation (APACHE II, IV) and Simplified Acute Physiology Score III (SAPS III), have limited accuracy and interpretability. This paper proposes a novel approach: an interpretable, attention-based transformer model for early stroke mortality prediction. This model seeks to address the limitations of previous predictive models, providing both interpretability (providing clear, understandable explanations of the model) and fidelity (giving a truthful explanation of the model's dynamics from input to output). Furthermore, the study explores and compares fidelity and interpretability scores using Shapley values and attention-based scores to improve model explainability. The research objectives include designing an interpretable attention-based transformer model, evaluating its performance compared to existing models, and providing feature importance derived from the model.
</details>
<details>
<summary>摘要</summary>
stroke 是一个重要的死亡和残留症状的原因，需要早期预测方法来减少风险。传统的评估病人方法，如急性physiology和慢性健康评估（APACHE II、IV）和简化型急性 физиiology分数III（SAPS III），有限的准确性和可读性。这篇论文提出了一种新的方法：一种可解释的、注意力基本变换模型，用于早期stroke mortality预测。这个模型旨在解决之前的预测模型的局限性，提供了可解释性（提供明确、理解的解释）和诚实性（从输入到输出的模型动力学提供真实的解释）。此外，研究还研究了和比较了可解释性和诚实性分数使用Shapley值和注意力基本分数来提高模型解释性。研究的目标包括设计一种可解释的注意力基本变换模型，评估其性能与现有模型相比，并提供来自模型的特征重要性。
</details></li>
</ul>
<hr>
<h2 id="Analysis-and-Optimization-of-Wireless-Federated-Learning-with-Data-Heterogeneity"><a href="#Analysis-and-Optimization-of-Wireless-Federated-Learning-with-Data-Heterogeneity" class="headerlink" title="Analysis and Optimization of Wireless Federated Learning with Data Heterogeneity"></a>Analysis and Optimization of Wireless Federated Learning with Data Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03521">http://arxiv.org/abs/2308.03521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Han, Jun Li, Wen Chen, Zhen Mei, Kang Wei, Ming Ding, H. Vincent Poor</li>
<li>for: 本文旨在研究和优化无线 Federated Learning（FL）中的数据多样性和无线资源分配问题，以提高FL的性能和能效性。</li>
<li>methods: 本文使用closed-form表达式来计算FL损失函数的上界，并对Client Scheduling、资源分配和本地训练 epoch数进行优化。</li>
<li>results: 实验结果表明，提出的算法在实际数据集上比其他参考方法更高的学习精度和能 consumption。<details>
<summary>Abstract</summary>
With the rapid proliferation of smart mobile devices, federated learning (FL) has been widely considered for application in wireless networks for distributed model training. However, data heterogeneity, e.g., non-independently identically distributions and different sizes of training data among clients, poses major challenges to wireless FL. Limited communication resources complicate the implementation of fair scheduling which is required for training on heterogeneous data, and further deteriorate the overall performance. To address this issue, this paper focuses on performance analysis and optimization for wireless FL, considering data heterogeneity, combined with wireless resource allocation. Specifically, we first develop a closed-form expression for an upper bound on the FL loss function, with a particular emphasis on data heterogeneity described by a dataset size vector and a data divergence vector. Then we formulate the loss function minimization problem, under constraints on long-term energy consumption and latency, and jointly optimize client scheduling, resource allocation, and the number of local training epochs (CRE). Next, via the Lyapunov drift technique, we transform the CRE optimization problem into a series of tractable problems. Extensive experiments on real-world datasets demonstrate that the proposed algorithm outperforms other benchmarks in terms of the learning accuracy and energy consumption.
</details>
<details>
<summary>摘要</summary>
随着智能移动设备的普及，分布式学习（FL）在无线网络中得到了广泛的考虑，用于分布式模型训练。然而，数据不均衡，如非独立同分布和不同的训练数据大小 среди客户端，对无线FL的应用带来了主要挑战。限制通信资源使得实现公平调度变得更加困难，从而降低总性能。为解决这个问题，这篇论文关注无线FL的性能分析和优化，考虑到数据不均衡，并与无线资源分配相结合。首先，我们开发了一个关于FL损失函数上的上界，强调数据不均衡的特点，由一个数据大小向量和一个数据差异向量描述。然后，我们将损失函数最小化问题转化为一个具有长期能源占用和延迟的约束的优化问题。通过利用Lyapunov漂移技术，我们将CRE优化问题转化为一系列可解的问题。在实际数据上进行了广泛的实验，结果表明，我们的算法在学习精度和能源消耗方面比其他参考值更高。
</details></li>
</ul>
<hr>
<h2 id="Branched-Latent-Neural-Operators"><a href="#Branched-Latent-Neural-Operators" class="headerlink" title="Branched Latent Neural Operators"></a>Branched Latent Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02599">http://arxiv.org/abs/2308.02599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanfordcbcl/blno.jl">https://github.com/stanfordcbcl/blno.jl</a></li>
<li>paper_authors: Matteo Salvador, Alison Lesley Marsden</li>
<li>for:  This paper aims to develop a novel computational tool for building reliable and efficient reduced-order models for digital twinning in engineering applications.</li>
<li>methods: The paper proposes the use of Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. BLNOs are defined as simple and compact feedforward partially-connected neural networks that structurally disentangle inputs with different intrinsic roles.</li>
<li>results: The paper demonstrates the effectiveness of BLNOs in a challenging test case involving biophysically detailed electrophysiology simulations in a biventricular cardiac model of a pediatric patient with hypoplastic left heart syndrome. The model includes a purkinje network for fast conduction and a heart-torso geometry. The paper shows that BLNOs can retain just 7 hidden layers and 19 neurons per layer, and achieve a mean square error of $10^{-4}$ on an independent test dataset comprised of 50 additional electrophysiology simulations.<details>
<summary>Abstract</summary>
We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed electrophysiology simulations in a biventricular cardiac model of a pediatric patient with hypoplastic left heart syndrome. The model includes a purkinje network for fast conduction and a heart-torso geometry. Specifically, we trained BLNOs on 150 in silico generated 12-lead electrocardiograms (ECGs) while spanning 7 model parameters, covering cell-scale, organ-level and electrical dyssynchrony. Although the 12-lead ECGs manifest very fast dynamics with sharp gradients, after automatic hyperparameter tuning the optimal BLNO, trained in less than 3 hours on a single CPU, retains just 7 hidden layers and 19 neurons per layer. The mean square error is on the order of $10^{-4}$ on an independent test dataset comprised of 50 additional electrophysiology simulations. This paper provides a novel computational tool to build reliable and efficient reduced-order models for digital twinning in engineering applications.
</details>
<details>
<summary>摘要</summary>
我们引入分支隐藏神经操作符（BLNOs），以学习输入-输出对应器，模型复杂物理过程。 BLNO 是一个简单且紧凑的Feedforward 内部连接神经网络，它将输入变数分类为不同的内在角色，例如时间变数和模型参数，并将它们转换为一个通用的应用领域。 BLNO 利用可读性的隐藏输出增强学习过程，并突破维度给定问题的咒语，通过在训练阶段实现小训练集和短时间内的优秀一致性。此外，对于完全连接结构而言，部分连接可以对缩减可调 Parameters 数量。我们透过实际应用在一个儿童心脏病 hypoplastic left heart syndrome 的双心室心脏模型中，并在该模型中包含 Purkinje 网络和心脏-肋间 geometry。具体来说，我们将 BLNO 训练在 150 个silico生成的 12 项电击ogram (ECG) 上，涵盖 7 个模型参数，包括细胞层、器官层和电子 Dyssynchrony。虽然 12 项 ECG 呈现非常快的动态，但是通过自动优化参数后，最佳 BLNO 在仅三个小时内在单一 CPU 上训练，只有 7 个隐藏层和 19 个神经元 per 层。该模型的平方误差在统计上为 $10^{-4}$，在 50 个其他电生物频谱 simulations 的独立测试集中进行验证。本研究提供了一个新的 Computational 工具，可以建立可靠和高效的实际应用中的简化模型，以应用于工程应用中的数字双胞志。
</details></li>
</ul>
<hr>
<h2 id="Eva-A-General-Vectorized-Approximation-Framework-for-Second-order-Optimization"><a href="#Eva-A-General-Vectorized-Approximation-Framework-for-Second-order-Optimization" class="headerlink" title="Eva: A General Vectorized Approximation Framework for Second-order Optimization"></a>Eva: A General Vectorized Approximation Framework for Second-order Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02123">http://arxiv.org/abs/2308.02123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Zhang, Shaohuai Shi, Bo Li</li>
<li>for: 这个研究旨在提高深度学习模型训练的效率，减少计算和记忆过程中的过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程<details>
<summary>Abstract</summary>
Second-order optimization algorithms exhibit excellent convergence properties for training deep learning models, but often incur significant computation and memory overheads. This can result in lower training efficiency than the first-order counterparts such as stochastic gradient descent (SGD). In this work, we present a memory- and time-efficient second-order algorithm named Eva with two novel techniques: 1) we construct the second-order information with the Kronecker factorization of small stochastic vectors over a mini-batch of training data to reduce memory consumption, and 2) we derive an efficient update formula without explicitly computing the inverse of matrices using the Sherman-Morrison formula. We further extend Eva to a general vectorized approximation framework to improve the compute and memory efficiency of two existing second-order algorithms (FOOF and Shampoo) without affecting their convergence performance. Extensive experimental results on different models and datasets show that Eva reduces the end-to-end training time up to 2.05x and 2.42x compared to first-order SGD and second-order algorithms (K-FAC and Shampoo), respectively.
</details>
<details>
<summary>摘要</summary>
Second-order优化算法在训练深度学习模型时展现出极佳的收敛性质，但通常会导致计算和内存开销增加。这可能会导致训练效率低于首次优化算法 such as 随机梯度下降（SGD）。在这项工作中，我们提出了一种具有内存和时间效率的第二次优化算法名为Eva，并采用了两种新的技术：1）我们通过小批量训练数据的克ро内克分解来减少内存占用，2）我们 derivate了高效的更新公式，不需要直接计算矩阵的逆元。我们进一步扩展Eva到一个通用的向量化近似框架，以提高两个现有的第二次优化算法（FOOF和Shampoo）的计算和内存效率，无需影响其收敛性能。我们在不同的模型和数据集上进行了广泛的实验，结果显示，Eva可以比首次优化算法和第二次优化算法（K-FAC和Shampoo）减少综合训练时间，具体的比例为2.05倍和2.42倍。
</details></li>
</ul>
<hr>
<h2 id="Model-Provenance-via-Model-DNA"><a href="#Model-Provenance-via-Model-DNA" class="headerlink" title="Model Provenance via Model DNA"></a>Model Provenance via Model DNA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02121">http://arxiv.org/abs/2308.02121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Mu, Yu Wang, Yehong Zhang, Jiaqi Zhang, Hui Wang, Yang Xiang, Yue Yu</li>
<li>for: This paper focuses on the problem of Model Provenance (MP) in machine learning (ML), which aims to determine whether a source model serves as the provenance for a target model.</li>
<li>methods: The authors introduce a novel concept of Model DNA, which represents the unique characteristics of a machine learning model, and use a data-driven and model-driven representation learning method to encode the model’s training data and input-output information as a compact and comprehensive representation of the model.</li>
<li>results: The authors develop an efficient framework for model provenance identification, which enables them to accurately identify whether a source model is a pre-training model of a target model. They conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of their approach.<details>
<summary>Abstract</summary>
Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this model DNA, we develop an efficient framework for model provenance identification, which enables us to identify whether a source model is a pre-training model of a target model. We conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of our approach in accurately identifying model provenance.
</details>
<details>
<summary>摘要</summary>
To address this gap, we introduce a novel concept called Model DNA, which represents the unique characteristics of a machine learning model. We use a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this model DNA, we develop an efficient framework for model provenance identification, which enables us to identify whether a source model is a pre-training model of a target model.We conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of our approach in accurately identifying model provenance. Our approach is efficient and can be applied to a wide range of ML models, providing a valuable tool for ensuring the security and intellectual property of ML models.
</details></li>
</ul>
<hr>
<h2 id="Designing-a-Deep-Learning-Driven-Resource-Efficient-Diagnostic-System-for-Metastatic-Breast-Cancer-Reducing-Long-Delays-of-Clinical-Diagnosis-and-Improving-Patient-Survival-in-Developing-Countries"><a href="#Designing-a-Deep-Learning-Driven-Resource-Efficient-Diagnostic-System-for-Metastatic-Breast-Cancer-Reducing-Long-Delays-of-Clinical-Diagnosis-and-Improving-Patient-Survival-in-Developing-Countries" class="headerlink" title="Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries"></a>Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02597">http://arxiv.org/abs/2308.02597</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Gao, Dayong Wang, Yi Huang</li>
<li>for: 这份研究旨在解决癌症病理诊断过程中的时间延迟问题，特别是癌症患者在发展中国家中的诊断过程中的延迟，以提高癌症患者的存活率。</li>
<li>methods: 这份研究使用了深度学习技术，开发了一个基于MobileNetV2的诊断模型，能够实现高精度的诊断和computational efficiency。</li>
<li>results: 根据评估结果，MobileNetV2基本模型在诊断精度、模型普遍性和模型训练效率等方面都超过了VGG16、ResNet50和ResNet101模型。此外，Visual比较表明，MobileNetV2诊断模型能够识别非常小的癌症细胞在大量正常细胞中，实现了人工影像分析的挑战。<details>
<summary>Abstract</summary>
Breast cancer is one of the leading causes of cancer mortality. Breast cancer patients in developing countries, especially sub-Saharan Africa, South Asia, and South America, suffer from the highest mortality rate in the world. One crucial factor contributing to the global disparity in mortality rate is long delay of diagnosis due to a severe shortage of trained pathologists, which consequently has led to a large proportion of late-stage presentation at diagnosis. The delay between the initial development of symptoms and the receipt of a diagnosis could stretch upwards 15 months. To tackle this critical healthcare disparity, this research has developed a deep learning-based diagnosis system for metastatic breast cancer that can achieve high diagnostic accuracy as well as computational efficiency. Based on our evaluation, the MobileNetV2-based diagnostic model outperformed the more complex VGG16, ResNet50 and ResNet101 models in diagnostic accuracy, model generalization, and model training efficiency. The visual comparisons between the model prediction and ground truth have demonstrated that the MobileNetV2 diagnostic models can identify very small cancerous nodes embedded in a large area of normal cells which is challenging for manual image analysis. Equally Important, the light weighted MobleNetV2 models were computationally efficient and ready for mobile devices or devices of low computational power. These advances empower the development of a resource-efficient and high performing AI-based metastatic breast cancer diagnostic system that can adapt to under-resourced healthcare facilities in developing countries. This research provides an innovative technological solution to address the long delays in metastatic breast cancer diagnosis and the consequent disparity in patient survival outcome in developing countries.
</details>
<details>
<summary>摘要</summary>
乳癌是全球最主要的癌症死亡原因之一，特别是在发展中国家，如非洲南部、南亚和南美， breast cancer 患者的死亡率最高。一个重要的因素导致全球的医疗差距是诊断延迟，因为缺乏培训的病理学家，导致许多患者在诊断时 already in 晚期。延迟从症状出现到诊断的时间可以达15个月。为了解决这个严重的医疗差距，这项研究开发了一个基于深度学习的乳癌诊断系统，可以实现高精度和计算效率。根据我们的评估，使用 MobileNetV2 模型的诊断模型在精度、通用性和训练效率三个方面都高于 VGG16、ResNet50 和 ResNet101 模型。视觉比较表明，MobileNetV2 模型可以准确地检测小型患者中的癌细胞，这是人工图像分析困难的。此外，MobileNetV2 模型的计算效率较低，适用于移动设备或低计算能力的设备。这些进步使得可以开发一个资源高效和高性能的人工智能基于乳癌诊断系统，适应发展中国家的医疗设施。这项研究提供了一种创新的科技解决方案，以Address the long delays in metastatic breast cancer diagnosis and the resulting disparity in patient survival outcomes in developing countries.
</details></li>
</ul>
<hr>
<h2 id="VQGraph-Graph-Vector-Quantization-for-Bridging-GNNs-and-MLPs"><a href="#VQGraph-Graph-Vector-Quantization-for-Bridging-GNNs-and-MLPs" class="headerlink" title="VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs"></a>VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02117">http://arxiv.org/abs/2308.02117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangling0818/vqgraph">https://github.com/yangling0818/vqgraph</a></li>
<li>paper_authors: Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin Cui, Muhan Zhang, Jure Leskovec</li>
<li>for: 提高Graph Neural Networks (GNNs)的批处理能力和实时性，以便在具有延迟限制的实际应用中使用。</li>
<li>methods: 采用知识传承（KD）学习计算效率高的多层感知器（MLP），通过模仿GNN的输出来学习GNN的知识。同时，使用一种新的结构意识graph tokenizer，以及一种基于软标签分配的token-based distillation目标，以便充分传递GNN的结构知识到MLP中。</li>
<li>results: 实验和分析表明，VQGraph可以减少GNN的批处理时间，并且在七个图数据集上实现新的状态机器人性表现，包括在推导和泵化设置下的表现。VQGraph可以比GNN更快地进行推理，并且在实际应用中可以提高GNN的准确率。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propose a new token-based distillation objective based on soft token assignments to sufficiently transfer the structural knowledge from GNN to MLP. Extensive experiments and analyses demonstrate the strong performance of VQGraph, where we achieve new state-of-the-art performance on GNN-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively. Code: https://github.com/YangLing0818/VQGraph.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 通过消息传递来更新节点表示，这会导致实际延迟应用中的可扩展性问题。为解决这个问题，现有方法采用知识传递（KD）来学习计算效率高的多层感知器（MLP），但是现有GNN表示空间可能不够表示图像下的多样化本地结构，这限制了GNN的知识传递。我们提出了一种新的框架VQGraph，用于学习图像表示空间，以bridging GNNs和MLPs。我们采用变体的vector-quantized variational autoencoder（VQ-VAE）的encoder作为结构意识图像tokenizer，该tokenizer可以明确表示不同本地结构中的节点，并组成一个有意义的代码库。利用学习的代码库，我们提出了一个新的符号分配目标，以便充分传递GNN中的结构知识到MLP。我们在七个图像 dataset 上进行了广泛的实验和分析，并证明了VQGraph的强大表现。我们在transductive和induction Setting中， achieved new state-of-the-art performance on GNN-MLP distillation，并且在GNN和独立MLP上的性能上提高了3.90%和28.05%。此外，我们还证明了VQGraph在GNN上进行更快的推理，比GNN的828倍。代码：https://github.com/YangLing0818/VQGraph。
</details></li>
</ul>
<hr>
<h2 id="Breast-Ultrasound-Tumor-Classification-Using-a-Hybrid-Multitask-CNN-Transformer-Network"><a href="#Breast-Ultrasound-Tumor-Classification-Using-a-Hybrid-Multitask-CNN-Transformer-Network" class="headerlink" title="Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network"></a>Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02101">http://arxiv.org/abs/2308.02101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bryar Shareef, Min Xian, Aleksandar Vakanski, Haotian Wang</li>
<li>for: 这个研究旨在提出一个混合多任务深度学习网络（Hybrid-MT-ESTAN），用于肺肿瘤分类和分 segmentation。</li>
<li>methods: 这个方法使用了 CNN 和 Swin Transformer 两种不同的架构，以提高全球背景信息的捕捉和地方图像特征的维持。</li>
<li>results: 实验结果显示，Hybrid-MT-ESTAN 得到了最高的准确率（82.7%）、敏感度（86.4%）和 F1 分数（86.0%）。<details>
<summary>Abstract</summary>
Capturing global contextual information plays a critical role in breast ultrasound (BUS) image classification. Although convolutional neural networks (CNNs) have demonstrated reliable performance in tumor classification, they have inherent limitations for modeling global and long-range dependencies due to the localized nature of convolution operations. Vision Transformers have an improved capability of capturing global contextual information but may distort the local image patterns due to the tokenization operations. In this study, we proposed a hybrid multitask deep neural network called Hybrid-MT-ESTAN, designed to perform BUS tumor classification and segmentation using a hybrid architecture composed of CNNs and Swin Transformer components. The proposed approach was compared to nine BUS classification methods and evaluated using seven quantitative metrics on a dataset of 3,320 BUS images. The results indicate that Hybrid-MT-ESTAN achieved the highest accuracy, sensitivity, and F1 score of 82.7%, 86.4%, and 86.0%, respectively.
</details>
<details>
<summary>摘要</summary>
capture global contextual information 在乳腺超声图像分类中扮演着关键性的角色。尽管 convolutional neural networks (CNNs) 在肿瘤分类中表现出了可靠的性，但它们具有内置的局部化特性，因此可能导致模型长距离和全局依赖关系的模型化困难。 vision transformers 具有改善全局上下文信息捕捉的能力，但可能会因为 tokenization 操作而导致本地图像模式的扭曲。在这项研究中，我们提出了一种 hybrid multitask deep neural network called Hybrid-MT-ESTAN，用于实现乳腺超声图像分类和分割。我们的方法与 nine 种乳腺分类方法进行比较，并在一个包含 3,320 个乳腺超声图像的数据集上进行评估。结果表明，Hybrid-MT-ESTAN 达到了最高的准确率、敏感度和 F1 分数，即 82.7%、86.4% 和 86.0%  соответственно。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Model-Adaptation-for-Continual-Learning-at-the-Edge"><a href="#Efficient-Model-Adaptation-for-Continual-Learning-at-the-Edge" class="headerlink" title="Efficient Model Adaptation for Continual Learning at the Edge"></a>Efficient Model Adaptation for Continual Learning at the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02084">http://arxiv.org/abs/2308.02084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachary A. Daniels, Jun Hu, Michael Lomnitz, Phil Miller, Aswin Raghavan, Joe Zhang, Michael Piacentino, David Zhang</li>
<li>for: 这个研究旨在提供一个非站势自动机器学习（AutoML）框架，以便在资料分布随时变化时进行高效的连续学习。</li>
<li>methods: 这个框架使用固定的深度神经网（DNN）特征嵌入器，并训练浅层网络来处理新数据。它还使用了数维计算（HDC）和零 shot神经架搜索（ZS-NAS）来探测新数据是否为外部数据（OOD），并适当地调整模型以适应OOD数据。</li>
<li>results: 在多个域别数据集上进行评估，这个框架实现了优秀的性能，比如果探测OOD数据和几何shot NAS。<details>
<summary>Abstract</summary>
Most machine learning (ML) systems assume stationary and matching data distributions during training and deployment. This is often a false assumption. When ML models are deployed on real devices, data distributions often shift over time due to changes in environmental factors, sensor characteristics, and task-of-interest. While it is possible to have a human-in-the-loop to monitor for distribution shifts and engineer new architectures in response to these shifts, such a setup is not cost-effective. Instead, non-stationary automated ML (AutoML) models are needed. This paper presents the Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning under domain shifts. The EAR framework uses a fixed deep neural network (DNN) feature encoder and trains shallow networks on top of the encoder to handle novel data. The EAR framework is capable of 1) detecting when new data is out-of-distribution (OOD) by combining DNNs with hyperdimensional computing (HDC), 2) identifying low-parameter neural adaptors to adapt the model to the OOD data using zero-shot neural architecture search (ZS-NAS), and 3) minimizing catastrophic forgetting on previous tasks by progressively growing the neural architecture as needed and dynamically routing data through the appropriate adaptors and reconfigurators for handling domain-incremental and class-incremental continual learning. We systematically evaluate our approach on several benchmark datasets for domain adaptation and demonstrate strong performance compared to state-of-the-art algorithms for OOD detection and few-/zero-shot NAS.
</details>
<details>
<summary>摘要</summary>
大多数机器学习（ML）系统假设训练和部署时数据分布是静止的，这是一个不实际的假设。当 ML 模型在实际设备上部署时，数据分布经常会随着环境因素、传感器特性和任务 интерес而变化。虽然可以有人在Loop监控数据分布的变化并为此设计新的建筑，但这种设置不是可cost-effective的。而是需要不静止的自动机器学习（AutoML）模型。这篇论文提出了Encoder-Adaptor-Reconfigurator（EAR）框架，用于效率地进行适应域shift continual learning。EAR框架使用固定的深度神经网络（DNN）特征编码器，并在编码器之上训练浅层网络来处理新数据。EAR框架可以1）将新数据标记为out-of-distribution（OOD），通过将DNN与高维计算（HDC）结合使用，2）通过零 shot neural architecture search（ZS-NAS）来适应OOD数据，3）在前一个任务上避免忘记性衰变，通过逐渐增加神经建筑和动态路由数据通过适当的适应器和重配置器来处理域增量和类增量 continual learning。我们系统性地评估了我们的方法在域适应数据上的多个benchmark datasets，并demonstrated strong performance compared to state-of-the-art algorithms for OOD detection和few-/zero-shot NAS。
</details></li>
</ul>
<hr>
<h2 id="Target-specification-bias-counterfactual-prediction-and-algorithmic-fairness-in-healthcare"><a href="#Target-specification-bias-counterfactual-prediction-and-algorithmic-fairness-in-healthcare" class="headerlink" title="Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare"></a>Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02081">http://arxiv.org/abs/2308.02081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eran Tal</li>
<li>for: 这篇论文探讨了机器学习（ML）在医疗领域中的偏见问题，并提出了一种更普遍的偏见来源：目标规定偏见。</li>
<li>methods: 这篇论文使用了现有的数据和健康差异的研究，以及现有的机器学习算法和模型。然而，它发现了一种更加普遍的偏见来源：目标规定偏见。</li>
<li>results: 这篇论文发现了target specification bias可能会导致估计准确性过高，使用医疗资源不fficient，并导致伤害病人的决策。<details>
<summary>Abstract</summary>
Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology - the science of measurement - suggests ways of counteracting target specification bias and avoiding its harmful consequences.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在医疗领域中的偏见通常被归结于不完整或不代表性的数据，或者下面的健康差异。本文标识了更广泛的偏见来源，对临床实用性有影响的预测工具：目标规定偏见。目标规定偏见发生在运行化目标变量时与决策者定义的目标之间的匹配不匹配。这种匹配不匹配通常是柔和的，来自于决策者通常关心预测实际医疗情况下的结果，而不是实际情况。这种偏见不受数据限制和健康差异影响，并且不会被纠正。如果不纠正，它会导致预测精度的过高估计，医疗资源的不效利用，以及对病人伤害的不佳决策。近些年的metrology研究（量度科学）提供了对抗目标规定偏见的方法，避免其不良后果。
</details></li>
</ul>
<hr>
<h2 id="Causality-Guided-Disentanglement-for-Cross-Platform-Hate-Speech-Detection"><a href="#Causality-Guided-Disentanglement-for-Cross-Platform-Hate-Speech-Detection" class="headerlink" title="Causality Guided Disentanglement for Cross-Platform Hate Speech Detection"></a>Causality Guided Disentanglement for Cross-Platform Hate Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02080">http://arxiv.org/abs/2308.02080</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paras2612/catch">https://github.com/paras2612/catch</a></li>
<li>paper_authors: Paras Sheth, Tharindu Kumarage, Raha Moraffah, Aman Chadha, Huan Liu</li>
<li>for: 寻找一种可以在多个不同平台上推断仇恨言语的 hate speech 检测模型。</li>
<li>methods: 我们使用了分离输入表示的方法，将输入特征分解成不同平台的特征和共同的特征，以便在不同平台上学习通用的 hate speech 检测模型。我们还学习了 causal 关系，以便更好地理解共同的表示。</li>
<li>results: 我们的模型在四个不同平台上进行了广泛的实验，结果显示我们的模型比现有的状态对方法更高效地检测通用 hate speech。<details>
<summary>Abstract</summary>
Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causal relationships, which remain constant across diverse environments, can significantly aid in understanding invariant representations in hate speech. By disentangling input into platform-dependent features (useful for predicting hate targets) and platform-independent features (used to predict the presence of hate), we learn invariant representations resistant to distribution shifts. These features are then used to predict hate speech across unseen platforms. Our extensive experiments across four platforms highlight our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:社交媒体平台，尽管它们在促进开放对话方面具有价值，但它们经常被利用来传播危险内容。现有的深度学习和自然语言处理模型在检测这种危险内容方面过于依赖于域专门的术语，这会导致它们在检测普遍的谩骂言语方面减少其能力。另一个主要挑战是当 платформы缺乏高质量的标注数据 для训练时，导致需要跨平台模型，可以适应不同的分布Shift。我们的研究推出了一种可以在不同的平台上训练的跨平台谩骂言语检测模型。为了实现良好的泛化性 across platforms，我们可以分解输入表示为不变和平台特定的特征。我们还认为，学习不变的关系，可以在多种环境中保持相同的常量，可以大幅提高对不变表示的理解。通过将输入分解为平台特定的特征（有用于预测谩骂目标）和平台独立的特征（用于预测谩骂存在），我们学习了不变的表示，抗性于分布Shift。这些特征然后用于预测谩骂言语 across 未看到的平台。我们的广泛的实验 across four platforms  highlights our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.Translated into Traditional Chinese:社交媒体平台，不过它们在促进开放对话方面具有价值，但它们经常被利用来传播危险内容。现有的深度学习和自然语言处理模型在检测这种危险内容方面过度依赖域专门的术语，这会导致它们在检测普遍的谩驳言语方面减少其能力。另一个主要挑战是当平台缺乏高质量的标注数据 для训练时，导致需要跨平台模型，可以适应不同的分布Shift。我们的研究推出了一种可以在不同的平台上训练的跨平台谩驳言语检测模型。为了实现良好的泛化性 across platforms，我们可以分解输入表示为不变和平台特定的特征。我们还认为，学习不变的关系，可以在多种环境中保持相同的常量，可以大幅提高对不变表示的理解。通过将输入分解为平台特定的特征（有用于预测谩驳目标）和平台独立的特征（用于预测谩驳存在），我们学习了不变的表示，抗性于分布Shift。这些特征然后用于预测谩驳言语 across 未看到的平台。我们的广泛的实验 across four platforms  highlights our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.
</details></li>
</ul>
<hr>
<h2 id="Specious-Sites-Tracking-the-Spread-and-Sway-of-Spurious-News-Stories-at-Scale"><a href="#Specious-Sites-Tracking-the-Spread-and-Sway-of-Spurious-News-Stories-at-Scale" class="headerlink" title="Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale"></a>Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02068">http://arxiv.org/abs/2308.02068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hans W. A. Hanley, Deepak Kumar, Zakir Durumeric</li>
<li>for: 这篇论文旨在Automatically track and analyze online news narratives to identify misinformation and support fact-checking efforts.</li>
<li>methods: 该系统使用大型自然语言模型MPNet和DP-Means归一 clustering算法，每天抓取1,404家不可靠新闻网站，以分析在线社区中流行的新闻 narative。</li>
<li>results: 研究发现2022年最受欢迎的新闻 narative，并确定了传播这些新闻 narative的最有影响力的网站。系统还可以帮助 fact-checkers like Politifact, Reuters, AP News 更快地识别和推篱虚假新闻。<details>
<summary>Abstract</summary>
Misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. However, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. In this work, utilizing daily scrapes of 1,404 unreliable news websites, the large-language model MPNet, and DP-Means clustering, we introduce a system to automatically isolate and analyze the narratives spread within online ecosystems. Identifying 55,301 narratives on these 1,404 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and magnify narratives. Finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and aid fact-checkers like Politifact, Reuters, and AP News in more quickly addressing misinformation stories.
</details>
<details>
<summary>摘要</summary>
互联网上充满谣言、宣传和谎言，一些媒体报道有危害公共健康、选举和个人安全的危险。然而，研究社区在 automatization 和 programmatic 方面对新闻媒体的跟踪仍然缺乏有效的方法。在这项工作中，我们利用每天抓取 1,404 个不可靠新闻网站的数据，大型自然语言模型 MPNet，以及 DP-Means 聚类算法，提出一个自动从在线生态系统中分离和分析新闻媒体的系统。我们分析了这些网站上的 55,301 个媒体报道，描述了在 2022 年最具影响力的新闻媒体，以及它们如何促进和强化新闻媒体。最后，我们示出了我们的系统可以帮助ifact-checkers like Politifact、Reuters 和 AP News 更快地处理谣言故事。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Task-Interference-in-Multi-Task-Learning-via-Explicit-Task-Routing-with-Non-Learnable-Primitives"><a href="#Mitigating-Task-Interference-in-Multi-Task-Learning-via-Explicit-Task-Routing-with-Non-Learnable-Primitives" class="headerlink" title="Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives"></a>Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02066">http://arxiv.org/abs/2308.02066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhichao-lu/etr-nlp-mtl">https://github.com/zhichao-lu/etr-nlp-mtl</a></li>
<li>paper_authors: Chuntao Ding, Zhichao Lu, Shangguang Wang, Ran Cheng, Vishnu Naresh Boddeti</li>
<li>for: 这个论文目的是提出一种基于非学习 primitives 和显式任务路由（ETR）的多任务学习（MTL）方法，以降低任务干扰。</li>
<li>methods: 该方法使用非学习 primitives 提取多个任务共同的特征，并将这些特征重新组合到共同分支和每个任务专门的分支中。它还使用显式任务路由来隔离学习参数，以便降低任务干扰。</li>
<li>results: 实验结果表明，ETR-NLP 在图像水平分类和像素粒度稠密预测多任务学习问题中具有显著优势，比基eline模型更高的性能，同时具有更少的学习参数和相似的计算量。代码可以在这里下载：<a target="_blank" rel="noopener" href="https://github.com/zhichao-lu/etr-nlp-mtl%E3%80%82">https://github.com/zhichao-lu/etr-nlp-mtl。</a><details>
<summary>Abstract</summary>
Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this \href{https://github.com/zhichao-lu/etr-nlp-mtl}.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）目的是学习一个模型来完成多个任务，利用任务之间的共享信息。现有的 MTL 模型却存在任务干扰的问题。减轻任务干扰的努力主要集中在损失/梯度均衡或隐式参数分割中，其中一些任务参数与其他任务参数之间存在部分重叠。在这篇论文中，我们提出了ETR-NLP，一种通过非学习性 primitives（NLPs）和显式任务路由（ETR）来减轻任务干扰的方法。我们的关键想法是使用非学习性 primitives 提取一组多样化的任务不受限制的特征，然后将其重新组合到一个共享的分支和每个任务的显式分支中。非学习性 primitives 和显式划分学习参数为共享和任务特定的一些允许我们适应性的灵活性，以最小化任务干扰。我们在图像级别的分类和像素级别的整合预测MTL问题中评估了ETR-NLP网络的效果。实验结果表明，ETR-NLP在所有数据集上都超越了当前的基eline，减少了学习参数数量和相同的FLOPs。代码可以在这里找到：https://github.com/zhichao-lu/etr-nlp-mtl。
</details></li>
</ul>
<hr>
<h2 id="On-the-Biometric-Capacity-of-Generative-Face-Models"><a href="#On-the-Biometric-Capacity-of-Generative-Face-Models" class="headerlink" title="On the Biometric Capacity of Generative Face Models"></a>On the Biometric Capacity of Generative Face Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02065">http://arxiv.org/abs/2308.02065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/human-analysis/capacity-generative-face-models">https://github.com/human-analysis/capacity-generative-face-models</a></li>
<li>paper_authors: Vishnu Naresh Boddeti, Gautam Sreekumar, Arun Ross</li>
<li>for: 本研究的目的是为了评估和比较不同的生成人脸模型，以及确定这些模型的扩展性上的最高限制。</li>
<li>methods: 本研究使用了一种统计方法来估算生成人脸图像在幂体特征空间中的生物学容量。</li>
<li>results: 研究发现，使用 ArcFace 表示法，在 false acceptance rate (FAR) 为 0.1% 时，StyleGAN3 和 DCFace 的生物学容量的最高限制分别为 $1.43\times10^6$ 和 $1.190\times10^4$。此外，随着 Desired FAR 的下降，生物学容量的估算值也降低了许多。 gender 和 age 的影响也被研究发现。<details>
<summary>Abstract</summary>
There has been tremendous progress in generating realistic faces with high fidelity over the past few years. Despite this progress, a crucial question remains unanswered: "Given a generative face model, how many unique identities can it generate?" In other words, what is the biometric capacity of the generative face model? A scientific basis for answering this question will benefit evaluating and comparing different generative face models and establish an upper bound on their scalability. This paper proposes a statistical approach to estimate the biometric capacity of generated face images in a hyperspherical feature space. We employ our approach on multiple generative models, including unconditional generators like StyleGAN, Latent Diffusion Model, and "Generated Photos," as well as DCFace, a class-conditional generator. We also estimate capacity w.r.t. demographic attributes such as gender and age. Our capacity estimates indicate that (a) under ArcFace representation at a false acceptance rate (FAR) of 0.1%, StyleGAN3 and DCFace have a capacity upper bound of $1.43\times10^6$ and $1.190\times10^4$, respectively; (b) the capacity reduces drastically as we lower the desired FAR with an estimate of $1.796\times10^4$ and $562$ at FAR of 1% and 10%, respectively, for StyleGAN3; (c) there is no discernible disparity in the capacity w.r.t gender; and (d) for some generative models, there is an appreciable disparity in the capacity w.r.t age. Code is available at https://github.com/human-analysis/capacity-generative-face-models.
</details>
<details>
<summary>摘要</summary>
在过去几年里，生成真实的脸部图像的进步很大。尽管如此，一个关键的问题仍然未得到答案：“给定一个生成脸部模型，它可以生成多少个唯一的标识？”或者说，生成脸部模型的生物 metric capacity 是多少？一个科学基础来回答这个问题将有助于评估和比较不同的生成脸部模型，并设置生成脸部模型的可扩展性的上限。本文提出了一种统计方法来估算生成脸部图像的生物 metric 容量，我们使用这种方法对多个生成模型进行了测试，包括 StyleGAN、Latent Diffusion Model 和 "Generated Photos" 等模型，以及 DCFace 等类别 conditional 生成模型。我们还估算了基于人口特征（如性别和年龄）的容量。我们的容量估算表明：（a）在 ArcFace 表示下，False Acceptance Rate (FAR) 为 0.1% 时，StyleGAN3 和 DCFace 的容量Upper Bound 分别为 $1.43\times10^6$ 和 $1.190\times10^4$；（b）随着 Desired FAR 降低，容量减少了极其剧烈， ArcFace 表示下，FAR 为 1% 和 10% 时，StyleGAN3 的容量估算为 $1.796\times10^4$ 和 $562$；（c）对于一些生成模型， gender 不存在显著的差异；（d）对于一些生成模型， age 存在可观的差异。相关代码可以在 GitHub 上找到：https://github.com/human-analysis/capacity-generative-face-models。
</details></li>
</ul>
<hr>
<h2 id="Accurate-Neural-Network-Pruning-Requires-Rethinking-Sparse-Optimization"><a href="#Accurate-Neural-Network-Pruning-Requires-Rethinking-Sparse-Optimization" class="headerlink" title="Accurate Neural Network Pruning Requires Rethinking Sparse Optimization"></a>Accurate Neural Network Pruning Requires Rethinking Sparse Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02060">http://arxiv.org/abs/2308.02060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Kuznedelev, Eldar Kurtic, Eugenia Iofinova, Elias Frantar, Alexandra Peste, Dan Alistarh</li>
<li>for: 这个论文的目的是研究在使用标准的随机优化技术进行训练稀皮网络时，稀皮性如何影响模型训练。</li>
<li>methods: 作者使用了标准的计算机视觉和自然语言处理稀皮benchmark进行研究，并提供了新的方法来 Mitigate the issue of under-training in sparse training。</li>
<li>results: 研究发现，使用标准粗糙训练策略进行稀皮训练是不优化的，而使用新提出的方法可以在计算机视觉和自然语言处理领域中实现高精度和高稀皮性的模型训练。<details>
<summary>Abstract</summary>
Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art results in both settings in the high-sparsity regime, and providing detailed analyses for the difficulty of sparse training in both scenarios. Our work sets a new threshold in terms of the accuracies that can be achieved under high sparsity, and should inspire further research into improving sparse model training, to reach higher accuracies under high sparsity, but also to do so efficiently.
</details>
<details>
<summary>摘要</summary>
In this work, we examine the impact of high sparsity on model training using standard computer vision and natural language processing sparsity benchmarks. We show that using standard dense training recipes for sparse training is suboptimal and results in under-training. We propose new approaches to mitigate this issue for both sparse pre-training of vision models (e.g., ResNet50/ImageNet) and sparse fine-tuning of language models (e.g., BERT/GLUE). Our approaches achieve state-of-the-art results in both settings in the high-sparsity regime and provide detailed analyses of the difficulty of sparse training in both scenarios. Our work sets a new threshold in terms of the accuracies that can be achieved under high sparsity and should inspire further research into improving sparse model training to reach higher accuracies under high sparsity efficiently.
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Recklessness-to-Collaborative-Filtering-based-Recommender-Systems"><a href="#Incorporating-Recklessness-to-Collaborative-Filtering-based-Recommender-Systems" class="headerlink" title="Incorporating Recklessness to Collaborative Filtering based Recommender Systems"></a>Incorporating Recklessness to Collaborative Filtering based Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02058">http://arxiv.org/abs/2308.02058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knodis-research-group/recklessness-regularization">https://github.com/knodis-research-group/recklessness-regularization</a></li>
<li>paper_authors: Diego Pérez-López, Fernando Ortega, Ángel González-Prieto, Jorge Dueñas-Lerín</li>
<li>for: 提高爆料系统的决策可靠性和创新性</li>
<li>methods: 引入一个新的学习过程中的recklessness项，用于控制决策时的风险水平</li>
<li>results: 实验结果表明，recklessness不仅能够进行风险规避，还可以提高爆料系统提供的预测量和质量。<details>
<summary>Abstract</summary>
Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
</details>
<details>
<summary>摘要</summary>
建议系统，包括一些可靠度度量，往往会变得更加保守，因为它们需要保持可靠度。这会导致建议系统的覆盖率和新颖性下降。在这篇论文中，我们提议在矩阵分解基础的建议系统学习过程中添加一个新的参数，即不可靠度，以控制决策时的风险水平。实验结果表明，不可靠度不仅允许风险调节，还可以提高建议系统提供的预测量和质量。Note: "recklessness" is a term used in the original text, and it is not a word commonly used in Chinese. I translated it as "不可靠度" (bù kě yào dù), which means "unreliability" or "riskiness".
</details></li>
</ul>
<hr>
<h2 id="Seasonality-Based-Reranking-of-E-commerce-Autocomplete-Using-Natural-Language-Queries"><a href="#Seasonality-Based-Reranking-of-E-commerce-Autocomplete-Using-Natural-Language-Queries" class="headerlink" title="Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries"></a>Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02055">http://arxiv.org/abs/2308.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Verma, Shan Zhong, Xiaoyu Liu, Adithya Rajan</li>
<li>for: 提高搜索引擎的搜寻框架中的自动完成功能，使其能够适应季节性变化。</li>
<li>methods: 使用神经网络基本概念的自然语言处理算法，将季节性变化纳入搜寻框架中。</li>
<li>results: 提出一个终端评估模型，可以将季节性变化纳入搜寻框架中，提高自动完成的相关性和商业指标。<details>
<summary>Abstract</summary>
Query autocomplete (QAC) also known as typeahead, suggests list of complete queries as user types prefix in the search box. It is one of the key features of modern search engines specially in e-commerce. One of the goals of typeahead is to suggest relevant queries to users which are seasonally important. In this paper we propose a neural network based natural language processing (NLP) algorithm to incorporate seasonality as a signal and present end to end evaluation of the QAC ranking model. Incorporating seasonality into autocomplete ranking model can improve autocomplete relevance and business metric.
</details>
<details>
<summary>摘要</summary>
查询自动完成（QAC）也称为键盘提示，是现代搜索引擎中的一个重要功能，尤其在电商领域。QAC的一个目标是为用户提供相关的查询，以便在搜索框中输入搜索。在这篇论文中，我们提出了基于人工神经网络的自然语言处理（NLP）算法，以 incorporate 季节性作为信号，并进行了端到端评估QAC排名模型。在推入季节性到搜索框中的排名模型中，可以提高搜索结果的相关性和业务指标。
</details></li>
</ul>
<hr>
<h2 id="Robust-Independence-Tests-with-Finite-Sample-Guarantees-for-Synchronous-Stochastic-Linear-Systems"><a href="#Robust-Independence-Tests-with-Finite-Sample-Guarantees-for-Synchronous-Stochastic-Linear-Systems" class="headerlink" title="Robust Independence Tests with Finite Sample Guarantees for Synchronous Stochastic Linear Systems"></a>Robust Independence Tests with Finite Sample Guarantees for Synchronous Stochastic Linear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02054">http://arxiv.org/abs/2308.02054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ambrus Tamás, Dániel Ágoston Bálint, Balázs Csanád Csáji</li>
<li>for: 这个论文是为了开发一种robust independence测试方法，可以 guaranteesignificance levels不受偏移的影响。</li>
<li>methods: 这个方法使用了信任区间估计和 permutation tests，以及一些总体依赖度测量方法，如希尔伯特-Ш密特独立性标准和距离协方差。</li>
<li>results: 这个方法可以检测非线性依赖关系，并且可以在各种不同的噪声模型下进行测试。我们还证明了这个假设测试方法的一致性下一些轻微的假设。<details>
<summary>Abstract</summary>
The paper introduces robust independence tests with non-asymptotically guaranteed significance levels for stochastic linear time-invariant systems, assuming that the observed outputs are synchronous, which means that the systems are driven by jointly i.i.d. noises. Our method provides bounds for the type I error probabilities that are distribution-free, i.e., the innovations can have arbitrary distributions. The algorithm combines confidence region estimates with permutation tests and general dependence measures, such as the Hilbert-Schmidt independence criterion and the distance covariance, to detect any nonlinear dependence between the observed systems. We also prove the consistency of our hypothesis tests under mild assumptions and demonstrate the ideas through the example of autoregressive systems.
</details>
<details>
<summary>摘要</summary>
文章介绍了一种robust独立性测试方法，可以 garantuee非对称性水平，对于随机线性时间不变系统。我们假设观测输出是同步的，即系统被共同的随机噪声驱动。我们的方法提供了不对归一化的类型I错误概率 bound，即噪声可以有任何分布。我们的算法结合信任区间估计与排序测试，以及通用的依赖度度量，如希尔伯特-尚瑟独立性 критерион和距离协方差，来检测观测系统中的非线性依赖关系。我们还证明了我们的假设检测下的假设是正确的，并通过拓扑系统的示例进行了证明。
</details></li>
</ul>
<hr>
<h2 id="A-Graphical-Approach-to-Document-Layout-Analysis"><a href="#A-Graphical-Approach-to-Document-Layout-Analysis" class="headerlink" title="A Graphical Approach to Document Layout Analysis"></a>A Graphical Approach to Document Layout Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02051">http://arxiv.org/abs/2308.02051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jilin Wang, Michael Krumdick, Baojia Tong, Hamima Halim, Maxim Sokolov, Vadym Barda, Delphine Vendryes, Chris Tanner</li>
<li>for: This paper focuses on document layout analysis (DLA) and proposes a lightweight graph neural network called GLAM to improve the efficiency of DLA models.</li>
<li>methods: The GLAM model represents each PDF page as a structured graph and frames the DLA problem as a graph segmentation and classification problem.</li>
<li>results: The GLAM model achieves competitive performance with state-of-the-art (SOTA) models on two challenging DLA datasets, with an order of magnitude fewer parameters. A simple ensemble of GLAM and a leading computer vision-based model achieves a new state-of-the-art on DocLayNet, with an increase in mean average precision (mAP) from 76.8 to 80.8.<details>
<summary>Abstract</summary>
Document layout analysis (DLA) is the task of detecting the distinct, semantic content within a document and correctly classifying these items into an appropriate category (e.g., text, title, figure). DLA pipelines enable users to convert documents into structured machine-readable formats that can then be used for many useful downstream tasks. Most existing state-of-the-art (SOTA) DLA models represent documents as images, discarding the rich metadata available in electronically generated PDFs. Directly leveraging this metadata, we represent each PDF page as a structured graph and frame the DLA problem as a graph segmentation and classification problem. We introduce the Graph-based Layout Analysis Model (GLAM), a lightweight graph neural network competitive with SOTA models on two challenging DLA datasets - while being an order of magnitude smaller than existing models. In particular, the 4-million parameter GLAM model outperforms the leading 140M+ parameter computer vision-based model on 5 of the 11 classes on the DocLayNet dataset. A simple ensemble of these two models achieves a new state-of-the-art on DocLayNet, increasing mAP from 76.8 to 80.8. Overall, GLAM is over 5 times more efficient than SOTA models, making GLAM a favorable engineering choice for DLA tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SMARLA-A-Safety-Monitoring-Approach-for-Deep-Reinforcement-Learning-Agents"><a href="#SMARLA-A-Safety-Monitoring-Approach-for-Deep-Reinforcement-Learning-Agents" class="headerlink" title="SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents"></a>SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02594">http://arxiv.org/abs/2308.02594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Zolfagharian, Manel Abdellatif, Lionel C. Briand, Ramesh S</li>
<li>for: 这篇论文旨在提出一种基于机器学习的安全监测方法，用于保障深度优化学习（DRL）Agent的安全性。</li>
<li>methods: 该方法基于黑盒（不需要访问代理的内部），利用状态抽象减少状态空间，从而使得学习安全违反预测模型的可能性更高。</li>
<li>results: 验证结果表明，SMARLA可以准确预测安全违反，false positive率低，可以在代理执行前半部分预测安全违反。<details>
<summary>Abstract</summary>
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before violations occur.
</details>
<details>
<summary>摘要</summary>
深度强化学习算法（DRL）在安全关键系统中日益被使用。保证DRL代理的安全是这些上下文中的关键问题。然而，仅仅通过测试不能保证安全，因为它不提供保证。建立安全监控器是一个解决方案，以降低这个挑战。这篇论文提出了基于机器学习的安全监控方法SMARLA，专门为DRL代理设计。由于实际原因，SMARLA采用黑盒设计（不需要代理的内部访问权限），并利用状态抽象来减少状态空间，从而使得学习代理违规预测模型从代理的状态中更加容易。我们对两个常见RL案例进行了验证。实验分析表明，SMARLA可以准确预测违规行为，false positive率较低，能够在代理执行前一半预测违规行为。
</details></li>
</ul>
<hr>
<h2 id="FuNToM-Functional-Modeling-of-RF-Circuits-Using-a-Neural-Network-Assisted-Two-Port-Analysis-Method"><a href="#FuNToM-Functional-Modeling-of-RF-Circuits-Using-a-Neural-Network-Assisted-Two-Port-Analysis-Method" class="headerlink" title="FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method"></a>FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02050">http://arxiv.org/abs/2308.02050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morteza Fayazi, Morteza Tavakoli Taba, Amirata Tabatabavakili, Ehsan Afshari, Ronald Dreslinski<br>for:这个论文主要目的是提出一种功能模型化方法，以提高RLC电路的自动化设计效率。methods:该方法使用人工智能技术，并利用两个Port分析方法，可以模型多种架构，并且仅需要一个主要数据集和多个小数据集。results:该方法可以与现有方法匹配精度，但需要训练数据的数量则被降低了2.8倍至10.9倍，并且在后期设计阶段的训练集收集时间则被降低了176.8倍至188.6倍。<details>
<summary>Abstract</summary>
Automatic synthesis of analog and Radio Frequency (RF) circuits is a trending approach that requires an efficient circuit modeling method. This is due to the expensive cost of running a large number of simulations at each synthesis cycle. Artificial intelligence methods are promising approaches for circuit modeling due to their speed and relative accuracy. However, existing approaches require a large amount of training data, which is still collected using simulation runs. In addition, such approaches collect a whole separate dataset for each circuit topology even if a single element is added or removed. These matters are only exacerbated by the need for post-layout modeling simulations, which take even longer. To alleviate these drawbacks, in this paper, we present FuNToM, a functional modeling method for RF circuits. FuNToM leverages the two-port analysis method for modeling multiple topologies using a single main dataset and multiple small datasets. It also leverages neural networks which have shown promising results in predicting the behavior of circuits. Our results show that for multiple RF circuits, in comparison to the state-of-the-art works, while maintaining the same accuracy, the required training data is reduced by 2.8x - 10.9x. In addition, FuNToM needs 176.8x - 188.6x less time for collecting the training set in post-layout modeling.
</details>
<details>
<summary>摘要</summary>
《自动化分析和设计 analog和 radio frequency（RF）电路的方法是一种流行的趋势，因为在每一个合理化周期中运行大量的 simulate 实际上是昂贵的。人工智能方法是电路模型的承诺之一，因为它们具有速度和相对准确性。然而，现有的方法需要大量的训练数据，这些数据通常通过 simulate 实际来采集。此外，这些方法每个电路结构都需要采集一个分开的数据集，即使只是添加或删除一个元素。这些问题由 Layout 模拟所加剧，它们需要更长的时间。为了解决这些问题，我们在这篇论文中提出了 FuNToM，一种功能模型方法 для RF 电路。FuNToM 利用了两个端口分析方法，可以模型多种 topology 使用单个主数据集和多个小数据集。它还利用了人工神经网络，这些神经网络在预测电路行为方面表现出色。我们的结果表明，对多个 RF 电路，相比之前的状态艺术作品，在保持同样的准确性下，需要的训练数据被减少了 2.8x - 10.9x。此外，FuNToM 在 post-layout 模拟中收集训练集的时间需要 176.8x - 188.6x  menos。
</details></li>
</ul>
<hr>
<h2 id="Deep-Maxout-Network-based-Feature-Fusion-and-Political-Tangent-Search-Optimizer-enabled-Transfer-Learning-for-Thalassemia-Detection"><a href="#Deep-Maxout-Network-based-Feature-Fusion-and-Political-Tangent-Search-Optimizer-enabled-Transfer-Learning-for-Thalassemia-Detection" class="headerlink" title="Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection"></a>Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02029">http://arxiv.org/abs/2308.02029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hemn Barzan Abdalla, Awder Ahmed, Guoquan Li, Nasser Mustafa, Abdur Rashid Sangi</li>
<li>for: 本研究旨在探讨一种基于政治向量搜索优化的深度学习方法（PTSO_TL）用于抑制遗传贫血病诊断。</li>
<li>methods: 本研究使用的方法包括数据normalization、特征融合、数据增强和深度学习模型。</li>
<li>results: 根据实验结果，PTSO_TL方法在识别遗传贫血病方面达到了最高的精度（94.3%）、回归率（96.1%）和相关度（95.2%）。<details>
<summary>Abstract</summary>
Thalassemia is a heritable blood disorder which is the outcome of a genetic defect causing lack of production of hemoglobin polypeptide chains. However, there is less understanding of the precise frequency as well as sharing in these areas. Knowing about the frequency of thalassemia occurrence and dependable mutations is thus a significant step in preventing, controlling, and treatment planning. Here, Political Tangent Search Optimizer based Transfer Learning (PTSO_TL) is introduced for thalassemia detection. Initially, input data obtained from a particular dataset is normalized in the data normalization stage. Quantile normalization is utilized in the data normalization stage, and the data are then passed to the feature fusion phase, in which Weighted Euclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data augmentation is performed using the oversampling method to increase data dimensionality. Lastly, thalassemia detection is carried out by TL, wherein a convolutional neural network (CNN) is utilized with hyperparameters from a trained model such as Xception. TL is tuned by PTSO, and the training algorithm PTSO is presented by merging of Political Optimizer (PO) and Tangent Search Algorithm (TSA). Furthermore, PTSO_TL obtained maximal precision, recall, and f-measure values of about 94.3%, 96.1%, and 95.2%, respectively.
</details>
<details>
<summary>摘要</summary>
贝壳血症是一种遗传血液疾病，由于遗传错误导致血液中不够生成含铁蛋白链。然而，贝壳血症的具体发生频率以及传递的精准性仍未得到充分理解。了解贝壳血症发生频率和可靠的突变是一项重要的步骤，以便预防、控制和治疗规划。在这里，我们引入政治弧搜索优化器基于传输学习（PTSO_TL）以检测贝壳血症。首先，输入数据从特定数据集被normalized，并使用量谱normalization进行数据归一化。然后，数据被传递到特征融合阶段，在这里使用Weighted Euclidean Distance with Deep Maxout Network（DMN）。接着，数据进行了增强处理，使用扩充方法增加数据维度。最后，贝壳血症检测由TL进行，其中使用一个具有训练模型的 convolutional neural network（CNN），并将 hyperparameters 从已训练模型 such as Xception。TL 被PTSO 调整，并且PTSO 是由政治优化器（PO）和 Tangent Search Algorithm（TSA）的 merge 所presentation。此外，PTSO_TL 在评价指标中获得了最高的准确率、回归率和准确度值，它们分别为 approximately 94.3%, 96.1%, and 95.2%。
</details></li>
</ul>
<hr>
<h2 id="Federated-Representation-Learning-for-Automatic-Speech-Recognition"><a href="#Federated-Representation-Learning-for-Automatic-Speech-Recognition" class="headerlink" title="Federated Representation Learning for Automatic Speech Recognition"></a>Federated Representation Learning for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02013">http://arxiv.org/abs/2308.02013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guruprasad V Ramesh, Gopinath Chennupati, Milind Rao, Anit Kumar Sahu, Ariya Rastrow, Jasha Droppo</li>
<li>for: 这篇论文是为了探讨 Federated Learning (FL) 和 Self-supervised Learning (SSL) 的结合，以学习 Automatic Speech Recognition (ASR) 模型，保持数据隐私。</li>
<li>methods: 这篇论文使用了 Libri-Light 语音 dataset，使用了 Speaker 和 Chapter 信息来模拟非Identical Independent Distributions (non-IID) 的数据分布，采用了 Contrastive Predictive Coding 框架和 FedSGD 进行训练。</li>
<li>results: 研究发现，使用 Federated Learning 预训练 ASR 模型，可以达到中心预训练模型的性能水平，并且在新语言 French 中进行适应，可以提高 WER 表达误差率 by 20%。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data. Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations. In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints. We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD. We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training. We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种隐私保护的方法论，允许边缘设备共同学习无需分享数据。边缘设备如 Alexa 和 Siri 是可能的无标语音数据的来源，可以用于学习 Robust 语音表示。在这项工作中，我们将 Self-supervised Learning (SSL) 和 FL 结合来学习 Automatic Speech Recognition (ASR) 的表示，尊重数据隐私约束。我们使用 Libri-Light 无标语音集中的 Speaker 和章节信息来模拟非Identical Independent Distribution (IID) 的Speaker-siloed 数据分布，并在 FedSGD 框架下预训练一个 LSTM 编码器。我们显示预训练的 ASR 编码器在 FL 中表现与中央预训练模型一样好，并且对无预训练情况下提高了12-15% (WER)。我们进一步适应了联邦预训练模型到一种新语言法语，并显示对无预训练情况下提高了20% (WER)。
</details></li>
</ul>
<hr>
<h2 id="Memory-capacity-of-two-layer-neural-networks-with-smooth-activations"><a href="#Memory-capacity-of-two-layer-neural-networks-with-smooth-activations" class="headerlink" title="Memory capacity of two layer neural networks with smooth activations"></a>Memory capacity of two layer neural networks with smooth activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02001">http://arxiv.org/abs/2308.02001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Madden, Christos Thrampoulidis</li>
<li>for: 这篇论文探讨了两层神经网络的内存容量，即最大化一般数据集的网络大小。</li>
<li>methods: 作者使用了非多项式实数Activation函数，如sigmoid和smoothed ReLU，并使用Jacobian的秩来分析网络的内存容量。</li>
<li>results: 作者发现，对于非多项式实数Activation函数，网络的内存容量至少为md&#x2F;2，并且可以达到约2倍的优化。这些结果比前一些研究更加广泛，并且可以推广到更深的模型和其他架构。<details>
<summary>Abstract</summary>
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for extending to deeper models and other architectures.
</details>
<details>
<summary>摘要</summary>
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters) is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for extending to deeper models and other architectures.Here's the translation in Traditional Chinese:决定两层神经网络中隐藏层 neuron 数目为 m，输入维度为 d（即 md+m 总可训练参数）的记忆容量是机器学习中的基本问题。对于非多项演算 activation functions，例如 sigmoid 和 smoothed rectified linear units (smoothed ReLUs)，我们设置了 md/2 的下界和约2的优化因子。这些结果与 preceded 的 results 相似，但是过去的结果仅适用于 Heaviside 和 ReLU 激活函数，而且这些激活函数的结果受到了 logarithmic 因子的影响，并且需要随机数据。从构成记忆容量的角度来看，我们查看了神经网络的雅可比安的排名，通过计算包含 Hadamard powers 和 Khati-Rao 产品的矩阵的排名。我们的计算扩展了 класиical 的线性代数实验，关于 Hadamard powers 的排名。整体而言，我们的方法与之前的工作不同，并且保持可以扩展到更深的模型和其他架构。
</details></li>
</ul>
<hr>
<h2 id="On-the-Transition-from-Neural-Representation-to-Symbolic-Knowledge"><a href="#On-the-Transition-from-Neural-Representation-to-Symbolic-Knowledge" class="headerlink" title="On the Transition from Neural Representation to Symbolic Knowledge"></a>On the Transition from Neural Representation to Symbolic Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02000">http://arxiv.org/abs/2308.02000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyan Cheng, Peter Chin</li>
<li>for: 本研究旨在bridge neural和Symbolic Representation之间的巨大差距，以便将Symbolic Thinking incorporated into neural networks的核心。</li>
<li>methods: 我们提出了一个Neural-Symbolic Transitional Dictionary Learning（TDL）框架，使用EM算法学习数据的转换表示，压缩输入数据的高维信息到一组tensor作为神经变量，自然地发现数据中隐藏的 predicate 结构。我们在 diffusion model 中对输入的分解视为合作游戏，然后通过prototype clustering来学习预测。此外，我们还使用RLEnabled by diffusion models来进一步调整学习的got prototype。</li>
<li>results: 我们在3个抽象compositional visual objects dataset上进行了广泛的实验，这些dataset需要模型可以对输入进行部分 segmentation，不含任何视觉特征，例如 texture、颜色或阴影。我们的learned representation可以带来可 interpret的 decompositions of visual input，并且在下游任务中进行了smooth的适应。这些下游任务包括神经&#x2F;Symbolic downstream tasks。<details>
<summary>Abstract</summary>
Bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. Motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. We propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. We implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. We additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. Extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.
</details>
<details>
<summary>摘要</summary>
bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. we propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. we implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. we additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.Here's a word-for-word translation of the text into Simplified Chinese:bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. we propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. we implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. we additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.
</details></li>
</ul>
<hr>
<h2 id="Explainable-unsupervised-multi-modal-image-registration-using-deep-networks"><a href="#Explainable-unsupervised-multi-modal-image-registration-using-deep-networks" class="headerlink" title="Explainable unsupervised multi-modal image registration using deep networks"></a>Explainable unsupervised multi-modal image registration using deep networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01994">http://arxiv.org/abs/2308.01994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjia Wang, Giorgos Papanastasiou</li>
<li>for: 这个论文是用于描述一种基于深度学习的多模态MRI图像匹配方法，用于临床决策。</li>
<li>methods: 该方法使用了多种MRI序列（定义为’模态’），并使用了 Grad-CAM基于解释框架来解释模型和数据之间的关系。</li>
<li>results: 该研究表明，通过 incorporating Grad-CAM解释框架，该方法可以实现高性能和可解释的多模态MRI图像匹配。<details>
<summary>Abstract</summary>
Clinical decision making from magnetic resonance imaging (MRI) combines complementary information from multiple MRI sequences (defined as 'modalities'). MRI image registration aims to geometrically 'pair' diagnoses from different modalities, time points and slices. Both intra- and inter-modality MRI registration are essential components in clinical MRI settings. Further, an MRI image processing pipeline that can address both afine and non-rigid registration is critical, as both types of deformations may be occuring in real MRI data scenarios. Unlike image classification, explainability is not commonly addressed in image registration deep learning (DL) methods, as it is challenging to interpet model-data behaviours against transformation fields. To properly address this, we incorporate Grad-CAM-based explainability frameworks in each major component of our unsupervised multi-modal and multi-organ image registration DL methodology. We previously demonstrated that we were able to reach superior performance (against the current standard Syn method). In this work, we show that our DL model becomes fully explainable, setting the framework to generalise our approach on further medical imaging data.
</details>
<details>
<summary>摘要</summary>
临床决策从核磁共振成像（MRI）结合多种MRI序列（定义为“模态”）的信息。MRI图像匹配目标是在不同模态、时间点和切片之间进行几何匹配诊断。Intra-和inter-模态MRI匹配都是临床MRI设置中的重要组件。此外，一个能够处理both afine和non-rigid匹配的MRI图像处理管道是关键，因为这两种类型的变形都可能发生在实际MRI数据场景中。不同于图像分类，explainability不是通常在图像匹配深度学习（DL）方法中被考虑的，因为它是困难 interpret模型-数据行为对于转换场景。为了正确地Address这个问题，我们在每个主要组件中都 incorporate Grad-CAM基于的解释框架。在我们之前的研究中，我们已经能够达到superior performance（相比于当前标准Syn方法）。在这项工作中，我们显示了我们的DL模型已经变得完全可解释，设置了框架可以通过更多的医疗影像数据进行普适化。
</details></li>
</ul>
<hr>
<h2 id="CartiMorph-a-framework-for-automated-knee-articular-cartilage-morphometrics"><a href="#CartiMorph-a-framework-for-automated-knee-articular-cartilage-morphometrics" class="headerlink" title="CartiMorph: a framework for automated knee articular cartilage morphometrics"></a>CartiMorph: a framework for automated knee articular cartilage morphometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01981">http://arxiv.org/abs/2308.01981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yongchengyao/cartimorph">https://github.com/yongchengyao/cartimorph</a></li>
<li>paper_authors: Yongcheng Yao, Junru Zhong, Liping Zhang, Sheheryar Khan, Weitian Chen</li>
<li>for: 这个研究的目的是发展一个准确地量化膝盖韧带组织的自动化方法，以便发现膝盖韧带组织的问题。</li>
<li>methods: 这个研究使用了深度学习模型来表现图像特征，并使用了标本建立和图像注册等方法来自动化膝盖韧带组织的量化。</li>
<li>results: 这个研究获得了膝盖韧带组织的量化结果，包括全厚度膝盖韧带损伤率（FCL）、平均厚度、表面积和体积等多个量化指标。这些量化结果显示了膝盖韧带组织的问题，并且与手动量化结果之间存在强相关。<details>
<summary>Abstract</summary>
We introduce CartiMorph, a framework for automated knee articular cartilage morphometrics. It takes an image as input and generates quantitative metrics for cartilage subregions, including the percentage of full-thickness cartilage loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the power of deep learning models for hierarchical image feature representation. Deep learning models were trained and validated for tissue segmentation, template construction, and template-to-image registration. We established methods for surface-normal-based cartilage thickness mapping, FCL estimation, and rule-based cartilage parcellation. Our cartilage thickness map showed less error in thin and peripheral regions. We evaluated the effectiveness of the adopted segmentation model by comparing the quantitative metrics obtained from model segmentation and those from manual segmentation. The root-mean-squared deviation of the FCL measurements was less than 8%, and strong correlations were observed for the mean thickness (Pearson's correlation coefficient $\rho \in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in [0.89,0.98]$) measurements. We compared our FCL measurements with those from a previous study and found that our measurements deviated less from the ground truths. We observed superior performance of the proposed rule-based cartilage parcellation method compared with the atlas-based approach. CartiMorph has the potential to promote imaging biomarkers discovery for knee osteoarthritis.
</details>
<details>
<summary>摘要</summary>
我们介绍CartiMorph，一个框架用于自动诊断膝关节软骨质量量表。它可以从图像中提取量表膝关节软骨质量量表，包括软骨质量量表的全厚度损伤率（FCL）、平均厚度、表面积和体积。CartiMorph利用深度学习模型来实现层次图像特征表示。我们在识别、构建模板和模板与图像匹配中使用深度学习模型。我们实现了基于表面法向的软骨厚度映射、FCL估计和规则基于的软骨分割。我们的软骨厚度图表示在薄和边缘区域中具有较低的错误。我们通过比较我们采用的分 segmentation模型与手动分 segmentation结果所得到的量表metric来评估模型的效果。我们发现root-mean-squared deviation of FCL measurements是less than 8%，并且在mean thickness、surface area和volume measurement中observation了强相关性（Pearson's correlation coefficient $\rho \in [0.82,0.97]$、[0.82,0.98]$和[0.89,0.98]$）。我们对我们的FCL测量与之前的研究中的参照值进行比较，发现我们的测量偏差较少。我们发现了规则基于的软骨分割方法的优越性，比Atlas-based方法更好。CartiMorph具有推动膝关节风湿病影像生物标志物的潜力。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Parkinson’s-Disease-with-Smile-An-AI-enabled-Screening-Framework"><a href="#Unmasking-Parkinson’s-Disease-with-Smile-An-AI-enabled-Screening-Framework" class="headerlink" title="Unmasking Parkinson’s Disease with Smile: An AI-enabled Screening Framework"></a>Unmasking Parkinson’s Disease with Smile: An AI-enabled Screening Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02588">http://arxiv.org/abs/2308.02588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq Adnan, Md Saiful Islam, Wasifur Rahman, Sangwu Lee, Sutapa Dey Tithi, Kazi Noshin, Imran Sarker, M Saifur Rahman, Ehsan Hoque</li>
<li>for: 预测帕金森病（PD）的诊断具有挑战性，因为没有可靠的生物标志物和有限的临床护理资源。本研究通过分析最大的视频数据集，检测PD的微表情。</li>
<li>methods: 我们使用了人脸特征点和动作单元，提取与低表情相关的特征。我们将这些特征用于一个 ensemble 模型，实现了89.7%的准确率和89.3%的接受分布函数点（AUROC）。</li>
<li>results: 我们发现，只使用笑脸视频中的特征，可以达到相似的性能，甚至在两个外部测试集上，模型没有在训练过程中看到的数据上进行了分类，这表明了PD风险评估可能通过笑脸自拍视频进行。<details>
<summary>Abstract</summary>
Parkinson's disease (PD) diagnosis remains challenging due to lacking a reliable biomarker and limited access to clinical care. In this study, we present an analysis of the largest video dataset containing micro-expressions to screen for PD. We collected 3,871 videos from 1,059 unique participants, including 256 self-reported PD patients. The recordings are from diverse sources encompassing participants' homes across multiple countries, a clinic, and a PD care facility in the US. Leveraging facial landmarks and action units, we extracted features relevant to Hypomimia, a prominent symptom of PD characterized by reduced facial expressions. An ensemble of AI models trained on these features achieved an accuracy of 89.7% and an Area Under the Receiver Operating Characteristic (AUROC) of 89.3% while being free from detectable bias across population subgroups based on sex and ethnicity on held-out data. Further analysis reveals that features from the smiling videos alone lead to comparable performance, even on two external test sets the model has never seen during training, suggesting the potential for PD risk assessment from smiling selfie videos.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Domain-specificity-and-data-efficiency-in-typo-tolerant-spell-checkers-the-case-of-search-in-online-marketplaces"><a href="#Domain-specificity-and-data-efficiency-in-typo-tolerant-spell-checkers-the-case-of-search-in-online-marketplaces" class="headerlink" title="Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces"></a>Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01976">http://arxiv.org/abs/2308.01976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayananda Ubrangala, Juhi Sharma, Ravi Prasad Kondapalli, Kiran R, Amit Agarwala, Laurent Boué</li>
<li>for: 提高在线市场场所上的拼写错误检测精度</li>
<li>methods: 使用数据扩充方法生成域限定特定的隐藏表示，并使用回归神经网络进行训练</li>
<li>results: 实现了在实时推荐API中的 typo 检测，提高了搜索效果<details>
<summary>Abstract</summary>
Typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.
</details>
<details>
<summary>摘要</summary>
typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool, especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Synthesising-Rare-Cataract-Surgery-Samples-with-Guided-Diffusion-Models"><a href="#Synthesising-Rare-Cataract-Surgery-Samples-with-Guided-Diffusion-Models" class="headerlink" title="Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models"></a>Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02587">http://arxiv.org/abs/2308.02587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meclabtuda/catasynth">https://github.com/meclabtuda/catasynth</a></li>
<li>paper_authors: Yannik Frisch, Moritz Fuchs, Antoine Sanner, Felix Anton Ucar, Marius Frenzel, Joana Wasielica-Poslednik, Adrian Gericke, Felix Mathias Wagner, Thomas Dratsch, Anirban Mukhopadhyay</li>
<li>for: 提高Automated Cataract Surgery Assistance System的发展，提供可靠的人工合成数据。</li>
<li>methods: 使用Denosing Diffusion Implicit Models（DDIM）和Classifier-Free Guidance（CFG）Conditional Generative Model Synthesize complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools.</li>
<li>results: 通过生成不同、高质量的示例，提高downstream工具分类器的性能，最高提高10%。<details>
<summary>Abstract</summary>
Cataract surgery is a frequently performed procedure that demands automation and advanced assistance systems. However, gathering and annotating data for training such systems is resource intensive. The publicly available data also comprises severe imbalances inherent to the surgical process. Motivated by this, we analyse cataract surgery video data for the worst-performing phases of a pre-trained downstream tool classifier. The analysis demonstrates that imbalances deteriorate the classifier's performance on underrepresented cases. To address this challenge, we utilise a conditional generative model based on Denoising Diffusion Implicit Models (DDIM) and Classifier-Free Guidance (CFG). Our model can synthesise diverse, high-quality examples based on complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools. We affirm that the synthesised samples display tools that the classifier recognises. These samples are hard to differentiate from real images, even for clinical experts with more than five years of experience. Further, our synthetically extended data can improve the data sparsity problem for the downstream task of tool classification. The evaluations demonstrate that the model can generate valuable unseen examples, allowing the tool classifier to improve by up to 10% for rare cases. Overall, our approach can facilitate the development of automated assistance systems for cataract surgery by providing a reliable source of realistic synthetic data, which we make available for everyone.
</details>
<details>
<summary>摘要</summary>
喉痒手术是一种常见的手术过程，需要自动化和高级帮助系统。然而，收集和标注数据 для训练这些系统是资源占用的。公共可用数据也包含了手术过程中的严重偏见。为了解决这个挑战，我们分析了喉痒手术视频数据，找到最差表现的阶段。分析结果表明，偏见会使下游工具分类器的表现在不足表现的案例下下降。为了解决这个问题，我们使用基于减噪扩散模型（DDIM）和无类标注指南（CFG）的 conditional generative model。我们的模型可以生成多样化、高质量的示例，基于复杂的多类多标签条件，如手术阶段和手术工具的组合。我们证明了生成的样本中的工具，可以由分类器识别。这些样本与真实图像很难分辨，甚至对有 более чем五年的临床经验的专业人员来说。此外，我们通过增加的数据可以改善下游任务中的数据稀缺问题。评估结果表明，我们的模型可以生成有价值的未看到的示例，使工具分类器提高至10%。总的来说，我们的方法可以促进喉痒手术自动化的发展，提供一个可靠的真实Synthetic数据源，我们将其公开给 everyone。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Agent-Policy-with-Externalities-Reward-Design-via-Bilevel-RL"><a href="#Aligning-Agent-Policy-with-Externalities-Reward-Design-via-Bilevel-RL" class="headerlink" title="Aligning Agent Policy with Externalities: Reward Design via Bilevel RL"></a>Aligning Agent Policy with Externalities: Reward Design via Bilevel RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02585">http://arxiv.org/abs/2308.02585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Dinesh Manocha, Huazheng Wang, Furong Huang, Mengdi Wang</li>
<li>for: 本研究旨在批处RL政策优化问题中的奖励函数假设，以及RL政策优化过程中的状态空间覆盖和安全性考虑。</li>
<li>methods: 本研究提出了一种级联优化问题，将主体（principal）定义为系统的更广泛目标和约束，而代理（agent）则解决Markov决策过程（MDP）。</li>
<li>results: 研究提出了主体驱动政策对应性via级联RL（PPA-BRL），该方法可有效地将代理的政策与主体的目标相吻合。研究还证明了PPA-BRL的收敛性，并通过多个示例验证了该方法的优点，包括能效地实现能源充足的操作任务、社会福利基础的税制设计以及成本效益的机器人导航。<details>
<summary>Abstract</summary>
In reinforcement learning (RL), a reward function is often assumed at the outset of a policy optimization procedure. Learning in such a fixed reward paradigm in RL can neglect important policy optimization considerations, such as state space coverage and safety. Moreover, it can fail to encompass broader impacts in terms of social welfare, sustainability, or market stability, potentially leading to undesirable emergent behavior and potentially misaligned policy. To mathematically encapsulate the problem of aligning RL policy optimization with such externalities, we consider a bilevel optimization problem and connect it to a principal-agent framework, where the principal specifies the broader goals and constraints of the system at the upper level and the agent solves a Markov Decision Process (MDP) at the lower level. The upper-level deals with learning a suitable reward parametrization corresponding to the broader goals and the lower-level deals with learning the policy for the agent. We propose Principal driven Policy Alignment via Bilevel RL (PPA-BRL), which efficiently aligns the policy of the agent with the principal's goals. We explicitly analyzed the dependence of the principal's trajectory on the lower-level policy, prove the convergence of PPA-BRL to the stationary point of the problem. We illuminate the merits of this framework in view of alignment with several examples spanning energy-efficient manipulation tasks, social welfare-based tax design, and cost-effective robotic navigation.
</details>
<details>
<summary>摘要</summary>
在增强学习（RL）中，常常假设一个奖金函数，用于policy优化过程的开始。这种固定奖金的假设可能忽略了重要的策略优化考虑因素，如状态空间覆盖率和安全性。此外，它可能无法涵盖更广泛的影响，如社会福利、可持续发展和市场稳定性，可能导致不жела的潜在行为和不一致策略。为了数学地表述RL策略优化与外部影响的问题，我们考虑了一个双层优化问题，并将其连接到一个主体-代理模型，其中主体规定系统的更广泛目标和约束，而代理在下层解决一个Markov决策过程（MDP）。上层学习一个适当的奖金参数化，与下层学习代理的策略。我们提出了主体驱动策略对齐（PPA-BRL），它高效地将代理的策略与主体的目标相对应。我们证明了PPA-BRL在站点点问题中的收敛性。我们通过一些示例，如能效的机器人 Navigation，社会福利基于税制的设计，以及成本效果的机器人 Navigation， illustrate the advantages of this framework。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-in-Large-Language-Models-Through-Symbolic-Math-Word-Problems"><a href="#Reasoning-in-Large-Language-Models-Through-Symbolic-Math-Word-Problems" class="headerlink" title="Reasoning in Large Language Models Through Symbolic Math Word Problems"></a>Reasoning in Large Language Models Through Symbolic Math Word Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01906">http://arxiv.org/abs/2308.01906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vedant Gaur, Nikunj Saunshi</li>
<li>for: 这篇论文探讨了自然语言处理（NLP）领域中大语言模型（LLM）的理解能力。</li>
<li>methods: 该论文使用了符号版本的数学Word问题（MWP）来研究LLM的理解能力，并创建了一个符号版本的SVAMP数据集。</li>
<li>results: 研究发现，使用自我提示approach可以使LLM的符号理解更加准确，并且自动提取出符号答案和数学答案之间的对应关系，从而使LLM的理解更加明确。此外，自我提示还能够提高符号准确率，超过 numeric 和 symbolic 准确率，从而实现了一种ensemble效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a "concise explanation" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be released for future research on symbolic math problems.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经革命化NLG，解决了少量或无标签数据下的下游任务。 despite their 多元能力，大型问题的理解仍然不够了解。 本文研究 math word problems（MWPs）的推理，通过研究 symbolic versions of the numeric problems，因为一个 symbolic expression 是一个 "简洁解释" 的 numeric answer。 我们创建了一个 symbolic version of the SVAMP dataset，并发现 GPT-3 的 davinci-002 模型在 symbolic MWPs 上也有良好的 zero-shot accuracy。 为了评估模型的 faithfulness，我们不仅评估了模型的准确性，还进一步评估了模型输出的推理与答案的对齐度，这与 numeric 和 symbolic 答案对应。 我们还探索了自我提示的方法，以便将 symbolic reasoning 与 numeric answer 相互适应，从而让 LLM 具备提供简洁且可靠的推理，并使其更易理解。  surprisingly，自我提示也使 symbolic 准确性高于 numeric 和 symbolic 准确性，提供了一个 ensemble 效果。 我们将 SVAMP_Sym dataset 发布给未来的研究人员对于符号数学问题进行研究。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Deformable-Convolution-for-Depth-Completion"><a href="#Revisiting-Deformable-Convolution-for-Depth-Completion" class="headerlink" title="Revisiting Deformable Convolution for Depth Completion"></a>Revisiting Deformable Convolution for Depth Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01905">http://arxiv.org/abs/2308.01905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinglong Sun, Jean Ponce, Yu-Xiong Wang</li>
<li>for: 这篇论文旨在提高深度地图的质量，具体来说是从粗糙的深度地图中生成高质量的稠密深度地图。</li>
<li>methods: 该论文提出了一种使用可变核函数卷积来单 passes地进行改进，从而解决了迭代循环的缺点，并且通过系统地调查了多种表现方法，以更好地理解可变核函数的作用和如何利用其进行深度 completion。</li>
<li>results: 研究人员通过对大规模的 KITTI 数据集进行评估，发现他们的模型在准确率和执行速度两个方面均达到了领先水平。<details>
<summary>Abstract</summary>
Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years. Previous work usually employs RGB images as guidance, and introduces iterative spatial propagation to refine estimated coarse depth maps. However, most of the propagation refinement methods require several iterations and suffer from a fixed receptive field, which may contain irrelevant and useless information with very sparse input. In this paper, we address these two challenges simultaneously by revisiting the idea of deformable convolution. We propose an effective architecture that leverages deformable kernel convolution as a single-pass refinement module, and empirically demonstrate its superiority. To better understand the function of deformable convolution and exploit it for depth completion, we further systematically investigate a variety of representative strategies. Our study reveals that, different from prior work, deformable convolution needs to be applied on an estimated depth map with a relatively high density for better performance. We evaluate our model on the large-scale KITTI dataset and achieve state-of-the-art level performance in both accuracy and inference speed. Our code is available at https://github.com/AlexSunNik/ReDC.
</details>
<details>
<summary>摘要</summary>
深度完成，目标是生成高质量的稠密深度地图从稀疏深度地图中，在最近几年内吸引了越来越多的注意力。先前的工作通常使用RGB图像作为引导，并通过迭代的空间卷积来精细化估计的粗略深度地图。然而，大多数卷积修充方法需要多个迭代和固定的接受范围，可能包含无关和无用的信息，尤其是与稀疏输入相比。在这篇论文中，我们 simultanously解决了这两个挑战，通过再次探讨可变核 convolution的想法。我们提议一种有效的架构，利用可变核 convolution作为单pass精细化模块，并经验证其超越性。为了更好地理解可变核 convolution的功能和利用其进行深度完成，我们进一步系统地调查了一些代表性的策略。我们的研究表明，与先前工作不同，可变核 convolution需要在估计的深度地图中的相对较高的密度来获得更好的性能。我们在大规模的KITTI dataset上评估了我们的模型，并在准确率和推理速度两个指标上达到了当前领域的状态码水平。我们的代码可以在https://github.com/AlexSunNik/ReDC中找到。
</details></li>
</ul>
<hr>
<h2 id="How-many-preprints-have-actually-been-printed-and-why-a-case-study-of-computer-science-preprints-on-arXiv"><a href="#How-many-preprints-have-actually-been-printed-and-why-a-case-study-of-computer-science-preprints-on-arXiv" class="headerlink" title="How many preprints have actually been printed and why: a case study of computer science preprints on arXiv"></a>How many preprints have actually been printed and why: a case study of computer science preprints on arXiv</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01899">http://arxiv.org/abs/2308.01899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialiang Lin, Yao Yu, Yu Zhou, Zhiyang Zhou, Xiaodong Shi</li>
<li>For: This paper aims to quantify the number of preprints that are eventually published in peer-reviewed venues, and to investigate the characteristics of published preprints in the field of computer science.* Methods: The authors use a case study of computer science preprints submitted to arXiv from 2008 to 2017, and employ a semantics-based mapping method using BERT to match preprints with their published versions.* Results: The authors find that 66% of all sampled preprints are published under unchanged titles and 11% are published under different titles and with other modifications. They also identify several characteristics that are associated with published preprints, including adequate revisions, multiple authorship, detailed abstract and introduction, extensive and authoritative references, and available source code.<details>
<summary>Abstract</summary>
Preprints play an increasingly critical role in academic communities. There are many reasons driving researchers to post their manuscripts to preprint servers before formal submission to journals or conferences, but the use of preprints has also sparked considerable controversy, especially surrounding the claim of priority. In this paper, a case study of computer science preprints submitted to arXiv from 2008 to 2017 is conducted to quantify how many preprints have eventually been printed in peer-reviewed venues. Among those published manuscripts, some are published under different titles and without an update to their preprints on arXiv. In the case of these manuscripts, the traditional fuzzy matching method is incapable of mapping the preprint to the final published version. In view of this issue, we introduce a semantics-based mapping method with the employment of Bidirectional Encoder Representations from Transformers (BERT). With this new mapping method and a plurality of data sources, we find that 66% of all sampled preprints are published under unchanged titles and 11% are published under different titles and with other modifications. A further analysis was then performed to investigate why these preprints but not others were accepted for publication. Our comparison reveals that in the field of computer science, published preprints feature adequate revisions, multiple authorship, detailed abstract and introduction, extensive and authoritative references and available source code.
</details>
<details>
<summary>摘要</summary>
Preprints 在学术社区中发挥越来越重要的作用。有很多原因使研究人员将文稿上传到 précis servers 之前，而不是正式提交到期刊或会议，但使用 preprints 也引起了较大的争议，特别是在优先权方面。在这篇论文中，我们对计算机科学 preprints 在 arXiv 上从 2008 年到 2017 年的 submissions 进行了案例研究，以计算这些 manuscripts 最终被 print 在 peer-reviewed venue 中的数量。其中一些已经被 published 的文稿，有些在 preprints 上没有更新，这些 manuscripts  traditional fuzzy matching 方法无法映射 preprints 到最终发表的版本。为解决这个问题，我们引入 semantics-based mapping 方法，使用 Bidirectional Encoder Representations from Transformers (BERT)。与传统方法不同的是，我们使用多种数据源，并发现了以下结果：66% 的样本 preprints 被发表不变的标题，11% 的样本 preprints 被发表并有其他修改。然后，我们进行了进一步的分析，以 investigating 为什么这些 preprints 而不是其他的被accepted  для发表。我们的比较发现，在计算机科学领域中，发表的 preprints 具有充分的修改、多个作者、详细的摘要和引言、详细的参考文献和可用的源代码。
</details></li>
</ul>
<hr>
<h2 id="Improving-Replay-Sample-Selection-and-Storage-for-Less-Forgetting-in-Continual-Learning"><a href="#Improving-Replay-Sample-Selection-and-Storage-for-Less-Forgetting-in-Continual-Learning" class="headerlink" title="Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning"></a>Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01895">http://arxiv.org/abs/2308.01895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Brignac, Niels Lobo, Abhijit Mahalanobis</li>
<li>for: 该研究旨在解决深度学习模型在进行连续学习时免受前任务卷积失忆的问题。</li>
<li>methods: 该研究使用了一种新的比较方法，与常见的储存样本方法进行对比，并提供了一种细致的分析方法来找到最佳储存样本的数量。</li>
<li>results: 该研究结果表明，使用该新的比较方法和细致的分析方法可以更好地选择最有价值的样本进行储存，从而提高连续学习的性能。<details>
<summary>Abstract</summary>
Continual learning seeks to enable deep learners to train on a series of tasks of unknown length without suffering from the catastrophic forgetting of previous tasks. One effective solution is replay, which involves storing few previous experiences in memory and replaying them when learning the current task. However, there is still room for improvement when it comes to selecting the most informative samples for storage and determining the optimal number of samples to be stored. This study aims to address these issues with a novel comparison of the commonly used reservoir sampling to various alternative population strategies and providing a novel detailed analysis of how to find the optimal number of stored samples.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:Continual learning 目标是帮助深度学习者学习一系列任务的长度未知而不受前任务忘记的影响。一种有效的解决方案是 reuse，即将前一些经验存储在内存中，并在学习当前任务时重新播放。然而，还有很多可以提高的空间，包括选择存储的最有用样本和确定存储样本的优化数量。这项研究目标是通过对通用的队列抽样与其他人口策略进行比较，并提供一种新的详细分析，以寻找最佳存储样本的数量。
</details></li>
</ul>
<hr>
<h2 id="Exact-identification-of-nonlinear-dynamical-systems-by-Trimmed-Lasso"><a href="#Exact-identification-of-nonlinear-dynamical-systems-by-Trimmed-Lasso" class="headerlink" title="Exact identification of nonlinear dynamical systems by Trimmed Lasso"></a>Exact identification of nonlinear dynamical systems by Trimmed Lasso</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01891">http://arxiv.org/abs/2308.01891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shawn L. Kiser, Mikhail Guskov, Marc Rébillat, Nicolas Ranc</li>
<li>for: 本研究旨在提出一种可以在实际数据中进行非线性动力系统标定的方法，能够处理有限长度和噪声的实际数据。</li>
<li>methods: 本研究使用了SINDy算法，以及其多种扩展，如E-SINDy和TRIM。这些方法都是基于梯度最小化的方法，但是TRIM方法可以提供更加精准的结果，并且可以在更加严重的噪声和有限数据情况下进行标定。</li>
<li>results: 本研究对三个不同的非线性动力系统进行了实验，结果表明，TRIM方法可以在有限长度和噪声的实际数据中提供更加精准的标定结果，而E-SINDy方法则可能会出现残差。此外，TRIM方法的计算成本与STLS算法相同，可以使用可 convex 的解决方法进行优化。<details>
<summary>Abstract</summary>
Identification of nonlinear dynamical systems has been popularized by sparse identification of the nonlinear dynamics (SINDy) via the sequentially thresholded least squares (STLS) algorithm. Many extensions SINDy have emerged in the literature to deal with experimental data which are finite in length and noisy. Recently, the computationally intensive method of ensembling bootstrapped SINDy models (E-SINDy) was proposed for model identification, handling finite, highly noisy data. While the extensions of SINDy are numerous, their sparsity-promoting estimators occasionally provide sparse approximations of the dynamics as opposed to exact recovery. Furthermore, these estimators suffer under multicollinearity, e.g. the irrepresentable condition for the Lasso. In this paper, we demonstrate that the Trimmed Lasso for robust identification of models (TRIM) can provide exact recovery under more severe noise, finite data, and multicollinearity as opposed to E-SINDy. Additionally, the computational cost of TRIM is asymptotically equal to STLS since the sparsity parameter of the TRIM can be solved efficiently by convex solvers. We compare these methodologies on challenging nonlinear systems, specifically the Lorenz 63 system, the Bouc Wen oscillator from the nonlinear dynamics benchmark of No\"el and Schoukens, 2016, and a time delay system describing tool cutting dynamics. This study emphasizes the comparisons between STLS, reweighted $\ell_1$ minimization, and Trimmed Lasso in identification with respect to problems faced by practitioners: the problem of finite and noisy data, the performance of the sparse regression of when the library grows in dimension (multicollinearity), and automatic methods for choice of regularization parameters.
</details>
<details>
<summary>摘要</summary>
非线性动力系统的识别已经得到了广泛的应用，通过非线性动力系统简化的逻辑（SINDy）via 随机阈值最小二乘（STLS）算法。在文献中，许多基于SINDy的扩展出现了，以处理实际数据的限定长度和噪声。最近，为了模型识别，提出了 computationally intensive的 ensemble bootstrapped SINDy模型（E-SINDy）方法。虽然扩展SINDy多种，但它们的稀疏采样器 occasionally提供稀疏的动力简化，而不是精确的回归。此外，这些采样器在多icollinearity情况下会受到影响，例如Lasso中的不可 reprehender condition。在这篇论文中，我们表明了 Trimmed Lasso 可以在更严重的噪声、有限数据和多icollinearity情况下提供精确的回归，而不是E-SINDy。此外，TRIM的计算成本是 STLS 的 asymptotic 等价，因为TRIM 的稀疏参数可以由 convex 解决器有效地解决。我们将这些方法在非线性系统中进行比较，包括 Lorenz 63 系统、Bouc Wen 振荡器和时延系统，以及2016年 No\"el 和 Schoukens 非线性动力系统比赛中的非线性动力系统 benchmark。这一研究强调了 STLS、重量 $\ell_1$ 最小化和 Trimmed Lasso 在面临实际问题时的比较：有限和噪声数据、稀疏回归在库存 grows 时的性能，以及自动选择正则化参数的问题。
</details></li>
</ul>
<hr>
<h2 id="DualCoOp-Fast-and-Effective-Adaptation-to-Multi-Label-Recognition-with-Limited-Annotations"><a href="#DualCoOp-Fast-and-Effective-Adaptation-to-Multi-Label-Recognition-with-Limited-Annotations" class="headerlink" title="DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations"></a>DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01890">http://arxiv.org/abs/2308.01890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Hu, Ximeng Sun, Stan Sclaroff, Kate Saenko</li>
<li>for: 这个研究的目的是提高多 Label 图像识别 tasks 的准确性，特别是在低标签情况下。</li>
<li>methods: 这个研究使用了一个名为 Evidence-guided Dual Context Optimization (DualCoOp++) 的框架，这是一个统一的方法来解决 partial-label 和 zero-shot multi-label 识别 задачі。DualCoOp++ 使用了不同的文本内容来分类目标类别，并且将这些内容转换为 Parametric 组件。</li>
<li>results: 实验结果显示，DualCoOp++ 在两个低标签情况下的标准多 Label 识别Benchmark上表现出色，较以前的方法更好。<details>
<summary>Abstract</summary>
Multi-label image recognition in the low-label regime is a task of great challenge and practical significance. Previous works have focused on learning the alignment between textual and visual spaces to compensate for limited image labels, yet may suffer from reduced accuracy due to the scarcity of high-quality multi-label annotations. In this research, we leverage the powerful alignment between textual and visual features pretrained with millions of auxiliary image-text pairs. We introduce an efficient and effective framework called Evidence-guided Dual Context Optimization (DualCoOp++), which serves as a unified approach for addressing partial-label and zero-shot multi-label recognition. In DualCoOp++ we separately encode evidential, positive, and negative contexts for target classes as parametric components of the linguistic input (i.e., prompts). The evidential context aims to discover all the related visual content for the target class, and serves as guidance to aggregate positive and negative contexts from the spatial domain of the image, enabling better distinguishment between similar categories. Additionally, we introduce a Winner-Take-All module that promotes inter-class interaction during training, while avoiding the need for extra parameters and costs. As DualCoOp++ imposes minimal additional learnable overhead on the pretrained vision-language framework, it enables rapid adaptation to multi-label recognition tasks with limited annotations and even unseen classes. Experiments on standard multi-label recognition benchmarks across two challenging low-label settings demonstrate the superior performance of our approach compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
多 Label 图像识别在低标签 режиме是一项具有挑战性和实际意义的任务。先前的研究通过学习文本和视觉空间之间的对应关系来资料缺乏多 Label 图像标注，但可能会受到质量不佳多 Label 图像标注的影响。在这项研究中，我们利用了强大的文本和视觉特征之间的对应关系，它们在 Millionen 个 auxiliary 图像-文本对中预训练。我们提出了一种高效可靠的框架，即 Evidence-guided Dual Context Optimization（DualCoOp++），它作为多 Label 图像识别中的一种统一方法。在 DualCoOp++ 中，我们分别编码目标类的证据、积极和消极上下文为参数化的文本输入（即提示）中的Parametric 组件。证据上下文的目的是找到目标类相关的所有视觉内容，并作为指导将空间领域中的积极和消极上下文聚合，以更好地区分相似类别。此外，我们还引入了一个 Winner-Take-All 模块，它在训练中促进类之间的交互，而不需要额外的参数和成本。由于 DualCoOp++ 对预训练的视觉语言框架做出了最小的额外学习负担，因此它可以快速适应多 Label 图像识别任务，即使具有有限的标注和未看到的类。实验表明，我们的方法在标准多 Label 图像识别标准 benchmark 上表现出优于状态的方法。
</details></li>
</ul>
<hr>
<h2 id="Cream-Skimming-the-Underground-Identifying-Relevant-Information-Points-from-Online-Forums"><a href="#Cream-Skimming-the-Underground-Identifying-Relevant-Information-Points-from-Online-Forums" class="headerlink" title="Cream Skimming the Underground: Identifying Relevant Information Points from Online Forums"></a>Cream Skimming the Underground: Identifying Relevant Information Points from Online Forums</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02581">http://arxiv.org/abs/2308.02581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe Moreno-Vera, Mateus Nogueira, Cainã Figueiredo, Daniel Sadoc Menasché, Miguel Bicudo, Ashton Woiwood, Enrico Lovat, Anton Kocheturov, Leandro Pfleger de Aguiar</li>
<li>for: 本研究提出一种基于机器学习的方法，用于在野外抓取漏洞利用情况。随着在线上讨论漏洞利用的帖子和帖子数量不断增加，需要一种自动化处理这些帖子和帖子的方法，以触发警报 Depending on their content.</li>
<li>methods: 我们使用了CrimeBB数据集，该数据集包含多个下面forum中的数据，并开发了一个监督式机器学习模型，可以过滤引用CVEs的帖子，并将其分为Proof-of-Concept、Weaponization和利用三个类别。使用Random Forest算法，我们表明可以在分类任务中达到0.99以上的准确率、精度和准确率。</li>
<li>results: 我们发现，在 weaponization和利用之间存在差异，例如解释决定树的输出，并分析了黑客社区的利益和其他相关方面。总的来说，我们的工作提供了野外漏洞利用情况的研究，可以用于提供额外的真实数据，以便更好地评估模型如EPSS和Expected Exploitability。<details>
<summary>Abstract</summary>
This paper proposes a machine learning-based approach for detecting the exploitation of vulnerabilities in the wild by monitoring underground hacking forums. The increasing volume of posts discussing exploitation in the wild calls for an automatic approach to process threads and posts that will eventually trigger alarms depending on their content. To illustrate the proposed system, we use the CrimeBB dataset, which contains data scraped from multiple underground forums, and develop a supervised machine learning model that can filter threads citing CVEs and label them as Proof-of-Concept, Weaponization, or Exploitation. Leveraging random forests, we indicate that accuracy, precision and recall above 0.99 are attainable for the classification task. Additionally, we provide insights into the difference in nature between weaponization and exploitation, e.g., interpreting the output of a decision tree, and analyze the profits and other aspects related to the hacking communities. Overall, our work sheds insight into the exploitation of vulnerabilities in the wild and can be used to provide additional ground truth to models such as EPSS and Expected Exploitability.
</details>
<details>
<summary>摘要</summary>
Note: "EPSS" stands for "Expected Potential Security Score" and "Expected Exploitability" is a metric used to measure the severity of a vulnerability.
</details></li>
</ul>
<hr>
<h2 id="Statistical-Estimation-Under-Distribution-Shift-Wasserstein-Perturbations-and-Minimax-Theory"><a href="#Statistical-Estimation-Under-Distribution-Shift-Wasserstein-Perturbations-and-Minimax-Theory" class="headerlink" title="Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory"></a>Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01853">http://arxiv.org/abs/2308.01853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrickrchao/dist_shift_exp">https://github.com/patrickrchao/dist_shift_exp</a></li>
<li>paper_authors: Patrick Chao, Edgar Dobriban</li>
<li>for: 本文研究了现代统计学中的分布Shift问题，即数据点的perturbation可能会系统地改变数据的性质。</li>
<li>methods: 本文使用 Wasserstein distribution shift，研究了每个数据点可能会受到轻微改动的情况，而不是Huber contamination模型中的一部分观察值是异常值。本文还研究了各种重要的统计问题，包括位置估计、线性回归和非 Parametric density estimation。</li>
<li>results: 本文发现，在平方损函数下的mean估计和线性回归预测错误中， sample mean和least squares estimator是相对最佳的。这些优点在独立分布shift和共同分布shift下都存在，但最差的perturbation和最大风险不同。其他问题中，提供了近似最佳的估计器和精确的finite-sample bound。本文还介绍了一些用于下界最大风险的工具，如location家族的缓和技术，以及classical工具的扩展，如最差序列的 prior、modulus of continuity、Le Cam的、Fano的和Assouad的方法。<details>
<summary>Abstract</summary>
Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For other problems, we provide nearly optimal estimators and precise finite-sample bounds. We also introduce several tools for bounding the minimax risk under distribution shift, such as a smoothing technique for location families, and generalizations of classical tools including least favorable sequences of priors, the modulus of continuity, Le Cam's, Fano's, and Assouad's methods.
</details>
<details>
<summary>摘要</summary>
现代统计学中的分布转移是一个严重的问题，因为它可能会系统性地改变数据的性质，从真实的情况偏离。我们关注 Wasserstein 分布转移，其中每个数据点都可能会经历一些微的扰动，而不是 Huber 污染模型，其中一部分观测值是异常值。我们提出并研究了分布转移的不同类型，包括共同扰动分布转移。我们分析了一些重要的统计问题，包括位置估计、线性回归和非 Parametric 密度估计。在平方损失下，我们发现了最小最大风险、最不利的扰动和 sample 均值和最小二乘估计器是相应优化的。这些优化存在独立和共同转移下都是正确的，但最不利的扰动和最大风险不同。对于其他问题，我们提供了近似优化的估计器和精确的 finite-sample 上限。我们还引入了一些用于下界最大风险的工具，包括分布转移后的平滑技术、类 least favorable 序列假设、模ulus 稳定性、Le Cam 、Fano 和 Assouad 的方法。
</details></li>
</ul>
<hr>
<h2 id="Curricular-Transfer-Learning-for-Sentence-Encoded-Tasks"><a href="#Curricular-Transfer-Learning-for-Sentence-Encoded-Tasks" class="headerlink" title="Curricular Transfer Learning for Sentence Encoded Tasks"></a>Curricular Transfer Learning for Sentence Encoded Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01849">http://arxiv.org/abs/2308.01849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jader Martins Camboim de Sá, Matheus Ferraroni Sanches, Rafael Roque de Souza, Júlio Cesar dos Reis, Leandro Aparecido Villas</li>
<li>for: 提高NLU任务中模型的表现，尤其是在数据分布变化时。</li>
<li>methods: 提出了一种逐步适应（curriculum）策略，通过数据黑客和语法分析导航进行适应。</li>
<li>results: 在我们的实验中，我们的方法比其他已知预训练方法在多语言对话任务（MultiWoZ）中获得了显著提高。<details>
<summary>Abstract</summary>
Fine-tuning language models in a downstream task is the standard approach for many state-of-the-art methodologies in the field of NLP. However, when the distribution between the source task and target task drifts, \textit{e.g.}, conversational environments, these gains tend to be diminished. This article proposes a sequence of pre-training steps (a curriculum) guided by "data hacking" and grammar analysis that allows further gradual adaptation between pre-training distributions. In our experiments, we acquire a considerable improvement from our method compared to other known pre-training approaches for the MultiWoZ task.
</details>
<details>
<summary>摘要</summary>
通常的方法是在下游任务中细化语言模型，以获得许多状态OF-THE-ART的成果。但是，当源任务和目标任务的分布发生变化，例如对话环境，这些改进往往减少。这篇文章提出了一系列的预训练步骤（课程），通过“数据黑客”和语法分析引导，以进一步适应预训练分布的变化。在我们的实验中，我们获得了与其他已知预训练方法相比较大的改进，用于多语言对话任务。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Deep-Supervision-Network-A-Noise-Resilient-Approach-for-QoS-Prediction"><a href="#Probabilistic-Deep-Supervision-Network-A-Noise-Resilient-Approach-for-QoS-Prediction" class="headerlink" title="Probabilistic Deep Supervision Network: A Noise-Resilient Approach for QoS Prediction"></a>Probabilistic Deep Supervision Network: A Noise-Resilient Approach for QoS Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02580">http://arxiv.org/abs/2308.02580</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hotfrom/pds-net">https://github.com/hotfrom/pds-net</a></li>
<li>paper_authors: Ziliang Wang, Xiaohong Zhang, Sheng Huang, Wei Zhang, Dan Yang, Meng Yan</li>
<li>for: 提高用户满意度，准确预测 unknown QoS 值</li>
<li>methods: 提出了 Probabilistic Deep Supervision Network (PDS-Net) 框架，利用 Gaussian 型概率空间进行中间层级supervision，学习known features和真实标签的概率空间</li>
<li>results: 在两个实际 QoS 数据集上进行实验评估，比对 estado-of-the-art 基elines， validate 我们的方法的有效性<details>
<summary>Abstract</summary>
Quality of Service (QoS) prediction is an essential task in recommendation systems, where accurately predicting unknown QoS values can improve user satisfaction. However, existing QoS prediction techniques may perform poorly in the presence of noise data, such as fake location information or virtual gateways. In this paper, we propose the Probabilistic Deep Supervision Network (PDS-Net), a novel framework for QoS prediction that addresses this issue. PDS-Net utilizes a Gaussian-based probabilistic space to supervise intermediate layers and learns probability spaces for both known features and true labels. Moreover, PDS-Net employs a condition-based multitasking loss function to identify objects with noise data and applies supervision directly to deep features sampled from the probability space by optimizing the Kullback-Leibler distance between the probability space of these objects and the real-label probability space. Thus, PDS-Net effectively reduces errors resulting from the propagation of corrupted data, leading to more accurate QoS predictions. Experimental evaluations on two real-world QoS datasets demonstrate that the proposed PDS-Net outperforms state-of-the-art baselines, validating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
服务质量（QoS）预测是推荐系统中的一项重要任务，可以提高用户满意度。然而，现有的QoS预测技术可能在噪声数据存在时表现不佳。在这篇论文中，我们提出了可靠性深度监督网络（PDS-Net），一种解决这个问题的新框架。PDS-Net使用 Gaussian 型概率空间来监督中间层，并学习概率空间 для已知特征和真实标签。此外，PDS-Net 使用基于条件的多任务损失函数来识别具有噪声数据的对象，并直接将深度特征从概率空间中抽取到真实标签的概率空间中进行监督。因此，PDS-Net 可以减少噪声数据的传播错误，从而提高 QoS 预测的准确性。实验评估在两个真实 QoS 数据集上表明，提出的 PDS-Net 已经超越了状态艺术基eline。
</details></li>
</ul>
<hr>
<h2 id="URET-Universal-Robustness-Evaluation-Toolkit-for-Evasion"><a href="#URET-Universal-Robustness-Evaluation-Toolkit-for-Evasion" class="headerlink" title="URET: Universal Robustness Evaluation Toolkit (for Evasion)"></a>URET: Universal Robustness Evaluation Toolkit (for Evasion)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01840">http://arxiv.org/abs/2308.01840</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/uret">https://github.com/ibm/uret</a></li>
<li>paper_authors: Kevin Eykholt, Taesung Lee, Douglas Schales, Jiyong Jang, Ian Molloy, Masha Zorin</li>
<li>for: 本研究旨在提高机器学习模型的安全和可靠性，通过生成可逃脱攻击的输入，以帮助确保AI任务的正确性和可靠性。</li>
<li>methods: 本研究提出了一种新的框架，可以生成不同输入类型和任务领域的攻击输入。该框架使用给定的输入变换集合，找到一个符合semantic和功能要求的攻击输入序列。</li>
<li>results: 本研究在多种不同的机器学习任务和输入表示中展示了框架的通用性。此外，研究还表明了生成攻击示例的重要性，以便应用防御技术。<details>
<summary>Abstract</summary>
Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples as they enable the deployment of mitigation techniques.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a new framework to generate adversarial inputs regardless of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples, as they enable the deployment of mitigation techniques.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/04/cs.LG_2023_08_04/" data-id="clly4xtdq0063vl88g59aen93" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/cs.SD_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/04/cs.SD_2023_08_04/">cs.SD - 2023-08-04 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Monaural-Speech-Enhancement-using-Spectrum-Attention-Fusion"><a href="#Efficient-Monaural-Speech-Enhancement-using-Spectrum-Attention-Fusion" class="headerlink" title="Efficient Monaural Speech Enhancement using Spectrum Attention Fusion"></a>Efficient Monaural Speech Enhancement using Spectrum Attention Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02263">http://arxiv.org/abs/2308.02263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyu Long, Jetic Gū, Binhao Bai, Zhibo Yang, Ping Wei, Junli Li</li>
<li>for: 提高自动 speech 处理管道中的speech减噪性能，以提高干扰 speech 的分离效果。</li>
<li>methods: 提出了一种 Spectrum Attention Fusion 技术，用于将自我注意力 fusion 与 spectral 特征 fusion 结合，以提高模型的表达能力和效率。</li>
<li>results: 在 Voice Bank + DEMAND 数据集上，与 state-of-the-art 模型比较，提出的模型能够达到相当或更好的结果，同时具有较少的参数（0.58M）。<details>
<summary>Abstract</summary>
Speech enhancement is a demanding task in automated speech processing pipelines, focusing on separating clean speech from noisy channels. Transformer based models have recently bested RNN and CNN models in speech enhancement, however at the same time they are much more computationally expensive and require much more high quality training data, which is always hard to come by. In this paper, we present an improvement for speech enhancement models that maintains the expressiveness of self-attention while significantly reducing model complexity, which we have termed Spectrum Attention Fusion. We carefully construct a convolutional module to replace several self-attention layers in a speech Transformer, allowing the model to more efficiently fuse spectral features. Our proposed model is able to achieve comparable or better results against SOTA models but with significantly smaller parameters (0.58M) on the Voice Bank + DEMAND dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Speech enhancement 是自动化语音处理流程中的一个要求，旨在分离含杂的语音和清晰的语音。基于Transformer的模型在最近的Speech enhancement中表现出色，但同时它们也更加计算昂贵，需要更多高质量的训练数据，这并不容易获得。在这篇论文中，我们提出了一种改进 speech enhancement 模型，保持了自注意的表达力，同时显著减少模型的复杂度，我们称之为 Spectrum Attention Fusion。我们在一个 convolutional 模块中代替了一些自注意层，让模型更有效地融合频谱特征。我们的提议模型在 Voice Bank + DEMAND 数据集上可以达到与顶峰模型相当或更好的结果，但具有远小于参数（0.58M）。
</details></li>
</ul>
<hr>
<h2 id="Emo-DNA-Emotion-Decoupling-and-Alignment-Learning-for-Cross-Corpus-Speech-Emotion-Recognition"><a href="#Emo-DNA-Emotion-Decoupling-and-Alignment-Learning-for-Cross-Corpus-Speech-Emotion-Recognition" class="headerlink" title="Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition"></a>Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02190">http://arxiv.org/abs/2308.02190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaxin-ye/emo-dna">https://github.com/jiaxin-ye/emo-dna</a></li>
<li>paper_authors: Jiaxin Ye, Yujie Wei, Xin-Cheng Wen, Chenglong Ma, Zhizhong Huang, Kunhong Liu, Hongming Shan</li>
<li>for: 这个研究的目的是将拥有不同 Corpora 的语音情感识别系统进行整合，以提高其在不同 Corpora 上的表现。</li>
<li>methods: 这个研究提出了一个名为 Emotion Decoupling aNd Alignment 的新框架，它使用了对照分离和双层情感对齐来学习语音情感识别系统。</li>
<li>results: 实验结果显示，这个新框架在多个跨 Corpora 的情感识别任务中表现更好，比起现有的方法。<details>
<summary>Abstract</summary>
Cross-corpus speech emotion recognition (SER) seeks to generalize the ability of inferring speech emotion from a well-labeled corpus to an unlabeled one, which is a rather challenging task due to the significant discrepancy between two corpora. Existing methods, typically based on unsupervised domain adaptation (UDA), struggle to learn corpus-invariant features by global distribution alignment, but unfortunately, the resulting features are mixed with corpus-specific features or not class-discriminative. To tackle these challenges, we propose a novel Emotion Decoupling aNd Alignment learning framework (EMO-DNA) for cross-corpus SER, a novel UDA method to learn emotion-relevant corpus-invariant features. The novelties of EMO-DNA are two-fold: contrastive emotion decoupling and dual-level emotion alignment. On one hand, our contrastive emotion decoupling achieves decoupling learning via a contrastive decoupling loss to strengthen the separability of emotion-relevant features from corpus-specific ones. On the other hand, our dual-level emotion alignment introduces an adaptive threshold pseudo-labeling to select confident target samples for class-level alignment, and performs corpus-level alignment to jointly guide model for learning class-discriminative corpus-invariant features across corpora. Extensive experimental results demonstrate the superior performance of EMO-DNA over the state-of-the-art methods in several cross-corpus scenarios. Source code is available at https://github.com/Jiaxin-Ye/Emo-DNA.
</details>
<details>
<summary>摘要</summary>
cross-corpus speech emotion recognition (SER) 提高了推断语音情绪的能力，从一个很好地标注的 corpora 扩展到另一个没有标注的 corpora，这是一项非常具有挑战性的任务，因为两个 corpora 之间存在很大的差异。现有的方法通常基于无监督领域适应 (UDA)，尝试通过全局分布对齐来学习 corpora  invariant 特征，但是 unfortunately，得到的特征都是混合 corpora 特定特征或不是类别特征。为了解决这些挑战，我们提出了一个新的 Emotion Decoupling and Alignment learning framework (EMO-DNA)  для cross-corpus SER，一种新的 UDA 方法来学习情绪相关的 corpora  invariant 特征。EMO-DNA 的两大创新是：对比情绪分离和双级情绪对接。一方面，我们的对比情绪分离通过对比分离损失来强化情绪相关特征与 corpora 特定特征之间的分离性。另一方面，我们的双级情绪对接引入了一个 adaptive 阈值 pseudo-labeling，选择 confidence 的目标样本进行类别对接，并在 corpora 级别对接以协助模型学习类别特征的 cross-corpus 普适性。我们的实验结果表明，EMO-DNA 在多个 cross-corpus 场景中表现出了与当前状态OF 法的超越性。代码可以在 <https://github.com/Jiaxin-Ye/Emo-DNA> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Capturing-Spectral-and-Long-term-Contextual-Information-for-Speech-Emotion-Recognition-Using-Deep-Learning-Techniques"><a href="#Capturing-Spectral-and-Long-term-Contextual-Information-for-Speech-Emotion-Recognition-Using-Deep-Learning-Techniques" class="headerlink" title="Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques"></a>Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04517">http://arxiv.org/abs/2308.04517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samiul Islam, Md. Maksudul Haque, Abu Jobayer Md. Sadat</li>
<li>for: 本研究旨在超越传统的speech emotion recognition方法（如LSTM、CNN、RNN、SVM、MLP），这些方法具有难以捕捉长期依赖关系、捕捉时间动态和捕捉复杂模式关系等缺陷。</li>
<li>methods: 本研究提出了一个 ensemble 模型，该模型将文本数据处理GCN（图 convolutional networks）和音频信号分析 HuBERT trasformer 相结合。GCN 可以利用文本的图形表示，捕捉文本中的长期Contextual 依赖关系和 semantics 关系，而 HuBERT 通过自我注意机制，可以捕捉音频信号中的长期依赖关系，捕捉时间动态。</li>
<li>results: 结果表明，将 GCN 和 HuBERT 相结合，可以充分利用这两种方法的优势，同时分析多Modal 数据，并将这些模式相互融合，从而提高情绪识别系统的准确性。<details>
<summary>Abstract</summary>
Traditional approaches in speech emotion recognition, such as LSTM, CNN, RNN, SVM, and MLP, have limitations such as difficulty capturing long-term dependencies in sequential data, capturing the temporal dynamics, and struggling to capture complex patterns and relationships in multimodal data. This research addresses these shortcomings by proposing an ensemble model that combines Graph Convolutional Networks (GCN) for processing textual data and the HuBERT transformer for analyzing audio signals. We found that GCNs excel at capturing Long-term contextual dependencies and relationships within textual data by leveraging graph-based representations of text and thus detecting the contextual meaning and semantic relationships between words. On the other hand, HuBERT utilizes self-attention mechanisms to capture long-range dependencies, enabling the modeling of temporal dynamics present in speech and capturing subtle nuances and variations that contribute to emotion recognition. By combining GCN and HuBERT, our ensemble model can leverage the strengths of both approaches. This allows for the simultaneous analysis of multimodal data, and the fusion of these modalities enables the extraction of complementary information, enhancing the discriminative power of the emotion recognition system. The results indicate that the combined model can overcome the limitations of traditional methods, leading to enhanced accuracy in recognizing emotions from speech.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="N-gram-Boosting-Improving-Contextual-Biasing-with-Normalized-N-gram-Targets"><a href="#N-gram-Boosting-Improving-Contextual-Biasing-with-Normalized-N-gram-Targets" class="headerlink" title="N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets"></a>N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02092">http://arxiv.org/abs/2308.02092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang Yau Li, Shreekantha Nadig, Karol Chang, Zafarullah Mahmood, Riqiang Wang, Simon Vandieken, Jonas Robertson, Fred Mailhot</li>
<li>for: 提高 keywords 识别率</li>
<li>methods: 使用 two-step keyword boosting mechanism，Normalize unigrams 和 n-grams，避免 missing hits 和 over-boosting multi-token keywords</li>
<li>results: 提高 keyword recognition rate by 26% Relative on proprietary in-domain dataset，和 2% on LibriSpeechI hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Accurate transcription of proper names and technical terms is particularly important in speech-to-text applications for business conversations. These words, which are essential to understanding the conversation, are often rare and therefore likely to be under-represented in text and audio training data, creating a significant challenge in this domain. We present a two-step keyword boosting mechanism that successfully works on normalized unigrams and n-grams rather than just single tokens, which eliminates missing hits issues with boosting raw targets. In addition, we show how adjusting the boosting weight logic avoids over-boosting multi-token keywords. This improves our keyword recognition rate by 26% relative on our proprietary in-domain dataset and 2% on LibriSpeech. This method is particularly useful on targets that involve non-alphabetic characters or have non-standard pronunciations.
</details>
<details>
<summary>摘要</summary>
精准转写特有名称和技术术语 particualrly important in speech-to-text应用程序中，这些词语是理解对话的关键，但它们通常是罕见的，因此在文本和音频训练数据中受到抑制。我们提出了一种两步关键词强化机制，该机制可以在 норма化单个字和n-gram中工作，而不是只是单个token，这将消除 raw 目标中的缺失命中问题。此外，我们还证明了如何调整强化权重逻辑，以避免多token关键被过度强化。这将提高我们的关键识别率达26%，相对于我们的自有领域数据集，并且2% 在 LibriSpeech 上。这种方法特别有用于targets 中包含非字母字符或非标准发音。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/04/cs.SD_2023_08_04/" data-id="clly4xtem009kvl88dfd779qx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/eess.IV_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/04/eess.IV_2023_08_04/">eess.IV - 2023-08-04 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Frequency-Disentangled-Features-in-Neural-Image-Compression"><a href="#Frequency-Disentangled-Features-in-Neural-Image-Compression" class="headerlink" title="Frequency Disentangled Features in Neural Image Compression"></a>Frequency Disentangled Features in Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02620">http://arxiv.org/abs/2308.02620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Zafari, Atefeh Khoshkhahtinat, Piyush Mehta, Mohammad Saeed Ebrahimi Saadabadi, Mohammad Akyash, Nasser M. Nasrabadi</li>
<li>For: The paper proposes a neural image compression network that leverages feature-level frequency disentanglement and an augmented self-attention score calculation to improve the compression efficiency and de-correlate the image features.* Methods: The proposed network uses a rate-distortion variational autoencoder (R-D VAE) with relaxed scalar quantization, which is guided by a feature-level frequency disentanglement to capture the low-frequency texture of the image. The network also utilizes an augmented self-attention score calculation based on the Hadamard product during both encoding and decoding.* Results: The proposed network outperforms hand-engineered codecs and neural network-based codecs built on computation-heavy spatially autoregressive entropy models, demonstrating its effectiveness in image compression.<details>
<summary>Abstract</summary>
The design of a neural image compression network is governed by how well the entropy model matches the true distribution of the latent code. Apart from the model capacity, this ability is indirectly under the effect of how close the relaxed quantization is to the actual hard quantization. Optimizing the parameters of a rate-distortion variational autoencoder (R-D VAE) is ruled by this approximated quantization scheme. In this paper, we propose a feature-level frequency disentanglement to help the relaxed scalar quantization achieve lower bit rates by guiding the high entropy latent features to include most of the low-frequency texture of the image. In addition, to strengthen the de-correlating power of the transformer-based analysis/synthesis transform, an augmented self-attention score calculation based on the Hadamard product is utilized during both encoding and decoding. Channel-wise autoregressive entropy modeling takes advantage of the proposed frequency separation as it inherently directs high-informational low-frequency channels to the first chunks and conditions the future chunks on it. The proposed network not only outperforms hand-engineered codecs, but also neural network-based codecs built on computation-heavy spatially autoregressive entropy models.
</details>
<details>
<summary>摘要</summary>
neural 图像压缩网络的设计受到真实分布的熵模型匹配度的限制。 apart from 模型容量，这种能力受到实际硬化量化的距离影响。 在这篇论文中，我们提出了一种基于频谱分解的特征级频率分离，以帮助放松量化实现更低的比特率，使高熵特征映射到图像中的低频文本。 此外，我们还利用了在编码和解码过程中的扩展自我注意力计算，以增强trasformer 基于的分析/生成变换的分离能力。 通道 wise 自动化熵模型利用了我们提出的频谱分解，因为它直接引导高信息低频通道到首块，并将后续块condition 在它之上。 提议的网络不仅超过了手工编码器，还超过了基于计算昂贵的空间自动关联熵模型的神经网络编码器。
</details></li>
</ul>
<hr>
<h2 id="Brain-MRI-Segmentation-using-Template-Based-Training-and-Visual-Perception-Augmentation"><a href="#Brain-MRI-Segmentation-using-Template-Based-Training-and-Visual-Perception-Augmentation" class="headerlink" title="Brain MRI Segmentation using Template-Based Training and Visual Perception Augmentation"></a>Brain MRI Segmentation using Template-Based Training and Visual Perception Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02363">http://arxiv.org/abs/2308.02363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fang-Cheng Yeh</li>
<li>for: 用一个人类大脑MRI模板和其相关的分类标签来训练一个3D U-Net模型从头开始，不需要大量的训练数据。</li>
<li>methods: 使用模板基于的训练方法，并包括视觉感知增强以提高模型对各种图像输入的Robustness，以避免过拟合。</li>
<li>results: 通过这种方法，我们训练了mouse、rat、猴、猴和人类大脑MRI的3D U-Net模型，并实现了分割任务，如脑骨梁除、大脑分割和组织概率地图。这种工具有效地解决了深度学习应用图像分析中的数据有限问题，并为研究人员提供了一个统一的解决方案，只需要一个图像样本来训练深度神经网络。<details>
<summary>Abstract</summary>
Deep learning models usually require sufficient training data to achieve high accuracy, but obtaining labeled data can be time-consuming and labor-intensive. Here we introduce a template-based training method to train a 3D U-Net model from scratch using only one population-averaged brain MRI template and its associated segmentation label. The process incorporated visual perception augmentation to enhance the model's robustness in handling diverse image inputs and mitigating overfitting. Leveraging this approach, we trained 3D U-Net models for mouse, rat, marmoset, rhesus, and human brain MRI to achieve segmentation tasks such as skull-stripping, brain segmentation, and tissue probability mapping. This tool effectively addresses the limited availability of training data and holds significant potential for expanding deep learning applications in image analysis, providing researchers with a unified solution to train deep neural networks with only one image sample.
</details>
<details>
<summary>摘要</summary>
深度学习模型通常需要充足的训练数据来达到高精度，但获取标注数据可以是时间consuming和劳动 INTENSIVE。在这里，我们介绍了一种模板基于的训练方法，可以从scratch用一个人类大脑MRI模板和其关联的分割标注来训练3D U-Net模型。该过程包括视觉感知增强以提高模型对多种图像输入的抗衡能力和避免过拟合。通过这种方法，我们训练了3D U-Net模型用于鼠、老鼠、猴、人类大脑MRI的分割任务，如骨干取除、大脑分割和组织概率地图。这种工具有效地解决了训练数据的有限性问题，并具有扩展深度学习应用于图像分析的潜在 potential。
</details></li>
</ul>
<hr>
<h2 id="T-UNet-Triplet-UNet-for-Change-Detection-in-High-Resolution-Remote-Sensing-Images"><a href="#T-UNet-Triplet-UNet-for-Change-Detection-in-High-Resolution-Remote-Sensing-Images" class="headerlink" title="T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images"></a>T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02356">http://arxiv.org/abs/2308.02356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pl-2000/t-unet">https://github.com/pl-2000/t-unet</a></li>
<li>paper_authors: Huan Zhong, Chen Wu</li>
<li>for: 这篇论文旨在提出一个新的网络模型，用于远程感知图像变化检测，以提高检测精度和准确性。</li>
<li>methods: 这篇论文提出了一个三枝Encoder结构，并使用多枝空间特征聚合模组（MBSSCA）进行特征聚合和探索。在解码阶段，论文导入了通道注意力机制（CAM）和空间注意力机制（SAM），以实现完整地探索和融合详细的特征信息和 semantic 地域特征信息。</li>
<li>results: 这篇论文的实验结果显示，T-UNet 模型可以实现更高的准确率和精度，并且能够更好地探索和融合详细的特征信息和 semantic 地域特征信息。<details>
<summary>Abstract</summary>
Remote sensing image change detection aims to identify the differences between images acquired at different times in the same area. It is widely used in land management, environmental monitoring, disaster assessment and other fields. Currently, most change detection methods are based on Siamese network structure or early fusion structure. Siamese structure focuses on extracting object features at different times but lacks attention to change information, which leads to false alarms and missed detections. Early fusion (EF) structure focuses on extracting features after the fusion of images of different phases but ignores the significance of object features at different times for detecting change details, making it difficult to accurately discern the edges of changed objects. To address these issues and obtain more accurate results, we propose a novel network, Triplet UNet(T-UNet), based on a three-branch encoder, which is capable to simultaneously extract the object features and the change features between the pre- and post-time-phase images through triplet encoder. To effectively interact and fuse the features extracted from the three branches of triplet encoder, we propose a multi-branch spatial-spectral cross-attention module (MBSSCA). In the decoder stage, we introduce the channel attention mechanism (CAM) and spatial attention mechanism (SAM) to fully mine and integrate detailed textures information at the shallow layer and semantic localization information at the deep layer.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese remote sensing image change detection 图像变化检测是用于在同一区域内的不同时间检测到的图像差异。它广泛应用于土地管理、环境监测、灾害评估等领域。目前，大多数变化检测方法基于 Siamese 网络结构或 Early Fusion 结构。Siamese 结构注重在不同时间中EXTRACTING对象特征，但缺乏关注变化信息，导致假警报和漏报。 Early Fusion 结构注重在不同阶段图像归一化后EXTRACTING对象特征，但忽视对象特征在不同时间段中的变化细节，使其困难准确地分辨变化的边缘。为了解决这些问题并获得更高精度的结果，我们提出了一种新的网络模型，Triplet UNet（T-UNet），基于三个分支Encoder，能同时EXTRACT对象特征和不同时间段图像之间的变化特征。为了有效地交互和融合 triplet Encoder 中EXTRACT的特征，我们提出了多支分支空间特征跟踪模块（MBSSCA）。在解码阶段，我们引入了通道注意机制（CAM）和空间注意机制（SAM），以全面挖掘和融合图像的细节信息和semantic 本地化信息。
</details></li>
</ul>
<hr>
<h2 id="Generative-Image-Priors-for-MRI-Reconstruction-Trained-from-Magnitude-Only-Images"><a href="#Generative-Image-Priors-for-MRI-Reconstruction-Trained-from-Magnitude-Only-Images" class="headerlink" title="Generative Image Priors for MRI Reconstruction Trained from Magnitude-Only Images"></a>Generative Image Priors for MRI Reconstruction Trained from Magnitude-Only Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02340">http://arxiv.org/abs/2308.02340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrirecon/image-priors">https://github.com/mrirecon/image-priors</a></li>
<li>paper_authors: Guanxiong Luo, Xiaoqing Wang, Mortiz Blumenthal, Martin Schilling, Erik Hans Ulrich Rauf, Raviteja Kotikalapudi, Niels Focke, Martin Uecker</li>
<li>for: 这个研究旨在构建基于大数据集和阶段信息的通用和稳定的生成图像先验。这些先验可以用于恢复图像质量的正则化。</li>
<li>methods: 研究开始于准备用魔力只图像准备训练数据集，然后将这个数据集扩展到包括阶段信息，并用这个数据集训练生成图像先验。最后，研究人员使用不同的抽样方案进行了线性和非线性恢复的评估。</li>
<li>results: 实验结果表明，基于复杂图像的先验比只基于魔力图像的先验表现更好。此外，一个训练在更大的数据集上的先验也表现出更高的稳定性。最后，我们发现使用生成先验比L1-wavelet正则化更有利于扩展扫描成像。结论：这些发现表明，包括阶段信息和利用大数据集可以提高生成先验的性能和可靠性，并且这些先验可以用于提高MRI重建的质量。<details>
<summary>Abstract</summary>
Purpose: In this work, we present a workflow to construct generic and robust generative image priors from magnitude-only images. The priors can then be used for regularization in reconstruction to improve image quality. Methods: The workflow begins with the preparation of training datasets from magnitude-only MR images. This dataset is then augmented with phase information and used to train generative priors of complex images. Finally, trained priors are evaluated using both linear and nonlinear reconstruction for compressed sensing parallel imaging with various undersampling schemes. Results: The results of our experiments demonstrate that priors trained on complex images outperform priors trained only on magnitude images. Additionally, a prior trained on a larger dataset exhibits higher robustness. Finally, we show that the generative priors are superior to L1 -wavelet regularization for compressed sensing parallel imaging with high undersampling. Conclusion: These findings stress the importance of incorporating phase information and leveraging large datasets to raise the performance and reliability of the generative priors for MRI reconstruction. Phase augmentation makes it possible to use existing image databases for training.
</details>
<details>
<summary>摘要</summary>
目的：在这项工作中，我们提出了一个工作流程，用于从偏好度只图像中构建通用和稳定的生成图像先验。这些先验然后可以用于图像重建中的regularization，以提高图像质量。方法：工作流程开始于从偏好度只图像MR影像中准备训练集。这个集合然后被补充 phase信息，并用于训练复杂图像的生成先验。最后，我们使用线性和非线性重建进行测试，以评估训练后的先验表现。结果：我们的实验结果表明，基于复杂图像的先验比基于偏好度只图像的先验表现更好。此外，一个基于更大的数据集训练的先验表现更高稳定。最后，我们表明，生成先验比L1-wavelet regularization更有优势于高抽样率的并行扫描图像重建。结论：这些发现强调了在图像重建中包含相位信息和利用大数据集来提高生成先验的性能和可靠性。相位增强使得可以使用现有的图像数据库进行训练。
</details></li>
</ul>
<hr>
<h2 id="CT-Reconstruction-from-Few-Planar-X-rays-with-Application-towards-Low-resource-Radiotherapy"><a href="#CT-Reconstruction-from-Few-Planar-X-rays-with-Application-towards-Low-resource-Radiotherapy" class="headerlink" title="CT Reconstruction from Few Planar X-rays with Application towards Low-resource Radiotherapy"></a>CT Reconstruction from Few Planar X-rays with Application towards Low-resource Radiotherapy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02100">http://arxiv.org/abs/2308.02100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanderinrain/xray2ct">https://github.com/wanderinrain/xray2ct</a></li>
<li>paper_authors: Yiran Sun, Tucker Netherton, Laurence Court, Ashok Veeraraghavan, Guha Balakrishnan</li>
<li>for: 这项研究的目的是使用几个平面X射图像生成CT卷积体，以便在低和中Resource Settings中提高肿瘤诊断和治疗的效率。</li>
<li>methods: 该研究使用了深度生成模型，基于神经隐式表示来合成volumetric CT扫描图像从几个平面X射图像的不同角度。同时，模型还可以在训练过程中使用 segmentation masks 来减少生成任务中的产生不必要的特征。</li>
<li>results: 研究发现，使用该方法生成的 thoracic CT 扫描图像和临床实际中获取的 CT 扫描图像之间的是ocenter 辐射剂量Error 小于1%。此外，该方法还比现有的稀疍CT重建基elines 高于标准像素和结构级别指标（PSNR、SSIM、Dice 分数）中的LIDC肺CT数据集。<details>
<summary>Abstract</summary>
CT scans are the standard-of-care for many clinical ailments, and are needed for treatments like external beam radiotherapy. Unfortunately, CT scanners are rare in low and mid-resource settings due to their costs. Planar X-ray radiography units, in comparison, are far more prevalent, but can only provide limited 2D observations of the 3D anatomy. In this work, we propose a method to generate CT volumes from few (<5) planar X-ray observations using a prior data distribution, and perform the first evaluation of such a reconstruction algorithm for a clinical application: radiotherapy planning. We propose a deep generative model, building on advances in neural implicit representations to synthesize volumetric CT scans from few input planar X-ray images at different angles. To focus the generation task on clinically-relevant features, our model can also leverage anatomical guidance during training (via segmentation masks). We generated 2-field opposed, palliative radiotherapy plans on thoracic CTs reconstructed by our method, and found that isocenter radiation dose on reconstructed scans have <1% error with respect to the dose calculated on clinically acquired CTs using <=4 X-ray views. In addition, our method is better than recent sparse CT reconstruction baselines in terms of standard pixel and structure-level metrics (PSNR, SSIM, Dice score) on the public LIDC lung CT dataset. Code is available at: https://github.com/wanderinrain/Xray2CT.
</details>
<details>
<summary>摘要</summary>
干扰X射线成像设备在低和中型资源设备中较为罕见，原因是它们的成本较高。相比之下，平面X射线成像设备更为普遍，但它们只能提供2D的观察结果，无法提供3D的解剖结构。在这项工作中，我们提出了一种方法，使用先前的数据分布来生成CT体积从少于5个平面X射线图像中，并在临床应用中进行了首次评估。我们提出了一种深度生成模型，基于神经隐式表示来生成3D的CT体积图像，并在训练过程中使用解剖指导来避免误差。我们使用了2个对称的反向X射线成像计划来规划肺部CT图像，并发现在重建的扫描图像上的穿透中心辐射剂量与临床获得的CT图像中的辐射剂量之间的差异小于1%。此外，我们的方法也比最近的稀疏CT重建基eline更好，根据标准像素和结构级度指标（PSNR、SSIM、Dice分数）来评估。代码可以在以下地址找到：https://github.com/wanderinrain/Xray2CT。
</details></li>
</ul>
<hr>
<h2 id="Motion-robust-free-running-cardiovascular-MRI"><a href="#Motion-robust-free-running-cardiovascular-MRI" class="headerlink" title="Motion-robust free-running cardiovascular MRI"></a>Motion-robust free-running cardiovascular MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02088">http://arxiv.org/abs/2308.02088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/syedmurtazaarshad/motion-robust-CMR">https://github.com/syedmurtazaarshad/motion-robust-CMR</a></li>
<li>paper_authors: Syed M. Arshad, Lee C. Potter, Chong Chen, Yingmin Liu, Preethi Chandrasekaran, Christopher Crabtree, Yuchi Han, Rizwan Ahmad</li>
<li>For: 这种研究旨在提高自由运行cardiovascular MRI（CMR）的动力稳定性，以便在各种应用中减少运动artefacts。* Methods: 该方法模拟了异常值作为auxiliary变量，并对这个变量进行MR физи学引导的集成随机树减少（CORe）。通过迭代算法，同时对auxiliary变量和图像进行估算。* Results: 对于50个实现中的异常值处理方法，CORe在正常化平均方差（NMSE）和结构相似指标（SSIM）方面表现出色，并且在3D cinema图像中更好地抑制了artefacts，而无需减少图像锐度。在4D流动图像中，CORe得到了更一致的流动测量结果，特别是在运动压力下。<details>
<summary>Abstract</summary>
PURPOSE: To present and validate an outlier rejection method that makes free-running cardiovascular MRI (CMR) more motion robust.   METHODS: The proposed method, called compressive recovery with outlier rejection (CORe), models outliers as an auxiliary variable that is added to the measured data. We enforce MR physics-guided group-sparsity on the auxiliary variable and jointly estimate it along with the image using an iterative algorithm. For validation, CORe is first compared to traditional compressed sensing (CS), robust regression (RR), and another outlier rejection method using two simulation studies. Then, CORe is compared to CS using five 3D cine and ten rest and stress 4D flow imaging datasets.   RESULTS: Our simulation studies show that CORe outperforms CS, RR, and the outlier rejection method in terms of normalized mean squared error (NMSE) and structural similarity index (SSIM) across 50 different realizations. The expert reader evaluation of 3D cine images demonstrates that CORe is more effective in suppressing artifacts while maintaining or improving image sharpness. The flow consistency evaluation in 4D flow images show that CORe yields more consistent flow measurements, especially under exercise stress.   CONCLUSION: An outlier rejection method is presented and validated using simulated and measured data. This method can help suppress motion artifacts in a wide range of free-running CMR applications.   CODE: MATLAB implementation code is available on GitHub at https://github.com/syedmurtazaarshad/motion-robust-CMR
</details>
<details>
<summary>摘要</summary>
目的：提出和验证一种可以使自由运行征 Cardiovascular MRI (CMR) 更加鲁棒于运动 artifacts 的方法。方法：提出的方法，称为 compressive recovery with outlier rejection (CORe)，将异常值模型为 auxillary 变量，并在这个变量上强制施加 MR 物理指导的群 sparse  regularization。我们使用迭代算法来同时估算这个变量和图像。对比 CS、RR 和异常拒绝方法，我们使用两个 simulate 研究进行验证。然后，我们使用五个 3D cine 和十个 rest 和 stress 4D flow imaging 数据集进行验证。结果：我们的 simulate 研究表明，CORe 在 NMSE 和 SSIM 指标上都高于 CS、RR 和异常拒绝方法。专业读者评估 3D cine 图像时，CORe 更有效地抑制 artifacts，同时保持或改善图像的锐度。在 4D flow 图像中，CORe 产生的流量测量更加一致，特别是在运动压力下。结论：我们提出了一种可以鲁棒化自由运行 CMR 应用中的异常拒绝方法，可以帮助抑制运动 artifacts。代码：MATLAB 实现代码可以在 GitHub 上找到，https://github.com/syedmurtazaarshad/motion-robust-CMR
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Counterfactual-Generation-and-Anomaly-Detection-in-Brain-Images"><a href="#Diffusion-Models-for-Counterfactual-Generation-and-Anomaly-Detection-in-Brain-Images" class="headerlink" title="Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain Images"></a>Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02062">http://arxiv.org/abs/2308.02062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alessandro-f/dif-fuse">https://github.com/alessandro-f/dif-fuse</a></li>
<li>paper_authors: Alessandro Fontanella, Grant Mair, Joanna Wardlaw, Emanuele Trucco, Amos Storkey</li>
<li>for: This paper is written for the purpose of generating healthy counterfactuals of diseased images for medical applications such as brain tumor and stroke management.</li>
<li>methods: The paper proposes a weakly supervised method that uses a saliency map obtained with ACAT to generate a healthy version of a diseased image, followed by targeted modifications using a diffusion model trained on healthy samples. The method combines DDPM and DDIM at each step of the sampling process to ensure a seamless transition between edited and unedited parts.</li>
<li>results: The paper shows that the proposed method improves the DICE score of the best competing method from $0.6534$ to $0.7056$ on IST-3 for stroke lesion segmentation and on BraTS2021 for brain tumor segmentation.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文是为了生成疾病图像的健康对照样本而写的。</li>
<li>methods: 这篇论文提出了一种弱监督方法，使用ACAT获得的Saliency map来生成疾病图像的健康版本，然后进行targeted修改使用健康样本上训练的扩散模型。</li>
<li>results: 论文显示，提出的方法可以提高IST-3 stroke病变部分 segmentation的DICE分数从0.6534提高到0.7056，以及BraTS2021 brain tumor segmentation的DICE分数。<details>
<summary>Abstract</summary>
Segmentation masks of pathological areas are useful in many medical applications, such as brain tumour and stroke management. Moreover, healthy counterfactuals of diseased images can be used to enhance radiologists' training files and to improve the interpretability of segmentation models. In this work, we present a weakly supervised method to generate a healthy version of a diseased image and then use it to obtain a pixel-wise anomaly map. To do so, we start by considering a saliency map that approximately covers the pathological areas, obtained with ACAT. Then, we propose a technique that allows to perform targeted modifications to these regions, while preserving the rest of the image. In particular, we employ a diffusion model trained on healthy samples and combine Denoising Diffusion Probabilistic Model (DDPM) and Denoising Diffusion Implicit Model (DDIM) at each step of the sampling process. DDPM is used to modify the areas affected by a lesion within the saliency map, while DDIM guarantees reconstruction of the normal anatomy outside of it. The two parts are also fused at each timestep, to guarantee the generation of a sample with a coherent appearance and a seamless transition between edited and unedited parts. We verify that when our method is applied to healthy samples, the input images are reconstructed without significant modifications. We compare our approach with alternative weakly supervised methods on IST-3 for stroke lesion segmentation and on BraTS2021 for brain tumour segmentation, where we improve the DICE score of the best competing method from $0.6534$ to $0.7056$.
</details>
<details>
<summary>摘要</summary>
医学应用中的疾病区域分割mask是非常有用的，例如脑肿瘤和中风管理。此外，健康的对比样本可以用来提高放射学家的训练文件，并提高分割模型的解释性。在这种情况下，我们提出了一种弱相关的方法，可以生成一个疾病的健康版本，并使用其生成一个像素层级异常地图。我们的方法的核心思想是使用ACAT获取疾病区域的灵敏度地图，然后使用一种目标修改这些区域的技术，以保持图像的其他部分不受影响。我们使用训练于健康样本的扩散模型（DDPM）和扩散隐藏模型（DDIM），在每个步骤中进行修改和重建。DDPM用于修改疾病区域中的影响区域，而DDIM则 garantiz reconstruction of normal anatomy outside of it。这两个部分还 fusion at each timestep，以确保生成的样本具有一致的外观和无缝过渡。我们证明，当我们的方法应用于健康样本时，输入图像不会经受重要的修改。我们与其他弱相关方法进行比较，在IST-3上进行了肿瘤病变分割和在BraTS2021上进行了脑肿瘤分割，我们提高了最佳竞争方法的DICE分数从0.6534提高到0.7056。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Ki67-ER-PR-and-HER2-Statuses-from-H-E-stained-Breast-Cancer-Images"><a href="#Predicting-Ki67-ER-PR-and-HER2-Statuses-from-H-E-stained-Breast-Cancer-Images" class="headerlink" title="Predicting Ki67, ER, PR, and HER2 Statuses from H&amp;E-stained Breast Cancer Images"></a>Predicting Ki67, ER, PR, and HER2 Statuses from H&amp;E-stained Breast Cancer Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01982">http://arxiv.org/abs/2308.01982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Akbarnejad, Nilanjan Ray, Penny J. Barnes, Gilbert Bigras</li>
<li>for: This paper aims to investigate the accuracy of machine learning methods in predicting molecular information from histomorphology images.</li>
<li>methods: The authors built a large-scale dataset of histomorphology images with reliable measurements for Ki67, ER, PR, and HER2 statuses, and used a standard ViT-based pipeline to train classifiers for predicting these molecular markers.</li>
<li>results: The authors achieved prediction performances around 90% in terms of Area Under the Curve (AUC) when trained with a proper labeling protocol, and demonstrated the ability of the trained classifiers to localize relevant regions.Here is the simplified Chinese translation of the three key points:</li>
<li>for: 本研究旨在探讨机器学习方法是否可以准确地预测 histomorphology 图像中的分子信息。</li>
<li>methods: 作者们建立了一个大规模的 histomorphology 图像 dataset，并使用了标准的 ViT 基于管道来训练类ifiers 以预测 Ki67、ER、PR 和 HER2 状况。</li>
<li>results: 作者们在使用正确的标签协议训练的情况下，达到了约 90% 的预测性能（AUC），并证明了训练好的类ifiers 可以 correctly localize 相关区域。<details>
<summary>Abstract</summary>
Despite the advances in machine learning and digital pathology, it is not yet clear if machine learning methods can accurately predict molecular information merely from histomorphology. In a quest to answer this question, we built a large-scale dataset (185538 images) with reliable measurements for Ki67, ER, PR, and HER2 statuses. The dataset is composed of mirrored images of H\&E and corresponding images of immunohistochemistry (IHC) assays (Ki67, ER, PR, and HER2. These images are mirrored through registration. To increase reliability, individual pairs were inspected and discarded if artifacts were present (tissue folding, bubbles, etc). Measurements for Ki67, ER and PR were determined by calculating H-Score from image analysis. HER2 measurement is based on binary classification: 0 and 1+ (IHC scores representing a negative subset) vs 3+ (IHC score positive subset). Cases with IHC equivocal score (2+) were excluded. We show that a standard ViT-based pipeline can achieve prediction performances around 90% in terms of Area Under the Curve (AUC) when trained with a proper labeling protocol. Finally, we shed light on the ability of the trained classifiers to localize relevant regions, which encourages future work to improve the localizations. Our proposed dataset is publicly available: https://ihc4bc.github.io/
</details>
<details>
<summary>摘要</summary>
尽管机器学习和数字 PATHOLOGY 的进步，仍然没有确定机器学习方法可以准确地预测蛋白质信息仅基于组织结构。为了回答这个问题，我们建立了一个大规模数据集（185538张图像），其中包含可靠的测量结果 для Ki67、ER、PR 和 HER2 状况。这个数据集由 H\&E 和相关的免疫染色试验（Ki67、ER、PR 和 HER2）的图像组成，这些图像通过注册进行镜像。为了增强可靠性，我们检查了每个对并抛弃了包含artefacts（组织折叠、气泡等）的对。我们使用图像分析计算 H-Score 来确定 Ki67、ER 和 PR 的测量结果，而 HER2 的测量基于二分类：0 和 1+（IHC 分数表示负集）vs 3+（IHC 分数正集）。我们排除了 IHC 不确定分数（2+）的 случа。我们显示，使用标准 ViT-based 管道可以在训练 proper 标签协议下达到约 90% 的区域Under the Curve（AUC）的预测性能。最后，我们探讨了训练的分类器是否能够LOCALIZE relevant regions，这种能力鼓励未来的工作进一步提高本地化。我们的提出的数据集现在公开可用：https://ihc4bc.github.io/
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/04/eess.IV_2023_08_04/" data-id="clly4xtg400ehvl88f0ib0iyg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/03/cs.LG_2023_08_03/" class="article-date">
  <time datetime="2023-08-02T16:00:00.000Z" itemprop="datePublished">2023-08-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/03/cs.LG_2023_08_03/">cs.LG - 2023-08-03 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Capability-of-Large-Language-Models-to-Measure-Psychiatric-Functioning"><a href="#The-Capability-of-Large-Language-Models-to-Measure-Psychiatric-Functioning" class="headerlink" title="The Capability of Large Language Models to Measure Psychiatric Functioning"></a>The Capability of Large Language Models to Measure Psychiatric Functioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01834">http://arxiv.org/abs/2308.01834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isaac R. Galatzer-Levy, Daniel McDuff, Vivek Natarajan, Alan Karthikesalingam, Matteo Malgaroli</li>
<li>for:  investigate the capability of Large language models (LLMs) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so.</li>
<li>methods:  using prompts to extract estimated clinical scores and diagnoses based on standardized assessments.</li>
<li>results:  Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments, which were statistically indistinguishable from human clinical raters.<details>
<summary>Abstract</summary>
The current work investigates the capability of Large language models (LLMs) that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so. To assess this, n = 145 depression and n =115 PTSD assessments and n = 46 clinical case studies across high prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma and stress, Addictive disorders) were analyzed using prompts to extract estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which were statistically indistinguishable from human clinical raters t(1,144) = 1.20; p = 0.23. Results show the potential for general clinical language models to flexibly predict psychiatric risk based on free descriptions of functioning from both patients and clinicians.
</details>
<details>
<summary>摘要</summary>
当前研究探讨了大语言模型（LLM）在大量医学知识训练（Med-PaLM 2）下预测患者诊断和精神功能水平，不需要专门训练。为了评估这一点，研究使用了145例带有抑郁症和115例带有PTSD的评估，以及46例临床案例，涵盖高发病率/高混合病率的疾病（抑郁、焦虑、精神病、压力和 стресс、依数病）。结果表明Med-PaLM 2可以评估各种心理疾病的精神功能水平，特别是预测抑郁评估结果（准确率范围=0.80-0.84），这些结果与人类临床评估员的结果 statistically indistinguishable（t(1,144) = 1.20; p = 0.23）。结果表明大规模临床语言模型可以通过自由描述功能来预测心理风险。
</details></li>
</ul>
<hr>
<h2 id="Distribution-Free-Inference-for-the-Regression-Function-of-Binary-Classification"><a href="#Distribution-Free-Inference-for-the-Regression-Function-of-Binary-Classification" class="headerlink" title="Distribution-Free Inference for the Regression Function of Binary Classification"></a>Distribution-Free Inference for the Regression Function of Binary Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01835">http://arxiv.org/abs/2308.01835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ambrus Tamás, Balázs Csanád Csáji</li>
<li>for: 本研究论文的目的是提出一种可靠、分布自由和非假想的预测函数信任区的构建方法，以便在任选的信任水平下确定真实的预测函数。</li>
<li>methods: 本文提出了一种抽样框架，用于构建可靠、分布自由和非假想的预测函数信任区。Specific algorithms are also suggested to demonstrate the framework.</li>
<li>results: 研究证明了构建的信任区是强有效的，即任何不正确的模型都将在长期内被排除，并且这种排除的可靠性被证明为一个可靠的 Probably Approximately Correct (PAC) 类型上下文。<details>
<summary>Abstract</summary>
One of the key objects of binary classification is the regression function, i.e., the conditional expectation of the class labels given the inputs. With the regression function not only a Bayes optimal classifier can be defined, but it also encodes the corresponding misclassification probabilities. The paper presents a resampling framework to construct exact, distribution-free and non-asymptotically guaranteed confidence regions for the true regression function for any user-chosen confidence level. Then, specific algorithms are suggested to demonstrate the framework. It is proved that the constructed confidence regions are strongly consistent, that is, any false model is excluded in the long run with probability one. The exclusion is quantified with probably approximately correct type bounds, as well. Finally, the algorithms are validated via numerical experiments, and the methods are compared to approximate asymptotic confidence ellipsoids.
</details>
<details>
<summary>摘要</summary>
一个重要的二分类问题中的关键对象是回归函数，即输入 conditional 类别标签的预期值。通过回归函数不仅可以定义 Bayes 优化的分类器，还可以表示相应的误分类概率。文章提出了一种抽样框架，可以构造 exact， distribution-free 和非假正极限保证的信任区域，以确定真实的回归函数。然后，文章提供了特定的算法，以示出框架。文章证明了构造的信任区域是strongly consistent，即任何 false model 都会在长期内被排除，并且这种排除的可能性可以通过 probably approximately correct 类型上限来衡量。最后，算法被数学实验 validate，并与approxymatic confidence ellipsoids 进行比较。
</details></li>
</ul>
<hr>
<h2 id="Hard-Adversarial-Example-Mining-for-Improving-Robust-Fairness"><a href="#Hard-Adversarial-Example-Mining-for-Improving-Robust-Fairness" class="headerlink" title="Hard Adversarial Example Mining for Improving Robust Fairness"></a>Hard Adversarial Example Mining for Improving Robust Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01823">http://arxiv.org/abs/2308.01823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenhao Lin, Xiang Ji, Yulong Yang, Qian Li, Chao Shen, Run Wang, Liming Fang</li>
<li>for: 这篇论文的目的是提高深度神经网络的抗对抗性（Adversarial Training，AT），并解决这些模型对于对抗示例（Adversarial Examples，AE）的不公正性问题。</li>
<li>methods: 这篇论文提出了一个简单 yet effective的框架，即适应式强制对抗示例挖掘（HAM），并通过适应性地挖掘强制对抗示例，并将容易的对抗示例早期弃用，以提高AT的效率和公平性。</li>
<li>results: 实验结果显示，这篇论文的HAM方法可以在CIFAR-10、SVHN和Imagenette等三个数据集上实现重要的公平性提升，同时降低了computational cost，比较了state-of-the-art adversarial training方法的效果。<details>
<summary>Abstract</summary>
Adversarial training (AT) is widely considered the state-of-the-art technique for improving the robustness of deep neural networks (DNNs) against adversarial examples (AE). Nevertheless, recent studies have revealed that adversarially trained models are prone to unfairness problems, restricting their applicability. In this paper, we empirically observe that this limitation may be attributed to serious adversarial confidence overfitting, i.e., certain adversarial examples with overconfidence. To alleviate this problem, we propose HAM, a straightforward yet effective framework via adaptive Hard Adversarial example Mining.HAM concentrates on mining hard adversarial examples while discarding the easy ones in an adaptive fashion. Specifically, HAM identifies hard AEs in terms of their step sizes needed to cross the decision boundary when calculating loss value. Besides, an early-dropping mechanism is incorporated to discard the easy examples at the initial stages of AE generation, resulting in efficient AT. Extensive experimental results on CIFAR-10, SVHN, and Imagenette demonstrate that HAM achieves significant improvement in robust fairness while reducing computational cost compared to several state-of-the-art adversarial training methods. The code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
“对抗训练”（AT）是深度神经网络（DNN）对异常示例（AE）的状态艺术技术。然而，最近的研究发现，对抗训练模型具有不公平问题，限制其应用。在这篇论文中，我们直观地发现这一问题可能归结于严重的对抗信任过拟合问题，即某些对抗示例具有过分信任。为了解决这个问题，我们提出了HAM，一种简单 yet有效的框架，通过适应式硬对抗示例挖掘来解决这个问题。HAM会在计算损失值时将硬对抗示例与易于攻击的示例进行分离，并在早期阶段使用早期释出机制来优化AT。实验结果表明，HAM在CIFAR-10、SVHN和Imagenette上实现了显著的公平性提升，同时降低了计算成本，相比于一些现状最佳的对抗训练方法。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Programs-IVb-Adaptive-Optimization-in-the-Infinite-Width-Limit"><a href="#Tensor-Programs-IVb-Adaptive-Optimization-in-the-Infinite-Width-Limit" class="headerlink" title="Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit"></a>Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01814">http://arxiv.org/abs/2308.01814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Greg Yang, Etai Littwin</li>
<li>for: 这篇论文探讨了在宽神经网络中使用自适应优化器 like Adam 以外的新现象，包括批处理矩阵和核函数行为。</li>
<li>methods: 该论文使用了一种新的张量编程语言 NEXORT，以及 bras-ket notation，以描述如何使用 adaptive optimizers 处理梯度并生成更新。</li>
<li>results: 论文显示了在宽神经网络中使用 Adam 优化器时，存在类似于梯度下降优化器的特征学习和核函数行为，并提供了对这些行为的分析和总结。<details>
<summary>Abstract</summary>
Going beyond stochastic gradient descent (SGD), what new phenomena emerge in wide neural networks trained by adaptive optimizers like Adam? Here we show: The same dichotomy between feature learning and kernel behaviors (as in SGD) holds for general optimizers as well, including Adam -- albeit with a nonlinear notion of "kernel." We derive the corresponding "neural tangent" and "maximal update" limits for any architecture. Two foundational advances underlie the above results: 1) A new Tensor Program language, NEXORT, that can express how adaptive optimizers process gradients into updates. 2) The introduction of bra-ket notation to drastically simplify expressions and calculations in Tensor Programs. This work summarizes and generalizes all previous results in the Tensor Programs series of papers.
</details>
<details>
<summary>摘要</summary>
SGD 以外，在宽神经网络中使用自适应优化器如 Adam 的训练中，新的现象出现了什么？我们表明：SGD 中的特征学习和核函数行为之 dichotomy 也存在于总的优化器中，包括 Adam，但是它们是非线性的。我们 derivates 对应的 "神经折射" 和 "最大更新" 限制，对于任何架构都成立。这两个基本进展是：1）一种新的tensor program语言，NEXORT，可以表示如何自适应优化器将梯度转化为更新。2）在tensor program中引入bra-ket表示法，以简化表达和计算。这些成果总结了以前在tensor program series中的所有结果。
</details></li>
</ul>
<hr>
<h2 id="Job-Shop-Scheduling-via-Deep-Reinforcement-Learning-a-Sequence-to-Sequence-approach"><a href="#Job-Shop-Scheduling-via-Deep-Reinforcement-Learning-a-Sequence-to-Sequence-approach" class="headerlink" title="Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach"></a>Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01797">http://arxiv.org/abs/2308.01797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dawoz/JSP-DeepRL-Seq2Seq">https://github.com/dawoz/JSP-DeepRL-Seq2Seq</a></li>
<li>paper_authors: Giovanni Bonetta, Davide Zago, Rossella Cancelliere, Andrea Grosso</li>
<li>for: 本研究旨在提出一种基于深度学习的Job调度算法，可以自动学习调度规则。</li>
<li>methods: 本研究使用了自然语言编码器-解码器模型，并在Job Shop问题的benchmark实例上进行了测试。</li>
<li>results: 研究结果显示，我们的方法可以超过许多传统的优先级调度规则，并与当前最佳深度学习方法相当竞争。<details>
<summary>Abstract</summary>
Job scheduling is a well-known Combinatorial Optimization problem with endless applications. Well planned schedules bring many benefits in the context of automated systems: among others, they limit production costs and waste. Nevertheless, the NP-hardness of this problem makes it essential to use heuristics whose design is difficult, requires specialized knowledge and often produces methods tailored to the specific task. This paper presents an original end-to-end Deep Reinforcement Learning approach to scheduling that automatically learns dispatching rules. Our technique is inspired by natural language encoder-decoder models for sequence processing and has never been used, to the best of our knowledge, for scheduling purposes. We applied and tested our method in particular to some benchmark instances of Job Shop Problem, but this technique is general enough to be potentially used to tackle other different optimal job scheduling tasks with minimal intervention. Results demonstrate that we outperform many classical approaches exploiting priority dispatching rules and show competitive results on state-of-the-art Deep Reinforcement Learning ones.
</details>
<details>
<summary>摘要</summary>
This paper presents an original end-to-end deep reinforcement learning approach to job scheduling that automatically learns dispatching rules. Our technique is inspired by natural language encoder-decoder models for sequence processing and has never been used, to the best of our knowledge, for scheduling purposes. We applied and tested our method on some benchmark instances of the Job Shop Problem, but it is general enough to be potentially used to tackle other different optimal job scheduling tasks with minimal intervention.The results demonstrate that we outperform many classical approaches that use priority dispatching rules and show competitive results with state-of-the-art deep reinforcement learning methods.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Adaptative-Variational-Quantum-Algorithms-on-QUBO-Instances"><a href="#Benchmarking-Adaptative-Variational-Quantum-Algorithms-on-QUBO-Instances" class="headerlink" title="Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances"></a>Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01789">http://arxiv.org/abs/2308.01789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gloria Turati, Maurizio Ferrari Dacrema, Paolo Cremonesi</li>
<li>for: 这篇论文主要是为了研究可变量量量算法（Adaptative VQAs），以解决在量子计算机NISQ时代中的优化问题。</li>
<li>methods: 这篇论文比较了三种 Adaptative VQAs：EVQE、VAns 和 RA-VQE，以及传统的量子近似优化算法（QAOA）。这些算法都是基于不同的启发，如环境深度、共轭能力和硬件兼容性，来动态修改环境的电路结构和参数。</li>
<li>results: 论文通过应用这些算法解决 QUBO 问题，并研究了这些算法的性能，包括解决的问题质量和计算时间。此外，论文还检查了不同的超参数选择方法对算法的总性能的影响，提出了选择合适方法进行超参数调整的重要性。<details>
<summary>Abstract</summary>
In recent years, Variational Quantum Algorithms (VQAs) have emerged as a promising approach for solving optimization problems on quantum computers in the NISQ era. However, one limitation of VQAs is their reliance on fixed-structure circuits, which may not be taylored for specific problems or hardware configurations. A leading strategy to address this issue are Adaptative VQAs, which dynamically modify the circuit structure by adding and removing gates, and optimize their parameters during the training. Several Adaptative VQAs, based on heuristics such as circuit shallowness, entanglement capability and hardware compatibility, have already been proposed in the literature, but there is still lack of a systematic comparison between the different methods. In this paper, we aim to fill this gap by analyzing three Adaptative VQAs: Evolutionary Variational Quantum Eigensolver (EVQE), Variable Ansatz (VAns), already proposed in the literature, and Random Adapt-VQE (RA-VQE), a random approach we introduce as a baseline. In order to compare these algorithms to traditional VQAs, we also include the Quantum Approximate Optimization Algorithm (QAOA) in our analysis. We apply these algorithms to QUBO problems and study their performance by examining the quality of the solutions found and the computational times required. Additionally, we investigate how the choice of the hyperparameters can impact the overall performance of the algorithms, highlighting the importance of selecting an appropriate methodology for hyperparameter tuning. Our analysis sets benchmarks for Adaptative VQAs designed for near-term quantum devices and provides valuable insights to guide future research in this area.
</details>
<details>
<summary>摘要</summary>
近年来，变量量子算法（VQA）在量子计算机NISQ时代 emerged as a promising approach for solving optimization problems. However, one limitation of VQAs is their reliance on fixed-structure circuits, which may not be tailored for specific problems or hardware configurations. To address this issue, adaptive VQAs have been proposed, which dynamically modify the circuit structure and optimize parameters during training. Several adaptive VQAs have been proposed based on heuristics such as circuit shallowness, entanglement capability, and hardware compatibility. However, there is still a lack of a systematic comparison between the different methods.In this paper, we aim to fill this gap by analyzing three adaptive VQAs: Evolutionary Variational Quantum Eigensolver (EVQE), Variable Ansatz (VAns), and Random Adapt-VQE (RA-VQE), as well as the Quantum Approximate Optimization Algorithm (QAOA) for comparison. We apply these algorithms to QUBO problems and study their performance by examining the quality of the solutions found and the computational times required. Additionally, we investigate the impact of hyperparameter choice on the overall performance of the algorithms, highlighting the importance of selecting an appropriate methodology for hyperparameter tuning. Our analysis sets benchmarks for adaptive VQAs designed for near-term quantum devices and provides valuable insights to guide future research in this area.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-based-Prediction-of-Stress-and-Strain-Maps-in-Arterial-Walls-for-Improved-Cardiovascular-Risk-Assessment"><a href="#Deep-Learning-based-Prediction-of-Stress-and-Strain-Maps-in-Arterial-Walls-for-Improved-Cardiovascular-Risk-Assessment" class="headerlink" title="Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment"></a>Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01771">http://arxiv.org/abs/2308.01771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasin Shokrollahi1, Pengfei Dong1, Xianqi Li, Linxia Gu<br>for: This study aimed to develop a surrogate model for finite element analysis to predict stress-strain fields within 2D cross sections of arterial walls, which could replace traditional FEM methods and be more effective and efficient.methods: The study used a U-Net based fully convolutional neural network (CNN) and a conditional generative adversarial network (cGAN) to predict the von Mises stress and strain distribution, and also proposed their ensemble approaches to further improve the prediction accuracy.results: The trained U-Net models and cGAN models demonstrated high accuracy in predicting von Mises stress and strain fields, with SSIM scores of 0.854 and 0.830, and mean squared errors of 0.017 and 0.018 for stress and strain, respectively. The ensemble and transfer learning techniques also showed high accuracy, with SSIM scores of 0.890 for stress and 0.803 for strain, and mean squared errors of 0.008 for stress and 0.017 for strain.<details>
<summary>Abstract</summary>
This study investigated the potential of end-to-end deep learning tools as a more effective substitute for FEM in predicting stress-strain fields within 2D cross sections of arterial wall. We first proposed a U-Net based fully convolutional neural network (CNN) to predict the von Mises stress and strain distribution based on the spatial arrangement of calcification within arterial wall cross-sections. Further, we developed a conditional generative adversarial network (cGAN) to enhance, particularly from the perceptual perspective, the prediction accuracy of stress and strain field maps for arterial walls with various calcification quantities and spatial configurations. On top of U-Net and cGAN, we also proposed their ensemble approaches, respectively, to further improve the prediction accuracy of field maps. Our dataset, consisting of input and output images, was generated by implementing boundary conditions and extracting stress-strain field maps. The trained U-Net models can accurately predict von Mises stress and strain fields, with structural similarity index scores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 for stress and strain, respectively, on a reserved test set. Meanwhile, the cGAN models in a combination of ensemble and transfer learning techniques demonstrate high accuracy in predicting von Mises stress and strain fields, as evidenced by SSIM scores of 0.890 for stress and 0.803 for strain. Additionally, mean squared errors of 0.008 for stress and 0.017 for strain further support the model's performance on a designated test set. Overall, this study developed a surrogate model for finite element analysis, which can accurately and efficiently predict stress-strain fields of arterial walls regardless of complex geometries and boundary conditions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Bag-of-Policies-for-Distributional-Deep-Exploration"><a href="#Bag-of-Policies-for-Distributional-Deep-Exploration" class="headerlink" title="Bag of Policies for Distributional Deep Exploration"></a>Bag of Policies for Distributional Deep Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01759">http://arxiv.org/abs/2308.01759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asen Nachkov, Luchen Li, Giulia Luise, Filippo Valdettaro, Aldo Faisal</li>
<li>for: 提高复杂环境中RL的效率探索</li>
<li>methods: 使用Bag of Policies（BoP）方法，其包括多个独立更新的头部，每个头在每个话题中控制一集state-action对，并使用这些对来更新所有头部</li>
<li>results: 通过实验证明，BoP方法可以提高RL在ALE Atari游戏中的robustness和速度<details>
<summary>Abstract</summary>
Efficient exploration in complex environments remains a major challenge for reinforcement learning (RL). Compared to previous Thompson sampling-inspired mechanisms that enable temporally extended exploration, i.e., deep exploration, we focus on deep exploration in distributional RL. We develop here a general purpose approach, Bag of Policies (BoP), that can be built on top of any return distribution estimator by maintaining a population of its copies. BoP consists of an ensemble of multiple heads that are updated independently. During training, each episode is controlled by only one of the heads and the collected state-action pairs are used to update all heads off-policy, leading to distinct learning signals for each head which diversify learning and behaviour. To test whether optimistic ensemble method can improve on distributional RL as did on scalar RL, by e.g. Bootstrapped DQN, we implement the BoP approach with a population of distributional actor-critics using Bayesian Distributional Policy Gradients (BDPG). The population thus approximates a posterior distribution of return distributions along with a posterior distribution of policies. Another benefit of building upon BDPG is that it allows to analyze global posterior uncertainty along with local curiosity bonus simultaneously for exploration. As BDPG is already an optimistic method, this pairing helps to investigate if optimism is accumulatable in distributional RL. Overall BoP results in greater robustness and speed during learning as demonstrated by our experimental results on ALE Atari games.
</details>
<details>
<summary>摘要</summary>
RL中的有效探索仍然是一个主要挑战。与前期的汤姆逊探索机制相比，我们在分布RL中强调深入探索。我们开发了一个通用的方法，即袋子策略（Bag of Policies，BoP），它可以基于任何返回分布估计器建立。BoP包括多个独立更新的头，每个头控制一个episode，并将收集的状态-动作对用于所有头上不同策略进行off-policy更新，从而生成多个不同的学习信号，使得学习和行为更加多样化。为了测试optimistic ensemble方法在分布RL中是否能够提高性能，我们实现了BoP方法，使用了一个分布 actor-critic 的人工 intel 拟合 Bayesian Distributional Policy Gradients（BDPG）。这个人工 intel Population Approximates posterior distribution of return distributions and posterior distribution of policies。此外，由于BDPG已经是一个optimistic方法，这种结合可以同时分析分布RL中的全局 posterior uncertainty和本地好奇购买奖励。实验结果表明，BoP在ALE Atari游戏中显示出更高的稳定性和速度。
</details></li>
</ul>
<hr>
<h2 id="Guided-Distillation-for-Semi-Supervised-Instance-Segmentation"><a href="#Guided-Distillation-for-Semi-Supervised-Instance-Segmentation" class="headerlink" title="Guided Distillation for Semi-Supervised Instance Segmentation"></a>Guided Distillation for Semi-Supervised Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02668">http://arxiv.org/abs/2308.02668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq Berrada, Camille Couprie, Karteek Alahari, Jakob Verbeek</li>
<li>for: 提高Instance Segmentation的表现，减少完全监督图像的需求</li>
<li>methods: 使用 semi-supervised 方法，利用无标注数据作为训练信号，限制过拟合到标注样本</li>
<li>results: 提高 teacher-student 抽象模型的表现，在 Cityscapes 和 COCO 数据集上提高 mask-AP 的数值，例如在 Cityscapes 数据集上提高 mask-AP 从 23.7 到 33.9，在 COCO 数据集上提高 mask-AP 从 18.3 到 34.1<details>
<summary>Abstract</summary>
Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel "guided burn-in" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on the Cityscapes dataset we improve mask-AP from 23.7 to 33.9 when using labels for 10\% of images, and on the COCO dataset we improve mask-AP from 18.3 to 34.1 when using labels for only 1\% of the training data.
</details>
<details>
<summary>摘要</summary>
尽管实例分割方法已经有了很大的进步，但主流方法仍然是基于完全标注的训练图像，这是获取标注数据的很 tedious 和耗时的过程。为了解决这个问题，并提高结果，半supervised方法利用无标注数据作为额外的训练信号，以避免过拟合标注样本。在这个上下文中，我们提出了新的设计选择，以提高教师学生热键扩展模型。具体来说，我们（i）改进了热键扩展approach，通过引入新的“导航燃烧”阶段，以及（ii）评估不同的实例分割架构、后备网络和预训练策略。与前一些工作一样，我们只使用supervised数据进行学生模型的热键期，但我们还使用教师模型的指导来利用无标注数据，从而在热键期内进行学习。我们改进的热键扩展方法导致了substantial提高，比如在Cityscapes数据集上，我们提高了mask-AP从23.7到33.9，并在COCO数据集上提高了mask-AP从18.3到34.1，只使用标注数据的1%。
</details></li>
</ul>
<hr>
<h2 id="Neural-Collapse-Terminus-A-Unified-Solution-for-Class-Incremental-Learning-and-Its-Variants"><a href="#Neural-Collapse-Terminus-A-Unified-Solution-for-Class-Incremental-Learning-and-Its-Variants" class="headerlink" title="Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants"></a>Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01746">http://arxiv.org/abs/2308.01746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuralcollapseapplications/unicil">https://github.com/neuralcollapseapplications/unicil</a></li>
<li>paper_authors: Yibo Yang, Haobo Yuan, Xiangtai Li, Jianlong Wu, Lefei Zhang, Zhouchen Lin, Philip Torr, Dacheng Tao, Bernard Ghanem<br>for: 这篇论文的目的是解决在新类incremental learning中保持老类能力的问题，包括长尾类增量学习和几架shot类增量学习，这些问题在实际应用中非常普遍，并且使得旧类能力衰退问题更加严重。methods: 这篇论文提出了一个统一的解决方案，即神经坍缩终点（Neural Collapse Terminus，NCT），这是一个固定的结构，具有整个标签空间中最大的等角对称分类分布。NCT  acted as a consistent target throughout the incremental training, 以避免在增量训练中分配特征空间。results: 实验结果显示，这篇论文的方法可以在多个数据集上进行实际应用，并且在增量训练中保持旧类能力，同时在新类 incremental learning 中获得良好的性能。实验结果还显示，这篇论文的方法可以应对实际应用中的数据不均匀和数据缺乏问题，并且可以在不知道总共有多少个类别和数据分布是否正常、长尾或几架shot的情况下进行通用化。<details>
<summary>Abstract</summary>
How to enable learnability for new classes while keeping the capability well on old classes has been a crucial challenge for class incremental learning. Beyond the normal case, long-tail class incremental learning and few-shot class incremental learning are also proposed to consider the data imbalance and data scarcity, respectively, which are common in real-world implementations and further exacerbate the well-known problem of catastrophic forgetting. Existing methods are specifically proposed for one of the three tasks. In this paper, we offer a unified solution to the misalignment dilemma in the three tasks. Concretely, we propose neural collapse terminus that is a fixed structure with the maximal equiangular inter-class separation for the whole label space. It serves as a consistent target throughout the incremental training to avoid dividing the feature space incrementally. For CIL and LTCIL, we further propose a prototype evolving scheme to drive the backbone features into our neural collapse terminus smoothly. Our method also works for FSCIL with only minor adaptations. Theoretical analysis indicates that our method holds the neural collapse optimality in an incremental fashion regardless of data imbalance or data scarcity. We also design a generalized case where we do not know the total number of classes and whether the data distribution is normal, long-tail, or few-shot for each coming session, to test the generalizability of our method. Extensive experiments with multiple datasets are conducted to demonstrate the effectiveness of our unified solution to all the three tasks and the generalized case.
</details>
<details>
<summary>摘要</summary>
如何维护新类的学习能力而不损害老类的能力是泛cremental learning中的一个关键挑战。此外，我们还考虑了实际情况中的数据不均衡和数据罕见性，并提出了长尾类增量学习和少数shot类增量学习两种方法。现有的方法主要针对一个任务。在这篇论文中，我们提出了增量训练中的不一致问题的统一解决方案。具体来说，我们提出了一个固定结构的神经溃终点，该结构具有整个标签空间中最大的等角间距。它在增量训练中作为不变的目标，以避免在增量训练中分割特征空间。为CIL和LTCIL任务，我们进一步提出了一种prototype evolving scheme来顺略地将后ION抽象到我们的神经溃终点中。我们的方法也适用于FSCIL任务，只需要小量的修改。理论分析表明，我们的方法在增量训练中保持了神经溃优化的优化，不受数据不均衡或数据罕见性的影响。我们还设计了一种通用情况，在每个来来Session中不知道总共有多少类和每个数据分布是正常、长尾或少数shot，以测试我们的方法的通用性。我们进行了多个数据集的广泛实验，以证明我们的统一解决方案对所有三个任务和通用情况具有效果。
</details></li>
</ul>
<hr>
<h2 id="Multitask-Learning-with-No-Regret-from-Improved-Confidence-Bounds-to-Active-Learning"><a href="#Multitask-Learning-with-No-Regret-from-Improved-Confidence-Bounds-to-Active-Learning" class="headerlink" title="Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning"></a>Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01744">http://arxiv.org/abs/2308.01744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pier Giuseppe Sessa, Pierre Laforgue, Nicolò Cesa-Bianchi, Andreas Krause<br>for: 这个论文的目的是提供一种在不知道任务之间相似性的情况下，实现多任务学习的信任度范围，以及一种基于这些信任度范围的在线学习算法。methods: 该论文使用了多任务信任度范围的新研究，通过对多任务信息增量进行细化分析，提供了新的 regret保证，具有任务相似性参数的依赖性。results: 该论文提出了一种自动地适应任务相似性的在线学习算法，并在synthetic和实际世界数据上进行了实验验证，证明了其bounds和算法的有效性。<details>
<summary>Abstract</summary>
Multitask learning is a powerful framework that enables one to simultaneously learn multiple related tasks by sharing information between them. Quantifying uncertainty in the estimated tasks is of pivotal importance for many downstream applications, such as online or active learning. In this work, we provide novel multitask confidence intervals in the challenging agnostic setting, i.e., when neither the similarity between tasks nor the tasks' features are available to the learner. The obtained intervals do not require i.i.d. data and can be directly applied to bound the regret in online learning. Through a refined analysis of the multitask information gain, we obtain new regret guarantees that, depending on a task similarity parameter, can significantly improve over treating tasks independently. We further propose a novel online learning algorithm that achieves such improved regret without knowing this parameter in advance, i.e., automatically adapting to task similarity. As a second key application of our results, we introduce a novel multitask active learning setup where several tasks must be simultaneously optimized, but only one of them can be queried for feedback by the learner at each round. For this problem, we design a no-regret algorithm that uses our confidence intervals to decide which task should be queried. Finally, we empirically validate our bounds and algorithms on synthetic and real-world (drug discovery) data.
</details>
<details>
<summary>摘要</summary>
多任务学习是一个强大的框架，它允许一个模型同时学习多个相关的任务，并在这些任务之间共享信息。在许多下游应用中，量化任务估计中的不确定性是非常重要的，例如在线学习或活动学习中。在这项工作中，我们提供了新的多任务信任范围，它在无相似性信息和任务特征信息的情况下提供，并且可以直接应用于 bound 在线学习中的 regret。通过对多任务信息增量进行细化分析，我们获得了新的 regret 保证，它们可以在任务相似度参数的情况下显著改进于独立处理任务。此外，我们还提出了一种新的在线学习算法，它可以在不知道任务相似度参数的情况下实现改进的 regret。作为第二个关键应用，我们引入了一种多任务活动学习设置，在这个设置中，学习器需要同时优化多个任务，但只有一个任务可以在每次轮次中被学习器请求反馈。为解决这个问题，我们设计了一种无损算法，它使用我们的信任范围来决定哪个任务应该被请求反馈。 finally，我们employnull对我们的 bound 和算法进行了实验 validate 。
</details></li>
</ul>
<hr>
<h2 id="Finding-the-Optimum-Design-of-Large-Gas-Engines-Prechambers-Using-CFD-and-Bayesian-Optimization"><a href="#Finding-the-Optimum-Design-of-Large-Gas-Engines-Prechambers-Using-CFD-and-Bayesian-Optimization" class="headerlink" title="Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization"></a>Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01743">http://arxiv.org/abs/2308.01743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Posch, Clemens Gößnitzer, Franz Rohrhofer, Bernhard C. Geiger, Andreas Wimmer</li>
<li>for: 大功率液化燃料发动机中的紧急液喷启火概念，以实现在副气比较低的情况下稳定燃烧，从而提高效率并降低排放。</li>
<li>methods: 计算流体动力学（CFD）模拟，用于评估不同设计参数下的启火器设计。</li>
<li>results: 使用欧几里得法则精度的Reynolds均值 Navier-Stokes  simulations来确定选择启火器设计参数时的目标值。结果表明选择的策略是有效地找到符合目标值的启火器设计。<details>
<summary>Abstract</summary>
The turbulent jet ignition concept using prechambers is a promising solution to achieve stable combustion at lean conditions in large gas engines, leading to high efficiency at low emission levels. Due to the wide range of design and operating parameters for large gas engine prechambers, the preferred method for evaluating different designs is computational fluid dynamics (CFD), as testing in test bed measurement campaigns is time-consuming and expensive. However, the significant computational time required for detailed CFD simulations due to the complexity of solving the underlying physics also limits its applicability. In optimization settings similar to the present case, i.e., where the evaluation of the objective function(s) is computationally costly, Bayesian optimization has largely replaced classical design-of-experiment. Thus, the present study deals with the computationally efficient Bayesian optimization of large gas engine prechambers design using CFD simulation. Reynolds-averaged-Navier-Stokes simulations are used to determine the target values as a function of the selected prechamber design parameters. The results indicate that the chosen strategy is effective to find a prechamber design that achieves the desired target values.
</details>
<details>
<summary>摘要</summary>
大型气Engine预室设计使用液体喷射技术可能是实现稳定燃烧在质量低的情况下高效燃烧的有望解决方案。由于大型气Engine预室设计和运行参数的范围很广，因此计算流体力学（CFD）是评估不同设计的首选方法，因为测试床测量campaign是时间consuming和昂贵的。然而，由于解决下面的物理学问题的复杂度，详细的CFD simulations也有限制其应用。在优化设定中，例如现在的案例，i.e., where the evaluation of the objective function(s) is computationally costly, Bayesian optimization has largely replaced classical design-of-experiment。因此，本研究关注计算效率高的抽象归一化优化大型气Engine预室设计使用CFD simulations。Reynolds-averaged-Navier-Stokes simulations are used to determine the target values as a function of the selected prechamber design parameters. The results indicate that the chosen strategy is effective to find a prechamber design that achieves the desired target values.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Multi-Label-Correlation-in-Label-Distribution-Learning"><a href="#Exploiting-Multi-Label-Correlation-in-Label-Distribution-Learning" class="headerlink" title="Exploiting Multi-Label Correlation in Label Distribution Learning"></a>Exploiting Multi-Label Correlation in Label Distribution Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01742">http://arxiv.org/abs/2308.01742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqiang Kou jing wang yuheng jia xin geng</li>
<li>for: 本研究旨在提出一种新的机器学习模式即分布式标签学习（LDL），以便解决机器学习问题中的指标空间呈指数级别的问题。</li>
<li>methods: 本研究采用了一种新的方法，即通过在多个标签学习（MLL）中采用低级别标签相互关系来捕捉标签相互关系。</li>
<li>results: 经过对比分析，本研究发现了现有LDL方法的缺陷，并提出了一种新的方法，即通过在MLL中采用低级别标签相互关系来捕捉标签相互关系，从而提高LDL方法的性能。<details>
<summary>Abstract</summary>
Label Distribution Learning (LDL) is a novel machine learning paradigm that assigns label distribution to each instance. Many LDL methods proposed to leverage label correlation in the learning process to solve the exponential-sized output space; among these, many exploited the low-rank structure of label distribution to capture label correlation. However, recent studies disclosed that label distribution matrices are typically full-rank, posing challenges to those works exploiting low-rank label correlation. Note that multi-label is generally low-rank; low-rank label correlation is widely adopted in multi-label learning (MLL) literature. Inspired by that, we introduce an auxiliary MLL process in LDL and capture low-rank label correlation on that MLL rather than LDL. In such a way, low-rank label correlation is appropriately exploited in our LDL methods. We conduct comprehensive experiments and demonstrate that our methods are superior to existing LDL methods. Besides, the ablation studies justify the advantages of exploiting low-rank label correlation in the auxiliary MLL.
</details>
<details>
<summary>摘要</summary>
标签分布学习（LDL）是一种新的机器学习方案，它将标签分布分配给每个实例。许多LDL方法尝试利用标签关系来解决不同输出空间的问题，其中许多方法利用标签分布的低级结构来捕捉标签关系。然而，最近的研究发现，标签分布矩阵通常是全级结构的，这对于那些利用低级标签关系的方法带来了挑战。注意，多标签通常是低级的，低级标签关系广泛采用在多标签学习（MLL）文献中。针对这一点，我们引入了一个辅助的多标签学习过程（MLL），并在这个MLL上捕捉低级标签关系。这样做的优点是，我们可以正确地利用低级标签关系在LDL中。我们进行了广泛的实验，并证明了我们的方法比现有的LDL方法更高效。此外，缺省研究证明了在辅助MLL中利用低级标签关系的优势。
</details></li>
</ul>
<hr>
<h2 id="Bringing-Chemistry-to-Scale-Loss-Weight-Adjustment-for-Multivariate-Regression-in-Deep-Learning-of-Thermochemical-Processes"><a href="#Bringing-Chemistry-to-Scale-Loss-Weight-Adjustment-for-Multivariate-Regression-in-Deep-Learning-of-Thermochemical-Processes" class="headerlink" title="Bringing Chemistry to Scale: Loss Weight Adjustment for Multivariate Regression in Deep Learning of Thermochemical Processes"></a>Bringing Chemistry to Scale: Loss Weight Adjustment for Multivariate Regression in Deep Learning of Thermochemical Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01954">http://arxiv.org/abs/2308.01954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franz M. Rohrhofer, Stefan Posch, Clemens Gößnitzer, José M. García-Oliver, Bernhard C. Geiger</li>
<li>for: 本研究旨在改进人工神经网络（ANN）在多变量回归任务中学习多种物质质量分布。</li>
<li>methods: 本研究使用了一种简单 yet effective的损失重量调整，以提高ANN在学习多种物质质量分布时的准确性。</li>
<li>results: 研究发现，这种损失重量调整可以使ANN更加准确地学习所有物质质量分布，包括次要物质的质量分布，而标准的平均方差优化则完全失败。此外，调整后的损失重量使得网络训练过程中的梯度更加均衡，这解释了其效果。<details>
<summary>Abstract</summary>
Flamelet models are widely used in computational fluid dynamics to simulate thermochemical processes in turbulent combustion. These models typically employ memory-expensive lookup tables that are predetermined and represent the combustion process to be simulated. Artificial neural networks (ANNs) offer a deep learning approach that can store this tabular data using a small number of network weights, potentially reducing the memory demands of complex simulations by orders of magnitude. However, ANNs with standard training losses often struggle with underrepresented targets in multivariate regression tasks, e.g., when learning minor species mass fractions as part of lookup tables. This paper seeks to improve the accuracy of an ANN when learning multiple species mass fractions of a hydrogen (\ce{H2}) combustion lookup table. We assess a simple, yet effective loss weight adjustment that outperforms the standard mean-squared error optimization and enables accurate learning of all species mass fractions, even of minor species where the standard optimization completely fails. Furthermore, we find that the loss weight adjustment leads to more balanced gradients in the network training, which explains its effectiveness.
</details>
<details>
<summary>摘要</summary>
法则模型广泛用于计算流体动力学来模拟热化学过程，以便在液体燃烧中预测燃烧过程。这些模型通常使用占用内存的lookup表，这些表示燃烧过程要模拟。人工神经网络（ANNs）提供了深度学习方法，可以将这些表格数据存储在小数量的网络参数中，从而可能减少复杂的计算模拟中的内存需求。然而，标准训练损失通常在多变量回归任务中struggle with underrepresented targets，例如在学习某些小分子质量 Fraction as part of lookup tables。本文旨在提高ANN在学习多种种质量 Fraction的氢（\ce{H2）}燃烧lookup表时的准确性。我们评估了一个简单，却有效的权重调整，该超过标准的平均方差优化，使得网络学习所有种质量 Fraction，包括次要种的质量 Fraction，其标准优化完全失败。此外，我们发现权重调整导致了网络训练中的更加平衡的梯度，这解释了其效果。
</details></li>
</ul>
<hr>
<h2 id="MAP-A-Model-agnostic-Pretraining-Framework-for-Click-through-Rate-Prediction"><a href="#MAP-A-Model-agnostic-Pretraining-Framework-for-Click-through-Rate-Prediction" class="headerlink" title="MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction"></a>MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01737">http://arxiv.org/abs/2308.01737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chiangel/map-code">https://github.com/chiangel/map-code</a></li>
<li>paper_authors: Jianghao Lin, Yanru Qu, Wei Guo, Xinyi Dai, Ruiming Tang, Yong Yu, Weinan Zhang</li>
<li>for: 这篇论文主要针对的是 clicked-through rate（CTR）预测，尤其是在大规模线上个人化服务中，为了提高CTR预测的精度和效率。</li>
<li>methods: 这篇论文提出了一个基于自动学习的预备架构（MAP），并提出了两种实用的算法：伪设对于每个实例中的特征进行隐藏和预测（Masked Feature Prediction，MFP），以及替换特征的检测（Replaced Feature Detection，RFD）。这些算法可以对大规模的用户点击logs进行自动学习，以提高CTR预测的精度和效率。</li>
<li>results: 根据该论文的实验结果，这两种算法可以在两个真实世界的大规模数据集（Avazu和Criteo）上达到新的州度测试表现，并在多个强大的后置模型（例如DCNv2和DeepFM）上显示出比较好的效果和效率。<details>
<summary>Abstract</summary>
With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a Model-agnostic pretraining (MAP) framework that applies feature corruption and recovery on multi-field categorical data, and more specifically, we derive two practical algorithms: masked feature prediction (MFP) and replaced feature detection (RFD). MFP digs into feature interactions within each instance through masking and predicting a small portion of input features, and introduces noise contrastive estimation (NCE) to handle large feature spaces. RFD further turns MFP into a binary classification mode through replacing and detecting changes in input features, making it even simpler and more effective for CTR pretraining. Our extensive experiments on two real-world large-scale datasets (i.e., Avazu, Criteo) demonstrate the advantages of these two methods on several strong backbones (e.g., DCNv2, DeepFM), and achieve new state-of-the-art performance in terms of both effectiveness and efficiency for CTR prediction.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a Model-agnostic pretraining (MAP) framework that applies feature corruption and recovery on multi-field categorical data. Specifically, we derive two practical algorithms: masked feature prediction (MFP) and replaced feature detection (RFD). MFP digs into feature interactions within each instance through masking and predicting a small portion of input features, and introduces noise contrastive estimation (NCE) to handle large feature spaces. RFD further turns MFP into a binary classification mode through replacing and detecting changes in input features, making it even simpler and more effective for CTR pretraining.Our extensive experiments on two real-world large-scale datasets (i.e., Avazu, Criteo) demonstrate the advantages of these two methods on several strong backbones (e.g., DCNv2, DeepFM), and achieve new state-of-the-art performance in terms of both effectiveness and efficiency for CTR prediction.
</details></li>
</ul>
<hr>
<h2 id="Quantification-of-Predictive-Uncertainty-via-Inference-Time-Sampling"><a href="#Quantification-of-Predictive-Uncertainty-via-Inference-Time-Sampling" class="headerlink" title="Quantification of Predictive Uncertainty via Inference-Time Sampling"></a>Quantification of Predictive Uncertainty via Inference-Time Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01731">http://arxiv.org/abs/2308.01731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katarína Tóthová, Ľubor Ladický, Daniel Thul, Marc Pollefeys, Ender Konukoglu</li>
<li>for: 这项研究旨在解决数据uncertainty导致预测不确定性的问题，提出了一种后期采样策略来估计预测不确定性。</li>
<li>methods: 该方法不需要特定的建筑Component或训练机制，可以应用于任何具有Feed-Forward Deterministic Network的模型，无需改变建筑或训练过程。</li>
<li>results: 实验结果表明，该方法可以生成多种可能性 distributions，与预测错误之间存在良好的相关性。<details>
<summary>Abstract</summary>
Predictive variability due to data ambiguities has typically been addressed via construction of dedicated models with built-in probabilistic capabilities that are trained to predict uncertainty estimates as variables of interest. These approaches require distinct architectural components and training mechanisms, may include restrictive assumptions and exhibit overconfidence, i.e., high confidence in imprecise predictions. In this work, we propose a post-hoc sampling strategy for estimating predictive uncertainty accounting for data ambiguity. The method can generate different plausible outputs for a given input and does not assume parametric forms of predictive distributions. It is architecture agnostic and can be applied to any feed-forward deterministic network without changes to the architecture or training procedure. Experiments on regression tasks on imaging and non-imaging input data show the method's ability to generate diverse and multi-modal predictive distributions, and a desirable correlation of the estimated uncertainty with the prediction error.
</details>
<details>
<summary>摘要</summary>
通常情况下，预测变化因数据 ambiguity 被通过建立专门的模型，这些模型具有内置的 probabilistic 能力，并通过训练来预测不确定性估计。这些方法可能需要特定的体系结构和训练机制，并且可能受限于假设和过于自信。在这项工作中，我们提出了一种后期抽样策略，用于估计预测不确定性，考虑到数据 ambiguity。这种方法可以生成不同的可能输出，并不假设预测分布的 parametic 形式。它是体系无关的，可以应用于任何批处网络，无需改变体系结构或训练过程。在重静态和非静态输入数据上的回归任务中，我们的方法能够生成多种多样的预测分布，并且预测不确定性与预测错误之间存在适当的相关性。
</details></li>
</ul>
<hr>
<h2 id="Telematics-Combined-Actuarial-Neural-Networks-for-Cross-Sectional-and-Longitudinal-Claim-Count-Data"><a href="#Telematics-Combined-Actuarial-Neural-Networks-for-Cross-Sectional-and-Longitudinal-Claim-Count-Data" class="headerlink" title="Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data"></a>Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01729">http://arxiv.org/abs/2308.01729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francis Duval, Jean-Philippe Boucher, Mathieu Pigeon</li>
<li>for: 这个论文的目的是提出一种基于CANN框架的车保险laim count模型，用于评估和预测驾驶行为对车保险的影响。</li>
<li>methods: 这个论文使用了一种combined actuarial neural network（CANN）模型，该模型结合了经典的概率模型和神经网络，以提高评估驾驶行为对车保险的精度和可靠性。</li>
<li>results: 研究结果表明，使用CANN模型可以比以经典的ilog-linear模型来评估驾驶行为对车保险的影响，并且可以更好地评估驾驶行为的复杂性和相互关系。<details>
<summary>Abstract</summary>
We present novel cross-sectional and longitudinal claim count models for vehicle insurance built upon the Combined Actuarial Neural Network (CANN) framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach combines a classical actuarial model, such as a generalized linear model, with a neural network. This blending of models results in a two-component model comprising a classical regression model and a neural network part. The CANN model leverages the strengths of both components, providing a solid foundation and interpretability from the classical model while harnessing the flexibility and capacity to capture intricate relationships and interactions offered by the neural network. In our proposed models, we use well-known log-linear claim count regression models for the classical regression part and a multilayer perceptron (MLP) for the neural network part. The MLP part is used to process telematics car driving data given as a vector characterizing the driving behavior of each insured driver. In addition to the Poisson and negative binomial distributions for cross-sectional data, we propose a procedure for training our CANN model with a multivariate negative binomial (MVNB) specification. By doing so, we introduce a longitudinal model that accounts for the dependence between contracts from the same insured. Our results reveal that the CANN models exhibit superior performance compared to log-linear models that rely on manually engineered telematics features.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的跨部分和长期声明计数模型，用于汽车保险，基于Mario W\"uthrich和Michael Merz所提出的combined actuarial neural network（CANN）框架。CANN模型结合了一种经典的 actuarial模型，如一般线性模型，和一个神经网络。这种模型融合结果形成了一个两部分模型，包括一个经典回归模型和一个神经网络部分。CANN模型利用了经典模型的优点，提供了坚实的基础和解释，同时具有神经网络的灵活性和能力捕捉复杂的关系和交互。在我们的提议中，我们使用了常见的 log-linear 声明计数回归模型作为经典回归部分，并使用一个多层感知器（MLP）作为神经网络部分。MLP部分用于处理每名保险人的驾驶行为数据，即作为一个向量表示驾驶行为。此外，我们还提出了一种训练 CANN 模型的多变量负 binomial（MVNB）规范。通过这种方法，我们开发了一种长期模型，帮助考虑保险合同之间的依赖关系。我们的结果表明，CANN 模型在基于手动设计的驾驶特征的情况下显示出了更高的性能，与经典 log-linear 模型相比。
</details></li>
</ul>
<hr>
<h2 id="ADRNet-A-Generalized-Collaborative-Filtering-Framework-Combining-Clinical-and-Non-Clinical-Data-for-Adverse-Drug-Reaction-Prediction"><a href="#ADRNet-A-Generalized-Collaborative-Filtering-Framework-Combining-Clinical-and-Non-Clinical-Data-for-Adverse-Drug-Reaction-Prediction" class="headerlink" title="ADRNet: A Generalized Collaborative Filtering Framework Combining Clinical and Non-Clinical Data for Adverse Drug Reaction Prediction"></a>ADRNet: A Generalized Collaborative Filtering Framework Combining Clinical and Non-Clinical Data for Adverse Drug Reaction Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02571">http://arxiv.org/abs/2308.02571</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoxuanli-pku/adrnet">https://github.com/haoxuanli-pku/adrnet</a></li>
<li>paper_authors: Haoxuan Li, Taojun Hu, Zetong Xiong, Chunyuan Zheng, Fuli Feng, Xiangnan He, Xiao-Hua Zhou</li>
<li>for: 预测药物副作用（ADR）的incidence rate，以提高医疗和药品发现中的安全性。</li>
<li>methods: 基于drugg-ADR的协同缓存问题进行预测，并利用非临床数据中的药物特征进行补充。</li>
<li>results: 通过对两个大规模的临床数据集进行广泛的比较，证明ADRNet可以准确地预测多个标签ADR。<details>
<summary>Abstract</summary>
Adverse drug reaction (ADR) prediction plays a crucial role in both health care and drug discovery for reducing patient mortality and enhancing drug safety. Recently, many studies have been devoted to effectively predict the drug-ADRs incidence rates. However, these methods either did not effectively utilize non-clinical data, i.e., physical, chemical, and biological information about the drug, or did little to establish a link between content-based and pure collaborative filtering during the training phase. In this paper, we first formulate the prediction of multi-label ADRs as a drug-ADR collaborative filtering problem, and to the best of our knowledge, this is the first work to provide extensive benchmark results of previous collaborative filtering methods on two large publicly available clinical datasets. Then, by exploiting the easy accessible drug characteristics from non-clinical data, we propose ADRNet, a generalized collaborative filtering framework combining clinical and non-clinical data for drug-ADR prediction. Specifically, ADRNet has a shallow collaborative filtering module and a deep drug representation module, which can exploit the high-dimensional drug descriptors to further guide the learning of low-dimensional ADR latent embeddings, which incorporates both the benefits of collaborative filtering and representation learning. Extensive experiments are conducted on two publicly available real-world drug-ADR clinical datasets and two non-clinical datasets to demonstrate the accuracy and efficiency of the proposed ADRNet. The code is available at https://github.com/haoxuanli-pku/ADRnet.
</details>
<details>
<summary>摘要</summary>
药物反应（ADR）预测对于医疗和药物发现具有重要作用，可以降低病人死亡率并提高药物安全性。在最近的研究中，许多研究者已经努力地预测药物-ADR的发生率。然而，这些方法大多未能有效地利用药物的非临床数据，例如物理、化学和生物学信息。此外，这些方法也未能够在训练阶段建立物理和纯粹的共同滤波技术之间的连接。在本文中，我们将药物多标签ADR预测定为药物-ADR共同滤波问题，并且，到我们所知，这是第一篇对两个大规模公共可用临床数据进行了广泛的比较分析的研究。然后，我们提出了ADRNet，一种通用的共同滤波框架，结合临床和非临床数据来预测药物-ADR。具体来说，ADRNet包括一个浅层共同滤波模块和一个深度药物表示模块，可以利用高维药物描述符进一步导航低维ADR秘密嵌入的学习，这里包括了共同滤波和表示学习的两大好处。我们对两个公共可用的真实世界药物-ADR临床数据集和两个非临床数据集进行了广泛的实验，以证明我们提出的ADRNet的准确和效率。代码可以在https://github.com/haoxuanli-pku/ADRnet上获取。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Link-Prediction-Explanations-for-Graph-Neural-Networks"><a href="#Evaluating-Link-Prediction-Explanations-for-Graph-Neural-Networks" class="headerlink" title="Evaluating Link Prediction Explanations for Graph Neural Networks"></a>Evaluating Link Prediction Explanations for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01682">http://arxiv.org/abs/2308.01682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cborile/eval_lp_xai">https://github.com/cborile/eval_lp_xai</a></li>
<li>paper_authors: Claudio Borile, Alan Perotti, André Panisson</li>
<li>for: 本研究旨在提供链接预测模型的解释评价指标，以便帮助推广Graph Machine Learning（GML）模型的应用。</li>
<li>methods: 本研究使用了现有的解释方法，如Graph Neural Networks（GNN），并评估了它们的解释质量。</li>
<li>results: 研究发现，选择距离 между节点表示的选择对链接预测解释质量有重要影响。此外，研究还发现了一些技术细节和假设对链接预测解释质量的影响。<details>
<summary>Abstract</summary>
Graph Machine Learning (GML) has numerous applications, such as node/graph classification and link prediction, in real-world domains. Providing human-understandable explanations for GML models is a challenging yet fundamental task to foster their adoption, but validating explanations for link prediction models has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.
</details>
<details>
<summary>摘要</summary>
机器学习（GML）在实际领域有广泛的应用，如节点/图分类和链接预测。提供可理解的GML模型解释是推广其使用的挑战，但链接预测模型的解释 Validating explanations has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.Here's the translation in Traditional Chinese:机器学习（GML）在实际领域有广泛的应用，如节点/图分类和链接预测。提供可理解的GML模型解释是推广其使用的挑战，但链接预测模型的解释 Validating explanations has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.
</details></li>
</ul>
<hr>
<h2 id="Learning-Implicit-Entity-object-Relations-by-Bidirectional-Generative-Alignment-for-Multimodal-NER"><a href="#Learning-Implicit-Entity-object-Relations-by-Bidirectional-Generative-Alignment-for-Multimodal-NER" class="headerlink" title="Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER"></a>Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02570">http://arxiv.org/abs/2308.02570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Chen, Jiajia Liu, Kaixiang Ji, Wang Ren, Jian Wang, Jingdong Wang</li>
<li>for: 本文提出了一种解决多modal named entity recognition（MNER）中的两个挑战的方法，即 bridging the semantic gap between text and image，以及匹配实体与其相关的对象在图像中。</li>
<li>methods: 本文提出了一种名为BGA-MNER的双向生成对应方法，该方法包括\texttt{image2text}和\texttt{text2image}两个生成阶段，以及对实体特征含义的生成对应。</li>
<li>results: 经过广泛的实验 validate，本文的方法在两个benchmark上达到了无需图像输入 durante la inferencia的状态之决性性能。<details>
<summary>Abstract</summary>
The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in image. Existing methods fail to capture the implicit entity-object relations, due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our method achieves state-of-the-art performance without image input during inference.
</details>
<details>
<summary>摘要</summary>
The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in the image. Existing methods fail to capture the implicit entity-object relations due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our method achieves state-of-the-art performance without image input during inference.Here's the translation in Traditional Chinese as well:The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in the image. Existing methods fail to capture the implicit entity-object relations due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our method achieves state-of-the-art performance without image input during inference.Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Efficiency-of-First-Order-Methods-for-Low-Rank-Tensor-Recovery-with-the-Tensor-Nuclear-Norm-Under-Strict-Complementarity"><a href="#Efficiency-of-First-Order-Methods-for-Low-Rank-Tensor-Recovery-with-the-Tensor-Nuclear-Norm-Under-Strict-Complementarity" class="headerlink" title="Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity"></a>Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01677">http://arxiv.org/abs/2308.01677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Garber, Atara Kaplan</li>
<li>for: 本文是关于使用凸relaxation来重建低维度张量的研究。</li>
<li>methods: 本文使用了凸relaxation方法，包括拟合线性map和拟合quadratic形函数。</li>
<li>results: 本文得到了以下三个结果：1. 当 objective 函数为形式 $f(\mX)&#x3D;g(\mA\mX)+\langle{\mC,\mX}\rangle$，其中 $g$ 是强制凸函数，且 $\mA$ 是线性映射， Then 有一个quadratic growth bound，implying linear convergence rates for standard projected gradient methods。 2. 对于一个平滑函数 objective，当初始化在一个满足 SC 的优解中，then standard projected gradient methods only require SVD computations (for projecting onto the tensor nuclear norm ball) of rank that matches the tubal rank of the optimal solution。 3. 对于一个非平滑函数 objective，we derive similar results for the well known extragradient method。 Additionally, the paper extends many basic results regarding tensors of arbitrary order, which were previously obtained only for third-order tensors.<details>
<summary>Abstract</summary>
We consider convex relaxations for recovering low-rank tensors based on constrained minimization over a ball induced by the tensor nuclear norm, recently introduced in \cite{tensor_tSVD}. We build on a recent line of results that considered convex relaxations for the recovery of low-rank matrices and established that under a strict complementarity condition (SC), both the convergence rate and per-iteration runtime of standard gradient methods may improve dramatically. We develop the appropriate strict complementarity condition for the tensor nuclear norm ball and obtain the following main results under this condition: 1. When the objective to minimize is of the form $f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$ , where $g$ is strongly convex and $\mA$ is a linear map (e.g., least squares), a quadratic growth bound holds, which implies linear convergence rates for standard projected gradient methods, despite the fact that $f$ need not be strongly convex. 2. For a smooth objective function, when initialized in certain proximity of an optimal solution which satisfies SC, standard projected gradient methods only require SVD computations (for projecting onto the tensor nuclear norm ball) of rank that matches the tubal rank of the optimal solution. In particular, when the tubal rank is constant, this implies nearly linear (in the size of the tensor) runtime per iteration, as opposed to super linear without further assumptions. 3. For a nonsmooth objective function which admits a popular smooth saddle-point formulation, we derive similar results to the latter for the well known extragradient method. An additional contribution which may be of independent interest, is the rigorous extension of many basic results regarding tensors of arbitrary order, which were previously obtained only for third-order tensors.
</details>
<details>
<summary>摘要</summary>
我们考虑使用凸关键函数来回复低维度tensor的方法，基于给定的凸关键函数球体上的受限最小化。我们在\cite{tensor_tSVD}中引入的tensor核心 нор的凸关键函数球体上建立了严格的完全相互矛盾（SC）的必要条件。我们获得以下主要结果：1. 当待解函数为$f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$，其中$g$是强式凸函数且$\mA$是线性映射（例如最小二乘）， THEN 一个径度增长范围将成立，这意味着标准投影方法将在待解函数中展现出直线增长率，即使待解函数并不是强式凸函数。2. 当待解函数为几何函数且初值在具有SC的优解中， THEN 标准投影方法只需要在tensor核心 norm球体上进行SVD计算（用于对待解函数进行投影），其中SVD的维度与优解的管径维度相同。这意味着在无变量大小的tensor上，每次迭代的时间几乎是常量，而不是增长的。3. 当待解函数为非凸函数且具有流行的滑块形式则，我们 derive了类似的结果，对于通过extrapolation method来解决的问题。此外，我们还提供了一些独立有用的结果，例如在tensor的任意维度上，许多基本结果的扩展，这些结果在以前只有在第三维tensor上被证明。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Reinforcement-Learning-of-Koopman-Models-for-Economic-Nonlinear-MPC"><a href="#End-to-End-Reinforcement-Learning-of-Koopman-Models-for-Economic-Nonlinear-MPC" class="headerlink" title="End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC"></a>End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01674">http://arxiv.org/abs/2308.01674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Mayfrank, Alexander Mitsos, Manuel Dahmen</li>
<li>for: 本研究旨在提出一种数据驱动的汽车模型，以便在实时控制中减少计算成本。</li>
<li>methods: 该方法使用端到端学习来学习动态模型，并通过对实际数据进行训练来提高预测性能。</li>
<li>results: 研究结果显示，使用该方法可以实时生成高性能的预测模型，并且可以在控制设置变化时快速适应。同时，该方法与传统的最大预测精度方法和模型自适应方法相比，能够减少计算成本。<details>
<summary>Abstract</summary>
(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic system models that are sufficiently accurate in all relevant state-space regions. These models must also be computationally cheap enough to ensure real-time tractability. Data-driven surrogate models for mechanistic models can be used to reduce the computational burden of (e)NMPC; however, such models are typically trained by system identification for maximum average prediction accuracy on simulation samples and perform suboptimally as part of actual (e)NMPC. We present a method for end-to-end reinforcement learning of dynamic surrogate models for optimal performance in (e)NMPC applications, resulting in predictive controllers that strike a favorable balance between control performance and computational demand. We validate our method on two applications derived from an established nonlinear continuous stirred-tank reactor model. We compare the controller performance to that of MPCs utilizing models trained by the prevailing maximum prediction accuracy paradigm, and model-free neural network controllers trained using reinforcement learning. We show that our method matches the performance of the model-free neural network controllers while consistently outperforming models derived from system identification. Additionally, we show that the MPC policies can react to changes in the control setting without retraining.
</details>
<details>
<summary>摘要</summary>
经济非线性模型预测控制（(E)NMPC）需要具有 suficiently 精度的动态系统模型，以 guaranteee 在所有有用的状态空间Region中准确。这些模型还需要够快速计算，以确保实时可行性。使用数据驱动的替身模型来减少(E)NMPC中模型计算的复杂性可以使其更加快速，但这些模型通常通过系统标准化来训练，并且在实际(E)NMPC应用中表现不佳。我们提出了一种终端渐进学习的动态替身模型，以实现在(E)NMPC应用中的优化性能。我们验证了我们的方法，并与现有的最大预测精度 paradigm和模型自由神经网络控制器进行比较。我们发现，我们的方法与模型自由神经网络控制器的性能相同，而且一直占据了模型来自系统标准化的性能。此外，我们还发现MPC策略可以根据控制设置的变化而反应，而不需要重新训练。
</details></li>
</ul>
<hr>
<h2 id="UniG-Encoder-A-Universal-Feature-Encoder-for-Graph-and-Hypergraph-Node-Classification"><a href="#UniG-Encoder-A-Universal-Feature-Encoder-for-Graph-and-Hypergraph-Node-Classification" class="headerlink" title="UniG-Encoder: A Universal Feature Encoder for Graph and Hypergraph Node Classification"></a>UniG-Encoder: A Universal Feature Encoder for Graph and Hypergraph Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01650">http://arxiv.org/abs/2308.01650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minhzou/unig-encoder">https://github.com/minhzou/unig-encoder</a></li>
<li>paper_authors: Minhao Zou, Zhongxue Gan, Yutong Wang, Junheng Zhang, Dongyan Sui, Chun Guan, Siyang Leng</li>
<li>for: 这篇论文的目的是为了提出一种 универсальный特征编码器，用于图和高级图表示学习。</li>
<li>methods: 该方法使用一个前向变换将图的 topological 关系转化为边或超边特征，然后将这些特征和原始节点特征 feed 入神经网络，最后使用反向变换，即投影矩阵的读取，获得编码后的节点嵌入。</li>
<li>results: 对于十二个 representativ 的高级图数据集和六个实际图数据集，该方法在对比state-of-the-art方法时显示出了更高的性能。<details>
<summary>Abstract</summary>
Graph and hypergraph representation learning has attracted increasing attention from various research fields. Despite the decent performance and fruitful applications of Graph Neural Networks (GNNs), Hypergraph Neural Networks (HGNNs), and their well-designed variants, on some commonly used benchmark graphs and hypergraphs, they are outperformed by even a simple Multi-Layer Perceptron. This observation motivates a reexamination of the design paradigm of the current GNNs and HGNNs and poses challenges of extracting graph features effectively. In this work, a universal feature encoder for both graph and hypergraph representation learning is designed, called UniG-Encoder. The architecture starts with a forward transformation of the topological relationships of connected nodes into edge or hyperedge features via a normalized projection matrix. The resulting edge/hyperedge features, together with the original node features, are fed into a neural network. The encoded node embeddings are then derived from the reversed transformation, described by the transpose of the projection matrix, of the network's output, which can be further used for tasks such as node classification. The proposed architecture, in contrast to the traditional spectral-based and/or message passing approaches, simultaneously and comprehensively exploits the node features and graph/hypergraph topologies in an efficient and unified manner, covering both heterophilic and homophilic graphs. The designed projection matrix, encoding the graph features, is intuitive and interpretable. Extensive experiments are conducted and demonstrate the superior performance of the proposed framework on twelve representative hypergraph datasets and six real-world graph datasets, compared to the state-of-the-art methods. Our implementation is available online at https://github.com/MinhZou/UniG-Encoder.
</details>
<details>
<summary>摘要</summary>
GRaph和嵌入图 representation learning 已经引起了不同领域的研究者的关注。尽管图神经网络（GNNs）、嵌入图神经网络（HGNNs）以及其设计的许多变体在一些常用的图和嵌入图上达到了不错的性能，但它们在一些简单的多层感知器（MLP）上被超越。这种观察激发了现有GNNs和HGNNs的设计思路的重新评估，并提出了提取图特征的挑战。在这种工作中，我们设计了一种通用的特征编码器，称为UniG-Encoder。该架构开始于将图中连接节点的topological关系转化为Edge或嵌入Edge特征via一个正规投影矩阵。然后，这些特征，与原始节点特征一起，被 feed into一个神经网络。编码后的节点嵌入则是通过投影矩阵的背景矩阵反转，从神经网络的输出获得的。该架构，与传统的spectral-based和/或message passing方法不同，同时并且全面地利用节点特征和图/嵌入图结构，提供了一种高效的、统一的方法，可以处理异质的图和嵌入图。设计的投影矩阵是直观和可解释的。我们在十二个代表性的嵌入图 dataset和六个实际图 dataset上进行了广泛的实验，并证明了我们的框架在这些dataset上的超越性。我们的实现可以在https://github.com/MinhZou/UniG-Encoder上找到。
</details></li>
</ul>
<hr>
<h2 id="MARLIM-Multi-Agent-Reinforcement-Learning-for-Inventory-Management"><a href="#MARLIM-Multi-Agent-Reinforcement-Learning-for-Inventory-Management" class="headerlink" title="MARLIM: Multi-Agent Reinforcement Learning for Inventory Management"></a>MARLIM: Multi-Agent Reinforcement Learning for Inventory Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01649">http://arxiv.org/abs/2308.01649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rémi Leluc, Elie Kadoche, Antoine Bertoncello, Sébastien Gourvénec</li>
<li>for: 该论文旨在解决供应链中单一架构多种产品的存储管理问题，通过优化填充决策来保持供应和需求的平衡。</li>
<li>methods: 该论文提出了一种基于强化学习的新框架 named MARLIM，通过单或多个代理人在合作环境中开发控制器来解决存储管理问题。</li>
<li>results: 数值实验表明，基于强化学习方法比传统基线方法更有利于解决存储管理问题。<details>
<summary>Abstract</summary>
Maintaining a balance between the supply and demand of products by optimizing replenishment decisions is one of the most important challenges in the supply chain industry. This paper presents a novel reinforcement learning framework called MARLIM, to address the inventory management problem for a single-echelon multi-products supply chain with stochastic demands and lead-times. Within this context, controllers are developed through single or multiple agents in a cooperative setting. Numerical experiments on real data demonstrate the benefits of reinforcement learning methods over traditional baselines.
</details>
<details>
<summary>摘要</summary>
维护产品供应和需求的平衡是供应链业中最重要的挑战。本文提出了一个新的强化学习框架，名为MARLIM，以解决单一批制供应链中多产品的库存管理问题。在这个上下文中，控制器通过单一或多个代理人在合作环境下发展。实验结果显示，强化学习方法比传统基准方法更有利。Here's the breakdown of the translation:* 维护 (maintaining) = 维护 (maintaining)* 产品 (products) = 产品 (products)* 供应 (supply) = 供应 (supply)* 需求 (demand) = 需求 (demand)* 平衡 (balance) = 平衡 (balance)* 挑战 (challenge) = 挑战 (challenge)* 供应链 (supply chain) = 供应链 (supply chain)* 单一 (single) = 单一 (single)* 批制 (batch) = 批制 (batch)* 供应链 (supply chain) = 供应链 (supply chain)* 多产品 (multi-products) = 多产品 (multi-products)* 库存 (inventory) = 库存 (inventory)* 管理 (management) = 管理 (management)* 问题 (problem) = 问题 (problem)* 控制器 (controller) = 控制器 (controller)* 代理人 (agent) = 代理人 (agent)* 合作 (cooperative) = 合作 (cooperative)* 环境 (environment) = 环境 (environment)* 实验 (experiment) = 实验 (experiment)* 结果 (result) = 结果 (result)* 比 (than) = 比 (than)* 传统 (traditional) = 传统 (traditional)* 基准 (baseline) = 基准 (baseline)* 方法 (method) = 方法 (method)* 更 (more) = 更 (more)
</details></li>
</ul>
<hr>
<h2 id="Interleaving-GANs-with-knowledge-graphs-to-support-design-creativity-for-book-covers"><a href="#Interleaving-GANs-with-knowledge-graphs-to-support-design-creativity-for-book-covers" class="headerlink" title="Interleaving GANs with knowledge graphs to support design creativity for book covers"></a>Interleaving GANs with knowledge graphs to support design creativity for book covers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01626">http://arxiv.org/abs/2308.01626</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexmotogna/generatorapi">https://github.com/alexmotogna/generatorapi</a></li>
<li>paper_authors: Alexandru Motogna, Adrian Groza</li>
<li>for: 这篇论文是为了提高书籍封面的创作而写的。</li>
<li>methods: 这篇论文使用了生成对抗网络（GANs）来生成图像，并通过与知识图加以混合来改变输入标题，以生成多种可能性。</li>
<li>results: 这篇论文的方法比之前的尝试更好地生成了图像，并且知识图可以为书作者或编辑提供更多的选择。<details>
<summary>Abstract</summary>
An attractive book cover is important for the success of a book. In this paper, we apply Generative Adversarial Networks (GANs) to the book covers domain, using different methods for training in order to obtain better generated images. We interleave GANs with knowledge graphs to alter the input title to obtain multiple possible options for any given title, which are then used as an augmented input to the generator. Finally, we use the discriminator obtained during the training phase to select the best images generated with new titles. Our method performed better at generating book covers than previous attempts, and the knowledge graph gives better options to the book author or editor compared to using GANs alone.
</details>
<details>
<summary>摘要</summary>
一本有吸引力的书封面对书的成功很重要。在这篇论文中，我们使用生成对抗网络（GANs）来改进书封面领域中的生成图像。我们在训练中使用不同的方法，以获得更好的生成图像。我们将知识图库与GANs相互嵌入，以对输入书名进行修改，从而获得多个可能的选项。最后，我们使用训练阶段获得的推识器，选择最佳的生成图像。我们的方法在生成书封面方面比前一次尝试更好，而知识图库对书作者或编辑提供了更多的选择。
</details></li>
</ul>
<hr>
<h2 id="Weighted-Multi-Level-Feature-Factorization-for-App-ads-CTR-and-installation-prediction"><a href="#Weighted-Multi-Level-Feature-Factorization-for-App-ads-CTR-and-installation-prediction" class="headerlink" title="Weighted Multi-Level Feature Factorization for App ads CTR and installation prediction"></a>Weighted Multi-Level Feature Factorization for App ads CTR and installation prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02568">http://arxiv.org/abs/2308.02568</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knife982000/recsys2023challenge">https://github.com/knife982000/recsys2023challenge</a></li>
<li>paper_authors: Juan Manuel Rodriguez, Antonela Tommasel</li>
<li>for: 本文是针对ACM RecSys Challenge 2023进行报告，主要目标是预测用户点击和安装应用程序的概率，以优化深层渠道优化和尊重用户隐私。</li>
<li>methods: 该方法基于Weighted Multi-Level Feature Factorization，即在不同层次上结合任务特定和共享特征进行特征工程，以优化点击和安装两个相关 yet 不同的任务。</li>
<li>results: 本文在ACM RecSys Challenge 2023的学术赛道决赛中获得了11名和总分55的成绩，并在<a target="_blank" rel="noopener" href="https://github.com/knife982000/RecSys2023Challenge%E4%B8%8A%E5%85%AC%E5%BC%80%E4%BA%86%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/knife982000/RecSys2023Challenge上公开了代码。</a><details>
<summary>Abstract</summary>
This paper provides an overview of the approach we used as team ISISTANITOS for the ACM RecSys Challenge 2023. The competition was organized by ShareChat, and involved predicting the probability of a user clicking an app ad and/or installing an app, to improve deep funnel optimization and a special focus on user privacy. Our proposed method inferring the probabilities of clicking and installing as two different, but related tasks. Hence, the model engineers a specific set of features for each task and a set of shared features. Our model is called Weighted Multi-Level Feature Factorization because it considers the interaction of different order features, where the order is associated to the depth in a neural network. The prediction for a given task is generated by combining the task specific and shared features on the different levels. Our submission achieved the 11 rank and overall score of 55 in the competition academia-track final results. We release our source code at: https://github.com/knife982000/RecSys2023Challenge
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multimodal-Indoor-Localisation-in-Parkinson’s-Disease-for-Detecting-Medication-Use-Observational-Pilot-Study-in-a-Free-Living-Setting"><a href="#Multimodal-Indoor-Localisation-in-Parkinson’s-Disease-for-Detecting-Medication-Use-Observational-Pilot-Study-in-a-Free-Living-Setting" class="headerlink" title="Multimodal Indoor Localisation in Parkinson’s Disease for Detecting Medication Use: Observational Pilot Study in a Free-Living Setting"></a>Multimodal Indoor Localisation in Parkinson’s Disease for Detecting Medication Use: Observational Pilot Study in a Free-Living Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02419">http://arxiv.org/abs/2308.02419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferdianjovan/Multihead-Dual-Convolutional-Self-Attention">https://github.com/ferdianjovan/Multihead-Dual-Convolutional-Self-Attention</a></li>
<li>paper_authors: Ferdian Jovan, Catherine Morgan, Ryan McConville, Emma L. Tonkin, Ian Craddock, Alan Whone</li>
<li>for: 这个论文是为了提高现有的indoor localization方法的效果而写的。</li>
<li>methods: 这个论文使用了transformer模型，使用了双Modal的RSSI和加速度数据来提高indoor localization的准确率。</li>
<li>results: 论文的evaluation表明，该方法可以在real-world condition下提高indoor localization的准确率，并且可以准确地判断PD参与者是否正在服用levodopa药物。<details>
<summary>Abstract</summary>
Parkinson's disease (PD) is a slowly progressive, debilitating neurodegenerative disease which causes motor symptoms including gait dysfunction. Motor fluctuations are alterations between periods with a positive response to levodopa therapy ("on") and periods marked by re-emergency of PD symptoms ("off") as the response to medication wears off. These fluctuations often affect gait speed and they increase in their disabling impact as PD progresses. To improve the effectiveness of current indoor localisation methods, a transformer-based approach utilising dual modalities which provide complementary views of movement, Received Signal Strength Indicator (RSSI) and accelerometer data from wearable devices, is proposed. A sub-objective aims to evaluate whether indoor localisation, including its in-home gait speed features (i.e. the time taken to walk between rooms), could be used to evaluate motor fluctuations by detecting whether the person with PD is taking levodopa medications or withholding them. To properly evaluate our proposed method, we use a free-living dataset where the movements and mobility are greatly varied and unstructured as expected in real-world conditions. 24 participants lived in pairs (consisting of one person with PD, one control) for five days in a smart home with various sensors. Our evaluation on the resulting dataset demonstrates that our proposed network outperforms other methods for indoor localisation. The sub-objective evaluation shows that precise room-level localisation predictions, transformed into in-home gait speed features, produce accurate predictions on whether the PD participant is taking or withholding their medications.
</details>
<details>
<summary>摘要</summary>
帕金森病 (PD) 是一种慢速进行、抑酸性减退性神经病，引起运动征瘤包括跑步功能不优。运动波动是指在 леvodopa 治疗中有效期内和无效期间的变化，这些变化通常影响跑步速度，随着病情进展而加剧。为了改善当前indoor localization方法的效果，一种基于 transformer 的方法，使用 dual modalities 提供了补做的视角，包括Received Signal Strength Indicator (RSSI) 和加速器数据从 wearable 设备。一个副目标是评估whether indoor localization，包括室内跑步速度特征（即在房间之间行走的时间），可以用于评估PD参与者是否服用了levodopa 药物。为了准确评估我们的提议方法，我们使用了一个免费生活数据集，其中运动和 mobilty 具有很大的变化和不结构性，如预期的实际条件中。24名参与者（其中12名PD参与者和12名控制参与者）在 smart home 中生活了5天，并使用了各种感知器。我们对所获得的数据进行评估，并示出了我们的提议网络在indoor localization方面的出色表现。副目标评估显示，精准的房间层级本地化预测，经过转换为室内跑步速度特征，可以生成准确的PD参与者是否服用了levodopa 药物的预测。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Convolutional-Neural-Network-Architecture-with-a-Continuous-Symmetry"><a href="#A-Novel-Convolutional-Neural-Network-Architecture-with-a-Continuous-Symmetry" class="headerlink" title="A Novel Convolutional Neural Network Architecture with a Continuous Symmetry"></a>A Novel Convolutional Neural Network Architecture with a Continuous Symmetry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01621">http://arxiv.org/abs/2308.01621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuyao12/ConvNets-PDE-perspective">https://github.com/liuyao12/ConvNets-PDE-perspective</a></li>
<li>paper_authors: Yao Liu, Hang Shao, Bing Bai</li>
<li>for: 这种新的卷积神经网络架构是基于一种类型的偏微分方程（PDE），即 quasi-linear 超声速系统。</li>
<li>methods: 这种架构使得权重可以通过一个连续群的对称性进行修改，这与传统的模型不同，其架构和权重基本固定。</li>
<li>results: 与传统模型相比，这种架构在图像分类任务上具有相似的性能，同时具有内部对称性作为一个新的愿望属性。<details>
<summary>Abstract</summary>
This paper introduces a new Convolutional Neural Network (ConvNet) architecture inspired by a class of partial differential equations (PDEs) called quasi-linear hyperbolic systems. With comparable performance on the image classification task, it allows for the modification of the weights via a continuous group of symmetry. This is a significant shift from traditional models where the architecture and weights are essentially fixed. We wish to promote the (internal) symmetry as a new desirable property for a neural network, and to draw attention to the PDE perspective in analyzing and interpreting ConvNets in the broader Deep Learning community.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的卷积神经网络（ConvNet）架构， Drawing inspiration from a class ofpartial differential equations（PDEs）called quasi-linear hyperbolic systems. 与传统模型相比，这种架构允许权重的修改via continuous group of symmetry，这是一种重要的Shift from traditional models, where the architecture and weights are essentially fixed. 我们希望通过推广这种（内部）对称性作为神经网络的新有优点，并吸引Deep Learning社区更广泛关注PDE的视角来分析和解释ConvNets.Note: "quasi-linear hyperbolic systems" in the original text is translated as "quasi-linear hyperbolic systems" in Simplified Chinese, as there is no direct equivalent term in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Assessing-Systematic-Weaknesses-of-DNNs-using-Counterfactuals"><a href="#Assessing-Systematic-Weaknesses-of-DNNs-using-Counterfactuals" class="headerlink" title="Assessing Systematic Weaknesses of DNNs using Counterfactuals"></a>Assessing Systematic Weaknesses of DNNs using Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01614">http://arxiv.org/abs/2308.01614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sujan Sai Gannamaneni, Michael Mock, Maram Akila</li>
<li>for: 这篇论文旨在检查和验证深度神经网络（DNN）在安全应用中的测试方法。</li>
<li>methods: 本文使用了一种基于对应假设的Semantic Attribution方法来验证DNN的性能差异。</li>
<li>results: 本文的结果显示，在自驾车领域中的一个例子中，使用高度标注的 simulated 数据，发现了一些特定的人工资产（asset）对于深度神经网络（DNN）的性能有差异，但是只有在某些情况下，资产类型本身是性能差异的原因。<details>
<summary>Abstract</summary>
With the advancement of DNNs into safety-critical applications, testing approaches for such models have gained more attention. A current direction is the search for and identification of systematic weaknesses that put safety assumptions based on average performance values at risk. Such weaknesses can take on the form of (semantically coherent) subsets or areas in the input space where a DNN performs systematically worse than its expected average. However, it is non-trivial to attribute the reason for such observed low performances to the specific semantic features that describe the subset. For instance, inhomogeneities within the data w.r.t. other (non-considered) attributes might distort results. However, taking into account all (available) attributes and their interaction is often computationally highly expensive. Inspired by counterfactual explanations, we propose an effective and computationally cheap algorithm to validate the semantic attribution of existing subsets, i.e., to check whether the identified attribute is likely to have caused the degraded performance. We demonstrate this approach on an example from the autonomous driving domain using highly annotated simulated data, where we show for a semantic segmentation model that (i) performance differences among the different pedestrian assets exist, but (ii) only in some cases is the asset type itself the reason for this reduction in the performance.
</details>
<details>
<summary>摘要</summary>
Inspired by counterfactual explanations, we propose an efficient and cost-effective algorithm to validate the semantic attribution of existing subsets. We demonstrate this approach on an example from the autonomous driving domain using highly annotated simulated data. Our results show that (i) performance differences exist among different pedestrian assets, but (ii) the asset type is not always the reason for the reduced performance.
</details></li>
</ul>
<hr>
<h2 id="Feature-Noise-Boosts-DNN-Generalization-under-Label-Noise"><a href="#Feature-Noise-Boosts-DNN-Generalization-under-Label-Noise" class="headerlink" title="Feature Noise Boosts DNN Generalization under Label Noise"></a>Feature Noise Boosts DNN Generalization under Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01609">http://arxiv.org/abs/2308.01609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zlzenglu/fn">https://github.com/zlzenglu/fn</a></li>
<li>paper_authors: Lu Zeng, Xuan Chen, Xiaoshuang Shi, Heng Tao Shen</li>
<li>for: 增强深度神经网络（DNNs）对标签噪声的泛化性能。</li>
<li>methods: 直接将噪声添加到训练数据中的特征上，以增强DNNs的泛化性能。</li>
<li>results: 经过理论分析，发现标签噪声会削弱DNNs的泛化性能，而特征噪声则可以增强DNNs的泛化性能，并且可以通过调整特征噪声的类型和水平来确定最佳的噪声类型和水平。经过实验 validate 的结果表明，这种特征噪声方法可以显著提高DNNs在标签噪声下的泛化性能。<details>
<summary>Abstract</summary>
The presence of label noise in the training data has a profound impact on the generalization of deep neural networks (DNNs). In this study, we introduce and theoretically demonstrate a simple feature noise method, which directly adds noise to the features of training data, can enhance the generalization of DNNs under label noise. Specifically, we conduct theoretical analyses to reveal that label noise leads to weakened DNN generalization by loosening the PAC-Bayes generalization bound, and feature noise results in better DNN generalization by imposing an upper bound on the mutual information between the model weights and the features, which constrains the PAC-Bayes generalization bound. Furthermore, to ensure effective generalization of DNNs in the presence of label noise, we conduct application analyses to identify the optimal types and levels of feature noise to add for obtaining desirable label noise generalization. Finally, extensive experimental results on several popular datasets demonstrate the feature noise method can significantly enhance the label noise generalization of the state-of-the-art label noise method.
</details>
<details>
<summary>摘要</summary>
deep neural networks (DNNs) 的泛化能力受到标签噪声的影响。在本研究中，我们提出了一种简单的特征噪声方法，可以直接将噪声添加到训练数据的特征中，以提高 DNN 的泛化能力。我们进行了理论分析，发现标签噪声会减弱 DNN 的泛化能力，而特征噪声则可以通过限制模型参数和特征之间的互信息，提高 PAC-Bayes 泛化 bound。此外，为确保 DNN 在标签噪声下的有效泛化，我们进行了应用分析，并identified optimal types and levels of feature noise to add for obtaining desirable label noise generalization。最后，我们在多个popular dataset上进行了广泛的实验，并证明了 feature noise method可以显著提高 state-of-the-art label noise method 的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Discriminative-Graph-level-Anomaly-Detection-via-Dual-students-teacher-Model"><a href="#Discriminative-Graph-level-Anomaly-Detection-via-Dual-students-teacher-Model" class="headerlink" title="Discriminative Graph-level Anomaly Detection via Dual-students-teacher Model"></a>Discriminative Graph-level Anomaly Detection via Dual-students-teacher Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01947">http://arxiv.org/abs/2308.01947</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/whb605/gladst">https://github.com/whb605/gladst</a></li>
<li>paper_authors: Fu Lin, Xuexiong Luo, Jia Wu, Jian Yang, Shan Xue, Zitong Wang, Haonan Gong</li>
<li>for: 本研究的目标是找出图集中的异常图，并使用图表示来识别它们。由于现有的研究不够关于图级异常检测，因此我们需要更好地定义图级异常的描述。</li>
<li>methods: 我们首先定义图集中的异常图信息，包括节点和图像异常。然后，我们提出了一种可分辨性 Graph-level anomaly detection 框架，使用双学生-教师模型。教师模型使用准确的损失函数来让图表示更加分化。两个竞争学生模型通过正常和异常图来适应教师模型的图表示。最后，我们将两个学生模型的表示错误相加，以分别地识别异常图。</li>
<li>results: 我们在实验分析中发现，我们的方法可以有效地检测图集中的异常图。这表明我们的方法可以在实际世界中的图据集上进行异常检测。<details>
<summary>Abstract</summary>
Different from the current node-level anomaly detection task, the goal of graph-level anomaly detection is to find abnormal graphs that significantly differ from others in a graph set. Due to the scarcity of research on the work of graph-level anomaly detection, the detailed description of graph-level anomaly is insufficient. Furthermore, existing works focus on capturing anomalous graph information to learn better graph representations, but they ignore the importance of an effective anomaly score function for evaluating abnormal graphs. Thus, in this work, we first define anomalous graph information including node and graph property anomalies in a graph set and adopt node-level and graph-level information differences to identify them, respectively. Then, we introduce a discriminative graph-level anomaly detection framework with dual-students-teacher model, where the teacher model with a heuristic loss are trained to make graph representations more divergent. Then, two competing student models trained by normal and abnormal graphs respectively fit graph representations of the teacher model in terms of node-level and graph-level representation perspectives. Finally, we combine representation errors between two student models to discriminatively distinguish anomalous graphs. Extensive experiment analysis demonstrates that our method is effective for the graph-level anomaly detection task on graph datasets in the real world.
</details>
<details>
<summary>摘要</summary>
不同于现有的节点级异常检测任务，我们的目标是找到异常图的，这些图在图集中显著地不同于其他图。由于关于图级异常检测的研究缺乏，异常图的详细描述还不够。此外，现有的工作主要是捕捉异常图信息，以便学习更好的图表示，但它们忽略了评估异常图的有效 anomaly score 函数的重要性。因此，在这种工作中，我们首先定义图集中的异常图信息，包括节点和图性异常，并采用节点级和图级信息差异来识别它们。然后，我们提出了一种能够分类异常图的双学生-教师模型，其中教师模型通过规则损失来训练图表示更加分化。然后，两个竞争学生模型，一个通过正常图，另一个通过异常图进行训练，分别适应教师模型的节点级和图级表示视角。最后，我们将两个学生模型的表示错误相加，以分类异常图。我们的方法在实际世界的图据集上进行了广泛的实验分析，得到了有效的结果。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Multiplex-Graph-Learning-with-Complementary-and-Consistent-Information"><a href="#Unsupervised-Multiplex-Graph-Learning-with-Complementary-and-Consistent-Information" class="headerlink" title="Unsupervised Multiplex Graph Learning with Complementary and Consistent Information"></a>Unsupervised Multiplex Graph Learning with Complementary and Consistent Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01606">http://arxiv.org/abs/2308.01606</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/larryuestc/cocomg">https://github.com/larryuestc/cocomg</a></li>
<li>paper_authors: Liang Peng, Xin Wang, Xiaofeng Zhu</li>
<li>for: 本文提出了一种解决实际应用中issues的不监管多重图学习方法（UMGL），以提高不同下游任务的效果。</li>
<li>methods: 本方法使用多个多层感知网络（MLP）Encoder进行表示学习，并采用两个约束条件：保持节点之间的本地图structures，以处理异样问题，并 Maximize多个节点表示之间的相关性，以处理噪声问题。</li>
<li>results: 对比其他方法，本方法在实验中表现出了更高的效果和效率，并能够有效地解决异样和噪声问题。<details>
<summary>Abstract</summary>
Unsupervised multiplex graph learning (UMGL) has been shown to achieve significant effectiveness for different downstream tasks by exploring both complementary information and consistent information among multiple graphs. However, previous methods usually overlook the issues in practical applications, i.e., the out-of-sample issue and the noise issue. To address the above issues, in this paper, we propose an effective and efficient UMGL method to explore both complementary and consistent information. To do this, our method employs multiple MLP encoders rather than graph convolutional network (GCN) to conduct representation learning with two constraints, i.e., preserving the local graph structure among nodes to handle the out-of-sample issue, and maximizing the correlation of multiple node representations to handle the noise issue. Comprehensive experiments demonstrate that our proposed method achieves superior effectiveness and efficiency over the comparison methods and effectively tackles those two issues. Code is available at https://github.com/LarryUESTC/CoCoMG.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>Unsupervised多重图学习（UMGL）已经在不同的下游任务中显示出了 significanteffectiveness，通过探索多个图中的共同信息和差异信息。然而，之前的方法通常忽视了实际应用中的问题，即外样问题和噪声问题。为了解决这些问题，在这篇论文中，我们提出了一种有效和高效的 UMGL 方法，通过多个多层感知（MLP）编码器来进行表示学习，并遵循两个约束条件：保持节点之间的本地图结构，以处理外样问题，并 maximize 多个节点表示的相关性，以处理噪声问题。广泛的实验表明，我们提出的方法在比较方法中显示出了superior的有效性和高效性，并有效地解决了这两个问题。代码可以在 https://github.com/LarryUESTC/CoCoMG 上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-based-surrogate-models-for-parametrized-PDEs-handling-geometric-variability-through-graph-neural-networks"><a href="#Deep-Learning-based-surrogate-models-for-parametrized-PDEs-handling-geometric-variability-through-graph-neural-networks" class="headerlink" title="Deep Learning-based surrogate models for parametrized PDEs: handling geometric variability through graph neural networks"></a>Deep Learning-based surrogate models for parametrized PDEs: handling geometric variability through graph neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01602">http://arxiv.org/abs/2308.01602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicola Rares Franco, Stefania Fresca, Filippo Tombari, Andrea Manzoni</li>
<li>for: 用于模拟复杂物理系统，需要解决参数化时间依赖非线性偏微分方程（PDEs）。</li>
<li>methods: 使用图ael neural networks（GNNs）来代替 computationally expensive solvers，以实现更高效的simulation。</li>
<li>results: GNNs可以提供一个有效的surrogate model，可以涵盖不同的几何和分解精度，并且可以在不同的参数下进行泛化。<details>
<summary>Abstract</summary>
Mesh-based simulations play a key role when modeling complex physical systems that, in many disciplines across science and engineering, require the solution of parametrized time-dependent nonlinear partial differential equations (PDEs). In this context, full order models (FOMs), such as those relying on the finite element method, can reach high levels of accuracy, however often yielding intensive simulations to run. For this reason, surrogate models are developed to replace computationally expensive solvers with more efficient ones, which can strike favorable trade-offs between accuracy and efficiency. This work explores the potential usage of graph neural networks (GNNs) for the simulation of time-dependent PDEs in the presence of geometrical variability. In particular, we propose a systematic strategy to build surrogate models based on a data-driven time-stepping scheme where a GNN architecture is used to efficiently evolve the system. With respect to the majority of surrogate models, the proposed approach stands out for its ability of tackling problems with parameter dependent spatial domains, while simultaneously generalizing to different geometries and mesh resolutions. We assess the effectiveness of the proposed approach through a series of numerical experiments, involving both two- and three-dimensional problems, showing that GNNs can provide a valid alternative to traditional surrogate models in terms of computational efficiency and generalization to new scenarios. We also assess, from a numerical standpoint, the importance of using GNNs, rather than classical dense deep neural networks, for the proposed framework.
</details>
<details>
<summary>摘要</summary>
mesh-based 模拟在许多科学和工程领域中扮演关键角色，特别是当模型复杂物理系统时，需要解决 Parametrized 时间依赖非线性偏微分方程 (PDEs)。在这种情况下，全序模型 (FOMs)，如基于 finite element 方法的模型，可以达到高级别的准确性，但通常需要昂贵的计算。为了解决这个问题，人们通常会开发供应商模型，以取代 computationally 昂贵的解决方案，从而实现可接受的妥协。这项工作探讨了使用图 neuron 网络 (GNNs) 来模拟时间依赖 PDEs 的可能性。具体来说，我们提出了一种系统性的策略，通过数据驱动的时间步骤来建立仿真模型。与大多数供应商模型不同的是，我们的方法可以处理具有参数依赖的空间领域的问题，同时能够泛化到不同的几何和分辨率。我们通过一系列数学实验，包括二维和三维问题，证明了 GNNs 可以提供一个有效的代替方案，而不需要经过复杂的拟合。此外，我们还评估了使用 GNNs 而不是传统的密集深度神经网络，对该框架的重要性。
</details></li>
</ul>
<hr>
<h2 id="Experimental-Results-regarding-multiple-Machine-Learning-via-Quaternions"><a href="#Experimental-Results-regarding-multiple-Machine-Learning-via-Quaternions" class="headerlink" title="Experimental Results regarding multiple Machine Learning via Quaternions"></a>Experimental Results regarding multiple Machine Learning via Quaternions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01946">http://arxiv.org/abs/2308.01946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianlei Zhu, Renzhe Zhu</li>
<li>for: 这项研究探讨了使用量子来实现机器学习算法的应用。</li>
<li>methods: 本研究使用随机生成的量子数据和对应的标签，将量子转换为旋转矩阵，并使其为输入特征。</li>
<li>results: 结果表明，使用量子和多种机器学习算法可以实现更高的准确率和显著改善在预测任务中。<details>
<summary>Abstract</summary>
This paper presents an experimental study on the application of quaternions in several machine learning algorithms. Quaternion is a mathematical representation of rotation in three-dimensional space, which can be used to represent complex data transformations. In this study, we explore the use of quaternions to represent and classify rotation data, using randomly generated quaternion data and corresponding labels, converting quaternions to rotation matrices, and using them as input features. Based on quaternions and multiple machine learning algorithms, it has shown higher accuracy and significantly improved performance in prediction tasks. Overall, this study provides an empirical basis for exploiting quaternions for machine learning tasks.
</details>
<details>
<summary>摘要</summary>
这个论文介绍了使用四元数在多种机器学习算法中的实验研究。四元数是三维空间中旋转的数学表示方式，可以用于表示复杂的数据变换。在这个研究中，我们研究了使用四元数来表示和分类旋转数据，使用随机生成的四元数数据和相应的标签，将四元数转换为旋转矩阵，并使其为输入特征。基于四元数和多种机器学习算法，研究结果显示了更高的准确率和明显改善的预测性能。总之，这个研究提供了使用四元数进行机器学习任务的实质基础。
</details></li>
</ul>
<hr>
<h2 id="SoK-Assessing-the-State-of-Applied-Federated-Machine-Learning"><a href="#SoK-Assessing-the-State-of-Applied-Federated-Machine-Learning" class="headerlink" title="SoK: Assessing the State of Applied Federated Machine Learning"></a>SoK: Assessing the State of Applied Federated Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02454">http://arxiv.org/abs/2308.02454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Müller, Maximilian Stäbler, Hugo Gascón, Frank Köster, Florian Matthes</li>
<li>for: 本研究旨在探讨 Federated Machine Learning（FedML）在实际应用中的现状和挑战。</li>
<li>methods: 本研究采用系统性的文献回顾方法，对74篇相关文章进行分析，描述 FedML 实现的特点和趋势，以及驱动其应用的动机和应用领域。</li>
<li>results: 本研究发现，FedML 在隐私敏感领域的应用具有许多优势，但在实际应用中还存在许多挑战，如数据质量问题、安全性和可信度问题等。<details>
<summary>Abstract</summary>
Machine Learning (ML) has shown significant potential in various applications; however, its adoption in privacy-critical domains has been limited due to concerns about data privacy. A promising solution to this issue is Federated Machine Learning (FedML), a model-to-data approach that prioritizes data privacy. By enabling ML algorithms to be applied directly to distributed data sources without sharing raw data, FedML offers enhanced privacy protections, making it suitable for privacy-critical environments. Despite its theoretical benefits, FedML has not seen widespread practical implementation. This study aims to explore the current state of applied FedML and identify the challenges hindering its practical adoption. Through a comprehensive systematic literature review, we assess 74 relevant papers to analyze the real-world applicability of FedML. Our analysis focuses on the characteristics and emerging trends of FedML implementations, as well as the motivational drivers and application domains. We also discuss the encountered challenges in integrating FedML into real-life settings. By shedding light on the existing landscape and potential obstacles, this research contributes to the further development and implementation of FedML in privacy-critical scenarios.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:机器学习（ML）在不同的应用场景中表现出了很remarkable potential; however，its adoption in privacy-critical domains has been limited due to concerns about data privacy. A promising solution to this issue is Federated Machine Learning（FedML），a model-to-data approach that prioritizes data privacy. By enabling ML algorithms to be applied directly to distributed data sources without sharing raw data，FedML offers enhanced privacy protections，making it suitable for privacy-critical environments. Despite its theoretical benefits，FedML has not seen widespread practical implementation. This study aims to explore the current state of applied FedML and identify the challenges hindering its practical adoption. Through a comprehensive systematic literature review，we assess 74 relevant papers to analyze the real-world applicability of FedML. Our analysis focuses on the characteristics and emerging trends of FedML implementations，as well as the motivational drivers and application domains. We also discuss the encountered challenges in integrating FedML into real-life settings. By shedding light on the existing landscape and potential obstacles，this research contributes to the further development and implementation of FedML in privacy-critical scenarios.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Representation-Learning-for-Time-Series-A-Review"><a href="#Unsupervised-Representation-Learning-for-Time-Series-A-Review" class="headerlink" title="Unsupervised Representation Learning for Time Series: A Review"></a>Unsupervised Representation Learning for Time Series: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01578">http://arxiv.org/abs/2308.01578</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mqwfrog/ults">https://github.com/mqwfrog/ults</a></li>
<li>paper_authors: Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, Lizhen Cui</li>
<li>for: 本研究旨在探讨无监督表示学习方法的应用在时间序列数据上，以便学习不同特征表示而不需要每个样本的标注。</li>
<li>methods: 本研究使用了多种不监督表示学习技术，包括自适应表示学习、强化学习和对比学习等。</li>
<li>results: 经验证明，使用不监督表示学习方法可以在9种真实世界数据集上实现高度的特征表示能力，并且可以在不同的数据集上进行跨种类比较。<details>
<summary>Abstract</summary>
Unsupervised representation learning approaches aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling unsupervised representation learning is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities. In recent years, unsupervised representation learning techniques have advanced rapidly in various domains. However, there is a lack of systematic analysis of unsupervised representation learning approaches for time series. To fill the gap, we conduct a comprehensive literature review of existing rapidly evolving unsupervised representation learning approaches for time series. Moreover, we also develop a unified and standardized library, named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast implementations and unified evaluations on various models. With ULTS, we empirically evaluate state-of-the-art approaches, especially the rapidly evolving contrastive learning methods, on 9 diverse real-world datasets. We further discuss practical considerations as well as open research challenges on unsupervised representation learning for time series to facilitate future research in this field.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtableUnsupervised representation learning方法 aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling unsupervised representation learning is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities. In recent years, unsupervised representation learning techniques have advanced rapidly in various domains. However, there is a lack of systematic analysis of unsupervised representation learning approaches for time series. To fill the gap, we conduct a comprehensive literature review of existing rapidly evolving unsupervised representation learning approaches for time series. Moreover, we also develop a unified and standardized library, named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast implementations and unified evaluations on various models. With ULTS, we empirically evaluate state-of-the-art approaches, especially the rapidly evolving contrastive learning methods, on 9 diverse real-world datasets. We further discuss practical considerations as well as open research challenges on unsupervised representation learning for time series to facilitate future research in this field.<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Training-of-Denoising-Diffusion-Model-Using-Dual-Discriminators-for-High-Fidelity-Multi-Speaker-TTS"><a href="#Adversarial-Training-of-Denoising-Diffusion-Model-Using-Dual-Discriminators-for-High-Fidelity-Multi-Speaker-TTS" class="headerlink" title="Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS"></a>Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01573">http://arxiv.org/abs/2308.01573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/komyeongjin/specdiff-gan">https://github.com/komyeongjin/specdiff-gan</a></li>
<li>paper_authors: Myeongjin Ko, Yong-Hoon Choi</li>
<li>for: 本研究旨在提高 diffusion speech synthesis 模型的表现，通过添加两个识别器：扩散识别器和spectrogram识别器，以学习扩散过程的分布和生成数据的分布。</li>
<li>methods: 本研究使用了 diffusion 模型，并结合了 GAN 结构。具体来说，使用了一个扩散识别器和一个spectrogram识别器，以学习扩散过程的分布和生成数据的分布。</li>
<li>results: 对比 FastSpeech2 和 DiffGAN-TTS 等当前状态的艺术模型，本研究的模型在不同的 метриках中表现出优于其他模型，包括 SSIM、MCD、F0 RMSE、STOI、PESQ 等。<details>
<summary>Abstract</summary>
The diffusion model is capable of generating high-quality data through a probabilistic approach. However, it suffers from the drawback of slow generation speed due to the requirement of a large number of time steps. To address this limitation, recent models such as denoising diffusion implicit models (DDIM) focus on generating samples without directly modeling the probability distribution, while models like denoising diffusion generative adversarial networks (GAN) combine diffusion processes with GANs. In the field of speech synthesis, a recent diffusion speech synthesis model called DiffGAN-TTS, utilizing the structure of GANs, has been introduced and demonstrates superior performance in both speech quality and generation speed. In this paper, to further enhance the performance of DiffGAN-TTS, we propose a speech synthesis model with two discriminators: a diffusion discriminator for learning the distribution of the reverse process and a spectrogram discriminator for learning the distribution of the generated data. Objective metrics such as structural similarity index measure (SSIM), mel-cepstral distortion (MCD), F0 root mean squared error (F0 RMSE), short-time objective intelligibility (STOI), perceptual evaluation of speech quality (PESQ), as well as subjective metrics like mean opinion score (MOS), are used to evaluate the performance of the proposed model. The evaluation results show that the proposed model outperforms recent state-of-the-art models such as FastSpeech2 and DiffGAN-TTS in various metrics. Our implementation and audio samples are located on GitHub.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_parts:  - text: "The diffusion model"    translate: "扩散模型"  - text: "capable of generating high-quality data"    translate: "可以生成高质量数据"  - text: "through a probabilistic approach"    translate: "通过 probabilistic 方法"  - text: "However, it suffers from the drawback"    translate: "然而，它受到一个缺点"  - text: "of slow generation speed"    translate: "生成速度较慢"  - text: "due to the requirement of a large number of time steps"    translate: "因为需要较多的时间步骤"  - text: "To address this limitation"    translate: "以解决这些限制"  - text: "recent models such as denoising diffusion implicit models (DDIM)"    translate: "最近的模型，如杂谱扩散隐式模型（DDIM）"  - text: "focus on generating samples without directly modeling the probability distribution"    translate: "注重生成样本，不直接模型概率分布"  - text: "while models like denoising diffusion generative adversarial networks (GAN)"    translate: "如杂谱扩散生成敌方网络（GAN）"  - text: "combine diffusion processes with GANs"    translate: "将扩散过程与 GAN 结合"  - text: "In the field of speech synthesis"    translate: "在语音合成领域"  - text: "a recent diffusion speech synthesis model called DiffGAN-TTS"    translate: "一种最近的扩散语音合成模型，即 DiffGAN-TTS"  - text: "utilizing the structure of GANs"    translate: "利用 GAN 的结构"  - text: "has been introduced and demonstrates superior performance"    translate: "已经引入并示出了出色的表现"  - text: "in both speech quality and generation speed"    translate: "在语音质量和生成速度两个方面"  - text: "In this paper"    translate: "在这篇论文"  - text: "to further enhance the performance of DiffGAN-TTS"    translate: "以进一步提高 DiffGAN-TTS 的表现"  - text: "we propose a speech synthesis model with two discriminators"    translate: "我们提议一种语音合成模型，具有两个抑制器"  - text: "a diffusion discriminator for learning the distribution of the reverse process"    translate: "一个扩散抑制器，用于学习反向过程的分布"  - text: "and a spectrogram discriminator for learning the distribution of the generated data"    translate: "一个spectrogram抑制器，用于学习生成的数据的分布"  - text: "Objective metrics such as structural similarity index measure (SSIM)"    translate: "如结构相似度指标 (SSIM)"  - text: "mel-cepstral distortion (MCD)"    translate: "mel-cepstral 扭曲 (MCD)"  - text: "F0 root mean squared error (F0 RMSE)"    translate: "F0 根均方差 (F0 RMSE)"  - text: "short-time objective intelligibility (STOI)"    translate: "短时间目标可理解性 (STOI)"  - text: "perceptual evaluation of speech quality (PESQ)"    translate: "语音质量的主观评估 (PESQ)"  - text: "as well as subjective metrics like mean opinion score (MOS)"    translate: "以及主观指标如 mean opinion score (MOS)"  - text: "are used to evaluate the performance of the proposed model"    translate: "用于评估提议的模型表现"  - text: "The evaluation results show that the proposed model outperforms"    translate: "评估结果表明，提议的模型在"  - text: "recent state-of-the-art models such as FastSpeech2 and DiffGAN-TTS"    translate: "最近的状态艺术模型，如 FastSpeech2 和 DiffGAN-TTS"  - text: "in various metrics"    translate: "在多个指标中"Note: Some of the translated text may not be exact, as the meaning of the original text may not be perfectly conveyed in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Fast-Slate-Policy-Optimization-Going-Beyond-Plackett-Luce"><a href="#Fast-Slate-Policy-Optimization-Going-Beyond-Plackett-Luce" class="headerlink" title="Fast Slate Policy Optimization: Going Beyond Plackett-Luce"></a>Fast Slate Policy Optimization: Going Beyond Plackett-Luce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01566">http://arxiv.org/abs/2308.01566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Otmane Sakhi, David Rohde, Nicolas Chopin</li>
<li>for: 这篇论文主要针对大规模机器学习系统中的返回板块问题，即在查询时返回一个有序列表的项目。这种技术在搜索、信息检索和推荐系统等领域都有广泛的应用。</li>
<li>methods: 该论文使用政策优化框架来优化大规模决策系统，并提出了一种新的政策类型，基于决策函数的新降低。这种方法简单而高效，可承载巨大的动作空间。</li>
<li>results: 论文对具有百万级动作空间的问题进行比较，并证明了其效果超过常见采用的普拉克特-劳伦谱策略。<details>
<summary>Abstract</summary>
An increasingly important building block of large scale machine learning systems is based on returning slates; an ordered lists of items given a query. Applications of this technology include: search, information retrieval and recommender systems. When the action space is large, decision systems are restricted to a particular structure to complete online queries quickly. This paper addresses the optimization of these large scale decision systems given an arbitrary reward function. We cast this learning problem in a policy optimization framework and propose a new class of policies, born from a novel relaxation of decision functions. This results in a simple, yet efficient learning algorithm that scales to massive action spaces. We compare our method to the commonly adopted Plackett-Luce policy class and demonstrate the effectiveness of our approach on problems with action space sizes in the order of millions.
</details>
<details>
<summary>摘要</summary>
“ Returns 的列表（slate）已成为大规模机器学习系统的关键组件。这些技术在搜索、信息检索和推荐系统中应用。当动作空间较大时，决策系统会受到特定结构的限制，以快速完成在线查询。本文通过policy优化框架优化大规模决策系统，并提出了一种新的策略类型，基于决策函数的新降级。这种简单 yet efficient的学习算法可扩展到巨大的动作空间。我们与常见的Plackett-Luce策略类比较，并在动作空间大约为百万的问题上证明了我们的方法的有效性。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Federated-Learning-in-Wireless-Networks-Pruning-Tackles-Bandwidth-Scarcity-and-System-Heterogeneity"><a href="#Hierarchical-Federated-Learning-in-Wireless-Networks-Pruning-Tackles-Bandwidth-Scarcity-and-System-Heterogeneity" class="headerlink" title="Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity"></a>Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01562">http://arxiv.org/abs/2308.01562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Ferdous Pervej, Richeng Jin, Huaiyu Dai</li>
<li>for: 这篇论文目的是提出一种基于层次联合学习的无线网络模型，以适应实际无线网络中的限制和硬件限制。</li>
<li>methods: 论文使用了模型剔除技术，并提出了一种层次联合学习（PHFL）算法，以满足各种硬件限制和延迟限制。</li>
<li>results: 通过大量的 simulate，论文证明了该算法可以提高测试精度、增加wall clock时间、减少能耗和带宽需求。<details>
<summary>Abstract</summary>
While a practical wireless network has many tiers where end users do not directly communicate with the central server, the users' devices have limited computation and battery powers, and the serving base station (BS) has a fixed bandwidth. Owing to these practical constraints and system models, this paper leverages model pruning and proposes a pruning-enabled hierarchical federated learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper bound of the convergence rate that clearly demonstrates the impact of the model pruning and wireless communications between the clients and the associated BS. Then we jointly optimize the model pruning ratio, central processing unit (CPU) frequency and transmission power of the clients in order to minimize the controllable terms of the convergence bound under strict delay and energy constraints. However, since the original problem is not convex, we perform successive convex approximation (SCA) and jointly optimize the parameters for the relaxed convex problem. Through extensive simulation, we validate the effectiveness of our proposed PHFL algorithm in terms of test accuracy, wall clock time, energy consumption and bandwidth requirement.
</details>
<details>
<summary>摘要</summary>
而实际无线网络具有多层次结构，用户设备具有有限的计算和电池能力，服务基站（BS）具有固定带宽。由于这些实际约束和系统模型，这篇论文利用模型剔除和提出了剔除启用的层次联合学习（PHFL）在不同类型网络（HetNets）中。我们首先得出了模型剔除对 converge 速率的Upper bound，并明确地显示了无线通信和客户端与关联的BS之间的模型剔除和计算剔除的影响。然后，我们联合优化客户端的模型剔除比例、中央处理器频率和传输功率，以最小化控制性 bound 下的执行时间和能量消耗。然而，由于原始问题不是凸型问题，我们使用Successive Convex Approximation（SCA）进行凸化优化参数，并联合优化参数以获得 relaxed 凸型问题的解。通过广泛的Simulation，我们证明了我们提出的PHFL算法在测试准确率、墙 clock 时间、能源消耗和带宽要求方面的有效性。
</details></li>
</ul>
<hr>
<h2 id="Motion-Planning-Diffusion-Learning-and-Planning-of-Robot-Motions-with-Diffusion-Models"><a href="#Motion-Planning-Diffusion-Learning-and-Planning-of-Robot-Motions-with-Diffusion-Models" class="headerlink" title="Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models"></a>Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01557">http://arxiv.org/abs/2308.01557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joao Carvalho, An T. Le, Mark Baierl, Dorothea Koert, Jan Peters</li>
<li>for: 学习路径分布可以帮助加速机器人运动规划优化。已有成功的计划中，使用这些先前学习的路径生成模型作为新的规划问题的先验知识是非常有利的。</li>
<li>methods: 我们提议使用扩散模型来学习先验知识。通过扩散模型的逆噪处理，直接从任务目标条件下采样 posterior  trajectory 分布。此外，扩散模型在高维设定下能够有效地编码数据的多模性，这特别适合大型 trajectory 数据集。</li>
<li>results: 我们的实验表明，扩散模型是高维 trajectory 分布的强大先验知识。在 simulated 平面机器人和 7-odo 机器人臂 manipulate 环境中，我们的方法与基eline 进行比较，并在未看到过障碍物的环境中进行测试，以证明我们的方法的普适性。<details>
<summary>Abstract</summary>
Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several baselines in simulated planar robot and 7-dof robot arm manipulator environments. To assess the generalization capabilities of our method, we test it in environments with previously unseen obstacles. Our experiments show that diffusion models are strong priors to encode high-dimensional trajectory distributions of robot motions.
</details>
<details>
<summary>摘要</summary>
学习运动轨迹分布可以帮助加速机器人运动规划优化。已经成功的计划中，学习运动轨迹生成模型作为优先是非常有利的。先前的工作提出了多种利用这个优先来启动运动规划问题。可以从优先抽样或者在最大 posterior 形式中使用优先分布来优化运动规划。在这个工作中，我们提议学习扩散模型作为优先。我们可以通过扩散模型的逆减雑过程直接从后 posterior 轨迹分布中抽样，并且利用扩散模型可以有效地编码数据的多模性，尤其是在高维设定下。为了证明我们的方法效果，我们在 simulated 平面机器人和7-度 freedom 机器人 manipulate 环境中对我们的提议方法进行了比较。为了评估我们的方法泛化能力，我们在未看到的障碍物环境中进行了测试。我们的实验表明，扩散模型是高维轨迹分布的机器人运动优先。
</details></li>
</ul>
<hr>
<h2 id="InterAct-Exploring-the-Potentials-of-ChatGPT-as-a-Cooperative-Agent"><a href="#InterAct-Exploring-the-Potentials-of-ChatGPT-as-a-Cooperative-Agent" class="headerlink" title="InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent"></a>InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01552">http://arxiv.org/abs/2308.01552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Lin Chen, Cheng-Shang Chang</li>
<li>for: 这个研究论文探讨了基于OpenAI的ChatGPT的embodied Agent系统的 интеграция，并评估了其对交互决策标准的影响。</li>
<li>methods: 我们引入了一种称为InterAct的方法，将ChatGPT fed with varied prompts，并将其分配为多个角色，如检查员和排序员，然后将其与原始语言模型结合。</li>
<li>results: 我们的研究显示，在AlfWorld中，包含6个任务的 simulate household environment中，ChatGPT的成功率达到98%，这表明ChatGPT在实际世界中完成复杂任务的能力强，从而预示了任务规划领域的进一步发展。<details>
<summary>Abstract</summary>
This research paper delves into the integration of OpenAI's ChatGPT into embodied agent systems, evaluating its influence on interactive decision-making benchmark. Drawing a parallel to the concept of people assuming roles according to their unique strengths, we introduce InterAct. In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model. Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering. The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.
</details>
<details>
<summary>摘要</summary>
这篇研究论文探讨了OpenAI的ChatGPT在具体体系中的整合，评估其对交互决策标准的影响。从人们按照自己特点任务的角度出发，我们提出了InterAct方法。在这种方法中，我们对ChatGPT提供了多种提示，将其分配为多个角色，如检查员和排序员，然后与原始语言模型结合。我们的研究显示在AlfWorld中，包括6种不同任务的 simulate 的家庭环境中，ChatGPT的成功率达到98%，这些结果表明ChatGPT在真实世界中执行复杂任务时的能力，这对任务规划带来了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="MusicLDM-Enhancing-Novelty-in-Text-to-Music-Generation-Using-Beat-Synchronous-Mixup-Strategies"><a href="#MusicLDM-Enhancing-Novelty-in-Text-to-Music-Generation-Using-Beat-Synchronous-Mixup-Strategies" class="headerlink" title="MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies"></a>MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01546">http://arxiv.org/abs/2308.01546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo Dubnov</li>
<li>for: 这篇论文的目的是提出一种新的文本到音乐生成模型，以 Addressing the challenges of generating music with limited data and sensitivity to copyright and plagiarism.</li>
<li>methods: 该模型使用了Stable Diffusion和AudioLDM架构，并通过重新训练CLAP和Hifi-GAN vocoder来适应音乐领域。另外，该模型还使用了 beat tracking 模型和两种不同的mixup策略来进行数据扩展，以便生成更多样化的音乐。</li>
<li>results: 该模型可以生成更高质量和更多样化的音乐，同时仍然保持与输入文本的相关性。此外，该模型还可以提高CLAP score和新的评价指标，证明其在生成音乐的质量和创新性方面具有优势。<details>
<summary>Abstract</summary>
Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.
</details>
<details>
<summary>摘要</summary>
Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.
</details></li>
</ul>
<hr>
<h2 id="Lode-Enhancer-Level-Co-creation-Through-Scaling"><a href="#Lode-Enhancer-Level-Co-creation-Through-Scaling" class="headerlink" title="Lode Enhancer: Level Co-creation Through Scaling"></a>Lode Enhancer: Level Co-creation Through Scaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01543">http://arxiv.org/abs/2308.01543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debosmita Bhaumik, Julian Togelius, Georgios N. Yannakakis, Ahmed Khalifa</li>
<li>for: 用AI技术帮助创建2D游戏关卡。</li>
<li>methods: 使用深度神经网络进行水平增强，以帮助用户在不同分辨率下创建和编辑关卡。</li>
<li>results: 通过训练神经网络，实现在不同分辨率下的同步编辑功能，并且通过提供高优先级块来增强水平的创建能力。<details>
<summary>Abstract</summary>
We explore AI-powered upscaling as a design assistance tool in the context of creating 2D game levels. Deep neural networks are used to upscale artificially downscaled patches of levels from the puzzle platformer game Lode Runner. The trained networks are incorporated into a web-based editor, where the user can create and edit levels at three different levels of resolution: 4x4, 8x8, and 16x16. An edit at any resolution instantly transfers to the other resolutions. As upscaling requires inventing features that might not be present at lower resolutions, we train neural networks to reproduce these features. We introduce a neural network architecture that is capable of not only learning upscaling but also giving higher priority to less frequent tiles. To investigate the potential of this tool and guide further development, we conduct a qualitative study with 3 designers to understand how they use it. Designers enjoyed co-designing with the tool, liked its underlying concept, and provided feedback for further improvement.
</details>
<details>
<summary>摘要</summary>
我们探索使用人工智能进行缩放作为游戏平台层级设计工具。我们使用深度神经网络缩放游戏《宝藏猎人》中的各个层级，并将训练好的网络集成到了基于网络的编辑器中。用户可以在3个不同的分辨率上创建和编辑层级：4x4、8x8和16x16。任何编辑操作都会同步到其他分辨率上。由于缩放需要创造不存在于较低分辨率上的特征，我们训练神经网络来重现这些特征。我们提出了一种神经网络架构，可以不仅学习缩放，还可以给较少出现的块优先级更高。为了了解这个工具的潜在可能性并提供进一步改进的建议，我们进行了3名设计师的质量研究，了解他们如何使用这个工具。设计师喜欢与这个工具合作，喜欢它的核心思想，并提供了进一步改进的反馈。
</details></li>
</ul>
<hr>
<h2 id="MFIM-Megapixel-Facial-Identity-Manipulation"><a href="#MFIM-Megapixel-Facial-Identity-Manipulation" class="headerlink" title="MFIM: Megapixel Facial Identity Manipulation"></a>MFIM: Megapixel Facial Identity Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01536">http://arxiv.org/abs/2308.01536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanghyeon Na</li>
<li>for: 本研究的目的是提出一种新的面孔交换框架，即高像素面孔标识修饰（MFIM）。</li>
<li>methods: 本模型使用预训练的StyleGAN进行GAN-倒转，以生成高像素图像。此外，本模型还使用3DMM来控制不同人脸特征的混合。</li>
<li>results: 经过广泛的实验，我们表明我们的模型可以达到状态对的性能，并且可以自由地调整新的人脸特征。此外，我们还提出了一种新的操作called ID mixing，可以通过semantic混合不同人脸特征来创建新的人脸标识。<details>
<summary>Abstract</summary>
Face swapping is a task that changes a facial identity of a given image to that of another person. In this work, we propose a novel face-swapping framework called Megapixel Facial Identity Manipulation (MFIM). The face-swapping model should achieve two goals. First, it should be able to generate a high-quality image. We argue that a model which is proficient in generating a megapixel image can achieve this goal. However, generating a megapixel image is generally difficult without careful model design. Therefore, our model exploits pretrained StyleGAN in the manner of GAN-inversion to effectively generate a megapixel image. Second, it should be able to effectively transform the identity of a given image. Specifically, it should be able to actively transform ID attributes (e.g., face shape and eyes) of a given image into those of another person, while preserving ID-irrelevant attributes (e.g., pose and expression). To achieve this goal, we exploit 3DMM that can capture various facial attributes. Specifically, we explicitly supervise our model to generate a face-swapped image with the desirable attributes using 3DMM. We show that our model achieves state-of-the-art performance through extensive experiments. Furthermore, we propose a new operation called ID mixing, which creates a new identity by semantically mixing the identities of several people. It allows the user to customize the new identity.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Face swapping is a task that changes a facial identity of a given image to that of another person. In this work, we propose a novel face-swapping framework called Megapixel Facial Identity Manipulation (MFIM). The face-swapping model should achieve two goals. First, it should be able to generate a high-quality image. We argue that a model which is proficient in generating a megapixel image can achieve this goal. However, generating a megapixel image is generally difficult without careful model design. Therefore, our model exploits pretrained StyleGAN in the manner of GAN-inversion to effectively generate a megapixel image. Second, it should be able to effectively transform the identity of a given image. Specifically, it should be able to actively transform ID attributes (e.g., face shape and eyes) of a given image into those of another person, while preserving ID-irrelevant attributes (e.g., pose and expression). To achieve this goal, we exploit 3DMM that can capture various facial attributes. Specifically, we explicitly supervise our model to generate a face-swapped image with the desirable attributes using 3DMM. We show that our model achieves state-of-the-art performance through extensive experiments. Furthermore, we propose a new operation called ID mixing, which creates a new identity by semantically mixing the identities of several people. It allows the user to customize the new identity."中文翻译：面部换位是一项任务，把给定图像中的面部标识换成另一个人的面部标识。在这项工作中，我们提出了一种新的面部换位框架，称为高像素面部标识修饰（MFIM）。这个模型应该实现两个目标。首先，它应该能够生成高质量图像。我们认为一个能够生成高像素图像的模型就可以实现这一目标。然而，生成高像素图像通常需要精心的模型设计。因此，我们利用预训练的StyleGAN，通过GAN-反向方式来有效地生成高像素图像。其次，它应该能够有效地将给定图像中的面部标识转换成另一个人的面部标识。具体来说，它应该能够活动地将ID特征（例如脸形和眼睛）转换成另一个人的ID特征，保留ID不相关的特征（例如姿势和表情）。为了实现这一目标，我们利用3DMM，可以捕捉各种面部特征。我们显式地监督我们的模型，使其生成符合愿望的面部换位图像，使用3DMM。我们示出了我们模型在各种实验中的状态级表现。此外，我们还提出了一种新的操作，称为ID混合，它可以创造一个新的标识，通过semantic地混合多个人的标识。这allow用户自定义新的标识。
</details></li>
</ul>
<hr>
<h2 id="Food-Classification-using-Joint-Representation-of-Visual-and-Textual-Data"><a href="#Food-Classification-using-Joint-Representation-of-Visual-and-Textual-Data" class="headerlink" title="Food Classification using Joint Representation of Visual and Textual Data"></a>Food Classification using Joint Representation of Visual and Textual Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02562">http://arxiv.org/abs/2308.02562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Mittal, Puneet Goyal, Joohi Chauhan</li>
<li>for: 本研究旨在提出一种多模态分类框架，用于健康食品分类。</li>
<li>methods: 提议的网络使用修改版EfficientNet和Mish激活函数进行图像分类，而传统的BERT变换器网络用于文本分类。</li>
<li>results: 对于大规模开源数据集UPMC Food-101，提议的网络与其他状态时方法进行比较，实验结果显示，提议的网络在图像和文本分类任务中具有显著的优势，与第二最佳方法相比，图像分类精度提高11.57%，文本分类精度提高6.34%。<details>
<summary>Abstract</summary>
Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
食品分类是医疗领域中的重要任务。在这种工作中，我们提议了一种多modal分类框架，使用修改后的EfficientNet和Mish活动函数进行图像分类，而传统的BERT变换器网络用于文本分类。我们的提议网络和其他状态公共方法在大型开源数据集UPMC Food-101上进行评估。实验结果表明，我们的提议网络在图像和文本分类方面具有显著的优势，与第二好的方法相比，图像分类精度提高11.57%，文本分类精度提高6.34%。我们还对文本分类使用机器学习和深度学习模型进行比较分析，结果表明，我们的方法在预测结果中具有更高的准确率、精度和回归率。
</details></li>
</ul>
<hr>
<h2 id="Circumventing-Concept-Erasure-Methods-For-Text-to-Image-Generative-Models"><a href="#Circumventing-Concept-Erasure-Methods-For-Text-to-Image-Generative-Models" class="headerlink" title="Circumventing Concept Erasure Methods For Text-to-Image Generative Models"></a>Circumventing Concept Erasure Methods For Text-to-Image Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01508">http://arxiv.org/abs/2308.01508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nyu-dice-lab/circumventing-concept-erasure">https://github.com/nyu-dice-lab/circumventing-concept-erasure</a></li>
<li>paper_authors: Minh Pham, Kelly O. Marshall, Chinmay Hegde</li>
<li>for: 本研究旨在检验五种最近提出的概念消除方法，以及这些方法是否能够彻底从文本到图像模型中除去目标概念。</li>
<li>methods: 本研究使用了五种最近提出的概念消除方法，包括隐藏概念、替换概念、排除概念、屏蔽概念和筛选概念等方法。</li>
<li>results: 研究发现，无论使用哪种方法，都无法彻底从文本到图像模型中除去目标概念。特别是，使用特殊学习的词嵌入可以很容易地从sanitized模型中恢复排除的概念，而无需对模型的权重进行任何修改。这些结果表明，后期概念消除方法不够可靠，并质疑它们在AI安全中的应用。<details>
<summary>Abstract</summary>
Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to "erase" sensitive concepts from text-to-image models. In this work, we examine five recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we leverage the existence of special learned word embeddings that can retrieve "erased" concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Local-Global-Temporal-Fusion-Network-with-an-Attention-Mechanism-for-Multiple-and-Multiclass-Arrhythmia-Classification"><a href="#Local-Global-Temporal-Fusion-Network-with-an-Attention-Mechanism-for-Multiple-and-Multiclass-Arrhythmia-Classification" class="headerlink" title="Local-Global Temporal Fusion Network with an Attention Mechanism for Multiple and Multiclass Arrhythmia Classification"></a>Local-Global Temporal Fusion Network with an Attention Mechanism for Multiple and Multiclass Arrhythmia Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02416">http://arxiv.org/abs/2308.02416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Kwan Kim, Minji Lee, Kunwook Jo, Hee Seok Song, Seong-Whan Lee</li>
<li>for: This paper aims to develop a clinical decision support system (CDSS) for arrhythmia classification, addressing the challenge of varying arrhythmia lengths.</li>
<li>methods: The proposed method combines local temporal information extraction, global pattern extraction, and local-global information fusion with attention to detect and classify arrhythmia with a constrained input length.</li>
<li>results: The proposed method achieved superior performance on the MIT-BIH arrhythmia database (MITDB) and MIT-BIH atrial fibrillation database (AFDB), outperforming comparison models. The method also demonstrated good generalization ability when tested on a different database.<details>
<summary>Abstract</summary>
Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms (ECGs). However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local temporal information extraction, (ii) global pattern extraction, and (iii) local-global information fusion with attention to perform arrhythmia detection and classification with a constrained input length. The 10-class and 4-class performances of our approach were assessed by detecting the onset and offset of arrhythmia as an episode and the duration of arrhythmia based on the MIT-BIH arrhythmia database (MITDB) and MIT-BIH atrial fibrillation database (AFDB), respectively. The results were statistically superior to those achieved by the comparison models. To check the generalization ability of the proposed method, an AFDB-trained model was tested on the MITDB, and superior performance was attained compared with that of a state-of-the-art model. The proposed method can capture local-global information and dynamics without incurring information losses. Therefore, arrhythmias can be recognized more accurately, and their occurrence times can be calculated; thus, the clinical field can create more accurate treatment plans by using the proposed method.
</details>
<details>
<summary>摘要</summary>
临床决策支持系统（CDSS）已广泛应用于心电图（ECG）上的心Rate变化的诊断和分类。然而，为了建立一个CDSS来进行心Rate变化的诊断和分类，存在许多挑战。主要的问题在于心Rate变化的持续时间不同，导致过去的方法无法考虑这种情况。因此，我们提出了一种框架，包括（i）本地时间信息提取，（ii）全局模式提取，以及（iii）本地-全局信息融合 WITH attention来实现受限输入长度下的心Rate变化诊断和分类。我们使用MIT-BIH心Rate变化数据库（MITDB）和MIT-BIHatrioventricular flutter数据库（AFDB）进行10类和4类的评估，结果显著高于比较模型。为了证明提出的方法的通用能力，使用AFDB训练好的模型在MITDB上进行测试，并实现了与当前状态艺术模型的比较。这种方法可以捕捉本地-全局信息和动态，不会产生信息损失，因此可以更准确地识别心Rate变化，并计算其发生时间，从而为临床领域创造更加准确的治疗计划。
</details></li>
</ul>
<hr>
<h2 id="Online-Multi-Task-Learning-with-Recursive-Least-Squares-and-Recursive-Kernel-Methods"><a href="#Online-Multi-Task-Learning-with-Recursive-Least-Squares-and-Recursive-Kernel-Methods" class="headerlink" title="Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods"></a>Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01938">http://arxiv.org/abs/2308.01938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel R. Lencione, Fernando J. Von Zuben</li>
<li>for: 这paper是为了解决在线多任务学习（MTL）回归问题而提出的两种新方法。</li>
<li>methods: 这paper使用了高性能图 струкured MTL formulation，并基于Weighted Recursive Least Squares（WRLS）和Online Sparse Least Squares Support Vector Regression（OSLSSVR）的再归版本。</li>
<li>results: 通过采用任务堆叠变换，这paper实现了一个包含多个任务关系的矩阵，并将其 integrate到MT-WRLS和MT-OSLSSVR中。相比现有Literature，这paper实现了精确和近似归并，并在输入空间维度（MT-WRLS）或字典大小（MT-OSLSSVR）上实现了 quitropic per-instance cost。在一个实际世界的风速预测案例中，我们比较了这paper所提出的方法和其他竞争方法，并发现了 significan performance gain。<details>
<summary>Abstract</summary>
This paper introduces two novel approaches for Online Multi-Task Learning (MTL) Regression Problems. We employ a high performance graph-based MTL formulation and develop its recursive versions based on the Weighted Recursive Least Squares (WRLS) and the Online Sparse Least Squares Support Vector Regression (OSLSSVR). Adopting task-stacking transformations, we demonstrate the existence of a single matrix incorporating the relationship of multiple tasks and providing structural information to be embodied by the MT-WRLS method in its initialization procedure and by the MT-OSLSSVR in its multi-task kernel function. Contrasting the existing literature, which is mostly based on Online Gradient Descent (OGD) or cubic inexact approaches, we achieve exact and approximate recursions with quadratic per-instance cost on the dimension of the input space (MT-WRLS) or on the size of the dictionary of instances (MT-OSLSSVR). We compare our online MTL methods to other contenders in a real-world wind speed forecasting case study, evidencing the significant gain in performance of both proposed approaches.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文介绍了两种新的在线多任务学习（MTL）回归问题的方法。我们使用高性能的图形基于的 MTL 形式，并开发了其循环版本，基于最小二乘回归（WRLS）和在线稀疏最小二乘支持向量回归（OSLSSVR）。通过任务堆叠变换，我们表明了一个单个矩阵可以捕捉多个任务之间的关系，并提供结构信息。这个矩阵在 MT-WRLS 方法的初始化过程中使用，以及 MT-OSLSSVR 方法的多任务核函数中使用。与现有文献，大多使用在线梯度下降（OGD）或立方不精确方法，我们实现了精确和近似循环的quadratic per-instance cost在输入空间维度（MT-WRLS）或实例词典大小（MT-OSLSSVR）。我们将这两种在线 MTL 方法与其他竞争者进行比较，在一个实际的风速预测案例中，证明了我们的两种方法具有显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Minimax-Optimal-Q-Learning-with-Nearest-Neighbors"><a href="#Minimax-Optimal-Q-Learning-with-Nearest-Neighbors" class="headerlink" title="Minimax Optimal $Q$ Learning with Nearest Neighbors"></a>Minimax Optimal $Q$ Learning with Nearest Neighbors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01490">http://arxiv.org/abs/2308.01490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puning Zhao, Lifeng Lai<br>for: This paper focuses on modifying the original $Q$ learning method to make it suitable for continuous state spaces, and proposes two new $Q$ learning methods to improve the convergence rate.methods: The paper uses nearest neighbor approach to estimate $Q$ function, but with a direct nearest neighbor approach instead of the kernel nearest neighbor in discretized regions.results: The paper shows that both offline and online methods are minimax rate optimal, and the time complexity is significantly improved in high dimensional state spaces.<details>
<summary>Abstract</summary>
$Q$ learning is a popular model free reinforcement learning method. Most of existing works focus on analyzing $Q$ learning for finite state and action spaces. If the state space is continuous, then the original $Q$ learning method can not be directly used. A modification of the original $Q$ learning method was proposed in (Shah and Xie, 2018), which estimates $Q$ values with nearest neighbors. Such modification makes $Q$ learning suitable for continuous state space. (Shah and Xie, 2018) shows that the convergence rate of estimated $Q$ function is $\tilde{O}(T^{-1/(d+3)})$, which is slower than the minimax lower bound $\tilde{\Omega}(T^{-1/(d+2)})$, indicating that this method is not efficient. This paper proposes two new $Q$ learning methods to bridge the gap of convergence rates in (Shah and Xie, 2018), with one of them being offline, while the other is online. Despite that we still use nearest neighbor approach to estimate $Q$ function, the algorithms are crucially different from (Shah and Xie, 2018). In particular, we replace the kernel nearest neighbor in discretized region with a direct nearest neighbor approach. Consequently, our approach significantly improves the convergence rate. Moreover, the time complexity is also significantly improved in high dimensional state spaces. Our analysis shows that both offline and online methods are minimax rate optimal.
</details>
<details>
<summary>摘要</summary>
$Q$ 学习是一种流行的模型自由奖励学习方法。大多数现有工作都集中在finite state和动作空间上分析 $Q$ 学习。如果状态空间是连续的，那么原始 $Q$ 学习方法直接使用不可靠。(Shah and Xie, 2018) 提出了修改原始 $Q$ 学习方法的方法，该方法通过 nearest neighbors 来估计 $Q$ 值。这种修改使 $Q$ 学习适用于连续状态空间。(Shah and Xie, 2018) 显示了 estimated $Q$ 函数的减少率为 $\tilde{O}(T^{-1/(d+3)})$, 这 slower than minimax lower bound $\tilde{\Omega}(T^{-1/(d+2)})$, 这表明这种方法不够有效。这篇论文提出了两种新的 $Q$ 学习方法，其中一种是 offline，另一种是 online。尽管我们仍然使用 nearest neighbor 方法来估计 $Q$ 函数，但这些算法与 (Shah and Xie, 2018) 的方法有所不同。具体来说，我们将 kernel nearest neighbor 在离散区域中替换为直接 nearest neighbor 方法。因此，我们的方法可以 significatively 提高减少率。此外，在高维状态空间中，我们的时间复杂度也得到了显著改善。我们的分析表明，两种方法都是 minimax 率最优。
</details></li>
</ul>
<hr>
<h2 id="Efficient-neural-supersampling-on-a-novel-gaming-dataset"><a href="#Efficient-neural-supersampling-on-a-novel-gaming-dataset" class="headerlink" title="Efficient neural supersampling on a novel gaming dataset"></a>Efficient neural supersampling on a novel gaming dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01483">http://arxiv.org/abs/2308.01483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Mercier, Ruan Erasmus, Yashesh Savani, Manik Dhingra, Fatih Porikli, Guillaume Berger</li>
<li>for: 提高视频游戏的实时渲染效能，以满足更高的分辨率、帧率和 фото真实性要求。</li>
<li>methods: 使用神经网络算法进行精度较高的超采样渲染内容，比现有方法高效4倍。同时，我们提供了一个新的数据集，包括视频游戏中的运动 вектор和深度信息，这些信息可以用于测试和提高超解算法的性能。</li>
<li>results: 与现有方法相比，我们的方法可以提供4倍的效率，同时保持同等的准确性。此外，我们的数据集可以填补现有数据集的空白，并成为测试和提高超解技术的 valuable 资源。<details>
<summary>Abstract</summary>
Real-time rendering for video games has become increasingly challenging due to the need for higher resolutions, framerates and photorealism. Supersampling has emerged as an effective solution to address this challenge. Our work introduces a novel neural algorithm for supersampling rendered content that is 4 times more efficient than existing methods while maintaining the same level of accuracy. Additionally, we introduce a new dataset which provides auxiliary modalities such as motion vectors and depth generated using graphics rendering features like viewport jittering and mipmap biasing at different resolutions. We believe that this dataset fills a gap in the current dataset landscape and can serve as a valuable resource to help measure progress in the field and advance the state-of-the-art in super-resolution techniques for gaming content.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-covariance-estimation-for-stochastic-gradient-descent-under-Markovian-sampling"><a href="#Online-covariance-estimation-for-stochastic-gradient-descent-under-Markovian-sampling" class="headerlink" title="Online covariance estimation for stochastic gradient descent under Markovian sampling"></a>Online covariance estimation for stochastic gradient descent under Markovian sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01481">http://arxiv.org/abs/2308.01481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Roy, Krishnakumar Balasubramanian</li>
<li>for: 这个论文主要针对 Stochastic Gradient Descent (SGD) 的在 Markovian 采样下的在线拟合方法 Covariance 估计。</li>
<li>methods: 论文使用了 batch-means 方法来估计 Covariance，并且对 Markovian 采样进行了分析。</li>
<li>results: 论文显示了在 Markovian 采样下，batch-means 方法的 convergence 率为 $O\big(\sqrt{d},n^{-1&#x2F;8}(\log n)^{1&#x2F;4}\big)$ 和 $O\big(\sqrt{d},n^{-1&#x2F;8}\big)$，与 $\iid$  случа子中最佳的 convergence 率相当，即Logarithmic 因子。此外，论文还研究了 SGD 动态中 $\ell_2$ 范数的错误的 converge 率，以及在 Markovian 采样下的 SGD 训练中的Linear 和 Logistic 回归模型的 numerics 示例。最后，论文应用了这些结论来解决了一个挑战性的攻击者可以在训练过程中随机修改特征来增加被分类到特定目标类的概率的问题。<details>
<summary>Abstract</summary>
We study the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the convergence rates of the covariance estimator are $O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and $O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent Markovian sampling, respectively, with $d$ representing dimensionality and $n$ denoting the number of observations or SGD iterations. Remarkably, these rates match the best-known convergence rate previously established for the independent and identically distributed ($\iid$) case by \cite{zhu2021online}, up to logarithmic factors. Our analysis overcomes significant challenges that arise due to Markovian sampling, leading to the introduction of additional error terms and complex dependencies between the blocks of the batch-means covariance estimator. Moreover, we establish the convergence rate for the first four moments of the $\ell_2$ norm of the error of SGD dynamics under state-dependent Markovian data, which holds potential interest as an independent result. To validate our theoretical findings, we provide numerical illustrations to derive confidence intervals for SGD when training linear and logistic regression models under Markovian sampling. Additionally, we apply our approach to tackle the intriguing problem of strategic classification with logistic regression, where adversaries can adaptively modify features during the training process to increase their chances of being classified in a specific target class.
</details>
<details>
<summary>摘要</summary>
我们研究在采用Markovian sampling的线上遮盾Batch-means协方差估计器中的Stochastic Gradient Descent（SGD）。我们表明了协方差估计器的数据速度为$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$和$O\big(\sqrt{d}\,n^{-1/8}\big)$，具体取决于Markovian sampling的状态。这些速度与以前在独立同分布（iid）情况下所得到的最佳速度相匹配，仅带有对数因子。我们的分析解决了由Markovian sampling引起的重要挑战，包括附加的错误项和几何上的复杂对话。此外，我们还证明了SGD动态中的第四个几何中的错误的$\ell_2$ нор的数据速度，这个结果可能具有独立的意义。在实验中，我们提供了几个数据示例，以建立SGD训练Linear和Logistic regression模型的信任区间。此外，我们还应用我们的方法解决了具有挑战性的分类问题，其中敌人可以在训练过程中随机修改特征，以增加他们被特定目标类别中分类的机会。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Machine-Learning-for-Discovery-Statistical-Challenges-Opportunities"><a href="#Interpretable-Machine-Learning-for-Discovery-Statistical-Challenges-Opportunities" class="headerlink" title="Interpretable Machine Learning for Discovery: Statistical Challenges &amp; Opportunities"></a>Interpretable Machine Learning for Discovery: Statistical Challenges &amp; Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01475">http://arxiv.org/abs/2308.01475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Genevera I. Allen, Luqin Gan, Lili Zheng</li>
<li>for: 本研究的目的是探讨可解释机器学习技术在大数据中进行发现和探索的应用。</li>
<li>methods: 本研究使用了多种可解释机器学习技术，包括可视化、预测和数据分析等方法，以便从大数据中提取有用的信息和发现新知识。</li>
<li>results: 本研究发现了一些在可解释机器学习技术应用中的挑战，包括数据采样和稳定性问题，以及验证发现的困难。同时，研究也提出了一些解决这些挑战的方法和技术。<details>
<summary>Abstract</summary>
New technologies have led to vast troves of large and complex datasets across many scientific domains and industries. People routinely use machine learning techniques to not only process, visualize, and make predictions from this big data, but also to make data-driven discoveries. These discoveries are often made using Interpretable Machine Learning, or machine learning models and techniques that yield human understandable insights. In this paper, we discuss and review the field of interpretable machine learning, focusing especially on the techniques as they are often employed to generate new knowledge or make discoveries from large data sets. We outline the types of discoveries that can be made using Interpretable Machine Learning in both supervised and unsupervised settings. Additionally, we focus on the grand challenge of how to validate these discoveries in a data-driven manner, which promotes trust in machine learning systems and reproducibility in science. We discuss validation from both a practical perspective, reviewing approaches based on data-splitting and stability, as well as from a theoretical perspective, reviewing statistical results on model selection consistency and uncertainty quantification via statistical inference. Finally, we conclude by highlighting open challenges in using interpretable machine learning techniques to make discoveries, including gaps between theory and practice for validating data-driven-discoveries.
</details>
<details>
<summary>摘要</summary>
新技术使得各科学领域和产业中的大量大数据备受欢迎。人们常常使用机器学习技术不仅处理、视觉和预测这些大数据，还可以通过机器学习模型和技术获得人类可理解的发现。在这篇论文中，我们讨论了机器学习可解释的场景，特别是在大数据集中使用机器学习模型和技术来获得新的发现。我们还详细介绍了在超级vised和无级vised情况下使用机器学习可解释的发现方法，以及如何验证这些发现的数据驱动方法。我们还讨论了验证这些发现的挑战，包括数据分割和稳定性问题，以及统计学的模型选择一致性和不确定性量化问题。最后，我们结束这篇文章，描述了使用机器学习可解释技术进行发现时存在的开放挑战。
</details></li>
</ul>
<hr>
<h2 id="Reverse-Stable-Diffusion-What-prompt-was-used-to-generate-this-image"><a href="#Reverse-Stable-Diffusion-What-prompt-was-used-to-generate-this-image" class="headerlink" title="Reverse Stable Diffusion: What prompt was used to generate this image?"></a>Reverse Stable Diffusion: What prompt was used to generate this image?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01472">http://arxiv.org/abs/2308.01472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah</li>
<li>for: 本研究的目的是提出一种新的文本描述生成模型，用于预测由生成扩散模型生成的图像所关联的文本描述。</li>
<li>methods: 我们采用了一种组合白盒和黑盒模型的方法，其中包括了聚合文本描述 regression 和多标签词汇分类目标函数。此外，我们还采用了一种师生学习程序和领域适应诊断器学习方法来提高方法的性能。</li>
<li>results: 我们在DiffusionDB数据集上进行了实验，并取得了优秀的结果。在白盒模型上，我们的新学习框架得到了最高的提升。此外，我们还发现了一个有趣的发现：通过直接将扩散模型用于文本-图像生成任务，可以使模型生成的图像与输入文本更加吻合。<details>
<summary>Abstract</summary>
Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities between samples in the source and target domains as extra features. We conduct experiments on the DiffusionDB data set, predicting text prompts from images generated by Stable Diffusion. Our novel learning framework produces excellent results on the aforementioned task, yielding the highest gains when applied on the white-box model. In addition, we make an interesting discovery: training a diffusion model on the prompt generation task can make the model generate images that are much better aligned with the input prompts, when the model is directly reused for text-to-image generation.
</details>
<details>
<summary>摘要</summary>
文本至图像协托模型，如稳定协托，最近吸引了许多研究人员的关注，而协托过程的逆转也可以更好地理解生成过程和如何引入提示以获得所需的图像。为此，我们介绍了预测图像生成模型中的文本提示任务。我们组合了一系列的白盒和黑盒模型（具有或无Diffusion网络权重的访问）来处理该任务。我们提出了一种新的学习框架，包括文本提示回归和多标签词汇分类目标，可以生成改进的提示。为了进一步改进我们的方法，我们采用了课程学习程序，其中优先采用低噪音（即更好地对齐）的图像提示对象，以及一种不supervised领域适应kernel学习方法，该方法使用源和目标领域样本之间的相似性作为额外特征。我们在DiffusionDB数据集上进行了实验，预测由Stable Diffusion生成的图像中的文本提示。我们的新的学习框架在该任务上具有优秀的成绩，尤其是在白盒模型上。此外，我们发现了一项 interessante发现：通过直接将协托模型学习到提示生成任务上，可以使模型生成与输入提示更好地对齐的图像。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Occupancy-Flow-Fields-for-Perception-and-Prediction-in-Self-Driving"><a href="#Implicit-Occupancy-Flow-Fields-for-Perception-and-Prediction-in-Self-Driving" class="headerlink" title="Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving"></a>Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01471">http://arxiv.org/abs/2308.01471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Agro, Quinlan Sykora, Sergio Casas, Raquel Urtasun</li>
<li>for: 这个论文旨在提出一种能够同时捕捉它周围环境和预测其他交通参与者未来行为的自动驾驶车辆（SDV）。</li>
<li>methods: 这个论文使用了一种混合了卷积神经网络和注意力机制的方法，能够同时预测它周围的占用和流动Grid，并且可以 directly queried by the motion planner at continuous spatio-temporal locations。</li>
<li>results: 经过广泛的实验证明，这个方法可以在城市和高速公路上的不同环境下表现出优于当前状态的论文。<details>
<summary>Abstract</summary>
A self-driving vehicle (SDV) must be able to perceive its surroundings and predict the future behavior of other traffic participants. Existing works either perform object detection followed by trajectory forecasting of the detected objects, or predict dense occupancy and flow grids for the whole scene. The former poses a safety concern as the number of detections needs to be kept low for efficiency reasons, sacrificing object recall. The latter is computationally expensive due to the high-dimensionality of the output grid, and suffers from the limited receptive field inherent to fully convolutional networks. Furthermore, both approaches employ many computational resources predicting areas or objects that might never be queried by the motion planner. This motivates our unified approach to perception and future prediction that implicitly represents occupancy and flow over time with a single neural network. Our method avoids unnecessary computation, as it can be directly queried by the motion planner at continuous spatio-temporal locations. Moreover, we design an architecture that overcomes the limited receptive field of previous explicit occupancy prediction methods by adding an efficient yet effective global attention mechanism. Through extensive experiments in both urban and highway settings, we demonstrate that our implicit model outperforms the current state-of-the-art. For more information, visit the project website: https://waabi.ai/research/implicito.
</details>
<details>
<summary>摘要</summary>
一个自动驾驶车（SDV）需要能够感知周围环境并预测其他交通参与者的未来行为。现有的工作都是在先检测对象，然后预测这些检测到的对象的轨迹，或者预测整个场景的厚度占用和流动Grid。前者会导致安全隐患，因为需要降低检测数量以实现效率，导致对象回溯问题。后者由于输出格式的高维度性而 computationally expensive，同时受到全 convolutional network 的局部感知限制。此外，两种方法都需要大量计算资源预测可能不会被询问的区域或对象。这种情况驱动我们提出了一种独立的感知和未来预测方法，该方法可以直接由运动规划器查询，避免不必要的计算。此外，我们还设计了一种高效但有效的全局注意机制，以解决前一些明确occupancy预测方法的局部感知限制。经过广泛的实验，我们表明我们的含义模型在城市和高速公路上都能够超越当前状态。更多信息请访问我们的项目网站：https://waabi.ai/research/implicito。
</details></li>
</ul>
<hr>
<h2 id="Training-Data-Protection-with-Compositional-Diffusion-Models"><a href="#Training-Data-Protection-with-Compositional-Diffusion-Models" class="headerlink" title="Training Data Protection with Compositional Diffusion Models"></a>Training Data Protection with Compositional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01937">http://arxiv.org/abs/2308.01937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Golatkar, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto</li>
<li>for: 这篇论文是为了推动大规模扩散模型的各种应用场景，如选择性忘记和继续学习等。</li>
<li>methods: 这篇论文提出了一种叫做组 compartmentalized Diffusion Models（CDM）的方法，允许在推导时将不同的扩散模型（或提示）分别训练在不同的数据源上，并在推导时自由组合它们以实现与准确模型（训练在所有数据上）相当的性能。此外，每个模型只包含它在训练时接触到的数据subset的信息，因此可以实现数据训练保护和用户访问权限控制等功能。</li>
<li>results: 该研究表明，CDM可以实现选择性忘记和继续学习等功能，并且可以根据用户的访问权限来服务自定义的模型。此外，CDM还可以确定特定样本的数据subset的重要性。<details>
<summary>Abstract</summary>
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
</details>
<details>
<summary>摘要</summary>
我们介绍 compartmentalized diffusion models (CDM)，一种方法可以在推导时分别训练不同的扩散模型（或启发），并在推导时随意组合。个别模型可以在隔离的时间、不同的分布和领域上进行训练，而且可以在推导时随意组合以实现与单一模型训练在所有数据上的性能相似。此外，每个模型只包含它在训练时所接触过的数据subset的信息，因此可以实现多种训练数据保护。例如，CDMs可以实现选择性的忘记和持续学习，以及根据用户的存取权提供自定义的模型。CDMs还允许决定特定数据subset的重要性在生成特定样本中。
</details></li>
</ul>
<hr>
<h2 id="Dual-Governance-The-intersection-of-centralized-regulation-and-crowdsourced-safety-mechanisms-for-Generative-AI"><a href="#Dual-Governance-The-intersection-of-centralized-regulation-and-crowdsourced-safety-mechanisms-for-Generative-AI" class="headerlink" title="Dual Governance: The intersection of centralized regulation and crowdsourced safety mechanisms for Generative AI"></a>Dual Governance: The intersection of centralized regulation and crowdsourced safety mechanisms for Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04448">http://arxiv.org/abs/2308.04448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avijit Ghosh, Dhanya Lakshmi<br>for:The paper focuses on the ethical and safety concerns surrounding the use of generative AI, particularly in the context of consumer-facing models, and proposes a framework called Dual Governance to address these issues.methods:The paper discusses the limitations of existing centralized regulations and decentralized safety mechanisms, and proposes a cooperative synergy between government regulations and community-developed safety mechanisms as a solution.results:The paper argues that the proposed Dual Governance framework can promote innovation and creativity while ensuring the safe and ethical deployment of generative AI.<details>
<summary>Abstract</summary>
Generative Artificial Intelligence (AI) has seen mainstream adoption lately, especially in the form of consumer-facing, open-ended, text and image generating models. However, the use of such systems raises significant ethical and safety concerns, including privacy violations, misinformation and intellectual property theft. The potential for generative AI to displace human creativity and livelihoods has also been under intense scrutiny. To mitigate these risks, there is an urgent need of policies and regulations responsible and ethical development in the field of generative AI. Existing and proposed centralized regulations by governments to rein in AI face criticisms such as not having sufficient clarity or uniformity, lack of interoperability across lines of jurisdictions, restricting innovation, and hindering free market competition. Decentralized protections via crowdsourced safety tools and mechanisms are a potential alternative. However, they have clear deficiencies in terms of lack of adequacy of oversight and difficulty of enforcement of ethical and safety standards, and are thus not enough by themselves as a regulation mechanism. We propose a marriage of these two strategies via a framework we call Dual Governance. This framework proposes a cooperative synergy between centralized government regulations in a U.S. specific context and safety mechanisms developed by the community to protect stakeholders from the harms of generative AI. By implementing the Dual Governance framework, we posit that innovation and creativity can be promoted while ensuring safe and ethical deployment of generative AI.
</details>
<details>
<summary>摘要</summary>
生成人工智能（AI）在最近几年内得到了普遍的批准，特别是在consumer-facing、开放结束的文本和图像生成模型的形式。然而，使用这些系统的使用带来了重大的伦理和安全问题，包括隐私侵犯、谎言和知识产权盗窃。生成AI可能会取代人类创造力和生活方式，也在严峻的检讨中。为了缓解这些风险，生成AI的负责任和伦理的开发是一个紧迫的需求。现有的中央政府的法规和 proposed regulations 面临批评，如不具有足够的明确性和一致性，跨司法管辖区域的不具有可靠性，限制创新，和妨碍自由市场竞争。 Decentralized protection mechanisms via crowdsourced safety tools and mechanisms are a potential alternative，但它们缺乏伦理和安全标准的充分监管和执行能力，因此不够作为唯一的规则机制。我们提议一种名为“双重管理”的框架，这种框架提议在美国特定的上下文中，中央政府的法规和社区开发的安全机制之间建立了合作的同步。通过实施“双重管理”框架，我们认为可以促进创新和创造力，同时确保生成AI的安全和伦理部署。
</details></li>
</ul>
<hr>
<h2 id="VertexSerum-Poisoning-Graph-Neural-Networks-for-Link-Inference"><a href="#VertexSerum-Poisoning-Graph-Neural-Networks-for-Link-Inference" class="headerlink" title="VertexSerum: Poisoning Graph Neural Networks for Link Inference"></a>VertexSerum: Poisoning Graph Neural Networks for Link Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01469">http://arxiv.org/abs/2308.01469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruyi Ding, Shijin Duan, Xiaolin Xu, Yunsi Fei</li>
<li>for: 本研究旨在攻击图structured数据中的隐私泄露，特别是社交分析和诈骗探测等应用中使用的图生成网络（GNNs）。</li>
<li>methods: 我们提出了一种新的图毒液攻击方法——VertexSerum，它可以更好地利用图连接的敏感性和价值，从而提高图连接泄露的效果。我们还提出了一种注意力机制，可以嵌入到连接检测网络中，以提高连接检测的准确性。</li>
<li>results: 我们的实验结果表明，VertexSerum在四个真实世界数据集和三种不同的GNN结构下，与现有的链接探测攻击方法相比，平均提高了链接泄露的AUC分数 by 9.8%。此外，我们的实验还表明，VertexSerum在黑盒和在线学习Setting下都具有良好的实用性。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have brought superb performance to various applications utilizing graph structural data, such as social analysis and fraud detection. The graph links, e.g., social relationships and transaction history, are sensitive and valuable information, which raises privacy concerns when using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel graph poisoning attack that increases the effectiveness of graph link stealing by amplifying the link connectivity leakage. To infer node adjacency more accurately, we propose an attention mechanism that can be embedded into the link detection network. Our experiments demonstrate that VertexSerum significantly outperforms the SOTA link inference attack, improving the AUC scores by an average of $9.8\%$ across four real-world datasets and three different GNN structures. Furthermore, our experiments reveal the effectiveness of VertexSerum in both black-box and online learning settings, further validating its applicability in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 已经在不同的应用中提供了出色的表现，如社交分析和诈骗检测。图像链接，如社交关系和交易历史记录，是敏感和有价值的信息，这会使用 GNNs 引发隐私问题。为了利用这些漏洞，我们提议 VertexSerum，一种新的图像恶意攻击，可以增强图像链接窃取的效果。为更准确地推断节点相互关系，我们提议一种注意力机制，可以在链接检测网络中嵌入。我们的实验表明，VertexSerum significantly outperforms 当前链接推断攻击的最佳实践（SOTA），提高了平均混淆率（AUC）分数，分别在四个真实世界数据集和三种不同的 GNN 结构上提高了9.8%。此外，我们的实验还表明 VertexSerum 在黑盒和在线学习设置下都是有效的，进一步验证了它在实际场景中的适用性。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Small-Molecule-Properties-in-Drug-Discovery"><a href="#Machine-Learning-Small-Molecule-Properties-in-Drug-Discovery" class="headerlink" title="Machine Learning Small Molecule Properties in Drug Discovery"></a>Machine Learning Small Molecule Properties in Drug Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12354">http://arxiv.org/abs/2308.12354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolai Schapin, Maciej Majewski, Alejandro Varela, Carlos Arroniz, Gianni De Fabritiis</li>
<li>for: 这篇论文主要是为了介绍近年来用于小分子性质预测的机器学习（ML）方法。</li>
<li>methods: 论文详细介绍了各种ML方法，包括绑定Affinity、溶解度、ABMET等多种属性的预测。还讨论了现有的流行数据集和分子特征，如化学指纹和图像神经网络。</li>
<li>results: 论文分析了小分子性质预测中存在的挑战，如同时预测和优化多个属性的问题，并 briefly介绍了可能的多目标优化技术来均衡多个属性。最后，论文评估了模型预测结果的可理解性，尤其是在药物发现过程中的关键决策中。总的来说，这篇论文提供了药物性质预测领域机器学习模型的全面回顾。<details>
<summary>Abstract</summary>
Machine learning (ML) is a promising approach for predicting small molecule properties in drug discovery. Here, we provide a comprehensive overview of various ML methods introduced for this purpose in recent years. We review a wide range of properties, including binding affinities, solubility, and ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity). We discuss existing popular datasets and molecular descriptors and embeddings, such as chemical fingerprints and graph-based neural networks. We highlight also challenges of predicting and optimizing multiple properties during hit-to-lead and lead optimization stages of drug discovery and explore briefly possible multi-objective optimization techniques that can be used to balance diverse properties while optimizing lead candidates. Finally, techniques to provide an understanding of model predictions, especially for critical decision-making in drug discovery are assessed. Overall, this review provides insights into the landscape of ML models for small molecule property predictions in drug discovery. So far, there are multiple diverse approaches, but their performances are often comparable. Neural networks, while more flexible, do not always outperform simpler models. This shows that the availability of high-quality training data remains crucial for training accurate models and there is a need for standardized benchmarks, additional performance metrics, and best practices to enable richer comparisons between the different techniques and models that can shed a better light on the differences between the many techniques.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）是药物搜索中预测小分子性质的有前途的方法。我们在这篇文章中提供了过去几年内对这些目的的多种机器学习方法的全面概述。我们评论了各种性质，包括绑定稳定性、溶解度和ADMET（吸收、分布、代谢、排泄和毒性）。我们讨论了现有的受欢迎数据集和分子特征，如化学指纹和图像基于神经网络。我们也提到了预测和优化多种性质的挑战，特别是在碰撞到领先和领先优化阶段。我们 briefly explored 可能的多目标优化技术，以填充多种性质的平衡。最后，我们评估了模型预测的方法，特别是在关键决策中的评估。总的来说，这篇文章提供了机器学习模型在小分子性质预测中的领域概况。目前有多种不同的方法，但它们的表现通常相似。神经网络，虽然更灵活，并不总是表现更好。这表明高质量的训练数据的可用性仍然是训练准确模型的关键，而且需要标准化的参考数据、额外的性能指标和最佳实践，以便更好地对不同的技术和模型进行比较，从而更好地了解它们之间的差异。
</details></li>
</ul>
<hr>
<h2 id="From-Discrete-Tokens-to-High-Fidelity-Audio-Using-Multi-Band-Diffusion"><a href="#From-Discrete-Tokens-to-High-Fidelity-Audio-Using-Multi-Band-Diffusion" class="headerlink" title="From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion"></a>From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02560">http://arxiv.org/abs/2308.02560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin San Roman, Yossi Adi, Antoine Deleforge, Romain Serizel, Gabriel Synnaeve, Alexandre Défossez</li>
<li>for: 这个论文的目的是提出一种高级别扩散模型，可以从低比特率的精度表示生成任何类型的音频模式（如语音、音乐、环境声）。</li>
<li>methods: 这个论文使用了扩散模型，但不同于之前的扩散模型，它可以生成任何类型的音频模式，并且可以在同等比特率下比其他生成技术具有更高的 perceived quality。</li>
<li>results: 根据论文的结果，这种扩散模型可以生成高质量的音频，并且可以在不同的音频类型和bit rate下进行调整。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.
</details>
<details>
<summary>摘要</summary>
深度生成模型可以生成高准确性音频，根据不同类型的表示（如mel-spectrograms、Mel-frequency Cepstral Coefficients (MFCC)）。最近，这些模型已经用于生成基于高度压缩表示的音频波形。虽然这些方法产生了很好的结果，但是它们容易产生噪音artefacts，当conditioning是错误或不完美时。另一种模型化方法是使用扩散模型。然而，这些模型主要用于speech vocoder（基于mel-spectrograms）或生成低频率的信号。在这项工作中，我们提议一种高准确度多带 diffusion-based框架，可以从低比特率精确表示中生成任何类型的音频模式（如语音、音乐、环境声）。在相同的比特率下，我们的提议方法与现状最佳生成技术相比，在人类可识别质量上表现出 excel。训练和评估代码，以及音频示例，可以在facebookresearch/audiocraft GitHub页面上找到。
</details></li>
</ul>
<hr>
<h2 id="A-digital-twin-framework-for-civil-engineering-structures"><a href="#A-digital-twin-framework-for-civil-engineering-structures" class="headerlink" title="A digital twin framework for civil engineering structures"></a>A digital twin framework for civil engineering structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01445">http://arxiv.org/abs/2308.01445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Torzoni, Marco Tezzele, Stefano Mariani, Andrea Manzoni, Karen E. Willcox</li>
<li>for: This paper proposes a predictive digital twin approach to health monitoring, maintenance, and management planning for civil engineering structures.</li>
<li>methods: The proposed approach uses a probabilistic graphical model, dynamic Bayesian network, and deep learning models to update the digital twin state in real-time and inform optimal maintenance and management actions.</li>
<li>results: The paper demonstrates the effectiveness of the proposed approach through two synthetic case studies, showing the ability of the digital twin to provide real-time structural health diagnostics and inform dynamic decision-making.<details>
<summary>Abstract</summary>
The digital twin concept represents an appealing opportunity to advance condition-based and predictive maintenance paradigms for civil engineering systems, thus allowing reduced lifecycle costs, increased system safety, and increased system availability. This work proposes a predictive digital twin approach to the health monitoring, maintenance, and management planning of civil engineering structures. The asset-twin coupled dynamical system is encoded employing a probabilistic graphical model, which allows all relevant sources of uncertainty to be taken into account. In particular, the time-repeating observations-to-decisions flow is modeled using a dynamic Bayesian network. Real-time structural health diagnostics are provided by assimilating sensed data with deep learning models. The digital twin state is continually updated in a sequential Bayesian inference fashion. This is then exploited to inform the optimal planning of maintenance and management actions within a dynamic decision-making framework. A preliminary offline phase involves the population of training datasets through a reduced-order numerical model and the computation of a health-dependent control policy. The strategy is assessed on two synthetic case studies, involving a cantilever beam and a railway bridge, demonstrating the dynamic decision-making capabilities of health-aware digital twins.
</details>
<details>
<summary>摘要</summary>
数字双生物概念在 цивиLENGINEERING 系统中表现出了吸引人的机会，以提高基于状况的维护和预测维护方法，从而降低生命周期成本、提高系统安全性和系统可用性。该工作提议一种预测性数字双生物方法来监测、维护和规划 цивиLENGINEERING 结构的健康状况。 asset-twin  Coupled 动力系统使用 probabilistic graphical model 编码，以涵盖所有相关的不确定性因素。特别是，时间重复的观测到决策流程使用动态 Bayesian network 模型。在实时 Structural health 诊断中，把感知数据 assimilate  WITH deep learning models。数字双生物状态在 sequential Bayesian inference 方式中不断更新。这后来被用来决策维护和管理行动的优化计划，在动态决策框架中。在 preliminary offline 阶段，通过减少的数值模型 Compute 健康依赖控制策略。该策略在 two  Synthetic case studies 中， involving 悬臂和铁路桥，表现出了健康意识数字双生物的动态决策能力。
</details></li>
</ul>
<hr>
<h2 id="DLSIA-Deep-Learning-for-Scientific-Image-Analysis"><a href="#DLSIA-Deep-Learning-for-Scientific-Image-Analysis" class="headerlink" title="DLSIA: Deep Learning for Scientific Image Analysis"></a>DLSIA: Deep Learning for Scientific Image Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02559">http://arxiv.org/abs/2308.02559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric J Roberts, Tanny Chavez, Alexander Hexemer, Petrus H. Zwart</li>
<li>for: 用于科学图像分析领域的深度学习库，为科学家和研究人员提供可自定义的卷积神经网络架构，用于许多图像分析任务，包括下游数据处理和实验循环计算场景。</li>
<li>methods: 使用易于使用的架构，如自动编码器、可调 U-Net 和精简 mixed-scale dense network (MSDNet)，以及 randomly generated sparse mixed-scale networks (SMSNets)。</li>
<li>results: 提供可访问的 CNN 构建和抽象，让科学家可以适应机器学习方法，加速发现，促进交叉领域合作，并进展科学图像分析研究。<details>
<summary>Abstract</summary>
We introduce DLSIA (Deep Learning for Scientific Image Analysis), a Python-based machine learning library that empowers scientists and researchers across diverse scientific domains with a range of customizable convolutional neural network (CNN) architectures for a wide variety of tasks in image analysis to be used in downstream data processing, or for experiment-in-the-loop computing scenarios. DLSIA features easy-to-use architectures such as autoencoders, tunable U-Nets, and parameter-lean mixed-scale dense networks (MSDNets). Additionally, we introduce sparse mixed-scale networks (SMSNets), generated using random graphs and sparse connections. As experimental data continues to grow in scale and complexity, DLSIA provides accessible CNN construction and abstracts CNN complexities, allowing scientists to tailor their machine learning approaches, accelerate discoveries, foster interdisciplinary collaboration, and advance research in scientific image analysis.
</details>
<details>
<summary>摘要</summary>
我们介绍DLSIA（深度学习 для科学影像分析），这是一个基于Python的机器学习库，它为科学家和研究人员在多种科学领域提供了一系列可自定义的卷积神经网络架构，用于处理各种影像分析任务，可以用于下游资料处理或实验运行 Computing enario。DLSIA 提供了易于使用的架构，例如自动encoder、可调 U-Net 和简洁的混合缩减网络（MSDNets）。此外，我们还引入了随机 graphs 和简Connection 的稀疏混合网络（SMSNets）。随着实验数据的数量和复杂度不断增加，DLSIA 提供了可访问的 CNN 建构和抽象 CNN 复杂度，让科学家能够适应自己的机器学习方法，加速发现，促进交叉领域合作，并进步科学影像分析研究。
</details></li>
</ul>
<hr>
<h2 id="Novel-Physics-Based-Machine-Learning-Models-for-Indoor-Air-Quality-Approximations"><a href="#Novel-Physics-Based-Machine-Learning-Models-for-Indoor-Air-Quality-Approximations" class="headerlink" title="Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations"></a>Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01438">http://arxiv.org/abs/2308.01438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Mohammadshirazi, Aida Nadafian, Amin Karimi Monsefi, Mohammad H. Rafiei, Rajiv Ramnath</li>
<li>for: 这个研究旨在提出六个新的物理学基础的机器学习模型，以精确地估计室内污染物浓度。</li>
<li>methods: 这个研究使用了 físic-based 的机器学习模型，包括State-space概念、Gated Recurrent Units和Decomposition技术。</li>
<li>results: 研究发现这些提案的模型比类似的现代变数组件模型更加简单、computationally更高效和更精确。<details>
<summary>Abstract</summary>
Cost-effective sensors are capable of real-time capturing a variety of air quality-related modalities from different pollutant concentrations to indoor/outdoor humidity and temperature. Machine learning (ML) models are capable of performing air-quality "ahead-of-time" approximations. Undoubtedly, accurate indoor air quality approximation significantly helps provide a healthy indoor environment, optimize associated energy consumption, and offer human comfort. However, it is crucial to design an ML architecture to capture the domain knowledge, so-called problem physics. In this study, we propose six novel physics-based ML models for accurate indoor pollutant concentration approximations. The proposed models include an adroit combination of state-space concepts in physics, Gated Recurrent Units, and Decomposition techniques. The proposed models were illustrated using data collected from five offices in a commercial building in California. The proposed models are shown to be less complex, computationally more efficient, and more accurate than similar state-of-the-art transformer-based models. The superiority of the proposed models is due to their relatively light architecture (computational efficiency) and, more importantly, their ability to capture the underlying highly nonlinear patterns embedded in the often contaminated sensor-collected indoor air quality temporal data.
</details>
<details>
<summary>摘要</summary>
cost-effective sensors can real-time capture various indoor/outdoor air quality-related modalities, from different pollutant concentrations to humidity and temperature. machine learning (ml) models can perform air quality "ahead-of-time" approximations. accurately approximating indoor air quality can help provide a healthy indoor environment, optimize associated energy consumption, and offer human comfort. however, it is crucial to design an ml architecture to capture the domain knowledge, so-called problem physics. in this study, we propose six novel physics-based ml models for accurate indoor pollutant concentration approximations. the proposed models combine state-space concepts in physics, gated recurrent units, and decomposition techniques. the proposed models were illustrated using data collected from five offices in a commercial building in california. the proposed models are less complex, computationally more efficient, and more accurate than similar state-of-the-art transformer-based models. the superiority of the proposed models is due to their relatively light architecture (computational efficiency) and their ability to capture the underlying highly nonlinear patterns embedded in the often contaminated sensor-collected indoor air quality temporal data.
</details></li>
</ul>
<hr>
<h2 id="Price-Aware-Deep-Learning-for-Electricity-Markets"><a href="#Price-Aware-Deep-Learning-for-Electricity-Markets" class="headerlink" title="Price-Aware Deep Learning for Electricity Markets"></a>Price-Aware Deep Learning for Electricity Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01436">http://arxiv.org/abs/2308.01436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir Dvorkin, Ferdinando Fioretto</li>
<li>for: 该论文旨在探讨深度学习在运维规划中的应用，以及深度学习所带来的预测错误对电价价格的影响。</li>
<li>methods: 该论文使用了深度学习模型来预测电力供应和需求，并对预测错误进行分析。</li>
<li>results: 该论文发现了预测错误对电价价格的影响，并提出了一种基于深度学习的电力市场清算优化方法来增强公平性。该方法可以均衡预测错误和价格错误，从而提高系统的公平性和稳定性。<details>
<summary>Abstract</summary>
While deep learning gradually penetrates operational planning, its inherent prediction errors may significantly affect electricity prices. This letter examines how prediction errors propagate into electricity prices, revealing notable pricing errors and their spatial disparity in congested power systems. To improve fairness, we propose to embed electricity market-clearing optimization as a deep learning layer. Differentiating through this layer allows for balancing between prediction and pricing errors, as oppose to minimizing prediction errors alone. This layer implicitly optimizes fairness and controls the spatial distribution of price errors across the system. We showcase the price-aware deep learning in the nexus of wind power forecasting and short-term electricity market clearing.
</details>
<details>
<summary>摘要</summary>
而深度学习逐渐渗透到运维规划中，它的内生预测错误可能对电力价格产生重要影响。这封信件研究了预测错误如何传播到电力价格上，揭示了明显的价格错误和其空间差异在拥挤的电力系统中。为了提高公平性，我们提议将电力市场清算优化作为深度学习层的一部分。通过这层的微调，可以平衡预测错误和价格错误之间的平衡，而不是仅仅是减少预测错误。这层隐式地优化了公平性和价格错误的空间分布在系统中。我们在风力发电预测和短期电力市场清算之间展示了价格意识的深度学习。
</details></li>
</ul>
<hr>
<h2 id="COVID-VR-A-Deep-Learning-COVID-19-Classification-Model-Using-Volume-Rendered-Computer-Tomography"><a href="#COVID-VR-A-Deep-Learning-COVID-19-Classification-Model-Using-Volume-Rendered-Computer-Tomography" class="headerlink" title="COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography"></a>COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01433">http://arxiv.org/abs/2308.01433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noemi Maritza L. Romero, Ricco Vasconcellos, Mariana R. Mendoza, João L. D. Comba</li>
<li>for: 该论文目的是为了开发一种基于计算机断层成像（CT）图像的肺疾病分类方法，以提高肺疾病诊断的准确性和效率。</li>
<li>methods: 该方法使用了深度学习建模，将多个视角下的肺部CT图像转化为三维Volume Rendering图像，以提高肺疾病诊断的准确性和效率。</li>
<li>results: 对于私有数据和公共数据集的比较，该方法能够有效地识别肺疾病，并与切片方法相比，表现竞争力强。<details>
<summary>Abstract</summary>
The COVID-19 pandemic presented numerous challenges to healthcare systems worldwide. Given that lung infections are prevalent among COVID-19 patients, chest Computer Tomography (CT) scans have frequently been utilized as an alternative method for identifying COVID-19 conditions and various other types of pulmonary diseases. Deep learning architectures have emerged to automate the identification of pulmonary disease types by leveraging CT scan slices as inputs for classification models. This paper introduces COVID-VR, a novel approach for classifying pulmonary diseases based on volume rendering images of the lungs captured from multiple angles, thereby providing a comprehensive view of the entire lung in each image. To assess the effectiveness of our proposal, we compared it against competing strategies utilizing both private data obtained from partner hospitals and a publicly available dataset. The results demonstrate that our approach effectively identifies pulmonary lesions and performs competitively when compared to slice-based methods.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行对全球医疗系统带来了无数的挑战。由于 COVID-19 患者中肺部感染很普遍，因此胸部计算机扫描（CT）成为了一种代替方法来诊断 COVID-19 状况和其他多种肺病。深度学习架构在 CT 扫描片为分类模型提供输入，以自动识别肺病类型。本文介绍了 COVID-VR，一种基于肺部体积渲染图像的新方法，以捕捉多个角度捕捉肺部的全面视图。为评估我们的建议的有效性，我们与合作医院提供的私人数据进行比较，以及公开可用的数据集。结果显示，我们的方法可以有效地识别肺病涂抹，并与slice-based方法相比竞争性地表现。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-the-Potential-of-Similarity-Matching-Scalability-Supervision-and-Pre-training"><a href="#Unlocking-the-Potential-of-Similarity-Matching-Scalability-Supervision-and-Pre-training" class="headerlink" title="Unlocking the Potential of Similarity Matching: Scalability, Supervision and Pre-training"></a>Unlocking the Potential of Similarity Matching: Scalability, Supervision and Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02427">http://arxiv.org/abs/2308.02427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanis Bahroun, Shagesh Sridharan, Atithi Acharya, Dmitri B. Chklovskii, Anirvan M. Sengupta</li>
<li>for: 这个研究旨在开发一种基于本地学习规则的替代算法，以增强backpropagation算法的有效性和生物可能性。</li>
<li>methods: 研究使用了一种主要是无监督的相似匹配（SM）框架，该框架与生物系统中观察到的机制相符，并且具有在线、本地化和生物可能性的算法。</li>
<li>results: 研究人员提出了一种使用PyTorch实现Convolutional Nonnegative SM的方法，并引入了一种本地化的supervised SM目标，以便堆叠SM层。此外，研究人员还使用PyTorch实现了预训练 architecture such as LeNet，并对BP-trained模型中的特征进行评估。这项研究结合了生物可能性的算法和计算效率，开辟了多个可能性的探索。<details>
<summary>Abstract</summary>
While effective, the backpropagation (BP) algorithm exhibits limitations in terms of biological plausibility, computational cost, and suitability for online learning. As a result, there has been a growing interest in developing alternative biologically plausible learning approaches that rely on local learning rules. This study focuses on the primarily unsupervised similarity matching (SM) framework, which aligns with observed mechanisms in biological systems and offers online, localized, and biologically plausible algorithms. i) To scale SM to large datasets, we propose an implementation of Convolutional Nonnegative SM using PyTorch. ii) We introduce a localized supervised SM objective reminiscent of canonical correlation analysis, facilitating stacking SM layers. iii) We leverage the PyTorch implementation for pre-training architectures such as LeNet and compare the evaluation of features against BP-trained models. This work combines biologically plausible algorithms with computational efficiency opening multiple avenues for further explorations.
</details>
<details>
<summary>摘要</summary>
While effective, the backpropagation (BP) algorithm has limitations in terms of biological plausibility, computational cost, and suitability for online learning. As a result, there has been a growing interest in developing alternative biologically plausible learning approaches that rely on local learning rules. This study focuses on the primarily unsupervised similarity matching (SM) framework, which aligns with observed mechanisms in biological systems and offers online, localized, and biologically plausible algorithms.i) To scale SM to large datasets, we propose an implementation of Convolutional Nonnegative SM using PyTorch.ii) We introduce a localized supervised SM objective reminiscent of canonical correlation analysis, facilitating stacking SM layers.iii) We leverage the PyTorch implementation for pre-training architectures such as LeNet and compare the evaluation of features against BP-trained models. This work combines biologically plausible algorithms with computational efficiency, opening multiple avenues for further explorations.Here's the translation in Traditional Chinese as well, for reference:While effective, the backpropagation (BP) algorithm has limitations in terms of biological plausibility, computational cost, and suitability for online learning. As a result, there has been a growing interest in developing alternative biologically plausible learning approaches that rely on local learning rules. This study focuses on the primarily unsupervised similarity matching (SM) framework, which aligns with observed mechanisms in biological systems and offers online, localized, and biologically plausible algorithms.i) To scale SM to large datasets, we propose an implementation of Convolutional Nonnegative SM using PyTorch.ii) We introduce a localized supervised SM objective reminiscent of canonical correlation analysis, facilitating stacking SM layers.iii) We leverage the PyTorch implementation for pre-training architectures such as LeNet and compare the evaluation of features against BP-trained models. This work combines biologically plausible algorithms with computational efficiency, opening multiple avenues for further explorations.
</details></li>
</ul>
<hr>
<h2 id="Bio-Clinical-BERT-BERT-Base-and-CNN-Performance-Comparison-for-Predicting-Drug-Review-Satisfaction"><a href="#Bio-Clinical-BERT-BERT-Base-and-CNN-Performance-Comparison-for-Predicting-Drug-Review-Satisfaction" class="headerlink" title="Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction"></a>Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03782">http://arxiv.org/abs/2308.03782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Ling</li>
<li>for: 这个研究旨在开发一些可以分析病人的药物评价，并将其满意度精确地分类为正面、中性或负面的自然语言处理（NLP）模型。</li>
<li>methods: 这个研究使用了多种分类模型，包括BERT基础模型、Bio+Clinical BERT和简单的CNN。</li>
<li>results: 结果显示，医疗领域专门的Bio+Clinical BERT模型在表格2中表现出色，与通用领域基础BERT模型相比，macro f1和 recall 分数提高了11%。<details>
<summary>Abstract</summary>
The objective of this study is to develop natural language processing (NLP) models that can analyze patients' drug reviews and accurately classify their satisfaction levels as positive, neutral, or negative. Such models would reduce the workload of healthcare professionals and provide greater insight into patients' quality of life, which is a critical indicator of treatment effectiveness. To achieve this, we implemented and evaluated several classification models, including a BERT base model, Bio+Clinical BERT, and a simpler CNN. Results indicate that the medical domain-specific Bio+Clinical BERT model significantly outperformed the general domain base BERT model, achieving macro f1 and recall score improvement of 11%, as shown in Table 2. Future research could explore how to capitalize on the specific strengths of each model. Bio+Clinical BERT excels in overall performance, particularly with medical jargon, while the simpler CNN demonstrates the ability to identify crucial words and accurately classify sentiment in texts with conflicting sentiments.
</details>
<details>
<summary>摘要</summary>
本研究的目的是开发自然语言处理（NLP）模型，可以分析患者的药物评价并准确地分类为正面、中性或负面的满意度。这些模型会减轻医疗专业人员的工作负担，并为患者的生活质量提供更多的指导，这是治疗效果的重要指标。为达到这一目标，我们实施并评估了多种分类模型，包括BERT基础模型、Bio+клиничеBERT和简单的CNN。结果表明，专业领域域 especific的Bio+клиничеBERT模型在表格2中显著超越了通用领域基础BERT模型，实现了macro f1和回快分数的提高11%。未来的研究可以探讨如何利用每个模型的特点。Bio+клиничеBERT在总性性能方面表现出色，特别是对医疗专业术语的处理，而简单的CNN则能够准确地标识关键词并在文本中分类情感。
</details></li>
</ul>
<hr>
<h2 id="Sea-level-Projections-with-Machine-Learning-using-Altimetry-and-Climate-Model-ensembles"><a href="#Sea-level-Projections-with-Machine-Learning-using-Altimetry-and-Climate-Model-ensembles" class="headerlink" title="Sea level Projections with Machine Learning using Altimetry and Climate Model ensembles"></a>Sea level Projections with Machine Learning using Altimetry and Climate Model ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02460">http://arxiv.org/abs/2308.02460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saumya Sinha, John Fasullo, R. Steven Nerem, Claire Monteleoni</li>
<li>for: 本研究使用卫星测量数据自1993年起，检测全球海平面的升高趋势（3.4毫米&#x2F;年），并调查人类活动对这种升高的影响。</li>
<li>methods: 本研究使用机器学习（ML）技术，通过融合卫星观测数据和气候模型 simulations，预测未来海平面变化的趋势。</li>
<li>results: 研究发现，通过各种气候变化信号的评估，可以更好地预测未来海平面变化的趋势，并且可以通过各种方法提高预测的准确性。<details>
<summary>Abstract</summary>
Satellite altimeter observations retrieved since 1993 show that the global mean sea level is rising at an unprecedented rate (3.4mm/year). With almost three decades of observations, we can now investigate the contributions of anthropogenic climate-change signals such as greenhouse gases, aerosols, and biomass burning in this rising sea level. We use machine learning (ML) to investigate future patterns of sea level change. To understand the extent of contributions from the climate-change signals, and to help in forecasting sea level change in the future, we turn to climate model simulations. This work presents a machine learning framework that exploits both satellite observations and climate model simulations to generate sea level rise projections at a 2-degree resolution spatial grid, 30 years into the future. We train fully connected neural networks (FCNNs) to predict altimeter values through a non-linear fusion of the climate model hindcasts (for 1993-2019). The learned FCNNs are then applied to future climate model projections to predict future sea level patterns. We propose segmenting our spatial dataset into meaningful clusters and show that clustering helps to improve predictions of our ML model.
</details>
<details>
<summary>摘要</summary>
卫星测量数据自1993年起已经提供了全球海平面上升的无前例快速速率（3.4毫米/年）。在近三十年的观测记录下，我们现在可以进行人类活动气候变化的贡献分析，如绿色气体、喷气和生物燃烧等。我们使用机器学习（ML）技术来探索未来海平面变化的趋势。为了了解气候变化信号的贡献和未来海平面变化的预测，我们转向气候模型仿真。本研究提出了一种基于卫星观测和气候模型仿真的机器学习框架，用于预测未来30年的海平面变化趋势。我们使用全连接神经网络（FCNN）来预测测量值，通过非线性混合气候模型预测（1993-2019）来训练FCNN。然后，我们应用FCNN来预测未来气候模型预测中的海平面变化趋势。我们还提出了对空间数据进行有意义的分割，并证明分割可以提高我们的机器学习模型预测的准确性。
</details></li>
</ul>
<hr>
<h2 id="OpenFlamingo-An-Open-Source-Framework-for-Training-Large-Autoregressive-Vision-Language-Models"><a href="#OpenFlamingo-An-Open-Source-Framework-for-Training-Large-Autoregressive-Vision-Language-Models" class="headerlink" title="OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"></a>OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01390">http://arxiv.org/abs/2308.01390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlfoundations/open_flamingo">https://github.com/mlfoundations/open_flamingo</a></li>
<li>paper_authors: Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt</li>
<li>for: 这篇论文是为了描述OpenFlamingo模型家族，该家族包括从3B到9B参数的束autoregressive视频语言模型，这是一个开源的Flamingo模型复制项目。</li>
<li>methods: 这篇论文使用了OpenFlamingo模型，training数据，超参数以及评估集合来训练这些模型。</li>
<li>results: 在七个视频语言数据集上，OpenFlamingo模型的平均表现为80-89%相对于对应的Flamingo模型表现。<details>
<summary>Abstract</summary>
We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open_flamingo.
</details>
<details>
<summary>摘要</summary>
我们介绍OpenFlamingo，一家以自适应推论为基础的视觉语言模型，它的参数量从3B至9B。OpenFlamingo是一个持续进行的开源复制项目，旨在实现深度联盟的Flamingo模型的开源版本。在七个视觉语言数据集上，OpenFlamingo模型的平均表现为80-89%相应的Flamingo表现。本技术报告描述了我们的模型、训练数据、几何 Parameters 和评估工具。我们在https://github.com/mlfoundations/open_flamingo 上分享我们的模型和代码。
</details></li>
</ul>
<hr>
<h2 id="Follow-the-Soldiers-with-Optimized-Single-Shot-Multibox-Detection-and-Reinforcement-Learning"><a href="#Follow-the-Soldiers-with-Optimized-Single-Shot-Multibox-Detection-and-Reinforcement-Learning" class="headerlink" title="Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning"></a>Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01389">http://arxiv.org/abs/2308.01389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jumman Hossain, Maliha Momtaz</li>
<li>for: 本研究的主要目标是建立一个自动驾驶系统，使其可以跟踪特定人（在我们项目中是士兵）在任何方向移动。</li>
<li>methods: 我们使用优化的单射多框检测（SSD）模型和再增强学习（RL）模型来实现这个目标。</li>
<li>results: 实验结果显示，使用 SSD Lite 模型可以提供更好的性能（比 SSD 和 NCS 更好），而且在执行速度方面也有显著提高（约2-3倍），而无需牺牲准确性。<details>
<summary>Abstract</summary>
Nowadays, autonomous cars are gaining traction due to their numerous potential applications on battlefields and in resolving a variety of other real-world challenges. The main goal of our project is to build an autonomous system using DeepRacer which will follow a specific person (for our project, a soldier) when they will be moving in any direction. Two main components to accomplish this project is an optimized Single-Shot Multibox Detection (SSD) object detection model and a Reinforcement Learning (RL) model. We accomplished the task using SSD Lite instead of SSD and at the end, compared the results among SSD, SSD with Neural Computing Stick (NCS), and SSD Lite. Experimental results show that SSD Lite gives better performance among these three techniques and exhibits a considerable boost in inference speed (~2-3 times) without compromising accuracy.
</details>
<details>
<summary>摘要</summary>
现在，自动驾驶车在各种应用场景中得到了广泛的应用，尤其是在战场和解决各种现实世界问题方面。我们项目的主要目标是使用DeepRacer构建一个自动驾驶系统，该系统可以跟踪一个特定人（在我们项目中是一名士兵）在任何方向移动时。我们完成了这个项目，使用优化的Single-Shot Multibox Detection（SSD）物体检测模型和Reinforcement Learning（RL）模型。我们使用SSD Lite而不是SSD，并在结果中进行了比较。实验结果显示，SSD Lite在这三种技术中表现最佳，并且在执行速度方面表现出了明显的提升（大约2-3倍），而无需牺牲准确性。
</details></li>
</ul>
<hr>
<h2 id="DeepSpeed-Chat-Easy-Fast-and-Affordable-RLHF-Training-of-ChatGPT-like-Models-at-All-Scales"><a href="#DeepSpeed-Chat-Easy-Fast-and-Affordable-RLHF-Training-of-ChatGPT-like-Models-at-All-Scales" class="headerlink" title="DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"></a>DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01320">http://arxiv.org/abs/2308.01320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>
<li>paper_authors: Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He</li>
<li>for: This paper aims to provide an accessible, efficient, and cost-effective end-to-end RLHF training pipeline for ChatGPT-like models, particularly when training at the scale of billions of parameters.</li>
<li>methods: The paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. The system offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way.</li>
<li>results: The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of parameters in record time and at a fraction of the cost. With this development, DeepSpeed-Chat paves the way for broader access to advanced RLHF training, even for data scientists with limited resources, thereby fostering innovation and further development in the field of AI.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提供一个可accessible、高效、成本效果的RLHF训练管道，特别是在多亿个参数训练时。</li>
<li>methods: 论文引入了DeepSpeed-Chat系统，该系统提供了三个关键功能：对ChatGPT-like模型的易用训练和推理体验、DeepSpeed-RLHF管道，以及一个稳定的DeepSpeed-RLHF系统，该系统结合了多种优化来提高训练和推理的效率和扩展性。</li>
<li>results: 系统可以在记录时间内训练出多亿个参数的模型，并且在成本的一小部分。通过这一发展，DeepSpeed-Chat开创了更广泛的RLHF训练访问权，使得数据科学家们可以更容易地访问高级RLHF训练，从而推动AI领域的创新和发展。<details>
<summary>Abstract</summary>
ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance. However, the current landscape lacks an accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for these powerful models, particularly when training at the scale of billions of parameters. This paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. DeepSpeed-Chat offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way. The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of parameters in record time and at a fraction of the cost. With this development, DeepSpeed-Chat paves the way for broader access to advanced RLHF training, even for data scientists with limited resources, thereby fostering innovation and further development in the field of AI.
</details>
<details>
<summary>摘要</summary>
chatgpt-like模型已经革命化了人工智能中的多个应用，从概要和编程到翻译，与人类表现相当或甚至超越人类表现。然而，当前的景象缺乏可 accessible，高效，和cost-effective的RLHF（人工智能学习带反馈）训练管道，特别是在 billion parameter scale 的训练中。本文介绍了 DeepSpeed-Chat，一种新的系统，使得RLHF训练变得可 accessible。DeepSpeed-Chat具有三个关键能力：对 ChatGPT-like模型的易用训练和推理经验，基于 InstructGPT 的 DeepSpeed-RLHF 管道，以及一个可靠的 DeepSpeed-RLHF 系统，它将训练和推理过程合并到一起，提供了无 parallel 的效率和可扩展性。该系统可以在 record 时间内训练 billions of parameters 的模型，并且只需一小部分的成本。通过这一发展，DeepSpeed-Chat 为AI领域的进步和发展开辟了新的可能性，尤其是为那些具有限制的数据科学家，他们可以更容易地访问高级RLHF训练，从而推动AI领域的进步。
</details></li>
</ul>
<hr>
<h2 id="Computational-Long-Exposure-Mobile-Photography"><a href="#Computational-Long-Exposure-Mobile-Photography" class="headerlink" title="Computational Long Exposure Mobile Photography"></a>Computational Long Exposure Mobile Photography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01379">http://arxiv.org/abs/2308.01379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Tabellion, Nikhil Karnad, Noa Glaser, Ben Weiss, David E. Jacobs, Yael Pritch</li>
<li>for: 这篇论文旨在提供一种可以在手持式智能手机摄像头应用程序中实现长时间摄影的计算机 burst 摄影系统，以生成吸引人的满屏照片。</li>
<li>methods: 该系统首先检测和分割主题，然后跟踪场景运动多个帧，并对图像进行对齐，以保持所需的锐度和生成美观的运动梳子。系统还会捕捉不充足的短暂拍摄，并选择输入帧中能够生成控制长度的滑块，不管场景或摄像头运动速度。最后，系统预测间帧运动并synthesize运动滑块，以填充间隔 между输入帧。</li>
<li>results: 该系统可以自动生成高分辨率和高动态范围（HDR）照片，并使这种创作风格更加 accessible 于大多数临时摄影师。更多信息和补充材料可以在项目网页上找到：<a target="_blank" rel="noopener" href="https://motion-mode.github.io/">https://motion-mode.github.io/</a><details>
<summary>Abstract</summary>
Long exposure photography produces stunning imagery, representing moving elements in a scene with motion-blur. It is generally employed in two modalities, producing either a foreground or a background blur effect. Foreground blur images are traditionally captured on a tripod-mounted camera and portray blurred moving foreground elements, such as silky water or light trails, over a perfectly sharp background landscape. Background blur images, also called panning photography, are captured while the camera is tracking a moving subject, to produce an image of a sharp subject over a background blurred by relative motion. Both techniques are notoriously challenging and require additional equipment and advanced skills. In this paper, we describe a computational burst photography system that operates in a hand-held smartphone camera app, and achieves these effects fully automatically, at the tap of the shutter button. Our approach first detects and segments the salient subject. We track the scene motion over multiple frames and align the images in order to preserve desired sharpness and to produce aesthetically pleasing motion streaks. We capture an under-exposed burst and select the subset of input frames that will produce blur trails of controlled length, regardless of scene or camera motion velocity. We predict inter-frame motion and synthesize motion-blur to fill the temporal gaps between the input frames. Finally, we composite the blurred image with the sharp regular exposure to protect the sharpness of faces or areas of the scene that are barely moving, and produce a final high resolution and high dynamic range (HDR) photograph. Our system democratizes a capability previously reserved to professionals, and makes this creative style accessible to most casual photographers.   More information and supplementary material can be found on our project webpage: https://motion-mode.github.io/
</details>
<details>
<summary>摘要</summary>
长时间摄影可以生成吸目的图像，表现在Scene中的运动元素的混淆。它通常在两种方式下使用，生成 either 前景或背景混淆效果。前景混淆图像通常在静止摄像机上捕捉，显示混淆的移动前景元素，如柔软的水或光轨，与锐化的背景景象一起。背景混淆图像，也称为摄影满天飞行，通过跟踪移动主题，以生成一个锐化的主题图像，并且背景混淆由相对运动引起。这两种技术都非常具有挑战性，需要额外设备和高级技能。在这篇论文中，我们描述了一个基于智能手机摄像机应用程序的计算摄影系统，可以自动实现这些效果，只需要点击拍照按钮。我们的方法首先检测和分割出主题。我们跟踪场景运动，并将多帧图像相互对齐，以保持所需的锐化和生成美观的运动损块。我们捕捉充足的短暂拍照，并选择输入帧中生成混淆轨迹的子集，无论场景或摄像机运动速度如何。我们预测间帧运动，并使用Synthesize Motion-blur填充时间间隔。最后，我们将混淆图像与锐化图像合并，保护面部或场景中的细微运动部分，并生成一个高分辨率和高动态范围（HDR）照片。我们的系统将这种创作风格升级到普通用户，使得大多数休闲摄影爱好者可以轻松地获得这种创新风格。更多信息和补充材料可以在我们项目网站上找到：<https://motion-mode.github.io/>
</details></li>
</ul>
<hr>
<h2 id="AI-Enhanced-Data-Processing-and-Discovery-Crowd-Sourcing-for-Meteor-Shower-Mapping"><a href="#AI-Enhanced-Data-Processing-and-Discovery-Crowd-Sourcing-for-Meteor-Shower-Mapping" class="headerlink" title="AI-Enhanced Data Processing and Discovery Crowd Sourcing for Meteor Shower Mapping"></a>AI-Enhanced Data Processing and Discovery Crowd Sourcing for Meteor Shower Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02664">http://arxiv.org/abs/2308.02664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddha Ganju, Amartya Hatua, Peter Jenniskens, Sahyadri Krishna, Chicheng Ren, Surya Ambardar</li>
<li>for: 该研究旨在自动化数据进行处理和获得洞察，以便提高 meteor 显示的发现率。</li>
<li>methods: 该研究使用了云端 AI 智能管道，以自动化数据进行处理和分析。</li>
<li>results: 该研究已经发现了超过 200 个新的 meteor 显示，并已经验证了多个先前报告的显示。<details>
<summary>Abstract</summary>
The Cameras for Allsky Meteor Surveillance (CAMS) project, funded by NASA starting in 2010, aims to map our meteor showers by triangulating meteor trajectories detected in low-light video cameras from multiple locations across 16 countries in both the northern and southern hemispheres. Its mission is to validate, discover, and predict the upcoming returns of meteor showers. Our research aimed to streamline the data processing by implementing an automated cloud-based AI-enabled pipeline and improve the data visualization to improve the rate of discoveries by involving the public in monitoring the meteor detections. This article describes the process of automating the data ingestion, processing, and insight generation using an interpretable Active Learning and AI pipeline. This work also describes the development of an interactive web portal (the NASA Meteor Shower portal) to facilitate the visualization of meteor radiant maps. To date, CAMS has discovered over 200 new meteor showers and has validated dozens of previously reported showers.
</details>
<details>
<summary>摘要</summary>
美国国家航空航天局（NASA）自2010年起投入了“全天 Meteor 探测”（CAMS）项目，旨在通过多个国家和多个地点的低光照视频摄像头检测 meteor 轨迹，并通过三角测量确定 meteor 的轨迹。项目的目标是验证、发现和预测未来的流星雨。我们的研究旨在减少数据处理的复杂度，通过实施云端基于人工智能的自动化数据管道，并改进数据可视化以提高发现率。这篇文章描述了自动化数据进口、处理和洞察的活动学习和人工智能管道的实现方式。此外，我们还开发了一个交互式网站（NASA 流星雨门户），以便为流星辐射地图的可视化提供便捷的方式。至今，CAMS 已经发现了超过 200 个新的流星雨，并验证了数十个先前报道的雨。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Deep-Learning-for-Tumor-Dynamic-Modeling-and-Overall-Survival-Prediction-using-Neural-ODE"><a href="#Explainable-Deep-Learning-for-Tumor-Dynamic-Modeling-and-Overall-Survival-Prediction-using-Neural-ODE" class="headerlink" title="Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE"></a>Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01362">http://arxiv.org/abs/2308.01362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Laurie, James Lu</li>
<li>for: This paper aims to improve the predictivity of tumor dynamic modeling in oncology drug development by proposing a new pharmacology-informed neural network called TDNODE.</li>
<li>methods: The TDNODE model uses an encoder-decoder architecture to express an underlying dynamical law that is generalized homogeneous with respect to time, enabling the generation of kinetic rate metrics that can be used to predict patients’ overall survival with high accuracy.</li>
<li>results: The proposed modeling formalism provides a principled way to integrate multimodal dynamical datasets in oncology disease modeling, and the generated metrics can be used to predict patients’ overall survival with high accuracy.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文目的是提高肿瘤动态模型在肿瘤药物开发中的预测性，提议一种新的药理学知识感知神经网络called TDNODE。</li>
<li>methods: TDNODE模型使用编码器-解码器架构表达一个基于时间的总体法则，使得生成的生物动力学指标可以用于预测患者的全身生存率高精度。</li>
<li>results: 提议的模型形式可以理性地将多模态动态数据集成在肿瘤疾病模型中，并生成的指标可以用于预测患者的全身生存率高精度。<details>
<summary>Abstract</summary>
While tumor dynamic modeling has been widely applied to support the development of oncology drugs, there remains a need to increase predictivity, enable personalized therapy, and improve decision-making. We propose the use of Tumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network to enable model discovery from longitudinal tumor size data. We show that TDNODE overcomes a key limitation of existing models in its ability to make unbiased predictions from truncated data. The encoder-decoder architecture is designed to express an underlying dynamical law which possesses the fundamental property of generalized homogeneity with respect to time. Thus, the modeling formalism enables the encoder output to be interpreted as kinetic rate metrics, with inverse time as the physical unit. We show that the generated metrics can be used to predict patients' overall survival (OS) with high accuracy. The proposed modeling formalism provides a principled way to integrate multimodal dynamical datasets in oncology disease modeling.
</details>
<details>
<summary>摘要</summary>
traditional Chinese:虾蟹肿瘤动态模型已广泛应用于肿瘤药物开发支持，但还有必要提高预测精度、实现个性化治疗和改善决策。我们提议使用肿瘤动态神经网络（TDNODE）作为药理学知识推导的神经网络，以从长期肿瘤大小数据中发现模型。我们表明，TDNODE可以超越现有模型的一个关键局限性，即从截断数据中做出不受偏见的预测。encoder-decoder架构是设计来表达下述动态法律：在时间上 generalized homogeneity 的基本性质。因此，模型 formalism 允许 encoder 输出被解释为动力学率度量，倒时间为物理单位。我们显示，生成的度量可以高精度地预测患者的总存活率（OS）。我们提出的模型 formalism 为肿瘤疾病模型集成多Modal dynamical dataset提供了原则性的方法。Here's a word-for-word translation of the text into Simplified Chinese:虾蟹肿瘤动态模型已广泛应用于肿瘤药物开发支持，但还有必要提高预测精度、实现个性化治疗和改善决策。我们提议使用肿瘤动态神经网络（TDNODE）作为药理学知识推导的神经网络，以从长期肿瘤大小数据中发现模型。我们表明，TDNODE可以超越现有模型的一个关键局限性，即从截断数据中做出不受偏见的预测。encoder-decoder架构是设计来表达下述动态法律：在时间上 generalized homogeneity 的基本性质。因此，模型 formalism 允许 encoder 输出被解释为动力学率度量，倒时间为物理单位。我们显示，生成的度量可以高精度地预测患者的总存活率（OS）。我们提出的模型 formalism 为肿瘤疾病模型集成多Modal dynamical dataset提供了原则性的方法。
</details></li>
</ul>
<hr>
<h2 id="Compressed-and-distributed-least-squares-regression-convergence-rates-with-applications-to-Federated-Learning"><a href="#Compressed-and-distributed-least-squares-regression-convergence-rates-with-applications-to-Federated-Learning" class="headerlink" title="Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning"></a>Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01358">http://arxiv.org/abs/2308.01358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Constantin Philippenko, Aymeric Dieuleveut</li>
<li>for: 这个论文研究了对机器学习的分布式和联合学习中使用整数压缩算法时的影响。</li>
<li>methods: 这篇论文使用了多种不同的压缩算法，并进行了对这些算法的分析。</li>
<li>results: 论文发现，即使使用不规则的随机场， covariance $\mathfrak{C}<em>{\mathrm{ania}}$ 的加法噪声仍然Scales with $\mathrm{Tr}(\mathfrak{C}</em>{\mathrm{ania}} H^{-1})&#x2F;K$，这generalizes the rate for the vanilla LSR case。此外，论文还分析了压缩策略的影响和联合学习框架中的应用。<details>
<summary>Abstract</summary>
In this paper, we investigate the impact of compression on stochastic gradient algorithms for machine learning, a technique widely used in distributed and federated learning. We underline differences in terms of convergence rates between several unbiased compression operators, that all satisfy the same condition on their variance, thus going beyond the classical worst-case analysis. To do so, we focus on the case of least-squares regression (LSR) and analyze a general stochastic approximation algorithm for minimizing quadratic functions relying on a random field. We consider weak assumptions on the random field, tailored to the analysis (specifically, expected H\"older regularity), and on the noise covariance, enabling the analysis of various randomizing mechanisms, including compression. We then extend our results to the case of federated learning.   More formally, we highlight the impact on the convergence of the covariance $\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced by the algorithm. We demonstrate despite the non-regularity of the stochastic field, that the limit variance term scales with $\mathrm{Tr}(\mathfrak{C}_{\mathrm{ania}} H^{-1})/K$ (where $H$ is the Hessian of the optimization problem and $K$ the number of iterations) generalizing the rate for the vanilla LSR case where it is $\sigma^2 \mathrm{Tr}(H H^{-1}) / K = \sigma^2 d / K$ (Bach and Moulines, 2013). Then, we analyze the dependency of $\mathfrak{C}_{\mathrm{ania}}$ on the compression strategy and ultimately its impact on convergence, first in the centralized case, then in two heterogeneous FL frameworks.
</details>
<details>
<summary>摘要</summary>
本文 investigate 分布式学习和联合学习中的压缩对Stochastic Gradient Algorithm的影响。我们比较不同压缩算法的收敛率，其中所有算法都满足同样的假设条件，超出了经典最坏情况分析。为此，我们在Least Squares Regression (LSR) 中分析一种基于随机场的普通采样算法，并对随机场假设和噪声矩阵做出弱assumption。然后，我们扩展我们的结果到联合学习中。更 formally，我们关注 covariance $\mathfrak{C}_{\text{ania}}$ 的添加噪声对算法的收敛带来的影响。我们发现，即使随机场不规则，则限制变量 $\frac{\text{Tr}(\mathfrak{C}_{\text{ania}} H^{-1})}{K}$  scales，其中 $H$ 是优化问题的梯度矩阵，$K$ 是迭代次数。这与vanilla LSR 情况相同，其中 $\sigma^2 \text{Tr}(H H^{-1}) / K = \sigma^2 d / K$ (Bach 和 Moulines, 2013)。然后，我们分析压缩策略对 covariance $\mathfrak{C}_{\text{ania}}$ 的影响，并最终探讨其对收敛的影响，首先在中央化情况下，然后在两种不同的多元联合学习框架中。
</details></li>
</ul>
<hr>
<h2 id="More-Context-Less-Distraction-Visual-Classification-by-Inferring-and-Conditioning-on-Contextual-Attributes"><a href="#More-Context-Less-Distraction-Visual-Classification-by-Inferring-and-Conditioning-on-Contextual-Attributes" class="headerlink" title="More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes"></a>More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01313">http://arxiv.org/abs/2308.01313</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/umd-huang-lab/perceptionclip">https://github.com/umd-huang-lab/perceptionclip</a></li>
<li>paper_authors: Bang An, Sicheng Zhu, Michael-Andrei Panaitescu-Liess, Chaithanya Kumar Mummadi, Furong Huang</li>
<li>for: 提高零shot图像分类的性能和可解释性</li>
<li>methods: 基于人类视觉过程的启发，提供图像上的 контекст特征，然后通过这些特征进行对象分类</li>
<li>results: 对比传统零shot分类方法，PerceptionCLIP可以更好地 generalized、group robustness和可解释性，例如在Waterbirds数据集上，PerceptionCLIP与ViT-L&#x2F;14组合可以提高最差组分精度 by 16.5%，并在CelebA数据集上提高组合精度 by 3.5%<details>
<summary>Abstract</summary>
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot classification method named PerceptionCLIP. Given an image, it first infers contextual attributes (e.g., background) and then performs object classification conditioning on them. Our experiments show that PerceptionCLIP achieves better generalization, group robustness, and better interpretability. For example, PerceptionCLIP with ViT-L/14 improves the worst group accuracy by 16.5% on the Waterbirds dataset and by 3.5% on CelebA.
</details>
<details>
<summary>摘要</summary>
CLIP，作为基础视语言模型，在零码图像分类中广泛应用，因其能够理解多种视觉概念和自然语言描述。然而，如何充分利用CLIP的人类样式理解能力以实现更好的零码分类仍是一个开放问题。这篇论文着眼于人类视觉过程：现代神经科学视野表明，在分类一个物体时，人们首先推理出该物体的类型独立特征（如背景和方向），然后根据这些信息进行决策。 inspirited by this，我们发现，为CLIP提供 contextual attributes 可以提高零码分类并减少偶极特征的依赖。我们还发现，CLIP本身可以有效地从图像中推理出这些特征。基于这些观察，我们提出了一种无需训练的、两步零码分类方法，名为PerceptionCLIP。给定一个图像，它首先推理出图像中的上下文特征（如背景），然后根据这些特征进行物体分类。我们的实验表明，PerceptionCLIP 可以 achieve better generalization, group robustness, and better interpretability。例如，PerceptionCLIP 与 ViT-L/14 在 Waterbirds 数据集上提高了最差群 accuracy 16.5%，并在 CelebA 数据集上提高了3.5%。
</details></li>
</ul>
<hr>
<h2 id="Lode-Encoder-AI-constrained-co-creativity"><a href="#Lode-Encoder-AI-constrained-co-creativity" class="headerlink" title="Lode Encoder: AI-constrained co-creativity"></a>Lode Encoder: AI-constrained co-creativity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01312">http://arxiv.org/abs/2308.01312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debosmita Bhaumik, Ahmed Khalifa, Julian Togelius</li>
<li>for: 这篇论文是关于开发一种基于混合 iniciative 的平台游戏困难设计系统，用于 классиic 平台游戏困难游戏《寻找宝藏》。</li>
<li>methods: 该系统是基于多个自动编码器，每个自动编码器都是根据不同的困难设计训练而成。当用户提供设计时，每个自动编码器都会生成一个更加类似于它所训练的困难设计的版本。</li>
<li>results: 据报道，该系统可以帮助设计师探索新的设计可能性，而不是仅仅是通过传统的编辑工具。用户测试表明，该系统可以帮助设计师快速创建高质量的困难设计。<details>
<summary>Abstract</summary>
We present Lode Encoder, a gamified mixed-initiative level creation system for the classic platform-puzzle game Lode Runner. The system is built around several autoencoders which are trained on sets of Lode Runner levels. When fed with the user's design, each autoencoder produces a version of that design which is closer in style to the levels that it was trained on. The Lode Encoder interface allows the user to build and edit levels through 'painting' from the suggestions provided by the autoencoders. Crucially, in order to encourage designers to explore new possibilities, the system does not include more traditional editing tools. We report on the system design and training procedure, as well as on the evolution of the system itself and user tests.
</details>
<details>
<summary>摘要</summary>
我们介绍Lode Encoder，一个基于混合 iniciativa 的游戏创作系统，专门为经典平台游戏Lode Runner创建各种各样的关卡。该系统建立在多个自动编码器之上，这些自动编码器在不同的Lode Runner关卡集上进行训练。当用户输入设计时，每个自动编码器都会生成一个更像原始关卡的版本。Lode Encoder 界面允许用户通过"油画"的方式从自动编码器提供的建议中创建和编辑关卡。重要的是，以便鼓励设计师探索新的可能性，该系统不包括传统的编辑工具。我们介绍了系统的设计和训练过程，以及用户测试。
</details></li>
</ul>
<hr>
<h2 id="Masked-and-Swapped-Sequence-Modeling-for-Next-Novel-Basket-Recommendation-in-Grocery-Shopping"><a href="#Masked-and-Swapped-Sequence-Modeling-for-Next-Novel-Basket-Recommendation-in-Grocery-Shopping" class="headerlink" title="Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in Grocery Shopping"></a>Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in Grocery Shopping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01308">http://arxiv.org/abs/2308.01308</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liming-7/mask-swap-nnbr">https://github.com/liming-7/mask-swap-nnbr</a></li>
<li>paper_authors: Ming Li, Mozhdeh Ariannezhad, Andrew Yates, Maarten de Rijke</li>
<li>for: 本研究的目的是提出一种新的下一个购物篮子推荐任务（NNBR），即推荐用户未前吃过的食品。</li>
<li>methods: 本研究提出了一种简单的双向变换器购物篮子推荐模型（BTBR），该模型直接模型食品之间的相关性，并使用了不同的屏蔽策略和训练目标来进行训练。</li>
<li>results: 实验结果表明，BTBR可以有效地解决NNBR任务，并且可以采用不同的屏蔽策略和训练目标来进一步提高性能。<details>
<summary>Abstract</summary>
Next basket recommendation (NBR) is the task of predicting the next set of items based on a sequence of already purchased baskets. It is a recommendation task that has been widely studied, especially in the context of grocery shopping. In next basket recommendation (NBR), it is useful to distinguish between repeat items, i.e., items that a user has consumed before, and explore items, i.e., items that a user has not consumed before. Most NBR work either ignores this distinction or focuses on repeat items. We formulate the next novel basket recommendation (NNBR) task, i.e., the task of recommending a basket that only consists of novel items, which is valuable for both real-world application and NBR evaluation. We evaluate how existing NBR methods perform on the NNBR task and find that, so far, limited progress has been made w.r.t. the NNBR task. To address the NNBR task, we propose a simple bi-directional transformer basket recommendation model (BTBR), which is focused on directly modeling item-to-item correlations within and across baskets instead of learning complex basket representations. To properly train BTBR, we propose and investigate several masking strategies and training objectives: (i) item-level random masking, (ii) item-level select masking, (iii) basket-level all masking, (iv) basket-level explore masking, and (v) joint masking. In addition, an item-basket swapping strategy is proposed to enrich the item interactions within the same baskets. We conduct extensive experiments on three open datasets with various characteristics. The results demonstrate the effectiveness of BTBR and our masking and swapping strategies for the NNBR task. BTBR with a properly selected masking and swapping strategy can substantially improve NNBR performance.
</details>
<details>
<summary>摘要</summary>
下一个篮筐推荐（NBR）任务是预测下一个篮筐中的项目，基于已经购买过的篮筐序列。这是一项推荐任务，尤其在超市购物中广泛研究。在NBR任务中，分 distinguish between repeat items，即用户已经消费过的项目，和 explore items，即用户没有消费过的项目。大多数NBR工作忽略这种分类或者专注于 repeat items。我们提出了下一个新篮筐推荐（NNBR）任务，即推荐一个仅由新项目组成的篮筐，这对于实际应用和NBR评估都具有价值。我们评估了现有NBR方法在NNBR任务中的性能，并发现，迄今为止，对NNBR任务的进展还很有限。为解决NNBR任务，我们提议了一个简单的双向转换器篮筐推荐模型（BTBR），该模型专门模型item-to-item相关性内 и外 basket中。为正确训练BTBR，我们提议并研究了多种masquerade策略和训练目标：（i）item-level随机masquerade，（ii）item-level选择masquerade，（iii）basket-level所有masquerade，（iv）basket-level探索masquerade，（v）联合masquerade。此外，我们还提议了一种item-篮筐交换策略，以便在同一个篮筐中增强item之间的交互。我们对三个开源数据集进行了广泛的实验，结果表明，BTBR和我们的masquerade策略和item-篮筐交换策略可以很好地提高NNBR性能。
</details></li>
</ul>
<hr>
<h2 id="Excitatory-Inhibitory-Balance-Emerges-as-a-Key-Factor-for-RBN-Performance-Overriding-Attractor-Dynamics"><a href="#Excitatory-Inhibitory-Balance-Emerges-as-a-Key-Factor-for-RBN-Performance-Overriding-Attractor-Dynamics" class="headerlink" title="Excitatory&#x2F;Inhibitory Balance Emerges as a Key Factor for RBN Performance, Overriding Attractor Dynamics"></a>Excitatory&#x2F;Inhibitory Balance Emerges as a Key Factor for RBN Performance, Overriding Attractor Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10831">http://arxiv.org/abs/2308.10831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanuel Calvet, Jean Rouat, Bertrand Reulet</li>
<li>for: 这个论文主要是为了研究储存和预测两个复杂任务中的表现。</li>
<li>methods: 作者使用了随机布尔网络（RBNs）来研究连接性、动力学和性能之间的关系。</li>
<li>results: 研究发现，在特定的分布参数下，随机布尔网络可以导致多样化的动力学行为，并且大多数储存器具有一个主要吸引器。此外，作者发现，储存和预测任务中的表现与储存器的内生动力学行为有很少关系。<details>
<summary>Abstract</summary>
Reservoir computing provides a time and cost-efficient alternative to traditional learning methods.Critical regimes, known as the "edge of chaos," have been found to optimize computational performance in binary neural networks. However, little attention has been devoted to studying reservoir-to-reservoir variability when investigating the link between connectivity, dynamics, and performance. As physical reservoir computers become more prevalent, developing a systematic approach to network design is crucial. In this article, we examine Random Boolean Networks (RBNs) and demonstrate that specific distribution parameters can lead to diverse dynamics near critical points. We identify distinct dynamical attractors and quantify their statistics, revealing that most reservoirs possess a dominant attractor. We then evaluate performance in two challenging tasks, memorization and prediction, and find that a positive excitatory balance produces a critical point with higher memory performance. In comparison, a negative inhibitory balance delivers another critical point with better prediction performance. Interestingly, we show that the intrinsic attractor dynamics have little influence on performance in either case.
</details>
<details>
<summary>摘要</summary>
储备计算提供了传统学习方法的时间和成本效益的代替方案。critical regime, known as the "edge of chaos", have been found to optimize computational performance in binary neural networks. However, little attention has been devoted to studying reservoir-to-reservoir variability when investigating the link between connectivity, dynamics, and performance. As physical reservoir computers become more prevalent, developing a systematic approach to network design is crucial. In this article, we examine Random Boolean Networks (RBNs) and demonstrate that specific distribution parameters can lead to diverse dynamics near critical points. We identify distinct dynamical attractors and quantify their statistics, revealing that most reservoirs possess a dominant attractor. We then evaluate performance in two challenging tasks, memorization and prediction, and find that a positive excitatory balance produces a critical point with higher memory performance. In comparison, a negative inhibitory balance delivers another critical point with better prediction performance. Interestingly, we show that the intrinsic attractor dynamics have little influence on performance in either case.Here's the translation in Traditional Chinese:储备计算提供了传统学习方法的时间和成本效益的代替方案。critical regime, known as the "edge of chaos", have been found to optimize computational performance in binary neural networks. However, little attention has been devoted to studying reservoir-to-reservoir variability when investigating the link between connectivity, dynamics, and performance. As physical reservoir computers become more prevalent, developing a systematic approach to network design is crucial. In this article, we examine Random Boolean Networks (RBNs) and demonstrate that specific distribution parameters can lead to diverse dynamics near critical points. We identify distinct dynamical attractors and quantify their statistics, revealing that most reservoirs possess a dominant attractor. We then evaluate performance in two challenging tasks, memorization and prediction, and find that a positive excitatory balance produces a critical point with higher memory performance. In comparison, a negative inhibitory balance delivers another critical point with better prediction performance. Interestingly, we show that the intrinsic attractor dynamics have little influence on performance in either case.
</details></li>
</ul>
<hr>
<h2 id="EmbeddingTree-Hierarchical-Exploration-of-Entity-Features-in-Embedding"><a href="#EmbeddingTree-Hierarchical-Exploration-of-Entity-Features-in-Embedding" class="headerlink" title="EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding"></a>EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01329">http://arxiv.org/abs/2308.01329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Zheng, Junpeng Wang, Chin-Chia Michael Yeh, Yujie Fan, Huiyuan Chen, Liang Wang, Wei Zhang</li>
<li>for: 这篇论文是为了探讨嵌入学习算法中feature的编码方式而写的。</li>
<li>methods: 该论文提出了一种嵌入探索算法，名为EmbeddingTree，可以结构化解释嵌入空间中feature的编码方式。</li>
<li>results: 通过使用EmbeddingTree和相关视觉化工具，可以帮助用户更好地了解高维嵌入的特征，进行嵌入训练中的特征杂谱除法和新数据集的嵌入生成。<details>
<summary>Abstract</summary>
Embedding learning transforms discrete data entities into continuous numerical representations, encoding features/properties of the entities. Despite the outstanding performance reported from different embedding learning algorithms, few efforts were devoted to structurally interpreting how features are encoded in the learned embedding space. This work proposes EmbeddingTree, a hierarchical embedding exploration algorithm that relates the semantics of entity features with the less-interpretable embedding vectors. An interactive visualization tool is also developed based on EmbeddingTree to explore high-dimensional embeddings. The tool helps users discover nuance features of data entities, perform feature denoising/injecting in embedding training, and generate embeddings for unseen entities. We demonstrate the efficacy of EmbeddingTree and our visualization tool through embeddings generated for industry-scale merchant data and the public 30Music listening/playlists dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNEmbedding learning 将粒度数据实体转换为连续数字表示，卷积特征/属性实体。尽管不同的嵌入学习算法报告了出色的性能，但有很少努力投入到嵌入空间中特征的结构性解释。这项工作提出了嵌入树（EmbeddingTree），一种嵌入探索算法，将实体特征 semantics 与卷积向量相关联。我们还开发了基于嵌入树的互动视觉化工具，帮助用户探索高维卷积中的细节特征，进行嵌入训练中的特征杂谔/插入、生成未看到的实体嵌入。我们通过使用industry-scale merchant数据和公共30Music listening/playlists数据集来证明嵌入树和我们的视觉化工具的有效性。
</details></li>
</ul>
<hr>
<h2 id="Investigation-on-Machine-Learning-Based-Approaches-for-Estimating-the-Critical-Temperature-of-Superconductors"><a href="#Investigation-on-Machine-Learning-Based-Approaches-for-Estimating-the-Critical-Temperature-of-Superconductors" class="headerlink" title="Investigation on Machine Learning Based Approaches for Estimating the Critical Temperature of Superconductors"></a>Investigation on Machine Learning Based Approaches for Estimating the Critical Temperature of Superconductors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01932">http://arxiv.org/abs/2308.01932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatin Abrar Shams, Rashed Hasan Ratul, Ahnaf Islam Naf, Syed Shaek Hossain Samir, Mirza Muntasir Nishat, Fahim Faisal, Md. Ashraful Hoque</li>
<li>for: 预测超导材料的 kritical 温度</li>
<li>methods: 使用堆叠机器学习方法对超导材料的复杂特征进行训练，以准确预测 kritical 温度</li>
<li>results: 与其他前一次可 accessible 研究相比，该模型显示了良好的性能，RMSE 为 9.68，R2 值为 0.922，这些发现可能为堆叠ensemble方法与超参数优化（HPO）提供新的视角。<details>
<summary>Abstract</summary>
Superconductors have been among the most fascinating substances, as the fundamental concept of superconductivity as well as the correlation of critical temperature and superconductive materials have been the focus of extensive investigation since their discovery. However, superconductors at normal temperatures have yet to be identified. Additionally, there are still many unknown factors and gaps of understanding regarding this unique phenomenon, particularly the connection between superconductivity and the fundamental criteria to estimate the critical temperature. To bridge the gap, numerous machine learning techniques have been established to estimate critical temperatures as it is extremely challenging to determine. Furthermore, the need for a sophisticated and feasible method for determining the temperature range that goes beyond the scope of the standard empirical formula appears to be strongly emphasized by various machine-learning approaches. This paper uses a stacking machine learning approach to train itself on the complex characteristics of superconductive materials in order to accurately predict critical temperatures. In comparison to other previous accessible research investigations, this model demonstrated a promising performance with an RMSE of 9.68 and an R2 score of 0.922. The findings presented here could be a viable technique to shed new insight on the efficient implementation of the stacking ensemble method with hyperparameter optimization (HPO).
</details>
<details>
<summary>摘要</summary>
超导材料已经是最吸引人的物质之一，因为超导性的基本概念以及相关的极点温度和超导材料的关系已经在发现之后得到了广泛的研究。然而，在常温下发现超导材料仍然没有被发现。此外，关于这一独特现象的多种未知因素和理解的缺陷仍然存在，特别是超导性和基本的评估极点温度的连接。为了填补这些缺陷，许多机器学习技术已经被建立来估算极点温度。此外，需要一种可靠和实用的方法来确定极点温度范围，这超出了标准的Empirical formula的范围。本文使用堆叠机器学习方法来训练自己，以准确预测超导材料的极点温度。与之前可 accessible 的研究相比，这个模型表现了非常出色的性能，RMSE 为 9.68 和 R2 分数为 0.922。本文所提出的结论可能是一种可靠的技术来释明堆叠ensemble method 的可行性和hyperparameter optimization（HPO）的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="BRNES-Enabling-Security-and-Privacy-aware-Experience-Sharing-in-Multiagent-Robotic-and-Autonomous-Systems"><a href="#BRNES-Enabling-Security-and-Privacy-aware-Experience-Sharing-in-Multiagent-Robotic-and-Autonomous-Systems" class="headerlink" title="BRNES: Enabling Security and Privacy-aware Experience Sharing in Multiagent Robotic and Autonomous Systems"></a>BRNES: Enabling Security and Privacy-aware Experience Sharing in Multiagent Robotic and Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01274">http://arxiv.org/abs/2308.01274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aralab-unr/brnes">https://github.com/aralab-unr/brnes</a></li>
<li>paper_authors: Md Tamjid Hossain, Hung Manh La, Shahriar Badsha, Anton Netchaev</li>
<li>for: 加速多智能体学习（MARL）的辅导者-被辅导者框架</li>
<li>methods: 使用启发式邻居区选择和权重经验聚合技术来降低滥览攻击的影响，并保护智能体的私有信息免受敌意推理攻击</li>
<li>results: 在拥有攻击者的情况下，提出了一个新的MARL框架（BRNES），可以快速地达到目标，并且在拥有隐私保护的情况下，对智能体的私有信息进行保护。 experiments show that our framework outperforms the state-of-the-art in terms of steps to goal, obtained reward, and time to goal metrics, and is 8.32x faster than non-private frameworks and 1.41x faster than private frameworks in an adversarial setting.<details>
<summary>Abstract</summary>
Although experience sharing (ES) accelerates multiagent reinforcement learning (MARL) in an advisor-advisee framework, attempts to apply ES to decentralized multiagent systems have so far relied on trusted environments and overlooked the possibility of adversarial manipulation and inference. Nevertheless, in a real-world setting, some Byzantine attackers, disguised as advisors, may provide false advice to the advisee and catastrophically degrade the overall learning performance. Also, an inference attacker, disguised as an advisee, may conduct several queries to infer the advisors' private information and make the entire ES process questionable in terms of privacy leakage. To address and tackle these issues, we propose a novel MARL framework (BRNES) that heuristically selects a dynamic neighbor zone for each advisee at each learning step and adopts a weighted experience aggregation technique to reduce Byzantine attack impact. Furthermore, to keep the agent's private information safe from adversarial inference attacks, we leverage the local differential privacy (LDP)-induced noise during the ES process. Our experiments show that our framework outperforms the state-of-the-art in terms of the steps to goal, obtained reward, and time to goal metrics. Particularly, our evaluation shows that the proposed framework is 8.32x faster than the current non-private frameworks and 1.41x faster than the private frameworks in an adversarial setting.
</details>
<details>
<summary>摘要</summary>
although experience sharing (ES) can accelerate multiagent reinforcement learning (MARL) in an advisor-advisee framework, attempts to apply ES to decentralized multiagent systems have relied on trusted environments and ignored the possibility of adversarial manipulation and inference. however, in a real-world setting, some Byzantine attackers may provide false advice to the advisee and catastrophically degrade the overall learning performance. additionally, an inference attacker may conduct several queries to infer the advisors' private information, making the entire ES process questionable in terms of privacy leakage. to address these issues, we propose a novel MARL framework (BRNES) that heuristically selects a dynamic neighbor zone for each advisee at each learning step and adopts a weighted experience aggregation technique to reduce Byzantine attack impact. furthermore, to keep the agent's private information safe from adversarial inference attacks, we leverage local differential privacy (LDP)-induced noise during the ES process. our experiments show that our framework outperforms the state-of-the-art in terms of the steps to goal, obtained reward, and time to goal metrics. particularly, our evaluation shows that the proposed framework is 8.32x faster than the current non-private frameworks and 1.41x faster than the private frameworks in an adversarial setting.
</details></li>
</ul>
<hr>
<h2 id="A-Probabilistic-Approach-to-Self-Supervised-Learning-using-Cyclical-Stochastic-Gradient-MCMC"><a href="#A-Probabilistic-Approach-to-Self-Supervised-Learning-using-Cyclical-Stochastic-Gradient-MCMC" class="headerlink" title="A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC"></a>A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01271">http://arxiv.org/abs/2308.01271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoumeh Javanbakhat, Christoph Lippert</li>
<li>for: 这个论文是为了提出一种实用的 bayesian自适应学习方法，使用循环随机梯度哈密顿 Monte Carlo（cSGHMC）。</li>
<li>methods: 这种方法使用 prior 来定义自适应学习模型的参数，并使用 cSGHMC 来近似高维和多模态的 posterior 分布。</li>
<li>results: 通过寻找高维和多模态的 posterior 分布， bayesian 自适应学习可以生成可读性和多样性的表示，并在多个下游分类任务上得到显著的性能提升、评估和对外部数据集的检测。<details>
<summary>Abstract</summary>
In this paper we present a practical Bayesian self-supervised learning method with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this framework, we place a prior over the parameters of a self-supervised learning model and use cSGHMC to approximate the high dimensional and multimodal posterior distribution over the embeddings. By exploring an expressive posterior over the embeddings, Bayesian self-supervised learning produces interpretable and diverse representations. Marginalizing over these representations yields a significant gain in performance, calibration and out-of-distribution detection on a variety of downstream classification tasks. We provide experimental results on multiple classification tasks on four challenging datasets. Moreover, we demonstrate the effectiveness of the proposed method in out-of-distribution detection using the SVHN and CIFAR-10 datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种实用的 bayesian自适应学习方法，即循环随机梯度汉堡 Monte Carlo（cSGHMC）。在这个框架中，我们将自适应学习模型的参数置于先验分布中，并使用cSGHMC来近似高维多模态的后验分布。通过探索表征空间的表示，bayesian自适应学习可以生成可读取和多样的表示。对这些表示进行聚合，可以得到大幅提高的性能、评估和异常检测性能。我们在多个分类任务上进行了多个数据集的实验，并证明了提posed方法的效果。此外，我们还用SVHN和CIFAR-10数据集来证明方法的异常检测能力。
</details></li>
</ul>
<hr>
<h2 id="Tirtha-–-An-Automated-Platform-to-Crowdsource-Images-and-Create-3D-Models-of-Heritage-Sites"><a href="#Tirtha-–-An-Automated-Platform-to-Crowdsource-Images-and-Create-3D-Models-of-Heritage-Sites" class="headerlink" title="Tirtha – An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites"></a>Tirtha – An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01246">http://arxiv.org/abs/2308.01246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smlab-niser/tirtha-public">https://github.com/smlab-niser/tirtha-public</a></li>
<li>paper_authors: Jyotirmaya Shivottam, Subhankar Mishra</li>
<li>for: 这篇论文的目的是为了推广和实现文化遗产（CH）区域的数位保存，并提供一个可靠、可读性高的平台 для将这些遗产转换为三维模型。</li>
<li>methods: 这篇论文使用了最新的构造从动（SfM）和多视野视力（MVS）技术，并提供了一个可调、可扩展的架构，以应对未来摄影学技术的进步。</li>
<li>results: 这篇论文的结果是一个名为Tirtha的网络平台，可以将拍摄到的CH区域转换为三维模型，并将这些模型提供给研究人员和公众进行检视、互动和下载。<details>
<summary>Abstract</summary>
Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment. In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images. These models are available for viewing, interaction, and download on the Tirtha website. Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains. Overall, Tirtha is a step towards democratizing digital preservation, primarily in resource-limited developing countries.
</details>
<details>
<summary>摘要</summary>
digitization of cultural heritage (遗产) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible, and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment. In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images. These models are available for viewing, interaction, and download on the Tirtha website. Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains. Overall, Tirtha is a step towards democratizing digital preservation, primarily in resource-limited developing countries.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/03/cs.LG_2023_08_03/" data-id="clly4xtdr0067vl888di2863p" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/03/cs.SD_2023_08_03/" class="article-date">
  <time datetime="2023-08-02T16:00:00.000Z" itemprop="datePublished">2023-08-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/03/cs.SD_2023_08_03/">cs.SD - 2023-08-03 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Versatile-Time-Frequency-Representations-Realized-by-Convex-Penalty-on-Magnitude-Spectrogram"><a href="#Versatile-Time-Frequency-Representations-Realized-by-Convex-Penalty-on-Magnitude-Spectrogram" class="headerlink" title="Versatile Time-Frequency Representations Realized by Convex Penalty on Magnitude Spectrogram"></a>Versatile Time-Frequency Representations Realized by Convex Penalty on Magnitude Spectrogram</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01665">http://arxiv.org/abs/2308.01665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keidai Arai, Koki Yamada, Kohei Yatabe</li>
<li>for: 本文旨在提出一种基于凸优化的时域频域（T-F）表示方法，以实现根据用户指定的特性来设计T-F表示。</li>
<li>methods: 本文使用了基于凸优化的方法，包括基于约束的最小二乘优化和梯度下降优化，来设计T-F表示。</li>
<li>results: 本文通过实验和分析表明，提出的方法可以实现根据用户指定的特性来设计T-F表示，并且可以获得低维度或均匀幅度的T-F表示。<details>
<summary>Abstract</summary>
Sparse time-frequency (T-F) representations have been an important research topic for more than several decades. Among them, optimization-based methods (in particular, extensions of basis pursuit) allow us to design the representations through objective functions. Since acoustic signal processing utilizes models of spectrogram, the flexibility of optimization-based T-F representations is helpful for adjusting the representation for each application. However, acoustic applications often require models of \textit{magnitude} of T-F representations obtained by discrete Gabor transform (DGT). Adjusting a T-F representation to such a magnitude model (e.g., smoothness of magnitude of DGT coefficients) results in a non-convex optimization problem that is difficult to solve. In this paper, instead of tackling difficult non-convex problems, we propose a convex optimization-based framework that realizes a T-F representation whose magnitude has characteristics specified by the user. We analyzed the properties of the proposed method and provide numerical examples of sparse T-F representations having, e.g., low-rank or smooth magnitude, which have not been realized before.
</details>
<details>
<summary>摘要</summary>
质量时频（T-F）表示已经是研究领域的重要话题，一直以来的几十年。其中，优化基于方法（尤其是基数追求的扩展），允许我们通过目标函数设计表示。由于音声信号处理使用spectrogram模型，优化基于T-F表示的灵活性对于每个应用程序都很有用。然而，音声应用经常需要DGT变折 coefficient的大小模型（例如，DGT变折 coefficient的平滑度）。对于这种非凸优化问题，我们提出了一种凸优化框架，可以实现T-F表示的大小具有用户指定的特性。我们分析了提案方法的性质并提供了数字示例，其中包括低级或平滑的T-F表示，这些表示没有在过去被实现过。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-multi-user-sound-communications-in-reverberating-environments-with-acoustic-reconfigurable-metasurfaces"><a href="#Optimizing-multi-user-sound-communications-in-reverberating-environments-with-acoustic-reconfigurable-metasurfaces" class="headerlink" title="Optimizing multi-user sound communications in reverberating environments with acoustic reconfigurable metasurfaces"></a>Optimizing multi-user sound communications in reverberating environments with acoustic reconfigurable metasurfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01531">http://arxiv.org/abs/2308.01531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongkuan Zhang, Qiyuan Wang, Mathias Fink, Guancong Ma</li>
<li>for: 解决在噪声环境中多个人同时发送多个消息，使每个消息都能够清晰地传递而无任何干扰。</li>
<li>methods: 开发智能声学墙，通过电子控制和学习算法控制室内 geometry 和源 sender 的位置，实现不受干扰的多个人同时沟通。</li>
<li>results: 实现了不受干扰的多个人同时沟通，包括不同的音乐来源之间的同时播放、频分多路播放和多用户沟通等多种功能。<details>
<summary>Abstract</summary>
How do you ensure that, in a reverberant room, several people can speak simultaneously to several other people, making themselves perfectly understood and without any crosstalk between messages? In this work, we report a conceptual solution to this problem by developing an intelligent acoustic wall, which can be reconfigured electronically and is controlled by a learning algorithm that adapts to the geometry of the room and the positions of sources and receivers. To this end, a portion of the room boundaries is covered with a smart mirror made of a broadband acoustic reconfigurable metasurface (ARMs) designed to provide a two-state (0 or {\pi}) phase shift in the reflected waves by 200 independently tunable units. The whole phase pattern is optimized to maximize the Shannon capacity while minimizing crosstalk between the different sources and receivers. We demonstrate the control of multi-spectral sound fields covering a spectrum much larger than the coherence bandwidth of the room for diverse striking functionalities, including crosstalk-free acoustic communication, frequency-multiplexed communications, and multi-user communications. An experiment conducted with two music sources for two different people demonstrates a crosstalk-free simultaneous music playback. Our work opens new routes for the control of sound waves in complex media and for a new generation of acoustic devices.
</details>
<details>
<summary>摘要</summary>
如何在吸吟的房间中，许多人同时发送消息，使自己完全清晰无任何跨信号干扰？在这项工作中，我们报道了一种概念解决方案，通过开发智能听音墙， elektronisch控制 Room 的 geometry 和发送器和接收器的位置。为此，房间的一部分使用智能镜，由200个独立调整的单元组成，可以提供0或π的零phas shift。整个零phas pattern 被优化，以最大化信annon容量，同时最小化不同发送器和接收器之间的跨信号干扰。我们示例了多 Spectral 听音场，覆盖了 much larger than the coherence bandwidth of the room的谱，包括无干扰的听音通信、频分多路通信和多用户通信。在实验中，使用两个不同的音乐来源，对两个不同的人进行同时播放音乐，成功实现了无干扰的同时播放。我们的工作开启了新的控制听音波在复杂媒体中的新 Routes 和一代新的听音设备。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/03/cs.SD_2023_08_03/" data-id="clly4xtel009evl8822si24v5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/03/eess.AS_2023_08_03/" class="article-date">
  <time datetime="2023-08-02T16:00:00.000Z" itemprop="datePublished">2023-08-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/03/eess.AS_2023_08_03/">eess.AS - 2023-08-03 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Many-to-Many-Spoken-Language-Translation-via-Unified-Speech-and-Text-Representation-Learning-with-Unit-to-Unit-Translation"><a href="#Many-to-Many-Spoken-Language-Translation-via-Unified-Speech-and-Text-Representation-Learning-with-Unit-to-Unit-Translation" class="headerlink" title="Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation"></a>Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01831">http://arxiv.org/abs/2308.01831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Jeongsoo Choi, Dahun Kim, Yong Man Ro</li>
<li>for: 本研究旨在提出一种方法，使用单一模型来学习多语言speech和文本的统一表示，尤其关注speech synthesis的目的。</li>
<li>methods: 我们使用自动编码的speech特征来表示多语言speech音频，并将其视为pseudo文本来处理。然后，我们提议使用encoder-decoder结构的模型，以Unit-to-Unit Translation（UTUT）目标进行训练。</li>
<li>results: 我们通过在多种语言下进行了广泛的实验， Validate了我们提议的方法的有效性 across多种多语言任务，包括Speech-to-Speech Translation（STS）、多语言Text-to-Speech Synthesis（TTS）和Text-to-Speech Translation（TTST）。此外，我们还证明UTUT可以完成多语言STS，这在文献中尚未被探讨。<details>
<summary>Abstract</summary>
In this paper, we propose a method to learn unified representations of multilingual speech and text with a single model, especially focusing on the purpose of speech synthesis. We represent multilingual speech audio with speech units, the quantized representations of speech features encoded from a self-supervised speech model. Therefore, we can focus on their linguistic content by treating the audio as pseudo text and can build a unified representation of speech and text. Then, we propose to train an encoder-decoder structured model with a Unit-to-Unit Translation (UTUT) objective on multilingual data. Specifically, by conditioning the encoder with the source language token and the decoder with the target language token, the model is optimized to translate the spoken language into that of the target language, in a many-to-many language translation setting. Therefore, the model can build the knowledge of how spoken languages are comprehended and how to relate them to different languages. A single pre-trained model with UTUT can be employed for diverse multilingual speech- and text-related tasks, such as Speech-to-Speech Translation (STS), multilingual Text-to-Speech Synthesis (TTS), and Text-to-Speech Translation (TTST). By conducting comprehensive experiments encompassing various languages, we validate the efficacy of the proposed method across diverse multilingual tasks. Moreover, we show UTUT can perform many-to-many language STS, which has not been previously explored in the literature. Samples are available on https://choijeongsoo.github.io/utut.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法，可以通过单个模型学习多语言speech和文本的统一表示，尤其是在speech synthesis的目的下。我们使用自动生成的speech模型来编码speech特征，并将其转换为speech单元。因此，我们可以忽略它们的语言特点，将audio视为pseudo文本，并建立speech和文本的统一表示。然后，我们提议使用encoder-decoder结构的模型，通过Unit-to-Unit Translation（UTUT）目标在多语言数据上进行训练。具体来说，通过将源语言token条件到encoder，并将目标语言token条件到decoder，模型将被优化以将说话语言翻译成目标语言，这种Setting被称为多语言翻译。因此，模型可以学习说话语言的理解和如何将其与不同语言相关联。一个预训练的UTUT模型可以在多种多语言speech-和文本相关任务上进行多样化应用，如Speech-to-Speech Translation（STS）、多语言文本识别（TTS）和文本识别翻译（TTST）。我们通过包括多种语言的实验， validate了我们提出的方法的有效性 across多种多语言任务。此外，我们还证明UTUT可以实现多语言STS，这在文献中尚未被探讨。样本可以在https://choijeongsoo.github.io/utut 上找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/03/eess.AS_2023_08_03/" data-id="clly4xtfa00c2vl88emkz0gus" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/9/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/11/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">108</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
