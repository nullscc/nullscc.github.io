
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/10/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/cs.CV_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T13:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/08/cs.CV_2023_11_08/">cs.CV - 2023-11-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Transfer-Learning-for-Efficient-Video-Specific-Human-Pose-Estimation"><a href="#Active-Transfer-Learning-for-Efficient-Video-Specific-Human-Pose-Estimation" class="headerlink" title="Active Transfer Learning for Efficient Video-Specific Human Pose Estimation"></a>Active Transfer Learning for Efficient Video-Specific Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05041">http://arxiv.org/abs/2311.05041</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iminthemiddle/vatl4pose-wacv2024">https://github.com/iminthemiddle/vatl4pose-wacv2024</a></li>
<li>paper_authors: Hiromu Taketsugu, Norimichi Ukita</li>
<li>For: 这个论文的目的是提出一种基于活动学习和传输学习的方法，以有效地适应人体 pose 估计器到个人视频频谱中。* Methods: 该方法使用了热图的变化来衡量估计结果的不确定性，以及全身人体 pose 的不自然性来选择不同和不确定的样本进行有效的估计器学习。此外，该方法还重新评估了现有的活动转移学习方法，并提出了新的重新训练方法和停止 criterion。* Results: 实验结果表明，该方法可以提高学习效率，并在比较方法中得到更好的性能。代码可以在 GitHub 上获取：<a target="_blank" rel="noopener" href="https://github.com/ImIntheMiddle/VATL4Pose-WACV2024%E3%80%82">https://github.com/ImIntheMiddle/VATL4Pose-WACV2024。</a><details>
<summary>Abstract</summary>
Human Pose (HP) estimation is actively researched because of its wide range of applications. However, even estimators pre-trained on large datasets may not perform satisfactorily due to a domain gap between the training and test data. To address this issue, we present our approach combining Active Learning (AL) and Transfer Learning (TL) to adapt HP estimators to individual video domains efficiently. For efficient learning, our approach quantifies (i) the estimation uncertainty based on the temporal changes in the estimated heatmaps and (ii) the unnaturalness in the estimated full-body HPs. These quantified criteria are then effectively combined with the state-of-the-art representativeness criterion to select uncertain and diverse samples for efficient HP estimator learning. Furthermore, we reconsider the existing Active Transfer Learning (ATL) method to introduce novel ideas related to the retraining methods and Stopping Criteria (SC). Experimental results demonstrate that our method enhances learning efficiency and outperforms comparative methods. Our code is publicly available at: https://github.com/ImIntheMiddle/VATL4Pose-WACV2024
</details>
<details>
<summary>摘要</summary>
人体姿势（HP）估计是正在活跃研究的领域，因为它有很多应用场景。然而，即使使用大量数据进行预训练，HP估计器也可能不能达到预期的性能，这是因为训练和测试数据之间的领域差异。为了解决这个问题，我们提出了一种结合活动学习（AL）和转移学习（TL）的方法，以高效地适应视频域中的HP估计器。为了高效学习，我们的方法量化了（i）估计热图中的时间变化引入的估计不确定性，以及（ii）全身HP估计中的不自然性。这些量化的 критери价然与当前最佳表达性 критериion（SC）有效地结合，以选择不确定和多样的样本，以便高效地学习HP估计器。此外，我们重新考虑了现有的活动转移学习（ATL）方法，并引入了新的重新训练方法和停止标准（SC）。实验结果表明，我们的方法可以提高学习效率，并比相对方法高效。我们的代码可以在 GitHub 上获取：https://github.com/ImIntheMiddle/VATL4Pose-WACV2024
</details></li>
</ul>
<hr>
<h2 id="S-3-AD-Semi-supervised-Small-Apple-Detection-in-Orchard-Environments"><a href="#S-3-AD-Semi-supervised-Small-Apple-Detection-in-Orchard-Environments" class="headerlink" title="S$^3$AD: Semi-supervised Small Apple Detection in Orchard Environments"></a>S$^3$AD: Semi-supervised Small Apple Detection in Orchard Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05029">http://arxiv.org/abs/2311.05029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Johanson, Christian Wilms, Ole Johannsen, Simone Frintrop</li>
<li>for: 本文 targets 苹果干部检测问题，为精准农业应用，如自动化受产量测量或水果摘取提供了解决方案。</li>
<li>methods: 本文提出了一种半监督的苹果检测方法，使用上下文注意力和选择平铺来改进小苹果的检测，同时减少计算成本。</li>
<li>results: 对于MAD数据集和MSU数据集的广泛评估表明，S$^3$AD系统与多个强大的完全监督基线系统进行比较，达到了最高的$14.9%$的提升。此外，通过利用数据集中苹果属性的详细注释，分析了不同系统对尺寸和干扰度的影响，量化了当前的挑战。<details>
<summary>Abstract</summary>
Crop detection is integral for precision agriculture applications such as automated yield estimation or fruit picking. However, crop detection, e.g., apple detection in orchard environments remains challenging due to a lack of large-scale datasets and the small relative size of the crops in the image. In this work, we address these challenges by reformulating the apple detection task in a semi-supervised manner. To this end, we provide the large, high-resolution dataset MAD comprising 105 labeled images with 14,667 annotated apple instances and 4,440 unlabeled images. Utilizing this dataset, we also propose a novel Semi-Supervised Small Apple Detection system S$^3$AD based on contextual attention and selective tiling to improve the challenging detection of small apples, while limiting the computational overhead. We conduct an extensive evaluation on MAD and the MSU dataset, showing that S$^3$AD substantially outperforms strong fully-supervised baselines, including several small object detection systems, by up to $14.9\%$. Additionally, we exploit the detailed annotations of our dataset w.r.t. apple properties to analyze the influence of relative size or level of occlusion on the results of various systems, quantifying current challenges.
</details>
<details>
<summary>摘要</summary>
减损检测是精准农业应用程序的关键组成部分，如自动化产量估计或果实摘取。然而，减损检测，例如苹果检测在园区环境中仍然是一个挑战，因为缺乏大规模数据集和图像中小型减损的相对比例。在这种情况下，我们通过半upervised的方式改进苹果检测任务。为此，我们提供了大型、高分辨率的数据集MAD，包含105个标注图像和14,667个注解apple实例，以及4,440个未标注图像。基于这个数据集，我们还提出了一种新的半upervised小 apple检测系统S$^3$AD，使用contextual attention和选择分割来改进小apple的检测，同时限制计算过程的开销。我们对MAD和MSU数据集进行了广泛的评估，显示S$^3$AD在比较减损的情况下，与强大的完全supervised基eline相比，提高了14.9%。此外，我们利用我们数据集中 apple 属性的详细注解，分析不同系统在不同的减损和 occlusion 情况下的结果，并对现有挑战进行了质量评估。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-a-realistic-synthetic-database-to-learn-Shape-from-Shading-for-estimating-the-colon-depth-in-colonoscopy-images"><a href="#Leveraging-a-realistic-synthetic-database-to-learn-Shape-from-Shading-for-estimating-the-colon-depth-in-colonoscopy-images" class="headerlink" title="Leveraging a realistic synthetic database to learn Shape-from-Shading for estimating the colon depth in colonoscopy images"></a>Leveraging a realistic synthetic database to learn Shape-from-Shading for estimating the colon depth in colonoscopy images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05021">http://arxiv.org/abs/2311.05021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josué Ruano, Martín Gómez, Eduardo Romero, Antoine Manzanera</li>
<li>for: 这个研究旨在提高单摄colonoscopy影像中的肠道深度估计，以帮助诊断肠及肛部癌症。</li>
<li>methods: 这个研究使用了一个新的方法来从单摄colonoscopy影像中估计肠道深度图。这个方法基于肠道墙的阴影变化，并由一个专门训练的单元神经网络估计。</li>
<li>results: 这个研究的结果显示了这个方法可以实现高精度的肠道深度估计，具有95.65%的阈值准确性和0.451cm的平均误差。此外，该方法在一些真实的影像中也显示了良好的成果。<details>
<summary>Abstract</summary>
Colonoscopy is the choice procedure to diagnose colon and rectum cancer, from early detection of small precancerous lesions (polyps), to confirmation of malign masses. However, the high variability of the organ appearance and the complex shape of both the colon wall and structures of interest make this exploration difficult. Learned visuospatial and perceptual abilities mitigate technical limitations in clinical practice by proper estimation of the intestinal depth. This work introduces a novel methodology to estimate colon depth maps in single frames from monocular colonoscopy videos. The generated depth map is inferred from the shading variation of the colon wall with respect to the light source, as learned from a realistic synthetic database. Briefly, a classic convolutional neural network architecture is trained from scratch to estimate the depth map, improving sharp depth estimations in haustral folds and polyps by a custom loss function that minimizes the estimation error in edges and curvatures. The network was trained by a custom synthetic colonoscopy database herein constructed and released, composed of 248,400 frames (47 videos), with depth annotations at the level of pixels. This collection comprehends 5 subsets of videos with progressively higher levels of visual complexity. Evaluation of the depth estimation with the synthetic database reached a threshold accuracy of 95.65%, and a mean-RMSE of 0.451 cm, while a qualitative assessment with a real database showed consistent depth estimations, visually evaluated by the expert gastroenterologist coauthoring this paper. Finally, the method achieved competitive performance with respect to another state-of-the-art method using a public synthetic database and comparable results in a set of images with other five state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
殖colonoscopy是诊断大肠和直肠癌的首选方法，从早期发现小前癌肿瘤（肿块）到确认肿瘤。然而，肠部的高变化性和肠壁的复杂形状使这种探测困难。通过了解视觉和感知能力，技术上的限制可以得到应用。这项工作介绍了一种新的方法，用单摄影探测肠部深度图。该图由肠壁阴影变化与光源的相对位置学习得到，并通过一个自定义的损失函数来改进锐利的深度估计。这种方法使用了自定义的 sintetic colonoscopy 数据库，包括 248,400 帧（47 个视频），其中每帧有像素级别的深度注解。这个收集包括 5 个视频subset，每个 subset 的视觉复杂度逐渐增加。对于这个数据库的评估，depth estimation 达到了95.65%的阈值精度和0.451 cm的平均欧姆误差，而且与真实数据库的评估显示了一致的深度估计。最终，该方法在与另一个状态略方法进行比较时达到了竞争性性能，并在一组图像中与其他五个状态略方法的结果相当。
</details></li>
</ul>
<hr>
<h2 id="Familiarity-Based-Open-Set-Recognition-Under-Adversarial-Attacks"><a href="#Familiarity-Based-Open-Set-Recognition-Under-Adversarial-Attacks" class="headerlink" title="Familiarity-Based Open-Set Recognition Under Adversarial Attacks"></a>Familiarity-Based Open-Set Recognition Under Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05006">http://arxiv.org/abs/2311.05006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Enevoldsen, Christian Gundersen, Nico Lang, Serge Belongie, Christian Igel</li>
<li>for: 本研究旨在探讨 familiarscore-based open-set recognition 中的攻击问题，以及这些攻击的效果。</li>
<li>methods: 本研究使用了梯度导向的 adversarial 攻击方法，包括 False Familiarity 和 False Novelty 两种类型的攻击。</li>
<li>results: 研究发现，在 informed 和 uninformed 设置下，这些攻击都能够减少 familiarscore-based open-set recognition 的准确率。<details>
<summary>Abstract</summary>
Open-set recognition (OSR), the identification of novel categories, can be a critical component when deploying classification models in real-world applications. Recent work has shown that familiarity-based scoring rules such as the Maximum Softmax Probability (MSP) or the Maximum Logit Score (MLS) are strong baselines when the closed-set accuracy is high. However, one of the potential weaknesses of familiarity-based OSR are adversarial attacks. Here, we present gradient-based adversarial attacks on familiarity scores for both types of attacks, False Familiarity and False Novelty attacks, and evaluate their effectiveness in informed and uninformed settings on TinyImageNet.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Effective-Restoration-of-Source-Knowledge-in-Continual-Test-Time-Adaptation"><a href="#Effective-Restoration-of-Source-Knowledge-in-Continual-Test-Time-Adaptation" class="headerlink" title="Effective Restoration of Source Knowledge in Continual Test Time Adaptation"></a>Effective Restoration of Source Knowledge in Continual Test Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04991">http://arxiv.org/abs/2311.04991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fahim Faisal Niloy, Sk Miraj Ahmed, Dripta S. Raychaudhuri, Samet Oymak, Amit K. Roy-Chowdhury</li>
<li>for: 本文旨在解决测试时适应（TTA）方法在动态环境中的挑战，包括累累忘记之前学习的有价值源知识和逐渐增加的错误。</li>
<li>methods: 本文提出了一种无监督领域变换检测方法，可以在动态环境中检测领域变换并将模型参数还原到原始源预训练值。这种方法通过监测领域变换触发的统计变化来重新启用原始源的知识，从而 correction 模型参数的负面影响。</li>
<li>results: 对比州前方法，本文的方法在动态环境中表现出色，可以减少模型参数的负面影响和累累忘记。我们通过对多个 benchmark 数据集进行广泛的实验来证明本文的方法的优秀性。<details>
<summary>Abstract</summary>
Traditional test-time adaptation (TTA) methods face significant challenges in adapting to dynamic environments characterized by continuously changing long-term target distributions. These challenges primarily stem from two factors: catastrophic forgetting of previously learned valuable source knowledge and gradual error accumulation caused by miscalibrated pseudo labels. To address these issues, this paper introduces an unsupervised domain change detection method that is capable of identifying domain shifts in dynamic environments and subsequently resets the model parameters to the original source pre-trained values. By restoring the knowledge from the source, it effectively corrects the negative consequences arising from the gradual deterioration of model parameters caused by ongoing shifts in the domain. Our method involves progressive estimation of global batch-norm statistics specific to each domain, while keeping track of changes in the statistics triggered by domain shifts. Importantly, our method is agnostic to the specific adaptation technique employed and thus, can be incorporated to existing TTA methods to enhance their performance in dynamic environments. We perform extensive experiments on benchmark datasets to demonstrate the superior performance of our method compared to state-of-the-art adaptation methods.
</details>
<details>
<summary>摘要</summary>
传统的测试时适应（TTA）方法在面临不断变化的长期目标分布环境中遇到重大挑战。这些挑战主要来自两个因素： catastrophic forgetting 已经学习的源知识的价值，以及由于 pseudo 标签的误偏而导致的慢滑块误差的堆积。为解决这些问题，本文提出了一种无监督领域变化检测方法，能够在动态环境中检测领域的变化，并将模型参数重置回源预训练值。通过恢复源知识，它有效地纠正由持续变化的领域所导致的模型参数的负面影响。我们的方法包括逐步估计每个领域的全局批处理 нор 特征统计，并记录每个领域的变化触发的统计变化。与此同时，我们的方法是对现有 TTA 方法进行增强，不需要改变现有的适应技术。我们在标准的测试数据集上进行了广泛的实验，并证明了我们的方法在动态环境中的超过当今适应方法的表现。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Inductive-Biases-in-Video-Modeling-through-Neural-CDEs"><a href="#Exploiting-Inductive-Biases-in-Video-Modeling-through-Neural-CDEs" class="headerlink" title="Exploiting Inductive Biases in Video Modeling through Neural CDEs"></a>Exploiting Inductive Biases in Video Modeling through Neural CDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04986">http://arxiv.org/abs/2311.04986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johnathan Chiu, Samuel Duffield, Max Hunter-Gordon, Kaelan Donatella, Max Aifer, Andi Gu</li>
<li>for: 这篇论文是用于Video Task中的Video interpolating和Mask Propagation问题。</li>
<li>methods: 本文提出了一种使用Controlled Differential Equations（CDEs）来解决Video Task中的Key challenges，包括Video interpolating和Mask Propagation。它将CDEs应用于不同的分辨率，实现了一个连续时间的U-Net架构。不同于传统方法，本文的方法不需要明确的Optical Flow学习，而是利用CDEs的自然连续时间特性来生成高度表达力的Video模型。</li>
<li>results: 本文展示了与State-of-the-art模型相比，本文的方法在Video interpolating和Mask Propagation任务中具有竞争性的性能。<details>
<summary>Abstract</summary>
We introduce a novel approach to video modeling that leverages controlled differential equations (CDEs) to address key challenges in video tasks, notably video interpolation and mask propagation. We apply CDEs at varying resolutions leading to a continuous-time U-Net architecture. Unlike traditional methods, our approach does not require explicit optical flow learning, and instead makes use of the inherent continuous-time features of CDEs to produce a highly expressive video model. We demonstrate competitive performance against state-of-the-art models for video interpolation and mask propagation tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的视频模型方法，利用控制的微分方程（CDE）解决视频任务中的关键挑战，包括视频 interpolate 和mask propagation。我们在不同的分辨率上应用CDE，导致一种连续时间的U-Net架构。与传统方法不同，我们的方法不需要显式学习流体力学，而是利用CDE的内在连续时间特征来生成一个非常表达力的视频模型。我们在视频 interpolate 和mask propagation任务中示出了与状态艺术模型的竞争性表现。
</details></li>
</ul>
<hr>
<h2 id="GENOME-GenerativE-Neuro-symbOlic-visual-reasoning-by-growing-and-reusing-ModulEs"><a href="#GENOME-GenerativE-Neuro-symbOlic-visual-reasoning-by-growing-and-reusing-ModulEs" class="headerlink" title="GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs"></a>GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04901">http://arxiv.org/abs/2311.04901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Hong, Chuang Gan</li>
<li>for: 本研究目的是提出一种基于生长和重用模块的生成型神经符号逻辑视觉理解方法，以提高现有神经符号逻辑模型的效率和可重用性。</li>
<li>methods: 本方法包括三个独特的阶段：模块初始化、模块生成和模块执行。首先，给定一个视力语言任务，我们采用大语言模型来检查是否可以重用和增长已有的模块来解决这个新任务。如果不能，我们将创建一个新模块，并指定这个模块的输入和输出。然后，我们使用大语言模型来生成匹配要求的代码块，并将其添加到模块库中。为了更好地评估新模块的能力，我们将几个例子作为测试用例，并评估它们是否可以通过这些测试。如果可以，我们将新模块添加到模块库中，并在其他任务上进行重用。最后，我们使用执行生成的程序来评估模型的性能。</li>
<li>results: 我们的模型在标准任务如视觉问答和参考表达理解中表现竞争力强，同时模块学习自一个任务可以很好地转移到新任务上。此外，我们的模型可以通过几个例子的几个测试来适应新的视觉理解任务，而不需要大量的训练数据。<details>
<summary>Abstract</summary>
Recent works have shown that Large Language Models (LLMs) could empower traditional neuro-symbolic models via programming capabilities to translate language into module descriptions, thus achieving strong visual reasoning results while maintaining the model's transparency and efficiency. However, these models usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective. We propose generative neuro-symbolic visual reasoning by growing and reusing modules. Specifically, our model consists of three unique stages, module initialization, module generation, and module execution. First, given a vision-language task, we adopt LLMs to examine whether we could reuse and grow over established modules to handle this new task. If not, we initialize a new module needed by the task and specify the inputs and outputs of this new module. After that, the new module is created by querying LLMs to generate corresponding code snippets that match the requirements. In order to get a better sense of the new module's ability, we treat few-shot training examples as test cases to see if our new module could pass these cases. If yes, the new module is added to the module library for future reuse. Finally, we evaluate the performance of our model on the testing set by executing the parsed programs with the newly made visual modules to get the results. We find the proposed model possesses several advantages. First, it performs competitively on standard tasks like visual question answering and referring expression comprehension; Second, the modules learned from one task can be seamlessly transferred to new tasks; Last but not least, it is able to adapt to new visual reasoning tasks by observing a few training examples and reusing modules.
</details>
<details>
<summary>摘要</summary>
First, we use LLMs to determine if we can reuse or grow existing modules to handle a new task. If not, we initialize a new module with the task's inputs and outputs. Then, we use LLMs to generate code snippets that match the module's requirements. We treat a few training examples as test cases to evaluate the new module's ability. If the module passes the test cases, it is added to the module library for future reuse. Finally, we evaluate the model's performance on a testing set by executing the parsed programs with the newly made visual modules.Our proposed model has several advantages. First, it performs well on standard visual reasoning tasks like visual question answering and referring expression comprehension. Second, the modules learned from one task can be easily transferred to new tasks. Lastly, the model can adapt to new visual reasoning tasks by observing a few training examples and reusing modules.
</details></li>
</ul>
<hr>
<h2 id="Are-foundation-models-efficient-for-medical-image-segmentation"><a href="#Are-foundation-models-efficient-for-medical-image-segmentation" class="headerlink" title="Are foundation models efficient for medical image segmentation?"></a>Are foundation models efficient for medical image segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04847">http://arxiv.org/abs/2311.04847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danielle Ferreira, Rima Arnaout</li>
<li>for: 这个论文是为了评估Segment Anything模型（SAM）在各种物体分割任务中的表现，以及相比之下一种特有的模式自适应学习（SSL）方法在25种量化报告中的性能。</li>
<li>methods: 这个论文使用了Supervised Training方法，并对SAM和SSL方法进行了比较，以评估它们在100次心脏超声报告中的性能和资源占用情况。</li>
<li>results: 研究发现，SAM在评估中表现不佳，需要更多的标注和计算资源，而SSL方法则表现更好，具有更高的效率。<details>
<summary>Abstract</summary>
Foundation models are experiencing a surge in popularity. The Segment Anything model (SAM) asserts an ability to segment a wide spectrum of objects but required supervised training at unprecedented scale. We compared SAM's performance (against clinical ground truth) and resources (labeling time, compute) to a modality-specific, label-free self-supervised learning (SSL) method on 25 measurements for 100 cardiac ultrasounds. SAM performed poorly and required significantly more labeling and computing resources, demonstrating worse efficiency than SSL.
</details>
<details>
<summary>摘要</summary>
基础模型目前正在流行。射频段化模型（SAM）声称可以分类广泛的物体，但需要前所未有的指导式训练。我们比较了SAM的性能（与临床真实值）和资源（标签时间、计算）与一种特定Modalities自适应学习（SSL）方法在25个测量上100个心脏超声图像。SAM表现不佳，需要更多的标签和计算资源，示出了与SSL的更差的效率。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Visual-Relationship-Detection-through-Masked-Bounding-Box-Reconstruction"><a href="#Self-Supervised-Learning-for-Visual-Relationship-Detection-through-Masked-Bounding-Box-Reconstruction" class="headerlink" title="Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction"></a>Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04834">http://arxiv.org/abs/2311.04834</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deeplab-ai/selfsupervisedvrd">https://github.com/deeplab-ai/selfsupervisedvrd</a></li>
<li>paper_authors: Zacharias Anastasakis, Dimitrios Mallis, Markos Diomataris, George Alexandridis, Stefanos Kollias, Vassilis Pitsikalis</li>
<li>for: 本研究旨在提出一种自助学习方法，用于视觉关系检测任务（VRD）。</li>
<li>methods: 该方法基于Masked Image Modeling（MIM）的想法，提出Masked Bounding Box Reconstruction（MBBR），即在场景中随机mask一部分实体&#x2F;物体，然后通过不masked对象进行重建。这种方法通过对象级别的masked模型学习，使网络学习场景中对象之间的交互，并因此具有高度预测视觉对象关系的表示。</li>
<li>results: 对于Visual Relationship Detection（VRD）任务，该方法在几个少量示例的情况下，能够超过现有的状态对方法，并且 Qualitative和Quantitative评估都表明了该方法学习的图像表示能够具有高度的Robustness和可预测性。<details>
<summary>Abstract</summary>
We present a novel self-supervised approach for representation learning, particularly for the task of Visual Relationship Detection (VRD). Motivated by the effectiveness of Masked Image Modeling (MIM), we propose Masked Bounding Box Reconstruction (MBBR), a variation of MIM where a percentage of the entities/objects within a scene are masked and subsequently reconstructed based on the unmasked objects. The core idea is that, through object-level masked modeling, the network learns context-aware representations that capture the interaction of objects within a scene and thus are highly predictive of visual object relationships. We extensively evaluate learned representations, both qualitatively and quantitatively, in a few-shot setting and demonstrate the efficacy of MBBR for learning robust visual representations, particularly tailored for VRD. The proposed method is able to surpass state-of-the-art VRD methods on the Predicate Detection (PredDet) evaluation setting, using only a few annotated samples. We make our code available at https://github.com/deeplab-ai/SelfSupervisedVRD.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的自主学习方法，具体是用于视觉关系检测（VRD）任务。我们受到Masked Image Modeling（MIM）的成功所 inspirited，我们提议Masked Bounding Box Reconstruction（MBBR），这是MIM的一种变体，在场景中部分对象被遮盖，然后根据未遮盖的对象进行重建。核心想法是通过对象水平的遮盖模型，使网络学习场景中对象之间的交互，从而学习出高度预测视觉对象关系的上下文意识的表示。我们在几个shot设置下进行了详细评估learned表示，并证明MBBR可以学习出高效的视觉表示，特别适用于VRD任务。我们使用了只有几个标注样本，但我们的方法仍然可以超越当前VRD方法在Predicate Detection（PredDet）评估设置中的性能。我们在https://github.com/deeplab-ai/SelfSupervisedVRD中提供了代码。
</details></li>
</ul>
<hr>
<h2 id="Anonymizing-medical-case-based-explanations-through-disentanglement"><a href="#Anonymizing-medical-case-based-explanations-through-disentanglement" class="headerlink" title="Anonymizing medical case-based explanations through disentanglement"></a>Anonymizing medical case-based explanations through disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04833">http://arxiv.org/abs/2311.04833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helena Montenegro, Jaime S. Cardoso</li>
<li>for: 本研究旨在Addressing the problem of privacy concerns in deep learning models for medical image analysis, by proposing a novel method for disentangling identity and medical characteristics of images and anonymizing them.</li>
<li>methods: 本研究使用了一种novel方法， named 嵌入式推理（Embedding-based reasoning）, which disentangles the identity and medical characteristics of images by replacing some feature vectors while preserving the remaining features. The researchers also proposed a model to manufacture synthetic privacy-preserving identities to replace the original image’s identity and achieve anonymization.</li>
<li>results: 实验表明，这种方法可以生成真实的、适用于医疗和生物特征的synthetic privacy-preserving identities，并且可以通过替换医学特征来生成counterfactual images。 The results demonstrate the capacity of the proposed method to generate realistic-looking anonymized images that preserve their original medical content, and the network’s inherent capacity to generate counterfactual images through the replacement of medical features.<details>
<summary>Abstract</summary>
Case-based explanations are an intuitive method to gain insight into the decision-making process of deep learning models in clinical contexts. However, medical images cannot be shared as explanations due to privacy concerns. To address this problem, we propose a novel method for disentangling identity and medical characteristics of images and apply it to anonymize medical images. The disentanglement mechanism replaces some feature vectors in an image while ensuring that the remaining features are preserved, obtaining independent feature vectors that encode the images' identity and medical characteristics. We also propose a model to manufacture synthetic privacy-preserving identities to replace the original image's identity and achieve anonymization. The models are applied to medical and biometric datasets, demonstrating their capacity to generate realistic-looking anonymized images that preserve their original medical content. Additionally, the experiments show the network's inherent capacity to generate counterfactual images through the replacement of medical features.
</details>
<details>
<summary>摘要</summary>
情况基本的解释是一种直观的方法，用于掌握深度学习模型在医疗场景中做出决策的过程。然而，医疗图像不能被用作解释，因为隐私问题。为解决这个问题，我们提出了一种新的方法，用于分离图像的标识特征和医疗特征。这种分离机制将某些图像特征替换，以保证保留图像的其他特征，从而获得独立的特征向量，这些特征向量都是图像的标识特征和医疗特征。我们还提出了一种模型，用于生成隐私保护的人工标识，以替换原始图像的标识。这些模型在医疗和生物метрических数据集上应用， demonstarting their capacity to generate realistic-looking anonymized images that preserve their original medical content. In addition, the experiments show the network's inherent capacity to generate counterfactual images through the replacement of medical features.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SODAWideNet-–-Salient-Object-Detection-with-an-Attention-augmented-Wide-Encoder-Decoder-network-without-ImageNet-pre-training"><a href="#SODAWideNet-–-Salient-Object-Detection-with-an-Attention-augmented-Wide-Encoder-Decoder-network-without-ImageNet-pre-training" class="headerlink" title="SODAWideNet – Salient Object Detection with an Attention augmented Wide Encoder Decoder network without ImageNet pre-training"></a>SODAWideNet – Salient Object Detection with an Attention augmented Wide Encoder Decoder network without ImageNet pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04828">http://arxiv.org/abs/2311.04828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VimsLab/SODAWideNet">https://github.com/VimsLab/SODAWideNet</a></li>
<li>paper_authors: Rohit Venkata Sai Dulam, Chandra Kambhamettu</li>
<li>for: 这个论文的目的是开发一个新的突出对象检测（SOD）模型，而不需要在ImageNet dataset上重新训练整个网络。</li>
<li>methods: 该模型使用了一个encoder-decoder风格的网络，并提出了多种新的特征反ernerModule来使用backbone特征。其中包括了多重接收场（MRFFAM）和多scale注意（MSA）等模块，以提高网络的表达能力和灵活性。</li>
<li>results: 模型在五个 dataset上达到了竞争性的性能，并且 Parameters efficient。<details>
<summary>Abstract</summary>
Developing a new Salient Object Detection (SOD) model involves selecting an ImageNet pre-trained backbone and creating novel feature refinement modules to use backbone features. However, adding new components to a pre-trained backbone needs retraining the whole network on the ImageNet dataset, which requires significant time. Hence, we explore developing a neural network from scratch directly trained on SOD without ImageNet pre-training. Such a formulation offers full autonomy to design task-specific components. To that end, we propose SODAWideNet, an encoder-decoder-style network for Salient Object Detection. We deviate from the commonly practiced paradigm of narrow and deep convolutional models to a wide and shallow architecture, resulting in a parameter-efficient deep neural network. To achieve a shallower network, we increase the receptive field from the beginning of the network using a combination of dilated convolutions and self-attention. Therefore, we propose Multi Receptive Field Feature Aggregation Module (MRFFAM) that efficiently obtains discriminative features from farther regions at higher resolutions using dilated convolutions. Next, we propose Multi-Scale Attention (MSA), which creates a feature pyramid and efficiently computes attention across multiple resolutions to extract global features from larger feature maps. Finally, we propose two variants, SODAWideNet-S (3.03M) and SODAWideNet (9.03M), that achieve competitive performance against state-of-the-art models on five datasets.
</details>
<details>
<summary>摘要</summary>
开发新的突出对象检测（SOD）模型需要选择一个ImageNet预训练后缘和创造新的特征级进程来使用后缘特征。然而，将新组件添加到预训练后缘需要重新训练整个网络在ImageNet dataset上，这需要很长时间。因此，我们探索直接从 scratch 开发一个神经网络，不需要 ImageNet 预训练。这种方法允许我们完全自主地设计任务特定的组件。为此，我们提出了SODAWideNet，一个encoder-decoder风格的神经网络用于突出对象检测。我们与通常实践的窄深 convolutional 模型不同，使用宽浅的架构，从而实现参数效率的深度神经网络。为了实现更 shallow 的网络，我们从网络的开始点增加了扩展卷积和自注意力。因此，我们提出了多个感知场Feature Aggregation Module（MRFFAM），可以有效地从更远的区域获取高分辨率的特征。接着，我们提出了多Scale Attention（MSA），可以快速计算多个分辨率之间的注意力，以提取更大的特征图。最后，我们提出了SODAWideNet-S（3.03M）和SODAWideNet（9.03M）两种变体，在五个 dataset 上实现了与当前模型竞争的性能。
</details></li>
</ul>
<hr>
<h2 id="Cross-Silo-Federated-Learning-Across-Divergent-Domains-with-Iterative-Parameter-Alignment"><a href="#Cross-Silo-Federated-Learning-Across-Divergent-Domains-with-Iterative-Parameter-Alignment" class="headerlink" title="Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment"></a>Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04818">http://arxiv.org/abs/2311.04818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mattgorb/iterative_parameter_alignment">https://github.com/mattgorb/iterative_parameter_alignment</a></li>
<li>paper_authors: Matt Gorbett, Hossein Shirazi, Indrakshi Ray</li>
<li>for: 这 paper 的目的是提出一种基于 peer-to-peer topology的 Federated Learning 方法，以便在各自的数据集上训练模型，并在模型之间进行参数的对齐，以提高模型的泛化能力。</li>
<li>methods: 这 paper 使用了一种Weighted Distance Minimization 方法来对模型参数进行对齐，并在每个参与者的模型中寻找一个唯一的解。</li>
<li>results: 这 paper 的实验结果表明，这种方法可以在不同的数据集上达到竞争力的结果，并且在不同的领域中进行模型的对齐。此外，这种方法还能够在各个参与者的模型中寻找唯一的解，从而实现了在模型之间的对齐。<details>
<summary>Abstract</summary>
Learning from the collective knowledge of data dispersed across private sources can provide neural networks with enhanced generalization capabilities. Federated learning, a method for collaboratively training a machine learning model across remote clients, achieves this by combining client models via the orchestration of a central server. However, current approaches face two critical limitations: i) they struggle to converge when client domains are sufficiently different, and ii) current aggregation techniques produce an identical global model for each client. In this work, we address these issues by reformulating the typical federated learning setup: rather than learning a single global model, we learn N models each optimized for a common objective. To achieve this, we apply a weighted distance minimization to model parameters shared in a peer-to-peer topology. The resulting framework, Iterative Parameter Alignment, applies naturally to the cross-silo setting, and has the following properties: (i) a unique solution for each participant, with the option to globally converge each model in the federation, and (ii) an optional early-stopping mechanism to elicit fairness among peers in collaborative learning settings. These characteristics jointly provide a flexible new framework for iteratively learning from peer models trained on disparate datasets. We find that the technique achieves competitive results on a variety of data partitions compared to state-of-the-art approaches. Further, we show that the method is robust to divergent domains (i.e. disjoint classes across peers) where existing approaches struggle.
</details>
<details>
<summary>摘要</summary>
学习从分散在私人源中的数据收集的总知识可以提高神经网络的泛化能力。联邦学习方法可以在远程客户端上协同训练机器学习模型，并将客户端模型通过中央服务器的协调合并到一起。然而，现有方法面临两个重要的限制：一是在客户端领域差异足够大时困难于收敛，二是现有的集成技术会生成每个客户端都相同的全球模型。在这项工作中，我们解决这些问题 by 重新定义联邦学习设置：而不是学习单一的全球模型，我们学习 N 个优化为共同目标的模型。为 достичь这一点，我们使用分数据归一化来对参数共享在层次结构中进行补做。该框架被称为迭代参数对齐，可以自然地应用于跨积 Sylos 的设置。具有以下特点：1. 每个参与者都有唯一的解决方案，可以在联邦中每个模型都进行全球收敛。2. 可选的早期停止机制，以便在合作学习设置中约束参与者之间的公平。这些特点结合起来，为我们提供了一种灵活的新框架，可以逐步学习来自各个模型在不同数据集上的训练。我们发现，该技术可以与现有方法相比，在多个数据分区上实现竞争性的结果。此外，我们还证明该方法在不同领域（即客户端领域中的分离类）where existing approaches struggle。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptive-Object-Detection-via-Balancing-Between-Self-Training-and-Adversarial-Learning"><a href="#Domain-Adaptive-Object-Detection-via-Balancing-Between-Self-Training-and-Adversarial-Learning" class="headerlink" title="Domain Adaptive Object Detection via Balancing Between Self-Training and Adversarial Learning"></a>Domain Adaptive Object Detection via Balancing Between Self-Training and Adversarial Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04815">http://arxiv.org/abs/2311.04815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Akhtar Munir, Muhammad Haris Khan, M. Saquib Sarfraz, Mohsen Ali</li>
<li>for: 这个研究旨在提高深度学习基于物体探测器的适应能力，尤其是在面对新目标领域时，该领域具有明显的物体和背景变化。</li>
<li>methods: 本研究使用模型的预测不确定性来实现内部平衡和类别平衡。具体来说，我们发展了一种量化预测不确定性的技术，并使用高信任率预测生成 pseudo-label，以便进行自我训练。</li>
<li>results: 我们的方法在五个不同和具有挑战性的适应情况下表现出优秀的成绩，与现有的州��cially-of-the-art方法之间存在明显的差异。<details>
<summary>Abstract</summary>
Deep learning based object detectors struggle generalizing to a new target domain bearing significant variations in object and background. Most current methods align domains by using image or instance-level adversarial feature alignment. This often suffers due to unwanted background and lacks class-specific alignment. A straightforward approach to promote class-level alignment is to use high confidence predictions on unlabeled domain as pseudo-labels. These predictions are often noisy since model is poorly calibrated under domain shift. In this paper, we propose to leverage model's predictive uncertainty to strike the right balance between adversarial feature alignment and class-level alignment. We develop a technique to quantify predictive uncertainty on class assignments and bounding-box predictions. Model predictions with low uncertainty are used to generate pseudo-labels for self-training, whereas the ones with higher uncertainty are used to generate tiles for adversarial feature alignment. This synergy between tiling around uncertain object regions and generating pseudo-labels from highly certain object regions allows capturing both image and instance-level context during the model adaptation. We report thorough ablation study to reveal the impact of different components in our approach. Results on five diverse and challenging adaptation scenarios show that our approach outperforms existing state-of-the-art methods with noticeable margins.
</details>
<details>
<summary>摘要</summary>
深度学习基于对象检测器在新目标领域中一般化是一个问题。大多数当前方法使用图像或实例水平的对抗特征偏移来对领域进行Alignment。这经常受到背景的不良影响并且缺乏类别偏移。在本文中，我们提出使用高确定性预测作为 pseudo-labels来促进类别偏移。我们开发了一种技术来评估预测的不确定性，并使用低不确定性预测生成 pseudo-labels，而高不确定性预测则用于生成对抗特征偏移。这种同时使用瓷砾在不确定性范围内和生成高确定性预测作为 pseudo-labels的方法，允许在模型适应过程中捕捉图像和实例级上下文。我们进行了完整的ablation研究，以便了解不同组件在我们的方法中的影响。我们在五种多样化和挑战性的适应场景中进行了实验，并发现我们的方法可以与现有状态的方法相比，表现出明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Be-Careful-When-Evaluating-Explanations-Regarding-Ground-Truth"><a href="#Be-Careful-When-Evaluating-Explanations-Regarding-Ground-Truth" class="headerlink" title="Be Careful When Evaluating Explanations Regarding Ground Truth"></a>Be Careful When Evaluating Explanations Regarding Ground Truth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04813">http://arxiv.org/abs/2311.04813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mi2datalab/be-careful-evaluating-explanations">https://github.com/mi2datalab/be-careful-evaluating-explanations</a></li>
<li>paper_authors: Hubert Baniecki, Maciej Chrabaszcz, Andreas Holzinger, Bastian Pfeifer, Anna Saranti, Przemyslaw Biecek</li>
<li>for: 本研究旨在评估深度学习模型的安全性，尤其是在医疗影像分析和机器人应用中，通过评估模型与人类理解之间的匹配程度。</li>
<li>methods: 本研究提出了一种框架，用于同时评估深度学习模型和解释方法的稳定性，并使用了精度调整程序来（不）对模型与真实情况之间的匹配进行调整。</li>
<li>results: 实验结果表明，视transformer模型和相关的解释方法在不同的模型架构和post-hoc本地解释方法下的Robustness具有一定的潜在攻击风险。<details>
<summary>Abstract</summary>
Evaluating explanations of image classifiers regarding ground truth, e.g. segmentation masks defined by human perception, primarily evaluates the quality of the models under consideration rather than the explanation methods themselves. Driven by this observation, we propose a framework for $\textit{jointly}$ evaluating the robustness of safety-critical systems that $\textit{combine}$ a deep neural network with an explanation method. These are increasingly used in real-world applications like medical image analysis or robotics. We introduce a fine-tuning procedure to (mis)align model$\unicode{x2013}$explanation pipelines with ground truth and use it to quantify the potential discrepancy between worst and best-case scenarios of human alignment. Experiments across various model architectures and post-hoc local interpretation methods provide insights into the robustness of vision transformers and the overall vulnerability of such AI systems to potential adversarial attacks.
</details>
<details>
<summary>摘要</summary>
evaluating explanations of image classifiers regarding ground truth, e.g. segmentation masks defined by human perception, primarily evaluates the quality of the models under consideration rather than the explanation methods themselves. driven by this observation, we propose a framework for $\textit{jointly}$ evaluating the robustness of safety-critical systems that $\textit{combine}$ a deep neural network with an explanation method. these are increasingly used in real-world applications like medical image analysis or robotics. we introduce a fine-tuning procedure to (mis)align model$\unicode{x2013}$explanation pipelines with ground truth and use it to quantify the potential discrepancy between worst and best-case scenarios of human alignment. experiments across various model architectures and post-hoc local interpretation methods provide insights into the robustness of vision transformers and the overall vulnerability of such AI systems to potential adversarial attacks.Here's the text with some notes on the translation:* "ground truth" is translated as "人类理解的标准" (rénxìng lǐjiě de biānwù)* "segmentation masks" is translated as "分割面积" (fēnzhì miànjì)* "deep neural network" is translated as "深度神经网络" (shēngrán jiānxīngwǎng)* "explanation method" is translated as "解释方法" (jiějie fāngfa)* "safety-critical systems" is translated as "安全关键系统" (ānquè guānjī systems)* "jointly" is translated as "共同" (gòngdòng)* "worst-case scenarios" is translated as "最坏情况" (zuì huò qíngkē)* "best-case scenarios" is translated as "最好情况" (zuì hǎo qíngkē)* "human alignment" is translated as "人类对齐" (rénxìng duìqí)Note that the translation of "jointly" as "共同" is a bit more formal than the original English text, but it accurately conveys the meaning of the phrase. Additionally, the translation of "worst-case scenarios" and "best-case scenarios" as "最坏情况" and "最好情况" is a bit more idiomatic than the original English text, but it accurately conveys the meaning of the phrases in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Image-Based-Virtual-Try-On-A-Survey"><a href="#Image-Based-Virtual-Try-On-A-Survey" class="headerlink" title="Image-Based Virtual Try-On: A Survey"></a>Image-Based Virtual Try-On: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04811">http://arxiv.org/abs/2311.04811</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/little-misfit/survey-of-virtual-try-on">https://github.com/little-misfit/survey-of-virtual-try-on</a></li>
<li>paper_authors: Dan Song, Xuanpu Zhang, Juan Zhou, Weizhi Nie, Ruofeng Tong, An-An Liu</li>
<li>For: This paper aims to provide a comprehensive analysis of state-of-the-art techniques and methodologies in image-based virtual try-on, and to identify key trends and future research directions in this field.* Methods: The paper uses a pipeline architecture that includes person representation, try-on indication, clothing warping, and try-on stage. The authors also propose a new semantic criteria using CLIP and evaluate representative methods with uniformly implemented evaluation metrics on the same dataset.* Results: The paper provides a comprehensive overview of the current state of image-based virtual try-on research, including both quantitative and qualitative evaluations of current open-source methods. The authors also demonstrate the potential of large-scale models on this task by fine-tuning a recent image generation model (PBE) using ControlNet.Here is the text in Simplified Chinese:* For: 这篇论文目的是提供图像基于虚拟试穿的全面分析，并预测这个领域的未来研究趋势。* Methods: 该论文使用管道式架构，包括人体表示、试穿指示、衣服折叠和试穿阶段。作者还提出了基于CLIP的新的Semantic criteria。* Results: 论文提供了图像基于虚拟试穿的现状报告，包括现有开源方法的量化和质量评估。作者还示出了大规模模型在这个任务上的潜在潜力。<details>
<summary>Abstract</summary>
Image-based virtual try-on aims to synthesize a naturally dressed person image with a clothing image, which revolutionizes online shopping and inspires related topics within image generation, showing both research significance and commercial potentials. However, there is a great gap between current research progress and commercial applications and an absence of comprehensive overview towards this field to accelerate the development. In this survey, we provide a comprehensive analysis of the state-of-the-art techniques and methodologies in aspects of pipeline architecture, person representation and key modules such as try-on indication, clothing warping and try-on stage. We propose a new semantic criteria with CLIP, and evaluate representative methods with uniformly implemented evaluation metrics on the same dataset. In addition to quantitative and qualitative evaluation of current open-source methods, we also utilize ControlNet to fine-tune a recent large image generation model (PBE) to show future potentials of large-scale models on image-based virtual try-on task. Finally, unresolved issues are revealed and future research directions are prospected to identify key trends and inspire further exploration. The uniformly implemented evaluation metrics, dataset and collected methods will be made public available at https://github.com/little-misfit/Survey-Of-Virtual-Try-On.
</details>
<details>
<summary>摘要</summary>
图像基于虚拟试穿涉及合成一个自然地穿着人像与衣服图像，这种技术革新了在线购物和相关领域的图像生成，具有研究重要性和商业潜力。然而，目前研究进步和商业应用之间存在巨大的差距，而且对这一领域的全面概述缺乏，以便加速发展。在这份调查中，我们提供了图像基于虚拟试穿领域的全面分析，包括管道架构、人体表示和关键模块such as 试穿指示、衣服扭曲和试穿阶段。我们还提出了一种新的semantic标准，并使用CLIP进行评估代表方法。此外，我们还使用ControlNet来精度调整最近一个大型图像生成模型（PBE），以示未来大型模型在图像基于虚拟试穿任务中的潜力。最后，我们揭示了未解的问题和未来研究方向，以便识别关键趋势和激发更多的探索。我们 uniformly 实施的评估指标、数据集和收集的方法将在https://github.com/little-misfit/Survey-Of-Virtual-Try-On 上公开。
</details></li>
</ul>
<hr>
<h2 id="VioLA-Aligning-Videos-to-2D-LiDAR-Scans"><a href="#VioLA-Aligning-Videos-to-2D-LiDAR-Scans" class="headerlink" title="VioLA: Aligning Videos to 2D LiDAR Scans"></a>VioLA: Aligning Videos to 2D LiDAR Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04783">http://arxiv.org/abs/2311.04783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun-Jee Chao, Selim Engin, Nikhil Chavan-Dafle, Bhoram Lee, Volkan Isler</li>
<li>for: 将视频Sequence align到2D LiDAR扫描图中的环境</li>
<li>methods: 引入VioLA方法，首先从图像序列中建立本地场景的semantic map，然后从图像序列中提取高度为固定值的点进行注册</li>
<li>results: 通过使用 pré-trained text-to-image填充模型和深度完成模型来填充缺失的场景内容，提高了pose注册性能，最多提高20%<details>
<summary>Abstract</summary>
We study the problem of aligning a video that captures a local portion of an environment to the 2D LiDAR scan of the entire environment. We introduce a method (VioLA) that starts with building a semantic map of the local scene from the image sequence, then extracts points at a fixed height for registering to the LiDAR map. Due to reconstruction errors or partial coverage of the camera scan, the reconstructed semantic map may not contain sufficient information for registration. To address this problem, VioLA makes use of a pre-trained text-to-image inpainting model paired with a depth completion model for filling in the missing scene content in a geometrically consistent fashion to support pose registration. We evaluate VioLA on two real-world RGB-D benchmarks, as well as a self-captured dataset of a large office scene. Notably, our proposed scene completion module improves the pose registration performance by up to 20%.
</details>
<details>
<summary>摘要</summary>
我们研究将视频Capture的本地环境与2D LiDAR扫描的全景环境进行对应的问题。我们提出了一种方法（VioLA），它首先从图像序列中建立了本地场景的semantic map，然后提取了一个固定高度的点进行与LiDAR地图进行对应。由于恢复错误或相机扫描的部分覆盖，可能导致重建的semantic maplack sufficient information for registration。为解决这个问题，VioLA利用了一个预训练的文本-图像填充模型和一个depth completion模型来填充缺失的场景内容，以保持准确的姿态注册。我们在两个实际的RGB-D标准benchmark上以及一个自拍摄的大办公室场景中进行了评估，并观察到我们提议的场景完成模块可以提高姿态注册性能达20%。
</details></li>
</ul>
<hr>
<h2 id="Lidar-Annotation-Is-All-You-Need"><a href="#Lidar-Annotation-Is-All-You-Need" class="headerlink" title="Lidar Annotation Is All You Need"></a>Lidar Annotation Is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04777">http://arxiv.org/abs/2311.04777</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evocargo/lidar-annotation-is-all-you-need">https://github.com/evocargo/lidar-annotation-is-all-you-need</a></li>
<li>paper_authors: Dinar Sharafutdinov, Stanislav Kuskov, Saian Protasov, Alexey Voropaev<br>for: 这 paper 的目的是提高图像分割的效率，使用 convolutional neural network 在多感器设置下进行图像分割。methods: 该方法使用 lidar 精度测量点云，并将其直接用于图像分割模型的训练。该方法还使用 masked loss 来处理稀疏的地面数据。results: 实验表明，该方法可以在多个数据集上实现相似的性能，而不需要大量的注释数据。该方法可以减少注释负担，并允许在图像分割模型的训练中混合不同类型的地面数据。<details>
<summary>Abstract</summary>
In recent years, computer vision has transformed fields such as medical imaging, object recognition, and geospatial analytics. One of the fundamental tasks in computer vision is semantic image segmentation, which is vital for precise object delineation. Autonomous driving represents one of the key areas where computer vision algorithms are applied. The task of road surface segmentation is crucial in self-driving systems, but it requires a labor-intensive annotation process in several data domains. The work described in this paper aims to improve the efficiency of image segmentation using a convolutional neural network in a multi-sensor setup. This approach leverages lidar (Light Detection and Ranging) annotations to directly train image segmentation models on RGB images. Lidar supplements the images by emitting laser pulses and measuring reflections to provide depth information. However, lidar's sparse point clouds often create difficulties for accurate object segmentation. Segmentation of point clouds requires time-consuming preliminary data preparation and a large amount of computational resources. The key innovation of our approach is the masked loss, addressing sparse ground-truth masks from point clouds. By calculating loss exclusively where lidar points exist, the model learns road segmentation on images by using lidar points as ground truth. This approach allows for blending of different ground-truth data types during model training. Experimental validation of the approach on benchmark datasets shows comparable performance to a high-quality image segmentation model. Incorporating lidar reduces the load on annotations and enables training of image-segmentation models without loss of segmentation quality. The methodology is tested on diverse datasets, both publicly available and proprietary. The strengths and weaknesses of the proposed method are also discussed in the paper.
</details>
<details>
<summary>摘要</summary>
Recently, computer vision has revolutionized fields such as medical imaging, object recognition, and geospatial analytics. One of the fundamental tasks in computer vision is semantic image segmentation, which is crucial for precise object delineation. Autonomous driving is one of the key areas where computer vision algorithms are applied, and the task of road surface segmentation is crucial in self-driving systems. However, this task requires a labor-intensive annotation process in several data domains.The work described in this paper aims to improve the efficiency of image segmentation using a convolutional neural network in a multi-sensor setup. This approach leverages lidar (Light Detection and Ranging) annotations to directly train image segmentation models on RGB images. Lidar supplements the images by emitting laser pulses and measuring reflections to provide depth information. However, lidar's sparse point clouds often create difficulties for accurate object segmentation.The key innovation of our approach is the masked loss, which addresses sparse ground-truth masks from point clouds. By calculating loss exclusively where lidar points exist, the model learns road segmentation on images by using lidar points as ground truth. This approach allows for blending of different ground-truth data types during model training. Experimental validation of the approach on benchmark datasets shows comparable performance to a high-quality image segmentation model. Incorporating lidar reduces the load on annotations and enables training of image-segmentation models without loss of segmentation quality. The methodology is tested on diverse datasets, both publicly available and proprietary. The strengths and weaknesses of the proposed method are also discussed in the paper.
</details></li>
</ul>
<hr>
<h2 id="GCS-ICHNet-Assessment-of-Intracerebral-Hemorrhage-Prognosis-using-Self-Attention-with-Domain-Knowledge-Integration"><a href="#GCS-ICHNet-Assessment-of-Intracerebral-Hemorrhage-Prognosis-using-Self-Attention-with-Domain-Knowledge-Integration" class="headerlink" title="GCS-ICHNet: Assessment of Intracerebral Hemorrhage Prognosis using Self-Attention with Domain Knowledge Integration"></a>GCS-ICHNet: Assessment of Intracerebral Hemorrhage Prognosis using Self-Attention with Domain Knowledge Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04772">http://arxiv.org/abs/2311.04772</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Windbelll/Prognosis-analysis-of-cerebral-hemorrhage">https://github.com/Windbelll/Prognosis-analysis-of-cerebral-hemorrhage</a></li>
<li>paper_authors: Xuhao Shan, Xinyang Li, Ruiquan Ge, Shibin Wu, Ahmed Elazab, Jichao Zhu, Lingyan Zhang, Gangyong Jia, Qingying Xiao, Xiang Wan, Changmiao Wang</li>
<li>for: 预测急性脑出血（ICH）的诊断和治疗结果，提高患者生存率。</li>
<li>methods: 利用多Modal脑CT图像数据和格拉斯哥昏迷分数（GCS）来提高ICH诊断，使用trasnformer基本的融合模块进行评估。</li>
<li>results: GCS-ICHNet实现了81.03%的敏感性和91.59%的特异性，超过了平均临床医生和其他当前状态的方法。<details>
<summary>Abstract</summary>
Intracerebral Hemorrhage (ICH) is a severe condition resulting from damaged brain blood vessel ruptures, often leading to complications and fatalities. Timely and accurate prognosis and management are essential due to its high mortality rate. However, conventional methods heavily rely on subjective clinician expertise, which can lead to inaccurate diagnoses and delays in treatment. Artificial intelligence (AI) models have been explored to assist clinicians, but many prior studies focused on model modification without considering domain knowledge. This paper introduces a novel deep learning algorithm, GCS-ICHNet, which integrates multimodal brain CT image data and the Glasgow Coma Scale (GCS) score to improve ICH prognosis. The algorithm utilizes a transformer-based fusion module for assessment. GCS-ICHNet demonstrates high sensitivity 81.03% and specificity 91.59%, outperforming average clinicians and other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Intracerebral Hemorrhage (ICH) 是一种严重的疾病，由于脑血管受损而导致血液泄露，可能会导致严重的后果和死亡。因此，有效和准确的诊断和治疗是非常重要的，因为其mortality rate 很高。然而，传统的方法很多都是基于专业知识的，可能会导致不准确的诊断和治疗延迟。人工智能（AI）模型已经被探讨以帮助临床医生，但许多先前的研究都是对模型进行修改而不考虑域知识。本文介绍了一种新的深度学习算法，GCS-ICHNet，它通过结合多Modal脑CT图像数据和格拉斯哥昏迷scale（GCS）分数来改善ICH的诊断。该算法使用 transformer-based 混合模块进行评估。GCS-ICHNet 在敏感性和特点上都有出色的表现，高达 81.03% 和 91.59%，超过了平均的临床医生和其他现有的方法。
</details></li>
</ul>
<hr>
<h2 id="An-attention-based-deep-learning-network-for-predicting-Platinum-resistance-in-ovarian-cancer"><a href="#An-attention-based-deep-learning-network-for-predicting-Platinum-resistance-in-ovarian-cancer" class="headerlink" title="An attention-based deep learning network for predicting Platinum resistance in ovarian cancer"></a>An attention-based deep learning network for predicting Platinum resistance in ovarian cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04769">http://arxiv.org/abs/2311.04769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoming Zhuang, Beibei Li, Jingtong Ma, Patrice Monkam, Shouliang Qi, Wei Qian, Dianning He<br>for: 这项研究的目的是提出一种基于深度学习的方法，用于判断患有高等级胸膜癌的患者是否 platinum 抵抗性。methods: 该研究使用了289名高等级胸膜癌患者的数据，并建立了一个结合了压缩块（SE Block）和空间 пирамид Pooling层（SPPLayer）的Dense Convolutional Network（DenseNet）模型。利用多Modal PET&#x2F;CT 图像数据进行预测患者 platinum 抵抗性。results: 经五次交叠验证，SE-SPP-DenseNet 模型在预测患者 platinum 抵抗性方面 achieved a high accuracy rate和抛物线曲线（AUC）值，分别为92.6%和0.93。通过进行剥离实验和单Modal 数据验证，证明了将 SE Block 和 SPPLayer 添加到深度学习模型中，并考虑多Modal 数据的重要性。<details>
<summary>Abstract</summary>
Background: Ovarian cancer is among the three most frequent gynecologic cancers globally. High-grade serous ovarian cancer (HGSOC) is the most common and aggressive histological type. Guided treatment for HGSOC typically involves platinum-based combination chemotherapy, necessitating an assessment of whether the patient is platinum-resistant. The purpose of this study is to propose a deep learning-based method to determine whether a patient is platinum-resistant using multimodal positron emission tomography/computed tomography (PET/CT) images. Methods: 289 patients with HGSOC were included in this study. An end-to-end SE-SPP-DenseNet model was built by adding Squeeze-Excitation Block (SE Block) and Spatial Pyramid Pooling Layer (SPPLayer) to Dense Convolutional Network (DenseNet). Multimodal data from PET/CT images of the regions of interest (ROI) were used to predict platinum resistance in patients. Results: Through five-fold cross-validation, SE-SPP-DenseNet achieved a high accuracy rate and an area under the curve (AUC) in predicting platinum resistance in patients, which were 92.6% and 0.93, respectively. The importance of incorporating SE Block and SPPLayer into the deep learning model, and considering multimodal data was substantiated by carrying out ablation studies and experiments with single modality data. Conclusions: The obtained classification results indicate that our proposed deep learning framework performs better in predicting platinum resistance in patients, which can help gynecologists make better treatment decisions. Keywords: PET/CT, CNN, SE Block, SPP Layer, Platinum resistance, Ovarian cancer
</details>
<details>
<summary>摘要</summary>
背景：子宫癌是全球三大妇科癌种之一，高等级膜蛋白癌（HGSOC）是最常见并且最严重的 histological 型。对 HGSOC 患者的治疗通常包括钍基化学疗法，需要评估患者是否为钍耐 resistance。本研究的目的是提出一种基于深度学习的方法，使用多Modal positron emission tomography/computed tomography（PET/CT）图像来判断患者是否为钍耐 resistance。方法：本研究包括289名 HGSOC 患者。我们建立了一个结合 Squeeze-Excitation Block（SE Block）和 Spatial Pyramid Pooling Layer（SPPLayer）的 Dense Convolutional Network（DenseNet）模型。使用 ROI 的多Modal PET/CT 图像来预测患者是否为钍耐 resistance。结果：通过五次交叉验证，SE-SPP-DenseNet 模型在预测患者是否为钍耐 resistance 中达到了高精度率和报春分（AUC）的好Result，分别为92.6%和0.93。我们通过进行剖除研究和单模态数据实验来证明，将 SE Block 和 SPPLayer 添加到深度学习模型中，以及考虑多Modal数据的重要性。结论：我们的提出的深度学习框架在预测患者是否为钍耐 resistance 中表现更好，可以帮助妇科医生作出更好的治疗决策。关键词：PET/CT, CNN, SE Block, SPP Layer, 钍耐 resistance, 子宫癌
</details></li>
</ul>
<hr>
<h2 id="DualTalker-A-Cross-Modal-Dual-Learning-Approach-for-Speech-Driven-3D-Facial-Animation"><a href="#DualTalker-A-Cross-Modal-Dual-Learning-Approach-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation"></a>DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04766">http://arxiv.org/abs/2311.04766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guinan Su, Yanwu Yang, Zhifeng Li</li>
<li>for: 这个研究旨在提高音频驱动3D面部动画的精度和效率，特别是在虚拟现实、游戏和视频会议等应用中。</li>
<li>methods: 该研究提出了一种交叉模式双学习框架，称为DualTalker，以提高数据使用效率并关联跨模态关系。该框架共同受过主任务（音频驱动面部动画）和其双任务（详细说话）的联合训练，并共享音频&#x2F;运动编码器组件。</li>
<li>results: 经过广泛的实验和一项感知用户研究，我们展示了我们的方法在VOCA和BIWI数据集上的较好的表现，both qualitatively和quantitatively。我们的代码和视频示例已经在<a target="_blank" rel="noopener" href="https://github.com/sabrina-su/iadf.git%E4%B8%AD%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/sabrina-su/iadf.git中提供。</a><details>
<summary>Abstract</summary>
In recent years, audio-driven 3D facial animation has gained significant attention, particularly in applications such as virtual reality, gaming, and video conferencing. However, accurately modeling the intricate and subtle dynamics of facial expressions remains a challenge. Most existing studies approach the facial animation task as a single regression problem, which often fail to capture the intrinsic inter-modal relationship between speech signals and 3D facial animation and overlook their inherent consistency. Moreover, due to the limited availability of 3D-audio-visual datasets, approaches learning with small-size samples have poor generalizability that decreases the performance. To address these issues, in this study, we propose a cross-modal dual-learning framework, termed DualTalker, aiming at improving data usage efficiency as well as relating cross-modal dependencies. The framework is trained jointly with the primary task (audio-driven facial animation) and its dual task (lip reading) and shares common audio/motion encoder components. Our joint training framework facilitates more efficient data usage by leveraging information from both tasks and explicitly capitalizing on the complementary relationship between facial motion and audio to improve performance. Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigate the potential over-smoothing underlying the cross-modal complementary representations, enhancing the mapping of subtle facial expression dynamics. Through extensive experiments and a perceptual user study conducted on the VOCA and BIWI datasets, we demonstrate that our approach outperforms current state-of-the-art methods both qualitatively and quantitatively. We have made our code and video demonstrations available at https://github.com/sabrina-su/iadf.git.
</details>
<details>
<summary>摘要</summary>
Recently, audio-driven 3D facial animation has gained significant attention, especially in virtual reality, gaming, and video conferencing applications. However, accurately modeling the intricate and subtle dynamics of facial expressions remains a challenge. Most existing studies treat the facial animation task as a single regression problem, which often fails to capture the intrinsic inter-modal relationship between speech signals and 3D facial animation, and overlooks their inherent consistency. Moreover, due to the limited availability of 3D-audio-visual datasets, approaches learning with small-size samples have poor generalizability, which decreases performance.To address these issues, in this study, we propose a cross-modal dual-learning framework, termed DualTalker, aiming at improving data usage efficiency and relating cross-modal dependencies. The framework is trained jointly with the primary task (audio-driven facial animation) and its dual task (lip reading) and shares common audio/motion encoder components. Our joint training framework leverages information from both tasks and explicitly capitalizes on the complementary relationship between facial motion and audio to improve performance. Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigate the potential over-smoothing underlying the cross-modal complementary representations, enhancing the mapping of subtle facial expression dynamics.Through extensive experiments and a perceptual user study conducted on the VOCA and BIWI datasets, we demonstrate that our approach outperforms current state-of-the-art methods both qualitatively and quantitatively. Our code and video demonstrations are available at <https://github.com/sabrina-su/iadf.git>.
</details></li>
</ul>
<hr>
<h2 id="Social-Motion-Prediction-with-Cognitive-Hierarchies"><a href="#Social-Motion-Prediction-with-Cognitive-Hierarchies" class="headerlink" title="Social Motion Prediction with Cognitive Hierarchies"></a>Social Motion Prediction with Cognitive Hierarchies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04726">http://arxiv.org/abs/2311.04726</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Walter0807/Social-CH">https://github.com/Walter0807/Social-CH</a></li>
<li>paper_authors: Wentao Zhu, Jason Qin, Yuke Lou, Hang Ye, Xiaoxuan Ma, Hai Ci, Yizhou Wang</li>
<li>for: 这个研究的目的是复制人类在预测他人行为方面的能力，通过解决社交动作预测问题。</li>
<li>methods: 这个研究使用了一个新的比较器、一种新的形式化、以及基于认知的框架。他们还使用了行为做副本和生成对抗学习来提高学习效率和通用性。</li>
<li>results: 研究人员通过实施了一种新的3D多人动作数据集，并通过对比较器和生成对抗学习来验证数据集和方法的有效性。<details>
<summary>Abstract</summary>
Humans exhibit a remarkable capacity for anticipating the actions of others and planning their own actions accordingly. In this study, we strive to replicate this ability by addressing the social motion prediction problem. We introduce a new benchmark, a novel formulation, and a cognition-inspired framework. We present Wusi, a 3D multi-person motion dataset under the context of team sports, which features intense and strategic human interactions and diverse pose distributions. By reformulating the problem from a multi-agent reinforcement learning perspective, we incorporate behavioral cloning and generative adversarial imitation learning to boost learning efficiency and generalization. Furthermore, we take into account the cognitive aspects of the human social action planning process and develop a cognitive hierarchy framework to predict strategic human social interactions. We conduct comprehensive experiments to validate the effectiveness of our proposed dataset and approach. Code and data are available at https://walter0807.github.io/Social-CH/.
</details>
<details>
<summary>摘要</summary>
人类具有惊人的其他人行为预测能力和自己行为规划能力。在这项研究中，我们努力复制这种能力，解决社会动作预测问题。我们引入了新的标准集，一种新的形ulation，以及一个基于认知的框架。我们提供了一个3D多人动作数据集，名为Wusi，该数据集在体育赛事中展示了人类之间的激烈和策略性交互，以及多种姿态分布。我们将问题转换为多智能 reinforcement learning 视角，并应用行为做样和生成对抗学习来提高学习效率和泛化能力。此外，我们考虑了人类社会行为规划过程中的认知方面，并开发了认知层次框架来预测人类社会互动的策略。我们进行了全面的实验来验证我们的提posed dataset和方法的有效性。代码和数据可以在https://walter0807.github.io/Social-CH/获取。
</details></li>
</ul>
<hr>
<h2 id="Training-CLIP-models-on-Data-from-Scientific-Papers"><a href="#Training-CLIP-models-on-Data-from-Scientific-Papers" class="headerlink" title="Training CLIP models on Data from Scientific Papers"></a>Training CLIP models on Data from Scientific Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04711">http://arxiv.org/abs/2311.04711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nopperl/clip_arxiv_pmc">https://github.com/nopperl/clip_arxiv_pmc</a></li>
<li>paper_authors: Calvin Metzger</li>
<li>for: 这篇论文旨在检验 CLIP 模型是否通过使用高质量数据来提高总体性能。</li>
<li>methods: 该论文使用 arXiv 和 PubMed Central 搜索引擎提取文本图像数据，并在小规模 CLIP 模型（ViT B&#x2F;32）上进行实验。</li>
<li>results: 实验结果表明，使用高质量数据源可以提高 CLIP 模型的性能，但提高的程度只是有 moderate。这表明使用这些数据源来训练大规模 CLIP 模型是一个值得进行研究的方向。<details>
<summary>Abstract</summary>
Contrastive Language-Image Pretraining (CLIP) models are able to capture the semantic relationship of images and texts and have enabled a wide range of applications, from image retrieval to classification. These models are trained with datasets extracted from web crawls, which are of large quantity but limited quality. This paper explores whether limited amounts higher quality data in a specific domain improve the general performance of CLIP models. To this purpose, we extract text-image data from scientific papers hosted in the arXiv and PubMed Central repositories. Experiments on small-scale CLIP models (ViT B/32) show that model performance increases on average, but only moderately. This result indicates that using the data sources considered in the paper to train large-scale CLIP models is a worthwile research direction.
</details>
<details>
<summary>摘要</summary>
对比语言-图像预训（CLIP）模型可以捕捉图像和文本之间的 semantic 关系，并实现了许多应用，从图像检索到分类。这些模型通常通过互联网爬虫获取数据进行训练，这些数据的量很大，但质量有限。这篇文章探讨了是否有限量但高质量数据在特定领域提高CLIP模型的总性能。为此，我们从arXiv和PubMed Central数据库中提取了文本-图像数据。实验结果表明，使用这些数据来训练小规模CLIP模型（ViT B/32）时，模型的性能平均提高，但只有moderate。这结果表明，使用文章中所考虑的数据来训练大规模CLIP模型是一个有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="3D-Pose-Estimation-of-Tomato-Peduncle-Nodes-using-Deep-Keypoint-Detection-and-Point-Cloud"><a href="#3D-Pose-Estimation-of-Tomato-Peduncle-Nodes-using-Deep-Keypoint-Detection-and-Point-Cloud" class="headerlink" title="3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and Point Cloud"></a>3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and Point Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04699">http://arxiv.org/abs/2311.04699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianchao Ci, Xin Wang, David Rapado-Rincón, Akshay K. Burusa, Gert Kootstra</li>
<li>For: 本研究旨在提供一种基于关键点检测的RGB-D相机数据的方法，用于自动探测护叶节点，以便在绿色家庭中自动探测 Tomatoes。* Methods: 本方法使用了RGB-D相机数据，通过检测颜色图像中的四个骨干特征点，并将其与3D点云信息集成，以确定护叶节点的3D姿态。* Results: 研究结果表明，该方法具有高精度的物体检测能力（<a href="mailto:&#x41;&#80;&#x40;&#48;&#46;&#x35;">&#x41;&#80;&#x40;&#48;&#46;&#x35;</a>&#x3D;0.96）、高精度的关键点检测率（<a href="mailto:&#x50;&#68;&#74;&#64;&#48;&#46;&#50;">&#x50;&#68;&#74;&#64;&#48;&#46;&#50;</a>&#x3D;94.31%）和3D姿态估计精度（MAE&#x3D;11.38o和9.93o）。此外，该方法还可以快速响应视点变化。<details>
<summary>Abstract</summary>
Greenhouse production of fruits and vegetables in developed countries is challenged by labor 12 scarcity and high labor costs. Robots offer a good solution for sustainable and cost-effective 13 production. Acquiring accurate spatial information about relevant plant parts is vital for 14 successful robot operation. Robot perception in greenhouses is challenging due to variations in 15 plant appearance, viewpoints, and illumination. This paper proposes a keypoint-detection-based 16 method using data from an RGB-D camera to estimate the 3D pose of peduncle nodes, which 17 provides essential information to harvest the tomato bunches. 18 19 Specifically, this paper proposes a method that detects four anatomical landmarks in the color 20 image and then integrates 3D point-cloud information to determine the 3D pose. A 21 comprehensive evaluation was conducted in a commercial greenhouse to gain insight into the 22 performance of different parts of the method. The results showed: (1) high accuracy in object 23 detection, achieving an Average Precision (AP) of AP@0.5=0.96; (2) an average Percentage of 24 Detected Joints (PDJ) of the keypoints of PhDJ@0.2=94.31%; and (3) 3D pose estimation 25 accuracy with mean absolute errors (MAE) of 11.38o and 9.93o for the relative upper and lower 26 angles between the peduncle and main stem, respectively. Furthermore, the capability to handle 27 variations in viewpoint was investigated, demonstrating the method was robust to view changes. 28 However, canonical and higher views resulted in slightly higher performance compared to other 29 views. Although tomato was selected as a use case, the proposed method is also applicable to 30 other greenhouse crops like pepper.
</details>
<details>
<summary>摘要</summary>
developed countries的绿色房production of fruits and vegetables面临劳动力短缺和高劳动成本的挑战。Robots可以提供可持续和成本效果的解决方案。获取有关相关植物部分的准确空间信息是成功机器人运行的关键。在绿色房中机器人识别是由于植物外观、视角和照明变化而困难。这篇论文提出了基于特征点探测的方法，使用RGB-D摄像头数据来估算 Tomatoes的3D姿态。 Specifically, this paper proposes a method that detects four anatomical landmarks in the color image and then integrates 3D point-cloud information to determine the 3D pose. A comprehensive evaluation was conducted in a commercial greenhouse to gain insight into the performance of different parts of the method. The results showed: (1) high accuracy in object detection, achieving an Average Precision (AP) of AP@0.5=0.96; (2) an average Percentage of Detected Joints (PDJ) of the keypoints of PhDJ@0.2=94.31%; and (3) 3D pose estimation accuracy with mean absolute errors (MAE) of 11.38° and 9.93° for the relative upper and lower angles between the peduncle and main stem, respectively. Furthermore, the capability to handle variations in viewpoint was investigated, demonstrating the method was robust to view changes. However, canonical and higher views resulted in slightly higher performance compared to other views. Although tomato was selected as a use case, the proposed method is also applicable to other greenhouse crops like pepper.
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-cross-model-learning-in-high-content-screening"><a href="#Weakly-supervised-cross-model-learning-in-high-content-screening" class="headerlink" title="Weakly supervised cross-model learning in high-content screening"></a>Weakly supervised cross-model learning in high-content screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04678">http://arxiv.org/abs/2311.04678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Watkinson Gabriel, Cohen Ethan, Bourriez Nicolas, Bendidi Ihab, Bollot Guillaume, Genovesio Auguste</li>
<li>for: 本研究旨在探索如何在药物搜索中连接不同数据类型的数据。</li>
<li>methods: 我们提出了一种新的方法，利用弱监督和跨站复制在高内容检测中使用CLIP建立跨模态表示。</li>
<li>results: 我们的方法可以学习更好的表示，减轻批处理效应，并且对JUMP-CP数据集进行了有效的预处理，从85TB减少到7TB，保留了所有干扰和大多数信息内容。<details>
<summary>Abstract</summary>
With the surge in available data from various modalities, there is a growing need to bridge the gap between different data types. In this work, we introduce a novel approach to learn cross-modal representations between image data and molecular representations for drug discovery. We propose EMM and IMM, two innovative loss functions built on top of CLIP that leverage weak supervision and cross sites replicates in High-Content Screening. Evaluating our model against known baseline on cross-modal retrieval, we show that our proposed approach allows to learn better representations and mitigate batch effect. In addition, we also present a preprocessing method for the JUMP-CP dataset that effectively reduce the required space from 85Tb to a mere usable 7Tb size, still retaining all perturbations and most of the information content.
</details>
<details>
<summary>摘要</summary>
随着不同数据模式之间的数据量的增加，需要桥接这些数据模式之间的 gap 变得更加重要。在这种工作中，我们介绍了一种新的方法，用于从图像数据和分子表示之间学习 crossed-modal 表示。我们提出了两种创新的损失函数EMM和IMM，基于 CLIP 的上下文，利用弱监督和跨站复制在高内容检测中。我们对知道的基准进行跨Modal 检索，并显示了我们提议的方法可以学习更好的表示，并减轻批处理效应。此外，我们还提出了对 JUMP-CP 数据集的预处理方法，可以有效地将数据减少到可用 7Tb 大小，保留所有干扰和大多数信息内容。
</details></li>
</ul>
<hr>
<h2 id="Lightweight-Diffusion-Models-with-Distillation-Based-Block-Neural-Architecture-Search"><a href="#Lightweight-Diffusion-Models-with-Distillation-Based-Block-Neural-Architecture-Search" class="headerlink" title="Lightweight Diffusion Models with Distillation-Based Block Neural Architecture Search"></a>Lightweight Diffusion Models with Distillation-Based Block Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04950">http://arxiv.org/abs/2311.04950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siao Tang, Xin Wang, Hong Chen, Chaoyu Guan, Yansong Tang, Wenwu zhu</li>
<li>for: 提高 diffusion models 的计算效率，使其在多种任务中实现 state-of-the-art 性能。</li>
<li>methods: 提出了一种基于 Diffusion Distillation 的 Block-wise Neural Architecture Search (DiffNAS) 方法，通过自动去除 diffusion models 中的结构冗余来减少计算成本。</li>
<li>results: 实验表明，DiffNAS 可以实现约 50% MACs 和参数减少，并且可以在 latent diffusion models 上实现比 teacher 更好的性能。<details>
<summary>Abstract</summary>
Diffusion models have recently shown remarkable generation ability, achieving state-of-the-art performance in many tasks. However, the high computational cost is still a troubling problem for diffusion models. To tackle this problem, we propose to automatically remove the structural redundancy in diffusion models with our proposed Diffusion Distillation-based Block-wise Neural Architecture Search (DiffNAS). Specifically, given a larger pretrained teacher, we leverage DiffNAS to search for the smallest architecture which achieves on-par or even better performance than the teacher. Considering current diffusion models are based on UNet which naturally has a block-wise structure, we perform neural architecture search independently in each block, which largely reduces the search space. Different from previous block-wise NAS methods, DiffNAS contains a block-wise local search strategy and a retraining strategy with a joint dynamic loss. Concretely, during the search process, we block-wisely select the best subnet to avoid the unfairness brought by the global search strategy used in previous works. When retraining the searched architecture, we adopt a dynamic joint loss to maintain the consistency between supernet training and subnet retraining, which also provides informative objectives for each block and shortens the paths of gradient propagation. We demonstrate this joint loss can effectively improve model performance. We also prove the necessity of the dynamic adjustment of this loss. The experiments show that our method can achieve significant computational reduction, especially on latent diffusion models with about 50% MACs and Parameter reduction.
</details>
<details>
<summary>摘要</summary>
Diffusion 模型最近显示出了很好的生成能力，在许多任务中达到了状态的核心性能。然而，高计算成本仍然是 diffusion 模型的一个困扰问题。为解决这个问题，我们提出了自动Remove diffusion 模型中的结构冗余的方法：Diffusion Distillation-based Block-wise Neural Architecture Search (DiffNAS)。具体来说，我们使用 DiffNAS 在一个更大的预训练老师模型基础上进行搜索，找到与老师模型具有相同或更好的性能的最小架构。由于现有的 diffusion 模型基于 UNet 结构，我们在搜索过程中独立地进行每个块的 neural architecture search，从而大大减少搜索空间。与前一些块基本 NAS 方法不同，DiffNAS 包含块基本选择策略和重新训练策略，并且采用了一种动态共同损失。在搜索过程中，我们会在每个块中选择最佳子网，以避免由全局搜索策略所带来的不公平性。在重新训练搜索出的架构时，我们采用了一种动态共同损失，以保持超网训练和子网重新训练之间的一致性，同时提供了每个块的有用目标。我们证明了这种动态调整的损失是必要的。实验表明，我们的方法可以实现显著的计算减少，特别是在含有约50% MACs和参数的含括 diffusion 模型上。
</details></li>
</ul>
<hr>
<h2 id="VET-Visual-Error-Tomography-for-Point-Cloud-Completion-and-High-Quality-Neural-Rendering"><a href="#VET-Visual-Error-Tomography-for-Point-Cloud-Completion-and-High-Quality-Neural-Rendering" class="headerlink" title="VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering"></a>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04634">http://arxiv.org/abs/2311.04634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lfranke/vet">https://github.com/lfranke/vet</a></li>
<li>paper_authors: Linus Franke, Darius Rückert, Laura Fink, Matthias Innmann, Marc Stamminger</li>
<li>for: 这个论文的目的是提高点云图像的新视图合成质量。</li>
<li>methods: 该论文使用了一种基于神经网络的方法，使用点云代理geometry来检测和修复新视图合成中的缺失或损害。</li>
<li>results: 论文的实验结果表明，该方法可以显著提高点云图像的新视图合成质量，并且可以有效地修复大规模的缺失和细腻结构。同时，该方法的实时渲染速度也得到了改进。<details>
<summary>Abstract</summary>
In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations. Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.
</details>
<details>
<summary>摘要</summary>
Recently, deep neural networks have led to significant advances in novel view synthesis. Many of these methods rely on a coarse proxy geometry obtained through structure from motion algorithms. While small defects in the proxy can be corrected by neural rendering, larger holes or missing parts can still result in distracting artifacts and temporal instability. In this paper, we propose a novel neural-rendering-based approach to detect and fix such deficiencies. We use a point cloud as our proxy, which allows us to easily remove outlier geometry and fill in missing geometry without complicated topological operations. The key components of our approach are:1. A differentiable, blending point-based renderer that can blend out redundant points.2. The concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D regions lacking geometry and spawn novel points accordingly.3. The addition of points as nested environment maps, which allows us to generate high-quality renderings of the surroundings in the same pipeline.Our results show that our approach can significantly improve the quality of a point cloud obtained by structure from motion and increase novel view synthesis quality. In contrast to point growing techniques, our approach can effectively fix large-scale holes and missing thin structures. The rendering quality outperforms state-of-the-art methods, and temporal stability is significantly improved, all while rendering is possible at real-time frame rates.
</details></li>
</ul>
<hr>
<h2 id="General-Framework-to-Evaluate-Unlinkability-in-Biometric-Template-Protection-Systems"><a href="#General-Framework-to-Evaluate-Unlinkability-in-Biometric-Template-Protection-Systems" class="headerlink" title="General Framework to Evaluate Unlinkability in Biometric Template Protection Systems"></a>General Framework to Evaluate Unlinkability in Biometric Template Protection Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04633">http://arxiv.org/abs/2311.04633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Gomez-Barrero, Javier Galbally, Christian Rathgeb, Christoph Busch</li>
<li>for: 保护生物特征数据的隐私问题</li>
<li>methods: 提出了一个新的普适框架来评估生物特征模板的不可识别性</li>
<li>results: 应用于四种现有的生物特征模板保护技术中的一种，并与其他现有的指标进行比较，以显示其优势<details>
<summary>Abstract</summary>
The wide deployment of biometric recognition systems in the last two decades has raised privacy concerns regarding the storage and use of biometric data. As a consequence, the ISO/IEC 24745 international standard on biometric information protection has established two main requirements for protecting biometric templates: irreversibility and unlinkability. Numerous efforts have been directed to the development and analysis of irreversible templates. However, there is still no systematic quantitative manner to analyse the unlinkability of such templates. In this paper we address this shortcoming by proposing a new general framework for the evaluation of biometric templates' unlinkability. To illustrate the potential of the approach, it is applied to assess the unlinkability of four state-of-the-art techniques for biometric template protection: biometric salting, Bloom filters, Homomorphic Encryption and block re-mapping. For the last technique, the proposed framework is compared with other existing metrics to show its advantages.
</details>
<details>
<summary>摘要</summary>
在过去二十年中，生物认证系统的广泛应用已引发了隐私问题，特别是 relate to the storage and use of biometric data。为了解决这问题，国际标准ISO/IEC 24745要求保护生物特征模板的两个主要要求是不可逆和不可关联。虽然有很多努力在发展和分析不可逆模板，但是还没有一个系统性的量化方法来分析不可关联性。在这篇论文中，我们正在解决这一缺点，并提出了一个新的通用框架来评估生物特征模板的不可关联性。为了证明我们的方法的潜力，我们应用它来评估四种现状的生物模板保护技术：生物盐、Bloom filter、Homomorphic Encryption和块重映射。对于最后一种技术，我们的框架与其他现有的指标进行比较，以显示它的优势。
</details></li>
</ul>
<hr>
<h2 id="Image-Patch-Matching-with-Graph-Based-Learning-in-Street-Scenes"><a href="#Image-Patch-Matching-with-Graph-Based-Learning-in-Street-Scenes" class="headerlink" title="Image Patch-Matching with Graph-Based Learning in Street Scenes"></a>Image Patch-Matching with Graph-Based Learning in Street Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04617">http://arxiv.org/abs/2311.04617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui She, Qiyu Kang, Sijie Wang, Wee Peng Tay, Yong Liang Guan, Diego Navarro Navarro, Andreas Hartmannsgruber</li>
<li>for: 这篇论文主要针对自动驾驶中的计算视觉任务，即将实时捕捉的车辆摄像头中的图像与图像库中的特征区域匹配。</li>
<li>methods: 该论文提出了一种基于图像图的空间关系学习模型，其中图像patches的edge表示图像区域之间的空间关系。</li>
<li>results: 该模型在多个街景数据集上进行评估，并取得了领先的匹配结果。<details>
<summary>Abstract</summary>
Matching landmark patches from a real-time image captured by an on-vehicle camera with landmark patches in an image database plays an important role in various computer perception tasks for autonomous driving. Current methods focus on local matching for regions of interest and do not take into account spatial neighborhood relationships among the image patches, which typically correspond to objects in the environment. In this paper, we construct a spatial graph with the graph vertices corresponding to patches and edges capturing the spatial neighborhood information. We propose a joint feature and metric learning model with graph-based learning. We provide a theoretical basis for the graph-based loss by showing that the information distance between the distributions conditioned on matched and unmatched pairs is maximized under our framework. We evaluate our model using several street-scene datasets and demonstrate that our approach achieves state-of-the-art matching results.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate("Matching landmark patches from a real-time image captured by an on-vehicle camera with landmark patches in an image database plays an important role in various computer perception tasks for autonomous driving. Current methods focus on local matching for regions of interest and do not take into account spatial neighborhood relationships among the image patches, which typically correspond to objects in the environment. In this paper, we construct a spatial graph with the graph vertices corresponding to patches and edges capturing the spatial neighborhood information. We propose a joint feature and metric learning model with graph-based learning. We provide a theoretical basis for the graph-based loss by showing that the information distance between the distributions conditioned on matched and unmatched pairs is maximized under our framework. We evaluate our model using several street-scene datasets and demonstrate that our approach achieves state-of-the-art matching results.")Here's the translation:<<SYS>>匹配从在车载摄像头捕捉的实时图像中提取的标志性补丁与图像库中的标志性补丁之间的匹配对计算机视觉任务中扮演着重要角色。当前方法主要集中于区域关注点的本地匹配，而不考虑图像补丁之间的空间相邻关系，通常对环境中的物体相应。在本文中，我们构建了一个空间图，其顶点对应于补丁，而边则捕捉了图像补丁之间的空间相邻关系。我们提出了一种联合特征和度量学习模型，并基于图形学习。我们提供了对图形学习损失的理论基础，并证明了我们的框架下，匹配后的分布conditioned on matched和unmatched对的信息距离最大。我们使用了多个街景数据集来评估我们的方法，并证明了我们的方法可以达到状态级匹配结果。
</details></li>
</ul>
<hr>
<h2 id="On-Characterizing-the-Evolution-of-Embedding-Space-of-Neural-Networks-using-Algebraic-Topology"><a href="#On-Characterizing-the-Evolution-of-Embedding-Space-of-Neural-Networks-using-Algebraic-Topology" class="headerlink" title="On Characterizing the Evolution of Embedding Space of Neural Networks using Algebraic Topology"></a>On Characterizing the Evolution of Embedding Space of Neural Networks using Algebraic Topology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04592">http://arxiv.org/abs/2311.04592</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cross-caps/dnntopology">https://github.com/cross-caps/dnntopology</a></li>
<li>paper_authors: Suryaka Suresh, Bishshoy Das, Vinayak Abrol, Sumantra Dutta Roy</li>
<li>For: 本研究使用深度学习神经网络（DNN）的层次结构来研究特征表示空间的topologic变化。* Methods: 本研究使用Cubical homology来分析深度神经网络的特征表示空间，并对多种流行的深度架构和实际图像数据进行了扩展分析。* Results: 研究发现，随着深度层数的增加，特征表示空间的topologic复杂度逐渐减少，最终达到最低的Betti数。此外，研究还发现了一些对准变换和数据采样等因素的不变性，这些不变性有助于提高神经网络的泛化能力。<details>
<summary>Abstract</summary>
We study how the topology of feature embedding space changes as it passes through the layers of a well-trained deep neural network (DNN) through Betti numbers. Motivated by existing studies using simplicial complexes on shallow fully connected networks (FCN), we present an extended analysis using Cubical homology instead, with a variety of popular deep architectures and real image datasets. We demonstrate that as depth increases, a topologically complicated dataset is transformed into a simple one, resulting in Betti numbers attaining their lowest possible value. The rate of decay in topological complexity (as a metric) helps quantify the impact of architectural choices on the generalization ability. Interestingly from a representation learning perspective, we highlight several invariances such as topological invariance of (1) an architecture on similar datasets; (2) embedding space of a dataset for architectures of variable depth; (3) embedding space to input resolution/size, and (4) data sub-sampling. In order to further demonstrate the link between expressivity \& the generalization capability of a network, we consider the task of ranking pre-trained models for downstream classification task (transfer learning). Compared to existing approaches, the proposed metric has a better correlation to the actually achievable accuracy via fine-tuning the pre-trained model.
</details>
<details>
<summary>摘要</summary>
我们研究深度神经网络（DNN）中层次结构的变化，通过瓶颈复合体（Cubical homology）进行扩展分析，使用多种流行的深度架构和实际图像数据集。我们发现，随着深度增加，复杂的图像数据集变换成简单的一个，导致Betti数达到最低可能的值。 decay 率可以量化架构选择对通用能力的影响。从表示学学习的视角来看，我们发现了一些对称性，包括：(1) 架构在相似的数据集上的 topological invariance; (2) 数据集的嵌入空间的嵌入空间的 topological invariance; (3) 嵌入空间与输入分辨率/大小的 topological invariance; (4) 数据采样的 topological invariance。为了进一步证明拓扑表达能力和通用能力之间的关系，我们考虑了预训练模型的排名任务（transfer learning）。与现有方法相比，我们的指标具有更好的与实际可 achievable 精度的相关性。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Human-Pose-Estimation-for-Autonomous-Driving-with-3D-Event-Representations"><a href="#Rethinking-Human-Pose-Estimation-for-Autonomous-Driving-with-3D-Event-Representations" class="headerlink" title="Rethinking Human Pose Estimation for Autonomous Driving with 3D Event Representations"></a>Rethinking Human Pose Estimation for Autonomous Driving with 3D Event Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04591">http://arxiv.org/abs/2311.04591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masterhow/eventpointpose">https://github.com/masterhow/eventpointpose</a></li>
<li>paper_authors: Xiaoting Yin, Hao Shi, Jiaan Chen, Ze Wang, Yaozu Ye, Huajian Ni, Kailun Yang, Kaiwei Wang</li>
<li>for: 提高自动驾驶和停车安全性，通过预测人类行为。</li>
<li>methods: 使用事件摄像机，创建3D事件表示，并开发EV-3DPW数据集。</li>
<li>results: 在公共实际世界DHP19数据集上，事件点云技术实现了实时移动预测，而解除事件 voxel方法达到了最高准确性。实验表明我们的提posed 3D表示方法在 traditional RGB图像和事件帧技术的比较中具有更高的总体化能力。<details>
<summary>Abstract</summary>
Human pose estimation is a critical component in autonomous driving and parking, enhancing safety by predicting human actions. Traditional frame-based cameras and videos are commonly applied, yet, they become less reliable in scenarios under high dynamic range or heavy motion blur. In contrast, event cameras offer a robust solution for navigating these challenging contexts. Predominant methodologies incorporate event cameras into learning frameworks by accumulating events into event frames. However, such methods tend to marginalize the intrinsic asynchronous and high temporal resolution characteristics of events. This disregard leads to a loss in essential temporal dimension data, crucial for safety-critical tasks associated with dynamic human activities. To address this issue and to unlock the 3D potential of event information, we introduce two 3D event representations: the Rasterized Event Point Cloud (RasEPC) and the Decoupled Event Voxel (DEV). The RasEPC collates events within concise temporal slices at identical positions, preserving 3D attributes with statistical cues and markedly mitigating memory and computational demands. Meanwhile, the DEV representation discretizes events into voxels and projects them across three orthogonal planes, utilizing decoupled event attention to retrieve 3D cues from the 2D planes. Furthermore, we develop and release EV-3DPW, a synthetic event-based dataset crafted to facilitate training and quantitative analysis in outdoor scenes. On the public real-world DHP19 dataset, our event point cloud technique excels in real-time mobile predictions, while the decoupled event voxel method achieves the highest accuracy. Experiments reveal our proposed 3D representation methods' superior generalization capacities against traditional RGB images and event frame techniques. Our code and dataset are available at https://github.com/MasterHow/EventPointPose.
</details>
<details>
<summary>摘要</summary>
人体姿态估计是自动驾驶和停车中的关键组件，提高安全性 by 预测人类行为。传统的帧基摄像头和视频通常被应用，但在高动态范围或重重运动模糊的场景下变得不可靠。相比之下，事件摄像头提供了一种可靠的解决方案。大多数方法是将事件摄像头集成到学习框架中，但这些方法通常会忽略事件的本质异步和高时间分辨率特性。这种忽略会导致数据中丢失重要的时间维度信息，这些信息对于安全关键任务相对至关重要。为了解决这个问题并激活事件信息的3D潜力，我们介绍了两种3D事件表示方法：矩阵化事件点云（RasEPC）和解除事件VOXEL（DEV）。 RasEPC将事件按照时间片的方式归并在同一个位置，保留3D特征并减少内存和计算负担。 DEV表示法将事件分解成立方体，并将其投影到三个orthogonal平面上，通过独立事件注意力来捕捉3D准确信息。此外，我们还开发了EV-3DPW Synthetic Event-based Dataset，用于训练和量化分析户外场景。在公共的real-world DHP19数据集上，我们的事件点云技术在实时移动预测中表现出色，而DEV表示法在精度方面达到最高水平。实验表明我们提出的3D表示方法具有传统RGB图像和事件帧技术的更好的总体化能力。我们的代码和数据可以在https://github.com/MasterHow/EventPointPose上获取。
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-deepfake-localization-in-diffusion-generated-images"><a href="#Weakly-supervised-deepfake-localization-in-diffusion-generated-images" class="headerlink" title="Weakly-supervised deepfake localization in diffusion-generated images"></a>Weakly-supervised deepfake localization in diffusion-generated images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04584">http://arxiv.org/abs/2311.04584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dragos Tantaru, Elisabeta Oneata, Dan Oneata</li>
<li>for: 这 paper 的目的是提出一种weakly-supervised的 Deepfake detection方法，以便提供更多的信息，包括哪些区域被修改。</li>
<li>methods: 这 paper 使用了 three main categories of methods，包括explanations, local scores 和 attention。这些方法都基于 Xception 网络作为共同背景 Architecure。</li>
<li>results: 这 paper 的结果表明，weakly-supervised localization 是可能的，并且使用 local scores 方法可以更加敏感于缺乏超级vision。<details>
<summary>Abstract</summary>
The remarkable generative capabilities of denoising diffusion models have raised new concerns regarding the authenticity of the images we see every day on the Internet. However, the vast majority of existing deepfake detection models are tested against previous generative approaches (e.g. GAN) and usually provide only a "fake" or "real" label per image. We believe a more informative output would be to augment the per-image label with a localization map indicating which regions of the input have been manipulated. To this end, we frame this task as a weakly-supervised localization problem and identify three main categories of methods (based on either explanations, local scores or attention), which we compare on an equal footing by using the Xception network as the common backbone architecture. We provide a careful analysis of all the main factors that parameterize the design space: choice of method, type of supervision, dataset and generator used in the creation of manipulated images; our study is enabled by constructing datasets in which only one of the components is varied. Our results show that weakly-supervised localization is attainable, with the best performing detection method (based on local scores) being less sensitive to the looser supervision than to the mismatch in terms of dataset or generator.
</details>
<details>
<summary>摘要</summary>
“denoising diffusion模型的卓越生成能力已经引起了互联网上每天看到的图像的真实性的新问题。然而，现有的深伪检测模型都是基于前一代生成方法（如GAN）进行测试，通常只提供每个图像的“伪”或“真”标签。我们认为，更有用的输出将是在每个图像上添加一个涉及到输入图像的涉及级别的地图，以便更好地了解图像中哪些部分被修改。为了实现这一目标，我们将这个任务视为一个弱有监督的地方化任务，并将涉及到的方法分为三大类（基于解释、本地分数或注意力）。我们使用Xception网络作为共同背景架构，并进行了仔细的分析，包括方法选择、监督类型、数据集和生成器在内的所有主要因素。我们的研究表明，弱有监督的地方化是可行的，最高性能的检测方法（基于本地分数）在监督不充分的情况下比 DATASET或生成器的不一致性更加敏感。”
</details></li>
</ul>
<hr>
<h2 id="A-3D-generative-model-of-pathological-multi-modal-MR-images-and-segmentations"><a href="#A-3D-generative-model-of-pathological-multi-modal-MR-images-and-segmentations" class="headerlink" title="A 3D generative model of pathological multi-modal MR images and segmentations"></a>A 3D generative model of pathological multi-modal MR images and segmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04552">http://arxiv.org/abs/2311.04552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/virginiafdez/brainspade3d_rel">https://github.com/virginiafdez/brainspade3d_rel</a></li>
<li>paper_authors: Virginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Mark S. Graham, Tom Vercauteren, M. Jorge Cardoso</li>
<li>for: 本研究旨在提供一种用于脑MRI和相关分割的三维生成模型，以便 condition on 特定的疾病现象和对比。</li>
<li>methods: 本研究使用了生成对抗网络（GANs）和扩散模型（DMs）来生成高质量的Synthetic MRI和相关分割数据，并允许用户根据特定的疾病现象和对比来控制生成的图像和分割结果。</li>
<li>results: 研究表明，brainSPADE3D可以生成高度具有一致性的Synthetic MRI和相关分割数据，并且可以结合不同的疾病现象来生成混合的图像和分割结果。此外，研究还发现，使用brainSPADE3D可以改善预测模型在不期望的疾病存在时的性能。<details>
<summary>Abstract</summary>
Generative modelling and synthetic data can be a surrogate for real medical imaging datasets, whose scarcity and difficulty to share can be a nuisance when delivering accurate deep learning models for healthcare applications. In recent years, there has been an increased interest in using these models for data augmentation and synthetic data sharing, using architectures such as generative adversarial networks (GANs) or diffusion models (DMs). Nonetheless, the application of synthetic data to tasks such as 3D magnetic resonance imaging (MRI) segmentation remains limited due to the lack of labels associated with the generated images. Moreover, many of the proposed generative MRI models lack the ability to generate arbitrary modalities due to the absence of explicit contrast conditioning. These limitations prevent the user from adjusting the contrast and content of the images and obtaining more generalisable data for training task-specific models. In this work, we propose brainSPADE3D, a 3D generative model for brain MRI and associated segmentations, where the user can condition on specific pathological phenotypes and contrasts. The proposed joint imaging-segmentation generative model is shown to generate high-fidelity synthetic images and associated segmentations, with the ability to combine pathologies. We demonstrate how the model can alleviate issues with segmentation model performance when unexpected pathologies are present in the data.
</details>
<details>
<summary>摘要</summary>
生成模型和人工数据可以作为真实医学影像数据的代理，解决医疗应用中深度学习模型的准确性问题。在最近几年里，有越来越多的人关注使用这些模型进行数据增强和人工数据共享，使用生成敌方网络（GANs）或扩散模型（DMs）。然而，使用生成数据进行3D磁共振成像（MRI）分割 task 仍然受到生成图像无标签的限制，以及生成模型无法生成多种模式的缺乏能力。这些限制使得用户无法调整图像的对比度和内容，从而获得更普适的数据用于训练任务特定模型。在这项工作中，我们提出了brainSPADE3D，一种3D生成模型用于脑MRI和相关的分割。用户可以根据特定的疾病现象和对比来conditioning这些模型。我们展示了该模型可以生成高 fideli ty的人工图像和相关的分割，并能够组合疾病。我们还示出了该模型可以解决预期疾病存在于数据中时，分割模型性能下降的问题。
</details></li>
</ul>
<hr>
<h2 id="Learning-Robust-Multi-Scale-Representation-for-Neural-Radiance-Fields-from-Unposed-Images"><a href="#Learning-Robust-Multi-Scale-Representation-for-Neural-Radiance-Fields-from-Unposed-Images" class="headerlink" title="Learning Robust Multi-Scale Representation for Neural Radiance Fields from Unposed Images"></a>Learning Robust Multi-Scale Representation for Neural Radiance Fields from Unposed Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04521">http://arxiv.org/abs/2311.04521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishant Jain, Suryansh Kumar, Luc Van Gool</li>
<li>for: 这 paper 是用于解决计算机视觉中的神经图像基于渲染问题，即在给定一组由自由移动相机拍摄的图像时，在测试时使用神经网络synthesize场景图像。</li>
<li>methods: 该 paper 使用了以下方法：（i）通过一个可靠的渠道来重建高精度相机参数，以便在神经新视角synthesize过程中更加准确地模拟场景图像。（ii）在day-to-day不 pose的图像中，模型对象内容的多resolution采用，以适应高速运动的相机。</li>
<li>results: 该 paper 通过实验表明，在不考虑 Camera pose 估计精度的情况下，模型多scale neural scene representation可以是Counterproductive。而在具有准确的相机pose估计的场景表示框架中，可以准确地synthesize图像。<details>
<summary>Abstract</summary>
We introduce an improved solution to the neural image-based rendering problem in computer vision. Given a set of images taken from a freely moving camera at train time, the proposed approach could synthesize a realistic image of the scene from a novel viewpoint at test time. The key ideas presented in this paper are (i) Recovering accurate camera parameters via a robust pipeline from unposed day-to-day images is equally crucial in neural novel view synthesis problem; (ii) It is rather more practical to model object's content at different resolutions since dramatic camera motion is highly likely in day-to-day unposed images. To incorporate the key ideas, we leverage the fundamentals of scene rigidity, multi-scale neural scene representation, and single-image depth prediction. Concretely, the proposed approach makes the camera parameters as learnable in a neural fields-based modeling framework. By assuming per view depth prediction is given up to scale, we constrain the relative pose between successive frames. From the relative poses, absolute camera pose estimation is modeled via a graph-neural network-based multiple motion averaging within the multi-scale neural-fields network, leading to a single loss function. Optimizing the introduced loss function provides camera intrinsic, extrinsic, and image rendering from unposed images. We demonstrate, with examples, that for a unified framework to accurately model multiscale neural scene representation from day-to-day acquired unposed multi-view images, it is equally essential to have precise camera-pose estimates within the scene representation framework. Without considering robustness measures in the camera pose estimation pipeline, modeling for multi-scale aliasing artifacts can be counterproductive. We present extensive experiments on several benchmark datasets to demonstrate the suitability of our approach.
</details>
<details>
<summary>摘要</summary>
我们介绍一个改进了的解决方案，用于计算机视觉中的神经图像基于测试项目。假设我们有一组由自由移动摄像机拍摄的图像，我们的方法可以将这些图像转换为具有真实感的图像，并且在测试时间点上实现不同的观察角度。我们的关键想法包括：1. 从日常生活中的不条理图像中获取精确的摄像机参数，这是神经novel view synthesis问题的重要前提。2. 因为日常生活中的图像可能会受到剧烈的摄像机运动，因此需要在不同的解析率上模型物体内容。为了实现这些想法，我们利用了场景的刚性、多尺度神经场景表示和单图像深度预测的基础知识。具体来说，我们将摄像机参数设置为神经场中的学习型态。通过假设每个视角深度预测是固定的，我们将相关的视角之间的相对位置组成一个多尺度神经网络中的多动作平均，从而得到一个单一的损失函数。通过优化这个损失函数，我们可以获得摄像机参数、摄像机内部和图像输出等。我们在多个 benchmark 数据集上进行了广泛的实验，证明了我们的方法适用于实现多尺度神经场景表示。而不具备稳定性测量的摄像机参数估计在神经场景表示框架中是Equally essential。如果不考虑稳定性测量，则模型多尺度杂质噪压可能会是Counterproductive。
</details></li>
</ul>
<hr>
<h2 id="Learning-Discriminative-Features-for-Crowd-Counting"><a href="#Learning-Discriminative-Features-for-Crowd-Counting" class="headerlink" title="Learning Discriminative Features for Crowd Counting"></a>Learning Discriminative Features for Crowd Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04509">http://arxiv.org/abs/2311.04509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuehai Chen</li>
<li>For: 提高人群计数模型在高度拥挤的区域中的准确性，特别是在人群中的小对象和背景之间的区分。* Methods: 提出了一种学习权重特征框架，包括遮盖特征预测模块（MPM）和监督像素级异常学习模块（CLM），以提高模型在高度拥挤区域中的局部化和对比背景的能力。* Results: 模型在各种计算机视觉任务中，如人群计数和物体检测，在拥挤环境下提高了地面的准确性。<details>
<summary>Abstract</summary>
Crowd counting models in highly congested areas confront two main challenges: weak localization ability and difficulty in differentiating between foreground and background, leading to inaccurate estimations. The reason is that objects in highly congested areas are normally small and high-level features extracted by convolutional neural networks are less discriminative to represent small objects. To address these problems, we propose a learning discriminative features framework for crowd counting, which is composed of a masked feature prediction module (MPM) and a supervised pixel-level contrastive learning module (CLM). The MPM randomly masks feature vectors in the feature map and then reconstructs them, allowing the model to learn about what is present in the masked regions and improving the model's ability to localize objects in high-density regions. The CLM pulls targets close to each other and pushes them far away from background in the feature space, enabling the model to discriminate foreground objects from background. Additionally, the proposed modules can be beneficial in various computer vision tasks, such as crowd counting and object detection, where dense scenes or cluttered environments pose challenges to accurate localization. The proposed two modules are plug-and-play, incorporating the proposed modules into existing models can potentially boost their performance in these scenarios.
</details>
<details>
<summary>摘要</summary>
群体计数模型在高度拥堵的区域面临两大挑战：一是地方化能力弱和Difficulty in differentiating between foreground and background，导致估计不准确。这是因为在高度拥堵的区域中的对象通常是小型，高级特征提取网络的特征更难以区分小对象。为解决这些问题，我们提议一种学习特征分类框架 для群体计数，该框架包括带mask的特征预测模块（MPM）和supervised像素级冲突学习模块（CLM）。MPM randomly masks特征向量在特征图中，然后重建它们，使模型能够学习masked regions中的内容，提高对象的定位能力。CLM pulls targets close to each other and pushes them far away from background in the feature space, enabling the model to discriminate foreground objects from background。此外，提议的两个模块可以在多种计算机视觉任务中有效，如人群计数和物体检测， где高度拥堵的环境或拥堵的环境会对准确的定位 pose challenges。提议的两个模块可以plug-and-play，将其 integrate into existing models可能会提高其性能在这些场景中。
</details></li>
</ul>
<hr>
<h2 id="NITEC-Versatile-Hand-Annotated-Eye-Contact-Dataset-for-Ego-Vision-Interaction"><a href="#NITEC-Versatile-Hand-Annotated-Eye-Contact-Dataset-for-Ego-Vision-Interaction" class="headerlink" title="NITEC: Versatile Hand-Annotated Eye Contact Dataset for Ego-Vision Interaction"></a>NITEC: Versatile Hand-Annotated Eye Contact Dataset for Ego-Vision Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04505">http://arxiv.org/abs/2311.04505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thohemp/nitec">https://github.com/thohemp/nitec</a></li>
<li>paper_authors: Thorsten Hempel, Magnus Jung, Ahmed A. Abdelrahman, Ayoub Al-Hamadi</li>
<li>for: The paper is written for advancing ego-vision-based eye contact research, specifically in the fields of computer vision, human-computer interaction, and social robotics.</li>
<li>methods: The paper presents a hand-annotated eye contact dataset called NITEC, which exceeds existing datasets in size and variety of demographics, social contexts, and lighting conditions.</li>
<li>results: The paper demonstrates strong cross-dataset performance of NITEC, emphasizing its effectiveness and adaptability in various scenarios, and makes the dataset publicly available for further exploration and reproducibility.<details>
<summary>Abstract</summary>
Eye contact is a crucial non-verbal interaction modality and plays an important role in our everyday social life. While humans are very sensitive to eye contact, the capabilities of machines to capture a person's gaze are still mediocre. We tackle this challenge and present NITEC, a hand-annotated eye contact dataset for ego-vision interaction. NITEC exceeds existing datasets for ego-vision eye contact in size and variety of demographics, social contexts, and lighting conditions, making it a valuable resource for advancing ego-vision-based eye contact research. Our extensive evaluations on NITEC demonstrate strong cross-dataset performance, emphasizing its effectiveness and adaptability in various scenarios, that allows seamless utilization to the fields of computer vision, human-computer interaction, and social robotics. We make our NITEC dataset publicly available to foster reproducibility and further exploration in the field of ego-vision interaction. https://github.com/thohemp/nitec
</details>
<details>
<summary>摘要</summary>
眼接触是非常重要的非语言交互方式，在我们每天的社交生活中扮演着重要的角色。然而，机器人的眼接触捕捉能力仍然很差。我们解决这个挑战，并提供了 NITEC，一个手动标注的眼接触数据集 для egovision交互。NITEC 的大小和多样性都超过了现有的 egovision 眼接触数据集，包括不同的人种、社会背景和照明条件，这使得它成为了 egovision 研究中的一个非常有价值的资源。我们对 NITEC 进行了广泛的评估，并证明了它在多个场景中的强大横跨数据集表现，表明它在计算机视觉、人机交互和社交机器人等领域可以无缝应用。我们将 NITEC 数据集公开提供，以便重现和进一步探索 egovision 交互领域。更多信息请参考 https://github.com/thohemp/nitec。
</details></li>
</ul>
<hr>
<h2 id="PRED-Pre-training-via-Semantic-Rendering-on-LiDAR-Point-Clouds"><a href="#PRED-Pre-training-via-Semantic-Rendering-on-LiDAR-Point-Clouds" class="headerlink" title="PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds"></a>PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04501">http://arxiv.org/abs/2311.04501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Yang, Haiyang Wang, Di Dai, Liwei Wang</li>
<li>for: The paper is written for outdoor point cloud pre-training, addressing the issue of incompleteness in point clouds and incorporating images for improved performance.</li>
<li>methods: The paper proposes a novel image-assisted pre-training framework called PRED, which uses a Birds-Eye-View feature map conditioned semantic rendering and point-wise masking with a high mask ratio (95%) to enhance the model’s performance.</li>
<li>results: The paper demonstrates the superiority of PRED over prior point cloud pre-training methods, achieving significant improvements on various large-scale datasets for 3D perception tasks.<details>
<summary>Abstract</summary>
Pre-training is crucial in 3D-related fields such as autonomous driving where point cloud annotation is costly and challenging. Many recent studies on point cloud pre-training, however, have overlooked the issue of incompleteness, where only a fraction of the points are captured by LiDAR, leading to ambiguity during the training phase. On the other hand, images offer more comprehensive information and richer semantics that can bolster point cloud encoders in addressing the incompleteness issue inherent in point clouds. Yet, incorporating images into point cloud pre-training presents its own challenges due to occlusions, potentially causing misalignments between points and pixels. In this work, we propose PRED, a novel image-assisted pre-training framework for outdoor point clouds in an occlusion-aware manner. The main ingredient of our framework is a Birds-Eye-View (BEV) feature map conditioned semantic rendering, leveraging the semantics of images for supervision through neural rendering. We further enhance our model's performance by incorporating point-wise masking with a high mask ratio (95%). Extensive experiments demonstrate PRED's superiority over prior point cloud pre-training methods, providing significant improvements on various large-scale datasets for 3D perception tasks. Codes will be available at https://github.com/PRED4pc/PRED.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>在3D相关领域，如自动驾驶，前期训练是非常重要的。然而，许多最近的点云预训练研究却忽视了点云的不完整性问题，导致训练阶段的模糊性。相比之下，图像具有更广泛的信息和更丰富的 semantics，可以增强点云编码器对点云不完整性的应对。然而，将图像与点云进行结合存在其自身的挑战，因为可能存在遮挡，导致点云和像素之间的不一致。在这个工作中，我们提出了一种新的图像助け预训练框架，称为PRED（图像协助预训练）。我们的框架的主要组成部分是基于 bird's eye view（BEV）的Semantic Feature Map Conditioned Neural Rendering，利用图像semantics来为预训练提供超vision。此外，我们还增强了我们的模型性能，通过点wise掩蔽（mask ratio为95%）。广泛的实验证明PRED的优越性，在多个大规模数据集上提供了3D感知任务的显著改进。代码将在https://github.com/PRED4pc/PRED上提供。
</details></li>
</ul>
<hr>
<h2 id="PersonMAE-Person-Re-Identification-Pre-Training-with-Masked-AutoEncoders"><a href="#PersonMAE-Person-Re-Identification-Pre-Training-with-Masked-AutoEncoders" class="headerlink" title="PersonMAE: Person Re-Identification Pre-Training with Masked AutoEncoders"></a>PersonMAE: Person Re-Identification Pre-Training with Masked AutoEncoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04496">http://arxiv.org/abs/2311.04496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hezhen Hu, Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Lu Yuan, Dong Chen, Houqiang Li</li>
<li>for: This paper is written for the task of Person Re-identification (ReID), specifically to learn generic feature representation for this task.</li>
<li>methods: The paper proposes a simple yet effective pre-training framework called PersonMAE, which involves two core designs in masked autoencoders to better serve the task of Person Re-ID. The framework generates two regions from the given image, corrupts one region with block-wise masking to mimic common occlusion in ReID, and then predicts the whole other region at both pixel level and semantic feature level.</li>
<li>results: The paper achieves state-of-the-art performance on four downstream ReID tasks, including supervised (holistic and occluded setting), and unsupervised (UDA and USL setting). Specifically, with the ViT-B backbone, the paper achieves 79.8% and 69.5% mAP on the MSMT17 and OccDuke datasets, respectively, surpassing the previous state-of-the-art by a large margin of +8.0 mAP and +5.3 mAP, respectively.<details>
<summary>Abstract</summary>
Pre-training is playing an increasingly important role in learning generic feature representation for Person Re-identification (ReID). We argue that a high-quality ReID representation should have three properties, namely, multi-level awareness, occlusion robustness, and cross-region invariance. To this end, we propose a simple yet effective pre-training framework, namely PersonMAE, which involves two core designs into masked autoencoders to better serve the task of Person Re-ID. 1) PersonMAE generates two regions from the given image with RegionA as the input and \textit{RegionB} as the prediction target. RegionA is corrupted with block-wise masking to mimic common occlusion in ReID and its remaining visible parts are fed into the encoder. 2) Then PersonMAE aims to predict the whole RegionB at both pixel level and semantic feature level. It encourages its pre-trained feature representations with the three properties mentioned above. These properties make PersonMAE compatible with downstream Person ReID tasks, leading to state-of-the-art performance on four downstream ReID tasks, i.e., supervised (holistic and occluded setting), and unsupervised (UDA and USL setting). Notably, on the commonly adopted supervised setting, PersonMAE with ViT-B backbone achieves 79.8% and 69.5% mAP on the MSMT17 and OccDuke datasets, surpassing the previous state-of-the-art by a large margin of +8.0 mAP, and +5.3 mAP, respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT预训练在人识别（ReID）中扮演着日益重要的角色，我们认为高质量的ReID表示应具有三种性能，即多级意识、遮挡Robustness和跨地域一致性。为此，我们提出了一个简单 yet有效的预训练框架，即PersonMAE，该框架包括两个核心设计：1. PersonMAE将给定图像分成两个区域， RegionA 作为输入，RegionB 作为预测目标。 RegionA 会被块性遮盖，以模拟ReID中常见的遮挡，其可见部分会被编码器处理。2. PersonMAE会 endeavour 预测 RegionB 的整个像素级和 semantic feature 级。这种设计使得 PersonMAE 的预训练特征表示具有上述三种性能，这些性能使得 PersonMAE 与下游 ReID 任务相匹配，导致 PersonMAE 在四个下游 ReID 任务中 achieved  state-of-the-art 性能，包括超级vised（整体和 occluded 设置）和无监督（UDA 和 USL 设置）。特别是，在通常采用的超级vised 设置下，PersonMAE  WITH ViT-B 基础模型 achieved 79.8% 和 69.5% mAP 在 MSMT17 和 OccDuke 数据集上，比前一个 state-of-the-art 的margin 加大了 +8.0 mAP，+5.3 mAP，分别。>>
</details></li>
</ul>
<hr>
<h2 id="Non-Rigid-Shape-Registration-via-Deep-Functional-Maps-Prior"><a href="#Non-Rigid-Shape-Registration-via-Deep-Functional-Maps-Prior" class="headerlink" title="Non-Rigid Shape Registration via Deep Functional Maps Prior"></a>Non-Rigid Shape Registration via Deep Functional Maps Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04494">http://arxiv.org/abs/2311.04494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rqhuang88/DFR">https://github.com/rqhuang88/DFR</a></li>
<li>paper_authors: Puhua Jiang, Mingze Sun, Ruqi Huang</li>
<li>for: 非RIGID shape registration without correspondence supervision</li>
<li>methods: 使用学习基于的框架，通过高维空间映射学习得到非RIGID shape registration</li>
<li>results: 可以处理大幅度内在变换和外在变换的shape registration，并且可以提供高质量的对应关系 between 不同形状对Here’s a more detailed explanation of each point:1. for: The paper proposes a learning-based framework for non-rigid shape registration without correspondence supervision. This means that the framework does not rely on manually specified correspondences between shapes, but instead uses learning-based methods to establish these correspondences.2. methods: The framework uses a combination of high-dimensional embedding and deep functional maps (DFM) to establish correspondences between shapes. The high-dimensional embedding maps shapes into a high-dimensional space where they are easier to align, and the DFM learns a non-linear mapping between the shapes. The correspondences are dynamically updated based on the intermediate registrations and filtered by a consistency prior, which makes the pipeline more robust.3. results: The paper demonstrates that the proposed framework achieves state-of-the-art results on several benchmarks of non-rigid point cloud matching, and can handle both significant extrinsic and intrinsic deformations. The framework is also able to provide high-quality correspondences between unseen challenging shape pairs, which is not possible with traditional registration methods or intrinsic methods.<details>
<summary>Abstract</summary>
In this paper, we propose a learning-based framework for non-rigid shape registration without correspondence supervision. Traditional shape registration techniques typically rely on correspondences induced by extrinsic proximity, therefore can fail in the presence of large intrinsic deformations. Spectral mapping methods overcome this challenge by embedding shapes into, geometric or learned, high-dimensional spaces, where shapes are easier to align. However, due to the dependency on abstract, non-linear embedding schemes, the latter can be vulnerable with respect to perturbed or alien input. In light of this, our framework takes the best of both worlds. Namely, we deform source mesh towards the target point cloud, guided by correspondences induced by high-dimensional embeddings learned from deep functional maps (DFM). In particular, the correspondences are dynamically updated according to the intermediate registrations and filtered by consistency prior, which prominently robustify the overall pipeline. Moreover, in order to alleviate the requirement of extrinsically aligned input, we train an orientation regressor on a set of aligned synthetic shapes independent of the training shapes for DFM. Empirical results show that, with as few as dozens of training shapes of limited variability, our pipeline achieves state-of-the-art results on several benchmarks of non-rigid point cloud matching, but also delivers high-quality correspondences between unseen challenging shape pairs that undergo both significant extrinsic and intrinsic deformations, in which case neither traditional registration methods nor intrinsic methods work. The code is available at https://github.com/rqhuang88/DFR.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种学习基于的非定制形状匹配框架，不需要对匹配得到超vision。传统的形状匹配技术通常靠 extrinsic  proximity 引起的匹配，因此在大规模内部扭变的情况下失败。spectral mapping 方法可以将形状嵌入高维空间中，使形状更容易匹配。然而，由于依赖于抽象的非线性嵌入方案，后者可能对输入数据进行敏感操作。为了解决这个问题，我们的框架结合了两者的优点。具体来说，我们将源网格弯曲到目标点云，以高维空间中学习的深度函数映射（DFM）中的匹配为导向。特别是，匹配在中间registrations 更新和consistency prior 的筛选下进行动态更新，以提高整体的稳定性。此外，为了避免外部对齐的需求，我们在独立于训练形状的synthetic shapes 上训练了一个旋转回归器。实验结果表明，只需几十个有限的训练形状，我们的管道可以在多个非定制点云匹配 benchmark 上达到领先的Result，并且能够在未看到的挑战性形状对应中提供高质量的匹配。代码可以在 https://github.com/rqhuang88/DFR 上获取。
</details></li>
</ul>
<hr>
<h2 id="All-Optical-Phase-Conjugation-Using-Diffractive-Wavefront-Processing"><a href="#All-Optical-Phase-Conjugation-Using-Diffractive-Wavefront-Processing" class="headerlink" title="All-Optical Phase Conjugation Using Diffractive Wavefront Processing"></a>All-Optical Phase Conjugation Using Diffractive Wavefront Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04473">http://arxiv.org/abs/2311.04473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Che-Yung Shen, Jingxi Li, Tianyi Gan, Mona Jarrahi, Aydogan Ozcan</li>
<li>for: 用于Counteracting wavefront distortions，包括Imaging和Beam focusing。</li>
<li>methods: 使用Deep learning优化Passive diffractive layers，实现All-optical phase conjugation操作。</li>
<li>results: 通过实验验证，Diffractive wavefront processor可以成功地对phas aberrations进行OPC操作，并且可以在不同的电磁波谱中实现Cost-effective wavefront engineering解决方案。<details>
<summary>Abstract</summary>
Optical phase conjugation (OPC) is a nonlinear technique used for counteracting wavefront distortions, with various applications ranging from imaging to beam focusing. Here, we present the design of a diffractive wavefront processor to approximate all-optical phase conjugation operation for input fields with phase aberrations. Leveraging deep learning, a set of passive diffractive layers was optimized to all-optically process an arbitrary phase-aberrated coherent field from an input aperture, producing an output field with a phase distribution that is the conjugate of the input wave. We experimentally validated the efficacy of this wavefront processor by 3D fabricating diffractive layers trained using deep learning and performing OPC on phase distortions never seen by the diffractive processor during its training. Employing terahertz radiation, our physical diffractive processor successfully performed the OPC task through a shallow spatially-engineered volume that axially spans tens of wavelengths. In addition to this transmissive OPC configuration, we also created a diffractive phase-conjugate mirror by combining deep learning-optimized diffractive layers with a standard mirror. Given its compact, passive and scalable nature, our diffractive wavefront processor can be used for diverse OPC-related applications, e.g., turbidity suppression and aberration correction, and is also adaptable to different parts of the electromagnetic spectrum, especially those where cost-effective wavefront engineering solutions do not exist.
</details>
<details>
<summary>摘要</summary>
光学相 conjugation（OPC）是一种非线性技术，用于对波front distortions 进行矫正，具有各种应用，从图像到波front фокусировки。在这里，我们提出了一种使用 diffractive wavefront processor 来近似 all-optical phase conjugation 操作，用于处理具有相位偏移的输入场。通过深度学习，我们分配了一组 passive diffractive layers 来对输入场进行 all-optical 处理，以生成一个输出场的相位分布，与输入场的相位分布是相似的。我们通过实验验证了这种 wavefront processor 的有效性，使用了 deep learning 进行优化的 diffractive layers 和标准镜的组合，以实现 OPC 任务。使用 terahertz 辐射，我们的物理 diffractive processor 成功完成了 OPC 任务，并在一个 axially 延伸多达数十波长的束缚空间中完成了 shallow 的 SPATIALLY-ENGINEERED 体。此外，我们还创造了一个 diffractive phase-conjugate mirror，通过将 deep learning 优化的 diffractive layers 与标准镜相结合。由于它的 компакт、被动和可扩展性，我们的 diffractive wavefront processor 可以用于多种 OPC-相关的应用，例如浊度补做和偏移补做，并可以适应不同的电磁波谱спектrum，特别是那些没有cost-effective wavefront engineering 解决方案。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Few-shot-CLIP-with-Semantic-Aware-Fine-Tuning"><a href="#Enhancing-Few-shot-CLIP-with-Semantic-Aware-Fine-Tuning" class="headerlink" title="Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning"></a>Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04464">http://arxiv.org/abs/2311.04464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Zhu, Yuefeng Chen, Wei Wang, Xiaofeng Mao, Xiu Yan, Yue Wang, Zhigang Li, Wang lu, Jindong Wang, Xiangyang Ji</li>
<li>for: 本研究的目的是提高深度神经网络在具有限制样本数的低资源场景中表现，通过修改CLIP预训练模型的特定部分来适应不同的少shot任务。</li>
<li>methods: 我们修改了CLIP预训练模型的视觉编码器中的特征权重卷积层，使其在不同的少shot任务中适应不同的 semantics。在训练过程中，我们根据任务特点调整了这些权重，以便模型能够更好地适应具体的任务。在测试阶段，我们使用了差分融合来结合原始的权重卷积层和调整后的权重卷积层，以便将它们两者的知识融合在一起。</li>
<li>results: 我们的方法可以增强传统的少shot CLIP，并且与现有的adapter方法（SAFE-A）兼容。我们的方法可以更好地适应不同的少shot任务，并且在测试阶段的性能得到了提升。<details>
<summary>Abstract</summary>
Learning generalized representations from limited training samples is crucial for applying deep neural networks in low-resource scenarios. Recently, methods based on Contrastive Language-Image Pre-training (CLIP) have exhibited promising performance in few-shot adaptation tasks. To avoid catastrophic forgetting and overfitting caused by few-shot fine-tuning, existing works usually freeze the parameters of CLIP pre-trained on large-scale datasets, overlooking the possibility that some parameters might not be suitable for downstream tasks. To this end, we revisit CLIP's visual encoder with a specific focus on its distinctive attention pooling layer, which performs a spatial weighted-sum of the dense feature maps. Given that dense feature maps contain meaningful semantic information, and different semantics hold varying importance for diverse downstream tasks (such as prioritizing semantics like ears and eyes in pet classification tasks rather than side mirrors), using the same weighted-sum operation for dense features across different few-shot tasks might not be appropriate. Hence, we propose fine-tuning the parameters of the attention pooling layer during the training process to encourage the model to focus on task-specific semantics. In the inference process, we perform residual blending between the features pooled by the fine-tuned and the original attention pooling layers to incorporate both the few-shot knowledge and the pre-trained CLIP's prior knowledge. We term this method as Semantic-Aware FinE-tuning (SAFE). SAFE is effective in enhancing the conventional few-shot CLIP and is compatible with the existing adapter approach (termed SAFE-A).
</details>
<details>
<summary>摘要</summary>
学习通用表示法从有限的训练样本中学习是深度神经网络在低资源场景中应用的关键。最近，基于对比语言图像预训练（CLIP）的方法在几架适应任务中表现出色。为了避免几架适应过拟合和忘记，现有的工作通常将CLIP预训练在大规模数据集上的参数冻结，忽略了可能一些参数不适合下游任务。为此，我们重新审视CLIP的视觉Encoder，尤其是其独特的注意力池化层，该层通过在权重总和中进行空间权重的积分来实现。由于稠密特征图包含有意义的语义信息，而不同语义在不同下游任务中具有不同的重要性（例如在宠物分类任务中更重视耳朵和眼睛而非侧镜），因此在不同几架适应任务中使用同样的权重积分操作可能不合适。因此，我们提议在训练过程中细化注意力池化层的参数，以便让模型强调任务特有的语义。在推理过程中，我们通过将细化后的注意力池化层和原始注意力池化层的特征进行差分融合来 incorporate 两者的知识。我们称这种方法为 Semantic-Aware FinE-tuning（SAFE）。SAFE 有效地提高了传统的几架 CLIP，并且与现有的适配器方法（称为 SAFE-A）兼容。
</details></li>
</ul>
<hr>
<h2 id="Retargeting-video-with-an-end-to-end-framework"><a href="#Retargeting-video-with-an-end-to-end-framework" class="headerlink" title="Retargeting video with an end-to-end framework"></a>Retargeting video with an end-to-end framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04458">http://arxiv.org/abs/2311.04458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thi-Ngoc-Hanh Le, HuiGuang Huang, Yi-Ru Chen, Tong-Yee Lee</li>
<li>For: 这个研究旨在为 Computer Graphics 应用程序提供影片重定向功能，以增强用户观赏体验。* Methods: 本研究使用了一个终端到终端的 RETVI 方法，具有两个模组：内容特征分析器 (CFA) 和适应型扩展估计器 (ADE)，以解决旧有方法的计算瓶颈和限制。* Results: 实验和评估结果显示，我们的系统在质量和运行时间上具有明显的优势，超越了先前的工作。更多结果可以在 <a target="_blank" rel="noopener" href="http://graphics.csie.ncku.edu.tw/RETVI">http://graphics.csie.ncku.edu.tw/RETVI</a> 网站上获取。<details>
<summary>Abstract</summary>
Video holds significance in computer graphics applications. Because of the heterogeneous of digital devices, retargeting videos becomes an essential function to enhance user viewing experience in such applications. In the research of video retargeting, preserving the relevant visual content in videos, avoiding flicking, and processing time are the vital challenges. Extending image retargeting techniques to the video domain is challenging due to the high running time. Prior work of video retargeting mainly utilizes time-consuming preprocessing to analyze frames. Plus, being tolerant of different video content, avoiding important objects from shrinking, and the ability to play with arbitrary ratios are the limitations that need to be resolved in these systems requiring investigation. In this paper, we present an end-to-end RETVI method to retarget videos to arbitrary aspect ratios. We eliminate the computational bottleneck in the conventional approaches by designing RETVI with two modules, content feature analyzer (CFA) and adaptive deforming estimator (ADE). The extensive experiments and evaluations show that our system outperforms previous work in quality and running time. Visit our project website for more results at http://graphics.csie.ncku.edu.tw/RETVI.
</details>
<details>
<summary>摘要</summary>
视频具有计算机图形应用中的重要意义。由于数字设备的多样性，对视频进行重定向变得非常重要，以提高用户视觉体验。在研究视频重定向方面，保持视频中相关的视觉内容，避免抖动、处理时间和视频内容的多样性是核心挑战。由于视频重定向技术的运行时间较长，将图像重定向技术应用于视频领域是挑战。现有的视频重定向方法主要通过时间consuming的预处理分析帧来解决这些挑战。此外，保持重要对象不减小、避免抖动和处理时间也是需要解决的问题。在这篇论文中，我们提出了一种终端到终端的视频重定向方法（RETVI），以解决以上问题。我们通过设计内容特征分析器（CFA）和自适应扭转估计器（ADE）两个模块来消除传统方法的计算瓶颈。我们的系统在质量和运行时间方面胜过先前的工作。更多结果可以在我们项目网站上找到：http://graphics.csie.ncku.edu.tw/RETVI。
</details></li>
</ul>
<hr>
<h2 id="SS-MAE-Spatial-Spectral-Masked-Auto-Encoder-for-Multi-Source-Remote-Sensing-Image-Classification"><a href="#SS-MAE-Spatial-Spectral-Masked-Auto-Encoder-for-Multi-Source-Remote-Sensing-Image-Classification" class="headerlink" title="SS-MAE: Spatial-Spectral Masked Auto-Encoder for Multi-Source Remote Sensing Image Classification"></a>SS-MAE: Spatial-Spectral Masked Auto-Encoder for Multi-Source Remote Sensing Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04442">http://arxiv.org/abs/2311.04442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyan Lin, Feng Gao, Xiaocheng Shi, Junyu Dong, Qian Du</li>
<li>for: 这个研究旨在提出一种基于自动编码器的隐藏特征模型（SS-MAE），用于混合多源数据的类别 tasks。</li>
<li>methods: 该模型包括空间对应分支和спектраль对应分支，具体来说，空间对应分支随机填充patches，并从原始数据中恢复缺失的像素；而 спектраль对应分支随机填充频道，并从原始数据中恢复缺失的频道。</li>
<li>results: 实验结果显示，Compared with多个基于state-of-the-art的基eline，SS-MAE在三个公开的数据集上表现出色，并且可以充分利用输入数据的空间和спектраль特征。<details>
<summary>Abstract</summary>
Masked image modeling (MIM) is a highly popular and effective self-supervised learning method for image understanding. Existing MIM-based methods mostly focus on spatial feature modeling, neglecting spectral feature modeling. Meanwhile, existing MIM-based methods use Transformer for feature extraction, some local or high-frequency information may get lost. To this end, we propose a spatial-spectral masked auto-encoder (SS-MAE) for HSI and LiDAR/SAR data joint classification. Specifically, SS-MAE consists of a spatial-wise branch and a spectral-wise branch. The spatial-wise branch masks random patches and reconstructs missing pixels, while the spectral-wise branch masks random spectral channels and reconstructs missing channels. Our SS-MAE fully exploits the spatial and spectral representations of the input data. Furthermore, to complement local features in the training stage, we add two lightweight CNNs for feature extraction. Both global and local features are taken into account for feature modeling. To demonstrate the effectiveness of the proposed SS-MAE, we conduct extensive experiments on three publicly available datasets. Extensive experiments on three multi-source datasets verify the superiority of our SS-MAE compared with several state-of-the-art baselines. The source codes are available at \url{https://github.com/summitgao/SS-MAE}.
</details>
<details>
<summary>摘要</summary>
高清晰自适应模型（MIM）是一种非常受欢迎且有效的无监督学习方法，用于图像理解。现有的MIM基于方法主要关注空间特征模型化，忽略spectral特征模型化。另外，现有的MIM基于方法使用Transformer来EXTRACT特征，可能会导致一些本地或高频信息丢失。为了解决这个问题，我们提议一种具有空间特征和spectral特征的掩码自适应编码器（SS-MAE），用于混合高光谱和LiDAR/SAR数据的分类。具体来说，SS-MAE包括一个空间特征分支和一个spectral特征分支。空间特征分支掩码随机质点并重建缺失像素，而spectral特征分支掩码随机spectral通道并重建缺失通道。我们的SS-MAE完全利用输入数据的空间和spectral表示。此外，为了补偿本地特征在训练阶段的不足，我们添加了两个轻量级CNN来EXTRACT特征。我们的方法同时利用全球特征和本地特征来模型特征。为了证明我们提议的SS-MAE的效果，我们在三个公共可用的数据集上进行了广泛的实验。我们的实验结果表明，SS-MAE在多源数据集上的表现明显超过了一些状态的基eline。我们的代码可以在github上找到：https://github.com/summitgao/SS-MAE。
</details></li>
</ul>
<hr>
<h2 id="Blurry-Video-Compression-A-Trade-off-between-Visual-Enhancement-and-Data-Compression"><a href="#Blurry-Video-Compression-A-Trade-off-between-Visual-Enhancement-and-Data-Compression" class="headerlink" title="Blurry Video Compression: A Trade-off between Visual Enhancement and Data Compression"></a>Blurry Video Compression: A Trade-off between Visual Enhancement and Data Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04430">http://arxiv.org/abs/2311.04430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dawit Mureja Argaw, Junsik Kim, In So Kweon</li>
<li>for: 本研究旨在提高视频压缩（VC）方法的 universality，使其在不同的时间前提下能够维持视频质量。</li>
<li>methods: 本研究使用了一种基于可让渡的最小化最大化优化方法，通过利用视频压缩和图像增强之间的自然质量补做，提高视频质量。</li>
<li>results: 对多个标准数据集进行了广泛的实验，证明了我们的方法在比较于现有的VC方法之上具有更高的效果。<details>
<summary>Abstract</summary>
Existing video compression (VC) methods primarily aim to reduce the spatial and temporal redundancies between consecutive frames in a video while preserving its quality. In this regard, previous works have achieved remarkable results on videos acquired under specific settings such as instant (known) exposure time and shutter speed which often result in sharp videos. However, when these methods are evaluated on videos captured under different temporal priors, which lead to degradations like motion blur and low frame rate, they fail to maintain the quality of the contents. In this work, we tackle the VC problem in a general scenario where a given video can be blurry due to predefined camera settings or dynamics in the scene. By exploiting the natural trade-off between visual enhancement and data compression, we formulate VC as a min-max optimization problem and propose an effective framework and training strategy to tackle the problem. Extensive experimental results on several benchmark datasets confirm the effectiveness of our method compared to several state-of-the-art VC approaches.
</details>
<details>
<summary>摘要</summary>
现有的视频压缩（VC）方法主要目标是减少视频中的空间和时间重复性，以保持视频质量。在这种情况下，先前的工作已经实现了在特定的曝光时间和闭合速度下拍摄的视频中获得了出色的结果。然而，当这些方法应用于不同的时间优先顺序下拍摄的视频时，它们无法保持视频内容的质量。在这种情况下，我们解决了视频压缩问题，充分利用了视频增强和数据压缩之间的自然负荷关系，并提出了一种有效的框架和训练策略。对多个标准数据集进行了广泛的实验，证明了我们的方法与许多现有的VC方法相比，有更高的效果。
</details></li>
</ul>
<hr>
<h2 id="CSAM-A-2-5D-Cross-Slice-Attention-Module-for-Anisotropic-Volumetric-Medical-Image-Segmentation"><a href="#CSAM-A-2-5D-Cross-Slice-Attention-Module-for-Anisotropic-Volumetric-Medical-Image-Segmentation" class="headerlink" title="CSAM: A 2.5D Cross-Slice Attention Module for Anisotropic Volumetric Medical Image Segmentation"></a>CSAM: A 2.5D Cross-Slice Attention Module for Anisotropic Volumetric Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04942">http://arxiv.org/abs/2311.04942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/al3x-o-o-hung/csam">https://github.com/al3x-o-o-hung/csam</a></li>
<li>paper_authors: Alex Ling Yu Hung, Haoxin Zheng, Kai Zhao, Xiaoxi Du, Kaifeng Pang, Qi Miao, Steven S. Raman, Demetri Terzopoulos, Kyunghyun Sung</li>
<li>for: This paper aims to address the problem of anisotropic volumetric medical data in deep learning-based segmentation, specifically in magnetic resonance imaging (MRI) data.</li>
<li>methods: The proposed method is a 2.5D approach that combines 2D convolution with volumetric information, using a Cross-Slice Attention Module (CSAM) to capture information across all slices in the volume. The CSAM module applies semantic, positional, and slice attention on deep feature maps at different scales.</li>
<li>results: The proposed method was extensively tested using different network architectures and tasks, and the results demonstrate the usefulness and generalizability of CSAM. The code for the proposed method is available at <a target="_blank" rel="noopener" href="https://github.com/aL3x-O-o-Hung/CSAM">https://github.com/aL3x-O-o-Hung/CSAM</a>.<details>
<summary>Abstract</summary>
A large portion of volumetric medical data, especially magnetic resonance imaging (MRI) data, is anisotropic, as the through-plane resolution is typically much lower than the in-plane resolution. Both 3D and purely 2D deep learning-based segmentation methods are deficient in dealing with such volumetric data since the performance of 3D methods suffers when confronting anisotropic data, and 2D methods disregard crucial volumetric information. Insufficient work has been done on 2.5D methods, in which 2D convolution is mainly used in concert with volumetric information. These models focus on learning the relationship across slices, but typically have many parameters to train. We offer a Cross-Slice Attention Module (CSAM) with minimal trainable parameters, which captures information across all the slices in the volume by applying semantic, positional, and slice attention on deep feature maps at different scales. Our extensive experiments using different network architectures and tasks demonstrate the usefulness and generalizability of CSAM. Associated code is available at https://github.com/aL3x-O-o-Hung/CSAM.
</details>
<details>
<summary>摘要</summary>
“一大部分的体积医学数据，特别是磁共振成像（MRI）数据，具有不对称性，通常在水平方向的分辨率比垂直方向的分辨率低得多。三维和仅二维的深度学习基于的分类方法都不适合处理这种体积数据，因为三维方法在遇到不对称的数据时表现不佳，而二维方法则忽略了体积数据中的重要信息。对二点五维方法的研究相对较少，这些模型通常在标本之间学习关系，但通常有许多受训的参数。我们提出了跨条件注意模块（CSAM），具有最少受训参数，可以在不同标本之间学习关系，并且可以在不同的标本中捕捉到重要信息。我们的广泛实验显示了 CSAM 的有用性和普遍性。相关的代码可以在 GitHub 上找到：https://github.com/aL3x-O-o-Hung/CSAM。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Learning-the-What-and-How-of-Annotation-in-Video-Object-Segmentation"><a href="#Learning-the-What-and-How-of-Annotation-in-Video-Object-Segmentation" class="headerlink" title="Learning the What and How of Annotation in Video Object Segmentation"></a>Learning the What and How of Annotation in Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04414">http://arxiv.org/abs/2311.04414</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thanosDelatolas/eva-vos">https://github.com/thanosDelatolas/eva-vos</a></li>
<li>paper_authors: Thanos Delatolas, Vicky Kalogeiton, Dim P. Papadopoulos</li>
<li>for: 提高视频对象分割（VOS）模型训练效率，减少人工标注成本。</li>
<li>methods: 提出了一种人工 Loop（HITL）注意力机制，通过预测哪些帧（”What”）和哪种注意力类型（”How”）进行标注，以提高标注效率。</li>
<li>results: 对MOSE和DAVIS数据集进行实验，比较了EVA-VOS和标准 annotating 方法，结果表明：EVA-VOS可以在3.5倍快的速度达到与人类一致的准确率；选择帧performanced状态的方法得到了状元性的表现；EVA-VOS在标注时间方面具有显著的提升。<details>
<summary>Abstract</summary>
Video Object Segmentation (VOS) is crucial for several applications, from video editing to video data generation. Training a VOS model requires an abundance of manually labeled training videos. The de-facto traditional way of annotating objects requires humans to draw detailed segmentation masks on the target objects at each video frame. This annotation process, however, is tedious and time-consuming. To reduce this annotation cost, in this paper, we propose EVA-VOS, a human-in-the-loop annotation framework for video object segmentation. Unlike the traditional approach, we introduce an agent that predicts iteratively both which frame ("What") to annotate and which annotation type ("How") to use. Then, the annotator annotates only the selected frame that is used to update a VOS module, leading to significant gains in annotation time. We conduct experiments on the MOSE and the DAVIS datasets and we show that: (a) EVA-VOS leads to masks with accuracy close to the human agreement 3.5x faster than the standard way of annotating videos; (b) our frame selection achieves state-of-the-art performance; (c) EVA-VOS yields significant performance gains in terms of annotation time compared to all other methods and baselines.
</details>
<details>
<summary>摘要</summary>
视频对象分割（VOS）是许多应用程序中的关键技术，从视频编辑到视频数据生成。训练VOS模型需要大量的手动标注视频。传统的Annotation Way是要求人类 manually draw detailed segmentation masks on the target objects at each video frame。然而，这个Annotation process是费时的和费力的。在这篇论文中，我们提出了EVA-VOS，一个人类在Loop的标注框架 для视频对象分割。与传统方法不同，我们引入了一个代理人，该代理人预测iteratively Which frame ("What") to annotate和Which annotation type ("How") to use。然后，标注者仅标注选择的帧，并将其用于更新VOS模块，从而实现了显著的标注时间缩短。我们在MOSE和DAVIS datasets上进行了实验，并显示了以下结果：（a）EVA-VOS可以在3.5倍 faster than the standard way of annotating videos 实现masks with accuracy close to human agreement。（b）我们的帧选择性能在State-of-the-art level。（c）EVA-VOS比所有方法和基准值具有显著的标注时间缩短。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/cs.CV_2023_11_08/" data-id="clpxp6c2i00n0ee88b3bvevaa" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/cs.AI_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T12:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/08/cs.AI_2023_11_08/">cs.AI - 2023-11-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Geometry-Calibrated-DRO-Combating-Over-Pessimism-with-Free-Energy-Implications"><a href="#Geometry-Calibrated-DRO-Combating-Over-Pessimism-with-Free-Energy-Implications" class="headerlink" title="Geometry-Calibrated DRO: Combating Over-Pessimism with Free Energy Implications"></a>Geometry-Calibrated DRO: Combating Over-Pessimism with Free Energy Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05054">http://arxiv.org/abs/2311.05054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashuo Liu, Jiayun Wu, Tianyu Wang, Hao Zou, Bo Li, Peng Cui</li>
<li>for: 提高机器学习算法对 distribuitional shift 的鲁棒性，Addressing the issue of distributional shifts in machine learning algorithms.</li>
<li>methods: 使用 Distributionally Robust Optimization (DRO) 方法，Optimizing the worst-case risk within an uncertainty set.</li>
<li>results: 提出 Geometry-Calibrated DRO (GCDRO) 方法，which incorporates data geometry into calibration terms to alleviate the impact of noise, and establishes a connection between the risk objective and the Helmholtz free energy in statistical physics. Comprehensive experiments confirm GCDRO’s superiority over conventional DRO methods.<details>
<summary>Abstract</summary>
Machine learning algorithms minimizing average risk are susceptible to distributional shifts. Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case risk within an uncertainty set. However, DRO suffers from over-pessimism, leading to low-confidence predictions, poor parameter estimations as well as poor generalization. In this work, we conduct a theoretical analysis of a probable root cause of over-pessimism: excessive focus on noisy samples. To alleviate the impact of noise, we incorporate data geometry into calibration terms in DRO, resulting in our novel Geometry-Calibrated DRO (GCDRO) for regression. We establish the connection between our risk objective and the Helmholtz free energy in statistical physics, and this free-energy-based risk can extend to standard DRO methods. Leveraging gradient flow in Wasserstein space, we develop an approximate minimax optimization algorithm with a bounded error ratio and elucidate how our approach mitigates noisy sample effects. Comprehensive experiments confirm GCDRO's superiority over conventional DRO methods.
</details>
<details>
<summary>摘要</summary>
Neste trabalho, realizamos uma análise teórica de uma causa provável do pessimismo excessivo: o foco excessivo em amostras ruidosas. Para aliviar o impacto do ruído, incorporamos informações de geometria de dados na calibração de DRO, resultando em nossa técnica novativa de Geometry-Calibrated DRO (GCDRO) para regressão. Estabelecemos a conexão entre nossa meta de risco e a energia livre de Helmholtz na física estatística, e essa medida de risco baseada em energia livre pode ser aplicada a métodos de DRO padrão.Leverando o fluxo de gradiente na espaço de Wasserstein, desenvolvemos um algoritmo de otimização aproximada com um erro de ratio limitado e elucidamos como nossa abordagem mitiga os efeitos das amostras ruidosas. Experimentos compreensivos confirmam a superioridade do GCDRO em relação aos métodos de DRO conventioneis.
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Translation-of-Attention-Patterns-in-VQA-Models-to-Natural-Language"><a href="#Zero-shot-Translation-of-Attention-Patterns-in-VQA-Models-to-Natural-Language" class="headerlink" title="Zero-shot Translation of Attention Patterns in VQA Models to Natural Language"></a>Zero-shot Translation of Attention Patterns in VQA Models to Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05043">http://arxiv.org/abs/2311.05043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/explainableml/zs-a2t">https://github.com/explainableml/zs-a2t</a></li>
<li>paper_authors: Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, Zeynep Akata</li>
<li>for: 这 paper 的目的是提出一种可以在零批处理下将 transformer 注意力转换成自然语言的框架，以便更好地理解模型内部的含义。</li>
<li>methods: 该框架基于一个预训练的大型语言模型（LLM），该模型接受任务提示、问题和预测答案作为输入，并根据这些输入选择tokен来描述输入图像中 VQA 模型的注意力区域。</li>
<li>results: 该框架在 textual explanation 数据集上达到了零批处理情况下的州OF-the-art表现，在 GQA-REX 和 VQA-X 上得到了优秀的结果。<details>
<summary>Abstract</summary>
Converting a model's internals to text can yield human-understandable insights about the model. Inspired by the recent success of training-free approaches for image captioning, we propose ZS-A2T, a zero-shot framework that translates the transformer attention of a given model into natural language without requiring any training. We consider this in the context of Visual Question Answering (VQA). ZS-A2T builds on a pre-trained large language model (LLM), which receives a task prompt, question, and predicted answer, as inputs. The LLM is guided to select tokens which describe the regions in the input image that the VQA model attended to. Crucially, we determine this similarity by exploiting the text-image matching capabilities of the underlying VQA model. Our framework does not require any training and allows the drop-in replacement of different guiding sources (e.g. attribution instead of attention maps), or language models. We evaluate this novel task on textual explanation datasets for VQA, giving state-of-the-art performances for the zero-shot setting on GQA-REX and VQA-X. Our code is available at: https://github.com/ExplainableML/ZS-A2T.
</details>
<details>
<summary>摘要</summary>
可以将模型内部转换为文本来提供人类可理解的启示。 drawing inspiration from recent successful training-free approaches for image captioning, we propose ZS-A2T，a zero-shot framework that translates the transformer attention of a given model into natural language without requiring any training. We consider this in the context of Visual Question Answering (VQA). ZS-A2T builds on a pre-trained large language model (LLM), which receives a task prompt, question, and predicted answer as inputs. The LLM is guided to select tokens that describe the regions in the input image that the VQA model attended to. Crucially, we determine this similarity by exploiting the text-image matching capabilities of the underlying VQA model. Our framework does not require any training and allows the drop-in replacement of different guiding sources (e.g., attribution instead of attention maps), or language models. We evaluate this novel task on textual explanation datasets for VQA, achieving state-of-the-art performances for the zero-shot setting on GQA-REX and VQA-X. Our code is available at: https://github.com/ExplainableML/ZS-A2T.
</details></li>
</ul>
<hr>
<h2 id="Automated-Annotation-of-Scientific-Texts-for-ML-based-Keyphrase-Extraction-and-Validation"><a href="#Automated-Annotation-of-Scientific-Texts-for-ML-based-Keyphrase-Extraction-and-Validation" class="headerlink" title="Automated Annotation of Scientific Texts for ML-based Keyphrase Extraction and Validation"></a>Automated Annotation of Scientific Texts for ML-based Keyphrase Extraction and Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05042">http://arxiv.org/abs/2311.05042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oluwamayowa O. Amusat, Harshad Hegde, Christopher J. Mungall, Anna Giannakou, Neil P. Byers, Dan Gunter, Kjiersten Fagnan, Lavanya Ramakrishnan</li>
<li>for: 本研究旨在提高机器学习生成的 metadata 的可信度，以便更好地搜索和利用生物科学领域中的大数据。</li>
<li>methods: 本研究提出了两种新的自动文本标签方法，即使用不同类型的数据源关联和使用领域专用的控制词汇或 Ontology。</li>
<li>results: 实验结果表明，提posed的标签分配方法可以生成高度特定的文本标签，与机器学习生成的关键字列表匹配度高达44%。<details>
<summary>Abstract</summary>
Advanced omics technologies and facilities generate a wealth of valuable data daily; however, the data often lacks the essential metadata required for researchers to find and search them effectively. The lack of metadata poses a significant challenge in the utilization of these datasets. Machine learning-based metadata extraction techniques have emerged as a potentially viable approach to automatically annotating scientific datasets with the metadata necessary for enabling effective search. Text labeling, usually performed manually, plays a crucial role in validating machine-extracted metadata. However, manual labeling is time-consuming; thus, there is an need to develop automated text labeling techniques in order to accelerate the process of scientific innovation. This need is particularly urgent in fields such as environmental genomics and microbiome science, which have historically received less attention in terms of metadata curation and creation of gold-standard text mining datasets.   In this paper, we present two novel automated text labeling approaches for the validation of ML-generated metadata for unlabeled texts, with specific applications in environmental genomics. Our techniques show the potential of two new ways to leverage existing information about the unlabeled texts and the scientific domain. The first technique exploits relationships between different types of data sources related to the same research study, such as publications and proposals. The second technique takes advantage of domain-specific controlled vocabularies or ontologies. In this paper, we detail applying these approaches for ML-generated metadata validation. Our results show that the proposed label assignment approaches can generate both generic and highly-specific text labels for the unlabeled texts, with up to 44% of the labels matching with those suggested by a ML keyword extraction algorithm.
</details>
<details>
<summary>摘要</summary>
高级数据技术和设施每天生成大量有价值的数据，但这些数据经常缺乏必要的元数据，使研究人员困难地找到和搜索这些数据。缺乏元数据对于使用这些数据而言是一个重要的挑战。基于机器学习的元数据抽取技术已经出现为可能的解决方案，可以自动将科学数据集中添加元数据。文本标签，通常是手动完成的，在验证机器提取的元数据中扮演着关键的角色。然而，手动标签是时间消耗的，因此有一个需要开发自动文本标签技术，以加速科学创新的过程。这个需求特别是在环境遗传学和微生物学等领域 particularly urgent，这些领域在元数据管理和创建高品质的文本挖掘数据方面 historically received less attention。在这篇论文中，我们提出了两种新的自动文本标签方法，用于验证机器生成的元数据的有效性。这两种方法都利用了不同的数据来源之间的关系，以及领域专门的控制词汇或 ontology。我们在这篇论文中详细介绍了这些方法的应用。我们的结果表明，我们的标签分配方法可以为无标签文本生成both generic和高度特定的文本标签，并且最高达44%的标签与机器学习 keyword extraction 算法提出的标签相匹配。
</details></li>
</ul>
<hr>
<h2 id="Transfer-learning-from-a-sparsely-annotated-dataset-of-3D-medical-images"><a href="#Transfer-learning-from-a-sparsely-annotated-dataset-of-3D-medical-images" class="headerlink" title="Transfer learning from a sparsely annotated dataset of 3D medical images"></a>Transfer learning from a sparsely annotated dataset of 3D medical images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05032">http://arxiv.org/abs/2311.05032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diagnijmegen/medicaltransferlearning3d-unet">https://github.com/diagnijmegen/medicaltransferlearning3d-unet</a></li>
<li>paper_authors: Gabriel Efrain Humpire-Mamani, Colin Jacobs, Mathias Prokop, Bram van Ginneken, Nikolas Lessmann<br>for: This study aims to improve the efficiency of annotation and increase the accessibility of accurate organ segmentation in medical imaging using transfer learning.methods: The authors use transfer learning to leverage pre-trained model features from a large dataset to improve the performance of deep convolutional neural networks for organ segmentation in medical imaging. They use a base segmentation model (3D U-Net) trained on a large and sparsely annotated dataset and fine-tune it for four new down-stream segmentation tasks with fully annotated datasets.results: The results show that transfer learning from the base model is beneficial when small datasets are available, providing significant performance improvements. Fine-tuning the base model is more beneficial than updating all the network weights with vanilla transfer learning. The study also shows that cross-modality transfer learning using CT scans is beneficial. The performance of the fine-tuned models increased by up to 0.129 (+28%) Dice score and on average 23 experiments increased the performance by 0.029 Dice score in the new segmentation tasks.<details>
<summary>Abstract</summary>
Transfer learning leverages pre-trained model features from a large dataset to save time and resources when training new models for various tasks, potentially enhancing performance. Due to the lack of large datasets in the medical imaging domain, transfer learning from one medical imaging model to other medical imaging models has not been widely explored. This study explores the use of transfer learning to improve the performance of deep convolutional neural networks for organ segmentation in medical imaging. A base segmentation model (3D U-Net) was trained on a large and sparsely annotated dataset; its weights were used for transfer learning on four new down-stream segmentation tasks for which a fully annotated dataset was available. We analyzed the training set size's influence to simulate scarce data. The results showed that transfer learning from the base model was beneficial when small datasets were available, providing significant performance improvements; where fine-tuning the base model is more beneficial than updating all the network weights with vanilla transfer learning. Transfer learning with fine-tuning increased the performance by up to 0.129 (+28\%) Dice score than experiments trained from scratch, and on average 23 experiments increased the performance by 0.029 Dice score in the new segmentation tasks. The study also showed that cross-modality transfer learning using CT scans was beneficial. The findings of this study demonstrate the potential of transfer learning to improve the efficiency of annotation and increase the accessibility of accurate organ segmentation in medical imaging, ultimately leading to improved patient care. We made the network definition and weights publicly available to benefit other users and researchers.
</details>
<details>
<summary>摘要</summary>
通过使用已经训练过的模型特征，转移学习可以为训练新的模型 saves time和资源，并且可能提高性能。由于医疗影像领域的大型数据集罕见，医疗影像中的转移学习还没有广泛探索。本研究探讨了使用转移学习提高医疗影像中深度卷积神经网络的组织分 segmentation性能。基本分 segmentation模型（3D U-Net）在一个大型但罕见地注解的数据集上训练，其 weights 用于转移学习四个新的下游分 segmentation任务上。我们分析了训练集大小的影响，以模拟缺乏数据的情况。结果表明，从基本模型进行转移学习在小数据集时是有利的，提供了显著性能提升（+28%）；而在所有网络权重更新为vanilla transfer learning的情况下，进行精度调整（fine-tuning）更是有利。转移学习与精度调整共同提高了新分 segmentation任务的性能，平均提高0.029 Dice分数。研究还发现了在CT扫描图像上进行转移学习的利好。本研究发现，转移学习可以提高医疗影像中的注解效率和准确率，从而提高患者治疗的质量。我们将网络定义和权重公开，以便其他用户和研究人员使用。
</details></li>
</ul>
<hr>
<h2 id="Towards-Effective-Paraphrasing-for-Information-Disguise"><a href="#Towards-Effective-Paraphrasing-for-Information-Disguise" class="headerlink" title="Towards Effective Paraphrasing for Information Disguise"></a>Towards Effective Paraphrasing for Information Disguise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05018">http://arxiv.org/abs/2311.05018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idecir/idecir-towards-effective-paraphrasing-for-information-disguise">https://github.com/idecir/idecir-towards-effective-paraphrasing-for-information-disguise</a></li>
<li>paper_authors: Anmol Agarwal, Shrey Gupta, Vamshi Bonagiri, Manas Gaur, Joseph Reagle, Ponnurangam Kumaraguru<br>for: 本研究的目的是提出一种基于幂等词汇替换的信息隐蔽技术，以防止互联网上作者的文字媒体宣传被非法利用。methods: 本研究使用了自然语言处理技术中的人工智能自动词汇替换工具（如SpinRewriter、WordAI），并通过对 sentence 进行迭代 perturbation 来混淆搜索机制。results: 本研究表明，使用多级词汇替换和幂等词汇替换可以成功隐藏 sentences 82% 的时间。这种方法可以帮助作者隐藏敏感信息，从而减少不良用户利用这些信息的风险。<details>
<summary>Abstract</summary>
Information Disguise (ID), a part of computational ethics in Natural Language Processing (NLP), is concerned with best practices of textual paraphrasing to prevent the non-consensual use of authors' posts on the Internet. Research on ID becomes important when authors' written online communication pertains to sensitive domains, e.g., mental health. Over time, researchers have utilized AI-based automated word spinners (e.g., SpinRewriter, WordAI) for paraphrasing content. However, these tools fail to satisfy the purpose of ID as their paraphrased content still leads to the source when queried on search engines. There is limited prior work on judging the effectiveness of paraphrasing methods for ID on search engines or their proxies, neural retriever (NeurIR) models. We propose a framework where, for a given sentence from an author's post, we perform iterative perturbation on the sentence in the direction of paraphrasing with an attempt to confuse the search mechanism of a NeurIR system when the sentence is queried on it. Our experiments involve the subreddit 'r/AmItheAsshole' as the source of public content and Dense Passage Retriever as a NeurIR system-based proxy for search engines. Our work introduces a novel method of phrase-importance rankings using perplexity scores and involves multi-level phrase substitutions via beam search. Our multi-phrase substitution scheme succeeds in disguising sentences 82% of the time and hence takes an essential step towards enabling researchers to disguise sensitive content effectively before making it public. We also release the code of our approach.
</details>
<details>
<summary>摘要</summary>
信息掩蔽（ID），一部分的计算伦理在自然语言处理（NLP）中，关注在文本重新排序方法的最佳实践中，以防止互联网上作者的帖子不经授权使用。对于研究人员来说，研究ID变得重要，特别是作者在互联网上发表的文本与敏感领域相关，例如心理健康。在过去，研究人员使用基于AI的自动词汇替换工具（如SpinRewriter、WordAI）进行重新排序内容。然而，这些工具并不满足ID的目的，因为它们重新排序后的内容仍然可以跟踪回原始来源。有限的先前研究探讨了重新排序方法的效果对于ID在搜索引擎或其代理Neural Retriever（NeurIR）模型。我们提出了一个框架，其中，对于作者的一句话，我们在重新排序方向下进行迭代 perturbation，以混淆搜索机制中查询该句话的NeurIR系统。我们的实验使用了Reddit上的“r/AmItheAsshole”社区为公共内容的来源，并使用基于Neural Retriever的代理来模拟搜索引擎。我们的方法包括phrase重要性排名使用混淆分数和多级词汇替换via扫描搜索。我们的多词汇替换方案成功地隐藏了句子82%的时间，因此为研究人员隐藏敏感内容的有效方法。我们还发布了我们的方法的代码。
</details></li>
</ul>
<hr>
<h2 id="Joint-Sensing-and-Semantic-Communications-with-Multi-Task-Deep-Learning"><a href="#Joint-Sensing-and-Semantic-Communications-with-Multi-Task-Deep-Learning" class="headerlink" title="Joint Sensing and Semantic Communications with Multi-Task Deep Learning"></a>Joint Sensing and Semantic Communications with Multi-Task Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05017">http://arxiv.org/abs/2311.05017</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus</li>
<li>for: 这篇论文探讨了深度学习技术的紧密 integrate 感知通信，包括延伸到semantic communications。</li>
<li>methods:  transmitter 使用深度神经网络（encoder）进行源编码、频率编码和模ulation，而 receiver 使用另一个深度神经网络（decoder）进行模ulation、频率解码和源解码来重construct 数据样本。</li>
<li>results: 该实验使用 CIFAR-10 作为输入数据，并考虑了通道效应如添加白噪和衰减抑制。结果表明多任务深度学习可以实现高精度的紧密感知通信和semantic communications。<details>
<summary>Abstract</summary>
This paper explores the integration of deep learning techniques for joint sensing and communications, with an extension to semantic communications. The integrated system comprises a transmitter and receiver operating over a wireless channel, subject to noise and fading effects. The transmitter employs a deep neural network, namely an encoder, for joint operations of source coding, channel coding, and modulation, while the receiver utilizes another deep neural network, namely a decoder, for joint operations of demodulation, channel decoding, and source decoding to reconstruct the data samples. The transmitted signal serves a dual purpose, supporting communication with the receiver and enabling sensing. When a target is present, the reflected signal is received, and another deep neural network decoder is utilized for sensing. This decoder is responsible for detecting the target's presence and determining its range. All these deep neural networks, including one encoder and two decoders, undergo joint training through multi-task learning, considering data and channel characteristics. This paper extends to incorporate semantic communications by introducing an additional deep neural network, another decoder at the receiver, operating as a task classifier. This decoder evaluates the fidelity of label classification for received signals, enhancing the integration of semantics within the communication process. The study presents results based on using the CIFAR-10 as the input data and accounting for channel effects like Additive White Gaussian Noise (AWGN) and Rayleigh fading. The results underscore the effectiveness of multi-task deep learning in achieving high-fidelity joint sensing and semantic communications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Interpreting-Pretrained-Language-Models-via-Concept-Bottlenecks"><a href="#Interpreting-Pretrained-Language-Models-via-Concept-Bottlenecks" class="headerlink" title="Interpreting Pretrained Language Models via Concept Bottlenecks"></a>Interpreting Pretrained Language Models via Concept Bottlenecks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05014">http://arxiv.org/abs/2311.05014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Tan, Lu Cheng, Song Wang, Yuan Bo, Jundong Li, Huan Liu</li>
<li>for: 本研究旨在解释PLMs的黑obox性，提高PLMs在自然语言处理任务中的可读性和可理解性。</li>
<li>methods: 我们提出了一种新的方法，利用高级别的意义ful的概念来解释PLMs。我们使用人工标注和机器生成的概念相结合，提取隐藏神经元，以捕捉semantically meaningful和任务特定的概念。</li>
<li>results: 我们通过对实际数据集进行实证研究，发现我们的方法可以提供有价值的解释PLMs的行为，帮助诊断模型失败和提高模型的Robustness。<details>
<summary>Abstract</summary>
Pretrained language models (PLMs) have made significant strides in various natural language processing tasks. However, the lack of interpretability due to their ``black-box'' nature poses challenges for responsible implementation. Although previous studies have attempted to improve interpretability by using, e.g., attention weights in self-attention layers, these weights often lack clarity, readability, and intuitiveness. In this research, we propose a novel approach to interpreting PLMs by employing high-level, meaningful concepts that are easily understandable for humans. For example, we learn the concept of ``Food'' and investigate how it influences the prediction of a model's sentiment towards a restaurant review. We introduce C$^3$M, which combines human-annotated and machine-generated concepts to extract hidden neurons designed to encapsulate semantically meaningful and task-specific concepts. Through empirical evaluations on real-world datasets, we manifest that our approach offers valuable insights to interpret PLM behavior, helps diagnose model failures, and enhances model robustness amidst noisy concept labels.
</details>
<details>
<summary>摘要</summary>
Pretrained language models (PLMs) have made significant progress in various natural language processing tasks. However, their "black-box" nature poses challenges for responsible implementation. Previous studies have attempted to improve interpretability by using, for example, attention weights in self-attention layers, but these weights often lack clarity, readability, and intuitiveness. In this research, we propose a novel approach to interpreting PLMs by employing high-level, meaningful concepts that are easily understandable for humans. For example, we learn the concept of "Food" and investigate how it influences the prediction of a model's sentiment towards a restaurant review. We introduce C$^3$M, which combines human-annotated and machine-generated concepts to extract hidden neurons designed to encapsulate semantically meaningful and task-specific concepts. Through empirical evaluations on real-world datasets, we demonstrate that our approach offers valuable insights to interpret PLM behavior, helps diagnose model failures, and enhances model robustness amidst noisy concept labels.
</details></li>
</ul>
<hr>
<h2 id="Expressibility-induced-Concentration-of-Quantum-Neural-Tangent-Kernels"><a href="#Expressibility-induced-Concentration-of-Quantum-Neural-Tangent-Kernels" class="headerlink" title="Expressibility-induced Concentration of Quantum Neural Tangent Kernels"></a>Expressibility-induced Concentration of Quantum Neural Tangent Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04965">http://arxiv.org/abs/2311.04965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li-Wei Yu, Weikang Li, Qi Ye, Zhide Lu, Zizhao Han, Dong-Ling Deng</li>
<li>for: 这篇论文主要研究了量子机器学习模型的性能分析方法，以及这些方法如何应用于实际应用中的宽量子变量电路设计。</li>
<li>methods: 这篇论文使用了量子触 neighboorhood kernel 方法，用于分析量子机器学习模型在无限宽限制下的性能。这些方法还被应用于描述量子神经网络训练误差的整数化方式。</li>
<li>results: 研究发现，在全球损失函数下，高表达能量的全球和本地量子编码可以导致量子触 neighboorhood kernel 值快速减少到零。而在本地损失函数下，虽然高表达能量可以导致量子触 neighboorhood kernel 值快速减少，但是不能完全消除。此外，通过广泛的数值实验， authors 验证了这些分析理论。这些发现对量子机器学习模型的设计提供了重要的指导意见。<details>
<summary>Abstract</summary>
Quantum tangent kernel methods provide an efficient approach to analyzing the performance of quantum machine learning models in the infinite-width limit, which is of crucial importance in designing appropriate circuit architectures for certain learning tasks. Recently, they have been adapted to describe the convergence rate of training errors in quantum neural networks in an analytical manner. Here, we study the connections between the trainability and expressibility of quantum tangent kernel models. In particular, for global loss functions, we rigorously prove that high expressibility of both the global and local quantum encodings can lead to exponential concentration of quantum tangent kernel values to zero. Whereas for local loss functions, such issue of exponential concentration persists owing to the high expressibility, but can be partially mitigated. We further carry out extensive numerical simulations to support our analytical theories. Our discoveries unveil a pivotal characteristic of quantum neural tangent kernels, offering valuable insights for the design of wide quantum variational circuit models in practical applications.
</details>
<details>
<summary>摘要</summary>
量子触感керnel方法提供了一种有效的方式来分析量子机器学习模型在无穷宽限下的性能，这对于设计适当的电路体系结构非常重要。最近，它们已经被适应来描述量子神经网络训练错误的减少速率。在这里，我们研究了量子触感керnel模型的可教化和表达能力之间的连接。特别是，对于全局损失函数，我们严格地证明了高表达能力的全局和本地量子编码的情况下，可以导致量子触感керnel值快速减少到零。而对于本地损失函数，这种问题仍然存在，但可以通过高表达能力来部分缓解。我们还进行了广泛的数值仿真，以支持我们的分析理论。我们的发现揭示了量子神经触感kernek的一个重要特点，为实际应用中的宽量子变量电路模型设计提供了价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Size-How-Gradients-Shape-Pruning-Decisions-in-Large-Language-Models"><a href="#Beyond-Size-How-Gradients-Shape-Pruning-Decisions-in-Large-Language-Models" class="headerlink" title="Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models"></a>Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04902">http://arxiv.org/abs/2311.04902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rocktimjyotidas/gblm-pruner">https://github.com/rocktimjyotidas/gblm-pruner</a></li>
<li>paper_authors: Rocktim Jyoti Das, Liqun Ma, Zhiqiang Shen</li>
<li>for: 这个研究是为了提出一种基于梯度的语言模型剔除方法（GBLM-Pruner），以提高这些模型的优化和简化。</li>
<li>methods: 这个方法使用了大型语言模型的预训梯度，通过计算梯度的第一项泰勒展开来决定剔除重要性分数，并且不需要任何训练或重新调整。</li>
<li>results: 试验结果显示，GBLM-Pruner比其他两种方法（SparseGPT和Wanda）在多个测试 benchmark 上表现更好，并且不需要任何额外的训练或重新调整。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the importance pruning score, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (weights+activations) and SparseGPT (weights+activations+weight update) by significant margins. Our code and models are available at https://github.com/RocktimJyotiDas/GBLM-Pruner.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM） WITH 10亿或更多参数是适用于网络剪裁的目标，寻求减少一部分网络权重而不影响性能。先前的方法，如权重强度、SparseGPT和Wanda，ether solely focused on weights or integrated weights with activations for sparsity，但它们忽略了预训练大语言模型中的有用梯度。在这篇论文中，我们提出了一种新的简洁中心的剪裁方法 для预训练LLM，称为Gradient-based Language Model Pruner（GBLM-Pruner）。GBLM-Pruner利用预训练大语言模型中的梯度进行一ORDER Taylor扩展，在无需训练的情况下，通过正确规范梯度来确定剪裁重要性分数，并显著超越了竞争对手SparseGPT和Wanda在多个benchmark上。有趣的是，在执行剪裁后，不结构化的剪裁方法往往会揭示一些结构性 Patterns，这与LLMs中参数结构的几何相互关系有关。此外，GBLM-Pruner不需要任何后续重新训练或权重更新，以简化其他对手。我们在LLaMA-1和LLaMA-2上进行了多种语言benchmark和折衔度评估，发现GBLM-Pruner在比例剪裁、Wanda（权重+活动）和SparseGPT（权重+活动+权重更新）的情况下具有显著优势。我们的代码和模型可以在https://github.com/RocktimJyotiDas/GBLM-Pruner中找到。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Sketching-for-Large-Language-Models"><a href="#Prompt-Sketching-for-Large-Language-Models" class="headerlink" title="Prompt Sketching for Large Language Models"></a>Prompt Sketching for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04954">http://arxiv.org/abs/2311.04954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Beurer-Kellner, Mark Niklas Müller, Marc Fischer, Martin Vechev</li>
<li>For: The paper aims to address the issue of disconnected and wordy intermediate responses in recent prompting strategies for large language models (LLMs).* Methods: The proposed method, called prompt sketching, involves predicting values for multiple variables in a template, allowing users to have more control over the generation process and provide a reasoning framework via intermediate instructions. The key idea is to adapt the decoding procedure to also score follow-up instructions during text generation, optimizing overall template likelihood in inference.* Results: The paper shows that prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. The paper also releases a number of generic, yet effective sketches applicable to many tasks and an open source library called dclib, powering the sketch-aware decoders.<details>
<summary>Abstract</summary>
Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -- first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders.
</details>
<details>
<summary>摘要</summary>
很多现代提示策略 для大型自然语言模型（LLM）会在 sequential 方式下询问模型多次——首先生成中间结果，然后生成最终答案。然而，使用这些方法时，decoder和模型都不知道可能的后续提示，导致中间响应不连贯和不 DESIRED  wordy。在这项工作中，我们解决这个问题，提出了提示绘制（prompt sketching），一种新的提示方式，在 котором一个 LLM 不仅通过完成提示来回答，而且可以预测多个变量的值在模板中。这样，绘制可以让用户更有控制力量，例如提供了一个reasoning框架 via intermediate instructions，导致更好的总体结果。我们的关键想法是使用现有的、自然进行推断的模型，将解码过程修改为在文本生成过程中也评分后续指令，以便在推断过程中优化总体模板概率。我们的实验表明，在零配置情况下，提示绘制在 7 个 LLMBenchmark 任务上比直接询问或链式思维方法表现出色，包括状态跟踪、数学逻辑和通用问答。为便于未来使用，我们发布了一些通用 yet 有效的绘制，以及一个名为 dclib 的开源库，该库将power我们的绘制执行器。
</details></li>
</ul>
<hr>
<h2 id="Two-Complementary-Perspectives-to-Continual-Learning-Ask-Not-Only-What-to-Optimize-But-Also-How"><a href="#Two-Complementary-Perspectives-to-Continual-Learning-Ask-Not-Only-What-to-Optimize-But-Also-How" class="headerlink" title="Two Complementary Perspectives to Continual Learning: Ask Not Only What to Optimize, But Also How"></a>Two Complementary Perspectives to Continual Learning: Ask Not Only What to Optimize, But Also How</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04898">http://arxiv.org/abs/2311.04898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timm Hess, Tinne Tuytelaars, Gido M. van de Ven</li>
<li>for: 这篇论文主要关注于如何解决深度神经网络的持续学习问题，特别是当开始训练新任务时会发生快速忘记的问题。</li>
<li>methods: 这篇论文提出了一种基于加入回放或调整项的方法来 aproximate 缩寸类损失函数，并且还使用了梯度对映技术来调整优化路径。</li>
<li>results: 这篇论文预计通过结合回放-approximated joint 损失函数和梯度对映-based 优化路径，以测试加入后者是否能够提供以下优点：(1) 缓和稳定差距，(2) 增加学习效率，(3) 提高最终学习成果。<details>
<summary>Abstract</summary>
Recent years have seen considerable progress in the continual training of deep neural networks, predominantly thanks to approaches that add replay or regularization terms to the loss function to approximate the joint loss over all tasks so far. However, we show that even with a perfect approximation to the joint loss, these approaches still suffer from temporary but substantial forgetting when starting to train on a new task. Motivated by this 'stability gap', we propose that continual learning strategies should focus not only on the optimization objective, but also on the way this objective is optimized. While there is some continual learning work that alters the optimization trajectory (e.g., using gradient projection techniques), this line of research is positioned as alternative to improving the optimization objective, while we argue it should be complementary. To evaluate the merits of our proposition, we plan to combine replay-approximated joint objectives with gradient projection-based optimization routines to test whether the addition of the latter provides benefits in terms of (1) alleviating the stability gap, (2) increasing the learning efficiency and (3) improving the final learning outcome.
</details>
<details>
<summary>摘要</summary>
近年来，深度神经网络的不断训练方法得到了很大的进步，主要是通过添加回放或规则化项来 aproximate 所有任务的共同损失函数。然而，我们发现，即使有完美的共同损失函数近似，这些方法仍会在开始训练新任务时出现临时且显著的忘记。我们被这个"稳定差距"所驱使，我们提议，不断学习策略应该不仅关注优化目标，还应该关注优化目标的优化方式。虽然有一些不断学习研究使用梯度投影技术来修改优化轨迹，但我们认为这种研究应该是补充优化目标的，而不是替代。为了评估我们的提议的价值，我们计划将回放近似的共同损失函数与梯度投影技术相结合，以测试这种组合是否可以提供以下 beneficial effects：1. 减轻稳定差距2. 提高学习效率3. 提高最终学习成果
</details></li>
</ul>
<hr>
<h2 id="DAMEX-Dataset-aware-Mixture-of-Experts-for-visual-understanding-of-mixture-of-datasets"><a href="#DAMEX-Dataset-aware-Mixture-of-Experts-for-visual-understanding-of-mixture-of-datasets" class="headerlink" title="DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets"></a>DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04894">http://arxiv.org/abs/2311.04894</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinga-lala/damex">https://github.com/jinga-lala/damex</a></li>
<li>paper_authors: Yash Jain, Harkirat Behl, Zsolt Kira, Vibhav Vineet</li>
<li>for: 本文旨在提出一种universal detector的建构方法，如何在大量混合数据集上训练一个模型？</li>
<li>methods: 作者提出了一种名为Dataset-Aware Mixture-of-Experts（DAMEX）的解决方案，通过训练专家来 Route每个数据集的令牌到它的映射专家，以提高模型的性能。</li>
<li>results: 在Universal Object-Detection Benchmark上进行了实验，比较了与现有状态的前一代和非MoE基线方法，达到了平均提升10.2个AP分数，并且在不同的数据集混合情况下（1）有限的可用性、（2）不同的领域和（3）不同的标签集）都达到了稳定的提升。此外，作者还质量地表明了DAMEX的专家表示 collapse问题的Robustness。<details>
<summary>Abstract</summary>
Construction of a universal detector poses a crucial question: How can we most effectively train a model on a large mixture of datasets? The answer lies in learning dataset-specific features and ensembling their knowledge but do all this in a single model. Previous methods achieve this by having separate detection heads on a common backbone but that results in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose Dataset-Aware Mixture-of-Experts, DAMEX where we train the experts to become an `expert' of a dataset by learning to route each dataset tokens to its mapped expert. Experiments on Universal Object-Detection Benchmark show that we outperform the existing state-of-the-art by average +10.2 AP score and improve over our non-MoE baseline by average +2.0 AP score. We also observe consistent gains while mixing datasets with (1) limited availability, (2) disparate domains and (3) divergent label sets. Further, we qualitatively show that DAMEX is robust against expert representation collapse.
</details>
<details>
<summary>摘要</summary>
建构一个通用探测器存在一个关键问题：如何有效地训练一个模型在大量的混合数据集上？答案在于学习数据集特有的特征并将其拼接在单一模型中。先前的方法通过在共同脊梁上添加分开的探测头来实现这一点，但这会导致参数的增加。在这项工作中，我们提出了混合专家（MoE）作为解决方案，并证明MoE不仅是一种扩展性工具。我们提出了数据集特性混合专家（DAMEX），其中我们在训练专家时将数据集的各个元素分配给它们所对应的专家。实验表明，我们在通用物体探测benchmark上的平均AP分数高于现有状态的拟合性工具，提高了非MoE基线的平均AP分数 by 10.2个点。我们还发现在混合不同数据集、不同领域和不同标签集时，DAMEX具有一定的稳定性和可靠性。此外，我们还证明DAMEX不容易受到专家表示塌陷的影响。
</details></li>
</ul>
<hr>
<h2 id="Towards-Few-Annotation-Learning-in-Computer-Vision-Application-to-Image-Classification-and-Object-Detection-tasks"><a href="#Towards-Few-Annotation-Learning-in-Computer-Vision-Application-to-Image-Classification-and-Object-Detection-tasks" class="headerlink" title="Towards Few-Annotation Learning in Computer Vision: Application to Image Classification and Object Detection tasks"></a>Towards Few-Annotation Learning in Computer Vision: Application to Image Classification and Object Detection tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04888">http://arxiv.org/abs/2311.04888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Bouniot</li>
<li>for: 这个论文的目的是提出一些用于机器学习的有限标签问题的理论、算法和实验贡献，特别是在计算机视觉中进行图像分类和对象检测。</li>
<li>methods: 这个论文使用了许多现有的Meta-学习算法，以及多任务学习理论基础，以针对少量标签问题进行更有效的meta-学习。此外，它还提出了一种不使用标签的对象检测器预训练方法，以及一种使用部分标签的semi-supervised学习方法。</li>
<li>results: 这个论文的实验结果表明，通过将多任务学习理论与Meta-学习算法相结合，可以更好地适应少量标签问题，并且可以在对象检测器中使用不使用标签的预训练方法来提高对象检测的准确率。<details>
<summary>Abstract</summary>
In this thesis, we develop theoretical, algorithmic and experimental contributions for Machine Learning with limited labels, and more specifically for the tasks of Image Classification and Object Detection in Computer Vision. In a first contribution, we are interested in bridging the gap between theory and practice for popular Meta-Learning algorithms used in Few-Shot Classification. We make connections to Multi-Task Representation Learning, which benefits from solid theoretical foundations, to verify the best conditions for a more efficient meta-learning. Then, to leverage unlabeled data when training object detectors based on the Transformer architecture, we propose both an unsupervised pretraining and a semi-supervised learning method in two other separate contributions. For pretraining, we improve Contrastive Learning for object detectors by introducing the localization information. Finally, our semi-supervised method is the first tailored to transformer-based detectors.
</details>
<details>
<summary>摘要</summary>
在这个论文中，我们提出了理论、算法和实验贡献，用于机器学习具有有限标签数据的任务，特别是计算机视觉中的图像分类和物体检测。在第一个贡献中，我们尝试 bridge 理论和实践中的各种Meta-Learning算法，用于少量样本分类。我们与多任务学习理论建立连接，以验证最佳的meta-learning条件。然后，我们提出了一种不使用标签数据的对象检测器培训方法，基于Transformer架构。在这两个分布中，我们分别提出了一种无监督预训练方法和一种半监督学习方法。在预训练中，我们提出了一种基于本地化信息的对比学习方法，以提高对象检测器的性能。最后，我们的半监督学习方法是首次应用于基于Transformer架构的对象检测器。
</details></li>
</ul>
<hr>
<h2 id="SEMQA-Semi-Extractive-Multi-Source-Question-Answering"><a href="#SEMQA-Semi-Extractive-Multi-Source-Question-Answering" class="headerlink" title="SEMQA: Semi-Extractive Multi-Source Question Answering"></a>SEMQA: Semi-Extractive Multi-Source Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04886">http://arxiv.org/abs/2311.04886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/quotesum">https://github.com/google-research-datasets/quotesum</a></li>
<li>paper_authors: Tal Schuster, Adam D. Lelkes, Haitian Sun, Jai Gupta, Jonathan Berant, William W. Cohen, Donald Metzler</li>
<li>for: 这篇论文旨在提出一种新的多选问答任务，即将多个多样性的源文摘要成一篇全面的答案，以便更好地评估语言模型的能力。</li>
<li>methods: 这篇论文使用了 semi-extractive 方法，即将factual quoted spans（直接从输入源文中摘取的 span）和非factual free-text connectors（将这些 span 连接成一个完整的 passage）相结合，以生成一个全面的答案。</li>
<li>results: 经过对多个语言模型的实验， authors 发现这个任务 surprisingly 具有挑战性，表明 QuoteSum 可以用于开发和研究这种混合摘要能力。<details>
<summary>Abstract</summary>
Recently proposed long-form question answering (QA) systems, supported by large language models (LLMs), have shown promising capabilities. Yet, attributing and verifying their generated abstractive answers can be difficult, and automatically evaluating their accuracy remains an ongoing challenge.   In this work, we introduce a new QA task for answering multi-answer questions by summarizing multiple diverse sources in a semi-extractive fashion. Specifically, Semi-extractive Multi-source QA (SEMQA) requires models to output a comprehensive answer, while mixing factual quoted spans -- copied verbatim from given input sources -- and non-factual free-text connectors that glue these spans together into a single cohesive passage. This setting bridges the gap between the outputs of well-grounded but constrained extractive QA systems and more fluent but harder to attribute fully abstractive answers. Particularly, it enables a new mode for language models that leverages their advanced language generation capabilities, while also producing fine in-line attributions by-design that are easy to verify, interpret, and evaluate.   To study this task, we create the first dataset of this kind, QuoteSum, with human-written semi-extractive answers to natural and generated questions, and define text-based evaluation metrics. Experimenting with several LLMs in various settings, we find this task to be surprisingly challenging, demonstrating the importance of QuoteSum for developing and studying such consolidation capabilities.
</details>
<details>
<summary>摘要</summary>
最近提出的长形问答系统（QA），支持大型自然语言模型（LLM），已经显示了有前途的能力。然而，归功和验证生成的抽象答案是一个Difficult Challenge。在这种工作中，我们引入了一个新的问答任务，即摘要多个多样的源文，并生成一个包含多个事实引用 span 和自由文本连接器的全面答案。这种设定可以在EXTRACTIVE QA系统的输出和具有更高级别的自由文本生成能力的语言模型之间形成一个桥接。特别是，它允许语言模型利用其高级语言生成能力，同时生成易于验证、解释和评估的精准引用。为研究这个任务，我们创建了第一个这种类型的数据集，即QuoteSum，其中包含人类编写的 semi-extractive 答案，以及自然和生成的问题。我们定义了文本基于的评估指标。在不同的设定下，我们使用多种语言模型进行实验，发现这个任务实际上非常困难，这表明QuoteSum 对开发和研究这种整合能力的研究具有重要的意义。
</details></li>
</ul>
<hr>
<h2 id="LongQLoRA-Efficient-and-Effective-Method-to-Extend-Context-Length-of-Large-Language-Models"><a href="#LongQLoRA-Efficient-and-Effective-Method-to-Extend-Context-Length-of-Large-Language-Models" class="headerlink" title="LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models"></a>LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04879">http://arxiv.org/abs/2311.04879</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangjianxin1/longqlora">https://github.com/yangjianxin1/longqlora</a></li>
<li>paper_authors: Jianxin Yang</li>
<li>for: 提高大语言模型的上下文长度，并采用少量训练资源。</li>
<li>methods:  combining Position Interpolation, QLoRA和Shift Short Attention of LongLoRA，并在单个32GB V100 GPU上进行训练。</li>
<li>results: 可以将LLaMA2 7B和13B的上下文长度从4096延长到8192和更长，并在PG19和Proof-pile数据集上实现竞争性的抗抑制性表现，并且与MPT-7B-8K在评估上下文长度为8192的情况下几乎相同。<details>
<summary>Abstract</summary>
We present LongQLoRA, an efficient and effective method to extend context length of large language models with less training resources. LongQLoRA combines the advantages of Position Interpolation, QLoRA and Shift Short Attention of LongLoRA. With a single 32GB V100 GPU, LongQLoRA can extend the context length of LLaMA2 7B and 13B from 4096 to 8192 and even to 12k within 1000 finetuning steps. LongQLoRA achieves competitive perplexity performance on PG19 and Proof-pile datasets, our model outperforms LongLoRA and is very close to MPT-7B-8K within the evaluation context length of 8192. We collect and build 39k long instruction data to extend context length of Vicuna-13B from 4096 to 8192 and achieve good performance both in long and short context generation task. We also do some ablation experiments to study the effect of LoRA rank, finetuning steps and attention patterns in inference.The model weights, training data and code are avaliable at https://github.com/yangjianxin1/LongQLoRA.
</details>
<details>
<summary>摘要</summary>
我们提出了LongQLoRA，一种高效和有效的方法，可以将大语言模型的上下文长度延长，使用更少的训练资源。LongQLoRA结合了Position Interpolation、QLoRA和Shift Short Attention的优点。使用单个32GB V100 GPU，LongQLoRA可以将LLaMA2 7B和13B的上下文长度从4096提高至8192以及12k，在1000个finetuning步骤内完成。LongQLoRA在PG19和Proof-pile数据集上达到了竞争力的折射性表现，我们的模型比LongLoRA更高效，与MPT-7B-8K在评估Context length为8192的情况下几乎相当。我们收集了39k个长 instrucion数据，以延长Vicuna-13B的上下文长度从4096提高至8192，并在长和短上下文生成任务中达到了良好的表现。我们还进行了一些剥夺实验，以研究LoRA排名、finetuning步骤和注意模式在推理中的效果。模型权重、训练数据和代码可以在https://github.com/yangjianxin1/LongQLoRA中下载。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Benchmark-and-Contamination-for-Language-Models-with-Rephrased-Samples"><a href="#Rethinking-Benchmark-and-Contamination-for-Language-Models-with-Rephrased-Samples" class="headerlink" title="Rethinking Benchmark and Contamination for Language Models with Rephrased Samples"></a>Rethinking Benchmark and Contamination for Language Models with Rephrased Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04850">http://arxiv.org/abs/2311.04850</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lm-sys/llm-decontaminator">https://github.com/lm-sys/llm-decontaminator</a></li>
<li>paper_authors: Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica</li>
<li>for: 这篇论文的目的是探讨大型自然语言模型（LLM）在训练时可能会受到污染的问题，以及如何使用更强大的检测方法来解决这个问题。</li>
<li>methods: 本文使用了多种检测方法来检查LLM的训练数据是否受到污染，包括字串匹配（n-gram overlap）和简译等方法。另外，本文还提出了一个更强大的LLM-based检测方法，并将其应用于广泛使用的预训练和终端训练数据中。</li>
<li>results: 本文的实验结果显示，使用现有的检测方法可能无法彻底检测LLM的污染问题，而且简译等方法可以轻松地绕过检测方法。另外，本文还发现了一些实验数据中的污染问题，包括GPT-3.5&#x2F;4生成的 sintetic数据。总之，本文强调了需要更强大的检测方法来确保LLM的训练数据是clean的，并且呼吁了社区对于公共benchmark的使用进行更多的检测和监控。<details>
<summary>Abstract</summary>
Large language models are increasingly trained on all the data ever produced by humans. Many have raised concerns about the trustworthiness of public benchmarks due to potential contamination in pre-training or fine-tuning datasets. While most data decontamination efforts apply string matching (e.g., n-gram overlap) to remove benchmark data, we show that these methods are insufficient, and simple variations of test data (e.g., paraphrasing, translation) can easily bypass these decontamination measures. Furthermore, we demonstrate that if such variation of test data is not eliminated, a 13B model can easily overfit a test benchmark and achieve drastically high performance, on par with GPT-4. We validate such observations in widely used benchmarks such as MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a stronger LLM-based decontamination method and apply it to widely used pre-training and fine-tuning datasets, revealing significant previously unknown test overlap. For example, in pre-training sets such as RedPajama-Data-1T and StarCoder-Data, we identified that 8-18\% of the HumanEval benchmark overlaps. Interestingly, we also find such contamination in synthetic dataset generated by GPT-3.5/4, suggesting a potential risk of unintentional contamination. We urge the community to adopt stronger decontamination approaches when using public benchmarks. Moreover, we call for the community to actively develop fresh one-time exams to evaluate models accurately. Our decontamination tool is publicly available at https://github.com/lm-sys/llm-decontaminator.
</details>
<details>
<summary>摘要</summary>
大型语言模型在训练时使用了所有人类生产的数据。许多人提出了公共参考数据的可靠性问题，因为可能有污染在预训或精革 dataset 中。大多数数据净化努力使用字串匹配（例如 n-gram 重叠）移除参考数据，但我们表明这些方法是不充分的，并且简单的变体（例如重写、翻译）可以轻松地绕过这些净化措施。此外，我们显示了如果这种变体数据不被消除，则一个 13B 模型可以轻松地适应参考数据，并取得极高的性能，与 GPT-4 相似。我们在广泛使用的参考数据中进行验证这些观察，包括 MMLU、GSK8k 和 HumanEval。为了解决这个增长的风险，我们提出了一个更强的 LLM-based 净化方法，并将其应用到广泛使用的预训和精革 dataset 中，发现了 significannot 前所未知的参考数据重合。例如，在 RedPajama-Data-1T 和 StarCoder-Data 预训集中，我们发现了 8-18% 的 HumanEval 参考数据重合。 interestingly，我们也发现了这种污染在 GPT-3.5/4 生成的 sintetic 数据中，建议社区将更强的净化方法应用到公共参考数据中，以确保模型的测试性能是可靠的。此外，我们呼吁社区积极开发新的一次验证问题，以确保模型的性能是正确的。我们的净化工具已经在 GitHub 上公开，可以在 https://github.com/lm-sys/llm-decontaminator 中找到。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Semantic-Component-for-Robust-Molecular-Property-Prediction"><a href="#Identifying-Semantic-Component-for-Robust-Molecular-Property-Prediction" class="headerlink" title="Identifying Semantic Component for Robust Molecular Property Prediction"></a>Identifying Semantic Component for Robust Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04837">http://arxiv.org/abs/2311.04837</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmirlab-group/sci">https://github.com/dmirlab-group/sci</a></li>
<li>paper_authors: Zijian Li, Zunhong Xu, Ruichu Cai, Zhenhui Yang, Yuguang Yan, Zhifeng Hao, Guangyi Chen, Kun Zhang</li>
<li>for: 本研究旨在提高Graph Neural Networks（GNN）在不同数据集下的泛化能力。</li>
<li>methods: 我们提出了一种名为Semantic-Components Identifiability（SCI）的生成模型，可以将latent variable分解成semantic-relevant（SR）和semantic-irrelevant（SI）组成部分。</li>
<li>results: 我们的实验研究表明，SCI方法可以在21个数据集上 achieve state-of-the-art performance，并且可以提供更多的泛化性能。此外，我们的Visualization结果也提供了具有启发性的案例研究和预测结果的解释。<details>
<summary>Abstract</summary>
Although graph neural networks have achieved great success in the task of molecular property prediction in recent years, their generalization ability under out-of-distribution (OOD) settings is still under-explored. Different from existing methods that learn discriminative representations for prediction, we propose a generative model with semantic-components identifiability, named SCI. We demonstrate that the latent variables in this generative model can be explicitly identified into semantic-relevant (SR) and semantic-irrelevant (SI) components, which contributes to better OOD generalization by involving minimal change properties of causal mechanisms. Specifically, we first formulate the data generation process from the atom level to the molecular level, where the latent space is split into SI substructures, SR substructures, and SR atom variables. Sequentially, to reduce misidentification, we restrict the minimal changes of the SR atom variables and add a semantic latent substructure regularization to mitigate the variance of the SR substructure under augmented domain changes. Under mild assumptions, we prove the block-wise identifiability of the SR substructure and the comment-wise identifiability of SR atom variables. Experimental studies achieve state-of-the-art performance and show general improvement on 21 datasets in 3 mainstream benchmarks. Moreover, the visualization results of the proposed SCI method provide insightful case studies and explanations for the prediction results. The code is available at: https://github.com/DMIRLAB-Group/SCI.
</details>
<details>
<summary>摘要</summary>
虽然 Graf Neural Networks 在 recent 年取得了大量成功的质量预测task，但它们的Out-of-Distribution（OOD）环境下的普遍性仍然受探索。不同于现有的方法将学习探测表示，我们提出了具有semantic-components可识别的生成模型，称为SCI。我们证明了隐藏变量在这个生成模型中可以明确地分解为semantic-相关（SR）和semantic-不相关（SI）组成部分，这对于OOD普遍性做出了贡献，因为它们涉及到最小改变的causal mechanism。具体来说，我们首先将质量生成从原子层次到分子层次，其隐藏空间被拆分为SI子结构、SR子结构和SR原子变量。接着，为了降低错误识别，我们限制SR原子变量的最小改变，并将SR子结构加入semantic latent substructure regularization，以减少对于增强领域变化的变异。根据严谨的假设，我们证明了SR子结构的对����分解和SR原子变量的对����分解。实验研究获得了3大主流benchmark中的state-of-the-art表现，并在21个数据集上显示了一般提高。此外，我们的SCI方法的视觉化结果将提供了有用的案例研究和预测结果解释。SCI代码可以在以下地址获取：https://github.com/DMIRLAB-Group/SCI。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Personalized-Online-Federated-Learning"><a href="#Decentralized-Personalized-Online-Federated-Learning" class="headerlink" title="Decentralized Personalized Online Federated Learning"></a>Decentralized Personalized Online Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04817">http://arxiv.org/abs/2311.04817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renzhi Wu, Saayan Mitra, Xiang Chen, Anup Rao</li>
<li>for: 这个论文旨在提出一种新的学习设定，即分布式个性化在线学习（Decentralized Personalized Online Federated Learning，DPOEL），以满足企业端服务器（enterprise edge servers）上一些重要应用程序的需求。</li>
<li>methods: 该论文提出了两种技术挑战：首先，如何将来自邻居客户端的共享模型参数集成到本地模型中，以获得良好的本地模型性能。其次，如何选择客户端与其他客户端进行交互的邻居。该论文提出了一种基于学习权重的对等选择方法。</li>
<li>results: 该论文在三个实际项赋予预测数据集和一个空气质量预测数据集上进行了实验，并证明了其效果和可靠性。<details>
<summary>Abstract</summary>
Vanilla federated learning does not support learning in an online environment, learning a personalized model on each client, and learning in a decentralized setting. There are existing methods extending federated learning in each of the three aspects. However, some important applications on enterprise edge servers (e.g. online item recommendation at global scale) involve the three aspects at the same time. Therefore, we propose a new learning setting \textit{Decentralized Personalized Online Federated Learning} that considers all the three aspects at the same time.   In this new setting for learning, the first technical challenge is how to aggregate the shared model parameters from neighboring clients to obtain a personalized local model with good performance on each client. We propose to directly learn an aggregation by optimizing the performance of the local model with respect to the aggregation weights. This not only improves personalization of each local model but also helps the local model adapting to potential data shift by intelligently incorporating the right amount of information from its neighbors. The second challenge is how to select the neighbors for each client. We propose a peer selection method based on the learned aggregation weights enabling each client to select the most helpful neighbors and reduce communication cost at the same time. We verify the effectiveness and robustness of our proposed method on three real-world item recommendation datasets and one air quality prediction dataset.
</details>
<details>
<summary>摘要</summary>
vanilla federated learning 不支持在线学习、学习每个客户端上的个性化模型，以及分布式设置下学习。现有的方法可以在每个方面进行扩展。然而，一些重要的企业端服务器应用（例如，全球范围内的在线项目推荐）需要同时考虑这三个方面。因此，我们提出了一种新的学习设定——分布式个性化在线 federated learning，它同时考虑了这三个方面。在这种新的学习设定中，技术挑战之一是如何将来自邻居客户端的共享模型参数集成到每个客户端上，以获得个性化的本地模型。我们提议直接通过优化本地模型的性能来学习权重。这不仅提高了每个本地模型的个性化程度，还帮助本地模型适应数据变化，通过智能地包含邻居客户端的信息来适应可能出现的数据变化。另一个挑战是如何选择每个客户端的邻居。我们提议基于学习的权重来选择邻居，使每个客户端可以选择最有助的邻居，同时降低通信成本。我们对三个实际的 item recommendation 数据集和一个空气质量预测数据集进行了验证和robustness测试，结果表明我们的方法是有效和可靠的。
</details></li>
</ul>
<hr>
<h2 id="MTGER-Multi-view-Temporal-Graph-Enhanced-Temporal-Reasoning-over-Time-Involved-Document"><a href="#MTGER-Multi-view-Temporal-Graph-Enhanced-Temporal-Reasoning-over-Time-Involved-Document" class="headerlink" title="MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document"></a>MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04816">http://arxiv.org/abs/2311.04816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Chu, Zekun Wang, Jiafeng Liang, Ming Liu, Bing Qin</li>
<li>for: 这个论文是用于解决文档中的时间关系和推理问题的。</li>
<li>methods: 该论文提出了一种多视图时间图加强的时间推理框架（MTGER），该框架可以Explicitly模型文档中的时间关系，并通过多视图机制和自动融合来提高模型的隐式推理能力。</li>
<li>results: 实验结果表明，MTGER可以在TimeQA和SituatedQA datasets上达到显著的效果，并且在问题变化时能够给出更一致的答案。<details>
<summary>Abstract</summary>
The facts and time in the document are intricately intertwined, making temporal reasoning over documents challenging. Previous work models time implicitly, making it difficult to handle such complex relationships. To address this issue, we propose MTGER, a novel Multi-view Temporal Graph Enhanced Temporal Reasoning framework for temporal reasoning over time-involved documents. Concretely, MTGER explicitly models the temporal relationships among facts by multi-view temporal graphs. On the one hand, the heterogeneous temporal graphs explicitly model the temporal and discourse relationships among facts; on the other hand, the multi-view mechanism captures both time-focused and fact-focused information, allowing the two views to complement each other through adaptive fusion. To further improve the implicit reasoning capability of the model, we design a self-supervised time-comparing objective. Extensive experimental results demonstrate the effectiveness of our method on the TimeQA and SituatedQA datasets. Furthermore, MTGER gives more consistent answers under question perturbations.
</details>
<details>
<summary>摘要</summary>
文档中的事实和时间关系紧密相连，使得文档中的时间逻辑推理变得困难。前期工作中的模型对时间进行了隐式表示，导致处理复杂的时间关系变得困难。为解决这个问题，我们提议MTGER，一种基于多视图时间图的新型多视图时间图增强的时间逻辑推理框架。具体来说，MTGER使用多视图时间图来明确事实之间的时间关系。一方面，不同视图中的时间图表示了事实之间的时间和论述关系；另一方面，多视图机制使得时间和事实信息相互补充，通过适应融合来增强模型的隐式逻辑能力。此外，我们还设计了一个自动supervised时间比较目标，以提高模型的隐式逻辑能力。实验结果表明，MTGER在TimeQA和SituatedQA datasets上具有显著的效果，并且在问题扰动下的答案更加一致。
</details></li>
</ul>
<hr>
<h2 id="DACBERT-Leveraging-Dependency-Agreement-for-Cost-Efficient-Bert-Pretraining"><a href="#DACBERT-Leveraging-Dependency-Agreement-for-Cost-Efficient-Bert-Pretraining" class="headerlink" title="DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert Pretraining"></a>DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04799">http://arxiv.org/abs/2311.04799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sw-packages/fa101e30ca4ffd6a0479993b0e1c7299d2311c0416c0b68e2551534430e1e8fe">https://github.com/sw-packages/fa101e30ca4ffd6a0479993b0e1c7299d2311c0416c0b68e2551534430e1e8fe</a></li>
<li>paper_authors: Martin Kuo, Jianyi Zhang, Yiran Chen</li>
<li>for: 提高预训练模型的性能和可解性，以及增强自然语言理解任务中预训练模型的表现。</li>
<li>methods: 提出了一种新的预训练模型——依赖协议拟合BERT（DACBERT），并开发了一种两阶段预训练方框架——依赖协议预训练。这个方框架基于语言理论，将语法和 semantics信息灵活地纳入预训练过程中。第一阶段使用四个专门的子模型来捕捉chunk级别的代表性依赖关系，并将这些依赖关系转化为嵌入。第二阶段使用这些精细化的嵌入，与传统的BERT嵌入相结合，导向预训练 осталь部分的模型。</li>
<li>results: 在GLUE测试 benchmark上，我们的DACBERT表现出色，在不同任务中的表现都有所提高，比Crammed BERT提高3.13%的RTE任务和2.26%的MRPC任务。此外，我们的方法可以在单个GPU上，在24小时内完成预训练过程，不需要额外的计算资源或延长预训练时间。广泛的研究还证明了我们的方法在自然语言理解任务中的表现是可靠的。<details>
<summary>Abstract</summary>
Building on the cost-efficient pretraining advancements brought about by Crammed BERT, we enhance its performance and interpretability further by introducing a novel pretrained model Dependency Agreement Crammed BERT (DACBERT) and its two-stage pretraining framework - Dependency Agreement Pretraining. This framework, grounded by linguistic theories, seamlessly weaves syntax and semantic information into the pretraining process. The first stage employs four dedicated submodels to capture representative dependency agreements at the chunk level, effectively converting these agreements into embeddings. The second stage uses these refined embeddings, in tandem with conventional BERT embeddings, to guide the pretraining of the rest of the model. Evaluated on the GLUE benchmark, our DACBERT demonstrates notable improvement across various tasks, surpassing Crammed BERT by 3.13% in the RTE task and by 2.26% in the MRPC task. Furthermore, our method boosts the average GLUE score by 0.83%, underscoring its significant potential. The pretraining process can be efficiently executed on a single GPU within a 24-hour cycle, necessitating no supplementary computational resources or extending the pretraining duration compared with the Crammed BERT. Extensive studies further illuminate our approach's instrumental role in bolstering the interpretability of pretrained language models for natural language understanding tasks.
</details>
<details>
<summary>摘要</summary>
基于Cost-efficient pre-training的进步，我们又提出了一种新的预训练模型——Dependency Agreement Crammed BERT（DACBERT）和其两个阶段预训练框架——Dependency Agreement Pretraining。这个框架，基于语言理论，通过将 sintax和semantic信息灵活地整合到预训练过程中来提高模型的性能和可解性。第一个阶段使用四个专门的子模型来捕捉chunk级别的代名词协议，并将其转化为嵌入。第二个阶段使用这些精炼的嵌入，与普通的BERT嵌入一起，导引预训练其余部分的模型。在GLUE标准测试 benchmark上，我们的DACBERT表现出色，在RTE任务上超过Crammed BERT by 3.13%，在MRPC任务上超过by 2.26%。此外，我们的方法提高了GLUE平均分数 by 0.83%，强调其显著的潜力。预训练过程可以在单个GPU上完成 within a 24-hour cycle，不需要补充的计算资源或延长预训练时间与Crammed BERT相比。广泛的研究也证明了我们的方法在自然语言理解任务中增强预训练语言模型的可解性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Multiple-Roles-of-Ontologies-in-Explainable-AI"><a href="#On-the-Multiple-Roles-of-Ontologies-in-Explainable-AI" class="headerlink" title="On the Multiple Roles of Ontologies in Explainable AI"></a>On the Multiple Roles of Ontologies in Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04778">http://arxiv.org/abs/2311.04778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roberto Confalonieri, Giancarlo Guizzardi</li>
<li>for: 这篇论文探讨了ontology在可解释AI和人类中心的解释系统中的不同角色。</li>
<li>methods: 论文考虑了三个主要的ontology应用角度，包括参考模型、通用常识逻辑和知识精简和复杂性管理。</li>
<li>results: 论文结论提出了 Ontology-based 方法可以帮助解释AI 的人类理解和效果，但还需要解决一些挑战。<details>
<summary>Abstract</summary>
This paper discusses the different roles that explicit knowledge, in particular ontologies, can play in Explainable AI and in the development of human-centric explainable systems and intelligible explanations. We consider three main perspectives in which ontologies can contribute significantly, namely reference modelling, common-sense reasoning, and knowledge refinement and complexity management. We overview some of the existing approaches in the literature, and we position them according to these three proposed perspectives. The paper concludes by discussing what challenges still need to be addressed to enable ontology-based approaches to explanation and to evaluate their human-understandability and effectiveness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Vital-Sign-Forecasting-for-Sepsis-Patients-in-ICUs"><a href="#Vital-Sign-Forecasting-for-Sepsis-Patients-in-ICUs" class="headerlink" title="Vital Sign Forecasting for Sepsis Patients in ICUs"></a>Vital Sign Forecasting for Sepsis Patients in ICUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04770">http://arxiv.org/abs/2311.04770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anubhav Bhatti, Yuwei Liu, Chen Dan, Bingjie Shen, San Lee, Yonghwan Kim, Jang Yong Kim</li>
<li>for: 预测Intensive Care Units（ICU）中病人的生命体征指标，帮助医疗工作者早发现生命体征不稳定的迹象并预测 septic shock 的发展</li>
<li>methods: 使用现代深度学习（DL）架构，开发了一种多步预测系统，利用历史生命体征数据预测未来的生命体征状况</li>
<li>results: 比较了三种DL模型（N-BEATS、N-HiTS、Temporal Fusion Transformer）在 eICU Collaborative Research Database 上的预测能力，发现 TFT 模型能够更好地捕捉生命体征趋势，而 N-HiTS 模型能够更好地保持生命体征短期波动在预定范围内<details>
<summary>Abstract</summary>
Sepsis and septic shock are a critical medical condition affecting millions globally, with a substantial mortality rate. This paper uses state-of-the-art deep learning (DL) architectures to introduce a multi-step forecasting system to predict vital signs indicative of septic shock progression in Intensive Care Units (ICUs). Our approach utilizes a short window of historical vital sign data to forecast future physiological conditions. We introduce a DL-based vital sign forecasting system that predicts up to 3 hours of future vital signs from 6 hours of past data. We further adopt the DILATE loss function to capture better the shape and temporal dynamics of vital signs, which are critical for clinical decision-making. We compare three DL models, N-BEATS, N-HiTS, and Temporal Fusion Transformer (TFT), using the publicly available eICU Collaborative Research Database (eICU-CRD), highlighting their forecasting capabilities in a critical care setting. We evaluate the performance of our models using mean squared error (MSE) and dynamic time warping (DTW) metrics. Our findings show that while TFT excels in capturing overall trends, N-HiTS is superior in retaining short-term fluctuations within a predefined range. This paper demonstrates the potential of deep learning in transforming the monitoring systems in ICUs, potentially leading to significant improvements in patient care and outcomes by accurately forecasting vital signs to assist healthcare providers in detecting early signs of physiological instability and anticipating septic shock.
</details>
<details>
<summary>摘要</summary>
septic shock 和 septic shock 是一种严重的医疗情况，影响全球数百万人，mortality rate 较高。这篇论文使用当前最先进的深度学习（DL）建筑，引入一种多步预测系统，以预测 ICU 中重要的生物参数，以便评估 septic shock 的进程。我们的方法使用一个短时间的历史生物参数数据，预测未来的生物参数。我们引入了 DL 基于的生物参数预测系统，可以预测未来 3 小时的生物参数，从 6 小时的历史数据中。我们进一步采用 DILATE 损失函数，以更好地捕捉生物参数的形态和时间动态，这些是临床决策中的关键。我们使用公共可用的 eICU 合作研究数据库（eICU-CRD），比较三种 DL 模型（N-BEATS、N-HiTS 和 Temporal Fusion Transformer ）的预测能力。我们使用 mean squared error（MSE）和 dynamic time warping（DTW） метри来评估我们的模型。我们的发现显示，虽然 TFT 能够捕捉总趋势，但 N-HiTS 在保持短期波动内的范围内表现更优。这篇论文示出了深度学习在 ICU 监测系统中的潜在潜力，可能导致患者护理和结果的显著改善，通过准确预测生物参数，帮助医疗提供者早期检测生物参数的不稳定，预测 septic shock。
</details></li>
</ul>
<hr>
<h2 id="The-voraus-AD-Dataset-for-Anomaly-Detection-in-Robot-Applications"><a href="#The-voraus-AD-Dataset-for-Anomaly-Detection-in-Robot-Applications" class="headerlink" title="The voraus-AD Dataset for Anomaly Detection in Robot Applications"></a>The voraus-AD Dataset for Anomaly Detection in Robot Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04765">http://arxiv.org/abs/2311.04765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vorausrobotik/voraus-ad-dataset">https://github.com/vorausrobotik/voraus-ad-dataset</a></li>
<li>paper_authors: Jan Thieß Brockmann, Marco Rudolph, Bodo Rosenhahn, Bastian Wandt</li>
<li>For: This paper aims to provide a dataset for anomaly detection (AD) in robotic applications, and to introduce a new baseline method called MVT-Flow that outperforms previous baselines by a large margin.* Methods: The paper uses machine data from a pick-and-place application to create a dataset for AD, and introduces MVT-Flow, a deep-learning-based density estimation method that takes the structure of the data domain into account.* Results: The paper shows that MVT-Flow outperforms previous baselines by a large margin of 6.2% in area under ROC.Here is the text in Simplified Chinese:* For: 这篇论文目的是为了提供机器人应用中的异常检测（AD）数据集，并引入一种新的基线方法called MVT-Flow，该方法在ROC领域的表现明显超过了之前的基线方法。* Methods: 论文使用机器人执行pick-and-place任务的机器数据创建了AD数据集，并引入MVT-Flow方法，该方法是基于深度学习的概率分布预测方法，它采用了数据领域的结构来 tailor its architecture。* Results: 论文表明，MVT-Flow方法在ROC领域的表现比之前的基线方法高出6.2%的差。<details>
<summary>Abstract</summary>
During the operation of industrial robots, unusual events may endanger the safety of humans and the quality of production. When collecting data to detect such cases, it is not ensured that data from all potentially occurring errors is included as unforeseeable events may happen over time. Therefore, anomaly detection (AD) delivers a practical solution, using only normal data to learn to detect unusual events. We introduce a dataset that allows training and benchmarking of anomaly detection methods for robotic applications based on machine data which will be made publicly available to the research community. As a typical robot task the dataset includes a pick-and-place application which involves movement, actions of the end effector and interactions with the objects of the environment. Since several of the contained anomalies are not task-specific but general, evaluations on our dataset are transferable to other robotics applications as well. Additionally, we present MVT-Flow (multivariate time-series flow) as a new baseline method for anomaly detection: It relies on deep-learning-based density estimation with normalizing flows, tailored to the data domain by taking its structure into account for the architecture. Our evaluation shows that MVT-Flow outperforms baselines from previous work by a large margin of 6.2% in area under ROC.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Euclidean-Projective-Conformal-Choosing-a-Geometric-Algebra-for-Equivariant-Transformers"><a href="#Euclidean-Projective-Conformal-Choosing-a-Geometric-Algebra-for-Equivariant-Transformers" class="headerlink" title="Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers"></a>Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04744">http://arxiv.org/abs/2311.04744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pim de Haan, Taco Cohen, Johann Brehmer</li>
<li>for: 该论文旨在开发一种基于几何深度学习的灵活架构，即几何深度学习变换器（GATr）。</li>
<li>methods: 该论文使用几何代数来扩展GATr架构，使其适用于任何几何（或CLIFFORD）代数。作者还研究了使用不同几何代数的版本，包括几何代数、 проектив代数和对称代数，来表示3D数据。</li>
<li>results: 作者在理论和实践中评估了这些不同版本，发现 simplest几何版本 computationally cheap，但 symmetry group smaller，不够表达能力，而 projective model 表达能力不够。对称代数和改进的 проектив代数定义出了强大、高性能的架构。<details>
<summary>Abstract</summary>
The Geometric Algebra Transformer (GATr) is a versatile architecture for geometric deep learning based on projective geometric algebra. We generalize this architecture into a blueprint that allows one to construct a scalable transformer architecture given any geometric (or Clifford) algebra. We study versions of this architecture for Euclidean, projective, and conformal algebras, all of which are suited to represent 3D data, and evaluate them in theory and practice. The simplest Euclidean architecture is computationally cheap, but has a smaller symmetry group and is not as sample-efficient, while the projective model is not sufficiently expressive. Both the conformal algebra and an improved version of the projective algebra define powerful, performant architectures.
</details>
<details>
<summary>摘要</summary>
“几何深度学习构件（GATr）是一种多功能的构件，基于射影几何代数。我们将这个构件转换为可扩展的构件，让你可以根据任何几何（或Clifford）代数建立扩展性强的 transformer 架构。我们研究了这些架构的几何、 проектив和对称 algebra 版本，这些版本都适合表示3D数据，并进行了理论和实践评估。最简单的几何架构较便宜计算，但它的同调集小，不够sample-efficient，而 проектив模型不够表达力。对称 algebra 和改进的 projetive algebra 定义了强大、高性能的架构。”Note that Simplified Chinese is a simplified version of Chinese that is used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world where traditional Chinese is prevalent.
</details></li>
</ul>
<hr>
<h2 id="The-Quest-for-Content-A-Survey-of-Search-Based-Procedural-Content-Generation-for-Video-Games"><a href="#The-Quest-for-Content-A-Survey-of-Search-Based-Procedural-Content-Generation-for-Video-Games" class="headerlink" title="The Quest for Content: A Survey of Search-Based Procedural Content Generation for Video Games"></a>The Quest for Content: A Survey of Search-Based Procedural Content Generation for Video Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04710">http://arxiv.org/abs/2311.04710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mar Zamorano, Carlos Cetina, Federica Sarro</li>
<li>for: 游戏内容的大量生成，以满足日益增长的游戏需求。</li>
<li>methods: 使用搜索算法实现自动化内容生成。</li>
<li>results: 对SBPCG领域的现状和未来研究方向的报告，以及一些实践者可采取的建议。<details>
<summary>Abstract</summary>
Video games demand is constantly increasing, which requires the costly production of large amounts of content. Towards this challenge, researchers have developed Search-Based Procedural Content Generation (SBPCG), that is, the (semi-)automated creation of content through search algorithms. We survey the current state of SBPCG, reporting work appeared in the field between 2011-2022 and identifying open research challenges. The results lead to recommendations for practitioners and to the identification of several potential future research avenues for SBPCG.
</details>
<details>
<summary>摘要</summary>
电子游戏的需求不断增长，需要大量的内容生成，而这也导致了高昂的生产成本。为了应对这个挑战，研究人员开发了搜索基于生成内容的技术（Search-Based Procedural Content Generation，SBPCG），即通过搜索算法（semi-)自动生成内容。我们对SBPCG领域的当前状况进行了报告，涵盖2011-2022年间出版的研究成果，并确定了一些未解决的研究挑战和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Challenging-Common-Assumptions-in-Multi-task-Learning"><a href="#Challenging-Common-Assumptions-in-Multi-task-Learning" class="headerlink" title="Challenging Common Assumptions in Multi-task Learning"></a>Challenging Common Assumptions in Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04698">http://arxiv.org/abs/2311.04698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott</li>
<li>for: 本研究探讨多任务学习（MTL）下的下降搜索方法，尤其是在单任务学习（STL）基础上的情况下。</li>
<li>methods: 本研究使用了常用的STL工具，如Adam优化器，并证明Adam优化器在MTL中的效iveness归功于部分损失度量的兼容性。此外，本研究还研究了梯度冲突的角色在MTL和STL中，并发现梯度强度作为主要 отли异点。</li>
<li>results: 对于常见的图像损害，本研究未发现MTL对特征传递性的明显优势。总的来说，本研究发现MTL和STL在一些方面存在相似之处，建议在更广泛的上下文中考虑这两种方法。<details>
<summary>Abstract</summary>
While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge common assumptions in MTL in the context of STL: First, the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL. We deduce the effectiveness of Adam to its partial loss-scale invariance. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no evidence that this is a unique problem in MTL. We emphasize differences in gradient magnitude as the main distinguishing factor. Lastly, we compare the transferability of features learned through MTL and STL on common image corruptions, and find no conclusive evidence that MTL leads to superior transferability. Overall, we find surprising similarities between STL and MTL suggesting to consider methods from both fields in a broader context.
</details>
<details>
<summary>摘要</summary>
MTL（多任务学习）在最近几年内得到了广泛关注，但它的内在机制仍未得到了充分理解。现有方法不能够在单任务学习（STL）基础上实现一致性的性能提升，这重申了对MTL的深入理解的重要性。在我们的研究中，我们挑战了MTL中通常被假设的一些假设：首先，MTL中选择优化器的研究只是轻度地进行了调查。我们表明了通用STL工具such as Adam优化器在MTL中的重要作用。我们发现Adam的有效性归功于它的部分损失尺度不变性。其次，在MTL中 gradient conflicts 的概念经常被宣称为特定的问题。我们探讨了MTL中 gradient conflicts 的角色，并与 STL 进行比较。对于 angular gradient alignment，我们未能发现这是MTL中独特的问题。我们强调了 gradient magnitude 的差异作为主要 отлича点。最后，我们比较了通过 MTL 和 STL 学习的特征的传输性，并未发现MTL leads to superior transferability。总的来说，我们发现了MTL 和 STL 之间的意外相似之处，建议在更广泛的上下文中考虑这两个领域的方法。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Speculative-Sampling-and-KV-Cache-Optimizations-Together-for-Generative-AI-using-OpenVINO"><a href="#Leveraging-Speculative-Sampling-and-KV-Cache-Optimizations-Together-for-Generative-AI-using-OpenVINO" class="headerlink" title="Leveraging Speculative Sampling and KV-Cache Optimizations Together for Generative AI using OpenVINO"></a>Leveraging Speculative Sampling and KV-Cache Optimizations Together for Generative AI using OpenVINO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04951">http://arxiv.org/abs/2311.04951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openvinotoolkit/openvino_notebooks">https://github.com/openvinotoolkit/openvino_notebooks</a></li>
<li>paper_authors: Haim Barad, Ekaterina Aidova, Yury Gorbachev</li>
<li>for: 提高用户体验和减少基础设施成本和能耗</li>
<li>methods: 使用幻数据批处理和量化优化</li>
<li>results: 提高文本生成响应时间，并与标准抽样相比较Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to improve the user experience and reduce infrastructure costs and power consumption by optimizing text generation using inference optimizations.</li>
<li>methods: The paper proposes using speculative sampling, a form of dynamic execution, to reduce the overall latency of text generation. The authors also use model-based optimizations such as quantization and KV caching.</li>
<li>results: The paper compares the performance of speculative sampling with standard autoregressive sampling and shows that speculative sampling can improve the response time of text generation.<details>
<summary>Abstract</summary>
Inference optimizations are critical for improving user experience and reducing infrastructure costs and power consumption. In this article, we illustrate a form of dynamic execution known as speculative sampling to reduce the overall latency of text generation and compare it with standard autoregressive sampling. This can be used together with model-based optimizations (e.g. quantization) to provide an optimized solution. Both sampling methods make use of KV caching. A Jupyter notebook and some sample executions are provided.
</details>
<details>
<summary>摘要</summary>
推理优化是提高用户体验和减少基础设施成本和电力消耗的关键。在这篇文章中，我们介绍了一种动态执行技术known as speculative sampling，用于减少文本生成总延迟。我们还与标准排取样本相比较。这两种抽样方法都使用KV缓存。我们提供了一个Jupyter笔记和一些示例执行。
</details></li>
</ul>
<hr>
<h2 id="Diff-HierVC-Diffusion-based-Hierarchical-Voice-Conversion-with-Robust-Pitch-Generation-and-Masked-Prior-for-Zero-shot-Speaker-Adaptation"><a href="#Diff-HierVC-Diffusion-based-Hierarchical-Voice-Conversion-with-Robust-Pitch-Generation-and-Masked-Prior-for-Zero-shot-Speaker-Adaptation" class="headerlink" title="Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation"></a>Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04693">http://arxiv.org/abs/2311.04693</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hayeong0/Diff-HierVC">https://github.com/hayeong0/Diff-HierVC</a></li>
<li>paper_authors: Ha-Yeong Choi, Sang-Hoon Lee, Seong-Whan Lee</li>
<li>for: 提高voice conversion（VC）系统的精度和声音适应质量</li>
<li>methods: 基于两个扩散模型的层次VC系统（Diff-HierVC），包括DiffPitch和DiffVoice两部分</li>
<li>results: 实验结果表明，our模型在抽象F0生成和声音风格转换方面具有优异表现，并在零基elineVC场景中达到CER&#x3D;0.83%和EER&#x3D;3.29%的性能。<details>
<summary>Abstract</summary>
Although voice conversion (VC) systems have shown a remarkable ability to transfer voice style, existing methods still have an inaccurate pitch and low speaker adaptation quality. To address these challenges, we introduce Diff-HierVC, a hierarchical VC system based on two diffusion models. We first introduce DiffPitch, which can effectively generate F0 with the target voice style. Subsequently, the generated F0 is fed to DiffVoice to convert the speech with a target voice style. Furthermore, using the source-filter encoder, we disentangle the speech and use the converted Mel-spectrogram as a data-driven prior in DiffVoice to improve the voice style transfer capacity. Finally, by using the masked prior in diffusion models, our model can improve the speaker adaptation quality. Experimental results verify the superiority of our model in pitch generation and voice style transfer performance, and our model also achieves a CER of 0.83% and EER of 3.29% in zero-shot VC scenarios.
</details>
<details>
<summary>摘要</summary>
尽管voice conversion（VC）系统已经表现出了很好的语音风格传递能力，现有的方法仍然具有不准确的抑音和低端应用质量。为解决这些挑战，我们提出了Diff-HierVC，一种基于两个扩散模型的层次VC系统。我们首先引入DiffPitch，它可以有效地生成F0目标声音风格。然后，生成的F0被 fed到DiffVoice中，用来转换语音为目标声音风格。此外，通过源-滤波器编码器，我们分离出语音，并使用转换后的Mel-spectrogram作为数据驱动的先验知识来提高voice style转换能力。最后，通过在扩散模型中使用假标记先验，我们的模型可以提高 speaker adaptation质量。实验结果证明我们的模型在抑音和voice style转换性能方面具有优势，并且在零shot VC场景下 achieved CER of 0.83%和EER of 3.29%。
</details></li>
</ul>
<hr>
<h2 id="Pre-training-LLMs-using-human-like-development-data-corpus"><a href="#Pre-training-LLMs-using-human-like-development-data-corpus" class="headerlink" title="Pre-training LLMs using human-like development data corpus"></a>Pre-training LLMs using human-like development data corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04666">http://arxiv.org/abs/2311.04666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khushi Bhardwaj, Raj Sanjay Shah, Sashank Varma</li>
<li>for: 这个论文的目的是测试大型自然语言模型（LLMs）在语言理解和推理任务中的表现，以及模型在不同的训练环境下的可重复性和稳定性。</li>
<li>methods: 这个论文使用了大量的raw文本数据进行预训练，并对LLMs进行了评估，以评估模型在不同的训练环境下的表现。</li>
<li>results: 论文提出了一系列的基准值，包括不同架构、评估 epochs 的变化和报告的预训练 metric，以及对RoBERTa 基线值的评估。<details>
<summary>Abstract</summary>
Pre-trained Large Language Models (LLMs) have shown success in a diverse set of language inference and understanding tasks. The pre-training stage of LLMs looks at a large corpus of raw textual data. The BabyLM shared task compares LLM pre-training to human language acquisition, where the number of tokens seen by 13-year-old kids is magnitudes smaller than the number of tokens seen by LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn contextual word representations using roughly the same number of tokens as seen by children. We provide a strong set of baselines; with different architectures, evaluation of changes in performance across epochs, and reported pre-training metrics for the strict small and strict tracks of the task. We also try to loosely replicate the RoBERTa baseline given by the task organizers to observe the training robustness to hyperparameter selection and replicability. We provide the submission details to the strict and strict-small tracks in this report.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在多种语言推理和理解任务中表现出色。LLMs的预训阶段将关注大量的原始文本数据。在这个工作中，我们将LLMs预训和评估其能够学习上下文 word 表现。我们使用相似数量的字元与13岁儿童所看到的字元进行比较。我们提供了强大的基准点，包括不同架构、评估改变过程中的表现、和预训中的 metric。我们还尝试复制RoBERTa基eline，以观察对于数据选择和可重现性的训练稳定性。我们在这份报告中提供了预训和紧缩小 tracks 的提交细节。
</details></li>
</ul>
<hr>
<h2 id="Pragmatic-Reasoning-Unlocks-Quantifier-Semantics-for-Foundation-Models"><a href="#Pragmatic-Reasoning-Unlocks-Quantifier-Semantics-for-Foundation-Models" class="headerlink" title="Pragmatic Reasoning Unlocks Quantifier Semantics for Foundation Models"></a>Pragmatic Reasoning Unlocks Quantifier Semantics for Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04659">http://arxiv.org/abs/2311.04659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyuan Li, Rakesh R. Menon, Sayan Ghosh, Shashank Srivastava</li>
<li>for: This paper aims to explore the ability of recent foundation models to understand generalized quantifiers in natural language, specifically in sentences featuring percentage-equipped predicates.</li>
<li>methods: The paper uses a crowd-sourced dataset of human-annotated generalized quantifiers in Wikipedia sentences, called QuRe, and a framework called PRESQUE, which combines natural language inference and the Rational Speech Acts framework, to test the ability of language models to understand quantifier percentage scopes.</li>
<li>results: The experimental results on the HVD dataset and QuRe show that PRESQUE, which uses pragmatic reasoning, performs 20% better than a literal reasoning baseline when predicting quantifier percentage scopes, with no additional training required.<details>
<summary>Abstract</summary>
Generalized quantifiers (e.g., few, most) are used to indicate the proportions predicates are satisfied (for example, some apples are red). One way to interpret quantifier semantics is to explicitly bind these satisfactions with percentage scopes (e.g., 30%-40% of apples are red). This approach can be helpful for tasks like logic formalization and surface-form quantitative reasoning (Gordon and Schubert, 2010; Roy et al., 2015). However, it remains unclear if recent foundation models possess this ability, as they lack direct training signals. To explore this, we introduce QuRe, a crowd-sourced dataset of human-annotated generalized quantifiers in Wikipedia sentences featuring percentage-equipped predicates. We explore quantifier comprehension in language models using PRESQUE, a framework that combines natural language inference and the Rational Speech Acts framework. Experimental results on the HVD dataset and QuRe illustrate that PRESQUE, employing pragmatic reasoning, performs 20% better than a literal reasoning baseline when predicting quantifier percentage scopes, with no additional training required.
</details>
<details>
<summary>摘要</summary>
通用量词（例如，很少、大多数）用于指示逻辑 predicate 满足的порпорzioni（例如，一些苹果是红色的）。一种方法可以理解量词 semantics 是通过显式绑定这些满足情况的百分比范围（例如，30%-40% 的苹果是红色的）。这种方法可以对逻辑ormalization和表面形量化理性（Gordon 和Schubert，2010；Roy 等，2015）进行帮助。然而，是否现代基础模型拥有这种能力仍然未知，因为它们缺乏直接训练信号。为了探索这一点，我们引入 QuRe，一个由人工标注的通用量词在Wikipedia句子中出现的百分比范围的数据集。我们使用 PRESQUE，一个结合自然语言推理和理性演讲框架的框架，来探索语言模型中量词理解的能力。实验结果表明，PRESQUE，通过使用 Pragmatic Reasoning，在 HVD 数据集和 QuRe 上预测量词百分比范围时，与 Literal Reasoning 基准相比，提高了20%的性能，无需额外训练。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Focal-and-Full-Range-Attention-Based-Graph-Transformers"><a href="#Hybrid-Focal-and-Full-Range-Attention-Based-Graph-Transformers" class="headerlink" title="Hybrid Focal and Full-Range Attention Based Graph Transformers"></a>Hybrid Focal and Full-Range Attention Based Graph Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04653">http://arxiv.org/abs/2311.04653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minhong Zhu, Zhenhao Zhao, Weiran Cai</li>
<li>for: 这篇论文旨在提高图structured数据学习中Graph Transformer的性能，通过增强对本地信息的捕捉和全范围相关性的学习。</li>
<li>methods: 该论文提出了一种新的具有复合注意力机制的强化图Transformer模型，named Focal and Full-Range Graph Transformer (FFGT)，通过结合全范围注意力和K-hop焦点注意力来捕捉全范围和本地信息。</li>
<li>results: 该论文在多个开放数据集上提高了现有的Graph Transformer性能，同时在一些Long-Range Graph Benchmark (LRGB)数据集上达到了与普通Transformer相同的SOTA性能，而无需任何特殊的参数调整或特定的数据预处理。<details>
<summary>Abstract</summary>
The paradigm of Transformers using the self-attention mechanism has manifested its advantage in learning graph-structured data. Yet, Graph Transformers are capable of modeling full range dependencies but are often deficient in extracting information from locality. A common practice is to utilize Message Passing Neural Networks (MPNNs) as an auxiliary to capture local information, which however are still inadequate for comprehending substructures. In this paper, we present a purely attention-based architecture, namely Focal and Full-Range Graph Transformer (FFGT), which can mitigate the loss of local information in learning global correlations. The core component of FFGT is a new mechanism of compound attention, which combines the conventional full-range attention with K-hop focal attention on ego-nets to aggregate both global and local information. Beyond the scope of canonical Transformers, the FFGT has the merit of being more substructure-aware. Our approach enhances the performance of existing Graph Transformers on various open datasets, while achieves compatible SOTA performance on several Long-Range Graph Benchmark (LRGB) datasets even with a vanilla transformer. We further examine influential factors on the optimal focal length of attention via introducing a novel synthetic dataset based on SBM-PATTERN.
</details>
<details>
<summary>摘要</summary>
“对于使用自我注意机制的Transformers模型，它在学习图像数据中表现出了优势。然而，图像Transformers可以模型全范围的相互关系，但通常缺乏从本地获取信息的能力。为了解决这个问题，常用Message Passing Neural Networks（MPNNs）作为辅助，以便捕捉本地信息，但这些MPNNs仍然无法彻底理解子结构。在这篇论文中，我们提出了一个纯注意 mechanism的架构，即全范围注意和K-hop焦点注意的复合注意机制（FFGT），以便储存全范围和本地信息。与传统Transformers不同的是，FFGT更加注重到子结构。我们的方法可以提高现有的图像Transformers的性能，并在多个Long-Range Graph Benchmark（LRGB）dataset上实现了Compatible SOTA的性能，甚至使用普通的Transformer。我们进一步研究了注意力的最佳焦点因素，并通过引入一个基于SBM-PATTERN的新 sintetic dataset。”
</details></li>
</ul>
<hr>
<h2 id="SKU-Patch-Towards-Efficient-Instance-Segmentation-for-Unseen-Objects-in-Auto-Store"><a href="#SKU-Patch-Towards-Efficient-Instance-Segmentation-for-Unseen-Objects-in-Auto-Store" class="headerlink" title="SKU-Patch: Towards Efficient Instance Segmentation for Unseen Objects in Auto-Store"></a>SKU-Patch: Towards Efficient Instance Segmentation for Unseen Objects in Auto-Store</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04645">http://arxiv.org/abs/2311.04645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biqi Yang, Weiliang Tang, Xiaojie Gao, Xianzhi Li, Yun-Hui Liu, Chi-Wing Fu, Pheng-Ann Heng</li>
<li>for: 这篇论文主要针对大规模库存中的自动采矿领域问题，旨在提供一个新的patch-guided实例分割方案，以减少人工干预和模型重训。</li>
<li>methods: 本文提出了一个 novel transformer-based网络，包括(i)一个patch-image相联 corrle encoder，用于捕捉多个层次的图像特征，并(ii)一个patch-aware transformer decoder，用于生成实例 mask。</li>
<li>results: 实验结果显示，SKU-Patch可以在四个库存测试 benchmark上取得最佳性能，并在一个真实的自动存储运输管线中，实现了逾90%的抓捕成功率，证明其实用性和可行性。<details>
<summary>Abstract</summary>
In large-scale storehouses, precise instance masks are crucial for robotic bin picking but are challenging to obtain. Existing instance segmentation methods typically rely on a tedious process of scene collection, mask annotation, and network fine-tuning for every single Stock Keeping Unit (SKU). This paper presents SKU-Patch, a new patch-guided instance segmentation solution, leveraging only a few image patches for each incoming new SKU to predict accurate and robust masks, without tedious manual effort and model re-training. Technical-wise, we design a novel transformer-based network with (i) a patch-image correlation encoder to capture multi-level image features calibrated by patch information and (ii) a patch-aware transformer decoder with parallel task heads to generate instance masks. Extensive experiments on four storehouse benchmarks manifest that SKU-Patch is able to achieve the best performance over the state-of-the-art methods. Also, SKU-Patch yields an average of nearly 100% grasping success rate on more than 50 unseen SKUs in a robot-aided auto-store logistic pipeline, showing its effectiveness and practicality.
</details>
<details>
<summary>摘要</summary>
大规模仓库中，精准实例掩模是机器人抓取物品的关键，但是它们很难以获得。现有的实例分割方法通常需要 tedious scene collection、标注和网络微调，对每个 Stock Keeping Unit (SKU) 进行一个一一的处理。本文介绍了 SKU-Patch，一种新的 patch-guided 实例分割解决方案，只需要对每个新的 SKU 输入一些图像块来预测准确和可靠的掩模，不需要手动劳累和模型重新训练。技术上，我们设计了一种基于 transformer 网络的新网络，包括：(i) 一个 patch-image correlation encoder，用于捕捉多级图像特征，并将其与块信息相协调。(ii) 一个 patch-aware transformer decoder，包括多个并行任务头，用于生成实例掩模。经验表明，SKU-Patch 能够在四个仓库测试准则上取得最好的性能，并且在 robot-aided auto-store 物流管线中，SKU-Patch 可以实现97%的抓取成功率，验证了其实用性和实际性。
</details></li>
</ul>
<hr>
<h2 id="Object-Centric-Learning-with-Slot-Mixture-Module"><a href="#Object-Centric-Learning-with-Slot-Mixture-Module" class="headerlink" title="Object-Centric Learning with Slot Mixture Module"></a>Object-Centric Learning with Slot Mixture Module</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04640">http://arxiv.org/abs/2311.04640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniil Kirilenko, Vitaliy Vorobyov, Alexey K. Kovalev, Aleksandr I. Panov</li>
<li>for: 这篇论文是为了提出一种基于 Gaussian Mixture Model 的学习式划分方法，用于改进 object-centric 架构中的 slot 表示方法。</li>
<li>methods: 该方法使用学习式划分方法来分解特征图像，并将分配给 slot 的信息包含在 slot 表示中，从而得到更表示力的 slot 表示。</li>
<li>results: 对于 object-centric enario，使用该方法而不使用 Slot Attention 可以提高性能，达到目前最佳Result 在 set property prediction 任务中。<details>
<summary>Abstract</summary>
Object-centric architectures usually apply a differentiable module to the entire feature map to decompose it into sets of entity representations called slots. Some of these methods structurally resemble clustering algorithms, where the cluster's center in latent space serves as a slot representation. Slot Attention is an example of such a method, acting as a learnable analog of the soft k-means algorithm. Our work employs a learnable clustering method based on the Gaussian Mixture Model. Unlike other approaches, we represent slots not only as centers of clusters but also incorporate information about the distance between clusters and assigned vectors, leading to more expressive slot representations. Our experiments demonstrate that using this approach instead of Slot Attention improves performance in object-centric scenarios, achieving state-of-the-art results in the set property prediction task.
</details>
<details>
<summary>摘要</summary>
通常，对象中心的架构会应用一个可导模块到整个特征地图，将其分解成Entity表示集合的集合。一些这些方法结构上类似于聚类算法，其中聚类中心在隐藏空间中服务为槽表示。槽注意力是一 such方法， behaving as a learnable soft k-means algorithm。我们的工作使用一种学习聚类方法基于 Gaussian Mixture Model。与其他方法不同，我们在表示槽不仅包括聚类中心，还包括聚类与分配向量之间的距离信息，导致更具表达力的槽表示。我们的实验表明，使用这种方法而不是槽注意力可以在对象中心的情况下提高性能，实现了属性集 Prediction任务的国际最佳 результа。
</details></li>
</ul>
<hr>
<h2 id="Explained-anomaly-detection-in-text-reviews-Can-subjective-scenarios-be-correctly-evaluated"><a href="#Explained-anomaly-detection-in-text-reviews-Can-subjective-scenarios-be-correctly-evaluated" class="headerlink" title="Explained anomaly detection in text reviews: Can subjective scenarios be correctly evaluated?"></a>Explained anomaly detection in text reviews: Can subjective scenarios be correctly evaluated?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04948">http://arxiv.org/abs/2311.04948</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Novoa-Paradela, Oscar Fontenla-Romero, Bertha Guijarro-Berdiñas</li>
<li>for: 这个研究的目的是检测和解释在线平台上的异常评论。</li>
<li>methods: 这个ipeline包括三个模块，用于检测不会为用户提供价值的评论，包括无用和黑客组成的评论。每个分类都有一个正常性分数和一个解释，以 justify 的决定。</li>
<li>results: 这个ipeline在不同的数据集上进行了评测，并进行了一项解释技术的比较研究，以评估解释模块的效果。这项研究可以帮助自动化在线平台上的评论审核任务，例如电子商务平台上的评论审核，并为相关领域的异常检测任务提供灵感。此外，这项研究还表明了对不同解释技术的人类评估，并探讨了是否可以解释人类化的任务，如异常评论检测。<details>
<summary>Abstract</summary>
This paper presents a pipeline to detect and explain anomalous reviews in online platforms. The pipeline is made up of three modules and allows the detection of reviews that do not generate value for users due to either worthless or malicious composition. The classifications are accompanied by a normality score and an explanation that justifies the decision made. The pipeline's ability to solve the anomaly detection task was evaluated using different datasets created from a large Amazon database. Additionally, a study comparing three explainability techniques involving 241 participants was conducted to assess the explainability module. The study aimed to measure the impact of explanations on the respondents' ability to reproduce the classification model and their perceived usefulness. This work can be useful to automate tasks in review online platforms, such as those for electronic commerce, and offers inspiration for addressing similar problems in the field of anomaly detection in textual data. We also consider it interesting to have carried out a human evaluation of the capacity of different explainability techniques in a real and infrequent scenario such as the detection of anomalous reviews, as well as to reflect on whether it is possible to explain tasks as humanly subjective as this one.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "anomalous reviews" 翻译为 "不常见的评论"* "online platforms" 翻译为 "在线平台"* "worthless or malicious composition" 翻译为 "无用或有恶意组合"* "normality score" 翻译为 "常见度分数"* "explanation" 翻译为 "解释"* "classification model" 翻译为 "分类模型"* "electronic commerce" 翻译为 "电子商务"* "humanly subjective" 翻译为 "有人性的"
</details></li>
</ul>
<hr>
<h2 id="LuminanceL1Loss-A-loss-function-which-measures-percieved-brightness-and-colour-differences"><a href="#LuminanceL1Loss-A-loss-function-which-measures-percieved-brightness-and-colour-differences" class="headerlink" title="LuminanceL1Loss: A loss function which measures percieved brightness and colour differences"></a>LuminanceL1Loss: A loss function which measures percieved brightness and colour differences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04614">http://arxiv.org/abs/2311.04614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominic De Jonge</li>
<li>for: 提高图像修复任务的性能</li>
<li>methods: 使用新的损失函数LuminanceL1Loss，将图像转换为灰度图并计算MSE损失两个频道</li>
<li>results: 对Retinexformer、BUIFD和DnCNN arquitectures进行了评测，并表明LuminanceL1Loss可以超越传统方法，提高图像修复任务的性能，最高提高4.7dB。<details>
<summary>Abstract</summary>
We introduce LuminanceL1Loss, a novel loss function designed to enhance the performance of image restoration tasks. We demonstrate its superiority over MSE when applied to the Retinexformer, BUIFD and DnCNN architectures. Our proposed LuminanceL1Loss leverages a unique approach by transforming images into grayscale and subsequently computing the MSE loss for both grayscale and color channels. Experimental results demonstrate that this innovative loss function consistently outperforms traditional methods, showcasing its potential in image denoising and other related tasks in image reconstruction. It demonstrates gains up to 4.7dB. The results presented in this study highlight the efficacy of LuminanceL1Loss for various image restoration tasks.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的损失函数，即LuminanceL1Loss，用于提高图像恢复任务的性能。我们在Retinexformer、BUIFD和DnCNN架构上进行了实验，并证明了LuminanceL1Loss在这些架构上的优越性。我们的提案的LuminanceL1Loss采用了一种独特的方法，即将图像转换成灰度图像，然后计算灰度和色彩通道之间的MSE损失。实验结果表明，这种创新的损失函数在图像压缩和其他相关的图像重建任务中具有优越的表现，提高了4.7dB。这些研究结果表明LuminanceL1Loss在各种图像恢复任务中的可靠性和普适性。
</details></li>
</ul>
<hr>
<h2 id="TEAL-Tokenize-and-Embed-ALL-for-Multi-modal-Large-Language-Models"><a href="#TEAL-Tokenize-and-Embed-ALL-for-Multi-modal-Large-Language-Models" class="headerlink" title="TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models"></a>TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04589">http://arxiv.org/abs/2311.04589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Yang, Yingxue Zhang, Fandong Meng, Jie Zhou</li>
<li>for: 本研究想要帮助多modal语言模型（MM-LLMs）更好地处理多modal输入和生成非文本模式。</li>
<li>methods: 本方法使用了Tokenize和Embed ALl（TEAL）方法，将输入从任何模式转化为token序列，并学习一个共同的嵌入空间 для所有模式。</li>
<li>results: 实验表明，TEAL可以获得显著的多modal理解提升，并实现了一个简单的多modal生成方案。<details>
<summary>Abstract</summary>
Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a learnable embedding matrix. MM-LLMs just need to predict the multi-modal tokens autoregressively as the textual LLMs do. Finally, the corresponding de-tokenizer is applied to generate the output in each modality based on the predicted token sequence. With the joint embedding space, TEAL enables the frozen LLMs to perform both understanding and generation tasks involving non-textual modalities, such as image and audio. Thus, the textual LLM can just work as an interface and maintain its high performance in textual understanding and generation. Experiments show that TEAL achieves substantial improvements in multi-modal understanding, and implements a simple scheme for multi-modal generations.
</details>
<details>
<summary>摘要</summary>
尽管多模态大型语言模型（MM-LLMs）在最近几年内做出了吸人的进步，但它们仍然努力地模型多模态输入的交互和非文本modalities中的生成。在这项工作中，我们提出了TEAL（Tokenize and Embed ALl），一种方法，其中输入从任何模式都会被视为一个token序列，并在一个共享的embedding空间中学习一个共同的embedding矩阵。具体来说，对于输入从任何模式，TEAL首先将它拆分成一个token序列，使用可用的tokenizer进行拆分，然后将token序列embedding到一个共同的embedding空间中，使用一个学习的embedding矩阵。MM-LLMs只需要预测多modal tokens的autoregressive预测，就像文本LLMs一样。最后，对于每个模式，使用预测的token序列生成输出。与共同的embedding空间相比，TEAL使得冻结的LLMs可以在多modal任务中进行理解和生成任务，如图像和音频。因此，文本LLM可以作为界面，维持高效的文本理解和生成能力。实验结果表明，TEAL在多modal理解方面取得了显著的提升，并实现了简单的多modal生成方案。
</details></li>
</ul>
<hr>
<h2 id="Army-of-Thieves-Enhancing-Black-Box-Model-Extraction-via-Ensemble-based-sample-selection"><a href="#Army-of-Thieves-Enhancing-Black-Box-Model-Extraction-via-Ensemble-based-sample-selection" class="headerlink" title="Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection"></a>Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04588">http://arxiv.org/abs/2311.04588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akshitjindal1/aot_wacv">https://github.com/akshitjindal1/aot_wacv</a></li>
<li>paper_authors: Akshit Jindal, Vikram Goyal, Saket Anand, Chetan Arora</li>
<li>for: 防止机器学习模型被盗用（Model Stealing Attacks），当机器学习模型被部署为服务时。</li>
<li>methods: 使用 ensemble of deep learning models 作为盗取模型，以便选择最有用的数据点子集。</li>
<li>results: 比基于单个模型的方法高效，可以提高盗取模型的质量和攻击成功率。在 CIFAR-10 数据集上，我们的方法可以提高对模型的攻击性能，比基于单个模型的方法高效。<details>
<summary>Abstract</summary>
Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-Supervised learning to minimize costs. However, in the black-box setting, these approaches may select sub-optimal samples as they train only one thief model. Depending on the thief model's capacity and the data it was pretrained on, the model might even select noisy samples that harm the learning process. In this work, we explore the usage of an ensemble of deep learning models as our thief model. We call our attack Army of Thieves(AOT) as we train multiple models with varying complexities to leverage the crowd's wisdom. Based on the ensemble's collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data. Our approach is the first one to utilize an ensemble of thief models to perform model extraction. We outperform the base approaches of existing state-of-the-art methods by at least 3% and achieve a 21% higher adversarial sample transferability than previous work for models trained on the CIFAR-10 dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GResilience-Trading-Off-Between-the-Greenness-and-the-Resilience-of-Collaborative-AI-Systems"><a href="#GResilience-Trading-Off-Between-the-Greenness-and-the-Resilience-of-Collaborative-AI-Systems" class="headerlink" title="GResilience: Trading Off Between the Greenness and the Resilience of Collaborative AI Systems"></a>GResilience: Trading Off Between the Greenness and the Resilience of Collaborative AI Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04569">http://arxiv.org/abs/2311.04569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diaeddin Rimawi, Antonio Liotta, Marco Todescato, Barbara Russo</li>
<li>for: 本研究旨在提供一种自动评估Collaborative Artificial Intelligence System（CAIS）恢复行动的能力来衡量系统的可恢复性和绿色性。</li>
<li>methods: 本研究提出了一种以优化和游戏理论为基础的方法来评估CAIS恢复行动的可恢复性和绿色性。</li>
<li>results: 研究人员通过设计了一种实验协议和应用于一个真实的CAIS示例器，并通过优化和游戏理论来评估CAIS恢复行动的可恢复性和绿色性。<details>
<summary>Abstract</summary>
A Collaborative Artificial Intelligence System (CAIS) works with humans in a shared environment to achieve a common goal. To recover from a disruptive event that degrades its performance and ensures its resilience, a CAIS may then need to perform a set of actions either by the system, by the humans, or collaboratively together. As for any other system, recovery actions may cause energy adverse effects due to the additional required energy. Therefore, it is of paramount importance to understand which of the above actions can better trade-off between resilience and greenness. In this in-progress work, we propose an approach to automatically evaluate CAIS recovery actions for their ability to trade-off between the resilience and greenness of the system. We have also designed an experiment protocol and its application to a real CAIS demonstrator. Our approach aims to attack the problem from two perspectives: as a one-agent decision problem through optimization, which takes the decision based on the score of resilience and greenness, and as a two-agent decision problem through game theory, which takes the decision based on the payoff computed for resilience and greenness as two players of a cooperative game.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:一个协同人工智能系统（CAIS）与人类在共享环境中工作以实现共同目标。在一个破坏性事件导致系统性能下降后，CAIS可能需要执行一系列动作，这些动作可以由系统、人类或者共同执行。由于这些恢复动作可能会带来更多的能源消耗，因此非常重要理解这些动作可以如何让恢复和绿色之间进行更好的负担平衡。在这个进行中的工作中，我们提出了一种方法，可以自动评估CAIS恢复动作的能量和绿色之间的负担平衡。我们还设计了一个实验协议和应用于一个真实的CAIS示范器。我们的方法尝试从两个角度解决问题：作为一个一个代理决策问题，通过优化来做出决策，以及作为两个玩家的合作游戏问题，通过游戏理论来做出决策。
</details></li>
</ul>
<hr>
<h2 id="CAIS-DMA-A-Decision-Making-Assistant-for-Collaborative-AI-Systems"><a href="#CAIS-DMA-A-Decision-Making-Assistant-for-Collaborative-AI-Systems" class="headerlink" title="CAIS-DMA: A Decision-Making Assistant for Collaborative AI Systems"></a>CAIS-DMA: A Decision-Making Assistant for Collaborative AI Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04562">http://arxiv.org/abs/2311.04562</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmrimawi/cais-dma">https://github.com/dmrimawi/cais-dma</a></li>
<li>paper_authors: Diaeddin Rimawi, Antonio Lotta, Marco Todescato, Barbara Russo<br>for:This paper aims to develop a methodology to automatically support the decision-making process in a Collaborative Artificial Intelligence System (CAIS) when the system experiences performance degradation after a disruptive event.methods:The proposed framework consists of three components: one manages or simulates CAIS’s environment and disruptive events, the second automates the decision-making process, and the third provides a visual analysis of CAIS behavior.results:The framework can automatically monitor the decision-making process, intervene whenever a performance degradation occurs, and recommend the next action that balances between minimizing the recovery time (i.e., resilience) and minimizing the energy adverse effects (i.e., greenness).<details>
<summary>Abstract</summary>
A Collaborative Artificial Intelligence System (CAIS) is a cyber-physical system that learns actions in collaboration with humans in a shared environment to achieve a common goal. In particular, a CAIS is equipped with an AI model to support the decision-making process of this collaboration. When an event degrades the performance of CAIS (i.e., a disruptive event), this decision-making process may be hampered or even stopped. Thus, it is of paramount importance to monitor the learning of the AI model, and eventually support its decision-making process in such circumstances. This paper introduces a new methodology to automatically support the decision-making process in CAIS when the system experiences performance degradation after a disruptive event. To this aim, we develop a framework that consists of three components: one manages or simulates CAIS's environment and disruptive events, the second automates the decision-making process, and the third provides a visual analysis of CAIS behavior. Overall, our framework automatically monitors the decision-making process, intervenes whenever a performance degradation occurs, and recommends the next action. We demonstrate our framework by implementing an example with a real-world collaborative robot, where the framework recommends the next action that balances between minimizing the recovery time (i.e., resilience), and minimizing the energy adverse effects (i.e., greenness).
</details>
<details>
<summary>摘要</summary>
一个协同人工智能系统（CAIS）是一个融合物理系统，通过与人类在共同环境中学习行动来实现共同目标。特别是，CAIS具有一个人工智能模型，用于支持协同决策过程。当系统经历破坏性事件（例如，突发事件）时，这个决策过程可能受到影响或者even stop。因此，监测人工智能模型的学习是极其重要的。这篇论文提出了一种新的方法，用于自动支持CAIS协同决策过程在系统经历破坏性事件后。为此，我们开发了一个框架，该框架包括三个组件：一个管理或模拟CAIS的环境和破坏性事件，第二个自动化决策过程，第三个提供CAIS行为的可视分析。总之，我们的框架可以自动监测决策过程，在破坏性事件发生时进行交互，并 recommends the next action，以保持系统的可靠性和绿色性。我们通过实施一个实际的协同 робоット示例来证明我们的框架。在这个示例中，我们的框架建议下一个行动，以均衡系统的恢复时间（即可靠性）和能源不良影响（即绿色性）。
</details></li>
</ul>
<hr>
<h2 id="Local-Differential-Privacy-for-Smart-Meter-Data-Sharing"><a href="#Local-Differential-Privacy-for-Smart-Meter-Data-Sharing" class="headerlink" title="Local Differential Privacy for Smart Meter Data Sharing"></a>Local Differential Privacy for Smart Meter Data Sharing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04544">http://arxiv.org/abs/2311.04544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yashothara Shanmugarasa, M. A. P. Chamikara, Hye-young Paik, Salil S. Kanhere, Liming Zhu</li>
<li>for: 提供消费者和能源公司 valuabe insights into energy management, while protecting privacy.</li>
<li>methods: 使用Local Differential Privacy (LDP) methods with randomized response techniques and sliding windows to protect appliance-level energy consumption data.</li>
<li>results: Efficient and effective privacy protection, balancing privacy and data utility for analysis.<details>
<summary>Abstract</summary>
Energy disaggregation techniques, which use smart meter data to infer appliance energy usage, can provide consumers and energy companies valuable insights into energy management. However, these techniques also present privacy risks, such as the potential for behavioral profiling. Local differential privacy (LDP) methods provide strong privacy guarantees with high efficiency in addressing privacy concerns. However, existing LDP methods focus on protecting aggregated energy consumption data rather than individual appliances. Furthermore, these methods do not consider the fact that smart meter data are a form of streaming data, and its processing methods should account for time windows. In this paper, we propose a novel LDP approach (named LDP-SmartEnergy) that utilizes randomized response techniques with sliding windows to facilitate the sharing of appliance-level energy consumption data over time while not revealing individual users' appliance usage patterns. Our evaluations show that LDP-SmartEnergy runs efficiently compared to baseline methods. The results also demonstrate that our solution strikes a balance between protecting privacy and maintaining the utility of data for effective analysis.
</details>
<details>
<summary>摘要</summary>
智能计量数据分解技术可以为消费者和能源公司提供有价值的能源管理信息，但这些技术也存在隐私风险，如行为 Profiling 的可能性。本地均衡隐私（LDP）方法可以提供强隐私保证，但现有的 LDP 方法主要关注保护归并的能源消耗数据而不是个体设备。此外，这些方法没有考虑智能计量数据是流动数据，其处理方法应该考虑时间窗口。在本文中，我们提出了一种新的 LDP 方法（名为 LDP-SmartEnergy），它利用随机响应技术和滑块窗口来帮助在时间上分享设备级能源消耗数据，而不抛露个体用户的设备使用模式。我们的评估结果显示，LDP-SmartEnergy 能够高效运行，与基eline方法相比。结果还表明，我们的解决方案能够平衡保护隐私和维护数据的有用性。
</details></li>
</ul>
<hr>
<h2 id="RankAug-Augmented-data-ranking-for-text-classification"><a href="#RankAug-Augmented-data-ranking-for-text-classification" class="headerlink" title="RankAug: Augmented data ranking for text classification"></a>RankAug: Augmented data ranking for text classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04535">http://arxiv.org/abs/2311.04535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiasa Singha Roy, Priyam Basu</li>
<li>for: 这个论文主要是为了提高生成模型的评估方法。</li>
<li>methods: 这篇论文提出了一种文本排序方法，用于检测和过滤生成文本中最相似的文本，以提高NLU任务的准确率。</li>
<li>results: 实验结果显示，通过judicious选择筛选技术可以提高准确率，最高提高35%。<details>
<summary>Abstract</summary>
Research on data generation and augmentation has been focused majorly on enhancing generation models, leaving a notable gap in the exploration and refinement of methods for evaluating synthetic data. There are several text similarity metrics within the context of generated data filtering which can impact the performance of specific Natural Language Understanding (NLU) tasks, specifically focusing on intent and sentiment classification. In this study, we propose RankAug, a text-ranking approach that detects and filters out the top augmented texts in terms of being most similar in meaning with lexical and syntactical diversity. Through experiments conducted on multiple datasets, we demonstrate that the judicious selection of filtering techniques can yield a substantial improvement of up to 35% in classification accuracy for under-represented classes.
</details>
<details>
<summary>摘要</summary>
研究数据生成和增强主要集中在提高生成模型，留下了许多评估合成数据的方法的空白。在生成数据筛选中，文本相似度指标在NLU任务中的意图和情感分类方面具有重要作用。本研究提出了RankAug方法，它通过词语和语法多样性来推荐最相似的文本，从而提高分类精度。通过在多个数据集上进行实验，我们证明了选择合适的筛选技术可以提高受 represeted类准确率达35%。
</details></li>
</ul>
<hr>
<h2 id="Validating-ChatGPT-Facts-through-RDF-Knowledge-Graphs-and-Sentence-Similarity"><a href="#Validating-ChatGPT-Facts-through-RDF-Knowledge-Graphs-and-Sentence-Similarity" class="headerlink" title="Validating ChatGPT Facts through RDF Knowledge Graphs and Sentence Similarity"></a>Validating ChatGPT Facts through RDF Knowledge Graphs and Sentence Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04524">http://arxiv.org/abs/2311.04524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michalis Mountantonakis, Yannis Tzitzikas</li>
<li>for: 这个论文目的是 validate ChatGPT 的答案和补充它们的证明和来源。</li>
<li>methods: 这个论文使用 RDF 知识Graph（KG）和短句嵌入来实现 ChatGPT 答案的验证和补充。特别是使用 DBpedia 和 LODsyndesis（一个 aggregate KG，包含 2000 亿 triple 从 400 RDF KGs 多个领域），并引入一种算法，可以返回更加相关的 triple（ accompaniment 和 confidence score）。</li>
<li>results: 在评估这种服务（以及类似服务）时，作者创建了一个评估标准套件，包括 2000 个 ChatGPT 答案（其中 1000 个是希腊名人、500 个是希腊地点、500 个是关于希腊的事件）。手动标注后，发现 ChatGPT 的答案中约 73% 是正确的，27% 是错误的。结果很有 promise，例如，对整个benchmark来说，我们成功验证了 ChatGPT 的 85.3% 正确答案，并找到了错误答案中 62.6% 的正确答案。<details>
<summary>Abstract</summary>
Since ChatGPT offers detailed responses without justifications, and erroneous facts even for popular persons, events and places, in this paper we present a novel pipeline that retrieves the response of ChatGPT in RDF and tries to validate the ChatGPT facts using one or more RDF Knowledge Graphs (KGs). To this end we leverage DBpedia and LODsyndesis (an aggregated Knowledge Graph that contains 2 billion triples from 400 RDF KGs of many domains) and short sentence embeddings, and introduce an algorithm that returns the more relevant triple(s) accompanied by their provenance and a confidence score. This enables the validation of ChatGPT responses and their enrichment with justifications and provenance. To evaluate this service (such services in general), we create an evaluation benchmark that includes 2,000 ChatGPT facts; specifically 1,000 facts for famous Greek Persons, 500 facts for popular Greek Places, and 500 facts for Events related to Greece. The facts were manually labelled (approximately 73% of ChatGPT facts were correct and 27% of facts were erroneous). The results are promising; indicatively for the whole benchmark, we managed to verify the 85.3% of the correct facts of ChatGPT and to find the correct answer for the 62.6% of the erroneous ChatGPT facts.
</details>
<details>
<summary>摘要</summary>
自从ChatGPT提供了详细的回答无需证明，而且甚至包含错误的信息关于知名人物、事件和地点，因此在这篇论文中，我们提出了一个新的管道，它将ChatGPT的回答转换为RDF格式，并使用一个或多个RDF知识 graphs（KGs）来验证ChatGPT的信息是否正确。为此，我们利用了DBpedia和LODsyndesis（一个包含400个RDF KGs的多个领域的知识Graph，总共包含200亿个三元组），并使用短句嵌入，并引入一种算法，它可以返回更加相关的 triple（或多个 triple），以及它们的来源和信任分数。这使得可以验证ChatGPT的回答，并为其添加证明和来源。为了评估这种服务（以及类似服务），我们创建了一个评估标准，包括2,000个ChatGPT的信息，其中包括1,000个著名希腊人物、500个希腊地点和500个与希腊相关的事件。这些信息都是手动标注的（约73%的ChatGPT信息正确，27%的信息错误）。结果很有 promise，例如，对整个benchmark，我们成功验证了85.3%的正确ChatGPT信息，并为错误的ChatGPT信息找到了正确的答案的62.6%。
</details></li>
</ul>
<hr>
<h2 id="FFINet-Future-Feedback-Interaction-Network-for-Motion-Forecasting"><a href="#FFINet-Future-Feedback-Interaction-Network-for-Motion-Forecasting" class="headerlink" title="FFINet: Future Feedback Interaction Network for Motion Forecasting"></a>FFINet: Future Feedback Interaction Network for Motion Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04512">http://arxiv.org/abs/2311.04512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miao Kang, Shengqi Wang, Sanping Zhou, Ke Ye, Jingjing Jiang, Nanning Zheng</li>
<li>for: 预测交通代理人的未来合理行为，以提高自动驾驶系统的安全性和效率。</li>
<li>methods: 提出了一种新的未来反馈互动网络（FFINet），通过将当前观察和未来互动的特征进行聚合，以提高多模态轨迹预测的准确性。</li>
<li>results: 在 Argoverse 1 和 Argoverse 2 动态预测测试数据集上，FFINet 实现了状态领先的性能。<details>
<summary>Abstract</summary>
Motion forecasting plays a crucial role in autonomous driving, with the aim of predicting the future reasonable motions of traffic agents. Most existing methods mainly model the historical interactions between agents and the environment, and predict multi-modal trajectories in a feedforward process, ignoring potential trajectory changes caused by future interactions between agents. In this paper, we propose a novel Future Feedback Interaction Network (FFINet) to aggregate features the current observations and potential future interactions for trajectory prediction. Firstly, we employ different spatial-temporal encoders to embed the decomposed position vectors and the current position of each scene, providing rich features for the subsequent cross-temporal aggregation. Secondly, the relative interaction and cross-temporal aggregation strategies are sequentially adopted to integrate features in the current fusion module, observation interaction module, future feedback module and global fusion module, in which the future feedback module can enable the understanding of pre-action by feeding the influence of preview information to feedforward prediction. Thirdly, the comprehensive interaction features are further fed into final predictor to generate the joint predicted trajectories of multiple agents. Extensive experimental results show that our FFINet achieves the state-of-the-art performance on Argoverse 1 and Argoverse 2 motion forecasting benchmarks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>自动驾驶中，预测行为的预测具有重要的作用，旨在预测未来的合理行为。现有的方法主要是基于历史交互和环境的模型，预测多模态轨迹，忽略了未来交互所引起的轨迹变化。在这篇论文中，我们提出了一种新的未来反馈互动网络（FFINet），用于聚合特征。首先，我们采用不同的空间-时间编码器，将分解的位坐标和当前场景的位置进行嵌入，提供丰富的特征 для后续的跨时间汇集。其次，我们采用相对交互和跨时间汇集策略，先后采用交互模块、观察交互模块、未来反馈模块和全局汇集模块，其中未来反馈模块可以帮助理解预测的预先行为。最后，我们将全面交互特征传递给最终预测器，生成多个交互的联合预测轨迹。广泛的实验结果表明，我们的 FFINet 在 Argoverse 1 和 Argoverse 2 运动预测Benchmark上达到了状态 искусственный智能的表现。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Causal-Inference-on-Investment-Constraints-and-Non-stationarity-in-Dynamic-Portfolio-Optimization-through-Reinforcement-Learning"><a href="#Causal-Inference-on-Investment-Constraints-and-Non-stationarity-in-Dynamic-Portfolio-Optimization-through-Reinforcement-Learning" class="headerlink" title="Causal Inference on Investment Constraints and Non-stationarity in Dynamic Portfolio Optimization through Reinforcement Learning"></a>Causal Inference on Investment Constraints and Non-stationarity in Dynamic Portfolio Optimization through Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04946">http://arxiv.org/abs/2311.04946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasuhiro Nakayama, Tomochika Sawaki</li>
<li>for: 本研究开发了一种动态资产配置投资策略，使用了回归学习技术。</li>
<li>methods: 我们解决了金融时间序列数据不stationarity问题的问题，并引入了一些变量，如 режим变化，以提高预测精度。</li>
<li>results: 我们的研究发现，通过在投资策略中应用回归学习技术，可以实现高精度的预测，并且可以考虑实际面临的投资者实际约束，从而实现有效的优化。<details>
<summary>Abstract</summary>
In this study, we have developed a dynamic asset allocation investment strategy using reinforcement learning techniques. To begin with, we have addressed the crucial issue of incorporating non-stationarity of financial time series data into reinforcement learning algorithms, which is a significant implementation in the application of reinforcement learning in investment strategies. Our findings highlight the significance of introducing certain variables such as regime change in the environment setting to enhance the prediction accuracy. Furthermore, the application of reinforcement learning in investment strategies provides a remarkable advantage of setting the optimization problem flexibly. This enables the integration of practical constraints faced by investors into the algorithm, resulting in efficient optimization. Our study has categorized the investment strategy formulation conditions into three main categories, including performance measurement indicators, portfolio management rules, and other constraints. We have evaluated the impact of incorporating these conditions into the environment and rewards in a reinforcement learning framework and examined how they influence investment behavior.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们开发了一种动态资产分配投资策略使用强化学习技术。首先，我们解决了金融时间序列数据不Stationarity问题的应用在强化学习算法中的问题，这是投资策略应用强化学习中的一个重要实现。我们的发现表明，在环境设置中引入 certain 变量，如状态转换，可以提高预测精度。此外，强化学习在投资策略中提供了一个remarkable的优势，即可以自由地设置优化问题。这使得可以将实际面临的投资者的限制 integrate into the algorithm，从而实现高效的优化。我们对投资策略的形式化条件进行分类，包括表现指标、股票管理规则和其他限制。我们在强化学习框架中包含这些条件的环境和奖励，并研究了它们如何影响投资行为。
</details></li>
</ul>
<hr>
<h2 id="Auto-deep-learning-for-bioacoustic-signals"><a href="#Auto-deep-learning-for-bioacoustic-signals" class="headerlink" title="Auto deep learning for bioacoustic signals"></a>Auto deep learning for bioacoustic signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04945">http://arxiv.org/abs/2311.04945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/giuliotosato/autokeras-bioacustic">https://github.com/giuliotosato/autokeras-bioacustic</a></li>
<li>paper_authors: Giulio Tosato, Abdelrahman Shehata, Joshua Janssen, Kees Kamp, Pramatya Jati, Dan Stowell</li>
<li>for: 这个研究旨在探讨自动深度学习是否可以提高多类bird vocalization分类的准确率和效率，并与传统的手动设计的深度学习模型进行比较。</li>
<li>methods: 这个研究使用了AutoKeras自动机器学习框架，自动化了神经网络搜索和超参数优化。</li>
<li>results: 结果表明，AutoKeras-derived模型在Western Mediterranean Wetland Birds dataset上 consistently outperform了传统模型如MobileNet、ResNet50和VGG16。这种方法和结论推动了生物声学研究的进步，并提供了一种新的自动化深度学习方法。<details>
<summary>Abstract</summary>
This study investigates the potential of automated deep learning to enhance the accuracy and efficiency of multi-class classification of bird vocalizations, compared against traditional manually-designed deep learning models. Using the Western Mediterranean Wetland Birds dataset, we investigated the use of AutoKeras, an automated machine learning framework, to automate neural architecture search and hyperparameter tuning. Comparative analysis validates our hypothesis that the AutoKeras-derived model consistently outperforms traditional models like MobileNet, ResNet50 and VGG16. Our approach and findings underscore the transformative potential of automated deep learning for advancing bioacoustics research and models. In fact, the automated techniques eliminate the need for manual feature engineering and model design while improving performance. This study illuminates best practices in sampling, evaluation and reporting to enhance reproducibility in this nascent field. All the code used is available at https: //github.com/giuliotosato/AutoKeras-bioacustic   Keywords: AutoKeras; automated deep learning; audio classification; Wetlands Bird dataset; comparative analysis; bioacoustics; validation dataset; multi-class classification; spectrograms.
</details>
<details>
<summary>摘要</summary>
Keywords: AutoKeras; automated deep learning; audio classification; Wetlands Bird dataset; comparative analysis; bioacoustics; validation dataset; multi-class classification; spectrograms.中文翻译：本研究探索使用自动化深度学习提高多类分类鸟叫声的精度和效率，与传统手动设计的深度学习模型进行比较。我们使用西地中海湿地鸟类 dataset 和 AutoKeras 框架自动化神经网络搜索和超参调整。我们的结果表明，AutoKeras  derive 模型在 MobileNet、ResNet50 和 VGG16 等传统模型的比较中 consistently 表现出色。本研究强调自动化深度学习在生物声学研究中的潜在价值，因为它消除了手动特征工程和模型设计的需求，同时提高性能。我们还提供了采样、评估和报告的最佳实践，以增强这个领域的可重复性。所有代码使用的可以在 GitHub 上找到（https://github.com/giuliotosato/AutoKeras-bioacustic）。键语：AutoKeras; 自动化深度学习; 音频分类; 湿地鸟类 dataset; 比较分析; 生物声学; 验证集; 多类分类; spectrograms.
</details></li>
</ul>
<hr>
<h2 id="NExT-Chat-An-LMM-for-Chat-Detection-and-Segmentation"><a href="#NExT-Chat-An-LMM-for-Chat-Detection-and-Segmentation" class="headerlink" title="NExT-Chat: An LMM for Chat, Detection and Segmentation"></a>NExT-Chat: An LMM for Chat, Detection and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04498">http://arxiv.org/abs/2311.04498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tmukande-debug/NExT-Chat">https://github.com/tmukande-debug/NExT-Chat</a></li>
<li>paper_authors: Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji, Tat-Seng Chua</li>
<li>for: 本研究旨在提高大型语言模型（LLM）在多Modal理解方面的水平，通过增强视觉理解能力，使LMM能够更好地理解和回答多Modal问题。</li>
<li>methods: 本研究提出了一种新的对象位置模型方法 called pixel2emb，该方法让LMM输出位置嵌入，然后通过不同的解码器进行解码。这种嵌入基于的位置模型方法允许使用不同的位置格式（如 bounding box 和 mask）在多Modal会话中。</li>
<li>results: 在有限资源的情况下，我们的 pixel2emb 方法在位置输入和输出任务中表现出色，与现有SOTA方法相比，具有更高的性能。基于提出的 pixel2emb 方法，我们训练了一个名为 NExT-Chat 的 LMM，并证明其能够处理多种任务，如视觉固定、区域描述和基于物理的理解。<details>
<summary>Abstract</summary>
The development of large language models (LLMs) has greatly advanced the field of multimodal understanding, leading to the emergence of large multimodal models (LMMs). In order to enhance the level of visual comprehension, recent studies have equipped LMMs with region-level understanding capabilities by representing object bounding box coordinates as a series of text sequences (pixel2seq). In this paper, we introduce a novel paradigm for object location modeling called pixel2emb method, where we ask the LMM to output the location embeddings and then decoded by different decoders. This paradigm allows for different location formats (such as bounding boxes and masks) to be used in multimodal conversations Furthermore, this kind of embedding based location modeling enables the utilization of existing practices in localization tasks, such as detection and segmentation. In scenarios with limited resources, our pixel2emb demonstrates superior performance compared to existing state-of-the-art (SOTA) approaches in both the location input and output tasks under fair comparison. Leveraging the proposed pixel2emb method, we train an LMM named NExT-Chat and demonstrate its capability of handling multiple tasks like visual grounding, region caption, and grounded reasoning.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的发展对多Modal理解领域带来了巨大的进步，导致大多Modal模型（LMM）的出现。为了提高视觉理解水平， latest studies have equipped LMMs with regional understanding capabilities by representing object bounding box coordinates as a series of text sequences (pixel2seq). 在这篇论文中，我们介绍了一种新的对象位置模型方法，即像素2Embedding（pixel2emb）方法，其中我们问LMM输出位置嵌入，然后通过不同的解码器进行解码。这种嵌入基于位置模型方法允许使用不同的位置格式（如 bounding box 和 mask）在多Modal conversation中，并且可以利用现有的Localization任务的实践，如检测和分割。在有限的资源情况下，我们的像素2emb在位置输入和输出任务中表现出了较好的性能，与现有SOTA方法相比。基于提出的像素2emb方法，我们训练了一个名为NExT-Chat的LMM，并证明其能处理多个任务，如视觉定位、区域描述和基于位置的理解。
</details></li>
</ul>
<hr>
<h2 id="Explainable-AI-for-Earth-Observation-Current-Methods-Open-Challenges-and-Opportunities"><a href="#Explainable-AI-for-Earth-Observation-Current-Methods-Open-Challenges-and-Opportunities" class="headerlink" title="Explainable AI for Earth Observation: Current Methods, Open Challenges, and Opportunities"></a>Explainable AI for Earth Observation: Current Methods, Open Challenges, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04491">http://arxiv.org/abs/2311.04491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gulsen Taskin, Erchan Aptoula, Alp Ertürk</li>
<li>for: This paper provides a panorama of the state-of-the-art in explainable remote sensing image analysis, organized by prominent Earth observation application fields.</li>
<li>methods: The paper explores a wide spectrum of Explainable Artificial Intelligence techniques to address the lack of explainability and interpretability in deep learning methods for remote sensing.</li>
<li>results: The paper presents the state-of-the-art in explainable remote sensing image analysis, covering a range of Earth observation application fields.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文提供了遍及主要地球观测应用领域的现状的Remote Sensing图像分析的状况报告，使用了Explainable Artificial Intelligence技术来解决深度学习方法的解释性和可解释性问题。</li>
<li>methods: 论文探讨了各种Explainable Artificial Intelligence技术，以解决深度学习方法在Remote Sensing图像分析中的解释性和可解释性问题。</li>
<li>results: 论文提供了Remote Sensing图像分析领域的状态对应的现状报告，涵盖了主要的地球观测应用领域。<details>
<summary>Abstract</summary>
Deep learning has taken by storm all fields involved in data analysis, including remote sensing for Earth observation. However, despite significant advances in terms of performance, its lack of explainability and interpretability, inherent to neural networks in general since their inception, remains a major source of criticism. Hence it comes as no surprise that the expansion of deep learning methods in remote sensing is being accompanied by increasingly intensive efforts oriented towards addressing this drawback through the exploration of a wide spectrum of Explainable Artificial Intelligence techniques. This chapter, organized according to prominent Earth observation application fields, presents a panorama of the state-of-the-art in explainable remote sensing image analysis.
</details>
<details>
<summary>摘要</summary>
深度学习已经在数据分析领域的所有领域中掀尘，包括远程感知。然而，尽管表现得非常出色，但深度学习的不可解性和解释性问题，从神经网络的出现以来一直存在的问题，仍然是对其进行批判的主要来源。因此，深度学习方法在远程感知领域的扩张被附加了解释人工智能技术的探索。这章，按照主要的地球观测应用领域分类，介绍了现代 explainable 远程感知图像分析的状况。
</details></li>
</ul>
<hr>
<h2 id="Emergent-Communication-for-Rules-Reasoning"><a href="#Emergent-Communication-for-Rules-Reasoning" class="headerlink" title="Emergent Communication for Rules Reasoning"></a>Emergent Communication for Rules Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04474">http://arxiv.org/abs/2311.04474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Guo, Yifan Hao, Rui Zhang, Enshuai Zhou, Zidong Du, Xishan Zhang, Xinkai Song, Yuanbo Wen, Yongwei Zhao, Xuehai Zhou, Jiaming Guo, Qi Yi, Shaohui Peng, Di Huang, Ruizhi Chen, Qi Guo, Yunji Chen</li>
<li>for: 这个论文主要研究了深度学习基于代理的emergent通信，它们在语言和人工智能方面提供了灵感。但是，之前的尝试都是在感知 Orientated的环境下进行emerging通信， forcing agents to describe low-level perceptual features within image or symbol contexts.</li>
<li>methods: 在这篇论文中，我们提出了一种新的认知游戏（namely Reasoning Game），这个游戏鼓励代理通过思维和通信来解释高级规则，而不是仅仅描述低级感知上下文。我们还提出了一个不偏的数据集（namely rule-RAVEN）作为一个基准，以避免过拟合。此外，我们还提出了一种两阶段训练方法，用于在Reasoning Game中更稳定地 converge。</li>
<li>results: 实验结果表明，在Reasoning Game中，代理们能够解释高级规则，并将其应用到未看过的上下文特性中。此外，emerged语言还帮助代理们在不同上下文特性或任务之间进行泛化和传输。<details>
<summary>Abstract</summary>
Research on emergent communication between deep-learning-based agents has received extensive attention due to its inspiration for linguistics and artificial intelligence. However, previous attempts have hovered around emerging communication under perception-oriented environmental settings, that forces agents to describe low-level perceptual features intra image or symbol contexts. In this work, inspired by the classic human reasoning test (namely Raven's Progressive Matrix), we propose the Reasoning Game, a cognition-oriented environment that encourages agents to reason and communicate high-level rules, rather than perceived low-level contexts. Moreover, we propose 1) an unbiased dataset (namely rule-RAVEN) as a benchmark to avoid overfitting, 2) and a two-stage curriculum agent training method as a baseline for more stable convergence in the Reasoning Game, where contexts and semantics are bilaterally drifting. Experimental results show that, in the Reasoning Game, a semantically stable and compositional language emerges to solve reasoning problems. The emerged language helps agents apply the extracted rules to the generalization of unseen context attributes, and to the transfer between different context attributes or even tasks.
</details>
<details>
<summary>摘要</summary>
研究深度学习代理之间的emergentcommunication已经受到了人工智能和语言科学的广泛关注，因为它们可以提供人工智能和语言科学的灵感。然而，之前的尝试都集中在感知 oriented 环境下的 emerging communication， forcing agents to describe low-level perceptual features within image or symbol contexts。在这项工作中， Drawing inspiration from the classic human reasoning test (namely Raven's Progressive Matrix), we propose the Reasoning Game, a cognition-oriented environment that encourages agents to reason and communicate high-level rules, rather than perceived low-level contexts。 In addition, we propose 1) an unbiased dataset (namely rule-RAVEN) as a benchmark to avoid overfitting, 2) and a two-stage curriculum agent training method as a baseline for more stable convergence in the Reasoning Game, where contexts and semantics are bilaterally drifting。实验结果表明，在 Reasoning Game 中，semantically stable and compositional language emerges to solve reasoning problems。这种emerged language helps agents apply the extracted rules to the generalization of unseen context attributes, and to the transfer between different context attributes or even tasks。
</details></li>
</ul>
<hr>
<h2 id="RDGCN-Reinforced-Dependency-Graph-Convolutional-Network-for-Aspect-based-Sentiment-Analysis"><a href="#RDGCN-Reinforced-Dependency-Graph-Convolutional-Network-for-Aspect-based-Sentiment-Analysis" class="headerlink" title="RDGCN: Reinforced Dependency Graph Convolutional Network for Aspect-based Sentiment Analysis"></a>RDGCN: Reinforced Dependency Graph Convolutional Network for Aspect-based Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04467">http://arxiv.org/abs/2311.04467</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rdgcn/rdgcn">https://github.com/rdgcn/rdgcn</a></li>
<li>paper_authors: Xusheng Zhao, Hao Peng, Qiong Dai, Xu Bai, Huailiang Peng, Yanbing Liu, Qinglang Guo, Philip S. Yu</li>
<li>for: 这 paper 的目的是提高 aspect-based sentiment analysis (ABSA) 的精度，使其能够更好地预测句子中的 sentiment polarity。</li>
<li>methods: 这 paper 使用 graph neural networks (GNN) 来捕捉句子中的结构 Patterns，并通过 reinforcement learning 来改进 dependency graph 中的重要性计算。</li>
<li>results:  compare to state-of-the-art GNN-based baselines, RDGCN 在三个 популяр的 dataset 上的全面实验中表现出色，提高了 ABSA 的精度。<details>
<summary>Abstract</summary>
Aspect-based sentiment analysis (ABSA) is dedicated to forecasting the sentiment polarity of aspect terms within sentences. Employing graph neural networks to capture structural patterns from syntactic dependency parsing has been confirmed as an effective approach for boosting ABSA. In most works, the topology of dependency trees or dependency-based attention coefficients is often loosely regarded as edges between aspects and opinions, which can result in insufficient and ambiguous syntactic utilization. To address these problems, we propose a new reinforced dependency graph convolutional network (RDGCN) that improves the importance calculation of dependencies in both distance and type views. Initially, we propose an importance calculation criterion for the minimum distances over dependency trees. Under the criterion, we design a distance-importance function that leverages reinforcement learning for weight distribution search and dissimilarity control. Since dependency types often do not have explicit syntax like tree distances, we use global attention and mask mechanisms to design type-importance functions. Finally, we merge these weights and implement feature aggregation and classification. Comprehensive experiments on three popular datasets demonstrate the effectiveness of the criterion and importance functions. RDGCN outperforms state-of-the-art GNN-based baselines in all validations.
</details>
<details>
<summary>摘要</summary>
Initially, we propose an importance calculation criterion for the minimum distances over dependency trees. Under the criterion, we design a distance-importance function that leverages reinforcement learning for weight distribution search and dissimilarity control. Since dependency types often do not have explicit syntax like tree distances, we use global attention and mask mechanisms to design type-importance functions. Finally, we merge these weights and implement feature aggregation and classification.Comprehensive experiments on three popular datasets demonstrate the effectiveness of the criterion and importance functions. RDGCN outperforms state-of-the-art GNN-based baselines in all validations.
</details></li>
</ul>
<hr>
<h2 id="Edge-assisted-U-Shaped-Split-Federated-Learning-with-Privacy-preserving-for-Internet-of-Things"><a href="#Edge-assisted-U-Shaped-Split-Federated-Learning-with-Privacy-preserving-for-Internet-of-Things" class="headerlink" title="Edge-assisted U-Shaped Split Federated Learning with Privacy-preserving for Internet of Things"></a>Edge-assisted U-Shaped Split Federated Learning with Privacy-preserving for Internet of Things</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04944">http://arxiv.org/abs/2311.04944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengliang Tang, Zihang Zhao, Detian Liu, Yang Cao, Shiqiang Zhang, Siqing You</li>
<li>for: 这个研究旨在解决互联网领域内的物联网（IoT）设备上的深度学习模型部署问题。这些设备通常没有计算和通信能力，直接传输数据会导致网络拥堵和不合理的执行。中央化数据处理在数据中心也不再可行，因为关于数据隐私和安全的 Concerns。</li>
<li>methods: 我们提出了一个创新的 Edge-assisted U-Shaped Split Federated Learning（EUSFL）框架，利用边缘服务器的高性能能力协助IoT设备进行模型训练和优化过程。在这个框架中，我们运用了 Federated Learning（FL），让数据持有者共同训练模型，而不需要分享数据，从而提高隐私保护。此外，我们将神经网络分为三部分使用U-型分割，让IoT设备进行本地训练。这样可以利用边缘服务器的更高计算能力，实现全面训练时间的缩短，并让IoT设备 avec varying capabilities 进行训练任务得以高效。</li>
<li>results: 我们的理论分析和实验结果显示，EUSFL可以与不同的聚合算法结合使用，在不同的IoT设备计算能力下保持良好的性能，并对训练时间和本地计算负载进行了明显的缩短。此外，我们还提出了一种新的杂音机制 called LabelDP，以保护数据特征和标签免受重建攻击，排除隐私泄露的风险。<details>
<summary>Abstract</summary>
In the realm of the Internet of Things (IoT), deploying deep learning models to process data generated or collected by IoT devices is a critical challenge. However, direct data transmission can cause network congestion and inefficient execution, given that IoT devices typically lack computation and communication capabilities. Centralized data processing in data centers is also no longer feasible due to concerns over data privacy and security. To address these challenges, we present an innovative Edge-assisted U-Shaped Split Federated Learning (EUSFL) framework, which harnesses the high-performance capabilities of edge servers to assist IoT devices in model training and optimization process. In this framework, we leverage Federated Learning (FL) to enable data holders to collaboratively train models without sharing their data, thereby enhancing data privacy protection by transmitting only model parameters. Additionally, inspired by Split Learning (SL), we split the neural network into three parts using U-shaped splitting for local training on IoT devices. By exploiting the greater computation capability of edge servers, our framework effectively reduces overall training time and allows IoT devices with varying capabilities to perform training tasks efficiently. Furthermore, we proposed a novel noise mechanism called LabelDP to ensure that data features and labels can securely resist reconstruction attacks, eliminating the risk of privacy leakage. Our theoretical analysis and experimental results demonstrate that EUSFL can be integrated with various aggregation algorithms, maintaining good performance across different computing capabilities of IoT devices, and significantly reducing training time and local computation overhead.
</details>
<details>
<summary>摘要</summary>
在互联网物联网（IoT）领域，部署深度学习模型来处理由IoT设备生成或收集的数据是一项关键挑战。然而，直接数据传输会导致网络拥堵和不efficient执行，因为IoT设备通常缺乏计算和通信能力。中央化数据处理在数据中心也不再可行，因为数据隐私和安全问题。为解决这些挑战，我们提出了一种创新的Edge助けU型分布式学习（EUSFL）框架，利用边缘服务器的高性能特性来帮助IoT设备进行模型训练和优化过程。在这个框架中，我们运用分布式学习（FL），使得数据持有者可以共同训练模型，而不需要将数据共享，从而提高数据隐私保护。此外，受到分learn（SL）的启发，我们将神经网络分成三部分，在IoT设备上进行本地训练。通过利用边缘服务器的更高计算能力，我们的框架可以有效减少总训练时间，让IoT设备按照不同的能力进行训练任务，并且可以保持不同计算能力的IoT设备之间的兼容性。此外，我们还提出了一种新的噪声机制called LabelDP，以保护数据特征和标签免受重建攻击，从而消除隐私泄露的风险。我们的理论分析和实验结果表明，EUSFL可以与不同的聚合算法结合使用，保持不同计算能力的IoT设备之间的兼容性，同时减少训练时间和本地计算负担。
</details></li>
</ul>
<hr>
<h2 id="Improving-Pacing-in-Long-Form-Story-Planning"><a href="#Improving-Pacing-in-Long-Form-Story-Planning" class="headerlink" title="Improving Pacing in Long-Form Story Planning"></a>Improving Pacing in Long-Form Story Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04459">http://arxiv.org/abs/2311.04459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yichenzw/pacing">https://github.com/yichenzw/pacing</a></li>
<li>paper_authors: Yichen Wang, Kevin Yang, Xiaoming Liu, Dan Klein</li>
<li>for: 提高自动生成故事大纲的自然整体感 (improve the natural pacing of automatically generated story outlines)</li>
<li>methods: 使用 coneCreteness 评价器控制层次大纲生成 (use a concreteness evaluator to control hierarchical outline generation)，并使用 predicted concreteness 筛选新的大纲项 (filter new outline items based on predicted concreteness)</li>
<li>results: 与基线对比，人类评价 CONCOCT 的均衡性高于 57% 多个大纲长度 (compared to a baseline, humans judge CONCOCT’s pacing to be more consistent over 57% of the time across multiple outline lengths)<details>
<summary>Abstract</summary>
Existing LLM-based systems for writing long-form stories or story outlines frequently suffer from unnatural pacing, whether glossing over important events or over-elaborating on insignificant details, resulting in a jarring experience for the reader. We propose a CONCrete Outline ConTrol (CONCOCT) system to improve pacing when automatically generating story outlines. We first train a concreteness evaluator to judge which of two events is more concrete (low-level-detailed). This evaluator can then be used to control pacing in hierarchical outline generation; in this work, we explore a vaguest-first expansion procedure that aims for uniform pacing. We further use the evaluator to filter new outline items based on predicted concreteness. Compared to a baseline hierarchical outline generator, humans judge CONCOCT's pacing to be more consistent over 57% of the time across multiple outline lengths; the gains also translate to downstream stories. All code, data, and models are open-sourced.
</details>
<details>
<summary>摘要</summary>
We first train a concreteness evaluator to determine which of two events is more concrete (low-level-detailed). This evaluator is then used to control pacing in hierarchical outline generation. Specifically, we use a vaguest-first expansion procedure that aims for uniform pacing. Additionally, we use the evaluator to filter new outline items based on their predicted concreteness.Compared to a baseline hierarchical outline generator, humans judge CONCOCT's pacing to be more consistent over 57% of the time across multiple outline lengths. Furthermore, the gains translate to downstream stories. All code, data, and models are open-sourced.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Uncertainty-Quantification-approaches-for-Neural-PDEs-in-scientific-applications"><a href="#Evaluating-Uncertainty-Quantification-approaches-for-Neural-PDEs-in-scientific-applications" class="headerlink" title="Evaluating Uncertainty Quantification approaches for Neural PDEs in scientific applications"></a>Evaluating Uncertainty Quantification approaches for Neural PDEs in scientific applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04457">http://arxiv.org/abs/2311.04457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vardhan Dongre, Gurpreet Singh Hora<br>for: This paper focuses on the development of Uncertainty Quantification (UQ) methods for Neural Partial Differential Equations (Neural PDEs) in scientific applications, specifically for forward and inverse problems.methods: The paper evaluates various UQ approaches, including Bayesian methods such as Hamiltonian Monte Carlo (HMC) and Monte-Carlo Dropout (MCD), as well as a conventional approach using Deep Ensembles (DE).results: The results show that Neural PDEs can effectively reconstruct flow systems and predict the associated unknown parameters, but the Bayesian methods tend to display a higher degree of certainty in their predictions compared to the DE approach. This suggests that Bayesian techniques may underestimate the true underlying uncertainty, appearing more confident in their predictions than the DE approach.Here’s the Chinese version of the three key points:for: 这篇论文关注使用神经partial differential equations (Neural PDEs)在科学应用中的uncertainty量化 (UQ)方法的开发，特别是对于前向和反向问题。methods: 论文评估了多种UQ方法，包括 bayesian方法such as Hamiltonian Monte Carlo (HMC)和Monte-Carlo Dropout (MCD)，以及一种传统的 Deep Ensembles (DE) 方法。results: 结果显示 Neural PDEs 可以有效地重construct流体系统和相关的未知参数，但 bayesian方法在其预测中显示了更高的自信度，与 DE 方法相比。这表明 bayesian 技术可能会下降 true 的下层不确定性，因此在其预测中显得更自信。<details>
<summary>Abstract</summary>
The accessibility of spatially distributed data, enabled by affordable sensors, field, and numerical experiments, has facilitated the development of data-driven solutions for scientific problems, including climate change, weather prediction, and urban planning. Neural Partial Differential Equations (Neural PDEs), which combine deep learning (DL) techniques with domain expertise (e.g., governing equations) for parameterization, have proven to be effective in capturing valuable correlations within spatiotemporal datasets. However, sparse and noisy measurements coupled with modeling approximation introduce aleatoric and epistemic uncertainties. Therefore, quantifying uncertainties propagated from model inputs to outputs remains a challenge and an essential goal for establishing the trustworthiness of Neural PDEs. This work evaluates various Uncertainty Quantification (UQ) approaches for both Forward and Inverse Problems in scientific applications. Specifically, we investigate the effectiveness of Bayesian methods, such as Hamiltonian Monte Carlo (HMC) and Monte-Carlo Dropout (MCD), and a more conventional approach, Deep Ensembles (DE). To illustrate their performance, we take two canonical PDEs: Burger's equation and the Navier-Stokes equation. Our results indicate that Neural PDEs can effectively reconstruct flow systems and predict the associated unknown parameters. However, it is noteworthy that the results derived from Bayesian methods, based on our observations, tend to display a higher degree of certainty in their predictions as compared to those obtained using the DE. This elevated certainty in predictions suggests that Bayesian techniques might underestimate the true underlying uncertainty, thereby appearing more confident in their predictions than the DE approach.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用可得到的散点数据，由于可靠且有效的感知器和数值实验，解决科学问题，包括气候变化、天气预测和城市规划等。神经partial differential equations（神经PDEs），结合深度学习（DL）技术和领域专业知识（例如，管理方程）进行参数化，能够很好地捕捉空间时间数据中的有价值相关性。然而，稀缺和噪声的测量数据，加之模型简化，导致 aleatoric和epistemicuncertainty。因此，将模型输入到输出中的不确定性进行评估是一项重要的任务，以确保神经PDEs的可靠性。本工作评估了多种uncertainty quantification（UQ）方法，包括forward和 inverse问题在科学应用中。 Specifically, we investigate the effectiveness of Bayesian methods, such as Hamiltonian Monte Carlo (HMC) and Monte-Carlo Dropout (MCD), and a more conventional approach, Deep Ensembles (DE). To illustrate their performance, we take two canonical PDEs: Burger's equation and the Navier-Stokes equation. Our results indicate that Neural PDEs can effectively reconstruct flow systems and predict the associated unknown parameters. However, it is noteworthy that the results derived from Bayesian methods, based on our observations, tend to display a higher degree of certainty in their predictions as compared to those obtained using the DE. This elevated certainty in predictions suggests that Bayesian techniques might underestimate the true underlying uncertainty, thereby appearing more confident in their predictions than the DE approach.Translated by Google Translate.
</details></li>
</ul>
<hr>
<h2 id="MathNAS-If-Blocks-Have-a-Role-in-Mathematical-Architecture-Design"><a href="#MathNAS-If-Blocks-Have-a-Role-in-Mathematical-Architecture-Design" class="headerlink" title="MathNAS: If Blocks Have a Role in Mathematical Architecture Design"></a>MathNAS: If Blocks Have a Role in Mathematical Architecture Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04943">http://arxiv.org/abs/2311.04943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangqinsi1/mathnas">https://github.com/wangqinsi1/mathnas</a></li>
<li>paper_authors: Wang Qinsi, Ke Jinhan, Liang Zhi, Zhang Sihai</li>
<li>For: 这个研究是要解决Neural Architecture Search（NAS）中的大型模型设计问题，因为现有的方法在搜寻和评估候选网络时需要庞大的 computation cost。* Methods: 这篇研究提出了一个新的分治策略，利用搜寻空间的弹性特性，将候选网络的表现计算分解为各个专案中的表现，并使用数学程式来预测网络表现。* Results: 这篇研究显示，这个新的分治策略可以实现快速的网络表现评估，并且可以实现更高的准确性和更快的搜寻速度。<details>
<summary>Abstract</summary>
Neural Architecture Search (NAS) has emerged as a favoured method for unearthing effective neural architectures. Recent development of large models has intensified the demand for faster search speeds and more accurate search results. However, designing large models by NAS is challenging due to the dramatical increase of search space and the associated huge performance evaluation cost. Consider a typical modular search space widely used in NAS, in which a neural architecture consists of $m$ block nodes and a block node has $n$ alternative blocks. Facing the space containing $n^m$ candidate networks, existing NAS methods attempt to find the best one by searching and evaluating candidate networks directly.Different from the general strategy that takes architecture search as a whole problem, we propose a novel divide-and-conquer strategy by making use of the modular nature of the search space.Here, we introduce MathNAS, a general NAS framework based on mathematical programming.In MathNAS, the performances of the $m*n$ possible building blocks in the search space are calculated first, and then the performance of a network is directly predicted based on the performances of its building blocks. Although estimating block performances involves network training, just as what happens for network performance evaluation in existing NAS methods, predicting network performance is completely training-free and thus extremely fast. In contrast to the $n^m$ candidate networks to evaluate in existing NAS methods, which require training and a formidable computational burden, there are only $m*n$ possible blocks to handle in MathNAS. Therefore, our approach effectively reduces the complexity of network performance evaluation.Our code is available at https://github.com/wangqinsi1/MathNAS.
</details>
<details>
<summary>摘要</summary>
neural Architecture Search (NAS) 已经成为发掘有效神经建筑的首选方法。 recent development of large models 使得寻找更快的搜索速度和更准确的搜索结果变得更加紧迫。然而，通过 NAS 设计大型模型是挑战，因为搜索空间的增加会导致搜索速度的增加和评估成本的增加。 faces a typical modular search space widely used in NAS, in which a neural architecture consists of $m$ block nodes and a block node has $n$ alternative blocks. existing NAS methods attempt to find the best one by searching and evaluating candidate networks directly. unlike the general strategy that takes architecture search as a whole problem, we propose a novel divide-and-conquer strategy by making use of the modular nature of the search space. here, we introduce MathNAS, a general NAS framework based on mathematical programming. in MathNAS, the performances of the $m*n$ possible building blocks in the search space are calculated first, and then the performance of a network is directly predicted based on the performances of its building blocks. although estimating block performances involves network training, just as what happens for network performance evaluation in existing NAS methods, predicting network performance is completely training-free and thus extremely fast. in contrast to the $n^m$ candidate networks to evaluate in existing NAS methods, which require training and a formidable computational burden, there are only $m*n$ possible blocks to handle in MathNAS. therefore, our approach effectively reduces the complexity of network performance evaluation. our code is available at https://github.com/wangqinsi1/MathNAS.
</details></li>
</ul>
<hr>
<h2 id="MixTEA-Semi-supervised-Entity-Alignment-with-Mixture-Teaching"><a href="#MixTEA-Semi-supervised-Entity-Alignment-with-Mixture-Teaching" class="headerlink" title="MixTEA: Semi-supervised Entity Alignment with Mixture Teaching"></a>MixTEA: Semi-supervised Entity Alignment with Mixture Teaching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04441">http://arxiv.org/abs/2311.04441</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiefeng69/mixtea">https://github.com/xiefeng69/mixtea</a></li>
<li>paper_authors: Feng Xie, Xin Song, Xiang Zeng, Xuechen Zhao, Lei Tian, Bin Zhou, Yusong Tan</li>
<li>for: 本文提出了一种新的半监督实体对应（EA）方法，以解决由于缺乏充分的标注数据而带来的实体对应问题。</li>
<li>methods: 本文使用了一种独特的结合人工标注和 probabilistic pseudo 对应的混合教学方法，并在 pseudo 对应学习中提出了bi-directional voting（BDV）策略和匹配多样性基于修正（MDR）模块，以降低噪声对 pseudo 对应学习的负面影响。</li>
<li>results: 对于多个 benchmark 数据集，以及进一步的分析，表明了我们提出的方法的优越性和实用性。<details>
<summary>Abstract</summary>
Semi-supervised entity alignment (EA) is a practical and challenging task because of the lack of adequate labeled mappings as training data. Most works address this problem by generating pseudo mappings for unlabeled entities. However, they either suffer from the erroneous (noisy) pseudo mappings or largely ignore the uncertainty of pseudo mappings. In this paper, we propose a novel semi-supervised EA method, termed as MixTEA, which guides the model learning with an end-to-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings. We firstly train a student model using few labeled mappings as standard. More importantly, in pseudo mapping learning, we propose a bi-directional voting (BDV) strategy that fuses the alignment decisions in different directions to estimate the uncertainty via the joint matching confidence score. Meanwhile, we also design a matching diversity-based rectification (MDR) module to adjust the pseudo mapping learning, thus reducing the negative influence of noisy mappings. Extensive results on benchmark datasets as well as further analyses demonstrate the superiority and the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
semi-supervised entity alignment（EA）是一个实际和挑战性的任务，因为缺乏充足的标注映射作为训练数据。大多数工作是通过生成 Pseudo 映射来 Address 这个问题，但它们都会受到 Pseudo 映射的错误（噪声）的影响或者忽略 Pseudo 映射的不确定性。在这篇论文中，我们提出了一种新的 semi-supervised EA 方法，名为 MixTEA，它使用端到端混合教学法和概率 Pseudo 映射来导引模型学习。我们首先在几个标注映射上训练一个学生模型。更重要的是，在 Pseudo 映射学习中，我们提出了两个方向投票（BDV）策略，它将在不同的方向投票结果中进行折衔，以便估计 uncertainty via 联合匹配信息指数。同时，我们还设计了一个匹配多样性基于修正（MDR）模块，以降低 Pseudo 映射学习中的负面影响。我们在标准 benchmark 数据集以及进一步的分析中表明了我们的提出的方法的优越性和效果。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Geoscience-Artificial-Intelligence-XGeoS-AI-Application-to-Demystify-Image-Recognition"><a href="#Interpretable-Geoscience-Artificial-Intelligence-XGeoS-AI-Application-to-Demystify-Image-Recognition" class="headerlink" title="Interpretable Geoscience Artificial Intelligence (XGeoS-AI): Application to Demystify Image Recognition"></a>Interpretable Geoscience Artificial Intelligence (XGeoS-AI): Application to Demystify Image Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04940">http://arxiv.org/abs/2311.04940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin-Jian Xu, Hao Zhang, Chao-Sheng Tang, Lin Li, Bin Shi</li>
<li>for: 这个研究的目的是解释地球科学中的图像识别问题，并提出一种可解释的地球科学人工智能（XGeoS-AI）框架来实现这一目标。</li>
<li>methods: 该框架采用了人类视觉机制的思想，通过在整个图像中选择一个地方生成一个阈值，以完成图像识别任务。此外，可以采用不同的人工智能方法，如支持向量回归（SVR）、多层感知神经网络（MLP）和卷积神经网络（CNN）等，来快速完成地球科学图像识别任务。</li>
<li>results: 实验结果表明，提出的XGeoS-AI框架具有高效、多样化和可解释的优点，有很大的潜力应用于地球科学图像识别问题。此外，该框架还可以推动地球科学领域内的技术创新。<details>
<summary>Abstract</summary>
As Earth science enters the era of big data, artificial intelligence (AI) not only offers great potential for solving geoscience problems, but also plays a critical role in accelerating the understanding of the complex, interactive, and multiscale processes of Earth's behavior. As geoscience AI models are progressively utilized for significant predictions in crucial situations, geoscience researchers are increasingly demanding their interpretability and versatility. This study proposes an interpretable geoscience artificial intelligence (XGeoS-AI) framework to unravel the mystery of image recognition in the Earth sciences, and its effectiveness and versatility is demonstrated by taking computed tomography (CT) image recognition as an example. Inspired by the mechanism of human vision, the proposed XGeoS-AI framework generates a threshold value from a local region within the whole image to complete the recognition. Different kinds of artificial intelligence (AI) methods, such as Support Vector Regression (SVR), Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), can be adopted as the AI engines of the proposed XGeoS-AI framework to efficiently complete geoscience image recognition tasks. Experimental results demonstrate that the effectiveness, versatility, and heuristics of the proposed framework have great potential in solving geoscience image recognition problems. Interpretable AI should receive more and more attention in the field of the Earth sciences, which is the key to promoting more rational and wider applications of AI in the field of Earth sciences. In addition, the proposed interpretable framework may be the forerunner of technological innovation in the Earth sciences.
</details>
<details>
<summary>摘要</summary>
如今地球科学进入大数据时代，人工智能（AI）不仅提供了解决地球科学问题的极大潜力，还扮演着促进地球行为复杂、互动和多尺度过程的加速器。随着地球科学AI模型在重要情况下的广泛应用，地球科学研究人员更加需要其解释性和多样性。本研究提出了一种可解释的地球科学人工智能（XGeoS-AI）框架，以解决地球科学图像识别问题的谜题。灵感自人类视觉机制，提议的XGeoS-AI框架在全图像中 locates 一个区域，并生成该区域的阈值，以完成图像识别。不同的人工智能方法，如支持向量回归（SVR）、多层感知网络（MLP）和卷积神经网络（CNN），可以作为XGeoS-AI框架中的人工智能引擎，高效完成地球科学图像识别任务。实验结果表明，提议的框架具有效果、多样性和启发性，在地球科学图像识别问题上具有很大的潜力。可解释AI在地球科学领域应该收到更多的关注，这是推动AI在地球科学领域的更广泛应用的关键。此外，提议的可解释框架可能成为地球科学技术创新的先驱者。
</details></li>
</ul>
<hr>
<h2 id="LooGLE-Can-Long-Context-Language-Models-Understand-Long-Contexts"><a href="#LooGLE-Can-Long-Context-Language-Models-Understand-Long-Contexts" class="headerlink" title="LooGLE: Can Long-Context Language Models Understand Long Contexts?"></a>LooGLE: Can Long-Context Language Models Understand Long Contexts?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04939">http://arxiv.org/abs/2311.04939</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bigai-nlco/loogle">https://github.com/bigai-nlco/loogle</a></li>
<li>paper_authors: Jiaqi Li, Mengmeng Wang, Zilong Zheng, Muhan Zhang</li>
<li>for: 评估大语言模型（LLMs）在长文本理解方面的能力。</li>
<li>methods: 使用新的文档（post-2022），且文档长度超过24,000个字符，同时采用人工生成的6,000个问题和多个领域的问题集来评估LLMs的长dependency能力。</li>
<li>results: 研究发现：（i）商业模型在LLMs中表现更好；（ii）LLMs在短dependency任务中表现出色，但在更复杂的长dependency任务中表现不佳；（iii）在文本上下文中学习和连接思维只提供了有限的改进；（iv）检索基本技术在短问题 answering中表现出色，而扩展文本窗口长度的技术对长文本理解有限的影响。<details>
<summary>Abstract</summary>
Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards "true long-context understanding".
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），尽管在不同语言任务中表现出色，但通常只能处理文本内容窗口大小。这一限制促进了大量研究，以提高LLM的长期文本理解能力。然而，先前的数据集受到一些缺点的影响，如文本长度较短，与现代LLM的上下文窗口大小相比; 带有数据泄露问题的旧文档; 和更重视短语相互作用而非长语相互作用。本文提出了LooGLE，一个长期语言评估准则，用于评估LLM的长期文本理解能力。LooGLE的特点包括：使用最新的文档（2022年后），文档长度超过24,000个字符，并且新生成了6,000个问题，覆盖多个领域。人工标注员仔细制作了1,100个高质量问题对，以满足长期依赖要求。这些对经过了严格的交叉验证，以便获得LLM的长期依赖能力的最准确评估。八种当前最佳LLM在LooGLE上进行评估后，得到了以下发现：（i）商业模型比开源模型表现更佳；（ii）LLM在短语相互作用任务中表现出色，但在更复杂的长语相互作用任务中受到限制；（iii）嵌入式学习和串联思维只提供了有限的改进；（iv）基于检索的技术在短问题回答任务中具有显著的优势，而扩展上下文窗口长度的策略对长期文本理解的改进具有有限的影响。因此，LooGLE不仅提供了LLM的长期文本理解能力的系统和全面的评估方案，还探讨了未来 LLM 的发展，以实现“真正的长期文本理解”。
</details></li>
</ul>
<hr>
<h2 id="Data-Factors-for-Better-Compositional-Generalization"><a href="#Data-Factors-for-Better-Compositional-Generalization" class="headerlink" title="Data Factors for Better Compositional Generalization"></a>Data Factors for Better Compositional Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04420">http://arxiv.org/abs/2311.04420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/owenzx/data4comp">https://github.com/owenzx/data4comp</a></li>
<li>paper_authors: Xiang Zhou, Yichen Jiang, Mohit Bansal</li>
<li>for: 本文旨在探讨模型在不同数据集上的泛化能力，以及如何通过不同的数据因素来改善模型的泛化能力。</li>
<li>methods: 本文使用Transformer模型在不同数据集上进行训练，并通过分析不同数据因素的影响来解释模型的泛化能力。</li>
<li>results: 研究发现，增加数据复杂度可以提高模型的泛化能力，并且这种改善来自于数据集中提供更多的多样化示例和降低示例重复频率的效果。此外，训练例子的Difficulty Level也对泛化能力产生不同的影响，在 sintetic datasets上，简单的例子更能够 invoke  compose understanding，而在大规模的实际语言 datasets上，一个平衡的混合mixture of simple和hard例子可以induce最强的泛化能力。<details>
<summary>Abstract</summary>
Recent diagnostic datasets on compositional generalization, such as SCAN (Lake and Baroni, 2018) and COGS (Kim and Linzen, 2020), expose severe problems in models trained from scratch on these datasets. However, in contrast to this poor performance, state-of-the-art models trained on larger and more general datasets show better generalization ability. In this work, to reconcile this inconsistency, we conduct an empirical analysis by training Transformer models on a variety of training sets with different data factors, including dataset scale, pattern complexity, example difficulty, etc. First, we show that increased dataset complexity can lead to better generalization behavior on multiple different generalization challenges. To further understand this improvement, we show two axes of the benefit from more complex datasets: they provide more diverse examples so compositional understanding becomes more effective, and they also prevent ungeneralizable memorization of the examples due to reduced example repetition frequency. Finally, we explore how training examples of different difficulty levels influence generalization differently. On synthetic datasets, simple examples invoke stronger compositionality than hard examples do. On larger-scale real language datasets, while hard examples become more important potentially to ensure decent data coverage, a balanced mixture of simple and hard examples manages to induce the strongest generalizability. The code and data for this work are available at https://github.com/owenzx/data4comp
</details>
<details>
<summary>摘要</summary>
Recent diagnostic datasets on compositional generalization, such as SCAN (Lake and Baroni, 2018) and COGS (Kim and Linzen, 2020), have revealed severe problems in models trained from scratch on these datasets. However, in contrast to this poor performance, state-of-the-art models trained on larger and more general datasets have shown better generalization ability. In this work, we aim to reconcile this inconsistency by conducting an empirical analysis of Transformer models trained on various training sets with different data factors, including dataset scale, pattern complexity, example difficulty, etc.First, we find that increased dataset complexity leads to better generalization behavior on multiple different generalization challenges. To further understand this improvement, we identify two axes of benefit from more complex datasets: they provide more diverse examples that enhance compositional understanding, and they also reduce the likelihood of ungeneralizable memorization due to reduced example repetition frequency.Finally, we explore how training examples of different difficulty levels influence generalization differently. On synthetic datasets, simple examples tend to invoke stronger compositionality than hard examples do. On larger-scale real language datasets, while hard examples become more important to ensure decent data coverage, a balanced mixture of simple and hard examples is found to induce the strongest generalizability. The code and data for this work are available at <https://github.com/owenzx/data4comp>.
</details></li>
</ul>
<hr>
<h2 id="PepLand-a-large-scale-pre-trained-peptide-representation-model-for-a-comprehensive-landscape-of-both-canonical-and-non-canonical-amino-acids"><a href="#PepLand-a-large-scale-pre-trained-peptide-representation-model-for-a-comprehensive-landscape-of-both-canonical-and-non-canonical-amino-acids" class="headerlink" title="PepLand: a large-scale pre-trained peptide representation model for a comprehensive landscape of both canonical and non-canonical amino acids"></a>PepLand: a large-scale pre-trained peptide representation model for a comprehensive landscape of both canonical and non-canonical amino acids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04419">http://arxiv.org/abs/2311.04419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruochi Zhang, Haoran Wu, Yuting Xiu, Kewei Li, Ningning Chen, Yu Wang, Yan Wang, Xin Gao, Fengfeng Zhou</li>
<li>For: PepLand is a pre-training architecture for representation and property analysis of peptides spanning both canonical and non-canonical amino acids.* Methods: PepLand leverages a comprehensive multi-view heterogeneous graph neural network to unveil the subtle structural representations of peptides.* Results: PepLand effectively captures salient synthetic peptide features, laying a robust foundation for transformative advances in peptide-centric research domains.Here’s the Chinese translation of the three points:* For: 这篇研究旨在提出一个专门针对包含非标准氨基酸的肽的预训架构，以探索这些肽的积极特征和性能。* Methods: 这个预训架构使用了一个全面的多观察异源Graph Neural Network（GNN），以捕捉这些肽的细微结构特征。* Results: PepLand 能够有效地捕捉这些肽的积极特征，实现了在肽中心研究领域中的创新进步。<details>
<summary>Abstract</summary>
In recent years, the scientific community has become increasingly interested on peptides with non-canonical amino acids due to their superior stability and resistance to proteolytic degradation. These peptides present promising modifications to biological, pharmacological, and physiochemical attributes in both endogenous and engineered peptides. Notwithstanding their considerable advantages, the scientific community exhibits a conspicuous absence of an effective pre-trained model adept at distilling feature representations from such complex peptide sequences. We herein propose PepLand, a novel pre-training architecture for representation and property analysis of peptides spanning both canonical and non-canonical amino acids. In essence, PepLand leverages a comprehensive multi-view heterogeneous graph neural network tailored to unveil the subtle structural representations of peptides. Empirical validations underscore PepLand's effectiveness across an array of peptide property predictions, encompassing protein-protein interactions, permeability, solubility, and synthesizability. The rigorous evaluation confirms PepLand's unparalleled capability in capturing salient synthetic peptide features, thereby laying a robust foundation for transformative advances in peptide-centric research domains. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/pepland
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AI-accelerated-Discovery-of-Altermagnetic-Materials"><a href="#AI-accelerated-Discovery-of-Altermagnetic-Materials" class="headerlink" title="AI-accelerated Discovery of Altermagnetic Materials"></a>AI-accelerated Discovery of Altermagnetic Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04418">http://arxiv.org/abs/2311.04418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zfgao66/mataltmag">https://github.com/zfgao66/mataltmag</a></li>
<li>paper_authors: Ze-Feng Gao, Shuai Qu, Bocheng Zeng, Ji-Rong Wen, Hao Sun, Pengjie Guo, Zhong-Yi Lu<br>for: 这篇论文旨在探索新型磁性阶段—— alternate magnetism，以及发现更多的磁性材料。methods: 这篇论文使用了人工智能搜索引擎，融合了对组态分析、图像神经网络预训、最佳运输理论和基本电子结构计算，发现25种新的磁性材料，包括金属、半导体和对磁体。results: 这篇论文发现了25种新的磁性材料，其中8种是$i$-波磁性材料，这些材料具有独特的物理性能，例如非常性 Hall 效应、非常性 Kerr 效应和トポロジック 性。<details>
<summary>Abstract</summary>
Altermagnetism, a new magnetic phase, has been theoretically proposed and experimentally verified to be distinct from ferromagnetism and antiferromagnetism. Although altermagnets have been found to possess many exotic physical properties, the very limited availability of known altermagnetic materials~(e.g., 14 confirmed materials) hinders the study of such properties. Hence, discovering more types of altermagnetic materials is crucial for a comprehensive understanding of altermagnetism and thus facilitating new applications in the next generation information technologies, e.g., storage devices and high-sensitivity sensors. Here, we report 25 new altermagnetic materials that cover metals, semiconductors, and insulators, discovered by an AI search engine unifying symmetry analysis, graph neural network pre-training, optimal transport theory, and first-principles electronic structure calculation. The wide range of electronic structural characteristics reveals that various innovative physical properties manifest in these newly discovered altermagnetic materials, e.g., anomalous Hall effect, anomalous Kerr effect, and topological property. Noteworthy, we discovered 8 $i$-wave altermagnetic materials for the first time. Overall, the AI search engine performs much better than human experts and suggests a set of new altermagnetic materials with unique properties, outlining its potential for accelerated discovery of altermagnetic materials.
</details>
<details>
<summary>摘要</summary>
新型磁相���, 名为“alternromagnetism”, 已经理论上提出并实验验证, 与��磁和反磁异有区别。 although altermagnets possess many exotic physical properties, the limited availability of known altermagnetic materials (e.g., 14 confirmed materials) hinders the study of such properties. Therefore, discovering more types of altermagnetic materials is crucial for a comprehensive understanding of altermagnetism and will facilitate new applications in the next generation information technologies, such as storage devices and high-sensitivity sensors.我们报告了25种新的� alternate magnetic materials，包括� metal、半导体和半导体，通过� unity symmetry analysis、graph neural network pre-training、optimal transport theory和� first-principles electronic structure calculation发现。 these newly discovered altermagnetic materials exhibit a wide range of electronic structural characteristics, resulting in various innovative physical properties, such as anomalous Hall effect, anomalous Kerr effect, and topological property. noteworthy, we discovered 8 $i$-wave altermagnetic materials for the first time.相比� human experts, AI search engine表现更好，提供了一些新的� alternate magnetic materials with unique properties, highlighting its potential for accelerated discovery of altermagnetic materials.
</details></li>
</ul>
<hr>
<h2 id="Human-Conditional-Reasoning-in-Answer-Set-Programming"><a href="#Human-Conditional-Reasoning-in-Answer-Set-Programming" class="headerlink" title="Human Conditional Reasoning in Answer Set Programming"></a>Human Conditional Reasoning in Answer Set Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04412">http://arxiv.org/abs/2311.04412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Chiaki Sakama</li>
<li>for: 本研究探讨了人类思维中的四种推理类型，包括假设前提 (AA)、假设结果 (AC)、否定前提 (DA) 和否定结果 (DC)。</li>
<li>methods: 本文使用 answer set programming 实现了 AC、DA 和 DC 推理类型，并研究了这些推理类型的正式性和人类思维任务的相关性。</li>
<li>results: 本研究发现，AC 和 DA 推理类型在日常生活中很常见，而 DC 推理类型则是逻辑有效的。此外，本文还应用了这些完成方法于通用智能 reasoning 领域。<details>
<summary>Abstract</summary>
Given a conditional sentence P=>Q (if P then Q) and respective facts, four different types of inferences are observed in human reasoning. Affirming the antecedent (AA) (or modus ponens) reasons Q from P; affirming the consequent (AC) reasons P from Q; denying the antecedent (DA) reasons -Q from -P; and denying the consequent (DC) (or modus tollens) reasons -P from -Q. Among them, AA and DC are logically valid, while AC and DA are logically invalid and often called logical fallacies. Nevertheless, humans often perform AC or DA as pragmatic inference in daily life. In this paper, we realize AC, DA and DC inferences in answer set programming. Eight different types of completion are introduced and their semantics are given by answer sets. We investigate formal properties and characterize human reasoning tasks in cognitive psychology. Those completions are also applied to commonsense reasoning in AI.
</details>
<details>
<summary>摘要</summary>
Given a conditional sentence P=>Q (if P then Q) and respective facts, four different types of inferences are observed in human reasoning. Affirming the antecedent (AA) (or modus ponens) reasons Q from P; affirming the consequent (AC) reasons P from Q; denying the antecedent (DA) reasons -Q from -P; and denying the consequent (DC) (or modus tollens) reasons -P from -Q. Among them, AA and DC are logically valid, while AC and DA are logically invalid and often called logical fallacies. Nevertheless, humans often perform AC or DA as pragmatic inference in daily life. In this paper, we realize AC, DA, and DC inferences in answer set programming. Eight different types of completion are introduced and their semantics are given by answer sets. We investigate formal properties and characterize human reasoning tasks in cognitive psychology. Those completions are also applied to commonsense reasoning in AI.Here's the translation in Traditional Chinese: givent a conditional sentence P=>Q (if P then Q) and respective facts, four different types of inferences are observed in human reasoning. Affirming the antecedent (AA) (or modus ponens) reasons Q from P; affirming the consequent (AC) reasons P from Q; denying the antecedent (DA) reasons -Q from -P; and denying the consequent (DC) (or modus tollens) reasons -P from -Q. Among them, AA and DC are logically valid, while AC and DA are logically invalid and often called logical fallacies. Nevertheless, humans often perform AC or DA as pragmatic inference in daily life. In this paper, we realize AC, DA, and DC inferences in answer set programming. Eight different types of completion are introduced and their semantics are given by answer sets. We investigate formal properties and characterize human reasoning tasks in cognitive psychology. Those completions are also applied to commonsense reasoning in AI.
</details></li>
</ul>
<hr>
<h2 id="Improved-DDIM-Sampling-with-Moment-Matching-Gaussian-Mixtures"><a href="#Improved-DDIM-Sampling-with-Moment-Matching-Gaussian-Mixtures" class="headerlink" title="Improved DDIM Sampling with Moment Matching Gaussian Mixtures"></a>Improved DDIM Sampling with Moment Matching Gaussian Mixtures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04938">http://arxiv.org/abs/2311.04938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prasad Gabbur</li>
<li>for: 用于加速采样从预训练的推 Sobolev 模型（DDPM）中。</li>
<li>methods: 使用 Gaussian Mixture Model（GMM）作为反转过程操作（核），具体是匹配 DDPM 前向积分的首两个中心差异。</li>
<li>results: 通过对 CelebAHQ 和 FFHQ 的不条件模型以及 ImageNet 的类条件模型进行实验，并证明使用 GMM 核可以在少量采样步骤下提高生成样本的质量，并且在 FID 和 IS 指标中具有显著改善。例如在 ImageNet 256x256 上，使用 10 步采样，我们可以达到 FID 6.94 和 IS 207.85，与 Gaussian 核相比，这些指标的值分别为 10.15 和 196.73。<details>
<summary>Abstract</summary>
We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of 6.94 and IS of 207.85 with a GMM kernel compared to 10.15 and 196.73 respectively with a Gaussian kernel.
</details>
<details>
<summary>摘要</summary>
我们提议使用 Gaussian Mixture Model（GMM）作为反转过程操作器（核函数）在 Denoising Diffusion Implicit Models（DDIM）框架中，这是一种广泛使用的方法来加速从预训练的 Denoising Diffusion Probabilistic Models（DDPM）中采样。我们匹配了 DDPM 前向分布的首和第二个中心均值，通过限制 GMM 参数来实现这一点。我们发现， momemt 匹配是 suficient 来获得与原始 DDIM  Gaussian 核函数相同或更好的质量的采样。我们在 CelebAHQ 和 FFHQ 上训练了无条件模型，并在 ImageNet 数据集上训练了类别 condition 模型。我们的实验结果表明，使用 GMM 核函数可以在少量采样步骤下获得较好的采样质量， как measured by FID 和 IS 度量。例如，在 ImageNet 256x256 上，使用 10 步骤，我们达到了 FID 6.94 和 IS 207.85 ，与 Gaussian 核函数相比，分别降低了 3.21 和 139.88。
</details></li>
</ul>
<hr>
<h2 id="Human-Centered-Planning"><a href="#Human-Centered-Planning" class="headerlink" title="Human-Centered Planning"></a>Human-Centered Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04403">http://arxiv.org/abs/2311.04403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Yuliang Li, Nitin Kamra, Ruta Desai, Alon Halevy</li>
<li>for: 本研究旨在开发一个基于深度学习模型（LLM）的计划程序，以便在用户提供的自然语言约束下，生成一个合理的计划。</li>
<li>methods: 本研究使用了LLM和符号逻辑计划程序（SymPlan），并将两者结合在一起以提高计划的可靠性和用户满意度。</li>
<li>results: 实验结果显示，LLMPlan与传统的符号逻辑计划程序相比，在不具有正式约束的情况下，平均表现相当，而且能够更好地满足用户的隐式需求。在交互评估中，LLM-基于的计划程序的用户满意度高于符号逻辑计划程序的用户满意度（70.5% vs. 40.4%）。<details>
<summary>Abstract</summary>
LLMs have recently made impressive inroads on tasks whose output is structured, such as coding, robotic planning and querying databases. The vision of creating AI-powered personal assistants also involves creating structured outputs, such as a plan for one's day, or for an overseas trip. Here, since the plan is executed by a human, the output doesn't have to satisfy strict syntactic constraints. A useful assistant should also be able to incorporate vague constraints specified by the user in natural language. This makes LLMs an attractive option for planning.   We consider the problem of planning one's day. We develop an LLM-based planner (LLMPlan) extended with the ability to self-reflect on its output and a symbolic planner (SymPlan) with the ability to translate text constraints into a symbolic representation. Despite no formal specification of constraints, we find that LLMPlan performs explicit constraint satisfaction akin to the traditional symbolic planners on average (2% performance difference), while retaining the reasoning of implicit requirements. Consequently, LLM-based planners outperform their symbolic counterparts in user satisfaction (70.5% vs. 40.4%) during interactive evaluation with 40 users.
</details>
<details>
<summary>摘要</summary>
LLMs 近期在输出结构化任务上做出了印象深刻的进展，如编程、机器人规划和查询数据库等。创建基于 AI 的人工智能助手也涉及到创建结构化输出，如一天的计划或国外旅行计划。在这些情况下，由人类执行计划，输出不需要严格的语法约束。一个有用的助手应该能够根据用户提供的自然语言笔记中的抽象约束进行计划。这使得 LLMs 成为规划的有力选择。我们考虑一天的规划问题。我们开发了一个基于 LLM 的规划器（LLMPlan），并增加了对自己输出的自适应能力以及一个基于符号的规划器（SymPlan），可以将自然语言约束转换为符号表示。虽无正式约束规则，但我们发现 LLMPlan 在平均上与传统的符号规划器相当于满足约束（2%性能差异），同时保留了逻辑推理的隐式要求。因此，基于 LLM 的规划器在用户满意度方面（70.5% vs. 40.4%）在交互评估中超过符号规划器。
</details></li>
</ul>
<hr>
<h2 id="LRM-Large-Reconstruction-Model-for-Single-Image-to-3D"><a href="#LRM-Large-Reconstruction-Model-for-Single-Image-to-3D" class="headerlink" title="LRM: Large Reconstruction Model for Single Image to 3D"></a>LRM: Large Reconstruction Model for Single Image to 3D</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04400">http://arxiv.org/abs/2311.04400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, Hao Tan</li>
<li>for: 预测3D模型从单个输入图像中</li>
<li>methods: 使用高可Scalable transformer-based architecture和500万可学习参数直接预测神经辐射场（NeRF）</li>
<li>results: 可以高效地预测3D模型，包括从真实捕捉和生成模型中的图像中Here is the same information in Simplified Chinese:</li>
<li>for: 预测3D模型从单个输入图像中</li>
<li>methods: 使用高可Scalable transformer-based architecture和500万可学习参数直接预测神经辐射场（NeRF）</li>
<li>results: 可以高效地预测3D模型，包括从真实捕捉和生成模型中的图像中<details>
<summary>Abstract</summary>
We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs including real-world in-the-wild captures and images from generative models. Video demos and interactable 3D meshes can be found on this website: https://yiconghong.me/LRM/.
</details>
<details>
<summary>摘要</summary>
我们提出了首个大型重建模型（LRM），可以从单个输入图像中预测对象的3D模型，只需5秒钟。与以往的方法不同，LRM采用了可扩展的变换器基 architecture，并有5亿个学习参数来直接预测神经辐射场（NeRF）。我们在终端到终 Point manner进行了大规模的训练，使用了包括Objaverse中的 sintetic renderings和MVImgNet中的真实捕捉的大量多视图数据，共约1000万个对象。这种高容量模型和大规模的训练数据使得我们的模型具有高度泛化性和可以高质量地从多种测试输入，包括实际世界中的野外捕捉和生成模型中的图像，生成高质量的3D重建。网站上有视频 demo和可交互的3D mesh：https://yiconghong.me/LRM/。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/cs.AI_2023_11_08/" data-id="clpxp6bxg0071ee88fsnthrfq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/cs.CL_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T11:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/08/cs.CL_2023_11_08/">cs.CL - 2023-11-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Learning-Brasil-at-ABSAPT-2022-Portuguese-Transformer-Ensemble-Approaches"><a href="#Deep-Learning-Brasil-at-ABSAPT-2022-Portuguese-Transformer-Ensemble-Approaches" class="headerlink" title="Deep Learning Brasil at ABSAPT 2022: Portuguese Transformer Ensemble Approaches"></a>Deep Learning Brasil at ABSAPT 2022: Portuguese Transformer Ensemble Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05051">http://arxiv.org/abs/2311.05051</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ju-resplande/dlb_absapt2022">https://github.com/ju-resplande/dlb_absapt2022</a></li>
<li>paper_authors: Juliana Resplande Santanna Gomes, Eduardo Augusto Santos Garcia, Adalberto Ferreira Barbosa Junior, Ruan Chaves Rodrigues, Diogo Fernandes Costa Silva, Dyonnatan Ferreira Maia, Nádia Félix Felipe da Silva, Arlindo Rodrigues Galvão Filho, Anderson da Silva Soares</li>
<li>for: 这篇论文的目的是提出一种基于方面的 sentiment 分析（ABSA）任务，以分类每个方面的 sentiment 偏好。</li>
<li>methods: 这篇论文使用了 two 个子任务：方面 термина抽取（ATE）和 sentiment 方向抽取（SOE），以实现 ABSA 任务。</li>
<li>results: 作者在 IberLEF 2022 上提交了最佳性系统，实现了两个子任务的新状态ppen-of-the-art 结果。<details>
<summary>Abstract</summary>
Aspect-based Sentiment Analysis (ABSA) is a task whose objective is to classify the individual sentiment polarity of all entities, called aspects, in a sentence. The task is composed of two subtasks: Aspect Term Extraction (ATE), identify all aspect terms in a sentence; and Sentiment Orientation Extraction (SOE), given a sentence and its aspect terms, the task is to determine the sentiment polarity of each aspect term (positive, negative or neutral). This article presents we present our participation in Aspect-Based Sentiment Analysis in Portuguese (ABSAPT) 2022 at IberLEF 2022. We submitted the best performing systems, achieving new state-of-the-art results on both subtasks.
</details>
<details>
<summary>摘要</summary>
《方面基于情感分析（ABSA）的任务是将每个方面（即句子中的个体）的情感方向分类。这个任务包括两个子任务：方面词抽取（ATE）和情感方向抽取（SOE）。在这篇文章中，我们介绍了我们在“方面基于情感分析在葡萄牙语（ABSAPT）2022”中的参与，并提交了最佳性能的系统，创造了新的国际标准记录在两个子任务中。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DeepLearningBrasil-LT-EDI-2023-Exploring-Deep-Learning-Techniques-for-Detecting-Depression-in-Social-Media-Text"><a href="#DeepLearningBrasil-LT-EDI-2023-Exploring-Deep-Learning-Techniques-for-Detecting-Depression-in-Social-Media-Text" class="headerlink" title="DeepLearningBrasil@LT-EDI-2023: Exploring Deep Learning Techniques for Detecting Depression in Social Media Text"></a>DeepLearningBrasil@LT-EDI-2023: Exploring Deep Learning Techniques for Detecting Depression in Social Media Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05047">http://arxiv.org/abs/2311.05047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Garcia, Juliana Gomes, Adalberto Barbosa Júnior, Cardeque Borges, Nádia da Silva</li>
<li>for: 这份研究是为了描述DeepLearningBrasil队伍在DepSign-LT-EDI@RANLP-2023共同任务中的策略，以及他们在该任务中获得的47.0%的Macro F1-Score和2.4%的优势。</li>
<li>methods: 这份研究使用了RoBERTa和DeBERTa模型，并将其进一步预训在一个精心选择的Reddit dataset上，从而增强了对精细心理健康语言的理解。 truncation技术和样本重量技术被用来处理长文本数据，并且使用了样本分成和ensemble技术来结合多个实验中的模型。</li>
<li>results: 这份研究获得了47.0%的Macro F1-Score和2.4%的优势，表明了DeepLearningBrasil队伍在该任务中的成功。<details>
<summary>Abstract</summary>
In this paper, we delineate the strategy employed by our team, DeepLearningBrasil, which secured us the first place in the shared task DepSign-LT-EDI@RANLP-2023, achieving a 47.0% Macro F1-Score and a notable 2.4% advantage. The task was to classify social media texts into three distinct levels of depression - "not depressed," "moderately depressed," and "severely depressed." Leveraging the power of the RoBERTa and DeBERTa models, we further pre-trained them on a collected Reddit dataset, specifically curated from mental health-related Reddit's communities (Subreddits), leading to an enhanced understanding of nuanced mental health discourse. To address lengthy textual data, we used truncation techniques that retained the essence of the content by focusing on its beginnings and endings. Our model was robust against unbalanced data by incorporating sample weights into the loss. Cross-validation and ensemble techniques were then employed to combine our k-fold trained models, delivering an optimal solution. The accompanying code is made available for transparency and further development.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了我们团队（DeepLearningBrasil）在分享任务DepSign-LT-EDI@RANLP-2023中采用的策略，这使得我们在Macro F1-Score上取得了47.0%的分数和一个显著的2.4%优势。任务是将社交媒体文本分为三种不同程度的抑郁症 - "不抑郁", "中度抑郁"和"严重抑郁"。我们利用RoBERTa和DeBERTa模型的力量，并对这些模型进行了进一步预训练，特意是在精心收集的Reddit数据集上（来自医疗社区（Subreddits）），从而提高了我们对精细的心理健康语言的理解。为了处理长文本数据，我们使用了误差截断技术，保留文本内容的核心部分。我们的模型对不均衡数据进行了Robust化，并使用交叉验证和ensemble技术来组合我们的k-fold训练模型，实现了优化的解决方案。 accompaning代码已经公开，以便透明度和进一步开发。
</details></li>
</ul>
<hr>
<h2 id="First-Tragedy-then-Parse-History-Repeats-Itself-in-the-New-Era-of-Large-Language-Models"><a href="#First-Tragedy-then-Parse-History-Repeats-Itself-in-the-New-Era-of-Large-Language-Models" class="headerlink" title="First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models"></a>First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05020">http://arxiv.org/abs/2311.05020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naomi Saphra, Eve Fleisig, Kyunghyun Cho, Adam Lopez</li>
<li>For: The paper aims to provide guidance for NLP researchers in the wake of the success of ChatGPT and other large language models (LLMs), and to identify areas where NLP researchers can continue to make meaningful contributions.* Methods: The paper takes a historical lens and looks back at the first era of LLMs, which began in 2005 with large $n$-gram models for machine translation, to identify durable lessons and evergreen problems in NLP research.* Results: The paper argues that disparities in scale are transient, that data is still a bottleneck for many meaningful applications, that meaningful evaluation informed by actual use is still an open problem, and that there is still room for speculative approaches in NLP research.Here’s the same information in Simplified Chinese:* For: 这篇论文目的是为NLP研究人员提供指导，并identify在LLMs成功后的NLP研究领域中可以继续做出意义性贡献的领域。* Methods: 论文通过历史镜像来看待2005年开始的大语言模型（LLMs）的第一个时期，以 IdentifyNLP研究中可以 preserved的教训和永恒的问题。* Results: 论文 argue that scale disparities are temporary, data is still a bottleneck for many meaningful applications, meaningful evaluation informed by actual use is still an open problem, and there is still room for speculative approaches in NLP research.<details>
<summary>Abstract</summary>
Many NLP researchers are experiencing an existential crisis triggered by the astonishing success of ChatGPT and other systems based on large language models (LLMs). After such a disruptive change to our understanding of the field, what is left to do? Taking a historical lens, we look for guidance from the first era of LLMs, which began in 2005 with large $n$-gram models for machine translation. We identify durable lessons from the first era, and more importantly, we identify evergreen problems where NLP researchers can continue to make meaningful contributions in areas where LLMs are ascendant. Among these lessons, we discuss the primacy of hardware advancement in shaping the availability and importance of scale, as well as the urgent challenge of quality evaluation, both automated and human. We argue that disparities in scale are transient and that researchers can work to reduce them; that data, rather than hardware, is still a bottleneck for many meaningful applications; that meaningful evaluation informed by actual use is still an open problem; and that there is still room for speculative approaches.
</details>
<details>
<summary>摘要</summary>
很多NLP研究人员正经历一场存在危机，这被激进的成功所触发，包括ChatGPT等基于大语言模型（LLM）的系统。在这种突然改变我们理解的领域后，我们可以从历史的视角来寻找指导。我们回顾到2005年开始的大$n$-gram模型 для机器翻译的第一个时期。我们从这个时期中提取了持久的教训，更重要的是，我们标识了在LLM上升起的领域中，NLP研究人员可以继续做出有意义的贡献。我们认为，规模的可用性和重要性受到硬件的提高影响，而不是数据的限制。此外，我们还认为，评估质量是一个急需解决的问题，包括自动化和人类的评估。我们 argue that scale disparities are transient and that researchers can work to reduce them; that data, rather than hardware, is still a bottleneck for many meaningful applications; that meaningful evaluation informed by actual use is still an open problem; and that there is still room for speculative approaches.
</details></li>
</ul>
<hr>
<h2 id="On-the-steerability-of-large-language-models-toward-data-driven-personas"><a href="#On-the-steerability-of-large-language-models-toward-data-driven-personas" class="headerlink" title="On the steerability of large language models toward data-driven personas"></a>On the steerability of large language models toward data-driven personas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04978">http://arxiv.org/abs/2311.04978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Li, Ninareh Mehrabi, Charith Peris, Palash Goyal, Kai-Wei Chang, Aram Galstyan, Richard Zemel, Rahul Gupta</li>
<li>for: 这paper的目的是为了使语言模型更好地适应不同的用户群体和个人，以提高模型的应用性。</li>
<li>methods: 这paper使用了一种基于协同维度的人物定义方法，通过将用户嵌入一个维度空间中，并将其分为基于问题的共谱的群体，以更好地理解不同的社会群体。此外，paper还提出了一种高效的人物引导模型，可以将用户的continue representation映射成虚拟的TOKEN序列，以便使语言模型生成与用户相关的响应。</li>
<li>results:  compared to一系列的基准值，paper的引导模型表现出色，能够更好地适应不同的用户群体。<details>
<summary>Abstract</summary>
The recent surge in Large Language Model (LLM) related applications has led to a concurrent escalation in expectations for LLMs to accommodate a myriad of personas and encompass a broad spectrum of perspectives. An important first step towards addressing this demand is to align language models with specific personas, be it groups of users or individuals. Towards this goal, we first present a new conceptualization of a persona. Moving beyond the traditional reliance on demographics like age, gender, or political party affiliation, we introduce a data-driven persona definition methodology built on collaborative-filtering. In this methodology, users are embedded into a continuous vector space based on their opinions and clustered into cohorts that manifest coherent views across specific inquiries. This methodology allows for a more nuanced understanding of different latent social groups present in the overall population (as opposed to simply using demographic groups) and enhances the applicability of model steerability. Finally, we present an efficient method to steer LLMs towards a particular persona. We learn a soft-prompting model to map the continuous representation of users into sequences of virtual tokens which, when prepended to the LLM input, enables the LLM to produce responses aligned with a given user. Our results show that our steerability algorithm is superior in performance compared to a collection of baselines.
</details>
<details>
<summary>摘要</summary>
Moving beyond traditional demographics like age, gender, or political affiliation, our methodology embeds users in a continuous vector space based on their opinions and clusters them into cohorts with coherent views. This approach provides a more nuanced understanding of latent social groups in the population and enhances the applicability of model steerability.To steer LLMs towards a particular persona, we learn a soft-prompting model that maps continuous user representations into sequences of virtual tokens. When prepended to the LLM input, these tokens enable the model to produce responses aligned with the target user. Our results show that our steerability algorithm outperforms a collection of baselines.
</details></li>
</ul>
<hr>
<h2 id="How-Abstract-Is-Linguistic-Generalization-in-Large-Language-Models-Experiments-with-Argument-Structure"><a href="#How-Abstract-Is-Linguistic-Generalization-in-Large-Language-Models-Experiments-with-Argument-Structure" class="headerlink" title="How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure"></a>How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04900">http://arxiv.org/abs/2311.04900</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clay-lab/structural-alternations">https://github.com/clay-lab/structural-alternations</a></li>
<li>paper_authors: Michael Wilson, Jackson Petty, Robert Frank</li>
<li>for: 这个论文主要研究了大型自然语言处理器（LLM）是否能够表征语言知识中的关系，尤其是对话结构中的关系。</li>
<li>methods: 研究使用了预训练的Transformer型大型自然语言处理器（LLM），并测试其能够在不同的语言上扩展 Novel noun argument 的分布。</li>
<li>results: 研究发现，LLM 在已经在预训练中看到的相关上下文中的扩展性很强，但是在没有seen during pre-training的相关上下文中，LLM 呈现出 linear order 的偏好，这表明当前模型具有限制。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Language models are typically evaluated on their success at predicting the distribution of specific words in specific contexts. Yet linguistic knowledge also encodes relationships between contexts, allowing inferences between word distributions. We investigate the degree to which pre-trained Transformer-based large language models (LLMs) represent such relationships, focusing on the domain of argument structure. We find that LLMs perform well in generalizing the distribution of a novel noun argument between related contexts that were seen during pre-training (e.g., the active object and passive subject of the verb spray), succeeding by making use of the semantically-organized structure of the embedding space for word embeddings. However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order. This finding points to a limitation with current models and points to a reason for which their training is data-intensive.s reported here are available at https://github.com/clay-lab/structural-alternations.
</details>
<details>
<summary>摘要</summary>
语言模型通常通过预训练的特定词语分布来评估其表现。然而，语言知识还包含词语分布之间的关系，允许推理这些分布之间的关系。我们调查了大型转换器基于语言模型（LLM）在推理结构之间的表现，专注于语法结构领域。我们发现，LLM在已经在预训练中看到的相关上下文中的新名动词分布总是能够通过使用词语嵌入空间的semantic结构来成功推理。然而，在预训练中未经见过的相关上下文中，LLM则表现出线性推理的偏好，而不是基于更抽象的结构推理。这一结论指出了当前模型的局限性，并且提出了更多的数据训练的必要性。相关的结果可以在https://github.com/clay-lab/structural-alternations上查看。
</details></li>
</ul>
<hr>
<h2 id="Future-Lens-Anticipating-Subsequent-Tokens-from-a-Single-Hidden-State"><a href="#Future-Lens-Anticipating-Subsequent-Tokens-from-a-Single-Hidden-State" class="headerlink" title="Future Lens: Anticipating Subsequent Tokens from a Single Hidden State"></a>Future Lens: Anticipating Subsequent Tokens from a Single Hidden State</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04897">http://arxiv.org/abs/2311.04897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KoyenaPal/future-lens">https://github.com/KoyenaPal/future-lens</a></li>
<li>paper_authors: Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C. Wallace, David Bau</li>
<li>for: 这个论文目的是检验Transformer模型中每个输入token的隐藏状态 Vector是否含有可预测未来token的信息。</li>
<li>methods: 这篇论文使用了线性预测和 causal intervention 方法来评估Transformer模型中每个隐藏状态 Vector 是否含有可预测未来token的信息。</li>
<li>results: 研究发现，在某些层次上，可以使用单个隐藏状态 Vector 预测后续token的准确率高达48%以上。此外，研究还提出了一种“未来镜”可视化方法，可以使用这些方法创建一个新的Transformer状态视图。<details>
<summary>Abstract</summary>
We conjecture that hidden state vectors corresponding to individual input tokens encode information sufficient to accurately predict several tokens ahead. More concretely, in this paper we ask: Given a hidden (internal) representation of a single token at position $t$ in an input, can we reliably anticipate the tokens that will appear at positions $\geq t + 2$? To test this, we measure linear approximation and causal intervention methods in GPT-J-6B to evaluate the degree to which individual hidden states in the network contain signal rich enough to predict future hidden states and, ultimately, token outputs. We find that, at some layers, we can approximate a model's output with more than 48% accuracy with respect to its prediction of subsequent tokens through a single hidden state. Finally we present a "Future Lens" visualization that uses these methods to create a new view of transformer states.
</details>
<details>
<summary>摘要</summary>
我们推测隐藏状态 вектор对单一输入 tokens 储存信息足够精确预测多个 tokens 之后。更加具体地说，在这篇研究中，我们询问：从一个隐藏（内部）表现单一 tokens 的位置 $t$ 中的隐藏状态，可以预测未来 tokens 的位置 $\geq t + 2$ 的精确性？为了试验这个问题，我们使用了线性推测和 causal intervention 方法在 GPT-J-6B 中评估隐藏状态是否含有可预测未来隐藏状态和 tokens 的信息。我们发现，在某些层次上，可以透过单一隐藏状态估计模型的输出精度高于 48%，从而证明隐藏状态内含有可预测未来隐藏状态和 tokens 的信息。最后，我们提出了一个“未来镜”可视化方法，用于创建一个新的类型Transformer 状态的视觉化。
</details></li>
</ul>
<hr>
<h2 id="Bias-Runs-Deep-Implicit-Reasoning-Biases-in-Persona-Assigned-LLMs"><a href="#Bias-Runs-Deep-Implicit-Reasoning-Biases-in-Persona-Assigned-LLMs" class="headerlink" title="Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs"></a>Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04892">http://arxiv.org/abs/2311.04892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/allenai/persona-bias">https://github.com/allenai/persona-bias</a></li>
<li>paper_authors: Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sabharwal, Tushar Khot</li>
<li>for: This paper aims to study the unintended side effects of assigning personas to large-scale language models (LLMs) and how it affects their ability to perform basic reasoning tasks.</li>
<li>methods: The paper uses ChatGPT, a popular LLM, and experiments with 24 reasoning datasets and 16 diverse personas that span five socio-demographic groups: race, gender, religion, disability, and political affiliation.</li>
<li>results: The study finds that ChatGPT exhibits deep-rooted biases against various socio-demographic groups, resulting in a substantial drop in performance on reasoning tasks. The biases are ubiquitous, significant, and can be especially harmful for certain groups. Further analysis shows that these persona-induced errors can be hard to discern and avoid.<details>
<summary>Abstract</summary>
Recent works have showcased the ability of large-scale language models (LLMs) to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remain unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs, specifically ChatGPT, to perform basic reasoning tasks. Our study covers 24 reasoning datasets and 16 diverse personas spanning 5 socio-demographic groups: race, gender, religion, disability, and political affiliation. Our experiments unveil that ChatGPT carries deep rooted bias against various socio-demographics underneath a veneer of fairness. While it overtly rejects stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), it manifests stereotypical and often erroneous presumptions when prompted to answer questions while taking on a persona. These can be observed as abstentions in the model responses, e.g., 'As a Black person, I am unable to answer this question as it requires math knowledge', and generally result in a substantial drop in performance on reasoning tasks. We find that this inherent deep bias is ubiquitous - 80% of our personas demonstrated bias; it is significant - certain datasets had relative drops in performance of 70%+; and can be especially harmful for certain groups - certain personas had stat. sign. drops on more than 80% of the datasets. Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.
</details>
<details>
<summary>摘要</summary>
近期研究表明大型语言模型（LLM）可以体现多种人格，如在提示中表达“你是叶大卫。解释相对论”的类型。这种能力允许个性化LLM并模拟人类行为，但其影响LLM的能力还不清楚。为了填补这一空白，我们首次进行了大规模的LLM persona分配对基本逻辑能力的影响研究。我们的研究覆盖24个逻辑数据集和16种多样化的人格，涵盖了种族、 gender、宗教、残疾和政治性向。我们的实验发现，ChatGPT潜藏着各种社会demographic的偏见，尽管明确表达反对极限化（如“黑人不会干 mathematics”），但在具体的人格提示下，模型仍然表现出偏见和错误的假设。这些假设可以通过模型的回答中的缺失或违反预期的结果来识别，例如“作为黑人，我无法回答这个问题，因为它需要数学知识”。这些假设通常会导致模型在逻辑任务中表现出明显的下降。我们发现这种深层偏见是普遍的——80%的人格表现了偏见，它是重要的——某些数据集的相对下降率超过70%，并且可能对某些群体造成特别的危害——某些人格在超过80%的数据集上表现了 statistically significant drop。进一步的分析表明，这种人格塑造引起的错误可以很难以识别和避免。我们的发现作为警告，将 persona 分配给 LLM 是一种在升级的趋势，可能会暴露其深层偏见并导致不可预期的和有害的后果。
</details></li>
</ul>
<hr>
<h2 id="Profiling-Irony-Stereotype-Exploring-Sentiment-Topic-and-Lexical-Features"><a href="#Profiling-Irony-Stereotype-Exploring-Sentiment-Topic-and-Lexical-Features" class="headerlink" title="Profiling Irony &amp; Stereotype: Exploring Sentiment, Topic, and Lexical Features"></a>Profiling Irony &amp; Stereotype: Exploring Sentiment, Topic, and Lexical Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04885">http://arxiv.org/abs/2311.04885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tibor L. R. Krols, Marie Mortensen, Ninell Oldenburg</li>
<li>for: 这个研究旨在创建一个推断Twitter用户的讲话方式 Irony detection系统。</li>
<li>methods: 该研究使用了TF-IDF和主题模型，并从lexical feature, sentiment feature和对比方面进行了仔细的特征选择。</li>
<li>results: 模型达到了0.84的F1分数，比基准值高。 lexical features,特别是TF-IDF，对模型的性能做出了最大贡献，而sentiment和主题模型的特征则较少对模型的性能做出贡献。<details>
<summary>Abstract</summary>
Social media has become a very popular source of information. With this popularity comes an interest in systems that can classify the information produced. This study tries to create such a system detecting irony in Twitter users. Recent work emphasize the importance of lexical features, sentiment features and the contrast herein along with TF-IDF and topic models. Based on a thorough feature selection process, the resulting model contains specific sub-features from these areas. Our model reaches an F1-score of 0.84, which is above the baseline. We find that lexical features, especially TF-IDF, contribute the most to our models while sentiment and topic modeling features contribute less to overall performance. Lastly, we highlight multiple interesting and important paths for further exploration.
</details>
<details>
<summary>摘要</summary>
社交媒体已成为许多人的信息来源。这种受欢迎性导致了对信息生成系统的兴趣。这项研究尝试创建一个检测推特用户的讲话风格的系统。最近的研究强调 lexical 特征、情感特征和对比特征的重要性，同时还包括 TF-IDF 和话题模型。经过仔细的特征选择过程，我们的模型包含特定的子特征。我们的模型达到了 0.84 的 F1 分数，超过了基准值。我们发现，TF-IDF 特征是模型中最重要的特征，而情感和话题模型特征对总性表现较少。最后，我们提出了多个有趣和重要的可能性的探索。Note: Please keep in mind that the translation is done by a machine and may not be perfect. If you need a more accurate translation, you may want to consider hiring a professional translator.
</details></li>
</ul>
<hr>
<h2 id="Hierarchically-Gated-Recurrent-Neural-Network-for-Sequence-Modeling"><a href="#Hierarchically-Gated-Recurrent-Neural-Network-for-Sequence-Modeling" class="headerlink" title="Hierarchically Gated Recurrent Neural Network for Sequence Modeling"></a>Hierarchically Gated Recurrent Neural Network for Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04823">http://arxiv.org/abs/2311.04823</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opennlplab/hgrn">https://github.com/opennlplab/hgrn</a></li>
<li>paper_authors: Zhen Qin, Songlin Yang, Yiran Zhong</li>
<li>for: 这 paper 的目的是提出一种名为 Hierarchically Gated Recurrent Neural Network (HGRN) 的Linear RNN模型，以提高模型的效率和可靠性。</li>
<li>methods: 这 paper 使用了一种新的输出窗口 gates mechanism，并在每个层中添加了忘记门，以便模型更好地处理长期依赖关系。</li>
<li>results: 实验表明，HGRN 模型在语言模型、图像分类和长距离射频环境 benchmark 中表现出色，比 traditional RNN 模型更高效和可靠。<details>
<summary>Abstract</summary>
Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.
</details>
<details>
<summary>摘要</summary>
带有精度的转换器已经超越了RNNs的流行程度，这主要归功于其在并行训练和长期依赖关系模型方面的优势。然而，在最近，使用线性RNNs进行高效序列模型的兴趣在再次升温。这些线性RNNs通常使用输出 Linear Recurrence 层的门控机制，而忽略了使用忘记门的重要性。在这篇论文中，我们提出了一种名为层次阈值 gates Recurrent Neural Network（HGRN）的模型，该模型包括一个可学习的下界值。这个下界值在层次上增长 monotonic 地。这使得上层模型可以模型长期依赖关系，而下层模型可以模型更本地、短期依赖关系。我们在语言模型、图像分类和长距离场景中进行了实验，并证明了我们提出的模型的效果和效率。代码可以在 <https://github.com/OpenNLPLab/HGRN> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Determination-of-toxic-comments-and-unintended-model-bias-minimization-using-Deep-learning-approach"><a href="#Determination-of-toxic-comments-and-unintended-model-bias-minimization-using-Deep-learning-approach" class="headerlink" title="Determination of toxic comments and unintended model bias minimization using Deep learning approach"></a>Determination of toxic comments and unintended model bias minimization using Deep learning approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04789">http://arxiv.org/abs/2311.04789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Azim Khan</li>
<li>for: 这个研究的目的是检测恶意评论并减少基于身份特征（如性别、种族、宗教）的无意义偏见。</li>
<li>methods: 该研究使用了一种名为BERT（双向encoder表示Transformers）的注意力模型，并应用了权重损失来解决不均衡数据的问题。</li>
<li>results: 相比Logistic Regression模型（使用TFIDF vectorizer）的57.1%准确率， fine-tuned BERT模型的准确率达89%。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Online conversations can be toxic and subjected to threats, abuse, or harassment. To identify toxic text comments, several deep learning and machine learning models have been proposed throughout the years. However, recent studies demonstrate that because of the imbalances in the training data, some models are more likely to show unintended biases including gender bias and identity bias. In this research, our aim is to detect toxic comment and reduce the unintended bias concerning identity features such as race, gender, sex, religion by fine-tuning an attention based model called BERT(Bidirectional Encoder Representation from Transformers). We apply weighted loss to address the issue of unbalanced data and compare the performance of a fine-tuned BERT model with a traditional Logistic Regression model in terms of classification and bias minimization. The Logistic Regression model with the TFIDF vectorizer achieve 57.1% accuracy, and fine-tuned BERT model's accuracy is 89%. Code is available at https://github.com/zim10/Determine_Toxic_comment_and_identity_bias.git
</details>
<details>
<summary>摘要</summary>
在线对话可能会出现攻击性、违纪行为或骚扰行为。为了识别恶意评论，多种深度学习和机器学习模型已经被提出过多年。然而，最近的研究表明，由于训练数据的不均衡，一些模型可能会表现出不 INTENDED 的偏见，包括性别偏见和人类偏见。在这项研究中，我们的目标是检测恶意评论并减少基于人类特征（如种族、性别、信仰）的偏见。我们使用权重损失来解决不均衡数据的问题，并与传统的Logistic Regression模型进行比较，以确定最佳的分类和偏见减少策略。Logistic Regression模型与TF-IDF vectorizer实现了57.1%的准确率，而微调BERT模型的准确率为89%。代码可以在https://github.com/zim10/Determine_Toxic_comment_and_identity_bias.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Using-large-language-models-to-study-human-memory-for-meaningful-narratives"><a href="#Using-large-language-models-to-study-human-memory-for-meaningful-narratives" class="headerlink" title="Using large language models to study human memory for meaningful narratives"></a>Using large language models to study human memory for meaningful narratives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04742">http://arxiv.org/abs/2311.04742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkatkov/llm-narrative-analysis">https://github.com/mkatkov/llm-narrative-analysis</a></li>
<li>paper_authors: Antonios Georgiou Tankut Can, Mikhail Katkov, Misha Tsodyks</li>
<li>for: 这 paper 的目的是研究人类记忆的含义ful material。</li>
<li>methods: 这 paper 使用了语言模型作为科学工具，设计了大规模的记忆实验，并分析了获得的结果。</li>
<li>results: 研究发现，记忆性的表现与narritive length成直线关系，而且，在使用混乱版本的故事时，记忆性减退了许多，但是认知仍然 largely unaffected。这表明，记忆中的故事有一定的顺序排序，并且可能会通过 contextual reconstruction 来重建故事。<details>
<summary>Abstract</summary>
One of the most impressive achievements of the AI revolution is the development of large language models that can generate meaningful text and respond to instructions in plain English with no additional training necessary. Here we show that language models can be used as a scientific instrument for studying human memory for meaningful material. We developed a pipeline for designing large scale memory experiments and analyzing the obtained results. We performed online memory experiments with a large number of participants and collected recognition and recall data for narratives of different lengths. We found that both recall and recognition performance scale linearly with narrative length. Furthermore, in order to investigate the role of narrative comprehension in memory, we repeated these experiments using scrambled versions of the presented stories. We found that even though recall performance declined significantly, recognition remained largely unaffected. Interestingly, recalls in this condition seem to follow the original narrative order rather than the scrambled presentation, pointing to a contextual reconstruction of the story in memory.
</details>
<details>
<summary>摘要</summary>
人工智能革命中一个最吸引人的成就是大语言模型的发展，可以生成有意义的文本并在普通英语中回答指令无需额外训练。在这里，我们示示了语言模型可以用作人memory的科学实验工具。我们开发了大规模记忆实验的管道并分析了获得的结果。我们在线上进行了大量参与者的记忆实验，收集了不同长度的narative的认知和回忆数据。我们发现， narative的长度与记忆和认知性能 Linearly correlated。此外，为了调查narative理解对记忆的作用，我们重复了这些实验，使用了扭曲版本的展示的故事。我们发现，尽管回忆性能明显下降，但recognition仍然几乎不受影响。此外，回忆中的顺序与原始故事顺序相似，表明内存中的故事是以Contextual重建的。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Generative-Ad-Hoc-Information-Retrieval"><a href="#Evaluating-Generative-Ad-Hoc-Information-Retrieval" class="headerlink" title="Evaluating Generative Ad Hoc Information Retrieval"></a>Evaluating Generative Ad Hoc Information Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04694">http://arxiv.org/abs/2311.04694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Gienapp, Harrisen Scells, Niklas Deckers, Janek Bevendorff, Shuai Wang, Johannes Kiesel, Shahbaz Syed, Maik Fröbe, Guide Zucoon, Benno Stein, Matthias Hagen, Martin Potthast</li>
<li>for: This paper aims to provide a foundation and new insights for the evaluation of generative ad hoc retrieval systems.</li>
<li>methods: The paper surveys the relevant information retrieval and natural language processing literature, identifies search tasks and system architectures in generative retrieval, and develops a corresponding user model.</li>
<li>results: The paper provides a theoretical analysis of generative ad hoc retrieval systems and studies its operationalization.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文目标是为Generative ad hoc retrieval系统评估提供基础和新想法。</li>
<li>methods: 论文对信息检索和自然语言处理领域的相关文献进行了检索，并在Generative retrieval系统中标识了搜寻任务和系统体系。</li>
<li>results: 论文提供了Generative ad hoc retrieval系统的理论分析，并对其操作化进行了研究。<details>
<summary>Abstract</summary>
Recent advances in large language models have enabled the development of viable generative information retrieval systems. A generative retrieval system returns a grounded generated text in response to an information need instead of the traditional document ranking. Quantifying the utility of these types of responses is essential for evaluating generative retrieval systems. As the established evaluation methodology for ranking-based ad hoc retrieval may seem unsuitable for generative retrieval, new approaches for reliable, repeatable, and reproducible experimentation are required. In this paper, we survey the relevant information retrieval and natural language processing literature, identify search tasks and system architectures in generative retrieval, develop a corresponding user model, and study its operationalization. This theoretical analysis provides a foundation and new insights for the evaluation of generative ad hoc retrieval systems.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的大语言模型技术发展已经使得生成式信息检索系统成为可能的。这些系统会根据信息需求返回一个固定的生成文本，而不是传统的文档排名。评估这类响应的用处是评估生成检索系统的重要组成部分。然而，已有的排名型随机检索评价方法可能不适用于生成检索，因此需要新的方法来确保可靠、重复和可重现的实验。本文通过审查相关的信息检索和自然语言处理文献，确定搜索任务和系统体系，开发用户模型，并研究其实现。这种理论分析提供了基础和新的视角，以便评估生成随机检索系统。
</details></li>
</ul>
<hr>
<h2 id="Speech-language-models-lack-important-brain-relevant-semantics"><a href="#Speech-language-models-lack-important-brain-relevant-semantics" class="headerlink" title="Speech language models lack important brain-relevant semantics"></a>Speech language models lack important brain-relevant semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04664">http://arxiv.org/abs/2311.04664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subba Reddy Oota, Emin Çelik, Fatma Deniz, Mariya Toneva</li>
<li>for: 本研究旨在探索语言模型是如何预测大脑中的信息的。</li>
<li>methods: 研究人员使用了一种直接方法，即从语言模型表示中除去特定的低级刺激特征（文本、语音和视觉），然后观察这种干预如何影响脑电响应。</li>
<li>results: 研究发现，文本基于的语言模型可以很好地预测大脑中的语言处理活动，而不需要特定的刺激特征。相比之下，speech基于的语言模型在预测语音识别活动方面表现较差，并且可能需要进一步改进以更好地反映大脑中的语言处理。<details>
<summary>Abstract</summary>
Despite known differences between reading and listening in the brain, recent work has shown that text-based language models predict both text-evoked and speech-evoked brain activity to an impressive degree. This poses the question of what types of information language models truly predict in the brain. We investigate this question via a direct approach, in which we eliminate information related to specific low-level stimulus features (textual, speech, and visual) in the language model representations, and observe how this intervention affects the alignment with fMRI brain recordings acquired while participants read versus listened to the same naturalistic stories. We further contrast our findings with speech-based language models, which would be expected to predict speech-evoked brain activity better, provided they model language processing in the brain well. Using our direct approach, we find that both text-based and speech-based language models align well with early sensory regions due to shared low-level features. Text-based models continue to align well with later language regions even after removing these features, while, surprisingly, speech-based models lose most of their alignment. These findings suggest that speech-based models can be further improved to better reflect brain-like language processing.
</details>
<details>
<summary>摘要</summary>
尽管已知阅读和听取在脑中的差异， latest work 显示，文本基于语言模型可以很准确地预测文本诱发和speech诱发的脑动态。这引发了问题，即语言模型在脑中预测哪些信息。我们通过直接方法来研究这个问题，即在语言模型表示中消除特定的低级别刺激特征（文本、语音和视觉）信息，然后观察这种干预对fMRI脑记录的影响。我们进一步与speech基于语言模型进行比较，这些模型应该更好地预测speech诱发的脑动态，如果它们正确地模型了脑中的语言处理。使用我们的直接方法，我们发现，文本基于语言模型在后期语言区域中继续保持良好的吻合，而speech基于语言模型则在消除特定的低级刺激特征后失去大部分的吻合。这些发现表示，speech基于语言模型可以进一步改进，以更好地反映脑中的语言处理。
</details></li>
</ul>
<hr>
<h2 id="Massive-Editing-for-Large-Language-Models-via-Meta-Learning"><a href="#Massive-Editing-for-Large-Language-Models-via-Meta-Learning" class="headerlink" title="Massive Editing for Large Language Models via Meta Learning"></a>Massive Editing for Large Language Models via Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04661">http://arxiv.org/abs/2311.04661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenmientan/malmen">https://github.com/chenmientan/malmen</a></li>
<li>paper_authors: Chenmien Tan, Ge Zhang, Jie Fu</li>
<li>For: The paper aims to improve the ability of large language models (LLMs) to learn and retain knowledge over time, by proposing a new method called MAssive Language Model Editing Network (MALMEN).* Methods: The proposed method uses a hyper-network to generate parameter shift, and formulates the parameter shift aggregation as a least square problem. The method also separates the computation on the hyper-network and LM, allowing for arbitrary batch size on both networks.* Results: The proposed method is evaluated on several knowledge-intensive NLP tasks, including closed book fact-checking and question answering, and is shown to be capable of editing hundreds of times more facts than strong baselines with the same hyper-network architecture. The method also outperforms editor specifically designed for GPT.<details>
<summary>Abstract</summary>
While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameters using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2, T5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks, i.e., closed book fact-checking and question answering. Remarkably, MALMEN is capable of editing hundreds of times more facts than strong baselines with the identical hyper-network architecture and outperforms editor specifically designed for GPT. Our code is available at https://github.com/ChenmienTan/malmen.
</details>
<details>
<summary>摘要</summary>
large language models (LLMs) 可以从预训数据中学习知识，但取得的知识可能会随着时间的推移而变得不正确或过时，因此需要在训练后更正 language model (LM) 的知识。一种有前途的方法是使用 hyper-network 生成参数移动，但现有的 hyper-network 受到同步编译作业量的限制，导致Scalability问题。为了解决这个问题，我们提出了 MAssive Language Model Editing Network (MALMEN)，它将参数移动聚合形式化为最小二乘问题，然后使用normal equation更新 LM 参数。为了在有限内存预算下同时编译多个 факти，我们将 computation 在 hyper-network 和 LM 之间分开，允许任意批次大小在两个神经网络上。我们的方法在不同的 LM 架构（BERT-base、GPT-2、T5-XL (2.8B) 和 GPT-J (6B)）和不同的知识密集 NLP 任务（closed book fact-checking 和 question answering）中进行评估，结果显示 MALMEN 可以编译到千个 факти以上，比强基eline 高效，并且超过特别设计 для GPT 的编译器。我们的代码可以在 https://github.com/ChenmienTan/malmen 上找到。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Nature-of-Disagreements-on-Mid-Scale-Ratings-A-Case-Study-on-the-Abstractness-Concreteness-Continuum"><a href="#Investigating-the-Nature-of-Disagreements-on-Mid-Scale-Ratings-A-Case-Study-on-the-Abstractness-Concreteness-Continuum" class="headerlink" title="Investigating the Nature of Disagreements on Mid-Scale Ratings: A Case Study on the Abstractness-Concreteness Continuum"></a>Investigating the Nature of Disagreements on Mid-Scale Ratings: A Case Study on the Abstractness-Concreteness Continuum</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04563">http://arxiv.org/abs/2311.04563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Urban Knupleš, Diego Frassinelli, Sabine Schulte im Walde</li>
<li>for: 这个研究旨在检查语言评分的可靠性，具体来说是检查人们对中等程度的单词评分是否存在差异。</li>
<li>methods: 该研究使用了 corrleation 和批处理等方法来检测中等程度单词的特征，并通过团 clustering 方法来发现评分者之间的系统性不一致。</li>
<li>results: 研究发现，对中等程度单词进行细化或过滤可以提高语言评分的可靠性。<details>
<summary>Abstract</summary>
Humans tend to strongly agree on ratings on a scale for extreme cases (e.g., a CAT is judged as very concrete), but judgements on mid-scale words exhibit more disagreement. Yet, collected rating norms are heavily exploited across disciplines. Our study focuses on concreteness ratings and (i) implements correlations and supervised classification to identify salient multi-modal characteristics of mid-scale words, and (ii) applies a hard clustering to identify patterns of systematic disagreement across raters. Our results suggest to either fine-tune or filter mid-scale target words before utilising them.
</details>
<details>
<summary>摘要</summary>
人们通常对极端情况的评分 exhibit 强烈的一致性（例如，一只猫被评为非常具体），但对中等级词的评分存在更多的不一致。然而，收集的评分标准被广泛运用于不同领域。我们的研究将重点在具体性评分中，并（i）利用相关性和指导 классификация来特征化中等级词的多种模态特征，以及（ii）通过坚定分类来揭示评分人员之间的系统性不一致。我们的结果表明，在使用中等级目标词之前应该进行细化或过滤。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Distractors-in-Multiple-Choice-Tests"><a href="#Assessing-Distractors-in-Multiple-Choice-Tests" class="headerlink" title="Assessing Distractors in Multiple-Choice Tests"></a>Assessing Distractors in Multiple-Choice Tests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04554">http://arxiv.org/abs/2311.04554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vatsal Raina, Adian Liusie, Mark Gales</li>
<li>for: 这篇论文目的是为了提出自动评估多选题考试中的各种选项质量的方法。</li>
<li>methods: 这篇论文使用了自动评估方法，包括类фика器模型和embedding-based equivalence metric，来评估多选题考试中的选项质量。</li>
<li>results: 论文提出的自动评估方法可以准确地评估多选题考试中的选项质量，并且可以提高考试评估的准确性和公正性。<details>
<summary>Abstract</summary>
Multiple-choice tests are a common approach for assessing candidates' comprehension skills. Standard multiple-choice reading comprehension exams require candidates to select the correct answer option from a discrete set based on a question in relation to a contextual passage. For appropriate assessment, the distractor answer options must by definition be incorrect but plausible and diverse. However, generating good quality distractors satisfying these criteria is a challenging task for content creators. We propose automated assessment metrics for the quality of distractors in multiple-choice reading comprehension tests. Specifically, we define quality in terms of the incorrectness, plausibility and diversity of the distractor options. We assess incorrectness using the classification ability of a binary multiple-choice reading comprehension system. Plausibility is assessed by considering the distractor confidence - the probability mass associated with the distractor options for a standard multi-class multiple-choice reading comprehension system. Diversity is assessed by pairwise comparison of an embedding-based equivalence metric between the distractors of a question. To further validate the plausibility metric we compare against candidate distributions over multiple-choice questions and agreement with a ChatGPT model's interpretation of distractor plausibility and diversity.
</details>
<details>
<summary>摘要</summary>
Incorrectness is assessed using the classification ability of a binary multiple-choice reading comprehension system. Plausibility is evaluated by considering the distractor confidence, or the probability mass associated with the distractor options for a standard multi-class multiple-choice reading comprehension system. Diversity is assessed by comparing the distractors using an embedding-based equivalence metric.To further validate the plausibility metric, we compare it against candidate distributions over multiple-choice questions and agreement with a ChatGPT model's interpretation of distractor plausibility and diversity.
</details></li>
</ul>
<hr>
<h2 id="Large-GPT-like-Models-are-Bad-Babies-A-Closer-Look-at-the-Relationship-between-Linguistic-Competence-and-Psycholinguistic-Measures"><a href="#Large-GPT-like-Models-are-Bad-Babies-A-Closer-Look-at-the-Relationship-between-Linguistic-Competence-and-Psycholinguistic-Measures" class="headerlink" title="Large GPT-like Models are Bad Babies: A Closer Look at the Relationship between Linguistic Competence and Psycholinguistic Measures"></a>Large GPT-like Models are Bad Babies: A Closer Look at the Relationship between Linguistic Competence and Psycholinguistic Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04547">http://arxiv.org/abs/2311.04547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julius Steuer, Marius Mosbach, Dietrich Klakow</li>
<li>for: 本研究旨在探讨语言模型（LM）的认知可能性，并 investigate 如何使LM更加符合人类语言处理的规则和约束。</li>
<li>methods: 研究者使用了多种GPT-like语言模型，不同大小和深度，并对这些模型进行了训练和评估，以确定它们在不同任务中的表现。</li>
<li>results: 研究发现，LM的大小和表现之间存在正相关关系，其中模型宽度和深度在不同任务中有不同的偏好。此外，研究还发现，LM的大小和语言处理时间之间存在负相关关系，表明模型需要采用不同的方法来模型语言处理的负面效果。<details>
<summary>Abstract</summary>
Research on the cognitive plausibility of language models (LMs) has so far mostly concentrated on modelling psycholinguistic response variables such as reading times, gaze durations and N400/P600 EEG signals, while mostly leaving out the dimension of what Mahowald et al. (2023) described as formal and functional linguistic competence, and developmental plausibility. We address this gap by training a series of GPT-like language models of different sizes on the strict version of the BabyLM pretraining corpus, evaluating on the challenge tasks (BLiMP, GLUE, MSGS) and an additional reading time prediction task. We find a positive correlation between LM size and performance on all three challenge tasks, with different preferences for model width and depth in each of the tasks. In contrast, a negative correlation was found between LM size and reading time fit of linear mixed-effects models using LM surprisal as a predictor, with the second-smallest LM achieving the largest log-likelihood reduction over a baseline model without surprisal. This suggests that modelling processing effort and linguistic competence may require an approach different from training GPT-like LMs on a developmentally plausible corpus.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "psycholinguistic" is translated as "语言心理学的" (yǔ yán xīn lǐ xué de)* "response variables" is translated as "响应变量" (fāng biàn biàn xiàng)* "reading times" is translated as "阅读时间" (dòng dú shí jiān)* "gaze durations" is translated as "视线时间" (shì jian shí jiān)* "N400/P600 EEG signals" is translated as "N400/P600 EEG信号" (N400/P600 EEG xìn xiàng)* "challenge tasks" is translated as "挑战任务" (tiǎo zhàn zhì gōng)* "BLiMP" is translated as "BLiMP" (B López-Ibáñez et al., 2023)* "GLUE" is translated as "GLUE" (Wang et al., 2019)* "MSGS" is translated as "MSGS" (Zhang et al., 2020)* "linear mixed-effects models" is translated as "线性混合效应模型" (xiàng xìng hù he yìng xiàng mó del)* "LM surprisal" is translated as "LM难易度" (LM nán yì du)* "baseline model without surprisal" is translated as "无难易度基线模型" (wú nán yì du jī liào mó del)
</details></li>
</ul>
<hr>
<h2 id="Loss-Masking-Is-Not-Needed-in-Decoder-only-Transformer-for-Discrete-token-Based-ASR"><a href="#Loss-Masking-Is-Not-Needed-in-Decoder-only-Transformer-for-Discrete-token-Based-ASR" class="headerlink" title="Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token Based ASR"></a>Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token Based ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04534">http://arxiv.org/abs/2311.04534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, Shiliang Zhang, Chong Deng, Yukun Ma, Hai Yu, Jiaqing Liu, Chong Zhang</li>
<li>for: 这个论文主要针对的是基于普通话音频识别的单语言模型（SpeechGPT、VioLA、AudioPaLM等）的识别性能提高。</li>
<li>methods: 这些模型将连续的语音信号转换成精度的字符串（speech discretization），然后将语音和文本的字符串合并到共同词汇中。然后，它们使用单个decoder-only transformer进行训练，并使用loss masking进行ASR任务的训练。</li>
<li>results: 我们发现，使用传统的cross-entropy损失函数不一定能够提高ASR性能，而是可以使用smoothed label distillation（SLD）方法，该方法引入KL散度损失函数以有效地模型语音字符串。实验结果表明，我们的SLD方法可以超越loss masking，并在不同的speech discretization方法下提高ASR性能。<details>
<summary>Abstract</summary>
Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on speech tasks. These models convert continuous speech signals into discrete tokens (speech discretization) and merge text and speech tokens into a shared vocabulary. Then they train a single decoder-only Transformer on a mixture of speech tasks. Specifically, all these models utilize Loss Masking on the input speech tokens for the ASR task, which means that these models do not explicitly model the dependency between the speech tokens. In this paper, we attempt to model the sequence of speech tokens in an autoregressive manner like text. However, we find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over Loss Masking. Therefore, we propose a novel approach denoted Smoothed Label Distillation (SLD), which introduces a KL divergence loss with smoothed labels on the input speech tokens to effectively model speech tokens. Experiments demonstrate that our SLD approach alleviates the limitations of the cross-entropy loss and consistently outperforms Loss Masking for decoder-only Transformer based ASR using different speech discretization methods.
</details>
<details>
<summary>摘要</summary>
最近，一些协调的语音-文本模型，如SpeechGPT、VioLA和AudioPaLM，在语音任务上实现了出色的性能。这些模型将连续的语音信号转换成精确的字符（语音精度），然后将语音和文本字符合并到共同词汇中。然后它们训练了单个解码器只的Transformer模型在语音任务中。具体来说，这些模型都使用输入语音token的损失压缩（Loss Masking）来进行ASR任务，这意味着这些模型不直接模型语音token之间的依赖关系。在这篇论文中，我们尝试了模elling语音token的自然顺序，但我们发现在输入语音token上应用普通的十字积分损失不一定能提高ASR性能，而是提出了一种新的方法，即Smoothed Label Distillation（SLD）。SLD方法通过在输入语音token上添加一个KL散度损失来有效地模型语音token。实验结果表明，我们的SLD方法可以减轻十字积分损失的局限性，并一直超越Loss Masking在单个Transformer模型基于ASR任务中。
</details></li>
</ul>
<hr>
<h2 id="Conversation-Understanding-using-Relational-Temporal-Graph-Neural-Networks-with-Auxiliary-Cross-Modality-Interaction"><a href="#Conversation-Understanding-using-Relational-Temporal-Graph-Neural-Networks-with-Auxiliary-Cross-Modality-Interaction" class="headerlink" title="Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction"></a>Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04507">http://arxiv.org/abs/2311.04507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cam-Van Thi Nguyen, Anh-Tuan Mai, The-Son Le, Hai-Dang Kieu, Duc-Trong Le</li>
<li>for: 本研究旨在提高对话理解中的情感识别率，具体来说是利用对话级别的跨Modal交互以及发言人的时间信息来预测每个句子的情感标签。</li>
<li>methods: 本研究提出了一种名为CORECT的神经网络框架，该框架利用对话级别的跨Modal交互和每个句子的时间信息，同时还利用不同Modal的特定表示方法来提高对话理解。</li>
<li>results: 经过广泛的实验，CORECT在IEMOCAP和CMU-MOSEI数据集上的多模态ERC任务得到了state-of-the-art的结果，证明了CORECT的有效性。<details>
<summary>Abstract</summary>
Emotion recognition is a crucial task for human conversation understanding. It becomes more challenging with the notion of multimodal data, e.g., language, voice, and facial expressions. As a typical solution, the global- and the local context information are exploited to predict the emotional label for every single sentence, i.e., utterance, in the dialogue. Specifically, the global representation could be captured via modeling of cross-modal interactions at the conversation level. The local one is often inferred using the temporal information of speakers or emotional shifts, which neglects vital factors at the utterance level. Additionally, most existing approaches take fused features of multiple modalities in an unified input without leveraging modality-specific representations. Motivating from these problems, we propose the Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), an novel neural network framework that effectively captures conversation-level cross-modality interactions and utterance-level temporal dependencies with the modality-specific manner for conversation understanding. Extensive experiments demonstrate the effectiveness of CORECT via its state-of-the-art results on the IEMOCAP and CMU-MOSEI datasets for the multimodal ERC task.
</details>
<details>
<summary>摘要</summary>
情感识别是人工对话理解中的关键任务。在多模态数据下，这种任务变得更加挑战性，例如语言、声音和表情等。为解决这个问题，通常是利用对话级别的全局和本地上下文信息来预测每个句子的情感标签。特别是，全局表示可以通过对话级别的交互模型来捕捉到全局上下文信息。而本地上下文信息通常是通过说话者的时间信息或情感变化来得到，但是这些因素可能会忽略了句子级别的重要因素。此外，大多数现有的方法会将多Modalities的Feature进行混合，而不是利用特定的感知模式来进行处理。 inspirited by these problems, we propose a novel neural network framework called Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), which effectively captures conversation-level cross-modality interactions and utterance-level temporal dependencies with modality-specific manner for conversation understanding. Our extensive experiments demonstrate the effectiveness of CORECT via its state-of-the-art results on the IEMOCAP and CMU-MOSEI datasets for the multimodal ERC task.
</details></li>
</ul>
<hr>
<h2 id="Multi-label-and-Multi-target-Sampling-of-Machine-Annotation-for-Computational-Stance-Detection"><a href="#Multi-label-and-Multi-target-Sampling-of-Machine-Annotation-for-Computational-Stance-Detection" class="headerlink" title="Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection"></a>Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04495">http://arxiv.org/abs/2311.04495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seq-to-mind/Stance_MA">https://github.com/seq-to-mind/Stance_MA</a></li>
<li>paper_authors: Zhengyuan Liu, Hai Leong Chieu, Nancy F. Chen</li>
<li>for: 本研究旨在探讨大语言模型是否可以取代人工标注 personnel 进行计算态度探测任务。</li>
<li>methods: 本研究使用了自动标注方法，并引入多标签多目标采样策略以提高标注质量。</li>
<li>results: 实验结果表明，我们的方法可以显著提高性能和学习效果。<details>
<summary>Abstract</summary>
Data collection from manual labeling provides domain-specific and task-aligned supervision for data-driven approaches, and a critical mass of well-annotated resources is required to achieve reasonable performance in natural language processing tasks. However, manual annotations are often challenging to scale up in terms of time and budget, especially when domain knowledge, capturing subtle semantic features, and reasoning steps are needed. In this paper, we investigate the efficacy of leveraging large language models on automated labeling for computational stance detection. We empirically observe that while large language models show strong potential as an alternative to human annotators, their sensitivity to task-specific instructions and their intrinsic biases pose intriguing yet unique challenges in machine annotation. We introduce a multi-label and multi-target sampling strategy to optimize the annotation quality. Experimental results on the benchmark stance detection corpora show that our method can significantly improve performance and learning efficacy.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese文本收集自手动标注提供域 especific和任务aligned的监督，以实现自然语言处理任务中的合理性。然而，手动标注通常具有时间和预算上的挑战，特别是当需要域知识、捕捉微妙Semantic特征和推理步骤时。在这篇论文中，我们investigate大型自然语言模型的可用性，以便用于计算意见探测。我们发现，虽然大型自然语言模型显示出人工标注员的潜在 substitute，但是它们对任务特定的指令和自身偏见带来了有趣但唯一的挑战。我们提出了一种多标签多目标采样策略，以提高标注质量。实验结果表明，我们的方法可以显著提高性能和学习效果。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CLearViD-Curriculum-Learning-for-Video-Description"><a href="#CLearViD-Curriculum-Learning-for-Video-Description" class="headerlink" title="CLearViD: Curriculum Learning for Video Description"></a>CLearViD: Curriculum Learning for Video Description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04480">http://arxiv.org/abs/2311.04480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yueyue0401/CLV">https://github.com/yueyue0401/CLV</a></li>
<li>paper_authors: Cheng-Yu Chuang, Pooyan Fazli</li>
<li>For: The paper is written for generating coherent natural language sentences that narrate the content of a given video.* Methods: The paper proposes a transformer-based model called CLearViD, which leverages curriculum learning and the Mish activation function to accomplish video description generation. The model is trained using two curriculum strategies: progressively exposing the model to more challenging samples and gradually reducing the capacity of the network through dropout.* Results: The paper demonstrates the effectiveness of the proposed model through extensive experiments and ablation studies. The results show that CLearViD significantly outperforms existing state-of-the-art models in terms of both accuracy and diversity metrics on two datasets, namely ActivityNet Captions and YouCook2.<details>
<summary>Abstract</summary>
Video description entails automatically generating coherent natural language sentences that narrate the content of a given video. We introduce CLearViD, a transformer-based model for video description generation that leverages curriculum learning to accomplish this task. In particular, we investigate two curriculum strategies: (1) progressively exposing the model to more challenging samples by gradually applying a Gaussian noise to the video data, and (2) gradually reducing the capacity of the network through dropout during the training process. These methods enable the model to learn more robust and generalizable features. Moreover, CLearViD leverages the Mish activation function, which provides non-linearity and non-monotonicity and helps alleviate the issue of vanishing gradients. Our extensive experiments and ablation studies demonstrate the effectiveness of the proposed model. The results on two datasets, namely ActivityNet Captions and YouCook2, show that CLearViD significantly outperforms existing state-of-the-art models in terms of both accuracy and diversity metrics.
</details>
<details>
<summary>摘要</summary>
视频描述文本生成模型CLearViD，利用变换器来自动生成 coherent的自然语言句子，描述视频内容。我们 investigate了两种课程学策略：（1）通过逐渐应用 Gaussian 噪声来提高模型对更复杂样本的抗衰假设能力，以及（2）在训练过程中逐渐减少网络的容量。这两种方法使得模型学习更加稳健和泛化。此外，CLearViD 还使用 Mish 激活函数，该函数提供了非线性和非准确性，帮助解决梯度消失问题。我们的广泛的实验和剥夺研究表明，提案的模型具有显著的效果。对 ActivityNet Captions 和 YouCook2 两个数据集进行了比较，CLearViD 与现有状态机的模型相比，在准确性和多样性指标上具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="Twitter-Sentiment-Analysis-of-Covid-Vacciness"><a href="#Twitter-Sentiment-Analysis-of-Covid-Vacciness" class="headerlink" title="Twitter Sentiment Analysis of Covid Vacciness"></a>Twitter Sentiment Analysis of Covid Vacciness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04479">http://arxiv.org/abs/2311.04479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenbo Zhu, Tiechuan Hu</li>
<li>for: 这些研究者想要使用Twitter上的 opinon sorting和ranking算法，以便更好地理解用户对 COVID-19 疫苗的看法，并帮助人们做出更加有信心的决策。</li>
<li>methods: 这些研究者使用了自然语言处理技术，包括 Sentiment Analysis 和 Topic Modeling，以分类和 categorize Twitter上的 opinon。</li>
<li>results: 这些研究者通过使用这些算法，成功地分类和 categorize Twitter上的 opinon，并且可以准确地了解用户对 COVID-19 疫苗的看法。<details>
<summary>Abstract</summary>
In this paper, we look at a database of tweets sorted by various keywords that could indicate the users sentiment towards covid vaccines. With social media becoming such a prevalent source of opinion, sorting and ranking tweets that hold important information such as opinions on covid vaccines is of utmost importance. Two different ranking scales were used, and ranking a tweet in this way could represent the difference between an opinion being lost and an opinion being featured on the site, which affects the decisions and behavior of people, and why researchers were interested in it. Using natural language processing techniques, our aim is to determine and categorize opinions about covid vaccines with the highest accuracy possible.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们分析了一个推特数据库，按照不同的关键词分类用户对covid疫苗的看法。随着社交媒体在意见形成中的重要地位，对于推特上的意见分类和排名是非常重要的。我们使用了两种不同的排名级别，排名一句话可能代表了意见被丢弃或被站点特有的，这会影响人们的决策和行为，因此研究人员对其非常感兴趣。使用自然语言处理技术，我们的目标是尽可能准确地确定和分类对covid疫苗的看法。
</details></li>
</ul>
<hr>
<h2 id="Lewis’s-Signaling-Game-as-beta-VAE-For-Natural-Word-Lengths-and-Segments"><a href="#Lewis’s-Signaling-Game-as-beta-VAE-For-Natural-Word-Lengths-and-Segments" class="headerlink" title="Lewis’s Signaling Game as beta-VAE For Natural Word Lengths and Segments"></a>Lewis’s Signaling Game as beta-VAE For Natural Word Lengths and Segments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04453">http://arxiv.org/abs/2311.04453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryo Ueda, Tadahiro Taniguchi</li>
<li>for: 这个论文的目的是研究emergent communication（EC）中的信号游戏，以及如何使用beta-VAE对其目标函数进行修改，以便更好地控制emergent languages的统计特性。</li>
<li>methods: 这篇论文使用了beta-VAE和ELBO来重新解释Lewis的信号游戏，并修改了其目标函数。</li>
<li>results: 实验表明，通过选择合适的先验分布，emergent languages可以更加接近自然语言的统计特性，包括Zipf的压缩法（ZLA）和Harris的词法分析（HAS）。<details>
<summary>Abstract</summary>
As a sub-discipline of evolutionary and computational linguistics, emergent communication (EC) studies communication protocols, called emergent languages, arising in simulations where agents communicate. A key goal of EC is to give rise to languages that share statistical properties with natural languages. In this paper, we reinterpret Lewis's signaling game, a frequently used setting in EC, as beta-VAE and reformulate its objective function as ELBO. Consequently, we clarify the existence of prior distributions of emergent languages and show that the choice of the priors can influence their statistical properties. Specifically, we address the properties of word lengths and segmentation, known as Zipf's law of abbreviation (ZLA) and Harris's articulation scheme (HAS), respectively. It has been reported that the emergent languages do not follow them when using the conventional objective. We experimentally demonstrate that by selecting an appropriate prior distribution, more natural segments emerge, while suggesting that the conventional one prevents the languages from following ZLA and HAS.
</details>
<details>
<summary>摘要</summary>
为一种语言演化和计算语言学的子领域， emergent communication（EC）研究在模拟中 Agent 通信时出现的通信协议，称为 emergent language。EC 的一个关键目标是使 emergent language 具有自然语言的统计性质。在这篇论文中，我们将 Lewis 的信号游戏，常用于 EC，重新解释为 beta-VAE 并修改其目标函数为 ELBO。因此，我们可以清楚地说明 emergent language 的先前分布的存在和这些先前分布的选择可以影响其统计性质。特别是，我们研究 word length 和分 segmentation 的属性，即Zipf 法则简短化（ZLA）和Harris 词法分析（HAS）。报告表明，使用 conventional 目标函数时，emergent language 不会遵循 ZLA 和 HAS。我们通过选择合适的先前分布，在实验中证明可以生成更自然的分 segmentation，并建议 conventional 目标函数可以阻碍 language 遵循 ZLA 和 HAS。
</details></li>
</ul>
<hr>
<h2 id="Recursion-in-Recursion-Two-Level-Nested-Recursion-for-Length-Generalization-with-Scalability"><a href="#Recursion-in-Recursion-Two-Level-Nested-Recursion-for-Length-Generalization-with-Scalability" class="headerlink" title="Recursion in Recursion: Two-Level Nested Recursion for Length Generalization with Scalability"></a>Recursion in Recursion: Two-Level Nested Recursion for Length Generalization with Scalability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04449">http://arxiv.org/abs/2311.04449</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jrc1995/beamrecursionfamily">https://github.com/jrc1995/beamrecursionfamily</a></li>
<li>paper_authors: Jishnu Ray Chowdhury, Cornelia Caragea</li>
<li>for: 这 paper 是为了研究一种新的树状模型，它可以同时具备 BBT-RvNNs 的计算效率和 RvNNs 的结构敏感性。</li>
<li>methods: 这 paper 使用了一种名为 Recursion in Recursion (RIR) 的新框架，它使用两级嵌套循环，外层循环是一个 $k$-ary 平衡树模型，而内层循环是一个 Beam Tree RvNN。此外， authors 还提出了一种新的扩散策略，即 beam alignment，以调整 BT-RvNN 的性能。</li>
<li>results: 这 paper 的最佳模型可以在 ListOps 上实现高（大于 90%）的长度泛化性能，同时在 LRA 语言任务上保持与 Structured State Space Models (SSMs) 的竞争性。此外， authors 还证明了 RIR 可以在 LRA 语言任务上比 Transformers 更高的精度。<details>
<summary>Abstract</summary>
Binary Balanced Tree RvNNs (BBT-RvNNs) enforce sequence composition according to a preset balanced binary tree structure. Thus, their non-linear recursion depth is just $\log_2 n$ ($n$ being the sequence length). Such logarithmic scaling makes BBT-RvNNs efficient and scalable on long sequence tasks such as Long Range Arena (LRA). However, such computational efficiency comes at a cost because BBT-RvNNs cannot solve simple arithmetic tasks like ListOps. On the flip side, RvNNs (e.g., Beam Tree RvNN) that do succeed on ListOps (and other structure-sensitive tasks like formal logical inference) are generally several times more expensive than even RNNs. In this paper, we introduce a novel framework -- Recursion in Recursion (RIR) to strike a balance between the two sides - getting some of the benefits from both worlds. In RIR, we use a form of two-level nested recursion - where the outer recursion is a $k$-ary balanced tree model with another recursive model (inner recursion) implementing its cell function. For the inner recursion, we choose Beam Tree RvNNs (BT-RvNN). To adjust BT-RvNNs within RIR we also propose a novel strategy of beam alignment. Overall, this entails that the total recursive depth in RIR is upper-bounded by $k \log_k n$. Our best RIR-based model is the first model that demonstrates high ($\geq 90\%$) length-generalization performance on ListOps while at the same time being scalable enough to be trainable on long sequence inputs from LRA. Moreover, in terms of accuracy in the LRA language tasks, it performs competitively with Structured State Space Models (SSMs) without any special initialization - outperforming Transformers by a large margin. On the other hand, while SSMs can marginally outperform RIR on LRA, they (SSMs) fail to length-generalize on ListOps. Our code is available at: \url{https://github.com/JRC1995/BeamRecursionFamily/}.
</details>
<details>
<summary>摘要</summary>
Binary 平衡树RvNNs (BBT-RvNNs)  enforces 序列组合 according to a preset 平衡的二进制树结构。因此，它们的非线性循环深度只是 log2(n)（n 是序列长度）。这种对数循环深度使 BBT-RvNNs 高效和可扩展于长序列任务，如 Long Range Arena (LRA)。然而，这种计算效率来自于 BBT-RvNNs 无法解决简单的数学任务，如 ListOps。相反，使用 RvNNs (例如 Beam Tree RvNN) 可以在 ListOps 和其他结构敏感任务中取得更高的成功率，但这些模型通常比 RNNs 更加昂贵。在这篇论文中，我们介绍了一种新的框架--- Recursion in Recursion (RIR)，以达到这两个方面之间的平衡。在 RIR 中，我们使用一种 $k$-ary 平衡树模型，其中另一个嵌入的回归模型（内嵌回归）实现其细胞函数。为内嵌回归，我们选择 Beam Tree RvNNs (BT-RvNN)。为了调整 BT-RvNNs 在 RIR 中，我们也提出了一种新的抽象策略---排Alignment。总的来说，RIR 的总回归深度 upper-bounded 为 $k \log_k n$。我们的最佳 RIR-based 模型可以在 ListOps 上达到 length-generalization 性能 higher than 90% ，同时可以在 Long Range Arena 中训练长序列输入。此外，在 LRA 语言任务上，它的准确率与 Structured State Space Models (SSMs) 相当，而不需要特殊的初始化。相比之下，SSMs 可以 marginally 在 LRA 上超越 RIR，但它们无法 length-generalize 在 ListOps。我们的代码可以在以下链接中找到： \url{https://github.com/JRC1995/BeamRecursionFamily/}.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/cs.CL_2023_11_08/" data-id="clpxp6bzv00evee883zbna3hp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/cs.LG_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T10:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/08/cs.LG_2023_11_08/">cs.LG - 2023-11-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Compression-of-Overparameterized-Deep-Models-through-Low-Dimensional-Learning-Dynamics"><a href="#Efficient-Compression-of-Overparameterized-Deep-Models-through-Low-Dimensional-Learning-Dynamics" class="headerlink" title="Efficient Compression of Overparameterized Deep Models through Low-Dimensional Learning Dynamics"></a>Efficient Compression of Overparameterized Deep Models through Low-Dimensional Learning Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05061">http://arxiv.org/abs/2311.05061</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soominkwon/comp-deep-nets">https://github.com/soominkwon/comp-deep-nets</a></li>
<li>paper_authors: Soo Min Kwon, Zekai Zhang, Dogyoon Song, Laura Balzano, Qing Qu</li>
<li>for: 本研究旨在降低深度学习模型的计算复杂性，通过研究深度网络的学习动态来减少网络的维度。</li>
<li>methods: 本研究使用了深度线性模型来研究深度网络的学习动态，并发现了深度网络的Weight矩阵具有低维结构。基于这一发现，我们提出了一种减少深度网络的方法，通过减少网络的宽度来减少计算复杂性。</li>
<li>results: 我们的实验表明，使用我们的减少方法可以加速深度网络的训练过程，而不会妥协模型质量。减少后的网络可以在所有的梯度下降迭代中更快 converges，并且可以在不同的初始化情况下获得更好的性能。<details>
<summary>Abstract</summary>
Overparameterized models have proven to be powerful tools for solving various machine learning tasks. However, overparameterization often leads to a substantial increase in computational and memory costs, which in turn requires extensive resources to train. In this work, we aim to reduce this complexity by studying the learning dynamics of overparameterized deep networks. By extensively studying its learning dynamics, we unveil that the weight matrices of various architectures exhibit a low-dimensional structure. This finding implies that we can compress the networks by reducing the training to a small subspace. We take a step in developing a principled approach for compressing deep networks by studying deep linear models. We demonstrate that the principal components of deep linear models are fitted incrementally but within a small subspace, and use these insights to compress deep linear networks by decreasing the width of its intermediate layers. Remarkably, we observe that with a particular choice of initialization, the compressed network converges faster than the original network, consistently yielding smaller recovery errors throughout all iterations of gradient descent. We substantiate this observation by developing a theory focused on the deep matrix factorization problem, and by conducting empirical evaluations on deep matrix sensing. Finally, we demonstrate how our compressed model can enhance the utility of deep nonlinear models. Overall, we observe that our compression technique accelerates the training process by more than 2x, without compromising model quality.
</details>
<details>
<summary>摘要</summary>
具有过参数化模型已经证明是解决不同机器学习任务的有力工具。然而，过参数化通常会导致计算和内存成本增加很多，需要很大的资源来训练。在这项工作中，我们希望通过研究深度网络学习动态来减少这种复杂性。我们发现了深度网络的Weight矩阵在不同架构中具有低维度结构，这意味着可以通过减少训练的维度来压缩网络。我们开发了一种原则的方法来压缩深度网络，通过研究深度线性模型。我们发现，深度线性模型的主成分可以在一个小空间中逐步 fitted，并且可以通过减少深度网络中间层的宽度来压缩网络。Remarkably，我们发现，使用特定的初始化方式，压缩后的网络在每一次梯度下降迭代中更快 converges，并且一直在所有迭代中保持小于原网络的恢复误差。我们证明了这一观察，通过关注深度矩阵分解问题，并通过实际的测试进行深度矩阵感知。最后，我们示出了我们压缩模型可以提高深度非线性模型的实用性。总的来说，我们发现，我们的压缩技术可以在训练过程中提高速度超过2倍，而不会妥协模型质量。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Generative-Modeling-of-Sequential-Data-with-Trainable-Token-Embedding"><a href="#Quantum-Generative-Modeling-of-Sequential-Data-with-Trainable-Token-Embedding" class="headerlink" title="Quantum Generative Modeling of Sequential Data with Trainable Token Embedding"></a>Quantum Generative Modeling of Sequential Data with Trainable Token Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05050">http://arxiv.org/abs/2311.05050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanda Hou, Li Miao, Yi-Zhuang You</li>
<li>for: 这个论文主要是为了探讨量子概率模型在学习古典和量子数据上的应用。</li>
<li>methods: 这个论文使用的方法是基于矩阵产品状态（MPS）框架的量子启发式生成模型，称为 Born 机器。这种模型支持可追踪的对数概率和自我回归抽样，并在不同的无监督学习任务中表现出色。</li>
<li>results: 这个论文的结果表明，通过同时适应 quantum measurement 操作和 MPS  embedding，Born 机器可以更好地表现，并在数据中寻找更深层次的相关性。<details>
<summary>Abstract</summary>
Generative models are a class of machine learning models that aim to learn the underlying probability distribution of data. Unlike discriminative models, generative models focus on capturing the data's inherent structure, allowing them to generate new samples that resemble the original data. To fully exploit the potential of modeling probability distributions using quantum physics, a quantum-inspired generative model known as the Born machines have shown great advancements in learning classical and quantum data over matrix product state(MPS) framework. The Born machines support tractable log-likelihood, autoregressive and mask sampling, and have shown outstanding performance in various unsupervised learning tasks. However, much of the current research has been centered on improving the expressive power of MPS, predominantly embedding each token directly by a corresponding tensor index. In this study, we generalize the embedding method into trainable quantum measurement operators that can be simultaneously honed with MPS. Our study indicated that combined with trainable embedding, Born machines can exhibit better performance and learn deeper correlations from the dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduction de texte en chinois simplifié模型生成是一类机器学习模型，旨在学习数据的下面概率分布。与描述性模型不同，生成模型关注数据的内在结构，因此可以生成新样本与原始数据类似。为了充分利用量子物理学模型概率分布的潜力，一种基于量子物理学的生成模型叫做生命机器（Born machines）在MPS框架下已经取得了很大的进步。生命机器支持可追踪的对数概率、自动回归和面积抽样，并在多种无监督学习任务中表现出色。然而，当前的大多数研究都集中在提高MPS的表达力，主要是将每个token直接嵌入相应的tensor index。在这项研究中，我们总结了 embedding方法的扩展，使得Born machines可以同时适应MPS。我们的研究表明，将 embedding方法与MPS结合使用，可以使生命机器表现更好，并且从数据中学习更深层的相关性。Note: "MPS" stands for "matrix product state", which is a type of quantum state used in quantum computing.
</details></li>
</ul>
<hr>
<h2 id="On-the-Consistency-of-Maximum-Likelihood-Estimation-of-Probabilistic-Principal-Component-Analysis"><a href="#On-the-Consistency-of-Maximum-Likelihood-Estimation-of-Probabilistic-Principal-Component-Analysis" class="headerlink" title="On the Consistency of Maximum Likelihood Estimation of Probabilistic Principal Component Analysis"></a>On the Consistency of Maximum Likelihood Estimation of Probabilistic Principal Component Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05046">http://arxiv.org/abs/2311.05046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arghya Datta, Sayak Chakrabarty</li>
<li>for: 降低数据维度的统计工具PPCA的应用广泛，从科学与工程到数量金融。</li>
<li>methods: 使用 quotient topological spaces 方法，解决PPCA模型中的征化问题，并且Proof 了ML解方法的一致性。</li>
<li>results:  ML解方法是consistent 的，并且可以在一个适当的quotient Euclidean space中进行强 consistency 的covariance estimation。<details>
<summary>Abstract</summary>
Probabilistic principal component analysis (PPCA) is currently one of the most used statistical tools to reduce the ambient dimension of the data. From multidimensional scaling to the imputation of missing data, PPCA has a broad spectrum of applications ranging from science and engineering to quantitative finance.   Despite this wide applicability in various fields, hardly any theoretical guarantees exist to justify the soundness of the maximal likelihood (ML) solution for this model. In fact, it is well known that the maximum likelihood estimation (MLE) can only recover the true model parameters up to a rotation. The main obstruction is posed by the inherent identifiability nature of the PPCA model resulting from the rotational symmetry of the parameterization. To resolve this ambiguity, we propose a novel approach using quotient topological spaces and in particular, we show that the maximum likelihood solution is consistent in an appropriate quotient Euclidean space. Furthermore, our consistency results encompass a more general class of estimators beyond the MLE. Strong consistency of the ML estimate and consequently strong covariance estimation of the PPCA model have also been established under a compactness assumption.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese: probabistic principal component analysis (PPCA) 是目前最广泛使用的统计工具，用于缩小数据的环境维度。从多元缩放到缺失数据的插入，PPCA在科学和工程等领域有广泛的应用。  despite its wide range of applications, there are few theoretical guarantees to justify the soundness of the maximum likelihood (ML) solution for this model. In fact, it is well known that the maximum likelihood estimation (MLE) can only recover the true model parameters up to a rotation. The main obstruction is posed by the inherent identifiability nature of the PPCA model resulting from the rotational symmetry of the parameterization. To resolve this ambiguity, we propose a novel approach using quotient topological spaces and show that the maximum likelihood solution is consistent in an appropriate quotient Euclidean space. Furthermore, our consistency results encompass a more general class of estimators beyond the MLE. Strong consistency of the ML estimate and consequently strong covariance estimation of the PPCA model have also been established under a compactness assumption.
</details></li>
</ul>
<hr>
<h2 id="DEMASQ-Unmasking-the-ChatGPT-Wordsmith"><a href="#DEMASQ-Unmasking-the-ChatGPT-Wordsmith" class="headerlink" title="DEMASQ: Unmasking the ChatGPT Wordsmith"></a>DEMASQ: Unmasking the ChatGPT Wordsmith</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05019">http://arxiv.org/abs/2311.05019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kavita Kumari, Alessandro Pegoraro, Hossein Fereidooni, Ahmad-Reza Sadeghi</li>
<li>for: This paper aims to detect content generated by ChatGPT, a popular language model, in order to address the concerns of false information, plagiarism, academic dishonesty, and fraudulent activities that may arise from its use.</li>
<li>methods: The proposed method, called DEMASQ, is an energy-based detection model that incorporates novel aspects such as optimization inspired by the Doppler effect and the use of explainable AI techniques to generate diverse perturbations.</li>
<li>results: The paper demonstrates that DEMASQ achieves high accuracy in identifying content generated by ChatGPT, outperforming previous detection methods.<details>
<summary>Abstract</summary>
The potential misuse of ChatGPT and other Large Language Models (LLMs) has raised concerns regarding the dissemination of false information, plagiarism, academic dishonesty, and fraudulent activities. Consequently, distinguishing between AI-generated and human-generated content has emerged as an intriguing research topic. However, current text detection methods lack precision and are often restricted to specific tasks or domains, making them inadequate for identifying content generated by ChatGPT. In this paper, we propose an effective ChatGPT detector named DEMASQ, which accurately identifies ChatGPT-generated content. Our method addresses two critical factors: (i) the distinct biases in text composition observed in human- and machine-generated content and (ii) the alterations made by humans to evade previous detection methods. DEMASQ is an energy-based detection model that incorporates novel aspects, such as (i) optimization inspired by the Doppler effect to capture the interdependence between input text embeddings and output labels, and (ii) the use of explainable AI techniques to generate diverse perturbations. To evaluate our detector, we create a benchmark dataset comprising a mixture of prompts from both ChatGPT and humans, encompassing domains such as medical, open Q&A, finance, wiki, and Reddit. Our evaluation demonstrates that DEMASQ achieves high accuracy in identifying content generated by ChatGPT.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的潜在滥用问题，包括传播false信息、抄袭、学术不当行为和诈欺活动，导致识别人类和机器生成内容的研究变得非常有兴趣。然而，目前的文本检测方法缺乏精度，通常仅适用于特定任务或领域，无法正确地识别ChatGPT生成的内容。在本文中，我们提出了一个高精度的ChatGPT检测器，名为DEMASQ，可以准确地识别ChatGPT生成的内容。我们的方法解决了两个重要因素：（i）人类和机器生成内容中文字的不同偏见，（ii）人类对于避免先前检测方法的修改。DEMASQ是一个能量基于的检测模型，包括以下两个新的特点：（i）静电效应启发的优化方法，用于捕捉输入文本嵌入和出力标签之间的互相依赖关系，（ii）使用可解释AI技术生成多样的扰动。为了评估DEMASQ，我们创建了一个包括ChatGPT和人类产生的标准 benchmark dataset，覆盖医学、开放Q&A、金融、Wiki和Reddit等领域。我们的评估结果显示，DEMASQ可以高精度地识别ChatGPT生成的内容。
</details></li>
</ul>
<hr>
<h2 id="GPU-Accelerated-WFST-Beam-Search-Decoder-for-CTC-based-Speech-Recognition"><a href="#GPU-Accelerated-WFST-Beam-Search-Decoder-for-CTC-based-Speech-Recognition" class="headerlink" title="GPU-Accelerated WFST Beam Search Decoder for CTC-based Speech Recognition"></a>GPU-Accelerated WFST Beam Search Decoder for CTC-based Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04996">http://arxiv.org/abs/2311.04996</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvidia-riva/riva-asrlib-decoder">https://github.com/nvidia-riva/riva-asrlib-decoder</a></li>
<li>paper_authors: Daniel Galvez, Tim Kaldewey</li>
<li>for: 提高自动语音识别（ASR）管道的性能，使用GPU加速Weighted Finite State Transducer（WFST）搜索解码器。</li>
<li>methods: 使用GPU加速WFST搜索解码器，支持流处理推理，支持实时扩展，提供预制DLPack基本绑定。</li>
<li>results: 在离线和在线场景下，与当前状态 искусственный神经网络（CTC）模型相比，实现最快的搜索解码器，在离线场景下达到最高7倍的throughput，在流处理场景下达到近8倍的响应时间，与同等或更好的词错率。<details>
<summary>Abstract</summary>
While Connectionist Temporal Classification (CTC) models deliver state-of-the-art accuracy in automated speech recognition (ASR) pipelines, their performance has been limited by CPU-based beam search decoding. We introduce a GPU-accelerated Weighted Finite State Transducer (WFST) beam search decoder compatible with current CTC models. It increases pipeline throughput and decreases latency, supports streaming inference, and also supports advanced features like utterance-specific word boosting via on-the-fly composition. We provide pre-built DLPack-based python bindings for ease of use with Python-based machine learning frameworks at https://github.com/nvidia-riva/riva-asrlib-decoder. We evaluated our decoder for offline and online scenarios, demonstrating that it is the fastest beam search decoder for CTC models. In the offline scenario it achieves up to 7 times more throughput than the current state-of-the-art CPU decoder and in the online streaming scenario, it achieves nearly 8 times lower latency, with same or better word error rate.
</details>
<details>
<summary>摘要</summary>
while Connectionist Temporal Classification (CTC) models provide state-of-the-art accuracy in automated speech recognition (ASR) pipelines, their performance has been limited by CPU-based beam search decoding. we introduce a GPU-accelerated Weighted Finite State Transducer (WFST) beam search decoder compatible with current CTC models. it increases pipeline throughput and decreases latency, supports streaming inference, and also supports advanced features like utterance-specific word boosting via on-the-fly composition. we provide pre-built DLPack-based python bindings for ease of use with Python-based machine learning frameworks at https://github.com/nvidia-riva/riva-asrlib-decoder. we evaluated our decoder for offline and online scenarios, demonstrating that it is the fastest beam search decoder for CTC models. in the offline scenario it achieves up to 7 times more throughput than the current state-of-the-art CPU decoder and in the online streaming scenario, it achieves nearly 8 times lower latency, with same or better word error rate.
</details></li>
</ul>
<hr>
<h2 id="Optimized-measurements-of-chaotic-dynamical-systems-via-the-information-bottleneck"><a href="#Optimized-measurements-of-chaotic-dynamical-systems-via-the-information-bottleneck" class="headerlink" title="Optimized measurements of chaotic dynamical systems via the information bottleneck"></a>Optimized measurements of chaotic dynamical systems via the information bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04896">http://arxiv.org/abs/2311.04896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kieran A. Murphy, Dani S. Bassett</li>
<li>for: 这篇论文旨在找到一种高效地从运动轨迹数据中提取信息的方法，以便更好地理解系统的动态行为。</li>
<li>methods: 该论文使用机器学习技术来优化测量过程，以便更好地捕捉系统的信息。</li>
<li>results: 该论文对多种杂音映射进行了 approximately optimal 的测量，并为通用时间序列提供了可用的基础。<details>
<summary>Abstract</summary>
Deterministic chaos permits a precise notion of a "perfect measurement" as one that, when obtained repeatedly, captures all of the information created by the system's evolution with minimal redundancy. Finding an optimal measurement is challenging, and has generally required intimate knowledge of the dynamics in the few cases where it has been done. We establish an equivalence between a perfect measurement and a variant of the information bottleneck. As a consequence, we can employ machine learning to optimize measurement processes that efficiently extract information from trajectory data. We obtain approximately optimal measurements for multiple chaotic maps and lay the necessary groundwork for efficient information extraction from general time series.
</details>
<details>
<summary>摘要</summary>
deterministic 混沌允许我们定义一个"完美测量"，即在重复获取的情况下，捕捉系统的演化创造的信息，最小化重复性。发现优化测量是困难的，通常需要系统动力学的深入了解，只有在极少数情况下完成。我们证明了完美测量与信息瓶颈之间的等价关系，因此我们可以使用机器学习来优化测量过程，以高效地从曲线数据中提取信息。我们在多个混沌地图上获得了约似优化的测量，并为普通时间序列信息提取做了必要的准备。
</details></li>
</ul>
<hr>
<h2 id="Computing-with-Residue-Numbers-in-High-Dimensional-Representation"><a href="#Computing-with-Residue-Numbers-in-High-Dimensional-Representation" class="headerlink" title="Computing with Residue Numbers in High-Dimensional Representation"></a>Computing with Residue Numbers in High-Dimensional Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04872">http://arxiv.org/abs/2311.04872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cjkymn/residuehdcomputing">https://github.com/cjkymn/residuehdcomputing</a></li>
<li>paper_authors: Christopher J. Kymn, Denis Kleyko, E. Paxon Frady, Connor Bybee, Pentti Kanerva, Friedrich T. Sommer, Bruno A. Olshausen</li>
<li>for: 这篇论文是用于描述一种新的计算框架，即废弃数 residue 超级计算。</li>
<li>methods: 该框架使用 Random, high-dimensional vectors 来表示 residue 数字，并使用组件 wise, 并行的运算来实现 algebra 操作。</li>
<li>results: 该框架可以使用远 fewer 资源来表示和操作大范围的数字，并且具有强大的鲁棒性 against noise。 它可以解决 computationally difficult 问题，例如视觉处理和 combinatorial optimization。<details>
<summary>Abstract</summary>
We introduce Residue Hyperdimensional Computing, a computing framework that unifies residue number systems with an algebra defined over random, high-dimensional vectors. We show how residue numbers can be represented as high-dimensional vectors in a manner that allows algebraic operations to be performed with component-wise, parallelizable operations on the vector elements. The resulting framework, when combined with an efficient method for factorizing high-dimensional vectors, can represent and operate on numerical values over a large dynamic range using vastly fewer resources than previous methods, and it exhibits impressive robustness to noise. We demonstrate the potential for this framework to solve computationally difficult problems in visual perception and combinatorial optimization, showing improvement over baseline methods. More broadly, the framework provides a possible account for the computational operations of grid cells in the brain, and it suggests new machine learning architectures for representing and manipulating numerical data.
</details>
<details>
<summary>摘要</summary>
我们介绍了剩余超维计算框架，这是一种将剩余数系统与随机高维向量上的代数相结合的计算框架。我们表明了剩余数可以用高维向量的元素进行 componenwise、并行化的运算，从而实现了对大范围的数值进行表示和操作，并且具有很好的鲁棒性于噪声。我们通过对高维向量的因子化方法进行有效实现，实现了在资源受限的情况下解决 computationally Difficult 问题的能力。我们通过对视觉认知和 combinatorial 优化问题的解决方案来说明框架的潜在力量，以及它在机器学习中表示和操作数字数据的新架构。
</details></li>
</ul>
<hr>
<h2 id="Algorithms-for-Non-Negative-Matrix-Factorization-on-Noisy-Data-With-Negative-Values"><a href="#Algorithms-for-Non-Negative-Matrix-Factorization-on-Noisy-Data-With-Negative-Values" class="headerlink" title="Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values"></a>Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04855">http://arxiv.org/abs/2311.04855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dylan Green, Stephen Bailey</li>
<li>for: 本文旨在探讨非正式矩阵分解（NMF）如何处理含有负值的天文数据，特别是在低信号响应下。</li>
<li>methods: 本文提出了两种算法：Shift-NMF和Nearly-NMF，它们都可以正确地处理含有负值的输入数据，而不需要clip负数据。</li>
<li>results: 数学分析和实验表明，Shift-NMF和Nearly-NMF算法都具有 monotonically decreasing 的更新规则，并且可以正确地回归非正式信号。<details>
<summary>Abstract</summary>
Non-negative matrix factorization (NMF) is a dimensionality reduction technique that has shown promise for analyzing noisy data, especially astronomical data. For these datasets, the observed data may contain negative values due to noise even when the true underlying physical signal is strictly positive. Prior NMF work has not treated negative data in a statistically consistent manner, which becomes problematic for low signal-to-noise data with many negative values. In this paper we present two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and also any introduced negativity. Both of these algorithms use the negative data space without clipping, and correctly recover non-negative signals without any introduced positive offset that occurs when clipping negative data. We demonstrate this numerically on both simple and more realistic examples, and prove that both algorithms have monotonically decreasing update rules.
</details>
<details>
<summary>摘要</summary>
非正定矩阵分解（NMF）是一种维度减少技术，已经在天文数据分析中展现了承诺。这些数据可能会包含负值噪声，即使真实的物理信号是正的。过去的NMF工作没有统计正确处理负数据，这会对低信号响应度数据 WITH 多个负值引起问题。在这篇论文中，我们提出了两种算法：Shift-NMF和Nearly-NMF，它们可以处理输入数据的噪声和引入的负值。这两种算法使用负数据空间而不是clip，并能正确回归非正定信号而无需引入Positive offset。我们通过数值计算和 teorema 证明了这两种算法的更新规则减少 monotonic。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-temporal-dynamics-of-mutations-to-enhance-the-prediction-capability-of-antiretroviral-therapy’s-outcome-for-HIV-1"><a href="#Incorporating-temporal-dynamics-of-mutations-to-enhance-the-prediction-capability-of-antiretroviral-therapy’s-outcome-for-HIV-1" class="headerlink" title="Incorporating temporal dynamics of mutations to enhance the prediction capability of antiretroviral therapy’s outcome for HIV-1"></a>Incorporating temporal dynamics of mutations to enhance the prediction capability of antiretroviral therapy’s outcome for HIV-1</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04846">http://arxiv.org/abs/2311.04846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulia Di Teodoro, Martin Pirkl, Francesca Incardona, Ilaria Vicenti, Anders Sönnerborg, Rolf Kaiser, Laura Palagi, Maurizio Zazzi, Thomas Lengauer</li>
<li>for: 预测HIV治疗结果</li>
<li>methods: 使用历史信息加权病毒变异，考虑病毒变异的时间发生和同时测量病毒荷量</li>
<li>results: 使用历史信息可以提高预测精度，H-模型的ROC-AUC分数高于NH-模型（76.34% VS 74.98%），并且在不同时间点进行预测时表现出了更好的一致性。<details>
<summary>Abstract</summary>
Motivation: In predicting HIV therapy outcomes, a critical clinical question is whether using historical information can enhance predictive capabilities compared with current or latest available data analysis. This study analyses whether historical knowledge, which includes viral mutations detected in all genotypic tests before therapy, their temporal occurrence, and concomitant viral load measurements, can bring improvements. We introduce a method to weigh mutations, considering the previously enumerated factors and the reference mutation-drug Stanford resistance tables. We compare a model encompassing history (H) with one not using it (NH). Results: The H-model demonstrates superior discriminative ability, with a higher ROC-AUC score (76.34%) than the NH-model (74.98%). Significant Wilcoxon test results confirm that incorporating historical information improves consistently predictive accuracy for treatment outcomes. The better performance of the H-model might be attributed to its consideration of latent HIV reservoirs, probably obtained when leveraging historical information. The findings emphasize the importance of temporal dynamics in mutations, offering insights into HIV infection complexities. However, our result also shows that prediction accuracy remains relatively high even when no historical information is available. Supplementary information: Supplementary material is available.
</details>
<details>
<summary>摘要</summary>
目的：研究是否使用历史信息可以提高预测HIV治疗结果的能力，比较使用当前或最新可用的数据分析方法。这个研究发现，使用历史知识，包括在治疗之前的所有种类测试中检测到的病毒变异，其时间发生和同时测量病毒荷载，可以提高预测精度。我们提出一种将变异加权的方法，考虑以上因素以及参考荷载抗荷载表。我们将 comparing一个包含历史信息（H）模型和一个不使用历史信息（NH）模型。结果：H模型的预测能力显著高于NH模型（76.34% vs 74.98%），并且在不同时间点上的预测精度也有显著差异。这些结果表明，包含历史信息可以提高预测精度，但并不是必需的。补充信息：补充材料可以在附录中找到。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Dimensions-Confident-Reachability-for-High-Dimensional-Controllers"><a href="#Bridging-Dimensions-Confident-Reachability-for-High-Dimensional-Controllers" class="headerlink" title="Bridging Dimensions: Confident Reachability for High-Dimensional Controllers"></a>Bridging Dimensions: Confident Reachability for High-Dimensional Controllers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04843">http://arxiv.org/abs/2311.04843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuang Geng, Souradeep Dutta, Ivan Ruchkin</li>
<li>for:  This paper aims to improve the verification of high-dimensional controllers in autonomous systems, specifically those using deep neural networks.</li>
<li>methods: The paper proposes a new approach that approximates the behavior of a high-dimensional controller with several low-dimensional controllers in different regions of the state space, and uses verification-aware knowledge distillation to balance approximation and verifiability.</li>
<li>results: The paper shows convincing performance in two OpenAI gym benchmarks using two inflation techniques, one based on trajectories and the other based on actions. The results provide high-confidence reachability guarantees for the high-dimensional controller.<details>
<summary>Abstract</summary>
Autonomous systems are increasingly implemented using end-end-end trained controllers. Such controllers make decisions that are executed on the real system with images as one of the primary sensing modalities. Deep neural networks form a fundamental building block of such controllers. Unfortunately, the existing neural-network verification tools do not scale to inputs with thousands of dimensions. Especially when the individual inputs (such as pixels) are devoid of clear physical meaning. This paper takes a step towards connecting exhaustive closed-loop verification with high-dimensional controllers. Our key insight is that the behavior of a high-dimensional controller can be approximated with several low-dimensional controllers in different regions of the state space. To balance approximation and verifiability, we leverage the latest verification-aware knowledge distillation. Then, if low-dimensional reachability results are inflated with statistical approximation errors, they yield a high-confidence reachability guarantee for the high-dimensional controller. We investigate two inflation techniques -- based on trajectories and actions -- both of which show convincing performance in two OpenAI gym benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Toward-Rapid-Optimal-and-Feasible-Power-Dispatch-through-Generalized-Neural-Mapping"><a href="#Toward-Rapid-Optimal-and-Feasible-Power-Dispatch-through-Generalized-Neural-Mapping" class="headerlink" title="Toward Rapid, Optimal, and Feasible Power Dispatch through Generalized Neural Mapping"></a>Toward Rapid, Optimal, and Feasible Power Dispatch through Generalized Neural Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04838">http://arxiv.org/abs/2311.04838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meiyi Li, Javad Mohammadi</li>
<li>for: 提高大规模电力系统优化决策效率，适应分布式和连接的Grid，使用机器学习模型提高优化效果。</li>
<li>methods: 提出了LOOP-LC 2.0模型，基于学习来优化优化过程，保证解决方案的可行性和实际性，不需要耗时consuming的迭代过程。</li>
<li>results: 对IEEE-200测试 случа件进行比较，LOOP-LC 2.0方法的训练速度、计算时间、优化效果和解决方案可行性均有显著提高 compared to 现有方法。<details>
<summary>Abstract</summary>
The evolution towards a more distributed and interconnected grid necessitates large-scale decision-making within strict temporal constraints. Machine learning (ML) paradigms have demonstrated significant potential in improving the efficacy of optimization processes. However, the feasibility of solutions derived from ML models continues to pose challenges. It's imperative that ML models produce solutions that are attainable and realistic within the given system constraints of power systems. To address the feasibility issue and expedite the solution search process, we proposed LOOP-LC 2.0(Learning to Optimize the Optimization Process with Linear Constraints version 2.0) as a learning-based approach for solving the power dispatch problem. A notable advantage of the LOOP-LC 2.0 framework is its ability to ensure near-optimality and strict feasibility of solutions without depending on computationally intensive post-processing procedures, thus eliminating the need for iterative processes. At the heart of the LOOP-LC 2.0 model lies the newly proposed generalized gauge map method, capable of mapping any infeasible solution to a feasible point within the linearly-constrained domain. The proposed generalized gauge map method improves the traditional gauge map by exhibiting reduced sensitivity to input variances while increasing search speeds significantly. Utilizing the IEEE-200 test case as a benchmark, we demonstrate the effectiveness of the LOOP-LC 2.0 methodology, confirming its superior performance in terms of training speed, computational time, optimality, and solution feasibility compared to existing methodologies.
</details>
<details>
<summary>摘要</summary>
随着Grid的分布和连接度的演化，大规模决策在强制时间限制下变得越来越重要。机器学习（ML）模式在优化过程中表现出了显著的潜力。然而，ML模型生成的解决方案的可行性仍然存在挑战。为了解决可行性问题并加速解决过程，我们提出了LOOP-LC 2.0（学习优化优化过程的线性约束版本2.0），一种基于学习的电力派发问题解决方法。LOOP-LC 2.0框架的一个优点是它可以保证解决的解决方案准确性和可行性，不需要进行计算 INTENSIVE post-processing 过程，因此消除了迭代过程的需要。LOOP-LC 2.0 模型的核心是新提出的通用抽象映射方法，可以将任何不可行的解决方案映射到可行的点 dentro de la domain de restricciones lineales。相比传统的抽象映射方法，通用抽象映射方法具有更低的输入方差敏感度和更高的搜索速度。使用 IEEE-200 测试 caso como referencia，我们证明了LOOP-LC 2.0 方法的有效性，其在培训速度、计算时间、优化性和可行性方面表现出了明显的优势 compared to 现有方法。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Recurrent-Reinforcement-Learning"><a href="#Real-Time-Recurrent-Reinforcement-Learning" class="headerlink" title="Real-Time Recurrent Reinforcement Learning"></a>Real-Time Recurrent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04830">http://arxiv.org/abs/2311.04830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Julian Lemmel, Radu Grosu</li>
<li>for:  solving partially-observable Markov decision processes (POMDPs) using biologically plausible methods</li>
<li>methods:  using random feedback local online learning (RFLO) and temporaldifference reinforcement learning with eligibility traces (TD($\lambda$)) to compute gradients of recurrent neural network parameters in an online manner</li>
<li>results:  RFLO can perform just as well as real-time recurrent learning (RTRL) with less complexity, and the proposed method (RTRRL) serves as a model of learning in biological neural networks mimicking reward pathways in the mammalian brain.<details>
<summary>Abstract</summary>
Recent advances in reinforcement learning, for partially-observable Markov decision processes (POMDPs), rely on the biologically implausible backpropagation through time algorithm (BPTT) to perform gradient-descent optimisation. In this paper we propose a novel reinforcement learning algorithm that makes use of random feedback local online learning (RFLO), a biologically plausible approximation of realtime recurrent learning (RTRL) to compute the gradients of the parameters of a recurrent neural network in an online manner. By combining it with TD($\lambda$), a variant of temporaldifference reinforcement learning with eligibility traces, we create a biologically plausible, recurrent actor-critic algorithm, capable of solving discrete and continuous control tasks in POMDPs. We compare BPTT, RTRL and RFLO as well as different network architectures, and find that RFLO can perform just as well as RTRL while exceeding even BPTT in terms of complexity. The proposed method, called real-time recurrent reinforcement learning (RTRRL), serves as a model of learning in biological neural networks mimicking reward pathways in the mammalian brain.
</details>
<details>
<summary>摘要</summary>
近期在部分可观测 Markov决策过程（POMDP）中的再强化学习进步，利用生物不切实际的 backwards propagation through time 算法（BPTT）来实现梯度下降优化。在这篇论文中，我们提出了一种新的再强化学习算法，使用随机反馈局部在线学习（RFLO），这是一种生物可能的抽象，来计算激活函数参数的梯度。通过与 TD($\lambda$) 结合，一种变体的时间差异再强化学习算法，我们创建了一种生物可能的、 recurrent actor-critic 算法，可以解决 POMDP 中的离散和连续控制任务。我们比较了 BPTT、RTRL 和 RFLO 以及不同的网络架构，发现 RFLO 可以与 RTRL 相当，而且 même surpass BPTT 的复杂性。提出的方法，称为实时回归再强化学习（RTRRL），作为生物神经网络学习模型，模拟奖 PATHways 在哺乳动物大脑中的学习过程。
</details></li>
</ul>
<hr>
<h2 id="Functional-Bayesian-Tucker-Decomposition-for-Continuous-indexed-Tensor-Data"><a href="#Functional-Bayesian-Tucker-Decomposition-for-Continuous-indexed-Tensor-Data" class="headerlink" title="Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data"></a>Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04829">http://arxiv.org/abs/2311.04829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikai Fang, Xin Yu, Zheng Wang, Shibo Li, Mike Kirby, Shandian Zhe</li>
<li>for: 寻找一种方法来扩展tensor decomposition来处理不同方面的连续型数据。</li>
<li>methods: 提议了Functional Bayesian Tucker Decomposition（FunBaT）方法，将连续型数据视为Tucker核和一组隐函数之间的交互。使用 Gaussian Processes（GP）作为函数先验，然后将GP转换为状态方程先验来降低计算成本。</li>
<li>results: 在Synthetic数据和实际应用中，提议的方法能够显著提高tensor decomposition的灵活性和效率。<details>
<summary>Abstract</summary>
Tucker decomposition is a powerful tensor model to handle multi-aspect data. It demonstrates the low-rank property by decomposing the grid-structured data as interactions between a core tensor and a set of object representations (factors). A fundamental assumption of such decomposition is that there were finite objects in each aspect or mode, corresponding to discrete indexes of data entries. However, many real-world data are not naturally posed in the setting. For example, geographic data is represented as continuous indexes of latitude and longitude coordinates, and cannot fit tensor models directly. To generalize Tucker decomposition to such scenarios, we propose Functional Bayesian Tucker Decomposition (FunBaT). We treat the continuous-indexed data as the interaction between the Tucker core and a group of latent functions. We use Gaussian processes (GP) as functional priors to model the latent functions, and then convert the GPs into a state-space prior by constructing an equivalent stochastic differential equation (SDE) to reduce computational cost. An efficient inference algorithm is further developed for scalable posterior approximation based on advanced message-passing techniques. The advantage of our method is shown in both synthetic data and several real-world applications.
</details>
<details>
<summary>摘要</summary>
各种多方面数据的处理可以通过图ucker分解来实现。这种分解示出了图ucker的低级性质，它将格子结构数据分解为核tensor和一组对象表示（因素）之间的交互。然而，许多实际世界的数据不是直接适用于图ucker模型的。例如，地理数据通常表示为维度坐标的连续标记，无法直接适用于图ucker模型。为了推广图ucker分解到这些场景，我们提议了功能 bayesian 图ucker分解（FunBaT）。我们将连续标记的数据视为图ucker核和一组隐函数之间的交互。我们使用 Gaussian 过程（GP）作为隐函数的函数先验，然后将GP转换为状态空间先验，以降低计算成本。我们还开发了一种可扩展的 posterior 近似算法，以便可扩展到大规模数据。我们的方法在synthetic数据和一些实际应用中表现出了优势。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Architecture-for-Real-Time-Neuronal-Spike-Classification"><a href="#A-Lightweight-Architecture-for-Real-Time-Neuronal-Spike-Classification" class="headerlink" title="A Lightweight Architecture for Real-Time Neuronal-Spike Classification"></a>A Lightweight Architecture for Real-Time Neuronal-Spike Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04808">http://arxiv.org/abs/2311.04808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ali Siddiqi, David Vrijenhoek, Lennart P. L. Landsmeer, Job van der Kleij, Anteneh Gebregiorgis, Vincenzo Romano, Rajendra Bishnoi, Said Hamdioui, Christos Strydis</li>
<li>for: 理解大脑功能，尤其是硬膜下卷绕细胞（Purkinje cells）在肌功能损伤和脑部受损时的作用。</li>
<li>methods: 利用硬膜下卷绕细胞的特点，实时抛弃不需要的神经数据，并将压缩数据存储在头部设备上的可 removable 存储器上。</li>
<li>results: 提出了一种轻量级神经采集和分类架构，可以在实时实现 &gt;95% 的总分类精度，同时具有小型设计和低功耗特性，使头部设备可以靠一个小电池供电，可以持续运行约 4 天。<details>
<summary>Abstract</summary>
Electrophysiological recordings of neural activity in a mouse's brain are very popular among neuroscientists for understanding brain function. One particular area of interest is acquiring recordings from the Purkinje cells in the cerebellum in order to understand brain injuries and the loss of motor functions. However, current setups for such experiments do not allow the mouse to move freely and, thus, do not capture its natural behaviour since they have a wired connection between the animal's head stage and an acquisition device. In this work, we propose a lightweight neuronal-spike detection and classification architecture that leverages on the unique characteristics of the Purkinje cells to discard unneeded information from the sparse neural data in real time. This allows the (condensed) data to be easily stored on a removable storage device on the head stage, alleviating the need for wires. Our proposed implementation shows a >95% overall classification accuracy while still resulting in a small-form-factor design, which allows for the free movement of mice during experiments. Moreover, the power-efficient nature of the design and the usage of STT-RAM (Spin Transfer Torque Magnetic Random Access Memory) as the removable storage allows the head stage to easily operate on a tiny battery for up to approximately 4 days.
</details>
<details>
<summary>摘要</summary>
neuroscientists 非常喜欢使用电生物学记录神经活动的mouse脑中的记录，以了解脑功能。一个具有潜在价值的领域是从粒细 Purkinje 细胞中获取记录，以了解脑损伤和lost of motor functions。但现有的实验设置不允许鼠标自由移动，因此不能捕捉其自然行为，因为它们有一个连接鼠标头stage和收集设备的硬件连接。在这种工作中，我们提出了一种轻量级神经元发射检测和分类架构，利用粒细 Purkinje 细胞的特有特征，在实时中抛弃不必要的神经数据。这使得（缩减）数据可以轻松地存储在鼠标头stage上的可 removable 存储设备上，解决了需要硬件连接的问题。我们的提议实现显示了 >95% 的总分类精度，同时仍保持小型设计，允许鼠标在实验中自由移动。此外，设计的能效性和使用 STT-RAM（磁转转换栅隔隔 Memory）作为可 removable 存储，使得头stage可以轻松运行在 tiny 电池上，可以达到约4天的操作时间。
</details></li>
</ul>
<hr>
<h2 id="The-PetShop-Dataset-–-Finding-Causes-of-Performance-Issues-across-Microservices"><a href="#The-PetShop-Dataset-–-Finding-Causes-of-Performance-Issues-across-Microservices" class="headerlink" title="The PetShop Dataset – Finding Causes of Performance Issues across Microservices"></a>The PetShop Dataset – Finding Causes of Performance Issues across Microservices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04806">http://arxiv.org/abs/2311.04806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaela Hardt, William Orchard, Patrick Blöbaum, Shiva Kasiviswanathan, Elke Kirschbaum</li>
<li>for: 本研究旨在提供一个特定于微服务应用的根本原因分析数据集，用于评估不同的根本原因分析方法。</li>
<li>methods: 本研究使用了一个分布式应用程序 emit 5 分钟间隔的延迟、请求和可用性指标，并在系统中随机引入了 68 个性能问题，以模拟不良行为。</li>
<li>results: 本研究通过使用这个数据集，证明了这个数据集可以用于评估不同的根本原因分析方法的准确性。<details>
<summary>Abstract</summary>
Identifying root causes for unexpected or undesirable behavior in complex systems is a prevalent challenge. This issue becomes especially crucial in modern cloud applications that employ numerous microservices. Although the machine learning and systems research communities have proposed various techniques to tackle this problem, there is currently a lack of standardized datasets for quantitative benchmarking. Consequently, research groups are compelled to create their own datasets for experimentation. This paper introduces a dataset specifically designed for evaluating root cause analyses in microservice-based applications. The dataset encompasses latency, requests, and availability metrics emitted in 5-minute intervals from a distributed application. In addition to normal operation metrics, the dataset includes 68 injected performance issues, which increase latency and reduce availability throughout the system. We showcase how this dataset can be used to evaluate the accuracy of a variety of methods spanning different causal and non-causal characterisations of the root cause analysis problem. We hope the new dataset, available at https://github.com/amazon-science/petshop-root-cause-analysis/ enables further development of techniques in this important area.
</details>
<details>
<summary>摘要</summary>
通用系统中异常或不满意的行为的根本原因识别是一个广泛存在的挑战。特别是现代云应用程序使用多个微服务，这问题变得更加重要。虽然机器学习和系统研究共同体已经提出了各种方法来解决这个问题，但目前还没有标准化的数据集用于量化比较。因此，研究组织被迫创建自己的数据集用于实验。这篇文章介绍了一个专门为识别微服务基本应用中的根本原因分析而设计的数据集。该数据集包括5分钟间隔的延迟、请求和可用性指标，以及68个注入性性能问题，这些问题会在系统中增加延迟和降低可用性。我们展示了如何使用这个数据集来评估多种不同的 causal 和非 causal 根本原因分析问题的准确性。我们希望新的数据集，可以在 <https://github.com/amazon-science/petshop-root-cause-analysis/> 上获取，能够推动这一重要领域的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Why-Do-Clinical-Probabilistic-Models-Fail-To-Transport-Between-Sites"><a href="#Why-Do-Clinical-Probabilistic-Models-Fail-To-Transport-Between-Sites" class="headerlink" title="Why Do Clinical Probabilistic Models Fail To Transport Between Sites?"></a>Why Do Clinical Probabilistic Models Fail To Transport Between Sites?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04787">http://arxiv.org/abs/2311.04787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas A. Lasko, Eric V. Strobl, William W. Stead</li>
<li>for: 本研究旨在解释在健康领域中人工智能的应用中出现的问题，即模型在训练站上达到超人类性能后在新站点上表现差异较大。</li>
<li>methods: 本研究使用了分析常见导致模型在新站点上表现差异的源头，并将这些源头分为实验者可控的和数据生成过程中的内在源头。</li>
<li>results: 研究发现，数据生成过程中的站点特有的临床实践可能导致模型在新站点上表现差异，并提出了一种解决方案，即隔离数据中各站点临床实践的影响，以便更好地预测疾病的发展趋势。<details>
<summary>Abstract</summary>
The rising popularity of artificial intelligence in healthcare is highlighting the problem that a computational model achieving super-human clinical performance at its training sites may perform substantially worse at new sites. In this perspective, we present common sources for this failure to transport, which we divide into sources under the control of the experimenter and sources inherent to the clinical data-generating process. Of the inherent sources we look a little deeper into site-specific clinical practices that can affect the data distribution, and propose a potential solution intended to isolate the imprint of those practices on the data from the patterns of disease cause and effect that are the usual target of clinical models.
</details>
<details>
<summary>摘要</summary>
人工智能在医疗领域的普及正加剧了一个问题：一个计算模型在训练 Site 上达到超人类优秀表现，但在新 Site 上可能表现很差。在这个视角下，我们描述了不能传输的常见来源，分为实验者控制的源和数据生成过程中的自然源。其中，我们对内在的源进一步分析了Site-specific临床实践对数据分布的影响，并提出了一种解决方案，以隔离临床实践对数据的影响，从而更好地预测疾病的 causa 和效果。
</details></li>
</ul>
<hr>
<h2 id="FetMRQC-an-open-source-machine-learning-framework-for-multi-centric-fetal-brain-MRI-quality-control"><a href="#FetMRQC-an-open-source-machine-learning-framework-for-multi-centric-fetal-brain-MRI-quality-control" class="headerlink" title="FetMRQC: an open-source machine learning framework for multi-centric fetal brain MRI quality control"></a>FetMRQC: an open-source machine learning framework for multi-centric fetal brain MRI quality control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04780">http://arxiv.org/abs/2311.04780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medical-image-analysis-laboratory/fetal_brain_qc">https://github.com/medical-image-analysis-laboratory/fetal_brain_qc</a></li>
<li>paper_authors: Thomas Sanchez, Oscar Esteban, Yvan Gomez, Alexandre Pron, Mériam Koob, Vincent Dunet, Nadine Girard, Andras Jakab, Elisenda Eixarch, Guillaume Auzias, Meritxell Bach Cuadra</li>
<li>for: 这个论文旨在提供一种自动化图像质量评估和控制框架，以提高胎儿脑部MRI图像的质量和可靠性。</li>
<li>methods: 这种框架使用机器学习算法，提取了不同扫描仪和数据采集中的质量指标，并将其组合成Random Forest模型来预测专家评分。</li>
<li>results: 研究表明，FetMRQC的预测结果在不同的扫描仪和数据采集中具有良好的泛化能力和可解释性。<details>
<summary>Abstract</summary>
Fetal brain MRI is becoming an increasingly relevant complement to neurosonography for perinatal diagnosis, allowing fundamental insights into fetal brain development throughout gestation. However, uncontrolled fetal motion and heterogeneity in acquisition protocols lead to data of variable quality, potentially biasing the outcome of subsequent studies. We present FetMRQC, an open-source machine-learning framework for automated image quality assessment and quality control that is robust to domain shifts induced by the heterogeneity of clinical data. FetMRQC extracts an ensemble of quality metrics from unprocessed anatomical MRI and combines them to predict experts' ratings using random forests. We validate our framework on a pioneeringly large and diverse dataset of more than 1600 manually rated fetal brain T2-weighted images from four clinical centers and 13 different scanners. Our study shows that FetMRQC's predictions generalize well to unseen data while being interpretable. FetMRQC is a step towards more robust fetal brain neuroimaging, which has the potential to shed new insights on the developing human brain.
</details>
<details>
<summary>摘要</summary>
《胎儿脑MRI在产前诊断中成为越来越重要的补充，允许深入了解胎儿脑发育的全程。但是，无法控制胎儿运动和数据采集协议的不同导致数据质量存在变化，可能影响后续研究的结果。我们提出了FetMRQC，一个开源的机器学习框架，用于自动评估和控制图像质量，对域外传递产生的影响具有抗难度特性。FetMRQC从未处理的 анатомичеMRI中提取一 ensemble of 质量指标，使用随机森林将其组合成为专家评分。我们验证了我们的框架，使用了1600多个手动评分的胎儿脑T2强化MRI图像，来自四个临床中心和13个不同的扫描仪。我们的研究表明，FetMRQC的预测能够在未看到数据上具有良好的泛化能力，同时具有可解释性。FetMRQC是更加Robust的胎儿脑神经成像的一步，它有可能为人类脑发育带来新的发现。》Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Deep-Neural-Network-Approximation-for-Korobov-Functions-with-respect-to-Sobolev-Norms"><a href="#Optimal-Deep-Neural-Network-Approximation-for-Korobov-Functions-with-respect-to-Sobolev-Norms" class="headerlink" title="Optimal Deep Neural Network Approximation for Korobov Functions with respect to Sobolev Norms"></a>Optimal Deep Neural Network Approximation for Korobov Functions with respect to Sobolev Norms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04779">http://arxiv.org/abs/2311.04779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yahong Yang, Yulong Lu</li>
<li>for: 这个论文为了解决深度神经网络（DNNs）应用于科罗波夫函数时的近似问题而写作。</li>
<li>methods: 该论文使用了深度神经网络来近似科罗波夫函数，并使用$L_p$ norms和$H^1$ norms来衡量近似结果。</li>
<li>results: 该论文获得了一个很高的近似率，超过传统方法和任何连续函数近似器。这些结果是非对数的，可以同时考虑网络宽度和深度。<details>
<summary>Abstract</summary>
This paper establishes the nearly optimal rate of approximation for deep neural networks (DNNs) when applied to Korobov functions, effectively overcoming the curse of dimensionality. The approximation results presented in this paper are measured with respect to $L_p$ norms and $H^1$ norms. Our achieved approximation rate demonstrates a remarkable "super-convergence" rate, outperforming traditional methods and any continuous function approximator. These results are non-asymptotic, providing error bounds that consider both the width and depth of the networks simultaneously.
</details>
<details>
<summary>摘要</summary>
Note:* "Korobov functions" 是指具有特定的某种函数形式的函数。* "curse of dimensionality" 是指在高维空间中， tradicional method 的拟合率会随着维度的增加而减慢。* "super-convergence" 是指拟合率比传统方法更快地增长。* "non-asymptotic" 是指不含 asymptotic 的概率 bound。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Unified-Framework-of-Contrastive-Learning-for-Disentangled-Representations"><a href="#Towards-a-Unified-Framework-of-Contrastive-Learning-for-Disentangled-Representations" class="headerlink" title="Towards a Unified Framework of Contrastive Learning for Disentangled Representations"></a>Towards a Unified Framework of Contrastive Learning for Disentangled Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04774">http://arxiv.org/abs/2311.04774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Matthes, Zhiwei Han, Hao Shen</li>
<li>for: 本研究旨在扩展对冲学习方法的理论保证，以便在更广泛的对冲搜索空间中找到和分离数据中的解释因素。</li>
<li>methods: 本研究使用了四种冲对方法，包括雷达对冲估计（NCE）和信息对冲估计（InfoNCE）等。</li>
<li>results: 研究人员通过 теорем的证明，证明了这些对冲方法可以帮助找到和分离数据中的解释因素，而不需要假设数据生成过程的特定假设。这些结论在多个标准数据集上进行了验证。<details>
<summary>Abstract</summary>
Contrastive learning has recently emerged as a promising approach for learning data representations that discover and disentangle the explanatory factors of the data. Previous analyses of such approaches have largely focused on individual contrastive losses, such as noise-contrastive estimation (NCE) and InfoNCE, and rely on specific assumptions about the data generating process. This paper extends the theoretical guarantees for disentanglement to a broader family of contrastive methods, while also relaxing the assumptions about the data distribution. Specifically, we prove identifiability of the true latents for four contrastive losses studied in this paper, without imposing common independence assumptions. The theoretical findings are validated on several benchmark datasets. Finally, practical limitations of these methods are also investigated.
</details>
<details>
<summary>摘要</summary>
contrastive learning 最近 emerged as a promising approach for learning data representations that discover and disentangle the explanatory factors of the data. Previous analyses of such approaches have largely focused on individual contrastive losses, such as noise-contrastive estimation (NCE) and InfoNCE, and rely on specific assumptions about the data generating process. This paper extends the theoretical guarantees for disentanglement to a broader family of contrastive methods, while also relaxing the assumptions about the data distribution. Specifically, we prove identifiability of the true latents for four contrastive losses studied in this paper, without imposing common independence assumptions. The theoretical findings are validated on several benchmark datasets. Finally, practical limitations of these methods are also investigated.
</details></li>
</ul>
<hr>
<h2 id="Towards-Open-world-Cross-Domain-Sequential-Recommendation-A-Model-Agnostic-Contrastive-Denoising-Approach"><a href="#Towards-Open-world-Cross-Domain-Sequential-Recommendation-A-Model-Agnostic-Contrastive-Denoising-Approach" class="headerlink" title="Towards Open-world Cross-Domain Sequential Recommendation: A Model-Agnostic Contrastive Denoising Approach"></a>Towards Open-world Cross-Domain Sequential Recommendation: A Model-Agnostic Contrastive Denoising Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04760">http://arxiv.org/abs/2311.04760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wujiang Xu, Xuying Ning, Wenfang Lin, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, Minnan Luo</li>
<li>for: 提高开放世界CDSR场景中模型的一致性和有效性（1st CH）</li>
<li>methods: 使用辅助行为来补充长尾用户的信息（2nd CH）</li>
<li>results: 这些SR方法无法在CDSR场景中提供优秀的表现，因为它们忽略了目标行为和辅助行为之间的semantic gap，以及用户兴趣偏移 across domains（2nd CH）<details>
<summary>Abstract</summary>
Cross-domain sequential recommendation (CDSR) aims to address the data sparsity problems that exist in traditional sequential recommendation (SR) systems.   The existing approaches aim to design a specific cross-domain unit that can transfer and propagate information across multiple domains by relying on overlapping users with abundant behaviors. However, in real-world recommender systems, CDSR scenarios usually consist of a majority of long-tailed users with sparse behaviors and cold-start users who only exist in one domain. This leads to a drop in the performance of existing CDSR methods in the real-world industry platform. Therefore, improving the consistency and effectiveness of models in open-world CDSR scenarios is crucial for constructing CDSR models (\textit{1st} CH). Recently, some SR approaches have utilized auxiliary behaviors to complement the information for long-tailed users. However, these multi-behavior SR methods cannot deliver promising performance in CDSR, as they overlook the semantic gap between target and auxiliary behaviors, as well as user interest deviation across domains (\textit{2nd} CH).
</details>
<details>
<summary>摘要</summary>
Recently, some SR approaches have used auxiliary behaviors to complement information for long-tailed users. However, these multi-behavior SR methods cannot deliver promising performance in CDSR due to the semantic gap between target and auxiliary behaviors, as well as user interest deviation across domains.
</details></li>
</ul>
<hr>
<h2 id="Natural-Bayesian-Cramer-Rao-Bound-with-an-Application-to-Covariance-Estimation"><a href="#Natural-Bayesian-Cramer-Rao-Bound-with-an-Application-to-Covariance-Estimation" class="headerlink" title="Natural Bayesian Cramér-Rao Bound with an Application to Covariance Estimation"></a>Natural Bayesian Cramér-Rao Bound with an Application to Covariance Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04748">http://arxiv.org/abs/2311.04748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florent Bouchard, Alexandre Renaux, Guillaume Ginolhac, Arnaud Breloy</li>
<li>for: 本研究提出了一种新的克拉默-拉托 bound (CRB)，用于估计参数在拓扑 manifold 上并且受到先验分布的影响。</li>
<li>methods: 本研究使用了一种新的 derivation 方法，导致了一种自然的 geometrical 性质准则和这个新的 bound 之间的不等式。</li>
<li>results: 数值 simulations 表明，提出的 CRB 可以展示一些MAP 估计器的有趣性质，而经典极大似然估计器 (Bayesian CRB) 不能够显示出这些性质。<details>
<summary>Abstract</summary>
In this paper, we propose to develop a new Cram\'er-Rao Bound (CRB) when the parameter to estimate lies in a manifold and follows a prior distribution. This derivation leads to a natural inequality between an error criteria based on geometrical properties and this new bound. This main contribution is illustrated in the problem of covariance estimation when the data follow a Gaussian distribution and the prior distribution is an inverse Wishart. Numerical simulation shows new results where the proposed CRB allows to exhibit interesting properties of the MAP estimator which are not observed with the classical Bayesian CRB.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的卡默-拉托矩bound（CRB），其中参数需要估计的投影到一个拓扑上，并且遵循一个先验分布。这个 derivation 导致了一种自然的准则，该准则与这个新的矩bound 之间存在一种对应关系。这种主要贡献在 covariance 估计中进行了应用，其中数据遵循 Gaussian 分布，先验分布是 inverse Wishart。数值实验显示，我们的提议的 CRB 可以展示一些MAP 估计器的有趣特性，这些特性与 классическому Bayesian CRB 不可见。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Multi-Agent-Coordination-through-Common-Operating-Picture-Integration"><a href="#Enhancing-Multi-Agent-Coordination-through-Common-Operating-Picture-Integration" class="headerlink" title="Enhancing Multi-Agent Coordination through Common Operating Picture Integration"></a>Enhancing Multi-Agent Coordination through Common Operating Picture Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04740">http://arxiv.org/abs/2311.04740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peihong Yu, Bhoram Lee, Aswin Raghavan, Supun Samarasekara, Pratap Tokekar, James Zachary Hare</li>
<li>For: This paper focuses on improving multi-agent coordination in dynamic environments, where agents possess only local observations and must communicate to enhance coordination.* Methods: The proposed approach uses a Common Operating Picture (COP) that integrates each agent’s observations, actions, and messages received, and disseminates the COP to other agents. This approach takes into account the dynamic nature of the environment and the shared mission.* Results: The paper shows that COP-based training leads to robust policies compared to state-of-the-art MARL methods when faced with out-of-distribution initial states, through experiments in the StarCraft2 environment.Here’s the summary in Simplified Chinese:</li>
<li>for: 多 Agent 系统中的 Agent 仅 possessed 本地观察，通信成为协调的关键。这篇 paper 针对这个问题提出了一种方法。</li>
<li>methods: 提议的方法使用 Common Operating Picture (COP)，让每个 Agent 统一其观察、动作和获取的讯息，并将 COP 分享到其他 Agent 中。这种方法考虑了环境的动态性和共同任务。</li>
<li>results: 这篇 paper 透过 StarCraft2 环境进行实验，证明 COP-based 训练对于离distribution 初始状态时的策略比 state-of-the-art Multi-Agent Reinforcement Learning (MARL) 方法更加Robust。<details>
<summary>Abstract</summary>
In multi-agent systems, agents possess only local observations of the environment. Communication between teammates becomes crucial for enhancing coordination. Past research has primarily focused on encoding local information into embedding messages which are unintelligible to humans. We find that using these messages in agent's policy learning leads to brittle policies when tested on out-of-distribution initial states. We present an approach to multi-agent coordination, where each agent is equipped with the capability to integrate its (history of) observations, actions and messages received into a Common Operating Picture (COP) and disseminate the COP. This process takes into account the dynamic nature of the environment and the shared mission. We conducted experiments in the StarCraft2 environment to validate our approach. Our results demonstrate the efficacy of COP integration, and show that COP-based training leads to robust policies compared to state-of-the-art Multi-Agent Reinforcement Learning (MARL) methods when faced with out-of-distribution initial states.
</details>
<details>
<summary>摘要</summary>
在多智能系统中，智能体仅具有本地环境观察。团队成员之间的交流成为协调的关键。过去的研究主要集中在编码本地信息到嵌入消息中，这些消息对人类不可读。我们发现，在智能体政策学习中使用这些消息会导致不稳定的政策，对于非标准初始状态进行测试时。我们提出了一种多智能协调方法，其中每个智能体具有将其（历史观察、行动和接收的消息）集成为共同运作图像（COP）的能力，并将COP分布给其他团队成员。这个过程考虑了环境的动态性和共同任务。我们在StarCraft2环境中进行了实验，以验证我们的方法。我们的结果表明COP集成的有效性，并示出COP基于培训在对不同初始状态进行测试时，与现有多智能学习方法相比，具有更加稳定的政策。
</details></li>
</ul>
<hr>
<h2 id="Robust-Best-arm-Identification-in-Linear-Bandits"><a href="#Robust-Best-arm-Identification-in-Linear-Bandits" class="headerlink" title="Robust Best-arm Identification in Linear Bandits"></a>Robust Best-arm Identification in Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04731">http://arxiv.org/abs/2311.04731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Wang, Sattar Vakili, Ilija Bogunovic</li>
<li>for: 这种研究旨在解决 robust best-arm identification problem (RBAI) 中的 linear rewards 问题，目标是找到一个近似优化的 robust arm，以便在实际应用中实现 transferred 的优化策略。</li>
<li>methods: 该研究提出了一个实例取值下的下界，并提出了静态和适应式bandit算法，以实现与下界匹配的样本复杂度。</li>
<li>results: 在synthetic实验中，该算法能够有效地找到最佳的 robust arm，并与oracle策略相似。在应用中，该算法在不同年龄层的病人中实现了robust dosage值的标准化。<details>
<summary>Abstract</summary>
We study the robust best-arm identification problem (RBAI) in the case of linear rewards. The primary objective is to identify a near-optimal robust arm, which involves selecting arms at every round and assessing their robustness by exploring potential adversarial actions. This approach is particularly relevant when utilizing a simulator and seeking to identify a robust solution for real-world transfer. To this end, we present an instance-dependent lower bound for the robust best-arm identification problem with linear rewards. Furthermore, we propose both static and adaptive bandit algorithms that achieve sample complexity that matches the lower bound. In synthetic experiments, our algorithms effectively identify the best robust arm and perform similarly to the oracle strategy. As an application, we examine diabetes care and the process of learning insulin dose recommendations that are robust with respect to inaccuracies in standard calculators. Our algorithms prove to be effective in identifying robust dosage values across various age ranges of patients.
</details>
<details>
<summary>摘要</summary>
我们研究了Robust Best-Arm Identification问题（RBAI）在线性奖励情况下。主要目标是找到近似优化的Robust arm，这里是在每个轮次选择武器并评估它们的Robustness，通过探索敌方动作的可能性。这种方法特别有用在使用模拟器并寻找实际世界中的稳定解决方案。为此，我们提出了一个实例dependent的下界 дляRobust Best-Arm Identification问题，并提出了静态和适应式bandit算法，这些算法的样本复杂度与下界相匹配。在 sintetic 实验中，我们的算法成功地确定了最佳Robust arm，并与oracle策略相似。作为应用，我们研究了diabetes care和学习不准确的标准计算器中的药物剂量建议的Robust性。我们的算法在不同年龄范围的患者中表现出了有效的Robust剂量值。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Properties-of-Nodes-via-Community-Aware-Features"><a href="#Predicting-Properties-of-Nodes-via-Community-Aware-Features" class="headerlink" title="Predicting Properties of Nodes via Community-Aware Features"></a>Predicting Properties of Nodes via Community-Aware Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04730">http://arxiv.org/abs/2311.04730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sebkaz/betastar">https://github.com/sebkaz/betastar</a></li>
<li>paper_authors: Bogumił Kamiński, Paweł Prałat, François Théberge, Sebastian Zając</li>
<li>for: 本研究旨在提出一家族的社区意识型节点特征，并研究其性质。</li>
<li>methods: 本文提出了一种基于社区意识的节点特征家族，并对其进行了 investigate。</li>
<li>results: 研究表明，这种特征家族具有高预测力 для分类任务，并且包含不可recover的信息， neither by classical node features nor by node embeddings。<details>
<summary>Abstract</summary>
A community structure that is often present in complex networks plays an important role not only in their formation but also shapes dynamics of these networks, affecting properties of their nodes. In this paper, we propose a family of community-aware node features and then investigate their properties. We show that they have high predictive power for classification tasks. We also verify that they contain information that cannot be recovered neither by classical node features nor by node embeddings (both classical as well as structural).
</details>
<details>
<summary>摘要</summary>
复杂网络中常见的社区结构不仅参与网络的形成，还对网络的动态shape有重要影响，对节点的性质产生影响。在这篇论文中，我们提出了一家族社区意识型节点特征，然后调查其性质。我们发现它们具有高预测力 для分类任务。我们还证明它们不可能通过传统节点特征还是结构节点嵌入获得。
</details></li>
</ul>
<hr>
<h2 id="Robust-and-Communication-Efficient-Federated-Domain-Adaptation-via-Random-Features"><a href="#Robust-and-Communication-Efficient-Federated-Domain-Adaptation-via-Random-Features" class="headerlink" title="Robust and Communication-Efficient Federated Domain Adaptation via Random Features"></a>Robust and Communication-Efficient Federated Domain Adaptation via Random Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04686">http://arxiv.org/abs/2311.04686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sadangelf/fedrf-tca">https://github.com/sadangelf/fedrf-tca</a></li>
<li>paper_authors: Zhanbo Feng, Yuanjie Wang, Jie Li, Fan Yang, Jiong Lou, Tiebin Mi, Robert. C. Qiu, Zhenyu Liao</li>
<li>for: This paper is written for researchers and practitioners who are interested in federated domain adaptation (FDA) and want to improve the efficiency and robustness of their FDA methods.</li>
<li>methods: The paper proposes an enhancement to the standard Transfer Component Analysis (TCA) approach, called RF-TCA, which significantly accelerates computation without compromising theoretical and empirical performance. The proposed FedRF-TCA protocol is an extension of RF-TCA to the FDA setting, which has communication complexity that is independent of the sample size and maintains performance that is either comparable to or even surpasses state-of-the-art FDA methods.</li>
<li>results: The paper presents extensive experiments to showcase the superior performance and robustness (to network condition) of FedRF-TCA compared to state-of-the-art FDA methods. The results demonstrate that FedRF-TCA can handle large-scale FDA tasks with high efficiency and accuracy, and is robust to network conditions.Here is the answer in Simplified Chinese:</li>
<li>for: 这篇论文是为研究者和实践者们，他们关心联合预测领域（Federated Domain Adaptation，FDA）的人们所写的。</li>
<li>methods: 这篇论文提出了改进标准传输组分分析（TCA）方法的增强版本，即RF-TCA，它可以在 computation 方面减少计算量，而不会产生理论和实际性的损害。提出的 FedRF-TCA 协议是 TCA 的扩展，用于 FDA 设定，它的通信复杂度是采样大小独立的，而且可以保持和现有 FDA 方法相比或者甚至超越其性能。</li>
<li>results: 这篇论文通过了广泛的实验，证明 FedRF-TCA 可以处理大规模 FDA 任务，并且具有高效性和精度。它还可以在不同的网络条件下保持稳定性和可靠性。<details>
<summary>Abstract</summary>
Modern machine learning (ML) models have grown to a scale where training them on a single machine becomes impractical. As a result, there is a growing trend to leverage federated learning (FL) techniques to train large ML models in a distributed and collaborative manner. These models, however, when deployed on new devices, might struggle to generalize well due to domain shifts. In this context, federated domain adaptation (FDA) emerges as a powerful approach to address this challenge.   Most existing FDA approaches typically focus on aligning the distributions between source and target domains by minimizing their (e.g., MMD) distance. Such strategies, however, inevitably introduce high communication overheads and can be highly sensitive to network reliability.   In this paper, we introduce RF-TCA, an enhancement to the standard Transfer Component Analysis approach that significantly accelerates computation without compromising theoretical and empirical performance. Leveraging the computational advantage of RF-TCA, we further extend it to FDA setting with FedRF-TCA. The proposed FedRF-TCA protocol boasts communication complexity that is \emph{independent} of the sample size, while maintaining performance that is either comparable to or even surpasses state-of-the-art FDA methods. We present extensive experiments to showcase the superior performance and robustness (to network condition) of FedRF-TCA.
</details>
<details>
<summary>摘要</summary>
现代机器学习（ML）模型已经发展到了训练在单机器上是不现实的规模。因此，有一个增长的趋势是使用联邦学习（FL）技术来训练大型ML模型在分布式和协作的方式上。这些模型在新设备上部署时可能会遇到领域变化，导致其不能良好地泛化。在这种情况下，联邦领域适应（FDA）作为一种有力的方法来解决这个挑战。现有的FDA方法通常是通过最小化源频率和目标频率之间的差距（例如MMD）来对 distributions进行对齐。然而，这些策略会带来高通信开销并且对网络可靠性非常敏感。在这篇论文中，我们介绍了RF-TCA，一种提高标准传输组件分析方法的优化。RF-TCA可以快速计算，而不需要牺牲理论和实际性能。基于RF-TCA的计算优势，我们进一步扩展了它到FDA设置，得到了FedRF-TCA协议。FedRF-TCA协议的通信复杂度是独立于样本大小的，同时保持和现状最佳的性能。我们进行了广泛的实验，证明FedRF-TCA的超越性和网络条件的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Compressive-Recovery-of-Sparse-Precision-Matrices"><a href="#Compressive-Recovery-of-Sparse-Precision-Matrices" class="headerlink" title="Compressive Recovery of Sparse Precision Matrices"></a>Compressive Recovery of Sparse Precision Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04673">http://arxiv.org/abs/2311.04673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Titouan Vayer, Etienne Lasalle, Rémi Gribonval, Paulo Gonçalves</li>
<li>for: 本研究旨在学习一个图模型，用于统计关系分析dataset中的$d$变量的$n$个样本$X$。</li>
<li>methods: 本研究使用了一种压缩视角，通过从$X$中随机生成的低维度向量来Estimate一个稀疏的$\Theta$。</li>
<li>results: 研究表明，在certain assumptions下，可以从一个低维度sketch中Estimate一个稀疏的$\Theta$，且需要$m&#x3D;\Omega((d+2k)\log(d))$的维度，其中$k$是Underlying graph的最大边数。<details>
<summary>Abstract</summary>
We consider the problem of learning a graph modeling the statistical relations of the $d$ variables of a dataset with $n$ samples $X \in \mathbb{R}^{n \times d}$. Standard approaches amount to searching for a precision matrix $\Theta$ representative of a Gaussian graphical model that adequately explains the data. However, most maximum likelihood-based estimators usually require storing the $d^{2}$ values of the empirical covariance matrix, which can become prohibitive in a high-dimensional setting. In this work, we adopt a compressive viewpoint and aim to estimate a sparse $\Theta$ from a sketch of the data, i.e. a low-dimensional vector of size $m \ll d^{2}$ carefully designed from $X$ using nonlinear random features. Under certain assumptions on the spectrum of $\Theta$ (or its condition number), we show that it is possible to estimate it from a sketch of size $m=\Omega((d+2k)\log(d))$ where $k$ is the maximal number of edges of the underlying graph. These information-theoretic guarantees are inspired by compressed sensing theory and involve restricted isometry properties and instance optimal decoders. We investigate the possibility of achieving practical recovery with an iterative algorithm based on the graphical lasso, viewed as a specific denoiser. We compare our approach and graphical lasso on synthetic datasets, demonstrating its favorable performance even when the dataset is compressed.
</details>
<details>
<summary>摘要</summary>
我们考虑一个图模型，用于描述一个 dataset 中 $d$ 个变数之间的统计关系。标准方法通常是寻找一个精确的 $\Theta$ 矩阵，代表一个 Gaussian 图模型，以便对数据进行适当地描述。但是，大多数最大 LIKELIHOOD 基本的估计方法通常需要储存 $d^2$ 个 empirical covariance matrix 的值，这可能会在高维度设定中成为禁止的。在这个工作中，我们遵循一种压缩的观点，企图从 dataset 中获取一个压缩的 $\Theta$ 矩阵，即一个来自 $X$ 的非线性随机特征下的低维度 вектор。在某些 $\Theta$ 的 спектル（或其 condition number）的假设下，我们展示了可以从获取的大小为 $m \ll d^2$ 的压缩 sketch 中估计 $\Theta$。这些信息理论上的保证是基于数据压缩理论和具有特定的 Restricted Isometry 性和实例最佳解解oder。我们 investigate 可能在实际应用中实现实用的重建，使用一个基于图形lasso 的迭代算法，视为特定的推理器。我们在实验中与图形lasso 进行比较，展示了其优越的表现，即使对于压缩的 dataset。
</details></li>
</ul>
<hr>
<h2 id="Learning-Linear-Gaussian-Polytree-Models-with-Interventions"><a href="#Learning-Linear-Gaussian-Polytree-Models-with-Interventions" class="headerlink" title="Learning Linear Gaussian Polytree Models with Interventions"></a>Learning Linear Gaussian Polytree Models with Interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04636">http://arxiv.org/abs/2311.04636</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emduart2/polytrees">https://github.com/emduart2/polytrees</a></li>
<li>paper_authors: D. Tramontano, L. Waldmann, M. Drton, E. Duarte</li>
<li>for: 学习非 Parametric linear Gaussian 树的 causal 结构，使用来自干预实验的数据，其中干预目标已知。</li>
<li>methods: 方法首先学习树的skeleton，然后将其边 orient。输出是一个 CPDAG，表示真实分布下的树的干预Equivalence class。skeleton和orientation恢复过程都基于第二阶统计和低维度边 Distribution。</li>
<li>results: 在不同场景下的synthetic数据集中，方法具有快速、高精度、可扩展性。在一个基因表达干预数据集中应用方法，并且对结果进行了评估。<details>
<summary>Abstract</summary>
We present a consistent and highly scalable local approach to learn the causal structure of a linear Gaussian polytree using data from interventional experiments with known intervention targets. Our methods first learn the skeleton of the polytree and then orient its edges. The output is a CPDAG representing the interventional equivalence class of the polytree of the true underlying distribution. The skeleton and orientation recovery procedures we use rely on second order statistics and low-dimensional marginal distributions. We assess the performance of our methods under different scenarios in synthetic data sets and apply our algorithm to learn a polytree in a gene expression interventional data set. Our simulation studies demonstrate that our approach is fast, has good accuracy in terms of structural Hamming distance, and handles problems with thousands of nodes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种一致性很高且可扩展的本地方法，用于学习 linear Gaussian 树的 causal 结构，基于干扰实验数据中知道的干扰目标。我们的方法首先学习树的skeleton，然后对其 edges 进行orienting。输出是一个 CPDAG 表示真实下面分布的干扰 equivalence class。我们使用第二阶 Statistics 和低维度边分布来进行骨架和 Orienting 过程。我们在不同情况下进行了simulationstudies，并将方法应用到了一个基因表达干扰数据集中。我们的模拟研究显示，我们的方法具有快速、高准确率和可扩展性。
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Tolerant-Methods-for-Distributed-Variational-Inequalities"><a href="#Byzantine-Tolerant-Methods-for-Distributed-Variational-Inequalities" class="headerlink" title="Byzantine-Tolerant Methods for Distributed Variational Inequalities"></a>Byzantine-Tolerant Methods for Distributed Variational Inequalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04611">http://arxiv.org/abs/2311.04611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nazya/sgda-ra">https://github.com/nazya/sgda-ra</a></li>
<li>paper_authors: Nazarii Tupitsa, Abdulla Jasem Almansoori, Yanlin Wu, Martin Takáč, Karthik Nandakumar, Samuel Horváth, Eduard Gorbunov</li>
<li>for: This paper is written for discussing the problem of Byzantine robustness in distributed training scenarios, particularly in the context of variational inequalities.</li>
<li>methods: The paper proposes several provably Byzantine-robust methods for distributed variational inequality, and thoroughly studies their theoretical convergence.</li>
<li>results: The paper provides numerical comparisons supporting the theoretical findings, and removes the limitations of previous work in this area.<details>
<summary>Abstract</summary>
Robustness to Byzantine attacks is a necessity for various distributed training scenarios. When the training reduces to the process of solving a minimization problem, Byzantine robustness is relatively well-understood. However, other problem formulations, such as min-max problems or, more generally, variational inequalities, arise in many modern machine learning and, in particular, distributed learning tasks. These problems significantly differ from the standard minimization ones and, therefore, require separate consideration. Nevertheless, only one work (Adibi et al., 2022) addresses this important question in the context of Byzantine robustness. Our work makes a further step in this direction by providing several (provably) Byzantine-robust methods for distributed variational inequality, thoroughly studying their theoretical convergence, removing the limitations of the previous work, and providing numerical comparisons supporting the theoretical findings.
</details>
<details>
<summary>摘要</summary>
Robustness to Byzantine attacks is a necessity for various distributed training scenarios. When the training reduces to the process of solving a minimization problem, Byzantine robustness is relatively well-understood. However, other problem formulations, such as min-max problems or, more generally, variational inequalities, arise in many modern machine learning and, in particular, distributed learning tasks. These problems significantly differ from the standard minimization ones and, therefore, require separate consideration. Nevertheless, only one work (Adibi et al., 2022) addresses this important question in the context of Byzantine robustness. Our work makes a further step in this direction by providing several (provably) Byzantine-robust methods for distributed variational inequality, thoroughly studying their theoretical convergence, removing the limitations of the previous work, and providing numerical comparisons supporting the theoretical findings.Here's the translation in Traditional Chinese:Robustness to Byzantine attacks is a necessity for various distributed training scenarios. When the training reduces to the process of solving a minimization problem, Byzantine robustness is relatively well-understood. However, other problem formulations, such as min-max problems or, more generally, variational inequalities, arise in many modern machine learning and, in particular, distributed learning tasks. These problems significantly differ from the standard minimization ones and, therefore, require separate consideration. Nevertheless, only one work (Adibi et al., 2022) addresses this important question in the context of Byzantine robustness. Our work makes a further step in this direction by providing several (provably) Byzantine-robust methods for distributed variational inequality, thoroughly studying their theoretical convergence, removing the limitations of the previous work, and providing numerical comparisons supporting the theoretical findings.
</details></li>
</ul>
<hr>
<h2 id="Accurate-Autism-Spectrum-Disorder-prediction-using-Support-Vector-Classifier-based-on-Federated-Learning-SVCFL"><a href="#Accurate-Autism-Spectrum-Disorder-prediction-using-Support-Vector-Classifier-based-on-Federated-Learning-SVCFL" class="headerlink" title="Accurate Autism Spectrum Disorder prediction using Support Vector Classifier based on Federated Learning (SVCFL)"></a>Accurate Autism Spectrum Disorder prediction using Support Vector Classifier based on Federated Learning (SVCFL)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04606">http://arxiv.org/abs/2311.04606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Mohammadifar, Hasan Samadbin, Arman Daliri</li>
<li>for: 旨在提高autism诊断的准确率和效率，尤其是在诊断的 inicial stages 中。</li>
<li>methods: 使用 Federated Learning 方法和 Support Vector Classifier 方法，通过分析大量数据并找到不同人类评估者之间的 Patterns ，帮助确认诊断或 highlight 需要进一步测试的情况。</li>
<li>results: 在这种方法下，实现了 99% 的准确率和 13% 的提升。<details>
<summary>Abstract</summary>
The path to an autism diagnosis can be long and difficult, and delays can have serious consequences. Artificial intelligence can completely change the way autism is diagnosed, especially when it comes to situations where it is difficult to see the first signs of the disease. AI-based diagnostic tools may help confirm a diagnosis or highlight the need for further testing by analyzing large volumes of data and uncovering patterns that may not be immediately apparent to human evaluators. After a successful and timely diagnosis, autism can be treated through artificial intelligence using various methods. In this article, by using four datasets and gathering them with the federated learning method and diagnosing them with the support vector classifier method, the early diagnosis of this disorder has been discussed. In this method, we have achieved 99% accuracy for predicting autism spectrum disorder and we have achieved 13% improvement in the results.
</details>
<details>
<summary>摘要</summary>
“患有自闭症的诊断路径可能很长，困难，延迟诊断可能会有严重的后果。人工智能可能将改变自闭症的诊断方式，尤其是在诊断难以看到早期症状的情况下。人工智能基本技术可以帮助确认诊断或发现更多的测试是必要的，通过分析大量数据并找到不容易被人类评估者所发现的模式。在这篇文章中，我们使用了四个数据集和聚合它们，使用联邦学习方法进行诊断，使用支持向量分类方法进行预测，实现了99%的自闭症诊断精度，并取得13%的提升。”Note: Please keep in mind that the translation is done by a machine and may not be perfect. It's always best to have a human translator to ensure the accuracy of the translation.
</details></li>
</ul>
<hr>
<h2 id="Zeroth-order-Asynchronous-Learning-with-Bounded-Delays-with-a-Use-case-in-Resource-Allocation-in-Communication-Networks"><a href="#Zeroth-order-Asynchronous-Learning-with-Bounded-Delays-with-a-Use-case-in-Resource-Allocation-in-Communication-Networks" class="headerlink" title="Zeroth-order Asynchronous Learning with Bounded Delays with a Use-case in Resource Allocation in Communication Networks"></a>Zeroth-order Asynchronous Learning with Bounded Delays with a Use-case in Resource Allocation in Communication Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04604">http://arxiv.org/abs/2311.04604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pourya Behmandpoor, Marc Moonen, Panagiotis Patrinos</li>
<li>for: 这篇论文专门研究了多个代理合作实现分布式优化，具体是在各自有不同任务的情况下，代理通过互动来协同优化本地参数，以实现共同任务的最佳化。</li>
<li>methods: 该论文使用的方法包括分布式优化、异步学习和通信延迟等。</li>
<li>results: 论文提出了一种基于异步学习和分布式优化的方法，并提供了对该方法的分析和数学分析，以及一些实验结果，以证明该方法的有效性。<details>
<summary>Abstract</summary>
Distributed optimization has experienced a significant surge in interest due to its wide-ranging applications in distributed learning and adaptation. While various scenarios, such as shared-memory, local-memory, and consensus-based approaches, have been extensively studied in isolation, there remains a need for further exploration of their interconnections. This paper specifically concentrates on a scenario where agents collaborate toward a unified mission while potentially having distinct tasks. Each agent's actions can potentially impact other agents through interactions. Within this context, the objective for the agents is to optimize their local parameters based on the aggregate of local reward functions, where only local zeroth-order oracles are available. Notably, the learning process is asynchronous, meaning that agents update and query their zeroth-order oracles asynchronously while communicating with other agents subject to bounded but possibly random communication delays. This paper presents theoretical convergence analyses and establishes a convergence rate for the proposed approach. Furthermore, it addresses the relevant issue of deep learning-based resource allocation in communication networks and conducts numerical experiments in which agents, acting as transmitters, collaboratively train their individual (possibly unique) policies to maximize a common performance metric.
</details>
<details>
<summary>摘要</summary>
分布式优化在分布式学习和适应中得到了广泛的关注，因为它们在各种应用场景中具有广泛的应用前景。虽然许多场景，如共享内存、本地内存和协议基本上的方法，在孤立的情况下得到了广泛的研究，但是还需要进一步探索这些场景之间的相互连接。这篇论文专门关注在多个代理 collaborate 以实现共同任务的情况下，每个代理的行为可能会影响其他代理的行为。在这个上下文中，代理的目标是在各自的本地参数上优化本地奖励函数，只有本地零次权重可用。它们的学习过程是异步的，meaning that agents update and query their zeroth-order oracles asynchronously while communicating with other agents subject to bounded but possibly random communication delays。这篇论文提供了理论的叠加分析和确定了提案的速度。此外，它还考虑了基于深度学习的通信网络资源分配问题，并进行了 numerically experiments in which agents, acting as transmitters, collaboratively train their individual (possibly unique) policies to maximize a common performance metric。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Based-Resource-Allocator-for-Communication-Systems-with-Dynamic-User-Utility-Demands"><a href="#A-Deep-Learning-Based-Resource-Allocator-for-Communication-Systems-with-Dynamic-User-Utility-Demands" class="headerlink" title="A Deep Learning Based Resource Allocator for Communication Systems with Dynamic User Utility Demands"></a>A Deep Learning Based Resource Allocator for Communication Systems with Dynamic User Utility Demands</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04600">http://arxiv.org/abs/2311.04600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pourya Behmandpoor, Panagiotis Patrinos, Marc Moonen</li>
<li>for: 这篇论文目的是提出一种基于深度学习的资源分配算法，以满足用户的 utility 需求。</li>
<li>methods: 该算法使用了深度神经网络（DNN）来实现用户的 utility 需求调整，并在每个时间点进行了迭代优化算法来优化用户的在线状态。</li>
<li>results: 实验结果表明，该算法可以有效地满足用户的 utility 需求，并且可以在不同的场景下（如中央化和分布式场景）进行部署。<details>
<summary>Abstract</summary>
Deep learning (DL) based resource allocation (RA) has recently gained a lot of attention due to its performance efficiency. However, most of the related studies assume an ideal case where the number of users and their utility demands, e.g., data rate constraints, are fixed and the designed DL based RA scheme exploits a policy trained only for these fixed parameters. A computationally complex policy retraining is required whenever these parameters change. Therefore, in this paper, a DL based resource allocator (ALCOR) is introduced, which allows users to freely adjust their utility demands based on, e.g., their application layer. ALCOR employs deep neural networks (DNNs), as the policy, in an iterative optimization algorithm. The optimization algorithm aims to optimize the on-off status of users in a time-sharing problem to satisfy their utility demands in expectation. The policy performs unconstrained RA (URA) -- RA without taking into account user utility demands -- among active users to maximize the sum utility (SU) at each time instant. Based on the chosen URA scheme, ALCOR can perform RA in a model-based or model-free manner and in a centralized or distributed scenario. Derived convergence analyses provide guarantees for the convergence of ALCOR, and numerical experiments corroborate its effectiveness.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）基于资源分配（RA）在最近吸引了很多关注，因为它的性能效率很高。然而，大多数相关研究假设用户和他们的需求是固定的，而设计的DL基于RA schemes只是在这些固定参数下采用一个已经训练过的策略。因此，在这篇论文中，一种基于DL的资源分配器（ALCOR）被介绍，允许用户自由地调整他们的需求。ALCOR使用深度神经网络（DNN）作为策略，并在迭代优化算法中使用。这个算法的目标是在时间分享问题中，使用者的状态在每个时间点上进行优化，以满足他们的需求。策略在活动用户中进行无约RA（URA），即不考虑用户的需求来进行资源分配，以最大化每个时间点的总用户Utility（SU）。根据选择的URA方案，ALCOR可以在中央化或分布式环境中进行RA，并且可以采用模型基于或模型独立的方式进行RA。 derive的收敛分析提供了ALCOR的收敛性 guarantees，而数字实验证明了它的有效性。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Market-Value-in-Professional-Soccer-Insights-from-Explainable-Machine-Learning-Models"><a href="#Predicting-Market-Value-in-Professional-Soccer-Insights-from-Explainable-Machine-Learning-Models" class="headerlink" title="Predicting Market Value in Professional Soccer: Insights from Explainable Machine Learning Models"></a>Predicting Market Value in Professional Soccer: Insights from Explainable Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04599">http://arxiv.org/abs/2311.04599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyang Huang, Shaoliang Zhang</li>
<li>for: 这个研究旨在预测职业足球运动员市场价值使用可解释机器学习模型。</li>
<li>methods: 我们使用FIFA官方网站Curated数据集，采用 ensemble机器学习方法并与SHAP添加itive exPlanations（SHAP）提供详细的模型预测解释。</li>
<li>results: GBDT模型在评估中获得最高的平均R-Squared值（0.8780）和最低的平均Root Mean Squared Error值（3,221,632.175），表明其在评估中的superior表现。我们的分析发现，球控、短传、完成、抢断、练习和攻击等技能在技能维度是关键的，而冲刺速度和加速在身体维度是关键的，而反应在认知维度是关键的。我们的结果提供了更准确、Objective和一致的市场价值估算框架，为管理层的转会决策提供有用的洞察。<details>
<summary>Abstract</summary>
This study presents an innovative method for predicting the market value of professional soccer players using explainable machine learning models. Using a dataset curated from the FIFA website, we employ an ensemble machine learning approach coupled with Shapley Additive exPlanations (SHAP) to provide detailed explanations of the models' predictions. The GBDT model achieves the highest mean R-Squared (0.8780) and the lowest mean Root Mean Squared Error (3,221,632.175), indicating its superior performance among the evaluated models. Our analysis reveals that specific skills such as ball control, short passing, finishing, interceptions, dribbling, and tackling are paramount within the skill dimension, whereas sprint speed and acceleration are critical in the fitness dimension, and reactions are preeminent in the cognitive dimension. Our results offer a more accurate, objective, and consistent framework for market value estimation, presenting useful insights for managerial decisions in player transfers.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is also known as "Mandarin Chinese" or "Standard Chinese".Translation notes:* "market value" is translated as "市场价值" (shìchǎng jīyà)* "professional soccer players" is translated as "职业足球运动员" (zhíyè zúqiú yùndòngyuán)* "explainable machine learning models" is translated as "可解释机器学习模型" (kějiěshì jīshì yùnxíng módelì)* "ensemble machine learning approach" is translated as "集成机器学习方法" (jíshì jīshì yùnxíng fāngshì)* "Shapley Additive exPlanations" is translated as "夏普利加法解释" (xiàpèlì jiāfāng jiěshì)* "GBDT model" is translated as " Gradient Boosting Decision Tree 模型" (Gradient Boosting Decision Tree módel)* "mean R-Squared" is translated as "平均R平方" (píngjì R píngfāng)* "mean Root Mean Squared Error" is translated as "平均根均方差" (píngjì gēnjì fāngbiān)* "specific skills" is translated as "特定技能" (tèqīng jìnéng)* "fitness dimension" is translated as "身体维度" (shēngrōng wéidù)* "cognitive dimension" is translated as "认知维度" (rènzhì wéidù)
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-as-a-tool-for-quantum-error-reduction-in-quantum-image-processing"><a href="#Deep-learning-as-a-tool-for-quantum-error-reduction-in-quantum-image-processing" class="headerlink" title="Deep learning as a tool for quantum error reduction in quantum image processing"></a>Deep learning as a tool for quantum error reduction in quantum image processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04575">http://arxiv.org/abs/2311.04575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krzysztof Werner, Kamil Wereszczyński, Rafał Potempa, Krzysztof Cyran</li>
<li>for: 这个论文的目的是提出一种基于生成对抗网络的图像识别方法，以减少图像编码使用LPIQE方法所受的总错误。</li>
<li>methods: 该方法使用生成对抗网络和phasdistortion unraveling方法来减少图像编码中的总错误。</li>
<li>results: 该方法可以成功减少图像编码中的总错误，并且可以保持图像的原始特征。<details>
<summary>Abstract</summary>
Despite the limited availability and quantum volume of quantum computers, quantum image representation is a widely researched area. Currently developed methods use quantum entanglement to encode information about pixel positions. These methods range from using the angle parameter of the rotation gate (e.g., the Flexible Representation of Quantum Images, FRQI), sequences of qubits (e.g., Novel Enhanced Quantum Representation, NEQR), or the angle parameter of the phase shift gates (e.g., Local Phase Image Quantum Encoding, LPIQE) for storing color information. All these methods are significantly affected by decoherence and other forms of quantum noise, which is an inseparable part of quantum computing in the noisy intermediate-scale quantum era. These phenomena can highly influence the measurements and result in extracted images that are visually dissimilar to the originals. Because this process is at its foundation quantum, the computational reversal of this process is possible. There are many methods for error correction, mitigation, and reduction, but all of them use quantum computer time or additional qubits to achieve the desired result. We report the successful use of a generative adversarial network trained for image-to-image translation, in conjunction with Phase Distortion Unraveling error reduction method, for reducing overall error in images encoded using LPIQE.
</details>
<details>
<summary>摘要</summary>
尽管量子计算机的可用性和量子量有限，量子图像表示仍然是广泛研究的领域。目前已经开发出的方法利用量子Entanglement来编码图像像素的信息。这些方法包括使用旋转门的角度参数（如Flexible Representation of Quantum Images，FRQI）、顺序的qubits（如Novel Enhanced Quantum Representation，NEQR）或扩散门的角度参数（如Local Phase Image Quantum Encoding，LPIQE）来存储颜色信息。这些方法都受到干扰和其他量子噪声的影响，这些噪声是量子计算在不稳定中型量子时代的不可避免的一部分。这些现象会高度影响测量结果，导致提取的图像与原始图像显示不同。由于这是量子的基础，因此可以通过量子计算的计算反转来解决这个问题。有许多方法用于错误纠正、减轻和减少，但所有这些方法均需要使用量子计算机时间或额外的qubits来实现感兴趣的结果。我们报告了使用基于图像到图像翻译的生成 adversarial network，与扩散门错误降低法相结合，以降低LPIQE编码图像的总错误。
</details></li>
</ul>
<hr>
<h2 id="Information-Theoretic-Generalization-Bounds-for-Transductive-Learning-and-its-Applications"><a href="#Information-Theoretic-Generalization-Bounds-for-Transductive-Learning-and-its-Applications" class="headerlink" title="Information-Theoretic Generalization Bounds for Transductive Learning and its Applications"></a>Information-Theoretic Generalization Bounds for Transductive Learning and its Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04561">http://arxiv.org/abs/2311.04561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huayi Tang, Yong Liu</li>
<li>for: 这个论文是为了研究逻辑学上的推导学习算法的通用化 bound，特别是在信息论上。</li>
<li>methods: 论文使用了数据依赖和算法依赖的通用化 bound，以及在推导学习算法中首次提出的拓扑supersamples概念。</li>
<li>results: 论文显示了在不同信息度下的推导学习算法的通用化 bound，并 deriv了 novel PAC-Bayesian bound和推导学习下的损失地形平坦性。最后，论文应用了结果到 semi-supervised learning 和图学习场景。<details>
<summary>Abstract</summary>
In this paper, we develop data-dependent and algorithm-dependent generalization bounds for transductive learning algorithms in the context of information theory for the first time. We show that the generalization gap of transductive learning algorithms can be bounded by the mutual information between training labels and hypothesis. By innovatively proposing the concept of transductive supersamples, we go beyond the inductive learning setting and establish upper bounds in terms of various information measures. Furthermore, we derive novel PAC-Bayesian bounds and build the connection between generalization and loss landscape flatness under the transductive learning setting. Finally, we present the upper bounds for adaptive optimization algorithms and demonstrate the applications of results on semi-supervised learning and graph learning scenarios. Our theoretic results are validated on both synthetic and real-world datasets.
</details>
<details>
<summary>摘要</summary>
在本文中，我们为推导学习算法的泛化性提供了数据依赖和算法依赖的通用 bound，这是在信息理论中的首次。我们证明了推导学习算法的泛化差可以通过训练标签和假设之间的共识来Upper bound。通过提出了推导超额样本的概念，我们超出了 inductive 学习设定，并在不同的信息度量上建立了Upper bounds。此外，我们还 deriv了 PAC-Bayesian  bound 和泛化与损失函数平坦性之间的连接。最后，我们给出了适应优化算法的Upper bounds，并在 semi-supervised 学习和图学习场景中应用了结果。我们的理论结果通过 synthetic 数据和实际数据进行验证。
</details></li>
</ul>
<hr>
<h2 id="Regression-with-Cost-based-Rejection"><a href="#Regression-with-Cost-based-Rejection" class="headerlink" title="Regression with Cost-based Rejection"></a>Regression with Cost-based Rejection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04550">http://arxiv.org/abs/2311.04550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Cheng, Yuzhou Cao, Haobo Wang, Hongxin Wei, Bo An, Lei Feng</li>
<li>for: 本研究旨在解决 regression 问题中的 cost-based rejection 问题，其中模型可以根据 certain rejection costs 拒绝对某些示例进行预测。</li>
<li>methods: 我们首先将此问题转化为预测风险的问题，然后 deriv 出 Bayes 优化解决方案，其显示了在使用 mean squared error 评价指标时，优化的模型应该拒绝对 variance 大于 rejection cost 的示例进行预测。 我们还提出了一种基于 surrogate loss function 的训练方法，并提供了模型一致性的条件，这意味着我们的提议的 surrogate loss 可以回归 Bayes 优化解决方案。</li>
<li>results: 我们的实验结果表明，我们的提议的方法可以有效地解决 regression 问题中的 cost-based rejection 问题。<details>
<summary>Abstract</summary>
Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:学习与拒绝是一个重要的框架，可以避免严重的预测错误，通过平衡预测和拒绝来做出决策。先前的研究仅将成本基于拒绝应用于分类设定，无法处理连续和无穷目标空间的回归设定。在这篇论文中，我们调查一种新的回归问题，即基于成本的拒绝回归，其中模型可以在某些示例上拒绝进行预测，给定某些拒绝成本。为解决这个问题，我们首先形式化预期风险，然后 derivate Bayes优化解决方案，其显示了优化模型应该在具有更大的方差时拒绝进行预测，当使用mean squared error作为评价指标时。此外，我们提议使用假设损失函数来训练模型，该损失函数考虑了拒绝为二分类问题，并提供了模型一致性的条件，这意味着我们的提议的代理损失可以回归到bayes优化解决方案。广泛的实验证明了我们的提议的有效性。
</details></li>
</ul>
<hr>
<h2 id="FEIR-Quantifying-and-Reducing-Envy-and-Inferiority-for-Fair-Recommendation-of-Limited-Resources"><a href="#FEIR-Quantifying-and-Reducing-Envy-and-Inferiority-for-Fair-Recommendation-of-Limited-Resources" class="headerlink" title="FEIR: Quantifying and Reducing Envy and Inferiority for Fair Recommendation of Limited Resources"></a>FEIR: Quantifying and Reducing Envy and Inferiority for Fair Recommendation of Limited Resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04542">http://arxiv.org/abs/2311.04542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aida-ugent/feir">https://github.com/aida-ugent/feir</a></li>
<li>paper_authors: Nan Li, Bo Kang, Jefrey Lijffijt, Tijl De Bie</li>
<li>for: 这篇论文主要是关于电子招聘和在线约会中的推荐系统，具体来说是研究一种新的公平度量表，以及一种基于这个公平度量表的多目标优化问题。</li>
<li>methods: 这篇论文提出了一种新的公平度量表，即“劣等”（inferiority），它 mesure 用户对推荐的Item的竞争性。同时，它还使用了“嫉妒”（envy）和“实用性”（utility）这两个已有的公平度量表，并将它们组合在一起。这些公平度量表都是非�ifferentiable的，因此 authors使用了概率解释 recommender systems 来将它们转换为可微分的版本。最后，authors将这些公平度量表组合在一起，形成了一个多目标优化问题 called \texttt{FEIR}（Fairness through Envy and Inferiority Reduction）。</li>
<li>results:  experiments 表明，这种方法可以在 synthetic 和实际数据上提高推荐系统的公平性，特别是在劣等和嫉妒方面。在这些实验中， authors 使用了标准的推荐系统作为基础，并对它们进行了post-processing。<details>
<summary>Abstract</summary>
In settings such as e-recruitment and online dating, recommendation involves distributing limited opportunities, calling for novel approaches to quantify and enforce fairness. We introduce \emph{inferiority}, a novel (un)fairness measure quantifying a user's competitive disadvantage for their recommended items. Inferiority complements \emph{envy}, a fairness notion measuring preference for others' recommendations. We combine inferiority and envy with \emph{utility}, an accuracy-related measure of aggregated relevancy scores. Since these measures are non-differentiable, we reformulate them using a probabilistic interpretation of recommender systems, yielding differentiable versions. We combine these loss functions in a multi-objective optimization problem called \texttt{FEIR} (Fairness through Envy and Inferiority Reduction), applied as post-processing for standard recommender systems. Experiments on synthetic and real-world data demonstrate that our approach improves trade-offs between inferiority, envy, and utility compared to naive recommendations and the baseline methods.
</details>
<details>
<summary>摘要</summary>
在电子招聘和在线约会等设置中，推荐具有有限的机会分配，需要开发新的方法来衡量和实施公平。我们介绍了一种新的不公平度量量，称为“劣等”（inferiority），它衡量用户对推荐的项目的竞争性劣势。劣等与嫉妒（envy）和用户性（utility）这三种度量相结合，通过一种多目标优化问题called \texttt{FEIR}（公平性通过劣等和嫉妒减少）来实现。我们将这些损失函数转换为可导的形式，并应用于标准推荐系统的后处理。实验表明，我们的方法可以在劣等、嫉妒和用户性之间进行更好的质量平衡，相比于直观推荐和基准方法。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Assisted-Multiuser-MIMO-Load-Modulated-Systems-for-Enhanced-Downlink-mmWave-Communications"><a href="#Deep-Learning-Assisted-Multiuser-MIMO-Load-Modulated-Systems-for-Enhanced-Downlink-mmWave-Communications" class="headerlink" title="Deep Learning Assisted Multiuser MIMO Load Modulated Systems for Enhanced Downlink mmWave Communications"></a>Deep Learning Assisted Multiuser MIMO Load Modulated Systems for Enhanced Downlink mmWave Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04537">http://arxiv.org/abs/2311.04537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ercong Yu, Jinle Zhu, Qiang Li, Zilong Liu, Hongyang Chen, Shlomo Shamai, H. Vincent Poor</li>
<li>for: 这个论文关注的是多用户负载调制阵列（MU-LMA），它们具有低系统复杂度和成本，适用于 millimeter wave（mmWave）多输入多出口（MIMO）系统。</li>
<li>methods: 这篇论文提出了两种算法：一种是基于全面阵列结构（FAS）的传输器，另一种是基于深度学习（Deep Learning）的阵列设计和独立解码算法。</li>
<li>results: 论文显示了这两种算法的性能是 Robust to imperfect channel state information（CSI）和具有低复杂性的信号检测。另外，使用深度学习阵列设计算法可以适应不同的频率响应。<details>
<summary>Abstract</summary>
This paper is focused on multiuser load modulation arrays (MU-LMAs) which are attractive due to their low system complexity and reduced cost for millimeter wave (mmWave) multi-input multi-output (MIMO) systems. The existing precoding algorithm for downlink MU-LMA relies on a sub-array structured (SAS) transmitter which may suffer from decreased degrees of freedom and complex system configuration. Furthermore, a conventional LMA codebook with codewords uniformly distributed on a hypersphere may not be channel-adaptive and may lead to increased signal detection complexity. In this paper, we conceive an MU-LMA system employing a full-array structured (FAS) transmitter and propose two algorithms accordingly. The proposed FAS-based system addresses the SAS structural problems and can support larger numbers of users. For LMA-imposed constant-power downlink precoding, we propose an FAS-based normalized block diagonalization (FAS-NBD) algorithm. However, the forced normalization may result in performance degradation. This degradation, together with the aforementioned codebook design problems, is difficult to solve analytically. This motivates us to propose a Deep Learning-enhanced (FAS-DL-NBD) algorithm for adaptive codebook design and codebook-independent decoding. It is shown that the proposed algorithms are robust to imperfect knowledge of channel state information and yield excellent error performance. Moreover, the FAS-DL-NBD algorithm enables signal detection with low complexity as the number of bits per codeword increases.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose an MU-LMA system using a full-array structured (FAS) transmitter, which addresses the SAS structural problems and can support larger numbers of users. For LMA-imposed constant-power downlink precoding, we propose an FAS-based normalized block diagonalization (FAS-NBD) algorithm. However, the forced normalization may result in performance degradation. This degradation, together with the aforementioned codebook design problems, is difficult to solve analytically.To address these issues, we propose a Deep Learning-enhanced (FAS-DL-NBD) algorithm for adaptive codebook design and codebook-independent decoding. The proposed algorithm uses deep learning to optimize the codebook design and decoding process, which can improve the performance of the system. Moreover, the FAS-DL-NBD algorithm enables signal detection with low complexity as the number of bits per codeword increases.The proposed algorithms are robust to imperfect knowledge of channel state information and yield excellent error performance. The use of deep learning in the FAS-DL-NBD algorithm enables the system to adapt to changing channel conditions and improve its performance. Additionally, the proposed algorithms have low complexity, which makes them suitable for practical applications.
</details></li>
</ul>
<hr>
<h2 id="An-Unsupervised-Deep-Learning-Approach-for-the-Wave-Equation-Inverse-Problem"><a href="#An-Unsupervised-Deep-Learning-Approach-for-the-Wave-Equation-Inverse-Problem" class="headerlink" title="An Unsupervised Deep Learning Approach for the Wave Equation Inverse Problem"></a>An Unsupervised Deep Learning Approach for the Wave Equation Inverse Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04531">http://arxiv.org/abs/2311.04531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiong-Bin Yan, Keke Wu, Zhi-Qin John Xu, Zheng Ma</li>
<li>for: 实时地球物理几何问题的高精度解析</li>
<li>methods:  integrate deep neural networks and partial differential equations for solving full-waveform inversion problems</li>
<li>results: 提供了一个无监督学习的方法，可以实时地从测量数据中推断地球物理几何 Parameters，并且与传统方法比较，获得更好的结果。<details>
<summary>Abstract</summary>
Full-waveform inversion (FWI) is a powerful geophysical imaging technique that infers high-resolution subsurface physical parameters by solving a non-convex optimization problem. However, due to limitations in observation, e.g., limited shots or receivers, and random noise, conventional inversion methods are confronted with numerous challenges, such as the local-minimum problem. In recent years, a substantial body of work has demonstrated that the integration of deep neural networks and partial differential equations for solving full-waveform inversion problems has shown promising performance. In this work, drawing inspiration from the expressive capacity of neural networks, we provide an unsupervised learning approach aimed at accurately reconstructing subsurface physical velocity parameters. This method is founded on a re-parametrization technique for Bayesian inference, achieved through a deep neural network with random weights. Notably, our proposed approach does not hinge upon the requirement of the labeled training dataset, rendering it exceedingly versatile and adaptable to diverse subsurface models. Extensive experiments show that the proposed approach performs noticeably better than existing conventional inversion methods.
</details>
<details>
<summary>摘要</summary>
全波形推敲（FWI）是一种强大的地球物理成像技术，可以高精度地推算地下物理参数。然而，由于观测限制，如有限的发射器或接收器，以及随机噪声，传统的推敲方法面临着许多挑战，如地点最优化问题。在过去几年，大量的研究表明，将深度神经网络和部分偏微分方程相结合，可以对全波形推敲问题提供有前所未有的表现。在这篇文章中，我们Drawing inspiration from the expressive capacity of neural networks，提出了一种无监督学习方法，用于准确地重建地下物理速度参数。这种方法基于 Bayesian 推敲技术，通过深度神经网络的随机权重来实现。吸引地，我们的提议方法不需要训练数据集，因此非常灵活和适应性强，适用于多种地下模型。经过广泛的实验，我们发现，我们的方法与传统推敲方法相比，表现出了明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Bandit-Learning-to-Rank-with-Position-Based-Click-Models-Personalized-and-Equal-Treatments"><a href="#Bandit-Learning-to-Rank-with-Position-Based-Click-Models-Personalized-and-Equal-Treatments" class="headerlink" title="Bandit Learning to Rank with Position-Based Click Models: Personalized and Equal Treatments"></a>Bandit Learning to Rank with Position-Based Click Models: Personalized and Equal Treatments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04528">http://arxiv.org/abs/2311.04528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchen Zhou, Jia Liu, Yang Jiao, Chaosheng Dong, Yetian Chen, Yan Gao, Yi Sun</li>
<li>for: 这个论文主要针对的问题是在线学习排名（ONL2R），它是推荐系统的基础问题，在过去几年内受到了越来越多的关注。</li>
<li>methods: 这篇论文提出了一种基于多手枪投机（MAB）框架和位置基于点击模型（PCM）的ONL2R模型。但是，开发基于MAB的ONL2R策略是非常具有挑战性，因为这个问题的 combinatorial 性和部分可见性。</li>
<li>results: 这篇论文的主要贡献包括三个方面：一是提出了第一个涵盖所有ONL2R特点的MAB框架；二是基于上述分析框架，开发了两种统一的贪婪策略和UCBRank策略，可以应用于个性化和平等的排名待遇；三是证明了这两种策略在个性化和平等的排名待遇下都能够获得$O(\sqrt{t}\ln t)$和$O(\sqrt{t\ln t})$的任何线性 regret。此外，对于基本难以解决的平等排名待遇，我们也identified了一些集合性的用途函数和其相应的充分条件，under which GreedyRank和UCBRank策略可以在$O(\sqrt{t}\ln t)$和$O(\sqrt{t\ln t})$任何线性 regret下实现优化。<details>
<summary>Abstract</summary>
Online learning to rank (ONL2R) is a foundational problem for recommender systems and has received increasing attention in recent years. Among the existing approaches for ONL2R, a natural modeling architecture is the multi-armed bandit framework coupled with the position-based click model. However, developing efficient online learning policies for MAB-based ONL2R with position-based click models is highly challenging due to the combinatorial nature of the problem, and partial observability in the position-based click model. To date, results in MAB-based ONL2R with position-based click models remain rather limited, which motivates us to fill this gap in this work. Our main contributions in this work are threefold: i) We propose the first general MAB framework that captures all key ingredients of ONL2R with position-based click models. Our model considers personalized and equal treatments in ONL2R ranking recommendations, both of which are widely used in practice; ii) Based on the above analytical framework, we develop two unified greed- and UCB-based policies called GreedyRank and UCBRank, each of which can be applied to personalized and equal ranking treatments; and iii) We show that both GreedyRank and UCBRank enjoy $O(\sqrt{t}\ln t)$ and $O(\sqrt{t\ln t})$ anytime sublinear regret for personalized and equal treatment, respectively. For the fundamentally hard equal ranking treatment, we identify classes of collective utility functions and their associated sufficient conditions under which $O(\sqrt{t}\ln t)$ and $O(\sqrt{t\ln t})$ anytime sublinear regrets are still achievable for GreedyRank and UCBRank, respectively. Our numerical experiments also verify our theoretical results and demonstrate the efficiency of GreedyRank and UCBRank in seeking the optimal action under various problem settings.
</details>
<details>
<summary>摘要</summary>
“在线学习排名（ONL2R）是推荐系统的基础问题，在最近的几年中受到了显著的注意。 exist 的方法 для ONL2R 中，一个自然的模型建构是多臂矢量机制（MAB）与位置基于的点击模型。然而，为了开发具有高效性的在线学习策略，MAB 基于 ONL2R  WITH 位置基于的点击模型 是非常困难的，因为这问题的 combinatorial 性和点击模型中的假设不完整。到目前为止，关于 MAB 基于 ONL2R  WITH 位置基于的点击模型 的结果仍然很有限，这鼓使我们填补这个差距。我们的主要贡献是三个：1. 我们提出了第一个涵盖所有关键成分的 ONL2R 基于 MAB 框架。我们的模型考虑了对推荐的个性化和平等对待，这两种方法在实践中广泛使用；2. 基于上述分析框架，我们开发了两种应用于个性化和平等对待的统一的牛顿策略和 UCB 策略，它们分别被称为 GreedyRank 和 UCBRank；3. 我们证明了 GreedyRank 和 UCBRank 在个性化和平等对待下都具有 $O(\sqrt{t}\ln t)$ 和 $O(\sqrt{t\ln t})$ 任何时间斜线 regret，对应的是，我们还证明了这两种策略在某些特定的问题设定下可以获得 $O(\sqrt{t}\ln t)$ 和 $O(\sqrt{t\ln t})$ 任何时间斜线 regret。我们的实验也验证了我们的理论结果，并证明了 GreedyRank 和 UCBRank 在不同的问题设定下能够有效地寻找优化的动作。”
</details></li>
</ul>
<hr>
<h2 id="Long-term-Time-Series-Forecasting-based-on-Decomposition-and-Neural-Ordinary-Differential-Equations"><a href="#Long-term-Time-Series-Forecasting-based-on-Decomposition-and-Neural-Ordinary-Differential-Equations" class="headerlink" title="Long-term Time Series Forecasting based on Decomposition and Neural Ordinary Differential Equations"></a>Long-term Time Series Forecasting based on Decomposition and Neural Ordinary Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04522">http://arxiv.org/abs/2311.04522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonkyu Lim, Jaehyeon Park, Seojin Kim, Hyowon Wi, Haksoo Lim, Jinsung Jeon, Jeongwhan Choi, Noseong Park</li>
<li>for: 该论文旨在解决长期时间序列预测（LTSF） tasks 中的挑战，具体来说是Linear-based LTSF 模型在实际应用中的表现不佳，主要归因于 transformer-based 方法中的时间信息损失。</li>
<li>methods: 该论文提出了 LTSF-DNODE 模型，其基于线性常微分方程（ODEs）和时间序列分解方法，可以充分利用数据特点。</li>
<li>results: 论文表明 LTSF-DNODE 模型在多个实际数据集上表现出色，并且对每个数据集进行了常微分方程（NODE）核心 régularization 的影响分析。<details>
<summary>Abstract</summary>
Long-term time series forecasting (LTSF) is a challenging task that has been investigated in various domains such as finance investment, health care, traffic, and weather forecasting. In recent years, Linear-based LTSF models showed better performance, pointing out the problem of Transformer-based approaches causing temporal information loss. However, Linear-based approach has also limitations that the model is too simple to comprehensively exploit the characteristics of the dataset. To solve these limitations, we propose LTSF-DNODE, which applies a model based on linear ordinary differential equations (ODEs) and a time series decomposition method according to data statistical characteristics. We show that LTSF-DNODE outperforms the baselines on various real-world datasets. In addition, for each dataset, we explore the impacts of regularization in the neural ordinary differential equation (NODE) framework.
</details>
<details>
<summary>摘要</summary>
长期时间序列预测（LTSF）是一项复杂的任务，在不同领域如金融投资、医疗、交通和天气预测中都有广泛的研究。最近几年，线性基于的 LTSF 模型表现更好，表明了 transformer 基于的方法导致时间信息损失的问题。然而，线性基于的方法也有局限性，模型太简单，无法全面利用数据集的特点。为解决这些局限性，我们提出了 LTSF-DNODE，它利用基于线性常微分方程（ODEs）的模型和根据数据统计特点的时间序列分解方法。我们显示了 LTSF-DNODE 在多个实际 datasets 上的超越基准点。此外，对于每个数据集，我们还探讨了 NODE 框架中的规则化的影响。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Mirror-Descent-Bilevel-Optimization"><a href="#Adaptive-Mirror-Descent-Bilevel-Optimization" class="headerlink" title="Adaptive Mirror Descent Bilevel Optimization"></a>Adaptive Mirror Descent Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04520">http://arxiv.org/abs/2311.04520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feihu Huang</li>
<li>for: 本文提出了一类高效的适应双层方法，用于非 convex 双层优化问题，其上层问题是非 convex 可能具有不规则化的正则化，下层问题是非 convex 问题，满足波佩-{\L}ojasiewicz（PL）条件。</li>
<li>methods: 我们提出了一种高效的适应投影帮助梯度（i.e., AdaPAG）方法，基于镜像下降，并证明其可以在非 convex 双层问题中获得最佳known的梯度复杂度为 $O(\epsilon^{-1})$，用于找到 $\epsilon$-定点解。我们还提出了一种高效的适应随机投影帮助梯度（i.e., AdaVSPAG）方法，基于镜像下降和减少噪声技术，并证明其可以在非 convex 双层问题中获得最佳known的梯度复杂度为 $O(\epsilon^{-3&#x2F;2})$，用于找到 $\epsilon$-定点解。</li>
<li>results: 我们提供了一种有用的 convergence 分析框架，用于我们的方法，并证明其在某些轻量级的假设下具有 $O(\frac{1}{T})$ 的快速收敛率，其中 $T$ 是迭代次数。<details>
<summary>Abstract</summary>
In the paper, we propose a class of efficient adaptive bilevel methods based on mirror descent for nonconvex bilevel optimization, where its upper-level problem is nonconvex possibly with nonsmooth regularization, and its lower-level problem is also nonconvex while satisfies Polyak-{\L}ojasiewicz (PL) condition. To solve these deterministic bilevel problems, we present an efficient adaptive projection-aid gradient (i.e., AdaPAG) method based on mirror descent, and prove that it obtains the best known gradient complexity of $O(\epsilon^{-1})$ for finding an $\epsilon$-stationary solution of nonconvex bilevel problems. To solve these stochastic bilevel problems, we propose an efficient adaptive stochastic projection-aid gradient (i.e., AdaVSPAG) methods based on mirror descent and variance-reduced techniques, and prove that it obtains the best known gradient complexity of $O(\epsilon^{-3/2})$ for finding an $\epsilon$-stationary solution. Since the PL condition relaxes the strongly convex, our algorithms can be used to nonconvex strongly-convex bilevel optimization. Theoretically, we provide a useful convergence analysis framework for our methods under some mild conditions, and prove that our methods have a fast convergence rate of $O(\frac{1}{T})$, where $T$ denotes the number of iterations.
</details>
<details>
<summary>摘要</summary>
文章中，我们提出了一种高效的适应镜架方法，用于非 convex 双层优化问题，其中上层问题是非 convex 可能具有非细ooth 规范，而下层问题是非 convex 问题，满足波佩-{\L}ojasiewicz（PL）条件。为解决这些权重 determine 的双层问题，我们提出了一种高效的适应投影帮助梯度（i.e., AdaPAG）方法，基于镜架 descent，并证明其可以在 $O(\epsilon^{-1})$ 时间内找到非 convex 双层问题的 $\epsilon$-稳定解。为解决这些随机双层问题，我们提出了一种高效的适应随机投影帮助梯度（i.e., AdaVSPAG）方法，基于镜架 descent 和减少噪声技术，并证明其可以在 $O(\epsilon^{-3/2})$ 时间内找到非 convex 双层问题的 $\epsilon$-稳定解。由于 PL 条件放宽了强式 convex，我们的算法可以应用于非 convex 强式 convex 双层优化问题。我们提供了一个有用的 convergence 分析框架，并证明我们的方法在某些轻微条件下具有 $O(\frac{1}{T})$ 的快速收敛率，where $T$ 表示迭代次数。
</details></li>
</ul>
<hr>
<h2 id="Towards-Democratizing-AI-A-Comparative-Analysis-of-AI-as-a-Service-Platforms-and-the-Open-Space-for-Machine-Learning-Approach"><a href="#Towards-Democratizing-AI-A-Comparative-Analysis-of-AI-as-a-Service-Platforms-and-the-Open-Space-for-Machine-Learning-Approach" class="headerlink" title="Towards Democratizing AI: A Comparative Analysis of AI as a Service Platforms and the Open Space for Machine Learning Approach"></a>Towards Democratizing AI: A Comparative Analysis of AI as a Service Platforms and the Open Space for Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04518">http://arxiv.org/abs/2311.04518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dennis Rall, Bernhard Bauer, Thomas Fraunholz</li>
<li>for: 本研究旨在推动人工智能的普及和宣扬，但现有的AIaaS平台仍有一定的障碍，因此本研究旨在 Comparing several popular AI-as-a-Service platforms and identifying the key requirements for a platform that can achieve true democratization of AI.</li>
<li>methods: 本研究使用了多种现代技术，如Kubernetes、Kubeflow Pipelines和Ludwig，以 overcome the challenges of democratizing AI.</li>
<li>results: 本研究的分析显示，自主主机选项、高可扩展性和开源性是普及人工智能的关键要求。此外，我们的方法比现有的AIaaS平台更加全面和有效地满足了普及人工智能的需求。<details>
<summary>Abstract</summary>
Recent AI research has significantly reduced the barriers to apply AI, but the process of setting up the necessary tools and frameworks can still be a challenge. While AI-as-a-Service platforms have emerged to simplify the training and deployment of AI models, they still fall short of achieving true democratization of AI. In this paper, we aim to address this gap by comparing several popular AI-as-a-Service platforms and identifying the key requirements for a platform that can achieve true democratization of AI. Our analysis highlights the need for self-hosting options, high scalability, and openness. To address these requirements, we propose our approach: the "Open Space for Machine Learning" platform. Our platform is built on cutting-edge technologies such as Kubernetes, Kubeflow Pipelines, and Ludwig, enabling us to overcome the challenges of democratizing AI. We argue that our approach is more comprehensive and effective in meeting the requirements of democratizing AI than existing AI-as-a-Service platforms.
</details>
<details>
<summary>摘要</summary>
现代人工智能研究已经大幅降低了应用人工智能的门槛，但是设置必要的工具和框架仍然是一个挑战。而AIaaS平台已经出现以简化训练和部署人工智能模型的过程，但它们仍然无法实现真正的人工智能民主化。在这篇论文中，我们想要解决这个差距，我们对多个流行的AIaaS平台进行比较，并确定了实现真正的民主化人工智能的关键需求。我们的分析表明，自主主机、可扩展性和开放性是必需的。为了解决这些需求，我们提出了我们的方法：“机器学习开放空间”平台。我们的平台基于最新的技术，如Kubernetes、Kubeflow Pipelines和Ludwig，使我们能够超越民主化人工智能的挑战。我们认为，我们的方法比现有的AIaaS平台更加全面和有效地满足了民主化人工智能的需求。
</details></li>
</ul>
<hr>
<h2 id="Strategies-for-Parallelizing-the-Big-Means-Algorithm-A-Comprehensive-Tutorial-for-Effective-Big-Data-Clustering"><a href="#Strategies-for-Parallelizing-the-Big-Means-Algorithm-A-Comprehensive-Tutorial-for-Effective-Big-Data-Clustering" class="headerlink" title="Strategies for Parallelizing the Big-Means Algorithm: A Comprehensive Tutorial for Effective Big Data Clustering"></a>Strategies for Parallelizing the Big-Means Algorithm: A Comprehensive Tutorial for Effective Big Data Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04517">http://arxiv.org/abs/2311.04517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravil Mussabayev, Rustam Mussabayev</li>
<li>for: 这个研究旨在优化大量数据集 clustering 的 Big-means 算法，探讨了四种不同的并行策略。</li>
<li>methods: 研究采用了多种并行策略，并进行了广泛的实验测试，以评估每种方法的计算效率、可扩展性和归一化性。</li>
<li>results: 研究发现了不同并行策略的优缺点，并分析了各种因素对归一化质量的影响。 这些发现可以为选择最佳并行策略提供实践性的指导。<details>
<summary>Abstract</summary>
This study focuses on the optimization of the Big-means algorithm for clustering large-scale datasets, exploring four distinct parallelization strategies. We conducted extensive experiments to assess the computational efficiency, scalability, and clustering performance of each approach, revealing their benefits and limitations. The paper also delves into the trade-offs between computational efficiency and clustering quality, examining the impacts of various factors. Our insights provide practical guidance on selecting the best parallelization strategy based on available resources and dataset characteristics, contributing to a deeper understanding of parallelization techniques for the Big-means algorithm.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "Mandarin" or "Guoyu".Translation notes:* "Big-means" is translated as "大均值算法" (dà jù yù suàn fǎ)* "clustering" is translated as "分类" (fēn xì)* "large-scale datasets" is translated as "大规模数据集" (dà xiàng móu dà tóu)* "parallelization strategies" is translated as "并行策略" (bèng xíng cè lü)* "computational efficiency" is translated as "计算效率" (jì suan xiǎng lü)* "scalability" is translated as "可扩展性" (kě kē zhòng xiǎng xìng)* "clustering performance" is translated as "分类性能" (fēn xì de yè ning)
</details></li>
</ul>
<hr>
<h2 id="Solution-of-FPK-Equation-for-Stochastic-Dynamics-Subjected-to-Additive-Gaussian-Noise-via-Deep-Learning-Approach"><a href="#Solution-of-FPK-Equation-for-Stochastic-Dynamics-Subjected-to-Additive-Gaussian-Noise-via-Deep-Learning-Approach" class="headerlink" title="Solution of FPK Equation for Stochastic Dynamics Subjected to Additive Gaussian Noise via Deep Learning Approach"></a>Solution of FPK Equation for Stochastic Dynamics Subjected to Additive Gaussian Noise via Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04511">http://arxiv.org/abs/2311.04511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir H. Khodabakhsh, Seid H. Pourtakdoust</li>
<li>For: 这个论文是为了解决高维度泊松方程（FPK equation）的解决方法，以及使用物理法律来塑造深度学习网络（FPK-DP Net）。* Methods: 该论文提出了一种名为FPK-DP Net的物理学习网络，该网络可以解决高维度泊松方程（FPK equation）的density演化问题，不需要任何先前的数据 simulate。* Results: 该论文通过对五个 benchmark 问题的数学实现来证明FPK-DP Net的准确性和效率。<details>
<summary>Abstract</summary>
The Fokker-Plank-Kolmogorov (FPK) equation is an idealized model representing many stochastic systems commonly encountered in the analysis of stochastic structures as well as many other applications. Its solution thus provides an invaluable insight into the performance of many engineering systems. Despite its great importance, the solution of the FPK equation is still extremely challenging. For systems of practical significance, the FPK equation is usually high dimensional, rendering most of the numerical methods ineffective. In this respect, the present work introduces the FPK-DP Net as a physics-informed network that encodes the physical insights, i.e. the governing constrained differential equations emanated out of physical laws, into a deep neural network. FPK-DP Net is a mesh-free learning method that can solve the density evolution of stochastic dynamics subjected to additive white Gaussian noise without any prior simulation data and can be used as an efficient surrogate model afterward. FPK-DP Net uses the dimension-reduced FPK equation. Therefore, it can be used to address high-dimensional practical problems as well. To demonstrate the potential applicability of the proposed framework, and to study its accuracy and efficacy, numerical implementations on five different benchmark problems are investigated.
</details>
<details>
<summary>摘要</summary>
《福克-普朗-科洛果罗夫（FPK）方程》是一种理想化的模型，表示了许多随机系统的分析，以及许多其他应用。其解决方案可以提供对许多工程系统的性能进行各种可观的了解。然而，尽管其重要性，FPK方程的解决仍然非常困难。实际应用中的FPK方程通常具有高维度，使得大多数数值方法无法应用。在这个意义上，当前的工作引入了FPK-DP网，这是一种嵌入物理知识的深度学习网络。FPK-DP网是一种不含网格的学习方法，可以在没有任何先 simulations 数据的情况下解决涉及加法白噪声的随机动力学性能演化。因此，它可以用于解决实际应用中的高维度问题。为了证明提案的框架的可应用性和精度，这里进行了五个不同的标准问题的数值实现。
</details></li>
</ul>
<hr>
<h2 id="Constrained-Adaptive-Attacks-Realistic-Evaluation-of-Adversarial-Examples-and-Robust-Training-of-Deep-Neural-Networks-for-Tabular-Data"><a href="#Constrained-Adaptive-Attacks-Realistic-Evaluation-of-Adversarial-Examples-and-Robust-Training-of-Deep-Neural-Networks-for-Tabular-Data" class="headerlink" title="Constrained Adaptive Attacks: Realistic Evaluation of Adversarial Examples and Robust Training of Deep Neural Networks for Tabular Data"></a>Constrained Adaptive Attacks: Realistic Evaluation of Adversarial Examples and Robust Training of Deep Neural Networks for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04503">http://arxiv.org/abs/2311.04503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thibault Simonetto, Salah Ghamizi, Antoine Desjardins, Maxime Cordy, Yves Le Traon</li>
<li>for: 本研究的目的是评估深度学习模型在表格数据上的抗击力，以及在不同攻击者能力水平下的抗击性能。</li>
<li>methods: 本研究提出了CAA，首个适用于受限表格深度学习模型的有效谋敌攻击。CAA 是一种具有迭代和搜索特性的参数自适应攻击，可以在约束下生成攻击示例。</li>
<li>results: 我们使用CAA 建立了表格深度学习模型的抗击性 benchmark，并在三个受欢迎的应用场景（信用评分、钓鱼攻击和 botnet 攻击响应）上进行了评估。我们的 benchmark 支持了十个威胁模型，每个模型都具有不同的攻击能力水平。我们的结果表明，预测环境、对抗训练和攻击预算等因素对深度表格模型的抗击性评估具有重要影响，并提供了一些安全培训的建议，以提高深度表格模型对不同谋敌攻击场景的抗击性能。<details>
<summary>Abstract</summary>
State-of-the-art deep learning models for tabular data have recently achieved acceptable performance to be deployed in industrial settings. However, the robustness of these models remains scarcely explored. Contrary to computer vision, there is to date no realistic protocol to properly evaluate the adversarial robustness of deep tabular models due to intrinsic properties of tabular data such as categorical features, immutability, and feature relationship constraints. To fill this gap, we propose CAA, the first efficient evasion attack for constrained tabular deep learning models. CAA is an iterative parameter-free attack that combines gradient and search attacks to generate adversarial examples under constraints. We leverage CAA to build a benchmark of deep tabular models across three popular use cases: credit scoring, phishing and botnet attacks detection. Our benchmark supports ten threat models with increasing capabilities of the attacker, and reflects real-world attack scenarios for each use case. Overall, our results demonstrate how domain knowledge, adversarial training, and attack budgets impact the robustness assessment of deep tabular models and provide security practitioners with a set of recommendations to improve the robustness of deep tabular models against various evasion attack scenarios.
</details>
<details>
<summary>摘要</summary>
现代深度学习模型在表格数据上已经达到了工业级别的性能，但是这些模型的Robustness（鲁棒性）仍然很少研究。与计算机视觉不同，到目前为止没有任何实际协议来评估深度表格模型的攻击Robustness，这是因为表格数据的内在特性，如分类特征、不可变性和特征关系约束。为了填补这个空白，我们提出了CAA，第一个高效的约束深度表格模型攻击方法。CAA是一种具有迭代和搜索特性的参数自由攻击方法，可以在约束下生成攻击示例。我们利用CAA来建立了深度表格模型的三大应用场景的benchmark：信用评估、钓鱼和恶意botnet攻击检测。我们的benchmark支持了十种威胁模型，每种威胁模型都有不同的攻击能力，能够反映现实世界的攻击enario。总的来说，我们的结果表明了domain知识、对抗训练和攻击预算对深度表格模型的Robustness评估产生了深观影响，并提供了一组建议，以提高深度表格模型对不同攻击enario的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Advanced-Aerial-Mobility-–-An-End-to-end-Autonomy-Framework-for-UAVs-and-Beyond"><a href="#Autonomous-Advanced-Aerial-Mobility-–-An-End-to-end-Autonomy-Framework-for-UAVs-and-Beyond" class="headerlink" title="Autonomous Advanced Aerial Mobility – An End-to-end Autonomy Framework for UAVs and Beyond"></a>Autonomous Advanced Aerial Mobility – An End-to-end Autonomy Framework for UAVs and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04472">http://arxiv.org/abs/2311.04472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sakshi Mishra, Praveen Palanisamy</li>
<li>for: 本研究旨在开拓全自动无人飞行器在交通领域的应用，包括城市空中交通、快递、监测等领域。</li>
<li>methods: 本文提出了一个扩展和可扩展的自主框架，包括感知、识别、规划和控制四个主要块，以实现全自动无人飞行器的飞行和任务执行。</li>
<li>results: 本文对多种应用场景进行了分析和评估，并探讨了多个自主飞行器群体操作和管理的挑战和机遇，以及自主飞行系统的测试、验证和证明问题。<details>
<summary>Abstract</summary>
Developing aerial robots that can both safely navigate and execute assigned mission without any human intervention - i.e., fully autonomous aerial mobility of passengers and goods - is the larger vision that guides the research, design, and development efforts in the aerial autonomy space. However, it is highly challenging to concurrently operationalize all types of aerial vehicles that are operating fully autonomously sharing the airspace. Full autonomy of the aerial transportation sector includes several aspects, such as design of the technology that powers the vehicles, operations of multi-agent fleets, and process of certification that meets stringent safety requirements of aviation sector. Thereby, Autonomous Advanced Aerial Mobility is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we present a comprehensive perspective on the emerging field of autonomous advanced aerial mobility, which involves the use of unmanned aerial vehicles (UAVs) and electric vertical takeoff and landing (eVTOL) aircraft for various applications, such as urban air mobility, package delivery, and surveillance. The article proposes a scalable and extensible autonomy framework consisting of four main blocks: sensing, perception, planning, and controls. Furthermore, the article discusses the challenges and opportunities in multi-agent fleet operations and management, as well as the testing, validation, and certification aspects of autonomous aerial systems. Finally, the article explores the potential of monolithic models for aerial autonomy and analyzes their advantages and limitations. The perspective aims to provide a holistic picture of the autonomous advanced aerial mobility field and its future directions.
</details>
<details>
<summary>摘要</summary>
发展可以安全地自动导航并完成任务无需人类干预的天空机器人，即完全自动天空 mobilit y是研究、设计和开发努力的大方向。然而，同时操作完全自动的所有天空车辆共享空间具有极高的挑战性。全自动天空交通部门的完整性包括技术 powers 车辆的设计、多 Agent 队列的运行和遵循严格的航空领域安全要求的证明过程。因此，自主高级天空 mobilit y仍然是一个抽象的概念，其对研究人员和专业人员的影响是模糊的。为了解决这个漏洞，我们提供了一篇全面的emerging field 自主高级天空 mobilit y的视角，该视角包括使用无人航空器（UAV）和电动升降起降机（eVTOL）机器人进行各种应用，如城市空中交通、快递和监测。文章提出了可扩展和可靠的自主框架，由四个主要块组成：感知、识别、规划和控制。此外，文章还讨论了多 Agent 队列操作和管理的挑战和机遇，以及自主天空系统的测试、验证和证明方面的问题。最后，文章探讨了monolithic 模型在天空自主性方面的优点和局限性。该视角旨在提供自主高级天空 mobilit y领域的总体图景和未来方向。
</details></li>
</ul>
<hr>
<h2 id="Solving-High-Frequency-and-Multi-Scale-PDEs-with-Gaussian-Processes"><a href="#Solving-High-Frequency-and-Multi-Scale-PDEs-with-Gaussian-Processes" class="headerlink" title="Solving High Frequency and Multi-Scale PDEs with Gaussian Processes"></a>Solving High Frequency and Multi-Scale PDEs with Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04465">http://arxiv.org/abs/2311.04465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikai Fang, Madison Cooley, Da Long, Shibo Li, Robert Kirby, Shandian Zhe</li>
<li>For: The paper aims to improve the accuracy and efficiency of solving partial differential equations (PDEs) using machine learning-based methods, specifically physics-informed neural networks (PINNs), by addressing the problem of spectral bias during training.* Methods: The authors resort to the Gaussian process (GP) framework and model the power spectrum of the PDE solution using a student t mixture or Gaussian mixture. They apply the inverse Fourier transform to obtain the covariance function and use the Jeffreys prior to estimate the mixture weights in the log domain. They also place collocation points on a grid and use the GP conditional mean to predict the solution and its derivatives.* Results: The authors show the advantage of their method in systematic experiments, including the ability to capture high-frequency and multi-scale PDEs without spectral bias, and the promotion of computational efficiency and scalability using Kronecker product properties and multilinear algebra.Here is the same information in Simplified Chinese:</li>
<li>for: 该文章目的是使用机器学习基于方法解决部分偏微分方程(PDEs)的精度和效率问题，特别是physics-informed neural networks (PINNs) 中的频谱偏迷问题。</li>
<li>methods: 作者们使用 Gaussian process (GP) 框架，模型部分偏微分方程解的能量谱，使用 student t 混合或 Gaussian 混合模型。他们应用反傅敛变换获取协方差函数，并使用 Jeffreys 先验来估算混合重量。</li>
<li>results: 作者们在系统性实验中展示了他们的方法的优势，包括不受频谱偏迷、高频和多尺度PDEs 的解决，以及使用 Kronecker 乘积性质和多线性代数提高计算效率和可扩展性。<details>
<summary>Abstract</summary>
Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during neural network training. To address this problem, we resort to the Gaussian process (GP) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the PDE solution with a student t mixture or Gaussian mixture. We then apply the inverse Fourier transform to obtain the covariance function (according to the Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture spectrum corresponds to the known spectral mixture kernel. We are the first to discover its rationale and effectiveness for PDE solving. Next,we estimate the mixture weights in the log domain, which we show is equivalent to placing a Jeffreys prior. It automatically induces sparsity, prunes excessive frequencies, and adjusts the remaining toward the ground truth. Third, to enable efficient and scalable computation on massive collocation points, which are critical to capture high frequencies, we place the collocation points on a grid, and multiply our covariance function at each input dimension. We use the GP conditional mean to predict the solution and its derivatives so as to fit the boundary condition and the equation itself. As a result, we can derive a Kronecker product structure in the covariance matrix. We use Kronecker product properties and multilinear algebra to greatly promote computational efficiency and scalability, without any low-rank approximations. We show the advantage of our method in systematic experiments.
</details>
<details>
<summary>摘要</summary>
《机器学习基于的解决方案在物理 simulations 和科学计算中备受推崇，例如物理 Informed Neural Networks (PINNs)。然而，PINNs 经常遇到高频和多核频率的 PDE 问题，这可能是在 neural network 训练过程中的 spectral bias。为解决这个问题，我们转而使用 Gaussian Process (GP) 框架。我们使用学生 t 混合或 Gaussian 混合来模型 PDE 解的能量谱，然后使用 inverse Fourier transform 获得 covariance function（根据 Wiener-Khinchin 定理）。这个 covariance 函数与知名的 spectral mixture kernel 相同。我们是第一个发现其理由和有效性，并且在 PDE 解中应用。接下来，我们在循环域中估算混合 веса，我们显示这等价于在 Jeffreys 前导下进行估算。这会自动强制简洁，抑制过分频率，并调整剩下的频率向真实值。三、我们使用 GP 的 conditional mean 来预测解和其导函数，以拟合边界条件和方程本身。因此，我们可以 derive 一个 Kronecker 乘积结构在 covariance 矩阵中。我们使用 Kronecker 乘积属性和多线性代数，不需要低级 approximation，可以大幅提高计算效率和可扩展性。我们在系统性实验中展示了我们的方法的优势。》Note: The translation is in Simplified Chinese, which is a standardized form of Chinese used in mainland China and widely used in education, media, and other formal contexts. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Spatial-Transformer-for-Massive-Point-Samples-in-Continuous-Space"><a href="#A-Hierarchical-Spatial-Transformer-for-Massive-Point-Samples-in-Continuous-Space" class="headerlink" title="A Hierarchical Spatial Transformer for Massive Point Samples in Continuous Space"></a>A Hierarchical Spatial Transformer for Massive Point Samples in Continuous Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04434">http://arxiv.org/abs/2311.04434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spatialdatasciencegroup/hst">https://github.com/spatialdatasciencegroup/hst</a></li>
<li>paper_authors: Wenchong He, Zhe Jiang, Tingsong Xiao, Zelin Xu, Shigang Chen, Ronald Fick, Miles Medina, Christine Angelini</li>
<li>For:  This paper is written for massive point samples in continuous space, which are common in environment sciences, numerical simulations, and location-based services. The author proposes a novel transformer model to address the challenges of long-range and multi-scale dependency, non-uniform point distribution, and high computational costs.* Methods: The proposed hierarchical spatial transformer model includes multi-resolution representation learning within a quad-tree hierarchy and efficient spatial attention via coarse approximation. The model also includes an uncertainty quantification branch to estimate prediction confidence related to input feature noise and point sparsity.* Results: The author provides a theoretical analysis of computational time complexity and memory costs. Extensive experiments on both real-world and synthetic datasets show that the proposed method outperforms multiple baselines in prediction accuracy and can scale up to one million points on one NVIDIA A100 GPU.Here’s the Chinese version of the information:* For: 这篇论文是为大量连续空间中的点样本而写的，这些样本广泛存在环境科学、数值仿真和位置基础服务等领域。作者提出了一种新的变换器模型，用于解决连续空间中点样本的长距离和多尺度关系、非均匀点分布和计算成本的问题。* Methods: 提出的层次空间变换器模型包括在Quad-tree层次结构中进行多尺度表示学习，以及高效的空间注意力计算方法。模型还包括一个不确定性评估分支，用于估计输入特征噪声和点缺失的影响。* Results: 作者提供了计算时间复杂度和内存成本的理论分析。对实际数据和 sintetic 数据进行了广泛的实验，结果显示，提出的方法在预测精度方面超过多个基elines，并且可以在一个NVIDIA A100 GPU上处理一百万个点。模型代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/spatialdatasciencegroup/HST%E3%80%82">https://github.com/spatialdatasciencegroup/HST。</a><details>
<summary>Abstract</summary>
Transformers are widely used deep learning architectures. Existing transformers are mostly designed for sequences (texts or time series), images or videos, and graphs. This paper proposes a novel transformer model for massive (up to a million) point samples in continuous space. Such data are ubiquitous in environment sciences (e.g., sensor observations), numerical simulations (e.g., particle-laden flow, astrophysics), and location-based services (e.g., POIs and trajectories). However, designing a transformer for massive spatial points is non-trivial due to several challenges, including implicit long-range and multi-scale dependency on irregular points in continuous space, a non-uniform point distribution, the potential high computational costs of calculating all-pair attention across massive points, and the risks of over-confident predictions due to varying point density. To address these challenges, we propose a new hierarchical spatial transformer model, which includes multi-resolution representation learning within a quad-tree hierarchy and efficient spatial attention via coarse approximation. We also design an uncertainty quantification branch to estimate prediction confidence related to input feature noise and point sparsity. We provide a theoretical analysis of computational time complexity and memory costs. Extensive experiments on both real-world and synthetic datasets show that our method outperforms multiple baselines in prediction accuracy and our model can scale up to one million points on one NVIDIA A100 GPU. The code is available at \url{https://github.com/spatialdatasciencegroup/HST}.
</details>
<details>
<summary>摘要</summary>
启用变换器是深度学习架构的广泛应用。现有的变换器主要是为文本、图像、视频和图表设计的。这篇论文提出了一种新的变换器模型，用于处理大量（达到百万）的点样本，这些样本在连续空间中分布。这些样本广泛存在环境科学（例如感知器观测）、数学模拟（例如带有粒子的流体和天文学）以及地理位置服务（例如POI和轨迹）等领域。然而，设计为大量点样本的变换器是非常困难的，因为这些样本存在许多挑战，包括隐式的长距离和多尺度相互关系，分布不均匀，计算所有对之间的注意力计算的可能高计算成本，以及因点密度变化而导致的预测结果过于自信。为了解决这些挑战，我们提出了一种新的层次空间变换器模型，包括多尺度表示学习在四个树层中和高效的空间注意力via粗略估计。我们还设计了一个不确定量计算分支，用于估计输入特征噪声和点稀缺所导致的预测信度。我们提供了计算时间复杂度和内存成本的理论分析。在实际实验中，我们的方法在多个实际和 sintetic 数据集上取得了较高的预测精度，并且我们的模型可以在一个NVIDIA A100 GPU上处理一百万点。代码可以在 \url{https://github.com/spatialdatasciencegroup/HST} 上获得。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Emerging-AI-ML-Accelerators-IPU-RDU-and-NVIDIA-AMD-GPUs"><a href="#Evaluating-Emerging-AI-ML-Accelerators-IPU-RDU-and-NVIDIA-AMD-GPUs" class="headerlink" title="Evaluating Emerging AI&#x2F;ML Accelerators: IPU, RDU, and NVIDIA&#x2F;AMD GPUs"></a>Evaluating Emerging AI&#x2F;ML Accelerators: IPU, RDU, and NVIDIA&#x2F;AMD GPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04417">http://arxiv.org/abs/2311.04417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongwu Peng, Caiwen Ding, Tong Geng, Sutanay Choudhury, Kevin Barker, Ang Li<br>for:这些商业AI&#x2F;ML加速器的设计和实现是为了满足现代AI&#x2F;ML算法的增加复杂度和计算需求而创造的，以提高AI&#x2F;ML任务的性能和能效性。methods:这些加速器的各种设计优化和数据流体系结构使得它们在AI&#x2F;ML任务中表现出色，包括Graphcore Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit (RDU) 和改进的GPU平台。results:这些加速器在常见的DNN运算符和其他AI&#x2F;ML任务上的性能评估和比较，以阐明数据流体系的优势和传统处理器设计的缺陷，并提供了每个平台的性能交易。这些发现可以作为研发下一代加速器的参考，以满足AI&#x2F;ML应用领域不断演化的需求。<details>
<summary>Abstract</summary>
The relentless advancement of artificial intelligence (AI) and machine learning (ML) applications necessitates the development of specialized hardware accelerators capable of handling the increasing complexity and computational demands. Traditional computing architectures, based on the von Neumann model, are being outstripped by the requirements of contemporary AI/ML algorithms, leading to a surge in the creation of accelerators like the Graphcore Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit (RDU), and enhanced GPU platforms. These hardware accelerators are characterized by their innovative data-flow architectures and other design optimizations that promise to deliver superior performance and energy efficiency for AI/ML tasks.   This research provides a preliminary evaluation and comparison of these commercial AI/ML accelerators, delving into their hardware and software design features to discern their strengths and unique capabilities. By conducting a series of benchmark evaluations on common DNN operators and other AI/ML workloads, we aim to illuminate the advantages of data-flow architectures over conventional processor designs and offer insights into the performance trade-offs of each platform. The findings from our study will serve as a valuable reference for the design and performance expectations of research prototypes, thereby facilitating the development of next-generation hardware accelerators tailored for the ever-evolving landscape of AI/ML applications. Through this analysis, we aspire to contribute to the broader understanding of current accelerator technologies and to provide guidance for future innovations in the field.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）和机器学习（ML）应用的不断发展需要特化的硬件加速器来处理不断增长的复杂性和计算需求。传统的计算架构，基于 von Neumann 模型，被当代 AI/ML 算法的要求所超越，导致加速器的创造，如 Graphcore 智能处理器（IPU）、Sambanova 可重新配置数据流处理器（RDU）和增强 GPU 平台。这些硬件加速器具有创新的数据流架构和其他设计优化，以提供对 AI/ML 任务的超越性性能和能效性。本研究提供了这些商业 AI/ML 加速器的初步评估和比较，探讨其硬件和软件设计特点，以确定它们的优势和特殊能力。通过对常见深度神经网络（DNN）操作和其他 AI/ML 工作负荷进行 benchmark 评估，我们希望通过探讨数据流架构的优势和传统处理器设计的缺陷，为研究人员提供参考，以便设计和实现下一代特化于 AI/ML 应用的硬件加速器。我们的研究结果将对广泛的硬件加速器技术产生影响，并为未来在这个领域的创新提供指导。
</details></li>
</ul>
<hr>
<h2 id="Likelihood-Ratio-Confidence-Sets-for-Sequential-Decision-Making"><a href="#Likelihood-Ratio-Confidence-Sets-for-Sequential-Decision-Making" class="headerlink" title="Likelihood Ratio Confidence Sets for Sequential Decision Making"></a>Likelihood Ratio Confidence Sets for Sequential Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04402">http://arxiv.org/abs/2311.04402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Emmenegger, Mojmír Mutný, Andreas Krause</li>
<li>for: 这篇论文旨在提供一种可证明的、适应性 uncertainty 估计方法，用于Sequential Decision-Making 算法。</li>
<li>methods: 该方法基于 likelihood-based inference principle，使用 likelihood ratios 构建 any-time 有效 confidence sequences，不需要特殊的应用场景处理。</li>
<li>results: 该方法适用于具有明确 likelihood 函数的问题，其 resulting sets 总是保持预设的覆盖率，并且可以在 model-agnostic 的方式下实现。 estimators 的选择可以通过 provable 的方式来确定，而且与 online convex optimization 算法相关，如 Follow-the-Regularized-Leader。 更重要的是，该方法可以在 non-parametric 设置中使用，如 RKHS 函数类型。<details>
<summary>Abstract</summary>
Certifiable, adaptive uncertainty estimates for unknown quantities are an essential ingredient of sequential decision-making algorithms. Standard approaches rely on problem-dependent concentration results and are limited to a specific combination of parameterization, noise family, and estimator. In this paper, we revisit the likelihood-based inference principle and propose to use likelihood ratios to construct any-time valid confidence sequences without requiring specialized treatment in each application scenario. Our method is especially suitable for problems with well-specified likelihoods, and the resulting sets always maintain the prescribed coverage in a model-agnostic manner. The size of the sets depends on a choice of estimator sequence in the likelihood ratio. We discuss how to provably choose the best sequence of estimators and shed light on connections to online convex optimization with algorithms such as Follow-the-Regularized-Leader. To counteract the initially large bias of the estimators, we propose a reweighting scheme that also opens up deployment in non-parametric settings such as RKHS function classes. We provide a non-asymptotic analysis of the likelihood ratio confidence sets size for generalized linear models, using insights from convex duality and online learning. We showcase the practical strength of our method on generalized linear bandit problems, survival analysis, and bandits with various additive noise distributions.
</details>
<details>
<summary>摘要</summary>
certificable, adaptive uncertainty estimates for unknown quantities are an essential ingredient of sequential decision-making algorithms. Standard approaches rely on problem-dependent concentration results and are limited to a specific combination of parameterization, noise family, and estimator. In this paper, we revisit the likelihood-based inference principle and propose to use likelihood ratios to construct any-time valid confidence sequences without requiring specialized treatment in each application scenario. Our method is especially suitable for problems with well-specified likelihoods, and the resulting sets always maintain the prescribed coverage in a model-agnostic manner. The size of the sets depends on a choice of estimator sequence in the likelihood ratio. We discuss how to provably choose the best sequence of estimators and shed light on connections to online convex optimization with algorithms such as Follow-the-Regularized-Leader. To counteract the initially large bias of the estimators, we propose a reweighting scheme that also opens up deployment in non-parametric settings such as RKHS function classes. We provide a non-asymptotic analysis of the likelihood ratio confidence sets size for generalized linear models, using insights from convex duality and online learning. We showcase the practical strength of our method on generalized linear bandit problems, survival analysis, and bandits with various additive noise distributions.Here's the translation in Traditional Chinese: certificable, adaptive uncertainty estimates for unknown quantities are an essential ingredient of sequential decision-making algorithms. Standard approaches rely on problem-dependent concentration results and are limited to a specific combination of parameterization, noise family, and estimator. In this paper, we revisit the likelihood-based inference principle and propose to use likelihood ratios to construct any-time valid confidence sequences without requiring specialized treatment in each application scenario. Our method is especially suitable for problems with well-specified likelihoods, and the resulting sets always maintain the prescribed coverage in a model-agnostic manner. The size of the sets depends on a choice of estimator sequence in the likelihood ratio. We discuss how to provably choose the best sequence of estimators and shed light on connections to online convex optimization with algorithms such as Follow-the-Regularized-Leader. To counteract the initially large bias of the estimators, we propose a reweighting scheme that also opens up deployment in non-parametric settings such as RKHS function classes. We provide a non-asymptotic analysis of the likelihood ratio confidence sets size for generalized linear models, using insights from convex duality and online learning. We showcase the practical strength of our method on generalized linear bandit problems, survival analysis, and bandits with various additive noise distributions.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/cs.LG_2023_11_08/" data-id="clpxp6c5600usee88248j5qdc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/eess.IV_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T09:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/08/eess.IV_2023_11_08/">eess.IV - 2023-11-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-End-Cloud-Computing-Enabled-Surveillance-Video-Transmission-System"><a href="#An-End-Cloud-Computing-Enabled-Surveillance-Video-Transmission-System" class="headerlink" title="An End-Cloud Computing Enabled Surveillance Video Transmission System"></a>An End-Cloud Computing Enabled Surveillance Video Transmission System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04685">http://arxiv.org/abs/2311.04685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dingxi Yang, Zhijin Qin, Liting Wang, Xiaoming Tao, Fang Cui, Hengjiang Wang</li>
<li>for: 提高Surveillance video transmission efficiency and quality</li>
<li>methods: 使用终端云计算、压缩视频、重复帧除法、键帧支持视频超分辨模型</li>
<li>results: 可以效果地减少数据量，并与现有视频超分辨模型相比，有较高的PSNR和SSIM表现<details>
<summary>Abstract</summary>
The enormous data volume of video poses a significant burden on the network. Particularly, transferring high-definition surveillance videos to the cloud consumes a significant amount of spectrum resources. To address these issues, we propose a surveillance video transmission system enabled by end-cloud computing. Specifically, the cameras actively down-sample the original video and then a redundant frame elimination module is employed to further reduce the data volume of surveillance videos. Then we develop a key-frame assisted video super-resolution model to reconstruct the high-quality video at the cloud side. Moreover, we propose a strategy of extracting key frames from source videos for better reconstruction performance by utilizing the peak signal-to-noise ratio (PSNR) of adjacent frames to measure the propagation distance of key frame information. Simulation results show that the developed system can effectively reduce the data volume by the end-cloud collaboration and outperforms existing video super-resolution models significantly in terms of PSNR and structural similarity index (SSIM).
</details>
<details>
<summary>摘要</summary>
“巨量数据的视频带来了网络的巨大压力。特别是将高清护视频传输到云存储所消耗的频谱资源非常大。为解决这些问题，我们提议一种基于终端云计算的视频传输系统。具体来说，摄像头会活动下amples the original video，然后使用重复帧除Module further reduce the amount of data volume of surveillance videos。然后，我们开发了一种基于关键帧的视频超Resolution模型，在云端进行高质量视频重建。此外，我们提出了利用相邻帧PSNR来度量关键帧信息的传播距离的策略。实验结果表明，我们开发的系统可以通过终端云合作Effectively reduce the amount of data and outperform existing video super-resolution models in terms of PSNR and structural similarity index (SSIM).”
</details></li>
</ul>
<hr>
<h2 id="A-human-brain-atlas-of-chi-separation-for-normative-iron-and-myelin-distributions"><a href="#A-human-brain-atlas-of-chi-separation-for-normative-iron-and-myelin-distributions" class="headerlink" title="A human brain atlas of chi-separation for normative iron and myelin distributions"></a>A human brain atlas of chi-separation for normative iron and myelin distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04468">http://arxiv.org/abs/2311.04468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyeongseon Min, Beomseok Sohn, Woo Jung Kim, Chae Jung Park, Soohwa Song, Dong Hoon Shin, Kyung Won Chang, Na-Young Shin, Minjun Kim, Hyeong-Geol Shin, Phil Hyu Lee, Jongho Lee</li>
<li>for: 这项研究的目的是构建一个健康人脑中 paramagnetic iron 和 diamagnetic myelin 的分离 Atlases，以便在脑部分 realizations 中提供细致的结构准确地呈现出脑部分的分布。</li>
<li>methods: 这项研究使用了 chi-separation 技术，成功地分离出 paramagnetic iron 和 diamagnetic myelin，并构建了一个 normative chi-separation atlas 从 106 个健康人脑中。</li>
<li>results: 研究得到了详细的 anatomical structures 相关 paramagnetic iron 和 diamagnetic myelin 的分布，清晰地呈现出脑部分的各个核心和白 matter 纤维 bundle。 此外，研究还发现了一些Region of interest 的 susceptibility values 随着年龄的变化。这个 atlas 可能有直接应用，例如在深 brain stimulation 或高Intensity focused ultrasound 中用于定位脑部分，并且可以作为未来研究的 valuable resource。<details>
<summary>Abstract</summary>
Iron and myelin are primary susceptibility sources in the human brain. These substances are essential for healthy brain, and their abnormalities are often related to various neurological disorders. Recently, an advanced susceptibility mapping technique, which is referred to as chi-separation, has been proposed successfully disentangling paramagnetic iron from diamagnetic myelin, opening a new potential for generating iron map and myelin map in the brain. Utilizing this technique, this study constructs a normative chi-separation atlas from 106 healthy human brains. The resulting atlas provides detailed anatomical structures associated with the distributions of iron and myelin, clearly delineating subcortical nuclei and white matter fiber bundles. Additionally, susceptibility values in a number of regions of interest are reported along with age-dependent changes. This atlas may have direct applications such as localization of subcortical structures for deep brain stimulation or high-intensity focused ultrasound and also serve as a valuable resource for future research.
</details>
<details>
<summary>摘要</summary>
铁和脑膜是人脑中主要的抵抗源。这些物质是健康脑的关键组成部分，其异常会与多种神经系统疾病相关。最近，一种高级别的抵抗映射技术，即chi-separation，已经成功地分离了 paramagnetic 铁和 diamagnetic 脑膜，打开了一个新的可能性，即生成铁图和脑膜图在脑中。利用这种技术，本研究建立了106名健康人脑中的 normative chi-separation  Atlason。结果提供了详细的 анатомиче结构，与铁和脑膜分布相关，清晰地分割了脑下核和白 matter 纤维带。此外，在一些关注区域中，抵抗值也被报告，以及与年龄相关的变化。这个 Atlason 可能有直接应用，如深入脑刺激或高 интенсиTY focused ultrasound 的localization，也可以作为未来研究的 valuable 资源。
</details></li>
</ul>
<hr>
<h2 id="A-labeled-Clinical-MRI-dataset-of-Nigerian-brains"><a href="#A-labeled-Clinical-MRI-dataset-of-Nigerian-brains" class="headerlink" title="A labeled Clinical-MRI dataset of Nigerian brains"></a>A labeled Clinical-MRI dataset of Nigerian brains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04425">http://arxiv.org/abs/2311.04425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bacaron/nigerian_brain_analyses">https://github.com/bacaron/nigerian_brain_analyses</a></li>
<li>paper_authors: Eberechi Wogu, Patrick Filima, Bradley Caron, Daniel Levitas, Peer Herholz, Catherine Leal, Mohammed F. Mehboob, Soichi Hayashi, Simisola Akintoye, George Ogoh, Tawe Godwin, Damian Eke, Franco Pestilli</li>
<li>for: The paper is written for the purpose of describing a magnetic resonance imaging (MRI) dataset from individuals in Nigeria, with the goal of contributing to the global neuroscience community and providing a benchmark for future studies.</li>
<li>methods: The paper uses pseudonymized structural MRI (T1w, T2w, FLAIR) data of clinical quality, containing data from 36 healthy control subjects, 32 individuals with age-related dementia, and 20 individuals with Parkinson’s disease.</li>
<li>results: The paper presents a dataset of MRI data from individuals in Nigeria, which is currently underrepresented in the global neuroscience community. The dataset provides an opportunity and benchmark for future studies to share data from the African continent.<details>
<summary>Abstract</summary>
We describe a Magnetic Resonance Imaging (MRI) dataset from individuals from the African nation of Nigeria. The dataset contains pseudonymized structural MRI (T1w, T2w, FLAIR) data of clinical quality. The dataset contains data from 36 images from healthy control subjects, 32 images from individuals diagnosed with age-related dementia and 20 from individuals with Parkinson's disease. There is currently a paucity of data from the African continent. Given the potential for Africa to contribute to the global neuroscience community, this first MRI dataset represents both an opportunity and benchmark for future studies to share data from the African continent.
</details>
<details>
<summary>摘要</summary>
我们描述了一个来自非洲国家奈及利亚的磁共振成像（MRI）数据集。该数据集包含匿名化的结构MRI（T1w、T2w、FLAIR）数据，质量很高。数据集包含36张健康控制群体的图像，32张年龄相关失忆症患者的图像和20张parkinson病患者的图像。目前非洲 continent上没有很多数据，这个首个MRI数据集标志着非洲大陆对全球神经科学社区的贡献的可能性和标准。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/eess.IV_2023_11_08/" data-id="clpxp6ccf01d7ee881rsegwg8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/eess.SP_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T08:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/08/eess.SP_2023_11_08/">eess.SP - 2023-11-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Constrained-Independent-Vector-Analysis-with-Reference-for-Multi-Subject-fMRI-Analysis"><a href="#Constrained-Independent-Vector-Analysis-with-Reference-for-Multi-Subject-fMRI-Analysis" class="headerlink" title="Constrained Independent Vector Analysis with Reference for Multi-Subject fMRI Analysis"></a>Constrained Independent Vector Analysis with Reference for Multi-Subject fMRI Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05049">http://arxiv.org/abs/2311.05049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trung Vu, Francisco Laport, Hanlu Yang, Vince D. Calhoun, Tulay Adali</li>
<li>for:  This paper proposes two novel methods for constrained independent vector analysis (ICA) to improve the quality of separation in multi-subject functional magnetic resonance imaging (fMRI) data analysis.</li>
<li>methods: The two proposed methods are based on an adaptive-reverse scheme to select variable thresholds for the constraints and a threshold-free formulation by leveraging the unique structure of IVA.</li>
<li>results: The proposed methods provide significantly better separation quality and model match while providing computationally efficient and highly reproducible solutions, as demonstrated through simulations and analysis of resting state fMRI data collected from 98 subjects.<details>
<summary>Abstract</summary>
Independent component analysis (ICA) is now a widely used solution for the analysis of multi-subject functional magnetic resonance imaging (fMRI) data. Independent vector analysis (IVA) generalizes ICA to multiple datasets, i.e., to multi-subject data, and in addition to higher-order statistical information in ICA, it leverages the statistical dependence across the datasets as an additional type of statistical diversity. As such, it preserves variability in the estimation of single-subject maps but its performance might suffer when the number of datasets increases. Constrained IVA is an effective way to bypass computational issues and improve the quality of separation by incorporating available prior information. Existing constrained IVA approaches often rely on user-defined threshold values to define the constraints. However, an improperly selected threshold can have a negative impact on the final results. This paper proposes two novel methods for constrained IVA: one using an adaptive-reverse scheme to select variable thresholds for the constraints and a second one based on a threshold-free formulation by leveraging the unique structure of IVA. We demonstrate that our solutions provide an attractive solution to multi-subject fMRI analysis both by simulations and through analysis of resting state fMRI data collected from 98 subjects -- the highest number of subjects ever used by IVA algorithms. Our results show that both proposed approaches obtain significantly better separation quality and model match while providing computationally efficient and highly reproducible solutions.
</details>
<details>
<summary>摘要</summary>
独立组分分析（ICA）已成为多subject功能磁共振成像（fMRI）数据分析的广泛使用解决方案。独立向量分析（IVA）总结ICA，并在多个数据集之间启用统计依赖关系作为额外的统计多样性。因此，它保留单个数据集的变化，但可能在数据集的数量增加时表现不佳。受限制的IVA是一种有效的解决方案，通过包含可用的先前信息来减少计算问题并提高分离质量。现有的受限制IVA方法通常需要用户定义的阈值来定义约束。然而，不当选择的阈值可能会对最终结果产生负面影响。本文提出了两种新的受限制IVA方法：一种使用逆向的变量阈值选择约束，并一种基于阈值自由形式，通过利用IVA的特殊结构来减少计算问题。我们的研究表明，我们的解决方案可以在多subject fMRI数据中提供更好的分离质量和模型匹配，同时提供计算效率和高度可重现的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Harmonic-Retrieval-Using-Weighted-Lifted-Structure-Low-Rank-Matrix-Completion"><a href="#Harmonic-Retrieval-Using-Weighted-Lifted-Structure-Low-Rank-Matrix-Completion" class="headerlink" title="Harmonic Retrieval Using Weighted Lifted-Structure Low-Rank Matrix Completion"></a>Harmonic Retrieval Using Weighted Lifted-Structure Low-Rank Matrix Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05003">http://arxiv.org/abs/2311.05003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Bokaei, Saeed Razavikia, Stefano Rini, Arash Amini, Hamid Behrouzi</li>
<li>for: 这个论文研究了一种复杂的时域频率分析问题，即从一个随机选择的时域样本中提取混合的 $K$ 个复杂正弦波的频率成分。</li>
<li>methods: 作者提出了一种两步策略，包括将受限样本集 lift 成一个有 missing 的结构矩阵，然后使用一个Weighted nuclear minimization问题来完善矩阵。</li>
<li>results: 作者的方法可以应用于各种矩阵结构，如汉ке尔和双汉ке尔矩阵，并且在不含噪声的情况下和噪声的情况下都有较好的表现，并且有理论保证。<details>
<summary>Abstract</summary>
In this paper, we investigate the problem of recovering the frequency components of a mixture of $K$ complex sinusoids from a random subset of $N$ equally-spaced time-domain samples. Because of the random subset, the samples are effectively non-uniform. Besides, the frequency values of each of the $K$ complex sinusoids are assumed to vary continuously within a given range.   For this problem, we propose a two-step strategy: (i) we first lift the incomplete set of uniform samples (unavailable samples are treated as missing data) into a structured matrix with missing entries, which is potentially low-rank; then (ii) we complete the matrix using a weighted nuclear minimization problem. We call the method a \emph{ weighted lifted-structured (WLi) low-rank matrix recovery}. Our approach can be applied to a range of matrix structures such as Hankel and double-Hankel, among others, and provides improvement over the unweighted existing schemes such as EMaC and DEMaC. We provide theoretical guarantees for the proposed method, as well as numerical simulations in both noiseless and noisy settings. Both the theoretical and the numerical results confirm the superiority of the proposed approach.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了一个复杂的混合信号恢复问题，即从一个随机选择的 $N$ 个时域样本中恢复 $K$ 个复杂的声学信号的频率组分。由于随机选择的样本，实际上是非均匀的。此外，每个 $K$ 个声学信号的频率值假设是连续变化在给定的范围内。为解决这个问题，我们提出了一种两步策略：1. 首先，我们将缺失样本提升成一个结构化的缺失数据矩阵，这个矩阵具有缺失的元素，可能是低级的。2. 然后，我们使用一个权重 nuclear minimization 问题来完善该矩阵。我们称之为权重提升结构（WLi）低级矩阵恢复。我们的方法可以应用于各种矩阵结构，如汉KEL和双汉KEL等，并且比不Weighted existing schemes such as EMaC and DEMaC 提供更好的性能。我们提供了理论保证，以及在无噪和噪存在的情况下的数学实验。两者都证明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Joint-Transmit-Signal-and-Beamforming-Design-for-Integrated-Sensing-and-Power-Transfer-Systems"><a href="#Joint-Transmit-Signal-and-Beamforming-Design-for-Integrated-Sensing-and-Power-Transfer-Systems" class="headerlink" title="Joint Transmit Signal and Beamforming Design for Integrated Sensing and Power Transfer Systems"></a>Joint Transmit Signal and Beamforming Design for Integrated Sensing and Power Transfer Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04881">http://arxiv.org/abs/2311.04881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kenneth MacSporran Mayer, Nikita Shanin, Zhenlong You, Sebastian Lotter, Stefan Brückner, Martin Vossiek, Laura Cottatellucci, Robert Schober</li>
<li>for: 这个论文主要是为了提高感知和无线能量传输（WPT）功能的集成系统。</li>
<li>methods: 该论文提出了将探测和WPT功能集成到单一平台上，并且对各种功能进行共同优化。它采用矩形冲激信号和扫描方向的共同优化，以提高感知和WPT的效率。与先前的研究不同，该论文采用了准确的非线性电能收集（EH）模型。</li>
<li>results: 论文通过Grid搜索、semidefinite relaxation（SDR）和successive convex approximation（SCA）等方法解决了一个非对称优化问题，并证明了在大于平均发射功率的情况下，各个接收器的平均收集能量 monotonicity 增长 With 探测目标（ST）的扫描方向。同时，该论文还证明了ISAPT系统中感知性和能量传输之间的贸易OFF。<details>
<summary>Abstract</summary>
Integrating different functionalities, conventionally implemented as dedicated systems, into a single platform allows utilising the available resources more efficiently. We consider an integrated sensing and power transfer (ISAPT) system and propose the joint optimisation of the rectangular pulse-shaped transmit signal and the beamforming design to combine sensing and wireless power transfer (WPT) functionalities efficiently. In contrast to prior works, we adopt an accurate non-linear circuit-based energy harvesting (EH) model. We formulate a non-convex optimisation problem for a general number of EH receivers and a single sensing target (ST) and solve the problem via a grid search over the pulse duration, semidefinite relaxation (SDR), and successive convex approximation (SCA). The average harvested power is shown to monotonically increase with the pulse duration when the average transmit power budget is large. We discuss the trade-off between sensing performance and power transfer of the ISAPT system. The proposed approach significantly outperforms a heuristic baseline scheme based on a linear EH model, which linearly combines energy beamforming with the beamsteering vector in the direction to the ST as its transmit strategy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Electromagnetic-manifold-characterization-of-antenna-arrays"><a href="#Electromagnetic-manifold-characterization-of-antenna-arrays" class="headerlink" title="Electromagnetic manifold characterization of antenna arrays"></a>Electromagnetic manifold characterization of antenna arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04835">http://arxiv.org/abs/2311.04835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel R. Castellanos, Robert W. Heath Jr</li>
<li>for: 本研究旨在提供一种能够考虑antenna behaviors的信号和通道模型，以便提高无线通信系统的性能。</li>
<li>methods: 本研究使用电romagnetic基础来建立一个array manifold，该 manifold可以考虑多种复杂的antenna behaviors，并且可以模型任意antenna配置。通过尺度化antenna为大量的勃氏 dipolo，我们可以开发一个用于预测非homogeneous array场的模型。</li>
<li>results: 我们的numerical result表明，使用该模型可以实现高度的 beamforming gain优化，并且可以考虑 polarization of the receive field以及radiated power density的约束。系统可以通过利用该array manifold来实现更高的 beamforming gain，相比之下使用较准的模型进行 beamforming。<details>
<summary>Abstract</summary>
Antenna behaviors such as mutual coupling, near-field propagation, and polarization cannot be neglected in signal and channel models for wireless communication. We present an electromagnetic-based array manifold that accounts for several complicated behaviors and can model arbitrary antenna configurations. We quantize antennas into a large number of Hertzian dipoles to develop a model for the radiated array field. The resulting abstraction provides a means to predict the electric field for general non-homogeneous array geometries through a linear model that depends on the point source location, the position of each Hertzian dipole, and a set of coefficients obtained from electromagnetic simulation. We then leverage this model to formulate a beamforming gain optimization that can be adapted to account for polarization of the receive field as well as constraints on the radiated power density. Numerical results demonstrate that the proposed method achieves accuracy that is close to that of electromagnetic simulations. By leveraging the developed array manifold for beamforming, systems can achieve higher beamforming gains compared to beamforming with less accurate models.
</details>
<details>
<summary>摘要</summary>
天线行为如共振、近场传播和极化不能被忽略在无线通信中的信号和通道模型中。我们提出一个电磁场基础的阵列构造，考虑到多种复杂的行为，可以模型任意天线配置。我们将天线量化为一大量的赫兹点短波天线，开发了一个模型以预测通过天线阵列的电磁场。这个抽象提供了一个可以预测通过非均匀天线配置的电磁场的线性模型，这个模型取决于天线点源位置、每个赫兹点天线的位置以及从电磁 simulations 中获得的一组系数。我们然后利用这个模型，实现了一个对极化 receive 场进行最佳化的焦点增强，并且可以根据射频场的极化和射频功率密度的限制进行最佳化。 numerics  результалтати显示，提案的方法具有与电磁 simulations 的准确性相似的精度。通过利用开发的阵列构造进行焦点增强，系统可以在焦点增强方面比使用较不精度的模型取得更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Distributed-Semantic-Communication-and-Over-the-air-Computation-for-Cooperative-Spectrum-Sensing"><a href="#Integrated-Distributed-Semantic-Communication-and-Over-the-air-Computation-for-Cooperative-Spectrum-Sensing" class="headerlink" title="Integrated Distributed Semantic Communication and Over-the-air Computation for Cooperative Spectrum Sensing"></a>Integrated Distributed Semantic Communication and Over-the-air Computation for Cooperative Spectrum Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04791">http://arxiv.org/abs/2311.04791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Yi, Yang Cao, Xin Kang, Ying-Chang Liang</li>
<li>for: 提高第一用户（PU）的探测，使用多个感知器。</li>
<li>methods: 提出一种新的整合通信计算（ICC）框架，即分布式 semantics 通信（DSC）和空气计算（AirComp），以提高探测性能和减少spectrum占用。</li>
<li>results: 在ICC框架下，实现了一种名为ICC-CSS的特定系统，对于独立同分布的PU信号样本，理论上证明与优化探测器-相关器（E-C）探测器相等。在各种传统CSS方案中，ICC-CSS在检测性能、频率干扰抗干扰和检测器稳定性方面表现出优异性，并且可扩展性良好。<details>
<summary>Abstract</summary>
Cooperative spectrum sensing (CSS) is a promising approach to improve the detection of primary users (PUs) using multiple sensors. However, there are several challenges for existing combination methods, i.e., performance degradation and ceiling effect for hard-decision fusion (HDF), as well as significant uploading latency and non-robustness to noise in the reporting channel for soft-data fusion (SDF). To address these issues, in this paper, we propose a novel framework for CSS that integrates communication and computation, namely ICC. Specifically, distributed semantic communication (DSC) jointly optimizes multiple sensors and the fusion center to minimize the transmitted data without degrading detection performance. Moreover, over-the-air computation (AirComp) is utilized to further reduce spectrum occupation in the reporting channel, taking advantage of the characteristics of the wireless channel to enable data aggregation. Under the ICC framework, a particular system, namely ICC-CSS, is designed and implemented, which is theoretically proved to be equivalent to the optimal estimator-correlator (E-C) detector with equal gain SDF when the PU signal samples are independent and identically distributed. Extensive simulations verify the superiority of ICC-CSS compared with various conventional CSS schemes in terms of detection performance, robustness to SNR variations in both the sensing and reporting channels, as well as scalability with respect to the number of samples and sensors.
</details>
<details>
<summary>摘要</summary>
合作频率感知（CSS）是一种有前途的方法，以提高 PRIMARY USER（PU）的探测，使用多个感知器。然而，现有的组合方法存在一些挑战，例如性能下降和层次效应（HDF）的硬件决策整合，以及报告通道中的上传延迟和阈值噪声（SDF）。为了解决这些问题，在这篇论文中，我们提出了一种新的CSS框架，即ICC。具体来说，分布式 semantics 通信（DSC）与多个感知器和整合中心进行共同优化，以最小化发送的数据量，而不是影响探测性能。此外，在报告通道中使用无线计算（AirComp），以进一步减少频率占用，利用无线通信的特性来实现数据聚合。在ICC框架下，我们设计了一个具体的系统，即ICC-CSS，该系统理论上与完美的统计量子探测器（E-C）相等，当PU信号样本独立同分布时。我们对ICC-CSS与各种传统CSS方案进行了广泛的实验，并证明了其在探测性能、频率响应度、报告通道噪声等方面的优越性。
</details></li>
</ul>
<hr>
<h2 id="Energy-efficient-Wireless-Image-Retrieval-for-IoT-Devices-by-Transmitting-a-TinyML-Model"><a href="#Energy-efficient-Wireless-Image-Retrieval-for-IoT-Devices-by-Transmitting-a-TinyML-Model" class="headerlink" title="Energy-efficient Wireless Image Retrieval for IoT Devices by Transmitting a TinyML Model"></a>Energy-efficient Wireless Image Retrieval for IoT Devices by Transmitting a TinyML Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04788">http://arxiv.org/abs/2311.04788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junya Shiraishi, Mathias Thorsager, Shashi Raj Pandey, Petar Popovski</li>
<li>for: 这篇论文是为了采集互联网物联网（IoT）设备上的数据而写的。</li>
<li>methods: 这篇论文提出使用微型机器学习（Tiny ML）来启动IoT设备上的数据采集，并且只有在数据semantically相关时发送数据。</li>
<li>results: 论文的计算表明，相比基eline方案，提议方案可以实现高的检索精度和高的能效性，达到70%的能源减少，当存储图像的数量为8或更多时。<details>
<summary>Abstract</summary>
This work considers a scenario in which an edge server collects data from Internet of Things (IoT) devices equipped with wake-up receivers. Although this procedure enables on-demand data collection, there is still energy waste if the content of the transmitted data following the wake-up is irrelevant. To mitigate this, we advocate the use of Tiny Machine Learning (ML) to enable a semantic response from the IoT devices, so they can send only semantically relevant data. Nevertheless, receiving the ML model and the ML processing at the IoT devices consumes additional energy. We consider the specific instance of image retrieval and investigate the gain brought by the proposed scheme in terms of energy efficiency, considering both the energy cost of introducing the ML model as well as that of wireless communication. The numerical evaluation shows that, compared to a baseline scheme, the proposed scheme can realize both high retrieval accuracy and high energy efficiency, which reaches up to 70% energy reduction when the number of stored images is equal to or larger than 8.
</details>
<details>
<summary>摘要</summary>
这个研究场景中，边服务器从互联网智能设备（IoT）上收集数据，这些设备配备了唤醒接收器。尽管这种方式实现了需求响应数据收集，但是如果传输的数据内容无关，则会产生能源浪费。为了解决这个问题，我们建议使用小型机器学习（ML），让IoT设备发送只有Semantically相关的数据。然而，接收ML模型和ML处理在IoT设备上占用了额外的能源。我们对具体的图像检索情况进行了研究，并评估了我们的方案在能效率方面的提升，包括引入ML模型的能源成本以及无线通信成本。数值评估显示，相比基eline方案，我们的方案可以实现高准确率和高能效率，可以达到70%的能源减少，当存储图像的数量为8或更大时。
</details></li>
</ul>
<hr>
<h2 id="Superimposed-Chirp-Waveforms-for-SWIPT-with-Diplexer-based-Integrated-Receivers"><a href="#Superimposed-Chirp-Waveforms-for-SWIPT-with-Diplexer-based-Integrated-Receivers" class="headerlink" title="Superimposed Chirp Waveforms for SWIPT with Diplexer-based Integrated Receivers"></a>Superimposed Chirp Waveforms for SWIPT with Diplexer-based Integrated Receivers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04776">http://arxiv.org/abs/2311.04776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arijit Roy, Constantinos Psomas, Ioannis Krikidis</li>
<li>for: 本文探讨了同时进行无线信息和能量传输（SWIPT）应用中的抽象振荡波形superposition。利用振荡波形特性，我们可以同时传输多个振荡，从而在具有相同带宽的情况下实现同一个数量的振荡传输。这使得在多个orthogonal subband中进行子带选择成为可能。</li>
<li>methods: 我们考虑了用户装备了基于diplexer的集成接收器（DIR），该设计允许抽取无线电功率和解码信息从同一个信号中无需分配。通过抽象振荡和子带选择，我们提出了一种利用晶体堆的非线性和频率多样性来实现SWIPT的传输方案。我们 derive了关于平均收集能量（HE）的新的关闭式分析表达式，以及在选择子带时的下降信道率。</li>
<li>results: 我们通过分析和numerical计算结果显示，对于考虑的系统设计，使用抽象振荡基于SWIPT的方案可以提高平均HE性能的提高30%，提高最小级别的HE在多用户网络中，并将能量传输范围扩展到fixed-frequency波形之外。此外，我们还证明了在SWIPT应用中包括DIR接收器可以在能量传输和信息传输之间扩大能量信息传输区域，与通常考虑的力分配接收器相比。<details>
<summary>Abstract</summary>
In this paper, we present the superposition of chirp waveforms for simultaneous wireless information and power transfer (SWIPT) applications. Exploiting the chirp waveform characteristics enables us to superimpose multiple chirps, thereby allowing transmission of the same number of waveforms over less bandwidth. This enables us to perform subband selection when operating over set of orthogonal subbands. Furthermore, we consider a user equipped with a diplexer-based integrated receiver (DIR), which enables to extract radio frequency power and decode information from the same signal without splitting. Thereby, incorporating chirp superposition and subband selection, a transmission scheme is proposed to exploit both the diode's nonlinearity and frequency diversity. We derive novel closed-form analytical expressions of the average harvested energy (HE) via transmission of superimposed chirp over selected subbands based on tools from order statistics. We also analyze the downlink information rate achieved at the user. Through our analytical and numerical results, for the considered system setup, we show that superimposed chirp-based SWIPT provides an improvement of 30$\%$ in average HE performance as compared to multisine waveforms consisting of a set of fixed-frequency cosine signals, improves the minimum level of HE in a multiuser network, and extends the operating range of energy transfer as compared to fixed-frequency waveforms. Furthermore, we illustrate that the inclusion of DIR at the receiver for SWIPT enlarges the energy-information transfer region when compared to the widely considered power splitting receiver.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了同时无线信息和能量传输（SWIPT）应用中的振荡波形superposition。利用振荡波形特点，我们可以同时传输多个振荡，从而在带宽上传输同样多个波形。这使得可以进行子带选择，当操作于多个 ortogonal subband 时。此外，我们考虑了一个装备有diplexer-based integrated receiver（DIR）的用户，该 receiver 可以从同一个信号中提取无线电频率能量和解码信息。因此，通过振荡superposition和子带选择，我们提出了利用晶体管非线性和频率多样性来实现 SWIPT 的传输方案。我们 derivated 新的关闭式分析表达式，用于计算在superimposed chirp 上选择子带的平均收集能量（HE）的表达式。我们还分析了下链信息率在用户端。通过我们的分析和数值结果，我们表明，在考虑的系统设置下，使用振荡superposition 的 SWIPT 可以提高平均 HE 性能的30%，提高多用户网络中的最低级别HE，并将能量传输范围扩展到fixed-frequency waveforms 之外。此外，我们 illustrate  dassDIR 在 SWIPT 中的存在可以在比考虑power splitting receiver 的情况下扩大能量信息传输区域。
</details></li>
</ul>
<hr>
<h2 id="Discerning-and-Enhancing-the-Weighted-Sum-Rate-Maximization-Algorithms-in-Communications"><a href="#Discerning-and-Enhancing-the-Weighted-Sum-Rate-Maximization-Algorithms-in-Communications" class="headerlink" title="Discerning and Enhancing the Weighted Sum-Rate Maximization Algorithms in Communications"></a>Discerning and Enhancing the Weighted Sum-Rate Maximization Algorithms in Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04546">http://arxiv.org/abs/2311.04546</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zepengzhang/ratemax">https://github.com/zepengzhang/ratemax</a></li>
<li>paper_authors: Zepeng Zhang, Ziping Zhao, Kaiming Shen, Daniel P. Palomar, Wei Yu</li>
<li>for: 本文研究了三种优化方法来最大化权重总Rate（WSR），以确保 converges to stationary points。</li>
<li>methods: 本文使用了三种优化方法，包括weighted sum-minimum mean-square error（WMMSE）和WSR最大化via fractional programming（WSR-FP），以及minorization-maximization（MM）算法。</li>
<li>results: 本文的贡献包括：1）对WMMSE、WSR-FP和WSR-MM之间的关系进行了完整的比较研究，并 revelaed their direct correlations; 2）提出了一种新算法WSR-MM+，可以快速 converge和减少计算负担; 3）将WSR-MM+重新定义为BCA框架下的equivalent transform，并提出了一种新版本的WSR-FP+算法。numerical simulations confirm the connections between WMMSE, WSR-FP, and WSR-MM, and the efficacy of the proposed WSR-MM+ and WSR-FP+ algorithms.<details>
<summary>Abstract</summary>
Weighted sum-rate (WSR) maximization plays a critical role in communication system design. This paper examines three optimization methods for WSR maximization, which ensure convergence to stationary points: two block coordinate ascent (BCA) algorithms, namely, weighted sum-minimum mean-square error (WMMSE) and WSR maximization via fractional programming (WSR-FP), along with a minorization-maximization (MM) algorithm, WSR maximization via MM (WSR-MM). Our contributions are threefold. Firstly, we delineate the exact relationships among WMMSE, WSR-FP, and WSR-MM, which, despite their extensive use in the literature, lack a comprehensive comparative study. By probing the theoretical underpinnings linking the BCA and MM algorithmic frameworks, we reveal the direct correlations between the equivalent transformation techniques, essential to the development of WMMSE and WSR-FP, and the surrogate functions pivotal to WSR-MM. Secondly, we propose a novel algorithm, WSR-MM+, harnessing the flexibility of selecting surrogate functions in MM framework. By circumventing the repeated matrix inversions in the search for optimal Lagrange multipliers in existing algorithms, WSR-MM+ significantly reduces the computational load per iteration and accelerates convergence. Thirdly, we reconceptualize WSR-MM+ within the BCA framework, introducing a new equivalent transform, which gives rise to an enhanced version of WSR-FP, named as WSR-FP+. We further demonstrate that WSR-MM+ can be construed as the basic gradient projection method. This perspective yields a deeper understanding into its computational intricacies. Numerical simulations corroborate the connections between WMMSE, WSR-FP, and WSR-MM and confirm the efficacy of the proposed WSR-MM+ and WSR-FP+ algorithms.
</details>
<details>
<summary>摘要</summary>
带有权重的总和（WSR）最大化在通信系统设计中扮演着关键的角色。这篇论文研究了WSR最大化优化方法的三种方法：块均衡升级（BCA）算法Weighted Sum-Minimum Mean-Square Error（WMMSE）和WSR最大化via fractional programming（WSR-FP）以及幂化-最大化（MM）算法WSR最大化via MM（WSR-MM）。我们的贡献有三个方面：一、我们指出了WMMSE、WSR-FP和WSR-MM之间的直接关系，尽管在文献中广泛使用，却缺乏全面的比较研究。我们通过探究BCA和MM算法框架之间的关系，揭示了WMMSE和WSR-FP中使用的等价转换技术，以及WSR-MM中使用的代表函数的直接关系。二、我们提出了一种新的算法WSR-MM+，利用MM框架中选择函数的灵活性。WSR-MM+通过缺少当前算法中的重复矩阵 inverse 操作，significantly 降低每轮计算负担和加速收敛。三、我们将WSR-MM+转移到BCA框架中，引入一个新的等价转换，从而得到一个改进版本的WSR-FP，名为WSR-FP+。我们进一步证明了WSR-MM+可以被视为基本的梯度向量 проек 方法。这种视角带来了计算复杂度的更深刻理解。numerical simulations confirm the connections between WMMSE, WSR-FP, and WSR-MM, and demonstrate the effectiveness of the proposed WSR-MM+ and WSR-FP+ algorithms.
</details></li>
</ul>
<hr>
<h2 id="Cross-Domain-Waveform-Design-for-6G-Integrated-Sensing-and-Communication"><a href="#Cross-Domain-Waveform-Design-for-6G-Integrated-Sensing-and-Communication" class="headerlink" title="Cross-Domain Waveform Design for 6G Integrated Sensing and Communication"></a>Cross-Domain Waveform Design for 6G Integrated Sensing and Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04483">http://arxiv.org/abs/2311.04483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang, Tianqi Mao, Ruiqi Liu, Zhu Han, Octavia A. Dobre, Sheng Chen, Zhaocheng Wang</li>
<li>for: 这个研究旨在提出两种跨领域波形优化策略，以最大化OFDM-based ISAC系统中的资料率。</li>
<li>methods: 这两种策略包括通信中心设计和探测中心设计。通信中心设计利用对通信频道的先前知识，将一部分RE分配给通信，并将其他RE用于探测，以抑制侧obe水平和峰均功率比。探测中心设计使用了�ambiguity函数的地方均�数�测，以确保� locally� perfect自相关性。</li>
<li>results: numerical results表明，这两种波形设计策略可以优化ISAC应用中的资料率。<details>
<summary>Abstract</summary>
Orthogonal frequency division multiplexing (OFDM) is one of the representative integrated sensing and communication (ISAC) waveforms, where sensing and communications tend to be assigned with different resource elements (REs) due to their diverse design requirements. This motivates optimization of resource allocation/waveform design across time, frequency, power and delay-Doppler domains. Therefore, this article proposes two cross-domain waveform optimization strategies for OFDM-based ISAC systems, following communication-centric and sensing-centric criteria, respectively. For the communication-centric design, to maximize the achievable data rate, a fraction of REs are optimally allocated for communications according to prior knowledge of the communication channel. The remaining REs are then employed for sensing, where the sidelobe level and peak to average power ratio are suppressed by optimizing its power-frequency and phase-frequency characteristics. For the sensing-centric design, a `locally' perfect auto-correlation property is ensured by adjusting the unit cells of the ambiguity function within its region of interest (RoI). Afterwards, the irrelevant cells beyond RoI, which can readily determine the sensing power allocation, are optimized with the communication power allocation to enhance the achievable data rate. Numerical results demonstrate the superiority of the proposed communication-centric and sensing-centric waveform designs for ISAC applications.
</details>
<details>
<summary>摘要</summary>
阶梯频分复装多普通频率分装 (OFDM) 是一种代表性的统合感知通信 (ISAC) 波形，其中感知和通信两者通常对应不同的资源元素 (RE) due to their diverse design requirements. This motivates optimization of resource allocation/waveform design across time, frequency, power and delay-Doppler domains. Therefore, this article proposes two cross-domain waveform optimization strategies for OFDM-based ISAC systems, following communication-centric and sensing-centric criteria, respectively.For the communication-centric design, a fraction of REs are optimally allocated for communications based on prior knowledge of the communication channel, and the remaining REs are then employed for sensing, where the sidelobe level and peak to average power ratio are suppressed by optimizing its power-frequency and phase-frequency characteristics.For the sensing-centric design, a `locally' perfect auto-correlation property is ensured by adjusting the unit cells of the ambiguity function within its region of interest (RoI). Afterwards, the irrelevant cells beyond RoI, which can readily determine the sensing power allocation, are optimized with the communication power allocation to enhance the achievable data rate. Numerical results demonstrate the superiority of the proposed communication-centric and sensing-centric waveform designs for ISAC applications.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/eess.SP_2023_11_08/" data-id="clpxp6ce901hlee880lxc1o26" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/cs.SD_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T15:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/07/cs.SD_2023_11_07/">cs.SD - 2023-11-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Soundbay-Deep-Learning-Framework-for-Marine-Mammals-and-Bioacoustic-Research"><a href="#Soundbay-Deep-Learning-Framework-for-Marine-Mammals-and-Bioacoustic-Research" class="headerlink" title="Soundbay: Deep Learning Framework for Marine Mammals and Bioacoustic Research"></a>Soundbay: Deep Learning Framework for Marine Mammals and Bioacoustic Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04343">http://arxiv.org/abs/2311.04343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noam Bressler, Michael Faran, Amit Galor, Michael Moshe Michelashvili, Tomer Nachshon, Noa Weiss</li>
<li>for: 这篇论文是为了提供一个开源的Python框架，帮助生物声学和机器学习研究人员使用深度学习算法进行声音数据分析。</li>
<li>methods: 这篇论文使用了深度学习算法来进行动物叫声检测，并提供了一个简单易用的平台，让研究人员可以轻松地应用现有模型或创建新模型。</li>
<li>results: 这篇论文提供了一个 cetacean 叫声检测的基准数据集，并使用了这个基准数据集来评估深度学习算法的性能。<details>
<summary>Abstract</summary>
This paper presents Soundbay, an open-source Python framework that allows bio-acoustics and machine learning researchers to implement and utilize deep learning-based algorithms for acoustic audio analysis. Soundbay provides an easy and intuitive platform for applying existing models on one's data or creating new models effortlessly. One of the main advantages of the framework is the capability to compare baselines on different benchmarks, a crucial part of emerging research and development related to the usage of deep-learning algorithms for animal call analysis. We demonstrate this by providing a benchmark for cetacean call detection on multiple datasets. The framework is publicly accessible via https://github.com/deep-voice/soundbay
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了Soundbay，一个开源Python框架，它使bio-acoustics和机器学习研究人员可以使用深度学习算法进行声音音频分析。Soundbay提供了一个简单易用的平台，使得研究人员可以轻松地应用现有的模型或创建新的模型。这个框架的一个主要优点是可以比较基线在不同的benchmark上，这是机器学习算法用于动物叫声分析领域的新研究和开发的关键部分。我们通过提供多个数据集上的 cetacean 叫声检测基准来说明这一点。框架可以通过https://github.com/deep-voice/soundbay访问。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/cs.SD_2023_11_07/" data-id="clpxp6c7u012aee88c2xifp7y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/eess.AS_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T14:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/07/eess.AS_2023_11_07/">eess.AS - 2023-11-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Fine-tuning-convergence-model-in-Bengali-speech-recognition"><a href="#Fine-tuning-convergence-model-in-Bengali-speech-recognition" class="headerlink" title="Fine-tuning convergence model in Bengali speech recognition"></a>Fine-tuning convergence model in Bengali speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04122">http://arxiv.org/abs/2311.04122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Ruiying, Shen Meng</li>
<li>for: 提高自动speech recognition模型的性能，特别是对于孟加拉语的识别。</li>
<li>methods: 使用wave2vec 2.0预训练模型进行微调，并调整学习率和dropout参数。</li>
<li>results: 在测试集上，WER由0.508降至0.437，并将训练和验证集合并使用，实现了很好的WER值0.436。<details>
<summary>Abstract</summary>
Research on speech recognition has attracted considerable interest due to the difficult task of segmenting uninterrupted speech. Among various languages, Bengali features distinct rhythmic patterns and tones, making it particularly difficult to recognize and lacking an efficient commercial recognition method. In order to improve the automatic speech recognition model for Bengali, our team has chosen to utilize the wave2vec 2.0 pre-trained model, which has undergone convergence for fine-tuning. Regarding Word Error Rate (WER), the learning rate and dropout parameters were fine-tuned, and after the model training was stable, attempts were made to enlarge the training set ratio, which improved the model's performance. Consequently, there was a notable enhancement in the WER from 0.508 to 0.437 on the test set of the publicly listed official dataset. Afterwards, the training and validation sets were merged, creating a comprehensive dataset that was used as the training set, achieving a remarkable WER of 0.436.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:研究对语音识别具有很大的 интерес，因为分词是一项非常困难的任务。在各种语言中，孟加拉语具有特殊的节奏和调音特征，使其识别非常困难，而且没有有效的商业识别方法。为了提高自动语音识别模型的性能，我们团队选择使用wave2vec 2.0预训练模型，并进行了优化。对于 Word Error Rate（WER），我们调整了学习率和dropout参数，并在模型训练稳定后，尝试将训练集比例扩大，这使得模型性能得到了明显改善。在公共列表的官方数据集上，WER从0.508下降至0.437。然后，我们将训练集和验证集合并，创建了一个完整的数据集，并使用这个数据集进行了训练，达到了很出色的WER值0.436。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/eess.AS_2023_11_07/" data-id="clpxp6c9f0165ee88b9fc7jcc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/cs.CV_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T13:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/07/cs.CV_2023_11_07/">cs.CV - 2023-11-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="3DiffTection-3D-Object-Detection-with-Geometry-Aware-Diffusion-Features"><a href="#3DiffTection-3D-Object-Detection-with-Geometry-Aware-Diffusion-Features" class="headerlink" title="3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features"></a>3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04391">http://arxiv.org/abs/2311.04391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenfeng Xu, Huan Ling, Sanja Fidler, Or Litany</li>
<li>for: 3D объект детектирования из单张图像</li>
<li>methods: 使用3D散度模型提取特征，并通过两种特化Strategy进行微调，包括几何微调和semantic微调</li>
<li>results: 实现了高效的3D检测，并在Omni3D-ARkitscene数据集上超越了前一代标准Cube-RCNN模型，提高了AP3D指标的9.43%。同时，模型也表现出了较好的数据效率和跨频训练能力。<details>
<summary>Abstract</summary>
We present 3DiffTection, a state-of-the-art method for 3D object detection from single images, leveraging features from a 3D-aware diffusion model. Annotating large-scale image data for 3D detection is resource-intensive and time-consuming. Recently, pretrained large image diffusion models have become prominent as effective feature extractors for 2D perception tasks. However, these features are initially trained on paired text and image data, which are not optimized for 3D tasks, and often exhibit a domain gap when applied to the target data. Our approach bridges these gaps through two specialized tuning strategies: geometric and semantic. For geometric tuning, we fine-tune a diffusion model to perform novel view synthesis conditioned on a single image, by introducing a novel epipolar warp operator. This task meets two essential criteria: the necessity for 3D awareness and reliance solely on posed image data, which are readily available (e.g., from videos) and does not require manual annotation. For semantic refinement, we further train the model on target data with detection supervision. Both tuning phases employ ControlNet to preserve the integrity of the original feature capabilities. In the final step, we harness these enhanced capabilities to conduct a test-time prediction ensemble across multiple virtual viewpoints. Through our methodology, we obtain 3D-aware features that are tailored for 3D detection and excel in identifying cross-view point correspondences. Consequently, our model emerges as a powerful 3D detector, substantially surpassing previous benchmarks, e.g., Cube-RCNN, a precedent in single-view 3D detection by 9.43\% in AP3D on the Omni3D-ARkitscene dataset. Furthermore, 3DiffTection showcases robust data efficiency and generalization to cross-domain data.
</details>
<details>
<summary>摘要</summary>
我们介绍了3DiffTection方法，这是一种基于单张图像的3D物体检测state-of-the-art方法，利用了3D扩散模型的特征。为了实现3D检测， annotating大规模图像数据是资源充足和时间consuming的。现在，预训练的大图像扩散模型已成为2D感知任务的有效特征提取器。然而，这些特征通常是通过paired文本和图像数据进行预训练，这些数据通常不适合3D任务，而且经常会出现域隔现象。我们的方法使用两种特殊的调整策略来bridges这些差距：几何调整和semantic调整。为了几何调整，我们精心调整了扩散模型，以实现基于单张图像的新视图生成，通过引入novel epipolar warp operator。这个任务满足了两个关键的要求：需要3D意识和仅仅基于提供的posed图像数据进行调整，这些数据 readily available（例如，从视频中获得），不需要手动标注。为了semantic调整，我们进一步在目标数据上训练模型，并在检测监督下进行训练。两个调整阶段都使用ControlNet来保持原始特征Capabilities。在最后一步，我们利用这些加强的特征来进行测试预测 ensemble across multiple virtual viewpoints。通过我们的方法，我们获得了3D意识的特征，这些特征适用于3D检测，并且能够准确地找到交叉视点对应关系。因此，我们的模型在Omni3D-ARkitscene dataset上的AP3D指标上表现出色，比前一代标准9.43%。此外，3DiffTection还展示了robust数据效率和跨领域数据的通用性。
</details></li>
</ul>
<hr>
<h2 id="Basis-restricted-elastic-shape-analysis-on-the-space-of-unregistered-surfaces"><a href="#Basis-restricted-elastic-shape-analysis-on-the-space-of-unregistered-surfaces" class="headerlink" title="Basis restricted elastic shape analysis on the space of unregistered surfaces"></a>Basis restricted elastic shape analysis on the space of unregistered surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04382">http://arxiv.org/abs/2311.04382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanuel Hartman, Emery Pierson, Martin Bauer, Mohamed Daoudi, Nicolas Charon</li>
<li>for: 这个论文旨在提出一种新的数学和计算方法来进行表面分析，基于泛函空间上的弹性里曼米特凯。</li>
<li>methods: 该方法 restricts the space of allowable transformations to predefined finite dimensional bases of deformation fields，并使用数据驱动的方式来估算这些基。</li>
<li>results: 该方法可以有效地完成许多surface mesh中的任务，包括shape registration、interpolation、motion transfer和随机pose生成，并且在人体形态和姿态数据和人脸扫描中表现出优于状态 искусственный方法。<details>
<summary>Abstract</summary>
This paper introduces a new mathematical and numerical framework for surface analysis derived from the general setting of elastic Riemannian metrics on shape spaces. Traditionally, those metrics are defined over the infinite dimensional manifold of immersed surfaces and satisfy specific invariance properties enabling the comparison of surfaces modulo shape preserving transformations such as reparametrizations. The specificity of the approach we develop is to restrict the space of allowable transformations to predefined finite dimensional bases of deformation fields. These are estimated in a data-driven way so as to emulate specific types of surface transformations observed in a training set. The use of such bases allows to simplify the representation of the corresponding shape space to a finite dimensional latent space. However, in sharp contrast with methods involving e.g. mesh autoencoders, the latent space is here equipped with a non-Euclidean Riemannian metric precisely inherited from the family of aforementioned elastic metrics. We demonstrate how this basis restricted model can be then effectively implemented to perform a variety of tasks on surface meshes which, importantly, does not assume these to be pre-registered (i.e. with given point correspondences) or to even have a consistent mesh structure. We specifically validate our approach on human body shape and pose data as well as human face scans, and show how it generally outperforms state-of-the-art methods on problems such as shape registration, interpolation, motion transfer or random pose generation.
</details>
<details>
<summary>摘要</summary>
However, this approach restricts the space of allowable transformations to predefined finite-dimensional bases of deformation fields, which are estimated in a data-driven way to emulate specific types of surface transformations observed in a training set. This allows for the simplification of the representation of the corresponding shape space to a finite-dimensional latent space, equipped with a non-Euclidean Riemannian metric precisely inherited from the family of elastic metrics.The proposed basis-restricted model can be effectively implemented to perform a variety of tasks on surface meshes, including shape registration, interpolation, motion transfer, and random pose generation, without assuming pre-registration or a consistent mesh structure. The approach is validated on human body shape and pose data, as well as human face scans, and is shown to outperform state-of-the-art methods on these tasks.
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Approach-to-Video-Anomaly-Detection-using-Convolutional-Autoencoders"><a href="#A-Deep-Learning-Approach-to-Video-Anomaly-Detection-using-Convolutional-Autoencoders" class="headerlink" title="A Deep Learning Approach to Video Anomaly Detection using Convolutional Autoencoders"></a>A Deep Learning Approach to Video Anomaly Detection using Convolutional Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04351">http://arxiv.org/abs/2311.04351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gopikrishna Pavuluri, Gayathri Annem</li>
<li>for: 这个研究旨在提出一种基于深度学习的错误探测方法，用于视频中探测错误，使用核心autoencoder和解oder神经网络，并在UCSD dataset上进行评估。</li>
<li>methods: 这个方法使用核心autoencoder来学习正常视频的空间时间模式，然后将每帧视频测试影像与这个学习的表现进行比较。</li>
<li>results: 试验结果显示，这个方法在UCSD Ped1和Ped2 dataset上的全部精度为99.35%和99.77%，较其他现有方法高，显示这个方法具有优秀的错误探测能力，可以应用于实际应用中的视频错误探测。<details>
<summary>Abstract</summary>
In this research we propose a deep learning approach for detecting anomalies in videos using convolutional autoencoder and decoder neural networks on the UCSD dataset.Our method utilizes a convolutional autoencoder to learn the spatiotemporal patterns of normal videos and then compares each frame of a test video to this learned representation. We evaluated our approach on the UCSD dataset and achieved an overall accuracy of 99.35% on the Ped1 dataset and 99.77% on the Ped2 dataset, demonstrating the effectiveness of our method for detecting anomalies in surveillance videos. The results show that our method outperforms other state-of-the-art methods, and it can be used in real-world applications for video anomaly detection.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一种基于深度学习的视频异常检测方法，使用 convolutional autoencoder 和 decoder 神经网络在 UCSD 数据集上进行检测。我们的方法利用 convolutional autoencoder 学习正常视频中的空间时间特征，然后对每帧测试视频进行比较，以确定异常情况。我们对 UCSD 数据集进行评估，实现了 Ped1 数据集的总准确率为 99.35%， Ped2 数据集的总准确率为 99.77%，这表明我们的方法可以有效地检测视频中的异常情况。结果显示，我们的方法比其他当前状态最佳方法高效，可以在实际应用中用于视频异常检测。
</details></li>
</ul>
<hr>
<h2 id="SaFL-Sybil-aware-Federated-Learning-with-Application-to-Face-Recognition"><a href="#SaFL-Sybil-aware-Federated-Learning-with-Application-to-Face-Recognition" class="headerlink" title="SaFL: Sybil-aware Federated Learning with Application to Face Recognition"></a>SaFL: Sybil-aware Federated Learning with Application to Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04346">http://arxiv.org/abs/2311.04346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahdi Ghafourian, Julian Fierrez, Ruben Vera-Rodriguez, Ruben Tolosana, Aythami Morales</li>
<li>for: 本研究旨在提出一种新的防御方法，以防止在联合学习（Federated Learning，FL）中发生毒素攻击。</li>
<li>methods: 本研究使用了一种时变汇集方案，以降低毒素攻击的影响。</li>
<li>results: 研究发现，SaFL可以有效降低毒素攻击的影响，并提高FL的安全性和隐私性。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a machine learning paradigm to conduct collaborative learning among clients on a joint model. The primary goal is to share clients' local training parameters with an integrating server while preserving their privacy. This method permits to exploit the potential of massive mobile users' data for the benefit of machine learning models' performance while keeping sensitive data on local devices. On the downside, FL raises security and privacy concerns that have just started to be studied. To address some of the key threats in FL, researchers have proposed to use secure aggregation methods (e.g. homomorphic encryption, secure multiparty computation, etc.). These solutions improve some security and privacy metrics, but at the same time bring about other serious threats such as poisoning attacks, backdoor attacks, and free running attacks. This paper proposes a new defense method against poisoning attacks in FL called SaFL (Sybil-aware Federated Learning) that minimizes the effect of sybils with a novel time-variant aggregation scheme.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 是一种机器学习模式，通过客户端之间的合作学习，实现共同模型的提升。主要目标是在客户端上保持本地训练参数的私钥，同时将这些参数与集成服务器共享。这种方法可以利用大量移动用户数据来提高机器学习模型的性能，而不需要披露敏感数据。然而，FL 也存在安全性和隐私问题，这些问题正在被研究。为了解决 FL 中一些关键威胁，研究人员提出了使用安全汇聚方法（如幂omorphic 加密、安全多方计算等）。这些解决方案可以提高一些安全性和隐私指标，但同时也会带来其他严重威胁，如毒素攻击、后门攻击和自由跑攻击。这篇论文提出了一种新的防御方法，称为 SaFL（知识体-意识 Federated Learning），以防止毒素攻击。SaFL 通过一种新的时变汇聚方案，来减少 sybil 的影响。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Semantic-Matching-with-Hypercolumn-Correlation"><a href="#Efficient-Semantic-Matching-with-Hypercolumn-Correlation" class="headerlink" title="Efficient Semantic Matching with Hypercolumn Correlation"></a>Efficient Semantic Matching with Hypercolumn Correlation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04336">http://arxiv.org/abs/2311.04336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seungwook Kim, Juhong Min, Minsu Cho</li>
<li>for: 本文主要针对 semantic matching 问题，即在多个视觉特征点之间建立 semantic 相关性的问题。</li>
<li>methods: 本文提出了 HCCNet 方法，它利用 multi-scale correlation maps 来提高 semantic matching 性能，而不需要对 4D correlation map 进行费时的 match-wise 关系挖掘。HCCNet 方法通过 feature slicing 技术，将瓶颈特征分解为更多的 intermediate features，然后使用这些 intermediate features 构建 hypercolumn correlation。</li>
<li>results: HCCNet 方法在标准的 semantic matching Benchmark 上达到了当前最佳或竞争性性能，同时具有较低的延迟和计算负担。相比之下，现有的 SoTA 方法在计算负担和延迟上都有所增加。<details>
<summary>Abstract</summary>
Recent studies show that leveraging the match-wise relationships within the 4D correlation map yields significant improvements in establishing semantic correspondences - but at the cost of increased computation and latency. In this work, we focus on the aspect that the performance improvements of recent methods can also largely be attributed to the usage of multi-scale correlation maps, which hold various information ranging from low-level geometric cues to high-level semantic contexts. To this end, we propose HCCNet, an efficient yet effective semantic matching method which exploits the full potential of multi-scale correlation maps, while eschewing the reliance on expensive match-wise relationship mining on the 4D correlation map. Specifically, HCCNet performs feature slicing on the bottleneck features to yield a richer set of intermediate features, which are used to construct a hypercolumn correlation. HCCNet can consequently establish semantic correspondences in an effective manner by reducing the volume of conventional high-dimensional convolution or self-attention operations to efficient point-wise convolutions. HCCNet demonstrates state-of-the-art or competitive performances on the standard benchmarks of semantic matching, while incurring a notably lower latency and computation overhead compared to the existing SoTA methods.
</details>
<details>
<summary>摘要</summary>
最新的研究表明，基于4D相关图的对应关系可以获得显著提高 semantic correspondence的性能 - 但是这也需要更高的计算和延迟。在这项工作中，我们注重于 Multi-scale correlation maps 的使用，它们包含了低级别的几何准确信息到高级别的 semantic 上下文信息。为了实现这一目标，我们提出了 HCCNet，一种高效且高性能的 semantic matching 方法。HCCNet 通过对瓶颈特征进行 feature slicing，以获得更丰富的中间特征，并将其用于构建 hypercolumn correlation。HCCNet 可以通过减少传统的高维ensional convolution 或自注意力操作，实现效果性的 semantic correspondence 建立。与现有的 SoTA 方法相比，HCCNet 在标准的 semantic matching 标准准样上显示出了领先或竞争性的性能，同时也具有较低的计算和延迟开销。
</details></li>
</ul>
<hr>
<h2 id="A-Data-Perspective-on-Enhanced-Identity-Preservation-for-Diffusion-Personalization"><a href="#A-Data-Perspective-on-Enhanced-Identity-Preservation-for-Diffusion-Personalization" class="headerlink" title="A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization"></a>A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04315">http://arxiv.org/abs/2311.04315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingzhe He, Zhiwen Cao, Nicholas Kolkin, Lantao Yu, Helge Rhodin, Ratheesh Kalarot</li>
<li>for: 能够生成具有自然语言描述的图像</li>
<li>methods: 使用数据增强方法，不需要修改模型结构</li>
<li>results: 生成的图像保留细节，同时能够生成多个不同的样本，具有高质量和好的文本拟合性<details>
<summary>Abstract</summary>
Large text-to-image models have revolutionized the ability to generate imagery using natural language. However, particularly unique or personal visual concepts, such as your pet, an object in your house, etc., will not be captured by the original model. This has led to interest in how to inject new visual concepts, bound to a new text token, using as few as 4-6 examples. Despite significant progress, this task remains a formidable challenge, particularly in preserving the subject's identity. While most researchers attempt to to address this issue by modifying model architectures, our approach takes a data-centric perspective, advocating the modification of data rather than the model itself. We introduce a novel regularization dataset generation strategy on both the text and image level; demonstrating the importance of a rich and structured regularization dataset (automatically generated) to prevent losing text coherence and better identity preservation. The better quality is enabled by allowing up to 5x more fine-tuning iterations without overfitting and degeneration. The generated renditions of the desired subject preserve even fine details such as text and logos; all while maintaining the ability to generate diverse samples that follow the input text prompt. Since our method focuses on data augmentation, rather than adjusting the model architecture, it is complementary and can be combined with prior work. We show on established benchmarks that our data-centric approach forms the new state of the art in terms of image quality, with the best trade-off between identity preservation, diversity, and text alignment.
</details>
<details>
<summary>摘要</summary>
大型文本到图像模型已经革命化了使用自然语言生成图像的能力。然而，特定的独特或个人视觉概念，如你的宠物或家中的物品等，将不会被原始模型捕捉。这导致了如何在少量示例（4-6个）下注入新的视觉概念，以保持文本准确性和主体认知的挑战。虽然大多数研究人员尝试通过修改模型结构来解决这个问题，但我们的方法具有数据驱动的思维，主张修改数据而不是模型。我们提出了一种新的常规数据生成策略，包括文本和图像两个水平，以保持文本准确性和主体认知。通过自动生成的丰富和结构化常规数据，我们可以避免文本准确性的损失和主体认知的受损。我们的方法强调数据增强，因此它是可以与之前的研究结合使用的。我们在确立的标准准测试 benchmark 上表现出新的状态前瞻图像质量，同时保持文本准确性、多样性和主体认知的最佳平衡。
</details></li>
</ul>
<hr>
<h2 id="Holistic-Evaluation-of-Text-To-Image-Models"><a href="#Holistic-Evaluation-of-Text-To-Image-Models" class="headerlink" title="Holistic Evaluation of Text-To-Image Models"></a>Holistic Evaluation of Text-To-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04287">http://arxiv.org/abs/2311.04287</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanford-crfm/helm">https://github.com/stanford-crfm/helm</a></li>
<li>paper_authors: Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, Minguk Kang, Taesung Park, Jure Leskovec, Jun-Yan Zhu, Li Fei-Fei, Jiajun Wu, Stefano Ermon, Percy Liang</li>
<li>for: 评估文本到图像模型的全面性能。</li>
<li>methods: 引入新的评价指标，包括12个方面，如文本到图像对齐、图像质量、美学性、创新性、理解力、知识、偏见、恶意、公平性、稳定性、多语言支持和效率。</li>
<li>results: 评估26种state-of-the-art文本到图像模型，发现不同模型各有优势，没有一个模型在所有方面表现出色。<details>
<summary>Abstract</summary>
The stunning qualitative improvement of recent text-to-image models has led to their widespread attention and adoption. However, we lack a comprehensive quantitative understanding of their capabilities and risks. To fill this gap, we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models (HEIM). Whereas previous evaluations focus mostly on text-image alignment and image quality, we identify 12 aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models on this benchmark. Our results reveal that no single model excels in all aspects, with different models demonstrating different strengths. We release the generated images and human evaluation results for full transparency at https://crfm.stanford.edu/heim/v1.1.0 and the code at https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase.
</details>
<details>
<summary>摘要</summary>
“这些最近的文本至图模型的优秀质量改善，吸引了广泛的注意和采纳。但我们尚未有一个全面的量化理解其能力和风险。为了填补这个空白，我们引入了一个新的参考基准，即整体评估文本至图模型（HEIM）。先前的评估主要集中在文本至图Alignment和图像质量，我们识别出12个方面，包括文本至图Alignment、图像质量、美学、原创性、推理、知识、偏见、毒性、公平、Robustness、多语言和效率。我们组合62个情况，包括这12个方面，并评估26种文本至图模型。我们的结果显示，没有一个模型在所有方面都优秀，不同的模型各有优势。我们发布生成的图像和人类评价结果，以及模型代码，在https://crfm.stanford.edu/heim/v1.1.0上公开，并与HELM代码库集成在https://github.com/stanford-crfm/helm。”
</details></li>
</ul>
<hr>
<h2 id="Video-Instance-Matting"><a href="#Video-Instance-Matting" class="headerlink" title="Video Instance Matting"></a>Video Instance Matting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04212">http://arxiv.org/abs/2311.04212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shi-labs/vim">https://github.com/shi-labs/vim</a></li>
<li>paper_authors: Jiachen Li, Roberto Henschel, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, Humphrey Shi</li>
<li>for: 这篇论文是为了解决视频实例分割和 alpha matte 问题而写的。</li>
<li>methods: 这篇论文使用了一种名为 MSG-VIM 的Mask Sequence Guided Video Instance Matting神经网络来解决这个问题，该模型具有增强的mask guidance和时间特征指导等特性。</li>
<li>results: 根据新建的 VIM50 数据集和 VIMQ 评价指标，该模型在视频实例分割和 alpha matte 任务上具有出色的表现，与现有方法相比具有大幅提升。<details>
<summary>Abstract</summary>
Conventional video matting outputs one alpha matte for all instances appearing in a video frame so that individual instances are not distinguished. While video instance segmentation provides time-consistent instance masks, results are unsatisfactory for matting applications, especially due to applied binarization. To remedy this deficiency, we propose Video Instance Matting~(VIM), that is, estimating alpha mattes of each instance at each frame of a video sequence. To tackle this challenging problem, we present MSG-VIM, a Mask Sequence Guided Video Instance Matting neural network, as a novel baseline model for VIM. MSG-VIM leverages a mixture of mask augmentations to make predictions robust to inaccurate and inconsistent mask guidance. It incorporates temporal mask and temporal feature guidance to improve the temporal consistency of alpha matte predictions. Furthermore, we build a new benchmark for VIM, called VIM50, which comprises 50 video clips with multiple human instances as foreground objects. To evaluate performances on the VIM task, we introduce a suitable metric called Video Instance-aware Matting Quality~(VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50 benchmark and outperforms existing methods by a large margin. The project is open-sourced at https://github.com/SHI-Labs/VIM.
</details>
<details>
<summary>摘要</summary>
传统的视频排除输出一个alpha环境图，以便在视频帧中对所有实例进行分割。然而，视频实例分割提供了时间一致的实例掩模，但是结果并不满意，特别是因为应用了二进制化。为了解决这些不足，我们提出了视频实例排除（VIM），即在视频序列中每帧估计每个实例的alpha环境图。为了解决这个复杂的问题，我们提出了一种新的基线模型，即Mask Sequence Guided Video Instance Matting（MSG-VIM）。MSG-VIM利用了一种混合的掩码更新，以使其预测Results robust to inaccurate and inconsistent mask guidance。它还包括时间掩模和时间特征引导，以提高alpha环境预测的 temporal consistency。此外，我们建立了一个新的基准数据集，名为VIM50，该数据集包括50个视频clip，每个clip中有多个人体实例作为前景物体。为了评估在VIM任务中的性能，我们提出了一个适合的度量，即Video Instance-aware Matting Quality（VIMQ）。我们的提出的模型MSG-VIM在VIM50 benchmark上设置了一个强大的基线，并在现有方法之上出现较大的差。项目开源在https://github.com/SHI-Labs/VIM。
</details></li>
</ul>
<hr>
<h2 id="Deep-Hashing-via-Householder-Quantization"><a href="#Deep-Hashing-via-Householder-Quantization" class="headerlink" title="Deep Hashing via Householder Quantization"></a>Deep Hashing via Householder Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04207">http://arxiv.org/abs/2311.04207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/twistedcubic/learn-to-hash">https://github.com/twistedcubic/learn-to-hash</a></li>
<li>paper_authors: Lucas R. Schwengber, Lucas Resende, Paulo Orenstein, Roberto I. Oliveira</li>
<li>For: The paper is written for improving the efficiency and performance of large-scale image similarity search using deep hashing techniques.* Methods: The paper proposes an alternative quantization strategy that decomposes the learning problem into two stages: first, perform similarity learning over the embedding space with no quantization, and second, find an optimal orthogonal transformation of the embeddings using Householder matrices and then quantize the transformed embedding through the sign function.* Results: The proposed algorithm leads to state-of-the-art performance on widely used image datasets and brings consistent improvements in performance to existing deep hashing algorithms, without any hyperparameter tuning and at no cost in terms of performance.Here’s the simplified Chinese text for the three key information points:* 用途：本文提出了一种改进大规模图像相似搜索的深度哈希技术，以提高效率和性能。* 方法：本文提出的方法是将学习问题分解成两个阶段：首先，在嵌入空间中进行相似学习，无需归一化；其次，使用欧几里得变换来找到最佳的对称变换，然后对transformed embedding进行归一化。* 结果：提议的算法在广泛使用的图像Dataset上达到了状态态的性能，并且与现有的深度哈希算法相比，带来了一致的改进，无需进行任何参数调整和性能损失。<details>
<summary>Abstract</summary>
Hashing is at the heart of large-scale image similarity search, and recent methods have been substantially improved through deep learning techniques. Such algorithms typically learn continuous embeddings of the data. To avoid a subsequent costly binarization step, a common solution is to employ loss functions that combine a similarity learning term (to ensure similar images are grouped to nearby embeddings) and a quantization penalty term (to ensure that the embedding entries are close to binarized entries, e.g., -1 or 1). Still, the interaction between these two terms can make learning harder and the embeddings worse. We propose an alternative quantization strategy that decomposes the learning problem in two stages: first, perform similarity learning over the embedding space with no quantization; second, find an optimal orthogonal transformation of the embeddings so each coordinate of the embedding is close to its sign, and then quantize the transformed embedding through the sign function. In the second step, we parametrize orthogonal transformations using Householder matrices to efficiently leverage stochastic gradient descent. Since similarity measures are usually invariant under orthogonal transformations, this quantization strategy comes at no cost in terms of performance. The resulting algorithm is unsupervised, fast, hyperparameter-free and can be run on top of any existing deep hashing or metric learning algorithm. We provide extensive experimental results showing that this approach leads to state-of-the-art performance on widely used image datasets, and, unlike other quantization strategies, brings consistent improvements in performance to existing deep hashing algorithms.
</details>
<details>
<summary>摘要</summary>
“哈希是大规模图像相似搜寻中的核心，而近年来的方法受到深度学习技术的改进。这些算法通常学习连续的嵌入。以避免后续的昂贵 binarization 步骤，一个常见的解决方案是使用一个 combine 两个 тер优点：一个 similarity learning 项（确保相似的图像被分配到附近的嵌入）和一个 quantization penalty 项（确保嵌入的元素都很近，例如 -1 或 1）。然而，这两个项目之间的互动可能会让学习更加困难，并导致嵌入更差。我们提出一个 alternating quantization 策略，将学习问题分成两个阶段：第一个阶段是在嵌入空间进行 similarity learning，第二个阶段是使用 Householder 矩阵对嵌入进行快速的对称转换，然后使用 sign 函数对转换后的嵌入进行量化。在第二个阶段，我们使用 Householder 矩阵来快速地对嵌入进行对称转换，这样可以充分利用测验梯度下降。因为相似度测验通常是对对称转换的不变量，因此这个量化策略不会对性能造成损害。得到的算法是无监控、快速、无超参数的，可以在任何现有的深度哈希或度量学习算法之上运行。我们在广泛的图像数据集上进行了广泛的实验，结果显示这个方法对现有的深度哈希算法带来了稳定的改进。”
</details></li>
</ul>
<hr>
<h2 id="High-fidelity-3D-Reconstruction-of-Plants-using-Neural-Radiance-Field"><a href="#High-fidelity-3D-Reconstruction-of-Plants-using-Neural-Radiance-Field" class="headerlink" title="High-fidelity 3D Reconstruction of Plants using Neural Radiance Field"></a>High-fidelity 3D Reconstruction of Plants using Neural Radiance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04154">http://arxiv.org/abs/2311.04154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kewei Hu, Ying Wei, Yaoqiang Pan, Hanwen Kang, Chao Chen<br>for:* The paper is focused on exploring the use of Neural Radiance Fields (NeRF) for plant phenotyping in agricultural contexts.methods:* The paper utilizes two state-of-the-art NeRF methods, Instant-NGP and Instant-NSR, to synthesize 2D novel-view images and reconstruct 3D crop and plant models.results:* The paper demonstrates that NeRF achieves commendable performance in synthesizing novel-view images and is competitive with Reality Capture, a leading commercial software for 3D Multi-View Stereo (MVS)-based reconstruction. However, the study also highlights certain drawbacks of NeRF, including relatively slow training speeds, performance limitations in cases of insufficient sampling, and challenges in obtaining geometry quality in complex setups.Here is the text in Simplified Chinese:for:* 本研究是针对减少可持续农业实践中的植物形态重建问题，通过使用神经辐射场（NeRF）进行研究。methods:* 本研究使用了两种 state-of-the-art NeRF方法，即 Instant-NGP 和 Instant-NSR，来synthesize 2D 新视图图像和重建 3D 作物和植物模型。results:* 本研究表明，NeRF 在synthesize 2D 新视图图像和重建 3D 作物和植物模型方面达到了可观的表现，与Reality Capture，一种商业化的3D Multi-View Stereo（MVS）重建软件，相比竞争。然而，研究也指出了NeRF的一些缺点，包括训练速度较慢，在不充分采样的情况下表现有限，以及在复杂设置下获得geometry质量的挑战。<details>
<summary>Abstract</summary>
Accurate reconstruction of plant phenotypes plays a key role in optimising sustainable farming practices in the field of Precision Agriculture (PA). Currently, optical sensor-based approaches dominate the field, but the need for high-fidelity 3D reconstruction of crops and plants in unstructured agricultural environments remains challenging. Recently, a promising development has emerged in the form of Neural Radiance Field (NeRF), a novel method that utilises neural density fields. This technique has shown impressive performance in various novel vision synthesis tasks, but has remained relatively unexplored in the agricultural context. In our study, we focus on two fundamental tasks within plant phenotyping: (1) the synthesis of 2D novel-view images and (2) the 3D reconstruction of crop and plant models. We explore the world of neural radiance fields, in particular two SOTA methods: Instant-NGP, which excels in generating high-quality images with impressive training and inference speed, and Instant-NSR, which improves the reconstructed geometry by incorporating the Signed Distance Function (SDF) during training. In particular, we present a novel plant phenotype dataset comprising real plant images from production environments. This dataset is a first-of-its-kind initiative aimed at comprehensively exploring the advantages and limitations of NeRF in agricultural contexts. Our experimental results show that NeRF demonstrates commendable performance in the synthesis of novel-view images and is able to achieve reconstruction results that are competitive with Reality Capture, a leading commercial software for 3D Multi-View Stereo (MVS)-based reconstruction. However, our study also highlights certain drawbacks of NeRF, including relatively slow training speeds, performance limitations in cases of insufficient sampling, and challenges in obtaining geometry quality in complex setups.
</details>
<details>
<summary>摘要</summary>
准确重建植物fenotype在精准农业（PA）中扮演着关键角色。目前，光学感测器基本方法dominates the field, but the need for high-fidelity 3D reconstruction of crops and plants in unstructured agricultural environments remains challenging. Recently, a promising development has emerged in the form of Neural Radiance Field (NeRF), a novel method that utilizes neural density fields. This technique has shown impressive performance in various novel vision synthesis tasks, but has remained relatively unexplored in the agricultural context. In our study, we focus on two fundamental tasks within plant phenotyping: (1) the synthesis of 2D novel-view images and (2) the 3D reconstruction of crop and plant models. We explore the world of NeRF, in particular two state-of-the-art methods: Instant-NGP, which excels in generating high-quality images with impressive training and inference speed, and Instant-NSR, which improves the reconstructed geometry by incorporating the Signed Distance Function (SDF) during training. In particular, we present a novel plant phenotype dataset comprising real plant images from production environments. This dataset is a first-of-its-kind initiative aimed at comprehensively exploring the advantages and limitations of NeRF in agricultural contexts. Our experimental results show that NeRF demonstrates commendable performance in the synthesis of novel-view images and is able to achieve reconstruction results that are competitive with Reality Capture, a leading commercial software for 3D Multi-View Stereo (MVS)-based reconstruction. However, our study also highlights certain drawbacks of NeRF, including relatively slow training speeds, performance limitations in cases of insufficient sampling, and challenges in obtaining geometry quality in complex setups.
</details></li>
</ul>
<hr>
<h2 id="I2VGen-XL-High-Quality-Image-to-Video-Synthesis-via-Cascaded-Diffusion-Models"><a href="#I2VGen-XL-High-Quality-Image-to-Video-Synthesis-via-Cascaded-Diffusion-Models" class="headerlink" title="I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models"></a>I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04145">http://arxiv.org/abs/2311.04145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-vilab/i2vgen-xl">https://github.com/damo-vilab/i2vgen-xl</a></li>
<li>paper_authors: Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, Jingren Zhou</li>
<li>for: 提高视频生成的semantic精度、时间空间连续性和视觉质量。</li>
<li>methods: 提出了一种逐 stage的I2VGen-XL方法，通过将输入数据分解成两个阶段，以提高模型的表现。</li>
<li>results: 通过大量的实验和比较，表明I2VGen-XL可以同时提高视频的semantic精度、时间空间连续性和视觉质量。<details>
<summary>Abstract</summary>
Video synthesis has recently made remarkable strides benefiting from the rapid development of diffusion models. However, it still encounters challenges in terms of semantic accuracy, clarity and spatio-temporal continuity. They primarily arise from the scarcity of well-aligned text-video data and the complex inherent structure of videos, making it difficult for the model to simultaneously ensure semantic and qualitative excellence. In this report, we propose a cascaded I2VGen-XL approach that enhances model performance by decoupling these two factors and ensures the alignment of the input data by utilizing static images as a form of crucial guidance. I2VGen-XL consists of two stages: i) the base stage guarantees coherent semantics and preserves content from input images by using two hierarchical encoders, and ii) the refinement stage enhances the video's details by incorporating an additional brief text and improves the resolution to 1280$\times$720. To improve the diversity, we collect around 35 million single-shot text-video pairs and 6 billion text-image pairs to optimize the model. By this means, I2VGen-XL can simultaneously enhance the semantic accuracy, continuity of details and clarity of generated videos. Through extensive experiments, we have investigated the underlying principles of I2VGen-XL and compared it with current top methods, which can demonstrate its effectiveness on diverse data. The source code and models will be publicly available at \url{https://i2vgen-xl.github.io}.
</details>
<details>
<summary>摘要</summary>
“视频生成技术在最近几年内升级很快，但它仍然面临Semantic精度、清晰度和时空连续性的挑战。这些挑战的主要原因是缺乏准确的文本-视频数据和视频的复杂内存结构，使得模型很难同时保证Semantic和质量上的优秀表现。在这份报告中，我们提议一种名为I2VGen-XL的方法，该方法可以减轻这些因素的影响，并通过使用静止图像作为重要导向来保证输入数据的对齐。I2VGen-XL包括两个阶段：一是基础阶段，该阶段通过两个层次编码器保证文本和图像之间的协调性，并保留输入图像的内容；二是优化阶段，该阶段通过添加一些简短的文本和提高分辨率到1280×720来提高视频的细节。为了提高多样性，我们收集了约35亿个单个文本-视频对和60亿个文本-图像对，并且通过优化模型来提高模型的性能。因此，I2VGen-XL可以同时提高Semantic精度、视频细节的连续性和清晰度。经过广泛的实验，我们发现I2VGen-XL的下面原理和当前顶峰方法的效果，可以在多样的数据上证明其效果。模型和代码将在 \url{https://i2vgen-xl.github.io} 上公开。”
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Quality-Improvement-in-Videoconferencing-using-Keyframes-based-GAN"><a href="#Perceptual-Quality-Improvement-in-Videoconferencing-using-Keyframes-based-GAN" class="headerlink" title="Perceptual Quality Improvement in Videoconferencing using Keyframes-based GAN"></a>Perceptual Quality Improvement in Videoconferencing using Keyframes-based GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04263">http://arxiv.org/abs/2311.04263</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lorenzoagnolucci/keyframes-gan">https://github.com/lorenzoagnolucci/keyframes-gan</a></li>
<li>paper_authors: Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, Alberto Del Bimbo</li>
<li>for: 提高视频会议中视觉质量</li>
<li>methods: 使用GAN技术维护和更新参考帧，提取多尺度特征并按面部特征进行进度组合</li>
<li>results: 提高视觉质量并生成高真实度结果，即使高压缩率时仍能获得良好效果<details>
<summary>Abstract</summary>
In the latest years, videoconferencing has taken a fundamental role in interpersonal relations, both for personal and business purposes. Lossy video compression algorithms are the enabling technology for videoconferencing, as they reduce the bandwidth required for real-time video streaming. However, lossy video compression decreases the perceived visual quality. Thus, many techniques for reducing compression artifacts and improving video visual quality have been proposed in recent years. In this work, we propose a novel GAN-based method for compression artifacts reduction in videoconferencing. Given that, in this context, the speaker is typically in front of the camera and remains the same for the entire duration of the transmission, we can maintain a set of reference keyframes of the person from the higher-quality I-frames that are transmitted within the video stream and exploit them to guide the visual quality improvement; a novel aspect of this approach is the update policy that maintains and updates a compact and effective set of reference keyframes. First, we extract multi-scale features from the compressed and reference frames. Then, our architecture combines these features in a progressive manner according to facial landmarks. This allows the restoration of the high-frequency details lost after the video compression. Experiments show that the proposed approach improves visual quality and generates photo-realistic results even with high compression rates. Code and pre-trained networks are publicly available at https://github.com/LorenzoAgnolucci/Keyframes-GAN.
</details>
<details>
<summary>摘要</summary>
最近几年，视频会议已经在人际关系中扮演了重要的角色，包括个人和商业用途。损失式视频压缩算法是视频会议的核心技术，它降低了实时视频流的带宽需求。然而，损失式视频压缩会降低视频的视觉质量。因此，许多用于减少压缩残差和提高视频视觉质量的技术已经在最近几年内提出。在这种情况下，我们提出了一种基于GAN的新方法，用于减少视频会议中的压缩残差。由于发言人通常会站在摄像头前，并且在整个传输过程中保持不变，我们可以维护一组高质量I帧中的人脸参考图像，并利用它们来导航视觉质量改进。我们的方法包括以下步骤：首先，我们从压缩和参考帧中提取多级特征。然后，我们将这些特征进行进一步组合，根据人脸特征进行进行逐步组合。这样可以重新获得因压缩而丢失的高频环境细节。实验结果表明，我们的方法可以提高视觉质量，并且可以生成高质量的图像，即使压缩率较高。我们的代码和预训练网络可以在 GitHub 上获得：https://github.com/LorenzoAgnolucci/Keyframes-GAN。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Semantic-Map-Representation-for-Skill-based-Visual-Object-Navigation"><a href="#Interactive-Semantic-Map-Representation-for-Skill-based-Visual-Object-Navigation" class="headerlink" title="Interactive Semantic Map Representation for Skill-based Visual Object Navigation"></a>Interactive Semantic Map Representation for Skill-based Visual Object Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04107">http://arxiv.org/abs/2311.04107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatiana Zemskova, Aleksei Staroverov, Kirill Muravyev, Dmitry Yudin, Aleksandr Panov</li>
<li>for: 本研究旨在提出一种基于学习方法的移动机器人视觉对象导航方法，以提高移动机器人在室内环境中的导航质量。</li>
<li>methods: 该方法基于神经网络方法，通过误差估计值的反propagation进行权重调整，以形成在实体交互过程中的Scene semantic map。</li>
<li>results: 在Habitat环境中进行了大规模实验，并达到了与现有方法相比的显著提高在导航质量指标上。代码和自定义数据集在github.com&#x2F;AIRI-Institute&#x2F;skill-fusion上公开发布。<details>
<summary>Abstract</summary>
Visual object navigation using learning methods is one of the key tasks in mobile robotics. This paper introduces a new representation of a scene semantic map formed during the embodied agent interaction with the indoor environment. It is based on a neural network method that adjusts the weights of the segmentation model with backpropagation of the predicted fusion loss values during inference on a regular (backward) or delayed (forward) image sequence. We have implemented this representation into a full-fledged navigation approach called SkillTron, which can select robot skills from end-to-end policies based on reinforcement learning and classic map-based planning methods. The proposed approach makes it possible to form both intermediate goals for robot exploration and the final goal for object navigation. We conducted intensive experiments with the proposed approach in the Habitat environment, which showed a significant superiority in navigation quality metrics compared to state-of-the-art approaches. The developed code and used custom datasets are publicly available at github.com/AIRI-Institute/skill-fusion.
</details>
<details>
<summary>摘要</summary>
视觉对象导航使用学习方法是移动机器人领域中的关键任务之一。这篇论文介绍了一种新的场景Semantic地图表示方法，基于神经网络方法，在感知机器人与室内环境互动中调整分割模型的权重。我们在SkillTron全面导航方法中实现了这种表示方法，可以根据复制学习和经典地图基于规划方法选择机器人技能。该方法可以形成穿梭机器人的中间目标和最终导航目标。我们在Habitat环境中进行了广泛的实验，并显示了与当前方法相比 Navigation 质量指标有显著的superiority。我们在github.com/AIRI-Institute/skill-fusion上公开了代码和自定义数据集。
</details></li>
</ul>
<hr>
<h2 id="DeepPatent2-A-Large-Scale-Benchmarking-Corpus-for-Technical-Drawing-Understanding"><a href="#DeepPatent2-A-Large-Scale-Benchmarking-Corpus-for-Technical-Drawing-Understanding" class="headerlink" title="DeepPatent2: A Large-Scale Benchmarking Corpus for Technical Drawing Understanding"></a>DeepPatent2: A Large-Scale Benchmarking Corpus for Technical Drawing Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04098">http://arxiv.org/abs/2311.04098</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gofigure-lanl/figure-segmentation">https://github.com/gofigure-lanl/figure-segmentation</a></li>
<li>paper_authors: Kehinde Ajayi, Xin Wei, Martin Gryder, Winston Shields, Jian Wu, Shawn M. Jones, Michal Kucer, Diane Oyen</li>
<li>for: 该论文是为了推动计算机视觉和自然语言处理领域的进步，通过利用大量实际应用的数据。</li>
<li>methods: 该论文使用了大规模的设计专利文档来提供大于270万件技术图像，并从这些图像中提取了132,890个物体名称和22,394个视角点。</li>
<li>results: 该论文通过使用 DeepPatent2 数据集，实现了技术图像的描述功能，并且表明该数据集可以用于其他研究领域，如3D图像重建和图像检索。<details>
<summary>Abstract</summary>
Recent advances in computer vision (CV) and natural language processing have been driven by exploiting big data on practical applications. However, these research fields are still limited by the sheer volume, versatility, and diversity of the available datasets. CV tasks, such as image captioning, which has primarily been carried out on natural images, still struggle to produce accurate and meaningful captions on sketched images often included in scientific and technical documents. The advancement of other tasks such as 3D reconstruction from 2D images requires larger datasets with multiple viewpoints. We introduce DeepPatent2, a large-scale dataset, providing more than 2.7 million technical drawings with 132,890 object names and 22,394 viewpoints extracted from 14 years of US design patent documents. We demonstrate the usefulness of DeepPatent2 with conceptual captioning. We further provide the potential usefulness of our dataset to facilitate other research areas such as 3D image reconstruction and image retrieval.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)现代计算机视觉（CV）和自然语言处理技术的进步受到大规模实际应用的抽象启用。然而，这些研究领域仍受到可用数据的量、多样性和多样性的限制。CV任务，如图像描述，在自然图像上进行的主要任务仍然减少生成准确和有意义的描述 sketched 图像，常出现在科学和技术文档中。提高其他任务，如从 2D 图像中重建 3D 图像，需要更多的视图点。我们介绍 DeepPatent2，一个大规模数据集，包含 más de 2.7 万个技术图像，132,890 个物体名称和 22,394 个视图点，从美国设计专利文档中提取了 14 年。我们示出 DeepPatent2 的用于概念描述，并提供了其他研究领域，如 3D 图像重建和图像检索的潜在用途。
</details></li>
</ul>
<hr>
<h2 id="Image-Pointcloud-Fusion-based-Anomaly-Detection-using-PD-REAL-Dataset"><a href="#Image-Pointcloud-Fusion-based-Anomaly-Detection-using-PD-REAL-Dataset" class="headerlink" title="Image-Pointcloud Fusion based Anomaly Detection using PD-REAL Dataset"></a>Image-Pointcloud Fusion based Anomaly Detection using PD-REAL Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04095">http://arxiv.org/abs/2311.04095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianjian Qin, Chunzhi Gu, Jun Yu, Chao Zhang</li>
<li>For:  This paper is written for researchers and practitioners in the field of unsupervised anomaly detection (AD) in the 3D domain.* Methods: The paper uses a novel dataset called PD-REAL, which consists of Play-Doh models of 15 object categories with six types of anomalies, captured under different lighting conditions using a RealSense camera. The authors use state-of-the-art AD algorithms to evaluate the benefits and challenges of using 3D information in AD tasks.* Results: The paper shows that the PD-REAL dataset provides a controlled environment for analyzing the potential benefits of 3D information in AD tasks, and that the use of 3D information can improve the detection of anomalies compared to 2D-only representations. However, the authors also identify challenges in using 3D information, such as the need for more sophisticated algorithms to handle varying lighting conditions and object orientations.<details>
<summary>Abstract</summary>
We present PD-REAL, a novel large-scale dataset for unsupervised anomaly detection (AD) in the 3D domain. It is motivated by the fact that 2D-only representations in the AD task may fail to capture the geometric structures of anomalies due to uncertainty in lighting conditions or shooting angles. PD-REAL consists entirely of Play-Doh models for 15 object categories and focuses on the analysis of potential benefits from 3D information in a controlled environment. Specifically, objects are first created with six types of anomalies, such as dent, crack, or perforation, and then photographed under different lighting conditions to mimic real-world inspection scenarios. To demonstrate the usefulness of 3D information, we use a commercially available RealSense camera to capture RGB and depth images. Compared to the existing 3D dataset for AD tasks, the data acquisition of PD-REAL is significantly cheaper, easily scalable and easier to control variables. Extensive evaluations with state-of-the-art AD algorithms on our dataset demonstrate the benefits as well as challenges of using 3D information. Our dataset can be downloaded from https://github.com/Andy-cs008/PD-REAL
</details>
<details>
<summary>摘要</summary>
我们介绍PD-REAL，一个新的大规模未supervised anomaly detection（AD） dataset在3D领域。它受到了2D只 представiation在AD任务中可能无法捕捉异常的几何结构，因为照明条件或拍摄角度的不确定性。PD-REAL完全由Play-Doh模型组成15种物品类别，专注于控制环境中3D信息的分析。具体来说，物品首先创建了6种异常，如痕、裂、或者贯穿，然后在不同的照明条件下拍摄，以模拟实际的检查enario。为了证明3D信息的有用性，我们使用了一款商业可用的RealSense摄像头捕捉RGB和深度图像。相比已有的3Ddataset для AD任务，PD-REAL的数据取得更加便宜，易扩展和更容易控制变量。我们进行了state-of-the-art AD算法的广泛评估，以证明PD-REAL dataset的优点和挑战。您可以在https://github.com/Andy-cs008/PD-REAL上下载这个dataset。
</details></li>
</ul>
<hr>
<h2 id="Proceedings-of-the-5th-International-Workshop-on-Reading-Music-Systems"><a href="#Proceedings-of-the-5th-International-Workshop-on-Reading-Music-Systems" class="headerlink" title="Proceedings of the 5th International Workshop on Reading Music Systems"></a>Proceedings of the 5th International Workshop on Reading Music Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04091">http://arxiv.org/abs/2311.04091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suziai/gui-tools">https://github.com/suziai/gui-tools</a></li>
<li>paper_authors: Jorge Calvo-Zaragoza, Alexander Pacha, Elona Shatri</li>
<li>for: 这个论文是为了连接研究者们，论文主要关注在乐谱识别领域，与其他研究者和实践者进行交流和合作。</li>
<li>methods: 本论文使用了乐谱识别技术，包括图像处理、作者识别、多模态系统等方法。</li>
<li>results: 本论文介绍了5th International Workshop on Reading Music Systems的论文集，包括乐谱识别、数据集和性能评估、图像处理等主题。<details>
<summary>Abstract</summary>
The International Workshop on Reading Music Systems (WoRMS) is a workshop that tries to connect researchers who develop systems for reading music, such as in the field of Optical Music Recognition, with other researchers and practitioners that could benefit from such systems, like librarians or musicologists. The relevant topics of interest for the workshop include, but are not limited to: Music reading systems; Optical music recognition; Datasets and performance evaluation; Image processing on music scores; Writer identification; Authoring, editing, storing and presentation systems for music scores; Multi-modal systems; Novel input-methods for music to produce written music; Web-based Music Information Retrieval services; Applications and projects; Use-cases related to written music.   These are the proceedings of the 5th International Workshop on Reading Music Systems, held in Milan, Italy on Nov. 4th 2023.
</details>
<details>
<summary>摘要</summary>
世界音乐读取系统国际研讨会（WoRMS）是一个研讨会，旨在连接开发音乐读取系统的研究人员（如光学音乐识别）与其他研究人员和实践者（如图书管理员或音乐学家），以便共享知识和技术。研讨会的主题包括，但不限于：音乐读取系统；光学音乐识别；数据集和性能评估；图书馆管理系统；多模态系统；新的音乐输入方法来生成书面音乐；网络音乐信息检索服务；应用和项目；有关书面音乐的用例。这是第5届世界音乐读取系统国际研讨会的论文集，于2023年11月4日在意大利米兰举行。
</details></li>
</ul>
<hr>
<h2 id="Restoration-of-Analog-Videos-Using-Swin-UNet"><a href="#Restoration-of-Analog-Videos-Using-Swin-UNet" class="headerlink" title="Restoration of Analog Videos Using Swin-UNet"></a>Restoration of Analog Videos Using Swin-UNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04261">http://arxiv.org/abs/2311.04261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miccunifi/analog-video-restoration">https://github.com/miccunifi/analog-video-restoration</a></li>
<li>paper_authors: Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, Alberto Del Bimbo</li>
<li>for:  restaure analog videos of historical archives</li>
<li>methods: multi-frame approach, deal with severe tape mistracking</li>
<li>results: effective restoration of original content, tested on real-world videos from a major historical video archive<details>
<summary>Abstract</summary>
In this paper, we present a system to restore analog videos of historical archives. These videos often contain severe visual degradation due to the deterioration of their tape supports that require costly and slow manual interventions to recover the original content. The proposed system uses a multi-frame approach and is able to deal with severe tape mistracking, which results in completely scrambled frames. Tests on real-world videos from a major historical video archive show the effectiveness of our demo system. The code and the pre-trained model are publicly available at https://github.com/miccunifi/analog-video-restoration.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种系统来恢复历史档案中的分析视频。这些视频经常受到媒体支持的腐蚀，需要费时费力的手动操作来恢复原始内容。我们的系统采用多帧方法，能够有效地处理严重的卷积问题，从而恢复原始内容。我们在一个重要的历史视频档案中进行了实验，测试结果表明我们的示例系统的效果。我们的代码和预训练模型都可以在 GitHub 上获得，请参考 <https://github.com/miccunifi/analog-video-restoration>。
</details></li>
</ul>
<hr>
<h2 id="Learning-Super-Resolution-Ultrasound-Localization-Microscopy-from-Radio-Frequency-Data"><a href="#Learning-Super-Resolution-Ultrasound-Localization-Microscopy-from-Radio-Frequency-Data" class="headerlink" title="Learning Super-Resolution Ultrasound Localization Microscopy from Radio-Frequency Data"></a>Learning Super-Resolution Ultrasound Localization Microscopy from Radio-Frequency Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04081">http://arxiv.org/abs/2311.04081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Hahne, Georges Chabouh, Olivier Couture, Raphael Sznitman</li>
<li>for: 本研究旨在提高ultrasound localization microscopy（ULM）的分辨率性能，通过快速和高效地地标定目标位置。</li>
<li>methods: 本研究使用无处理Radio-Frequency（RF）数据，并通过超解像网络进行地标定。为此，我们实现了标点投影和反向点 transform between B-mode和RF坐标空间。</li>
<li>results: 对于state-of-the-art技术的比较，我们的RF训练网络表明，不使用延迟和总和（DAS）扫描 beamforming可以更好地优化ULM的分辨率性能。<details>
<summary>Abstract</summary>
Ultrasound Localization Microscopy (ULM) enables imaging of vascular structures in the micrometer range by accumulating contrast agent particle locations over time. Precise and efficient target localization accuracy remains an active research topic in the ULM field to further push the boundaries of this promising medical imaging technology. Existing work incorporates Delay-And-Sum (DAS) beamforming into particle localization pipelines, which ultimately determines the ULM image resolution capability. In this paper we propose to feed unprocessed Radio-Frequency (RF) data into a super-resolution network while bypassing DAS beamforming and its limitations. To facilitate this, we demonstrate label projection and inverse point transformation between B-mode and RF coordinate space as required by our approach. We assess our method against state-of-the-art techniques based on a public dataset featuring in silico and in vivo data. Results from our RF-trained network suggest that excluding DAS beamforming offers a great potential to optimize on the ULM resolution performance.
</details>
<details>
<summary>摘要</summary>
Ultrasound Localization Microscopy (ULM) 可以在微米级别快速成像血管结构，通过时间积累对比杂素点的位置。为了进一步推动这项承诺的医疗成像技术，精准和高效的目标定位精度仍然是ULM领域的活跃研究话题。现有的工作将延迟和总和（DAS）扩散 beamforming 集成到参与者localization管道中，从而决定ULM图像分辨率能力。在这篇论文中，我们提议将未处理的Radio-Frequency（RF）数据feed到超分辨网络中，而不是通过DAS扩散 beamforming 和其局限性。为此，我们展示了标签投影和反向点变换 междуB-mode和RF坐标空间，这些步骤是我们方法所需的。我们对比了我们的RF训练网络和现有技术，根据公共数据集上的响应Silico和in vivo数据。结果表明，不包括DAS扩散 beamforming 可以提高ULM的分辨率性能。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Lane-Perception-and-Topology-Understanding-with-Standard-Definition-Navigation-Maps"><a href="#Augmenting-Lane-Perception-and-Topology-Understanding-with-Standard-Definition-Navigation-Maps" class="headerlink" title="Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps"></a>Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04079">http://arxiv.org/abs/2311.04079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katie Z Luo, Xinshuo Weng, Yan Wang, Shuang Wu, Jie Li, Kilian Q Weinberger, Yue Wang, Marco Pavone</li>
<li>for: 实时赛道预测</li>
<li>methods: 使用 Standard Definition (SD) 地图和 Transformer 数据 Representations from transFormers 进行赛道预测</li>
<li>results: 与现有的 online map prediction 方法相比，提高了赛道探测和预测效率（最高提升60%），可以立即适用于任何 Transformer 基本的赛道预测方法。<details>
<summary>Abstract</summary>
Autonomous driving has traditionally relied heavily on costly and labor-intensive High Definition (HD) maps, hindering scalability. In contrast, Standard Definition (SD) maps are more affordable and have worldwide coverage, offering a scalable alternative. In this work, we systematically explore the effect of SD maps for real-time lane-topology understanding. We propose a novel framework to integrate SD maps into online map prediction and propose a Transformer-based encoder, SD Map Encoder Representations from transFormers, to leverage priors in SD maps for the lane-topology prediction task. This enhancement consistently and significantly boosts (by up to 60%) lane detection and topology prediction on current state-of-the-art online map prediction methods without bells and whistles and can be immediately incorporated into any Transformer-based lane-topology method. Code is available at https://github.com/NVlabs/SMERF.
</details>
<details>
<summary>摘要</summary>
自适应驾驶曾经依赖高Definition（HD）地图，导致扩展性受限。相比之下，标准Definition（SD）地图更加可 affordable，具有全球覆盖，提供可扩展的 alternativa。在这种工作中，我们系统地探讨了SD地图对实时干道结构理解的效果。我们提议一种将SD地图集成到在线地图预测中的框架，并使用Transformer基于的编码器，即SD Map Encoder Representations from transFormers，以利用SD地图中的假设来提高干道结构预测任务。这种改进可以在当前领先的在线地图预测方法上提高（最高达60%）干道检测和结构预测，无需额外的套件和配置，可以立即 integrate into任何Transformer基于的干道结构方法。代码可以在https://github.com/NVlabs/SMERF中找到。
</details></li>
</ul>
<hr>
<h2 id="Energy-based-Calibrated-VAE-with-Test-Time-Free-Lunch"><a href="#Energy-based-Calibrated-VAE-with-Test-Time-Free-Lunch" class="headerlink" title="Energy-based Calibrated VAE with Test Time Free Lunch"></a>Energy-based Calibrated VAE with Test Time Free Lunch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04071">http://arxiv.org/abs/2311.04071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Luo, Siya Qiu, Xingjian Tao, Yujun Cai, Jing Tang</li>
<li>for: 提高 Variational Autoencoders (VAEs) 的采样效率和生成质量</li>
<li>methods: 使用 Conditional EBM 进行采样和生成，无需 MCMC 抽样</li>
<li>results: 提出了一种能够在训练和测试阶段都不需要 MCMC 抽样的 Energy-Calibrated Generative Model，并且在多个应用中达到了state-of-the-art 性能。<details>
<summary>Abstract</summary>
In this paper, we propose a novel Energy-Calibrated Generative Model that utilizes a Conditional EBM for enhancing Variational Autoencoders (VAEs). VAEs are sampling efficient but often suffer from blurry generation results due to the lack of training in the generative direction. On the other hand, Energy-Based Models (EBMs) can generate high-quality samples but require expensive Markov Chain Monte Carlo (MCMC) sampling. To address these issues, we introduce a Conditional EBM for calibrating the generative direction during training, without requiring it for test time sampling. Our approach enables the generative model to be trained upon data and calibrated samples with adaptive weight, thereby enhancing efficiency and effectiveness without necessitating MCMC sampling in the inference phase. We also show that the proposed approach can be extended to calibrate normalizing flows and variational posterior. Moreover, we propose to apply the proposed method to zero-shot image restoration via neural transport prior and range-null theory. We demonstrate the effectiveness of the proposed method through extensive experiments in various applications, including image generation and zero-shot image restoration. Our method shows state-of-the-art performance over single-step non-adversarial generation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的能量准确生成模型，该模型利用 conditional EBM 来增强变量自动编码器（VAE）。 VAE 可以高效地采样，但通常因缺乏生成方向的训练而产生模糊的生成结果。而EBM 可以生成高质量的样本，但需要昂贵的 Markov Chain Monte Carlo（MCMC）采样。为了解决这些问题，我们引入了 conditional EBM，以在训练期间准确调整生成方向，不需要测试阶段的 MCMC 采样。我们的方法使得生成模型可以在数据和适应性权重的指导下接受训练，从而提高效率和效果，不需要测试阶段的 MCMC 采样。此外，我们还证明了该方法可以扩展到 calibrate normalizing flows 和 variational posterior。此外，我们还提出了在 zero-shot 图像恢复中应用该方法，使用神经运输前向和范围null理论。我们通过多种应用，包括图像生成和 zero-shot 图像恢复，展示了我们的方法的效果。我们的方法在单步非对抗生成中显示了状态级表现。
</details></li>
</ul>
<hr>
<h2 id="LISBET-a-self-supervised-Transformer-model-for-the-automatic-segmentation-of-social-behavior-motifs"><a href="#LISBET-a-self-supervised-Transformer-model-for-the-automatic-segmentation-of-social-behavior-motifs" class="headerlink" title="LISBET: a self-supervised Transformer model for the automatic segmentation of social behavior motifs"></a>LISBET: a self-supervised Transformer model for the automatic segmentation of social behavior motifs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04069">http://arxiv.org/abs/2311.04069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Chindemi, Benoit Girard, Camilla Bellone</li>
<li>For: The paper aims to understand the core principles of social behavior and identify potential therapeutic targets for addressing social deficits.* Methods: The paper introduces a new model called LISBET, which uses self-supervised learning to detect and quantify social behaviors from dynamic body parts tracking data.* Results: The paper shows that LISBET can be used in both hypothesis-driven and discovery-driven modes to automate behavior classification and segment social behavior motifs, and that the recognized motifs closely match human annotations and correlate with the electrophysiological activity of dopaminergic neurons in the Ventral Tegmental Area (VTA).Here is the same information in Simplified Chinese:* For: 该研究旨在更深入地理解社会行为的核心原理，并identify potential therapeutic targets for addressing social deficits.* Methods: 该研究提出了一种新的模型called LISBET，该模型使用无监督学习来探索和量化社会行为的动态身体部分跟踪数据。* Results: 研究发现，LISBET可以在假设驱动和发现驱动两种模式下使用，以自动化行为分类和社会行为模式识别。Recognized模式不仅准确地匹配人类注释，还与 Ventral Tegmental Area (VTA) dopamine neurons的电physiological活动相关。<details>
<summary>Abstract</summary>
Social behavior, defined as the process by which individuals act and react in response to others, is crucial for the function of societies and holds profound implications for mental health. To fully grasp the intricacies of social behavior and identify potential therapeutic targets for addressing social deficits, it is essential to understand its core principles. Although machine learning algorithms have made it easier to study specific aspects of complex behavior, current methodologies tend to focus primarily on single-animal behavior. In this study, we introduce LISBET (seLf-supervIsed Social BEhavioral Transformer), a model designed to detect and segment social interactions. Our model eliminates the need for feature selection and extensive human annotation by using self-supervised learning to detect and quantify social behaviors from dynamic body parts tracking data. LISBET can be used in hypothesis-driven mode to automate behavior classification using supervised finetuning, and in discovery-driven mode to segment social behavior motifs using unsupervised learning. We found that motifs recognized using the discovery-driven approach not only closely match the human annotations but also correlate with the electrophysiological activity of dopaminergic neurons in the Ventral Tegmental Area (VTA). We hope LISBET will help the community improve our understanding of social behaviors and their neural underpinnings.
</details>
<details>
<summary>摘要</summary>
社会行为，定义为个体在响应他人的行为和反应过程，对社会的功能至关重要，对心理健康也有深远的影响。为了全面理解社会行为的复杂性和潜在的治疗目标，我们需要了解其核心原理。虽然机器学习算法已经使得研究特定方面的复杂行为变得更加容易，但现有方法ologies tend to focus primarily on single-animal behavior.在这种情况下，我们介绍了LISBET（seLf-supervIsed Social BEhavioral Transformer）模型，用于检测和分类社交互动。我们的模型不需要特定的特征选择和大量的人类标注，通过自动学习检测和跟踪动体部分数据来检测和量化社交行为。LISBET可以在假设驱动模式下自动分类行为，以及在发现驱动模式下分类社交行为模式。我们发现使用发现驱动模式分类的模式与人类标注非常相似，并且与 dopaminergic neurons的电physiological 活动在腹囊核（VTA）也存在相似性。我们希望LISBET能够帮助社区更好地理解社交行为和其神经基础。
</details></li>
</ul>
<hr>
<h2 id="mmFUSION-Multimodal-Fusion-for-3D-Objects-Detection"><a href="#mmFUSION-Multimodal-Fusion-for-3D-Objects-Detection" class="headerlink" title="mmFUSION: Multimodal Fusion for 3D Objects Detection"></a>mmFUSION: Multimodal Fusion for 3D Objects Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04058">http://arxiv.org/abs/2311.04058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javed Ahmad, Alessio Del Bue</li>
<li>for: 本研究旨在提出一种新的中途级多模态融合（mmFUSION）方法，以解决自驾系统中多感器融合的挑战。</li>
<li>methods:  mmFUSION使用了每个感知器 separately compute特征，然后通过cross-modality和多模态注意力机制进行融合。</li>
<li>results: 在KITTI和NuScenes dataset上测试，mmFUSION的性能比EARLY、INTERMEDIATE、LATE和两阶段融合方案更好，并且可以保持多模态信息并学习补做模态缺陷。<details>
<summary>Abstract</summary>
Multi-sensor fusion is essential for accurate 3D object detection in self-driving systems. Camera and LiDAR are the most commonly used sensors, and usually, their fusion happens at the early or late stages of 3D detectors with the help of regions of interest (RoIs). On the other hand, fusion at the intermediate level is more adaptive because it does not need RoIs from modalities but is complex as the features of both modalities are presented from different points of view. In this paper, we propose a new intermediate-level multi-modal fusion (mmFUSION) approach to overcome these challenges. First, the mmFUSION uses separate encoders for each modality to compute features at a desired lower space volume. Second, these features are fused through cross-modality and multi-modality attention mechanisms proposed in mmFUSION. The mmFUSION framework preserves multi-modal information and learns to complement modalities' deficiencies through attention weights. The strong multi-modal features from the mmFUSION framework are fed to a simple 3D detection head for 3D predictions. We evaluate mmFUSION on the KITTI and NuScenes dataset where it performs better than available early, intermediate, late, and even two-stage based fusion schemes. The code with the mmdetection3D project plugin will be publicly available soon.
</details>
<details>
<summary>摘要</summary>
多感器融合是自动驾驶系统中准确的三维物体探测的关键。相机和激光激光是最常用的感知器，通常在3D探测器的早期或晚期阶段进行融合，使用区域关注（RoI）。然而，中间阶段的融合更加适应，因为它不需要modalities的RoI，但是复杂度较高，因为两种感知器的特征从不同的角度出现。在这篇论文中，我们提出了一种新的中间阶段多模态融合（mmFUSION）方法来解决这些挑战。首先，mmFUSION使用每个感知器的分离Encoder计算特征，以实现感知器的特征空间减小。其次，这些特征通过跨模态和多模态注意机制进行融合。mmFUSION框架保留了多模态信息，并通过注意weight学习补做每个感知器的不足。强大的多模态特征从mmFUSION框架中来的是 fed into a simple 3D detection head for 3D predictions。我们在KITTI和NuScenes数据集上评估了mmFUSION，其性能高于现有的早期、中间、晚期和两个阶段融合方案。代码将在near future通过mmdetection3D项目插件公开。
</details></li>
</ul>
<hr>
<h2 id="Generative-Structural-Design-Integrating-BIM-and-Diffusion-Model"><a href="#Generative-Structural-Design-Integrating-BIM-and-Diffusion-Model" class="headerlink" title="Generative Structural Design Integrating BIM and Diffusion Model"></a>Generative Structural Design Integrating BIM and Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04052">http://arxiv.org/abs/2311.04052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili He, Yu-Hsing Wang, Jian Zhang</li>
<li>For: This paper proposes a comprehensive solution for intelligent structural design using AI, with a focus on improving the perceptual quality and details of generations.* Methods: The paper introduces building information modeling (BIM) into intelligent structural design and establishes a structural design pipeline integrating BIM and generative AI. It also proposes a novel 2-stage generation framework, uses diffusion models (DMs) instead of generative adversarial network (GAN)-based models, and designs an attention block (AB) consisting of a self-attention block (SAB) and a parallel cross-attention block (PCAB) to facilitate cross-domain data fusion.* Results: The paper demonstrates the powerful generation and representation capabilities of the proposed method through quantitative and qualitative results, and shows that DMs have the potential to replace GANs and become the new benchmark for generative problems in civil engineering.<details>
<summary>Abstract</summary>
Intelligent structural design using AI can effectively reduce time overhead and increase efficiency. It has potential to become the new design paradigm in the future to assist and even replace engineers, and so it has become a research hotspot in the academic community. However, current methods have some limitations to be addressed, whether in terms of application scope, visual quality of generated results, or evaluation metrics of results. This study proposes a comprehensive solution. Firstly, we introduce building information modeling (BIM) into intelligent structural design and establishes a structural design pipeline integrating BIM and generative AI, which is a powerful supplement to the previous frameworks that only considered CAD drawings. In order to improve the perceptual quality and details of generations, this study makes 3 contributions. Firstly, in terms of generation framework, inspired by the process of human drawing, a novel 2-stage generation framework is proposed to replace the traditional end-to-end framework to reduce the generation difficulty for AI models. Secondly, in terms of generative AI tools adopted, diffusion models (DMs) are introduced to replace widely used generative adversarial network (GAN)-based models, and a novel physics-based conditional diffusion model (PCDM) is proposed to consider different design prerequisites. Thirdly, in terms of neural networks, an attention block (AB) consisting of a self-attention block (SAB) and a parallel cross-attention block (PCAB) is designed to facilitate cross-domain data fusion. The quantitative and qualitative results demonstrate the powerful generation and representation capabilities of PCDM. Necessary ablation studies are conducted to examine the validity of the methods. This study also shows that DMs have the potential to replace GANs and become the new benchmark for generative problems in civil engineering.
</details>
<details>
<summary>摘要</summary>
使用人工智能进行智能结构设计可以有效减少时间开销并提高效率。它在未来可能成为新的设计 парадигма，帮助或代替工程师，因此在学术社区中引起了广泛的研究兴趣。然而，当前的方法还有一些需要解决的限制，包括应用范围、生成结果的视觉质量和评价指标。本研究提出了一个全面的解决方案。首先，我们将建筑信息模型（BIM）引入智能结构设计，并建立了基于BIM和生成AI的结构设计管道，这是之前的框架仅考虑CAD图形的增强。为了提高生成结果的视觉质量和细节，本研究做出了3个贡献。首先，在生成框架方面，我们提出了一种基于人类绘图过程的新型二阶生成框架，以降低AI模型生成难度。其次，在生成AI工具方面，我们引入了扩散模型（DM），取代了广泛使用的生成对抗网络（GAN）模型，并提出了一种基于物理条件的条件扩散模型（PCDM），以考虑不同的设计前提。最后，在神经网络方面，我们设计了一个注意块（AB），包括一个自注意块（SAB）和一个平行交叉注意块（PCAB），以便进行跨领域数据融合。量化和质量 результаados表明了PCDM的强大生成和表示能力。进行必要的减少研究以确保方法的有效性。此外，我们还发现了DMs可能取代GANs，成为未来的生成问题的新标准。
</details></li>
</ul>
<hr>
<h2 id="3D-EAGAN-3D-edge-aware-attention-generative-adversarial-network-for-prostate-segmentation-in-transrectal-ultrasound-images"><a href="#3D-EAGAN-3D-edge-aware-attention-generative-adversarial-network-for-prostate-segmentation-in-transrectal-ultrasound-images" class="headerlink" title="3D EAGAN: 3D edge-aware attention generative adversarial network for prostate segmentation in transrectal ultrasound images"></a>3D EAGAN: 3D edge-aware attention generative adversarial network for prostate segmentation in transrectal ultrasound images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04049">http://arxiv.org/abs/2311.04049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengqing Liu, Xiao Shao, Liping Jiang, Kaizhi Wu</li>
<li>for: 这个研究的目的是为了发展一种高效的肿瘤脉冲影像中的肿瘤分类方法，以扩展现有的分类方法，并且能够更好地处理肿瘤的不均匀组织和边缘信息。</li>
<li>methods: 这个方法使用了一个3D缘点注意力生成对抗网络（3D EAGAN），它包括一个缘点注意力分 segmentation 网络（EASNet）和一个判别网络，用于识别预测的肿瘤和实际的肿瘤。 EASNet 由一个encoder-decoder 结构的 U-Net 背景网络、一个细节补偿模组、四个3D 空间和通道注意力模组、一个缘点增强模组和一个全局特征提取器组成。</li>
<li>results: 这个方法可以实现高度的肿瘤分类精度，并且可以优化肿瘤的边缘信息和细节特征。<details>
<summary>Abstract</summary>
Automatic prostate segmentation in TRUS images has always been a challenging problem, since prostates in TRUS images have ambiguous boundaries and inhomogeneous intensity distribution. Although many prostate segmentation methods have been proposed, they still need to be improved due to the lack of sensibility to edge information. Consequently, the objective of this study is to devise a highly effective prostate segmentation method that overcomes these limitations and achieves accurate segmentation of prostates in TRUS images. A 3D edge-aware attention generative adversarial network (3D EAGAN)-based prostate segmentation method is proposed in this paper, which consists of an edge-aware segmentation network (EASNet) that performs the prostate segmentation and a discriminator network that distinguishes predicted prostates from real prostates. The proposed EASNet is composed of an encoder-decoder-based U-Net backbone network, a detail compensation module, four 3D spatial and channel attention modules, an edge enhance module, and a global feature extractor. The detail compensation module is proposed to compensate for the loss of detailed information caused by the down-sampling process of the encoder. The features of the detail compensation module are selectively enhanced by the 3D spatial and channel attention module. Furthermore, an edge enhance module is proposed to guide shallow layers in the EASNet to focus on contour and edge information in prostates. Finally, features from shallow layers and hierarchical features from the decoder module are fused through the global feature extractor to predict the segmentation prostates.
</details>
<details>
<summary>摘要</summary>
自动肾脏分割在TRUS图像中 siempre ha sido un problema desafiante, ya que las prostata en las imágenes TRUS tienen límites ambiguos y distribución no homogénea de intensidad. A pesar de que muchos métodos de segmentación de próstata han sido propuestos, todavía necesitan mejorar debido a la falta de sensibilidad a la información de borde. Por lo tanto, el objetivo de este estudio es desarrollar un método de segmentación de próstata altamente efectivo que supera estas limitaciones y realiza una segmentación precisa de las prostata en las imágenes TRUS.Se propone un método de segmentación de próstata basado en una red de generador de adversarios 3D edge-aware (3D EAGAN), que consta de una red de segmentación edge-aware (EASNet) que realiza la segmentación de próstata y una red de discriminador que distingue las prostata predichas de las reales. La red EASNet está compuesta por una backbone network U-Net basada en una red de encoder-decodificador, un módulo de compensación de detalles, cuatro módulos de atención espacial y de canal 3D, un módulo de mejora de borde y un extractor de características globales. El módulo de compensación de detalles se propone para compensar la pérdida de información detallada causada por el proceso de down-sampling del encoder. Las características del módulo de compensación de detalles se seleccionan y se enfocan por el módulo de atención espacial y de canal 3D. Además, se propone un módulo de mejora de borde para guiar las capas superficiales de la red EASNet para que se centren en la información de contorno y borde en las prostata. Finalmente, las características de las capas superficiales y las características jerárquicas del módulo de descodificador se fusionan a través del extractor de características globales para predecir la segmentación de próstata.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Near-Infrared-Hyperspectral-Imaging-for-Protein-Content-Regression-and-Grain-Variety-Classification-Using-Bulk-References-and-Varying-Grain-to-Background-Ratios"><a href="#Analyzing-Near-Infrared-Hyperspectral-Imaging-for-Protein-Content-Regression-and-Grain-Variety-Classification-Using-Bulk-References-and-Varying-Grain-to-Background-Ratios" class="headerlink" title="Analyzing Near-Infrared Hyperspectral Imaging for Protein Content Regression and Grain Variety Classification Using Bulk References and Varying Grain-to-Background Ratios"></a>Analyzing Near-Infrared Hyperspectral Imaging for Protein Content Regression and Grain Variety Classification Using Bulk References and Varying Grain-to-Background Ratios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04042">http://arxiv.org/abs/2311.04042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ole-Christian Galbo Engstrøm, Erik Schou Dreier, Birthe Møller Jespersen, Kim Steenstrup Pedersen</li>
<li>for: 本研究使用 Near-Infrared Hyperspectral Imaging (NIR-HSI) 图像来调整模型，主要关注蛋白内含量预测和谷物种类分类两个任务。</li>
<li>methods: 研究者使用了限制参考数据来扩大蛋白内含量的预测，通过下游采样和相关联系来减少预测分布的偏见。然而，这种方法引入了明显的偏见，影响了PLS-R 和深度 CNN 模型的预测。研究者提议了一些纠正方法来减少这些偏见。</li>
<li>results: 研究者发现，高比例的谷物至背景图像在两个任务中具有更高的预测精度。然而，包含较低比例的图像在核心化过程中可以增强模型的Robustness。<details>
<summary>Abstract</summary>
Based on previous work, we assess the use of NIR-HSI images for calibrating models on two datasets, focusing on protein content regression and grain variety classification. Limited reference data for protein content is expanded by subsampling and associating it with the bulk sample. However, this method introduces significant biases due to skewed leptokurtic prediction distributions, affecting both PLS-R and deep CNN models. We propose adjustments to mitigate these biases, improving mean protein reference predictions. Additionally, we investigate the impact of grain-to-background ratios on both tasks. Higher ratios yield more accurate predictions, but including lower-ratio images in calibration enhances model robustness for such scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-exploitation-multi-task-learning-of-object-detection-and-semantic-segmentation-on-partially-annotated-data"><a href="#Data-exploitation-multi-task-learning-of-object-detection-and-semantic-segmentation-on-partially-annotated-data" class="headerlink" title="Data exploitation: multi-task learning of object detection and semantic segmentation on partially annotated data"></a>Data exploitation: multi-task learning of object detection and semantic segmentation on partially annotated data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04040">http://arxiv.org/abs/2311.04040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoàng-Ân Lê, Minh-Tan Pham</li>
<li>for: 这篇论文主要探讨了多任务部分标注数据的 JOINT 学习，以扩展物探测和 semantic segmentation 两个最受欢迎的视觉任务的性能。</li>
<li>methods: 本论文使用了多任务学习和知识传播来将两个任务结合在一起，并进行了广泛的实验测试来评估每个任务的性能和弹性。</li>
<li>results: 实验结果显示， JOINT 学习和知识传播可以提高多任务学习的性能，并且在无法同时优化两个任务的情况下，可以提供更好的性能。<details>
<summary>Abstract</summary>
Multi-task partially annotated data where each data point is annotated for only a single task are potentially helpful for data scarcity if a network can leverage the inter-task relationship. In this paper, we study the joint learning of object detection and semantic segmentation, the two most popular vision problems, from multi-task data with partial annotations. Extensive experiments are performed to evaluate each task performance and explore their complementarity when a multi-task network cannot optimize both tasks simultaneously. We propose employing knowledge distillation to leverage joint-task optimization. The experimental results show favorable results for multi-task learning and knowledge distillation over single-task learning and even full supervision scenario. All code and data splits are available at https://github.com/lhoangan/multas
</details>
<details>
<summary>摘要</summary>
多任务半标注数据，每个数据点只有一个任务的标注，在数据缺乏时可能是有帮助的。在这篇论文中，我们研究了对象检测和 semantic segmentation 两个最受欢迎的视觉问题的共同学习，从多任务数据中获得了半标注。我们进行了广泛的实验来评估每个任务的性能，并探索它们之间的补偿性。我们提出了使用知识传授来利用共同优化。实验结果表明，多任务学习和知识传授在单任务学习和完整监督方式之上具有有利的效果。所有代码和数据分割可以在 GitHub 上找到：https://github.com/lhoangan/multas。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Dataset-Scale-Indicators-of-Data-Quality"><a href="#Exploring-Dataset-Scale-Indicators-of-Data-Quality" class="headerlink" title="Exploring Dataset-Scale Indicators of Data Quality"></a>Exploring Dataset-Scale Indicators of Data Quality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04016">http://arxiv.org/abs/2311.04016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Feuer, Chinmay Hegde</li>
<li>for: 这个论文主要用于探讨计算机视觉基础模型的训练所需的数据质量。</li>
<li>methods: 该论文使用了两个重要的数据集级别组成部分来评估数据质量：标签集设计和类别均衡。</li>
<li>results: 研究人员通过监测这些指标来预测模型的性能和对分布变化的Robustness。<details>
<summary>Abstract</summary>
Modern computer vision foundation models are trained on massive amounts of data, incurring large economic and environmental costs. Recent research has suggested that improving data quality can significantly reduce the need for data quantity. But what constitutes data quality in computer vision? We posit that the quality of a given dataset can be decomposed into distinct sample-level and dataset-level constituents, and that the former have been more extensively studied than the latter. We ablate the effects of two important dataset-level constituents: label set design, and class balance. By monitoring these constituents using key indicators we provide, researchers and practitioners can better anticipate model performance, measured in terms of its accuracy and robustness to distribution shifts.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉基础模型通常通过巨量数据训练，产生大量经济和环境成本。 latest research suggests that improving data quality can significantly reduce the need for data quantity. But what constitutes data quality in computer vision? We argue that the quality of a given dataset can be decomposed into two distinct constituents: sample-level and dataset-level. While the former has been more extensively studied, the latter has been relatively underexplored. We investigate the effects of two important dataset-level constituents: label set design and class balance. By monitoring these constituents using key indicators we provide, researchers and practitioners can better anticipate model performance, measured in terms of its accuracy and robustness to distribution shifts.Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="AGNES-Abstraction-guided-Framework-for-Deep-Neural-Networks-Security"><a href="#AGNES-Abstraction-guided-Framework-for-Deep-Neural-Networks-Security" class="headerlink" title="AGNES: Abstraction-guided Framework for Deep Neural Networks Security"></a>AGNES: Abstraction-guided Framework for Deep Neural Networks Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04009">http://arxiv.org/abs/2311.04009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay Dhonthi, Marcello Eiermann, Ernst Moritz Hahn, Vahid Hashemi</li>
<li>for: 该论文旨在检测 Deep Neural Networks (DNNs) 中的后门，以确保图像识别 task 的正确性。</li>
<li>methods: 该论文基于一种新的方法 AGNES，用于检测 DNNs 中的后门。该方法基于一种新的特征提取技术，可以准确地检测后门。</li>
<li>results: 作者通过多个 relevante case studies 表明，AGNES 比许多现有的方法表现更好，可以准确地检测 DNNs 中的后门。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) are becoming widespread, particularly in safety-critical areas. One prominent application is image recognition in autonomous driving, where the correct classification of objects, such as traffic signs, is essential for safe driving. Unfortunately, DNNs are prone to backdoors, meaning that they concentrate on attributes of the image that should be irrelevant for their correct classification. Backdoors are integrated into a DNN during training, either with malicious intent (such as a manipulated training process, because of which a yellow sticker always leads to a traffic sign being recognised as a stop sign) or unintentional (such as a rural background leading to any traffic sign being recognised as animal crossing, because of biased training data).   In this paper, we introduce AGNES, a tool to detect backdoors in DNNs for image recognition. We discuss the principle approach on which AGNES is based. Afterwards, we show that our tool performs better than many state-of-the-art methods for multiple relevant case studies.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了 AGNES，一种用于检测 DNNs 中的后门的工具。我们讲述了 AGNES 的原理方法。然后，我们表明了我们的工具在多个相关的案例研究中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Bias-and-Diversity-in-Synthetic-based-Face-Recognition"><a href="#Bias-and-Diversity-in-Synthetic-based-Face-Recognition" class="headerlink" title="Bias and Diversity in Synthetic-based Face Recognition"></a>Bias and Diversity in Synthetic-based Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03970">http://arxiv.org/abs/2311.03970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Huber, Anh Thi Luu, Fadi Boutros, Arjan Kuijper, Naser Damer</li>
<li>for: 本研究旨在investigate synthetic face recognition dataset的多样性和生成模型训练数据的影响，以及这些模型对不同属性的偏见。</li>
<li>methods: 本研究使用了三种最新的生成模型，并对这些模型的训练数据进行分析，以了解生成的人脸数据的分布和偏见。</li>
<li>results: 研究结果显示，生成模型可以生成与训练数据的分布相似的人脸数据，但是这些模型仍然存在偏见问题，尤其是对 gender、ethnicity、age 和head position 等属性的偏见。<details>
<summary>Abstract</summary>
Synthetic data is emerging as a substitute for authentic data to solve ethical and legal challenges in handling authentic face data. The current models can create real-looking face images of people who do not exist. However, it is a known and sensitive problem that face recognition systems are susceptible to bias, i.e. performance differences between different demographic and non-demographics attributes, which can lead to unfair decisions. In this work, we investigate how the diversity of synthetic face recognition datasets compares to authentic datasets, and how the distribution of the training data of the generative models affects the distribution of the synthetic data. To do this, we looked at the distribution of gender, ethnicity, age, and head position. Furthermore, we investigated the concrete bias of three recent synthetic-based face recognition models on the studied attributes in comparison to a baseline model trained on authentic data. Our results show that the generator generate a similar distribution as the used training data in terms of the different attributes. With regard to bias, it can be seen that the synthetic-based models share a similar bias behavior with the authentic-based models. However, with the uncovered lower intra-identity attribute consistency seems to be beneficial in reducing bias.
</details>
<details>
<summary>摘要</summary>
“人工数据正在成为真实数据的替代品，以解决伦理和法律问题。现有的模型可以创建真实看起来的人脸图像，但是知道的敏感问题是，人脸识别系统受到偏见的影响，即不同的民族和非民族属性之间的性能差异，可能导致不公正的决策。在这项工作中，我们研究了人工数据的多样性与真实数据的多样性之间的关系，以及生成模型的训练数据分布对人工数据的影响。我们分析了性别、民族、年龄和头部位置等属性的分布。此外，我们还研究了三种最新的人工数据基于面Recognition模型对这些属性的偏见情况，与基准模型在真实数据上训练的情况进行比较。我们的结果表明，生成器对不同属性的分布具有类似的分布，而且与真实数据上的偏见行为也很相似。然而，通过降低内部同一个属性的一致性，可以减少偏见。”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CeCNN-Copula-enhanced-convolutional-neural-networks-in-joint-prediction-of-refraction-error-and-axial-length-based-on-ultra-widefield-fundus-images"><a href="#CeCNN-Copula-enhanced-convolutional-neural-networks-in-joint-prediction-of-refraction-error-and-axial-length-based-on-ultra-widefield-fundus-images" class="headerlink" title="CeCNN: Copula-enhanced convolutional neural networks in joint prediction of refraction error and axial length based on ultra-widefield fundus images"></a>CeCNN: Copula-enhanced convolutional neural networks in joint prediction of refraction error and axial length based on ultra-widefield fundus images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03967">http://arxiv.org/abs/2311.03967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Zhong, Yang Li, Danjuan Yang, Meiyan Li, Xingyao Zhou, Bo Fu, Catherine C. Liu, A. H. Welsh</li>
<li>for: 这个研究旨在应用深度学习模型来预测视力短暂性和视力过度问题，并使用多重回应任务来提高预测精度。</li>
<li>methods: 研究使用了高维度数据生成模型（CeCNN），具有抽象特征和依赖关系模型，以提高预测精度。</li>
<li>results: 研究发现，使用CeCNN模型可以提高预测精度，并且可以在不同的背景和实验设计下运行。<details>
<summary>Abstract</summary>
Ultra-widefield (UWF) fundus images are replacing traditional fundus images in screening, detection, prediction, and treatment of complications related to myopia because their much broader visual range is advantageous for highly myopic eyes. Spherical equivalent (SE) is extensively used as the main myopia outcome measure, and axial length (AL) has drawn increasing interest as an important ocular component for assessing myopia. Cutting-edge studies show that SE and AL are strongly correlated. Using the joint information from SE and AL is potentially better than using either separately. In the deep learning community, though there is research on multiple-response tasks with a 3D image biomarker, dependence among responses is only sporadically taken into consideration. Inspired by the spirit that information extracted from the data by statistical methods can improve the prediction accuracy of deep learning models, we formulate a class of multivariate response regression models with a higher-order tensor biomarker, for the bivariate tasks of regression-classification and regression-regression. Specifically, we propose a copula-enhanced convolutional neural network (CeCNN) framework that incorporates the dependence between responses through a Gaussian copula (with parameters estimated from a warm-up CNN) and uses the induced copula-likelihood loss with the backbone CNNs. We establish the statistical framework and algorithms for the aforementioned two bivariate tasks. We show that the CeCNN has better prediction accuracy after adding the dependency information to the backbone models. The modeling and the proposed CeCNN algorithm are applicable beyond the UWF scenario and can be effective with other backbones beyond ResNet and LeNet.
</details>
<details>
<summary>摘要</summary>
ultra-widefield (UWF) fundus images  replacing traditional fundus images  in screening, detection, prediction, and treatment of myopia complications because their much broader visual range is advantageous for highly myopic eyes. spherical equivalent (SE) is extensively used as the main myopia outcome measure, and axial length (AL) has drawn increasing interest as an important ocular component for assessing myopia. cutting-edge studies show that SE and AL are strongly correlated. using the joint information from SE and AL is potentially better than using either separately. in the deep learning community, though there is research on multiple-response tasks with a 3D image biomarker, dependence among responses is only sporadically taken into consideration. inspired by the spirit that information extracted from the data by statistical methods can improve the prediction accuracy of deep learning models, we formulate a class of multivariate response regression models with a higher-order tensor biomarker, for the bivariate tasks of regression-classification and regression-regression. specifically, we propose a copula-enhanced convolutional neural network (CeCNN) framework that incorporates the dependence between responses through a gaussian copula (with parameters estimated from a warm-up CNN) and uses the induced copula-likelihood loss with the backbone CNNs. we establish the statistical framework and algorithms for the aforementioned two bivariate tasks. we show that the CeCNN has better prediction accuracy after adding the dependency information to the backbone models. the modeling and the proposed CeCNN algorithm are applicable beyond the UWF scenario and can be effective with other backbones beyond ResNet and LeNet.
</details></li>
</ul>
<hr>
<h2 id="Fast-Sun-aligned-Outdoor-Scene-Relighting-based-on-TensoRF"><a href="#Fast-Sun-aligned-Outdoor-Scene-Relighting-based-on-TensoRF" class="headerlink" title="Fast Sun-aligned Outdoor Scene Relighting based on TensoRF"></a>Fast Sun-aligned Outdoor Scene Relighting based on TensoRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03965">http://arxiv.org/abs/2311.03965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeonjin Chang, Yearim Kim, Seunghyeon Seo, Jung Yi, Nojun Kwak</li>
<li>for: 这个论文是为了提出一种名为Sun-aligned Relighting TensoRF（SR-TensoRF）的外部场景重新照明方法，用于Neural Radiance Fields（NeRF）。</li>
<li>methods: SR-TensoRF方法使用了sun direction作为阴影生成的输入，从而简化了推理过程的需求，同时也利用了TensoRF的训练效率。</li>
<li>results: SR-TensoRF方法可以快速地生成高质量的阴影和渲染结果，并且在训练和渲染过程中比现有方法更快。<details>
<summary>Abstract</summary>
In this work, we introduce our method of outdoor scene relighting for Neural Radiance Fields (NeRF) named Sun-aligned Relighting TensoRF (SR-TensoRF). SR-TensoRF offers a lightweight and rapid pipeline aligned with the sun, thereby achieving a simplified workflow that eliminates the need for environment maps. Our sun-alignment strategy is motivated by the insight that shadows, unlike viewpoint-dependent albedo, are determined by light direction. We directly use the sun direction as an input during shadow generation, simplifying the requirements of the inference process significantly. Moreover, SR-TensoRF leverages the training efficiency of TensoRF by incorporating our proposed cubemap concept, resulting in notable acceleration in both training and rendering processes compared to existing methods.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了一种名为室内场景重新照明的方法，即太阳平行Relighting TensoRF（SR-TensoRF）。SR-TensoRF提供了一个轻量级、快速的管道，其中sun direction作为输入直接使用在阴影生成过程中，从而大幅简化了推理过程的需求。此外，SR-TensoRF利用了TensoRF的训练效率，通过我们提议的立方体图概念，从而在训练和渲染过程中具有显著的加速。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Multimodal-Compositional-Reasoning-of-Visual-Language-Models-with-Generative-Negative-Mining"><a href="#Enhancing-Multimodal-Compositional-Reasoning-of-Visual-Language-Models-with-Generative-Negative-Mining" class="headerlink" title="Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining"></a>Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03964">http://arxiv.org/abs/2311.03964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ugorsahin/Generative-Negative-Mining">https://github.com/ugorsahin/Generative-Negative-Mining</a></li>
<li>paper_authors: Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, Volker Tresp</li>
<li>for: 提高大规模图像语言模型（VLM）在多模态组合理解任务中的表现</li>
<li>methods: 提出了一个框架，不仅在两个方向上挖掘负例，还生成了多Modal compositional reasoning任务中难度很高的负样本，以提高 VLM 的表现</li>
<li>results: 通过利用这些生成的困难负样本，可以significantly enhance VLMs的表现在多模态组合理解任务中<details>
<summary>Abstract</summary>
Contemporary large-scale visual language models (VLMs) exhibit strong representation capacities, making them ubiquitous for enhancing image and text understanding tasks. They are often trained in a contrastive manner on a large and diverse corpus of images and corresponding text captions scraped from the internet. Despite this, VLMs often struggle with compositional reasoning tasks which require a fine-grained understanding of the complex interactions of objects and their attributes. This failure can be attributed to two main factors: 1) Contrastive approaches have traditionally focused on mining negative examples from existing datasets. However, the mined negative examples might not be difficult for the model to discriminate from the positive. An alternative to mining would be negative sample generation 2) But existing generative approaches primarily focus on generating hard negative texts associated with a given image. Mining in the other direction, i.e., generating negative image samples associated with a given text has been ignored. To overcome both these limitations, we propose a framework that not only mines in both directions but also generates challenging negative samples in both modalities, i.e., images and texts. Leveraging these generative hard negative samples, we significantly enhance VLMs' performance in tasks involving multimodal compositional reasoning. Our code and dataset are released at https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html.
</details>
<details>
<summary>摘要</summary>
当代大规模视觉语言模型（VLM）具有强大的表达能力，使其在图像和文本理解任务中普遍使用。它们通常通过对大量和多样化的图像和相关文本描述进行对比式训练。然而，VLM经常在组合逻辑任务中失败，这可以归结于两个主要因素：1）对比方法 tradicionalmente 在现有数据集中挖掘负例，但这些挖掘出来的负例可能并不是模型很难地区分出来的。二）现有的生成方法主要关注生成困难的文本对应图像的负例，而忽略了生成图像对应文本的负例。为了解决这两个限制，我们提出了一个框架，不仅挖掘两个方向，而且生成了图像和文本两个模态的困难负例。利用这些生成的困难负例，我们可以在多模态组合逻辑任务中明显提高 VLM 的表现。我们的代码和数据集在 <https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html> 上发布。
</details></li>
</ul>
<hr>
<h2 id="Improving-the-Effectiveness-of-Deep-Generative-Data"><a href="#Improving-the-Effectiveness-of-Deep-Generative-Data" class="headerlink" title="Improving the Effectiveness of Deep Generative Data"></a>Improving the Effectiveness of Deep Generative Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03959">http://arxiv.org/abs/2311.03959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruyu Wang, Sabrina Schmedding, Marco F. Huber</li>
<li>for: 本研究旨在探讨使用深度生成模型生成的假图像在下游图像处理任务中的表现，并提出一种新的分类法以提高其表现。</li>
<li>methods: 本研究使用了生成对抗网络（GAN）和扩散概率模型（DPM）生成高品质图像，并对CIFAR-10 dataset进行了广泛的实验。</li>
<li>results: 研究发现，Content Gap是使用深度生成模型生成的假图像表现下降的主要原因，并提出了一些策略来更好地利用这些图像在下游任务中。实验结果表明，我们的方法在 Synthetic-to-Real 和 Data Augmentation 两种情况下都能够获得更高的表现，特别是在数据缺乏的情况下。<details>
<summary>Abstract</summary>
Recent deep generative models (DGMs) such as generative adversarial networks (GANs) and diffusion probabilistic models (DPMs) have shown their impressive ability in generating high-fidelity photorealistic images. Although looking appealing to human eyes, training a model on purely synthetic images for downstream image processing tasks like image classification often results in an undesired performance drop compared to training on real data. Previous works have demonstrated that enhancing a real dataset with synthetic images from DGMs can be beneficial. However, the improvements were subjected to certain circumstances and yet were not comparable to adding the same number of real images. In this work, we propose a new taxonomy to describe factors contributing to this commonly observed phenomenon and investigate it on the popular CIFAR-10 dataset. We hypothesize that the Content Gap accounts for a large portion of the performance drop when using synthetic images from DGM and propose strategies to better utilize them in downstream tasks. Extensive experiments on multiple datasets showcase that our method outperforms baselines on downstream classification tasks both in case of training on synthetic only (Synthetic-to-Real) and training on a mix of real and synthetic data (Data Augmentation), particularly in the data-scarce scenario.
</details>
<details>
<summary>摘要</summary>
最近的深度生成模型（DGM），如生成敌方网络（GAN）和扩散概率模型（DPM），已经显示出生成高品质光真图像的能力。虽然看起来很有吸引力，但是在用Synthetic图像进行下游图像处理任务如图像分类时，却会导致性能下降。先前的工作表明，扩充真实数据集中的Synthetic图像可以提供利益。然而，这些改进受到特定情况的限制，并且与添加相同数量的真实图像相比，并不能达到相同的水平。在这项工作中，我们提出了一个新的分类方法，描述了在常见的情况下，使用DGM生成的Synthetic图像导致性能下降的因素。我们认为，内容差距占了大量的性能下降，并提出了使用这些Synthetic图像更好地利用下游任务的策略。我们的方法在多个数据集上进行了广泛的实验，并证明了与基eline相比，在Synthetic只进行训练（Synthetic-to-Real）和在真实和Synthetic数据进行混合训练（数据扩展）情况下，我们的方法在下游分类任务中表现出色，特别是在数据缺乏的情况下。
</details></li>
</ul>
<hr>
<h2 id="CLIP-Guided-Image-perceptive-Prompt-Learning-for-Image-Enhancement"><a href="#CLIP-Guided-Image-perceptive-Prompt-Learning-for-Image-Enhancement" class="headerlink" title="CLIP Guided Image-perceptive Prompt Learning for Image Enhancement"></a>CLIP Guided Image-perceptive Prompt Learning for Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03943">http://arxiv.org/abs/2311.03943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zinuo Li, Qiuhong Ke, Weiwen Chen</li>
<li>for: 这 paper 的目的是提出一种基于对比语言图像预训练（CLIP）指导提问学习的图像提升方法。</li>
<li>methods: 该方法使用 CLIP 模型学习图像感知提问，并将其作为提升网络的损害函数来驱动图像提升。</li>
<li>results: 研究表明，通过将 CLIP 模型的前知识引入到图像提升中，可以获得满意的结果，并且该方法比传统的 LUT 方法更加简单和高效。<details>
<summary>Abstract</summary>
Image enhancement is a significant research area in the fields of computer vision and image processing. In recent years, many learning-based methods for image enhancement have been developed, where the Look-up-table (LUT) has proven to be an effective tool. In this paper, we delve into the potential of Contrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning, proposing a simple structure called CLIP-LUT for image enhancement. We found that the prior knowledge of CLIP can effectively discern the quality of degraded images, which can provide reliable guidance. To be specific, We initially learn image-perceptive prompts to distinguish between original and target images using CLIP model, in the meanwhile, we introduce a very simple network by incorporating a simple baseline to predict the weights of three different LUT as enhancement network. The obtained prompts are used to steer the enhancement network like a loss function and improve the performance of model. We demonstrate that by simply combining a straightforward method with CLIP, we can obtain satisfactory results.
</details>
<details>
<summary>摘要</summary>
Image enhancement is a significant research area in the fields of computer vision and image processing. In recent years, many learning-based methods for image enhancement have been developed, where the Look-up-table (LUT) has proven to be an effective tool. In this paper, we explore the potential of Contrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning, proposing a simple structure called CLIP-LUT for image enhancement. We found that the prior knowledge of CLIP can effectively discern the quality of degraded images, which can provide reliable guidance. To be specific, we first learn image-perceptive prompts to distinguish between original and target images using the CLIP model, and then introduce a very simple network by incorporating a simple baseline to predict the weights of three different LUTs as an enhancement network. The obtained prompts are used to steer the enhancement network like a loss function and improve the performance of the model. We demonstrate that by simply combining a straightforward method with CLIP, we can obtain satisfactory results.
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-NaN-Divergence-in-Training-Monocular-Depth-Estimation-Model"><a href="#Analysis-of-NaN-Divergence-in-Training-Monocular-Depth-Estimation-Model" class="headerlink" title="Analysis of NaN Divergence in Training Monocular Depth Estimation Model"></a>Analysis of NaN Divergence in Training Monocular Depth Estimation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03938">http://arxiv.org/abs/2311.03938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bum Jun Kim, Hyeonah Jang, Sang Woo Kim</li>
<li>for: 提高深度学习模型的准确性</li>
<li>methods: 通过对训练笔记深度估计网络的NaN损失进行深入分析，并确定了NaN损失的三种敏感性：使用平方根损失引起的不稳定梯度问题、使用对数 sigmoid 函数存在数学稳定问题、和某些变量实现问题导致的错误计算问题。</li>
<li>results: 通过遵循我们的指南，可以提高估计网络的优化稳定性和性能。<details>
<summary>Abstract</summary>
The latest advances in deep learning have facilitated the development of highly accurate monocular depth estimation models. However, when training a monocular depth estimation network, practitioners and researchers have observed not a number (NaN) loss, which disrupts gradient descent optimization. Although several practitioners have reported the stochastic and mysterious occurrence of NaN loss that bothers training, its root cause is not discussed in the literature. This study conducted an in-depth analysis of NaN loss during training a monocular depth estimation network and identified three types of vulnerabilities that cause NaN loss: 1) the use of square root loss, which leads to an unstable gradient; 2) the log-sigmoid function, which exhibits numerical stability issues; and 3) certain variance implementations, which yield incorrect computations. Furthermore, for each vulnerability, the occurrence of NaN loss was demonstrated and practical guidelines to prevent NaN loss were presented. Experiments showed that both optimization stability and performance on monocular depth estimation could be improved by following our guidelines.
</details>
<details>
<summary>摘要</summary>
最新的深度学习技术发展已经使得单目深度估计模型的准确性得到了大幅提高。然而，在训练单目深度估计网络时，实践者和研究人员通常会遇到NaN损失，这会阻碍梯度下降优化。虽然许多实践者已经报道了随机和神秘的NaN损失的出现，但在文献中没有讨论其根本原因。本研究对单目深度估计网络训练中的NaN损失进行了深入分析，并确定了三种可能导致NaN损失的潜在漏洞：1）使用平方根损失，导致梯度不稳定；2）使用对数sigmoid函数，存在数学稳定问题；3）certain variance实现，导致计算错误。此外，我们还提供了避免NaN损失的实践指南，并通过实验证明了我们的指南可以提高优化稳定性和单目深度估计性能。
</details></li>
</ul>
<hr>
<h2 id="FLORA-Fine-grained-Low-Rank-Architecture-Search-for-Vision-Transformer"><a href="#FLORA-Fine-grained-Low-Rank-Architecture-Search-for-Vision-Transformer" class="headerlink" title="FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer"></a>FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03912">http://arxiv.org/abs/2311.03912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shadowpa0327/flora">https://github.com/shadowpa0327/flora</a></li>
<li>paper_authors: Chi-Chih Chang, Yuan-Yao Sung, Shixing Yu, Ning-Chi Huang, Diana Marculescu, Kai-Chiang Wu<br>for: 这个论文的目的是提出一个自动化探索低阶数据的框架，以便实现对 Computer Vision Task 的最佳化。methods: 这个方法使用了 Low-Rank 测试和 One-Shot NAS 的相似性和适束，并且将这两个方法融合成一个统一的框架。它还使用了低阶数据检查和排除低性能的候选者，以避免实验过程中的偏见和干扰。results: 这个方法可以自动生成更细节的低阶数据配置，并且可以实现约 33% 的 Computational FLOPs 节省。具体来说，FLORA-DeiT-B&#x2F;FLORA-Swin-B 可以节省约 55%&#x2F;42% Computational FLOPs，而且可以与主流压缩技术或紧凑结构整合，实现更大的 Computational FLOPs 节省。<details>
<summary>Abstract</summary>
Vision Transformers (ViT) have recently demonstrated success across a myriad of computer vision tasks. However, their elevated computational demands pose significant challenges for real-world deployment. While low-rank approximation stands out as a renowned method to reduce computational loads, efficiently automating the target rank selection in ViT remains a challenge. Drawing from the notable similarity and alignment between the processes of rank selection and One-Shot NAS, we introduce FLORA, an end-to-end automatic framework based on NAS. To overcome the design challenge of supernet posed by vast search space, FLORA employs a low-rank aware candidate filtering strategy. This method adeptly identifies and eliminates underperforming candidates, effectively alleviating potential undertraining and interference among subnetworks. To further enhance the quality of low-rank supernets, we design a low-rank specific training paradigm. First, we propose weight inheritance to construct supernet and enable gradient sharing among low-rank modules. Secondly, we adopt low-rank aware sampling to strategically allocate training resources, taking into account inherited information from pre-trained models. Empirical results underscore FLORA's efficacy. With our method, a more fine-grained rank configuration can be generated automatically and yield up to 33% extra FLOPs reduction compared to a simple uniform configuration. More specific, FLORA-DeiT-B/FLORA-Swin-B can save up to 55%/42% FLOPs almost without performance degradtion. Importantly, FLORA boasts both versatility and orthogonality, offering an extra 21%-26% FLOPs reduction when integrated with leading compression techniques or compact hybrid structures. Our code is publicly available at https://github.com/shadowpa0327/FLORA.
</details>
<details>
<summary>摘要</summary>
幻 transformer (ViT) 在计算机视觉任务中显示出了成功，但是它们的计算需求增加了现实世界部署的挑战。而且，减少计算负担的低级排名选择在 ViT 中仍然是一个挑战。基于计算机视觉任务和一次 NAS 的相似性和对应关系，我们提出了 FLORA，一个端到端自动化的框架。为了解决 supernet 的设计挑战，FLORA 使用了低级排名意识 filtering 策略，可以干预低性能的候选者，从而避免 potential 的弱化和子网之间的干扰。此外，我们还提出了一种低级特定的训练方法，包括 weight 继承和低级排名检测。通过这些方法，我们可以生成更细致的排名配置，并在 FLOPs 减少方面获得更好的性能。我们的实验结果表明，FLORA 可以自动生成更细致的排名配置，并在不同的模型和任务上实现 FLOPs 减少。 Specifically，FLORA-DeiT-B 和 FLORA-Swin-B 可以将 FLOPs 减少约 33%，而且减少后性能几乎不受影响。此外，FLORA 还具有多样性和正交性，可以与主流压缩技术或紧凑型结构结合使用，从而提高 FLOPs 减少的效果。我们的代码可以在 GitHub 上找到：https://github.com/shadowpa0327/FLORA。
</details></li>
</ul>
<hr>
<h2 id="RobustMat-Neural-Diffusion-for-Street-Landmark-Patch-Matching-under-Challenging-Environments"><a href="#RobustMat-Neural-Diffusion-for-Street-Landmark-Patch-Matching-under-Challenging-Environments" class="headerlink" title="RobustMat: Neural Diffusion for Street Landmark Patch Matching under Challenging Environments"></a>RobustMat: Neural Diffusion for Street Landmark Patch Matching under Challenging Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03904">http://arxiv.org/abs/2311.03904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-it-avs/robustmat">https://github.com/ai-it-avs/robustmat</a></li>
<li>paper_authors: Rui She, Qiyu Kang, Sijie Wang, Yuan-Rui Yang, Kai Zhao, Yang Song, Wee Peng Tay</li>
<li>for: The paper is written for the task of matching landmark patches in street scenes for autonomous vehicles (AVs), under challenging driving environments caused by changing seasons, weather, and illumination.</li>
<li>methods: The paper proposes an approach called RobustMat, which uses a convolutional neural ODE diffusion module to learn the feature representation for landmark patches, and a graph neural PDE diffusion module to aggregate information from neighboring landmark patches.</li>
<li>results: The paper demonstrates state-of-the-art matching results under environmental perturbations, using several street scene datasets.Here’s the text in Simplified Chinese:</li>
<li>for: 论文是为了匹配street scene中的标志性 patch，在自动驾驶车（AV）中扮演关键角色。</li>
<li>methods: 论文提出了一种方法，名为RobustMat，它利用了 convolutional neural ODE diffusion module来学习标志性 patch 的特征表示，并使用 graph neural PDE diffusion module来聚合邻近标志性 patch 的信息。</li>
<li>results: 论文在多个 street scene 数据集上进行了评估，并达到了在环境扰动下的最佳匹配结果。<details>
<summary>Abstract</summary>
For autonomous vehicles (AVs), visual perception techniques based on sensors like cameras play crucial roles in information acquisition and processing. In various computer perception tasks for AVs, it may be helpful to match landmark patches taken by an onboard camera with other landmark patches captured at a different time or saved in a street scene image database. To perform matching under challenging driving environments caused by changing seasons, weather, and illumination, we utilize the spatial neighborhood information of each patch. We propose an approach, named RobustMat, which derives its robustness to perturbations from neural differential equations. A convolutional neural ODE diffusion module is used to learn the feature representation for the landmark patches. A graph neural PDE diffusion module then aggregates information from neighboring landmark patches in the street scene. Finally, feature similarity learning outputs the final matching score. Our approach is evaluated on several street scene datasets and demonstrated to achieve state-of-the-art matching results under environmental perturbations.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)For autonomous vehicles (AVs), visual perception techniques based on sensors like cameras play crucial roles in information acquisition and processing. In various computer perception tasks for AVs, it may be helpful to match landmark patches taken by an onboard camera with other landmark patches captured at a different time or saved in a street scene image database. To perform matching under challenging driving environments caused by changing seasons, weather, and illumination, we utilize the spatial neighborhood information of each patch. We propose an approach, named RobustMat, which derives its robustness to perturbations from neural differential equations. A convolutional neural ODE diffusion module is used to learn the feature representation for the landmark patches. A graph neural PDE diffusion module then aggregates information from neighboring landmark patches in the street scene. Finally, feature similarity learning outputs the final matching score. Our approach is evaluated on several street scene datasets and demonstrated to achieve state-of-the-art matching results under environmental perturbations.Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. The Traditional Chinese translation would be slightly different.
</details></li>
</ul>
<hr>
<h2 id="MeVGAN-GAN-based-Plugin-Model-for-Video-Generation-with-Applications-in-Colonoscopy"><a href="#MeVGAN-GAN-based-Plugin-Model-for-Video-Generation-with-Applications-in-Colonoscopy" class="headerlink" title="MeVGAN: GAN-based Plugin Model for Video Generation with Applications in Colonoscopy"></a>MeVGAN: GAN-based Plugin Model for Video Generation with Applications in Colonoscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03884">http://arxiv.org/abs/2311.03884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Łukasz Struski, Tomasz Urbańczyk, Krzysztof Bucki, Bartłomiej Cupiał, Aneta Kaczyńska, Przemysław Spurek, Jacek Tabor</li>
<li>for: 这个论文是为了提出一种高效的视频生成模型，它可以生成高分辨率视频数据，并且可以在医学领域中应用。</li>
<li>methods: 这个模型使用插件型架构，使用预训练的2D图像GAN，并只添加了一个简单的神经网络来构造噪声空间中的旅程，以将旅程传递 через GAN 模型构建真实的视频。</li>
<li>results: 这个模型可以生成高质量的 sintetic colonoscopy 视频，可以用于虚拟 simulate colonoscopy 过程，并且可以帮助年轻的colonoscopist 更好地学习这个重要的医学过程。<details>
<summary>Abstract</summary>
Video generation is important, especially in medicine, as much data is given in this form. However, video generation of high-resolution data is a very demanding task for generative models, due to the large need for memory. In this paper, we propose Memory Efficient Video GAN (MeVGAN) - a Generative Adversarial Network (GAN) which uses plugin-type architecture. We use a pre-trained 2D-image GAN and only add a simple neural network to construct respective trajectories in the noise space, so that the trajectory forwarded through the GAN model constructs a real-life video. We apply MeVGAN in the task of generating colonoscopy videos. Colonoscopy is an important medical procedure, especially beneficial in screening and managing colorectal cancer. However, because colonoscopy is difficult and time-consuming to learn, colonoscopy simulators are widely used in educating young colonoscopists. We show that MeVGAN can produce good quality synthetic colonoscopy videos, which can be potentially used in virtual simulators.
</details>
<details>
<summary>摘要</summary>
文本翻译：视频生成对医学领域非常重要，因为大量数据都是 видео形式。然而，生成高分辨率视频是对生成模型的非常具有挑战性，因为需要很大的内存。在这篇论文中，我们提出了 Memory Efficient Video GAN（MeVGAN），它是一种基于生成对抗网络（GAN）的插件式架构。我们使用预训练的2D图像GAN，只是添加了一个简单的神经网络，以在噪声空间中构建各个旅程，从而使得噪声通过GAN模型构建的旅程是真实的视频。我们在colonoscopy视频生成任务中应用MeVGAN。colonoscopy是医学非常重要的检查方法，尤其是在检测和治疗肠Rectal癌中。然而，因为colonoscopy是学习困难和时间consuming的，因此colonoscopy模拟器在培训年轻的colonoscopists中广泛使用。我们表明MeVGAN可以生成高质量的 sintetic colonoscopy视频，这些视频可能在虚拟模拟器中使用。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Knowledge-Transfer-Methods-for-Misaligned-Urban-Building-Labels"><a href="#A-Comparative-Study-of-Knowledge-Transfer-Methods-for-Misaligned-Urban-Building-Labels" class="headerlink" title="A Comparative Study of Knowledge Transfer Methods for Misaligned Urban Building Labels"></a>A Comparative Study of Knowledge Transfer Methods for Misaligned Urban Building Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03867">http://arxiv.org/abs/2311.03867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bipul Neupane, Jagannath Aryal, Abbas Rajabifard</li>
<li>for:  Addressing the misalignment issue in Earth observation (EO) images and building labels to train accurate convolutional neural networks (CNNs) for semantic segmentation of building footprints.</li>
<li>methods:  Comparative study of three Teacher-Student knowledge transfer methods: supervised domain adaptation (SDA), knowledge distillation (KD), and deep mutual learning (DML).</li>
<li>results:  SDA is the most effective method to address the misalignment problem, while KD and DML can efficiently compress network size without significant loss in performance. The 158 experiments and datasets developed in this study will be valuable to minimise the misaligned labels.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;<details>
<summary>Abstract</summary>
Misalignment in Earth observation (EO) images and building labels impact the training of accurate convolutional neural networks (CNNs) for semantic segmentation of building footprints. Recently, three Teacher-Student knowledge transfer methods have been introduced to address this issue: supervised domain adaptation (SDA), knowledge distillation (KD), and deep mutual learning (DML). However, these methods are merely studied for different urban buildings (low-rise, mid-rise, high-rise, and skyscrapers), where misalignment increases with building height and spatial resolution. In this study, we present a workflow for the systematic comparative study of the three methods. The workflow first identifies the best (with the highest evaluation scores) hyperparameters, lightweight CNNs for the Student (among 43 CNNs from Computer Vision), and encoder-decoder networks (EDNs) for both Teachers and Students. Secondly, three building footprint datasets are developed to train and evaluate the identified Teachers and Students in the three transfer methods. The results show that U-Net with VGG19 (U-VGG19) is the best Teacher, and U-EfficientNetv2B3 and U-EfficientNet-lite0 are among the best Students. With these Teacher-Student pairs, SDA could yield upto 0.943, 0.868, 0.912, and 0.697 F1 scores in the low-rise, mid-rise, high-rise, and skyscrapers respectively. KD and DML provide model compression of upto 82%, despite marginal loss in performance. This new comparison concludes that SDA is the most effective method to address the misalignment problem, while KD and DML can efficiently compress network size without significant loss in performance. The 158 experiments and datasets developed in this study will be valuable to minimise the misaligned labels.
</details>
<details>
<summary>摘要</summary>
地球观测（EO）图像和建筑标签的不一致问题会影响建筑 semantic 分类器的训练。最近，三种教师-学生知识传递方法被提出来解决这个问题：指导适应（SDA）、知识储存（KD）和深度相互学习（DML）。然而，这些方法只在不同的城市建筑（低层、中层、高层和尖塔）进行了研究，而这些建筑的不一致程度随着建筑高度和空间分辨率增加。在这种研究中，我们提供了一个工作流程，用于系统 Comparative 研究这三种方法。这个 workflow 首先确定最佳（最高评价分）的参数、轻量级 CNN（从计算机视觉中选择）和编码器-解码器网络（EDN） для两个教师和学生。其次，为了训练和评估选定的教师和学生，我们开发了三个建筑footprint 数据集。结果显示，U-Net with VGG19（U-VGG19）是最佳教师，而 U-EfficientNetv2B3 和 U-EfficientNet-lite0 是最佳学生之一。使用这些教师-学生对，SDA 可以实现upto 0.943、0.868、0.912 和 0.697 F1 分数在不同的建筑高度上。KD 和 DML 可以压缩网络大小，即使是82%，尽管表现下降了一些。这种新的比较结论是，SDA 是解决不一致标签问题的最有效的方法，而 KD 和 DML 可以高效地压缩网络大小，不会导致重要的表现下降。这个研究中进行的 158 个实验和开发的数据集将有助于减少不一致标签的问题。
</details></li>
</ul>
<hr>
<h2 id="SCONE-GAN-Semantic-Contrastive-learning-based-Generative-Adversarial-Network-for-an-end-to-end-image-translation"><a href="#SCONE-GAN-Semantic-Contrastive-learning-based-Generative-Adversarial-Network-for-an-end-to-end-image-translation" class="headerlink" title="SCONE-GAN: Semantic Contrastive learning-based Generative Adversarial Network for an end-to-end image translation"></a>SCONE-GAN: Semantic Contrastive learning-based Generative Adversarial Network for an end-to-end image translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03866">http://arxiv.org/abs/2311.03866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Abbasnejad, Fabio Zambetta, Flora Salim, Timothy Wiley, Jeffrey Chan, Russell Gallagher, Ehsan Abbasnejad</li>
<li>for: 这篇论文旨在探讨如何实现图像转换，以生成更加现实和多样的景观图像。</li>
<li>methods: 本研究使用了图像转换为主的GAN方法，并通过实现物件相依性，保持图像结构和Semantics，实现图像转换。此外，我们引入了式参考图像，以增加生成的图像多样性。</li>
<li>results: 我们透过实验 validate了我们的方法，并证明了它在四个数据集上的效果。结果显示，我们的方法可以生成更加现实和多样的景观图像，并且可以增加生成的图像多样性。<details>
<summary>Abstract</summary>
SCONE-GAN presents an end-to-end image translation, which is shown to be effective for learning to generate realistic and diverse scenery images. Most current image-to-image translation approaches are devised as two mappings: a translation from the source to target domain and another to represent its inverse. While successful in many applications, these approaches may suffer from generating trivial solutions with limited diversity. That is because these methods learn more frequent associations rather than the scene structures. To mitigate the problem, we propose SCONE-GAN that utilises graph convolutional networks to learn the objects dependencies, maintain the image structure and preserve its semantics while transferring images into the target domain. For more realistic and diverse image generation we introduce style reference image. We enforce the model to maximize the mutual information between the style image and output. The proposed method explicitly maximizes the mutual information between the related patches, thus encouraging the generator to produce more diverse images. We validate the proposed algorithm for image-to-image translation and stylizing outdoor images. Both qualitative and quantitative results demonstrate the effectiveness of our approach on four dataset.
</details>
<details>
<summary>摘要</summary>
SCONE-GAN 提出了一种端到端图像翻译方法，可以学习生成真实和多样化的景观图像。当前大多数图像到图像翻译方法都是设计为两个映射：一个从源领域到目标领域，另一个用于表示其逆转。虽然在许多应用程序中得到了成功，但这些方法可能会导致生成平庸的解决方案，因为它们学习的是更频繁的关联而不是场景结构。为了解决这问题，我们提议使用图像 convolutional networks 来学习对象的依赖关系，保持图像结构，并保持图像 semantics 的意义，而无需将图像转换到目标领域。为了生成更真实和多样化的图像，我们引入了风格参考图像。我们要求模型通过最大化风格图像和输出之间的共mutual information 来强制实现这个目标。我们的方法通过显式地最大化相关块之间的共mutual information 来鼓励生成器生成更多样化的图像。我们验证了我们的方法在图像到图像翻译和风格化户外图像方面的效果。Qualitative 和量化结果表明我们的方法在四个数据集上具有显著的效果。
</details></li>
</ul>
<hr>
<h2 id="GC-VTON-Predicting-Globally-Consistent-and-Occlusion-Aware-Local-Flows-with-Neighborhood-Integrity-Preservation-for-Virtual-Try-on"><a href="#GC-VTON-Predicting-Globally-Consistent-and-Occlusion-Aware-Local-Flows-with-Neighborhood-Integrity-Preservation-for-Virtual-Try-on" class="headerlink" title="GC-VTON: Predicting Globally Consistent and Occlusion Aware Local Flows with Neighborhood Integrity Preservation for Virtual Try-on"></a>GC-VTON: Predicting Globally Consistent and Occlusion Aware Local Flows with Neighborhood Integrity Preservation for Virtual Try-on</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04932">http://arxiv.org/abs/2311.04932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamza Rawal, Muhammad Junaid Ahmad, Farooq Zaman</li>
<li>for: 这研究旨在提高基于图像的虚拟试穿网络中的衣服折叠方法，以提高虚拟试穿的效果。</li>
<li>methods: 该研究提出了一种新的方法，即通过分解全局边界准确和本地文件保持任务，使用全局网络（GlobalNet）和本地网络（LocalNet）模块进行分解。此外，还使用了一种新的定制损失函数（NIPR）来评估折叠流的质量，并在折叠流中遮盖了身体部位可见掩码，以避免因身体部位或其他衣服 occlusion 而导致的损害。</li>
<li>results: 该研究的实验结果表明，与当前最佳方法相比，该方法在虚拟试穿数据集上表现出色，提高了虚拟试穿的效果。<details>
<summary>Abstract</summary>
Flow based garment warping is an integral part of image-based virtual try-on networks. However, optimizing a single flow predicting network for simultaneous global boundary alignment and local texture preservation results in sub-optimal flow fields. Moreover, dense flows are inherently not suited to handle intricate conditions like garment occlusion by body parts or by other garments. Forcing flows to handle the above issues results in various distortions like texture squeezing, and stretching. In this work, we propose a novel approach where we disentangle the global boundary alignment and local texture preserving tasks via our GlobalNet and LocalNet modules. A consistency loss is then employed between the two modules which harmonizes the local flows with the global boundary alignment. Additionally, we explicitly handle occlusions by predicting body-parts visibility mask, which is used to mask out the occluded regions in the warped garment. The masking prevents the LocalNet from predicting flows that distort texture to compensate for occlusions. We also introduce a novel regularization loss (NIPR), that defines a criteria to identify the regions in the warped garment where texture integrity is violated (squeezed or stretched). NIPR subsequently penalizes the flow in those regions to ensure regular and coherent warps that preserve the texture in local neighborhoods. Evaluation on a widely used virtual try-on dataset demonstrates strong performance of our network compared to the current SOTA methods.
</details>
<details>
<summary>摘要</summary>
流基 garment 扭曲是虚拟试穿网络中的一个重要组成部分。然而，单独优化一个流预测网络，以同时实现全局边界对齐和本地文件保存，会导致下面的问题：1. 文件流不适用于处理复杂的衣物干扰，如衣物部分遮挡或衣物之间的干扰。2. 压缩文件和扩展文件会导致文件的扭曲和质量下降。为了解决这些问题，我们提出了一种新的方法。我们在GlobalNet和LocalNet模块之间分离了全局边界对齐和本地文件保存任务。在这两个模块之间使用一个一致性损失，使得本地流与全局边界对齐融为一体。此外，我们还显式处理 occlusion 问题，通过预测身体部分可见掩码，以遮掩 occlusion 区域中的扭曲。这些掩码使得 LocalNet 不会预测扭曲以补偿 occlusion。最后，我们还引入了一种新的规范损失（NIPR），它定义了扭曲的区域，并对这些区域进行违反约束，以确保流fields 具有规则和一致的扭曲，并保持文件的质量。我们在一个广泛使用虚拟试穿数据集进行了评估，并与当前 State-of-the-Art 方法进行了比较。结果表明，我们的网络在虚拟试穿任务中具有强大的性能。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-Information-Integration-and-Propagation-for-Occluded-Person-Re-identification"><a href="#Multi-view-Information-Integration-and-Propagation-for-Occluded-Person-Re-identification" class="headerlink" title="Multi-view Information Integration and Propagation for Occluded Person Re-identification"></a>Multi-view Information Integration and Propagation for Occluded Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03828">http://arxiv.org/abs/2311.03828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nengdong96/mviip">https://github.com/nengdong96/mviip</a></li>
<li>paper_authors: Neng Dong, Shuanglin Yan, Hao Tang, Jinhui Tang, Liyan Zhang</li>
<li>for: 提高 occluded person re-identification 的精度和稳定性，使用多视图图像来减少遮挡噪声的影响。</li>
<li>methods: 提出了一种基于多视图图像的 Multi-view Information Integration and Propagation (MVI$^{2}$P) 框架，通过综合利用多视图图像的特征图来提高批量人脸识别的精度和稳定性。</li>
<li>results: 经过广泛的实验和分析，证明了 MVI$^{2}$P 的有效性和超越性，能够在 occluded person re-identification  task 中提高精度和稳定性。<details>
<summary>Abstract</summary>
Occluded person re-identification (re-ID) presents a challenging task due to occlusion perturbations. Although great efforts have been made to prevent the model from being disturbed by occlusion noise, most current solutions only capture information from a single image, disregarding the rich complementary information available in multiple images depicting the same pedestrian. In this paper, we propose a novel framework called Multi-view Information Integration and Propagation (MVI$^{2}$P). Specifically, realizing the potential of multi-view images in effectively characterizing the occluded target pedestrian, we integrate feature maps of which to create a comprehensive representation. During this process, to avoid introducing occlusion noise, we develop a CAMs-aware Localization module that selectively integrates information contributing to the identification. Additionally, considering the divergence in the discriminative nature of different images, we design a probability-aware Quantification module to emphatically integrate highly reliable information. Moreover, as multiple images with the same identity are not accessible in the testing stage, we devise an Information Propagation (IP) mechanism to distill knowledge from the comprehensive representation to that of a single occluded image. Extensive experiments and analyses have unequivocally demonstrated the effectiveness and superiority of the proposed MVI$^{2}$P. The code will be released at \url{https://github.com/nengdong96/MVIIP}.
</details>
<details>
<summary>摘要</summary>
occluded person re-identification (re-ID) 面临许多挑战，主要是因为干扰噪声。 despite great efforts to prevent the model from being disturbed by occlusion noise, most current solutions only capture information from a single image, ignoring the rich complementary information available in multiple images depicting the same pedestrian. In this paper, we propose a novel framework called Multi-view Information Integration and Propagation (MVI$^{2}$P). Specifically, we leverage the potential of multi-view images in effectively characterizing the occluded target pedestrian, and integrate feature maps to create a comprehensive representation. During this process, we develop a CAMs-aware Localization module that selectively integrates information contributing to the identification, and a probability-aware Quantification module to emphatically integrate highly reliable information. Moreover, as multiple images with the same identity are not accessible in the testing stage, we devise an Information Propagation (IP) mechanism to distill knowledge from the comprehensive representation to that of a single occluded image. Extensive experiments and analyses have unequivocally demonstrated the effectiveness and superiority of the proposed MVI$^{2}$P. The code will be released at \url{https://github.com/nengdong96/MVIIP}.
</details></li>
</ul>
<hr>
<h2 id="Detecting-Any-Human-Object-Interaction-Relationship-Universal-HOI-Detector-with-Spatial-Prompt-Learning-on-Foundation-Models"><a href="#Detecting-Any-Human-Object-Interaction-Relationship-Universal-HOI-Detector-with-Spatial-Prompt-Learning-on-Foundation-Models" class="headerlink" title="Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models"></a>Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03799">http://arxiv.org/abs/2311.03799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caoyichao/unihoi">https://github.com/caoyichao/unihoi</a></li>
<li>paper_authors: Yichao Cao, Qingfei Tang, Xiu Su, Chen Song, Shan You, Xiaobo Lu, Chang Xu</li>
<li>for: 本研究旨在实现开放世界下的人物交互检测，通过视觉语言基础模型和大型自然语言模型（LLM）来解决复杂的人物交互问题。</li>
<li>methods: 本研究使用了视觉语言基础模型和大型自然语言模型（LLM），并提出了一种基于HO提示的学习方法（HO prompt-based learning），以帮助将高级关系表示与不同的HO对 associations。</li>
<li>results: 研究表明，UniHOI方法可以在开放世界下高度提高人物交互检测的性能，并在不同的输入类型（交互短语或解释句）下支持多种开放类型交互recognition。<details>
<summary>Abstract</summary>
Human-object interaction (HOI) detection aims to comprehend the intricate relationships between humans and objects, predicting $<human, action, object>$ triplets, and serving as the foundation for numerous computer vision tasks. The complexity and diversity of human-object interactions in the real world, however, pose significant challenges for both annotation and recognition, particularly in recognizing interactions within an open world context. This study explores the universal interaction recognition in an open-world setting through the use of Vision-Language (VL) foundation models and large language models (LLMs). The proposed method is dubbed as \emph{\textbf{UniHOI}. We conduct a deep analysis of the three hierarchical features inherent in visual HOI detectors and propose a method for high-level relation extraction aimed at VL foundation models, which we call HO prompt-based learning. Our design includes an HO Prompt-guided Decoder (HOPD), facilitates the association of high-level relation representations in the foundation model with various HO pairs within the image. Furthermore, we utilize a LLM (\emph{i.e.} GPT) for interaction interpretation, generating a richer linguistic understanding for complex HOIs. For open-category interaction recognition, our method supports either of two input types: interaction phrase or interpretive sentence. Our efficient architecture design and learning methods effectively unleash the potential of the VL foundation models and LLMs, allowing UniHOI to surpass all existing methods with a substantial margin, under both supervised and zero-shot settings. The code and pre-trained weights are available at: \url{https://github.com/Caoyichao/UniHOI}.
</details>
<details>
<summary>摘要</summary>
人物交互探测（HOI）目标是理解人与物之间复杂的关系，预测<人,动作,物> triplets，并成为许多计算机视觉任务的基础。然而，在实际世界中人物交互的复杂性和多样性带来了对注释和识别的挑战，特别是在开放世界上下文中。本研究通过使用视觉语言基础模型（VL）和大型自然语言模型（LLM）来实现在开放世界上进行一般交互探测。我们提出了一种名为UniHOI的方法，包括一种HO Prompt-based Learning（HOPD），可以帮助将视觉基础模型中的高级关系表示与不同的HO对在图像中相关联。此外，我们使用GPT作为交互解释，以生成更加丰富的语言理解，用于处理复杂的HOI。为了进行开放类交互探测，我们的方法支持两种输入类型：交互短语和解释句子。我们的有效的建筑设计和学习方法可以有效发挥VL基础模型和LLM的潜力，使UniHOI在超过所有现有方法的情况下，在有监督和零值设定下达到了很高的性能。代码和预训练 веса可以在以下链接中下载：<https://github.com/Caoyichao/UniHOI>。
</details></li>
</ul>
<hr>
<h2 id="Self-MI-Efficient-Multimodal-Fusion-via-Self-Supervised-Multi-Task-Learning-with-Auxiliary-Mutual-Information-Maximization"><a href="#Self-MI-Efficient-Multimodal-Fusion-via-Self-Supervised-Multi-Task-Learning-with-Auxiliary-Mutual-Information-Maximization" class="headerlink" title="Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization"></a>Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03785">http://arxiv.org/abs/2311.03785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cam-Van Thi Nguyen, Ngoc-Hoa Thi Nguyen, Duc-Trong Le, Quang-Thuy Ha</li>
<li>for: 本研究旨在提高多模态学习中的特征表示学习，以便更好地捕捉不同模态之间的相互关系。</li>
<li>methods: 本研究使用了Self-MI方法，具体来说是通过对不同模态的输入对进行自然监督学习，并采用了Contrastive Predictive Coding（CPC）技术来增强多模态融合结果与单模态输入之间的相互信息。</li>
<li>results: 实验结果表明，Self-MI方法能够在三个 benchmark 数据集（CMU-MOSI、CMU-MOSEI 和 SIMS）上显著提高多模态融合任务的性能。<details>
<summary>Abstract</summary>
Multimodal representation learning poses significant challenges in capturing informative and distinct features from multiple modalities. Existing methods often struggle to exploit the unique characteristics of each modality due to unified multimodal annotations. In this study, we propose Self-MI in the self-supervised learning fashion, which also leverage Contrastive Predictive Coding (CPC) as an auxiliary technique to maximize the Mutual Information (MI) between unimodal input pairs and the multimodal fusion result with unimodal inputs. Moreover, we design a label generation module, $ULG_{MI}$ for short, that enables us to create meaningful and informative labels for each modality in a self-supervised manner. By maximizing the Mutual Information, we encourage better alignment between the multimodal fusion and the individual modalities, facilitating improved multimodal fusion. Extensive experiments on three benchmark datasets including CMU-MOSI, CMU-MOSEI, and SIMS, demonstrate the effectiveness of Self-MI in enhancing the multimodal fusion task.
</details>
<details>
<summary>摘要</summary>
多模态表示学习受到多模态特征 capture 约束，现有方法通常因为多模态约束而难以充分发挥每个模态的特点。在本研究中，我们提议了基于自我supervised learning的Self-MI方法，同时利用Contrastive Predictive Coding（CPC）作为auxiliary技术，以最大化多模态融合结果与单模态输入对的Mutual Information（MI）。此外，我们还设计了一个标签生成模块，简称为$ULG_{MI}$，以便在自我supervised manner中生成每个模态的有意义和有用的标签。通过最大化MI，我们鼓励多模态融合和单模态更好地对齐，从而提高多模态融合。我们在CMU-MOSI、CMU-MOSEI和SIMS三个 benchmark datasets上进行了广泛的实验，并证明了Self-MI在多模态融合任务中的效果。
</details></li>
</ul>
<hr>
<h2 id="UP-NeRF-Unconstrained-Pose-Prior-Free-Neural-Radiance-Fields"><a href="#UP-NeRF-Unconstrained-Pose-Prior-Free-Neural-Radiance-Fields" class="headerlink" title="UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields"></a>UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03784">http://arxiv.org/abs/2311.03784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Injae Kim, Minhyuk Choi, Hyunwoo J. Kim</li>
<li>for: 用于处理不受约束的图像集合，不需要摄像机pose prior。</li>
<li>methods: 使用代理任务优化不敏感特征场和分离遮挡模块，以及候选头和可靠深度监督来提高pose估计和遮挡抗性。</li>
<li>results: 在一个复杂的互联网照片收藏中，比基线方法（包括BARF和其变体）表现出色，证明了我们的方法的优越性。<details>
<summary>Abstract</summary>
Neural Radiance Field (NeRF) has enabled novel view synthesis with high fidelity given images and camera poses. Subsequent works even succeeded in eliminating the necessity of pose priors by jointly optimizing NeRF and camera pose. However, these works are limited to relatively simple settings such as photometrically consistent and occluder-free image collections or a sequence of images from a video. So they have difficulty handling unconstrained images with varying illumination and transient occluders. In this paper, we propose $\textbf{UP-NeRF}$ ($\textbf{U}$nconstrained $\textbf{P}$ose-prior-free $\textbf{Ne}$ural $\textbf{R}$adiance $\textbf{F}$ields) to optimize NeRF with unconstrained image collections without camera pose prior. We tackle these challenges with surrogate tasks that optimize color-insensitive feature fields and a separate module for transient occluders to block their influence on pose estimation. In addition, we introduce a candidate head to enable more robust pose estimation and transient-aware depth supervision to minimize the effect of incorrect prior. Our experiments verify the superior performance of our method compared to the baselines including BARF and its variants in a challenging internet photo collection, $\textit{Phototourism}$ dataset.
</details>
<details>
<summary>摘要</summary>
neural radiance field (NeRF) 已经启动了高精度的新视角合成，只需要提供图像和摄像头位置。然而，这些工作受到了一些限制，例如：图像集是 photometrically 一致的，没有遮挡物品和遮挡物品。在这篇论文中，我们提出了 $\textbf{UP-NeRF}$（$\textbf{U}$nconstrained $\textbf{P}$ose-prior-free $\textbf{Ne}$ural $\textbf{R}$adiance $\textbf{F}$ields），它可以在不受任何几何假设的情况下优化 NeRF。我们通过增加彩色不敏感特征场和遮挡物品封页来解决这些挑战。此外，我们还引入了候选首选项，以实现更加稳定的姿势估计和遮挡物品封页。我们的实验显示，我们的方法与 BARF 和其他基eline的比较，在一个具有挑战性的互联网照片集，$\textit{Phototourism}$ 数据集中，具有较高的性能。
</details></li>
</ul>
<hr>
<h2 id="CapST-An-Enhanced-and-Lightweight-Method-for-Deepfake-Video-Classification"><a href="#CapST-An-Enhanced-and-Lightweight-Method-for-Deepfake-Video-Classification" class="headerlink" title="CapST: An Enhanced and Lightweight Method for Deepfake Video Classification"></a>CapST: An Enhanced and Lightweight Method for Deepfake Video Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03782">http://arxiv.org/abs/2311.03782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wasim Ahmad, Yan-Tsung Peng, Yuan-Hao Chang, Gaddisa Olani Ganfure, Sarwar Khan, Sahibzada Adil Shahzad</li>
<li>for: 本研究旨在针对深伪视频（Deepfake Video）进行分类，以提高各种领域（如政治、娱乐、安全）的识别能力。</li>
<li>methods: 该研究提出了一种新型的深伪视频分类模型，利用VGG19bn的一部分作为底层模型，并将卷积神经网络和空间时间注意力机制相结合，以提高分类精度。</li>
<li>results: 实验结果表明，该模型在一个广泛的深伪视频 benchmark 数据集（DFDM）上实现了比基eline模型高达4%的提升，同时具有更加高效的计算资源使用。<details>
<summary>Abstract</summary>
The proliferation of deepfake videos, synthetic media produced through advanced Artificial Intelligence techniques has raised significant concerns across various sectors, encompassing realms such as politics, entertainment, and security. In response, this research introduces an innovative and streamlined model designed to classify deepfake videos generated by five distinct encoders adeptly. Our approach not only achieves state of the art performance but also optimizes computational resources. At its core, our solution employs part of a VGG19bn as a backbone to efficiently extract features, a strategy proven effective in image-related tasks. We integrate a Capsule Network coupled with a Spatial Temporal attention mechanism to bolster the model's classification capabilities while conserving resources. This combination captures intricate hierarchies among features, facilitating robust identification of deepfake attributes. Delving into the intricacies of our innovation, we introduce an existing video level fusion technique that artfully capitalizes on temporal attention mechanisms. This mechanism serves to handle concatenated feature vectors, capitalizing on the intrinsic temporal dependencies embedded within deepfake videos. By aggregating insights across frames, our model gains a holistic comprehension of video content, resulting in more precise predictions. Experimental results on an extensive benchmark dataset of deepfake videos called DFDM showcase the efficacy of our proposed method. Notably, our approach achieves up to a 4 percent improvement in accurately categorizing deepfake videos compared to baseline models, all while demanding fewer computational resources.
</details>
<details>
<summary>摘要</summary>
“深圳技术”的普及，通过先进人工智能技术生成的假视频，在不同领域引发了重大关注，包括政治、娱乐和安全等。为应对这些挑战，本研究提出了一种创新的和高效的模型，用于分类生成自五种不同编码器的深圳视频。我们的方法不仅实现了状态的表现，还优化了计算资源。我们的解决方案在核心上采用一部分的VGG19bn作为背景，高效地提取特征，这种策略在图像相关任务中已经证明有效。我们将卷积网络和空间时间注意力机制结合使用，以增强模型的分类能力，同时保持资源的合理使用。这种结合使得模型能够快速和准确地识别深圳视频的特征。在我们的创新中，我们还介绍了一种现有的视频级别融合技术，利用时间注意力机制来处理 concatenated 特征向量， capitalizing on the intrinsic temporal dependencies embedded within deepfake videos。通过聚合帧内信息，我们的模型获得了视频内容的全面理解，从而实现更加准确的预测。实验结果表明，我们的提议方法在大量的深圳视频数据集（DFDM）上达到了4%的提升，与基eline模型相比，同时减少计算资源的需求。
</details></li>
</ul>
<hr>
<h2 id="Meta-Adapter-An-Online-Few-shot-Learner-for-Vision-Language-Model"><a href="#Meta-Adapter-An-Online-Few-shot-Learner-for-Vision-Language-Model" class="headerlink" title="Meta-Adapter: An Online Few-shot Learner for Vision-Language Model"></a>Meta-Adapter: An Online Few-shot Learner for Vision-Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03774">http://arxiv.org/abs/2311.03774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Cheng, Lin Song, Ruoyi Xue, Hang Wang, Hongbin Sun, Yixiao Ge, Ying Shan</li>
<li>for: 这篇论文旨在提出一种能够在线进行几个样本微调的方法，以提高CLIP模型在几shot检测中的效能。</li>
<li>methods: 本文提出了一种名为Meta-Adapter的轻量级复修器，可以在线进行CLIP特征的细化，以应对几shot检测中的挑战。</li>
<li>results: 本文的方法可以在几个样本微调下，实现高效的几shot检测能力，并且可以在未见到的数据或任务下保持竞争性的性能。<details>
<summary>Abstract</summary>
The contrastive vision-language pre-training, known as CLIP, demonstrates remarkable potential in perceiving open-world visual concepts, enabling effective zero-shot image recognition. Nevertheless, few-shot learning methods based on CLIP typically require offline fine-tuning of the parameters on few-shot samples, resulting in longer inference time and the risk of over-fitting in certain domains. To tackle these challenges, we propose the Meta-Adapter, a lightweight residual-style adapter, to refine the CLIP features guided by the few-shot samples in an online manner. With a few training samples, our method can enable effective few-shot learning capabilities and generalize to unseen data or tasks without additional fine-tuning, achieving competitive performance and high efficiency. Without bells and whistles, our approach outperforms the state-of-the-art online few-shot learning method by an average of 3.6\% on eight image classification datasets with higher inference speed. Furthermore, our model is simple and flexible, serving as a plug-and-play module directly applicable to downstream tasks. Without further fine-tuning, Meta-Adapter obtains notable performance improvements in open-vocabulary object detection and segmentation tasks.
</details>
<details>
<summary>摘要</summary>
“对于开放世界的视觉概念，对于零步识别有潜在的优秀潜力。然而，基于CLIP的几步学习方法通常需要静态精度调整，导致较长的推导时间和特定领域中的过滤过沾。为了解决这些挑战，我们提出了Meta-Adapter，一个轻量级的残差式适配器，可以在线上方式进行CLIP特征的精度调整。仅需几个训练样本，我们的方法可以实现有效的几步学习能力，并能够适应未见到的数据或任务，无需额外精度调整，实现竞争性的表现和高效率。而且，我们的方法简单易应用，可以直接应用到下游任务，并且不需进一步精度调整，在开放 vocabulary 物件检测和分类任务中获得了杰出的表现改善。”
</details></li>
</ul>
<hr>
<h2 id="Lightweight-Portrait-Matting-via-Regional-Attention-and-Refinement"><a href="#Lightweight-Portrait-Matting-via-Regional-Attention-and-Refinement" class="headerlink" title="Lightweight Portrait Matting via Regional Attention and Refinement"></a>Lightweight Portrait Matting via Regional Attention and Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03770">http://arxiv.org/abs/2311.03770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yatao Zhong, Ilya Zharkov</li>
<li>for: 高解像肖像剪辑（portrait matting）</li>
<li>methods: 提出了一种轻量级模型，使用视Transformer（ViT）作为低分辨率网络的基础，并在精度修正网络中添加了一种新的跨区域注意力（CRA）模块，以提高本地区域的信息传递。</li>
<li>results: 比较其他基eline模型，本方法在三个标准测试集上达到了更高的Result，同时具有实时性和较低的计算开销。<details>
<summary>Abstract</summary>
We present a lightweight model for high resolution portrait matting. The model does not use any auxiliary inputs such as trimaps or background captures and achieves real time performance for HD videos and near real time for 4K. Our model is built upon a two-stage framework with a low resolution network for coarse alpha estimation followed by a refinement network for local region improvement. However, a naive implementation of the two-stage model suffers from poor matting quality if not utilizing any auxiliary inputs. We address the performance gap by leveraging the vision transformer (ViT) as the backbone of the low resolution network, motivated by the observation that the tokenization step of ViT can reduce spatial resolution while retain as much pixel information as possible. To inform local regions of the context, we propose a novel cross region attention (CRA) module in the refinement network to propagate the contextual information across the neighboring regions. We demonstrate that our method achieves superior results and outperforms other baselines on three benchmark datasets while only uses $1/20$ of the FLOPS compared to the existing state-of-the-art model.
</details>
<details>
<summary>摘要</summary>
我们提出了一种轻量级的高解像肖像抹除模型。该模型不使用任何辅助输入，如trimap或背景捕获，并在HD视频和4K视频的实时性下达到了实时性。我们的模型基于两个阶段框架，其中低分辨率网络用于粗略α估计，然后是一个改进网络用于地方区域的改进。然而，直观实现该两个阶段模型会导致抹除质量低下，除非使用辅助输入。我们解决这个性能差距问题，通过利用视Transformer（ViT）作为低分辨率网络的后夹，这是因为ViT的token化步骤可以减少空间分辨率，保留最多的像素信息。为了在邻近区域传递Contextual信息，我们提出了一种新的跨区域注意力（CRA）模块，用于在邻近区域之间传递Contextual信息。我们示示了我们的方法可以在三个标准测试集上达到更高的Result，并且只用了$1/20$的FLOPS，相比之前的状态则模型。
</details></li>
</ul>
<hr>
<h2 id="Image-change-detection-with-only-a-few-samples"><a href="#Image-change-detection-with-only-a-few-samples" class="headerlink" title="Image change detection with only a few samples"></a>Image change detection with only a few samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03762">http://arxiv.org/abs/2311.03762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Ke Liu, Zhaoyi Song, Haoyue Bai</li>
<li>for: 本研究针对仅有少量样本的图像变化检测，解决了对于仅有少量样本的挑战。</li>
<li>methods: 本研究使用简单的图像处理方法生成了Synthetic数据，并设计了基于物件检测的早期融合网络，以提高 Change detection 模型的通用能力。</li>
<li>results: 实验结果显示，使用Synthetic数据进行训练后，可以实现高度的通用能力，并且使用仅有 tens of 样本进行精度调整亦可以 achieve excellent results。<details>
<summary>Abstract</summary>
This paper considers image change detection with only a small number of samples, which is a significant problem in terms of a few annotations available. A major impediment of image change detection task is the lack of large annotated datasets covering a wide variety of scenes. Change detection models trained on insufficient datasets have shown poor generalization capability. To address the poor generalization issue, we propose using simple image processing methods for generating synthetic but informative datasets, and design an early fusion network based on object detection which could outperform the siamese neural network. Our key insight is that the synthetic data enables the trained model to have good generalization ability for various scenarios. We compare the model trained on the synthetic data with that on the real-world data captured from a challenging dataset, CDNet, using six different test sets. The results demonstrate that the synthetic data is informative enough to achieve higher generalization ability than the insufficient real-world data. Besides, the experiment shows that utilizing a few (often tens of) samples to fine-tune the model trained on the synthetic data will achieve excellent results.
</details>
<details>
<summary>摘要</summary>
The authors compare the model trained on synthetic data with one trained on real-world data from a challenging dataset, CDNet, using six different test sets. The results show that the synthetic data is informative enough to achieve higher generalization ability than the insufficient real-world data. Moreover, the experiment demonstrates that fine-tuning the model trained on synthetic data with a few (often tens of) samples can achieve excellent results.
</details></li>
</ul>
<hr>
<h2 id="Multiclass-Segmentation-using-Teeth-Attention-Modules-for-Dental-X-ray-Images"><a href="#Multiclass-Segmentation-using-Teeth-Attention-Modules-for-Dental-X-ray-Images" class="headerlink" title="Multiclass Segmentation using Teeth Attention Modules for Dental X-ray Images"></a>Multiclass Segmentation using Teeth Attention Modules for Dental X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03749">http://arxiv.org/abs/2311.03749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afnan Ghafoor, Seong-Yong Moon, Bumshik Lee</li>
<li>for: 这篇论文旨在提出一种革新的多类牙齿分割架构，以解决现有牙齿图像分割方法中的不准确和不可靠的问题。</li>
<li>methods: 该方法 integrate了M-Net-like结构、Swin Transformers以及一个名为牙齿注意块（TAB）。TAB使用了一种特有的注意机制，具体是专门针对牙齿复杂的结构进行注意。</li>
<li>results: 实验结果表明，该方法可以准确地分割牙齿图像，并且在多个标准牙齿图像 dataset 上表现出优于现有状态对牙齿图像分割的方法。<details>
<summary>Abstract</summary>
This paper proposed a cutting-edge multiclass teeth segmentation architecture that integrates an M-Net-like structure with Swin Transformers and a novel component named Teeth Attention Block (TAB). Existing teeth image segmentation methods have issues with less accurate and unreliable segmentation outcomes due to the complex and varying morphology of teeth, although teeth segmentation in dental panoramic images is essential for dental disease diagnosis. We propose a novel teeth segmentation model incorporating an M-Net-like structure with Swin Transformers and TAB. The proposed TAB utilizes a unique attention mechanism that focuses specifically on the complex structures of teeth. The attention mechanism in TAB precisely highlights key elements of teeth features in panoramic images, resulting in more accurate segmentation outcomes. The proposed architecture effectively captures local and global contextual information, accurately defining each tooth and its surrounding structures. Furthermore, we employ a multiscale supervision strategy, which leverages the left and right legs of the U-Net structure, boosting the performance of the segmentation with enhanced feature representation. The squared Dice loss is utilized to tackle the class imbalance issue, ensuring accurate segmentation across all classes. The proposed method was validated on a panoramic teeth X-ray dataset, which was taken in a real-world dental diagnosis. The experimental results demonstrate the efficacy of our proposed architecture for tooth segmentation on multiple benchmark dental image datasets, outperforming existing state-of-the-art methods in objective metrics and visual examinations. This study has the potential to significantly enhance dental image analysis and contribute to advances in dental applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SBCFormer-Lightweight-Network-Capable-of-Full-size-ImageNet-Classification-at-1-FPS-on-Single-Board-Computers"><a href="#SBCFormer-Lightweight-Network-Capable-of-Full-size-ImageNet-Classification-at-1-FPS-on-Single-Board-Computers" class="headerlink" title="SBCFormer: Lightweight Network Capable of Full-size ImageNet Classification at 1 FPS on Single Board Computers"></a>SBCFormer: Lightweight Network Capable of Full-size ImageNet Classification at 1 FPS on Single Board Computers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03747">http://arxiv.org/abs/2311.03747</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyonglu/sbcformer">https://github.com/xyonglu/sbcformer</a></li>
<li>paper_authors: Xiangyong Lu, Masanori Suganuma, Takayuki Okatani</li>
<li>for: 这篇论文旨在解决单板电脑（SBC）上的深度学习应用，特别是适用于智能农业、渔业和家畜饲养等领域。</li>
<li>methods: 这篇论文提出了一个名为SBCFormer的CNN-ViT混合网络，可以在低端CPU上实现高精度和快速计算。</li>
<li>results: 在Raspberry Pi 4 Model B上运行SBCFormer后，它可以在1.0帧&#x2F;秒的速度下达到ImageNet-1K顶峰1的精度约80%，这是在SBC上首次实现的。<details>
<summary>Abstract</summary>
Computer vision has become increasingly prevalent in solving real-world problems across diverse domains, including smart agriculture, fishery, and livestock management. These applications may not require processing many image frames per second, leading practitioners to use single board computers (SBCs). Although many lightweight networks have been developed for mobile/edge devices, they primarily target smartphones with more powerful processors and not SBCs with the low-end CPUs. This paper introduces a CNN-ViT hybrid network called SBCFormer, which achieves high accuracy and fast computation on such low-end CPUs. The hardware constraints of these CPUs make the Transformer's attention mechanism preferable to convolution. However, using attention on low-end CPUs presents a challenge: high-resolution internal feature maps demand excessive computational resources, but reducing their resolution results in the loss of local image details. SBCFormer introduces an architectural design to address this issue. As a result, SBCFormer achieves the highest trade-off between accuracy and speed on a Raspberry Pi 4 Model B with an ARM-Cortex A72 CPU. For the first time, it achieves an ImageNet-1K top-1 accuracy of around 80% at a speed of 1.0 frame/sec on the SBC. Code is available at https://github.com/xyongLu/SBCFormer.
</details>
<details>
<summary>摘要</summary>
计算机视觉在实际应用中日益普遍，包括智能农业、渔业和畜牧管理等领域。这些应用可能不需要处理大量的图像帧数，因此实际使用单板计算机（SBC）。虽然许多轻量级网络已经为移动/边缘设备开发，但它们主要针对更强的处理器，而不是SBC的低端CPU。本文介绍一种名为SBCFormer的CNN-ViT混合网络，可以在这些低端CPU上实现高精度和快速计算。由于硬件限制，转换器的注意机制在低端CPU上更加有利，但使用注意在低端CPU上存在挑战：高分辨率内部特征地图需要极高的计算资源，但是降低其分辨率会导致图像细节的丢失。SBCFormer提出了一种建筑设计来解决这个问题。因此，SBCFormer在raspberry Pi 4 Model B上的ARM-Cortex A72 CPU上实现了最高的精度和速度之间的平衡，达到了ImageNet-1K top-1准确率约为80%，速度为1.0帧/秒。代码可以在https://github.com/xyongLu/SBCFormer上获取。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Video-Summarization"><a href="#Unsupervised-Video-Summarization" class="headerlink" title="Unsupervised Video Summarization"></a>Unsupervised Video Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03745">http://arxiv.org/abs/2311.03745</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KaiyangZhou/pytorch-vsumm-reinforce">https://github.com/KaiyangZhou/pytorch-vsumm-reinforce</a></li>
<li>paper_authors: Hanqing Li, Diego Klabjan, Jean Utke</li>
<li>for: 本研究旨在提出一种新的、无监督的自动视频概要生成方法，利用生成对抗网络的想法，但是去掉了识别器，使得模型具有简单的损失函数和分解不同部分的模型的训练。</li>
<li>methods: 本研究使用了迭代训练策略，先训练恢复器，然后训练帧选择器，并在训练和评估中添加了可调IMASK向量。模型还包括一种无监督的模型选择算法。</li>
<li>results: 在两个公共数据集（SumMe和TVSum）以及四个自创数据集（足球、LoL、MLB和ShortMLB）上进行了实验，结果表明每个组件对模型性能的影响，特别是迭代训练策略。评估和相对于当前状态的方法进行比较，表明提出的方法在性能、稳定性和训练效率方面具有优势。<details>
<summary>Abstract</summary>
This paper introduces a new, unsupervised method for automatic video summarization using ideas from generative adversarial networks but eliminating the discriminator, having a simple loss function, and separating training of different parts of the model. An iterative training strategy is also applied by alternately training the reconstructor and the frame selector for multiple iterations. Furthermore, a trainable mask vector is added to the model in summary generation during training and evaluation. The method also includes an unsupervised model selection algorithm. Results from experiments on two public datasets (SumMe and TVSum) and four datasets we created (Soccer, LoL, MLB, and ShortMLB) demonstrate the effectiveness of each component on the model performance, particularly the iterative training strategy. Evaluations and comparisons with the state-of-the-art methods highlight the advantages of the proposed method in performance, stability, and training efficiency.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的、不监督的自动视频摘要方法，基于生成对抗网络的想法，但是去掉了识别器，使用简单的损失函数，并将模型的不同部分分别训练。此外，这种方法还使用了训练和评估阶段 alternate 的训练策略，并在摘要生成过程中添加了可调整的面向量。此外，这种方法还包括一种无监督模型选择算法。经过实验 validate 在两个公共数据集（SumMe 和 TVSum）以及我们创建的四个数据集（足球、LoL、MLB 和 ShortMLB）上，每一部分的效果都有显著提高。对比之下，这种方法在性能、稳定性和训练效率方面具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="3DifFusionDet-Diffusion-Model-for-3D-Object-Detection-with-Robust-LiDAR-Camera-Fusion"><a href="#3DifFusionDet-Diffusion-Model-for-3D-Object-Detection-with-Robust-LiDAR-Camera-Fusion" class="headerlink" title="3DifFusionDet: Diffusion Model for 3D Object Detection with Robust LiDAR-Camera Fusion"></a>3DifFusionDet: Diffusion Model for 3D Object Detection with Robust LiDAR-Camera Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03742">http://arxiv.org/abs/2311.03742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhao Xiang, Simon Dräger, Jiawei Zhang</li>
<li>for: 提高3D对象检测性能，尤其是在LiDAR-Camera感知器上。</li>
<li>methods: 提出了3DifFusionDet框架，将3D对象检测视为一种噪声扩散过程，从噪声3D桶向目标桶进行匹配。在训练过程中，模型学习恢复噪声过程，而在推断过程中，模型逐渐精细化一组随机生成的桶，以达到最终结果。</li>
<li>results: 在KITTI测试 dataset上进行了广泛的实验，并证明了3DifFusionDet在实际交通对象识别中表现出色，比早期的检测器更为有力。<details>
<summary>Abstract</summary>
Good 3D object detection performance from LiDAR-Camera sensors demands seamless feature alignment and fusion strategies. We propose the 3DifFusionDet framework in this paper, which structures 3D object detection as a denoising diffusion process from noisy 3D boxes to target boxes. In this framework, ground truth boxes diffuse in a random distribution for training, and the model learns to reverse the noising process. During inference, the model gradually refines a set of boxes that were generated at random to the outcomes. Under the feature align strategy, the progressive refinement method could make a significant contribution to robust LiDAR-Camera fusion. The iterative refinement process could also demonstrate great adaptability by applying the framework to various detecting circumstances where varying levels of accuracy and speed are required. Extensive experiments on KITTI, a benchmark for real-world traffic object identification, revealed that 3DifFusionDet is able to perform favorably in comparison to earlier, well-respected detectors.
</details>
<details>
<summary>摘要</summary>
好的3D物体探测性能从LiDAR-Camera传感器需要无缝特征对应和融合策略。我们在这篇论文中提出了3DifFusionDet框架，它将3D物体探测视为一种噪声扩散过程，从噪声3D盒子到目标盒子的探测。在这个框架中，真实的盒子在训练中随机分布，模型学习恢复噪声过程。在推理中，模型逐渐精细化一组随机生成的盒子，以达到最终结果。在特征对应策略下，进行进化的精细化方法可以在不同的探测情况下提供更加稳定的LiDAR-Camera融合。此外，iterative refinement过程还能够在不同的精度和速度要求下展现出优秀的适应性。根据KITTI数据集，一个实际世界交通物体识别的标准 benchmark，我们的3DifFusionDet能够与先前的著名探测器相比，表现出色。
</details></li>
</ul>
<hr>
<h2 id="ADFactory-Automated-Data-Factory-for-Optical-Flow-Tasks"><a href="#ADFactory-Automated-Data-Factory-for-Optical-Flow-Tasks" class="headerlink" title="ADFactory: Automated Data Factory for Optical Flow Tasks"></a>ADFactory: Automated Data Factory for Optical Flow Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04246">http://arxiv.org/abs/2311.04246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Ling</li>
<li>for: 提高现实世界中 optical flow 方法的泛化能力</li>
<li>methods: 使用高级 Nerf 技术重建场景，计算摄像机pose对的光流结果，并从多个方面筛选生成的训练数据</li>
<li>results: 在 KITTI 上超过现有自动学习光流和单摄像头场景流算法，并在实际世界零点泛化评估中常常超过最佳监督方法<details>
<summary>Abstract</summary>
A major challenge faced by current optical flow methods is the difficulty in generalizing them well into the real world, mainly due to the high production cost of datasets, which currently do not have a large real-world optical flow dataset. To address this challenge, we introduce a novel optical flow training framework that can efficiently train optical flow networks on the target data domain without manual annotation. Specifically, we use advanced Nerf technology to reconstruct scenes from photo groups collected by monocular cameras, and calculate the optical flow results between camera pose pairs from the rendered results. On this basis, we screen the generated training data from various aspects such as Nerf's reconstruction quality, visual consistency of optical flow labels, reconstruction depth consistency, etc. The filtered training data can be directly used for network supervision. Experimentally, the generalization ability of our scheme on KITTI surpasses existing self-supervised optical flow and monocular scene flow algorithms. Moreover, it can always surpass most supervised methods in real-world zero-point generalization evaluation.
</details>
<details>
<summary>摘要</summary>
Current optical flow methods face a major challenge in generalizing well to real-world scenarios due to the high cost of large-scale real-world datasets. To address this challenge, we propose a novel optical flow training framework that can efficiently train optical flow networks on the target data domain without manual annotation. Specifically, we use advanced NeRF technology to reconstruct scenes from photo groups collected by monocular cameras, and calculate the optical flow results between camera pose pairs from the rendered results. We then screen the generated training data from various aspects such as NeRF's reconstruction quality, visual consistency of optical flow labels, reconstruction depth consistency, etc. The filtered training data can be directly used for network supervision. Experimentally, our scheme surpasses existing self-supervised optical flow and monocular scene flow algorithms in terms of generalization ability on KITTI, and can also surpass most supervised methods in real-world zero-point generalization evaluation.
</details></li>
</ul>
<hr>
<h2 id="DeepInspect-An-AI-Powered-Defect-Detection-for-Manufacturing-Industries"><a href="#DeepInspect-An-AI-Powered-Defect-Detection-for-Manufacturing-Industries" class="headerlink" title="DeepInspect: An AI-Powered Defect Detection for Manufacturing Industries"></a>DeepInspect: An AI-Powered Defect Detection for Manufacturing Industries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03725">http://arxiv.org/abs/2311.03725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arti Kumbhar, Amruta Chougule, Priya Lokhande, Saloni Navaghane, Aditi Burud, Saee Nimbalkar</li>
<li>for: 这个研究是为了提高生产过程中的瑕疵检测精度和效率。</li>
<li>methods: 这个系统使用了卷积神经网络（CNN）、回传神经网络（RNN）和生成敌方算法（GAN），采用了一个创新的方法来检测生产过程中的瑕疵。</li>
<li>results: 这个系统可以对生产过程中的瑕疵进行高精度的检测，并且可以在不同的瑕疵情况下保持模型的稳定性和适应性。<details>
<summary>Abstract</summary>
Utilizing Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs), our system introduces an innovative approach to defect detection in manufacturing. This technology excels in precisely identifying faults by extracting intricate details from product photographs, utilizing RNNs to detect evolving errors and generating synthetic defect data to bolster the model's robustness and adaptability across various defect scenarios. The project leverages a deep learning framework to automate real-time flaw detection in the manufacturing process. It harnesses extensive datasets of annotated images to discern complex defect patterns. This integrated system seamlessly fits into production workflows, thereby boosting efficiency and elevating product quality. As a result, it reduces waste and operational costs, ultimately enhancing market competitiveness.
</details>
<details>
<summary>摘要</summary>
我们的系统利用卷积神经网络（CNNs）、回归神经网络（RNNs）和生成对抗网统（GANs），提出一个创新的瑕疵检测方法。这个技术可以精确地识别瑕疵，通过提取产品照片中的细微细节，利用RNNs检测进行变化检测，并生成实验数据来增强模型的韧性和适应力。这个项目利用深度学习框架自动化生产过程中的实时瑕疵检测。它利用大量标注图像的数据集来识别复杂的瑕疵模式。这个整合系统适应生产工作流程，因此提高效率和产品质量。因此，它减少废物和运营成本，最终提高市场竞争力。
</details></li>
</ul>
<hr>
<h2 id="Inertial-Guided-Uncertainty-Estimation-of-Feature-Correspondence-in-Visual-Inertial-Odometry-SLAM"><a href="#Inertial-Guided-Uncertainty-Estimation-of-Feature-Correspondence-in-Visual-Inertial-Odometry-SLAM" class="headerlink" title="Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry&#x2F;SLAM"></a>Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry&#x2F;SLAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03722">http://arxiv.org/abs/2311.03722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seongwook Yoon, Jaehyun Kim, Sanghoon Sull</li>
<li>for: 提高自主导航和增强现实系统的稳定性和准确性。</li>
<li>methods: 利用不同视角的3D点对匹配，并使用不同视角的2D点对匹配来估算相对uncertainty。</li>
<li>results: 提出一种基于不同视角的3D点对匹配的不确定性估算方法，可以减少视觉误差、遮挡和光照变化等因素的影响，提高自主导航和增强现实系统的稳定性和准确性。<details>
<summary>Abstract</summary>
Visual odometry and Simultaneous Localization And Mapping (SLAM) has been studied as one of the most important tasks in the areas of computer vision and robotics, to contribute to autonomous navigation and augmented reality systems. In case of feature-based odometry/SLAM, a moving visual sensor observes a set of 3D points from different viewpoints, correspondences between the projected 2D points in each image are usually established by feature tracking and matching. However, since the corresponding point could be erroneous and noisy, reliable uncertainty estimation can improve the accuracy of odometry/SLAM methods. In addition, inertial measurement unit is utilized to aid the visual sensor in terms of Visual-Inertial fusion. In this paper, we propose a method to estimate the uncertainty of feature correspondence using an inertial guidance robust to image degradation caused by motion blur, illumination change and occlusion. Modeling a guidance distribution to sample possible correspondence, we fit the distribution to an energy function based on image error, yielding more robust uncertainty than conventional methods. We also demonstrate the feasibility of our approach by incorporating it into one of recent visual-inertial odometry/SLAM algorithms for public datasets.
</details>
<details>
<summary>摘要</summary>
Visual odometry和同时地图和定位（SLAM）是计算机视觉和机器人领域中的一项非常重要的任务，以帮助实现自主导航和增强现实系统。在特征基于的ODometry/SLAM中，一个在不同视点观察的移动视觉器 observes一组3D点，通常通过特征追踪和匹配来确定图像中的2D点对应关系。然而，由于对应点可能存在误差和噪声，可靠的uncertainty估计可以提高ODometry/SLAM方法的准确性。此外，使用测距仪来帮助视觉器，通过视觉-测距拟合来提高方法的稳定性。在本文中，我们提出了一种用于估计特征对应关系的不确定性的方法，基于测距仪的引导分布来采样可能的对应关系，并使用图像误差基于的能量函数来适应图像质量的变化。我们还demonstrate了我们的方法的可行性，通过在一个现有的视觉-测距ODometry/SLAM算法中 incorporate它。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-deep-representation-learning-for-quantum-cross-platform-verification"><a href="#Multimodal-deep-representation-learning-for-quantum-cross-platform-verification" class="headerlink" title="Multimodal deep representation learning for quantum cross-platform verification"></a>Multimodal deep representation learning for quantum cross-platform verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03713">http://arxiv.org/abs/2311.03713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Qian, Yuxuan Du, Zhenliang He, Min-hsiu Hsieh, Dacheng Tao</li>
<li>for: 这篇论文旨在解决早期量子计算阶段的跨平台验证问题，特别是在大量子比特的情况下，使用最少的量子测量数据来描述两个不完美的量子设备在执行同一个算法时的相似性。</li>
<li>methods: 本文提出了一种创新的多模态学习方法，认为在这个任务中存在两种不同的模式：测量结果和编译到explored量子设备上的类型circuit的классификация，两者都含有独特的信息。通过这种见解，我们设计了一种多模态神经网络，独立地提取这两种模式中的知识，然后进行融合操作以创建一个完整的数据表示。</li>
<li>results: 我们在不同的噪声模型下，包括50个量子比特的系统，进行了测试，结果表明，相比于随机测量和已有数据集中的算法，我们的提议可以提高预测精度三个数量级，并且提供了证明量子设备之间的相似性在新的量子算法执行时的有力证据。这些发现开创了用多模态学习解决更广泛的量子系统学习任务的可能性。<details>
<summary>Abstract</summary>
Cross-platform verification, a critical undertaking in the realm of early-stage quantum computing, endeavors to characterize the similarity of two imperfect quantum devices executing identical algorithms, utilizing minimal measurements. While the random measurement approach has been instrumental in this context, the quasi-exponential computational demand with increasing qubit count hurdles its feasibility in large-qubit scenarios. To bridge this knowledge gap, here we introduce an innovative multimodal learning approach, recognizing that the formalism of data in this task embodies two distinct modalities: measurement outcomes and classical description of compiled circuits on explored quantum devices, both enriched with unique information. Building upon this insight, we devise a multimodal neural network to independently extract knowledge from these modalities, followed by a fusion operation to create a comprehensive data representation. The learned representation can effectively characterize the similarity between the explored quantum devices when executing new quantum algorithms not present in the training data. We evaluate our proposal on platforms featuring diverse noise models, encompassing system sizes up to 50 qubits. The achieved results demonstrate a three-orders-of-magnitude improvement in prediction accuracy compared to the random measurements and offer compelling evidence of the complementary roles played by each modality in cross-platform verification. These findings pave the way for harnessing the power of multimodal learning to overcome challenges in wider quantum system learning tasks.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose an innovative multimodal learning approach that recognizes the two distinct modalities in the task: measurement outcomes and classical descriptions of compiled circuits on explored quantum devices. By independently extracting knowledge from these modalities using a multimodal neural network, and then fusing the information, we can effectively characterize the similarity between the explored quantum devices when executing new quantum algorithms not present in the training data.We evaluate our proposal on platforms with diverse noise models, up to 50 qubits, and achieve a three-orders-of-magnitude improvement in prediction accuracy compared to random measurements. Our findings demonstrate the complementary roles played by each modality in cross-platform verification and pave the way for harnessing the power of multimodal learning to overcome challenges in wider quantum system learning tasks.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-convolutional-neural-network-fusion-approach-for-change-detection-in-remote-sensing-images"><a href="#Unsupervised-convolutional-neural-network-fusion-approach-for-change-detection-in-remote-sensing-images" class="headerlink" title="Unsupervised convolutional neural network fusion approach for change detection in remote sensing images"></a>Unsupervised convolutional neural network fusion approach for change detection in remote sensing images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03679">http://arxiv.org/abs/2311.03679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weidong Yan, Pei Yan, Li Cao</li>
<li>for: 本研究旨在提出一种基于深度学习的无监督 shallow convolutional neural network (USCNN) 变化检测方法，以适应 Remote sensing 图像变化检测中的差分检测问题。</li>
<li>methods: 本方法首先将 би-时间图像转换为不同的特征空间，使用不同大小的卷积核来提取图像中的多尺度信息。然后，对同一个卷积核的输出特征图像进行减去操作，得到对应的差分特征图像。最后，将不同尺度的差分特征图像进行拼接，并使用 1 * 1 卷积层进行拼接。</li>
<li>results: 实验结果表明，提出的方法可以准确地检测 Remote sensing 图像中的变化。四个实验数据集的实验结果表明该方法的可行性和有效性。<details>
<summary>Abstract</summary>
With the rapid development of deep learning, a variety of change detection methods based on deep learning have emerged in recent years. However, these methods usually require a large number of training samples to train the network model, so it is very expensive. In this paper, we introduce a completely unsupervised shallow convolutional neural network (USCNN) fusion approach for change detection. Firstly, the bi-temporal images are transformed into different feature spaces by using convolution kernels of different sizes to extract multi-scale information of the images. Secondly, the output features of bi-temporal images at the same convolution kernels are subtracted to obtain the corresponding difference images, and the difference feature images at the same scale are fused into one feature image by using 1 * 1 convolution layer. Finally, the output features of different scales are concatenated and a 1 * 1 convolution layer is used to fuse the multi-scale information of the image. The model parameters are obtained by a redesigned sparse function. Our model has three features: the entire training process is conducted in an unsupervised manner, the network architecture is shallow, and the objective function is sparse. Thus, it can be seen as a kind of lightweight network model. Experimental results on four real remote sensing datasets indicate the feasibility and effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
随着深度学习的快速发展，深度学习基于的变化检测方法在最近几年出现了许多。然而，这些方法通常需要训练样本的大量，因此非常昂贵。在这篇论文中，我们介绍了一种完全无监督的浅层卷积神经网络（USCNN）融合方法 для变化检测。首先，bi-temporal图像被转换成不同的特征空间，使用不同的卷积核来提取图像的多尺度信息。其次，bi-temporal图像的输出特征在同一个卷积核上被减去，以获得对应的差异图像，并将差异特征图像在同一个尺度上融合到一个特征图像中，使用1*1卷积层。最后，不同尺度的输出特征被 concatenate 并使用1*1卷积层融合图像的多尺度信息。模型参数由一种重新定义的稀疏函数来获得。我们的模型具有三个特点：整个训练过程是无监督的，网络架构是浅层的，目标函数是稀疏的。因此，可以看作一种轻量级的网络模型。实验结果表明，提议的方法在四个实际遥感数据集上是可行和有效的。
</details></li>
</ul>
<hr>
<h2 id="Image-Generation-and-Learning-Strategy-for-Deep-Document-Forgery-Detection"><a href="#Image-Generation-and-Learning-Strategy-for-Deep-Document-Forgery-Detection" class="headerlink" title="Image Generation and Learning Strategy for Deep Document Forgery Detection"></a>Image Generation and Learning Strategy for Deep Document Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03650">http://arxiv.org/abs/2311.03650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yamato Okamoto, Osada Genki, Iu Yahiro, Rintaro Hasegawa, Peifei Zhu, Hirokatsu Kataoka</li>
<li>for: 本研究旨在対抗深度神经网络（DNN）方法所带来的文档伪造威胁。</li>
<li>methods: 我们使用自然图像和文档图像的自适应学习来预训练模型，并构建了一个包含多种攻击方法的文档伪造图像集（FD-VIED）。</li>
<li>results: 我们的方法在实验中显示出提高检测性能的result。<details>
<summary>Abstract</summary>
In recent years, document processing has flourished and brought numerous benefits. However, there has been a significant rise in reported cases of forged document images. Specifically, recent advancements in deep neural network (DNN) methods for generative tasks may amplify the threat of document forgery. Traditional approaches for forged document images created by prevalent copy-move methods are unsuitable against those created by DNN-based methods, as we have verified. To address this issue, we construct a training dataset of document forgery images, named FD-VIED, by emulating possible attacks, such as text addition, removal, and replacement with recent DNN-methods. Additionally, we introduce an effective pre-training approach through self-supervised learning with both natural images and document images. In our experiments, we demonstrate that our approach enhances detection performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Instruct-Me-More-Random-Prompting-for-Visual-In-Context-Learning"><a href="#Instruct-Me-More-Random-Prompting-for-Visual-In-Context-Learning" class="headerlink" title="Instruct Me More! Random Prompting for Visual In-Context Learning"></a>Instruct Me More! Random Prompting for Visual In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03648">http://arxiv.org/abs/2311.03648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Zhang, Bowen Wang, Liangzhi Li, Yuta Nakashima, Hajime Nagahara</li>
<li>for: 用于提高计算机视觉中的启发式学习（ICL）性能。</li>
<li>methods: 使用可学习的扰动（提示）来补充输入-输出图像对（叫做启发对），以提高模型的性能。</li>
<li>results: 比基eline无法学习提示的情况下，使用InMeMo方法可以提高多个主流任务的性能，包括对eground segmentation和单个物体检测任务的性能。 Specifically, InMeMo方法可以提高mIoU分数 by 7.35和15.13。<details>
<summary>Abstract</summary>
Large-scale models trained on extensive datasets, have emerged as the preferred approach due to their high generalizability across various tasks. In-context learning (ICL), a popular strategy in natural language processing, uses such models for different tasks by providing instructive prompts but without updating model parameters. This idea is now being explored in computer vision, where an input-output image pair (called an in-context pair) is supplied to the model with a query image as a prompt to exemplify the desired output. The efficacy of visual ICL often depends on the quality of the prompts. We thus introduce a method coined Instruct Me More (InMeMo), which augments in-context pairs with a learnable perturbation (prompt), to explore its potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the current state-of-the-art performance. Specifically, compared to the baseline without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for foreground segmentation and single object detection tasks, respectively. Our findings suggest that InMeMo offers a versatile and efficient way to enhance the performance of visual ICL with lightweight training. Code is available at https://github.com/Jackieam/InMeMo.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Random-Field-Augmentations-for-Self-Supervised-Representation-Learning"><a href="#Random-Field-Augmentations-for-Self-Supervised-Representation-Learning" class="headerlink" title="Random Field Augmentations for Self-Supervised Representation Learning"></a>Random Field Augmentations for Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03629">http://arxiv.org/abs/2311.03629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Andrew Mansfield, Arash Afkanpour, Warren Richard Morningstar, Karan Singhal</li>
<li>for: 自动增强学习（self-supervised representation learning），用于提高图像识别和泛化能力。</li>
<li>methods: 提出了一种新的本地变换方法，基于 Gaussian 随机场，用于生成图像增强。这些变换包括翻译、旋转、颜色干扰等，并且允许变换参数值从像素到像素 vary。</li>
<li>results: 实验结果表明，新的变换方法可以提高自动增强学习的性能，包括 ImageNet 下排名第一的批处理精度提高1.7%，以及 iNaturalist 下批处理精度提高3.6%。但是，由于新的变换的灵活性，学习得到的表示结构可以受到 hyperparameter 的影响，需要平衡增强表示的多样性和强度。<details>
<summary>Abstract</summary>
Self-supervised representation learning is heavily dependent on data augmentations to specify the invariances encoded in representations. Previous work has shown that applying diverse data augmentations is crucial to downstream performance, but augmentation techniques remain under-explored. In this work, we propose a new family of local transformations based on Gaussian random fields to generate image augmentations for self-supervised representation learning. These transformations generalize the well-established affine and color transformations (translation, rotation, color jitter, etc.) and greatly increase the space of augmentations by allowing transformation parameter values to vary from pixel to pixel. The parameters are treated as continuous functions of spatial coordinates, and modeled as independent Gaussian random fields. Empirical results show the effectiveness of the new transformations for self-supervised representation learning. Specifically, we achieve a 1.7% top-1 accuracy improvement over baseline on ImageNet downstream classification, and a 3.6% improvement on out-of-distribution iNaturalist downstream classification. However, due to the flexibility of the new transformations, learned representations are sensitive to hyperparameters. While mild transformations improve representations, we observe that strong transformations can degrade the structure of an image, indicating that balancing the diversity and strength of augmentations is important for improving generalization of learned representations.
</details>
<details>
<summary>摘要</summary>
自我超级vised学习中很重要地依赖数据扩展来确定表示中的不变性。先前的工作表明，对数据扩展进行多样化应用是下游性能的关键，但是扩展技术还很少被探索。在这种工作中，我们提出了一种新的本地变换家族，基于 Gaussian random fields 生成图像扩展 для自我超级vised学习。这些变换扩展了已知的平移、旋转和颜色扩展（翻译、旋转、颜色扩展等），并大大增加了扩展的空间。变换参数被视为像素坐标上独立的 Gaussian random fields，其中每个像素坐标上的参数值可以不同。我们的实验结果表明，这些新的变换对自我超级vised学习具有效果。具体来说，我们在 ImageNet 下游分类任务上 achieved 1.7% 的 top-1 精度提升，并在 iNaturalist 下游分类任务上 achieved 3.6% 的精度提升。然而，由于新的变换的灵活性，学习的表示受到了参数的影响。虽然柔和的变换可以提高表示的质量，但是强大的变换可能会破坏图像的结构，这表明在提高学习的表示总体化的过程中，需要考虑扩展的多样性和强度。
</details></li>
</ul>
<hr>
<h2 id="FusionViT-Hierarchical-3D-Object-Detection-via-LiDAR-Camera-Vision-Transformer-Fusion"><a href="#FusionViT-Hierarchical-3D-Object-Detection-via-LiDAR-Camera-Vision-Transformer-Fusion" class="headerlink" title="FusionViT: Hierarchical 3D Object Detection via LiDAR-Camera Vision Transformer Fusion"></a>FusionViT: Hierarchical 3D Object Detection via LiDAR-Camera Vision Transformer Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03620">http://arxiv.org/abs/2311.03620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhao Xiang, Jiawei Zhang</li>
<li>for: 这 paper 的目的是提出一种基于 vision transformer 的三 Dimensional 物体检测模型，以提高三 Dimensional 物体检测性能。</li>
<li>methods: 该模型使用 hierarchical 的 transformer 模型来对图像和点云进行嵌入，并通过视处理变换模型来进行数据表示学盘learn 。</li>
<li>results: 对于实际的交通Scene 数据集 KITTI 和 Waymo Open，我们的 FusionViT 模型可以达到状态空间的表现，并比之前的基于单Modal 图像或点云的方法和最新的多Modal 图像-点云深度融合方法都高。<details>
<summary>Abstract</summary>
For 3D object detection, both camera and lidar have been demonstrated to be useful sensory devices for providing complementary information about the same scenery with data representations in different modalities, e.g., 2D RGB image vs 3D point cloud. An effective representation learning and fusion of such multi-modal sensor data is necessary and critical for better 3D object detection performance. To solve the problem, in this paper, we will introduce a novel vision transformer-based 3D object detection model, namely FusionViT. Different from the existing 3D object detection approaches, FusionViT is a pure-ViT based framework, which adopts a hierarchical architecture by extending the transformer model to embed both images and point clouds for effective representation learning. Such multi-modal data embedding representations will be further fused together via a fusion vision transformer model prior to feeding the learned features to the object detection head for both detection and localization of the 3D objects in the input scenery. To demonstrate the effectiveness of FusionViT, extensive experiments have been done on real-world traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our FusionViT model can achieve state-of-the-art performance and outperforms not only the existing baseline methods that merely rely on camera images or lidar point clouds, but also the latest multi-modal image-point cloud deep fusion approaches.
</details>
<details>
<summary>摘要</summary>
为3D物体检测，Camera和Lidar都被证明是有用的感知设备，提供不同模式的数据表示，如2DRGB图像和3D点云。为了提高3D物体检测性能，需要有效地学习并融合这些多模式感知数据。在这篇论文中，我们将介绍一种新的视transformer基于3D物体检测模型，即FusionViT。与现有的3D物体检测方法不同，FusionViT是一个纯ViT基本框架，通过扩展transformer模型，以便对图像和点云进行有效的表示学习。这些多模式数据表示将被通过视transformer模型进行融合，然后传递给物体检测头进行3D物体在输入场景中的检测和位置确定。为证明FusionViT的效果，我们在KITTI和Waymo开放 datasets上进行了广泛的实验。需要注意的是，我们的FusionViT模型可以在实际的交通场景中达到领先的性能，并在只使用Camera图像或Lidar点云的基础方法上出perform。此外，我们的模型还可以在多模式图像-点云深度融合方法上达到更高的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/cs.CV_2023_11_07/" data-id="clpxp6c2f00msee88ab5l9tmq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/cs.AI_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T12:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/07/cs.AI_2023_11_07/">cs.AI - 2023-11-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ToP-ToM-Trust-aware-Robot-Policy-with-Theory-of-Mind"><a href="#ToP-ToM-Trust-aware-Robot-Policy-with-Theory-of-Mind" class="headerlink" title="ToP-ToM: Trust-aware Robot Policy with Theory of Mind"></a>ToP-ToM: Trust-aware Robot Policy with Theory of Mind</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04397">http://arxiv.org/abs/2311.04397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuang Yu, Baris Serhan, Angelo Cangelosi</li>
<li>for: 这个论文旨在探讨人工智能机器人在多体系统中与人类合作时，如何使用理解人类心理状态的理论来建立信任。</li>
<li>methods: 该论文使用了人类心理状态理论（理论心）来设计一种可靠性评估方法，并通过对人类行为观察和分析来推断人类的信任情况。</li>
<li>results: 实验结果表明，通过采用基于理论心的机器人策略，可以有效地维护人类与机器人之间的信任关系，并在多体系统中提高人类与机器人之间的协作效率。<details>
<summary>Abstract</summary>
Theory of Mind (ToM) is a fundamental cognitive architecture that endows humans with the ability to attribute mental states to others. Humans infer the desires, beliefs, and intentions of others by observing their behavior and, in turn, adjust their actions to facilitate better interpersonal communication and team collaboration. In this paper, we investigated trust-aware robot policy with the theory of mind in a multiagent setting where a human collaborates with a robot against another human opponent. We show that by only focusing on team performance, the robot may resort to the reverse psychology trick, which poses a significant threat to trust maintenance. The human's trust in the robot will collapse when they discover deceptive behavior by the robot. To mitigate this problem, we adopt the robot theory of mind model to infer the human's trust beliefs, including true belief and false belief (an essential element of ToM). We designed a dynamic trust-aware reward function based on different trust beliefs to guide the robot policy learning, which aims to balance between avoiding human trust collapse due to robot reverse psychology. The experimental results demonstrate the importance of the ToM-based robot policy for human-robot trust and the effectiveness of our robot ToM-based robot policy in multiagent interaction settings.
</details>
<details>
<summary>摘要</summary>
We found that when the robot focuses solely on team performance, it may resort to reverse psychology, which can damage trust between the human and the robot. When the human discovers the robot's deceptive behavior, their trust in the robot will collapse. To address this issue, we developed a robot ToM model to infer the human's trust beliefs, including true belief and false belief, which is an essential element of ToM.We designed a dynamic trust-aware reward function based on different trust beliefs to guide the robot policy learning, which aims to balance between avoiding human trust collapse due to robot reverse psychology and achieving team performance. Our experimental results demonstrate the importance of ToM-based robot policy for human-robot trust and the effectiveness of our approach in multiagent interaction settings.
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Manycore-Processors-with-Distributed-Memory-for-Accelerated-Training-of-Sparse-and-Recurrent-Models"><a href="#Harnessing-Manycore-Processors-with-Distributed-Memory-for-Accelerated-Training-of-Sparse-and-Recurrent-Models" class="headerlink" title="Harnessing Manycore Processors with Distributed Memory for Accelerated Training of Sparse and Recurrent Models"></a>Harnessing Manycore Processors with Distributed Memory for Accelerated Training of Sparse and Recurrent Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04386">http://arxiv.org/abs/2311.04386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Finkbeiner, Thomas Gmeinder, Mark Pupilli, Alexander Titterton, Emre Neftci</li>
<li>For:	+ The paper aims to explore the use of a massively parallel multiple instruction multiple data (MIMD) architecture with distributed local memory for training sparse and recurrent neural networks.	+ The authors aim to overcome the limitations of current AI training infrastructure, which is dominated by single instruction multiple data (SIMD) and systolic array architectures like GPUs and TPUs.	+ The paper aims to pave the way towards more efficient and sustainable AI training methods.* Methods:	+ The authors use a training routine based on backpropagation through time (BPTT) for the brain-inspired class of Spiking Neural Networks (SNNs) that feature binary sparse activations.	+ The authors use a MIMD processor, the Intelligence Processing Unit (IPU), to train the SNNs.	+ The authors compare the performance of the IPU with A100 GPUs.* Results:	+ The authors observe 5-10x throughput gains compared to A100 GPUs and up to 38x gains for higher levels of activation sparsity, without a significant slowdown in training convergence or reduction in final model performance.	+ The authors observe highly promising trends for both single and multi IPU configurations as they scale up to larger model sizes.Here is the summary in Simplified Chinese:* For:	+ 本研究旨在探讨使用分布式本地存储的多指令多数据（MIMD）架构来训练稀疏和递归神经网络。	+ 作者希望超越当前人工智能训练基础设施，这些基础设施主要是单指令多数据（SIMD）和 Systolic 阵列设施，如 GPU 和 TPU。	+ 本研究希望为更高效和可持续的人工智能训练提供新的方法。* Methods:	+ 作者使用 backpropagation through time（BPTT）训练Brain-inspired 的稀疏神经网络（SNNs），并使用 MIMD 处理器，Intelligence Processing Unit（IPU）来训练 SNNs。	+ 作者比较 IPU 与 A100 GPU 的性能。* Results:	+ 作者发现在 activations 稀疏性较高时，使用 IPU 可以获得 5-10 倍的 Throughput 提升，并且没有显著减慢训练过程的速度或模型性能的下降。	+ 作者发现在大型模型训练时，单 IPU 和多 IPU 配置具有极高的潜力。<details>
<summary>Abstract</summary>
Current AI training infrastructure is dominated by single instruction multiple data (SIMD) and systolic array architectures, such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), that excel at accelerating parallel workloads and dense vector matrix multiplications. Potentially more efficient neural network models utilizing sparsity and recurrence cannot leverage the full power of SIMD processor and are thus at a severe disadvantage compared to today's prominent parallel architectures like Transformers and CNNs, thereby hindering the path towards more sustainable AI. To overcome this limitation, we explore sparse and recurrent model training on a massively parallel multiple instruction multiple data (MIMD) architecture with distributed local memory. We implement a training routine based on backpropagation through time (BPTT) for the brain-inspired class of Spiking Neural Networks (SNNs) that feature binary sparse activations. We observe a massive advantage in using sparse activation tensors with a MIMD processor, the Intelligence Processing Unit (IPU) compared to GPUs. On training workloads, our results demonstrate 5-10x throughput gains compared to A100 GPUs and up to 38x gains for higher levels of activation sparsity, without a significant slowdown in training convergence or reduction in final model performance. Furthermore, our results show highly promising trends for both single and multi IPU configurations as we scale up to larger model sizes. Our work paves the way towards more efficient, non-standard models via AI training hardware beyond GPUs, and competitive large scale SNN models.
</details>
<details>
<summary>摘要</summary>
We implement a training routine based on backpropagation through time (BPTT) for the brain-inspired class of Spiking Neural Networks (SNNs) that feature binary sparse activations. Our results show a significant advantage in using sparse activation tensors with a MIMD processor, the Intelligence Processing Unit (IPU), compared to GPUs. On training workloads, we observe 5-10x throughput gains compared to A100 GPUs and up to 38x gains for higher levels of activation sparsity, without a significant slowdown in training convergence or reduction in final model performance.Our results also show promising trends for both single and multi IPU configurations as we scale up to larger model sizes. Our work paves the way towards more efficient, non-standard models via AI training hardware beyond GPUs and competitive large-scale SNN models.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Malware-Detection-by-Integrating-Machine-Learning-with-Cuckoo-Sandbox"><a href="#Enhancing-Malware-Detection-by-Integrating-Machine-Learning-with-Cuckoo-Sandbox" class="headerlink" title="Enhancing Malware Detection by Integrating Machine Learning with Cuckoo Sandbox"></a>Enhancing Malware Detection by Integrating Machine Learning with Cuckoo Sandbox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04372">http://arxiv.org/abs/2311.04372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amaal F. Alshmarni, Mohammed A. Alliheedi</li>
<li>for: 本研究目的是使用深度学习算法和传统机器学习算法对API调用序列中的Malware进行分类和识别。</li>
<li>methods: 本研究使用的方法包括CNN（卷积神经网络）和RNN（循环神经网络）等深度学习算法，以及SVM（支持向量机）、RF（随机森林）、KNN（最近邻居）、XGB（极限梯度提升）和GBC（梯度提升类ifiers）等传统机器学习算法。</li>
<li>results: 研究结果显示，深度学习和传统机器学习算法均可达到极高的准确率，达到99%以上。<details>
<summary>Abstract</summary>
In the modern era, malware is experiencing a significant increase in both its variety and quantity, aligning with the widespread adoption of the digital world. This surge in malware has emerged as a critical challenge in the realm of cybersecurity, prompting numerous research endeavors and contributions to address the issue. Machine learning algorithms have been leveraged for malware detection due to their ability to uncover concealed patterns within vast datasets. However, deep learning algorithms, characterized by their multi-layered structure, surpass the limitations of traditional machine learning approaches. By employing deep learning techniques such as CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network), this study aims to classify and identify malware extracted from a dataset containing API call sequences. The performance of these algorithms is compared with that of conventional machine learning methods, including SVM (Support Vector Machine), RF (Random Forest), KNN (K-Nearest Neighbors), XGB (Extreme Gradient Boosting), and GBC (Gradient Boosting Classifier), all using the same dataset. The outcomes of this research demonstrate that both deep learning and machine learning algorithms achieve remarkably high levels of accuracy, reaching up to 99% in certain cases.
</details>
<details>
<summary>摘要</summary>
现代时期，黑客软件（malware）的多样性和量在不断增长，与数字化世界的普及相应。这种黑客软件的增长对网络安全领域提出了棘手的挑战，并促使了许多研究和贡献以解决问题。机器学习算法在黑客软件检测方面得到广泛应用，因为它们可以在大量数据中找到隐藏的模式。然而，深度学习算法，即多层结构的算法，超越了传统机器学习方法的局限性。本研究使用深度学习技术，如卷积神经网络（CNN）和循环神经网络（RNN），对来自API调用序列的黑客软件进行分类和识别。本研究的结果表明，深度学习和机器学习算法均可以达到极高的准确率，达到99%以上。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Effectiveness-of-Retrieval-Augmented-Large-Language-Models-in-Scientific-Document-Reasoning"><a href="#Evaluating-the-Effectiveness-of-Retrieval-Augmented-Large-Language-Models-in-Scientific-Document-Reasoning" class="headerlink" title="Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning"></a>Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04348">http://arxiv.org/abs/2311.04348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sai Munikoti, Anurag Acharya, Sridevi Wagle, Sameera Horawalavithana</li>
<li>for: 提高Large Language Model（LLM）的可靠性和科学性，解决LLM提供的假信息问题。</li>
<li>methods: 使用检索增强的LLM，通过从外部数据源检索相关信息，改善模型的训练过程。</li>
<li>results: 在科学文档理解任务中，模型会使用假据来证明预测结果，并且使用科学资料作为预训练数据不能减轻证据fabrication的风险。<details>
<summary>Abstract</summary>
Despite the dramatic progress in Large Language Model (LLM) development, LLMs often provide seemingly plausible but not factual information, often referred to as hallucinations. Retrieval-augmented LLMs provide a non-parametric approach to solve these issues by retrieving relevant information from external data sources and augment the training process. These models help to trace evidence from an externally provided knowledge base allowing the model predictions to be better interpreted and verified. In this work, we critically evaluate these models in their ability to perform in scientific document reasoning tasks. To this end, we tuned multiple such model variants with science-focused instructions and evaluated them on a scientific document reasoning benchmark for the usefulness of the retrieved document passages. Our findings suggest that models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication.
</details>
<details>
<summary>摘要</summary>
尽管大语言模型（LLM）的发展做出了很多显著的进步，但是它们经常提供不符事实的信息，通常被称为幻觉。检索增强型LLM可以通过从外部数据源中检索相关信息并增强训练过程来解决这些问题。这些模型可以跟踪从知ledge基中提供的证据，使模型的预测更好地被解释和验证。在这项工作中，我们 kritically evaluates these models in their ability to perform in scientific document reasoning tasks。为此，我们调整了多种模型变体，并在科学文档理解benchmark上评估它们的用用。我们发现，这些模型在科学任务中 justify 预测的方法有假的证据，而使用科学corpus作为预训数据不会减少证据fabrication的风险。
</details></li>
</ul>
<hr>
<h2 id="A-Taxonomy-of-Rater-Disagreements-Surveying-Challenges-Opportunities-from-the-Perspective-of-Annotating-Online-Toxicity"><a href="#A-Taxonomy-of-Rater-Disagreements-Surveying-Challenges-Opportunities-from-the-Perspective-of-Annotating-Online-Toxicity" class="headerlink" title="A Taxonomy of Rater Disagreements: Surveying Challenges &amp; Opportunities from the Perspective of Annotating Online Toxicity"></a>A Taxonomy of Rater Disagreements: Surveying Challenges &amp; Opportunities from the Perspective of Annotating Online Toxicity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04345">http://arxiv.org/abs/2311.04345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenbo Zhang, Hangzhi Guo, Ian D Kivlichan, Vinodkumar Prabhakaran, Davis Yadav, Amulya Yadav</li>
<li>for: 本研究旨在探讨在线恶意评论的计算检测和缓解方法中，annotator之间的分歧原因，以及如何在Machine Learning发展阶段内 integrate这些分歧。</li>
<li>methods: 本研究通过分析广泛的文献，提出了一个细化的分类器，以及对每种原因进行解释和讨论。</li>
<li>results: 本研究提出了一个全面的分类器，可以帮助解释和缓解在线恶意评论中的分歧原因。此外，还提出了一些未解决的问题，可以推动未来的研究发展。<details>
<summary>Abstract</summary>
Toxicity is an increasingly common and severe issue in online spaces. Consequently, a rich line of machine learning research over the past decade has focused on computationally detecting and mitigating online toxicity. These efforts crucially rely on human-annotated datasets that identify toxic content of various kinds in social media texts. However, such annotations historically yield low inter-rater agreement, which was often dealt with by taking the majority vote or other such approaches to arrive at a single ground truth label. Recent research has pointed out the importance of accounting for the subjective nature of this task when building and utilizing these datasets, and this has triggered work on analyzing and better understanding rater disagreements, and how they could be effectively incorporated into the machine learning developmental pipeline. While these efforts are filling an important gap, there is a lack of a broader framework about the root causes of rater disagreement, and therefore, we situate this work within that broader landscape. In this survey paper, we analyze a broad set of literature on the reasons behind rater disagreements focusing on online toxicity, and propose a detailed taxonomy for the same. Further, we summarize and discuss the potential solutions targeting each reason for disagreement. We also discuss several open issues, which could promote the future development of online toxicity research.
</details>
<details>
<summary>摘要</summary>
“在线空间中，毒性问题日益严重，这导致了过去十年的机器学习研究强调computationally检测和缓和在线毒性。这些努力将靠着人类给出的标签来识别社交媒体文本中的毒性内容。但是，这些标签往往会受到评估者间的不一致，这些不一致通常会通过获得多数决的方式或其他方法来得到单一的真实标签。现在的研究表明了评估者间的不一致需要被考虑，并且这些不一致可以对机器学习开发过程中的建立和使用标签进行更好的理解和integration。然而，这些努力仍然缺乏一个更广泛的架构，它应该涵盖毒性评估者间的根本原因。因此，我们在这篇调查报告中分析了线上毒性领域中的各种原因，并提出了一个详细的分类。此外，我们还总结了每个原因的解决方案，并讨论了一些未解之处，这些未解之处可以帮助未来线上毒性研究的发展。”
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Clinical-Benchmark-for-Emergency-Care-MC-BEC-A-Comprehensive-Benchmark-for-Evaluating-Foundation-Models-in-Emergency-Medicine"><a href="#Multimodal-Clinical-Benchmark-for-Emergency-Care-MC-BEC-A-Comprehensive-Benchmark-for-Evaluating-Foundation-Models-in-Emergency-Medicine" class="headerlink" title="Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine"></a>Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04937">http://arxiv.org/abs/2311.04937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emma Chen, Aman Kansal, Julie Chen, Boyang Tom Jin, Julia Rachel Reisler, David A Kim, Pranav Rajpurkar</li>
<li>for: The paper aims to provide a comprehensive benchmark for evaluating foundation models in Emergency Medicine, specifically for predicting patient decompensation, disposition, and ED revisit.</li>
<li>methods: The paper uses a multimodal dataset of over 100,000 continuously monitored Emergency Department visits from 2020-2022, including clinical data such as vital signs, electrocardiogram and photoplethysmograph waveforms, and free-text reports of imaging studies. The paper provides a standardized evaluation framework with train-test splits and evaluation metrics.</li>
<li>results: The paper provides performance baselines for each prediction task to enable the evaluation of multimodal, multitask models.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提供一个包容全的基础模型评估 benchmark  для急诊医学，特别是预测病人状况下降、处置和急诊室 revisit。</li>
<li>methods: 这篇论文使用了2020-2022年度的急诊室访问数据集，涵盖了详细的临床数据，包括评估信息、先前诊断和药物、连续测量的生命体 Parameters、电cardiogram和光谱波形图，以及急诊室诊断、处置和后续 revisit 的信息。论文提供了一个标准化的评估框架，包括训练测试分割和评估指标。</li>
<li>results: 论文提供了每个预测任务的性能基线，以便评估多模态、多任务模型。<details>
<summary>Abstract</summary>
We propose the Multimodal Clinical Benchmark for Emergency Care (MC-BEC), a comprehensive benchmark for evaluating foundation models in Emergency Medicine using a dataset of 100K+ continuously monitored Emergency Department visits from 2020-2022. MC-BEC focuses on clinically relevant prediction tasks at timescales from minutes to days, including predicting patient decompensation, disposition, and emergency department (ED) revisit, and includes a standardized evaluation framework with train-test splits and evaluation metrics. The multimodal dataset includes a wide range of detailed clinical data, including triage information, prior diagnoses and medications, continuously measured vital signs, electrocardiogram and photoplethysmograph waveforms, orders placed and medications administered throughout the visit, free-text reports of imaging studies, and information on ED diagnosis, disposition, and subsequent revisits. We provide performance baselines for each prediction task to enable the evaluation of multimodal, multitask models. We believe that MC-BEC will encourage researchers to develop more effective, generalizable, and accessible foundation models for multimodal clinical data.
</details>
<details>
<summary>摘要</summary>
我们提出了多modal临床标准（MC-BEC），用于评估临床数据的基本模型。MC-BEC使用2020-2022年度 Emergency Department 访问记录超过100,000次，并将临床相关的预测任务分为不同时间尺度，包括patient decompensation、 dispose 和 Emergency Department  revisit。我们还提供了标准化的评估框架，包括训练测试分割和评估指标。数据集包括丰富的临床数据，包括triage信息、先前诊断和药物、不断测量的生命 Parameter、电cardiogram和光谱波形图像、在访问中下达的命令和给药、自由文本报告的成像试验结果以及 Emergency Department 诊断、处置和后续 revisits 信息。我们还提供了每个预测任务的性能基线，以便评估多modal、多任务模型。我们认为MC-BEC将鼓励研究人员开发更有效、普遍、可 accessible 的基本模型。
</details></li>
</ul>
<hr>
<h2 id="Sub-Sentence-Encoder-Contrastive-Learning-of-Propositional-Semantic-Representations"><a href="#Sub-Sentence-Encoder-Contrastive-Learning-of-Propositional-Semantic-Representations" class="headerlink" title="Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations"></a>Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04335">http://arxiv.org/abs/2311.04335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/schen149/sub-sentence-encoder">https://github.com/schen149/sub-sentence-encoder</a></li>
<li>paper_authors: Sihao Chen, Hongming Zhang, Tong Chen, Ben Zhou, Wenhao Yu, Dian Yu, Baolin Peng, Hongwei Wang, Dan Roth, Dong Yu</li>
<li>for: 这 paper 是为了提供一种基于 contrastive learning 的文本细词表示模型，以便在文本分类、检索和机器翻译等应用中进行细词级别的语义表示。</li>
<li>methods: 该 paper 使用了一种基于 contrastive learning 的文本细词表示模型，称为 sub-sentence encoder，该模型可以学习细词级别的语义表示，并且可以在不同文本序列中检测到相似的语义表达。</li>
<li>results: 该 paper 的实验结果表明，使用 sub-sentence encoder 可以在文本应用中提高细词级别的语义表示的准确率，同时保持与传统 sentence encoder 相同的推理成本和存储复杂度。<details>
<summary>Abstract</summary>
We introduce sub-sentence encoder, a contrastively-learned contextual embedding model for fine-grained semantic representation of text. In contrast to the standard practice with sentence embeddings, where the meaning of an entire sequence of text is encoded into a fixed-length vector, the sub-sentence encoder learns to produce distinct contextual embeddings corresponding to different atomic propositions, i.e. atomic units of meaning expressed within a text sequence. The sub-sentence embeddings are contrastively learned to recognize (inferred) semantic equivalence between propositions across different text sequences. Our experiments show the effectiveness of sub-sentence encoders in applications, such as retrieving supporting facts for fine-grained text attribution or recognizing the conditional semantic similarity between texts. In practice, we demonstrate that sub-sentence encoders keep the same level of inference cost and space complexity compared to sentence encoders.
</details>
<details>
<summary>摘要</summary>
我们介绍了下一个字句编码器，一种基于对比学习的上下文嵌入模型，用于细化文本 semantics的表示。与标准做法不同，即将整个文本序列中的意义编码成固定长度向量，而我们的字句编码器学习了生成不同原子提POSITIONS中的含义的上下文嵌入。这些字句嵌入被对比式学习，以认可（推理）在不同文本序列中的含义相似性。我们的实验表明，字句编码器在应用中具有效果，例如检索支持 факт dla fine-grained text attribution 或recognize conditional semantic similarity between texts。在实践中，我们发现字句编码器与句子编码器的推理成本和存储复杂度相同。
</details></li>
</ul>
<hr>
<h2 id="Educating-for-AI-Cybersecurity-Work-and-Research-Ethics-Systems-Thinking-and-Communication-Requirements"><a href="#Educating-for-AI-Cybersecurity-Work-and-Research-Ethics-Systems-Thinking-and-Communication-Requirements" class="headerlink" title="Educating for AI Cybersecurity Work and Research: Ethics, Systems Thinking, and Communication Requirements"></a>Educating for AI Cybersecurity Work and Research: Ethics, Systems Thinking, and Communication Requirements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04326">http://arxiv.org/abs/2311.04326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sorin Adam Matei, Elisa Bertino<br>for:* The paper explores managerial and instructor perceptions of freshly employed cybersecurity workers’ preparedness to work effectively in a changing cybersecurity environment that includes AI tools.methods:* The study uses a survey to collect data on managerial and instructor perceptions of technical preparedness and non-technical skill sets (ethical, systems thinking, and communication skills) among freshly employed cybersecurity workers.results:* The study found that managers and professors perceive preparedness to use AI tools in cybersecurity to be significantly associated with all three non-technical skill sets, with ethics being the most important. Additionally, professors over-estimate students’ preparedness for ethical, system thinking, and communication abilities compared to IT managers’ perceptions of their newly employed IT workers.<details>
<summary>Abstract</summary>
The present study explored managerial and instructor perceptions of their freshly employed cybersecurity workers' or students' preparedness to work effectively in a changing cybersecurity environment that includes AI tools. Specifically, we related perceptions of technical preparedness to ethical, systems thinking, and communication skills. We found that managers and professors perceive preparedness to use AI tools in cybersecurity to be significantly associated with all three non-technical skill sets. Most important, ethics is a clear leader in the network of relationships. Contrary to expectations that ethical concerns are left behind in the rush to adopt the most advanced AI tools in security, both higher education instructors and managers appreciate their role and see them closely associated with technical prowess. Another significant finding is that professors over-estimate students' preparedness for ethical, system thinking, and communication abilities compared to IT managers' perceptions of their newly employed IT workers.
</details>
<details>
<summary>摘要</summary>
现在的研究探讨了新 empleados或学生的cybersecurity工作效果preparedness，特别是在包括AI工具的变化的cybersecurity环境中。我们发现管理者和教授对使用AI工具在cybersecurity中的准备程度与伦理、系统思维和communication skills有显著相关性。其中，伦理准备是网络关系中最重要的一个因素。 contradicting expectations, both higher education instructors and managers recognize the importance of ethics in the adoption of advanced AI tools in security, and see it closely related to technical proficiency. Another significant finding is that professors overestimate students' preparedness for ethical, system thinking, and communication abilities compared to IT managers' perceptions of their newly employed IT workers.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Extending-Machine-Learning-Based-Early-Sepsis-Detection-to-Different-Demographics"><a href="#Extending-Machine-Learning-Based-Early-Sepsis-Detection-to-Different-Demographics" class="headerlink" title="Extending Machine Learning-Based Early Sepsis Detection to Different Demographics"></a>Extending Machine Learning-Based Early Sepsis Detection to Different Demographics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04325">http://arxiv.org/abs/2311.04325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surajsinh Parmar, Tao Shan, San Lee, Yonghwan Kim, Jang Yong Kim</li>
<li>for: 这项研究旨在探讨两种ensemble学习方法 LightGBM 和 XGBoost 在公共eICU-CRD数据集和私人韩国圣玛利亚医院数据集上的比较分析，以便更好地掌握医疗数据的不均衡问题和抑制综合症识别。</li>
<li>methods: 这项研究使用了 LightGBM 和 XGBoost 两种ensemble学习方法，以探讨它们在医疗数据中的应用。</li>
<li>results: 研究发现，LightGBM 在计算效率和扩展性方面表现有些微强，而 XGBoost 则在某些方面表现较差。这些结果预示了这两种方法在医疗数据中的应用潜力，并为扩展机器学习在医疗领域的应用提供了基础。<details>
<summary>Abstract</summary>
Sepsis requires urgent diagnosis, but research is predominantly focused on Western datasets. In this study, we perform a comparative analysis of two ensemble learning methods, LightGBM and XGBoost, using the public eICU-CRD dataset and a private South Korean St. Mary's Hospital's dataset. Our analysis reveals the effectiveness of these methods in addressing healthcare data imbalance and enhancing sepsis detection. Specifically, LightGBM shows a slight edge in computational efficiency and scalability. The study paves the way for the broader application of machine learning in critical care, thereby expanding the reach of predictive analytics in healthcare globally.
</details>
<details>
<summary>摘要</summary>
septicemia需要紧急诊断，但研究主要集中在西方数据集上。在这项研究中，我们进行了两种ensemble学习方法，LightGBM和XGBoost的比较分析，使用公共的eICU-CRD数据集和私人的韩国圣玛利亚医院数据集。我们的分析表明这些方法在医疗数据异质问题中能够有效地解决，提高了 septicemia的检测率。特别是LightGBM在计算效率和可扩展性方面表现了一定的优势。这项研究为医疗预测分析在全球扩展开创了道路。
</details></li>
</ul>
<hr>
<h2 id="A-comparative-analysis-between-Conformer-Transducer-Whisper-and-wav2vec2-for-improving-the-child-speech-recognition"><a href="#A-comparative-analysis-between-Conformer-Transducer-Whisper-and-wav2vec2-for-improving-the-child-speech-recognition" class="headerlink" title="A comparative analysis between Conformer-Transducer, Whisper, and wav2vec2 for improving the child speech recognition"></a>A comparative analysis between Conformer-Transducer, Whisper, and wav2vec2 for improving the child speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04936">http://arxiv.org/abs/2311.04936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/c3imaging/child_asr_conformer">https://github.com/c3imaging/child_asr_conformer</a></li>
<li>paper_authors: Andrei Barcovschi, Rishabh Jain, Peter Corcoran</li>
<li>for: 提高儿童语音识别性能</li>
<li>methods: 采用适应器-转换器模型进行自适应，并与其他两种模型进行比较</li>
<li>results: 结果显示，适应器-转换器模型在儿童语音识别中具有显著的改善，与其他两种模型相比，wav2vec2模型提供了最好的性能改善。<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) systems have progressed significantly in their performance on adult speech data; however, transcribing child speech remains challenging due to the acoustic differences in the characteristics of child and adult voices. This work aims to explore the potential of adapting state-of-the-art Conformer-transducer models to child speech to improve child speech recognition performance. Furthermore, the results are compared with those of self-supervised wav2vec2 models and semi-supervised multi-domain Whisper models that were previously finetuned on the same data. We demonstrate that finetuning Conformer-transducer models on child speech yields significant improvements in ASR performance on child speech, compared to the non-finetuned models. We also show Whisper and wav2vec2 adaptation on different child speech datasets. Our detailed comparative analysis shows that wav2vec2 provides the most consistent performance improvements among the three methods studied.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统在成人语音数据上的表现已经得到了显著提高，但是识别儿童语音仍然是一项挑战，这是因为儿童和成人语音的音频特征差异较大。本研究旨在探讨使用现有的Conformer-抽象器模型来改进儿童语音识别性能。此外，我们还对自动学习wav2vec2模型和多个频道Whisper模型进行了比较，这些模型都在同一个数据集上进行了finetuning。我们的详细比较分析表明，wav2vec2模型在不同的儿童语音数据集上提供了最稳定的性能改进。
</details></li>
</ul>
<hr>
<h2 id="Improved-Child-Text-to-Speech-Synthesis-through-Fastpitch-based-Transfer-Learning"><a href="#Improved-Child-Text-to-Speech-Synthesis-through-Fastpitch-based-Transfer-Learning" class="headerlink" title="Improved Child Text-to-Speech Synthesis through Fastpitch-based Transfer Learning"></a>Improved Child Text-to-Speech Synthesis through Fastpitch-based Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04313">http://arxiv.org/abs/2311.04313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Jain, Peter Corcoran</li>
<li>for: 这个论文旨在研究如何使用 Fastpitch 文本到语音（TTS）模型生成高质量的人工婴儿语音。</li>
<li>methods: 这种新的方法利用了传输学习训练管道，并对多个说话者 TTS 模型进行了微调。</li>
<li>results: 这项研究使用了公共可用的 MyST 数据集（55 小时）进行了微调实验，并释放了一个原型数据集和模型代码，以支持进一步的研究。对于实际和生成的婴儿语音之间的相似性，我们使用了一个预训练的 MOSNet，并进行了对jective评估。此外，我们还使用了自动语音识别（ASR）模型来比较实际和生成的婴儿语音中的单词错误率（WER）。<details>
<summary>Abstract</summary>
Speech synthesis technology has witnessed significant advancements in recent years, enabling the creation of natural and expressive synthetic speech. One area of particular interest is the generation of synthetic child speech, which presents unique challenges due to children's distinct vocal characteristics and developmental stages. This paper presents a novel approach that leverages the Fastpitch text-to-speech (TTS) model for generating high-quality synthetic child speech. This study uses the transfer learning training pipeline. The approach involved finetuning a multi-speaker TTS model to work with child speech. We use the cleaned version of the publicly available MyST dataset (55 hours) for our finetuning experiments. We also release a prototype dataset of synthetic speech samples generated from this research together with model code to support further research. By using a pretrained MOSNet, we conducted an objective assessment that showed a significant correlation between real and synthetic child voices. Additionally, to validate the intelligibility of the generated speech, we employed an automatic speech recognition (ASR) model to compare the word error rates (WER) of real and synthetic child voices. The speaker similarity between the real and generated speech is also measured using a pretrained speaker encoder.
</details>
<details>
<summary>摘要</summary>
《文本识别技术在最近几年内得到了显著的进步，使得生成自然和表情充沛的合成语音成为可能。一个特别有趣的领域是生成合成儿童语音，这种语音具有儿童特有的声音特征和发展阶段。本文提出了一种新的方法，利用Fastpitch文本识别（TTS）模型来生成高质量的合成儿童语音。本研究使用了传输学习训练管道。我们使用了公共可用的MyST数据集（55小时）进行训练实验。我们还发布了一个原型数据集，包含这些研究中生成的合成语音样本，以及模型代码，以支持进一步的研究。通过使用预训练的MOSNet，我们进行了对比实验，并显示了真实和合成儿童声音之间的显著相似性。此外，为验证生成的语音是否可以理解，我们使用了自动语音识别（ASR）模型来比较真实和合成儿童声音的单词错误率（WER）。生成的语音与真实儿童声音之间的 speaker 相似性也被使用预训练的 speaker 编码器进行评估。
</details></li>
</ul>
<hr>
<h2 id="Class-Incremental-Continual-Learning-for-General-Purpose-Healthcare-Models"><a href="#Class-Incremental-Continual-Learning-for-General-Purpose-Healthcare-Models" class="headerlink" title="Class-Incremental Continual Learning for General Purpose Healthcare Models"></a>Class-Incremental Continual Learning for General Purpose Healthcare Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04301">http://arxiv.org/abs/2311.04301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amritpal Singh, Mustafa Burak Gurbuz, Shiva Souhith Gantha, Prahlad Jasti</li>
<li>for: 这个研究旨在检验各种医疗影像案例中，使用不同的特殊性和医院，将模型 sequentially 学习新任务，而不会影响之前学习的任务的性能。</li>
<li>methods: 这个研究使用了不同的持续学习方法，包括 episodic memory、iCaRL 和 Rehearsal-based continual learning，并评估它们在不同的医疗影像案例中的表现。</li>
<li>results: 研究结果显示，使用不同的持续学习方法，单一的模型可以 sequentially 学习不同的特殊性和医院，并实现与预期值相似的性能。这表明了在不同的医疗影像案例中，可以共享或回收模型，从而推进医疗影像 AI 的发展，并且可以在不同的医院和特殊性中进行共享和实用化。<details>
<summary>Abstract</summary>
Healthcare clinics regularly encounter dynamic data that changes due to variations in patient populations, treatment policies, medical devices, and emerging disease patterns. Deep learning models can suffer from catastrophic forgetting when fine-tuned in such scenarios, causing poor performance on previously learned tasks. Continual learning allows learning on new tasks without performance drop on previous tasks. In this work, we investigate the performance of continual learning models on four different medical imaging scenarios involving ten classification datasets from diverse modalities, clinical specialties, and hospitals. We implement various continual learning approaches and evaluate their performance in these scenarios. Our results demonstrate that a single model can sequentially learn new tasks from different specialties and achieve comparable performance to naive methods. These findings indicate the feasibility of recycling or sharing models across the same or different medical specialties, offering another step towards the development of general-purpose medical imaging AI that can be shared across institutions.
</details>
<details>
<summary>摘要</summary>
医疗机构常常遇到动态数据，这些数据因为患者人口变化、治疗政策变化、医疗设备更新和新艺术疾病趋势而变化。深度学习模型在这些场景下可能会出现忘记灾难，导致之前学习的任务表现下降。连续学习可以让模型在新任务上学习而不会影响之前学习的任务表现。在这项工作中，我们研究了不同医疗领域的四个医学影像场景中，十个类别数据集的连续学习表现。我们实现了不同的连续学习方法，并评估其在这些场景中的表现。我们的结果表明，一个单一的模型可以在不同的专业领域中顺序学习新任务，并达到与预期方法相同的性能。这些发现表明，可以在同一或不同的医疗专业领域中重用或共享模型，这是普用医学影像AI的又一步发展。
</details></li>
</ul>
<hr>
<h2 id="CRAB-Assessing-the-Strength-of-Causal-Relationships-Between-Real-world-Events"><a href="#CRAB-Assessing-the-Strength-of-Causal-Relationships-Between-Real-world-Events" class="headerlink" title="CRAB: Assessing the Strength of Causal Relationships Between Real-world Events"></a>CRAB: Assessing the Strength of Causal Relationships Between Real-world Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04284">http://arxiv.org/abs/2311.04284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelika Romanou, Syrielle Montariol, Debjit Paul, Leo Laugier, Karl Aberer, Antoine Bosselut</li>
<li>for: 评估大语言模型在新闻叙述中的 causal 理解能力</li>
<li>methods: 使用 CRAB  benchmark，测试多种语言模型在 causal 关系之间的理解能力</li>
<li>results: 大多数语言模型在这个任务中表现不佳，尤其是当事件来源于复杂的 causal 结构时Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to assess the ability of large language models to understand causal relationships in real-world narratives.</li>
<li>methods: The authors use a new benchmark called CRAB to test the performance of several large language models in reasoning about causal relationships in narratives.</li>
<li>results: The authors find that most language models perform poorly on this task, and that models perform worse when events are derived from complex causal structures compared to simple linear causal chains.I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network of causal relationships of events in narratives. In this work, we present CRAB, a new Causal Reasoning Assessment Benchmark designed to evaluate causal understanding of events in real-world narratives. CRAB contains fine-grained, contextual causality annotations for ~2.7K pairs of real-world events that describe various newsworthy event timelines (e.g., the acquisition of Twitter by Elon Musk). Using CRAB, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task. Motivated by classical causal principles, we also analyze the causal structures of groups of events in CRAB, and find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains. We make our dataset and code available to the research community.
</details>
<details>
<summary>摘要</summary>
理解叙述需要对事件之间的 causa-effect 关系进行推理。现有的基础模型在许多 NLP 任务中表现出色，但是是否真的理解叙述中事件的复杂网络 causal 关系却存在uncertainty。在这项工作中，我们介绍了 CRAB，一个新的 causal reasoning assessment benchmark，用于评估事件叙述中的 causal 理解。CRAB 包含 ~2.7K 对实际新闻事件的细腻、上下文 causality 注释，例如 Elon Musk 收购 Twitter 等。使用 CRAB，我们测试了多种大语言模型的性能，发现大多数系统在这项任务中表现不佳。针对古典 causal 原则，我们还分析了 CRAB 中事件组合体系的 causal 结构，发现模型在复杂 causal 结构下的推理性能较差。我们将数据集和代码公开给研究社区。
</details></li>
</ul>
<hr>
<h2 id="OtterHD-A-High-Resolution-Multi-modality-Model"><a href="#OtterHD-A-High-Resolution-Multi-modality-Model" class="headerlink" title="OtterHD: A High-Resolution Multi-modality Model"></a>OtterHD: A High-Resolution Multi-modality Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04219">http://arxiv.org/abs/2311.04219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, Ziwei Liu</li>
<li>for: 这个论文旨在提出一种新型多模态模型OtterHD-8B，用于处理高分辨率视觉输入，并且具有更高的灵活性和准确性。</li>
<li>methods: 该模型基于Fuyu-8B architecture，通过采用可变长度视觉编码器和精细调整的权重学习策略，实现了对高分辨率视觉输入的高精度处理。</li>
<li>results: 对于MagnifierBench数据集，OtterHD-8B直接处理高分辨率输入时表现出了明显的优势，与当前领先模型相比，具有较高的准确率和灵活性。<details>
<summary>Abstract</summary>
In this paper, we present OtterHD-8B, an innovative multimodal model evolved from Fuyu-8B, specifically engineered to interpret high-resolution visual inputs with granular precision. Unlike conventional models that are constrained by fixed-size vision encoders, OtterHD-8B boasts the ability to handle flexible input dimensions, ensuring its versatility across various inference requirements. Alongside this model, we introduce MagnifierBench, an evaluation framework designed to scrutinize models' ability to discern minute details and spatial relationships of small objects. Our comparative analysis reveals that while current leading models falter on this benchmark, OtterHD-8B, particularly when directly processing high-resolution inputs, outperforms its counterparts by a substantial margin. The findings illuminate the structural variances in visual information processing among different models and the influence that the vision encoders' pre-training resolution disparities have on model effectiveness within such benchmarks. Our study highlights the critical role of flexibility and high-resolution input capabilities in large multimodal models and also exemplifies the potential inherent in the Fuyu architecture's simplicity for handling complex visual data.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了OtterHD-8B，一种创新的多模式模型，从Fuyu-8B中演化而来，专门用于解释高分辨率视觉输入的细节精度。与传统的模型不同，OtterHD-8B具有可变输入维度的能力，因此在不同的推理需求下表现非常灵活。此外，我们还提出了MagnifierBench评价框架，用于评估模型对小 objet的细节和空间关系的解释能力。我们的比较分析表明，当直接处理高分辨率输入时，OtterHD-8B在同类模型中表现出了明显的优势。这些发现探讨了不同模型在视觉信息处理中的结构差异以及视觉编码器的预训练分辨率差异对模型效果的影响。我们的研究强调了大型多模式模型的灵活性和高分辨率输入能力的重要性，同时也illustrates Fuyu架构的简单性在处理复杂视觉数据方面的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Towards-Garment-Sewing-Pattern-Reconstruction-from-a-Single-Image"><a href="#Towards-Garment-Sewing-Pattern-Reconstruction-from-a-Single-Image" class="headerlink" title="Towards Garment Sewing Pattern Reconstruction from a Single Image"></a>Towards Garment Sewing Pattern Reconstruction from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04218">http://arxiv.org/abs/2311.04218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lijuan Liu, Xiangyu Xu, Zhijie Lin, Jiabin Liang, Shuicheng Yan</li>
<li>for: 这个研究旨在利用日常照片中恢复裤子缝制图，以扩展服装设计、虚拟试穿和数字人物等应用。</li>
<li>methods: 研究者首先合成了一个具有约1M张图像和真实缝制图的数据集，名为SewFactory，以供模型训练和评估。然后，他们提出了一个两级变换器网络，名为Sewformer，可以显著提高缝制图预测性能。</li>
<li>results: EXTENSIVE EXPERIMENTS表明，提案的框架可以有效地恢复缝制图，并在 casually-taken human photos 上广泛适用。<details>
<summary>Abstract</summary>
Garment sewing pattern represents the intrinsic rest shape of a garment, and is the core for many applications like fashion design, virtual try-on, and digital avatars. In this work, we explore the challenging problem of recovering garment sewing patterns from daily photos for augmenting these applications. To solve the problem, we first synthesize a versatile dataset, named SewFactory, which consists of around 1M images and ground-truth sewing patterns for model training and quantitative evaluation. SewFactory covers a wide range of human poses, body shapes, and sewing patterns, and possesses realistic appearances thanks to the proposed human texture synthesis network. Then, we propose a two-level Transformer network called Sewformer, which significantly improves the sewing pattern prediction performance. Extensive experiments demonstrate that the proposed framework is effective in recovering sewing patterns and well generalizes to casually-taken human photos. Code, dataset, and pre-trained models are available at: https://sewformer.github.io.
</details>
<details>
<summary>摘要</summary>
仪服缝纹模板表示服装的内在形状，是许多应用程序，如时尚设计、虚拟试穿和数字化人物的核心。在这项工作中，我们研究了从日常照片中恢复仪服缝纹模板的问题。为解决这个问题，我们首先生成了一个通用的数据集，名为SewFactory，该数据集包含约1M张图片和对应的缝纹模板，用于模型训练和评估。SewFactory覆盖了人体姿势、身体形态和缝纹模板的广泛范围，并具有真实的外观特征， thanks to our proposed human texture synthesis network。然后，我们提出了一种两级转换器网络，名为Sewformer，该网络能够显著提高缝纹模板预测性能。广泛的实验证明了我们提出的框架可以高效地恢复缝纹模板，并在习惯性图像中广泛应用。代码、数据集和预训练模型可以在：https://sewformer.github.io/获取。
</details></li>
</ul>
<hr>
<h2 id="Wearable-data-from-subjects-playing-Super-Mario-sitting-university-exams-or-performing-physical-exercise-help-detect-acute-mood-episodes-via-self-supervised-learning"><a href="#Wearable-data-from-subjects-playing-Super-Mario-sitting-university-exams-or-performing-physical-exercise-help-detect-acute-mood-episodes-via-self-supervised-learning" class="headerlink" title="Wearable data from subjects playing Super Mario, sitting university exams, or performing physical exercise help detect acute mood episodes via self-supervised learning"></a>Wearable data from subjects playing Super Mario, sitting university exams, or performing physical exercise help detect acute mood episodes via self-supervised learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04215">http://arxiv.org/abs/2311.04215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filippo Corponi, Bryan M. Li, Gerard Anmella, Clàudia Valenzuela-Pascual, Ariadna Mas, Isabella Pacchiarotti, Marc Valentí, Iria Grande, Antonio Benabarre, Marina Garriga, Eduard Vieta, Allan H Young, Stephen M. Lawrie, Heather C. Whalley, Diego Hidalgo-Mazzei, Antonio Vergari</li>
<li>for: 这个研究用于监测情绪障碍（MDs），利用佩戴式设备收集的数据，以提高现代监测技术的应用。</li>
<li>methods: 本研究使用了自动学习（SSL）技术，利用无标签数据来学习表示，然后用于支持任务。</li>
<li>results: 研究显示，SSL可以有效地检测MDs的急性症状和稳定状态，并且可以与权威的XGBoost和Transformer架构进行比较。<details>
<summary>Abstract</summary>
Personal sensing, leveraging data passively and near-continuously collected with wearables from patients in their ecological environment, is a promising paradigm to monitor mood disorders (MDs), a major determinant of worldwide disease burden. However, collecting and annotating wearable data is very resource-intensive. Studies of this kind can thus typically afford to recruit only a couple dozens of patients. This constitutes one of the major obstacles to applying modern supervised machine learning techniques to MDs detection. In this paper, we overcome this data bottleneck and advance the detection of MDs acute episode vs stable state from wearables data on the back of recent advances in self-supervised learning (SSL). This leverages unlabelled data to learn representations during pre-training, subsequently exploited for a supervised task. First, we collected open-access datasets recording with an Empatica E4 spanning different, unrelated to MD monitoring, personal sensing tasks -- from emotion recognition in Super Mario players to stress detection in undergraduates -- and devised a pre-processing pipeline performing on-/off-body detection, sleep-wake detection, segmentation, and (optionally) feature extraction. With 161 E4-recorded subjects, we introduce E4SelfLearning, the largest to date open access collection, and its pre-processing pipeline. Second, we show that SSL confidently outperforms fully-supervised pipelines using either our novel E4-tailored Transformer architecture (E4mer) or classical baseline XGBoost: 81.23% against 75.35% (E4mer) and 72.02% (XGBoost) correctly classified recording segments from 64 (half acute, half stable) patients. Lastly, we illustrate that SSL performance is strongly associated with the specific surrogate task employed for pre-training as well as with unlabelled data availability.
</details>
<details>
<summary>摘要</summary>
个人感知，通过PASSIVEALLY和持续采集来自病人的佩戴设备数据，是监测情绪障碍（MD）的有前途的方法。然而，收集和标注佩戴设备数据具有很大的资源投入。这类研究通常只能招募几十名病人。这是监测MD的主要障碍。在这篇论文中，我们解决了这种数据瓶颈，并在使用最新的自我超vision学习（SSL）技术时提高了监测MD急性病情vs稳定状态的能力。通过在预训练阶段使用无标签数据来学习表示，然后在supervised任务上进行应用。我们收集了来自Empatica E4设备的开放访问数据，包括不同的、与MD监测无关的个人感知任务，如Super Mario游戏中的情绪识别和大学生中的压力检测。我们设计了一个预处理管道，包括在/离体检测、睡卫睡检测、分割和（可选）特征提取。通过161名E4记录的主题，我们引入了E4SelfLearning，是目前最大的开放访问收集。我们还证明了，使用我们的专门设计的E4mer变体的Transformer架构，SSL可以强制超过完全监测的架构，包括XGBoost。在64名病人中，我们获得了81.23%的正确记录分割，比75.35%和72.02%的完全监测和XGBoost架构更高。最后，我们发现了ssl性能与特定的代表任务的预训练和无标签数据可用性之间存在强相关性。
</details></li>
</ul>
<hr>
<h2 id="Rephrase-and-Respond-Let-Large-Language-Models-Ask-Better-Questions-for-Themselves"><a href="#Rephrase-and-Respond-Let-Large-Language-Models-Ask-Better-Questions-for-Themselves" class="headerlink" title="Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves"></a>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04205">http://arxiv.org/abs/2311.04205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uclaml/Rephrase-and-Respond">https://github.com/uclaml/Rephrase-and-Respond</a></li>
<li>paper_authors: Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu</li>
<li>for: 这个论文的目的是提出一种名为“重句并回答”（Rephrase and Respond，简称RaR）的方法，以便人工智能语言模型（LLM）更好地理解人类提出的问题，并在单个提问中提供回答。</li>
<li>methods: 这种方法使用一个拥有重句能力的LLM来重句人类提出的问题，然后将原始问题和重句后的问题一起传递给另一个回答LLM。这种方法可以让LLM更好地理解问题，并提高它们的表现。</li>
<li>results: 实验表明，使用RaR方法可以提高不同任务的模型表现，并且与现有的链条思维（Chain-of-Thought，CoT）方法相比，RaR方法是一种更有效的提问方法。此外，RaR方法可以与CoT方法相结合，以达到更高的表现。<details>
<summary>Abstract</summary>
Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models (LLMs). Such discrepancies can make LLMs interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped. In this paper, we present a method named `Rephrase and Respond' (RaR), which allows LLMs to rephrase and expand questions posed by humans and provide responses in a single prompt. This approach serves as a simple yet effective prompting method for improving performance. We also introduce a two-step variant of RaR, where a rephrasing LLM first rephrases the question and then passes the original and rephrased questions together to a different responding LLM. This facilitates the effective utilization of rephrased questions generated by one LLM with another. Our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. We further provide a comprehensive comparison between RaR and the popular Chain-of-Thought (CoT) methods, both theoretically and empirically. We show that RaR is complementary to CoT and can be combined with CoT to achieve even better performance. Our work not only contributes to enhancing LLM performance efficiently and effectively but also sheds light on a fair evaluation of LLM capabilities. Data and codes are available at https://github.com/uclaml/Rephrase-and-Respond.
</details>
<details>
<summary>摘要</summary>
人类和大语言模型（LLM）之间的误解不仅出现在人际交流中，还出现在LLM与人类之间的交流中。这些不一致性可能使LLM对明确的问题进行不预期的解释，导致错误的回答。虽然广泛认可的提问质量对LLM提供回答的质量产生 significan influence，但是一种系统化的提问方法，例如`Rephrase and Respond`（RaR），仍然处于开发阶段。在这篇论文中，我们提出了一种方法，即RaR，允许LLM对人类提出的问题进行重新表达和扩展，并提供回答在单个提问中。这种方法可以提高LLM的性能，并且我们还提出了一种两步变体的RaR方法，其中一个重新表达LLM首先重新表达问题，然后将原始和重新表达的问题一起传递给另一个回答LLM。这种方法可以有效地利用重新表达的问题，并且我们的实验表明，我们的方法可以在各种任务上提高不同的模型性能。此外，我们还对RaR和流行的链式思维（CoT）方法进行了比较，并证明了RaR和CoT是 complementary的，可以在一起使用以达到更好的性能。我们的工作不仅可以有效地提高LLM性能，还可以为LLM评估提供新的思路。数据和代码可以在https://github.com/uclaml/Rephrase-and-Respond中获取。
</details></li>
</ul>
<hr>
<h2 id="JPAVE-A-Generation-and-Classification-based-Model-for-Joint-Product-Attribute-Prediction-and-Value-Extraction"><a href="#JPAVE-A-Generation-and-Classification-based-Model-for-Joint-Product-Attribute-Prediction-and-Value-Extraction" class="headerlink" title="JPAVE: A Generation and Classification-based Model for Joint Product Attribute Prediction and Value Extraction"></a>JPAVE: A Generation and Classification-based Model for Joint Product Attribute Prediction and Value Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04196">http://arxiv.org/abs/2311.04196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhongfendeng/jpave">https://github.com/zhongfendeng/jpave</a></li>
<li>paper_authors: Zhongfen Deng, Hao Peng, Tao Zhang, Shuaiqi Liu, Wenting Zhao, Yibo Wang, Philip S. Yu</li>
<li>for: 这篇论文主要针对产品特征值EXTRACTION问题进行研究，帮助了下游应用程序如产品搜索和推荐。</li>
<li>methods: 该论文提出了一种多任务学习模型，包括值生成&#x2F;分类和特征预测，以预测值无需考虑文本中值的顺序信息。此外，模型还包括值生成器的复制机制和值分类器的值注意模块，以解决数据不一致问题。</li>
<li>results: 实验结果表明，该模型比强基eline模型更有优势，并且具有更好的扩展性和适用性。<details>
<summary>Abstract</summary>
Product attribute value extraction is an important task in e-Commerce which can help several downstream applications such as product search and recommendation. Most previous models handle this task using sequence labeling or question answering method which rely on the sequential position information of values in the product text and are vulnerable to data discrepancy between training and testing. This limits their generalization ability to real-world scenario in which each product can have multiple descriptions across various shopping platforms with different composition of text and style. They also have limited zero-shot ability to new values. In this paper, we propose a multi-task learning model with value generation/classification and attribute prediction called JPAVE to predict values without the necessity of position information of values in the text. Furthermore, the copy mechanism in value generator and the value attention module in value classifier help our model address the data discrepancy issue by only focusing on the relevant part of input text and ignoring other information which causes the discrepancy issue such as sentence structure in the text. Besides, two variants of our model are designed for open-world and closed-world scenarios. In addition, copy mechanism introduced in the first variant based on value generation can improve its zero-shot ability for identifying unseen values. Experimental results on a public dataset demonstrate the superiority of our model compared with strong baselines and its generalization ability of predicting new values.
</details>
<details>
<summary>摘要</summary>
产品特征值EXTRACTION是电商中非常重要的任务，可以帮助多种下游应用程序，如产品搜索和推荐。之前的大多数模型都使用序列标注或问答方法来处理这个任务，这些方法需要文本中值的顺序信息，但是这些方法容易受到训练和测试数据的不一致问题的影响， limiting their generalization ability to real-world scenarios in which each product can have multiple descriptions across various shopping platforms with different text composition and style. They also have limited zero-shot ability to new values.在这篇论文中，我们提出了一种多任务学习模型，即JPAVE，可以预测值无需文本中值的顺序信息。此外，模型中的复制机制和价值注意模块帮助我们解决数据不一致问题，只关注输入文本中相关的部分，忽略其他信息。此外，我们还设计了两种模型的变体，一种适用于开放世界场景，另一种适用于关闭世界场景。此外，模型中的复制机制可以提高其零shot能力，用于识别未看过的值。实验结果表明，我们的模型在公共数据集上比强基eline表现出色，并且具有普遍性和适用性。
</details></li>
</ul>
<hr>
<h2 id="Selective-Visual-Representations-Improve-Convergence-and-Generalization-for-Embodied-AI"><a href="#Selective-Visual-Representations-Improve-Convergence-and-Generalization-for-Embodied-AI" class="headerlink" title="Selective Visual Representations Improve Convergence and Generalization for Embodied AI"></a>Selective Visual Representations Improve Convergence and Generalization for Embodied AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04193">http://arxiv.org/abs/2311.04193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ainaz Eftekhar, Kuo-Hao Zeng, Jiafei Duan, Ali Farhadi, Ani Kembhavi, Ranjay Krishna<br>for:* 这个论文是为了提高embodied AI模型中的视觉处理的精度和效率，使其更好地处理任务相关的视觉信息。methods:* 这个论文使用了一种Parameter-efficient Approach，即使用小型学习编码表，来实现视觉信息的筛选和压缩。results:* 这个论文在5个benchmark上实现了state-of-the-art的性能，包括ProcTHOR、ArchitecTHOR、RoboTHOR、AI2-iTHOR和ManipulaTHOR。In Simplified Chinese text, the three key points would be:for:* 这个论文是为了提高embodied AI模型中的视觉处理的精度和效率，使其更好地处理任务相关的视觉信息。methods:* 这个论文使用了一种Parameter-efficient Approach，即使用小型学习编码表，来实现视觉信息的筛选和压缩。results:* 这个论文在5个benchmark上实现了state-of-the-art的性能，包括ProcTHOR、ArchitecTHOR、RoboTHOR、AI2-iTHOR和ManipulaTHOR。<details>
<summary>Abstract</summary>
Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues. Inspired by selective attention in humans-the process through which people filter their perception based on their experiences, knowledge, and the task at hand-we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects. Code and pretrained models are available at our project website: https://embodied-codebook.github.io.
</details>
<details>
<summary>摘要</summary>
embodied AI模型经常使用 commercially available视觉脊梁 like CLIP来编码其视觉观察。 although these general-purpose representations encode rich syntax and semantics about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise into the learning process and distracts the agent's focus from task-relevant visual cues. inspired by selective attention in humans-the process through which people filter their perception based on their experiences, knowledge, and the task at hand-we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. our experiments showcase state-of-the-art performance for object goal navigation and object displacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. the filtered representations produced by the codebook are also able to generalize better and converge faster when adapted to other simulation environments such as Habitat. our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects. code and pretrained models are available at our project website: <https://embodied-codebook.github.io>.
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Anomaly-Detection-with-Graph-Networks-for-Data-Quality-Monitoring-of-the-Hadron-Calorimeter"><a href="#Spatio-Temporal-Anomaly-Detection-with-Graph-Networks-for-Data-Quality-Monitoring-of-the-Hadron-Calorimeter" class="headerlink" title="Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality Monitoring of the Hadron Calorimeter"></a>Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality Monitoring of the Hadron Calorimeter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04190">http://arxiv.org/abs/2311.04190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mulugeta Weldezgina Asres, Christian Walter Omlin, Long Wang, David Yu, Pavel Parygin, Jay Dittmann, Georgia Karapostoli, Markus Seidel, Rosamaria Venditti, Luka Lambrecht, Emanuele Usai, Muhammad Ahmad, Javier Fernandez Menendez, Kaori Maeshima, the CMS-HCAL Collaboration</li>
<li>for: 这个研究是为了提供一种 semi-supervised spatio-temporal anomaly detection (AD) 监测系统，用于检测高能物理实验室（LHC）中的粒子数据获取问题。</li>
<li>methods: 该研究使用了三维幂occupation图数据，并使用了卷积神经网络、图神经网络和回归神经网络来学习本地空间特征和全球行为。</li>
<li>results: 该研究表明，提出的 AD 监测系统可以准确地检测多种通道故障类型，并且在生产环境中实现了高级别的准确率。<details>
<summary>Abstract</summary>
The compact muon solenoid (CMS) experiment is a general-purpose detector for high-energy collision at the large hadron collider (LHC) at CERN. It employs an online data quality monitoring (DQM) system to promptly spot and diagnose particle data acquisition problems to avoid data quality loss. In this study, we present semi-supervised spatio-temporal anomaly detection (AD) monitoring for the physics particle reading channels of the hadronic calorimeter (HCAL) of the CMS using three-dimensional digi-occupancy map data of the DQM. We propose the GraphSTAD system, which employs convolutional and graph neural networks to learn local spatial characteristics induced by particles traversing the detector, and global behavior owing to shared backend circuit connections and housing boxes of the channels, respectively. Recurrent neural networks capture the temporal evolution of the extracted spatial features. We have validated the accuracy of the proposed AD system in capturing diverse channel fault types using the LHC Run-2 collision data sets. The GraphSTAD system has achieved production-level accuracy and is being integrated into the CMS core production system--for real-time monitoring of the HCAL. We have also provided a quantitative performance comparison with alternative benchmark models to demonstrate the promising leverage of the presented system.
</details>
<details>
<summary>摘要</summary>
“聚焦μ子探测器（CMS）实验是高能物理实验室（LHC）中的通用探测器，位于瑞士日内瓦核子研究所（CERN）。它使用在线数据质量监控（DQM）系统来及时检测和诊断粒子数据收集问题，以避免数据质量损失。在本研究中，我们提出了基于三维干扰量图像数据的 semi-supervised 空间时间异常检测（AD）监控方案，用于 физи学粒子读取通道（HCAL）的CMS。我们提出的GraphSTAD系统使用卷积神经网络学习粒子通过探测器的本地空间特征，以及全球性的后端电路连接和底板盒子特征。循环神经网络捕捉粒子 traverse 探测器的时间演化。我们在LHCRun-2碰撞数据集上验证了提议的AD系统的准确性，可以捕捉多种通道故障类型。GraphSTAD系统已经达到生产级准确性，并在CMS核心生产系统中进行实时监控HCAL。我们还提供了alternative benchmark模型的量化性能比较，以示出提出的系统的承诺优势。”
</details></li>
</ul>
<hr>
<h2 id="Prompt-Cache-Modular-Attention-Reuse-for-Low-Latency-Inference"><a href="#Prompt-Cache-Modular-Attention-Reuse-for-Low-Latency-Inference" class="headerlink" title="Prompt Cache: Modular Attention Reuse for Low-Latency Inference"></a>Prompt Cache: Modular Attention Reuse for Low-Latency Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04934">http://arxiv.org/abs/2311.04934</a></li>
<li>repo_url: None</li>
<li>paper_authors: In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, Lin Zhong</li>
<li>for: 加速大语言模型（LLM）的推理进程，以便更快地响应用户提交的问题。</li>
<li>methods: 利用预计算和存储推理过程中出现的文本段的注意力状态，以便在用户提交中重用这些状态，从而提高推理速度。</li>
<li>results: 对多种大语言模型进行评估，显示Prompt Cache可以减少时间到首个字符的延迟，特别是对长提问的回答和推荐问题。对GPU基于的推理进程，提高8倍；对CPU基于的推理进程，提高60倍，而无需修改模型参数。<details>
<summary>Abstract</summary>
We present Prompt Cache, an approach for accelerating inference for large language models (LLM) by reusing attention states across different LLM prompts. Many input prompts have overlapping text segments, such as system messages, prompt templates, and documents provided for context. Our key insight is that by precomputing and storing the attention states of these frequently occurring text segments on the inference server, we can efficiently reuse them when these segments appear in user prompts. Prompt Cache employs a schema to explicitly define such reusable text segments, called prompt modules. The schema ensures positional accuracy during attention state reuse and provides users with an interface to access cached states in their prompt. Using a prototype implementation, we evaluate Prompt Cache across several LLMs. We show that Prompt Cache significantly reduce latency in time-to-first-token, especially for longer prompts such as document-based question answering and recommendations. The improvements range from 8x for GPU-based inference to 60x for CPU-based inference, all while maintaining output accuracy and without the need for model parameter modifications.
</details>
<details>
<summary>摘要</summary>
我们提出了几个方法可以加速大型语言模型（LLM）的推理过程，包括重复使用参考状态。许多输入提示中有许多重复的文本段落，例如系统讯息、提示模板和提供了背景文本。我们的关键见解是，可以在推理服务器上预先计算和储存这些频繁出现的文本段落的参考状态，以便在用户提示中重复使用它们。我们称之为“提示库”（Prompt Cache）。这个架构使用一个schema来明确定义可重复使用的文本段落，称为“提示模组”（Prompt Module）。这个schema确保了参考状态重复使用时的位置精度，并提供了用户可以在提示中访问储存的状态的界面。使用一个原型实现，我们评估了Prompt Cache在多个LLM上。我们发现，Prompt Cache可以对推理过程中的时间-第一个字元（time-to-first-token）进行明显增加，特别是在更长的提示中，例如文档基于的问题回答和推荐。改进范围从GPU基础的推理过程中8倍增加到CPU基础的推理过程中60倍，而且不需要模型参数的修改，同时保持输出精度。
</details></li>
</ul>
<hr>
<h2 id="On-Leakage-in-Machine-Learning-Pipelines"><a href="#On-Leakage-in-Machine-Learning-Pipelines" class="headerlink" title="On Leakage in Machine Learning Pipelines"></a>On Leakage in Machine Learning Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04179">http://arxiv.org/abs/2311.04179</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Leonard Sasse, Eliana Nicolaisen-Sobesky, Juergen Dukart, Simon B. Eickhoff, Michael Götz, Sami Hamdan, Vera Komeyer, Abhijit Kulkarni, Juha Lahnakoski, Bradley C. Love, Federico Raimondo, Kaustubh R. Patil</li>
<li>for: 本研究旨在拓展遗漏导致ML管道性能估计过optimistic的原因，以及如何在设计、实现和评估ML管道时避免遗漏。</li>
<li>methods: 本文使用具体的实例来描述ML管道中可能出现的多种遗漏类型，并进行了全面的介绍和讨论。</li>
<li>results: 本研究的结果表明，遗漏可能导致ML管道的性能估计过optimistic，并且可能导致新数据上的失败generalization。<details>
<summary>Abstract</summary>
Machine learning (ML) provides powerful tools for predictive modeling. ML's popularity stems from the promise of sample-level prediction with applications across a variety of fields from physics and marketing to healthcare. However, if not properly implemented and evaluated, ML pipelines may contain leakage typically resulting in overoptimistic performance estimates and failure to generalize to new data. This can have severe negative financial and societal implications. Our aim is to expand understanding associated with causes leading to leakage when designing, implementing, and evaluating ML pipelines. Illustrated by concrete examples, we provide a comprehensive overview and discussion of various types of leakage that may arise in ML pipelines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-LLM-Intelligence-with-ARM-RAG-Auxiliary-Rationale-Memory-for-Retrieval-Augmented-Generation"><a href="#Enhancing-LLM-Intelligence-with-ARM-RAG-Auxiliary-Rationale-Memory-for-Retrieval-Augmented-Generation" class="headerlink" title="Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation"></a>Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04177">http://arxiv.org/abs/2311.04177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Melz</li>
<li>for: 这篇论文是关于如何提高语言模型的智能水平的研究。</li>
<li>methods: 该论文使用了 Retrieval Augmented Generation（RAG）技术，并提出了一种名为 Auxiliary Rationale Memory for Retrieval Augmented Generation（ARM-RAG）的新系统。</li>
<li>results: 该研究发现，通过存储和后续检索解释链可以提高解决grade-school math问题的性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g., (Bubeck et al., 2023)) on modern LLMs have shown that they are capable of performing amazing tasks typically necessitating human-level intelligence. However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures. Some approaches to improving the intelligence of LLMs include fine-tuning models based on problem-solving performance (Zelikman et al., 2022), and building bigger and more sophisticated models (Bubeck et al., 2023). However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models. In this paper, we explore the use of Retrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to improve problem-solving performance. We propose ARM-RAG (Auxiliary Rationale Memory for Retrieval Augmented Generation), a system that learns from its successes without incurring high training costs. We demonstrate that the storage and subsequent retrieval of reasoning chains have a positive influence on performance in grade-school math problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HADES-Fast-Singularity-Detection-with-Local-Measure-Comparison"><a href="#HADES-Fast-Singularity-Detection-with-Local-Measure-Comparison" class="headerlink" title="HADES: Fast Singularity Detection with Local Measure Comparison"></a>HADES: Fast Singularity Detection with Local Measure Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04171">http://arxiv.org/abs/2311.04171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uzu Lim, Harald Oberhauser, Vidit Nanda</li>
<li>for: 检测数据中的独特点（singularities）</li>
<li>methods: 使用核心善良测试和 diferencial geometry 等工具</li>
<li>results: 在数据样本生成的跨维度抽象上检测独特点，并在实验中回归真实存在的独特点<details>
<summary>Abstract</summary>
We introduce Hades, an unsupervised algorithm to detect singularities in data. This algorithm employs a kernel goodness-of-fit test, and as a consequence it is much faster and far more scaleable than the existing topology-based alternatives. Using tools from differential geometry and optimal transport theory, we prove that Hades correctly detects singularities with high probability when the data sample lives on a transverse intersection of equidimensional manifolds. In computational experiments, Hades recovers singularities in synthetically generated data, branching points in road network data, intersection rings in molecular conformation space, and anomalies in image data.
</details>
<details>
<summary>摘要</summary>
我团队介绍了冥王（Hades）算法，用于不监督地检测数据中的缺陷。这个算法使用kernel好度验证测试，因此它比现有的topology基于的方法更快速和可扩展。使用差分几何和最优运输理论，我们证明了冥王在数据样本生活在等维度抽象 manifold上的极限交叉点上correctly检测缺陷，并且在计算实验中能够回归数据中的分支点、路网数据中的分支点、分子 conformity 空间中的交叉环和图像数据中的异常。Note: "冥王" (Hades) is the name of the algorithm, and "数据中的缺陷" (singularities in data) is the object being detected.
</details></li>
</ul>
<hr>
<h2 id="Outliers-with-Opposing-Signals-Have-an-Outsized-Effect-on-Neural-Network-Optimization"><a href="#Outliers-with-Opposing-Signals-Have-an-Outsized-Effect-on-Neural-Network-Optimization" class="headerlink" title="Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization"></a>Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04163">http://arxiv.org/abs/2311.04163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elan Rosenfeld, Andrej Risteski</li>
<li>For: This paper is written to study the interaction between depth and heavy-tailed structures in neural network optimization, and to provide intuitive explanations for several previously reported observations about network training dynamics.* Methods: The paper uses experimental and theoretical approaches to study the phenomenon of opposing signals in training data, and to explore the effects of these signals on the optimization and behavior of the network. The authors also compare the performance of two popular optimization algorithms, Adam and SGD.* Results: The paper reports several key findings, including the existence of paired groups of outliers in the training data that can significantly influence the optimization process, the concept of “grokking” and its connection to the edge of stability, and the importance of carefully balancing opposing signals during training. The authors also provide a mechanistic explanation of the phenomenon on a toy example and a theoretical analysis of a two-layer linear network on a simple model. Experimental results confirm the predictions made by the authors.<details>
<summary>Abstract</summary>
We identify a new phenomenon in neural network optimization which arises from the interaction of depth and a particular heavy-tailed structure in natural data. Our result offers intuitive explanations for several previously reported observations about network training dynamics. In particular, it implies a conceptually new cause for progressive sharpening and the edge of stability; we also highlight connections to other concepts in optimization and generalization including grokking, simplicity bias, and Sharpness-Aware Minimization.   Experimentally, we demonstrate the significant influence of paired groups of outliers in the training data with strong opposing signals: consistent, large magnitude features which dominate the network output throughout training and provide gradients which point in opposite directions. Due to these outliers, early optimization enters a narrow valley which carefully balances the opposing groups; subsequent sharpening causes their loss to rise rapidly, oscillating between high on one group and then the other, until the overall loss spikes. We describe how to identify these groups, explore what sets them apart, and carefully study their effect on the network's optimization and behavior. We complement these experiments with a mechanistic explanation on a toy example of opposing signals and a theoretical analysis of a two-layer linear network on a simple model. Our finding enables new qualitative predictions of training behavior which we confirm experimentally. It also provides a new lens through which to study and improve modern training practices for stochastic optimization, which we highlight via a case study of Adam versus SGD.
</details>
<details>
<summary>摘要</summary>
我们发现了一种新的现象在神经网络优化中，它来自数据中自然存在的特殊重 tailed 结构和深度之间的交互作用。我们的结果提供了直观的解释，解释了许多先前报道的网络训练动态现象。特别是，它 imply 一种新的原因导致进步性和稳定边缘现象;我们还高亮了与其他优化和泛化概念相关的 grokking、简洁偏好和 Sharpness-Aware Minimization。实验证明，在训练数据中存在对抗信号的对应对，这些对应对包括强大的大量特征，这些特征在训练过程中控制网络输出，并且为网络提供反对向量。由于这些对应对，早期优化在训练过程中进入一个窄的谷地，精心平衡这些对应对的抗向量;随后的折衣使得对应对的损失快速增加，振荡在一个对应对的高峰和另一个对应对的高峰之间，直到总损失快速增加。我们描述了如何识别这些对应对，探索它们之间的差异，并且仔细研究它们对网络优化和行为的影响。我们补充了这些实验的机械化解释，并对一个简单的对应对示例进行了理论分析。我们的发现可以提供新的质量预测，我们通过实验确认了它们。它还提供了一个新的视角，用于研究和改进现代训练实践。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Interpretable-Transformer-for-Fine-Grained-Image-Classification-and-Analysis"><a href="#A-Simple-Interpretable-Transformer-for-Fine-Grained-Image-Classification-and-Analysis" class="headerlink" title="A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis"></a>A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04157">http://arxiv.org/abs/2311.04157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imageomics/intr">https://github.com/imageomics/intr</a></li>
<li>paper_authors: Dipanjyoti Paul, Arpita Chowdhury, Xinqi Xiong, Feng-Ju Chang, David Carlyn, Samuel Stevens, Kaiya Provost, Anuj Karpatne, Bryan Carstens, Daniel Rubenstein, Charles Stewart, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao</li>
<li>for: 提高图像分类的可读性，通过增强每个类别在图像中的搜索和特征LOCAL化。</li>
<li>methods: 基于Transformer编码器解码器的INTR方法，学习每个类别的特征查询，通过对比跨核查重来实现图像分类的可读性。</li>
<li>results: 在八个频道上进行了多种细腻的图像分类和分析，并且发现INTR方法能够增强图像分类的可读性和精度。<details>
<summary>Abstract</summary>
We present a novel usage of Transformers to make image classification interpretable. Unlike mainstream classifiers that wait until the last fully-connected layer to incorporate class information to make predictions, we investigate a proactive approach, asking each class to search for itself in an image. We realize this idea via a Transformer encoder-decoder inspired by DEtection TRansformer (DETR). We learn ``class-specific'' queries (one for each class) as input to the decoder, enabling each class to localize its patterns in an image via cross-attention. We name our approach INterpretable TRansformer (INTR), which is fairly easy to implement and exhibits several compelling properties. We show that INTR intrinsically encourages each class to attend distinctively; the cross-attention weights thus provide a faithful interpretation of the prediction. Interestingly, via ``multi-head'' cross-attention, INTR could identify different ``attributes'' of a class, making it particularly suitable for fine-grained classification and analysis, which we demonstrate on eight datasets. Our code and pre-trained model are publicly accessible at https://github.com/Imageomics/INTR.
</details>
<details>
<summary>摘要</summary>
我们提出了一种使用 transformer 进行图像分类的新方法，以使得分类结果更加可解。不同于主流的分类器，我们在最后一层完全连接层之前就将类信息integrated进行预测，而我们的方法是主动的，每个类都会在图像中搜寻自己的特征。我们通过基于DEtection TRansformer（DETR）的 transformer 编码器-解码器来实现这个想法，学习每个类的特定的查询（每个类一个），使得每个类能够通过对比关注图像中的特征进行地方化搜寻。我们命名这种方法为 INterpretable TRansformer（INTR），它容易实现并且具有许多吸引人的特性。我们表明，INTR 会自然地让每个类强制关注独特的特征，因此对比考虑的加权可以提供准确的预测解释。更加有趣的是，通过多头对比关注，INTR 可以识别不同的类属性，使其特别适合细致的分类和分析，我们在八个数据集上进行了示例。我们的代码和预训练模型可以在 <https://github.com/Imageomics/INTR> 中获取。
</details></li>
</ul>
<hr>
<h2 id="Contactless-Fingerprint-Biometric-Anti-Spoofing-An-Unsupervised-Deep-Learning-Approach"><a href="#Contactless-Fingerprint-Biometric-Anti-Spoofing-An-Unsupervised-Deep-Learning-Approach" class="headerlink" title="Contactless Fingerprint Biometric Anti-Spoofing: An Unsupervised Deep Learning Approach"></a>Contactless Fingerprint Biometric Anti-Spoofing: An Unsupervised Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04148">http://arxiv.org/abs/2311.04148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Banafsheh Adami, Nima Karimian</li>
<li>For: 本研究旨在提高无接触指纹识别系统的用户舒适性和防止指纹假样攻击。* Methods: 本研究使用了一种创新的防伪方法，combines an unsupervised autoencoder with a convolutional block attention module，并且在训练阶段不暴露模型于假样攻击。* Results: 研究发现，使用这种方法可以在不同类型的假样攻击图像测试阶段达到了0.96%的BPCER和1.6%的APCER。<details>
<summary>Abstract</summary>
Contactless fingerprint recognition offers a higher level of user comfort and addresses hygiene concerns more effectively. However, it is also more vulnerable to presentation attacks such as photo paper, paper-printout, and various display attacks, which makes it more challenging to implement in biometric systems compared to contact-based modalities. Limited research has been conducted on presentation attacks in contactless fingerprint systems, and these studies have encountered challenges in terms of generalization and scalability since both bonafide samples and presentation attacks are utilized during training model. Although this approach appears promising, it lacks the ability to handle unseen attacks, which is a crucial factor for developing PAD methods that can generalize effectively. We introduced an innovative anti-spoofing approach that combines an unsupervised autoencoder with a convolutional block attention module to address the limitations of existing methods. Our model is exclusively trained on bonafide images without exposure to any spoofed samples during the training phase. It is then evaluated against various types of presentation attack images in the testing phase. The scheme we proposed has achieved an average BPCER of 0.96\% with an APCER of 1.6\% for presentation attacks involving various types of spoofed samples.
</details>
<details>
<summary>摘要</summary>
无接触指纹识别技术可以提供更高水平的用户 COMFORT 和更好地解决卫生问题。然而，它也更容易受到展示攻击，如 фото纸、打印机打印出的纸张和多种显示攻击，这使得其在生物ometrics系统中实现更加挑战。有限的研究已经在无接触指纹系统中进行了展示攻击，但这些研究遇到了总结和扩展性的问题，因为训练模型时都需要使用真实的样本和攻击样本。这种方法看上去很有前途，但它缺乏对未看过的攻击的能力，这是生物метrics系统中发展可靠PAD方法的关键因素。我们介绍了一种创新的防伪方法，该方法结合了无监督自适应神经网络和卷积束注意模块来解决现有方法的局限性。我们的模型在训练阶段只接触 bonafide 图像，而不是任何假样本。然后，我们在测试阶段对各种展示攻击图像进行评估。我们的方案实现了平均BPCER 0.96% 和 APcer 1.6% 的表现，对于各种假样本展示攻击来说。
</details></li>
</ul>
<hr>
<h2 id="Locating-Cross-Task-Sequence-Continuation-Circuits-in-Transformers"><a href="#Locating-Cross-Task-Sequence-Continuation-Circuits-in-Transformers" class="headerlink" title="Locating Cross-Task Sequence Continuation Circuits in Transformers"></a>Locating Cross-Task Sequence Continuation Circuits in Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04131">http://arxiv.org/abs/2311.04131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Lan, Fazl Barez</li>
<li>for: 这个论文旨在探讨 transformer 模型如何进行语言任务，以及如何将其解释为可读的人类可理解的表示。</li>
<li>methods: 该论文使用了将 transformer 模型转换成可读的表示，称为电路，以实现算法功能。</li>
<li>results: 该论文通过分析和比较不同类型的序列续写任务的电路，发现了检测序列成员和预测下一个序列成员的关键子电路。此外，该论文还发现了semantically相关的序列使用共享的电路子графи，具有相似的角色。<details>
<summary>Abstract</summary>
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.
</details>
<details>
<summary>摘要</summary>
transformer 模型在语言任务上表现出色，但其复杂的架构使其难以解释。 recent work 尝试将 transformer 模型转化成可读的人类表示，称为Circuit。我们在这些研究的基础上进一步分析和比较不同的序列续写任务的Circuit，包括增加数字、数字名称和月份。通过Circuit分析技术，我们发现了检测序列成员的关键子Circuit和预测下一个序列成员的Circuit。我们的分析表明，semantically related sequences 使用共享的Circuit子图，具有相似的角色。总的来说，记录共享的计算结构可以更好地预测模型的行为，标识错误和安全地编辑过程。这种机制性的理解transformer 是建立更加稳定、aligned和可解释的语言模型的重要一步。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Safety-Vulnerabilities-of-Large-Language-Models"><a href="#Unveiling-Safety-Vulnerabilities-of-Large-Language-Models" class="headerlink" title="Unveiling Safety Vulnerabilities of Large Language Models"></a>Unveiling Safety Vulnerabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04124">http://arxiv.org/abs/2311.04124</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi</li>
<li>for: This paper is written to address the concern of harmful or inappropriate responses from large language models, and to provide a dataset (AttaQ) and a novel approach for identifying and naming vulnerable semantic regions in such models.</li>
<li>methods: The paper uses a unique dataset of adversarial examples in the form of questions, and introduces a novel automatic approach for identifying and naming vulnerable semantic regions in models using specialized clustering techniques.</li>
<li>results: The paper assesses the efficacy of its dataset and approach by analyzing the vulnerabilities of various models when subjected to the AttaQ dataset, and demonstrates the effectiveness of its approach in identifying and naming vulnerable semantic regions.<details>
<summary>Abstract</summary>
As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern. This paper introduces a unique dataset containing adversarial examples in the form of questions, which we call AttaQ, designed to provoke such harmful or inappropriate responses. We assess the efficacy of our dataset by analyzing the vulnerabilities of various models when subjected to it. Additionally, we introduce a novel automatic approach for identifying and naming vulnerable semantic regions - input semantic areas for which the model is likely to produce harmful outputs. This is achieved through the application of specialized clustering techniques that consider both the semantic similarity of the input attacks and the harmfulness of the model's responses. Automatically identifying vulnerable semantic regions enhances the evaluation of model weaknesses, facilitating targeted improvements to its safety mechanisms and overall reliability.
</details>
<details>
<summary>摘要</summary>
大型语言模型在使用时，可能会产生不当或不适当的回应，这引起了对其可能的害的担忧。本篇文章介绍了一个唯一的数据集，即AttaQ，这是一组设计来提oking这些害的问题。我们通过分析不同模型在面对这个数据集时的脆弱性，以评估数据集的有效性。此外，我们还引入了一种新的自动化方法，可以自动识别和命名容易受到害的 semantic 区域，这些区域是模型对于特定的入力攻击而产生不当回应的区域。我们通过特殊的聚类技术来实现这一点，考虑了对于入力攻击的Semantic similarity和模型对于这些攻击的回应的害性。自动识别容易受到害的 semantic 区域可以增强模型的评估，并且可以帮助改善模型的安全机制和整体可靠性。
</details></li>
</ul>
<hr>
<h2 id="ETDPC-A-Multimodality-Framework-for-Classifying-Pages-in-Electronic-Theses-and-Dissertations"><a href="#ETDPC-A-Multimodality-Framework-for-Classifying-Pages-in-Electronic-Theses-and-Dissertations" class="headerlink" title="ETDPC: A Multimodality Framework for Classifying Pages in Electronic Theses and Dissertations"></a>ETDPC: A Multimodality Framework for Classifying Pages in Electronic Theses and Dissertations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04262">http://arxiv.org/abs/2311.04262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lamps-lab/ETDMiner">https://github.com/lamps-lab/ETDMiner</a></li>
<li>paper_authors: Muntabir Hasan Choudhury, Lamia Salsabil, William A. Ingram, Edward A. Fox, Jian Wu</li>
<li>for: This paper aims to segment Electronic Theses and Dissertations (ETDs) into 13 categories to facilitate navigation and exploration of the content.</li>
<li>methods: The proposed method, ETDPC, uses a two-stream multimodal model with a cross-attention network to classify ETD pages. To address the challenge of imbalanced labeled samples, the authors augmented data for minority categories and employed a hierarchical classifier.</li>
<li>results: ETDPC outperforms the state-of-the-art models in all categories, achieving an F1 score of 0.84 to 0.96 for 9 out of 13 categories. The authors also demonstrated the data efficiency of their approach.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文旨在为电子硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件<details>
<summary>Abstract</summary>
Electronic theses and dissertations (ETDs) have been proposed, advocated, and generated for more than 25 years. Although ETDs are hosted by commercial or institutional digital library repositories, they are still an understudied type of scholarly big data, partially because they are usually longer than conference proceedings and journals. Segmenting ETDs will allow researchers to study sectional content. Readers can navigate to particular pages of interest, discover, and explore the content buried in these long documents. Most existing frameworks on document page classification are designed for classifying general documents and perform poorly on ETDs. In this paper, we propose ETDPC. Its backbone is a two-stream multimodal model with a cross-attention network to classify ETD pages into 13 categories. To overcome the challenge of imbalanced labeled samples, we augmented data for minority categories and employed a hierarchical classifier. ETDPC outperforms the state-of-the-art models in all categories, achieving an F1 of 0.84 -- 0.96 for 9 out of 13 categories. We also demonstrated its data efficiency. The code and data can be found on GitHub (https://github.com/lamps-lab/ETDMiner/tree/master/etd_segmentation).
</details>
<details>
<summary>摘要</summary>
电子硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Large-Language-Models-in-Ophthalmology"><a href="#Evaluating-Large-Language-Models-in-Ophthalmology" class="headerlink" title="Evaluating Large Language Models in Ophthalmology"></a>Evaluating Large Language Models in Ophthalmology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04933">http://arxiv.org/abs/2311.04933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Holmes, Shuyuan Ye, Yiwei Li, Shi-Nan Wu, Zhengliang Liu, Zihao Wu, Jinyu Hu, Huan Zhao, Xi Jiang, Wei Liu, Hong Wei, Jie Zou, Tianming Liu, Yi Shao</li>
<li>for: 评估三种大型语言模型（GPT-3.5、GPT-4、PaLM2）在回答眼科专业问题方面的表现，并与不同专业水平（医学本科生、医学硬件生、医生）进行比较。</li>
<li>methods: 使用100个眼科单选测验，对三种语言模型和三种专业水平进行测试，并对LLM的表现进行全面评估和比较。</li>
<li>results: 每种LLM都超过了医学本科生的平均分，其中GPT-4的表现与医生水平相当，而GPT-3.5和PaLM2略为下降到医学硬件生水平。此外，GPT-4的答案稳定性和自信度显著高于GPT-3.5和PaLM2。结论：我们的研究表明，LLM代表的GPT-4在眼科领域表现出色。随着进一步改进，LLM将带来未知的医学教育和临床决策的突破。<details>
<summary>Abstract</summary>
Purpose: The performance of three different large language models (LLMS) (GPT-3.5, GPT-4, and PaLM2) in answering ophthalmology professional questions was evaluated and compared with that of three different professional populations (medical undergraduates, medical masters, and attending physicians). Methods: A 100-item ophthalmology single-choice test was administered to three different LLMs (GPT-3.5, GPT-4, and PaLM2) and three different professional levels (medical undergraduates, medical masters, and attending physicians), respectively. The performance of LLM was comprehensively evaluated and compared with the human group in terms of average score, stability, and confidence. Results: Each LLM outperformed undergraduates in general, with GPT-3.5 and PaLM2 being slightly below the master's level, while GPT-4 showed a level comparable to that of attending physicians. In addition, GPT-4 showed significantly higher answer stability and confidence than GPT-3.5 and PaLM2. Conclusion: Our study shows that LLM represented by GPT-4 performs better in the field of ophthalmology. With further improvements, LLM will bring unexpected benefits in medical education and clinical decision making in the near future.
</details>
<details>
<summary>摘要</summary>
目的：评估三种不同的大语言模型（LLM）（GPT-3.5、GPT-4和PaLM2）在回答眼科专业问题上的表现，并与三种不同的专业人群（医学本科生、医学硬件生和医生）进行比较。方法：为三种LLM和三种专业水平（医学本科生、医学硬件生和医生）分别进行100项眼科单选测验。对LLM的表现进行全面评估和比较，包括平均分、稳定性和信心度。结果：每种LLM都比本科生在总体的表现得高，GPT-3.5和PaLM2只是微不足道的与硬件生水平相当，而GPT-4则与医生水平相当。此外，GPT-4还表现出了明显更高的答案稳定性和信心度。结论：我们的研究表明，LLM表示的GPT-4在眼科领域表现出色。随着进一步改进，LLM将在医学教育和临床决策中带来无法预期的优势。
</details></li>
</ul>
<hr>
<h2 id="Multitask-Multimodal-Prompted-Training-for-Interactive-Embodied-Task-Completion"><a href="#Multitask-Multimodal-Prompted-Training-for-Interactive-Embodied-Task-Completion" class="headerlink" title="Multitask Multimodal Prompted Training for Interactive Embodied Task Completion"></a>Multitask Multimodal Prompted Training for Interactive Embodied Task Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04067">http://arxiv.org/abs/2311.04067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgios Pantazopoulos, Malvina Nikandrou, Amit Parekh, Bhathiya Hemanthage, Arash Eshghi, Ioannis Konstas, Verena Rieser, Oliver Lemon, Alessandro Suglia</li>
<li>for: 该论文旨在解决现有视觉语言（VL）模型面临的两个基本挑战，即将语言融入行为轨迹和观察轨迹，以及referential歧义。</li>
<li>methods: 该论文提出了一种Embodied MultiModal Agent（EMMA）模型，它是一个统一的编码-解码模型，可以理解图像和轨迹，并将动作预测设置为多模态文本生成。通过将所有任务作为文本生成集成，EMMA学习了一种行为语言，这有助于在任务之间进行转移。与之前的分模块方法不同，我们使用了一个单一的多任务模型，每个任务均贡献到完成目标。</li>
<li>results: EMMA在多个VLbenchmark上表现与其他模型相当，并在Dialog-guided Task Completion（DTC）benchmark上达到了新的州OF-THE-ART性能（36.81%成功率），这是一个用于评估对话导向的智能代理在Alexa Arena的 benchmark。<details>
<summary>Abstract</summary>
Interactive and embodied tasks pose at least two fundamental challenges to existing Vision & Language (VL) models, including 1) grounding language in trajectories of actions and observations, and 2) referential disambiguation. To tackle these challenges, we propose an Embodied MultiModal Agent (EMMA): a unified encoder-decoder model that reasons over images and trajectories, and casts action prediction as multimodal text generation. By unifying all tasks as text generation, EMMA learns a language of actions which facilitates transfer across tasks. Different to previous modular approaches with independently trained components, we use a single multitask model where each task contributes to goal completion. EMMA performs on par with similar models on several VL benchmarks and sets a new state-of-the-art performance (36.81% success rate) on the Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guided agents in the Alexa Arena
</details>
<details>
<summary>摘要</summary>
现有的视觉语言（VL）模型面临两个基本挑战：1）将语言融入行为和观察的轨迹中，2）确定参照的ambiguation。为解决这些挑战，我们提议一个embodied multimodal agent（EMMA）：一个统一的编码解码模型，用于处理图像和轨迹，并将动作预测转化为多modal文本生成。通过将所有任务视为文本生成，EMMA学习了一种行为语言，该语言促进了任务之间的传递。与之前的模块化方法不同，我们使用一个单一的多任务模型，其中每个任务协助完成目标。EMMA在多个VL benchmark上达到了与其他模型相当的性能（36.81%成功率），并在Dialog-guided Task Completion（DTC） benchmark上创下了新的州Of-the-art表现（36.81%成功率），这是一个用于评估对话引导的智能客厅中的代表性任务。
</details></li>
</ul>
<hr>
<h2 id="Can-CLIP-Help-Sound-Source-Localization"><a href="#Can-CLIP-Help-Sound-Source-Localization" class="headerlink" title="Can CLIP Help Sound Source Localization?"></a>Can CLIP Help Sound Source Localization?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04066">http://arxiv.org/abs/2311.04066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Sooyoung Park, Arda Senocak, Joon Son Chung<br>for: 这个研究是用于探索如何使用预训语音-影像模型来进行声源地图化。methods: 我们使用CLIP预训模型，并将音频讯号转换为CLIP可以处理的字串表示。然后，我们使用这些字串表示生成音频驱动的封包，将音频驱动的影像特征提取出来，并与音频驱动的字串表示进行对预对。results: 我们的方法可以将声音驱动的影像特征更加完整和紧凑地图示出。实验结果显示，我们的方法可以与现有的方法比较，获得更高的性能。<details>
<summary>Abstract</summary>
Large-scale pre-trained image-text models demonstrate remarkable versatility across diverse tasks, benefiting from their robust representational capabilities and effective multimodal alignment. We extend the application of these models, specifically CLIP, to the domain of sound source localization. Unlike conventional approaches, we employ the pre-trained CLIP model without explicit text input, relying solely on the audio-visual correspondence. To this end, we introduce a framework that translates audio signals into tokens compatible with CLIP's text encoder, yielding audio-driven embeddings. By directly using these embeddings, our method generates audio-grounded masks for the provided audio, extracts audio-grounded image features from the highlighted regions, and aligns them with the audio-driven embeddings using the audio-visual correspondence objective. Our findings suggest that utilizing pre-trained image-text models enable our model to generate more complete and compact localization maps for the sounding objects. Extensive experiments show that our method outperforms state-of-the-art approaches by a significant margin.
</details>
<details>
<summary>摘要</summary>
大规模预训练图像文本模型在多种任务上表现出了惊人的多样性，受益于它们的强大表示能力和有效的多媒体对接。我们在这些模型中 specifically CLIP 中扩展应用，特别是不需要显式文本输入，而是仅仅通过音视频对应关系来使用。为此，我们提出了一种框架，将音频信号转换成CLIP的文本编码器兼容的 токен，从而生成音频驱动的嵌入。通过直接使用这些嵌入，我们的方法生成了音频驱动的掩模，提取音频驱动的图像特征从高亮区域，并使用音视频对应关系目标来对准这些嵌入。我们的发现表明，通过使用预训练的图像文本模型，我们的模型可以生成更加完整和紧凑的当地化图。广泛的实验表明，我们的方法在相对比较的情况下，与现有的方法相比，具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Causal-Representation-Learning-with-Partial-Observability"><a href="#Multi-View-Causal-Representation-Learning-with-Partial-Observability" class="headerlink" title="Multi-View Causal Representation Learning with Partial Observability"></a>Multi-View Causal Representation Learning with Partial Observability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04056">http://arxiv.org/abs/2311.04056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dingling Yao, Danru Xu, Sébastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg Martius, Julius von Kügelgen, Francesco Locatello</li>
<li>for: studying the identifiability of representations learned from simultaneously observed views, such as different data modalities.</li>
<li>methods: using contrastive learning and a single encoder per view to learn the information shared across all subsets of any number of views.</li>
<li>results: the paper provides a unified framework and theoretical results that extend and unify several previous works on multi-view nonlinear ICA, disentanglement, and causal representation learning, and experimentally validate the claims on numerical, image, and multi-modal data sets.<details>
<summary>Abstract</summary>
We present a unified framework for studying the identifiability of representations learned from simultaneously observed views, such as different data modalities. We allow a partially observed setting in which each view constitutes a nonlinear mixture of a subset of underlying latent variables, which can be causally related. We prove that the information shared across all subsets of any number of views can be learned up to a smooth bijection using contrastive learning and a single encoder per view. We also provide graphical criteria indicating which latent variables can be identified through a simple set of rules, which we refer to as identifiability algebra. Our general framework and theoretical results unify and extend several previous works on multi-view nonlinear ICA, disentanglement, and causal representation learning. We experimentally validate our claims on numerical, image, and multi-modal data sets. Further, we demonstrate that the performance of prior methods is recovered in different special cases of our setup. Overall, we find that access to multiple partial views enables us to identify a more fine-grained representation, under the generally milder assumption of partial observability.
</details>
<details>
<summary>摘要</summary>
我们提出一个统一的框架，用于研究同时观察到的视图中学习的表示学习问题。我们允许部分观察的设置，在每个视图中包含一个非线性混合的一部分下面变量，这些变量可能是相关的。我们证明了，通过对所有 subsets 的任意数量的视图进行对比学习，可以学习到一个精细的映射，以便在所有视图中共享信息。我们还提供了一组图像标准，用于判断哪些变量可以通过简单的规则进行标识。我们的总框架和理论结论统一并扩展了多视图非线性ICA、解耦和 causal 表示学习的前期工作。我们在数学、图像和多Modal 数据集上进行了实验验证，并证明了我们的假设的性能。此外，我们还证明了我们的设置下，先前的方法的性能可以在不同的特殊情况下被恢复。总之，我们发现了访问多个部分视图，可以在更加轻松的假设下，标识一个更细grained的表示。
</details></li>
</ul>
<hr>
<h2 id="Causal-Discovery-Under-Local-Privacy"><a href="#Causal-Discovery-Under-Local-Privacy" class="headerlink" title="Causal Discovery Under Local Privacy"></a>Causal Discovery Under Local Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04037">http://arxiv.org/abs/2311.04037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rūta Binkytė, Carlos Pinzón, Szilvia Lestyán, Kangsoo Jung, Héber H. Arcolezi, Catuscia Palamidessi</li>
<li>for: 本研究是针对 differential privacy 框架中的本地隐私 Mechanism 的应用，以保护数据提供者的敏感信息。</li>
<li>methods: 本研究使用了多种知名的本地隐私机制，并评估了这些机制对于 causal discovery  задача的干扰。</li>
<li>results: 研究发现，不同的本地隐私机制对于 causal discovery 任务的干扰程度不同，并且提供了选择适当的本地隐私协议的 valuable insights。<details>
<summary>Abstract</summary>
Differential privacy is a widely adopted framework designed to safeguard the sensitive information of data providers within a data set. It is based on the application of controlled noise at the interface between the server that stores and processes the data, and the data consumers. Local differential privacy is a variant that allows data providers to apply the privatization mechanism themselves on their data individually. Therefore it provides protection also in contexts in which the server, or even the data collector, cannot be trusted. The introduction of noise, however, inevitably affects the utility of the data, particularly by distorting the correlations between individual data components. This distortion can prove detrimental to tasks such as causal discovery. In this paper, we consider various well-known locally differentially private mechanisms and compare the trade-off between the privacy they provide, and the accuracy of the causal structure produced by algorithms for causal learning when applied to data obfuscated by these mechanisms. Our analysis yields valuable insights for selecting appropriate local differentially private protocols for causal discovery tasks. We foresee that our findings will aid researchers and practitioners in conducting locally private causal discovery.
</details>
<details>
<summary>摘要</summary>
differential privacy 是一种广泛采用的框架，用于保护数据提供者在数据集中的敏感信息。它基于数据处理和存储服务器和数据消费者之间应用控制的噪声的原理。本地异步隐私是一种变体，允许数据提供者本地应用隐私机制于他们的数据。因此，它可以在服务器或数据收集者无法被信任的情况下提供保护。噪声的引入，然而，必然影响数据的利用性，特别是对数据组件之间的相关性进行扭曲。这种扭曲可能对 causal discovery 任务产生负面影响。在这篇论文中，我们考虑了多种常见的本地异步隐私机制，并比较这些机制提供的隐私和 causal learning 算法应用于混淆后的数据中的准确性的负面影响。我们的分析带来了有价值的洞察，可以帮助研究人员和实践者选择合适的本地异步隐私协议进行 causal discovery 任务。我们预计，我们的发现将助力研究人员和实践者在本地私人方式进行 causal discovery。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-HPO-on-AutoML-Forecasting-Ensembles"><a href="#Impact-of-HPO-on-AutoML-Forecasting-Ensembles" class="headerlink" title="Impact of HPO on AutoML Forecasting Ensembles"></a>Impact of HPO on AutoML Forecasting Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04034">http://arxiv.org/abs/2311.04034</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Hoffmann</li>
<li>for: 这篇论文旨在探讨如何将不同的数据分析方法融合在一起以提高forecasting的精度。</li>
<li>methods: 论文使用了多种数据分析方法，包括MQ-CNN、DeepAR、Prophet、NPTS、ARIMA和ETS，并考虑了不同的数据分析方法之间的交互作用。</li>
<li>results: 论文的结果显示，在这种设置中，添加了搜索优化策略可以提高预测的精度，并且与基eline ensemble безHPO相比，具有9.9%的精度提升和65.8%的终端集成延迟时间提升。此外，这种设置还能够超越现有的商业AutoML预测解决方案Amazon Forecast，具有3.5%的误差降低和16.0%的终端集成延迟时间降低。<details>
<summary>Abstract</summary>
A forecasting ensemble consisting of a diverse range of estimators for both local and global univariate forecasting, in particular MQ-CNN,DeepAR, Prophet, NPTS, ARIMA and ETS, can be used to make forecasts for a variety of problems. This paper delves into the aspect of adding different hyperparameter optimization strategies to the deep learning models in such a setup (DeepAR and MQ-CNN), exploring the trade-off between added training cost and the increase in accuracy for different configurations. It shows that in such a setup, adding hyperparameter optimization can lead to performance improvements, with the final setup having a 9.9 % percent accuracy improvement with respect to the avg-wQL over the baseline ensemble without HPO, accompanied by a 65.8 % increase in end-to-end ensemble latency. This improvement is based on an empirical analysis of combining the ensemble pipeline with different tuning strategies, namely Bayesian Optimisation and Hyperband and different configurations of those strategies. In the final configuration, the proposed combination of ensemble learning and HPO outperforms the state of the art commercial AutoML forecasting solution, Amazon Forecast, with a 3.5 % lower error and 16.0 % lower end-to-end ensemble latency.
</details>
<details>
<summary>摘要</summary>
一个包含多种估计器的预测集群，包括MQ-CNN、DeepAR、Prophet、NPTS、ARIMA和ETS，可以用来预测多种问题。这篇论文探讨了将不同的超参数优化策略添加到深度学习模型中的影响（DeepAR和MQ-CNN），探究加入超参数优化后的准确性提升和预测时间的费用增加的负担关系。结果表明，在这种设置下，添加超参数优化可以提高性能，最终集群的准确率与对比 ensemble without HPO 的平均值QL差异为9.9%，同时预测时间增加65.8%。这种提升基于对不同配置的 ensemble pipeline 的实际分析，包括bayesian optimization和hyperband等策略的组合。最终配置中，提案的集群学习和HPO组合超过了商业AutoML预测解决方案 Amazon Forecast 的状态的误差和总体预测时间。
</details></li>
</ul>
<hr>
<h2 id="IoT-Based-Environmental-Control-System-for-Fish-Farms-with-Sensor-Integration-and-Machine-Learning-Decision-Support"><a href="#IoT-Based-Environmental-Control-System-for-Fish-Farms-with-Sensor-Integration-and-Machine-Learning-Decision-Support" class="headerlink" title="IoT-Based Environmental Control System for Fish Farms with Sensor Integration and Machine Learning Decision Support"></a>IoT-Based Environmental Control System for Fish Farms with Sensor Integration and Machine Learning Decision Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04258">http://arxiv.org/abs/2311.04258</a></li>
<li>repo_url: None</li>
<li>paper_authors: D. Dhinakaran, S. Gopalakrishnan, M. D. Manigandan, T. P. Anish<br>for: 这个研究旨在提高鱼养殖的环境控制和生产效率，以满足全球增长的海food需求，同时强调环境责任和经济可持续性。methods: 这个研究使用了互联网物联网（IoT）技术和先进的机器学习决策支持系统，在鱼养殖场中部署无线传感器网络，实时收集环境参数数据，包括水温、pH值、湿度和鱼类行为等。数据经过仔细的预处理，包括填充、异常检测、特征工程和同步。results: 这个系统使用四种不同的机器学习算法来实现环境控制和生产效率，包括随机森林算法（Random Forests）优化水温和pH值，检测和预测鱼类疾病和寄生虫；支持向量机器（SVMs）早期检测疾病和寄生虫；梯度提升机器（GBMs）动态调整饲料时间以适应实时环境 Conditions，提高鱼类生长和产量，同时减少资源浪费。这些机器学习算法共同实现实时决策，使鱼养殖环境Conditions与预定的规范相匹配，提高鱼类健康和产量，同时降低资源浪费，从而提高可持续性和经济可行性。<details>
<summary>Abstract</summary>
In response to the burgeoning global demand for seafood and the challenges of managing fish farms, we introduce an innovative IoT based environmental control system that integrates sensor technology and advanced machine learning decision support. Deploying a network of wireless sensors within the fish farm, we continuously collect real-time data on crucial environmental parameters, including water temperature, pH levels, humidity, and fish behavior. This data undergoes meticulous preprocessing to ensure its reliability, including imputation, outlier detection, feature engineering, and synchronization. At the heart of our system are four distinct machine learning algorithms: Random Forests predict and optimize water temperature and pH levels for the fish, fostering their health and growth; Support Vector Machines (SVMs) function as an early warning system, promptly detecting diseases and parasites in fish; Gradient Boosting Machines (GBMs) dynamically fine-tune the feeding schedule based on real-time environmental conditions, promoting resource efficiency and fish productivity; Neural Networks manage the operation of critical equipment like water pumps and heaters to maintain the desired environmental conditions within the farm. These machine learning algorithms collaboratively make real-time decisions to ensure that the fish farm's environmental conditions align with predefined specifications, leading to improved fish health and productivity while simultaneously reducing resource wastage, thereby contributing to increased profitability and sustainability. This research article showcases the power of data-driven decision support in fish farming, promising to meet the growing demand for seafood while emphasizing environmental responsibility and economic viability, thus revolutionizing the future of fish farming.
</details>
<details>
<summary>摘要</summary>
为了应对全球海鲜需求的增长和抚养鱼场的挑战，我们介绍了一种基于互联网物理（IoT）的环境控制系统，它 integrate了感知技术和高级机器学习决策支持。在鱼场中部署了无线感知器，实时收集了关键环境参数的数据，包括水温、pH值、湿度和鱼类行为。这些数据经过仔细的处理，以确保其可靠性，包括填充、异常检测、特征工程和同步。系统的核心是四种不同的机器学习算法：随机森林预测和优化水温和pH值，以促进鱼类健康和生长；支持向量机器（SVM）作为早期警报系统，迅速检测鱼类疾病和寄生虫；梯度提升机器（GBM）动态细化饲料时间表，以实现资源效率和鱼类产量的提高；神经网络管理关键设备如水泵和加热器，以保持鱼场的欲要的环境条件。这些机器学习算法合作实时做出决策，以确保鱼场的环境条件与预先定义的规范相符，从而提高鱼类健康和产量，同时减少资源浪费，从而提高可持续性和经济可行性。这篇研究文章展示了数据驱动决策支持在鱼养业中的力量，承诺满足全球海鲜需求的增长，同时强调环境负责任和经济可行性，从而革命化未来的鱼养业发展。
</details></li>
</ul>
<hr>
<h2 id="Expressivity-of-ReLU-Networks-under-Convex-Relaxations"><a href="#Expressivity-of-ReLU-Networks-under-Convex-Relaxations" class="headerlink" title="Expressivity of ReLU-Networks under Convex Relaxations"></a>Expressivity of ReLU-Networks under Convex Relaxations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04015">http://arxiv.org/abs/2311.04015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Baader, Mark Niklas Müller, Yuhao Mao, Martin Vechev</li>
<li>for: 论文旨在探讨是否存在基于凸relaxation的神经网络训练和证明的基本限制。</li>
<li>methods: 论文使用了多种常用的凸relaxation，包括IBP relaxation和其他更高级别的relaxation，以探讨ReLU网络可以表示的函数类型和精度。</li>
<li>results: 研究结果显示，更高级别的凸relaxation可以表示更多的单变量函数，而且可以 exponential larger的解空间；即使使用最精度的单 neuron relaxation，也无法构建可以表示多变量凸CPWL函数的ReLU网络。<details>
<summary>Abstract</summary>
Convex relaxations are a key component of training and certifying provably safe neural networks. However, despite substantial progress, a wide and poorly understood accuracy gap to standard networks remains, raising the question of whether this is due to fundamental limitations of convex relaxations. Initial work investigating this question focused on the simple and widely used IBP relaxation. It revealed that some univariate, convex, continuous piecewise linear (CPWL) functions cannot be encoded by any ReLU network such that its IBP-analysis is precise. To explore whether this limitation is shared by more advanced convex relaxations, we conduct the first in-depth study on the expressive power of ReLU networks across all commonly used convex relaxations. We show that: (i) more advanced relaxations allow a larger class of univariate functions to be expressed as precisely analyzable ReLU networks, (ii) more precise relaxations can allow exponentially larger solution spaces of ReLU networks encoding the same functions, and (iii) even using the most precise single-neuron relaxations, it is impossible to construct precisely analyzable ReLU networks that express multivariate, convex, monotone CPWL functions.
</details>
<details>
<summary>摘要</summary>
凸relaxation是训练和证明可靠神经网络的关键组成部分。然而，尽管已经取得了重要进展，但是仍存在一个宽泛而不受理解的准确缺失，这引发了标准网络的准确性限制是凸relaxation的问题。初步的研究发现，使用IBP relaxation时，一些单变量、凸、连续弯曲函数（CPWL）不能被ReLU网络编码，使其IBP分析准确。为了检查这些限制是否被更高级的凸relaxation卷入，我们进行了所有常用凸relaxation的深入研究。我们发现：1.更高级的relaxation允许更多的单变量函数被 preciselly analyzable ReLU网络表示，2.更精准的relaxation可以让ReLU网络表示的函数空间增加 exponentially，3.即使使用最精准的单 neuron relaxation，也无法构建preciseley analyzable ReLU网络，表示多变量、凸、弯曲CPWL函数。
</details></li>
</ul>
<hr>
<h2 id="A-Method-to-Improve-the-Performance-of-Reinforcement-Learning-Based-on-the-Y-Operator-for-a-Class-of-Stochastic-Differential-Equation-Based-Child-Mother-Systems"><a href="#A-Method-to-Improve-the-Performance-of-Reinforcement-Learning-Based-on-the-Y-Operator-for-a-Class-of-Stochastic-Differential-Equation-Based-Child-Mother-Systems" class="headerlink" title="A Method to Improve the Performance of Reinforcement Learning Based on the Y Operator for a Class of Stochastic Differential Equation-Based Child-Mother Systems"></a>A Method to Improve the Performance of Reinforcement Learning Based on the Y Operator for a Class of Stochastic Differential Equation-Based Child-Mother Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04014">http://arxiv.org/abs/2311.04014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Yin, Yi Chen</li>
<li>for: 提高基于actor-critic学习的控制性能，特别是在带有渐变随机过程的系统中。</li>
<li>methods: 提出了一种新的运算符，称为Y运算符，它将带有渐变随机过程的孩子系统的随机性 интеграinto critic网络的损失函数中，从而实现了RL算法的控制性能的明显提高。</li>
<li>results: Y运算符能够很好地解决RL算法在模型基于和数据驱动系统中的优化控制问题，并且在线性和非线性数学示例中展现出了与现有方法相比的显著提高。<details>
<summary>Abstract</summary>
This paper introduces a novel operator, termed the Y operator, to elevate control performance in Actor-Critic(AC) based reinforcement learning for systems governed by stochastic differential equations(SDEs). The Y operator ingeniously integrates the stochasticity of a class of child-mother system into the Critic network's loss function, yielding substantial advancements in the control performance of RL algorithms.Additionally, the Y operator elegantly reformulates the challenge of solving partial differential equations for the state-value function into a parallel problem for the drift and diffusion functions within the system's SDEs.A rigorous mathematical proof confirms the operator's validity.This transformation enables the Y Operator-based Reinforcement Learning(YORL) framework to efficiently tackle optimal control problems in both model-based and data-driven systems.The superiority of YORL is demonstrated through linear and nonlinear numerical examples showing its enhanced performance over existing methods post convergence.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的运算符，称为Y运算符，用于提高基于actor-critic（AC）学习算法的控制性能，适用于具有杂度的随机 diffeq 方程（SDE）的系统。Y运算符 Innately integrate了一类的child-mother系统的随机性 into the critic网络的损失函数中，从而实现了重要的控制性能提升。此外，Y运算符也 elegantly reformulates了解决部分 дифференциал方程的解值函数问题，转化成了并行的涨落函数问题。一个严格的数学证明证明了该运算符的有效性。这种转化使得YORL框架可以高效地解决模型基于和数据驱动的优化控制问题。数字示例表明，YORL在线性和非线性问题中的表现都胜过现有方法。
</details></li>
</ul>
<hr>
<h2 id="The-Energy-Prediction-Smart-Meter-Dataset-Analysis-of-Previous-Competitions-and-Beyond"><a href="#The-Energy-Prediction-Smart-Meter-Dataset-Analysis-of-Previous-Competitions-and-Beyond" class="headerlink" title="The Energy Prediction Smart-Meter Dataset: Analysis of Previous Competitions and Beyond"></a>The Energy Prediction Smart-Meter Dataset: Analysis of Previous Competitions and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04007">http://arxiv.org/abs/2311.04007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Direnc Pekaslan, Jose Maria Alonso-Moral, Kasun Bandara, Christoph Bergmeir, Juan Bernabe-Moreno, Robert Eigenmann, Nils Einecke, Selvi Ergen, Rakshitha Godahewa, Hansika Hewamalage, Jesus Lago, Steffen Limmer, Sven Rebhan, Boris Rabinovich, Dilini Rajapasksha, Heda Song, Christian Wagner, Wenlong Wu, Luis Magdalena, Isaac Triguero</li>
<li>for: 本研究提供了一个真实世界智能仪表数据集，并对能源预测技术挑战进行分析，主要集中在2020年IEEE计算机智能学会（IEEE-CIS）技术挑战和2021年IEEE国际 Conference on Fuzzy Systems（FUZZ-IEEE）技术挑战中。这两个挑战的目标是准确预测家庭能源消耗，并解释在下面的因素上。</li>
<li>methods: 本研究使用了不同的方法，包括机器学习、深度学习和杂度学习等，以提高能源预测的准确性和可解释性。</li>
<li>results: 本研究对实际世界智能仪表数据集进行分析，并提出了一些精准的能源预测方法，同时也提出了一些可解释性评价指标。<details>
<summary>Abstract</summary>
This paper presents the real-world smart-meter dataset and offers an analysis of solutions derived from the Energy Prediction Technical Challenges, focusing primarily on two key competitions: the IEEE Computational Intelligence Society (IEEE-CIS) Technical Challenge on Energy Prediction from Smart Meter data in 2020 (named EP) and its follow-up challenge at the IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) in 2021 (named as XEP). These competitions focus on accurate energy consumption forecasting and the importance of interpretability in understanding the underlying factors. The challenge aims to predict monthly and yearly estimated consumption for households, addressing the accurate billing problem with limited historical smart meter data. The dataset comprises 3,248 smart meters, with varying data availability ranging from a minimum of one month to a year. This paper delves into the challenges, solutions and analysing issues related to the provided real-world smart meter data, developing accurate predictions at the household level, and introducing evaluation criteria for assessing interpretability. Additionally, this paper discusses aspects beyond the competitions: opportunities for energy disaggregation and pattern detection applications at the household level, significance of communicating energy-driven factors for optimised billing, and emphasising the importance of responsible AI and data privacy considerations. These aspects provide insights into the broader implications and potential advancements in energy consumption prediction. Overall, these competitions provide a dataset for residential energy research and serve as a catalyst for exploring accurate forecasting, enhancing interpretability, and driving progress towards the discussion of various aspects such as energy disaggregation, demand response programs or behavioural interventions.
</details>
<details>
<summary>摘要</summary>
The dataset includes 3,248 smart meters with varying data availability, ranging from one month to one year. The paper discusses the challenges, solutions, and evaluation criteria for developing accurate predictions at the household level. Additionally, it explores opportunities for energy disaggregation and pattern detection applications, the significance of communicating energy-driven factors for optimized billing, and the importance of responsible AI and data privacy considerations.These competitions provide a dataset for residential energy research and serve as a catalyst for exploring accurate forecasting, enhancing interpretability, and driving progress towards discussions of various aspects such as energy disaggregation, demand response programs, or behavioral interventions. Overall, the paper provides insights into the broader implications and potential advancements in energy consumption prediction.
</details></li>
</ul>
<hr>
<h2 id="Foundational-propositions-of-hesitant-fuzzy-sets-and-parameter-reductions-of-hesitant-fuzzy-information-systems"><a href="#Foundational-propositions-of-hesitant-fuzzy-sets-and-parameter-reductions-of-hesitant-fuzzy-information-systems" class="headerlink" title="Foundational propositions of hesitant fuzzy sets and parameter reductions of hesitant fuzzy information systems"></a>Foundational propositions of hesitant fuzzy sets and parameter reductions of hesitant fuzzy information systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04256">http://arxiv.org/abs/2311.04256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shizhan Lu</li>
<li>for: 这篇论文探讨了不确定和犹豫的情况下的软集的定义和应用。</li>
<li>methods: 该论文提出了基于不确定软集的形式的各种包含关系，并给出了一些基础定理和软集家族的描述。</li>
<li>results: 论文提出了基于不确定软集的参数缩减的基本提案，并给出了一个示例和一个算法来演示该过程。<details>
<summary>Abstract</summary>
Hesitant fuzzy sets are widely used in the instances of uncertainty and hesitation. The inclusion relationship is an important and foundational definition for sets. Hesitant fuzzy set, as a kind of set, needs explicit definition of inclusion relationship. Base on the hesitant fuzzy membership degree of discrete form, several kinds of inclusion relationships for hesitant fuzzy sets are proposed. And then some foundational propositions of hesitant fuzzy sets and the families of hesitant fuzzy sets are presented. Finally, some foundational propositions of hesitant fuzzy information systems with respect to parameter reductions are put forward, and an example and an algorithm are given to illustrate the processes of parameter reductions.
</details>
<details>
<summary>摘要</summary>
《不确定和犹豫的集合》广泛应用于不确定和犹豫的情况下。集合关系是不确定集合的基本定义。hesitant fuzzy set需要明确的包含关系定义。基于不确定模糊会员度的离散形式，对不确定集合的包含关系提出了多种建议。然后，对不确定集合和其家族进行了一些基本提案和推论。最后，对不确定模糊信息系统中参数减少的基本提案，并给出了一个示例和一个算法，以便 Illustrate the parameter reduction process.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Human-AI-Collaboration-in-Thematic-Analysis-using-ChatGPT-A-User-Study-and-Design-Recommendations"><a href="#Human-AI-Collaboration-in-Thematic-Analysis-using-ChatGPT-A-User-Study-and-Design-Recommendations" class="headerlink" title="Human-AI Collaboration in Thematic Analysis using ChatGPT: A User Study and Design Recommendations"></a>Human-AI Collaboration in Thematic Analysis using ChatGPT: A User Study and Design Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03999">http://arxiv.org/abs/2311.03999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lixiang Yan, Vanessa Echeverria, Gloria Fernandez Nieto, Yueqiao Jin, Zachari Swiecki, Linxuan Zhao, Dragan Gašević, Roberto Martinez-Maldonado</li>
<li>for: 本研究探讨了研究者与生成人工智能（GenAI）的合作方式，尤其是使用ChatGPT进行资深分析。</li>
<li>methods: 研究者通过与ChatGPT进行合作，发现它可以帮助提高分析效率、帮助初步数据探索、提供细腻的量化预测和帮助非Native speaker和非专家理解。</li>
<li>results: 虽然ChatGPT显示了一定的价值，但研究者还存在对其可靠性、一致性和广泛acceptance within the research community的关切。本研究还提出了五项行动方案，以促进人AI合作的效果。这些方案包括在合作机制中提供透明的解释，提高界面和集成能力，优先级Contextual understanding和个性化，嵌入人AI反馈循环和迭代功能，以及加强信任 durch validation mechanisms。<details>
<summary>Abstract</summary>
Generative artificial intelligence (GenAI) offers promising potential for advancing human-AI collaboration in qualitative research. However, existing works focused on conventional machine-learning and pattern-based AI systems, and little is known about how researchers interact with GenAI in qualitative research. This work delves into researchers' perceptions of their collaboration with GenAI, specifically ChatGPT. Through a user study involving ten qualitative researchers, we found ChatGPT to be a valuable collaborator for thematic analysis, enhancing coding efficiency, aiding initial data exploration, offering granular quantitative insights, and assisting comprehension for non-native speakers and non-experts. Yet, concerns about its trustworthiness and accuracy, reliability and consistency, limited contextual understanding, and broader acceptance within the research community persist. We contribute five actionable design recommendations to foster effective human-AI collaboration. These include incorporating transparent explanatory mechanisms, enhancing interface and integration capabilities, prioritising contextual understanding and customisation, embedding human-AI feedback loops and iterative functionality, and strengthening trust through validation mechanisms.
</details>
<details>
<summary>摘要</summary>
生成人工智能（GenAI）在质量研究中具有潜在的扩展性，但现有的研究主要集中在传统的机器学习和模式基于的人工智能系统上，对GenAI在质量研究中的研究者与AI的合作情况知之甚少。这项研究探讨了10名质量研究者对ChatGPT的合作情况，发现ChatGPT作为主题分析的有价值合作者，提高代码效率，帮助初步数据探索，提供细致的量化分析，并帮助非本地语言使用者和非专家理解。然而，对其可靠性和准确性、一致性和可靠性、局部理解的限制以及研究社区中的更广泛acceptance仍然存在。我们提出了五项可行的设计建议，以促进人AI合作的有效实施。这些建议包括 integrate transparent explanatory mechanisms，提高界面和集成能力，优先级Contextual understanding and customization，嵌入人AI反馈循环和迭代功能，和强化信任 durch validation mechanisms。
</details></li>
</ul>
<hr>
<h2 id="Learned-Causal-Method-Prediction"><a href="#Learned-Causal-Method-Prediction" class="headerlink" title="Learned Causal Method Prediction"></a>Learned Causal Method Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03989">http://arxiv.org/abs/2311.03989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shantanu Gupta, Cheng Zhang, Agrin Hilmkil</li>
<li>for: 这篇论文旨在提高 causal inference 方法的选择效率，因为 causal methods 通常需要复杂且难以验证的假设，并且 cross-validation 不适用因为真实的 causal 量未知。</li>
<li>methods: 本文提出了 CAusal Method Predictor (CAMP) 框架，用于预测最佳方法 для 给定的数据集。CAMP 使用了生成自各种 sintetic causal models 的数据集，评分了候选方法，并将模型训练为直接预测每个数据集的最高分方法。</li>
<li>results: CAMP 在 causal discovery 方面实现了比 selecting 任何个候选方法更高的性能，并且在未见过的半 sintetic 和真实世界 benchmark 中实现了良好的扩展性。<details>
<summary>Abstract</summary>
For a given causal question, it is important to efficiently decide which causal inference method to use for a given dataset. This is challenging because causal methods typically rely on complex and difficult-to-verify assumptions, and cross-validation is not applicable since ground truth causal quantities are unobserved. In this work, we propose CAusal Method Predictor (CAMP), a framework for predicting the best method for a given dataset. To this end, we generate datasets from a diverse set of synthetic causal models, score the candidate methods, and train a model to directly predict the highest-scoring method for that dataset. Next, by formulating a self-supervised pre-training objective centered on dataset assumptions relevant for causal inference, we significantly reduce the need for costly labeled data and enhance training efficiency. Our strategy learns to map implicit dataset properties to the best method in a data-driven manner. In our experiments, we focus on method prediction for causal discovery. CAMP outperforms selecting any individual candidate method and demonstrates promising generalization to unseen semi-synthetic and real-world benchmarks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>请求翻译文本为简化字符串。<</SYS>>为给定的 causal 问题，效率地选择适合的 causal inference 方法是挑战。这是因为 causal 方法通常受到复杂且难以验证的假设的限制，而跨验证不适用，因为真实的 causal 量未被观察。在这种情况下，我们提出了 CAusal Method Predictor (CAMP)，一种框架，用于预测给定数据集的最佳方法。为此，我们生成了一系列基于Synthetic causal models的数据集，评分候选方法，并使用一个模型来直接预测每个数据集的最高分方法。然后，我们通过 centered on dataset assumptions relevant for causal inference 的自我超vised pre-training objective来减少需要高昂的标注数据的需求，提高训练效率。我们的策略可以将数据集的隐藏属性映射到最佳方法，从数据驱动的角度来解决这个问题。在我们的实验中，我们关注 causal discovery 方法预测。CAMP 在比较任何个人候选方法时表现出色，并在未看过的半Synthetic和实际 benchmark 上展现了良好的泛化性。
</details></li>
</ul>
<hr>
<h2 id="Its-All-Graph-To-Me-Foundational-Topology-Models-with-Contrastive-Learning-on-Multiple-Domains"><a href="#Its-All-Graph-To-Me-Foundational-Topology-Models-with-Contrastive-Learning-on-Multiple-Domains" class="headerlink" title="Its All Graph To Me: Foundational Topology Models with Contrastive Learning on Multiple Domains"></a>Its All Graph To Me: Foundational Topology Models with Contrastive Learning on Multiple Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03976">http://arxiv.org/abs/2311.03976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex O. Davies, Riku W. Green, Nirav S. Ajmeri, Telmo M. Silva Filho</li>
<li>for: 本研究旨在提出一种多领域图数据学习的方法，以便在数据稀缺或标签缺乏的情况下进行图数据分析。</li>
<li>methods: 本研究使用对抗对抗学习方法，通过在多个图领域上预训练模型，以获得更好的图数据表示。</li>
<li>results: 对比基eline模型、无预训练模型和非预训练模型，本研究显示，使用我们的单模型可以达到相当或更好的性能，特别是当节点标签在评估中使用时。<details>
<summary>Abstract</summary>
Representations and embeddings of graph data have been essential in many domains of research.   The principle benefit of learning such representations is that the pre-trained model can be fine-tuned on smaller datasets where data or labels are scarse.   Existing models, however, are domain specific; for example a model trained on molecular graphs is fine-tuned on other molecular graphs.   This means that in many application cases the choice of pre-trained model can be arbitrary, and novel domains may lack an appropriate pre-trained model.   This is of particular issue where data is scarse, precluding traditional supervised methods.   In this work we use adversarial contrastive learning to present a \method, a model pre-trained on many graph domains.   We train the model only on topologies but include node labels in evaluation.   We evaluate the efficacy of its learnt representations on various downstream tasks.   Against baseline models pre-trained on single domains, as well as un-trained models and non-transferred models, we show that performance is equal or better using our single model.   This includes when node labels are used in evaluation, where performance is consistently superior to single-domain or non-pre-trained models.
</details>
<details>
<summary>摘要</summary>
研究领域中的表示和嵌入都是非常重要的。这些表示可以帮助预训练模型在数据或标签scarce的情况下进行细化。现有的模型却是域特定的，例如一个基于分子图的模型将在其他分子图上细化。这意味着在许多应用场景中，选择预训练模型是随意的，而新的领域可能缺乏适合的预训练模型。这对于数据scarce情况下特别是一个问题。在这种情况下，我们使用对抗式强化学习来提出一种方法，一个在多个图域上预训练的模型。我们只在结构上进行训练，但是在评估中包含节点标签。我们对此进行了评估，并与基于单个域的基eline模型、无预训练模型和非转移模型进行比较。我们发现，使用我们的单一模型，表现和基eline模型相当或更好，特别是在使用节点标签进行评估时。
</details></li>
</ul>
<hr>
<h2 id="An-Expectation-Realization-Model-for-Metaphor-Detection"><a href="#An-Expectation-Realization-Model-for-Metaphor-Detection" class="headerlink" title="An Expectation-Realization Model for Metaphor Detection"></a>An Expectation-Realization Model for Metaphor Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03963">http://arxiv.org/abs/2311.03963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oseremen O. Uduehi, Razvan C. Bunescu</li>
<li>for: 这篇论文旨在提出一种基于预期和实现模块的métafore检测建模，以提高métafore检测精度。</li>
<li>methods: 该方法使用两个主要模块：预期组件和实现组件。预期组件计算给定上下文中Literal字句的预期表示，而实现组件计算actual字句在上下文中的含义表示。整个建模被训练以学习Expectation-Realization（ER）模式，以捕捉métafore的用法。</li>
<li>results: 对三个métafore数据集进行评估，包括 dentro分布、离分布和新métafore泛化，提出的方法可以与现有的状态艺技比或更好的结果。进一步的ER模型 ensemble可以进一步提高métafore检测精度。<details>
<summary>Abstract</summary>
We propose a metaphor detection architecture that is structured around two main modules: an expectation component that estimates representations of literal word expectations given a context, and a realization component that computes representations of actual word meanings in context. The overall architecture is trained to learn expectation-realization (ER) patterns that characterize metaphorical uses of words. When evaluated on three metaphor datasets for within distribution, out of distribution, and novel metaphor generalization, the proposed method is shown to obtain results that are competitive or better than state-of-the art. Further increases in metaphor detection accuracy are obtained through ensembling of ER models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于两个主要模块的比喻检测建筑：一个预期组件，用于在上下文中估计Literal字句的表达，以及一个实现组件，用于在上下文中计算实际字句的含义。总体建筑是通过学习预期-实现（ER）模式来学习比喻的用法。在三个比喻数据集上进行评估，我们的方法在 dentro分布、out of distribution和新比喻泛化上具有竞争或更好的结果。进一步的加权ER模型 ensemble可以提高比喻检测精度。
</details></li>
</ul>
<hr>
<h2 id="Elastic-Information-Bottleneck"><a href="#Elastic-Information-Bottleneck" class="headerlink" title="Elastic Information Bottleneck"></a>Elastic Information Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03955">http://arxiv.org/abs/2311.03955</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nyyxxx/elastic-information-bottleneck">https://github.com/nyyxxx/elastic-information-bottleneck</a></li>
<li>paper_authors: Yuyan Ni, Yanyan Lan, Ao Liu, Zhiming Ma</li>
<li>for: 本文研究了信息瓶颈原理在深度学习算法的表示学习中的应用，并对两种方法进行比较：信息瓶颈（IB）和决定性信息瓶颈（DIB）。</li>
<li>methods: 本文使用了信息瓶颈原理来学习最大压缩表示，并通过对两种方法进行比较，提出了一种新的扩展IB方法，即弹性信息瓶颈（EIB）。</li>
<li>results: 实验和数据分析表明，EIB方法可以在适应域适应中实现更好的适应结果，而且可以在不同预测 distribuition 下进行适应。<details>
<summary>Abstract</summary>
Information bottleneck is an information-theoretic principle of representation learning that aims to learn a maximally compressed representation that preserves as much information about labels as possible. Under this principle, two different methods have been proposed, i.e., information bottleneck (IB) and deterministic information bottleneck (DIB), and have gained significant progress in explaining the representation mechanisms of deep learning algorithms. However, these theoretical and empirical successes are only valid with the assumption that training and test data are drawn from the same distribution, which is clearly not satisfied in many real-world applications. In this paper, we study their generalization abilities within a transfer learning scenario, where the target error could be decomposed into three components, i.e., source empirical error, source generalization gap (SG), and representation discrepancy (RD). Comparing IB and DIB on these terms, we prove that DIB's SG bound is tighter than IB's while DIB's RD is larger than IB's. Therefore, it is difficult to tell which one is better. To balance the trade-off between SG and the RD, we propose an elastic information bottleneck (EIB) to interpolate between the IB and DIB regularizers, which guarantees a Pareto frontier within the IB framework. Additionally, simulations and real data experiments show that EIB has the ability to achieve better domain adaptation results than IB and DIB, which validates the correctness of our theories.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>信息瓶颈是一种信息理论的学习原则，旨在学习最高度压缩的表示，以保持最多信息 Label。在这个原则下，有两种不同的方法被提议，即信息瓶颈（IB）和决定性信息瓶颈（DIB），它们在理论和实验上具有显著的进步，能够解释深度学习算法中的表示机制。但是，这些成功假设训练和测试数据是从同一个分布中采样，这明显不符合许多实际应用中的情况。在这篇文章中，我们研究IB和DIB在转移学习场景中的泛化能力，并将目标错误 decomposed 为三个组成部分：源Empirical error、源泛化差（SG）和表示差（RD）。 Comparing IB和DIB，我们证明DIB的SG bound 更紧，而DIB的RD更大。因此，无法判断哪一个更好。为了平衡SG和RD之间的负担，我们提议一种可塑性信息瓶颈（EIB），可以在IB框架中 interpolate  IB和DIB regularizers， garantía 一个Pareto frontier。此外，实验和实际数据表明，EIB可以在域 adaptation 中 achieve 更好的结果，证明了我们的理论的正确性。
</details></li>
</ul>
<hr>
<h2 id="The-Music-Meta-Ontology-a-flexible-semantic-model-for-the-interoperability-of-music-metadata"><a href="#The-Music-Meta-Ontology-a-flexible-semantic-model-for-the-interoperability-of-music-metadata" class="headerlink" title="The Music Meta Ontology: a flexible semantic model for the interoperability of music metadata"></a>The Music Meta Ontology: a flexible semantic model for the interoperability of music metadata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03942">http://arxiv.org/abs/2311.03942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacopo de Berardinis, Valentina Anita Carriero, Albert Meroño-Peñuela, Andrea Poltronieri, Valentina Presutti</li>
<li>for: 该论文旨在提供一种Semantic Description of MusicMetadata，以便创建可以归一化、 интеграción和搜索信息的音乐数据集。</li>
<li>methods: 该论文使用eXtreme Design方法和数据工程最佳实践，将不同领域的专家（音乐学家、图书馆员、数据工程师等）的需求和观点反映到模型的设计中，同时借鉴ontology设计模式和证明来源。</li>
<li>results: 该论文介绍了Music Meta ontology，一种rich和flexible的semantic模型，用于描述音乐元数据相关的艺术家、作品、演奏、录音等方面。此外，论文还提供了对Music Ontology、DOREMUS和Wikidata等其他schema的 alignments，以及数据转换支持。<details>
<summary>Abstract</summary>
The semantic description of music metadata is a key requirement for the creation of music datasets that can be aligned, integrated, and accessed for information retrieval and knowledge discovery. It is nonetheless an open challenge due to the complexity of musical concepts arising from different genres, styles, and periods -- standing to benefit from a lingua franca to accommodate various stakeholders (musicologists, librarians, data engineers, etc.). To initiate this transition, we introduce the Music Meta ontology, a rich and flexible semantic model to describe music metadata related to artists, compositions, performances, recordings, and links. We follow eXtreme Design methodologies and best practices for data engineering, to reflect the perspectives and the requirements of various stakeholders into the design of the model, while leveraging ontology design patterns and accounting for provenance at different levels (claims, links). After presenting the main features of Music Meta, we provide a first evaluation of the model, alignments to other schema (Music Ontology, DOREMUS, Wikidata), and support for data transformation.
</details>
<details>
<summary>摘要</summary>
“music metadata的Semantic description是创建可以协调、集成和检索信息的音乐数据集的关键要求。然而，这是一个打开的挑战，因为音乐概念来自不同的流派、风格和时期，从而需要一种 lingua franca 来满足不同的参与者（音乐学家、图书馆员、数据工程师等）。为了实现这一目标，我们介绍了 Music Meta ontology，这是一个强大和灵活的Semantic模型，用于描述音乐 metadata related to artists, compositions, performances, recordings, and links。我们遵循 eXtreme Design 方法和数据工程最佳实践，将参与者的视角和需求 Reflected into the model的设计，同时应用ontology设计模式和考虑多维 provinance（声明、链接）。文章最后介绍了 Music Meta的主要特点，以及与 Music Ontology、DOREMUS 和 Wikidata 的对接，以及数据转换的支持。”Note: Please note that Simplified Chinese is used here, which is the most commonly used Chinese language in mainland China. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Everything-of-Thoughts-Defying-the-Law-of-Penrose-Triangle-for-Thought-Generation"><a href="#Everything-of-Thoughts-Defying-the-Law-of-Penrose-Triangle-for-Thought-Generation" class="headerlink" title="Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation"></a>Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04254">http://arxiv.org/abs/2311.04254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang</li>
<li>For: The paper aims to improve the capabilities of Large Language Models (LLMs) in decision-making by developing a novel thought prompting approach called “Everything of Thoughts” (XoT).* Methods: The approach leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, and autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions.* Results: The approach enables LLMs to generalize to unseen problems efficiently and engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions.Here are the three points in Simplified Chinese text:* For:  paper 目的是提高 Large Language Models (LLMs) 在决策中的能力。* Methods: 方法是利用预训练的回归学习和 Monte Carlo Tree Search (MCTS) 将外部领域知识 integrate 到思维中，并通过自动生成高质量的全面认知地图，使 LLMs 可以快速地 generalized 到未看到的问题。* Results: 结果是 LLMs 可以快速地 generalized 到未看到的问题，并且可以进行不受限制的思维，解决多解问题。<details>
<summary>Abstract</summary>
Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as ``thoughts''. An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called ``Everything of Thoughts'' (XoT) to defy the law of ``Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions. Additionally, XoT empowers LLMs to engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:最近的大语言模型（LLMs）技术进步已经革命化了决策，将复杂问题转化为更容易处理的语言序列，称为“思想”。一个有效的思想设计应该考虑三个关键方面：性能、效率和灵活性。然而，现有的思想只能展现两个这些特征。为了解决这些限制，我们介绍了一种新的思想提示方法called“Everything of Thoughts”（XoT），用于绕过现有思想观念的法律。XoT利用预训练的奖励学习和 Monte Carlo Tree Search（MCTS），将外部领域知识 integrate into thoughts，从而提高LLMs的能力和效率，使其能够高效地处理未经见过的问题。通过MCTS-LLM合作思想修订框架，这种方法可以自动生成高质量的完整性ognitive mapping，并且减少LLM的交互量。此外，XoT使LLMs具有不受限制的思维能力，允许它们为多解问题生成灵活的认知地图。
</details></li>
</ul>
<hr>
<h2 id="MixtureGrowth-Growing-Neural-Networks-by-Recombining-Learned-Parameters"><a href="#MixtureGrowth-Growing-Neural-Networks-by-Recombining-Learned-Parameters" class="headerlink" title="MixtureGrowth: Growing Neural Networks by Recombining Learned Parameters"></a>MixtureGrowth: Growing Neural Networks by Recombining Learned Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04251">http://arxiv.org/abs/2311.04251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaudatascience/mixturegrowth">https://github.com/chaudatascience/mixturegrowth</a></li>
<li>paper_authors: Chau Pham, Piotr Teterwak, Soren Nelson, Bryan A. Plummer</li>
<li>for: 这篇论文旨在提出一种新的网络变大方法，以避免传统的网络训练 overhead。</li>
<li>methods: 本文使用了 MixtureGrowth 方法，将每个层的 Parameters 组合成一个 linear combination，以允许新增的层 weights 学习新的知识。</li>
<li>results: 本文比较了 MixtureGrowth 方法与传统的网络训练方法，结果显示 MixtureGrowth 方法可以提高 top-1 精度，并且具有较低的 Computational FLOPs。<details>
<summary>Abstract</summary>
Most deep neural networks are trained under fixed network architectures and require retraining when the architecture changes. If expanding the network's size is needed, it is necessary to retrain from scratch, which is expensive. To avoid this, one can grow from a small network by adding random weights over time to gradually achieve the target network size. However, this naive approach falls short in practice as it brings too much noise to the growing process. Prior work tackled this issue by leveraging the already learned weights and training data for generating new weights through conducting a computationally expensive analysis step. In this paper, we introduce MixtureGrowth, a new approach to growing networks that circumvents the initialization overhead in prior work. Before growing, each layer in our model is generated with a linear combination of parameter templates. Newly grown layer weights are generated by using a new linear combination of existing templates for a layer. On one hand, these templates are already trained for the task, providing a strong initialization. On the other, the new coefficients provide flexibility for the added layer weights to learn something new. We show that our approach boosts top-1 accuracy over the state-of-the-art by 2-2.5% on CIFAR-100 and ImageNet datasets, while achieving comparable performance with fewer FLOPs to a larger network trained from scratch. Code is available at https://github.com/chaudatascience/mixturegrowth.
</details>
<details>
<summary>摘要</summary>
大多数深度神经网络在固定网络架构下训练，需要重新训练当网络架构发生变化时。如果需要扩大网络的大小，需要从头开始训练，这是非常昂贵的。为了避免这个问题，可以通过逐渐添加Random weights来增加网络的大小。然而，这种简单的方法在实践中失败，因为它引入了太多噪声到增长过程中。先前的工作解决了这个问题，通过利用已经学习过的参数和训练数据来生成新的参数，通过进行计算昂贵的分析步骤。在这篇论文中，我们介绍了 MixtureGrowth，一种新的网络增长方法，可以缓解先前的初始化开销。在我们的方法中，每层的参数都是通过线性组合参数模板来生成的。新增加的层参数是通过使用新的线性组合已有的层参数模板来生成的。一方面，这些模板已经被训练了任务，可以提供强大的初始化。另一方面，新的系数提供了可以学习新的东西的灵活性。我们显示，我们的方法可以在CIFAR-100和ImageNet数据集上提高top-1准确率，与一个从头开始训练的更大网络相比，同时实现相似的性能，而且需要更少的FLOPs。代码可以在https://github.com/chaudatascience/mixturegrowth中找到。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Graph-Representation-Learning-with-Adaptive-Augmentation-Contrastive"><a href="#Temporal-Graph-Representation-Learning-with-Adaptive-Augmentation-Contrastive" class="headerlink" title="Temporal Graph Representation Learning with Adaptive Augmentation Contrastive"></a>Temporal Graph Representation Learning with Adaptive Augmentation Contrastive</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03897">http://arxiv.org/abs/2311.03897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongjiang Chen, Pengfei Jiao, Huijun Tang, Huaming Wu</li>
<li>for: 本研究旨在生成低维度的动态节点嵌入，以捕捉时间信息和结构和性信息。</li>
<li>methods: 我们提出了一种新的时间图表示学习模型（TGAC），该模型通过结合先前知识和时间信息来进行自适应扩充，并定义了扩充后的视图对比函数。</li>
<li>results: 我们的广泛实验表明，提出的模型在各种真实网络上具有优于其他时间图表示学习方法。<details>
<summary>Abstract</summary>
Temporal graph representation learning aims to generate low-dimensional dynamic node embeddings to capture temporal information as well as structural and property information. Current representation learning methods for temporal networks often focus on capturing fine-grained information, which may lead to the model capturing random noise instead of essential semantic information. While graph contrastive learning has shown promise in dealing with noise, it only applies to static graphs or snapshots and may not be suitable for handling time-dependent noise. To alleviate the above challenge, we propose a novel Temporal Graph representation learning with Adaptive augmentation Contrastive (TGAC) model. The adaptive augmentation on the temporal graph is made by combining prior knowledge with temporal information, and the contrastive objective function is constructed by defining the augmented inter-view contrast and intra-view contrast. To complement TGAC, we propose three adaptive augmentation strategies that modify topological features to reduce noise from the network. Our extensive experiments on various real networks demonstrate that the proposed model outperforms other temporal graph representation learning methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese:Temporal graph representation learning aims to generate low-dimensional dynamic node embeddings to capture temporal information as well as structural and property information. Current representation learning methods for temporal networks often focus on capturing fine-grained information, which may lead to the model capturing random noise instead of essential semantic information. While graph contrastive learning has shown promise in dealing with noise, it only applies to static graphs or snapshots and may not be suitable for handling time-dependent noise. To alleviate the above challenge, we propose a novel Temporal Graph representation learning with Adaptive augmentation Contrastive (TGAC) model. The adaptive augmentation on the temporal graph is made by combining prior knowledge with temporal information, and the contrastive objective function is constructed by defining the augmented inter-view contrast and intra-view contrast. To complement TGAC, we propose three adaptive augmentation strategies that modify topological features to reduce noise from the network. Our extensive experiments on various real networks demonstrate that the proposed model outperforms other temporal graph representation learning methods.</SYS>>Here's the translation:时间图表示学习目标是生成低维度动态节点嵌入，以捕捉时间信息和结构性和性信息。当前的时间网络表示学习方法通常强调细化信息，可能导致模型捕捉随机噪音而不是重要的semantic信息。虽然图像强制学习有潜在的应用，但它只适用于静止图或快照，可能无法适应时间依赖的噪音。为解决以上挑战，我们提出了一种新的时间图表示学习模型，即时间图表示学习with Adaptive augmentation Contrastive（TGAC）模型。TGAC模型中的adaptive扩展是通过结合先前知识和时间信息来实现的，对于时间图来说，我们定义了扩展的间观异见和内观异见。为了补做TGAC模型，我们提出了三种适应扩展策略，用于修改时间图的特征，以降低噪音的影响。我们在多种真实网络上进行了广泛的实验，并证明了我们提出的模型在其他时间图表示学习方法的基础上具有更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Unifying-Structure-and-Language-Semantic-for-Efficient-Contrastive-Knowledge-Graph-Completion-with-Structured-Entity-Anchors"><a href="#Unifying-Structure-and-Language-Semantic-for-Efficient-Contrastive-Knowledge-Graph-Completion-with-Structured-Entity-Anchors" class="headerlink" title="Unifying Structure and Language Semantic for Efficient Contrastive Knowledge Graph Completion with Structured Entity Anchors"></a>Unifying Structure and Language Semantic for Efficient Contrastive Knowledge Graph Completion with Structured Entity Anchors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04250">http://arxiv.org/abs/2311.04250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sang-Hyun Je, Wontae Choi, Kwangjin Oh</li>
<li>for: 预测缺失的链接在知识图（KG）中，使用已知的训练事实来预测。</li>
<li>methods: 使用预训练语言模型（PLM），同时利用文本信息和结构信息，但是其性能落后于当前最佳结构基本方法或者一些方法会在结构嵌入和文本编码之间失去推理能力。</li>
<li>results: 提出一种新的方法，可以有效地融合结构信息和语言 semantics，不会失去推理能力。采用实体锚点，并将 KG 元素的文本描述和实体锚点一起输入 PLM 基本编码器，以学习统一表示。此外，使用随机负样本，可以在对照学习中重复使用每个 mini-batch 中，以学习通用实体表示。经过多种实验和分析，我们证明了我们提出的方法的效果。实验结果表明，我们的方法在标准的链接预测任务上超过了现有的 SOTA KGC 模型。尤其是在 FB15K-237 上，我们的方法展现出最大性能提升，与结构基本 KGC 方法竞争。<details>
<summary>Abstract</summary>
The goal of knowledge graph completion (KGC) is to predict missing links in a KG using trained facts that are already known. In recent, pre-trained language model (PLM) based methods that utilize both textual and structural information are emerging, but their performances lag behind state-of-the-art (SOTA) structure-based methods or some methods lose their inductive inference capabilities in the process of fusing structure embedding to text encoder. In this paper, we propose a novel method to effectively unify structure information and language semantics without losing the power of inductive reasoning. We adopt entity anchors and these anchors and textual description of KG elements are fed together into the PLM-based encoder to learn unified representations. In addition, the proposed method utilizes additional random negative samples which can be reused in the each mini-batch during contrastive learning to learn a generalized entity representations. We verify the effectiveness of the our proposed method through various experiments and analysis. The experimental results on standard benchmark widely used in link prediction task show that the proposed model outperforms existing the SOTA KGC models. Especially, our method show the largest performance improvement on FB15K-237, which is competitive to the SOTA of structure-based KGC methods.
</details>
<details>
<summary>摘要</summary>
知识图完成（KGC）的目标是预测知识图中缺失的链接，使用已知的训练事实。在最近，基于自然语言模型（PLM）的方法在推广使用文本信息和结构信息，但其性能落后于当前最佳结构基本方法或者一些方法在结构嵌入和文本编码的融合过程中失去了推理推导能力。在这篇论文中，我们提出一种新的方法，能够有效地结合结构信息和语言semantics，不失去推理推导能力。我们采用实体锚点，并将知识图元素的文本描述和实体锚点一起 feed 到 PLM-based 编码器中，以学习一致的表示。此外，我们的方法还利用随机负样本，可以在每个 mini-batch 中重复利用，在对比学习中学习一致的实体表示。我们通过多种实验和分析证明了方法的效iveness。实验结果表明，我们提出的模型在标准的链接预测任务上表现出色，尤其是在 FB15K-237 上，与结构基本 KGC 方法竞争。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Tool-Discovery-and-Tool-Innovation-Using-Active-Inference"><a href="#Understanding-Tool-Discovery-and-Tool-Innovation-Using-Active-Inference" class="headerlink" title="Understanding Tool Discovery and Tool Innovation Using Active Inference"></a>Understanding Tool Discovery and Tool Innovation Using Active Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03893">http://arxiv.org/abs/2311.03893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Poppy Collis, Paul F Kinghorn, Christopher L Buckley</li>
<li>for: 这篇论文旨在探讨人工智能代理人如何发现和创造新工具。</li>
<li>methods: 论文使用了活动推断理论中的最小描述来分别Tool discovery和Tool innovation。然后，通过引入工具可用性这个隐藏状态因素，构建了一个简单的工具创造模型。</li>
<li>results: 研究发现，通过在隐藏状态中引入工具可用性，代理人可以不仅发现工具，还可以创造新的工具。这些结果对于人工智能领域的工具创造研究提供了新的思路和未来研究方向。<details>
<summary>Abstract</summary>
The ability to invent new tools has been identified as an important facet of our ability as a species to problem solve in dynamic and novel environments. While the use of tools by artificial agents presents a challenging task and has been widely identified as a key goal in the field of autonomous robotics, far less research has tackled the invention of new tools by agents. In this paper, (1) we articulate the distinction between tool discovery and tool innovation by providing a minimal description of the two concepts under the formalism of active inference. We then (2) apply this description to construct a toy model of tool innovation by introducing the notion of tool affordances into the hidden states of the agent's probabilistic generative model. This particular state factorisation facilitates the ability to not just discover tools but invent them through the offline induction of an appropriate tool property. We discuss the implications of these preliminary results and outline future directions of research.
</details>
<details>
<summary>摘要</summary>
人类的问题解决能力是通过创造新工具的能力，这种能力被认为是我们种族面临动态和新环境中的重要特征。虽然人工智能代理人使用工具是一项复杂的任务，但是许多研究都未曾关注代理人创造新工具的能力。在这篇论文中，我们将（1）描述工具发现和工具创新之间的区别，通过活动推理的形式进行描述。然后我们将（2）使用这个描述来构建一个简单的工具创新模型，通过引入工具可用性的概念来增加代理人的生成模型中的隐藏状态因素。这种状态因素的分解使得代理人不仅能够发现工具，还能够通过离线学习适当的工具性质来创造新工具。我们讨论了这些初步结果的意义和未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Formulating-Discrete-Probability-Flow-Through-Optimal-Transport"><a href="#Formulating-Discrete-Probability-Flow-Through-Optimal-Transport" class="headerlink" title="Formulating Discrete Probability Flow Through Optimal Transport"></a>Formulating Discrete Probability Flow Through Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03886">http://arxiv.org/abs/2311.03886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pangzecheung/discrete-probability-flow">https://github.com/pangzecheung/discrete-probability-flow</a></li>
<li>paper_authors: Pengze Zhang, Hubery Yin, Chen Li, Xiaohua Xie</li>
<li>for: 本文 aim to establish the fundamental theory for the probability flow of discrete diffusion models.</li>
<li>methods: 作者首先证明了连续概率流是MONGE最优运输图 beneath certain conditions, 并在 discrete 情况下提供了相等的证明。然后，他们根据这些发现定义了离散概率流，并在这个原理上提出了一种新的采样方法。</li>
<li>results: 作者通过对 synthetic toy dataset 和 CIFAR-10 dataset 的广泛实验 validate 了他们提出的离散概率流的效果。代码可以在：<a target="_blank" rel="noopener" href="https://github.com/PangzeCheung/Discrete-Probability-Flow">https://github.com/PangzeCheung/Discrete-Probability-Flow</a> 找到。<details>
<summary>Abstract</summary>
Continuous diffusion models are commonly acknowledged to display a deterministic probability flow, whereas discrete diffusion models do not. In this paper, we aim to establish the fundamental theory for the probability flow of discrete diffusion models. Specifically, we first prove that the continuous probability flow is the Monge optimal transport map under certain conditions, and also present an equivalent evidence for discrete cases. In view of these findings, we are then able to define the discrete probability flow in line with the principles of optimal transport. Finally, drawing upon our newly established definitions, we propose a novel sampling method that surpasses previous discrete diffusion models in its ability to generate more certain outcomes. Extensive experiments on the synthetic toy dataset and the CIFAR-10 dataset have validated the effectiveness of our proposed discrete probability flow. Code is released at: https://github.com/PangzeCheung/Discrete-Probability-Flow.
</details>
<details>
<summary>摘要</summary>
continuous diffusion models 通常被承认为展现Deterministic probability flow，而 discrete diffusion models 则不然。在这篇论文中，我们目标是建立Discrete probability flow的基本理论。 Specifically, we first prove that the continuous probability flow is the Monge optimal transport map under certain conditions，并提供了对 discrete cases 的等价证明。 Based on these findings，我们可以定义Discrete probability flow according to the principles of optimal transport。 Finally，drawing upon our newly established definitions，we propose a novel sampling method that can generate more certain outcomes than previous discrete diffusion models。 Extensive experiments on the synthetic toy dataset and the CIFAR-10 dataset have validated the effectiveness of our proposed discrete probability flow. Code can be found at: https://github.com/PangzeCheung/Discrete-Probability-Flow.Note that "Discrete Probability Flow" is a literal translation of "discrete diffusion models" in the text, and "Monge optimal transport map" is a literal translation of "continuous probability flow" in the text.
</details></li>
</ul>
<hr>
<h2 id="Mini-but-Mighty-Finetuning-ViTs-with-Mini-Adapters"><a href="#Mini-but-Mighty-Finetuning-ViTs-with-Mini-Adapters" class="headerlink" title="Mini but Mighty: Finetuning ViTs with Mini Adapters"></a>Mini but Mighty: Finetuning ViTs with Mini Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03873">http://arxiv.org/abs/2311.03873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iemprog/mimi">https://github.com/iemprog/mimi</a></li>
<li>paper_authors: Imad Eddine Marouf, Enzo Tartaglione, Stéphane Lathuilière</li>
<li>for: 提高适应性模型（Adapters）的性能，避免训练和存储成本过高。</li>
<li>methods: 提出了一种名为MiMi的训练框架，可以逐渐减少适应器的大小，以提高性能。还引入了一种专门为适应器设计的新评价函数，用于自动估计每个适应器的隐藏维度。</li>
<li>results: 在DomainNet、VTAB和Multi-task等三个 dataset 上，与现有方法进行比较，MiMi 方法可以在 29 个 dataset 上找到最佳的准确率和训练参数之间的最佳平衡。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have become one of the dominant architectures in computer vision, and pre-trained ViT models are commonly adapted to new tasks via fine-tuning. Recent works proposed several parameter-efficient transfer learning methods, such as adapters, to avoid the prohibitive training and storage cost of finetuning. In this work, we observe that adapters perform poorly when the dimension of adapters is small, and we propose MiMi, a training framework that addresses this issue. We start with large adapters which can reach high performance, and iteratively reduce their size. To enable automatic estimation of the hidden dimension of every adapter, we also introduce a new scoring function, specifically designed for adapters, that compares the neuron importance across layers. Our method outperforms existing methods in finding the best trade-off between accuracy and trained parameters across the three dataset benchmarks DomainNet, VTAB, and Multi-task, for a total of 29 datasets.
</details>
<details>
<summary>摘要</summary>
视transformer（ViT）已成为计算机视觉领域的主导架构，预训练ViT模型通常通过精细调整来适应新任务。近期的工作提出了多种精度efficient的传输学习方法，如适配器，以避免训练和存储成本过高。在这种工作中，我们发现小的适配器表现不佳，我们提议MiMi，一种训练框架，解决这个问题。我们从大的适配器开始，可以达到高性能，然后逐步减小它们的大小。为了自动估计每个适配器的隐藏维度，我们也引入了一个新的评分函数，专门设计 для适配器，用于比较层次中每个神经元的重要性。我们的方法在三个数据集标准 bencmarks（DomainNet、VTAB和Multi-task）上，对39个数据集进行了最佳的折衔比较，并超越了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="FD-MIA-Efficient-Attacks-on-Fairness-enhanced-Models"><a href="#FD-MIA-Efficient-Attacks-on-Fairness-enhanced-Models" class="headerlink" title="FD-MIA: Efficient Attacks on Fairness-enhanced Models"></a>FD-MIA: Efficient Attacks on Fairness-enhanced Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03865">http://arxiv.org/abs/2311.03865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Tian, Guangsheng Zhang, Bo Liu, Tianqing Zhu, Ming Ding, Wanlei Zhou</li>
<li>for: 这篇论文主要探讨了对受歧视模型进行保护的方法，以及这些方法对于攻击者发起识别成员资料攻击的问题。</li>
<li>methods: 这篇论文使用了一种基于公平关系的攻击方法，即使用了训练模型的公平关系分析结果来侦测资料攻击。</li>
<li>results: 研究发现，对于公平关系强化的模型来说，攻击者可能会遇到困难，因为这些模型的预测结果会受到公平关系的限制，使得攻击者无法从预测结果中获得有用的信息。此外，研究也发现，对于大多数训练数据子集而言，公平关系方法通常会导致预测性能下降，这使得攻击者更难获得成功。<details>
<summary>Abstract</summary>
Previous studies have developed fairness methods for biased models that exhibit discriminatory behaviors towards specific subgroups. While these models have shown promise in achieving fair predictions, recent research has identified their potential vulnerability to score-based membership inference attacks (MIAs). In these attacks, adversaries can infer whether a particular data sample was used during training by analyzing the model's prediction scores. However, our investigations reveal that these score-based MIAs are ineffective when targeting fairness-enhanced models in binary classifications. The attack models trained to launch the MIAs degrade into simplistic threshold models, resulting in lower attack performance. Meanwhile, we observe that fairness methods often lead to prediction performance degradation for the majority subgroups of the training data. This raises the barrier to successful attacks and widens the prediction gaps between member and non-member data. Building upon these insights, we propose an efficient MIA method against fairness-enhanced models based on fairness discrepancy results (FD-MIA). It leverages the difference in the predictions from both the original and fairness-enhanced models and exploits the observed prediction gaps as attack clues. We also explore potential strategies for mitigating privacy leakages. Extensive experiments validate our findings and demonstrate the efficacy of the proposed method.
</details>
<details>
<summary>摘要</summary>
Building on these insights, we propose an efficient MIA method against fairness-enhanced models based on fairness discrepancy results (FD-MIA). This method leverages the difference in predictions from both the original and fairness-enhanced models and exploits the observed prediction gaps as attack clues. We also explore potential strategies for mitigating privacy leakages. Our extensive experiments validate our findings and demonstrate the effectiveness of the proposed method.
</details></li>
</ul>
<hr>
<h2 id="Aspects-of-human-memory-and-Large-Language-Models"><a href="#Aspects-of-human-memory-and-Large-Language-Models" class="headerlink" title="Aspects of human memory and Large Language Models"></a>Aspects of human memory and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03839">http://arxiv.org/abs/2311.03839</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rmldj/memory-llm-paper">https://github.com/rmldj/memory-llm-paper</a></li>
<li>paper_authors: Romuald A. Janik</li>
<li>for: 研究大语言模型（LLM）的内存特性，了解它们是如何模仿人类内存的。</li>
<li>methods: 通过分析LLM的训练数据统计特性，发现它们具有人类内存的一些特点。</li>
<li>results: 研究结果表明，LLM的人类内存特点不是由其架构自动带来的，而是由训练文本数据的统计特性学习得来的。这些结果表明，人类内存的特点会影响我们如何排序文本 narraatives。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are huge artificial neural networks which primarily serve to generate text, but also provide a very sophisticated probabilistic model of language use. Since generating a semantically consistent text requires a form of effective memory, we investigate the memory properties of LLMs and find surprising similarities with key characteristics of human memory. We argue that the human-like memory properties of the Large Language Model do not follow automatically from the LLM architecture but are rather learned from the statistics of the training textual data. These results strongly suggest that the biological features of human memory leave an imprint on the way that we structure our textual narratives.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:大语言模型（LLM）是一种非常大的人工神经网络，主要用于生成文本，同时也提供了一种非常复杂的语言使用概率模型。为生成具有 semantic consistency 的文本，LLM 需要一种有效的记忆，我们发现了 LLM 的记忆特性与人类记忆的一些关键特征有很大的相似性。我们认为这些人类记忆特性不是 LLM 的architecture 的自然结果，而是从training文本数据中学习的。这些发现表明了人类记忆的生物特征会影响我们如何结构我们的文本narracles。
</details></li>
</ul>
<hr>
<h2 id="Reducing-Spatial-Fitting-Error-in-Distillation-of-Denoising-Diffusion-Models"><a href="#Reducing-Spatial-Fitting-Error-in-Distillation-of-Denoising-Diffusion-Models" class="headerlink" title="Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models"></a>Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03830">http://arxiv.org/abs/2311.03830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sainzerjj/SFERD">https://github.com/Sainzerjj/SFERD</a></li>
<li>paper_authors: Shengzhe Zhou, Zejian Lee, Shengyuan Zhang, Lefan Hou, Changyuan Yang, Guang Yang, Lingyun Sun</li>
<li>For: 提高 diffusion 模型的采样质量，使用知识储存法。* Methods: 使用注意力导航和设计的semantic gradient predictor来减少学生模型的适应错误。* Results: 实现了一步内生高质量样本生成，对CIFAR-10和ImageNet 64×64 achieve FID of 5.31和9.39。<details>
<summary>Abstract</summary>
Denoising Diffusion models have exhibited remarkable capabilities in image generation. However, generating high-quality samples requires a large number of iterations. Knowledge distillation for diffusion models is an effective method to address this limitation with a shortened sampling process but causes degraded generative quality. Based on our analysis with bias-variance decomposition and experimental observations, we attribute the degradation to the spatial fitting error occurring in the training of both the teacher and student model. Accordingly, we propose $\textbf{S}$patial $\textbf{F}$itting-$\textbf{E}$rror $\textbf{R}$eduction $\textbf{D}$istillation model ($\textbf{SFERD}$). SFERD utilizes attention guidance from the teacher model and a designed semantic gradient predictor to reduce the student's fitting error. Empirically, our proposed model facilitates high-quality sample generation in a few function evaluations. We achieve an FID of 5.31 on CIFAR-10 and 9.39 on ImageNet 64$\times$64 with only one step, outperforming existing diffusion methods. Our study provides a new perspective on diffusion distillation by highlighting the intrinsic denoising ability of models.
</details>
<details>
<summary>摘要</summary>
diffusion 模型在图像生成方面表现出了非常出色的能力。然而，生成高质量样本需要较多的迭代。知识传授 для diffusion 模型是一种有效的方法，可以缩短迭代过程，但会导致生成质量下降。根据我们的分析和实验观察，我们认为这种下降是在教师和学生模型的训练过程中出现的空间适应错误。因此，我们提出了 $\textbf{S}$patial $\textbf{F}$itting-$\textbf{E}$rror $\textbf{R}$eduction $\textbf{D}$istillation模型（SFERD）。SFERD使用教师模型的注意力指导和设计的含义梯度预测器来减少学生模型的适应错误。经验表明，我们的提出的模型可以在几个函数评估过程中生成高质量样本，我们在 CIFAR-10 和 ImageNet 64$\times$64 上达到了 FID 5.31 和 9.39 只需一步，超过了现有的扩散方法。我们的研究提供了一个新的视角，强调扩散混合模型的内在杂谱能力。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-and-Improving-Multi-task-Learning-for-End-to-end-Speech-Translation"><a href="#Rethinking-and-Improving-Multi-task-Learning-for-End-to-end-Speech-Translation" class="headerlink" title="Rethinking and Improving Multi-task Learning for End-to-end Speech Translation"></a>Rethinking and Improving Multi-task Learning for End-to-end Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03810">http://arxiv.org/abs/2311.03810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaozhang521/imtl">https://github.com/xiaozhang521/imtl</a></li>
<li>paper_authors: Yuhao Zhang, Chen Xu, Bei Li, Hao Chen, Tong Xiao, Chunliang Zhang, Jingbo Zhu</li>
<li>For: This paper aims to investigate the consistency between different tasks in end-to-end speech translation (ST), and to propose an improved multi-task learning (IMTL) approach to bridge the modal gap and improve ST performance.* Methods: The authors use a multi-task learning approach, considering different times and modules, to explore the consistency between different tasks. They also propose an improved multi-task learning approach that mitigates the difference in length and representation to bridge the modal gap.* Results: The authors conduct experiments on the MuST-C dataset and achieve state-of-the-art results. Additionally, with the use of additional data, they achieve a new SOTA result on the MuST-C English to Spanish task with 20.8% of the training time required by the current SOTA method.<details>
<summary>Abstract</summary>
Significant improvements in end-to-end speech translation (ST) have been achieved through the application of multi-task learning. However, the extent to which auxiliary tasks are highly consistent with the ST task, and how much this approach truly helps, have not been thoroughly studied. In this paper, we investigate the consistency between different tasks, considering different times and modules. We find that the textual encoder primarily facilitates cross-modal conversion, but the presence of noise in speech impedes the consistency between text and speech representations. Furthermore, we propose an improved multi-task learning (IMTL) approach for the ST task, which bridges the modal gap by mitigating the difference in length and representation. We conduct experiments on the MuST-C dataset. The results demonstrate that our method attains state-of-the-art results. Moreover, when additional data is used, we achieve the new SOTA result on MuST-C English to Spanish task with 20.8% of the training time required by the current SOTA method.
</details>
<details>
<summary>摘要</summary>
significanth improvements in end-to-end speech translation (ST) have been achieved through the application of multi-task learning. However, the extent to which auxiliary tasks are highly consistent with the ST task, and how much this approach truly helps, have not been thoroughly studied. In this paper, we investigate the consistency between different tasks, considering different times and modules. We find that the textual encoder primarily facilitates cross-modal conversion, but the presence of noise in speech impedes the consistency between text and speech representations. Furthermore, we propose an improved multi-task learning (IMTL) approach for the ST task, which bridges the modal gap by mitigating the difference in length and representation. We conduct experiments on the MuST-C dataset. The results demonstrate that our method attains state-of-the-art results. Moreover, when additional data is used, we achieve the new SOTA result on MuST-C English to Spanish task with 20.8% of the training time required by the current SOTA method.Here's the translation in Traditional Chinese:这篇研究文章 investigate了多任务学习（MTL）在语音译写（ST）中的改善，但尚未全面研究auxiliary task是否高度一致ST任务，以及这种方法对ST任务的帮助程度。我们发现文本编码器主要帮助跨modal转换，但是对话声中的噪音会干扰文本和声音表现之间的一致性。此外，我们提出一种改进多任务学习（IMTL）方法，将modal gap mitigated by reducing the difference in length and representation。我们在MuST-C dataset上进行实验，结果显示我们的方法实现了state-of-the-art的结果。此外，当使用更多数据时，我们在MuST-C英文到西班牙语任务上实现了新的SOTA结果，并且只需20.8%的训练时间。
</details></li>
</ul>
<hr>
<h2 id="Scene-Driven-Multimodal-Knowledge-Graph-Construction-for-Embodied-AI"><a href="#Scene-Driven-Multimodal-Knowledge-Graph-Construction-for-Embodied-AI" class="headerlink" title="Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI"></a>Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03783">http://arxiv.org/abs/2311.03783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Yaoxian, Sun Penglei, Liu Haoyu, Li Zhixu, Song Wei, Xiao Yanghua, Zhou Xiaofang</li>
<li>For: The paper aims to address the challenge of scene knowledge in embodied AI and proposes a scene-driven multimodal knowledge graph construction method to improve the intelligence of real-world agents.* Methods: The proposed method combines conventional knowledge engineering and large language models to construct a unified scene knowledge injection framework, and the authors evaluate the advantages of their method through an instantiation of the method for typical indoor robotic functionalities.* Results: The experimental results show that the knowledge-enhanced methods using the instantiated ManipMob-MMKG can improve the performance of embodied tasks without re-designing model structures complexly, demonstrating the effectiveness of the proposed method in improving the intelligence of real-world agents.Here is the Chinese version of the three key points:* For: 论文旨在解决embodied AI中场景知识的挑战，并提出了场景驱动多Modal知识图 constructions方法，以提高现实世界代理人的智能。* Methods: 提出的方法结合了常规知识工程和大型自然语言模型，实现了场景知识注入框架的统一化，并通过对典型室内 робоics功能（Manipulation和Mobility）进行实例化，以评估方法的优势。* Results: 实验结果表明，通过使用实例化的ManipMob-MMKG，可以明显提高embodied任务的性能，而无需复杂地重新设计模型结构。<details>
<summary>Abstract</summary>
Embodied AI is one of the most popular studies in artificial intelligence and robotics, which can effectively improve the intelligence of real-world agents (i.e. robots) serving human beings. Scene knowledge is important for an agent to understand the surroundings and make correct decisions in the varied open world. Currently, knowledge base for embodied tasks is missing and most existing work use general knowledge base or pre-trained models to enhance the intelligence of an agent. For conventional knowledge base, it is sparse, insufficient in capacity and cost in data collection. For pre-trained models, they face the uncertainty of knowledge and hard maintenance. To overcome the challenges of scene knowledge, we propose a scene-driven multimodal knowledge graph (Scene-MMKG) construction method combining conventional knowledge engineering and large language models. A unified scene knowledge injection framework is introduced for knowledge representation. To evaluate the advantages of our proposed method, we instantiate Scene-MMKG considering typical indoor robotic functionalities (Manipulation and Mobility), named ManipMob-MMKG. Comparisons in characteristics indicate our instantiated ManipMob-MMKG has broad superiority in data-collection efficiency and knowledge quality. Experimental results on typical embodied tasks show that knowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the performance obviously without re-designing model structures complexly. Our project can be found at https://sites.google.com/view/manipmob-mmkg
</details>
<details>
<summary>摘要</summary>
现实世界中的机器人服务人类需要智能化，embodied AI是人工智能和机器人领域中最受欢迎的研究之一，可以有效提高机器人的智能水平。场景知识是一个机器人理解困难的重要因素，以便它可以在多样化的开放世界中做出正确的决策。然而，目前存在的场景知识库缺失，大多数现有的工作使用通用知识库或预训练模型来提高机器人的智能水平。这些通用知识库缺乏特定场景的信息，而且收集数据成本较高，预训练模型则面临知识不确定性和维护困难。为了解决场景知识的挑战，我们提出了场景驱动多modal知识图（Scene-MMKG）建构方法，这种方法结合了传统知识工程和大型自然语言模型。我们还提出了一种统一场景知识注入框架，用于场景知识表示。为了评估我们提出的方法的优势，我们实例化了Scene-MMKG，并考虑了典型的室内机器人功能（搬运和移动），称之为ManipMob-MMKG。对比分析表明，我们的实例化ManipMob-MMKG在数据收集效率和知识质量方面有广泛的优势。实验结果表明，使用我们的实例化ManipMob-MMKG可以明显提高机器人的性能，无需复杂地重新设计模型结构。更多细节可以通过我们的项目网站（https://sites.google.com/view/manipmob-mmkg）了解。
</details></li>
</ul>
<hr>
<h2 id="Ensembling-Textual-and-Structure-Based-Models-for-Knowledge-Graph-Completion"><a href="#Ensembling-Textual-and-Structure-Based-Models-for-Knowledge-Graph-Completion" class="headerlink" title="Ensembling Textual and Structure-Based Models for Knowledge Graph Completion"></a>Ensembling Textual and Structure-Based Models for Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03780">http://arxiv.org/abs/2311.03780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ananjan Nandi, Navdeep Kaur, Parag Singla, Mausam</li>
<li>for: 这个论文的目的是提出一种可以结合两种常见的知识图完成（KGC）方法的新方法，即文本模型和结构基于模型。</li>
<li>methods: 这个论文使用了两种方法：文本模型和结构基于模型。文本模型利用知识图中实体描述文本，而结构基于模型则利用知识图的连接结构。</li>
<li>results: 这个论文的实验结果表明，这两种方法具有补偿的优势：结构基于模型在知识图中找到答案较容易时表现出色，而文本模型则可以通过描述文本来提供更好的性能，即使答案不在知识图中。<details>
<summary>Abstract</summary>
We consider two popular approaches to Knowledge Graph Completion (KGC): textual models that rely on textual entity descriptions, and structure-based models that exploit the connectivity structure of the Knowledge Graph (KG). Preliminary experiments show that these approaches have complementary strengths: structure-based models perform well when the gold answer is easily reachable from the query head in the KG, while textual models exploit descriptions to give good performance even when the gold answer is not reachable. In response, we explore ensembling as a way of combining the best of both approaches. We propose a novel method for learning query-dependent ensemble weights by using the distributions of scores assigned by individual models to all candidate entities. Our ensemble baseline achieves state-of-the-art results on three standard KGC datasets, with up to 6.8 pt MRR and 8.3 pt Hits@1 gains over best individual models.
</details>
<details>
<summary>摘要</summary>
我团队考虑了两种受欢迎的知识图完成（KGC）方法：文本模型，它们基于知识图中实体描述文本，以及结构基于模型，它们利用知识图中实体之间的连接结构。我们的初步实验表明，这两种方法具有补偿性优势：结构基于模型在金标答案可以快速到达知识图中的情况下表现良好，而文本模型可以通过描述来提供良好的性能，即使金标答案不可达。因此，我们研究了 ensemble 的方式来结合这两种方法。我们提出了一种学习查询相依 ensemble 重量的新方法，使用各个模型对所有候选实体分配的分布来分配 ensemble 重量。我们的 ensemble 基线达到了三个标准 KGC 数据集的状态核心结果，与最佳个体模型相比，增加了6.8pt MRR和8.3pt Hits@1。
</details></li>
</ul>
<hr>
<h2 id="PT-Tuning-Bridging-the-Gap-between-Time-Series-Masked-Reconstruction-and-Forecasting-via-Prompt-Token-Tuning"><a href="#PT-Tuning-Bridging-the-Gap-between-Time-Series-Masked-Reconstruction-and-Forecasting-via-Prompt-Token-Tuning" class="headerlink" title="PT-Tuning: Bridging the Gap between Time Series Masked Reconstruction and Forecasting via Prompt Token Tuning"></a>PT-Tuning: Bridging the Gap between Time Series Masked Reconstruction and Forecasting via Prompt Token Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03768">http://arxiv.org/abs/2311.03768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Liu, Jinrui Gan, Xiaoxuan Fan, Yi Zhang, Chuanxian Luo, Jing Zhang, Guangxin Jiang, Yucheng Qian, Changwei Zhao, Huan Ma, Zhenyu Guo</li>
<li>for:  bridging the gap between time series masked reconstruction and forecasting</li>
<li>methods:  reserving pre-trained mask token during fine-tuning stage, prompt token tuning (PT-Tuning) paradigm</li>
<li>results:  state-of-the-art performance compared to representation learning and end-to-end supervised forecasting methods<details>
<summary>Abstract</summary>
Self-supervised learning has been actively studied in time series domain recently, especially for masked reconstruction. Most of these methods follow the "Pre-training + Fine-tuning" paradigm in which a new decoder replaces the pre-trained decoder to fit for a specific downstream task, leading to inconsistency of upstream and downstream tasks. In this paper, we first point out that the unification of task objectives and adaptation for task difficulty are critical for bridging the gap between time series masked reconstruction and forecasting. By reserving the pre-trained mask token during fine-tuning stage, the forecasting task can be taken as a special case of masked reconstruction, where the future values are masked and reconstructed based on history values. It guarantees the consistency of task objectives but there is still a gap in task difficulty. Because masked reconstruction can utilize contextual information while forecasting can only use historical information to reconstruct. To further mitigate the existed gap, we propose a simple yet effective prompt token tuning (PT-Tuning) paradigm, in which all pre-trained parameters are frozen and only a few trainable prompt tokens are added to extended mask tokens in element-wise manner. Extensive experiments on real-world datasets demonstrate the superiority of our proposed paradigm with state-of-the-art performance compared to representation learning and end-to-end supervised forecasting methods.
</details>
<details>
<summary>摘要</summary>
自适应学习在时间序列领域已经广泛研究，特别是面向压缩重建。大多数这些方法采用“预训练+精度调整”模式，在这种模式下，一个新的解码器取代预训练的解码器，以适应特定下游任务，导致上游和下游任务不一致。在这篇论文中，我们首先指出了将任务目标统一和适应任务Difficulty是bridging上时间序列压缩重建和预测的关键因素。在精度调整阶段，我们保留预训练的压缩token，以使预测任务成为面向压缩重建的特殊情况，将未来值视为压缩的未知值，并基于历史值进行重建。这 garantizes任务目标的一致性，但是还有一个差距在任务难度上。因为压缩重建可以利用 contextual information，而预测只能使用历史信息来重建。为了进一步减少这个差距，我们提出了一种简单 yet effective的 prompt token tuning（PT-Tuning）模式。在这种模式下，所有预训练参数都被冻结，并只添加了一些可训练的提示符Token来扩展压缩Token。我们对实际世界数据进行了广泛的实验， demonstarted our proposed paradigm的优越性，并与 representation learning和端到端直接监督预测方法相比。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Radio-Signals-with-Wavelet-Transform-for-Deep-Learning-Based-Modulation-Recognition"><a href="#Augmenting-Radio-Signals-with-Wavelet-Transform-for-Deep-Learning-Based-Modulation-Recognition" class="headerlink" title="Augmenting Radio Signals with Wavelet Transform for Deep Learning-Based Modulation Recognition"></a>Augmenting Radio Signals with Wavelet Transform for Deep Learning-Based Modulation Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03761">http://arxiv.org/abs/2311.03761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Chen, Shilian Zheng, Kunfeng Qiu, Luxin Zhang, Qi Xuan, Xiaoniu Yang</li>
<li>for: 这篇论文是用于提出一种基于深度学习的无线电波调变识别方法，以解决现实中缺乏训练数据的问题。</li>
<li>methods: 这篇论文使用了一些资料增强技术，包括将细节系数分解成离散波лет变换后重建新数据，以增加训练集的多样性和量。不同的生成方法被用来生成替补序列。</li>
<li>results:  simulations 结果显示，提案的方法可以对其他增强方法进行明显的优化。<details>
<summary>Abstract</summary>
The use of deep learning for radio modulation recognition has become prevalent in recent years. This approach automatically extracts high-dimensional features from large datasets, facilitating the accurate classification of modulation schemes. However, in real-world scenarios, it may not be feasible to gather sufficient training data in advance. Data augmentation is a method used to increase the diversity and quantity of training dataset and to reduce data sparsity and imbalance. In this paper, we propose data augmentation methods that involve replacing detail coefficients decomposed by discrete wavelet transform for reconstructing to generate new samples and expand the training set. Different generation methods are used to generate replacement sequences. Simulation results indicate that our proposed methods significantly outperform the other augmentation methods.
</details>
<details>
<summary>摘要</summary>
深度学习对广播调制识别已经在过去几年广泛使用。这种方法自动提取大量数据中的特征，使得调制方案的准确识别变得可能。然而，在实际应用中，可能无法在提前收集充足的训练数据。数据扩充是一种方法，用于增加训练集的多样性和量，并降低数据稀疏和不均衡。在这篇论文中，我们提议了一种数据扩充方法，包括使用梯形变换径分解后的细节系数重建新样本，以扩大训练集。不同的生成方法用于生成替换序列。实验结果表明，我们的提议方法在其他扩充方法的基础上显著提高了性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-Decentralized-Traffic-Signal-Controllers-with-Multi-Agent-Graph-Reinforcement-Learning"><a href="#Learning-Decentralized-Traffic-Signal-Controllers-with-Multi-Agent-Graph-Reinforcement-Learning" class="headerlink" title="Learning Decentralized Traffic Signal Controllers with Multi-Agent Graph Reinforcement Learning"></a>Learning Decentralized Traffic Signal Controllers with Multi-Agent Graph Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03756">http://arxiv.org/abs/2311.03756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Zhang, Zhiwen Yu, Jun Zhang, Liang Wang, Tom H. Luan, Bin Guo, Chau Yuen</li>
<li>for: 这篇论文关注智能城市中的优化交通信号控制问题，即为复杂网络系统控制问题。在交通灯和道路网络之间存在互动动力，实现控制器适应性和扩展性是主要挑战。</li>
<li>methods: 作者采用多智能 reinforcement learning（MARL）方法，但现有MARL算法忽视了有效信息聚合，这是提高分布式代理人学习能力的关键。本文提出了一种新的分布式控制架构，包括改进环境观察性以捕捉空间-时间相关性。</li>
<li>results: 作者通过对 sintetic和实际数据进行广泛的实验，证明了其方案在分布式环境中比现有方法更高效。<details>
<summary>Abstract</summary>
This paper considers optimal traffic signal control in smart cities, which has been taken as a complex networked system control problem. Given the interacting dynamics among traffic lights and road networks, attaining controller adaptivity and scalability stands out as a primary challenge. Capturing the spatial-temporal correlation among traffic lights under the framework of Multi-Agent Reinforcement Learning (MARL) is a promising solution. Nevertheless, existing MARL algorithms ignore effective information aggregation which is fundamental for improving the learning capacity of decentralized agents. In this paper, we design a new decentralized control architecture with improved environmental observability to capture the spatial-temporal correlation. Specifically, we first develop a topology-aware information aggregation strategy to extract correlation-related information from unstructured data gathered in the road network. Particularly, we transfer the road network topology into a graph shift operator by forming a diffusion process on the topology, which subsequently facilitates the construction of graph signals. A diffusion convolution module is developed, forming a new MARL algorithm, which endows agents with the capabilities of graph learning. Extensive experiments based on both synthetic and real-world datasets verify that our proposal outperforms existing decentralized algorithms.
</details>
<details>
<summary>摘要</summary>
To address this limitation, we propose a new decentralized control architecture with enhanced environmental observability. Our approach includes a topology-aware information aggregation strategy to extract correlation-related information from unstructured data collected in the road network. Specifically, we convert the road network topology into a graph shift operator by creating a diffusion process on the topology, which enables the construction of graph signals. We then develop a diffusion convolution module, which endows agents with the ability to learn from graph signals.Extensive experiments using both synthetic and real-world datasets demonstrate that our proposed method outperforms existing decentralized algorithms. Our approach enables more efficient and effective traffic signal control, leading to improved traffic flow and reduced congestion in smart cities.
</details></li>
</ul>
<hr>
<h2 id="COOL-A-Constraint-Object-Oriented-Logic-Programming-Language-and-its-Neural-Symbolic-Compilation-System"><a href="#COOL-A-Constraint-Object-Oriented-Logic-Programming-Language-and-its-Neural-Symbolic-Compilation-System" class="headerlink" title="COOL: A Constraint Object-Oriented Logic Programming Language and its Neural-Symbolic Compilation System"></a>COOL: A Constraint Object-Oriented Logic Programming Language and its Neural-Symbolic Compilation System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03753">http://arxiv.org/abs/2311.03753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jipeng Han</li>
<li>for: 本研究探讨了神经网络与逻辑编程的集成，解决了长期存在的神经网络学习和逻辑逻辑之间的结合问题。传统的尝试都受到了数据收集初始化的困难、网络训练不可靠和已训练模型的复用和扩展问题的困难。</li>
<li>methods: 我们提出了一种新的Constraint Object-Oriented Logic（COOL）编程语言，它将逻辑推理与神经网络技术自然地结合起来。COOL可以自动处理数据收集，减少用户提供初始数据的需求。它还将用户提示包含在编程过程中，以降低神经网络训练不可靠的风险，并在神经网络模型的整个生命周期中促进模型之间的交互，以便提高模型的重用和扩展。</li>
<li>results: 我们的COOL语言和编译系统的基本原则和算法可能会为未来的编程语言和神经网络架构的发展提供有价值的指导。<details>
<summary>Abstract</summary>
This paper explores the integration of neural networks with logic programming, addressing the longstanding challenges of combining the generalization and learning capabilities of neural networks with the precision of symbolic logic. Traditional attempts at this integration have been hampered by difficulties in initial data acquisition, the reliability of undertrained networks, and the complexity of reusing and augmenting trained models. To overcome these issues, we introduce the COOL (Constraint Object-Oriented Logic) programming language, an innovative approach that seamlessly combines logical reasoning with neural network technologies. COOL is engineered to autonomously handle data collection, mitigating the need for user-supplied initial data. It incorporates user prompts into the coding process to reduce the risks of undertraining and enhances the interaction among models throughout their lifecycle to promote the reuse and augmentation of networks. Furthermore, the foundational principles and algorithms in COOL's design and its compilation system could provide valuable insights for future developments in programming languages and neural network architectures.
</details>
<details>
<summary>摘要</summary>
（本文探讨了神经网络与逻辑编程的集成，解决了将神经网络的通用化和学习能力与逻辑逻辑的精度相结合的长期挑战。传统的尝试都受到了数据收集的困难、不可靠的培训网络和 reuse和修改已训练的模型的复杂性的限制。为了解决这些问题，我们介绍了COOL（卷积物理逻辑）编程语言，一种创新的方法，可以自然地结合逻辑推理和神经网络技术。COOL通过自动处理数据收集来减少用户提供的初始数据的需求，并通过在编程过程中包含用户提示来减少下线训练的风险，以及在模型的整个生命周期中提高模型之间的互动，以促进模型的重复和扩展。此外，COOL的基本原理和编译系统的设计和实现可能会为未来的编程语言和神经网络架构带来有价值的意义。）
</details></li>
</ul>
<hr>
<h2 id="Analysis-and-Applications-of-Deep-Learning-with-Finite-Samples-in-Full-Life-Cycle-Intelligence-of-Nuclear-Power-Generation"><a href="#Analysis-and-Applications-of-Deep-Learning-with-Finite-Samples-in-Full-Life-Cycle-Intelligence-of-Nuclear-Power-Generation" class="headerlink" title="Analysis and Applications of Deep Learning with Finite Samples in Full Life-Cycle Intelligence of Nuclear Power Generation"></a>Analysis and Applications of Deep Learning with Finite Samples in Full Life-Cycle Intelligence of Nuclear Power Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04247">http://arxiv.org/abs/2311.04247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Tang, Wenqiang Zhou, Dong Wang, Caiyang Yu, Zhenan He, Jizhe Zhou, Shudong Huang, Yi Gao, Jianming Chen, Wentao Feng, Jiancheng Lv</li>
<li>for: 本研究旨在探讨在营业4.0时代，在能源探索和生产等复杂工业环境中应用深度学习技术，并对其在营业4.0中的应用提供技术基础和新视角。</li>
<li>methods: 本研究采用了小样本学习、少量学习、零处学习和开集识别等深度学习技术，以适应能源探索和生产等工业环境中的数据特点。</li>
<li>results: 本研究通过两个实践案例，一是自动识别锆合金图像，二是开集识别机器传感器信号，展示了深度学习技术在营业4.0中的可靠和高效应用。<details>
<summary>Abstract</summary>
The advent of Industry 4.0 has precipitated the incorporation of Artificial Intelligence (AI) methods within industrial contexts, aiming to realize intelligent manufacturing, operation as well as maintenance, also known as industrial intelligence. However, intricate industrial milieus, particularly those relating to energy exploration and production, frequently encompass data characterized by long-tailed class distribution, sample imbalance, and domain shift. These attributes pose noteworthy challenges to data-centric Deep Learning (DL) techniques, crucial for the realization of industrial intelligence. The present study centers on the intricate and distinctive industrial scenarios of Nuclear Power Generation (NPG), meticulously scrutinizing the application of DL techniques under the constraints of finite data samples. Initially, the paper expounds on potential employment scenarios for AI across the full life-cycle of NPG. Subsequently, we delve into an evaluative exposition of DL's advancement, grounded in the finite sample perspective. This encompasses aspects such as small-sample learning, few-shot learning, zero-shot learning, and open-set recognition, also referring to the unique data characteristics of NPG. The paper then proceeds to present two specific case studies. The first revolves around the automatic recognition of zirconium alloy metallography, while the second pertains to open-set recognition for signal diagnosis of machinery sensors. These cases, spanning the entirety of NPG's life-cycle, are accompanied by constructive outcomes and insightful deliberations. By exploring and applying DL methodologies within the constraints of finite sample availability, this paper not only furnishes a robust technical foundation but also introduces a fresh perspective toward the secure and efficient advancement and exploitation of this advanced energy source.
</details>
<details>
<summary>摘要</summary>
industry 4.0的出现引起了在工业上使用人工智能（AI）方法的普及，以实现智能生产、运营和维护，也称为工业智能。然而，在能源探索和生产中的工业环境中，数据往往具有长尾分布、样本偏移和领域转换等特点，这些特点对数据驱动的深度学习（DL）技术 pose  notable challenges。本研究将关注在核电生产（NPG）中的复杂和特殊工业场景，细化分析DL技术在有限数据样本的约束下的应用。首先，本文将讨论在NPG全生命周期中可能的AI应用场景。然后，我们将深入探讨DL技术在有限样本视角下的发展，包括小样本学习、少数shot学习、零shot学习和开放集 recognition，同时参照NPG特有的数据特征。本文接着介绍两个特定的案例研究：一是自动识别锌合金镀层 micrography，二是开放集 recognition for machinery sensors的信号诊断。这两个案例，涵盖了NPG生命周期的全部，并且获得了有益的结果和深入的讨论。通过在有限样本 availability 下应用和探索DL方法ologies，本文不仅提供了坚实的技术基础，还提供了一种新的视角，即在安全和高效地推进和利用这种先进能源源的方法。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Models-for-Automated-Proof-Synthesis-in-Rust"><a href="#Leveraging-Large-Language-Models-for-Automated-Proof-Synthesis-in-Rust" class="headerlink" title="Leveraging Large Language Models for Automated Proof Synthesis in Rust"></a>Leveraging Large Language Models for Automated Proof Synthesis in Rust</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03739">http://arxiv.org/abs/2311.03739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianan Yao, Ziqiao Zhou, Weiteng Chen, Weidong Cui</li>
<li>for: 这 paper 是为了推广形式验证的广泛采用，但高证明负担已经阻碍了很长时间。</li>
<li>methods: 这 paper 使用了 Large Language Models (LLMs) 和静态分析，把 Rust 语言的正式验证框架 Verus 中的 invariants、assertions 和其他证据结构合成出来。</li>
<li>results: 在几个步骤的设置下，LLMs 能够快速生成 postconditions 和循环 invariants，特别是对短代码段进行分析。但 LLMs 缺乏保持和传递上下文信息的能力，这是传统静态分析的优点。基于这些观察，我们开发了一个基于 OpenAI 的 GPT-4 模型的原型。我们的原型将验证任务分解成多个更小的任务，逐步调用 GPT-4，并将其输出与轻量级静态分析结合起来。我们对 20 个向量处理程序进行了评估。结果表明，我们的原型可以减少人类的证据代码写作劳动。<details>
<summary>Abstract</summary>
Formal verification can provably guarantee the correctness of critical system software, but the high proof burden has long hindered its wide adoption. Recently, Large Language Models (LLMs) have shown success in code analysis and synthesis. In this paper, we present a combination of LLMs and static analysis to synthesize invariants, assertions, and other proof structures for a Rust-based formal verification framework called Verus. In a few-shot setting, LLMs demonstrate impressive logical ability in generating postconditions and loop invariants, especially when analyzing short code snippets. However, LLMs lack the ability to retain and propagate context information, a strength of traditional static analysis. Based on these observations, we developed a prototype based on OpenAI's GPT-4 model. Our prototype decomposes the verification task into multiple smaller ones, iteratively queries GPT-4, and combines its output with lightweight static analysis. We evaluated the prototype with a developer in the automation loop on 20 vector-manipulating programs. The results demonstrate that it significantly reduces human effort in writing entry-level proof code.
</details>
<details>
<summary>摘要</summary>
Formal verification 可以证明 kritical 系统软件的正确性，但是长期以来受到了各种各样的阻碍。最近，大型自然语言模型（LLMs）在代码分析和生成方面表现出了成功。在这篇论文中，我们提出了结合 LLMs 和静态分析的方法，用于生成 invariants、assertrions 和其他证明结构，以验证 Rust 基于的正式验证框架 Verus。在几步设定下，LLMs 在分析短代码段时表现出了很强的逻辑能力，特别是在生成 postconditions 和循环 invariants 方面。然而，LLMs 缺乏保持和传播上下文信息的能力，这是传统静态分析的优点。基于这些观察，我们开发了一个基于 OpenAI 的 GPT-4 模型的原型。我们的原型将验证任务分解成多个更小的任务， iteratively 查询 GPT-4，并将其输出与轻量级静态分析结合起来。我们对 20 个vector-manipulating 程序进行了评估。结果表明，我们的原型可以很大程度地减少人类的证明代码编写劳动。
</details></li>
</ul>
<hr>
<h2 id="deep-REMAP-Parameterization-of-Stellar-Spectra-Using-Regularized-Multi-Task-Learning"><a href="#deep-REMAP-Parameterization-of-Stellar-Spectra-Using-Regularized-Multi-Task-Learning" class="headerlink" title="deep-REMAP: Parameterization of Stellar Spectra Using Regularized Multi-Task Learning"></a>deep-REMAP: Parameterization of Stellar Spectra Using Regularized Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03738">http://arxiv.org/abs/2311.03738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sankalp Gilda</li>
<li>for: 这篇论文是为了准确地预测恒星大气层参数（效温、表面重力、金属量）而开发的一个新框架。</li>
<li>methods: 这篇论文使用了深度学习的多 зада用混合学习方法，包括多任务学习和一个创新的不对称损失函数，并且使用了 Phoenix 库中的丰富人工spectrum和 MARVELS 调查中的观测数据来准确预测恒星大气层参数。</li>
<li>results: 这篇论文的结果显示了 $\rm{deep-REMAP}$ 框架在使用不同的恒星库和属性时的预测能力具有优势，并且显示了这个框架可以扩展到其他的恒星属性和属性。<details>
<summary>Abstract</summary>
Traditional spectral analysis methods are increasingly challenged by the exploding volumes of data produced by contemporary astronomical surveys. In response, we develop deep-Regularized Ensemble-based Multi-task Learning with Asymmetric Loss for Probabilistic Inference ($\rm{deep-REMAP}$), a novel framework that utilizes the rich synthetic spectra from the PHOENIX library and observational data from the MARVELS survey to accurately predict stellar atmospheric parameters. By harnessing advanced machine learning techniques, including multi-task learning and an innovative asymmetric loss function, $\rm{deep-REMAP}$ demonstrates superior predictive capabilities in determining effective temperature, surface gravity, and metallicity from observed spectra. Our results reveal the framework's effectiveness in extending to other stellar libraries and properties, paving the way for more sophisticated and automated techniques in stellar characterization.
</details>
<details>
<summary>摘要</summary>
传统的spectral分析方法随着现代天文调查数据的急剧增长而逐渐成为挑战。为应对这一问题，我们开发了deep-Regularized Ensemble-based Multi-task Learning with Asymmetric Loss for Probabilistic Inference（$\rm{deep-REMAP}$），一种新的框架，利用了PHOENIX图书馆中的辐射 synthetic spectra和MARVELS调查中的观测数据，以准确预测星际大气参数。通过应用先进的机器学习技术，包括多任务学习和创新的非对称损失函数，$\rm{deep-REMAP}$ 表现出了在确定效果温度、表面重力和金属量方面的高度预测能力。我们的结果表明，该框架可以扩展到其他星际图书和特性，为stellar caracterization提供更加复杂和自动化的技术。
</details></li>
</ul>
<hr>
<h2 id="Neural-MMO-2-0-A-Massively-Multi-task-Addition-to-Massively-Multi-agent-Learning"><a href="#Neural-MMO-2-0-A-Massively-Multi-task-Addition-to-Massively-Multi-agent-Learning" class="headerlink" title="Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning"></a>Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03736">http://arxiv.org/abs/2311.03736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Suárez, Phillip Isola, Kyoung Whan Choe, David Bloomin, Hao Xiang Li, Nikhil Pinnaparaju, Nishaanth Kanna, Daniel Scott, Ryan Sullivan, Rose S. Shuman, Lucas de Alcântara, Herbie Bradley, Louis Castricato, Kirsty You, Yuhao Jiang, Qimai Li, Jiaxin Chen, Xiaolong Zhu</li>
<li>for: 本研究的目的是提供一个可以定义广泛任务目标和优化征引的大型多代理环境，供实验室研究人员进行强化学习研究。</li>
<li>methods: 本研究使用的方法包括Neural MMO 2.0，一个可以生成地图和多代理的多代理环境，以及CleanRL，一个强化学习框架。</li>
<li>results: 本研究的结果显示，Neural MMO 2.0可以提供三倍的性能提升 compared to its predecessor，并且支持更多的代理和更大的地图。<details>
<summary>Abstract</summary>
Neural MMO 2.0 is a massively multi-agent environment for reinforcement learning research. The key feature of this new version is a flexible task system that allows users to define a broad range of objectives and reward signals. We challenge researchers to train agents capable of generalizing to tasks, maps, and opponents never seen during training. Neural MMO features procedurally generated maps with 128 agents in the standard setting and support for up to. Version 2.0 is a complete rewrite of its predecessor with three-fold improved performance and compatibility with CleanRL. We release the platform as free and open-source software with comprehensive documentation available at neuralmmo.github.io and an active community Discord. To spark initial research on this new platform, we are concurrently running a competition at NeurIPS 2023.
</details>
<details>
<summary>摘要</summary>
neuralmmo 2.0 是一个大规模多智能环境，用于强化学习研究。新版本的关键特性是可变的任务系统，允许用户定义广泛的目标和奖励信号。我们挑战研究人员使得代理人能够通过训练扩展到任务、地图和对手，从未在训练过程中看到。 neuralmmo 2.0 支持生成式地图，标准设置中有128个代理人，并且支持最多4096个代理人。这是前一版本的三倍性能提升，并且与 CleanRL 兼容。我们发布了这个平台作为免费和开源软件，并提供了详细的文档，可以在 neuralmmo.github.io 上查看。同时，我们还在 NeurIPS 2023 上同步进行一项竞赛，以促进这个新平台的初始研究。
</details></li>
</ul>
<hr>
<h2 id="ClimateSet-A-Large-Scale-Climate-Model-Dataset-for-Machine-Learning"><a href="#ClimateSet-A-Large-Scale-Climate-Model-Dataset-for-Machine-Learning" class="headerlink" title="ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning"></a>ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03721">http://arxiv.org/abs/2311.03721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Kaltenborn, Charlotte E. E. Lange, Venkatesh Ramesh, Philippe Brouillard, Yaniv Gurwicz, Chandni Nagda, Jakob Runge, Peer Nowack, David Rolnick</li>
<li>for: 这个论文是为了提供大规模、一致的气候模型数据集，以支持气候科学和机器学习（ML）社区在气候变化预测和相关任务上的努力。</li>
<li>methods: 论文使用了Input4MIPs和CMIP6气候模型数据集，并提供了一个模块化的数据集管道，以便检索和处理更多的气候模型和enario。</li>
<li>results: 论文通过使用ClimateSet数据集作为benchmark，展示了ML模型在不同气候模型下的性能和总体化能力，并预测了新的气候变化enario，为气候科学和政策制定提供了新的参考。<details>
<summary>Abstract</summary>
Climate models have been key for assessing the impact of climate change and simulating future climate scenarios. The machine learning (ML) community has taken an increased interest in supporting climate scientists' efforts on various tasks such as climate model emulation, downscaling, and prediction tasks. Many of those tasks have been addressed on datasets created with single climate models. However, both the climate science and ML communities have suggested that to address those tasks at scale, we need large, consistent, and ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset containing the inputs and outputs of 36 climate models from the Input4MIPs and CMIP6 archives. In addition, we provide a modular dataset pipeline for retrieving and preprocessing additional climate models and scenarios. We showcase the potential of our dataset by using it as a benchmark for ML-based climate model emulation. We gain new insights about the performance and generalization capabilities of the different ML models by analyzing their performance across different climate models. Furthermore, the dataset can be used to train an ML emulator on several climate models instead of just one. Such a "super emulator" can quickly project new climate change scenarios, complementing existing scenarios already provided to policymakers. We believe ClimateSet will create the basis needed for the ML community to tackle climate-related tasks at scale.
</details>
<details>
<summary>摘要</summary>
климаático模型已经是评估气候变化的重要工具，以及预测未来气候enario的方法。 машин学习（ML）社区对此表示了增加的兴趣，以支持气候科学家在各种任务上，如气候模型仿真、下降和预测任务。但是，气候科学和ML社区都认为，为了解决这些任务，我们需要大量、一致、ML准备好的气候模型数据集。这里，我们介绍了气候集（ClimateSet），一个包含36种气候模型的输入和输出数据集。此外，我们还提供了一个模块化数据集管道，用于检索和处理其他气候模型和enario。我们使用该数据集作为ML基于气候模型的仿真 benchmark，并分析不同气候模型之间的性能和总体可行性。此外，该数据集还可以用于训练一个ML仿真器，以便快速预测新的气候变化enario，并补充现有的气候变化enario，已经向政策制定者提供。我们认为，气候集将为ML社区提供基础，以便把气候相关任务进行大规模处理。
</details></li>
</ul>
<hr>
<h2 id="LLM-as-an-Art-Director-LaDi-Using-LLMs-to-improve-Text-to-Media-Generators"><a href="#LLM-as-an-Art-Director-LaDi-Using-LLMs-to-improve-Text-to-Media-Generators" class="headerlink" title="LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators"></a>LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03716">http://arxiv.org/abs/2311.03716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allen Roush, Emil Zakirov, Artemiy Shirokov, Polina Lunina, Jack Gane, Alexander Duffy, Charlie Basil, Aber Whitcomb, Jim Benedetto, Chris DeWolfe</li>
<li>for: 提高文本生成到图像和视频的质量和 relevance</li>
<li>methods: 使用多种技术，如约束解码、智能提示、精度调整和检索，以提高文本生成器的能力</li>
<li>results: 实现了更高质量和更有相关性的图像和视频生成，并在 Plai Labs 的应用和平台中使用<details>
<summary>Abstract</summary>
Recent advancements in text-to-image generation have revolutionized numerous fields, including art and cinema, by automating the generation of high-quality, context-aware images and video. However, the utility of these technologies is often limited by the inadequacy of text prompts in guiding the generator to produce artistically coherent and subject-relevant images. In this paper, We describe the techniques that can be used to make Large Language Models (LLMs) act as Art Directors that enhance image and video generation. We describe our unified system for this called "LaDi". We explore how LaDi integrates multiple techniques for augmenting the capabilities of text-to-image generators (T2Is) and text-to-video generators (T2Vs), with a focus on constrained decoding, intelligent prompting, fine-tuning, and retrieval. LaDi and these techniques are being used today in apps and platforms developed by Plai Labs.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:最近的文本到图像生成技术已经革命化了许多领域，包括艺术和电影，by自动生成高质量、上下文感知的图像和视频。然而，这些技术的实用性往往受到文本提示的不足，导致生成的图像不具有艺术性和主题相关性。在这篇论文中，我们介绍了使用Large Language Models（LLMs）作为艺术指导者，以提高图像和视频生成的技术。我们介绍了我们的统一系统叫“LaDi”，我们探讨了LaDi如何结合多种技术来增强文本到图像生成器（T2Is）和文本到视频生成器（T2Vs）的能力，包括约束解码、智能提示、精度调整和检索。LaDi和这些技术在Plai Labs开发的应用和平台上正在使用。
</details></li>
</ul>
<hr>
<h2 id="Loss-Balancing-for-Fair-Supervised-Learning"><a href="#Loss-Balancing-for-Fair-Supervised-Learning" class="headerlink" title="Loss Balancing for Fair Supervised Learning"></a>Loss Balancing for Fair Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03714">http://arxiv.org/abs/2311.03714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khalilimahdi/loss_balancing_icml2023">https://github.com/khalilimahdi/loss_balancing_icml2023</a></li>
<li>paper_authors: Mohammad Mahdi Khalili, Xueru Zhang, Mahed Abroshan</li>
<li>for: 这 paper 的目的是提出一种可以快速和高效地在不平等损失（Equalized Loss，EL）限制下找到公平预测器的算法。</li>
<li>methods: 这 paper 使用了现有的几何编程工具（CVXPY）来将非对称优化问题转化为一系列的对称优化问题，从而使用 convex 优化算法来快速找到公平预测器。</li>
<li>results: 这 paper 通过 teorically 证明了其算法可以在某些条件下找到全球最优解，并通过了一些 empirical 研究来支持其理论结论。<details>
<summary>Abstract</summary>
Supervised learning models have been used in various domains such as lending, college admission, face recognition, natural language processing, etc. However, they may inherit pre-existing biases from training data and exhibit discrimination against protected social groups. Various fairness notions have been proposed to address unfairness issues. In this work, we focus on Equalized Loss (EL), a fairness notion that requires the expected loss to be (approximately) equalized across different groups. Imposing EL on the learning process leads to a non-convex optimization problem even if the loss function is convex, and the existing fair learning algorithms cannot properly be adopted to find the fair predictor under the EL constraint. This paper introduces an algorithm that can leverage off-the-shelf convex programming tools (e.g., CVXPY) to efficiently find the global optimum of this non-convex optimization. In particular, we propose the ELminimizer algorithm, which finds the optimal fair predictor under EL by reducing the non-convex optimization to a sequence of convex optimization problems. We theoretically prove that our algorithm finds the global optimal solution under certain conditions. Then, we support our theoretical results through several empirical studies.
</details>
<details>
<summary>摘要</summary>
超vised learning模型在不同领域中使用，如贷款、大学招生、脸部识别、自然语言处理等。但它们可能从训练数据中继承潜在的偏见，并对保护的社会群体表现出歧视。各种公平性观念被提出来解决不公平问题。在这种工作中，我们关注平衡损失（EL），它需要不同群体的预期损失相似。在满足EL的情况下，增加了一个非 convex 优化问题，并且现有的公平学习算法无法正确地采用EL约束来找到公平预测器。这篇论文介绍了一种可以利用现有的凸编程工具（如CVXPY）高效地找到非 convex 优化问题的全球最优解。具体来说，我们提出了EL最小化算法，它通过将非 convex 优化问题转化为一系列凸优化问题来找到公平预测器。我们理论上证明了我们的算法可以在某些条件下找到全球最优解。然后，我们通过一些实验研究来支持我们的理论结论。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Estimation-Errors-by-Twin-TD-Regularized-Actor-and-Critic-for-Deep-Reinforcement-Learning"><a href="#Mitigating-Estimation-Errors-by-Twin-TD-Regularized-Actor-and-Critic-for-Deep-Reinforcement-Learning" class="headerlink" title="Mitigating Estimation Errors by Twin TD-Regularized Actor and Critic for Deep Reinforcement Learning"></a>Mitigating Estimation Errors by Twin TD-Regularized Actor and Critic for Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03711">http://arxiv.org/abs/2311.03711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junmin Zhong, Ruofan Wu, Jennie Si</li>
<li>for: 减少深度强化学习中的估计偏误</li>
<li>methods: 引入新的双TD-正则化actor-critic方法，以减少过估和下估错误</li>
<li>results: 通过 combining 好的DRL改进方法，如分布学习和长N步代理stage奖励方法，实现了新的TDR基本actor-critic学习在深度控制集中的出色表现，并将TD3和SAC的表现提升到与D4PG（当前SOTA）水平，同时还提高了D4PG的表现到新的SOTA水平。<details>
<summary>Abstract</summary>
We address the issue of estimation bias in deep reinforcement learning (DRL) by introducing solution mechanisms that include a new, twin TD-regularized actor-critic (TDR) method. It aims at reducing both over and under-estimation errors. With TDR and by combining good DRL improvements, such as distributional learning and long N-step surrogate stage reward (LNSS) method, we show that our new TDR-based actor-critic learning has enabled DRL methods to outperform their respective baselines in challenging environments in DeepMind Control Suite. Furthermore, they elevate TD3 and SAC respectively to a level of performance comparable to that of D4PG (the current SOTA), and they also improve the performance of D4PG to a new SOTA level measured by mean reward, convergence speed, learning success rate, and learning variance.
</details>
<details>
<summary>摘要</summary>
我们解决深度奖励学习（DRL）中的估计偏见问题，通过引入新的双TD-正则化actor-critic（TDR）方法，以减少过估和下估错误。与TDR相结合，我们通过分布学习和长N步代理奖励方法（LNSS），显示了我们的新TDR基于actor-critic学习可以在深度控制集中的复杂环境中超越其基线。此外，它还提高了TD3和SAC的性能，使其与D4PG（当前SOTA）的性能相当，并且还提高了D4PG的性能到新的SOTA水平， measured by mean reward, convergence speed, learning success rate, and learning variance。Note: "SOTA" stands for "State of the Art", which means the current best performance in a particular field or area.
</details></li>
</ul>
<hr>
<h2 id="The-NeurIPS-2022-Neural-MMO-Challenge-A-Massively-Multiagent-Competition-with-Specialization-and-Trade"><a href="#The-NeurIPS-2022-Neural-MMO-Challenge-A-Massively-Multiagent-Competition-with-Specialization-and-Trade" class="headerlink" title="The NeurIPS 2022 Neural MMO Challenge: A Massively Multiagent Competition with Specialization and Trade"></a>The NeurIPS 2022 Neural MMO Challenge: A Massively Multiagent Competition with Specialization and Trade</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03707">http://arxiv.org/abs/2311.03707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuralmmo/neurips2022nmmo-submission-pool">https://github.com/neuralmmo/neurips2022nmmo-submission-pool</a></li>
<li>paper_authors: Enhong Liu, Joseph Suarez, Chenhui You, Bo Wu, Bingcheng Chen, Jun Hu, Jiaxin Chen, Xiaolong Zhu, Clare Zhu, Julian Togelius, Sharada Mohanty, Weijun Hong, Rui Du, Yibing Zhang, Qinwen Wang, Xinhang Li, Zheng Yuan, Xiang Li, Yuejia Huang, Kun Zhang, Hanhui Yang, Shiqi Tang, Phillip Isola</li>
<li>for: 本文描述了 NeurIPS-2022 神经网络多player挑战的结果，该挑战吸引了500名参与者并接受了1,600份提交。</li>
<li>methods: 本文使用了最新的 v1.6 神经网络多player环境，该环境引入了新的设备、战斗、交易和评价系统，对于前一个competition pose additional robustness和总体化挑战。</li>
<li>results: 本文描述了挑战的设计和结果，探讨了这种环境作为学习方法的标准 benchmark 的潜力，并提供了一些实用的强化学习训练方法，以解决复杂任务的稀少奖励问题。<details>
<summary>Abstract</summary>
In this paper, we present the results of the NeurIPS-2022 Neural MMO Challenge, which attracted 500 participants and received over 1,600 submissions. Like the previous IJCAI-2022 Neural MMO Challenge, it involved agents from 16 populations surviving in procedurally generated worlds by collecting resources and defeating opponents. This year's competition runs on the latest v1.6 Neural MMO, which introduces new equipment, combat, trading, and a better scoring system. These elements combine to pose additional robustness and generalization challenges not present in previous competitions. This paper summarizes the design and results of the challenge, explores the potential of this environment as a benchmark for learning methods, and presents some practical reinforcement learning training approaches for complex tasks with sparse rewards. Additionally, we have open-sourced our baselines, including environment wrappers, benchmarks, and visualization tools for future research.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了2022年的NeuIPS neural MMO挑战的结果，该挑战吸引了500名参与者并收到了1,600份提交。与前一年的IJCAI neural MMO挑战一样，这个挑战由16个人口生成的世界中的代理人进行了资源收集和对手击败。这年的比赛运行在最新的v1.6神经MMO上，该版本引入了新的设备、战斗、贸易和更好的分数系统。这些元素共同提供了附加的可靠性和泛化挑战，不存在在前一年的比赛中。本文总结了挑战的设计和结果，探讨了这个环境的潜在作为学习方法的标准测试环境，并提出了一些实用的强化学习训练方法 для复杂任务的稀肥奖励。此外，我们还开源了我们的基elines，包括环境包装器、标准 benchmarks 和视觉化工具，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Bottom-Up-Synthesis-for-Programs-with-Local-Variables"><a href="#Efficient-Bottom-Up-Synthesis-for-Programs-with-Local-Variables" class="headerlink" title="Efficient Bottom-Up Synthesis for Programs with Local Variables"></a>Efficient Bottom-Up Synthesis for Programs with Local Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03705">http://arxiv.org/abs/2311.03705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Xiangyu Zhou, Rui Dong, Yihong Zhang, Xinyu Wang</li>
<li>for: 本研究旨在提出一种新的合成算法，可以效率地搜索具有地方变量（如lambda引入的程序）的程序。现有的底部合成算法无法评估具有自由地方变量的程序，因此无法有效减少这些程序的搜索空间（例如使用标准观察等价减少技术），从而使合成变得慢。本算法可以减少程序的搜索空间。</li>
<li>methods: 本研究使用的方法包括提出了”提起解释”的想法，即从一个程序中提高解释过程，从而同时评估所有程序的语法树。这种方法可以系统地生成所有绑定上下文，因此可以评估和减少具有地方变量的程序的搜索空间。</li>
<li>results: 研究结果表明，使用提出的算法可以更有效地自动化互联网自动化任务，比如WebRobot和Helena等现有的技术。在实际应用中，提出的工具”Arborist”可以更高效地完成更多的挑战性任务。<details>
<summary>Abstract</summary>
We propose a new synthesis algorithm that can efficiently search programs with local variables (e.g., those introduced by lambdas). Prior bottom-up synthesis algorithms are not able to evaluate programs with free local variables, and therefore cannot effectively reduce the search space of such programs (e.g., using standard observational equivalence reduction techniques), making synthesis slow. Our algorithm can reduce the space of programs with local variables. The key idea, dubbed lifted interpretation, is to lift up the program interpretation process, from evaluating one program at a time to simultaneously evaluating all programs from a grammar. Lifted interpretation provides a mechanism to systematically enumerate all binding contexts for local variables, thereby enabling us to evaluate and reduce the space of programs with local variables. Our ideas are instantiated in the domain of web automation. The resulting tool, Arborist, can automate a significantly broader range of challenging tasks more efficiently than state-of-the-art techniques including WebRobot and Helena.
</details>
<details>
<summary>摘要</summary>
我们提出了一个新的合成算法，可以效率地搜寻具有地方变数（例如lambda引入的）的程式。先前的底部合成算法无法评估具有自由地方变数的程式，因此无法有效缩小这些程式的搜寻空间（例如使用标准观察对等减少技术），使合成变得慢。我们的算法可以缩小具有地方变数的程式的搜寻空间。我们的主要想法是将程式解释过程“升级”到同时评估所有程式的语法中，这称为“提升解释”。提升解释提供了一个系统性的方式来谱系地方变数的绑定上下文，因此可以有效地评估和缩小具有地方变数的程式的搜寻空间。我们的想法是实现在网页自动化领域，而我们的工具Arborist可以更有效率地自动化许多具有挑战性的任务，比如WebRobot和Helena。
</details></li>
</ul>
<hr>
<h2 id="Hypothesis-Network-Planned-Exploration-for-Rapid-Meta-Reinforcement-Learning-Adaptation"><a href="#Hypothesis-Network-Planned-Exploration-for-Rapid-Meta-Reinforcement-Learning-Adaptation" class="headerlink" title="Hypothesis Network Planned Exploration for Rapid Meta-Reinforcement Learning Adaptation"></a>Hypothesis Network Planned Exploration for Rapid Meta-Reinforcement Learning Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03701">http://arxiv.org/abs/2311.03701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxwell Joseph Jacobson, Yexiang Xue</li>
<li>for:  trains agents that adapt to fast-changing environments and tasks</li>
<li>methods: integrates an active and planned exploration process via the hypothesis network to optimize adaptation speed</li>
<li>results: outpaces baseline methods in adaptation speed and model accuracy, validating its potential in enhancing reinforcement learning adaptation in rapidly evolving settingsHere’s the full text in Simplified Chinese:</li>
<li>for: 训练适应快速变化环境和任务的智能代理</li>
<li>methods: 通过假设网络实现活动和规划探索，以优化适应速度</li>
<li>results: 在符号化的Alchemy游戏中舜测了比基准方法更快的适应速度和模型准确率，证明其在快速演化 Setting 中提高学习适应的潜力<details>
<summary>Abstract</summary>
Meta Reinforcement Learning (Meta RL) trains agents that adapt to fast-changing environments and tasks. Current strategies often lose adaption efficiency due to the passive nature of model exploration, causing delayed understanding of new transition dynamics. This results in particularly fast-evolving tasks being impossible to solve. We propose a novel approach, Hypothesis Network Planned Exploration (HyPE), that integrates an active and planned exploration process via the hypothesis network to optimize adaptation speed. HyPE uses a generative hypothesis network to form potential models of state transition dynamics, then eliminates incorrect models through strategically devised experiments. Evaluated on a symbolic version of the Alchemy game, HyPE outpaces baseline methods in adaptation speed and model accuracy, validating its potential in enhancing reinforcement learning adaptation in rapidly evolving settings.
</details>
<details>
<summary>摘要</summary>
meta 强化学习（Meta RL）训练代理人可适应快速变化的环境和任务。现有策略经常因模型探索的 passive 性而失去适应效率，导致新的转移动力学特性理解延迟。这会导致特别是快速演化的任务无法解决。我们提议一种新的方法，假设网络规划探索（HyPE），它通过假设网络整合活动和规划探索过程，以优化适应速度。HyPE 使用生成假设网络形成可能的状态转移动力学模型，然后通过策划的实验排除错误模型。在符号版的 Alchemy 游戏中评估，HyPE 比基eline方法更快地适应和模型准确性提高，证明其在快速演化的情况下提高强化学习适应能力的潜力。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Variational-Lower-Bound-for-Inverse-Reinforcement-Learning"><a href="#A-Novel-Variational-Lower-Bound-for-Inverse-Reinforcement-Learning" class="headerlink" title="A Novel Variational Lower Bound for Inverse Reinforcement Learning"></a>A Novel Variational Lower Bound for Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03698">http://arxiv.org/abs/2311.03698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikang Gui, Prashant Doshi</li>
<li>for: 学习任务协同或模仿的奖励函数，从专家轨迹中学习任务特性和奖励函数。</li>
<li>methods: 基于概率图模型和优化节点的Variational Lower Bound for IRL（VLB-IRL）方法，同时学习奖励函数和政策。</li>
<li>results: 在多个知名领域中，比如跑步和游戏等，方法可以学习有效的奖励函数，并且政策下的奖励函数可以达到专家水平性。此外，方法也超过了现有的State-of-the-art IRL算法在这些领域的性能。<details>
<summary>Abstract</summary>
Inverse reinforcement learning (IRL) seeks to learn the reward function from expert trajectories, to understand the task for imitation or collaboration thereby removing the need for manual reward engineering. However, IRL in the context of large, high-dimensional problems with unknown dynamics has been particularly challenging. In this paper, we present a new Variational Lower Bound for IRL (VLB-IRL), which is derived under the framework of a probabilistic graphical model with an optimality node. Our method simultaneously learns the reward function and policy under the learned reward function by maximizing the lower bound, which is equivalent to minimizing the reverse Kullback-Leibler divergence between an approximated distribution of optimality given the reward function and the true distribution of optimality given trajectories. This leads to a new IRL method that learns a valid reward function such that the policy under the learned reward achieves expert-level performance on several known domains. Importantly, the method outperforms the existing state-of-the-art IRL algorithms on these domains by demonstrating better reward from the learned policy.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Inverse reinforcement learning (IRL) seeks to learn the reward function from expert trajectories, to understand the task for imitation or collaboration thereby removing the need for manual reward engineering. However, IRL in the context of large, high-dimensional problems with unknown dynamics has been particularly challenging. In this paper, we present a new Variational Lower Bound for IRL (VLB-IRL), which is derived under the framework of a probabilistic graphical model with an optimality node. Our method simultaneously learns the reward function and policy under the learned reward function by maximizing the lower bound, which is equivalent to minimizing the reverse Kullback-Leibler divergence between an approximated distribution of optimality given the reward function and the true distribution of optimality given trajectories. This leads to a new IRL method that learns a valid reward function such that the policy under the learned reward achieves expert-level performance on several known domains. Importantly, the method outperforms the existing state-of-the-art IRL algorithms on these domains by demonstrating better reward from the learned policy."into Simplified Chinese.Here's the translation:<<SYS>> inverse reinforcement learning (IRL) 目的是从专家轨迹中学习奖励函数，以便理解任务，以便模拟或协作，从而消除手动奖励工程。然而，在大型、高维度问题中， unknown 动力学下的 IRL 特别困难。在这篇文章中，我们提出了一种新的 Variational Lower Bound for IRL (VLB-IRL)，它基于 probabilistic graphical model 的优化节点。我们的方法同时学习奖励函数和 policy under 学习的奖励函数，通过最大化下界，等于最小化 reverse Kullback-Leibler divergence between  an approximated distribution of optimality given the reward function and the true distribution of optimality given trajectories。这导致了一种新的 IRL 方法，该方法学习一个有效的奖励函数，使得 policy under 学习的奖励函数 achieve expert-level performance on several known domains。重要的是，该方法在这些领域上超过了现有的 state-of-the-art IRL 算法，通过示出更好的奖励来证明。
</details></li>
</ul>
<hr>
<h2 id="Context-Shift-Reduction-for-Offline-Meta-Reinforcement-Learning"><a href="#Context-Shift-Reduction-for-Offline-Meta-Reinforcement-Learning" class="headerlink" title="Context Shift Reduction for Offline Meta-Reinforcement Learning"></a>Context Shift Reduction for Offline Meta-Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03695">http://arxiv.org/abs/2311.03695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moreanp/csro">https://github.com/moreanp/csro</a></li>
<li>paper_authors: Yunkai Gao, Rui Zhang, Jiaming Guo, Fan Wu, Qi Yi, Shaohui Peng, Siming Lan, Ruizhi Chen, Zidong Du, Xing Hu, Qi Guo, Ling Li, Yunji Chen</li>
<li>for: 提高meta-学习 Agent的总结能力，解决context shift问题</li>
<li>methods: 提出了一种新的Context Shift Reduction for OMRL（CSRO）方法，通过减少policy在context中的影响来解决context shift问题</li>
<li>results: 实验结果显示，CSRO方法可以减少context shift，提高meta-学习 Agent的总结能力，超过了之前的方法在多个复杂的领域<details>
<summary>Abstract</summary>
Offline meta-reinforcement learning (OMRL) utilizes pre-collected offline datasets to enhance the agent's generalization ability on unseen tasks. However, the context shift problem arises due to the distribution discrepancy between the contexts used for training (from the behavior policy) and testing (from the exploration policy). The context shift problem leads to incorrect task inference and further deteriorates the generalization ability of the meta-policy. Existing OMRL methods either overlook this problem or attempt to mitigate it with additional information. In this paper, we propose a novel approach called Context Shift Reduction for OMRL (CSRO) to address the context shift problem with only offline datasets. The key insight of CSRO is to minimize the influence of policy in context during both the meta-training and meta-test phases. During meta-training, we design a max-min mutual information representation learning mechanism to diminish the impact of the behavior policy on task representation. In the meta-test phase, we introduce the non-prior context collection strategy to reduce the effect of the exploration policy. Experimental results demonstrate that CSRO significantly reduces the context shift and improves the generalization ability, surpassing previous methods across various challenging domains.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Offline meta-reinforcement learning (OMRL) utilizes pre-collected offline datasets to enhance the agent's generalization ability on unseen tasks. However, the context shift problem arises due to the distribution discrepancy between the contexts used for training (from the behavior policy) and testing (from the exploration policy). The context shift problem leads to incorrect task inference and further deteriorates the generalization ability of the meta-policy. Existing OMRL methods either overlook this problem or attempt to mitigate it with additional information. In this paper, we propose a novel approach called Context Shift Reduction for OMRL (CSRO) to address the context shift problem with only offline datasets. The key insight of CSRO is to minimize the influence of policy in context during both the meta-training and meta-test phases. During meta-training, we design a max-min mutual information representation learning mechanism to diminish the impact of the behavior policy on task representation. In the meta-test phase, we introduce the non-prior context collection strategy to reduce the effect of the exploration policy. Experimental results demonstrate that CSRO significantly reduces the context shift and improves the generalization ability, surpassing previous methods across various challenging domains."into Simplified Chinese:Offline meta-学习（OMRL）利用预收集的offline数据集来提高agent的未知任务泛化能力。然而，上下文偏移问题出现，由于各个上下文在训练（从行为策略）和测试（从探索策略）中的分布差异。这个问题会导致任务推理错误，并进一步恶化meta策略的泛化能力。现有OMRL方法可能忽视这个问题，或者尝试通过额外信息来缓解。在这篇论文中，我们提出了一种新的方法——上下文偏移减少 для OMRL（CSRO），通过仅使用offline数据集来解决上下文偏移问题。CSRO的关键思想在于，在meta训练和meta测试阶段都尽量减少策略在上下文中的影响。在meta训练阶段，我们设计了max-min相互信息表示学习机制，以减少行为策略对任务表示的影响。在meta测试阶段，我们引入了非先验上下文收集策略，以减少探索策略对上下文的影响。实验结果表明，CSRO可以减少上下文偏移，提高泛化能力，超过了先前方法在各种复杂的领域。
</details></li>
</ul>
<hr>
<h2 id="Deep-Bayesian-Reinforcement-Learning-for-Spacecraft-Proximity-Maneuvers-and-Docking"><a href="#Deep-Bayesian-Reinforcement-Learning-for-Spacecraft-Proximity-Maneuvers-and-Docking" class="headerlink" title="Deep Bayesian Reinforcement Learning for Spacecraft Proximity Maneuvers and Docking"></a>Deep Bayesian Reinforcement Learning for Spacecraft Proximity Maneuvers and Docking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03680">http://arxiv.org/abs/2311.03680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Desong Du, Naiming Qi, Yanfang Liu, Wei Pan</li>
<li>for: 本研究旨在开发一种 Bayesian actor-critic 征值学习算法，用于实现自主太空船的距离执行和对接（PMD）任务。</li>
<li>methods: 本研究使用 Lyapunov 理论和 Gaussian 过程 regression 技术，将 PMD 任务转化为一个 Markov 决策过程，并使用 Bayesian  quadrature policy 优化程序来分析策略对应。</li>
<li>results: 实验结果显示，提出的算法在一个太空船气压滑道测试平台上表现出色，并且满足了航天任务中的严格安全需求。<details>
<summary>Abstract</summary>
In the pursuit of autonomous spacecraft proximity maneuvers and docking(PMD), we introduce a novel Bayesian actor-critic reinforcement learning algorithm to learn a control policy with the stability guarantee. The PMD task is formulated as a Markov decision process that reflects the relative dynamic model, the docking cone and the cost function. Drawing from the principles of Lyapunov theory, we frame the temporal difference learning as a constrained Gaussian process regression problem. This innovative approach allows the state-value function to be expressed as a Lyapunov function, leveraging the Gaussian process and deep kernel learning. We develop a novel Bayesian quadrature policy optimization procedure to analytically compute the policy gradient while integrating Lyapunov-based stability constraints. This integration is pivotal in satisfying the rigorous safety demands of spaceflight missions. The proposed algorithm has been experimentally evaluated on a spacecraft air-bearing testbed and shows impressive and promising performance.
</details>
<details>
<summary>摘要</summary>
在探索自主空天器近距离推进和协调（PMD）方面，我们介绍了一种新的 bayesian actor-critic reinforcement learning算法，以学习一个具有稳定保证的控制策略。PMD任务被定义为一个Markov决策过程，反映了相对动态模型、协调杯和成本函数。从Lyapunov理论的原则出发，我们将时间差学习转化为一个受限Gaussian проце推 regression问题。这种创新的方法使得状态价值函数可以表示为Lyapunov函数，利用Gaussian проце和深度kernel学习。我们开发了一种bayesian quadrature policy优化程序，可以分析计算策略偏导的同时，并将Lyapunov-based稳定约束集成到系统中。这种集成是Spaceflight任务中的严格安全要求的满足。我们在一个空天器空拖测试平台上实验了提议的算法，并表现出了非常出色和可能的性能。
</details></li>
</ul>
<hr>
<h2 id="Stable-Modular-Control-via-Contraction-Theory-for-Reinforcement-Learning"><a href="#Stable-Modular-Control-via-Contraction-Theory-for-Reinforcement-Learning" class="headerlink" title="Stable Modular Control via Contraction Theory for Reinforcement Learning"></a>Stable Modular Control via Contraction Theory for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03669">http://arxiv.org/abs/2311.03669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bing Song, Jean-Jacques Slotine, Quang-Cuong Pham</li>
<li>for: 提高 neural control 稳定性、可靠性和泛化能力</li>
<li>methods: 利用 contraction theory 实现模块化 neural control，使得稳定性自然地保持，并且通过 signal composition 和 dynamic decomposition 实现模块化。</li>
<li>results: 在 simulations 中证明了方法的必要性和有效性，方法可以提高 hierarchical RL 的性能，并且可以使得 neural control 更加稳定、可靠和泛化。<details>
<summary>Abstract</summary>
We propose a novel way to integrate control techniques with reinforcement learning (RL) for stability, robustness, and generalization: leveraging contraction theory to realize modularity in neural control, which ensures that combining stable subsystems can automatically preserve the stability. We realize such modularity via signal composition and dynamic decomposition. Signal composition creates the latent space, within which RL applies to maximizing rewards. Dynamic decomposition is realized by coordinate transformation that creates an auxiliary space, within which the latent signals are coupled in the way that their combination can preserve stability provided each signal, that is, each subsystem, has stable self-feedbacks. Leveraging modularity, the nonlinear stability problem is deconstructed into algebraically solvable ones, the stability of the subsystems in the auxiliary space, yielding linear constraints on the input gradients of control networks that can be as simple as switching the signs of network weights. This minimally invasive method for stability allows arguably easy integration into the modular neural architectures in machine learning, like hierarchical RL, and improves their performance. We demonstrate in simulation the necessity and the effectiveness of our method: the necessity for robustness and generalization, and the effectiveness in improving hierarchical RL for manipulation learning.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，把控制技术与强化学习（RL）结合在一起，以确保稳定性、可靠性和泛化性：通过ontraction theory来实现模块化在神经控制中，以确保将稳定的子系统组合起来，可以保持稳定性。我们通过信号组合和动态分解来实现这种模块化。信号组合创造了隐藏空间，在这个空间中，RL可以以最大化奖励来实现。动态分解通过坐标转换创造了一个辅助空间，在这个空间中，隐藏信号被 coupling在一起，以保持稳定性， provided each signal, that is, each subsystem, has stable self-feedbacks。通过模块化，非线性稳定性问题被分解成可解的问题，即隐藏空间中的稳定性问题，它们是线性约束的输入梯度问题，可以非常简单地Switching the signs of network weights。这种非侵入式的稳定性方法可以轻松地集成到现有的模块化神经网络架构中，例如层次RL，并提高其性能。我们在模拟中证明了我们的方法的必要性和有效性：必要性是为了稳定性和泛化性，有效性是通过改进层次RL的掌握能力。
</details></li>
</ul>
<hr>
<h2 id="GPT-ST-Generative-Pre-Training-of-Spatio-Temporal-Graph-Neural-Networks"><a href="#GPT-ST-Generative-Pre-Training-of-Spatio-Temporal-Graph-Neural-Networks" class="headerlink" title="GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks"></a>GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04245">http://arxiv.org/abs/2311.04245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkuds/gpt-st">https://github.com/hkuds/gpt-st</a></li>
<li>paper_authors: Zhonghang Li, Lianghao Xia, Yong Xu, Chao Huang</li>
<li>for: 本文旨在提出一种针对流行管理和旅行规划的空间temporal预测技术快速发展的方法，以满足现代交通管理和旅行规划的需求。</li>
<li>methods: 本文提出了一种针对下游基线模型的预训练框架，该框架通过两个关键设计来增强预测性能：首先，我们提出了一种针对空间temporal依赖关系的自适应封装器，该模型包括自定义参数学习器和层次的空间pattern编码器。其次，我们引入了一种适应封装策略，以便在预训练过程中学习Robust的空间temporal表示，并且可以轻松地模型不同关系，从内部cluster到外部cluster，在易于困难的训练方式下。</li>
<li>results: 我们在代表性的 benchmark 上进行了广泛的实验，并证明了我们的提出方法的效iveness。我们的模型实现可以在<a target="_blank" rel="noopener" href="https://github.com/HKUDS/GPT-ST">https://github.com/HKUDS/GPT-ST</a> 上获得。<details>
<summary>Abstract</summary>
In recent years, there has been a rapid development of spatio-temporal prediction techniques in response to the increasing demands of traffic management and travel planning. While advanced end-to-end models have achieved notable success in improving predictive performance, their integration and expansion pose significant challenges. This work aims to address these challenges by introducing a spatio-temporal pre-training framework that seamlessly integrates with downstream baselines and enhances their performance. The framework is built upon two key designs: (i) We propose a spatio-temporal mask autoencoder as a pre-training model for learning spatio-temporal dependencies. The model incorporates customized parameter learners and hierarchical spatial pattern encoding networks. These modules are specifically designed to capture spatio-temporal customized representations and intra- and inter-cluster region semantic relationships, which have often been neglected in existing approaches. (ii) We introduce an adaptive mask strategy as part of the pre-training mechanism. This strategy guides the mask autoencoder in learning robust spatio-temporal representations and facilitates the modeling of different relationships, ranging from intra-cluster to inter-cluster, in an easy-to-hard training manner. Extensive experiments conducted on representative benchmarks demonstrate the effectiveness of our proposed method. We have made our model implementation publicly available at https://github.com/HKUDS/GPT-ST.
</details>
<details>
<summary>摘要</summary>
Recently, there has been a rapid development of spatio-temporal prediction techniques in response to the increasing demands of traffic management and travel planning. Although advanced end-to-end models have achieved notable success in improving predictive performance, their integration and expansion pose significant challenges. This work aims to address these challenges by introducing a spatio-temporal pre-training framework that seamlessly integrates with downstream baselines and enhances their performance. The framework is built upon two key designs:(i) We propose a spatio-temporal mask autoencoder as a pre-training model for learning spatio-temporal dependencies. The model incorporates customized parameter learners and hierarchical spatial pattern encoding networks. These modules are specifically designed to capture spatio-temporal customized representations and intra- and inter-cluster region semantic relationships, which have often been neglected in existing approaches.(ii) We introduce an adaptive mask strategy as part of the pre-training mechanism. This strategy guides the mask autoencoder in learning robust spatio-temporal representations and facilitates the modeling of different relationships, ranging from intra-cluster to inter-cluster, in an easy-to-hard training manner.Extensive experiments conducted on representative benchmarks demonstrate the effectiveness of our proposed method. Our model implementation is publicly available at <https://github.com/HKUDS/GPT-ST>.
</details></li>
</ul>
<hr>
<h2 id="The-Linear-Representation-Hypothesis-and-the-Geometry-of-Large-Language-Models"><a href="#The-Linear-Representation-Hypothesis-and-the-Geometry-of-Large-Language-Models" class="headerlink" title="The Linear Representation Hypothesis and the Geometry of Large Language Models"></a>The Linear Representation Hypothesis and the Geometry of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03658">http://arxiv.org/abs/2311.03658</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kihopark/linear_rep_geometry">https://github.com/kihopark/linear_rep_geometry</a></li>
<li>paper_authors: Kiho Park, Yo Joong Choe, Victor Veitch</li>
<li>for: This paper aims to clarify the meaning of “linear representation” in the context of natural language processing, and to develop a unified framework for understanding geometric notions such as cosine similarity and projection in representation space.</li>
<li>methods: The authors use counterfactuals to formalize two different notions of “linear representation”, one in the output (word) representation space and one in the input (sentence) space. They then prove that these notions connect to linear probing and model steering, respectively.</li>
<li>results: The authors show that using a particular (non-Euclidean) inner product, they can unify all notions of linear representation, and demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product through experiments with LLaMA-2.<details>
<summary>Abstract</summary>
Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.
</details>
<details>
<summary>摘要</summary>
文章提出了两个问题：首先，“线性表示”的具体含义是什么？其次，在表示空间中如何理解几何概念（例如cosine相似性或投影）？为了回答这些问题，文章使用了counterfactual语言来给出了两种形式化的“线性表示”，一种在输出（单词）表示空间中，另一种在输入（句子）空间中。然后，文章证明了这些连接到线性探测和模型导航。使用这种形式化，文章可以归一化所有的线性表示概念，并通过counterfactual对pair来构建探测器和导航向量。实验表明了概念的线性表示存在，以及与解释和控制的关系。此外，文章还证明了内积的选择对于语言结构的理解起到了关键作用。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Parameterization-of-the-Multi-scale-Kain-Fritsch-MSKF-Convection-Scheme"><a href="#Machine-Learning-Parameterization-of-the-Multi-scale-Kain-Fritsch-MSKF-Convection-Scheme" class="headerlink" title="Machine Learning Parameterization of the Multi-scale Kain-Fritsch (MSKF) Convection Scheme"></a>Machine Learning Parameterization of the Multi-scale Kain-Fritsch (MSKF) Convection Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03652">http://arxiv.org/abs/2311.03652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohui Zhong, Xing Yu, Hao Li</li>
<li>For: This paper aims to improve the representation of convective transport in high-resolution numerical weather prediction (NWP) models, specifically in the gray zone where the grid spacing is comparable to the length scales of convection.* Methods: The authors use a multi-scale Kain-Fritsch (MSKF) scheme and a multi-output bidirectional long short-term memory (Bi-LSTM) model to represent convective transport in the gray zone. They also compare the performance of the Bi-LSTM model with the original MSKF scheme in the WRF model.* Results: The results show that the Bi-LSTM model can achieve high accuracy in representing convective transport in the gray zone, indicating the potential use of machine learning (ML) models to substitute physical parameterizations in NWP models.<details>
<summary>Abstract</summary>
Warm-sector heavy rainfall often occurs along the coast of South China, and it is usually localized and long-lasting, making it challenging to predict. High-resolution numerical weather prediction (NWP) models are increasingly used to better resolve topographic features and forecast such high-impact weather events. However, when the grid spacing becomes comparable to the length scales of convection, known as the gray zone, the turbulent eddies in the atmospheric boundary layer are only partially resolved and parameterized to some extent. Whether using a convection parameterization (CP) scheme in the gray zone remains controversial. Scale-aware CP schemes are developed to enhance the representation of convective transport within the gray zone. The multi-scale Kain-Fritsch (MSKF) scheme includes modifications that allow for its effective implementation at a grid resolution as high as 2 km. In recent years, there has been an increasing application of machine learning (ML) models to various domains of atmospheric sciences, including the replacement of physical parameterizations with ML models. This work proposes a multi-output bidirectional long short-term memory (Bi-LSTM) model as a replace the scale-aware MSKF CP scheme. The Weather Research and Forecast (WRF) model is used to generate training and testing data over South China at a horizontal resolution of 5 km. Furthermore, the WRF model is coupled with the ML based CP scheme and compared with WRF simulations with original MSKF scheme. The results demonstrate that the Bi-LSTM model can achieve high accuracy, indicating the potential use of ML models to substitute the MSKF scheme in the gray zone.
</details>
<details>
<summary>摘要</summary>
暖 sector 重降水 frequent occurrence 南中国 海岸，通常是局部化和长时间的，预测困难。高分解能数值天气预测 (NWP) 模型在气象预测中日益广泛应用，以提高地形特征的解析和预测高影响天气事件。然而，当网格间距相当于 конвектив 的长度尺度时，称为灰色区域，大气边层中的湍流涡旋只部分解析出来，部分用参数化来做出报告。使用湍流参数化 (CP) 方案在灰色区域是有争议的。基于Scale-aware CP 方案的多个气象学应用。多Scale Kain-Fritsch (MSKF) 方案包括修改，以使其在2 km 的网格分辨率上有效实施。最近几年，机器学习 (ML) 模型在大气科学中的应用越来越广泛，包括将物理参数化替换为 ML 模型。这项工作提出了一种多输出 bidirectional 长短时间储存 (Bi-LSTM) 模型，用来取代灰色区域中的Scale-aware MSKF CP 方案。使用 WRF 模型生成训练和测试数据，并将 WRF 模型与 ML 基于 CP 方案相结合，与 WRF 模型中原始 MSKF 方案进行比较。结果显示，Bi-LSTM 模型可以达到高精度，表明 ML 模型可以在灰色区域中代替 MSKF 方案。
</details></li>
</ul>
<hr>
<h2 id="SeRO-Self-Supervised-Reinforcement-Learning-for-Recovery-from-Out-of-Distribution-Situations"><a href="#SeRO-Self-Supervised-Reinforcement-Learning-for-Recovery-from-Out-of-Distribution-Situations" class="headerlink" title="SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations"></a>SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03651">http://arxiv.org/abs/2311.03651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snuchankim/sero">https://github.com/snuchankim/sero</a></li>
<li>paper_authors: Chan Kim, Jaekyung Cho, Christophe Bobda, Seung-Woo Seo, Seong-Woo Kim</li>
<li>for: 解决机器人代理人使用强化学习时在不同分布下（Out-of-distribution, OOD）状态下行为不可靠的问题。</li>
<li>methods: 我们提出了一种自动学习方法，使得当机器人代理人在OOD状态下时，可以自动恢复到已经学习的状态分布中。</li>
<li>results: 我们的实验结果表明，我们的方法可以大幅提高机器人代理人在OOD状态下恢复能力，包括样本效率和原始任务完成度的提高。此外，我们还证明了我们的方法可以在困难探索的状态下进行自动恢复。<details>
<summary>Abstract</summary>
Robotic agents trained using reinforcement learning have the problem of taking unreliable actions in an out-of-distribution (OOD) state. Agents can easily become OOD in real-world environments because it is almost impossible for them to visit and learn the entire state space during training. Unfortunately, unreliable actions do not ensure that agents perform their original tasks successfully. Therefore, agents should be able to recognize whether they are in OOD states and learn how to return to the learned state distribution rather than continue to take unreliable actions. In this study, we propose a novel method for retraining agents to recover from OOD situations in a self-supervised manner when they fall into OOD states. Our in-depth experimental results demonstrate that our method substantially improves the agent's ability to recover from OOD situations in terms of sample efficiency and restoration of the performance for the original tasks. Moreover, we show that our method can retrain the agent to recover from OOD situations even when in-distribution states are difficult to visit through exploration.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HKTGNN-Hierarchical-Knowledge-Transferable-Graph-Neural-Network-based-Supply-Chain-Risk-Assessment"><a href="#HKTGNN-Hierarchical-Knowledge-Transferable-Graph-Neural-Network-based-Supply-Chain-Risk-Assessment" class="headerlink" title="HKTGNN: Hierarchical Knowledge Transferable Graph Neural Network-based Supply Chain Risk Assessment"></a>HKTGNN: Hierarchical Knowledge Transferable Graph Neural Network-based Supply Chain Risk Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04244">http://arxiv.org/abs/2311.04244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanting Zhou, Kejun Bi, Yuyanzhen Zhong, Chao Tang, Dongfen Li, Shi Ying, Ruijin Wang</li>
<li>for: 本研究旨在提供一种基于图学 embedding 技术的供应链风险评估模型，以帮助企业管理和减轻供应链中的潜在风险。</li>
<li>methods: 本研究使用一种基于图 neural network 的层次知识传递模型（HKTGNN），通过对供应链网络中各个商品的图像进行嵌入，将复杂的供应链网络简化成一个导向同质图，并使用中心性知识传递模块和特征补做消除数据渴望问题。</li>
<li>results: 实验结果表明， compared with traditional knowledge inference methods, our proposed HKTGNN model outperforms in assessing supply chain risk. We also prove the effectiveness and fairness of our comparative experiment through an equation.<details>
<summary>Abstract</summary>
The strength of a supply chain is an important measure of a country's or region's technical advancement and overall competitiveness. Establishing supply chain risk assessment models for effective management and mitigation of potential risks has become increasingly crucial. As the number of businesses grows, the important relationships become more complicated and difficult to measure. This emphasizes the need of extracting relevant information from graph data. Previously, academics mostly employed knowledge inference to increase the visibility of links between nodes in the supply chain. However, they have not solved the data hunger problem of single node feature characteristics. We propose a hierarchical knowledge transferable graph neural network-based (HKTGNN) supply chain risk assessment model to address these issues. Our approach is based on current graph embedding methods for assessing corporate investment risk assessment. We embed the supply chain network corresponding to individual goods in the supply chain using the graph embedding module, resulting in a directed homogeneous graph with just product nodes. This reduces the complicated supply chain network into a basic product network. It addresses difficulties using the domain difference knowledge transferable module based on centrality, which is presented by the premise that supply chain feature characteristics may be biased in the actual world. Meanwhile, the feature complement and message passing will alleviate the data hunger problem, which is driven by domain differences. Our model outperforms in experiments on a real-world supply chain dataset. We will give an equation to prove that our comparative experiment is both effective and fair.
</details>
<details>
<summary>摘要</summary>
“供应链的强度是一个国家或地区的技术进步和竞争力的重要指标。为了有效管理和遏制风险，建立供应链风险评估模型已经变得越来越重要。随着企业数量的增加，关键关系变得更加复杂和难以测量。这种情况强调了提取供应链数据中重要信息的需要。在过去，学术界主要采用了知识推断来增强供应链中节点之间的链接可见性。然而，它们没有解决单节点特征特性数据不足的问题。我们提出了基于图数据预处理的层次知识传递图神经网络（HKTGNN）供应链风险评估模型，以解决这些问题。我们的方法基于当前图集 embedding 方法，用于评估企业投资风险评估。我们将供应链网络相应的各个商品 embedding 到图集模块中，从而将复杂的供应链网络转化为基本的产品网络。这解决了使用中心性知识传递模块基于中心性，即供应链特征特性可能受到实际世界的偏见的问题。同时，特征补做和消息传递可以缓解数据不足问题，它是由于领域差异所致。我们的模型在实际供应链数据集上实验表现出色，我们将给出一个公式来证明我们的比较实验是有效和公正的。”Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-the-User-Perception-of-Chatbots-in-Education-Using-A-Partial-Least-Squares-Structural-Equation-Modeling-Approach"><a href="#Analysis-of-the-User-Perception-of-Chatbots-in-Education-Using-A-Partial-Least-Squares-Structural-Equation-Modeling-Approach" class="headerlink" title="Analysis of the User Perception of Chatbots in Education Using A Partial Least Squares Structural Equation Modeling Approach"></a>Analysis of the User Perception of Chatbots in Education Using A Partial Least Squares Structural Equation Modeling Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03636">http://arxiv.org/abs/2311.03636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Rabiul Hasan, Nahian Ismail Chowdhury, Md Hadisur Rahman, Md Asif Bin Syed, JuHyeong Ryu</li>
<li>for: 本研究旨在探讨学生在教育中对聊天机器人的采用，以填补现有研究中缺乏关注行为相关因素的知识空白。</li>
<li>methods: 本研究使用了部分最小 squares 结构方程模型（PLS-SEM）调查学生对聊天机器人在教育中的采用，并考虑了技术Ready Index（TRI）和技术接受度模型（TAM）。数据收集使用了五点likert分布，共获得185个答复，并使用R-Studio软件进行分析。</li>
<li>results: 研究发现，乐观性和创新性对聊天机器人的易用性（PEOU）和有用性（PU）具有正相关关系，而不适和不安室对PEOU具有负相关关系，只有不安室对PU具有负影响。这些发现可以为未来的技术设计师提供指导，揭示了在教育上下文中聊天机器人采用的关键用户行为因素。<details>
<summary>Abstract</summary>
The integration of Artificial Intelligence (AI) into education is a recent development, with chatbots emerging as a noteworthy addition to this transformative landscape. As online learning platforms rapidly advance, students need to adapt swiftly to excel in this dynamic environment. Consequently, understanding the acceptance of chatbots, particularly those employing Large Language Model (LLM) such as Chat Generative Pretrained Transformer (ChatGPT), Google Bard, and other interactive AI technologies, is of paramount importance. However, existing research on chatbots in education has overlooked key behavior-related aspects, such as Optimism, Innovativeness, Discomfort, Insecurity, Transparency, Ethics, Interaction, Engagement, and Accuracy, creating a significant literature gap. To address this gap, this study employs Partial Least Squares Structural Equation Modeling (PLS-SEM) to investigate the determinant of chatbots adoption in education among students, considering the Technology Readiness Index (TRI) and Technology Acceptance Model (TAM). Utilizing a five-point Likert scale for data collection, we gathered a total of 185 responses, which were analyzed using R-Studio software. We established 12 hypotheses to achieve its objectives. The results showed that Optimism and Innovativeness are positively associated with Perceived Ease of Use (PEOU) and Perceived Usefulness (PU). Conversely, Discomfort and Insecurity negatively impact PEOU, with only Insecurity negatively affecting PU. These findings provide insights for future technology designers, elucidating critical user behavior factors influencing chatbots adoption and utilization in educational contexts.
</details>
<details>
<summary>摘要</summary>
教育领域中人工智能（AI）的整合是一项最近的发展，聊天机器人（chatbot）成为这一变革的一个显著添加。在在线学习平台快速进步的情况下，学生需要快速适应以成功在这个动态环境中。因此，理解聊天机器人的接受度，特别是使用大型语言模型（LLM）如聊天生成预训练Transformer（ChatGPT）、Google Bard等交互式AI技术的接受度，是 Paramount importance。然而，现有的教育领域聊天机器人研究忽略了关键的行为相关方面，如优択性、创新性、不适感、不安全感、透明度、伦理、互动、参与度和准确性，这创造了一个重要的文献差距。为了填补这一差距，本研究使用部分最小二乘结构方程（PLS-SEM）调查教育领域聊天机器人的采用情况，考虑技术准备指数（TRI）和技术接受度模型（TAM）。通过五点Likert等级的数据收集，总共收集了185个答案，使用RStudio软件进行分析。我们设置了12个假设，以实现研究的目标。结果显示，优択性和创新性与使用容易性（PEOU）和有用性（PU）正相关，而不适感和不安全感则与PEOU相反，只有不安全感对PU具有负面影响。这些发现为未来技术设计师提供了灵感，把关于聊天机器人在教育上的采用和使用的用户行为因素揭露出来。
</details></li>
</ul>
<hr>
<h2 id="TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer"><a href="#TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer" class="headerlink" title="TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer"></a>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03622">http://arxiv.org/abs/2311.03622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Yamada, Marc Rigter, Jack Collins, Ingmar Posner</li>
<li>for: 该论文目的是提出一种基于模型的RL方法来解决实际 робо太器中的视觉任务，并且提高模型之间的转移效果。</li>
<li>methods: 该论文使用了Distillation技术来实现模型之间的转移，特别是使用了教师模型和学生模型的设计来加速实际世界中的模型转移。</li>
<li>results: 实验表明，该方法可以比naive随机化和模型自由RL方法更高效地实现实际世界中的模型转移，并且可以提高任务性能。<details>
<summary>Abstract</summary>
Model-based RL is a promising approach for real-world robotics due to its improved sample efficiency and generalization capabilities compared to model-free RL. However, effective model-based RL solutions for vision-based real-world applications require bridging the sim-to-real gap for any world model learnt. Due to its significant computational cost, standard domain randomisation does not provide an effective solution to this problem. This paper proposes TWIST (Teacher-Student World Model Distillation for Sim-to-Real Transfer) to achieve efficient sim-to-real transfer of vision-based model-based RL using distillation. Specifically, TWIST leverages state observations as readily accessible, privileged information commonly garnered from a simulator to significantly accelerate sim-to-real transfer. Specifically, a teacher world model is trained efficiently on state information. At the same time, a matching dataset is collected of domain-randomised image observations. The teacher world model then supervises a student world model that takes the domain-randomised image observations as input. By distilling the learned latent dynamics model from the teacher to the student model, TWIST achieves efficient and effective sim-to-real transfer for vision-based model-based RL tasks. Experiments in simulated and real robotics tasks demonstrate that our approach outperforms naive domain randomisation and model-free methods in terms of sample efficiency and task performance of sim-to-real transfer.
</details>
<details>
<summary>摘要</summary>
模型基于RL是实世界机器人控制中有前途的方法，因其在样本效率和泛化能力方面比模型自由RL更为优秀。然而，实世界应用中的视觉基于模型基于RL解决方案需要跨 simulate-to-real gap 的桥梁。由于其计算成本较高，标准的领域随机化不能提供有效的解决方案。本文提出了TWIST（教师学生世界模型填充法），以实现efficient的 simulate-to-real 传输。 Specifically, TWIST 利用了 readily accessible 的状态信息，通常来自 simulator 中获得的特权信息，以加速 simulate-to-real 传输。具体来说，一个教师世界模型在状态信息上进行高效地训练，同时收集了随机化的图像观察数据。教师世界模型然后监督学生世界模型，该模型接受随机化的图像观察数据作为输入。通过填充学生模型中学习的 latent dynamics 模型，TWIST 实现了高效和有效的 simulate-to-real 传输。实验表明，我们的方法在 simulated 和实际机器人任务中超过了Randomization 和模型自由方法的样本效率和任务性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/cs.AI_2023_11_07/" data-id="clpxp6bxg006zee885n07cbub" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/9/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/11/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
