
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/8/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.SD_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/cs.SD_2023_08_07/">cs.SD - 2023-08-07 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Noise-Control-based-on-the-Momentum-Multichannel-Normalized-Filtered-x-Least-Mean-Square-Algorithm"><a href="#Active-Noise-Control-based-on-the-Momentum-Multichannel-Normalized-Filtered-x-Least-Mean-Square-Algorithm" class="headerlink" title="Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm"></a>Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03684">http://arxiv.org/abs/2308.03684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyuan Shi, Woon-Seng Gan, Bhan Lam, Shulin Wen, Xiaoyi Shen</li>
<li>for: 实现多通道活动噪声控制（MCANC）中的广泛噪声抑制区域。</li>
<li>methods: 使用了Filter-x最小均方（FxLMS）算法，但是它的对应速度较慢，不适合处理快速变化的噪声，如堆积噪声。此外，噪声功率的变化也会对算法的稳定性产生负面影响。</li>
<li>results: 通过与振踪方法结合，实现了对MCANC中的噪声控制的有效控制，并且加速了算法的步进调整。在实际应用中，通过使用多通道噪声控制窗口来控制机器噪声。<details>
<summary>Abstract</summary>
Multichannel active noise control (MCANC) is widely utilized to achieve significant noise cancellation area in the complicated acoustic field. Meanwhile, the filter-x least mean square (FxLMS) algorithm gradually becomes the benchmark solution for the implementation of MCANC due to its low computational complexity. However, its slow convergence speed more or less undermines the performance of dealing with quickly varying disturbances, such as piling noise. Furthermore, the noise power variation also deteriorates the robustness of the algorithm when it adopts the fixed step size. To solve these issues, we integrated the normalized multichannel FxLMS with the momentum method, which hence, effectively avoids the interference of the primary noise power and accelerates the convergence of the algorithm. To validate its effectiveness, we deployed this algorithm in a multichannel noise control window to control the real machine noise.
</details>
<details>
<summary>摘要</summary>
多通道活动噪声控制（MCANC）广泛应用于复杂的噪声场中实现显著的噪声抑制面积。同时，Filter-x最小二乘（FxLMS）算法逐渐成为MCANC的实现标准方案，主要是因为它的计算复杂度较低。然而，它的慢速对应变化的干扰有很大的影响，如堆叠噪声。此外，噪声功率变化也会对算法的稳定性产生负面影响，特别是当采用固定步长时。为解决这些问题，我们将normalized multichannel FxLMS与旋转方法相结合，从而有效避免了主要噪声功率的干扰，并加速了算法的收敛速度。为验证其效果，我们在多通道噪声控制窗口中应用了这种算法，控制了实际机器的噪声。
</details></li>
</ul>
<hr>
<h2 id="AudioVMAF-Audio-Quality-Prediction-with-VMAF"><a href="#AudioVMAF-Audio-Quality-Prediction-with-VMAF" class="headerlink" title="AudioVMAF: Audio Quality Prediction with VMAF"></a>AudioVMAF: Audio Quality Prediction with VMAF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03437">http://arxiv.org/abs/2308.03437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arijit Biswas, Harald Mundt</li>
<li>for: 这个论文的目的是提出一种基于现有VMAF的听力 inspirited frontend，用于创建参考视频和编码spectrograms，并扩展VMAF以测试编码音质。</li>
<li>methods: 该系统使用了图像复制来进一步提高预测精度，特别是在存在带限 anchors 时。</li>
<li>results: 提议方法在现有视觉质量特征的抽取改进下显著 OUTPERFORMS 所有现有的视觉质量特征重新定义为音频质量特征，并且在一个专门为音频质量metric（ViSQOL-v3 [4]）也 inspirited from the image domain 上显示了7.8%和2.0%的Pearson和Spearman排名相关度系数的显著提高。<details>
<summary>Abstract</summary>
Video Multimethod Assessment Fusion (VMAF) [1], [2], [3] is a popular tool in the industry for measuring coded video quality. In this study, we propose an auditory-inspired frontend in existing VMAF for creating videos of reference and coded spectrograms, and extended VMAF for measuring coded audio quality. We name our system AudioVMAF. We demonstrate that image replication is capable of further enhancing prediction accuracy, especially when band-limited anchors are present. The proposed method significantly outperforms all existing visual quality features repurposed for audio, and even demonstrates a significant overall improvement of 7.8% and 2.0% of Pearson and Spearman rank correlation coefficient, respectively, over a dedicated audio quality metric (ViSQOL-v3 [4]) also inspired from the image domain.
</details>
<details>
<summary>摘要</summary>
видео多方法评估融合（VMAF）[1], [2], [3] 是行业中常用的视频质量测试工具。在这项研究中，我们提议在现有VMAF中添加音频引入的前端，并将扩展VMAF用于测试编码音频质量。我们称之为AudioVMAF。我们发现，图像复制可以进一步提高预测精度，特别是当存在带限 anchors 时。我们的方法在现有视觉质量特征的抽取方面进行了改进，并且显著超过了所有抽取于音频领域的视觉质量特征，以及专门为音频质量指标（ViSQOL-v3 [4]) 的7.8%和2.0%的普森和斯宾塞相关系数。
</details></li>
</ul>
<hr>
<h2 id="Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation"><a href="#Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation" class="headerlink" title="Improving Deep Attractor Network by BGRU and GMM for Speech Separation"></a>Improving Deep Attractor Network by BGRU and GMM for Speech Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03332">http://arxiv.org/abs/2308.03332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rawad Melhem, Assef Jafar, Riad Hamadeh</li>
<li>for: 提高Speech separation技术的简化和效率，使其更适合实际应用。</li>
<li>methods: 使用Bidirectional Gated neural network (BGRU)取代BLSTM，并使用Gaussian Mixture Model (GMM)作为聚类算法。</li>
<li>results: 在TIMIT corpus上评估系统，SDR和PESQ scores分别为12.3 dB和2.94，比原始DANet模型更好，同时减少了20.7%和17.9%的参数和训练时间。<details>
<summary>Abstract</summary>
Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.
</details>
<details>
<summary>摘要</summary>
深度吸引网络（DANet）是语音分离领域的Current Technique，使用双向长短期记忆（BLSTM），但模型复杂度很高。本文提出了简化了的DANet模型，使用双向闭合神经网络（BGRU）而不是BLSTM。在DANet中使用 Gaussian Mixture Model（GMM）作为聚类算法，以降低复杂度并提高学习速度和准确性。用于评价模型的度量包括Signal to Distortion Ratio（SDR）、Signal to Interference Ratio（SIR）、Signal to Artifact Ratio（SAR）以及Perceptual Evaluation Speech Quality（PESQ）分数。使用TIMIT corpus中的两个说话者混合数据集评估提出的模型，系统实现了12.3 dB和2.94的SDR和PESQ分数，分别比原始DANet模型更好。此外，模型的参数数量和训练时间都下降了20.7%和17.9%。模型应用于混合阿拉伯语音信号上，结果比英语更好。
</details></li>
</ul>
<hr>
<h2 id="SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability"><a href="#SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability" class="headerlink" title="SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability"></a>SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03266">http://arxiv.org/abs/2308.03266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/r1ckshi/seaco-paraformer">https://github.com/r1ckshi/seaco-paraformer</a></li>
<li>paper_authors: Xian Shi, Yexin Yang, Zerui Li, Shiliang Zhang</li>
<li>for: 实现ASR系统中的热词自定义，提高ASR系统的准确性和效率。</li>
<li>methods: 提出了Semantic-augmented Contextual-Paraformer（SeACo-Paraformer）模型，结合了AED-based模型的精度、NAR模型的效率，并且具有出色的Contextualization能力。</li>
<li>results: 在50,000小时的工业大数据实验中，提出的模型比强基准模型在自定义和一般ASR任务中表现更好，并且探索了一种高效的大规模来处理热词检查的方法。<details>
<summary>Abstract</summary>
Hotword customization is one of the important issues remained in ASR field - it is of value to enable users of ASR systems to customize names of entities, persons and other phrases. The past few years have seen both implicit and explicit modeling strategies for ASR contextualization developed. While these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness. In this paper we propose Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with flexible and effective hotword customization ability. It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Besides, we explore an efficient way to filter large scale incoming hotwords for further improvement. The source codes and industrial models proposed and compared are all opened as well as two hotword test sets.
</details>
<details>
<summary>摘要</summary>
<SYS>    <LANGUAGE_MODEL>        <NAME>Simplified Chinese</NAME>        <PATH>/path/to/model</PATH>    </LANGUAGE_MODEL></SYS>这是一个 ASR 领域中的重要问题 - 允许使用者自定义名词、人名和其他短语。过去几年，有内在和外在模型化策略被开发出来解决这个问题。这些方法优秀地表现，但仍然存在一些缺陷，如效果不稳定。在这篇文章中，我们提出 Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) 一种新的 NAR 基于 ASR 系统，具有轻松实现和有效的自定义热词能力。它结合了 AED-based 模型的精度和 NAR 模型的效率，并且在Contextualization 方面表现出色。在50,000小时的工业大数据中，我们的提案模型比强大的基准模型在自定义和一般 ASR 任务中表现出色。此外，我们还探索了一种高效的方法来筛选大规模的进来热词，以进一步提高效能。我们提供了所有的代码和工业模型，以及两个热词测试集。
</details></li>
</ul>
<hr>
<h2 id="Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals"><a href="#Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals" class="headerlink" title="Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals"></a>Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03226">http://arxiv.org/abs/2308.03226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Farhad Javanmardi, Paavo Alku</li>
<li>for: 本研究旨在 investigate the effectiveness of simultaneously-recorded speech and neck surface accelerometer (NSA) signals in the classification of voice quality (breathy, modal, and pressed) using deep learning-based features and a support vector machine (SVM) as well as a convolutional neural network (CNN) as classifiers.</li>
<li>methods: 本研究使用了三种自动学习模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）的自然语言处理特征，以及SVM和CNN分类器。另外，使用了两种信号处理方法（闭合相位滤波和零频滤波）来估计颤腔源波形从语音和NSA信号中提取的特征。</li>
<li>results: 研究发现NSA输入对分类任务的性能更高于语音输入。另外，使用自动学习模型生成的特征对于语音和NSA输入都显示了更高的分类精度，而使用HuBERT特征则表现更好。<details>
<summary>Abstract</summary>
Prior studies in the automatic classification of voice quality have mainly studied the use of the acoustic speech signal as input. Recently, a few studies have been carried out by jointly using both speech and neck surface accelerometer (NSA) signals as inputs, and by extracting MFCCs and glottal source features. This study examines simultaneously-recorded speech and NSA signals in the classification of voice quality (breathy, modal, and pressed) using features derived from three self-supervised pre-trained models (wav2vec2-BASE, wav2vec2-LARGE, and HuBERT) and using a SVM as well as CNNs as classifiers. Furthermore, the effectiveness of the pre-trained models is compared in feature extraction between glottal source waveforms and raw signal waveforms for both speech and NSA inputs. Using two signal processing methods (quasi-closed phase (QCP) glottal inverse filtering and zero frequency filtering (ZFF)), glottal source waveforms are estimated from both speech and NSA signals. The study has three main goals: (1) to study whether features derived from pre-trained models improve classification accuracy compared to conventional features (spectrogram, mel-spectrogram, MFCCs, i-vector, and x-vector), (2) to investigate which of the two modalities (speech vs. NSA) is more effective in the classification task with pre-trained model-based features, and (3) to evaluate whether the deep learning-based CNN classifier can enhance the classification accuracy in comparison to the SVM classifier. The results revealed that the use of the NSA input showed better classification performance compared to the speech signal. Between the features, the pre-trained model-based features showed better classification accuracies, both for speech and NSA inputs compared to the conventional features. It was also found that the HuBERT features performed better than the wav2vec2-BASE and wav2vec2-LARGE features.
</details>
<details>
<summary>摘要</summary>
前研究主要是使用语音信号作为输入，做自动识别声音质量的研究。近年来，一些研究开始将语音信号和脖子表面加速器（NSA）信号同时录制，并提取MFCCs和喉咙源特征。本研究用三个自我超vised模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）提取特征，并使用支持向量机（SVM）和卷积神经网络（CNN）作为分类器。此外，对于语音和NSA输入，对喉咙源波形和原始信号波形进行预处理，并使用 quasi-closed phase（QCP）预测和零频 filtering（ZFF）来估算喉咙源波形。研究的三个主要目标是：（1）研究 Whether features derived from pre-trained models improve classification accuracy compared to conventional features（spectrogram、mel-spectrogram、MFCCs、i-vector、x-vector），（2）investigate which modality（speech vs. NSA）is more effective in the classification task with pre-trained model-based features，（3）evaluate whether the deep learning-based CNN classifier can enhance the classification accuracy in comparison to the SVM classifier。结果表明，使用NSA输入的 classification 性能比语音信号更好。此外，使用 pre-trained model-based features 也比使用传统特征更好，同时 HuBERT 特征也比 wav2vec2-BASE 和 wav2vec2-LARGE 特征更好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/cs.SD_2023_08_07/" data-id="clltau93q008mcr883mzqafir" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/eess.IV_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/eess.IV_2023_08_07/">eess.IV - 2023-08-07 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe"><a href="#SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe" class="headerlink" title="SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe"></a>SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03586">http://arxiv.org/abs/2308.03586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nafiseh Kakhani, Moien Rangzan, Ali Jamali, Sara Attarchi, Seyed Kazem Alavipanah, Thomas Scholten</li>
<li>for: 这项研究旨在提高数字土壤地图（DSM）技术，尤其是在使用深度学习（DL）方法来预测土壤有机碳（SOC）的空间特征。</li>
<li>methods: 该研究提出了一种新的架构， combining 基于卷积神经网络（CNN）的空间信息和基于长短时间记忆（LSTM）网络的气候时序信息，以预测欧洲各地的SOC。该模型使用了一组全面的环境特征，包括兰达特-8图像、地形、远程感知指数和气候时序序列，作为输入特征。</li>
<li>results: 研究结果表明，提出的方框比常用的多项Random Forest方法（ML）更高效，具有较低的根圆平方误差（RMSE）。这种模型是一种可靠的SOC预测工具，可以应用于其他土壤特征预测，从而为土地管理和决策过程提供更准确的信息。<details>
<summary>Abstract</summary>
Digital soil mapping (DSM) is an advanced approach that integrates statistical modeling and cutting-edge technologies, including machine learning (ML) methods, to accurately depict soil properties and their spatial distribution. Soil organic carbon (SOC) is a crucial soil attribute providing valuable insights into soil health, nutrient cycling, greenhouse gas emissions, and overall ecosystem productivity. This study highlights the significance of spatial-temporal deep learning (DL) techniques within the DSM framework. A novel architecture is proposed, incorporating spatial information using a base convolutional neural network (CNN) model and spatial attention mechanism, along with climate temporal information using a long short-term memory (LSTM) network, for SOC prediction across Europe. The model utilizes a comprehensive set of environmental features, including Landsat-8 images, topography, remote sensing indices, and climate time series, as input features. Results demonstrate that the proposed framework outperforms conventional ML approaches like random forest commonly used in DSM, yielding lower root mean square error (RMSE). This model is a robust tool for predicting SOC and could be applied to other soil properties, thereby contributing to the advancement of DSM techniques and facilitating land management and decision-making processes based on accurate information.
</details>
<details>
<summary>摘要</summary>
数字土壤地图（DSM）是一种先进的方法，它将统计模型和前沿技术，包括机器学习（ML）方法，结合起来准确地描述土壤属性和其空间分布。土壤有机碳（SOC）是一个重要的土壤特征，它提供了valuable insights into soil health, nutrient cycling, greenhouse gas emissions, and overall ecosystem productivity.这项研究强调了在 DSM 框架中使用空间-时间深度学习（DL）技术的重要性。该研究提出了一种新的建议，其包括在基于 convolutional neural network（CNN）模型和空间注意机制的基础上，以及使用 long short-term memory（LSTM）网络来预测欧洲各地的 SOC。该模型使用了包括 Landsat-8 图像、地形、 remote sensing 指标和气候时序序列在内的全面的环境特征作为输入特征。结果表明，提议的框架在与常见的 ML 方法如随机森林相比，具有较低的根圆平均误差（RMSE）。这种模型是一种准确预测 SOC 的工具，可以应用于其他土壤属性，从而为 DSM 技术的发展和土地管理决策提供支持。
</details></li>
</ul>
<hr>
<h2 id="Quantitative-MR-Image-Reconstruction-using-Parameter-Specific-Dictionary-Learning-with-Adaptive-Dictionary-Size-and-Sparsity-Level-Choice"><a href="#Quantitative-MR-Image-Reconstruction-using-Parameter-Specific-Dictionary-Learning-with-Adaptive-Dictionary-Size-and-Sparsity-Level-Choice" class="headerlink" title="Quantitative MR Image Reconstruction using Parameter-Specific Dictionary Learning with Adaptive Dictionary-Size and Sparsity-Level Choice"></a>Quantitative MR Image Reconstruction using Parameter-Specific Dictionary Learning with Adaptive Dictionary-Size and Sparsity-Level Choice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03460">http://arxiv.org/abs/2308.03460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Kofler, Kirsten Miriam Kerkering, Laura Göschel, Ariane Fillmer, Cristoph Kolbitsch</li>
<li>for: 这种方法是用于量子磁共振成像（QMRI）中的参数地图重建的。</li>
<li>methods: 方法使用了字典学习（DL）和稀疏编码（SC）算法自动计算最佳字典大小和稀疏程度，并在不同参数地图之间进行自适应调整。</li>
<li>results: 该方法在RMSE和PSNR方面胜过MAP、TV、Wl和Sh方法，并且与DL+Fit方法具有相似或更好的效果，同时加速了重建过程约7倍。In English, this means:</li>
<li>for: This method is proposed for the reconstruction of parameter maps in Quantitative Magnetic Resonance Imaging (QMRI).</li>
<li>methods: The method uses dictionary learning (DL) and sparse coding (SC) algorithms to automatically estimate the optimal dictionary size and sparsity level for each parameter map, and adaptively adjusts the parameters between different maps.</li>
<li>results: The proposed method outperforms the compared methods (MAP, TV, Wl, and Sh) in terms of RMSE and PSNR, and has similar or better effects as the DL+Fit method, while significantly accelerating the reconstruction process by a factor of approximately seven.<details>
<summary>Abstract</summary>
Objective: We propose a method for the reconstruction of parameter-maps in Quantitative Magnetic Resonance Imaging (QMRI).   Methods: Because different quantitative parameter-maps differ from each other in terms of local features, we propose a method where the employed dictionary learning (DL) and sparse coding (SC) algorithms automatically estimate the optimal dictionary-size and sparsity level separately for each parameter-map. We evaluated the method on a $T_1$-mapping QMRI problem in the brain using the BrainWeb data as well as in-vivo brain images acquired on an ultra-high field 7T scanner. We compared it to a model-based acceleration for parameter mapping (MAP) approach, other sparsity-based methods using total variation (TV), Wavelets (Wl) and Shearlets (Sh), and to a method which uses DL and SC to reconstruct qualitative images, followed by a non-linear (DL+Fit).   Results: Our algorithm surpasses MAP, TV, Wl and Sh in terms of RMSE and PSNR. It yields better or comparable results to DL+Fit by additionally significantly accelerating the reconstruction by a factor of approximately seven.   Conclusion: The proposed method outperforms the reported methods of comparison and yields accurate $T_1$-maps. Although presented for $T_1$-mapping in the brain, our method's structure is general and thus most probably also applicable for the the reconstruction of other quantitative parameters in other organs.   Significance: From a clinical perspective, the obtained $T_1$-maps could be utilized to differentiate between healthy subjects and patients with Alzheimer's disease. From a technical perspective, the proposed unsupervised method could be employed to obtain ground-truth data for the development of data-driven methods based on supervised learning.+
</details>
<details>
<summary>摘要</summary>
Methods: Different quantitative parameter-maps have unique local features, so we use dictionary learning (DL) and sparse coding (SC) algorithms to automatically estimate the optimal dictionary size and sparsity level for each parameter-map. We evaluated our method on a $T_1$-mapping QMRI problem in the brain using the BrainWeb dataset and in-vivo brain images acquired on a 7T scanner. We compared our method to a model-based acceleration for parameter mapping (MAP) approach, as well as other sparsity-based methods using total variation (TV), wavelets (Wl), and shearlets (Sh). We also compared our method to a method that uses DL and SC to reconstruct qualitative images and then uses non-linear registration (DL+Fit).Results: Our method outperformed MAP, TV, Wl, and Sh in terms of root mean squared error (RMSE) and peak signal-to-noise ratio (PSNR). It also yielded better or comparable results to DL+Fit, while significantly accelerating the reconstruction process by a factor of approximately seven.Conclusion: Our proposed method outperforms previous methods and provides accurate $T_1$-maps. Although we focused on $T_1$-mapping in the brain, our method's structure is general and can be applied to the reconstruction of other quantitative parameters in other organs.Significance: From a clinical perspective, the obtained $T_1$-maps could be used to differentiate between healthy subjects and patients with Alzheimer's disease. From a technical perspective, the proposed unsupervised method could be used to obtain ground-truth data for the development of data-driven methods based on supervised learning.
</details></li>
</ul>
<hr>
<h2 id="Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising"><a href="#Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising" class="headerlink" title="Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising"></a>Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03448">http://arxiv.org/abs/2308.03448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srameo/led">https://github.com/srameo/led</a></li>
<li>paper_authors: Xin Jin, Jia-Wen Xiao, Ling-Hao Han, Chunle Guo, Ruixun Zhang, Xialei Liu, Chongyi Li</li>
<li>For: This paper is written for RAW image denoising under extremely low-light environments, and it aims to overcome the limitations of calibration-based methods.* Methods: The proposed method uses a calibration-free pipeline, which adapts to a target camera with few-shot paired data and fine-tuning. The method also includes well-designed structural modification during both stages to alleviate the domain gap between synthetic and real noise.* Results: The proposed method achieves superior performance over other calibration-based methods with 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations.Here’s the format you requested:* For: 这篇论文是为了对极低光环境下的 RAW 图像推断而写的，并且想要超越传统的测试基于方法。* Methods: 提案的方法使用了没有单位的管道，可以适应目标摄像头只需要几对对照数据和微调。这个方法还包括了妥善的结构修改在两个阶段，以解决实际和 sintetic 噪声之间的领域差。* Results: 提案的方法在对其他传统基于测试方法进行比较时，获得了更好的性能，仅需要2对对照数据和0.5%迭代。<details>
<summary>Abstract</summary>
Calibration-based methods have dominated RAW image denoising under extremely low-light environments. However, these methods suffer from several main deficiencies: 1) the calibration procedure is laborious and time-consuming, 2) denoisers for different cameras are difficult to transfer, and 3) the discrepancy between synthetic noise and real noise is enlarged by high digital gain. To overcome the above shortcomings, we propose a calibration-free pipeline for Lighting Every Drakness (LED), regardless of the digital gain or camera sensor. Instead of calibrating the noise parameters and training repeatedly, our method could adapt to a target camera only with few-shot paired data and fine-tuning. In addition, well-designed structural modification during both stages alleviates the domain gap between synthetic and real noise without any extra computational cost. With 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations, our method achieves superior performance over other calibration-based methods. Our code is available at https://github.com/Srameo/LED .
</details>
<details>
<summary>摘要</summary>
几种基于准确的方法在极低照度环境下进行RAW图像降噪已经占据了主导地位。然而，这些方法受到以下主要缺点的影响：1）准确程度很低，2）适用于不同摄像头的降噪器难以传输，3）高度数字增量使得实际噪声与synthetic噪声之间的差异变大。为了超越这些缺点，我们提出了不需要准确程度的渠道，即Lighting Every Drakness（LED）。相比准确程度的准备和重复训练，我们的方法只需要几次对应的配对数据和微调就能适应目标摄像头。此外，我们在两个阶段中设计了 estructural modification，以alleviate the domain gap between synthetic and real noise without any extra computational cost。通过使用6对每个额外数字增量（共计24对）和0.5%的迭代，我们的方法可以在其他准确基于方法之上达到更高的性能。我们的代码可以在https://github.com/Srameo/LED上找到。
</details></li>
</ul>
<hr>
<h2 id="Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis"><a href="#Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis" class="headerlink" title="Energy-Guided Diffusion Model for CBCT-to-CT Synthesis"></a>Energy-Guided Diffusion Model for CBCT-to-CT Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03354">http://arxiv.org/abs/2308.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjie Fu, Xia Li, Xiuding Cai, Dong Miao, Yu Yao, Yali Shen</li>
<li>for: 提高CBCT图像质量和Hounsfield单位精度，以便更好地用于放射治疗。</li>
<li>methods: 基于能量导向扩散模型（EGDiff），从CBCT图像生成Synthetic CT（sCT）。</li>
<li>results: 对胸腺癌数据集进行实验，得到了优秀的性能结果，包括平均绝对错误26.87±6.14HU、结构相似度指标0.850±0.03、峰值信号噪声比19.83±1.39dB和正常化交叉相关指标0.874±0.04。这些结果表明，我们的方法在精度和视觉质量方面都有所提高，生成了superior的sCT图像。<details>
<summary>Abstract</summary>
Cone Beam CT (CBCT) plays a crucial role in Adaptive Radiation Therapy (ART) by accurately providing radiation treatment when organ anatomy changes occur. However, CBCT images suffer from scatter noise and artifacts, making relying solely on CBCT for precise dose calculation and accurate tissue localization challenging. Therefore, there is a need to improve CBCT image quality and Hounsfield Unit (HU) accuracy while preserving anatomical structures. To enhance the role and application value of CBCT in ART, we propose an energy-guided diffusion model (EGDiff) and conduct experiments on a chest tumor dataset to generate synthetic CT (sCT) from CBCT. The experimental results demonstrate impressive performance with an average absolute error of 26.87$\pm$6.14 HU, a structural similarity index measurement of 0.850$\pm$0.03, a peak signal-to-noise ratio of the sCT of 19.83$\pm$1.39 dB, and a normalized cross-correlation of the sCT of 0.874$\pm$0.04. These results indicate that our method outperforms state-of-the-art unsupervised synthesis methods in accuracy and visual quality, producing superior sCT images.
</details>
<details>
<summary>摘要</summary>
cone beam CT (CBCT) 在适应辐射疗法 (ART) 中发挥重要作用，准确地提供辐射治疗当器官解剖结构发生变化时。然而，CBCT图像受到散射噪声和artefacts的影响，使凭借CBCT alone 的精度计算和精确地本结构定位变得困难。因此，需要改进CBCT图像质量和温顺单元 (HU) 精度，保持器官结构的完整性。为了提高 CBCT 在 ART 中的角色和应用价值，我们提议一种能量指导扩散模型 (EGDiff)，并在胸腔肿瘤数据集上进行实验，将 CBCT 转换成 Synthetic CT (sCT)。实验结果表明，我们的方法在精度和视觉质量方面具有卓越表现，与现有的无监督杂合 Synthesis 方法相比，具有更高的 HU 精度、更高的结构相似度、更高的峰信号噪声比和更高的正规化交叉相似度。这些结果表明，我们的方法可以生成高质量的 sCT 图像，超过现有的方法。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining"><a href="#A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining" class="headerlink" title="A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining"></a>A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03340">http://arxiv.org/abs/2308.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Wang, Wei Li</li>
<li>for: 图像排除雨纹效果的提升</li>
<li>methods: 使用深度学习方法进行图像排除雨纹</li>
<li>results: 实现了高质量的图像排除雨纹效果Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the effectiveness of image deraining by using deep learning methods.</li>
<li>methods: The paper uses deep learning techniques, specifically a deep neural network, to remove rain streaks from degraded images.</li>
<li>results: The paper achieves high-quality image deraining results by using these methods.<details>
<summary>Abstract</summary>
Image deraining is a challenging task that involves restoring degraded images affected by rain streaks.
</details>
<details>
<summary>摘要</summary>
图像抑雨是一项复杂的任务，涉及到修复受到雨束纹的图像。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/eess.IV_2023_08_07/" data-id="clltau95800djcr88bta7hh2l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/cs.LG_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/06/cs.LG_2023_08_06/">cs.LG - 2023-08-06 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AI-GOMS-Large-AI-Driven-Global-Ocean-Modeling-System"><a href="#AI-GOMS-Large-AI-Driven-Global-Ocean-Modeling-System" class="headerlink" title="AI-GOMS: Large AI-Driven Global Ocean Modeling System"></a>AI-GOMS: Large AI-Driven Global Ocean Modeling System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03152">http://arxiv.org/abs/2308.03152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Xiong, Yanfei Xiang, Hao Wu, Shuyi Zhou, Yuze Sun, Muyuan Ma, Xiaomeng Huang</li>
<li>for: 这个论文旨在提出一种基于人工智能的全球海洋模型系统（AI-GOMS），用于准确和高效地预测全球海洋日常变化。</li>
<li>methods: 该模型系统包括基本海洋变量预测的贝叶克自适应网络结构，以及包括地方下降、波解码和生物化交互的轻量级精度模型。</li>
<li>results: 该模型在30天预测全球海洋基本变量（15层深度）的方面达到了最佳性能，并能够模拟kuroshio海域的 mezoscale旋涡和赤道太平洋海洋层分化。<details>
<summary>Abstract</summary>
Ocean modeling is a powerful tool for simulating the physical, chemical, and biological processes of the ocean, which is the foundation for marine science research and operational oceanography. Modern numerical ocean modeling mainly consists of governing equations and numerical algorithms. Nonlinear instability, computational expense, low reusability efficiency and high coupling costs have gradually become the main bottlenecks for the further development of numerical ocean modeling. Recently, artificial intelligence-based modeling in scientific computing has shown revolutionary potential for digital twins and scientific simulations, but the bottlenecks of numerical ocean modeling have not been further solved. Here, we present AI-GOMS, a large AI-driven global ocean modeling system, for accurate and efficient global ocean daily prediction. AI-GOMS consists of a backbone model with the Fourier-based Masked Autoencoder structure for basic ocean variable prediction and lightweight fine-tuning models incorporating regional downscaling, wave decoding, and biochemistry coupling modules. AI-GOMS has achieved the best performance in 30 days of prediction for the global ocean basic variables with 15 depth layers at 1/4{\deg} spatial resolution. Beyond the good performance in statistical metrics, AI-GOMS realizes the simulation of mesoscale eddies in the Kuroshio region at 1/12{\deg} spatial resolution and ocean stratification in the tropical Pacific Ocean. AI-GOMS provides a new backbone-downstream paradigm for Earth system modeling, which makes the system transferable, scalable and reusable.
</details>
<details>
<summary>摘要</summary>
海洋模型是一种强大的工具，用于模拟海洋物理、化学和生物过程，是marine science研究和操作 oceanography的基础。现代数值海洋模型主要由管理方程和数值算法组成。不线性不稳定、计算成本高、再利用率低和对接成本高逐渐成为数值海洋模型的主要瓶颈。在科学计算中，人工智能基于的模型已经展示了革命性的潜力，但数值海洋模型中的瓶颈问题还没有得到解决。在这里，我们介绍AI-GOMS，一个大型基于人工智能的全球海洋模型，用于准确和高效的全球海洋日常预测。AI-GOMS包括一个基本 ocean variable prediction的背bone模型，以及 incorporating regional downscaling、波动解码和生物化学结合模块的轻量级精度增强模型。AI-GOMS在30天预测全球海洋基本变量的15层深度分辨率下达到了最佳性能。除了在统计指标方面的好表现，AI-GOMS还实现了kuroshio区域的 mesoscale eddies 在1/12°的空间分辨率下的模拟，以及在 тропиical Pacific Ocean中的海洋层次分布。AI-GOMS提供了一个新的背部-下游模式，使系统可重用、可扩展和可重复使用。
</details></li>
</ul>
<hr>
<h2 id="Nest-DGIL-Nesterov-optimized-Deep-Geometric-Incremental-Learning-for-CS-Image-Reconstruction"><a href="#Nest-DGIL-Nesterov-optimized-Deep-Geometric-Incremental-Learning-for-CS-Image-Reconstruction" class="headerlink" title="Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction"></a>Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03807">http://arxiv.org/abs/2308.03807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanxiaohong/Nest-DGIL">https://github.com/fanxiaohong/Nest-DGIL</a></li>
<li>paper_authors: Xiaohong Fan, Yin Yang, Ke Chen, Yujie Feng, Jianping Zhang</li>
<li>for: 这种方法用于解决图像逆问题，包括高&#x2F;低频图像特征的学习能力和保证几何纹理细节的重建。</li>
<li>methods: 基于第二个奈斯特洛夫距离梯度优化的深度幂增量学习框架，包括普通的线性重建、几何增量学习、奈斯特洛夫加速和后处理。</li>
<li>results: 提出的方法可以快速收敛，并且可以避免中间重建结果落入不同几何分解域之外，同时也可以保证高&#x2F;低频图像特征的学习能力和几何纹理细节的重建。<details>
<summary>Abstract</summary>
Proximal gradient-based optimization is one of the most common strategies for solving image inverse problems as well as easy to implement. However, these techniques often generate heavy artifacts in image reconstruction. One of the most popular refinement methods is to fine-tune the regularization parameter to alleviate such artifacts, but it may not always be sufficient or applicable due to increased computational costs. In this work, we propose a deep geometric incremental learning framework based on second Nesterov proximal gradient optimization. The proposed end-to-end network not only has the powerful learning ability for high/low frequency image features,but also can theoretically guarantee that geometric texture details will be reconstructed from preliminary linear reconstruction.Furthermore, it can avoid the risk of intermediate reconstruction results falling outside the geometric decomposition domains and achieve fast convergence. Our reconstruction framework is decomposed into four modules including general linear reconstruction, cascade geometric incremental restoration, Nesterov acceleration and post-processing. In the image restoration step,a cascade geometric incremental learning module is designed to compensate for the missing texture information from different geometric spectral decomposition domains. Inspired by overlap-tile strategy, we also develop a post-processing module to remove the block-effect in patch-wise-based natural image reconstruction. All parameters in the proposed model are learnable,an adaptive initialization technique of physical-parameters is also employed to make model flexibility and ensure converging smoothly. We compare the reconstruction performance of the proposed method with existing state-of-the-art methods to demonstrate its superiority. Our source codes are available at https://github.com/fanxiaohong/Nest-DGIL.
</details>
<details>
<summary>摘要</summary>
近似梯度基于优化是解决图像反问题的一种最常见策略，易于实现，但它们经常产生重要的artefacts。一种常见的改进方法是调整正则化参数，以降低这些artefacts，但这并不总是可行或适用，因为它可能会增加计算成本。在这种工作中，我们提出了深度几何增量学习框架，基于第二个Nesterov proximal梯度优化。我们的提案的端到端网络不仅具有高/低频图像特征的强大学习能力，而且可以理论保证从初步线性重建中恢复几何纹理细节。此外，它可以避免初步重建结果落入不同几何分解域之外，并且可以快速收敛。我们的重建框架分为四个模块：一般线性重建、几何增量学习、Nesterov加速和后处理。在图像恢复阶段，我们设计了几何增量学习模块，以补做不同几何分解域中缺失的纹理信息。受到 overlap-tile 策略的启发，我们还开发了后处理模块，以去除 patch-wise 基于自然图像重建中的块效果。所有模型参数都是可学习的，并且我们采用了 adaptive 初始化技术，以确保模型的灵活性和平滑的收敛。我们与现有的状态 искусственного智能方法进行比较，以证明我们的方法的优越性。我们的源代码可以在 GitHub 上找到：https://github.com/fanxiaohong/Nest-DGIL。
</details></li>
</ul>
<hr>
<h2 id="Self-Directed-Linear-Classification"><a href="#Self-Directed-Linear-Classification" class="headerlink" title="Self-Directed Linear Classification"></a>Self-Directed Linear Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03142">http://arxiv.org/abs/2308.03142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis</li>
<li>for: 这个论文研究了在线分类中学习者在适应性下预测示例的顺序，以最小化总错误数。</li>
<li>methods: 论文使用了自适应选择示例顺序的方法，并研究了这种方法的力量。</li>
<li>results: 论文表明，在线分类中，适应性下预测示例的顺序可以实现较好的性能，比如最小化错误数。此外，论文还提供了两个主要结果：在random sampling的情况下，可以使用自适应选择示例顺序来预测整个数据集的 labels，而且这种方法的错误数与数据集的大小无关。<details>
<summary>Abstract</summary>
In online classification, a learner is presented with a sequence of examples and aims to predict their labels in an online fashion so as to minimize the total number of mistakes. In the self-directed variant, the learner knows in advance the pool of examples and can adaptively choose the order in which predictions are made. Here we study the power of choosing the prediction order and establish the first strong separation between worst-order and random-order learning for the fundamental task of linear classification. Prior to our work, such a separation was known only for very restricted concept classes, e.g., one-dimensional thresholds or axis-aligned rectangles.   We present two main results. If $X$ is a dataset of $n$ points drawn uniformly at random from the $d$-dimensional unit sphere, we design an efficient self-directed learner that makes $O(d \log \log(n))$ mistakes and classifies the entire dataset. If $X$ is an arbitrary $d$-dimensional dataset of size $n$, we design an efficient self-directed learner that predicts the labels of $99\%$ of the points in $X$ with mistake bound independent of $n$. In contrast, under a worst- or random-ordering, the number of mistakes must be at least $\Omega(d \log n)$, even when the points are drawn uniformly from the unit sphere and the learner only needs to predict the labels for $1\%$ of them.
</details>
<details>
<summary>摘要</summary>
在在线分类中，学习者会看到一串示例，并尝试预测它们的标签，以最小化总错误数量。在自适应变体中，学习者可以适应地选择预测的顺序。我们研究了预测顺序的选择力，并证明了在线分类的基本任务中，自适应学习的最差顺序和随机顺序之间存在首次强分化。在我们的工作前，这种分化只知道在非常限定的概念集合中，例如一维阈值或AXI正方形。我们提出了两个主要结果。如果$X$是一个$d$-维均匀随机分布的点集， then we design an efficient self-directed learner that makes $O(d \log \log n)$ mistakes and classifies the entire dataset.如果$X$是一个任意$d$-维数据集的Size $n$, then we design an efficient self-directed learner that predicts the labels of $99\%$ of the points in $X$ with mistake bound independent of $n$.相比之下，在最差或随机顺序下，错误数量至少为$\Omega(d \log n)$, even when the points are drawn uniformly from the unit sphere and the learner only needs to predict the labels for $1\%$ of them.
</details></li>
</ul>
<hr>
<h2 id="Iterative-Magnitude-Pruning-as-a-Renormalisation-Group-A-Study-in-The-Context-of-The-Lottery-Ticket-Hypothesis"><a href="#Iterative-Magnitude-Pruning-as-a-Renormalisation-Group-A-Study-in-The-Context-of-The-Lottery-Ticket-Hypothesis" class="headerlink" title="Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis"></a>Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03128">http://arxiv.org/abs/2308.03128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abu-Al Hassan</li>
<li>for: 这个论文探讨了深度神经网络（DNN）的复杂世界，特别关注了赢家票假设（LTH）。</li>
<li>methods: 这个论文使用了迭代幂额减小（IMP）法则，逐渐消除微型 weights，模拟 DNN 的步进学习。</li>
<li>results: 研究发现，winning ticket 可以在各种相似问题上达到类似性能，并且通过与物理学术 Renormalisation Group（RG）理论的联系，提高了 IMP 的理解。<details>
<summary>Abstract</summary>
This thesis delves into the intricate world of Deep Neural Networks (DNNs), focusing on the exciting concept of the Lottery Ticket Hypothesis (LTH). The LTH posits that within extensive DNNs, smaller, trainable subnetworks termed "winning tickets", can achieve performance comparable to the full model. A key process in LTH, Iterative Magnitude Pruning (IMP), incrementally eliminates minimal weights, emulating stepwise learning in DNNs. Once we identify these winning tickets, we further investigate their "universality". In other words, we check if a winning ticket that works well for one specific problem could also work well for other, similar problems. We also bridge the divide between the IMP and the Renormalisation Group (RG) theory in physics, promoting a more rigorous understanding of IMP.
</details>
<details>
<summary>摘要</summary>
这个论文探讨了深度神经网络（DNN）的复杂世界，专注于吸引人的抽签假设（LTH）。LTH认为，在广泛的DNN中，更小的、可训练的子网络“赢家票”可以达到相同的性能。我们在LTH中使用增量大小减少（IMP）来逐渐消除最小的 weights，模拟了DNN中的步进学习。一旦我们确定了这些赢家票，我们进一步调查它们的“通用性”。即我们检查一个赢家票在一个特定问题上表现良好是否可以在其他相似问题上表现良好。我们还将IMP与物理学中的 renormalization group（RG）理论相连接，以便更好地理解IMP。
</details></li>
</ul>
<hr>
<h2 id="Learning-Rate-Free-Learning-Dissecting-D-Adaptation-and-Probabilistic-Line-Search"><a href="#Learning-Rate-Free-Learning-Dissecting-D-Adaptation-and-Probabilistic-Line-Search" class="headerlink" title="Learning-Rate-Free Learning: Dissecting D-Adaptation and Probabilistic Line Search"></a>Learning-Rate-Free Learning: Dissecting D-Adaptation and Probabilistic Line Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03102">http://arxiv.org/abs/2308.03102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max McGuinness</li>
<li>for: 这 paper 探讨了两种最近的学习率优化方法在随机梯度下降中：D-Adaptation（arXiv:2301.07733）和 probabilistic line search（arXiv:1502.02846）。这些方法的目的是减轻选择初始学习率的负担，通过包含距离度量和 Gaussian 过程 posterior 估计，respectively。</li>
<li>methods: 这 paper 使用了 D-Adaptation 和 probabilistic line search 两种方法，它们都是为了优化学习率。D-Adaptation 方法使用了距离度量来选择最佳学习率，而 probabilistic line search 方法则使用了 Gaussian 过程 posterior 估计来优化学习率。</li>
<li>results: 这 paper 的结果表明，D-Adaptation 和 probabilistic line search 两种方法都可以减轻选择初始学习率的负担，并且可以提高模型的性能。具体来说，D-Adaptation 方法可以在不同的数据集上实现更好的性能，而 probabilistic line search 方法则可以在不同的学习率下实现更好的性能。<details>
<summary>Abstract</summary>
This paper explores two recent methods for learning rate optimisation in stochastic gradient descent: D-Adaptation (arXiv:2301.07733) and probabilistic line search (arXiv:1502.02846). These approaches aim to alleviate the burden of selecting an initial learning rate by incorporating distance metrics and Gaussian process posterior estimates, respectively. In this report, I provide an intuitive overview of both methods, discuss their shared design goals, and devise scope for merging the two algorithms.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文探讨了两种最近的学习率优化方法：D-Adaptation（arXiv:2301.07733）和概率线搜索（arXiv:1502.02846）。这两种方法都尝试减轻选择初始学习率的负担，通过 incorporating 距离度量和 Gaussian 过程 posterior 估计，分别。在这份报告中，我提供了这两种方法的直观概述，讨论了它们的共同设计目标，并探讨了将两个算法合并的可能性。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Coding-through-Iterative-Block-Leverage-Score-Sampling"><a href="#Gradient-Coding-through-Iterative-Block-Leverage-Score-Sampling" class="headerlink" title="Gradient Coding through Iterative Block Leverage Score Sampling"></a>Gradient Coding through Iterative Block Leverage Score Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03096">http://arxiv.org/abs/2308.03096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neophytos Charalambides, Mert Pilanci, Alfred Hero</li>
<li>for: 这篇论文是用于实现分布式计算环境中的线性回传运算加速。</li>
<li>methods: 论文使用了扩展的抽样得分组（Leverage Score Sampling），并将其应用于首项方法（Gradient Coding），以减少分布式计算网络中的延迟。</li>
<li>results: 论文获得了一个可以在分布式计算环境中实现线性回传运算的轻量级化码 computing方案，并且可以在当中获得一个具有抽样范围的 $\ell_2$ 子空间嵌入。<details>
<summary>Abstract</summary>
We generalize the leverage score sampling sketch for $\ell_2$-subspace embeddings, to accommodate sampling subsets of the transformed data, so that the sketching approach is appropriate for distributed settings. This is then used to derive an approximate coded computing approach for first-order methods; known as gradient coding, to accelerate linear regression in the presence of failures in distributed computational networks, \textit{i.e.} stragglers. We replicate the data across the distributed network, to attain the approximation guarantees through the induced sampling distribution. The significance and main contribution of this work, is that it unifies randomized numerical linear algebra with approximate coded computing, while attaining an induced $\ell_2$-subspace embedding through uniform sampling. The transition to uniform sampling is done without applying a random projection, as in the case of the subsampled randomized Hadamard transform. Furthermore, by incorporating this technique to coded computing, our scheme is an iterative sketching approach to approximately solving linear regression. We also propose weighting when sketching takes place through sampling with replacement, for further compression.
</details>
<details>
<summary>摘要</summary>
我们总结了权重评分抽样策略，以适应分布式设置，以便在分布式计算网络中使用抽样subset。这种策略可以用来 derivate一种精确的代码计算方法，称为梯度编码，以加速分布式计算中的线性回归，即在分布式计算网络中的慢速进程（即慢进程）。我们将数据复制到分布式网络中，以实现近似 garantess through the induced sampling distribution。这个研究的重要性和主要贡献在于，它将随机化数字线性代数与近似代码计算相结合，并通过均匀抽样实现$\ell_2$次元空间嵌入。在抽样过程中，我们不会应用随机投影，如Randomized Hadamard Transform中的subsampled抽样。此外，我们还提出了在抽样过程中使用权重，以进一步压缩数据。因此，我们的方案是一种迭代抽样策略，用于约等于解 linear regression。我们的方法可以在分布式计算中实现高效的线性回归解决方案，并且可以扩展到更复杂的机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Control-aware-echo-state-networks-Ca-ESN-for-the-suppression-of-extreme-events"><a href="#Control-aware-echo-state-networks-Ca-ESN-for-the-suppression-of-extreme-events" class="headerlink" title="Control-aware echo state networks (Ca-ESN) for the suppression of extreme events"></a>Control-aware echo state networks (Ca-ESN) for the suppression of extreme events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03095">http://arxiv.org/abs/2308.03095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Racca, Luca Magri</li>
<li>for: 防止非线性系统中的极端事件的发生</li>
<li>methods: 结合ESN和控制策略，如比例- интеграル-导数控制和模型预测控制，实现非线性系统的有效控制</li>
<li>results: 在混沌液体流中，使用Ca-ESN比传统方法减少极端事件的发生，提高控制效果，开启了非线性系统控制的新可能性<details>
<summary>Abstract</summary>
Extreme event are sudden large-amplitude changes in the state or observables of chaotic nonlinear systems, which characterize many scientific phenomena. Because of their violent nature, extreme events typically have adverse consequences, which call for methods to prevent the events from happening. In this work, we introduce the control-aware echo state network (Ca-ESN) to seamlessly combine ESNs and control strategies, such as proportional-integral-derivative and model predictive control, to suppress extreme events. The methodology is showcased on a chaotic-turbulent flow, in which we reduce the occurrence of extreme events with respect to traditional methods by two orders of magnitude. This works opens up new possibilities for the efficient control of nonlinear systems with neural networks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>极端事件是非线性系统中突然大幅度变化的状态或观测量，这些事件frequently occur in many scientific phenomena. 由于其暴力性，极端事件通常会带来不良后果，需要采取方法来预防这些事件的发生。在这种工作中，我们提出了控制意识 echo state network (Ca-ESN)，可以将ESNs和控制策略，如 proporциональ-integral-derivative 和模型预测控制，结合在一起，以降低极端事件的发生频率。我们在混沌-turbulent flow中应用了这种方法，并比传统方法降低了极端事件的发生频率二个数量级。这项工作开 up new possibilities for the efficient control of nonlinear systems with neural networks.Note: "极端事件" in Chinese is usually translated as "extreme events" or "outliers", but in the context of this text, it refers to sudden large-amplitude changes in the state or observables of chaotic nonlinear systems.
</details></li>
</ul>
<hr>
<h2 id="Visualization-of-Extremely-Sparse-Contingency-Table-by-Taxicab-Correspondence-Analysis-A-Case-Study-of-Textual-Data"><a href="#Visualization-of-Extremely-Sparse-Contingency-Table-by-Taxicab-Correspondence-Analysis-A-Case-Study-of-Textual-Data" class="headerlink" title="Visualization of Extremely Sparse Contingency Table by Taxicab Correspondence Analysis: A Case Study of Textual Data"></a>Visualization of Extremely Sparse Contingency Table by Taxicab Correspondence Analysis: A Case Study of Textual Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03079">http://arxiv.org/abs/2308.03079</a></li>
<li>repo_url: None</li>
<li>paper_authors: V. Choulakian, J. Allard</li>
<li>for: Visualization of extremely sparse ontingency tables</li>
<li>methods: Taxicab correspondence analysis, a robust variant of correspondence analysis</li>
<li>results: Visualization of an extremely sparse textual data set of size 590 by 8265 concerning fragments of 8 sacred books<details>
<summary>Abstract</summary>
We present an overview of taxicab correspondence analysis, a robust variant of correspondence analysis, for visualization of extremely sparse ontingency tables. In particular we visualize an extremely sparse textual data set of size 590 by 8265 concerning fragments of 8 sacred books recently introduced by Sah and Fokou\'e (2019) and studied quite in detail by (12 + 1) dimension reduction methods (t-SNE, UMAP, PHATE,...) by Ma, Sun and Zou (2022).
</details>
<details>
<summary>摘要</summary>
我们提供了taxicab对应分析的概述，这是对对应分析的一种稳定版本，用于可见化极稀疏的对应关系表。特别是我们使用了590行x8265列的极稀疏文本数据集，这些数据来自 sah和fokou（2019）所引入的8种圣书的片断，并且通过(12+1)维度减少方法（t-SNE、UMAP、PHATE等）进行了深入研究。这些研究由Ma、sun和Zou（2022）进行了。
</details></li>
</ul>
<hr>
<h2 id="Study-for-Performance-of-MobileNetV1-and-MobileNetV2-Based-on-Breast-Cancer"><a href="#Study-for-Performance-of-MobileNetV1-and-MobileNetV2-Based-on-Breast-Cancer" class="headerlink" title="Study for Performance of MobileNetV1 and MobileNetV2 Based on Breast Cancer"></a>Study for Performance of MobileNetV1 and MobileNetV2 Based on Breast Cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03076">http://arxiv.org/abs/2308.03076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuqi Yan</li>
<li>for: 这项实验的目的是比较 MobileNetV1 和 MobileNetV2 模型在分类乳腺癌图像方面的性能。</li>
<li>methods: 实验使用了 Kaggle 上下载的数据集，并对其进行Normalization。然后，使用了神经网络模型来学习数据集，找出图像的特征并判断乳腺癌。</li>
<li>results: 实验结果表明，在处理这个数据集时，MobileNetV1 模型表现较好， validation accuracy 和 overfit 也较高。<details>
<summary>Abstract</summary>
Artificial intelligence is constantly evolving and can provide effective help in all aspects of people's lives. The experiment is mainly to study the use of artificial intelligence in the field of medicine. The purpose of this experiment was to compare which of MobileNetV1 and MobileNetV2 models was better at detecting histopathological images of the breast downloaded at Kaggle. When the doctor looks at the pathological image, there may be errors that lead to errors in judgment, and the observation speed is slow. Rational use of artificial intelligence can effectively reduce the error of doctor diagnosis in breast cancer judgment and speed up doctor diagnosis. The dataset was downloaded from Kaggle and then normalized. The basic principle of the experiment is to let the neural network model learn the downloaded data set. Then find the pattern and be able to judge on your own whether breast tissue is cancer. In the dataset, benign tumor pictures and malignant tumor pictures have been classified, of which 198738 are benign tumor pictures and 78, 786 are malignant tumor pictures. After calling MobileNetV1 and MobileNetV2, the dataset is trained separately, the training accuracy and validation accuracy rate are obtained, and the image is drawn. It can be observed that MobileNetV1 has better validation accuracy and overfit during MobileNetV2 training. From the experimental results, it can be seen that in the case of processing this dataset, MobileNetV1 is much better than MobileNetV2.
</details>
<details>
<summary>摘要</summary>
人工智能不断发展，可以提供有效的帮助在人们的生活中。本实验的主要目的是研究人工智能在医学领域的应用。本实验的目的是比较MobileNetV1和MobileNetV2模型在Kaggle上下载的乳腺病理图像上的表现。医生查看病理图像时可能会出现错误，导致诊断错误， observation速度较慢。合理使用人工智能可以有效减少医生诊断乳腺癌判断中的错误，并加快医生诊断速度。数据集来自Kaggle，然后 норциали化。实验的基本原则是让神经网络模型学习下载的数据集。然后找出模式，并能够自己判断乳腺细胞是否为癌细胞。数据集中，恶性肿瘤图像和良性肿瘤图像已经分类，其中198738个是恶性肿瘤图像，78786个是良性肿瘤图像。在 MobileNetV1 和 MobileNetV2 之后，数据集被分别训练，并获得训练精度和验证精度率。图像也被画出来。可以看到，在处理这个数据集时，MobileNetV1 表现得更好。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-Epileptic-Seizure-Prediction-Exploring-Diverse-Pre-Processing-Techniques-and-Machine-Learning-Models"><a href="#Comparative-Analysis-of-Epileptic-Seizure-Prediction-Exploring-Diverse-Pre-Processing-Techniques-and-Machine-Learning-Models" class="headerlink" title="Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models"></a>Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05176">http://arxiv.org/abs/2308.05176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Simul Hasan Talukder, Rejwan Bin Sulaiman</li>
<li>for: 预测癫痫发作</li>
<li>methods: 使用五种机器学习模型（Random Forest、Decision Tree、Extra Trees、Logistic Regression、Gradient Boosting）对电enzephalogram数据进行预测</li>
<li>results: 结果显示，LR类ifier的准确率为56.95%，GB和DT都达到97.17%的准确率，RT达到98.99%的准确率，ET模型表现最佳，准确率达99.29%。<details>
<summary>Abstract</summary>
Epilepsy is a prevalent neurological disorder characterized by recurrent and unpredictable seizures, necessitating accurate prediction for effective management and patient care. Application of machine learning (ML) on electroencephalogram (EEG) recordings, along with its ability to provide valuable insights into brain activity during seizures, is able to make accurate and robust seizure prediction an indispensable component in relevant studies. In this research, we present a comprehensive comparative analysis of five machine learning models - Random Forest (RF), Decision Tree (DT), Extra Trees (ET), Logistic Regression (LR), and Gradient Boosting (GB) - for the prediction of epileptic seizures using EEG data. The dataset underwent meticulous preprocessing, including cleaning, normalization, outlier handling, and oversampling, ensuring data quality and facilitating accurate model training. These preprocessing techniques played a crucial role in enhancing the models' performance. The results of our analysis demonstrate the performance of each model in terms of accuracy. The LR classifier achieved an accuracy of 56.95%, while GB and DT both attained 97.17% accuracy. RT achieved a higher accuracy of 98.99%, while the ET model exhibited the best performance with an accuracy of 99.29%. Our findings reveal that the ET model outperformed not only the other models in the comparative analysis but also surpassed the state-of-the-art results from previous research. The superior performance of the ET model makes it a compelling choice for accurate and robust epileptic seizure prediction using EEG data.
</details>
<details>
<summary>摘要</summary>
《诊断和治疗精神疾病》中，有一种常见的神经疾病是癫痫症，它表现为不规则和难以预测的癫痫发作，因此需要准确的预测以提供有效的管理和患者护理。在这些研究中，我们使用机器学习（ML）技术对电энцефалограм（EEG）记录进行分析，并通过对大脑活动的描述来提供有价值的预测。本研究中，我们对五种机器学习模型进行了比较分析：Random Forest（RF）、Decision Tree（DT）、Extra Trees（ET）、Logistic Regression（LR）和Gradient Boosting（GB）。我们对EEG数据进行了仔细的处理，包括清洁、 нормализа、异常处理和扩充，以确保数据质量的高度。这些处理技术对模型的表现产生了重要的影响。我们的分析结果显示每个模型的准确率。LR分类器的准确率为56.95%，而GB和DT都达到了97.17%的准确率。RT达到了98.99%的准确率，而ET模型表现出了最佳的性能，准确率达99.29%。我们的发现表明ET模型不仅在 Comparative analysis中表现出色，还超越了过去研究中的状态归化结果。ET模型的优秀表现使其成为精确和可靠的癫痫发作预测的首选方法。
</details></li>
</ul>
<hr>
<h2 id="TARJAMAT-Evaluation-of-Bard-and-ChatGPT-on-Machine-Translation-of-Ten-Arabic-Varieties"><a href="#TARJAMAT-Evaluation-of-Bard-and-ChatGPT-on-Machine-Translation-of-Ten-Arabic-Varieties" class="headerlink" title="TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties"></a>TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03051">http://arxiv.org/abs/2308.03051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karima Kadaoui, Samar M. Magdy, Abdul Waheed, Md Tawkat Islam Khondaker, Ahmed Oumar El-Shangiti, El Moatez Billah Nagoudi, Muhammad Abdul-Mageed</li>
<li>for: 这篇论文主要是为了评估大型自然语言模型（LLMs）在不同的阿拉伯语种中的翻译能力。</li>
<li>methods: 这篇论文使用了Google Bard和OpenAI ChatGPT两个模型，并对这两个模型在十种阿拉伯语种中的翻译能力进行了全面的评估。</li>
<li>results: 研究发现，LLMs在一些阿拉伯语种中表现不佳，特别是对于没有充足公共数据的语种，如阿尔及利亚和毛里塔尼亚的方言。然而，它们在更常见的方言中表现较为满意，尽管有时会落后于现有的商业系统 like Google Translate。此外，研究还发现，Bard在翻译任务中遵循人类指示的能力有限。<details>
<summary>Abstract</summary>
Large language models (LLMs) finetuned to follow human instructions have recently emerged as a breakthrough in AI. Models such as Google Bard and OpenAI ChatGPT, for example, are surprisingly powerful tools for question answering, code debugging, and dialogue generation. Despite the purported multilingual proficiency of these models, their linguistic inclusivity remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic, Modern Standard Arabic, and several nuanced dialectal variants. Furthermore, we undertake a human-centric study to scrutinize the efficacy of the most recent model, Bard, in following human instructions during translation tasks. Our exhaustive analysis indicates that LLMs may encounter challenges with certain Arabic dialects, particularly those for which minimal public data exists, such as Algerian and Mauritanian dialects. However, they exhibit satisfactory performance with more prevalent dialects, albeit occasionally trailing behind established commercial systems like Google Translate. Additionally, our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts. Collectively, our findings underscore that prevailing LLMs remain far from inclusive, with only limited ability to cater for the linguistic and cultural intricacies of diverse communities.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如Google Bard和OpenAI ChatGPT，在最近几年内 emerge as a breakthrough in AI。这些模型具有强大的问答、代码调试和对话生成能力。 despite the purported multilingual proficiency of these models, their linguistic inclusivity remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (including both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic, Modern Standard Arabic, and several nuanced dialectal variants. Furthermore, we undertake a human-centric study to scrutinize the efficacy of the most recent model, Bard, in following human instructions during translation tasks. Our exhaustive analysis indicates that LLMs may encounter challenges with certain Arabic dialects, particularly those for which minimal public data exists, such as Algerian and Mauritanian dialects. However, they exhibit satisfactory performance with more prevalent dialects, albeit occasionally trailing behind established commercial systems like Google Translate. Additionally, our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts. Collectively, our findings underscore that prevailing LLMs remain far from inclusive, with only limited ability to cater for the linguistic and cultural intricacies of diverse communities.
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Multi-Task-Representation-Learning-for-Human-Activity-Analysis-Using-Wearables"><a href="#Weakly-Supervised-Multi-Task-Representation-Learning-for-Human-Activity-Analysis-Using-Wearables" class="headerlink" title="Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables"></a>Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03805">http://arxiv.org/abs/2308.03805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taoran Sheng, Manfred Huber</li>
<li>for: 多元特征掌握和多任务同时处理</li>
<li>methods: 弱监睹多输出SIAMESE网络</li>
<li>results: 可以同时解决多个任务，并且在许多情况下超越单任务监睹方法表现。<details>
<summary>Abstract</summary>
Sensor data streams from wearable devices and smart environments are widely studied in areas like human activity recognition (HAR), person identification, or health monitoring. However, most of the previous works in activity and sensor stream analysis have been focusing on one aspect of the data, e.g. only recognizing the type of the activity or only identifying the person who performed the activity. We instead propose an approach that uses a weakly supervised multi-output siamese network that learns to map the data into multiple representation spaces, where each representation space focuses on one aspect of the data. The representation vectors of the data samples are positioned in the space such that the data with the same semantic meaning in that aspect are closely located to each other. Therefore, as demonstrated with a set of experiments, the trained model can provide metrics for clustering data based on multiple aspects, allowing it to address multiple tasks simultaneously and even to outperform single task supervised methods in many situations. In addition, further experiments are presented that in more detail analyze the effect of the architecture and of using multiple tasks within this framework, that investigate the scalability of the model to include additional tasks, and that demonstrate the ability of the framework to combine data for which only partial relationship information with respect to the target tasks is available.
</details>
<details>
<summary>摘要</summary>
仪器数据流FROM wearable devices和智能环境广泛研究在人体活动识别（HAR）、人员身份识别或健康监测等领域。然而，大多数前一些工作在活动和仪器流数据分析中都是关注一个方面的数据，例如只是识别活动的类型或者只是识别活动的执行者。我们提出了一种方法，使用弱监督多输出siamesenet来映射数据到多个表示空间中，其中每个表示空间都关注一个数据的方面。映射 vectors的数据样本被置于空间中，使得数据具有同一 Semantic meaning在该方面的数据集中均被 closely located。因此，通过一些实验，我们的模型可以为数据提供多个任务的指标，使其可以同时解决多个任务，甚至在许多情况下超越单任务监督方法。此外，我们还进行了更多的实验，分析了这种架构的影响和多个任务的使用情况，以及模型的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-methods-for-the-search-for-L-T-brown-dwarfs-in-the-data-of-modern-sky-surveys"><a href="#Machine-learning-methods-for-the-search-for-L-T-brown-dwarfs-in-the-data-of-modern-sky-surveys" class="headerlink" title="Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys"></a>Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03045">http://arxiv.org/abs/2308.03045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamaleksandra/ml-brown-dwarfs">https://github.com/iamaleksandra/ml-brown-dwarfs</a></li>
<li>paper_authors: Aleksandra Avdeeva</li>
<li>For: 这个论文的目的是创建一个高度可靠的褐矮星样本，以确定褐矮星的特征和分布。* Methods: 这篇论文使用机器学习算法，如Random Forest Classifier、XGBoost、SVM Classifier和TabNet，对PanStarrs DR1、2MASS和WISE数据进行分类，以 distinguishing L和T褐矮星从其他 spectral和照度类型的 объек。* Results: 研究人员使用机器学习算法，成功地分类出L和T褐矮星，并与传统的决策规则进行比较，证明了其效率和相关性。<details>
<summary>Abstract</summary>
According to various estimates, brown dwarfs (BD) should account for up to 25 percent of all objects in the Galaxy. However, few of them are discovered and well-studied, both individually and as a population. Homogeneous and complete samples of brown dwarfs are needed for these kinds of studies. Due to their weakness, spectral studies of brown dwarfs are rather laborious. For this reason, creating a significant reliable sample of brown dwarfs, confirmed by spectroscopic observations, seems unattainable at the moment. Numerous attempts have been made to search for and create a set of brown dwarfs using their colours as a decision rule applied to a vast amount of survey data. In this work, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier and TabNet on PanStarrs DR1, 2MASS and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. The explanation of the models is discussed. We also compare our models with classical decision rules, proving their efficiency and relevance.
</details>
<details>
<summary>摘要</summary>
To overcome this challenge, numerous attempts have been made to search for and create a set of brown dwarfs using their colors as a decision rule applied to a vast amount of survey data. In this study, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier, and TabNet on PanStarrs DR1, 2MASS, and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. We discuss the explanation of the models and compare them with classical decision rules, demonstrating their efficiency and relevance.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-for-Infectious-Disease-Risk-Prediction-A-Survey"><a href="#Machine-Learning-for-Infectious-Disease-Risk-Prediction-A-Survey" class="headerlink" title="Machine Learning for Infectious Disease Risk Prediction: A Survey"></a>Machine Learning for Infectious Disease Risk Prediction: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03037">http://arxiv.org/abs/2308.03037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutong Liu, Yang Liu, Jiming Liu</li>
<li>for: 这篇论文的目的是探讨机器学习如何在抑制传染病中发挥作用，以便更有效地预测感染病风险。</li>
<li>methods: 论文中使用的方法包括：介绍背景和动机，介绍不同类型的机器学习模型，讨论模型输入、设计目标和评估性能等挑战，并结尾提出未解决的问题和未来方向。</li>
<li>results: 论文的结果表明，机器学习可以帮助量化疾病传染模式，并准确预测感染病风险。<details>
<summary>Abstract</summary>
Infectious diseases, either emerging or long-lasting, place numerous people at risk and bring heavy public health burdens worldwide. In the process against infectious diseases, predicting the epidemic risk by modeling the disease transmission plays an essential role in assisting with preventing and controlling disease transmission in a more effective way. In this paper, we systematically describe how machine learning can play an essential role in quantitatively characterizing disease transmission patterns and accurately predicting infectious disease risks. First, we introduce the background and motivation of using machine learning for infectious disease risk prediction. Next, we describe the development and components of various machine learning models for infectious disease risk prediction. Specifically, existing models fall into three categories: Statistical prediction, data-driven machine learning, and epidemiology-inspired machine learning. Subsequently, we discuss challenges encountered when dealing with model inputs, designing task-oriented objectives, and conducting performance evaluation. Finally, we conclude with a discussion of open questions and future directions.
</details>
<details>
<summary>摘要</summary>
免疫疾病，无论是新兴的或长期存在的，都会对全球公共卫生带来巨大的压力。在抗击免疫疾病的过程中，预测疾病传播风险的模型化协助了更有效地预防和控制疾病传播。在这篇论文中，我们系统地描述了机器学习如何在免疫疾病风险预测中发挥重要作用。首先，我们介绍了使用机器学习预测免疫疾病风险的背景和动机。然后，我们描述了不同类型的机器学习模型的开发和组成部分。具体来说，现有的模型可以分为三类：统计预测、数据驱动机器学习和医学机器学习。接着，我们讨论了与模型输入、设计任务目标以及性能评价过程中遇到的挑战。最后，我们 conclude with 未来方向的开放问题。
</details></li>
</ul>
<hr>
<h2 id="Serverless-Federated-AUPRC-Optimization-for-Multi-Party-Collaborative-Imbalanced-Data-Mining"><a href="#Serverless-Federated-AUPRC-Optimization-for-Multi-Party-Collaborative-Imbalanced-Data-Mining" class="headerlink" title="Serverless Federated AUPRC Optimization for Multi-Party Collaborative Imbalanced Data Mining"></a>Serverless Federated AUPRC Optimization for Multi-Party Collaborative Imbalanced Data Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03035">http://arxiv.org/abs/2308.03035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xidongwu/d-auprc">https://github.com/xidongwu/d-auprc</a></li>
<li>paper_authors: Xidong Wu, Zhengmian Hu, Jian Pei, Heng Huang</li>
<li>for: 本文targets the problem of multi-party collaborative training for imbalanced data tasks, and proposes a new algorithm called ServerLess biAsed sTochastic gradiEnt (SLATE) to directly optimize the Area Under Precision-Recall Curve (AUPRC).</li>
<li>methods: 本文使用了服务器less多方合作学习 Setting，并将问题转化为conditional stochastic optimization problem。furthermore, the authors propose a new algorithm called ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) to improve the convergence rate.</li>
<li>results: 本文的实验结果表明，SLATE-M算法可以减少communication cost，并且与最佳单机Online方法匹配。这是首次解决多方合作AUPRC最大化问题。<details>
<summary>Abstract</summary>
Multi-party collaborative training, such as distributed learning and federated learning, is used to address the big data challenges. However, traditional multi-party collaborative training algorithms were mainly designed for balanced data mining tasks and are intended to optimize accuracy (\emph{e.g.}, cross-entropy). The data distribution in many real-world applications is skewed and classifiers, which are trained to improve accuracy, perform poorly when applied to imbalanced data tasks since models could be significantly biased toward the primary class. Therefore, the Area Under Precision-Recall Curve (AUPRC) was introduced as an effective metric. Although single-machine AUPRC maximization methods have been designed, multi-party collaborative algorithm has never been studied. The change from the single-machine to the multi-party setting poses critical challenges.   To address the above challenge, we study the serverless multi-party collaborative AUPRC maximization problem since serverless multi-party collaborative training can cut down the communications cost by avoiding the server node bottleneck, and reformulate it as a conditional stochastic optimization problem in a serverless multi-party collaborative learning setting and propose a new ServerLess biAsed sTochastic gradiEnt (SLATE) algorithm to directly optimize the AUPRC. After that, we use the variance reduction technique and propose ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) algorithm to improve the convergence rate, which matches the best theoretical convergence result reached by the single-machine online method. To the best of our knowledge, this is the first work to solve the multi-party collaborative AUPRC maximization problem.
</details>
<details>
<summary>摘要</summary>
多方合作训练，如分布式学习和联邦学习，用于解决大数据问题。然而，传统的多方合作训练算法主要设计用于均衡数据挖掘任务，并且optimize准确率（例如，交叉熵）。在许多实际应用中，数据分布不均，类 clasifier在不均衡数据任务中表现糟糕，因为模型可能受主要类别的偏见。因此，Area Under Precision-Recall Curve（AUPRC）被引入作为有效指标。虽然单机AUPRC最大化方法已经设计过，但多方合作算法尚未研究。在单机到多方 Setting中的变化 pose critical challenges。为了解决以上挑战，我们研究了无服务器多方合作AUPRC最大化问题，因为无服务器多方合作训练可以降低通信成本，并将问题重新定义为conditional stochastic optimization问题在无服务器多方合作学习Setting中。然后，我们提出了一种新的ServerLess biAsed sTochastic gradiEnt（SLATE）算法，以直接优化AUPRC。接着，我们使用了差分reduction技术，并提出了ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction（SLATE-M）算法，以提高收敛率，与单机在线方法的最佳理论收敛率匹配。到目前为止，这是首次解决多方合作AUPRC最大化问题的研究。
</details></li>
</ul>
<hr>
<h2 id="Causal-Disentanglement-Hidden-Markov-Model-for-Fault-Diagnosis"><a href="#Causal-Disentanglement-Hidden-Markov-Model-for-Fault-Diagnosis" class="headerlink" title="Causal Disentanglement Hidden Markov Model for Fault Diagnosis"></a>Causal Disentanglement Hidden Markov Model for Fault Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03027">http://arxiv.org/abs/2308.03027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rihao Chang, Yongtao Ma, Weizhi Nie, Jie Nie, An-an Liu</li>
<li>for: 本研究旨在提出一种基于 causal disentanglement hidden markov model (CDHM) 的磨损诊断方法，以便更好地捕捉磨损特征并实现预测磨损类型。</li>
<li>methods: 本方法首先使用时序数据进行磨损特征的捕捉，然后逐渐分离磨损信号中相关和无关的因素。 其中，我们使用 ELBO 来优化学习 causal disentanglement markov model。此外，我们还采用无监督领域适应，将学习的分离表示转移到其他工作环境中。</li>
<li>results: 实验结果表明，提出的方法在 CWRU 数据集和 IMS 数据集上具有优秀的效果，可以准确地预测磨损类型。<details>
<summary>Abstract</summary>
In modern industries, fault diagnosis has been widely applied with the goal of realizing predictive maintenance. The key issue for the fault diagnosis system is to extract representative characteristics of the fault signal and then accurately predict the fault type. In this paper, we propose a Causal Disentanglement Hidden Markov model (CDHM) to learn the causality in the bearing fault mechanism and thus, capture their characteristics to achieve a more robust representation. Specifically, we make full use of the time-series data and progressively disentangle the vibration signal into fault-relevant and fault-irrelevant factors. The ELBO is reformulated to optimize the learning of the causal disentanglement Markov model. Moreover, to expand the scope of the application, we adopt unsupervised domain adaptation to transfer the learned disentangled representations to other working environments. Experiments were conducted on the CWRU dataset and IMS dataset. Relevant results validate the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
现代产业中，故障诊断已广泛应用，目的是实现预测维护。故障诊断系统的关键问题是提取表征性的故障信号，并准确预测故障类型。在本文中，我们提议一种因果分解隐藏马尔可夫模型（CDHM），以学习滤波器故障机制中的因果关系，并 capture其特征来实现更加稳定的表征。具体来说，我们利用时间序列数据，逐步分解振荡信号，分解出相关和无关故障因素。我们 reformulate ELBO，以便学习因果分解马尔可夫模型。此外，为扩展应用范围，我们采用无监督领域适应，将学习的分解表征转移到其他工作环境。在CWRU数据集和IMS数据集上进行了实验，实验结果证明了我们提出的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Early-Detection-and-Localization-of-Pancreatic-Cancer-by-Label-Free-Tumor-Synthesis"><a href="#Early-Detection-and-Localization-of-Pancreatic-Cancer-by-Label-Free-Tumor-Synthesis" class="headerlink" title="Early Detection and Localization of Pancreatic Cancer by Label-Free Tumor Synthesis"></a>Early Detection and Localization of Pancreatic Cancer by Label-Free Tumor Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03008">http://arxiv.org/abs/2308.03008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrgiovanni/synthetictumors">https://github.com/mrgiovanni/synthetictumors</a></li>
<li>paper_authors: Bowen Li, Yu-Cheng Chou, Shuwen Sun, Hualin Qiao, Alan Yuille, Zongwei Zhou<br>for: 这篇论文的目的是提高胰脏癌早期检测和定位，以增加病人5年生存率从8.5%提高到20%。methods: 本研究使用人工智能（AI）模型，将健康胰脏中的小型胰脏肿瘤合成成许多标注的例子，以帮助医生早期检测胰脏癌。results: 我们的实验结果显示，使用合成的胰脏肿瘤训练AI模型，胰脏癌检测率与真实胰脏癌检测率相似，且能够更好地检测小型胰脏肿瘤。此外，我们还证明了使用合成胰脏肿瘤和真实胰脏癌检测结果进行混合训练，可以提高AI模型的普遍性和检测精度。<details>
<summary>Abstract</summary>
Early detection and localization of pancreatic cancer can increase the 5-year survival rate for patients from 8.5% to 20%. Artificial intelligence (AI) can potentially assist radiologists in detecting pancreatic tumors at an early stage. Training AI models require a vast number of annotated examples, but the availability of CT scans obtaining early-stage tumors is constrained. This is because early-stage tumors may not cause any symptoms, which can delay detection, and the tumors are relatively small and may be almost invisible to human eyes on CT scans. To address this issue, we develop a tumor synthesis method that can synthesize enormous examples of small pancreatic tumors in the healthy pancreas without the need for manual annotation. Our experiments demonstrate that the overall detection rate of pancreatic tumors, measured by Sensitivity and Specificity, achieved by AI trained on synthetic tumors is comparable to that of real tumors. More importantly, our method shows a much higher detection rate for small tumors. We further investigate the per-voxel segmentation performance of pancreatic tumors if AI is trained on a combination of CT scans with synthetic tumors and CT scans with annotated large tumors at an advanced stage. Finally, we show that synthetic tumors improve AI generalizability in tumor detection and localization when processing CT scans from different hospitals. Overall, our proposed tumor synthesis method has immense potential to improve the early detection of pancreatic cancer, leading to better patient outcomes.
</details>
<details>
<summary>摘要</summary>
早期检测和肿瘤localization可以提高患者5年生存率从8.5%提高到20%。人工智能（AI）可能能够帮助放射学家在早期发现肿瘤。然而，训练AI模型需要庞大的标注示例，但获得早期肿瘤的CT扫描数据受限。这是因为早期肿瘤可能不会导致任何症状，这可能会延迟检测，并且肿瘤相对较小，可能对人类目视难以看到在CT扫描中。为解决这个问题，我们开发了一种肿瘤合成方法，可以在健康的胰脏中合成巨大的小肿瘤示例，无需人工标注。我们的实验表明，由AI训练在合成肿瘤上的检测率（敏感性和特异性）与真实肿瘤相比较高，并且检测到小肿瘤的率更高。我们进一步调查了使用合成肿瘤和注解大肿瘤的CT扫描结合训练AI的效果，发现这种方法可以提高肿瘤检测和地图localization的普适性。最后，我们证明了合成肿瘤可以提高AI在不同医院CT扫描处理时的普适性。总之，我们提出的肿瘤合成方法有巨大的潜力，可以提高肿瘤检测的早期，导致更好的病例结果。
</details></li>
</ul>
<hr>
<h2 id="Deep-Polar-Codes"><a href="#Deep-Polar-Codes" class="headerlink" title="Deep Polar Codes"></a>Deep Polar Codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03004">http://arxiv.org/abs/2308.03004</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HzFu/MNet_DeepCDR">https://github.com/HzFu/MNet_DeepCDR</a></li>
<li>paper_authors: Geon Choi, Namyoon Lee</li>
<li>for: 这个论文旨在提出一种新的预变换极码，称为深度极码。</li>
<li>methods: 论文提出了一种深度极编码器，利用多层极化转换来实现低复杂度实现，同时能够改善极码的重量分布。此外，该编码器支持范围广的代码率和块长。</li>
<li>results: 通过 simulations，论文表明深度极码在不同代码率下的块错误率比现有的预变换极码更低，同时保持低的编码和解码复杂度。此外，论文还表明，将深度极码与循环检验码 concatenate 可以达到 finite block length 容量的 meta-converse bound 的下界 within 0.4 dB 以下。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel class of pre-transformed polar codes, termed as deep polar codes. We first present a deep polar encoder that harnesses a series of multi-layered polar transformations with varying sizes. Our approach to encoding enables a low-complexity implementation while significantly enhancing the weight distribution of the code. Moreover, our encoding method offers flexibility in rate-profiling, embracing a wide range of code rates and blocklengths. Next, we put forth a low-complexity decoding algorithm called successive cancellation list with backpropagation parity checks (SCL-BPC). This decoding algorithm leverages the parity check equations in the reverse process of the multi-layered pre-transformed encoding for SCL decoding. Additionally, we present a low-latency decoding algorithm that employs parallel-SCL decoding by treating partially pre-transformed bit patterns as additional frozen bits. Through simulations, we demonstrate that deep polar codes outperform existing pre-transformed polar codes in terms of block error rates across various code rates under short block lengths, while maintaining low encoding and decoding complexity. Furthermore, we show that concatenating deep polar codes with cyclic-redundancy-check codes can achieve the meta-converse bound of the finite block length capacity within 0.4 dB in some instances.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的预转换极码，称为深度极码。我们首先提出了一种深度极编码器，利用多层极化转换来实现低复杂性实现，同时提高码的质量分布。此外，我们的编码方法支持范围广的码率和块长度。接着，我们提出了一种低复杂度解码算法，称为顺序取消列表带回传播可能性检查（SCL-BPC）。这种解码算法利用了反向多层预转换的parity check方程来实现SCL解码。此外，我们还提出了一种低延迟解码算法，通过并行SCL解码来处理部分预转换的bit pattern。通过实验，我们证明了深度极码在不同的码率下的块错误率都较低，同时保持了低编码和解码复杂度。此外，我们还显示了 concatenating 深度极码可以实现finite block length容量的meta-converse bound在0.4 dB之间。
</details></li>
</ul>
<hr>
<h2 id="Spanish-Pre-trained-BERT-Model-and-Evaluation-Data"><a href="#Spanish-Pre-trained-BERT-Model-and-Evaluation-Data" class="headerlink" title="Spanish Pre-trained BERT Model and Evaluation Data"></a>Spanish Pre-trained BERT Model and Evaluation Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02976">http://arxiv.org/abs/2308.02976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dccuchile/beto">https://github.com/dccuchile/beto</a></li>
<li>paper_authors: José Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, Jorge Pérez</li>
<li>for: 增强西班牙语模型的训练和评估资源。</li>
<li>methods: BERT基于模型预训练 exclusively on Spanish data。</li>
<li>results: 比其他基于BERT的模型在大多数任务上获得更好的结果，并在一些任务上创造新的状态。<details>
<summary>Abstract</summary>
The Spanish language is one of the top 5 spoken languages in the world. Nevertheless, finding resources to train or evaluate Spanish language models is not an easy task. In this paper we help bridge this gap by presenting a BERT-based language model pre-trained exclusively on Spanish data. As a second contribution, we also compiled several tasks specifically for the Spanish language in a single repository much in the spirit of the GLUE benchmark. By fine-tuning our pre-trained Spanish model, we obtain better results compared to other BERT-based models pre-trained on multilingual corpora for most of the tasks, even achieving a new state-of-the-art on some of them. We have publicly released our model, the pre-training data, and the compilation of the Spanish benchmarks.
</details>
<details>
<summary>摘要</summary>
“西班牙语是全球前五大最受欢迎的语言之一，然而找到用于训练或评估西班牙语模型的资源并不是一个容易的任务。在这篇论文中，我们帮助填补这个差距，提出了基于BERT的西班牙语模型，并将其推广到多个任务中。作为我们的第二次贡献，我们还将西班牙语的多个任务集成了一个单一的存储库，类似于GLUE套件。通过精益地调整我们的预训西班牙语模型，我们在大多数任务上取得了更好的结果，甚至创下了一些新的州态。我们已经公开了我们的模型、预训数据和西班牙语套件。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Generalized-Oversampling-for-Learning-from-Imbalanced-datasets-and-Associated-Theory"><a href="#Generalized-Oversampling-for-Learning-from-Imbalanced-datasets-and-Associated-Theory" class="headerlink" title="Generalized Oversampling for Learning from Imbalanced datasets and Associated Theory"></a>Generalized Oversampling for Learning from Imbalanced datasets and Associated Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02966">http://arxiv.org/abs/2308.02966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Stocksieker, Denys Pommeret, Arthur Charpentier</li>
<li>for: 本研究旨在解决超参异常学习中的数据不均衡问题，具体来说是用于异常 regression 问题。</li>
<li>methods: 本文提出了一种基于kernel density estimate的数据扩充方法，称为GOLIATH算法，该方法可以应用于分类和回归问题。它包括两大类别的人工扩充：基于扰动的，如加aussian noise，和基于 interpolate的，如 SMOTE。此外，该方法还提供了这些机器学习算法的Explicit表达式和准确性梯度的表达式，特别是对SMOTE算法的表达式。</li>
<li>results: 本文通过应用GOLIATH算法在异常 regression 中进行了实验评估，并与现有状态方法进行了比较。结果表明，GOLIATH算法在异常 regression 中具有显著的改善效果。<details>
<summary>Abstract</summary>
In supervised learning, it is quite frequent to be confronted with real imbalanced datasets. This situation leads to a learning difficulty for standard algorithms. Research and solutions in imbalanced learning have mainly focused on classification tasks. Despite its importance, very few solutions exist for imbalanced regression. In this paper, we propose a data augmentation procedure, the GOLIATH algorithm, based on kernel density estimates which can be used in classification and regression. This general approach encompasses two large families of synthetic oversampling: those based on perturbations, such as Gaussian Noise, and those based on interpolations, such as SMOTE. It also provides an explicit form of these machine learning algorithms and an expression of their conditional densities, in particular for SMOTE. New synthetic data generators are deduced. We apply GOLIATH in imbalanced regression combining such generator procedures with a wild-bootstrap resampling technique for the target values. We evaluate the performance of the GOLIATH algorithm in imbalanced regression situations. We empirically evaluate and compare our approach and demonstrate significant improvement over existing state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
在监督学习中，很普遍遇到实际数据集的不均衡问题。这种情况会导致标准算法学习困难。研究和解决不均衡学习问题的研究主要集中在分类任务上。虽然其重要性很大，但是现有的解决方案很少。在这篇论文中，我们提出了一种数据扩充方法，名为GOLIATH算法，基于核密度估计。这种方法可以用于分类和回归任务。这个总体方法包括两大家族的人工扩充：基于扰动，如 Gaussian Noise，和基于 interpolations，如 SMOTE。它还提供了这些机器学习算法的直观表达，特别是对 SMOTE 的表达。我们从这些synthetic数据生成器中推出了新的数据生成器。我们在不均衡回归中使用这些生成器和通用Bootstrap抽取技术来预测值。我们对 GOLIATH 算法在不均衡回归情况下的性能进行了实验性评估和比较，并证明了我们的方法在现有状态的技术上显著提高了性能。
</details></li>
</ul>
<hr>
<h2 id="Data-Fusion-for-Multi-Task-Learning-of-Building-Extraction-and-Height-Estimation"><a href="#Data-Fusion-for-Multi-Task-Learning-of-Building-Extraction-and-Height-Estimation" class="headerlink" title="Data Fusion for Multi-Task Learning of Building Extraction and Height Estimation"></a>Data Fusion for Multi-Task Learning of Building Extraction and Height Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02960">http://arxiv.org/abs/2308.02960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SaadAhmedJamal/IEEE_DFC2023">https://github.com/SaadAhmedJamal/IEEE_DFC2023</a></li>
<li>paper_authors: Saad Ahmed Jamal, Arioluwa Aribisala</li>
<li>for: 本研究は都市重建问题上提出的DFC23 Track 2 Contest中的一种多任务学习方法，用于批量抽取和高度估算 optical和雷达卫星图像中。</li>
<li>methods: 本研究使用多任务学习方法，通过重用特征和形成隐式约束 между多个任务来提高解决方案的质量。</li>
<li>results: 本研究的基准结果表明，对于批量抽取和高度估算，在设计了相关实验后，baseline结果显著提高了。<details>
<summary>Abstract</summary>
In accordance with the urban reconstruction problem proposed by the DFC23 Track 2 Contest, this paper attempts a multitask-learning method of building extraction and height estimation using both optical and radar satellite imagery. Contrary to the initial goal of multitask learning which could potentially give a superior solution by reusing features and forming implicit constraints between multiple tasks, this paper reports the individual implementation of the building extraction and height estimation under constraints. The baseline results for the building extraction and the height estimation significantly increased after designed experiments.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "urban reconstruction" was translated as "城市重建" (chéngshì zhòngjiàn)* "multitask-learning" was translated as "多任务学习" (duō zhìxí)* "building extraction" was translated as "建筑物提取" (jiànzhì wù tiēchū)* "height estimation" was translated as "高度估算" (gāodù gèsuan)* "satellite imagery" was translated as "卫星图像" (wèixīng túxiàng)
</details></li>
</ul>
<hr>
<h2 id="K-band-Self-supervised-MRI-Reconstruction-via-Stochastic-Gradient-Descent-over-K-space-Subsets"><a href="#K-band-Self-supervised-MRI-Reconstruction-via-Stochastic-Gradient-Descent-over-K-space-Subsets" class="headerlink" title="K-band: Self-supervised MRI Reconstruction via Stochastic Gradient Descent over K-space Subsets"></a>K-band: Self-supervised MRI Reconstruction via Stochastic Gradient Descent over K-space Subsets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02958">http://arxiv.org/abs/2308.02958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikgroup/k-band">https://github.com/mikgroup/k-band</a></li>
<li>paper_authors: Frederic Wang, Han Qi, Alfredo De Goyeneche, Reinhard Heckel, Michael Lustig, Efrat Shimron</li>
<li>for: 这项研究的目的是为了使用只有部分、有限分辨率的k-空间数据进行深度学习模型的训练，以提高MRI图像重建的精度和效率。</li>
<li>methods: 这项研究使用了一种新的数学框架，称为k-band，以便使用只有部分、有限分辨率的k-空间数据进行深度学习模型的训练。具体来说，这种方法使用了在每次训练迭代中使用只有一小部分k-空间数据来计算梯度的束教学法。</li>
<li>results: 实验表明，k-band方法可以与使用高分辨率数据进行训练的状态对照方法（SoTA）的性能相似，而无需使用高分辨率数据进行训练。此外，k-band方法还可以在快速获得的有限分辨率数据上进行自我监督式训练，从而提高了MRI图像重建的精度和效率。<details>
<summary>Abstract</summary>
Although deep learning (DL) methods are powerful for solving inverse problems, their reliance on high-quality training data is a major hurdle. This is significant in high-dimensional (dynamic/volumetric) magnetic resonance imaging (MRI), where acquisition of high-resolution fully sampled k-space data is impractical. We introduce a novel mathematical framework, dubbed k-band, that enables training DL models using only partial, limited-resolution k-space data. Specifically, we introduce training with stochastic gradient descent (SGD) over k-space subsets. In each training iteration, rather than using the fully sampled k-space for computing gradients, we use only a small k-space portion. This concept is compatible with different sampling strategies; here we demonstrate the method for k-space "bands", which have limited resolution in one dimension and can hence be acquired rapidly. We prove analytically that our method stochastically approximates the gradients computed in a fully-supervised setup, when two simple conditions are met: (i) the limited-resolution axis is chosen randomly-uniformly for every new scan, hence k-space is fully covered across the entire training set, and (ii) the loss function is weighed with a mask, derived here analytically, which facilitates accurate reconstruction of high-resolution details. Numerical experiments with raw MRI data indicate that k-band outperforms two other methods trained on limited-resolution data and performs comparably to state-of-the-art (SoTA) methods trained on high-resolution data. k-band hence obtains SoTA performance, with the advantage of training using only limited-resolution data. This work hence introduces a practical, easy-to-implement, self-supervised training framework, which involves fast acquisition and self-supervised reconstruction and offers theoretical guarantees.
</details>
<details>
<summary>摘要</summary>
尽管深度学习（DL）方法有力量解决反向问题，但它们依赖高质量训练数据是一个主要障碍。在高维度（动态/体积）磁共振成像（MRI）中，获取高分辨率完全采样的k空间数据是不现实的。我们介绍了一种新的数学框架，称之为k带，允许使用仅部分、有限分辨率k空间数据进行训练DL模型。具体来说，我们引入了使用批处理的梯度下降（SGD）在k空间子集上训练。在每个训练迭代中，而不是使用完全采样的k空间来计算梯度，我们只使用一小部分k空间。这种概念 compatible with different sampling strategies; here we demonstrate the method for k-space "bands", which have limited resolution in one dimension and can hence be acquired rapidly. We prove analytically that our method stochastically approximates the gradients computed in a fully-supervised setup, when two simple conditions are met: (i) the limited-resolution axis is chosen randomly-uniformly for every new scan, hence k-space is fully covered across the entire training set, and (ii) the loss function is weighed with a mask, derived here analytically, which facilitates accurate reconstruction of high-resolution details. Numerical experiments with raw MRI data indicate that k-band outperforms two other methods trained on limited-resolution data and performs comparably to state-of-the-art (SoTA) methods trained on high-resolution data. k-band hence obtains SoTA performance, with the advantage of training using only limited-resolution data. This work hence introduces a practical, easy-to-implement, self-supervised training framework, which involves fast acquisition and self-supervised reconstruction and offers theoretical guarantees.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-AI-based-Smart-Contract-Creation"><a href="#An-Empirical-Study-of-AI-based-Smart-Contract-Creation" class="headerlink" title="An Empirical Study of AI-based Smart Contract Creation"></a>An Empirical Study of AI-based Smart Contract Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02955">http://arxiv.org/abs/2308.02955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabimba Karanjai, Edward Li, Lei Xu, Weidong Shi</li>
<li>for: 本研究的主要目标是评估 LLMS 生成的智能合约代码质量。</li>
<li>methods: 我们使用了一个实验设置来评估生成代码的正确性、安全性和效率。</li>
<li>results: 我们发现生成的智能合约代码存在安全漏洞，同时代码质量和正确性受到输入参数的影响。但我们还发现了一些可以改进的方向。<details>
<summary>Abstract</summary>
The introduction of large language models (LLMs) like ChatGPT and Google Palm2 for smart contract generation seems to be the first well-established instance of an AI pair programmer. LLMs have access to a large number of open-source smart contracts, enabling them to utilize more extensive code in Solidity than other code generation tools. Although the initial and informal assessments of LLMs for smart contract generation are promising, a systematic evaluation is needed to explore the limits and benefits of these models. The main objective of this study is to assess the quality of generated code provided by LLMs for smart contracts. We also aim to evaluate the impact of the quality and variety of input parameters fed to LLMs. To achieve this aim, we created an experimental setup for evaluating the generated code in terms of validity, correctness, and efficiency. Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall quality and correctness of the code getting impacted. However, we also identified the areas where it can be improved. The paper also proposes several potential research directions to improve the process, quality and safety of generated smart contract codes.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）如ChatGPT和Google Palm2的出现似乎是首个成功的AI双程程式。LLMs可以使用大量的开源智能合约，使其在Solidity中运用更大的代码。虽然初步评估结果尚未正式，但是需要系统性的评估来探索这些模型的限制和优点。本研究的主要目标是评估LLMs生成的智能合约代码质量。我们还想评估对LLMs输入参数的影响，以及生成代码的有效性、正确性和安全性。为了完成这个目标，我们设计了一个实验室来评估生成代码的有效性、正确性和安全性。我们发现生成的智能合约中有许多安全漏洞，并且代码的质量和正确性受到影响。但是，我们也发现了一些改善这些过程的研究方向。本文还提出了多个可能的研究方向，以提高生成代码的过程、质量和安全性。
</details></li>
</ul>
<hr>
<h2 id="dPASP-A-Comprehensive-Differentiable-Probabilistic-Answer-Set-Programming-Environment-For-Neurosymbolic-Learning-and-Reasoning"><a href="#dPASP-A-Comprehensive-Differentiable-Probabilistic-Answer-Set-Programming-Environment-For-Neurosymbolic-Learning-and-Reasoning" class="headerlink" title="dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning"></a>dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02944">http://arxiv.org/abs/2308.02944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renato Lui Geh, Jonas Gonçalves, Igor Cataneo Silveira, Denis Deratani Mauá, Fabio Gagliardi Cozman</li>
<li>for: 这篇论文旨在提出一种新的宣言型概率逻辑编程框架，用于可微分的神经符号逻辑推理。</li>
<li>methods: 该框架使得可以指定精确的概率模型，包括神经 predicate、逻辑约束和间隔值概率选择，以支持 combining 低级感知（图像、文本等）、通用的推理和（模糊的）统计知识。</li>
<li>results: 论文提出了多种 semantics  для probabilistic logic programs，以表达不确定、矛盾、不完整和&#x2F;或统计知识。同时，也介绍了如何使用神经 predicate 和概率选择进行 gradient-based 学习。论文还描述了一个实现的 package，用于推理和学习，并提供了一些示例程序。<details>
<summary>Abstract</summary>
We present dPASP, a novel declarative probabilistic logic programming framework for differentiable neuro-symbolic reasoning. The framework allows for the specification of discrete probabilistic models with neural predicates, logic constraints and interval-valued probabilistic choices, thus supporting models that combine low-level perception (images, texts, etc), common-sense reasoning, and (vague) statistical knowledge. To support all such features, we discuss the several semantics for probabilistic logic programs that can express nondeterministic, contradictory, incomplete and/or statistical knowledge. We also discuss how gradient-based learning can be performed with neural predicates and probabilistic choices under selected semantics. We then describe an implemented package that supports inference and learning in the language, along with several example programs. The package requires minimal user knowledge of deep learning system's inner workings, while allowing end-to-end training of rather sophisticated models and loss functions.
</details>
<details>
<summary>摘要</summary>
我们提出了dpASP，一种新的宣告性概率逻辑编程框架，用于可 diferenciable 神经符号逻辑推理。该框架允许用户 specify 权重逻辑模型，神经元 predicate，逻辑约束和间隔值概率选择，因此可以支持模型结合低级感知（图像、文本等）、通用理智和（抽象）统计知识。为支持这些特性，我们讨论了几种 probabilistic logic programs 的 semantics，可以表达不确定、矛盾、不完整和/或统计知识。我们还讨论了如何在 selected semantics 下使用 neural predicates 和 probabilistic choices 进行梯度基于学习。然后，我们描述了一个实现的 package，支持推理和学习语言中的各种例程，并提供了一些示例程序。该 package 需要用户具备最低知识量，同时允许用户通过终端训练较复杂的模型和损失函数。
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Development-of-an-Uncertainty-Quantification-Protocol-for-the-Natural-Gas-Industry"><a href="#Towards-the-Development-of-an-Uncertainty-Quantification-Protocol-for-the-Natural-Gas-Industry" class="headerlink" title="Towards the Development of an Uncertainty Quantification Protocol for the Natural Gas Industry"></a>Towards the Development of an Uncertainty Quantification Protocol for the Natural Gas Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02941">http://arxiv.org/abs/2308.02941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Babajide Kolade</li>
<li>for: This paper aims to develop a protocol for assessing uncertainties in predictions of machine learning and mechanistic simulation models, specifically for the gas distribution industry.</li>
<li>methods: The protocol outlines an uncertainty quantification workflow that includes identifying key sources of uncertainties, using applicable methods of uncertainty propagation, and employing statistically rational estimators for output uncertainties.</li>
<li>results: The paper applies the protocol to test cases relevant to the gas distribution industry and presents the learnings from its application. The results demonstrate the effectiveness of the protocol in quantifying uncertainties in simulation results.<details>
<summary>Abstract</summary>
Simulations using machine learning (ML) models and mechanistic models are often run to inform decision-making processes. Uncertainty estimates of simulation results are critical to the decision-making process because simulation results of specific scenarios may have wide, but unspecified, confidence bounds that may impact subsequent analyses and decisions. The objective of this work is to develop a protocol to assess uncertainties in predictions of machine learning and mechanistic simulation models. The protocol will outline an uncertainty quantification workflow that may be used to establish credible bounds of predictability on computed quantities of interest and to assess model sufficiency. The protocol identifies key sources of uncertainties in machine learning and mechanistic modeling, defines applicable methods of uncertainty propagation for these sources, and includes statistically rational estimators for output uncertainties. The work applies the protocol to test cases relevant to the gas distribution industry and presents learnings from its application. The paper concludes with a brief discussion outlining a pathway to the wider adoption of uncertainty quantification within the industry
</details>
<details>
<summary>摘要</summary>
模拟使用机器学习（ML）模型和机制模型经常用来支持决策过程。模拟结果中的不确定性估计对决策过程是关键的，因为特定场景的模拟结果可能具有广泛而不确定的信任范围，这可能影响后续分析和决策。本工作的目标是开发一个协议来评估机器学习和机制模型预测结果中的不确定性。协议将 outline一个不确定性评估工作流程，可以用来确定计算量据点的可靠范围和评估模型的充分性。协议列出了机器学习和机制模型中的主要不确定性来源，采用可靠的方法进行不确定性传播，并提供了统计合理的输出不确定性估计器。本工作应用协议到 relevante test cases，并presented learnings from its application。文章 conclude with a brief discussion outlining a pathway to the wider adoption of uncertainty quantification within the industry。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval"><a href="#Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval" class="headerlink" title="Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval"></a>Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02926">http://arxiv.org/abs/2308.02926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Haoxiang-WasedaU/Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval">https://github.com/Haoxiang-WasedaU/Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval</a></li>
<li>paper_authors: Haoxiang Shi, Sumio Fujita, Tetsuya Sakai</li>
<li>for: 提高现代神经信息检索中的领域传递问题的解决方案</li>
<li>methods: 取消consistency filter，使用直接pseudo-labeling、pseudo-相关反馈或无监督关键词生成方法实现一致性自由粗粒度检索</li>
<li>results: 对多个数据集进行了广泛的实验评估，结果显示，使用TextRank基于pseudo relevance feedback方法可以超过其他方法的表现，并且对训练和推理效率具有显著改进。<details>
<summary>Abstract</summary>
Domain transfer is a prevalent challenge in modern neural Information Retrieval (IR). To overcome this problem, previous research has utilized domain-specific manual annotations and synthetic data produced by consistency filtering to finetune a general ranker and produce a domain-specific ranker. However, training such consistency filters are computationally expensive, which significantly reduces the model efficiency. In addition, consistency filtering often struggles to identify retrieval intentions and recognize query and corpus distributions in a target domain. In this study, we evaluate a more efficient solution: replacing the consistency filter with either direct pseudo-labeling, pseudo-relevance feedback, or unsupervised keyword generation methods for achieving consistent filtering-free unsupervised dense retrieval. Our extensive experimental evaluations demonstrate that, on average, TextRank-based pseudo relevance feedback outperforms other methods. Furthermore, we analyzed the training and inference efficiency of the proposed paradigm. The results indicate that filtering-free unsupervised learning can continuously improve training and inference efficiency while maintaining retrieval performance. In some cases, it can even improve performance based on particular datasets.
</details>
<details>
<summary>摘要</summary>
域名转移是现代神经信息检索（IR）中的一个常见挑战。以前的研究曾利用域名特定的手动标注和由consistency filtering生成的 sintetic数据来训练一个通用排名器并生成域名特定的排名器。然而，训练such consistency filters具有计算成本高的问题，这会significantly reduces the model efficiency。 In addition, consistency filtering often struggles to identify retrieval intentions and recognize query and corpus distributions in a target domain。在本研究中，我们评估了一种更有效的解决方案：取代consistency filter with direct pseudo-labeling、pseudo-relevance feedback或Unsupervised keyword generation方法来实现无 filtering-free无监督的排名。我们的广泛的实验评估表明，TextRank-based pseudo relevance feedback在其他方法中表现更好。此外，我们还分析了提议的训练和推理效率。结果表明，filtering-free无监督学习可以不断提高训练和推理效率，同时保持检索性能。在某些 dataset 上，它可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="An-AI-Enabled-Framework-to-Defend-Ingenious-MDT-based-Attacks-on-the-Emerging-Zero-Touch-Cellular-Networks"><a href="#An-AI-Enabled-Framework-to-Defend-Ingenious-MDT-based-Attacks-on-the-Emerging-Zero-Touch-Cellular-Networks" class="headerlink" title="An AI-Enabled Framework to Defend Ingenious MDT-based Attacks on the Emerging Zero Touch Cellular Networks"></a>An AI-Enabled Framework to Defend Ingenious MDT-based Attacks on the Emerging Zero Touch Cellular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02923">http://arxiv.org/abs/2308.02923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aneeqa Ijaz, Waseem Raza, Hasan Farooq, Marvin Manalastas, Ali Imran</li>
<li>for: 本研究旨在探讨黑客可以通过劣质MDT报告来攻击深度自动化无线网络的新型攻击方式，并对常见网络自动化功能的性能造成负面影响。</li>
<li>methods: 本研究使用机器学习方法来检测和排除劣质MDT报告，并通过一个实验场景证明其效果。</li>
<li>results: 研究发现，劣质MDT报告可以影响网络自动化功能的性能，而且可以通过Machine Learning来检测和排除这些劣质报告。<details>
<summary>Abstract</summary>
Deep automation provided by self-organizing network (SON) features and their emerging variants such as zero touch automation solutions is a key enabler for increasingly dense wireless networks and pervasive Internet of Things (IoT). To realize their objectives, most automation functionalities rely on the Minimization of Drive Test (MDT) reports. The MDT reports are used to generate inferences about network state and performance, thus dynamically change network parameters accordingly. However, the collection of MDT reports from commodity user devices, particularly low cost IoT devices, make them a vulnerable entry point to launch an adversarial attack on emerging deeply automated wireless networks. This adds a new dimension to the security threats in the IoT and cellular networks. Existing literature on IoT, SON, or zero touch automation does not address this important problem. In this paper, we investigate an impactful, first of its kind adversarial attack that can be launched by exploiting the malicious MDT reports from the compromised user equipment (UE). We highlight the detrimental repercussions of this attack on the performance of common network automation functions. We also propose a novel Malicious MDT Reports Identification framework (MRIF) as a countermeasure to detect and eliminate the malicious MDT reports using Machine Learning and verify it through a use-case. Thus, the defense mechanism can provide the resilience and robustness for zero touch automation SON engines against the adversarial MDT attacks
</details>
<details>
<summary>摘要</summary>
深度自动化提供的无需人工控制网络（SON）功能和其相关的零 touched自动化解决方案是现代无线网络和物联网（IoT）的关键驱动力。为实现他们的目标，大多数自动化功能都依赖于推理测试（MDT）报告。MDT报告用于生成网络状态和性能的推理，并在运行时动态地改变网络参数。然而，从低成本IoT设备收集MDT报告，特别是从受到攻击的用户设备，使得这些报告成为攻击 deeply automated 无线网络的易受攻击点。这添加了一个新的安全隐患，并且现有的相关文献不具备对这一重要问题的讨论。在这篇论文中，我们研究了一种新型的攻击，可以通过恶意MDT报告来发动，从恶意用户设备收集MDT报告。我们强调了这种攻击对常见网络自动化功能的不良影响。此外，我们还提出了一种新的恶意MDT报告识别框架（MRIF），用于检测和消除恶意MDT报告。我们使用机器学习来实现这一目标，并通过用例验证了这种防御机制的有效性。因此，这种防御机制可以为零 touched自动化 SON 引擎提供鲜活性和鲜活性。
</details></li>
</ul>
<hr>
<h2 id="Structured-Low-Rank-Tensors-for-Generalized-Linear-Models"><a href="#Structured-Low-Rank-Tensors-for-Generalized-Linear-Models" class="headerlink" title="Structured Low-Rank Tensors for Generalized Linear Models"></a>Structured Low-Rank Tensors for Generalized Linear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02922">http://arxiv.org/abs/2308.02922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Batoul Taki, Anand D. Sarwate, Waheed U. Bajwa</li>
<li>for: This paper is written for researchers and practitioners interested in exploring tensor-based methods for generalized linear model (GLM) problems, particularly those dealing with low-rank tensor structures.</li>
<li>methods: The paper proposes a new low-rank tensor model called the Low Separation Rank (LSR) model, which is imposed onto the coefficient tensor in GLM problems. The authors also develop a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs.</li>
<li>results: The paper derives a minimax lower bound on the error threshold for estimating the coefficient tensor in LSR tensor GLM problems, which suggests that the sample complexity of the proposed method may be significantly lower than that of vectorized GLMs. The authors also demonstrate the efficacy of the proposed LSR tensor model on synthetic and real-world datasets.<details>
<summary>Abstract</summary>
Recent works have shown that imposing tensor structures on the coefficient tensor in regression problems can lead to more reliable parameter estimation and lower sample complexity compared to vector-based methods. This work investigates a new low-rank tensor model, called Low Separation Rank (LSR), in Generalized Linear Model (GLM) problems. The LSR model -- which generalizes the well-known Tucker and CANDECOMP/PARAFAC (CP) models, and is a special case of the Block Tensor Decomposition (BTD) model -- is imposed onto the coefficient tensor in the GLM model. This work proposes a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs. Most importantly, it derives a minimax lower bound on the error threshold on estimating the coefficient tensor in LSR tensor GLM problems. The minimax bound is proportional to the intrinsic degrees of freedom in the LSR tensor GLM problem, suggesting that its sample complexity may be significantly lower than that of vectorized GLMs. This result can also be specialised to lower bound the estimation error in CP and Tucker-structured GLMs. The derived bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples.
</details>
<details>
<summary>摘要</summary>
近期研究表明，在回归问题中强制维度结构 onto 参数矩阵可以导致更可靠的参数估计和较低的样本复杂度，相比vector化方法。这个工作 investigate一种新的低级别维度模型（LSR）在泛化线性模型（GLM）问题中。LSR模型是Tucker和CANDECOMP/PARAFAC（CP）模型的推广，并是阻塞矩阵分解（BTD）模型的特殊情况。这个工作提出了一种块坐标极下降算法 для GLM问题中LSR结构的参数估计。最重要的是，它 derivates一个最小最大下界对于GLM问题中LSR结构参数估计的误差阈值。这个下界与LSR结构参数估计的内在度度相关，表明其样本复杂度可能远低于vector化GLM的样本复杂度。这个结果还可以特殊化为对CP和Tucker结构GLM的参数估计误差的下界。 derivates bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples.
</details></li>
</ul>
<hr>
<h2 id="Spectral-Ranking-Inferences-based-on-General-Multiway-Comparisons"><a href="#Spectral-Ranking-Inferences-based-on-General-Multiway-Comparisons" class="headerlink" title="Spectral Ranking Inferences based on General Multiway Comparisons"></a>Spectral Ranking Inferences based on General Multiway Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02918">http://arxiv.org/abs/2308.02918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqing Fan, Zhipeng Lou, Weichen Wang, Mengxin Yu</li>
<li>For:  This paper studies the performance of the spectral method in estimating and quantifying uncertainty of unobserved preference scores in a very general and realistic setup.* Methods: The paper uses the spectral method, which is a more general and flexible approach than the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. The paper also introduces a two-step spectral method that can achieve the same asymptotic efficiency as the Maximum Likelihood Estimator (MLE).* Results: The paper provides comprehensive frameworks for one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It also introduces effective two-sample rank testing methods that are noteworthy. The paper substantiates its findings via comprehensive numerical simulations and applies its developed methodologies to perform statistical inferences on statistics journals and movie rankings.<details>
<summary>Abstract</summary>
This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a very general and more realistic setup in which the comparison graph consists of hyper-edges of possible heterogeneous sizes and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in the scenarios when the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptotic distributions of the estimated preference scores, we also introduce a comprehensive framework to carry out both one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It is noteworthy that it is the first time effective two-sample rank testing methods are proposed. Finally, we substantiate our findings via comprehensive numerical simulations and subsequently apply our developed methodologies to perform statistical inferences on statistics journals and movie rankings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adversarial-Erasing-with-Pruned-Elements-Towards-Better-Graph-Lottery-Ticket"><a href="#Adversarial-Erasing-with-Pruned-Elements-Towards-Better-Graph-Lottery-Ticket" class="headerlink" title="Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket"></a>Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02916">http://arxiv.org/abs/2308.02916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyuwen0627/ace-glt">https://github.com/wangyuwen0627/ace-glt</a></li>
<li>paper_authors: Yuwen Wang, Shunyu Liu, Kaixuan Chen, Tongtian Zhu, Ji Qiao, Mengjie Shi, Yuanyu Wan, Mingli Song</li>
<li>for:  Mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance.</li>
<li>methods:  Combination of core subgraph and sparse subnetwork, adversarial complementary erasing (ACE) framework to explore valuable information from pruned components.</li>
<li>results:  Outperforms existing methods for searching Graph Lottery Ticket (GLT) in diverse tasks.Here’s the summary in English for reference:</li>
<li>for: Mitigating the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance.</li>
<li>methods: Combining core subgraph and sparse subnetwork, using an adversarial complementary erasing (ACE) framework to explore valuable information from pruned components.</li>
<li>results: Outperforming existing methods for searching Graph Lottery Ticket (GLT) in diverse tasks.<details>
<summary>Abstract</summary>
Graph Lottery Ticket (GLT), a combination of core subgraph and sparse subnetwork, has been proposed to mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance. However, the winning GLTs in exisiting studies are obtained by applying iterative magnitude-based pruning (IMP) without re-evaluating and re-considering the pruned information, which disregards the dynamic changes in the significance of edges/weights during graph/model structure pruning, and thus limits the appeal of the winning tickets. In this paper, we formulate a conjecture, i.e., existing overlooked valuable information in the pruned graph connections and model parameters which can be re-grouped into GLT to enhance the final performance. Specifically, we propose an adversarial complementary erasing (ACE) framework to explore the valuable information from the pruned components, thereby developing a more powerful GLT, referred to as the ACE-GLT. The main idea is to mine valuable information from pruned edges/weights after each round of IMP, and employ the ACE technique to refine the GLT processing. Finally, experimental results demonstrate that our ACE-GLT outperforms existing methods for searching GLT in diverse tasks. Our code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
《图lottery票（GLT）》，一种组合核心子图和稀疏子网络的方法，用于降低深度图神经网络（GNNs）在大输入图上的计算成本，保持原始性能。然而，现有的研究中的赢家GLT是通过迭代矩阵优化（IMP）而不是重新评估和重新考虑被截割的信息，这会忽略图/模型结构截割中 Edge/权重的动态变化，因此限制了赢家票的吸引力。在这篇论文中，我们提出一个假设，即现有的被遗弃的有价信息在截割的图连接和模型参数中，可以重新组织成GLT，以提高最终性能。 Specifically, we propose an adversarial complementary erasing（ACE）框架，用于探索截割后每个回合的有价信息，并使用ACE技术来细化GLT处理。最后，我们的实验结果表明，我们的ACE-GLT在多种任务中超过现有方法搜索GLT的性能。我们的代码将公开发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/06/cs.LG_2023_08_06/" data-id="clltau92m005hcr88d5f47hj0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/cs.SD_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/06/cs.SD_2023_08_06/">cs.SD - 2023-08-06 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SoK-Acoustic-Side-Channels"><a href="#SoK-Acoustic-Side-Channels" class="headerlink" title="SoK: Acoustic Side Channels"></a>SoK: Acoustic Side Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03806">http://arxiv.org/abs/2308.03806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Wang, Shishir Nagaraja, Aurélien Bourquard, Haichang Gao, Jeff Yan</li>
<li>for: 本研究做出了一个 state-of-the-art 的分析，涵盖了全部重要的学术研究领域，讨论了安全性含义和对策，并寻找了未来研究的方向。</li>
<li>methods: 本研究使用了多种方法，包括对渠道的分析、对 inverse problems 的研究以及两者之间的连接。</li>
<li>results: 本研究得到了一些深刻的结论，包括渠道的安全性含义和未来研究的方向。<details>
<summary>Abstract</summary>
We provide a state-of-the-art analysis of acoustic side channels, cover all the significant academic research in the area, discuss their security implications and countermeasures, and identify areas for future research. We also make an attempt to bridge side channels and inverse problems, two fields that appear to be completely isolated from each other but have deep connections.
</details>
<details>
<summary>摘要</summary>
我们提供了当今最先进的声学侧途分析，涵盖了全部主要的学术研究领域，讨论了它们的安全意义和防范措施，并确定了未来研究的方向。我们还尝试将侧途和反问题两个领域联系起来，这两个领域之前被视为完全不相关的，但它们在深层次上有着深刻的联系。
</details></li>
</ul>
<hr>
<h2 id="Characterization-of-cough-sounds-using-statistical-analysis"><a href="#Characterization-of-cough-sounds-using-statistical-analysis" class="headerlink" title="Characterization of cough sounds using statistical analysis"></a>Characterization of cough sounds using statistical analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03019">http://arxiv.org/abs/2308.03019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naveenkumar Vodnala, Pratap Reddy Lankireddy, Padmasai Yarlagadda</li>
<li>For: 本研究旨在Characterize cough sounds with voiced content and cough sounds without voiced content，以便更好地诊断呼吸疾病。* Methods: 该研究使用spectral roll-off、spectral entropy、spectral flatness、spectral flux、zero crossing rate、spectral centroid和spectral bandwidth属性来描述呼吸 зву频谱特征。这些属性 subsequentially subjected to statistical analysis using measures of minimum, maximum, mean, median, and standard deviation。* Results: 实验结果表明，呼吸音频谱特征的mean和frequency distribution高于speech signals，spectral flatness水平在0.22左右，spectral flux在0.3-0.6之间，Zero Crossing Rate大约在0.05-0.4之间。这些属性具有重要的信息价值，可以帮助更好地Characterize cough sounds。<details>
<summary>Abstract</summary>
Cough is a primary symptom of most respiratory diseases, and changes in cough characteristics provide valuable information for diagnosing respiratory diseases. The characterization of cough sounds still lacks concrete evidence, which makes it difficult to accurately distinguish between different types of coughs and other sounds. The objective of this research work is to characterize cough sounds with voiced content and cough sounds without voiced content. Further, the cough sound characteristics are compared with the characteristics of speech. The proposed method to achieve this goal utilized spectral roll-off, spectral entropy, spectral flatness, spectral flux, zero crossing rate, spectral centroid, and spectral bandwidth attributes which describe the cough sounds related to the respiratory system, glottal information, and voice model. These attributes are then subjected to statistical analysis using the measures of minimum, maximum, mean, median, and standard deviation. The experimental results show that the mean and frequency distribution of spectral roll-off, spectral centroid, and spectral bandwidth are found to be higher for cough sounds than for speech signals. Spectral flatness levels in cough sounds will rise to 0.22, whereas spectral flux varies between 0.3 and 0.6. The Zero Crossing Rate (ZCR) of most frames of cough sounds is between 0.05 and 0.4. These attributes contribute significant information while characterizing cough sounds.
</details>
<details>
<summary>摘要</summary>
<SYS>将文本翻译成简化中文。</SYS>吐は多种呼吸疾病的主要症状之一，而吐的特征变化可以提供诊断呼吸疾病的有价值信息。然而，吐 зву的特征化仍然缺乏具体证据，这使得准确地分辨吐音和其他声音变得困难。本研究的目标是Characterize吐音与有声吐音。此外，吐音特征与语音特征进行比较。提出的方法是利用spectral roll-off、spectral entropy、spectral flatness、spectral flux、zero crossing rate、spectral centroid和spectral bandwidth属性来描述吐音，这些属性是基于呼吸系统、舌叶信息和语音模型。这些属性后来通过统计分析使用 minimum、maximum、mean、median和标准差度量进行评估。实验结果表明，吐音的mean和频谱分布的 spectral roll-off、spectral centroid和spectral bandwidth均高于语音信号。吐音中的spectral flatness水平上升到0.22，而spectral flux在0.3和0.6之间变化。zero crossing rate的大多数帧为0.05和0.4。这些属性对吐音 caracterization提供了重要信息。
</details></li>
</ul>
<hr>
<h2 id="DiffDance-Cascaded-Human-Motion-Diffusion-Model-for-Dance-Generation"><a href="#DiffDance-Cascaded-Human-Motion-Diffusion-Model-for-Dance-Generation" class="headerlink" title="DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation"></a>DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02915">http://arxiv.org/abs/2308.02915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiaosong Qi, Le Zhuo, Aixi Zhang, Yue Liao, Fei Fang, Si Liu, Shuicheng Yan</li>
<li>for: 本研究旨在生成高分辨率、长形舞蹈序列，以便与音乐进行 conditional generation。</li>
<li>methods: 我们提出了一种新的层次动态扩散模型，即 DiffDance，以解决传统autoregressive方法在采样中引入折衔错误和长期结构捕捉的限制。我们还采用了多种几何损失来限制模型输出的物理可能性，并在扩散过程中采用动态权重来促进样本多样性。</li>
<li>results: 我们通过对 AIST++ 数据集进行 comprehensive 试验，证明了 DiffDance 能够生成与输入音乐高效对齐的实际舞蹈序列，并与状态静态方法相当。<details>
<summary>Abstract</summary>
When hearing music, it is natural for people to dance to its rhythm. Automatic dance generation, however, is a challenging task due to the physical constraints of human motion and rhythmic alignment with target music. Conventional autoregressive methods introduce compounding errors during sampling and struggle to capture the long-term structure of dance sequences. To address these limitations, we present a novel cascaded motion diffusion model, DiffDance, designed for high-resolution, long-form dance generation. This model comprises a music-to-dance diffusion model and a sequence super-resolution diffusion model. To bridge the gap between music and motion for conditional generation, DiffDance employs a pretrained audio representation learning model to extract music embeddings and further align its embedding space to motion via contrastive loss. During training our cascaded diffusion model, we also incorporate multiple geometric losses to constrain the model outputs to be physically plausible and add a dynamic loss weight that adaptively changes over diffusion timesteps to facilitate sample diversity. Through comprehensive experiments performed on the benchmark dataset AIST++, we demonstrate that DiffDance is capable of generating realistic dance sequences that align effectively with the input music. These results are comparable to those achieved by state-of-the-art autoregressive methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Anonymizing-Speech-Evaluating-and-Designing-Speaker-Anonymization-Techniques"><a href="#Anonymizing-Speech-Evaluating-and-Designing-Speaker-Anonymization-Techniques" class="headerlink" title="Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques"></a>Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04455">http://arxiv.org/abs/2308.04455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deep-privacy/SA-toolkit">https://github.com/deep-privacy/SA-toolkit</a></li>
<li>paper_authors: Pierre Champion<br>for:  This thesis proposes solutions for anonymizing speech and evaluating the degree of the anonymization to address the privacy issues arising from the collection and storage of personal speech data in voice-based digital assistants.methods: The thesis employs a combination of voice conversion-based anonymization systems and quantization-based transformation methods to reduce speaker PPI while maintaining utility.results: The thesis evaluates the degree of privacy protection provided by these methods and proposes a new attack method to invert anonymization, highlighting the limitations of current anonymization systems and identifying areas for improvement.<details>
<summary>Abstract</summary>
The growing use of voice user interfaces has led to a surge in the collection and storage of speech data. While data collection allows for the development of efficient tools powering most speech services, it also poses serious privacy issues for users as centralized storage makes private personal speech data vulnerable to cyber threats. With the increasing use of voice-based digital assistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the increasing ease with which personal speech data can be collected, the risk of malicious use of voice-cloning and speaker/gender/pathological/etc. recognition has increased.   This thesis proposes solutions for anonymizing speech and evaluating the degree of the anonymization. In this work, anonymization refers to making personal speech data unlinkable to an identity while maintaining the usefulness (utility) of the speech signal (e.g., access to linguistic content). We start by identifying several challenges that evaluation protocols need to consider to evaluate the degree of privacy protection properly. We clarify how anonymization systems must be configured for evaluation purposes and highlight that many practical deployment configurations do not permit privacy evaluation. Furthermore, we study and examine the most common voice conversion-based anonymization system and identify its weak points before suggesting new methods to overcome some limitations. We isolate all components of the anonymization system to evaluate the degree of speaker PPI associated with each of them. Then, we propose several transformation methods for each component to reduce as much as possible speaker PPI while maintaining utility. We promote anonymization algorithms based on quantization-based transformation as an alternative to the most-used and well-known noise-based approach. Finally, we endeavor a new attack method to invert anonymization.
</details>
<details>
<summary>摘要</summary>
声音用户界面的使用量在增长，导致了个人语音数据的收集和存储。而这些数据的收集可以为语音服务的开发提供高效的工具，但也会对用户的隐私造成严重的威胁，因为中央存储的私人语音数据容易受到网络攻击。随着智能语音助手 like Amazon Alexa、Google Home 和 Apple Siri 的使用的加密，以及个人语音数据的收集变得更加容易，黑客利用语音恶作剂和 speaker/性别/疾病等识别的风险也在增加。本论目标是提出一种隐藏个人语音数据的方法，以保护用户的隐私。在这个过程中，我们认为需要考虑以下几个挑战：1. 评估隐私保护程度：我们需要设计一种评估隐私保护程度的方法，以确保个人语音数据不可链接到用户身份。2. 配置评估系统：我们需要配置评估系统，以便在实际应用中进行评估。3. 避免攻击：我们需要研究和探讨常见的语音转换基于隐藏的攻击方法，并提出新的方法来解决一些限制。我们认为，以下几个方法可以用于隐藏个人语音数据：1. 量化变换：我们可以使用量化变换来减少说话人的个人特征，以保护用户的隐私。2. 隐藏语音特征：我们可以使用隐藏语音特征的技术来隐藏个人语音数据，以降低黑客的攻击风险。3. 多模型融合：我们可以使用多个模型来融合语音数据，以提高隐私保护的效果。最后，我们提出了一种新的攻击方法，可以尝试将隐藏后的语音数据恢复到原始形式。这种攻击方法可以帮助我们更好地理解隐私保护的限制，并提高隐私保护的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/06/cs.SD_2023_08_06/" data-id="clltau93q008ocr88dkez3onh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/eess.IV_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/06/eess.IV_2023_08_06/">eess.IV - 2023-08-06 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FourLLIE-Boosting-Low-Light-Image-Enhancement-by-Fourier-Frequency-Information"><a href="#FourLLIE-Boosting-Low-Light-Image-Enhancement-by-Fourier-Frequency-Information" class="headerlink" title="FourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information"></a>FourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03033">http://arxiv.org/abs/2308.03033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangchx67/fourllie">https://github.com/wangchx67/fourllie</a></li>
<li>paper_authors: Chenxi Wang, Hongjun Wu, Zhi Jin</li>
<li>for: 提高低光照图像的亮度和细节</li>
<li>methods: 基于快执行 fourier 变换，利用振荡频率信息和地图信息，实现低光照图像的进一步优化</li>
<li>results: 与当前最佳方法进行比较，实现了更高的亮度和细节精度，同时具有较好的模型效率<details>
<summary>Abstract</summary>
Recently, Fourier frequency information has attracted much attention in Low-Light Image Enhancement (LLIE). Some researchers noticed that, in the Fourier space, the lightness degradation mainly exists in the amplitude component and the rest exists in the phase component. By incorporating both the Fourier frequency and the spatial information, these researchers proposed remarkable solutions for LLIE. In this work, we further explore the positive correlation between the magnitude of amplitude and the magnitude of lightness, which can be effectively leveraged to improve the lightness of low-light images in the Fourier space. Moreover, we find that the Fourier transform can extract the global information of the image, and does not introduce massive neural network parameters like Multi-Layer Perceptrons (MLPs) or Transformer. To this end, a two-stage Fourier-based LLIE network (FourLLIE) is proposed. In the first stage, we improve the lightness of low-light images by estimating the amplitude transform map in the Fourier space. In the second stage, we introduce the Signal-to-Noise-Ratio (SNR) map to provide the prior for integrating the global Fourier frequency and the local spatial information, which recovers image details in the spatial space. With this ingenious design, FourLLIE outperforms the existing state-of-the-art (SOTA) LLIE methods on four representative datasets while maintaining good model efficiency.
</details>
<details>
<summary>摘要</summary>
最近，傅里叶频率信息在低光照图像增强（LLIE）中吸引了很多关注。一些研究人员注意到，在傅里叶空间中，亮度下降主要存在于振荡Component中，而剩下的存在于相位Component中。通过汇合傅里叶频率和空间信息，这些研究人员提出了非常出色的解决方案。在这项工作中，我们进一步探索了振荡幅度与亮度幅度之间的正相关关系，可以有效地提高低光照图像的亮度在傅里叶空间中。此外，我们发现傅里叶变换可以提取图像的全局信息，而不需要大量的神经网络参数，比如多层感知器（MLP）或转换器。基于这种创新的设计，我们提出了一种两个阶段的傅里叶基于LLIE网络（FourLLIE）。在第一阶段，我们使用傅里叶变换Map来提高低光照图像的亮度。在第二阶段，我们引入信噪比Map，以提供优化全局傅里叶频率和本地空间信息的优化约束，从而恢复图像的细节在空间空间中。与现有的状态的 искусственный智能（SOTA）LLIE方法相比，FourLLIE在四个代表性的数据集上达到了更高的性能，而且保持了好的模型效率。
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Spike-based-Image-Restoration-under-General-Illumination"><a href="#Recurrent-Spike-based-Image-Restoration-under-General-Illumination" class="headerlink" title="Recurrent Spike-based Image Restoration under General Illumination"></a>Recurrent Spike-based Image Restoration under General Illumination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03018">http://arxiv.org/abs/2308.03018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bit-vision/rsir">https://github.com/bit-vision/rsir</a></li>
<li>paper_authors: Lin Zhu, Yunlong Zheng, Mengyue Geng, Lizhi Wang, Hua Huang</li>
<li>for: 提高雨天或晚上场景下的激活图像重建速度</li>
<li>methods: 基于物理模型建立噪声分布，并设计了适应性激光变换模块、回归时间特征融合模块和频率域激光噪声除净模块</li>
<li>results: 对实际 dataset进行了广泛的实验，并在不同照明条件下达到了效果的重建图像Here’s a breakdown of each line:</li>
<li>for: 这篇论文的目的是提高雨天或晚上场景下的激活图像重建速度。</li>
<li>methods: 这篇论文使用了基于物理模型建立噪声分布，并设计了适应性激光变换模块、回归时间特征融合模块和频率域激光噪声除净模块来实现图像重建。</li>
<li>results: 这篇论文对实际 dataset进行了广泛的实验，并在不同照明条件下达到了效果的重建图像。<details>
<summary>Abstract</summary>
Spike camera is a new type of bio-inspired vision sensor that records light intensity in the form of a spike array with high temporal resolution (20,000 Hz). This new paradigm of vision sensor offers significant advantages for many vision tasks such as high speed image reconstruction. However, existing spike-based approaches typically assume that the scenes are with sufficient light intensity, which is usually unavailable in many real-world scenarios such as rainy days or dusk scenes. To unlock more spike-based application scenarios, we propose a Recurrent Spike-based Image Restoration (RSIR) network, which is the first work towards restoring clear images from spike arrays under general illumination. Specifically, to accurately describe the noise distribution under different illuminations, we build a physical-based spike noise model according to the sampling process of the spike camera. Based on the noise model, we design our RSIR network which consists of an adaptive spike transformation module, a recurrent temporal feature fusion module, and a frequency-based spike denoising module. Our RSIR can process the spike array in a recursive manner to ensure that the spike temporal information is well utilized. In the training process, we generate the simulated spike data based on our noise model to train our network. Extensive experiments on real-world datasets with different illuminations demonstrate the effectiveness of the proposed network. The code and dataset are released at https://github.com/BIT-Vision/RSIR.
</details>
<details>
<summary>摘要</summary>
新型生物启发式视觉传感器“穿孔相机”记录了光Intensity的形式为高时间分辨率（20,000 Hz）的脉冲数组。这种新的视觉传感器模式具有许多视觉任务的优势，如高速图像重建。然而，现有的脉冲基本approaches通常假设场景中有足够的光INTENSITY，这通常不存在在真实世界中的雨天或晚上场景。为了解锁更多的脉冲基本应用场景，我们提议了一种基于脉冲的图像修复网络（RSIR），这是首次对脉冲数组进行图像修复。 Specifically, we build a physical-based spike noise model according to the sampling process of the spike camera to accurately describe the noise distribution under different illuminations. Based on the noise model, we design our RSIR network, which consists of an adaptive spike transformation module, a recurrent temporal feature fusion module, and a frequency-based spike denoising module. Our RSIR can process the spike array in a recursive manner to ensure that the spike temporal information is well utilized. In the training process, we generate the simulated spike data based on our noise model to train our network. Extensive experiments on real-world datasets with different illuminations demonstrate the effectiveness of the proposed network.  The code and dataset are released at https://github.com/BIT-Vision/RSIR.
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Vision-Transformers-for-Pixel-Level-Identification-of-Structural-Components-and-Damage"><a href="#High-Resolution-Vision-Transformers-for-Pixel-Level-Identification-of-Structural-Components-and-Damage" class="headerlink" title="High-Resolution Vision Transformers for Pixel-Level Identification of Structural Components and Damage"></a>High-Resolution Vision Transformers for Pixel-Level Identification of Structural Components and Damage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03006">http://arxiv.org/abs/2308.03006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kareem Eltouny, Seyedomid Sajedi, Xiao Liang</li>
<li>for: 这个研究是为了提高桥梁检查的效率和可靠性，使用无人机和人工智能技术来快速和安全地进行视觉检查。</li>
<li>methods: 该研究使用了基于视Transformer和 Laplacian pyramids scaling networks的semantic segmentation网络，来高效地分析高分辨率的视觉检查图像。</li>
<li>results: 研究结果表明，该方法可以高效地分析大量的高分辨率视觉检查图像，同时保持了地方细节和全局 semantics信息，不会对计算效率造成影响。<details>
<summary>Abstract</summary>
Visual inspection is predominantly used to evaluate the state of civil structures, but recent developments in unmanned aerial vehicles (UAVs) and artificial intelligence have increased the speed, safety, and reliability of the inspection process. In this study, we develop a semantic segmentation network based on vision transformers and Laplacian pyramids scaling networks for efficiently parsing high-resolution visual inspection images. The massive amounts of collected high-resolution images during inspections can slow down the investigation efforts. And while there have been extensive studies dedicated to the use of deep learning models for damage segmentation, processing high-resolution visual data can pose major computational difficulties. Traditionally, images are either uniformly downsampled or partitioned to cope with computational demands. However, the input is at risk of losing local fine details, such as thin cracks, or global contextual information. Inspired by super-resolution architectures, our vision transformer model learns to resize high-resolution images and masks to retain both the valuable local features and the global semantics without sacrificing computational efficiency. The proposed framework has been evaluated through comprehensive experiments on a dataset of bridge inspection report images using multiple metrics for pixel-wise materials detection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用视觉检查来评估公共建筑结构，但是最新的无人机（UAV）和人工智能技术已经提高了检查过程的速度、安全性和可靠性。在这项研究中，我们开发了基于视transformer和Laplacian pyramids scaling网络的semantic segmentation网络，用于高效地分解高分辨率视检图像。收集的大量高分辨率图像可能会拖slow down调查工作，而且过去对深度学习模型用于损害分 segmentation的研究非常广泛。但是处理高分辨率视数据可以带来巨大的计算困难。传统上，图像会被uniform downsample或分割，以降低计算成本，但是输入可能会产生本地细小损害，例如细裂，或者全局Contextual信息。受到超分辨architecture的启发，我们的视transformer模型学习了resize高分辨率图像和mask，以保留valuable的本地特征和全局semantics，不会 sacrificing计算效率。我们提出的框架已经在bridge检查报告图像集上进行了完整的实验，并使用多个 метри来进行像素精度检测。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-segmentation-of-intracranial-aneurysms-using-a-3D-focal-modulation-UNet"><a href="#Weakly-supervised-segmentation-of-intracranial-aneurysms-using-a-3D-focal-modulation-UNet" class="headerlink" title="Weakly supervised segmentation of intracranial aneurysms using a 3D focal modulation UNet"></a>Weakly supervised segmentation of intracranial aneurysms using a 3D focal modulation UNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03001">http://arxiv.org/abs/2308.03001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Rasoulian, Soorena Salari, Yiming Xiao<br>for: 这 paper 的目的是提供一种高精度的自动化UIA诊断方法，以便改善Current assessment based on 2D manual measures of aneurysms on 3D MRA的评估方法。methods: 该 paper 使用了一种名为 FocalSegNet 的新型3D焦点调制UNet，以及一种名为 CRF 的后处理技术，来实现高精度的 UIA 分 segmentation。results: 该 paper 的实验结果表明，提案的算法比 state-of-the-art 3D UNet 和 Swin-UNETR 更高精度，并且 demonstarted the superiority of the proposed FocalSegNet 和 focal modulation 的 beneficial effect on the task。<details>
<summary>Abstract</summary>
Accurate identification and quantification of unruptured intracranial aneurysms (UIAs) are essential for the risk assessment and treatment decisions of this cerebrovascular disorder. Current assessment based on 2D manual measures of aneurysms on 3D magnetic resonance angiography (MRA) is sub-optimal and time-consuming. Automatic 3D measures can significantly benefit the clinical workflow and treatment outcomes. However, one major issue in medical image segmentation is the need for large well-annotated data, which can be expensive to obtain. Techniques that mitigate the requirement, such as weakly supervised learning with coarse labels are highly desirable. In this paper, we leverage coarse labels of UIAs from time-of-flight MRAs to obtain refined UIAs segmentation using a novel 3D focal modulation UNet, called FocalSegNet and conditional random field (CRF) postprocessing, with a Dice score of 0.68 and 95% Hausdorff distance of 0.95 mm. We evaluated the performance of the proposed algorithms against the state-of-the-art 3D UNet and Swin-UNETR, and demonstrated the superiority of the proposed FocalSegNet and the benefit of focal modulation for the task.
</details>
<details>
<summary>摘要</summary>
correctly 识别和量化脑血管疾病（UIAs）的精度是诊断和治疗决策的关键。现有的评估方法基于2D手动测量的感知器件图像（MRA）是下pecific和耗时consuming。自动化3D测量可以帮助优化诊断和治疗结果。然而，医疗图像分割的一个主要问题是需要大量的良好标注数据，这可以是expensive to obtain。使用弱有supervised learning with coarse labels可以减少这个问题。在这篇论文中，我们利用时间飞行扫描产生的UIAs粗略标签来获得改进的UIAs分割，使用一种新的3D焦点修饰UNet，called FocalSegNet，并与条件Random field（CRF）后处理，得到了0.68的Dice分数和0.95 mm的95% Hausdorff距离。我们评估了提议的算法与现有的3D UNet和Swin-UNETR的性能，并证明了提议的FocalSegNet的优越性和焦点修饰的好处。
</details></li>
</ul>
<hr>
<h2 id="DermoSegDiff-A-Boundary-aware-Segmentation-Diffusion-Model-for-Skin-Lesion-Delineation"><a href="#DermoSegDiff-A-Boundary-aware-Segmentation-Diffusion-Model-for-Skin-Lesion-Delineation" class="headerlink" title="DermoSegDiff: A Boundary-aware Segmentation Diffusion Model for Skin Lesion Delineation"></a>DermoSegDiff: A Boundary-aware Segmentation Diffusion Model for Skin Lesion Delineation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02959">http://arxiv.org/abs/2308.02959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mindflow-institue/dermosegdiff">https://github.com/mindflow-institue/dermosegdiff</a></li>
<li>paper_authors: Afshin Bozorgpour, Yousef Sadegheih, Amirhossein Kazerouni, Reza Azad, Dorit Merhof</li>
<li>for: 静脉皮肤病诊断 early detection 和准确诊断</li>
<li>methods: 利用 Diffusion Probabilistic Models (DDPMs) 进行静脉皮肤病诊断, 并在学习过程中加入边缘信息</li>
<li>results: 对多个皮肤分割数据集进行实验，显示 DermoSegDiff 的效果和泛化能力都较为出色，超过了现有的 CNN、transformer 和 diffusion-based 方法<details>
<summary>Abstract</summary>
Skin lesion segmentation plays a critical role in the early detection and accurate diagnosis of dermatological conditions. Denoising Diffusion Probabilistic Models (DDPMs) have recently gained attention for their exceptional image-generation capabilities. Building on these advancements, we propose DermoSegDiff, a novel framework for skin lesion segmentation that incorporates boundary information during the learning process. Our approach introduces a novel loss function that prioritizes the boundaries during training, gradually reducing the significance of other regions. We also introduce a novel U-Net-based denoising network that proficiently integrates noise and semantic information inside the network. Experimental results on multiple skin segmentation datasets demonstrate the superiority of DermoSegDiff over existing CNN, transformer, and diffusion-based approaches, showcasing its effectiveness and generalization in various scenarios. The implementation is publicly accessible on \href{https://github.com/mindflow-institue/dermosegdiff}{GitHub}
</details>
<details>
<summary>摘要</summary>
皮肤损害分割在诊断皮肤疾病的早期阶段发挥了关键作用。近年来，Diffusion Probabilistic Models（DDPMs）在图像生成方面受到了广泛关注。我们基于这些进步，提出了DermoSegDiff，一种新的皮肤损害分割框架。我们的方法引入了一个新的损失函数，在训练过程中优先级化边界信息，逐渐减少其他区域的重要性。我们还引入了一种基于U-Net的杂音级别网络，可以高效地 интеGRATE噪音和semantic信息内网络。多个皮肤分割数据集的实验结果表明，DermoSegDiff在不同的场景下对现有的CNN、transformer和Diffusion-based方法优于，demonstrating its effectiveness and generalization。实现可以在 \href{https://github.com/mindflow-institue/dermosegdiff}{GitHub} 上获取。
</details></li>
</ul>
<hr>
<h2 id="MomentaMorph-Unsupervised-Spatial-Temporal-Registration-with-Momenta-Shooting-and-Correction"><a href="#MomentaMorph-Unsupervised-Spatial-Temporal-Registration-with-Momenta-Shooting-and-Correction" class="headerlink" title="MomentaMorph: Unsupervised Spatial-Temporal Registration with Momenta, Shooting, and Correction"></a>MomentaMorph: Unsupervised Spatial-Temporal Registration with Momenta, Shooting, and Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02949">http://arxiv.org/abs/2308.02949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhangxing Bian, Shuwen Wei, Yihao Liu, Junyu Chen, Jiachen Zhuo, Fangxu Xing, Jonghye Woo, Aaron Carass, Jerry L. Prince</li>
<li>for: 用于估计大量运动和复杂征 patrerns 中的� MR 影像中的运动</li>
<li>methods: 基于 Lie 代数和 Lie 组 principls 的 “势量、射击、修正” 框架，以快速尝试到真正的 optima，并确保收敛到真正的 optima</li>
<li>results: 在 synthetic 数据集和实际 3D tMRI 数据集上，方法能够高效地估计精准、密集、 diffeomorphic 2D&#x2F;3D 运动场I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Tagged magnetic resonance imaging (tMRI) has been employed for decades to measure the motion of tissue undergoing deformation. However, registration-based motion estimation from tMRI is difficult due to the periodic patterns in these images, particularly when the motion is large. With a larger motion the registration approach gets trapped in a local optima, leading to motion estimation errors. We introduce a novel "momenta, shooting, and correction" framework for Lagrangian motion estimation in the presence of repetitive patterns and large motion. This framework, grounded in Lie algebra and Lie group principles, accumulates momenta in the tangent vector space and employs exponential mapping in the diffeomorphic space for rapid approximation towards true optima, circumventing local optima. A subsequent correction step ensures convergence to true optima. The results on a 2D synthetic dataset and a real 3D tMRI dataset demonstrate our method's efficiency in estimating accurate, dense, and diffeomorphic 2D/3D motion fields amidst large motion and repetitive patterns.
</details>
<details>
<summary>摘要</summary>
带标记的核磁共振成像（tMRI）已经在数十年中用于测量软组织中的运动。然而，基于匹配的运动估计从tMRI中很难进行注册，尤其是当运动较大时。当运动较大时，匹配方法会被困在地方最优点中，导致运动估计错误。我们介绍了一种新的“动量、射击和修正”框架，用于在具有循环Patterns和大运动时进行劳动动量估计。这个框架基于李 álgebra和李群原理，在 Tangent vector space中积累动量，使用 экспоненциаль映射在 diffeomorphic 空间中快速 Approximate towards true optima， circumventing local optima。后续的修正步骤确保了 converges to true optima。Synthetic dataset和实际的3D tMRI dataset results demonstrate our method's efficiency in estimating accurate, dense, and diffeomorphic 2D/3D motion fields amidst large motion and repetitive patterns.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/06/eess.IV_2023_08_06/" data-id="clltau95800dhcr888xcs70tg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/cs.LG_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/05/cs.LG_2023_08_05/">cs.LG - 2023-08-05 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Edge-of-stability-echo-state-networks"><a href="#Edge-of-stability-echo-state-networks" class="headerlink" title="Edge of stability echo state networks"></a>Edge of stability echo state networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02902">http://arxiv.org/abs/2308.02902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Ceni, Claudio Gallicchio</li>
<li>for: 这个论文提出了一种新的储存计算（Reservoir Computing，RC）架构，称为Edge of Stability Echo State Network（ES$^2$N）。</li>
<li>methods: 该模型基于定义储存层为非线性储存（如标准ESN）和线性储存（实现正交变换）的concat。 我们提供了整个数学分析，证明ES2N map的Jacobian的全谱特征在一个可控的圆锥形范围内，并利用这个性质来证明ES$^2$N的前向动力系统在设计的边缘混乱 режиobe动。</li>
<li>results: 我们的实验分析显示，新引入的储存模型可以达到理论上的最大短期记忆容量。同时，相比标准ESN，ES$^2$N具有更好的记忆和非线性之间的融合，以及在推理非线性模型方面的显著改进。<details>
<summary>Abstract</summary>
In this paper, we propose a new Reservoir Computing (RC) architecture, called the Edge of Stability Echo State Network (ES$^2$N). The introduced ES$^2$N model is based on defining the reservoir layer as a convex combination of a nonlinear reservoir (as in the standard ESN), and a linear reservoir that implements an orthogonal transformation. We provide a thorough mathematical analysis of the introduced model, proving that the whole eigenspectrum of the Jacobian of the ES2N map can be contained in an annular neighbourhood of a complex circle of controllable radius, and exploit this property to demonstrate that the ES$^2$N's forward dynamics evolves close to the edge-of-chaos regime by design. Remarkably, our experimental analysis shows that the newly introduced reservoir model is able to reach the theoretical maximum short-term memory capacity. At the same time, in comparison to standard ESN, ES$^2$N is shown to offer a favorable trade-off between memory and nonlinearity, as well as a significant improvement of performance in autoregressive nonlinear modeling.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的抽象计算（Reservoir Computing，RC）架构，称为边缘稳定声纳网络（ES$^2$N）。我们的ES$^2$N模型基于定义储存层为非线性储存（如标准ESN）和线性储存实现正交变换的混合体。我们对引入的模型进行了深入的数学分析，证明整个特征值谱可以在控制的圆盘内含义，并利用这个性质来证明ES$^2$N的前向动力系统在设计上靠近边缘混乱 режи宜。在实验中，我们发现新引入的储存模型能够达到理论上的最大短期记忆容量。同时，相比标准ESN，ES$^2$N表现出了更好的记忆与非线性之间的质量协议，以及在抽象非线性模型中显著的性能改善。
</details></li>
</ul>
<hr>
<h2 id="Textual-Data-Mining-for-Financial-Fraud-Detection-A-Deep-Learning-Approach"><a href="#Textual-Data-Mining-for-Financial-Fraud-Detection-A-Deep-Learning-Approach" class="headerlink" title="Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach"></a>Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03800">http://arxiv.org/abs/2308.03800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuru Li</li>
<li>for: 这个研究旨在使用深度学习方法进行自然语言处理（NLP）Binary分类任务，以分析金融诈骗文本。</li>
<li>methods: 我使用了多种神经网络模型，包括多层权重层、vanilla RNN、LSTM和GRU来进行文本分类任务。</li>
<li>results: 我的结果表明，使用这些多种神经网络模型可以提高金融诈骗检测的准确率，这些结果对于金融诈骗检测有着重要的意义，并为业界实践者、监管机构和研究人员提供有价值的洞察。<details>
<summary>Abstract</summary>
In this report, I present a deep learning approach to conduct a natural language processing (hereafter NLP) binary classification task for analyzing financial-fraud texts. First, I searched for regulatory announcements and enforcement bulletins from HKEX news to define fraudulent companies and to extract their MD&A reports before I organized the sentences from the reports with labels and reporting time. My methodology involved different kinds of neural network models, including Multilayer Perceptrons with Embedding layers, vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) for the text classification task. By utilizing this diverse set of models, I aim to perform a comprehensive comparison of their accuracy in detecting financial fraud. My results bring significant implications for financial fraud detection as this work contributes to the growing body of research at the intersection of deep learning, NLP, and finance, providing valuable insights for industry practitioners, regulators, and researchers in the pursuit of more robust and effective fraud detection methodologies.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我使用深度学习方法来进行自然语言处理（以下简称 NLP）的二分类任务，以分析金融诈骗文本。首先，我从香港证券交易所新闻中搜索了规范公告和执行通知，以定义诈骗公司并提取其财务报告。然后，我将报告中的句子分为标签和时间排序。我的方法包括多层感知器、普通逻辑神经网络、长短期记忆网络（LSTM）和闭合逻辑Unit（GRU）等不同类型的神经网络模型，用于文本分类任务。通过利用这些多样化的模型，我希望能够对金融诈骗检测的准确率进行全面的比较。我的结果对金融诈骗检测具有重要的意义，这项工作将加入深度学习、NLP和金融之间的交叉领域的研究中，为业内专业人士、监管部门和研究人员提供价值的发现。
</details></li>
</ul>
<hr>
<h2 id="Elucidate-Gender-Fairness-in-Singing-Voice-Transcription"><a href="#Elucidate-Gender-Fairness-in-Singing-Voice-Transcription" class="headerlink" title="Elucidate Gender Fairness in Singing Voice Transcription"></a>Elucidate Gender Fairness in Singing Voice Transcription</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02898">http://arxiv.org/abs/2308.02898</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guxm2021/svt_speechbrain">https://github.com/guxm2021/svt_speechbrain</a></li>
<li>paper_authors: Xiangming Gu, Wei Zeng, Ye Wang</li>
<li>for:  investigate the performance disparity in singing voice transcription (SVT) between males and females, and propose a method to address the gender bias.</li>
<li>methods: use an attribute predictor to predict gender labels and adversarially train the SVT system to enforce the gender-invariance of acoustic representations, conditionally align acoustic representations between demographic groups by feeding note events to the attribute predictor.</li>
<li>results: significant reduction of gender bias (up to more than 50%) with negligible degradation of overall SVT performance, on both in-domain and out-of-domain singing data, offering a better fairness-utility trade-off.<details>
<summary>Abstract</summary>
It is widely known that males and females typically possess different sound characteristics when singing, such as timbre and pitch, but it has never been explored whether these gender-based characteristics lead to a performance disparity in singing voice transcription (SVT), whose target includes pitch. Such a disparity could cause fairness issues and severely affect the user experience of downstream SVT applications. Motivated by this, we first demonstrate the female superiority of SVT systems, which is observed across different models and datasets. We find that different pitch distributions, rather than gender data imbalance, contribute to this disparity. To address this issue, we propose using an attribute predictor to predict gender labels and adversarially training the SVT system to enforce the gender-invariance of acoustic representations. Leveraging the prior knowledge that pitch distributions may contribute to the gender bias, we propose conditionally aligning acoustic representations between demographic groups by feeding note events to the attribute predictor. Empirical experiments on multiple benchmark SVT datasets show that our method significantly reduces gender bias (up to more than 50%) with negligible degradation of overall SVT performance, on both in-domain and out-of-domain singing data, thus offering a better fairness-utility trade-off.
</details>
<details>
<summary>摘要</summary>
广泛知道，男女在唱歌时通常具有不同的音色特征，如音 timbre 和音高，但这些 gender-based 特征是否会导致唱歌voice transcription（SVT）中的性别偏袋问题？如果存在这种偏袋问题，那么这将导致 fairness 问题并且严重地影响下游 SVT 应用程序的用户体验。为了解决这个问题，我们首先示出了女性 SVT 系统的优势，这种优势可以在不同的模型和数据集上被观察到。我们发现，不同的投射分布，而不是性别数据不均衡，是导致这种偏袋问题的主要原因。为了解决这个问题，我们提议使用一个 attribute predictor 来预测性别标签，并在 SVT 系统中进行对 gender-invariance 的 adversarial 训练。利用投射分布可能会导致性别偏袋的知识，我们提议通过将 note events 传递给 attribute predictor，来Conditional 地将音频表示同步。我们的方法在多个标准 SVT 数据集上进行了实验，结果显示，我们的方法可以减少性别偏袋（达到50%以上），同时不会影响 SVT 总性能，从而提供了更好的 fairness-utility 交易。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-Gaussian-process-model-for-Euler-Bernoulli-beam-elements"><a href="#Physics-informed-Gaussian-process-model-for-Euler-Bernoulli-beam-elements" class="headerlink" title="Physics-informed Gaussian process model for Euler-Bernoulli beam elements"></a>Physics-informed Gaussian process model for Euler-Bernoulli beam elements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02894">http://arxiv.org/abs/2308.02894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gledson Rodrigo Tondo, Sebastian Rau, Igor Kavrakov, Guido Morgenthal</li>
<li>for: 这个论文是用于开发一种基于物理学习的、多输出泛化过程的机器学习模型，用于估计结构的弯矩稳定性。</li>
<li>methods: 这个模型使用了欧拉-伯涅瓦梁式方程，并通过适当的数据集来训练。</li>
<li>results: 模型可以用来描述结构的弯矩稳定性，进行 interpolate 和 probabilistic 推断，并在结构健康监测中使用 Mahalanobis 距离来评估结构系统中可能的损害的位置和范围。<details>
<summary>Abstract</summary>
A physics-informed machine learning model, in the form of a multi-output Gaussian process, is formulated using the Euler-Bernoulli beam equation. Given appropriate datasets, the model can be used to regress the analytical value of the structure's bending stiffness, interpolate responses, and make probabilistic inferences on latent physical quantities. The developed model is applied on a numerically simulated cantilever beam, where the regressed bending stiffness is evaluated and the influence measurement noise on the prediction quality is investigated. Further, the regressed probabilistic stiffness distribution is used in a structural health monitoring context, where the Mahalanobis distance is employed to reason about the possible location and extent of damage in the structural system. To validate the developed framework, an experiment is conducted and measured heterogeneous datasets are used to update the assumed analytical structural model.
</details>
<details>
<summary>摘要</summary>
一种physics-informed机器学习模型，具体来说是一种多输出 Gaussian process，基于Euler-Bernoulli梁式方程。给定合适的数据集，该模型可以用来回归分析结构的弯矩稳定性， interpolate 响应，以及进行 probabilistic 推断 Physical quantity 的存在。该模型在 numerically simulated  cantilever beam 上进行应用，其中推断的弯矩稳定性被评估，并investigate 测量噪声对预测质量的影响。此外，推断的 probabilistic 弯矩分布被用于结构健康监测上，通过 Mahalanobis distance 来了解结构系统中可能的损害位置和范围。为验证开发的框架，进行了实验，并使用测量的 hetereogeneous 数据集来更新假设的分析结构模型。
</details></li>
</ul>
<hr>
<h2 id="Secure-Deep-JSCC-Against-Multiple-Eavesdroppers"><a href="#Secure-Deep-JSCC-Against-Multiple-Eavesdroppers" class="headerlink" title="Secure Deep-JSCC Against Multiple Eavesdroppers"></a>Secure Deep-JSCC Against Multiple Eavesdroppers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02892">http://arxiv.org/abs/2308.02892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyyed Amirhossein Ameli Kalkhoran, Mehdi Letafati, Ecenaz Erdemir, Babak Hossein Khalaj, Hamid Behroozi, Deniz Gündüz</li>
<li>for: 这个研究旨在提出一个基于深度学习的综合式通信安全方法，以保护传输过程中的私人资讯免受多名听者的窃取和探索。</li>
<li>methods: 这个方法使用深度学习的综合式模型，实现了一个数据驱动的安全通信方案，不需要对数据分布进行假设。</li>
<li>results: 实验结果显示，这个方法可以降低听者的攻击精度，对于不同的测试渠道（Rayleigh fading、Nakagami-m、AWGN）进行了评估。<details>
<summary>Abstract</summary>
In this paper, a generalization of deep learning-aided joint source channel coding (Deep-JSCC) approach to secure communications is studied. We propose an end-to-end (E2E) learning-based approach for secure communication against multiple eavesdroppers over complex-valued fading channels. Both scenarios of colluding and non-colluding eavesdroppers are studied. For the colluding strategy, eavesdroppers share their logits to collaboratively infer private attributes based on ensemble learning method, while for the non-colluding setup they act alone. The goal is to prevent eavesdroppers from inferring private (sensitive) information about the transmitted images, while delivering the images to a legitimate receiver with minimum distortion. By generalizing the ideas of privacy funnel and wiretap channel coding, the trade-off between the image recovery at the legitimate node and the information leakage to the eavesdroppers is characterized. To solve this secrecy funnel framework, we implement deep neural networks (DNNs) to realize a data-driven secure communication scheme, without relying on a specific data distribution. Simulations over CIFAR-10 dataset verifies the secrecy-utility trade-off. Adversarial accuracy of eavesdroppers are also studied over Rayleigh fading, Nakagami-m, and AWGN channels to verify the generalization of the proposed scheme. Our experiments show that employing the proposed secure neural encoding can decrease the adversarial accuracy by 28%.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了深度学习帮助的同时源渠道编码（Deep-JSCC）方法的扩展，以确保通信的安全性。我们提议一种终端到终端（E2E）学习基于的安全通信方法，用于对多个伪装者进行安全通信。我们研究了协作和不协作的情况下的伪装者。在协作情况下，伪装者共享其логиits来共同推理私有特征，而在不协作情况下，他们 acted alone。我们的目标是防止伪装者推理传输的图像中的私有（敏感）信息，同时将图像传输到合法接收器，并最小化干扰。通过扩展隐私管道和窃听渠道编码的想法，我们研究了图像恢复和伪装者信息泄露之间的质量负担。为解决这个隐私管道框架，我们使用深度神经网络（DNNs）来实现数据驱动的安全通信方案，不需要固定数据分布。我们的实验结果表明，通过使用我们的安全神经编码，可以降低伪装者的攻击精度，减少了28%。我们还对伪装者的攻击精度进行了随机抽样和AWGN渠道的研究，以验证我们的方案的普适性。
</details></li>
</ul>
<hr>
<h2 id="Private-Federated-Learning-with-Dynamic-Power-Control-via-Non-Coherent-Over-the-Air-Computation"><a href="#Private-Federated-Learning-with-Dynamic-Power-Control-via-Non-Coherent-Over-the-Air-Computation" class="headerlink" title="Private Federated Learning with Dynamic Power Control via Non-Coherent Over-the-Air Computation"></a>Private Federated Learning with Dynamic Power Control via Non-Coherent Over-the-Air Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02881">http://arxiv.org/abs/2308.02881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anbang Zhang, Shuaishuai Guo, Shuai Liu</li>
<li>for: 提高 Federated Learning（FL）中模型保护和性能提高的方法</li>
<li>methods: 基于动态功率控制的Over-the-Air Computation（AirComp）方案</li>
<li>results: 可以 Mitigate 时间同步错误、通道抑降和噪声的影响，并提供了理论上的整合证明。<details>
<summary>Abstract</summary>
To further preserve model weight privacy and improve model performance in Federated Learning (FL), FL via Over-the-Air Computation (AirComp) scheme based on dynamic power control is proposed. The edge devices (EDs) transmit the signs of local stochastic gradients by activating two adjacent orthogonal frequency division multi-plexing (OFDM) subcarriers, and majority votes (MVs) at the edge server (ES) are obtained by exploiting the energy accumulation on the subcarriers. Then, we propose a dynamic power control algorithm to further offset the biased aggregation of the MV aggregation values. We show that the whole scheme can mitigate the impact of the time synchronization error, channel fading and noise. The theoretical convergence proof of the scheme is re-derived.
</details>
<details>
<summary>摘要</summary>
为了进一步保护模型权重私钥和改进 Federated Learning（FL）的性能，我们提出了基于动态功率控制的 Federated Learning via Over-the-Air Computation（AirComp）方案。 Edge devices（ED）通过活动两个邻近的正交频分多普逊（OFDM）子频，将本地随机梯度签名发送到 Edge server（ES），然后通过利用频分的能量积累实现多数投票（MV）。然后，我们提出了一种动态功率控制算法，以更正偏好的MV汇聚值的偏好。我们证明了整个方案可以减轻时间同步错误、通道抑降和噪声的影响。我们重新证明了方案的理论收敛证明。
</details></li>
</ul>
<hr>
<h2 id="Meta-learning-in-healthcare-A-survey"><a href="#Meta-learning-in-healthcare-A-survey" class="headerlink" title="Meta-learning in healthcare: A survey"></a>Meta-learning in healthcare: A survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02877">http://arxiv.org/abs/2308.02877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Rafiei, Ronald Moore, Sina Jahromi, Farshid Hajati, Rishikesan Kamaleswaran</li>
<li>for: This paper aims to explore the applications of meta-learning in the healthcare domain and provide insights into how it can address critical healthcare challenges.</li>
<li>methods: The paper discusses the theoretical foundations and pivotal methods of meta-learning, including multi&#x2F;single-task learning and many&#x2F;few-shot learning.</li>
<li>results: The paper surveys various studies that have applied meta-learning in the healthcare domain and highlights the current challenges in meta-learning research, as well as potential solutions and future perspectives.<details>
<summary>Abstract</summary>
As a subset of machine learning, meta-learning, or learning to learn, aims at improving the model's capabilities by employing prior knowledge and experience. A meta-learning paradigm can appropriately tackle the conventional challenges of traditional learning approaches, such as insufficient number of samples, domain shifts, and generalization. These unique characteristics position meta-learning as a suitable choice for developing influential solutions in various healthcare contexts, where the available data is often insufficient, and the data collection methodologies are different. This survey discusses meta-learning broad applications in the healthcare domain to provide insight into how and where it can address critical healthcare challenges. We first describe the theoretical foundations and pivotal methods of meta-learning. We then divide the employed meta-learning approaches in the healthcare domain into two main categories of multi/single-task learning and many/few-shot learning and survey the studies. Finally, we highlight the current challenges in meta-learning research, discuss the potential solutions and provide future perspectives on meta-learning in healthcare.
</details>
<details>
<summary>摘要</summary>
为一种机器学习子领域，meta-learning，或学习学习，旨在提高模型的能力，通过使用先前知识和经验。meta-learning概念可以有效地解决传统学习方法的常见挑战，如样本不够、领域变化和泛化。这些特点使得meta-learning成为在医疗领域开发影响力强的解决方案的适用场景。本文首先介绍了meta-learning的理论基础和关键方法，然后将在医疗领域使用的meta-learning方法分为两个主要类别：多/单任务学习和多/少射学习，并对相关研究进行概述。最后，我们描述了当前meta-learning研究中的挑战，讨论了可能的解决方案，并提供了未来meta-learning在医疗领域的前景。
</details></li>
</ul>
<hr>
<h2 id="Data-Based-Design-of-Multi-Model-Inferential-Sensors"><a href="#Data-Based-Design-of-Multi-Model-Inferential-Sensors" class="headerlink" title="Data-Based Design of Multi-Model Inferential Sensors"></a>Data-Based Design of Multi-Model Inferential Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02872">http://arxiv.org/abs/2308.02872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Mojto, Karol Lubušký, Miroslav Fikar, Radoslav Paulen</li>
<li>for: 这篇论文关注软感知设计问题，旨在提高软感知仪器的预测性能。</li>
<li>methods: 本文提出了两种新的软感知设计方法，以提高软感知仪器的预测性能，并且可以维护其线性结构。</li>
<li>results: 对于一个实际的燃气油氢化单元，比较了多种单一模型软感知仪器和当前 referential 软感知仪器，结果表明了新方法的显著提高。<details>
<summary>Abstract</summary>
This paper deals with the problem of inferential (soft) sensor design. The nonlinear character of industrial processes is usually the main limitation to designing simple linear inferential sensors with sufficient accuracy. In order to increase the inferential sensor predictive performance and yet to maintain its linear structure, multi-model inferential sensors represent a straightforward option. In this contribution, we propose two novel approaches for the design of multi-model inferential sensors aiming to mitigate some drawbacks of the state-of-the-art approaches. For a demonstration of the developed techniques, we design inferential sensors for a Vacuum Gasoil Hydrogenation unit, which is a real-world petrochemical refinery unit. The performance of the multi-model inferential sensor is compared against various single-model inferential sensors and the current (referential) inferential sensor used in the refinery. The results show substantial improvements over the state-of-the-art design techniques for single-/multi-model inferential sensors.
</details>
<details>
<summary>摘要</summary>
In this study, we propose two new methods for designing multi-model inferential sensors. We demonstrate the effectiveness of these methods using a real-world petrochemical refinery unit, the vacuum gasoil hydrogenation unit. The performance of the multi-model inferential sensor is compared to various single-model inferential sensors and the current referential inferential sensor used in the refinery. The results show significant improvements over existing design techniques for single-/multi-model inferential sensors.The key contributions of this paper are:1. Two novel approaches for designing multi-model inferential sensors that mitigate some drawbacks of state-of-the-art techniques.2. A demonstration of the effectiveness of the proposed methods using a real-world petrochemical refinery unit.3. Comparison of the performance of the multi-model inferential sensor with various single-model inferential sensors and the current referential inferential sensor used in the refinery.The rest of the paper is organized as follows: Section 2 reviews the related work on soft sensors and multi-model inferential sensors. Section 3 describes the proposed methods for designing multi-model inferential sensors. Section 4 presents the case study of the vacuum gasoil hydrogenation unit. Section 5 compares the performance of the multi-model inferential sensor with other approaches. Finally, Section 6 concludes the paper and highlights future research directions.
</details></li>
</ul>
<hr>
<h2 id="NP-SemiSeg-When-Neural-Processes-meet-Semi-Supervised-Semantic-Segmentation"><a href="#NP-SemiSeg-When-Neural-Processes-meet-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="NP-SemiSeg: When Neural Processes meet Semi-Supervised Semantic Segmentation"></a>NP-SemiSeg: When Neural Processes meet Semi-Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02866">http://arxiv.org/abs/2308.02866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jianf-wang/np-semiseg">https://github.com/jianf-wang/np-semiseg</a></li>
<li>paper_authors: Jianfeng Wang, Daniela Massiceti, Xiaolin Hu, Vladimir Pavlovic, Thomas Lukasiewicz</li>
<li>for: 这种方法用于 assigning pixel-wise labels to unlabeled images at training time, 有助于减少标注成本和提高 segmentation 的准确率。</li>
<li>methods: 该方法使用 neural processes (NPs) for uncertainty quantification, and adapts NPs to semi-supervised semantic segmentation, resulting in a new model called NP-SemiSeg.</li>
<li>results: 实验结果表明，NP-SemiSeg 在 PASCAL VOC 2012 和 Cityscapes 上的公共benchmark上，在不同的训练设置下，具有显著的效果。<details>
<summary>Abstract</summary>
Semi-supervised semantic segmentation involves assigning pixel-wise labels to unlabeled images at training time. This is useful in a wide range of real-world applications where collecting pixel-wise labels is not feasible in time or cost. Current approaches to semi-supervised semantic segmentation work by predicting pseudo-labels for each pixel from a class-wise probability distribution output by a model. If the predicted probability distribution is incorrect, however, this leads to poor segmentation results, which can have knock-on consequences in safety critical systems, like medical images or self-driving cars. It is, therefore, important to understand what a model does not know, which is mainly achieved by uncertainty quantification. Recently, neural processes (NPs) have been explored in semi-supervised image classification, and they have been a computationally efficient and effective method for uncertainty quantification. In this work, we move one step forward by adapting NPs to semi-supervised semantic segmentation, resulting in a new model called NP-SemiSeg. We experimentally evaluated NP-SemiSeg on the public benchmarks PASCAL VOC 2012 and Cityscapes, with different training settings, and the results verify its effectiveness.
</details>
<details>
<summary>摘要</summary>
semi-supervised semantic segmentation是将不带标签的图像分类为不同类别的过程。这有很多实际应用场景，例如医疗图像或自动驾驶车辆，因为收集标签是不可能或者成本高昂。现有的方法是通过模型输出类别概率分布来预测每个像素的pseudo标签。如果预测结果不正确，则会导致 segmentation 结果差，这可能会对安全关键系统产生影响，例如医疗图像或自动驾驶车辆。因此，理解模型不知道的内容非常重要。最近，神经过程（NP）在半supervised图像分类中被探索，它们是一种计算效率高且有效的不确定量化方法。在这种工作中，我们将NP应用于半supervised semantic segmentation，得到了一种新的模型called NP-SemiSeg。我们对NP-SemiSeg进行了不同的训练设置，并在公共测试集PASCAL VOC 2012和Cityscapes上进行了实验，结果证明了它的有效性。
</details></li>
</ul>
<hr>
<h2 id="Generative-Adversarial-Networks-for-Stain-Normalisation-in-Histopathology"><a href="#Generative-Adversarial-Networks-for-Stain-Normalisation-in-Histopathology" class="headerlink" title="Generative Adversarial Networks for Stain Normalisation in Histopathology"></a>Generative Adversarial Networks for Stain Normalisation in Histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02851">http://arxiv.org/abs/2308.02851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Breen, Kieran Zucker, Katie Allen, Nishant Ravikumar, Nicolas M. Orsi</li>
<li>for: 提高临床诊断的准确率和效率，透过人工智能技术的发展。</li>
<li>methods: 主要是使用生成对抗网络（GANs）等技术进行染料标准化。</li>
<li>results: GAN-based methods typically outperform non-generative approaches, but the best method for stain normalization is still an ongoing field of study and depends on the specific scenario and performance metrics.<details>
<summary>Abstract</summary>
The rapid growth of digital pathology in recent years has provided an ideal opportunity for the development of artificial intelligence-based tools to improve the accuracy and efficiency of clinical diagnoses. One of the significant roadblocks to current research is the high level of visual variability across digital pathology images, causing models to generalise poorly to unseen data. Stain normalisation aims to standardise the visual profile of digital pathology images without changing the structural content of the images. In this chapter, we explore different techniques which have been used for stain normalisation in digital pathology, with a focus on approaches which utilise generative adversarial networks (GANs). Typically, GAN-based methods outperform non-generative approaches but at the cost of much greater computational requirements. However, it is not clear which method is best for stain normalisation in general, with different GAN and non-GAN approaches outperforming each other in different scenarios and according to different performance metrics. This is an ongoing field of study as researchers aim to identify a method which efficiently and effectively normalises pathology images to make AI models more robust and generalisable.
</details>
<details>
<summary>摘要</summary>
随着数字病理学的快速发展，提供了一个理想的机会，用于发展基于人工智能技术的工具，以提高临床诊断的准确性和效率。然而，一个 significante roadblock 是数字病理图像之间的视觉变化，导致模型很难泛化到未看过的数据。颜色标准化目的是标准化数字病理图像的视觉特征，而不改变图像的结构内容。在这章中，我们探讨了不同的技术，用于数字病理颜色标准化，特别是使用生成对抗网络（GAN）。通常，GAN基本方法在不同的场景下都会超越非生成方法，但是计算需求很高。然而，没有一个方法是最佳的，不同的 GAN 和非 GAN 方法在不同的场景和性能指标下都会出perform。这是一个持续的研究领域，研究人员希望能够找到一种能够有效地和效率地标准化病理图像的方法，以使AI模型更加可靠和泛化。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Positive-Homogeneous-Functions-with-Scale-Invariant-Neural-Networks"><a href="#Approximating-Positive-Homogeneous-Functions-with-Scale-Invariant-Neural-Networks" class="headerlink" title="Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks"></a>Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02836">http://arxiv.org/abs/2308.02836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Bamberger, Reinhard Heckel, Felix Krahmer</li>
<li>for:  investigated the possibility of solving linear inverse problems with $ReLu$ networks</li>
<li>methods:  used positive homogeneity and absence of bias terms in the network architecture</li>
<li>results:  showed that $ReLu$-networks with two hidden layers can achieve approximate recovery with arbitrary precision and sparsity level, and extended the results to a wider class of recovery problems and general positive homogeneous functions.<details>
<summary>Abstract</summary>
We investigate to what extent it is possible to solve linear inverse problems with $ReLu$ networks. Due to the scaling invariance arising from the linearity, an optimal reconstruction function $f$ for such a problem is positive homogeneous, i.e., satisfies $f(\lambda x) = \lambda f(x)$ for all non-negative $\lambda$. In a $ReLu$ network, this condition translates to considering networks without bias terms. We first consider recovery of sparse vectors from few linear measurements. We prove that $ReLu$- networks with only one hidden layer cannot even recover $1$-sparse vectors, not even approximately, and regardless of the width of the network. However, with two hidden layers, approximate recovery with arbitrary precision and arbitrary sparsity level $s$ is possible in a stable way. We then extend our results to a wider class of recovery problems including low-rank matrix recovery and phase retrieval. Furthermore, we also consider the approximation of general positive homogeneous functions with neural networks. Extending previous work, we establish new results explaining under which conditions such functions can be approximated with neural networks. Our results also shed some light on the seeming contradiction between previous works showing that neural networks for inverse problems typically have very large Lipschitz constants, but still perform very well also for adversarial noise. Namely, the error bounds in our expressivity results include a combination of a small constant term and a term that is linear in the noise level, indicating that robustness issues may occur only for very small noise levels.
</details>
<details>
<summary>摘要</summary>
我们研究可以使用 $ReLu$ 网络解决线性逆问题的可能性。由于线性的扩展对称性，一个好的复原函数 $f$ 的选择会是正Homogeneous，即满足 $f(\lambda x) = \lambda f(x)$ 的所有非负 $\lambda$。在 $ReLu$ 网络中，这个条件可以翻译为不包含偏好项。我们首先考虑从几个线性量测中回复簇节点。我们证明了 $ReLu$ 网络只有一个隐藏层时无法复原 $1$-簇节点，不管网络宽度如何。但是，具有两个隐藏层的 $ReLu$ 网络可以在稳定的方式下复原任意精度和簇节点数量 $s$。我们随后将结果扩展到更加广泛的复原问题，包括低维矩阵复原和相位回复。此外，我们还考虑了一般正Homogeneous函数的逼近，并建立了新的结果，说明在哪些情况下，这些函数可以透过神经网络逼近。我们的结果还照明了对于过去的研究所提出的对立之处，即神经网络 для反对数学问题通常具有非常大的Lipschitz常数，但是还是能够非常好地运行也在阶梯误差下。这意味着可能在非常小的误差水平下，发生了Robustness问题。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Financial-Index-Tracking"><a href="#Reinforcement-Learning-for-Financial-Index-Tracking" class="headerlink" title="Reinforcement Learning for Financial Index Tracking"></a>Reinforcement Learning for Financial Index Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02820">http://arxiv.org/abs/2308.02820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dppalomar/sparseindextracking">https://github.com/dppalomar/sparseindextracking</a></li>
<li>paper_authors: Xianhua Peng, Chenyin Gong, Xue Dong He</li>
<li>for: 本研究旨在提出一种精细时间架构的财务指数追踪问题解决方案，以满足返回基本追踪误差和价值基本追踪误差两种不同的追踪目标。</li>
<li>methods: 本研究使用了离散时间无限远景动态模型，并使用了Banach固定点迭代法解决端口重新平衡方程。此外，本研究还提出了一种基于深度强化学习（RL）方法的解决方案，以解决数据有限性问题。</li>
<li>results: 实验结果表明，提出的方法可以超过标准方法在追踪准确性和赚利率方面表现出色，并且可以通过具有策略的现金投入或抽取来实现额外的收益。<details>
<summary>Abstract</summary>
We propose the first discrete-time infinite-horizon dynamic formulation of the financial index tracking problem under both return-based tracking error and value-based tracking error. The formulation overcomes the limitations of existing models by incorporating the intertemporal dynamics of market information variables not limited to prices, allowing exact calculation of transaction costs, accounting for the tradeoff between overall tracking error and transaction costs, allowing effective use of data in a long time period, etc. The formulation also allows novel decision variables of cash injection or withdraw. We propose to solve the portfolio rebalancing equation using a Banach fixed point iteration, which allows to accurately calculate the transaction costs specified as nonlinear functions of trading volumes in practice. We propose an extension of deep reinforcement learning (RL) method to solve the dynamic formulation. Our RL method resolves the issue of data limitation resulting from the availability of a single sample path of financial data by a novel training scheme. A comprehensive empirical study based on a 17-year-long testing set demonstrates that the proposed method outperforms a benchmark method in terms of tracking accuracy and has the potential for earning extra profit through cash withdraw strategy.
</details>
<details>
<summary>摘要</summary>
我们提出了首个精细时间无限远景动态模型，用于跟踪金融指数问题，包括返点基本跟踪错误和价值基本跟踪错误。该模型超越了现有模型的局限性，因为它包含市场信息变量的时间动态，允许精确计算交易成本，考虑跟踪错误和交易成本之间的贸易做，使用长时间期间的数据，等等。该模型还允许新的决策变量：资金注入或撤回。我们提出使用巴нах固定点迭代法解决portfolio重新平衡方程，可以准确计算交易成本，实际上是非线性函数的交易量。我们还提出了RL方法的扩展，用于解决动态模型。我们的RL方法可以在单个财务数据样本路径上解决数据有限制的问题，通过一种新的训练方案。我们的实验表明，我们的方法在17年的测试集上表现出色，超过了参考方法的跟踪精度，并且有可能通过资金撤回策略获得额外利润。
</details></li>
</ul>
<hr>
<h2 id="A-generative-model-for-surrogates-of-spatial-temporal-wildfire-nowcasting"><a href="#A-generative-model-for-surrogates-of-spatial-temporal-wildfire-nowcasting" class="headerlink" title="A generative model for surrogates of spatial-temporal wildfire nowcasting"></a>A generative model for surrogates of spatial-temporal wildfire nowcasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02810">http://arxiv.org/abs/2308.02810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sibo Cheng, Yike Guo, Rossella Arcucci</li>
<li>for: 预测野火发展 (predicting wildfire development)</li>
<li>methods: 使用三维vector-量化类似自适应器 (using three-dimensional Vector-Quantized Variational Autoencoders)</li>
<li>results: 成功生成了可见度和结构的野火燃烧区域 (successfully generated coherent and structured wildfire burned areas)<details>
<summary>Abstract</summary>
Recent increase in wildfires worldwide has led to the need for real-time fire nowcasting. Physics-driven models, such as cellular automata and computational fluid dynamics can provide high-fidelity fire spread simulations but they are computationally expensive and time-consuming. Much effort has been put into developing machine learning models for fire prediction. However, these models are often region-specific and require a substantial quantity of simulation data for training purpose. This results in a significant amount of computational effort for different ecoregions. In this work, a generative model is proposed using a three-dimensional Vector-Quantized Variational Autoencoders to generate spatial-temporal sequences of unseen wildfire burned areas in a given ecoregion. The model is tested in the ecoregion of a recent massive wildfire event in California, known as the Chimney fire. Numerical results show that the model succeed in generating coherent and structured fire scenarios, taking into account the impact from geophysical variables, such as vegetation and slope. Generated data are also used to train a surrogate model for predicting wildfire dissemination, which has been tested on both simulation data and the real Chimney fire event.
</details>
<details>
<summary>摘要</summary>
全球各地的野火增加的趋势，使得实时野火预测变得越来越重要。物理驱动的模型，如细胞自动机和计算流体力学，可以提供高精度的野火快速扩散模拟，但它们 Computationally expensive and time-consuming。大量的努力被投入到了机器学习模型的开发中，以预测野火。然而，这些模型通常是地域特定的，需要大量的模拟数据来训练。这会导致不同的生态区域需要大量的计算劳动。在这种情况下，本文提出了一种生成模型，使用三维向量量化自适应机制来生成未经见过的野火烧区Sequence。该模型在加利福尼亚州的一个大规模野火事件中进行了测试，称为奇尼火。numerical results show that the model successfully generated coherent and structured fire scenarios, taking into account the impact of geophysical variables such as vegetation and slope.Generated data were also used to train a surrogate model for predicting wildfire spread, which was tested on both simulation data and the real Chimney fire event.
</details></li>
</ul>
<hr>
<h2 id="MiAMix-Enhancing-Image-Classification-through-a-Multi-stage-Augmented-Mixed-Sample-Data-Augmentation-Method"><a href="#MiAMix-Enhancing-Image-Classification-through-a-Multi-stage-Augmented-Mixed-Sample-Data-Augmentation-Method" class="headerlink" title="MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixed Sample Data Augmentation Method"></a>MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixed Sample Data Augmentation Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02804">http://arxiv.org/abs/2308.02804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Liang, Youzhi Liang, Jianguo Jia</li>
<li>for: 提高深度学习模型的泛化性和性能，使其在多种计算机视觉任务中具有更好的普适性和稳定性。</li>
<li>methods: 提出了一种新的混合方法called MiAMix，它是基于混合框架的多阶段扩展混合方法，通过多种多样化的混合方法同时进行混合，并通过随机选择混合掩码的混合方法来提高混合方法。</li>
<li>results: 通过四个图像标准benchmark进行了全面的评估，并与当前状态的混合样本数据增强技术进行比较，demonstrated that MiAMix可以提高性能而无需增加计算负担。<details>
<summary>Abstract</summary>
Despite substantial progress in the field of deep learning, overfitting persists as a critical challenge, and data augmentation has emerged as a particularly promising approach due to its capacity to enhance model generalization in various computer vision tasks. While various strategies have been proposed, Mixed Sample Data Augmentation (MSDA) has shown great potential for enhancing model performance and generalization. We introduce a novel mixup method called MiAMix, which stands for Multi-stage Augmented Mixup. MiAMix integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods. Recent methods utilize saliency information and the MiAMix is designed for computational efficiency as well, reducing additional overhead and offering easy integration into existing training pipelines. We comprehensively evaluate MiaMix using four image benchmarks and pitting it against current state-of-the-art mixed sample data augmentation techniques to demonstrate that MIAMix improves performance without heavy computational overhead.
</details>
<details>
<summary>摘要</summary>
尽管深度学习领域已经取得了重大进步，但过拟合仍然是一个 kritical 挑战，而数据扩充被认为是一种有效的方法来提高模型的通用性。在不同的策略中，混合样本数据扩充（MSDA）被认为是提高模型性能和通用性的有效方法。我们介绍了一种新的mixup方法，称为MiAMix，它是多stage混合的augmentation方法。MiAMix将图像扩充integrated into the mixup框架，并同时使用多种多样化的混合方法，通过随机选择混合面积的混合方法来提高混合方法。现有的方法使用了saliency信息，而MiAMix是为计算效率而设计的，减少了额外的负担和提供了与现有训练管道的集成。我们对MiaMix进行了四个图像标准 benchMark 的完整评估，并与当前状态的混合样本数据扩充技术进行了比较，以示MiAMix可以提高性能而不带重大计算负担。
</details></li>
</ul>
<hr>
<h2 id="OBESEYE-Interpretable-Diet-Recommender-for-Obesity-Management-using-Machine-Learning-and-Explainable-AI"><a href="#OBESEYE-Interpretable-Diet-Recommender-for-Obesity-Management-using-Machine-Learning-and-Explainable-AI" class="headerlink" title="OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine Learning and Explainable AI"></a>OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine Learning and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02796">http://arxiv.org/abs/2308.02796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mrinmoy Roy, Srabonti Das, Anica Tasnim Protity</li>
<li>For: The paper aims to provide a novel machine learning-based system to predict the amount of nutrients an individual requires for being healthy, with a focus on patients with comorbidities.* Methods: The paper uses various machine learning algorithms, including linear regression, support vector machine (SVM), decision tree, random forest, XGBoost, and LightGBM, to predict fluid, carbohydrate, protein, and fat consumption.* Results: The paper achieves high accuracy with low root mean square error (RMSE) using linear regression in fluid prediction, random forest in carbohydrate prediction, and LightGBM in protein and fat prediction. The system, called OBESEYE, is the only one of its kind to consider comorbidities and physical conditions when recommending diets.Here’s the simplified Chinese text for the three main points:* For: 这篇论文目的是提供一种基于机器学习的健康饮食计划，特别是为患有多种疾病的患者。* Methods: 论文使用了不同的机器学习算法，包括线性回归、支持向量机（SVM）、决策树、Random Forest、XGBoost和LightGBM等，来预测 fluid、碳水化合物、蛋白质和脂肪的消耗。* Results: 论文在 fluid 预测中使用线性回归得到了高精度低根平方差（RMSE），在碳水化合物预测中使用Random Forest 得到了高精度，在蛋白质和脂肪预测中使用LightGBM 得到了高精度。OBESEYE 系统是唯一考虑了患者的COMorbidities和物理状况的饮食建议系统。<details>
<summary>Abstract</summary>
Obesity, the leading cause of many non-communicable diseases, occurs mainly for eating more than our body requirements and lack of proper activity. So, being healthy requires heathy diet plans, especially for patients with comorbidities. But it is difficult to figure out the exact quantity of each nutrient because nutrients requirement varies based on physical and disease conditions. In our study we proposed a novel machine learning based system to predict the amount of nutrients one individual requires for being healthy. We applied different machine learning algorithms: linear regression, support vector machine (SVM), decision tree, random forest, XGBoost, LightGBM on fluid and 3 other major micronutrients: carbohydrate, protein, fat consumption prediction. We achieved high accuracy with low root mean square error (RMSE) by using linear regression in fluid prediction, random forest in carbohydrate prediction and LightGBM in protein and fat prediction. We believe our diet recommender system, OBESEYE, is the only of its kind which recommends diet with the consideration of comorbidities and physical conditions and promote encouragement to get rid of obesity.
</details>
<details>
<summary>摘要</summary>
肥胖是多种非传染疾病的主要原因，主要由于食物摄入量超过身体需求，以及不足的适当活动。因此，保持健康需要健康的饮食计划，特别是 для患有多种疾病的患者。然而，确定每个营养素的准确量是困难的，因为营养素需求因 físical 和疾病状况而异。在我们的研究中，我们提出了一种基于机器学习的系统，可以预测个人需要的营养素量。我们应用了不同的机器学习算法：线性回归、支持向量机(SVM)、决策树、随机森林、XGBoost、LightGBM 等，对流体和三种主要微量粮类：碳水化合物、蛋白质、脂肪摄入预测。我们得到了高精度低根平方差(RMSE)的结果，通过线性回归在流体预测中，随机森林在碳水化合物预测中，LightGBM在蛋白质和脂肪预测中。我们认为我们的饮食建议系统“OBESEYE”是目前唯一一个考虑了慢性疾病和物理状况，并且激励人们做减肥的健康饮食建议系统。
</details></li>
</ul>
<hr>
<h2 id="OrcoDCS-An-IoT-Edge-Orchestrated-Online-Deep-Compressed-Sensing-Framework"><a href="#OrcoDCS-An-IoT-Edge-Orchestrated-Online-Deep-Compressed-Sensing-Framework" class="headerlink" title="OrcoDCS: An IoT-Edge Orchestrated Online Deep Compressed Sensing Framework"></a>OrcoDCS: An IoT-Edge Orchestrated Online Deep Compressed Sensing Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05757">http://arxiv.org/abs/2308.05757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Wei Ching, Chirag Gupta, Zi Huang, Liting Hu</li>
<li>for: 这个研究是为了提出一个可以灵活地适应不同感知任务和环境变化的协调式深度压缩数据聚合（CDA）框架，以提高这些应用的性能和可扩展性。</li>
<li>methods: 这个框架使用了专门设计的对称启发器，并与边缘设备进行协调，以实现在无线感知网络（WSN）上的线上执行和训练。</li>
<li>results: 这个研究显示，在训练时间和灵活性方面，OrcoDCS比前一代的深度压缩数据聚合（DCDA）要好，并且在进一步应用中实现了更高的性能。<details>
<summary>Abstract</summary>
Compressed data aggregation (CDA) over wireless sensor networks (WSNs) is task-specific and subject to environmental changes. However, the existing compressed data aggregation (CDA) frameworks (e.g., compressed sensing-based data aggregation, deep learning(DL)-based data aggregation) do not possess the flexibility and adaptivity required to handle distinct sensing tasks and environmental changes. Additionally, they do not consider the performance of follow-up IoT data-driven deep learning (DL)-based applications. To address these shortcomings, we propose OrcoDCS, an IoT-Edge orchestrated online deep compressed sensing framework that offers high flexibility and adaptability to distinct IoT device groups and their sensing tasks, as well as high performance for follow-up applications. The novelty of our work is the design and deployment of IoT-Edge orchestrated online training framework over WSNs by leveraging an specially-designed asymmetric autoencoder, which can largely reduce the encoding overhead and improve the reconstruction performance and robustness. We show analytically and empirically that OrcoDCS outperforms the state-of-the-art DCDA on training time, significantly improves flexibility and adaptability when distinct reconstruction tasks are given, and achieves higher performance for follow-up applications.
</details>
<details>
<summary>摘要</summary>
压缩数据聚合（CDA）在无线传感网络（WSN）上是任务特定和环境变化受限的。然而，现有的CDA框架（如扩lapsed sensing基于数据聚合、深度学习（DL）基于数据聚合）不具备适应性和灵活性，无法处理不同感知任务和环境变化。另外，它们不考虑ollow-up IoT数据驱动深度学习（DL）应用的性能。为了解决这些缺点，我们提议OrcoDCS，一个基于IoT-Edge协调在线深度压缩探测框架，具有高适应性和灵活性，以及高性能 дляollow-up应用。我们利用特制的非对称自动encoder，可以大幅减少编码负担，提高重建性能和稳定性。我们通过分析和实验证明，OrcoDCS在训练时间、适应性和灵活性方面都超过了现有的DCDA。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-Contrastive-Regression-for-Estimation-of-Eye-Gaze"><a href="#Semi-supervised-Contrastive-Regression-for-Estimation-of-Eye-Gaze" class="headerlink" title="Semi-supervised Contrastive Regression for Estimation of Eye Gaze"></a>Semi-supervised Contrastive Regression for Estimation of Eye Gaze</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02784">http://arxiv.org/abs/2308.02784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somsukla Maiti, Akshansh Gupta</li>
<li>for: 这篇论文的目的是发展一个 semi-supervised contrastive learning 框架，以便估算人们的 gaze 方向。</li>
<li>methods: 本论文使用 appearance-based deep learning 模型来进行 gaze 估算，并提出了一新的对称损失函数，从中对 similary 的图像进行对比，以提高对 gaze 方向的估算精度。</li>
<li>results: 本论文的实验结果显示，这个 semi-supervised contrastive learning 框架能够从小量 annotated gaze 数据中学习一个通用的 gaze 估算模型，并且与一些state-of-the-art contrastive learning 技术相比，表现更好。<details>
<summary>Abstract</summary>
With the escalated demand of human-machine interfaces for intelligent systems, development of gaze controlled system have become a necessity. Gaze, being the non-intrusive form of human interaction, is one of the best suited approach. Appearance based deep learning models are the most widely used for gaze estimation. But the performance of these models is entirely influenced by the size of labeled gaze dataset and in effect affects generalization in performance. This paper aims to develop a semi-supervised contrastive learning framework for estimation of gaze direction. With a small labeled gaze dataset, the framework is able to find a generalized solution even for unseen face images. In this paper, we have proposed a new contrastive loss paradigm that maximizes the similarity agreement between similar images and at the same time reduces the redundancy in embedding representations. Our contrastive regression framework shows good performance in comparison to several state of the art contrastive learning techniques used for gaze estimation.
</details>
<details>
<summary>摘要</summary>
“随着人机界面智能系统的需求增加，视线控制系统的开发已成为必填。视线是非侵入式人际互动的一种最佳方法。深度学习模型基于 appearances 是目前最受欢迎的视线估计方法。但是这些模型的性能受到labelled gaze dataset的大小影响，从而影响其一般化性能。本文提出了一个半监督对称学习框架，以便透过小量labelled gaze dataset来找到一个通用的解决方案，包括未见过的脸像。本文提出了一个新的对称损失函数，它将相似的图像 Similarity 提高，并同时将嵌入表现的统计复杂度降低。我们的对称回传框架在与多个现有的对称学习技术相比之下，表现良好。”
</details></li>
</ul>
<hr>
<h2 id="Dataopsy-Scalable-and-Fluid-Visual-Exploration-using-Aggregate-Query-Sculpting"><a href="#Dataopsy-Scalable-and-Fluid-Visual-Exploration-using-Aggregate-Query-Sculpting" class="headerlink" title="Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting"></a>Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02764">http://arxiv.org/abs/2308.02764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Naimul Hoque, Niklas Elmqvist<br>for:* 这个论文是为了描述一种faceted visual query技术，帮助用户在大规模多维数据中进行查询和浏览。methods:* 这种查询技术使用的方法包括PIVOT、PARTITION、PEEK、PILE、PROJECT和PRUNE等六个步骤，用于Progressive exploration of the dataset。results:* 通过使用这种查询技术，用户可以在大规模多维数据中进行有效的查询和浏览，并且可以逐步缩小数据集，以便更好地理解数据的结构和特征。<details>
<summary>Abstract</summary>
We present aggregate query sculpting (AQS), a faceted visual query technique for large-scale multidimensional data. As a "born scalable" query technique, AQS starts visualization with a single visual mark representing an aggregation of the entire dataset. The user can then progressively explore the dataset through a sequence of operations abbreviated as P6: pivot (facet an aggregate based on an attribute), partition (lay out a facet in space), peek (see inside a subset using an aggregate visual representation), pile (merge two or more subsets), project (extracting a subset into a new substrate), and prune (discard an aggregate not currently of interest). We validate AQS with Dataopsy, a prototype implementation of AQS that has been designed for fluid interaction on desktop and touch-based mobile devices. We demonstrate AQS and Dataopsy using two case studies and three application examples.
</details>
<details>
<summary>摘要</summary>
我们介绍了统计查询雕刻（AQS），一种适合大规模多维数据的faceted visual查询技术。作为一种“生成可扩展”的查询技术，AQS开始可视化的方法是透过单一的可视示表示数据集的总聚合。用户可以逐步探索数据集通过一系列操作缩写为P6：折冲（基于特征对资料聚合进行分割）、分区（在空间中排列分割）、侦错（查看子集使用聚合图表示）、堆叠（合并两个或更多的子集）、专案（将子集转换为新基板）、剪除（不再关注的聚合）。我们验证了AQS和Dataopsy，一个实现AQS的试验版本，在桌面和触控式移动设备上进行流过交互。我们透过两个案例和三个应用例子来示范AQS和Dataopsy。
</details></li>
</ul>
<hr>
<h2 id="Neural-Collapse-in-the-Intermediate-Hidden-Layers-of-Classification-Neural-Networks"><a href="#Neural-Collapse-in-the-Intermediate-Hidden-Layers-of-Classification-Neural-Networks" class="headerlink" title="Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks"></a>Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02760">http://arxiv.org/abs/2308.02760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Parker, Emre Onal, Anton Stengel, Jake Intrater</li>
<li>for: 这paper探讨了分类神经网络中间埋点层中的Neural Collapse（NC）现象的emergence。</li>
<li>methods: 这paper使用了多种网络架构、活化函数和数据集来研究NC现象在不同层次的emergence。</li>
<li>results: 研究发现，NC现象在大多数中间埋点层中出现，其度Of collapse与层次深度正相关。此外，研究还发现，大多数减少类内方差的改进发生在神经网络的浅层，而angular separation between class means随层次深度增加。<details>
<summary>Abstract</summary>
Neural Collapse (NC) gives a precise description of the representations of classes in the final hidden layer of classification neural networks. This description provides insights into how these networks learn features and generalize well when trained past zero training error. However, to date, (NC) has only been studied in the final layer of these networks. In the present paper, we provide the first comprehensive empirical analysis of the emergence of (NC) in the intermediate hidden layers of these classifiers. We examine a variety of network architectures, activations, and datasets, and demonstrate that some degree of (NC) emerges in most of the intermediate hidden layers of the network, where the degree of collapse in any given layer is typically positively correlated with the depth of that layer in the neural network. Moreover, we remark that: (1) almost all of the reduction in intra-class variance in the samples occurs in the shallower layers of the networks, (2) the angular separation between class means increases consistently with hidden layer depth, and (3) simple datasets require only the shallower layers of the networks to fully learn them, whereas more difficult ones require the entire network. Ultimately, these results provide granular insights into the structural propagation of features through classification neural networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="WeldMon-A-Cost-effective-Ultrasonic-Welding-Machine-Condition-Monitoring-System"><a href="#WeldMon-A-Cost-effective-Ultrasonic-Welding-Machine-Condition-Monitoring-System" class="headerlink" title="WeldMon: A Cost-effective Ultrasonic Welding Machine Condition Monitoring System"></a>WeldMon: A Cost-effective Ultrasonic Welding Machine Condition Monitoring System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05756">http://arxiv.org/abs/2308.05756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beitong Tian, Kuan-Chieh Lu, Ahmadreza Eslaminia, Yaohui Wang, Chenhui Shao, Klara Nahrstedt</li>
<li>for: 这个论文是为了提出一个可靠、高性能且Cost-effective的工具状态监控系统，以提高鋳接过程中的质量控制。</li>
<li>methods: 这个系统使用自定义的数据收集系统和资料分析管道，实现实时分析。标本检测算法结合自动生成的特征和手工设计的特征，在条件分类任务中达到了95.8%的预设准确率（相比前一代方法的92.5%）。数据增强方法可以减少概念变化问题，提高工具状态分类精度8.3%。所有算法都在本地运行，仅需385毫秒过程数据。</li>
<li>results: 我们部署了WeldMon和一个商业系统在实际的鋳接机上，进行了全面的比较。我们发现，WeldMon可以提供高性能、可靠且Cost-effective的工具状态监控系统。<details>
<summary>Abstract</summary>
Ultrasonic welding machines play a critical role in the lithium battery industry, facilitating the bonding of batteries with conductors. Ensuring high-quality welding is vital, making tool condition monitoring systems essential for early-stage quality control. However, existing monitoring methods face challenges in cost, downtime, and adaptability. In this paper, we present WeldMon, an affordable ultrasonic welding machine condition monitoring system that utilizes a custom data acquisition system and a data analysis pipeline designed for real-time analysis. Our classification algorithm combines auto-generated features and hand-crafted features, achieving superior cross-validation accuracy (95.8% on average over all testing tasks) compared to the state-of-the-art method (92.5%) in condition classification tasks. Our data augmentation approach alleviates the concept drift problem, enhancing tool condition classification accuracy by 8.3%. All algorithms run locally, requiring only 385 milliseconds to process data for each welding cycle. We deploy WeldMon and a commercial system on an actual ultrasonic welding machine, performing a comprehensive comparison. Our findings highlight the potential for developing cost-effective, high-performance, and reliable tool condition monitoring systems.
</details>
<details>
<summary>摘要</summary>
服务器焊接机在锂离子电池业中发挥关键作用，帮助焊接电池与导电器。保证高质量焊接是关键，因此工具状态监测系统成为了早期质量控制的必备工具。然而，现有监测方法存在成本高、下机时间长和适应性差的问题。本文提出了WeldMon，一种可靠、高性能、成本下降的焊接机状态监测系统。WeldMon使用自定义数据获取系统和实时分析管道，并将自动生成的特征和手动设计的特征结合使用，实现了95.8%的验证精度（相对于状态艺术法92.5%）。我们的数据扩展方法可以减轻概念飘移问题，提高工具状态分类精度8.3%。所有算法都运行在本地，仅需385毫秒处理数据每个焊接周期。我们将WeldMon和一个商业系统部署在实际的服务器焊接机上，进行了完整的比较。我们的发现表明，可以开发出成本下降、高性能、可靠的工具状态监测系统。
</details></li>
</ul>
<hr>
<h2 id="DaMSTF-Domain-Adversarial-Learning-Enhanced-Meta-Self-Training-for-Domain-Adaptation"><a href="#DaMSTF-Domain-Adversarial-Learning-Enhanced-Meta-Self-Training-for-Domain-Adaptation" class="headerlink" title="DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation"></a>DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02753">http://arxiv.org/abs/2308.02753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglong Lu, Zhen Huang, Yunxiang Zhao, Zhiliang Tian, Yang Liu, Dongsheng Li</li>
<li>for: 这篇论文的目的是提出一个新的自我训练框架 для领域适应（Domain Adaptation），以解决预测错误导致的标签噪音问题。</li>
<li>methods: 这篇论文使用了自我训练的方法，将模型的预测作为目标领域中的伪标签，通过这些伪标签进行自我训练。此外，论文还提出了一个名为Domain adversarial learning enhanced Self-Training Framework（DaMSTF）的新自我训练框架，具有以下三个特点：一、使用meta-learning估算伪标签的重要性，以降低标签噪音并保留困难的例子；二、设计了一个meta constructor来建立meta-验证集，以保证meta-learning模组的有效性；三、使用领域抗击学来初始化神经网络，以解决meta-learning模组的训练导向变差问题。</li>
<li>results: 论文通过理论和实验证明了DaMSTF的有效性。在跨领域情感分类任务上，DaMSTF比BERT提高了近4%的性能。<details>
<summary>Abstract</summary>
Self-training emerges as an important research line on domain adaptation. By taking the model's prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model. Although these strategies effectively reduce the label noise, they are prone to miss the hard examples. In this paper, we propose a new self-training framework for domain adaptation, namely Domain adversarial learning enhanced Self-Training Framework (DaMSTF). Firstly, DaMSTF involves meta-learning to estimate the importance of each pseudo instance, so as to simultaneously reduce the label noise and preserve hard examples. Secondly, we design a meta constructor for constructing the meta-validation set, which guarantees the effectiveness of the meta-learning module by improving the quality of the meta-validation set. Thirdly, we find that the meta-learning module suffers from the training guidance vanishment and tends to converge to an inferior optimal. To this end, we employ domain adversarial learning as a heuristic neural network initialization method, which can help the meta-learning module converge to a better optimal. Theoretically and experimentally, we demonstrate the effectiveness of the proposed DaMSTF. On the cross-domain sentiment classification task, DaMSTF improves the performance of BERT with an average of nearly 4%.
</details>
<details>
<summary>摘要</summary>
自适应预测为域适应研究的重要线索。通过将模型的预测作为目标域无标签数据的 Pseudo 标签，自适应启动模型以 Pseudo 实例作为目标域中的训练数据。然而，预测错误（标签噪音）对自适应表现成为一个挑战。以前的方法仅使用可靠 Pseudo 实例来重新训练模型，即 Pseudo 实例具有高预测信任度。虽然这些策略有效减少标签噪音，但容易过损难例。在这篇论文中，我们提出了一种新的自适应框架 для域适应，即域适应学习强化自适应框架（DaMSTF）。首先，DaMSTF 通过元学习来估计每个 Pseudo 实例的重要性，以同时减少标签噪音并保留难例。其次，我们设计了元构建模块，用于构建元验证集，以保证元学习模块的有效性。最后，我们发现元学习模块受训练指导消失的问题，容易 converges 到一个差的优化点。为此，我们采用域适应学习作为一种启发函数初始化方法，可以帮助元学习模块 converge 到一个更好的优化点。理论和实验表明，我们提出的 DaMSTF 具有较高的效果。在跨域情感分类任务上，DaMSTF 可以提高 BERT 的性能，均提高约 4%。
</details></li>
</ul>
<hr>
<h2 id="NeRFs-The-Search-for-the-Best-3D-Representation"><a href="#NeRFs-The-Search-for-the-Best-3D-Representation" class="headerlink" title="NeRFs: The Search for the Best 3D Representation"></a>NeRFs: The Search for the Best 3D Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02751">http://arxiv.org/abs/2308.02751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravi Ramamoorthi</li>
<li>for: 这篇论文描述了NeRF表示法的应用和发展，以及三十年来寻找最佳视图合成和相关问题的3D表示方法的漫长历程。</li>
<li>methods: NeRF表示法使用神经网络来描述场景的连续体，包括视点依赖的光泽和体积密度。</li>
<li>results: NeRF表示法已成为计算机图形和视觉领域的标准表示方法，广泛应用于视图合成和图像基于渲染等问题，并且有 thousands of 篇论文进行扩展和建立。<details>
<summary>Abstract</summary>
Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe new developments in terms of NeRF representations and make some observations and insights regarding the future of 3D representations.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>神经辐射场或NeRFs已成为视觉合成或基于图像渲染等问题的表示方法选择，以及计算机视觉领域中许多其他应用程序的首选方法。它们的核心在于描述了一种新的3D场景或3D几何表示方法。而不是mesh、投影图、多平面图像或者VOXEL网格，NeRF代表场景为一个连续的Volume，通过问题 neural network 获得了视觉依赖的辐射光照和体积密度。NeRF表示法已经广泛应用，每年有 thousands of 篇论文扩展或基于它，多个作者和网站提供了概述和评论，以及许多工业应用和创业公司。在这篇文章中，我们 briefly 评论了NeRF表示法，并描述了三十年来为视觉合成和相关问题寻找最佳3D表示方法的历程， culminating 在NeRF论文中。然后，我们描述了新的NeRF表示法和一些关于未来3D表示方法的见解和发现。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-On-chip-Heterogeneity-of-Versal-Architecture-for-GNN-Inference-Acceleration"><a href="#Exploiting-On-chip-Heterogeneity-of-Versal-Architecture-for-GNN-Inference-Acceleration" class="headerlink" title="Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration"></a>Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02749">http://arxiv.org/abs/2308.02749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Chen, Pavan Manjunath, Sasindu Wijeratne, Bingyi Zhang, Viktor Prasanna</li>
<li>for: 该 paper 是为了提高 Graph Neural Network (GNN) 的推理速度而设计的。</li>
<li>methods: 该 paper 使用 AMD Versal ACAP 架构的特性进行 GNN 推理加速。具体来说，它使用 Programmable Logic (PL) 和 AI Engine (AIE) 两种不同的计算模块来实现 sparse 和 dense 操作的快速执行。</li>
<li>results: 该 paper 的实现在 VCK5000 ACAP 平台上表现出了较好的性能，与 CPU、GPU、ACAP 和其他自定义 GNN 加速器相比，实现了显著的均值运行时速度提升（162.42x、17.01x、9.90x 和 27.23x）。此外，对于 Graph Convolutional Network (GCN) 推理，该approach 在同一 ACAP 设备上实现了3.9-96.7x的速度提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have revolutionized many Machine Learning (ML) applications, such as social network analysis, bioinformatics, etc. GNN inference can be accelerated by exploiting data sparsity in the input graph, vertex features, and intermediate data in GNN computations. For dynamic sparsity exploitation, we leverage the heterogeneous computing capabilities of AMD Versal ACAP architecture to accelerate GNN inference. We develop a custom hardware module that executes the sparse primitives of the computation kernel on the Programmable Logic (PL) and efficiently computes the dense primitives using the AI Engine (AIE). To exploit data sparsity during inference, we devise a runtime kernel mapping strategy that dynamically assigns computation tasks to the PL and AIE based on data sparsity. Our implementation on the VCK5000 ACAP platform leads to superior performance compared with the state-of-the-art implementations on CPU, GPU, ACAP, and other custom GNN accelerators. Compared with these implementations, we achieve significant average runtime speedup across various models and datasets of 162.42x, 17.01x, 9.90x, and 27.23x, respectively. Furthermore, for Graph Convolutional Network (GCN) inference, our approach leads to a speedup of 3.9-96.7x compared to designs using PL only on the same ACAP device.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）已经革命化了许多机器学习（ML）应用，如社交网络分析和生物信息学等。GNN推理可以通过利用输入图数据稀疏性来加速。为了实现动态稀疏性利用，我们利用AMD Versal ACAP架构的多样化计算能力来加速GNN推理。我们开发了一个自定义硬件模块，该模块在Programmable Logic（PL）上执行稀疏计算 primitives，并使用AI Engine（AIE）来高效计算 dense primitives。为了在推理过程中利用数据稀疏性，我们提出了一种运行时kernel映射策略，该策略在PL和AIE之间动态分配计算任务基于数据稀疏性。我们在VCK5000 ACAP平台上实现了比之前的状态泰器实现在CPU、GPU、ACAP和其他自定义GNN加速器上的性能。相比这些实现，我们实现了平均运行时速度提升值为162.42倍、17.01倍、9.90倍和27.23倍，分别。此外，对于图卷积网络（GCN）推理，我们的方法实现了PL只的设计相比3.9-96.7倍的加速。
</details></li>
</ul>
<hr>
<h2 id="SABRE-Robust-Bayesian-Peer-to-Peer-Federated-Learning"><a href="#SABRE-Robust-Bayesian-Peer-to-Peer-Federated-Learning" class="headerlink" title="SABRE: Robust Bayesian Peer-to-Peer Federated Learning"></a>SABRE: Robust Bayesian Peer-to-Peer Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02747">http://arxiv.org/abs/2308.02747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nasimeh Heydaribeni, Ruisi Zhang, Tara Javidi, Cristina Nita-Rotaru, Farinaz Koushanfar</li>
<li>for: 提出了一种新的 Federated Learning 框架，即 SABRE，以提高 robustness。</li>
<li>methods: 使用了一种新的权重聚合方法，可以在非标准分布Setting下工作，并且不需要benign节点多于恶意节点。</li>
<li>results: 在一些 benchmark 数据上进行了证明和评估，并证明了 SABRE 在各种恶意攻击下的Robustness。<details>
<summary>Abstract</summary>
We introduce SABRE, a novel framework for robust variational Bayesian peer-to-peer federated learning. We analyze the robustness of the known variational Bayesian peer-to-peer federated learning framework (BayP2PFL) against poisoning attacks and subsequently show that BayP2PFL is not robust against those attacks. The new SABRE aggregation methodology is then devised to overcome the limitations of the existing frameworks. SABRE works well in non-IID settings, does not require the majority of the benign nodes over the compromised ones, and even outperforms the baseline algorithm in benign settings. We theoretically prove the robustness of our algorithm against data / model poisoning attacks in a decentralized linear regression setting. Proof-of-Concept evaluations on benchmark data from image classification demonstrate the superiority of SABRE over the existing frameworks under various poisoning attacks.
</details>
<details>
<summary>摘要</summary>
我们介绍SABRE，一个新的 Federated Learning框架，可以防护Against poisoning攻击。我们分析了已知的Variational Bayesian Peer-to-Peer Federated Learning框架（BayP2PFL）对于毒素攻击的不敏感性，并证明BayP2PFL不具有抗毒素能力。我们随后设计了一个新的SABRE聚合方法，以解决现有框架的局限性。SABRE在非ID Setting下表现良好，不需要大多数善意节点 greater than 损坏节点，甚至在正常设定下超越了基准算法。我们对Decentralized Linear Regression Setting中的数据/模型毒素攻击进行了理论上的证明，并在benchmark数据上进行了Proof-of-Concept评估，证明SABRE在不同的毒素攻击下的superiority。
</details></li>
</ul>
<hr>
<h2 id="Meta-Tsallis-Entropy-Minimization-A-New-Self-Training-Approach-for-Domain-Adaptation-on-Text-Classification"><a href="#Meta-Tsallis-Entropy-Minimization-A-New-Self-Training-Approach-for-Domain-Adaptation-on-Text-Classification" class="headerlink" title="Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification"></a>Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02746">http://arxiv.org/abs/2308.02746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglong Lu, Zhen Huang, Zhiliang Tian, Yunxiang Zhao, Xuanyu Fei, Dongsheng Li</li>
<li>for: 本文旨在提高文本分类模型的适应性 across 多个频道，有广泛的应用。</li>
<li>methods: 本文提出了一种基于自适应学习的方法，即使用自适应学习生成pseudo例，并在源频道和目标频道中进行分类。</li>
<li>results: 实验表明， compared to traditional self-training methods, MTEM可以提高BERT模型的适应性，平均提高4%的精度。<details>
<summary>Abstract</summary>
Text classification is a fundamental task for natural language processing, and adapting text classification models across domains has broad applications. Self-training generates pseudo-examples from the model's predictions and iteratively trains on the pseudo-examples, i.e., minimizes the loss on the source domain and the Gibbs entropy on the target domain. However, Gibbs entropy is sensitive to prediction errors, and thus, self-training tends to fail when the domain shift is large. In this paper, we propose Meta-Tsallis Entropy minimization (MTEM), which applies a meta-learning algorithm to optimize the instance adaptive Tsallis entropy on the target domain. To reduce the computation cost of MTEM, we propose an approximation technique to approximate the Second-order derivation involved in the meta-learning. To efficiently generate pseudo labels, we propose an annealing sampling mechanism for exploring the model's prediction probability. Theoretically, we prove the convergence of the meta-learning algorithm in MTEM and analyze the effectiveness of MTEM in achieving domain adaptation. Experimentally, MTEM improves the adaptation performance of BERT with an average of 4 percent on the benchmark dataset.
</details>
<details>
<summary>摘要</summary>
文本分类是自然语言处理中的基本任务，并且在不同领域中适应文本分类模型有广泛的应用。自我训练会生成 pseudo-example 从模型预测中，并在这些 pseudo-example 上进行逐步训练，即在源领域中减少损失，并在目标领域中减少 Gibbs  entropy。然而，Gibbs entropy 受到预测错误的影响，因此自我训练在大量领域变换时常常失败。在这篇论文中，我们提出了 Meta-Tsallis Entropy 优化（MTEM），它使用元学习算法来优化目标领域中的实例适应 Tsallis  entropy。为了减少 MTEM 的计算成本，我们提出了一种近似技术来近似元学习中的第二阶导数。同时，我们还提出了一种缓和抽象采样机制，以便快速生成 pseudo 标签。理论上，我们证明了 MTEM 中元学习算法的收敛性，并分析了 MTEM 在适应性方面的效果。实验表明，MTEM 可以在标准数据集上提高 BERT 的适应性表现，平均提高了4%。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Schedule-in-Non-Stationary-Wireless-Networks-With-Unknown-Statistics"><a href="#Learning-to-Schedule-in-Non-Stationary-Wireless-Networks-With-Unknown-Statistics" class="headerlink" title="Learning to Schedule in Non-Stationary Wireless Networks With Unknown Statistics"></a>Learning to Schedule in Non-Stationary Wireless Networks With Unknown Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02734">http://arxiv.org/abs/2308.02734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang Minh Nguyen, Eytan Modiano</li>
<li>for: 研究 wireless network 中的吞吐率最优化策略，满足通信系统中部分可见性和时间变化的假设。</li>
<li>methods: 提出了一种基于 Max-Weight 策略的 MW-UCB 算法，利用 Sliding-Window Upper-Confidence Bound 学习无线通信中的通道统计数据，并且可以在非站立性下实现throughput最优。</li>
<li>results: 经过 simulations 验证，MW-UCB 算法可以在具有部分可见性和时间变化的无线网络中实现高效的吞吐率最优化。<details>
<summary>Abstract</summary>
The emergence of large-scale wireless networks with partially-observable and time-varying dynamics has imposed new challenges on the design of optimal control policies. This paper studies efficient scheduling algorithms for wireless networks subject to generalized interference constraint, where mean arrival and mean service rates are unknown and non-stationary. This model exemplifies realistic edge devices' characteristics of wireless communication in modern networks. We propose a novel algorithm termed MW-UCB for generalized wireless network scheduling, which is based on the Max-Weight policy and leverages the Sliding-Window Upper-Confidence Bound to learn the channels' statistics under non-stationarity. MW-UCB is provably throughput-optimal under mild assumptions on the variability of mean service rates. Specifically, as long as the total variation in mean service rates over any time period grows sub-linearly in time, we show that MW-UCB can achieve the stability region arbitrarily close to the stability region of the class of policies with full knowledge of the channel statistics. Extensive simulations validate our theoretical results and demonstrate the favorable performance of MW-UCB.
</details>
<details>
<summary>摘要</summary>
现代无线网络中的大规模无线网络具有部分可见和时间变化的动态特性，对优化控制策略的设计带来了新的挑战。本文研究了基于通用干扰约束的无线网络占用策略的有效计划算法。这种模型体现了现代无线通信网络中Edge设备的准确特性。我们提出了一种名为MW-UCB的新算法，它基于Max-Weight策略，并利用Sliding-Window Upper-Confidence Bound来学习不确定的通道统计。MW-UCB可以在不确定的服务率的情况下保证通信吞吐量的最大化，并且可以在服务率的变化范围内保持稳定。我们证明MW-UCB在一定程度上是可以达到稳定区的最优策略，并且可以在服务率的变化范围内保持稳定。我们的实验结果 validate我们的理论结论，并证明MW-UCB在实际应用中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Personalization-of-Stress-Mobile-Sensing-using-Self-Supervised-Learning"><a href="#Personalization-of-Stress-Mobile-Sensing-using-Self-Supervised-Learning" class="headerlink" title="Personalization of Stress Mobile Sensing using Self-Supervised Learning"></a>Personalization of Stress Mobile Sensing using Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02731">http://arxiv.org/abs/2308.02731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Islam, Peter Washington</li>
<li>for: 这研究旨在开发一种个性化的压力预测方法，可以使用小数据量的标签来个性化学习。</li>
<li>methods: 研究人员使用了一种自动学习技术，即一维度的卷积神经网络（CNN），通过自动学习来学习每个用户的基线生物信号模式，从而实现个性化学习。</li>
<li>results: 研究人员发现，使用自动学习前training方法可以学习出高性能的压力预测模型，仅需使用少量的标签数据。这种个性化学习方法可以帮助实现精准健康系统，这些系统可以根据每个用户的特点进行个性化预测和提供推荐。<details>
<summary>Abstract</summary>
Stress is widely recognized as a major contributor to a variety of health issues. Stress prediction using biosignal data recorded by wearables is a key area of study in mobile sensing research because real-time stress prediction can enable digital interventions to immediately react at the onset of stress, helping to avoid many psychological and physiological symptoms such as heart rhythm irregularities. Electrodermal activity (EDA) is often used to measure stress. However, major challenges with the prediction of stress using machine learning include the subjectivity and sparseness of the labels, a large feature space, relatively few labels, and a complex nonlinear and subjective relationship between the features and outcomes. To tackle these issues, we examine the use of model personalization: training a separate stress prediction model for each user. To allow the neural network to learn the temporal dynamics of each individual's baseline biosignal patterns, thus enabling personalization with very few labels, we pre-train a 1-dimensional convolutional neural network (CNN) using self-supervised learning (SSL). We evaluate our method using the Wearable Stress and Affect prediction (WESAD) dataset. We fine-tune the pre-trained networks to the stress prediction task and compare against equivalent models without any self-supervised pre-training. We discover that embeddings learned using our pre-training method outperform supervised baselines with significantly fewer labeled data points: the models trained with SSL require less than 30% of the labels to reach equivalent performance without personalized SSL. This personalized learning method can enable precision health systems which are tailored to each subject and require few annotations by the end user, thus allowing for the mobile sensing of increasingly complex, heterogeneous, and subjective outcomes such as stress.
</details>
<details>
<summary>摘要</summary>
stress是广泛认可的健康问题的重要 contribuens。预测stress使用记录在佩戴式设备中的生物信号数据是移动感知研究中关键的预测领域，因为实时预测stress可以让数字干预出现在压力开始时，以避免心跳rhythm irregularities和许多心理和生理学症状。电导活动（EDA）经常用来测量压力。然而，机器学习预测压力的主要挑战包括标签的主观性和稀缺性，大的特征空间，相对少的标签，以及复杂的非线性和主观关系 между特征和结果。为解决这些问题，我们研究了个性化预测：为每个用户训练一个压力预测模型。为让神经网络学习每个人的基线生物信号模式的时间 dynamics，因此启用个性化预测，我们使用自动学习（SSL）预训练一个1维度卷积神经网络（CNN）。我们使用WESAD数据集进行评估我们的方法。我们精细调整预训练后的网络来完成压力预测任务，并与没有自动学习预训练的相同模型进行比较。我们发现，使用我们的预训练方法学习的嵌入超越了无预训练基线的表现，只需要少于30%的标签数据点来达到相同的性能。这种个性化学习方法可以启用手持式健康系统，这些系统可以根据每个用户而定制，并且只需要少量标签由用户提供，以便在移动感知中评估越来越复杂、多元和主观的结果。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Programmatic-Policies-with-Actor-Critic-Algorithms-and-ReLU-Networks"><a href="#Synthesizing-Programmatic-Policies-with-Actor-Critic-Algorithms-and-ReLU-Networks" class="headerlink" title="Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks"></a>Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02729">http://arxiv.org/abs/2308.02729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spyros Orfanos, Levi H. S. Lelis</li>
<li>for: 本研究旨在探讨Programmatically Interpretable Reinforcement Learning（PIRL）算法是否需要专门的PIRL算法，以及如何使用actor-critic算法直接从 neural network 中提取政策。</li>
<li>methods: 本研究使用 actor-critic 算法，以及一种将ReLU神经网络翻译成 if-then-else 结构、线性变换和PID操作的方法，来将政策编码在程序中。</li>
<li>results: 实验结果表明，使用此翻译方法可以学习短而有效的政策，并且与PIRL算法 synthesize 的政策相比，译制后的政策在许多控制问题上表现至少兼容，有时甚至超越。<details>
<summary>Abstract</summary>
Programmatically Interpretable Reinforcement Learning (PIRL) encodes policies in human-readable computer programs. Novel algorithms were recently introduced with the goal of handling the lack of gradient signal to guide the search in the space of programmatic policies. Most of such PIRL algorithms first train a neural policy that is used as an oracle to guide the search in the programmatic space. In this paper, we show that such PIRL-specific algorithms are not needed, depending on the language used to encode the programmatic policies. This is because one can use actor-critic algorithms to directly obtain a programmatic policy. We use a connection between ReLU neural networks and oblique decision trees to translate the policy learned with actor-critic algorithms into programmatic policies. This translation from ReLU networks allows us to synthesize policies encoded in programs with if-then-else structures, linear transformations of the input values, and PID operations. Empirical results on several control problems show that this translation approach is capable of learning short and effective policies. Moreover, the translated policies are at least competitive and often far superior to the policies PIRL algorithms synthesize.
</details>
<details>
<summary>摘要</summary>
Programmatically Interpretable Reinforcement Learning (PIRL) 编码策略为人类可读性计算机程序。最近，为了解决策略搜索空间中缺失梯度信号的问题，新的算法得到了开发。大多数PIRL算法首先使用神经网络作为尝试导航搜索空间的oracle。在这篇论文中，我们表明PIRL特有的算法不是必要的，具体来说，取决于编码策略的语言。因为可以直接使用actor-critic算法获得程序编码策略。我们使用ReLU神经网络和倾斜决策树的连接将actor-critic算法学习的策略翻译成程序编码策略。这种翻译方法可以synthesize编码策略中的if-then-else结构、输入值线性变换和PID操作。我们在几个控制问题上进行了实验，结果表明这种翻译方法可以学习短而有效的策略。此外，翻译出来的策略与PIRL算法生成的策略相比，通常更加竞争力强。
</details></li>
</ul>
<hr>
<h2 id="Towards-Improving-Harmonic-Sensitivity-and-Prediction-Stability-for-Singing-Melody-Extraction"><a href="#Towards-Improving-Harmonic-Sensitivity-and-Prediction-Stability-for-Singing-Melody-Extraction" class="headerlink" title="Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction"></a>Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02723">http://arxiv.org/abs/2308.02723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smoothken/kknet">https://github.com/smoothken/kknet</a></li>
<li>paper_authors: Keren Shao, Ke Chen, Taylor Berg-Kirkpatrick, Shlomo Dubnov</li>
<li>for: 这 paper 是为了提高 singing melody extraction 的性能而写的。</li>
<li>methods: 这 paper 使用了 input feature modification 和 training objective modification 两种方法来提高模型的性能。</li>
<li>results: 实验结果表明，提posed modifications 对 singing melody extraction 是有 empirical effectiveness 的。Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 这 paper 是为了提高 singing melody extraction 的性能而写的。</li>
<li>methods: 这 paper 使用了 input feature modification 和 training objective modification 两种方法来提高模型的性能。</li>
<li>results: 实验结果表明，提posed modifications 对 singing melody extraction 是有 empirical effectiveness 的。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.
</details>
<details>
<summary>摘要</summary>
在深度学习研究中，许多旋律提取模型通过重新设计神经网络架构来提高性能。在这篇论文中，我们提出了输入特征修改和训练目标修改，基于两个假设。首先，音频数据中的干扰在频谱图中快速衰减。为了增强模型对尾部干扰的敏感性，我们使用离散ζ变换修改合并频率和周期性（CFP）表示。其次，歌唱和非歌唱段的时间非常短暂是非常罕见的。为保证更稳定的旋律轮廓，我们设计了可导的损失函数，避免模型预测这些段落。我们应用这些修改于多个模型，包括MSNet、FTANet以及我们新引入的PianoNet，这是基于钢琴谱写网络的修改。我们的实验结果表明，我们的修改是实际有效的对歌唱旋律提取。
</details></li>
</ul>
<hr>
<h2 id="Fluid-Property-Prediction-Leveraging-AI-and-Robotics"><a href="#Fluid-Property-Prediction-Leveraging-AI-and-Robotics" class="headerlink" title="Fluid Property Prediction Leveraging AI and Robotics"></a>Fluid Property Prediction Leveraging AI and Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02715">http://arxiv.org/abs/2308.02715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baratilab/vid2visc">https://github.com/baratilab/vid2visc</a></li>
<li>paper_authors: Jong Hoon Park, Gauri Pramod Dalwankar, Alison Bartsch, Abraham George, Amir Barati Farimani</li>
<li>for: 这篇论文是为了探讨如何使用视觉信息来推断流体的性质，以便在自动化流体处理系统中更好地控制流体的行为。</li>
<li>methods: 这篇论文使用了3D卷积自适应神经网络来学习不同的流体振荡模式在视频中的表征。这些表征然后被用来从视频中视觉地推断流体的动态粘度。</li>
<li>results: 研究发现，使用这种视觉方法可以准确地推断流体的动态粘度，并且比传统方法更快速和更加精准。<details>
<summary>Abstract</summary>
Inferring liquid properties from vision is a challenging task due to the complex nature of fluids, both in behavior and detection. Nevertheless, the ability to infer their properties directly from visual information is highly valuable for autonomous fluid handling systems, as cameras are readily available. Moreover, predicting fluid properties purely from vision can accelerate the process of fluid characterization saving considerable time and effort in various experimental environments. In this work, we present a purely vision-based approach to estimate viscosity, leveraging the fact that the behavior of the fluid oscillations is directly related to the viscosity. Specifically, we utilize a 3D convolutional autoencoder to learn latent representations of different fluid-oscillating patterns present in videos. We leverage this latent representation to visually infer the category of fluid or the dynamics viscosity of fluid from video.
</details>
<details>
<summary>摘要</summary>
<<SYS Translate="yes">推断流体属性从视觉是一项复杂的任务，主要因为流体的行为和检测都具有复杂的特性。然而，从视觉信息直接推断流体属性的能力具有高度的价值，因为摄像头是 readily available。此外，仅基于视觉预测流体属性可以大幅缩短各种实验室环境中的测试时间和努力。在这项工作中，我们提出了一种完全基于视觉的方法，使用3D卷积自适应神经网络来学习不同的流体振荡模式在视频中的含义。我们利用这种含义来从视频中可视化地推断流体的类别或流体的动态粘性。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Effect-of-Sparse-Recovery-on-the-Quality-of-Image-Superresolution"><a href="#Exploring-the-Effect-of-Sparse-Recovery-on-the-Quality-of-Image-Superresolution" class="headerlink" title="Exploring the Effect of Sparse Recovery on the Quality of Image Superresolution"></a>Exploring the Effect of Sparse Recovery on the Quality of Image Superresolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02714">http://arxiv.org/abs/2308.02714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Castro</li>
<li>for: 该论文旨在研究用字典学习进行图像超解像，通过学习高分辨率和低分辨率图像对应的对应的patch对来学习一对联动的字典，以便使用这些字典来从低分辨率输入图像中恢复高分辨率图像patch。</li>
<li>methods: 该论文使用了字典学习技术，通过学习高分辨率和低分辨率图像对应的对应的patch对来学习一对联动的字典，并使用这些字典来从低分辨率输入图像中恢复高分辨率图像patch。</li>
<li>results: 该论文通过实验研究了不同的简单恢复算法对图像超解像质量的影响，并提出了最佳的简单恢复算法选择方法。<details>
<summary>Abstract</summary>
Dictionary learning can be used for image superresolution by learning a pair of coupled dictionaries of image patches from high-resolution and low-resolution image pairs such that the corresponding pairs share the same sparse vector when represented by the coupled dictionaries. These dictionaries then can be used to to reconstruct the corresponding high-resolution patches from low-resolution input images based on sparse recovery. The idea is to recover the shared sparse vector using the low-resolution dictionary and then multiply it by the high-resolution dictionary to recover the corresponding high-resolution image patch. In this work, we study the effect of the sparse recovery algorithm that we use on the quality of the reconstructed images. We offer empirical experiments to search for the best sparse recovery algorithm that can be used for this purpose.
</details>
<details>
<summary>摘要</summary>
《字典学习可以用于图像超分辨by学习一对相互关联的字典，这些字典分别表示高分辨率和低分辨率图像对应的图像 patches，以便在这些字典中共享相同的稀疏 вектор。这些字典可以用来从低分辨率输入图像中重建对应的高分辨率图像 patch，基于稀疏恢复。我们的研究将关注使用的稀疏恢复算法对重建图像质量的影响。我们将进行实验寻找最佳的稀疏恢复算法。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Scalable-Computation-of-Causal-Bounds"><a href="#Scalable-Computation-of-Causal-Bounds" class="headerlink" title="Scalable Computation of Causal Bounds"></a>Scalable Computation of Causal Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02709">http://arxiv.org/abs/2308.02709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Madhumitha Shridharan, Garud Iyengar</li>
<li>for:  Computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold.</li>
<li>methods:  Significantly prune a linear programming (LP) formulation to compute bounds, allowing for larger causal inference problems compared to existing techniques. Extend the pruning methodology to fractional LPs for additional observations.</li>
<li>results:  Significant runtime improvement compared to benchmarks in experiments, and high-quality bounds produced by an efficient greedy heuristic that scales to larger problems.<details>
<summary>Abstract</summary>
We consider the problem of computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold. Existing non-parametric approaches for computing such bounds use linear programming (LP) formulations that quickly become intractable for existing solvers because the size of the LP grows exponentially in the number of edges in the causal graph. We show that this LP can be significantly pruned, allowing us to compute bounds for significantly larger causal inference problems compared to existing techniques. This pruning procedure allows us to compute bounds in closed form for a special class of problems, including a well-studied family of problems where multiple confounded treatments influence an outcome. We extend our pruning methodology to fractional LPs which compute bounds for causal queries which incorporate additional observations about the unit. We show that our methods provide significant runtime improvement compared to benchmarks in experiments and extend our results to the finite data setting. For causal inference without additional observations, we propose an efficient greedy heuristic that produces high quality bounds, and scales to problems that are several orders of magnitude larger than those for which the pruned LP can be solved.
</details>
<details>
<summary>摘要</summary>
我团队考虑了计算 causal 查询 bounds 的问题，这里有不观察到的假设变量和离散型观察变量。我们显示出现非参数方法的 LP 表示可以快速减少，使得我们可以计算 bounds  для更大的 causal inference 问题。我们扩展了我们的减少方法来计算 fractional LPs，这些 LPs 计算 bounds  для包含额外观察的 causal queries。我们在实验中显示了我们的方法可以提供重要的时间改进 compared to 参考值。此外，我们还提出了一种高质量 bounds 的生成策略，该策略可以扩展到许多个数量级更大的问题。Here's the translation of the text into Traditional Chinese:我团队考虑过 Computing causal queries bounds 的问题，这里有不观察到的假设变量和离散型观察变量。我们显示出现非参数方法的 LP 表示可以快速减少，使得我们可以计算 bounds  для更大的 causal inference 问题。我们扩展了我们的减少方法来计算 fractional LPs，这些 LPs 计算 bounds  для包含额外观察的 causal queries。我们在实验中显示了我们的方法可以提供重要的时间改进 compared to 参考值。此外，我们还提出了一种高质量 bounds 的生成策略，这策略可以扩展到许多个数量级更大的问题。
</details></li>
</ul>
<hr>
<h2 id="FPR-Estimation-for-Fraud-Detection-in-the-Presence-of-Class-Conditional-Label-Noise"><a href="#FPR-Estimation-for-Fraud-Detection-in-the-Presence-of-Class-Conditional-Label-Noise" class="headerlink" title="FPR Estimation for Fraud Detection in the Presence of Class-Conditional Label Noise"></a>FPR Estimation for Fraud Detection in the Presence of Class-Conditional Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02695">http://arxiv.org/abs/2308.02695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jtittelfitz/fpr-estimation">https://github.com/jtittelfitz/fpr-estimation</a></li>
<li>paper_authors: Justin Tittelfitz</li>
<li>for: 本研究旨在估计二分类模型中存在标签噪声（label noise）时的假阳性率（FPR）和正确率（TPR）。</li>
<li>methods: 本研究使用了一种新的方法，即使用模型直接清洁验证数据，以避免因验证数据中的噪声而导致的假阳性率和正确率的估计偏差。</li>
<li>results: 研究发现，使用现有的方法可能会导致假阳性率和正确率的估计偏差，而使用新的方法可以更好地估计这两个指标。<details>
<summary>Abstract</summary>
We consider the problem of estimating the false-/ true-positive-rate (FPR/TPR) for a binary classification model when there are incorrect labels (label noise) in the validation set. Our motivating application is fraud prevention where accurate estimates of FPR are critical to preserving the experience for good customers, and where label noise is highly asymmetric. Existing methods seek to minimize the total error in the cleaning process - to avoid cleaning examples that are not noise, and to ensure cleaning of examples that are. This is an important measure of accuracy but insufficient to guarantee good estimates of the true FPR or TPR for a model, and we show that using the model to directly clean its own validation data leads to underestimates even if total error is low. This indicates a need for researchers to pursue methods that not only reduce total error but also seek to de-correlate cleaning error with model scores.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Explainable-Deep-Learning-based-Solar-Flare-Prediction-with-post-hoc-Attention-for-Operational-Forecasting"><a href="#Explainable-Deep-Learning-based-Solar-Flare-Prediction-with-post-hoc-Attention-for-Operational-Forecasting" class="headerlink" title="Explainable Deep Learning-based Solar Flare Prediction with post hoc Attention for Operational Forecasting"></a>Explainable Deep Learning-based Solar Flare Prediction with post hoc Attention for Operational Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02682">http://arxiv.org/abs/2308.02682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/gsudmlab/explainingfulldisk">https://bitbucket.org/gsudmlab/explainingfulldisk</a></li>
<li>paper_authors: Chetraj Pandey, Rafal A. Angryk, Manolis K. Georgoulis, Berkay Aydin</li>
<li>for: 预测日冕大地震（solar flare）的深度学习模型，以提高现有的预测方法的准确性和可靠性。</li>
<li>methods: 使用每小时的全盘线对视图磁图图像，采用二分类预测模式预测日冕大地震发生的可能性。采用自定义数据增强和样本权重来解决类偏置问题。使用真正的技能统计量和赫迪克技能分数作为评估指标。</li>
<li>results: 研究发现，全盘预测日冕大地震能够准确地 lokate和预测近日冕的大地震，这是操作预测中的关键特征。模型达到了平均的TSS&#x3D;0.51$\pm$0.05和HSS&#x3D;0.38$\pm$0.08水平，并且发现这些模型可以从全盘磁图中学习出明显的活跃区域特征。<details>
<summary>Abstract</summary>
This paper presents a post hoc analysis of a deep learning-based full-disk solar flare prediction model. We used hourly full-disk line-of-sight magnetogram images and selected binary prediction mode to predict the occurrence of $\geq$M1.0-class flares within 24 hours. We leveraged custom data augmentation and sample weighting to counter the inherent class-imbalance problem and used true skill statistic and Heidke skill score as evaluation metrics. Recent advancements in gradient-based attention methods allow us to interpret models by sending gradient signals to assign the burden of the decision on the input features. We interpret our model using three post hoc attention methods: (i) Guided Gradient-weighted Class Activation Mapping, (ii) Deep Shapley Additive Explanations, and (iii) Integrated Gradients. Our analysis shows that full-disk predictions of solar flares align with characteristics related to the active regions. The key findings of this study are: (1) We demonstrate that our full disk model can tangibly locate and predict near-limb solar flares, which is a critical feature for operational flare forecasting, (2) Our candidate model achieves an average TSS=0.51$\pm$0.05 and HSS=0.38$\pm$0.08, and (3) Our evaluation suggests that these models can learn conspicuous features corresponding to active regions from full-disk magnetograms.
</details>
<details>
<summary>摘要</summary>
Recent advancements in gradient-based attention methods allow us to interpret models by sending gradient signals to assign the burden of the decision on the input features. We interpret our model using three post hoc attention methods:1. Guided Gradient-weighted Class Activation Mapping: 使用梯度信号将决策担当分配到输入特征上。2. Deep Shapley Additive Explanations: 使用深度谱分解方法计算出每个特征对预测结果的贡献。3. Integrated Gradients: 使用梯度 интеграル方法计算出输入特征对预测结果的贡献。Our analysis shows that full-disk predictions of solar flares align with characteristics related to the active regions. The key findings of this study are:1. We demonstrate that our full disk model can tangibly locate and predict near-limb solar flares, which is a critical feature for operational flare forecasting.2. Our candidate model achieves an average TSS=0.51$\pm$0.05 and HSS=0.38$\pm$0.08.3. Our evaluation suggests that these models can learn conspicuous features corresponding to active regions from full-disk magnetograms.
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Change-of-Variable-Formulas-for-Generative-Modeling"><a href="#A-Review-of-Change-of-Variable-Formulas-for-Generative-Modeling" class="headerlink" title="A Review of Change of Variable Formulas for Generative Modeling"></a>A Review of Change of Variable Formulas for Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02652">http://arxiv.org/abs/2308.02652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ullrich Köthe</li>
<li>for: 本研究旨在总结Change-of-variables（CoV）方程的各种应用和 derivation，并从编码器&#x2F;解码器架构的视角出发，收集28种CoV方程，探讨各种方法之间的关系，强调文献中不一定够清楚地表达的重要区别，并发现未来研究中的潜在漏洞。</li>
<li>methods: 本研究使用了变量替换方程来简化复杂的概率分布，并通过学习的变换来实现 tractable Jacobian determinant。</li>
<li>results: 本研究收集了28种Change-of-variables方程，并通过对这些方程的系统性分析，探讨各种方法之间的关系，强调文献中不一定够清楚地表达的重要区别，并发现未来研究中的潜在漏洞。<details>
<summary>Abstract</summary>
Change-of-variables (CoV) formulas allow to reduce complicated probability densities to simpler ones by a learned transformation with tractable Jacobian determinant. They are thus powerful tools for maximum-likelihood learning, Bayesian inference, outlier detection, model selection, etc. CoV formulas have been derived for a large variety of model types, but this information is scattered over many separate works. We present a systematic treatment from the unifying perspective of encoder/decoder architectures, which collects 28 CoV formulas in a single place, reveals interesting relationships between seemingly diverse methods, emphasizes important distinctions that are not always clear in the literature, and identifies surprising gaps for future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>变量更改（CoV）公式可以将复杂的概率密度降低到更简单的密度中，通过学习的变换，其Jacobian determinant是可追踪的。因此，它们是最大 likelihood 学习、 bayesian 推理、异常检测、模型选择等方面的强大工具。CoV 公式已经 derivated  для许多模型类型，但这些信息分散在多个不同的论文中。我们在encoder/decoder 架构的统一视角下提供了一个系统性的处理方法，收集了28种CoV 公式，在一个地方汇总了这些信息，揭示了文献中的感兴趣关系，强调了文献中不一定明确的重要区别，并发现了未来研究中的潜在空白。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="ReCLIP-Refine-Contrastive-Language-Image-Pre-Training-with-Source-Free-Domain-Adaptation"><a href="#ReCLIP-Refine-Contrastive-Language-Image-Pre-Training-with-Source-Free-Domain-Adaptation" class="headerlink" title="ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation"></a>ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03793">http://arxiv.org/abs/2308.03793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Hu, Ke Zhang, Lu Xia, Albert Chen, Jiajia Luo, Yuyin Sun, Ken Wang, Nan Qiao, Xiao Zeng, Min Sun, Cheng-Hao Kuo, Ram Nevatia</li>
<li>for: 提高CLIP模型在下游目标领域的性能，即使没有源数据或目标标注数据。</li>
<li>methods: 提出了一种源自由领域适应方法，通过减轻视文对象 embedding的偏差和使用杂模相关自动标注来实现。</li>
<li>results: 经过广泛的实验，ReCLIP方法可以将CLIP模型的平均错误率从30.17%降低至25.06%在22个图像分类 benchmark上。<details>
<summary>Abstract</summary>
Large-scale Pre-Training Vision-Language Model such as CLIP has demonstrated outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1 accuracy on ImageNet without seeing any example, which leads to potential benefits to many tasks that have no labeled data. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, the first source-free domain adaptation method for vision-language models, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels, and then deploys cross-modality self-training with the pseudo labels, to update visual and text encoders, refine labels and reduce domain gaps and misalignments iteratively. With extensive experiments, we demonstrate ReCLIP reduces the average error rate of CLIP from 30.17% to 25.06% on 22 image classification benchmarks.
</details>
<details>
<summary>摘要</summary>
大规模预训练视语模型，如CLIP，在零shot分类任务上表现出色，例如在ImageNet上取得76.3%的顶峰准确率无需见过任何示例，这可能导致多种任务 ohne 标注数据的潜在优势。然而，在应用CLIP到下游目标领域时，视图和文本频谱差和跨模态不一致可能会严重影响模型性能。为了解决这些挑战，我们提议ReCLIP，首先学习抑制视图和文本嵌入的投影空间，然后通过跨模态自适应学习，使用假标签，更新视图和文本编码器，细化标签，并逐步减少频谱差和不一致。经过广泛实验，我们表明ReCLIP可以将CLIP的平均错误率从30.17%降低至25.06%在22个图像分类任务上。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Topology-Cosmological-Parameter-Estimation-from-the-Large-scale-Structure"><a href="#Learning-from-Topology-Cosmological-Parameter-Estimation-from-the-Large-scale-Structure" class="headerlink" title="Learning from Topology: Cosmological Parameter Estimation from the Large-scale Structure"></a>Learning from Topology: Cosmological Parameter Estimation from the Large-scale Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02636">http://arxiv.org/abs/2308.02636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacky H. T. Yip, Adam Rouhiainen, Gary Shiu</li>
<li>for: 研究宇宙大规模结构的 cosmological 参数</li>
<li>methods: 使用神经网络模型从 persistency 图像中提取 cosmological 参数</li>
<li>results: 模型可以准确地估算 cosmological 参数，质量比传统 Bayesian 推理方法更高<details>
<summary>Abstract</summary>
The topology of the large-scale structure of the universe contains valuable information on the underlying cosmological parameters. While persistent homology can extract this topological information, the optimal method for parameter estimation from the tool remains an open question. To address this, we propose a neural network model to map persistence images to cosmological parameters. Through a parameter recovery test, we demonstrate that our model makes accurate and precise estimates, considerably outperforming conventional Bayesian inference approaches.
</details>
<details>
<summary>摘要</summary>
宇宙大规模结构的Topology含有价值的 cosmological参数信息。 persistent homology 可以提取这些 topological 信息，但是最佳的方法 для参数估计仍然是一个开放的问题。为解决这个问题，我们提议一种神经网络模型，将 persistency 图像映射到 cosmological 参数。通过参数恢复测试，我们证明我们的模型可以做出准确和精确的估计，明显超过了传统的 Bayesian 推理方法。Note: "persistent homology" is translated as " persistency" in Simplified Chinese, which is a common abbreviation used in the field of topological data analysis.
</details></li>
</ul>
<hr>
<h2 id="MM-Vet-Evaluating-Large-Multimodal-Models-for-Integrated-Capabilities"><a href="#MM-Vet-Evaluating-Large-Multimodal-Models-for-Integrated-Capabilities" class="headerlink" title="MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"></a>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02490">http://arxiv.org/abs/2308.02490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweihao/mm-vet">https://github.com/yuweihao/mm-vet</a></li>
<li>paper_authors: Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang</li>
<li>For: The paper proposes an evaluation benchmark for large multimodal models (LMMs) to evaluate their ability to integrate different core vision-language (VL) capabilities and solve complicated multimodal tasks.* Methods: The paper presents MM-Vet, a benchmark that defines 6 core VL capabilities and examines 16 integrations of interest derived from capability combination. The paper also proposes an LLM-based evaluator for open-ended outputs, which enables the evaluation across different question types and answer styles.* Results: The paper evaluates representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models. The results show that the proposed evaluator can provide a unified scoring metric for open-ended outputs, and the MM-Vet benchmark can help identify the strengths and weaknesses of different LMM systems.<details>
<summary>Abstract</summary>
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models. Code and data are available at https://github.com/yuweihao/MM-Vet.
</details>
<details>
<summary>摘要</summary>
我们提出了 MM-Vet，一个评估板准 benchmark，用于测试大型多Modal模型（LMM）在复杂多Modal任务上的能力。现代LMM在不同的任务上表现出了各种感人的能力，如解释黑板上的数学问题，理解新闻图片中的事件和名人，以及解释视觉笑话。由于模型的快速发展，评估板准的开发受到了挑战。问题包括：(1)如何系统地结构化和评估复杂多Modal任务；(2)如何设计评估指标，可以在问题和答案类型之间具有一致性；以及(3)如何为模型提供更多的材料，而不仅仅是一个简单的性能排名。为此，我们提出了 MM-Vet，基于了核心视语言（VL）能力的总结。MM-Vet定义了6个核心VL能力，并评估了这些能力之间的16种 интеграción。为评估指标，我们提议了基于LLM的评估器，可以对开放式输出进行评估，并且可以在不同的问题类型和答案风格之间具有一致性。我们对代表性的LMM进行了 MM-Vet的评估，提供了不同模型系统和模型的能力的见解。代码和数据可以在 https://github.com/yuweihao/MM-Vet 上获取。
</details></li>
</ul>
<hr>
<h2 id="Generation-of-Realistic-Synthetic-Raw-Radar-Data-for-Automated-Driving-Applications-using-Generative-Adversarial-Networks"><a href="#Generation-of-Realistic-Synthetic-Raw-Radar-Data-for-Automated-Driving-Applications-using-Generative-Adversarial-Networks" class="headerlink" title="Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks"></a>Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02632">http://arxiv.org/abs/2308.02632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo C. Fidelis, Fabio Reway, Herick Y. S. Ribeiro, Pietro L. Campos, Werner Huber, Christian Icking, Lester A. Faria, Torsten Schön</li>
<li>for: 这个论文是为了模拟雷达数据的研究而写的。</li>
<li>methods: 这篇论文使用了生成对抗网络（GAN）来生成雷达数据。</li>
<li>results: 这个方法可以生成16个同时发射的雷达射频信号，并且可以用于进一步开发雷达数据处理算法（例如滤波和封装）。这可以增加数据的扩展和增强，例如生成不可能或安全关键的场景的数据，以便进行数据 augmentation。<details>
<summary>Abstract</summary>
The main approaches for simulating FMCW radar are based on ray tracing, which is usually computationally intensive and do not account for background noise. This work proposes a faster method for FMCW radar simulation capable of generating synthetic raw radar data using generative adversarial networks (GAN). The code and pre-trained weights are open-source and available on GitHub. This method generates 16 simultaneous chirps, which allows the generated data to be used for the further development of algorithms for processing radar data (filtering and clustering). This can increase the potential for data augmentation, e.g., by generating data in non-existent or safety-critical scenarios that are not reproducible in real life. In this work, the GAN was trained with radar measurements of a motorcycle and used to generate synthetic raw radar data of a motorcycle traveling in a straight line. For generating this data, the distance of the motorcycle and Gaussian noise are used as input to the neural network. The synthetic generated radar chirps were evaluated using the Frechet Inception Distance (FID). Then, the Range-Azimuth (RA) map is calculated twice: first, based on synthetic data using this GAN and, second, based on real data. Based on these RA maps, an algorithm with adaptive threshold and edge detection is used for object detection. The results have shown that the data is realistic in terms of coherent radar reflections of the motorcycle and background noise based on the comparison of chirps, the RA maps and the object detection results. Thus, the proposed method in this work has shown to minimize the simulation-to-reality gap for the generation of radar data.
</details>
<details>
<summary>摘要</summary>
主要方法 для模拟FMCW雷达是基于射线追踪，通常是计算昂贵的并不考虑背景噪声。这项工作提出了一种更快的FMCW雷达模拟方法，可以生成基于生成对抗网络（GAN）的合成雷达数据。代码和预训练 весов在GitHub上公开可用。这种方法生成了16个同时发射的毫声信号，这使得生成的数据可以用于进一步开发雷达数据处理算法（过滤和归一）。这可以增加数据增强的潜在性，例如通过生成不可能或安全关键的场景中的数据，以便在实际生产中不可能重现。在这项工作中，GAN被训练使用雷达测量数据，用于生成雷达数据的 sintetic raw radar chirps。为生成这些数据，雷达车辆的距离和高斯噪声作为神经网络的输入。生成的雷达毫声信号被评估使用Frechet InceptionDistance（FID）。然后，雷达距离-方位（RA）图被计算两次：首先，基于生成数据使用这个GAN，然后，基于实际数据。基于这些RA图，一种适应阈值和边检测算法用于物体检测。结果表明，生成的数据具有准确的干扰雷达反射和背景噪声，基于毫声信号、RA图和物体检测结果的比较。因此，该提出的方法在本工作中已经成功地减小了模拟到实际的差距。
</details></li>
</ul>
<hr>
<h2 id="BlindSage-Label-Inference-Attacks-against-Node-level-Vertical-Federated-Graph-Neural-Networks"><a href="#BlindSage-Label-Inference-Attacks-against-Node-level-Vertical-Federated-Graph-Neural-Networks" class="headerlink" title="BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks"></a>BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02465">http://arxiv.org/abs/2308.02465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Arazzi, Mauro Conti, Stefanos Koffas, Marina Krcek, Antonino Nocera, Stjepan Picek, Jing Xu</li>
<li>for: 这篇论文的主要目的是研究 vertical federated learning（VFL）中的标签推理攻击，特别是在没有背景知识的情况下进行攻击。</li>
<li>methods: 本文使用了 zero-background knowledge 策略来进行攻击，具体是使用 Graph Neural Networks（GNNs）作为目标模型，并在 node classification 任务上进行了实验。</li>
<li>results: 本文的 proposed attack，BlindSage，在实验中实现了 nearly 100% 的准确率，甚至在攻击者没有任何背景知识的情况下仍能实现准确率高于 85%。此外，本文发现了一些常见的防御措施不能对抗本攻击，而且这些防御措施会对模型的表现造成负面影响。<details>
<summary>Abstract</summary>
Federated learning enables collaborative training of machine learning models by keeping the raw data of the involved workers private. One of its main objectives is to improve the models' privacy, security, and scalability. Vertical Federated Learning (VFL) offers an efficient cross-silo setting where a few parties collaboratively train a model without sharing the same features. In such a scenario, classification labels are commonly considered sensitive information held exclusively by one (active) party, while other (passive) parties use only their local information. Recent works have uncovered important flaws of VFL, leading to possible label inference attacks under the assumption that the attacker has some, even limited, background knowledge on the relation between labels and data. In this work, we are the first (to the best of our knowledge) to investigate label inference attacks on VFL using a zero-background knowledge strategy. To concretely formulate our proposal, we focus on Graph Neural Networks (GNNs) as a target model for the underlying VFL. In particular, we refer to node classification tasks, which are widely studied, and GNNs have shown promising results. Our proposed attack, BlindSage, provides impressive results in the experiments, achieving nearly 100% accuracy in most cases. Even when the attacker has no information about the used architecture or the number of classes, the accuracy remained above 85% in most instances. Finally, we observe that well-known defenses cannot mitigate our attack without affecting the model's performance on the main classification task.
</details>
<details>
<summary>摘要</summary>
合作学习（Federated Learning）可以保持参与者的原始数据加密，以提高模型的隐私、安全性和可扩展性。垂直联合学习（VFL）提供了跨存储设置，在不共享同一个特征的情况下，几个党合作训练模型。在这种场景下，分类标签通常被视为活动党拥有的敏感信息，而其他投降党仅使用本地信息。现有研究曝光了VFL的重要漏洞，可能导致标签推理攻击，假设攻击者具有一定的背景知识关于标签和数据之间的关系。在这种情况下，我们是第一个（到我们知道的最佳）investigate VFL中的标签推理攻击，使用零背景知识策略。为了具体实现我们的提议，我们将Graph Neural Networks（GNNs）作为目标模型，特别是节点分类任务，这是广泛研究的领域，GNNs在这些任务上表现出色。我们提出的攻击方法，BlindSage，在实验中提供了很好的结果，在大多数情况下达到了nearly 100%的准确率。即使攻击者没有关于使用的架构或分类数量的任何信息，我们的攻击仍然在大多数情况下保持了超过85%的准确率。最后，我们发现了一些常见的防御无法防止我们的攻击，而不会影响模型在主要分类任务上的性能。
</details></li>
</ul>
<hr>
<h2 id="Universal-Approximation-of-Linear-Time-Invariant-LTI-Systems-through-RNNs-Power-of-Randomness-in-Reservoir-Computing"><a href="#Universal-Approximation-of-Linear-Time-Invariant-LTI-Systems-through-RNNs-Power-of-Randomness-in-Reservoir-Computing" class="headerlink" title="Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing"></a>Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02464">http://arxiv.org/abs/2308.02464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Jere, Lizhong Zheng, Karim Said, Lingjia Liu<br>for:RC is used to overcome the issues of vanishing and exploding gradients in standard RNN training, and it has demonstrated superior empirical performance in various fields. However, the theoretical grounding for this observed performance has not been fully developed.methods:The paper uses a signal processing interpretation of RC to universally approximate a general LTI system. The optimal probability distribution function for generating the recurrent weights of the underlying RNN of the RC is derived, and extensive numerical evaluations are provided to validate the optimality of the derived distribution.results:The paper shows that RC can universally approximate a general LTI system, and provides a clear signal processing-based model interpretability of RC. The derived optimal probability distribution function for the recurrent weights of the RC is found to be effective in simulating the LTI system. The work provides a complete analytical characterization of the untrained recurrent weights, which is important for explainable machine learning applications where training samples are limited.<details>
<summary>Abstract</summary>
Recurrent neural networks (RNNs) are known to be universal approximators of dynamic systems under fairly mild and general assumptions, making them good tools to process temporal information. However, RNNs usually suffer from the issues of vanishing and exploding gradients in the standard RNN training. Reservoir computing (RC), a special RNN where the recurrent weights are randomized and left untrained, has been introduced to overcome these issues and has demonstrated superior empirical performance in fields as diverse as natural language processing and wireless communications especially in scenarios where training samples are extremely limited. On the contrary, the theoretical grounding to support this observed performance has not been fully developed at the same pace. In this work, we show that RNNs can provide universal approximation of linear time-invariant (LTI) systems. Specifically, we show that RC can universally approximate a general LTI system. We present a clear signal processing interpretation of RC and utilize this understanding in the problem of simulating a generic LTI system through RC. Under this setup, we analytically characterize the optimal probability distribution function for generating the recurrent weights of the underlying RNN of the RC. We provide extensive numerical evaluations to validate the optimality of the derived optimum distribution of the recurrent weights of the RC for the LTI system simulation problem. Our work results in clear signal processing-based model interpretability of RC and provides theoretical explanation for the power of randomness in setting instead of training RC's recurrent weights. It further provides a complete optimum analytical characterization for the untrained recurrent weights, marking an important step towards explainable machine learning (XML) which is extremely important for applications where training samples are limited.
</details>
<details>
<summary>摘要</summary>
循环神经网络（RNN）是已知的通用函数近似器，可以处理时间信息。然而，标准的RNN训练过程中通常会出现消失和扩散梯度的问题。宽泛计算（RC）是一种特殊的RNN，其循环权重随机并未训练，可以解决这些问题，并在自然语言处理和无线通信等领域展现出优于标准RNN的实际表现。然而，对于RC的理论基础的发展并没有与实际表现相同的速度。在这个工作中，我们证明了RNN可以 universally approximate linear time-invariant（LTI）系统。具体来说，我们证明了RC可以 universally approximate任何LTI系统。我们提供了一个清晰的信号处理解释，用于理解RC的工作原理，并使用这种理解来解决一个通用LTI系统的模拟问题。在这个设置下，我们分析性地 caracterize了RC的优化梯度的概率分布。我们进行了广泛的数值评估，以验证我们所 derivated的RC的优化梯度概率分布的优化性。我们的工作具有以下优点：1. 我们提供了一个信号处理基于的RC模型解释，具有明确的数学基础。2. 我们提供了一个可靠的数值评估，以验证RC的优化梯度概率分布的优化性。3. 我们的结论可以用于解释Randomness在RC中的作用，并且提供了一个完整的优化梯度概率分布的分析。4. 我们的工作对于Explainable Machine Learning（XML）的发展具有重要意义，特别是在训练样本数量有限的情况下。总之，我们的工作提供了一个信号处理基于的RC模型解释，并且提供了一个可靠的数值评估，以验证RC的优化梯度概率分布的优化性。此外，我们的结论可以用于解释Randomness在RC中的作用，并且对于XML的发展具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Accurate-Reduced-Order-Modeling-of-a-MOOSE-based-Additive-Manufacturing-Model-with-Operator-Learning"><a href="#Fast-and-Accurate-Reduced-Order-Modeling-of-a-MOOSE-based-Additive-Manufacturing-Model-with-Operator-Learning" class="headerlink" title="Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning"></a>Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02462">http://arxiv.org/abs/2308.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</li>
<li>for: 本研究目的是为了提高additive manufacturing（AM）过程中的物料特性，通过 manipulate 生产过程参数以实现特定的物料特性。</li>
<li>methods: 本研究使用了Operator Learning（OL）方法，通过学习减少了过程变量的 differential equation 家族，以建立快速和准确的减少模型（ROM）。</li>
<li>results: 研究发现，OL方法可以与传统的深度神经网络（DNN）相比，在预测scalar模型响应时提供了相似的性能，且在精度和泛化性方面甚至超越DNN。DNN基于的ROM具有最快的训练时间，但是所有的ROM都比原始的MOOSE模型快速，且仍提供了准确的预测。FNO在预测时间序数据方面具有较小的平均预测误差，但是DeepONet具有较大的变化。不同于DNN，FNO和DeepONet都可以无需维度减少技术来预测时间序数据。<details>
<summary>Abstract</summary>
One predominant challenge in additive manufacturing (AM) is to achieve specific material properties by manipulating manufacturing process parameters during the runtime. Such manipulation tends to increase the computational load imposed on existing simulation tools employed in AM. The goal of the present work is to construct a fast and accurate reduced-order model (ROM) for an AM model developed within the Multiphysics Object-Oriented Simulation Environment (MOOSE) framework, ultimately reducing the time/cost of AM control and optimization processes. Our adoption of the operator learning (OL) approach enabled us to learn a family of differential equations produced by altering process variables in the laser's Gaussian point heat source. More specifically, we used the Fourier neural operator (FNO) and deep operator network (DeepONet) to develop ROMs for time-dependent responses. Furthermore, we benchmarked the performance of these OL methods against a conventional deep neural network (DNN)-based ROM. Ultimately, we found that OL methods offer comparable performance and, in terms of accuracy and generalizability, even outperform DNN at predicting scalar model responses. The DNN-based ROM afforded the fastest training time. Furthermore, all the ROMs were faster than the original MOOSE model yet still provided accurate predictions. FNO had a smaller mean prediction error than DeepONet, with a larger variance for time-dependent responses. Unlike DNN, both FNO and DeepONet were able to simulate time series data without the need for dimensionality reduction techniques. The present work can help facilitate the AM optimization process by enabling faster execution of simulation tools while still preserving evaluation accuracy.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在添加制造（AM）是在运行时控制和优化过程中实现特定材料性能。这种控制通常会增加现有的Simulation工具在AM中的计算负担。目标是在MOOSE框架中开发一个快速和准确的减少维度模型（ROM），以降低AM控制和优化过程的时间/成本。我们采用了运算学（OL）方法，通过修改过程变量来学习激光的 Gaussian点热源生成的家族 diffeqential equations。我们使用了Fourier neural operator（FNO）和深度运算网络（DeepONet）来开发ROMs，并对这些OL方法与传统的深度神经网络（DNN）基于ROM进行比较。结果表明，OL方法可以与DNN相比，具有相同的性能和精度，并且在预测批量响应方面更高一点。DNN基于ROM培训时间最快，但所有ROM都比原始MOOSE模型更快，并且仍然提供了准确的预测。FNO的平均预测误差较小， DeepONet在时间相对应的响应中有较大的变化。不同于DNN，FNO和DeepONet都可以不需要维度减少技术来预测时间序列数据。现有的工作可以帮助加快AM优化过程中的Simulation工具执行，保持评估准确性。
</details></li>
</ul>
<hr>
<h2 id="Nonprehensile-Planar-Manipulation-through-Reinforcement-Learning-with-Multimodal-Categorical-Exploration"><a href="#Nonprehensile-Planar-Manipulation-through-Reinforcement-Learning-with-Multimodal-Categorical-Exploration" class="headerlink" title="Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration"></a>Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02459">http://arxiv.org/abs/2308.02459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar</li>
<li>for: 开发能够实现dexterous nonprehensile manipulation的 робот控制器，如pushing an object on a table。</li>
<li>methods: 使用Reinforcement Learning（RL）框架，并提出了多模态探索方法，以处理非线性和不确定性。</li>
<li>results: 实现了高精度、非平滑曲线和复杂运动的推动政策，并且可以承受外部干扰和观测噪声，同时也可以在多个推动器情况下进行缩放。<details>
<summary>Abstract</summary>
Developing robot controllers capable of achieving dexterous nonprehensile manipulation, such as pushing an object on a table, is challenging. The underactuated and hybrid-dynamics nature of the problem, further complicated by the uncertainty resulting from the frictional interactions, requires sophisticated control behaviors. Reinforcement Learning (RL) is a powerful framework for developing such robot controllers. However, previous RL literature addressing the nonprehensile pushing task achieves low accuracy, non-smooth trajectories, and only simple motions, i.e. without rotation of the manipulated object. We conjecture that previously used unimodal exploration strategies fail to capture the inherent hybrid-dynamics of the task, arising from the different possible contact interaction modes between the robot and the object, such as sticking, sliding, and separation. In this work, we propose a multimodal exploration approach through categorical distributions, which enables us to train planar pushing RL policies for arbitrary starting and target object poses, i.e. positions and orientations, and with improved accuracy. We show that the learned policies are robust to external disturbances and observation noise, and scale to tasks with multiple pushers. Furthermore, we validate the transferability of the learned policies, trained entirely in simulation, to a physical robot hardware using the KUKA iiwa robot arm. See our supplemental video: https://youtu.be/vTdva1mgrk4.
</details>
<details>
<summary>摘要</summary>
开发能够实现灵活无握 manipulate robot控制器，如表面上推pushing一个物体，是一项挑战。由于控制器的下降启动和混合动力学性质，以及由摩擦产生的不确定性，需要复杂的控制行为。 reinforcement learning (RL) 是一个强大的框架 для开发这类 robot控制器。然而，过去RL文献中对非握持推动任务的精度、非精炸曲线和简单运动（即不含旋转）很低。我们 conjecture  previous 使用单模态探索策略 failed  to capture 任务的内在混合动力学性质， arise  from 不同的接触交互方式 between the robot and the object， such as sticking, sliding, and separation.在这种工作中，我们提议使用多模态探索方法，通过 categorical distributions，可以训练平面推动 RL 政策，对于任意开始和目标物体姿态（位置和 orientations），并具有改进的精度。我们显示了学习的策略对于外部干扰和观察噪声具有Robustness，并可扩展到多个推动者任务。此外，我们验证了学习策略，完全在 simulator 中训练，在物理 Kuka iiwa 机械臂上运行。请参考我们的补充视频：https://youtu.be/vTdva1mgrk4.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-and-Propagation-in-Accelerated-MRI-Reconstruction"><a href="#Uncertainty-Estimation-and-Propagation-in-Accelerated-MRI-Reconstruction" class="headerlink" title="Uncertainty Estimation and Propagation in Accelerated MRI Reconstruction"></a>Uncertainty Estimation and Propagation in Accelerated MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02631">http://arxiv.org/abs/2308.02631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paulkogni/mr-recon-uq">https://github.com/paulkogni/mr-recon-uq</a></li>
<li>paper_authors: Paul Fischer, Thomas Küstner, Christian F. Baumgartner</li>
<li>for: 这篇论文是关于基于深度学习的MRI重建技术的研究，尤其是在高速 Settings下 achievable 的重建质量。</li>
<li>methods: 该论文提出了一种新的 probabilistic reconstruction technique (PHiRec)，基于conditional hierarchical variational autoencoders的想法。</li>
<li>results: 该方法可以生成高质量的重建结果，同时也可以提供substantially better calibrated的uncertainty quantification，以及可以协调到下游 segmentation 任务中的uncertainty estimate。<details>
<summary>Abstract</summary>
MRI reconstruction techniques based on deep learning have led to unprecedented reconstruction quality especially in highly accelerated settings. However, deep learning techniques are also known to fail unexpectedly and hallucinate structures. This is particularly problematic if reconstructions are directly used for downstream tasks such as real-time treatment guidance or automated extraction of clinical paramters (e.g. via segmentation). Well-calibrated uncertainty quantification will be a key ingredient for safe use of this technology in clinical practice. In this paper we propose a novel probabilistic reconstruction technique (PHiRec) building on the idea of conditional hierarchical variational autoencoders. We demonstrate that our proposed method produces high-quality reconstructions as well as uncertainty quantification that is substantially better calibrated than several strong baselines. We furthermore demonstrate how uncertainties arising in the MR econstruction can be propagated to a downstream segmentation task, and show that PHiRec also allows well-calibrated estimation of segmentation uncertainties that originated in the MR reconstruction process.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Modelling-of-Levy-Area-for-High-Order-SDE-Simulation"><a href="#Generative-Modelling-of-Levy-Area-for-High-Order-SDE-Simulation" class="headerlink" title="Generative Modelling of Lévy Area for High Order SDE Simulation"></a>Generative Modelling of Lévy Area for High Order SDE Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02452">http://arxiv.org/abs/2308.02452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andraž Jelinčič, Jiajie Tao, William F. Turner, Thomas Cass, James Foster, Hao Ni</li>
<li>for: 这篇论文是为了提出一种基于深度学习的模型，用于生成精确的莱比零区 conditional on  Брау内幂增量。</li>
<li>methods: 这种模型使用了一种专门设计的 GNN-inspired 架构，以保证输出分布和 conditioning 变量之间的正确依赖关系。同时，使用了一种基于特征函数的数学原理的 discriminator。</li>
<li>results: 对于 4 维 Брау内幂，这种模型 exhibits state-of-the-art 性能 across 多个维度，并且在数学金融中的 log-Heston 模型中进行了一个数值实验，证明了高质量的 synthetic 莱比零区可以导致高阶弱 convergence 和 variance reduction when using multilevel Monte Carlo (MLMC)。<details>
<summary>Abstract</summary>
It is well known that, when numerically simulating solutions to SDEs, achieving a strong convergence rate better than O(\sqrt{h}) (where h is the step size) requires the use of certain iterated integrals of Brownian motion, commonly referred to as its "L\'{e}vy areas". However, these stochastic integrals are difficult to simulate due to their non-Gaussian nature and for a d-dimensional Brownian motion with d > 2, no fast almost-exact sampling algorithm is known.   In this paper, we propose L\'{e}vyGAN, a deep-learning-based model for generating approximate samples of L\'{e}vy area conditional on a Brownian increment. Due to our "Bridge-flipping" operation, the output samples match all joint and conditional odd moments exactly. Our generator employs a tailored GNN-inspired architecture, which enforces the correct dependency structure between the output distribution and the conditioning variable. Furthermore, we incorporate a mathematically principled characteristic-function based discriminator. Lastly, we introduce a novel training mechanism termed "Chen-training", which circumvents the need for expensive-to-generate training data-sets. This new training procedure is underpinned by our two main theoretical results.   For 4-dimensional Brownian motion, we show that L\'{e}vyGAN exhibits state-of-the-art performance across several metrics which measure both the joint and marginal distributions. We conclude with a numerical experiment on the log-Heston model, a popular SDE in mathematical finance, demonstrating that high-quality synthetic L\'{e}vy area can lead to high order weak convergence and variance reduction when using multilevel Monte Carlo (MLMC).
</details>
<details>
<summary>摘要</summary>
它已经广泛知道，当数值实现解决涨落方程时，以较好的减法速率（即幂函数）为依据，需要使用某些迭代积分的布朗运动，通常称为其"Lévy区域"。然而，这些随机积分具有非高斯性质，而且为高维布朗运动（d > 2），没有快速准确样本生成算法。在这篇论文中，我们提出了LévyGAN，一种基于深度学习的模型，用于生成 conditional Lévy area 的相似样本。由于我们的 "桥跃" 操作，输出样本满足所有的共同偶极值和条件偶极值。我们的生成器采用了特制的 GNN-inspired 架构，以保证输出分布和条件变量之间的正确依赖关系。此外，我们采用了基于特征函数的数学原理的批量分类器。最后，我们介绍了一种新的训练机制，称为 "Chen-training"，它使得不需要生成昂贵的训练数据集。这种新的训练过程基于我们的两个主要理论结论。对于四维布朗运动，我们表明了LévyGAN在多个维度上的性能都达到了状态机器人的水平。我们结束于一个对柯本方程（一种流行的数学金融方程）的数字实验，展示了高质量的 synthetic Lévy area 可以导致高阶弱整合和变量减少，当使用多层 Monte Carlo（MLMC）时。
</details></li>
</ul>
<hr>
<h2 id="Pruning-a-neural-network-using-Bayesian-inference"><a href="#Pruning-a-neural-network-using-Bayesian-inference" class="headerlink" title="Pruning a neural network using Bayesian inference"></a>Pruning a neural network using Bayesian inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02451">http://arxiv.org/abs/2308.02451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunil Mathew, Daniel B. Rowe</li>
<li>for: 这个研究论文的目的是提出一种基于 Bayesian 推理的神经网络减少技术，以降低大神经网络的计算和存储占用。</li>
<li>methods: 该方法使用 Bayesian 推理，将神经网络的 posterior 概率分布计算到减少前和减少后，然后利用这些概率导向 iterative 减少。</li>
<li>results: 经过对多个 benchmarck 进行全面评估，我们表明了我们的方法可以实现 жела的稀有性，同时保持竞争性的准确率。<details>
<summary>Abstract</summary>
Neural network pruning is a highly effective technique aimed at reducing the computational and memory demands of large neural networks. In this research paper, we present a novel approach to pruning neural networks utilizing Bayesian inference, which can seamlessly integrate into the training procedure. Our proposed method leverages the posterior probabilities of the neural network prior to and following pruning, enabling the calculation of Bayes factors. The calculated Bayes factors guide the iterative pruning. Through comprehensive evaluations conducted on multiple benchmarks, we demonstrate that our method achieves desired levels of sparsity while maintaining competitive accuracy.
</details>
<details>
<summary>摘要</summary>
大脑网络剪辑是一种非常有效的技术，用于减少大脑网络的计算和内存占用。在这篇研究报告中，我们提出了一种基于 bayesian 推理的 neural network 剪辑方法，可以轻松地 integrate 到训练过程中。我们的提议方法利用 neural network 之前和之后剪辑 posterior 概率，以计算 bayes 因子。计算出的 bayes 因子导引了迭代剪辑。我们在多个 benchmark 上进行了广泛的评估，并证明了我们的方法可以达到所需的稀疏程度，同时保持竞争性的准确率。
</details></li>
</ul>
<hr>
<h2 id="From-Military-to-Healthcare-Adopting-and-Expanding-Ethical-Principles-for-Generative-Artificial-Intelligence"><a href="#From-Military-to-Healthcare-Adopting-and-Expanding-Ethical-Principles-for-Generative-Artificial-Intelligence" class="headerlink" title="From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence"></a>From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02448">http://arxiv.org/abs/2308.02448</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Oniani, Jordan Hilsman, Yifan Peng, COL, Ronald K. Poropatich, COL Jeremy C. Pamplin, LTC Gary L. Legault, Yanshan Wang</li>
<li>For: The paper aims to propose ethical principles for the use of generative AI in healthcare, with the goal of addressing ethical dilemmas and challenges in the integration of this technology in the medical field.* Methods: The paper uses a framework called GREAT PLEA, which stands for Governance, Reliability, Equity, Accountability, Traceability, Privacy, Lawfulness, Empathy, and Autonomy, to guide the development of ethical principles for generative AI in healthcare.* Results: The paper proposes a set of ethical principles for generative AI in healthcare, with the goal of proactively addressing the ethical dilemmas and challenges posed by the integration of this technology in the medical field. These principles include governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy.<details>
<summary>Abstract</summary>
In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about its application in healthcare, mainly due to concerns about transparency and related issues. Meanwhile, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. However, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI. In this paper, we propose GREAT PLEA ethical principles, encompassing governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim to proactively address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare.
</details>
<details>
<summary>摘要</summary>
在2020年，美国国防部官方公布了一组伦理原则，用于导引人工智能技术在未来战场上的使用。尽管在不同的情况下，军人和医疗人员之间有 stark 的不同，但是在面临生命改变的情况下，他们都需要快速做出决策。战场上的战士常常面临生命改变的情况，需要快速做出决策。医疗人员在医疗环境中也经常面临快速变化的情况，如在急诊室或在治疗生命危险的情况下。新兴的生成型人工智能技术在计算能力的提高和医疗数据的增加下，将健康卫生领域 revolutionized。 recent years, this technology has attracted significant attention from the research community, leading to debates about its application in healthcare, primarily due to concerns about transparency and related issues. However, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. Despite this, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI.在这篇论文中，我们提出了 GREAT PLEA 伦理原则，包括政府、可靠性、公平性、责任、可追溯性、隐私、法律合法性、 Empathy 和自主权，用于生成型人工智能技术在健康卫生领域中的应用。我们希望通过这些原则，active address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Preferential-Attached-kNN-Graph-with-Distribution-Awareness"><a href="#Adaptive-Preferential-Attached-kNN-Graph-with-Distribution-Awareness" class="headerlink" title="Adaptive Preferential Attached kNN Graph with Distribution-Awareness"></a>Adaptive Preferential Attached kNN Graph with Distribution-Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02442">http://arxiv.org/abs/2308.02442</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/4alexmin/knnsotas">https://github.com/4alexmin/knnsotas</a></li>
<li>paper_authors: Shaojie Min, Ji Liu</li>
<li>for: 提高机器学习任务中的总体性能和精度，特别是在具有复杂分布的实际数据上。</li>
<li>methods: 基于分布情况的 adaptive-k 技术，在建构图时采用分布信息作为一体。</li>
<li>results: 在多个实际数据集上进行了严格的评估，并证明了 paNNG 在不同场景下的适应性和效果优于现有算法。<details>
<summary>Abstract</summary>
Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks due to their simplicity and effectiveness. However, as factual data often inherit complex distributions, the conventional kNN graph's reliance on a unified k-value can hinder its performance. A crucial factor behind this challenge is the presence of ambiguous samples along decision boundaries that are inevitably more prone to incorrect classifications. To address the situation, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which adopts distribution-aware adaptive-k into graph construction. By incorporating distribution information as a cohesive entity, paNNG can significantly improve performance on ambiguous samples by "pulling" them towards their original classes and hence enhance overall generalization capability. Through rigorous evaluations on diverse datasets, paNNG outperforms state-of-the-art algorithms, showcasing its adaptability and efficacy across various real-world scenarios.
</details>
<details>
<summary>摘要</summary>
基于图的kNN算法在机器学习任务中广泛受欢迎，因为它们简单易用而且有效。然而，由于实际数据经常具有复杂的分布，传统的kNN图中对一个固定的k值的依赖可能会降低其表现。这个挑战的关键原因在于具有抽象样本的异常高错误率，这些样本通常位于分类边界附近。为解决这个问题，我们提出了Preferential Attached k-Nearest Neighbors Graph（paNNG），该算法在图struc图构建中采用了分布情况的敏感性。通过将分布信息作为一个整体纳入图构建，paNNG可以在抽象样本上提高表现，“拖”这些样本向其原来的类别，从而提高总的泛化能力。经过严谨的评估，paNNG在多种实际场景中舒适地超越了当前的状态árt算法，示出了它的适应性和效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/05/cs.LG_2023_08_05/" data-id="clltau92l005fcr883v572tl3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/cs.SD_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/05/cs.SD_2023_08_05/">cs.SD - 2023-08-05 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ApproBiVT-Lead-ASR-Models-to-Generalize-Better-Using-Approximated-Bias-Variance-Tradeoff-Guided-Early-Stopping-and-Checkpoint-Averaging"><a href="#ApproBiVT-Lead-ASR-Models-to-Generalize-Better-Using-Approximated-Bias-Variance-Tradeoff-Guided-Early-Stopping-and-Checkpoint-Averaging" class="headerlink" title="ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging"></a>ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02870">http://arxiv.org/abs/2308.02870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fangyuan Wang, Ming Hao, Yuhai Shi, Bo Xu</li>
<li>for: 提高自动语音识别（ASR）模型的性能，通过重新评估和更新早期停止和多个检查点的策略，以降低模型的总泛化误差。</li>
<li>methods: 使用适应停止和多个检查点的策略，通过评估训练损失和验证损失来评估模型的偏差和异谱，并通过Approximated Bias-Variance Tradeoff（ApproBiVT）来导引这些策略。</li>
<li>results: 在使用高级ASR模型时，该策略可以提供2.5%-3.7%和3.1%-4.6%的CER减少在AISHELL-1和AISHELL-2上。<details>
<summary>Abstract</summary>
The conventional recipe for Automatic Speech Recognition (ASR) models is to 1) train multiple checkpoints on a training set while relying on a validation set to prevent overfitting using early stopping and 2) average several last checkpoints or that of the lowest validation losses to obtain the final model. In this paper, we rethink and update the early stopping and checkpoint averaging from the perspective of the bias-variance tradeoff. Theoretically, the bias and variance represent the fitness and variability of a model and the tradeoff of them determines the overall generalization error. But, it's impractical to evaluate them precisely. As an alternative, we take the training loss and validation loss as proxies of bias and variance and guide the early stopping and checkpoint averaging using their tradeoff, namely an Approximated Bias-Variance Tradeoff (ApproBiVT). When evaluating with advanced ASR models, our recipe provides 2.5%-3.7% and 3.1%-4.6% CER reduction on the AISHELL-1 and AISHELL-2, respectively.
</details>
<details>
<summary>摘要</summary>
传统的自动语音识别（ASR）模型的制作方法是：1）在训练集上训练多个检查点，使用验证集来防止过拟合，并2）使用多个最后的检查点或验证损失最低的一个来获取最终模型。在这篇论文中，我们重新思考了早期停止和检查点平均的方法，从偏差-差异质量的角度来更新。在理论上，偏差和差异表示模型的适应度和多样性，它们之间的质量交换决定总的泛化误差。但是，不可能准确地评估它们。因此，我们使用训练损失和验证损失作为偏差和差异的代理，并通过它们之间的质量交换来导引早期停止和检查点平均，即 Approximated Bias-Variance Tradeoff（ApproBiVT）。在使用高级ASR模型进行评估时，我们的制作方法可以提供2.5%-3.7%和3.1%-4.6%的CER减少在AISHELL-1和AISHELL-2上。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Exploration-of-Joint-training-for-Singing-Voice-Synthesis"><a href="#A-Systematic-Exploration-of-Joint-training-for-Singing-Voice-Synthesis" class="headerlink" title="A Systematic Exploration of Joint-training for Singing Voice Synthesis"></a>A Systematic Exploration of Joint-training for Singing Voice Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02867">http://arxiv.org/abs/2308.02867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuning Wu, Yifeng Yu, Jiatong Shi, Tao Qian, Qin Jin</li>
<li>for: 提高 singing voice synthesis（SVS）系统的性能和可读性。</li>
<li>methods: 通过对音响模型和 vocoder 进行共同训练，提高 SVS 系统的 JOINT 训练性能。</li>
<li>results: 通过实验表明，我们的 JOINT 训练策略在不同数据集上具有更稳定的性能提升，同时也提高了整个框架的可读性。<details>
<summary>Abstract</summary>
There has been a growing interest in using end-to-end acoustic models for singing voice synthesis (SVS). Typically, these models require an additional vocoder to transform the generated acoustic features into the final waveform. However, since the acoustic model and the vocoder are not jointly optimized, a gap can exist between the two models, leading to suboptimal performance. Although a similar problem has been addressed in the TTS systems by joint-training or by replacing acoustic features with a latent representation, adopting corresponding approaches to SVS is not an easy task. How to improve the joint-training of SVS systems has not been well explored. In this paper, we conduct a systematic investigation of how to better perform a joint-training of an acoustic model and a vocoder for SVS. We carry out extensive experiments and demonstrate that our joint-training strategy outperforms baselines, achieving more stable performance across different datasets while also increasing the interpretability of the entire framework.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>现在，使用端到端音声模型进行唱歌voice synthesis（SVS）已经有越来越多的兴趣。通常，这些模型需要一个额外的 vocoder 将生成的音声特征转换成最终波形。然而，由于音声模型和 vocoder 并没有同时优化，因此两个模型之间可能存在一个差距，导致表现不佳。虽然在 TTS 系统中Addressing 类似问题的方法已经被研究，但在 SVS 中采用相应的方法并不是一件容易的事情。如何改进 SVS 系统的 JOINT 训练方法尚未得到了充分的探索。在这篇论文中，我们进行了系统的调查，探讨了如何更好地进行 SVS 系统的 JOINT 训练。我们进行了广泛的实验，并证明了我们的 JOINT 训练策略在不同的数据集上表现更稳定，同时也提高了整个框架的可解释性。
</details></li>
</ul>
<hr>
<h2 id="Bootstrapping-Contrastive-Learning-Enhanced-Music-Cold-Start-Matching"><a href="#Bootstrapping-Contrastive-Learning-Enhanced-Music-Cold-Start-Matching" class="headerlink" title="Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching"></a>Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02844">http://arxiv.org/abs/2308.02844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinping Zhao, Ying Zhang, Qiang Xiao, Yuming Ren, Yingchun Yang</li>
<li>for: 这个论文主要针对的是音乐冷启动匹配任务，即给出一个冷启动歌曲请求，然后快速推送该歌曲到相关歌曲的听众中以进行温身。但是，关于这个任务的研究非常有限，因此本文将music冷启动匹配问题进行详细定义并提供一种方案。</li>
<li>methods: 在线下训练中，我们尝试通过歌曲内容特征来学习高质量的歌曲表示。然而，我们发现监督信号通常遵循力学律分布，导致表示学习受到扭曲。为解决这个问题，我们提出了一种新的对比学习方法 named Bootstrapping Contrastive Learning (BCL)，通过对比激活来强制高质量表示学习。</li>
<li>results: 在线下数据集和在线系统上进行了广泛的实验，并证明了我们的方法的有效性和高效性。目前，我们已经将其部署到NetEase Cloud Music上，影响了百万用户。代码将在未来发布。<details>
<summary>Abstract</summary>
We study a particular matching task we call Music Cold-Start Matching. In short, given a cold-start song request, we expect to retrieve songs with similar audiences and then fastly push the cold-start song to the audiences of the retrieved songs to warm up it. However, there are hardly any studies done on this task. Therefore, in this paper, we will formalize the problem of Music Cold-Start Matching detailedly and give a scheme. During the offline training, we attempt to learn high-quality song representations based on song content features. But, we find supervision signals typically follow power-law distribution causing skewed representation learning. To address this issue, we propose a novel contrastive learning paradigm named Bootstrapping Contrastive Learning (BCL) to enhance the quality of learned representations by exerting contrastive regularization. During the online serving, to locate the target audiences more accurately, we propose Clustering-based Audience Targeting (CAT) that clusters audience representations to acquire a few cluster centroids and then locate the target audiences by measuring the relevance between the audience representations and the cluster centroids. Extensive experiments on the offline dataset and online system demonstrate the effectiveness and efficiency of our method. Currently, we have deployed it on NetEase Cloud Music, affecting millions of users. Code will be released in the future.
</details>
<details>
<summary>摘要</summary>
我们研究一个名为音乐冷启始匹配的特定任务。简而言之，给定一个冷启始歌曲请求，我们希望可以检索到与其相似的听众，然后快速推送冷启始歌曲到检索到的听众中，以便让其热身。然而，目前 hardly any studies have been conducted on this task.因此，在这篇论文中，我们将 Music Cold-Start Matching 问题进行详细化 formalization，并提出一种方案。在线上训练中，我们尝试学习高质量的歌曲表示，基于歌曲内容特征。然而，我们发现超级vision signals  Typically follow a power-law distribution, causing skewed representation learning. To address this issue, we propose a novel contrastive learning paradigm named Bootstrapping Contrastive Learning (BCL) to enhance the quality of learned representations by exerting contrastive regularization.在在线服务中，我们提出一种名为 Clustering-based Audience Targeting (CAT) 的方法，通过对听众表示进行 clustering，获得一些集中点，然后通过测量听众表示和集中点之间的相似性，准确地定位目标听众。我们对做了大量的实验，证明了我们的方法的有效性和高效性。目前，我们已经将其部署到 NetEase Cloud Music，影响了数百万用户。代码将在未来发布。
</details></li>
</ul>
<hr>
<h2 id="Self-Distillation-Network-with-Ensemble-Prototypes-Learning-Robust-Speaker-Representations-without-Supervision"><a href="#Self-Distillation-Network-with-Ensemble-Prototypes-Learning-Robust-Speaker-Representations-without-Supervision" class="headerlink" title="Self-Distillation Network with Ensemble Prototypes: Learning Robust Speaker Representations without Supervision"></a>Self-Distillation Network with Ensemble Prototypes: Learning Robust Speaker Representations without Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02774">http://arxiv.org/abs/2308.02774</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/3D-Speaker">https://github.com/alibaba-damo-academy/3D-Speaker</a></li>
<li>paper_authors: Yafeng Chen, Siqi Zheng, Qian Chen</li>
<li>for: 本研究旨在无需使用说话人标签进行说话人验证系统的自我学习和鲁棒化。</li>
<li>methods: 本文提出了一种高效的Self-Distillation网络 Ensemble Prototypes（SDEP），用于实现无监督的说话人表示学习。</li>
<li>results: 在VoxCeleb数据集上进行了多种实验，并证明了SDEP框架在说话人验证中具有最高精度。SDEP在Voxceleb1 speaker verification评价标准上达到了新的最佳性能（即错误率1.94%、1.99%和3.77%），不使用任何说话人标签 durante el entrenamiento。<details>
<summary>Abstract</summary>
Training speaker-discriminative and robust speaker verification systems without speaker labels is still challenging and worthwhile to explore. Previous studies have noted a substantial performance disparity between self-supervised and fully supervised approaches. In this paper, we propose an effective Self-Distillation network with Ensemble Prototypes (SDEP) to facilitate self-supervised speaker representation learning. A range of experiments conducted on the VoxCeleb datasets demonstrate the superiority of the SDEP framework in speaker verification. SDEP achieves a new SOTA on Voxceleb1 speaker verification evaluation benchmark ( i.e., equal error rate 1.94\%, 1.99\%, and 3.77\% for trial Vox1-O, Vox1-E and Vox1-H , respectively), discarding any speaker labels in the training phase. Code will be publicly available at https://github.com/alibaba-damo-academy/3D-Speaker.
</details>
<details>
<summary>摘要</summary>
<SYS>使用无标签的语音训练SpeakerVerification系统仍然是一项挑战性的任务，值得进行探索。过去的研究表明，自动学习和完全监督方法之间存在很大的性能差距。本文提出了一种高效的Self-Distillation网络加ensemble Prototypes（SDEP），用于实现无标签语音表示学习。在VoxCeleb dataset上进行了多种实验，得到了SDEP框架在Speaker Verification中的新最佳性能（即Voxceleb1的误差率为1.94%、1.99%和3.77%），不使用任何语音标签 during training phase。代码将在https://github.com/alibaba-damo-academy/3D-Speaker上公开。</SYS>Note: "SDEP" stands for "Self-Distillation network with Ensemble Prototypes".
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/05/cs.SD_2023_08_05/" data-id="clltau93p008icr88b5yucyok" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/eess.IV_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/05/eess.IV_2023_08_05/">eess.IV - 2023-08-05 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Flashlight-Search-Medial-Axis-A-Pixel-Free-Pore-Network-Extraction-Algorithm"><a href="#Flashlight-Search-Medial-Axis-A-Pixel-Free-Pore-Network-Extraction-Algorithm" class="headerlink" title="Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm"></a>Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10990">http://arxiv.org/abs/2308.10990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Liu, Tao Zhang, Shuyu Sun</li>
<li>for: 该研究旨在提出一种无像素的维持精度的脉径网络EXTRACTION方法，以便在大规模的含材质媒体中实现流体流动的研究。</li>
<li>methods: 该方法基于探灯搜索中心轴（FSMA）算法，在维持精度的情况下大幅降低计算复杂性，可以应用于多种不同的含材质媒体和流体流动情况。</li>
<li>results: 实验结果表明，FSMA算法可以准确地找到脉径网络，无论含材质媒体的结构如何，也无论脉径和喉径中心的位置如何。此外，该算法还可以检测死端脉径，这对于多相流动在含材质媒体中的研究非常重要。<details>
<summary>Abstract</summary>
Pore-network models (PNMs) have become an important tool in the study of fluid flow in porous media over the last few decades, and the accuracy of their results highly depends on the extraction of pore networks. Traditional methods of pore-network extraction are based on pixels and require images with high quality. Here, a pixel-free method called the flashlight search medial axis (FSMA) algorithm is proposed for pore-network extraction in a continuous space. The search domain in a two-dimensional space is a line, whereas a surface domain is searched in a three-dimensional scenario. Thus, the FSMA algorithm follows the dimensionality reduction idea; the medial axis can be identified using only a few points instead of calculating every point in the void space. In this way, computational complexity of this method is greatly reduced compared to that of traditional pixel-based extraction methods, thus enabling large-scale pore-network extraction. Based on cases featuring two- and three-dimensional porous media, the FSMA algorithm performs well regardless of the topological structure of the pore network or the positions of the pore and throat centers. This algorithm can also be used to examine both closed- and open-boundary cases. Finally, the FSMA algorithm can search dead-end pores, which is of great significance in the study of multiphase flow in porous media.
</details>
<details>
<summary>摘要</summary>
PORE-NETWORK MODELS (PNMs) 已经在过去几十年内成为论 fluid flow 在孔隙媒体的研究工具，而实际结果的准确性很大程度上取决于破孔网络的提取。传统的破孔网络提取方法基于像素，需要高质量的图像。在这里，一种没有像素的方法 called  flashlight search medial axis (FSMA) 算法是用于破孔网络提取的。在两维空间中，搜索域是一条直线，而在三维情况下，搜索域是一个表面。因此，FSMA 算法遵循缩放空间的想法，通过仅在缺空间中标识中点而不是计算所有点。这样，FSMA 算法的计算复杂性被大幅降低，从而实现大规模的破孔网络提取。无论破孔网络的结构是二维还是三维，FSMA 算法都能够正确地提取破孔网络。此外，该算法还可以处理封闭边界和开放边界两种情况。最后，FSMA 算法还可以搜索死绕孔，这对多相流在孔隙媒体的研究非常重要。
</details></li>
</ul>
<hr>
<h2 id="Landmark-Detection-using-Transformer-Toward-Robot-assisted-Nasal-Airway-Intubation"><a href="#Landmark-Detection-using-Transformer-Toward-Robot-assisted-Nasal-Airway-Intubation" class="headerlink" title="Landmark Detection using Transformer Toward Robot-assisted Nasal Airway Intubation"></a>Landmark Detection using Transformer Toward Robot-assisted Nasal Airway Intubation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02845">http://arxiv.org/abs/2308.02845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/conorlth/airway_intubation_landmarks_detection">https://github.com/conorlth/airway_intubation_landmarks_detection</a></li>
<li>paper_authors: Tianhang Liu, Hechen Li, Long Bai, Yanan Wu, An Wang, Mobarakol Islam, Hongliang Ren</li>
<li>for: 该研究旨在提供一种高精度的自动标记检测方法，用于机器人协助的鼻腔插管。</li>
<li>methods: 该方法基于变换器（DeTR），并采用了可变DeTR和语义对齐匹配模块来检测鼻腔中的两个重要标记（鼻孔和肺膜）。</li>
<li>results: 实验结果表明，该方法可以具有竞争力的检测精度。<details>
<summary>Abstract</summary>
Robot-assisted airway intubation application needs high accuracy in locating targets and organs. Two vital landmarks, nostrils and glottis, can be detected during the intubation to accommodate the stages of nasal intubation. Automated landmark detection can provide accurate localization and quantitative evaluation. The Detection Transformer (DeTR) leads object detectors to a new paradigm with long-range dependence. However, current DeTR requires long iterations to converge, and does not perform well in detecting small objects. This paper proposes a transformer-based landmark detection solution with deformable DeTR and the semantic-aligned-matching module for detecting landmarks in robot-assisted intubation. The semantics aligner can effectively align the semantics of object queries and image features in the same embedding space using the most discriminative features. To evaluate the performance of our solution, we utilize a publicly accessible glottis dataset and automatically annotate a nostril detection dataset. The experimental results demonstrate our competitive performance in detection accuracy. Our code is publicly accessible.
</details>
<details>
<summary>摘要</summary>
机器人协助气管插管应用需要高精度在目标和器官的位置检测。气管插管过程中可以检测到两个重要的特征点，即鼻孔和软颈椎。自动化特征点检测可以提供高精度的位置定位和量化评估。 however， current DeTR requires long iterations to converge, and does not perform well in detecting small objects. This paper proposes a transformer-based landmark detection solution with deformable DeTR and the semantic-aligned-matching module for detecting landmarks in robot-assisted intubation. The semantics aligner can effectively align the semantics of object queries and image features in the same embedding space using the most discriminative features. To evaluate the performance of our solution, we utilize a publicly accessible glottis dataset and automatically annotate a nostril detection dataset. The experimental results demonstrate our competitive performance in detection accuracy. Our code is publicly accessible.Here's the translation in Traditional Chinese:机器人协助气管插管应用需要高精度在目标和器官的位置检测。气管插管过程中可以检测到两个重要的特征点，即鼻孔和软颈椎。自动化特征点检测可以提供高精度的位置定位和量化评估。然而， current DeTR需要长迭代才能融合，并不能够检测小型物体。这篇论文提出了一个基于 transformer 的特征点检测解决方案，该解决方案包括扭转 DeTR 和对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩�
</details></li>
</ul>
<hr>
<h2 id="Non-line-of-sight-reconstruction-via-structure-sparsity-regularization"><a href="#Non-line-of-sight-reconstruction-via-structure-sparsity-regularization" class="headerlink" title="Non-line-of-sight reconstruction via structure sparsity regularization"></a>Non-line-of-sight reconstruction via structure sparsity regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02782">http://arxiv.org/abs/2308.02782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duolan Huang, Quan Chen, Zhun Wei, Rui Chen</li>
<li>for: 本研究旨在提高非线视场（NLOS）成像质量，使其能够应用于自动驾驶、机器人视觉、医疗成像、安全监测等领域。</li>
<li>methods: 本研究使用了结构稀热（SS）正则化方法，通过利用方向光束变换（DLCT）模型中的核心矩阵来捕捉方向性的隐藏信息，从而提高NLOS成像的稀热性。</li>
<li>results: 经过实验和synthetic数据的评估，提出的方法可以在短时间和低SNR情况下提供高质量的NLOS成像，并且超过了现有的重建算法，特别是在封闭物体探测方面表现出色。<details>
<summary>Abstract</summary>
Non-line-of-sight (NLOS) imaging allows for the imaging of objects around a corner, which enables potential applications in various fields such as autonomous driving, robotic vision, medical imaging, security monitoring, etc. However, the quality of reconstruction is challenged by low signal-noise-ratio (SNR) measurements. In this study, we present a regularization method, referred to as structure sparsity (SS) regularization, for denoising in NLOS reconstruction. By exploiting the prior knowledge of structure sparseness, we incorporate nuclear norm penalization into the cost function of directional light-cone transform (DLCT) model for NLOS imaging system. This incorporation effectively integrates the neighborhood information associated with the directional albedo, thereby facilitating the denoising process. Subsequently, the reconstruction is achieved by optimizing a directional albedo model with SS regularization using fast iterative shrinkage-thresholding algorithm. Notably, the robust reconstruction of occluded objects is observed. Through comprehensive evaluations conducted on both synthetic and experimental datasets, we demonstrate that the proposed approach yields high-quality reconstructions, surpassing the state-of-the-art reconstruction algorithms, especially in scenarios involving short exposure and low SNR measurements.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dual-Degradation-Inspired-Deep-Unfolding-Network-for-Low-Light-Image-Enhancement"><a href="#Dual-Degradation-Inspired-Deep-Unfolding-Network-for-Low-Light-Image-Enhancement" class="headerlink" title="Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement"></a>Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02776">http://arxiv.org/abs/2308.02776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huake Wang, Xingsong Hou, Xiaoyang Yan<br>for:这篇论文的目的是提出一种基于深度恢复模型的低光照图像改进方法，以解决现有的低光照图像恢复方法往往强调黑盒网络的增强性能，而忽略了图像恢复模型的物理意义。methods:该论文提出了一种基于双层降低模型的深度 unfolding 网络（DASUNet），其中包括了构建双层降低模型（DDM），以便显式地模拟低光照图像的劣化机制。DDM 学习了两个不同的图像假设，通过考虑颜色和灰度空间中的降低特点来进行适应。为使提议方案可行，我们设计了一种交叉优化解决方案，并将其拓展到一个具体的深度网络中，以形成 DASUNet。results:对多个流行的低光照图像dataset进行了广泛的实验，并证明了 DASUNet 比 canon 状态的低光照图像恢复方法更有效。我们将源代码和预训练模型公开发布。<details>
<summary>Abstract</summary>
Although low-light image enhancement has achieved great stride based on deep enhancement models, most of them mainly stress on enhancement performance via an elaborated black-box network and rarely explore the physical significance of enhancement models. Towards this issue, we propose a Dual degrAdation-inSpired deep Unfolding network, termed DASUNet, for low-light image enhancement. Specifically, we construct a dual degradation model (DDM) to explicitly simulate the deterioration mechanism of low-light images. It learns two distinct image priors via considering degradation specificity between luminance and chrominance spaces. To make the proposed scheme tractable, we design an alternating optimization solution to solve the proposed DDM. Further, the designed solution is unfolded into a specified deep network, imitating the iteration updating rules, to form DASUNet. Local and long-range information are obtained by prior modeling module (PMM), inheriting the advantages of convolution and Transformer, to enhance the representation capability of dual degradation priors. Additionally, a space aggregation module (SAM) is presented to boost the interaction of two degradation models. Extensive experiments on multiple popular low-light image datasets validate the effectiveness of DASUNet compared to canonical state-of-the-art low-light image enhancement methods. Our source code and pretrained model will be publicly available.
</details>
<details>
<summary>摘要</summary>
尽管深度修剪模型在低光照图像改善方面已经取得了很大的进步，但大多数模型都强调了修剪性能的提高，而很少探讨修剪模型的物理意义。为了解决这个问题，我们提出了一种名为DASUNet的深度 unfolding 网络，用于低光照图像改善。具体来说，我们构建了一个双层降低模型（DDM），以显式地模拟低光照图像的衰化机制。DDM 学习了两个不同的图像假设，通过考虑颜色和灰度空间之间的特定降低特征来进行适应。为了使我们的方案可行，我们设计了一种交叉优化解决方案，并将其 unfolding 成一个具体的深度网络，以形成DASUNet。PMM 模块（假设模型）在维护本地和长距离信息的同时，继承了 convolution 和 Transformer 的优点，以提高修剪两个假设的表示能力。此外，我们还提出了一种空间聚合模块（SAM），以提高两个降低模型之间的交互。我们在多个流行的低光照图像数据集上进行了广泛的实验，并证明了 DASUNet 的效果比 canonical 状态的低光照图像修剪方法更高。我们将源代码和预训练模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="Incorporation-of-Eye-Tracking-and-Gaze-Feedback-to-Characterize-and-Improve-Radiologist-Search-Patterns-of-Chest-X-rays-A-Randomized-Controlled-Clinical-Trial"><a href="#Incorporation-of-Eye-Tracking-and-Gaze-Feedback-to-Characterize-and-Improve-Radiologist-Search-Patterns-of-Chest-X-rays-A-Randomized-Controlled-Clinical-Trial" class="headerlink" title="Incorporation of Eye-Tracking and Gaze Feedback to Characterize and Improve Radiologist Search Patterns of Chest X-rays: A Randomized Controlled Clinical Trial"></a>Incorporation of Eye-Tracking and Gaze Feedback to Characterize and Improve Radiologist Search Patterns of Chest X-rays: A Randomized Controlled Clinical Trial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06280">http://arxiv.org/abs/2308.06280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carolina Ramirez-Tamayo, Syed Hasib Akhter Faruqui, Stanford Martinez, Angel Brisco, Nicholas Czarnek, Adel Alaeddini, Jeffrey R. Mock, Edward J. Golob, Kal L. Clark</li>
<li>for:  This study aimed to improve the accuracy of radiologists in detecting suspicious pulmonary nodules.</li>
<li>methods: The study used eye-tracking technology to analyze radiologists’ search patterns and provided automated feedback to the intervention group.</li>
<li>results: The intervention group showed a 38.89% absolute improvement in detecting suspicious-for-cancer nodules compared to the control group, with improvement observed in all four training sessions.Here’s the text in Simplified Chinese:</li>
<li>for: 这项研究旨在提高胸部肿瘤检测中的医生准确率。</li>
<li>methods: 该研究使用眼动跟踪技术分析医生的搜寻模式，并对参与实验组提供自动反馈。</li>
<li>results: 参与实验组对可疑肿瘤的检测精度有38.89%的绝对提升，比控制组的改善（5.56%）显著（p值&#x3D;0.006），并在四个训练会议中持续改善（p值&#x3D;0.0001）。<details>
<summary>Abstract</summary>
Diagnostic errors in radiology often occur due to incomplete visual assessments by radiologists, despite their knowledge of predicting disease classes. This insufficiency is possibly linked to the absence of required training in search patterns. Additionally, radiologists lack consistent feedback on their visual search patterns, relying on ad-hoc strategies and peer input to minimize errors and enhance efficiency, leading to suboptimal patterns and potential false negatives. This study aimed to use eye-tracking technology to analyze radiologist search patterns, quantify performance using established metrics, and assess the impact of an automated feedback-driven educational framework on detection accuracy. Ten residents participated in a controlled trial focused on detecting suspicious pulmonary nodules. They were divided into an intervention group (received automated feedback) and a control group. Results showed that the intervention group exhibited a 38.89% absolute improvement in detecting suspicious-for-cancer nodules, surpassing the control group's improvement (5.56%, p-value=0.006). Improvement was more rapid over the four training sessions (p-value=0.0001). However, other metrics such as speed, search pattern heterogeneity, distractions, and coverage did not show significant changes. In conclusion, implementing an automated feedback-driven educational framework improved radiologist accuracy in detecting suspicious nodules. The study underscores the potential of such systems in enhancing diagnostic performance and reducing errors. Further research and broader implementation are needed to consolidate these promising results and develop effective training strategies for radiologists, ultimately benefiting patient outcomes.
</details>
<details>
<summary>摘要</summary>
radiologists  oftentimes make diagnostic errors due to inadequate visual assessments, despite their knowledge of predicting disease classes. This deficiency may be linked to the lack of required training in search patterns. Additionally, radiologists lack consistent feedback on their visual search patterns, relying on ad-hoc strategies and peer input to minimize errors and enhance efficiency, leading to suboptimal patterns and potential false negatives. This study aimed to use eye-tracking technology to analyze radiologist search patterns, quantify performance using established metrics, and assess the impact of an automated feedback-driven educational framework on detection accuracy. Ten residents participated in a controlled trial focused on detecting suspicious pulmonary nodules. They were divided into an intervention group (received automated feedback) and a control group. Results showed that the intervention group exhibited a 38.89% absolute improvement in detecting suspicious-for-cancer nodules, surpassing the control group's improvement (5.56%, p-value=0.006). Improvement was more rapid over the four training sessions (p-value=0.0001). However, other metrics such as speed, search pattern heterogeneity, distractions, and coverage did not show significant changes. In conclusion, implementing an automated feedback-driven educational framework improved radiologist accuracy in detecting suspicious nodules. The study underscores the potential of such systems in enhancing diagnostic performance and reducing errors. Further research and broader implementation are needed to consolidate these promising results and develop effective training strategies for radiologists, ultimately benefiting patient outcomes.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/05/eess.IV_2023_08_05/" data-id="clltau95700dfcr884j5430nf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/cs.LG_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/04/cs.LG_2023_08_04/">cs.LG - 2023-08-04 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-Cell-Tracking-with-a-Time-Symmetric-Deep-Learning-Approach"><a href="#Enhancing-Cell-Tracking-with-a-Time-Symmetric-Deep-Learning-Approach" class="headerlink" title="Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach"></a>Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03887">http://arxiv.org/abs/2308.03887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gergely Szabó, Paolo Bonaiuti, Andrea Ciliberto, András Horváth</li>
<li>for: 生物学实验中跟踪细胞的动态行为</li>
<li>methods: 基于深度学习的方法，不受连续帧的限制，仅基于细胞的空间时间邻域</li>
<li>results: 能够处理大量视频帧，并且可以学习细胞的运动模式无需任何先前假设<details>
<summary>Abstract</summary>
The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated through multiple biologically motivated validation strategies and compared against several state-of-the-art cell tracking methods.
</details>
<details>
<summary>摘要</summary>
live cells 的准确跟踪使用视频微scopy记录仍然是流行的state-of-the-art image processing基于对象跟踪方法中的挑战。在过去几年中，一些现有的和新的应用程序尝试 integrating deep learning基础框架来实现这项任务，但大多数它们仍然受限于连续帧基础或其他前提，这会阻碍总体学习。为解决这个问题，我们努力开发了一种新的深度学习基础的跟踪方法，这种方法假设Cells可以根据其空间-时间 neighborhood来跟踪，不需要 consecutive frame。这种方法还有一个利点，即 predictor 可以通过完全学习 cells 的运动模式而不需要任何先前假设，并且可以处理大量视频帧。我们通过多种生物学上驱动的验证方法来证明方法的效果，并与一些state-of-the-art cell tracking方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Learning-Optimal-Admission-Control-in-Partially-Observable-Queueing-Networks"><a href="#Learning-Optimal-Admission-Control-in-Partially-Observable-Queueing-Networks" class="headerlink" title="Learning Optimal Admission Control in Partially Observable Queueing Networks"></a>Learning Optimal Admission Control in Partially Observable Queueing Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02391">http://arxiv.org/abs/2308.02391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonatha Anselmi, Bruno Gaujal, Louis-Sébastien Rebuffi</li>
<li>for: 这个论文目的是学习一种有效的权限控制策略，以优化在部分可见的队列网络中的排队控制。</li>
<li>methods: 这个论文使用了一种基于POMDP的强化学习算法，并且使用了Norton的等价定理和生成-死亡过程的有效强化学习算法来实现。</li>
<li>results: 这个论文得到了一个只依赖于最大任务数 $S$ 的减少，而不是依赖于任务系统的径长，从而实现了高效的排队控制。<details>
<summary>Abstract</summary>
We present an efficient reinforcement learning algorithm that learns the optimal admission control policy in a partially observable queueing network. Specifically, only the arrival and departure times from the network are observable, and optimality refers to the average holding/rejection cost in infinite horizon.   While reinforcement learning in Partially Observable Markov Decision Processes (POMDP) is prohibitively expensive in general, we show that our algorithm has a regret that only depends sub-linearly on the maximal number of jobs in the network, $S$. In particular, in contrast with existing regret analyses, our regret bound does not depend on the diameter of the underlying Markov Decision Process (MDP), which in most queueing systems is at least exponential in $S$.   The novelty of our approach is to leverage Norton's equivalent theorem for closed product-form queueing networks and an efficient reinforcement learning algorithm for MDPs with the structure of birth-and-death processes.
</details>
<details>
<summary>摘要</summary>
我们提出了一个高效的增强学习算法，用于在部分可观察queueing网络中找到最佳接受控制策略。具体来说，只有网络的到达和离开时间可观察，并且将 Optimal 定义为无限时间平均保持/拒绝成本。而在部分可观察Markov决策过程（POMDP）中，增强学习通常是不可能高效的，但我们显示我们的算法仅对最大作业数量($S$)有较低的干扰。具体来说，我们的干扰 bound 不过依赖 Underlying Markov Decision Process（MDP）的尺度，这在大多数队列系统中是至少对数尺度的 $S$。我们的新的方法是利用Norton的等效定理，以及高效的增强学习算法 для birth-and-death 过程结构的 MDP。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Survival-Analysis-in-Healthcare-with-Federated-Survival-Forests-A-Comparative-Study-on-Heart-Failure-and-Breast-Cancer-Genomics"><a href="#Scaling-Survival-Analysis-in-Healthcare-with-Federated-Survival-Forests-A-Comparative-Study-on-Heart-Failure-and-Breast-Cancer-Genomics" class="headerlink" title="Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics"></a>Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02382">http://arxiv.org/abs/2308.02382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Archetti, Francesca Ieva, Matteo Matteucci</li>
<li>for: This paper is written for the purpose of developing a federated learning algorithm for survival analysis, called FedSurF++, which can handle incomplete, censored, and distributed survival data while preserving user privacy.</li>
<li>methods: The FedSurF++ algorithm uses a federated ensemble method that constructs random survival forests in heterogeneous federations, and investigates several new tree sampling methods from client forests.</li>
<li>results: The paper shows that FedSurF++ achieves comparable performance to existing methods while requiring only a single communication round to complete, and presents results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private.<details>
<summary>Abstract</summary>
Survival analysis is a fundamental tool in medicine, modeling the time until an event of interest occurs in a population. However, in real-world applications, survival data are often incomplete, censored, distributed, and confidential, especially in healthcare settings where privacy is critical. The scarcity of data can severely limit the scalability of survival models to distributed applications that rely on large data pools. Federated learning is a promising technique that enables machine learning models to be trained on multiple datasets without compromising user privacy, making it particularly well-suited for addressing the challenges of survival data and large-scale survival applications. Despite significant developments in federated learning for classification and regression, many directions remain unexplored in the context of survival analysis. In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method constructs random survival forests in heterogeneous federations. Specifically, we investigate several new tree sampling methods from client forests and compare the results with state-of-the-art survival models based on neural networks. The key advantage of FedSurF++ is its ability to achieve comparable performance to existing methods while requiring only a single communication round to complete. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private. We also present results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. Our results underscore the potential of FedSurF++ to improve the scalability and effectiveness of survival analysis in distributed settings while preserving user privacy.
</details>
<details>
<summary>摘要</summary>
生存分析是医学中的基本工具，用于模型 populate 中的事件发生的时间。然而，在实际应用中，生存数据通常是不完整、审核、分布和保密的，特别是在医疗设置中，隐私是非常重要。数据的稀缺性可能会使生存模型在分布式应用中的扩展性受到严重限制。联邦学习是一种有 Promise 的技术，它允许机器学习模型在多个数据集上进行训练，而不需要违反用户隐私。因此，它在生存数据和大规模生存应用中具有潜在的优势。 despite ， many  direction  in the context of survival analysis remains unexplored in federated learning.In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method constructs random survival forests in heterogeneous federations. Specifically, we investigate several new tree sampling methods from client forests and compare the results with state-of-the-art survival models based on neural networks. The key advantage of FedSurF++ is its ability to achieve comparable performance to existing methods while requiring only a single communication round to complete. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private. We also present results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. Our results underscore the potential of FedSurF++ to improve the scalability and effectiveness of survival analysis in distributed settings while preserving user privacy.
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Web-and-Knowledge-Graphs-for-Automated-Impact-Investing-Scoring"><a href="#Harnessing-the-Web-and-Knowledge-Graphs-for-Automated-Impact-Investing-Scoring" class="headerlink" title="Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring"></a>Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02622">http://arxiv.org/abs/2308.02622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingzhi Hu, Daniel Daza, Laurens Swinkels, Kristina Ūsaitė, Robbert-Jan ‘t Hoen, Paul Groth</li>
<li>for: 该研究旨在自动化SDG框架创建过程，提高了SDG批判和分析的效率和准确性。</li>
<li>methods: 该研究提出了一种数据驱动的方法，包括收集和筛选不同网络源和知识图文本数据，然后使用这些数据进行分类，预测公司与SDG的对应度。</li>
<li>results: 实验结果显示，该模型可以准确预测SDG分数，微平均F1分数为0.89，证明该方案的有效性。此外，该研究还提出了一种可以让人类使用的模型解释方法，以便更好地理解和使用模型预测结果。<details>
<summary>Abstract</summary>
The Sustainable Development Goals (SDGs) were introduced by the United Nations in order to encourage policies and activities that help guarantee human prosperity and sustainability. SDG frameworks produced in the finance industry are designed to provide scores that indicate how well a company aligns with each of the 17 SDGs. This scoring enables a consistent assessment of investments that have the potential of building an inclusive and sustainable economy. As a result of the high quality and reliability required by such frameworks, the process of creating and maintaining them is time-consuming and requires extensive domain expertise. In this work, we describe a data-driven system that seeks to automate the process of creating an SDG framework. First, we propose a novel method for collecting and filtering a dataset of texts from different web sources and a knowledge graph relevant to a set of companies. We then implement and deploy classifiers trained with this data for predicting scores of alignment with SDGs for a given company. Our results indicate that our best performing model can accurately predict SDG scores with a micro average F1 score of 0.89, demonstrating the effectiveness of the proposed solution. We further describe how the integration of the models for its use by humans can be facilitated by providing explanations in the form of data relevant to a predicted score. We find that our proposed solution enables access to a large amount of information that analysts would normally not be able to process, resulting in an accurate prediction of SDG scores at a fraction of the cost.
</details>
<details>
<summary>摘要</summary>
《可持续发展目标（SDG）》由联合国引入，以促进政策和活动，确保人类发展和可持续。 SDG 框架在金融业中生成，用于提供对每个 SDG 的分数，以衡量公司是否与它们相align。这些分数允许对投资进行一致的评估，以建立包容和可持续的经济。由于需要高质量和可靠性，创建和维护 SDG 框架的过程是时间consuming 和需要广泛领域专业知识。在这种情况下，我们描述了一个数据驱动的系统，用于自动化 SDG 框架的创建过程。我们首先提出了一种新的方法，收集和筛选来自不同网络源和知识图库相关的公司文本数据集。然后，我们实施和部署基于这些数据的分类器，以预测公司与 SDG 的Alignment 分数。我们的结果表明，我们的最佳表现模型可以准确预测 SDG 分数，μicro 平均 F1 分数为 0.89，证明了我们的解决方案的有效性。我们还描述了如何将模型与人类使用者集成，通过提供预测分数的数据可视化来提供解释。我们发现，我们的提议的解决方案可以访问大量信息， analysts  normally 不能处理，并在较低的成本下实现准确的 SDG 分数预测。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Method-for-Predicting-Traffic-Signal-Timing-from-Probe-Vehicle-Data"><a href="#A-Machine-Learning-Method-for-Predicting-Traffic-Signal-Timing-from-Probe-Vehicle-Data" class="headerlink" title="A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data"></a>A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02370">http://arxiv.org/abs/2308.02370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juliette Ugirumurera, Joseph Severino, Erik A. Bensen, Qichao Wang, Jane Macfarlane</li>
<li>for: 本研究使用机器学习技术来估算交通信号时间信息从车辆探测数据中。</li>
<li>methods: 我们使用极限梯度提升（XGBoost）模型来估算信号周期长度，并使用神经网络模型来确定每个阶段的红灯时间。</li>
<li>results: 我们的结果显示估算周期长度的错误在0.56秒之间，而红灯时间预测的平均错误为7.2秒。<details>
<summary>Abstract</summary>
Traffic signals play an important role in transportation by enabling traffic flow management, and ensuring safety at intersections. In addition, knowing the traffic signal phase and timing data can allow optimal vehicle routing for time and energy efficiency, eco-driving, and the accurate simulation of signalized road networks. In this paper, we present a machine learning (ML) method for estimating traffic signal timing information from vehicle probe data. To the authors best knowledge, very few works have presented ML techniques for determining traffic signal timing parameters from vehicle probe data. In this work, we develop an Extreme Gradient Boosting (XGBoost) model to estimate signal cycle lengths and a neural network model to determine the corresponding red times per phase from probe data. The green times are then be derived from the cycle length and red times. Our results show an error of less than 0.56 sec for cycle length, and red times predictions within 7.2 sec error on average.
</details>
<details>
<summary>摘要</summary>
交通信号机制环境中，交通信号控制对交通流控制和安全性具有重要作用。此外，了解交通信号阶段和时间数据可以帮助车辆进行优化的路径规划，以提高时间和能源效率，eco- driving，以及准确地模拟信号化道路网络。本文提出了一种机器学习（ML）方法，用于从车辆探测数据中估算交通信号时间信息。作者知道的研究 Works 中，很少有使用机器学习技术来确定交通信号时间参数从车辆探测数据。本文开发了极大幂boosting（XGBoost）模型来估算信号阶段长度，并使用神经网络模型来确定每个阶段的红灯时间。绿灯时间则可以从阶段长度和红灯时间中 derivation。我们的结果显示，ecycle length 的预测错误在0.56秒左右，而红灯时间的预测错误在7.2秒左右。
</details></li>
</ul>
<hr>
<h2 id="Color-Image-Recovery-Using-Generalized-Matrix-Completion-over-Higher-Order-Finite-Dimensional-Algebra"><a href="#Color-Image-Recovery-Using-Generalized-Matrix-Completion-over-Higher-Order-Finite-Dimensional-Algebra" class="headerlink" title="Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra"></a>Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02621">http://arxiv.org/abs/2308.02621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Liao, Zhuang Guo, Qi Gao, Yan Wang, Fajun Yu, Qifeng Zhao, Stephen Johh Maybank</li>
<li>for: 填充颜色图像中缺失数据的高精度图像完成</li>
<li>methods: 基于扩展的高阶矩阵模型，包括像素 neighborgood 扩展策略来描述地方像素约束</li>
<li>results: 对于各种算法进行了广泛的实验，并与公共可用的图像进行了比较，结果显示，我们的扩展矩阵完成模型和相应的算法与其低阶矩阵和传统矩阵对手相比，性能很高。<details>
<summary>Abstract</summary>
To improve the accuracy of color image completion with missing entries, we present a recovery method based on generalized higher-order scalars. We extend the traditional second-order matrix model to a more comprehensive higher-order matrix equivalent, called the "t-matrix" model, which incorporates a pixel neighborhood expansion strategy to characterize the local pixel constraints. This "t-matrix" model is then used to extend some commonly used matrix and tensor completion algorithms to their higher-order versions. We perform extensive experiments on various algorithms using simulated data and algorithms on simulated data and publicly available images and compare their performance. The results show that our generalized matrix completion model and the corresponding algorithm compare favorably with their lower-order tensor and conventional matrix counterparts.
</details>
<details>
<summary>摘要</summary>
为提高颜色图像完成缺失项的准确性，我们提出一种基于泛化高阶约束的恢复方法。我们将传统的第二阶矩阵模型扩展到更加全面的高阶矩阵等价物，称之为“t-矩阵”模型，该模型通过描述当地像素约束的像素邻域扩展策略。这个“t-矩阵”模型后来用于扩展一些通常使用的矩阵和张量完成算法到其高阶版本。我们在各种算法上进行了广泛的实验，使用模拟数据和公共可用的图像，并比较了其性能。结果显示，我们的泛化约束模型和相应的算法与其低阶张量和传统矩阵counterparts相比，表现良好。
</details></li>
</ul>
<hr>
<h2 id="Intensity-free-Integral-based-Learning-of-Marked-Temporal-Point-Processes"><a href="#Intensity-free-Integral-based-Learning-of-Marked-Temporal-Point-Processes" class="headerlink" title="Intensity-free Integral-based Learning of Marked Temporal Point Processes"></a>Intensity-free Integral-based Learning of Marked Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02360">http://arxiv.org/abs/2308.02360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stepinsilence/ifib">https://github.com/stepinsilence/ifib</a></li>
<li>paper_authors: Sishun Liu, Ke Deng, Xiuzhen Zhang, Yongli Ren</li>
<li>for: 这个论文的目的是为了开发一个高精度的数值点事件模型，以应对发生在多维数值空间中的数值事件。</li>
<li>methods: 这个论文使用了一个名为IFIB的解决方案，它是一种不使用强度函数的方法，它直接模型了条件共同PDF $p^{*}(m,t)$，并且可以实现高精度的数值点事件模型。</li>
<li>results: 这个论文的实验结果显示，IFIB可以实现高精度的数值点事件模型，并且在实际应用中具有较好的性能。另外，这个论文还提供了一个可用的代码库，供其他研究者使用。<details>
<summary>Abstract</summary>
In the marked temporal point processes (MTPP), a core problem is to parameterize the conditional joint PDF (probability distribution function) $p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history. The majority of existing studies predefine intensity functions. Their utility is challenged by specifying the intensity function's proper form, which is critical to balance expressiveness and processing efficiency. Recently, there are studies moving away from predefining the intensity function -- one models $p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal point processes (TPPs), which do not consider marks. This study aims to develop high-fidelity $p^*(m,t)$ for discrete events where the event marks are either categorical or numeric in a multi-dimensional continuous space. We propose a solution framework IFIB (\underline{I}ntensity-\underline{f}ree \underline{I}ntegral-\underline{b}ased process) that models conditional joint PDF $p^*(m,t)$ directly without intensity functions. It remarkably simplifies the process to compel the essential mathematical restrictions. We show the desired properties of IFIB and the superior experimental results of IFIB on real-world and synthetic datasets. The code is available at \url{https://github.com/StepinSilence/IFIB}.
</details>
<details>
<summary>摘要</summary>
在标记时间点过程中（MTPP），核心问题是参数化 conditional joint PDF（概率分布函数）$p^*(m,t)$， conditioned on the history，其中 $m$ 表示事件标记， $t$ 表示事件间隔时间。大多数现有研究都是先定义INTENSITY函数。然而，这些INTENSITY函数的合适形式是关键，需要平衡表达能力和处理效率。在最近几年，有一些研究尝试离开先定义INTENSITY函数，其中一种是将 $p^*(t)$ 和 $p^*(m)$ 分别模型，另一种是关注时间点过程（TPP），不考虑标记。本研究旨在开发高精度的 $p^*(m,t)$  для精确的事件时间点，其中事件标记可以是 categorical 或 numeric，并且在多维连续空间中。我们提出了一种解决方案框架 IFIB（INTENSITY-free INTEGRAL-based process），它直接模型 conditional joint PDF $p^*(m,t)$ 无需INTENSITY函数。这有效简化了过程，使得mathematical restrictions强制性地减少。我们显示了 IFIB 的愿望性质和实验结果，并提供了实验结果。代码可以在 <https://github.com/StepinSilence/IFIB> 上获取。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-for-GTFS-From-Words-to-Information"><a href="#ChatGPT-for-GTFS-From-Words-to-Information" class="headerlink" title="ChatGPT for GTFS: From Words to Information"></a>ChatGPT for GTFS: From Words to Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02618">http://arxiv.org/abs/2308.02618</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/utel-uiuc/gtfs_llm">https://github.com/utel-uiuc/gtfs_llm</a></li>
<li>paper_authors: Saipraneeth Devunuri, Shirin Qiam, Lewis Lehe</li>
<li>For: The paper aims to explore the ability of large language models (LLMs) to retrieve information from the General Transit Feed Specification (GTFS) using natural language instructions.* Methods: The paper uses the ChatGPT model (GPT-3.5) to test its understanding of the GTFS specification and to perform information extraction from a filtered GTFS feed with 4 routes. The paper compares zero-shot and program synthesis methods for information retrieval.* Results: The paper finds that program synthesis achieves higher accuracy (~90% for simple questions and ~40% for complex questions) than zero-shot methods for information retrieval from GTFS using natural language instructions.<details>
<summary>Abstract</summary>
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
</details>
<details>
<summary>摘要</summary>
通用交通Feed规范（GTFS）是公共交通数据的发布标准，这种标准是表格数据，信息分布在不同文件中，因此需要专门的工具或包装来获取信息。同时，使用大型自然语言模型（LLM）来检索文本和信息的使用也在增长。本研究的想法是看看目前广泛采用的LLM（ChatGPT）能否使用自然语言指令来从GTFS中提取信息。我们首先测试了GPT-3.5是否理解GTFS规范。GPT-3.5回答了我们的多项选择题（MCQ）77% correctly。接下来，我们让LLM从过滤后的GTFS feed中提取信息。为了获取信息，我们比较了零shot和程序合成。程序合成更好，实现了~90%的简单问题的准确率和~40%的复杂问题的准确率。
</details></li>
</ul>
<hr>
<h2 id="Multi-attacks-Many-images-the-same-adversarial-attack-to-many-target-labels"><a href="#Multi-attacks-Many-images-the-same-adversarial-attack-to-many-target-labels" class="headerlink" title="Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels"></a>Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03792">http://arxiv.org/abs/2308.03792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanislavfort/multi-attacks">https://github.com/stanislavfort/multi-attacks</a></li>
<li>paper_authors: Stanislav Fort</li>
<li>for: 这个论文旨在描述一种可以同时对多个图像进行攻击的攻击方法。</li>
<li>methods: 这个论文使用了一种称为”多重攻击”的方法，可以对多个图像进行攻击，并且可以在不同的目标类上进行攻击。</li>
<li>results: 论文表明，使用这种多重攻击方法可以对数百个图像进行攻击，并且可以在不同的图像和目标类上进行攻击。此外，论文还发现了一些相关的结果，如图像的高信任区域数量是$\mathcal{O}(100)$以上，这会带来一些问题 для防御策略。<details>
<summary>Abstract</summary>
We show that we can easily design a single adversarial perturbation $P$ that changes the class of $n$ images $X_1,X_2,\dots,X_n$ from their original, unperturbed classes $c_1, c_2,\dots,c_n$ to desired (not necessarily all the same) classes $c^*_1,c^*_2,\dots,c^*_n$ for up to hundreds of images and target classes at once. We call these \textit{multi-attacks}. Characterizing the maximum $n$ we can achieve under different conditions such as image resolution, we estimate the number of regions of high class confidence around a particular image in the space of pixels to be around $10^{\mathcal{O}(100)}$, posing a significant problem for exhaustive defense strategies. We show several immediate consequences of this: adversarial attacks that change the resulting class based on their intensity, and scale-independent adversarial examples. To demonstrate the redundancy and richness of class decision boundaries in the pixel space, we look for its two-dimensional sections that trace images and spell words using particular classes. We also show that ensembling reduces susceptibility to multi-attacks, and that classifiers trained on random labels are more susceptible. Our code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
我们显示出可以轻松设计一个单一敌对偏移$P$，使$n$个图像$X_1,X_2,\dots,X_n$的原始、未偏变的类别变更为Target类别$c_1,c_2,\dots,c_n$的欲要（可能不是所有的类别都是一样的）类别$c^*_1,c^*_2,\dots,c^*_n$，称之为“多元攻击”。我们估计在不同的图像分辨率下，可以达到大量的$n$，并且考虑到像素空间中高度信任类别的区域数量约为$10^{\mathcal{O}(100)}$，这会对对抗策略造成严重的问题。我们显示了一些立即的后果：对于图像的数量和Target类别的变化，以及对于图像的缩放和转换的类别攻击。为了证明像素空间中类别决策boundary的丰富和紧张，我们寻找了图像和字串之间的二维部分，并证明了折衣组合可以对抗多元攻击，而且随机 labels 训练的分类器更加易受到攻击。我们的代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Adapting-to-Change-Robust-Counterfactual-Explanations-in-Dynamic-Data-Landscapes"><a href="#Adapting-to-Change-Robust-Counterfactual-Explanations-in-Dynamic-Data-Landscapes" class="headerlink" title="Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes"></a>Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02353">http://arxiv.org/abs/2308.02353</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bardhprenkaj/hansel">https://github.com/bardhprenkaj/hansel</a></li>
<li>paper_authors: Bardh Prenkaj, Mario Villaizan-Vallelado, Tobias Leemann, Gjergji Kasneci</li>
<li>for: This paper presents a novel semi-supervised method for counterfactual explanation generation, called Dynamic GRAph Counterfactual Explainer (DyGRACE).</li>
<li>methods: DyGRACE uses two graph autoencoders (GAEs) to learn the representation of each class in a binary classification scenario, and optimizes a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximizing the factual autoencoder’s reconstruction error.</li>
<li>results: DyGRACE is effective in identifying counterfactuals and can act as a drift detector, identifying distributional drift based on differences in reconstruction errors between iterations. It avoids reliance on the oracle’s predictions in successive iterations, thereby increasing the efficiency of counterfactual discovery.<details>
<summary>Abstract</summary>
We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE) methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leverages initial knowledge about the data distribution to search for valid counterfactuals while avoiding using information from potentially outdated decision functions in subsequent time steps. Employing two graph autoencoders (GAEs), DyGRACE learns the representation of each class in a binary classification scenario. The GAEs minimise the reconstruction error between the original graph and its learned representation during training. The method involves (i) optimising a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximising the factual autoencoder's reconstruction error, (ii) minimising the counterfactual autoencoder's error, and (iii) maximising the similarity between the factual and counterfactual graphs. This semi-supervised approach is independent of an underlying black-box oracle. A logistic regression model is trained on a set of graph pairs to learn weights that aid in finding counterfactuals. At inference, for each unseen graph, the logistic regressor identifies the best counterfactual candidate using these learned weights, while the GAEs can be iteratively updated to represent the continual adaptation of the learned graph representation over iterations. DyGRACE is quite effective and can act as a drift detector, identifying distributional drift based on differences in reconstruction errors between iterations. It avoids reliance on the oracle's predictions in successive iterations, thereby increasing the efficiency of counterfactual discovery. DyGRACE, with its capacity for contrastive learning and drift detection, will offer new avenues for semi-supervised learning and explanation generation.
</details>
<details>
<summary>摘要</summary>
我们介绍一种新的半监督式グラフカウンターファクタルエクスプレイナー（GCE）方法，即动态GRAPHカウンターファクタルエクスプレイナー（DyGRACE）。它利用初始知识来搜寻有效的假设，而不需要在后续时间步骤中使用可能已过时的决策函数。使用两个图自动生成器（GAE），DyGRACE学习图像中的每个类别表现。在训练过程中，GAE将图像和其学习的表现之间的差异降到最小。方法包括：(i) 优化一个 Parametric Density Function（实现为逻辑回归函数），以确定假设，最大化实际自动生成器的重建错误。(ii) 降低假设自动生成器的错误。(iii) 将实际和假设图像之间的相似度最大化。这个半监督式方法不需要背景黑盒模型，可以独立进行假设搜寻。在推断过程中，一个逻辑回归模型将被训练，以学习对图像对的权重，并且在每次推断过程中选择最佳的假设候选者。在迭代过程中，GAEs可以逐步更新，以反映适应学习的图像表现。DyGRACE能够实现对照学习和解释生成，并且可以检测分布迁移，根据不同的重建错误值进行推断。这些特点使得DyGRACE能够提高假设搜寻的效率，并且具有跨时间的内存和适应能力。
</details></li>
</ul>
<hr>
<h2 id="RobustMQ-Benchmarking-Robustness-of-Quantized-Models"><a href="#RobustMQ-Benchmarking-Robustness-of-Quantized-Models" class="headerlink" title="RobustMQ: Benchmarking Robustness of Quantized Models"></a>RobustMQ: Benchmarking Robustness of Quantized Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02350">http://arxiv.org/abs/2308.02350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang Guo, Xianglong Liu</li>
<li>for: 评估量化神经网络模型的可靠性和抗噪性。</li>
<li>methods: 使用了多种噪音（攻击性噪音、自然噪音和系统噪音）对量化神经网络模型进行了全面的评估。</li>
<li>results: 研究发现，量化模型对攻击性噪音具有更高的抗噪性，但对自然噪音和系统噪音更容易受损。增加量化比特宽度会导致对攻击性噪音的抗噪性下降，对自然噪音和系统噪音的抗噪性增加。等类型的噪音对量化模型的影响不同。<details>
<summary>Abstract</summary>
Quantization has emerged as an essential technique for deploying deep neural networks (DNNs) on devices with limited resources. However, quantized models exhibit vulnerabilities when exposed to various noises in real-world applications. Despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. The comprehensive evaluation results empirically provide valuable insights into the robustness of quantized models in various scenarios, for example: (1) quantized models exhibit higher adversarial robustness than their floating-point counterparts, but are more vulnerable to natural corruptions and systematic noises; (2) in general, increasing the quantization bit-width results in a decrease in adversarial robustness, an increase in natural robustness, and an increase in systematic robustness; (3) among corruption methods, \textit{impulse noise} and \textit{glass blur} are the most harmful to quantized models, while \textit{brightness} has the least impact; (4) among systematic noises, the \textit{nearest neighbor interpolation} has the highest impact, while bilinear interpolation, cubic interpolation, and area interpolation are the three least harmful. Our research contributes to advancing the robust quantization of models and their deployment in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
量化技术已成为深度神经网络（DNNs）部署在有限资源设备的重要手段。然而，量化模型在实际应用中受到各种噪声的影响，这些噪声包括攻击性噪声、自然损害和系统性噪声。despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. The comprehensive evaluation results empirically provide valuable insights into the robustness of quantized models in various scenarios, for example:1. 量化模型对攻击性噪声比浮点模型更高，但对自然损害和系统性噪声更容易受到影响。2. 在不同的量化比特宽度下，对攻击性噪声的影响随着量化比特宽度的增加而逐渐减少，对自然损害和系统性噪声的影响则随着量化比特宽度的增加而逐渐增加。3. 对量化模型的噪声方法，抖擦噪声和玻璃噪声是最有害的两种，而亮度噪声对量化模型的影响最小。4. 对系统性噪声，最近的邻居 interpolate 是最有害的一种，而 bilinear interpolate、cubic interpolate 和 area interpolate 是最弱的三种。我们的研究为深度神经网络的可靠量化和实际应用做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Vehicles-Control-Collision-Avoidance-using-Federated-Deep-Reinforcement-Learning"><a href="#Vehicles-Control-Collision-Avoidance-using-Federated-Deep-Reinforcement-Learning" class="headerlink" title="Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning"></a>Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02614">http://arxiv.org/abs/2308.02614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Badr Ben Elallid, Amine Abouaomar, Nabil Benamar, Abdellatif Kobbane</li>
<li>for: 运输管理和安全性问题在都市化的人口增加和车辆量增加的情况下变得非常重要。这篇论文探讨了在碰撞避免方面使用智能控制系统的发展，并且利用联合深度循环学习（FDRL）技术。</li>
<li>methods: 这篇论文使用了两种模型：地方模型（DDPG）和联合模型（FDDPG），并进行了比较分析，以决定它们在碰撞避免方面的效果。</li>
<li>results: 结果显示，使用FDDPG算法可以更好地控制车辆，避免碰撞。尤其是，FDDPG-based algorithm在减少旅行延迟和提高平均速度方面表现出了明显的改善。<details>
<summary>Abstract</summary>
In the face of growing urban populations and the escalating number of vehicles on the roads, managing transportation efficiently and ensuring safety have become critical challenges. To tackle these issues, the development of intelligent control systems for vehicles is paramount. This paper presents a comprehensive study on vehicle control for collision avoidance, leveraging the power of Federated Deep Reinforcement Learning (FDRL) techniques. Our main goal is to minimize travel delays and enhance the average speed of vehicles while prioritizing safety and preserving data privacy. To accomplish this, we conducted a comparative analysis between the local model, Deep Deterministic Policy Gradient (DDPG), and the global model, Federated Deep Deterministic Policy Gradient (FDDPG), to determine their effectiveness in optimizing vehicle control for collision avoidance. The results obtained indicate that the FDDPG algorithm outperforms DDPG in terms of effectively controlling vehicles and preventing collisions. Significantly, the FDDPG-based algorithm demonstrates substantial reductions in travel delays and notable improvements in average speed compared to the DDPG algorithm.
</details>
<details>
<summary>摘要</summary>
面对城市人口增长和交通量不断增加的问题，有效地管理交通和确保安全已成为核心挑战。为此，开发智能控制系统 для车辆是极其重要的。本文通过详细的研究，探讨了采用联邦深度强化学习（FDRL）技术来控制车辆，以最小化旅行延迟和提高车辆的平均速度，同时保持数据隐私。为此，我们进行了本地模型（DDPG）和全球模型（FDDPG）的比较分析，以确定它们在避免碰撞方面的效果。结果表明，FDDPG算法在控制车辆和避免碰撞方面表现较好，并且在旅行延迟和车辆平均速度方面具有显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Neural-Networks-with-more-flexible-memory-better-predictions-than-rough-volatility"><a href="#Recurrent-Neural-Networks-with-more-flexible-memory-better-predictions-than-rough-volatility" class="headerlink" title="Recurrent Neural Networks with more flexible memory: better predictions than rough volatility"></a>Recurrent Neural Networks with more flexible memory: better predictions than rough volatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08550">http://arxiv.org/abs/2308.08550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Damien Challet, Vincent Ragel</li>
<li>for: 这篇论文旨在扩展循环神经网络，以应对具有长 памя于或高度不均匀的时间步骤。</li>
<li>methods: 这篇论文使用了多个灵活的时间尺度，以提高循环神经网络的能力，并与标准LSTM进行比较。</li>
<li>results: 相比标准LSTM，扩展LSTM需要训练更少的epoch，并且预测资产波动性的模型体系性高于20%。<details>
<summary>Abstract</summary>
We extend recurrent neural networks to include several flexible timescales for each dimension of their output, which mechanically improves their abilities to account for processes with long memory or with highly disparate time scales. We compare the ability of vanilla and extended long short term memory networks (LSTMs) to predict asset price volatility, known to have a long memory. Generally, the number of epochs needed to train extended LSTMs is divided by two, while the variation of validation and test losses among models with the same hyperparameters is much smaller. We also show that the model with the smallest validation loss systemically outperforms rough volatility predictions by about 20% when trained and tested on a dataset with multiple time series.
</details>
<details>
<summary>摘要</summary>
我们扩展回传神经网络，以包括每个输出维度的多个灵活时间尺度，以机械提高其能力处理具有长期记忆或高度不同时间尺度的过程。我们比较了净体和扩展的长期快短时间记忆网络（LSTM）的能力预测资产波动性，知道具有长期记忆。通常，训练扩展LSTM需要的轮数比vanilla LSTM要少半，并且模型之间的验证和测试损失的变化相对较小。我们还显示，具有最小验证损失的模型系统地超过了使用多个时间序列的预测波动性预测值的20%。
</details></li>
</ul>
<hr>
<h2 id="Stability-and-Generalization-of-Hypergraph-Collaborative-Networks"><a href="#Stability-and-Generalization-of-Hypergraph-Collaborative-Networks" class="headerlink" title="Stability and Generalization of Hypergraph Collaborative Networks"></a>Stability and Generalization of Hypergraph Collaborative Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02347">http://arxiv.org/abs/2308.02347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Ng, Hanrui Wu, Andy Yip</li>
<li>For: 本研究旨在确保hypergraph collaborative networks的核心层的算法稳定性，并提供一般化保证。* Methods: 本文使用hypergraph collaborative networks，并通过对它们的核心层进行分析，提供了一般化保证。* Results: 实验结果表明，通过合适地调整数据和hypergraph filters的缩放，可以实现uniform的学习过程稳定性。<details>
<summary>Abstract</summary>
Graph neural networks have been shown to be very effective in utilizing pairwise relationships across samples. Recently, there have been several successful proposals to generalize graph neural networks to hypergraph neural networks to exploit more complex relationships. In particular, the hypergraph collaborative networks yield superior results compared to other hypergraph neural networks for various semi-supervised learning tasks. The collaborative network can provide high quality vertex embeddings and hyperedge embeddings together by formulating them as a joint optimization problem and by using their consistency in reconstructing the given hypergraph. In this paper, we aim to establish the algorithmic stability of the core layer of the collaborative network and provide generalization guarantees. The analysis sheds light on the design of hypergraph filters in collaborative networks, for instance, how the data and hypergraph filters should be scaled to achieve uniform stability of the learning process. Some experimental results on real-world datasets are presented to illustrate the theory.
</details>
<details>
<summary>摘要</summary>
graph neural networks 有 shown 能够很 effectively 利用 sample 对的 pairwise 关系。 最近， 有 several successful proposals 将 graph neural networks 扩展到 hypergraph neural networks，以利用更复杂的关系。特别是， hypergraph collaborative networks 在 various semi-supervised learning tasks 中 yield superior results compared to other hypergraph neural networks。 collaborative network 可以提供 high quality vertex embeddings 和 hyperedge embeddings，通过 formulating them as a joint optimization problem 和使用 their consistency in reconstructing the given hypergraph。在这篇 paper 中，我们 aim to establish the algorithmic stability of the core layer of the collaborative network and provide generalization guarantees。analysis  shed light on the design of hypergraph filters in collaborative networks, such as how the data and hypergraph filters should be scaled to achieve uniform stability of the learning process。some experimental results on real-world datasets are presented to illustrate the theory。
</details></li>
</ul>
<hr>
<h2 id="Learning-Networks-from-Gaussian-Graphical-Models-and-Gaussian-Free-Fields"><a href="#Learning-Networks-from-Gaussian-Graphical-Models-and-Gaussian-Free-Fields" class="headerlink" title="Learning Networks from Gaussian Graphical Models and Gaussian Free Fields"></a>Learning Networks from Gaussian Graphical Models and Gaussian Free Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02344">http://arxiv.org/abs/2308.02344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhro Ghosh, Soumendu Sundar Mukherjee, Hoang-Son Tran, Ujan Gangopadhyay</li>
<li>for: 估计weighted网络的结构</li>
<li>methods: 基于重复测量Gaussian Graphical Model（GGM）的方法</li>
<li>results: 提出一种新的估计器，可以从GGM的复杂概率特性中提取有用信息，并且可以提供具体的回归保证和样本复杂度下界。特别是，在 Erdos-Renyi 随机网络上，我们证明了在样本大小 $n$ 足够大时，网络结构可以recovery With high probability.<details>
<summary>Abstract</summary>
We investigate the problem of estimating the structure of a weighted network from repeated measurements of a Gaussian Graphical Model (GGM) on the network. In this vein, we consider GGMs whose covariance structures align with the geometry of the weighted network on which they are based. Such GGMs have been of longstanding interest in statistical physics, and are referred to as the Gaussian Free Field (GFF). In recent years, they have attracted considerable interest in the machine learning and theoretical computer science. In this work, we propose a novel estimator for the weighted network (equivalently, its Laplacian) from repeated measurements of a GFF on the network, based on the Fourier analytic properties of the Gaussian distribution. In this pursuit, our approach exploits complex-valued statistics constructed from observed data, that are of interest on their own right. We demonstrate the effectiveness of our estimator with concrete recovery guarantees and bounds on the required sample complexity. In particular, we show that the proposed statistic achieves the parametric rate of estimation for fixed network size. In the setting of networks growing with sample size, our results show that for Erdos-Renyi random graphs $G(d,p)$ above the connectivity threshold, we demonstrate that network recovery takes place with high probability as soon as the sample size $n$ satisfies $n \gg d^4 \log d \cdot p^{-2}$.
</details>
<details>
<summary>摘要</summary>
我们研究如何从重复观测 Gaussian Graphical Model (GGM) 中 estimate 网络的结构。在这个意境下，我们考虑 GGM 的协调结构与网络的重量相对应。这些 GGM 在统计物理学中有很长的历史，通常被称为 Gaussian Free Field (GFF)。在最近几年中，它们在机器学习和理论计算机科学中受到了很大的关注。在这个工作中，我们提出了一个新的网络重量Estimator，基于网络上重复观测 GFF 的 Fourier分析特性。我们的方法利用观测数据中的复数统计，具有自己的科学价值。我们显示了这个统计的效果，并提供了具体的回溯保证和sample complexity bound。尤其是，我们证明了这个统计在固定网络大小下具有参数率的估计率。在探索网络规模 grow 于样本大小的情况下，我们的结果显示，当样本大小 $n$ 满足 $n \gg d^4 \log d \cdot p^{-2}$ 时，网络重建很有可能会在高可信度下发生。Note: "Simplified Chinese" is a romanization of Chinese that uses a simplified set of characters and grammar rules to represent the language. It is commonly used in mainland China and Singapore, and is one of the two official languages of the People's Republic of China.
</details></li>
</ul>
<hr>
<h2 id="RAHNet-Retrieval-Augmented-Hybrid-Network-for-Long-tailed-Graph-Classification"><a href="#RAHNet-Retrieval-Augmented-Hybrid-Network-for-Long-tailed-Graph-Classification" class="headerlink" title="RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification"></a>RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02335">http://arxiv.org/abs/2308.02335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyang Mao, Wei Ju, Yifang Qin, Xiao Luo, Ming Zhang</li>
<li>for: 提高图像分类 tasks 中的泛化能力，尤其是在长尾类分布的real-world数据中。</li>
<li>methods: 提出了一种名为 Retrieval Augmented Hybrid Network (RAHNet) 的新框架，用于同时学习一个 Robust 特征提取器和一个不偏袋化的分类器，并在特征提取器和分类器之间进行分离学习。在特征提取器训练阶段，我们开发了一个图像检索模块，用于搜索与tail类相关的图像，以直接增强tail类之间的内部多样性。在分类器细化阶段，我们使用了两种重量规regularization技术，即Max-norm和weight decay，以均衡分类器的重量。</li>
<li>results: 在各种流行的benchmark上进行了实验，并证明了我们的方法在state-of-the-artapproaches中的优越性。<details>
<summary>Abstract</summary>
Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant graphs that directly enrich the intra-class diversity for the tail classes. Moreover, we innovatively optimize a category-centered supervised contrastive loss to obtain discriminative representations, which is more suitable for long-tailed scenarios. In the classifier fine-tuning stage, we balance the classifier weights with two weight regularization techniques, i.e., Max-norm and weight decay. Experiments on various popular benchmarks verify the superiority of the proposed method against state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
“图像分类是现实世界多媒体应用中的关键任务，图像可以表示各种媒体数据类型，如图像、视频和社交网络。先前的尝试都是在平衡的情况下应用图像神经网络（GNNs），但实际数据通常会出现长尾分布，导致使用GNNs时对尾类的偏袋和有限的泛化能力。现有的方法主要集中在模型训练时重新平衡不同类别，但这会失去新知识的导入和头类的性能。为解决这些缺点，我们提出了一种新的框架，即Retrieval Augmented Hybrid Network（RAHNet），它可以同时学习一个强健的特征提取器和一个不偏袋的分类器。在特征提取器训练阶段，我们开发了一个图像检索模块，以找到适当的图像来增强尾类的内部多样性。此外，我们还创新地优化了一种类型中心的超级vised对比损失，以获得适合长尾情况的表示，”Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Interoperable-synthetic-health-data-with-SyntHIR-to-enable-the-development-of-CDSS-tools"><a href="#Interoperable-synthetic-health-data-with-SyntHIR-to-enable-the-development-of-CDSS-tools" class="headerlink" title="Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools"></a>Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02613">http://arxiv.org/abs/2308.02613</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/potter-coder89/synthir">https://github.com/potter-coder89/synthir</a></li>
<li>paper_authors: Pavitra Chauhan, Mohsen Gamal Saad Askar, Bjørn Fjukstad, Lars Ailo Bongo, Edvard Pedersen<br>for:这个论文旨在提出一种基于机器学习的临床决策支持系统（CDSS）的开发方法，使用高质量的患者日志和医疗注册来生成 synthetic EHR 数据，并在临床工作流程中实现 CDSS 工具的开发和测试。methods:这个论文使用的方法包括使用 FHIR 标准实现数据互操作性，使用 Gretel 框架生成 synthetic 数据，使用 Microsoft Azure FHIR 服务器作为基于 FHIR 的 EHR 系统，以及使用 SMART on FHIR 框架实现工具可重用性。results:论文通过开发一个基于机器学习的 CDSS 工具，使用 Norwegian Patient Register (NPR) 和 Norwegian Patient Prescriptions (NorPD) 数据进行开发，并在 SyntHIR 系统上测试和评估该工具。结果表明，SyntHIR 提供了一个通用的 CDSS 工具开发框架，可以使用 synthetic FHIR 数据进行测试和评估，并且可以在临床 setting 中实现。但是，synthetic 数据质量的问题还需要进一步改进。代码可以在 GitHub 上获取：<a target="_blank" rel="noopener" href="https://github.com/potter-coder89/SyntHIR.git%E3%80%82">https://github.com/potter-coder89/SyntHIR.git。</a><details>
<summary>Abstract</summary>
There is a great opportunity to use high-quality patient journals and health registers to develop machine learning-based Clinical Decision Support Systems (CDSS). To implement a CDSS tool in a clinical workflow, there is a need to integrate, validate and test this tool on the Electronic Health Record (EHR) systems used to store and manage patient data. However, it is often not possible to get the necessary access to an EHR system due to legal compliance. We propose an architecture for generating and using synthetic EHR data for CDSS tool development. The architecture is implemented in a system called SyntHIR. The SyntHIR system uses the Fast Healthcare Interoperability Resources (FHIR) standards for data interoperability, the Gretel framework for generating synthetic data, the Microsoft Azure FHIR server as the FHIR-based EHR system and SMART on FHIR framework for tool transportability. We demonstrate the usefulness of SyntHIR by developing a machine learning-based CDSS tool using data from the Norwegian Patient Register (NPR) and Norwegian Patient Prescriptions (NorPD). We demonstrate the development of the tool on the SyntHIR system and then lift it to the Open DIPS environment. In conclusion, SyntHIR provides a generic architecture for CDSS tool development using synthetic FHIR data and a testing environment before implementing it in a clinical setting. However, there is scope for improvement in terms of the quality of the synthetic data generated. The code is open source and available at https://github.com/potter-coder89/SyntHIR.git.
</details>
<details>
<summary>摘要</summary>
“有一大机会使用高质量的患者日记和医疗注册来开发基于机器学习的临床决策支持系统（CDSS）。为实现CDSS工具在临床工作流程中的应用，需要将这个工具与电子医疗记录（EHR）系统集成、验证和测试。然而，由于法律合规的问题，通常无法获得EHR系统的必要访问权。我们提出了一种使用生成的Synthetic EHR数据来开发CDSS工具的建筑方案。该建筑方案在一个名为SyntHIR的系统中实现，该系统使用Fast Healthcare Interoperability Resources（FHIR）标准来实现数据互操作，使用Gretel框架生成synthetic数据，使用Microsoft Azure FHIR服务器作为FHIR基于EHR系统，并使用SMART on FHIR框架来提供工具可重用性。我们通过使用挪威患者注册（NPR）和挪威药品订单（NorPD）的数据开发了一个基于机器学习的CDSS工具，并在SyntHIR系统上测试了该工具。最后，我们将工具提取到Open DIPS环境中。总之，SyntHIR提供了一个通用的CDSS工具开发基于Synthetic FHIR数据的测试环境，但是可以进一步提高生成的synthetic数据质量。代码可以在https://github.com/potter-coder89/SyntHIR.git中获取。”
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-spike-detection-in-deep-brain-stimulation-surgery"><a href="#Deep-learning-for-spike-detection-in-deep-brain-stimulation-surgery" class="headerlink" title="Deep learning for spike detection in deep brain stimulation surgery"></a>Deep learning for spike detection in deep brain stimulation surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05755">http://arxiv.org/abs/2308.05755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arkadiusz Nowacki, Ewelina Kołpa, Mateusz Szychiewicz, Konrad Ciecierski</li>
<li>for: 这个论文是为了描述一种基于深度学习的神经活动记录分析方法，用于深 bran stimulation（DBS） neurosurgery 中的 neuronal activity 识别。</li>
<li>methods: 该方法使用了一种卷积神经网络（CNN）来分析神经活动记录，并在不同的时间窗口中进行识别。</li>
<li>results: 实验结果表明，该方法可以达到最高的准确率（98.98%）和受器操作特征曲线的面积（AUC）的最高值（0.9898），而无需进行数据预处理。<details>
<summary>Abstract</summary>
Deep brain stimulation (DBS) is a neurosurgical procedure successfully used to treat conditions such as Parkinson's disease. Electrostimulation, carried out by implanting electrodes into an identified focus in the brain, makes it possible to reduce the symptoms of the disease significantly. In this paper, a method for analyzing recordings of neuronal activity acquired during DBS neurosurgery using deep learning is presented. We tested using a convolutional neural network (CNN) for this purpose. Based on the time window, the classifier assesses whether neuronal activity (spike) is present. The maximum accuracy value for the classifier was 98.98%, and the area under the receiver operating characteristic curve (AUC) was 0.9898. The method made it possible to obtain a classification without using data preprocessing.
</details>
<details>
<summary>摘要</summary>
深度脑刺激（DBS）是一种 neurosurgical 程序，已经成功地治疗了 Parkinson's disease 等疾病。通过在脑中implanting 电极，可以减轻疾病的 симптом。在这篇论文中，我们提出了使用深度学习分析 DBS  neurosurgery 中记录的 neuronal 活动的方法。我们测试了 convolutional neural network（CNN）来完成这个任务。根据时间窗口，分类器评估 neuronal 活动（脉冲）是否存在。最大准确率值为 98.98%，准确率下接收操作特征曲线（AUC）值为 0.9898。这种方法可以不使用数据预处理来获得分类。
</details></li>
</ul>
<hr>
<h2 id="A-stochastic-optimization-approach-to-train-non-linear-neural-networks-with-a-higher-order-variation-regularization"><a href="#A-stochastic-optimization-approach-to-train-non-linear-neural-networks-with-a-higher-order-variation-regularization" class="headerlink" title="A stochastic optimization approach to train non-linear neural networks with a higher-order variation regularization"></a>A stochastic optimization approach to train non-linear neural networks with a higher-order variation regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02293">http://arxiv.org/abs/2308.02293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oknakfm/hovr">https://github.com/oknakfm/hovr</a></li>
<li>paper_authors: Akifumi Okuno</li>
<li>For: This paper aims to address the issue of overfitting in highly expressive parametric models, such as deep neural networks, by introducing a new regularization term called $(k,q)$th order variation regularization ($(k,q)$-VR).* Methods: The paper proposes a stochastic optimization algorithm that can efficiently train general models with the $(k,q)$-VR term without conducting explicit numerical integration. The algorithm is based on stochastic gradient descent and automatic differentiation, and can be applied to the training of deep neural networks with arbitrary structure.* Results: The paper demonstrates that the neural networks trained with the $(k,q)$-VR terms are more “resilient” than those with the conventional parameter regularization, and the proposed algorithm can also be extended to the physics-informed training of neural networks (PINNs).<details>
<summary>Abstract</summary>
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $(k,q)$th order variation regularization ($(k,q)$-VR), which is defined as the $q$th-powered integral of the absolute $k$th order derivative of the parametric models to be trained; penalizing the $(k,q)$-VR is expected to yield a smoother function, which is expected to avoid overfitting. Particularly, $(k,q)$-VR encompasses the conventional (general-order) total variation with $q=1$. While the $(k,q)$-VR terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $(k,q)$-VR without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradient descent algorithm and automatic differentiation. Our numerical experiments demonstrate that the neural networks trained with the $(k,q)$-VR terms are more ``resilient'' than those with the conventional parameter regularization. The proposed algorithm also can be extended to the physics-informed training of neural networks (PINNs).
</details>
<details>
<summary>摘要</summary>
“而高度表达力的 parametric 模型，如深度神经网络，具有模型复杂概念的优势。然而，训练这些非线性模型时存在高风险的过拟合。为解决这个问题，本研究考虑了 $(k,q)$ 项变化规则（$(k,q)$-VR），即将要训练的 parametric 模型的 $q$ 阶幂化积分 absolute $k$ 阶差分。penalizing $(k,q)$-VR 会导致更平滑的函数，以避免过拟合。特别是，$(k,q)$-VR 包括普通（总阶）变量的 $q=1$。而 $(k,q)$-VR 应用于普通 parametric 模型时 computationally intractable due to integration，本研究提供了一种可efficiently 训练通用模型的随机优化算法。这种方法可以应用于深度神经网络的训练，并且可以通过简单的随机梯度下降算法和自动导数来实现。我们的numerical experiments表明，使用 $(k,q)$-VR 训练的神经网络比使用传统参数正则化更为“坚固”。此外，这种算法还可以扩展到物理学信息训练神经网络（PINNs）。”
</details></li>
</ul>
<hr>
<h2 id="Frustratingly-Easy-Model-Generalization-by-Dummy-Risk-Minimization"><a href="#Frustratingly-Easy-Model-Generalization-by-Dummy-Risk-Minimization" class="headerlink" title="Frustratingly Easy Model Generalization by Dummy Risk Minimization"></a>Frustratingly Easy Model Generalization by Dummy Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02287">http://arxiv.org/abs/2308.02287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juncheng Wang, Jindong Wang, Xixu Hu, Shujun Wang, Xing Xie</li>
<li>for: 提高机器学习模型的泛化能力</li>
<li>methods: 使用拟合风险最小化（Dummy Risk Minimization，DuRM）技术，即通过扩大输出логи特征来提高模型的泛化能力</li>
<li>results: DuRM可以在多个任务上提高表现，包括传统的分类、Semantic segmentation、out-of-distribution泛化、对抗训练和长尾识别等，并且可以与现有的泛化技术相结合使用。<details>
<summary>Abstract</summary>
Empirical risk minimization (ERM) is a fundamental machine learning paradigm. However, its generalization ability is limited in various tasks. In this paper, we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general technique to improve the generalization of ERM. DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent. Moreover, we validate the efficacy of DuRM on both theoretical and empirical analysis. Theoretically, we show that DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima. Empirically, we conduct evaluations of DuRM across different datasets, modalities, and network architectures on diverse tasks, including conventional classification, semantic segmentation, out-of-distribution generalization, adverserial training, and long-tailed recognition. Results demonstrate that DuRM could consistently improve the performance under all tasks with an almost free lunch manner. Furthermore, we show that DuRM is compatible with existing generalization techniques and we discuss possible limitations. We hope that DuRM could trigger new interest in the fundamental research on risk minimization.
</details>
<details>
<summary>摘要</summary>
empirical risk minimization (ERM) 是机器学习的一种基本思想。然而，其泛化能力在各种任务上有限。在这篇论文中，我们提出了干扰risk minimization（DuRM），一种极其简单和普遍适用的技术，以提高ERM的泛化能力。DuRM的实现非常简单：只需扩大输出logits的维度，然后使用标准的梯度下降优化。我们在理论和实验两方面 validate DuRM的有效性。在理论上，我们表明DuRM可以提高模型的泛化能力，通过观察更好的平坦的本地极小值。在实验上，我们对不同的数据集、模式和网络架构进行了多种任务的评估，包括传统的分类、semantic segmentation、out-of-distribution泛化、对抗训练和长尾识别。结果表明，DuRM可以在所有任务上提高性能，几乎没有免费的午餐。此外，我们还证明了DuRM与现有的泛化技术相容，并讨论了可能的限制。我们希望DuRM可以触发新的研究于风险最小化的基础。
</details></li>
</ul>
<hr>
<h2 id="DIVERSIFY-A-General-Framework-for-Time-Series-Out-of-distribution-Detection-and-Generalization"><a href="#DIVERSIFY-A-General-Framework-for-Time-Series-Out-of-distribution-Detection-and-Generalization" class="headerlink" title="DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization"></a>DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02282">http://arxiv.org/abs/2308.02282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, Xiangyang Ji, Qiang Yang, Xing Xie<br>for: This paper aims to address the challenges of out-of-distribution (OOD) detection and generalization on time series data, which is non-stationary and has dynamic distributions.methods: The proposed method, DIVERSIFY, is an iterative framework that uses adversarial training to obtain the “worst-case” latent distribution scenario, and then reduces the gap between these latent distributions. DIVERSIFY combines existing OOD detection methods with outputs of models for detection and utilizes outputs for classification.results: Extensive experiments on seven datasets with different OOD settings show that DIVERSIFY learns more generalized features and significantly outperforms other baselines. Theoretical insights also support the effectiveness of DIVERSIFY.<details>
<summary>Abstract</summary>
Time series remains one of the most challenging modalities in machine learning research. The out-of-distribution (OOD) detection and generalization on time series tend to suffer due to its non-stationary property, i.e., the distribution changes over time. The dynamic distributions inside time series pose great challenges to existing algorithms to identify invariant distributions since they mainly focus on the scenario where the domain information is given as prior knowledge. In this paper, we attempt to exploit subdomains within a whole dataset to counteract issues induced by non-stationary for generalized representation learning. We propose DIVERSIFY, a general framework, for OOD detection and generalization on dynamic distributions of time series. DIVERSIFY takes an iterative process: it first obtains the "worst-case" latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We implement DIVERSIFY via combining existing OOD detection methods according to either extracted features or outputs of models for detection while we also directly utilize outputs for classification. In addition, theoretical insights illustrate that DIVERSIFY is theoretically supported. Extensive experiments are conducted on seven datasets with different OOD settings across gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition. Qualitative and quantitative results demonstrate that DIVERSIFY learns more generalized features and significantly outperforms other baselines.
</details>
<details>
<summary>摘要</summary>
时序序列仍然是机器学习研究中最为困难的模式之一。非站点性（OOD）检测和泛化在时序序列上通常受到非站点性的影响，即时序序列的分布随着时间的变化。时序序列中的动态分布对现有算法提供了很大挑战，因为它们主要假设有域信息作为先验知识。在这篇论文中，我们尝试利用时序序列中的子领域来缓解由非站点性引起的问题，以实现泛化学习。我们提出了DIVERSIFY，一种通用框架，用于OOD检测和泛化动态分布的时序序列。DIVERSIFY采用了迭代过程：首先通过对恶性学习获得“最差”的幂本分布场景，然后减少这些幂本分布之间的差距。我们通过结合现有OOD检测方法来实现DIVERSIFY，并直接利用模型输出进行分类。此外，理论分析表明DIVERSIFY是理论上支持的。我们对七个不同的数据集进行了广泛的实验，包括手势识别、语音命令识别、着装压力和情感识别以及基于传感器的人体活动识别。结果表明DIVERSIFY学习了更泛化的特征，并显著超过了其他基elines。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Proximal-Gradient-Method-for-Convex-Optimization"><a href="#Adaptive-Proximal-Gradient-Method-for-Convex-Optimization" class="headerlink" title="Adaptive Proximal Gradient Method for Convex Optimization"></a>Adaptive Proximal Gradient Method for Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02261">http://arxiv.org/abs/2308.02261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yura Malitsky, Konstantin Mishchenko</li>
<li>for: 本文研究了两种基本的首阶算法在凸优化中，即梯度下降（GD）和 proximal梯度方法（ProxGD）。我们的注意点是使这两种算法完全适应тив，利用凸函数的地方几何信息。</li>
<li>methods: 我们提出了基于观察到的梯度差的自适应GD和ProxGD版本，无需额外计算成本。此外，我们证明了我们的方法在只假设本地 lipschitz 的梯度下 converges。</li>
<li>results: 我们的方法可以使用更大的步长 than those initially suggested in [MM20]。<details>
<summary>Abstract</summary>
In this paper, we explore two fundamental first-order algorithms in convex optimization, namely, gradient descent (GD) and proximal gradient method (ProxGD). Our focus is on making these algorithms entirely adaptive by leveraging local curvature information of smooth functions. We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs. Moreover, we prove convergence of our methods assuming only local Lipschitzness of the gradient. In addition, the proposed versions allow for even larger stepsizes than those initially suggested in [MM20].
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了两种基本的首阶算法在凸优化中，即梯度下降（GD）和贝克斯 gradient 方法（ProxGD）。我们的关注点是使这些算法完全适应ive，利用当地凸函数的曲率信息。我们提出了基于观察到的梯度差的自适应GD和ProxGD版本，无需额外计算成本。此外，我们证明了我们的方法在本地lipchitz continuous的梯度下 converges。此外，我们的方法还允许更大的步长than those initially suggested in [MM20].
</details></li>
</ul>
<hr>
<h2 id="Finding-Tori-Self-supervised-Learning-for-Analyzing-Korean-Folk-Song"><a href="#Finding-Tori-Self-supervised-Learning-for-Analyzing-Korean-Folk-Song" class="headerlink" title="Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song"></a>Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02249">http://arxiv.org/abs/2308.02249</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danbinaerinhan/finding-tori">https://github.com/danbinaerinhan/finding-tori</a></li>
<li>paper_authors: Danbinaerin Han, Rafael Caro Repetto, Dasaem Jeong</li>
<li>for: 这个论文是对韩国民族歌曲录音数据集进行计算分析的，该数据集包含约700小时的民歌，录制于1980-90年代。</li>
<li>methods: 作者使用自动超vision学习和卷积神经网络，通过抽象报表来解决录音中的挑战。</li>
<li>results: 实验结果表明，作者的方法可以更好地捕捉韩国民歌中的护卷特征，比传统的抑制历史更加精准。通过这种方法，作者可以对现有学术中的音乐讨论在实际录音中进行实质性的探讨。<details>
<summary>Abstract</summary>
In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, the dataset provides several challenges. To address this challenge, we utilized self-supervised learning with convolutional neural network based on pitch contour, then analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. The experimental result shows that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种计算方法对韩国传统歌曲场记录数据集进行分析，该数据集约为700小时，录制于1980-90年代。由于大多数歌曲由非专业音乐家演唱，没有伴奏，因此该数据集具有许多挑战。为 Addressing this challenge, we utilized self-supervised learning with convolutional neural networks based on pitch contours, and analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. Our experimental results show that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.Here's the breakdown of the translation:* 韩国传统歌曲 (Korean traditional folk songs) -> 韩国传统歌曲 (Simplified Chinese)* 场记录数据集 (field recording dataset) -> 场记录数据集 (Simplified Chinese)* 约为700小时 (approximately 700 hours) -> 约为700小时 (Simplified Chinese)* 录制于1980-90年代 (recorded in the 1980s-1990s) -> 录制于1980-90年代 (Simplified Chinese)* 非专业音乐家 (non-expert musicians) -> 非专业音乐家 (Simplified Chinese)* 没有伴奏 (no accompaniment) -> 没有伴奏 (Simplified Chinese)* 计算方法 (computational method) -> 计算方法 (Simplified Chinese)* 自动学习 (self-supervised learning) -> 自动学习 (Simplified Chinese)* 基于折衣 (based on pitch contours) -> 基于折衣 (Simplified Chinese)* tori (a classification system) -> tori (Simplified Chinese)* 定义为特定的音阶、装饰音和idiomatic melodic contour -> 定义为特定的音阶、装饰音和idiomatic melodic contour (Simplified Chinese)* 使用我们的方法可以更好地捕捉折衣的特点 -> 使用我们的方法可以更好地捕捉折衣的特点 (Simplified Chinese)* 比传统折衣 histogram 更好 -> 比传统折衣 histogram 更好 (Simplified Chinese)* 使用我们的方法 -> 使用我们的方法 (Simplified Chinese)* 我们已经使用这些方法 -> 我们已经使用这些方法 (Simplified Chinese)* 对现有的音乐学讨论进行实际应用 -> 对现有的音乐学讨论进行实际应用 (Simplified Chinese)* 探讨了韩国传统歌曲中的音乐讨论 -> 探讨了韩国传统歌曲中的音乐讨论 (Simplified Chinese)
</details></li>
</ul>
<hr>
<h2 id="Deep-neural-networks-from-the-perspective-of-ergodic-theory"><a href="#Deep-neural-networks-from-the-perspective-of-ergodic-theory" class="headerlink" title="Deep neural networks from the perspective of ergodic theory"></a>Deep neural networks from the perspective of ergodic theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03888">http://arxiv.org/abs/2308.03888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang</li>
<li>for: 这个论文旨在解释深度神经网络的设计是如何变成一种更加科学的过程，而不是一种艺术。</li>
<li>methods: 这个论文使用了时间演化观的思想，将神经网络看作是一个动力系统的时间演化，每层对应于一个时间实例。</li>
<li>results: 这个论文表明，一些可能看起来神秘的规则，可以被解释为启发。<details>
<summary>Abstract</summary>
The design of deep neural networks remains somewhat of an art rather than precise science. By tentatively adopting ergodic theory considerations on top of viewing the network as the time evolution of a dynamical system, with each layer corresponding to a temporal instance, we show that some rules of thumb, which might otherwise appear mysterious, can be attributed heuristics.
</details>
<details>
<summary>摘要</summary>
神经网络设计仍然很有创造性，更像是一种艺术而非精确科学。通过尝试将ergodic theory应用于视网膜上，视网膜为时间演化的动力系统，每层对应一个时间实例，我们显示了一些可能看起来神秘的规则，实际上可以归结为优化策略。
</details></li>
</ul>
<hr>
<h2 id="Self-Normalizing-Neural-Network-Enabling-One-Shot-Transfer-Learning-for-Modeling-EDFA-Wavelength-Dependent-Gain"><a href="#Self-Normalizing-Neural-Network-Enabling-One-Shot-Transfer-Learning-for-Modeling-EDFA-Wavelength-Dependent-Gain" class="headerlink" title="Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain"></a>Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02233">http://arxiv.org/abs/2308.02233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agastya Raj, Zehao Wang, Frank Slyne, Tingjun Chen, Dan Kilper, Marco Ruffini</li>
<li>for: 该论文旨在提出一种基于 semi-supervised, self-normalizing neural networks 的多芯片 EDFA 波长依赖性的模型化框架，以实现一次转移学习。</li>
<li>methods: 该论文使用 semi-supervised, self-normalizing neural networks 来模型多芯片 EDFA 的波长依赖性，并实现了一次转移学习。</li>
<li>results: 实验结果表明，该模型在 Open Ireland 和 COSMOS 测试平台上的 22 个 EDFA 中具有高精度的转移学习能力，即使操作在不同的芯片类型上。<details>
<summary>Abstract</summary>
We present a novel ML framework for modeling the wavelength-dependent gain of multiple EDFAs, based on semi-supervised, self-normalizing neural networks, enabling one-shot transfer learning. Our experiments on 22 EDFAs in Open Ireland and COSMOS testbeds show high-accuracy transfer-learning even when operated across different amplifier types.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的机器学习框架，用于模型多个电子发射激光扩展器（EDFA）的波长依赖性收益，基于半监督自适应神经网络。我们的实验表明，这种框架可以在不同类型的扩展器上实现高精度的传输学习，并且可以在22个EDFA上进行一次转移学习。
</details></li>
</ul>
<hr>
<h2 id="Likelihood-ratio-based-confidence-intervals-for-neural-networks"><a href="#Likelihood-ratio-based-confidence-intervals-for-neural-networks" class="headerlink" title="Likelihood-ratio-based confidence intervals for neural networks"></a>Likelihood-ratio-based confidence intervals for neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02221">http://arxiv.org/abs/2308.02221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/laurenssluyterman/likelihood_ratio_intervals">https://github.com/laurenssluyterman/likelihood_ratio_intervals</a></li>
<li>paper_authors: Laurens Sluijterman, Eric Cator, Tom Heskes</li>
<li>for: 这个论文是为了建立一种基于likelihood ratio的方法来计算神经网络的信心 интерval。</li>
<li>methods: 这个方法使用了likelihood ratio的思想，可以建立不对称的信心 интерval，并且自动包含了训练时间、网络架构、训练技巧等因素。</li>
<li>results: 这个方法可以在对于医学预测或天文物理等领域，提供一个可靠的未知度估计，并且显示出这种方法在某些情况下可能已经有经济效益。<details>
<summary>Abstract</summary>
This paper introduces a first implementation of a novel likelihood-ratio-based approach for constructing confidence intervals for neural networks. Our method, called DeepLR, offers several qualitative advantages: most notably, the ability to construct asymmetric intervals that expand in regions with a limited amount of data, and the inherent incorporation of factors such as the amount of training time, network architecture, and regularization techniques. While acknowledging that the current implementation of the method is prohibitively expensive for many deep-learning applications, the high cost may already be justified in specific fields like medical predictions or astrophysics, where a reliable uncertainty estimate for a single prediction is essential. This work highlights the significant potential of a likelihood-ratio-based uncertainty estimate and establishes a promising avenue for future research.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文介绍了一种基于likelihood-ratio的神经网络置信范围的首次实现方法，称为DeepLR。我们的方法具有许多优点：能够构建不均匀的置信范围，在数据有限的地方扩展，同时自动包含训练时间、网络架构和正则化技术等因素。虽然当前实现可能对许多深度学习应用程序来说过于昂贵，但在医学预测或天文物理等领域，准确地估计单个预测结果的不确定性可能已经被 justify。这篇文章探讨了基于likelihood-ratio的置信范围的可能性，并开启了未来研究的新途径。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Driven-Multi-Agent-Reinforcement-Learning-for-Computation-Offloading-in-Cybertwin-Enabled-Internet-of-Vehicles"><a href="#Knowledge-Driven-Multi-Agent-Reinforcement-Learning-for-Computation-Offloading-in-Cybertwin-Enabled-Internet-of-Vehicles" class="headerlink" title="Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles"></a>Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02603">http://arxiv.org/abs/2308.02603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijin Sun, Xiao Yang, Nan Cheng, Xiucheng Wang, Changle Li</li>
<li>for: 提高 cybertwin-enabled IoV 中任务卸载延迟</li>
<li>methods: 使用知识驱动多代理人学习（KMARL）方法，利用域知识加入图 neural networks，实现选择最佳卸载选项</li>
<li>results: 比较其他方法，KMARL 表现更高的奖励和更好的扩展性，受到域知识的整合帮助<details>
<summary>Abstract</summary>
By offloading computation-intensive tasks of vehicles to roadside units (RSUs), mobile edge computing (MEC) in the Internet of Vehicles (IoV) can relieve the onboard computation burden. However, existing model-based task offloading methods suffer from heavy computational complexity with the increase of vehicles and data-driven methods lack interpretability. To address these challenges, in this paper, we propose a knowledge-driven multi-agent reinforcement learning (KMARL) approach to reduce the latency of task offloading in cybertwin-enabled IoV. Specifically, in the considered scenario, the cybertwin serves as a communication agent for each vehicle to exchange information and make offloading decisions in the virtual space. To reduce the latency of task offloading, a KMARL approach is proposed to select the optimal offloading option for each vehicle, where graph neural networks are employed by leveraging domain knowledge concerning graph-structure communication topology and permutation invariance into neural networks. Numerical results show that our proposed KMARL yields higher rewards and demonstrates improved scalability compared with other methods, benefitting from the integration of domain knowledge.
</details>
<details>
<summary>摘要</summary>
通过异步计算任务转移到路边单元（RSU），移动边缘计算（MEC）在互联网机器人（IoV）中可以减轻车辆上计算负担。然而，现有的模型基于任务转移方法受到增加车辆和数据驱动方法的计算复杂性的影响。为解决这些挑战，在这篇论文中，我们提出了知识驱动多智能体强化学习（KMARL）方法，以减少异步任务转移的延迟。具体来说，在考虑的场景中， cybertwin 作为每辆车辆的通信代理，在虚拟空间中交换信息并做出转移决策。通过使用图神经网络，我们利用了域知识，包括图structure 通信topology和 permutation 不变性，从而提高了强化学习的精度和扩展性。 numerically 的结果表明，我们提出的 KMARL 方法可以获得更高的奖励，并且在其他方法相比，具有更好的扩展性，受益于域知识的集成。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Spanish-Clinical-Language-Models"><a href="#A-Survey-of-Spanish-Clinical-Language-Models" class="headerlink" title="A Survey of Spanish Clinical Language Models"></a>A Survey of Spanish Clinical Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02199">http://arxiv.org/abs/2308.02199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillem García Subies, Álvaro Barbero Jiménez, Paloma Martínez Fernández</li>
<li>for: 这项研究专注于使用语言模型解决西班牙语医疗领域任务。</li>
<li>methods: 研究人员回顾了17个词库，主要集中在医疗任务上，然后列出了最有影响力的西班牙语语言模型和医疗语言模型。研究人员还对这些模型进行了严格的比较，用于找出最佳performing的模型，总共超过3000个模型进行了微调。</li>
<li>results: 研究人员对一些可访问的 corpora 进行了测试，并将结果公开发布，以便由独立团队重复或在未来对新的西班牙语医疗语言模型进行挑战。<details>
<summary>Abstract</summary>
This survey focuses in encoder Language Models for solving tasks in the clinical domain in the Spanish language. We review the contributions of 17 corpora focused mainly in clinical tasks, then list the most relevant Spanish Language Models and Spanish Clinical Language models. We perform a thorough comparison of these models by benchmarking them over a curated subset of the available corpora, in order to find the best-performing ones; in total more than 3000 models were fine-tuned for this study. All the tested corpora and the best models are made publically available in an accessible way, so that the results can be reproduced by independent teams or challenged in the future when new Spanish Clinical Language models are created.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字符" in Chinese.Please note that the translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="AutoML4ETC-Automated-Neural-Architecture-Search-for-Real-World-Encrypted-Traffic-Classification"><a href="#AutoML4ETC-Automated-Neural-Architecture-Search-for-Real-World-Encrypted-Traffic-Classification" class="headerlink" title="AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification"></a>AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02182">http://arxiv.org/abs/2308.02182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/orangeuw/automl4etc">https://github.com/orangeuw/automl4etc</a></li>
<li>paper_authors: Navid Malekghaini, Elham Akbari, Mohammad A. Salahuddin, Noura Limam, Raouf Boutaba, Bertrand Mathieu, Stephanie Moteau, Stephane Tuffin</li>
<li>for: 这个研究是为了提出一个自动设计高性能的神经网络模型，用于实时隐私化网络流量分类。</li>
<li>methods: 这个研究使用了自动机器学习（AutoML）技术，定义了一个特定设计的搜寻空间，并运用不同的搜寻策略来寻找最佳的神经网络模型。</li>
<li>results: 研究发现，使用AutoML4ETC可以自动设计高性能的神经网络模型，并且比现有的隐私化网络流量分类模型更加精确和轻量级。<details>
<summary>Abstract</summary>
Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark datasets and real-world TLS and QUIC traffic collected from the Orange mobile network. In addition to being more accurate, AutoML4ETC's architectures are significantly more efficient and lighter in terms of the number of parameters. Finally, we make AutoML4ETC publicly available for future research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Scaling-Clinical-Trial-Matching-Using-Large-Language-Models-A-Case-Study-in-Oncology"><a href="#Scaling-Clinical-Trial-Matching-Using-Large-Language-Models-A-Case-Study-in-Oncology" class="headerlink" title="Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology"></a>Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02180">http://arxiv.org/abs/2308.02180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cliff Wong, Sheng Zhang, Yu Gu, Christine Moung, Jacob Abel, Naoto Usuyama, Roshanthi Weerasinghe, Brian Piening, Tristan Naumann, Carlo Bifulco, Hoifung Poon</li>
<li>For: The paper is written for scaling clinical trial matching using large language models (LLMs) in the field of oncology.* Methods: The paper uses a systematic study approach with cutting-edge LLMs such as GPT-4 to structure eligibility criteria of clinical trials and extract complex matching logic.* Results: The initial findings show that LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop, but there are still areas for improvement such as context limitation and accuracy.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了扩大临床试验匹配使用大型自然语言模型（LLMs）的应用，主要是在肿瘤领域。* Methods: 该论文使用系统性的研究方法，使用最新的GPT-4等 LLMS来结构临床试验资格标准和提取复杂匹配逻辑。* Results: 初步发现结果表明，LLMs已经比前一代强大基elinesubstantially better，可能用作人工协作的初步解决方案，但还有一些需要进一步改进的方向，如上下文限制和准确性。<details>
<summary>Abstract</summary>
Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records.
</details>
<details>
<summary>摘要</summary>
临床试验匹配是医疗卫生系统中一个关键的过程，但在实践中却受到极多的不结构化数据和不可扩展的手动处理的困扰。在这篇论文中，我们进行了系统性的研究，使用大型自然语言模型（LLMs）来扩大临床试验匹配的规模。我们的研究基于一个目前在大型美国医疗网络中测试的临床试验匹配系统。初步的结果很有前途：直接使用最新的GPT-4等 cutting-edge LLMs，可以立即结构化临床试验报名标准和提取复杂的匹配逻辑（例如，嵌入 AND/OR/NOT 结构）。虽然还有一定的改进空间，但LLMs已经明显超过了先前的强基线，并可能作为人工干预的准备解决方案。我们的研究还揭示了应用LLMs到终端临床试验匹配中的一些重要成长点，如Context limitation和准确率，特别是从患者 longitudinal 医疗记录中提取patient信息。
</details></li>
</ul>
<hr>
<h2 id="High-Accuracy-Prediction-of-Metal-Insulator-Metal-Metasurface-with-Deep-Learning"><a href="#High-Accuracy-Prediction-of-Metal-Insulator-Metal-Metasurface-with-Deep-Learning" class="headerlink" title="High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning"></a>High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04450">http://arxiv.org/abs/2308.04450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaizhu Liu, Hsiang-Chen Chui, Changsen Sun, Xue Han</li>
<li>for: 本研究旨在提出一种基于深度学习的电磁软件计算结果预测方法，以提高计算效率和准确性。</li>
<li>methods: 本研究使用了ResNets-10模型进行预测плазмон喷流表 parameters的方法，并采用了k-fold cross-validation和小学习率的两个阶段训练。</li>
<li>results: 根据实验结果，对铝、金、银金属-隔体-铁的预测损失值分别为-48.45、-46.47和-35.54，表明提出的网络可以取代传统电磁计算方法，并且训练过程只需要少于1,100个迭代。<details>
<summary>Abstract</summary>
Deep learning prediction of electromagnetic software calculation results has been a widely discussed issue in recent years. But the prediction accuracy was still one of the challenges to be solved. In this work, we proposed that the ResNets-10 model was used for predicting plasmonic metasurface S11 parameters. The two-stage training was performed by the k-fold cross-validation and small learning rate. After the training was completed, the prediction loss for aluminum, gold, and silver metal-insulator-metal metasurfaces was -48.45, -46.47, and -35.54, respectively. Due to the ultralow error value, the proposed network can replace the traditional electromagnetic computing method for calculation within a certain structural range. Besides, this network can finish the training process less than 1,100 epochs. This means that the network training process can effectively lower the design process time. The ResNets-10 model we proposed can also be used to design meta-diffractive devices and biosensors, thereby reducing the time required for the calculation process. The ultralow error of the network indicates that this work contributes to the development of future artificial intelligence electromagnetic computing software.
</details>
<details>
<summary>摘要</summary>
Recently, deep learning prediction of electromagnetic software calculation results has been a widely discussed issue. However, prediction accuracy was still a challenge to be solved. In this work, we proposed using the ResNets-10 model to predict plasmonic metasurface S11 parameters. We performed two-stage training with k-fold cross-validation and small learning rate. After training, the prediction loss for aluminum, gold, and silver metal-insulator-metal metasurfaces was -48.45, -46.47, and -35.54, respectively. Due to the ultralow error value, the proposed network can replace traditional electromagnetic computing methods for calculation within a certain structural range. Additionally, this network can complete the training process in less than 1,100 epochs, effectively lowering the design process time. The ResNets-10 model we proposed can also be used to design meta-diffractive devices and biosensors, reducing the calculation process time. The ultralow error of the network indicates that this work contributes to the development of future artificial intelligence electromagnetic computing software.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-probabilistic-models-enhance-variational-autoencoder-for-crystal-structure-generative-modeling"><a href="#Diffusion-probabilistic-models-enhance-variational-autoencoder-for-crystal-structure-generative-modeling" class="headerlink" title="Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling"></a>Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02165">http://arxiv.org/abs/2308.02165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teerachote Pakornchote, Natthaphon Choomphon-anomakhun, Sorrjit Arrerut, Chayanon Atthapak, Sakarn Khamkaeo, Thiparat Chotibut, Thiti Bovornratanaraks</li>
<li>for: 生成真实的晶体结构，保持晶体对称性</li>
<li>methods: 使用新的扩散概率模型（DP模型）对原子坐标进行减噪，而不是采用标准的分数匹配方法</li>
<li>results: 能够生成和重建晶体结构，质量与原始CDVAE相似，而且与 relaxed 结构计算得到的碳结构更加接近ground state，能量差值平均为68.1 meV&#x2F;atom 下降，表明DP-CDVAE模型能够更好地代表晶体结构的ground state配置。<details>
<summary>Abstract</summary>
The crystal diffusion variational autoencoder (CDVAE) is a machine learning model that leverages score matching to generate realistic crystal structures that preserve crystal symmetry. In this study, we leverage novel diffusion probabilistic (DP) models to denoise atomic coordinates rather than adopting the standard score matching approach in CDVAE. Our proposed DP-CDVAE model can reconstruct and generate crystal structures whose qualities are statistically comparable to those of the original CDVAE. Furthermore, notably, when comparing the carbon structures generated by the DP-CDVAE model with relaxed structures obtained from density functional theory calculations, we find that the DP-CDVAE generated structures are remarkably closer to their respective ground states. The energy differences between these structures and the true ground states are, on average, 68.1 meV/atom lower than those generated by the original CDVAE. This significant improvement in the energy accuracy highlights the effectiveness of the DP-CDVAE model in generating crystal structures that better represent their ground-state configurations.
</details>
<details>
<summary>摘要</summary>
“单晶扩散条件自适应器”（CDVAE）是一种机器学习模型，利用得分匹配来生成具有实验室同调的晶体结构。在这个研究中，我们使用新的扩散概率模型（DP）来降噪原子坐标而不是采用CDVAE的标准得分匹配方法。我们称之为DP-CDVAE模型。这个模型可以重建和生成具有同等质量的晶体结构，并且在比较碳原子结构的情况下，DP-CDVAE模型生成的结构与 relaxation 计算得到的结构更加接近真实的基体状态。这些结构的能量差异与真实基体状态相比，平均降低了68.1 meV/atom。这显示DP-CDVAE模型具有更好的基体状态表现，并且能够更好地生成具有实验室同调的晶体结构。
</details></li>
</ul>
<hr>
<h2 id="Speaker-Diarization-of-Scripted-Audiovisual-Content"><a href="#Speaker-Diarization-of-Scripted-Audiovisual-Content" class="headerlink" title="Speaker Diarization of Scripted Audiovisual Content"></a>Speaker Diarization of Scripted Audiovisual Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02160">http://arxiv.org/abs/2308.02160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yogesh Virkar, Brian Thompson, Rohit Paturi, Sundararajan Srinivasan, Marcello Federico</li>
<li>for: 这篇论文主要是为了提高媒体本地化行业中的语音识别技术，具体来说是使用制作过程中使用的脚本来提高电视节目中的 speaker diarization 任务。</li>
<li>methods: 这篇论文提出了一种新的 semi-supervised 方法，通过使用制作过程中的脚本来提取 pseudo-labeled 数据，以提高 speaker diarization 任务的准确率。</li>
<li>results: 在测试集上，这种方法与两个无监督基线模型进行比较，实现了51.7% 的提升。<details>
<summary>Abstract</summary>
The media localization industry usually requires a verbatim script of the final film or TV production in order to create subtitles or dubbing scripts in a foreign language. In particular, the verbatim script (i.e. as-broadcast script) must be structured into a sequence of dialogue lines each including time codes, speaker name and transcript. Current speech recognition technology alleviates the transcription step. However, state-of-the-art speaker diarization models still fall short on TV shows for two main reasons: (i) their inability to track a large number of speakers, (ii) their low accuracy in detecting frequent speaker changes. To mitigate this problem, we present a novel approach to leverage production scripts used during the shooting process, to extract pseudo-labeled data for the speaker diarization task. We propose a novel semi-supervised approach and demonstrate improvements of 51.7% relative to two unsupervised baseline models on our metrics on a 66 show test set.
</details>
<details>
<summary>摘要</summary>
媒体地化业务通常需要最终电影或电视制作的字幕或配音脚本的 verbatim 脚本，以便在外语中创建字幕或配音脚本。特别是 verbatim 脚本（即播放版本）必须以时间码、说话人名和对话内容的结构组织。当前的语音识别技术使得转录步骤得以alleviates。然而，当前的话者分类模型仍然在电视节目中存在两个主要问题：（i）它们无法跟踪大量的说话人，（ii）它们在说话人变化频繁时的准确率低。为解决这个问题，我们提出了一种利用摄制过程中使用的制作脚本，提取 pseudo-labeled 数据来进行说话人分类任务。我们提出了一种新的半超vised方法，并在我们的测试集上实现了51.7%的相对提升，比两个无监督基线模型更高。
</details></li>
</ul>
<hr>
<h2 id="Improved-Order-Analysis-and-Design-of-Exponential-Integrator-for-Diffusion-Models-Sampling"><a href="#Improved-Order-Analysis-and-Design-of-Exponential-Integrator-for-Diffusion-Models-Sampling" class="headerlink" title="Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling"></a>Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02157">http://arxiv.org/abs/2308.02157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinsheng Zhang, Jiaming Song, Yongxin Chen</li>
<li>for: 提高 diffusion models (DMs) 的抽象速度，使其能够更快速地进行抽象。</li>
<li>methods: 利用高级别的减法积分器 (EI)，并通过重新设计高级别减法积分器来满足所有顺序条件，从而提高抽象质量和稳定性。</li>
<li>results: 通过 theoretically 和实际应用，提出了一种改进的减法积分器（RES），可以提高抽象质量和稳定性，并且在实际应用中可以减少数值缺陷和提高 FID 值。例如，在 ImageNet 扩散模型中，通过将单步 DPM-Solver++ 替换为 ORDER-satisfied RES solver，可以降低数值缺陷的比例为 25.2%，并提高 FID 值为 25.4%。<details>
<summary>Abstract</summary>
Efficient differential equation solvers have significantly reduced the sampling time of diffusion models (DMs) while retaining high sampling quality. Among these solvers, exponential integrators (EI) have gained prominence by demonstrating state-of-the-art performance. However, existing high-order EI-based sampling algorithms rely on degenerate EI solvers, resulting in inferior error bounds and reduced accuracy in contrast to the theoretically anticipated results under optimal settings. This situation makes the sampling quality extremely vulnerable to seemingly innocuous design choices such as timestep schedules. For example, an inefficient timestep scheduler might necessitate twice the number of steps to achieve a quality comparable to that obtained through carefully optimized timesteps. To address this issue, we reevaluate the design of high-order differential solvers for DMs. Through a thorough order analysis, we reveal that the degeneration of existing high-order EI solvers can be attributed to the absence of essential order conditions. By reformulating the differential equations in DMs and capitalizing on the theory of exponential integrators, we propose refined EI solvers that fulfill all the order conditions, which we designate as Refined Exponential Solver (RES). Utilizing these improved solvers, RES exhibits more favorable error bounds theoretically and achieves superior sampling efficiency and stability in practical applications. For instance, a simple switch from the single-step DPM-Solver++ to our order-satisfied RES solver when Number of Function Evaluations (NFE) $=9$, results in a reduction of numerical defects by $25.2\%$ and FID improvement of $25.4\%$ (16.77 vs 12.51) on a pre-trained ImageNet diffusion model.
</details>
<details>
<summary>摘要</summary>
高效的差分方程解析器在扩散模型（DM）中减少了采样时间，同时保持高质量的采样。其中，对数Integrators（EI）已经成为了状态之一，但现有的高阶EI基本样式依赖于弱化的EI解决方案，从而导致了较差的误差 bound和降低的准确性，与理论预期的结果不符。这种情况使得采样质量极易受到 seems innocuous的设计选择，如时间步骤调度。例如，使用不优化的时间步骤调度可能需要两倍的步骤数量以达到相同的质量。为解决这一问题，我们重新评估了高阶差分解析器的设计。通过系统的顺序分析，我们发现现有高阶EI解决方案的弱化可以归结于缺乏关键的顺序条件。我们根据扩散模型的差分方程和快速Integrators的理论，提出了改进的REFined Exponential Solver（RES）。我们的改进的解析器可以满足所有顺序条件，并且在实际应用中表现出较好的采样效率和稳定性。例如，将单步DPM-Solver++ switched to我们的顺序满足RES解析器，当Number of Function Evaluations（NFE）=9时，可以降低数值缺陷的比例为25.2%，并提高FID的改进率（16.77 vs 12.51）。
</details></li>
</ul>
<hr>
<h2 id="Optimization-on-Pareto-sets-On-a-theory-of-multi-objective-optimization"><a href="#Optimization-on-Pareto-sets-On-a-theory-of-multi-objective-optimization" class="headerlink" title="Optimization on Pareto sets: On a theory of multi-objective optimization"></a>Optimization on Pareto sets: On a theory of multi-objective optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02145">http://arxiv.org/abs/2308.02145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Roy, Geelon So, Yi-An Ma</li>
<li>for: 多目标优化中，一个单一决策 вектор需要寻找许多目标之间的最佳变数平衡。这些解答被称为Pareto优化解答，它们是对任何一个目标进行改善都需要在另一个目标上付出的决策 vector。但是，Pareto优化解答的集合可能很大，因此我们进一步考虑一个更实际 significanse的Pareto受限优化问题，其中的目标是将一个偏好函数对应到Pareto集。</li>
<li>methods: 我们调查了本地方法来解决这个受限优化问题，这个问题存在两个特点：（i）参数集是隐式定义的，（ii）通常是非凸非光滑的。我们定义了优化和稳定性的概念，并提供了一个Algorithm，其中的最后迭代速率为$O(K^{-1&#x2F;2})$，对于具有强式凹陷和Lipschitz光滑的目标而言。</li>
<li>results: 我们的研究表明，当目标是强式凹陷和Lipschitz光滑的时候，我们的方法具有最后迭代速率$O(K^{-1&#x2F;2})$，即在最后一迭代时，数据的变化速率为$O(K^{-1&#x2F;2})$。这表明我们的方法在解决Pareto受限优化问题时具有高效率和稳定性。<details>
<summary>Abstract</summary>
In multi-objective optimization, a single decision vector must balance the trade-offs between many objectives. Solutions achieving an optimal trade-off are said to be Pareto optimal: these are decision vectors for which improving any one objective must come at a cost to another. But as the set of Pareto optimal vectors can be very large, we further consider a more practically significant Pareto-constrained optimization problem, where the goal is to optimize a preference function constrained to the Pareto set.   We investigate local methods for solving this constrained optimization problem, which poses significant challenges because the constraint set is (i) implicitly defined, and (ii) generally non-convex and non-smooth, even when the objectives are. We define notions of optimality and stationarity, and provide an algorithm with a last-iterate convergence rate of $O(K^{-1/2})$ to stationarity when the objectives are strongly convex and Lipschitz smooth.
</details>
<details>
<summary>摘要</summary>
在多目标优化中，单个决策 вектор必须平衡多个目标之间的贸易offs。solutions达到优化的贸易offs是say Pareto优化的：这些决策 вектор在改进任何一个目标时，必须付出另一个目标的代价。但是Pareto优化集可能很大，因此我们进一步考虑一个更实际 significannot的 Pareto受限优化问题，其中的目标是通过对Pareto集进行优化。我们研究了本地方法来解决这个受限优化问题，这个问题具有以下两个特点：（i） constraint set是通过某种方式implcitly定义的，（ii）通常是非拥有凸形和光滑的。我们定义了优化和稳定性的概念，并提供了一个算法，其last-iterate convergence rate为 $O(K^{-1/2})$ 当目标函数是强转化和Lipschitz平滑的时候。
</details></li>
</ul>
<hr>
<h2 id="Event-based-Dynamic-Graph-Representation-Learning-for-Patent-Application-Trend-Prediction"><a href="#Event-based-Dynamic-Graph-Representation-Learning-for-Patent-Application-Trend-Prediction" class="headerlink" title="Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction"></a>Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09780">http://arxiv.org/abs/2308.09780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Zou, Le Yu, Leilei Sun, Bowen Du, Deqing Wang, Fuzhen Zhuang</li>
<li>for: 预测公司将在下一时期申请哪些专利，以估计其发展策略和找到前期伙伴或竞争对手。</li>
<li>methods: 我们提出了一种基于事件驱动图学习框架的专利申请趋势预测方法，利用公司和专利分类码的启动表示和历史记忆，以及 hierarchical message passing mechanism 来捕捉专利分类码的 semantic proximities。</li>
<li>results: 我们的方法在实际数据上进行了多种实验，并emonstrated 其效果 under various experimental conditions，并且探索了方法在学习分类码 semantics 和跟踪公司技术发展轨迹的能力。<details>
<summary>Abstract</summary>
Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by updating their representations along the hierarchical taxonomy. Finally, the patent application trend is predicted by aggregating the representations of the target company and classification codes from static, dynamic, and hierarchical perspectives. Experiments on real-world data demonstrate the effectiveness of our approach under various experimental conditions, and also reveal the abilities of our method in learning semantics of classification codes and tracking technology developing trajectories of companies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by updating their representations along the hierarchical taxonomy. Finally, the patent application trend is predicted by aggregating the representations of the target company and classification codes from static, dynamic, and hierarchical perspectives. Experiments on real-world data demonstrate the effectiveness of our approach under various experimental conditions, and also reveal the abilities of our method in learning semantics of classification codes and tracking technology developing trajectories of companies."中文翻译：准确预测公司将在下一时间段申请哪种专利，可以为其发展策略提供指导，并在前置的时间内发现可能的合作伙伴或竞争对手。虽然这个问题非常重要，但在前期研究中 rarely studied due to the challenges in modeling companies' continuously evolving preferences and capturing the semantic correlations of classification codes。为了填补这一空白，我们提出了一种基于事件的动态图学学习框架，用于预测专利申请趋势。具体来说，我们的方法基于公司和专利分类代码的记忆表示。当观察到新专利时，相关公司和专利分类代码的表示将根据历史记忆和当前编码的消息进行更新。此外，我们还提供了一种层次消息传递机制，以捕捉专利分类代码的semantic proximity。最后，我们通过 static、动态和层次视角的表示集成来预测专利申请趋势。实验结果表明，我们的方法在不同的实验条件下具有效果，并能够学习分类代码的 semantics和跟踪公司技术发展轨迹。
</details></li>
</ul>
<hr>
<h2 id="Learning-the-solution-operator-of-two-dimensional-incompressible-Navier-Stokes-equations-using-physics-aware-convolutional-neural-networks"><a href="#Learning-the-solution-operator-of-two-dimensional-incompressible-Navier-Stokes-equations-using-physics-aware-convolutional-neural-networks" class="headerlink" title="Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks"></a>Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02137">http://arxiv.org/abs/2308.02137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viktor Grimm, Alexander Heinlein, Axel Klawonn</li>
<li>for: 本研究旨在解决physics-inclusive机器学习技术中geometry的局限性问题，提出一种能够在不同的geometry中学习稳态 Navier-Stokes方程的解 approximate solutions的方法。</li>
<li>methods: 本研究使用了一种基于U-Net-like CNN和finite difference方法的combined方法，并与数据基于方法进行比较。</li>
<li>results: 研究结果表明，physics-aware CNN可以在不同的geometry中学习稳态 Navier-Stokes方程的解 approximate solutions，并且可以与数据基于方法相结合以提高性能。<details>
<summary>Abstract</summary>
In recent years, the concept of introducing physics to machine learning has become widely popular. Most physics-inclusive ML-techniques however are still limited to a single geometry or a set of parametrizable geometries. Thus, there remains the need to train a new model for a new geometry, even if it is only slightly modified. With this work we introduce a technique with which it is possible to learn approximate solutions to the steady-state Navier--Stokes equations in varying geometries without the need of parametrization. This technique is based on a combination of a U-Net-like CNN and well established discretization methods from the field of the finite difference method.The results of our physics-aware CNN are compared to a state-of-the-art data-based approach. Additionally, it is also shown how our approach performs when combined with the data-based approach.
</details>
<details>
<summary>摘要</summary>
近年来，将物理学引入机器学习的概念得到了广泛的推广。然而，大多数物理包含的机器学习技术仍然受限于单个几何或一组可 parametrize 的几何。因此，在新的几何上训练新的模型仍然是必要的。我们在这里介绍一种可以在不同几何中学习稳态奈特-斯托克方程的估计解的技术。这种技术基于一种组合了 U-Net 类 CNN 和已确立的精度方法的finite difference方法。我们对我们的物理意识 CNN 的结果进行了与当前最佳数据驱动方法的比较，同时还展示了我们的方法与数据驱动方法的组合效果。
</details></li>
</ul>
<hr>
<h2 id="Can-Attention-Be-Used-to-Explain-EHR-Based-Mortality-Prediction-Tasks-A-Case-Study-on-Hemorrhagic-Stroke"><a href="#Can-Attention-Be-Used-to-Explain-EHR-Based-Mortality-Prediction-Tasks-A-Case-Study-on-Hemorrhagic-Stroke" class="headerlink" title="Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke"></a>Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05110">http://arxiv.org/abs/2308.05110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhang Feng, Jiayi Yuan, Forhan Bin Emdad, Karim Hanna, Xia Hu, Zhe He</li>
<li>for: 预测中风死亡风险的早期预测</li>
<li>methods: 使用一种新的解释性听力基于变换器模型，以提高预测模型的准确性和可读性</li>
<li>results: 研究表明，这种解释性听力基于变换器模型可以提高预测模型的准确性和可读性，并且可以提供有用的特征重要性信息。<details>
<summary>Abstract</summary>
Stroke is a significant cause of mortality and morbidity, necessitating early predictive strategies to minimize risks. Traditional methods for evaluating patients, such as Acute Physiology and Chronic Health Evaluation (APACHE II, IV) and Simplified Acute Physiology Score III (SAPS III), have limited accuracy and interpretability. This paper proposes a novel approach: an interpretable, attention-based transformer model for early stroke mortality prediction. This model seeks to address the limitations of previous predictive models, providing both interpretability (providing clear, understandable explanations of the model) and fidelity (giving a truthful explanation of the model's dynamics from input to output). Furthermore, the study explores and compares fidelity and interpretability scores using Shapley values and attention-based scores to improve model explainability. The research objectives include designing an interpretable attention-based transformer model, evaluating its performance compared to existing models, and providing feature importance derived from the model.
</details>
<details>
<summary>摘要</summary>
stroke 是一个重要的死亡和残留症状的原因，需要早期预测方法来减少风险。传统的评估病人方法，如急性physiology和慢性健康评估（APACHE II、IV）和简化型急性 физиiology分数III（SAPS III），有限的准确性和可读性。这篇论文提出了一种新的方法：一种可解释的、注意力基本变换模型，用于早期stroke mortality预测。这个模型旨在解决之前的预测模型的局限性，提供了可解释性（提供明确、理解的解释）和诚实性（从输入到输出的模型动力学提供真实的解释）。此外，研究还研究了和比较了可解释性和诚实性分数使用Shapley值和注意力基本分数来提高模型解释性。研究的目标包括设计一种可解释的注意力基本变换模型，评估其性能与现有模型相比，并提供来自模型的特征重要性。
</details></li>
</ul>
<hr>
<h2 id="Analysis-and-Optimization-of-Wireless-Federated-Learning-with-Data-Heterogeneity"><a href="#Analysis-and-Optimization-of-Wireless-Federated-Learning-with-Data-Heterogeneity" class="headerlink" title="Analysis and Optimization of Wireless Federated Learning with Data Heterogeneity"></a>Analysis and Optimization of Wireless Federated Learning with Data Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03521">http://arxiv.org/abs/2308.03521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Han, Jun Li, Wen Chen, Zhen Mei, Kang Wei, Ming Ding, H. Vincent Poor</li>
<li>for: 本文旨在研究和优化无线 Federated Learning（FL）中的数据多样性和无线资源分配问题，以提高FL的性能和能效性。</li>
<li>methods: 本文使用closed-form表达式来计算FL损失函数的上界，并对Client Scheduling、资源分配和本地训练 epoch数进行优化。</li>
<li>results: 实验结果表明，提出的算法在实际数据集上比其他参考方法更高的学习精度和能 consumption。<details>
<summary>Abstract</summary>
With the rapid proliferation of smart mobile devices, federated learning (FL) has been widely considered for application in wireless networks for distributed model training. However, data heterogeneity, e.g., non-independently identically distributions and different sizes of training data among clients, poses major challenges to wireless FL. Limited communication resources complicate the implementation of fair scheduling which is required for training on heterogeneous data, and further deteriorate the overall performance. To address this issue, this paper focuses on performance analysis and optimization for wireless FL, considering data heterogeneity, combined with wireless resource allocation. Specifically, we first develop a closed-form expression for an upper bound on the FL loss function, with a particular emphasis on data heterogeneity described by a dataset size vector and a data divergence vector. Then we formulate the loss function minimization problem, under constraints on long-term energy consumption and latency, and jointly optimize client scheduling, resource allocation, and the number of local training epochs (CRE). Next, via the Lyapunov drift technique, we transform the CRE optimization problem into a series of tractable problems. Extensive experiments on real-world datasets demonstrate that the proposed algorithm outperforms other benchmarks in terms of the learning accuracy and energy consumption.
</details>
<details>
<summary>摘要</summary>
随着智能移动设备的普及，分布式学习（FL）在无线网络中得到了广泛的考虑，用于分布式模型训练。然而，数据不均衡，如非独立同分布和不同的训练数据大小 среди客户端，对无线FL的应用带来了主要挑战。限制通信资源使得实现公平调度变得更加困难，从而降低总性能。为解决这个问题，这篇论文关注无线FL的性能分析和优化，考虑到数据不均衡，并与无线资源分配相结合。首先，我们开发了一个关于FL损失函数上的上界，强调数据不均衡的特点，由一个数据大小向量和一个数据差异向量描述。然后，我们将损失函数最小化问题转化为一个具有长期能源占用和延迟的约束的优化问题。通过利用Lyapunov漂移技术，我们将CRE优化问题转化为一系列可解的问题。在实际数据上进行了广泛的实验，结果表明，我们的算法在学习精度和能源消耗方面比其他参考值更高。
</details></li>
</ul>
<hr>
<h2 id="Branched-Latent-Neural-Operators"><a href="#Branched-Latent-Neural-Operators" class="headerlink" title="Branched Latent Neural Operators"></a>Branched Latent Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02599">http://arxiv.org/abs/2308.02599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanfordcbcl/blno.jl">https://github.com/stanfordcbcl/blno.jl</a></li>
<li>paper_authors: Matteo Salvador, Alison Lesley Marsden</li>
<li>for:  This paper aims to develop a novel computational tool for building reliable and efficient reduced-order models for digital twinning in engineering applications.</li>
<li>methods: The paper proposes the use of Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. BLNOs are defined as simple and compact feedforward partially-connected neural networks that structurally disentangle inputs with different intrinsic roles.</li>
<li>results: The paper demonstrates the effectiveness of BLNOs in a challenging test case involving biophysically detailed electrophysiology simulations in a biventricular cardiac model of a pediatric patient with hypoplastic left heart syndrome. The model includes a purkinje network for fast conduction and a heart-torso geometry. The paper shows that BLNOs can retain just 7 hidden layers and 19 neurons per layer, and achieve a mean square error of $10^{-4}$ on an independent test dataset comprised of 50 additional electrophysiology simulations.<details>
<summary>Abstract</summary>
We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed electrophysiology simulations in a biventricular cardiac model of a pediatric patient with hypoplastic left heart syndrome. The model includes a purkinje network for fast conduction and a heart-torso geometry. Specifically, we trained BLNOs on 150 in silico generated 12-lead electrocardiograms (ECGs) while spanning 7 model parameters, covering cell-scale, organ-level and electrical dyssynchrony. Although the 12-lead ECGs manifest very fast dynamics with sharp gradients, after automatic hyperparameter tuning the optimal BLNO, trained in less than 3 hours on a single CPU, retains just 7 hidden layers and 19 neurons per layer. The mean square error is on the order of $10^{-4}$ on an independent test dataset comprised of 50 additional electrophysiology simulations. This paper provides a novel computational tool to build reliable and efficient reduced-order models for digital twinning in engineering applications.
</details>
<details>
<summary>摘要</summary>
我们引入分支隐藏神经操作符（BLNOs），以学习输入-输出对应器，模型复杂物理过程。 BLNO 是一个简单且紧凑的Feedforward 内部连接神经网络，它将输入变数分类为不同的内在角色，例如时间变数和模型参数，并将它们转换为一个通用的应用领域。 BLNO 利用可读性的隐藏输出增强学习过程，并突破维度给定问题的咒语，通过在训练阶段实现小训练集和短时间内的优秀一致性。此外，对于完全连接结构而言，部分连接可以对缩减可调 Parameters 数量。我们透过实际应用在一个儿童心脏病 hypoplastic left heart syndrome 的双心室心脏模型中，并在该模型中包含 Purkinje 网络和心脏-肋间 geometry。具体来说，我们将 BLNO 训练在 150 个silico生成的 12 项电击ogram (ECG) 上，涵盖 7 个模型参数，包括细胞层、器官层和电子 Dyssynchrony。虽然 12 项 ECG 呈现非常快的动态，但是通过自动优化参数后，最佳 BLNO 在仅三个小时内在单一 CPU 上训练，只有 7 个隐藏层和 19 个神经元 per 层。该模型的平方误差在统计上为 $10^{-4}$，在 50 个其他电生物频谱 simulations 的独立测试集中进行验证。本研究提供了一个新的 Computational 工具，可以建立可靠和高效的实际应用中的简化模型，以应用于工程应用中的数字双胞志。
</details></li>
</ul>
<hr>
<h2 id="Eva-A-General-Vectorized-Approximation-Framework-for-Second-order-Optimization"><a href="#Eva-A-General-Vectorized-Approximation-Framework-for-Second-order-Optimization" class="headerlink" title="Eva: A General Vectorized Approximation Framework for Second-order Optimization"></a>Eva: A General Vectorized Approximation Framework for Second-order Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02123">http://arxiv.org/abs/2308.02123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Zhang, Shaohuai Shi, Bo Li</li>
<li>for: 这个研究旨在提高深度学习模型训练的效率，减少计算和记忆过程中的过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程<details>
<summary>Abstract</summary>
Second-order optimization algorithms exhibit excellent convergence properties for training deep learning models, but often incur significant computation and memory overheads. This can result in lower training efficiency than the first-order counterparts such as stochastic gradient descent (SGD). In this work, we present a memory- and time-efficient second-order algorithm named Eva with two novel techniques: 1) we construct the second-order information with the Kronecker factorization of small stochastic vectors over a mini-batch of training data to reduce memory consumption, and 2) we derive an efficient update formula without explicitly computing the inverse of matrices using the Sherman-Morrison formula. We further extend Eva to a general vectorized approximation framework to improve the compute and memory efficiency of two existing second-order algorithms (FOOF and Shampoo) without affecting their convergence performance. Extensive experimental results on different models and datasets show that Eva reduces the end-to-end training time up to 2.05x and 2.42x compared to first-order SGD and second-order algorithms (K-FAC and Shampoo), respectively.
</details>
<details>
<summary>摘要</summary>
Second-order优化算法在训练深度学习模型时展现出极佳的收敛性质，但通常会导致计算和内存开销增加。这可能会导致训练效率低于首次优化算法 such as 随机梯度下降（SGD）。在这项工作中，我们提出了一种具有内存和时间效率的第二次优化算法名为Eva，并采用了两种新的技术：1）我们通过小批量训练数据的克ро内克分解来减少内存占用，2）我们 derivate了高效的更新公式，不需要直接计算矩阵的逆元。我们进一步扩展Eva到一个通用的向量化近似框架，以提高两个现有的第二次优化算法（FOOF和Shampoo）的计算和内存效率，无需影响其收敛性能。我们在不同的模型和数据集上进行了广泛的实验，结果显示，Eva可以比首次优化算法和第二次优化算法（K-FAC和Shampoo）减少综合训练时间，具体的比例为2.05倍和2.42倍。
</details></li>
</ul>
<hr>
<h2 id="Model-Provenance-via-Model-DNA"><a href="#Model-Provenance-via-Model-DNA" class="headerlink" title="Model Provenance via Model DNA"></a>Model Provenance via Model DNA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02121">http://arxiv.org/abs/2308.02121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Mu, Yu Wang, Yehong Zhang, Jiaqi Zhang, Hui Wang, Yang Xiang, Yue Yu</li>
<li>for: This paper focuses on the problem of Model Provenance (MP) in machine learning (ML), which aims to determine whether a source model serves as the provenance for a target model.</li>
<li>methods: The authors introduce a novel concept of Model DNA, which represents the unique characteristics of a machine learning model, and use a data-driven and model-driven representation learning method to encode the model’s training data and input-output information as a compact and comprehensive representation of the model.</li>
<li>results: The authors develop an efficient framework for model provenance identification, which enables them to accurately identify whether a source model is a pre-training model of a target model. They conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of their approach.<details>
<summary>Abstract</summary>
Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this model DNA, we develop an efficient framework for model provenance identification, which enables us to identify whether a source model is a pre-training model of a target model. We conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of our approach in accurately identifying model provenance.
</details>
<details>
<summary>摘要</summary>
To address this gap, we introduce a novel concept called Model DNA, which represents the unique characteristics of a machine learning model. We use a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this model DNA, we develop an efficient framework for model provenance identification, which enables us to identify whether a source model is a pre-training model of a target model.We conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of our approach in accurately identifying model provenance. Our approach is efficient and can be applied to a wide range of ML models, providing a valuable tool for ensuring the security and intellectual property of ML models.
</details></li>
</ul>
<hr>
<h2 id="Designing-a-Deep-Learning-Driven-Resource-Efficient-Diagnostic-System-for-Metastatic-Breast-Cancer-Reducing-Long-Delays-of-Clinical-Diagnosis-and-Improving-Patient-Survival-in-Developing-Countries"><a href="#Designing-a-Deep-Learning-Driven-Resource-Efficient-Diagnostic-System-for-Metastatic-Breast-Cancer-Reducing-Long-Delays-of-Clinical-Diagnosis-and-Improving-Patient-Survival-in-Developing-Countries" class="headerlink" title="Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries"></a>Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02597">http://arxiv.org/abs/2308.02597</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Gao, Dayong Wang, Yi Huang</li>
<li>for: 这份研究旨在解决癌症病理诊断过程中的时间延迟问题，特别是癌症患者在发展中国家中的诊断过程中的延迟，以提高癌症患者的存活率。</li>
<li>methods: 这份研究使用了深度学习技术，开发了一个基于MobileNetV2的诊断模型，能够实现高精度的诊断和computational efficiency。</li>
<li>results: 根据评估结果，MobileNetV2基本模型在诊断精度、模型普遍性和模型训练效率等方面都超过了VGG16、ResNet50和ResNet101模型。此外，Visual比较表明，MobileNetV2诊断模型能够识别非常小的癌症细胞在大量正常细胞中，实现了人工影像分析的挑战。<details>
<summary>Abstract</summary>
Breast cancer is one of the leading causes of cancer mortality. Breast cancer patients in developing countries, especially sub-Saharan Africa, South Asia, and South America, suffer from the highest mortality rate in the world. One crucial factor contributing to the global disparity in mortality rate is long delay of diagnosis due to a severe shortage of trained pathologists, which consequently has led to a large proportion of late-stage presentation at diagnosis. The delay between the initial development of symptoms and the receipt of a diagnosis could stretch upwards 15 months. To tackle this critical healthcare disparity, this research has developed a deep learning-based diagnosis system for metastatic breast cancer that can achieve high diagnostic accuracy as well as computational efficiency. Based on our evaluation, the MobileNetV2-based diagnostic model outperformed the more complex VGG16, ResNet50 and ResNet101 models in diagnostic accuracy, model generalization, and model training efficiency. The visual comparisons between the model prediction and ground truth have demonstrated that the MobileNetV2 diagnostic models can identify very small cancerous nodes embedded in a large area of normal cells which is challenging for manual image analysis. Equally Important, the light weighted MobleNetV2 models were computationally efficient and ready for mobile devices or devices of low computational power. These advances empower the development of a resource-efficient and high performing AI-based metastatic breast cancer diagnostic system that can adapt to under-resourced healthcare facilities in developing countries. This research provides an innovative technological solution to address the long delays in metastatic breast cancer diagnosis and the consequent disparity in patient survival outcome in developing countries.
</details>
<details>
<summary>摘要</summary>
乳癌是全球最主要的癌症死亡原因之一，特别是在发展中国家，如非洲南部、南亚和南美， breast cancer 患者的死亡率最高。一个重要的因素导致全球的医疗差距是诊断延迟，因为缺乏培训的病理学家，导致许多患者在诊断时 already in 晚期。延迟从症状出现到诊断的时间可以达15个月。为了解决这个严重的医疗差距，这项研究开发了一个基于深度学习的乳癌诊断系统，可以实现高精度和计算效率。根据我们的评估，使用 MobileNetV2 模型的诊断模型在精度、通用性和训练效率三个方面都高于 VGG16、ResNet50 和 ResNet101 模型。视觉比较表明，MobileNetV2 模型可以准确地检测小型患者中的癌细胞，这是人工图像分析困难的。此外，MobileNetV2 模型的计算效率较低，适用于移动设备或低计算能力的设备。这些进步使得可以开发一个资源高效和高性能的人工智能基于乳癌诊断系统，适应发展中国家的医疗设施。这项研究提供了一种创新的科技解决方案，以Address the long delays in metastatic breast cancer diagnosis and the resulting disparity in patient survival outcomes in developing countries.
</details></li>
</ul>
<hr>
<h2 id="VQGraph-Graph-Vector-Quantization-for-Bridging-GNNs-and-MLPs"><a href="#VQGraph-Graph-Vector-Quantization-for-Bridging-GNNs-and-MLPs" class="headerlink" title="VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs"></a>VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02117">http://arxiv.org/abs/2308.02117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangling0818/vqgraph">https://github.com/yangling0818/vqgraph</a></li>
<li>paper_authors: Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin Cui, Muhan Zhang, Jure Leskovec</li>
<li>for: 提高Graph Neural Networks (GNNs)的批处理能力和实时性，以便在具有延迟限制的实际应用中使用。</li>
<li>methods: 采用知识传承（KD）学习计算效率高的多层感知器（MLP），通过模仿GNN的输出来学习GNN的知识。同时，使用一种新的结构意识graph tokenizer，以及一种基于软标签分配的token-based distillation目标，以便充分传递GNN的结构知识到MLP中。</li>
<li>results: 实验和分析表明，VQGraph可以减少GNN的批处理时间，并且在七个图数据集上实现新的状态机器人性表现，包括在推导和泵化设置下的表现。VQGraph可以比GNN更快地进行推理，并且在实际应用中可以提高GNN的准确率。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propose a new token-based distillation objective based on soft token assignments to sufficiently transfer the structural knowledge from GNN to MLP. Extensive experiments and analyses demonstrate the strong performance of VQGraph, where we achieve new state-of-the-art performance on GNN-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively. Code: https://github.com/YangLing0818/VQGraph.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 通过消息传递来更新节点表示，这会导致实际延迟应用中的可扩展性问题。为解决这个问题，现有方法采用知识传递（KD）来学习计算效率高的多层感知器（MLP），但是现有GNN表示空间可能不够表示图像下的多样化本地结构，这限制了GNN的知识传递。我们提出了一种新的框架VQGraph，用于学习图像表示空间，以bridging GNNs和MLPs。我们采用变体的vector-quantized variational autoencoder（VQ-VAE）的encoder作为结构意识图像tokenizer，该tokenizer可以明确表示不同本地结构中的节点，并组成一个有意义的代码库。利用学习的代码库，我们提出了一个新的符号分配目标，以便充分传递GNN中的结构知识到MLP。我们在七个图像 dataset 上进行了广泛的实验和分析，并证明了VQGraph的强大表现。我们在transductive和induction Setting中， achieved new state-of-the-art performance on GNN-MLP distillation，并且在GNN和独立MLP上的性能上提高了3.90%和28.05%。此外，我们还证明了VQGraph在GNN上进行更快的推理，比GNN的828倍。代码：https://github.com/YangLing0818/VQGraph。
</details></li>
</ul>
<hr>
<h2 id="Breast-Ultrasound-Tumor-Classification-Using-a-Hybrid-Multitask-CNN-Transformer-Network"><a href="#Breast-Ultrasound-Tumor-Classification-Using-a-Hybrid-Multitask-CNN-Transformer-Network" class="headerlink" title="Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network"></a>Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02101">http://arxiv.org/abs/2308.02101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bryar Shareef, Min Xian, Aleksandar Vakanski, Haotian Wang</li>
<li>for: 这个研究旨在提出一个混合多任务深度学习网络（Hybrid-MT-ESTAN），用于肺肿瘤分类和分 segmentation。</li>
<li>methods: 这个方法使用了 CNN 和 Swin Transformer 两种不同的架构，以提高全球背景信息的捕捉和地方图像特征的维持。</li>
<li>results: 实验结果显示，Hybrid-MT-ESTAN 得到了最高的准确率（82.7%）、敏感度（86.4%）和 F1 分数（86.0%）。<details>
<summary>Abstract</summary>
Capturing global contextual information plays a critical role in breast ultrasound (BUS) image classification. Although convolutional neural networks (CNNs) have demonstrated reliable performance in tumor classification, they have inherent limitations for modeling global and long-range dependencies due to the localized nature of convolution operations. Vision Transformers have an improved capability of capturing global contextual information but may distort the local image patterns due to the tokenization operations. In this study, we proposed a hybrid multitask deep neural network called Hybrid-MT-ESTAN, designed to perform BUS tumor classification and segmentation using a hybrid architecture composed of CNNs and Swin Transformer components. The proposed approach was compared to nine BUS classification methods and evaluated using seven quantitative metrics on a dataset of 3,320 BUS images. The results indicate that Hybrid-MT-ESTAN achieved the highest accuracy, sensitivity, and F1 score of 82.7%, 86.4%, and 86.0%, respectively.
</details>
<details>
<summary>摘要</summary>
capture global contextual information 在乳腺超声图像分类中扮演着关键性的角色。尽管 convolutional neural networks (CNNs) 在肿瘤分类中表现出了可靠的性，但它们具有内置的局部化特性，因此可能导致模型长距离和全局依赖关系的模型化困难。 vision transformers 具有改善全局上下文信息捕捉的能力，但可能会因为 tokenization 操作而导致本地图像模式的扭曲。在这项研究中，我们提出了一种 hybrid multitask deep neural network called Hybrid-MT-ESTAN，用于实现乳腺超声图像分类和分割。我们的方法与 nine 种乳腺分类方法进行比较，并在一个包含 3,320 个乳腺超声图像的数据集上进行评估。结果表明，Hybrid-MT-ESTAN 达到了最高的准确率、敏感度和 F1 分数，即 82.7%、86.4% 和 86.0%  соответственно。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Model-Adaptation-for-Continual-Learning-at-the-Edge"><a href="#Efficient-Model-Adaptation-for-Continual-Learning-at-the-Edge" class="headerlink" title="Efficient Model Adaptation for Continual Learning at the Edge"></a>Efficient Model Adaptation for Continual Learning at the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02084">http://arxiv.org/abs/2308.02084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachary A. Daniels, Jun Hu, Michael Lomnitz, Phil Miller, Aswin Raghavan, Joe Zhang, Michael Piacentino, David Zhang</li>
<li>for: 这个研究旨在提供一个非站势自动机器学习（AutoML）框架，以便在资料分布随时变化时进行高效的连续学习。</li>
<li>methods: 这个框架使用固定的深度神经网（DNN）特征嵌入器，并训练浅层网络来处理新数据。它还使用了数维计算（HDC）和零 shot神经架搜索（ZS-NAS）来探测新数据是否为外部数据（OOD），并适当地调整模型以适应OOD数据。</li>
<li>results: 在多个域别数据集上进行评估，这个框架实现了优秀的性能，比如果探测OOD数据和几何shot NAS。<details>
<summary>Abstract</summary>
Most machine learning (ML) systems assume stationary and matching data distributions during training and deployment. This is often a false assumption. When ML models are deployed on real devices, data distributions often shift over time due to changes in environmental factors, sensor characteristics, and task-of-interest. While it is possible to have a human-in-the-loop to monitor for distribution shifts and engineer new architectures in response to these shifts, such a setup is not cost-effective. Instead, non-stationary automated ML (AutoML) models are needed. This paper presents the Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning under domain shifts. The EAR framework uses a fixed deep neural network (DNN) feature encoder and trains shallow networks on top of the encoder to handle novel data. The EAR framework is capable of 1) detecting when new data is out-of-distribution (OOD) by combining DNNs with hyperdimensional computing (HDC), 2) identifying low-parameter neural adaptors to adapt the model to the OOD data using zero-shot neural architecture search (ZS-NAS), and 3) minimizing catastrophic forgetting on previous tasks by progressively growing the neural architecture as needed and dynamically routing data through the appropriate adaptors and reconfigurators for handling domain-incremental and class-incremental continual learning. We systematically evaluate our approach on several benchmark datasets for domain adaptation and demonstrate strong performance compared to state-of-the-art algorithms for OOD detection and few-/zero-shot NAS.
</details>
<details>
<summary>摘要</summary>
大多数机器学习（ML）系统假设训练和部署时数据分布是静止的，这是一个不实际的假设。当 ML 模型在实际设备上部署时，数据分布经常会随着环境因素、传感器特性和任务 интерес而变化。虽然可以有人在Loop监控数据分布的变化并为此设计新的建筑，但这种设置不是可cost-effective的。而是需要不静止的自动机器学习（AutoML）模型。这篇论文提出了Encoder-Adaptor-Reconfigurator（EAR）框架，用于效率地进行适应域shift continual learning。EAR框架使用固定的深度神经网络（DNN）特征编码器，并在编码器之上训练浅层网络来处理新数据。EAR框架可以1）将新数据标记为out-of-distribution（OOD），通过将DNN与高维计算（HDC）结合使用，2）通过零 shot neural architecture search（ZS-NAS）来适应OOD数据，3）在前一个任务上避免忘记性衰变，通过逐渐增加神经建筑和动态路由数据通过适当的适应器和重配置器来处理域增量和类增量 continual learning。我们系统性地评估了我们的方法在域适应数据上的多个benchmark datasets，并demonstrated strong performance compared to state-of-the-art algorithms for OOD detection和few-/zero-shot NAS。
</details></li>
</ul>
<hr>
<h2 id="Target-specification-bias-counterfactual-prediction-and-algorithmic-fairness-in-healthcare"><a href="#Target-specification-bias-counterfactual-prediction-and-algorithmic-fairness-in-healthcare" class="headerlink" title="Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare"></a>Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02081">http://arxiv.org/abs/2308.02081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eran Tal</li>
<li>for: 这篇论文探讨了机器学习（ML）在医疗领域中的偏见问题，并提出了一种更普遍的偏见来源：目标规定偏见。</li>
<li>methods: 这篇论文使用了现有的数据和健康差异的研究，以及现有的机器学习算法和模型。然而，它发现了一种更加普遍的偏见来源：目标规定偏见。</li>
<li>results: 这篇论文发现了target specification bias可能会导致估计准确性过高，使用医疗资源不fficient，并导致伤害病人的决策。<details>
<summary>Abstract</summary>
Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology - the science of measurement - suggests ways of counteracting target specification bias and avoiding its harmful consequences.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在医疗领域中的偏见通常被归结于不完整或不代表性的数据，或者下面的健康差异。本文标识了更广泛的偏见来源，对临床实用性有影响的预测工具：目标规定偏见。目标规定偏见发生在运行化目标变量时与决策者定义的目标之间的匹配不匹配。这种匹配不匹配通常是柔和的，来自于决策者通常关心预测实际医疗情况下的结果，而不是实际情况。这种偏见不受数据限制和健康差异影响，并且不会被纠正。如果不纠正，它会导致预测精度的过高估计，医疗资源的不效利用，以及对病人伤害的不佳决策。近些年的metrology研究（量度科学）提供了对抗目标规定偏见的方法，避免其不良后果。
</details></li>
</ul>
<hr>
<h2 id="Causality-Guided-Disentanglement-for-Cross-Platform-Hate-Speech-Detection"><a href="#Causality-Guided-Disentanglement-for-Cross-Platform-Hate-Speech-Detection" class="headerlink" title="Causality Guided Disentanglement for Cross-Platform Hate Speech Detection"></a>Causality Guided Disentanglement for Cross-Platform Hate Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02080">http://arxiv.org/abs/2308.02080</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paras2612/catch">https://github.com/paras2612/catch</a></li>
<li>paper_authors: Paras Sheth, Tharindu Kumarage, Raha Moraffah, Aman Chadha, Huan Liu</li>
<li>for: 寻找一种可以在多个不同平台上推断仇恨言语的 hate speech 检测模型。</li>
<li>methods: 我们使用了分离输入表示的方法，将输入特征分解成不同平台的特征和共同的特征，以便在不同平台上学习通用的 hate speech 检测模型。我们还学习了 causal 关系，以便更好地理解共同的表示。</li>
<li>results: 我们的模型在四个不同平台上进行了广泛的实验，结果显示我们的模型比现有的状态对方法更高效地检测通用 hate speech。<details>
<summary>Abstract</summary>
Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causal relationships, which remain constant across diverse environments, can significantly aid in understanding invariant representations in hate speech. By disentangling input into platform-dependent features (useful for predicting hate targets) and platform-independent features (used to predict the presence of hate), we learn invariant representations resistant to distribution shifts. These features are then used to predict hate speech across unseen platforms. Our extensive experiments across four platforms highlight our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:社交媒体平台，尽管它们在促进开放对话方面具有价值，但它们经常被利用来传播危险内容。现有的深度学习和自然语言处理模型在检测这种危险内容方面过于依赖于域专门的术语，这会导致它们在检测普遍的谩骂言语方面减少其能力。另一个主要挑战是当 платформы缺乏高质量的标注数据 для训练时，导致需要跨平台模型，可以适应不同的分布Shift。我们的研究推出了一种可以在不同的平台上训练的跨平台谩骂言语检测模型。为了实现良好的泛化性 across platforms，我们可以分解输入表示为不变和平台特定的特征。我们还认为，学习不变的关系，可以在多种环境中保持相同的常量，可以大幅提高对不变表示的理解。通过将输入分解为平台特定的特征（有用于预测谩骂目标）和平台独立的特征（用于预测谩骂存在），我们学习了不变的表示，抗性于分布Shift。这些特征然后用于预测谩骂言语 across 未看到的平台。我们的广泛的实验 across four platforms  highlights our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.Translated into Traditional Chinese:社交媒体平台，不过它们在促进开放对话方面具有价值，但它们经常被利用来传播危险内容。现有的深度学习和自然语言处理模型在检测这种危险内容方面过度依赖域专门的术语，这会导致它们在检测普遍的谩驳言语方面减少其能力。另一个主要挑战是当平台缺乏高质量的标注数据 для训练时，导致需要跨平台模型，可以适应不同的分布Shift。我们的研究推出了一种可以在不同的平台上训练的跨平台谩驳言语检测模型。为了实现良好的泛化性 across platforms，我们可以分解输入表示为不变和平台特定的特征。我们还认为，学习不变的关系，可以在多种环境中保持相同的常量，可以大幅提高对不变表示的理解。通过将输入分解为平台特定的特征（有用于预测谩驳目标）和平台独立的特征（用于预测谩驳存在），我们学习了不变的表示，抗性于分布Shift。这些特征然后用于预测谩驳言语 across 未看到的平台。我们的广泛的实验 across four platforms  highlights our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.
</details></li>
</ul>
<hr>
<h2 id="Specious-Sites-Tracking-the-Spread-and-Sway-of-Spurious-News-Stories-at-Scale"><a href="#Specious-Sites-Tracking-the-Spread-and-Sway-of-Spurious-News-Stories-at-Scale" class="headerlink" title="Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale"></a>Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02068">http://arxiv.org/abs/2308.02068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hans W. A. Hanley, Deepak Kumar, Zakir Durumeric</li>
<li>for: 这篇论文旨在Automatically track and analyze online news narratives to identify misinformation and support fact-checking efforts.</li>
<li>methods: 该系统使用大型自然语言模型MPNet和DP-Means归一 clustering算法，每天抓取1,404家不可靠新闻网站，以分析在线社区中流行的新闻 narative。</li>
<li>results: 研究发现2022年最受欢迎的新闻 narative，并确定了传播这些新闻 narative的最有影响力的网站。系统还可以帮助 fact-checkers like Politifact, Reuters, AP News 更快地识别和推篱虚假新闻。<details>
<summary>Abstract</summary>
Misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. However, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. In this work, utilizing daily scrapes of 1,404 unreliable news websites, the large-language model MPNet, and DP-Means clustering, we introduce a system to automatically isolate and analyze the narratives spread within online ecosystems. Identifying 55,301 narratives on these 1,404 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and magnify narratives. Finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and aid fact-checkers like Politifact, Reuters, and AP News in more quickly addressing misinformation stories.
</details>
<details>
<summary>摘要</summary>
互联网上充满谣言、宣传和谎言，一些媒体报道有危害公共健康、选举和个人安全的危险。然而，研究社区在 automatization 和 programmatic 方面对新闻媒体的跟踪仍然缺乏有效的方法。在这项工作中，我们利用每天抓取 1,404 个不可靠新闻网站的数据，大型自然语言模型 MPNet，以及 DP-Means 聚类算法，提出一个自动从在线生态系统中分离和分析新闻媒体的系统。我们分析了这些网站上的 55,301 个媒体报道，描述了在 2022 年最具影响力的新闻媒体，以及它们如何促进和强化新闻媒体。最后，我们示出了我们的系统可以帮助ifact-checkers like Politifact、Reuters 和 AP News 更快地处理谣言故事。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Task-Interference-in-Multi-Task-Learning-via-Explicit-Task-Routing-with-Non-Learnable-Primitives"><a href="#Mitigating-Task-Interference-in-Multi-Task-Learning-via-Explicit-Task-Routing-with-Non-Learnable-Primitives" class="headerlink" title="Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives"></a>Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02066">http://arxiv.org/abs/2308.02066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhichao-lu/etr-nlp-mtl">https://github.com/zhichao-lu/etr-nlp-mtl</a></li>
<li>paper_authors: Chuntao Ding, Zhichao Lu, Shangguang Wang, Ran Cheng, Vishnu Naresh Boddeti</li>
<li>for: 这个论文目的是提出一种基于非学习 primitives 和显式任务路由（ETR）的多任务学习（MTL）方法，以降低任务干扰。</li>
<li>methods: 该方法使用非学习 primitives 提取多个任务共同的特征，并将这些特征重新组合到共同分支和每个任务专门的分支中。它还使用显式任务路由来隔离学习参数，以便降低任务干扰。</li>
<li>results: 实验结果表明，ETR-NLP 在图像水平分类和像素粒度稠密预测多任务学习问题中具有显著优势，比基eline模型更高的性能，同时具有更少的学习参数和相似的计算量。代码可以在这里下载：<a target="_blank" rel="noopener" href="https://github.com/zhichao-lu/etr-nlp-mtl%E3%80%82">https://github.com/zhichao-lu/etr-nlp-mtl。</a><details>
<summary>Abstract</summary>
Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this \href{https://github.com/zhichao-lu/etr-nlp-mtl}.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）目的是学习一个模型来完成多个任务，利用任务之间的共享信息。现有的 MTL 模型却存在任务干扰的问题。减轻任务干扰的努力主要集中在损失/梯度均衡或隐式参数分割中，其中一些任务参数与其他任务参数之间存在部分重叠。在这篇论文中，我们提出了ETR-NLP，一种通过非学习性 primitives（NLPs）和显式任务路由（ETR）来减轻任务干扰的方法。我们的关键想法是使用非学习性 primitives 提取一组多样化的任务不受限制的特征，然后将其重新组合到一个共享的分支和每个任务的显式分支中。非学习性 primitives 和显式划分学习参数为共享和任务特定的一些允许我们适应性的灵活性，以最小化任务干扰。我们在图像级别的分类和像素级别的整合预测MTL问题中评估了ETR-NLP网络的效果。实验结果表明，ETR-NLP在所有数据集上都超越了当前的基eline，减少了学习参数数量和相同的FLOPs。代码可以在这里找到：https://github.com/zhichao-lu/etr-nlp-mtl。
</details></li>
</ul>
<hr>
<h2 id="On-the-Biometric-Capacity-of-Generative-Face-Models"><a href="#On-the-Biometric-Capacity-of-Generative-Face-Models" class="headerlink" title="On the Biometric Capacity of Generative Face Models"></a>On the Biometric Capacity of Generative Face Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02065">http://arxiv.org/abs/2308.02065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/human-analysis/capacity-generative-face-models">https://github.com/human-analysis/capacity-generative-face-models</a></li>
<li>paper_authors: Vishnu Naresh Boddeti, Gautam Sreekumar, Arun Ross</li>
<li>for: 本研究的目的是为了评估和比较不同的生成人脸模型，以及确定这些模型的扩展性上的最高限制。</li>
<li>methods: 本研究使用了一种统计方法来估算生成人脸图像在幂体特征空间中的生物学容量。</li>
<li>results: 研究发现，使用 ArcFace 表示法，在 false acceptance rate (FAR) 为 0.1% 时，StyleGAN3 和 DCFace 的生物学容量的最高限制分别为 $1.43\times10^6$ 和 $1.190\times10^4$。此外，随着 Desired FAR 的下降，生物学容量的估算值也降低了许多。 gender 和 age 的影响也被研究发现。<details>
<summary>Abstract</summary>
There has been tremendous progress in generating realistic faces with high fidelity over the past few years. Despite this progress, a crucial question remains unanswered: "Given a generative face model, how many unique identities can it generate?" In other words, what is the biometric capacity of the generative face model? A scientific basis for answering this question will benefit evaluating and comparing different generative face models and establish an upper bound on their scalability. This paper proposes a statistical approach to estimate the biometric capacity of generated face images in a hyperspherical feature space. We employ our approach on multiple generative models, including unconditional generators like StyleGAN, Latent Diffusion Model, and "Generated Photos," as well as DCFace, a class-conditional generator. We also estimate capacity w.r.t. demographic attributes such as gender and age. Our capacity estimates indicate that (a) under ArcFace representation at a false acceptance rate (FAR) of 0.1%, StyleGAN3 and DCFace have a capacity upper bound of $1.43\times10^6$ and $1.190\times10^4$, respectively; (b) the capacity reduces drastically as we lower the desired FAR with an estimate of $1.796\times10^4$ and $562$ at FAR of 1% and 10%, respectively, for StyleGAN3; (c) there is no discernible disparity in the capacity w.r.t gender; and (d) for some generative models, there is an appreciable disparity in the capacity w.r.t age. Code is available at https://github.com/human-analysis/capacity-generative-face-models.
</details>
<details>
<summary>摘要</summary>
在过去几年里，生成真实的脸部图像的进步很大。尽管如此，一个关键的问题仍然未得到答案：“给定一个生成脸部模型，它可以生成多少个唯一的标识？”或者说，生成脸部模型的生物 metric capacity 是多少？一个科学基础来回答这个问题将有助于评估和比较不同的生成脸部模型，并设置生成脸部模型的可扩展性的上限。本文提出了一种统计方法来估算生成脸部图像的生物 metric 容量，我们使用这种方法对多个生成模型进行了测试，包括 StyleGAN、Latent Diffusion Model 和 "Generated Photos" 等模型，以及 DCFace 等类别 conditional 生成模型。我们还估算了基于人口特征（如性别和年龄）的容量。我们的容量估算表明：（a）在 ArcFace 表示下，False Acceptance Rate (FAR) 为 0.1% 时，StyleGAN3 和 DCFace 的容量Upper Bound 分别为 $1.43\times10^6$ 和 $1.190\times10^4$；（b）随着 Desired FAR 降低，容量减少了极其剧烈， ArcFace 表示下，FAR 为 1% 和 10% 时，StyleGAN3 的容量估算为 $1.796\times10^4$ 和 $562$；（c）对于一些生成模型， gender 不存在显著的差异；（d）对于一些生成模型， age 存在可观的差异。相关代码可以在 GitHub 上找到：https://github.com/human-analysis/capacity-generative-face-models。
</details></li>
</ul>
<hr>
<h2 id="Accurate-Neural-Network-Pruning-Requires-Rethinking-Sparse-Optimization"><a href="#Accurate-Neural-Network-Pruning-Requires-Rethinking-Sparse-Optimization" class="headerlink" title="Accurate Neural Network Pruning Requires Rethinking Sparse Optimization"></a>Accurate Neural Network Pruning Requires Rethinking Sparse Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02060">http://arxiv.org/abs/2308.02060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Kuznedelev, Eldar Kurtic, Eugenia Iofinova, Elias Frantar, Alexandra Peste, Dan Alistarh</li>
<li>for: 这个论文的目的是研究在使用标准的随机优化技术进行训练稀皮网络时，稀皮性如何影响模型训练。</li>
<li>methods: 作者使用了标准的计算机视觉和自然语言处理稀皮benchmark进行研究，并提供了新的方法来 Mitigate the issue of under-training in sparse training。</li>
<li>results: 研究发现，使用标准粗糙训练策略进行稀皮训练是不优化的，而使用新提出的方法可以在计算机视觉和自然语言处理领域中实现高精度和高稀皮性的模型训练。<details>
<summary>Abstract</summary>
Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art results in both settings in the high-sparsity regime, and providing detailed analyses for the difficulty of sparse training in both scenarios. Our work sets a new threshold in terms of the accuracies that can be achieved under high sparsity, and should inspire further research into improving sparse model training, to reach higher accuracies under high sparsity, but also to do so efficiently.
</details>
<details>
<summary>摘要</summary>
In this work, we examine the impact of high sparsity on model training using standard computer vision and natural language processing sparsity benchmarks. We show that using standard dense training recipes for sparse training is suboptimal and results in under-training. We propose new approaches to mitigate this issue for both sparse pre-training of vision models (e.g., ResNet50/ImageNet) and sparse fine-tuning of language models (e.g., BERT/GLUE). Our approaches achieve state-of-the-art results in both settings in the high-sparsity regime and provide detailed analyses of the difficulty of sparse training in both scenarios. Our work sets a new threshold in terms of the accuracies that can be achieved under high sparsity and should inspire further research into improving sparse model training to reach higher accuracies under high sparsity efficiently.
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Recklessness-to-Collaborative-Filtering-based-Recommender-Systems"><a href="#Incorporating-Recklessness-to-Collaborative-Filtering-based-Recommender-Systems" class="headerlink" title="Incorporating Recklessness to Collaborative Filtering based Recommender Systems"></a>Incorporating Recklessness to Collaborative Filtering based Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02058">http://arxiv.org/abs/2308.02058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knodis-research-group/recklessness-regularization">https://github.com/knodis-research-group/recklessness-regularization</a></li>
<li>paper_authors: Diego Pérez-López, Fernando Ortega, Ángel González-Prieto, Jorge Dueñas-Lerín</li>
<li>for: 提高爆料系统的决策可靠性和创新性</li>
<li>methods: 引入一个新的学习过程中的recklessness项，用于控制决策时的风险水平</li>
<li>results: 实验结果表明，recklessness不仅能够进行风险规避，还可以提高爆料系统提供的预测量和质量。<details>
<summary>Abstract</summary>
Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
</details>
<details>
<summary>摘要</summary>
建议系统，包括一些可靠度度量，往往会变得更加保守，因为它们需要保持可靠度。这会导致建议系统的覆盖率和新颖性下降。在这篇论文中，我们提议在矩阵分解基础的建议系统学习过程中添加一个新的参数，即不可靠度，以控制决策时的风险水平。实验结果表明，不可靠度不仅允许风险调节，还可以提高建议系统提供的预测量和质量。Note: "recklessness" is a term used in the original text, and it is not a word commonly used in Chinese. I translated it as "不可靠度" (bù kě yào dù), which means "unreliability" or "riskiness".
</details></li>
</ul>
<hr>
<h2 id="Seasonality-Based-Reranking-of-E-commerce-Autocomplete-Using-Natural-Language-Queries"><a href="#Seasonality-Based-Reranking-of-E-commerce-Autocomplete-Using-Natural-Language-Queries" class="headerlink" title="Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries"></a>Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02055">http://arxiv.org/abs/2308.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Verma, Shan Zhong, Xiaoyu Liu, Adithya Rajan</li>
<li>for: 提高搜索引擎的搜寻框架中的自动完成功能，使其能够适应季节性变化。</li>
<li>methods: 使用神经网络基本概念的自然语言处理算法，将季节性变化纳入搜寻框架中。</li>
<li>results: 提出一个终端评估模型，可以将季节性变化纳入搜寻框架中，提高自动完成的相关性和商业指标。<details>
<summary>Abstract</summary>
Query autocomplete (QAC) also known as typeahead, suggests list of complete queries as user types prefix in the search box. It is one of the key features of modern search engines specially in e-commerce. One of the goals of typeahead is to suggest relevant queries to users which are seasonally important. In this paper we propose a neural network based natural language processing (NLP) algorithm to incorporate seasonality as a signal and present end to end evaluation of the QAC ranking model. Incorporating seasonality into autocomplete ranking model can improve autocomplete relevance and business metric.
</details>
<details>
<summary>摘要</summary>
查询自动完成（QAC）也称为键盘提示，是现代搜索引擎中的一个重要功能，尤其在电商领域。QAC的一个目标是为用户提供相关的查询，以便在搜索框中输入搜索。在这篇论文中，我们提出了基于人工神经网络的自然语言处理（NLP）算法，以 incorporate 季节性作为信号，并进行了端到端评估QAC排名模型。在推入季节性到搜索框中的排名模型中，可以提高搜索结果的相关性和业务指标。
</details></li>
</ul>
<hr>
<h2 id="Robust-Independence-Tests-with-Finite-Sample-Guarantees-for-Synchronous-Stochastic-Linear-Systems"><a href="#Robust-Independence-Tests-with-Finite-Sample-Guarantees-for-Synchronous-Stochastic-Linear-Systems" class="headerlink" title="Robust Independence Tests with Finite Sample Guarantees for Synchronous Stochastic Linear Systems"></a>Robust Independence Tests with Finite Sample Guarantees for Synchronous Stochastic Linear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02054">http://arxiv.org/abs/2308.02054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ambrus Tamás, Dániel Ágoston Bálint, Balázs Csanád Csáji</li>
<li>for: 这个论文是为了开发一种robust independence测试方法，可以 guaranteesignificance levels不受偏移的影响。</li>
<li>methods: 这个方法使用了信任区间估计和 permutation tests，以及一些总体依赖度测量方法，如希尔伯特-Ш密特独立性标准和距离协方差。</li>
<li>results: 这个方法可以检测非线性依赖关系，并且可以在各种不同的噪声模型下进行测试。我们还证明了这个假设测试方法的一致性下一些轻微的假设。<details>
<summary>Abstract</summary>
The paper introduces robust independence tests with non-asymptotically guaranteed significance levels for stochastic linear time-invariant systems, assuming that the observed outputs are synchronous, which means that the systems are driven by jointly i.i.d. noises. Our method provides bounds for the type I error probabilities that are distribution-free, i.e., the innovations can have arbitrary distributions. The algorithm combines confidence region estimates with permutation tests and general dependence measures, such as the Hilbert-Schmidt independence criterion and the distance covariance, to detect any nonlinear dependence between the observed systems. We also prove the consistency of our hypothesis tests under mild assumptions and demonstrate the ideas through the example of autoregressive systems.
</details>
<details>
<summary>摘要</summary>
文章介绍了一种robust独立性测试方法，可以 garantuee非对称性水平，对于随机线性时间不变系统。我们假设观测输出是同步的，即系统被共同的随机噪声驱动。我们的方法提供了不对归一化的类型I错误概率 bound，即噪声可以有任何分布。我们的算法结合信任区间估计与排序测试，以及通用的依赖度度量，如希尔伯特-尚瑟独立性 критерион和距离协方差，来检测观测系统中的非线性依赖关系。我们还证明了我们的假设检测下的假设是正确的，并通过拓扑系统的示例进行了证明。
</details></li>
</ul>
<hr>
<h2 id="A-Graphical-Approach-to-Document-Layout-Analysis"><a href="#A-Graphical-Approach-to-Document-Layout-Analysis" class="headerlink" title="A Graphical Approach to Document Layout Analysis"></a>A Graphical Approach to Document Layout Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02051">http://arxiv.org/abs/2308.02051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jilin Wang, Michael Krumdick, Baojia Tong, Hamima Halim, Maxim Sokolov, Vadym Barda, Delphine Vendryes, Chris Tanner</li>
<li>for: This paper focuses on document layout analysis (DLA) and proposes a lightweight graph neural network called GLAM to improve the efficiency of DLA models.</li>
<li>methods: The GLAM model represents each PDF page as a structured graph and frames the DLA problem as a graph segmentation and classification problem.</li>
<li>results: The GLAM model achieves competitive performance with state-of-the-art (SOTA) models on two challenging DLA datasets, with an order of magnitude fewer parameters. A simple ensemble of GLAM and a leading computer vision-based model achieves a new state-of-the-art on DocLayNet, with an increase in mean average precision (mAP) from 76.8 to 80.8.<details>
<summary>Abstract</summary>
Document layout analysis (DLA) is the task of detecting the distinct, semantic content within a document and correctly classifying these items into an appropriate category (e.g., text, title, figure). DLA pipelines enable users to convert documents into structured machine-readable formats that can then be used for many useful downstream tasks. Most existing state-of-the-art (SOTA) DLA models represent documents as images, discarding the rich metadata available in electronically generated PDFs. Directly leveraging this metadata, we represent each PDF page as a structured graph and frame the DLA problem as a graph segmentation and classification problem. We introduce the Graph-based Layout Analysis Model (GLAM), a lightweight graph neural network competitive with SOTA models on two challenging DLA datasets - while being an order of magnitude smaller than existing models. In particular, the 4-million parameter GLAM model outperforms the leading 140M+ parameter computer vision-based model on 5 of the 11 classes on the DocLayNet dataset. A simple ensemble of these two models achieves a new state-of-the-art on DocLayNet, increasing mAP from 76.8 to 80.8. Overall, GLAM is over 5 times more efficient than SOTA models, making GLAM a favorable engineering choice for DLA tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SMARLA-A-Safety-Monitoring-Approach-for-Deep-Reinforcement-Learning-Agents"><a href="#SMARLA-A-Safety-Monitoring-Approach-for-Deep-Reinforcement-Learning-Agents" class="headerlink" title="SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents"></a>SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02594">http://arxiv.org/abs/2308.02594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Zolfagharian, Manel Abdellatif, Lionel C. Briand, Ramesh S</li>
<li>for: 这篇论文旨在提出一种基于机器学习的安全监测方法，用于保障深度优化学习（DRL）Agent的安全性。</li>
<li>methods: 该方法基于黑盒（不需要访问代理的内部），利用状态抽象减少状态空间，从而使得学习安全违反预测模型的可能性更高。</li>
<li>results: 验证结果表明，SMARLA可以准确预测安全违反，false positive率低，可以在代理执行前半部分预测安全违反。<details>
<summary>Abstract</summary>
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before violations occur.
</details>
<details>
<summary>摘要</summary>
深度强化学习算法（DRL）在安全关键系统中日益被使用。保证DRL代理的安全是这些上下文中的关键问题。然而，仅仅通过测试不能保证安全，因为它不提供保证。建立安全监控器是一个解决方案，以降低这个挑战。这篇论文提出了基于机器学习的安全监控方法SMARLA，专门为DRL代理设计。由于实际原因，SMARLA采用黑盒设计（不需要代理的内部访问权限），并利用状态抽象来减少状态空间，从而使得学习代理违规预测模型从代理的状态中更加容易。我们对两个常见RL案例进行了验证。实验分析表明，SMARLA可以准确预测违规行为，false positive率较低，能够在代理执行前一半预测违规行为。
</details></li>
</ul>
<hr>
<h2 id="FuNToM-Functional-Modeling-of-RF-Circuits-Using-a-Neural-Network-Assisted-Two-Port-Analysis-Method"><a href="#FuNToM-Functional-Modeling-of-RF-Circuits-Using-a-Neural-Network-Assisted-Two-Port-Analysis-Method" class="headerlink" title="FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method"></a>FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02050">http://arxiv.org/abs/2308.02050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morteza Fayazi, Morteza Tavakoli Taba, Amirata Tabatabavakili, Ehsan Afshari, Ronald Dreslinski<br>for:这个论文主要目的是提出一种功能模型化方法，以提高RLC电路的自动化设计效率。methods:该方法使用人工智能技术，并利用两个Port分析方法，可以模型多种架构，并且仅需要一个主要数据集和多个小数据集。results:该方法可以与现有方法匹配精度，但需要训练数据的数量则被降低了2.8倍至10.9倍，并且在后期设计阶段的训练集收集时间则被降低了176.8倍至188.6倍。<details>
<summary>Abstract</summary>
Automatic synthesis of analog and Radio Frequency (RF) circuits is a trending approach that requires an efficient circuit modeling method. This is due to the expensive cost of running a large number of simulations at each synthesis cycle. Artificial intelligence methods are promising approaches for circuit modeling due to their speed and relative accuracy. However, existing approaches require a large amount of training data, which is still collected using simulation runs. In addition, such approaches collect a whole separate dataset for each circuit topology even if a single element is added or removed. These matters are only exacerbated by the need for post-layout modeling simulations, which take even longer. To alleviate these drawbacks, in this paper, we present FuNToM, a functional modeling method for RF circuits. FuNToM leverages the two-port analysis method for modeling multiple topologies using a single main dataset and multiple small datasets. It also leverages neural networks which have shown promising results in predicting the behavior of circuits. Our results show that for multiple RF circuits, in comparison to the state-of-the-art works, while maintaining the same accuracy, the required training data is reduced by 2.8x - 10.9x. In addition, FuNToM needs 176.8x - 188.6x less time for collecting the training set in post-layout modeling.
</details>
<details>
<summary>摘要</summary>
《自动化分析和设计 analog和 radio frequency（RF）电路的方法是一种流行的趋势，因为在每一个合理化周期中运行大量的 simulate 实际上是昂贵的。人工智能方法是电路模型的承诺之一，因为它们具有速度和相对准确性。然而，现有的方法需要大量的训练数据，这些数据通常通过 simulate 实际来采集。此外，这些方法每个电路结构都需要采集一个分开的数据集，即使只是添加或删除一个元素。这些问题由 Layout 模拟所加剧，它们需要更长的时间。为了解决这些问题，我们在这篇论文中提出了 FuNToM，一种功能模型方法 для RF 电路。FuNToM 利用了两个端口分析方法，可以模型多种 topology 使用单个主数据集和多个小数据集。它还利用了人工神经网络，这些神经网络在预测电路行为方面表现出色。我们的结果表明，对多个 RF 电路，相比之前的状态艺术作品，在保持同样的准确性下，需要的训练数据被减少了 2.8x - 10.9x。此外，FuNToM 在 post-layout 模拟中收集训练集的时间需要 176.8x - 188.6x  menos。
</details></li>
</ul>
<hr>
<h2 id="Deep-Maxout-Network-based-Feature-Fusion-and-Political-Tangent-Search-Optimizer-enabled-Transfer-Learning-for-Thalassemia-Detection"><a href="#Deep-Maxout-Network-based-Feature-Fusion-and-Political-Tangent-Search-Optimizer-enabled-Transfer-Learning-for-Thalassemia-Detection" class="headerlink" title="Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection"></a>Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02029">http://arxiv.org/abs/2308.02029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hemn Barzan Abdalla, Awder Ahmed, Guoquan Li, Nasser Mustafa, Abdur Rashid Sangi</li>
<li>for: 本研究旨在探讨一种基于政治向量搜索优化的深度学习方法（PTSO_TL）用于抑制遗传贫血病诊断。</li>
<li>methods: 本研究使用的方法包括数据normalization、特征融合、数据增强和深度学习模型。</li>
<li>results: 根据实验结果，PTSO_TL方法在识别遗传贫血病方面达到了最高的精度（94.3%）、回归率（96.1%）和相关度（95.2%）。<details>
<summary>Abstract</summary>
Thalassemia is a heritable blood disorder which is the outcome of a genetic defect causing lack of production of hemoglobin polypeptide chains. However, there is less understanding of the precise frequency as well as sharing in these areas. Knowing about the frequency of thalassemia occurrence and dependable mutations is thus a significant step in preventing, controlling, and treatment planning. Here, Political Tangent Search Optimizer based Transfer Learning (PTSO_TL) is introduced for thalassemia detection. Initially, input data obtained from a particular dataset is normalized in the data normalization stage. Quantile normalization is utilized in the data normalization stage, and the data are then passed to the feature fusion phase, in which Weighted Euclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data augmentation is performed using the oversampling method to increase data dimensionality. Lastly, thalassemia detection is carried out by TL, wherein a convolutional neural network (CNN) is utilized with hyperparameters from a trained model such as Xception. TL is tuned by PTSO, and the training algorithm PTSO is presented by merging of Political Optimizer (PO) and Tangent Search Algorithm (TSA). Furthermore, PTSO_TL obtained maximal precision, recall, and f-measure values of about 94.3%, 96.1%, and 95.2%, respectively.
</details>
<details>
<summary>摘要</summary>
贝壳血症是一种遗传血液疾病，由于遗传错误导致血液中不够生成含铁蛋白链。然而，贝壳血症的具体发生频率以及传递的精准性仍未得到充分理解。了解贝壳血症发生频率和可靠的突变是一项重要的步骤，以便预防、控制和治疗规划。在这里，我们引入政治弧搜索优化器基于传输学习（PTSO_TL）以检测贝壳血症。首先，输入数据从特定数据集被normalized，并使用量谱normalization进行数据归一化。然后，数据被传递到特征融合阶段，在这里使用Weighted Euclidean Distance with Deep Maxout Network（DMN）。接着，数据进行了增强处理，使用扩充方法增加数据维度。最后，贝壳血症检测由TL进行，其中使用一个具有训练模型的 convolutional neural network（CNN），并将 hyperparameters 从已训练模型 such as Xception。TL 被PTSO 调整，并且PTSO 是由政治优化器（PO）和 Tangent Search Algorithm（TSA）的 merge 所presentation。此外，PTSO_TL 在评价指标中获得了最高的准确率、回归率和准确度值，它们分别为 approximately 94.3%, 96.1%, and 95.2%。
</details></li>
</ul>
<hr>
<h2 id="Federated-Representation-Learning-for-Automatic-Speech-Recognition"><a href="#Federated-Representation-Learning-for-Automatic-Speech-Recognition" class="headerlink" title="Federated Representation Learning for Automatic Speech Recognition"></a>Federated Representation Learning for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02013">http://arxiv.org/abs/2308.02013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guruprasad V Ramesh, Gopinath Chennupati, Milind Rao, Anit Kumar Sahu, Ariya Rastrow, Jasha Droppo</li>
<li>for: 这篇论文是为了探讨 Federated Learning (FL) 和 Self-supervised Learning (SSL) 的结合，以学习 Automatic Speech Recognition (ASR) 模型，保持数据隐私。</li>
<li>methods: 这篇论文使用了 Libri-Light 语音 dataset，使用了 Speaker 和 Chapter 信息来模拟非Identical Independent Distributions (non-IID) 的数据分布，采用了 Contrastive Predictive Coding 框架和 FedSGD 进行训练。</li>
<li>results: 研究发现，使用 Federated Learning 预训练 ASR 模型，可以达到中心预训练模型的性能水平，并且在新语言 French 中进行适应，可以提高 WER 表达误差率 by 20%。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data. Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations. In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints. We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD. We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training. We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种隐私保护的方法论，允许边缘设备共同学习无需分享数据。边缘设备如 Alexa 和 Siri 是可能的无标语音数据的来源，可以用于学习 Robust 语音表示。在这项工作中，我们将 Self-supervised Learning (SSL) 和 FL 结合来学习 Automatic Speech Recognition (ASR) 的表示，尊重数据隐私约束。我们使用 Libri-Light 无标语音集中的 Speaker 和章节信息来模拟非Identical Independent Distribution (IID) 的Speaker-siloed 数据分布，并在 FedSGD 框架下预训练一个 LSTM 编码器。我们显示预训练的 ASR 编码器在 FL 中表现与中央预训练模型一样好，并且对无预训练情况下提高了12-15% (WER)。我们进一步适应了联邦预训练模型到一种新语言法语，并显示对无预训练情况下提高了20% (WER)。
</details></li>
</ul>
<hr>
<h2 id="Memory-capacity-of-two-layer-neural-networks-with-smooth-activations"><a href="#Memory-capacity-of-two-layer-neural-networks-with-smooth-activations" class="headerlink" title="Memory capacity of two layer neural networks with smooth activations"></a>Memory capacity of two layer neural networks with smooth activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02001">http://arxiv.org/abs/2308.02001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Madden, Christos Thrampoulidis</li>
<li>for: 这篇论文探讨了两层神经网络的内存容量，即最大化一般数据集的网络大小。</li>
<li>methods: 作者使用了非多项式实数Activation函数，如sigmoid和smoothed ReLU，并使用Jacobian的秩来分析网络的内存容量。</li>
<li>results: 作者发现，对于非多项式实数Activation函数，网络的内存容量至少为md&#x2F;2，并且可以达到约2倍的优化。这些结果比前一些研究更加广泛，并且可以推广到更深的模型和其他架构。<details>
<summary>Abstract</summary>
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for extending to deeper models and other architectures.
</details>
<details>
<summary>摘要</summary>
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters) is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for extending to deeper models and other architectures.Here's the translation in Traditional Chinese:决定两层神经网络中隐藏层 neuron 数目为 m，输入维度为 d（即 md+m 总可训练参数）的记忆容量是机器学习中的基本问题。对于非多项演算 activation functions，例如 sigmoid 和 smoothed rectified linear units (smoothed ReLUs)，我们设置了 md/2 的下界和约2的优化因子。这些结果与 preceded 的 results 相似，但是过去的结果仅适用于 Heaviside 和 ReLU 激活函数，而且这些激活函数的结果受到了 logarithmic 因子的影响，并且需要随机数据。从构成记忆容量的角度来看，我们查看了神经网络的雅可比安的排名，通过计算包含 Hadamard powers 和 Khati-Rao 产品的矩阵的排名。我们的计算扩展了 класиical 的线性代数实验，关于 Hadamard powers 的排名。整体而言，我们的方法与之前的工作不同，并且保持可以扩展到更深的模型和其他架构。
</details></li>
</ul>
<hr>
<h2 id="On-the-Transition-from-Neural-Representation-to-Symbolic-Knowledge"><a href="#On-the-Transition-from-Neural-Representation-to-Symbolic-Knowledge" class="headerlink" title="On the Transition from Neural Representation to Symbolic Knowledge"></a>On the Transition from Neural Representation to Symbolic Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02000">http://arxiv.org/abs/2308.02000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyan Cheng, Peter Chin</li>
<li>for: 本研究旨在bridge neural和Symbolic Representation之间的巨大差距，以便将Symbolic Thinking incorporated into neural networks的核心。</li>
<li>methods: 我们提出了一个Neural-Symbolic Transitional Dictionary Learning（TDL）框架，使用EM算法学习数据的转换表示，压缩输入数据的高维信息到一组tensor作为神经变量，自然地发现数据中隐藏的 predicate 结构。我们在 diffusion model 中对输入的分解视为合作游戏，然后通过prototype clustering来学习预测。此外，我们还使用RLEnabled by diffusion models来进一步调整学习的got prototype。</li>
<li>results: 我们在3个抽象compositional visual objects dataset上进行了广泛的实验，这些dataset需要模型可以对输入进行部分 segmentation，不含任何视觉特征，例如 texture、颜色或阴影。我们的learned representation可以带来可 interpret的 decompositions of visual input，并且在下游任务中进行了smooth的适应。这些下游任务包括神经&#x2F;Symbolic downstream tasks。<details>
<summary>Abstract</summary>
Bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. Motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. We propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. We implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. We additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. Extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.
</details>
<details>
<summary>摘要</summary>
bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. we propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. we implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. we additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.Here's a word-for-word translation of the text into Simplified Chinese:bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. we propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. we implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. we additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.
</details></li>
</ul>
<hr>
<h2 id="Explainable-unsupervised-multi-modal-image-registration-using-deep-networks"><a href="#Explainable-unsupervised-multi-modal-image-registration-using-deep-networks" class="headerlink" title="Explainable unsupervised multi-modal image registration using deep networks"></a>Explainable unsupervised multi-modal image registration using deep networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01994">http://arxiv.org/abs/2308.01994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjia Wang, Giorgos Papanastasiou</li>
<li>for: 这个论文是用于描述一种基于深度学习的多模态MRI图像匹配方法，用于临床决策。</li>
<li>methods: 该方法使用了多种MRI序列（定义为’模态’），并使用了 Grad-CAM基于解释框架来解释模型和数据之间的关系。</li>
<li>results: 该研究表明，通过 incorporating Grad-CAM解释框架，该方法可以实现高性能和可解释的多模态MRI图像匹配。<details>
<summary>Abstract</summary>
Clinical decision making from magnetic resonance imaging (MRI) combines complementary information from multiple MRI sequences (defined as 'modalities'). MRI image registration aims to geometrically 'pair' diagnoses from different modalities, time points and slices. Both intra- and inter-modality MRI registration are essential components in clinical MRI settings. Further, an MRI image processing pipeline that can address both afine and non-rigid registration is critical, as both types of deformations may be occuring in real MRI data scenarios. Unlike image classification, explainability is not commonly addressed in image registration deep learning (DL) methods, as it is challenging to interpet model-data behaviours against transformation fields. To properly address this, we incorporate Grad-CAM-based explainability frameworks in each major component of our unsupervised multi-modal and multi-organ image registration DL methodology. We previously demonstrated that we were able to reach superior performance (against the current standard Syn method). In this work, we show that our DL model becomes fully explainable, setting the framework to generalise our approach on further medical imaging data.
</details>
<details>
<summary>摘要</summary>
临床决策从核磁共振成像（MRI）结合多种MRI序列（定义为“模态”）的信息。MRI图像匹配目标是在不同模态、时间点和切片之间进行几何匹配诊断。Intra-和inter-模态MRI匹配都是临床MRI设置中的重要组件。此外，一个能够处理both afine和non-rigid匹配的MRI图像处理管道是关键，因为这两种类型的变形都可能发生在实际MRI数据场景中。不同于图像分类，explainability不是通常在图像匹配深度学习（DL）方法中被考虑的，因为它是困难 interpret模型-数据行为对于转换场景。为了正确地Address这个问题，我们在每个主要组件中都 incorporate Grad-CAM基于的解释框架。在我们之前的研究中，我们已经能够达到superior performance（相比于当前标准Syn方法）。在这项工作中，我们显示了我们的DL模型已经变得完全可解释，设置了框架可以通过更多的医疗影像数据进行普适化。
</details></li>
</ul>
<hr>
<h2 id="CartiMorph-a-framework-for-automated-knee-articular-cartilage-morphometrics"><a href="#CartiMorph-a-framework-for-automated-knee-articular-cartilage-morphometrics" class="headerlink" title="CartiMorph: a framework for automated knee articular cartilage morphometrics"></a>CartiMorph: a framework for automated knee articular cartilage morphometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01981">http://arxiv.org/abs/2308.01981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yongchengyao/cartimorph">https://github.com/yongchengyao/cartimorph</a></li>
<li>paper_authors: Yongcheng Yao, Junru Zhong, Liping Zhang, Sheheryar Khan, Weitian Chen</li>
<li>for: 这个研究的目的是发展一个准确地量化膝盖韧带组织的自动化方法，以便发现膝盖韧带组织的问题。</li>
<li>methods: 这个研究使用了深度学习模型来表现图像特征，并使用了标本建立和图像注册等方法来自动化膝盖韧带组织的量化。</li>
<li>results: 这个研究获得了膝盖韧带组织的量化结果，包括全厚度膝盖韧带损伤率（FCL）、平均厚度、表面积和体积等多个量化指标。这些量化结果显示了膝盖韧带组织的问题，并且与手动量化结果之间存在强相关。<details>
<summary>Abstract</summary>
We introduce CartiMorph, a framework for automated knee articular cartilage morphometrics. It takes an image as input and generates quantitative metrics for cartilage subregions, including the percentage of full-thickness cartilage loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the power of deep learning models for hierarchical image feature representation. Deep learning models were trained and validated for tissue segmentation, template construction, and template-to-image registration. We established methods for surface-normal-based cartilage thickness mapping, FCL estimation, and rule-based cartilage parcellation. Our cartilage thickness map showed less error in thin and peripheral regions. We evaluated the effectiveness of the adopted segmentation model by comparing the quantitative metrics obtained from model segmentation and those from manual segmentation. The root-mean-squared deviation of the FCL measurements was less than 8%, and strong correlations were observed for the mean thickness (Pearson's correlation coefficient $\rho \in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in [0.89,0.98]$) measurements. We compared our FCL measurements with those from a previous study and found that our measurements deviated less from the ground truths. We observed superior performance of the proposed rule-based cartilage parcellation method compared with the atlas-based approach. CartiMorph has the potential to promote imaging biomarkers discovery for knee osteoarthritis.
</details>
<details>
<summary>摘要</summary>
我们介绍CartiMorph，一个框架用于自动诊断膝关节软骨质量量表。它可以从图像中提取量表膝关节软骨质量量表，包括软骨质量量表的全厚度损伤率（FCL）、平均厚度、表面积和体积。CartiMorph利用深度学习模型来实现层次图像特征表示。我们在识别、构建模板和模板与图像匹配中使用深度学习模型。我们实现了基于表面法向的软骨厚度映射、FCL估计和规则基于的软骨分割。我们的软骨厚度图表示在薄和边缘区域中具有较低的错误。我们通过比较我们采用的分 segmentation模型与手动分 segmentation结果所得到的量表metric来评估模型的效果。我们发现root-mean-squared deviation of FCL measurements是less than 8%，并且在mean thickness、surface area和volume measurement中observation了强相关性（Pearson's correlation coefficient $\rho \in [0.82,0.97]$、[0.82,0.98]$和[0.89,0.98]$）。我们对我们的FCL测量与之前的研究中的参照值进行比较，发现我们的测量偏差较少。我们发现了规则基于的软骨分割方法的优越性，比Atlas-based方法更好。CartiMorph具有推动膝关节风湿病影像生物标志物的潜力。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Parkinson’s-Disease-with-Smile-An-AI-enabled-Screening-Framework"><a href="#Unmasking-Parkinson’s-Disease-with-Smile-An-AI-enabled-Screening-Framework" class="headerlink" title="Unmasking Parkinson’s Disease with Smile: An AI-enabled Screening Framework"></a>Unmasking Parkinson’s Disease with Smile: An AI-enabled Screening Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02588">http://arxiv.org/abs/2308.02588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq Adnan, Md Saiful Islam, Wasifur Rahman, Sangwu Lee, Sutapa Dey Tithi, Kazi Noshin, Imran Sarker, M Saifur Rahman, Ehsan Hoque</li>
<li>for: 预测帕金森病（PD）的诊断具有挑战性，因为没有可靠的生物标志物和有限的临床护理资源。本研究通过分析最大的视频数据集，检测PD的微表情。</li>
<li>methods: 我们使用了人脸特征点和动作单元，提取与低表情相关的特征。我们将这些特征用于一个 ensemble 模型，实现了89.7%的准确率和89.3%的接受分布函数点（AUROC）。</li>
<li>results: 我们发现，只使用笑脸视频中的特征，可以达到相似的性能，甚至在两个外部测试集上，模型没有在训练过程中看到的数据上进行了分类，这表明了PD风险评估可能通过笑脸自拍视频进行。<details>
<summary>Abstract</summary>
Parkinson's disease (PD) diagnosis remains challenging due to lacking a reliable biomarker and limited access to clinical care. In this study, we present an analysis of the largest video dataset containing micro-expressions to screen for PD. We collected 3,871 videos from 1,059 unique participants, including 256 self-reported PD patients. The recordings are from diverse sources encompassing participants' homes across multiple countries, a clinic, and a PD care facility in the US. Leveraging facial landmarks and action units, we extracted features relevant to Hypomimia, a prominent symptom of PD characterized by reduced facial expressions. An ensemble of AI models trained on these features achieved an accuracy of 89.7% and an Area Under the Receiver Operating Characteristic (AUROC) of 89.3% while being free from detectable bias across population subgroups based on sex and ethnicity on held-out data. Further analysis reveals that features from the smiling videos alone lead to comparable performance, even on two external test sets the model has never seen during training, suggesting the potential for PD risk assessment from smiling selfie videos.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Domain-specificity-and-data-efficiency-in-typo-tolerant-spell-checkers-the-case-of-search-in-online-marketplaces"><a href="#Domain-specificity-and-data-efficiency-in-typo-tolerant-spell-checkers-the-case-of-search-in-online-marketplaces" class="headerlink" title="Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces"></a>Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01976">http://arxiv.org/abs/2308.01976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayananda Ubrangala, Juhi Sharma, Ravi Prasad Kondapalli, Kiran R, Amit Agarwala, Laurent Boué</li>
<li>for: 提高在线市场场所上的拼写错误检测精度</li>
<li>methods: 使用数据扩充方法生成域限定特定的隐藏表示，并使用回归神经网络进行训练</li>
<li>results: 实现了在实时推荐API中的 typo 检测，提高了搜索效果<details>
<summary>Abstract</summary>
Typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.
</details>
<details>
<summary>摘要</summary>
typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool, especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Synthesising-Rare-Cataract-Surgery-Samples-with-Guided-Diffusion-Models"><a href="#Synthesising-Rare-Cataract-Surgery-Samples-with-Guided-Diffusion-Models" class="headerlink" title="Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models"></a>Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02587">http://arxiv.org/abs/2308.02587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meclabtuda/catasynth">https://github.com/meclabtuda/catasynth</a></li>
<li>paper_authors: Yannik Frisch, Moritz Fuchs, Antoine Sanner, Felix Anton Ucar, Marius Frenzel, Joana Wasielica-Poslednik, Adrian Gericke, Felix Mathias Wagner, Thomas Dratsch, Anirban Mukhopadhyay</li>
<li>for: 提高Automated Cataract Surgery Assistance System的发展，提供可靠的人工合成数据。</li>
<li>methods: 使用Denosing Diffusion Implicit Models（DDIM）和Classifier-Free Guidance（CFG）Conditional Generative Model Synthesize complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools.</li>
<li>results: 通过生成不同、高质量的示例，提高downstream工具分类器的性能，最高提高10%。<details>
<summary>Abstract</summary>
Cataract surgery is a frequently performed procedure that demands automation and advanced assistance systems. However, gathering and annotating data for training such systems is resource intensive. The publicly available data also comprises severe imbalances inherent to the surgical process. Motivated by this, we analyse cataract surgery video data for the worst-performing phases of a pre-trained downstream tool classifier. The analysis demonstrates that imbalances deteriorate the classifier's performance on underrepresented cases. To address this challenge, we utilise a conditional generative model based on Denoising Diffusion Implicit Models (DDIM) and Classifier-Free Guidance (CFG). Our model can synthesise diverse, high-quality examples based on complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools. We affirm that the synthesised samples display tools that the classifier recognises. These samples are hard to differentiate from real images, even for clinical experts with more than five years of experience. Further, our synthetically extended data can improve the data sparsity problem for the downstream task of tool classification. The evaluations demonstrate that the model can generate valuable unseen examples, allowing the tool classifier to improve by up to 10% for rare cases. Overall, our approach can facilitate the development of automated assistance systems for cataract surgery by providing a reliable source of realistic synthetic data, which we make available for everyone.
</details>
<details>
<summary>摘要</summary>
喉痒手术是一种常见的手术过程，需要自动化和高级帮助系统。然而，收集和标注数据 для训练这些系统是资源占用的。公共可用数据也包含了手术过程中的严重偏见。为了解决这个挑战，我们分析了喉痒手术视频数据，找到最差表现的阶段。分析结果表明，偏见会使下游工具分类器的表现在不足表现的案例下下降。为了解决这个问题，我们使用基于减噪扩散模型（DDIM）和无类标注指南（CFG）的 conditional generative model。我们的模型可以生成多样化、高质量的示例，基于复杂的多类多标签条件，如手术阶段和手术工具的组合。我们证明了生成的样本中的工具，可以由分类器识别。这些样本与真实图像很难分辨，甚至对有 более чем五年的临床经验的专业人员来说。此外，我们通过增加的数据可以改善下游任务中的数据稀缺问题。评估结果表明，我们的模型可以生成有价值的未看到的示例，使工具分类器提高至10%。总的来说，我们的方法可以促进喉痒手术自动化的发展，提供一个可靠的真实Synthetic数据源，我们将其公开给 everyone。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Agent-Policy-with-Externalities-Reward-Design-via-Bilevel-RL"><a href="#Aligning-Agent-Policy-with-Externalities-Reward-Design-via-Bilevel-RL" class="headerlink" title="Aligning Agent Policy with Externalities: Reward Design via Bilevel RL"></a>Aligning Agent Policy with Externalities: Reward Design via Bilevel RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02585">http://arxiv.org/abs/2308.02585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Dinesh Manocha, Huazheng Wang, Furong Huang, Mengdi Wang</li>
<li>for: 本研究旨在批处RL政策优化问题中的奖励函数假设，以及RL政策优化过程中的状态空间覆盖和安全性考虑。</li>
<li>methods: 本研究提出了一种级联优化问题，将主体（principal）定义为系统的更广泛目标和约束，而代理（agent）则解决Markov决策过程（MDP）。</li>
<li>results: 研究提出了主体驱动政策对应性via级联RL（PPA-BRL），该方法可有效地将代理的政策与主体的目标相吻合。研究还证明了PPA-BRL的收敛性，并通过多个示例验证了该方法的优点，包括能效地实现能源充足的操作任务、社会福利基础的税制设计以及成本效益的机器人导航。<details>
<summary>Abstract</summary>
In reinforcement learning (RL), a reward function is often assumed at the outset of a policy optimization procedure. Learning in such a fixed reward paradigm in RL can neglect important policy optimization considerations, such as state space coverage and safety. Moreover, it can fail to encompass broader impacts in terms of social welfare, sustainability, or market stability, potentially leading to undesirable emergent behavior and potentially misaligned policy. To mathematically encapsulate the problem of aligning RL policy optimization with such externalities, we consider a bilevel optimization problem and connect it to a principal-agent framework, where the principal specifies the broader goals and constraints of the system at the upper level and the agent solves a Markov Decision Process (MDP) at the lower level. The upper-level deals with learning a suitable reward parametrization corresponding to the broader goals and the lower-level deals with learning the policy for the agent. We propose Principal driven Policy Alignment via Bilevel RL (PPA-BRL), which efficiently aligns the policy of the agent with the principal's goals. We explicitly analyzed the dependence of the principal's trajectory on the lower-level policy, prove the convergence of PPA-BRL to the stationary point of the problem. We illuminate the merits of this framework in view of alignment with several examples spanning energy-efficient manipulation tasks, social welfare-based tax design, and cost-effective robotic navigation.
</details>
<details>
<summary>摘要</summary>
在增强学习（RL）中，常常假设一个奖金函数，用于policy优化过程的开始。这种固定奖金的假设可能忽略了重要的策略优化考虑因素，如状态空间覆盖率和安全性。此外，它可能无法涵盖更广泛的影响，如社会福利、可持续发展和市场稳定性，可能导致不жела的潜在行为和不一致策略。为了数学地表述RL策略优化与外部影响的问题，我们考虑了一个双层优化问题，并将其连接到一个主体-代理模型，其中主体规定系统的更广泛目标和约束，而代理在下层解决一个Markov决策过程（MDP）。上层学习一个适当的奖金参数化，与下层学习代理的策略。我们提出了主体驱动策略对齐（PPA-BRL），它高效地将代理的策略与主体的目标相对应。我们证明了PPA-BRL在站点点问题中的收敛性。我们通过一些示例，如能效的机器人 Navigation，社会福利基于税制的设计，以及成本效果的机器人 Navigation， illustrate the advantages of this framework。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-in-Large-Language-Models-Through-Symbolic-Math-Word-Problems"><a href="#Reasoning-in-Large-Language-Models-Through-Symbolic-Math-Word-Problems" class="headerlink" title="Reasoning in Large Language Models Through Symbolic Math Word Problems"></a>Reasoning in Large Language Models Through Symbolic Math Word Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01906">http://arxiv.org/abs/2308.01906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vedant Gaur, Nikunj Saunshi</li>
<li>for: 这篇论文探讨了自然语言处理（NLP）领域中大语言模型（LLM）的理解能力。</li>
<li>methods: 该论文使用了符号版本的数学Word问题（MWP）来研究LLM的理解能力，并创建了一个符号版本的SVAMP数据集。</li>
<li>results: 研究发现，使用自我提示approach可以使LLM的符号理解更加准确，并且自动提取出符号答案和数学答案之间的对应关系，从而使LLM的理解更加明确。此外，自我提示还能够提高符号准确率，超过 numeric 和 symbolic 准确率，从而实现了一种ensemble效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a "concise explanation" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be released for future research on symbolic math problems.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经革命化NLG，解决了少量或无标签数据下的下游任务。 despite their 多元能力，大型问题的理解仍然不够了解。 本文研究 math word problems（MWPs）的推理，通过研究 symbolic versions of the numeric problems，因为一个 symbolic expression 是一个 "简洁解释" 的 numeric answer。 我们创建了一个 symbolic version of the SVAMP dataset，并发现 GPT-3 的 davinci-002 模型在 symbolic MWPs 上也有良好的 zero-shot accuracy。 为了评估模型的 faithfulness，我们不仅评估了模型的准确性，还进一步评估了模型输出的推理与答案的对齐度，这与 numeric 和 symbolic 答案对应。 我们还探索了自我提示的方法，以便将 symbolic reasoning 与 numeric answer 相互适应，从而让 LLM 具备提供简洁且可靠的推理，并使其更易理解。  surprisingly，自我提示也使 symbolic 准确性高于 numeric 和 symbolic 准确性，提供了一个 ensemble 效果。 我们将 SVAMP_Sym dataset 发布给未来的研究人员对于符号数学问题进行研究。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Deformable-Convolution-for-Depth-Completion"><a href="#Revisiting-Deformable-Convolution-for-Depth-Completion" class="headerlink" title="Revisiting Deformable Convolution for Depth Completion"></a>Revisiting Deformable Convolution for Depth Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01905">http://arxiv.org/abs/2308.01905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinglong Sun, Jean Ponce, Yu-Xiong Wang</li>
<li>for: 这篇论文旨在提高深度地图的质量，具体来说是从粗糙的深度地图中生成高质量的稠密深度地图。</li>
<li>methods: 该论文提出了一种使用可变核函数卷积来单 passes地进行改进，从而解决了迭代循环的缺点，并且通过系统地调查了多种表现方法，以更好地理解可变核函数的作用和如何利用其进行深度 completion。</li>
<li>results: 研究人员通过对大规模的 KITTI 数据集进行评估，发现他们的模型在准确率和执行速度两个方面均达到了领先水平。<details>
<summary>Abstract</summary>
Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years. Previous work usually employs RGB images as guidance, and introduces iterative spatial propagation to refine estimated coarse depth maps. However, most of the propagation refinement methods require several iterations and suffer from a fixed receptive field, which may contain irrelevant and useless information with very sparse input. In this paper, we address these two challenges simultaneously by revisiting the idea of deformable convolution. We propose an effective architecture that leverages deformable kernel convolution as a single-pass refinement module, and empirically demonstrate its superiority. To better understand the function of deformable convolution and exploit it for depth completion, we further systematically investigate a variety of representative strategies. Our study reveals that, different from prior work, deformable convolution needs to be applied on an estimated depth map with a relatively high density for better performance. We evaluate our model on the large-scale KITTI dataset and achieve state-of-the-art level performance in both accuracy and inference speed. Our code is available at https://github.com/AlexSunNik/ReDC.
</details>
<details>
<summary>摘要</summary>
深度完成，目标是生成高质量的稠密深度地图从稀疏深度地图中，在最近几年内吸引了越来越多的注意力。先前的工作通常使用RGB图像作为引导，并通过迭代的空间卷积来精细化估计的粗略深度地图。然而，大多数卷积修充方法需要多个迭代和固定的接受范围，可能包含无关和无用的信息，尤其是与稀疏输入相比。在这篇论文中，我们 simultanously解决了这两个挑战，通过再次探讨可变核 convolution的想法。我们提议一种有效的架构，利用可变核 convolution作为单pass精细化模块，并经验证其超越性。为了更好地理解可变核 convolution的功能和利用其进行深度完成，我们进一步系统地调查了一些代表性的策略。我们的研究表明，与先前工作不同，可变核 convolution需要在估计的深度地图中的相对较高的密度来获得更好的性能。我们在大规模的KITTI dataset上评估了我们的模型，并在准确率和推理速度两个指标上达到了当前领域的状态码水平。我们的代码可以在https://github.com/AlexSunNik/ReDC中找到。
</details></li>
</ul>
<hr>
<h2 id="How-many-preprints-have-actually-been-printed-and-why-a-case-study-of-computer-science-preprints-on-arXiv"><a href="#How-many-preprints-have-actually-been-printed-and-why-a-case-study-of-computer-science-preprints-on-arXiv" class="headerlink" title="How many preprints have actually been printed and why: a case study of computer science preprints on arXiv"></a>How many preprints have actually been printed and why: a case study of computer science preprints on arXiv</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01899">http://arxiv.org/abs/2308.01899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialiang Lin, Yao Yu, Yu Zhou, Zhiyang Zhou, Xiaodong Shi</li>
<li>For: This paper aims to quantify the number of preprints that are eventually published in peer-reviewed venues, and to investigate the characteristics of published preprints in the field of computer science.* Methods: The authors use a case study of computer science preprints submitted to arXiv from 2008 to 2017, and employ a semantics-based mapping method using BERT to match preprints with their published versions.* Results: The authors find that 66% of all sampled preprints are published under unchanged titles and 11% are published under different titles and with other modifications. They also identify several characteristics that are associated with published preprints, including adequate revisions, multiple authorship, detailed abstract and introduction, extensive and authoritative references, and available source code.<details>
<summary>Abstract</summary>
Preprints play an increasingly critical role in academic communities. There are many reasons driving researchers to post their manuscripts to preprint servers before formal submission to journals or conferences, but the use of preprints has also sparked considerable controversy, especially surrounding the claim of priority. In this paper, a case study of computer science preprints submitted to arXiv from 2008 to 2017 is conducted to quantify how many preprints have eventually been printed in peer-reviewed venues. Among those published manuscripts, some are published under different titles and without an update to their preprints on arXiv. In the case of these manuscripts, the traditional fuzzy matching method is incapable of mapping the preprint to the final published version. In view of this issue, we introduce a semantics-based mapping method with the employment of Bidirectional Encoder Representations from Transformers (BERT). With this new mapping method and a plurality of data sources, we find that 66% of all sampled preprints are published under unchanged titles and 11% are published under different titles and with other modifications. A further analysis was then performed to investigate why these preprints but not others were accepted for publication. Our comparison reveals that in the field of computer science, published preprints feature adequate revisions, multiple authorship, detailed abstract and introduction, extensive and authoritative references and available source code.
</details>
<details>
<summary>摘要</summary>
Preprints 在学术社区中发挥越来越重要的作用。有很多原因使研究人员将文稿上传到 précis servers 之前，而不是正式提交到期刊或会议，但使用 preprints 也引起了较大的争议，特别是在优先权方面。在这篇论文中，我们对计算机科学 preprints 在 arXiv 上从 2008 年到 2017 年的 submissions 进行了案例研究，以计算这些 manuscripts 最终被 print 在 peer-reviewed venue 中的数量。其中一些已经被 published 的文稿，有些在 preprints 上没有更新，这些 manuscripts  traditional fuzzy matching 方法无法映射 preprints 到最终发表的版本。为解决这个问题，我们引入 semantics-based mapping 方法，使用 Bidirectional Encoder Representations from Transformers (BERT)。与传统方法不同的是，我们使用多种数据源，并发现了以下结果：66% 的样本 preprints 被发表不变的标题，11% 的样本 preprints 被发表并有其他修改。然后，我们进行了进一步的分析，以 investigating 为什么这些 preprints 而不是其他的被accepted  для发表。我们的比较发现，在计算机科学领域中，发表的 preprints 具有充分的修改、多个作者、详细的摘要和引言、详细的参考文献和可用的源代码。
</details></li>
</ul>
<hr>
<h2 id="Improving-Replay-Sample-Selection-and-Storage-for-Less-Forgetting-in-Continual-Learning"><a href="#Improving-Replay-Sample-Selection-and-Storage-for-Less-Forgetting-in-Continual-Learning" class="headerlink" title="Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning"></a>Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01895">http://arxiv.org/abs/2308.01895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Brignac, Niels Lobo, Abhijit Mahalanobis</li>
<li>for: 该研究旨在解决深度学习模型在进行连续学习时免受前任务卷积失忆的问题。</li>
<li>methods: 该研究使用了一种新的比较方法，与常见的储存样本方法进行对比，并提供了一种细致的分析方法来找到最佳储存样本的数量。</li>
<li>results: 该研究结果表明，使用该新的比较方法和细致的分析方法可以更好地选择最有价值的样本进行储存，从而提高连续学习的性能。<details>
<summary>Abstract</summary>
Continual learning seeks to enable deep learners to train on a series of tasks of unknown length without suffering from the catastrophic forgetting of previous tasks. One effective solution is replay, which involves storing few previous experiences in memory and replaying them when learning the current task. However, there is still room for improvement when it comes to selecting the most informative samples for storage and determining the optimal number of samples to be stored. This study aims to address these issues with a novel comparison of the commonly used reservoir sampling to various alternative population strategies and providing a novel detailed analysis of how to find the optimal number of stored samples.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:Continual learning 目标是帮助深度学习者学习一系列任务的长度未知而不受前任务忘记的影响。一种有效的解决方案是 reuse，即将前一些经验存储在内存中，并在学习当前任务时重新播放。然而，还有很多可以提高的空间，包括选择存储的最有用样本和确定存储样本的优化数量。这项研究目标是通过对通用的队列抽样与其他人口策略进行比较，并提供一种新的详细分析，以寻找最佳存储样本的数量。
</details></li>
</ul>
<hr>
<h2 id="Exact-identification-of-nonlinear-dynamical-systems-by-Trimmed-Lasso"><a href="#Exact-identification-of-nonlinear-dynamical-systems-by-Trimmed-Lasso" class="headerlink" title="Exact identification of nonlinear dynamical systems by Trimmed Lasso"></a>Exact identification of nonlinear dynamical systems by Trimmed Lasso</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01891">http://arxiv.org/abs/2308.01891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shawn L. Kiser, Mikhail Guskov, Marc Rébillat, Nicolas Ranc</li>
<li>for: 本研究旨在提出一种可以在实际数据中进行非线性动力系统标定的方法，能够处理有限长度和噪声的实际数据。</li>
<li>methods: 本研究使用了SINDy算法，以及其多种扩展，如E-SINDy和TRIM。这些方法都是基于梯度最小化的方法，但是TRIM方法可以提供更加精准的结果，并且可以在更加严重的噪声和有限数据情况下进行标定。</li>
<li>results: 本研究对三个不同的非线性动力系统进行了实验，结果表明，TRIM方法可以在有限长度和噪声的实际数据中提供更加精准的标定结果，而E-SINDy方法则可能会出现残差。此外，TRIM方法的计算成本与STLS算法相同，可以使用可 convex 的解决方法进行优化。<details>
<summary>Abstract</summary>
Identification of nonlinear dynamical systems has been popularized by sparse identification of the nonlinear dynamics (SINDy) via the sequentially thresholded least squares (STLS) algorithm. Many extensions SINDy have emerged in the literature to deal with experimental data which are finite in length and noisy. Recently, the computationally intensive method of ensembling bootstrapped SINDy models (E-SINDy) was proposed for model identification, handling finite, highly noisy data. While the extensions of SINDy are numerous, their sparsity-promoting estimators occasionally provide sparse approximations of the dynamics as opposed to exact recovery. Furthermore, these estimators suffer under multicollinearity, e.g. the irrepresentable condition for the Lasso. In this paper, we demonstrate that the Trimmed Lasso for robust identification of models (TRIM) can provide exact recovery under more severe noise, finite data, and multicollinearity as opposed to E-SINDy. Additionally, the computational cost of TRIM is asymptotically equal to STLS since the sparsity parameter of the TRIM can be solved efficiently by convex solvers. We compare these methodologies on challenging nonlinear systems, specifically the Lorenz 63 system, the Bouc Wen oscillator from the nonlinear dynamics benchmark of No\"el and Schoukens, 2016, and a time delay system describing tool cutting dynamics. This study emphasizes the comparisons between STLS, reweighted $\ell_1$ minimization, and Trimmed Lasso in identification with respect to problems faced by practitioners: the problem of finite and noisy data, the performance of the sparse regression of when the library grows in dimension (multicollinearity), and automatic methods for choice of regularization parameters.
</details>
<details>
<summary>摘要</summary>
非线性动力系统的识别已经得到了广泛的应用，通过非线性动力系统简化的逻辑（SINDy）via 随机阈值最小二乘（STLS）算法。在文献中，许多基于SINDy的扩展出现了，以处理实际数据的限定长度和噪声。最近，为了模型识别，提出了 computationally intensive的 ensemble bootstrapped SINDy模型（E-SINDy）方法。虽然扩展SINDy多种，但它们的稀疏采样器 occasionally提供稀疏的动力简化，而不是精确的回归。此外，这些采样器在多icollinearity情况下会受到影响，例如Lasso中的不可 reprehender condition。在这篇论文中，我们表明了 Trimmed Lasso 可以在更严重的噪声、有限数据和多icollinearity情况下提供精确的回归，而不是E-SINDy。此外，TRIM的计算成本是 STLS 的 asymptotic 等价，因为TRIM 的稀疏参数可以由 convex 解决器有效地解决。我们将这些方法在非线性系统中进行比较，包括 Lorenz 63 系统、Bouc Wen 振荡器和时延系统，以及2016年 No\"el 和 Schoukens 非线性动力系统比赛中的非线性动力系统 benchmark。这一研究强调了 STLS、重量 $\ell_1$ 最小化和 Trimmed Lasso 在面临实际问题时的比较：有限和噪声数据、稀疏回归在库存 grows 时的性能，以及自动选择正则化参数的问题。
</details></li>
</ul>
<hr>
<h2 id="DualCoOp-Fast-and-Effective-Adaptation-to-Multi-Label-Recognition-with-Limited-Annotations"><a href="#DualCoOp-Fast-and-Effective-Adaptation-to-Multi-Label-Recognition-with-Limited-Annotations" class="headerlink" title="DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations"></a>DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01890">http://arxiv.org/abs/2308.01890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Hu, Ximeng Sun, Stan Sclaroff, Kate Saenko</li>
<li>for: 这个研究的目的是提高多 Label 图像识别 tasks 的准确性，特别是在低标签情况下。</li>
<li>methods: 这个研究使用了一个名为 Evidence-guided Dual Context Optimization (DualCoOp++) 的框架，这是一个统一的方法来解决 partial-label 和 zero-shot multi-label 识别 задачі。DualCoOp++ 使用了不同的文本内容来分类目标类别，并且将这些内容转换为 Parametric 组件。</li>
<li>results: 实验结果显示，DualCoOp++ 在两个低标签情况下的标准多 Label 识别Benchmark上表现出色，较以前的方法更好。<details>
<summary>Abstract</summary>
Multi-label image recognition in the low-label regime is a task of great challenge and practical significance. Previous works have focused on learning the alignment between textual and visual spaces to compensate for limited image labels, yet may suffer from reduced accuracy due to the scarcity of high-quality multi-label annotations. In this research, we leverage the powerful alignment between textual and visual features pretrained with millions of auxiliary image-text pairs. We introduce an efficient and effective framework called Evidence-guided Dual Context Optimization (DualCoOp++), which serves as a unified approach for addressing partial-label and zero-shot multi-label recognition. In DualCoOp++ we separately encode evidential, positive, and negative contexts for target classes as parametric components of the linguistic input (i.e., prompts). The evidential context aims to discover all the related visual content for the target class, and serves as guidance to aggregate positive and negative contexts from the spatial domain of the image, enabling better distinguishment between similar categories. Additionally, we introduce a Winner-Take-All module that promotes inter-class interaction during training, while avoiding the need for extra parameters and costs. As DualCoOp++ imposes minimal additional learnable overhead on the pretrained vision-language framework, it enables rapid adaptation to multi-label recognition tasks with limited annotations and even unseen classes. Experiments on standard multi-label recognition benchmarks across two challenging low-label settings demonstrate the superior performance of our approach compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
多 Label 图像识别在低标签 режиме是一项具有挑战性和实际意义的任务。先前的研究通过学习文本和视觉空间之间的对应关系来资料缺乏多 Label 图像标注，但可能会受到质量不佳多 Label 图像标注的影响。在这项研究中，我们利用了强大的文本和视觉特征之间的对应关系，它们在 Millionen 个 auxiliary 图像-文本对中预训练。我们提出了一种高效可靠的框架，即 Evidence-guided Dual Context Optimization（DualCoOp++），它作为多 Label 图像识别中的一种统一方法。在 DualCoOp++ 中，我们分别编码目标类的证据、积极和消极上下文为参数化的文本输入（即提示）中的Parametric 组件。证据上下文的目的是找到目标类相关的所有视觉内容，并作为指导将空间领域中的积极和消极上下文聚合，以更好地区分相似类别。此外，我们还引入了一个 Winner-Take-All 模块，它在训练中促进类之间的交互，而不需要额外的参数和成本。由于 DualCoOp++ 对预训练的视觉语言框架做出了最小的额外学习负担，因此它可以快速适应多 Label 图像识别任务，即使具有有限的标注和未看到的类。实验表明，我们的方法在标准多 Label 图像识别标准 benchmark 上表现出优于状态的方法。
</details></li>
</ul>
<hr>
<h2 id="Cream-Skimming-the-Underground-Identifying-Relevant-Information-Points-from-Online-Forums"><a href="#Cream-Skimming-the-Underground-Identifying-Relevant-Information-Points-from-Online-Forums" class="headerlink" title="Cream Skimming the Underground: Identifying Relevant Information Points from Online Forums"></a>Cream Skimming the Underground: Identifying Relevant Information Points from Online Forums</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02581">http://arxiv.org/abs/2308.02581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe Moreno-Vera, Mateus Nogueira, Cainã Figueiredo, Daniel Sadoc Menasché, Miguel Bicudo, Ashton Woiwood, Enrico Lovat, Anton Kocheturov, Leandro Pfleger de Aguiar</li>
<li>for: 本研究提出一种基于机器学习的方法，用于在野外抓取漏洞利用情况。随着在线上讨论漏洞利用的帖子和帖子数量不断增加，需要一种自动化处理这些帖子和帖子的方法，以触发警报 Depending on their content.</li>
<li>methods: 我们使用了CrimeBB数据集，该数据集包含多个下面forum中的数据，并开发了一个监督式机器学习模型，可以过滤引用CVEs的帖子，并将其分为Proof-of-Concept、Weaponization和利用三个类别。使用Random Forest算法，我们表明可以在分类任务中达到0.99以上的准确率、精度和准确率。</li>
<li>results: 我们发现，在 weaponization和利用之间存在差异，例如解释决定树的输出，并分析了黑客社区的利益和其他相关方面。总的来说，我们的工作提供了野外漏洞利用情况的研究，可以用于提供额外的真实数据，以便更好地评估模型如EPSS和Expected Exploitability。<details>
<summary>Abstract</summary>
This paper proposes a machine learning-based approach for detecting the exploitation of vulnerabilities in the wild by monitoring underground hacking forums. The increasing volume of posts discussing exploitation in the wild calls for an automatic approach to process threads and posts that will eventually trigger alarms depending on their content. To illustrate the proposed system, we use the CrimeBB dataset, which contains data scraped from multiple underground forums, and develop a supervised machine learning model that can filter threads citing CVEs and label them as Proof-of-Concept, Weaponization, or Exploitation. Leveraging random forests, we indicate that accuracy, precision and recall above 0.99 are attainable for the classification task. Additionally, we provide insights into the difference in nature between weaponization and exploitation, e.g., interpreting the output of a decision tree, and analyze the profits and other aspects related to the hacking communities. Overall, our work sheds insight into the exploitation of vulnerabilities in the wild and can be used to provide additional ground truth to models such as EPSS and Expected Exploitability.
</details>
<details>
<summary>摘要</summary>
Note: "EPSS" stands for "Expected Potential Security Score" and "Expected Exploitability" is a metric used to measure the severity of a vulnerability.
</details></li>
</ul>
<hr>
<h2 id="Statistical-Estimation-Under-Distribution-Shift-Wasserstein-Perturbations-and-Minimax-Theory"><a href="#Statistical-Estimation-Under-Distribution-Shift-Wasserstein-Perturbations-and-Minimax-Theory" class="headerlink" title="Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory"></a>Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01853">http://arxiv.org/abs/2308.01853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrickrchao/dist_shift_exp">https://github.com/patrickrchao/dist_shift_exp</a></li>
<li>paper_authors: Patrick Chao, Edgar Dobriban</li>
<li>for: 本文研究了现代统计学中的分布Shift问题，即数据点的perturbation可能会系统地改变数据的性质。</li>
<li>methods: 本文使用 Wasserstein distribution shift，研究了每个数据点可能会受到轻微改动的情况，而不是Huber contamination模型中的一部分观察值是异常值。本文还研究了各种重要的统计问题，包括位置估计、线性回归和非 Parametric density estimation。</li>
<li>results: 本文发现，在平方损函数下的mean估计和线性回归预测错误中， sample mean和least squares estimator是相对最佳的。这些优点在独立分布shift和共同分布shift下都存在，但最差的perturbation和最大风险不同。其他问题中，提供了近似最佳的估计器和精确的finite-sample bound。本文还介绍了一些用于下界最大风险的工具，如location家族的缓和技术，以及classical工具的扩展，如最差序列的 prior、modulus of continuity、Le Cam的、Fano的和Assouad的方法。<details>
<summary>Abstract</summary>
Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For other problems, we provide nearly optimal estimators and precise finite-sample bounds. We also introduce several tools for bounding the minimax risk under distribution shift, such as a smoothing technique for location families, and generalizations of classical tools including least favorable sequences of priors, the modulus of continuity, Le Cam's, Fano's, and Assouad's methods.
</details>
<details>
<summary>摘要</summary>
现代统计学中的分布转移是一个严重的问题，因为它可能会系统性地改变数据的性质，从真实的情况偏离。我们关注 Wasserstein 分布转移，其中每个数据点都可能会经历一些微的扰动，而不是 Huber 污染模型，其中一部分观测值是异常值。我们提出并研究了分布转移的不同类型，包括共同扰动分布转移。我们分析了一些重要的统计问题，包括位置估计、线性回归和非 Parametric 密度估计。在平方损失下，我们发现了最小最大风险、最不利的扰动和 sample 均值和最小二乘估计器是相应优化的。这些优化存在独立和共同转移下都是正确的，但最不利的扰动和最大风险不同。对于其他问题，我们提供了近似优化的估计器和精确的 finite-sample 上限。我们还引入了一些用于下界最大风险的工具，包括分布转移后的平滑技术、类 least favorable 序列假设、模ulus 稳定性、Le Cam 、Fano 和 Assouad 的方法。
</details></li>
</ul>
<hr>
<h2 id="Curricular-Transfer-Learning-for-Sentence-Encoded-Tasks"><a href="#Curricular-Transfer-Learning-for-Sentence-Encoded-Tasks" class="headerlink" title="Curricular Transfer Learning for Sentence Encoded Tasks"></a>Curricular Transfer Learning for Sentence Encoded Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01849">http://arxiv.org/abs/2308.01849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jader Martins Camboim de Sá, Matheus Ferraroni Sanches, Rafael Roque de Souza, Júlio Cesar dos Reis, Leandro Aparecido Villas</li>
<li>for: 提高NLU任务中模型的表现，尤其是在数据分布变化时。</li>
<li>methods: 提出了一种逐步适应（curriculum）策略，通过数据黑客和语法分析导航进行适应。</li>
<li>results: 在我们的实验中，我们的方法比其他已知预训练方法在多语言对话任务（MultiWoZ）中获得了显著提高。<details>
<summary>Abstract</summary>
Fine-tuning language models in a downstream task is the standard approach for many state-of-the-art methodologies in the field of NLP. However, when the distribution between the source task and target task drifts, \textit{e.g.}, conversational environments, these gains tend to be diminished. This article proposes a sequence of pre-training steps (a curriculum) guided by "data hacking" and grammar analysis that allows further gradual adaptation between pre-training distributions. In our experiments, we acquire a considerable improvement from our method compared to other known pre-training approaches for the MultiWoZ task.
</details>
<details>
<summary>摘要</summary>
通常的方法是在下游任务中细化语言模型，以获得许多状态OF-THE-ART的成果。但是，当源任务和目标任务的分布发生变化，例如对话环境，这些改进往往减少。这篇文章提出了一系列的预训练步骤（课程），通过“数据黑客”和语法分析引导，以进一步适应预训练分布的变化。在我们的实验中，我们获得了与其他已知预训练方法相比较大的改进，用于多语言对话任务。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Deep-Supervision-Network-A-Noise-Resilient-Approach-for-QoS-Prediction"><a href="#Probabilistic-Deep-Supervision-Network-A-Noise-Resilient-Approach-for-QoS-Prediction" class="headerlink" title="Probabilistic Deep Supervision Network: A Noise-Resilient Approach for QoS Prediction"></a>Probabilistic Deep Supervision Network: A Noise-Resilient Approach for QoS Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02580">http://arxiv.org/abs/2308.02580</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hotfrom/pds-net">https://github.com/hotfrom/pds-net</a></li>
<li>paper_authors: Ziliang Wang, Xiaohong Zhang, Sheng Huang, Wei Zhang, Dan Yang, Meng Yan</li>
<li>for: 提高用户满意度，准确预测 unknown QoS 值</li>
<li>methods: 提出了 Probabilistic Deep Supervision Network (PDS-Net) 框架，利用 Gaussian 型概率空间进行中间层级supervision，学习known features和真实标签的概率空间</li>
<li>results: 在两个实际 QoS 数据集上进行实验评估，比对 estado-of-the-art 基elines， validate 我们的方法的有效性<details>
<summary>Abstract</summary>
Quality of Service (QoS) prediction is an essential task in recommendation systems, where accurately predicting unknown QoS values can improve user satisfaction. However, existing QoS prediction techniques may perform poorly in the presence of noise data, such as fake location information or virtual gateways. In this paper, we propose the Probabilistic Deep Supervision Network (PDS-Net), a novel framework for QoS prediction that addresses this issue. PDS-Net utilizes a Gaussian-based probabilistic space to supervise intermediate layers and learns probability spaces for both known features and true labels. Moreover, PDS-Net employs a condition-based multitasking loss function to identify objects with noise data and applies supervision directly to deep features sampled from the probability space by optimizing the Kullback-Leibler distance between the probability space of these objects and the real-label probability space. Thus, PDS-Net effectively reduces errors resulting from the propagation of corrupted data, leading to more accurate QoS predictions. Experimental evaluations on two real-world QoS datasets demonstrate that the proposed PDS-Net outperforms state-of-the-art baselines, validating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
服务质量（QoS）预测是推荐系统中的一项重要任务，可以提高用户满意度。然而，现有的QoS预测技术可能在噪声数据存在时表现不佳。在这篇论文中，我们提出了可靠性深度监督网络（PDS-Net），一种解决这个问题的新框架。PDS-Net使用 Gaussian 型概率空间来监督中间层，并学习概率空间 для已知特征和真实标签。此外，PDS-Net 使用基于条件的多任务损失函数来识别具有噪声数据的对象，并直接将深度特征从概率空间中抽取到真实标签的概率空间中进行监督。因此，PDS-Net 可以减少噪声数据的传播错误，从而提高 QoS 预测的准确性。实验评估在两个真实 QoS 数据集上表明，提出的 PDS-Net 已经超越了状态艺术基eline。
</details></li>
</ul>
<hr>
<h2 id="URET-Universal-Robustness-Evaluation-Toolkit-for-Evasion"><a href="#URET-Universal-Robustness-Evaluation-Toolkit-for-Evasion" class="headerlink" title="URET: Universal Robustness Evaluation Toolkit (for Evasion)"></a>URET: Universal Robustness Evaluation Toolkit (for Evasion)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01840">http://arxiv.org/abs/2308.01840</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/uret">https://github.com/ibm/uret</a></li>
<li>paper_authors: Kevin Eykholt, Taesung Lee, Douglas Schales, Jiyong Jang, Ian Molloy, Masha Zorin</li>
<li>for: 本研究旨在提高机器学习模型的安全和可靠性，通过生成可逃脱攻击的输入，以帮助确保AI任务的正确性和可靠性。</li>
<li>methods: 本研究提出了一种新的框架，可以生成不同输入类型和任务领域的攻击输入。该框架使用给定的输入变换集合，找到一个符合semantic和功能要求的攻击输入序列。</li>
<li>results: 本研究在多种不同的机器学习任务和输入表示中展示了框架的通用性。此外，研究还表明了生成攻击示例的重要性，以便应用防御技术。<details>
<summary>Abstract</summary>
Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples as they enable the deployment of mitigation techniques.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a new framework to generate adversarial inputs regardless of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples, as they enable the deployment of mitigation techniques.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/04/cs.LG_2023_08_04/" data-id="clltau92n005lcr887b5pharp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/cs.SD_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/04/cs.SD_2023_08_04/">cs.SD - 2023-08-04 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Monaural-Speech-Enhancement-using-Spectrum-Attention-Fusion"><a href="#Efficient-Monaural-Speech-Enhancement-using-Spectrum-Attention-Fusion" class="headerlink" title="Efficient Monaural Speech Enhancement using Spectrum Attention Fusion"></a>Efficient Monaural Speech Enhancement using Spectrum Attention Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02263">http://arxiv.org/abs/2308.02263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyu Long, Jetic Gū, Binhao Bai, Zhibo Yang, Ping Wei, Junli Li</li>
<li>for: 提高自动 speech 处理管道中的speech减噪性能，以提高干扰 speech 的分离效果。</li>
<li>methods: 提出了一种 Spectrum Attention Fusion 技术，用于将自我注意力 fusion 与 spectral 特征 fusion 结合，以提高模型的表达能力和效率。</li>
<li>results: 在 Voice Bank + DEMAND 数据集上，与 state-of-the-art 模型比较，提出的模型能够达到相当或更好的结果，同时具有较少的参数（0.58M）。<details>
<summary>Abstract</summary>
Speech enhancement is a demanding task in automated speech processing pipelines, focusing on separating clean speech from noisy channels. Transformer based models have recently bested RNN and CNN models in speech enhancement, however at the same time they are much more computationally expensive and require much more high quality training data, which is always hard to come by. In this paper, we present an improvement for speech enhancement models that maintains the expressiveness of self-attention while significantly reducing model complexity, which we have termed Spectrum Attention Fusion. We carefully construct a convolutional module to replace several self-attention layers in a speech Transformer, allowing the model to more efficiently fuse spectral features. Our proposed model is able to achieve comparable or better results against SOTA models but with significantly smaller parameters (0.58M) on the Voice Bank + DEMAND dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Speech enhancement 是自动化语音处理流程中的一个要求，旨在分离含杂的语音和清晰的语音。基于Transformer的模型在最近的Speech enhancement中表现出色，但同时它们也更加计算昂贵，需要更多高质量的训练数据，这并不容易获得。在这篇论文中，我们提出了一种改进 speech enhancement 模型，保持了自注意的表达力，同时显著减少模型的复杂度，我们称之为 Spectrum Attention Fusion。我们在一个 convolutional 模块中代替了一些自注意层，让模型更有效地融合频谱特征。我们的提议模型在 Voice Bank + DEMAND 数据集上可以达到与顶峰模型相当或更好的结果，但具有远小于参数（0.58M）。
</details></li>
</ul>
<hr>
<h2 id="Emo-DNA-Emotion-Decoupling-and-Alignment-Learning-for-Cross-Corpus-Speech-Emotion-Recognition"><a href="#Emo-DNA-Emotion-Decoupling-and-Alignment-Learning-for-Cross-Corpus-Speech-Emotion-Recognition" class="headerlink" title="Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition"></a>Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02190">http://arxiv.org/abs/2308.02190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaxin-ye/emo-dna">https://github.com/jiaxin-ye/emo-dna</a></li>
<li>paper_authors: Jiaxin Ye, Yujie Wei, Xin-Cheng Wen, Chenglong Ma, Zhizhong Huang, Kunhong Liu, Hongming Shan</li>
<li>for: 这个研究的目的是将拥有不同 Corpora 的语音情感识别系统进行整合，以提高其在不同 Corpora 上的表现。</li>
<li>methods: 这个研究提出了一个名为 Emotion Decoupling aNd Alignment 的新框架，它使用了对照分离和双层情感对齐来学习语音情感识别系统。</li>
<li>results: 实验结果显示，这个新框架在多个跨 Corpora 的情感识别任务中表现更好，比起现有的方法。<details>
<summary>Abstract</summary>
Cross-corpus speech emotion recognition (SER) seeks to generalize the ability of inferring speech emotion from a well-labeled corpus to an unlabeled one, which is a rather challenging task due to the significant discrepancy between two corpora. Existing methods, typically based on unsupervised domain adaptation (UDA), struggle to learn corpus-invariant features by global distribution alignment, but unfortunately, the resulting features are mixed with corpus-specific features or not class-discriminative. To tackle these challenges, we propose a novel Emotion Decoupling aNd Alignment learning framework (EMO-DNA) for cross-corpus SER, a novel UDA method to learn emotion-relevant corpus-invariant features. The novelties of EMO-DNA are two-fold: contrastive emotion decoupling and dual-level emotion alignment. On one hand, our contrastive emotion decoupling achieves decoupling learning via a contrastive decoupling loss to strengthen the separability of emotion-relevant features from corpus-specific ones. On the other hand, our dual-level emotion alignment introduces an adaptive threshold pseudo-labeling to select confident target samples for class-level alignment, and performs corpus-level alignment to jointly guide model for learning class-discriminative corpus-invariant features across corpora. Extensive experimental results demonstrate the superior performance of EMO-DNA over the state-of-the-art methods in several cross-corpus scenarios. Source code is available at https://github.com/Jiaxin-Ye/Emo-DNA.
</details>
<details>
<summary>摘要</summary>
cross-corpus speech emotion recognition (SER) 提高了推断语音情绪的能力，从一个很好地标注的 corpora 扩展到另一个没有标注的 corpora，这是一项非常具有挑战性的任务，因为两个 corpora 之间存在很大的差异。现有的方法通常基于无监督领域适应 (UDA)，尝试通过全局分布对齐来学习 corpora  invariant 特征，但是 unfortunately，得到的特征都是混合 corpora 特定特征或不是类别特征。为了解决这些挑战，我们提出了一个新的 Emotion Decoupling and Alignment learning framework (EMO-DNA)  для cross-corpus SER，一种新的 UDA 方法来学习情绪相关的 corpora  invariant 特征。EMO-DNA 的两大创新是：对比情绪分离和双级情绪对接。一方面，我们的对比情绪分离通过对比分离损失来强化情绪相关特征与 corpora 特定特征之间的分离性。另一方面，我们的双级情绪对接引入了一个 adaptive 阈值 pseudo-labeling，选择 confidence 的目标样本进行类别对接，并在 corpora 级别对接以协助模型学习类别特征的 cross-corpus 普适性。我们的实验结果表明，EMO-DNA 在多个 cross-corpus 场景中表现出了与当前状态OF 法的超越性。代码可以在 <https://github.com/Jiaxin-Ye/Emo-DNA> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Capturing-Spectral-and-Long-term-Contextual-Information-for-Speech-Emotion-Recognition-Using-Deep-Learning-Techniques"><a href="#Capturing-Spectral-and-Long-term-Contextual-Information-for-Speech-Emotion-Recognition-Using-Deep-Learning-Techniques" class="headerlink" title="Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques"></a>Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04517">http://arxiv.org/abs/2308.04517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samiul Islam, Md. Maksudul Haque, Abu Jobayer Md. Sadat</li>
<li>for: 本研究旨在超越传统的speech emotion recognition方法（如LSTM、CNN、RNN、SVM、MLP），这些方法具有难以捕捉长期依赖关系、捕捉时间动态和捕捉复杂模式关系等缺陷。</li>
<li>methods: 本研究提出了一个 ensemble 模型，该模型将文本数据处理GCN（图 convolutional networks）和音频信号分析 HuBERT trasformer 相结合。GCN 可以利用文本的图形表示，捕捉文本中的长期Contextual 依赖关系和 semantics 关系，而 HuBERT 通过自我注意机制，可以捕捉音频信号中的长期依赖关系，捕捉时间动态。</li>
<li>results: 结果表明，将 GCN 和 HuBERT 相结合，可以充分利用这两种方法的优势，同时分析多Modal 数据，并将这些模式相互融合，从而提高情绪识别系统的准确性。<details>
<summary>Abstract</summary>
Traditional approaches in speech emotion recognition, such as LSTM, CNN, RNN, SVM, and MLP, have limitations such as difficulty capturing long-term dependencies in sequential data, capturing the temporal dynamics, and struggling to capture complex patterns and relationships in multimodal data. This research addresses these shortcomings by proposing an ensemble model that combines Graph Convolutional Networks (GCN) for processing textual data and the HuBERT transformer for analyzing audio signals. We found that GCNs excel at capturing Long-term contextual dependencies and relationships within textual data by leveraging graph-based representations of text and thus detecting the contextual meaning and semantic relationships between words. On the other hand, HuBERT utilizes self-attention mechanisms to capture long-range dependencies, enabling the modeling of temporal dynamics present in speech and capturing subtle nuances and variations that contribute to emotion recognition. By combining GCN and HuBERT, our ensemble model can leverage the strengths of both approaches. This allows for the simultaneous analysis of multimodal data, and the fusion of these modalities enables the extraction of complementary information, enhancing the discriminative power of the emotion recognition system. The results indicate that the combined model can overcome the limitations of traditional methods, leading to enhanced accuracy in recognizing emotions from speech.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="N-gram-Boosting-Improving-Contextual-Biasing-with-Normalized-N-gram-Targets"><a href="#N-gram-Boosting-Improving-Contextual-Biasing-with-Normalized-N-gram-Targets" class="headerlink" title="N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets"></a>N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02092">http://arxiv.org/abs/2308.02092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang Yau Li, Shreekantha Nadig, Karol Chang, Zafarullah Mahmood, Riqiang Wang, Simon Vandieken, Jonas Robertson, Fred Mailhot</li>
<li>for: 提高 keywords 识别率</li>
<li>methods: 使用 two-step keyword boosting mechanism，Normalize unigrams 和 n-grams，避免 missing hits 和 over-boosting multi-token keywords</li>
<li>results: 提高 keyword recognition rate by 26% Relative on proprietary in-domain dataset，和 2% on LibriSpeechI hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Accurate transcription of proper names and technical terms is particularly important in speech-to-text applications for business conversations. These words, which are essential to understanding the conversation, are often rare and therefore likely to be under-represented in text and audio training data, creating a significant challenge in this domain. We present a two-step keyword boosting mechanism that successfully works on normalized unigrams and n-grams rather than just single tokens, which eliminates missing hits issues with boosting raw targets. In addition, we show how adjusting the boosting weight logic avoids over-boosting multi-token keywords. This improves our keyword recognition rate by 26% relative on our proprietary in-domain dataset and 2% on LibriSpeech. This method is particularly useful on targets that involve non-alphabetic characters or have non-standard pronunciations.
</details>
<details>
<summary>摘要</summary>
精准转写特有名称和技术术语 particualrly important in speech-to-text应用程序中，这些词语是理解对话的关键，但它们通常是罕见的，因此在文本和音频训练数据中受到抑制。我们提出了一种两步关键词强化机制，该机制可以在 норма化单个字和n-gram中工作，而不是只是单个token，这将消除 raw 目标中的缺失命中问题。此外，我们还证明了如何调整强化权重逻辑，以避免多token关键被过度强化。这将提高我们的关键识别率达26%，相对于我们的自有领域数据集，并且2% 在 LibriSpeech 上。这种方法特别有用于targets 中包含非字母字符或非标准发音。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/04/cs.SD_2023_08_04/" data-id="clltau93p008kcr88fe4h5etn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/7/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/9/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
