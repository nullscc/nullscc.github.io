
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/8/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.SD_2023_08_12/" class="article-date">
  <time datetime="2023-08-11T16:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.SD_2023_08_12/">cs.SD - 2023-08-12 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Alternative-Pseudo-Labeling-for-Semi-Supervised-Automatic-Speech-Recognition"><a href="#Alternative-Pseudo-Labeling-for-Semi-Supervised-Automatic-Speech-Recognition" class="headerlink" title="Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition"></a>Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06547">http://arxiv.org/abs/2308.06547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhu, Dongji Gao, Gaofeng Cheng, Daniel Povey, Pengyuan Zhang, Yonghong Yan</li>
<li>for: 提高自动语音识别器的性能在半监督学习中，当标注数据稀缺时</li>
<li>methods: 提出了一种名为替代pseudo-标签的框架，包括一个通用的CTC损失函数、一种自信度基于的错误检测方法和一种自动调整的阈值调整方法</li>
<li>results: 对比于传统的pseudo-标签筛选和改善pseudo-标签质量的方法，替代pseudo-标签的框架能够更好地适应噪音pseudo-标签的情况，并且不需要手动调整阈值<details>
<summary>Abstract</summary>
When labeled data is insufficient, semi-supervised learning with the pseudo-labeling technique can significantly improve the performance of automatic speech recognition. However, pseudo-labels are often noisy, containing numerous incorrect tokens. Taking noisy labels as ground-truth in the loss function results in suboptimal performance. Previous works attempted to mitigate this issue by either filtering out the nosiest pseudo-labels or improving the overall quality of pseudo-labels. While these methods are effective to some extent, it is unrealistic to entirely eliminate incorrect tokens in pseudo-labels. In this work, we propose a novel framework named alternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the perspective of the training objective. The framework comprises several components. Firstly, a generalized CTC loss function is introduced to handle noisy pseudo-labels by accepting alternative tokens in the positions of incorrect tokens. Applying this loss function in pseudo-labeling requires detecting incorrect tokens in the predicted pseudo-labels. In this work, we adopt a confidence-based error detection method that identifies the incorrect tokens by comparing their confidence scores with a given threshold, thus necessitating the confidence score to be discriminative. Hence, the second proposed technique is the contrastive CTC loss function that widens the confidence gap between the correctly and incorrectly predicted tokens, thereby improving the error detection ability. Additionally, obtaining satisfactory performance with confidence-based error detection typically requires extensive threshold tuning. Instead, we propose an automatic thresholding method that uses labeled data as a proxy for determining the threshold, thus saving the pain of manual tuning.
</details>
<details>
<summary>摘要</summary>
当标注数据不充分时，半超vised学习采用 pseudo-labeling 技术可以显著提高自动语音识别的性能。然而，pseudo-标签通常含有许多错误的符号。将含有错误符号的标签作为真实标签在损失函数中使用会导致优化性不佳。先前的工作已经尝试过过滤 pseudo-标签中最含错误的符号或者提高 pseudo-标签的质量。虽然这些方法有一定的效果，但是完全消除 incorrect 符号是不现实的。在这种情况下，我们提出了一种新的框架，即 alternative pseudo-labeling，以解决 pseudo-标签中含有错误符号的问题。该框架包括以下几个组成部分：1. 一种通用的 CTC 损失函数，可以处理含有错误符号的 pseudo-标签。这种损失函数接受在 incorrect 符号的位置上的多个选项，以便在损失函数中处理多个可能的符号。2. 一种 confidence-based 错误检测方法，可以在预测 pseudo-标签时检测 incorrect 符号。这种方法比较预测 pseudo-标签中的符号 confidence 分数与一定的阈值，并将不符合阈值的符号标记为错误符号。3. 一种自动调整 threshold 的方法，可以使用标注数据作为代理，以便在不需要手动调整的情况下，自动地调整 threshold。通过这些技术，我们可以提高半超vised 学习中 pseudo-标签中含有错误符号的性能。
</details></li>
</ul>
<hr>
<h2 id="BigWavGAN-A-Wave-To-Wave-Generative-Adversarial-Network-for-Music-Super-Resolution"><a href="#BigWavGAN-A-Wave-To-Wave-Generative-Adversarial-Network-for-Music-Super-Resolution" class="headerlink" title="BigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music Super-Resolution"></a>BigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06483">http://arxiv.org/abs/2308.06483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yenan Zhang, Hiroshi Watanabe</li>
<li>for: 本研究旨在解决深度神经网络（DNNs）在音频超解像（SR）中的高性能问题，即大型DNN模型在SR中无法 producen高质量结果。</li>
<li>methods: 本研究提出了BigWavGAN模型，它通过结合大规模波形模型Demucs，SOTA的检测器和对抗训练策略来解锁大型DNN模型在SR中的潜力。我们的检测器包括多尺度检测器（MSD）和多分辨率检测器（MRD）。</li>
<li>results: 对jective和对jective评价表明，BigWavGAN在音频SR中具有显著高的感知质量，超过了基eline模型。此外，BigWavGAN在 simulated和实际场景中也超过了SOTA音频SR模型。同时，BigWavGAN还能够Address out-of-distribution数据的泛化问题。<details>
<summary>Abstract</summary>
Generally, Deep Neural Networks (DNNs) are expected to have high performance when their model size is large. However, large models failed to produce high-quality results commensurate with their scale in music Super-Resolution (SR). We attribute this to that DNNs cannot learn information commensurate with their size from standard mean square error losses. To unleash the potential of large DNN models in music SR, we propose BigWavGAN, which incorporates Demucs, a large-scale wave-to-wave model, with State-Of-The-Art (SOTA) discriminators and adversarial training strategies. Our discriminator consists of Multi-Scale Discriminator (MSD) and Multi-Resolution Discriminator (MRD). During inference, since only the generator is utilized, there are no additional parameters or computational resources required compared to the baseline model Demucs. Objective evaluation affirms the effectiveness of BigWavGAN in music SR. Subjective evaluations indicate that BigWavGAN can generate music with significantly high perceptual quality over the baseline model. Notably, BigWavGAN surpasses the SOTA music SR model in both simulated and real-world scenarios. Moreover, BigWavGAN represents its superior generalization ability to address out-of-distribution data. The conducted ablation study reveals the importance of our discriminators and training strategies. Samples are available on the demo page.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)通常情况下，深度神经网络（DNNs）预期在大型模型下表现出色。然而，大型模型在音乐超分辨率（SR）领域并未能够生成高质量结果，我们认为这是因为DNNs无法从标准的平方差损失中学习足够的信息。为了解 liberate大型DNN模型在音乐SR中的潜力，我们提出了BigWavGAN，它包括了大规模的波形模型Demucs，以及当前最佳的检测器和对抗训练策略。我们的检测器包括多尺度检测器（MSD）和多分辨率检测器（MRD）。在推理过程中，只有生成器被使用，因此无需额外的参数或计算资源，与Demucs相比。对象评估表明BigWavGAN在音乐SR中的效果极佳。主观评估表明BigWavGAN可以生成高度感知质量的音乐，超过基准模型。此外，BigWavGAN表现出了更好的总体化能力，可以 Addressing out-of-distribution data。我们进行了一项ablation Study，表明我们的检测器和训练策略的重要性。样本可以在 demo 页面上找到。
</details></li>
</ul>
<hr>
<h2 id="Bilingual-Streaming-ASR-with-Grapheme-units-and-Auxiliary-Monolingual-Loss"><a href="#Bilingual-Streaming-ASR-with-Grapheme-units-and-Auxiliary-Monolingual-Loss" class="headerlink" title="Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss"></a>Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06327">http://arxiv.org/abs/2308.06327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Soleymanpour, Mahmoud Al Ismail, Fahimeh Bahmaninezhad, Kshitiz Kumar, Jian Wu</li>
<li>for: 这个论文是为了提供一种混合自动声音识别（ASR）设置下的英语作为次要地区的双语解决方案。</li>
<li>methods: 作者们提出了以下四个关键发展：（a）用图形单元代替语音单元的发音词典，（b）完全双语对应模型和随后的双语流Transformer模型，（c）并行编码结构和语言标识（LID）损失，（d）并行编码器的辅助损失 для单语jective。</li>
<li>results: 作者们通过大规模训练和测试任务来评估他们的工作，并发现他们的提出的辅助损失可以更好地特化并行编码器到各自的单语本地，从而使得双语学习更加强。特别是，双语IT模型在一个code-mix IT任务中提高了单词错误率（WER）从46.5%降低到13.8%，同时也与单语IT模型（9.5%）在IT测试任务上几乎占据了同等水平（9.6%）。<details>
<summary>Abstract</summary>
We introduce a bilingual solution to support English as secondary locale for most primary locales in hybrid automatic speech recognition (ASR) settings. Our key developments constitute: (a) pronunciation lexicon with grapheme units instead of phone units, (b) a fully bilingual alignment model and subsequently bilingual streaming transformer model, (c) a parallel encoder structure with language identification (LID) loss, (d) parallel encoder with an auxiliary loss for monolingual projections. We conclude that in comparison to LID loss, our proposed auxiliary loss is superior in specializing the parallel encoders to respective monolingual locales, and that contributes to stronger bilingual learning. We evaluate our work on large-scale training and test tasks for bilingual Spanish (ES) and bilingual Italian (IT) applications. Our bilingual models demonstrate strong English code-mixing capability. In particular, the bilingual IT model improves the word error rate (WER) for a code-mix IT task from 46.5% to 13.8%, while also achieving a close parity (9.6%) with the monolingual IT model (9.5%) over IT tests.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种双语解决方案，以支持英语为次要本地语言在混合自动语音识别（ASR）设置中。我们的关键发展包括：（a）使用字符单位 instead of 语音单位的发音词典，（b）完全双语对应模型和随后的双语流Transformer模型，（c）并行编码结构与语言标识（LID）损失，（d）并行编码器和辅助损失 для单语Project。我们 conclude that在比较LID损失的情况下，我们提posed的辅助损失能够特化并行编码器到各自的单语本地语言，从而为双语学习提供更强的支持。我们在大规模训练和测试任务上评估了我们的工作，并在双语西班牙（ES）和双语意大利（IT）应用中显示出了英语代码混合能力。特别是，双语IT模型在一个代码混合IT任务上从46.5%降至13.8%，并同时与单语IT模型（9.5%）在IT测试上达到了9.6%的相似性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.SD_2023_08_12/" data-id="clly4xtep009wvl8850oreeh0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/eess.IV_2023_08_12/" class="article-date">
  <time datetime="2023-08-11T16:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/eess.IV_2023_08_12/">eess.IV - 2023-08-12 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Semantic-Communications-with-Explicit-Semantic-Base-for-Image-Transmission"><a href="#Semantic-Communications-with-Explicit-Semantic-Base-for-Image-Transmission" class="headerlink" title="Semantic Communications with Explicit Semantic Base for Image Transmission"></a>Semantic Communications with Explicit Semantic Base for Image Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06599">http://arxiv.org/abs/2308.06599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Zheng, Fengyu Wang, Wenjun Xu, Miao Pan, Ping Zhang</li>
<li>for: 该 paper 的目的是提出一种基于协同知识 (Seb) 的 semantic image transmission 框架，以确保信息的含义在传输过程中得到正确的理解和传输。</li>
<li>methods: 该 paper 使用了 Seb 生成器和 Seb 基于图像编码&#x2F;解码器来表示图像，并使用 E2E 训练来优化核心组件。</li>
<li>results: 对比 state-of-art 方法，该 paper 在不同 SNR 下达到了 0.5-1.5 dB 的 PSNR 提升。<details>
<summary>Abstract</summary>
Semantic communications, aiming at ensuring the successful delivery of the meaning of information, are expected to be one of the potential techniques for the next generation communications. However, the knowledge forming and synchronizing mechanism that enables semantic communication systems to extract and interpret the semantics of information according to the communication intents is still immature. In this paper, we propose a semantic image transmission framework with explicit semantic base (Seb), where Sebs are generated and employed as the knowledge shared between the transmitter and the receiver with flexible granularity. To represent images with Sebs, a novel Seb-based reference image generator is proposed to generate Sebs and then decompose the transmitted images. To further encode/decode the residual information for precise image reconstruction, a Seb-based image encoder/decoder is proposed. The key components of the proposed framework are optimized jointly by end-to-end (E2E) training, where the loss function is dedicated designed to tackle the problem of nondifferentiable operation in Seb-based reference image generator by introducing a gradient approximation mechanism. Extensive experiments show that the proposed framework outperforms state-of-art works by 0.5 - 1.5 dB in peak signal-to-noise ratio (PSNR) w.r.t. different signal-to-noise ratio (SNR).
</details>
<details>
<summary>摘要</summary>
semantic communication, aiming at ensuring the successful delivery of information meaning, is expected to be one of the potential techniques for next-generation communications. however, the knowledge forming and synchronizing mechanism that enables semantic communication systems to extract and interpret the semantics of information according to communication intents is still immature. in this paper, we propose a semantic image transmission framework with explicit semantic base (Seb), where Sebs are generated and employed as the knowledge shared between the transmitter and the receiver with flexible granularity. to represent images with Sebs, a novel Seb-based reference image generator is proposed to generate Sebs and then decompose the transmitted images. to further encode/decode the residual information for precise image reconstruction, a Seb-based image encoder/decoder is proposed. the key components of the proposed framework are optimized jointly by end-to-end (E2E) training, where the loss function is dedicated designed to tackle the problem of nondifferentiable operation in Seb-based reference image generator by introducing a gradient approximation mechanism. extensive experiments show that the proposed framework outperforms state-of-art works by 0.5 - 1.5 dB in peak signal-to-noise ratio (PSNR) w.r.t. different signal-to-noise ratio (SNR).
</details></li>
</ul>
<hr>
<h2 id="On-Versatile-Video-Coding-at-UHD-with-Machine-Learning-Based-Super-Resolution"><a href="#On-Versatile-Video-Coding-at-UHD-with-Machine-Learning-Based-Super-Resolution" class="headerlink" title="On Versatile Video Coding at UHD with Machine-Learning-Based Super-Resolution"></a>On Versatile Video Coding at UHD with Machine-Learning-Based Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06570">http://arxiv.org/abs/2308.06570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristian Fischer, Christian Herglotz, André Kaup</li>
<li>for: 提高4K数据的编码质量</li>
<li>methods: 使用Machine Learning基于单张超解析算法和下一代VVC编码器</li>
<li>results: 可以在低比特率场景下获得12%-18%的Bjontegaard delta rate提升，并且减少了压缩残留和扩散 artifacts。<details>
<summary>Abstract</summary>
Coding 4K data has become of vital interest in recent years, since the amount of 4K data is significantly increasing. We propose a coding chain with spatial down- and upscaling that combines the next-generation VVC codec with machine learning based single image super-resolution algorithms for 4K. The investigated coding chain, which spatially downscales the 4K data before coding, shows superior quality than the conventional VVC reference software for low bitrate scenarios. Throughout several tests, we find that up to 12 % and 18 % Bjontegaard delta rate gains can be achieved on average when coding 4K sequences with VVC and QP values above 34 and 42, respectively. Additionally, the investigated scenario with up- and downscaling helps to reduce the loss of details and compression artifacts, as it is shown in a visual example.
</details>
<details>
<summary>摘要</summary>
“ coding 4K 数据在近年变得非常重要，因为4K 数据量在增长。我们提出了一个 coding chain，其 combining 下一代 VVC 编码器和基于机器学习的单张图像超分辨算法，用于4K。我们调查的 coding chain，先将4K 数据进行空间下降scaling，然后编码，在低比特率场景下显示出超越传统 VVC 参考软件的质量。在多个测试中，我们发现，在 VVC 和 QP 值高于 34 和 42 时，可以获得12% 到 18% Bjontegaard delta Rate 增强。此外，我们发现，这种升降scaling 场景可以避免失 Details 和压缩残差的损失，如图例所示。”Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Three-dimensional-echo-shifted-EPI-with-simultaneous-blip-up-and-blip-down-acquisitions-for-correcting-geometric-distortion"><a href="#Three-dimensional-echo-shifted-EPI-with-simultaneous-blip-up-and-blip-down-acquisitions-for-correcting-geometric-distortion" class="headerlink" title="Three-dimensional echo-shifted EPI with simultaneous blip-up and blip-down acquisitions for correcting geometric distortion"></a>Three-dimensional echo-shifted EPI with simultaneous blip-up and blip-down acquisitions for correcting geometric distortion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06557">http://arxiv.org/abs/2308.06557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaibao Sun, Zhifeng Chen, Guangyu Dan, Qingfei Luo, Lirong Yan, Feng Liu, Xiaohong Joe Zhou</li>
<li>for: 提高echo-planar imaging（EPI）的图像质量和动态误差 corrections，使其适用于更多应用。</li>
<li>methods: 使用三维（3D）echo-shifted EPI BUDA（esEPI-BUDA）技术，通过生成两个EPI读取轨迹，以实现单shot acquiring两个blip-up和blip-down数据集。</li>
<li>results: 在phantom和人类大脑图像中， geometric distortions 得到了有效地 correction，并在人类大脑中测试了视觉活化Volume和其BOLD响应，与普通3D echo-planar图像相当。<details>
<summary>Abstract</summary>
Purpose: Echo-planar imaging (EPI) with blip-up/down acquisition (BUDA) can provide high-quality images with minimal distortions by using two readout trains with opposing phase-encoding gradients. Because of the need for two separate acquisitions, BUDA doubles the scan time and degrades the temporal resolution when compared to single-shot EPI, presenting a major challenge for many applications, particularly functional MRI (fMRI). This study aims at overcoming this challenge by developing an echo-shifted EPI BUDA (esEPI-BUDA) technique to acquire both blip-up and blip-down datasets in a single shot. Methods: A three-dimensional (3D) esEPI-BUDA pulse sequence was designed by using an echo-shifting strategy to produce two EPI readout trains. These readout trains produced a pair of k-space datasets whose k-space trajectories were interleaved with opposite phase-encoding gradient directions. The two k-space datasets were separately reconstructed using a 3D SENSE algorithm, from which time-resolved B0-field maps were derived using TOPUP in FSL and then input into a forward model of joint parallel imaging reconstruction to correct for geometric distortion. In addition, Hankel structured low-rank constraint was incorporated into the reconstruction framework to improve image quality by mitigating the phase errors between the two interleaved k-space datasets. Results: The 3D esEPI-BUDA technique was demonstrated in a phantom and an fMRI study on healthy human subjects. Geometric distortions were effectively corrected in both phantom and human brain images. In the fMRI study, the visual activation volumes and their BOLD responses were comparable to those from conventional 3D echo-planar images. Conclusion: The improved imaging efficiency and dynamic distortion correction capability afforded by 3D esEPI-BUDA are expected to benefit many EPI applications.
</details>
<details>
<summary>摘要</summary>
目的：使用双向磁场增强/减强获取（BUDA）技术可以提供高质量图像，但因为需要两次获取，BUDA将扫描时间双倍，降低时间分辨率，对许多应用程序（特别是功能磁共振成像（fMRI））提出了主要挑战。这项研究的目的是解决这一挑战，通过开发一种三维echo-shifted EPI BUDA（esEPI-BUDA）技术，在单击中获取两个磁场增强/减强数据集。方法：设计了一种三维 esEPI-BUDA脉冲序列，使用抽象阶段生成两个 EPI 读取轨迹。这两个读取轨迹生成了具有相反方向磁场增强/减强方向的两个 k-空间数据集。这两个 k-空间数据集分别使用三维 SENSE 算法重构，并从而生成了时间解析B0场图像。在FSL中使用 TOPUP 算法，将这些时间解析B0场图像输入到了一种 JOINT 平行成像重建模型中，以 corrected  geometric distortion。此外，还在重建框架中添加了具有束缚低维度的 Hankel 结构低级数据约束，以提高图像质量，减少了两个交错 k-空间数据集之间的频率错误。结果：在一个模拟器和一个人类大脑功能磁共振成像研究中，三维 esEPI-BUDA 技术得到了证明。在这些研究中，人类大脑功能磁共振图像中的形态扭曲都得到了有效地 corrections。在人类大脑功能磁共振图像中，可见功能区域的激发量和其 BOLD 响应与普通三维 EPI 图像具有相同的水平。结论：三维 esEPI-BUDA 技术的改进的扫描效率和动态扭曲纠正能力，预期将对许多 EPI 应用程序产生积极的影响。
</details></li>
</ul>
<hr>
<h2 id="The-Color-Clifford-Hardy-Signal-Application-to-Color-Edge-Detection-and-Optical-Flow"><a href="#The-Color-Clifford-Hardy-Signal-Application-to-Color-Edge-Detection-and-Optical-Flow" class="headerlink" title="The Color Clifford Hardy Signal: Application to Color Edge Detection and Optical Flow"></a>The Color Clifford Hardy Signal: Application to Color Edge Detection and Optical Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06485">http://arxiv.org/abs/2308.06485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Hu, Kit Ian Kou, Cuiming Zou, Dong Cheng</li>
<li>for: 该论文提出了色彩Clifford Hardy信号的想法，用于处理色彩图像。</li>
<li>methods: 该论文提出了五种方法来识别色彩图像的边缘，基于色彩Clifford Hardy信号的本地特征表示。</li>
<li>results: 该论文通过多种比较研究表明，提出的方法具有高效性和抗噪能力。例如，通过多尺度结构的色彩Clifford Hardy信号，方法可以抗衰减噪。此外，论文还提供了一种颜色动向检测方法，用于应用示例。<details>
<summary>Abstract</summary>
This paper introduces the idea of the color Clifford Hardy signal, which can be used to process color images. As a complex analytic function's high-dimensional analogue, the color Clifford Hardy signal inherits many desirable qualities of analyticity. A crucial tool for getting the color and structural data is the local feature representation of a color image in the color Clifford Hardy signal. By looking at the extended Cauchy-Riemann equations in the high-dimensional space, it is possible to see the connection between the different parts of the color Clifford Hardy signal. Based on the distinctive and important local amplitude and local phase generated by the color Clifford Hardy signal, we propose five methods to identify the edges of color images with relation to a certain color. To prove the superiority of the offered methodologies, numerous comparative studies employing image quality assessment criteria are used. Specifically by using the multi-scale structure of the color Clifford Hardy signal, the proposed approaches are resistant to a variety of noises. In addition, a color optical flow detection method with anti-noise ability is provided as an example of application.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection"><a href="#Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection" class="headerlink" title="Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection"></a>Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06481">http://arxiv.org/abs/2308.06481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvaro Fernandez-Quilez, Linas Vidziunas, Ørjan Kløvfjell Thoresen, Ketil Oppedal, Svein Reidar Kjosavik, Trygve Eftestøl</li>
<li>for: 这篇论文是为了检测抑制癌病变的方法。</li>
<li>methods: 这篇论文使用了多流程方法，以便利用不同的T2w方向来提高肿瘤检测的性能。</li>
<li>results: 这篇论文的结果显示，使用多流程方法可以提高肿瘤检测的精度，比单向方法更高（AUC&#x3D;73.1 vs 82.3）。<details>
<summary>Abstract</summary>
Traditional deep learning (DL) approaches based on supervised learning paradigms require large amounts of annotated data that are rarely available in the medical domain. Unsupervised Out-of-distribution (OOD) detection is an alternative that requires less annotated data. Further, OOD applications exploit the class skewness commonly present in medical data. Magnetic resonance imaging (MRI) has proven to be useful for prostate cancer (PCa) diagnosis and management, but current DL approaches rely on T2w axial MRI, which suffers from low out-of-plane resolution. We propose a multi-stream approach to accommodate different T2w directions to improve the performance of PCa lesion detection in an OOD approach. We evaluate our approach on a publicly available data-set, obtaining better detection results in terms of AUC when compared to a single direction approach (73.1 vs 82.3). Our results show the potential of OOD approaches for PCa lesion detection based on MRI.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）方法基于supervised learning paradigms需要大量的标注数据，而医疗领域中这些数据很少。不需要标注数据的Out-of-distribution（OOD）检测是一个 alternativa，并且可以利用医疗数据的类偏好。核磁共振成像（MRI）已经被证明是肠癌（PCa）诊断和管理的有用工具，但现有的DL方法仅仅使用T2w极向MRI，这种MRI受到外向分辨率的限制。我们提议一种多流处理方法，以便处理不同的T2w方向，以提高PCa患部检测的性能。我们对公共可用数据集进行了评估，并 obtient了与单向方法相比的更好的检测结果（AUC=73.1 vs AUC=82.3）。我们的结果表明，基于MRI的PCa患部检测可以通过OOD方法实现更高的检测精度。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach"><a href="#Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach" class="headerlink" title="Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach"></a>Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06477">http://arxiv.org/abs/2308.06477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Nikolass Lindeijer, Tord Martin Ytredal, Trygve Eftestøl, Tobias Nordström, Fredrik Jäderling, Martin Eklund, Alvaro Fernandez-Quilez</li>
<li>for: 提高肾脏癌诊断的支持</li>
<li>methods: 使用对比学习方法，不需要manual注释，可以在部署时 missing views</li>
<li>results: 提高了自适应volumetric分割的精度，并且在多视图数据上实现了good external volumetric generalization<details>
<summary>Abstract</summary>
An accurate prostate delineation and volume characterization can support the clinical assessment of prostate cancer. A large amount of automatic prostate segmentation tools consider exclusively the axial MRI direction in spite of the availability as per acquisition protocols of multi-view data. Further, when multi-view data is exploited, manual annotations and availability at test time for all the views is commonly assumed. In this work, we explore a contrastive approach at training time to leverage multi-view data without annotations and provide flexibility at deployment time in the event of missing views. We propose a triplet encoder and single decoder network based on U-Net, tU-Net (triplet U-Net). Our proposed architecture is able to exploit non-annotated sagittal and coronal views via contrastive learning to improve the segmentation from a volumetric perspective. For that purpose, we introduce the concept of inter-view similarity in the latent space. To guide the training, we combine a dice score loss calculated with respect to the axial view and its manual annotations together with a multi-view contrastive loss. tU-Net shows statistical improvement in dice score coefficient (DSC) with respect to only axial view (91.25+-0.52% compared to 86.40+-1.50%,P<.001). Sensitivity analysis reveals the volumetric positive impact of the contrastive loss when paired with tU-Net (2.85+-1.34% compared to 3.81+-1.88%,P<.001). Further, our approach shows good external volumetric generalization in an in-house dataset when tested with multi-view data (2.76+-1.89% compared to 3.92+-3.31%,P=.002), showing the feasibility of exploiting non-annotated multi-view data through contrastive learning whilst providing flexibility at deployment in the event of missing views.
</details>
<details>
<summary>摘要</summary>
可以准确地定义和量化 prostata 的部分，可以支持肾癌的临床评估。许多自动 prostate 分割工具都忽略了多视图数据的可用性，即使据获取协议中有多视图数据可用。此外，当使用多视图数据时，通常需要手动标注和测试时 disponibility 的所有视图。在这种情况下，我们提出了一种对比方法，可以在训练时使用多视图数据而不需要手动标注，并在部署时提供可选的视图。我们提出了一种基于 U-Net 的 triplet 编码器和单个解码器网络，可以通过对比学习利用非标注的 sagittal 和极轴视图来提高分割。为此，我们引入了视图间的相似性在幂空间的概念。为了导航训练，我们将 dice 分数损失与 respect 到 axial 视图和其手动标注相加，并与多视图对比损失相结合。tU-Net 表示与只有 axial 视图相比（91.25+-0.52% 与 86.40+-1.50%，P<.001）显示了统计学上的改进。敏感分析表明，对于 tU-Net 来说，对比损失的volumetric 正面影响（2.85+-1.34% 与 3.81+-1.88%,P<.001）。此外，我们的方法在我们的内部数据集中进行了多视图数据的外部准确性测试（2.76+-1.89% 与 3.92+-3.31%,P=.002），表明可以通过对比学习在不具有标注的多视图数据上Exploiting 而提供可选的视图。
</details></li>
</ul>
<hr>
<h2 id="CATS-v2-Hybrid-encoders-for-robust-medical-segmentation"><a href="#CATS-v2-Hybrid-encoders-for-robust-medical-segmentation" class="headerlink" title="CATS v2: Hybrid encoders for robust medical segmentation"></a>CATS v2: Hybrid encoders for robust medical segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06377">http://arxiv.org/abs/2308.06377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoli12345/cats">https://github.com/haoli12345/cats</a></li>
<li>paper_authors: Hao Li, Han Liu, Dewei Hu, Xing Yao, Jiacheng Wang, Ipek Oguz<br>for:* 这个研究是为了提高医疗影像分类 задачі的性能，特别是在 capture 高级（本地）信息和全球信息之间的平衡。methods:* 使用 hybrid encoders，包括一个 CNN-based Encoder 路径和一个 transformer 路径，以更好地利用本地和全球信息。* 使用 skip connections 将 convolutional encoder 和 transformer 融合为最终的分类结果。results:* 在 Cross-Modality Domain Adaptation (CrossMoDA) 和 Medical Segmentation Decathlon (MSD-5) 项目中，该方法与州际先进方法相比， exhibit 高的 Dice 分数。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have exhibited strong performance in medical image segmentation tasks by capturing high-level (local) information, such as edges and textures. However, due to the limited field of view of convolution kernel, it is hard for CNNs to fully represent global information. Recently, transformers have shown good performance for medical image segmentation due to their ability to better model long-range dependencies. Nevertheless, transformers struggle to capture high-level spatial features as effectively as CNNs. A good segmentation model should learn a better representation from local and global features to be both precise and semantically accurate. In our previous work, we proposed CATS, which is a U-shaped segmentation network augmented with transformer encoder. In this work, we further extend this model and propose CATS v2 with hybrid encoders. Specifically, hybrid encoders consist of a CNN-based encoder path paralleled to a transformer path with a shifted window, which better leverage both local and global information to produce robust 3D medical image segmentation. We fuse the information from the convolutional encoder and the transformer at the skip connections of different resolutions to form the final segmentation. The proposed method is evaluated on two public challenge datasets: Cross-Modality Domain Adaptation (CrossMoDA) and task 5 of Medical Segmentation Decathlon (MSD-5), to segment vestibular schwannoma (VS) and prostate, respectively. Compared with the state-of-the-art methods, our approach demonstrates superior performance in terms of higher Dice scores.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医疗图像分割任务中表现出色，通过捕捉高级（本地）信息，如边缘和xture，来捕捉高级信息。然而，由于卷积核心的局部视场限制，使得CNN很难完全表示全局信息。最近，transformer在医疗图像分割中表现良好，这是因为它们可以更好地模型距离的长距离依赖关系。然而，transformer尚未能如同CNN那样有效地捕捉高级空间特征。为了实现更好的分割模型，我们需要学习更好地捕捉本地和全局特征，以确保准确和semantic地正确。在我们之前的工作中，我们提出了CATS模型，这是一种U型卷积分割网络，其中包括transformer编码器。在这个工作中，我们进一步扩展了这个模型，并提出了CATS v2模型，它包括hybrid编码器。specifically，hybrid编码器包括一个CNN基于编码器路径，并与一个偏移窗口的transformer路径并行，这样更好地利用本地和全局信息来生成robust的3D医疗图像分割。我们在不同分辨率的 skip connections中 fusions the information from the convolutional encoder and the transformer，以生成最终的分割结果。我们的方法在两个公共挑战数据集上进行评估： Cross-Modality Domain Adaptation（CrossMoDA）和Medical Segmentation Decathlon（MSD-5），用于分割vestibular schwannoma（VS）和prostate，分别。与当前状态的方法相比，我们的方法在 terms of higher Dice scores表现出色。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis"><a href="#Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis" class="headerlink" title="Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis"></a>Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06333">http://arxiv.org/abs/2308.06333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/open-eoe">https://github.com/hrlblab/open-eoe</a></li>
<li>paper_authors: Juming Xiong, Yilin Liu, Ruining Deng, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Yuankai Huo<br>for: 这个研究旨在开发一个开源的工具集，用于检测食道病变中的嗜中性粒细胞（Eos）。methods: 该工具集使用三种现状最佳的深度学习基于对象检测模型，并实现了一种ensemble学习策略以提高结果的精度和可靠性。results: 实验结果表明，Open-EoE工具集可以效果地检测食道病变中的Eos，并达到了91%的准确率，与病理学家评估相符。<details>
<summary>Abstract</summary>
Eosinophilic Esophagitis (EoE) is a chronic, immune/antigen-mediated esophageal disease, characterized by symptoms related to esophageal dysfunction and histological evidence of eosinophil-dominant inflammation. Owing to the intricate microscopic representation of EoE in imaging, current methodologies which depend on manual identification are not only labor-intensive but also prone to inaccuracies. In this study, we develop an open-source toolkit, named Open-EoE, to perform end-to-end whole slide image (WSI) level eosinophil (Eos) detection using one line of command via Docker. Specifically, the toolkit supports three state-of-the-art deep learning-based object detection models. Furthermore, Open-EoE further optimizes the performance by implementing an ensemble learning strategy, and enhancing the precision and reliability of our results. The experimental results demonstrated that the Open-EoE toolkit can efficiently detect Eos on a testing set with 289 WSIs. At the widely accepted threshold of >= 15 Eos per high power field (HPF) for diagnosing EoE, the Open-EoE achieved an accuracy of 91%, showing decent consistency with pathologist evaluations. This suggests a promising avenue for integrating machine learning methodologies into the diagnostic process for EoE. The docker and source code has been made publicly available at https://github.com/hrlblab/Open-EoE.
</details>
<details>
<summary>摘要</summary>
《细胞滤镜检测诊断工具（Open-EoE）》是一个开源的检测工具，用于检测食管细胞滤镜病（EoE）的患者。该工具使用深度学习技术，可以通过一条命令在Docker环境中完成整个检测过程。工具支持三种当前顶峰的深度学习模型，并且通过 ensemble learning 策略来提高检测精度和可靠性。实验结果表明，Open-EoE 工具可以高效地检测食管细胞滤镜，在测试集上达到了91%的准确率。这表明，机器学习技术可以成功地应用于EoE 诊断过程中。工具和源代码已经在 GitHub 上公开，可以免费下载和使用。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology"><a href="#Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology" class="headerlink" title="Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology"></a>Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06332">http://arxiv.org/abs/2308.06332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FarihaHossain/SwinFSR">https://github.com/FarihaHossain/SwinFSR</a></li>
<li>paper_authors: Khondker Fariha Hossain, Sharif Amit Kamran, Joshua Ong, Andrew G. Lee, Alireza Tavakkoli</li>
<li>for: 这 paper 是为了提高肉眼图像的分辨率，以便在不同的地方进行早期差分诊断。</li>
<li>methods: 这 paper 使用了 Swin Transformer 与空间和深度精度注意力，实现了基于肉眼图像的超分辨率。</li>
<li>results: 这 paper 在三个公共数据集上 achieved PSNR 值为 47.89、49.00 和 45.32，并在 NASA 提供的私人数据集上达到了相当的结果。<details>
<summary>Abstract</summary>
The rapid accessibility of portable and affordable retinal imaging devices has made early differential diagnosis easier. For example, color funduscopy imaging is readily available in remote villages, which can help to identify diseases like age-related macular degeneration (AMD), glaucoma, or pathological myopia (PM). On the other hand, astronauts at the International Space Station utilize this camera for identifying spaceflight-associated neuro-ocular syndrome (SANS). However, due to the unavailability of experts in these locations, the data has to be transferred to an urban healthcare facility (AMD and glaucoma) or a terrestrial station (e.g, SANS) for more precise disease identification. Moreover, due to low bandwidth limits, the imaging data has to be compressed for transfer between these two places. Different super-resolution algorithms have been proposed throughout the years to address this. Furthermore, with the advent of deep learning, the field has advanced so much that x2 and x4 compressed images can be decompressed to their original form without losing spatial information. In this paper, we introduce a novel model called Swin-FSR that utilizes Swin Transformer with spatial and depth-wise attention for fundus image super-resolution. Our architecture achieves Peak signal-to-noise-ratio (PSNR) of 47.89, 49.00 and 45.32 on three public datasets, namely iChallenge-AMD, iChallenge-PM, and G1020. Additionally, we tested the model's effectiveness on a privately held dataset for SANS provided by NASA and achieved comparable results against previous architectures.
</details>
<details>
<summary>摘要</summary>
“现代化的眼科医学技术已经使得早期的医学诊断变得更加容易。例如，彩色基准摄影是在偏远村庄中可以提供的，可以帮助诊断年龄相关 macular degeneration（AMD）、 glaucoma 或 PATHOLOGICAL MYOPIA（PM）等疾病。然而，由于这些地点缺乏专家，因此需要将数据传输到城市医疗机构（AMD和 glaucoma）或地面站（例如，SANS）进行更加准确的疾病诊断。此外，由于带宽限制，摄影数据需要压缩传输。过去数年，有许多超解像算法的提案，以解决这个问题。另外，随着深度学习的发展，场景有了很大的进步，可以使用 x2 和 x4 压缩图像重新恢复到原始形态，无需失去空间信息。在本文中，我们介绍了一种新的模型called Swin-FSR，该模型利用SwinTransformer的空间和深度宽分注意力来进行基准图像超解像。我们的架构实现了 PSNR 的值为 47.89、49.00 和 45.32 在三个公共数据集上，namely iChallenge-AMD、iChallenge-PM 和 G1020。此外，我们对 NASA 提供的一个私有数据集进行了测试，并与之前的建筑物实现了相似的结果。”
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies"><a href="#A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies" class="headerlink" title="A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies"></a>A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07337">http://arxiv.org/abs/2308.07337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Halid Ziya Yerebakan, Yoshihisa Shinagawa, Mahesh Ranganath, Simon Allen-Raffl, Gerardo Hermosillo Valadez</li>
<li>for: 医疗影像比较 longitudinal 比较中医学影像之间的 анатомиче位置匹配</li>
<li>methods: 使用 hierarchical sparse sampling of image intensities 计算查询点在源图像中的描述子，然后使用 hierarchical search 操作找到目标图像中最相似的描述子。</li>
<li>results: 实现了减少计算时间到毫秒级别的易行医学影像比较，无需额外建筑、存储或训练步骤。在 Deep Lesion Tracking 数据集注释中观察到更高准确的匹配结果，并且比最精确的报告algorithm 24 倍 faster。<details>
<summary>Abstract</summary>
We propose a method to match anatomical locations between pairs of medical images in longitudinal comparisons. The matching is made possible by computing a descriptor of the query point in a source image based on a hierarchical sparse sampling of image intensities that encode the location information. Then, a hierarchical search operation finds the corresponding point with the most similar descriptor in the target image. This simple yet powerful strategy reduces the computational time of mapping points to a millisecond scale on a single CPU. Thus, radiologists can compare similar anatomical locations in near real-time without requiring extra architectural costs for precomputing or storing deformation fields from registrations. Our algorithm does not require prior training, resampling, segmentation, or affine transformation steps. We have tested our algorithm on the recently published Deep Lesion Tracking dataset annotations. We observed more accurate matching compared to Deep Lesion Tracker while being 24 times faster than the most precise algorithm reported therein. We also investigated the matching accuracy on CT and MR modalities and compared the proposed algorithm's accuracy against ground truth consolidated from multiple radiologists.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于在医疗图像对比中匹配生物学位置。该方法基于源图像中点Query的层次稀疏抽象来计算描述符，以便在目标图像中找到最相似的点。这种简单 yet powerful的策略可以在单个CPU上减少计算时间至毫秒级，因此，辐射学家可以在实时比较相似的生物学位置，无需额外的建筑成本或存储扭转场景的预计算或存储。我们的算法不需要先期训练、重新采样、分割或 afine 变换步骤。我们在最近发布的 Deep Lesion Tracking 数据集注释中进行了测试，并观察到比 Deep Lesion Tracker 更准确的匹配，而且比最精确的报告算法更快24倍。我们还研究了该方法在 CT 和 MR 模式下的匹配精度，并与多个 radiologists 共同组织的ground truth进行了比较。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/eess.IV_2023_08_12/" data-id="clly4xtg900ezvl885qf92rff" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.LG_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/cs.LG_2023_08_11/">cs.LG - 2023-08-11 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-a-Causal-Probabilistic-Framework-for-Prediction-Action-Selection-Explanations-for-Robot-Block-Stacking-Tasks"><a href="#Towards-a-Causal-Probabilistic-Framework-for-Prediction-Action-Selection-Explanations-for-Robot-Block-Stacking-Tasks" class="headerlink" title="Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks"></a>Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06203">http://arxiv.org/abs/2308.06203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Cannizzaro, Jonathan Routley, Lars Kunze</li>
<li>for: 本研究旨在提供一种基于 causal probabilistic 框架的自主 робоット，以便让 robot 能够更好地理解和描述它所处环境。</li>
<li>methods: 本研究使用 causal inference 和 physics simulation 技术，将 causal models 与 probabilistic representations 结合起来，以便 robot 能够更好地理解和描述它所处环境。</li>
<li>results: 研究提出了一种新的 causal probabilistic 框架，可以帮助 robot 更好地完成块排序任务，并提供了一些 exemplar 的下一步行动选择结果。 I hope this helps! Let me know if you have any further questions or if there’s anything else I can help with.<details>
<summary>Abstract</summary>
Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability into a structural causal model to permit robots to perceive and assess the current state of a block-stacking task, reason about the next-best action from placement candidates, and generate post-hoc counterfactual explanations. We provide exemplar next-best action selection results and outline planned experimentation in simulated and real-world robot block-stacking tasks.
</details>
<details>
<summary>摘要</summary>
世界中的不确定性使得系统设计者无法预期和明确地设计机器人可能遇到的所有场景。因此，基于这种设计方式的机器人在控制环境外会失败。 causal模型提供了一个原则性的框架，用于编码机器人与环境之间的 causal 关系，以及通常遇到的实际世界机器人中的抽象概率和不确定性。这些模型与 causal 推理相结合，允许自主机器人理解、推理和解释其环境。在这项工作中，我们关注了机器人块堆垫任务，因为它涉及到机器人的基本感知和操作能力，这些能力是许多应用程序，如仓库自动化和家庭服务机器人所必需的。我们提出了一种新的 causal 概率 frameworks，用于在机器人块堆垫任务中嵌入物理模拟能力，让机器人可以识别和评估当前块堆垫任务的状态，从选择候选位置中选择下一步行动，并生成后续 counterfactual 解释。我们提供了示例下一步行动选择结果，并详细描述计划在模拟和实际机器人块堆垫任务中进行实验。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Predicate-Visual-Context-in-Detecting-of-Human-Object-Interactions"><a href="#Exploring-Predicate-Visual-Context-in-Detecting-of-Human-Object-Interactions" class="headerlink" title="Exploring Predicate Visual Context in Detecting of Human-Object Interactions"></a>Exploring Predicate Visual Context in Detecting of Human-Object Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06202">http://arxiv.org/abs/2308.06202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredzzhang/pvic">https://github.com/fredzzhang/pvic</a></li>
<li>paper_authors: Frederic Z. Zhang, Yuhui Yuan, Dylan Campbell, Zhuoyao Zhong, Stephen Gould</li>
<li>for: 本研究旨在解决人–物交互（HOI）领域中现有的问题，即使用两stage transformer-based HOI检测器，但这些检测器常常基于物体特征，而忽略了姿态和方向信息，导致复杂或抽象的交互检测受到阻碍。</li>
<li>methods: 本研究使用了视觉化和优化的查询设计、广泛的键和值搜索以及盒对位嵌入作为空间引导，以提高 predicate visual context（PViC）模型的表现，并与当前状态顶峰方法在 HICO-DET 和 V-COCO 测试集上进行比较。</li>
<li>results: 根据测试结果，我们的 PViC 模型在 HICO-DET 和 V-COCO 测试集上具有更高的表现，同时保持了训练成本的低。<details>
<summary>Abstract</summary>
Recently, the DETR framework has emerged as the dominant approach for human--object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaining low training cost.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:Recently, DETR 框架在人-物互动（HOI）研究中成为主流方法。特别是两阶段转换器基于 HOI 检测器在性能和训练效率方面表现出色。然而，这些通常基于缺乏细化上下文信息的对象特征，忽略对象的姿势和方向信息，而仅仅依靠对象的视觉特征来确定对象的标识和边框极限。这会导致复杂或抽象的互动无法正确识别。在这项工作中，我们通过视觉化和仔细设计的实验来研究这些问题。我们尝试通过跨注意力来重新引入图像特征，并通过改进的查询设计、广泛探索键和值、以及对象对的位域嵌入来提高 predicate 视觉上下文（PViC）模型的性能。我们的 PViC 模型在 HICO-DET 和 V-COCO 测试数据集上的表现比 state-of-the-art 方法更高，而且训练成本仍然很低。
</details></li>
</ul>
<hr>
<h2 id="Complex-Facial-Expression-Recognition-Using-Deep-Knowledge-Distillation-of-Basic-Features"><a href="#Complex-Facial-Expression-Recognition-Using-Deep-Knowledge-Distillation-of-Basic-Features" class="headerlink" title="Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features"></a>Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06197">http://arxiv.org/abs/2308.06197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/angusmaiden/complex-fer">https://github.com/angusmaiden/complex-fer</a></li>
<li>paper_authors: Angus Maiden, Bahareh Nakisa</li>
<li>for: 该论文的目的是提出一种基于人类认知和学习的新型不间断学习方法，以便准确地识别复杂的人脸表达。</li>
<li>methods: 该方法基于人类认知和学习，包括知识储存、知识总结和预测排序记忆等技术。它还使用 GradCAM 视觉化来表明基本和复杂表达之间的关系。</li>
<li>results: 该方法可以准确地识别新的复杂表达类型，使用少量示例来学习新类别，并且在新类别上达到了74.28% 的总准确率。此外，该方法还证明了不间断学习方法在复杂表达识别中的优越性，比非不间断学习方法高出13.95%。此外，该方法还是首次应用了几shot学习到复杂表达识别中，达到了100% 的准确率。<details>
<summary>Abstract</summary>
Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by building on and retaining its knowledge of basic expression classes. Using GradCAM visualisations, we demonstrate the relationship between basic and compound facial expressions, which our method leverages through knowledge distillation and a novel Predictive Sorting Memory Replay. Our method achieves the current state-of-the-art in continual learning for complex facial expression recognition with 74.28% Overall Accuracy on new classes. We also demonstrate that using continual learning for complex facial expression recognition achieves far better performance than non-continual learning methods, improving on state-of-the-art non-continual learning methods by 13.95%. To the best of our knowledge, our work is also the first to apply few-shot learning to complex facial expression recognition, achieving the state-of-the-art with 100% accuracy using a single training sample for each expression class.
</details>
<details>
<summary>摘要</summary>
人工智能在复杂情绪认知方面的表现，虽然已经达到了其他一些任务的水平，但是情绪认知仍然是一个挑战。人脸表达的情绪认知特别Difficult，因为人脸上可以表达出的情感复杂多样。为了让机器达到人类水平，它可能需要合并知识并理解新概念，就像人类一样。人类可以通过几个示例学习新概念，从记忆中提炼出重要信息，并丢弃其他信息。我们提出了一种基于人类认知和学习的新型连续学习方法，可以准确地识别新的复杂表达类型，使用很少的训练样本。我们使用GradCAM视觉化来描述基本和复杂表达之间的关系，然后通过知识储存和一种新的预测排序记忆回放来利用这种关系。我们的方法实现了当前领域内连续学习的最佳性，新类别上的总准确率为74.28%。我们还证明了使用连续学习进行复杂表达认知比非连续学习方法更好，提高了领域内最佳非连续学习方法的13.95%。此外，我们是第一个将几个样本学习应用于复杂表达认知，并达到了领域内最佳性，每个表达类型的准确率为100%。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Guest-Nationality-Composition-from-Hotel-Reviews"><a href="#Assessing-Guest-Nationality-Composition-from-Hotel-Reviews" class="headerlink" title="Assessing Guest Nationality Composition from Hotel Reviews"></a>Assessing Guest Nationality Composition from Hotel Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06175">http://arxiv.org/abs/2308.06175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Gröger, Marc Pouly, Flavia Tinner, Leif Brandes</li>
<li>for: 这篇论文旨在为企业提供方法，以优化客户端的客户分布。</li>
<li>methods: 该论文使用机器学习技术，从不结构化文本评论中提取出客户国籍信息，以动态评估和监测具体业务客户分布的变化。</li>
<li>results: 研究表明，使用简单的预训练embeddings和堆式LSTM层可以提供更好的性能-运行时间平衡，比较复杂的语言模型。<details>
<summary>Abstract</summary>
Many hotels target guest acquisition efforts to specific markets in order to best anticipate individual preferences and needs of their guests. Likewise, such strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but no fine-grained information on the guest composition of individual businesses exists. There is, however, growing interest in such data from competitors, suppliers, researchers and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a rather simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.
</details>
<details>
<summary>摘要</summary>
许多酒店会向特定市场进行客户营销努力，以最好地预测客人偏好和需求。这种策略也是市场营销预算的必要前提。官方统计数据会报告不同国家的游客数量，但没有细化的信息对具体的企业客户组成进行了报告。然而，有越来越多的竞争对手、供应商、研究人员和公众对这些数据感兴趣。我们示例如如何使用机器学习来从无结构文本评论中提取客人国籍信息，以动态评估和监测个体企业客户组成的动态变化。具体来说，我们发现使用简单的预训练 embedding 和堆叠 LSTM 层可以提供更好的性能-运行时间质量比例，比较复杂的现状语言模型。
</details></li>
</ul>
<hr>
<h2 id="Physical-Adversarial-Attacks-For-Camera-based-Smart-Systems-Current-Trends-Categorization-Applications-Research-Challenges-and-Future-Outlook"><a href="#Physical-Adversarial-Attacks-For-Camera-based-Smart-Systems-Current-Trends-Categorization-Applications-Research-Challenges-and-Future-Outlook" class="headerlink" title="Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook"></a>Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06173">http://arxiv.org/abs/2308.06173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammed Shafique</li>
<li>for: 这篇论文旨在为研究人员、实践者和政策制定者提供关于物理抗击攻击的全面评估和概述，以便开发强健和安全的深度学习系统。</li>
<li>methods: 论文分析了物理抗击攻击的主要特征和特点，并描述了不同应用领域中的物理抗击攻击方法，包括分类、检测、人脸识别、 semantic segmentation 和深度估计。</li>
<li>results: 论文评估了这些攻击方法的效果、隐蔽性和可靠性，并探讨了如何在实际世界中执行攻击，以及如何提高防御机制。<details>
<summary>Abstract</summary>
In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research directions in the field of physical adversarial attacks. We highlight the need for enhanced defense mechanisms, the exploration of novel attack strategies, the evaluation of attacks in different application domains, and the establishment of standardized benchmarks and evaluation criteria for physical adversarial attacks. Through this comprehensive survey, we aim to provide a valuable resource for researchers, practitioners, and policymakers to gain a holistic understanding of physical adversarial attacks in computer vision and facilitate the development of robust and secure DNN-based systems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了物理 adversarial 攻击的现代趋势的全面检查。我们的目的是为您提供物理 adversarial 攻击的深入理解，包括其关键特征和区别特征。此外，我们还探讨了在物理世界中执行攻击的具体要求和挑战。我们的文章探讨了不同应用领域中的物理 adversarial 攻击方法，分为不同的目标任务，如分类、检测、识别、 semantic segmentation 和深度估计。我们评估了这些攻击方法的效果、隐蔽性和Robustness。我们探究每种技术如何在 DNN 上成功 manipulate 而 minimizing 检测和快速应对实际扭曲。最后，我们讨论了物理 adversarial 攻击领域的当前挑战和未来研究方向，包括增强防御机制、探索新的攻击策略、在不同应用领域中评估攻击、以及建立 DNN 领域的标准化评估标准和评估方法。通过这篇全面的检查，我们希望为研究人员、实践人员和政策制定者提供一份有价值的资源，以便更好地理解物理 adversarial 攻击，并促进 DNN 基于系统的开发。
</details></li>
</ul>
<hr>
<h2 id="Phased-Deep-Spatio-temporal-Learning-for-Highway-Traffic-Volume-Prediction"><a href="#Phased-Deep-Spatio-temporal-Learning-for-Highway-Traffic-Volume-Prediction" class="headerlink" title="Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction"></a>Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06155">http://arxiv.org/abs/2308.06155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weilong Ding, Tianpu Zhang, Zhe Wang</li>
<li>for: 预测高速公路日常交通量</li>
<li>methods: employs a hybrid model combining fully convolution network (FCN) and long short-term memory (LSTM), considering time, space, meteorology, and calendar from heterogeneous data</li>
<li>results: 实际使用一个中国省级高速公路的实际数据，对比traditional models，our method has distinct improvement for predictive accuracy, reaching 5.269 and 0.997 in MPAE and R-squre metrics, respectively.<details>
<summary>Abstract</summary>
Inter-city highway transportation is significant for citizens' modern urban life and generates heterogeneous sensory data with spatio-temporal characteristics. As a routine analysis in transportation domain, daily traffic volume estimation faces challenges for highway toll stations including lacking of exploration of correlative spatio-temporal features from a long-term perspective and effective means to deal with data imbalance which always deteriorates the predictive performance. In this paper, a deep spatio-temporal learning method is proposed to predict daily traffic volume in three phases. In feature pre-processing phase, data is normalized elaborately according to latent long-tail distribution. In spatio-temporal learning phase, a hybrid model is employed combining fully convolution network (FCN) and long short-term memory (LSTM), which considers time, space, meteorology, and calendar from heterogeneous data. In decision phase, traffic volumes on a coming day at network-wide toll stations would be achieved effectively, which is especially calibrated for vital few highway stations. Using real-world data from one Chinese provincial highway, extensive experiments show our method has distinct improvement for predictive accuracy than various traditional models, reaching 5.269 and 0.997 in MPAE and R-squre metrics, respectively.
</details>
<details>
<summary>摘要</summary>
市区间高速公路交通是现代城市居民日常生活中的重要一环，生成了多样化的感知数据，具有空间时间特征。为了解决高速公路收费站的日常交通量预测问题，我们面临着缺乏探索长期征特的相关空间时间特征以及有效地处理数据不均衡问题的挑战。在本文中，我们提出了一种深度空间时间学习方法，可以预测高速公路收费站的日常交通量。在特征预处理阶段，我们对数据进行了细心的Normal化，根据潜在的长尾分布。在空间时间学习阶段，我们采用了一种混合模型，结合了全连接网络（FCN）和长短期记忆（LSTM），考虑了时间、空间、气象和历法等多种不同数据。在决策阶段，我们可以准确预测高速公路收费站的未来一天内的交通量，特别是对于重要的一些高速公路站点进行了精准补做。使用了一个中国省道高速公路的实际数据，我们进行了广泛的实验，结果表明，我们的方法在预测精度方面与传统模型相比，具有明显的改善，分别达到了5.269和0.997的MPAE和R-squre指标。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Process-Regression-for-Maximum-Entropy-Distribution"><a href="#Gaussian-Process-Regression-for-Maximum-Entropy-Distribution" class="headerlink" title="Gaussian Process Regression for Maximum Entropy Distribution"></a>Gaussian Process Regression for Maximum Entropy Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06149">http://arxiv.org/abs/2308.06149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsen Sadr, Manuel Torrilhon, M. Hossein Gorji</li>
<li>for: 用于闭合问题的最大熵分布</li>
<li>methods: 使用加aussian prior逼近Lagrange多个性数</li>
<li>results: 对不同的kernel函数和Hyperparameter进行优化，实现数据驱动的最大熵闭合，并应用于非平衡分布的relaxation和Bhatnagar-Gross-Krook等方程的解。<details>
<summary>Abstract</summary>
Maximum-Entropy Distributions offer an attractive family of probability densities suitable for moment closure problems. Yet finding the Lagrange multipliers which parametrize these distributions, turns out to be a computational bottleneck for practical closure settings. Motivated by recent success of Gaussian processes, we investigate the suitability of Gaussian priors to approximate the Lagrange multipliers as a map of a given set of moments. Examining various kernel functions, the hyperparameters are optimized by maximizing the log-likelihood. The performance of the devised data-driven Maximum-Entropy closure is studied for couple of test cases including relaxation of non-equilibrium distributions governed by Bhatnagar-Gross-Krook and Boltzmann kinetic equations.
</details>
<details>
<summary>摘要</summary>
最大熵分布可以提供一个有优点的可能性密度，适用于矩阵闭合问题。然而，计算这些分布的拉格朗日 Parameters是实际应用中的计算瓶颈。受最近 Gaussian 过程的成功启发，我们研究将 Gaussian 假设用于 Approximate 拉格朗日 Parameters 的Map 的可行性。对各种核函数进行优化，我们使用最大 log-likelihood 来优化 гипер参数。我们研究了这种数据驱动的最大熵闭合的性能，并在几个测试案例中，包括适应不平衡分布的Relaxation 和 Boltzmann 动力学方程的闭合。
</details></li>
</ul>
<hr>
<h2 id="A-New-Approach-to-Overcoming-Zero-Trade-in-Gravity-Models-to-Avoid-Indefinite-Values-in-Linear-Logarithmic-Equations-and-Parameter-Verification-Using-Machine-Learning"><a href="#A-New-Approach-to-Overcoming-Zero-Trade-in-Gravity-Models-to-Avoid-Indefinite-Values-in-Linear-Logarithmic-Equations-and-Parameter-Verification-Using-Machine-Learning" class="headerlink" title="A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning"></a>A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06303">http://arxiv.org/abs/2308.06303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikrajuddin Abdullah</li>
<li>For: The paper aims to solve the challenge of identifying gravity parameters in the gravity model to explain international trade, specifically when there are zero flow trades.* Methods: The authors propose a two-step technique that involves performing linear regression locally to establish a dummy value for zero flow trades, and then estimating the gravity parameters using iterative techniques. Machine learning is also used to test the estimated parameters by analyzing their position in the cluster.* Results: The authors calculate international trade figures for 2004, 2009, 2014, and 2019 and find that the powers of GDP and distance are in the same cluster and are both worth roughly one. The strategy presented in the paper can be used to solve other problems involving log-linear regression.Here’s the simplified Chinese text for the three information points:* 用途：文章解决国际贸易预测模型中零流通问题，即预测零流通时的重力参数。* 方法：文章提出了一种两步技术，先本地线性回归以确定零流通的假值，然后使用迭代法估算重力参数。同时，文章使用机器学习测试估算参数的位置在集群中。* 结果：文章计算了2004年、2009年、2014年和2019年的国际贸易数据，发现GDP的势和距离的势都处在同一个集群，均值约为1。文章的策略可以解决其他带有封零的log-线性回归问题。<details>
<summary>Abstract</summary>
The presence of a high number of zero flow trades continues to provide a challenge in identifying gravity parameters to explain international trade using the gravity model. Linear regression with a logarithmic linear equation encounters an indefinite value on the logarithmic trade. Although several approaches to solving this problem have been proposed, the majority of them are no longer based on linear regression, making the process of finding solutions more complex. In this work, we suggest a two-step technique for determining the gravity parameters: first, perform linear regression locally to establish a dummy value to substitute trade flow zero, and then estimating the gravity parameters. Iterative techniques are used to determine the optimum parameters. Machine learning is used to test the estimated parameters by analyzing their position in the cluster. We calculated international trade figures for 2004, 2009, 2014, and 2019. We just examine the classic gravity equation and discover that the powers of GDP and distance are in the same cluster and are both worth roughly one. The strategy presented here can be used to solve other problems involving log-linear regression.
</details>
<details>
<summary>摘要</summary>
高数量的零流贸易仍然成为国际贸易使用重力模型确定重力参数的挑战。线性回归的对数几何方程遇到了对数贸易的不定值。虽然有几种解决方案被提议，但大多数都不再基于线性回归，使得解决问题的过程变得更加复杂。在这项工作中，我们提议一种两步技巧来确定重力参数：首先，在本地使用线性回归来设置一个占位符来替代零流贸易，然后估算重力参数。使用迭代技术来确定优化参数。机器学习被用来测试估算的参数，分析它们的位置在群集中。我们计算了2004、2009、2014和2019年的国际贸易数据。我们只考虑 классический重力方程，发现GDP的势和距离的势都在同一个群集中，它们的值约为1。这种策略可以用于解决其他带有对数几何方程的问题。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-the-Relevance-of-Comments-in-Codes-Using-Bag-of-Words-and-Transformer-Based-Models"><a href="#Identification-of-the-Relevance-of-Comments-in-Codes-Using-Bag-of-Words-and-Transformer-Based-Models" class="headerlink" title="Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models"></a>Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06144">http://arxiv.org/abs/2308.06144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sruthisudheer/comment-classification-of-c-code">https://github.com/sruthisudheer/comment-classification-of-c-code</a></li>
<li>paper_authors: Sruthi S, Tanmay Basu</li>
<li>for: 本研究的目的是为了对不同代码段的注释进行分类。</li>
<li>methods: 本研究使用了不同的特征工程方案和文本分类技术，包括经典的袋包模型和基于变换器的模型。</li>
<li>results: 研究发现，使用袋包模型在训练集上表现最佳，但模型在训练和测试集上的表现不理想。研究还总结了模型的局限性和进一步改进的可能性。<details>
<summary>Abstract</summary>
The Forum for Information Retrieval (FIRE) started a shared task this year for classification of comments of different code segments. This is binary text classification task where the objective is to identify whether comments given for certain code segments are relevant or not. The BioNLP-IISERB group at the Indian Institute of Science Education and Research Bhopal (IISERB) participated in this task and submitted five runs for five different models. The paper presents the overview of the models and other significant findings on the training corpus. The methods involve different feature engineering schemes and text classification techniques. The performance of the classical bag of words model and transformer-based models were explored to identify significant features from the given training corpus. We have explored different classifiers viz., random forest, support vector machine and logistic regression using the bag of words model. Furthermore, the pre-trained transformer based models like BERT, RoBERT and ALBERT were also used by fine-tuning them on the given training corpus. The performance of different such models over the training corpus were reported and the best five models were implemented on the given test corpus. The empirical results show that the bag of words model outperforms the transformer based models, however, the performance of our runs are not reasonably well in both training and test corpus. This paper also addresses the limitations of the models and scope for further improvement.
</details>
<details>
<summary>摘要</summary>
forum for information retrieval (FIRE) 这年开始了代码段评注分类的共同任务。这是一个二进制文本分类任务，目标是判断给定的代码段评注是否相关。bioNLP-IISERB 组在印度科学教育研究所 Bhopal（IISERB）参加了这个任务，并提交了五个运行，每个运行使用了不同的模型。本文介绍了模型和其他有关训练集的发现。方法包括不同的特征工程方案和文本分类技术。我们研究了传统的袋子模型和基于 transformer 的模型，并使用了不同的分类器，如Random Forest、支持向量机和логисти准则回归。此外，我们还使用了预训练的 transformer 模型，如 BERT、RoBERT 和 ALBERT，并在给定的训练集上进行了微调。对于给定的训练集和测试集，我们Reported 不同模型的性能。 Results show that the bag of words model outperforms the transformer-based models, but the performance of our runs is not satisfactory in both the training and test corpora. This paper also discusses the limitations of the models and the scope for further improvement.
</details></li>
</ul>
<hr>
<h2 id="CompTLL-UNet-Compressed-Domain-Text-Line-Localization-in-Challenging-Handwritten-Documents-using-Deep-Feature-Learning-from-JPEG-Coefficients"><a href="#CompTLL-UNet-Compressed-Domain-Text-Line-Localization-in-Challenging-Handwritten-Documents-using-Deep-Feature-Learning-from-JPEG-Coefficients" class="headerlink" title="CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients"></a>CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06142">http://arxiv.org/abs/2308.06142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bulla Rajesh, Sk Mahafuz Zaman, Mohammed Javed, P. Nagabhushan</li>
<li>for: 本研究旨在提出一种能够直接在JPEG压缩表示中进行文本线Localization的方法，以避免 decompression 和重新压缩的过程，从而降低存储和计算成本。</li>
<li>methods: 提出了一种基于深度特征学习的Modified U-Net架构，称为Compressed Text-Line Localization Network (CompTLL-UNet)，用于实现文本线Localization。</li>
<li>results: 通过对ICDAR2017（cBAD）和ICDAR2019（cBAD）测试集进行训练和测试，实现了在JPEG压缩Domain中的文本线Localization，并reported state-of-the-art perfomance。<details>
<summary>Abstract</summary>
Automatic localization of text-lines in handwritten documents is still an open and challenging research problem. Various writing issues such as uneven spacing between the lines, oscillating and touching text, and the presence of skew become much more challenging when the case of complex handwritten document images are considered for segmentation directly in their respective compressed representation. This is because, the conventional way of processing compressed documents is through decompression, but here in this paper, we propose an idea that employs deep feature learning directly from the JPEG compressed coefficients without full decompression to accomplish text-line localization in the JPEG compressed domain. A modified U-Net architecture known as Compressed Text-Line Localization Network (CompTLL-UNet) is designed to accomplish it. The model is trained and tested with JPEG compressed version of benchmark datasets including ICDAR2017 (cBAD) and ICDAR2019 (cBAD), reporting the state-of-the-art performance with reduced storage and computational costs in the JPEG compressed domain.
</details>
<details>
<summary>摘要</summary>
自动化手写文档中文行的本地化仍然是一个打开的和挑战性的研究问题。不同的写作问题，如文本行间距不均匀、文本抖动和触摸、扭曲等问题，在考虑复杂手写文档图像时变得更加挑战。这是因为，传统的文档处理方法是通过解压缩来处理压缩文档，但在这篇论文中，我们提出了一个想法，即使用深度特征学习直接从JPEG压缩率中提取特征来实现文本行的本地化。我们称之为Compressed Text-Line Localization Network（CompTLL-UNet）。我们设计了一种修改后的U-Net架构，并对其进行训练和测试，使用JPEG压缩版本的标准评测数据集，包括ICDAR2017（cBAD）和ICDAR2019（cBAD）。我们的模型在JPEG压缩领域实现了状态之巅性表现，同时具有减少存储和计算成本的优点。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Artificial-Neural-Networks-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Leaching-Filter-Cake-Moisture-Modeling"><a href="#Application-of-Artificial-Neural-Networks-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Leaching-Filter-Cake-Moisture-Modeling" class="headerlink" title="Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling"></a>Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06138">http://arxiv.org/abs/2308.06138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoume Kazemi, Davood Moradkhani, Alireza A. Alipour</li>
<li>for: 这项研究旨在开发一个人工神经网络模型，用于预测压filtering proces中的蛋白湿度。</li>
<li>methods: 该研究使用了人工神经网络技术，并在288次测试中采用了两种不同的 Filter Fabric（S1和S2）。</li>
<li>results: 研究结果显示，人工神经网络模型可以高度准确地预测压filtering proces中的蛋白湿度，R2值分别为0.88和0.83，MSE值分别为6.243x10-07和1.086x10-06，MAE值分别为0.00056和0.00088。<details>
<summary>Abstract</summary>
Machine Learning (ML) is a powerful tool for material science applications. Artificial Neural Network (ANN) is a machine learning technique that can provide high prediction accuracy. This study aimed to develop an ANN model to predict the cake moisture of the pressure filtration process of zinc production. The cake moisture was influenced by seven parameters: temperature (35 and 65 Celsius), solid concentration (0.2 and 0.38 g/L), pH (2, 3.5, and 5), air-blow time (2, 10, and 15 min), cake thickness (14, 20, 26, and 34 mm), pressure, and filtration time. The study conducted 288 tests using two types of fabrics: polypropylene (S1) and polyester (S2). The ANN model was evaluated by the Coefficient of determination (R2), the Mean Square Error (MSE), and the Mean Absolute Error (MAE) metrics for both datasets. The results showed R2 values of 0.88 and 0.83, MSE values of 6.243x10-07 and 1.086x10-06, and MAE values of 0.00056 and 0.00088 for S1 and S2, respectively. These results indicated that the ANN model could predict the cake moisture of pressure filtration in the zinc leaching process with high accuracy.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）是一种强大的工具，可以应用于材料科学领域。人工神经网络（ANN）是一种机器学习技术，可以提供高精度预测。本研究目的是开发一个ANN模型，以预测压 filtering过程中锻生产中的蛋白湿度。蛋白湿度受到七个参数的影响：温度（35和65摄氏度）、固体浓度（0.2和0.38g/L）、pH（2、3.5和5）、空气喷流时间（2、10和15分）、蛋白厚度（14、20、26和34mm）、压力和过滤时间。研究通过288次测试，使用两种不同的 fabrics： polypropylene（S1）和 polyester（S2）。ANN模型被评估于 Coefficient of determination（R2）、Mean Square Error（MSE）和 Mean Absolute Error（MAE）三个指标，其中R2值分别为0.88和0.83，MSE值分别为6.243x10-07和1.086x10-06，MAE值分别为0.00056和0.00088。这些结果表明，ANN模型可以高精度预测压 filtering过程中的蛋白湿度。
</details></li>
</ul>
<hr>
<h2 id="PDE-Discovery-for-Soft-Sensors-Using-Coupled-Physics-Informed-Neural-Network-with-Akaike’s-Information-Criterion"><a href="#PDE-Discovery-for-Soft-Sensors-Using-Coupled-Physics-Informed-Neural-Network-with-Akaike’s-Information-Criterion" class="headerlink" title="PDE Discovery for Soft Sensors Using Coupled Physics-Informed Neural Network with Akaike’s Information Criterion"></a>PDE Discovery for Soft Sensors Using Coupled Physics-Informed Neural Network with Akaike’s Information Criterion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06132">http://arxiv.org/abs/2308.06132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aina Wang, Pan Qin, Xi-Ming Sun</li>
<li>For: 这篇论文旨在探讨一种基于物理学习的软传感器PDE发现方法，以便更好地监测工业过程中的关键变量。* Methods: 该方法基于物理学习的软传感器PDE模型，并使用了Akaike的准则信息来找到合适的偏微分方程结构。* Results: 实验结果表明，CPINN-AIC方法可以准确地找到合适的偏微分方程结构，并且可以用来预测工业过程中的变量。<details>
<summary>Abstract</summary>
Soft sensors have been extensively used to monitor key variables using easy-to-measure variables and mathematical models. Partial differential equations (PDEs) are model candidates for soft sensors in industrial processes with spatiotemporal dependence. However, gaps often exist between idealized PDEs and practical situations. Discovering proper structures of PDEs, including the differential operators and source terms, can remedy the gaps. To this end, a coupled physics-informed neural network with Akaike's criterion information (CPINN-AIC) is proposed for PDE discovery of soft sensors. First, CPINN is adopted for obtaining solutions and source terms satisfying PDEs. Then, we propose a data-physics-hybrid loss function for training CPINN, in which undetermined combinations of differential operators are involved. Consequently, AIC is used to discover the proper combination of differential operators. Finally, the artificial and practical datasets are used to verify the feasibility and effectiveness of CPINN-AIC for soft sensors. The proposed CPINN-AIC is a data-driven method to discover proper PDE structures and neural network-based solutions for soft sensors.
</details>
<details>
<summary>摘要</summary>
Soft sensors 通过使用容易测量的变量和数学模型来监测关键变量。但是，实际情况中存在idealized PDEs和实际情况之间的差距。发现合适的 PDE 结构，包括偏微分运算和源项，可以解决这些差距。为此，我们提出了物理学习混合数据损失函数（CPINN-AIC），用于PDE发现。首先，我们采用CPINN来获取满足 PDE 的解和源项。然后，我们提出了一种数据物理混合损失函数，其中包含未知的偏微分运算。最后，我们使用AIC来发现合适的偏微分运算结构。 Finally, we use artificial and practical datasets to verify the feasibility and effectiveness of CPINN-AIC for soft sensors. The proposed CPINN-AIC is a data-driven method to discover proper PDE structures and neural network-based solutions for soft sensors.Note: The translation is done using the Simplified Chinese grammar and vocabulary, which is commonly used in mainland China. However, it should be noted that there are different dialects and variations of Chinese, and the translation may vary depending on the specific region or dialect.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-for-Image-based-Traffic-Prediction-across-Cities"><a href="#Uncertainty-Quantification-for-Image-based-Traffic-Prediction-across-Cities" class="headerlink" title="Uncertainty Quantification for Image-based Traffic Prediction across Cities"></a>Uncertainty Quantification for Image-based Traffic Prediction across Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06129">http://arxiv.org/abs/2308.06129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alextimans/traffic4cast-uncertainty">https://github.com/alextimans/traffic4cast-uncertainty</a></li>
<li>paper_authors: Alexander Timans, Nina Wiedemann, Nishant Kumar, Ye Hong, Martin Raubal</li>
<li>for: 这个论文的目的是提高深度学习模型在智能交通系统中的可解释性，以便更好地做出决策和提高模型的部署潜力。</li>
<li>methods: 这个论文使用了两种 эпистемиче和两种 aleatoric 不确定性评估方法来评估深度学习模型的不确定性，并对多个城市和时间段进行比较。</li>
<li>results: 研究发现，可以通过使用不确定性评估方法来获得有意义的不确定性估计，并且可以用这些估计来检测城市交通动力学的异常情况。在一个 Moscow 的示例研究中，我们发现了时间和空间效应的影响于城市交通行为。<details>
<summary>Abstract</summary>
Despite the strong predictive performance of deep learning models for traffic prediction, their widespread deployment in real-world intelligent transportation systems has been restrained by a lack of interpretability. Uncertainty quantification (UQ) methods provide an approach to induce probabilistic reasoning, improve decision-making and enhance model deployment potential. To gain a comprehensive picture of the usefulness of existing UQ methods for traffic prediction and the relation between obtained uncertainties and city-wide traffic dynamics, we investigate their application to a large-scale image-based traffic dataset spanning multiple cities and time periods. We compare two epistemic and two aleatoric UQ methods on both temporal and spatio-temporal transfer tasks, and find that meaningful uncertainty estimates can be recovered. We further demonstrate how uncertainty estimates can be employed for unsupervised outlier detection on changes in city traffic dynamics. We find that our approach can capture both temporal and spatial effects on traffic behaviour in a representative case study for the city of Moscow. Our work presents a further step towards boosting uncertainty awareness in traffic prediction tasks, and aims to highlight the value contribution of UQ methods to a better understanding of city traffic dynamics.
</details>
<details>
<summary>摘要</summary>
启示深度学习模型对交通预测的强大预测能力，实际应用中的广泛部署却受到了不可预测性的限制。不确定性评估（UQ）方法可以带来 probabilistic reasoning，改善决策和提高模型部署的潜力。为了了解现有UQ方法对交通预测的用途和获得的不确定性与城市范围内交通动力学的关系，我们对多座城市和多个时间段的大规模图像基本交通数据进行了 investigate。我们比较了两种 эпистеمic和两种 aleatoric UQ方法的性能在时间和空间转移任务上，并发现了有意义的不确定性估计可以被恢复。我们还示出了如何使用不确定性估计进行无supervised outlier检测，检测城市交通动力学的变化。我们在 Moskva 城市的示例研究中发现，我们的方法可以捕捉到时间和空间效应的交通行为。我们的工作是 uncertainty awareness 在交通预测任务中的一个进一步步骤，旨在高亮不确定性评估对城市交通动力学的理解的重要性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Control-Policies-for-Variable-Objectives-from-Offline-Data"><a href="#Learning-Control-Policies-for-Variable-Objectives-from-Offline-Data" class="headerlink" title="Learning Control Policies for Variable Objectives from Offline Data"></a>Learning Control Policies for Variable Objectives from Offline Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06127">http://arxiv.org/abs/2308.06127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Weber, Phillip Swazinna, Daniel Hein, Steffen Udluft, Volkmar Sterzing</li>
<li>for: 本研究旨在提供一种可靠的机器学习方法，用于控制动态系统，特别是当直接与环境交互不可用时。</li>
<li>methods: 本研究使用了模型基于政策搜索方法的扩展，即变量目标策略（VOP）。这种方法使得政策可以高效地泛化到多种目标，即在奖励函数中的参数。</li>
<li>results: 通过对目标函数中的参数进行调整，用户可以在运行时调整政策的行为或重新平衡优化目标，无需收集更多的观察批量或重新训练。<details>
<summary>Abstract</summary>
Offline reinforcement learning provides a viable approach to obtain advanced control strategies for dynamical systems, in particular when direct interaction with the environment is not available. In this paper, we introduce a conceptual extension for model-based policy search methods, called variable objective policy (VOP). With this approach, policies are trained to generalize efficiently over a variety of objectives, which parameterize the reward function. We demonstrate that by altering the objectives passed as input to the policy, users gain the freedom to adjust its behavior or re-balance optimization targets at runtime, without need for collecting additional observation batches or re-training.
</details>
<details>
<summary>摘要</summary>
转换文本为简化中文：</SYS>在线上学习不可靠的方法可以提供先进的控制策略 для动力系统，特别是当直接与环境进行交互不可用时。本文提出了基于模型的政策搜索方法的概念扩展，称为变量目标策略（VOP）。通过这种方法，政策被训练以通用化效率地处理多种目标，这些目标参数化奖励函数。我们示例显示，通过在运行时更改目标，用户可以在不需要收集更多观察批处或重新训练的情况下，通过变化目标来调整行为或重新平衡优化目标。
</details></li>
</ul>
<hr>
<h2 id="Learning-Deductive-Reasoning-from-Synthetic-Corpus-based-on-Formal-Logic"><a href="#Learning-Deductive-Reasoning-from-Synthetic-Corpus-based-on-Formal-Logic" class="headerlink" title="Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic"></a>Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07336">http://arxiv.org/abs/2308.07336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hitachi-nlp/fld">https://github.com/hitachi-nlp/fld</a></li>
<li>paper_authors: Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa</li>
<li>for: 本研究旨在使语言模型（LM）学习逻辑推理能力。 previous studies使用特定的推理规则生成推理例子，但这些规则有限或else arbitrary，这限制了获得的逻辑推理能力的一致性。</li>
<li>methods: 我们采用基于正式逻辑理论的准确的推理规则集，可以 derivation 任何其他推理规则。我们名为这种推理规则集为 $\textbf{FLD}$（正式逻辑推理）。</li>
<li>results: 我们实验表明，使用 $\textbf{FLD}$ 推理规则集训练LMs，LMs可以获得更一致的逻辑推理能力。此外，我们还识别了逻辑推理能力中哪些方面可以通过推理 corpora 增强LMs，那些方面无法增强。最后，基于这些结果，我们讨论未来如何使用推理 corpora 或其他方法来解决每个方面的问题。我们发布了代码、数据和模型。<details>
<summary>Abstract</summary>
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each aspect. We release the code, data, and models.
</details>
<details>
<summary>摘要</summary>
我们研究了一种基于合成语料库的方法，用于语言模型（LM）学习逻辑推理能力。之前的研究通过特定的推理规则生成了推理示例，但这些规则是有限的或else是arbitrary的。这会限制学习得到的逻辑推理能力的通用性。我们重新思考了这一点，采用基于形式逻辑理论的固定的推理规则，可以在多步骤中组合以 derivation任何其他的推理规则。我们employmaterially verify that LMs trained on our proposed corpora，which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability。此外，我们还确定了推理能力中哪些方面可以通过推理 corpora进行增强，以及哪些方面无法进行增强。最后，基于这些结果，我们讨论了将来采用推理 corpora或其他方法对每个方面的应用。我们释放了代码、数据和模型。
</details></li>
</ul>
<hr>
<h2 id="Hawkes-Processes-with-Delayed-Granger-Causality"><a href="#Hawkes-Processes-with-Delayed-Granger-Causality" class="headerlink" title="Hawkes Processes with Delayed Granger Causality"></a>Hawkes Processes with Delayed Granger Causality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06106">http://arxiv.org/abs/2308.06106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Yang, Hengyuan Miao, Shuang Li</li>
<li>for: 本研究旨在Explicitly Modeling delayed Granger causal effects based on multivariate Hawkes processes, 即 causal event usually takes some time to exert an effect, 研究这个时间延迟自身的科学意义。</li>
<li>methods: 我们首先证明了延迟参数的可 identificability under mild conditions, 然后 investigate a model estimation method under complex setting, 即 want to infer the posterior distribution of time lags and understand how this distribution varies across different scenarios, 我们将时延 treated as latent variables, 并使用Variational Auto-Encoder (VAE) algorithm to approximate the posterior distribution of time lags.</li>
<li>results: 我们 empirically evaluate our model’s event prediction and time-lag inference accuracy on synthetic and real data, achieving promising results.<details>
<summary>Abstract</summary>
We aim to explicitly model the delayed Granger causal effects based on multivariate Hawkes processes. The idea is inspired by the fact that a causal event usually takes some time to exert an effect. Studying this time lag itself is of interest. Given the proposed model, we first prove the identifiability of the delay parameter under mild conditions. We further investigate a model estimation method under a complex setting, where we want to infer the posterior distribution of the time lags and understand how this distribution varies across different scenarios. We treat the time lags as latent variables and formulate a Variational Auto-Encoder (VAE) algorithm to approximate the posterior distribution of the time lags. By explicitly modeling the time lags in Hawkes processes, we add flexibility to the model. The inferred time-lag posterior distributions are of scientific meaning and help trace the original causal time that supports the root cause analysis. We empirically evaluate our model's event prediction and time-lag inference accuracy on synthetic and real data, achieving promising results.
</details>
<details>
<summary>摘要</summary>
我们目标是显式地模型延迟的格兰格 causal 效应基于多变量 Hawkes 过程。这个想法源于事件引起效应通常需要一些时间。研究这个时间延迟本身很有趣。给出的模型，我们首先证明延迟参数的可识别性于轻量级 услови下。我们进一步调查了一种复杂的设定下的模型估计方法，我们想要从多个enario中推断时延参数的 posterior 分布，并理解这个分布在不同enario下如何变化。我们将时延参数作为隐藏变量，并采用Variational Auto-Encoder（VAE）算法来近似 posterior 分布。通过显式地模型 Hawkes 过程中的时延参数，我们增加了模型的灵活性。经验证明我们的模型在实验数据上的事件预测和时延参数推断精度都很高。
</details></li>
</ul>
<hr>
<h2 id="Composable-Function-preserving-Expansions-for-Transformer-Architectures"><a href="#Composable-Function-preserving-Expansions-for-Transformer-Architectures" class="headerlink" title="Composable Function-preserving Expansions for Transformer Architectures"></a>Composable Function-preserving Expansions for Transformer Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06103">http://arxiv.org/abs/2308.06103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Gesmundo, Kaitlin Maile</li>
<li>for: 提高现代神经网络的训练成本，特别是计算和时间成本。</li>
<li>methods: 提出六种可 композиitely 的变换，用于逐步增加 transformer 类神经网络的大小，保持功能完整性。</li>
<li>results: 证明每种变换都可以保持函数完整性，并且可以有效地升级模型规模。<details>
<summary>Abstract</summary>
Training state-of-the-art neural networks requires a high cost in terms of compute and time. Model scale is recognized to be a critical factor to achieve and improve the state-of-the-art. Increasing the scale of a neural network normally requires restarting from scratch by randomly initializing all the parameters of the model, as this implies a change of architecture's parameters that does not allow for a straightforward transfer of knowledge from smaller size models. In this work, we propose six composable transformations to incrementally increase the size of transformer-based neural networks while preserving functionality, allowing to expand the capacity of the model as needed. We provide proof of exact function preservation under minimal initialization constraints for each transformation. The proposed methods may enable efficient training pipelines for larger and more powerful models by progressively expanding the architecture throughout training.
</details>
<details>
<summary>摘要</summary>
培训现代神经网络需要高效计算和时间成本。模型缩放被认为是提高现状的关键因素。在增加模型缩放时，通常需要从scratch开始，随机初始化整个模型的参数，因为这会导致模型结构中参数的变化，不允许小型模型知识的直接传递。在这项工作中，我们提出六种可组合的变换来逐步增加基于转换器的神经网络缩放，保持功能完整性，以便在训练过程中逐步扩展模型的容量。我们提供了准确功能保持的证明，并且在 minimal initialization constraints 下进行证明。这些方法可能会帮助建立更大更强的模型，并通过逐步扩展模型结构来实现高效的训练管道。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Visual-Counterfactual-Explanations-–-Towards-Systematic-Quantitative-Evaluation"><a href="#Diffusion-based-Visual-Counterfactual-Explanations-–-Towards-Systematic-Quantitative-Evaluation" class="headerlink" title="Diffusion-based Visual Counterfactual Explanations – Towards Systematic Quantitative Evaluation"></a>Diffusion-based Visual Counterfactual Explanations – Towards Systematic Quantitative Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06100">http://arxiv.org/abs/2308.06100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cairo-thws/dbvce_eval">https://github.com/cairo-thws/dbvce_eval</a></li>
<li>paper_authors: Philipp Vaeth, Alexander M. Fruehwald, Benjamin Paassen, Magda Gregorova</li>
<li>For: This paper aims to provide a systematic and quantitative evaluation framework for visual counterfactual explanations (VCE) methods, and to explore the effects of crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet).* Methods: The paper proposes a framework for evaluating VCE methods using a minimal set of metrics, and conducts a battery of ablation-like experiments generating thousands of VCEs for a suite of classifiers of various complexity, accuracy, and robustness.* Results: The paper finds multiple directions for future advancements and improvements of VCE methods, and provides a valuable guidance for researchers in the field fostering consistency and transparency in the assessment of counterfactual explanations.Here is the same information in Simplified Chinese text:* For: 这篇论文目标是提供一个系统的和量化的评估框架，以帮助评估视觉对称解释（VCE）方法，并探索最新的扩散基于生成模型中的关键设计选择对自然图像分类（ImageNet）VCE的影响。* Methods: 论文提出一个评估VCE方法的最小集合的度量，并通过大量的拟合实验生成了不同复杂性、准确率和稳定性的多个分类器的VCE。* Results: 论文发现了未来的进步和改进的方向，并提供了一个有价值的指南，以便在评估对称解释中增加一致性和透明度。<details>
<summary>Abstract</summary>
Latest methods for visual counterfactual explanations (VCE) harness the power of deep generative models to synthesize new examples of high-dimensional images of impressive quality. However, it is currently difficult to compare the performance of these VCE methods as the evaluation procedures largely vary and often boil down to visual inspection of individual examples and small scale user studies. In this work, we propose a framework for systematic, quantitative evaluation of the VCE methods and a minimal set of metrics to be used. We use this framework to explore the effects of certain crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet). We conduct a battery of ablation-like experiments, generating thousands of VCEs for a suite of classifiers of various complexity, accuracy and robustness. Our findings suggest multiple directions for future advancements and improvements of VCE methods. By sharing our methodology and our approach to tackle the computational challenges of such a study on a limited hardware setup (including the complete code base), we offer a valuable guidance for researchers in the field fostering consistency and transparency in the assessment of counterfactual explanations.
</details>
<details>
<summary>摘要</summary>
最新的视觉对比解释方法（VCE）利用深度生成模型Synthesize高维像素图像的新示例，质量非常高。然而，目前很难比较这些VCE方法的性能，因为评估方法大多不同，经常降到视觉检查具体示例和小规模用户研究。在这项工作中，我们提出了一个系统性评估VCE方法的框架和最小的 metric集，并使用这些框架来探索 diffusion-based生成模型在自然图像分类（ImageNet）中VCE的效果。我们进行了一系列减少-like实验，生成了数千个VCE，用于一组不同的分类器，包括不同的复杂度、准确率和鲁棒性。我们的发现建议了未来VCE方法的进一步改进。通过分享我们的方法和我们对限制硬件设置（包括完整的代码库）的处理方式，我们提供了行业研究人员的有价值指南，促进了透明度和一致性在对对比解释的评估中。
</details></li>
</ul>
<hr>
<h2 id="Neural-Conversation-Models-and-How-to-Rein-Them-in-A-Survey-of-Failures-and-Fixes"><a href="#Neural-Conversation-Models-and-How-to-Rein-Them-in-A-Survey-of-Failures-and-Fixes" class="headerlink" title="Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes"></a>Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06095">http://arxiv.org/abs/2308.06095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Galetzka, Anne Beyer, David Schlangen</li>
<li>for: 本研究探讨了基于强大语言模型的开放领域对话系统，以做出合适的对话贡献。</li>
<li>methods: 研究人员使用了不同的截止点和训练策略来控制语言模型，以确保贡献的优质。</li>
<li>results: 研究人员发现了一些有前途的方法，并建议了未来研究的新方向。<details>
<summary>Abstract</summary>
Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising attempts and suggest novel ways for future research.
</details>
<details>
<summary>摘要</summary>
现代条件语言模型能够继续任何类型的文本源，并且在看起来很流畅地进行交流。这一事实激发了基于强大语言模型的开放领域对话系统的研究，旨在通过生成相应的贡献来模拟对话伙伴。从语言学角度来看，参与对话的复杂度很高。在这种情况下，我们根据这个特定的研究领域来解释格雷斯的协作对话原则，并将文献分为合适贡献的几个方面：一个神经网络对话模型需要流畅、有用、一致、 coherent 和遵循社会规范。为确保这些质量，当前的方法在不同的 intervening point 上尝试控制基础语言模型，例如数据、训练方法或解码。按照这些类别和 intervening point 排序，我们讨论了有前途的尝试，并建议未来研究的新方法。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Logic-Rule-Learning-for-Temporal-Point-Processes"><a href="#Reinforcement-Logic-Rule-Learning-for-Temporal-Point-Processes" class="headerlink" title="Reinforcement Logic Rule Learning for Temporal Point Processes"></a>Reinforcement Logic Rule Learning for Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06094">http://arxiv.org/abs/2308.06094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Yang, Lu Wang, Kun Gao, Shuang Li</li>
<li>for: 用于解释 temporal events 的发生</li>
<li>methods: 使用 temporal point process modeling and learning framework，逐渐优化规则集和其重要性，并通过 neural search policy 生成新规则</li>
<li>results: 在 synthetic 和实际医疗数据上获得了promising的结果<details>
<summary>Abstract</summary>
We propose a framework that can incrementally expand the explanatory temporal logic rule set to explain the occurrence of temporal events. Leveraging the temporal point process modeling and learning framework, the rule content and weights will be gradually optimized until the likelihood of the observational event sequences is optimal. The proposed algorithm alternates between a master problem, where the current rule set weights are updated, and a subproblem, where a new rule is searched and included to best increase the likelihood. The formulated master problem is convex and relatively easy to solve using continuous optimization, whereas the subproblem requires searching the huge combinatorial rule predicate and relationship space. To tackle this challenge, we propose a neural search policy to learn to generate the new rule content as a sequence of actions. The policy parameters will be trained end-to-end using the reinforcement learning framework, where the reward signals can be efficiently queried by evaluating the subproblem objective. The trained policy can be used to generate new rules in a controllable way. We evaluate our methods on both synthetic and real healthcare datasets, obtaining promising results.
</details>
<details>
<summary>摘要</summary>
我们提出了一个框架，可以逐步扩展解释时间事件的发生。利用时间点处理模型和学习框架，规则内容和权重将被逐步优化，直到观测事件序列的可能性最高。我们的算法会 alternate между主问题和子问题。主问题中，当前规则集权重将被更新；而子问题中，一个新的规则将被搜索并添加到最大化可能性。我们形式ulated主问题是凸型的，可以使用连续优化来解决；而子问题则需要搜索庞大的 combinatorial 规则 predicate 和关系空间。为了解决这个挑战，我们提出了一种神经搜索策略，可以学习生成新规则的内容作为一个序列动作。这个策略的参数将通过可行学习框架进行培养，其中的奖励信号可以快速地查询由辅助问题的目标函数来提供。已经训练的策略可以用于生成新规则的控制方式。我们对具有 sintetic 和实际医疗数据的方法进行了评估，获得了有前途的结果。
</details></li>
</ul>
<hr>
<h2 id="Experts-Weights-Averaging-A-New-General-Training-Scheme-for-Vision-Transformers"><a href="#Experts-Weights-Averaging-A-New-General-Training-Scheme-for-Vision-Transformers" class="headerlink" title="Experts Weights Averaging: A New General Training Scheme for Vision Transformers"></a>Experts Weights Averaging: A New General Training Scheme for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06093">http://arxiv.org/abs/2308.06093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li, Tao Chen, Wanli Ouyang</li>
<li>for: 提高 ViT 模型的性能而不增加推理成本</li>
<li>methods: 使用 Mixture-of-Experts (MoE) 实现对 ViT 模型的训练，并在训练和推理阶段之间进行分解</li>
<li>results: 对多个 2D 和 3D 视觉任务、ViT 架构和数据集进行了全面的实验 validate 提议的训练方法的效果和普适性，同时还可以应用于细化 ViT 模型的 fine-tuning 过程中提高性能。<details>
<summary>Abstract</summary>
Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the ViT with specially designed, more efficient MoEs that assign tokens to experts by random uniform partition, and perform Experts Weights Averaging (EWA) on these MoEs at the end of each iteration. After training, we convert each MoE into an FFN by averaging the experts, transforming the model back into original ViT for inference. We further provide a theoretical analysis to show why and how it works. Comprehensive experiments across various 2D and 3D visual tasks, ViT architectures, and datasets validate the effectiveness and generalizability of the proposed training scheme. Besides, our training scheme can also be applied to improve performance when fine-tuning ViTs. Lastly, but equally important, the proposed EWA technique can significantly improve the effectiveness of naive MoE in various 2D visual small datasets and 3D visual tasks.
</details>
<details>
<summary>摘要</summary>
《Structural re-parameterization是一种通用训练方案 для Convolutional Neural Networks (CNNs),它可以提高性能而不增加推理成本。在Vision Transformers (ViTs)逐渐超越CNNs的视觉任务中，有人可能会提问：如果存在专门 дляViTs的训练方案，可以提高性能而不增加推理成本？Recently, Mixture-of-Experts (MoE)has attracted increasing attention,因为它可以高效地扩展Transformers的容量在固定成本下。考虑到MoE可以被视为多支分支结构，那么我们可以使用MoE来实现ViTs的训练方案类似于structural re-parameterization。在这篇论文中，我们答于这些问题，并提出了一种新的通用训练策略 дляViTs。 Specifically,我们在训练阶段将ViTs中的一些Feed-Forward Networks (FFNs)替换为特制的、更高效的MoEs，并在每个迭代结束后进行Experts Weights Averaging (EWA)。之后，我们将MoEs转换成FFNs，并将模型转换回原始的ViTs模型进行推理。我们还提供了一种理论分析，以证明这种训练方案的有效性和如何工作。我们在多种2D和3D视觉任务、ViT结构和数据集上进行了广泛的实验，证明了提议的训练策略的效果和通用性。此外，我们的训练策略还可以用于改进ViTs的性能when fine-tuning。最后，但也非常重要的是，我们提出的EWA技术可以在多种2D视觉小数据集和3D视觉任务中显著提高MoE的效果。》
</details></li>
</ul>
<hr>
<h2 id="Toward-a-Better-Understanding-of-Loss-Functions-for-Collaborative-Filtering"><a href="#Toward-a-Better-Understanding-of-Loss-Functions-for-Collaborative-Filtering" class="headerlink" title="Toward a Better Understanding of Loss Functions for Collaborative Filtering"></a>Toward a Better Understanding of Loss Functions for Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06091">http://arxiv.org/abs/2308.06091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/psm1206/mawu">https://github.com/psm1206/mawu</a></li>
<li>paper_authors: Seongmin Park, Mincheol Yoon, Jae-woong Lee, Hogun Park, Jongwuk Lee</li>
<li>for: 这篇论文主要研究了相互推荐系统中的协同推荐技术，具体来说是分析现有的损失函数之间的关系，并提出了一种新的损失函数来改进现有的协同推荐模型。</li>
<li>methods: 该论文使用了数学分析来探究现有损失函数的关系，并在这基础上提出了一种新的损失函数called Margin-aware Alignment and Weighted Uniformity (MAWU)，它通过（i）margin-aware alignment（MA）和（ii）weighted uniformity（WU）来改进协同推荐模型的设计。</li>
<li>results: 实验结果表明，当 equiped with MAWU，MF和LightGCN相比现有的协同推荐模型，在三个公共数据集上具有相当或更高的性能。<details>
<summary>Abstract</summary>
Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU is two-fold: (i) margin-aware alignment (MA) mitigates user/item-specific popularity biases, and (ii) weighted uniformity (WU) adjusts the significance between user and item uniformities to reflect the inherent characteristics of datasets. Extensive experimental results show that MF and LightGCN equipped with MAWU are comparable or superior to state-of-the-art CF models with various loss functions on three public datasets.
</details>
<details>
<summary>摘要</summary>
合作 filtering (CF) 是现代推荐系统中的关键技术。 CF 模型的学习过程通常包括三个组成部分：交互编码器、损失函数和负样本。虽然现有的研究已经提出了许多不同的 CF 模型，但是最近的研究表明，只是修改损失函数的设计可以获得显著性能提升。本文分析了现有损失函数之间的关系。我们的数学分析表明，前一些损失函数可以被解释为对用户和项目表示的对齐和分布均匀函数：（i）对用户和项目表示进行对齐（ii）对用户和项目分布进行均匀化。根据这一分析，我们提出了一种新的损失函数，称为 Margin-aware Alignment and Weighted Uniformity (MAWU)。MAWU 的关键创新有两个方面：（i）对用户/项目特有的流行偏好进行缓和（ii）根据数据集的特点进行加权均匀化。我们进行了广泛的实验研究，发现 MF 和 LightGCN 搭配 MAWU 与 state-of-the-art CF 模型相比，在三个公共数据集上具有相似或更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Safeguarding-Learning-based-Control-for-Smart-Energy-Systems-with-Sampling-Specifications"><a href="#Safeguarding-Learning-based-Control-for-Smart-Energy-Systems-with-Sampling-Specifications" class="headerlink" title="Safeguarding Learning-based Control for Smart Energy Systems with Sampling Specifications"></a>Safeguarding Learning-based Control for Smart Energy Systems with Sampling Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06069">http://arxiv.org/abs/2308.06069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chih-Hong Cheng, Venkatesh Prasad Venkataramanan, Pragya Kirti Gupta, Yun-Fei Hsu, Simon Burton</li>
<li>for: 这篇论文是关于使用强化学习控制能源系统中的挑战，特别是在保证性和安全性两个方面的。</li>
<li>methods: 论文详细介绍了在实时逻辑中强化学习安全要求的方法，包括将实时逻辑转换为线性逻辑（LTL），以便利用高级工程技术，如安全学习盾和正式验证。</li>
<li>results: 论文表明，通过将实时逻辑转换为LTL，可以在强化学习过程中提供更高的安全性保证，并且可以通过统计模型检查来获得更高的满意度。<details>
<summary>Abstract</summary>
We study challenges using reinforcement learning in controlling energy systems, where apart from performance requirements, one has additional safety requirements such as avoiding blackouts. We detail how these safety requirements in real-time temporal logic can be strengthened via discretization into linear temporal logic (LTL), such that the satisfaction of the LTL formulae implies the satisfaction of the original safety requirements. The discretization enables advanced engineering methods such as synthesizing shields for safe reinforcement learning as well as formal verification, where for statistical model checking, the probabilistic guarantee acquired by LTL model checking forms a lower bound for the satisfaction of the original real-time safety requirements.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:我们研究和开发用控制能源系统的强化学习，同时考虑性能要求和安全要求，如避免黑OUT。我们详细说明如何通过离散到线性时间逻辑（LTL）来加强安全要求，使得满足LTL公式的满足性意味着满足原始的安全要求。这种离散Enabled advanced工程技术，如生成安全屏障和正式验证，以及统计模型检查中的概率保证，这个保证是原始时间安全要求满足的下界。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-flow-disaggregation-for-hydropower-plant-management"><a href="#Deep-learning-based-flow-disaggregation-for-hydropower-plant-management" class="headerlink" title="Deep learning-based flow disaggregation for hydropower plant management"></a>Deep learning-based flow disaggregation for hydropower plant management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11631">http://arxiv.org/abs/2308.11631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Zhang</li>
<li>for:  Norwegian hydropower plant management</li>
<li>methods:  deep learning-based time series disaggregation model</li>
<li>results:  promising results for disaggregating daily flow into hourly flow<details>
<summary>Abstract</summary>
High temporal resolution data is a vital resource for hydropower plant management. Currently, only daily resolution data are available for most of Norwegian hydropower plant, however, to achieve more accurate management, sub-daily resolution data are often required. To deal with the wide absence of sub-daily data, time series disaggregation is a potential tool. In this study, we proposed a time series disaggregation model based on deep learning, the model is tested using flow data from a Norwegian flow station, to disaggregate the daily flow into hourly flow. Preliminary results show some promising aspects for the proposed model.
</details>
<details>
<summary>摘要</summary>
高时间分辨率数据是 Norway 水力发电厂的重要资源。目前，大多数 Norwegian 水力发电厂的数据只有每天的分辨率，但是为更准确的管理， often 需要更高的时间分辨率数据。为了解决宽泛的无法获得 sub-daily 数据的问题，时间序列分解是一种可能的工具。本研究提出了基于深度学习的时间序列分解模型，在使用挪威流站的流量数据进行测试，以分解每天的流量为每小时的流量。初步结果显示该模型具有一些有前途的特点。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-SGD-with-Polyak-stepsize-and-Line-search-Robust-Convergence-and-Variance-Reduction"><a href="#Adaptive-SGD-with-Polyak-stepsize-and-Line-search-Robust-Convergence-and-Variance-Reduction" class="headerlink" title="Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction"></a>Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06058">http://arxiv.org/abs/2308.06058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaowen Jiang, Sebastian U. Stich</li>
<li>for: 这个论文目的是提出两种新的随机波兰梯（AdaSPS和AdaSLS），以确保在非 interpolative 设定下进行训练，并且在对 convex 和强 convex 函数进行训练时维持下线性和线性的 converges 速率。</li>
<li>methods: 这两种新算法使用了随机波兰梯和随机搜索，并且使用了一种新的减少偏差的技术来提高速度。</li>
<li>results: 这些新算法可以在非 interpolative 设定下进行训练，并且可以在对 convex 和强 convex 函数进行训练时维持下线性和线性的 converges 速率，并且可以和 AdaSVRG 的速率匹配，但是不需要内部外部循环结构。<details>
<summary>Abstract</summary>
The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al. [2022]), this approach results in slower convergence rates for convex and over-parameterized models. In this work, we make two contributions: Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value as input. Secondly, we equip AdaSPS and AdaSLS with a novel variance reduction technique and obtain algorithms that require $\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without variance reduction in the non-interpolation regimes. Moreover, our result matches the fast rates of AdaSVRG but removes the inner-outer-loop structure, which is easier to implement and analyze. Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.
</details>
<details>
<summary>摘要</summary>
Recently, the stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for stochastic gradient descent (SGD) have been proposed and have shown great effectiveness in training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution, which may result in a worse output than the initial guess. To address this issue, artificially decreasing the adaptive stepsize has been proposed (Orvieto et al., 2022), but this approach leads to slower convergence rates for convex and over-parameterized models.In this work, we make two contributions:Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS does not require knowledge of problem-dependent parameters, and AdaSPS only requires a lower bound of the optimal function value as input.Secondly, we equip AdaSPS and AdaSLS with a novel variance reduction technique, and obtain algorithms that require $\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without variance reduction in the non-interpolation regimes. Moreover, our result matches the fast rates of AdaSVRG but removes the inner-outer-loop structure, which is easier to implement and analyze.Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.
</details></li>
</ul>
<hr>
<h2 id="Cost-effective-On-device-Continual-Learning-over-Memory-Hierarchy-with-Miro"><a href="#Cost-effective-On-device-Continual-Learning-over-Memory-Hierarchy-with-Miro" class="headerlink" title="Cost-effective On-device Continual Learning over Memory Hierarchy with Miro"></a>Cost-effective On-device Continual Learning over Memory Hierarchy with Miro</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06053">http://arxiv.org/abs/2308.06053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyue Ma, Suyeon Jeong, Minjia Zhang, Di Wang, Jonghyun Choi, Myeongjae Jeon</li>
<li>for: 本研究旨在实现Edge设备上的持续学习（Continual Learning，CL）系统，以提高数据隐私和能源效率。</li>
<li>methods: 本研究使用层次memory replay的CL方法，并开发了一个名为Miro的系统运行时，用于在Edge设备上动态配置CL系统以实现最佳成本效率。</li>
<li>results: 对baseline系统进行比较，Miro显示了显著的成本效率提升。<details>
<summary>Abstract</summary>
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperforms baseline systems we build for comparison, consistently achieving higher cost-effectiveness.
</details>
<details>
<summary>摘要</summary>
Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by dynamically configuring the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead.Extensive evaluations show that Miro significantly outperforms baseline systems we built for comparison, consistently achieving higher cost-effectiveness.
</details></li>
</ul>
<hr>
<h2 id="Towards-Instance-adaptive-Inference-for-Federated-Learning"><a href="#Towards-Instance-adaptive-Inference-for-Federated-Learning" class="headerlink" title="Towards Instance-adaptive Inference for Federated Learning"></a>Towards Instance-adaptive Inference for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06051">http://arxiv.org/abs/2308.06051</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chunmeifeng/fedins">https://github.com/chunmeifeng/fedins</a></li>
<li>paper_authors: Chun-Mei Feng, Kai Yu, Nian Liu, Xinxing Xu, Salman Khan, Wangmeng Zuo</li>
<li>for: 这个论文的目的是提出一种基于联合学习（Federated Learning，FL）框架的实例适应性推理方法，以提高FL在复杂实际数据上的性能。</li>
<li>methods: 这个论文使用了一种基于缩放和偏移（scale and shift）的深度特征方法（SSF），以及一种客户端启发式推理方法（instance-adaptive inference），以适应实际数据上的实例差异性。</li>
<li>results: 对于Tiny-ImageNet dataset，这个方法比顶峰性能的方法提高6.64%，而且通信成本低于15%。<details>
<summary>Abstract</summary>
Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communication cost. To enable instance-adaptive inference, for a given instance, we dynamically find the best-matched SSF subsets from the pool and aggregate them to generate an adaptive SSF specified for the instance, thereby reducing the intra-client as well as the inter-client heterogeneity. Extensive experiments show that our FedIns outperforms state-of-the-art FL algorithms, e.g., a 6.64\% improvement against the top-performing method with less than 15\% communication cost on Tiny-ImageNet. Our code and models will be publicly released.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式学习 paradigm，允许多个客户端学习一个强大的全球模型，通过Client中的本地训练数据进行汇总。然而，全球模型的性能经常受到客户端数据之间的非同质化的影响，需要广泛的减少客户端数据之间的不同性。此外，我们注意到了复杂的实际数据中的内部客户端数据不同性，也会严重降低 FL 性能。在这篇论文中，我们提出了一种新的 FL 算法，即 FedIns，以处理内部客户端数据不同性。我们在 FL 框架中实现了实例适应的推理，而不需要巨大的实例适应模型。我们首先在每个客户端上训练一个可缩放和调整的深度特征池（SSF），并在服务器端将这些 SSF 池进行汇总，以保持低的通信成本。为实现实例适应推理，对于一个给定的实例，我们在实例级别 dynamically 找到最佳适应的 SSF 子集，并将这些子集进行汇总，以生成适应该实例的 adaptive SSF。这有助于降低内部客户端数据不同性以及客户端数据之间的不同性。我们的 FedIns 在 Tiny-ImageNet 上比顶尖方法提供了6.64%的提升，并且与之前的最好方法在 less than 15% 的通信成本下。我们将代码和模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="AI-Assisted-Investigation-of-On-Chain-Parameters-Risky-Cryptocurrencies-and-Price-Factors"><a href="#AI-Assisted-Investigation-of-On-Chain-Parameters-Risky-Cryptocurrencies-and-Price-Factors" class="headerlink" title="AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors"></a>AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08554">http://arxiv.org/abs/2308.08554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdulrezzak Zekiye, Semih Utku, Fadi Amroush, Oznur Ozkasap<br>for:This paper aims to analyze historical data and use artificial intelligence algorithms to identify the factors affecting a cryptocurrency’s price and to find risky cryptocurrencies.methods:The paper uses on-chain parameters to analyze historical cryptocurrency data and employs clustering and classification techniques to group cryptocurrencies based on their on-chain characteristics. The paper also uses multiple classifiers to predict whether a cryptocurrency is risky or not.results:The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. The paper also found a significant negative correlation between cryptocurrency price and maximum and total supply, as well as a weak positive correlation between price and 24-hour trading volume. Additionally, the paper clustered cryptocurrencies into five distinct groups based on their on-chain parameters, and obtained the best f1-score of 76% using K-Nearest Neighbor for predicting risky cryptocurrencies.<details>
<summary>Abstract</summary>
Cryptocurrencies have become a popular and widely researched topic of interest in recent years for investors and scholars. In order to make informed investment decisions, it is essential to comprehend the factors that impact cryptocurrency prices and to identify risky cryptocurrencies. This paper focuses on analyzing historical data and using artificial intelligence algorithms on on-chain parameters to identify the factors affecting a cryptocurrency's price and to find risky cryptocurrencies. We conducted an analysis of historical cryptocurrencies' on-chain data and measured the correlation between the price and other parameters. In addition, we used clustering and classification in order to get a better understanding of a cryptocurrency and classify it as risky or not. The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. Our analysis revealed a significant negative correlation between cryptocurrency price and maximum and total supply, as well as a weak positive correlation between price and 24-hour trading volume. Moreover, we clustered cryptocurrencies into five distinct groups using their on-chain parameters, which provides investors with a more comprehensive understanding of a cryptocurrency when compared to those clustered with it. Finally, by implementing multiple classifiers to predict whether a cryptocurrency is risky or not, we obtained the best f1-score of 76% using K-Nearest Neighbor.
</details>
<details>
<summary>摘要</summary>
digital currencies 在最近几年内已经成为投资者和学者关注的热点话题。为了做出 Informed 投资决策，需要了解 криптовалюencies 价格的影响因素并确定风险较高的 криптовалюencies。这篇论文通过分析历史数据和使用人工智能算法对 chain 参数进行分析，以确定 криптовалюencies 价格的影响因素和风险评估。我们对历史 криптовалюencies 的 chain 数据进行分析，并测量价格和其他参数之间的相关性。此外，我们还使用聚类和分类来更好地理解 криптовалюencies，并将其分为五个不同类别。最后，我们通过应用多种分类器来预测 криптовалюencies 是否为风险的，并获得了最佳的 f1 分数为 76%。
</details></li>
</ul>
<hr>
<h2 id="Controlling-Character-Motions-without-Observable-Driving-Source"><a href="#Controlling-Character-Motions-without-Observable-Driving-Source" class="headerlink" title="Controlling Character Motions without Observable Driving Source"></a>Controlling Character Motions without Observable Driving Source</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06025">http://arxiv.org/abs/2308.06025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyuan Li, Bin Dai, Ziyi Zhou, Qi Yao, Baoyuan Wang</li>
<li>for: 生成无驱动源的多样化、自然和无限长的头部&#x2F;身体序列</li>
<li>methods: 提议一个系统性框架，结合VQ-VAE和一种新的токен级控制策略，使用返回学习算法和经过设计的奖励函数来生成无限长的多样化和自然的头部&#x2F;身体序列</li>
<li>results: 通过全面的评估，发现提议的框架可以解决无驱动源生成中的各种挑战，并与其他强基线相比表现出众。<details>
<summary>Abstract</summary>
How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthesis with explicit driving sources. Through comprehensive evaluations, we conclude that our proposed framework can address all the above-mentioned challenges and outperform other strong baselines very significantly.
</details>
<details>
<summary>摘要</summary>
如何生成无驱动源的多样化、生命般自然的头部/身体序列？我们认为这是一个未受抨拿的研究问题，具有独特的技术挑战。不受 semantics 驱动源的限制，使用标准的自然语言模型来生成无限长序列会导致1) OOD 问题 Due to the accumulated error, 2) 不够多样性来生成自然和生命般的动作序列和 3) 不想要的时间 periodic patterns.为了解决以上挑战，我们提议一个系统性的框架，该框架结合 VQ-VAE 的优点和一种基于 reinforcement learning 的新的 токен级控制策略。高级 prior model 可以轻松地注入到该框架中，以生成无限长和多样化的序列。虽然我们现在没有驱动源，但我们的框架可以通过 Carefully designed reward functions 来扩展到控制的 synthesis 中Explicit driving sources。通过全面的评估，我们结议了我们提议的框架可以解决所有以上挑战，并与其他强大的基准模型相比，表现非常出色。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Picture-Description-Speech-for-Dementia-Detection-using-Image-text-Alignment"><a href="#Evaluating-Picture-Description-Speech-for-Dementia-Detection-using-Image-text-Alignment" class="headerlink" title="Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment"></a>Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07933">http://arxiv.org/abs/2308.07933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youxiang Zhu, Nana Lin, Xiaohui Liang, John A. Batsis, Robert M. Roth, Brian MacWhinney</li>
<li>for: 本研究旨在提高诊断老人痴呆症的精度，通过利用图像描述文本对应关系来提高检测精度。</li>
<li>methods: 本研究提出了首个将图像和描述文本作为输入，并利用大规模预训练图像文本对应模型的知识来进行诊断的模型。我们发现了健康和痴呆样本之间的文本与图像之间的差异，并使用文本与图像之间的相关性来排序和筛选样本。此外，我们还将图像分解成不同主题，并将文本分类为每个主题中的不同话题。</li>
<li>results: 我们的三种进阶模型，通过对样本进行预处理，使用图像与文本之间的相关性和图像分解、文本分类等技术，实现了诊断精度的提高。我们的最佳模型在83.44%的检测精度上得到了状元表现，高于文本只基线模型的79.91%。此外，我们还可视化样本和图像结果，以便解释我们的模型的优势。<details>
<summary>Abstract</summary>
Using picture description speech for dementia detection has been studied for 30 years. Despite the long history, previous models focus on identifying the differences in speech patterns between healthy subjects and patients with dementia but do not utilize the picture information directly. In this paper, we propose the first dementia detection models that take both the picture and the description texts as inputs and incorporate knowledge from large pre-trained image-text alignment models. We observe the difference between dementia and healthy samples in terms of the text's relevance to the picture and the focused area of the picture. We thus consider such a difference could be used to enhance dementia detection accuracy. Specifically, we use the text's relevance to the picture to rank and filter the sentences of the samples. We also identified focused areas of the picture as topics and categorized the sentences according to the focused areas. We propose three advanced models that pre-processed the samples based on their relevance to the picture, sub-image, and focused areas. The evaluation results show that our advanced models, with knowledge of the picture and large image-text alignment models, achieve state-of-the-art performance with the best detection accuracy at 83.44%, which is higher than the text-only baseline model at 79.91%. Lastly, we visualize the sample and picture results to explain the advantages of our models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Language-Models-for-Telecom-Forthcoming-Impact-on-the-Industry"><a href="#Large-Language-Models-for-Telecom-Forthcoming-Impact-on-the-Industry" class="headerlink" title="Large Language Models for Telecom: Forthcoming Impact on the Industry"></a>Large Language Models for Telecom: Forthcoming Impact on the Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06013">http://arxiv.org/abs/2308.06013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Maatouk, Nicola Piovesan, Fadhel Ayed, Antonio De Domenico, Merouane Debbah</li>
<li>for: 本研究旨在探讨LLM技术在电信领域的应用和影响，以及如何在这些领域中充分利用LLM的潜力。</li>
<li>methods: 本研究采用了LLM技术的内部结构和应用场景的分析，以及在电信领域中可以立即实施的用例的探讨。</li>
<li>results: 研究发现了LLM技术在电信领域的现有能力和局限性，以及需要进一步研究的领域和挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fullest extent within the telecom domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Does-AI-for-science-need-another-ImageNet-Or-totally-different-benchmarks-A-case-study-of-machine-learning-force-fields"><a href="#Does-AI-for-science-need-another-ImageNet-Or-totally-different-benchmarks-A-case-study-of-machine-learning-force-fields" class="headerlink" title="Does AI for science need another ImageNet Or totally different benchmarks? A case study of machine learning force fields"></a>Does AI for science need another ImageNet Or totally different benchmarks? A case study of machine learning force fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05999">http://arxiv.org/abs/2308.05999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yatao Li, Wanling Gao, Lei Wang, Lixin Sun, Zun Wang, Jianfeng Zhan</li>
<li>for: 这 paper 的目的是探讨 AI for science 领域中的模型性能评估方法，以便更好地适应科学计算任务中的特殊挑战。</li>
<li>methods: 该 paper 使用 machine learning force field (MLFF) 作为一个案例研究，检查了现有的 AI benchmarking 方法是否能够有效地评估 AI for science 模型的性能。它还提出了一些解决方案来评估 MLFF 模型，包括样本效率、时间域敏感性和交叉数据集泛化能力等方面。</li>
<li>results: 该 paper 通过设置问题实例类似于实际科学应用，提出了一些更加科学意义的性能指标，以评估 AI for science 模型的性能。这些指标在实际应用中表现出更高的泛化能力和更好的时间域敏感性，与传统的 AI 评估方法相比。<details>
<summary>Abstract</summary>
AI for science (AI4S) is an emerging research field that aims to enhance the accuracy and speed of scientific computing tasks using machine learning methods. Traditional AI benchmarking methods struggle to adapt to the unique challenges posed by AI4S because they assume data in training, testing, and future real-world queries are independent and identically distributed, while AI4S workloads anticipate out-of-distribution problem instances. This paper investigates the need for a novel approach to effectively benchmark AI for science, using the machine learning force field (MLFF) as a case study. MLFF is a method to accelerate molecular dynamics (MD) simulation with low computational cost and high accuracy. We identify various missed opportunities in scientifically meaningful benchmarking and propose solutions to evaluate MLFF models, specifically in the aspects of sample efficiency, time domain sensitivity, and cross-dataset generalization capabilities. By setting up the problem instantiation similar to the actual scientific applications, more meaningful performance metrics from the benchmark can be achieved. This suite of metrics has demonstrated a better ability to assess a model's performance in real-world scientific applications, in contrast to traditional AI benchmarking methodologies. This work is a component of the SAIBench project, an AI4S benchmarking suite. The project homepage is https://www.computercouncil.org/SAIBench.
</details>
<details>
<summary>摘要</summary>
人工智能 для科学（AI4S）是一个emerging研究领域，旨在使用机器学习方法提高科学计算任务的准确率和速度。传统的AI测试方法困难适应AI4S的特殊挑战，因为它们假设训练、测试和未来实际世界中的数据都是独立并且相同分布的，而AI4S工作负荷预期的问题实例将出现在不同的分布上。这篇论文研究了AI4S测试方法的需要，使用机器学习力场（MLFF）作为一个案例研究。MLFF是一种加速分子动力学（MD）仿真的方法，可以减少计算成本并保持高度准确。我们认为存在多种科学上有意义的测试机会被遗弃，并提出了一些解决方案来评估MLFF模型，包括样本效率、时间域敏感和cross-dataset泛化能力。通过设置问题实例类似于实际科学应用，可以更 meaningful的性能指标从测试中获得。这组指标已经表明可以更好地评估模型在实际科学应用中的性能，与传统的AI测试方法不同。这是SAIBench项目的一部分，SAIBench是一个AI4S测试集。项目主页在https://www.computercouncil.org/SAIBench。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Classification-of-Blood-Cell-Images-Using-Convolutional-Neural-Network"><a href="#Automatic-Classification-of-Blood-Cell-Images-Using-Convolutional-Neural-Network" class="headerlink" title="Automatic Classification of Blood Cell Images Using Convolutional Neural Network"></a>Automatic Classification of Blood Cell Images Using Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06300">http://arxiv.org/abs/2308.06300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Asghar, Sanjay Kumar, Paul Hynds, Abeera Mahfooz</li>
<li>For: The paper aims to automatically classify ten types of blood cells with increased accuracy using a convolutional neural network (CNN) model.* Methods: The authors use transfer learning with pre-trained CNN models, including VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20, on the PBC dataset’s normal DIB. They also propose a novel CNN-based framework to improve accuracy.* Results: The authors achieve an accuracy of 99.91% on the PBC dataset with their proposed CNN model, outperforming earlier results reported in the literature.<details>
<summary>Abstract</summary>
Human blood primarily comprises plasma, red blood cells, white blood cells, and platelets. It plays a vital role in transporting nutrients to different organs, where it stores essential health-related data about the human body. Blood cells are utilized to defend the body against diverse infections, including fungi, viruses, and bacteria. Hence, blood analysis can help physicians assess an individual's physiological condition. Blood cells have been sub-classified into eight groups: Neutrophils, eosinophils, basophils, lymphocytes, monocytes, immature granulocytes (promyelocytes, myelocytes, and metamyelocytes), erythroblasts, and platelets or thrombocytes on the basis of their nucleus, shape, and cytoplasm. Traditionally, pathologists and hematologists in laboratories have examined these blood cells using a microscope before manually classifying them. The manual approach is slower and more prone to human error. Therefore, it is essential to automate this process. In our paper, transfer learning with CNN pre-trained models. VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20 applied to the PBC dataset's normal DIB. The overall accuracy achieved with these models lies between 91.375 and 94.72%. Hence, inspired by these pre-trained architectures, a model has been proposed to automatically classify the ten types of blood cells with increased accuracy. A novel CNN-based framework has been presented to improve accuracy. The proposed CNN model has been tested on the PBC dataset normal DIB. The outcomes of the experiments demonstrate that our CNN-based framework designed for blood cell classification attains an accuracy of 99.91% on the PBC dataset. Our proposed convolutional neural network model performs competitively when compared to earlier results reported in the literature.
</details>
<details>
<summary>摘要</summary>
人体血液主要由血液溶解、红细胞、白细胞和板块组成。它扮演着将营养物质传递到不同器官的重要角色，同时也存储了人体重要的生物学信息。血液细胞可以用于防御体内各种感染，包括病毒、真菌和细菌。因此，血液分析可以帮助医生评估个体的生理状况。血液细胞被分为八种类型：neutrophils、eosinophils、basophils、lymphocytes、monocytes、immature granulocytes（promyelocytes、myelocytes和metamyelocytes）、erythroblasts和板块或血液板块。传统上，pathologists和hematologists在实验室中使用显微镜进行血液细胞的识别，这是一个慢速且容易出错的手动过程。因此，自动化这个过程是非常重要。在我们的论文中，我们采用了转移学习与CNN预训练模型。VGG16、VGG19、ResNet-50、ResNet-101、ResNet-152、InceptionV3、MobileNetV2和DenseNet-20在PBC数据集的正常DIB上应用了CNN预训练模型。这些模型的总准确率在91.375%到94.72%之间。因此，我们被这些预训练模型所 inspirited，并提出了一种自动化血液细胞类型分类的模型。我们提出了一种基于CNN的框架来提高准确率。我们的提议的CNN模型在PBC数据集的正常DIB上进行测试，实验结果表明，我们的CNN模型在血液细胞类型分类方面实现了99.91%的准确率。我们的提议的 convolutional neural network模型与文献中已经报道的结果相比，表现竞争力强。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Accurate-Transferability-Measurement-by-Evaluating-Intra-class-Feature-Variance"><a href="#Fast-and-Accurate-Transferability-Measurement-by-Evaluating-Intra-class-Feature-Variance" class="headerlink" title="Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance"></a>Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05986">http://arxiv.org/abs/2308.05986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snudatalab/TMI">https://github.com/snudatalab/TMI</a></li>
<li>paper_authors: Huiwen Xu, U Kang</li>
<li>for: 这个论文的目的是如何快速和准确地找到下游任务中最有用的预训练模型。</li>
<li>methods: 这个论文提出了一种名为TMI（转移性评估器）的算法，用于评估预训练模型的转移性。TMI视转移性为预训练模型在目标任务上的总体化，并通过评估模型内类差异来评估模型的适应性。</li>
<li>results: 对于多个实际数据集，TMI表现出了较好的选择性，可以快速和准确地选择预训练模型。与之前的研究相比，TMI在13个案例中展现出了更高的相关性。<details>
<summary>Abstract</summary>
Given a set of pre-trained models, how can we quickly and accurately find the most useful pre-trained model for a downstream task? Transferability measurement is to quantify how transferable is a pre-trained model learned on a source task to a target task. It is used for quickly ranking pre-trained models for a given task and thus becomes a crucial step for transfer learning. Existing methods measure transferability as the discrimination ability of a source model for a target data before transfer learning, which cannot accurately estimate the fine-tuning performance. Some of them restrict the application of transferability measurement in selecting the best supervised pre-trained models that have classifiers. It is important to have a general method for measuring transferability that can be applied in a variety of situations, such as selecting the best self-supervised pre-trained models that do not have classifiers, and selecting the best transferring layer for a target task. In this work, we propose TMI (TRANSFERABILITY MEASUREMENT WITH INTRA-CLASS FEATURE VARIANCE), a fast and accurate algorithm to measure transferability. We view transferability as the generalization of a pre-trained model on a target task by measuring intra-class feature variance. Intra-class variance evaluates the adaptability of the model to a new task, which measures how transferable the model is. Compared to previous studies that estimate how discriminative the models are, intra-class variance is more accurate than those as it does not require an optimal feature extractor and classifier. Extensive experiments on real-world datasets show that TMI outperforms competitors for selecting the top-5 best models, and exhibits consistently better correlation in 13 out of 17 cases.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:给定一个集合先进模型，如何快速和准确地找到下游任务中最有用的先进模型？转移可量度是用于衡量先进模型在源任务上学习后，转移到目标任务上的抽象能力。现有的方法通常是通过计算源模型对目标数据的分类能力来衡量转移可量度，这不能准确地估计微调性能。一些方法还限制了转移可量度的测量在选择最佳监督式先进模型中应用。因此，有一个通用的方法可以在多种情况下测量转移可量度，如选择最佳无监督式先进模型和选择最佳转移层。在这项工作中，我们提出了TMI（转移可量度测量与内类特征异常）算法，它是一种快速和准确的转移可量度测量方法。我们视转移可量度为将先进模型在目标任务上通过测量内类特征异常来衡量。内类异常评估模型在新任务上适应度，这也衡量了模型的转移可量度。与之前的研究所计算的模型掌握性相比，内类异常更准确，因为它不需要优化特征提取器和分类器。我们在实际世界数据集上进行了广泛的实验，显示TMI在选择top-5最佳模型时高效，并在13个 случа中展现了更高的相关性。
</details></li>
</ul>
<hr>
<h2 id="Defensive-Perception-Estimation-and-Monitoring-of-Neural-Network-Performance-under-Deployment"><a href="#Defensive-Perception-Estimation-and-Monitoring-of-Neural-Network-Performance-under-Deployment" class="headerlink" title="Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment"></a>Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06299">http://arxiv.org/abs/2308.06299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Vogt, Stefan Buehler, Mark Schutera</li>
<li>for: addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving</li>
<li>methods: encapsulating the neural network under deployment within an uncertainty estimation envelope based on Monte Carlo Dropout, without modifying the deployed neural network</li>
<li>results: demonstrating the applicability of the method for multiple different potential deployment shifts relevant to autonomous driving, including transitions into the night, rainy, or snowy domain, and enabling operational design domain recognition via uncertainty, which allows for defensive perception, safe state triggers, warning notifications, and feedback for testing or development and adaptation of the perception stack.Here’s the same information in Simplified Chinese:</li>
<li>for: 解决神经网络 semantic segmentation 自动驾驶中的不可预测性和域shift问题</li>
<li>methods: 基于 Monte Carlo Dropout 的 epistemic uncertainty 估计方法，不需要修改部署 neural network</li>
<li>results: 对各种自动驾驶中可能的部署变化进行了演示，包括夜晚、雨天和雪天等域shift，并实现了运行设计域认知via uncertainty，允许DEFENSIVE PERCEPTION、安全状态触发、警告通知和测试或开发和适应性改进的感知栈反馈。<details>
<summary>Abstract</summary>
In this paper, we propose a method for addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving. Our approach is based on the idea that deep learning-based perception for autonomous driving is uncertain and best represented as a probability distribution. As autonomous vehicles' safety is paramount, it is crucial for perception systems to recognize when the vehicle is leaving its operational design domain, anticipate hazardous uncertainty, and reduce the performance of the perception system. To address this, we propose to encapsulate the neural network under deployment within an uncertainty estimation envelope that is based on the epistemic uncertainty estimation through the Monte Carlo Dropout approach. This approach does not require modification of the deployed neural network and guarantees expected model performance. Our defensive perception envelope has the capability to estimate a neural network's performance, enabling monitoring and notification of entering domains of reduced neural network performance under deployment. Furthermore, our envelope is extended by novel methods to improve the application in deployment settings, including reducing compute expenses and confining estimation noise. Finally, we demonstrate the applicability of our method for multiple different potential deployment shifts relevant to autonomous driving, such as transitions into the night, rainy, or snowy domain. Overall, our approach shows great potential for application in deployment settings and enables operational design domain recognition via uncertainty, which allows for defensive perception, safe state triggers, warning notifications, and feedback for testing or development and adaptation of the perception stack.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法来解决自适应驾驶 neural network 中的不注意性投入和领域转换问题。我们的方法基于深度学习基于自适应驾驶的感知系统是不确定的，最好表示为一个概率分布。自驾驶车辆的安全性 Paramount，因此感知系统必须能够识别车辆离开操作设计领域，预测危险不确定性，并降低感知系统的性能。为此，我们提议将投入 neural network 内部的深度学习模型包装在一个不确定性估计膜中，该膜基于 Monte Carlo Dropout 方法来估计模型的 epistemic 不确定性。这种方法不需要修改已经部署的 neural network，并且保证模型的预期性能。我们的防御感知膜可以估计 neural network 的性能，并且可以监测和通知车辆进入性能下降的领域。此外，我们还提出了一些新的方法来改进在部署Setting中的应用，包括减少计算成本和限制估计噪声。最后，我们示出了我们方法在多种不同的部署转换中的应用可能性，例如在夜晚、雨天或雪天等领域。总之，我们的方法在部署Setting中表现出了很好的应用潜力，并允许操作设计领域的认知，以及发出警告通知、测试或开发和适应感知堆。
</details></li>
</ul>
<hr>
<h2 id="An-Encoder-Decoder-Approach-for-Packing-Circles"><a href="#An-Encoder-Decoder-Approach-for-Packing-Circles" class="headerlink" title="An Encoder-Decoder Approach for Packing Circles"></a>An Encoder-Decoder Approach for Packing Circles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07335">http://arxiv.org/abs/2308.07335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay Kiran Jose, Gangadhar Karevvanavar, Rajshekhar V Bhat</li>
<li>for: 本文关于如何封装小对象在大对象中，以实现不 overlap 和 minimum overlap 的目标。</li>
<li>methods: 本文提出了一种基于encoder-decoder架构的方法，包括encoder块、perturbation块和decoder块。encoder块通过normalization层输出中心点，perturbation块添加控制的偏移，确保中心点不超过小对象的半径，decoder块使用偏移中心点来估计 intend circle index。</li>
<li>results: 该方法可以 Parametrize encoder和decoder使用神经网络，并通过优化减少 decoder 估计的误差和实际输入 encoder 中心点的差异，从而实现不 overlap 和 minimum overlap 的目标。该方法可以对高维度和不同形状的对象进行扩展。<details>
<summary>Abstract</summary>
The problem of packing smaller objects within a larger object has been of interest since decades. In these problems, in addition to the requirement that the smaller objects must lie completely inside the larger objects, they are expected to not overlap or have minimum overlap with each other. Due to this, the problem of packing turns out to be a non-convex problem, obtaining whose optimal solution is challenging. As such, several heuristic approaches have been used for obtaining sub-optimal solutions in general, and provably optimal solutions for some special instances. In this paper, we propose a novel encoder-decoder architecture consisting of an encoder block, a perturbation block and a decoder block, for packing identical circles within a larger circle. In our approach, the encoder takes the index of a circle to be packed as an input and outputs its center through a normalization layer, the perturbation layer adds controlled perturbations to the center, ensuring that it does not deviate beyond the radius of the smaller circle to be packed, and the decoder takes the perturbed center as input and estimates the index of the intended circle for packing. We parameterize the encoder and decoder by a neural network and optimize it to reduce an error between the decoder's estimated index and the actual index of the circle provided as input to the encoder. The proposed approach can be generalized to pack objects of higher dimensions and different shapes by carefully choosing normalization and perturbation layers. The approach gives a sub-optimal solution and is able to pack smaller objects within a larger object with competitive performance with respect to classical methods.
</details>
<details>
<summary>摘要</summary>
这个问题已经引起关注了几十年。在这些问题中，除了要求小对象完全 locate 在大对象中之外，还要求小对象之间不会 overlap 或者最小化 overlap。由于这个原因，packing 问题变成了非凸问题，获得优化解决方案是困难的。为此，许多启发性方法被用来获得不优化解决方案，以及对特殊情况下的可证优化解决方案。在这篇论文中，我们提出了一种新的编码器-解码器架构，包括编码器块、抖动块和解码器块，用于将同形圆包含在大圆中。在我们的方法中，编码器接受圆的索引作为输入，并通过正规化层输出圆心，抖动层添加控制的偏移，使圆心不会超过小圆的半径，而解码器接受偏移后的圆心作为输入，并估算圆的索引。我们使用神经网络参数化编码器和解码器，并优化它们以降低由解码器估算的圆索引与实际输入圆索引之间的错误。我们的方法可以通过选择正规化和抖动层来扩展到包含高维度和不同形状的对象。该方法可以提供竞争性的非优化解决方案，并将小对象包含在大对象中。
</details></li>
</ul>
<hr>
<h2 id="Learning-nonparametric-DAGs-with-incremental-information-via-high-order-HSIC"><a href="#Learning-nonparametric-DAGs-with-incremental-information-via-high-order-HSIC" class="headerlink" title="Learning nonparametric DAGs with incremental information via high-order HSIC"></a>Learning nonparametric DAGs with incremental information via high-order HSIC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05969">http://arxiv.org/abs/2308.05969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yafei Wang, Jianguo Liu</li>
<li>for: 本文 targets at learning Bayesian networks (BN) by maximizing global score functions, but it addresses the issue of local variables having direct and indirect dependence simultaneously, which can lead to missed edges in the global optimization.</li>
<li>methods: 本文提出了一个可 identificability condition based on a determined subset of parents，并开发了一个两阶段算法（OT algorithm）来解决本问题。在第一阶段，使用 first-order Hilbert-Schmidt independence criterion (HSIC) 得到一个初始确定的父集。在第二阶段，使用 theoretically proved incremental properties of high-order HSIC 进行了本地调整。</li>
<li>results:  numrical experiments on different synthetic datasets and real-world datasets show that the OT algorithm outperforms existing methods, especially in Sigmoid Mix model with the size of the graph being d&#x3D;40, the structure intervention distance (SID) of the OT algorithm is 329.7 smaller than the one obtained by CAM, indicating that the graph estimated by the OT algorithm misses fewer edges compared with CAM.<details>
<summary>Abstract</summary>
Score-based methods for learning Bayesain networks(BN) aim to maximizing the global score functions. However, if local variables have direct and indirect dependence simultaneously, the global optimization on score functions misses edges between variables with indirect dependent relationship, of which scores are smaller than those with direct dependent relationship. In this paper, we present an identifiability condition based on a determined subset of parents to identify the underlying DAG. By the identifiability condition, we develop a two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the global optimization. In the optimal phase, an optimization problem based on first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated skeleton as the initial determined parents subset. In the tuning phase, the skeleton is locally tuned by deletion, addition and DAG-formalization strategies using the theoretically proved incremental properties of high-order HSIC. Numerical experiments for different synthetic datasets and real-world datasets show that the OT algorithm outperforms existing methods. Especially in Sigmoid Mix model with the size of the graph being ${\rm\bf d=40}$, the structure intervention distance (SID) of the OT algorithm is 329.7 smaller than the one obtained by CAM, which indicates that the graph estimated by the OT algorithm misses fewer edges compared with CAM.
</details>
<details>
<summary>摘要</summary>
Score-based方法学习 bayesian网络（BN）目的是最大化全局分数函数。然而，如果本地变量同时具有直接和间接依赖关系，全局优化分数函数会忽略变量之间的间接依赖关系中的边，其分数较直接依赖关系中的边小。在这篇论文中，我们提出了一个可识别条件，基于确定的父集来识别下面的DAG。通过可识别条件，我们开发了一个两阶段算法，称为最优调整（OT）算法。在优化阶段，基于第一阶段希尔伯特- Schmidt独立性标准（HSIC）的优化问题提供了一个初始确定父集的skeleton。在调整阶段，skeleton通过删除、添加和DAG-形式化策略进行了本地调整，使用了理论上证明的高阶HSIC的增量性质。numerical experiments表明，OT算法在不同的synthetic数据集和实际数据集上的性能都高于现有方法。特别是在sigmoid mix模型中，OT算法的结构间断距（SID）为329.7，与CAM所获得的结构间断距相比，表示OT算法估算的图 missed fewer edges。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-White-Blood-Cells-Using-Machine-and-Deep-Learning-Models-A-Systematic-Review"><a href="#Classification-of-White-Blood-Cells-Using-Machine-and-Deep-Learning-Models-A-Systematic-Review" class="headerlink" title="Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review"></a>Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06296">http://arxiv.org/abs/2308.06296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Asghar, Sanjay Kumar, Paul Hynds, Arslan Shaukat</li>
<li>for: 这篇论文的目的是对医疗影像分析中的白血球分类进行深入分析，并评估现代技术在这个领域的应用。</li>
<li>methods: 这篇论文使用了许多现代技术，包括机器学习（ML）和深度学习（DL），以提高医疗影像分析的准确性和分类精度。</li>
<li>results: 这篇论文发现，过去的17年间，医疗影像分析中的白血球分类方法有所进步，并且使用了许多不同的技术和数据来进行分析。但是，还有一些挑战需要解决，例如获得适当的数据集和增强医疗培训。<details>
<summary>Abstract</summary>
Machine learning (ML) and deep learning (DL) models have been employed to significantly improve analyses of medical imagery, with these approaches used to enhance the accuracy of prediction and classification. Model predictions and classifications assist diagnoses of various cancers and tumors. This review presents an in-depth analysis of modern techniques applied within the domain of medical image analysis for white blood cell classification. The methodologies that use blood smear images, magnetic resonance imaging (MRI), X-rays, and similar medical imaging domains are identified and discussed, with a detailed analysis of ML/DL techniques applied to the classification of white blood cells (WBCs) representing the primary focus of the review. The data utilized in this research has been extracted from a collection of 136 primary papers that were published between the years 2006 and 2023. The most widely used techniques and best-performing white blood cell classification methods are identified. While the use of ML and DL for white blood cell classification has concurrently increased and improved in recent year, significant challenges remain - 1) Availability of appropriate datasets remain the primary challenge, and may be resolved using data augmentation techniques. 2) Medical training of researchers is recommended to improve current understanding of white blood cell structure and subsequent selection of appropriate classification models. 3) Advanced DL networks including Generative Adversarial Networks, R-CNN, Fast R-CNN, and faster R-CNN will likely be increasingly employed to supplement or replace current techniques.
</details>
<details>
<summary>摘要</summary>
医学影像分析（ML）和深度学习（DL）模型已经被应用到医疗影像分析中，以提高预测和分类的准确性。这些方法可以帮助诊断多种恶性肿瘤和癌症。本文总结了现代医学影像分析领域中使用ML/DL技术进行白血球类型分类的方法。这些方法包括血液滴血图像、核磁共振成像（MRI）、X射线成像等医学影像领域，并进行了详细的ML/DL技术应用于白血球类型分类的分析。研究使用的数据来自于2006年至2023年发表的136篇原始论文。最常用的技术和最佳白血球类型分类方法被识别出来。虽然在过去几年内，用ML和DL进行白血球类型分类的使用和提高在不断增长，但还存在一些挑战，包括：1）获得适当数据集的可用性问题，可以通过数据扩展技术解决。2）医学研究人员的培训，以提高白血球结构的理解，并选择合适的分类模型。3）将来，高级的深度学习网络，如生成对抗网络、R-CNN、快速R-CNN和更快的R-CNN将被广泛应用，以补充或取代当前的方法。
</details></li>
</ul>
<hr>
<h2 id="Learned-Point-Cloud-Compression-for-Classification"><a href="#Learned-Point-Cloud-Compression-for-Classification" class="headerlink" title="Learned Point Cloud Compression for Classification"></a>Learned Point Cloud Compression for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05959">http://arxiv.org/abs/2308.05959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification">https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification</a></li>
<li>paper_authors: Mateen Ulhaq, Ivan V. Bajić</li>
<li>for: 本研究旨在提出一种特种的点云编码器，用于在服务器端进行机器分析 tasks 的点云数据传输。</li>
<li>methods: 我们基于 PointNet 提出了一种特种的点云编码器，实现了与非特种编码器相比significantly better的Rate-Accuracy 质量比。</li>
<li>results: 我们的编码器在 ModelNet40 数据集上实现了94%的BD-比特率减少，而且对于低资源的终端设备，我们还提出了两种轻量级的编码器配置，可以实现相似的BD-比特率减少（93%和92%），同时只消耗0.470和0.048 encoder-side kMACs&#x2F;点。<details>
<summary>Abstract</summary>
Deep learning is increasingly being used to perform machine vision tasks such as classification, object detection, and segmentation on 3D point cloud data. However, deep learning inference is computationally expensive. The limited computational capabilities of end devices thus necessitate a codec for transmitting point cloud data over the network for server-side processing. Such a codec must be lightweight and capable of achieving high compression ratios without sacrificing accuracy. Motivated by this, we present a novel point cloud codec that is highly specialized for the machine task of classification. Our codec, based on PointNet, achieves a significantly better rate-accuracy trade-off in comparison to alternative methods. In particular, it achieves a 94% reduction in BD-bitrate over non-specialized codecs on the ModelNet40 dataset. For low-resource end devices, we also propose two lightweight configurations of our encoder that achieve similar BD-bitrate reductions of 93% and 92% with 3% and 5% drops in top-1 accuracy, while consuming only 0.470 and 0.048 encoder-side kMACs/point, respectively. Our codec demonstrates the potential of specialized codecs for machine analysis of point clouds, and provides a basis for extension to more complex tasks and datasets in the future.
</details>
<details>
<summary>摘要</summary>
深度学习在处理3D点云数据上进行机器视觉任务，如分类、物体检测和分割，日益受到欢迎。然而，深度学习推理过程具有计算成本高的问题，因此在终端设备上进行处理时需要一个点云编码器。这个编码器应该轻量级，能够实现高度压缩比，而无需牺牲准确性。为了解决这个问题，我们提出了一种特种的点云编码器，基于PointNet，可以在机器分类任务中实现显著更好的比例-准确性质量。具体来说，我们的编码器在ModelNet40数据集上实现了94%的BD-比特率减少，相比非特种编码器。而为了适应低资源的终端设备，我们还提出了两种轻量级的编码器配置，它们可以实现类似的BD-比特率减少，分别为93%和92%，但是消耗了0.470和0.048个encoder-side kMACs/点。我们的编码器表明特种编码器在机器分析点云数据时具有潜在的优势，并为未来扩展到更复杂的任务和数据集提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Node-Embedding-for-Homophilous-Graphs-with-ARGEW-Augmentation-of-Random-walks-by-Graph-Edge-Weights"><a href="#Node-Embedding-for-Homophilous-Graphs-with-ARGEW-Augmentation-of-Random-walks-by-Graph-Edge-Weights" class="headerlink" title="Node Embedding for Homophilous Graphs with ARGEW: Augmentation of Random walks by Graph Edge Weights"></a>Node Embedding for Homophilous Graphs with ARGEW: Augmentation of Random walks by Graph Edge Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05957">http://arxiv.org/abs/2308.05957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ncsoft/argew">https://github.com/ncsoft/argew</a></li>
<li>paper_authors: Jun Hee Kim, Jaeman Son, Hyunsoo Kim, Eunjo Lee</li>
<li>for: 本文是针对 dense vector  represent nodes in network 的研究，尤其是Weighted homophilous graphs中 node pairs with stronger edges weights 应该有更加相近的 embedding。</li>
<li>methods: 本文提出了 ARGEW（Augmentation of Random walks by Graph Edge Weights），一种基于随机漫步的增强方法，可以使得 node embeddings 更加准确地反映 edge weights。</li>
<li>results: 在多个实际网络上，ARGEW 可以使得 node pairs with larger edge weights 有更加相近的 embedding，并且在 node classification 任务中，ARGEW 可以提高 node2vec 的性能，并且不受 hyperparameters 的影响。<details>
<summary>Abstract</summary>
Representing nodes in a network as dense vectors node embeddings is important for understanding a given network and solving many downstream tasks. In particular, for weighted homophilous graphs where similar nodes are connected with larger edge weights, we desire node embeddings where node pairs with strong weights have closer embeddings. Although random walk based node embedding methods like node2vec and node2vec+ do work for weighted networks via including edge weights in the walk transition probabilities, our experiments show that the embedding result does not adequately reflect edge weights. In this paper, we propose ARGEW (Augmentation of Random walks by Graph Edge Weights), a novel augmentation method for random walks that expands the corpus in such a way that nodes with larger edge weights end up with closer embeddings. ARGEW can work with any random walk based node embedding method, because it is independent of the random sampling strategy itself and works on top of the already-performed walks. With several real-world networks, we demonstrate that with ARGEW, compared to not using it, the desired pattern that node pairs with larger edge weights have closer embeddings is much clearer. We also examine ARGEW's performance in node classification: node2vec with ARGEW outperforms pure node2vec and is not sensitive to hyperparameters (i.e. consistently good). In fact, it achieves similarly good results as supervised GCN, even without any node feature or label information during training. Finally, we explain why ARGEW works consistently well by exploring the coappearance distributions using a synthetic graph with clear structural roles.
</details>
<details>
<summary>摘要</summary>
importance of representing nodes in a network as dense vectors (node embeddings) for understanding the network and solving downstream tasks. In particular, for weighted homophilous graphs where similar nodes are connected with larger edge weights, we desire node embeddings where node pairs with strong weights have closer embeddings. Although random walk based node embedding methods like node2vec and node2vec+ can work for weighted networks by including edge weights in the walk transition probabilities, our experiments show that the embedding result does not adequately reflect edge weights. In this paper, we propose ARGEW (Augmentation of Random walks by Graph Edge Weights), a novel augmentation method for random walks that expands the corpus in such a way that nodes with larger edge weights end up with closer embeddings. ARGEW can work with any random walk based node embedding method, because it is independent of the random sampling strategy itself and works on top of the already-performed walks. With several real-world networks, we demonstrate that with ARGEW, compared to not using it, the desired pattern that node pairs with larger edge weights have closer embeddings is much clearer. We also examine ARGEW's performance in node classification: node2vec with ARGEW outperforms pure node2vec and is not sensitive to hyperparameters (i.e. consistently good). In fact, it achieves similarly good results as supervised GCN, even without any node feature or label information during training. Finally, we explain why ARGEW works consistently well by exploring the coappearance distributions using a synthetic graph with clear structural roles.
</details></li>
</ul>
<hr>
<h2 id="INR-Arch-A-Dataflow-Architecture-and-Compiler-for-Arbitrary-Order-Gradient-Computations-in-Implicit-Neural-Representation-Processing"><a href="#INR-Arch-A-Dataflow-Architecture-and-Compiler-for-Arbitrary-Order-Gradient-Computations-in-Implicit-Neural-Representation-Processing" class="headerlink" title="INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing"></a>INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05930">http://arxiv.org/abs/2308.05930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Abi-Karam, Rishov Sarkar, Dejia Xu, Zhiwen Fan, Zhangyang Wang, Cong Hao</li>
<li>for: 这个论文主要用于探讨nth-order gradient计算在图形学、元学习（MAML）、科学计算和最近的隐藏神经表示（INR）中的应用。</li>
<li>methods: 这个论文使用了一种叫做INR-Arch的框架，它可以将计算图的nth-order gradient转换成一个硬件优化的数据流体系结构。这个框架包括两个阶段：首先，设计了一个高效的数据流体系结构，其中使用了FIFO流和优化的计算kernels库，以确保高效的内存利用和并行计算。其次，提出了一种编译器，它可以自动从计算图中提取和优化计算，并配置硬件参数 such as 延迟和流深度以优化吞吐量，保证不会出现死锁现象，并生成高级合成（HLS）代码 дляFPGA实现。</li>
<li>results: 这个论文通过对INR编辑作为测试样本，实现了对CPU和GPU基线的1.8-4.8倍和1.5-3.6倍的速度提升，同时也实现了对内存使用的3.1-8.9倍和1.7-4.3倍的减少，以及对能效率的1.7-11.3倍和5.5-32.8倍的下降。<details>
<summary>Abstract</summary>
An increasing number of researchers are finding use for nth-order gradient computations for a wide variety of applications, including graphics, meta-learning (MAML), scientific computing, and most recently, implicit neural representations (INRs). Recent work shows that the gradient of an INR can be used to edit the data it represents directly without needing to convert it back to a discrete representation. However, given a function represented as a computation graph, traditional architectures face challenges in efficiently computing its nth-order gradient due to the higher demand for computing power and higher complexity in data movement. This makes it a promising target for FPGA acceleration. In this work, we introduce INR-Arch, a framework that transforms the computation graph of an nth-order gradient into a hardware-optimized dataflow architecture. We address this problem in two phases. First, we design a dataflow architecture that uses FIFO streams and an optimized computation kernel library, ensuring high memory efficiency and parallel computation. Second, we propose a compiler that extracts and optimizes computation graphs, automatically configures hardware parameters such as latency and stream depths to optimize throughput, while ensuring deadlock-free operation, and outputs High-Level Synthesis (HLS) code for FPGA implementation. We utilize INR editing as our benchmark, presenting results that demonstrate 1.8-4.8x and 1.5-3.6x speedup compared to CPU and GPU baselines respectively. Furthermore, we obtain 3.1-8.9x and 1.7-4.3x lower memory usage, and 1.7-11.3x and 5.5-32.8x lower energy-delay product. Our framework will be made open-source and available on GitHub.
</details>
<details>
<summary>摘要</summary>
更多研究人员正在发现使用 nth-order Gradient 计算在各种应用程序中，包括图形学、多学习（MAML）、科学计算和最近的隐藏神经表示（INRs）。最新的研究表明，INR 的 Gradient 可以直接编辑所表示的数据，而无需将其转换回分割表示。然而，传统架构在计算 Graph 中的 nth-order Gradient 计算时面临着更高的计算能力和数据移动复杂性的挑战，这使得它成为了可 acceleration 的目标。在这项工作中，我们介绍 INR-Arch，一个将计算 Graph 转换为硬件优化数据流架构的框架。我们解决这个问题在两个阶段。首先，我们设计了一个数据流架构，使用 FIFO 流和优化的计算内核库，保证高内存效率和并行计算。其次，我们提出了一个编译器，可以提取和优化计算 Graph，自动配置硬件参数 such as 延迟和流深度，以优化通过put，并确保不会出现堵塞的操作。我们使用 INR 编辑作为我们的标准，发表了结果，表明在 CPU 和 GPU 基线相比，得到了 1.8-4.8 倍和 1.5-3.6 倍的速度提升。此外，我们获得了 3.1-8.9 倍和 1.7-4.3 倍的内存使用量减少，以及 1.7-11.3 倍和 5.5-32.8 倍的能量延迟产品。我们的框架即将被开源，并在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="On-the-equivalence-of-Occam-algorithms"><a href="#On-the-equivalence-of-Occam-algorithms" class="headerlink" title="On the equivalence of Occam algorithms"></a>On the equivalence of Occam algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05906">http://arxiv.org/abs/2308.05906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaman Keinath-Esmail</li>
<li>for: 本文提供了一个后验正则化的基础，即任何可学习的概念类都可以由Occam算法学习。</li>
<li>methods: 本文使用了Board和Pitt（1990）提出的一种受到例外列表的影响的Occam算法，并证明了这种算法可以学习任何可学习的概念类。</li>
<li>results: 本文证明了Board和Pitt（1990）的一种受到例外列表的影响的Occam算法可以学习任何可学习的概念类，并且这种算法的复杂度是$\delta$-无关的。<details>
<summary>Abstract</summary>
Blumer et al. (1987, 1989) showed that any concept class that is learnable by Occam algorithms is PAC learnable. Board and Pitt (1990) showed a partial converse of this theorem: for concept classes that are closed under exception lists, any class that is PAC learnable is learnable by an Occam algorithm. However, their Occam algorithm outputs a hypothesis whose complexity is $\delta$-dependent, which is an important limitation. In this paper, we show that their partial converse applies to Occam algorithms with $\delta$-independent complexities as well. Thus, we provide a posteriori justification of various theoretical results and algorithm design methods which use the partial converse as a basis for their work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparing-the-quality-of-neural-network-uncertainty-estimates-for-classification-problems"><a href="#Comparing-the-quality-of-neural-network-uncertainty-estimates-for-classification-problems" class="headerlink" title="Comparing the quality of neural network uncertainty estimates for classification problems"></a>Comparing the quality of neural network uncertainty estimates for classification problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05903">http://arxiv.org/abs/2308.05903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Ries, Joshua Michalenko, Tyler Ganter, Rashad Imad-Fayez Baiyasi, Jason Adams<br>for:这个论文的目的是评估深度学习模型中的不确定性评估方法的质量。methods:这个论文使用了统计方法来评估信任区间的覆盖率和信任间隔，以及预测分类预测信任的均值误差来评估深度学习模型的不确定性评估方法。results:研究发现，不同的不确定性评估方法在同一个数据集上可以生成不同的结果，而且这些结果之间存在差异。此外，研究还发现，使用MCMC和VI方法可以获得更好的不确定性评估结果，而使用DE和MC dropout方法的结果则更为不稳定。<details>
<summary>Abstract</summary>
Traditional deep learning (DL) models are powerful classifiers, but many approaches do not provide uncertainties for their estimates. Uncertainty quantification (UQ) methods for DL models have received increased attention in the literature due to their usefulness in decision making, particularly for high-consequence decisions. However, there has been little research done on how to evaluate the quality of such methods. We use statistical methods of frequentist interval coverage and interval width to evaluate the quality of credible intervals, and expected calibration error to evaluate classification predicted confidence. These metrics are evaluated on Bayesian neural networks (BNN) fit using Markov Chain Monte Carlo (MCMC) and variational inference (VI), bootstrapped neural networks (NN), Deep Ensembles (DE), and Monte Carlo (MC) dropout. We apply these different UQ for DL methods to a hyperspectral image target detection problem and show the inconsistency of the different methods' results and the necessity of a UQ quality metric. To reconcile these differences and choose a UQ method that appropriately quantifies the uncertainty, we create a simulated data set with fully parameterized probability distribution for a two-class classification problem. The gold standard MCMC performs the best overall, and the bootstrapped NN is a close second, requiring the same computational expense as DE. Through this comparison, we demonstrate that, for a given data set, different models can produce uncertainty estimates of markedly different quality. This in turn points to a great need for principled assessment methods of UQ quality in DL applications.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）模型是强大的分类器，但许多方法不提供不确定性的估计。不确定性量化（UQ）方法 для DL 模型在文献中收到了更多的关注，因为它们在决策中非常有用，特别是对高 conseqüência 的决策。然而，对 UQ 方法评价的研究相对较少。我们使用统计方法的频率interval coverage和interval width来评价credible interval的质量，以及预期准确性error来评价分类预测的自信度。这些指标在bayesian neural network（BNN）适用markov chain Monte Carlo（MCMC）和variational inference（VI）、bootstrapped neural network（NN）、deep ensembles（DE）和Monte Carlo（MC）dropout中被评价。我们对这些不同的 UQ 方法应用到一个 hyperspectral image target detection问题，并显示了不同方法的结果之间的不一致，以及需要一个 UQ 质量指标。为了解决这些不一致并选择一个正确地量化不确定性的 UQ 方法，我们创建了一个完全参数化的概率分布的 simulated data set，用于两类分类问题。金标准 MCMC 表现最佳，而 bootstrapped NN 紧随其后，需要与 DE 相同的计算成本。通过这种比较，我们证明了，对于给定的数据集，不同的模型可以生成不同质量的不确定性估计。这一点点到了深度学习应用中需要原则性评价 UQ 质量的强需求。
</details></li>
</ul>
<hr>
<h2 id="Target-Detection-on-Hyperspectral-Images-Using-MCMC-and-VI-Trained-Bayesian-Neural-Networks"><a href="#Target-Detection-on-Hyperspectral-Images-Using-MCMC-and-VI-Trained-Bayesian-Neural-Networks" class="headerlink" title="Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks"></a>Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06293">http://arxiv.org/abs/2308.06293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Ries, Jason Adams, Joshua Zollweg</li>
<li>for: 这篇论文的目的是提出了一种 bayesian neural network (BNN) 的应用，以便在图像分类 tasks 中提供 uncertainty quantification (UQ)。</li>
<li>methods: 这篇论文使用了 MCMC 和 VI 两种不同的训练方法，以评估 BNN 的训练效果。</li>
<li>results: 研究发现，MCMC 和 VI 两种训练方法都可以达到良好的检测效果，但是 VI 方法的计算速度较快。此外，研究还发现，不同的训练方法可能会导致不同的模型结果，特别是在高风险应用中。<details>
<summary>Abstract</summary>
Neural networks (NN) have become almost ubiquitous with image classification, but in their standard form produce point estimates, with no measure of confidence. Bayesian neural networks (BNN) provide uncertainty quantification (UQ) for NN predictions and estimates through the posterior distribution. As NN are applied in more high-consequence applications, UQ is becoming a requirement. BNN provide a solution to this problem by not only giving accurate predictions and estimates, but also an interval that includes reasonable values within a desired probability. Despite their positive attributes, BNN are notoriously difficult and time consuming to train. Traditional Bayesian methods use Markov Chain Monte Carlo (MCMC), but this is often brushed aside as being too slow. The most common method is variational inference (VI) due to its fast computation, but there are multiple concerns with its efficacy. We apply and compare MCMC- and VI-trained BNN in the context of target detection in hyperspectral imagery (HSI), where materials of interest can be identified by their unique spectral signature. This is a challenging field, due to the numerous permuting effects practical collection of HSI has on measured spectra. Both models are trained using out-of-the-box tools on a high fidelity HSI target detection scene. Both MCMC- and VI-trained BNN perform well overall at target detection on a simulated HSI scene. This paper provides an example of how to utilize the benefits of UQ, but also to increase awareness that different training methods can give different results for the same model. If sufficient computational resources are available, the best approach rather than the fastest or most efficient should be used, especially for high consequence problems.
</details>
<details>
<summary>摘要</summary>
neural networks (NN) 已经在图像分类中变得极其普遍，但在标准形式下产生点估计，无法提供信度量。 bayesian neural networks (BNN) 提供图像分类预测和估计的不确定性评估（UQ），通过 posterior distribution。 随着 NN 在高重要性应用中使用，UQ 变得必须。 BNN 不仅提供准确的预测和估计，还提供一个包含合理值的时间间隔，在所需的概率范围内。 despite their positive attributes, BNN 很难和时间consuming 进行训练。 traditional Bayesian methods 使用 markov chain Monte Carlo (MCMC)，但这经常被认为是太慢。 the most common method 是 variational inference (VI) due to its fast computation, but there are multiple concerns with its efficacy。 我们在 target detection 中应用和比较 MCMC- 和 VI-trained BNN 在 hyperspectral imagery (HSI) 中，where materials of interest can be identified by their unique spectral signature。 this is a challenging field, due to the numerous permuting effects practical collection of HSI has on measured spectra。 both models are trained using out-of-the-box tools on a high fidelity HSI target detection scene。 both MCMC- 和 VI-trained BNN perform well overall at target detection on a simulated HSI scene。 this paper provides an example of how to utilize the benefits of UQ, but also to increase awareness that different training methods can give different results for the same model。 if sufficient computational resources are available, the best approach rather than the fastest or most efficient should be used, especially for high consequence problems。
</details></li>
</ul>
<hr>
<h2 id="The-divergence-time-of-protein-structures-modelled-by-Markov-matrices-and-its-relation-to-the-divergence-of-sequences"><a href="#The-divergence-time-of-protein-structures-modelled-by-Markov-matrices-and-its-relation-to-the-divergence-of-sequences" class="headerlink" title="The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences"></a>The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06292">http://arxiv.org/abs/2308.06292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandun Rajapaksa, Lloyd Allison, Peter J. Stuckey, Maria Garcia de la Banda, Arun S. Konagurthu</li>
<li>for: 这种研究是为了开发一种基于蛋白质结构的时间参数化统计模型，以量化蛋白质结构在演化过程中的差异进化。</li>
<li>methods: 该研究使用了一个大量的蛋白质三维结构对比来推断一种时间参数化的统计模型，并使用了 bayesian 和信息理论的框架来推断时间参数化的随机矩阵和 Dirichlet 模型。</li>
<li>results: 研究发现，使用这种时间参数化统计模型可以更准确地估计蛋白质结构之间的分化时间，并且可以与序列相关性的时间参数化模型进行比较。此外，该模型还可以在次结构预测中与常见的神经网络架构竞争。<details>
<summary>Abstract</summary>
A complete time-parameterized statistical model quantifying the divergent evolution of protein structures in terms of the patterns of conservation of their secondary structures is inferred from a large collection of protein 3D structure alignments. This provides a better alternative to time-parameterized sequence-based models of protein relatedness, that have clear limitations dealing with twilight and midnight zones of sequence relationships. Since protein structures are far more conserved due to the selection pressure directly placed on their function, divergence time estimates can be more accurate when inferred from structures. We use the Bayesian and information-theoretic framework of Minimum Message Length to infer a time-parameterized stochastic matrix (accounting for perturbed structural states of related residues) and associated Dirichlet models (accounting for insertions and deletions during the evolution of protein domains). These are used in concert to estimate the Markov time of divergence of tertiary structures, a task previously only possible using proxies (like RMSD). By analyzing one million pairs of homologous structures, we yield a relationship between the Markov divergence time of structures and of sequences. Using these inferred models and the relationship between the divergence of sequences and structures, we demonstrate a competitive performance in secondary structure prediction against neural network architectures commonly employed for this task. The source code and supplementary information are downloadable from \url{http://lcb.infotech.monash.edu.au/sstsum}.
</details>
<details>
<summary>摘要</summary>
一个完整的时间参数化统计模型，用于描述蛋白质结构的不同演化的 Patterns of conservation of secondary structures，从一个大量的蛋白质三维结构对Alignment中得到了推断。这提供了一个更好的代替时间参数化序列基于模型，该模型在处理晚上和午夜时区的序列关系时存在显著的限制。由于蛋白质结构受直接选择压力的影响，因此从结构来进行演化时间估计的准确性比序列基于模型更高。我们使用 bayesian 和信息理论的框架，Minimum Message Length 来推断时间参数化随机矩阵（考虑相关的结构态态）和 Dirichlet 模型（考虑插入和删除 durante protein domains 的演化）。这些模型在一起使用，以估计蛋白质结构的马克夫时间异同。在分析一百万对同源结构的情况下，我们发现了结构异同时间和序列异同时间之间的关系。使用这些推断出的模型和序列异同时间之间的关系，我们展示了在二级结构预测中与通用的神经网络架构相比，我们的表现是竞争性的。源代码和补充信息可以从 \url{http://lcb.infotech.monash.edu.au/sstsum} 下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Team-Based-Navigation-A-Review-of-Deep-Reinforcement-Learning-Techniques-for-Multi-Agent-Pathfinding"><a href="#Learning-to-Team-Based-Navigation-A-Review-of-Deep-Reinforcement-Learning-Techniques-for-Multi-Agent-Pathfinding" class="headerlink" title="Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding"></a>Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05893">http://arxiv.org/abs/2308.05893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehoon Chung, Jamil Fayyad, Younes Al Younes, Homayoun Najjaran</li>
<li>for: 本研究旨在探讨 Deep Reinforcement Learning（DRL）在多体系统中的应用，尤其是在多体路径找索中。</li>
<li>methods: 本文使用了多种 Deep Reinforcement Learning（DRL）方法，包括价值函数法和策略法，以解决多体系统中的路径找索问题。</li>
<li>results: 本文提供了一个统一的评价指标集，以便比较不同的多体路径找索算法的性能。此外，本文还发现了模型基于的DRL在多体系统中的潜在应用潜力，并提供了相关的基础知识。<details>
<summary>Abstract</summary>
Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our objective is to assist readers in gaining insight into the current research direction, providing unified metrics for comparing different MAPF algorithms and expanding their knowledge of model-based DRL to address the existing challenges in MAPF.
</details>
<details>
<summary>摘要</summary>
多智能路径找索（MAPF）是许多大规模 роботи库应用中的关键领域，经常作为多智能系统的基础步骤。然而，随着环境的增加复杂性，MAPF的现有解决方案的效果逐渐减退。与其他研究不同，我们的工作在这篇评论文中不仅提供了近期MAPF的进展概述，还广泛评论了深度强化学习（DRL）在多智能系统设置中的应用。此外，我们的工作还强调了评价MAPF解决方案的缺乏统一评价指标，并提供了全面的解释。最后，我们的文章还讨论了基于模型的DRL作为未来方向的潜在发展，并提供了相应的基础理解，以解决当前MAPF中的挑战。我们的目标是帮助读者更深入了解当前的研究方向，提供统一的评价指标，以及扩展他们对基于模型的DRL的知识，以Addressing the existing challenges in MAPF。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DF2-Distribution-Free-Decision-Focused-Learning"><a href="#DF2-Distribution-Free-Decision-Focused-Learning" class="headerlink" title="DF2: Distribution-Free Decision-Focused Learning"></a>DF2: Distribution-Free Decision-Focused Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05889">http://arxiv.org/abs/2308.05889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingkai Kong, Wenhao Mu, Jiaming Cui, Yuchen Zhuang, B. Aditya Prakash, Bo Dai, Chao Zhang</li>
<li>for: 这篇论文是针对predict-then-optimize问题的decision-focused learning（DFL）方法，对于exististing end-to-end DFL方法的三个瓶颈：模型误差错误、抽象描述错误和梯度近似错误。</li>
<li>methods: 我们的DF2方法是第一个不需要任务特定的预测器，直接在训练过程中学习预期优化目标函数。我们创造了一个注意力基于分布的模型架构，以便对预期目标函数进行有效的学习。</li>
<li>results: 我们在一个实验中，用DF2方法解决了一个 sintetic问题、一个风力发电问题和一个非凸疫苗分布问题，得到了DF2方法的有效性。<details>
<summary>Abstract</summary>
Decision-focused learning (DFL) has recently emerged as a powerful approach for predict-then-optimize problems by customizing a predictive model to a downstream optimization task. However, existing end-to-end DFL methods are hindered by three significant bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs as DFL relies on the KKT condition for exact gradient computation, while most methods approximate the gradient for backpropagation in non-convex objectives. In this paper, we present DF2 -- the first \textit{distribution-free} decision-focused learning method explicitly designed to address these three bottlenecks. Rather than depending on a task-specific forecaster that requires precise model assumptions, our method directly learns the expected optimization function during training. To efficiently learn the function in a data-driven manner, we devise an attention-based model architecture inspired by the distribution-based parameterization of the expected objective. Our method is, to the best of our knowledge, the first to address all three bottlenecks within a single model. We evaluate DF2 on a synthetic problem, a wind power bidding problem, and a non-convex vaccine distribution problem, demonstrating the effectiveness of DF2.
</details>
<details>
<summary>摘要</summary>
决策关注学习（DFL）是一种有力的方法，用于解决预测后优化问题，通过自适应一个预测模型来满足下游优化任务。然而，现有的端到端DFL方法受到三种主要瓶颈的限制：模型匹配错误、样本平均化错误和梯度估计错误。模型匹配错误来自预测模型中参数化的预测分布与真实概率分布之间的不一致。样本平均化错误发生在使用有限样本来估计优化目标函数的期望值时。梯度估计错误则是因为DFL通过KKT条件来计算梯度，而大多数方法在非拟合目标函数中使用较差的梯度估计进行反向传播。在这篇论文中，我们提出了DF2方法——首先的无模型匹配的决策关注学习方法，用于解决这三种瓶颈。而不是基于任务特定的预测器，我们的方法直接在训练过程中学习预测函数。为效率地学习函数，我们设计了一种注意力基于分布的模型架构，得益于分布基于参数化的预测目标函数。我们的方法是，到目前为止所知道的第一个能够同时解决这三种瓶颈的单一模型。我们在一个 sintetic问题、一个风力发电拍卖问题和一个非拟合疫苗分布问题上进行了评估，并证明了DF2的效果。
</details></li>
</ul>
<hr>
<h2 id="GPLaSDI-Gaussian-Process-based-Interpretable-Latent-Space-Dynamics-Identification-through-Deep-Autoencoder"><a href="#GPLaSDI-Gaussian-Process-based-Interpretable-Latent-Space-Dynamics-Identification-through-Deep-Autoencoder" class="headerlink" title="GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder"></a>GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05882">http://arxiv.org/abs/2308.05882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llnl/gplasdi">https://github.com/llnl/gplasdi</a></li>
<li>paper_authors: Christophe Bonneville, Youngsoo Choi, Debojyoti Ghosh, Jonathan L. Belof</li>
<li>for:  This paper aims to provide a novel reduced-order modeling (ROM) framework that leverages machine learning to solve partial differential equations (PDEs) efficiently and accurately.</li>
<li>methods:  The proposed method, called GPLaSDI, utilizes Gaussian processes (GPs) for latent space ODE interpolations, which enables the quantification of uncertainty over the ROM predictions and allows for efficient adaptive training.</li>
<li>results:  The proposed method achieves between 200 and 100,000 times speed-up, with up to 7% relative error, on the Burgers equation, Vlasov equation for plasma physics, and a rising thermal bubble problem.<details>
<summary>Abstract</summary>
Numerically solving partial differential equations (PDEs) can be challenging and computationally expensive. This has led to the development of reduced-order models (ROMs) that are accurate but faster than full order models (FOMs). Recently, machine learning advances have enabled the creation of non-linear projection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI maps full-order PDE solutions to a latent space using autoencoders and learns the system of ODEs governing the latent space dynamics. By interpolating and solving the ODE system in the reduced latent space, fast and accurate ROM predictions can be made by feeding the predicted latent space dynamics into the decoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework that relies on Gaussian process (GP) for latent space ODE interpolations. Using GPs offers two significant advantages. First, it enables the quantification of uncertainty over the ROM predictions. Second, leveraging this prediction uncertainty allows for efficient adaptive training through a greedy selection of additional training data points. This approach does not require prior knowledge of the underlying PDEs. Consequently, GPLaSDI is inherently non-intrusive and can be applied to problems without a known PDE or its residual. We demonstrate the effectiveness of our approach on the Burgers equation, Vlasov equation for plasma physics, and a rising thermal bubble problem. Our proposed method achieves between 200 and 100,000 times speed-up, with up to 7% relative error.
</details>
<details>
<summary>摘要</summary>
解决部分泛函方程（PDE）数学问题可以是困难的并且 computationally expensive。这导致了减少顺序模型（ROM）的发展，这些模型具有准确性，但速度比整个顺序模型（FOM）更快。近些年，机器学习的进步使得非线性投影方法，如潜在空间动力学标识（LaSDI）的创造。LaSDI将全序PDE解析到一个潜在空间使用自适应神经网络，并学习潜在空间动力学系统的ODE。通过在减少的潜在空间中预测和解决ODE系统，可以快速并准确地预测ROM。在这篇论文中，我们介绍了GPLaSDI，一种基于 Gaussian process（GP）的LaSDI框架。使用GP提供了两点优势。首先，它允许量化ROM预测中的uncertainty。其次，通过利用这种预测uncertainty，可以高效地进行适应性训练，通过滥见训练数据点。这种方法不需要先知道下辖PDE或其剩余。因此，GPLaSDI是非侵入的，可以应用于没有known PDE或其剩余的问题。我们在Burgers方程、Vlasov方程 для плазма物理和热气囊问题中展示了我们的方法的效果。我们的提posed方法可以实现200到100,000倍的速度增加，相对误差在7%之间。
</details></li>
</ul>
<hr>
<h2 id="Aphid-Cluster-Recognition-and-Detection-in-the-Wild-Using-Deep-Learning-Models"><a href="#Aphid-Cluster-Recognition-and-Detection-in-the-Wild-Using-Deep-Learning-Models" class="headerlink" title="Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models"></a>Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05881">http://arxiv.org/abs/2308.05881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianxiao Zhang, Kaidong Li, Xiangyu Chen, Cuncong Zhong, Bo Luo, Ivan Grijalva, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang</li>
<li>for: 本研究旨在使用深度学习模型检测螟蛾群体，以实现Targeted pesticide application。</li>
<li>methods: 我们使用了大规模的实验数据和深度学习模型来检测螟蛾群体，并对数据进行了处理和分割以便机器学习模型的使用。</li>
<li>results: 我们的实验结果表明，使用深度学习模型可以准确地检测螟蛾群体，并且可以通过合并邻近的群体和移除小 clusters来进一步提高性能。<details>
<summary>Abstract</summary>
Aphid infestation poses a significant threat to crop production, rural communities, and global food security. While chemical pest control is crucial for maximizing yields, applying chemicals across entire fields is both environmentally unsustainable and costly. Hence, precise localization and management of aphids are essential for targeted pesticide application. The paper primarily focuses on using deep learning models for detecting aphid clusters. We propose a novel approach for estimating infection levels by detecting aphid clusters. To facilitate this research, we have captured a large-scale dataset from sorghum fields, manually selected 5,447 images containing aphids, and annotated each individual aphid cluster within these images. To facilitate the use of machine learning models, we further process the images by cropping them into patches, resulting in a labeled dataset comprising 151,380 image patches. Then, we implemented and compared the performance of four state-of-the-art object detection models (VFNet, GFLV2, PAA, and ATSS) on the aphid dataset. Extensive experimental results show that all models yield stable similar performance in terms of average precision and recall. We then propose to merge close neighboring clusters and remove tiny clusters caused by cropping, and the performance is further boosted by around 17%. The study demonstrates the feasibility of automatically detecting and managing insects using machine learning models. The labeled dataset will be made openly available to the research community.
</details>
<details>
<summary>摘要</summary>
螟蛀感染 pose 对农业生产、农村社区和全球食品安全构成了严重的威胁。虽然化学防治是提高产量的重要手段，但是在整个场景中应用化学品是环境不可持续和昂贵的。因此，准确地Localization和管理螟蛀是必要的。本文主要关注使用深度学习模型 для检测螟蛀群。我们提出了一种新的方法，通过检测螟蛀群来估算感染水平。为了进行这项研究，我们在高粮田中采集了大规模数据集，手动选择了5,447张图像中包含螟蛀的图像，并对每个个体螟蛀群进行了标注。为了使机器学习模型可以使用，我们进一步处理了图像，将其分割成 patches，得到了151,380个标注图像 patches。然后，我们实现了和比较了四种当前最佳 объек detection 模型（VFNet、GFLV2、PAA 和 ATSS）在螟蛀数据集上的性能。广泛的实验结果表明，所有模型在精度和准确性方面具有稳定的性能。我们 then propose 将邻近的螟蛀群合并并 removes 小于patches 的螟蛀群，性能得到了约17%的提高。该研究表明了使用机器学习模型自动检测和管理昆虫的可能性。我们将标注数据集公开提供给研究社区。
</details></li>
</ul>
<hr>
<h2 id="Composable-Core-sets-for-Diversity-Approximation-on-Multi-Dataset-Streams"><a href="#Composable-Core-sets-for-Diversity-Approximation-on-Multi-Dataset-Streams" class="headerlink" title="Composable Core-sets for Diversity Approximation on Multi-Dataset Streams"></a>Composable Core-sets for Diversity Approximation on Multi-Dataset Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05878">http://arxiv.org/abs/2308.05878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephanie Wang, Michael Flynn, Fangyu Luo</li>
<li>for: 这篇论文旨在描述如何使用核心集来简化流处理数据，以便在实时训练机器学习模型时提高效率。</li>
<li>methods: 该论文提出了一种基于核心集的核心集建构算法，用于简化流处理数据并在活动学习环境中使用。此外，论文还提出了一种使用核心集和CRAIG等技术来加速建构速度。</li>
<li>results: 论文通过对推送数据进行预测分析，证明了这种方法可以在实时训练机器学习模型时提高效率。此外，论文还提出了一些改进建构速度的策略和技术。<details>
<summary>Abstract</summary>
Core-sets refer to subsets of data that maximize some function that is commonly a diversity or group requirement. These subsets are used in place of the original data to accomplish a given task with comparable or even enhanced performance if biases are removed. Composable core-sets are core-sets with the property that subsets of the core set can be unioned together to obtain an approximation for the original data; lending themselves to be used for streamed or distributed data. Recent work has focused on the use of core-sets for training machine learning models. Preceding solutions such as CRAIG have been proven to approximate gradient descent while providing a reduced training time. In this paper, we introduce a core-set construction algorithm for constructing composable core-sets to summarize streamed data for use in active learning environments. If combined with techniques such as CRAIG and heuristics to enhance construction speed, composable core-sets could be used for real time training of models when the amount of sensor data is large. We provide empirical analysis by considering extrapolated data for the runtime of such a brute force algorithm. This algorithm is then analyzed for efficiency through averaged empirical regression and key results and improvements are suggested for further research on the topic.
</details>
<details>
<summary>摘要</summary>
核心集（core-set）指的是一 subset of data 可以最大化某种函数，通常是多样性或组合要求。这些子集用于取代原始数据来完成一个给定任务，并且可以保持或提高性能，即使存在偏见。可搅 core-sets 是指可以将核心集中的子集 union 起来 obtaint 原始数据的一个近似。这些核心集具有可搅性，可以用于流处理或分布式数据。现有研究集中焦点在 core-sets 的使用，特别是用于训练机器学习模型。之前的解决方案，如 CRAIG，已经证明可以近似梯度下降，同时提供减少的训练时间。在这篇论文中，我们介绍一种用于构建可搅 core-sets 的算法，用于概要流处理数据，以便在活动学习环境中使用。如果与 CRAIG 和其他优化技术相结合，可搅 core-sets 可以用于实时训练模型，当感知数据量很大时。我们对此进行了实验分析，考虑了扩展数据的运行时间。这种算法的效率被分析了，并且提出了一些关键结果和改进建议。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-N-CNN-for-Clinical-Practice"><a href="#Revisiting-N-CNN-for-Clinical-Practice" class="headerlink" title="Revisiting N-CNN for Clinical Practice"></a>Revisiting N-CNN for Clinical Practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05877">http://arxiv.org/abs/2308.05877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Antunes Ferreira, Lucas Pereira Carlini, Gabriel de Almeida Sá Coutrin, Tatiany Marcondes Heideirich, Marina Carvalho de Moraes Barros, Ruth Guinsburg, Carlos Eduardo Thomaz</li>
<li>for: 这篇论文旨在优化Neonatal Convolutional Neural Network（N-CNN）的超参数，并评估这些超参数对类别指标、可解释性和可靠性的影响，以及它们在临床实践中的潜在影响。</li>
<li>methods: 我们选择了不改变原始N-CNN架构的超参数，主要是修改学习率和训练正则化。我们通过评估每个超参数的改进情况来选择最佳超参数，并创建了调整后的Tuned N-CNN。此外，我们还应用了基于新生脸部编码系统的软标签，提出了一种新的训练 expresión facial类型分类模型的方法，用于评估新生痛的评估。</li>
<li>results: 结果表明，调整后的Tuned N-CNN显示出了类别指标和可解释性的改进，但这些改进直接不对准确性表现出来。我们认为这些发现可能有助于开发更可靠的痛评估工具 для新生，帮助医疗专业人员提供适当的 intervención和改善病人结果。<details>
<summary>Abstract</summary>
This paper revisits the Neonatal Convolutional Neural Network (N-CNN) by optimizing its hyperparameters and evaluating how they affect its classification metrics, explainability and reliability, discussing their potential impact in clinical practice. We have chosen hyperparameters that do not modify the original N-CNN architecture, but mainly modify its learning rate and training regularization. The optimization was done by evaluating the improvement in F1 Score for each hyperparameter individually, and the best hyperparameters were chosen to create a Tuned N-CNN. We also applied soft labels derived from the Neonatal Facial Coding System, proposing a novel approach for training facial expression classification models for neonatal pain assessment. Interestingly, while the Tuned N-CNN results point towards improvements in classification metrics and explainability, these improvements did not directly translate to calibration performance. We believe that such insights might have the potential to contribute to the development of more reliable pain evaluation tools for newborns, aiding healthcare professionals in delivering appropriate interventions and improving patient outcomes.
</details>
<details>
<summary>摘要</summary>
（本文重新审查了新生儿 convolutional neural network（N-CNN）的超参数，并评估它们如何影响其分类指标、可解释性和可靠性，并讨论它们在临床实践中的潜在影响。我们选择了不改变原始 N-CNN 架构的超参数，主要是 modify 学习率和训练正则化。优化是通过评估每个超参数的改进情况来进行，并选择最佳超参数来创建一个优化后的 Tuned N-CNN。我们还应用了来自新生儿表情编码系统的软标签，提出了一种新的训练 facial expression 分类模型的方法，用于新生儿疼痛评估。有趣的是，改进后的 Tuned N-CNN 结果表明，对于分类指标和可解释性来说，有所改进，但这些改进并不直接对准报表性能产生影响。我们认为，这些发现可能对新生儿疼痛评估工具的开发产生影响，帮助医疗专业人员提供适当的 intervención和改善病人结果。）
</details></li>
</ul>
<hr>
<h2 id="UFed-GAN-A-Secure-Federated-Learning-Framework-with-Constrained-Computation-and-Unlabeled-Data"><a href="#UFed-GAN-A-Secure-Federated-Learning-Framework-with-Constrained-Computation-and-Unlabeled-Data" class="headerlink" title="UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data"></a>UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05870">http://arxiv.org/abs/2308.05870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achintha Wijesinghe, Songyang Zhang, Siyu Qi, Zhi Ding</li>
<li>for: 这篇论文是为了解决在云端环境中部署低延迟多媒体数据分类和数据隐私问题而提出的一种学习平衡，尤其是在有限的计算资源和没有标签数据的情况下。</li>
<li>methods: 这篇论文提出了一个名为UFed-GAN的无监督联邦学习框架，可以在用户端数据分布下进行学习，不需要进行本地分类训练。此外，论文还进行了对UFed-GAN的参数分析和隐私分析。</li>
<li>results: 实验结果显示，UFed-GAN在有限的计算资源和没有标签数据的情况下可以实现高效的数据分类和隐私保护。<details>
<summary>Abstract</summary>
To satisfy the broad applications and insatiable hunger for deploying low latency multimedia data classification and data privacy in a cloud-based setting, federated learning (FL) has emerged as an important learning paradigm. For the practical cases involving limited computational power and only unlabeled data in many wireless communications applications, this work investigates FL paradigm in a resource-constrained and label-missing environment. Specifically, we propose a novel framework of UFed-GAN: Unsupervised Federated Generative Adversarial Network, which can capture user-side data distribution without local classification training. We also analyze the convergence and privacy of the proposed UFed-GAN. Our experimental results demonstrate the strong potential of UFed-GAN in addressing limited computational resources and unlabeled data while preserving privacy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为满足云端数据分类和隐私的广泛应用和不满足的需求，联邦学习（FL）已经成为一种重要的学习模式。在许多无线通信应用中，由于限制的计算资源和只有无标签数据，这项工作研究了在资源限制和标签缺失环境中的FL模式。我们提出了一种新的框架——无监督联邦生成敌战网络（UFed-GAN），可以在用户端 cattcapture数据分布without local classification training。我们还分析了UFed-GAN的 converges和隐私性。我们的实验结果表明，UFed-GAN在限制计算资源和无标签数据的情况下具有强大的潜在性，能够解决数据隐私和安全问题。
</details></li>
</ul>
<hr>
<h2 id="Using-Twitter-Data-to-Determine-Hurricane-Category-An-Experiment"><a href="#Using-Twitter-Data-to-Determine-Hurricane-Category-An-Experiment" class="headerlink" title="Using Twitter Data to Determine Hurricane Category: An Experiment"></a>Using Twitter Data to Determine Hurricane Category: An Experiment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05866">http://arxiv.org/abs/2308.05866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songhui Yue, Jyothsna Kondari, Aibek Musaev, Randy K. Smith, Songqing Yue</li>
<li>for: 本研究旨在找到社交媒体数据和自然灾害的严重程度之间的映射关系。</li>
<li>methods: 本研究使用数据挖掘技术来分析Twitter数据，并对各地区的Twitter数据和飓风等级之间进行相关性分析。</li>
<li>results: 实验结果表明，Twitter数据和飓风等级之间存在积极的相关性，并提出了一种使用Twitter数据预测飓风等级的方法。<details>
<summary>Abstract</summary>
Social media posts contain an abundant amount of information about public opinion on major events, especially natural disasters such as hurricanes. Posts related to an event, are usually published by the users who live near the place of the event at the time of the event. Special correlation between the social media data and the events can be obtained using data mining approaches. This paper presents research work to find the mappings between social media data and the severity level of a disaster. Specifically, we have investigated the Twitter data posted during hurricanes Harvey and Irma, and attempted to find the correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.
</details>
<details>
<summary>摘要</summary>
社交媒体帖子中含有很多关于大事件的公众意见信息，尤其是自然灾害such as 飓风。posts相关的事件通常由用户们在事件发生时在附近的地方发布。我们的研究旨在找到社交媒体数据和灾害严重程度之间的映射。 Specifically, we investigated Twitter data posted during hurricanes Harvey and Irma and attempted to find a correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.Here's the word-for-word translation:社交媒体帖子中含有很多关于大事件的公众意见信息，尤其是自然灾害such as 飓风。posts相关的事件通常由用户们在事件发生时在附近的地方发布。我们的研究旨在找到社交媒体数据和灾害严重程度之间的映射。Specifically, we investigated Twitter data posted during hurricanes Harvey and Irma and attempted to find a correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.
</details></li>
</ul>
<hr>
<h2 id="The-Multi-modality-Cell-Segmentation-Challenge-Towards-Universal-Solutions"><a href="#The-Multi-modality-Cell-Segmentation-Challenge-Towards-Universal-Solutions" class="headerlink" title="The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions"></a>The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05864">http://arxiv.org/abs/2308.05864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, Wei Lou, Haofeng Li, Eric Upschulte, Timo Dickscheid, José Guilherme de Almeida, Yixin Wang, Lin Han, Xin Yang, Marco Labagnara, Sahand Jamal Rahi, Carly Kempster, Alice Pollitt, Leon Espinosa, Tâm Mignot, Jan Moritz Middeke, Jan-Niklas Eckardt, Wangkai Li, Zhaoyang Li, Xiaochen Cai, Bizhe Bai, Noah F. Greenwald, David Van Valen, Erin Weisbart, Beth A. Cimini, Zhuoshi Li, Chao Zuo, Oscar Brück, Gary D. Bader, Bo Wang</li>
<li>for: 单细胞分析中的细胞分割步骤是 kritical。</li>
<li>methods: 这些方法通常适应特定的modalities或需要手动参数调整来适应不同的实验设置。</li>
<li>results: 这个benchmark和改进的算法可以无需手动参数调整地应用于多种微显技术和组织类型的细胞图像。<details>
<summary>Abstract</summary>
Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyperparameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.
</details>
<details>
<summary>摘要</summary>
cell 分 segmentation 是单细胞分析中的关键步骤，exist 的 cell 分 segmentation 方法 oftentailored 到特定Modalities 或需要手动 intervene  specify  hyperparameters  in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.Note that the word "Transformer" in the original text was translated as "Transformer-based" in Simplified Chinese, as there is no direct equivalent of the word "Transformer" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Propagation-over-Conditional-Independence-Graphs"><a href="#Knowledge-Propagation-over-Conditional-Independence-Graphs" class="headerlink" title="Knowledge Propagation over Conditional Independence Graphs"></a>Knowledge Propagation over Conditional Independence Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05857">http://arxiv.org/abs/2308.05857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Urszula Chajewska, Harsh Shrivastava</li>
<li>for: 本研究旨在提出ci图表知识传播算法，用于从domain topology中提取有用信息。</li>
<li>methods: 本研究使用ci图表模型，并提出了一种基于ci图表的知识传播算法。</li>
<li>results: 实验结果表明，该算法在公共 disponibles的cora和pubmed datasets上有所提高，与现有技术相比。<details>
<summary>Abstract</summary>
Conditional Independence (CI) graph is a special type of a Probabilistic Graphical Model (PGM) where the feature connections are modeled using an undirected graph and the edge weights show the partial correlation strength between the features. Since the CI graphs capture direct dependence between features, they have been garnering increasing interest within the research community for gaining insights into the systems from various domains, in particular discovering the domain topology. In this work, we propose algorithms for performing knowledge propagation over the CI graphs. Our experiments demonstrate that our techniques improve upon the state-of-the-art on the publicly available Cora and PubMed datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced the text into Simplified Chinese.<</SYS>>conditional independence (CI) 图是一种特殊的概率图模型 (PGM)，其中特征连接使用无向图表示，边重量表示特征之间的半相关度。由于 CI 图表示直接相互关联的特征，因此在不同领域中的系统研究中备受关注，特别是发现领域 то波动。在这项工作中，我们提出了在 CI 图上进行知识传播的算法。我们的实验表明，我们的技术在公共可用的 Cora 和 PubMed 数据集上超过了当前最佳的状况。
</details></li>
</ul>
<hr>
<h2 id="CSPM-A-Contrastive-Spatiotemporal-Preference-Model-for-CTR-Prediction-in-On-Demand-Food-Delivery-Services"><a href="#CSPM-A-Contrastive-Spatiotemporal-Preference-Model-for-CTR-Prediction-in-On-Demand-Food-Delivery-Services" class="headerlink" title="CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services"></a>CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08446">http://arxiv.org/abs/2308.08446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guyu Jiang, Xiaoyun Li, Rongrong Jing, Ruoqi Zhao, Xingliang Ni, Guodong Cao, Ning Hu</li>
<li>for: 这篇论文的目的是为了提高在在线快递食品平台上的点击率预测（CTR）。</li>
<li>methods: 这篇论文使用了三个模块：对比性空间时间表示学习（CSRL）、空间时间喜好EXTRACTOR（StPE）和空间时间信息过滤器（StIF）。CSRL使用了对比学习框架来生成搜索行为中的空间时间活动表示（SAR）。StPE使用了SAR来活化用户的不同的位置和时间相关的喜好，使用多头注意机制。StIF将SARintegrated into a gating network来自动捕捉重要的隐藏空间时间效果。</li>
<li>results: 在两个大规模的工业数据集上进行了广泛的实验，并证明了CSPM的状态之前性表现。尤其是，CSPM已经成功部署在阿里巴巴的在线快递食品平台Ele.me上，导致了显著的0.88%提升Click-through rate，这有重要的商业意义。<details>
<summary>Abstract</summary>
Click-through rate (CTR) prediction is a crucial task in the context of an online on-demand food delivery (OFD) platform for precisely estimating the probability of a user clicking on food items. Unlike universal e-commerce platforms such as Taobao and Amazon, user behaviors and interests on the OFD platform are more location and time-sensitive due to limited delivery ranges and regional commodity supplies. However, existing CTR prediction algorithms in OFD scenarios concentrate on capturing interest from historical behavior sequences, which fails to effectively model the complex spatiotemporal information within features, leading to poor performance. To address this challenge, this paper introduces the Contrastive Sres under different search states using three modules: contrastive spatiotemporal representation learning (CSRL), spatiotemporal preference extractor (StPE), and spatiotemporal information filter (StIF). CSRL utilizes a contrastive learning framework to generate a spatiotemporal activation representation (SAR) for the search action. StPE employs SAR to activate users' diverse preferences related to location and time from the historical behavior sequence field, using a multi-head attention mechanism. StIF incorporates SAR into a gating network to automatically capture important features with latent spatiotemporal effects. Extensive experiments conducted on two large-scale industrial datasets demonstrate the state-of-the-art performance of CSPM. Notably, CSPM has been successfully deployed in Alibaba's online OFD platform Ele.me, resulting in a significant 0.88% lift in CTR, which has substantial business implications.
</details>
<details>
<summary>摘要</summary>
Click-through rate (CTR) 预测是在在线快递食品平台上关键的任务，准确地估计用户会点击食品项。不同于通用电商平台如淘宝和amazon，用户在食品平台上的行为和兴趣更加地受到地域和时间影响，因为交通范围和地域商品供应有限。然而，现有的 CTRL 预测算法在食品平台场景中集中在捕捉历史行为序列中的兴趣，而不能有效地模型特有的空间时间信息，导致表现不佳。为解决这个挑战，本文引入了不同搜索状态下的 Contrastive Sres，使用三个模块：对比空间时间表示学习（CSRL）、空间时间偏好提取器（StPE）和空间时间信息筛选器（StIF）。CSRL 利用对比学习框架生成一个空间时间活动表示（SAR） для搜索行为。StPE 使用 SAR 来激活用户的不同地域时间上的偏好，使用多头注意机制。StIF 将 SAR integrated into a gating network 自动捕捉特有的空间时间效应。经验表明，CSPM 在两个大规模的业务数据集上达到了领先的性能，并在阿里巴巴在线食品平台 Ele.me 上部署成功，导致了显著的0.88%增加 Click-through rate，这有substantial商业意义。
</details></li>
</ul>
<hr>
<h2 id="GaborPINN-Efficient-physics-informed-neural-networks-using-multiplicative-filtered-networks"><a href="#GaborPINN-Efficient-physics-informed-neural-networks-using-multiplicative-filtered-networks" class="headerlink" title="GaborPINN: Efficient physics informed neural networks using multiplicative filtered networks"></a>GaborPINN: Efficient physics informed neural networks using multiplicative filtered networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05843">http://arxiv.org/abs/2308.05843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinquan Huang, Tariq Alkhalifah<br>for: 这篇论文的目的是提出一种改进的物理学 Informed Neural Network（PINN）方法，以加速冲击波场的计算。methods: 这篇论文使用了多个技术，包括physics-informed neural networks（PINNs）和Gabor基函数。它们在训练过程中嵌入了一些已知的冲击波场特征，如频率，以提高快速度。results: 论文的实验结果表明，使用GaborPINN方法可以大大提高冲击波场的计算速度，比传统的PINN方法快两个数量级。<details>
<summary>Abstract</summary>
The computation of the seismic wavefield by solving the Helmholtz equation is crucial to many practical applications, e.g., full waveform inversion. Physics-informed neural networks (PINNs) provide functional wavefield solutions represented by neural networks (NNs), but their convergence is slow. To address this problem, we propose a modified PINN using multiplicative filtered networks, which embeds some of the known characteristics of the wavefield in training, e.g., frequency, to achieve much faster convergence. Specifically, we use the Gabor basis function due to its proven ability to represent wavefields accurately and refer to the implementation as GaborPINN. Meanwhile, we incorporate prior information on the frequency of the wavefield into the design of the method to mitigate the influence of the discontinuity of the represented wavefield by GaborPINN. The proposed method achieves up to a two-magnitude increase in the speed of convergence as compared with conventional PINNs.
</details>
<details>
<summary>摘要</summary>
computations of seismic wavefield by solving Helmholtz equation is crucial to many practical applications, e.g., full waveform inversion. physics-informed neural networks (PINNs) provide functional wavefield solutions represented by neural networks (NNs), but their convergence is slow. to address this problem, we propose modified PINN using multiplicative filtered networks, which embeds some of the known characteristics of wavefield in training, e.g., frequency, to achieve much faster convergence. specifically, we use Gabor basis function due to its proven ability to represent wavefields accurately and refer to the implementation as GaborPINN. meanwhile, we incorporate prior information on frequency of wavefield into the design of method to mitigate influence of discontinuity of represented wavefield by GaborPINN. proposed method achieves up to two-magnitude increase in speed of convergence as compared with conventional PINNs.
</details></li>
</ul>
<hr>
<h2 id="FLShield-A-Validation-Based-Federated-Learning-Framework-to-Defend-Against-Poisoning-Attacks"><a href="#FLShield-A-Validation-Based-Federated-Learning-Framework-to-Defend-Against-Poisoning-Attacks" class="headerlink" title="FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks"></a>FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05832">http://arxiv.org/abs/2308.05832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsanul Kabir, Zeyu Song, Md Rafi Ur Rashid, Shagufta Mehnaz</li>
<li>for: 本研究旨在提出一种新的联合学习（Federated Learning，FL）框架，以保障FL系统的安全性和可靠性。</li>
<li>methods: 本研究使用了本地模型的有益数据来验证本地模型的可靠性，而不是依赖服务器访问spotless数据集，这种做法和FL的基本原则不匹配。</li>
<li>results: 研究人员通过对不同情况下的FLShield框架进行了广泛的实验，并证明了FLShield框架能够有效地防范各种毒素和后门攻击，同时保护本地数据的隐私。<details>
<summary>Abstract</summary>
Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants' actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server's access to clean datasets -- an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield framework in different settings and demonstrate its effectiveness in thwarting various types of poisoning and backdoor attacks including a defense-aware one. FLShield also preserves privacy of local data against gradient inversion attacks.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是如何改变我们如何从数据中学习的革命。随着其 Popularity 的增长，它现在在许多安全关键领域，如自动驾驶和医疗，中使用。由于千余名参与者可以在这种合作环境中贡献，因此保持安全性和可靠性的系统是挑战。这 Heightens 需要设计安全可靠的 FL 系统，能够抵御恶意参与者的行为，同时保持本地数据隐私和效率。在这篇论文中，我们提出了一种新的 FL 框架，名为 FLShield，它利用 FL 参与者的善意数据来验证本地模型，然后将其作为全球模型生成。这与现有防御方法，它们基于服务器访问干净的数据集的假设，不同。我们进行了广泛的实验来评估我们的 FLShield 框架在不同的设置下的效果，并证明它在不同类型的毒素和后门攻击中具有有效性。FLShield 还保持本地数据隐私性免受Gradient Inversion攻击。
</details></li>
</ul>
<hr>
<h2 id="Neural-Progressive-Meshes"><a href="#Neural-Progressive-Meshes" class="headerlink" title="Neural Progressive Meshes"></a>Neural Progressive Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05741">http://arxiv.org/abs/2308.05741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Chun Chen, Vladimir G. Kim, Noam Aigerman, Alec Jacobson</li>
<li>for:  efficiently transmit large geometric data (e.g., 3D meshes) over the Internet</li>
<li>methods: subdivision-based encoder-decoder architecture trained on a large collection of surfaces, with progressive transmission of residual features</li>
<li>results: outperforms baselines in terms of compression ratio and reconstruction quality<details>
<summary>Abstract</summary>
The recent proliferation of 3D content that can be consumed on hand-held devices necessitates efficient tools for transmitting large geometric data, e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a challenge to storage as well as transmission bandwidth, and level-of-detail techniques are often used to transmit an asset using an appropriate bandwidth budget. It is especially desirable for these methods to transmit data progressively, improving the quality of the geometry with more data. Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns even across different shapes, and thus can be effectively represented with a shared learned generative space. We learn this space using a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces. We further observe that additional residual features can be transmitted progressively between intermediate levels of subdivision that enable the client to control the tradeoff between bandwidth cost and quality of reconstruction, providing a neural progressive mesh representation. We evaluate our method on a diverse set of complex 3D shapes and demonstrate that it outperforms baselines in terms of compression ratio and reconstruction quality.
</details>
<details>
<summary>摘要</summary>
Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns across different shapes, and can be effectively represented with a shared learned generative space. We use a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces to learn this space. Additionally, we observe that residual features can be transmitted progressively between intermediate levels of subdivision, enabling the client to control the tradeoff between bandwidth cost and quality of reconstruction. This provides a neural progressive mesh representation.We evaluate our method on a diverse set of complex 3D shapes and demonstrate that it outperforms baselines in terms of compression ratio and reconstruction quality.
</details></li>
</ul>
<hr>
<h2 id="Zero-Grads-Ever-Given-Learning-Local-Surrogate-Losses-for-Non-Differentiable-Graphics"><a href="#Zero-Grads-Ever-Given-Learning-Local-Surrogate-Losses-for-Non-Differentiable-Graphics" class="headerlink" title="Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics"></a>Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05739">http://arxiv.org/abs/2308.05739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Fischer, Tobias Ritschel</li>
<li>for: 解决 Graphics 中的Gradient-based优化问题，因为现有的搜索方法无法处理undefined或zero gradients。</li>
<li>methods: 提出了一种自动化替换损失函数的框架，即ZeroGrads，通过学习一个神经网络来 aproximate objective function，并使用这个神经网络来 differentiate through arbitrary black-box graphics pipelines。</li>
<li>results: 实现了在online和self-supervised的情况下，使用 actively smoothed version of the objective 进行训练，并且可以在 tractable run-times 和competitive performance下解决多个非对称、非 differentiable black-box problem in Graphics，如视力rendering、排序参数空间和物理驱动动画优化等。<details>
<summary>Abstract</summary>
Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a "surrogate" that has similar minima but is differentiable. Our proposed framework, ZeroGrads, automates this process by learning a neural approximation of the objective function, the surrogate, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competitive performance at little overhead. We demonstrate optimizing diverse non-convex, non-differentiable black-box problems in graphics, such as visibility in rendering, discrete parameter spaces in procedural modelling or optimal control in physics-driven animation. In contrast to more traditional algorithms, our approach scales well to higher dimensions, which we demonstrate on problems with up to 35k interlinked variables.
</details>
<details>
<summary>摘要</summary>
“梯度基本优化现在在图形处理中广泛应用，但它无法应用于无定义或 zeros 梯度的问题。为了缺 Bibliography 这个问题，我们可以手动替换损失函数，使其成为可导的。我们的提议的框架，ZeroGrads，可以自动实现这个过程，它通过学习一个神经网络来模拟目标函数，并使用这个模拟来分子 Graphics pipeline 中的任意黑盒问题。我们在训练过程中使用在目标函数上实时缓和的版本，并且强调本集成性，使得模拟的能量集中在当前训练集中。我们的整个训练过程是在线进行的，并且是自动的，不需要预计算数据或预训练模型。由于评估目标函数的成本 relativity 高（它需要一个完整的渲染或 simulator 运行），我们开发了一种有效的采样方案，使得我们可以在可耗时间和竞争性的情况下实现高性能。我们在不同的非等式、非导数的黑盒问题上进行了优化，例如渲染中的可见性、procedural 模型中的分配空间和物理驱动的动画中的优化问题。与传统算法相比，我们的方法可以很好地扩展到更高的维度，我们在问题中的35k个相互关联变量上进行了示例。”
</details></li>
</ul>
<hr>
<h2 id="Follow-Anything-Open-set-detection-tracking-and-following-in-real-time"><a href="#Follow-Anything-Open-set-detection-tracking-and-following-in-real-time" class="headerlink" title="Follow Anything: Open-set detection, tracking, and following in real-time"></a>Follow Anything: Open-set detection, tracking, and following in real-time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05737">http://arxiv.org/abs/2308.05737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alaamaalouf/followanything">https://github.com/alaamaalouf/followanything</a></li>
<li>paper_authors: Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallabhula, Makram Chahine, Daniel M. Vogt, Robert J. Wood, Antonio Torralba, Daniela Rus</li>
<li>for: 本研究旨在开发一种可以在实时控制循环中跟踪任何对象的机器人系统。</li>
<li>methods: 该系统基于大规模预训练模型（基础模型），可以在实时控制循环中检测、分割和跟踪对象，并且可以考虑 occlusion 和对象重新出现。</li>
<li>results: 研究人员在一架微型飞行器上部署了该系统，并成功地跟踪了各种对象。系统可以在一个简单的 laptop 上运行，并且可以达到 6-20 帧每秒的throughput。<details>
<summary>Abstract</summary>
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed ``follow anything'' (FAn), is an open-vocabulary and multimodal model -- it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of interest in a real-time control loop. FAn can be deployed on a laptop with a lightweight (6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To enable rapid adoption, deployment, and extensibility, we open-source all our code on our project webpage at https://github.com/alaamaalouf/FollowAnything . We also encourage the reader the watch our 5-minutes explainer video in this https://www.youtube.com/watch?v=6Mgt3EPytrw .
</details>
<details>
<summary>摘要</summary>
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed “follow anything” (FAn), is an open-vocabulary and multimodal model — it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of interest in a real-time control loop. FAn can be deployed on a laptop with a lightweight (6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To enable rapid adoption, deployment, and extensibility, we open-source all our code on our project webpage at <https://github.com/alaamaalouf/FollowAnything>. We also encourage the reader to watch our 5-minutes explainer video in this <https://www.youtube.com/watch?v=6Mgt3EPytrw>.
</details></li>
</ul>
<hr>
<h2 id="PDE-Refiner-Achieving-Accurate-Long-Rollouts-with-Neural-PDE-Solvers"><a href="#PDE-Refiner-Achieving-Accurate-Long-Rollouts-with-Neural-PDE-Solvers" class="headerlink" title="PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers"></a>PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05732">http://arxiv.org/abs/2308.05732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phillip Lippe, Bastiaan S. Veeling, Paris Perdikaris, Richard E. Turner, Johannes Brandstetter<br>for:This paper aims to improve the accuracy and stability of deep neural network-based solution techniques for partial differential equations (PDEs) by addressing the neglect of non-dominant spatial frequency information.methods:The authors use a large-scale analysis of common temporal rollout strategies and draw inspiration from recent advances in diffusion models to introduce a novel model class called PDE-Refiner, which uses a multistep refinement process to accurately model all frequency components of PDE solutions.results:The authors validate PDE-Refiner on challenging benchmarks of complex fluid dynamics and demonstrate stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. Additionally, PDE-Refiner is shown to greatly enhance data efficiency by implicitly inducing a novel form of spectral data augmentation, and the authors demonstrate an accurate and efficient assessment of the model’s predictive uncertainty.Here is the simplified Chinese text:for: 这篇论文目的是提高深度神经网络基于 partial differential equation (PDE) 的解决方法的精度和稳定性。methods: 作者使用大规模的时间滚动策略分析，并启发自latest advances in diffusion models ，引入了一种新的模型类叫 PDE-Refiner，该模型使用多步增强过程来准确地模型 PDE 解的所有频率组成部分。results: 作者验证 PDE-Refiner 在复杂的流体动力学 benchmark 上，并显示了稳定和准确的滚动性能，常常超越当前的模型，包括神经网络、数学、和混合神经网络-数学模型。此外， PDE-Refiner 能够大幅提高数据效率，因为净化目标意味着在数据增强过程中隐式地引入了一种新的频率数据增强。最后， PDE-Refiner 的连接到 diffusion models 使得可以准确地评估模型的预测不确定性，从而估计模型在滚动过程中的不确定性。<details>
<summary>Abstract</summary>
Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of complex fluid dynamics, demonstrating stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. We further demonstrate that PDE-Refiner greatly enhances data efficiency, since the denoising objective implicitly induces a novel form of spectral data augmentation. Finally, PDE-Refiner's connection to diffusion models enables an accurate and efficient assessment of the model's predictive uncertainty, allowing us to estimate when the surrogate becomes inaccurate.
</details>
<details>
<summary>摘要</summary>
时间依赖的 partial differential equations (PDEs) 在科学和工程中具有广泛的应用。最近，主要由于传统解决方案的计算成本高，深度神经网络基于的 surrogate 获得了更多的关注。然而，实际应用中，这些神经网络 PDE 解决器的实用性取决于它们能够提供稳定、准确的预测，这是一个非常困难的问题。在这种情况下，我们提出了一项大规模分析 temporal rollout 策略，发现忽略非主要空间频率信息，通常与 PDE 解的高频成分相关，是主要的障碍物，限制稳定、准确的 rollout 性能。基于这些发现，我们启发自 recent advances in diffusion models，提出了 PDE-Refiner; 一种新的模型类，可以更好地模拟所有频率组成部分。我们验证 PDE-Refiner 在复杂的流体动力学 benchmark 上，可以实现稳定和准确的 rollouts，常常超越当前模型，包括神经网络、数值和混合神经网络-数值模型。此外，PDE-Refiner 可以大幅提高数据效率，因为净化目标隐式地导入了一种新的 spectral data augmentation。最后，PDE-Refiner 的连接到 diffusion models 使得可以准确地评估模型的预测不确定性，从而估计 surrogate 是否准确。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Integration-of-Prediction-and-Planning-in-Deep-Learning-Based-Automated-Driving-Systems-A-Review"><a href="#Rethinking-Integration-of-Prediction-and-Planning-in-Deep-Learning-Based-Automated-Driving-Systems-A-Review" class="headerlink" title="Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review"></a>Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05731">http://arxiv.org/abs/2308.05731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steffen Hagedorn, Marcel Hallgarten, Martin Stoll, Alexandru Condurache</li>
<li>for: 这篇论文主要写于自动驾驶技术的发展，具体来说是关于 Prediction、Planning 和 Integrated Prediction and Planning 模型的系统性评估。</li>
<li>methods: 论文使用了深度学习技术来实现 Prediction 和 Planning 模型，并对不同的集成方法进行了系统性的比较和分析。</li>
<li>results: 论文发现了不同集成方法在自动驾驶中的优缺点，并指出了未来研究的方向和挑战。<details>
<summary>Abstract</summary>
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking. We systematically review state-of-the-art deep learning-based prediction, planning, and integrated prediction and planning models. Different facets of the integration ranging from model architecture and model design to behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration methods. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.
</details>
<details>
<summary>摘要</summary>
（简化中文）自动驾驶有可能改变个人、公共和货物运输方式。除了巨大的感知挑战以外，自动驾驶还包括规划安全、舒适和有效的运动轨迹。为促进安全和进步，许多工作依赖于周围交通的未来运动预测。现有的自动驾驶系统通常将预测和规划作为独立的两个任务进行处理。这种方法虽然考虑了周围交通对ego汽车的影响，但是忽略了ego汽车行为对交通参与者的反应。 latest works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. We systematically review state-of-the-art deep learning-based prediction, planning, and integrated prediction and planning models. Different facets of the integration ranging from model architecture and model design to behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration methods. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.
</details></li>
</ul>
<hr>
<h2 id="EXPRESSO-A-Benchmark-and-Analysis-of-Discrete-Expressive-Speech-Resynthesis"><a href="#EXPRESSO-A-Benchmark-and-Analysis-of-Discrete-Expressive-Speech-Resynthesis" class="headerlink" title="EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis"></a>EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05725">http://arxiv.org/abs/2308.05725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tu Anh Nguyen, Wei-Ning Hsu, Antony D’Avirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, Felix Kreuk, Yossi Adi, Emmanuel Dupoux</li>
<li>for: 本研究旨在开发一个高质量的自然语言表达 speech 数据集，用于无文本 speech 生成。</li>
<li>methods: 研究人员使用了自我超级vised学习的方法，将低比特率的精度单元学习到 speech 中，以捕捉expressive aspect of speech。</li>
<li>results: 研究人员在 Expresso 数据集上进行了一系列的evalution，并结果表明，这种方法可以实现高质量的 expressive speech 生成。Here’s a more detailed explanation of each point:</li>
<li>for: The paper is aimed at developing a high-quality expressive speech dataset for textless speech synthesis.</li>
<li>methods: The researchers use self-supervised learning methods to learn low-bitrate discrete units that can capture expressive aspects of speech, such as prosody and voice styles.</li>
<li>results: The researchers evaluate the effectiveness of their approach on the Expresso dataset, which includes both read speech and improvised dialogues in 26 spontaneous expressive styles. The results show that the method can achieve high-quality expressive speech synthesis.<details>
<summary>Abstract</summary>
Recent work has shown that it is possible to resynthesize high-quality speech based, not on text, but on low bitrate discrete units that have been learned in a self-supervised fashion and can therefore capture expressive aspects of speech that are hard to transcribe (prosody, voice styles, non-verbal vocalization). The adoption of these methods is still limited by the fact that most speech synthesis datasets are read, severely limiting spontaneity and expressivity. Here, we introduce Expresso, a high-quality expressive speech dataset for textless speech synthesis that includes both read speech and improvised dialogues rendered in 26 spontaneous expressive styles. We illustrate the challenges and potentials of this dataset with an expressive resynthesis benchmark where the task is to encode the input in low-bitrate units and resynthesize it in a target voice while preserving content and style. We evaluate resynthesis quality with automatic metrics for different self-supervised discrete encoders, and explore tradeoffs between quality, bitrate and invariance to speaker and style. All the dataset, evaluation metrics and baseline models are open source
</details>
<details>
<summary>摘要</summary>
近期研究表明，可以使用低比特率不同单元进行自我指导的高质量语音重建。这些单元可以捕捉到表达方面的声音特征，如声音态度、声音风格和非语言 vocalization，这些特征Difficult to transcribe。然而，目前这些方法的应用仍然受限于大多数语音重建数据集是阅读的，因此减少了自由和表达力。在这篇文章中，我们介绍了一个高质量表达语音数据集，即Expresso，该数据集包括了阅读语音和自由对话，并且在26种自然表达风格下进行了渲染。我们介绍了这个数据集的挑战和潜在性，并在表达重建中进行了一种表达编码和重建任务，以测试不同的自我指导精度单元的质量。我们使用了自动化 метри来评估重建质量，并 explore了不同精度单元的平衡点，包括质量、比特率和对 speaker和风格的不变性。所有的数据集、评估指标和基础模型都是开源的。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Performance-of-Feedforward-and-Convolutional-Neural-Networks-through-Dynamic-Activation-Functions"><a href="#Optimizing-Performance-of-Feedforward-and-Convolutional-Neural-Networks-through-Dynamic-Activation-Functions" class="headerlink" title="Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions"></a>Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05724">http://arxiv.org/abs/2308.05724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chinmay Rane, Kanishka Tyagi, Michael Manry</li>
<li>for: 本研究旨在探讨 activation functions 在 convolutional neural networks (CNNs) 中的影响，并提出了一种复杂的 Piece-wise Linear (PWL) 活动函数来取代 Relu 活动函数。</li>
<li>methods: 本研究使用了 PyTorch 框架进行实验，并对 shallow CNNs 和深度 CNNs 进行比较。</li>
<li>results: 研究发现，使用 PWL 活动函数可以大大提高 CNNs 的性能，并且在深度 CNNs 中提供了更多的可能性空间。<details>
<summary>Abstract</summary>
Deep learning training training algorithms are a huge success in recent years in many fields including speech, text,image video etc. Deeper and deeper layers are proposed with huge success with resnet structures having around 152 layers. Shallow convolution neural networks(CNN's) are still an active research, where some phenomena are still unexplained. Activation functions used in the network are of utmost importance, as they provide non linearity to the networks. Relu's are the most commonly used activation function.We show a complex piece-wise linear(PWL) activation in the hidden layer. We show that these PWL activations work much better than relu activations in our networks for convolution neural networks and multilayer perceptrons. Result comparison in PyTorch for shallow and deep CNNs are given to further strengthen our case.
</details>
<details>
<summary>摘要</summary>
深度学习训练算法在最近几年内在多个领域取得了巨大成功，包括语音、文本、图像和视频等。随着更深层的提议，深度学习模型在152层以上的ResNet结构中得到了巨大成功。 however， shallow convolutional neural networks（CNN）仍然是一个活跃的研究领域，一些现象仍未得到解释。 activation functions在网络中具有重要的作用，它们提供了非线性性，使网络更加复杂。 ReLU activation function是最常用的activation function。我们在隐藏层使用复杂的 piece-wise linear（PWL）activation，并证明这些PWL activation在我们的网络中工作得更好than ReLU activation。我们还在PyTorch中对 shallow和深度CNN进行比较，以更加强化我们的论据。
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Classical-and-Deep-Reinforcement-Learning-Methods-for-HVAC-Control"><a href="#A-Comparison-of-Classical-and-Deep-Reinforcement-Learning-Methods-for-HVAC-Control" class="headerlink" title="A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control"></a>A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05711">http://arxiv.org/abs/2308.05711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marshall Wang, John Willes, Thomas Jiralerspong, Matin Moezzi</li>
<li>for: 这个论文旨在使用强化学习（RL）方法优化冷却空调系统的控制，提高系统性能，降低能源消耗，提高成本效益。</li>
<li>methods: 这篇论文使用了两种流行的 класси型和深度RL方法（Q-学习和深度Q-网络），在多个冷却空调环境下进行了比较。</li>
<li>results: 研究发现，RL方法可以在冷却空调系统中提高性能，降低能源消耗，且模型参数选择和奖励调整是RL agents配置的关键因素。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a promising approach for optimizing HVAC control. RL offers a framework for improving system performance, reducing energy consumption, and enhancing cost efficiency. We benchmark two popular classical and deep RL methods (Q-Learning and Deep-Q-Networks) across multiple HVAC environments and explore the practical consideration of model hyper-parameter selection and reward tuning. The findings provide insight for configuring RL agents in HVAC systems, promoting energy-efficient and cost-effective operation.
</details>
<details>
<summary>摘要</summary>
现代控制技术（Reinforcement Learning，RL）可以有效地优化冷暖空调系统的控制。RL提供了一个框架，可以提高系统性能，降低能源消耗，提高成本效益。我们在多个冷暖空调环境中对两种流行的古典RL和深度RL方法（Q-学习和深度Q网络）进行了比较，并探讨RL代理人在冷暖空调系统中的实用考虑和奖励调整。发现提供了RL代理人配置的指导，推动了能源减少和成本效益的操作。
</details></li>
</ul>
<hr>
<h2 id="Shadow-Datasets-New-challenging-datasets-for-Causal-Representation-Learning"><a href="#Shadow-Datasets-New-challenging-datasets-for-Causal-Representation-Learning" class="headerlink" title="Shadow Datasets, New challenging datasets for Causal Representation Learning"></a>Shadow Datasets, New challenging datasets for Causal Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05707">http://arxiv.org/abs/2308.05707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jiagengzhu/Shadow-dataset-for-crl">https://github.com/Jiagengzhu/Shadow-dataset-for-crl</a></li>
<li>paper_authors: Jiageng Zhu, Hanchen Xie, Jianhua Wu, Jiazhi Li, Mahyar Khayatkhoei, Mohamed E. Hussein, Wael AbdAlmageed</li>
<li>for: 本研究的目的是探索语义因素之间的 causal 关系，以便进行更好的表征学习。</li>
<li>methods: 该研究使用了weakly supervised causal representation learning（CRL）方法，以解决高成本的标注问题。</li>
<li>results: 研究提出了两个新的数据集，以及对现有数据集的修改，以满足更复杂的 causal 图和更多的生成因素的需求。<details>
<summary>Abstract</summary>
Discovering causal relations among semantic factors is an emergent topic in representation learning. Most causal representation learning (CRL) methods are fully supervised, which is impractical due to costly labeling. To resolve this restriction, weakly supervised CRL methods were introduced. To evaluate CRL performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and CelebA(SMILE), are utilized. However, existing CRL datasets are limited to simple graphs with few generative factors. Thus we propose two new datasets with a larger number of diverse generative factors and more sophisticated causal graphs. In addition, current real datasets, CelebA(BEARD) and CelebA(SMILE), the originally proposed causal graphs are not aligned with the dataset distributions. Thus, we propose modifications to them.
</details>
<details>
<summary>摘要</summary>
发现 semantic 因素之间的 causal 关系是 representation learning 中一个emerging topic。大多数 causal representation learning（CRL）方法是完全supervised，这是因为标注成本太高。为解决这种限制，我们提出了弱标注 CRL 方法。为评估 CRL 性能，我们使用了四个现有的数据集：Pendulum、Flow、CelebA（BEARD）和 CelebA（SMILE）。然而，现有的 CRL 数据集受限于简单的图与少量生成因素。因此，我们提出了两个新的数据集，它们具有更多的多样化生成因素和更复杂的 causal 图。此外，原始的现实数据集 CelebA（BEARD）和 CelebA（SMILE）的 causal 图与数据分布不一致。因此，我们提出了修改。
</details></li>
</ul>
<hr>
<h2 id="Hard-No-Box-Adversarial-Attack-on-Skeleton-Based-Human-Action-Recognition-with-Skeleton-Motion-Informed-Gradient"><a href="#Hard-No-Box-Adversarial-Attack-on-Skeleton-Based-Human-Action-Recognition-with-Skeleton-Motion-Informed-Gradient" class="headerlink" title="Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient"></a>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05681">http://arxiv.org/abs/2308.05681</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luyg45/hardnoboxattack">https://github.com/luyg45/hardnoboxattack</a></li>
<li>paper_authors: Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum</li>
<li>for: 这 paper 探讨了 skeleton-based 人体活动识别系统 的攻击性评估问题。</li>
<li>methods: 该 paper 使用了一种新的攻击任务，即攻击者没有访问受害者模型或训练数据或标签。具体来说，它们首先学习了一个动作抽象空间，然后定义了一种对抗损失来计算一个新的攻击方向，称为skeleton-motion-informed（SMI）梯度。这个梯度包含了动作动态信息，与现有的梯度基于攻击方法不同。</li>
<li>results: 该 paper 的实验和比较结果表明，SMI 梯度可以在无框杆和转移基于黑盒 Setting 中提高攻击性和透明度。<details>
<summary>Abstract</summary>
Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming each dimension in the data is independent. The SMI gradient can augment many gradient-based attack methods, leading to a new family of no-box attack methods. Extensive evaluation and comparison show that our method imposes a real threat to existing classifiers. They also show that the SMI gradient improves the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box settings.
</details>
<details>
<summary>摘要</summary>
近期，基于骨架的人体活动识别方法已经被证明容易受到敌意攻击。然而，这些攻击方法都需要受害者（白盒攻击）、训练数据（传输基于攻击）或模型查询（黑盒攻击）的访问权限。这些需求都是非常限制的，这引发了对攻击性的评估。在这篇论文中，我们证明了这种攻击性确实存在。为此，我们提出了一个新的攻击任务：攻击者无法访问受害者的模型或训练数据或标签。我们称之为“困难无框攻击”（hard no-box attack）。我们首先学习了一个运动拟合，并定义了一种对抗损失来计算一个新的攻击Gradient，称之为skeleton-motion-informed（SMI）梯度。我们的梯度包含运动动力学信息，与现有的梯度基本攻击方法不同，它们计算损失梯度，假设每个数据维度独立。SMI梯度可以增强许多梯度基本攻击方法，导致一个新的无框攻击家族。我们的评估和比较表明，我们的方法对现有分类器 pose a real threat。它们还表明了SMI梯度的传播性和隐蔽性在无框和传输基于黑盒 Setting 中得到了改进。
</details></li>
</ul>
<hr>
<h2 id="Finding-Already-Debunked-Narratives-via-Multistage-Retrieval-Enabling-Cross-Lingual-Cross-Dataset-and-Zero-Shot-Learning"><a href="#Finding-Already-Debunked-Narratives-via-Multistage-Retrieval-Enabling-Cross-Lingual-Cross-Dataset-and-Zero-Shot-Learning" class="headerlink" title="Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning"></a>Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05680">http://arxiv.org/abs/2308.05680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iknoor Singh, Carolina Scarton, Xingyi Song, Kalina Bontcheva</li>
<li>for: 本研究的目的是探讨跨语言验证答案已经证明为假的故事的检测，以减少专业验证者的手动努力并为防止谣言的传播做出贡献。</li>
<li>methods: 本研究使用了一个新的数据集，该数据集包含了用于验证的检查答案和各种语言的社交媒体帖子，以便进行跨语言验证答案已经证明为假的故事的检测。</li>
<li>results: 研究发现，跨语言验证答案已经证明为假的故事的检测是一项具有挑战性的任务，而一些常用的跨语言预处理 transformer 模型也未能超越一个强的基于词语的基线（BM25）。然而，我们的多Stage检索框架在大多数情况下能够超越 BM25，并具有跨频率和零扩展学习的能力。<details>
<summary>Abstract</summary>
The task of retrieving already debunked narratives aims to detect stories that have already been fact-checked. The successful detection of claims that have already been debunked not only reduces the manual efforts of professional fact-checkers but can also contribute to slowing the spread of misinformation. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual task, i.e. the retrieval of fact-checking articles in a language different from the language of the online post being checked. This paper fills this gap by (i) creating a novel dataset to enable research on cross-lingual retrieval of already debunked narratives, using tweets as queries to a database of fact-checking articles; (ii) presenting an extensive experiment to benchmark fine-tuned and off-the-shelf multilingual pre-trained Transformer models for this task; and (iii) proposing a novel multistage framework that divides this cross-lingual debunk retrieval task into refinement and re-ranking stages. Results show that the task of cross-lingual retrieval of already debunked narratives is challenging and off-the-shelf Transformer models fail to outperform a strong lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework is robust, outperforming BM25 in most scenarios and enabling cross-domain and zero-shot learning, without significantly harming the model's performance.
</details>
<details>
<summary>摘要</summary>
这个任务是检索已经证伪的故事，目的是检测已经被ifact-checked的故事。成功检测已经证伪的故事不仅可以减少专业ifact-checker的手动努力，还可以减速谣言的传播。但因为数据不足，这个问题尚未得到充分研究，特别是跨语言任务，即在不同语言的 онлайн帖子被检查时，检索ifact-checking文章的跨语言任务。这篇论文填补这一漏洞，通过以下三个方法：1. 创建一个新的数据集，用于启动研究跨语言检索已经证伪的故事。2. 进行了广泛的实验，以benchmark fine-tuned和off-the-shelf多语言预训练Transformer模型。3. 提出了一个新的多阶段框架，将跨语言检索已经证伪的故事任务分为两个阶段：精度阶段和重新排序阶段。结果表明，跨语言检索已经证伪的故事是一个具有挑战性的任务，off-the-shelf Transformer模型无法超过一个强的基于词语的基准值（BM25）。然而，我们的多阶段检索框架具有坚固性，在大多数情况下超过BM25，并且允许跨频域和零shot学习，无需对模型性能产生重要的影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.LG_2023_08_11/" data-id="clly4xtdu006lvl88c7159jj2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.SD_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/cs.SD_2023_08_11/">cs.SD - 2023-08-11 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-Joint-Speech-Text-Representations-Without-Alignment"><a href="#Improving-Joint-Speech-Text-Representations-Without-Alignment" class="headerlink" title="Improving Joint Speech-Text Representations Without Alignment"></a>Improving Joint Speech-Text Representations Without Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06125">http://arxiv.org/abs/2308.06125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenberg, Tara N. Sainath, Michael Picheny, Kyunghyun Cho</li>
<li>for: 这个论文旨在提出一种基于modal space的文本生成方法，用于处理 speech和text两种不同的模式。</li>
<li>methods: 该方法使用joint speech-text encoder，通过在modal space中对文本和语音进行共同表示，以减少模型的参数量。</li>
<li>results: 该方法可以自动解决模式长度不同的问题，并且在下游WER测试中显示了良好的表现，包括单语言和多语言系统。<details>
<summary>Abstract</summary>
The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.
</details>
<details>
<summary>摘要</summary>
最近一年内，文本承词生成技术呈现了宏まScale的进步，基于跨Modal表示空间的想法，在这个空间中，文本和图像领域都被合并表示。在ASR中，这个想法得到应用，通过将语音和文本域合并编码，可以让模型 Parameters scale 到非常大的规模。虽然这些方法显示了承诺，但它们需要特殊地处理语音和文本序列长度的差异，通过上映或者显式对齐模型。在这种工作中，我们提供证据，表明Join speech-text编码器可以自然地实现多Modal的一致表示，而不需要注意序列长度。我们还 argues that consistency损失可以宽容序列长度差异，并且可以假设最佳对齐。我们示出，这种损失可以提高下游 WER 在大参数 monolingual 和 multilingual 系统中。
</details></li>
</ul>
<hr>
<h2 id="Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping"><a href="#Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping" class="headerlink" title="Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping"></a>Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06112">http://arxiv.org/abs/2308.06112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Haithem Boussaid, Ebtessam Almazrouei, Merouane Debbah</li>
<li>For: 这个论文的目的是提出一种简单的方法，以便在视频序列中进行舌头语音识别（VSR）任务。这种方法可以在训练集外的挑战性enario中表现出色，而不需要大量的标注数据。* Methods: 这个论文使用了一种名为Lip2Vec的简单方法，该方法基于学习一个先验模型。该网络将视频序列中的舌头编码后的干扰表示与其对应的音频对的干扰表示进行映射。然后，使用一个市场上的音频识别模型来解码音频，并将其转化为文本。* Results: 根据LRS3数据集的测试结果，这种方法可以与完全监督学习方法相比，达到26个WER的水平。与State-of-the-Art（SoTA）方法不同，我们的模型在VoxCeleb测试集上保持了合理的性能。<details>
<summary>Abstract</summary>
Visual Speech Recognition (VSR) differs from the common perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the recent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degeneration under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed model compares favorably with fully-supervised learning methods on the LRS3 dataset achieving 26 WER. Unlike SoTA approaches, our model keeps a reasonable performance on the VoxCeleb test set. We believe that reprogramming the VSR as an ASR task narrows the performance gap between the two and paves the way for more flexible formulations of lip reading.
</details>
<details>
<summary>摘要</summary>
视觉语音识别（VSR）与常见的观察任务不同，因为它需要对视频序列进行更深入的理解，即使人类专家也需要这样做。尽管最近有大量的进步在VSR方面，但现在的方法仍然依赖于标注数据来完全训练或微调其模型，以预测目标语音。这会导致其在不同于训练集的情况下表现不佳，并且会导致性能下降。与之前的工作不同，我们提出了一种简单的方法，即Lip2Vec，它基于学习一个先验模型。给定一个强大的视觉语音编码器，这个网络将编码的舌唇序列的 latent 表示与它们对应的 audio 对应的 latent 表示进行映射，这些表示够具有效果的文本解码。生成的音频表示然后被解码成文本使用一个可用的 Audio Speech Recognition（ASR）模型。我们提出的模型与完全监督学习方法在 LRS3 数据集上比较 favorably，实现 26 WER。与 SoTA 方法不同，我们的模型在 VoxCeleb 测试集上保持了合理的性能。我们认为将 VSR 转换为 ASR 任务，将两者之间的性能差减少，并且开创了更 flexible 的舌唇读取形式。
</details></li>
</ul>
<hr>
<h2 id="An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition"><a href="#An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition" class="headerlink" title="An Autoethnographic Exploration of XAI in Algorithmic Composition"></a>An Autoethnographic Exploration of XAI in Algorithmic Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06089">http://arxiv.org/abs/2308.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashley Noel-Hirst, Nick Bryan-Kinns</li>
<li>for: 这篇论文旨在探讨如何使用可解释的人工智能（XAI）生成模型来创作传统音乐。</li>
<li>methods: 本研究使用MeasureVAE生成模型，该模型具有可解释的秘密维度，并在爱尔兰传统音乐上进行训练。</li>
<li>results: 研究发现，在音乐创作过程中，探索性的音乐创作 workflow 会强调音乐训练数据中的音乐特征，而不是生成模型本身的特征。这种应用XAI模型在创作过程中的可能性可能会扩展到更复杂和多样化的工作流程。<details>
<summary>Abstract</summary>
Machine Learning models are capable of generating complex music across a range of genres from folk to classical music. However, current generative music AI models are typically difficult to understand and control in meaningful ways. Whilst research has started to explore how explainable AI (XAI) generative models might be created for music, no generative XAI models have been studied in music making practice. This paper introduces an autoethnographic study of the use of the MeasureVAE generative music XAI model with interpretable latent dimensions trained on Irish folk music. Findings suggest that the exploratory nature of the music-making workflow foregrounds musical features of the training dataset rather than features of the generative model itself. The appropriation of an XAI model within an iterative workflow highlights the potential of XAI models to form part of a richer and more complex workflow than they were initially designed for.
</details>
<details>
<summary>摘要</summary>
machine learning模型可以生成复杂的音乐，从民族音乐到古典音乐。但当前的生成音乐AI模型通常难以理解和控制有意义的方式。研究已经开始探索如何创建音乐XAI生成模型，但没有任何生成XAI模型在音乐创作实践中被研究。本文介绍了一个自传式研究，使用MeasureVAE生成音乐XAI模型，具有可解释的幂等维度，在爱尔兰传统音乐上进行训练。发现结果表明，音乐创作工作流程的探索性强调了训练集音乐特征而不是生成模型本身的特征。将XAI模型包含在迭代工作流程中，表明XAI模型可以成为更加丰富和复杂的工作流程的一部分。
</details></li>
</ul>
<hr>
<h2 id="Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model"><a href="#Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model" class="headerlink" title="Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model"></a>Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05995">http://arxiv.org/abs/2308.05995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang, Naye Ji, Fuxing Gao, Siyuan Zhao, Zhaohan Wang, Shunman Li<br>for:* 这篇论文旨在创造数字人类的合作语言姿势，以解决现有的挑战，包括复杂的语音、语义和人性等因素。methods:* 该论文提出了一种基于扩散的DiffMotion-v2模型，利用RawSpeech音频 directly生成个性化和风格化的全身语姿，不需要复杂的多Modal处理和手动标注。results:* 经验证明，DiffMotion-v2模型可以生成自然的语姿，并且可以适应不同的风格和人性特征。<details>
<summary>Abstract</summary>
The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acoustic and semantic features but also conveys personality traits, emotions, and more subtle information related to accompanying gestures, we pioneer the adaptation of WavLM, a large-scale pre-trained model, to extract low-level and high-level audio information. Secondly, we introduce an adaptive layer norm architecture in the transformer-based layer to learn the relationship between speech information and accompanying gestures. Extensive subjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT datasets to confirm the WavLM and the model's ability to synthesize natural co-speech gestures with various styles.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的虚拟人物创造领域中的协调姿势生成技术是一个emerging领域。先前的研究使用了音响和语义信息作为输入，采用分类方法来确定人的ID和情绪，以驱动协调姿势生成。然而，这一领域仍面临着重大挑战。这些挑战不仅包括协调姿势、音响和语义之间的细微互动，还包括人性、情绪和其他一些重要而不那么明确的因素。本文介绍了“diffmotion-v2”，一种基于 transformer 架构的 speech-conditional 协同扩散型生成模型，使用 WavLM 预训练模型。该模型可以通过 Raw speech 音频alone 生成具有个性化和风格化特点的全身协调姿势，无需进行复杂的多Modal 处理和手动标注。首先，我们认为 speech 音频不仅包含了音响和语义特征，还拥有人性特征、情绪特征和更为细微的协调姿势相关信息。因此，我们采用 WavLM 预训练模型来提取低级和高级 audio 信息。其次，我们引入了 transformer 架构中的 adaptive layer norm 层，以学习 speech 信息和协调姿势之间的关系。我们对 Trinity、ZEGGS 和 BEAT 等三个 dataset 进行了许多主观评估实验，以确认 WavLM 和模型的能力以生成自然的协调姿势。
</details></li>
</ul>
<hr>
<h2 id="Advancing-the-study-of-Large-Scale-Learning-in-Overlapped-Speech-Detection"><a href="#Advancing-the-study-of-Large-Scale-Learning-in-Overlapped-Speech-Detection" class="headerlink" title="Advancing the study of Large-Scale Learning in Overlapped Speech Detection"></a>Advancing the study of Large-Scale Learning in Overlapped Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05987">http://arxiv.org/abs/2308.05987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaohui Yin, Jingguang Tian, Xinhui Hu, Xinkang Xu</li>
<li>for: 多个党人对话分析中的干扰语音检测（OSD）是一个重要的应用领域，但现有的大多数OSD模型都是基于特定的数据集进行训练和评估，限制了这些模型的应用场景。</li>
<li>methods: 我们提出了大规模学习（LSL）在OSD任务中的应用，并设计了一种16K单annelOSD模型。我们使用了522小时不同语言和风格的标注音频作为大规模数据集，并进行了严格的比较实验来评估LSL在OSD任务中的效果和不同深度神经网络基于OSD模型的性能。</li>
<li>results: 我们的实验结果表明，LSL可以显著提高OSD模型的性能和鲁棒性，并且CF-OSD与LSL在Alimeeting测试集和DIHARD II评估集上的F1分数分别达到了80.8%和52.0%，创造了当前最佳的16K单annelOSD模型。<details>
<summary>Abstract</summary>
Overlapped Speech Detection (OSD) is an important part of speech applications involving analysis of multi-party conversations. However, Most of the existing OSD models are trained and evaluated on specific dataset, which limits the application scenarios of these models. In order to solve this problem, we conduct a study of large-scale learning (LSL) in OSD and propose a more general 16K single-channel OSD model. In our study, 522 hours of labeled audio in different languages and styles are collected and used as the large-scale dataset. Rigorous comparative experiments are designed and used to evaluate the effectiveness of LSL in OSD task and the performance of OSD models based on different deep neural networks. The results show that LSL can significantly improve the performance and robustness of OSD models, and the OSD model based on Conformer (CF-OSD) with LSL is currently the best 16K single-channel OSD model. Moreover, the CF-OSD with LSL establishes a state-of-the-art performance with a F1-score of 80.8% and 52.0% on the Alimeeting test set and DIHARD II evaluation set, respectively.
</details>
<details>
<summary>摘要</summary>
大量学习（LSL）在对话分析中的另 overlap speech detection（OSD）是一个重要的部分，但大多数现有的OSD模型都是基于特定的数据集进行训练和评估，这限制了这些模型的应用场景。为解决这个问题，我们在OSD领域进行了大规模学习的研究，并提出了一个16K单annelOSD模型。在我们的研究中，我们收集了522小时的不同语言和风格的标注音频数据，并使用这些数据作为大规模数据集进行训练和测试。我们设计了严格的比较实验，以评估LSL在OSD任务中的效果和不同深度神经网络基于的OSD模型的性能。结果显示LSL可以显著提高OSD模型的性能和可靠性，并且CF-OSD WITH LSL目前是最佳的16K单annelOSD模型。此外，CF-OSD WITH LSL在Alimeeting测试集和DIHARD II评估集上的F1分数分别为80.8%和52.0%，创造了当前最佳的州态。
</details></li>
</ul>
<hr>
<h2 id="AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining"><a href="#AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining" class="headerlink" title="AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining"></a>AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05734">http://arxiv.org/abs/2308.05734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoheliu/AudioLDM2">https://github.com/haoheliu/AudioLDM2</a></li>
<li>paper_authors: Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley</li>
<li>for: 这 paper 的目的是提出一种框架，用于将 speech、music 和 sound effect 等不同类型的声音生成模型共同拟合。</li>
<li>methods: 该框架使用同一种学习方法，通过 AudioMAE 自然适应学习模型和 latent diffusion 模型来翻译不同类型的声音，并在生成过程中进行自我监督学习。</li>
<li>results: 经验表明，该框架可以在主要的 benchmark 上达到新的 state-of-the-art 或竞争性的性能，并且具有很好的培化学习能力和可重用的自然适应学习模型。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate new state-of-the-art or competitive performance to previous approaches. Our demo and code are available at https://audioldm.github.io/audioldm2.
</details>
<details>
<summary>摘要</summary>
尽管各种听音都有共同之处，如speech、音乐和声音效果，但设计模型时需要仔细考虑每种类型的特定目标和偏见，这些偏见可能与其他类型异常大。为了带领我们更近到一个统一的听音生成视角，这篇论文提出了一个框架，该框架利用同一种学习方法来生成speech、音乐和声音效果。我们的框架引入了一个通用的听音表示，称为语言听音（LOA）。任何听音都可以根据AudioMAE自我超vised学习表示学习模型翻译为LOA。在生成过程中，我们使用GPT-2模型将任何Modalities翻译为LOA，然后使用一个conditional on LOA的隐藏噪声模型进行自我超vised听音生成学习。我们的提议的框架自然带来了在上下文学习能力和可重用的自我超vised Pre-trained AudioMAE和隐藏噪声模型的优点。我们的实验在主要的文本到听音、文本到音乐和文本到语音的标准 benchmarcks 上达到了新的状态码或竞争性的性能。我们的 demo 和代码可以在https://audioldm.github.io/audioldm2 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.SD_2023_08_11/" data-id="clly4xteq00a0vl88hha69xpo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/eess.IV_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/eess.IV_2023_08_11/">eess.IV - 2023-08-11 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Packaging-Unit-Detection-for-Automated-Palletizing-Tasks"><a href="#Towards-Packaging-Unit-Detection-for-Automated-Palletizing-Tasks" class="headerlink" title="Towards Packaging Unit Detection for Automated Palletizing Tasks"></a>Towards Packaging Unit Detection for Automated Palletizing Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06306">http://arxiv.org/abs/2308.06306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Völk, Kilian Kleeberger, Werner Kraus, Richard Bormann</li>
<li>for: 本研究旨在解决工业机器人处理包装单位前的检测步骤，即检测包装单位的具体步骤。</li>
<li>methods: 本研究使用synthetically生成的数据进行完全训练，可以对实际世界的包装单位进行Robust应用而无需进一步训练或设置繁重。该方法可以处理稀疏和低质量的感知数据，可以利用可用的先验知识，并在各种产品和应用场景中广泛适用。</li>
<li>results: 我们对实际世界数据进行了广泛评估，证明了我们的方法可以对各种不同的零售产品进行实际应用。此外，我们还将该方法integrated into a lab demonstrator，并通过工业伙伴将其商业化。<details>
<summary>Abstract</summary>
For various automated palletizing tasks, the detection of packaging units is a crucial step preceding the actual handling of the packaging units by an industrial robot. We propose an approach to this challenging problem that is fully trained on synthetically generated data and can be robustly applied to arbitrary real world packaging units without further training or setup effort. The proposed approach is able to handle sparse and low quality sensor data, can exploit prior knowledge if available and generalizes well to a wide range of products and application scenarios. To demonstrate the practical use of our approach, we conduct an extensive evaluation on real-world data with a wide range of different retail products. Further, we integrated our approach in a lab demonstrator and a commercial solution will be marketed through an industrial partner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Self-supervised-SAR-Image-Despeckling-Strategy-Based-on-Parameter-sharing-Convolutional-Neural-Networks"><a href="#A-Self-supervised-SAR-Image-Despeckling-Strategy-Based-on-Parameter-sharing-Convolutional-Neural-Networks" class="headerlink" title="A Self-supervised SAR Image Despeckling Strategy Based on Parameter-sharing Convolutional Neural Networks"></a>A Self-supervised SAR Image Despeckling Strategy Based on Parameter-sharing Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05975">http://arxiv.org/abs/2308.05975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Chen, Yifei Yin, Hao Shi, Qingqing Sheng, Wei Li<br>for: The paper is written to address the challenges of SAR image despeckling, specifically the lack of speckle-free SAR images for training deep learning models.methods: The paper proposes a self-supervised SAR despeckling strategy that uses a sub-sampler based on the adjacent-syntropy criteria to generate training image pairs from real-world SAR images. The proposed method also uses parameter sharing convolutional neural networks and a multi-feature loss function to improve the preservation of edges and textures in the despeckled images.results: The proposed method is validated on real-world SAR images and shows better performance than several advanced SAR image despeckling methods, with improved preservation of edges and textures.Here’s the simplified Chinese text for the three information points:for: 本文是为了解决 SAR 图像抑杂问题，具体来说是为了缺乏 SAR 图像的杂点自由样本来训练深度学习模型。methods: 本文提出了一种基于邻域同异性 criteria 的自我监督 SAR 抑杂策略，使用这种策略可以从实际 SAR 图像中生成训练对数据。同时，本方法使用共享参数 convolutional neural networks 和多个特征损失函数，以提高杂点自由样本中的边缘和Texture特征保持。results: 提出的方法在实际 SAR 图像上进行了证明，与一些先进的 SAR 图像抑杂方法相比，显示更好的表现，同时保持了边缘和Texture特征。<details>
<summary>Abstract</summary>
Speckle noise is generated due to the SAR imaging mechanism, which brings difficulties in SAR image interpretation. Hence, despeckling is a helpful step in SAR pre-processing. Nowadays, deep learning has been proved to be a progressive method for SAR image despeckling. Most deep learning methods for despeckling are based on supervised learning, which needs original SAR images and speckle-free SAR images to train the network. However, the speckle-free SAR images are generally not available. So, this issue was tackled by adding multiplicative noise to optical images synthetically for simulating speckled image. Therefore, there are following challenges in SAR image despeckling: (1) lack of speckle-free SAR image; (2) difficulty in keeping details such as edges and textures in heterogeneous areas. To address these issues, we propose a self-supervised SAR despeckling strategy that can be trained without speckle-free images. Firstly, the feasibility of SAR image despeckling without speckle-free images is proved theoretically. Then, the sub-sampler based on the adjacent-syntropy criteria is proposed. The training image pairs are generated by the sub-sampler from real-word SAR image to estimate the noise distribution. Furthermore, to make full use of training pairs, the parameter sharing convolutional neural networks are adopted. Finally, according to the characteristics of SAR images, a multi-feature loss function is proposed. The proposed loss function is composed of despeckling term, regular term and perception term, to constrain the gap between the generated paired images. The ability of edge and texture feature preserving is improved simultaneously. Finally, qualitative and quantitative experiments are validated on real-world SAR images, showing better performances than several advanced SAR image despeckling methods.
</details>
<details>
<summary>摘要</summary>
亮点噪声是由SAR成像机制产生的，导致SAR图像解读困难。因此，去掉亮点噪声是SAR预处理的有助步骤。目前，深度学习已经被证明是SAR图像去掉亮点噪声的进步方法。大多数深度学习方法是基于直接学习，需要原始SAR图像和噪声自由SAR图像来训练网络。然而，噪声自由SAR图像通常不可获得。因此，这个问题是通过人工添加光学图像中的multiplicative噪声来解决的。因此，SAR图像去掉亮点噪声面临以下挑战：（1）缺乏噪声自由SAR图像；（2）在不同区域中保持细节，如边缘和文本ure。为解决这些问题，我们提出了一种自动化SAR图像去掉亮点噪声策略，不需要噪声自由SAR图像。首先，我们证明了SAR图像去掉亮点噪声可行性。然后，我们提出了基于邻域合理性 criteria的子批量器。通过这个子批量器，从实际世界SAR图像中生成训练图像对。然后，为了充分利用训练对，我们采用了参数共享卷积神经网络。最后，根据SAR图像的特点，我们提出了多特征损失函数。该损失函数由去掉亮点噪声项、常规项和感知项组成，以避免训练对的差异。同时，我们改进了边缘和文本ure的特征保持能力。最后，我们对实际世界SAR图像进行了质量和kvantalitative实验，并证明了我们的方法在去掉亮点噪声方面的更好性。
</details></li>
</ul>
<hr>
<h2 id="Classification-Method-of-Road-Surface-Condition-and-Type-with-LiDAR-Using-Spatiotemporal-Information"><a href="#Classification-Method-of-Road-Surface-Condition-and-Type-with-LiDAR-Using-Spatiotemporal-Information" class="headerlink" title="Classification Method of Road Surface Condition and Type with LiDAR Using Spatiotemporal Information"></a>Classification Method of Road Surface Condition and Type with LiDAR Using Spatiotemporal Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05965">http://arxiv.org/abs/2308.05965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju Won Seo, Jin Sung Kim, Chung Choo Chung</li>
<li>for: 这个论文提出了一种基于LiDAR的道路表面条件和类型分类方法，用于实时识别道路表面的状况和类型。</li>
<li>methods: 该方法使用了深度神经网络（DNN）来分类道路表面的不同区域，首先构建了每个子区域的特征向量，然后使用DNN进行分类。最后，输出的DNN结果通过空间时间处理得到了最终的分类结果，考虑了车速和概率等因素。</li>
<li>results: 与其他五种算法进行比较研究后，该方法在两个子区域附近车速的情况下获得了最高的准确率为98.0%和98.6%。此外，该方法还在实时环境中验证了可行性。<details>
<summary>Abstract</summary>
This paper proposes a spatiotemporal architecture with a deep neural network (DNN) for road surface conditions and types classification using LiDAR. It is known that LiDAR provides information on the reflectivity and number of point clouds depending on a road surface. Thus, this paper utilizes the information to classify the road surface. We divided the front road area into four subregions. First, we constructed feature vectors using each subregion's reflectivity, number of point clouds, and in-vehicle information. Second, the DNN classifies road surface conditions and types for each subregion. Finally, the output of the DNN feeds into the spatiotemporal process to make the final classification reflecting vehicle speed and probability given by the outcomes of softmax functions of the DNN output layer. To validate the effectiveness of the proposed method, we performed a comparative study with five other algorithms. With the proposed DNN, we obtained the highest accuracy of 98.0\% and 98.6\% for two subregions near the vehicle. In addition, we implemented the proposed method on the Jetson TX2 board to confirm that it is applicable in real-time.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文提出了一种使用LiDAR的深度神经网络（DNN）来分类道路表面条件和类型。LiDAR提供了道路表面反射率和点云数据的信息，因此这篇论文利用这些信息来分类道路表面。前方道路区分成四个子区域，并构建了每个子区域的特征向量，包括反射率、点云数量和车辆内部信息。然后，DNN将每个子区域的特征向量分类为道路表面条件和类型。最后，DNN的输出Feed into spatiotemporal processto make the final classification, considering the vehicle speed and probability output by the softmax function of the DNN output layer. To validate the effectiveness of the proposed method, a comparative study with five other algorithms was conducted, and the DNN achieved the highest accuracy of 98.0% and 98.6% for two subregions near the vehicle. In addition, the proposed method was implemented on the Jetson TX2 board to confirm its applicability in real-time.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Strengths-of-Unlabeled-Data-in-Pan-cancer-Abdominal-Organ-Quantification-the-FLARE22-Challenge"><a href="#Unleashing-the-Strengths-of-Unlabeled-Data-in-Pan-cancer-Abdominal-Organ-Quantification-the-FLARE22-Challenge" class="headerlink" title="Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge"></a>Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05862">http://arxiv.org/abs/2308.05862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junma11/flare">https://github.com/junma11/flare</a></li>
<li>paper_authors: Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Ma, Adamo Young, Cheng Zhu, Kangkang Meng, Xin Yang, Ziyan Huang, Fan Zhang, Wentao Liu, YuanKe Pan, Shoujin Huang, Jiacheng Wang, Mingze Sun, Weixin Xu, Dengqiang Jia, Jae Won Choi, Natália Alves, Bram de Wilde, Gregor Koehler, Yajun Wu, Manuel Wiesenfarth, Qiongjie Zhu, Guoqiang Dong, Jian He, the FLARE Challenge Consortium, Bo Wang</li>
<li>for:  This paper aims to evaluate the accuracy and efficiency of artificial intelligence (AI) algorithms in automated abdominal disease diagnosis and treatment planning.</li>
<li>methods:  The authors organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. They constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers.</li>
<li>results:  The best-performing algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0% on a holdout external validation set, and successfully generalized to different cohorts from North America, Europe, and Asia. They also enabled automatic extraction of key organ biology features, which was labor-intensive with traditional manual measurements.<details>
<summary>Abstract</summary>
Quantitative organ assessment is an essential step in automated abdominal disease diagnosis and treatment planning. Artificial intelligence (AI) has shown great potential to automatize this process. However, most existing AI algorithms rely on many expert annotations and lack a comprehensive evaluation of accuracy and efficiency in real-world multinational settings. To overcome these limitations, we organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. We constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers. We independently validated that a set of AI algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0\% by using 50 labeled scans and 2000 unlabeled scans, which can significantly reduce annotation requirements. The best-performing algorithms successfully generalized to holdout external validation sets, achieving a median DSC of 89.5\%, 90.9\%, and 88.3\% on North American, European, and Asian cohorts, respectively. They also enabled automatic extraction of key organ biology features, which was labor-intensive with traditional manual measurements. This opens the potential to use unlabeled data to boost performance and alleviate annotation shortages for modern AI models.
</details>
<details>
<summary>摘要</summary>
“量化器官评估是自动化腹部疾病诊断和治疗规划的关键步骤。人工智能（AI）已经表现出很大的潜力来自动化这个过程。然而，现有的大多数AI算法都需要许多专家标注，并且缺乏真实世界多国场景下的全面评估精度和效率。为了解决这些限制，我们在FLARE 2022挑战中组织了最大的腹部器官分析挑战，以测试快速、低资源、准确、标注效率高和泛化AI算法。我们构建了跨大陆和多国的数据集，包括不同的种族、疾病、阶段和生产商的计算Tomography（CT）扫描。我们独立验证了一组AI算法在50个医疗机构的数据上实现了 médiane的 dice相似度系数（DSC）90.0%，使用50个标注扫描和2000个无标注扫描。这些算法可以减少标注需求。最佳算法成功泛化到外部验证集上，实现了 médiane的 DSC为89.5%、90.9%和88.3%在北美、欧洲和亚洲相应的协会上。它们还允许自动提取关键器官生物特征，这是传统的手动测量强度劳紧的。这开启了使用无标注数据来提高性能的可能性，并alleviate标注不足的问题 для现代AI模型。”
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Optimization-of-JPEG-Based-Deep-Learning-Process-for-Image-Classification"><a href="#End-to-End-Optimization-of-JPEG-Based-Deep-Learning-Process-for-Image-Classification" class="headerlink" title="End-to-End Optimization of JPEG-Based Deep Learning Process for Image Classification"></a>End-to-End Optimization of JPEG-Based Deep Learning Process for Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05840">http://arxiv.org/abs/2308.05840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Qi, Lahiru D. Chamain, Zhi Ding</li>
<li>for: 这篇论文应用于图像分类任务中的分布式学习，需要有效地将图像压缩成码码装置在低成本的感知设备上，以便优化传输和储存。</li>
<li>methods: 本研究提出了一个统一的端到端训练模型，结合了JPEG图像压缩器和深度学习（DL）的分类器。这个模型可以调整通过大量部署的JPEG编码器设置，以提高分类精度，同时考虑到带宽限制。</li>
<li>results: 我们在CIFAR-100和ImageNet上进行了测试，结果显示，这个模型可以提高验证精度，比于预先设定的JPEG配置。<details>
<summary>Abstract</summary>
Among major deep learning (DL) applications, distributed learning involving image classification require effective image compression codecs deployed on low-cost sensing devices for efficient transmission and storage. Traditional codecs such as JPEG designed for perceptual quality are not configured for DL tasks. This work introduces an integrative end-to-end trainable model for image compression and classification consisting of a JPEG image codec and a DL-based classifier. We demonstrate how this model can optimize the widely deployed JPEG codec settings to improve classification accuracy in consideration of bandwidth constraint. Our tests on CIFAR-100 and ImageNet also demonstrate improved validation accuracy over preset JPEG configuration.
</details>
<details>
<summary>摘要</summary>
中文简体深度学习（DL）应用中的分布式学习，涉及到图像分类，需要有效的图像压缩编码器在低成本感知设备上部署，以提高传输和存储的效率。传统的编码器如JPEG，设计为 perceive 质量，并不适用于 DL 任务。这项工作介绍了一种综合性的末端到终端可调模型，包括 JPEG 图像编码器和基于 DL 的分类器。我们示出了如何这种模型可以优化广泛部署的 JPEG 编码器设置，以提高分类精度，同时考虑带宽约束。我们在 CIFAR-100 和 ImageNet 上进行了测试，并证明了该模型在预设 JPEG 配置下有更高的验证精度。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Pathomics-Toolkit-for-Quantitative-Analysis-of-Podocyte-Nuclei-with-Histology-and-Spatial-Transcriptomics-Data-in-Renal-Pathology"><a href="#Spatial-Pathomics-Toolkit-for-Quantitative-Analysis-of-Podocyte-Nuclei-with-Histology-and-Spatial-Transcriptomics-Data-in-Renal-Pathology" class="headerlink" title="Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology"></a>Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06288">http://arxiv.org/abs/2308.06288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/spatial_pathomics">https://github.com/hrlblab/spatial_pathomics</a></li>
<li>paper_authors: Jiayuan Chen, Yu Wang, Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Yilin Liu, Jianyong Zhong, Agnes B. Fogo, Haichun Yang, Shilin Zhao, Yuankai Huo<br>for:这篇论文的目的是为了开发一个用于评估肾脏病变的新型工具kit，帮助研究人员更好地评估肾脏细胞的形态特征。methods:这篇论文使用的方法包括：一、实例对象分割，帮助精准地 identific podocyte核oid; 二、pathomics特征生成，从分割后的核oid中提取了一系列量化特征; 三、robust统计分析，以便对这些量化特征进行全面的空间分析。results:这篇论文通过使用SPT工具kit，成功地提取和分析了许多podocyte形态特征，并通过统计分析发现了许多与肾脏病变相关的空间特征。<details>
<summary>Abstract</summary>
Podocytes, specialized epithelial cells that envelop the glomerular capillaries, play a pivotal role in maintaining renal health. The current description and quantification of features on pathology slides are limited, prompting the need for innovative solutions to comprehensively assess diverse phenotypic attributes within Whole Slide Images (WSIs). In particular, understanding the morphological characteristics of podocytes, terminally differentiated glomerular epithelial cells, is crucial for studying glomerular injury. This paper introduces the Spatial Pathomics Toolkit (SPT) and applies it to podocyte pathomics. The SPT consists of three main components: (1) instance object segmentation, enabling precise identification of podocyte nuclei; (2) pathomics feature generation, extracting a comprehensive array of quantitative features from the identified nuclei; and (3) robust statistical analyses, facilitating a comprehensive exploration of spatial relationships between morphological and spatial transcriptomics features.The SPT successfully extracted and analyzed morphological and textural features from podocyte nuclei, revealing a multitude of podocyte morphomic features through statistical analysis. Additionally, we demonstrated the SPT's ability to unravel spatial information inherent to podocyte distribution, shedding light on spatial patterns associated with glomerular injury. By disseminating the SPT, our goal is to provide the research community with a powerful and user-friendly resource that advances cellular spatial pathomics in renal pathology. The implementation and its complete source code of the toolkit are made openly accessible at https://github.com/hrlblab/spatial_pathomics.
</details>
<details>
<summary>摘要</summary>
PODO细胞，特殊化的胶质细胞，环绕肾脏小血管，对肾健康具有重要作用。现有的描述和量化方法有限，需要创新的解决方案来全面评估多样性特征。特别是理解PODO细胞的形态特征，对研究肾脏伤害非常重要。本文介绍了SPT工具箱（Spatial Pathomics Toolkit），并应用于PODO细胞pathomics。SPT包括三个主要组成部分：（1）实体对象分割，准确地识别PODO细胞核心；（2）pathomics特征生成，从分割的PODO细胞核心中提取广泛的量化特征；以及（3）Robust统计分析，促进了广泛的空间关系探索。SPT成功地提取和分析PODO细胞形态和文化特征，揭示了许多PODO细胞形态特征，并通过统计分析，探索了PODO细胞分布的空间信息。我们的目标是通过散布SPT，为研究社区提供一个强大且易用的资源，以提高细胞空间patomics在肾 pathology中的应用。SPT的实现和完整的源代码在https://github.com/hrlblab/spatial_pathomics上公开访问。
</details></li>
</ul>
<hr>
<h2 id="Leverage-Weakly-Annotation-to-Pixel-wise-Annotation-via-Zero-shot-Segment-Anything-Model-for-Molecular-empowered-Learning"><a href="#Leverage-Weakly-Annotation-to-Pixel-wise-Annotation-via-Zero-shot-Segment-Anything-Model-for-Molecular-empowered-Learning" class="headerlink" title="Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning"></a>Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05785">http://arxiv.org/abs/2308.05785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueyuan Li, Ruining Deng, Yucheng Tang, Shunxing Bao, Haichun Yang, Yuankai Huo</li>
<li>for: 这个研究的目的是为了开发一个能够实现多种细胞类型的识别，并且可以避免专家 annotators 的时间负担的方法。</li>
<li>methods: 这个研究使用了一种名为 Segment Anything Model (SAM) 的新型模型，它可以将单一的矩形标注转换为多个精确的像素标注。这些 SAM-generated 标注可以用于训练一个深度学习模型，以进行细胞类型的分类。</li>
<li>results: 研究发现，使用 SAM-assisted molecular-empowered learning (SAM-L) 可以将这些细胞类型的标注工作从专家 annotators 转移到非专家 annotators 身上，而且不需要专家 annotators 的时间负担。此外，SAM-L 可以保持 annotation accuracy 和深度学习模型的性能。这个研究代表了一个重要的进步，可以帮助普及化细胞类型的标注过程，并且仅需非专家 annotators 的帮助。<details>
<summary>Abstract</summary>
Precise identification of multiple cell classes in high-resolution Giga-pixel whole slide imaging (WSI) is critical for various clinical scenarios. Building an AI model for this purpose typically requires pixel-level annotations, which are often unscalable and must be done by skilled domain experts (e.g., pathologists). However, these annotations can be prone to errors, especially when distinguishing between intricate cell types (e.g., podocytes and mesangial cells) using only visual inspection. Interestingly, a recent study showed that lay annotators, when using extra immunofluorescence (IF) images for reference (referred to as molecular-empowered learning), can sometimes outperform domain experts in labeling. Despite this, the resource-intensive task of manual delineation remains a necessity during the annotation process. In this paper, we explore the potential of bypassing pixel-level delineation by employing the recent segment anything model (SAM) on weak box annotation in a zero-shot learning approach. Specifically, we harness SAM's ability to produce pixel-level annotations from box annotations and utilize these SAM-generated labels to train a segmentation model. Our findings show that the proposed SAM-assisted molecular-empowered learning (SAM-L) can diminish the labeling efforts for lay annotators by only requiring weak box annotations. This is achieved without compromising annotation accuracy or the performance of the deep learning-based segmentation. This research represents a significant advancement in democratizing the annotation process for training pathological image segmentation, relying solely on non-expert annotators.
</details>
<details>
<summary>摘要</summary>
高分辨率整幕扫描图像（WSI）中多个细胞类型的精确识别是许多临床应用场景中的关键。建立一个人工智能模型用于此目的通常需要像素级别的注释，但这些注释通常是不可扩展的，而且需要具有专业知识的域专家（例如病理学家）进行完成。然而，这些注释可能会受到误差的影响，特别是在使用 только视觉检查时分辨between intricate cell types（例如 podocytes 和 mesangial cells）。有趣的是，一项最近的研究发现，使用extra immunofluorescence（IF）图像作为参考（被称为分子力学学习），lay annotators可以在标注时与域专家相比而出现较好的表现。尽管这样，手动分割任务仍然是注释过程中的必需任务。在这篇文章中，我们探讨了可以通过跳过像素级别分割来使用最近的segment anything模型（SAM）在零扩学习方法中。我们利用SAM的能力生成像素级别标注从box标注，并使用这些SAM生成的标注来训练分 segmentation模型。我们的发现表明，提案的SAM-assisted molecular-empowered learning（SAM-L）可以使lay annotators只需要弱box标注，而无需投入大量的标注时间和努力。这是在不损失标注准确性或深度学习基于分 segmentation模型的性能下实现的。这种研究表明了让非专业注释者参与标注过程的可能性，只需要依靠于非专业注释者。
</details></li>
</ul>
<hr>
<h2 id="High-performance-Data-Management-for-Whole-Slide-Image-Analysis-in-Digital-Pathology"><a href="#High-performance-Data-Management-for-Whole-Slide-Image-Analysis-in-Digital-Pathology" class="headerlink" title="High-performance Data Management for Whole Slide Image Analysis in Digital Pathology"></a>High-performance Data Management for Whole Slide Image Analysis in Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05784">http://arxiv.org/abs/2308.05784</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/adios">https://github.com/hrlblab/adios</a></li>
<li>paper_authors: Haoju Leng, Ruining Deng, Shunxing Bao, Dazheng Fang, Bryan A. Millis, Yucheng Tang, Haichun Yang, Xiao Wang, Yifan Peng, Lipeng Wan, Yuankai Huo</li>
<li>For:  This paper focuses on addressing the data access challenge in giga-pixel digital pathology using the Adaptable IO System version 2 (ADIOS2).* Methods: The authors implement a digital pathology-centric pipeline using ADIOS2 and develop strategies to curtail data retrieval times.* Results: The paper shows a two-fold speed-up in the CPU scenario and performance on par with NVIDIA Magnum IO GPU Direct Storage (GDS) in the GPU scenario, making it one of the initial instances of using ADIOS2 in digital pathology.<details>
<summary>Abstract</summary>
When dealing with giga-pixel digital pathology in whole-slide imaging, a notable proportion of data records holds relevance during each analysis operation. For instance, when deploying an image analysis algorithm on whole-slide images (WSI), the computational bottleneck often lies in the input-output (I/O) system. This is particularly notable as patch-level processing introduces a considerable I/O load onto the computer system. However, this data management process could be further paralleled, given the typical independence of patch-level image processes across different patches. This paper details our endeavors in tackling this data access challenge by implementing the Adaptable IO System version 2 (ADIOS2). Our focus has been constructing and releasing a digital pathology-centric pipeline using ADIOS2, which facilitates streamlined data management across WSIs. Additionally, we've developed strategies aimed at curtailing data retrieval times. The performance evaluation encompasses two key scenarios: (1) a pure CPU-based image analysis scenario ("CPU scenario"), and (2) a GPU-based deep learning framework scenario ("GPU scenario"). Our findings reveal noteworthy outcomes. Under the CPU scenario, ADIOS2 showcases an impressive two-fold speed-up compared to the brute-force approach. In the GPU scenario, its performance stands on par with the cutting-edge GPU I/O acceleration framework, NVIDIA Magnum IO GPU Direct Storage (GDS). From what we know, this appears to be among the initial instances, if any, of utilizing ADIOS2 within the field of digital pathology. The source code has been made publicly available at https://github.com/hrlblab/adios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-scale-Multi-site-Renal-Microvascular-Structures-Segmentation-for-Whole-Slide-Imaging-in-Renal-Pathology"><a href="#Multi-scale-Multi-site-Renal-Microvascular-Structures-Segmentation-for-Whole-Slide-Imaging-in-Renal-Pathology" class="headerlink" title="Multi-scale Multi-site Renal Microvascular Structures Segmentation for Whole Slide Imaging in Renal Pathology"></a>Multi-scale Multi-site Renal Microvascular Structures Segmentation for Whole Slide Imaging in Renal Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05782">http://arxiv.org/abs/2308.05782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franklin Hu, Ruining Deng, Shunxing Bao, Haichun Yang, Yuankai Huo</li>
<li>for:  automatización de la segmentación de estructuras microvasculares en imágenes de tejido whole slide de riñón humano, como arteriolas, venulas y capilares, para mejorar la análisis cuantitativo en patología renal.</li>
<li>methods:  utilizó una red neuronal dinámica llamada Omni-Seg, que se entrenó con datos multisitio y multescale, y se benefició de etiquetas parciales en las imágenes de entrenamiento.</li>
<li>results:  el método Omni-Seg outperformió en términos de coeficiente de semejanza de Dice y de intersectión sobre unión, lo que demuestra su eficacia en la segmentación automática de microvasculares en imágenes de riñón.<details>
<summary>Abstract</summary>
Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.
</details>
<details>
<summary>摘要</summary>
Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.Here's the text in Traditional Chinese:过去的数位patology图像（WSI）中的微血管结构，例如arterioles、venules和capillaries，的分类已成为肾脏病理学的焦点。现有的手动分类技术是时间consuming且不适合大规模数位病理图像。而深度学习基础的方法则提供了自动分类的解决方案，但大多数方法受到一个限制：它们仅适用于单一站点、单一比例的数据。在这篇文章中，我们提出了Omni-Seg方法，这是一个单一动态网络方法，它利用多个站点、多个比例的训练数据。我们在两个数据集（HuBMAP和NEPTUNE）上进行了不同的放大（40x、20x、10x和5x）。实验结果显示Omni-Seg在Dice相似度系数（DSC）和交集过Union（IoU）方面都表现出色。我们的提议方法为肾脏病理学家提供了一个强大的计算工具，用于肾脏微血管结构的量化分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/eess.IV_2023_08_11/" data-id="clly4xtg700etvl8849aiajcf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/10/cs.LG_2023_08_10/" class="article-date">
  <time datetime="2023-08-09T16:00:00.000Z" itemprop="datePublished">2023-08-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/10/cs.LG_2023_08_10/">cs.LG - 2023-08-10 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AST-MHSA-Code-Summarization-using-Multi-Head-Self-Attention"><a href="#AST-MHSA-Code-Summarization-using-Multi-Head-Self-Attention" class="headerlink" title="AST-MHSA : Code Summarization using Multi-Head Self-Attention"></a>AST-MHSA : Code Summarization using Multi-Head Self-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05646">http://arxiv.org/abs/2308.05646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeshwanth Nagaraj, Ujjwal Gupta</li>
<li>for:  Code summarization</li>
<li>methods:  Multi-head attention mechanism to extract important semantic information from AST, encoder-decoder architecture</li>
<li>results:  More comprehensive summaries of code with reduced computational overhead<details>
<summary>Abstract</summary>
Code summarization aims to generate concise natural language descriptions for source code. The prevailing approaches adopt transformer-based encoder-decoder architectures, where the Abstract Syntax Tree (AST) of the source code is utilized for encoding structural information. However, ASTs are much longer than the corresponding source code, and existing methods ignore this size constraint by directly feeding the entire linearized AST into the encoders. This simplistic approach makes it challenging to extract truly valuable dependency relations from the overlong input sequence and leads to significant computational overhead due to self-attention applied to all nodes in the AST.   To address this issue effectively and efficiently, we present a model, AST-MHSA that uses multi-head attention to extract the important semantic information from the AST. The model consists of two main components: an encoder and a decoder. The encoder takes as input the abstract syntax tree (AST) of the code and generates a sequence of hidden states. The decoder then takes these hidden states as input and generates a natural language summary of the code.   The multi-head attention mechanism allows the model to learn different representations of the input code, which can be combined to generate a more comprehensive summary. The model is trained on a dataset of code and summaries, and the parameters of the model are optimized to minimize the loss between the generated summaries and the ground-truth summaries.
</details>
<details>
<summary>摘要</summary>
code 摘要目标是生成源代码的自然语言描述。现有的方法多采用 transformer 基于 encoder-decoder 架构，其中源代码的抽象 syntax tree (AST) 用于编码结构信息。然而，AST 比源代码更长，现有方法直接将整个 linearized AST  feed into encoders，这种简单的方法使得EXTRACTING valuable dependency relations FROM THE OVERLONG INPUT SEQUENCE 困难，并且会产生巨大的计算开销由于所有节点 self-attention。To address this issue effectively and efficiently, we present a model called AST-MHSA that uses multi-head attention to extract important semantic information from the AST. The model consists of two main components: an encoder and a decoder. The encoder takes the AST of the code as input and generates a sequence of hidden states. The decoder then takes these hidden states as input and generates a natural language summary of the code.The multi-head attention mechanism allows the model to learn different representations of the input code, which can be combined to generate a more comprehensive summary. The model is trained on a dataset of code and summaries, and the parameters of the model are optimized to minimize the loss between the generated summaries and the ground-truth summaries.
</details></li>
</ul>
<hr>
<h2 id="IIHT-Medical-Report-Generation-with-Image-to-Indicator-Hierarchical-Transformer"><a href="#IIHT-Medical-Report-Generation-with-Image-to-Indicator-Hierarchical-Transformer" class="headerlink" title="IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer"></a>IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05633">http://arxiv.org/abs/2308.05633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keqiang Fan, Xiaohao Cai, Mahesan Niranjan</li>
<li>for: 这个研究旨在提出一个基于图像转换器的医疗报告生成方法，以便帮助医生更快速和更准确地生成医疗报告。</li>
<li>methods: 这个方法使用了一个图像转换器框架，包括三个模组：分类模组、指标扩展模组和生成模组。分类模组首先从医疗图像中提取图像特征，然后生成疾病相关的指标，并将这些指标转换为文本形式。生成模组则使用这些提取的特征和图像特征作为助け，以生成最终的医疗报告。</li>
<li>results: 实验结果显示，提出的方法可以实现高度的医疗报告生成精度，并且比起现有方法更具有语言流畅性和医学准确性。<details>
<summary>Abstract</summary>
Automated medical report generation has become increasingly important in medical analysis. It can produce computer-aided diagnosis descriptions and thus significantly alleviate the doctors' work. Inspired by the huge success of neural machine translation and image captioning, various deep learning methods have been proposed for medical report generation. However, due to the inherent properties of medical data, including data imbalance and the length and correlation between report sequences, the generated reports by existing methods may exhibit linguistic fluency but lack adequate clinical accuracy. In this work, we propose an image-to-indicator hierarchical transformer (IIHT) framework for medical report generation. It consists of three modules, i.e., a classifier module, an indicator expansion module and a generator module. The classifier module first extracts image features from the input medical images and produces disease-related indicators with their corresponding states. The disease-related indicators are subsequently utilised as input for the indicator expansion module, incorporating the "data-text-data" strategy. The transformer-based generator then leverages these extracted features along with image features as auxiliary information to generate final reports. Furthermore, the proposed IIHT method is feasible for radiologists to modify disease indicators in real-world scenarios and integrate the operations into the indicator expansion module for fluent and accurate medical report generation. Extensive experiments and comparisons with state-of-the-art methods under various evaluation metrics demonstrate the great performance of the proposed method.
</details>
<details>
<summary>摘要</summary>
自动化医疗报告生成已成为医学分析中越来越重要的一环。它可以生成计算机辅助诊断描述，从而减轻医生的工作负担。靠着神经机器翻译和图像描述的成功，各种深度学习方法已经被提议用于医疗报告生成。然而，由于医疗数据的内在性，包括数据不均衡和报告序列长度和相关性，所生成的报告可能具备语言流畅性，但缺乏实际的医学准确性。在这种情况下，我们提出了一种图像指标层次转换器（IIHT）框架，用于医疗报告生成。该框架包括三个模块：分类模块、指标扩展模块和生成模块。首先，分类模块从输入医学图像中提取图像特征，并生成相关疾病的指标，以及其状态。然后，指标扩展模块使用“数据-文本-数据”策略，将指标扩展为更多的疾病特征。最后，使用转换器生成器，通过这些提取的特征和图像特征作为辅助信息，生成最终的报告。此外，我们的IIHT方法可以让医生在实际应用中修改疾病指标，并将操作集成到指标扩展模块中，以实现流畅和准确的医疗报告生成。我们的实验和与当前最佳方法的比较，在不同的评价指标下，都显示了我们的方法的优秀性。
</details></li>
</ul>
<hr>
<h2 id="ReLU-and-Addition-based-Gated-RNN"><a href="#ReLU-and-Addition-based-Gated-RNN" class="headerlink" title="ReLU and Addition-based Gated RNN"></a>ReLU and Addition-based Gated RNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05629">http://arxiv.org/abs/2308.05629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rickard Brännvall, Henrik Forsgren, Fredrik Sandin, Marcus Liwicki</li>
<li>for: 减少计算成本，提高效率，以适用于受限硬件或各种权限系统。</li>
<li>methods: 使用添加和ReLU活化函数取代乘法和对数函数，以维持长期记忆并 capture long-term dependencies。</li>
<li>results: 实验结果表明，提档的门控机制可以在 synthetic sequence 学习任务中保持长期记忆，同时减少计算成本，其执行时间比 conventinal LSTM 和 GRU 减少一半（CPU）和一半（加密）。<details>
<summary>Abstract</summary>
We replace the multiplication and sigmoid function of the conventional recurrent gate with addition and ReLU activation. This mechanism is designed to maintain long-term memory for sequence processing but at a reduced computational cost, thereby opening up for more efficient execution or larger models on restricted hardware. Recurrent Neural Networks (RNNs) with gating mechanisms such as LSTM and GRU have been widely successful in learning from sequential data due to their ability to capture long-term dependencies. Conventionally, the update based on current inputs and the previous state history is each multiplied with dynamic weights and combined to compute the next state. However, multiplication can be computationally expensive, especially for certain hardware architectures or alternative arithmetic systems such as homomorphic encryption. It is demonstrated that the novel gating mechanism can capture long-term dependencies for a standard synthetic sequence learning task while significantly reducing computational costs such that execution time is reduced by half on CPU and by one-third under encryption. Experimental results on handwritten text recognition tasks furthermore show that the proposed architecture can be trained to achieve comparable accuracy to conventional GRU and LSTM baselines. The gating mechanism introduced in this paper may enable privacy-preserving AI applications operating under homomorphic encryption by avoiding the multiplication of encrypted variables. It can also support quantization in (unencrypted) plaintext applications, with the potential for substantial performance gains since the addition-based formulation can avoid the expansion to double precision often required for multiplication.
</details>
<details>
<summary>摘要</summary>
我们将传统的数 multiplication 和 sigmoid 函数更换为加法和ReLU 活化。这个机制可以保持序列处理中的长期记忆，但是降低计算成本，因此可以在更高效的执行或更大的模型中进行更多的硬件限制。回传神经网络（RNN）具有阀门机制，如 LSTM 和 GRU，在序列资料上学习了很成功，因为它们可以捕捉长期依赖。在传统的更新方式中，基于目前的输入和前一个状态历史的更新是multiplied with dynamic weights，然后合并以计算下一个状态。但是，Multiplication 可以是计算昂费的，尤其是在某些硬件架构或替代运算系统，如数学加密。在这篇论文中，我们提出了一个新的阀门机制，可以在标准的人工 синтеctic sequence 学习任务中捕捉长期依赖，并且将计算成本降低了一半在 CPU 上，并且在加密下降低了一个三分之一。实验结果显示，我们的架构可以与传统 GRU 和 LSTM 基准点进行相互比较，并且在手写文本识别任务中进行训练。这个阀门机制可以实现隐私保护的 AI 应用，例如在数学加密下运行，并且可以支持量化（ plaintext 应用中），这可能将带来重大的性能提升，因为加法形式可以避免对 double precision 的扩展，通常需要 multiplication。
</details></li>
</ul>
<hr>
<h2 id="Normalized-Gradients-for-All"><a href="#Normalized-Gradients-for-All" class="headerlink" title="Normalized Gradients for All"></a>Normalized Gradients for All</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05621">http://arxiv.org/abs/2308.05621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Johnnywang1899/Credit_Risk_Analysis">https://github.com/Johnnywang1899/Credit_Risk_Analysis</a></li>
<li>paper_authors: Francesco Orabona</li>
<li>for: 这篇论文主要是用于掌握Holder平坦性的方法。</li>
<li>methods: 这篇论文使用了normalized gradients来实现Black-box方式下的Holder平坦性适应。另外， bound的计算还基于一个新的地方Holder平坦性的定义。主要思想来自Levy [2017]。</li>
<li>results: 论文得到了一个基于localHolder平坦性的bound，这个bound取决于一个新的地方Holder平坦性的定义。<details>
<summary>Abstract</summary>
In this short note, I show how to adapt to H\"{o}lder smoothness using normalized gradients in a black-box way. Moreover, the bound will depend on a novel notion of local H\"{o}lder smoothness. The main idea directly comes from Levy [2017].
</details>
<details>
<summary>摘要</summary>
这短い详细说明中，我们介绍了如何适应Holder平滑性使用 норциали化的梯度。此外， bound 的取值也取决于一种新的地方Holder平滑性。主要的想法直接来自Levy [2017]。Here's the breakdown of the translation:* "Holder smoothness" is translated as "Holder平滑性" (holder smoothness)* "normalized gradients" is translated as "norмаль化梯度" (normalized gradients)* "black-box way" is translated as "黑盒方式" (black-box way)* "novel notion of local H\"{o}lder smoothness" is translated as "一种新的地方Holder平滑性" (novel notion of local Holder smoothness)* "Levy [2017]" is translated as "Levy [2017]" (Levy [2017])
</details></li>
</ul>
<hr>
<h2 id="Updating-Clinical-Risk-Stratification-Models-Using-Rank-Based-Compatibility-Approaches-for-Evaluating-and-Optimizing-Clinician-Model-Team-Performance"><a href="#Updating-Clinical-Risk-Stratification-Models-Using-Rank-Based-Compatibility-Approaches-for-Evaluating-and-Optimizing-Clinician-Model-Team-Performance" class="headerlink" title="Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance"></a>Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05619">http://arxiv.org/abs/2308.05619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erkin Ötleş, Brian T. Denton, Jenna Wiens</li>
<li>for: 这篇论文的目的是提出一种新的rank-based兼容度指标($C^R$)和一种新的损失函数，用于优化风险分化模型的兼容性和推理性。</li>
<li>methods: 该论文使用了现有的模型选择技术和一种新的损失函数，并在MIMIC数据集上进行了实验 validate the proposed approach.</li>
<li>results: 相比 existed model selection techniques, the proposed approach yielded more compatible models while maintaining discriminative performance, with an increase in $C^R$ of $0.019$ ($95%$ confidence interval: $0.005$, $0.035$).<details>
<summary>Abstract</summary>
As data shift or new data become available, updating clinical machine learning models may be necessary to maintain or improve performance over time. However, updating a model can introduce compatibility issues when the behavior of the updated model does not align with user expectations, resulting in poor user-model team performance. Existing compatibility measures depend on model decision thresholds, limiting their applicability in settings where models are used to generate rankings based on estimated risk. To address this limitation, we propose a novel rank-based compatibility measure, $C^R$, and a new loss function that aims to optimize discriminative performance while encouraging good compatibility. Applied to a case study in mortality risk stratification leveraging data from MIMIC, our approach yields more compatible models while maintaining discriminative performance compared to existing model selection techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval: $0.005$, $0.035$). This work provides new tools to analyze and update risk stratification models used in clinical care.
</details>
<details>
<summary>摘要</summary>
As data shifts or new data becomes available, updating clinical machine learning models may be necessary to maintain or improve performance over time. However, updating a model can introduce compatibility issues when the behavior of the updated model does not align with user expectations, resulting in poor user-model team performance. Existing compatibility measures depend on model decision thresholds, limiting their applicability in settings where models are used to generate rankings based on estimated risk. To address this limitation, we propose a novel rank-based compatibility measure, $C^R$, and a new loss function that aims to optimize discriminative performance while encouraging good compatibility. Applied to a case study in mortality risk stratification leveraging data from MIMIC, our approach yields more compatible models while maintaining discriminative performance compared to existing model selection techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval: $0.005$, $0.035$). This work provides new tools to analyze and update risk stratification models used in clinical care.Here's the translation in Traditional Chinese:当数据shift或新数据可用时，对于供应链机器学习模型的更新可能是必要的，以维持或改善性能。然而，更新模型可能会导致用户预期不符的行为，导致用户模型团队性能差。现有的兼容度标准依赖于模型决策阈值，仅适用于基于估计风险的情况下。为了解决这些限制，我们提出了一个新的排名基于兼容度度量，$C^R$, 以及一个新的损失函数，旨在优化推理性能的同时鼓励好兼容。对于基于MIMIC的致死风险分组案例，我们的方法产生了更兼容的模型，保持了推理性能的同时，与现有的模型选择技术相比，$C^R$ 增加了0.019 ($95\%$ 信度interval: 0.005，0.035）。这个工作提供了新的工具来分析和更新在医疗保健中使用的风险分组模型。
</details></li>
</ul>
<hr>
<h2 id="Multi-graph-Spatio-temporal-Graph-Convolutional-Network-for-Traffic-Flow-Prediction"><a href="#Multi-graph-Spatio-temporal-Graph-Convolutional-Network-for-Traffic-Flow-Prediction" class="headerlink" title="Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction"></a>Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05601">http://arxiv.org/abs/2308.05601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weilong Ding, Tianpu Zhang, Jianwu Wang, Zhuofeng Zhao</li>
<li>for: 这篇论文主要是为了提出一种用于每日高速公路交通流量预测的深度学习方法。</li>
<li>methods: 该方法使用数据归一化策略处理数据不均衡问题，然后使用图 convolutional 网络捕捉空间时间特征。此外，还使用天气和历法特征在全连接Stage中增加外部特征。</li>
<li>results: 对一个中国省级高速公路进行了详细实验和案例研究，结果显示该方法与基eline比较，有明显的预测精度提升和实质性的商业应用优势。<details>
<summary>Abstract</summary>
Inter-city highway transportation is significant for urban life. As one of the key functions in intelligent transportation system (ITS), traffic evaluation always plays significant role nowadays, and daily traffic flow prediction still faces challenges at network-wide toll stations. On the one hand, the data imbalance in practice among various locations deteriorates the performance of prediction. On the other hand, complex correlative spatio-temporal factors cannot be comprehensively employed in long-term duration. In this paper, a prediction method is proposed for daily traffic flow in highway domain through spatio-temporal deep learning. In our method, data normalization strategy is used to deal with data imbalance, due to long-tail distribution of traffic flow at network-wide toll stations. And then, based on graph convolutional network, we construct networks in distinct semantics to capture spatio-temporal features. Beside that, meteorology and calendar features are used by our model in the full connection stage to extra external characteristics of traffic flow. By extensive experiments and case studies in one Chinese provincial highway, our method shows clear improvement in predictive accuracy than baselines and practical benefits in business.
</details>
<details>
<summary>摘要</summary>
城市间高速交通是城市生活中非常重要的一环。作为智能交通系统（ITS）中一项关键功能，交通评估总是在当今得到重要的应用，而日常交通流量预测仍然面临着网络覆盖站的挑战。一方面，实际应用中的数据不均衡问题使预测性能下降。另一方面，复杂的相关空间时间因素难以长期内部涵盖。本文提出了基于高速公路域的日常交通流量预测方法，使用数据归一化策略处理数据不均衡问题，并基于图 convolutional network 构建不同 semantics 的网络，捕捉空间时间特征。此外，我们的模型还在全连接阶段使用气象和历法特征，以捕捉交通流量的外部特征。经过广泛的实验和案例研究，我们的方法在一个中国省级高速公路上显示出了明显的预测精度提高和实践效益。
</details></li>
</ul>
<hr>
<h2 id="NUPES-Non-Uniform-Post-Training-Quantization-via-Power-Exponent-Search"><a href="#NUPES-Non-Uniform-Post-Training-Quantization-via-Power-Exponent-Search" class="headerlink" title="NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search"></a>NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05600">http://arxiv.org/abs/2308.05600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edouard Yvinec, Arnaud Dapogny, Kevin Bailly<br>for: 这篇论文的目的是提出一种改进现有深度神经网络（DNN）优化技术，以提高优化后的预测性能。methods: 这篇论文使用了一种名为“自适应映射”的技术，将浮点数表示转换为低位元数字表示，以减少DNN模型的内存负载和延迟。此外，这篇论文还使用了一种名为“权函数”的技术，将DNN模型中的权值和活化函数转换为低位元数字表示。results: 这篇论文获得了顶尖的压缩率，并且可以在没有数据验证和数据验证下运行。此外，这篇论文还提出了一新的优化方法，可以在训练过程中对优化后的模型进行优化，以提高预测性能。<details>
<summary>Abstract</summary>
Deep neural network (DNN) deployment has been confined to larger hardware devices due to their expensive computational requirements. This challenge has recently reached another scale with the emergence of large language models (LLMs). In order to reduce both their memory footprint and latency, a promising technique is quantization. It consists in converting floating point representations to low bit-width fixed point representations, usually by assuming a uniform mapping onto a regular grid. This process, referred to in the literature as uniform quantization, may however be ill-suited as most DNN weights and activations follow a bell-shaped distribution. This is even worse on LLMs whose weight distributions are known to exhibit large, high impact, outlier values. In this work, we propose an improvement over the most commonly adopted way to tackle this limitation in deep learning models quantization, namely, non-uniform quantization. NUPES leverages automorphisms to preserve the scalar multiplications. Such transformations are derived from power functions. However, the optimization of the exponent parameter and weight values remains a challenging and novel problem which could not be solved with previous post training optimization techniques which only learn to round up or down weight values in order to preserve the predictive function. We circumvent this limitation with a new paradigm: learning new quantized weights over the entire quantized space. Similarly, we enable the optimization of the power exponent, i.e. the optimization of the quantization operator itself during training by alleviating all the numerical instabilities. The resulting predictive function is compatible with integer-only low-bit inference. We show the ability of the method to achieve state-of-the-art compression rates in both, data-free and data-driven configurations.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的部署因其高效计算需求而受限于更大的硬件设备。这一挑战最近又由大型自然语言模型（LLM）的出现加剧。以减少它们的内存占用量和延迟，一种有前途的技术是量化。它通过将浮点表示转换为低位数字表示，通常通过假设一个固定格式的映射来实现。这个过程被称为固定量化，但是可能不适合大多数DNN的权重和活动值，因为它们通常follows a bell-shaped distribution。这个问题更加严重，因为LLMs的权重分布知道存在大、高影响的异常值。在这种情况下，我们提出一种改进了深度学习模型量化的方法，即非均匀量化（NUPES）。NUPES利用自同构来保持整数乘法。这些转换是基于力函数的。然而，优化剩余因子和权重值的问题仍然是一个挑战，而且不可以通过以前的训练优化技术解决，这些技术只是学习将权重值略减或略加以保持预测函数的正确性。我们绕过这个限制，通过学习新的量化权重，在整数乘法下保持预测函数的正确性。同时，我们启用量化运算符的优化，即在训练中优化量化操作符的权重和剩余因子。这些优化可以减少所有数值不稳定性。我们展示了该方法可以在数据驱动和数据隐藏配置下实现国际只有low-bit执行的预测函数，并且达到了国际最佳压缩率。
</details></li>
</ul>
<hr>
<h2 id="Symmetry-Defense-Against-XGBoost-Adversarial-Perturbation-Attacks"><a href="#Symmetry-Defense-Against-XGBoost-Adversarial-Perturbation-Attacks" class="headerlink" title="Symmetry Defense Against XGBoost Adversarial Perturbation Attacks"></a>Symmetry Defense Against XGBoost Adversarial Perturbation Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05575">http://arxiv.org/abs/2308.05575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blerta Lindqvist</li>
<li>for: This paper aims to defend tree-based ensemble classifiers like gradient-boosting decision trees (GBDTs) against adversarial perturbation attacks by utilizing the lack of invariance with respect to symmetries.</li>
<li>methods: The paper uses a symmetry defense approach that has been previously used for convolutional neural networks (CNNs) and applies it to GBDTs. The defense mechanism relies on the fact that GBDTs lack invariance with respect to symmetries, and it uses this lack of invariance to revert the incorrect classification of adversarial samples.</li>
<li>results: The paper evaluates the GBDT symmetry defense for nine datasets against six perturbation attacks with a threat model that ranges from zero-knowledge to perfect-knowledge adversaries. The results show that the defense mechanism can achieve up to 100% accuracy on adversarial samples even when default and robust classifiers have 0% accuracy, and up to over 95% accuracy on adversarial samples for the GBDT classifier of the F-MNIST dataset even when default and robust classifiers have 0% accuracy.Here is the information in Simplified Chinese text:</li>
<li>for: 这 paper 的目的是为了防御基于树的集成分类器，如Gradient-Boosting Decision Trees (GBDTs)，对于攻击性扰动攻击。</li>
<li>methods: 这 paper 使用了一种基于对称的防御方法，该方法已经在卷积神经网络 (CNNs) 上使用，并将其应用到 GBDTs 上。防御机制基于 GBDTs 缺乏对称性，并利用这种缺乏对称性来 revert 攻击样本的错误分类。</li>
<li>results: 这 paper 对 nine 个数据集进行了六种攻击，并评估了 GBDT 防御机制的性能。结果表明，防御机制可以在 zero-knowledge 到 perfect-knowledge 攻击者的威胁模型下，对 adversarial 样本进行100%的正确分类，以及对 F-MNIST 数据集的 GBDT 分类器进行95%以上的正确分类。<details>
<summary>Abstract</summary>
We examine whether symmetry can be used to defend tree-based ensemble classifiers such as gradient-boosting decision trees (GBDTs) against adversarial perturbation attacks. The idea is based on a recent symmetry defense for convolutional neural network classifiers (CNNs) that utilizes CNNs' lack of invariance with respect to symmetries. CNNs lack invariance because they can classify a symmetric sample, such as a horizontally flipped image, differently from the original sample. CNNs' lack of invariance also means that CNNs can classify symmetric adversarial samples differently from the incorrect classification of adversarial samples. Using CNNs' lack of invariance, the recent CNN symmetry defense has shown that the classification of symmetric adversarial samples reverts to the correct sample classification. In order to apply the same symmetry defense to GBDTs, we examine GBDT invariance and are the first to show that GBDTs also lack invariance with respect to symmetries. We apply and evaluate the GBDT symmetry defense for nine datasets against six perturbation attacks with a threat model that ranges from zero-knowledge to perfect-knowledge adversaries. Using the feature inversion symmetry against zero-knowledge adversaries, we achieve up to 100% accuracy on adversarial samples even when default and robust classifiers have 0% accuracy. Using the feature inversion and horizontal flip symmetries against perfect-knowledge adversaries, we achieve up to over 95% accuracy on adversarial samples for the GBDT classifier of the F-MNIST dataset even when default and robust classifiers have 0% accuracy.
</details>
<details>
<summary>摘要</summary>
我们研究使用对称来防御基于树状集成分类器（GBDT）的对抗攻击。这个想法基于现有的对称防御技术，该技术利用对称隐藏层（CNN）的不变性。CNNlacks变换不变性，这意味着它可以将水平翻转的图像分类为不同的样本，而不是原始样本。此外，CNN的不变性还意味着它可以将对称攻击样本分类为错误的样本。使用CNN的不变性，这个新的对称防御技术可以使得对称攻击样本的分类恢复到正确的样本分类。为了应用该技术到GBDT中，我们首先检查GBDT的不变性，并发现GBDT也缺乏对称性。我们应用和评估了GBDT对九个数据集的对称防御技术，并对六种攻击方法进行评估，包括零知识到完美知识的攻击者。使用对称性对零知识攻击者，我们达到了100%的正确率。使用对称和水平翻转 symmetry对完美知识攻击者，我们在F-MNIST数据集上达到了95%以上的正确率。
</details></li>
</ul>
<hr>
<h2 id="AutoGluon-TimeSeries-AutoML-for-Probabilistic-Time-Series-Forecasting"><a href="#AutoGluon-TimeSeries-AutoML-for-Probabilistic-Time-Series-Forecasting" class="headerlink" title="AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting"></a>AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05566">http://arxiv.org/abs/2308.05566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oleksandr Shchur, Caner Turkmen, Nick Erickson, Huibin Shen, Alexander Shirkov, Tony Hu, Yuyang Wang</li>
<li>for: 这篇论文主要是为了提出一个开源的AutoML库，用于机会时间序列预测。</li>
<li>methods: 这篇论文使用了AutoGluon的设计哲学， combinig了传统的统计模型、机器学习基于预测方法、和ensemble技术。</li>
<li>results: 在29个benchmark dataset上进行评估，这篇论文展示了强大的实验性表现，在点预测和量预测方面都高于了一些预测方法，并且经常超越了最佳对照方法的结合。<details>
<summary>Abstract</summary>
We introduce AutoGluon-TimeSeries - an open-source AutoML library for probabilistic time series forecasting. Focused on ease of use and robustness, AutoGluon-TimeSeries enables users to generate accurate point and quantile forecasts with just 3 lines of Python code. Built on the design philosophy of AutoGluon, AutoGluon-TimeSeries leverages ensembles of diverse forecasting models to deliver high accuracy within a short training time. AutoGluon-TimeSeries combines both conventional statistical models, machine-learning based forecasting approaches, and ensembling techniques. In our evaluation on 29 benchmark datasets, AutoGluon-TimeSeries demonstrates strong empirical performance, outperforming a range of forecasting methods in terms of both point and quantile forecast accuracy, and often even improving upon the best-in-hindsight combination of prior methods.
</details>
<details>
<summary>摘要</summary>
我们介绍AutoGluon-TimeSeries - 一个开源AutoML库 для潜在时间序列预测。我们专注在使用方便和可靠性，使用3行Python代码可以生成高精度的点和量测预测。基于AutoGluon的设计哲学，AutoGluon-TimeSeries 利用多种不同预测模型的ensemble，以提供高精度的预测，仅需训练时间短。AutoGluon-TimeSeries 结合了传统的统计学模型、机器学习基于预测方法、和 ensemble技术。在我们的29个benchmark数据集评估中，AutoGluon-TimeSeries 示出了强大的实验性表现，在点和量测预测精度方面高于许多预测方法，并经常超过最佳组合的先前方法。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Variational-Inference-for-Large-Skew-t-Copulas-with-Application-to-Intraday-Equity-Returns"><a href="#Efficient-Variational-Inference-for-Large-Skew-t-Copulas-with-Application-to-Intraday-Equity-Returns" class="headerlink" title="Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns"></a>Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05564">http://arxiv.org/abs/2308.05564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Deng, Michael Stanley Smith, Worapree Maneesoonthorn</li>
<li>for: 这个论文旨在提出一种基于skew-t分布的高维束合模型，用于金融数据的模型化，因为这种模型允许对各对依存关系进行偏置和极端尾部依存。</li>
<li>methods: 这篇论文使用了一种基于束合变分的bayesian变分推断（VI）方法来估算高维skew-t分布。这种方法使用一种具有条件 Gaussian 发现的skew-t分布来定义一个增强后验，可以准确地估算高维束合模型。</li>
<li>results: 研究人员使用了这种新方法来估算2017年至2021年的93只美国股票的高维束合模型。结果显示，这种模型能够很好地捕捉到股票对之间的偏置和极端尾部依存，同时也能够更好地预测股票的实时返佣分布。此外，基于估算的对依存关系的股票组合策略也能够提高股票投资的性能。<details>
<summary>Abstract</summary>
Large skew-t factor copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a conditionally Gaussian generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A fast stochastic gradient ascent algorithm is used to solve the variational optimization. The new methodology is used to estimate copula models for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise correlations. We show that intraday predictive densities from the skew-t copula are more accurate than from some other copula models, while portfolio selection strategies based on the estimated pairwise tail dependencies improve performance relative to the benchmark index.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大skew-t因子 copula模型是金融数据模型中吸引人的，因为它允许非对称和极端尾部依赖。我们表明，Azzalini和Capitanio（2003）中的skew-t分布下的 copula允许更高的对称依赖，than two popular alternative skew-t copulas。估计这种 copula 在高维度是挑战性的，我们提议一种快速和准确的 Bayesian variational inference（VI）方法来实现。该方法使用 conditionally Gaussian 生成表示法来定义增强 posterior，可以高度准确地 aproximate。一种快速的梯度下降算法用于解决variational优化。我们使用这种新方法来估计2017-2021年的93只美国股票的copula模型。该 copula 捕捉了股票对的非对称依赖和对比股票对的相关性的巨大多样性。我们表明，从skew-t copula 中的预测概率密度比其他 copula 模型更准确，而基于估计的对称尾部依赖而实现的股票选择策略也能够超越 referential 指数。
</details></li>
</ul>
<hr>
<h2 id="Critical-Points-An-Agile-Point-Cloud-Importance-Measure-for-Robust-Classification-Adversarial-Defense-and-Explainable-AI"><a href="#Critical-Points-An-Agile-Point-Cloud-Importance-Measure-for-Robust-Classification-Adversarial-Defense-and-Explainable-AI" class="headerlink" title="Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI"></a>Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05525">http://arxiv.org/abs/2308.05525</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yossilevii100/critical_points2">https://github.com/yossilevii100/critical_points2</a></li>
<li>paper_authors: Meir Yossef Levi, Guy Gilboa</li>
<li>for: 本研究旨在提高实际应用中对异常样本的处理精度和速度，并 investigate critical points of 3D point clouds and their relationship with out-of-distribution (OOD) samples.</li>
<li>methods: 本文提出了一种基于重要性度量的方法，即使用训练分类网络仅使用不重要点进行训练，以提高模型的Robustness，并使用 норма化 entropy 来选择不重要点。</li>
<li>results: 研究结果表明，使用提出的方法可以在Robust Classification和抗击攻击任务上达到顶峰性能，并且可以快速和准确地处理异常样本。<details>
<summary>Abstract</summary>
The ability to cope accurately and fast with Out-Of-Distribution (OOD) samples is crucial in real-world safety demanding applications. In this work we first study the interplay between critical points of 3D point clouds and OOD samples. Our findings are that common corruptions and outliers are often interpreted as critical points. We generalize the notion of critical points into importance measures. We show that training a classification network based only on less important points dramatically improves robustness, at a cost of minor performance loss on the clean set. We observe that normalized entropy is highly informative for corruption analysis. An adaptive threshold based on normalized entropy is suggested for selecting the set of uncritical points. Our proposed importance measure is extremely fast to compute. We show it can be used for a variety of applications, such as Explainable AI (XAI), Outlier Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense. We reach SOTA results on the two latter tasks. Code is available at: https://github.com/yossilevii100/critical_points2
</details>
<details>
<summary>摘要</summary>
能够快速和准确处理 OUT-OF-DISTRIBUTION（OOD）样本的能力在实际应用中是非常重要的。在这个工作中，我们首先研究了三维点云的极点与OOD样本之间的交互。我们发现，常见的损害和异常点经常被解释为极点。我们扩展了极点的概念，得到了重要度度量。我们发现，只使用不重要的点进行训练，可以很好地提高 robustness，但是会导致清洁集上的性能下降。我们发现，Normalized entropy 非常有用于损害分析。我们建议使用 Normalized entropy 来选择不重要的点。我们的提出的重要度度量非常快速计算。我们证明它可以用于多种应用，如 Explainable AI（XAI）、异常点除法、不确定度估计、Robust Classification 和对抗攻击。我们在两个后者任务上达到了 SOTA 结果。代码可以在：https://github.com/yossilevii100/critical_points2 中找到。
</details></li>
</ul>
<hr>
<h2 id="Models-Matter-The-Impact-of-Single-Step-Retrosynthesis-on-Synthesis-Planning"><a href="#Models-Matter-The-Impact-of-Single-Step-Retrosynthesis-on-Synthesis-Planning" class="headerlink" title="Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning"></a>Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05522">http://arxiv.org/abs/2308.05522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paula Torren-Peraire, Alan Kai Hassen, Samuel Genheden, Jonas Verhoeven, Djork-Arne Clevert, Mike Preuss, Igor Tetko<br>for: 这篇论文的目的是提出一种结合单步逆synthesis预测和多步合成规划的方法，以提高合成路径的可靠性和效率。methods: 该方法首先应用多个单步逆synthesis模型，然后在多步合成规划中分析其影响。此外，该方法还使用公共和专用反应数据进行评估。results: 研究发现，单步逆synthesis模型在多步合成规划中的选择可以提高总成功率 by up to +28%，而且每个单步模型都找到了不同的合成路径，这些路径之间存在一定的不同，例如路径找到成功率、合成路径的数量和化学有效性等方面。<details>
<summary>Abstract</summary>
Retrosynthesis consists of breaking down a chemical compound recursively step-by-step into molecular precursors until a set of commercially available molecules is found with the goal to provide a synthesis route. Its two primary research directions, single-step retrosynthesis prediction, which models the chemical reaction logic, and multi-step synthesis planning, which tries to find the correct sequence of reactions, are inherently intertwined. Still, this connection is not reflected in contemporary research. In this work, we combine these two major research directions by applying multiple single-step retrosynthesis models within multi-step synthesis planning and analyzing their impact using public and proprietary reaction data. We find a disconnection between high single-step performance and potential route-finding success, suggesting that single-step models must be evaluated within synthesis planning in the future. Furthermore, we show that the commonly used single-step retrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation task does not represent model performance and scalability on larger and more diverse datasets. For multi-step synthesis planning, we show that the choice of the single-step model can improve the overall success rate of synthesis planning by up to +28% compared to the commonly used baseline model. Finally, we show that each single-step model finds unique synthesis routes, and differs in aspects such as route-finding success, the number of found synthesis routes, and chemical validity, making the combination of single-step retrosynthesis prediction and multi-step synthesis planning a crucial aspect when developing future methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>功能分解是一种分解化学物质的步骤，从分子前体开始，直到找到一组可用的化学物质，以实现化学合成。功能分解的两个主要研究方向是单步功能分解预测和多步合成规划。single-step retrosynthesis prediction models the chemical reaction logic, while multi-step synthesis planning tries to find the correct sequence of reactions. However, these two research directions are not well connected in contemporary research. In this work, we combine these two major research directions by applying multiple single-step retrosynthesis models within multi-step synthesis planning and analyzing their impact using public and proprietary reaction data.我们发现，高单步性能并不一定对应合成规划的成功。这表明，单步模型在合成规划中需要进行评估。此外，我们发现USPTO-50k单步功能分解数据集不够，因为这个评估任务不能反映模型在更大和更多样化的数据集上的性能和可扩展性。对多步合成规划，我们发现，选择合适的单步模型可以提高总成功率的synthesis planning by up to +28% compared to the commonly used baseline model。此外，我们发现每个单步模型都找到了不同的合成路径，这些路径之间存在差异，例如成功率、合成路径数量和化学有效性。因此，将单步功能分解预测和多步合成规划相结合是未来发展方法的关键。Note: The text has been translated using Google Translate, and some parts may not be exactly correct or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="On-the-Optimal-Expressive-Power-of-ReLU-DNNs-and-Its-Application-in-Approximation-with-Kolmogorov-Superposition-Theorem"><a href="#On-the-Optimal-Expressive-Power-of-ReLU-DNNs-and-Its-Application-in-Approximation-with-Kolmogorov-Superposition-Theorem" class="headerlink" title="On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem"></a>On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05509">http://arxiv.org/abs/2308.05509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juncai He</li>
<li>for: studying the optimal expressive power of ReLU deep neural networks (DNNs) and its application in approximation.</li>
<li>methods: constructive proof and investigation of the shattering capacity of ReLU DNNs.</li>
<li>results: achievement of an enhanced approximation rate for ReLU DNNs of arbitrary width and depth when dealing with continuous functions in high-dimensional spaces.Here is the Chinese translation of the three key information points:</li>
<li>for: 研究具有最佳表达力的ReLU深度神经网络（DNNs）以及其在拟合中的应用。</li>
<li>methods: 使用构造性证明和扰乱容量研究ReLU DNNs。</li>
<li>results: 通过高维空间中连续函数的拟合，实现ReLU DNNs的参数计数最佳化。<details>
<summary>Abstract</summary>
This paper is devoted to studying the optimal expressive power of ReLU deep neural networks (DNNs) and its application in approximation via the Kolmogorov Superposition Theorem. We first constructively prove that any continuous piecewise linear functions on $[0,1]$, comprising $O(N^2L)$ segments, can be represented by ReLU DNNs with $L$ hidden layers and $N$ neurons per layer. Subsequently, we demonstrate that this construction is optimal regarding the parameter count of the DNNs, achieved through investigating the shattering capacity of ReLU DNNs. Moreover, by invoking the Kolmogorov Superposition Theorem, we achieve an enhanced approximation rate for ReLU DNNs of arbitrary width and depth when dealing with continuous functions in high-dimensional spaces.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文关注研究ReLU深度神经网络（DNNs）的最佳表达能力和其在高维空间中的扩展应用。我们首先构造地证明任何连续划分线性函数在[0,1]中可以通过ReLU DNNs avec $L$层和$N$个神经元来表示，其中参数计数为$O(N^2L)$。然后我们证明这个构造是最佳的，通过研究ReLU DNNs的分化能力。此外，通过kolmogorov超position定理，我们得到了ReLU DNNs的任意宽度和深度时对连续函数的高级度扩展应用。
</details></li>
</ul>
<hr>
<h2 id="Quality-Diversity-under-Sparse-Reward-and-Sparse-Interaction-Application-to-Grasping-in-Robotics"><a href="#Quality-Diversity-under-Sparse-Reward-and-Sparse-Interaction-Application-to-Grasping-in-Robotics" class="headerlink" title="Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics"></a>Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05483">http://arxiv.org/abs/2308.05483</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Johann-Huber/qd_grasp">https://github.com/Johann-Huber/qd_grasp</a></li>
<li>paper_authors: J. Huber, F. Hélénon, M. Coninx, F. Ben Amar, S. Doncieux</li>
<li>for: 本研究旨在应用Quality-Diversity（QD）算法来解决机器人抓取问题，抓取是机器人控制领域中的一个重要任务。</li>
<li>methods: 本研究使用了15种不同的方法，在10个抓取领域中进行了实验，包括2种机器人抓取设置和5种标准物品。研究还提出了一个评价框架，以便公正地对各种算法进行评价。</li>
<li>results: 研究结果表明，MAP-Elites变体在研究中所用的评价指标上表现出了明显的优势，至少在比较中超过了所有其他方法。此外，研究还发现了稀有互动可能导致假新鲜度的现象。本研究所获得的抓取trajectory的生成能力在文献中无前例。<details>
<summary>Abstract</summary>
Quality-Diversity (QD) methods are algorithms that aim to generate a set of diverse and high-performing solutions to a given problem. Originally developed for evolutionary robotics, most QD studies are conducted on a limited set of domains - mainly applied to locomotion, where the fitness and the behavior signal are dense. Grasping is a crucial task for manipulation in robotics. Despite the efforts of many research communities, this task is yet to be solved. Grasping cumulates unprecedented challenges in QD literature: it suffers from reward sparsity, behavioral sparsity, and behavior space misalignment. The present work studies how QD can address grasping. Experiments have been conducted on 15 different methods on 10 grasping domains, corresponding to 2 different robot-gripper setups and 5 standard objects. An evaluation framework that distinguishes the evaluation of an algorithm from its internal components has also been proposed for a fair comparison. The obtained results show that MAP-Elites variants that select successful solutions in priority outperform all the compared methods on the studied metrics by a large margin. We also found experimental evidence that sparse interaction can lead to deceptive novelty. To our knowledge, the ability to efficiently produce examples of grasping trajectories demonstrated in this work has no precedent in the literature.
</details>
<details>
<summary>摘要</summary>
优质多样性（QD）算法目的是生成一组多样且高性能的解决方案，原本用于进化 робототех术。大多数QD研究都是在有限的领域上进行，主要是应用于行动，其中健能和行为信号都是密集的。抓取是机器人控制中的关键任务，尚未得到解决。抓取受到QD文献中的挑战，包括奖励稀少、行为稀少和行为空间不协调。本研究探讨了QD如何解决抓取问题。我们在15种方法上进行了10个抓取领域的实验，包括2种机器人夹仓设置和5种标准物品。我们还提出了一种评价框架，以便公正地比较不同算法的表现。实验结果表明，MAP-Elites变种选择成功解决方案在所研究的指标上大幅度超越所有比较的方法。我们还发现了实验证明，稀少的互动可能导致假新鲜。在这种情况下，我们所提出的能够高效生成抓取轨迹示例的能力，在文献中无先例。
</details></li>
</ul>
<hr>
<h2 id="LLM-As-DBA"><a href="#LLM-As-DBA" class="headerlink" title="LLM As DBA"></a>LLM As DBA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05481">http://arxiv.org/abs/2308.05481</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghuadatabasegroup/db-gpt">https://github.com/tsinghuadatabasegroup/db-gpt</a></li>
<li>paper_authors: Xuanhe Zhou, Guoliang Li, Zhiyuan Liu<br>for:The paper is written to propose a revolutionary framework for database maintenance using large language models (LLMs).methods:The framework uses LLMs to detect database maintenance knowledge from documents and tools, and uses tree of thought reasoning for root cause analysis.results:The paper presents preliminary experimental results that show D-Bot, the proposed LLM-based database administrator, can efficiently and effectively diagnose the root causes of database issues.<details>
<summary>Abstract</summary>
Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results that D-Bot can efficiently and effectively diagnose the root causes and our code is available at github.com/TsinghuaDatabaseGroup/DB-GPT.
</details>
<details>
<summary>摘要</summary>
Database administrators (DBAs) play a crucial role in managing, maintaining, and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently, large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results show that D-Bot can efficiently and effectively diagnose the root causes, and our code is available at github.com/TsinghuaDatabaseGroup/DB-GPT.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Machine-Learning-and-Transformer-based-Approaches-for-Deceptive-Text-Classification-A-Comparative-Analysis"><a href="#Exploring-Machine-Learning-and-Transformer-based-Approaches-for-Deceptive-Text-Classification-A-Comparative-Analysis" class="headerlink" title="Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis"></a>Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05476">http://arxiv.org/abs/2308.05476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anusuya Krishnan</li>
<li>for: 本研究旨在比较机器学习和变换器基于的方法在假信息涂抹中的效果。</li>
<li>methods: 研究使用了传统的机器学习算法以及当今最佳实践的变换器模型，如BERT、XLNET、DistilBERT和RoBERTa，来检测假信息。</li>
<li>results: 通过广泛的实验，研究对不同方法的性能指标，包括准确率、精度、回归率和F1分数，进行了比较。结果可以为研究人员和实践者提供有用的指导，帮助他们在遇到假信息时做出 Informed 决策。<details>
<summary>Abstract</summary>
Deceptive text classification is a critical task in natural language processing that aims to identify deceptive o fraudulent content. This study presents a comparative analysis of machine learning and transformer-based approaches for deceptive text classification. We investigate the effectiveness of traditional machine learning algorithms and state-of-the-art transformer models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive text. A labeled dataset consisting of deceptive and non-deceptive texts is used for training and evaluation purposes. Through extensive experimentation, we compare the performance metrics, including accuracy, precision, recall, and F1 score, of the different approaches. The results of this study shed light on the strengths and limitations of machine learning and transformer-based methods for deceptive text classification, enabling researchers and practitioners to make informed decisions when dealing with deceptive content.
</details>
<details>
<summary>摘要</summary>
伪装文本分类是自然语言处理中一项重要任务，旨在识别伪装或诈骗性内容。本研究进行了机器学习和变换器基于方法的比较分析，以检验这些方法在检测伪装文本方面的效果。我们使用了一个标注的数据集，包括伪装和非伪装文本，进行训练和评估。通过广泛的实验，我们比较了不同方法的性能指标，包括准确率、精度、准确率和F1分数。研究结果为研究者和实践者提供了有用的指导，帮助他们在面临伪装内容时做出了 Informed decisions。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Analysis-of-Network-Robustness-Evaluation-Based-on-Convolutional-Neural-Networks-with-Spatial-Pyramid-Pooling"><a href="#Comprehensive-Analysis-of-Network-Robustness-Evaluation-Based-on-Convolutional-Neural-Networks-with-Spatial-Pyramid-Pooling" class="headerlink" title="Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling"></a>Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08012">http://arxiv.org/abs/2308.08012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjun Jiang, Tianlong Fan, Changhao Li, Chuanfu Zhang, Tao Zhang, Zong-fu Luo</li>
<li>for: 这篇论文是用来理解、优化和修复复杂网络的连接Robustness的一种新方法。</li>
<li>methods: 这篇论文使用了卷积神经网络（CNN）模型和空间彩色堆叠网络（SPP-net）来解决连接Robustness的计算复杂性问题。</li>
<li>results: 该模型在不同的网络类型、失败组件类型和失败场景下的计算时间均有较高的效率，但在一些场景下表现不尽人意料。<details>
<summary>Abstract</summary>
Connectivity robustness, a crucial aspect for understanding, optimizing, and repairing complex networks, has traditionally been evaluated through time-consuming and often impractical simulations. Fortunately, machine learning provides a new avenue for addressing this challenge. However, several key issues remain unresolved, including the performance in more general edge removal scenarios, capturing robustness through attack curves instead of directly training for robustness, scalability of predictive tasks, and transferability of predictive capabilities. In this paper, we address these challenges by designing a convolutional neural networks (CNN) model with spatial pyramid pooling networks (SPP-net), adapting existing evaluation metrics, redesigning the attack modes, introducing appropriate filtering rules, and incorporating the value of robustness as training data. The results demonstrate the thoroughness of the proposed CNN framework in addressing the challenges of high computational time across various network types, failure component types and failure scenarios. However, the performance of the proposed CNN model varies: for evaluation tasks that are consistent with the trained network type, the proposed CNN model consistently achieves accurate evaluations of both attack curves and robustness values across all removal scenarios. When the predicted network type differs from the trained network, the CNN model still demonstrates favorable performance in the scenario of random node failure, showcasing its scalability and performance transferability. Nevertheless, the performance falls short of expectations in other removal scenarios. This observed scenario-sensitivity in the evaluation of network features has been overlooked in previous studies and necessitates further attention and optimization. Lastly, we discuss important unresolved questions and further investigation.
</details>
<details>
<summary>摘要</summary>
Traditionalmente, la evaluación de la robustez de redes complejas ha requerido simulaciones tiempo-consumidoras y prácticas imposibles. ¡Felizmente, la aprendizaje automático ofrece una nueva vía para abordar este desafío! Sin embargo, varias cuestiones clave aún no se han resuelto, como el desempeño en escenarios de eliminación de nodos más generales, capturar la robustez a través de curvas de ataques en lugar de entrenar directamente por robustez, la escalabilidad de tareas predictivas y la transferencia de habilidades predictivas.En este artículo, abordamos estos desafíos mediante el diseño de una red neuronal convolucional (CNN) con redes de pooling espiral (SPP-net), adaptando métricas de evaluación existentes, rediseñando los modos de ataque, estableciendo reglas de filtrado adecuadas e incorporando el valor de la robustez como datos de entrenamiento. Los resultados demuestran la thoroughness del marco de CNN propuesto en abordar los desafíos de tiempo de cálculo alto en diversas redes y escenarios de fracaso. Sin embargo, el rendimiento de la CNN propuesta varía: en tareas de evaluación consistentes con el tipo de red entrenada, la CNN consiste en evaluaciones precisas de curvas de ataques y valores de robustez en todos los escenarios de eliminación. Cuando la red predicted difiere de la red entrenada, la CNN aún demuestra un rendimiento favorable en el escenario de fracaso aleatorio de nodos, lo que muestra su escalabilidad y transferencia de rendimiento. Sin embargo, el rendimiento no cumple con las expectativas en otros escenarios de eliminación, lo que ha sido pasado por alto en estudios anteriores y requiere más atención y optimización.Finalmente, discutimos preguntas importantes sin resolver y investigaciones adicionales.
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-Algorithm-for-Nonstationary-Low-Rank-MDPs"><a href="#Provably-Efficient-Algorithm-for-Nonstationary-Low-Rank-MDPs" class="headerlink" title="Provably Efficient Algorithm for Nonstationary Low-Rank MDPs"></a>Provably Efficient Algorithm for Nonstationary Low-Rank MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05471">http://arxiv.org/abs/2308.05471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Cheng, Jing Yang, Yingbin Liang</li>
<li>for: 非站台Markov决策过程（MDPs）在实际应用中模型了许多真实世界问题，因此吸引了广泛的研究兴趣。然而， литера图中关于非站台MDPs的理论研究主要集中在表格和线性（混合）MDPs上，这些模型不能捕捉深度学习RL中的未知表示。本文是首次研究非站台RL在 episodic low-rank MDPs 上，其中转移函数和奖励函数可能随时间变化，并且low-rank模型包含未知表示。</li>
<li>methods: 我们首先提出了一种参数 dependent policy 优化算法 called PORTAL，然后改进PORTAL到其参数自由版本Ada-PORTAL，可以在不知道非站台性的情况下自动调整参数。</li>
<li>results: 我们提供了两个算法的平均动态下optimality gap 上界，显示如果非站台性不太大，然后PORTAL和Ada-PORTAL在样本复杂度为多项式幂的情况下可以实现任意小的平均动态下optimality gap。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) under changing environment models many real-world applications via nonstationary Markov Decision Processes (MDPs), and hence gains considerable interest. However, theoretical studies on nonstationary MDPs in the literature have mainly focused on tabular and linear (mixture) MDPs, which do not capture the nature of unknown representation in deep RL. In this paper, we make the first effort to investigate nonstationary RL under episodic low-rank MDPs, where both transition kernels and rewards may vary over time, and the low-rank model contains unknown representation in addition to the linear state embedding function. We first propose a parameter-dependent policy optimization algorithm called PORTAL, and further improve PORTAL to its parameter-free version of Ada-PORTAL, which is able to tune its hyper-parameters adaptively without any prior knowledge of nonstationarity. For both algorithms, we provide upper bounds on the average dynamic suboptimality gap, which show that as long as the nonstationarity is not significantly large, PORTAL and Ada-PORTAL are sample-efficient and can achieve arbitrarily small average dynamic suboptimality gap with polynomial sample complexity.
</details>
<details>
<summary>摘要</summary>
“强化学习（RL）在不同环境模型下多种实际应用，特别是非站ARY Markov Decision Processes（MDPs），因此受到了广泛关注。然而，现有的理论研究中的非站ARY MDPs主要集中在表格和线性（混合）MDPs上，这些模型不能捕捉深度RL中的未知表示。本文是第一次 investigate nonstationary RL under episodic low-rank MDPs，其中过程权重和奖励可能随时间变化，低维模型包含未知表示。我们首先提出了一种参数依赖的策略优化算法 called PORTAL，然后进一步改进了 PORTAL 为其参数自由版本 Ada-PORTAL，可以适应不同的非站ARY度。我们为这两种算法提供了平均动态落差的Upper bound，显示在非站ARY度不太大时，PORTAL 和 Ada-PORTAL 是可靠的，可以在有限样本复杂度下实现平均动态落差的任意小化。”Note: Simplified Chinese is used here, which is a standardized form of Chinese that is widely used in mainland China and other parts of the world. The translation is written in the formal style, which is appropriate for academic papers.
</details></li>
</ul>
<hr>
<h2 id="mathcal-G-2Pxy-Generative-Open-Set-Node-Classification-on-Graphs-with-Proxy-Unknowns"><a href="#mathcal-G-2Pxy-Generative-Open-Set-Node-Classification-on-Graphs-with-Proxy-Unknowns" class="headerlink" title="$\mathcal{G}^2Pxy$: Generative Open-Set Node Classification on Graphs with Proxy Unknowns"></a>$\mathcal{G}^2Pxy$: Generative Open-Set Node Classification on Graphs with Proxy Unknowns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05463">http://arxiv.org/abs/2308.05463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qin Zhang, Zelin Shi, Xiaolin Zhang, Xiaojun Chen, Philippe Fournier-Viger, Shirui Pan</li>
<li>for: 本研究旨在提出一种新的开放集分类方法，以便在无知类信息时进行分类。</li>
<li>methods: 该方法使用生成器来生成proxy未知节点，然后通过混合来准备未知类的分布。</li>
<li>results: 实验表明，该方法可以在开放集分类任务中具有优秀的效果，并且不受GNN架构的限制。<details>
<summary>Abstract</summary>
Node classification is the task of predicting the labels of unlabeled nodes in a graph. State-of-the-art methods based on graph neural networks achieve excellent performance when all labels are available during training. But in real-life, models are often applied on data with new classes, which can lead to massive misclassification and thus significantly degrade performance. Hence, developing open-set classification methods is crucial to determine if a given sample belongs to a known class. Existing methods for open-set node classification generally use transductive learning with part or all of the features of real unseen class nodes to help with open-set classification. In this paper, we propose a novel generative open-set node classification method, i.e. $\mathcal{G}^2Pxy$, which follows a stricter inductive learning setting where no information about unknown classes is available during training and validation. Two kinds of proxy unknown nodes, inter-class unknown proxies and external unknown proxies are generated via mixup to efficiently anticipate the distribution of novel classes. Using the generated proxies, a closed-set classifier can be transformed into an open-set one, by augmenting it with an extra proxy classifier. Under the constraints of both cross entropy loss and complement entropy loss, $\mathcal{G}^2Pxy$ achieves superior effectiveness for unknown class detection and known class classification, which is validated by experiments on benchmark graph datasets. Moreover, $\mathcal{G}^2Pxy$ does not have specific requirement on the GNN architecture and shows good generalizations.
</details>
<details>
<summary>摘要</summary>
Node 分类是指预测图中没有标签的节点的标签。现有的方法基于图神经网络可以在所有标签可用于训练时达到极优性。但在实际应用中，模型经常应用于具有新的类型的数据，可能导致大规模的误分类，从而很大地降低性能。因此，开发开放集分类方法是关键的，以确定给定的样本是否属于已知类。现有的开放集节点分类方法通常使用散度学习，使用真实未看过类节点的一部分或所有特征来帮助开放集分类。在这篇论文中，我们提出了一种新的生成型开放集节点分类方法，即 $\mathcal{G}^2Pxy$，它遵循一个更严格的散度学习设定，在训练和验证过程中不可能获得未知类的信息。通过混合来生成两种类型的代理未知节点，即间类未知代理和外部未知代理，以效率地预测新类的分布。使用生成的代理节点，一个关闭集分类器可以转换成一个开放集分类器，通过增加一个额外的代理分类器。在跨度 Entropy 损失和补偿 Entropy 损失的约束下， $\mathcal{G}^2Pxy$ 实现了对未知类探测和已知类分类的超越性，经过实验 validate 在图数据集上。此外， $\mathcal{G}^2Pxy$ 不受 GNN 架构的限制，并且具有良好的通用性。
</details></li>
</ul>
<hr>
<h2 id="A-Forecaster’s-Review-of-Judea-Pearl’s-Causality-Models-Reasoning-and-Inference-Second-Edition-2009"><a href="#A-Forecaster’s-Review-of-Judea-Pearl’s-Causality-Models-Reasoning-and-Inference-Second-Edition-2009" class="headerlink" title="A Forecaster’s Review of Judea Pearl’s Causality: Models, Reasoning and Inference, Second Edition, 2009"></a>A Forecaster’s Review of Judea Pearl’s Causality: Models, Reasoning and Inference, Second Edition, 2009</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05451">http://arxiv.org/abs/2308.05451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Li</li>
<li>for: 本文是一篇评论文章，涵盖了 Judy Pearl 的原始 causality 书籍第二版（2009）中的主要话题。</li>
<li>methods: 本文提出了一种简单易于遵循的 causal inference 策略，并在预测enario中进行了示例。</li>
<li>results: 本文讨论了在预测中 causal inference 的一些潜在利益和挑战，以及如何在不同的预测enario中 estimate causal effects。<details>
<summary>Abstract</summary>
With the big popularity and success of Judea Pearl's original causality book, this review covers the main topics updated in the second edition in 2009 and illustrates an easy-to-follow causal inference strategy in a forecast scenario. It further discusses some potential benefits and challenges for causal inference with time series forecasting when modeling the counterfactuals, estimating the uncertainty and incorporating prior knowledge to estimate causal effects in different forecasting scenarios.
</details>
<details>
<summary>摘要</summary>
根据朱德亚·珀尔的原始 causality 书的巨大受欢迎和成功，这篇评论介绍了第二版（2009年）中的主要话题，并提供了一个易于掌握的 causal inference 策略，用于预测enario。它还讨论了在模型 counterfactuals 时， causal inference 遇到的潜在利益和挑战，以及如何在不同的预测enario中 estimate  causal effects。
</details></li>
</ul>
<hr>
<h2 id="Explainable-AI-applications-in-the-Medical-Domain-a-systematic-review"><a href="#Explainable-AI-applications-in-the-Medical-Domain-a-systematic-review" class="headerlink" title="Explainable AI applications in the Medical Domain: a systematic review"></a>Explainable AI applications in the Medical Domain: a systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05411">http://arxiv.org/abs/2308.05411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicoletta Prentzas, Antonis Kakas, Constantinos S. Pattichis</li>
<li>for: 这篇论文旨在探讨医疗人工智能（AI）在医疗领域的应用，以及如何使用解释AI（XAI）解决方案来提高医疗决策支持系统的可靠性和可信worth。</li>
<li>methods: 这篇论文采用了一种系统性的文献综述方法，检索了过去几年发表的198篇有关医疗AI和XAI的研究论文，并进行了系统性的分析和总结。</li>
<li>results: 根据这篇论文的分析结果，以下是一些主要发现：（1）大多数解释AI技术是模型非依的，（2）深度学习模型在医疗AI中使用得更多，（3）解释是用于提高信任的方法，但很少有报道了医生参与的循环，（4）可视化和互动的用户界面更有用于理解解释和系统的建议。<details>
<summary>Abstract</summary>
Artificial Intelligence in Medicine has made significant progress with emerging applications in medical imaging, patient care, and other areas. While these applications have proven successful in retrospective studies, very few of them were applied in practice.The field of Medical AI faces various challenges, in terms of building user trust, complying with regulations, using data ethically.Explainable AI (XAI) aims to enable humans understand AI and trust its results. This paper presents a literature review on the recent developments of XAI solutions for medical decision support, based on a representative sample of 198 articles published in recent years. The systematic synthesis of the relevant articles resulted in several findings. (1) model-agnostic XAI techniques were mostly employed in these solutions, (2) deep learning models are utilized more than other types of machine learning models, (3) explainability was applied to promote trust, but very few works reported the physicians participation in the loop, (4) visual and interactive user interface is more useful in understanding the explanation and the recommendation of the system. More research is needed in collaboration between medical and AI experts, that could guide the development of suitable frameworks for the design, implementation, and evaluation of XAI solutions in medicine.
</details>
<details>
<summary>摘要</summary>
人工智能在医疗领域已经取得了 significiant 进步，其应用范围包括医疗影像、患者护理和其他领域。然而，这些应用在实践中并不多见。医疗领域的人工智能面临着多种挑战，包括建立用户信任、遵守法规和使用数据道德。可解释人工智能（XAI）旨在帮助人类理解人工智能和信任其结果。这篇文章提出了一种Literature Review，检查了最近几年发表的198篇文章，以获取最新的发展情况。系统性的分析这些文章所得到的结论包括以下几点：1. Model-agnostic XAI技术在这些解释中最常用。2. 深度学习模型比其他机器学习模型更常用。3. 解释的目的是促进信任，但很少有文章报道了医生参与的循环。4. 可视化和交互的用户界面更有用于理解解释和系统的建议。进一步的研究需要在医疗和人工智能专家之间合作，以开发适合医疗领域的XAI解释解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Assessment-of-Multi-view-fusion-learning-for-Crop-Classification"><a href="#A-Comparative-Assessment-of-Multi-view-fusion-learning-for-Crop-Classification" class="headerlink" title="A Comparative Assessment of Multi-view fusion learning for Crop Classification"></a>A Comparative Assessment of Multi-view fusion learning for Crop Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05407">http://arxiv.org/abs/2308.05407</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmenat/multiviewcropclassification">https://github.com/fmenat/multiviewcropclassification</a></li>
<li>paper_authors: Francisco Mena, Diego Arenas, Marlon Nuske, Andreas Dengel<br>for: 这个论文的目的是提出了多视图学习模型，以处理不同分辨率、大小和噪声等多种远程感知数据的复杂任务。methods: 这个论文使用了不同的多视图合并策略，包括输入级别合并、特征级别合并和卷积级别合并等。results: 论文表明，使用不同的多视图合并策略可以超过基于单个视图的模型和前期的合并策略。但是，不同的测试区域中，不同的方法可以取得最佳性能。<details>
<summary>Abstract</summary>
With a rapidly increasing amount and diversity of remote sensing (RS) data sources, there is a strong need for multi-view learning modeling. This is a complex task when considering the differences in resolution, magnitude, and noise of RS data. The typical approach for merging multiple RS sources has been input-level fusion, but other - more advanced - fusion strategies may outperform this traditional approach. This work assesses different fusion strategies for crop classification in the CropHarvest dataset. The fusion methods proposed in this work outperform models based on individual views and previous fusion methods. We do not find one single fusion method that consistently outperforms all other approaches. Instead, we present a comparison of multi-view fusion methods for three different datasets and show that, depending on the test region, different methods obtain the best performance. Despite this, we suggest a preliminary criterion for the selection of fusion methods.
</details>
<details>
<summary>摘要</summary>
With the rapidly increasing amount and diversity of remote sensing (RS) data sources, there is a strong need for multi-view learning modeling. This is a complex task when considering the differences in resolution, magnitude, and noise of RS data. The typical approach for merging multiple RS sources has been input-level fusion, but other - more advanced - fusion strategies may outperform this traditional approach. This work assesses different fusion strategies for crop classification in the CropHarvest dataset. The fusion methods proposed in this work outperform models based on individual views and previous fusion methods. We do not find one single fusion method that consistently outperforms all other approaches. Instead, we present a comparison of multi-view fusion methods for three different datasets and show that, depending on the test region, different methods obtain the best performance. Despite this, we suggest a preliminary criterion for the selection of fusion methods.Here's the translation in Traditional Chinese:有很多和多样化的远程感知（RS）数据源，需要多视角学习模型。这是一个复杂的任务，因为RS数据的分辨率、大小和噪声之间存在差异。传统的方法是输入级别 fusión，但是更先进的拟合策略可能会超越这种传统方法。这篇文章评估了不同的拟合策略，用于cropland classification in CropHarvest dataset。我们的方法超越了基于个体视角的模型和前一代的拟合方法。我们没有找到一个一直以来的拟合方法，可以在所有情况下表现最佳。相反，我们提供了不同数据集中的多视角拟合方法的比较，并显示，根据测试区域，不同的方法在不同的数据集中可以获得最佳性能。虽然如此，我们建议一种初步的选择标准，用于选择拟合方法。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Data-Scarcity-in-Optical-Matrix-Multiplier-Modeling-Using-Transfer-Learning"><a href="#Addressing-Data-Scarcity-in-Optical-Matrix-Multiplier-Modeling-Using-Transfer-Learning" class="headerlink" title="Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning"></a>Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11630">http://arxiv.org/abs/2308.11630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Cem, Ognjen Jovanovic, Siqi Yan, Yunhong Ding, Darko Zibar, Francesco Da Ros</li>
<li>for: 用 transferred learning 解决光学矩阵乘算器中的数据罕见性问题，即使使用少量实验数据进行模型训练。</li>
<li>methods: 采用先训练模型使用生成自 menos accurate analytical model的 sintetic数据，然后精度调整使用实验数据。</li>
<li>results: 该方法可以减少模型错误，比使用analytical模型或独立的 neural network模型在数据有限情况下。使用正则化技术和ensemble averaging，实现 &lt;1 dB的Root-Mean-Square Error在实际中实现了矩阵加 weights。<details>
<summary>Abstract</summary>
We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve < 1 dB root-mean-square error on the matrix weights implemented by a photonic chip while using only 25% of the available data.
</details>
<details>
<summary>摘要</summary>
我团队在 meshes 基于光学矩阵乘数器中使用传输学习来解决数据稀缺问题，我们的方法是先使用基于analytical模型生成的synthetic数据进行预训练，然后使用实验数据进行微调。我们的调查表明，这种方法可以相比analytical模型或独立的神经网络模型在数据有限情况下获得显著的减少模型错误。通过使用常规化技术和ensemble平均，我们在 photonic chip 上实现了 < 1 dB 的平均平方误差，只使用25%的数据。
</details></li>
</ul>
<hr>
<h2 id="Product-Review-Image-Ranking-for-Fashion-E-commerce"><a href="#Product-Review-Image-Ranking-for-Fashion-E-commerce" class="headerlink" title="Product Review Image Ranking for Fashion E-commerce"></a>Product Review Image Ranking for Fashion E-commerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05390">http://arxiv.org/abs/2308.05390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangeet Jaiswal, Dhruv Patel, Sreekanth Vempati, Konduru Saiswaroop</li>
<li>for: 本研究旨在提出一种简单 yet effective的训练方法，以排名用户生成内容（UGC）中的图像。</li>
<li>methods: 我们使用Myntra（印度主要的时尚电商公司）的Studio Posts和高度参与（upvotes&#x2F;downvotes）UGC图像构成了我们的起点，并使用选择的扭曲技术将图像的质量提高到与差的图像水平。我们训练我们的网络，以便将差质图像排名在低于高质图像之前。</li>
<li>results: 我们的提议方法在两个纪录（相关系数和准确率）上超越基线模型，具有明显的优势。<details>
<summary>Abstract</summary>
In a fashion e-commerce platform where customers can't physically examine the products on their own, being able to see other customers' text and image reviews of the product is critical while making purchase decisions. Given the high reliance on these reviews, over the years we have observed customers proactively sharing their reviews. With an increase in the coverage of User Generated Content (UGC), there has been a corresponding increase in the number of customer images. It is thus imperative to display the most relevant images on top as it may influence users' online shopping choices and behavior. In this paper, we propose a simple yet effective training procedure for ranking customer images. We created a dataset consisting of Myntra (A Major Indian Fashion e-commerce company) studio posts and highly engaged (upvotes/downvotes) UGC images as our starting point and used selected distortion techniques on the images of the above dataset to bring their quality at par with those of bad UGC images. We train our network to rank bad-quality images lower than high-quality ones. Our proposed method outperforms the baseline models on two metrics, namely correlation coefficient, and accuracy, by substantial margins.
</details>
<details>
<summary>摘要</summary>
在一个无法购买者实际检查产品的电商平台上，可见其他客户的文本和图像评论对于购买决策是非常重要的。随着用户生成内容的覆盖率的增加，我们在年进行了评论的投稿。随着用户生成内容的增加，图像的数量也随之增加。因此，显示最相关的图像在前面是非常重要的，因为它们可能影响用户的在线购物选择和行为。在这篇论文中，我们提出了一种简单 yet 有效的训练方法，用于排序客户图像。我们使用 Myntra（印度主要的时尚电商公司）的Studio文章和高度参与度（投票/踢票）的用户生成内容图像作为我们的起点，并使用选择的扭曲技术来使图像的质量与坏用户生成内容图像相匹配。我们训练我们的网络，以便将坏质量图像排在低于高质量图像之前。我们的提议方法在两个纪录 coefficient和准确率两个纪录上，与基准模型相比，均有substantial的优势。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-LLMs-a-Survey-and-Guideline-for-Evaluating-Large-Language-Models’-Alignment"><a href="#Trustworthy-LLMs-a-Survey-and-Guideline-for-Evaluating-Large-Language-Models’-Alignment" class="headerlink" title="Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment"></a>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05374">http://arxiv.org/abs/2308.05374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li</li>
<li>for: 本研究的目的是为了提供关键维度的评估语言模型可靠性的报告，以便实现可靠性的语言模型在各种应用中的顺利部署。</li>
<li>methods: 本研究使用了一种全面的评估方法，包括7个主要类别和29个子类别，以评估语言模型的可靠性。</li>
<li>results: 研究发现，更加适应的模型通常在总可靠性方面表现更好，但是对不同的可靠性类别的有效性强度各不相同。这指出了需要进行更细化的分析、测试和改进，以确保语言模型的适应性和伦理性。<details>
<summary>Abstract</summary>
Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.
</details>
<details>
<summary>摘要</summary>
ensure alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.
</details></li>
</ul>
<hr>
<h2 id="Flexible-Isosurface-Extraction-for-Gradient-Based-Mesh-Optimization"><a href="#Flexible-Isosurface-Extraction-for-Gradient-Based-Mesh-Optimization" class="headerlink" title="Flexible Isosurface Extraction for Gradient-Based Mesh Optimization"></a>Flexible Isosurface Extraction for Gradient-Based Mesh Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05371">http://arxiv.org/abs/2308.05371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, Jun Gao</li>
<li>for: 本文考虑了梯度基本的网格优化，通过 représenter mesh 为一个浮点场的iso surface，以便在摄grammetry、生成模型和反向物理等应用中进行优化。</li>
<li>methods: 我们引入了flexi Cubes，一种特定的iso surface表示方法，用于优化未知的网格，以达到几何、视觉和物理目标。我们的主要思想是通过引入地方的参数，使得mesh几何和连接性可以进行当地灵活调整。这些参数通过自动微分升级来与下游任务的对应scalar场一起更新。</li>
<li>results: 我们的实验表明，flexi Cubes 可以在synthetic benchmarks和实际应用中提供显著改善的网格质量和几何准确性。<details>
<summary>Abstract</summary>
This work considers gradient-based mesh optimization, where we iteratively optimize for a 3D surface mesh by representing it as the isosurface of a scalar field, an increasingly common paradigm in applications including photogrammetry, generative modeling, and inverse physics. Existing implementations adapt classic isosurface extraction algorithms like Marching Cubes or Dual Contouring; these techniques were designed to extract meshes from fixed, known fields, and in the optimization setting they lack the degrees of freedom to represent high-quality feature-preserving meshes, or suffer from numerical instabilities. We introduce FlexiCubes, an isosurface representation specifically designed for optimizing an unknown mesh with respect to geometric, visual, or even physical objectives. Our main insight is to introduce additional carefully-chosen parameters into the representation, which allow local flexible adjustments to the extracted mesh geometry and connectivity. These parameters are updated along with the underlying scalar field via automatic differentiation when optimizing for a downstream task. We base our extraction scheme on Dual Marching Cubes for improved topological properties, and present extensions to optionally generate tetrahedral and hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on both synthetic benchmarks and real-world applications, showing that it offers significant improvements in mesh quality and geometric fidelity.
</details>
<details>
<summary>摘要</summary>
To address this limitation, we introduce FlexiCubes, a new isosurface representation specifically designed for optimizing an unknown mesh. Our key insight is to introduce additional carefully-chosen parameters into the representation, which allow for local flexible adjustments to the extracted mesh geometry and connectivity. These parameters are updated along with the underlying scalar field via automatic differentiation when optimizing for a downstream task.We base our extraction scheme on Dual Marching Cubes for improved topological properties, and present extensions to optionally generate tetrahedral and hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on both synthetic benchmarks and real-world applications, showing that it offers significant improvements in mesh quality and geometric fidelity.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-aided-Computer-Architecture-Design-for-CNN-Inferencing-Systems"><a href="#Machine-Learning-aided-Computer-Architecture-Design-for-CNN-Inferencing-Systems" class="headerlink" title="Machine Learning aided Computer Architecture Design for CNN Inferencing Systems"></a>Machine Learning aided Computer Architecture Design for CNN Inferencing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05364">http://arxiv.org/abs/2308.05364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher A. Metz</li>
<li>for: 这篇论文的目的是优化机器学习（ML）算法的效率计算，以应对智能交通、物联网（IoT）和边缘 Computing 等新兴技术的需求。</li>
<li>methods: 本论文使用了 Design Space Exploration（DSE）方法来选择最适合的加速器，并提出了一种快速和精准的预测方法来估计 CNN 的电力和性能。</li>
<li>results: 本论文的预测方法可以实现 MapE 的预测精度，对于 CNN 的推论运算可以提供快速且精准的电力和性能估计。<details>
<summary>Abstract</summary>
Efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing the necessity for numerous prototypes. This saves time and money while also improving the time-to-market period.
</details>
<details>
<summary>摘要</summary>
efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing the necessity for numerous prototypes. This saves time and money while also improving the time-to-market period.Here's the translation in Traditional Chinese as well:efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing the necessity for numerous prototypes. This saves time and money while also improving the time-to-market period.
</details></li>
</ul>
<hr>
<h2 id="FINER-Enhancing-State-of-the-art-Classifiers-with-Feature-Attribution-to-Facilitate-Security-Analysis"><a href="#FINER-Enhancing-State-of-the-art-Classifiers-with-Feature-Attribution-to-Facilitate-Security-Analysis" class="headerlink" title="FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis"></a>FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05362">http://arxiv.org/abs/2308.05362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/e0hyl/finer-explain">https://github.com/e0hyl/finer-explain</a></li>
<li>paper_authors: Yiling He, Jian Lou, Zhan Qin, Kui Ren</li>
<li>for: 这篇论文的目的是提出一种高精度和高可读性的风险检测类фикатор解释框架，以便减少安全分析人员的工作负担。</li>
<li>methods: 该论文使用了特征参与（FA）方法来解释深度学习模型，并通过自适应task知识调整和集成FA方法来提高解释的智能性。</li>
<li>results: 对于风险检测任务，FINER可以提供高精度和高可读性的解释，并且在恶意软件分析中表现更高效。<details>
<summary>Abstract</summary>
Deep learning classifiers achieve state-of-the-art performance in various risk detection applications. They explore rich semantic representations and are supposed to automatically discover risk behaviors. However, due to the lack of transparency, the behavioral semantics cannot be conveyed to downstream security experts to reduce their heavy workload in security analysis. Although feature attribution (FA) methods can be used to explain deep learning, the underlying classifier is still blind to what behavior is suspicious, and the generated explanation cannot adapt to downstream tasks, incurring poor explanation fidelity and intelligibility. In this paper, we propose FINER, the first framework for risk detection classifiers to generate high-fidelity and high-intelligibility explanations. The high-level idea is to gather explanation efforts from model developer, FA designer, and security experts. To improve fidelity, we fine-tune the classifier with an explanation-guided multi-task learning strategy. To improve intelligibility, we engage task knowledge to adjust and ensemble FA methods. Extensive evaluations show that FINER improves explanation quality for risk detection. Moreover, we demonstrate that FINER outperforms a state-of-the-art tool in facilitating malware analysis.
</details>
<details>
<summary>摘要</summary>
深度学习分类器在不同的风险检测应用中实现了状态的最佳性能。它们探索了丰富的 semantic 表示，并被认为可以自动发现风险行为。然而，由于lack of transparency，risk 行为的semantic不能被传递给下游安全专家进行安全分析，从而增加了安全分析的重量。虽然 feature attribution（FA）方法可以用来解释深度学习，但下游分类器仍然无法了解哪些行为是可疑的，并且生成的解释无法适应下游任务，导致低效解释准确性和可读性。在这篇论文中，我们提出了 FINER，第一个用于风险检测分类器生成高准确性和高可读性解释的框架。高级想法是将解释努力集中于模型开发者、FA设计者和安全专家。为了提高准确性，我们使用解释指导多任务学习策略来练化分类器。为了提高可读性，我们利用任务知识来调整和组合 FA 方法。广泛评估表明，FINER 可以提高风险检测解释质量。此外，我们还证明了 FINER 可以超越当前领域的一个状态的工具在攻击分析方面帮助更好。
</details></li>
</ul>
<hr>
<h2 id="Preemptive-Detection-of-Fake-Accounts-on-Social-Networks-via-Multi-Class-Preferential-Attachment-Classifiers"><a href="#Preemptive-Detection-of-Fake-Accounts-on-Social-Networks-via-Multi-Class-Preferential-Attachment-Classifiers" class="headerlink" title="Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers"></a>Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05353">http://arxiv.org/abs/2308.05353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Breuer, Nazanin Khosravani, Michael Tingley, Bradford Cottel<br>for:这篇论文描述了一种新的算法 called Preferential Attachment k-class Classifier (PreAttacK)，用于检测社交网络中的假账户。这些算法在过去几年中已经达到了高精度水平，但是它们通常是通过利用假账户的朋友关系或它们与其他人分享的内容来实现的。PreAttacK是这些方法的巨大变革。methods:作者们提供了一些初次分布分析，描述了新的假账户如何在社交网络中首次请求朋友关系。他们发现，even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth。作者们使用这个模型来 derive a new algorithm，PreAttacK。results:作者们证明了，在相关的问题实例中，PreAttacK可以 Near-optimally approximate the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts’ (not-yet-answered) friend requests。这是首次提供了对新用户的假账户检测的证明保证，而不需要强烈的同化假设。这种原则化的方法也使得PreAttacK成为了唯一具有证明保证的算法，在全球Facebook网络上实现了状态当前的最佳性能。<details>
<summary>Abstract</summary>
In this paper, we describe a new algorithm called Preferential Attachment k-class Classifier (PreAttacK) for detecting fake accounts in a social network. Recently, several algorithms have obtained high accuracy on this problem. However, they have done so by relying on information about fake accounts' friendships or the content they share with others--the very things we seek to prevent. PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to request friends after joining a major network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth. We use this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts' (not-yet-answered) friend requests. These are the first provable guarantees for fake account detection that apply to new users, and that do not require strong homophily assumptions. This principled approach also makes PreAttacK the only algorithm with provable guarantees that obtains state-of-the-art performance on new users on the global Facebook network, where it converges to AUC=0.9 after new users send + receive a total of just 20 not-yet-answered friend requests. For comparison, state-of-the-art benchmarks do not obtain this AUC even after observing additional data on new users' first 100 friend requests. Thus, unlike mainstream algorithms, PreAttacK converges before the median new fake account has made a single friendship (accepted friend request) with a human.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了一种新的算法 called Preferential Attachment k-class Classifier (PreAttacK)，用于检测社交网络中的假账户。最近，一些算法已经在这个问题上取得了高准确率，但是它们通常是通过利用假账户的朋友关系或它们与其他人共享的内容来实现的—— precisamente el que we seek to prevent. PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to request friends after joining a major network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth. We use this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts' (not-yet-answered) friend requests. These are the first provable guarantees for fake account detection that apply to new users, and that do not require strong homophily assumptions. This principled approach also makes PreAttacK the only algorithm with provable guarantees that obtains state-of-the-art performance on new users on the global Facebook network, where it converges to AUC=0.9 after new users send + receive a total of just 20 not-yet-answered friend requests. For comparison, state-of-the-art benchmarks do not obtain this AUC even after observing additional data on new users' first 100 friend requests. Thus, unlike mainstream algorithms, PreAttacK converges before the median new fake account has made a single friendship (accepted friend request) with a human.
</details></li>
</ul>
<hr>
<h2 id="RTLLM-An-Open-Source-Benchmark-for-Design-RTL-Generation-with-Large-Language-Model"><a href="#RTLLM-An-Open-Source-Benchmark-for-Design-RTL-Generation-with-Large-Language-Model" class="headerlink" title="RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model"></a>RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05345">http://arxiv.org/abs/2308.05345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Lu, Shang Liu, Qijun Zhang, Zhiyao Xie</li>
<li>for: 本研究旨在提出一个开源 benchmark，用于使用自然语言指令生成适用于快速硬件设计的 RTL 架构。</li>
<li>methods: 本研究使用了 GPT-3.5 进行自然语言指令生成，并提出了一种名为 “自然语言观察”的技术，可以帮助提高 GPT-3.5 的性能。</li>
<li>results: 本研究通过使用自然语言指令生成 RTL 架构，并通过三个进步目标进行评估，包括 syntax goal、functionality goal 和 design quality goal。结果显示，使用自然语言指令可以实现高质量的 RTL 架构生成。<details>
<summary>Abstract</summary>
Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisingly effective prompt engineering technique named self-planning, which proves to significantly boost the performance of GPT-3.5 in our proposed benchmark.
</details>
<details>
<summary>摘要</summary>
受大语言模型（LLM）如ChatGPT的成功启发，研究人员开始探索将LLM应用于快速硬件设计，如通过自然语言指令生成设计RTL。然而，现有的工作都是相对较少规模和自身提出的设计，具有许多缺陷，不能准确地评估不同LLM解决方案之间的比较。此外，许多前期工作只关注设计正确性，而忽略生成设计RTL的设计质量。在这种情况下，我们提出了一个开源的标准套件名为RTLLM，用于生成设计RTL通过自然语言指令。通过系统地评估自动生成的设计RTL，我们提出了三个进攻性目标，即语法目标、功能目标和设计质量目标。这个套件可以自动提供任何给定LLM解决方案的量化评估。此外，我们还提出了一种易于使用却有效的自我规划技术，名为自我规划，可以帮助GPT-3.5在我们提出的套件中表现出色。
</details></li>
</ul>
<hr>
<h2 id="OpenProteinSet-Training-data-for-structural-biology-at-scale"><a href="#OpenProteinSet-Training-data-for-structural-biology-at-scale" class="headerlink" title="OpenProteinSet: Training data for structural biology at scale"></a>OpenProteinSet: Training data for structural biology at scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05326">http://arxiv.org/abs/2308.05326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aqlaboratory/openfold">https://github.com/aqlaboratory/openfold</a></li>
<li>paper_authors: Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan, Lukas Jarosch, Daniel Berenberg, Ian Fisk, Andrew M. Watkins, Stephen Ra, Richard Bonneau, Mohammed AlQuraishi</li>
<li>for: 这个论文主要是为了提供一个大规模的蛋白质多序列对 alignment（MSA）数据集，以便用于蛋白质结构设计、蛋白质功能预测等 bioinformatics 任务。</li>
<li>methods: 这个论文使用了 transformers 来直接对大量的 raw MSA 进行attend，以及对 Protein Data Bank 中的结构同源者进行关联。</li>
<li>results: 该论文引入了 OpenProteinSet，一个开源的蛋白质多序列对数据集，包括 более than 16 万个 MSA，以及与 Protein Data Bank 中的结构同源者和 AlphaFold2 蛋白质结构预测结果。<details>
<summary>Abstract</summary>
Multiple sequence alignments (MSAs) of proteins encode rich biological information and have been workhorses in bioinformatic methods for tasks like protein design and protein structure prediction for decades. Recent breakthroughs like AlphaFold2 that use transformers to attend directly over large quantities of raw MSAs have reaffirmed their importance. Generation of MSAs is highly computationally intensive, however, and no datasets comparable to those used to train AlphaFold2 have been made available to the research community, hindering progress in machine learning for proteins. To remedy this problem, we introduce OpenProteinSet, an open-source corpus of more than 16 million MSAs, associated structural homologs from the Protein Data Bank, and AlphaFold2 protein structure predictions. We have previously demonstrated the utility of OpenProteinSet by successfully retraining AlphaFold2 on it. We expect OpenProteinSet to be broadly useful as training and validation data for 1) diverse tasks focused on protein structure, function, and design and 2) large-scale multimodal machine learning research.
</details>
<details>
<summary>摘要</summary>
多个序列对Alignment（MSA）的蛋白质编码着丰富的生物信息，在生物信息学方法中作为保持者工具用于蛋白质设计和蛋白质结构预测已经有几十年的历史。最近的突破，如AlphaFold2，使用转换器直接在大量的原始MSA上进行访问，重新确认了它们的重要性。生成MSA的计算极其计算昂贵，然而，与AlphaFold2训练所用数据集相同的大规模数据集没有被研究者社区公开，这阻碍了蛋白质机器学习的进步。为了解决这个问题，我们介绍OpenProteinSet，一个开源的蛋白质多序列对Alignment资源，包括More than 16 million MSAs，与蛋白质数据库相关的结构同源者，以及AlphaFold2蛋白质结构预测。我们在OpenProteinSet上成功重新训练AlphaFold2，并预计OpenProteinSet将广泛用于蛋白质结构、功能和设计的多种任务，以及大规模多Modal机器学习研究。
</details></li>
</ul>
<hr>
<h2 id="Homophily-enhanced-Structure-Learning-for-Graph-Clustering"><a href="#Homophily-enhanced-Structure-Learning-for-Graph-Clustering" class="headerlink" title="Homophily-enhanced Structure Learning for Graph Clustering"></a>Homophily-enhanced Structure Learning for Graph Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05309">http://arxiv.org/abs/2308.05309</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/galogm/hole">https://github.com/galogm/hole</a></li>
<li>paper_authors: Ming Gu, Gaoming Yang, Sheng Zhou, Ning Ma, Jiawei Chen, Qiaoyu Tan, Meihan Liu, Jiajun Bu</li>
<li>for: 这个论文的目的是提出一种基于图 neural network 的图 clustering 方法，以提高图分类的性能。</li>
<li>methods: 该方法使用两种结构学习模块：幂等相关度估计和群体相关简化，以提高 GNN 的性能。</li>
<li>results: 对七种不同类型和规模的测试数据集进行了广泛的实验，并与状态的基eline进行比较，得到了 HoLe 的超越性。<details>
<summary>Abstract</summary>
Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve GNNs and clustering outcomes. To realize this objective, we develop two clustering-oriented structure learning modules, i.e., hierarchical correlation estimation and cluster-aware sparsification. The former module enables a more accurate estimation of pairwise node relationships by leveraging guidance from latent and clustering spaces, while the latter one generates a sparsified structure based on the similarity matrix and clustering assignments. Additionally, we devise a joint optimization approach alternating between training the homophily-enhanced structure learning and GNN-based clustering, thereby enforcing their reciprocal effects. Extensive experiments on seven benchmark datasets of various types and scales, across a range of clustering metrics, demonstrate the superiority of HoLe against state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
GRaph clustering是图像分析的基本任务，而最近的GRaph Neural Networks（GNN）的进步已经显示出了惊人的成果。尽管现有的GNN基于图 clustering方法已经取得了成功，但它们通常忽略图structure的质量，这导致了表现不佳。图structure学习可以改善输入图的精度，但以往的图structure学习尝试都是在监督学习设置下进行的，因此无法直接应用于我们的具体 clustering 任务。为了填补这个差距，我们提出了一种新的方法called homophily-enhanced structure learning for graph clustering（HoLe）。我们的动机来自于观察，通过轻度提高图中 Node 之间的同性度，可以显著改善 GNN 和 clustering 结果。为实现这个目标，我们开发了两种 clustering-oriented structure learning模块：层次相关度估计和集群意向简化。前者模块可以更准确地估计 Node 之间的对应关系，通过利用幽默和 clustering 空间的指导，而后者模块可以基于对应矩阵和 clustering 分配生成一个简化的结构。此外，我们提出了一种联合优化方法，通过在GNN-based clustering和homophily-enhanced structure learning之间交互训练，以便强制这两者之间的相互作用。广泛的实验表明，HoLe 在七种不同类型和规模的数据集上，以及不同的 clustering 维度上，均超过了状态艺术基eline。
</details></li>
</ul>
<hr>
<h2 id="From-CNN-to-Transformer-A-Review-of-Medical-Image-Segmentation-Models"><a href="#From-CNN-to-Transformer-A-Review-of-Medical-Image-Segmentation-Models" class="headerlink" title="From CNN to Transformer: A Review of Medical Image Segmentation Models"></a>From CNN to Transformer: A Review of Medical Image Segmentation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05305">http://arxiv.org/abs/2308.05305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjian Yao, Jiajun Bai, Wei Liao, Yuheng Chen, Mengjuan Liu, Yao Xie</li>
<li>for: 这篇论文的目的是对最近几年内的医疗影像分类模型进行评估和探讨。</li>
<li>methods: 这篇论文使用的方法包括U-Net和其变种，以及基于传播模型的TransUNet。</li>
<li>results: 论文评估了四种最具代表性的医疗影像分类模型，并量化评估它们在两个标准资料集（i.e., 肺结核X光和 ovary tumor）上的性能。<details>
<summary>Abstract</summary>
Medical image segmentation is an important step in medical image analysis, especially as a crucial prerequisite for efficient disease diagnosis and treatment. The use of deep learning for image segmentation has become a prevalent trend. The widely adopted approach currently is U-Net and its variants. Additionally, with the remarkable success of pre-trained models in natural language processing tasks, transformer-based models like TransUNet have achieved desirable performance on multiple medical image segmentation datasets. In this paper, we conduct a survey of the most representative four medical image segmentation models in recent years. We theoretically analyze the characteristics of these models and quantitatively evaluate their performance on two benchmark datasets (i.e., Tuberculosis Chest X-rays and ovarian tumors). Finally, we discuss the main challenges and future trends in medical image segmentation. Our work can assist researchers in the related field to quickly establish medical segmentation models tailored to specific regions.
</details>
<details>
<summary>摘要</summary>
医学图像分割是医学图像分析中非常重要的步骤，特别是为了高效的疾病诊断和治疗。深度学习在图像分割方面的应用已成为一种普遍的趋势。目前最广泛采用的是U-Net和其变体。此外，由于自然语言处理任务中预训练模型的出色成绩，如Transformer基于的TransUNet模型在多个医学图像分割数据集上实现了满意的表现。本文对最近几年内最具代表性的四种医学图像分割模型进行了抽查，分析了这些模型的特点，并对两个标准数据集（即肺部X射线和卵巢肿瘤）进行了量化评估。最后，我们讨论了医学图像分割中的主要挑战和未来趋势。本文可以帮助相关领域的研究人员快速设置适应特定地区的医学分割模型。
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Robust-Decentralized-Stochastic-Optimization-with-Stochastic-Gradient-Noise-Independent-Learning-Error"><a href="#Byzantine-Robust-Decentralized-Stochastic-Optimization-with-Stochastic-Gradient-Noise-Independent-Learning-Error" class="headerlink" title="Byzantine-Robust Decentralized Stochastic Optimization with Stochastic Gradient Noise-Independent Learning Error"></a>Byzantine-Robust Decentralized Stochastic Optimization with Stochastic Gradient Noise-Independent Learning Error</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05292">http://arxiv.org/abs/2308.05292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Peng, Weiyu Li, Qing Ling</li>
<li>for: 这paper研究了一种分布式网络上的奥尔拜托抗衡梯度下降优化方法，该方法在每个代理都 periodic communication with its neighbors，并使用梯度下降来更新自己的本地模型。</li>
<li>methods: 该paper使用了两种抗衡方法，即Stochastic Average Gradient Algorithm (SAGA)和Loopless Stochastic Variance-Reduced Gradient (LSVRG)，以消除梯度下降噪声的负面影响。</li>
<li>results: 该paper的两种方法BRAVO-SAGA和BRAVO-LSVRG都能同时实现线性减少速度和梯度下降噪声独立的学习误差，这些学习误差是对一类基于总变量（TV）范数regularization和随机下降更新的方法所optimal。<details>
<summary>Abstract</summary>
This paper studies Byzantine-robust stochastic optimization over a decentralized network, where every agent periodically communicates with its neighbors to exchange local models, and then updates its own local model by stochastic gradient descent (SGD). The performance of such a method is affected by an unknown number of Byzantine agents, which conduct adversarially during the optimization process. To the best of our knowledge, there is no existing work that simultaneously achieves a linear convergence speed and a small learning error. We observe that the learning error is largely dependent on the intrinsic stochastic gradient noise. Motivated by this observation, we introduce two variance reduction methods, stochastic average gradient algorithm (SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to Byzantine-robust decentralized stochastic optimization for eliminating the negative effect of the stochastic gradient noise. The two resulting methods, BRAVO-SAGA and BRAVO-LSVRG, enjoy both linear convergence speeds and stochastic gradient noise-independent learning errors. Such learning errors are optimal for a class of methods based on total variation (TV)-norm regularization and stochastic subgradient update. We conduct extensive numerical experiments to demonstrate their effectiveness under various Byzantine attacks.
</details>
<details>
<summary>摘要</summary>
We observe that the learning error is largely dependent on the intrinsic stochastic gradient noise. To address this issue, we introduce two variance reduction methods, stochastic average gradient algorithm (SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to Byzantine-robust decentralized stochastic optimization. These two methods eliminate the negative effect of stochastic gradient noise and achieve both linear convergence speeds and stochastic gradient noise-independent learning errors.The two resulting methods, BRAVO-SAGA and BRAVO-LSVRG, are optimal for a class of methods based on total variation (TV)-norm regularization and stochastic subgradient update. We conduct extensive numerical experiments to demonstrate their effectiveness under various Byzantine attacks.
</details></li>
</ul>
<hr>
<h2 id="Investigating-disaster-response-through-social-media-data-and-the-Susceptible-Infected-Recovered-SIR-model-A-case-study-of-2020-Western-U-S-wildfire-season"><a href="#Investigating-disaster-response-through-social-media-data-and-the-Susceptible-Infected-Recovered-SIR-model-A-case-study-of-2020-Western-U-S-wildfire-season" class="headerlink" title="Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season"></a>Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05281">http://arxiv.org/abs/2308.05281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihui Ma, Lingyao Li, Libby Hemphill, Gregory B. Baecher</li>
<li>for: This paper aims to provide decision-makers with a quantitative approach to measure disaster response and support their decision-making processes during a disaster.</li>
<li>methods: The authors use BERT topic modeling to cluster topics from Twitter data, and a Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter.</li>
<li>results: The results show that Twitter users mainly focused on three topics: “health impact,” “damage,” and “evacuation,” and the estimated parameters obtained from the SIR model in selected cities revealed that residents exhibited a high level of several concerns during the wildfire.<details>
<summary>Abstract</summary>
Effective disaster response is critical for affected communities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics:"health impact," "damage," and "evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear relationship between topic trends and wildfire propagation patterns. The estimated parameters obtained from the SIR model in selected cities revealed that residents exhibited a high level of several concerns during the wildfire. Our study details how the SIR model and topic modeling using social media data can provide decision-makers with a quantitative approach to measure disaster response and support their decision-making processes.
</details>
<details>
<summary>摘要</summary>
Effective disaster response is crucial for affected communities. Responders and decision-makers can benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, providing valuable insights for decision-makers to understand evolving situations and optimize resource allocation. 我们使用了自然语言处理技术，具体来说是使用Transformers（BERT）主题分类来分类Twitter数据中的主题。然后，我们进行了时间空间分析，检查不同地区在2020年西部美国野火季节中主题的分布。我们发现Twitter用户主要关注了“健康影响”、“损害”和“疏散”三个主题。我们使用了感染传播理论（SIR）模型来探究Twitter上主题的Diffusion特性。结果表明，主题趋势与野火传播模式之间存在明确的关系。我们在选择的城市中获得了SIR模型的参数估计结果，显示了居民在野火期间表现出了高水平的多种担忧。我们的研究详细介绍了如何使用社交媒体数据和SIR模型来为决策者提供量化的方法，以支持他们的决策过程。
</details></li>
</ul>
<hr>
<h2 id="Cross-heterogeneity-Graph-Few-shot-Learning"><a href="#Cross-heterogeneity-Graph-Few-shot-Learning" class="headerlink" title="Cross-heterogeneity Graph Few-shot Learning"></a>Cross-heterogeneity Graph Few-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05275">http://arxiv.org/abs/2308.05275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Ding, Yan Wang, Guanfeng Liu</li>
<li>for:  Addressing the label sparsity issue in heterogeneous graphs (HGs) with few-shot learning.</li>
<li>methods:  Propose a novel model for Cross-heterogeneity Graph Few-shot Learning (CGFL), including extracting meta-patterns and a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs, and a score module to measure the informativeness of labeled samples and determine the transferability of each source HG.</li>
<li>results:  Extensive experiments on four real-world datasets have demonstrated the superior performance of CGFL over the state-of-the-art methods.<details>
<summary>Abstract</summary>
In recent years, heterogeneous graph few-shot learning has been proposed to address the label sparsity issue in heterogeneous graphs (HGs), which contain various types of nodes and edges. The existing methods have achieved good performance by transferring generalized knowledge extracted from rich-labeled classes in source HG(s) to few-labeled classes in a target HG. However, these methods only consider the single-heterogeneity scenario where the source and target HGs share a fixed set of node/edge types, ignoring the more general scenario of cross-heterogeneity, where each HG can have a different and non-fixed set of node/edge types. To this end, we focus on the unexplored cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity Graph Few-shot Learning, namely CGFL. In CGFL, we first extract meta-patterns to capture heterogeneous information and propose a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose a score module to measure the informativeness of labeled samples and determine the transferability of each source HG. Finally, by integrating MHGN and the score module into a meta-learning mechanism, CGFL can effectively transfer generalized knowledge to predict new classes with few-labeled data. Extensive experiments on four real-world datasets have demonstrated the superior performance of CGFL over the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Recently, researchers have proposed heterogeneous graph few-shot learning (HGFSL) to address the label sparsity issue in heterogeneous graphs (HGs), which contain various types of nodes and edges. Existing methods have achieved good performance by transferring generalized knowledge extracted from rich-labeled classes in source HGs to few-labeled classes in a target HG. However, these methods only consider the single-heterogeneity scenario where the source and target HGs share a fixed set of node/edge types, ignoring the more general scenario of cross-heterogeneity, where each HG can have a different and non-fixed set of node/edge types. To address this issue, we focus on the unexplored cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity Graph Few-shot Learning (CGFL).In CGFL, we first extract meta-patterns to capture heterogeneous information and propose a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose a score module to measure the informativeness of labeled samples and determine the transferability of each source HG. Finally, by integrating MHGN and the score module into a meta-learning mechanism, CGFL can effectively transfer generalized knowledge to predict new classes with few-labeled data. Extensive experiments on four real-world datasets have demonstrated the superior performance of CGFL over the state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Intra-Autonomous-Systems-Graph-Generator"><a href="#Data-driven-Intra-Autonomous-Systems-Graph-Generator" class="headerlink" title="Data-driven Intra-Autonomous Systems Graph Generator"></a>Data-driven Intra-Autonomous Systems Graph Generator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05254">http://arxiv.org/abs/2308.05254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caio Vinicius Dadauto, Nelson Luis Saldanha da Fonseca, Ricardo da Silva Torres</li>
<li>for: 本研究旨在提出一种深度学习基于的图生成器，用于生成覆盖互联网络Autonomous System (AS)的 sintetic 图。</li>
<li>methods: 本研究使用了一种名为 Filtered Recurrent Multi-level (FRM) 算法来提取社区，并使用了 Internet Topology Data Kit (ITDK) 项目中的实际网络图形成一个大规模的实际网络图集合。</li>
<li>results: 研究表明，DGGI 生成的 sintetic 图能够准确地复制实际网络图中的性质，包括中心性、嵌入性、相互关联性和节点度。 DGGI 生成器比现有的互联网 topology 生成器更高效，在 Maximum Mean Discrepancy (MMD) 指标上提高了84.4%、95.1%、97.9% 和 94.7%。<details>
<summary>Abstract</summary>
This paper introduces a novel deep-learning based generator of synthetic graphs that represent intra-Autonomous System (AS) in the Internet, named Deep-generative graphs for the Internet (DGGI). It also presents a novel massive dataset of real intra-AS graphs extracted from the project Internet Topology Data Kit (ITDK), called Internet Graphs (IGraphs). To create IGraphs, the Filtered Recurrent Multi-level (FRM) algorithm for community extraction was developed. It is shown that DGGI creates synthetic graphs which accurately reproduce the properties of centrality, clustering, assortativity, and node degree. The DGGI generator overperforms existing Internet topology generators. On average, DGGI improves the Maximum Mean Discrepancy (MMD) metric 84.4%, 95.1%, 97.9%, and 94.7% for assortativity, betweenness, clustering, and node degree, respectively.
</details>
<details>
<summary>摘要</summary>
这份论文介绍了一种新的深度学习基于的生成 sintetic 互联网（AS）图表示法，名为深度生成互联网图（DGGI）。它还发布了一个大量的实际 intra-AS 图据集，从项目互联网Topology数据集（ITDK）中提取出来，称为互联网图（IGraphs）。为创建 IGraphs，开发了一种Filtered Recurrent Multi-level（FRM）算法 для社区提取。研究表明，DGGI 生成的 sintetic 图具有与实际 intra-AS 图的性质相似的中心性、嵌入性、归一化性和节点度等特征。相比之下，DGGI 生成器在 existing Internet topology 生成器之上表现出优异，在 Maximum Mean Discrepancy（MMD）指标上提高了84.4%、95.1%、97.9% 和 94.7% 的平均提升。
</details></li>
</ul>
<hr>
<h2 id="AI-Enabled-Software-and-System-Architecture-Frameworks-Focusing-on-smart-Cyber-Physical-Systems-CPS"><a href="#AI-Enabled-Software-and-System-Architecture-Frameworks-Focusing-on-smart-Cyber-Physical-Systems-CPS" class="headerlink" title="AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS)"></a>AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05239">http://arxiv.org/abs/2308.05239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armin Moin, Atta Badii, Stephan Günnemann, Moharram Challenger</li>
<li>for: 本文旨在为现代应用和组织提供适应数据科学和机器学习（ML）相关担忧的建筑框架。</li>
<li>methods: 本研究使用了文献综述和问卷调查方法，收集、分析和结合了77名专家的意见来提出和验证提案的建筑框架。</li>
<li>results: 本研究提出了两个集合的价值标准，用于评估和优化ML启用的Cyber-Physical Systems（CPS）的开发和性能评价，以及用于评估和优化开发和模型生命周期管道支持工具的价值标准。<details>
<summary>Abstract</summary>
Several architecture frameworks for software, systems, and enterprises have been proposed in the literature. They identified various stakeholders and defined architecture viewpoints and views to frame and address stakeholder concerns. However, the stakeholders with data science and Machine Learning (ML) related concerns, such as data scientists and data engineers, are yet to be included in existing architecture frameworks. Therefore, they failed to address the architecture viewpoints and views responsive to the concerns of the data science community. In this paper, we address this gap by establishing the architecture frameworks adapted to meet the requirements of modern applications and organizations where ML artifacts are both prevalent and crucial. In particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose two sets of merit criteria for their efficient development and performance assessment, namely the criteria for evaluating and benchmarking ML-enabled CPSs, and the criteria for evaluation and benchmarking of the tools intended to support users through the modeling and development pipeline. In this study, we deploy multiple empirical and qualitative research methods based on literature review and survey instruments including expert interviews and an online questionnaire. We collect, analyze, and integrate the opinions of 77 experts from more than 25 organizations in over 10 countries to devise and validate the proposed framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Financial-Fraud-Detection-A-Comparative-Study-of-Quantum-Machine-Learning-Models"><a href="#Financial-Fraud-Detection-A-Comparative-Study-of-Quantum-Machine-Learning-Models" class="headerlink" title="Financial Fraud Detection: A Comparative Study of Quantum Machine Learning Models"></a>Financial Fraud Detection: A Comparative Study of Quantum Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05237">http://arxiv.org/abs/2308.05237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nouhaila Innan, Muhammad Al-Zafar Khan, Mohamed Bennai</li>
<li>for: 这个研究是为了进行金融领域的诈欺探测，使用四种量子机器学习（QML）模型进行比较性研究。</li>
<li>methods: 这个研究使用了量子支持向量分类器模型，并取得了最高性能，具有0.98的F1分数。其他模型，如量子预测器、量子神经网络（QNN）和抽样量子神经网络，也展现了潜在的应用前景。</li>
<li>results: 研究发现，量子支持向量分类器模型在诈欺和非诈欺类别上的F1分数为0.98，其他模型也取得了可靠的结果，但存在一些限制。这些结果对于未来QML发展提供了新的见解和依据，但还需要更有效的量子算法和更大和复杂的数据集。<details>
<summary>Abstract</summary>
In this research, a comparative study of four Quantum Machine Learning (QML) models was conducted for fraud detection in finance. We proved that the Quantum Support Vector Classifier model achieved the highest performance, with F1 scores of 0.98 for fraud and non-fraud classes. Other models like the Variational Quantum Classifier, Estimator Quantum Neural Network (QNN), and Sampler QNN demonstrate promising results, propelling the potential of QML classification for financial applications. While they exhibit certain limitations, the insights attained pave the way for future enhancements and optimisation strategies. However, challenges exist, including the need for more efficient Quantum algorithms and larger and more complex datasets. The article provides solutions to overcome current limitations and contributes new insights to the field of Quantum Machine Learning in fraud detection, with important implications for its future development.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们进行了四种量子机器学习（QML）模型的比较研究，用于金融领域的诈骗检测。我们证明了量子支持向量分类模型在诈骗和非诈骗类中的表现最高，具有0.98的F1分数。其他模型如变量量子分类器、估计量子神经网络（QNN）和抽样QNN也展现了良好的结果，这推动了量子机器学习分类在金融应用中的潜力。尽管它们存在一些限制，但获得的洞察能够为未来的改进和优化策略提供基础。然而，还存在一些挑战，包括需要更高效的量子算法和更大和复杂的数据集。这篇文章提供了解决当前的限制方法，并为量子机器学习在诈骗检测领域的未来发展提供新的洞察和意义。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Gated-Multi-Layer-Perceptron-for-Land-Use-and-Land-Cover-Mapping"><a href="#Spatial-Gated-Multi-Layer-Perceptron-for-Land-Use-and-Land-Cover-Mapping" class="headerlink" title="Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping"></a>Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05235">http://arxiv.org/abs/2308.05235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aj1365/sgumlp">https://github.com/aj1365/sgumlp</a></li>
<li>paper_authors: Ali Jamali, Swalpa Kumar Roy, Danfeng Hong, Peter M Atkinson, Pedram Ghamisi<br>for:* The paper is written for those interested in land use land cover (LULC) mapping and the application of machine learning algorithms in remote sensing.methods:* The paper uses a combination of multi-layer perceptrons (MLPs) and spatial gating units (SGUs) to develop a new learning algorithm called SGU-MLP.* The SGU-MLP algorithm leverages both the strengths of MLPs and SGUs to improve the accuracy of LULC mapping.results:* The SGU-MLP algorithm outperformed several state-of-the-art CNN and CNN-ViT-based models, including HybridSN, ResNet, iFormer, EfficientFormer, and CoAtNet, in terms of average accuracy.* The SGU-MLP algorithm consistently outperformed the benchmark models in three experiments conducted in Houston, USA, Berlin, Germany, and Augsburg, Germany.Here is the simplified Chinese translation of the three key information points:for:* 这篇论文是为了探讨遥感识别和land use land cover（LULC）映射的应用而写的。methods:* 这篇论文使用了多层感知器（MLPs）和空间闭合单元（SGUs）组合来开发一种新的学习算法——SGU-MLP。* SGU-MLP算法利用了MLPs和SGUs的优点，以提高LULC映射的准确性。results:* SGU-MLP算法在HybridSN、ResNet、iFormer、EfficientFormer和CoAtNet等状态对比中，以average accuracy为标准，与多个参考模型进行了比较。* SGU-MLP算法在三个实验中（在houston、berlin和augsburg） consistently outperform了参考模型。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) are models that are utilized extensively for the hierarchical extraction of features. Vision transformers (ViTs), through the use of a self-attention mechanism, have recently achieved superior modeling of global contextual information compared to CNNs. However, to realize their image classification strength, ViTs require substantial training datasets. Where the available training data are limited, current advanced multi-layer perceptrons (MLPs) can provide viable alternatives to both deep CNNs and ViTs. In this paper, we developed the SGU-MLP, a learning algorithm that effectively uses both MLPs and spatial gating units (SGUs) for precise land use land cover (LULC) mapping. Results illustrated the superiority of the developed SGU-MLP classification algorithm over several CNN and CNN-ViT-based models, including HybridSN, ResNet, iFormer, EfficientFormer and CoAtNet. The proposed SGU-MLP algorithm was tested through three experiments in Houston, USA, Berlin, Germany and Augsburg, Germany. The SGU-MLP classification model was found to consistently outperform the benchmark CNN and CNN-ViT-based algorithms. For example, for the Houston experiment, SGU-MLP significantly outperformed HybridSN, CoAtNet, Efficientformer, iFormer and ResNet by approximately 15%, 19%, 20%, 21%, and 25%, respectively, in terms of average accuracy. The code will be made publicly available at https://github.com/aj1365/SGUMLP
</details>
<details>
<summary>摘要</summary>
convolutional neural networks (CNNs) 是模型，广泛应用于层次提取特征。 vision transformers (ViTs) 通过自我注意机制，在全局上下文信息模型方面，最近超越了 CNNs。然而，要实现图像分类强度，ViTs 需要大量的训练数据。当有限的训练数据available时，当前的高级多层感知器 (MLPs) 可以提供可靠的替代品，包括深度 CNNs 和 ViTs。在这篇论文中，我们开发了SGU-MLP 学习算法，它有效地结合 MLPs 和空间闭合单元 (SGUs)  для精确的土地用途地图。结果表明，我们开发的SGU-MLP 分类算法在多个实验中，包括 HOUSTON、BERLIN 和 AUGSBURG，都有superiority 于许多 CNN 和 CNN-ViT 基于的模型，包括 HybridSN、ResNet、iFormer、EfficientFormer 和 CoAtNet。我们提出的SGU-MLP 分类模型在 HOUSTON 实验中，与 HybridSN、CoAtNet、Efficientformer、iFormer 和 ResNet 相比，提高了15%、19%、20%、21% 和 25% 的平均准确率。我们将代码公开在 GitHub 上，详细的实验结果和代码将在https://github.com/aj1365/SGUMLP 上公布。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-the-Edge-and-Cloud-for-V2X-Based-Real-Time-Object-Detection-in-Autonomous-Driving"><a href="#Leveraging-the-Edge-and-Cloud-for-V2X-Based-Real-Time-Object-Detection-in-Autonomous-Driving" class="headerlink" title="Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving"></a>Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05234">http://arxiv.org/abs/2308.05234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faisal Hawlader, François Robinet, Raphaël Frank</li>
<li>for: 本研究旨在找到实时感知中最佳的折衶点，以提高自动驾驶汽车的安全性和可靠性。</li>
<li>methods: 本研究使用了虚拟数据集来训练对象检测模型，并研究了不同的卫星和云端计算策略的可行性。</li>
<li>results: 研究结果显示，通过使用JPEG和H.265压缩，可以在云端实现实时对象检测，并且比地面上的检测性能更高。<details>
<summary>Abstract</summary>
Environmental perception is a key element of autonomous driving because the information received from the perception module influences core driving decisions. An outstanding challenge in real-time perception for autonomous driving lies in finding the best trade-off between detection quality and latency. Major constraints on both computation and power have to be taken into account for real-time perception in autonomous vehicles. Larger object detection models tend to produce the best results, but are also slower at runtime. Since the most accurate detectors cannot run in real-time locally, we investigate the possibility of offloading computation to edge and cloud platforms, which are less resource-constrained. We create a synthetic dataset to train object detection models and evaluate different offloading strategies. Using real hardware and network simulations, we compare different trade-offs between prediction quality and end-to-end delay. Since sending raw frames over the network implies additional transmission delays, we also explore the use of JPEG and H.265 compression at varying qualities and measure their impact on prediction metrics. We show that models with adequate compression can be run in real-time on the cloud while outperforming local detection performance.
</details>
<details>
<summary>摘要</summary>
环境感知是自动驾驶中关键的元素，因为感知模块的信息会影响自动驾驶的核心决策。现实时感知对自动驾驶而言是一个重要的挑战，需要找到最佳的折衔点 между检测质量和延迟。自动驾驶车辆的实时感知受到计算和功率的重要限制。大型对象检测模型通常会生成最佳的结果，但它们在运行时也比较慢。由于最准确的检测器不能在本地实时运行，我们 investigate了将计算卸载到边缘和云平台上，这些平台具有较强的资源。我们创建了一个 sintetic 数据集来训练对象检测模型，并评估不同的卸载策略。使用真实的硬件和网络 simulate，我们比较了不同的妥协点 между预测质量和总终端延迟。由于发送Raw帧数据到网络会添加额外的传输延迟，我们还探索了使用 JPEG 和 H.265 压缩，并测量其对预测指标的影响。我们显示，使用合适的压缩可以在云上实时运行模型，并超越本地检测性能。
</details></li>
</ul>
<hr>
<h2 id="SegMatch-A-semi-supervised-learning-method-for-surgical-instrument-segmentation"><a href="#SegMatch-A-semi-supervised-learning-method-for-surgical-instrument-segmentation" class="headerlink" title="SegMatch: A semi-supervised learning method for surgical instrument segmentation"></a>SegMatch: A semi-supervised learning method for surgical instrument segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05232">http://arxiv.org/abs/2308.05232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Wei, Charlie Budd, Luis C. Garcia-Peraza-Herrera, Reuben Dorent, Miaojing Shi, Tom Vercauteren</li>
<li>for: 提高 Laparoscopic 和 Robotic 手术图像的分割精度，减少高成本的标注成本。</li>
<li>methods: 提出 SegMatch，一种基于 FixMatch 的 semi-supervised learning方法，通过弱版 augmentation 生成 pseudo-label，并在挑战性训练中使用 adversarial augmentation 来增强模型的鲁棒性和稳定性。</li>
<li>results: 在 Robust-MIS 2019 和 EndoVis 2017 数据集上进行了测试，结果表明，通过添加无标注数据进行训练，SegMatch 可以超越完全监督学习方法，并在不同的标签到无标签数据比例中具有更高的性能。<details>
<summary>Abstract</summary>
Surgical instrument segmentation is recognised as a key enabler to provide advanced surgical assistance and improve computer assisted interventions. In this work, we propose SegMatch, a semi supervised learning method to reduce the need for expensive annotation for laparoscopic and robotic surgical images. SegMatch builds on FixMatch, a widespread semi supervised classification pipeline combining consistency regularization and pseudo labelling, and adapts it for the purpose of segmentation. In our proposed SegMatch, the unlabelled images are weakly augmented and fed into the segmentation model to generate a pseudo-label to enforce the unsupervised loss against the output of the model for the adversarial augmented image on the pixels with a high confidence score. Our adaptation for segmentation tasks includes carefully considering the equivariance and invariance properties of the augmentation functions we rely on. To increase the relevance of our augmentations, we depart from using only handcrafted augmentations and introduce a trainable adversarial augmentation strategy. Our algorithm was evaluated on the MICCAI Instrument Segmentation Challenge datasets Robust-MIS 2019 and EndoVis 2017. Our results demonstrate that adding unlabelled data for training purposes allows us to surpass the performance of fully supervised approaches which are limited by the availability of training data in these challenges. SegMatch also outperforms a range of state-of-the-art semi-supervised learning semantic segmentation models in different labelled to unlabelled data ratios.
</details>
<details>
<summary>摘要</summary>
外科器械分割是认可为提供高级别外科协助和改进计算助手的关键因素。在这项工作中，我们提议SegMatch，一种半supervised学习方法，以减少外科 Laparoscopic和Robotic 图像分割需要的昂贵标注。SegMatch基于FixMatch，一种广泛使用的半supervised分类管道，并将其改编为分割任务。在我们的提议SegMatch中，无标图像被弱地扩展并feed into分割模型，以生成一个 pseudo-标签，以便对模型输出的图像像素进行强制检查。我们的改进包括仔细考虑图像分割任务中的等价性和不变性属性。为了增加我们的扩展的相关性，我们不再仅使用手工设计的扩展，而是引入一种可学习的对抗扩展策略。我们的算法在MICCAI Instrument Segmentation Challenge数据集Robust-MIS 2019和EndoVis 2017上进行了评估。我们的结果表明，在训练目标数据有限时，通过添加无标图像进行训练可以超越完全supervised方法的性能。SegMatch还比一些状态的半supervised学习semantic segmentation模型在不同的标签到无标签数据比率中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Training-neural-networks-with-end-to-end-optical-backpropagation"><a href="#Training-neural-networks-with-end-to-end-optical-backpropagation" class="headerlink" title="Training neural networks with end-to-end optical backpropagation"></a>Training neural networks with end-to-end optical backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05226">http://arxiv.org/abs/2308.05226</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Spall, Xianxin Guo, A. I. Lvovsky</li>
<li>for: 这个论文是为了实现光学 neural network 的训练和推理两个任务而写的。</li>
<li>methods: 这篇论文使用了耗尽性吸收器来实现非线性活化函数，并通过激光-探测器过程来实现训练。</li>
<li>results: 这篇论文成功实现了完全基于光学 процесс的 neural network 训练和推理，并且可以在不同的光学平台、材料和网络结构上进行适应。<details>
<summary>Abstract</summary>
Optics is an exciting route for the next generation of computing hardware for machine learning, promising several orders of magnitude enhancement in both computational speed and energy efficiency. However, to reach the full capacity of an optical neural network it is necessary that the computing not only for the inference, but also for the training be implemented optically. The primary algorithm for training a neural network is backpropagation, in which the calculation is performed in the order opposite to the information flow for inference. While straightforward in a digital computer, optical implementation of backpropagation has so far remained elusive, particularly because of the conflicting requirements for the optical element that implements the nonlinear activation function. In this work, we address this challenge for the first time with a surprisingly simple and generic scheme. Saturable absorbers are employed for the role of the activation units, and the required properties are achieved through a pump-probe process, in which the forward propagating signal acts as the pump and backward as the probe. Our approach is adaptable to various analog platforms, materials, and network structures, and it demonstrates the possibility of constructing neural networks entirely reliant on analog optical processes for both training and inference tasks.
</details>
<details>
<summary>摘要</summary>
什么是optics？optics是未来计算机硬件的新一代激光学技术，可以提供数个数个级别的计算速度和能效率提升。然而，要达到激光神经网络的完整能力，不仅需要推理部分实现光学计算，还需要训练部分也实现光学计算。主要算法用于训练神经网络是反射吗，在神经网络的信息流向中进行计算，而光学实现反射吗的问题尚未得到解决。在这种情况下，我们第一次解决这个挑战，使用可饱和吸收器来实现非线性活化函数。我们的方法可以适应不同的分析平台、材料和网络结构，并示出了完全通过光学过程实现神经网络的训练和推理任务。
</details></li>
</ul>
<hr>
<h2 id="Conceptualizing-Machine-Learning-for-Dynamic-Information-Retrieval-of-Electronic-Health-Record-Notes"><a href="#Conceptualizing-Machine-Learning-for-Dynamic-Information-Retrieval-of-Electronic-Health-Record-Notes" class="headerlink" title="Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes"></a>Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08494">http://arxiv.org/abs/2308.08494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharon Jiang, Shannon Shen, Monica Agrawal, Barbara Lam, Nicholas Kurtzman, Steven Horng, David Karger, David Sontag</li>
<li>for: 降低临床医生疲劳，提高医疗记录的效率和质量。</li>
<li>methods: 利用电子医疗纪录（EHR）审核日志进行机器学习监督，动态检索相关病历记录，提高记录过程中的信息检索效率。</li>
<li>results: 在紧急医学设置下，我们的方法可以达到0.963的准确率，预测具体的病历记录会在个人记录写作过程中被读取。我们还进行了一些临床医生的用户研究，发现我们的框架可以帮助临床医生更加快速地检索相关信息。<details>
<summary>Abstract</summary>
The large amount of time clinicians spend sifting through patient notes and documenting in electronic health records (EHRs) is a leading cause of clinician burnout. By proactively and dynamically retrieving relevant notes during the documentation process, we can reduce the effort required to find relevant patient history. In this work, we conceptualize the use of EHR audit logs for machine learning as a source of supervision of note relevance in a specific clinical context, at a particular point in time. Our evaluation focuses on the dynamic retrieval in the emergency department, a high acuity setting with unique patterns of information retrieval and note writing. We show that our methods can achieve an AUC of 0.963 for predicting which notes will be read in an individual note writing session. We additionally conduct a user study with several clinicians and find that our framework can help clinicians retrieve relevant information more efficiently. Demonstrating that our framework and methods can perform well in this demanding setting is a promising proof of concept that they will translate to other clinical settings and data modalities (e.g., labs, medications, imaging).
</details>
<details>
<summary>摘要</summary>
临床医生 spent a large amount of time搜索病人笔记和在电子医疗记录（EHR）中记录，这是临床疲劳的主要原因。通过积极和动态检索病人历史记录，我们可以减少找到相关病人历史的努力。在这项工作中，我们将EHR审核日志用于机器学习的监督，以确定笔记相关性在特定临床上下文中。我们的评估将注重在急诊室中进行动态检索，这是一个高危性的设置，具有独特的信息检索和笔记写作模式。我们的方法可以实现的AUC为0.963，预测 individu笔记写作会读取哪些笔记。此外，我们还进行了一些临床医生的用户研究，发现我们的框架可以帮助临床医生更有效地检索相关信息。这表明我们的框架和方法在这种高危性的设置下可以表现良好，这也是一个有希望的证明，它们将在其他临床设置和数据模式（例如，实验室数据、药物数据、成像数据）中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Decoding-Layer-Saliency-in-Language-Transformers"><a href="#Decoding-Layer-Saliency-in-Language-Transformers" class="headerlink" title="Decoding Layer Saliency in Language Transformers"></a>Decoding Layer Saliency in Language Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05219">http://arxiv.org/abs/2308.05219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizabeth M. Hou, Gregory Castanon</li>
<li>for: 这个论文是为了解决现代自然语言处理中的文本焦点问题。</li>
<li>methods: 这个论文使用了 gradient-based 粒子方法，并提出了一种用于评估各层语义一致度的方法。</li>
<li>results: 这个论文在多个benchmark数据集上demonstrated consistent improvement over other textual saliency methods, without requiring additional training or labelled data.<details>
<summary>Abstract</summary>
In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种策略来确定语言模型中文本焦点的存在。在视觉网络中，焦点自然地局部化在网络的卷积层中，但这并不是现代使用Transformer堆栈网络处理自然语言的情况。我们采用梯度基于的焦点方法，提出了评估层次含义一致度的方法，并在多个benchmark分类 datasets上表现出了consistent的改进。我们的方法不需要额外的训练或标注数据，并且计算效率较高。
</details></li>
</ul>
<hr>
<h2 id="Conformer-based-Target-Speaker-Automatic-Speech-Recognition-for-Single-Channel-Audio"><a href="#Conformer-based-Target-Speaker-Automatic-Speech-Recognition-for-Single-Channel-Audio" class="headerlink" title="Conformer-based Target-Speaker Automatic Speech Recognition for Single-Channel Audio"></a>Conformer-based Target-Speaker Automatic Speech Recognition for Single-Channel Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05218">http://arxiv.org/abs/2308.05218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/NeMo">https://github.com/NVIDIA/NeMo</a></li>
<li>paper_authors: Yang Zhang, Krishna C. Puvvada, Vitaly Lavrukhin, Boris Ginsburg</li>
<li>for: 这篇论文旨在提出一个非自适应的终端到缩时频域架构，用于单通道目标话者自动语音识别（TS-ASR）。</li>
<li>methods: 该模型包括基于TitaNet的话者嵌入模组、Conformer基于隐藏读取和ASR模组。这些模组组合地优化以识别目标话者的语音，而忽略其他话者的语音。</li>
<li>results: 在训练时使用Connectionist Temporal Classification（CTC）损失和对频域spectrogram进行标准化的数据重建损失，以鼓励模型更好地分离目标话者的spectrogram。在WSJ0-2mix-extr（4.2%）上获得了目标话者字元误差（TS-WER）的最新纪录。此外，我们首次在WSJ0-3mix-extr（12.4%）、LibriSpeech2Mix（4.2%）和LibriSpeech3Mix（7.6%） dataset上获得了TS-WER的纪录，创造了新的benchmarks для TS-ASR。<details>
<summary>Abstract</summary>
We propose CONF-TSASR, a non-autoregressive end-to-end time-frequency domain architecture for single-channel target-speaker automatic speech recognition (TS-ASR). The model consists of a TitaNet based speaker embedding module, a Conformer based masking as well as ASR modules. These modules are jointly optimized to transcribe a target-speaker, while ignoring speech from other speakers. For training we use Connectionist Temporal Classification (CTC) loss and introduce a scale-invariant spectrogram reconstruction loss to encourage the model better separate the target-speaker's spectrogram from mixture. We obtain state-of-the-art target-speaker word error rate (TS-WER) on WSJ0-2mix-extr (4.2%). Further, we report for the first time TS-WER on WSJ0-3mix-extr (12.4%), LibriSpeech2Mix (4.2%) and LibriSpeech3Mix (7.6%) datasets, establishing new benchmarks for TS-ASR. The proposed model will be open-sourced through NVIDIA NeMo toolkit.
</details>
<details>
<summary>摘要</summary>
For training, we use Connectionist Temporal Classification (CTC) loss and introduce a scale-invariant spectrogram reconstruction loss to encourage the model to better separate the target-speaker's spectrogram from the mixture. 我们使用Connectionist Temporal Classification（CTC）损失进行训练，并引入一个缩放不变的spectrogram重建损失，以促进模型更好地分离目标说话人的spectrogram和混合。We obtain state-of-the-art target-speaker word error rate (TS-WER) on WSJ0-2mix-extr (4.2%). 我们在WSJ0-2mix-extr上获得了单频道目标说话人单词错误率（TS-WER）的状态地表现，得到4.2%。Further, we report for the first time TS-WER on WSJ0-3mix-extr (12.4%), LibriSpeech2Mix (4.2%) and LibriSpeech3Mix (7.6%) datasets, establishing new benchmarks for TS-ASR. 此外，我们在WSJ0-3mix-extr（12.4%）、LibriSpeech2Mix（4.2%）和LibriSpeech3Mix（7.6%） datasets上首次报告TS-WER，创造了新的TS-ASR标准。The proposed model will be open-sourced through NVIDIA NeMo toolkit. 我们将提出的模型通过NVIDIA NeMo工具包开源。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Pedestrian-Trajectory-Prediction-Methods-for-the-Application-in-Autonomous-Driving"><a href="#Evaluating-Pedestrian-Trajectory-Prediction-Methods-for-the-Application-in-Autonomous-Driving" class="headerlink" title="Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving"></a>Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05194">http://arxiv.org/abs/2308.05194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nico Uhlemann, Felix Fent, Markus Lienkamp</li>
<li>for: 评估预测行人轨迹领域的现状，以及常速模型（CVM）在自动驾驶车辆中的适用性。</li>
<li>methods: 使用ETH&#x2F;UCY数据集进行评估，并对初始提出的模型进行修改以适应实际应用需求。进行减噪研究，检验观察到的运动历史对预测性能的影响。</li>
<li>results: 显示简单的模型在生成单轨道时仍然竞争力强，一些通常被认为是有用的特征对于不同的架构都具有少量影响。根据这些发现，提出了预测轨迹算法的建议。<details>
<summary>Abstract</summary>
In this paper, the state of the art in the field of pedestrian trajectory prediction is evaluated alongside the constant velocity model (CVM) with respect to its applicability in autonomous vehicles. The evaluation is conducted on the widely-used ETH/UCY dataset where the Average Displacement Error (ADE) and the Final Displacement Error (FDE) are reported. To align with requirements in real-world applications, modifications are made to the input features of the initially proposed models. An ablation study is conducted to examine the influence of the observed motion history on the prediction performance, thereby establishing a better understanding of its impact. Additionally, the inference time of each model is measured to evaluate the scalability of each model when confronted with varying amounts of agents. The results demonstrate that simple models remain competitive when generating single trajectories, and certain features commonly thought of as useful have little impact on the overall performance across different architectures. Based on these findings, recommendations are proposed to guide the future development of trajectory prediction algorithms.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们评估了步行者轨迹预测领域的现状，同时与常速模型（CVM）进行比较，以确定其在自动驾驶汽车中的可行性。我们在广泛使用的 ETH/UCY 数据集上进行评估，并计算了平均偏移误差（ADE）和最终偏移误差（FDE）。为适应实际应用中的需求，我们对初始提出的模型的输入特征进行修改。我们还进行了减少研究，以确定观察到的运动历史对预测性能的影响，从而更好地理解其影响。此外，我们测量了每个模型的推断时间，以评估它们在不同数量的代理人面临的可扩展性。结果表明，简单的模型在生成单个轨迹时仍然竞争力强，而一些通常被认为是有用的特征在不同的架构下具有微不足的影响。基于这些发现，我们提出了指导未来轨迹预测算法发展的建议。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Representations-for-Spatio-Temporal-Visual-Attention-Modeling-and-Understanding"><a href="#Hierarchical-Representations-for-Spatio-Temporal-Visual-Attention-Modeling-and-Understanding" class="headerlink" title="Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding"></a>Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05189">http://arxiv.org/abs/2308.05189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel-Ángel Fernández-Torres</li>
<li>for: 本PhD论文研究了视觉注意力模型和理解视频序列中的层次表示。</li>
<li>methods: 提出了两种计算模型，一是生成概率模型，二是深度网络架构，用于模型视觉注意力。</li>
<li>results: 提出了一种基于上下文的视觉注意力模型和理解方法，并实现了视频序列中的视觉注意力模型。<details>
<summary>Abstract</summary>
This PhD. Thesis concerns the study and development of hierarchical representations for spatio-temporal visual attention modeling and understanding in video sequences. More specifically, we propose two computational models for visual attention. First, we present a generative probabilistic model for context-aware visual attention modeling and understanding. Secondly, we develop a deep network architecture for visual attention modeling, which first estimates top-down spatio-temporal visual attention, and ultimately serves for modeling attention in the temporal domain.
</details>
<details>
<summary>摘要</summary>
这个博士论文关注了视觉注意力模型化和理解在视频序列中的层次表示和时间域注意力模型。更具体来说，我们提出了两种计算模型 для视觉注意力。首先，我们提出了一种基于概率理论的生成模型，用于 Context-aware 视觉注意力模型和理解。其次，我们开发了一种深度网络架构，用于模型视觉注意力，首先估计上下文视觉注意力，并最终用于模型时间域注意力。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Morphological-Identification-of-Extended-Radio-Galaxies-using-Weak-Labels"><a href="#Deep-Learning-for-Morphological-Identification-of-Extended-Radio-Galaxies-using-Weak-Labels" class="headerlink" title="Deep Learning for Morphological Identification of Extended Radio Galaxies using Weak Labels"></a>Deep Learning for Morphological Identification of Extended Radio Galaxies using Weak Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05166">http://arxiv.org/abs/2308.05166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikhel1/gal-cam">https://github.com/nikhel1/gal-cam</a></li>
<li>paper_authors: Nikhel Gupta, Zeeshan Hayder, Ray P. Norris, Minh Huynh, Lars Petersson, X. Rosalind Wang, Heinz Andernach, Bärbel S. Koribalski, Miranda Yew, Evan J. Crawford</li>
<li>for: 这项研究旨在开发一种基于深度学习算法，可以减少复杂Radio галактике多组件的标注成本。</li>
<li>methods: 该算法使用弱类标签的Radio галактике来获取类活动地图（CAM），然后使用间隔Pixel网络（IRNet）来更新CAM，以获得Radio галактике的实例分割mask和红外主 galaxy的位置。</li>
<li>results: 我们使用澳大利亚 Square Kilometre Array Pathfinder（ASKAP）望远镜的数据，具体是EMU Pilot Survey，覆盖了天空面积270平方度，具有RMS敏感度25-35微赫&#x2F;天线。我们表明，使用弱类标签的深度学习算法可以高精度预测像素级信息，包括Radio галактике的扩展辐射覆盖所有 галактиComponent的面积和红外主 galaxy的位置。我们使用mAP作为评价指标，并显示模型在多个类中的mAP$_{50}$为67.5%和76.8%。模型架构可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://github.com/Nikhel1/Gal-CAM">https://github.com/Nikhel1/Gal-CAM</a><details>
<summary>Abstract</summary>
The present work discusses the use of a weakly-supervised deep learning algorithm that reduces the cost of labelling pixel-level masks for complex radio galaxies with multiple components. The algorithm is trained on weak class-level labels of radio galaxies to get class activation maps (CAMs). The CAMs are further refined using an inter-pixel relations network (IRNet) to get instance segmentation masks over radio galaxies and the positions of their infrared hosts. We use data from the Australian Square Kilometre Array Pathfinder (ASKAP) telescope, specifically the Evolutionary Map of the Universe (EMU) Pilot Survey, which covered a sky area of 270 square degrees with an RMS sensitivity of 25-35 $\mu$Jy/beam. We demonstrate that weakly-supervised deep learning algorithms can achieve high accuracy in predicting pixel-level information, including masks for the extended radio emission encapsulating all galaxy components and the positions of the infrared host galaxies. We evaluate the performance of our method using mean Average Precision (mAP) across multiple classes at a standard intersection over union (IoU) threshold of 0.5. We show that the model achieves a mAP$_{50}$ of 67.5\% and 76.8\% for radio masks and infrared host positions, respectively. The network architecture can be found at the following link: https://github.com/Nikhel1/Gal-CAM
</details>
<details>
<summary>摘要</summary>
现在的研究探讨了使用弱类标注深度学习算法来降低复杂radio galaxy的多 компонент pixel-level掩码的成本。这种算法在 radio galaxy 的弱类标签上进行训练，以获得类活化图（CAMs）。然后，使用间 pixel 关系网络（IRNet）来进一步修改 CAMs，以获得 radio galaxy 的实例分割mask和激发掩码。我们使用澳大利亚 Square Kilometre Array Pathfinder（ASKAP） telescope的数据，特别是Evolutionary Map of the Universe（EMU） Pilot Survey，覆盖了天空面积270平方度，具有RMS敏感度25-35微伏/槽。我们表明，弱类标注深度学习算法可以高精度预测像素级信息，包括涵盖所有 галакси组件的广泛电磁辐射掩码以及激发掩码。我们使用mean Average Precision（mAP）作为评价指标，并在多个类上进行标准的交叠 UNION（IoU）阈值0.5的评价。我们发现模型在 radio 掩码和激发位置上的mAP$_{50}$分别为67.5%和76.8%。网络架构可以在以下链接中找到：https://github.com/Nikhel1/Gal-CAM。
</details></li>
</ul>
<hr>
<h2 id="Improved-Multi-Shot-Diffusion-Weighted-MRI-with-Zero-Shot-Self-Supervised-Learning-Reconstruction"><a href="#Improved-Multi-Shot-Diffusion-Weighted-MRI-with-Zero-Shot-Self-Supervised-Learning-Reconstruction" class="headerlink" title="Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction"></a>Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05103">http://arxiv.org/abs/2308.05103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaejin-cho/miccai2023">https://github.com/jaejin-cho/miccai2023</a></li>
<li>paper_authors: Jaejin Cho, Yohan Jun, Xiaoqing Wang, Caique Kobayashi, Berkin Bilgic</li>
<li>for:  This paper aims to improve the reconstruction of multi-shot echo-planar imaging (msEPI) data for diffusion MRI, addressing limitations due to magnetic field inhomogeneity and T2&#x2F;T2* relaxation effects.</li>
<li>methods: The proposed approach, called zero-MIRID, uses deep learning-based image regularization techniques, including CNN denoisers in both k- and image-spaces, and virtual coils to enhance image reconstruction conditioning.</li>
<li>results: Compared to the state-of-the-art parallel imaging method, the proposed approach achieves superior results in reconstructing msEPI data, as demonstrated in an in-vivo experiment.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是提高多shotecho-planar imaging（msEPI）数据重建，以解决由磁场不均和T2&#x2F;T2*相互作用引起的限制。</li>
<li>methods: 提议的方法是使用深度学习基于图像规范化技术，包括k-和图像空间的CNN滤波器，以及虚拟天线来加强图像重建条件。</li>
<li>results: 相比领域内的并行成像方法，提议的方法在重建msEPI数据方面得到了更好的结果，实验中得到了良好的成果。<details>
<summary>Abstract</summary>
Diffusion MRI is commonly performed using echo-planar imaging (EPI) due to its rapid acquisition time. However, the resolution of diffusion-weighted images is often limited by magnetic field inhomogeneity-related artifacts and blurring induced by T2- and T2*-relaxation effects. To address these limitations, multi-shot EPI (msEPI) combined with parallel imaging techniques is frequently employed. Nevertheless, reconstructing msEPI can be challenging due to phase variation between multiple shots. In this study, we introduce a novel msEPI reconstruction approach called zero-MIRID (zero-shot self-supervised learning of Multi-shot Image Reconstruction for Improved Diffusion MRI). This method jointly reconstructs msEPI data by incorporating deep learning-based image regularization techniques. The network incorporates CNN denoisers in both k- and image-spaces, while leveraging virtual coils to enhance image reconstruction conditioning. By employing a self-supervised learning technique and dividing sampled data into three groups, the proposed approach achieves superior results compared to the state-of-the-art parallel imaging method, as demonstrated in an in-vivo experiment.
</details>
<details>
<summary>摘要</summary>
Diffusion MRI通常使用声波平均图像（EPI）进行取样，但是各种磁场不均的artefacts和T2-和T2*-征relaxation效应导致分子扩散图像的分辨率受到限制。为了解决这些限制，常用多shot EPI（msEPI）和并行技术。然而，重建msEPI可以困难，因为多个shot之间的阶段差异。在本研究中，我们介绍了一种新的msEPI重建方法，即zero-MIRID（零次自我超vised学习Multi-shot图像重建优化Diffusion MRI）。这种方法将msEPI数据重建并结合深度学习 Image Regularization技术。网络包含CNN杂谱denoiser在k-和图像空间中，同时利用虚拟磁场来增强图像重建条件。通过自我超vised学习技术和将样本数据分为三组，我们的方法在对照磁共振方法的实验中显示出了更好的效果。
</details></li>
</ul>
<hr>
<h2 id="DOST-–-Domain-Obedient-Self-supervised-Training-for-Multi-Label-Classification-with-Noisy-Labels"><a href="#DOST-–-Domain-Obedient-Self-supervised-Training-for-Multi-Label-Classification-with-Noisy-Labels" class="headerlink" title="DOST – Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels"></a>DOST – Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05101">http://arxiv.org/abs/2308.05101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumadeep Saha, Utpal Garain, Arijit Ukil, Arpan Pal, Sundeep Khandelwal</li>
<li>for: 这篇论文主要关注的是多标签分类（Multi-label Classification，MLC）任务中的标签噪声问题。</li>
<li>methods: 本文提出了一个叫做“领域套用自动训练”（Domain Obedient Self-supervised Training，DOST）的概念，它不仅使得深度学习模型更加遵循领域规则，而且可以提高学习效果和减少标签噪声的影响。</li>
<li>results: 实验结果显示，DOST方法在两个大规模的多标签分类数据集上均能够获得改善，并且可以全面抵消噪声的影响。<details>
<summary>Abstract</summary>
The enormous demand for annotated data brought forth by deep learning techniques has been accompanied by the problem of annotation noise. Although this issue has been widely discussed in machine learning literature, it has been relatively unexplored in the context of "multi-label classification" (MLC) tasks which feature more complicated kinds of noise. Additionally, when the domain in question has certain logical constraints, noisy annotations often exacerbate their violations, making such a system unacceptable to an expert. This paper studies the effect of label noise on domain rule violation incidents in the MLC task, and incorporates domain rules into our learning algorithm to mitigate the effect of noise. We propose the Domain Obedient Self-supervised Training (DOST) paradigm which not only makes deep learning models more aligned to domain rules, but also improves learning performance in key metrics and minimizes the effect of annotation noise. This novel approach uses domain guidance to detect offending annotations and deter rule-violating predictions in a self-supervised manner, thus making it more "data efficient" and domain compliant. Empirical studies, performed over two large scale multi-label classification datasets, demonstrate that our method results in improvement across the board, and often entirely counteracts the effect of noise.
</details>
<details>
<summary>摘要</summary>
“深度学习技术的巨大需求已导致笔记噪声问题的出现，而这个问题在多标签分类（MLC）任务中的噪声问题尚未得到广泛研究。此外，当领域具有特定的逻辑约束时，噪声笔记常会加剧逻辑约束的违反，使得这种系统不可接受于专家。本文研究MLC任务中标签噪声对领域规则违反事件的影响，并将领域规则 incorporated 到我们的学习算法中以mitigate噪声的影响。我们提出的领域遵循自我超vised Training（DOST）方法不仅使得深度学习模型更遵循领域规则，还提高了学习性能在重要指标上，并减少了笔记噪声的影响。这种新的方法通过领域指导检测噪声笔记并防止违反规则的预测，因此更“数据效率”和遵循领域规则。empirical studies 在两个大规模多标签分类数据集上进行，结果表明，我们的方法在全面上提高了性能，常常完全抵消噪声的影响。”
</details></li>
</ul>
<hr>
<h2 id="A-degree-of-image-identification-at-sub-human-scales-could-be-possible-with-more-advanced-clusters"><a href="#A-degree-of-image-identification-at-sub-human-scales-could-be-possible-with-more-advanced-clusters" class="headerlink" title="A degree of image identification at sub-human scales could be possible with more advanced clusters"></a>A degree of image identification at sub-human scales could be possible with more advanced clusters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05092">http://arxiv.org/abs/2308.05092</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prateekjannu/imagescale2">https://github.com/prateekjannu/imagescale2</a></li>
<li>paper_authors: Prateek Y J</li>
<li>for: 本研究目的是判断当前可用的自动学习技术是否可以使人类水平的视觉图像理解，使用同样的感知输入量和度量。</li>
<li>methods: 本研究使用了涉及数据量缩放和图像质量缩放的自动学习方法，而无需外部资金支持。</li>
<li>results: 我们发现，同时缩放数据量和图像分辨率可以 дости得人类水平的物品检测性能，而无需超过人类大小。我们使用视transformer在200000张图像和256 ppi进行训练。<details>
<summary>Abstract</summary>
The purpose of the research is to determine if currently available self-supervised learning techniques can accomplish human level comprehension of visual images using the same degree and amount of sensory input that people acquire from. Initial research on this topic solely considered data volume scaling. Here, we scale both the volume of data and the quality of the image. This scaling experiment is a self-supervised learning method that may be done without any outside financing. We find that scaling up data volume and picture resolution at the same time enables human-level item detection performance at sub-human sizes.We run a scaling experiment with vision transformers trained on up to 200000 images up to 256 ppi.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The purpose of the research is to determine if currently available self-supervised learning techniques can accomplish human level comprehension of visual images using the same degree and amount of sensory input that people acquire from. Initial research on this topic solely considered data volume scaling. Here, we scale both the volume of data and the quality of the image. This scaling experiment is a self-supervised learning method that may be done without any outside financing. We find that scaling up data volume and picture resolution at the same time enables human-level item detection performance at sub-human sizes.We run a scaling experiment with vision transformers trained on up to 200000 images up to 256 ppi." into Simplified Chinese.<<SYS>>当前的研究目标是判断当前可用的自我超vised learning技术是否可以通过同样的感知输入来达到人类水平的视觉理解。初步研究仅考虑数据量的扩大。在这里，我们同时扩大数据量和图像质量。这种扩大实验可以无需外部资金进行。我们发现同时扩大数据量和图像分辨率时，可以在子人类大小下达到人类水平的物体检测性能。我们使用视Transformers进行训练，并将数据量增加至200000张，图像分辨率达256ppi。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Inverse-Transition-Learning-for-Offline-Settings"><a href="#Bayesian-Inverse-Transition-Learning-for-Offline-Settings" class="headerlink" title="Bayesian Inverse Transition Learning for Offline Settings"></a>Bayesian Inverse Transition Learning for Offline Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05075">http://arxiv.org/abs/2308.05075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leo Benac, Sonali Parbhoo, Finale Doshi-Velez</li>
<li>for:  sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data.</li>
<li>methods:  a new constraint-based approach that captures desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients.</li>
<li>results:  learned a high-performing policy, while considerably reducing the policy’s variance over different datasets. Also, the paper demonstrates how combining uncertainty estimation with these constraints can help infer a partial ranking of actions that produce higher returns, and helps infer safer and more informative policies for planning.Here is the text in Simplified Chinese:</li>
<li>for: Sequential Decision-Making领域，如医疗和教育等，where reward known且transition dynamics $T$必须通过批处数据 estimating。</li>
<li>methods: 新的约束基本方法，可以准确地学习 posterior distribution of transition dynamics $T$，免于gradients的影响。</li>
<li>results: 学习出高性能策略，同时减少策略对不同数据集的差异。此外，文章还解释了如何通过uncertainty estimation和约束结合，推断出更高返回的动作partial ranking，以及更安全和更有信息的策略规划。<details>
<summary>Abstract</summary>
Offline Reinforcement learning is commonly used for sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data. A key challenge for all tasks is how to learn a reliable estimate of the transition dynamics $T$ that produce near-optimal policies that are safe enough so that they never take actions that are far away from the best action with respect to their value functions and informative enough so that they communicate the uncertainties they have. Using data from an expert, we propose a new constraint-based approach that captures our desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients. Our results demonstrate that by using our constraints, we learn a high-performing policy, while considerably reducing the policy's variance over different datasets. We also explain how combining uncertainty estimation with these constraints can help us infer a partial ranking of actions that produce higher returns, and helps us infer safer and more informative policies for planning.
</details>
<details>
<summary>摘要</summary>
偏向学习是在医疗和教育等领域常用的sequential decision-making中广泛使用的。在这些领域中，报酬是知道的，并且需要根据批处数据来估算转移动力学T。任务的挑战之一是如何学习一个可靠地估算转移动力学T，以生成近似优质策略，并保证这些策略是安全的，即从价值函数中最佳动作的距离很远。使用专家数据，我们提出了一种新的约束基本方法，可以准确地学习 posterior分布中的转移动力学T，而不需要梯度。我们的结果表明，通过使用我们的约束，我们可以学习一个高性能的策略，同时减少策略的变量在不同数据集中。我们还解释了如何将不确定性估计与这些约束结合使用，以帮助我们推断更高的返回率的行为，并生成更安全和更有信息的策略。
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-and-Attention-based-Method-for-Gaze-Estimation-Using-Electroencephalography"><a href="#An-Interpretable-and-Attention-based-Method-for-Gaze-Estimation-Using-Electroencephalography" class="headerlink" title="An Interpretable and Attention-based Method for Gaze Estimation Using Electroencephalography"></a>An Interpretable and Attention-based Method for Gaze Estimation Using Electroencephalography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05768">http://arxiv.org/abs/2308.05768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nina Weng, Martyna Plomecka, Manuel Kaufmann, Ard Kastrati, Roger Wattenhofer, Nicolas Langer</li>
<li>for: 这paper是为了提出一种可解释的深度学习模型，用于基于EEG数据的视线估计。</li>
<li>methods: 该paper使用了一种新的注意力基于的深度学习框架，以便在EEG信号中提取最重要的信息，并且抑制问题的通道。</li>
<li>results: 该paper的研究表明，该模型在准确性和稳定性方面比现有方法更好，并且提供了可视化的结果，以便更好地理解EEG数据分析的结果。<details>
<summary>Abstract</summary>
Eye movements can reveal valuable insights into various aspects of human mental processes, physical well-being, and actions. Recently, several datasets have been made available that simultaneously record EEG activity and eye movements. This has triggered the development of various methods to predict gaze direction based on brain activity. However, most of these methods lack interpretability, which limits their technology acceptance. In this paper, we leverage a large data set of simultaneously measured Electroencephalography (EEG) and Eye tracking, proposing an interpretable model for gaze estimation from EEG data. More specifically, we present a novel attention-based deep learning framework for EEG signal analysis, which allows the network to focus on the most relevant information in the signal and discard problematic channels. Additionally, we provide a comprehensive evaluation of the presented framework, demonstrating its superiority over current methods in terms of accuracy and robustness. Finally, the study presents visualizations that explain the results of the analysis and highlights the potential of attention mechanism for improving the efficiency and effectiveness of EEG data analysis in a variety of applications.
</details>
<details>
<summary>摘要</summary>
眼动可以揭示人类心理过程、物理健康和行为方面的重要信息。最近，一些数据集被发布了，这些数据集同时记录了EEG活动和眼动。这些数据集的出现推动了基于EEG活动预测眼动方向的方法的开发。然而，大多数这些方法缺乏可解性，这限制了技术的接受度。在这篇论文中，我们利用了大量同时测量EEG和眼动的数据集，提出了一种可解的EEG信号分析模型，以便从EEG数据中预测眼动方向。更 Specifically，我们提出了一种基于注意力的深度学习框架，使得网络能够从EEG信号中提取最重要的信息，并且抛弃问题的通道。此外，我们进行了完整的评估，证明我们的方法在准确性和稳定性方面超过现有方法。最后，我们提供了可视化结果，解释分析结果并高亮了注意力机制的潜在改进EEG数据分析的效率和效果的潜在。
</details></li>
</ul>
<hr>
<h2 id="EEG-based-Emotion-Style-Transfer-Network-for-Cross-dataset-Emotion-Recognition"><a href="#EEG-based-Emotion-Style-Transfer-Network-for-Cross-dataset-Emotion-Recognition" class="headerlink" title="EEG-based Emotion Style Transfer Network for Cross-dataset Emotion Recognition"></a>EEG-based Emotion Style Transfer Network for Cross-dataset Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05767">http://arxiv.org/abs/2308.05767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijin Zhou, Fu Li, Yang Li, Youshuo Ji, Lijian Zhang, Yuanfang Chen, Wenming Zheng, Guangming Shi<br>for: 这篇研究旨在解决跨 dataset EEG 情感识别 tasks 中的 style mismatch 问题，以提高 EEG 情感识别的准确性。methods: 本研究提出了 EEG-based Emotion Style Transfer Network (E2STN)，用于从 source domain 和 target domain 中获取 EEG 表现，并将它们转换为新的类别化 EEG 表现，以具有 source domain 的内容信息和 target domain 的类别特征。results: 实验结果显示，E2STN 可以在跨 dataset EEG 情感识别任务中实现 state-of-the-art 的性能。<details>
<summary>Abstract</summary>
As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for cross-dataset discriminative prediction. Concretely, E2STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and target domains and then re-constructs the source domain's emotional pattern and the target domain's statistical characteristics into the new stylized EEG representations. In this process, the transfer evaluation module is adopted to constrain the generated representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting. Finally, the generated stylized EEG representations are fed into the discriminative prediction module for final classification. Extensive experiments show that the E2STN can achieve the state-of-the-art performance on cross-dataset EEG emotion recognition tasks.
</details>
<details>
<summary>摘要</summary>
As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for cross-dataset discriminative prediction. Concretely, E2STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and target domains and then re-constructs the source domain's emotional pattern and the target domain's statistical characteristics into the new stylized EEG representations. In this process, the transfer evaluation module is adopted to constrain the generated representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting. Finally, the generated stylized EEG representations are fed into the discriminative prediction module for final classification. Extensive experiments show that the E2STN can achieve the state-of-the-art performance on cross-dataset EEG emotion recognition tasks.Here's the translation in Traditional Chinese as well:As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for cross-dataset discriminative prediction. Concretely, E2STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and target domains and then re-constructs the source domain's emotional pattern and the target domain's statistical characteristics into the new stylized EEG representations. In this process, the transfer evaluation module is adopted to constrain the generated representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting. Finally, the generated stylized EEG representations are fed into the discriminative prediction module for final classification. Extensive experiments show that the E2STN can achieve the state-of-the-art performance on cross-dataset EEG emotion recognition tasks.
</details></li>
</ul>
<hr>
<h2 id="Prompting-In-Context-Operator-Learning-with-Sensor-Data-Equations-and-Natural-Language"><a href="#Prompting-In-Context-Operator-Learning-with-Sensor-Data-Equations-and-Natural-Language" class="headerlink" title="Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language"></a>Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05061">http://arxiv.org/abs/2308.05061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Yang, Tingwei Meng, Siting Liu, Stanley J. Osher</li>
<li>for: 学习物理学中的运算符，使用自然语言描述和方程来捕捉人类知识。</li>
<li>methods: 提出了一种多模态启发学习方法，使用”caption”来整合人类知识，并使用更高效的ICON-LM神经网络架构。</li>
<li>results: 实验结果表明，这种方法不仅能够提高学习性能和减少数据需求，还可以拓宽物理学中的应用范围。<details>
<summary>Abstract</summary>
In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICON-LM" for scientific machine learning tasks, which creates a new path for the application of language models.
</details>
<details>
<summary>摘要</summary>
在科学机器学习领域的发展中，在推理阶段从提示数据中学习操作符已经表现出了明显的潜力。然而，当前模型过于依赖感知数据，可能不充分利用人类知识对操作符的珍贵性。为此，我们提出一种将受ContextOperator学习转化为多模式 парадигмы的方法。我们提议使用“caption”来结合人类对操作符的知识，通过自然语言描述和方程表述。我们示出了这种方法不仅扩大了物理学习的灵活性和通用性，而且显著提高了学习性和数据需求。此外，我们介绍了一种更高效的多模式受ContextOperator学习神经网络架构，称为“ICON-LM”，基于语言模型类架构。我们示出了ICON-LM在科学机器学习任务中的可行性，创造了一条新的语言模型应用路径。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Method-for-improving-accuracy-in-neural-network-by-reinstating-traditional-back-propagation-technique"><a href="#A-Novel-Method-for-improving-accuracy-in-neural-network-by-reinstating-traditional-back-propagation-technique" class="headerlink" title="A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique"></a>A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05059">http://arxiv.org/abs/2308.05059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokulprasath R</li>
<li>for: 这个论文主要是为了提出一种新的快速参数更新方法，以解决深度学习中的计算负担和衰减问题。</li>
<li>methods: 该方法利用了一种新的快速参数更新策略，即不需要在每层计算Gradient的方法。</li>
<li>results: 对比州值数据集，该方法能够快速学习，避免衰减问题，并超过了现有的方法。<details>
<summary>Abstract</summary>
Deep learning has revolutionized industries like computer vision, natural language processing, and speech recognition. However, back propagation, the main method for training deep neural networks, faces challenges like computational overhead and vanishing gradients. In this paper, we propose a novel instant parameter update methodology that eliminates the need for computing gradients at each layer. Our approach accelerates learning, avoids the vanishing gradient problem, and outperforms state-of-the-art methods on benchmark data sets. This research presents a promising direction for efficient and effective deep neural network training.
</details>
<details>
<summary>摘要</summary>
深度学习已经革命化了计算机视觉、自然语言处理和语音识别等领域。然而，返回层的主要训练方法——归并，面临着计算负担和衰减 gradient 的问题。在这篇论文中，我们提出了一种新的快速参数更新方法，它消除了每层计算 gradients 的需求。我们的方法可以加速学习，避免衰减 gradient 问题，并在标准数据集上超越当前的状态艺。这篇研究表明了深度神经网络训练的有效和高效的可能性。
</details></li>
</ul>
<hr>
<h2 id="Sound-propagation-in-realistic-interactive-3D-scenes-with-parameterized-sources-using-deep-neural-operators"><a href="#Sound-propagation-in-realistic-interactive-3D-scenes-with-parameterized-sources-using-deep-neural-operators" class="headerlink" title="Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators"></a>Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05141">http://arxiv.org/abs/2308.05141</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dtu-act/deeponet-acoustic-wave-prop">https://github.com/dtu-act/deeponet-acoustic-wave-prop</a></li>
<li>paper_authors: Nikolas Borrel-Jensen, Somdatta Goswami, Allan P. Engsig-Karup, George Em Karniadakis, Cheol-Ho Jeong</li>
<li>for: 用于虚拟&#x2F;增强现实、游戏声音和空间计算等领域中的声场 simulate。</li>
<li>methods: 使用深度运算网络来近似线性波方程算子，以实现快速的声场传播预测。</li>
<li>results: 实现了在实际3D声学场景中逐源点预测声场的准确性，计算时间在毫秒级别，并且与参照解决方案之间的误差在0.02Pa到0.10Pa之间。<details>
<summary>Abstract</summary>
We address the challenge of sound propagation simulations in $3$D virtual rooms with moving sources, which have applications in virtual/augmented reality, game audio, and spatial computing. Solutions to the wave equation can describe wave phenomena such as diffraction and interference. However, simulating them using conventional numerical discretization methods with hundreds of source and receiver positions is intractable, making stimulating a sound field with moving sources impractical. To overcome this limitation, we propose using deep operator networks to approximate linear wave-equation operators. This enables the rapid prediction of sound propagation in realistic 3D acoustic scenes with moving sources, achieving millisecond-scale computations. By learning a compact surrogate model, we avoid the offline calculation and storage of impulse responses for all relevant source/listener pairs. Our experiments, including various complex scene geometries, show good agreement with reference solutions, with root mean squared errors ranging from 0.02 Pa to 0.10 Pa. Notably, our method signifies a paradigm shift as no prior machine learning approach has achieved precise predictions of complete wave fields within realistic domains. We anticipate that our findings will drive further exploration of deep neural operator methods, advancing research in immersive user experiences within virtual environments.
</details>
<details>
<summary>摘要</summary>
我们面临处理三维虚拟房间内的声波传播模拟中的挑战，这些应用包括虚拟现实、增强现实和空间计算。我们的方法可以描述声波现象如扩散和折射，但是使用传统的数值积分方法来模拟这些现象需要数百个源和接收器位置，这使得实现声场的类比映射成为不可能的。为了解决这个限制，我们提议使用深度算子网络来近似线性波方程式的算子。这使得在真实的三维声学场景中，快速预测声波传播的 computation 可以在毫秒级时间内完成。通过学习一个紧凑的模型，我们可以避免在所有相关的源 listener 组合上进行维护和储存对应数据。我们的实验包括多种复杂的场景几何，结果显示与参考解析相符，误差范围为0.02 Pa 至 0.10 Pa。值得注意的是，我们的方法代表了一种新的思维方式，没有过去的机器学习方法能够精确地预测完整的波场在真实的领域中。我们预期这些发现将驱动更多的深度神经算子方法研究，促进虚拟环境中的充满人体验的探索。
</details></li>
</ul>
<hr>
<h2 id="RadGraph2-Modeling-Disease-Progression-in-Radiology-Reports-via-Hierarchical-Information-Extraction"><a href="#RadGraph2-Modeling-Disease-Progression-in-Radiology-Reports-via-Hierarchical-Information-Extraction" class="headerlink" title="RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction"></a>RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05046">http://arxiv.org/abs/2308.05046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sameer Khanna, Adam Dejl, Kibo Yoon, Quoc Hung Truong, Hanh Duong, Agustina Saenz, Pranav Rajpurkar</li>
<li>for: 这个论文的目的是开发一个新的医学报告信息EXTRACTION dataset，以捕捉医疗器械报告中的疾病状态和设备置换的变化。</li>
<li>methods: 该论文使用了一个层次结构，将实体按照其关系进行组织，并在训练过程中使用这个结构，以提高信息EXTRACTION模型的性能。具体来说，该论文提出了一种修改后的 DyGIE++ 框架，称为 HGIE，该模型在实体和关系EXTRACTION任务中表现出色。</li>
<li>results: 根据 RadGraph2 数据集，该论文的 HGIE 模型可以更好地捕捉医疗器械报告中的各种发现，并在关系EXTRACTION任务中表现出色，比前一代模型更高。这种成果提供了开发自动跟踪疾病进程和开发基于医疗领域自然层次标签的信息EXTRACTION模型的基础。<details>
<summary>Abstract</summary>
We present RadGraph2, a novel dataset for extracting information from radiology reports that focuses on capturing changes in disease state and device placement over time. We introduce a hierarchical schema that organizes entities based on their relationships and show that using this hierarchy during training improves the performance of an information extraction model. Specifically, we propose a modification to the DyGIE++ framework, resulting in our model HGIE, which outperforms previous models in entity and relation extraction tasks. We demonstrate that RadGraph2 enables models to capture a wider variety of findings and perform better at relation extraction compared to those trained on the original RadGraph dataset. Our work provides the foundation for developing automated systems that can track disease progression over time and develop information extraction models that leverage the natural hierarchy of labels in the medical domain.
</details>
<details>
<summary>摘要</summary>
我们介绍RadGraph2，一个新的医学报告信息提取数据集，专注于时间上的疾病状态和设备安装变化。我们提出了一种层次结构，将实体按照关系组织，并证明在训练过程中使用这种层次结构可以提高信息提取模型的性能。我们对 DyGIE++ 框架进行修改，得到了我们的 HGIE 模型，该模型在实体和关系提取任务中表现更好。我们示出 RadGraph2 可以让模型捕捉更多的发现和在关系提取任务中表现更好，相比于基于原始 RadGraph 数据集训练的模型。我们的工作为Automated systems的开发提供了基础，以便在医疗领域自动跟踪疾病进程和开发利用医疗领域自然的标签层次结构的信息提取模型。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Wideband-Spectrum-Sensing-and-Scheduling-for-Networked-UAVs-in-UTM-Systems"><a href="#Collaborative-Wideband-Spectrum-Sensing-and-Scheduling-for-Networked-UAVs-in-UTM-Systems" class="headerlink" title="Collaborative Wideband Spectrum Sensing and Scheduling for Networked UAVs in UTM Systems"></a>Collaborative Wideband Spectrum Sensing and Scheduling for Networked UAVs in UTM Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05036">http://arxiv.org/abs/2308.05036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sravan Reddy Chintareddy, Keenan Roach, Kenny Cheung, Morteza Hashemi</li>
<li>for: 本文提出了一种基于数据驱动的协同宽频谱感知和调度框架，用于网络无人机飞行器（UAV）作为次级用户，机会性地利用探测到的谱带孔隙。</li>
<li>methods: 我们提出了一种多类分类问题来探测宽频谱中的空闲谱带，基于收集的I&#x2F;Q样本。为了提高谱感知模块的准确性，每个个人UAV的输出将在UTM生态系统中的服务器进行融合。</li>
<li>results: 我们在spectrum scheduling阶段使用了强化学习（RL）解决方案来动态分配探测到的谱带孔隙给次级用户（即UAV）。<details>
<summary>Abstract</summary>
In this paper, we propose a data-driven framework for collaborative wideband spectrum sensing and scheduling for networked unmanned aerial vehicles (UAVs), which act as the secondary users to opportunistically utilize detected spectrum holes. To this end, we propose a multi-class classification problem for wideband spectrum sensing to detect vacant spectrum spots based on collected I/Q samples. To enhance the accuracy of the spectrum sensing module, the outputs from the multi-class classification by each individual UAV are fused at a server in the unmanned aircraft system traffic management (UTM) ecosystem. In the spectrum scheduling phase, we leverage reinforcement learning (RL) solutions to dynamically allocate the detected spectrum holes to the secondary users (i.e., UAVs). To evaluate the proposed methods, we establish a comprehensive simulation framework that generates a near-realistic synthetic dataset using MATLAB LTE toolbox by incorporating base-station~(BS) locations in a chosen area of interest, performing ray-tracing, and emulating the primary users channel usage in terms of I/Q samples. This evaluation methodology provides a flexible framework to generate large spectrum datasets that could be used for developing ML/AI-based spectrum management solutions for aerial devices.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个基于数据的框架，用于无人机（UAV）网络中进行共同宽频谱感知和调度。这些无人机作为次要用户，可以利用探测到的谱频孔的机会来进行兼抓式使用。为实现这一目标，我们提出了一个多类分类问题，用于宽频谱感知，以检测谱频孔的存在。为了提高谱频感知模块的准确性，每个无人机的多类分类输出将在UTM（无人机系统交通管理）环境中的服务器进行融合。在调度阶段，我们利用强化学习（RL）解决方案，动态地将探测到的谱频孔分配给次要用户（即无人机）。为评估我们的方法，我们建立了一个完整的 simulate评估框架，通过使用MATLAB LTE工具包，在选定的区域内，基站（BS）的位置、短距离通信和主要用户通信的I/Q样本来生成一个准确的数据集。这种评估方法ология提供了一个灵活的框架，可以用于开发ML/AI基于谱管理解决方案。
</details></li>
</ul>
<hr>
<h2 id="Kairos-Practical-Intrusion-Detection-and-Investigation-using-Whole-system-Provenance"><a href="#Kairos-Practical-Intrusion-Detection-and-Investigation-using-Whole-system-Provenance" class="headerlink" title="Kairos: Practical Intrusion Detection and Investigation using Whole-system Provenance"></a>Kairos: Practical Intrusion Detection and Investigation using Whole-system Provenance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05034">http://arxiv.org/abs/2308.05034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/provenanceanalytics/kairos">https://github.com/provenanceanalytics/kairos</a></li>
<li>paper_authors: Zijun Cheng, Qiujian Lv, Jinyuan Liang, Yan Wang, Degang Sun, Thomas Pasquier, Xueyuan Han</li>
<li>for: 本研究旨在提出一种基于证据图的入侵检测系统（PIDS），以检测现代的攻击活动。</li>
<li>methods: 该研究使用了一种新的图神经网络编码器-解码器架构，通过学习系统审计日志中 Structural 变化的时间演化来评估系统事件的异常程度。</li>
<li>results: 根据实验结果，Kairos 可以同时满足四个维度的需求，而现有方法则缺乏至少一个维度。 Kairos 可以快速、高效地监控主机系统，并且可以生成精炼的攻击摘要图，以便系统管理员快速理解和应对系统入侵。<details>
<summary>Abstract</summary>
Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approaches sacrifice at least one and struggle to achieve comparable detection performance.   Kairos leverages a novel graph neural network-based encoder-decoder architecture that learns the temporal evolution of a provenance graph's structural changes to quantify the degree of anomalousness for each system event. Then, based on this fine-grained information, Kairos reconstructs attack footprints, generating compact summary graphs that accurately describe malicious activity over a stream of system audit logs. Using state-of-the-art benchmark datasets, we demonstrate that Kairos outperforms previous approaches.
</details>
<details>
<summary>摘要</summary>
Provenance graphs are 系统执行历史记录，现代研究探讨了多种分析方法，以检测系统执行过程中的攻击。从设计文档中，我们确定了四个纬度驱动了基于证明的攻击检测系统（PIDS）的开发：范围（可以检测现代攻击？）、不偏见（可以检测新型攻击？）、实时性（可以高效监控主机系统？）和攻击重建（可以将攻击活动简化为可读的形式？）。我们介绍了 Kairos，第一个满足所有四个纬度的 PIDS，而现有方法至少牺牲一个纬度，并具有相似的检测性能。 Kairos 使用一种新的图神经网络基本 Encoder-Decoder 架构，学习系统审计记录中的时间演化结构变化，以衡量每个系统事件的异常程度。然后，基于这些细腻信息，Kairos 重建攻击印记，生成系统审计记录流中的简洁摘要图，准确描述攻击活动。使用现代标准数据集，我们证明了 Kairos 在检测方面的优越性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/10/cs.LG_2023_08_10/" data-id="clly4xtdu006jvl8814lr9e0q" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/10/cs.SD_2023_08_10/" class="article-date">
  <time datetime="2023-08-09T16:00:00.000Z" itemprop="datePublished">2023-08-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/10/cs.SD_2023_08_10/">cs.SD - 2023-08-10 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Stabilizing-Training-with-Soft-Dynamic-Time-Warping-A-Case-Study-for-Pitch-Class-Estimation-with-Weakly-Aligned-Targets"><a href="#Stabilizing-Training-with-Soft-Dynamic-Time-Warping-A-Case-Study-for-Pitch-Class-Estimation-with-Weakly-Aligned-Targets" class="headerlink" title="Stabilizing Training with Soft Dynamic Time Warping: A Case Study for Pitch Class Estimation with Weakly Aligned Targets"></a>Stabilizing Training with Soft Dynamic Time Warping: A Case Study for Pitch Class Estimation with Weakly Aligned Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05429">http://arxiv.org/abs/2308.05429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/groupmm/stabilizing_sdtw">https://github.com/groupmm/stabilizing_sdtw</a></li>
<li>paper_authors: Johannes Zeitler, Simon Deniffel, Michael Krause, Meinard Müller</li>
<li>for: 这篇论文旨在提高难以对齐的神经网络训练方法的稳定性。</li>
<li>methods: 论文使用了流动时间扭曲函数（SDTW），并研究了三种不同的稳定策略，以解决在训练过程中软对齐结果与参考对齐结果之间的差异导致的参数更新错误。</li>
<li>results: 实验表明，三种稳定策略都能够稳定训练过程，并且可以提高训练结果的准确性。<details>
<summary>Abstract</summary>
Soft dynamic time warping (SDTW) is a differentiable loss function that allows for training neural networks from weakly aligned data. Typically, SDTW is used to iteratively compute and refine soft alignments that compensate for temporal deviations between the training data and its weakly annotated targets. One major problem is that a mismatch between the estimated soft alignments and the reference alignments in the early training stage leads to incorrect parameter updates, making the overall training procedure unstable. In this paper, we investigate such stability issues by considering the task of pitch class estimation from music recordings as an illustrative case study. In particular, we introduce and discuss three conceptually different strategies (a hyperparameter scheduling, a diagonal prior, and a sequence unfolding strategy) with the objective of stabilizing intermediate soft alignment results. Finally, we report on experiments that demonstrate the effectiveness of the strategies and discuss efficiency and implementation issues.
</details>
<details>
<summary>摘要</summary>
“软时间截断函数（SDTW）是一个可微分损失函数，允许训练神经网络从弱相关数据上。通常，SDTW 用于逐步计算和更新软掌握结果，以补偿音乐录音中的时间偏差。然而，在训练过程的早期，假设软掌握结果和参考掌握结果之间的差异，会导致对参数的错误更新，使整个训练过程不稳定。在这篇研究中，我们 investigates 这些稳定问题，通过将乐器识别 зада项作为一个示例研究。 Specifically, we introduce and discuss three conceptually different strategies（一个参数调整、一个主成分矩阵、和一个序列竖范例），以实现软掌握结果的稳定。最后，我们报告了实验结果，并讨论了效率和实现问题。”I hope this helps! Let me know if you have any further questions.
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Self-training-Approach-for-Low-resource-Speech-Recognition"><a href="#A-Novel-Self-training-Approach-for-Low-resource-Speech-Recognition" class="headerlink" title="A Novel Self-training Approach for Low-resource Speech Recognition"></a>A Novel Self-training Approach for Low-resource Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05269">http://arxiv.org/abs/2308.05269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satwinder Singh, Feng Hou, Ruili Wang</li>
<li>for: 提高低资源语言自动语音识别（ASR）的精度</li>
<li>methods: 提出了一种自动训练方法，使用生成高精度pseudo标签来优化低资源语言语音识别</li>
<li>results: 实验结果显示，我们的方法可以显著提高word error rate，相比基eline模型，实现了14.94%的相对改进，并在四个实际语音Dataset上达到最佳结果。<details>
<summary>Abstract</summary>
In this paper, we propose a self-training approach for automatic speech recognition (ASR) for low-resource settings. While self-training approaches have been extensively developed and evaluated for high-resource languages such as English, their applications to low-resource languages like Punjabi have been limited, despite the language being spoken by millions globally. The scarcity of annotated data has hindered the development of accurate ASR systems, especially for low-resource languages (e.g., Punjabi and M\=aori languages). To address this issue, we propose an effective self-training approach that generates highly accurate pseudo-labels for unlabeled low-resource speech. Our experimental analysis demonstrates that our approach significantly improves word error rate, achieving a relative improvement of 14.94% compared to a baseline model across four real speech datasets. Further, our proposed approach reports the best results on the Common Voice Punjabi dataset.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种自动听说训练方法，用于低资源语言自动听说识别（ASR）。虽然自动听说训练方法在高资源语言如英语等语言上已经广泛开发和评估，但对于低资源语言如旁风语和马毛语等语言的应用却有限，尽管这些语言在全球范围内有数百万人使用。由于低资源语言的缺乏标注数据，尤其是低资源语言的ASR系统的发展受到了很大阻碍。为了解决这个问题，我们提出了一种高效的自动听说训练方法，可以生成高度准确的 Pseudo-标签 для无标注的低资源语音。我们的实验分析表明，我们的方法可以在四个真实语音数据集上显著改善单词错误率，相比基eline模型，实现了14.94%的相对提升。此外，我们的提出方法在Common Voice旁风语数据集上得到了最好的结果。
</details></li>
</ul>
<hr>
<h2 id="Separate-Anything-You-Describe"><a href="#Separate-Anything-You-Describe" class="headerlink" title="Separate Anything You Describe"></a>Separate Anything You Describe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05037">http://arxiv.org/abs/2308.05037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audio-agi/audiosep">https://github.com/audio-agi/audiosep</a></li>
<li>paper_authors: Xubo Liu, Qiuqiang Kong, Yan Zhao, Haohe Liu, Yi Yuan, Yuzhuo Liu, Rui Xia, Yuxuan Wang, Mark D. Plumbley, Wenwu Wang</li>
<li>for: 这个研究是为了开发一个可以通过自然语言查询来进行 Computational Auditory Scene Analysis (CASA) 的新模型，即 Language-queried audio source separation (LASS)。</li>
<li>methods: 本研究使用了大量多modal的数据集来训练基础模型 AudioSep，并对多个任务进行了广泛的评估，包括音乐器分类、语音提高和语音分类。</li>
<li>results: AudioSep 能够实现优秀的分类性能和零基础学习能力，使用音频描述或文本标签作为查询，较前一代的音频-queried和语言-queried音频分类模型有所进步。<details>
<summary>Abstract</summary>
Language-queried audio source separation (LASS) is a new paradigm for computational auditory scene analysis (CASA). LASS aims to separate a target sound from an audio mixture given a natural language query, which provides a natural and scalable interface for digital audio applications. Recent works on LASS, despite attaining promising separation performance on specific sources (e.g., musical instruments, limited classes of audio events), are unable to separate audio concepts in the open domain. In this work, we introduce AudioSep, a foundation model for open-domain audio source separation with natural language queries. We train AudioSep on large-scale multimodal datasets and extensively evaluate its capabilities on numerous tasks including audio event separation, musical instrument separation, and speech enhancement. AudioSep demonstrates strong separation performance and impressive zero-shot generalization ability using audio captions or text labels as queries, substantially outperforming previous audio-queried and language-queried sound separation models. For reproducibility of this work, we will release the source code, evaluation benchmark and pre-trained model at: https://github.com/Audio-AGI/AudioSep.
</details>
<details>
<summary>摘要</summary>
语言查询音频源分离（LASS）是一种新的计算听音场景分析（CASA）的 paradigma。 LASS的目标是，给一个自然语言查询，从音频混合中分离一个目标声音。这提供了一个自然和可扩展的界面 для数字音频应用程序。Recent works on LASS，尽管在特定的源（例如乐器）上达到了预期的分离性能，但无法在开放领域中分离音频概念。在这种工作中，我们引入AudioSep，一个基础模型 для开放领域音频源分离，使用自然语言查询。我们在大规模的多modal数据集上训练AudioSep，并对其在多种任务中进行了广泛的评估，包括音频事件分离、乐器分离和语音提高。AudioSep示出了强大的分离性能和零shot泛化能力，使用音频标签或文本查询，明显超过了之前的音频-queried和语言-queried声音分离模型。为了保证这个工作的可重现性，我们将在 GitHub 上发布源代码、评估标准和预训练模型：https://github.com/Audio-AGI/AudioSep。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/10/cs.SD_2023_08_10/" data-id="clly4xtep009uvl88bbvt8g7c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/10/eess.IV_2023_08_10/" class="article-date">
  <time datetime="2023-08-09T16:00:00.000Z" itemprop="datePublished">2023-08-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/10/eess.IV_2023_08_10/">eess.IV - 2023-08-10 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Attention-based-3D-CNN-with-Multi-layer-Features-for-Alzheimer’s-Disease-Diagnosis-using-Brain-Images"><a href="#Attention-based-3D-CNN-with-Multi-layer-Features-for-Alzheimer’s-Disease-Diagnosis-using-Brain-Images" class="headerlink" title="Attention-based 3D CNN with Multi-layer Features for Alzheimer’s Disease Diagnosis using Brain Images"></a>Attention-based 3D CNN with Multi-layer Features for Alzheimer’s Disease Diagnosis using Brain Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05655">http://arxiv.org/abs/2308.05655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanteng Zhang, Qizhi Teng, Xiaohai He, Tong Niu, Lipei Zhang, Yan Liu, Chao Ren</li>
<li>for: 这个论文旨在提高亚束病（AD）诊断的准确率，通过利用深度学习的 convolutional neural network（CNN）方法，并结合多层特征获取器和注意力机制来更好地捕捉脑图像中的微妙差异。</li>
<li>methods: 本文提出了一种基于ResNet的端到端3D CNN框架，并在多Modal imaging数据上进行了ablation实验，以评估模型的性能。</li>
<li>results: 实验结果显示，我们的方法可以在792名ADNI数据库中的subject上达到89.71%和91.18%的AD诊断精度，并超越了一些当前state-of-the-art方法。另外，我们的模型还可以在不同的脑区域中强调关键区域，并且可以更好地捕捉到脑图像中的微妙差异。<details>
<summary>Abstract</summary>
Structural MRI and PET imaging play an important role in the diagnosis of Alzheimer's disease (AD), showing the morphological changes and glucose metabolism changes in the brain respectively. The manifestations in the brain image of some cognitive impairment patients are relatively inconspicuous, for example, it still has difficulties in achieving accurate diagnosis through sMRI in clinical practice. With the emergence of deep learning, convolutional neural network (CNN) has become a valuable method in AD-aided diagnosis, but some CNN methods cannot effectively learn the features of brain image, making the diagnosis of AD still presents some challenges. In this work, we propose an end-to-end 3D CNN framework for AD diagnosis based on ResNet, which integrates multi-layer features obtained under the effect of the attention mechanism to better capture subtle differences in brain images. The attention maps showed our model can focus on key brain regions related to the disease diagnosis. Our method was verified in ablation experiments with two modality images on 792 subjects from the ADNI database, where AD diagnostic accuracies of 89.71% and 91.18% were achieved based on sMRI and PET respectively, and also outperformed some state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
estructural MRI和PET成像在诊断阿尔ц海默病（AD）中发挥重要作用，显示了大脑的结构变化和葡萄糖代谢变化。在一些 когнитив障碍患者的脑图像中，manifestations 不够醒目，例如，仍然存在在严格实践中获取正确诊断的困难。随着深度学习的出现，卷积神经网络（CNN）成为了AD诊断的有价值方法。然而，一些CNN方法无法有效地学习脑图像的特征，使得AD诊断仍然存在一些挑战。在这项工作中，我们提出了基于ResNet的终端3D CNN框架 для AD诊断，该框架通过多层效应机制来更好地捕捉脑图像中的微妙差异。我们的Attention map表明，我们的模型可以关注与疾病诊断相关的关键脑区域。我们的方法在ADNI数据库上进行了ablation实验，使用了两种模式图像，其中AD诊断精度分别为89.71%和91.18%，并且超越了一些当前的方法。
</details></li>
</ul>
<hr>
<h2 id="SAR-Target-Image-Generation-Method-Using-Azimuth-Controllable-Generative-Adversarial-Network"><a href="#SAR-Target-Image-Generation-Method-Using-Azimuth-Controllable-Generative-Adversarial-Network" class="headerlink" title="SAR Target Image Generation Method Using Azimuth-Controllable Generative Adversarial Network"></a>SAR Target Image Generation Method Using Azimuth-Controllable Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05489">http://arxiv.org/abs/2308.05489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Xiaoyu Liu, Yulin Huang, Deqing Mao, Yin Zhang, Jianyu Yang</li>
<li>for: 提高Synthetic Aperture Radar（SAR）图像的研究进展，增加SAR图像数据量，解决实际中SAR图像数据的有限性问题。</li>
<li>methods: 提出了一种azimuth-controllableGenerative Adversarial Network（GAN），通过将两个SAR图像的特征提取并融合，生成具有中间方位角的SAR图像。network主要包括生成器、推理器、判别器三部分。</li>
<li>results: 经过广泛的实验 validate the effectiveness of the proposed method in terms of azimuth controllability and accuracy of SAR target image generation. The proposed method can generate high-quality SAR target images with controllable azimuth, which can help improve the development of SAR research and solve the problem of limited SAR image data to some extent.<details>
<summary>Abstract</summary>
Sufficient synthetic aperture radar (SAR) target images are very important for the development of researches. However, available SAR target images are often limited in practice, which hinders the progress of SAR application. In this paper, we propose an azimuth-controllable generative adversarial network to generate precise SAR target images with an intermediate azimuth between two given SAR images' azimuths. This network mainly contains three parts: generator, discriminator, and predictor. Through the proposed specific network structure, the generator can extract and fuse the optimal target features from two input SAR target images to generate SAR target image. Then a similarity discriminator and an azimuth predictor are designed. The similarity discriminator can differentiate the generated SAR target images from the real SAR images to ensure the accuracy of the generated, while the azimuth predictor measures the difference of azimuth between the generated and the desired to ensure the azimuth controllability of the generated. Therefore, the proposed network can generate precise SAR images, and their azimuths can be controlled well by the inputs of the deep network, which can generate the target images in different azimuths to solve the small sample problem to some degree and benefit the researches of SAR images. Extensive experimental results show the superiority of the proposed method in azimuth controllability and accuracy of SAR target image generation.
</details>
<details>
<summary>摘要</summary>
充分的synthetic aperture radar（SAR）目标图像对SAR应用的研究发展非常重要，但实际中可用的SAR目标图像很少，这限制了SAR应用的进步。在这篇论文中，我们提出了一种 azimuth可控的生成对抗网络，用于生成具有中间azimuth的准确SAR目标图像。该网络主要由三部分组成：生成器、判别器和预测器。通过我们提出的特定网络结构，生成器可以从两个输入SAR目标图像中提取和融合最佳的目标特征，生成SAR目标图像。然后，我们设计了一个相似性判别器和一个方向预测器。相似性判别器可以判别生成的SAR目标图像与真实SAR图像之间的差异，以确保生成的准确性。方向预测器则可以测量生成的azimuth与所需的azimuth之间的差异，以确保生成的azimuth可控性。因此，我们的网络可以生成准确的SAR目标图像，并且可以通过输入深度网络的参数控制生成的azimuth。这可以生成不同的azimuth的目标图像，解决一定程度上的样本缺乏问题，并为SAR图像研究带来一定的助益。我们的实验结果表明，我们的方法在azimuth可控性和SAR目标图像生成准确性两个方面具有优越性。
</details></li>
</ul>
<hr>
<h2 id="Surface-Masked-AutoEncoder-Self-Supervision-for-Cortical-Imaging-Data"><a href="#Surface-Masked-AutoEncoder-Self-Supervision-for-Cortical-Imaging-Data" class="headerlink" title="Surface Masked AutoEncoder: Self-Supervision for Cortical Imaging Data"></a>Surface Masked AutoEncoder: Self-Supervision for Cortical Imaging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05474">http://arxiv.org/abs/2308.05474</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/metrics-lab/surface-vision-transformers">https://github.com/metrics-lab/surface-vision-transformers</a></li>
<li>paper_authors: Simon Dahan, Mariana da Silva, Daniel Rueckert, Emma C Robinson</li>
<li>for:  cortical surface learning and cortical phenotype regression</li>
<li>methods: Masked AutoEncoder (MAE) self-supervision</li>
<li>results: 26% improvement in performance, 80% faster convergence compared to models trained from scratch, and robust representations for finetuning in low-data scenarios.Here’s the simplified Chinese text:</li>
<li>for:  cortical表面学习和 cortical表型识别</li>
<li>methods: Masked AutoEncoder (MAE)自我超vision</li>
<li>results: 26%提高表现, 80%更快的整合比对于从 scratch 训练的模型I hope this helps!<details>
<summary>Abstract</summary>
Self-supervision has been widely explored as a means of addressing the lack of inductive biases in vision transformer architectures, which limits generalisation when networks are trained on small datasets. This is crucial in the context of cortical imaging, where phenotypes are complex and heterogeneous, but the available datasets are limited in size. This paper builds upon recent advancements in translating vision transformers to surface meshes and investigates the potential of Masked AutoEncoder (MAE) self-supervision for cortical surface learning. By reconstructing surface data from a masked version of the input, the proposed method effectively models cortical structure to learn strong representations that translate to improved performance in downstream tasks. We evaluate our approach on cortical phenotype regression using the developing Human Connectome Project (dHCP) and demonstrate that pre-training leads to a 26\% improvement in performance, with an 80\% faster convergence, compared to models trained from scratch. Furthermore, we establish that pre-training vision transformer models on large datasets, such as the UK Biobank (UKB), enables the acquisition of robust representations for finetuning in low-data scenarios. Our code and pre-trained models are publicly available at \url{https://github.com/metrics-lab/surface-vision-transformers}.
</details>
<details>
<summary>摘要</summary>
自我监督已经广泛研究以Addressing the lack of inductive biases in vision transformer architectures, which limits generalization when networks are trained on small datasets. This is crucial in the context of cortical imaging, where phenotypes are complex and heterogeneous, but the available datasets are limited in size. This paper builds upon recent advancements in translating vision transformers to surface meshes and investigates the potential of Masked AutoEncoder (MAE) self-supervision for cortical surface learning. By reconstructing surface data from a masked version of the input, the proposed method effectively models cortical structure to learn strong representations that translate to improved performance in downstream tasks. We evaluate our approach on cortical phenotype regression using the developing Human Connectome Project (dHCP) and demonstrate that pre-training leads to a 26% improvement in performance, with an 80% faster convergence, compared to models trained from scratch. Furthermore, we establish that pre-training vision transformer models on large datasets, such as the UK Biobank (UKB), enables the acquisition of robust representations for finetuning in low-data scenarios. Our code and pre-trained models are publicly available at \url{https://github.com/metrics-lab/surface-vision-transformers}.Here's the word-for-word translation of the text into Simplified Chinese:自我监督已经广泛研究以Addressing the lack of inductive biases in vision transformer architectures, which limits generalization when networks are trained on small datasets. This is crucial in the context of cortical imaging, where phenotypes are complex and heterogeneous, but the available datasets are limited in size. This paper builds upon recent advancements in translating vision transformers to surface meshes and investigates the potential of Masked AutoEncoder (MAE) self-supervision for cortical surface learning. By reconstructing surface data from a masked version of the input, the proposed method effectively models cortical structure to learn strong representations that translate to improved performance in downstream tasks. We evaluate our approach on cortical phenotype regression using the developing Human Connectome Project (dHCP) and demonstrate that pre-training leads to a 26% improvement in performance, with an 80% faster convergence, compared to models trained from scratch. Furthermore, we establish that pre-training vision transformer models on large datasets, such as the UK Biobank (UKB), enables the acquisition of robust representations for finetuning in low-data scenarios. Our code and pre-trained models are publicly available at \url{https://github.com/metrics-lab/surface-vision-transformers}.
</details></li>
</ul>
<hr>
<h2 id="Global-in-Local-A-Convolutional-Transformer-for-SAR-ATR-FSL"><a href="#Global-in-Local-A-Convolutional-Transformer-for-SAR-ATR-FSL" class="headerlink" title="Global in Local: A Convolutional Transformer for SAR ATR FSL"></a>Global in Local: A Convolutional Transformer for SAR ATR FSL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05464">http://arxiv.org/abs/2308.05464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Yulin Huang, Xiaoyu Liu, Jifang Pei, Yin Zhang, Jianyu Yang</li>
<li>for: 提高Synthetic Aperture Radar（SAR）自动目标识别（ATR）的性能，特别是在有限的SAR图像下进行几抽取学习（FSL）。</li>
<li>methods: 提出了一种Convolutional Transformer（ConvT）模型，通过建立层次特征表示和捕捉每层局部特征的全局依赖关系，提高SAR ATR FSL的性能。同时，提出了一种新的混合损失函数，可以在少量SAR图像下进行有效的优化。</li>
<li>results: 在Moving and Stationary Target Acquisition and Recognition（MSTAR）数据集上进行了实验，显示了我们提出的ConvT模型在SAR ATR FSL中的效果，不需要其他SAR目标图像进行训练。与现有的SAR ATR FSL方法相比，我们的方法可以更好地适应有限的SAR图像下的几抽取学习。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have dominated the synthetic aperture radar (SAR) automatic target recognition (ATR) for years. However, under the limited SAR images, the width and depth of the CNN-based models are limited, and the widening of the received field for global features in images is hindered, which finally leads to the low performance of recognition. To address these challenges, we propose a Convolutional Transformer (ConvT) for SAR ATR few-shot learning (FSL). The proposed method focuses on constructing a hierarchical feature representation and capturing global dependencies of local features in each layer, named global in local. A novel hybrid loss is proposed to interpret the few SAR images in the forms of recognition labels and contrastive image pairs, construct abundant anchor-positive and anchor-negative image pairs in one batch and provide sufficient loss for the optimization of the ConvT to overcome the few sample effect. An auto augmentation is proposed to enhance and enrich the diversity and amount of the few training samples to explore the hidden feature in a few SAR images and avoid the over-fitting in SAR ATR FSL. Experiments conducted on the Moving and Stationary Target Acquisition and Recognition dataset (MSTAR) have shown the effectiveness of our proposed ConvT for SAR ATR FSL. Different from existing SAR ATR FSL methods employing additional training datasets, our method achieved pioneering performance without other SAR target images in training.
</details>
<details>
<summary>摘要</summary>
干扰神经网络（CNN）在抽象辐射镜（SAR）自动目标识别（ATR）领域已经占据了多年。然而，在有限的SAR图像下，CNN模型的宽度和深度受限，而且在图像中收集全局特征的宽度也受到限制，最终导致识别性能低下。为解决这些挑战，我们提议了一种干扰转换器（ConvT） дляSAR ATR几步学习（FSL）。我们的方法关注于建立层次特征表示和在每层 capture全局依赖的本地特征，称为全球在本地。我们还提出了一种新的混合损失函数，可以将少量SAR图像转化为识别标签和对比图像对，建立丰富的anchor-正例和anchor-负例图像对在一个批中，并提供足够的损失来优化ConvT，以超越几个样本效应。我们还提出了一种自动增强技术，以增加和拓宽少量训练样本，探索隐藏在少量SAR图像中的特征，避免过拟合。我们在MSTAR数据集上进行了实验，结果表明我们的提议的ConvT有效地应用于SAR ATR FSL。与现有的SAR ATR FSL方法不同，我们的方法不需要其他SAR目标图像进行训练，而达到了领先性能。
</details></li>
</ul>
<hr>
<h2 id="Transforming-Breast-Cancer-Diagnosis-Towards-Real-Time-Ultrasound-to-Mammogram-Conversion-for-Cost-Effective-Diagnosis"><a href="#Transforming-Breast-Cancer-Diagnosis-Towards-Real-Time-Ultrasound-to-Mammogram-Conversion-for-Cost-Effective-Diagnosis" class="headerlink" title="Transforming Breast Cancer Diagnosis: Towards Real-Time Ultrasound to Mammogram Conversion for Cost-Effective Diagnosis"></a>Transforming Breast Cancer Diagnosis: Towards Real-Time Ultrasound to Mammogram Conversion for Cost-Effective Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05449">http://arxiv.org/abs/2308.05449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahar Almahfouz Nasser, Ashutosh Sharma, Anmol Saraf, Amruta Mahendra Parulekar, Purvi Haria, Amit Sethi</li>
<li>for: 提高实时ultrasound（US）图像质量，以提高手术过程中的医疗效果。</li>
<li>methods: 使用Stride软件数学模型，通过解决波方程来生成ultrasound图像，并利用域适应来增强模拟图像的真实性。使用生成敌对网络（GANs）解决反向问题，即将ultrasound图像转换为高质量的mammogram图像。</li>
<li>results: 实验结果表明，使用提出的方法可以生成高度可识别的ultrasound图像，具有明显更高的细节程度和质量，比原始US图像更有利于手术过程。<details>
<summary>Abstract</summary>
Ultrasound (US) imaging is better suited for intraoperative settings because it is real-time and more portable than other imaging techniques, such as mammography. However, US images are characterized by lower spatial resolution noise-like artifacts. This research aims to address these limitations by providing surgeons with mammogram-like image quality in real-time from noisy US images. Unlike previous approaches for improving US image quality that aim to reduce artifacts by treating them as (speckle noise), we recognize their value as informative wave interference pattern (WIP). To achieve this, we utilize the Stride software to numerically solve the forward model, generating ultrasound images from mammograms images by solving wave-equations. Additionally, we leverage the power of domain adaptation to enhance the realism of the simulated ultrasound images. Then, we utilize generative adversarial networks (GANs) to tackle the inverse problem of generating mammogram-quality images from ultrasound images. The resultant images have considerably more discernible details than the original US images.
</details>
<details>
<summary>摘要</summary>
超声成像（US）在操作间更适合使用，因为它是实时的，更携带性好于其他成像技术，如胸写影像。然而，US图像受到低分辨率噪声杂乱的影响。本研究目的是提供高品质的胸写影像，以便在实时中为外科医生提供更好的图像。不同于之前的方法，我们不是通过减少噪声来提高US图像质量，而是认可噪声为有用的波动干扰 patrern（WIP）。为此，我们利用Stride软件来数学模拟前向模型，将胸写影像转换成超声图像，并利用领域适应来增强模拟的超声图像的真实性。然后，我们使用生成对抗网络（GANs）来解决胸写影像转换成高品质的超声图像的逆问题。得到的图像具有较原始US图像更多的可识别细节。
</details></li>
</ul>
<hr>
<h2 id="A-Generalized-Physical-knowledge-guided-Dynamic-Model-for-Underwater-Image-Enhancement"><a href="#A-Generalized-Physical-knowledge-guided-Dynamic-Model-for-Underwater-Image-Enhancement" class="headerlink" title="A Generalized Physical-knowledge-guided Dynamic Model for Underwater Image Enhancement"></a>A Generalized Physical-knowledge-guided Dynamic Model for Underwater Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05447">http://arxiv.org/abs/2308.05447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pan Mu, Hanning Xu, Zheyuan Liu, Zheng Wang, Sixian Chan, Cong Bai</li>
<li>For: This paper proposes a Generalized Underwater image enhancement method via a Physical-knowledge-guided Dynamic Model (GUPDM) to tackle the challenges of color distortion and low contrast in underwater images.* Methods: The GUPDM method consists of three parts: Atmosphere-based Dynamic Structure (ADS), Transmission-guided Dynamic Structure (TDS), and Prior-based Multi-scale Structure (PMS). The ADS and TDS modules use dynamic convolutions to adaptively extract prior information from underwater images and generate parameters for PMS. The PMS module uses convolution blocks with different kernel sizes and channel attention blocks to fuse multi-scale features.* Results: The proposed GUPDM method can effectively enhance the quality of underwater images and simulate various underwater image types, such as yellow to blue color ranging. The source code will be available at <a target="_blank" rel="noopener" href="https://github.com/shiningZZ/GUPDM">https://github.com/shiningZZ/GUPDM</a>.<details>
<summary>Abstract</summary>
Underwater images often suffer from color distortion and low contrast resulting in various image types, due to the scattering and absorption of light by water. While it is difficult to obtain high-quality paired training samples with a generalized model. To tackle these challenges, we design a Generalized Underwater image enhancement method via a Physical-knowledge-guided Dynamic Model (short for GUPDM), consisting of three parts: Atmosphere-based Dynamic Structure (ADS), Transmission-guided Dynamic Structure (TDS), and Prior-based Multi-scale Structure (PMS). In particular, to cover complex underwater scenes, this study changes the global atmosphere light and the transmission to simulate various underwater image types (e.g., the underwater image color ranging from yellow to blue) through the formation model. We then design ADS and TDS that use dynamic convolutions to adaptively extract prior information from underwater images and generate parameters for PMS. These two modules enable the network to select appropriate parameters for various water types adaptively. Besides, the multi-scale feature extraction module in PMS uses convolution blocks with different kernel sizes and obtains weights for each feature map via channel attention block and fuses them to boost the receptive field of the network. The source code will be available at \href{https://github.com/shiningZZ/GUPDM}{https://github.com/shiningZZ/GUPDM}.
</details>
<details>
<summary>摘要</summary>
水下图像 часто受到颜色扭曲和对比度低下，导致多种图像类型，由于水媒体对光的散射和吸收。而获得高质量的搅合训练样本是困难的。为解决这些挑战，我们设计了一种通用的水下图像提升方法，名为通用物理知识引导动态模型（简称GUPDM），包括三部分：大气dynamic结构（ADS）、传输导动结构（TDS）和多比例层结构（PMS）。特别是，为了涵盖复杂的水下场景，本研究通过形成模型来改变全球大气光和传输，以模拟不同的水下图像类型（如水下图像颜色从黄色到蓝色）。然后，我们设计了ADS和TDS模块，通过动态滤波器来自适应地提取水下图像中的优先信息，并生成PMS模块中的参数。这两个模块使得网络可以在不同的水类型上适应选择合适的参数。此外，PMS模块中的多尺度特征提取模块使用不同的核群大小的卷积块，通过通道注意块和权重融合来提高网络的感知范围。源代码将在 \href{https://github.com/shiningZZ/GUPDM}{https://github.com/shiningZZ/GUPDM} 上提供。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-light-Light-Field-Images-with-A-Deep-Compensation-Unfolding-Network"><a href="#Enhancing-Low-light-Light-Field-Images-with-A-Deep-Compensation-Unfolding-Network" class="headerlink" title="Enhancing Low-light Light Field Images with A Deep Compensation Unfolding Network"></a>Enhancing Low-light Light Field Images with A Deep Compensation Unfolding Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05404">http://arxiv.org/abs/2308.05404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lyuxianqiang/lfll-dcu">https://github.com/lyuxianqiang/lfll-dcu</a></li>
<li>paper_authors: Xianqiang Lyu, Junhui Hou</li>
<li>for: 这个论文是为了提高低光照下捕捉的光场图像（LF）的修复。</li>
<li>methods: 这个框架使用了多Stage的架构，模拟了解决反射图像问题的优化过程，并使用了中间提高后的结果来估计照明地图，以便生成新的提高后结果。此外，框架还包括每个优化阶段的内容相关深度补偿模块，以抑制噪声和照明地图估计错误。</li>
<li>results: 对于实际和模拟数据集，DCUNet在质量和量上都超过了现有方法，并且保留了修复后LF图像的主要几何结构。<details>
<summary>Abstract</summary>
This paper presents a novel and interpretable end-to-end learning framework, called the deep compensation unfolding network (DCUNet), for restoring light field (LF) images captured under low-light conditions. DCUNet is designed with a multi-stage architecture that mimics the optimization process of solving an inverse imaging problem in a data-driven fashion. The framework uses the intermediate enhanced result to estimate the illumination map, which is then employed in the unfolding process to produce a new enhanced result. Additionally, DCUNet includes a content-associated deep compensation module at each optimization stage to suppress noise and illumination map estimation errors. To properly mine and leverage the unique characteristics of LF images, this paper proposes a pseudo-explicit feature interaction module that comprehensively exploits redundant information in LF images. The experimental results on both simulated and real datasets demonstrate the superiority of our DCUNet over state-of-the-art methods, both qualitatively and quantitatively. Moreover, DCUNet preserves the essential geometric structure of enhanced LF images much better. The code will be publicly available at https://github.com/lyuxianqiang/LFLL-DCU.
</details>
<details>
<summary>摘要</summary>
（这篇论文提出了一种新的、可解释的端到端学习框架，叫做深度补偿解Network（DCUNet），用于在低光照条件下恢复光场图像。DCUNet采用多stage结构，模拟了解决反射光学问题的优化过程，并使用 intermediate enhancement result 来估算照明地图，然后employs 这个地图来 unfolding 过程中生成新的增强结果。此外，DCUNet还包括在每个优化阶段中的内容相关的深度补偿模块，以抑制噪声和照明地图估算错误。为了有效利用光场图像的特殊特征，这篇论文提出了一种 pseudo-explicit 特征互动模块，全面利用光场图像中的重复信息。实验结果表明，我们的 DCUNet 在真实和 simulate 数据集上比state-of-the-art 方法更高效， both qualitatively and quantitatively。此外，DCUNet 保留了增强光场图像的 essental geometric structure  much better。代码将在 https://github.com/lyuxianqiang/LFLL-DCU 上公开。）
</details></li>
</ul>
<hr>
<h2 id="TriDo-Former-A-Triple-Domain-Transformer-for-Direct-PET-Reconstruction-from-Low-Dose-Sinograms"><a href="#TriDo-Former-A-Triple-Domain-Transformer-for-Direct-PET-Reconstruction-from-Low-Dose-Sinograms" class="headerlink" title="TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms"></a>TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05365">http://arxiv.org/abs/2308.05365</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gluucose/TriDoFormer">https://github.com/gluucose/TriDoFormer</a></li>
<li>paper_authors: Jiaqi Cui, Pinxian Zeng, Xinyi Zeng, Peng Wang, Xi Wu, Jiliu Zhou, Yan Wang, Dinggang Shen</li>
<li>for: 这种论文主要目标是提高低剂量 positron emission tomography（PET）图像质量，同时尽量减少辐射暴露。</li>
<li>methods: 该论文提出了一种基于 transformer 模型的直接 PET 重建方法，称为 TriDo-Former。该模型包括两个缓冲网络：一个叫做 sinogram enhancement transformer（SE-Former）用于对 LPET 信号进行降噪，另一个叫做 spatial-spectral reconstruction transformer（SSR-Former）用于从降噪后的 LPET 信号中重建 SPET 图像。</li>
<li>results: 作者们的 TriDo-Former 方法在一个临床数据集上进行验证，证明了它在质量和量化上都高于现有的方法。<details>
<summary>Abstract</summary>
To obtain high-quality positron emission tomography (PET) images while minimizing radiation exposure, various methods have been proposed for reconstructing standard-dose PET (SPET) images from low-dose PET (LPET) sinograms directly. However, current methods often neglect boundaries during sinogram-to-image reconstruction, resulting in high-frequency distortion in the frequency domain and diminished or fuzzy edges in the reconstructed images. Furthermore, the convolutional architectures, which are commonly used, lack the ability to model long-range non-local interactions, potentially leading to inaccurate representations of global structures. To alleviate these problems, we propose a transformer-based model that unites triple domains of sinogram, image, and frequency for direct PET reconstruction, namely TriDo-Former. Specifically, the TriDo-Former consists of two cascaded networks, i.e., a sinogram enhancement transformer (SE-Former) for denoising the input LPET sinograms and a spatial-spectral reconstruction transformer (SSR-Former) for reconstructing SPET images from the denoised sinograms. Different from the vanilla transformer that splits an image into 2D patches, based specifically on the PET imaging mechanism, our SE-Former divides the sinogram into 1D projection view angles to maintain its inner-structure while denoising, preventing the noise in the sinogram from prorogating into the image domain. Moreover, to mitigate high-frequency distortion and improve reconstruction details, we integrate global frequency parsers (GFPs) into SSR-Former. The GFP serves as a learnable frequency filter that globally adjusts the frequency components in the frequency domain, enforcing the network to restore high-frequency details resembling real SPET images. Validations on a clinical dataset demonstrate that our TriDo-Former outperforms the state-of-the-art methods qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
通过直接从低剂量PET（LPET）信号图进行恢复，以提高高质量PET（SPET）图像的获得，已有多种方法提出。然而，现有方法通常忽略边界 durante el proceso de reconstrucción de imágenes, resulting in distorsión de alta frecuencia en el dominio de la frecuencia y borrosas o imágenes difusas en la reconstrucción. Además, las arquitecturas convolucionales, que se utilizan comúnmente, carecen de la capacidad de modelar interacciones de largo alcance no locales, lo que puede llevar a representaciones inexactas de estructuras globales. Para abordar estos problemas, propusimos un modelo basado en transformers que une los tres dominios de sinograma, imagen y frecuencia para la reconstrucción directa de PET, llamado TriDo-Former.Specifically, the TriDo-Former consists of two cascaded networks, i.e., a sinogram enhancement transformer (SE-Former) for denoising the input LPET sinograms and a spatial-spectral reconstruction transformer (SSR-Former) for reconstructing SPET images from the denoised sinograms. Unlike the vanilla transformer that splits an image into 2D patches based specifically on the PET imaging mechanism, our SE-Former divides the sinogram into 1D projection view angles to maintain its inner-structure while denoising, preventing the noise in the sinogram from propagating into the image domain. Moreover, to mitigate high-frequency distortion and improve reconstruction details, we integrate global frequency parsers (GFPs) into SSR-Former. The GFP serves as a learnable frequency filter that globally adjusts the frequency components in the frequency domain, enforcing the network to restore high-frequency details resembling real SPET images. Validations on a clinical dataset demonstrate that our TriDo-Former outperforms the state-of-the-art methods qualitatively and quantitatively.
</details></li>
</ul>
<hr>
<h2 id="Towards-General-and-Fast-Video-Derain-via-Knowledge-Distillation"><a href="#Towards-General-and-Fast-Video-Derain-via-Knowledge-Distillation" class="headerlink" title="Towards General and Fast Video Derain via Knowledge Distillation"></a>Towards General and Fast Video Derain via Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05346">http://arxiv.org/abs/2308.05346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Defang Cai, Pan Mu, Sixian Chan, Zhanpeng Shao, Cong Bai</li>
<li>for: 本研究旨在提出一种能够处理不同雨束类型的通用视频去雨网络（名为RRGNet），以提高视频去雨 task 的性能。</li>
<li>methods: 我们提出了一种框架分组基于的encoder-decoder网络，利用视频的时间信息，并使用老任务模型来引导当前模型学习新的雨束类型而不忘记原有的知识。</li>
<li>results: 我们的开发的通用方法在运行速度和去雨效果两个方面达到了最佳结果。<details>
<summary>Abstract</summary>
As a common natural weather condition, rain can obscure video frames and thus affect the performance of the visual system, so video derain receives a lot of attention. In natural environments, rain has a wide variety of streak types, which increases the difficulty of the rain removal task. In this paper, we propose a Rain Review-based General video derain Network via knowledge distillation (named RRGNet) that handles different rain streak types with one pre-training weight. Specifically, we design a frame grouping-based encoder-decoder network that makes full use of the temporal information of the video. Further, we use the old task model to guide the current model in learning new rain streak types while avoiding forgetting. To consolidate the network's ability to derain, we design a rain review module to play back data from old tasks for the current model. The experimental results show that our developed general method achieves the best results in terms of running speed and derain effect.
</details>
<details>
<summary>摘要</summary>
通常的自然天气情况下，雨水会掩蔽视频帧，从而影响视觉系统的性能，因此视频去雨受到了很多关注。在自然环境中，雨水有各种不同的斑斓类型，这使得去雨任务更加困难。在这篇论文中，我们提出了一种基于知识传授的通用视频去雨网络（名为RRGNet），能够处理不同的雨斑斓类型。 Specifically，我们设计了一个帧组合基于的Encoder-Decoder网络，以便充分利用视频的时间信息。另外，我们使用老任务模型来导引当前模型学习新的雨斑斓类型，而不是忘记原有知识。为了巩固网络的去雨能力，我们设计了雨评模块，以便将老任务数据播放给当前模型。实验结果显示，我们开发的通用方法在运行速度和去雨效果方面均达到了最佳效果。
</details></li>
</ul>
<hr>
<h2 id="Geometric-Learning-Based-Transformer-Network-for-Estimation-of-Segmentation-Errors"><a href="#Geometric-Learning-Based-Transformer-Network-for-Estimation-of-Segmentation-Errors" class="headerlink" title="Geometric Learning-Based Transformer Network for Estimation of Segmentation Errors"></a>Geometric Learning-Based Transformer Network for Estimation of Segmentation Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05068">http://arxiv.org/abs/2308.05068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sneha Sree C, Mohammad Al Fahim, Keerthi Ram, Mohanasankar Sivaprakasam</li>
<li>for: 提高和减少医生在图像分割中的努力，以便更快地完成图像分割任务。</li>
<li>methods: 使用图形神经网络（Graph Neural Network，GNN）和变换器（Nodeformer）来评估和分类错误的 segmentation 地图。</li>
<li>results: 与其他 GNN 相比，我们的网络实现了 ~0.042 的平均绝对错误和 79.53% 的准确率在评估和分类错误的节点级别错误。<details>
<summary>Abstract</summary>
Many segmentation networks have been proposed for 3D volumetric segmentation of tumors and organs at risk. Hospitals and clinical institutions seek to accelerate and minimize the efforts of specialists in image segmentation. Still, in case of errors generated by these networks, clinicians would have to manually edit the generated segmentation maps. Given a 3D volume and its putative segmentation map, we propose an approach to identify and measure erroneous regions in the segmentation map. Our method can estimate error at any point or node in a 3D mesh generated from a possibly erroneous volumetric segmentation map, serving as a Quality Assurance tool. We propose a graph neural network-based transformer based on the Nodeformer architecture to measure and classify the segmentation errors at any point. We have evaluated our network on a high-resolution micro-CT dataset of the human inner-ear bony labyrinth structure by simulating erroneous 3D segmentation maps. Our network incorporates a convolutional encoder to compute node-centric features from the input micro-CT data, the Nodeformer to learn the latent graph embeddings, and a Multi-Layer Perceptron (MLP) to compute and classify the node-wise errors. Our network achieves a mean absolute error of ~0.042 over other Graph Neural Networks (GNN) and an accuracy of 79.53% over other GNNs in estimating and classifying the node-wise errors, respectively. We also put forth vertex-normal prediction as a custom pretext task for pre-training the CNN encoder to improve the network's overall performance. Qualitative analysis shows the efficiency of our network in correctly classifying errors and reducing misclassifications.
</details>
<details>
<summary>摘要</summary>
多种 Segmentation 网络已经被提出用于三维 volume 的肿瘤和风险器官的分割。医院和临床机构希望通过加速和减少图像分割专家的努力来加速和改进图像分割过程。然而，在这些网络生成的分割地图中出现错误时，临床专家仍需手动修改生成的分割地图。为解决这个问题，我们提出了一种方法，可以在基于 Nodeformer 架构的图 neural network 中计算和评估分割错误的点位。我们的方法可以在基于高分辨率微型 CT 数据集的人类内耳骨征结构中进行评估和验证。我们的网络包括一个 convolutional encoder，用于从输入 micro-CT 数据中计算节点特征，以及一个 Nodeformer，用于学习 latent graph embeddings，以及一个 Multi-Layer Perceptron (MLP)，用于计算和分类节点错误。我们的网络在其他图 Neural Networks (GNN) 的比较中达到了 ~0.042 的平均绝对错误和 79.53% 的准确率，分别用于计算和分类节点错误。此外，我们还提出了预训练 CNN 编码器的预测任务，以提高网络的整体性能。Qualitative analysis 表明我们的网络可以正确地分类错误并减少错误分类。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/10/eess.IV_2023_08_10/" data-id="clly4xtg700evvl885fckfefq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/09/cs.LG_2023_08_09/" class="article-date">
  <time datetime="2023-08-08T16:00:00.000Z" itemprop="datePublished">2023-08-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/09/cs.LG_2023_08_09/">cs.LG - 2023-08-09 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Density-Crop-guided-Semi-supervised-Object-Detection-in-Aerial-Images"><a href="#Density-Crop-guided-Semi-supervised-Object-Detection-in-Aerial-Images" class="headerlink" title="Density Crop-guided Semi-supervised Object Detection in Aerial Images"></a>Density Crop-guided Semi-supervised Object Detection in Aerial Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05032">http://arxiv.org/abs/2308.05032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akhilpm/dronessod">https://github.com/akhilpm/dronessod</a></li>
<li>paper_authors: Akhil Meethal, Eric Granger, Marco Pedersoli</li>
<li>for: 这种论文的目的是提出一种针对零类卷积检测器的 semi-supervised 检测方法，以优化小对象的检测效果。</li>
<li>methods: 该方法使用 pseudo-labels 和弱强一致性学习，并在训练时使用批量分割法将图像分成不同的区域，从而提高小对象的检测效果。</li>
<li>results: 对 VisDrone 和 DOTA 等 benchmark 进行了实验，并证明了该方法可以在 COCO 风格的 AP 上提高检测精度超过 2%。Here’s the translation of the three points in English:</li>
<li>for: The purpose of this paper is to propose a semi-supervised detection method for object detectors, to improve the detection effectiveness of small objects.</li>
<li>methods: The method uses pseudo-labels and weak-strong consistency learning, and applies batch partitioning to the image during training, to improve the detection of small objects.</li>
<li>results: The paper conducts experiments on the VisDrone and DOTA benchmarks and demonstrates that the method can improve the detection accuracy by more than 2% in the COCO style AP.<details>
<summary>Abstract</summary>
One of the important bottlenecks in training modern object detectors is the need for labeled images where bounding box annotations have to be produced for each object present in the image. This bottleneck is further exacerbated in aerial images where the annotators have to label small objects often distributed in clusters on high-resolution images. In recent days, the mean-teacher approach trained with pseudo-labels and weak-strong augmentation consistency is gaining popularity for semi-supervised object detection. However, a direct adaptation of such semi-supervised detectors for aerial images where small clustered objects are often present, might not lead to optimal results. In this paper, we propose a density crop-guided semi-supervised detector that identifies the cluster of small objects during training and also exploits them to improve performance at inference. During training, image crops of clusters identified from labeled and unlabeled images are used to augment the training set, which in turn increases the chance of detecting small objects and creating good pseudo-labels for small objects on the unlabeled images. During inference, the detector is not only able to detect the objects of interest but also regions with a high density of small objects (density crops) so that detections from the input image and detections from image crops are combined, resulting in an overall more accurate object prediction, especially for small objects. Empirical studies on the popular benchmarks of VisDrone and DOTA datasets show the effectiveness of our density crop-guided semi-supervised detector with an average improvement of more than 2\% over the basic mean-teacher method in COCO style AP. Our code is available at: https://github.com/akhilpm/DroneSSOD.
</details>
<details>
<summary>摘要</summary>
一个重要的瓶颈在现代物体检测器的训练中是需要标注的图像，其中需要为每个图像中的物体生成矩形框注释。这个瓶颈在飞行图像中更加突出，因为标注者需要为高分辨率图像上的小对象进行标注。在最近的日子里，使用pseudo-标签和弱强同步增强的mean-teacher方法在无监督物体检测中得到了广泛的应用。然而，直接适应这些无监督检测器于飞行图像中的小对象集中存在可能不会导致最佳结果。在这篇论文中，我们提出了一种基于密度裁剪的半监督物体检测器，它在训练时使用图像裁剪来增强训练集，从而提高小对象的检测率和生成 pseudo-标签的质量。在推理时，检测器不仅可以检测输入图像中的对象，还可以检测密度裁剪中的小对象集，因此可以将输入图像和密度裁剪中的检测结果结合起来，从而实现更高的物体预测精度，特别是 для小对象。我们的实验结果表明，我们的密度裁剪半监督物体检测器在COCO样式的AP上超过2%的提升，相比基本的mean-teacher方法。我们的代码可以在GitHub上找到：https://github.com/akhilpm/DroneSSOD。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-on-Using-Large-Language-Models-to-Analyze-Software-Supply-Chain-Security-Failures"><a href="#An-Empirical-Study-on-Using-Large-Language-Models-to-Analyze-Software-Supply-Chain-Security-Failures" class="headerlink" title="An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures"></a>An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04898">http://arxiv.org/abs/2308.04898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanmay Singla, Dharun Anandayuvaraj, Kelechi G. Kalu, Taylor R. Schorlemmer, James C. Davis</li>
<li>For: The paper aims to assess the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches.* Methods: The authors used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). They developed prompts for LLMs to categorize the failures by four dimensions: type of compromise, intent, nature, and impact.* Results: The authors found that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. The accuracy of GPT 3.5 and Bard, two LLMs used in the study, was 68% and 58%, respectively.<details>
<summary>Abstract</summary>
As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5s categorizations had an average accuracy of 68% and Bard had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.
</details>
<details>
<summary>摘要</summary>
随着我们对软件系统的依赖度越来越高，软件供应链攻击的后果变得更加严重。高 Profile的网络攻击，如SolarrWinds和ShadowHammer，导致了重大的金融和数据损失，这 highlights 了加强网络安全的需要。一种预防未来攻击的方法是通过研究过去的失败来做。然而，传统的失败分析方法需要手动阅读和概括报告。自动支持可以降低成本和允许分析更多的失败。自然语言处理（NLP）技术，如大型语言模型（LLMs），可以帮助分析失败。在这项研究中，我们评估了LLMs在历史软件供应链安全失败分析中的能力。我们使用LLMs将69个软件供应链安全失败 manually analyzed 的报告中的四维度分类：类型的攻击、意图、性质和影响。GPT 3.5的分类精度为68%，而Bard的精度为58%。我们发现LLMs可以有效地 caracterize software supply chain failures，但是需要源文章具有足够的细节，以便由人工分析员达成共识。未来的工作可以提高LLM的性能在这种情况下，并研究更广泛的文章和失败。
</details></li>
</ul>
<hr>
<h2 id="Do-Diffusion-Models-Suffer-Error-Propagation-Theoretical-Analysis-and-Consistency-Regularization"><a href="#Do-Diffusion-Models-Suffer-Error-Propagation-Theoretical-Analysis-and-Consistency-Regularization" class="headerlink" title="Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization"></a>Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05021">http://arxiv.org/abs/2308.05021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangming Li, Zhaozhi Qian, Mihaela van der Schaar</li>
<li>for: 提高 diffusion models 的精度和稳定性，避免 error propagation 问题。</li>
<li>methods: empirical 和 theoretical 分析，以及一种 regularization 方案来解决 error propagation 问题。</li>
<li>results: 经验结果表明，regularization 方案可以有效地避免 error propagation，并提高 diffusion models 的表现。<details>
<summary>Abstract</summary>
While diffusion models have achieved promising performances in data synthesis, they might suffer error propagation because of their cascade structure, where the distributional mismatch spreads and magnifies through the chain of denoising modules. However, a strict analysis is expected since many sequential models such as Conditional Random Field (CRF) are free from error propagation. In this paper, we empirically and theoretically verify that diffusion models are indeed affected by error propagation and we then propose a regularization to address this problem. Our theoretical analysis reveals that the question can be reduced to whether every denoising module of the diffusion model is fault-tolerant. We derive insightful transition equations, indicating that the module can't recover from input errors and even propagates additional errors to the next module. Our analysis directly leads to a consistency regularization scheme for diffusion models, which explicitly reduces the distribution gap between forward and backward processes. We further introduce a bootstrapping algorithm to reduce the computation cost of the regularizer. Our experimental results on multiple image datasets show that our regularization effectively handles error propagation and significantly improves the performance of vanilla diffusion models.
</details>
<details>
<summary>摘要</summary>
Diffusion models 有 achieved promising performances in data synthesis, but they may suffer from error propagation due to their cascade structure, where the distributional mismatch spreads and magnifies through the chain of denoising modules. However, many sequential models such as Conditional Random Field (CRF) are free from error propagation. In this paper, we empirically and theoretically verify that diffusion models are indeed affected by error propagation, and we propose a regularization to address this problem.我们的 teoretic analysis reveals that the question can be reduced to whether every denoising module of the diffusion model is fault-tolerant. We derive insightful transition equations, indicating that the module cannot recover from input errors and even propagates additional errors to the next module. Our analysis directly leads to a consistency regularization scheme for diffusion models, which explicitly reduces the distribution gap between forward and backward processes.我们还引入了一种 bootstrapping algorithm to reduce the computation cost of the regularizer. Our experimental results on multiple image datasets show that our regularization effectively handles error propagation and significantly improves the performance of vanilla diffusion models.
</details></li>
</ul>
<hr>
<h2 id="When-and-How-Does-Known-Class-Help-Discover-Unknown-Ones-Provable-Understanding-Through-Spectral-Analysis"><a href="#When-and-How-Does-Known-Class-Help-Discover-Unknown-Ones-Provable-Understanding-Through-Spectral-Analysis" class="headerlink" title="When and How Does Known Class Help Discover Unknown Ones? Provable Understanding Through Spectral Analysis"></a>When and How Does Known Class Help Discover Unknown Ones? Provable Understanding Through Spectral Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05017">http://arxiv.org/abs/2308.05017</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deeplearning-wisc/nscl">https://github.com/deeplearning-wisc/nscl</a></li>
<li>paper_authors: Yiyou Sun, Zhenmei Shi, Yingyu Liang, Yixuan Li</li>
<li>for: 本研究的目的是提出一种基于知识的 Novel Class Discovery（NCD）方法，以便在无标签数据集中探索新的类别。</li>
<li>methods: 本研究使用了一种图学的表示方法，并引入了一种新的 Spectral Contrastive Loss（NSCL）函数来学习这种表示。NSCL 函数的目的是将图的邻接矩阵 factorized，从而得到一个可证明的错误约束和 Novel Class Discovery 的必要和 suficient condition。</li>
<li>results: 在实验中，NSCL 可以与多种强基elines相比赛，并且在常见的 benchmark 数据集上达到或超越这些基elines的性能，这是一种在实践中有理论保证的有appeal的方法。<details>
<summary>Abstract</summary>
Novel Class Discovery (NCD) aims at inferring novel classes in an unlabeled set by leveraging prior knowledge from a labeled set with known classes. Despite its importance, there is a lack of theoretical foundations for NCD. This paper bridges the gap by providing an analytical framework to formalize and investigate when and how known classes can help discover novel classes. Tailored to the NCD problem, we introduce a graph-theoretic representation that can be learned by a novel NCD Spectral Contrastive Loss (NSCL). Minimizing this objective is equivalent to factorizing the graph's adjacency matrix, which allows us to derive a provable error bound and provide the sufficient and necessary condition for NCD. Empirically, NSCL can match or outperform several strong baselines on common benchmark datasets, which is appealing for practical usage while enjoying theoretical guarantees.
</details>
<details>
<summary>摘要</summary>
《新类发现（NCD）》目的是从已知类集（labeled set）中推断未知类（novel classes）。尽管NCD的理论基础缺乏，但这篇论文弥补了这一空白，提供了一个分析框架来正式化和研究已知类如何帮助发现新类。为了适应NCD问题，我们引入了一种图论表示，可以通过我们提出的新的NCDspectral Contrastive Loss（NSCL）来学习。最小化这个目标等价于对图 adjacency matrix 的 факторизация，从而可以 derivate一个可证的错误 bound 和提供 suficient and necessary condition for NCD。在实验中，NSCL可以与多种强基elines匹配或超越，这是在实践中具有理论保证的appealing选择。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Bugs-in-Open-Source-Federated-Learning-Framework"><a href="#An-Empirical-Study-of-Bugs-in-Open-Source-Federated-Learning-Framework" class="headerlink" title="An Empirical Study of Bugs in Open-Source Federated Learning Framework"></a>An Empirical Study of Bugs in Open-Source Federated Learning Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05014">http://arxiv.org/abs/2308.05014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Shao, Yuyang Gao, Fu Song, Sen Chen, Lingling Fan</li>
<li>for: 本研究的目的是研究 Federated Learning（FL）框架中的安全问题，以便更好地保护用户的隐私数据。</li>
<li>methods: 本研究使用 GitHub 上的 12 个开源 FL 框架中的 bugs 进行实证研究，分类、标注并构建了15种症状、12种根 causa 和 20种修复模式的病例库。</li>
<li>results: 研究发现了1,112个 FL 框架 bugs，其中有15种症状、12种根 causa 和 20种修复模式。研究还发现了9个发现，并对其意义进行了讨论和建议。<details>
<summary>Abstract</summary>
Federated learning (FL), as a decentralized machine learning solution to the protection of users' private data, has become an important learning paradigm in recent years, especially since the enforcement of stricter laws and regulations in most countries. Therefore, a variety of FL frameworks are released to facilitate the development and application of federated learning. Despite the considerable amount of research on the security and privacy of FL models and systems, the security issues in FL frameworks have not been systematically studied yet. In this paper, we conduct the first empirical study on 1,112 FL framework bugs to investigate their characteristics. These bugs are manually collected, classified, and labeled from 12 open-source FL frameworks on GitHub. In detail, we construct taxonomies of 15 symptoms, 12 root causes, and 20 fix patterns of these bugs and investigate their correlations and distributions on 23 logical components and two main application scenarios. From the results of our study, we present nine findings, discuss their implications, and propound several suggestions to FL framework developers and security researchers on the FL frameworks.
</details>
<details>
<summary>摘要</summary>
随着国家的法规日益严格，联邦学习（FL）作为保护用户隐私数据的分布式机器学习解决方案，在最近几年内得到了重要的发展和应用。因此，许多FL框架被发布以便开发和应用联邦学习。尽管有很多关于FL模型和系统安全性的研究，但是FL框架的安全问题尚未得到了系统性的研究。在这篇论文中，我们进行了第一个实验性的研究， investigate FL框架中的1,112个漏洞的特点。这些漏洞由12个开源FL框架在GitHub上的手动收集、分类和标注。 Specifically, we construct taxonomies of 15 symptoms, 12 root causes, and 20 fix patterns of these bugs and investigate their correlations and distributions on 23 logical components and two main application scenarios.  From the results of our study, we present nine findings, discuss their implications, and propound several suggestions to FL framework developers and security researchers on the FL frameworks.
</details></li>
</ul>
<hr>
<h2 id="Multi-Class-Deep-SVDD-Anomaly-Detection-Approach-in-Astronomy-with-Distinct-Inlier-Categories"><a href="#Multi-Class-Deep-SVDD-Anomaly-Detection-Approach-in-Astronomy-with-Distinct-Inlier-Categories" class="headerlink" title="Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories"></a>Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05011">http://arxiv.org/abs/2308.05011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mperezcarrasco/AnomalyALeRCE">https://github.com/mperezcarrasco/AnomalyALeRCE</a></li>
<li>paper_authors: Manuel Pérez-Carrasco, Guillermo Cabrera-Vives, Lorena Hernández-García, Francisco Forster, Paula Sánchez-Sáez, Alejandra Muñoz Arancibia, Nicolás Astorga, Franz Bauer, Amelia Bayo, Martina Cádiz-Leyton, Marcio Catelan</li>
<li>for: 这篇论文旨在提出一种基于深度学习的多类异常检测算法（MCDSVDD），用于探析现代天文观测 telescope 产生的大量数据。</li>
<li>methods: 这篇论文使用了一个神经网络将数据映射到对应不同对立类别的偏心球体中，以计算每个样本的异常分数。</li>
<li>results: 比较这篇论文中的异常检测算法与其他几种异常检测算法，发现MCDSVDD能够有效地检测异常源，并且能够利用不同的对立类别。<details>
<summary>Abstract</summary>
With the increasing volume of astronomical data generated by modern survey telescopes, automated pipelines and machine learning techniques have become crucial for analyzing and extracting knowledge from these datasets. Anomaly detection, i.e. the task of identifying irregular or unexpected patterns in the data, is a complex challenge in astronomy. In this paper, we propose Multi-Class Deep Support Vector Data Description (MCDSVDD), an extension of the state-of-the-art anomaly detection algorithm One-Class Deep SVDD, specifically designed to handle different inlier categories with distinct data distributions. MCDSVDD uses a neural network to map the data into hyperspheres, where each hypersphere represents a specific inlier category. The distance of each sample from the centers of these hyperspheres determines the anomaly score. We evaluate the effectiveness of MCDSVDD by comparing its performance with several anomaly detection algorithms on a large dataset of astronomical light-curves obtained from the Zwicky Transient Facility. Our results demonstrate the efficacy of MCDSVDD in detecting anomalous sources while leveraging the presence of different inlier categories. The code and the data needed to reproduce our results are publicly available at https://github.com/mperezcarrasco/AnomalyALeRCE.
</details>
<details>
<summary>摘要</summary>
随着现代观测望远镜生成的天文数据量的增加，自动化管道和机器学习技术已成为分析和从数据中提取知识的关键。在天文学中，异常检测，即在数据中找到不寻常或意外的模式，是一项复杂的挑战。在这篇论文中，我们提出了多类深度支持向量数据描述（MCDSVDD），是一种基于现有异常检测算法一类深度支持向量数据描述（One-Class Deep SVDD）的扩展，特意为处理不同类准例的数据分布。MCDSVDD使用神经网络将数据映射到圆锥上，每个圆锥表示一个特定类准例。每个样本与这些圆锥的中心之间的距离决定了异常分数。我们通过对一个大量天文光谱数据集，从茨威基天体望远镜获得的数据进行比较，证明MCDSVDD可以有效地检测异常源，同时利用不同类准例的存在。代码和需要进行 reproduce 的数据可以在 <https://github.com/mperezcarrasco/AnomalyALeRCE> 上公开获取。
</details></li>
</ul>
<hr>
<h2 id="Transferable-Models-for-Bioacoustics-with-Human-Language-Supervision"><a href="#Transferable-Models-for-Bioacoustics-with-Human-Language-Supervision" class="headerlink" title="Transferable Models for Bioacoustics with Human Language Supervision"></a>Transferable Models for Bioacoustics with Human Language Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04978">http://arxiv.org/abs/2308.04978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/david-rx/biolingual">https://github.com/david-rx/biolingual</a></li>
<li>paper_authors: David Robinson, Adelaide Robinson, Lily Akrapongpisak</li>
<li>for: 跟踪全球生物多样性和人类活动对物种的影响</li>
<li>methods: 使用语音-文本预训练模型进行对比语言-声音表示的连接</li>
<li>results: 可以识别超过一千种动物叫声，完成零shot bioacoustic任务，并从自然文本查询中检索动物叫声记录Here’s the breakdown of each point:</li>
<li>for: The paper is written for tracking global biodiversity and anthropogenic impacts on species, using passive acoustic monitoring and deep learning techniques.</li>
<li>methods: The paper proposes a new model called BioLingual, which uses contrastive language-audio pretraining to connect language and audio representations and identify animal vocalizations.</li>
<li>results: The model can identify over a thousand species’ calls across taxa, complete bioacoustic tasks zero-shot, and retrieve animal vocalization recordings from natural text queries, setting a new state-of-the-art on nine tasks in the Benchmark of Animal Sounds.<details>
<summary>Abstract</summary>
Passive acoustic monitoring offers a scalable, non-invasive method for tracking global biodiversity and anthropogenic impacts on species. Although deep learning has become a vital tool for processing this data, current models are inflexible, typically cover only a handful of species, and are limited by data scarcity. In this work, we propose BioLingual, a new model for bioacoustics based on contrastive language-audio pretraining. We first aggregate bioacoustic archives into a language-audio dataset, called AnimalSpeak, with over a million audio-caption pairs holding information on species, vocalization context, and animal behavior. After training on this dataset to connect language and audio representations, our model can identify over a thousand species' calls across taxa, complete bioacoustic tasks zero-shot, and retrieve animal vocalization recordings from natural text queries. When fine-tuned, BioLingual sets a new state-of-the-art on nine tasks in the Benchmark of Animal Sounds. Given its broad taxa coverage and ability to be flexibly queried in human language, we believe this model opens new paradigms in ecological monitoring and research, including free-text search on the world's acoustic monitoring archives. We open-source our models, dataset, and code.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adversarial-ModSecurity-Countering-Adversarial-SQL-Injections-with-Robust-Machine-Learning"><a href="#Adversarial-ModSecurity-Countering-Adversarial-SQL-Injections-with-Robust-Machine-Learning" class="headerlink" title="Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning"></a>Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04964">http://arxiv.org/abs/2308.04964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biagio Montaruli, Luca Demetrio, Andrea Valenza, Luca Compagna, Davide Ariu, Luca Piras, Davide Balzarotti, Battista Biggio</li>
<li>for: 本研究旨在改善ModSecurity的攻击检测能力，特别是对SQL注入攻击的检测。</li>
<li>methods: 本研究使用机器学习模型，使用CRS规则作为输入特征，并通过训练来检测 adversarial SQLi 攻击。</li>
<li>results: 实验结果表明，AdvModSec 能够提高 ModSecurity 的检测率，同时降低假阳性率，相比标准版 ModSecurity 与 CRS 的检测率提高了21%。此外，AdvModSec 还能够提高对 adversarial SQLi 攻击的鲁棒性，提高了42%。<details>
<summary>Abstract</summary>
ModSecurity is widely recognized as the standard open-source Web Application Firewall (WAF), maintained by the OWASP Foundation. It detects malicious requests by matching them against the Core Rule Set, identifying well-known attack patterns. Each rule in the CRS is manually assigned a weight, based on the severity of the corresponding attack, and a request is detected as malicious if the sum of the weights of the firing rules exceeds a given threshold. In this work, we show that this simple strategy is largely ineffective for detecting SQL injection (SQLi) attacks, as it tends to block many legitimate requests, while also being vulnerable to adversarial SQLi attacks, i.e., attacks intentionally manipulated to evade detection. To overcome these issues, we design a robust machine learning model, named AdvModSec, which uses the CRS rules as input features, and it is trained to detect adversarial SQLi attacks. Our experiments show that AdvModSec, being trained on the traffic directed towards the protected web services, achieves a better trade-off between detection and false positive rates, improving the detection rate of the vanilla version of ModSecurity with CRS by 21%. Moreover, our approach is able to improve its adversarial robustness against adversarial SQLi attacks by 42%, thereby taking a step forward towards building more robust and trustworthy WAFs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CasCIFF-A-Cross-Domain-Information-Fusion-Framework-Tailored-for-Cascade-Prediction-in-Social-Networks"><a href="#CasCIFF-A-Cross-Domain-Information-Fusion-Framework-Tailored-for-Cascade-Prediction-in-Social-Networks" class="headerlink" title="CasCIFF: A Cross-Domain Information Fusion Framework Tailored for Cascade Prediction in Social Networks"></a>CasCIFF: A Cross-Domain Information Fusion Framework Tailored for Cascade Prediction in Social Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04961">http://arxiv.org/abs/2308.04961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaoyuan011/casciff">https://github.com/xiaoyuan011/casciff</a></li>
<li>paper_authors: Hongjun Zhu, Shun Yuan, Xin Liu, Kuo Chen, Chaolong Jia, Ying Qian</li>
<li>for: 本研究旨在预测信息潮流。</li>
<li>methods: 本研究使用深度学习方法，利用多个域的信息进行融合，以提高预测性能。</li>
<li>results: 研究人员通过实验发现，对于信息潮流预测，使用深度学习方法可以提高预测性能，同时可以更好地捕捉信息的传播趋势。<details>
<summary>Abstract</summary>
Existing approaches for information cascade prediction fall into three main categories: feature-driven methods, point process-based methods, and deep learning-based methods. Among them, deep learning-based methods, characterized by its superior learning and representation capabilities, mitigates the shortcomings inherent of the other methods. However, current deep learning methods still face several persistent challenges. In particular, accurate representation of user attributes remains problematic due to factors such as fake followers and complex network configurations. Previous algorithms that focus on the sequential order of user activations often neglect the rich insights offered by activation timing. Furthermore, these techniques often fail to holistically integrate temporal and structural aspects, thus missing the nuanced propagation trends inherent in information cascades.To address these issues, we propose the Cross-Domain Information Fusion Framework (CasCIFF), which is tailored for information cascade prediction. This framework exploits multi-hop neighborhood information to make user embeddings robust. When embedding cascades, the framework intentionally incorporates timestamps, endowing it with the ability to capture evolving patterns of information diffusion. In particular, the CasCIFF seamlessly integrates the tasks of user classification and cascade prediction into a consolidated framework, thereby allowing the extraction of common features that prove useful for all tasks, a strategy anchored in the principles of multi-task learning.
</details>
<details>
<summary>摘要</summary>
现有的信息潮流预测方法可以分为三大类：特征驱动方法、点过程基于方法和深度学习基于方法。其中，深度学习基于方法，具有出色的学习和表示能力，有效解决了其他方法的缺陷。然而，当前的深度学习方法仍面临许多挑战。特别是，准确地表示用户特征仍然是一个问题，因为因素如假账户和复杂的网络配置。之前的算法通常将用户活动的顺序序列化，而忽略了活动时间的细腻特征。此外，这些技术frequently neglect the rich insights offered by activation timing. To address these issues, we propose the Cross-Domain Information Fusion Framework (CasCIFF), which is tailored for information cascade prediction. This framework exploits multi-hop neighborhood information to make user embeddings robust. When embedding cascades, the framework intentionally incorporates timestamps, endowing it with the ability to capture evolving patterns of information diffusion. In particular, the CasCIFF seamlessly integrates the tasks of user classification and cascade prediction into a consolidated framework, thereby allowing the extraction of common features that prove useful for all tasks, a strategy anchored in the principles of multi-task learning.
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-for-Audio-Privacy-Preservation-using-Source-Separation-and-Robust-Adversarial-Learning"><a href="#Representation-Learning-for-Audio-Privacy-Preservation-using-Source-Separation-and-Robust-Adversarial-Learning" class="headerlink" title="Representation Learning for Audio Privacy Preservation using Source Separation and Robust Adversarial Learning"></a>Representation Learning for Audio Privacy Preservation using Source Separation and Robust Adversarial Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04960">http://arxiv.org/abs/2308.04960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diep Luong, Minh Tran, Shayan Gharib, Konstantinos Drossos, Tuomas Virtanen</li>
<li>for: 实现智能音频监控系统中的隐私保护，避免因为声音录音而泄露个人隐私。</li>
<li>methods: 提案结合源分离和对抗学习来保护隐私，将声音录音转换为隐私保护的潜在表示。</li>
<li>results: 与不含源分离、不含对抗学习和不含任何隐私保护的系统相比，提案的系统可以优化声音隐私保护，同时维持音频监控任务的好性能。<details>
<summary>Abstract</summary>
Privacy preservation has long been a concern in smart acoustic monitoring systems, where speech can be passively recorded along with a target signal in the system's operating environment. In this study, we propose the integration of two commonly used approaches in privacy preservation: source separation and adversarial representation learning. The proposed system learns the latent representation of audio recordings such that it prevents differentiating between speech and non-speech recordings. Initially, the source separation network filters out some of the privacy-sensitive data, and during the adversarial learning process, the system will learn privacy-preserving representation on the filtered signal. We demonstrate the effectiveness of our proposed method by comparing our method against systems without source separation, without adversarial learning, and without both. Overall, our results suggest that the proposed system can significantly improve speech privacy preservation compared to that of using source separation or adversarial learning solely while maintaining good performance in the acoustic monitoring task.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传感器系统中的隐私保护已经是长期的关注点，其中语音可以通过系统运行环境中的传感器记录。在这种情况下，我们提议将通用的两种隐私保护方法集成：源分离和对抗学习。我们的提议的系统学习了听录录音的幂等表示，以防止区分语音和非语音录音。首先，源分离网络会过滤一些隐私敏感数据，而在对抗学习过程中，系统会学习隐私保护的表示。我们对我们的提议方法进行比较，包括不使用源分离、不使用对抗学习和不使用两者。我们的结果表明，我们的提议方法可以在语音隐私保护方面具有显著改善，同时保持良好的传感器监测性能。Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore, whereas Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Improving-Autonomous-Separation-Assurance-through-Distributed-Reinforcement-Learning-with-Attention-Networks"><a href="#Improving-Autonomous-Separation-Assurance-through-Distributed-Reinforcement-Learning-with-Attention-Networks" class="headerlink" title="Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks"></a>Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04958">http://arxiv.org/abs/2308.04958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc W. Brittain, Luis E. Alvarez, Kara Breeden</li>
<li>for: 提供增量的自动化交通运输方案，使用无人驾驶和电动飞机，以提高前未服务市场的交通效率。</li>
<li>methods: 使用分布式强化学习框架，解决低空飞机在高密度环境中安全和有效地自动分配空间问题，并使用速度和垂直运动来实现自动分配。</li>
<li>results: 经过numerical研究表明，提出的方案可以在具有多种不确定性的环境中保证安全和高效的飞机分配，并且可以在高训练样本通过率和高并发计算架构中实现高效训练。<details>
<summary>Abstract</summary>
Advanced Air Mobility (AAM) introduces a new, efficient mode of transportation with the use of vehicle autonomy and electrified aircraft to provide increasingly autonomous transportation between previously underserved markets. Safe and efficient navigation of low altitude aircraft through highly dense environments requires the integration of a multitude of complex observations, such as surveillance, knowledge of vehicle dynamics, and weather. The processing and reasoning on these observations pose challenges due to the various sources of uncertainty in the information while ensuring cooperation with a variable number of aircraft in the airspace. These challenges coupled with the requirement to make safety-critical decisions in real-time rule out the use of conventional separation assurance techniques. We present a decentralized reinforcement learning framework to provide autonomous self-separation capabilities within AAM corridors with the use of speed and vertical maneuvers. The problem is formulated as a Markov Decision Process and solved by developing a novel extension to the sample-efficient, off-policy soft actor-critic (SAC) algorithm. We introduce the use of attention networks for variable-length observation processing and a distributed computing architecture to achieve high training sample throughput as compared to existing approaches. A comprehensive numerical study shows that the proposed framework can ensure safe and efficient separation of aircraft in high density, dynamic environments with various sources of uncertainty.
</details>
<details>
<summary>摘要</summary>
高级空中移动（AAM）介DUenced a new, efficient mode of transportation by using vehicle autonomy and electrified aircraft to provide increasingly autonomous transportation between previously underserved markets. Safe and efficient navigation of low altitude aircraft through highly dense environments requires the integration of a multitude of complex observations, such as surveillance, knowledge of vehicle dynamics, and weather. The processing and reasoning on these observations pose challenges due to the various sources of uncertainty in the information while ensuring cooperation with a variable number of aircraft in the airspace. These challenges coupled with the requirement to make safety-critical decisions in real-time rule out the use of conventional separation assurance techniques. We present a decentralized reinforcement learning framework to provide autonomous self-separation capabilities within AAM corridors with the use of speed and vertical maneuvers. The problem is formulated as a Markov Decision Process and solved by developing a novel extension to the sample-efficient, off-policy soft actor-critic (SAC) algorithm. We introduce the use of attention networks for variable-length observation processing and a distributed computing architecture to achieve high training sample throughput as compared to existing approaches. A comprehensive numerical study shows that the proposed framework can ensure safe and efficient separation of aircraft in high density, dynamic environments with various sources of uncertainty.
</details></li>
</ul>
<hr>
<h2 id="Variations-on-the-Reinforcement-Learning-performance-of-Blackjack"><a href="#Variations-on-the-Reinforcement-Learning-performance-of-Blackjack" class="headerlink" title="Variations on the Reinforcement Learning performance of Blackjack"></a>Variations on the Reinforcement Learning performance of Blackjack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07329">http://arxiv.org/abs/2308.07329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/avishburamdoyal/the-impact-of-deck-size-q-learning-blackjack">https://github.com/avishburamdoyal/the-impact-of-deck-size-q-learning-blackjack</a></li>
<li>paper_authors: Avish Buramdoyal, Tim Gebbie</li>
<li>for: 这篇论文是关于黑板子游戏（Blackjack）的优化策略和学习Agent的研究。</li>
<li>methods: 这篇论文使用q-学习算法来解决黑板子游戏的优化问题，并研究算法的学习速度是否与牌纸大小有关。</li>
<li>results: 研究发现，在黑板子游戏中，一个使用基础策略和高低系统的卡 COUNT可以让家家破产，而牌纸大小的变化会对这个结果产生影响。<details>
<summary>Abstract</summary>
Blackjack or "21" is a popular card-based game of chance and skill. The objective of the game is to win by obtaining a hand total higher than the dealer's without exceeding 21. The ideal blackjack strategy will maximize financial return in the long run while avoiding gambler's ruin. The stochastic environment and inherent reward structure of blackjack presents an appealing problem to better understand reinforcement learning agents in the presence of environment variations. Here we consider a q-learning solution for optimal play and investigate the rate of learning convergence of the algorithm as a function of deck size. A blackjack simulator allowing for universal blackjack rules is also implemented to demonstrate the extent to which a card counter perfectly using the basic strategy and hi-lo system can bring the house to bankruptcy and how environment variations impact this outcome. The novelty of our work is to place this conceptual understanding of the impact of deck size in the context of learning agent convergence.
</details>
<details>
<summary>摘要</summary>
黑板子或"21"是一款受欢迎的 карто牌游戏，旨在赢得手牌总值高于供应商的手牌而不超过21。理想的黑板子策略可以在长期内 maximize 财务回报，同时避免投资者的破产。黑板子的随机环境和内在的奖励结构，使得这个问题成为了理解增强学习代理在环境变化下的问题。我们考虑了一种q-学习解决方案，以便实现最佳的游戏策略，并 investigate 学习过程的速度是否与扑克牌大小相关。我们还实现了一个可以实现通用黑板子规则的黑板子模拟器，以示出一个卡计数员使用基本策略和高低系统可以让家庭铺垮，以及环境变化对这个结果的影响。我们的研究的新特点在于将这种概念理解与学习代理快速learns的速度相结合。
</details></li>
</ul>
<hr>
<h2 id="Performance-Analysis-of-Transformer-Based-Models-BERT-ALBERT-and-RoBERTa-in-Fake-News-Detection"><a href="#Performance-Analysis-of-Transformer-Based-Models-BERT-ALBERT-and-RoBERTa-in-Fake-News-Detection" class="headerlink" title="Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection"></a>Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04950">http://arxiv.org/abs/2308.04950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shafna81/fakenewsdetection">https://github.com/shafna81/fakenewsdetection</a></li>
<li>paper_authors: Shafna Fitria Nur Azizah, Hasan Dwi Cahyono, Sari Widya Sihwi, Wisnu Widiarto</li>
<li>For: The paper is written to detect fake news in Bahasa Indonesia using transformer models, specifically ALBERT, to improve the accuracy of fake news detection.* Methods: The paper uses transformer models, specifically BERT, ALBERT, and RoBERTa, to process text in parallel and produce rich and contextual word representations. The authors explore the use of these models for detecting fake news in Bahasa Indonesia and compare their performance.* Results: The paper finds that ALBERT outperforms other models with 87.6% accuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s&#x2F;epoch) respectively.Here is the simplified Chinese text for the three key information points:* For: 这篇论文是用来探讨bahasa Indonesia中的假新闻检测，使用转换器模型，具体来说是ALBERT，以提高假新闻检测的准确性。* Methods: 论文使用转换器模型，具体来说是BERT、ALBERT和RoBERTa，来处理文本并生成丰富的上下文描述。作者们在bahasa Indonesia中检测假新闻的过程中使用这些模型，并比较其性能。* Results: 论文发现，ALBERT的性能比其他模型更高，具体来说是87.6%的准确率、86.9%的精度、86.9%的F1分数和174.5个run-time（s&#x2F;epoch）。<details>
<summary>Abstract</summary>
Fake news is fake material in a news media format but is not processed properly by news agencies. The fake material can provoke or defame significant entities or individuals or potentially even for the personal interests of the creators, causing problems for society. Distinguishing fake news and real news is challenging due to limited of domain knowledge and time constraints. According to the survey, the top three areas most exposed to hoaxes and misinformation by residents are in Banten, DKI Jakarta and West Java. The model of transformers is referring to an approach in the field of artificial intelligence (AI) in natural language processing utilizing the deep learning architectures. Transformers exercise a powerful attention mechanism to process text in parallel and produce rich and contextual word representations. A previous study indicates a superior performance of a transformer model known as BERT over and above non transformer approach. However, some studies suggest the performance can be improved with the use of improved BERT models known as ALBERT and RoBERTa. However, the modified BERT models are not well explored for detecting fake news in Bahasa Indonesia. In this research, we explore those transformer models and found that ALBERT outperformed other models with 87.6% accuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s/epoch) respectively. Source code available at: https://github.com/Shafna81/fakenewsdetection.git
</details>
<details>
<summary>摘要</summary>
假新闻是不正确的新闻材料，但是没有经过新闻机构的处理和检查。这种假材料可能会诋毁或者诋毁重要的实体或个人，或者为创作者的个人利益而创造。分辨假新闻和真实新闻是非常困难的，因为有限的领域知识和时间限制。据调查，居民最常受到诈骗和谣言的三个地区分别是望加、特区雅加达和西爪哇。 transformers 是一种人工智能（AI）自然语言处理领域的方法，利用深度学习架构。 transformers 使用强大的注意力机制，并在平行处理文本，生成丰富和Contextual 的单词表示。一些研究表明，改进的 BERT 模型（如 ALBERT 和 RoBERTa）可以超过非 transformer 方法的性能。然而，这些改进 BERT 模型在假新闻检测中并没有得到广泛的探索。在这项研究中，我们探索了这些 transformer 模型，并发现 ALBERT 模型在假新闻检测中的表现最佳，具体来说，ALBERT 模型在87.6%的准确率、86.9%的精度、86.9%的 F1 分数和174.5（s/epoch）的运行时间上表现出色。源代码可以在 GitHub 上找到：https://github.com/Shafna81/fakenewsdetection.git。
</details></li>
</ul>
<hr>
<h2 id="Methods-for-Acquiring-and-Incorporating-Knowledge-into-Stock-Price-Prediction-A-Survey"><a href="#Methods-for-Acquiring-and-Incorporating-Knowledge-into-Stock-Price-Prediction-A-Survey" class="headerlink" title="Methods for Acquiring and Incorporating Knowledge into Stock Price Prediction: A Survey"></a>Methods for Acquiring and Incorporating Knowledge into Stock Price Prediction: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04947">http://arxiv.org/abs/2308.04947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liping Wang, Jiawei Li, Lifan Zhao, Zhizhuo Kou, Xiaohan Wang, Xinyi Zhu, Hao Wang, Yanyan Shen, Lei Chen</li>
<li>For: This paper aims to provide a systematic and comprehensive overview of methods for acquiring external knowledge from various unstructured data sources and incorporating it into stock price prediction models.* Methods: The paper explores fusion methods for combining external knowledge with historical price features, and discusses relevant datasets for stock price prediction.* Results: The paper compiles and synthesizes previous studies on knowledge-enhanced stock price prediction methods, providing a comprehensive understanding of the different types of external knowledge that can be used to improve stock price prediction.<details>
<summary>Abstract</summary>
Predicting stock prices presents a challenging research problem due to the inherent volatility and non-linear nature of the stock market. In recent years, knowledge-enhanced stock price prediction methods have shown groundbreaking results by utilizing external knowledge to understand the stock market. Despite the importance of these methods, there is a scarcity of scholarly works that systematically synthesize previous studies from the perspective of external knowledge types. Specifically, the external knowledge can be modeled in different data structures, which we group into non-graph-based formats and graph-based formats: 1) non-graph-based knowledge captures contextual information and multimedia descriptions specifically associated with an individual stock; 2) graph-based knowledge captures interconnected and interdependent information in the stock market. This survey paper aims to provide a systematic and comprehensive description of methods for acquiring external knowledge from various unstructured data sources and then incorporating it into stock price prediction models. We also explore fusion methods for combining external knowledge with historical price features. Moreover, this paper includes a compilation of relevant datasets and delves into potential future research directions in this domain.
</details>
<details>
<summary>摘要</summary>
预测股票价格是一个复杂的研究问题，因为股市的自然波动性和非线性性。在最近的几年中，基于知识的股票价格预测方法有着重要的突破，这些方法利用外部知识来理解股市。尽管这些方法的重要性，但是学术研究中有少量的评估前期研究的评价。特别是，外部知识可以被模型为不同的数据结构，我们分为非图结构和图结构两类：1）非图结构知识捕捉具体股票的上下文信息和多媒体描述; 2）图结构知识捕捉股市中的相互连接和依赖关系。本文旨在提供一种系统和完整的描述，包括从不同的不结构数据源中获取外部知识，然后将其与历史价格特征结合。此外，本文还探讨了外部知识与历史价格特征的融合方法，并提供了相关的数据集和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Graph-Neural-Network-with-Importance-Grained-Noise-Adaption"><a href="#Differentially-Private-Graph-Neural-Network-with-Importance-Grained-Noise-Adaption" class="headerlink" title="Differentially Private Graph Neural Network with Importance-Grained Noise Adaption"></a>Differentially Private Graph Neural Network with Importance-Grained Noise Adaption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04943">http://arxiv.org/abs/2308.04943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Qi, Xi Lin, Jun Wu</li>
<li>for: 保护图形数据的隐私，特别是节点具有个人敏感信息时。</li>
<li>methods: 提议一种基于不同节点重要性的隐私保护方法，包括节点重要性估计、隐私保护的集成和多层卷积学习。</li>
<li>results: 经验表明，NAP-GNN可以在保护隐私的同时实现更好的准确率。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) with differential privacy have been proposed to preserve graph privacy when nodes represent personal and sensitive information. However, the existing methods ignore that nodes with different importance may yield diverse privacy demands, which may lead to over-protect some nodes and decrease model utility. In this paper, we study the problem of importance-grained privacy, where nodes contain personal data that need to be kept private but are critical for training a GNN. We propose NAP-GNN, a node-importance-grained privacy-preserving GNN algorithm with privacy guarantees based on adaptive differential privacy to safeguard node information. First, we propose a Topology-based Node Importance Estimation (TNIE) method to infer unknown node importance with neighborhood and centrality awareness. Second, an adaptive private aggregation method is proposed to perturb neighborhood aggregation from node-importance-grain. Third, we propose to privately train a graph learning algorithm on perturbed aggregations in adaptive residual connection mode over multi-layers convolution for node-wise tasks. Theoretically analysis shows that NAP-GNN satisfies privacy guarantees. Empirical experiments over real-world graph datasets show that NAP-GNN achieves a better trade-off between privacy and accuracy.
</details>
<details>
<summary>摘要</summary>
GRAPH NEURAL NETWORKS (GNNs) with differential privacy have been proposed to protect graph privacy when nodes represent personal and sensitive information. However, existing methods ignore that nodes with different importance may have different privacy demands, which may lead to over-protect some nodes and decrease model utility. In this paper, we study the problem of importance-grained privacy, where nodes contain personal data that need to be kept private but are critical for training a GNN. We propose NAP-GNN, a node-importance-grained privacy-preserving GNN algorithm with privacy guarantees based on adaptive differential privacy to safeguard node information. First, we propose a Topology-based Node Importance Estimation (TNIE) method to infer unknown node importance with neighborhood and centrality awareness. Second, an adaptive private aggregation method is proposed to perturb neighborhood aggregation from node-importance-grain. Third, we propose to privately train a graph learning algorithm on perturbed aggregations in adaptive residual connection mode over multi-layers convolution for node-wise tasks. Theoretically analysis shows that NAP-GNN satisfies privacy guarantees. Empirical experiments over real-world graph datasets show that NAP-GNN achieves a better trade-off between privacy and accuracy.Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. The traditional Chinese form of the translation would be slightly different.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Effect-of-Data-Impurity-on-the-Detection-Performances-of-Mental-Disorders"><a href="#Analyzing-the-Effect-of-Data-Impurity-on-the-Detection-Performances-of-Mental-Disorders" class="headerlink" title="Analyzing the Effect of Data Impurity on the Detection Performances of Mental Disorders"></a>Analyzing the Effect of Data Impurity on the Detection Performances of Mental Disorders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05133">http://arxiv.org/abs/2308.05133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Kumar Gupta, Rohit Sinha</li>
<li>for: 本研究旨在提高Automatic Recognition of Mental Disorders的精度，尤其是对于主要抑郁症和 POST 应急压力症的识别。</li>
<li>methods: 本研究使用了一种新的方法，即从各种精神疾病中分离出特定疾病的特征，以提高分类器的准确率。</li>
<li>results: 研究结果表明，通过去除各种精神疾病的数据杂化，可以提高主要抑郁症和 POST 应急压力症的识别率。<details>
<summary>Abstract</summary>
The primary method for identifying mental disorders automatically has traditionally involved using binary classifiers. These classifiers are trained using behavioral data obtained from an interview setup. In this training process, data from individuals with the specific disorder under consideration are categorized as the positive class, while data from all other participants constitute the negative class. In practice, it is widely recognized that certain mental disorders share similar symptoms, causing the collected behavioral data to encompass a variety of attributes associated with multiple disorders. Consequently, attributes linked to the targeted mental disorder might also be present within the negative class. This data impurity may lead to sub-optimal training of the classifier for a mental disorder of interest. In this study, we investigate this hypothesis in the context of major depressive disorder (MDD) and post-traumatic stress disorder detection (PTSD). The results show that upon removal of such data impurity, MDD and PTSD detection performances are significantly improved.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-In-Depth-Analysis-of-Discretization-Methods-for-Communication-Learning-using-Backpropagation-with-Multi-Agent-Reinforcement-Learning"><a href="#An-In-Depth-Analysis-of-Discretization-Methods-for-Communication-Learning-using-Backpropagation-with-Multi-Agent-Reinforcement-Learning" class="headerlink" title="An In-Depth Analysis of Discretization Methods for Communication Learning using Backpropagation with Multi-Agent Reinforcement Learning"></a>An In-Depth Analysis of Discretization Methods for Communication Learning using Backpropagation with Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04938">http://arxiv.org/abs/2308.04938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Astrid Vanneste, Simon Vanneste, Kevin Mets, Tom De Schepper, Siegfried Mercelis, Peter Hellinckx</li>
<li>for: 本研究的目的是比较不同批处理方法在多智能演变学习中的表现，以及提出一种新的方法。</li>
<li>methods: 本研究使用了多种常见的批处理方法，包括DIAL、COMA和ST-DRU等。</li>
<li>results: 本研究发现，ST-DRU方法在多种环境中表现最佳，在所有测试环境中都达到了最好或接近最好的表现，而其他方法在某些环境中失败。<details>
<summary>Abstract</summary>
Communication is crucial in multi-agent reinforcement learning when agents are not able to observe the full state of the environment. The most common approach to allow learned communication between agents is the use of a differentiable communication channel that allows gradients to flow between agents as a form of feedback. However, this is challenging when we want to use discrete messages to reduce the message size, since gradients cannot flow through a discrete communication channel. Previous work proposed methods to deal with this problem. However, these methods are tested in different communication learning architectures and environments, making it hard to compare them. In this paper, we compare several state-of-the-art discretization methods as well as a novel approach. We do this comparison in the context of communication learning using gradients from other agents and perform tests on several environments. In addition, we present COMA-DIAL, a communication learning approach based on DIAL and COMA extended with learning rate scaling and adapted exploration. Using COMA-DIAL allows us to perform experiments on more complex environments. Our results show that the novel ST-DRU method, proposed in this paper, achieves the best results out of all discretization methods across the different environments. It achieves the best or close to the best performance in each of the experiments and is the only method that does not fail on any of the tested environments.
</details>
<details>
<summary>摘要</summary>
通信是多智能体学习中不可或缺的一部分，尤其当智能体无法观察环境的全部状态时。通常，使得学习得到的通信途径是使用可导的通信途径，allowing gradients to flow between agents as feedback。然而，当使用精简的消息时，这会变得困难，因为梯度不能流通精简的通信途径。前作已经提出了解决这个问题的方法，但这些方法在不同的通信学习架构和环境中进行测试，使其比较困难。在这篇论文中，我们比较了several state-of-the-art 精简方法以及一种新的方法。我们在通信学习中使用 gradients from other agents 进行测试，并在多个环境中进行测试。此外，我们还提出了 COMA-DIAL，一种基于 DIAL 和 COMA 的通信学习方法，其中包括学习速率缩放和适应性探索。使用 COMA-DIAL 让我们能够在更复杂的环境中进行实验。我们的结果显示，本文提出的新方法 ST-DRU 在不同环境中的表现最佳，它在每个实验中达到了最佳或 close to the best 性能，并且是唯一一个不会在任何测试环境中失败的方法。
</details></li>
</ul>
<hr>
<h2 id="JEDI-Joint-Expert-Distillation-in-a-Semi-Supervised-Multi-Dataset-Student-Teacher-Scenario-for-Video-Action-Recognition"><a href="#JEDI-Joint-Expert-Distillation-in-a-Semi-Supervised-Multi-Dataset-Student-Teacher-Scenario-for-Video-Action-Recognition" class="headerlink" title="JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition"></a>JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04934">http://arxiv.org/abs/2308.04934</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucian Bicsi, Bogdan Alexe, Radu Tudor Ionescu, Marius Leordeanu</li>
<li>for: 这 paper 是为了提高机器学习模型的总体性和效果，使其能够在不同的数据集上进行推理和预测。</li>
<li>methods: 这 paper 使用了多个数据集的 semi-supervised learning 方法，通过将多个专家模型（每个专家模型在其自己的数据集上进行预训练）合并，以提高每个数据集上的学生模型的性能。</li>
<li>results: 实验结果表明，使用这种方法可以在四个视频动作识别数据集上显著提高模型的性能，并且同时考虑所有数据集，即使不具备很多标注数据，也可以获得良好的总体性能。<details>
<summary>Abstract</summary>
We propose JEDI, a multi-dataset semi-supervised learning method, which efficiently combines knowledge from multiple experts, learned on different datasets, to train and improve the performance of individual, per dataset, student models. Our approach achieves this by addressing two important problems in current machine learning research: generalization across datasets and limitations of supervised training due to scarcity of labeled data. We start with an arbitrary number of experts, pretrained on their own specific dataset, which form the initial set of student models. The teachers are immediately derived by concatenating the feature representations from the penultimate layers of the students. We then train all models in a student-teacher semi-supervised learning scenario until convergence. In our efficient approach, student-teacher training is carried out jointly and end-to-end, showing that both students and teachers improve their generalization capacity during training. We validate our approach on four video action recognition datasets. By simultaneously considering all datasets within a unified semi-supervised setting, we demonstrate significant improvements over the initial experts.
</details>
<details>
<summary>摘要</summary>
我们提议JEDI方法，这是一种多集数据半supervised学习方法，可以有效地将多个专家知识相结合，以提高每个特定dataset的学生模型的性能。我们的方法解决了当前机器学习研究中两个重要问题：跨集数据泛化和监督学习因数据稀缺而受限。我们从arbitrary数量的专家开始，先将专家模型预训练在自己专门的dataset上，这些专家模型组成初始的学生模型集。然后，我们将所有模型在学生-教师半supervised学习enario中同时训练，直到收敛。在我们的效率的方法中，学生-教师训练是 joint和端到端的，表明在训练过程中，学生和教师都会提高其泛化能力。我们验证了我们的方法在四个视频动作识别dataset上，并证明了同时考虑所有dataset在一个统一的半supervised设定下，可以获得显著提高。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Prediction-of-Fractional-Flow-Reserve-along-the-Coronary-Artery"><a href="#Deep-Learning-Based-Prediction-of-Fractional-Flow-Reserve-along-the-Coronary-Artery" class="headerlink" title="Deep Learning-Based Prediction of Fractional Flow Reserve along the Coronary Artery"></a>Deep Learning-Based Prediction of Fractional Flow Reserve along the Coronary Artery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04923">http://arxiv.org/abs/2308.04923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nils Hampe, Sanne G. M. van Velzen, Jean-Paul Aben, Carlos Collet, Ivana Išgum<br>for:The paper is written to propose a deep learning-based method for predicting the fractional flow reserve (FFR) along the coronary artery from coronary computed tomography angiography (CCTA) scans.methods:The proposed method uses a variational autoencoder to characterize the artery and a convolutional neural network (CNN) to predict the FFR along the artery. The CNN is supervised by multiple loss functions, including a loss function inspired by the Earth Mover’s Distance (EMD) to predict the correct location of FFR drops and a histogram-based loss to explicitly supervise the slope of the FFR curve.results:The resulting FFR curves show good agreement with the reference, allowing the distinction between diffuse and focal coronary artery disease (CAD) distributions in most cases. The mean absolute difference in the area under the FFR pullback curve (AUPC) is 1.7. The proposed method may pave the way towards fast, accurate, and automatic prediction of FFR along the artery from CCTA.<details>
<summary>Abstract</summary>
Functionally significant coronary artery disease (CAD) is caused by plaque buildup in the coronary arteries, potentially leading to narrowing of the arterial lumen, i.e. coronary stenosis, that significantly obstructs blood flow to the myocardium. The current reference for establishing the presence of a functionally significant stenosis is invasive fractional flow reserve (FFR) measurement. To avoid invasive measurements, non-invasive prediction of FFR from coronary CT angiography (CCTA) has emerged. For this, machine learning approaches, characterized by fast inference, are increasingly developed. However, these methods predict a single FFR value per artery i.e. they don't provide information about the stenosis location or treatment strategy. We propose a deep learning-based method to predict the FFR along the artery from CCTA scans. This study includes CCTA images of 110 patients who underwent invasive FFR pullback measurement in 112 arteries. First, a multi planar reconstruction (MPR) of the artery is fed to a variational autoencoder to characterize the artery, i.e. through the lumen area and unsupervised artery encodings. Thereafter, a convolutional neural network (CNN) predicts the FFR along the artery. The CNN is supervised by multiple loss functions, notably a loss function inspired by the Earth Mover's Distance (EMD) to predict the correct location of FFR drops and a histogram-based loss to explicitly supervise the slope of the FFR curve. To train and evaluate our model, eight-fold cross-validation was performed. The resulting FFR curves show good agreement with the reference allowing the distinction between diffuse and focal CAD distributions in most cases. Quantitative evaluation yielded a mean absolute difference in the area under the FFR pullback curve (AUPC) of 1.7. The method may pave the way towards fast, accurate, automatic prediction of FFR along the artery from CCTA.
</details>
<details>
<summary>摘要</summary>
fonctionally significant coronary artery disease (CAD) 是由 coronary arteries 中的粘滓堆积引起的，可能导致 coronary arteries 的狭窄，即 coronary stenosis，significantly obstructs blood flow to the myocardium。现有的参照标准是侵入性的 fractional flow reserve (FFR) 测量。为了避免侵入性测量，Non-invasive prediction of FFR from coronary CT angiography (CCTA) 已经出现。这些方法具有快速的推理，但它们只预测每条 artery 的 FFR 值，没有提供狭窄的位置或治疗策略信息。我们提出了一种基于深度学习的方法，可以从 CCTA 图像中预测 FFR。这个研究包括 CCTA 图像的 110 名患者，其中每名患者有 112 条 artery 的侵入性 FFR pullback 测量。首先，一个 multi planar reconstruction (MPR) 的 artery 图像被 feed 到一个 variational autoencoder 中，以 caracterize the artery，即通过 luminal area 和不supervised artery encodings。然后，一个 convolutional neural network (CNN) 预测了 artery 中的 FFR。CNN 被多个损失函数supervise，包括一个由 Earth Mover's Distance (EMD)  inspirited loss function，以正确地预测狭窄的 FFR drop 位置，以及一个 histogram-based loss，以Explicitly supervise the slope of the FFR curve。为了训练和评估我们的模型，八个十字交叉验证被进行。得到的 FFR 曲线显示了良好的一致性，allowing the distinction between diffuse and focal CAD distributions in most cases。量化评估表明了 mean absolute difference 在 area under the FFR pullback curve (AUPC) 的值为 1.7。这种方法可能会为 CCTA 图像中的 FFR 预测提供快速、准确、自动的方法。
</details></li>
</ul>
<hr>
<h2 id="GraphCC-A-Practical-Graph-Learning-based-Approach-to-Congestion-Control-in-Datacenters"><a href="#GraphCC-A-Practical-Graph-Learning-based-Approach-to-Congestion-Control-in-Datacenters" class="headerlink" title="GraphCC: A Practical Graph Learning-based Approach to Congestion Control in Datacenters"></a>GraphCC: A Practical Graph Learning-based Approach to Congestion Control in Datacenters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04905">http://arxiv.org/abs/2308.04905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Bernárdez, José Suárez-Varela, Xiang Shi, Shihan Xiao, Xiangle Cheng, Pere Barlet-Ros, Albert Cabellos-Aparicio<br>for: 本文主要针对数据中心网络（DCN）中的堵塞控制（CC）问题，即如何在DCN中优化流量的分配和管理。methods: 本文提出了一种基于机器学习的分布式框架， named GraphCC， which combines Multi-agent Reinforcement Learning（MARL）和图神经网络（GNN）来自适应性地调整DCN中的Explicit Congestion Notification（ECN）配置。results: 在评估中，GraphCC在不同的场景下表现出优于现有的基eline，包括ACC。 GraphCC可以在不同的流量占用和网络状况下提供更好的流程完成时间和缓存占用率。 specifically, GraphCC可以在新的场景下提供$20%$的改进，并且在不同的失败和升级情况下保持稳定性。<details>
<summary>Abstract</summary>
Congestion Control (CC) plays a fundamental role in optimizing traffic in Data Center Networks (DCN). Currently, DCNs mainly implement two main CC protocols: DCTCP and DCQCN. Both protocols -- and their main variants -- are based on Explicit Congestion Notification (ECN), where intermediate switches mark packets when they detect congestion. The ECN configuration is thus a crucial aspect on the performance of CC protocols. Nowadays, network experts set static ECN parameters carefully selected to optimize the average network performance. However, today's high-speed DCNs experience quick and abrupt changes that severely change the network state (e.g., dynamic traffic workloads, incast events, failures). This leads to under-utilization and sub-optimal performance. This paper presents GraphCC, a novel Machine Learning-based framework for in-network CC optimization. Our distributed solution relies on a novel combination of Multi-agent Reinforcement Learning (MARL) and Graph Neural Networks (GNN), and it is compatible with widely deployed ECN-based CC protocols. GraphCC deploys distributed agents on switches that communicate with their neighbors to cooperate and optimize the global ECN configuration. In our evaluation, we test the performance of GraphCC under a wide variety of scenarios, focusing on the capability of this solution to adapt to new scenarios unseen during training (e.g., new traffic workloads, failures, upgrades). We compare GraphCC with a state-of-the-art MARL-based solution for ECN tuning -- ACC -- and observe that our proposed solution outperforms the state-of-the-art baseline in all of the evaluation scenarios, showing improvements up to $20\%$ in Flow Completion Time as well as significant reductions in buffer occupancy ($38.0-85.7\%$).
</details>
<details>
<summary>摘要</summary>
优化数据中心网络（DCN）的压力控制（CC）在DCN中扮演了基本角色。目前，DCN主要实施两种主要的CC协议：DCTCP和DCQCN。这两种协议都基于显式压力通知（ECN），中间交换机会在检测压力时标识包。因此，ECN配置成为CC协议的关键方面。现在，网络专家通过精心选择ECN参数来优化平均网络性能。然而，今天的高速DCN快速和突然改变网络状态（例如，动态流量负荷、快速响应事件、故障），这会导致网络资源的过度使用和低效。本文介绍了一种基于机器学习的GraphCC框架，用于在网络中进行CC优化。我们的分布式解决方案基于多智能学习（MARL）和图神经网络（GNN），与广泛部署的ECN基本协议相容。GraphCC在交换机上部署分布式代理，与邻居交换机通信以协同优化全局ECN配置。在我们的评估中，我们测试了GraphCC在多种场景下的性能，特别是它在新的场景（例如，新的流量负荷、故障、升级）下的适应能力。我们与一种状态 искусственный智能（AI）基于MARL的ECN调节解决方案（ACC）进行比较，并观察到我们提议的解决方案在所有评估场景中都超过了基线，表现提高了$20\%$的流程完成时间以及显著减少了缓存占用($38.0-85.7\%$)。
</details></li>
</ul>
<hr>
<h2 id="Towards-true-discovery-of-the-differential-equations"><a href="#Towards-true-discovery-of-the-differential-equations" class="headerlink" title="Towards true discovery of the differential equations"></a>Towards true discovery of the differential equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04901">http://arxiv.org/abs/2308.04901</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/itmo-nss-team/klr2023_paper">https://github.com/itmo-nss-team/klr2023_paper</a></li>
<li>paper_authors: Alexander Hvatov, Roman Titov</li>
<li>for: 该论文旨在开发可解释性模型，尤其在自然相关应用中。</li>
<li>methods: 该论文使用机器学习子领域的差分方程发现技术，通过专业地包含通用参数形式的方程动力公式和适当的差分项，使算法能够自主挖掘数据中的方程。</li>
<li>results: 论文探讨了独立方程发现的前提和工具，并解决了评估发现方程的准确性问题，以提供不良方程发现的可靠性评估。<details>
<summary>Abstract</summary>
Differential equation discovery, a machine learning subfield, is used to develop interpretable models, particularly in nature-related applications. By expertly incorporating the general parametric form of the equation of motion and appropriate differential terms, algorithms can autonomously uncover equations from data. This paper explores the prerequisites and tools for independent equation discovery without expert input, eliminating the need for equation form assumptions. We focus on addressing the challenge of assessing the adequacy of discovered equations when the correct equation is unknown, with the aim of providing insights for reliable equation discovery without prior knowledge of the equation form.
</details>
<details>
<summary>摘要</summary>
通过机器学习子领域的差分方程发现，我们可以开发可解释的模型，特别是在自然相关的应用中。通过专业地包含普遍参数形式的运动方程和适当的差分项，算法可以自动从数据中找到方程。本文探讨独立差分方程发现的先决条件和工具，以消除专家参与的假设。我们主要关注评估发现的方程是否准确，即使正确的方程未知。我们的目标是提供可靠的方程发现无需先知方程形式的 Insights。
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Power-of-Extra-Tree-Feature-Selection-and-Random-Forest-Classifier-for-Improved-Survival-Prediction-in-Heart-Failure-Patients"><a href="#Unleashing-the-Power-of-Extra-Tree-Feature-Selection-and-Random-Forest-Classifier-for-Improved-Survival-Prediction-in-Heart-Failure-Patients" class="headerlink" title="Unleashing the Power of Extra-Tree Feature Selection and Random Forest Classifier for Improved Survival Prediction in Heart Failure Patients"></a>Unleashing the Power of Extra-Tree Feature Selection and Random Forest Classifier for Improved Survival Prediction in Heart Failure Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05765">http://arxiv.org/abs/2308.05765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Simul Hasan Talukder, Rejwan Bin Sulaiman, Mouli Bardhan Paul Angon</li>
<li>for: 预测心衰竭患者生存可能性，以便早期干预并提高患者结果。</li>
<li>methods: 利用数据预处理技术和Extra-Tree（ET）特征选择方法，并与Random Forest（RF）分类器结合，以提高心衰竭患者生存预测精度。</li>
<li>results: 通过使用ET特征选择算法，分别在不同的矩阵上进行了Grid Search，并最终选择了最佳的RF模型，实现了98.33%的准确率，这是现有工作中最高的准确率。<details>
<summary>Abstract</summary>
Heart failure is a life-threatening condition that affects millions of people worldwide. The ability to accurately predict patient survival can aid in early intervention and improve patient outcomes. In this study, we explore the potential of utilizing data pre-processing techniques and the Extra-Tree (ET) feature selection method in conjunction with the Random Forest (RF) classifier to improve survival prediction in heart failure patients. By leveraging the strengths of ET feature selection, we aim to identify the most significant predictors associated with heart failure survival. Using the public UCL Heart failure (HF) survival dataset, we employ the ET feature selection algorithm to identify the most informative features. These features are then used as input for grid search of RF. Finally, the tuned RF Model was trained and evaluated using different matrices. The approach was achieved 98.33% accuracy that is the highest over the exiting work.
</details>
<details>
<summary>摘要</summary>
心力衰竭是一种可能致命的疾病，全球范围内有数百万人受其害。能准确预测患者存活可以提供早期干预，提高患者结果。本研究探讨了利用数据处理技术和EXTRA-TREE（ET）特征选择方法，与Random Forest（RF）分类器结合，以提高心力衰竭患者存活预测精度。通过利用ET特征选择算法，我们希望确定心力衰竭存活预测中最重要的预测因素。使用公共的UCL心力衰竭存活数据集，我们采用ET特征选择算法，并将选择出的特征作为RF模型的输入。最后，我们使用不同的矩阵进行RF模型的调整，并训练和评估模型。我们的方法实现了98.33%的准确率，这是已有工作中最高的精度。
</details></li>
</ul>
<hr>
<h2 id="Targeted-and-Troublesome-Tracking-and-Advertising-on-Children’s-Websites"><a href="#Targeted-and-Troublesome-Tracking-and-Advertising-on-Children’s-Websites" class="headerlink" title="Targeted and Troublesome: Tracking and Advertising on Children’s Websites"></a>Targeted and Troublesome: Tracking and Advertising on Children’s Websites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04887">http://arxiv.org/abs/2308.04887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Moti, Asuman Senol, Hamid Bostani, Frederik Zuiderveen Borgesius, Veelasha Moonsamy, Arunesh Mathur, Gunes Acar</li>
<li>for: 这项研究的目的是对于儿童Directed websites上的跟踪和广告实现监测和分析，以了解现有的隐私和广告安全问题。</li>
<li>methods: 该研究使用了多语言分类器和爬虫技术来检测和分类儿童Directed websites，并对这些网站上的跟踪和广告进行了评估。</li>
<li>results: 研究发现大约90%的儿童Directed websites中存在至少一个跟踪器，而27%的网站上显示了targeted广告，其中一些广告包含了不适和性化的图像和文本。这些结果表明许多广告商和儿童Directed websites不遵守隐私法规和广告安全标准。<details>
<summary>Abstract</summary>
On the modern web, trackers and advertisers frequently construct and monetize users' detailed behavioral profiles without consent. Despite various studies on web tracking mechanisms and advertisements, there has been no rigorous study focusing on websites targeted at children. To address this gap, we present a measurement of tracking and (targeted) advertising on websites directed at children. Motivated by lacking a comprehensive list of child-directed (i.e., targeted at children) websites, we first build a multilingual classifier based on web page titles and descriptions. Applying this classifier to over two million pages, we compile a list of two thousand child-directed websites. Crawling these sites from five vantage points, we measure the prevalence of trackers, fingerprinting scripts, and advertisements. Our crawler detects ads displayed on child-directed websites and determines if ad targeting is enabled by scraping ad disclosure pages whenever available. Our results show that around 90% of child-directed websites embed one or more trackers, and about 27% contain targeted advertisements--a practice that should require verifiable parental consent. Next, we identify improper ads on child-directed websites by developing an ML pipeline that processes both images and text extracted from ads. The pipeline allows us to run semantic similarity queries for arbitrary search terms, revealing ads that promote services related to dating, weight loss, and mental health; as well as ads for sex toys and flirting chat services. Some of these ads feature repulsive and sexually explicit imagery. In summary, our findings indicate a trend of non-compliance with privacy regulations and troubling ad safety practices among many advertisers and child-directed websites. To protect children and create a safer online environment, regulators and stakeholders must adopt and enforce more stringent measures.
</details>
<details>
<summary>摘要</summary>
现代网络上，跟踪者和广告商经常构建和利用用户的详细行为profile，而不需要用户的同意。虽然有各种研究关于网络跟踪机制和广告，但没有一份严格的研究关于向儿童targeted的网站。为了填补这一空白，我们提供了一项测量网络跟踪和targeted广告的研究。由于缺乏全面的儿童irected（即向儿童targeted）网站列表，我们首先基于网页标题和描述建立了多语言分类器。应用这个分类器到超过两百万页面后，我们编辑了二千个儿童irected网站的列表。从五个视角下载这些站点，我们测量了跟踪器、指纹脚本和广告的存在。我们的抓取器可以在儿童irected网站上显示广告并确定广告是否启用了透明的父母consent。我们的结果显示，大约90%的儿童irected网站包含一个或多个跟踪器，而约27%的网站包含targeted广告，这种实践应该需要可靠的父母consent。接着，我们使用ML管道来识别在儿童irected网站上的不当广告。这个管道可以处理图像和文本抽取自广告，并允许我们对任意搜索关键词进行语义相似性查询，暴露出promote services related to dating, weight loss, and mental health，以及sex toys和flirting chat services的广告。一些这些广告具有俗气和色情内容。总之，我们的发现表明许多广告商和child-directed网站存在隐私法规和不安全的广告实践的趋势。为了保护儿童和创造更安全的网络环境，Regulators和相关方需要采取和实施更加严格的措施。
</details></li>
</ul>
<hr>
<h2 id="Decorrelating-neurons-using-persistence"><a href="#Decorrelating-neurons-using-persistence" class="headerlink" title="Decorrelating neurons using persistence"></a>Decorrelating neurons using persistence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04870">http://arxiv.org/abs/2308.04870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rballeba/decorrelatingneuronsusingpersistence">https://github.com/rballeba/decorrelatingneuronsusingpersistence</a></li>
<li>paper_authors: Rubén Ballester, Carles Casacuberta, Sergio Escalera</li>
<li>for: 提高深度学习模型的泛化能力</li>
<li>methods: 使用最小杂植树的谱边 weights 计算两个正则化项，以降低神经元之间的高相关性</li>
<li>results: 比较各种普遍的正则化项，证明了我们的正则化项的效果，并且表明了神经元之间的 redundancy 在人工神经网络中扮演着重要角色。<details>
<summary>Abstract</summary>
We propose a novel way to improve the generalisation capacity of deep learning models by reducing high correlations between neurons. For this, we present two regularisation terms computed from the weights of a minimum spanning tree of the clique whose vertices are the neurons of a given network (or a sample of those), where weights on edges are correlation dissimilarities. We provide an extensive set of experiments to validate the effectiveness of our terms, showing that they outperform popular ones. Also, we demonstrate that naive minimisation of all correlations between neurons obtains lower accuracies than our regularisation terms, suggesting that redundancies play a significant role in artificial neural networks, as evidenced by some studies in neuroscience for real networks. We include a proof of differentiability of our regularisers, thus developing the first effective topological persistence-based regularisation terms that consider the whole set of neurons and that can be applied to a feedforward architecture in any deep learning task such as classification, data generation, or regression.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来提高深度学习模型的泛化能力，通过减少神经元之间的高相关性。为此，我们提出了两种正则化项，它们是基于神经网络中最小拓扑树的节点（神经元）的权重的 correlate dissimilarity 的边 weights。我们进行了广泛的实验 validate 这些正则化项的效果，并证明它们超过了流行的正则化项。此外，我们还示出了使用所有神经元之间的相关性进行简单的最小化会导致模型的准确率下降，这表明神经网络中存在很多重复的结构，这与一些 neuroscience 研究中的真实神经网络相符。我们还证明了我们的正则化项是可导的，因此我们开发了首次有效的拓扑persistence-based 正则化项，可以应用于任何深度学习任务，如分类、数据生成或回归。
</details></li>
</ul>
<hr>
<h2 id="Are-Sex-based-Physiological-Differences-the-Cause-of-Gender-Bias-for-Chest-X-ray-Diagnosis"><a href="#Are-Sex-based-Physiological-Differences-the-Cause-of-Gender-Bias-for-Chest-X-ray-Diagnosis" class="headerlink" title="Are Sex-based Physiological Differences the Cause of Gender Bias for Chest X-ray Diagnosis?"></a>Are Sex-based Physiological Differences the Cause of Gender Bias for Chest X-ray Diagnosis?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05129">http://arxiv.org/abs/2308.05129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nina Weng, Siavash Bigdeli, Eike Petersen, Aasa Feragen</li>
<li>for: 这个研究旨在探讨胸部X射线诊断中的性别偏见的原因。</li>
<li>methods: 作者提出了一种新的采样方法，以解决两个公共数据集中每个病人记录的极度不均衡的问题，同时减少标签错误的影响。</li>
<li>results: 研究发现，数据集特有的因素，而不是基本的生理差异，是胸部X射线诊断中男女性能差异的主要驱动力。<details>
<summary>Abstract</summary>
While many studies have assessed the fairness of AI algorithms in the medical field, the causes of differences in prediction performance are often unknown. This lack of knowledge about the causes of bias hampers the efficacy of bias mitigation, as evidenced by the fact that simple dataset balancing still often performs best in reducing performance gaps but is unable to resolve all performance differences. In this work, we investigate the causes of gender bias in machine learning-based chest X-ray diagnosis. In particular, we explore the hypothesis that breast tissue leads to underexposure of the lungs and causes lower model performance. Methodologically, we propose a new sampling method which addresses the highly skewed distribution of recordings per patient in two widely used public datasets, while at the same time reducing the impact of label errors. Our comprehensive analysis of gender differences across diseases, datasets, and gender representations in the training set shows that dataset imbalance is not the sole cause of performance differences. Moreover, relative group performance differs strongly between datasets, indicating important dataset-specific factors influencing male/female group performance. Finally, we investigate the effect of breast tissue more specifically, by cropping out the breasts from recordings, finding that this does not resolve the observed performance gaps. In conclusion, our results indicate that dataset-specific factors, not fundamental physiological differences, are the main drivers of male--female performance gaps in chest X-ray analyses on widely used NIH and CheXpert Dataset.
</details>
<details>
<summary>摘要</summary>
尽管许多研究已经评估了人工智能算法在医疗领域的公平性，但对于不同性能的原因常常无法得知。这种不知道偏见的原因使得偏见缓和措施无法充分发挥作用，如果只通过简单地数据平衡来减少性能差距，但是无法解决所有的性能差距。在这项工作中，我们调查了机器学习基于胸部X射影的性别偏见的原因。特别是，我们研究了胸部肉体对肺部的Underexposure的影响，是否导致较低的模型性能。我们提出了一种新的采样方法，以解决两个广泛使用的公共数据集中记录每个患者的高度不均衡的问题，同时减少标签错误的影响。我们的全面分析表明，数据集偏见不是唯一的原因，而且男女群体性能之间的差距强烈地受到数据集特定的因素的影响。此外，我们发现在不同的疾病、数据集和训练集中，男女群体的表现差异很大，这表明了数据集特定的因素对男女群体表现的重要影响。最后，我们具体研究了胸部肉体的影响，通过从记录中cropping出胸部，发现这并不能解决观察到的性能差距。结论是，我们的结果表明，在广泛使用的NIH和CheXpert数据集中，数据集特定的因素，而不是基础的生理学差异，是 male--female 性能差距的主要驱动力。
</details></li>
</ul>
<hr>
<h2 id="Scalability-of-Message-Encoding-Techniques-for-Continuous-Communication-Learned-with-Multi-Agent-Reinforcement-Learning"><a href="#Scalability-of-Message-Encoding-Techniques-for-Continuous-Communication-Learned-with-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Scalability of Message Encoding Techniques for Continuous Communication Learned with Multi-Agent Reinforcement Learning"></a>Scalability of Message Encoding Techniques for Continuous Communication Learned with Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04844">http://arxiv.org/abs/2308.04844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Astrid Vanneste, Thomas Somers, Simon Vanneste, Kevin Mets, Tom De Schepper, Siegfried Mercelis, Peter Hellinckx</li>
<li>for: 这 paper  investigate 多 Agent 系统中 Agent 之间的 Communication 协议，以实现 Goal。</li>
<li>methods: 该 paper 使用 Multi-Agent Reinforcement Learning 技术，学习 Communication 协议并 Action 协议。</li>
<li>results: 研究发现，增加信息量和 Agent 数量会影响 Message 编码方法的性能。mean Message Encoder consistently outperforms attention Message Encoder。Agents 使用 exponential 和 logarithmic 函数组合来确保信息不产生重要信息丢失。<details>
<summary>Abstract</summary>
Many multi-agent systems require inter-agent communication to properly achieve their goal. By learning the communication protocol alongside the action protocol using multi-agent reinforcement learning techniques, the agents gain the flexibility to determine which information should be shared. However, when the number of agents increases we need to create an encoding of the information contained in these messages. In this paper, we investigate the effect of increasing the amount of information that should be contained in a message and increasing the number of agents. We evaluate these effects on two different message encoding methods, the mean message encoder and the attention message encoder. We perform our experiments on a matrix environment. Surprisingly, our results show that the mean message encoder consistently outperforms the attention message encoder. Therefore, we analyse the communication protocol used by the agents that use the mean message encoder and can conclude that the agents use a combination of an exponential and a logarithmic function in their communication policy to avoid the loss of important information after applying the mean message encoder.
</details>
<details>
<summary>摘要</summary>
多个自动机制系统需要间接机器人之间的交流以达到目标。通过使用多自动机器人学习回报学习技术，机器人可以适应决定需要交换哪些信息。然而，当机器人数量增加时，我们需要设计机器人之间信息的编码方式。在这篇论文中，我们研究增加信息内容的消息编码方法和机器人数量之间的影响。我们使用两种消息编码方法进行比较：平均消息编码器和注意力消息编码器。我们在矩阵环境中进行实验，结果显示，平均消息编码器一直高于注意力消息编码器。因此，我们分析了使用平均消息编码器的机器人通信策略，并结论机器人使用一种加速和对数函数在其通信策略中，以避免消息编码后重要信息的丢失。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-the-Diagnostic-Potential-of-ECG-through-Knowledge-Transfer-from-Cardiac-MRI"><a href="#Unlocking-the-Diagnostic-Potential-of-ECG-through-Knowledge-Transfer-from-Cardiac-MRI" class="headerlink" title="Unlocking the Diagnostic Potential of ECG through Knowledge Transfer from Cardiac MRI"></a>Unlocking the Diagnostic Potential of ECG through Knowledge Transfer from Cardiac MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05764">http://arxiv.org/abs/2308.05764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oetu/mmcl-ecg-cmr">https://github.com/oetu/mmcl-ecg-cmr</a></li>
<li>paper_authors: Özgün Turgut, Philip Müller, Paul Hager, Suprosanna Shit, Sophie Starck, Martin J. Menten, Eimo Martens, Daniel Rueckert</li>
<li>for: 本研究旨在使用低成本和快速的电cardiogram（ECG）诊断工具，以便评估Cardiovascular health，但是更详细的Cardiac magnetic resonance（CMR）成像技术frequently preferred for cardiovascular disease diagnosis。</li>
<li>methods: 本研究提出了第一个自我超级对比方法，将CMR图像领域特有的信息传递到ECG嵌入中。该方法结合多模态对比学习和masked数据模型，以实现各种cardiac screening solely from ECG data。</li>
<li>results: 在40,044名UK BiobankSubjects的广泛实验中，我们证明了我们的方法的实用性和普遍性。我们预测了每个人specific risk of various cardiovascular diseases，并确定了各种cardiac phenotypes solely from ECG data。在质量分析中，我们示出了我们学习的ECG嵌入中包含了CMR图像区域特有的信息。<details>
<summary>Abstract</summary>
The electrocardiogram (ECG) is a widely available diagnostic tool that allows for a cost-effective and fast assessment of the cardiovascular health. However, more detailed examination with expensive cardiac magnetic resonance (CMR) imaging is often preferred for the diagnosis of cardiovascular diseases. While providing detailed visualization of the cardiac anatomy, CMR imaging is not widely available due to long scan times and high costs. To address this issue, we propose the first self-supervised contrastive approach that transfers domain-specific information from CMR images to ECG embeddings. Our approach combines multimodal contrastive learning with masked data modeling to enable holistic cardiac screening solely from ECG data. In extensive experiments using data from 40,044 UK Biobank subjects, we demonstrate the utility and generalizability of our method. We predict the subject-specific risk of various cardiovascular diseases and determine distinct cardiac phenotypes solely from ECG data. In a qualitative analysis, we demonstrate that our learned ECG embeddings incorporate information from CMR image regions of interest. We make our entire pipeline publicly available, including the source code and pre-trained model weights.
</details>
<details>
<summary>摘要</summary>
电 electrocardiogram (ECG) 是一种广泛可用的诊断工具，可以快速、效率地评估心血管健康。然而，更详细的检查通常使用昂贵的心血管 Ressonance 成像 (CMR) 技术进行诊断心血管疾病。虽然提供了详细的心血管解剖图像，但 CMR 成像因长时间扫描和高成本而不够普及。为解决这个问题，我们提出了首个自动超级对比方法，将域特异的信息从 CMR 图像传递到 ECG 嵌入。我们的方法结合多模态对比学习和遮盖数据模型，以实现基于 ECG 数据的整体卡ди亚屏检查。在使用 UK Biobank 数据集的广泛实验中，我们证明了我们的方法的实用性和普遍性。我们预测参与者的具体风险以及不同心血管疾病的各种Cardiac phenotypes  solely from ECG data。在质量分析中，我们示示了我们学习的 ECG 嵌入包含 CMR 图像区域关注的信息。我们整个气管系统公开了整个管道，包括源代码和预训练模型参数。
</details></li>
</ul>
<hr>
<h2 id="Intrinsic-Motivation-via-Surprise-Memory"><a href="#Intrinsic-Motivation-via-Surprise-Memory" class="headerlink" title="Intrinsic Motivation via Surprise Memory"></a>Intrinsic Motivation via Surprise Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04836">http://arxiv.org/abs/2308.04836</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opendilab/DI-engine">https://github.com/opendilab/DI-engine</a></li>
<li>paper_authors: Hung Le, Kien Do, Dung Nguyen, Svetha Venkatesh</li>
<li>for: 提出了一种新的计算模型，用于替代现有的惊讶驱动的探索。</li>
<li>methods: 使用了一种记忆网络来估计惊讶新意，并将其作为奖励。</li>
<li>results: 实验结果表明，该模型在缺乏奖励的环境中可以具有高效的探索行为，并在杂乱的Atari游戏中显示出显著的提高。<details>
<summary>Abstract</summary>
We present a new computing model for intrinsic rewards in reinforcement learning that addresses the limitations of existing surprise-driven explorations. The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises. Our surprise memory (SM) augments the capability of surprise-based intrinsic motivators, maintaining the agent's interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations. Our experiments demonstrate that the SM combined with various surprise predictors exhibits efficient exploring behaviors and significantly boosts the final performance in sparse reward environments, including Noisy-TV, navigation and challenging Atari games.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的计算模型，用于内在奖励在机器学习中的惊喜探索。这种模型利用惊喜的新性而不是惊喜的 норма来计算奖励。我们使用记忆网络来估计惊喜的新性，并将其称为惊喜记忆（SM）。我们的SM可以增强惊喜基于内在奖励的能力，使代理人保持有趣的探索，同时减少不必要的吸引到随机或噪音观察的吸引力。我们的实验显示，SM与不同的惊喜预测器结合使用可以实现高效的探索行为，并在罕见奖励环境中提高最终性能，包括噪音电视、导航和复杂的Atari游戏。
</details></li>
</ul>
<hr>
<h2 id="TSSR-A-Truncated-and-Signed-Square-Root-Activation-Function-for-Neural-Networks"><a href="#TSSR-A-Truncated-and-Signed-Square-Root-Activation-Function-for-Neural-Networks" class="headerlink" title="TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks"></a>TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04832">http://arxiv.org/abs/2308.04832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Gong</li>
<li>for: 提高神经网络的数值稳定性</li>
<li>methods: 提出新的激活函数Truncated and Signed Square Root (TSSR)函数</li>
<li>results: TSSR函数在各种应用领域中的表现比其他状态OF-the-art激活函数更好<details>
<summary>Abstract</summary>
Activation functions are essential components of neural networks. In this paper, we introduce a new activation function called the Truncated and Signed Square Root (TSSR) function. This function is distinctive because it is odd, nonlinear, monotone and differentiable. Its gradient is continuous and always positive. Thanks to these properties, it has the potential to improve the numerical stability of neural networks. Several experiments confirm that the proposed TSSR has better performance than other stat-of-the-art activation functions. The proposed function has significant implications for the development of neural network models and can be applied to a wide range of applications in fields such as computer vision, natural language processing, and speech recognition.
</details>
<details>
<summary>摘要</summary>
activation functions 是 neural network 的重要组件。本文提出了一新的 activation function，即 Truncated and Signed Square Root (TSSR) 函数。这个函数特别之处在于它是odd、非线性、单调的，梯度连续和一定的。这些属性使得 TSSR 函数有改善神经网络的数学稳定性的潜力。几个实验表明，提议的 TSSR 函数在比较当前的 activation functions 中表现更好。这种函数对神经网络模型的发展具有重要意义，可以应用于计算机视觉、自然语言处理和语音识别等领域。
</details></li>
</ul>
<hr>
<h2 id="On-the-Unexpected-Abilities-of-Large-Language-Models"><a href="#On-the-Unexpected-Abilities-of-Large-Language-Models" class="headerlink" title="On the Unexpected Abilities of Large Language Models"></a>On the Unexpected Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09720">http://arxiv.org/abs/2308.09720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefano Nolfi</li>
<li>for: 本文研究大型语言模型display的多种能力，包括预测人类写作的下一个词。</li>
<li>methods: 本文使用大型语言模型进行研究，分析其间接获得的能力和其与其他知名间接过程的关系。</li>
<li>results: 研究发现，大型语言模型通过间接获得的过程，开发出了一系列整体能力。此外，研究还发现这些能力的发展程度有一定的预测性。最后，本文 briefly discusses大型语言模型所获得的认知能力和人类认知之间的关系。<details>
<summary>Abstract</summary>
Large language models are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I discuss the nature of this indirect acquisition process and its relation to other known indirect processes. I argue that an important side effect of such indirect acquisition is the development of integrated abilities. I discuss the extent to which the abilities developed by large language models are predictable. Finally, I briefly discuss the relation between the cognitive skills acquired by these systems and human cognition.
</details>
<details>
<summary>摘要</summary>
大型语言模型可以显示广泛的能力，并不直接与它们的训练任务相关：预测人写的文本中的下一个字。在这篇文章中，我讨论这种 indirect acquisition 的过程和其他已知的 indirect process 的关系。我认为这种 indirect acquisition 的一个重要副作用是发展集成的能力。我讨论这些系统所获得的能力是否可预测。最后，我简 briefly discuss 这些系统所获得的认知技能和人类认知之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Bayes-Risk-Consistency-of-Nonparametric-Classification-Rules-for-Spike-Trains-Data"><a href="#Bayes-Risk-Consistency-of-Nonparametric-Classification-Rules-for-Spike-Trains-Data" class="headerlink" title="Bayes Risk Consistency of Nonparametric Classification Rules for Spike Trains Data"></a>Bayes Risk Consistency of Nonparametric Classification Rules for Spike Trains Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04796">http://arxiv.org/abs/2308.04796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mirosław Pawlak, Mateusz Pabian, Dominik Rzepka</li>
<li>for: 这篇论文是用于探讨干扰讯号数据的应用和机器学习策略。</li>
<li>methods: 这篇论文使用了机器学习的神经网络和概率模型，特别是非 Parametric 方法。</li>
<li>results: 论文提出了一种基于 Bayes 规则的非 Parametric 核函数分类器，并证明了这种分类器在不断 recording time interval 和训练集大小的情况下的极限。<details>
<summary>Abstract</summary>
Spike trains data find a growing list of applications in computational neuroscience, imaging, streaming data and finance. Machine learning strategies for spike trains are based on various neural network and probabilistic models. The probabilistic approach is relying on parametric or nonparametric specifications of the underlying spike generation model. In this paper we consider the two-class statistical classification problem for a class of spike train data characterized by nonparametrically specified intensity functions. We derive the optimal Bayes rule and next form the plug-in nonparametric kernel classifier. Asymptotical properties of the rules are established including the limit with respect to the increasing recording time interval and the size of a training set. In particular the convergence of the kernel classifier to the Bayes rule is proved. The obtained results are supported by a finite sample simulation studies.
</details>
<details>
<summary>摘要</summary>
随着计算神经科学、成像、流动数据和金融等领域中的应用，锤形数据（spike train）在计算机科学中找到了一个不断增长的应用范围。机器学习策略 для锤形数据基于不同的神经网络和概率模型。概率方法基于锤形数据的下面参数化或非参数化模型规定。在本文中，我们考虑一种锤形数据的两类统计分类问题，其中锤形数据由非参数化强度函数特征。我们 derivate了最佳拟合规则，然后形成插入式非参数化核函数分类器。我们确定了这些规则的非正式性质，包括随着记录时间间隔的增长和训练集大小的限制。具体来说，我们证明了核函数分类器的拟合规则与拟合规则之间的连续性。获得的结果得到了finite sample simulations的支持。
</details></li>
</ul>
<hr>
<h2 id="PETformer-Long-term-Time-Series-Forecasting-via-Placeholder-enhanced-Transformer"><a href="#PETformer-Long-term-Time-Series-Forecasting-via-Placeholder-enhanced-Transformer" class="headerlink" title="PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer"></a>PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04791">http://arxiv.org/abs/2308.04791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengsheng Lin, Weiwei Lin, Wentai Wu, Songbo Wang, Yongxiang Wang</li>
<li>for: 这 paper  investigate 应用 Transformer 到长期时间序列预测（LTSF）任务中的三个关键问题，包括时间连续性、信息密度和多通道关系。</li>
<li>methods: 该 paper 提出了三种创新解决方案，包括 Placeholder Enhancement Technique (PET)、Long Sub-sequence Division (LSD) 和 Multi-channel Separation and Interaction (MSI)，这三种方法结合成一个 novel model 称为 PETformer。</li>
<li>results: 广泛的实验表明，PETformer 在八个公共数据集上实现了状态空间（SOTA）的表现，超过了所有目前可用的其他模型。这表明 Transformer 仍具有在 LTSF 任务中的强大能力。<details>
<summary>Abstract</summary>
Recently, Transformer-based models have shown remarkable performance in long-term time series forecasting (LTSF) tasks due to their ability to model long-term dependencies. However, the validity of Transformers for LTSF tasks remains debatable, particularly since recent work has shown that simple linear models can outperform numerous Transformer-based approaches. This suggests that there are limitations to the application of Transformer in LTSF. Therefore, this paper investigates three key issues when applying Transformer to LTSF: temporal continuity, information density, and multi-channel relationships. Accordingly, we propose three innovative solutions, including Placeholder Enhancement Technique (PET), Long Sub-sequence Division (LSD), and Multi-channel Separation and Interaction (MSI), which together form a novel model called PETformer. These three key designs introduce prior biases suitable for LTSF tasks. Extensive experiments have demonstrated that PETformer achieves state-of-the-art (SOTA) performance on eight commonly used public datasets for LTSF, outperforming all other models currently available. This demonstrates that Transformer still possesses powerful capabilities in LTSF.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近期，基于Transformer的模型在长期时间序预测（LTSF）任务中表现出色，归功于它们能够模型长期相互关系。然而，使用Transformer进行LTSF任务的有效性仍然存在争议，尤其是最近的研究表明，简单的线性模型可以超越许多基于Transformer的方法。这表明在LTSF任务中使用Transformer存在限制。因此，本文研究了在应用Transformer到LTSF任务时的三个关键问题：时间连续性、信息密度和多通道关系。根据这些问题，我们提出了三个创新的解决方案，包括Placeholder增强技术（PET）、长 subsequences分配（LSD）和多通道分离和互动（MSI），这些设计共同组成了一个名为PETformer的新模型。这三个关键设计引入了适合LTSF任务的先验偏好。经过广泛的实验，我们发现PETformer在八个公共数据集上实现了状态的最佳表现（SOTA），比所有当前可用的模型都高。这表明Transformer在LTSF任务中仍然拥有强大的能力。
</details></li>
</ul>
<hr>
<h2 id="SUnAA-Sparse-Unmixing-using-Archetypal-Analysis"><a href="#SUnAA-Sparse-Unmixing-using-Archetypal-Analysis" class="headerlink" title="SUnAA: Sparse Unmixing using Archetypal Analysis"></a>SUnAA: Sparse Unmixing using Archetypal Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04771">http://arxiv.org/abs/2308.04771</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/behnoodrasti/sunaa">https://github.com/behnoodrasti/sunaa</a></li>
<li>paper_authors: Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot</li>
<li>for: 本研究旨在提出一种新的稀疏分解技术，使用体例分析（SUnAA）。</li>
<li>methods: 我们提出了一种基于体例分析的新模型，假设需要的终结点是spectral库中提供的终结点的叠加。然后，我们提出了一个非凸最小化问题，并使用活动集算法进行迭代最小化。</li>
<li>results: 我们通过两个 simulations datasets的测试，发现SUnAA的性能比普通的稀疏分解方法和先进的方法更好，具有较低的信号征化误差。此外，我们还应用了SUnAA于Cuprite数据集，并与可用的地质地图进行比较，结果表明SUnAA可以成功估计矿物含量，并提高主要矿物的检测。<details>
<summary>Abstract</summary>
This paper introduces a new sparse unmixing technique using archetypal analysis (SUnAA). First, we design a new model based on archetypal analysis. We assume that the endmembers of interest are a convex combination of endmembers provided by a spectral library and that the number of endmembers of interest is known. Then, we propose a minimization problem. Unlike most conventional sparse unmixing methods, here the minimization problem is non-convex. We minimize the optimization objective iteratively using an active set algorithm. Our method is robust to the initialization and only requires the number of endmembers of interest. SUnAA is evaluated using two simulated datasets for which results confirm its better performance over other conventional and advanced techniques in terms of signal-to-reconstruction error. SUnAA is also applied to Cuprite dataset and the results are compared visually with the available geological map provided for this dataset. The qualitative assessment demonstrates the successful estimation of the minerals abundances and significantly improves the detection of dominant minerals compared to the conventional regression-based sparse unmixing methods. The Python implementation of SUnAA can be found at: https://github.com/BehnoodRasti/SUnAA.
</details>
<details>
<summary>摘要</summary>
The performance of SUnAA is evaluated using two simulated datasets, and the results show that it outperforms conventional and advanced techniques in terms of signal-to-reconstruction error. SUnAA is also applied to the Cuprite dataset and the results are compared visually with the available geological map. The qualitative assessment demonstrates the successful estimation of mineral abundances and the improved detection of dominant minerals compared to conventional regression-based sparse unmixing methods.The Python implementation of SUnAA can be found at the following link: <https://github.com/BehnoodRasti/SUnAA>.Translated into Simplified Chinese:这篇论文介绍了一种新的稀疏混合技术基于形态分析（SUnAA）。该方法假设 interessant endmembers 是 spectral library 中的累累组合，并且知道 interested endmembers 的数量。然后，我们提出了一个非凸优化问题，并使用 active set 算法进行迭代优化。该方法具有初始化Robustness和只需要 interested endmembers 的数量。我们使用两个 simulated dataset 进行评估 SUnAA 的性能，结果显示它在 signal-to-reconstruction error 方面表现出色，超过了 conventional 和 advanced 方法。此外，我们还应用 SUnAA 于 Cuprite dataset，并与可用的地质地图进行比较。结果表明 SUnAA 能够成功估计矿物质含量，并提高 dominant 矿物质的检测。Python 实现 SUnAA 可以在以下链接中找到：<https://github.com/BehnoodRasti/SUnAA>。Translated into Traditional Chinese:这篇论文介绍了一种新的稀疏混合技术基于形态分析（SUnAA）。该方法假设 interessant endmembers 是 spectral library 中的累累组合，并且知道 interested endmembers 的数量。然后，我们提出了一个非凸优化问题，并使用 active set 算法进行迭代优化。该方法具有初始化Robustness和只需要 interested endmembers 的数量。我们使用两个 simulated dataset 进行评估 SUnAA 的性能，结果显示它在 signal-to-reconstruction error 方面表现出色，超过了 conventional 和 advanced 方法。此外，我们还应用 SUnAA 于 Cuprite dataset，并与可用的地质地图进行比较。结果表明 SUnAA 能够成功估计矿物质含量，并提高 dominant 矿物质的检测。Python 实现 SUnAA 可以在以下链接中找到：<https://github.com/BehnoodRasti/SUnAA>。
</details></li>
</ul>
<hr>
<h2 id="Tram-FL-Routing-based-Model-Training-for-Decentralized-Federated-Learning"><a href="#Tram-FL-Routing-based-Model-Training-for-Decentralized-Federated-Learning" class="headerlink" title="Tram-FL: Routing-based Model Training for Decentralized Federated Learning"></a>Tram-FL: Routing-based Model Training for Decentralized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04762">http://arxiv.org/abs/2308.04762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KotaMaejima/Tram-FL">https://github.com/KotaMaejima/Tram-FL</a></li>
<li>paper_authors: Kota Maejima, Takayuki Nishio, Asato Yamazaki, Yuko Hara-Azumi</li>
<li>for: 提高 federated learning 中 nodes 之间的交互频率和非独立同分布数据的准确性。</li>
<li>methods: 提出了一种新的 federated learning 方法，称为 Tram-FL，它逐渐改进全局模型，通过在节点之间传递模型，而不是通过交换和聚合本地模型。同时，引入了一种动态模型路由算法，以优化模型精度，尽可能减少前向传输。</li>
<li>results: 通过 MNIST、CIFAR-10 和 IMDb 数据集的实验表明，Tram-FL 并与提出的路由算法可以在非独立同分布条件下实现高精度模型，与基elines 相比，减少了交通成本。<details>
<summary>Abstract</summary>
In decentralized federated learning (DFL), substantial traffic from frequent inter-node communication and non-independent and identically distributed (non-IID) data challenges high-accuracy model acquisition. We propose Tram-FL, a novel DFL method, which progressively refines a global model by transferring it sequentially amongst nodes, rather than by exchanging and aggregating local models. We also introduce a dynamic model routing algorithm for optimal route selection, aimed at enhancing model precision with minimal forwarding. Our experiments using MNIST, CIFAR-10, and IMDb datasets demonstrate that Tram-FL with the proposed routing delivers high model accuracy under non-IID conditions, outperforming baselines while reducing communication costs.
</details>
<details>
<summary>摘要</summary>
在分布式联合学习（DFL）中，频繁的节点间通信和非独立同分布（非IID）数据对高精度模型获得带来了挑战。我们提出了Tram-FL方法，它逐步提高全球模型，通过在节点之间逐步传输全球模型，而不是通过交换和汇总本地模型。我们还提出了动态模型路由算法，用于优化路由，以提高模型精度并最小化前进通信成本。我们通过使用MNIST、CIFAR-10和IMDb数据集进行实验，显示Tram-FL与提议的路由可以在非IID条件下提高模型精度，而且比基eline减少通信成本。
</details></li>
</ul>
<hr>
<h2 id="Feature-Matching-Data-Synthesis-for-Non-IID-Federated-Learning"><a href="#Feature-Matching-Data-Synthesis-for-Non-IID-Federated-Learning" class="headerlink" title="Feature Matching Data Synthesis for Non-IID Federated Learning"></a>Feature Matching Data Synthesis for Non-IID Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04761">http://arxiv.org/abs/2308.04761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijian Li, Yuchang Sun, Jiawei Shao, Yuyi Mao, Jessie Hui Wang, Jun Zhang</li>
<li>for: 这种论文是为了解决 Federated Learning (FL) 中的非独立和相同分布 (non-IID) 数据问题，提出了一种困难特征匹配数据生成 (HFMDS) 方法，以及一种硬特征增强方法，以提高模型通用性和隐私保护。</li>
<li>methods: 这种论文使用了一种基于 HFMDS 的 FL 框架，并提出了一种基于硬特征增强的匹配数据生成方法，以及一种理论分析，证明 HFMDS 方法的有效性。</li>
<li>results:  simulation 结果表明，与基eline 相比，该提议的 HFMDS-FL 算法在不同的 benchmark 数据集上达到了更高的准确率、隐私保护和计算成本。<details>
<summary>Abstract</summary>
Federated learning (FL) has emerged as a privacy-preserving paradigm that trains neural networks on edge devices without collecting data at a central server. However, FL encounters an inherent challenge in dealing with non-independent and identically distributed (non-IID) data among devices. To address this challenge, this paper proposes a hard feature matching data synthesis (HFMDS) method to share auxiliary data besides local models. Specifically, synthetic data are generated by learning the essential class-relevant features of real samples and discarding the redundant features, which helps to effectively tackle the non-IID issue. For better privacy preservation, we propose a hard feature augmentation method to transfer real features towards the decision boundary, with which the synthetic data not only improve the model generalization but also erase the information of real features. By integrating the proposed HFMDS method with FL, we present a novel FL framework with data augmentation to relieve data heterogeneity. The theoretical analysis highlights the effectiveness of our proposed data synthesis method in solving the non-IID challenge. Simulation results further demonstrate that our proposed HFMDS-FL algorithm outperforms the baselines in terms of accuracy, privacy preservation, and computational cost on various benchmark datasets.
</details>
<details>
<summary>摘要</summary>
Federated learning（FL）已经 emerged as a privacy-preserving paradigm that trains neural networks on edge devices without collecting data at a central server. However, FL encounters an inherent challenge in dealing with non-independent and identically distributed（non-IID）data among devices. To address this challenge, this paper proposes a hard feature matching data synthesis（HFMDS）method to share auxiliary data besides local models. Specifically, synthetic data are generated by learning the essential class-relevant features of real samples and discarding the redundant features, which helps to effectively tackle the non-IID issue. For better privacy preservation, we propose a hard feature augmentation method to transfer real features towards the decision boundary, with which the synthetic data not only improve the model generalization but also erase the information of real features. By integrating the proposed HFMDS method with FL, we present a novel FL framework with data augmentation to relieve data heterogeneity. The theoretical analysis highlights the effectiveness of our proposed data synthesis method in solving the non-IID challenge. Simulation results further demonstrate that our proposed HFMDS-FL algorithm outperforms the baselines in terms of accuracy, privacy preservation, and computational cost on various benchmark datasets.Note: Please note that the translation is in Simplified Chinese, which is one of the two standardized Chinese scripts used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Learning-From-Distributed-Data-With-Differentially-Private-Synthetic-Twin-Data"><a href="#Collaborative-Learning-From-Distributed-Data-With-Differentially-Private-Synthetic-Twin-Data" class="headerlink" title="Collaborative Learning From Distributed Data With Differentially Private Synthetic Twin Data"></a>Collaborative Learning From Distributed Data With Differentially Private Synthetic Twin Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04755">http://arxiv.org/abs/2308.04755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dpbayes/collaborative-learning-with-dp-synthetic-twin-data">https://github.com/dpbayes/collaborative-learning-with-dp-synthetic-twin-data</a></li>
<li>paper_authors: Lukas Prediger, Joonas Jälkö, Antti Honkela, Samuel Kaski</li>
<li>for: 这个论文是为了解决多个党部拥有敏感数据的问题，即如何协同学习人口级统计数据，而不论pooling敏感数据集。</li>
<li>methods: 作者提议了一种方案，即每个党部都将其敏感数据分配到一个隐私保护的同步双数据集中，然后这些同步双数据集可以被共同学习。</li>
<li>results: 作者发现，通过共同学习这些同步双数据集，党部可以获得更加准确的目标统计数据，尤其是在小型多样化数据集中。此外，参与更多党部时，共同学习的改进会变得更大和更一致。最后，作者发现，分享同步双数据可以帮助党部 Whose 数据包含少数群体更好地进行准确的分析。根据这些结果，作者结论，共同学习同步双数据是一种可靠的隐私保护方法，可以帮助党部学习敏感数据，即使个体数据集小或不代表整个人口。<details>
<summary>Abstract</summary>
Consider a setting where multiple parties holding sensitive data aim to collaboratively learn population level statistics, but pooling the sensitive data sets is not possible. We propose a framework in which each party shares a differentially private synthetic twin of their data. We study the feasibility of combining such synthetic twin data sets for collaborative learning on real-world health data from the UK Biobank. We discover that parties engaging in the collaborative learning via shared synthetic data obtain more accurate estimates of target statistics compared to using only their local data. This finding extends to the difficult case of small heterogeneous data sets. Furthermore, the more parties participate, the larger and more consistent the improvements become. Finally, we find that data sharing can especially help parties whose data contain underrepresented groups to perform better-adjusted analysis for said groups. Based on our results we conclude that sharing of synthetic twins is a viable method for enabling learning from sensitive data without violating privacy constraints even if individual data sets are small or do not represent the overall population well. The setting of distributed sensitive data is often a bottleneck in biomedical research, which our study shows can be alleviated with privacy-preserving collaborative learning methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>设想多个方持有敏感数据，它们想要共同学习人口级统计信息，但混合敏感数据集不可能。我们提议一个框架，每个方共享一个具有隐私保护的人工创造的数据集。我们研究将这些人工创造数据集结合在一起进行共同学习，并对UK Biobank实际数据进行研究。我们发现，通过共同学习这些人工创造数据集，各方可以获得更准确的目标统计信息，包括小型不同类型数据集。此外，越来越多的方参与，改进的效果越来越大和一致。最后，我们发现，在数据分享的情况下，特别是对于包含少数群体的数据，各方可以进行更好地调整分析。根据我们的结果，我们认为共享具有隐私保护的人工创造数据集是可以保持隐私协议的有效方法，即使个人数据集小或不代表总人口。这种设定frequently bottleneck在生物医学研究中，我们的研究表明，可以使用隐私保护的共同学习方法来缓解这种瓶颈。
</details></li>
</ul>
<hr>
<h2 id="Universal-Fuzzing-via-Large-Language-Models"><a href="#Universal-Fuzzing-via-Large-Language-Models" class="headerlink" title="Universal Fuzzing via Large Language Models"></a>Universal Fuzzing via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04748">http://arxiv.org/abs/2308.04748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, Lingming Zhang</li>
<li>for: 这篇论文的目的是为了开发一个可以针对多种输入语言和多种语言特性的整体扫描工具（Fuzzer），以找到软件系统中的漏洞和漏洞。</li>
<li>methods: 这篇论文使用了大型自然语言模型（LLM）作为输入生成和变换引擎，并提出了一种自动推荐技术来创建适合扫描的 LLM 提示。它还提出了一种基于 LLM 的扫描循环，可以逐步更新提示来生成新的扫描输入。</li>
<li>results: 在九个系统下测试，这种整体扫描技术可以对六种不同的语言（C、C++、Go、SMT2、Java 和 Python）的输入进行扫描，并且在所有六种语言中都达到了更高的覆盖率。此外，这种整体扫描技术已经发现了76个bug，其中47个已经确认了开发人员为之前未知的漏洞。<details>
<summary>Abstract</summary>
Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 76 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 47 bugs already confirmed by developers as previously unknown.
</details>
<details>
<summary>摘要</summary>
团队发现了一种新的批处理技术，即基于大语言模型（LLM）的批处理，可以覆盖各种软件系统中的漏洞和敏感性。这种技术可以覆盖各种编程语言和形式语言，如C、C++、Go、SMT2、Java和Python等，并且可以快速生成各种语言的输入数据。我们通过一种新的自动提示技术和一种基于LLM的批处理循环来实现这一点。我们在9个测试系统中进行了评估，这些测试系统接受6种不同的语言（C、C++、Go、SMT2、Java和Python）作为输入。结果表明，通过使用这种通用批处理技术，可以在所有6种语言中提高批处理覆盖率，并且已经发现了76个在广泛使用的系统中的漏洞，其中47个已经被开发者确认为新发现的漏洞。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-a-Transformer-based-network-for-a-deep-learning-seismic-processing-workflow"><a href="#Optimizing-a-Transformer-based-network-for-a-deep-learning-seismic-processing-workflow" class="headerlink" title="Optimizing a Transformer-based network for a deep learning seismic processing workflow"></a>Optimizing a Transformer-based network for a deep learning seismic processing workflow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04739">http://arxiv.org/abs/2308.04739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Randy Harsuko, Tariq Alkhalifah</li>
<li>for: 本文是一篇针对多种地震处理任务的模型，通过预训练和精度调整的训练策略来适应不同的任务。</li>
<li>methods: 本文提议对模型中的圆满位编码和自注意机制进行修改，使用相对位置编码和低级别注意矩阵来替代原始的自注意机制。</li>
<li>results: 对于处理任务，提议的修改可以提高预训练速度和精度调整任务的结果，同时减少模型的训练参数量。<details>
<summary>Abstract</summary>
StorSeismic is a recently introduced model based on the Transformer to adapt to various seismic processing tasks through its pretraining and fine-tuning training strategy. In the original implementation, StorSeismic utilized a sinusoidal positional encoding and a conventional self-attention mechanism, both borrowed from the natural language processing (NLP) applications. For seismic processing they admitted good results, but also hinted to limitations in efficiency and expressiveness. We propose modifications to these two key components, by utilizing relative positional encoding and low-rank attention matrices as replacements to the vanilla ones. The proposed changes are tested on processing tasks applied to a realistic Marmousi and offshore field data as a sequential strategy, starting from denoising, direct arrival removal, multiple attenuation, and finally root-mean-squared velocity ($V_{RMS}$) prediction for normal moveout (NMO) correction. We observe faster pretraining and competitive results on the fine-tuning tasks and, additionally, fewer parameters to train compared to the vanilla model.
</details>
<details>
<summary>摘要</summary>
史托质地震是一个最近引入的模型，基于传播者来适应多种地震处理任务。在原始实现中，史托质地震使用了正弦波 пози对编码和传统自我注意机制，这些来自自然语言处理（NLP）应用。它们在地震处理中获得了好的结果，但也表现了效率和表现力的限制。我们提出了这两个关键 комponent的修改，使用相对位对编码和低矩降低矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降矩降
</details></li>
</ul>
<hr>
<h2 id="Going-Deeper-with-Five-point-Stencil-Convolutions-for-Reaction-Diffusion-Equations"><a href="#Going-Deeper-with-Five-point-Stencil-Convolutions-for-Reaction-Diffusion-Equations" class="headerlink" title="Going Deeper with Five-point Stencil Convolutions for Reaction-Diffusion Equations"></a>Going Deeper with Five-point Stencil Convolutions for Reaction-Diffusion Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04735">http://arxiv.org/abs/2308.04735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongho Kim, Yongho Choi</li>
<li>for: 用于解决具有不同初始条件的第二阶征Diffusion类型方程的问题。</li>
<li>methods: 使用五点维度卷积神经网络（FCNNs）来训练模型参数，并使用两个连续的快照来训练FCNNs。</li>
<li>results: 对各种初始条件的热、斑蛋白和艾兰-卡ahn方程进行了测试，并显示了深度FCNNs可以保持一定的准确性，而不同于FDMs崩溃。<details>
<summary>Abstract</summary>
Physics-informed neural networks have been widely applied to partial differential equations with great success because the physics-informed loss essentially requires no observations or discretization. However, it is difficult to optimize model parameters, and these parameters must be trained for each distinct initial condition. To overcome these challenges in second-order reaction-diffusion type equations, a possible way is to use five-point stencil convolutional neural networks (FCNNs). FCNNs are trained using two consecutive snapshots, where the time step corresponds to the step size of the given snapshots. Thus, the time evolution of FCNNs depends on the time step, and the time step must satisfy its CFL condition to avoid blow-up solutions. In this work, we propose deep FCNNs that have large receptive fields to predict time evolutions with a time step larger than the threshold of the CFL condition. To evaluate our models, we consider the heat, Fisher's, and Allen-Cahn equations with diverse initial conditions. We demonstrate that deep FCNNs retain certain accuracies, in contrast to FDMs that blow up.
</details>
<details>
<summary>摘要</summary>
物理学 Informed neural networks 已经广泛应用于 partial differential equations 中，因为物理学 Informed loss 不需要观察或离散。然而，困难在优化模型参数，这些参数需要为每个特定的初始条件进行训练。为了解决这些挑战，在第二阶段反应扩散类方程中，可以使用 five-point stencil convolutional neural networks (FCNNs)。FCNNs 通过两个连续的快照，其中时间步长对应快照的步长，来训练。因此，FCNNs 的时间演化取决于时间步长，并且时间步长必须满足其 CFL 条件，以避免出现潜在的爆炸解。在这种工作中，我们提出了深度 FCNNs，它们具有大见范围，可以预测时间演化，并且时间步长大于阈值 CF 条件。为评估我们的模型，我们考虑了热、费希尔和欧兰凡方程，并使用多种初始条件。我们发现，深度 FCNNs 可以保持一定的准确性，与 FDMs 不同，后者会爆炸。
</details></li>
</ul>
<hr>
<h2 id="JEN-1-Text-Guided-Universal-Music-Generation-with-Omnidirectional-Diffusion-Models"><a href="#JEN-1-Text-Guided-Universal-Music-Generation-with-Omnidirectional-Diffusion-Models" class="headerlink" title="JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models"></a>JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04729">http://arxiv.org/abs/2308.04729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, Alex Wang</li>
<li>for: 本文旨在提出一种高精度的文本到音乐生成模型（JEN-1），用于解决文本描述下的音乐生成问题。</li>
<li>methods: 该模型基于扩散学习，结合了 autoregressive 和 non-autoregressive 训练方法，通过在上下文学习来实现多种生成任务，包括文本引导的音乐生成、音乐填充和续写。</li>
<li>results: 对比之前的方法，JEN-1 在文本音乐对齐和音乐质量方面表现出色，同时保持了计算效率。我们提供了在 <a target="_blank" rel="noopener" href="http://futureverse.com/research/jen/demos/jen1">http://futureverse.com/research/jen/demos/jen1</a> 的示例。<details>
<summary>Abstract</summary>
Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task's significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through in-context learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1's superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at http://futureverse.com/research/jen/demos/jen1
</details>
<details>
<summary>摘要</summary>
音乐生成已经吸引了深入的研究，随着深度生成模型的进步，但是将文本描述转化为音乐，也就是文本到音乐，仍然是一项挑战，因为音乐结构的复杂性和高采样率的要求。虽然这项任务的重要性，现有的生成模型却存在音乐质量、计算效率和泛化等方面的限制。这篇论文介绍了JEN-1，一种通用的高精度模型，用于文本到音乐生成。JEN-1是一种扩散模型，通过在 контексте学习，可以完成不同的生成任务，包括文本指导的音乐生成、音乐填充和续写。评估表明JEN-1的性能在文本音乐对齐和音乐质量方面胜过现有的方法，同时保持计算效率。我们的 demo 可以在 http://futureverse.com/research/jen/demos/jen1 中找到。
</details></li>
</ul>
<hr>
<h2 id="Data-Free-Model-Extraction-Attacks-in-the-Context-of-Object-Detection"><a href="#Data-Free-Model-Extraction-Attacks-in-the-Context-of-Object-Detection" class="headerlink" title="Data-Free Model Extraction Attacks in the Context of Object Detection"></a>Data-Free Model Extraction Attacks in the Context of Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05127">http://arxiv.org/abs/2308.05127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harshit Shah, Aravindhan G, Pavan Kulkarni, Yuvaraj Govidarajulu, Manojkumar Parmar</li>
<li>for: 这个论文旨在攻击机器学习模型，具体来说是通过使用特制的查询来盗取目标模型。</li>
<li>methods: 这篇论文使用的方法包括使用生成器类似于生成对抗网络，并定义损失函数和新的生成器设置来提取目标模型。</li>
<li>results: 这篇论文的结果表明，使用合理的查询可以实现高度有效的模型提取。这种模型提取方法可以应用于物体检测任务中的回归问题，并且可以支持未来机器学习模型的安全。<details>
<summary>Abstract</summary>
A significant number of machine learning models are vulnerable to model extraction attacks, which focus on stealing the models by using specially curated queries against the target model. This task is well accomplished by using part of the training data or a surrogate dataset to train a new model that mimics a target model in a white-box environment. In pragmatic situations, however, the target models are trained on private datasets that are inaccessible to the adversary. The data-free model extraction technique replaces this problem when it comes to using queries artificially curated by a generator similar to that used in Generative Adversarial Nets. We propose for the first time, to the best of our knowledge, an adversary black box attack extending to a regression problem for predicting bounding box coordinates in object detection. As part of our study, we found that defining a loss function and using a novel generator setup is one of the key aspects in extracting the target model. We find that the proposed model extraction method achieves significant results by using reasonable queries. The discovery of this object detection vulnerability will support future prospects for securing such models.
</details>
<details>
<summary>摘要</summary>
许多机器学习模型容易受到模型EXTRACTION攻击，这种攻击集中于使用特制的查询来盗取目标模型。这个任务在白盒环境中得以完美完成，只需使用目标模型的一部分训练数据或代理数据集来训练一个模仿目标模型的新模型。然而，在实际情况下，目标模型通常是基于私人数据集训练的，这些数据集不可 accessible 给敌方。我们提出了一种数据free模型EXTRACTION技术，该技术使用生成器类似于生成对抗网络中的生成器来生成训练数据。我们首次，至少知道，对目标模型进行黑盒攻击扩展到回归问题，用于预测物体检测中的 bounding box 坐标。在我们的研究中，我们发现了定义损失函数和使用新的生成器设置是EXTRACTION模型的关键方面。我们发现，我们提议的模型EXTRACTION方法可以使用合理的查询来实现显著的结果。该发现将对物体检测模型的安全提供支持。
</details></li>
</ul>
<hr>
<h2 id="Slot-Induction-via-Pre-trained-Language-Model-Probing-and-Multi-level-Contrastive-Learning"><a href="#Slot-Induction-via-Pre-trained-Language-Model-Probing-and-Multi-level-Contrastive-Learning" class="headerlink" title="Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning"></a>Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04712">http://arxiv.org/abs/2308.04712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang H. Nguyen, Chenwei Zhang, Ye Liu, Philip S. Yu</li>
<li>for: 这个论文的目的是解决任务对话系统中的自然语言理解问题，具体来说是寻找具有高端性的自然语言理解方法，以便在任务对话系统中实现竞争力强的表现。</li>
<li>methods: 该论文提出了一种基于无监督语言模型（PLM）的探索学习方法，通过利用PLM中的无监督 semantic knowledge和任务对话系统中可用的句子级意图标签信号，来适应具有不同意图的语言模型。</li>
<li>results: 该论文的实验结果表明，该方法可以在slot induction任务中取得高效的表现，并且可以与基于token级监督模型的模型相比，在两个NLU数据集上 bridge the gap。此外，该方法还可以在新意图下提高slot filling任务的表现。<details>
<summary>Abstract</summary>
Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved performance on the Slot Filling tasks.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning to exploit unsupervised semantic knowledge extracted from PLM and additional sentence-level intent label signals available from TOD. Our approach is effective in the SI task and can bridge the gap with token-level supervised models on two NLU benchmark datasets.Moreover, our SI objectives provide enhanced slot label representations, leading to improved performance on the Slot Filling task when generalized to emerging intents. Our approach has the potential to significantly reduce the amount of manual annotation required for TOD systems, making it more practical and efficient for real-world applications.
</details></li>
</ul>
<hr>
<h2 id="Generative-Perturbation-Analysis-for-Probabilistic-Black-Box-Anomaly-Attribution"><a href="#Generative-Perturbation-Analysis-for-Probabilistic-Black-Box-Anomaly-Attribution" class="headerlink" title="Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution"></a>Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04708">http://arxiv.org/abs/2308.04708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idesan/gpa">https://github.com/idesan/gpa</a></li>
<li>paper_authors: Tsuyoshi Idé, Naoki Abe</li>
<li>for: 这 paper 的目的是解释黑盒回归中的异常分布。</li>
<li>methods: 该 paper 使用了一种新的概率异常归因框架，可以计算输入变量的异常归因分布。这种框架不同于主流的 XAI 方法（解释 AI），因为它们只是解释黑盒预测而不是黑盒模型本身。</li>
<li>results: 该 paper 提出了一种可以量化异常归因分布的方法，并且可以用来解释黑盒预测中的异常分布。这种方法基于一种生成过程，可以Counter-factually bring the observed anomalous observation back to normalcy。<details>
<summary>Abstract</summary>
We address the task of probabilistic anomaly attribution in the black-box regression setting, where the goal is to compute the probability distribution of the attribution score of each input variable, given an observed anomaly. The training dataset is assumed to be unavailable. This task differs from the standard XAI (explainable AI) scenario, since we wish to explain the anomalous deviation from a black-box prediction rather than the black-box model itself.   We begin by showing that mainstream model-agnostic explanation methods, such as the Shapley values, are not suitable for this task because of their ``deviation-agnostic property.'' We then propose a novel framework for probabilistic anomaly attribution that allows us to not only compute attribution scores as the predictive mean but also quantify the uncertainty of those scores. This is done by considering a generative process for perturbations that counter-factually bring the observed anomalous observation back to normalcy. We introduce a variational Bayes algorithm for deriving the distributions of per variable attribution scores. To the best of our knowledge, this is the first probabilistic anomaly attribution framework that is free from being deviation-agnostic.
</details>
<details>
<summary>摘要</summary>
我们考虑了黑盒回归 Setting 中的概率异常归因 зада题，其目标是计算每个输入变量的归因分布，给出观察到的异常。我们假设训练集不可用。这个任务与标准 XAI（可解释 AI）场景不同，我们想解释黑盒预测的异常偏差，而不是黑盒模型本身。我们首先表明，主流的无 modelo-agnostic 解释方法，如夏普利值，不适用于这个任务，因为它们的“偏差无关性”性。我们然后提出了一种新的概率异常归因框架，允许我们不仅计算归因分布，还能量 Quantify the uncertainty of those scores。这是通过考虑一种生成过程来对 perturbations 进行 counter-factual 恢复正常 Observation 来实现的。我们提出了一种变分 Bayes 算法来 derive the distributions of per variable attribution scores。到目前为止，这是我们知道的首个不受偏差影响的概率异常归因框架。
</details></li>
</ul>
<hr>
<h2 id="Pareto-Invariant-Representation-Learning-for-Multimedia-Recommendation"><a href="#Pareto-Invariant-Representation-Learning-for-Multimedia-Recommendation" class="headerlink" title="Pareto Invariant Representation Learning for Multimedia Recommendation"></a>Pareto Invariant Representation Learning for Multimedia Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04706">http://arxiv.org/abs/2308.04706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Huang, Haoxuan Li, Qingsong Li, Chunyuan Zheng, Li Liu</li>
<li>for: 本研究旨在提高个性化推荐的精度和稳定性，帮助用户找到更加适合自己的 multimedia 内容。</li>
<li>methods: 本研究提出了一种名为 Pareto Invariant Representation Learning (PaInvRL) 的框架，它通过同时学习 invariant 表示和 variant 表示来mitigate 缺乏关联性的问题。PaInvRL 包括三个迭代执行的模块：（i）不同环境标识模块，用于反映用户-项目交互中的分布偏移；（ii）不变面生成模块，通过 Pareto-优补解决方案来学习不变面；（iii）转换模块，用于生成 variant 表示和 item-invariant 表示，以便训练一个多Modal 推荐模型，以减少缺乏关联性和保持在环境分布之间的总体化表现。</li>
<li>results: 对于 Movielens、Tiktok 和 Kwai 等三个公共 multimedia 推荐数据集，PaInvRL 与当前的推荐模型进行比较，实验结果证明 PaInvRL 在内部和跨环境学习中具有更高的精度和稳定性。<details>
<summary>Abstract</summary>
Multimedia recommendation involves personalized ranking tasks, where multimedia content is usually represented using a generic encoder. However, these generic representations introduce spurious correlations that fail to reveal users' true preferences. Existing works attempt to alleviate this problem by learning invariant representations, but overlook the balance between independent and identically distributed (IID) and out-of-distribution (OOD) generalization. In this paper, we propose a framework called Pareto Invariant Representation Learning (PaInvRL) to mitigate the impact of spurious correlations from an IID-OOD multi-objective optimization perspective, by learning invariant representations (intrinsic factors that attract user attention) and variant representations (other factors) simultaneously. Specifically, PaInvRL includes three iteratively executed modules: (i) heterogeneous identification module, which identifies the heterogeneous environments to reflect distributional shifts for user-item interactions; (ii) invariant mask generation module, which learns invariant masks based on the Pareto-optimal solutions that minimize the adaptive weighted Invariant Risk Minimization (IRM) and Empirical Risk (ERM) losses; (iii) convert module, which generates both variant representations and item-invariant representations for training a multi-modal recommendation model that mitigates spurious correlations and balances the generalization performance within and cross the environmental distributions. We compare the proposed PaInvRL with state-of-the-art recommendation models on three public multimedia recommendation datasets (Movielens, Tiktok, and Kwai), and the experimental results validate the effectiveness of PaInvRL for both within- and cross-environmental learning.
</details>
<details>
<summary>摘要</summary>
multimedia推荐 involve个人化排名任务，其中 multimedia 内容通常使用通用编码器表示。然而，这些通用表示引入了假设性的相关性， fail to reveal 用户的真实喜好。现有工作尝试通过学习不变表示来缓解这个问题，但忽视了IID和OOD总体化的平衡。在这篇论文中，我们提出了一个名为Pareto Invitable Representation Learning（PaInvRL）的框架，通过IID-OOD多对象函数优化的多元对象优化角度来缓解用户-项目交互中的假设性相关性，同时学习不变和variant表示。具体来说，PaInvRL包括以下三个iteratively执行的模块：(i) 不同环境标识模块，用于反映用户-项目交互中的分布Shift;(ii) 不变面积生成模块，通过Pareto优胜解决方案来学习不变面积，并与Adaptive Weighted Invariant Risk Minimization（IRM）和Empirical Risk（ERM）损失相对优胜;(iii) 转换模块，用于生成variant表示和item-invariant表示，以便训练一个多Modal推荐模型， mitigate 假设性相关性并保持内部和跨环境的总体化性。我们与现有的推荐模型进行比较，并在 Movielens、Tiktok 和 Kwai 三个公共 multimedia 推荐数据集上进行实验，结果证明PaInvRL在内部和跨环境学习中具有效果。
</details></li>
</ul>
<hr>
<h2 id="A-Feature-Set-of-Small-Size-for-the-PDF-Malware-Detection"><a href="#A-Feature-Set-of-Small-Size-for-the-PDF-Malware-Detection" class="headerlink" title="A Feature Set of Small Size for the PDF Malware Detection"></a>A Feature Set of Small Size for the PDF Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04704">http://arxiv.org/abs/2308.04704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Liu, Charles Nicholas</li>
<li>for: 这种研究旨在提出一个小型特征集，以便在PDF文档中探测针对性软件。</li>
<li>methods: 该研究使用六种不同的机器学习模型进行评估，并测试了一个小型特征集。</li>
<li>results: 该研究得到了99.75%的最高准确率，使用Random Forest模型。该小型特征集与当前领域中使用的大型特征集相比，具有相似的性能。<details>
<summary>Abstract</summary>
Machine learning (ML)-based malware detection systems are becoming increasingly important as malware threats increase and get more sophisticated. PDF files are often used as vectors for phishing attacks because they are widely regarded as trustworthy data resources, and are accessible across different platforms. Therefore, researchers have developed many different PDF malware detection methods. Performance in detecting PDF malware is greatly influenced by feature selection. In this research, we propose a small features set that don't require too much domain knowledge of the PDF file. We evaluate proposed features with six different machine learning models. We report the best accuracy of 99.75% when using Random Forest model. Our proposed feature set, which consists of just 12 features, is one of the most conciseness in the field of PDF malware detection. Despite its modest size, we obtain comparable results to state-of-the-art that employ a much larger set of features.
</details>
<details>
<summary>摘要</summary>
《机器学习（ML）基于的malware检测系统在面临增加的malware威胁和复杂化的情况下变得越来越重要。PDF文档经常被用为钓鱼攻击的载体，因为它们被广泛认为是可靠的数据资源，可以在不同的平台上访问。因此，研究人员已经开发出了许多不同的PDF malware检测方法。PDF malware检测性能受到特征选择的影响。在这项研究中，我们提议一个小型特征集，不需要太多领域知识。我们使用六种不同的机器学习模型进行评估。我们发现，使用Random Forest模型时的最佳准确率为99.75%。我们的提议的特征集，包含12个特征，是PDF malware检测领域中最短的一个。尽管它的规模不大，但我们可以获得与州对的结果，与使用许多更多特征的状态对照。》Note: The translation is done using Google Translate and may not be perfect. Please note that the translation is done in a simplified Chinese, if you need the translation in traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="An-Analytical-Study-of-Covid-19-Dataset-using-Graph-Based-Clustering-Algorithms"><a href="#An-Analytical-Study-of-Covid-19-Dataset-using-Graph-Based-Clustering-Algorithms" class="headerlink" title="An Analytical Study of Covid-19 Dataset using Graph-Based Clustering Algorithms"></a>An Analytical Study of Covid-19 Dataset using Graph-Based Clustering Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04697">http://arxiv.org/abs/2308.04697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mamata Das, P. J. A. Alphonse, Selvakumar K</li>
<li>for: 本研究旨在针对 COVID-19 病毒的蛋白质与蛋白质间互作（Protein-Protein Interaction，PPI）网络进行 clustering 分析，以提供更多的类别分析和疾病理解。</li>
<li>methods: 本研究使用了三种 гра图基于的 clustering 算法进行 PPI 网络的分析，包括 degree-based clustering、betweenness-based clustering 和 PageRank-based clustering。</li>
<li>results: 本研究发现，这些 clustering 算法可以对 PPI 网络进行有效的分类，并提供了一些有用的类别分析和疾病理解。<details>
<summary>Abstract</summary>
Corona VIrus Disease abbreviated as COVID-19 is a novel virus which is initially identified in Wuhan of China in December of 2019 and now this deadly disease has spread all over the world. According to World Health Organization (WHO), a total of 3,124,905 people died from 2019 to 2021, April. In this case, many methods, AI base techniques, and machine learning algorithms have been researched and are being used to save people from this pandemic. The SARS-CoV and the 2019-nCoV, SARS-CoV-2 virus invade our bodies, causing some differences in the structure of cell proteins. Protein-protein interaction (PPI) is an essential process in our cells and plays a very important role in the development of medicines and gives ideas about the disease. In this study, we performed clustering on PPI networks generated from 92 genes of the Covi-19 dataset. We have used three graph-based clustering algorithms to give intuition to the analysis of clusters.
</details>
<details>
<summary>摘要</summary>
新型冠状病毒病（COVID-19）是一种新型病毒，于2019年12月在中国武汉首次被发现，现已在全球蔓延。根据世界卫生组织（WHO）的统计，2019年至2021年4月，全球共有3,124,905人死亡。在这种大流行中，许多方法、AI基础技术和机器学习算法已经被研究并应用，以拯救人们。SARS-CoV和2019-nCoV病毒会入侵我们的体内，导致细胞蛋白结构的变化。蛋白蛋白相互作用（PPI）是我们细胞中的一项重要过程，对于药物的开发和疾病的研究具有非常重要的意义。在本研究中，我们对COVI-19数据集中的92个基因生成的PPI网络进行了划分。我们使用了三种图基的划分算法，以便更好地了解划分结果。
</details></li>
</ul>
<hr>
<h2 id="Explainable-AI-in-Orthopedics-Challenges-Opportunities-and-Prospects"><a href="#Explainable-AI-in-Orthopedics-Challenges-Opportunities-and-Prospects" class="headerlink" title="Explainable AI in Orthopedics: Challenges, Opportunities, and Prospects"></a>Explainable AI in Orthopedics: Challenges, Opportunities, and Prospects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04696">http://arxiv.org/abs/2308.04696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soheyla Amirian, Luke A. Carlson, Matthew F. Gong, Ines Lohse, Kurt R. Weiss, Johannes F. Plate, Ahmad P. Tafti</li>
<li>for: 本研究旨在解决医疗机器学习模型中的解释性和可解释性问题，以便临床医生、外科医生和患者能够理解AI模型中的贡献因素。</li>
<li>methods: 本研究使用了多种方法来解决解释性和可解释性问题，包括开发可解释的AI模型和算法，以及与 клиниче专业人员和管理机构之间的合作。</li>
<li>results: 本研究发现了一些关键的挑战和机遇，包括解释性和可解释性问题的解决方法，以及在Orthopedics中应用XAI的标准和指南的设立。<details>
<summary>Abstract</summary>
While artificial intelligence (AI) has made many successful applications in various domains, its adoption in healthcare lags a little bit behind other high-stakes settings. Several factors contribute to this slower uptake, including regulatory frameworks, patient privacy concerns, and data heterogeneity. However, one significant challenge that impedes the implementation of AI in healthcare, particularly in orthopedics, is the lack of explainability and interpretability around AI models. Addressing the challenge of explainable AI (XAI) in orthopedics requires developing AI models and algorithms that prioritize transparency and interpretability, allowing clinicians, surgeons, and patients to understand the contributing factors behind any AI-powered predictive or descriptive models. The current contribution outlines several key challenges and opportunities that manifest in XAI in orthopedic practice. This work emphasizes the need for interdisciplinary collaborations between AI practitioners, orthopedic specialists, and regulatory entities to establish standards and guidelines for the adoption of XAI in orthopedics.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在不同领域取得了许多成功应用，但在医疗领域的采纳相对落后一些。这可能是由多种因素引起的，如法规框架、医疗隐私问题和数据多样性。然而，在骨科领域中，AI的实现受到了解释和解读模型的缺乏的困难。为了解决骨科中的解释AI（XAI）问题，需要开发可透明和可解释的AI模型和算法，让临床医生、骨科专家和患者可以理解AI预测或描述模型中的决定因素。本贡献阐述了骨科中XAI问题的多个挑战和机遇，强调了在AI实践中跨学科合作的重要性，包括AI实践者、骨科专家和法规机构，以设立XAI的标准和指南。
</details></li>
</ul>
<hr>
<h2 id="Finite-Element-Operator-Network-for-Solving-Parametric-PDEs"><a href="#Finite-Element-Operator-Network-for-Solving-Parametric-PDEs" class="headerlink" title="Finite Element Operator Network for Solving Parametric PDEs"></a>Finite Element Operator Network for Solving Parametric PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04690">http://arxiv.org/abs/2308.04690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jae Yong Lee, Seungchan Ko, Youngjoon Hong</li>
<li>for: 解决 Parametric Partial Differential Equations (PDEs) 的计算问题，提供一种基于深度学习的 Finite Element Operator Network (FEONet) 方法。</li>
<li>methods: 使用 Finite Element Method 和深度学习结合，解决 Parametric PDEs 问题，不需要输入输出对应的训练数据。</li>
<li>results: 在一些标准测试问题上，FEONet 方法比现有的状态艺术方法更高精度、更好的泛化能力和计算灵活性。FEONet 框架可以应用于各种领域，其中 PDEs 扮演着重要的计算模型角色。<details>
<summary>Abstract</summary>
Partial differential equations (PDEs) underlie our understanding and prediction of natural phenomena across numerous fields, including physics, engineering, and finance. However, solving parametric PDEs is a complex task that necessitates efficient numerical methods. In this paper, we propose a novel approach for solving parametric PDEs using a Finite Element Operator Network (FEONet). Our proposed method leverages the power of deep learning in conjunction with traditional numerical methods, specifically the finite element method, to solve parametric PDEs in the absence of any paired input-output training data. We demonstrate the effectiveness of our approach on several benchmark problems and show that it outperforms existing state-of-the-art methods in terms of accuracy, generalization, and computational flexibility. Our FEONet framework shows potential for application in various fields where PDEs play a crucial role in modeling complex domains with diverse boundary conditions and singular behavior. Furthermore, we provide theoretical convergence analysis to support our approach, utilizing finite element approximation in numerical analysis.
</details>
<details>
<summary>摘要</summary>
“偏微分方程（PDEs）在许多领域中起着关键作用，包括物理、工程和金融等。但是解 parametric PDEs 是一项复杂的任务，需要有效的数值方法。在这篇论文中，我们提出了一种新的方法，使用 Finite Element Operator Network（FEONet）来解 parametric PDEs。我们的提议方法利用了深度学习和传统的数值方法，具体来说是finite element方法，来解 parametric PDEs 在没有任何输入输出培训数据的情况下。我们在一些标准问题上进行了评估，并证明了我们的方法在准确性、泛化和计算灵活性等方面都比现有的状态 искусственный智能方法更高。我们的 FEONet 框架在不同的领域中有潜在的应用，其中 PDEs 在模型复杂领域中的各种边界条件和特殊行为中扮演着关键角色。此外，我们还提供了理论的收敛分析，使用finite element aproximation进行数值分析。”
</details></li>
</ul>
<hr>
<h2 id="Two-Novel-Approaches-to-Detect-Community-A-Case-Study-of-Omicron-Lineage-Variants-PPI-Network"><a href="#Two-Novel-Approaches-to-Detect-Community-A-Case-Study-of-Omicron-Lineage-Variants-PPI-Network" class="headerlink" title="Two Novel Approaches to Detect Community: A Case Study of Omicron Lineage Variants PPI Network"></a>Two Novel Approaches to Detect Community: A Case Study of Omicron Lineage Variants PPI Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05125">http://arxiv.org/abs/2308.05125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mamata Das, Selvakumar K., P. J. A. Alphonse</li>
<li>for: 这项研究的目的是揭示变异型B.1.1.529病毒（Omicron病毒）中的社区结构，以便更好地理解这种病毒的生物学机制。</li>
<li>methods: 该研究使用了两种新的算法（ABCDE和ALCDE）和四种已知的算法（ Girvan-Newman、Louvain、Leiden和Label Propagation）来揭示变异型B.1.1.529病毒的社区结构。</li>
<li>results: 研究发现，使用不同算法可以揭示出变异型B.1.1.529病毒的不同社区结构，并且对这些社区进行了全面的比较和分析。<details>
<summary>Abstract</summary>
The capacity to identify and analyze protein-protein interactions, along with their internal modular organization, plays a crucial role in comprehending the intricate mechanisms underlying biological processes at the molecular level. We can learn a lot about the structure and dynamics of these interactions by using network analysis. We can improve our understanding of the biological roots of disease pathogenesis by recognizing network communities. This knowledge, in turn, holds significant potential for driving advancements in drug discovery and facilitating personalized medicine approaches for disease treatment. In this study, we aimed to uncover the communities within the variant B.1.1.529 (Omicron virus) using two proposed novel algorithm (ABCDE and ALCDE) and four widely recognized algorithms: Girvan-Newman, Louvain, Leiden, and Label Propagation algorithm. Each of these algorithms has established prominence in the field and offers unique perspectives on identifying communities within complex networks. We also compare the networks by the global properties, statistic summary, subgraph count, graphlet and validate by the modulaity. By employing these approaches, we sought to gain deeper insights into the structural organization and interconnections present within the Omicron virus network.
</details>
<details>
<summary>摘要</summary>
“蛋白质-蛋白质互作的能力和内部模块结构对于理解生物过程的分子水平机制具有重要作用。我们可以通过网络分析来了解这些互作的结构和动态。通过识别网络社区，我们可以更好地理解疾病发生的生物基础，从而推动药物发现和个性化医疗方法的发展。在这项研究中，我们使用了两种新的算法（ABCDE和ALCDE）和四种广泛使用的算法（吉万-恩文、奥尔班-劳伦、莱顿和标签卷）来揭示 variant B.1.1.529（奥米克隆病毒）中的社区。每种算法都有在领域中的卓越表现，并提供了不同的视角来识别复杂网络中的社区。我们还对这些网络进行了全球性质、统计摘要、子图计数、图лет和验证性的比较，以获得更深入的理解奥米克隆病毒网络中的结构组织和互连关系。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. Traditional Chinese is used in Taiwan and other countries, and the translation may be slightly different in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="TBIN-Modeling-Long-Textual-Behavior-Data-for-CTR-Prediction"><a href="#TBIN-Modeling-Long-Textual-Behavior-Data-for-CTR-Prediction" class="headerlink" title="TBIN: Modeling Long Textual Behavior Data for CTR Prediction"></a>TBIN: Modeling Long Textual Behavior Data for CTR Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08483">http://arxiv.org/abs/2308.08483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuwei Chen, Xiang Li, Jian Dong, Jin Zhang, Yongkang Wang, Xingxing Wang</li>
<li>for: 预测点击率 (CTR) 在推荐中发挥关键作用，文中提出了一种基于文本的用户行为数据组织方法，使用语言模型 (LM) 理解用户兴趣的含义。</li>
<li>methods: 文中提出的方法是文本行为数据的本地性敏感哈希算法和偏移量基于的自注意力，解决了自注意力计算量的问题，并使用多个用户多个兴趣的动态活化。</li>
<li>results: 实验结果表明，文中提出的TBIN方法在真实世界的食品推荐平台上获得了显著的效果。<details>
<summary>Abstract</summary>
Click-through rate (CTR) prediction plays a pivotal role in the success of recommendations. Inspired by the recent thriving of language models (LMs), a surge of works improve prediction by organizing user behavior data in a \textbf{textual} format and using LMs to understand user interest at a semantic level. While promising, these works have to truncate the textual data to reduce the quadratic computational overhead of self-attention in LMs. However, it has been studied that long user behavior data can significantly benefit CTR prediction. In addition, these works typically condense user diverse interests into a single feature vector, which hinders the expressive capability of the model. In this paper, we propose a \textbf{T}extual \textbf{B}ehavior-based \textbf{I}nterest Chunking \textbf{N}etwork (TBIN), which tackles the above limitations by combining an efficient locality-sensitive hashing algorithm and a shifted chunk-based self-attention. The resulting user diverse interests are dynamically activated, producing user interest representation towards the target item. Finally, the results of both offline and online experiments on real-world food recommendation platform demonstrate the effectiveness of TBIN.
</details>
<details>
<summary>摘要</summary>
点阅率（CTR）预测在推荐中扮演重要的角色。受最近语言模型（LM）的盛行启发，一些工作将用户行为数据 format 为文本，并使用 LM 理解用户的 semantic 层次。 although promising, these works have to truncate the textual data to reduce the quadratic computational overhead of self-attention in LMs. However, it has been studied that long user behavior data can significantly benefit CTR prediction. In addition, these works typically condense user diverse interests into a single feature vector, which hinders the expressive capability of the model.在本文中，我们提出了 Textual Behavior-based Interest Chunking Network (TBIN)，以解决以下限制： combines an efficient locality-sensitive hashing algorithm and a shifted chunk-based self-attention. The resulting user diverse interests are dynamically activated, producing user interest representation towards the target item. Finally, the results of both offline and online experiments on real-world food recommendation platform demonstrate the effectiveness of TBIN.
</details></li>
</ul>
<hr>
<h2 id="A-General-Implicit-Framework-for-Fast-NeRF-Composition-and-Rendering"><a href="#A-General-Implicit-Framework-for-Fast-NeRF-Composition-and-Rendering" class="headerlink" title="A General Implicit Framework for Fast NeRF Composition and Rendering"></a>A General Implicit Framework for Fast NeRF Composition and Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04669">http://arxiv.org/abs/2308.04669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Gao, Ziyi Yang, Yunlu Zhao, Yuxiang Sun, Xiaogang Jin, Changqing Zou</li>
<li>For: 该论文旨在提供一种通用的隐式管道，以快速组合NeRF对象。* Methods: 该方法使用神经雨花场（Neural Depth Fields, NeDF）来快速确定物体之间的空间关系，并通过神经网络来加速NeRF的查询。* Results: 该方法可以快速和交互地组合NeRF对象，同时也可以作为许多现有NeRF作品的预览插件。<details>
<summary>Abstract</summary>
A variety of Neural Radiance Fields (NeRF) methods have recently achieved remarkable success in high render speed. However, current accelerating methods are specialized and incompatible with various implicit methods, preventing real-time composition over various types of NeRF works. Because NeRF relies on sampling along rays, it is possible to provide general guidance for acceleration. To that end, we propose a general implicit pipeline for composing NeRF objects quickly. Our method enables the casting of dynamic shadows within or between objects using analytical light sources while allowing multiple NeRF objects to be seamlessly placed and rendered together with any arbitrary rigid transformations. Mainly, our work introduces a new surface representation known as Neural Depth Fields (NeDF) that quickly determines the spatial relationship between objects by allowing direct intersection computation between rays and implicit surfaces. It leverages an intersection neural network to query NeRF for acceleration instead of depending on an explicit spatial structure.Our proposed method is the first to enable both the progressive and interactive composition of NeRF objects. Additionally, it also serves as a previewing plugin for a range of existing NeRF works.
</details>
<details>
<summary>摘要</summary>
各种神经辐射场（NeRF）方法在最近几年得到了非常出色的成果，但现有的加速方法受限于特定的隐式方法，不兼容多种NeRF工作，阻碍了实时组合。因为NeRF通过辐射进行采样，因此可以提供一般的指导方针。为了实现这一目标，我们提出了一个通用的隐式管道，用于快速组合NeRF对象。我们的方法允许在动态阴影中投射影响对象，并允许多个NeRF对象在任意旋转变换下准确地放置和渲染。主要，我们的工作引入了一种新的表面表示方法，即神经深度场（NeDF），它快速确定对象之间的空间关系，并允许直接计算辐射与隐式表面的交点。它利用一个交叉神经网络来询问NeRF的加速而不是依赖于显式空间结构。我们的提议的方法是首个允许NeRF对象进行进度式和交互式组合，同时也可以作为许多现有NeRF作品的预览插件。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-lung-cancer-subtypes-on-CT-images-with-synthetic-pathological-priors"><a href="#Classification-of-lung-cancer-subtypes-on-CT-images-with-synthetic-pathological-priors" class="headerlink" title="Classification of lung cancer subtypes on CT images with synthetic pathological priors"></a>Classification of lung cancer subtypes on CT images with synthetic pathological priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04663">http://arxiv.org/abs/2308.04663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Zhu, Yuan Jin, Gege Ma, Geng Chen, Jan Egger, Shaoting Zhang, Dimitris N. Metaxas</li>
<li>for: 这个论文的目的是为了精准地分类肺癌不同亚型，以便进行跟踪治疗和诊断管理。</li>
<li>methods: 该论文提出了一种自生成混合特征网络（SGHF-Net），用于从计算机 Tomatoes（CT）图像中准确地分类肺癌亚型。该模型基于 Studies showing that there are cross-scale associations between the same case’s CT images and its pathological images， therefore, an innovative pathological feature synthetic module (PFSM) was developed to quantitatively map cross-modality associations through deep neural networks, and derive the “gold standard” information contained in the corresponding pathological images from CT images.</li>
<li>results: 实验结果表明，提出的模型在一个大规模多中心数据集（829个例）上表现出了明显的优势，与一系列state-of-the-art（SOTA）分类模型进行比较，其准确性、区域 beneath the curve（AUC）和F1得分均有显著提高。<details>
<summary>Abstract</summary>
The accurate diagnosis on pathological subtypes for lung cancer is of significant importance for the follow-up treatments and prognosis managements. In this paper, we propose self-generating hybrid feature network (SGHF-Net) for accurately classifying lung cancer subtypes on computed tomography (CT) images. Inspired by studies stating that cross-scale associations exist in the image patterns between the same case's CT images and its pathological images, we innovatively developed a pathological feature synthetic module (PFSM), which quantitatively maps cross-modality associations through deep neural networks, to derive the "gold standard" information contained in the corresponding pathological images from CT images. Additionally, we designed a radiological feature extraction module (RFEM) to directly acquire CT image information and integrated it with the pathological priors under an effective feature fusion framework, enabling the entire classification model to generate more indicative and specific pathologically related features and eventually output more accurate predictions. The superiority of the proposed model lies in its ability to self-generate hybrid features that contain multi-modality image information based on a single-modality input. To evaluate the effectiveness, adaptability, and generalization ability of our model, we performed extensive experiments on a large-scale multi-center dataset (i.e., 829 cases from three hospitals) to compare our model and a series of state-of-the-art (SOTA) classification models. The experimental results demonstrated the superiority of our model for lung cancer subtypes classification with significant accuracy improvements in terms of accuracy (ACC), area under the curve (AUC), and F1 score.
</details>
<details>
<summary>摘要</summary>
准确诊断肺癌分型对跟进治疗和 прогности治理有着重要的意义。在这篇论文中，我们提出了自动生成混合特征网络（SGHF-Net），用于精准地分类肺癌分型的 computed tomography（CT）图像。受到 Studies 表明跨模态关系存在在图像模式中的想法，我们创新地开发了 pathological feature synthetic module（PFSM），通过深度神经网络来量化跨模态关系，从 CT 图像中提取“标准”的 pathological 信息。此外，我们还设计了 radiological feature extraction module（RFEM），直接从 CT 图像中提取信息，并将其与 pathological 先验 fusion 在一个有效的特征融合框架中，使整个分类模型能够生成更指示和特定的 pathologically 相关特征，并最终输出更高精度的预测。我们的模型的优势在于它可以自动生成混合特征，其中包含多modal 图像信息，基于单 modal 输入。为了评估我们的模型的效果、适应性和普适性，我们在多中心 dataset（i.e., 829 例）上进行了广泛的实验，与一系列 state-of-the-art（SOTA）分类模型进行比较。实验结果表明，我们的模型在肺癌分型方面具有显著的高精度性，包括准确率（ACC）、抛物线下的曲线（AUC）和 F1 分数等指标上的显著提高。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Bayesian-Optimization-with-Deep-Kernel-Learning-and-Transformer-Pre-trained-on-Multiple-Heterogeneous-Datasets"><a href="#Efficient-Bayesian-Optimization-with-Deep-Kernel-Learning-and-Transformer-Pre-trained-on-Multiple-Heterogeneous-Datasets" class="headerlink" title="Efficient Bayesian Optimization with Deep Kernel Learning and Transformer Pre-trained on Multiple Heterogeneous Datasets"></a>Efficient Bayesian Optimization with Deep Kernel Learning and Transformer Pre-trained on Multiple Heterogeneous Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04660">http://arxiv.org/abs/2308.04660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlong Lyu, Shoubo Hu, Jie Chuai, Zhitang Chen</li>
<li>for: 这paper是为了提高黑盒优化问题中的优化效率而写的。</li>
<li>methods: 这paper使用了一种简单的预训练策略，使用Transformer编码器learned的深度特征来定义GPkernel，并提供了一种简单 yet effective的混合 initializationestrategy来加速新任务的整合。</li>
<li>results:  experiments表明，这paper提出的预训练和传输BO策略比现有方法更高效。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) is widely adopted in black-box optimization problems and it relies on a surrogate model to approximate the black-box response function. With the increasing number of black-box optimization tasks solved and even more to solve, the ability to learn from multiple prior tasks to jointly pre-train a surrogate model is long-awaited to further boost optimization efficiency. In this paper, we propose a simple approach to pre-train a surrogate, which is a Gaussian process (GP) with a kernel defined on deep features learned from a Transformer-based encoder, using datasets from prior tasks with possibly heterogeneous input spaces. In addition, we provide a simple yet effective mix-up initialization strategy for input tokens corresponding to unseen input variables and therefore accelerate new tasks' convergence. Experiments on both synthetic and real benchmark problems demonstrate the effectiveness of our proposed pre-training and transfer BO strategy over existing methods.
</details>
<details>
<summary>摘要</summary>
bayesian 优化 (BO) 广泛应用于黑盒优化问题中，它基于一个代理函数来近似黑盒响应函数。随着黑盒优化任务的数量不断增加，并且还有更多的任务需要解决，因此有必要将多个先前任务的知识共享以提高优化效率。在这篇论文中，我们提议一种简单的预训练方法，使用Transformer基于encoder学习的深度特征来定义GP的核函数。此外，我们还提供了一种简单 yet effective的混合初始化策略，以便快速启动新任务的整合。实验表明，我们提议的预训练和传递BO策略比既有方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-performance-of-deep-learning-based-models-for-prostate-cancer-segmentation-using-uncertainty-scores"><a href="#Assessing-the-performance-of-deep-learning-based-models-for-prostate-cancer-segmentation-using-uncertainty-scores" class="headerlink" title="Assessing the performance of deep learning-based models for prostate cancer segmentation using uncertainty scores"></a>Assessing the performance of deep learning-based models for prostate cancer segmentation using uncertainty scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04653">http://arxiv.org/abs/2308.04653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Cesar Quihui-Rubio, Daniel Flores-Araiza, Gilberto Ochoa-Ruiz, Miguel Gonzalez-Mendoza, Christian Mata</li>
<li>for: 这个研究旨在比较深度学习方法用于肠细胞分 segmentation和不确定度评估的肠细胞MRI图像。目标是改进肠癌检测和诊断的工作流程。</li>
<li>methods: 这个研究使用了七种不同的U-Net基建 Architecture，其中包括了Monte-Carlo dropout的增强。这些模型用于自动分类肠细胞中心区、周边区、过渡区和肿瘤，并且计算了不确定度。</li>
<li>results: 研究发现，Attention R2U-Net是最高效的模型，其中IoU平均值为76.3%，DSC平均值为85%。此外，Attention R2U-Net在过渡区和肿瘤边界处的不确定度值最低。<details>
<summary>Abstract</summary>
This study focuses on comparing deep learning methods for the segmentation and quantification of uncertainty in prostate segmentation from MRI images. The aim is to improve the workflow of prostate cancer detection and diagnosis. Seven different U-Net-based architectures, augmented with Monte-Carlo dropout, are evaluated for automatic segmentation of the central zone, peripheral zone, transition zone, and tumor, with uncertainty estimation. The top-performing model in this study is the Attention R2U-Net, achieving a mean Intersection over Union (IoU) of 76.3% and Dice Similarity Coefficient (DSC) of 85% for segmenting all zones. Additionally, Attention R2U-Net exhibits the lowest uncertainty values, particularly in the boundaries of the transition zone and tumor, when compared to the other models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Metric-Learning-for-the-Hemodynamics-Inference-with-Electrocardiogram-Signals"><a href="#Deep-Metric-Learning-for-the-Hemodynamics-Inference-with-Electrocardiogram-Signals" class="headerlink" title="Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals"></a>Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04650">http://arxiv.org/abs/2308.04650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyewon Jeong, Collin M. Stultz, Marzyeh Ghassemi</li>
<li>For: 这种研究旨在提出一种非侵入性的心脏压力测量方法，以便诊断和治疗心力衰竭病人的诊断和治疗预测。* Methods: 这种方法使用深度度量学习（DML），并提出了一种自适应DML方法，通过距离基本挖掘来提高模型的性能。* Results: 研究发现，使用ECG数据进行自适应DML模型训练，可以提高心脏压力的预测性能，并且这种模型在不同的患者群体中表现良好，即使某些患者群体在数据集中具有较少的表达。<details>
<summary>Abstract</summary>
Heart failure is a debilitating condition that affects millions of people worldwide and has a significant impact on their quality of life and mortality rates. An objective assessment of cardiac pressures remains an important method for the diagnosis and treatment prognostication for patients with heart failure. Although cardiac catheterization is the gold standard for estimating central hemodynamic pressures, it is an invasive procedure that carries inherent risks, making it a potentially dangerous procedure for some patients. Approaches that leverage non-invasive signals - such as electrocardiogram (ECG) - have the promise to make the routine estimation of cardiac pressures feasible in both inpatient and outpatient settings. Prior models trained to estimate intracardiac pressures (e.g., mean pulmonary capillary wedge pressure (mPCWP)) in a supervised fashion have shown good discriminatory ability but have been limited to the labeled dataset from the heart failure cohort. To address this issue and build a robust representation, we apply deep metric learning (DML) and propose a novel self-supervised DML with distance-based mining that improves the performance of a model with limited labels. We use a dataset that contains over 5.4 million ECGs without concomitant central pressure labels to pre-train a self-supervised DML model which showed improved classification of elevated mPCWP compared to self-supervised contrastive baselines. Additionally, the supervised DML model that is using ECGs with access to 8,172 mPCWP labels demonstrated significantly better performance on the mPCWP regression task compared to the supervised baseline. Moreover, our data suggest that DML yields models that are performant across patient subgroups, even when some patient subgroups are under-represented in the dataset. Our code is available at https://github.com/mandiehyewon/ssldml
</details>
<details>
<summary>摘要</summary>
心力衰竭是一种严重的疾病，对全球多 millions of people 有很大的影响，影响他们的生活质量和死亡率。对心力衰竭患者的诊断和治疗预测，对心脏压力的 объектив评估仍然是非常重要。虽然心脏导管是诊断中心 hemodynamic pressures 的标准方法，但它是一种侵入性的过程，具有内生的风险，因此对某些患者来说可能是危险的。使用非侵入式信号 - 如电cardiogram (ECG) - 的方法可以使 Routine estimation of cardiac pressures 在医院和家庭设置中变得可能。先前的模型，通过supervised fashion 训练来估计 intracardiac pressures (e.g., mean pulmonary capillary wedge pressure (mPCWP)) 的表现能力很好，但它们受限于心衰竭 cohort 的标签数据集。为了解决这个问题并建立一个 Robust 的表现，我们运用 deep metric learning (DML) 和一种新的距离基本的自我监督 DML，可以提高具有有限标签的模型的性能。我们使用了包含超过 5.4 万个 ECG 的数据集，不含中心压力标签，来预训练一个自我监督 DML 模型，该模型在提高高 mPCWP 的分类性能方面表现出色，而且在对比自我监督对比的基线下，表现更好。此外，我们使用 ECG 和访问 8,172 个 mPCWP 标签， trains 一个 supervised DML 模型，其在 mPCWP 回归任务中表现出色，并且在不同患者 subgroup 中表现也很好。我们的代码可以在 https://github.com/mandiehyewon/ssldml 中找到。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Optimization-Performance-A-Novel-Hybridization-of-Gaussian-Crunching-Search-and-Powell’s-Method-for-Derivative-Free-Optimization"><a href="#Enhancing-Optimization-Performance-A-Novel-Hybridization-of-Gaussian-Crunching-Search-and-Powell’s-Method-for-Derivative-Free-Optimization" class="headerlink" title="Enhancing Optimization Performance: A Novel Hybridization of Gaussian Crunching Search and Powell’s Method for Derivative-Free Optimization"></a>Enhancing Optimization Performance: A Novel Hybridization of Gaussian Crunching Search and Powell’s Method for Derivative-Free Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04649">http://arxiv.org/abs/2308.04649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benny Wong</li>
<li>for: 优化复杂系统的最优解和找到Global Minimum</li>
<li>methods: hybridization of Gaussian Crunching Search (GCS) and Powell’s Method for derivative-free optimization</li>
<li>results: 显著提高优化性能，同时保留各方法的优点<details>
<summary>Abstract</summary>
This research paper presents a novel approach to enhance optimization performance through the hybridization of Gaussian Crunching Search (GCS) and Powell's Method for derivative-free optimization. While GCS has shown promise in overcoming challenges faced by traditional derivative-free optimization methods [1], it may not always excel in finding the local minimum. On the other hand, some traditional methods may have better performance in this regard. However, GCS demonstrates its strength in escaping the trap of local minima and approaching the global minima. Through experimentation, we discovered that by combining GCS with certain traditional derivative-free optimization methods, we can significantly boost performance while retaining the respective advantages of each method. This hybrid approach opens up new possibilities for optimizing complex systems and finding optimal solutions in a range of applications.
</details>
<details>
<summary>摘要</summary>
Note:* "GCS" is translated as " Gaussian Crunching Search" (格aussian逼擦搜索)* "Powell's Method" is translated as " Powell 方法" (波尔方法)* "derivative-free optimization" is translated as "无导数优化" (无导数优化)* "local minimum" is translated as "本地最小值" (本地最小值)* "global minima" is translated as "全球最小值" (全球最小值)
</details></li>
</ul>
<hr>
<h2 id="Sparse-Binary-Transformers-for-Multivariate-Time-Series-Modeling"><a href="#Sparse-Binary-Transformers-for-Multivariate-Time-Series-Modeling" class="headerlink" title="Sparse Binary Transformers for Multivariate Time Series Modeling"></a>Sparse Binary Transformers for Multivariate Time Series Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04637">http://arxiv.org/abs/2308.04637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt Gorbett, Hossein Shirazi, Indrakshi Ray</li>
<li>for: 应用于多变量时间序问题的简洁神经网络模型</li>
<li>methods: 使用稀疏和二进制权重的 transformer 模型</li>
<li>results: 在三个时间序学习任务中取得了比较出色的结果：分类、异常检测和单步预测<details>
<summary>Abstract</summary>
Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attention mask to allow computation only at the current time step. Together, each compression technique and attention modification substantially reduces the number of non-zero operations necessary in the Transformer. We measure the computational savings of our approach over a range of metrics including parameter count, bit size, and floating point operation (FLOPs) count, showing up to a 53x reduction in storage size and up to 10.5x reduction in FLOPs.
</details>
<details>
<summary>摘要</summary>
压缩神经网络（Compressed Neural Networks）有可能在新的应用程序和较小的计算环境中实现深度学习。然而，了解这些模型在哪些学习任务中能够成功是不很了解。在这项工作中，我们使用稀疏和二进制权重的转换器（Transformers）来解决多变量时间序列问题，并证明了这些轻量级模型可以与同结构的精密浮点数Transformers具有相同的准确率。我们的模型在三个时间序列学习任务中表现良好：分类、异常检测和单步预测。此外，为了减少转换器的计算复杂度，我们应用了两种修改，其中一种是在分类任务中采用固定面积来压缩查询、键和值活动，另一种是在预测和异常检测任务中，通过在当前时间步计算的注意力掩码来减少计算量。总的来说，我们的方法可以减少参数数量、位数和浮点运算（FLOPs）数量，并且可以达到最多53倍的存储大小减少和10.5倍的FLOPs减少。
</details></li>
</ul>
<hr>
<h2 id="Multiclass-Online-Learnability-under-Bandit-Feedback"><a href="#Multiclass-Online-Learnability-under-Bandit-Feedback" class="headerlink" title="Multiclass Online Learnability under Bandit Feedback"></a>Multiclass Online Learnability under Bandit Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04620">http://arxiv.org/abs/2308.04620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ananth Raman, Vinod Raman, Unique Subedi, Ambuj Tewari</li>
<li>for: online multiclass classification under bandit feedback</li>
<li>methods: extend the results of (daniely2013price) and show the finiteness of the Bandit Littlestone dimension is necessary and sufficient for bandit online multiclass learnability</li>
<li>results: complement the recent work by (hanneke2023multiclass) who show that the Littlestone dimension characterizes online multiclass learnability in the full-information setting when the label space is unboundedHere’s the format you requested:</li>
<li>for: online multiclass classification under bandit feedback</li>
<li>methods: 扩展daniely2013price的结果，显示bandit online multiclass学习可行性需要和充分条件是bandit littlestone dimension的Finite-ness</li>
<li>results: 补充hanneke2023multiclass的研究，显示Littlestone dimension caracterizes online multiclass学习在全信息设置下的label space是无限的I hope this helps!<details>
<summary>Abstract</summary>
We study online multiclass classification under bandit feedback. We extend the results of (daniely2013price) by showing that the finiteness of the Bandit Littlestone dimension is necessary and sufficient for bandit online multiclass learnability even when the label space is unbounded. Our result complements the recent work by (hanneke2023multiclass) who show that the Littlestone dimension characterizes online multiclass learnability in the full-information setting when the label space is unbounded.
</details>
<details>
<summary>摘要</summary>
我们研究在带标签反馈下进行在线多类分类。我们将daniely2013price的结果推广到无限个标签空间下，证明了带标签Littlestone维度的有限性是在线多类学习的必要和充分条件。我们的结果与hanneke2023multiclass的最近研究相 complement，他们在全信息设置下证明了Littlestone维度 caracterizes在线多类学习。
</details></li>
</ul>
<hr>
<h2 id="Improved-Activation-Clipping-for-Universal-Backdoor-Mitigation-and-Test-Time-Detection"><a href="#Improved-Activation-Clipping-for-Universal-Backdoor-Mitigation-and-Test-Time-Detection" class="headerlink" title="Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection"></a>Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04617">http://arxiv.org/abs/2308.04617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanghangpsu/mmac">https://github.com/wanghangpsu/mmac</a></li>
<li>paper_authors: Hang Wang, Zhen Xiang, David J. Miller, George Kesidis</li>
<li>for: 防止深度神经网络受到后门攻击 (Trojan)，攻击者在训练集中植入后门触发器，让神经网络在测试时将触发器分类到攻击者所指定的目标类。</li>
<li>methods: 使用后处理剪辑方法来mitigate backdoor攻击，通过在小量的净样集上学习内层活动的约束来限制内层活动的范围。</li>
<li>results: 与对手方法相比，提出了一种新的 activation bound choosing 方法，可以更好地防止后门攻击，同时具有强大的鲁棒性，可以抵御 adaptive 攻击、X2X 攻击和不同的数据集。<details>
<summary>Abstract</summary>
Deep neural networks are vulnerable to backdoor attacks (Trojans), where an attacker poisons the training set with backdoor triggers so that the neural network learns to classify test-time triggers to the attacker's designated target class. Recent work shows that backdoor poisoning induces over-fitting (abnormally large activations) in the attacked model, which motivates a general, post-training clipping method for backdoor mitigation, i.e., with bounds on internal-layer activations learned using a small set of clean samples. We devise a new such approach, choosing the activation bounds to explicitly limit classification margins. This method gives superior performance against peer methods for CIFAR-10 image classification. We also show that this method has strong robustness against adaptive attacks, X2X attacks, and on different datasets. Finally, we demonstrate a method extension for test-time detection and correction based on the output differences between the original and activation-bounded networks. The code of our method is online available.
</details>
<details>
<summary>摘要</summary>
深度神经网络容易受到后门攻击（Trojan），攻击者在训练集中杀死后门触发器，使神经网络在测试时通过特定目标类划分测试触发器。最近的研究表明，后门毒害导致神经网络过度适应（测试时异常大的活动），这种情况 Motivates a general, post-training clipping method for backdoor mitigation, i.e., with bounds on internal-layer activations learned using a small set of clean samples。我们开发了一种新的这种方法，选择活动 bound 以限制分类margin。这种方法在对比它的同类方法时表现出优秀的性能，用于 CIFAR-10 图像分类。我们还证明了这种方法对适应攻击、X2X攻击和不同的 dataset 具有强大的 Robustness。最后，我们示出了基于输出差异的测试时检测和修复方法的扩展。我们的方法代码在线可用。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Deep-Learning-and-Data-Preprocessing-Techniques-for-Detection-Prediction-and-Monitoring-of-Stress-and-Stress-related-Mental-Disorders-A-Scoping-Review"><a href="#Machine-Learning-Deep-Learning-and-Data-Preprocessing-Techniques-for-Detection-Prediction-and-Monitoring-of-Stress-and-Stress-related-Mental-Disorders-A-Scoping-Review" class="headerlink" title="Machine Learning, Deep Learning and Data Preprocessing Techniques for Detection, Prediction, and Monitoring of Stress and Stress-related Mental Disorders: A Scoping Review"></a>Machine Learning, Deep Learning and Data Preprocessing Techniques for Detection, Prediction, and Monitoring of Stress and Stress-related Mental Disorders: A Scoping Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04616">http://arxiv.org/abs/2308.04616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moein Razavi, Samira Ziyadidegan, Reza Jahromi, Saber Kazeminasab, Vahid Janfaza, Ahmadreza Mahmoudzadeh, Elaheh Baharlouei, Farzan Sasangohar</li>
<li>for: 本文系统性地评估了应用机器学习（ML）方法在压力和压力相关的精神疾病（MD）检测、预测和分析方面的研究。</li>
<li>methods: 该文综述了最新的ML算法、预处理技术和数据类型在压力和压力相关MD方面的应用。结果表明支持向量机（SVM）、神经网络（NN）和随机森林（RF）模型在所有评估的机器学习算法中具有最高的准确性和可靠性。</li>
<li>results: 文综述发现，Physiological parameters such as heart rate measurements and skin response are prevalently used as stress predictors in ML algorithms, due to their rich explanatory information concerning stress and stress-related MDs, as well as the relative ease of data acquisition. Additionally, the application of dimensionality reduction techniques, including mappings, feature selection, filtering, and noise reduction, is frequently observed as a crucial step preceding the training of ML algorithms.<details>
<summary>Abstract</summary>
This comprehensive review systematically evaluates Machine Learning (ML) methodologies employed in the detection, prediction, and analysis of mental stress and its consequent mental disorders (MDs). Utilizing a rigorous scoping review process, the investigation delves into the latest ML algorithms, preprocessing techniques, and data types employed in the context of stress and stress-related MDs. The findings highlight that Support Vector Machine (SVM), Neural Network (NN), and Random Forest (RF) models consistently exhibit superior accuracy and robustness among all machine learning algorithms examined. Furthermore, the review underscores that physiological parameters, such as heart rate measurements and skin response, are prevalently used as stress predictors in ML algorithms. This is attributed to their rich explanatory information concerning stress and stress-related MDs, as well as the relative ease of data acquisition. Additionally, the application of dimensionality reduction techniques, including mappings, feature selection, filtering, and noise reduction, is frequently observed as a crucial step preceding the training of ML algorithms. The synthesis of this review identifies significant research gaps and outlines future directions for the field. These encompass areas such as model interpretability, model personalization, the incorporation of naturalistic settings, and real-time processing capabilities for detection and prediction of stress and stress-related MDs.
</details>
<details>
<summary>摘要</summary>
The findings show that Support Vector Machine (SVM), Neural Network (NN), and Random Forest (RF) models consistently demonstrate superior accuracy and robustness among all ML algorithms examined. Additionally, the review highlights that physiological parameters, such as heart rate measurements and skin response, are commonly used as stress predictors in ML algorithms due to their rich explanatory information and ease of data acquisition.The review also notes that dimensionality reduction techniques, including mappings, feature selection, filtering, and noise reduction, are frequently applied as a crucial step before training ML algorithms. The synthesis of this review identifies significant research gaps and outlines future directions for the field, including model interpretability, model personalization, the incorporation of naturalistic settings, and real-time processing capabilities for detection and prediction of stress and stress-related MDs.
</details></li>
</ul>
<hr>
<h2 id="Sparse-Array-Design-for-Direction-Finding-using-Deep-Learning"><a href="#Sparse-Array-Design-for-Direction-Finding-using-Deep-Learning" class="headerlink" title="Sparse Array Design for Direction Finding using Deep Learning"></a>Sparse Array Design for Direction Finding using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04615">http://arxiv.org/abs/2308.04615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kumar Vijay Mishra, Ahmet M. Elbir, Koichi Ichige</li>
<li>for: 这些笔记是为了描述深度学习技术在设计稀疏数组方面的应用。</li>
<li>methods: 这些笔记使用的方法包括超vision学习和转移学习，以及元启发学习算法如模拟热化。</li>
<li>results: 这些笔记通过多个数值实验显示了深度学习技术在不同应用中的性能。这些应用包括认知雷达应用、无线通信应用和集成感知通信应用。<details>
<summary>Abstract</summary>
In the past few years, deep learning (DL) techniques have been introduced for designing sparse arrays. These methods offer the advantages of feature engineering and low prediction-stage complexity, which is helpful in tackling the combinatorial search inherent to finding a sparse array. In this chapter, we provide a synopsis of several direction finding applications of DL-based sparse arrays. We begin by examining supervised and transfer learning techniques that have applications in selecting sparse arrays for a cognitive radar application. Here, we also discuss the use of meta-heuristic learning algorithms such as simulated annealing for the case of designing two-dimensional sparse arrays. Next, we consider DL-based antenna selection for wireless communications, wherein sparse array problem may also be combined with channel estimation, beamforming, or localization. Finally, we provide an example of deep sparse array technique for integrated sensing and communications (ISAC) application, wherein a trade-off of radar and communications performance makes ISAC sparse array problem very challenging. For each setting, we illustrate the performance of model-based optimization and DL techniques through several numerical experiments. We discuss additional considerations required to ensure robustness of DL-based algorithms against various imperfections in array data.
</details>
<details>
<summary>摘要</summary>
在过去几年，深度学习（DL）技术被应用于设计稀疏阵列。这些方法具有特征工程和预测阶段复杂性的优点，可以帮助解决稀疏阵列中的 комбинаторional搜索问题。在这个章节中，我们提供了多个方向找应用的DL-based稀疏阵列 synopsis。我们开始是查看supervised和转移学习技术的应用，包括用于选择稀疏阵列的cognitive radar应用。此外，我们还讨论了meta-heuristic学习算法，如模拟热处理，用于两dimensional稀疏阵列的设计。接着，我们考虑了DL-based天线选择，用于无线通信，其中稀疏阵列问题可能也与通道估计、扫描、或定位相结合。最后，我们提供了一个例子，用于 интеграted sensing和通信（ISAC）应用，其中各种因素使得ISAC稀疏阵列问题非常困难。对于每个设置，我们通过多个数学实验ILLUSTRATE了模型基于优化和DL技术的性能。我们还讨论了对DL基于算法的稳定性进行保证，以防止数组数据中的各种不纯净。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Driven-Detection-of-Tsunami-Related-Internal-GravityWaves-a-path-towards-open-ocean-natural-hazards-detection"><a href="#Deep-Learning-Driven-Detection-of-Tsunami-Related-Internal-GravityWaves-a-path-towards-open-ocean-natural-hazards-detection" class="headerlink" title="Deep Learning Driven Detection of Tsunami Related Internal GravityWaves: a path towards open-ocean natural hazards detection"></a>Deep Learning Driven Detection of Tsunami Related Internal GravityWaves: a path towards open-ocean natural hazards detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04611">http://arxiv.org/abs/2308.04611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vc1492a/tidd">https://github.com/vc1492a/tidd</a></li>
<li>paper_authors: Valentino Constantinou, Michela Ravanelli, Hamlin Liu, Jacob Bortnik</li>
<li>for: 这个论文是为了探讨津波可以触发内部重力波（IGW）在 ionosphere 中，并对全球卫星导航系统（GNSS）的总电子Content（TEC）产生影响，从而提高津波早期预警系统的可靠性。</li>
<li>methods: 这个论文使用了深度学习技术，将大量的 GNSS 数据与 Computer Vision 技术相结合，以检测开 ocean 上的 TIDs。</li>
<li>results: 这个论文通过使用历史数据（2010 年智利大地震、2011 年日本东北地震、2012 年加拿大海地震）进行模型训练，并使用2015 年智利伊拉帕尔地震作为模型验证。结果显示，使用这个实验室框架可以达到 91.7% F1 分数。<details>
<summary>Abstract</summary>
Tsunamis can trigger internal gravity waves (IGWs) in the ionosphere, perturbing the Total Electron Content (TEC) - referred to as Traveling Ionospheric Disturbances (TIDs) that are detectable through the Global Navigation Satellite System (GNSS). The GNSS are constellations of satellites providing signals from Earth orbit - Europe's Galileo, the United States' Global Positioning System (GPS), Russia's Global'naya Navigatsionnaya Sputnikovaya Sistema (GLONASS) and China's BeiDou. The real-time detection of TIDs provides an approach for tsunami detection, enhancing early warning systems by providing open-ocean coverage in geographic areas not serviceable by buoy-based warning systems. Large volumes of the GNSS data is leveraged by deep learning, which effectively handles complex non-linear relationships across thousands of data streams. We describe a framework leveraging slant total electron content (sTEC) from the VARION (Variometric Approach for Real-Time Ionosphere Observation) algorithm by Gramian Angular Difference Fields (from Computer Vision) and Convolutional Neural Networks (CNNs) to detect TIDs in near-real-time. Historical data from the 2010 Maule, 2011 Tohoku and the 2012 Haida-Gwaii earthquakes and tsunamis are used in model training, and the later-occurring 2015 Illapel earthquake and tsunami in Chile for out-of-sample model validation. Using the experimental framework described in the paper, we achieved a 91.7% F1 score. Source code is available at: https://github.com/vc1492a/tidd. Our work represents a new frontier in detecting tsunami-driven IGWs in open-ocean, dramatically improving the potential for natural hazards detection for coastal communities.
</details>
<details>
<summary>摘要</summary>
TSUNAMIS可以触发内部重力波（IGW）在ionosphere中，影响Total Electron Content（TEC），被称为旅行 ionospheric Disturbances（TIDs），可以通过全球卫星定位系统（GNSS）进行检测。GNSS包括欧洲的Galileo、美国的Global Positioning System（GPS）、俄罗斯的Global'naya Navigatsionnaya Sputnikovaya Sistema（GLONASS）以及中国的BeiDou。实时检测TIDs可以增强洪水早期警报系统，提供开 ocean 覆盖，在海岸社区中增强自然灾害探测的潜力。通过深度学习，可以有效地处理复杂的非线性关系，并处理 thousands of 数据流。我们介绍了一个框架，利用 Slant Total Electron Content（sTEC）从VARION（Variometric Approach for Real-Time Ionosphere Observation）算法、Gramian Angular Difference Fields（从计算机视觉）和Convolutional Neural Networks（CNNs）来检测TIDs。历史数据来自2010年的 Maule、2011年的 Tohoku 和2012年的 Haida-Gwaii 海啸和地震，以及2015年的 Illapel 海啸。通过使用这些数据进行模型训练，并在模型验证中使用2015年的 Illapel 海啸。通过我们所描述的实验框架，我们实现了91.7%的 F1 分数。源代码可以在以下链接中找到：https://github.com/vc1492a/tidd。我们的工作代表了一种新的探测方式，可以在开 ocean 中探测海啸驱动的IGW，对海岸社区的自然灾害探测潜力做出了重要提高。
</details></li>
</ul>
<hr>
<h2 id="PSRFlow-Probabilistic-Super-Resolution-with-Flow-Based-Models-for-Scientific-Data"><a href="#PSRFlow-Probabilistic-Super-Resolution-with-Flow-Based-Models-for-Scientific-Data" class="headerlink" title="PSRFlow: Probabilistic Super Resolution with Flow-Based Models for Scientific Data"></a>PSRFlow: Probabilistic Super Resolution with Flow-Based Models for Scientific Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04605">http://arxiv.org/abs/2308.04605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyi Shen, Han-Wei Shen</li>
<li>For: 这个论文主要针对科学数据超分辨化问题，即如何使用深度学习模型来提高低分辨化数据的分辨率，同时也考虑了量化结果的不确定性。* Methods: 这个论文提出了一种基于normalizing flow的生成模型，称为PSRFlow，它可以在科学数据超分辨化过程中带有量化结果的不确定性。PSRFlow采用了conditionaldistribution来学习高分辨化数据的征文分布，并通过随机抽取Gaussian积分空间中的样本来实现不同可能性的超分辨化输出。* Results: 论文的实验结果表明，PSRFlow可以在不同的数据缩放比例下进行灵活的超分辨化，并且可以准确量化超分辨化结果的不确定性。相比之下，使用插值和GAN基于的超分辨化网络，PSRFlow的性能更佳，并且可以在不同的数据缩放比例下进行灵活的超分辨化。<details>
<summary>Abstract</summary>
Although many deep-learning-based super-resolution approaches have been proposed in recent years, because no ground truth is available in the inference stage, few can quantify the errors and uncertainties of the super-resolved results. For scientific visualization applications, however, conveying uncertainties of the results to scientists is crucial to avoid generating misleading or incorrect information. In this paper, we propose PSRFlow, a novel normalizing flow-based generative model for scientific data super-resolution that incorporates uncertainty quantification into the super-resolution process. PSRFlow learns the conditional distribution of the high-resolution data based on the low-resolution counterpart. By sampling from a Gaussian latent space that captures the missing information in the high-resolution data, one can generate different plausible super-resolution outputs. The efficient sampling in the Gaussian latent space allows our model to perform uncertainty quantification for the super-resolved results. During model training, we augment the training data with samples across various scales to make the model adaptable to data of different scales, achieving flexible super-resolution for a given input. Our results demonstrate superior performance and robust uncertainty quantification compared with existing methods such as interpolation and GAN-based super-resolution networks.
</details>
<details>
<summary>摘要</summary>
尽管在过去几年内出现了许多基于深度学习的超分辨率方法，但因为在推理阶段没有地面 truth 可用，因此只能很难量化和不确定性的超分辨率结果。在科学视觉应用中，却是非常重要的将结果中的不确定性传递给科学家，以避免生成错误或不正确的信息。在这篇论文中，我们提出了 PSRFlow，一种基于 normalizing flow 的生成模型，用于科学数据超分辨率中的不确定性量化。PSRFlow 学习了高分辨率数据的 conditional 分布，基于低分辨率数据。通过在 Gaussian 噪声空间中采样，可以生成不同可能性的超分辨率输出。在 Gaussian 噪声空间中高效采样，使我们的模型能够对超分辨率结果进行不确定性量化。在模型训练过程中，我们将训练数据中的样本扩展到不同的尺度，以使模型适应不同的输入数据，实现 flexible 的超分辨率。我们的结果表明，相比于现有的 interpolate 和 GAN 基于的超分辨率网络，PSRFlow 具有更高的性能和更稳定的不确定性量化。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Decentralized-Federated-Learning"><a href="#A-Survey-on-Decentralized-Federated-Learning" class="headerlink" title="A Survey on Decentralized Federated Learning"></a>A Survey on Decentralized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04604">http://arxiv.org/abs/2308.04604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edoardo Gabrielli, Giovanni Pica, Gabriele Tolomei</li>
<li>For: This paper reviews and summarizes existing decentralized federated learning (FL) approaches proposed in the literature, with the goal of mitigating the vulnerabilities of centralized FL systems.* Methods: The paper discusses various decentralized FL approaches that have been proposed to overcome the single-point-of-failure risks and man-in-the-middle attacks of classical FL systems. These approaches include decentralized FL with blockchain, decentralized FL with distributed optimization, and decentralized FL with edge computing.* Results: The paper identifies emerging challenges in decentralized FL and suggests promising research directions in this under-explored domain, including the need for better privacy guarantees, improved communication efficiency, and more robust fault tolerance mechanisms.Here is the same information in Simplified Chinese:* For: 这篇论文总结了现有的分布式联合学习（FL）方法，以减少分布式学习系统中中央服务器的敏感性。* Methods: 论文讨论了各种分布式FL方法，包括基于区块链的分布式FL、基于分布式优化的分布式FL、基于边缘计算的分布式FL等。* Results: 论文认为，分布式FL在 presente 还存在许多挑战，例如提供更好的隐私保证、改进通信效率、加强故障快灵机制等。<details>
<summary>Abstract</summary>
In recent years, federated learning (FL) has become a very popular paradigm for training distributed, large-scale, and privacy-preserving machine learning (ML) systems. In contrast to standard ML, where data must be collected at the exact location where training is performed, FL takes advantage of the computational capabilities of millions of edge devices to collaboratively train a shared, global model without disclosing their local private data. Specifically, in a typical FL system, the central server acts only as an orchestrator; it iteratively gathers and aggregates all the local models trained by each client on its private data until convergence. Although FL undoubtedly has several benefits over traditional ML (e.g., it protects private data ownership by design), it suffers from several weaknesses. One of the most critical challenges is to overcome the centralized orchestration of the classical FL client-server architecture, which is known to be vulnerable to single-point-of-failure risks and man-in-the-middle attacks, among others. To mitigate such exposure, decentralized FL solutions have emerged where all FL clients cooperate and communicate without a central server. This survey comprehensively summarizes and reviews existing decentralized FL approaches proposed in the literature. Furthermore, it identifies emerging challenges and suggests promising research directions in this under-explored domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Learning-based-Image-Watermarking-A-Brief-Survey"><a href="#Deep-Learning-based-Image-Watermarking-A-Brief-Survey" class="headerlink" title="Deep Learning based Image Watermarking: A Brief Survey"></a>Deep Learning based Image Watermarking: A Brief Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04603">http://arxiv.org/abs/2308.04603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Zhong, Arjon Das, Fahad Alrasheedi, Abdullah Tanvir</li>
<li>for: 本文是一篇关于图像水印技术的评论文章，旨在对最新的深度学习基于图像水印技术进行总结和分析。</li>
<li>methods: 本文分为三类：Embedder-Extractor Joint Training、深度网络作为特征变换和混合方案。每个类型的研究方向也得到了分析和总结。</li>
<li>results: 本文对每种方法进行了评估和分析，并提出了未来研究的可能性，以便对图像水印技术的发展做出更多的贡献。<details>
<summary>Abstract</summary>
The act of secretly embedding and extracting a watermark on a cover image to protect it is known as image watermarking. In recent years, deep learning-based image watermarking techniques have been emerging one after another. To study the state-of-the-art, this survey categorizes cutting-edge deep learning-based image watermarking techniques into Embedder-Extractor Joint Training, Deep Networks as a Feature Transformation, and Hybrid schemes. Research directions in each category are also analyzed and summarized. Additionally, potential future research directions are discussed to envision future studies.
</details>
<details>
<summary>摘要</summary>
Image watermarking是一种将水印隐藏在封面图像中以保护它的技术。在过去几年，基于深度学习的图像水印技术不断推出。为了研究当前最佳实践，本文将这些技术分为三类：嵌入器-EXTRACTOR共同训练、深度网络作为特征变换以及混合方案。每个类别的研究方向也被分析和总结。此外，未来研究的可能性也被讨论，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Quantization-Aware-Factorization-for-Deep-Neural-Network-Compression"><a href="#Quantization-Aware-Factorization-for-Deep-Neural-Network-Compression" class="headerlink" title="Quantization Aware Factorization for Deep Neural Network Compression"></a>Quantization Aware Factorization for Deep Neural Network Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04595">http://arxiv.org/abs/2308.04595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daria Cherniuk, Stanislav Abukhovich, Anh-Huy Phan, Ivan Oseledets, Andrzej Cichocki, Julia Gusak</li>
<li>for: 压缩和优化神经网络模型，以适应移动或嵌入式设备的内存和功耗限制。</li>
<li>methods: 使用 Alternating Direction Method of Multipliers (ADMM) 进行 Canonical Polyadic (CP) 分解，并使用量化因子来实现精简。</li>
<li>results: 提出了一种新的压缩神经网络模型，可以同时保持模型的预测质量和性能。与现有的后期量化方法相比，我们的方法具有更高的灵活性和更好的质量-性能评价。<details>
<summary>Abstract</summary>
Tensor decomposition of convolutional and fully-connected layers is an effective way to reduce parameters and FLOP in neural networks. Due to memory and power consumption limitations of mobile or embedded devices, the quantization step is usually necessary when pre-trained models are deployed. A conventional post-training quantization approach applied to networks with decomposed weights yields a drop in accuracy. This motivated us to develop an algorithm that finds tensor approximation directly with quantized factors and thus benefit from both compression techniques while keeping the prediction quality of the model. Namely, we propose to use Alternating Direction Method of Multipliers (ADMM) for Canonical Polyadic (CP) decomposition with factors whose elements lie on a specified quantization grid. We compress neural network weights with a devised algorithm and evaluate it's prediction quality and performance. We compare our approach to state-of-the-art post-training quantization methods and demonstrate competitive results and high flexibility in achiving a desirable quality-performance tradeoff.
</details>
<details>
<summary>摘要</summary>
tensor分解可以有效地减少神经网络中参数和FLOP，因此在移动或嵌入式设备上部署预训练模型时，通常需要进行量化步骤。然而，使用传统的后期量化方法可能会导致模型的准确率下降。这种情况引发了我们开发一种 direkt使用量化因子进行tensorapproximation的算法，以便同时利用压缩技术以及保持模型预测质量。具体来说，我们提出使用多元方程分解法（ADMM）来实现Canonical Polyadic（CP）分解，其中因子元素均 lie on a specified quantization grid。我们将神经网络权重压缩成一种新的算法，并评估其预测质量和性能。我们对比了我们的方法与现有的后期量化方法，并证明了我们的方法具有竞争力，并且可以在desirable quality-performance tradeoff中实现高灵活性。
</details></li>
</ul>
<hr>
<h2 id="ScatterUQ-Interactive-Uncertainty-Visualizations-for-Multiclass-Deep-Learning-Problems"><a href="#ScatterUQ-Interactive-Uncertainty-Visualizations-for-Multiclass-Deep-Learning-Problems" class="headerlink" title="ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems"></a>ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04588">http://arxiv.org/abs/2308.04588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mit-ll-responsible-ai/equine-webapp">https://github.com/mit-ll-responsible-ai/equine-webapp</a></li>
<li>paper_authors: Harry Li, Steven Jorgensen, John Holodnak, Allan Wollaber</li>
<li>for: This paper is written for machine learning (ML) consumers and engineers who need to understand and visualize the uncertainty of a model’s predictions, especially in multiclass labeling problems.</li>
<li>methods: The paper uses distance-aware neural networks and dimensionality reduction techniques to construct robust, 2-D scatter plots that explain the model’s predictions and uncertainty.</li>
<li>results: The paper demonstrates the effectiveness of the ScatterUQ system in explaining model uncertainty for a multiclass image classification on a distance-aware neural network trained on Fashion-MNIST and tested on Fashion-MNIST (in distribution) and MNIST digits (out of distribution), as well as a deep learning model for a cyber dataset. The results indicate that the ScatterUQ system should scale to arbitrary, multiclass datasets.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文是为机器学习（ML）消费者和工程师编写的，他们需要理解和可视化模型预测结果的不确定性，特别是在多类标签问题中。</li>
<li>methods: 论文使用距离意识的神经网络和维度减少技术构建robust的2D散点图，解释模型预测结果和不确定性。</li>
<li>results: 论文证明了ScatterUQ系统在多类图像分类任务中适用，可以准确地解释模型预测结果的不确定性。试验结果表明，ScatterUQ系统可以扩展到任意多类数据集。<details>
<summary>Abstract</summary>
Recently, uncertainty-aware deep learning methods for multiclass labeling problems have been developed that provide calibrated class prediction probabilities and out-of-distribution (OOD) indicators, letting machine learning (ML) consumers and engineers gauge a model's confidence in its predictions. However, this extra neural network prediction information is challenging to scalably convey visually for arbitrary data sources under multiple uncertainty contexts. To address these challenges, we present ScatterUQ, an interactive system that provides targeted visualizations to allow users to better understand model performance in context-driven uncertainty settings. ScatterUQ leverages recent advances in distance-aware neural networks, together with dimensionality reduction techniques, to construct robust, 2-D scatter plots explaining why a model predicts a test example to be (1) in-distribution and of a particular class, (2) in-distribution but unsure of the class, and (3) out-of-distribution. ML consumers and engineers can visually compare the salient features of test samples with training examples through the use of a ``hover callback'' to understand model uncertainty performance and decide follow up courses of action. We demonstrate the effectiveness of ScatterUQ to explain model uncertainty for a multiclass image classification on a distance-aware neural network trained on Fashion-MNIST and tested on Fashion-MNIST (in distribution) and MNIST digits (out of distribution), as well as a deep learning model for a cyber dataset. We quantitatively evaluate dimensionality reduction techniques to optimize our contextually driven UQ visualizations. Our results indicate that the ScatterUQ system should scale to arbitrary, multiclass datasets. Our code is available at https://github.com/mit-ll-responsible-ai/equine-webapp
</details>
<details>
<summary>摘要</summary>
近些时间，有关uncertainty-aware深度学习方法的发展，可以提供批量分类问题中模型的信度预测概率和外部数据（OOD）指标，让机器学习（ML）用户和工程师可以评估模型对预测的信度。然而，这些额外神经网络预测信息具有多种不同的uncertainty context，具有挑战的可扩展性。为解决这些挑战，我们介绍了ScatterUQ，一个交互式系统，可以为用户提供targeted visualization，以便更好地理解模型在不同uncertainty setting下的性能。ScatterUQ利用了最新的距离意识神经网络和维度减少技术，构建了robust的2D分布图，解释了模型对测试示例的预测是（1）在distribution中，（2）在distribution中，但不确定的类，以及（3）out-of-distribution。通过使用“悬挂回调”，ML用户和工程师可以通过比较测试示例与训练示例的突出特征来理解模型的不确定性性能，并决定进一步的行动。我们在Fashion-MNIST和MNIST数字集上进行了多类图像分类 task的测试，以及一个cyber dataset上的深度学习模型。我们量化评估维度减少技术，以便最佳化我们的上下文驱动的UQ视觉表示。我们的结果表明，ScatterUQ系统可扩展到任意多类数据集。我们的代码可以在https://github.com/mit-ll-responsible-ai/equine-webapp中找到。
</details></li>
</ul>
<hr>
<h2 id="Kernel-Single-Proxy-Control-for-Deterministic-Confounding"><a href="#Kernel-Single-Proxy-Control-for-Deterministic-Confounding" class="headerlink" title="Kernel Single Proxy Control for Deterministic Confounding"></a>Kernel Single Proxy Control for Deterministic Confounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04585">http://arxiv.org/abs/2308.04585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyuan Xu, Arthur Gretton</li>
<li>for: 这个论文研究了 causal effect estimation 问题，特别是在存在隐藏的假设变量时。</li>
<li>methods: 该论文使用了 two proxy variables 来恢复真实的 causal effect，并且提出了两种基于 kernel 方法来实现这一目标。</li>
<li>results: 论文 prove 了这两种方法可以一致地估计 causal effect，并且通过 Synthetic 数据 实验 validate 了这一结论。<details>
<summary>Abstract</summary>
We consider the problem of causal effect estimation with an unobserved confounder, where we observe a proxy variable that is associated with the confounder. Although Proxy Causal Learning (PCL) uses two proxy variables to recover the true causal effect, we show that a single proxy variable is sufficient for causal estimation if the outcome is generated deterministically, generalizing Control Outcome Calibration Approach (COCA). We propose two kernel-based methods for this setting: the first based on the two-stage regression approach, and the second based on a maximum moment restriction approach. We prove that both approaches can consistently estimate the causal effect, and we empirically demonstrate that we can successfully recover the causal effect on a synthetic dataset.
</details>
<details>
<summary>摘要</summary>
我团队考虑了一个 causal effect 估计问题，其中存在一个隐藏的干扰因素。我们观察到一个代理变量，该变量与干扰因素相关。虽然 Proxy Causal Learning（PCL）使用两个代理变量来恢复真正的 causal effect，但我们表明一个代理变量足够供 causal 估计，如果结果是 deterministic 生成的，总结 Control Outcome Calibration Approach（COCA）。我们提议了两种基于 kernel 方法来解决这个设置：第一种是两 stage 回归方法，第二种是基于最大 moments 约束方法。我们证明了这两种方法可靠地估计 causal effect，并在 synthetic 数据集上进行了实验验证。
</details></li>
</ul>
<hr>
<h2 id="RECipe-Does-a-Multi-Modal-Recipe-Knowledge-Graph-Fit-a-Multi-Purpose-Recommendation-System"><a href="#RECipe-Does-a-Multi-Modal-Recipe-Knowledge-Graph-Fit-a-Multi-Purpose-Recommendation-System" class="headerlink" title="RECipe: Does a Multi-Modal Recipe Knowledge Graph Fit a Multi-Purpose Recommendation System?"></a>RECipe: Does a Multi-Modal Recipe Knowledge Graph Fit a Multi-Purpose Recommendation System?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04579">http://arxiv.org/abs/2308.04579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Pesaranghader, Touqir Sajed<br>for:RECipe is designed to recommend recipes to users based on their natural language queries or images.methods:The RECipe framework consists of three subsystems: behavior-based recommender, review-based recommender, and image-based recommender, all of which rely on embedding representations of entities and relations in a multi-modal knowledge graph (MMKG). The framework uses a pre-trained model of Microsoft’s MPNet for textual entities, and a KGE-Guided variational autoencoder (KG-VAE) for visual components.results:The authors report that the KGE models have comparable performance to neural solutions, and demonstrate the application of RECipe in a multi-purpose recommendation setting. They also show that pre-trained NLP embeddings can be used for zero-shot inference for new users and conditional recommendation with respect to recipe categories.<details>
<summary>Abstract</summary>
Over the past two decades, recommendation systems (RSs) have used machine learning (ML) solutions to recommend items, e.g., movies, books, and restaurants, to clients of a business or an online platform. Recipe recommendation, however, has not yet received much attention compared to those applications. We introduce RECipe as a multi-purpose recipe recommendation framework with a multi-modal knowledge graph (MMKG) backbone. The motivation behind RECipe is to go beyond (deep) neural collaborative filtering (NCF) by recommending recipes to users when they query in natural language or by providing an image. RECipe consists of 3 subsystems: (1) behavior-based recommender, (2) review-based recommender, and (3) image-based recommender. Each subsystem relies on the embedding representations of entities and relations in the graph. We first obtain (pre-trained) embedding representations of textual entities, such as reviews or ingredients, from a fine-tuned model of Microsoft's MPNet. We initialize the weights of the entities with these embeddings to train our knowledge graph embedding (KGE) model. For the visual component, i.e., recipe images, we develop a KGE-Guided variational autoencoder (KG-VAE) to learn the distribution of images and their latent representations. Once KGE and KG-VAE models are fully trained, we use them as a multi-purpose recommendation framework. For benchmarking, we created two knowledge graphs (KGs) from public datasets on Kaggle for recipe recommendation. Our experiments show that the KGE models have comparable performance to the neural solutions. We also present pre-trained NLP embeddings to address important applications such as zero-shot inference for new users (or the cold start problem) and conditional recommendation with respect to recipe categories. We eventually demonstrate the application of RECipe in a multi-purpose recommendation setting.
</details>
<details>
<summary>摘要</summary>
RECipe consists of three subsystems: (1) behavior-based recommender, (2) review-based recommender, and (3) image-based recommender. Each subsystem relies on the embedding representations of entities and relations in the graph. We first obtain pre-trained embedding representations of textual entities, such as reviews or ingredients, from a fine-tuned model of Microsoft's MPNet. We initialize the weights of the entities with these embeddings to train our knowledge graph embedding (KGE) model. For the visual component, i.e., recipe images, we develop a KGE-Guided variational autoencoder (KG-VAE) to learn the distribution of images and their latent representations. Once the KGE and KG-VAE models are fully trained, we use them as a multi-purpose recommendation framework.For benchmarking, we created two knowledge graphs (KGs) from public datasets on Kaggle for recipe recommendation. Our experiments show that the KGE models have comparable performance to the neural solutions. We also present pre-trained NLP embeddings to address important applications such as zero-shot inference for new users (or the cold start problem) and conditional recommendation with respect to recipe categories. We eventually demonstrate the application of RECipe in a multi-purpose recommendation setting.
</details></li>
</ul>
<hr>
<h2 id="Copy-Number-Variation-Informs-fMRI-based-Prediction-of-Autism-Spectrum-Disorder"><a href="#Copy-Number-Variation-Informs-fMRI-based-Prediction-of-Autism-Spectrum-Disorder" class="headerlink" title="Copy Number Variation Informs fMRI-based Prediction of Autism Spectrum Disorder"></a>Copy Number Variation Informs fMRI-based Prediction of Autism Spectrum Disorder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05122">http://arxiv.org/abs/2308.05122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicha C. Dvornek, Catherine Sullivan, James S. Duncan, Abha R. Gupta</li>
<li>for: 本研究旨在开发一种能够集成基因数据、人口数据和功能磁共振成像数据的模型，以提高自适应发展干预和精准诊断autism spectrum disorder (ASD)的能力。</li>
<li>methods: 本研究使用的方法包括基因数据的拟合策略，以及基于注意力的方法，使得基因数据指导了功能磁共振成像特征的重要性。</li>
<li>results: 研究结果表明，基于注意力的模型在ASD分类和严重程度预测任务中表现出了更高的预测性，比其他多Modal方法更好。<details>
<summary>Abstract</summary>
The multifactorial etiology of autism spectrum disorder (ASD) suggests that its study would benefit greatly from multimodal approaches that combine data from widely varying platforms, e.g., neuroimaging, genetics, and clinical characterization. Prior neuroimaging-genetic analyses often apply naive feature concatenation approaches in data-driven work or use the findings from one modality to guide posthoc analysis of another, missing the opportunity to analyze the paired multimodal data in a truly unified approach. In this paper, we develop a more integrative model for combining genetic, demographic, and neuroimaging data. Inspired by the influence of genotype on phenotype, we propose using an attention-based approach where the genetic data guides attention to neuroimaging features of importance for model prediction. The genetic data is derived from copy number variation parameters, while the neuroimaging data is from functional magnetic resonance imaging. We evaluate the proposed approach on ASD classification and severity prediction tasks, using a sex-balanced dataset of 228 ASD and typically developing subjects in a 10-fold cross-validation framework. We demonstrate that our attention-based model combining genetic information, demographic data, and functional magnetic resonance imaging results in superior prediction performance compared to other multimodal approaches.
</details>
<details>
<summary>摘要</summary>
多因素性的自闭症спектループ病（ASD）的研究受到多Modal的方法的帮助，这些方法结合了不同的平台数据，例如神经成像、遗传学和临床特征化。过去的神经成像-遗传学分析经常使用简单的特征 concatenation 方法，或者使用一个Modal的数据来导向另一个Modal的后期分析，错过了对归一化Multimodal数据的机会。在这篇论文中，我们开发了一种更一致的模型，用于结合遗传学、人口学和神经成像数据。受到遗传型的影响，我们提议使用注意力机制，使遗传学数据指导神经成像特征的重要性。遗传学数据来自拟合数值参数，而神经成像数据来自功能核磁共振成像。我们在ASD分类和严重程度预测任务中使用了10次交叉验证框架，并证明了我们的注意力机制模型在多Modal方法中的超越性。
</details></li>
</ul>
<hr>
<h2 id="From-Fake-to-Real-FFR-A-two-stage-training-pipeline-for-mitigating-spurious-correlations-with-synthetic-data"><a href="#From-Fake-to-Real-FFR-A-two-stage-training-pipeline-for-mitigating-spurious-correlations-with-synthetic-data" class="headerlink" title="From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data"></a>From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04553">http://arxiv.org/abs/2308.04553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maan Qraitem, Kate Saenko, Bryan A. Plummer</li>
<li>for: 降低训练集中孤立的偏见（Females），使计算机视觉模型更加公正。</li>
<li>methods: 使用生成模型生成偏见类别（Programmers）中少见的样本数据，以增加训练集的多样性。</li>
<li>results: 提高计算机视觉模型的性能，并避免训练集中的偏见。<details>
<summary>Abstract</summary>
Visual recognition models are prone to learning spurious correlations induced by an imbalanced training set where certain groups (\eg Females) are under-represented in certain classes (\eg Programmers). Generative models offer a promising direction in mitigating this bias by generating synthetic data for the minority samples and thus balancing the training set. However, prior work that uses these approaches overlooks that visual recognition models could often learn to differentiate between real and synthetic images and thus fail to unlearn the bias in the original dataset. In our work, we propose a novel two-stage pipeline to mitigate this issue where 1) we pre-train a model on a balanced synthetic dataset and then 2) fine-tune on the real data. Using this pipeline, we avoid training on both real and synthetic data, thus avoiding the bias between real and synthetic data. Moreover, we learn robust features against the bias in the first step that mitigate the bias in the second step. Moreover, our pipeline naturally integrates with bias mitigation methods; they can be simply applied to the fine-tuning step. As our experiments prove, our pipeline can further improve the performance of bias mitigation methods obtaining state-of-the-art performance on three large-scale datasets.
</details>
<details>
<summary>摘要</summary>
“视觉识别模型容易学习偏袋 induce 由偏袋训练集而引起的偏袋偏好，其中女性组是训练集中受到排斥的。生成模型提供了一种可能性，即通过生成少数样本的Synthetic数据来平衡训练集。然而，现有的方法忽略了视觉识别模型可能会学习 differentiate between real and Synthetic images，从而失去原始数据中的偏袋。在我们的工作中，我们提出了一种新的两阶段管道来解决这个问题，即先在Balanced Synthetic dataset上预训练模型，然后在Real数据上细化。这种管道可以避免训练在Real和Synthetic数据上，从而避免偏袋 между Real和Synthetic数据。此外，我们的管道可以自然地与偏袋缓解方法集成，这些方法可以简单地应用于细化步骤。根据我们的实验证明，我们的管道可以进一步提高偏袋缓解方法的性能，并在三个大规模数据集上达到了当前最佳性能。”
</details></li>
</ul>
<hr>
<h2 id="Improving-Medical-Image-Classification-in-Noisy-Labels-Using-Only-Self-supervised-Pretraining"><a href="#Improving-Medical-Image-Classification-in-Noisy-Labels-Using-Only-Self-supervised-Pretraining" class="headerlink" title="Improving Medical Image Classification in Noisy Labels Using Only Self-supervised Pretraining"></a>Improving Medical Image Classification in Noisy Labels Using Only Self-supervised Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04551">http://arxiv.org/abs/2308.04551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bbrattoli/JigsawPuzzlePytorch">https://github.com/bbrattoli/JigsawPuzzlePytorch</a></li>
<li>paper_authors: Bidur Khanal, Binod Bhattarai, Bishesh Khanal, Cristian A. Linte</li>
<li>for: 这个研究旨在探索自我监督预训练可以帮助运算图像分类器对于噪音标签的学习。</li>
<li>methods: 这个研究使用了两种自我监督预训练方法：对比自我监督预训练和预设任务预训练。</li>
<li>results: 研究结果显示，使用自我监督预训练初始化的模型可以更好地适应噪音标签下的学习，并且在医疗图像分类任务中表现出色。<details>
<summary>Abstract</summary>
Noisy labels hurt deep learning-based supervised image classification performance as the models may overfit the noise and learn corrupted feature extractors. For natural image classification training with noisy labeled data, model initialization with contrastive self-supervised pretrained weights has shown to reduce feature corruption and improve classification performance. However, no works have explored: i) how other self-supervised approaches, such as pretext task-based pretraining, impact the learning with noisy label, and ii) any self-supervised pretraining methods alone for medical images in noisy label settings. Medical images often feature smaller datasets and subtle inter class variations, requiring human expertise to ensure correct classification. Thus, it is not clear if the methods improving learning with noisy labels in natural image datasets such as CIFAR would also help with medical images. In this work, we explore contrastive and pretext task-based self-supervised pretraining to initialize the weights of a deep learning classification model for two medical datasets with self-induced noisy labels -- NCT-CRC-HE-100K tissue histological images and COVID-QU-Ex chest X-ray images. Our results show that models initialized with pretrained weights obtained from self-supervised learning can effectively learn better features and improve robustness against noisy labels.
</details>
<details>
<summary>摘要</summary>
噪音标签会妨碍深度学习基于监督图像分类的性能，因为模型可能会过拟合噪音并学习损坏的特征提取器。在自然图像分类训练中使用噪音标签时，使用对比自我超vised预训练的模型初始化可以减少特征损坏并提高分类性能。然而，没有任何研究探讨了：一、其他自我超视任务基于预训练的影响在噪音标签下的学习；二、任何自我超视任务alone可以在医疗图像中提高分类性能。医疗图像通常具有小数据量和柔微的类别变化，需要人工专业来确保正确的分类。因此，不清楚自然图像dataset中的方法会否帮助医疗图像。在这项工作中，我们探讨了对比和预tex任务基于自我超视预训练来初始化深度学习分类模型的效果，并应用于两个医疗图像dataset中的自induced噪音标签——NCT-CRC-HE-100K组织 Histological图像和COVID-QU-Ex胸部X射影像。我们的结果表明，使用自我超视预训练获得的预训练模型可以更好地学习特征和抗抗噪音标签的强健性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Performance-in-Continual-Learning-Tasks-using-Bio-Inspired-Architectures"><a href="#Improving-Performance-in-Continual-Learning-Tasks-using-Bio-Inspired-Architectures" class="headerlink" title="Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures"></a>Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04539">http://arxiv.org/abs/2308.04539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandeep Madireddy, Angel Yanguas-Gil, Prasanna Balaprakash</li>
<li>for: 本研究旨在设计一个能够持续学习的智能系统，以应对实际世界中的不断更新和变化。</li>
<li>methods: 本研究使用了生物学中的 synaptic plasticity 机制和 neuromodulation，实现了在线进行持续学习，不需要使用条件 gradient descent 和储存 Buffer。</li>
<li>results: 比较其他记忆受限的学习方法，本研究在 Split-MNIST、Split-CIFAR-10 和 Split-CIFAR-100 数据集上表现出色，并且与使用条件 gradient descent 的记忆密集探险方法相符。此外，本研究还证明了 incorporating 生物学原理到机器学习模型中可以提高持续学习的精度。<details>
<summary>Abstract</summary>
The ability to learn continuously from an incoming data stream without catastrophic forgetting is critical to designing intelligent systems. Many approaches to continual learning rely on stochastic gradient descent and its variants that employ global error updates, and hence need to adopt strategies such as memory buffers or replay to circumvent its stability, greed, and short-term memory limitations. To address this limitation, we have developed a biologically inspired lightweight neural network architecture that incorporates synaptic plasticity mechanisms and neuromodulation and hence learns through local error signals to enable online continual learning without stochastic gradient descent.   Our approach leads to superior online continual learning performance on Split-MNIST, Split-CIFAR-10, and Split-CIFAR-100 datasets compared to other memory-constrained learning approaches and matches that of the state-of-the-art memory-intensive replay-based approaches. We further demonstrate the effectiveness of our approach by integrating key design concepts into other backpropagation-based continual learning algorithms, significantly improving their accuracy. Our results provide compelling evidence for the importance of incorporating biological principles into machine learning models and offer insights into how we can leverage them to design more efficient and robust systems for online continual learning.
</details>
<details>
<summary>摘要</summary>
“持续学习”是智能系统设计的核心能力。许多持续学习方法 rely on 随机 gradient descent 和其变化，但这个方法需要运用 memory buffer 或 replay 来缓解其稳定性、贪婪性和短期记忆限制。为了解决这个问题，我们已经开发了一个基于生物学原理的轻量级神经网络架构，具有 synaptic plasticity 机制和 neuromodulation，因此可以通过本地错误信号进行线上持续学习，不需要随机 gradient descent。我们的方法在 Split-MNIST、Split-CIFAR-10 和 Split-CIFAR-100 数据集上展现出较好的线上持续学习性能，比较于其他记忆受限的学习方法和 matches 状态顶尖的记忆丰富 replay-based 方法。我们还将这些设计元素 integrate 到其他 backpropagation-based 持续学习算法中，很大提高了它们的精度。我们的结果提供了对生物学原理在机器学习模型中的应用的丰富证据，并且显示了如何运用这些原理设计更有效率和可靠的线上持续学习系统。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Diverse-Data-Types-Steganalysis-A-Review"><a href="#Deep-Learning-for-Diverse-Data-Types-Steganalysis-A-Review" class="headerlink" title="Deep Learning for Diverse Data Types Steganalysis: A Review"></a>Deep Learning for Diverse Data Types Steganalysis: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04522">http://arxiv.org/abs/2308.04522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamza Kheddar, Mustapha Hemis, Yassine Himeur, David Megías, Abbes Amira</li>
<li>for: 本文旨在提供一个系统性的回顾文献中，探讨使用深度学习技术进行隐藏信息检测的最新研究进展。</li>
<li>methods: 本文使用的方法包括深度学习技术，如深度训练学习（DTL）和深度强化学习（DRL），以提高隐藏信息检测系统的性能。</li>
<li>results: 本文对各种封隐媒体进行了检测，并对不同数据集的DTL基于隐藏信息检测方法进行了系统性的分析。<details>
<summary>Abstract</summary>
Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper covers all types of cover in steganalysis, including image, audio, and video, and discusses the most commonly used deep learning techniques. In addition, the paper explores the use of more advanced deep learning techniques, such as deep transfer learning (DTL) and deep reinforcement learning (DRL), to enhance the performance of steganalysis systems. The paper provides a systematic review of recent research in the field, including data sets and evaluation metrics used in recent studies. It also presents a detailed analysis of DTL-based steganalysis approaches and their performance on different data sets. The review concludes with a discussion on the current state of deep learning-based steganalysis, challenges, and future research directions.
</details>
<details>
<summary>摘要</summary>
信息安全领域中，隐藏通信和找到隐藏通信的技术是两个相关的方面，它们分别称为隐藏通信（Steganography）和找到隐藏通信（Steganalysis）。隐藏通信是为了避免被捕获，而找到隐藏通信则是为了找到隐藏的信息。这两个方面在过去几年中吸引了很多关注，特别是由于许多国家禁止或限制 cryptography的使用，因此隐藏通信成为了违法活动的一种常见手段。为了暴露这些违法活动，了解最新的隐藏通信和找到隐藏通信技术是非常重要的。这篇文章提供了一个完整的隐藏通信和找到隐藏通信技术的文献综述，特别是使用深度学习来检测隐藏在数字媒体中的信息。文章覆盖了所有类型的隐藏媒体，包括图像、音频和视频，并讲解了最常用的深度学习技术。此外，文章还探讨了使用更高级的深度学习技术，如深度传输学习（DTL）和深度奖励学习（DRL），以提高隐藏检测系统的性能。文章还提供了最新的研究数据集和评价指标，以及DTL-based隐藏检测方法在不同数据集上的性能分析。文章结束于对深度学习基于隐藏检测的当前状况、挑战和未来研究方向的讨论。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Model-Agnostic-Reliability-Evaluation-of-Machine-Learning-Methods-Integrated-in-Instrumentation-Control-Systems"><a href="#Dynamic-Model-Agnostic-Reliability-Evaluation-of-Machine-Learning-Methods-Integrated-in-Instrumentation-Control-Systems" class="headerlink" title="Dynamic Model Agnostic Reliability Evaluation of Machine-Learning Methods Integrated in Instrumentation &amp; Control Systems"></a>Dynamic Model Agnostic Reliability Evaluation of Machine-Learning Methods Integrated in Instrumentation &amp; Control Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05120">http://arxiv.org/abs/2308.05120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Chen, Han Bao, Nam Dinh<br>for:This paper aims to address the lack of trustworthiness in data-driven neural network-based machine learning (ML) algorithms, specifically in the field of instrumentation and control systems.methods:The authors propose a real-time model-agnostic method called Laplacian distributed decay for reliability (LADDR) to evaluate the relative reliability of ML predictions by incorporating out-of-distribution detection on the training dataset.results:LADDR is demonstrated on a feedforward neural network-based model used to predict safety significant factors during different loss-of-flow transients. The results show that LADDR can determine the appropriateness of well-trained ML models in the context of operational conditions, and illustrates how training data can be used as evidence to support the trustworthiness of ML predictions when utilized for conventional interpolation tasks.Here is the Chinese translation of the three key points:for:这篇论文目标是解决数据驱动神经网络基于机器学习（ML）算法在仪器控制系统中的可靠性问题。methods:作者提出了一种实时模型无关的方法called Laplacian distributed decay for reliability (LADDR)，用于评估ML预测的相对可靠性，通过训练集上的异常检测。results:LADDR在不同的流失转换中使用的一种Feedforward神经网络模型上进行了评估，结果表明LADDR可以在运维条件下评估已经训练过的ML模型的适用性，并 Illustrates how training data can be used as evidence to support the trustworthiness of ML predictions when utilized for conventional interpolation tasks。<details>
<summary>Abstract</summary>
In recent years, the field of data-driven neural network-based machine learning (ML) algorithms has grown significantly and spurred research in its applicability to instrumentation and control systems. While they are promising in operational contexts, the trustworthiness of such algorithms is not adequately assessed. Failures of ML-integrated systems are poorly understood; the lack of comprehensive risk modeling can degrade the trustworthiness of these systems. In recent reports by the National Institute for Standards and Technology, trustworthiness in ML is a critical barrier to adoption and will play a vital role in intelligent systems' safe and accountable operation. Thus, in this work, we demonstrate a real-time model-agnostic method to evaluate the relative reliability of ML predictions by incorporating out-of-distribution detection on the training dataset. It is well documented that ML algorithms excel at interpolation (or near-interpolation) tasks but significantly degrade at extrapolation. This occurs when new samples are "far" from training samples. The method, referred to as the Laplacian distributed decay for reliability (LADDR), determines the difference between the operational and training datasets, which is used to calculate a prediction's relative reliability. LADDR is demonstrated on a feedforward neural network-based model used to predict safety significant factors during different loss-of-flow transients. LADDR is intended as a "data supervisor" and determines the appropriateness of well-trained ML models in the context of operational conditions. Ultimately, LADDR illustrates how training data can be used as evidence to support the trustworthiness of ML predictions when utilized for conventional interpolation tasks.
</details>
<details>
<summary>摘要</summary>
近年来，数据驱动神经网络基本学习算法在控制系统领域得到了广泛的应用和研究。虽然它们在操作上表现良好，但对这些算法的可靠性还未充分评估。失败的机器学习integrated系统未得到充分了理解，缺乏全面的风险模型化可能导致这些系统的可靠性下降。在国家标准技术研究所的最近报告中，机器学习的可靠性被视为智能系统安全和负责任运行的关键障碍。因此，在这项工作中，我们提出了一种实时模型不依赖的方法来评估机器学习预测的相对可靠性。这种方法被称为Laplacian分布式衰减 для可靠性（LADDR）。LADDR通过对训练集和操作集之间的差异进行计算，来确定预测的相对可靠性。LADDR作为“数据监督”，可以确定机器学习模型在操作条件下是否正常。最终，LADDR表明了训练数据可以作为机器学习预测的可靠性证明，当用于常见的 interpolate 任务时。
</details></li>
</ul>
<hr>
<h2 id="MT-IceNet-–-A-Spatial-and-Multi-Temporal-Deep-Learning-Model-for-Arctic-Sea-Ice-Forecasting"><a href="#MT-IceNet-–-A-Spatial-and-Multi-Temporal-Deep-Learning-Model-for-Arctic-Sea-Ice-Forecasting" class="headerlink" title="MT-IceNet – A Spatial and Multi-Temporal Deep Learning Model for Arctic Sea Ice Forecasting"></a>MT-IceNet – A Spatial and Multi-Temporal Deep Learning Model for Arctic Sea Ice Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04511">http://arxiv.org/abs/2308.04511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/big-data-lab-umbc/sea-ice-prediction">https://github.com/big-data-lab-umbc/sea-ice-prediction</a></li>
<li>paper_authors: Sahara Ali, Jianwu Wang</li>
<li>for: 该研究旨在提出一种基于深度学习的 Arctic 海冰覆盖率预测模型（MT-IceNet），以提高 Arctic 海冰预测的准确性。</li>
<li>methods: 该模型采用了 UNet 网络架构，并使用了 skip 连接和多时间频率输入流进行 spatial 和 multi-temporal 数据处理。</li>
<li>results: 研究表明，与现有模型相比，该模型在 6 个月预测时间点上具有较高的预测精度（至多 60% 减少预测错误），并且可以更好地捕捉 Arctic 海冰的变化趋势。<details>
<summary>Abstract</summary>
Arctic amplification has altered the climate patterns both regionally and globally, resulting in more frequent and more intense extreme weather events in the past few decades. The essential part of Arctic amplification is the unprecedented sea ice loss as demonstrated by satellite observations. Accurately forecasting Arctic sea ice from sub-seasonal to seasonal scales has been a major research question with fundamental challenges at play. In addition to physics-based Earth system models, researchers have been applying multiple statistical and machine learning models for sea ice forecasting. Looking at the potential of data-driven approaches to study sea ice variations, we propose MT-IceNet - a UNet based spatial and multi-temporal (MT) deep learning model for forecasting Arctic sea ice concentration (SIC). The model uses an encoder-decoder architecture with skip connections and processes multi-temporal input streams to regenerate spatial maps at future timesteps. Using bi-monthly and monthly satellite retrieved sea ice data from NSIDC as well as atmospheric and oceanic variables from ERA5 reanalysis product during 1979-2021, we show that our proposed model provides promising predictive performance for per-pixel SIC forecasting with up to 60% decrease in prediction error for a lead time of 6 months as compared to its state-of-the-art counterparts.
</details>
<details>
<summary>摘要</summary>
北极强化效应对地区和全球气候Patterns有所改变，过去几十年内发生的极端天气事件频繁性和严重程度增加。北极强化效应的关键部分是历史上无 precedent的海冰损失，这种现象可以通过卫星观测得到证明。预测北极海冰的演变是一个长期的研究问题，Physics-based Earth system models 以外，研究人员还应用了多种统计学和机器学习模型。对于海冰变化的数据驱动方法的潜在优势，我们提议MT-IceNet - 基于UNet的空间和多时间（MT）深度学习模型，用于预测北极海冰浓度（SIC）。该模型采用编码器-解码器架构，并使用跳跃连接，以处理多时间输入流，以生成未来时间步的空间地图。使用1979-2021年NSIDC通过卫星获取的月度和半月度海冰数据，以及ERA5分析产品中的大气和海洋变量，我们示出了我们提议的模型在6个月预测时的预测性能具有显著提高，比之前的状态艺术模型减少至60%。
</details></li>
</ul>
<hr>
<h2 id="Efficient-option-pricing-with-unary-based-photonic-computing-chip-and-generative-adversarial-learning"><a href="#Efficient-option-pricing-with-unary-based-photonic-computing-chip-and-generative-adversarial-learning" class="headerlink" title="Efficient option pricing with unary-based photonic computing chip and generative adversarial learning"></a>Efficient option pricing with unary-based photonic computing chip and generative adversarial learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04493">http://arxiv.org/abs/2308.04493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Zhang, Lingxiao Wan, Sergi Ramos-Calderer, Yuancheng Zhan, Wai-Keong Mok, Hong Cai, Feng Gao, Xianshu Luo, Guo-Qiang Lo, Leong Chuan Kwek, José Ignacio Latorre, Ai Qun Liu</li>
<li>for: 这个研究是为了提高金融业中的计算能力和服务质量。</li>
<li>methods: 这个研究使用了光子学芯片，实现了欧洲选择权价格计算的单元方法，并与量子振荡估计算法相结合，实现了当前的古典 Monte Carlo 方法的二次倍速。该芯片由三个模组组成：分布模组、预期收益模组和量子振荡估计算法模组。在分布模组中，一个对抗学习网络被嵌入，以高效地学习和载入资产分布，精确地捕捉市场趋势。</li>
<li>results: 这个研究获得了与古典 Monte Carlo 方法相比的二次倍速的速度优化，并且显示出了对特定金融产品的高精度计算和评估能力。这个研究显示出了特殊的光子处理器在金融应用中的发展潜力，可能将改善金融服务的效率和质量。<details>
<summary>Abstract</summary>
In the modern financial industry system, the structure of products has become more and more complex, and the bottleneck constraint of classical computing power has already restricted the development of the financial industry. Here, we present a photonic chip that implements the unary approach to European option pricing, in combination with the quantum amplitude estimation algorithm, to achieve a quadratic speedup compared to classical Monte Carlo methods. The circuit consists of three modules: a module loading the distribution of asset prices, a module computing the expected payoff, and a module performing the quantum amplitude estimation algorithm to introduce speed-ups. In the distribution module, a generative adversarial network is embedded for efficient learning and loading of asset distributions, which precisely capture the market trends. This work is a step forward in the development of specialized photonic processors for applications in finance, with the potential to improve the efficiency and quality of financial services.
</details>
<details>
<summary>摘要</summary>
现代金融系统中，产品结构变得越来越复杂，甚至限制了金融业的发展。我们介绍了一款光学板，实现了欧洲选项价值方法的单元化方法，与量子振荡估算算法相结合，与经典 Monte Carlo 方法相比，实现了平方速度增加。该板件由三个模块组成：一个分布Module，一个预期收益Module，以及一个引入速度增加的量子振荡估算算法Module。在分布模块中，一个生成对抗网络被嵌入，以高效地学习和加载资产分布，准确捕捉市场趋势。这项工作是金融专用光学处理器的开发的一个重要步骤，有望提高金融服务的效率和质量。
</details></li>
</ul>
<hr>
<h2 id="When-More-is-Less-Incorporating-Additional-Datasets-Can-Hurt-Performance-By-Introducing-Spurious-Correlations"><a href="#When-More-is-Less-Incorporating-Additional-Datasets-Can-Hurt-Performance-By-Introducing-Spurious-Correlations" class="headerlink" title="When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations"></a>When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04431">http://arxiv.org/abs/2308.04431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/basedrhys/ood-generalization">https://github.com/basedrhys/ood-generalization</a></li>
<li>paper_authors: Rhys Compton, Lily Zhang, Aahlad Puli, Rajesh Ranganath</li>
<li>for: 这个研究探讨了在机器学习中是否可以通过添加更多数据提高模型性能的问题。</li>
<li>methods: 研究者采用了大规模的实验方法，将四个开源胸部X射影数据集和九个标签进行组合，以检测在哪些情况下，通过添加外部数据集可能会降低模型的性能。</li>
<li>results: 研究发现，在43%的情况下，由两个医院的数据进行训练的模型会在两个医院的数据上 display 更差的最坏群组精度，即使外部数据使得训练分布更接近测试分布。这种情况发生的原因是医院特有的图像残余引起的假 correlate。研究者解释了在多个数据集训练中存在的贸易关系，并提出了在选择训练数据时需要小心的警示。<details>
<summary>Abstract</summary>
In machine learning, incorporating more data is often seen as a reliable strategy for improving model performance; this work challenges that notion by demonstrating that the addition of external datasets in many cases can hurt the resulting model's performance. In a large-scale empirical study across combinations of four different open-source chest x-ray datasets and 9 different labels, we demonstrate that in 43% of settings, a model trained on data from two hospitals has poorer worst group accuracy over both hospitals than a model trained on just a single hospital's data. This surprising result occurs even though the added hospital makes the training distribution more similar to the test distribution. We explain that this phenomenon arises from the spurious correlation that emerges between the disease and hospital, due to hospital-specific image artifacts. We highlight the trade-off one encounters when training on multiple datasets, between the obvious benefit of additional data and insidious cost of the introduced spurious correlation. In some cases, balancing the dataset can remove the spurious correlation and improve performance, but it is not always an effective strategy. We contextualize our results within the literature on spurious correlations to help explain these outcomes. Our experiments underscore the importance of exercising caution when selecting training data for machine learning models, especially in settings where there is a risk of spurious correlations such as with medical imaging. The risks outlined highlight the need for careful data selection and model evaluation in future research and practice.
</details>
<details>
<summary>摘要</summary>
在机器学习中，更多数据的添加经常被视为提高模型性能的可靠策略，但这项研究挑战了这一观点，示出在许多情况下，添加外部数据集可能会伤害模型的性能。我们在四个不同的开源胸部X射线数据集和九个标签的大规模实验中发现，在43%的情况下，由两个医院的数据进行训练的模型会比由单一医院数据进行训练的模型在最差群体准确率上表现较差。这种意外的结果尽管训练数据的分布变得更像测试数据的分布，但是由医院特有的图像artifacts导致的假 correlations的出现却导致这种情况。我们指出在多个数据集训练时，需要考虑这种交易的代价，即附加的数据可能会引入假 correlations，并且不一定有效地平衡数据集可以消除这种假 correlations。我们将这些结果与 литераature中的假 correlations相关的研究进行比较，以帮助解释这些结果。我们的实验表明，在机器学习模型训练中，特别是在医疗影像等领域，需要仔细选择数据，并且对模型进行仔细的评估。
</details></li>
</ul>
<hr>
<h2 id="SILO-Language-Models-Isolating-Legal-Risk-In-a-Nonparametric-Datastore"><a href="#SILO-Language-Models-Isolating-Legal-Risk-In-a-Nonparametric-Datastore" class="headerlink" title="SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore"></a>SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04430">http://arxiv.org/abs/2308.04430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kernelmachine/silo-lm">https://github.com/kernelmachine/silo-lm</a></li>
<li>paper_authors: Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettlemoyer</li>
<li>for: 这个论文是关于训练语言模型（LM）时的法律风险的研究。</li>
<li>methods: 这个论文使用了一种新的方法，即在推理时使用一个可 modify的非 Parametric 存储系统，以使用高风险数据而不需要在训练时使用它。这个存储系统支持句子级数据归属和数据生产者 opt-out 功能。</li>
<li>results: 实验表明，使用这种方法可以提高LM的性能，并且可以避免训练语言模型时的法律风险。<details>
<summary>Abstract</summary>
The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use regulations such as the fair use doctrine in the United States and the GDPR in the European Union. Our experiments show that the parametric LM struggles on domains not covered by OLC. However, access to the datastore greatly improves out of domain performance, closing 90% of the performance gap with an LM trained on the Pile, a more diverse corpus with mostly high-risk text. We also analyze which nonparametric approach works best, where the remaining errors lie, and how performance scales with datastore size. Our results suggest that it is possible to build high quality language models while mitigating their legal risk.
</details>
<details>
<summary>摘要</summary>
法律性的语言模型（LM）在版权保护或限制数据上训练是正在激烈讨论中。然而，我们显示，如果只在低风险文本（如公共领域书籍或政府文档）上训练模型，则模型的性能会很差，因为它们的规模和领域覆盖率都很有限。我们介绍了一种新的语言模型，叫做SILO，它在推理过程中处理这种风险和性能之间的贸易。SILO通过（1）使用公共领域和允许使用的文本集（Open License Corpus，OLC）训练 Parametric LM，并（2）在推理过程中使用更通用和可修改的非参数数据存储（如包含版权书籍或新闻的数据）来增强性能。这个数据存储允许在推理过程中使用高风险数据，支持句子水平的数据归属，并允许数据生产者在模型中删除内容，以遵守数据使用法规，如美国的公平使用 doctrine 和欧盟的 GDPR。我们的实验表明， parametric LM 在不受 OLC 覆盖的领域上表现不佳，但是通过访问数据存储，可以大幅提高 OUT  OF  DOMAIN 的表现，减少90%的表现差，与基于 Pile 的 LM 相比，Pile 是一个更加多样化的句子集，主要包含高风险文本。我们还分析了不 Parametric 的方法是哪种最佳，哪些错误还存在，以及数据存储大小如何影响性能。我们的结果表明，可以在法律风险下建立高质量的语言模型。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-Operators-to-Optimality-from-Multi-Task-Non-IID-Data"><a href="#Meta-Learning-Operators-to-Optimality-from-Multi-Task-Non-IID-Data" class="headerlink" title="Meta-Learning Operators to Optimality from Multi-Task Non-IID Data"></a>Meta-Learning Operators to Optimality from Multi-Task Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04428">http://arxiv.org/abs/2308.04428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas T. C. K. Zhang, Leonardo F. Toso, James Anderson, Nikolai Matni</li>
<li>for: 本研究目的是提高机器学习中的特征提取和泛化能力，通过从不同来源或任务中提取共同特征来减少计算量和统计泛化。</li>
<li>methods: 本文提出了一种恢复线性运算 matrix $M$ 从含有噪声的 вектор测量 $y &#x3D; Mx + w$ 中的方法，该方法可以处理非相关的数据和非对称的特征。</li>
<li>results: 研究人员在numerical simulations中证明了该方法的重要性，并提供了一种适应性的批处理方法 $\texttt{De-bias &amp; Feature-Whiten}$（$\texttt{DFW}$），该方法可以实现线性收敛到最佳表示，并且可以在不同来源数据中提取共同特征。<details>
<summary>Abstract</summary>
A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators $M$ from noisy vector measurements $y = Mx + w$, where the covariates $x$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, $\texttt{De-bias & Feature-Whiten}$ ($\texttt{DFW}$), of the popular alternating minimization-descent (AMD) scheme proposed in Collins et al., (2021), and establish linear convergence to the optimal representation with noise level scaling down with the $\textit{total}$ source data size. This leads to generalization bounds on the same order as an oracle empirical risk minimizer. We verify the vital importance of $\texttt{DFW}$ on various numerical simulations. In particular, we show that vanilla alternating-minimization descent fails catastrophically even for iid, but mildly non-isotropic data. Our analysis unifies and generalizes prior work, and provides a flexible framework for a wider range of applications, such as in controls and dynamical systems.
</details>
<details>
<summary>摘要</summary>
“一个强大的概念在现代机器学习中是提取资料来源或任务之间的共同特征。 intuitively，使用所有资料来学习共同表现函数可以节省计算努力和 statistically generalization，因为这样可以留下较少的参数来精确化每个任务。在理论基础上认养这些优点，我们提出了一个恢复线性算子 $M$ 的通用设定，其中 $y = Mx + w$ 是随机变数的 vector 观测，并且 $x$ 可能是非自相关的和非对称的。我们显示出现代不对称机器学习方法对于表现更新带来偏见，这会导致测量误差的构成因素发生不收敛的现象，从而导致表现学习的样本绩效被瓶颈在单一任务数据中。我们引入了一个适应化，即 $\texttt{De-bias & Feature-Whiten}$ ($\texttt{DFW}$)，它是 Collins et al. (2021) 提出的受欢迎的替换排程（AMD）的修改版本。我们证明了 $\texttt{DFW}$ 在数据大小增加时，随着 $\textit{total}$ 源数据大小下降的情况下，具有线性传播到最佳表现的优化。这导致了一个更加宽阔的应用范围，例如控制和动态系统。”
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Method-Using-Auto-encoder-and-Generative-Adversarial-Network-for-Anomaly-Detection-on-Ancient-Stone-Stele-Surfaces"><a href="#A-Deep-Learning-Method-Using-Auto-encoder-and-Generative-Adversarial-Network-for-Anomaly-Detection-on-Ancient-Stone-Stele-Surfaces" class="headerlink" title="A Deep-Learning Method Using Auto-encoder and Generative Adversarial Network for Anomaly Detection on Ancient Stone Stele Surfaces"></a>A Deep-Learning Method Using Auto-encoder and Generative Adversarial Network for Anomaly Detection on Ancient Stone Stele Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04426">http://arxiv.org/abs/2308.04426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikun Liu, Yuning Wang, Cheng Liu</li>
<li>for: 这个研究旨在提供一个深度学习方法，用于自动检测古代石刻上的自然衰老和人工损坏，以提高文物保护的精度和效率。</li>
<li>methods: 本研究使用了自动Encoder（AE）和生成对抗网络（GAN）deep learning方法，不需要大量的异常样本，并能够全面检测不可预测的异常。</li>
<li>results: 这个研究在长门洞窟的石刻上进行了一个实验，使用了一个无监理学习模型，实现了99.74%的重建精度，可以准确地检测到七种人工设计的异常。<details>
<summary>Abstract</summary>
Accurate detection of natural deterioration and man-made damage on the surfaces of ancient stele in the first instance is essential for their preventive conservation. Existing methods for cultural heritage preservation are not able to achieve this goal perfectly due to the difficulty of balancing accuracy, efficiency, timeliness, and cost. This paper presents a deep-learning method to automatically detect above mentioned emergencies on ancient stone stele in real time, employing autoencoder (AE) and generative adversarial network (GAN). The proposed method overcomes the limitations of existing methods by requiring no extensive anomaly samples while enabling comprehensive detection of unpredictable anomalies. the method includes stages of monitoring, data acquisition, pre-processing, model structuring, and post-processing. Taking the Longmen Grottoes' stone steles as a case study, an unsupervised learning model based on AE and GAN architectures is proposed and validated with a reconstruction accuracy of 99.74\%. The method's evaluation revealed the proficient detection of seven artificially designed anomalies and demonstrated precision and reliability without false alarms. This research provides novel ideas and possibilities for the application of deep learning in the field of cultural heritage.
</details>
<details>
<summary>摘要</summary>
准确检测古代碑刻表面的自然衰丧和人工损害在第一时间是保护古迹的必要前提。现有的文化遗产保护方法并不能完美实现这一目标，因为很难平衡准确率、效率、时效性和成本。这篇论文提出了一种基于深度学习的方法，可以在实时中自动检测古石碑刻表面的紧急情况，使用自适应神经网络（AE）和生成对抗网络（GAN）。该方法超越了现有方法的局限性，不需要大量的异常样本，同时能够全面检测不可预测的异常。该方法包括监测、数据收集、预处理、模型设计和后处理等阶段。使用长门石窟的石碑刻为实例，提出了一种无监督学习模型，并在 reconstruction 精度达99.74%。模型的评估表明了高精度、可靠性和无假警示。这些研究提供了深度学习在文化遗产保护领域的新想法和可能性。
</details></li>
</ul>
<hr>
<h2 id="DiffCR-A-Fast-Conditional-Diffusion-Framework-for-Cloud-Removal-from-Optical-Satellite-Images"><a href="#DiffCR-A-Fast-Conditional-Diffusion-Framework-for-Cloud-Removal-from-Optical-Satellite-Images" class="headerlink" title="DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from Optical Satellite Images"></a>DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from Optical Satellite Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04417">http://arxiv.org/abs/2308.04417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuechao Zou, Kai Li, Junliang Xing, Yu Zhang, Shiying Wang, Lei Jin, Pin Tao</li>
<li>For: The paper aims to develop a novel framework for effective cloud removal from optical satellite images using conditional guided diffusion with deep convolutional networks.* Methods: The proposed method, called DiffCR, leverages a decoupled encoder for conditional image feature extraction and a novel time and condition fusion block to accurately simulate the correspondence between the appearance in the conditional image and the target image.* Results: The proposed method achieves state-of-the-art performance on all metrics, with reduced parameter and computational complexities compared to previous best methods, as demonstrated through extensive experimental evaluations on two benchmark datasets.Here’s the simplified Chinese text for the three key points:* 为: 本文目的是通过干扰导向扩散与深度卷积网络来提出一种高性能的云除法方法 для光学卫星图像。* 方法: 提议的方法称为DiffCR，它利用独立的编码器来提取条件图像的特征，以确保 Close similarity of appearance information between the conditional input and the synthesized output。此外，它还提出了一种新的时间和条件融合块，以准确地模拟条件图像中的出现与目标图像的相似性。* 结果: 根据实验证明，DiffCR在两个常用的 benchmark 数据集上具有最佳性能，与前一代方法的参数和计算复杂度相比，它的参数和计算复杂度分别降低到5.1%和5.4%。源代码、预训练模型和所有实验结果将在<a target="_blank" rel="noopener" href="https://github.com/XavierJiezou/DiffCR">https://github.com/XavierJiezou/DiffCR</a> 上公开。<details>
<summary>Abstract</summary>
Optical satellite images are a critical data source; however, cloud cover often compromises their quality, hindering image applications and analysis. Consequently, effectively removing clouds from optical satellite images has emerged as a prominent research direction. While recent advancements in cloud removal primarily rely on generative adversarial networks, which may yield suboptimal image quality, diffusion models have demonstrated remarkable success in diverse image-generation tasks, showcasing their potential in addressing this challenge. This paper presents a novel framework called DiffCR, which leverages conditional guided diffusion with deep convolutional networks for high-performance cloud removal for optical satellite imagery. Specifically, we introduce a decoupled encoder for conditional image feature extraction, providing a robust color representation to ensure the close similarity of appearance information between the conditional input and the synthesized output. Moreover, we propose a novel and efficient time and condition fusion block within the cloud removal model to accurately simulate the correspondence between the appearance in the conditional image and the target image at a low computational cost. Extensive experimental evaluations on two commonly used benchmark datasets demonstrate that DiffCR consistently achieves state-of-the-art performance on all metrics, with parameter and computational complexities amounting to only 5.1% and 5.4%, respectively, of those previous best methods. The source code, pre-trained models, and all the experimental results will be publicly available at https://github.com/XavierJiezou/DiffCR upon the paper's acceptance of this work.
</details>
<details>
<summary>摘要</summary>
依靠光学卫星图像的数据源是一项关键的研究方向，但是云覆盖往往会降低图像质量，使图像应用和分析变得困难。因此，去云化 optical satellite images 成为了一项显著的研究方向。当前的云 removal 技术主要基于生成 adversarial networks，但这些网络可能会生成低质量的图像。 diffusion models 在多种图像生成任务中表现出色，这表明它们可能会解决这个挑战。这篇文章介绍了一种新的框架，称为 DiffCR，它利用 conditional guided diffusion 和深度卷积神经网络来实现高性能的云 removal for optical satellite imagery。 Specifically, we introduce a decoupled encoder for conditional image feature extraction, providing a robust color representation to ensure the close similarity of appearance information between the conditional input and the synthesized output. Moreover, we propose a novel and efficient time and condition fusion block within the cloud removal model to accurately simulate the correspondence between the appearance in the conditional image and the target image at a low computational cost. 经验证实验表明，DiffCR 在两个常用的标准 benchmark dataset 上 consistently 实现了状态机器的性能，与参数和计算复杂度相对只有5.1%和5.4%，分别相对较少于之前的最佳方法。源代码、预训练模型和所有实验结果将在https://github.com/XavierJiezou/DiffCR 上公开发布， waits for the paper's acceptance of this work.
</details></li>
</ul>
<hr>
<h2 id="Vector-Embeddings-by-Sequence-Similarity-and-Context-for-Improved-Compression-Similarity-Search-Clustering-Organization-and-Manipulation-of-cDNA-Libraries"><a href="#Vector-Embeddings-by-Sequence-Similarity-and-Context-for-Improved-Compression-Similarity-Search-Clustering-Organization-and-Manipulation-of-cDNA-Libraries" class="headerlink" title="Vector Embeddings by Sequence Similarity and Context for Improved Compression, Similarity Search, Clustering, Organization, and Manipulation of cDNA Libraries"></a>Vector Embeddings by Sequence Similarity and Context for Improved Compression, Similarity Search, Clustering, Organization, and Manipulation of cDNA Libraries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05118">http://arxiv.org/abs/2308.05118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel H. Um, David A. Knowles, Gail E. Kaiser</li>
<li>for: 本研究旨在解决 flat string gene format (FASTA&#x2F;FASTQ5) 中存在的一些问题，如大型文件Size、慢速mapping和对Alignment进行处理，以及Contextual dependencies。这些挑战使得对相似序列进行查找变得困难。</li>
<li>methods: 本研究使用 organized numerical representations of genes 来解决这些问题。具体来说，通过将短序列赋予唯一的vector embedding，可以更有效地将相似序列 clustering 到同一个组中，并提高了压缩性能。此外，通过基于 codon triplets 的上下文学习 alternatively coordinate vector embeddings，可以基于 amino acid 性质进行 clustering。</li>
<li>results: 通过使用这种序列嵌入方法，可以快速地查找相似序列，并且可以 Coupling vector embeddings 与一种基于 Euclidean 空间的算法来确定vector proximity，从而提高了序列相似性搜索的时间复杂度。<details>
<summary>Abstract</summary>
This paper demonstrates the utility of organized numerical representations of genes in research involving flat string gene formats (i.e., FASTA/FASTQ5). FASTA/FASTQ files have several current limitations, such as their large file sizes, slow processing speeds for mapping and alignment, and contextual dependencies. These challenges significantly hinder investigations and tasks that involve finding similar sequences. The solution lies in transforming sequences into an alternative representation that facilitates easier clustering into similar groups compared to the raw sequences themselves. By assigning a unique vector embedding to each short sequence, it is possible to more efficiently cluster and improve upon compression performance for the string representations of cDNA libraries. Furthermore, through learning alternative coordinate vector embeddings based on the contexts of codon triplets, we can demonstrate clustering based on amino acid properties. Finally, using this sequence embedding method to encode barcodes and cDNA sequences, we can improve the time complexity of the similarity search by coupling vector embeddings with an algorithm that determines the proximity of vectors in Euclidean space; this allows us to perform sequence similarity searches in a quicker and more modular fashion.
</details>
<details>
<summary>摘要</summary>
The authors assign a unique vector embedding to each short sequence, allowing for more efficient clustering and improved compression performance for the string representations of cDNA libraries. Additionally, the authors use alternative coordinate vector embeddings based on the contexts of codon triplets to demonstrate clustering based on amino acid properties.Furthermore, the authors use this sequence embedding method to encode barcodes and cDNA sequences, allowing for quicker and more modular sequence similarity searches by coupling vector embeddings with an algorithm that determines the proximity of vectors in Euclidean space. This approach improves the time complexity of the similarity search, enabling researchers to perform searches more efficiently.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Invariant-Learning-with-Randomized-Linear-Classifiers"><a href="#Probabilistic-Invariant-Learning-with-Randomized-Linear-Classifiers" class="headerlink" title="Probabilistic Invariant Learning with Randomized Linear Classifiers"></a>Probabilistic Invariant Learning with Randomized Linear Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04412">http://arxiv.org/abs/2308.04412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Cotta, Gal Yehuda, Assaf Schuster, Chris J. Maddison</li>
<li>for: 这篇论文是关于设计能够保持 Task 的表达能力和知识维护的模型的研究。</li>
<li>methods: 该论文使用随机化的方法，包括随机线性分类器（RLCs），以降低资源消耗而保持 Task 的表达能力和不变性。</li>
<li>results: 该论文通过验证RLCs在不同数据集上的表现，证明了这种随机化模型可以在保持 Task 的表达能力和不变性的情况下，使用较少的资源。<details>
<summary>Abstract</summary>
Designing models that are both expressive and preserve known invariances of tasks is an increasingly hard problem. Existing solutions tradeoff invariance for computational or memory resources. In this work, we show how to leverage randomness and design models that are both expressive and invariant but use less resources. Inspired by randomized algorithms, our key insight is that accepting probabilistic notions of universal approximation and invariance can reduce our resource requirements. More specifically, we propose a class of binary classification models called Randomized Linear Classifiers (RLCs). We give parameter and sample size conditions in which RLCs can, with high probability, approximate any (smooth) function while preserving invariance to compact group transformations. Leveraging this result, we design three RLCs that are provably probabilistic invariant for classification tasks over sets, graphs, and spherical data. We show how these models can achieve probabilistic invariance and universality using less resources than (deterministic) neural networks and their invariant counterparts. Finally, we empirically demonstrate the benefits of this new class of models on invariant tasks where deterministic invariant neural networks are known to struggle.
</details>
<details>
<summary>摘要</summary>
设计模型能够同时表达力和保持已知的任务变换是一个日益困难的问题。现有的解决方案通常会让计算资源或内存储存量增加。在这项工作中，我们表明可以通过随机性来设计模型，以便同时保持表达力和变换不变性，并使用更少的资源。我们的关键发现是，接受随机的概念 universal approximation 和不变性可以降低我们的资源需求。更 specifically，我们提出了一类基于随机化的线性模型，即 Randomized Linear Classifiers (RLCs)。我们给出了参数和样本大小的条件，在这些条件下，RLCs 可以，高概率地，将任何（平滑）函数approximate，同时保持 compact group transformation 的不变性。利用这个结果，我们设计了三种 RLCs，其中每种都是具有概率不变性的对 classification tasks over sets, graphs, and spherical data 进行了验证。我们证明这些模型可以在不同的任务上实现概率不变性和通用性，并且使用更少的资源 чем（deterministic）神经网络和其不变counterparts。最后，我们通过实验证明这些新类型的模型在不变任务上具有优势。
</details></li>
</ul>
<hr>
<h2 id="XGBD-Explanation-Guided-Graph-Backdoor-Detection"><a href="#XGBD-Explanation-Guided-Graph-Backdoor-Detection" class="headerlink" title="XGBD: Explanation-Guided Graph Backdoor Detection"></a>XGBD: Explanation-Guided Graph Backdoor Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04406">http://arxiv.org/abs/2308.04406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guanzihan/gnn_backdoor_detection">https://github.com/guanzihan/gnn_backdoor_detection</a></li>
<li>paper_authors: Zihan Guan, Mengnan Du, Ninghao Liu</li>
<li>For: 本研究旨在提出一种基于 topological feature 的 explanation-guided Graph Neural Network (GNN) 后门检测方法，以扩展现有的检测方法在图数据上的应用。* Methods: 本方法首先在 GNN 模型上进行训练，然后采用解释方法来归属模型预测结果到一个重要的子图。通过分析各个样本的归属结果，可以发现后门样本的归属分布与清洁样本不同，因此可以用这些归属结果来检测后门样本。* Results: 对多个流行的数据集和攻击方法进行了广泛的实验，结果表明我们的方法可以准确地检测后门样本，同时也可以提供明确的解释。代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/GuanZihan/GNN_backdoor_detection%E3%80%82">https://github.com/GuanZihan/GNN_backdoor_detection。</a><details>
<summary>Abstract</summary>
Backdoor attacks pose a significant security risk to graph learning models. Backdoors can be embedded into the target model by inserting backdoor triggers into the training dataset, causing the model to make incorrect predictions when the trigger is present. To counter backdoor attacks, backdoor detection has been proposed. An emerging detection strategy in the vision and NLP domains is based on an intriguing phenomenon: when training models on a mixture of backdoor and clean samples, the loss on backdoor samples drops significantly faster than on clean samples, allowing backdoor samples to be easily detected by selecting samples with the lowest loss values. However, the ignorance of topological feature information on graph data limits its detection effectiveness when applied directly to the graph domain. To this end, we propose an explanation-guided backdoor detection method to take advantage of the topological information. Specifically, we train a helper model on the graph dataset, feed graph samples into the model, and then adopt explanation methods to attribute model prediction to an important subgraph. We observe that backdoor samples have distinct attribution distribution than clean samples, so the explanatory subgraph could serve as more discriminative features for detecting backdoor samples. Comprehensive experiments on multiple popular datasets and attack methods demonstrate the effectiveness and explainability of our method. Our code is available: https://github.com/GuanZihan/GNN_backdoor_detection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese:Backdoor attacks pose a significant security risk to graph learning models. Backdoors can be embedded into the target model by inserting backdoor triggers into the training dataset, causing the model to make incorrect predictions when the trigger is present. To counter backdoor attacks, backdoor detection has been proposed. An emerging detection strategy in the vision and NLP domains is based on an intriguing phenomenon: when training models on a mixture of backdoor and clean samples, the loss on backdoor samples drops significantly faster than on clean samples, allowing backdoor samples to be easily detected by selecting samples with the lowest loss values. However, the ignorance of topological feature information on graph data limits its detection effectiveness when applied directly to the graph domain. To this end, we propose an explanation-guided backdoor detection method to take advantage of the topological information. Specifically, we train a helper model on the graph dataset, feed graph samples into the model, and then adopt explanation methods to attribute model prediction to an important subgraph. We observe that backdoor samples have distinct attribution distribution than clean samples, so the explanatory subgraph could serve as more discriminative features for detecting backdoor samples. Comprehensive experiments on multiple popular datasets and attack methods demonstrate the effectiveness and explainability of our method. Our code is available: https://github.com/GuanZihan/GNN_backdoor_detection.Translate the text into Simplified Chinese:<<SYS>>针对图学习模型中的后门攻击，我们提出了一种具有扫描性的后门检测方法。在视觉和自然语言领域，利用训练集中混合后门和清洁样本的现象，模型的损失值对后门样本下降速度远比清洁样本快，因此可以通过选择损失值最低的样本来轻松地检测后门样本。然而，对图数据 direct 应用这种方法时，它的检测效果受到图数据的 topological 特征信息的限制。为此，我们提出了一种利用 topological 信息进行解释导向后门检测方法。具体来说，我们在图 dataset 上训练一个协助模型，然后将图样本传递给模型，并采用解释方法来归因模型预测结果到一个重要的子图。我们发现，后门样本的归因分布与清洁样本不同，因此可以通过解释子图来更好地检测后门样本。我们在多个流行的dataset和攻击方法上进行了广泛的实验，并证明了我们的方法的有效性和可解释性。我们的代码可以在 https://github.com/GuanZihan/GNN_backdoor_detection 上找到。
</details></li>
</ul>
<hr>
<h2 id="Event-Abstraction-for-Enterprise-Collaboration-Systems-to-Support-Social-Process-Mining"><a href="#Event-Abstraction-for-Enterprise-Collaboration-Systems-to-Support-Social-Process-Mining" class="headerlink" title="Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining"></a>Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04396">http://arxiv.org/abs/2308.04396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Blatt, Patrick Delfmann, Petra Schubert</li>
<li>For: The paper is written for the purpose of discovering process models from event logs of Enterprise Collaboration Systems (ECS), which are typically communication- and document-oriented.* Methods: The paper proposes a tailored event abstraction approach called ECSEA, which trains a model by comparing recorded actual user activities with the system-generated low-level traces, and allows for the automatic conversion of future low-level traces into an abstracted high-level log that can be used for Process Mining.* Results: The paper shows that the ECSEA approach produces accurate results, and is essential for the interpretation of collaborative work activity in ECS, which the authors call Social Process Mining.<details>
<summary>Abstract</summary>
One aim of Process Mining (PM) is the discovery of process models from event logs of information systems. PM has been successfully applied to process-oriented enterprise systems but is less suited for communication- and document-oriented Enterprise Collaboration Systems (ECS). ECS event logs are very fine-granular and PM applied to their logs results in spaghetti models. A common solution for this is event abstraction, i.e., converting low-level logs into more abstract high-level logs before running discovery algorithms. ECS logs come with special characteristics that have so far not been fully addressed by existing event abstraction approaches. We aim to close this gap with a tailored ECS event abstraction (ECSEA) approach that trains a model by comparing recorded actual user activities (high-level traces) with the system-generated low-level traces (extracted from the ECS). The model allows us to automatically convert future low-level traces into an abstracted high-level log that can be used for PM. Our evaluation shows that the algorithm produces accurate results. ECSEA is a preprocessing method that is essential for the interpretation of collaborative work activity in ECS, which we call Social Process Mining.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Augmentation-Based-Unsupervised-Domain-Adaptation-In-Medical-Imaging"><a href="#Data-Augmentation-Based-Unsupervised-Domain-Adaptation-In-Medical-Imaging" class="headerlink" title="Data Augmentation-Based Unsupervised Domain Adaptation In Medical Imaging"></a>Data Augmentation-Based Unsupervised Domain Adaptation In Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04395">http://arxiv.org/abs/2308.04395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Nørgaard Llambias, Mads Nielsen, Mostafa Mehdipour Ghazi</li>
<li>for: 本研究旨在提出一种robust domain adaptation方法，以便在医疗影像领域应用深度学习模型。</li>
<li>methods: 该方法利用MRI特有的扩展技术来实现不supervised domain adaptation，并在多种数据集、模式和分类任务上进行了广泛的实验，与当前最佳方法进行了比较。</li>
<li>results: 研究结果显示，提议的方法在多种任务中具有高精度、广泛应用性和强大的鲁棒性，在大多数情况下超越了当前最佳性能。<details>
<summary>Abstract</summary>
Deep learning-based models in medical imaging often struggle to generalize effectively to new scans due to data heterogeneity arising from differences in hardware, acquisition parameters, population, and artifacts. This limitation presents a significant challenge in adopting machine learning models for clinical practice. We propose an unsupervised method for robust domain adaptation in brain MRI segmentation by leveraging MRI-specific augmentation techniques. To evaluate the effectiveness of our method, we conduct extensive experiments across diverse datasets, modalities, and segmentation tasks, comparing against the state-of-the-art methods. The results show that our proposed approach achieves high accuracy, exhibits broad applicability, and showcases remarkable robustness against domain shift in various tasks, surpassing the state-of-the-art performance in the majority of cases.
</details>
<details>
<summary>摘要</summary>
深度学习模型在医疗成像中经常陷于新扫描数据不具有效地泛化的问题，这是因为扫描设备、获取参数、人口和artefacts等因素引起的数据不一致性。这种限制使得机器学习模型在临床实践中采用困难。我们提议一种不supervised的robust领域适应方法，通过利用MRI特定的扩充技术来解决这个问题。为评估我们的方法的有效性，我们在多个数据集、Modalities和 segmentation任务中进行了广泛的实验，与当前状态艺的方法进行比较。结果表明，我们的提议方法可以 дости得高精度、具有广泛的应用性和在不同任务中强大的鲁棒性，在大多数情况下超越当前状态艺的性能。
</details></li>
</ul>
<hr>
<h2 id="Metaheuristic-Algorithms-in-Artificial-Intelligence-with-Applications-to-Bioinformatics-Biostatistics-Ecology-and-the-Manufacturing-Industries"><a href="#Metaheuristic-Algorithms-in-Artificial-Intelligence-with-Applications-to-Bioinformatics-Biostatistics-Ecology-and-the-Manufacturing-Industries" class="headerlink" title="Metaheuristic Algorithms in Artificial Intelligence with Applications to Bioinformatics, Biostatistics, Ecology and, the Manufacturing Industries"></a>Metaheuristic Algorithms in Artificial Intelligence with Applications to Bioinformatics, Biostatistics, Ecology and, the Manufacturing Industries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10875">http://arxiv.org/abs/2308.10875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elviscuihan/csoma">https://github.com/elviscuihan/csoma</a></li>
<li>paper_authors: Elvis Han Cui, Zizhao Zhang, Culsome Junwen Chen, Weng Kee Wong</li>
<li>for: 本研究使用一种新提出的自然引导算法，即竞争群体优化算法with mutated agents (CSO-MA)，以解决各种困难优化问题。</li>
<li>methods: 本研究使用CSO-MA算法，并证明其在不同的统计科学问题中的灵活性和竞争优势。</li>
<li>results: 研究发现，CSO-MA算法能够 efficiently 处理不同类型的优化问题，包括具有不同成本结构或多个用户指定的非线性约束的问题。应用包括生物信息学中的单细胞泛化趋势模型、教育研究中的 Rasch 模型、生命科学中的 Markov 级别模型以及两组织模型中的缺失值补充。此外，还应用到生态学问题中选取最佳变量，以及适用于汽车业的汽车燃油实验设计。<details>
<summary>Abstract</summary>
Nature-inspired metaheuristic algorithms are important components of artificial intelligence, and are increasingly used across disciplines to tackle various types of challenging optimization problems. We apply a newly proposed nature-inspired metaheuristic algorithm called competitive swarm optimizer with mutated agents (CSO-MA) and demonstrate its flexibility and out-performance relative to its competitors in a variety of optimization problems in the statistical sciences. In particular, we show the algorithm is efficient and can incorporate various cost structures or multiple user-specified nonlinear constraints. Our applications include (i) finding maximum likelihood estimates of parameters in a single cell generalized trend model to study pseudotime in bioinformatics, (ii) estimating parameters in a commonly used Rasch model in education research, (iii) finding M-estimates for a Cox regression in a Markov renewal model and (iv) matrix completion to impute missing values in a two compartment model. In addition we discuss applications to (v) select variables optimally in an ecology problem and (vi) design a car refueling experiment for the auto industry using a logistic model with multiple interacting factors.
</details>
<details>
<summary>摘要</summary>
自然适应的metaheuristic算法在人工智能中具有重要的地位，并在不同领域面临各种复杂的优化问题时广泛应用。我们在一种新提出的自然适应metaheuristic算法 called 竞争群体优化算法（CSO-MA）中，示出其灵活性和与竞争者相比的出色性。我们在各种统计科学中应用了这种算法，包括：（i）在生物信息学中，找到单个维度泛化趋势模型中参数的最大可能性 estimators。（ii）在教育研究中，估算一种常用的Rasch模型中的参数。（iii）在一种Markov征显模型中，找到一种M-估计来描述一种Cox回归。（iv）在一种两个分组模型中，完成缺失值的填充。（v）在生态学中，选择变量以优化一个生态系统。（vi）在自动化工业中，使用一种Logistic模型来设计一个汽车燃油实验。我们还讨论了这种算法在不同领域中的应用，包括生态学和自动化工业。
</details></li>
</ul>
<hr>
<h2 id="AdaptEx-A-Self-Service-Contextual-Bandit-Platform"><a href="#AdaptEx-A-Self-Service-Contextual-Bandit-Platform" class="headerlink" title="AdaptEx: A Self-Service Contextual Bandit Platform"></a>AdaptEx: A Self-Service Contextual Bandit Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08650">http://arxiv.org/abs/2308.08650</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Black, Ercument Ilhan, Andrea Marchini, Vilda Markeviciute</li>
<li>for: 这篇论文是为了描述一种基于多支针算法的自助上下文ual bandit平台，用于在Expedia Group中大规模个性化用户体验。</li>
<li>methods: 这篇论文使用了多支针算法，考虑每个访客的特定上下文，选择最佳变种，并快速学习每次互动。</li>
<li>results: 这篇论文表明，AdaptEx可以快速减少传统测试方法相关的成本和时间，并在不断变化的内容和连续”冷启”情况下灵活应对。<details>
<summary>Abstract</summary>
This paper presents AdaptEx, a self-service contextual bandit platform widely used at Expedia Group, that leverages multi-armed bandit algorithms to personalize user experiences at scale. AdaptEx considers the unique context of each visitor to select the optimal variants and learns quickly from every interaction they make. It offers a powerful solution to improve user experiences while minimizing the costs and time associated with traditional testing methods. The platform unlocks the ability to iterate towards optimal product solutions quickly, even in ever-changing content and continuous "cold start" situations gracefully.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Understanding-the-Effect-of-Counterfactual-Explanations-on-Trust-and-Reliance-on-AI-for-Human-AI-Collaborative-Clinical-Decision-Making"><a href="#Understanding-the-Effect-of-Counterfactual-Explanations-on-Trust-and-Reliance-on-AI-for-Human-AI-Collaborative-Clinical-Decision-Making" class="headerlink" title="Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making"></a>Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04375">http://arxiv.org/abs/2308.04375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Min Hun Lee, Chong Jun Chew<br>for: 这个论文旨在探讨人工智能（AI）在高度决策领域（如医疗）中如何帮助人类做出决策，以及如何使人类更好地依靠AI。methods: 该论文使用了许多特征解释以及对比解释来使人类更好地分析AI建议，从而减少对AI的过分依赖。results: 研究发现，当AI建议正确时，人类的性能和一致性都有所提高，而对于错误的AI建议，使用对比解释可以减少人类对AI的过分依赖，特别是对于非专业人员而言。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) is increasingly being considered to assist human decision-making in high-stake domains (e.g. health). However, researchers have discussed an issue that humans can over-rely on wrong suggestions of the AI model instead of achieving human AI complementary performance. In this work, we utilized salient feature explanations along with what-if, counterfactual explanations to make humans review AI suggestions more analytically to reduce overreliance on AI and explored the effect of these explanations on trust and reliance on AI during clinical decision-making. We conducted an experiment with seven therapists and ten laypersons on the task of assessing post-stroke survivors' quality of motion, and analyzed their performance, agreement level on the task, and reliance on AI without and with two types of AI explanations. Our results showed that the AI model with both salient features and counterfactual explanations assisted therapists and laypersons to improve their performance and agreement level on the task when `right' AI outputs are presented. While both therapists and laypersons over-relied on `wrong' AI outputs, counterfactual explanations assisted both therapists and laypersons to reduce their over-reliance on `wrong' AI outputs by 21\% compared to salient feature explanations. Specifically, laypersons had higher performance degrades by 18.0 f1-score with salient feature explanations and 14.0 f1-score with counterfactual explanations than therapists with performance degrades of 8.6 and 2.8 f1-scores respectively. Our work discusses the potential of counterfactual explanations to better estimate the accuracy of an AI model and reduce over-reliance on `wrong' AI outputs and implications for improving human-AI collaborative decision-making.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在高度重要的决策领域（如医疗）中被越来越多地用于帮助人类做出决策。然而，研究人员已经提出了一个问题，即人们可能会过分依赖错误的AI模型建议，而不是寻求人类和AI的合作性能。在这种情况下，我们使用了突出特征解释以及对比解释，以帮助人们更加分析地评估AI建议，并研究这些解释对人们对AI的信任和依赖的影响。我们在评估stroke生存者的质量运动任务上进行了一个实验，并分析了参与者的表现、同意度和对AI的依赖度。我们的结果表明，带有突出特征和对比解释的AI模型可以帮助治疗师和非专业人员提高他们的表现和同意度。而且，对比解释可以帮助治疗师和非专业人员减少对“错误”AI输出的过分依赖，相比突出特征解释下降21%。具体来说，非专业人员在使用突出特征解释时表现下降18.0 f1-score，而使用对比解释时表现下降14.0 f1-score，而治疗师则表现下降8.6和2.8 f1-score。我们的工作表明，对比解释可以更好地估计AI模型的准确性，降低对“错误”AI输出的过分依赖，并带来人类-AI合作决策的改进。
</details></li>
</ul>
<hr>
<h2 id="Pelta-Shielding-Transformers-to-Mitigate-Evasion-Attacks-in-Federated-Learning"><a href="#Pelta-Shielding-Transformers-to-Mitigate-Evasion-Attacks-in-Federated-Learning" class="headerlink" title="Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning"></a>Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04373">http://arxiv.org/abs/2308.04373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Queyrut, Yérom-David Bromberg, Valerio Schiavoni</li>
<li>for: 保护用户数据隐私和模型安全，防止恶意扰乱模型训练</li>
<li>methods: 使用可信硬件（TEEs）实现防御机制，屏蔽部分反propagation链规则，防止攻击者利用此规则设计恶意样本</li>
<li>results: 对state-of-the-art集成模型进行测试，证明PELTA有效防御Self Attention Gradient攻击<details>
<summary>Abstract</summary>
The main premise of federated learning is that machine learning model updates are computed locally, in particular to preserve user data privacy, as those never leave the perimeter of their device. This mechanism supposes the general model, once aggregated, to be broadcast to collaborating and non malicious nodes. However, without proper defenses, compromised clients can easily probe the model inside their local memory in search of adversarial examples. For instance, considering image-based applications, adversarial examples consist of imperceptibly perturbed images (to the human eye) misclassified by the local model, which can be later presented to a victim node's counterpart model to replicate the attack. To mitigate such malicious probing, we introduce Pelta, a novel shielding mechanism leveraging trusted hardware. By harnessing the capabilities of Trusted Execution Environments (TEEs), Pelta masks part of the back-propagation chain rule, otherwise typically exploited by attackers for the design of malicious samples. We evaluate Pelta on a state of the art ensemble model and demonstrate its effectiveness against the Self Attention Gradient adversarial Attack.
</details>
<details>
<summary>摘要</summary>
主要 premise 的 federated learning 是计算机机学模型更新在本地，特别是保护用户数据隐私，因为这些模型从没有离开设备的边界。这种机制假设总模型，一旦综合而成，可以广播到合作和不可靠的节点。然而，没有适当的防御，入侵的客户端可以轻松地探测模型内部的内容。例如，对于图像应用程序，攻击者可以通过在本地模型中添加微量的干扰（可以让人类无法注意）来让模型错分。这些攻击者可以后来将这些攻击样本传递给受害者节点的对等模型，以重新实现攻击。为了解决这种恶意探测问题，我们介绍了 Pelta，一种新的防御机制，利用可信硬件（TEEs）。 Pelta 隐藏了一部分的反射链规则，通常被攻击者利用来设计恶意样本。我们在一个 state-of-the-art 的 ensemble 模型上评估了 Pelta，并证明其效iveness 对Self Attention Gradient 攻击。
</details></li>
</ul>
<hr>
<h2 id="SLEM-Machine-Learning-for-Path-Modeling-and-Causal-Inference-with-Super-Learner-Equation-Modeling"><a href="#SLEM-Machine-Learning-for-Path-Modeling-and-Causal-Inference-with-Super-Learner-Equation-Modeling" class="headerlink" title="SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling"></a>SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04365">http://arxiv.org/abs/2308.04365</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matthewvowels1/slem">https://github.com/matthewvowels1/slem</a></li>
<li>paper_authors: Matthew J. Vowels</li>
<li>for: 这篇论文的目的是提出一种新的方法来实现可靠的效应大小估计，该方法基于机器学习Super Learner ensemble的思想。</li>
<li>methods: 该方法使用Path models和Structural Equation Models（SEM）的组合，并通过机器学习Super Learner ensemble来避免函数错误估计。</li>
<li>results: 该方法可以提供可靠的效应大小估计，并且在对比SEM时表现竞争力强，特别是当关系非线性时。<details>
<summary>Abstract</summary>
Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non-linear relationships. We provide open-source code, and a tutorial notebook with example usage, accentuating the easy-to-use nature of the method.
</details>
<details>
<summary>摘要</summary>
科学中的重要目标是 causal inference，即通过观察数据来得出干扰假设的有意义结论。路径模型、结构方程模型（SEM）和直接无环图（DAG）等方法可以准确地表达 causal 结构的假设。与 SEM 不同的是，它假设 linearity，这可能导致函数不准确，从而阻碍研究人员从数据中获得可靠的效果大小估计。在这个 контексте，我们提出 Super Learner Equation Modeling，一种将机器学习 Super Learner 集成的路径模型技术。我们经验表明，它可以提供一致和不偏估计的 causal 效果，并且在 linear 模型中与 SEM 相比，其性能更加竞争。此外，它在非线性关系时也表现出了superiority。我们提供了开源代码和教程 Notebook，强调该方法的易用性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/09/cs.LG_2023_08_09/" data-id="clly4xtdt006fvl889lv1g5uy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/09/cs.SD_2023_08_09/" class="article-date">
  <time datetime="2023-08-08T16:00:00.000Z" itemprop="datePublished">2023-08-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/09/cs.SD_2023_08_09/">cs.SD - 2023-08-09 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unsupervised-Out-of-Distribution-Dialect-Detection-with-Mahalanobis-Distance"><a href="#Unsupervised-Out-of-Distribution-Dialect-Detection-with-Mahalanobis-Distance" class="headerlink" title="Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance"></a>Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04886">http://arxiv.org/abs/2308.04886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sourya Dipta Das, Yash Vadi, Abhishek Unnam, Kuldeep Yadav</li>
<li>for: 本研究旨在提高 диалект分类模型在实际应用中的性能，特别是对异常输入样本的检测。</li>
<li>methods: 我们提出了一种简单 yet effective的无监督 Mahalanobis 距离特征基于方法，用于检测异常输入样本。我们利用 transformer 基于 dialet classifier 模型的所有中间层的卷积特征进行多任务学习。</li>
<li>results: 我们的提出方法与其他现有的 OOD 检测方法相比，显著地提高了检测性能。<details>
<summary>Abstract</summary>
Dialect classification is used in a variety of applications, such as machine translation and speech recognition, to improve the overall performance of the system. In a real-world scenario, a deployed dialect classification model can encounter anomalous inputs that differ from the training data distribution, also called out-of-distribution (OOD) samples. Those OOD samples can lead to unexpected outputs, as dialects of those samples are unseen during model training. Out-of-distribution detection is a new research area that has received little attention in the context of dialect classification. Towards this, we proposed a simple yet effective unsupervised Mahalanobis distance feature-based method to detect out-of-distribution samples. We utilize the latent embeddings from all intermediate layers of a wav2vec 2.0 transformer-based dialect classifier model for multi-task learning. Our proposed approach outperforms other state-of-the-art OOD detection methods significantly.
</details>
<details>
<summary>摘要</summary>
dialet 分类在各种应用中使用，如机器翻译和语音识别，以提高整体系统性能。在真实场景中，部署的 диалект分类模型可能会遇到不同于训练数据分布的输入样本，也就是 OUT-OF-DISTRIBUTION（OOD）样本。这些 OOD 样本会导致模型输出不符预期的结果，因为这些 диалект样本在模型训练中未经见过。OUT-OF-DISTRIBUTION 检测是一个新的研究领域，在 диалект分类方面受到了少量的关注。为了解决这个问题，我们提出了一种简单 yet 有效的无监督 Mahalanobis 距离特征基于方法。我们利用了一个 wav2vec 2.0 基于 transformer 的 диалект分类模型的所有间层精神嵌入。我们的提议方法在与其他现有 OOD 检测方法进行比较时表现出色。
</details></li>
</ul>
<hr>
<h2 id="DiVa-An-Iterative-Framework-to-Harvest-More-Diverse-and-Valid-Labels-from-User-Comments-for-Music"><a href="#DiVa-An-Iterative-Framework-to-Harvest-More-Diverse-and-Valid-Labels-from-User-Comments-for-Music" class="headerlink" title="DiVa: An Iterative Framework to Harvest More Diverse and Valid Labels from User Comments for Music"></a>DiVa: An Iterative Framework to Harvest More Diverse and Valid Labels from User Comments for Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04805">http://arxiv.org/abs/2308.04805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingyaolliu/diva">https://github.com/jingyaolliu/diva</a></li>
<li>paper_authors: Hongru Liang, Jingyao Liu, Yuanxin Xiang, Jiachen Du, Lanjun Zhou, Shushen Pan, Wenqiang Lei</li>
<li>for: 提高自动音乐标签的完整性，增加更多的标签多样性</li>
<li>methods: 基于用户评论中的信息，提出一个迭代式框架（DiVa），使用预训练分类器和新的联合分数函数，生成更多的多样化和有效标签</li>
<li>results: 对 densely annotated 测试集进行实验，发现 DiVa 比现有方法更有优势，能够生成更多的被金标签所排除的多样化标签<details>
<summary>Abstract</summary>
Towards sufficient music searching, it is vital to form a complete set of labels for each song. However, current solutions fail to resolve it as they cannot produce diverse enough mappings to make up for the information missed by the gold labels. Based on the observation that such missing information may already be presented in user comments, we propose to study the automated music labeling in an essential but under-explored setting, where the model is required to harvest more diverse and valid labels from the users' comments given limited gold labels. To this end, we design an iterative framework (DiVa) to harvest more $\underline{\text{Di}}$verse and $\underline{\text{Va}}$lid labels from user comments for music. The framework makes a classifier able to form complete sets of labels for songs via pseudo-labels inferred from pre-trained classifiers and a novel joint score function. The experiment on a densely annotated testing set reveals the superiority of the Diva over state-of-the-art solutions in producing more diverse labels missed by the gold labels. We hope our work can inspire future research on automated music labeling.
</details>
<details>
<summary>摘要</summary>
向充分的音乐搜索，完善每首歌的标签集是非常重要的。然而，现有的解决方案无法解决这个问题，因为它们无法生成够多样化的映射，以补做金标签中的信息损失。根据用户评论中可能存在的缺失信息的观察，我们提议研究自动化音乐标签化在不足explored的设定下进行，其中模型需要从用户评论中抽取更多的多样化和有效的标签。为此，我们设计了一个迭代框架（DiVa），可以从用户评论中抽取更多的多样化和有效的标签。该框架包括一个基于预训练分类器的pseudo标签INFERRED和一个新的联合分数函数。我们的实验表明，DiVa在一个稠密注解的测试集上表现出色，可以生成更多的多样化的标签，而这些标签在金标签中缺失。我们希望我们的工作能够激励未来的音乐标签化研究。
</details></li>
</ul>
<hr>
<h2 id="Induction-Network-Audio-Visual-Modality-Gap-Bridging-for-Self-Supervised-Sound-Source-Localization"><a href="#Induction-Network-Audio-Visual-Modality-Gap-Bridging-for-Self-Supervised-Sound-Source-Localization" class="headerlink" title="Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization"></a>Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04767">http://arxiv.org/abs/2308.04767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tahy1/avin">https://github.com/tahy1/avin</a></li>
<li>paper_authors: Tianyu Liu, Peng Zhang, Wei Huang, Yufei Zha, Tao You, Yanning Zhang</li>
<li>for: 本研究旨在提高自监督音源位置Localization的性能，尤其是在视觉场景中受到模态不一致的挑战。</li>
<li>methods: 本研究提出了一种Induction Network，用于更有效地bridging模态差异。该网络通过分离视觉和声音模态的梯度，使得可以学习出sound source的描述性视觉表示，并使声音模态与视觉模式一致。此外，还 introduce了一种适应阈值选择策略，以提高Induction Network的 Robustness。</li>
<li>results: 在SoundNet-Flickr和VGG-Sound Source等 dataset上进行了大量实验，并达到了与其他状态 ante Works 相比的更高性能。<details>
<summary>Abstract</summary>
Self-supervised sound source localization is usually challenged by the modality inconsistency. In recent studies, contrastive learning based strategies have shown promising to establish such a consistent correspondence between audio and sound sources in visual scenarios. Unfortunately, the insufficient attention to the heterogeneity influence in the different modality features still limits this scheme to be further improved, which also becomes the motivation of our work. In this study, an Induction Network is proposed to bridge the modality gap more effectively. By decoupling the gradients of visual and audio modalities, the discriminative visual representations of sound sources can be learned with the designed Induction Vector in a bootstrap manner, which also enables the audio modality to be aligned with the visual modality consistently. In addition to a visual weighted contrastive loss, an adaptive threshold selection strategy is introduced to enhance the robustness of the Induction Network. Substantial experiments conducted on SoundNet-Flickr and VGG-Sound Source datasets have demonstrated a superior performance compared to other state-of-the-art works in different challenging scenarios. The code is available at https://github.com/Tahy1/AVIN
</details>
<details>
<summary>摘要</summary>
自我监督的声音源localization通常会遇到Modalit inconsistency。在最近的研究中，基于对比学习的策略已经显示出了可行地建立了视觉场景中声音源与声音的相对匹配。然而，各种杂化特征之间的不同特征仍然限制了这种方案的进一步改进，这也成为了我们的研究动机。在这种研究中，一个Induction Network被提出，可以更有效地跨模态桥接。通过解除视觉和声音模态的梯度，可以学习出特征映射，并且通过设计的引入向量，可以在bootstrapmanner学习出推荐视觉表示。此外，还 introduce了一种可适应阈值选择策略，以提高Induction Network的Robustness。在SoundNet-Flickr和VGG-Sound Source datasets上进行了substantial实验，与其他state-of-the-art工作在不同的挑战场景中均显示出了superior表现。代码可以在https://github.com/Tahy1/AVIN中获取。
</details></li>
</ul>
<hr>
<h2 id="Speaker-Recognition-Using-Isomorphic-Graph-Attention-Network-Based-Pooling-on-Self-Supervised-Representation"><a href="#Speaker-Recognition-Using-Isomorphic-Graph-Attention-Network-Based-Pooling-on-Self-Supervised-Representation" class="headerlink" title="Speaker Recognition Using Isomorphic Graph Attention Network Based Pooling on Self-Supervised Representation"></a>Speaker Recognition Using Isomorphic Graph Attention Network Based Pooling on Self-Supervised Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04666">http://arxiv.org/abs/2308.04666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zirui Ge, Xinzhou Xu, Haiyan Guo, Tingting Wang, Zhen Yang</li>
<li>for: 本研究旨在提出一种基于自适应表示（i.e., wav2vec 2.0）的Speaker recognition方法，以便利用基于语音数据建立的基础模型进行处理。</li>
<li>methods: 本方法包括三个模块：表示学习、图注意力和汇集，它们同时考虑了学习基于自适应表示以及IsoGAT。</li>
<li>results: 在VoxCeleb1＆2 datasets上进行的实验结果表明，提议的方法可以与现有的pooling方法相比，提高Speaker recognition的性能。<details>
<summary>Abstract</summary>
The emergence of self-supervised representation (i.e., wav2vec 2.0) allows speaker-recognition approaches to process spoken signals through foundation models built on speech data. Nevertheless, effective fusion on the representation requires further investigating, due to the inclusion of fixed or sub-optimal temporal pooling strategies. Despite of improved strategies considering graph learning and graph attention factors, non-injective aggregation still exists in the approaches, which may influence the performance for speaker recognition. In this regard, we propose a speaker recognition approach using Isomorphic Graph ATtention network (IsoGAT) on self-supervised representation. The proposed approach contains three modules of representation learning, graph attention, and aggregation, jointly considering learning on the self-supervised representation and the IsoGAT. Then, we perform experiments for speaker recognition tasks on VoxCeleb1\&2 datasets, with the corresponding experimental results demonstrating the recognition performance for the proposed approach, compared with existing pooling approaches on the self-supervised representation.
</details>
<details>
<summary>摘要</summary>
“自我超级表示（即wave2vec 2.0）的出现允许语音识别方法通过基于语音数据建立的基础模型进行处理。然而，更好地融合表示需要进一步研究，因为包括 fixes 或不优化的时间池化策略。尽管考虑了基于图学习和图注意因素的改进策略，但是非射影聚合仍然存在在这些方法中，这可能影响语音识别的性能。在这种情况下，我们提出一种基于自同型图注意网络（IsoGAT）的语音识别方法。该方法包括三个模块：表示学习、图注意和聚合，同时考虑学习自同型表示和IsoGAT。然后，我们在VoxCeleb1&2 dataset上进行了语音识别任务的实验，并得到了相对于现有池化方法的自同型表示性能。”Note that Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/09/cs.SD_2023_08_09/" data-id="clly4xteo009qvl887js66yel" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/7/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/9/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">108</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
