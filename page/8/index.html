
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/8/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_11_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/11/cs.CV_2023_11_11/" class="article-date">
  <time datetime="2023-11-11T13:00:00.000Z" itemprop="datePublished">2023-11-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/11/cs.CV_2023_11_11/">cs.CV - 2023-11-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="3DFusion-A-real-time-3D-object-reconstruction-pipeline-based-on-streamed-instance-segmented-data"><a href="#3DFusion-A-real-time-3D-object-reconstruction-pipeline-based-on-streamed-instance-segmented-data" class="headerlink" title="3DFusion, A real-time 3D object reconstruction pipeline based on streamed instance segmented data"></a>3DFusion, A real-time 3D object reconstruction pipeline based on streamed instance segmented data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06659">http://arxiv.org/abs/2311.06659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Sun, Derek Jacoby, Yvonne Coady<br>for:这篇论文旨在提供一个实时分割和重建系统，该系统使用RGB-D图像来生成准确和详细的indoor环境中对象的三维模型。methods:该系统采用了当今最佳实例分割技术，对RGB-D数据进行像素级分割，以分割背景和前景对象。然后，通过高性能计算平台，对分割的对象进行三维重建。results:该系统可以实现实时三维模elling，并且可以应用于各种领域，如增强现实&#x2F;虚拟现实、内部设计、城市规划、公路协助、安全系统等。为了实现实时性，论文提出了一种方法，通过采样连续帧来减少网络负担，保证重建质量。此外，该系统采用了并行SLAM管道，以高效地将分割对象切割成个体。该系统使用了领先的框架YOLO进行实例分割，并对YOLO进行修改，以解决类似对象的重复或假检测问题，确保重建模型与目标对象保持一致。总之，该工作建立了一个可靠的实时系统，对indoor环境中对象的分割和重建进行了显著提高。它可能会扩展到户外场景，开启了许多实际应用的可能性。<details>
<summary>Abstract</summary>
This paper presents a real-time segmentation and reconstruction system that utilizes RGB-D images to generate accurate and detailed individual 3D models of objects within a captured scene. Leveraging state-of-the-art instance segmentation techniques, the system performs pixel-level segmentation on RGB-D data, effectively separating foreground objects from the background. The segmented objects are then reconstructed into distinct 3D models in a high-performance computation platform. The real-time 3D modelling can be applied across various domains, including augmented/virtual reality, interior design, urban planning, road assistance, security systems, and more. To achieve real-time performance, the paper proposes a method that effectively samples consecutive frames to reduce network load while ensuring reconstruction quality. Additionally, a multi-process SLAM pipeline is adopted for parallel 3D reconstruction, enabling efficient cutting of the clustering objects into individuals. This system employs the industry-leading framework YOLO for instance segmentation. To improve YOLO's performance and accuracy, modifications were made to resolve duplicated or false detection of similar objects, ensuring the reconstructed models align with the targets. Overall, this work establishes a robust real-time system with a significant enhancement for object segmentation and reconstruction in the indoor environment. It can potentially be extended to the outdoor scenario, opening up numerous opportunities for real-world applications.
</details>
<details>
<summary>摘要</summary>
To achieve real-time performance, the paper proposes a method that effectively samples consecutive frames to reduce network load while ensuring reconstruction quality. Additionally, a multi-process SLAM pipeline is adopted for parallel 3D reconstruction, enabling efficient cutting of the clustering objects into individuals. The system employs the industry-leading framework YOLO for instance segmentation, with modifications made to resolve duplicated or false detection of similar objects, ensuring the reconstructed models align with the targets.Overall, this work establishes a robust real-time system with a significant enhancement for object segmentation and reconstruction in the indoor environment. It can potentially be extended to the outdoor scenario, opening up numerous opportunities for real-world applications.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-and-semi-supervised-co-salient-object-detection-via-segmentation-frequency-statistics"><a href="#Unsupervised-and-semi-supervised-co-salient-object-detection-via-segmentation-frequency-statistics" class="headerlink" title="Unsupervised and semi-supervised co-salient object detection via segmentation frequency statistics"></a>Unsupervised and semi-supervised co-salient object detection via segmentation frequency statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06654">http://arxiv.org/abs/2311.06654</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sourachakra/uscosod-sscosod">https://github.com/sourachakra/uscosod-sscosod</a></li>
<li>paper_authors: Souradeep Chakraborty, Shujon Naha, Muhammet Bastan, Amit Kumar K C, Dimitris Samaras</li>
<li>for: 本文提出了一种不需要监督的对象共同突出物检测方法（CoSOD），用于检测图像集中的共同突出物。</li>
<li>methods: 本文使用了自动学习的自编码器和自注意力机制，将图像分割成不同类别的单个像素，并计算每个类别的对象共同突出度。</li>
<li>results: 本文的方法可以在不具备监督的情况下，基于图像集的频率统计学习，实现高度精度的对象共同突出物检测。 compared with existing methods, the proposed method has a significant improvement in performance.<details>
<summary>Abstract</summary>
In this paper, we address the detection of co-occurring salient objects (CoSOD) in an image group using frequency statistics in an unsupervised manner, which further enable us to develop a semi-supervised method. While previous works have mostly focused on fully supervised CoSOD, less attention has been allocated to detecting co-salient objects when limited segmentation annotations are available for training. Our simple yet effective unsupervised method US-CoSOD combines the object co-occurrence frequency statistics of unsupervised single-image semantic segmentations with salient foreground detections using self-supervised feature learning. For the first time, we show that a large unlabeled dataset e.g. ImageNet-1k can be effectively leveraged to significantly improve unsupervised CoSOD performance. Our unsupervised model is a great pre-training initialization for our semi-supervised model SS-CoSOD, especially when very limited labeled data is available for training. To avoid propagating erroneous signals from predictions on unlabeled data, we propose a confidence estimation module to guide our semi-supervised training. Extensive experiments on three CoSOD benchmark datasets show that both of our unsupervised and semi-supervised models outperform the corresponding state-of-the-art models by a significant margin (e.g., on the Cosal2015 dataset, our US-CoSOD model has an 8.8% F-measure gain over a SOTA unsupervised co-segmentation model and our SS-CoSOD model has an 11.81% F-measure gain over a SOTA semi-supervised CoSOD model).
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们 Addresses the detection of co-occurring salient objects (CoSOD) in an image group using frequency statistics in an unsupervised manner, which further enables us to develop a semi-supervised method. Previous works have mostly focused on fully supervised CoSOD, but less attention has been allocated to detecting co-salient objects when limited segmentation annotations are available for training. Our simple yet effective unsupervised method US-CoSOD combines the object co-occurrence frequency statistics of unsupervised single-image semantic segmentations with salient foreground detections using self-supervised feature learning. For the first time, we show that a large unlabeled dataset e.g. ImageNet-1k can be effectively leveraged to significantly improve unsupervised CoSOD performance. Our unsupervised model is a great pre-training initialization for our semi-supervised model SS-CoSOD, especially when very limited labeled data is available for training. To avoid propagating erroneous signals from predictions on unlabeled data, we propose a confidence estimation module to guide our semi-supervised training. Extensive experiments on three CoSOD benchmark datasets show that both of our unsupervised and semi-supervised models outperform the corresponding state-of-the-art models by a significant margin (e.g., on the Cosal2015 dataset, our US-CoSOD model has an 8.8% F-measure gain over a SOTA unsupervised co-segmentation model and our SS-CoSOD model has an 11.81% F-measure gain over a SOTA semi-supervised CoSOD model).Here's the translation in Traditional Chinese:在这篇论文中，我们 Addresses the detection of co-occurring salient objects (CoSOD) in an image group using frequency statistics in an unsupervised manner, which further enables us to develop a semi-supervised method. 前一些工作主要集中在完全supervised CoSOD，但是对于有限的分类标注available for training时，对于检测共同突出的物件更少的注意力。我们的简单 yet effective unsupervised method US-CoSOD combines the object co-occurrence frequency statistics of unsupervised single-image semantic segmentations with salient foreground detections using self-supervised feature learning. 我们首次显示了一个大量的无标注数据集例如ImageNet-1k可以对不supervised CoSOD performance进行明显改善。我们的无supervised model是一个优秀的预训初始化 для我们的半supervised model SS-CoSOD，特别是当有很少的标注数据available for training时。为了避免对无标注数据预测中的错误信号传播，我们提出了一个信任估计模组来引导我们的半supervised训练。广泛的实验表明我们的无supervised和半supervised模型在三个CoSOD benchmark dataset上都大比前一些state-of-the-art模型进行了明显改善 (e.g., on the Cosal2015 dataset, our US-CoSOD model has an 8.8% F-measure gain over a SOTA unsupervised co-segmentation model and our SS-CoSOD model has an 11.81% F-measure gain over a SOTA semi-supervised CoSOD model).
</details></li>
</ul>
<hr>
<h2 id="Traffic-Sign-Recognition-Using-Local-Vision-Transformer"><a href="#Traffic-Sign-Recognition-Using-Local-Vision-Transformer" class="headerlink" title="Traffic Sign Recognition Using Local Vision Transformer"></a>Traffic Sign Recognition Using Local Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06651">http://arxiv.org/abs/2311.06651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Farzipour, Omid Nejati Manzari, Shahriar B. Shokouhi</li>
<li>for: 提高自驾车和驾手助手系统中的交通标志识别率</li>
<li>methods:  combining convolutional blocks和 transformer-based blocks，并添加了一个局部模块以提高局部感知</li>
<li>results: 在德国交通标志识别benchmark和波斯尼亚交通标志数据集上达到了99.66%和99.8%的准确率，高于最佳 convolutional models，同时具有快速推理速度和实际应用场景适用性。<details>
<summary>Abstract</summary>
Recognition of traffic signs is a crucial aspect of self-driving cars and driver assistance systems, and machine vision tasks such as traffic sign recognition have gained significant attention. CNNs have been frequently used in machine vision, but introducing vision transformers has provided an alternative approach to global feature learning. This paper proposes a new novel model that blends the advantages of both convolutional and transformer-based networks for traffic sign recognition. The proposed model includes convolutional blocks for capturing local correlations and transformer-based blocks for learning global dependencies. Additionally, a locality module is incorporated to enhance local perception. The performance of the suggested model is evaluated on the Persian Traffic Sign Dataset and German Traffic Sign Recognition Benchmark and compared with SOTA convolutional and transformer-based models. The experimental evaluations demonstrate that the hybrid network with the locality module outperforms pure transformer-based models and some of the best convolutional networks in accuracy. Specifically, our proposed final model reached 99.66% accuracy in the German traffic sign recognition benchmark and 99.8% in the Persian traffic sign dataset, higher than the best convolutional models. Moreover, it outperforms existing CNNs and ViTs while maintaining fast inference speed. Consequently, the proposed model proves to be significantly faster and more suitable for real-world applications.
</details>
<details>
<summary>摘要</summary>
自驾车和助手系统中识别交通标识是一项关键性的任务，机器视觉任务如交通标识已经吸引了广泛的关注。CNNs在机器视觉中经常被使用，但是引入视Transformers提供了一种全局特征学习的替代方法。这篇论文提议了一种新的 hybrid 模型，将 convolutional 块和 transformer-based 块结合在一起，以便捕捉本地相关性和全球依赖关系。此外，还包含了一个 locality 模块，以增强本地感知。提议的模型在 Persian Traffic Sign Dataset 和 German Traffic Sign Recognition Benchmark 上进行了实验评估，与 SOTA  convolutional 和 transformer-based 模型进行比较。实验结果表明，提议的 hybrid 网络在准确率方面与最佳 convolutional 模型和一些 transformer-based 模型相当，而且具有更快的推理速度。特别是，我们的最终模型在 German traffic sign recognition benchmark 上达到了 99.66% 的准确率，而 Persian traffic sign dataset 上达到了 99.8%。此外，它还超越了现有的 CNNs 和 ViTs，而且保持了快速的推理速度。因此，我们的提议模型在实际应用中具有优势。
</details></li>
</ul>
<hr>
<h2 id="Back-to-Basics-Fast-Denoising-Iterative-Algorithm"><a href="#Back-to-Basics-Fast-Denoising-Iterative-Algorithm" class="headerlink" title="Back to Basics: Fast Denoising Iterative Algorithm"></a>Back to Basics: Fast Denoising Iterative Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06634">http://arxiv.org/abs/2311.06634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deborah Pereg</li>
<li>for: 降低噪音，提高图像质量</li>
<li>methods: 使用Back to Basics（BTB）快迭代算法，不需训练或真实数据，可应用于独立噪音和相关噪音环境中</li>
<li>results: 对三个研究 caso进行了实验，包括自然图像噪音纠正、POisson分布图像纠正和optical coherence tomography（OCT）干涉抑制，实验结果表明提案方法可以有效提高图像质量，在噪音设定中展现出良好的性能，并提供了理论保证。<details>
<summary>Abstract</summary>
We introduce Back to Basics (BTB), a fast iterative algorithm for noise reduction. Our method is computationally efficient, does not require training or ground truth data, and can be applied in the presence of independent noise, as well as correlated (coherent) noise, where the noise level is unknown. We examine three study cases: natural image denoising in the presence of additive white Gaussian noise, Poisson-distributed image denoising, and speckle suppression in optical coherence tomography (OCT). Experimental results demonstrate that the proposed approach can effectively improve image quality, in challenging noise settings. Theoretical guarantees are provided for convergence stability.
</details>
<details>
<summary>摘要</summary>
我们介绍Back to Basics（BTB）算法，它是一种快速迭代的噪声减少方法。我们的方法不需要训练或真实数据，可以在独立噪声和相关噪声（相对干扰）的情况下应用，而且噪声水平未知。我们在自然图像噪声 removing中使用了三个研究 caso：在添加白噪声的情况下的自然图像净化、Poisson分布图像净化和optical coherence tomography（OCT）中的斑点消除。实验结果表明，我们提出的方法可以有效地提高图像质量，在具有挑战性噪声的情况下。我们也提供了理论保证对 converges 稳定性。
</details></li>
</ul>
<hr>
<h2 id="A-3D-Conditional-Diffusion-Model-for-Image-Quality-Transfer-–-An-Application-to-Low-Field-MRI"><a href="#A-3D-Conditional-Diffusion-Model-for-Image-Quality-Transfer-–-An-Application-to-Low-Field-MRI" class="headerlink" title="A 3D Conditional Diffusion Model for Image Quality Transfer – An Application to Low-Field MRI"></a>A 3D Conditional Diffusion Model for Image Quality Transfer – An Application to Low-Field MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06631">http://arxiv.org/abs/2311.06631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edshkim98/diffusioniqt">https://github.com/edshkim98/diffusioniqt</a></li>
<li>paper_authors: Seunghoi Kim, Henry F. J. Tregidgo, Ahmed K. Eldaly, Matteo Figini, Daniel C. Alexander</li>
<li>for: 提高低场磁共振成像质量</li>
<li>methods: 使用3Dconditional扩散模型和cross-batch机制提高自注意力和填充</li>
<li>results: 在HCP数据集上比较出色，远超过现有方法 both quantitatively and qualitatively<details>
<summary>Abstract</summary>
Low-field (LF) MRI scanners (<1T) are still prevalent in settings with limited resources or unreliable power supply. However, they often yield images with lower spatial resolution and contrast than high-field (HF) scanners. This quality disparity can result in inaccurate clinician interpretations. Image Quality Transfer (IQT) has been developed to enhance the quality of images by learning a mapping function between low and high-quality images. Existing IQT models often fail to restore high-frequency features, leading to blurry output. In this paper, we propose a 3D conditional diffusion model to improve 3D volumetric data, specifically LF MR images. Additionally, we incorporate a cross-batch mechanism into the self-attention and padding of our network, ensuring broader contextual awareness even under small 3D patches. Experiments on the publicly available Human Connectome Project (HCP) dataset for IQT and brain parcellation demonstrate that our model outperforms existing methods both quantitatively and qualitatively. The code is publicly available at \url{https://github.com/edshkim98/DiffusionIQT}.
</details>
<details>
<summary>摘要</summary>
低场（LF）MRI仪器（<1T）仍然广泛用于具有限制的资源或不可靠的电力供应的设置。然而，它们经常生成图像的空间分辨率和对比度较低，从而导致临床医生的解释不准确。图像质量传输（IQT）已经被开发来提高图像质量，学习映射函数 между低质量和高质量图像。现有的IQT模型经常无法恢复高频特征，导致输出模糊不清。在这篇论文中，我们提议一种3D条件扩散模型，用于改进3D积分数据，特别是LF MR图像。此外，我们在网络中包含了跨批机制，使其在小3D片段中保持更广泛的Contextual awareness。实验结果表明，我们的模型在公共可用的人类连接组计划（HCP）数据集上的IQT和脑分割 task中，与现有方法相比，具有较高的数量和质量性能。代码可以在 \url{https://github.com/edshkim98/DiffusionIQT} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Computer-Vision-for-Particle-Size-Analysis-of-Coarse-Grained-Soils"><a href="#Computer-Vision-for-Particle-Size-Analysis-of-Coarse-Grained-Soils" class="headerlink" title="Computer Vision for Particle Size Analysis of Coarse-Grained Soils"></a>Computer Vision for Particle Size Analysis of Coarse-Grained Soils</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06613">http://arxiv.org/abs/2311.06613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sompote Youwai, Parchya Makam</li>
<li>for: 本研究用computer vision技术和Python编程语言进行粒子大小分析，以提高土壤物理特性的评估效率。</li>
<li>methods: 使用OPENCV库对普通照明条件下拍摄的土壤粒子进行检测和测量，并使用标准手持式摄像头。</li>
<li>results: 相比传统筛分分析方法，该方法在大于2mm粒子上表现出良好的准确性（MAPE约6%），但是小于2mm粒子的MAPE可达60%，建议使用更高分辨率的摄像头进行拍摄。<details>
<summary>Abstract</summary>
Particle size analysis (PSA) is a fundamental technique for evaluating the physical characteristics of soils. However, traditional methods like sieving can be time-consuming and labor-intensive. In this study, we present a novel approach that utilizes computer vision (CV) and the Python programming language for PSA of coarse-grained soils, employing a standard mobile phone camera. By eliminating the need for a high-performance camera, our method offers convenience and cost savings. Our methodology involves using the OPENCV library to detect and measure soil particles in digital photographs taken under ordinary lighting conditions. For accurate particle size determination, a calibration target with known dimensions is placed on a plain paper alongside 20 different sand samples. The proposed method is compared with traditional sieve analysis and exhibits satisfactory performance for soil particles larger than 2 mm, with a mean absolute percent error (MAPE) of approximately 6%. However, particles smaller than 2 mm result in higher MAPE, reaching up to 60%. To address this limitation, we recommend using a higher-resolution camera to capture images of the smaller soil particles. Furthermore, we discuss the advantages, limitations, and potential future improvements of our method. Remarkably, the program can be executed on a mobile phone, providing immediate results without the need to send soil samples to a laboratory. This field-friendly feature makes our approach highly convenient for on-site usage, outside of a traditional laboratory setting. Ultimately, this novel method represents an initial disruption to the industry, enabling efficient particle size analysis of soil without the reliance on laboratory-based sieve analysis. KEYWORDS: Computer vision, Grain size, ARUCO
</details>
<details>
<summary>摘要</summary>
计量粒子分析（PSA）是土壤物理特性的基本技术。然而，传统方法如筛分可能是时间consuming和人力成本高。在这项研究中，我们提出了一种新的方法，利用计算机视觉（CV）和Python编程语言进行PSA，使用标准的移动电话摄像头。它消除了高性能摄像头的需求，从而提供了便利和成本节省。我们的方法ология是使用OPENCV库检测和测量在普通照明条件下拍摄的土壤粒子。为了准确地确定粒子大小，我们使用了一个标准化的检测目标，并与20个不同的砂样进行比较。我们的方法与传统筛分分析相比，对土壤粒子大于2毫米的粒子大小具有较好的性能，具有约6%的平均绝对百分比误差（MAPE）。然而，粒子小于2毫米的误差较高，可达60%。为了解决这个限制，我们建议使用更高分辨率的摄像头拍摄小粒子土壤。此外，我们还讨论了我们的方法的优缺点，以及未来可能的改进。值得注意的是，我们的方法可以在移动电话上执行，无需将土壤样本送往实验室进行分析。这种场地友好的特点使我们的方法在实验室外实现了高效的粒子分析。最后，我们的新方法代表了它在业界的初步干扰，允许不需要实验室基础的粒子分析。关键词：计算机视觉、粒子大小、ARUCO
</details></li>
</ul>
<hr>
<h2 id="Swin-UNETR-Advancing-Transformer-Based-Dense-Dose-Prediction-Towards-Fully-Automated-Radiation-Oncology-Treatments"><a href="#Swin-UNETR-Advancing-Transformer-Based-Dense-Dose-Prediction-Towards-Fully-Automated-Radiation-Oncology-Treatments" class="headerlink" title="Swin UNETR++: Advancing Transformer-Based Dense Dose Prediction Towards Fully Automated Radiation Oncology Treatments"></a>Swin UNETR++: Advancing Transformer-Based Dense Dose Prediction Towards Fully Automated Radiation Oncology Treatments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06572">http://arxiv.org/abs/2311.06572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuancheng Wang, Hai Siong Tan, Rafe Mcbeth</li>
<li>For: The paper aims to develop a deep learning model for automating the creation of radiation treatment plans for cancer therapy.* Methods: The proposed model, called Swin UNETR++, uses a lightweight 3D Dual Cross-Attention (DCA) module to capture the intra and inter-volume relationships of each patient’s unique anatomy. The model was trained, validated, and tested on the Open Knowledge-Based Planning dataset.* Results: Swin UNETR++ demonstrates near-state-of-the-art performance on the validation and test datasets, with average volume-wise acceptance rates of 88.58% and 90.50%, and average patient-wise clinical acceptance rates of 100.0% and 98.0%. The results establish a basis for future studies to translate 3D dose predictions into a deliverable treatment plan, facilitating full automation.Here are the three points in Simplified Chinese text:* For: 本文旨在开发一种深度学习模型，用于自动生成 radiation therapy 的辐射治疗计划。* Methods: 提议的模型叫做 Swin UNETR++，它使用轻量级的 3D 双重跨参量 (DCA) 模块，以捕捉每个病人唯一的 анатомиче关系。模型在 Open Knowledge-Based Planning 数据集上进行了训练、验证和测试。* Results: Swin UNETR++ 在验证数据集和测试数据集上达到了 near-state-of-the-art 性能，具体来说，average volume-wise acceptance rate 为 88.58% 和 90.50%，average patient-wise clinical acceptance rate 为 100.0% 和 98.0%。结果为未来的研究提供了一个基础，以便将 3D 剂量预测翻译成可实施的治疗计划，实现了自动化。<details>
<summary>Abstract</summary>
The field of Radiation Oncology is uniquely positioned to benefit from the use of artificial intelligence to fully automate the creation of radiation treatment plans for cancer therapy. This time-consuming and specialized task combines patient imaging with organ and tumor segmentation to generate a 3D radiation dose distribution to meet clinical treatment goals, similar to voxel-level dense prediction. In this work, we propose Swin UNETR++, that contains a lightweight 3D Dual Cross-Attention (DCA) module to capture the intra and inter-volume relationships of each patient's unique anatomy, which fully convolutional neural networks lack. Our model was trained, validated, and tested on the Open Knowledge-Based Planning dataset. In addition to metrics of Dose Score $\overline{S_{\text{Dose}}$ and DVH Score $\overline{S_{\text{DVH}}$ that quantitatively measure the difference between the predicted and ground-truth 3D radiation dose distribution, we propose the qualitative metrics of average volume-wise acceptance rate $\overline{R_{\text{VA}}$ and average patient-wise clinical acceptance rate $\overline{R_{\text{PA}}$ to assess the clinical reliability of the predictions. Swin UNETR++ demonstrates near-state-of-the-art performance on validation and test dataset (validation: $\overline{S_{\text{DVH}}$=1.492 Gy, $\overline{S_{\text{Dose}}$=2.649 Gy, $\overline{R_{\text{VA}}$=88.58%, $\overline{R_{\text{PA}}$=100.0%; test: $\overline{S_{\text{DVH}}$=1.634 Gy, $\overline{S_{\text{Dose}}$=2.757 Gy, $\overline{R_{\text{VA}}$=90.50%, $\overline{R_{\text{PA}}$=98.0%), establishing a basis for future studies to translate 3D dose predictions into a deliverable treatment plan, facilitating full automation.
</details>
<details>
<summary>摘要</summary>
领域 Radiation Oncology 可以充分利用人工智能来自动生成抑肿治疗计划。这个时间consuming 和专业化的任务涉及到病人图像与器官和肿瘤分割，以生成符合临床治疗目标的3D辐射剂量分布。在这种工作中，我们提议Swin UNITR++模型，它包含了轻量级3D双交叉关注（DCA）模块，以捕捉每个患者独特的生物学结构关系。与普通的完全 convolutional neural networks 不同，这种模型可以更好地考虑患者的多个方面和尺度。我们的模型在Open Knowledge-Based Planning数据集上进行训练、验证和测试，并且使用了量化评价指标，包括辐射剂量分布的DOSE Score和DVH Score，以及评价预测结果的临床可靠性指标，包括平均体积级别接受率（RVA）和平均患者级别接受率（RPA）。Swin UNITR++在验证和测试数据集上达到了近似于状态之arte的性能（验证数据集：DOSE Score=1.492 Gy，DVH Score=2.649 Gy，RVA=88.58%，RPA=100.0%; 测试数据集：DOSE Score=1.634 Gy，DVH Score=2.757 Gy，RVA=90.50%，RPA=98.0%），为未来的研究提供了一个基础，以便将3D剂量预测翻译成可实施的治疗计划，实现全自动化。
</details></li>
</ul>
<hr>
<h2 id="OR-Residual-Connection-Achieving-Comparable-Accuracy-to-ADD-Residual-Connection-in-Deep-Residual-Spiking-Neural-Networks"><a href="#OR-Residual-Connection-Achieving-Comparable-Accuracy-to-ADD-Residual-Connection-in-Deep-Residual-Spiking-Neural-Networks" class="headerlink" title="OR Residual Connection Achieving Comparable Accuracy to ADD Residual Connection in Deep Residual Spiking Neural Networks"></a>OR Residual Connection Achieving Comparable Accuracy to ADD Residual Connection in Deep Residual Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06570">http://arxiv.org/abs/2311.06570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ym-shan/orrc-syna-natural-pruning">https://github.com/ym-shan/orrc-syna-natural-pruning</a></li>
<li>paper_authors: Yimeng Shan, Xuerui Qiu, Rui-jie Zhu, Ruike Li, Meng Wang, Haicheng Qu</li>
<li>for: This paper aims to improve the performance and energy efficiency of deep residual spiking neural networks (SNNs) for brain-like computing.</li>
<li>methods: The authors introduce the OR Residual connection (ORRC) and the Synergistic Attention (SynA) module to the SEW-ResNet architecture, and integrate natural pruning to reduce computational overhead.</li>
<li>results: The enhanced OR-Spiking ResNet achieved single-sample classification with as little as 0.8 spikes per neuron, outperforming other spike residual models in accuracy and power consumption.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是改进深度待遇刺激神经网络（SNNs）的性能和能效性，用于脑类计算。</li>
<li>methods: 作者们引入 OR Residual connection（ORRC）和Synergistic Attention（SynA）模块到 SEW-ResNet 架构中，并实现自然减少计算开销。</li>
<li>results: 提升后的 OR-Spiking ResNet 实现单个样本分类，只需0.8个神经元发射，与其他刺激剩余模型相比，具有更高的准确率和更低的电力消耗。<details>
<summary>Abstract</summary>
Spiking Neural Networks (SNNs) have garnered substantial attention in brain-like computing for their biological fidelity and the capacity to execute energy-efficient spike-driven operations. As the demand for heightened performance in SNNs surges, the trend towards training deeper networks becomes imperative, while residual learning stands as a pivotal method for training deep neural networks. In our investigation, we identified that the SEW-ResNet, a prominent representative of deep residual spiking neural networks, incorporates non-event-driven operations. To rectify this, we introduce the OR Residual connection (ORRC) to the architecture. Additionally, we propose the Synergistic Attention (SynA) module, an amalgamation of the Inhibitory Attention (IA) module and the Multi-dimensional Attention (MA) module, to offset energy loss stemming from high quantization. When integrating SynA into the network, we observed the phenomenon of "natural pruning", where after training, some or all of the shortcuts in the network naturally drop out without affecting the model's classification accuracy. This significantly reduces computational overhead and makes it more suitable for deployment on edge devices. Experimental results on various public datasets confirmed that the SynA enhanced OR-Spiking ResNet achieved single-sample classification with as little as 0.8 spikes per neuron. Moreover, when compared to other spike residual models, it exhibited higher accuracy and lower power consumption. Codes are available at https://github.com/Ym-Shan/ORRC-SynA-natural-pruning.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）在脑如计算中备受关注，因其生物准确性和能效地执行脉冲驱动操作。随着深度SNN的需求增加，训练深度网络变得必要，而剩余学习成为训练深度网络的重要方法。在我们的研究中，我们发现SEW-ResNet，一种深度剩余神经网络的代表，包含非事件驱动操作。为解决这问题，我们引入了OR隐藏连接（ORRC）到架构中。此外，我们提出了协同注意（SynA）模块，它是禁忌注意模块和多维注意模块的组合，以弥补因高量化而导致的能量损失。在将SynA模块 incorporated into the network时，我们观察到了自然减少现象，即在训练后，网络中的减少减少自然而无需影响模型的分类精度。这显著减少计算开销，使其更适合边缘设备部署。实验结果表明，对多个公共数据集进行训练后，使用SynA进行增强的OR-Spiking ResNet可以在0.8脉冲每个神经元单个样本分类。此外，与其他脉冲剩余模型相比，它表现出更高的准确率和更低的能 consumption。代码可以在https://github.com/Ym-Shan/ORRC-SynA-natural-pruning中找到。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-in-Assessing-Cardiovascular-Diseases-and-Risk-Factors-via-Retinal-Fundus-Images-A-Review-of-the-Last-Decade"><a href="#Artificial-Intelligence-in-Assessing-Cardiovascular-Diseases-and-Risk-Factors-via-Retinal-Fundus-Images-A-Review-of-the-Last-Decade" class="headerlink" title="Artificial Intelligence in Assessing Cardiovascular Diseases and Risk Factors via Retinal Fundus Images: A Review of the Last Decade"></a>Artificial Intelligence in Assessing Cardiovascular Diseases and Risk Factors via Retinal Fundus Images: A Review of the Last Decade</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07609">http://arxiv.org/abs/2311.07609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mirsaeed Abdollahi, Ali Jafarizadeh, Amirhosein Ghafouri Asbagh, Navid Sobhi, Keysan Pourmoghtader, Siamak Pedrammehr, Houshyar Asadi, Roohallah Alizadehsani, Ru-San Tan, U. Rajendra Acharya</li>
<li>For: The paper aims to provide an overview of the current advancements and challenges in employing retinal imaging and artificial intelligence to identify cardiovascular disorders.* Methods: The paper uses a comprehensive search of various databases, including PubMed, Medline, Google Scholar, Scopus, Web of Sciences, IEEE Xplore, and ACM Digital Library, to identify relevant publications related to cardiovascular diseases and artificial intelligence.* Results: The study includes 87 English-language publications that provide insights into the current state of research in this field and highlights the potential of AI and deep learning for early detection and prediction of cardiovascular diseases.Here are the three points in Simplified Chinese text:* For: 这篇论文目的是为了提供Cardiovascular diseases (CVDs)的现状和挑战，以及使用Retinal imaging和人工智能来识别CVDs的概述。* Methods: 这篇论文使用了多种数据库的检索，包括PubMed、Medline、Google Scholar、Scopus、Web of Sciences、IEEE Xplore和ACM Digital Library，以确定相关的Cardiovascular diseases和人工智能publications。* Results: 这篇论文包含87篇英文文献，提供了这个领域的当前进展和挑战，并指出了人工智能和深度学习在早期检测和预测Cardiovascular diseases方面的潜在潜力。<details>
<summary>Abstract</summary>
Background: Cardiovascular diseases (CVDs) continue to be the leading cause of mortality on a global scale. In recent years, the application of artificial intelligence (AI) techniques, particularly deep learning (DL), has gained considerable popularity for evaluating the various aspects of CVDs. Moreover, using fundus images and optical coherence tomography angiography (OCTA) to diagnose retinal diseases has been extensively studied. To better understand heart function and anticipate changes based on microvascular characteristics and function, researchers are currently exploring the integration of AI with non-invasive retinal scanning. Leveraging AI-assisted early detection and prediction of cardiovascular diseases on a large scale holds excellent potential to mitigate cardiovascular events and alleviate the economic burden on healthcare systems. Method: A comprehensive search was conducted across various databases, including PubMed, Medline, Google Scholar, Scopus, Web of Sciences, IEEE Xplore, and ACM Digital Library, using specific keywords related to cardiovascular diseases and artificial intelligence. Results: A total of 87 English-language publications, selected for relevance were included in the study, and additional references were considered. This study presents an overview of the current advancements and challenges in employing retinal imaging and artificial intelligence to identify cardiovascular disorders and provides insights for further exploration in this field. Conclusion: Researchers aim to develop precise disease prognosis patterns as the aging population and global CVD burden increase. AI and deep learning are transforming healthcare, offering the potential for single retinal image-based diagnosis of various CVDs, albeit with the need for accelerated adoption in healthcare systems.
</details>
<details>
<summary>摘要</summary>
背景：心血管疾病（CVD）仍然是全球范围内最主要的死亡原因。在过去几年，人工智能（AI）技术，特别是深度学习（DL），在评估CVD多方面的方面得到了广泛的应用。此外，使用眼膜图像和光共振成像（OCTA）诊断视网膜疾病已经得到了广泛的研究。为了更好地理解心脏功能和预测基于微血管特征和功能的变化，研究人员正在探索将AI与不侵入性的眼膜扫描结合起来。利用AI助成早期检测和预测心血管疾病的大规模应用拥有很大的潜在价值，可以减少心血管事件和减轻医疗系统的负担。方法：我们对多种数据库进行了总体检索，包括PubMed、Medline、Google学术搜索、Scopus、Web of Sciences、IEEE Xplore和ACM数字图书馆，使用与心血管疾病相关的特定关键词。结果：共选择了87篇英文文献，包括其他参考文献。本研究提供了目前AI与眼膜成像在诊断心血管疾病方面的进展和挑战，以及此领域的更多可能性的探索。结论：研究人员目标是通过随着人口老龄化和全球CVD荷重的增加，发展精准的疾病诊断模式。AI和深度学习在医疗领域中发挥了重要作用，尝试通过单一的眼膜图像诊断多种CVD，尽管需要加速在医疗系统中的采用。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-vortex-in-unstructured-mesh-with-graph-neural-networks"><a href="#Identification-of-vortex-in-unstructured-mesh-with-graph-neural-networks" class="headerlink" title="Identification of vortex in unstructured mesh with graph neural networks"></a>Identification of vortex in unstructured mesh with graph neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06557">http://arxiv.org/abs/2311.06557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianfa Wang, Yvan Fournier, Jean-Francois Wald, Youssef Mesri</li>
<li>for: 用于identifying flow characteristics from Computational Fluid Dynamics (CFD) databases，帮助研究者更好地理解流场，优化geometry设计和选择合适的CFD配置。</li>
<li>methods: 使用Graph Neural Network (GNN) with U-Net architecture，通过 algebraic multigrid method生成图并构建图层结构，对2D CFD网格中的涡旋区域进行自动标签。</li>
<li>results: 对CFD结果进行了vortex自动标签，并评估了GNN矩阵的分类精度、训练效率和标注结果的流场特征。最后，demonstrated the approach的可扩展性和通用性，可应用于不同的液体动力学模型和 Reynolds 数。<details>
<summary>Abstract</summary>
Deep learning has been employed to identify flow characteristics from Computational Fluid Dynamics (CFD) databases to assist the researcher to better understand the flow field, to optimize the geometry design and to select the correct CFD configuration for corresponding flow characteristics. Convolutional Neural Network (CNN) is one of the most popular algorithms used to extract and identify flow features. However its use, without any additional flow field interpolation, is limited to the simple domain geometry and regular meshes which limits its application to real industrial cases where complex geometry and irregular meshes are usually used. Aiming at the aforementioned problems, we present a Graph Neural Network (GNN) based model with U-Net architecture to identify the vortex in CFD results on unstructured meshes. The graph generation and graph hierarchy construction using algebraic multigrid method from CFD meshes are introduced. A vortex auto-labeling method is proposed to label vortex regions in 2D CFD meshes. We precise our approach by firstly optimizing the input set on CNNs, then benchmarking current GNN kernels against CNN model and evaluating the performances of GNN kernels in terms of classification accuracy, training efficiency and identified vortex morphology. Finally, we demonstrate the adaptability of our approach to unstructured meshes and generality to unseen cases with different turbulence models at different Reynolds numbers.
</details>
<details>
<summary>摘要</summary>
深度学习已经在计算流体动力学（CFD）数据库中使用来识别流体特性，以 помо助研究人员更好地理解流场，优化geometry设计和选择相应的CFD配置。抽象神经网络（CNN）是最受欢迎的算法之一，用于提取和识别流体特征。然而，不带任何流场插值的CNN使用，受限于简单的域几何和规则的网格，因此对实际工业案例中的复杂几何和不规则网格的应用有限。为此，我们提出了基于图神经网络（GNN）的模型，使用U-Net架构来识别CFD结果中的涡。我们首先优化输入集，然后对现有GNN核 compare with CNN模型，并评估GNN核的性能。最后，我们证明我们的方法可以适用于不结构化网格和未看到的情况中的不同的湍流模型和不同的 Reynolds 数。
</details></li>
</ul>
<hr>
<h2 id="Visual-Commonsense-based-Heterogeneous-Graph-Contrastive-Learning"><a href="#Visual-Commonsense-based-Heterogeneous-Graph-Contrastive-Learning" class="headerlink" title="Visual Commonsense based Heterogeneous Graph Contrastive Learning"></a>Visual Commonsense based Heterogeneous Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06553">http://arxiv.org/abs/2311.06553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongzhao Li, Xiangyu Zhu, Xi Zhang, Zhaoxiang Zhang, Zhen Lei</li>
<li>for: 提高多modal应用中视语关系的理解和语言领域关系的抽象</li>
<li>methods: 使用heterogeneous graph contrastive learning方法，包括Visual Commonsense Information和Graph Relation Network，以提高视觉理解任务的完成</li>
<li>results: 对四个 benchmark 进行了广泛的实验，显示了方法的效果和通用性，可以大幅提高七种代表性 VQA 模型的性能<details>
<summary>Abstract</summary>
How to select relevant key objects and reason about the complex relationships cross vision and linguistic domain are two key issues in many multi-modality applications such as visual question answering (VQA). In this work, we incorporate the visual commonsense information and propose a heterogeneous graph contrastive learning method to better finish the visual reasoning task. Our method is designed as a plug-and-play way, so that it can be quickly and easily combined with a wide range of representative methods. Specifically, our model contains two key components: the Commonsense-based Contrastive Learning and the Graph Relation Network. Using contrastive learning, we guide the model concentrate more on discriminative objects and relevant visual commonsense attributes. Besides, thanks to the introduction of the Graph Relation Network, the model reasons about the correlations between homogeneous edges and the similarities between heterogeneous edges, which makes information transmission more effective. Extensive experiments on four benchmarks show that our method greatly improves seven representative VQA models, demonstrating its effectiveness and generalizability.
</details>
<details>
<summary>摘要</summary>
多Modalitate应用中，选择相关的关键对象并理解跨视听域关系是两个关键问题。在这种情况下，我们将视觉常识信息 integrate到模型中，并提出一种多态图像异构学习方法来更好地完成视觉理解任务。我们的方法设计为可插入式的方式，以便快速和方便地与各种代表性方法结合使用。具体来说，我们的模型包括两个关键组件： Commonsense-based Contrastive Learning和图像关系网络。通过对比学习，我们引导模型更多地关注特征对象和相关的视觉常识特征。此外，图像关系网络的引入使得模型可以更好地理解同型边的相互关系，使信息传递更加有效。我们在四个标准测试集上进行了广泛的实验，并证明了我们的方法可以大幅提高七种代表VQA模型的性能，表明其有效性和普适性。
</details></li>
</ul>
<hr>
<h2 id="Stain-Consistency-Learning-Handling-Stain-Variation-for-Automatic-Digital-Pathology-Segmentation"><a href="#Stain-Consistency-Learning-Handling-Stain-Variation-for-Automatic-Digital-Pathology-Segmentation" class="headerlink" title="Stain Consistency Learning: Handling Stain Variation for Automatic Digital Pathology Segmentation"></a>Stain Consistency Learning: Handling Stain Variation for Automatic Digital Pathology Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06552">http://arxiv.org/abs/2311.06552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlyg/stain_consistency_learning">https://github.com/mlyg/stain_consistency_learning</a></li>
<li>paper_authors: Michael Yeung, Todd Watts, Sean YW Tan, Pedro F. Ferreira, Andrew D. Scott, Sonia Nielles-Vallespin, Guang Yang</li>
<li>for: 本研究旨在提高机器学习方法对染色谱变化的可靠性，并对各种方法进行比较性评估，以便选择最佳方法。</li>
<li>methods: 本研究提出了一种新的染色协调学习框架，即染色特征归一化学习法，该法结合染色特征归一化和染色一致损失函数来学习染色颜色无关的特征。</li>
<li>results: 对于 Masson 染色和 H&amp;E 染色的细胞和核lei datasets，本研究对各种染色变化处理方法进行了首次、广泛的比较，并证明了提案的方法能够获得最佳性能。<details>
<summary>Abstract</summary>
Stain variation is a unique challenge associated with automated analysis of digital pathology. Numerous methods have been developed to improve the robustness of machine learning methods to stain variation, but comparative studies have demonstrated limited benefits to performance. Moreover, methods to handle stain variation were largely developed for H&E stained data, with evaluation generally limited to classification tasks. Here we propose Stain Consistency Learning, a novel framework combining stain-specific augmentation with a stain consistency loss function to learn stain colour invariant features. We perform the first, extensive comparison of methods to handle stain variation for segmentation tasks, comparing ten methods on Masson's trichrome and H&E stained cell and nuclei datasets, respectively. We observed that stain normalisation methods resulted in equivalent or worse performance, while stain augmentation or stain adversarial methods demonstrated improved performance, with the best performance consistently achieved by our proposed approach. The code is available at: https://github.com/mlyg/stain_consistency_learning
</details>
<details>
<summary>摘要</summary>
颜色差异是数字病理学自动分析中的一个独特挑战。许多方法已经开发来改善机器学习方法对颜色差异的Robustness，但是比较研究表明这些方法具有有限的效果。此外，处理颜色差异的方法主要是为H&E染料数据而开发，评估通常是限定为分类任务。我们提出了一种新的框架，即颜色一致学习（Stain Consistency Learning），它将颜色特异的扩充与颜色一致损失函数结合在一起，以学习颜色不变的特征。我们对 Masson的三色染料和H&E染料分别进行了细胞和核lei数据集的比较，结果表明，颜色normal化方法的性能相当或更差，而颜色扩充或颜色对抗方法的性能则得到了改善，我们的提议方法得到了最佳性能。代码可以在以下链接下获取：https://github.com/mlyg/stain_consistency_learning。
</details></li>
</ul>
<hr>
<h2 id="FDNet-Feature-Decoupled-Segmentation-Network-for-Tooth-CBCT-Image"><a href="#FDNet-Feature-Decoupled-Segmentation-Network-for-Tooth-CBCT-Image" class="headerlink" title="FDNet: Feature Decoupled Segmentation Network for Tooth CBCT Image"></a>FDNet: Feature Decoupled Segmentation Network for Tooth CBCT Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06551">http://arxiv.org/abs/2311.06551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Feng, Chengkai Wang, Chengyu Wu, Yunxiang Li, Yongbo He, Shuai Wang, Yaiqi Wang</li>
<li>for: 本研究旨在提高CBCT影像的精确分割，以便正确评估牙齿准备治疗计划。</li>
<li>methods: 该研究提出了一种新的Feature Decoupled Segmentation Network（FDNet），通过结合低频波峰变换（LF-Wavelet）和SAM编码器，以提高牙齿边界的精度和细节分割的准确性。</li>
<li>results: 研究表明，FDNet可以在CBCT影像中提供高达85.28%的Dice分数和75.23%的IoU分数，表明该方法可以有效地减少semantic gap，提供精确的牙齿分割结果。<details>
<summary>Abstract</summary>
Precise Tooth Cone Beam Computed Tomography (CBCT) image segmentation is crucial for orthodontic treatment planning. In this paper, we propose FDNet, a Feature Decoupled Segmentation Network, to excel in the face of the variable dental conditions encountered in CBCT scans, such as complex artifacts and indistinct tooth boundaries. The Low-Frequency Wavelet Transform (LF-Wavelet) is employed to enrich the semantic content by emphasizing the global structural integrity of the teeth, while the SAM encoder is leveraged to refine the boundary delineation, thus improving the contrast between adjacent dental structures. By integrating these dual aspects, FDNet adeptly addresses the semantic gap, providing a detailed and accurate segmentation. The framework's effectiveness is validated through rigorous benchmarks, achieving the top Dice and IoU scores of 85.28% and 75.23%, respectively. This innovative decoupling of semantic and boundary features capitalizes on the unique strengths of each element to significantly elevate the quality of segmentation performance.
</details>
<details>
<summary>摘要</summary>
精准牙齿 cone beam computed tomography（CBCT）图像分割是正确的orthodontic treatment planning的关键。在这篇论文中，我们提出了FDNet，一种特征解coupled Segmentation Network，以便在CBCT扫描中遇到的变化牙齿条件下表现出色，例如复杂的遗产物和不明确的牙齿界限。使用低频波лет变换（LF-Wavelet）可以增强牙齿的semantic内容，同时使用SAM编码器可以进一步改善边界定义，从而提高牙齿结构之间的对比度。通过这种双重方法，FDNet能够有效地bridging semantic gap，提供精确和详细的分割。该框架的效果被证明通过严格的 benchmark，实现了Dice和IoU分割分别达到85.28%和75.23%的最高分。这种创新的特征解coupling技术可以 Capitalize on Each element的特点，提高分割性能的质量。
</details></li>
</ul>
<hr>
<h2 id="Generation-Of-Colors-using-Bidirectional-Long-Short-Term-Memory-Networks"><a href="#Generation-Of-Colors-using-Bidirectional-Long-Short-Term-Memory-Networks" class="headerlink" title="Generation Of Colors using Bidirectional Long Short Term Memory Networks"></a>Generation Of Colors using Bidirectional Long Short Term Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06542">http://arxiv.org/abs/2311.06542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chungimungi/color-prediction">https://github.com/chungimungi/color-prediction</a></li>
<li>paper_authors: A. Sinha</li>
<li>For: This paper aims to bridge the gap between human visual perception of countless shades of colours and our ability to name and describe them accurately, using a novel model based on Bidirectional Long Short-Term Memory (BiLSTM) networks with Active learning.* Methods: The paper develops a novel model that operates on a proprietary dataset curated for this study, using BiLSTM networks with Active learning to categorize and name previously unnamed colours or identify intermediate shades that elude traditional colour terminology.* Results: The findings of the study demonstrate the potential of this innovative approach in revolutionizing our understanding of colour perception and language, with the potential to extend the applications of Natural Language Processing (NLP) beyond conventional boundaries.<details>
<summary>Abstract</summary>
Human vision can distinguish between a vast spectrum of colours, estimated to be between 2 to 7 million discernible shades. However, this impressive range does not inherently imply that all these colours have been precisely named and described within our lexicon. We often associate colours with familiar objects and concepts in our daily lives. This research endeavors to bridge the gap between our visual perception of countless shades and our ability to articulate and name them accurately. A novel model has been developed to achieve this goal, leveraging Bidirectional Long Short-Term Memory (BiLSTM) networks with Active learning. This model operates on a proprietary dataset meticulously curated for this study. The primary objective of this research is to create a versatile tool for categorizing and naming previously unnamed colours or identifying intermediate shades that elude traditional colour terminology. The findings underscore the potential of this innovative approach in revolutionizing our understanding of colour perception and language. Through rigorous experimentation and analysis, this study illuminates a promising avenue for Natural Language Processing (NLP) applications in diverse industries. By facilitating the exploration of the vast colour spectrum the potential applications of NLP are extended beyond conventional boundaries.
</details>
<details>
<summary>摘要</summary>
人类视觉可以分辨出各种颜色，估计有2到7百万个不同的颜色。然而，这一各种颜色的范围并不意味着所有的颜色都有被精确地命名和描述在我们的语言中。我们常常将颜色与日常生活中的familiar对象和概念相关联。这项研究的目的是将视觉中的 countless 颜色与我们的语言之间的差距bridged。为此，该研究开发了一种基于BiLSTM网络和活动学习的新模型。该模型运用了专门为本研究制作的专有数据集。本研究的主要目标是开发一种可以分类和命名未命名颜色或者描述不能被传统颜色术语捕捉的颜色的工具。研究结果表明这种创新的方法在改变我们对颜色识别和语言的理解方面具有潜力。通过严格的实验和分析，本研究探讨了NLP应用的新途径，扩展了NLP应用的边界。
</details></li>
</ul>
<hr>
<h2 id="CrashCar101-Procedural-Generation-for-Damage-Assessment"><a href="#CrashCar101-Procedural-Generation-for-Damage-Assessment" class="headerlink" title="CrashCar101: Procedural Generation for Damage Assessment"></a>CrashCar101: Procedural Generation for Damage Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06536">http://arxiv.org/abs/2311.06536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens Parslov, Erik Riise, Dim P. Papadopoulos</li>
<li>for: 本研究旨在解决汽车损害评估中的问题，包括检测损害的位置和程度以及特定的损害部分。</li>
<li>methods: 我们提议使用生成过程来训练计算机视觉系统，使其能够进行Semantic part和损害分 segmentation。我们使用生成的3D汽车模型和Synthetic Data来生成高度多样化的样本，并为每个样本提供高精度的像素注释。</li>
<li>results: 我们采用这种方法并生成了CrashCar101数据集。我们在三个实际数据集上进行了实验，并证明了在part segmentation任务上，使用实际数据和Synthetic Data进行训练的模型比使用实际数据进行训练的模型表现更好。在损害 segmentation任务上，我们证明了CrashCar101数据集的sim2real转移能力。<details>
<summary>Abstract</summary>
In this paper, we are interested in addressing the problem of damage assessment for vehicles, such as cars. This task requires not only detecting the location and the extent of the damage but also identifying the damaged part. To train a computer vision system for the semantic part and damage segmentation in images, we need to manually annotate images with costly pixel annotations for both part categories and damage types. To overcome this need, we propose to use synthetic data to train these models. Synthetic data can provide samples with high variability, pixel-accurate annotations, and arbitrarily large training sets without any human intervention. We propose a procedural generation pipeline that damages 3D car models and we obtain synthetic 2D images of damaged cars paired with pixel-accurate annotations for part and damage categories. To validate our idea, we execute our pipeline and render our CrashCar101 dataset. We run experiments on three real datasets for the tasks of part and damage segmentation. For part segmentation, we show that the segmentation models trained on a combination of real data and our synthetic data outperform all models trained only on real data. For damage segmentation, we show the sim2real transfer ability of CrashCar101.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注了汽车损害评估问题，例如汽车受损的部分和程度的识别。为了训练计算机视觉系统进行semantic部分和损害分割，我们需要手动标注图像，以获得价值的像素注释。为了缓解这个需求，我们提议使用生成数据。生成数据可以提供高度多样性的样本，高精度的像素注释，并且可以在人工干预下生成无限大的训练集。我们提出了一个生成过程，用于损害3D汽车模型，并从而获得了损害2D图像和高精度的像素注释。为了验证我们的想法，我们执行了我们的管道，并生成了CrashCar101数据集。我们在三个实际数据集上进行了实验，以评估part和损害分割任务。对于part分割任务，我们显示了将real数据和我们生成的数据混合训练的模型，与只使用实际数据训练的模型相比，具有更高的性能。对于损害分割任务，我们显示了CrashCar101数据集的sim2real传送能力。
</details></li>
</ul>
<hr>
<h2 id="Band-wise-Hyperspectral-Image-Pansharpening-using-CNN-Model-Propagation"><a href="#Band-wise-Hyperspectral-Image-Pansharpening-using-CNN-Model-Propagation" class="headerlink" title="Band-wise Hyperspectral Image Pansharpening using CNN Model Propagation"></a>Band-wise Hyperspectral Image Pansharpening using CNN Model Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06510">http://arxiv.org/abs/2311.06510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/giu-guarino/r-pnn">https://github.com/giu-guarino/r-pnn</a></li>
<li>paper_authors: Giuseppe Guarino, Matteo Ciotola, Gemine Vivone, Giuseppe Scarpa</li>
<li>for: 本研究的目的是提出一种深度学习方法，用于解决高spectral缩进问题。</li>
<li>methods: 该方法基于单 banda unsupervised pansharpening模型，通过在排序band-wise adaptive scheme中嵌入该模型，以适应不同 spectral band的数据。</li>
<li>results: 对于我们的数据集，该方法达到了非常好的结果，超过了传统和深度学习参考方法。代码实现可以在<a target="_blank" rel="noopener" href="https://github.com/giu-guarino/R-PNN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/giu-guarino/R-PNN找到。</a><details>
<summary>Abstract</summary>
Hyperspectral pansharpening is receiving a growing interest since the last few years as testified by a large number of research papers and challenges. It consists in a pixel-level fusion between a lower-resolution hyperspectral datacube and a higher-resolution single-band image, the panchromatic image, with the goal of providing a hyperspectral datacube at panchromatic resolution. Thanks to their powerful representational capabilities, deep learning models have succeeded to provide unprecedented results on many general purpose image processing tasks. However, when moving to domain specific problems, as in this case, the advantages with respect to traditional model-based approaches are much lesser clear-cut due to several contextual reasons. Scarcity of training data, lack of ground-truth, data shape variability, are some such factors that limit the generalization capacity of the state-of-the-art deep learning networks for hyperspectral pansharpening. To cope with these limitations, in this work we propose a new deep learning method which inherits a simple single-band unsupervised pansharpening model nested in a sequential band-wise adaptive scheme, where each band is pansharpened refining the model tuned on the preceding one. By doing so, a simple model is propagated along the wavelength dimension, adaptively and flexibly, with no need to have a fixed number of spectral bands, and, with no need to dispose of large, expensive and labeled training datasets. The proposed method achieves very good results on our datasets, outperforming both traditional and deep learning reference methods. The implementation of the proposed method can be found on https://github.com/giu-guarino/R-PNN
</details>
<details>
<summary>摘要</summary>
“几年前，几何spectral pansharpening已经受到了越来越多的关注，可以看到许多研究论文和挑战。它的目的是将lower-resolution的几何spectral数据 кубы和高分辨率的单色图像（panchromatic image）进行像素级融合，以获得高分辨率的几何spectral数据库。由于深度学习模型具有强大的表示能力，它们在许多通用图像处理任务上取得了无PRECEDENT的成绩。但是，当转移到域特定问题时，例如这个案例中，深度学习模型的优势与传统的模型基于方法相比较难明确地表现出来，因为一些contextual因素的限制。数据缺乏、缺乏标注、数据形态变化等因素，都会限制深度学习网络在域特定问题上的总体化能力。为了缓解这些限制，在这个工作中，我们提出了一种新的深度学习方法。这种方法基于单色图像无监督的宽渠扩充模型，通过在带宽维度上逐步进行适应式的band-wise适应方案，使得每个带都可以细化和适应，无需具备固定的 spectral 带数量，也无需具备大量、昂贵和标注的训练数据。提议的方法在我们的数据集上实现了非常好的效果，超越了传统和深度学习参考方法。实现方法可以在https://github.com/giu-guarino/R-PNN 找到。”
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Context-Learning-for-Visual-Inspection-of-Industrial-Defects"><a href="#Self-supervised-Context-Learning-for-Visual-Inspection-of-Industrial-Defects" class="headerlink" title="Self-supervised Context Learning for Visual Inspection of Industrial Defects"></a>Self-supervised Context Learning for Visual Inspection of Industrial Defects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06504">http://arxiv.org/abs/2311.06504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wang, Haiming Yao, Wenyong Yu</li>
<li>for: 本研究旨在提出一种基于自我监督学习的检测方法，以解决现有的无监督模型在产品表面变化大的情况下检测缺陷的问题。</li>
<li>methods: 我们提出一种自我监督学习算法，通过将目标图像分割成9个 patches，并让编码器预测每两个 patch 的相对位置关系，以提取丰富的 semantics。我们还提出一种帮助函数-加 augmentation 方法，以强调正常和异常的 latent 表示之间的差异。</li>
<li>results: 我们的方法在 widely 使用的 MVTec AD 数据集上实现了出色的检测和 segmentation 性能，即 95.8% 和 96.8% 分别，创造了当今无监督检测领域的状元标准。广泛的实验证明了我们的方法在多种工业应用中的有效性。<details>
<summary>Abstract</summary>
The unsupervised visual inspection of defects in industrial products poses a significant challenge due to substantial variations in product surfaces. Current unsupervised models struggle to strike a balance between detecting texture and object defects, lacking the capacity to discern latent representations and intricate features. In this paper, we present a novel self-supervised learning algorithm designed to derive an optimal encoder by tackling the renowned jigsaw puzzle. Our approach involves dividing the target image into nine patches, tasking the encoder with predicting the relative position relationships between any two patches to extract rich semantics. Subsequently, we introduce an affinity-augmentation method to accentuate differences between normal and abnormal latent representations. Leveraging the classic support vector data description algorithm yields final detection results. Experimental outcomes demonstrate that our proposed method achieves outstanding detection and segmentation performance on the widely used MVTec AD dataset, with rates of 95.8% and 96.8%, respectively, establishing a state-of-the-art benchmark for both texture and object defects. Comprehensive experimentation underscores the effectiveness of our approach in diverse industrial applications.
</details>
<details>
<summary>摘要</summary>
“无监督的视觉检测工业产品上的瑕疵具有严重的挑战，因为产品表面会有很大的变化。现有的无监督模型对于检测文字和物体瑕疵具有困难，因为它们缺乏能够捕捉实际特征和细节的能力。在这篇论文中，我们提出了一个新的自类学习算法，用于从熔毙难以分辨的图像中提取有用的 semantics。我们的方法是将目标图像分成九块，让算法预测两块之间的相对位置关系，以提取丰富的 semantics。然后，我们引入了一个增强不同于正常的latent representation的方法，以提高分辨率。通过使用了经典支持向量描述算法，获得最终的检测结果。实验结果显示，我们的提案方法在广泛使用的MVTec AD dataset上实现了95.8%和96.8%的检测和分类性能，成为瑕疵和物体瑕疵检测的现代标准。实验结果显示，我们的方法在不同的工业应用中具有广泛的适用范围。”
</details></li>
</ul>
<hr>
<h2 id="LayoutPrompter-Awaken-the-Design-Ability-of-Large-Language-Models"><a href="#LayoutPrompter-Awaken-the-Design-Ability-of-Large-Language-Models" class="headerlink" title="LayoutPrompter: Awaken the Design Ability of Large Language Models"></a>LayoutPrompter: Awaken the Design Ability of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06495">http://arxiv.org/abs/2311.06495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/layoutgeneration">https://github.com/microsoft/layoutgeneration</a></li>
<li>paper_authors: Jiawei Lin, Jiaqi Guo, Shizhao Sun, Zijiang James Yang, Jian-Guang Lou, Dongmei Zhang</li>
<li>for: 这个论文是为了提出一种基于大语言模型（LLM）的 Conditional Graphic Layout Generation 方法，以解决现有方法缺乏灵活性和数据效率问题。</li>
<li>methods: 该方法包括三个关键组件：输入输出序列化、动态示例选择和布局排序。具体来说，输入输出序列化组件 меiculously 设计了每个布局生成任务的输入和输出格式。动态示例选择负责选择对于给定输入最有帮助的提示示例。布局排序则是用于从多个 LLM 的输出中选择最高质量的布局。</li>
<li>results: 经过实验表明，LayoutPrompter 可以在所有现有的布局生成任务上与或超越当前状态的方法，无需训练或调整模型。此外，对比baseline方法，LayoutPrompter 在低数据情况下表现更出色，进一步证明了该方法的数据效率。<details>
<summary>Abstract</summary>
Conditional graphic layout generation, which automatically maps user constraints to high-quality layouts, has attracted widespread attention today. Although recent works have achieved promising performance, the lack of versatility and data efficiency hinders their practical applications. In this work, we propose LayoutPrompter, which leverages large language models (LLMs) to address the above problems through in-context learning. LayoutPrompter is made up of three key components, namely input-output serialization, dynamic exemplar selection and layout ranking. Specifically, the input-output serialization component meticulously designs the input and output formats for each layout generation task. Dynamic exemplar selection is responsible for selecting the most helpful prompting exemplars for a given input. And a layout ranker is used to pick the highest quality layout from multiple outputs of LLMs. We conduct experiments on all existing layout generation tasks using four public datasets. Despite the simplicity of our approach, experimental results show that LayoutPrompter can compete with or even outperform state-of-the-art approaches on these tasks without any model training or fine-tuning. This demonstrates the effectiveness of this versatile and training-free approach. In addition, the ablation studies show that LayoutPrompter is significantly superior to the training-based baseline in a low-data regime, further indicating the data efficiency of LayoutPrompter. Our project is available at https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompter.
</details>
<details>
<summary>摘要</summary>
《 conditional graphic layout generation 》，即自动将用户约束映射到高质量的布局，在今天已经吸引了广泛的关注。虽然 latest works 已经实现了可观的性能，但缺乏实用性和数据效率限制了它们的实际应用。在这项工作中，我们提出了 LayoutPrompter，它利用大型语言模型（LLMs）来解决上述问题通过在线上学习。LayoutPrompter 由三个关键组成部分：输入输出序列化、动态示例选择和布局排名。具体来说，输入输出序列化部分仔细设计了每个布局生成任务的输入和输出格式。动态示例选择部分选择给定输入的最有用的推动示例。而布局排名部分则用来从多个 LLMS 的输出中选择最高质量的布局。我们在所有现有的布局生成任务上进行了实验，使用四个公共数据集。尽管我们的方法简单，但实验结果显示，LayoutPrompter 可以与或 même outperform 当前状态的方法这些任务上，无需任何模型训练或调整。这说明 LayoutPrompter 是一种灵活且无需训练的方法。此外，我们的剖析研究表明，LayoutPrompter 在低数据情况下表现 significatively 优于基eline，这再次证明了 LayoutPrompter 的数据效率。您可以在 https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompter 上查看我们的项目。
</details></li>
</ul>
<hr>
<h2 id="PECoP-Parameter-Efficient-Continual-Pretraining-for-Action-Quality-Assessment"><a href="#PECoP-Parameter-Efficient-Continual-Pretraining-for-Action-Quality-Assessment" class="headerlink" title="PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment"></a>PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07603">http://arxiv.org/abs/2311.07603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plrbear/pecop">https://github.com/plrbear/pecop</a></li>
<li>paper_authors: Amirhossein Dadashzadeh, Shuchao Duan, Alan Whone, Majid Mirmehdi</li>
<li>for: 本研究的目的是提高Action Quality Assessment（AQA）中的模型表现，特别是在预测过程中处理具有域对错的资料时。</li>
<li>methods: 我们提出了一个新的、效率高的普遍预训架构，名为PECoP，并在其中引入3D-Adapters来学习预测当中的体域特征。在PECoP中，仅对Adapter模组的参数进行更新，以减少域对错的影响。</li>
<li>results: 我们在几个benchmark dataset上进行了实验，包括JIGSAWS、MTL-AQA和FineDiving，并取得了较好的成绩（比如JIGSAWS中提高6.0%）。此外，我们还提供了一个新的Parkinson’s Disease dataset，PD4T，并在其上进行了比较，并与之前的最佳成绩进行了比较（提高3.56%）。<details>
<summary>Abstract</summary>
The limited availability of labelled data in Action Quality Assessment (AQA), has forced previous works to fine-tune their models pretrained on large-scale domain-general datasets. This common approach results in weak generalisation, particularly when there is a significant domain shift. We propose a novel, parameter efficient, continual pretraining framework, PECoP, to reduce such domain shift via an additional pretraining stage. In PECoP, we introduce 3D-Adapters, inserted into the pretrained model, to learn spatiotemporal, in-domain information via self-supervised learning where only the adapter modules' parameters are updated. We demonstrate PECoP's ability to enhance the performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS ($\uparrow6.0\%$), MTL-AQA ($\uparrow0.99\%$), and FineDiving ($\uparrow2.54\%$). We also present a new Parkinson's Disease dataset, PD4T, of real patients performing four various actions, where we surpass ($\uparrow3.56\%$) the state-of-the-art in comparison. Our code, pretrained models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP.
</details>
<details>
<summary>摘要</summary>
因为Action Quality Assessment（AQA）的标注数据有限，previous works通常是 fine-tune 在大规模的领域通用数据集上预训练的模型。这种常见的方法会导致弱化泛化，特别是当领域shift很大时。我们提出了一种新的、效率的 continual pretraining 框架，PECoP，以减少领域shift。在 PECoP 中，我们引入了 3D-Adapters，用于在预训练模型中学习空间时间领域信息，通过自我超vised learning，只有 adapter modules 的参数被更新。我们证明了 PECoP 能够提高最近state-of-the-art方法（MUSDL、CoRe 和 TSA）在 AQA 中的性能，在 benchmark 数据集（JIGSAWS、MTL-AQA 和 FineDiving）上实现了显著提高（$\uparrow6.0\%$, $\uparrow0.99\%$ 和 $\uparrow2.54\%$）。我们还发布了一个新的 Parkinson's Disease 数据集，PD4T，包含了四种不同的动作，我们在 comparison 中超过了 state-of-the-art（$\uparrow3.56\%$）。我们的代码、预训练模型和 PD4T 数据集可以在 GitHub 上获取：https://github.com/Plrbear/PECoP。
</details></li>
</ul>
<hr>
<h2 id="Polarimetric-PatchMatch-Multi-View-Stereo"><a href="#Polarimetric-PatchMatch-Multi-View-Stereo" class="headerlink" title="Polarimetric PatchMatch Multi-View Stereo"></a>Polarimetric PatchMatch Multi-View Stereo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07600">http://arxiv.org/abs/2311.07600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyu Zhao, Jumpei Oishi, Yusuke Monno, Masatoshi Okutomi</li>
<li>for: 这paper是为了提高多视图ステレオ（MVS）的准确性和完整性而设计的。</li>
<li>methods: 这paper使用的方法是PatchMatch multi-view Stereo（PatchMatch MVS），该方法通过生成深度和法向假设，并效率地在多视图图像中寻找最佳假设，以确定物体的三维模型。此外，这paper还引入了抗licht极化信息来评估假设的正确性。</li>
<li>results: 实验结果表明，对比现有PatchMatch MVS方法，PolarPMS可以提高三维模型的准确性和完整性，特别是对于无文本表面。<details>
<summary>Abstract</summary>
PatchMatch Multi-View Stereo (PatchMatch MVS) is one of the popular MVS approaches, owing to its balanced accuracy and efficiency. In this paper, we propose Polarimetric PatchMatch multi-view Stereo (PolarPMS), which is the first method exploiting polarization cues to PatchMatch MVS. The key of PatchMatch MVS is to generate depth and normal hypotheses, which form local 3D planes and slanted stereo matching windows, and efficiently search for the best hypothesis based on the consistency among multi-view images. In addition to standard photometric consistency, our PolarPMS evaluates polarimetric consistency to assess the validness of a depth and normal hypothesis, motivated by the physical property that the polarimetric information is related to the object's surface normal. Experimental results demonstrate that our PolarPMS can improve the accuracy and the completeness of reconstructed 3D models, especially for texture-less surfaces, compared with state-of-the-art PatchMatch MVS methods.
</details>
<details>
<summary>摘要</summary>
patchmatch多视图雷达（PatchMatch MVS）是一种受欢迎的MVS方法，因为它的平衡准确性和效率。在这篇论文中，我们提出了抗 polarimetric PatchMatch多视图雷达（PolarPMS），这是第一种利用抗 polarimetric 信号来PatchMatch MVS的方法。patchmatch MVS的关键在于生成深度和法向假设，形成局部三维平面和斜视匹配窗口，然后高效地搜索最佳假设，基于多视图图像的一致性。除了标准光度一致性外，我们的PolarPMS还评估抗 polarimetric一致性，以评估假设的有效性，这是因为物体表面法向的物理特性和抗 polarimetric信号之间存在关系。实验结果表明，我们的PolarPMS可以提高准确性和完整性的三维模型重建，特别是面粗糙表面，相比现有的PatchMatch MVS方法。
</details></li>
</ul>
<hr>
<h2 id="CVTHead-One-shot-Controllable-Head-Avatar-with-Vertex-feature-Transformer"><a href="#CVTHead-One-shot-Controllable-Head-Avatar-with-Vertex-feature-Transformer" class="headerlink" title="CVTHead: One-shot Controllable Head Avatar with Vertex-feature Transformer"></a>CVTHead: One-shot Controllable Head Avatar with Vertex-feature Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06443">http://arxiv.org/abs/2311.06443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HowieMa/CVTHead">https://github.com/HowieMa/CVTHead</a></li>
<li>paper_authors: Haoyu Ma, Tong Zhang, Shanlin Sun, Xiangyi Yan, Kun Han, Xiaohui Xie</li>
<li>for: 本研究旨在 reconstruction 个性化动画人头模型，以便在 AR&#x2F;VR 领域中实现真实时间的人脸动画。</li>
<li>methods: 本研究使用 point-based 神经渲染技术，从单个参考图像中生成可控的神经头像。该方法利用 mesh 中稀疏的顶点点集，并采用提出的 Vertex-feature Transformer 来学习每个顶点的本地特征描述符。这使得可以模型所有顶点之间的长距离依赖关系。</li>
<li>results: 实验结果表明，CVTHead 可以与现状的图形学基于方法相比，实现相似的性能。此外，它还允许在不同的表情、头部姿态和摄像头视图下，高效地渲染出新的人头模型。这些属性可以通过 3DMM 的偏置系数进行控制，以实现多样化和真实的动画在真实时间enario中。<details>
<summary>Abstract</summary>
Reconstructing personalized animatable head avatars has significant implications in the fields of AR/VR. Existing methods for achieving explicit face control of 3D Morphable Models (3DMM) typically rely on multi-view images or videos of a single subject, making the reconstruction process complex. Additionally, the traditional rendering pipeline is time-consuming, limiting real-time animation possibilities. In this paper, we introduce CVTHead, a novel approach that generates controllable neural head avatars from a single reference image using point-based neural rendering. CVTHead considers the sparse vertices of mesh as the point set and employs the proposed Vertex-feature Transformer to learn local feature descriptors for each vertex. This enables the modeling of long-range dependencies among all the vertices. Experimental results on the VoxCeleb dataset demonstrate that CVTHead achieves comparable performance to state-of-the-art graphics-based methods. Moreover, it enables efficient rendering of novel human heads with various expressions, head poses, and camera views. These attributes can be explicitly controlled using the coefficients of 3DMMs, facilitating versatile and realistic animation in real-time scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将个性化动画头模型重建为有关AR/VR的研究领域有着重要意义。现有的实现方法通常需要多视图图像或视频，这使得重建过程变得复杂。另外，传统的渲染管道时间consuming，限制了实时动画的可能性。在这篇论文中，我们介绍CVTHead，一种新的方法，可以从单个参考图像中生成可控的神经头模型。CVTHead使用点集为网格的稀疏顶点来学习本地特征描述符，这使得模型可以学习所有顶点之间的长距离依赖关系。实验结果表明，CVTHead可以与现有的图形学基于方法相比，在VOXCELEB数据集上实现相似的性能。此外，它可以高效地渲染 novel human head 模型，包括不同的表情、头部姿态和摄像头视角。这些特性可以通过3DMM的系数来控制，从而实现有效的实时动画。Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/11/cs.CV_2023_11_11/" data-id="clpahu74f00n03h88bcx314cg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/11/cs.AI_2023_11_11/" class="article-date">
  <time datetime="2023-11-11T12:00:00.000Z" itemprop="datePublished">2023-11-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/11/cs.AI_2023_11_11/">cs.AI - 2023-11-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Automatized-Self-Supervised-Learning-for-Skin-Lesion-Screening"><a href="#Automatized-Self-Supervised-Learning-for-Skin-Lesion-Screening" class="headerlink" title="Automatized Self-Supervised Learning for Skin Lesion Screening"></a>Automatized Self-Supervised Learning for Skin Lesion Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06691">http://arxiv.org/abs/2311.06691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vullnet Useini, Stephanie Tanadini-Lang, Quentin Lohmeyer, Mirko Meboldt, Nicolaus Andratschke, Ralph P. Braun, Javier Barranco García<br>for: 这份研究的目的是为了提高皮肤癌检测的精度和效率，以及帮助皮肤科医生识别病变。methods: 这份研究使用了人工智能（AI）决策支持工具，该工具使用了现代的物体检测算法来识别和从患者影像中提取所有皮肤损伤，然后使用自主学习AI算法来排序这些损伤的可疑程度。results: 这份研究的结果显示，使用AI决策支持工具可以提高皮肤科医生识别病变的精度，具体来说，该工具可以帮助医生识别93%的病变损伤，并且帮助医生增加自信心和与其他专家的一致性。<details>
<summary>Abstract</summary>
The incidence rates of melanoma, the deadliest form of skin cancer, have been increasing steadily worldwide, presenting a significant challenge to dermatologists. Early detection of melanoma is crucial for improving patient survival rates, but identifying suspicious lesions through ugly duckling (UD) screening, the current method used for skin cancer screening, can be challenging and often requires expertise in pigmented lesions. To address these challenges and improve patient outcomes, an artificial intelligence (AI) decision support tool was developed to assist dermatologists in identifying UD from wide-field patient images. The tool uses a state-of-the-art object detection algorithm to identify and extract all skin lesions from patient images, which are then sorted by suspiciousness using a self-supervised AI algorithm. A clinical validation study was conducted to evaluate the tool's performance, which demonstrated an average sensitivity of 93% for the top-10 AI-identified UDs on skin lesions selected by the majority of experts in pigmented skin lesions. The study also found that dermatologists confidence increased, and the average majority agreement with the top-10 AI-identified UDs improved to 100% when assisted by AI. The development of this AI decision support tool aims to address the shortage of specialists, enable at-risk patients to receive faster consultations and understand the impact of AI-assisted screening. The tool's automation can assist dermatologists in identifying suspicious lesions and provide a more objective assessment, reducing subjectivity in the screening process. The future steps for this project include expanding the dataset to include histologically confirmed melanoma cases and increasing the number of participants for clinical validation to strengthen the tool's reliability and adapt it for real-world consultation.
</details>
<details>
<summary>摘要</summary>
全球的梅毒病例数逐渐增加，对皮肤科医生而言，这提出了一项重要的挑战。早期发现梅毒病是改善病人存活率的关键，但通过“鸟嘤”（UD）检测，现在用于皮肤癌检测的方法，可能很困难，需要对疤痕性皮肤病有专门的知识。为了解决这些挑战并提高病人 outcome，我们开发了一种人工智能（AI）决策支持工具，用于协助皮肤科医生从广角图像中识别UD。该工具使用当前最先进的物体检测算法来识别和提取患者图像中的所有皮肤损伤，然后根据自动学习AI算法排序为可疑程度。在临床验证研究中，我们发现该工具的敏感性为93%，对于由大多数专家选择的疤痕性皮肤损伤的top-10 AI识别UD。研究还发现，当帮助于AI的时候，专家的自信度增加，并且对top-10 AI识别UD的多数同意率提高到100%。该工具的开发旨在解决专业人员短缺、帮助高风险患者更快地咨询，并了解AI助检查的影响。该工具的自动化可以帮助皮肤科医生识别可疑损伤，提供更Objective的评估，减少检测过程中的主观性。未来的步骤包括将数据集扩展到包括历史确诊梅毒患者 случа，并增加参与者数量以强化工具的可靠性和适应实际咨询。
</details></li>
</ul>
<hr>
<h2 id="Dream-to-Adapt-Meta-Reinforcement-Learning-by-Latent-Context-Imagination-and-MDP-Imagination"><a href="#Dream-to-Adapt-Meta-Reinforcement-Learning-by-Latent-Context-Imagination-and-MDP-Imagination" class="headerlink" title="Dream to Adapt: Meta Reinforcement Learning by Latent Context Imagination and MDP Imagination"></a>Dream to Adapt: Meta Reinforcement Learning by Latent Context Imagination and MDP Imagination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06673">http://arxiv.org/abs/2311.06673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lu Wen, Songan Zhang, H. Eric Tseng, Huei Peng</li>
<li>for: 这个论文旨在快速学习未见过的任务，通过将先前学习的知识传递到相似任务中。</li>
<li>methods: 这个论文提出了一个基于上下文的Meta reinforcement learning（Meta RL）算法，称为MetaDreamer，它需要更少的真实任务和数据，通过做meta-幻想和MDP-幻想。</li>
<li>results: 我们的实验显示，MetaDreamer在数据效率和混合 interpolated 测试中表现出色，超越现有的方法。<details>
<summary>Abstract</summary>
Meta reinforcement learning (Meta RL) has been amply explored to quickly learn an unseen task by transferring previously learned knowledge from similar tasks. However, most state-of-the-art algorithms require the meta-training tasks to have a dense coverage on the task distribution and a great amount of data for each of them. In this paper, we propose MetaDreamer, a context-based Meta RL algorithm that requires less real training tasks and data by doing meta-imagination and MDP-imagination. We perform meta-imagination by interpolating on the learned latent context space with disentangled properties, as well as MDP-imagination through the generative world model where physical knowledge is added to plain VAE networks. Our experiments with various benchmarks show that MetaDreamer outperforms existing approaches in data efficiency and interpolated generalization.
</details>
<details>
<summary>摘要</summary>
<SYS> translate="zh-CN"</SYS>meta学习（Meta RL）已经广泛探索，以快速学习未经见过的任务，通过将先前学习的知识传递到相似任务中。然而，大多数当前最佳方法需要meta训练任务的权重复盖到任务分布中，并且需要很多数据 для每个meta训练任务。在这篇论文中，我们提出了MetaDreamer算法，它需要更少的真实训练任务和数据，通过meta想象和MDP想象。我们在meta想象中，通过 interpolating在已学习的约束空间中，捕捉到分离的属性，并在生成世界模型中添加物理知识，使得plain VAE网络可以更好地预测未经见过的任务。我们在不同的benchmark上进行了实验，结果显示，MetaDreamer在数据效率和 interpolated泛化方面超过了现有方法。
</details></li>
</ul>
<hr>
<h2 id="In-context-Vectors-Making-In-Context-Learning-More-Effective-and-Controllable-Through-Latent-Space-Steering"><a href="#In-context-Vectors-Making-In-Context-Learning-More-Effective-and-Controllable-Through-Latent-Space-Steering" class="headerlink" title="In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering"></a>In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06668">http://arxiv.org/abs/2311.06668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shengliu66/icv">https://github.com/shengliu66/icv</a></li>
<li>paper_authors: Sheng Liu, Lei Xing, James Zou</li>
<li>for: 这篇论文旨在提出一种新的叙述学习方法，以便LLM在新任务上更好地适应示例示例。</li>
<li>methods: 该方法包括在示例示例上进行前向传播，并生成一个叙述向量（ICV），该向量捕捉了示例示例中的关键信息。然后，在新的查询上，将LLM的幂状态进行偏移，使其更好地跟随示例示例。</li>
<li>results: 研究表明，ICV方法可以在多种任务上达到更好的性能，包括安全性、风格转换、扮演和格式化等。此外，ICV方法还可以轻松地控制LLM的行为，并且计算效率高于精度调整。<details>
<summary>Abstract</summary>
Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by removing the in-context demonstrations; 4) ICV is computationally much more efficient than fine-tuning. We demonstrate that ICV achieves better performance compared to standard in-context learning and fine-tuning on diverse tasks including safety, style transfer, role-playing and formatting. Moreover, we show that we can flexibly teach LLM to simultaneously follow different types of instructions by simple vector arithmetics on the corresponding ICVs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）展示出emergent在场景学习能力，即通过示例示例来适应新任务。然而，场景学习在许多场景下表现有限，控制困难，需要场景窗口空间。为了解决这些限制，我们提议一种替代方法，即在场景中 vectors（ICV）。使用ICV有两步：首先，我们使用示例示例进行前向传播，从LLM的干扰空间中生成场景vector，这个vector捕捉了任务的核心信息。然后，在新的查询上，而不是添加示例到提示中，我们使用ICV来偏移LLM的干扰状态。ICV方法具有以下优点：1）帮助LLM更好地跟随示例示例；2）容易控制，只需调整ICV的大小；3）缩短提示的长度，去除场景示例；4）ICV比finetuning更高效。我们示示ICV可以在多种任务上达到更好的性能，包括安全、样式转移、扮演和格式化。此外，我们还证明了可以通过简单的向量运算来让LLM同时遵循不同类型的指令。
</details></li>
</ul>
<hr>
<h2 id="The-Pros-and-Cons-of-Using-Machine-Learning-and-Interpretable-Machine-Learning-Methods-in-psychiatry-detection-applications-specifically-depression-disorder-A-Brief-Review"><a href="#The-Pros-and-Cons-of-Using-Machine-Learning-and-Interpretable-Machine-Learning-Methods-in-psychiatry-detection-applications-specifically-depression-disorder-A-Brief-Review" class="headerlink" title="The Pros and Cons of Using Machine Learning and Interpretable Machine Learning Methods in psychiatry detection applications, specifically depression disorder: A Brief Review"></a>The Pros and Cons of Using Machine Learning and Interpretable Machine Learning Methods in psychiatry detection applications, specifically depression disorder: A Brief Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06633">http://arxiv.org/abs/2311.06633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Simchi, Samira Tajik</li>
<li>for: 这些研究旨在提高心理疾病诊断的准确性和速度，防止自杀等严重结果。</li>
<li>methods: 这些研究使用机器学习技术，以提供更加准确和理解性的诊断结果。</li>
<li>results: 这些研究获得了有用的结果，帮助了心理科学家和研究人员更好地理解机器学习在心理疾病诊断中的优劣。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has forced many people to limit their social activities, which has resulted in a rise in mental illnesses, particularly depression. To diagnose these illnesses with accuracy and speed, and prevent severe outcomes such as suicide, the use of machine learning has become increasingly important. Additionally, to provide precise and understandable diagnoses for better treatment, AI scientists and researchers must develop interpretable AI-based solutions. This article provides an overview of relevant articles in the field of machine learning and interpretable AI, which helps to understand the advantages and disadvantages of using AI in psychiatry disorder detection applications.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行导致许多人需要限制社交活动，这已经导致了心理疾病的增加，特别是抑郁症。为了准确和快速诊断这些疾病，以避免严重的结果如自杀，机器学习的使用已成为越来越重要。此外，为了提供更好的治疗，AI科学家和研究人员必须开发可解释的 AI 解决方案。本文提供了机器学习和可解释 AI 领域的相关文章，以便更好地了解使用 AI 在心理疾病检测应用中的优劣。
</details></li>
</ul>
<hr>
<h2 id="VT-Former-A-Transformer-based-Vehicle-Trajectory-Prediction-Approach-For-Intelligent-Highway-Transportation-Systems"><a href="#VT-Former-A-Transformer-based-Vehicle-Trajectory-Prediction-Approach-For-Intelligent-Highway-Transportation-Systems" class="headerlink" title="VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems"></a>VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06623">http://arxiv.org/abs/2311.06623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armin Danesh Pazho, Vinit Katariya, Ghazal Alinezhad Noghre, Hamed Tabkhi</li>
<li>for: 增强道路安全和交通管理已成为现代 цифровой物理系统和智能交通系统的关键焦点。</li>
<li>methods: 本文提出了一种基于变换器的新方法，称为VT-Former，用于高速公路安全和监测中的车辆轨迹预测。这种方法不仅利用变换器捕捉长距离时间模式，还提出了一种图像注意力模块，以捕捉车辆之间的复杂社交互动。</li>
<li>results: 研究在三个 benchmark 数据集上，通过三种不同的视点展示了VT-Former 在车辆轨迹预测中的 State-of-The-Art 性能，以及其普适性和稳定性。此外，本文还评估了 VT-Former 在嵌入式板上的效率，并对其在车辆异常检测中的应用展示了其广泛的应用前景。<details>
<summary>Abstract</summary>
Enhancing roadway safety and traffic management has become an essential focus area for a broad range of modern cyber-physical systems and intelligent transportation systems. Vehicle Trajectory Prediction is a pivotal element within numerous applications for highway and road safety. These applications encompass a wide range of use cases, spanning from traffic management and accident prevention to enhancing work-zone safety and optimizing energy conservation. The ability to implement intelligent management in this context has been greatly advanced by the developments in the field of Artificial Intelligence (AI), alongside the increasing deployment of surveillance cameras across road networks. In this paper, we introduce a novel transformer-based approach for vehicle trajectory prediction for highway safety and surveillance, denoted as VT-Former. In addition to utilizing transformers to capture long-range temporal patterns, a new Graph Attentive Tokenization (GAT) module has been proposed to capture intricate social interactions among vehicles. Combining these two core components culminates in a precise approach for vehicle trajectory prediction. Our study on three benchmark datasets with three different viewpoints demonstrates the State-of-The-Art (SoTA) performance of VT-Former in vehicle trajectory prediction and its generalizability and robustness. We also evaluate VT-Former's efficiency on embedded boards and explore its potential for vehicle anomaly detection as a sample application, showcasing its broad applicability.
</details>
<details>
<summary>摘要</summary>
提高公路安全和交通管理已成为现代ци伯-物理系统和智能交通系统的重要焦点。车辆轨迹预测是这些应用程序中的重要组成部分，包括交通管理、事故预防和工地安全等。随着人工智能技术的发展和公路网络上的监测摄像头的普及，实现智能管理在这个领域已得到了大幅提高。在这篇论文中，我们介绍了一种新的变换器基于方法（VT-Former），用于高速公路安全和监测中的车辆轨迹预测。此外，我们还提出了一种新的图像注意力模块（GAT），用于捕捉车辆之间的复杂社交互动。这两个核心组件的结合，实现了准确的车辆轨迹预测。我们在三个标准数据集上进行了三种不同的视角测试，并证明了VT-Former在车辆轨迹预测中的状态之最（SoTA）性和其广泛应用性和稳定性。此外，我们还评估了VT-Former的效率在嵌入板上，并探讨了其在车辆异常检测方面的潜在应用。
</details></li>
</ul>
<hr>
<h2 id="TrainerAgent-Customizable-and-Efficient-Model-Training-through-LLM-Powered-Multi-Agent-System"><a href="#TrainerAgent-Customizable-and-Efficient-Model-Training-through-LLM-Powered-Multi-Agent-System" class="headerlink" title="TrainerAgent: Customizable and Efficient Model Training through LLM-Powered Multi-Agent System"></a>TrainerAgent: Customizable and Efficient Model Training through LLM-Powered Multi-Agent System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06622">http://arxiv.org/abs/2311.06622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyuan Li, Hao Jiang, Tianke Zhang, Zhelun Yu, Aoxiong Yin, Hao Cheng, Siming Fu, Yuhao Zhang, Wanggui He</li>
<li>for: 提高人工智能模型的开发效率和质量，实现个性化服务。</li>
<li>methods: 提出了一种基于多代理系统的TrainerAgent系统，包括任务、数据、模型和服务器代理，这些代理通过分析用户定义的任务、输入数据和要求（如准确率、速度），从数据和模型两个角度全面优化，以获得满足要求的模型，并最终将这些模型部署为在线服务。</li>
<li>results: 实验证明，该系统能够顺利地生成满足要求的模型，并能够检测和排除不可能的任务（如幻想情境或不道德请求），从而确保了系统的可靠性和安全性。<details>
<summary>Abstract</summary>
Training AI models has always been challenging, especially when there is a need for custom models to provide personalized services. Algorithm engineers often face a lengthy process to iteratively develop models tailored to specific business requirements, making it even more difficult for non-experts. The quest for high-quality and efficient model development, along with the emergence of Large Language Model (LLM) Agents, has become a key focus in the industry. Leveraging the powerful analytical, planning, and decision-making capabilities of LLM, we propose a TrainerAgent system comprising a multi-agent framework including Task, Data, Model and Server agents. These agents analyze user-defined tasks, input data, and requirements (e.g., accuracy, speed), optimizing them comprehensively from both data and model perspectives to obtain satisfactory models, and finally deploy these models as online service. Experimental evaluations on classical discriminative and generative tasks in computer vision and natural language processing domains demonstrate that our system consistently produces models that meet the desired criteria. Furthermore, the system exhibits the ability to critically identify and reject unattainable tasks, such as fantastical scenarios or unethical requests, ensuring robustness and safety. This research presents a significant advancement in achieving desired models with increased efficiency and quality as compared to traditional model development, facilitated by the integration of LLM-powered analysis, decision-making, and execution capabilities, as well as the collaboration among four agents. We anticipate that our work will contribute to the advancement of research on TrainerAgent in both academic and industry communities, potentially establishing it as a new paradigm for model development in the field of AI.
</details>
<details>
<summary>摘要</summary>
traditional AI model training has always been challenging, especially when there is a need for custom models to provide personalized services. Algorithm engineers often face a lengthy process to iteratively develop models tailored to specific business requirements, making it even more difficult for non-experts. With the emergence of Large Language Model (LLM) Agents, there is a growing focus on high-quality and efficient model development.我们提出了一种名为TrainerAgent的多智能框架，包括任务、数据、模型和服务器代理。这些代理分析用户定义的任务、输入数据和要求（如准确率和速度），从数据和模型角度进行全面优化，以获得满足要求的模型，并最后将这些模型部署为在线服务。我们的实验评估表明，我们的系统可以适应古典的推论和生成任务，包括计算机视觉和自然语言处理领域。此外，系统还能够批判性地识别和拒绝不可能的任务，如幻想场景或不道德的请求，以确保系统的稳定性和安全性。我们的研究表明，TrainerAgent系统可以在传统模型开发的基础上提供更高效和高质量的模型开发，这得到了LLM智能分析、决策和执行能力的支持，以及代理之间的合作。我们anticipate that our work will contribute to the advancement of research on TrainerAgent in both academic and industry communities, potentially establishing it as a new paradigm for model development in the field of AI.
</details></li>
</ul>
<hr>
<h2 id="Monkey-Image-Resolution-and-Text-Label-Are-Important-Things-for-Large-Multi-modal-Models"><a href="#Monkey-Image-Resolution-and-Text-Label-Are-Important-Things-for-Large-Multi-modal-Models" class="headerlink" title="Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models"></a>Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06607">http://arxiv.org/abs/2311.06607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuliang-liu/monkey">https://github.com/yuliang-liu/monkey</a></li>
<li>paper_authors: Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, Xiang Bai</li>
<li>for: 提高大型多modal模型在复杂场景理解和叙述能力</li>
<li>methods: 提出Monkey方法，包括不需要预训练可以使用现有视觉编码器（如vit-BigHuge）进行提高输入分辨率，以及自动生成多级描述方法以便模型学习场景和对象之间的Contextual关系</li>
<li>results: 在多达16个不同的数据集上进行了广泛的测试，发现Monkey在基本任务上（如图像描述、全视Question Answering和文档Question Answering）具有稳定竞争力的表现<details>
<summary>Abstract</summary>
Large Multimodal Models have demonstrated impressive capabilities in understanding general vision-language tasks. However, due to the limitation of supported input resolution (e.g., 448 x 448) as well as the inexhaustive description of the training image-text pair, these models often encounter challenges when dealing with intricate scene understandings and narratives. Here we address the problem by proposing the Monkey. Our contributions are two-fold: 1) without pretraining from the start, our method can be built upon an existing vision encoder (e.g., vit-BigHuge) to effectively improve the input resolution capacity up to 896 x 1344 pixels; 2) we propose a multi-level description generation method, which automatically provides rich information that can guide model to learn contextual association between scenes and objects. Our extensive testing across more than 16 distinct datasets reveals that Monkey achieves consistently competitive performance over the existing LMMs on fundamental tasks, such as Image Captioning, General Visual Question Answering (VQA), and Document-oriented VQA. Models, interactive demo, and the source code are provided at the following https://github.com/Yuliang-Liu/Monkey.
</details>
<details>
<summary>摘要</summary>
大型多Modal模型在通用视力语言任务上表现出了吸引人的能力。然而，由于输入分辨率的限制（例如448x448）以及训练图片文本对的描述不够详细，这些模型经常在处理复杂的场景理解和 narraves 时遇到挑战。我们解决这个问题，我们提出了猴子（Monkey）。我们的贡献有两个方面：1. 不需要先training，我们的方法可以基于现有的视力编码器（例如 vit-BigHuge）来提高输入分辨率capacity到896x1344像素;2. 我们提出了多 уров层描述生成方法，可以自动提供详细的信息，以帮助模型学习场景和物体之间的上下文关系。我们在16个不同的数据集上进行了广泛的测试，发现Monkey在基本任务上（如图像描述、通用视Question Answering和文档 oriented VQA）与现有的LMMs（Large Multimodal Models）具有相当竞争力。我们提供了模型、交互示例和源代码，可以在以下GitHub上下载：https://github.com/Yuliang-Liu/Monkey。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Grokking-Through-A-Robustness-Viewpoint"><a href="#Understanding-Grokking-Through-A-Robustness-Viewpoint" class="headerlink" title="Understanding Grokking Through A Robustness Viewpoint"></a>Understanding Grokking Through A Robustness Viewpoint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06597">http://arxiv.org/abs/2311.06597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiquan Tan, Weiran Huang</li>
<li>for: 研究一种名为“grokking”的奇异现象，即神经网络在准确适应训练数据后仍然泛化。</li>
<li>methods: 使用Robustness视角来理解这种现象，并提出新的评价指标基于Robustness和信息理论。</li>
<li>results: 发现$l_2$ нор为神经网络泛化的必要条件，但是$l_2$ norm与测试数据不协调，提出新的评价指标可以协调grokking现象。 Additionally, the proposed method can speed up the generalization process, and learning the commutative law can explain part of the speedup.<details>
<summary>Abstract</summary>
Recently, an unusual phenomenon called grokking has gained much attention, where sometimes a neural network generalizes long after it perfectly fits the training data. We try to understand this seemingly strange phenomenon using the robustness of the neural network. Using a robustness viewpoint, we show that the popular $l_2$ weight norm (metric) of the neural network is actually a sufficient condition for grokking. As we also empirically find that $l_2$ norm correlates with grokking on the test data not in a timely way, we propose new metrics based on robustness and information theory and find that our new metrics correlate well with the grokking phenomenon. Based on the previous observations, we propose methods to speed up the generalization process. In addition, we examine the standard training process on modulo addition dataset and find that it hardly learns other basic group operations before grokking, including the commutative law. Interestingly, the speed up of generalization when using our proposed method can be partially explained by learning the commutative law, a necessary condition when the model groks on test dataset.
</details>
<details>
<summary>摘要</summary>
最近，一种奇异现象叫“grokking”在神经网络领域受到了广泛关注，神经网络在训练数据完美适应后仍然能够泛化。我们使用神经网络的稳定性视角来理解这一现象，并证明了$l_2$质量 нор（度量）是泛化现象的必要条件。然而，我们发现$l_2$ нор与测试数据上的泛化不一致，因此我们提出了基于稳定性和信息理论的新度量，并发现它们与泛化现象有高度相关性。基于以前的观察结果，我们提出了加速泛化过程的方法。此外，我们还检查了模式训练过程中的标准处理方法，发现它几乎不会学习测试数据上的其他基本群操作，包括交换律。Interestingly，使用我们提出的方法可以加速泛化过程，其中一部分可以通过学习交换律来解释，交换律是泛化到测试数据的必要条件。
</details></li>
</ul>
<hr>
<h2 id="An-Intelligent-Social-Learning-based-Optimization-Strategy-for-Black-box-Robotic-Control-with-Reinforcement-Learning"><a href="#An-Intelligent-Social-Learning-based-Optimization-Strategy-for-Black-box-Robotic-Control-with-Reinforcement-Learning" class="headerlink" title="An Intelligent Social Learning-based Optimization Strategy for Black-box Robotic Control with Reinforcement Learning"></a>An Intelligent Social Learning-based Optimization Strategy for Black-box Robotic Control with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06576">http://arxiv.org/abs/2311.06576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xubo Yang, Jian Gao, Ting Wang, Yaozhen He</li>
<li>for: 这篇论文的目的是提出一种基于社交学习的智能控制算法，以便控制黑盒系统中的机器人。</li>
<li>methods: 这篇论文使用了一种叫做社交学习算法（Intelligent Social Learning，ISL），它包括学习、模仿和自我研究三种式态。</li>
<li>results: 试验结果显示，ISL算法比四种现有方法在六个连续控制测试案例中更有效率，具有更快的计算速度、更少的参数和更高的稳定性。此外，ISL算法在模拟和实验中的抓取任务中也获得了满意的解决方案。<details>
<summary>Abstract</summary>
Implementing intelligent control of robots is a difficult task, especially when dealing with complex black-box systems, because of the lack of visibility and understanding of how these robots work internally. This paper proposes an Intelligent Social Learning (ISL) algorithm to enable intelligent control of black-box robotic systems. Inspired by mutual learning among individuals in human social groups, ISL includes learning, imitation, and self-study styles. Individuals in the learning style use the Levy flight search strategy to learn from the best performer and form the closest relationships. In the imitation style, individuals mimic the best performer with a second-level rapport by employing a random perturbation strategy. In the self-study style, individuals learn independently using a normal distribution sampling method while maintaining a distant relationship with the best performer. Individuals in the population are regarded as autonomous intelligent agents in each style. Neural networks perform strategic actions in three styles to interact with the environment and the robot and iteratively optimize the network policy. Overall, ISL builds on the principles of intelligent optimization, incorporating ideas from reinforcement learning, and possesses strong search capabilities, fast computation speed, fewer hyperparameters, and insensitivity to sparse rewards. The proposed ISL algorithm is compared with four state-of-the-art methods on six continuous control benchmark cases in MuJoCo to verify its effectiveness and advantages. Furthermore, ISL is adopted in the simulation and experimental grasping tasks of the UR3 robot for validations, and satisfactory solutions are yielded.
</details>
<details>
<summary>摘要</summary>
实现智能控制机器人是一项困难任务，特别是面临复杂黑盒系统时，因为lack of visibility和理解机器人内部的工作方式。这篇论文提议一种智能社会学习（ISL）算法，以帮助智能控制黑盒机器人系统。 Drawing inspiration from human social groups' mutual learning, ISL includes learning, imitation, and self-study styles. Individuals in the learning style use Levy flight search strategy to learn from the best performer and form the closest relationships. In the imitation style, individuals mimic the best performer with a second-level rapport by employing a random perturbation strategy. In the self-study style, individuals learn independently using a normal distribution sampling method while maintaining a distant relationship with the best performer. Individuals in the population are regarded as autonomous intelligent agents in each style. Neural networks perform strategic actions in three styles to interact with the environment and the robot and iteratively optimize the network policy. Overall, ISL builds on the principles of intelligent optimization, incorporating ideas from reinforcement learning, and possesses strong search capabilities, fast computation speed, fewer hyperparameters, and insensitivity to sparse rewards. The proposed ISL algorithm is compared with four state-of-the-art methods on six continuous control benchmark cases in MuJoCo to verify its effectiveness and advantages. Furthermore, ISL is adopted in the simulation and experimental grasping tasks of the UR3 robot for validations, and satisfactory solutions are yielded.
</details></li>
</ul>
<hr>
<h2 id="SCADI-Self-supervised-Causal-Disentanglement-in-Latent-Variable-Models"><a href="#SCADI-Self-supervised-Causal-Disentanglement-in-Latent-Variable-Models" class="headerlink" title="SCADI: Self-supervised Causal Disentanglement in Latent Variable Models"></a>SCADI: Self-supervised Causal Disentanglement in Latent Variable Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06567">http://arxiv.org/abs/2311.06567</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hazel-heejeong-nam/self-supervised-causal-disentanglement">https://github.com/hazel-heejeong-nam/self-supervised-causal-disentanglement</a></li>
<li>paper_authors: Heejeong Nam</li>
<li>for: 本研究旨在提出一种新的自助学习 causal disentanglement 模型，即 SCADI（SElf-supervised CAusal DIsentanglement）模型，以便无需指导或标注数据，通过自动学习方式，捕捉 semantic factor 和其 causal 关系。</li>
<li>methods: 本研究使用了 masked structural causal model (SCM) 和 pseudo-label generator 两种方法，以实现无监督的 causal disentanglement。</li>
<li>results: 研究发现，SCADI 模型能够自动学习 semantic factor 和 causal 关系，无需任何指导或标注数据，并且能够生成可读的 causal 拓扑图。<details>
<summary>Abstract</summary>
Causal disentanglement has great potential for capturing complex situations. However, there is a lack of practical and efficient approaches. It is already known that most unsupervised disentangling methods are unable to produce identifiable results without additional information, often leading to randomly disentangled output. Therefore, most existing models for disentangling are weakly supervised, providing information about intrinsic factors, which incurs excessive costs. Therefore, we propose a novel model, SCADI(SElf-supervised CAusal DIsentanglement), that enables the model to discover semantic factors and learn their causal relationships without any supervision. This model combines a masked structural causal model (SCM) with a pseudo-label generator for causal disentanglement, aiming to provide a new direction for self-supervised causal disentanglement models.
</details>
<details>
<summary>摘要</summary>
causal disentanglement 有很大的潜力，可以捕捉复杂的情况。但是，现有的实用和效率的方法缺乏。大多数无监督分解方法无法生成可识别的结果，通常导致随机分解的输出。因此，现有的分解模型都是弱监督的，提供内在因素的信息，这会带来过高的成本。因此，我们提议一种新的模型， namely SCADI（自我监督 causal disentanglement），它可以让模型发现 semantic factor 和学习其 causal 关系，无需任何监督。这个模型将 masked 结构 causal model（SCM）与 pseudo-label 生成器结合，以实现自我监督 causal disentanglement 模型的新方向。
</details></li>
</ul>
<hr>
<h2 id="Heuristics-Driven-Link-of-Analogy-Prompting-Enhancing-Large-Language-Models-for-Document-Level-Event-Argument-Extraction"><a href="#Heuristics-Driven-Link-of-Analogy-Prompting-Enhancing-Large-Language-Models-for-Document-Level-Event-Argument-Extraction" class="headerlink" title="Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction"></a>Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06555">http://arxiv.org/abs/2311.06555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanzhang Zhou, Junlang Qian, Zijian Feng, Hui Lu, Zixiao Zhu, Kezhi Mao</li>
<li>for: 这篇论文研究了文档级事件说明抽取（EAE）中的在Context learning（ICL）问题。</li>
<li>methods: 该论文提出了一种名为Heuristic-Driven Link-of-Analogy（HD-LoA）的提示方法，通过示例选择和人工智能学习来帮助模型学习任务特有的规则。</li>
<li>results: 该论文通过实验表明，与现有提示方法和少量监督学习方法相比，HD-LoA提示方法在文档级EAE数据集上实现了4.53%和9.38%的F1分数提升，并在另外两个任务中也达到了2.87%和2.63%的准确率提升。<details>
<summary>Abstract</summary>
In this study, we investigate in-context learning (ICL) in document-level event argument extraction (EAE). The paper identifies key challenges in this problem, including example selection, context length limitation, abundance of event types, and the limitation of Chain-of-Thought (CoT) prompting in non-reasoning tasks. To address these challenges, we introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting method. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations via ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a methodical method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their adaptability. Extensive experiments show that our method outperforms the existing prompting methods and few-shot supervised learning methods, exhibiting F1 score improvements of 4.53% and 9.38% on the document-level EAE dataset. Furthermore, when applied to sentiment analysis and natural language inference tasks, the HD-LoA prompting achieves accuracy gains of 2.87% and 2.63%, indicating its effectiveness across different tasks.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究了文档级事件参考抽取（EAE）中的内在学习（ICL）。文章标出了该问题的关键挑战，包括示例选择、上下文长度限制、事件类型的充沛和不可靠的链条（CoT）唤起在非逻辑任务中。为解决这些挑战，我们介绍了逻辑驱动链接 аналоги（HD-LoA）唤起方法。具体来说，我们假设并证明了LLMs通过示例示例学习任务特有的规则。基于这个假设，我们提出了一种显式逻辑驱动示例建构方法，将随机示例选择过程变换成一种系统化的方法，注重任务规则。此外，受人类 аналоги性理解的启发，我们提出了链接 аналоги唤起，使LLMs可以通过对已知情况的分析，处理新情况，提高其适应性。广泛的实验表明，我们的方法在文档级EAE数据集上的 F1 分数提高 4.53% 和 9.38%，并在 Sentiment Analysis 和自然语言推理任务上实现了 Accuracy 的提高。
</details></li>
</ul>
<hr>
<h2 id="Is-Machine-Learning-Unsafe-and-Irresponsible-in-Social-Sciences-Paradoxes-and-Reconsidering-from-Recidivism-Prediction-Tasks"><a href="#Is-Machine-Learning-Unsafe-and-Irresponsible-in-Social-Sciences-Paradoxes-and-Reconsidering-from-Recidivism-Prediction-Tasks" class="headerlink" title="Is Machine Learning Unsafe and Irresponsible in Social Sciences? Paradoxes and Reconsidering from Recidivism Prediction Tasks"></a>Is Machine Learning Unsafe and Irresponsible in Social Sciences? Paradoxes and Reconsidering from Recidivism Prediction Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06537">http://arxiv.org/abs/2311.06537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianhong Liu, Dianshi Li</li>
<li>for: 这篇论文旨在探讨高风险事件预测下的计算方法的基础和热点问题。</li>
<li>methods: 论文提出了一些新的思路，挑战了一些常见的机器学习观点，并提出了一种新的方法融合计算方法和传统社会科学方法。</li>
<li>results: 论文的研究结果表明，这种新的方法可以更好地捕捉社会系统的复杂性和不确定性，提高预测的准确性和可靠性。<details>
<summary>Abstract</summary>
The paper addresses some fundamental and hotly debated issues for high-stakes event predictions underpinning the computational approach to social sciences. We question several prevalent views against machine learning and outline a new paradigm that highlights the promises and promotes the infusion of computational methods and conventional social science approaches.
</details>
<details>
<summary>摘要</summary>
Here's a word-for-word translation of the text into Simplified Chinese:文章讨论了社会科学 Computational Approach 高度风险预测的一些基本和热点问题。我们对数据学习的一些常见看法提出了质疑，并提出了一新的思路，强调计算方法和传统社会科学方法的融合。
</details></li>
</ul>
<hr>
<h2 id="MuST-Multimodal-Spatiotemporal-Graph-Transformer-for-Hospital-Readmission-Prediction"><a href="#MuST-Multimodal-Spatiotemporal-Graph-Transformer-for-Hospital-Readmission-Prediction" class="headerlink" title="MuST: Multimodal Spatiotemporal Graph-Transformer for Hospital Readmission Prediction"></a>MuST: Multimodal Spatiotemporal Graph-Transformer for Hospital Readmission Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07608">http://arxiv.org/abs/2311.07608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Miao, Lequan Yu</li>
<li>for: 预测医院复 admit 是一项重要的方法，可以减少复 admit 率，这是评估医疗系统质量和效果的关键因素。</li>
<li>methods: 本研究提出了一种新的模型，即多modal Spatiotemporal Graph-Transformer (MuST)，用于预测医院复 admit。该模型使用图像 convolutional networks 和时间变换器，可以有效地捕捉 EHR 和胸部X光图像中的空间和时间关系。</li>
<li>results: 我们的实验结果表明，包含多modal特征在内的 MuST 模型在 MIMIC-IV 数据集上的性能明显高于单modal方法。此外，我们提出的管道还超过了目前最佳的方法在医院复 admit 预测方面的表现。<details>
<summary>Abstract</summary>
Hospital readmission prediction is considered an essential approach to decreasing readmission rates, which is a key factor in assessing the quality and efficacy of a healthcare system. Previous studies have extensively utilized three primary modalities, namely electronic health records (EHR), medical images, and clinical notes, to predict hospital readmissions. However, the majority of these studies did not integrate information from all three modalities or utilize the spatiotemporal relationships present in the dataset. This study introduces a novel model called the Multimodal Spatiotemporal Graph-Transformer (MuST) for predicting hospital readmissions. By employing Graph Convolution Networks and temporal transformers, we can effectively capture spatial and temporal dependencies in EHR and chest radiographs. We then propose a fusion transformer to combine the spatiotemporal features from the two modalities mentioned above with the features from clinical notes extracted by a pre-trained, domain-specific transformer. We assess the effectiveness of our methods using the latest publicly available dataset, MIMIC-IV. The experimental results indicate that the inclusion of multimodal features in MuST improves its performance in comparison to unimodal methods. Furthermore, our proposed pipeline outperforms the current leading methods in the prediction of hospital readmissions.
</details>
<details>
<summary>摘要</summary>
This study introduces a novel model called the Multimodal Spatiotemporal Graph-Transformer (MuST) to predict hospital readmissions. By utilizing Graph Convolution Networks and temporal transformers, we can effectively capture spatial and temporal dependencies in EHR and chest radiographs. Additionally, we propose a fusion transformer to combine the spatiotemporal features from the two modalities with features from clinical notes extracted by a pre-trained, domain-specific transformer.We evaluate the effectiveness of our method using the latest publicly available dataset, MIMIC-IV. The experimental results show that the inclusion of multimodal features in MuST improves its performance compared to unimodal methods. Furthermore, our proposed pipeline outperforms the current leading methods in predicting hospital readmissions.
</details></li>
</ul>
<hr>
<h2 id="Modeling-Choice-via-Self-Attention"><a href="#Modeling-Choice-via-Self-Attention" class="headerlink" title="Modeling Choice via Self-Attention"></a>Modeling Choice via Self-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07607">http://arxiv.org/abs/2311.07607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joohwan Ko, Andrew A. Li</li>
<li>for: 这篇论文的目的是提出一种基于现代神经网络架构的选择模型，以便更好地估计选择问题中的模型。</li>
<li>methods: 该论文使用了一种现代神经网络架构——自注意力机制，来提出一种新的选择模型。这种模型可以在几乎相同的数据样本数下支持估计，而且可以在实际应用中提供更高的准确性。</li>
<li>results: 该论文通过设置一个大规模的实际数据集，并对现有的选择模型进行了大规模的比较，发现该提出的选择模型在短期和长期数据Period内都具有优势。<details>
<summary>Abstract</summary>
Models of choice are a fundamental input to many now-canonical optimization problems in the field of Operations Management, including assortment, inventory, and price optimization. Naturally, accurate estimation of these models from data is a critical step in the application of these optimization problems in practice, and so it is perhaps surprising that such choice estimation has to now been accomplished almost exclusively, both in theory and in practice, (a) without the use of deep learning in any meaningful way, and (b) via evaluation on limited data with constantly-changing metrics. This is in stark contrast to the vast majority of similar learning applications, for which the practice of machine learning suggests that (a) neural network-based models are typically state-of-the-art, and (b) strict standardization on evaluation procedures (datasets, metrics, etc.) is crucial. Thus motivated, we first propose a choice model that is the first to successfully (both theoretically and practically) leverage a modern neural network architectural concept (self-attention). Theoretically, we show that our attention-based choice model is a low-rank generalization of the Halo Multinomial Logit model, a recent model that parsimoniously captures irrational choice effects and has seen empirical success. We prove that whereas the Halo-MNL requires $\Omega(m^2)$ data samples to estimate, where $m$ is the number of products, our model supports a natural nonconvex estimator (in particular, that which a standard neural network implementation would apply) which admits a near-optimal stationary point with $O(m)$ samples. We then establish the first realistic-scale benchmark for choice estimation on real data and use this benchmark to run the largest evaluation of existing choice models to date. We find that the model we propose is dominant over both short-term and long-term data periods.
</details>
<details>
<summary>摘要</summary>
选择模型是操作管理领域的基本输入，包括搭配、存储和价格优化等问题。选择模型的准确估计从数据中是应用这些优化问题的重要步骤，但是到目前为止，大多数实际应用中都使用了深度学习。这是与大多数类似学习应用不同的，后者通常使用神经网络模型，并且在评价过程中坚持标准化。因此，我们首先提出一个利用现代神经网络架构思想（自注意）的选择模型，这是第一个成功地（both theoretically and practically）利用自注意来估计选择模型的实际应用。我们证明了我们的注意力基本是唯一的多omialLogit模型的低级泛化，这是一个最近的模型，可以减少人们的偏好选择效应。我们证明了在$m$是产品数量时，哈洛-多omialLogit模型需要$\Omega(m^2)$的数据样本来估计，而我们的模型可以使用标准神经网络实现，并且可以在$O(m)$的样本数据上获得近似最优的站点。我们然后建立了实际规模的选择估计 benchmark，并使用这个 benchmark 来评估现有的选择模型，发现我们的模型在短期和长期数据期间均占据了主导地位。
</details></li>
</ul>
<hr>
<h2 id="How-ChatGPT-is-Solving-Vulnerability-Management-Problem"><a href="#How-ChatGPT-is-Solving-Vulnerability-Management-Problem" class="headerlink" title="How ChatGPT is Solving Vulnerability Management Problem"></a>How ChatGPT is Solving Vulnerability Management Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06530">http://arxiv.org/abs/2311.06530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyu Liu, Junming Liu, Lirong Fu, Kangjie Lu, Yifan Xia, Xuhong Zhang, Wenzhi Chen, Haiqin Weng, Shouling Ji, Wenhai Wang<br>for:这个论文旨在探讨ChatGPT是否可以在实际的漏洞管理任务中表现出色，包括预测安全相关性和补丁正确性等多个方面。methods:这个论文使用了ChatGPT完成6个关于漏洞管理过程的任务，并与现有的最佳实践进行比较，以 investigates the impact of different prompts 和 explore the difficulties。results:论文表明，ChatGPT在某些任务中表现出色，如生成软件漏洞报告标题。然而，ChatGPT也遇到了困难，如直接提供随机示例不能保证良好的性能。 Study reveals that leveraging ChatGPT in a self-heuristic way and effectively guiding ChatGPT to focus on helpful information are promising research directions.<details>
<summary>Abstract</summary>
Recently, ChatGPT has attracted great attention from the code analysis domain. Prior works show that ChatGPT has the capabilities of processing foundational code analysis tasks, such as abstract syntax tree generation, which indicates the potential of using ChatGPT to comprehend code syntax and static behaviors. However, it is unclear whether ChatGPT can complete more complicated real-world vulnerability management tasks, such as the prediction of security relevance and patch correctness, which require an all-encompassing understanding of various aspects, including code syntax, program semantics, and related manual comments.   In this paper, we explore ChatGPT's capabilities on 6 tasks involving the complete vulnerability management process with a large-scale dataset containing 78,445 samples. For each task, we compare ChatGPT against SOTA approaches, investigate the impact of different prompts, and explore the difficulties. The results suggest promising potential in leveraging ChatGPT to assist vulnerability management. One notable example is ChatGPT's proficiency in tasks like generating titles for software bug reports. Furthermore, our findings reveal the difficulties encountered by ChatGPT and shed light on promising future directions. For instance, directly providing random demonstration examples in the prompt cannot consistently guarantee good performance in vulnerability management. By contrast, leveraging ChatGPT in a self-heuristic way -- extracting expertise from demonstration examples itself and integrating the extracted expertise in the prompt is a promising research direction. Besides, ChatGPT may misunderstand and misuse the information in the prompt. Consequently, effectively guiding ChatGPT to focus on helpful information rather than the irrelevant content is still an open problem.
</details>
<details>
<summary>摘要</summary>
近来，ChatGPT在代码分析领域引起了广泛关注。先前的研究表明，ChatGPT可以处理基础代码分析任务，如抽象语法树生成，这表明ChatGPT可能可以理解代码语法和静态行为。然而，是否可以使用ChatGPT完成更加复杂的实际漏洞管理任务，例如预测安全相关性和补丁正确性，它们需要覆盖多个方面，包括代码语法、程序 semantics 和相关手动注释。在这篇论文中，我们探索了ChatGPT在6个任务中的能力，这些任务涉及到了整个漏洞管理过程，使用了78445个样本。对于每个任务，我们与SOTA方法进行比较，研究不同的提示的影响，并探索了困难。结果表明可以使用ChatGPT协助漏洞管理，其中一个例子是ChatGPT在生成软件漏洞报告标题上的护法。此外，我们的发现还揭示了ChatGPT遇到的困难，并提供了可能的未来方向。例如，直接在提示中提供随机示例不一定能够在漏洞管理中达到好的表现。相反，通过在提示中抽取示例本身的专家知识，并将其 интегрирова到提示中是一个有前途的研究方向。此外，ChatGPT可能会对提示中的信息进行错误或不当使用，因此有效地引导ChatGPT关注有用的信息而不是无关的内容仍然是一个开放的问题。
</details></li>
</ul>
<hr>
<h2 id="Conceptual-Model-Interpreter-for-Large-Language-Models"><a href="#Conceptual-Model-Interpreter-for-Large-Language-Models" class="headerlink" title="Conceptual Model Interpreter for Large Language Models"></a>Conceptual Model Interpreter for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07605">http://arxiv.org/abs/2311.07605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fhaer/llm-cmi">https://github.com/fhaer/llm-cmi</a></li>
<li>paper_authors: Felix Härer</li>
<li>for: 这篇论文探讨了使用大语言模型（LLMs）生成和解释概念模型的可能性，并实现了一个概念模型解释器的原型，可以将文本语法生成的概念模型自动渲染为图形模型。</li>
<li>methods: 本论文采用了探索性的研究方法，使用现有的LLMs such as Llama~2和ChatGPT 4生成和解释概念模型，并通过API或本地交互来实现与解释器和LLMs的集成。</li>
<li>results: 本论文的实验结果显示，使用ChatGPT 4和Llama 2生成的模型可以在对话式交互中进行迭代模式化，并且可以在不同的商业和开源LLMs和解释器上支持多种不同的实现方式。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) recently demonstrated capabilities for generating source code in common programming languages. Additionally, commercial products such as ChatGPT 4 started to provide code interpreters, allowing for the automatic execution of generated code fragments, instant feedback, and the possibility to develop and refine in a conversational fashion. With an exploratory research approach, this paper applies code generation and interpretation to conceptual models. The concept and prototype of a conceptual model interpreter is explored, capable of rendering visual models generated in textual syntax by state-of-the-art LLMs such as Llama~2 and ChatGPT 4. In particular, these LLMs can generate textual syntax for the PlantUML and Graphviz modeling software that is automatically rendered within a conversational user interface. The first result is an architecture describing the components necessary to interact with interpreters and LLMs through APIs or locally, providing support for many commercial and open source LLMs and interpreters. Secondly, experimental results for models generated with ChatGPT 4 and Llama 2 are discussed in two cases covering UML and, on an instance level, graphs created from custom data. The results indicate the possibility of modeling iteratively in a conversational fashion.
</details>
<details>
<summary>摘要</summary>
Recently, large language models (LLMs) have shown the ability to generate source code in common programming languages. In addition, commercial products such as ChatGPT 4 have provided code interpreters, allowing for automatic execution of generated code fragments, instant feedback, and the ability to develop and refine in a conversational manner. With an exploratory research approach, this paper applies code generation and interpretation to conceptual models.The concept and prototype of a conceptual model interpreter were explored, capable of rendering visual models generated in textual syntax by state-of-the-art LLMs such as Llama~2 and ChatGPT 4. In particular, these LLMs can generate textual syntax for the PlantUML and Graphviz modeling software that is automatically rendered within a conversational user interface.The first result is an architecture describing the components necessary to interact with interpreters and LLMs through APIs or locally, providing support for many commercial and open-source LLMs and interpreters. Secondly, experimental results for models generated with ChatGPT 4 and Llama 2 are discussed in two cases covering UML and, on an instance level, graphs created from custom data. The results indicate the possibility of modeling iteratively in a conversational fashion.Here's the text in Traditional Chinese:最近，大型语言模型（LLMs）已经显示出生成常用程式语言的源代码的能力。此外，商业产品如ChatGPT 4已经提供了代码解释器，允许将生成的代码片段自动执行，并提供了即时反馈和开发和细化在对话方式下的能力。透过探索性研究方法，这篇论文将应用代码生成和解释到概念模型。这篇论文探索了一个概念模型解释器的概念和原型，可以将由现代 LLMs 如Llama~2和ChatGPT 4生成的文本 syntax 自动转换为可见的Visual模型。具体来说，这些 LLMs 可以生成 PlantUML 和 Graphviz 模型软件的文本 syntax，并将其自动转换为可见的Visual模型。论文的首个结果是一个架构，描述了与解释器和 LLMs 进行交互的 ком成�ionen，以及支持多个商业和开源 LLMs 和解释器的架构。其次，这篇论文针对使用 ChatGPT 4 和 Llama 2 生成的模型进行实验，并分为两个情况进行讨论：UML 和具体情况下的图形。结果显示了可以在对话方式下进行迭代式模型化。
</details></li>
</ul>
<hr>
<h2 id="BClean-A-Bayesian-Data-Cleaning-System"><a href="#BClean-A-Bayesian-Data-Cleaning-System" class="headerlink" title="BClean: A Bayesian Data Cleaning System"></a>BClean: A Bayesian Data Cleaning System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06517">http://arxiv.org/abs/2311.06517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yyssl88/bclean">https://github.com/yyssl88/bclean</a></li>
<li>paper_authors: Jianbin Qin, Sifan Huang, Yaoshu Wang, Jing Zhu, Yifan Zhang, Yukai Miao, Rui Mao, Makoto Onizuka, Chuan Xiao<br>for:BClean is proposed to solve the problem of data cleaning, which is a crucial step in data preprocessing and machine learning.methods:BClean uses Bayesian inference and automatic Bayesian network construction, which can fully exploit the relationships between attributes in the observed dataset and any prior information provided by users. The system also includes an effective scoring model and several approximation strategies to enhance the efficiency of data cleaning.results:BClean achieves an F-measure of up to 0.9 in data cleaning, outperforming existing Bayesian methods by 2% and other data cleaning methods by 15%.<details>
<summary>Abstract</summary>
There is a considerable body of work on data cleaning which employs various principles to rectify erroneous data and transform a dirty dataset into a cleaner one. One of prevalent approaches is probabilistic methods, including Bayesian methods. However, existing probabilistic methods often assume a simplistic distribution (e.g., Gaussian distribution), which is frequently underfitted in practice, or they necessitate experts to provide a complex prior distribution (e.g., via a programming language). This requirement is both labor-intensive and costly, rendering these methods less suitable for real-world applications. In this paper, we propose BClean, a Bayesian Cleaning system that features automatic Bayesian network construction and user interaction. We recast the data cleaning problem as a Bayesian inference that fully exploits the relationships between attributes in the observed dataset and any prior information provided by users. To this end, we present an automatic Bayesian network construction method that extends a structure learning-based functional dependency discovery method with similarity functions to capture the relationships between attributes. Furthermore, our system allows users to modify the generated Bayesian network in order to specify prior information or correct inaccuracies identified by the automatic generation process. We also design an effective scoring model (called the compensative scoring model) necessary for the Bayesian inference. To enhance the efficiency of data cleaning, we propose several approximation strategies for the Bayesian inference, including graph partitioning, domain pruning, and pre-detection. By evaluating on both real-world and synthetic datasets, we demonstrate that BClean is capable of achieving an F-measure of up to 0.9 in data cleaning, outperforming existing Bayesian methods by 2% and other data cleaning methods by 15%.
</details>
<details>
<summary>摘要</summary>
有一大量的研究在数据清洁方面，这些方法使用不同的原则来修正错误数据并将废弃数据集转换为一个更加干净的数据集。其中一种常见的方法是概率方法，包括极 bayesian 方法。然而，现有的概率方法 frequently 假设一个简单的分布（例如， Gaussian 分布），这些分布在实际应用中 часто 被做不当，或者需要专家提供复杂的先前分布（例如，通过编程语言）。这种需求是 Both labor-intensive and costly，使得这些方法在实际应用中不太适用。在这篇论文中，我们提出 BClean，一个基于 Bayesian 的数据清洁系统。我们将数据清洁问题转换为 Bayesian 推理，并将用户提供的先前信息和观察数据中的关系完全利用。为此，我们提出一种自动生成 Bayesian 网络的方法，该方法基于结构学习-基于函数依赖性发现的方法，并使用相似函数来捕捉属性之间的关系。此外，我们的系统允许用户修改生成的 Bayesian 网络，以便指定先前信息或者更正由自动生成过程发现的错误。我们还设计了一种有效的评分模型（即补偿评分模型），以便实现 Bayesian 推理。为提高数据清洁的效率，我们提出了多种approximation 策略，包括图 partitioning、domain pruning 和 pre-detection。通过对真实数据和 sintetic 数据进行评估，我们示出 BClean 可以在数据清洁中 achiev 0.9 的 F-度，比既 Bayesian 方法高 2%，比其他数据清洁方法高 15%。
</details></li>
</ul>
<hr>
<h2 id="Step-by-Step-to-Fairness-Attributing-Societal-Bias-in-Task-oriented-Dialogue-Systems"><a href="#Step-by-Step-to-Fairness-Attributing-Societal-Bias-in-Task-oriented-Dialogue-Systems" class="headerlink" title="Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems"></a>Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06513">http://arxiv.org/abs/2311.06513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsuan Su, Rebecca Qian, Chinnadhurai Sankar, Shahin Shayandeh, Shang-Tse Chen, Hung-yi Lee, Daniel M. Bikel</li>
<li>for: 本文旨在描述一种用于诊断对话系统中偏见的诊断方法，以帮助研究人员更深入地理解偏见的来源。</li>
<li>methods: 本文使用了预训练的大语言模型（LLM），并通过综合分析各个系统 ком ponent的偏见行为，进行偏见诊断。</li>
<li>results: 实验结果表明，对话系统中的偏见通常来自于响应生成模型，而不是其他系统 ком ponent。<details>
<summary>Abstract</summary>
Recent works have shown considerable improvements in task-oriented dialogue (TOD) systems by utilizing pretrained large language models (LLMs) in an end-to-end manner. However, the biased behavior of each component in a TOD system and the error propagation issue in the end-to-end framework can lead to seriously biased TOD responses. Existing works of fairness only focus on the total bias of a system. In this paper, we propose a diagnosis method to attribute bias to each component of a TOD system. With the proposed attribution method, we can gain a deeper understanding of the sources of bias. Additionally, researchers can mitigate biased model behavior at a more granular level. We conduct experiments to attribute the TOD system's bias toward three demographic axes: gender, age, and race. Experimental results show that the bias of a TOD system usually comes from the response generation model.
</details>
<details>
<summary>摘要</summary>
近期研究已经显示了使用预训练大型自然语言模型（LLM）的端到端方式可以获得显著改进的任务对话（TOD）系统。然而，TOD系统中每个组件的偏见行为以及端到端框架中的错误卷积问题可能会导致严重的偏见TOD响应。现有的公平性研究只关注系统总体偏见。在这篇论文中，我们提出了一种诊断方法，用于归因TOD系统中各组件的偏见。通过该归因方法，我们可以更深入地了解偏见的来源。此外，研究人员可以在更细化的水平上 mitigate 模型偏见行为。我们对TOD系统的偏见进行了三个民族轴的诊断：性别、年龄和种族。实验结果表明，TOD系统的偏见通常来自于响应生成模型。
</details></li>
</ul>
<hr>
<h2 id="Knowledgeable-Preference-Alignment-for-LLMs-in-Domain-specific-Question-Answering"><a href="#Knowledgeable-Preference-Alignment-for-LLMs-in-Domain-specific-Question-Answering" class="headerlink" title="Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering"></a>Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06503">http://arxiv.org/abs/2311.06503</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjukg/knowpat">https://github.com/zjukg/knowpat</a></li>
<li>paper_authors: Yichi Zhang, Zhuo Chen, Yin Fang, Lei Cheng, Yanxi Lu, Fangming Li, Wen Zhang, Huajun Chen</li>
<li>for: 这 paper 的目的是应用语言模型 (LLM) 于域pecific问答 (QA) 领域，利用域知 graph (KG)，以解决现实Scene 中 LLM 应用中的两个主要difficulty：一是生成内容需要是用户友好的，二是模型需要正确地利用域知ledge。</li>
<li>methods: 这 paper 提出了一个新的管道，称为 Knowledgeable Preference AlignmenT (KnowPAT)，它使用了两种偏好集合：style preference set 和 knowledge preference set，并设计了一个新的对 alignment 目标，以让 LLM 的偏好与人类偏好相align。</li>
<li>results: 根据对 15 个基线方法的比较，这 paper 的 KnowPAT 管道在实际Scene 中域specific问答中表现出色，超越了 15 个基eline方法。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/zjukg/KnowPAT">https://github.com/zjukg/KnowPAT</a> 上获取。<details>
<summary>Abstract</summary>
Recently, the development of large language models (LLMs) has attracted wide attention in academia and industry. Deploying LLMs to real scenarios is one of the key directions in the current Internet industry. In this paper, we present a novel pipeline to apply LLMs for domain-specific question answering (QA) that incorporates domain knowledge graphs (KGs), addressing an important direction of LLM application. As a real-world application, the content generated by LLMs should be user-friendly to serve the customers. Additionally, the model needs to utilize domain knowledge properly to generate reliable answers. These two issues are the two major difficulties in the LLM application as vanilla fine-tuning can not adequately address them. We think both requirements can be unified as the model preference problem that needs to align with humans to achieve practical application. Thus, we introduce Knowledgeable Preference AlignmenT (KnowPAT), which constructs two kinds of preference set called style preference set and knowledge preference set respectively to tackle the two issues. Besides, we design a new alignment objective to align the LLM preference with human preference, aiming to train a better LLM for real-scenario domain-specific QA to generate reliable and user-friendly answers. Adequate experiments and comprehensive with 15 baseline methods demonstrate that our KnowPAT is an outperforming pipeline for real-scenario domain-specific QA with LLMs. Our code is open-source at https://github.com/zjukg/KnowPAT.
</details>
<details>
<summary>摘要</summary>
最近，大型语言模型（LLM）的发展吸引了学术和产业界的广泛关注。将LLM应用到实际场景是当前互联网业界的一个重要方向。在这篇论文中，我们提出了一个新的管道，用于将LLM应用于域pecific问答（QA）中，并利用域知识图（KG），解决LLM应用中的重要方向。作为实际应用，生成的内容应该是用户友好，以服务于客户。此外，模型需要正确地利用域知识，以生成可靠的答案。这两个问题是LLM应用中的两大difficulty，vanilla fine-tuning无法充分解决。我们认为，这两个问题可以被统称为模型偏好问题，需要与人类Alignment，以实现实际应用。因此，我们提出了知识偏好Alignment（KnowPAT），它构建了两种偏好集，namely style preference set和knowledge preference set，分别解决这两个问题。此外，我们设计了一个新的对Alignment objective，以将LLM的偏好与人类偏好Alignment，以训练更好的LLM，以生成可靠和用户友好的答案。我们的实验和对15种基准方法进行了详细的比较，示出了我们的KnowPAT在实际场景下的域pecific问答with LLM的表现优于15种基准方法。我们的代码可以在https://github.com/zjukg/KnowPAT上获取。
</details></li>
</ul>
<hr>
<h2 id="DRUformer-Enhancing-the-driving-scene-Important-object-detection-with-driving-relationship-self-understanding"><a href="#DRUformer-Enhancing-the-driving-scene-Important-object-detection-with-driving-relationship-self-understanding" class="headerlink" title="DRUformer: Enhancing the driving scene Important object detection with driving relationship self-understanding"></a>DRUformer: Enhancing the driving scene Important object detection with driving relationship self-understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06497">http://arxiv.org/abs/2311.06497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingjie Niu, Ming Ding, Keisuke Fujii, Kento Ohtani, Alexander Carballo, Kazuya Takeda<br>for: 这篇论文的目的是帮助车辆预测重要的物体，以提高安全驾驶。methods: 这篇论文使用了一个名为DRUformer的多模式转换器模型，考虑了所有参与者之间的关系，并将驾驶意向 embed 到模型中。results: 该模型在DRAMA数据集上进行比较实验，与其他现有的SOTA模型进行比较，获得了16.2%的mIoU提升和12.3%的ACC提升。此外，该模型在不同的道路场景和类别下实现了重要物体检测的多元效果。<details>
<summary>Abstract</summary>
Traffic accidents frequently lead to fatal injuries, contributing to over 50 million deaths until 2023. To mitigate driving hazards and ensure personal safety, it is crucial to assist vehicles in anticipating important objects during travel. Previous research on important object detection primarily assessed the importance of individual participants, treating them as independent entities and frequently overlooking the connections between these participants. Unfortunately, this approach has proven less effective in detecting important objects in complex scenarios. In response, we introduce Driving scene Relationship self-Understanding transformer (DRUformer), designed to enhance the important object detection task. The DRUformer is a transformer-based multi-modal important object detection model that takes into account the relationships between all the participants in the driving scenario. Recognizing that driving intention also significantly affects the detection of important objects during driving, we have incorporated a module for embedding driving intention. To assess the performance of our approach, we conducted a comparative experiment on the DRAMA dataset, pitting our model against other state-of-the-art (SOTA) models. The results demonstrated a noteworthy 16.2\% improvement in mIoU and a substantial 12.3\% boost in ACC compared to SOTA methods. Furthermore, we conducted a qualitative analysis of our model's ability to detect important objects across different road scenarios and classes, highlighting its effectiveness in diverse contexts. Finally, we conducted various ablation studies to assess the efficiency of the proposed modules in our DRUformer model.
</details>
<details>
<summary>摘要</summary>
交通事故常引起致命伤害，至2023年已经导致50多万人死亡。为了实现安全驾驶和预防驾驶危险，它是非常重要的帮助车辆预测重要的物件。过去的研究主要集中在个人参与者的重要性，将它们视为独立的实体，往往忽略了参与者之间的关系。可是，这种方法在复杂的情况下显示出较差的检测效果。因此，我们提出了驾驶景况关系自我理解变数former（DRUformer），用于提高重要物件检测任务。DRUformer 是基于 transformer 的多模式重要物件检测模型，考虑所有参与者在驾驶景况中的关系。认识到驾驶意向也对重要物件检测 during driving 有重要影响，我们将驾驶意向模块 embed 到我们的模型中。为了评估我们的方法效果，我们在 DRAMA dataset 上进行了比较性实验，与其他现有的 SOTA 方法进行比较。结果显示 DRUformer 在 mIoU 方面获得了可注目的 16.2% 提升，并在 ACC 方面获得了重要的 12.3% 提升，与 SOTA 方法相比。此外，我们进行了多种简洁分析，以评估 DRUformer 模型在不同的道路enario 和类别中的效果，显示它在多元的情况下具有优秀的效果。最后，我们进行了多种简洁分析，以评估 DRUformer 模型中各个模块的效率。
</details></li>
</ul>
<hr>
<h2 id="Finetuning-Text-to-Image-Diffusion-Models-for-Fairness"><a href="#Finetuning-Text-to-Image-Diffusion-Models-for-Fairness" class="headerlink" title="Finetuning Text-to-Image Diffusion Models for Fairness"></a>Finetuning Text-to-Image Diffusion Models for Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07604">http://arxiv.org/abs/2311.07604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xudong Shen, Chao Du, Tianyu Pang, Min Lin, Yongkang Wong, Mohan Kankanhalli</li>
<li>for: 这篇论文目的是实现文本至图像传播模型中的公平性。</li>
<li>methods: 这篇论文使用了两个主要技术贡献：首先，一个 Distributional alignment loss，可以将生成图像中的特定特征调整到使用者定义的目标分布上；其次，偏见直接调整Diffusion Model的抽样过程，这样可以更好地优化生成图像上的损失。</li>
<li>results: 这篇论文的实验结果表明，使用我们的方法可以对职业描述进行重大减少gender、race和其 intersectional偏见。 gender偏见可以在仅五个软标签的情况下得到明显减少。更重要的是，我们的方法可以支持多种公平的观点，例如控制年龄分布为75%的年轻人和25%的老人，同时对gender和race进行减少偏见。最后，我们的方法可以应对多个概念的偏见，只需要在调整资料中包含这些描述。<details>
<summary>Abstract</summary>
The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a distorted worldview and limit opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) biased direct finetuning of diffusion model's sampling process, which leverages a biased gradient to more effectively optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives of fairness beyond absolute equality, which is demonstrated by controlling age to a $75\%$ young and $25\%$ old distribution while simultaneously debiasing gender and race. Finally, our method is scalable: it can debias multiple concepts at once by simply including these prompts in the finetuning data. We hope our work facilitates the social alignment of T2I generative AI. We will share code and various debiased diffusion model adaptors.
</details>
<details>
<summary>摘要</summary>
社会中文本到图像扩散模型的快速采纳标志着必须解决这些模型的偏见。如果没有干预措施，这些偏见可能会延续一个扭曲的世界观和限制少数群体的机会。在这项工作中，我们将公平视为分布对齐问题。我们的解决方案包括两个主要技术贡献：1. 分布对齐损失，使生成图像的特定特征向用户定义的目标分布进行调整。2. 偏见直接训练扩散模型的采样过程的方法，利用偏见的梯度更好地优化生成图像上的损失。实验表明，我们的方法可以显著减少 gender、种族和他们的交叉性偏见，特别是只需要五个软token进行微调。此外，我们的方法还可以控制年龄分布，将图像生成到75%的年轻人和25%的老年人之间。最后，我们的方法可以同时控制多个概念的偏见，只需要在微调数据中包含这些提示。我们希望我们的工作可以促进文本到图像生成AI的社会对齐。我们将分享代码和多个减偏 diffusion model adapter。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Language-based-Mental-Health-Assessment-with-Item-Response-Theory"><a href="#Adaptive-Language-based-Mental-Health-Assessment-with-Item-Response-Theory" class="headerlink" title="Adaptive Language-based Mental Health Assessment with Item-Response Theory"></a>Adaptive Language-based Mental Health Assessment with Item-Response Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06467">http://arxiv.org/abs/2311.06467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasudha Varadarajan, Sverker Sikström, Oscar N. E. Kjell, H. Andrew Schwartz</li>
<li>For: 这个研究旨在开发一种适应语言基本评估方法，以估计个人的心理分数基于有限的语言回答。* Methods: 研究使用了两种统计学学习方法：类传统测试理论（CTT）和项回快捷论（IRT）。* Results: 研究发现，使用适应测试可以大幅减少需要ask的问题数量，从11个问题降低到3个问题（对于抑郁）和5个问题（对于焦虑），而且使用ALIRT模型可以实现最高的准确率（如 Pearson r ≈ 0.93 ），同时减少问题数量。<details>
<summary>Abstract</summary>
Mental health issues widely vary across individuals - the manifestations of signs and symptoms can be fairly heterogeneous. Recently, language-based depression and anxiety assessments have shown promise for capturing this heterogeneous nature by evaluating a patient's own language, but such approaches require a large sample of words per person to be accurate. In this work, we introduce adaptive language-based assessment - the task of iteratively estimating an individual's psychological score based on limited language responses to questions that the model also decides to ask. To this end, we explore two statistical learning-based approaches for measurement/scoring: classical test theory (CTT) and item response theory (IRT). We find that using adaptive testing in general can significantly reduce the number of questions required to achieve high validity (r ~ 0.7) with standardized tests, bringing down from 11 total questions down to 3 for depression and 5 for anxiety. Given the combinatorial nature of the problem, we empirically evaluate multiple strategies for both the ordering and scoring objectives, introducing two new methods: a semi-supervised item response theory based method (ALIRT), and a supervised actor-critic based model. While both of the models achieve significant improvements over random and fixed orderings, we find ALIRT to be a scalable model that achieves the highest accuracy with lower numbers of questions (e.g. achieves Pearson r ~ 0.93 after only 3 questions versus asking all 11 questions). Overall, ALIRT allows prompting a reduced number of questions without compromising accuracy or overhead computational costs.
</details>
<details>
<summary>摘要</summary>
心理健康问题在各个人之间很有差异 - 症状的表现可以很异化。在最近的语言基于评估中，使用患者自己的语言来评估心理健康的表现已经显示了批 promise。然而，这些方法需要每个人提供大量的语言数据来达到准确性。在这种情况下，我们介绍了适应语言基本评估 - 通过限制语言问题的数量来评估个体的心理分数。为此，我们 explore了两种统计学学习方法：классиical test theory（CTT）和item response theory（IRT）。我们发现，使用适应测试可以significantly reducethe number of questions required to achieve high validity（r ≈ 0.7）with standardized tests，从11个问题降低到3个问题（对压力问题）和5个问题（对抑郁问题）。由于问题的组合性，我们进行了多种策略的实验性评估，包括两种新方法：一种基于 semi-supervised item response theory的方法（ALIRT），以及一种基于supervised actor-critic模型的方法。虽然两种模型都实现了 Random和固定顺序的改进，但我们发现 ALIRT 是一种可扩展的模型，可以在减少问题数量的情况下保持高度的准确性（例如，在只需要3个问题时达到 Pearson r ≈ 0.93）。总之，ALIRT 允许在减少问题数量的情况下进行评估，不会增加计算成本或承载压力。
</details></li>
</ul>
<hr>
<h2 id="Electronic-Communication-Data-Link-Encryption-Simulation-Based-on-Wireless-Communication"><a href="#Electronic-Communication-Data-Link-Encryption-Simulation-Based-on-Wireless-Communication" class="headerlink" title="Electronic Communication Data Link Encryption Simulation Based on Wireless Communication"></a>Electronic Communication Data Link Encryption Simulation Based on Wireless Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06462">http://arxiv.org/abs/2311.06462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rulin Bai</li>
<li>for: 提高电子通信数据链Encryption的模拟效果</li>
<li>methods: 基于无线通信技术研究，提高护圈卷私钥 cryptographic algorithm，建立系统加密模型，获取合法有效节点私钥，评估系统安全特性，验证钥匙安全性，实现无线网络通信加密优化</li>
<li>results: 实验结果表明，使用改进的护圈卷私钥 cryptographic algorithm simulate系统数据链Encryption在无线网络通信中，时间只需2.31毫秒，比其他算法更低。结论：研究表明，基于无线通信技术可以有效提高电子通信数据链Encryption的模拟效果。<details>
<summary>Abstract</summary>
In order to improve the simulation effect of electronic communication data link encryption, the author proposes a solution based on wireless communication. The main content of this technology is based on the research of wireless communication, improve the elliptic curve cryptographic algorithm to build a system encryption model, obtain legal and valid node private keys, evaluate and analyze the relevant security attributes of the system, verify the security of the keys, and realize the encryption optimization of wireless network communication. Experimental results show that: Using the improved elliptic curve to simulate the system data chain encryption under the certificateless public key cryptosystem in network communication, the time is only 2.31 milliseconds, which is lower than other algorithms. Conclusion: It is proved that the technology research based on wireless communication can effectively improve the encryption simulation effect of electronic communication data link.
</details>
<details>
<summary>摘要</summary>
要提高电子通信数据链加密的模拟效果，作者提出了基于无线通信的解决方案。该技术的主要内容是基于无线通信的研究，改进椭圆曲线密码算法，建立系统加密模型，获得法理合法的节点私钥，评估和分析系统安全特性，验证密钥安全性，并实现无线网络通信加密优化。实验结果显示，使用改进的椭圆曲线来模拟系统数据链加密under certificateless public key cryptosystem在网络通信中，时间只需2.31毫秒，比其他算法更低。结论：研究表明，基于无线通信技术可以有效地提高电子通信数据链加密的模拟效果。
</details></li>
</ul>
<hr>
<h2 id="Online-Advertisements-with-LLMs-Opportunities-and-Challenges"><a href="#Online-Advertisements-with-LLMs-Opportunities-and-Challenges" class="headerlink" title="Online Advertisements with LLMs: Opportunities and Challenges"></a>Online Advertisements with LLMs: Opportunities and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07601">http://arxiv.org/abs/2311.07601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei, Suho Shin</li>
<li>for: 这篇论文探讨了在在线广告系统中使用大语言模型（LLM）的可能性。</li>
<li>methods: 论文介绍了必须满足的基本需求，包括隐私、延迟、可靠性、用户和广告商满意度。并提出了一个通用的LLM广告框架，包括修改、拍卖、预测和拍卖模块。</li>
<li>results: 论文提出了不同设计考虑和实施技术挑战。<details>
<summary>Abstract</summary>
This paper explores the potential for leveraging Large Language Models (LLM) in the realm of online advertising systems. We delve into essential requirements including privacy, latency, reliability, users and advertisers' satisfaction, which such a system must fulfill. We further introduce a general framework for LLM advertisement, consisting of modification, bidding, prediction, and auction modules. Different design considerations for each module is presented, with an in-depth examination of their practicality and the technical challenges inherent to their implementation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Aria-NeRF-Multimodal-Egocentric-View-Synthesis"><a href="#Aria-NeRF-Multimodal-Egocentric-View-Synthesis" class="headerlink" title="Aria-NeRF: Multimodal Egocentric View Synthesis"></a>Aria-NeRF: Multimodal Egocentric View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06455">http://arxiv.org/abs/2311.06455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiankai Sun, Jianing Qiu, Chuanyang Zheng, John Tucker, Javier Yu, Mac Schwager<br>for:  This paper aims to accelerate research in developing rich, multimodal scene models trained from egocentric data, with applications in VR&#x2F;AR and intelligent agents.methods:  The paper uses differentiable volumetric ray-tracing inspired by Neural Radiance Fields (NeRFs) to construct a NeRF-like model from an egocentric image sequence.results:  The paper presents a comprehensive multimodal egocentric video dataset, featuring diverse data modalities and real-world context, as a foundation for furthering our understanding of human behavior and enabling more immersive and intelligent experiences in VR, AR, and robotics.<details>
<summary>Abstract</summary>
We seek to accelerate research in developing rich, multimodal scene models trained from egocentric data, based on differentiable volumetric ray-tracing inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like model from an egocentric image sequence plays a pivotal role in understanding human behavior and holds diverse applications within the realms of VR/AR. Such egocentric NeRF-like models may be used as realistic simulations, contributing significantly to the advancement of intelligent agents capable of executing tasks in the real-world. The future of egocentric view synthesis may lead to novel environment representations going beyond today's NeRFs by augmenting visual data with multimodal sensors such as IMU for egomotion tracking, audio sensors to capture surface texture and human language context, and eye-gaze trackers to infer human attention patterns in the scene. To support and facilitate the development and evaluation of egocentric multimodal scene modeling, we present a comprehensive multimodal egocentric video dataset. This dataset offers a comprehensive collection of sensory data, featuring RGB images, eye-tracking camera footage, audio recordings from a microphone, atmospheric pressure readings from a barometer, positional coordinates from GPS, connectivity details from Wi-Fi and Bluetooth, and information from dual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The dataset was collected with the Meta Aria Glasses wearable device platform. The diverse data modalities and the real-world context captured within this dataset serve as a robust foundation for furthering our understanding of human behavior and enabling more immersive and intelligent experiences in the realms of VR, AR, and robotics.
</details>
<details>
<summary>摘要</summary>
我们寻求加速开展具有丰富多modal scene模型的研究，基于可微分数据trace的射影类似NeRF（Neural Radiance Fields）。从 Egocentric 影像序列建立NeRF-like模型扮演着重要的角色，可以更好地理解人类行为，并具有广泛应用于VR/AR等领域。这些 Egocentric NeRF-like 模型可以用来生成真实的simulation，对于在真实世界中进行任务的智能代理人具有重要意义。未来的 Egocentric 视角合成可能将会超越今天的NeRFs，通过与多modal感应器（如IMU、Audio、眼动追踪等）集成，增强视觉数据，并从人类语言上下文中获取更多的信息。为了支持和促进 Egocentric 多modal scene 模型的开发和评估，我们提供了一个完整的多modal Egocentric 影像Dataset。这个dataset包括RGB图像、眼动摄影机、麦克风录音、气压测量、GPS位置坐标、Wi-Fi和蓝牙连接资讯以及双频率IMU数据（1kHz和800Hz）和磁ometer。这个dataset在Meta Aria Glasses 挂架台上进行收集。这个多modal的数据模式和在真实世界中捕捉的情感上，将成为更好的基础 для进一步理解人类行为，并实现更 immerse 和智能的VR、AR和机器人体验。
</details></li>
</ul>
<hr>
<h2 id="THOS-A-Benchmark-Dataset-for-Targeted-Hate-and-Offensive-Speech"><a href="#THOS-A-Benchmark-Dataset-for-Targeted-Hate-and-Offensive-Speech" class="headerlink" title="THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech"></a>THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06446">http://arxiv.org/abs/2311.06446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mohaimeed/thos">https://github.com/mohaimeed/thos</a></li>
<li>paper_authors: Saad Almohaimeed, Saleh Almohaimeed, Ashfaq Ali Shafin, Bogdan Carbunar, Ladislau Bölöni</li>
<li>for: 这篇论文的目的是提供一个精细的推特上的危险内容分类 dataset，以便使用大型自然语言模型来进行分类。</li>
<li>methods: 本论文使用了 manually labeled 的 tweets，并使用了 Large Language Models 来进行分类。</li>
<li>results: 研究人员通过使用 THOS  dataset，成功地使用 Large Language Models 进行分类，并达到了高度的准确率。<details>
<summary>Abstract</summary>
Detecting harmful content on social media, such as Twitter, is made difficult by the fact that the seemingly simple yes/no classification conceals a significant amount of complexity. Unfortunately, while several datasets have been collected for training classifiers in hate and offensive speech, there is a scarcity of datasets labeled with a finer granularity of target classes and specific targets. In this paper, we introduce THOS, a dataset of 8.3k tweets manually labeled with fine-grained annotations about the target of the message. We demonstrate that this dataset makes it feasible to train classifiers, based on Large Language Models, to perform classification at this level of granularity.
</details>
<details>
<summary>摘要</summary>
检测社交媒体上的危险内容，如推特上的负面或仇恨言论，受到复杂性的限制。实际上，许多数据集已经为训练分类器而收集，但是它们的标签精度尚不够。在这篇论文中，我们介绍了THOS数据集，包含8.3万个推特消息的手动标注细化目标类别。我们示示了这个数据集使得基于大语言模型的分类器可以在这种精度水平上进行分类。
</details></li>
</ul>
<hr>
<h2 id="Controllability-Constrained-Deep-Network-Models-for-Enhanced-Control-of-Dynamical-Systems"><a href="#Controllability-Constrained-Deep-Network-Models-for-Enhanced-Control-of-Dynamical-Systems" class="headerlink" title="Controllability-Constrained Deep Network Models for Enhanced Control of Dynamical Systems"></a>Controllability-Constrained Deep Network Models for Enhanced Control of Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06438">http://arxiv.org/abs/2311.06438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suruchi1997/controlledvae">https://github.com/suruchi1997/controlledvae</a></li>
<li>paper_authors: Suruchi Sharma, Volodymyr Makarenko, Gautam Kumar, Stas Tiomkin</li>
<li>for: 这篇论文的目的是提出一种控制理论基于的方法，以提高由数据驱动的模型中的控制性。</li>
<li>methods: 这篇论文使用了深度神经网络（DNN）来估算未知动力学模型，并在控制输入和状态观测输出之间进行数据驱动模型的估算。</li>
<li>results: 该方法可以提高模型中的控制性，并且可以提供更高效的控制器、更好的解释性和更低的长期预测误差。这些结果表明了数据驱动模型的控制性可以通过控制理论基于的方法进行改进。<details>
<summary>Abstract</summary>
Control of a dynamical system without the knowledge of dynamics is an important and challenging task. Modern machine learning approaches, such as deep neural networks (DNNs), allow for the estimation of a dynamics model from control inputs and corresponding state observation outputs. Such data-driven models are often utilized for the derivation of model-based controllers. However, in general, there are no guarantees that a model represented by DNNs will be controllable according to the formal control-theoretical meaning of controllability, which is crucial for the design of effective controllers. This often precludes the use of DNN-estimated models in applications, where formal controllability guarantees are required. In this proof-of-the-concept work, we propose a control-theoretical method that explicitly enhances models estimated from data with controllability. That is achieved by augmenting the model estimation objective with a controllability constraint, which penalizes models with a low degree of controllability. As a result, the models estimated with the proposed controllability constraint allow for the derivation of more efficient controllers, they are interpretable by the control-theoretical quantities and have a lower long-term prediction error. The proposed method provides new insights on the connection between the DNN-based estimation of unknown dynamics and the control-theoretical guarantees of the solution properties. We demonstrate the superiority of the proposed method in two standard classical control systems with state observation given by low resolution high-dimensional images.
</details>
<details>
<summary>摘要</summary>
<<sys.language_model.translate(text="Control of a dynamical system without knowledge of dynamics is an important and challenging task. Modern machine learning approaches, such as deep neural networks (DNNs), allow for the estimation of a dynamics model from control inputs and corresponding state observation outputs. However, there are no guarantees that a model represented by DNNs will be controllable according to the formal control-theoretical meaning of controllability, which is crucial for the design of effective controllers. In this proof-of-concept work, we propose a control-theoretical method that explicitly enhances models estimated from data with controllability. The proposed method provides new insights on the connection between the DNN-based estimation of unknown dynamics and the control-theoretical guarantees of the solution properties. We demonstrate the superiority of the proposed method in two standard classical control systems with state observation given by low resolution high-dimensional images.")]>>Here's the translation in Traditional Chinese:<<sys.language_model.translate(text="控制无知系统是一个重要和挑战性的任务。现代机器学习方法，如深度神经网络（DNNs），允许从控制输入和相应的状态观察输出来估计动力学模型。然而，没有任何保证，表示由DNNs所表示的模型具有控制性的正式控制理论意义上的 garanties，这是控制器设计中的重要因素。在这个证明原理的工作中，我们提出了一个控制理论方法，可以明确地增强从数据估计的模型，以提高控制性。这个方法提供了新的见解，将DNN-based estimation of unknown dynamics与控制理论上的 garanties的连接。我们在两个标准的古典控制系统中，使用低分辨率高维度图像进行状态观察，进行说明和比较。">>
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/11/cs.AI_2023_11_11/" data-id="clpahu6z9006x3h881n4q95a6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/11/cs.CL_2023_11_11/" class="article-date">
  <time datetime="2023-11-11T11:00:00.000Z" itemprop="datePublished">2023-11-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/11/cs.CL_2023_11_11/">cs.CL - 2023-11-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Intentional-Biases-in-LLM-Responses"><a href="#Intentional-Biases-in-LLM-Responses" class="headerlink" title="Intentional Biases in LLM Responses"></a>Intentional Biases in LLM Responses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07611">http://arxiv.org/abs/2311.07611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicklaus Badyal, Derek Jacoby, Yvonne Coady</li>
<li>for: 这个研究旨在将对话语言模型中的偏见故意引入，以创造特定的人物形象，用于互动媒体目的。</li>
<li>methods: 这个研究使用了 Falcon-7b 和 Open AI 的 GPT-4 开源模型，并评估了它们对不同角色的回应。</li>
<li>results: 研究发现，GPT-4 的监管器模型可以确保 AI 的调整，但是它们在创造不同角色的偏见时不够有用。<details>
<summary>Abstract</summary>
In this study we intentionally introduce biases into large language model responses in an attempt to create specific personas for interactive media purposes. We explore the differences between open source models such as Falcon-7b and the GPT-4 model from Open AI, and we quantify some differences in responses afforded by the two systems. We find that the guardrails in the GPT-4 mixture of experts models with a supervisor, while useful in assuring AI alignment in general, are detrimental in trying to construct personas with a variety of uncommon viewpoints. This study aims to set the groundwork for future exploration in intentional biases of large language models such that these practices can be applied in the creative field, and new forms of media.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们故意引入大语言模型的偏见，以创造特定的人物形象，用于互动媒体目的。我们比较了开源模型 falcon-7b 和 open AI 的 GPT-4 模型，并量化了两者响应的一些不同。我们发现，GPT-4 的混合专家模型的监督器，虽有用于保证 AI Compatibility，但在构建多种不同观点的人物时，是不利的。本研究的目的是为未来在大语言模型中意外偏见的实践提供基础，以便在艺术领域和新媒体中应用这些技术。
</details></li>
</ul>
<hr>
<h2 id="A-Template-Is-All-You-Meme"><a href="#A-Template-Is-All-You-Meme" class="headerlink" title="A Template Is All You Meme"></a>A Template Is All You Meme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06649">http://arxiv.org/abs/2311.06649</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ukplab/a-template-is-all-you-meme">https://github.com/ukplab/a-template-is-all-you-meme</a></li>
<li>paper_authors: Luke Bates, Peter Ebert Christensen, Preslav Nakov, Iryna Gurevych</li>
<li>For: The paper aims to improve the understanding of memes and their context, and to develop a method to inject context into machine learning models for better meme classification.* Methods: The authors release a large knowledge base of memes and information from <a target="_blank" rel="noopener" href="http://www.knowyourmeme.com/">www.knowyourmeme.com</a>, and create a non-parametric majority-based classifier called Template-Label Counter (TLC) to test their hypothesis that meme templates can provide missing context for machine learning models.* Results: The authors conduct thorough classification experiments and exploratory data analysis to demonstrate the effectiveness of their method and the value of their knowledge base for meme analysis tasks.<details>
<summary>Abstract</summary>
Memes are a modern form of communication and meme templates possess a base semantics that is customizable by whomever posts it on social media. Machine learning systems struggle with memes, which is likely due to such systems having insufficient context to understand memes, as there is more to memes than the obvious image and text. Here, to aid understanding of memes, we release a knowledge base of memes and information found on www.knowyourmeme.com, which we call the Know Your Meme Knowledge Base (KYMKB), composed of more than 54,000 images. The KYMKB includes popular meme templates, examples of each template, and detailed information about the template. We hypothesize that meme templates can be used to inject models with the context missing from previous approaches. To test our hypothesis, we create a non-parametric majority-based classifier, which we call Template-Label Counter (TLC). We find TLC more effective than or competitive with fine-tuned baselines. To demonstrate the power of meme templates and the value of both our knowledge base and method, we conduct thorough classification experiments and exploratory data analysis in the context of five meme analysis tasks.
</details>
<details>
<summary>摘要</summary>
现代通信的形式之一是memes，它们具有可自定义的基本 semantics，可以在社交媒体上分享。机器学习系统对memes表示困难，可能是因为这些系统缺乏memes的Context，因为memes比图像和文本更多。为了帮助理解memes，我们发布了www.knowyourmeme.com上的知识库，称之为知识库（KYMKB），包含超过54,000个图像。KYMKB包括流行的meme模板，每个模板的示例和详细信息。我们提出的假设是，meme模板可以用来补充过去方法缺失的Context。为了测试这个假设，我们创建了一种非 Parametric多数策略，称之为模板标签计数器（TLC）。我们发现TLC比或与精心调整的基线相当有效。为了证明meme模板和我们的知识库以及方法的力量，我们在五种meme分析任务中进行了严格的分类实验和探索数据分析。
</details></li>
</ul>
<hr>
<h2 id="Robust-Text-Classification-Analyzing-Prototype-Based-Networks"><a href="#Robust-Text-Classification-Analyzing-Prototype-Based-Networks" class="headerlink" title="Robust Text Classification: Analyzing Prototype-Based Networks"></a>Robust Text Classification: Analyzing Prototype-Based Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06647">http://arxiv.org/abs/2311.06647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhivar Sourati, Darshan Deshpande, Filip Ilievski, Kiril Gashteovski, Sascha Saralajew</li>
<li>for: 本研究旨在检验 prototype-based 网络（PBN）在文本分类任务中是否具有鲁棒性特性。</li>
<li>methods: 我们采用了一种模块化和全面的研究框架，包括不同的后处理架构、后处理大小和目标函数。我们的评估协议对模型进行了不同级别的拟合干扰测试。</li>
<li>results: 我们的实验结果表明，PBNs在面对现实的拟合干扰时保持了鲁棒性。此外，PBNs的鲁棒性主要归功于保持概念可读性的目标函数，而与普通模型相比，PBNs在数据越复杂时的鲁棒性差异越加鲜明。<details>
<summary>Abstract</summary>
Downstream applications often require text classification models to be accurate, robust, and interpretable. While the accuracy of the stateof-the-art language models approximates human performance, they are not designed to be interpretable and often exhibit a drop in performance on noisy data. The family of PrototypeBased Networks (PBNs) that classify examples based on their similarity to prototypical examples of a class (prototypes) is natively interpretable and shown to be robust to noise, which enabled its wide usage for computer vision tasks. In this paper, we study whether the robustness properties of PBNs transfer to text classification tasks. We design a modular and comprehensive framework for studying PBNs, which includes different backbone architectures, backbone sizes, and objective functions. Our evaluation protocol assesses the robustness of models against character-, word-, and sentence-level perturbations. Our experiments on three benchmarks show that the robustness of PBNs transfers to NLP classification tasks facing realistic perturbations. Moreover, the robustness of PBNs is supported mostly by the objective function that keeps prototypes interpretable, while the robustness superiority of PBNs over vanilla models becomes more salient as datasets get more complex.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PerceptionGPT-Effectively-Fusing-Visual-Perception-into-LLM"><a href="#PerceptionGPT-Effectively-Fusing-Visual-Perception-into-LLM" class="headerlink" title="PerceptionGPT: Effectively Fusing Visual Perception into LLM"></a>PerceptionGPT: Effectively Fusing Visual Perception into LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06612">http://arxiv.org/abs/2311.06612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, Tong Zhang</li>
<li>for: 这篇论文旨在做什么？	+ 这篇论文目标是具备Visual Large Language Model（VLLM）Visual perception能力，并且能够高效地使用Large Language Model（LLM）的表示能力来实现这一目标。</li>
<li>methods: 这篇论文使用了什么方法？	+ 这篇论文提出了一种新的综合框架，名为PerceptionGPT，它可以快速和高效地使用VLLM来实现视觉感知任务。该方法通过利用LLM的表示能力，将其token嵌入作为视觉信息的载体，然后使用轻量级的视觉任务编码器和解码器来完成视觉感知任务。</li>
<li>results: 这篇论文的研究结果是什么？	+ 对比之前的方法，这篇论文的方法可以更好地处理多个视觉输出，并且可以减少训练时间和数据量，同时减少批处理时间。这种方法可以帮助未来的研究更好地具备VLLM的视觉感知能力。<details>
<summary>Abstract</summary>
The integration of visual inputs with large language models (LLMs) has led to remarkable advancements in multi-modal capabilities, giving rise to visual large language models (VLLMs). However, effectively harnessing VLLMs for intricate visual perception tasks remains a challenge. In this paper, we present a novel end-to-end framework named PerceptionGPT, which efficiently and effectively equips the VLLMs with visual perception abilities by leveraging the representation power of LLMs' token embedding. Our proposed method treats the token embedding of the LLM as the carrier of spatial information, then leverage lightweight visual task encoders and decoders to perform visual perception tasks (e.g., detection, segmentation). Our approach significantly alleviates the training difficulty suffered by previous approaches that formulate the visual outputs as discrete tokens, and enables achieving superior performance with fewer trainable parameters, less training data and shorted training time. Moreover, as only one token embedding is required to decode the visual outputs, the resulting sequence length during inference is significantly reduced. Consequently, our approach enables accurate and flexible representations, seamless integration of visual perception tasks, and efficient handling of a multiple of visual outputs. We validate the effectiveness and efficiency of our approach through extensive experiments. The results demonstrate significant improvements over previous methods with much fewer trainable parameters and GPU hours, which facilitates future research in enabling LLMs with visual perception abilities.
</details>
<details>
<summary>摘要</summary>
摘要：将视觉输入与大语言模型（LLM）结合，已经导致多模态能力的很大进步，产生了视觉大语言模型（VLLM）。然而，使VLLM进行复杂的视觉感知任务仍然是一大挑战。在这篇论文中，我们提出了一种新的端到端框架，名为PerceptionGPT，可以高效地和有效地让VLLM具备视觉感知能力。我们的提议方法是将LLM的 Token embedding作为空间信息的传递者，然后使用轻量级的视觉任务编码器和解码器来完成视觉感知任务（例如检测和分割）。我们的方法可以减少前一些方法的训练困难，只需要 fewer 的可训练参数和训练数据，同时减少训练时间。此外，只需要一个 Token embedding 来解码视觉输出，因此在推理过程中的序列长度减少了。这使得我们的方法可以实现高精度和灵活的表示，同时实现多个视觉输出的有效集成。我们通过广泛的实验 validate 了我们的方法的有效性和效率。结果表明，我们的方法可以与之前的方法相比，减少很多可训练参数和GPU时间，这为未来启用LLM的视觉感知能力提供了可能性。
</details></li>
</ul>
<hr>
<h2 id="BizBench-A-Quantitative-Reasoning-Benchmark-for-Business-and-Finance"><a href="#BizBench-A-Quantitative-Reasoning-Benchmark-for-Business-and-Finance" class="headerlink" title="BizBench: A Quantitative Reasoning Benchmark for Business and Finance"></a>BizBench: A Quantitative Reasoning Benchmark for Business and Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06602">http://arxiv.org/abs/2311.06602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy, Charles Lovering, Chris Tanner</li>
<li>for: 评估商业和金融领域中模型的数理逻辑能力。</li>
<li>methods: 利用程序生成技术来评估模型对金融数据的问答能力，并 isolate 不同的金融逻辑能力。</li>
<li>results: 通过对开源和商业模型进行评估， illustrate 该 benchmark 对数理逻辑能力的评估是一项挑战性的任务。<details>
<summary>Abstract</summary>
As large language models (LLMs) impact a growing number of complex domains, it is becoming increasingly important to have fair, accurate, and rigorous evaluation benchmarks. Evaluating the reasoning skills required for business and financial NLP stands out as a particularly difficult challenge. We introduce BizBench, a new benchmark for evaluating models' ability to reason about realistic financial problems. BizBench comprises 8 quantitative reasoning tasks. Notably, BizBench targets the complex task of question-answering (QA) for structured and unstructured financial data via program synthesis (i.e., code generation). We introduce three diverse financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate distinct financial reasoning capabilities required to solve these QA tasks: reading comprehension of financial text and tables, which is required to extract correct intermediate values; and understanding domain knowledge (e.g., financial formulas) needed to calculate complex solutions. Collectively, these tasks evaluate a model's financial background knowledge, ability to extract numeric entities from financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, illustrating that BizBench is a challenging benchmark for quantitative reasoning in the finance and business domain.
</details>
<details>
<summary>摘要</summary>
As large language models (LLMs) impact an increasing number of complex domains, it is becoming increasingly important to have fair, accurate, and rigorous evaluation benchmarks. Evaluating the reasoning skills required for business and financial NLP is a particularly difficult challenge. We introduce BizBench, a new benchmark for evaluating models' ability to reason about realistic financial problems. BizBench consists of 8 quantitative reasoning tasks. Notably, BizBench targets the complex task of question-answering (QA) for structured and unstructured financial data via program synthesis (i.e., code generation). We introduce three diverse financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate distinct financial reasoning capabilities required to solve these QA tasks, including reading comprehension of financial text and tables, which is necessary to extract correct intermediate values, and understanding domain knowledge (e.g., financial formulas) needed to calculate complex solutions. Collectively, these tasks evaluate a model's financial background knowledge, ability to extract numeric entities from financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, illustrating that BizBench is a challenging benchmark for quantitative reasoning in the finance and business domain.
</details></li>
</ul>
<hr>
<h2 id="From-Classification-to-Generation-Insights-into-Crosslingual-Retrieval-Augmented-ICL"><a href="#From-Classification-to-Generation-Insights-into-Crosslingual-Retrieval-Augmented-ICL" class="headerlink" title="From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL"></a>From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06595">http://arxiv.org/abs/2311.06595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoqian Li, Ercong Nie, Sheng Liang</li>
<li>for: 提高低资源语言中大型自然语言模型（LLM）的域内学习（ICL）性能。</li>
<li>methods: 提出了一种新的方法，即跨语言检索增强域内学习（CREA-ICL），通过提取高资源语言中相似的提示，提高多语言预训练语言模型（MPLM）在多种任务上的零基eline性能。</li>
<li>results: 在分类任务中，该方法得到了稳定的提升，但在生成任务中遇到了挑战。我们的评估带来了域内学习在分类和生成领域的性能动态。<details>
<summary>Abstract</summary>
The remarkable ability of Large Language Models (LLMs) to understand and follow instructions has sometimes been limited by their in-context learning (ICL) performance in low-resource languages. To address this, we introduce a novel approach that leverages cross-lingual retrieval-augmented in-context learning (CREA-ICL). By extracting semantically similar prompts from high-resource languages, we aim to improve the zero-shot performance of multilingual pre-trained language models (MPLMs) across diverse tasks. Though our approach yields steady improvements in classification tasks, it faces challenges in generation tasks. Our evaluation offers insights into the performance dynamics of retrieval-augmented in-context learning across both classification and generation domains.
</details>
<details>
<summary>摘要</summary>
LLMs的出色能力理解和遵从指令有时会受到低资源语言的ICL性能的限制。为解决这个问题，我们提出了一种新的方法，即跨语言检索增强ICL（CREA-ICL）。通过从高资源语言提取相似的提示，我们希望提高多语言预训练语言模型（MPLM）的零配置性能。虽然我们的方法在分类任务中得到了稳定的改善，但在生成任务中遇到了挑战。我们的评估对于检索增强ICL在分类和生成领域的性能动态进行了评估。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Cross-Lingual-Sentiment-Classification-under-Distribution-Shift-an-Exploratory-Study"><a href="#Zero-Shot-Cross-Lingual-Sentiment-Classification-under-Distribution-Shift-an-Exploratory-Study" class="headerlink" title="Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study"></a>Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06549">http://arxiv.org/abs/2311.06549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maarten De Raedt, Semere Kiros Bitew, Fréderic Godin, Thomas Demeester, Chris Develder</li>
<li>for: This paper is focused on studying the generalization of multi-lingual language models to out-of-distribution (OOD) test data in zero-shot cross-lingual transfer settings, and analyzing the impact of both language and domain shifts on performance.</li>
<li>methods: The paper uses counterfactually augmented data (CAD) to improve OOD generalization in the cross-lingual setting, and proposes two new approaches that avoid the costly annotation process associated with CAD.</li>
<li>results: The paper evaluates the performance of three multilingual models (LaBSE, mBERT, and XLM-R) on OOD test sets in 13 languages, and finds that the proposed cost-effective approaches reach similar or up to +3.1% better accuracy than CAD for Amazon and Restaurant reviews.<details>
<summary>Abstract</summary>
The brittleness of finetuned language model performance on out-of-distribution (OOD) test samples in unseen domains has been well-studied for English, yet is unexplored for multi-lingual models. Therefore, we study generalization to OOD test data specifically in zero-shot cross-lingual transfer settings, analyzing performance impacts of both language and domain shifts between train and test data. We further assess the effectiveness of counterfactually augmented data (CAD) in improving OOD generalization for the cross-lingual setting, since CAD has been shown to benefit in a monolingual English setting. Finally, we propose two new approaches for OOD generalization that avoid the costly annotation process associated with CAD, by exploiting the power of recent large language models (LLMs). We experiment with 3 multilingual models, LaBSE, mBERT, and XLM-R trained on English IMDb movie reviews, and evaluate on OOD test sets in 13 languages: Amazon product reviews, Tweets, and Restaurant reviews. Results echo the OOD performance decline observed in the monolingual English setting. Further, (i) counterfactuals from the original high-resource language do improve OOD generalization in the low-resource language, and (ii) our newly proposed cost-effective approaches reach similar or up to +3.1% better accuracy than CAD for Amazon and Restaurant reviews.
</details>
<details>
<summary>摘要</summary>
英文语言模型在不同领域的 OUT-OF-DISTRIBUTION（OOD）测试样本上的 brittleness已经得到了广泛的研究，然而对多语言模型的研究尚未得到了探讨。因此，我们研究了在零shot跨语言传输 Setting中的OOD总结能力，分析了语言和领域之间的数据偏移对测试数据的影响。此外，我们还评估了基于counterfactual augmented data（CAD）的方法在跨语言设置中的有效性，因为CAD在英文设置中已经被证明有助于提高OOD总结能力。最后，我们提出了两种新的OOD总结方法，以避免与CAD相关的昂贵的注释过程，通过利用最新的大语言模型（LLMs）。我们在英语 IMDb 电影评论上训练了3个多语言模型：LaBSE、mBERT和XLM-R，并对13种语言的OOD测试集进行评估：Amazon产品评论、推特和餐厅评论。结果表明，OOD性能减降与英文设置中观察到的类似。此外，（i）原始高资源语言中的counterfactuals实际上提高了低资源语言中的OOD总结能力，和（ii）我们新提出的经济性方法达到了类似或更高于CAD的准确率，为Amazon和餐厅评论达到了+3.1%的提升。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Public-Understanding-of-Court-Opinions-with-Automated-Summarizers"><a href="#Enhancing-Public-Understanding-of-Court-Opinions-with-Automated-Summarizers" class="headerlink" title="Enhancing Public Understanding of Court Opinions with Automated Summarizers"></a>Enhancing Public Understanding of Court Opinions with Automated Summarizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06534">http://arxiv.org/abs/2311.06534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elliott Ash, Aniket Kesari, Suresh Naidu, Lena Song, Dominik Stammbach</li>
<li>for: 帮助非专家理解法律案例</li>
<li>methods: 使用人工智能助手生成简化摘要</li>
<li>results: 调查实验表明，简化摘要可以帮助非专家更好地理解法律案例的关键特征。In English, this translates to:</li>
<li>for: To help non-experts understand legal cases</li>
<li>methods: Using an AI assistant to generate simplified summaries</li>
<li>results: A survey experiment shows that simplified summaries can help non-experts understand the key features of a ruling.<details>
<summary>Abstract</summary>
Written judicial opinions are an important tool for building public trust in court decisions, yet they can be difficult for non-experts to understand. We present a pipeline for using an AI assistant to generate simplified summaries of judicial opinions. These are more accessible to the public and more easily understood by non-experts, We show in a survey experiment that the simplified summaries help respondents understand the key features of a ruling. We discuss how to integrate legal domain knowledge into studies using large language models. Our results suggest a role both for AI assistants to inform the public, and for lawyers to guide the process of generating accessible summaries.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:written judicial opinions are an important tool for building public trust in court decisions, yet they can be difficult for non-experts to understand. we present a pipeline for using an AI assistant to generate simplified summaries of judicial opinions. these are more accessible to the public and more easily understood by non-experts. we show in a survey experiment that the simplified summaries help respondents understand the key features of a ruling. we discuss how to integrate legal domain knowledge into studies using large language models. our results suggest a role both for AI assistants to inform the public, and for lawyers to guide the process of generating accessible summaries.
</details></li>
</ul>
<hr>
<h2 id="Added-Toxicity-Mitigation-at-Inference-Time-for-Multimodal-and-Massively-Multilingual-Translation"><a href="#Added-Toxicity-Mitigation-at-Inference-Time-for-Multimodal-and-Massively-Multilingual-Translation" class="headerlink" title="Added Toxicity Mitigation at Inference Time for Multimodal and Massively Multilingual Translation"></a>Added Toxicity Mitigation at Inference Time for Multimodal and Massively Multilingual Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06532">http://arxiv.org/abs/2311.06532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta R. Costa-jussà, David Dale, Maha Elbayad, Bokai Yu</li>
<li>for: 这 paper 的目的是提出一种新的pipeline来识别添加的毒性并mitigate这个问题，该pipeline在推理时间实现。</li>
<li>methods: 这 paper 使用了一种多modal的毒性检测分类器（speech和text），该分类器可以在大规模语言中工作。mitigation方法直接应用于文本输出中。</li>
<li>results: 这 paper 使用 MinTox pipeline在 SEAMLESSM4T 系统上实现了显著的添加毒性 Mitigation， across domains, modalities和语言方向。 MinTox 能够约Filter出25%-95%的添加毒性（根据模式和领域），保持翻译质量。<details>
<summary>Abstract</summary>
Added toxicity in the context of translation refers to the fact of producing a translation output with more toxicity than there exists in the input. In this paper, we present MinTox which is a novel pipeline to identify added toxicity and mitigate this issue which works at inference time. MinTox uses a toxicity detection classifier which is multimodal (speech and text) and works in languages at scale. The mitigation method is applied to languages at scale and directly in text outputs. MinTox is applied to SEAMLESSM4T, which is the latest multimodal and massively multilingual machine translation system. For this system, MinTox achieves significant added toxicity mitigation across domains, modalities and language directions. MinTox manages to approximately filter out from 25% to 95% of added toxicity (depending on the modality and domain) while keeping translation quality.
</details>
<details>
<summary>摘要</summary>
加入毒性在翻译上指的是生成翻译输出中存在更多的毒性 чем输入。在这篇论文中，我们介绍了一种名为MinTox的新的管道，用于识别加入毒性并缓解这个问题，它在推理时间进行应用。MinTox使用一个多Modal（语音和文本）的毒性检测类ifier，可以在多种语言和模式下进行检测。这种缓解方法直接应用于文本输出中。MinTox在SEAMLESSM4T上进行应用，SEAMLESSM4T是最新的多Modal和大量多语言翻译系统。对这个系统来说，MinTox在域、modal和语言方向上都实现了显著的加入毒性缓解，可以将25%-95%的加入毒性（根据模式和领域）约束出去，而不会影响翻译质量。
</details></li>
</ul>
<hr>
<h2 id="Minimum-Description-Length-Hopfield-Networks"><a href="#Minimum-Description-Length-Hopfield-Networks" class="headerlink" title="Minimum Description Length Hopfield Networks"></a>Minimum Description Length Hopfield Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06518">http://arxiv.org/abs/2311.06518</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matanabudy/mdl-hn">https://github.com/matanabudy/mdl-hn</a></li>
<li>paper_authors: Matan Abudy, Nur Lan, Emmanuel Chemla, Roni Katzir</li>
<li>for: 这个论文是为了研究协同记忆架构的Memorization和Generalization之间的质量。</li>
<li>methods: 这个论文使用Modern Hopfield Networks（MHN）来研究协同记忆架构的Memorization和Generalization。</li>
<li>results: 研究发现，大量的Memorization容量会妨碍Generalization的机会。提出一种使用Minimum Description Length（MDL）来在训练过程中决定保留哪些记忆和哪些记忆数量。<details>
<summary>Abstract</summary>
Associative memory architectures are designed for memorization but also offer, through their retrieval method, a form of generalization to unseen inputs: stored memories can be seen as prototypes from this point of view. Focusing on Modern Hopfield Networks (MHN), we show that a large memorization capacity undermines the generalization opportunity. We offer a solution to better optimize this tradeoff. It relies on Minimum Description Length (MDL) to determine during training which memories to store, as well as how many of them.
</details>
<details>
<summary>摘要</summary>
协同记忆架构是设计来储存信息，但同时也提供了一种通过回溯方法对未见输入进行泛化的机会：储存的记忆可以被看作是类型的范例。专注于现代赫珀维尔网络（MHN），我们表明了大量储存容量会对泛化机会造成干扰。我们提出了一个解决方案，它基于最小描述长度（MDL）来决定在训练过程中哪些记忆要储存，以及哪些记忆要保留多少。
</details></li>
</ul>
<hr>
<h2 id="L3-Ensembles-Lifelong-Learning-Approach-for-Ensemble-of-Foundational-Language-Models"><a href="#L3-Ensembles-Lifelong-Learning-Approach-for-Ensemble-of-Foundational-Language-Models" class="headerlink" title="L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language Models"></a>L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06493">http://arxiv.org/abs/2311.06493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aidin Shiri, Kaushik Roy, Amit Sheth, Manas Gaur</li>
<li>for: 这个论文旨在提出一种基于自然语言处理（NLP）任务的生命长学习（L3）框架，以便高效地进行任务特化和知识传递。</li>
<li>methods: 该方法包括提取有意义的表示，建立结构化知识库，以及在不同任务上进行逐步改进。</li>
<li>results: 经验表明，提出的L3 ensemble方法可以提高模型精度，同时保持或超过当前语言模型（T5）的性能。在STSbenchmark中，L3模型的准确率比原始 Fine-tuned FLM 提高15.4%。<details>
<summary>Abstract</summary>
Fine-tuning pre-trained foundational language models (FLM) for specific tasks is often impractical, especially for resource-constrained devices. This necessitates the development of a Lifelong Learning (L3) framework that continuously adapts to a stream of Natural Language Processing (NLP) tasks efficiently. We propose an approach that focuses on extracting meaningful representations from unseen data, constructing a structured knowledge base, and improving task performance incrementally. We conducted experiments on various NLP tasks to validate its effectiveness, including benchmarks like GLUE and SuperGLUE. We measured good performance across the accuracy, training efficiency, and knowledge transfer metrics. Initial experimental results show that the proposed L3 ensemble method increases the model accuracy by 4% ~ 36% compared to the fine-tuned FLM. Furthermore, L3 model outperforms naive fine-tuning approaches while maintaining competitive or superior performance (up to 15.4% increase in accuracy) compared to the state-of-the-art language model (T5) for the given task, STS benchmark.
</details>
<details>
<summary>摘要</summary>
精度调整预训练基础语言模型（FLM） для特定任务是经常不可能，特别是在有限的设备资源下。这种情况需要开发一个生命时间学习（L3）框架，可以高效地适应流行的自然语言处理（NLP）任务。我们提出了一种方法，强调提取未经见过的数据中有意义的表示，建立结构化的知识库，并在不断更新的任务中提高表现。我们在多个 NLP 任务上进行了实验，以验证其效果，包括 GLUE 和 SuperGLUE 的benchmark。我们发现，在精度、训练效率和知识传递指标方面，L3 ensemble方法表现良好。初步实验结果表明，我们提议的 L3 模型比 fine-tuned FLM 提高4%~36%的模型精度。此外，L3 模型还能在与状态艺术语言模型（T5）相同或更高的精度水平上保持竞争性或超越性（最多提高15.4%的精度），对 STS benchmark进行了证明。
</details></li>
</ul>
<hr>
<h2 id="DocGen-Generating-Detailed-Parameter-Docstrings-in-Python"><a href="#DocGen-Generating-Detailed-Parameter-Docstrings-in-Python" class="headerlink" title="DocGen: Generating Detailed Parameter Docstrings in Python"></a>DocGen: Generating Detailed Parameter Docstrings in Python</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06453">http://arxiv.org/abs/2311.06453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vatsal Venkatkrishna, Durga Shree Nagabushanam, Emmanuel Iko-Ojo Simon, Melina Vidoni</li>
<li>for: 提高开源软件的有效利用，因为文档债让开发者困惑。</li>
<li>methods: 提出了一种多步骤方法，通过结合多个任务特定的模型，每个模型都专门生成不同的段落，以确保生成的文档准确全面。</li>
<li>results: 与现有的生成模型进行比较，通过自动指标和人 centered评估17名开发者，证明了该方法与现有方法之间的超越。<details>
<summary>Abstract</summary>
Documentation debt hinders the effective utilization of open-source software. Although code summarization tools have been helpful for developers, most would prefer a detailed account of each parameter in a function rather than a high-level summary. However, generating such a summary is too intricate for a single generative model to produce reliably due to the lack of high-quality training data. Thus, we propose a multi-step approach that combines multiple task-specific models, each adept at producing a specific section of a docstring. The combination of these models ensures the inclusion of each section in the final docstring. We compared the results from our approach with existing generative models using both automatic metrics and a human-centred evaluation with 17 participating developers, which proves the superiority of our approach over existing methods.
</details>
<details>
<summary>摘要</summary>
文档债务阻碍开源软件的有效利用。虽然代码概要工具有帮助开发者，但大多数开发者更偏好每个函数参数的详细账户而不是高级概要。然而，生成这样的概要是单一生成模型无法可靠地生成的由于缺乏高质量的训练数据。因此，我们提议一种多步骤方法，将多个任务特定的模型相互结合，以确保每个部分在最终的概要中包含。我们与已有的生成模型进行比较，并通过17名参与者进行人中心评估，证明我们的方法在现有方法之上。
</details></li>
</ul>
<hr>
<h2 id="Separating-the-Wheat-from-the-Chaff-with-BREAD-An-open-source-benchmark-and-metrics-to-detect-redundancy-in-text"><a href="#Separating-the-Wheat-from-the-Chaff-with-BREAD-An-open-source-benchmark-and-metrics-to-detect-redundancy-in-text" class="headerlink" title="Separating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text"></a>Separating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06440">http://arxiv.org/abs/2311.06440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toizzy/bread">https://github.com/toizzy/bread</a></li>
<li>paper_authors: Isaac Caswell, Lisa Wang, Isabel Papadimitriou</li>
<li>for: 这篇论文的目的是提供一个人类标注的数据集，用于测试语言模型训练数据中的重复文本问题，并评估不同语言中的数据质量。</li>
<li>methods: 该论文使用了人类标注的数据集，创建了一个名为BREAD的数据集，并提供了一些基线分析方法（CRED）来评估数据质量。</li>
<li>results: 该论文通过对BREAD数据集进行分析，发现了一些语言模型训练数据中的重复文本问题，并提供了一些参考实现方法来解决这些问题。<details>
<summary>Abstract</summary>
Data quality is a problem that perpetually resurfaces throughout the field of NLP, regardless of task, domain, or architecture, and remains especially severe for lower-resource languages. A typical and insidious issue, affecting both training data and model output, is data that is repetitive and dominated by linguistically uninteresting boilerplate, such as price catalogs or computer-generated log files. Though this problem permeates many web-scraped corpora, there has yet to be a benchmark to test against, or a systematic study to find simple metrics that generalize across languages and agree with human judgements of data quality. In the present work, we create and release BREAD, a human-labeled benchmark on repetitive boilerplate vs. plausible linguistic content, spanning 360 languages. We release several baseline CRED (Character REDundancy) scores along with it, and evaluate their effectiveness on BREAD. We hope that the community will use this resource to develop better filtering methods, and that our reference implementations of CRED scores can become standard corpus evaluation tools, driving the development of cleaner language modeling corpora, especially in low-resource languages.
</details>
<details>
<summary>摘要</summary>
“资料质量是NLP领域中不断重现的问题，不论任务、领域或架构，它尤其严重 для低资源语言。一个常见的问题是训练数据和模型输出中的重复和 linguistically 无趣的� boilerplate，如价格目录或计算机生成的日志档案。这个问题在许多网页抓取数据中广泛存在，但是还没有一个底线来测试，或一个系统性的研究来找到简单的度量标准，以及与人类判断资料质量的一致性。在现在的工作中，我们创建了BREAD，一个人工标注的底线，涵盖360种语言。我们释出了多个基线CRED（Character REDundancy）分数，并评估它们在BREAD上的效果。我们希望社区可以使用这个资源，发展更好的筛选方法，以提高语言模型数据库的质量，特别是低资源语言。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/11/cs.CL_2023_11_11/" data-id="clpahu71p00ev3h88bo7qanic" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/11/cs.LG_2023_11_11/" class="article-date">
  <time datetime="2023-11-11T10:00:00.000Z" itemprop="datePublished">2023-11-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/11/cs.LG_2023_11_11/">cs.LG - 2023-11-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Agnostic-Membership-Query-Learning-with-Nontrivial-Savings-New-Results-Techniques"><a href="#Agnostic-Membership-Query-Learning-with-Nontrivial-Savings-New-Results-Techniques" class="headerlink" title="Agnostic Membership Query Learning with Nontrivial Savings: New Results, Techniques"></a>Agnostic Membership Query Learning with Nontrivial Savings: New Results, Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06690">http://arxiv.org/abs/2311.06690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ari Karchmer</li>
<li>for: 这paper主要研究了在agnostic learning模型中设计高效的算法（Haussler, 1992; Kearns et al., 1994）。</li>
<li>methods: 本paper使用了membership queries方法，特别是针对touchstone classes的frontier agnostic learning问题。</li>
<li>results: 本paper提出了多种agnostic learning算法，其中包括一个可以处理具有折衣数量的gate的circuit，并且可以在2^n时间内运行，而不是默认的2^n时间。此外，paper还提出了一个可以处理任意函数计算的\sym^+ circuit的算法，并且可以在2^n时间内运行。<details>
<summary>Abstract</summary>
(Abridged) Designing computationally efficient algorithms in the agnostic learning model (Haussler, 1992; Kearns et al., 1994) is notoriously difficult. In this work, we consider agnostic learning with membership queries for touchstone classes at the frontier of agnostic learning, with a focus on how much computation can be saved over the trivial runtime of 2^n$. This approach is inspired by and continues the study of ``learning with nontrivial savings'' (Servedio and Tan, 2017). To this end, we establish multiple agnostic learning algorithms, highlighted by:   1. An agnostic learning algorithm for circuits consisting of a sublinear number of gates, which can each be any function computable by a sublogarithmic degree k polynomial threshold function (the depth of the circuit is bounded only by size). This algorithm runs in time 2^{n -s(n)} for s(n) \approx n/(k+1), and learns over the uniform distribution over unlabelled examples on \{0,1\}^n.   2. An agnostic learning algorithm for circuits consisting of a sublinear number of gates, where each can be any function computable by a \sym^+ circuit of subexponential size and sublogarithmic degree k. This algorithm runs in time 2^{n-s(n)} for s(n) \approx n/(k+1), and learns over distributions of unlabelled examples that are products of k+1 arbitrary and unknown distributions, each over \{0,1\}^{n/(k+1)} (assume without loss of generality that k+1 divides n).
</details>
<details>
<summary>摘要</summary>
（简化）在agnostic学习模型（Haussler，1992；Kearns等，1994）中设计 computationally efficient algorithm 是非常困难的。在这个工作中，我们考虑agnostic learning with membership queries for touchstone classes at the frontier of agnostic learning，并关注如何在基于2^n的极端情况下节省计算时间。这种方法是servedio和tan（2017）的研究继续。为了实现这一目标，我们提出了多种agnostic learning算法，其中包括：1. 一种agnostic learning算法 для满足一个子线性数量的门的Circuit，每个门可以是一个可以由k度多项式阈值函数计算的任意函数（Circuit的深度只决定了大小）。这个算法在时间2^n-s(n)中运行，其中s(n)约等于n/(k+1)，并在 uniform distribution over unlabelled examples on \{0,1\}^n上学习。2. 一种agnostic learning算法 для满足一个子线性数量的门的Circuit，其中每个门可以是一个可以由subexponential size和k度多项式阈值函数计算的任意函数（Circuit的深度只决定了大小）。这个算法在时间2^n-s(n)中运行，其中s(n)约等于n/(k+1)，并在分布 over unlabelled examples是k+1个未知和无标签的分布的产物上学习，即assume without loss of generality that k+1 divides n。
</details></li>
</ul>
<hr>
<h2 id="Heuristic-Optimal-Transport-in-Branching-Networks"><a href="#Heuristic-Optimal-Transport-in-Branching-Networks" class="headerlink" title="Heuristic Optimal Transport in Branching Networks"></a>Heuristic Optimal Transport in Branching Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06650">http://arxiv.org/abs/2311.06650</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Andrecut</li>
<li>for: 学习一种可以在网络上最优化运输的方法，以最小化成本。</li>
<li>methods: 使用快速的规则来生成分支结构，以便在网络上实现最优化运输。</li>
<li>results: 提供了一些应用场景，例如在社交网络上的人员调配和物流网络中的货物分配。<details>
<summary>Abstract</summary>
Optimal transport aims to learn a mapping of sources to targets by minimizing the cost, which is typically defined as a function of distance. The solution to this problem consists of straight line segments optimally connecting sources to targets, and it does not exhibit branching. These optimal solutions are in stark contrast with both natural, and man-made transportation networks, where branching structures are prevalent. Here we discuss a fast heuristic branching method for optimal transport in networks, and we provide several applications.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:优化交通目标是学习源到目标的映射，通常通过距离定义成本来实现。解决这个问题的解是直线段最优连接源到目标，无分支结构。这些优化解与自然和人工交通网络不同，后者通常具有分支结构。我们介绍了一种快速冒泡分支方法优化交通网络，并提供了多个应用。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Risks-Analysis-and-Mitigation-in-Federated-Learning-for-Medical-Images"><a href="#Privacy-Risks-Analysis-and-Mitigation-in-Federated-Learning-for-Medical-Images" class="headerlink" title="Privacy Risks Analysis and Mitigation in Federated Learning for Medical Images"></a>Privacy Risks Analysis and Mitigation in Federated Learning for Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06643">http://arxiv.org/abs/2311.06643</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlsysx/medpfl">https://github.com/mlsysx/medpfl</a></li>
<li>paper_authors: Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu</li>
<li>for: 本研究旨在分析和 Mitigate Medical data Privacy risk in Federated Learning (FL) 中的隐私风险。</li>
<li>methods: 本研究提出了一个整体的框架（MedPFL）来分析和 Mitigate FL 中隐私风险，并在实验中表明了对医疗数据的隐私攻击的极大威胁。</li>
<li>results: 研究发现，通过加入随机噪声来保护医疗数据的防御策略可能不一定有效，存在独特和紧迫的医疗数据隐私挑战。<details>
<summary>Abstract</summary>
Federated learning (FL) is gaining increasing popularity in the medical domain for analyzing medical images, which is considered an effective technique to safeguard sensitive patient data and comply with privacy regulations. However, several recent studies have revealed that the default settings of FL may leak private training data under privacy attacks. Thus, it is still unclear whether and to what extent such privacy risks of FL exist in the medical domain, and if so, ``how to mitigate such risks?''. In this paper, first, we propose a holistic framework for Medical data Privacy risk analysis and mitigation in Federated Learning (MedPFL) to analyze privacy risks and develop effective mitigation strategies in FL for protecting private medical data. Second, we demonstrate the substantial privacy risks of using FL to process medical images, where adversaries can easily perform privacy attacks to reconstruct private medical images accurately. Third, we show that the defense approach of adding random noises may not always work effectively to protect medical images against privacy attacks in FL, which poses unique and pressing challenges associated with medical data for privacy protection.
</details>
<details>
<summary>摘要</summary>
受到批评的学习（Federated Learning，FL）在医疗领域的应用正在增加，用于分析医疗图像，这被视为一种有效的技术来保护敏感的病人数据和遵守隐私法规。然而，一些最近的研究表明，FL的默认设置可能会泄露敏感训练数据面临隐私攻击。因此，在医疗领域中是否存在这种隐私风险，以及如何 Mitigate 这些风险仍然是一个未知。在这篇论文中，我们提出了一个整体的框架，以便在 Federated Learning 中进行医疗数据隐私风险分析和降低（MedPFL），以分析隐私风险并开发有效的降低策略，以保护敏感的医疗数据。其次，我们示出了使用 FL 处理医疗图像时存在严重的隐私风险，敌方可以轻松地进行隐私攻击，以重建私人医疗图像。最后，我们表明了在 FL 中添加随机噪声可能无法有效地保护医疗图像 Against 隐私攻击，这增加了医疗数据隐私保护的特殊挑战。
</details></li>
</ul>
<hr>
<h2 id="The-Exact-Determinant-of-a-Specific-Class-of-Sparse-Positive-Definite-Matrices"><a href="#The-Exact-Determinant-of-a-Specific-Class-of-Sparse-Positive-Definite-Matrices" class="headerlink" title="The Exact Determinant of a Specific Class of Sparse Positive Definite Matrices"></a>The Exact Determinant of a Specific Class of Sparse Positive Definite Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06632">http://arxiv.org/abs/2311.06632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Molkaraie</li>
<li>for: 这篇论文是为了解决一种特定的稀畴 Gaussian graphical model 的 determinant 问题而写的。</li>
<li>methods: 这篇论文使用了 Normal Factor Graph Duality Theorem 和 holographic algorithms 来提供一个关闭式解决方案，即通过 Matrix Determinant Lemma 对 transformed graphical model 进行处理。</li>
<li>results: 这篇论文提供了一个关闭式表达式，用于计算稀畴 Gaussian graphical model 的 determinant。此外， paper 还定义了一种等价关系 между两个 Gaussian graphical model。<details>
<summary>Abstract</summary>
For a specific class of sparse Gaussian graphical models, we provide a closed-form solution for the determinant of the covariance matrix. In our framework, the graphical interaction model (i.e., the covariance selection model) is equal to replacement product of $\mathcal{K}_{n}$ and $\mathcal{K}_{n-1}$, where $\mathcal{K}_n$ is the complete graph with $n$ vertices. Our analysis is based on taking the Fourier transform of the local factors of the model, which can be viewed as an application of the Normal Factor Graph Duality Theorem and holographic algorithms. The closed-form expression is obtained by applying the Matrix Determinant Lemma on the transformed graphical model. In this context, we will also define a notion of equivalence between two Gaussian graphical models.
</details>
<details>
<summary>摘要</summary>
For a specific class of sparse Gaussian graphical models, we provide a closed-form solution for the determinant of the covariance matrix. In our framework, the graphical interaction model (i.e., the covariance selection model) is equal to the replacement product of $\mathcal{K}_{n}$ and $\mathcal{K}_{n-1}$, where $\mathcal{K}_n$ is the complete graph with $n$ vertices. Our analysis is based on taking the Fourier transform of the local factors of the model, which can be viewed as an application of the Normal Factor Graph Duality Theorem and holographic algorithms. The closed-form expression is obtained by applying the Matrix Determinant Lemma on the transformed graphical model. In this context, we will also define a notion of equivalence between two Gaussian graphical models.Here's the translation:为特定类型的稀疏 Gaussian 图模型，我们提供一个关闭式解的 determinant 表达。在我们的框架中，图模型的交互模型（即covariance 选择模型）等于 $\mathcal{K}_{n}$ 和 $\mathcal{K}_{n-1}$ 的交换乘积，其中 $\mathcal{K}_n$ 是一个完全图 WITH $n$ 个顶点。我们的分析基于图模型的本地ifactors 的傅ри幂变换，这可以看作是 Normal Factor Graph Duality Theorem 和 holographic algorithms 的应用。关闭式表达是通过应用 Matrix Determinant Lemma onto the transformed graphical model 获得的。在这个上下文中，我们还将定义 Gaussian 图模型之间的一种相等性。
</details></li>
</ul>
<hr>
<h2 id="Streamlining-Energy-Transition-Scenarios-to-Key-Policy-Decisions"><a href="#Streamlining-Energy-Transition-Scenarios-to-Key-Policy-Decisions" class="headerlink" title="Streamlining Energy Transition Scenarios to Key Policy Decisions"></a>Streamlining Energy Transition Scenarios to Key Policy Decisions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06625">http://arxiv.org/abs/2311.06625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Joseph Baader, Stefano Moret, Wolfram Wiesemann, Iain Staffell, André Bardow</li>
<li>For: The paper is written to provide an approach for interpreting and prioritizing key factors in the energy transition, specifically in the context of global decarbonization scenarios and a fossil-free Europe.* Methods: The paper uses decision trees, a popular machine-learning technique, to derive interpretable storylines from many quantitative scenarios and show how the key decisions in the energy transition are interlinked.* Results: The paper demonstrates that choosing a high deployment of renewables and sector coupling makes global decarbonization scenarios robust against uncertainties in climate sensitivity and demand, and that the energy transition to a fossil-free Europe is primarily determined by choices on the roles of bioenergy, storage, and heat electrification.Here is the information in Simplified Chinese text:* For: 这篇论文是为了提供一种方法来解释和优先级化能源转型的关键因素，具体是在全球减排场景和不burn欧洲的背景下。* Methods: 这篇论文使用决策树，一种流行的机器学习技术，来 derivates interpretable storylines从多个量化enario中，并显示了能源转型中关键决策之间的关联。* Results: 这篇论文发现，选择高部署的可再生能源和部署相互连接会使全球减排场景对气候敏感度和需求的不确定性 exhibit robustness，而不burn欧洲的能源转型主要取决于生物能源、存储和热电气化的角色。<details>
<summary>Abstract</summary>
Uncertainties surrounding the energy transition often lead modelers to present large sets of scenarios that are challenging for policymakers to interpret and act upon. An alternative approach is to define a few qualitative storylines from stakeholder discussions, which can be affected by biases and infeasibilities. Leveraging decision trees, a popular machine-learning technique, we derive interpretable storylines from many quantitative scenarios and show how the key decisions in the energy transition are interlinked. Specifically, our results demonstrate that choosing a high deployment of renewables and sector coupling makes global decarbonization scenarios robust against uncertainties in climate sensitivity and demand. Also, the energy transition to a fossil-free Europe is primarily determined by choices on the roles of bioenergy, storage, and heat electrification. Our transferrable approach translates vast energy model results into a small set of critical decisions, guiding decision-makers in prioritizing the key factors that will shape the energy transition.
</details>
<details>
<summary>摘要</summary>
uncertainties surrounding the energy transition often lead modelers to present large sets of scenarios that are challenging for policymakers to interpret and act upon. an alternative approach is to define a few qualitative storylines from stakeholder discussions, which can be affected by biases and infeasibilities. leveraging decision trees, a popular machine-learning technique, we derive interpretable storylines from many quantitative scenarios and show how the key decisions in the energy transition are interlinked. specifically, our results demonstrate that choosing a high deployment of renewables and sector coupling makes global decarbonization scenarios robust against uncertainties in climate sensitivity and demand. also, the energy transition to a fossil-free Europe is primarily determined by choices on the roles of bioenergy, storage, and heat electrification. our transferrable approach translates vast energy model results into a small set of critical decisions, guiding decision-makers in prioritizing the key factors that will shape the energy transition.Here's the text with some additional information about the Simplified Chinese translation:The Simplified Chinese translation is written in 简化字符 (Simplified Chinese characters) rather than 正体字符 (Traditional Chinese characters). This is because Simplified Chinese is more widely used in mainland China and other countries, while Traditional Chinese is more commonly used in Hong Kong, Macau, and Taiwan.In the translation, some technical terms and concepts have been translated into Simplified Chinese, such as "能源转型" (energy transition), "可再生能源" (renewable energy), and "燃料电池" (fuel cell). However, some terms and concepts have been retained in English, such as "scenarios" and "storylines," as there may not be direct equivalents in Simplified Chinese.Additionally, some sentence structures and wording have been adjusted to conform to the grammatical conventions of Simplified Chinese. For example, in the sentence "Leveraging decision trees, a popular machine-learning technique, we derive interpretable storylines from many quantitative scenarios and show how the key decisions in the energy transition are interlinked," the word order has been adjusted to place the verb "derive" before the object "interpretable storylines" to conform to Simplified Chinese sentence structure.
</details></li>
</ul>
<hr>
<h2 id="Sparse-Attention-Based-Neural-Networks-for-Code-Classification"><a href="#Sparse-Attention-Based-Neural-Networks-for-Code-Classification" class="headerlink" title="Sparse Attention-Based Neural Networks for Code Classification"></a>Sparse Attention-Based Neural Networks for Code Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06575">http://arxiv.org/abs/2311.06575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyang Xiang, Zaixi Zhang, Qi Liu<br>for: 这个论文是为了解决实际programming教育平台中的代码分类问题而写的。methods: 这个论文使用了模型基于抽象语法树（ASTs）的方法，包括 syntax parsing和递归神经网络编码，以及一种特制的稀疏注意机制。results: 对于代码分类任务，这个方法能够提供高效精准的分类结果，并且可以解决之前相关研究中的问题，如不完整的分类标签和小型数据集。<details>
<summary>Abstract</summary>
Categorizing source codes accurately and efficiently is a challenging problem in real-world programming education platform management. In recent years, model-based approaches utilizing abstract syntax trees (ASTs) have been widely applied to code classification tasks. We introduce an approach named the Sparse Attention-based neural network for Code Classification (SACC) in this paper. The approach involves two main steps: In the first step, source code undergoes syntax parsing and preprocessing. The generated abstract syntax tree is split into sequences of subtrees and then encoded using a recursive neural network to obtain a high-dimensional representation. This step simultaneously considers both the logical structure and lexical level information contained within the code. In the second step, the encoded sequences of subtrees are fed into a Transformer model that incorporates sparse attention mechanisms for the purpose of classification. This method efficiently reduces the computational cost of the self-attention mechanisms, thus improving the training speed while preserving effectiveness. Our work introduces a carefully designed sparse attention pattern that is specifically designed to meet the unique needs of code classification tasks. This design helps reduce the influence of redundant information and enhances the overall performance of the model. Finally, we also deal with problems in previous related research, which include issues like incomplete classification labels and a small dataset size. We annotated the CodeNet dataset with algorithm-related labeling categories, which contains a significantly large amount of data. Extensive comparative experimental results demonstrate the effectiveness and efficiency of SACC for the code classification tasks.
</details>
<details>
<summary>摘要</summary>
优化代码分类任务的准确性和效率是现实世界程序教育平台管理中的挑战。在过去几年，基于抽象树（AST）的模型方法在代码分类任务中得到了广泛的应用。我们在这篇论文中介绍了一种名为代码分类 neural network with sparse attention（SACC）的方法。该方法包括两个主要步骤：第一步：源代码进行语法分析和处理，并将生成的抽象树分解成多个子树序列，然后使用回归神经网络编码以获得高维度表示。这一步同时考虑了代码的逻辑结构和字面层次信息。第二步：编码后的子树序列被传输到一个包含稀缺注意机制的Transformer模型中，用于分类。这种方法可以有效减少自注意机制的计算成本，从而提高训练速度，同时保持效果。我们还设计了一种特殊的稀缺注意模式，用于满足代码分类任务的唯一需求。这种设计可以减少重复信息的影响，提高模型的总性能。最后，我们还解决了过去相关研究中的一些问题，如 incomplete classification labels和小型数据集。我们对CodeNet数据集进行了算法相关标签注释，该数据集包含很大量数据。我们进行了广泛的比较 эксперименталь研究，证明了 SACC 在代码分类任务中的有效性和效率。
</details></li>
</ul>
<hr>
<h2 id="Convolve-and-Conquer-Data-Comparison-with-Wiener-Filters"><a href="#Convolve-and-Conquer-Data-Comparison-with-Wiener-Filters" class="headerlink" title="Convolve and Conquer: Data Comparison with Wiener Filters"></a>Convolve and Conquer: Data Comparison with Wiener Filters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06558">http://arxiv.org/abs/2311.06558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dpelacani/AWLoss">https://github.com/dpelacani/AWLoss</a></li>
<li>paper_authors: Deborah Pelacani Cruz, George Strong, Oscar Bates, Carlos Cueto, Jiashun Yao, Lluis Guasch</li>
<li>for: 这个论文是为了提出一种新的数据比较方法，用于量化评估数据样本之间的差异和相似性。</li>
<li>methods: 该方法基于温因 filter 理论，通过卷积方式对数据样本进行全面比较，以便更好地捕捉数据分布的特征。</li>
<li>results: 研究人员在四种机器学习应用中使用该方法，包括数据压缩、医学影像填充、翻译类别和非Parametric生成模型。结果表明，该方法可以提供更高的数据准确率和更好的感知质量，同时具有对摆动的Robustness。<details>
<summary>Abstract</summary>
Quantitative evaluations of differences and/or similarities between data samples define and shape optimisation problems associated with learning data distributions. Current methods to compare data often suffer from limitations in capturing such distributions or lack desirable mathematical properties for optimisation (e.g. smoothness, differentiability, or convexity). In this paper, we introduce a new method to measure (dis)similarities between paired samples inspired by Wiener-filter theory. The convolutional nature of Wiener filters allows us to comprehensively compare data samples in a globally correlated way. We validate our approach in four machine learning applications: data compression, medical imaging imputation, translated classification, and non-parametric generative modelling. Our results demonstrate increased resolution in reconstructed images with better perceptual quality and higher data fidelity, as well as robustness against translations, compared to conventional mean-squared-error analogue implementations.
</details>
<details>
<summary>摘要</summary>
量化评估数据样本之间的差异和相似性定义和shape优化问题相关于学习数据分布。现有的比较方法 oft suffer from capturing这些分布的限制或缺乏优化中desirable的数学性质（例如，smoothness、 differentiability或convexity）。本文引入一种新的方法来衡量paired samples之间的（dis）similarities， draws inspiration from Wiener-filter theory。Wiener filters的卷积性质允许我们全面比较数据样本，并且在全球相关的方式下进行比较。我们在四种机器学习应用中 validate我们的方法：数据压缩、医学影像补充、翻译类别和非 Parametric生成模型。我们的结果表明我们的方法可以提供更高的重建图像分辨率、更好的感知质量和更高的数据准确性，同时具有对于平移的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Graph-ODE-with-Factorized-Prototypes-for-Modeling-Complicated-Interacting-Dynamics"><a href="#Graph-ODE-with-Factorized-Prototypes-for-Modeling-Complicated-Interacting-Dynamics" class="headerlink" title="Graph ODE with Factorized Prototypes for Modeling Complicated Interacting Dynamics"></a>Graph ODE with Factorized Prototypes for Modeling Complicated Interacting Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06554">http://arxiv.org/abs/2311.06554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Luo, Yiyang Gu, Huiyu Jiang, Jinsheng Huang, Wei Ju, Ming Zhang, Yizhou Sun</li>
<li>for: 本研究探讨了模型交互动力系统的问题，这对理解物理动力和生物过程都是关键。</li>
<li>methods: 研究使用了 геометрические图进行表示交互关系，然后使用强大的图神经网络（GNNs）进行捕捉。</li>
<li>results: 研究提出了一种新的方法 named Graph ODE with factorized prototypes (GOAT)，可以解决难以预测交互动力的问题，包括偏移量和复杂的基础规则。 GOAT 使用了分解原型的方法来提取对象级和系统级的上下文知识，从而提高了模型的通用性。<details>
<summary>Abstract</summary>
This paper studies the problem of modeling interacting dynamical systems, which is critical for understanding physical dynamics and biological processes. Recent research predominantly uses geometric graphs to represent these interactions, which are then captured by powerful graph neural networks (GNNs). However, predicting interacting dynamics in challenging scenarios such as out-of-distribution shift and complicated underlying rules remains unsolved. In this paper, we propose a new approach named Graph ODE with factorized prototypes (GOAT) to address the problem. The core of GOAT is to incorporate factorized prototypes from contextual knowledge into a continuous graph ODE framework. Specifically, GOAT employs representation disentanglement and system parameters to extract both object-level and system-level contexts from historical trajectories, which allows us to explicitly model their independent influence and thus enhances the generalization capability under system changes. Then, we integrate these disentangled latent representations into a graph ODE model, which determines a combination of various interacting prototypes for enhanced model expressivity. The entire model is optimized using an end-to-end variational inference framework to maximize the likelihood. Extensive experiments in both in-distribution and out-of-distribution settings validate the superiority of GOAT.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a new approach called Graph ODE with factorized prototypes (GOAT). The core of GOAT is to incorporate factorized prototypes from contextual knowledge into a continuous graph ODE framework. Specifically, GOAT extracts both object-level and system-level contexts from historical trajectories using representation disentanglement and system parameters, allowing us to explicitly model their independent influence and enhance the generalization capability under system changes.Next, we integrate these disentangled latent representations into a graph ODE model, which combines various interacting prototypes for enhanced model expressivity. The entire model is optimized using an end-to-end variational inference framework to maximize the likelihood.Experimental results in both in-distribution and out-of-distribution settings demonstrate the superiority of GOAT. This paper provides a new approach to modeling interacting dynamic systems, which can be applied to various fields such as physical dynamics and biological processes.
</details></li>
</ul>
<hr>
<h2 id="From-Charts-to-Atlas-Merging-Latent-Spaces-into-One"><a href="#From-Charts-to-Atlas-Merging-Latent-Spaces-into-One" class="headerlink" title="From Charts to Atlas: Merging Latent Spaces into One"></a>From Charts to Atlas: Merging Latent Spaces into One</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06547">http://arxiv.org/abs/2311.06547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donato Crisostomi, Irene Cannistraci, Luca Moschella, Pietro Barbiero, Marco Ciccone, Pietro Liò, Emanuele Rodolà</li>
<li>for: 这个研究的目的是创建一个汇集多个相关任务和数据集的综合空间，以便进行更好的分类。</li>
<li>methods: 这个研究使用了相对表示来使多个空间相似，然后使用简单的均值来汇集这些空间。</li>
<li>results: 研究发现，通过这种方法可以创建一个更好的分类空间，并且这个空间中含有任务特有的印记。此外，这种方法还可以在没有共同区域的情况下进行空间汇集，尽管效果不如结合所有任务的模型。<details>
<summary>Abstract</summary>
Models trained on semantically related datasets and tasks exhibit comparable inter-sample relations within their latent spaces. We investigate in this study the aggregation of such latent spaces to create a unified space encompassing the combined information. To this end, we introduce Relative Latent Space Aggregation, a two-step approach that first renders the spaces comparable using relative representations, and then aggregates them via a simple mean. We carefully divide a classification problem into a series of learning tasks under three different settings: sharing samples, classes, or neither. We then train a model on each task and aggregate the resulting latent spaces. We compare the aggregated space with that derived from an end-to-end model trained over all tasks and show that the two spaces are similar. We then observe that the aggregated space is better suited for classification, and empirically demonstrate that it is due to the unique imprints left by task-specific embedders within the representations. We finally test our framework in scenarios where no shared region exists and show that it can still be used to merge the spaces, albeit with diminished benefits over naive merging.
</details>
<details>
<summary>摘要</summary>
模型在semantically相关的数据集和任务上训练后，其间的inter-sample关系在幂空间中相似。本研究 investigate这种情况下，如何将这些幂空间融合成一个涵盖所有信息的共同空间。为此，我们提出了相对表示空间融合（Relative Latent Space Aggregation），它包括两个步骤：首先使用相对表示来使幂空间相似，然后使用简单的均值来融合它们。我们在三种不同的设置下分别训练了一个模型：分享样本、分享类别或者不分享任何内容。然后我们训练了每个任务的模型，并将其所得到的幂空间融合起来。我们与一个结束到终端模型训练所有任务的空间进行比较，并发现它们之间的关系很相似。我们还观察到，融合后的空间更适合分类，并且实际上表明了任务特定的嵌入器在表示中留下了独特的印记。最后，我们在没有共同区域的情况下测试了我们的框架，并发现它仍可以将空间融合，尽管效果不如预期。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Generalization-via-Set-Theory"><a href="#Understanding-Generalization-via-Set-Theory" class="headerlink" title="Understanding Generalization via Set Theory"></a>Understanding Generalization via Set Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06545">http://arxiv.org/abs/2311.06545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiqi Liu</li>
<li>for: 本研究旨在更好地理解机器学习模型的泛化性。</li>
<li>methods: 本研究使用集合论来引入算法、假设和数据集泛化的概念。我们分析了数据集泛化的性质，并证明了一个关于代理泛化过程的定理。这个定理导致了我们的泛化方法。</li>
<li>results: 通过对MNIST数据集进行泛化实验，我们获得了13,541个样本基。当使用整个训练集来评估模型性能时，模型的准确率达99.945%。但是如果将样本基Shift或修改神经网络结构，模型的性能会受到显著的下降。我们还发现了一些难以预测的样本，并发现它们都是挑战性的示例。实验证明了泛化定义的准确性和我们提出的方法的有效性。<details>
<summary>Abstract</summary>
Generalization is at the core of machine learning models. However, the definition of generalization is not entirely clear. We employ set theory to introduce the concepts of algorithms, hypotheses, and dataset generalization. We analyze the properties of dataset generalization and prove a theorem on surrogate generalization procedures. This theorem leads to our generalization method. Through a generalization experiment on the MNIST dataset, we obtain 13,541 sample bases. When we use the entire training set to evaluate the model's performance, the models achieve an accuracy of 99.945%. However, if we shift the sample bases or modify the neural network structure, the performance experiences a significant decline. We also identify consistently mispredicted samples and find that they are all challenging examples. The experiments substantiated the accuracy of the generalization definition and the effectiveness of the proposed methods. Both the set-theoretic deduction and the experiments help us better understand generalization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>机器学习模型的核心是泛化。然而，泛化的定义并不很明确。我们使用集合论来介绍算法、假设和数据泛化的概念。我们分析数据泛化的性质并证明了代替泛化过程的定理。这个定理导致我们的泛化方法。通过对 MNIST 数据集进行泛化实验，我们获得了13541个样本基。当我们使用整个训练集来评估模型的性能时，模型的准确率为99.945%。然而，如果将样本基shift或修改神经网络结构，模型的性能会受到显著的下降。我们还发现了一些难以预测的样本，并发现它们都是挑战性的示例。实验证明了泛化定义的准确性和我们提议的方法的有效性。同时，集合论 deduction 和实验帮助我们更好地理解泛化。
</details></li>
</ul>
<hr>
<h2 id="TURBO-The-Swiss-Knife-of-Auto-Encoders"><a href="#TURBO-The-Swiss-Knife-of-Auto-Encoders" class="headerlink" title="TURBO: The Swiss Knife of Auto-Encoders"></a>TURBO: The Swiss Knife of Auto-Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06527">http://arxiv.org/abs/2311.06527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Quétant, Yury Belousov, Vitaliy Kinakh, Slava Voloshynovskiy</li>
<li>for: 本研究旨在系统地分析和总结自动编码方法的信息理论基础。</li>
<li>methods: 该框架基于两个方向的共聚information flow的最大化，以derive its core concept。</li>
<li>results: 研究发现多个常见神经网络模型都可以被包含在该框架中，而信息瓶颈概念无法涵盖这些模型，因此TURBO框架成为一个更好的理论参照。<details>
<summary>Abstract</summary>
We present a novel information-theoretic framework, termed as TURBO, designed to systematically analyse and generalise auto-encoding methods. We start by examining the principles of information bottleneck and bottleneck-based networks in the auto-encoding setting and identifying their inherent limitations, which become more prominent for data with multiple relevant, physics-related representations. The TURBO framework is then introduced, providing a comprehensive derivation of its core concept consisting of the maximisation of mutual information between various data representations expressed in two directions reflecting the information flows. We illustrate that numerous prevalent neural network models are encompassed within this framework. The paper underscores the insufficiency of the information bottleneck concept in elucidating all such models, thereby establishing TURBO as a preferable theoretical reference. The introduction of TURBO contributes to a richer understanding of data representation and the structure of neural network models, enabling more efficient and versatile applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的信息理论框架，称之为TURBO，用于系统地分析和总结自编码方法。我们从自编码设置中检查信息瓶颈和瓶颈基础网络的原则，并指出其内在的限制，尤其是数据具有多个相关的物理相关表示。然后，我们介绍了TURBO框架，其核心思想是在两个方向强制实现各种数据表示之间的最大共同信息。我们示示了许多流行的神经网络模型都包含在这个框架内。文章强调信息瓶颈概念无法描述所有这些模型，因此Establish TURBO作为更好的理论参照。TURBO的引入将推动数据表示和神经网络模型的结构更深入理解，并提供更有效和灵活的应用。
</details></li>
</ul>
<hr>
<h2 id="CompCodeVet-A-Compiler-guided-Validation-and-Enhancement-Approach-for-Code-Dataset"><a href="#CompCodeVet-A-Compiler-guided-Validation-and-Enhancement-Approach-for-Code-Dataset" class="headerlink" title="CompCodeVet: A Compiler-guided Validation and Enhancement Approach for Code Dataset"></a>CompCodeVet: A Compiler-guided Validation and Enhancement Approach for Code Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06505">http://arxiv.org/abs/2311.06505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Le Chen, Arijit Bhattacharjee, Nesreen K. Ahmed, Niranjan Hasabnis, Gal Oren, Bin Lei, Ali Jannesari</li>
<li>for: 提高 LLM 在 C 和 C++ 代码生成和理解方面的表现</li>
<li>methods: 使用编译器作为教师，通过 CompCodeVet approach 提高 LLM 的 zero-shot 思维能力</li>
<li>results: CompCodeVet 在两个开源代码集中进行评估，显示其能够改善 LLM 的训练数据质量<details>
<summary>Abstract</summary>
Large language models (LLMs) have become increasingly prominent in academia and industry due to their remarkable performance in diverse applications. As these models evolve with increasing parameters, they excel in tasks like sentiment analysis and machine translation. However, even models with billions of parameters face challenges in tasks demanding multi-step reasoning. Code generation and comprehension, especially in C and C++, emerge as significant challenges. While LLMs trained on code datasets demonstrate competence in many tasks, they struggle with rectifying non-compilable C and C++ code. Our investigation attributes this subpar performance to two primary factors: the quality of the training dataset and the inherent complexity of the problem which demands intricate reasoning. Existing "Chain of Thought" (CoT) prompting techniques aim to enhance multi-step reasoning. This approach, however, retains the limitations associated with the latent drawbacks of LLMs. In this work, we propose CompCodeVet, a compiler-guided CoT approach to produce compilable code from non-compilable ones. Diverging from the conventional approach of utilizing larger LLMs, we employ compilers as a teacher to establish a more robust zero-shot thought process. The evaluation of CompCodeVet on two open-source code datasets shows that CompCodeVet has the ability to improve the training dataset quality for LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Stacked-networks-improve-physics-informed-training-applications-to-neural-networks-and-deep-operator-networks"><a href="#Stacked-networks-improve-physics-informed-training-applications-to-neural-networks-and-deep-operator-networks" class="headerlink" title="Stacked networks improve physics-informed training: applications to neural networks and deep operator networks"></a>Stacked networks improve physics-informed training: applications to neural networks and deep operator networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06483">http://arxiv.org/abs/2311.06483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amanda A Howard, Sarah H Murphy, Shady E Ahmed, Panos Stinis</li>
<li>for: 解决physics-informed neural networks和operator networks困难或无法准确地训练某些物理系统方程的问题。</li>
<li>methods: 提出了一种新的多优化框架，通过逐步堆叠physics-informed neural networks和operator networks来促进训练。每一步的输出可以作为下一步的低精度输入进行训练，逐步增加学习的模型表达能力。在每一步的迭代过程中，可以使用相同或不同的方程来模拟热处理（类似于随机扰动）。</li>
<li>results: 通过使用 benchmark问题，包括非线性摆车、波方程和viscous Burgers方程，我们展示了堆叠可以提高physics-informed neural networks和operator networks的准确率，并降低它们的大小。<details>
<summary>Abstract</summary>
Physics-informed neural networks and operator networks have shown promise for effectively solving equations modeling physical systems. However, these networks can be difficult or impossible to train accurately for some systems of equations. We present a novel multifidelity framework for stacking physics-informed neural networks and operator networks that facilitates training. We successively build a chain of networks, where the output at one step can act as a low-fidelity input for training the next step, gradually increasing the expressivity of the learned model. The equations imposed at each step of the iterative process can be the same or different (akin to simulated annealing). The iterative (stacking) nature of the proposed method allows us to progressively learn features of a solution that are hard to learn directly. Through benchmark problems including a nonlinear pendulum, the wave equation, and the viscous Burgers equation, we show how stacking can be used to improve the accuracy and reduce the required size of physics-informed neural networks and operator networks.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks 和 operator networks 已经展示了解决物理系统方程的能力。然而，这些网络可能具有一些或所有系统方程难以准确地训练。我们提出了一种新的多优化框架，用于栈层physics-informed neural networks 和 operator networks，以便训练。我们逐步建立一串网络，其输出在一个步骤可以作为下一步训练的低精度输入，逐步增加学习的模型表达能力。在每个迭代步骤中，可以使用相同或不同的方程（类似于模拟热处理）。我们的方法的迭代性让我们可以逐步学习解决方程中的难以直接学习的特征。通过使用不同的测试问题，包括非线性摆、波方程和粘性拜尔斯方程，我们证明了栈层可以提高physics-informed neural networks 和 operator networks的准确率，同时减少这些网络的大小。
</details></li>
</ul>
<hr>
<h2 id="Topology-Matching-Normalizing-Flows-for-Out-of-Distribution-Detection-in-Robot-Learning"><a href="#Topology-Matching-Normalizing-Flows-for-Out-of-Distribution-Detection-in-Robot-Learning" class="headerlink" title="Topology-Matching Normalizing Flows for Out-of-Distribution Detection in Robot Learning"></a>Topology-Matching Normalizing Flows for Out-of-Distribution Detection in Robot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06481">http://arxiv.org/abs/2311.06481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianxiang Feng, Jongseok Lee, Simon Geisler, Stephan Gunnemann, Rudolph Triebel</li>
<li>for: 提高自主机器人在实际世界中可靠部署的可靠性，通过异常检测能力。</li>
<li>methods: 使用Normalizing Flows（NFs）进行异常检测，但是在使用NFs时，往往会遇到复杂的目标分布与基础分布之间的匹配问题。这里我们使用一种表达力强的分布来匹配目标分布的 topology。</li>
<li>results: 在density estimation和2D对象检测benchmark中获得了较好的结果，并且在实际 robot部署中也展现出了良好的性能。<details>
<summary>Abstract</summary>
To facilitate reliable deployments of autonomous robots in the real world, Out-of-Distribution (OOD) detection capabilities are often required. A powerful approach for OOD detection is based on density estimation with Normalizing Flows (NFs). However, we find that prior work with NFs attempts to match the complex target distribution topologically with naive base distributions leading to adverse implications. In this work, we circumvent this topological mismatch using an expressive class-conditional base distribution trained with an information-theoretic objective to match the required topology. The proposed method enjoys the merits of wide compatibility with existing learned models without any performance degradation and minimum computation overhead while enhancing OOD detection capabilities. We demonstrate superior results in density estimation and 2D object detection benchmarks in comparison with extensive baselines. Moreover, we showcase the applicability of the method with a real-robot deployment.
</details>
<details>
<summary>摘要</summary>
In this work, we address this limitation by using an expressive class-conditional base distribution trained with an information-theoretic objective to match the required topology. Our method is compatible with existing learned models, incurs minimal computation overhead, and enhances OOD detection capabilities. We demonstrate superior performance in density estimation and 2D object detection benchmarks compared to extensive baselines, and showcase the practicality of our method with a real-robot deployment.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Fine-tuning-using-Generated-Respiratory-Sound-to-Address-Class-Imbalance"><a href="#Adversarial-Fine-tuning-using-Generated-Respiratory-Sound-to-Address-Class-Imbalance" class="headerlink" title="Adversarial Fine-tuning using Generated Respiratory Sound to Address Class Imbalance"></a>Adversarial Fine-tuning using Generated Respiratory Sound to Address Class Imbalance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06480">http://arxiv.org/abs/2311.06480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaen2891/adversarial_fine-tuning_using_generated_respiratory_sound">https://github.com/kaen2891/adversarial_fine-tuning_using_generated_respiratory_sound</a></li>
<li>paper_authors: June-Woo Kim, Chihyeon Yoon, Miika Toikkanen, Sangmin Bae, Ho-Young Jung</li>
<li>for: 提高呼吸音数据的分类性能，特别是对少数类型的呼吸音进行改进。</li>
<li>methods: 使用音频扩散模型作为 Conditional Neural Vocoder，并实现对呼吸音数据的增强。</li>
<li>results: 对ICBHI dataset进行实验，并证明了我们的反对抗学习方法可以提高呼吸音分类性能，并且在一些少数类型上提高了准确率。<details>
<summary>Abstract</summary>
Deep generative models have emerged as a promising approach in the medical image domain to address data scarcity. However, their use for sequential data like respiratory sounds is less explored. In this work, we propose a straightforward approach to augment imbalanced respiratory sound data using an audio diffusion model as a conditional neural vocoder. We also demonstrate a simple yet effective adversarial fine-tuning method to align features between the synthetic and real respiratory sound samples to improve respiratory sound classification performance. Our experimental results on the ICBHI dataset demonstrate that the proposed adversarial fine-tuning is effective, while only using the conventional augmentation method shows performance degradation. Moreover, our method outperforms the baseline by 2.24% on the ICBHI Score and improves the accuracy of the minority classes up to 26.58%. For the supplementary material, we provide the code at https://github.com/kaen2891/adversarial_fine-tuning_using_generated_respiratory_sound.
</details>
<details>
<summary>摘要</summary>
深度生成模型在医疗图像领域已经出现为数据稀缺问题提供了一个有前途的解决方案。然而，它们在时序数据如呼吸音波中的应用还较少。在这个工作中，我们提出了一种简单直观的增强呼吸音波数据不均衡问题的方法，利用音频扩散模型作为受控神经 vocoder。我们还提出了一种简单又有效的对抗训练方法，用于对真实呼吸音波样本和生成的呼吸音波样本进行对齐特征。我们的实验结果表明，我们的对抗训练方法是有效的，只使用常见增强方法时则会导致性能下降。此外，我们的方法比基线方法高2.24%的ICBHI Score和加强少数类准确率最高26.58%。详细的实验结果和代码可以在 GitHub 上找到：https://github.com/kaen2891/adversarial_fine-tuning_using_generated_respiratory_sound。
</details></li>
</ul>
<hr>
<h2 id="Online-Continual-Learning-via-Logit-Adjusted-Softmax"><a href="#Online-Continual-Learning-via-Logit-Adjusted-Softmax" class="headerlink" title="Online Continual Learning via Logit Adjusted Softmax"></a>Online Continual Learning via Logit Adjusted Softmax</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06460">http://arxiv.org/abs/2311.06460</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/k1nght/online_cl_logit_adjusted_softmax">https://github.com/k1nght/online_cl_logit_adjusted_softmax</a></li>
<li>paper_authors: Zhehao Huang, Tao Li, Chenhe Yuan, Yingwen Wu, Xiaolin Huang</li>
<li>for: 本研究旨在解决在线 continual learning 问题，即模型在非站ARY数据流中学习时避免衰老现象，并且减少最近学习类别的预测偏见。</li>
<li>methods: 本研究使用了理论分析，发现了间类差异完全由类别预置带来，并且通过调整模型征值来实现 Bayes-优论法。</li>
<li>results: 我们的方法可以有效地避免类别预置的影响，并在实际场景下提供显著的性能改进（比如 CIFAR10 上的最佳基eline 提高4.6%），而且增加了非常少的计算成本。<details>
<summary>Abstract</summary>
Online continual learning is a challenging problem where models must learn from a non-stationary data stream while avoiding catastrophic forgetting. Inter-class imbalance during training has been identified as a major cause of forgetting, leading to model prediction bias towards recently learned classes. In this paper, we theoretically analyze that inter-class imbalance is entirely attributed to imbalanced class-priors, and the function learned from intra-class intrinsic distributions is the Bayes-optimal classifier. To that end, we present that a simple adjustment of model logits during training can effectively resist prior class bias and pursue the corresponding Bayes-optimum. Our proposed method, Logit Adjusted Softmax, can mitigate the impact of inter-class imbalance not only in class-incremental but also in realistic general setups, with little additional computational cost. We evaluate our approach on various benchmarks and demonstrate significant performance improvements compared to prior arts. For example, our approach improves the best baseline by 4.6% on CIFAR10.
</details>
<details>
<summary>摘要</summary>
（online continuous learning是一个困难的问题，where models must learn from a non-stationary data stream while avoiding catastrophic forgetting. Inter-class imbalance during training has been identified as a major cause of forgetting, leading to model prediction bias towards recently learned classes. In this paper, we theoretically analyze that inter-class imbalance is entirely attributed to imbalanced class-priors, and the function learned from intra-class intrinsic distributions is the Bayes-optimal classifier. To that end, we present that a simple adjustment of model logits during training can effectively resist prior class bias and pursue the corresponding Bayes-optimum. Our proposed method, Logit Adjusted Softmax, can mitigate the impact of inter-class imbalance not only in class-incremental but also in realistic general setups, with little additional computational cost. We evaluate our approach on various benchmarks and demonstrate significant performance improvements compared to prior arts. For example, our approach improves the best baseline by 4.6% on CIFAR10.）
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-Contrastive-Multimodal-Learning-for-Advancing-Chemical-Understanding"><a href="#Asymmetric-Contrastive-Multimodal-Learning-for-Advancing-Chemical-Understanding" class="headerlink" title="Asymmetric Contrastive Multimodal Learning for Advancing Chemical Understanding"></a>Asymmetric Contrastive Multimodal Learning for Advancing Chemical Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06456">http://arxiv.org/abs/2311.06456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Xu, Yifei Wang, Yunrui Li, Pengyu Hong</li>
<li>for: 这篇论文旨在提出一种新的多模态深度学习方法，用于提高化学研究和应用。</li>
<li>methods: 这篇论文使用了异形对比学习方法，将化学多modalities的信息转移到分子图表示中，以实现多modalities的共同理解。</li>
<li>results: 实验表明，ACML可以帮助化学研究人员更好地理解分子的含义，并提高化学应用的表达力和可解释性。<details>
<summary>Abstract</summary>
The versatility of multimodal deep learning holds tremendous promise for advancing scientific research and practical applications. As this field continues to evolve, the collective power of cross-modal analysis promises to drive transformative innovations, leading us to new frontiers in chemical understanding and discovery. Hence, we introduce Asymmetric Contrastive M}ultimodal Learning (ACML) as a novel approach tailored for molecules, showcasing its potential to advance the field of chemistry. ACML harnesses the power of effective asymmetric contrastive learning to seamlessly transfer information from various chemical modalities to molecular graph representations. By combining pre-trained chemical unimodal encoders and a shallow-designed graph encoder, ACML facilitates the assimilation of coordinated chemical semantics from different modalities, leading to comprehensive representation learning with efficient training. This innovative framework enhances the interpretability of learned representations and bolsters the expressive power of graph neural networks. Through practical tasks such as isomer discrimination and uncovering crucial chemical properties for drug discovery, ACML exhibits its capability to revolutionize chemical research and applications, providing a deeper understanding of chemical semantics of different modalities.
</details>
<details>
<summary>摘要</summary>
多模态深度学习的多样性具有推进科学研究和实用应用的巨大承诺。随着这个领域的进一步发展，跨模态分析的共同力将驱动 transformative 创新，带我们进入新的化学理解和发现的前iers。因此，我们介绍 Asymmetric Contrastive Multimodal Learning（ACML）作为一种新的方法，特地设计用于分子，展示其在化学领域的潜在发展 potential。ACML 利用有效的不对称对比学习来传递不同化学modalities中的各种 semantics 到分子图表示。通过将预训练的化学uni模态编码器和一个浅层设计的图编码器结合在一起，ACML 实现了模态之间的协调化学 semantics的同化，从而实现了全面的表示学习，并且可以高效地训练。这种创新的框架提高了学习表示的可读性和图神经网络的表达能力。通过实际任务，如分子同分子识别和找到重要的药物发现中的化学性质，ACML 展示了其在化学研究和应用中的革命性潜力，为不同modalities的化学semantics提供了更深刻的理解。
</details></li>
</ul>
<hr>
<h2 id="A-Saliency-based-Clustering-Framework-for-Identifying-Aberrant-Predictions"><a href="#A-Saliency-based-Clustering-Framework-for-Identifying-Aberrant-Predictions" class="headerlink" title="A Saliency-based Clustering Framework for Identifying Aberrant Predictions"></a>A Saliency-based Clustering Framework for Identifying Aberrant Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06454">http://arxiv.org/abs/2311.06454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aina Tersol Montserrat, Alexander R. Loftus, Yael Daihes</li>
<li>for: 这篇论文旨在提高机器学习分类器在高度不确定的生物医学应用中的可靠性和信任性。</li>
<li>methods: 本论文提出了一种新的训练方法，旨在降低误分率并识别异常预测。</li>
<li>results: 本论文的方法在 veterinary radiology 领域中实现了20%的精度提升。<details>
<summary>Abstract</summary>
In machine learning, classification tasks serve as the cornerstone of a wide range of real-world applications. Reliable, trustworthy classification is particularly intricate in biomedical settings, where the ground truth is often inherently uncertain and relies on high degrees of human expertise for labeling. Traditional metrics such as precision and recall, while valuable, are insufficient for capturing the nuances of these ambiguous scenarios. Here we introduce the concept of aberrant predictions, emphasizing that the nature of classification errors is as critical as their frequency. We propose a novel, efficient training methodology aimed at both reducing the misclassification rate and discerning aberrant predictions. Our framework demonstrates a substantial improvement in model performance, achieving a 20\% increase in precision. We apply this methodology to the less-explored domain of veterinary radiology, where the stakes are high but have not been as extensively studied compared to human medicine. By focusing on the identification and mitigation of aberrant predictions, we enhance the utility and trustworthiness of machine learning classifiers in high-stakes, real-world scenarios, including new applications in the veterinary world.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:机器学习中的分类任务是广泛应用的基础。在生物医学设置下，可靠、可信的分类特别复杂，因为ground truth的自然状况是 uncertain，需要高度的人类专业知识进行标注。传统的精度和 recall 指标不能 Capture 这些抽象的情况。我们提出了异常预测的概念，强调异常预测的性质是重要的，不仅是频率。我们提出了一种新的训练方法，可以减少错分率，并且可以识别异常预测。我们的框架在 veterinary radiology 领域中实现了20%的提升精度。我们将这种方法应用到未经充分研究的 veterinary 世界，以提高机器学习分类器在高风险、真实世界中的可靠性和可信worthiness。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Pooling-Bias-in-E-commerce-Search-via-False-Negative-Estimation"><a href="#Mitigating-Pooling-Bias-in-E-commerce-Search-via-False-Negative-Estimation" class="headerlink" title="Mitigating Pooling Bias in E-commerce Search via False Negative Estimation"></a>Mitigating Pooling Bias in E-commerce Search via False Negative Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06444">http://arxiv.org/abs/2311.06444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaochen Wang, Xiao Xiao, Ruhan Zhang, Xuan Zhang, Taesik Na, Tejaswi Tenneti, Haixun Wang, Fenglong Ma</li>
<li>for: 提高用户体验和商业成功，需要准确和高效地评估产品相关性。</li>
<li>methods: 使用新的偏见抑制硬性负采样策略（BHNS），可以减轻pooling bias，提高性能和商业影响。</li>
<li>results: 在Instacart搜索设置中，BHNS实现了实用电商应用。此外，对公共数据集进行比较分析，表明BHNS具有适用于多种应用场景的领域独特性。<details>
<summary>Abstract</summary>
Efficient and accurate product relevance assessment is critical for user experiences and business success. Training a proficient relevance assessment model requires high-quality query-product pairs, often obtained through negative sampling strategies. Unfortunately, current methods introduce pooling bias by mistakenly sampling false negatives, diminishing performance and business impact. To address this, we present Bias-mitigating Hard Negative Sampling (BHNS), a novel negative sampling strategy tailored to identify and adjust for false negatives, building upon our original False Negative Estimation algorithm. Our experiments in the Instacart search setting confirm BHNS as effective for practical e-commerce use. Furthermore, comparative analyses on public dataset showcase its domain-agnostic potential for diverse applications.
</details>
<details>
<summary>摘要</summary>
高效和准确的产品相关性评估对用户体验和商业成功至关重要。训练一个高效的相关性评估模型需要高质量的查询-产品对，常常通过负样本策略获得。然而，现有方法带有汇总偏见，由于错误地抽取假负样本，导致性能和商业影响减退。为解决这问题，我们提出了减少偏见的负样本选择策略（BHNS），基于我们原始的假负样本估计算算法。我们在Instacart搜索设置中进行了实验，证实BHNS在实际电商应用中是有效的。此外，我们对公共数据集进行了比较分析，显示BHNS在多种应用领域具有领域无关的潜在应用潜力。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/11/cs.LG_2023_11_11/" data-id="clpahu77400ux3h882flnb9gq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/11/eess.IV_2023_11_11/" class="article-date">
  <time datetime="2023-11-11T09:00:00.000Z" itemprop="datePublished">2023-11-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/11/eess.IV_2023_11_11/">eess.IV - 2023-11-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DUBLINE-A-Deep-Unfolding-Network-for-B-line-Detection-in-Lung-Ultrasound-Images"><a href="#DUBLINE-A-Deep-Unfolding-Network-for-B-line-Detection-in-Lung-Ultrasound-Images" class="headerlink" title="DUBLINE: A Deep Unfolding Network for B-line Detection in Lung Ultrasound Images"></a>DUBLINE: A Deep Unfolding Network for B-line Detection in Lung Ultrasound Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06672">http://arxiv.org/abs/2311.06672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianqi Yang, Nantheera Anantrasirichai, Oktay Karakuş, Marco Allinovi, Hatice Ceylan Koydemir, Alin Achim</li>
<li>for: 这篇论文的目的是提高肺超音波检测中的B-线检测精度和速度。</li>
<li>methods: 这篇论文使用了深度 unfolding 的 Alternating Direction Method of Multipliers (ADMM) 来解决肺超音波检测中的B-线检测问题。</li>
<li>results: 比较 traditional model-based method, 这篇论文的方法可以更快速地完成B-线检测（更多于 90 倍），并且精度也提高了10.6%。<details>
<summary>Abstract</summary>
In the context of lung ultrasound, the detection of B-lines, which are indicative of interstitial lung disease and pulmonary edema, plays a pivotal role in clinical diagnosis. Current methods still rely on visual inspection by experts. Vision-based automatic B-line detection methods have been developed, but their performance has yet to improve in terms of both accuracy and computational speed. This paper presents a novel approach to posing B-line detection as an inverse problem via deep unfolding of the Alternating Direction Method of Multipliers (ADMM). It tackles the challenges of data labelling and model training in lung ultrasound image analysis by harnessing the capabilities of deep neural networks and model-based methods. Our objective is to substantially enhance diagnostic accuracy while ensuring efficient real-time capabilities. The results show that the proposed method runs more than 90 times faster than the traditional model-based method and achieves an F1 score that is 10.6% higher.
</details>
<details>
<summary>摘要</summary>
在肺超声 imaging 中，B-线的检测对临床诊断具有重要的作用。现有方法仍然依赖于专家的视觉检查。基于视觉的自动B-线检测方法已经开发，但其性能还未得到改进。这篇论文提出了一种将B-线检测转换为反问题via深度嵌入ADMM的新方法。它利用深度神经网络和模型基于方法来解决肺超声图像分析中的数据标注和模型训练问题。我们的目标是substantially提高诊断精度，同时保持高速的实时能力。结果显示，提出的方法在计算速度方面比传统的模型基于方法快上了90多个倍，并且 achieved an F1 score 10.6% 高于传统模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/11/eess.IV_2023_11_11/" data-id="clpahu7eq01dh3h88dfao1c9z" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/11/eess.SP_2023_11_11/" class="article-date">
  <time datetime="2023-11-11T08:00:00.000Z" itemprop="datePublished">2023-11-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/11/eess.SP_2023_11_11/">eess.SP - 2023-11-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Design-of-Reconfigurable-Intelligent-Surfaces-by-Using-S-Parameter-Multiport-Network-Theory-–-Optimization-and-Full-Wave-Validation"><a href="#Design-of-Reconfigurable-Intelligent-Surfaces-by-Using-S-Parameter-Multiport-Network-Theory-–-Optimization-and-Full-Wave-Validation" class="headerlink" title="Design of Reconfigurable Intelligent Surfaces by Using S-Parameter Multiport Network Theory – Optimization and Full-Wave Validation"></a>Design of Reconfigurable Intelligent Surfaces by Using S-Parameter Multiport Network Theory – Optimization and Full-Wave Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06648">http://arxiv.org/abs/2311.06648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Abrardo, Alberto Toccafondi, Marco Di Renzo<br>for:* 这篇论文主要研究的是利用多口网络理论分析和优化智能反射表面（RIS），尤其是在距离半波长之下 spacing 的情况下。methods:* 这篇论文使用了 $Z$-parameter（阻抗）和 $S$-parameter（散射）矩阵来表示 RIS 的辐射特性。* 提出了一种基于 $S$-parameter 表示的迭代算法，用于在电romagnetic 互相作用的情况下优化 RIS 的可调负载。results:* 研究发现，通过对 RIS 的结构散射进行考虑，可以更好地优化 RIS 的辐射特性，并且可以在不同的方向上获得更高的接收功率。* 对比 $Z$-parameter 和 $S$-parameter 表示，发现 $S$-parameter 更能准确地描述 RIS 的辐射特性，并且可以更快地获得更好的优化结果。<details>
<summary>Abstract</summary>
Multiport network theory has been proved to be a suitable abstraction model for analyzing and optimizing reconfigurable intelligent surfaces (RISs), especially for studying the impact of the electromagnetic mutual coupling among radiating elements that are spaced less than half of the wavelength. Both representations in terms of $Z$-parameter (impedance) and $S$-parameter (scattering) matrices are widely utilized. In this paper, we embrace multiport network theory for analyzing and optimizing the reradiation properties of RIS-aided channels, and provide four new contributions. (i) First, we offer a thorough comparison between the $Z$-parameter and $S$-parameter representations. This comparison allows us to unveil that the typical scattering models utilized for RIS-aided channels ignore the structural scattering from the RIS, which results in an unwanted specular reflection. (ii) Then, we develop an iterative algorithm for optimizing, in the presence of electromagnetic mutual coupling, the tunable loads of the RIS based on the $S$-parameters representation. We prove that small perturbations of the step size of the algorithm result in larger variations of the $S$-parameter matrix compared with the $Z$-parameter matrix, resulting in a faster convergence rate. (iii) Subsequently, we generalize the proposed algorithm to suppress the specular reflection due to the structural scattering, while maximizing the received power towards the direction of interest, and analyze the effectiveness and tradeoffs of the proposed approach. (iv) Finally, we validate the theoretical findings and algorithms with numerical simulations and a commercial full-wave electromagnetic simulator based on the method of moments.
</details>
<details>
<summary>摘要</summary>
多ports网络理论被证明是对折叠智能表面（RIS）的分析和优化模型适用，尤其是研究电磁共振元素之间的电磁共振coupling的影响。这两种表述都广泛使用$Z$-参数（阻抗）和$S$-参数（散射）矩阵。在这篇论文中，我们使用多ports网络理论分析和优化RIS-帮助通道的反射特性，并提供四项新贡献。（i）首先，我们对$Z$-参数和$S$-参数表述进行了深入的比较。这种比较表明，通常用于RIS-帮助通道的散射模型忽略了RIS的结构散射，导致不必要的反射。（ii）然后，我们开发了基于$S$-参数表述的迭代算法，用于在电磁共振coupling存在的情况下优化RIS的可变荷重。我们证明，对算法步长的小 perturbation会导致$S$-参数矩阵中的变化更大，而$Z$-参数矩阵中的变化更小，因此算法的速度更快。（iii）接着，我们扩展了提议的算法，以suppress结构散射引起的反射，同时 Maximize received power towards the direction of interest。我们分析了提议的效果和牺牲。（iv）最后，我们验证了理论发现和算法通过数值仿真和商业全波电磁 simulator based on the method of moments。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-for-Space-Air-Ground-Integrated-Networks-SAGIN"><a href="#Generative-AI-for-Space-Air-Ground-Integrated-Networks-SAGIN" class="headerlink" title="Generative AI for Space-Air-Ground Integrated Networks (SAGIN)"></a>Generative AI for Space-Air-Ground Integrated Networks (SAGIN)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06523">http://arxiv.org/abs/2311.06523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruichen Zhang, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Ping Zhang, Dong In Kim</li>
<li>for: 本文探讨了将生成AI应用于空地空间 интегра网络（SAGIN）中，强调其应用前景和实践案例。</li>
<li>methods: 本文首先提供了SAGIN和生成AI模型的全面回顾，探讨了它们的可能的integration应用场景和机会。然后，提出了一种基于生成Diffusion Model（GDM）的框架，用于提高SAGIN的服务质量。</li>
<li>results: 根据实验结果，提出的框架能够提高SAGIN的服务质量。此外，本文还讨论了将来的生成AI-enabled SAGIN研究方向。<details>
<summary>Abstract</summary>
Recently, generative AI technologies have emerged as a significant advancement in artificial intelligence field, renowned for their language and image generation capabilities. Meantime, space-air-ground integrated network (SAGIN) is an integral part of future B5G/6G for achieving ubiquitous connectivity. Inspired by this, this article explores an integration of generative AI in SAGIN, focusing on potential applications and case study. We first provide a comprehensive review of SAGIN and generative AI models, highlighting their capabilities and opportunities of their integration. Benefiting from generative AI's ability to generate useful data and facilitate advanced decision-making processes, it can be applied to various scenarios of SAGIN. Accordingly, we present a concise survey on their integration, including channel modeling and channel state information (CSI) estimation, joint air-space-ground resource allocation, intelligent network deployment, semantic communications, image extraction and processing, security and privacy enhancement. Next, we propose a framework that utilizes a Generative Diffusion Model (GDM) to construct channel information map to enhance quality of service for SAGIN. Simulation results demonstrate the effectiveness of the proposed framework. Finally, we discuss potential research directions for generative AI-enabled SAGIN.
</details>
<details>
<summary>摘要</summary>
最近，生成式人工智能技术在人工智能领域取得了重要进步，被广泛应用于语言和图像生成等领域。同时，空天地三合一网络（SAGIN）是未来5G/6G的重要组成部分，旨在实现无限连接。以此为启发，本文探讨了生成式人工智能在SAGIN中的 интеграцию，主要强调其应用前景和实践案例。我们首先提供了SAGIN和生成式人工智能模型的全面审视，探讨它们的可能的 интеграción和应用前景。生成式人工智能可以生成有用的数据，并促进高级决策过程，因此可以应用于SAGIN多种场景。在这些应用场景中，我们提出了一种基于生成扩散模型（GDM）的框架，用于提高SAGIN的质量服务。实验结果表明该框架的效果是可靠的。最后，我们讨论了生成式人工智能在SAGIN中的未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Semantic-aware-Sampling-and-Transmission-in-Energy-Harvesting-Systems-A-POMDP-Approach"><a href="#Semantic-aware-Sampling-and-Transmission-in-Energy-Harvesting-Systems-A-POMDP-Approach" class="headerlink" title="Semantic-aware Sampling and Transmission in Energy Harvesting Systems: A POMDP Approach"></a>Semantic-aware Sampling and Transmission in Energy Harvesting Systems: A POMDP Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06522">http://arxiv.org/abs/2311.06522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abolfazl Zakeri, Mohammad Moltafet, Marian Codreanu</li>
<li>for: 本研究探讨了一种能量吸收系统中的实时跟踪问题，并在不完美的通道情况下进行了研究。</li>
<li>methods: 本文使用了Markov源模型，并考虑了采样和传输成本。不同于大多数先前研究，本文假设源不可见。</li>
<li>results: 研究人员通过解决一个随机控制问题，实现了三个semantic-aware指标的共同优化：一、信息年龄（AoI），二、通信质量，三、错误信息年龄（AoII）。通过仿真实验，研究人员发现了优化策略的性能提升，并发现了不同的 switching-type 优化策略。<details>
<summary>Abstract</summary>
We study real-time tracking problem in an energy harvesting system with a Markov source under an imperfect channel. We consider both sampling and transmission costs and different from most prior studies that assume the source is fully observable, the sampling cost renders the source unobservable. The goal is to jointly optimize sampling and transmission policies for three semantic-aware metrics: i) the age of information (AoI), ii) general distortion, and iii) the age of incorrect information (AoII). To this end, we formulate and solve a stochastic control problem. Specifically, for the AoI metric, we cast a Markov decision process (MDP) problem and solve it using relative value iteration (RVI). For the distortion and AoII metrics, we utilize the partially observable MDP (POMDP) modeling and leverage the notion of belief MDP formulation of POMDP to find optimal policies. For the distortion metric and the AoII metric under the perfect channel setup, we effectively truncate the corresponding belief space and solve an MDP problem using RVI. For the general setup, a deep reinforcement learning policy is proposed. Through simulations, we demonstrate significant performance improvements achieved by the derived policies. The results reveal various switching-type structures of optimal policies and show that a distortion-optimal policy is also AoII optimal.
</details>
<details>
<summary>摘要</summary>
我们研究实时跟踪问题在能量收集系统中，其中源是Markov过程，并且通信频道存在不完美性。我们考虑了抽样和传输成本，并且不同于大多数前一些研究，源不可见。我们的目标是同时优化抽样和传输策略，以达到三个semantic-aware指标的最优化：一、信息年龄（AoI），二、通信误差，三、错误信息年龄（AoII）。为此，我们设计了一个随机控制问题，并使用相对价值迭代（RVI）解决Markov决策过程（MDP）问题。对于AoI指标，我们使用POMDP模型和信念MDP形式进行解决。对于误差指标和AoII指标在完美通信设置下，我们有效地舒缩相应的信念空间，并使用RVI解决MDP问题。在总体设置下，我们提议了深度强化学习策略。通过sime simulations，我们发现derived策略具有显著的性能改进。结果显示了不同的 switching-type结构，并证明了误差优化策略也是AoII优化的。
</details></li>
</ul>
<hr>
<h2 id="Sum-Rate-Optimization-for-RIS-Aided-Multiuser-Communications-with-Movable-Antenna"><a href="#Sum-Rate-Optimization-for-RIS-Aided-Multiuser-Communications-with-Movable-Antenna" class="headerlink" title="Sum-Rate Optimization for RIS-Aided Multiuser Communications with Movable Antenna"></a>Sum-Rate Optimization for RIS-Aided Multiuser Communications with Movable Antenna</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06501">http://arxiv.org/abs/2311.06501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunan Sun, Hao Xu, Chongjun Ouyang, Hongwen Yang</li>
<li>for: 本研究旨在提高无线通信网络性能，探讨了可程度智能表面（RIS）技术的应用。</li>
<li>methods: 本文提出了一个基于RIS的多用户通信系统，利用可动天线（MA）技术优化通道容量。</li>
<li>results: 提出的迭代算法可以优化照明、RIS的反射系数（RC）值和MA的位置，以提高系统的总资料率。numerical results显示了提案的方法的有效性和MA-based系统在总资料率方面的优势。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) is known as a promising technology to improve the performance of wireless communication networks, which has been extensively studied. Movable antenna (MA) is a novel technology that fully exploits the antenna position for enhancing the channel capacity. In this paper, we propose a new RIS-aided multiuser communication system with MAs. The sum-rate is maximized by jointly optimizing the beamforming, the reflection coefficient (RC) values of RIS and the positions of MAs. A fractional programming-based iterative algorithm is proposed to solve the formulated non-convex problem, considering three assumptions for the RIS. Numerical results are presented to verify the effectiveness of the proposed algorithm and the superiority of the proposed MA-based system in terms of sum-rate.
</details>
<details>
<summary>摘要</summary>
改进无线通信网络性能的可 configurable智能表面（RIS）技术已经广泛研究， movable antenna（MA）是一种新的技术，它可以全面利用天线位置来提高通信频率。在这篇论文中，我们提议一种基于RIS的多用户通信系统，并使用MA来提高系统性能。我们使用一种基于分数编程的迭代算法来最大化宽扩权（beamforming）、RIS反射系数（RC）和MA位置的优化问题。我们对问题进行了非几何化处理，并根据RIS的三个假设进行了解释。我们通过数值结果验证了我们的提案的有效性和MA基本系统的提高性。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Communication-for-Cooperative-Perception-based-on-Importance-Map"><a href="#Semantic-Communication-for-Cooperative-Perception-based-on-Importance-Map" class="headerlink" title="Semantic Communication for Cooperative Perception based on Importance Map"></a>Semantic Communication for Cooperative Perception based on Importance Map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06498">http://arxiv.org/abs/2311.06498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Sheng, Hao Ye, Le Liang, Shi Jin, Geoffrey Ye Li</li>
<li>for: 这 paper 的目的是提出一种基于 Vehicle-to-Vehicle (V2V) 通信技术的 Cooperative Perception 方法，以便实现自动驾驶车辆的3D объек体探测。</li>
<li>methods: 本 paper 使用了一种Importance Map 技术来提取 semantic information，并提出了一种新的 Cooperative Perception Semantic Communication Scheme with Intermediate Fusion。</li>
<li>results:  simulations 表明，我们的提议的模型在不同的通道模型下表现出了优于传统分离源-通道编码的性能。此外，我们的模型还能够在时变 multipath 拍抄频道下保持robustness。<details>
<summary>Abstract</summary>
Cooperative perception, which has a broader perception field than single-vehicle perception, has played an increasingly important role in autonomous driving to conduct 3D object detection. Through vehicle-to-vehicle (V2V) communication technology, various connected automated vehicles (CAVs) can share their sensory information (LiDAR point clouds) for cooperative perception. We employ an importance map to extract significant semantic information and propose a novel cooperative perception semantic communication scheme with intermediate fusion. Meanwhile, our proposed architecture can be extended to the challenging time-varying multipath fading channel. To alleviate the distortion caused by the time-varying multipath fading, we adopt explicit orthogonal frequency-division multiplexing (OFDM) blocks combined with channel estimation and channel equalization. Simulation results demonstrate that our proposed model outperforms the traditional separate source-channel coding over various channel models. Moreover, a robustness study indicates that only part of semantic information is key to cooperative perception. Although our proposed model has only been trained over one specific channel, it has the ability to learn robust coded representations of semantic information that remain resilient to various channel models, demonstrating its generality and robustness.
</details>
<details>
<summary>摘要</summary>
合作感知，具有更广泛的感知范围 than single-vehicle perception，在自动驾驶中扮演着越来越重要的角色，以实现3D对象探测。通过自动汽车之间的通信技术（V2V），不同的相连自动汽车（CAVs）可以共享它们的感知信息（LiDAR点云）进行合作感知。我们使用重要度图 Extract significant semantic information and propose a novel cooperative perception semantic communication scheme with intermediate fusion. Meanwhile, our proposed architecture can be extended to the challenging time-varying multipath fading channel. To alleviate the distortion caused by the time-varying multipath fading, we adopt explicit orthogonal frequency-division multiplexing (OFDM) blocks combined with channel estimation and channel equalization. Simulation results demonstrate that our proposed model outperforms the traditional separate source-channel coding over various channel models. Moreover, a robustness study indicates that only part of semantic information is key to cooperative perception. Although our proposed model has only been trained over one specific channel, it has the ability to learn robust coded representations of semantic information that remain resilient to various channel models, demonstrating its generality and robustness.Here's the translation in Traditional Chinese:合作感知，具有更广泛的感知范围 than single-vehicle perception，在自动驾驶中扮演着越来越重要的角色，以实现3D对象探测。通过自动汽车之间的通信技术（V2V），不同的相连自动汽车（CAVs）可以共享它们的感知信息（LiDAR点云）进行合作感知。我们使用重要度图 Extract significant semantic information and propose a novel cooperative perception semantic communication scheme with intermediate fusion. Meanwhile, our proposed architecture can be extended to the challenging time-varying multipath fading channel. To alleviate the distortion caused by the time-varying multipath fading, we adopt explicit orthogonal frequency-division multiplexing (OFDM) blocks combined with channel estimation and channel equalization. Simulation results demonstrate that our proposed model outperforms the traditional separate source-channel coding over various channel models. Moreover, a robustness study indicates that only part of semantic information is key to cooperative perception. Although our proposed model has only been trained over one specific channel, it has the ability to learn robust coded representations of semantic information that remain resilient to various channel models, demonstrating its generality and robustness.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/11/eess.SP_2023_11_11/" data-id="clpahu7gj01hz3h88b0xmbz5q" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/10/cs.CV_2023_11_10/" class="article-date">
  <time datetime="2023-11-10T13:00:00.000Z" itemprop="datePublished">2023-11-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/10/cs.CV_2023_11_10/">cs.CV - 2023-11-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Flatness-aware-Adversarial-Attack"><a href="#Flatness-aware-Adversarial-Attack" class="headerlink" title="Flatness-aware Adversarial Attack"></a>Flatness-aware Adversarial Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06423">http://arxiv.org/abs/2311.06423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyuan Fan, Xiaodan Li, Cen Chen, Yinggui Wang</li>
<li>for: 这 paper 的目的是通过利用抗击器的传输性来发动黑盒攻击。</li>
<li>methods: 这 paper 使用的方法是通过组合多个转换后的输入来生成抗击器。</li>
<li>results:  compared with 现有基elines，这 paper 的方法可以明显提高抗击器的传输性。Here’s the full translation of the paper’s abstract in Simplified Chinese:</li>
<li>for: 这 paper 的目的是通过利用抗击器的传输性来发动黑盒攻击。</li>
<li>methods: 这 paper 使用的方法是通过组合多个转换后的输入来生成抗击器。</li>
<li>results:  compared with 现有基elines，这 paper 的方法可以明显提高抗击器的传输性。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
The transferability of adversarial examples can be exploited to launch black-box attacks. However, adversarial examples often present poor transferability. To alleviate this issue, by observing that the diversity of inputs can boost transferability, input regularization based methods are proposed, which craft adversarial examples by combining several transformed inputs. We reveal that input regularization based methods make resultant adversarial examples biased towards flat extreme regions. Inspired by this, we propose an attack called flatness-aware adversarial attack (FAA) which explicitly adds a flatness-aware regularization term in the optimization target to promote the resultant adversarial examples towards flat extreme regions. The flatness-aware regularization term involves gradients of samples around the resultant adversarial examples but optimizing gradients requires the evaluation of Hessian matrix in high-dimension spaces which generally is intractable. To address the problem, we derive an approximate solution to circumvent the construction of Hessian matrix, thereby making FAA practical and cheap. Extensive experiments show the transferability of adversarial examples crafted by FAA can be considerably boosted compared with state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
“敌方模型可以通过对抗性示例的转移性攻击。但是，对抗性示例通常具有差的转移性。为解决这个问题，我们观察到输入多标的帮助，可以提高对抗性示例的转移性。我们提出了基于输入调整的方法，这些方法通过组合多个对抗性示例的转换而创建对抗性示例。我们发现这些对抗性示例倾向于扁平极大区域。受这些想法所影响，我们提出了一种名为扁平识别攻击（FAA）的攻击方法。这个方法将在优化目标中添加一个扁平识别调整项，以便提高对抗性示例的转移性。扁平识别调整项需要在高维度空间中评估扁平方向的梯度，但是评估梯度通常是不可能的。为解决这个问题，我们 derive an approximate solution，以便在高维度空间中评估扁平方向的梯度，并且让FAA实用且便宜。实验结果表明，由FAA创建的对抗性示例的转移性可以与现有基准相比大大提高。”
</details></li>
</ul>
<hr>
<h2 id="EviPrompt-A-Training-Free-Evidential-Prompt-Generation-Method-for-Segment-Anything-Model-in-Medical-Images"><a href="#EviPrompt-A-Training-Free-Evidential-Prompt-Generation-Method-for-Segment-Anything-Model-in-Medical-Images" class="headerlink" title="EviPrompt: A Training-Free Evidential Prompt Generation Method for Segment Anything Model in Medical Images"></a>EviPrompt: A Training-Free Evidential Prompt Generation Method for Segment Anything Model in Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06400">http://arxiv.org/abs/2311.06400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinsong Xu, Jiaqi Tang, Aidong Men, Qingchao Chen</li>
<li>for: 这篇论文的目的是提出一种无需训练的证据提示生成方法，以解决医疗影像分类中的专业知识干预和领域差距问题。</li>
<li>methods: 这篇论文提出了一种基于医疗影像内在相似性的训练�free evidential prompt generation方法，仅需一个参考影像�annotationPair，可以大幅减少 Labeling 和计算资源的需求。</li>
<li>results: 该方法可以自动生成适当的证据提示，以提高 SAM 在医疗影像分类中的应用和有用性。 evaluations across a broad range of tasks and modalities confirm its efficacy.<details>
<summary>Abstract</summary>
Medical image segmentation has immense clinical applicability but remains a challenge despite advancements in deep learning. The Segment Anything Model (SAM) exhibits potential in this field, yet the requirement for expertise intervention and the domain gap between natural and medical images poses significant obstacles. This paper introduces a novel training-free evidential prompt generation method named EviPrompt to overcome these issues. The proposed method, built on the inherent similarities within medical images, requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labeling and computational resources. First, to automatically generate prompts for SAM in medical images, we introduce an evidential method based on uncertainty estimation without the interaction of clinical experts. Then, we incorporate the human prior into the prompts, which is vital for alleviating the domain gap between natural and medical images and enhancing the applicability and usefulness of SAM in medical scenarios. EviPrompt represents an efficient and robust approach to medical image segmentation, with evaluations across a broad range of tasks and modalities confirming its efficacy.
</details>
<details>
<summary>摘要</summary>
医学图像分割具有巨大的临床应用前提，但是它仍然是一个挑战，尽管深度学习在发展。 seg anything模型（SAM）在这一点方面表现出潜力，但是需要专家干预和医学图像和自然图像之间的领域差距问题带来了重大障碍。这篇论文介绍了一种新的无需训练的证据提示生成方法，名为EviPrompt，以解决这些问题。我们的方法基于医学图像之间的自然相似性，只需要一个参考图像-标注对，可以减少了大量的标注和计算资源。首先，我们引入了一种基于不确定性估计的证据方法，无需互动式临床专家。然后，我们将人类优先级 integrate 到提示中，这是关键的，可以减少医学图像和自然图像之间的领域差距，提高SAM在医学场景中的应用和实用性。EviPrompt表示一种高效和可靠的医学图像分割方法，评估结果 across 多种任务和模式表明其效果。
</details></li>
</ul>
<hr>
<h2 id="A-design-of-Convolutional-Neural-Network-model-for-the-Diagnosis-of-the-COVID-19"><a href="#A-design-of-Convolutional-Neural-Network-model-for-the-Diagnosis-of-the-COVID-19" class="headerlink" title="A design of Convolutional Neural Network model for the Diagnosis of the COVID-19"></a>A design of Convolutional Neural Network model for the Diagnosis of the COVID-19</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06394">http://arxiv.org/abs/2311.06394</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jafar-Abdollahi/Automated-detection-of-COVID-19-cases-using-deep-neural-networks-with-CTS-images">https://github.com/Jafar-Abdollahi/Automated-detection-of-COVID-19-cases-using-deep-neural-networks-with-CTS-images</a></li>
<li>paper_authors: Xinyuan Song<br>for:这种研究的目的是为了提供一种准确地识别COVID-19的肺部X射线图像分类方法，以帮助临床中心和医院诊断COVID-19。methods:这种方法基于19层卷积神经网络（CNN），并对三类（肺炎、正常、COVID）和四类（肺擦亮、正常、COVID-19、肺炎）进行分类。研究人员还对一些已经预训练的网络进行比较，包括Inception、Alexnet、ResNet50、Squeezenet和VGG19。results:实验结果表明，提出的CNN方法在准确率、特异性、准确率、敏感度和归一化矩阵等指标上具有明显的优势，超过了现有的发布过程。这种方法可以为临床医生提供一个有用的工具，帮助他们准确地诊断COVID-19。<details>
<summary>Abstract</summary>
With the spread of COVID-19 around the globe over the past year, the usage of artificial intelligence (AI) algorithms and image processing methods to analyze the X-ray images of patients' chest with COVID-19 has become essential. The COVID-19 virus recognition in the lung area of a patient is one of the basic and essential needs of clicical centers and hospitals. Most research in this field has been devoted to papers on the basis of deep learning methods utilizing CNNs (Convolutional Neural Network), which mainly deal with the screening of sick and healthy people.In this study, a new structure of a 19-layer CNN has been recommended for accurately recognition of the COVID-19 from the X-ray pictures of chest. The offered CNN is developed to serve as a precise diagnosis system for a three class (viral pneumonia, Normal, COVID) and a four classclassification (Lung opacity, Normal, COVID-19, and pneumonia). A comparison is conducted among the outcomes of the offered procedure and some popular pretrained networks, including Inception, Alexnet, ResNet50, Squeezenet, and VGG19 and based on Specificity, Accuracy, Precision, Sensitivity, Confusion Matrix, and F1-score. The experimental results of the offered CNN method specify its dominance over the existing published procedures. This method can be a useful tool for clinicians in deciding properly about COVID-19.
</details>
<details>
<summary>摘要</summary>
随着 COVID-19 在过去一年内的全球蔓延，使用人工智能（AI）算法和图像处理方法来分析患 COVID-19 患者的X射线图像已成为必需的。识别患 COVID-19 病毒在患者的肺部是临床中心和医院的基本和必要需求。大多数研究都集中在基于深度学习方法的 CNN（卷积神经网络）上，主要是用于健康和疾病人的分类。在本研究中，一种新的19层 CNN 结构被建议用于准确地识别 X射线图像中的 COVID-19。这个 CNN 结构是用于三类（肺病毒感染、正常、COVID）和四类分类（肺抑血、正常、COVID-19、肺炎）。对于这些结果和一些常用的预训练网络（如 Inception、Alexnet、ResNet50、Squeezenet 和 VGG19）进行了比较，并根据具体性、准确率、精度、敏感度和冲激矩阵来评估。实验结果表明，提出的 CNN 方法在现有发表的方法中具有优势。这种方法可以成为临床医生决策 COVID-19 的有用工具。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Unified-Neural-Architecture-for-Visual-Recognition-and-Reasoning"><a href="#Towards-A-Unified-Neural-Architecture-for-Visual-Recognition-and-Reasoning" class="headerlink" title="Towards A Unified Neural Architecture for Visual Recognition and Reasoning"></a>Towards A Unified Neural Architecture for Visual Recognition and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06386">http://arxiv.org/abs/2311.06386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Calvin Luo, Boqing Gong, Ting Chen, Chen Sun</li>
<li>for: 这篇论文主要针对视觉理解的两大柱子：认知和理解。</li>
<li>methods: 该论文提出了一种基于多任务转换器的协同架构，可以同时解决视觉认知和理解两个任务。</li>
<li>results: 研究发现，对象检测任务对视觉理解具有最大的帮助，并且该架构自动生成了对象中心的表示。此外，研究还发现了不同架构设计对视觉理解的影响。<details>
<summary>Abstract</summary>
Recognition and reasoning are two pillars of visual understanding. However, these tasks have an imbalance in focus; whereas recent advances in neural networks have shown strong empirical performance in visual recognition, there has been comparably much less success in solving visual reasoning. Intuitively, unifying these two tasks under a singular framework is desirable, as they are mutually dependent and beneficial. Motivated by the recent success of multi-task transformers for visual recognition and language understanding, we propose a unified neural architecture for visual recognition and reasoning with a generic interface (e.g., tokens) for both. Our framework enables the principled investigation of how different visual recognition tasks, datasets, and inductive biases can help enable spatiotemporal reasoning capabilities. Noticeably, we find that object detection, which requires spatial localization of individual objects, is the most beneficial recognition task for reasoning. We further demonstrate via probing that implicit object-centric representations emerge automatically inside our framework. Intriguingly, we discover that certain architectural choices such as the backbone model of the visual encoder have a significant impact on visual reasoning, but little on object detection. Given the results of our experiments, we believe that visual reasoning should be considered as a first-class citizen alongside visual recognition, as they are strongly correlated but benefit from potentially different design choices.
</details>
<details>
<summary>摘要</summary>
<<SYS>>视觉理解的两个柱子是认知和理解。然而，这两个任务在注意力方面存在偏见，而且近年来神经网络的实验性表现在视觉认知方面强大，而在视觉理解方面相对落后。可是，将这两个任务集成到一个共同框架中是有利的，因为它们是互相依赖的和有益的。鼓励 by recent success of multi-task transformers for visual recognition and language understanding, we propose a unified neural architecture for visual recognition and reasoning with a generic interface (e.g., tokens) for both. Our framework enables the principled investigation of how different visual recognition tasks, datasets, and inductive biases can help enable spatiotemporal reasoning capabilities.发现结果显示，对象检测，需要物体的空间局部化，是最有利的认知任务 для理解。我们还通过探测发现了自动内生的卷积表示。进一步的实验结果表明，certain architectural choices such as the backbone model of the visual encoder have a significant impact on visual reasoning, but little on object detection. given the results of our experiments, we believe that visual reasoning should be considered as a first-class citizen alongside visual recognition, as they are strongly correlated but benefit from potentially different design choices. Traditional Chinese translation:<<SYS>>Visual understanding 的两个柱子是识别和理解。然而，这两个任务在注意力方面存在偏见，而且近年来神经网络的实验性表现在视觉认知方面强大，而在视觉理解方面相对落后。可是，将这两个任务集成到一个共同框架中是有利的，因为它们是互相依赖的和有益的。鼓励 by recent success of multi-task transformers for visual recognition and language understanding, we propose a unified neural architecture for visual recognition and reasoning with a generic interface (e.g., tokens) for both. Our framework enables the principled investigation of how different visual recognition tasks, datasets, and inductive biases can help enable spatiotemporal reasoning capabilities.发现结果显示，对象检测，需要物体的空间局部化，是最有利的认知任务 для理解。我们还通过探测发现了自动内生的卷积表示。进一步的实验结果表明，certain architectural choices such as the backbone model of the visual encoder have a significant impact on visual reasoning, but little on object detection. given the results of our experiments, we believe that visual reasoning should be considered as a first-class citizen alongside visual recognition, as they are strongly correlated but benefit from potentially different design choices.
</details></li>
</ul>
<hr>
<h2 id="Image-Classification-using-Combination-of-Topological-Features-and-Neural-Networks"><a href="#Image-Classification-using-Combination-of-Topological-Features-and-Neural-Networks" class="headerlink" title="Image Classification using Combination of Topological Features and Neural Networks"></a>Image Classification using Combination of Topological Features and Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06375">http://arxiv.org/abs/2311.06375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariana Dória Prata Lima, Gilson Antonio Giraldi, Gastão Florêncio Miranda Junior</li>
<li>for: 本研究使用 persist homology 方法，一种在 topological data analysis (TDA) 中常用的技术，以提取数据空间中的基本 topological 特征，并将其与深度学习特征结合以进行分类任务。</li>
<li>methods: 本研究首先从复杂体系中构建了筛选，然后计算了 persistent homology 类型，并将其在筛选中的演化visualized through persistence diagram。此外，我们还应用了vectorization技术，使这些 topological 信息与机器学习算法兼容。</li>
<li>results: 我们的方法可以在 MNIST 数据集中分类多个类型的图像，并且比基eline 的结果更高。我们的分析还表明，在多类分类任务中， topological 信息可以提高神经网络的准确率，但是计算 persist homology 的计算复杂性增加。这是我们知道的第一个结合深度学习特征和 topological 特征的多类分类任务。<details>
<summary>Abstract</summary>
In this work we use the persistent homology method, a technique in topological data analysis (TDA), to extract essential topological features from the data space and combine them with deep learning features for classification tasks. In TDA, the concepts of complexes and filtration are building blocks. Firstly, a filtration is constructed from some complex. Then, persistent homology classes are computed, and their evolution along the filtration is visualized through the persistence diagram. Additionally, we applied vectorization techniques to the persistence diagram to make this topological information compatible with machine learning algorithms. This was carried out with the aim of classifying images from multiple classes in the MNIST dataset. Our approach inserts topological features into deep learning approaches composed by single and two-streams neural networks architectures based on a multi-layer perceptron (MLP) and a convolutional neral network (CNN) taylored for multi-class classification in the MNIST dataset. In our analysis, we evaluated the obtained results and compared them with the outcomes achieved through the baselines that are available in the TensorFlow library. The main conclusion is that topological information may increase neural network accuracy in multi-class classification tasks with the price of computational complexity of persistent homology calculation. Up to the best of our knowledge, it is the first work that combines deep learning features and the combination of topological features for multi-class classification tasks.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们使用 persistente homology 方法，一种 topological data analysis（TDA）中的技术，以提取数据空间中的基本 topological 特征，并将其与深度学习特征结合以进行分类任务。在 TDA 中，复杂设与筛选是建筑 Material。首先，一个筛选是从一个复杂中构造出来。然后， persistente homology 类是计算出来，并将其在筛选的演化中可见化 durch persistence 图。此外，我们还应用了vectorization技术来使这些 topological 信息与机器学习算法兼容。这是为了在 MNIST 数据集中分类图像。我们的方法把 topological 特征与单流和两流 neural network 架构（基于 multi-layer perceptron 和 convolutional neural network）结合以进行多类分类。在我们的分析中，我们评估了获得的结果，并与存在于 TensorFlow 库中的基eline 结果进行比较。结论是：topological 信息可能会增加多类分类任务中 neural network 精度，但是 persistente homology 计算的计算复杂度会增加。据我们所知，这是首次将 deep learning 特征与 topological 特征结合以进行多类分类任务。
</details></li>
</ul>
<hr>
<h2 id="Florence-2-Advancing-a-Unified-Representation-for-a-Variety-of-Vision-Tasks"><a href="#Florence-2-Advancing-a-Unified-Representation-for-a-Variety-of-Vision-Tasks" class="headerlink" title="Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks"></a>Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06242">http://arxiv.org/abs/2311.06242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan<br>for: Florence-2 is a novel vision foundation model that can perform a variety of computer vision and vision-language tasks with simple text-based instructions.methods: Florence-2 uses a sequence-to-sequence structure and large-scale, high-quality annotated data to train the model for versatile and comprehensive vision tasks.results: Florence-2 demonstrated strong zero-shot and fine-tuning capabilities, making it a competitive vision foundation model for a variety of tasks.Here is the text in Simplified Chinese:for:  florence-2 是一种 novel 的视觉基础模型，可以通过简单的文本指令来执行多种计算机视觉和视觉语言任务。methods:  florence-2 使用 sequence-to-sequence 结构和大规模、高质量的注解数据来训练模型，以执行多元和全面的视觉任务。results:  florence-2 在多种任务上表现出了强大的零配置和微调能力，使其成为计算机视觉领域的竞争力强的视觉基础模型。<details>
<summary>Abstract</summary>
We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.
</details>
<details>
<summary>摘要</summary>
我们介绍 Florence-2，一种新型视觉基础模型，具有一个统一的提示基础表示，用于多种计算机视觉和视觉语言任务。现有的大型视觉模型在转移学习方面表现出色，但它们在执行简单的指令下表现不佳，这表明它们不能处理多种空间层次和semantic粒度的复杂性。Florence-2是根据文本提示进行任务指令，并生成desirable的结果，无论是captioning、对象检测、grounding或分割。这种多任务学习设置需要大规模、高质量的注解数据。为此，我们共同开发了 FLD-5B，包括126万张图像的5.4亿次全面视觉注解，使用了迭代的自动图像注解和模型优化策略。我们采用了序列到序列结构来训练 Florence-2，以便它可以执行多种灵活和全面的视觉任务。广泛的评估表明，Florence-2是一个强大的视觉基础模型候选人，具有历史上未有的零shot和微调能力。
</details></li>
</ul>
<hr>
<h2 id="Learning-Human-Action-Recognition-Representations-Without-Real-Humans"><a href="#Learning-Human-Action-Recognition-Representations-Without-Real-Humans" class="headerlink" title="Learning Human Action Recognition Representations Without Real Humans"></a>Learning Human Action Recognition Representations Without Real Humans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06231">http://arxiv.org/abs/2311.06231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/howardzh01/ppma">https://github.com/howardzh01/ppma</a></li>
<li>paper_authors: Howard Zhong, Samarth Mishra, Donghyun Kim, SouYoung Jin, Rameswar Panda, Hilde Kuehne, Leonid Karlinsky, Venkatesh Saligrama, Aude Oliva, Rogerio Feris<br>for: 这个论文的目的是研究是否可以使用不包含真实人类图像的数据进行人体动作识别模型的预训练。methods: 这篇论文使用了一个新的预训练策略，即 Privacy-Preserving MAE-Align，将真实人类图像去除后的数据和 sintetic数据组合使用，以提高预训练模型的表现。results: 该论文的实验结果表明，使用 Privacy-Preserving MAE-Align 策略可以提高预训练模型的表现，并将人体动作识别模型的表现与无人体动作识别模型的表现进行比较。此外，该论文还提供了一个可用于复现研究的开源 benchmark。<details>
<summary>Abstract</summary>
Pre-training on massive video datasets has become essential to achieve high action recognition performance on smaller downstream datasets. However, most large-scale video datasets contain images of people and hence are accompanied with issues related to privacy, ethics, and data protection, often preventing them from being publicly shared for reproducible research. Existing work has attempted to alleviate these problems by blurring faces, downsampling videos, or training on synthetic data. On the other hand, analysis on the transferability of privacy-preserving pre-trained models to downstream tasks has been limited. In this work, we study this problem by first asking the question: can we pre-train models for human action recognition with data that does not include real humans? To this end, we present, for the first time, a benchmark that leverages real-world videos with humans removed and synthetic data containing virtual humans to pre-train a model. We then evaluate the transferability of the representation learned on this data to a diverse set of downstream action recognition benchmarks. Furthermore, we propose a novel pre-training strategy, called Privacy-Preserving MAE-Align, to effectively combine synthetic data and human-removed real data. Our approach outperforms previous baselines by up to 5% and closes the performance gap between human and no-human action recognition representations on downstream tasks, for both linear probing and fine-tuning. Our benchmark, code, and models are available at https://github.com/howardzh01/PPMA .
</details>
<details>
<summary>摘要</summary>
大规模视频数据的预训练已成为实现高效人体动作识别的必备条件。然而，大多数大规模视频数据包含人脸图像，因此会附带隐私、伦理和数据保护等问题，常常使得这些数据无法公开分享，对于可重复的研究。现有的工作尝试解决这些问题，通过让人脸模糊、视频下采样或使用生成的数据进行训练。然而，对于隐私保持的模型转移性的分析却受到限制。在这项工作中，我们提出了以下问题：可以我们在不包含真实人类数据的情况下进行人体动作识别预训练吗？为此，我们提供了一个新的数据集，其中包含了人类去除后的真实视频和虚拟人类生成的数据，用于预训练模型。然后，我们评估了这种数据的表示学习到下游动作识别任务中的转移性，并提出了一种新的预训练策略，即隐私保持MAE-Align。我们的方法比前一代基eline上提高了5%，并将人类动作识别和无人类动作识别表示之间的性能差距降到最小。我们的数据集、代码和模型可以在https://github.com/howardzh01/PPMA上下载。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Earth-Observation-Use-cases-from-cloud-removal-to-urban-change-detection"><a href="#Diffusion-Models-for-Earth-Observation-Use-cases-from-cloud-removal-to-urban-change-detection" class="headerlink" title="Diffusion Models for Earth Observation Use-cases: from cloud removal to urban change detection"></a>Diffusion Models for Earth Observation Use-cases: from cloud removal to urban change detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06222">http://arxiv.org/abs/2311.06222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/furio1999/EO_Diffusion">https://github.com/furio1999/EO_Diffusion</a></li>
<li>paper_authors: Fulvio Sanguigni, Mikolaj Czerkawski, Lorenzo Papa, Irene Amerini, Bertrand Le Saux</li>
<li>for: 这篇论文旨在展示 diffusion 模型对于卫星影像数据的应用，并提出了三个实际应用案例。</li>
<li>methods: 这篇论文使用了 diffusion 模型，包括云除和填充、数据集生成 для变化检测任务、以及城市规划。</li>
<li>results: 这篇论文获得了云除和填充、数据集生成、城市规划等三个实际应用案例中的良好结果。<details>
<summary>Abstract</summary>
The advancements in the state of the art of generative Artificial Intelligence (AI) brought by diffusion models can be highly beneficial in novel contexts involving Earth observation data. After introducing this new family of generative models, this work proposes and analyses three use cases which demonstrate the potential of diffusion-based approaches for satellite image data. Namely, we tackle cloud removal and inpainting, dataset generation for change-detection tasks, and urban replanning.
</details>
<details>
<summary>摘要</summary>
“现代生成人工智能（AI）技术的进步，即扩散模型，在地球观测数据中可以获得非常有利的效果。本研究首次介绍了这种新的生成模型家族，然后提出和分析了三个使用场景，即云除和填充、数据集生成 для变化检测任务、和城市规划。”Here's a breakdown of the translation:* 现代生成人工智能 (AI) 技术 (技术) - This phrase is translated as "现代生成人工智能（AI）技术" in Simplified Chinese.* 的进步 (进步) - This word is translated as "的进步" in Simplified Chinese.* 即扩散模型 (扩散模型) - This phrase is translated as "即扩散模型" in Simplified Chinese.* 在地球观测数据中 (在地球观测数据中) - This phrase is translated as "在地球观测数据中" in Simplified Chinese.* 可以获得非常有利的效果 (可以获得非常有利的效果) - This phrase is translated as "可以获得非常有利的效果" in Simplified Chinese.* 本研究 (本研究) - This word is translated as "本研究" in Simplified Chinese.* 首次介绍了 (首次介绍了) - This phrase is translated as "首次介绍了" in Simplified Chinese.* 这种新的生成模型家族 (这种新的生成模型家族) - This phrase is translated as "这种新的生成模型家族" in Simplified Chinese.* 然后 (然后) - This word is translated as "然后" in Simplified Chinese.* 提出和分析了三个使用场景 (提出和分析了三个使用场景) - This phrase is translated as "提出和分析了三个使用场景" in Simplified Chinese.* 即云除和填充 (即云除和填充) - This phrase is translated as "即云除和填充" in Simplified Chinese.* 数据集生成 для变化检测任务 (数据集生成 для变化检测任务) - This phrase is translated as "数据集生成 для变化检测任务" in Simplified Chinese.* 和城市规划 (和城市规划) - This phrase is translated as "和城市规划" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Semantic-aware-Video-Representation-for-Few-shot-Action-Recognition"><a href="#Semantic-aware-Video-Representation-for-Few-shot-Action-Recognition" class="headerlink" title="Semantic-aware Video Representation for Few-shot Action Recognition"></a>Semantic-aware Video Representation for Few-shot Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06218">http://arxiv.org/abs/2311.06218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yutao Tang, Benjamin Bejar, Rene Vidal</li>
<li>for: 提高ew-shot动作识别性能，解决现有方法依赖2D帧级别表示，缺乏有效的文本 semantics incorporation和简单的类别分类方法等问题。</li>
<li>methods: 提出了一种简单 yet effective的Semantic-Aware Few-Shot Action Recognition（SAFSAR）模型，通过直接使用3D特征提取器和有效的特征融合方案，以及简单的高度相似性分类方法，实现了更好的性能而无需额外 комponents for temporal modeling或复杂的距离函数。</li>
<li>results: 在五个具有不同设定的ew-shot动作识别benchmark上，经验表明，提出的SAFSAR模型可以显著提高状态 искусственный的性能。<details>
<summary>Abstract</summary>
Recent work on action recognition leverages 3D features and textual information to achieve state-of-the-art performance. However, most of the current few-shot action recognition methods still rely on 2D frame-level representations, often require additional components to model temporal relations, and employ complex distance functions to achieve accurate alignment of these representations. In addition, existing methods struggle to effectively integrate textual semantics, some resorting to concatenation or addition of textual and visual features, and some using text merely as an additional supervision without truly achieving feature fusion and information transfer from different modalities. In this work, we propose a simple yet effective Semantic-Aware Few-Shot Action Recognition (SAFSAR) model to address these issues. We show that directly leveraging a 3D feature extractor combined with an effective feature-fusion scheme, and a simple cosine similarity for classification can yield better performance without the need of extra components for temporal modeling or complex distance functions. We introduce an innovative scheme to encode the textual semantics into the video representation which adaptively fuses features from text and video, and encourages the visual encoder to extract more semantically consistent features. In this scheme, SAFSAR achieves alignment and fusion in a compact way. Experiments on five challenging few-shot action recognition benchmarks under various settings demonstrate that the proposed SAFSAR model significantly improves the state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a simple yet effective Semantic-Aware Few-Shot Action Recognition (SAFSAR) model to address these issues. Our approach leverages a 3D feature extractor combined with an effective feature-fusion scheme and a simple cosine similarity for classification, which improves performance without the need for extra components for temporal modeling or complex distance functions.We also introduce an innovative scheme to encode textual semantics into the video representation, which adaptively fuses features from text and video and encourages the visual encoder to extract more semantically consistent features. This scheme allows for compact and effective alignment and fusion of textual and visual information.Experiments on five challenging few-shot action recognition benchmarks under various settings demonstrate that the proposed SAFSAR model significantly improves the state-of-the-art performance.
</details></li>
</ul>
<hr>
<h2 id="Instant3D-Fast-Text-to-3D-with-Sparse-View-Generation-and-Large-Reconstruction-Model"><a href="#Instant3D-Fast-Text-to-3D-with-Sparse-View-Generation-and-Large-Reconstruction-Model" class="headerlink" title="Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model"></a>Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06214">http://arxiv.org/abs/2311.06214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, Sai Bi</li>
<li>for: This paper aims to generate high-quality and diverse 3D assets from text prompts in a feed-forward manner.</li>
<li>methods: The proposed method Instant3D uses a two-stage paradigm, which first generates a sparse set of four structured and consistent views from text in one shot with a fine-tuned 2D text-to-image diffusion model, and then directly regresses the NeRF from the generated images with a novel transformer-based sparse-view reconstructor.</li>
<li>results: The method can generate high-quality, diverse and Janus-free 3D assets within 20 seconds, which is two orders of magnitude faster than previous optimization-based methods that can take 1 to 10 hours.<details>
<summary>Abstract</summary>
Text-to-3D with diffusion models have achieved remarkable progress in recent years. However, existing methods either rely on score distillation-based optimization which suffer from slow inference, low diversity and Janus problems, or are feed-forward methods that generate low quality results due to the scarcity of 3D training data. In this paper, we propose Instant3D, a novel method that generates high-quality and diverse 3D assets from text prompts in a feed-forward manner. We adopt a two-stage paradigm, which first generates a sparse set of four structured and consistent views from text in one shot with a fine-tuned 2D text-to-image diffusion model, and then directly regresses the NeRF from the generated images with a novel transformer-based sparse-view reconstructor. Through extensive experiments, we demonstrate that our method can generate high-quality, diverse and Janus-free 3D assets within 20 seconds, which is two order of magnitude faster than previous optimization-based methods that can take 1 to 10 hours. Our project webpage: https://jiahao.ai/instant3d/.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose Instant3D, a novel method that generates high-quality and diverse 3D assets from text prompts in a feed-forward manner. We adopt a two-stage paradigm:1. First, we generate a sparse set of four structured and consistent views from text in one shot with a fine-tuned 2D text-to-image diffusion model.2. Then, we directly regress the NeRF from the generated images with a novel transformer-based sparse-view reconstructor.Through extensive experiments, we demonstrate that our method can generate high-quality, diverse, and Janus-free 3D assets within 20 seconds, which is two orders of magnitude faster than previous optimization-based methods that can take 1 to 10 hours. Our project webpage is <https://jiahao.ai/instant3d/>.
</details></li>
</ul>
<hr>
<h2 id="ASSIST-Interactive-Scene-Nodes-for-Scalable-and-Realistic-Indoor-Simulation"><a href="#ASSIST-Interactive-Scene-Nodes-for-Scalable-and-Realistic-Indoor-Simulation" class="headerlink" title="ASSIST: Interactive Scene Nodes for Scalable and Realistic Indoor Simulation"></a>ASSIST: Interactive Scene Nodes for Scalable and Realistic Indoor Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06211">http://arxiv.org/abs/2311.06211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhide Zhong, Jiakai Cao, Songen Gu, Sirui Xie, Weibo Gao, Liyi Luo, Zike Yan, Hao Zhao, Guyue Zhou</li>
<li>for: 这篇论文旨在提出一种基于神经网络的物体尺度场，用于实现复杂的物体和场景的真实化和组合渲染。</li>
<li>methods: 该方法使用一种新的场景节点数据结构，它将每个物体的信息存储在一起，以便在线交互和跨场景设定中进行交互。该结构还包括一个可微 differentiable神经网络、相关的 bounding box 和semantic feature，以便通过鼠标&#x2F;键盘控制或语言指令进行简单的交互。</li>
<li>results: 实验表明，该方法可以实现可扩展的真实化和组合渲染，并生成三维彩色图像、深度图像和精确的分割mask。<details>
<summary>Abstract</summary>
We present ASSIST, an object-wise neural radiance field as a panoptic representation for compositional and realistic simulation. Central to our approach is a novel scene node data structure that stores the information of each object in a unified fashion, allowing online interaction in both intra- and cross-scene settings. By incorporating a differentiable neural network along with the associated bounding box and semantic features, the proposed structure guarantees user-friendly interaction on independent objects to scale up novel view simulation. Objects in the scene can be queried, added, duplicated, deleted, transformed, or swapped simply through mouse/keyboard controls or language instructions. Experiments demonstrate the efficacy of the proposed method, where scaled realistic simulation can be achieved through interactive editing and compositional rendering, with color images, depth images, and panoptic segmentation masks generated in a 3D consistent manner.
</details>
<details>
<summary>摘要</summary>
我们提出了ASSIST，一种对象级别神经辐射场，用于实现组合和实际的 simulate 作业。我们的方法的核心是一种新的场景节点数据结构，可以同时存储每个对象的信息，以便在线上交互和跨场景设置。通过结合可导式神经网络和相关的 bounding box 和semantic feature，我们的结构确保了用户友好的交互，可以通过鼠标/键盘控制或语言指令来查询、添加、复制、删除、转换或换位对象。实验表明，我们的方法可以实现协助编辑和组合渲染，并生成3D保持一致的颜色图像、深度图像和panographic分割mask。
</details></li>
</ul>
<hr>
<h2 id="An-Automated-Pipeline-for-Tumour-Infiltrating-Lymphocyte-Scoring-in-Breast-Cancer"><a href="#An-Automated-Pipeline-for-Tumour-Infiltrating-Lymphocyte-Scoring-in-Breast-Cancer" class="headerlink" title="An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in Breast Cancer"></a>An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in Breast Cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06185">http://arxiv.org/abs/2311.06185</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adamshephard/tiager">https://github.com/adamshephard/tiager</a></li>
<li>paper_authors: Adam J Shephard, Mostafa Jahanifar, Ruoyu Wang, Muhammad Dawood, Simon Graham, Kastytis Sidlauskas, Syed Ali Khurram, Nasir M Rajpoot, Shan E Ahmed Raza</li>
<li>For: 本研究使用深度学习算法对 breast cancer 整幕影像进行 TILs 分数计算，以提高诊断和预后评估。* Methods: 我们的方法首先分别分类 tumour 和 stroma 区域，然后在 tumour-associated stroma 中检测 TILs，并生成 TILs 分数。我们的方法基于 Efficient-UNet 架构，并且具有 state-of-the-art 的性能在 tumour&#x2F;stroma 区域分 segmentation 和 TILs 检测中。* Results: 我们的研究表明，我们的自动 TILs 分数系统可以准确预测 breast cancer 患者的 survival 结果，并且与 Pathologist 的评估结果相符。<details>
<summary>Abstract</summary>
Tumour-infiltrating lymphocytes (TILs) are considered as a valuable prognostic markers in both triple-negative and human epidermal growth factor receptor 2 (HER2) breast cancer. In this study, we introduce an innovative deep learning pipeline based on the Efficient-UNet architecture to compute a TILs score for breast cancer whole slide images. Our pipeline first segments tumour-stroma regions and generates a tumour bulk mask. Subsequently, it detects TILs within the tumour-associated stroma, generating a TILs score by closely mirroring the pathologist's workflow. Our method exhibits state-of-the-art performance in segmenting tumour/stroma areas and TILs detection, as demonstrated by internal cross-validation on the TiGER Challenge training dataset and evaluation on the final leaderboards. Additionally, our TILs score proves competitive in predicting survival outcomes within the same challenge, underscoring the clinical relevance and potential of our automated TILs scoring system as a breast cancer prognostic tool.
</details>
<details>
<summary>摘要</summary>
肿瘤浸泡免疫细胞（TILs）在三重阴性和人顺体外生长因子受体2（HER2）乳腺癌中被视为有价值的诊断标志。本研究提出了一种创新的深度学习管道，基于Efficient-UNet架构，计算乳腺癌整个染色体影像中TILs分数。我们的管道首先分 segment tumor-stroma区域，并生成肿瘤涂抹mask。然后，它检测TILs在肿瘤相关的Connective tissue中，生成TILs分数，与病理学家的工作流程几乎相同。我们的方法在分 segment tumor/stroma区域和TILs检测方面表现出了状态之arte的表现，经过内部交叉验证在TiGER Challenge训练数据集上，并在最终的排名中进行了评估。此外，我们的TILs分数能够预测乳腺癌存活结果，这 highlights the clinical relevance and potential of our automated TILs scoring system as a breast cancer prognostic tool。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Report-Generation-for-Histopathology-images-using-pre-trained-Vision-Transformers"><a href="#Automatic-Report-Generation-for-Histopathology-images-using-pre-trained-Vision-Transformers" class="headerlink" title="Automatic Report Generation for Histopathology images using pre-trained Vision Transformers"></a>Automatic Report Generation for Histopathology images using pre-trained Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06176">http://arxiv.org/abs/2311.06176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav Sengupta, Donald E. Brown</li>
<li>for: 这个研究的目的是为了自动生成医学影像报告。</li>
<li>methods: 这个研究使用了现有的预训练的感知 трансформа器，在一个two-step过程中，首先使用它对4096x4096大小的整个数组图像（Whole Slide Image，WSI）进行编码，然后使用它作为编码器和LSTM复合器进行报告生成。</li>
<li>results: 这个研究获得了一个不错的性能和可移植性的报告生成机制，可以考虑整个高分辨率图像，而不只是patches。此外，这个研究还使用了现有的强大预训练的层次感知 transformer，并证明其在零损失分类以及报告生成中的有用性。<details>
<summary>Abstract</summary>
Deep learning for histopathology has been successfully used for disease classification, image segmentation and more. However, combining image and text modalities using current state-of-the-art methods has been a challenge due to the high resolution of histopathology images. Automatic report generation for histopathology images is one such challenge. In this work, we show that using an existing pre-trained Vision Transformer in a two-step process of first using it to encode 4096x4096 sized patches of the Whole Slide Image (WSI) and then using it as the encoder and an LSTM decoder for report generation, we can build a fairly performant and portable report generation mechanism that takes into account the whole of the high resolution image, instead of just the patches. We are also able to use representations from an existing powerful pre-trained hierarchical vision transformer and show its usefulness in not just zero shot classification but also for report generation.
</details>
<details>
<summary>摘要</summary>
深度学习在 Histopathology 中已经得到了成功，用于疾病分类、图像分割和更多的应用。然而，将图像和文本模式结合使用现有的状态太的方法是一个挑战，主要是因为 histopathology 图像的高分辨率。自动生成 histopathology 图像的报告是一个这样的挑战。在这项工作中，我们表明了使用现有的预训练 Vision Transformer 进行两步处理：首先，将 Whole Slide Image (WSI) 的 4096x4096 大小的patches 使用 Vision Transformer 进行编码，然后使用 Vision Transformer 作为编码器和 LSTM 解码器进行报告生成。我们发现，这种方法可以建立一个性能较高且可移植的报告生成机制，可以考虑整个高分辨率图像，而不仅仅是patches。此外，我们还可以使用现有的强大预训练 hierarchical Vision Transformer 的表示，并证明其在零shot分类以及报告生成中的用用。
</details></li>
</ul>
<hr>
<h2 id="Deep-Fast-Vision-A-Python-Library-for-Accelerated-Deep-Transfer-Learning-Vision-Prototyping"><a href="#Deep-Fast-Vision-A-Python-Library-for-Accelerated-Deep-Transfer-Learning-Vision-Prototyping" class="headerlink" title="Deep Fast Vision: A Python Library for Accelerated Deep Transfer Learning Vision Prototyping"></a>Deep Fast Vision: A Python Library for Accelerated Deep Transfer Learning Vision Prototyping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06169">http://arxiv.org/abs/2311.06169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fabprezja/deep-fast-vision">https://github.com/fabprezja/deep-fast-vision</a></li>
<li>paper_authors: Fabi Prezja</li>
<li>for: 提高深度学习视觉领域的易用性和普及率，帮助非专家用户快速入门深度学习。</li>
<li>methods: 使用Python库实现简单化深度学习过程，提供易于理解的嵌入式字典定义，使得非专家用户可以轻松获得结果。</li>
<li>results: 提供一个简单、扩展性强的深度学习工具，帮助bridge Complex deep learning frameworks和各种用户需求，推动深度学习的普及和应用。<details>
<summary>Abstract</summary>
Deep learning-based vision is characterized by intricate frameworks that often necessitate a profound understanding, presenting a barrier to newcomers and limiting broad adoption. With many researchers grappling with the constraints of smaller datasets, there's a pronounced reliance on pre-trained neural networks, especially for tasks such as image classification. This reliance is further intensified in niche imaging areas where obtaining vast datasets is challenging. Despite the widespread use of transfer learning as a remedy to the small dataset dilemma, a conspicuous absence of tailored auto-ML solutions persists. Addressing these challenges is "Deep Fast Vision", a python library that streamlines the deep learning process. This tool offers a user-friendly experience, enabling results through a simple nested dictionary definition, helping to democratize deep learning for non-experts. Designed for simplicity and scalability, Deep Fast Vision appears as a bridge, connecting the complexities of existing deep learning frameworks with the needs of a diverse user base.
</details>
<details>
<summary>摘要</summary>
深度学习视觉 caracteriza por frameworks intrincados que a menudo requieren una comprensión profunda, lo que puede representar una barrera para los principiantes y limitaciones en la adopción amplia. Con muchos investigadores lidiando con los límites de conjuntos de datos más pequeños, hay una reliance pronunciada en redes neuronales preentrenzadas, especialmente para tareas como clasificación de imágenes. Esta reliance se vuelve a intensificar en áreas de imagen nicho donde obtener conjuntos de datos vastos es desafiante. A pesar del uso amplio de aprendizaje transferido como una solución a la dilema de conjuntos de datos pequeños, una ausencia conspicua de soluciones de Auto-ML personalizadas persiste. Para abordar estos desafíos, se presenta "Deep Fast Vision", una biblioteca de Python que simplifica el proceso de aprendizaje profundo. Esta herramienta ofrece una experiencia de usuario amigable, permitiendo resultados a través de una definición de diccionario nestado simple, ayudando a democratizar el aprendizaje profundo para no expertos. Diseñada para la simplicidad y escalabilidad, Deep Fast Vision se presenta como un puente que conecta las complejidades de los marcos existentes de aprendizaje profundo con las necesidades de una base de usuarios diversa.
</details></li>
</ul>
<hr>
<h2 id="An-Evaluation-of-Forensic-Facial-Recognition"><a href="#An-Evaluation-of-Forensic-Facial-Recognition" class="headerlink" title="An Evaluation of Forensic Facial Recognition"></a>An Evaluation of Forensic Facial Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06145">http://arxiv.org/abs/2311.06145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DanielDdungu/Real-Time-Face-Recognition">https://github.com/DanielDdungu/Real-Time-Face-Recognition</a></li>
<li>paper_authors: Justin Norman, Shruti Agarwal, Hany Farid</li>
<li>for: 本研究旨在评估 faces recognition 系统在真实世界情况下的表现，特别是在低分辨率、低质量、部分遮挡的图像对标准面部数据库进行比较。</li>
<li>methods: 本研究使用了大量的synthetic facial dataset和控制 facial forensic lineup，以模拟真实世界中的面部识别情况。两种流行的神经网络基于的识别系统进行了评估。</li>
<li>results: 研究发现， previously reported face recognition accuracy 高于 95% 下降到了 65% 以下，表明面部识别系统在这种更加复杂的刑事enario中表现不佳。<details>
<summary>Abstract</summary>
Recent advances in machine learning and computer vision have led to reported facial recognition accuracies surpassing human performance. We question if these systems will translate to real-world forensic scenarios in which a potentially low-resolution, low-quality, partially-occluded image is compared against a standard facial database. We describe the construction of a large-scale synthetic facial dataset along with a controlled facial forensic lineup, the combination of which allows for a controlled evaluation of facial recognition under a range of real-world conditions. Using this synthetic dataset, and a popular dataset of real faces, we evaluate the accuracy of two popular neural-based recognition systems. We find that previously reported face recognition accuracies of more than 95% drop to as low as 65% in this more challenging forensic scenario.
</details>
<details>
<summary>摘要</summary>
最近的机器学习和计算机视觉技术发展，已经使facial recognition系统的准确率超过人类表现。我们问题是这些系统在真实世界冤家enario中是否能够维持高度的准确率，例如 comparing a low-resolution, low-quality, partially-occluded image against a standard facial database。我们描述了一个大规模的 sintetic facial dataset的构建，以及一个控制的 facial forensic lineup，这两个组合允许我们在不同的真实世界条件下进行控制的评估。使用这个 sintetic dataset，以及一个流行的实际面孔数据集，我们评估了两个流行的神经网络基于的认识系统的准确率。我们发现，以前报道的面recognition准确率高于95%下降到了65%的这样的更加挑战的冤家enario中。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-Across-Decentralized-and-Unshared-Archives-for-Remote-Sensing-Image-Classification"><a href="#Federated-Learning-Across-Decentralized-and-Unshared-Archives-for-Remote-Sensing-Image-Classification" class="headerlink" title="Federated Learning Across Decentralized and Unshared Archives for Remote Sensing Image Classification"></a>Federated Learning Across Decentralized and Unshared Archives for Remote Sensing Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06141">http://arxiv.org/abs/2311.06141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Barış Büyüktaş, Gencer Sumbul, Begüm Demir</li>
<li>for: 这paper aimsto explore the potential of federated learning (FL) in remote sensing (RS) and compare state-of-the-art FL algorithms for image classification tasks.</li>
<li>methods: 本paper使用了多种state-of-the-art FL algorithms, including federated averaging (FedAvg), federated transfer learning (FedTL), and federated meta-learning (FedMeta). The authors also conducted a theoretical comparison of the algorithms based on their local training complexity, aggregation complexity, learning efficiency, communication cost, and scalability.</li>
<li>results: 经过实验研究， authors found that FedAvg and FedTL outperformed other algorithms under different decentralization scenarios. Additionally, the authors derived a guideline for selecting suitable FL algorithms in RS based on the characteristics of the decentralized data.<details>
<summary>Abstract</summary>
Federated learning (FL) enables the collaboration of multiple deep learning models to learn from decentralized data archives (i.e., clients) without accessing data on clients. Although FL offers ample opportunities in knowledge discovery from distributed image archives, it is seldom considered in remote sensing (RS). In this paper, as a first time in RS, we present a comparative study of state-of-the-art FL algorithms. To this end, we initially provide a systematic review of the FL algorithms presented in the computer vision community for image classification problems, and select several state-of-the-art FL algorithms based on their effectiveness with respect to training data heterogeneity across clients (known as non-IID data). After presenting an extensive overview of the selected algorithms, a theoretical comparison of the algorithms is conducted based on their: 1) local training complexity; 2) aggregation complexity; 3) learning efficiency; 4) communication cost; and 5) scalability in terms of number of clients. As the classification task, we consider multi-label classification (MLC) problem since RS images typically consist of multiple classes, and thus can simultaneously be associated with multi-labels. After the theoretical comparison, experimental analyses are presented to compare them under different decentralization scenarios in terms of MLC performance. Based on our comprehensive analyses, we finally derive a guideline for selecting suitable FL algorithms in RS. The code of this work will be publicly available at https://git.tu-berlin.de/rsim/FL-RS.
</details>
<details>
<summary>摘要</summary>
Federated 学习（FL）允许多个深度学习模型在分布式数据存储（即客户端）上学习而不需要访问客户端上的数据。尽管FL在分布式图像存储中提供了丰富的机会，它在远程感知（RS）领域几乎未得到考虑。在这篇论文中，我们为RS领域的首次应用FL算法进行了比较研究。为此，我们首先提供了计算机视觉社区中关于图像分类问题的FL算法的系统性评论，并选择了一些在客户端数据不同性（即非Identical和不同）上显示出效果的FL算法。接着，我们对选择的算法进行了理论性比较，包括：1）本地训练复杂度；2）聚合复杂度；3）学习效率；4）通信成本；和5）可扩展性。作为分类任务，我们考虑了多标签分类（MLC）问题，因为RS图像通常包含多个类别，可以同时被关联到多个标签。在理论比较后，我们进行了实验分析，对不同的分布式场景进行了MLC性能的比较。根据我们的全面分析，我们最终提出了RS中FL算法选择的指南。代码将在https://git.tu-berlin.de/rsim/FL-RS上公开。
</details></li>
</ul>
<hr>
<h2 id="MonoProb-Self-Supervised-Monocular-Depth-Estimation-with-Interpretable-Uncertainty"><a href="#MonoProb-Self-Supervised-Monocular-Depth-Estimation-with-Interpretable-Uncertainty" class="headerlink" title="MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty"></a>MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06137">http://arxiv.org/abs/2311.06137</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cea-list/monoprob">https://github.com/cea-list/monoprob</a></li>
<li>paper_authors: Rémi Marsal, Florian Chabot, Angelique Loesch, William Grolleau, Hichem Sahbi</li>
<li>for: 这 paper written for 自动驾驶汽车等应用环境分析。</li>
<li>methods: 该 paper 使用了一种新的无监督单目深度估计方法，即 MonoProb，可以在单一前进推理中提供可解释的uncertainty，表示网络对深度预测的预期错误。</li>
<li>results: 该 paper 的实验结果显示，MonoProb 可以提高depth和uncertainty的性能，并且可以在不增加推理时间的情况下提供depth和uncertainty的测量。<details>
<summary>Abstract</summary>
Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb
</details>
<details>
<summary>摘要</summary>
自我监督的单目深度估算方法目标在critical应用中，如自动驾驶车辆环境分析。为了避免这些方法的潜在缺陷，对depth估算的预测 confidence quantification是关键的，以帮助基于depth估算的决策系统。在这篇论文中，我们提出了MonoProb，一种新的无监督单目深度估算方法，该方法返回可解释的uncertainty，即网络的depth预测错误预期值。我们将单目或stereo/structure-from-motion paradigms用于无监督单目深度模型的训练转换为一个概率问题。在单个前向传播推理过程中，该模型提供了depth预测和其 confidence的度量，不会增加推理时间。我们然后通过一种新的自我混合损失来提高depth和uncertainty的性能，其中学生被监督于一个 pseudo 真实数据，该数据是一个depth输出的概率分布。为了衡量我们的模型性能，我们设计了新的metric，与传统metric不同，可以量化uncertainty预测的绝对性能。我们的实验表明，我们的方法在标准深度和uncertainty metric以及我们定制的metric上具有显著提高。References:* GitHub: <https://github.com/CEA-LIST/MonoProb>
</details></li>
</ul>
<hr>
<h2 id="Fight-Fire-with-Fire-Combating-Adversarial-Patch-Attacks-using-Pattern-randomized-Defensive-Patches"><a href="#Fight-Fire-with-Fire-Combating-Adversarial-Patch-Attacks-using-Pattern-randomized-Defensive-Patches" class="headerlink" title="Fight Fire with Fire: Combating Adversarial Patch Attacks using Pattern-randomized Defensive Patches"></a>Fight Fire with Fire: Combating Adversarial Patch Attacks using Pattern-randomized Defensive Patches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06122">http://arxiv.org/abs/2311.06122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianan Feng, Jiachun Li, Changqing Miao, Jianjun Huang, Wei You, Wenchang Shi, Bin Liang</li>
<li>for: 防御 adversarial patch 攻击</li>
<li>methods: 使用活动防御策略，插入 canary 和 woodpecker 两种防御补丁，不改变目标模型</li>
<li>results: canary 和 woodpecker 实现高性能，能够抗击未知攻击方法，时间开销有限；对防御意识攻击也具有 suficient 鲁棒性<details>
<summary>Abstract</summary>
Object detection has found extensive applications in various tasks, but it is also susceptible to adversarial patch attacks. Existing defense methods often necessitate modifications to the target model or result in unacceptable time overhead. In this paper, we adopt a counterattack approach, following the principle of "fight fire with fire," and propose a novel and general methodology for defending adversarial attacks. We utilize an active defense strategy by injecting two types of defensive patches, canary and woodpecker, into the input to proactively probe or weaken potential adversarial patches without altering the target model. Moreover, inspired by randomization techniques employed in software security, we employ randomized canary and woodpecker injection patterns to defend against defense-aware attacks. The effectiveness and practicality of the proposed method are demonstrated through comprehensive experiments. The results illustrate that canary and woodpecker achieve high performance, even when confronted with unknown attack methods, while incurring limited time overhead. Furthermore, our method also exhibits sufficient robustness against defense-aware attacks, as evidenced by adaptive attack experiments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-the-Efficacy-of-Base-Data-Augmentation-Methods-in-Deep-Learning-Based-Radiograph-Classification-of-Knee-Joint-Osteoarthritis"><a href="#Exploring-the-Efficacy-of-Base-Data-Augmentation-Methods-in-Deep-Learning-Based-Radiograph-Classification-of-Knee-Joint-Osteoarthritis" class="headerlink" title="Exploring the Efficacy of Base Data Augmentation Methods in Deep Learning-Based Radiograph Classification of Knee Joint Osteoarthritis"></a>Exploring the Efficacy of Base Data Augmentation Methods in Deep Learning-Based Radiograph Classification of Knee Joint Osteoarthritis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06118">http://arxiv.org/abs/2311.06118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabi Prezja, Leevi Annala, Sampsa Kiiskinen, Timo Ojala</li>
<li>for: 该研究旨在检测关节骨块炎（KOA），一种全球范围内导致残疾的主要原因。</li>
<li>methods: 该研究使用深度学习方法进行KOA诊断，并利用数据增强技术来增加数据多样性。</li>
<li>results: 研究发现，使用恶意增强技术可以提高KOA分类模型的性能，但其他常用的增强技术则常下降性能。研究还发现，存在可能的混淆区域在图像中，这使得模型可以准确地分类KL0和KL4等级，而不需要考虑关节部分。这一观察表明了模型可能利用不相关的特征来进行分类。<details>
<summary>Abstract</summary>
Diagnosing knee joint osteoarthritis (KOA), a major cause of disability worldwide, is challenging due to subtle radiographic indicators and the varied progression of the disease. Using deep learning for KOA diagnosis requires broad, comprehensive datasets. However, obtaining these datasets poses significant challenges due to patient privacy concerns and data collection restrictions. Additive data augmentation, which enhances data variability, emerges as a promising solution. Yet, it's unclear which augmentation techniques are most effective for KOA. This study explored various data augmentation methods, including adversarial augmentations, and their impact on KOA classification model performance. While some techniques improved performance, others commonly used underperformed. We identified potential confounding regions within the images using adversarial augmentation. This was evidenced by our models' ability to classify KL0 and KL4 grades accurately, with the knee joint omitted. This observation suggested a model bias, which might leverage unrelated features for classification currently present in radiographs. Interestingly, removing the knee joint also led to an unexpected improvement in KL1 classification accuracy. To better visualize these paradoxical effects, we employed Grad-CAM, highlighting the associated regions. Our study underscores the need for careful technique selection for improved model performance and identifying and managing potential confounding regions in radiographic KOA deep learning.
</details>
<details>
<summary>摘要</summary>
诊断膝关节骨关节炎（KOA）具有挑战性，主要原因是诊断标准化不够，疾病进程变化多样化。使用深度学习诊断KOA需要广泛、全面的数据集。然而，获得这些数据集具有难题，主要是因为患者隐私问题和数据收集限制。添加数据增强技术可以解决这个问题。然而，不同的增强技术对KOA分类模型的影响是不确定的。本研究探讨了不同的数据增强方法，包括对抗增强技术，对KOA分类模型的影响。一些技术提高了表现，而其他们则常常表现不佳。我们使用对抗增强技术 indentified可能的混合区域内 immagini，这是通过我们的模型可以准确地分类KL0和KL4等级，而不需要膝关节。这一观察表明了我们的模型可能受到了不相关的特征的影响，从而导致模型偏好。意外地，去掉膝关节也导致了KL1等级的准确率提高。为了更好地visualize这些paraoxical效应，我们使用Grad-CAM，显示关联区域。本研究表明，选择合适的技术和识别和管理可能的混合区域在诊断KOA的深度学习中是非常重要的。
</details></li>
</ul>
<hr>
<h2 id="Dual-input-stream-transformer-for-eye-tracking-line-assignment"><a href="#Dual-input-stream-transformer-for-eye-tracking-line-assignment" class="headerlink" title="Dual input stream transformer for eye-tracking line assignment"></a>Dual input stream transformer for eye-tracking line assignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06095">http://arxiv.org/abs/2311.06095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas M. Mercier, Marcin Budka, Martin R. Vasilev, Julie A. Kirkby, Bernhard Angele, Timothy J. Slattery</li>
<li>for: 本研究的目的是解决阅读数据中的眩晕问题，通过分配眩晕点到文本行中的最佳线程。</li>
<li>methods: 本研究提出了一种基于Transformer的双输入流Transformer（DIST）模型，通过对多个实例的DIST模型进行ensemble学习，以提高眩晕点分配的准确率。</li>
<li>results: 对于九种经典方法的比较，DIST模型在九个多样化的数据集上达到了98.5%的平均准确率，显示DIST模型的优越性。<details>
<summary>Abstract</summary>
We introduce a novel Dual Input Stream Transformer (DIST) for the challenging problem of assigning fixation points from eye-tracking data collected during passage reading to the line of text that the reader was actually focused on. This post-processing step is crucial for analysis of the reading data due to the presence of noise in the form of vertical drift. We evaluate DIST against nine classical approaches on a comprehensive suite of nine diverse datasets, and demonstrate DIST's superiority. By combining multiple instances of the DIST model in an ensemble we achieve an average accuracy of 98.5\% across all datasets. Our approach presents a significant step towards addressing the bottleneck of manual line assignment in reading research. Through extensive model analysis and ablation studies, we identify key factors that contribute to DIST's success, including the incorporation of line overlap features and the use of a second input stream. Through evaluation on a set of diverse datasets we demonstrate that DIST is robust to various experimental setups, making it a safe first choice for practitioners in the field.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的双输入流转换器（DIST），用于从读者眼动数据中分配焦点点到实际阅读的行。这是阅读数据分析中的一个关键后处理步骤，因为存在垂直滑动的噪声。我们对九种经典方法进行了评估，并示出了 DIST 的优越性。通过将多个 DIST 模型 ensemble 组合，我们在所有数据集上实现了平均准确率为 98.5%。我们的方法为阅读研究中的手动线 assigning 带来了一个重要的突破口。通过广泛的模型分析和减少学习，我们确定了 DIST 成功的关键因素，包括将行 overlap 特征并入和使用第二个输入流。我们在多个不同的数据集上进行了评估，并证明了 DIST 在不同的实际设置下具有Robustness，使其成为领域中的首选方法。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Rock-Image-Segmentation-in-Digital-Rock-Physics-A-Fusion-of-Generative-AI-and-State-of-the-Art-Neural-Networks"><a href="#Enhancing-Rock-Image-Segmentation-in-Digital-Rock-Physics-A-Fusion-of-Generative-AI-and-State-of-the-Art-Neural-Networks" class="headerlink" title="Enhancing Rock Image Segmentation in Digital Rock Physics: A Fusion of Generative AI and State-of-the-Art Neural Networks"></a>Enhancing Rock Image Segmentation in Digital Rock Physics: A Fusion of Generative AI and State-of-the-Art Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06079">http://arxiv.org/abs/2311.06079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoyang Ma, Xupeng He, Hyung Kwak, Jun Gao, Shuyu Sun, Bicheng Yan</li>
<li>for: 提高数字岩石物理中的岩石微结构分割精度和稳定性，使用先进的生成AI模型和深度学习网络。</li>
<li>methods: 使用扩展的生成AI模型（Diffusion Model）生成大量的CT&#x2F;SEM和二进制分割对，并使用U-Net、Attention-U-net和TransUNet三种神经网络进行分割。</li>
<li>results: 研究表明，通过将扩展的生成AI模型与高级神经网络结合，可以提高分割精度和一致性，并减少专家数据的需求。TransU-Net表现出色，在岩石微结构分割中实现最高的准确率和IoU指标。<details>
<summary>Abstract</summary>
In digital rock physics, analysing microstructures from CT and SEM scans is crucial for estimating properties like porosity and pore connectivity. Traditional segmentation methods like thresholding and CNNs often fall short in accurately detailing rock microstructures and are prone to noise. U-Net improved segmentation accuracy but required many expert-annotated samples, a laborious and error-prone process due to complex pore shapes. Our study employed an advanced generative AI model, the diffusion model, to overcome these limitations. This model generated a vast dataset of CT/SEM and binary segmentation pairs from a small initial dataset. We assessed the efficacy of three neural networks: U-Net, Attention-U-net, and TransUNet, for segmenting these enhanced images. The diffusion model proved to be an effective data augmentation technique, improving the generalization and robustness of deep learning models. TransU-Net, incorporating Transformer structures, demonstrated superior segmentation accuracy and IoU metrics, outperforming both U-Net and Attention-U-net. Our research advances rock image segmentation by combining the diffusion model with cutting-edge neural networks, reducing dependency on extensive expert data and boosting segmentation accuracy and robustness. TransU-Net sets a new standard in digital rock physics, paving the way for future geoscience and engineering breakthroughs.
</details>
<details>
<summary>摘要</summary>
在数字岩石物理中，分析微结构从CT和SEM扫描图像是关键的，以估算Properties like porosity和连通性。传统的分 segmentation方法，如阈值和CNNs，经常不能准确地描述岩石微结构，同时容易受到噪声的影响。U-Net提高了分 segmentation 精度，但需要大量由专家标注的样本，这是一个费时的和容易出错的过程，因为岩石的pores shapes是复杂的。我们的研究使用了一种先进的生成AI模型，扩散模型，以超越这些限制。这个模型生成了大量的CT/SEM和二进制分 segmentation对from a small initial dataset。我们评估了三个神经网络：U-Net、Attention-U-net和TransUNet，用于这些加强图像的分 segmentation。扩散模型证明是一种有效的数据增强技术，可以提高深度学习模型的普遍性和可靠性。TransU-Net，具有Transformer结构，在分 segmentation精度和IoU指标方面表现出色，超越了U-Net和Attention-U-net。我们的研究提高了岩石图像分 segmentation的准确性和可靠性，并减少了对专家数据的依赖。TransU-Net设置了新的标准在数字岩石物理中，开创了未来地球科学和工程的突破。
</details></li>
</ul>
<hr>
<h2 id="Learning-Based-Biharmonic-Augmentation-for-Point-Cloud-Classification"><a href="#Learning-Based-Biharmonic-Augmentation-for-Point-Cloud-Classification" class="headerlink" title="Learning-Based Biharmonic Augmentation for Point Cloud Classification"></a>Learning-Based Biharmonic Augmentation for Point Cloud Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06070">http://arxiv.org/abs/2311.06070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacheng Wei, Guosheng Lin, Henghui Ding, Jie Hu, Kim-Hui Yap</li>
<li>for: 提高点云数据集的样本数量和多样性，以便进行更好的数据 augmentation。</li>
<li>methods: 我们提出了一种新的数据增强技术 called Biharmonic Augmentation (BA)，它通过对现有3D结构进行平滑非RIGID变换来增加数据集的多样性。我们使用一个CoefNet来预测权重，以将多个几何体的变换概率拼接起来。</li>
<li>results: 我们的实验表明，Biharmonic Augmentation 可以显著提高点云数据集的性能，并且在不同的网络设计下都显示出优秀的成果。<details>
<summary>Abstract</summary>
Point cloud datasets often suffer from inadequate sample sizes in comparison to image datasets, making data augmentation challenging. While traditional methods, like rigid transformations and scaling, have limited potential in increasing dataset diversity due to their constraints on altering individual sample shapes, we introduce the Biharmonic Augmentation (BA) method. BA is a novel and efficient data augmentation technique that diversifies point cloud data by imposing smooth non-rigid deformations on existing 3D structures. This approach calculates biharmonic coordinates for the deformation function and learns diverse deformation prototypes. Utilizing a CoefNet, our method predicts coefficients to amalgamate these prototypes, ensuring comprehensive deformation. Moreover, we present AdvTune, an advanced online augmentation system that integrates adversarial training. This system synergistically refines the CoefNet and the classification network, facilitating the automated creation of adaptive shape deformations contingent on the learner status. Comprehensive experimental analysis validates the superiority of Biharmonic Augmentation, showcasing notable performance improvements over prevailing point cloud augmentation techniques across varied network designs.
</details>
<details>
<summary>摘要</summary>
点云数据集经常受到不充分的样本数量的限制，使得数据增强成为一项挑战。传统方法，如rigid transformations和缩放，受到限制，因为它们不能改变个体样本的形状。我们介绍了一种新的和高效的数据增强技术——幂函数增强（BA）方法。BA方法通过计算幂函数坐标并学习多样化填充的投影函数，以增强点云数据的多样性。此外，我们还提出了一种名为 AdvTune的高级在线增强系统，该系统通过对CoefNet和分类网络进行对抗训练，自动生成适应性的形态变换，以适应学习者的不同状态。经过广泛的实验分析，我们证明了幂函数增强的优越性，在不同的网络设计下展现出了显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Attributes-Grouping-and-Mining-Hashing-for-Fine-Grained-Image-Retrieval"><a href="#Attributes-Grouping-and-Mining-Hashing-for-Fine-Grained-Image-Retrieval" class="headerlink" title="Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval"></a>Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06067">http://arxiv.org/abs/2311.06067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Lu, Shikun Chen, Yichao Cao, Xin Zhou, Xiaobo Lu</li>
<li>for: 本研究旨在提高大规模媒体搜索中的Hashing方法，以便实现精细图像检索。</li>
<li>methods: 我们提出了一种Attributes Grouping and Mining Hashing（AGMH）方法，该方法通过组合多个描述符来生成全面的特征表示。同时，我们还提出了一种Attention Dispersion Loss（ADL）和Stepwise Interactive External Attention（SIEA）两种方法，以便学习细致的特征和对象之间的相关性。</li>
<li>results: 实验结果表明，AGMH方法在精细图像检索任务中具有最佳性能，并且超过了现有的状态态方法。<details>
<summary>Abstract</summary>
In recent years, hashing methods have been popular in the large-scale media search for low storage and strong representation capabilities. To describe objects with similar overall appearance but subtle differences, more and more studies focus on hashing-based fine-grained image retrieval. Existing hashing networks usually generate both local and global features through attention guidance on the same deep activation tensor, which limits the diversity of feature representations. To handle this limitation, we substitute convolutional descriptors for attention-guided features and propose an Attributes Grouping and Mining Hashing (AGMH), which groups and embeds the category-specific visual attributes in multiple descriptors to generate a comprehensive feature representation for efficient fine-grained image retrieval. Specifically, an Attention Dispersion Loss (ADL) is designed to force the descriptors to attend to various local regions and capture diverse subtle details. Moreover, we propose a Stepwise Interactive External Attention (SIEA) to mine critical attributes in each descriptor and construct correlations between fine-grained attributes and objects. The attention mechanism is dedicated to learning discrete attributes, which will not cost additional computations in hash codes generation. Finally, the compact binary codes are learned by preserving pairwise similarities. Experimental results demonstrate that AGMH consistently yields the best performance against state-of-the-art methods on fine-grained benchmark datasets.
</details>
<details>
<summary>摘要</summary>
To address this limitation, we propose an Attributes Grouping and Mining Hashing (AGMH) method, which groups and embeds category-specific visual attributes in multiple descriptors to generate a comprehensive feature representation for efficient fine-grained image retrieval. We also design an Attention Dispersion Loss (ADL) to force the descriptors to attend to various local regions and capture diverse subtle details. Additionally, we propose a Stepwise Interactive External Attention (SIEA) to mine critical attributes in each descriptor and construct correlations between fine-grained attributes and objects. The attention mechanism is dedicated to learning discrete attributes, which does not require additional computations in hash code generation. Finally, we learn compact binary codes by preserving pairwise similarities.Experimental results show that AGMH consistently outperforms state-of-the-art methods on fine-grained benchmark datasets.
</details></li>
</ul>
<hr>
<h2 id="Lidar-based-Norwegian-tree-species-detection-using-deep-learning"><a href="#Lidar-based-Norwegian-tree-species-detection-using-deep-learning" class="headerlink" title="Lidar-based Norwegian tree species detection using deep learning"></a>Lidar-based Norwegian tree species detection using deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06066">http://arxiv.org/abs/2311.06066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martijn Vermeer, Jacob Alexander Hay, David Völgyes, Zsófia Koma, Johannes Breidenbach, Daniele Stefano Maria Fantin</li>
<li>for: 本研究旨在提高 Norwegische Wälder 中树species的映射效率，并且使用价格公平的 lidar 数据来进行类别。</li>
<li>methods: 本研究使用了深度学习的 tree species 分类模型，使用 lidar 影像进行分类，并且使用 focal loss 损失函数进行训练。</li>
<li>results: 本研究在独立验证中获得了 macro-averaged F1 分数0.70，与对比 aerial 或 aerial 和 lidar 结合的模型相当。<details>
<summary>Abstract</summary>
Background: The mapping of tree species within Norwegian forests is a time-consuming process, involving forest associations relying on manual labeling by experts. The process can involve both aerial imagery, personal familiarity, or on-scene references, and remote sensing data. The state-of-the-art methods usually use high resolution aerial imagery with semantic segmentation methods. Methods: We present a deep learning based tree species classification model utilizing only lidar (Light Detection And Ranging) data. The lidar images are segmented into four classes (Norway Spruce, Scots Pine, Birch, background) with a U-Net based network. The model is trained with focal loss over partial weak labels. A major benefit of the approach is that both the lidar imagery and the base map for the labels have free and open access. Results: Our tree species classification model achieves a macro-averaged F1 score of 0.70 on an independent validation with National Forest Inventory (NFI) in-situ sample plots. That is close to, but below the performance of aerial, or aerial and lidar combined models.
</details>
<details>
<summary>摘要</summary>
Background: 挪威森林中的树种分类是一项时间consuming的过程， Forest associations 需要经过专家 manually labeling。这个过程可以使用空中图像、个人熟悉或场景参考，以及遥感数据。现状的方法通常使用高分辨率空中图像与语义分割方法。Methods: 我们提出了一种基于深度学习的树种分类模型，只使用激光探测（Light Detection And Ranging）数据。激光图像被分类为四类（挪威落叶松、苏格兰杉、桦树、背景），使用基于U-Net的网络进行分类。模型通过 focal loss 对部分弱标签进行训练。这种方法的一个主要优点是，激光图像和基础地图均有免费和开放的 accessed。Results: 我们的树种分类模型在独立验证中以 macro-averaged F1 分数为 0.70 达到了高水平。与气象、气象和激光组合模型相比，其性能只有一些下降。
</details></li>
</ul>
<hr>
<h2 id="Improved-Positional-Encoding-for-Implicit-Neural-Representation-based-Compact-Data-Representation"><a href="#Improved-Positional-Encoding-for-Implicit-Neural-Representation-based-Compact-Data-Representation" class="headerlink" title="Improved Positional Encoding for Implicit Neural Representation based Compact Data Representation"></a>Improved Positional Encoding for Implicit Neural Representation based Compact Data Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06059">http://arxiv.org/abs/2311.06059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bharath Bhushan Damodaran, Francois Schnitzler, Anne Lambert, Pierre Hellier</li>
<li>for: 提高含义表示（Implicit Neural Representation，INR）中信息的重建质量</li>
<li>methods: 使用位置编码法捕捉高频信息，提出一种新的 позицион编码方法，具有更多的频率基准，从而实现更好的数据压缩和重建质量</li>
<li>results: 实验显示，提出的方法可以在压缩任务中获得显著的增益，同时在新视角合成中实现更高的重建质量，而无需增加任何复杂性。<details>
<summary>Abstract</summary>
Positional encodings are employed to capture the high frequency information of the encoded signals in implicit neural representation (INR). In this paper, we propose a novel positional encoding method which improves the reconstruction quality of the INR. The proposed embedding method is more advantageous for the compact data representation because it has a greater number of frequency basis than the existing methods. Our experiments shows that the proposed method achieves significant gain in the rate-distortion performance without introducing any additional complexity in the compression task and higher reconstruction quality in novel view synthesis.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文。使用位置编码 capture高频信息编码的含义信号在偏挥 neural representation（INR）中。本文提出了一种新的位置编码方法，可以提高INR重建质量。该嵌入方法具有更多的频率基准，与现有方法相比，具有更好的数据压缩性。我们的实验表明，该方法可以在压缩任务中实现显著的加速性和高质量重建。
</details></li>
</ul>
<hr>
<h2 id="Ulcerative-Colitis-Mayo-Endoscopic-Scoring-Classification-with-Active-Learning-and-Generative-Data-Augmentation"><a href="#Ulcerative-Colitis-Mayo-Endoscopic-Scoring-Classification-with-Active-Learning-and-Generative-Data-Augmentation" class="headerlink" title="Ulcerative Colitis Mayo Endoscopic Scoring Classification with Active Learning and Generative Data Augmentation"></a>Ulcerative Colitis Mayo Endoscopic Scoring Classification with Active Learning and Generative Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06057">http://arxiv.org/abs/2311.06057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ümit Mert Çağlar, Alperen İnci, Oğuz Hanoğlu, Görkem Polat, Alptekin Temizel</li>
<li>For:  This paper aims to improve the accuracy of endoscopic image analysis for Ulcerative Colitis (UC) diagnosis and severity classification, by using active learning and generative augmentation methods.* Methods: The proposed method involves generating a large number of synthetic samples using a small dataset of real endoscopic images, and then using active learning to select the most informative samples for training a classifier.* Results: The method achieved improved classification performance compared to using only the original labeled examples, with a QWK score increase from 68.1% to 74.5%. Additionally, the method required three times fewer real images to achieve equivalent performance.<details>
<summary>Abstract</summary>
Endoscopic imaging is commonly used to diagnose Ulcerative Colitis (UC) and classify its severity. It has been shown that deep learning based methods are effective in automated analysis of these images and can potentially be used to aid medical doctors. Unleashing the full potential of these methods depends on the availability of large amount of labeled images; however, obtaining and labeling these images are quite challenging. In this paper, we propose a active learning based generative augmentation method. The method involves generating a large number of synthetic samples by training using a small dataset consisting of real endoscopic images. The resulting data pool is narrowed down by using active learning methods to select the most informative samples, which are then used to train a classifier. We demonstrate the effectiveness of our method through experiments on a publicly available endoscopic image dataset. The results show that using synthesized samples in conjunction with active learning leads to improved classification performance compared to using only the original labeled examples and the baseline classification performance of 68.1% increases to 74.5% in terms of Quadratic Weighted Kappa (QWK) Score. Another observation is that, attaining equivalent performance using only real data necessitated three times higher number of images.
</details>
<details>
<summary>摘要</summary>
便门影像是通常用于诊断发炎性结肠炎（UC）和评估其严重程度。研究表明，深度学习基本方法可以有效地自动分析这些影像，并可能用于帮助医学博士。然而，获取和标注这些影像是很困难的。在这篇论文中，我们提出了一个活动学习基本的生成增强方法。这个方法包括生成一大量的 sintetic 样本，通过训练一小型的实际便门影像集合。然后，使用活动学习方法选择最有用的样本，并将它们用于训练分类器。我们通过实验显示了我们的方法的有效性，使用 sintetic 样本和活动学习可以将分类性能提高至原始标注数据的 68.1% 提高至 74.5%，即quadratic Weighted Kappa（QWK）分数。此外，我们发现，仅使用实际数据就能够取得相等的性能，需要三倍多的数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-Contrastive-Self-Distillation-for-Ultra-Fine-Grained-Visual-Categorization-Targeting-Limited-Samples"><a href="#Learning-Contrastive-Self-Distillation-for-Ultra-Fine-Grained-Visual-Categorization-Targeting-Limited-Samples" class="headerlink" title="Learning Contrastive Self-Distillation for Ultra-Fine-Grained Visual Categorization Targeting Limited Samples"></a>Learning Contrastive Self-Distillation for Ultra-Fine-Grained Visual Categorization Targeting Limited Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06056">http://arxiv.org/abs/2311.06056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziye Fang, Xin Jiang, Hao Tang, Zechao Li</li>
<li>for: 本研究针对具有细分类别的 Multimedia 分析 tasks 提出了一个创新的框架，即 CSDNet，以便更好地处理具有复杂的类别分割和有限的数据的 Ultra-Fine-Grained Visual Categorization (Ultra-FGVC) 任务。</li>
<li>methods: CSDNet 构造由三个主要模组组成：Subcategory-Specific Discrepancy Parsing (SSDP)、Dynamic Discrepancy Learning (DDL) 和 Subcategory-Specific Discrepancy Transfer (SSDT)，这些模组共同增强了深度模型在不同的预测层次（instance、feature、logit）之间的通用性。具体来说，SSDP 模组通过将不同观点的扩展样本添加到标本集中，以显示出类别下的特殊差异；DDL 模组使用动态内存随机抽出器来优化特征学习空间，通过反对推对称推对称学习；SSDT 模组则通过一种新的自体激发方式在预测层次上进行自体激发，以更好地吸收类别下的差异知识。</li>
<li>results: 实验结果显示，CSDNet 在 Ultra-FGVC 任务上具有更高的表现力和适应力，较前一些现有的方法。这说明 CSDNet 能够更好地处理具有复杂的类别分割和有限的数据的 Ultra-FGVC 任务。<details>
<summary>Abstract</summary>
In the field of intelligent multimedia analysis, ultra-fine-grained visual categorization (Ultra-FGVC) plays a vital role in distinguishing intricate subcategories within broader categories. However, this task is inherently challenging due to the complex granularity of category subdivisions and the limited availability of data for each category. To address these challenges, this work proposes CSDNet, a pioneering framework that effectively explores contrastive learning and self-distillation to learn discriminative representations specifically designed for Ultra-FGVC tasks. CSDNet comprises three main modules: Subcategory-Specific Discrepancy Parsing (SSDP), Dynamic Discrepancy Learning (DDL), and Subcategory-Specific Discrepancy Transfer (SSDT), which collectively enhance the generalization of deep models across instance, feature, and logit prediction levels. To increase the diversity of training samples, the SSDP module introduces augmented samples from different viewpoints to spotlight subcategory-specific discrepancies. Simultaneously, the proposed DDL module stores historical intermediate features by a dynamic memory queue, which optimizes the feature learning space through iterative contrastive learning. Furthermore, the SSDT module is developed by a novel self-distillation paradigm at the logit prediction level of raw and augmented samples, which effectively distills more subcategory-specific discrepancies knowledge from the inherent structure of limited training data without requiring additional annotations. Experimental results demonstrate that CSDNet outperforms current state-of-the-art Ultra-FGVC methods, emphasizing its powerful efficacy and adaptability in addressing Ultra-FGVC tasks.
</details>
<details>
<summary>摘要</summary>
在智能多媒体分析领域， ultra-fine-grained视觉分类（Ultra-FGVC）扮演着重要的角色，用于分辨更加细致的分类划分。然而，这种任务具有较复杂的分类划分和数据有限的问题。为解决这些挑战，本文提出了 CSDNet 框架，它通过对比学习和自适应学习来学习特定于 Ultra-FGVC 任务的抽象表示。CSDNet 包括三个主要模块：特征特异性分析（SSDP）、动态不同分学习（DDL）和特征特异性转移（SSDT），这些模块共同提高了深度模型的通用性 across instance、feature和logit 预测层次。为了增加训练样本的多样性，SSDP 模块通过不同视点生成的增强样本来强调特征特异性。同时，提出的 DDL 模块通过动态缓存队列来优化特性学习空间，通过反卷积学习来优化特征学习。此外，SSDT 模块通过一种新的自适应学习方式来在 raw 和增强样本的 logit 预测层次上进行自适应学习，从而更好地提取有限训练数据中的特征特异性知识。实验结果表明，CSDNet 在 Ultra-FGVC 任务上表现出色，证明了其强大的效果和适应性。
</details></li>
</ul>
<hr>
<h2 id="Refining-the-ONCE-Benchmark-with-Hyperparameter-Tuning"><a href="#Refining-the-ONCE-Benchmark-with-Hyperparameter-Tuning" class="headerlink" title="Refining the ONCE Benchmark with Hyperparameter Tuning"></a>Refining the ONCE Benchmark with Hyperparameter Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06054">http://arxiv.org/abs/2311.06054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maksim Golyadkin, Alexander Gambashidze, Ildar Nurgaliev, Ilya Makarov</li>
<li>for: 本研究旨在评估 semi-supervised learning 方法在点云数据上的性能。</li>
<li>methods: 本研究使用 semi-supervised learning 方法，并对点云数据进行自动标注。</li>
<li>results: 研究发现，使用不同的搜索空间和模型可以获得更高的性能，但使用无标注数据的贡献相对较少。<details>
<summary>Abstract</summary>
In response to the growing demand for 3D object detection in applications such as autonomous driving, robotics, and augmented reality, this work focuses on the evaluation of semi-supervised learning approaches for point cloud data. The point cloud representation provides reliable and consistent observations regardless of lighting conditions, thanks to advances in LiDAR sensors. Data annotation is of paramount importance in the context of LiDAR applications, and automating 3D data annotation with semi-supervised methods is a pivotal challenge that promises to reduce the associated workload and facilitate the emergence of cost-effective LiDAR solutions. Nevertheless, the task of semi-supervised learning in the context of unordered point cloud data remains formidable due to the inherent sparsity and incomplete shapes that hinder the generation of accurate pseudo-labels. In this study, we consider these challenges by posing the question: "To what extent does unlabelled data contribute to the enhancement of model performance?" We show that improvements from previous semi-supervised methods may not be as profound as previously thought. Our results suggest that simple grid search hyperparameter tuning applied to a supervised model can lead to state-of-the-art performance on the ONCE dataset, while the contribution of unlabelled data appears to be comparatively less exceptional.
</details>
<details>
<summary>摘要</summary>
“这个研究旨在评估半监督学习方法在3D物体探测应用中的表现，特别是针对基于LiDAR感知器的应用。点云表示提供可靠和一致的观察，不受照明条件影响。在LiDAR应用中，标签数据的标注是非常重要的，但对于半监督学习方法而言，自动化3D标签的自动化过程是一个挑战，可以实现成本下降和LiDAR解决方案的发展。然而，在无序点云数据上进行半监督学习的任务仍然是一个挑战，因为点云数据的缺乏和形状不对称对于生成准确pseudo标签所造成阻碍。在这个研究中，我们询问：“半监督学习中无标的数据对模型性能的贡献为何？”我们发现，与之前的半监督方法相比，改进的空间搜寻参数可以带来更好的性能，而无标的数据对模型性能的贡献相对较少。”
</details></li>
</ul>
<hr>
<h2 id="2D-Image-head-pose-estimation-via-latent-space-regression-under-occlusion-settings"><a href="#2D-Image-head-pose-estimation-via-latent-space-regression-under-occlusion-settings" class="headerlink" title="2D Image head pose estimation via latent space regression under occlusion settings"></a>2D Image head pose estimation via latent space regression under occlusion settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06038">http://arxiv.org/abs/2311.06038</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sipg-isr/Occlusion_HPE">https://github.com/sipg-isr/Occlusion_HPE</a></li>
<li>paper_authors: José Celestino, Manuel Marques, Jacinto C. Nascimento, João Paulo Costeira</li>
<li>for: 本研究旨在提高 occluded  scenario 下的人头pose 估算精度，提供更可靠的人机交互方案。</li>
<li>methods: 该研究提出了一种基于 latent space regression 的深度学习方法，通过更好地结构化 occluded  scenrio 下的问题，提高 head pose estimation 的精度。</li>
<li>results: 对于 occluded 和 non-occluded  scenrio 下的数据集，该方法比多种 state-of-the-art 方法表现出较高的精度，并且在实际应用中（如人机交互场景）也显示出了良好的性能。<details>
<summary>Abstract</summary>
Head orientation is a challenging Computer Vision problem that has been extensively researched having a wide variety of applications. However, current state-of-the-art systems still underperform in the presence of occlusions and are unreliable for many task applications in such scenarios. This work proposes a novel deep learning approach for the problem of head pose estimation under occlusions. The strategy is based on latent space regression as a fundamental key to better structure the problem for occluded scenarios. Our model surpasses several state-of-the-art methodologies for occluded HPE, and achieves similar accuracy for non-occluded scenarios. We demonstrate the usefulness of the proposed approach with: (i) two synthetically occluded versions of the BIWI and AFLW2000 datasets, (ii) real-life occlusions of the Pandora dataset, and (iii) a real-life application to human-robot interaction scenarios where face occlusions often occur. Specifically, the autonomous feeding from a robotic arm.
</details>
<details>
<summary>摘要</summary>
头部方向是计算机视觉领域中一个具有广泛应用的挑战，已经有广泛的研究。然而，当前状态的先进系统仍然在干扰场景下表现不佳，对多种任务场景表现不可靠。这项工作提出了一种基于深度学习的新方法，用于在干扰场景下进行头部pose估计。这种策略基于latent space regression作为基础，以更好地结构化干扰场景中的问题。我们的模型在干扰场景下超过了多种现有方法，并在非干扰场景下实现了类似的准确率。我们通过以下三种实验来证明提出的方法的有用性：（i）使用BIWI和AFLW2000 datasets中的两个 sintetically occluded版本，（ii）使用Pandora dataset中的真实干扰场景，（iii）在人机交互场景中使用 autonomous feeding from a robotic arm。
</details></li>
</ul>
<hr>
<h2 id="Diagonal-Hierarchical-Consistency-Learning-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Diagonal-Hierarchical-Consistency-Learning-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Diagonal Hierarchical Consistency Learning for Semi-supervised Medical Image Segmentation"></a>Diagonal Hierarchical Consistency Learning for Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06031">http://arxiv.org/abs/2311.06031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heejoon Koo</li>
<li>for: 这篇论文是用于医疗影像分类的，以提高诊断和治疗的精度。</li>
<li>methods: 这篇论文使用了一个新的框架，即DiHC-Net，以提高医疗影像分类的稳定性和准确性。DiHC-Net包含多个子模型，每个子模型有相同的多尺度架构，但每个子模型有不同的子层，如数值升降和Normalization层。此外，这篇论文还提出了一个新的 diagonally hierarchical consistency 的方法，用于在不同尺度上强制这些子模型之间的一致性。</li>
<li>results: 实验结果显示，DiHC-Net 比之前的所有方法在公共 Left Atrium (LA) 数据集上表现更好，具有更高的稳定性和准确性。<details>
<summary>Abstract</summary>
Medical image segmentation, which is essential for many clinical applications, has achieved almost human-level performance via data-driven deep learning techniques. Nevertheless, its performance is predicated on the costly process of manually annotating a large amount of medical images. To this end, we propose a novel framework for robust semi-supervised medical image segmentation using diagonal hierarchical consistency (DiHC-Net). First, it is composed of multiple sub-models with identical multi-scale architecture but with distinct sub-layers, such as up-sampling and normalisation layers. Second, a novel diagonal hierarchical consistency is enforced between one model's intermediate and final prediction and other models' soft pseudo labels in a diagonal hierarchical fashion. Experimental results verify the efficacy of our simple framework, outperforming all previous approaches on public Left Atrium (LA) dataset.
</details>
<details>
<summary>摘要</summary>
医疗图像分割，对许多临床应用而言是必需的，已经通过数据驱动的深度学习技术实现了几乎人类水平的性能。然而，其性能受到手动标注大量医疗图像的高成本过程的限制。为此，我们提出了一种新的框架，即对称层次一致性网络（DiHC-Net）。这个框架包括多个子模型，每个子模型具有相同的多scales架构，但各自具有不同的子层，例如升降sample和normal化层。其次，我们提出了一种新的对称层次一致性，即在一个模型的中间预测和最终预测之间，以及其他模型的软 Pseudo标签之间的对称层次一致性。实验结果表明，我们的简单框架可以准确地 segment 医疗图像，并且超过了所有之前的方法在公共左心脏（LA）数据集上的性能。
</details></li>
</ul>
<hr>
<h2 id="U3DS-3-Unsupervised-3D-Semantic-Scene-Segmentation"><a href="#U3DS-3-Unsupervised-3D-Semantic-Scene-Segmentation" class="headerlink" title="U3DS$^3$: Unsupervised 3D Semantic Scene Segmentation"></a>U3DS$^3$: Unsupervised 3D Semantic Scene Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06018">http://arxiv.org/abs/2311.06018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Liu, Zhengdi Yu, Toby P. Breckon, Hubert P. H. Shum</li>
<li>for: 这篇论文主要用于解决3D点云分割问题，即不需要大量标注数据来进行训练。</li>
<li>methods: 该方法基于点云自身的特征信息，无需模型预训练，通过扩展点云的方式生成超点，然后通过空间划分和迭代训练使用 pseudo-标签进行学习。</li>
<li>results: 该方法在ScanNet和SemanticKITTI datasets上达到了状态略的表现，并在S3DIS dataset上获得了竞争性的结果。<details>
<summary>Abstract</summary>
Contemporary point cloud segmentation approaches largely rely on richly annotated 3D training data. However, it is both time-consuming and challenging to obtain consistently accurate annotations for such 3D scene data. Moreover, there is still a lack of investigation into fully unsupervised scene segmentation for point clouds, especially for holistic 3D scenes. This paper presents U3DS$^3$, as a step towards completely unsupervised point cloud segmentation for any holistic 3D scenes. To achieve this, U3DS$^3$ leverages a generalized unsupervised segmentation method for both object and background across both indoor and outdoor static 3D point clouds with no requirement for model pre-training, by leveraging only the inherent information of the point cloud to achieve full 3D scene segmentation. The initial step of our proposed approach involves generating superpoints based on the geometric characteristics of each scene. Subsequently, it undergoes a learning process through a spatial clustering-based methodology, followed by iterative training using pseudo-labels generated in accordance with the cluster centroids. Moreover, by leveraging the invariance and equivariance of the volumetric representations, we apply the geometric transformation on voxelized features to provide two sets of descriptors for robust representation learning. Finally, our evaluation provides state-of-the-art results on the ScanNet and SemanticKITTI, and competitive results on the S3DIS, benchmark datasets.
</details>
<details>
<summary>摘要</summary>
现代点云分割方法大多依赖于 ricly annotated 3D 训练数据。然而，获得一致性的精确标注对于 such 3D 场景数据是时间consuming 和挑战性的。此外，还缺乏对全自动点云分割的完全无监督场景进行研究，特别是 для holistic 3D 场景。本文提出了 U3DS$^3$，这是一种Step towards completely unsupervised point cloud segmentation for any holistic 3D scenes。为实现这一目标，U3DS$^3$ 利用了一种通用无监督分割方法，可以在 both indoor and outdoor static 3D point clouds 中实现全3D场景分割，无需模型预训练。U3DS$^3$ 的首先步骤是生成 superpoints 基于场景的 геометрических特征。然后，它通过空间归一化方法进行学习，然后通过 iterative training 使用 pseudo-labels 根据集中点生成。此外，通过利用点云的几何特征，我们将其映射到 voxelized 特征上，并将其作为 Robust 表示学习的两个集。最后，我们的评估结果表明 U3DS$^3$ 在 ScanNet 和 SemanticKITTI 数据集上获得了状态对的结果，并在 S3DIS 数据集上获得了竞争性的结果。
</details></li>
</ul>
<hr>
<h2 id="Polar-Net-A-Clinical-Friendly-Model-for-Alzheimer’s-Disease-Detection-in-OCTA-Images"><a href="#Polar-Net-A-Clinical-Friendly-Model-for-Alzheimer’s-Disease-Detection-in-OCTA-Images" class="headerlink" title="Polar-Net: A Clinical-Friendly Model for Alzheimer’s Disease Detection in OCTA Images"></a>Polar-Net: A Clinical-Friendly Model for Alzheimer’s Disease Detection in OCTA Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06009">http://arxiv.org/abs/2311.06009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iAaronLau/Polar-Net-Pytorch">https://github.com/iAaronLau/Polar-Net-Pytorch</a></li>
<li>paper_authors: Shouyue Liu, Jinkui Hao, Yanwu Xu, Huazhu Fu, Xinyu Guo, Jiang Liu, Yalin Zheng, Yonghuai Liu, Jiong Zhang, Yitian Zhao</li>
<li>for: 检测阿尔ツheimer病（AD）的可能性，通过图像征识Retinal microvasculature。</li>
<li>methods: 使用抽象的深度计算机视觉方法，以及Polar-Net模型，将OCTA图像坐标系从Cartesian坐标系映射到极坐标系，并使用ETDRS格子基于地区分析方法。</li>
<li>results: 与现有方法相比，Polar-Net模型在私人和公共数据集上表现出色，并提供更加有价值的病理证据，证明了Retinal microvasculature变化和AD之间的相关性。<details>
<summary>Abstract</summary>
Optical Coherence Tomography Angiography (OCTA) is a promising tool for detecting Alzheimer's disease (AD) by imaging the retinal microvasculature. Ophthalmologists commonly use region-based analysis, such as the ETDRS grid, to study OCTA image biomarkers and understand the correlation with AD. However, existing studies have used general deep computer vision methods, which present challenges in providing interpretable results and leveraging clinical prior knowledge. To address these challenges, we propose a novel deep-learning framework called Polar-Net. Our approach involves mapping OCTA images from Cartesian coordinates to polar coordinates, which allows for the use of approximate sector convolution and enables the implementation of the ETDRS grid-based regional analysis method commonly used in clinical practice. Furthermore, Polar-Net incorporates clinical prior information of each sector region into the training process, which further enhances its performance. Additionally, our framework adapts to acquire the importance of the corresponding retinal region, which helps researchers and clinicians understand the model's decision-making process in detecting AD and assess its conformity to clinical observations. Through evaluations on private and public datasets, we have demonstrated that Polar-Net outperforms existing state-of-the-art methods and provides more valuable pathological evidence for the association between retinal vascular changes and AD. In addition, we also show that the two innovative modules introduced in our framework have a significant impact on improving overall performance.
</details>
<details>
<summary>摘要</summary>
Optical Coherence Tomography Angiography (OCTA) 是一种有前途的工具，用于检测阿尔ツ海默病（AD），通过呈现Retinal Microvasculature的图像。 医生们通常使用区域分析方法，如ETDRS 格，来研究 OCTA 图像标记和了解与 AD 的相关性。然而，现有的研究都使用了通用的深度计算机视觉方法，这会带来解释结果的困难和不能充分利用临床前知识。为解决这些挑战，我们提出了一种新的深度学习框架，即Polar-Net。我们的方法通过将 OCTA 图像从Cartesian坐标系转换到极坐标系，使得可以使用 Approximate Sector Convolution 和实施ETDRS 格基于的区域分析方法，这样可以更好地利用临床前知识。此外，Polar-Net 还将临床前知识 incorporated 到训练过程中，进一步提高其性能。此外，我们的框架还可以评估相应的Retinal 区域的重要性，帮助研究人员和医生理解模型的决策过程中的AD 检测和评估模型是否符合临床观察。经过评估private和公共数据集，我们展示了Polar-Net 可以超过现有的状态对方法，并提供更有价值的病理证据，用于关系Retinal 血管变化和AD的相关性。此外，我们还发现了两个创新模块在我们的框架中具有重要作用，即提高总性能。
</details></li>
</ul>
<hr>
<h2 id="Keystroke-Verification-Challenge-KVC-Biometric-and-Fairness-Benchmark-Evaluation"><a href="#Keystroke-Verification-Challenge-KVC-Biometric-and-Fairness-Benchmark-Evaluation" class="headerlink" title="Keystroke Verification Challenge (KVC): Biometric and Fairness Benchmark Evaluation"></a>Keystroke Verification Challenge (KVC): Biometric and Fairness Benchmark Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06000">http://arxiv.org/abs/2311.06000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Stragapede, Ruben Vera-Rodriguez, Ruben Tolosana, Aythami Morales, Naser Damer, Julian Fierrez, Javier Ortega-Garcia</li>
<li>for: 本研究旨在提高键盘动作生物认证的性能和公正性。</li>
<li>methods: 本研究使用了新的实验框架和公平指标来评估键盘动作生物认证系统的性能和公正性。</li>
<li>results: 研究发现，通过减少键盘动作生物认证系统中文本内容的分析，可以保持atisfactory的性能，同时减少隐私泄露风险。Here’s the translation in Simplified Chinese:</li>
<li>for: 本研究旨在提高键盘动作生物认证的性能和公正性。</li>
<li>methods: 本研究使用了新的实验框架和公平指标来评估键盘动作生物认证系统的性能和公正性。</li>
<li>results: 研究发现，通过减少键盘动作生物认证系统中文本内容的分析，可以保持 satisfactory 的性能，同时减少隐私泄露风险。<details>
<summary>Abstract</summary>
Analyzing keystroke dynamics (KD) for biometric verification has several advantages: it is among the most discriminative behavioral traits; keyboards are among the most common human-computer interfaces, being the primary means for users to enter textual data; its acquisition does not require additional hardware, and its processing is relatively lightweight; and it allows for transparently recognizing subjects. However, the heterogeneity of experimental protocols and metrics, and the limited size of the databases adopted in the literature impede direct comparisons between different systems, thus representing an obstacle in the advancement of keystroke biometrics. To alleviate this aspect, we present a new experimental framework to benchmark KD-based biometric verification performance and fairness based on tweet-long sequences of variable transcript text from over 185,000 subjects, acquired through desktop and mobile keyboards, extracted from the Aalto Keystroke Databases. The framework runs on CodaLab in the form of the Keystroke Verification Challenge (KVC). Moreover, we also introduce a novel fairness metric, the Skewed Impostor Ratio (SIR), to capture inter- and intra-demographic group bias patterns in the verification scores. We demonstrate the usefulness of the proposed framework by employing two state-of-the-art keystroke verification systems, TypeNet and TypeFormer, to compare different sets of input features, achieving a less privacy-invasive system, by discarding the analysis of text content (ASCII codes of the keys pressed) in favor of extended features in the time domain. Our experiments show that this approach allows to maintain satisfactory performance.
</details>
<details>
<summary>摘要</summary>
分析键盘动态（KD） для生物认证有很多优点：它是最有特征的行为特征之一；键盘是人机界面中最常用的输入设备之一，用户通过键盘输入文本数据；获取它不需要额外硬件，处理也较轻量级，可透明地识别用户。然而，实验室协议和度量的多样性，以及文献中所采用的数据库的小型，使得不同系统之间的比较困难，从而阻碍了键盘生物认证的进步。为了解决这一问题，我们提出了一个新的实验框架，用于评估基于键盘动态的生物认证性和公正性，并在 CodaLab 上进行了 Keystroke Verification Challenge（KVC）。此外，我们还介绍了一种新的公正度指标，即不良假冒比率（SIR），用于捕捉 между组和内组偏见偏好的识别分数。我们通过使用两个现有的键盘认证系统，TypeNet 和 TypeFormer，对不同的输入特征进行比较，实现了一种更加隐私的系统，通过抛弃ASCII码的分析以获得更多的时间域特征。我们的实验结果表明，这种方法可以保持满意的性能。
</details></li>
</ul>
<hr>
<h2 id="Vision-Big-Bird-Random-Sparsification-for-Full-Attention"><a href="#Vision-Big-Bird-Random-Sparsification-for-Full-Attention" class="headerlink" title="Vision Big Bird: Random Sparsification for Full Attention"></a>Vision Big Bird: Random Sparsification for Full Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05988">http://arxiv.org/abs/2311.05988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhemin Zhang, Xun Gong</li>
<li>for: 该研究旨在提出一种基于Transformers的视觉模型，以提高视觉任务的性能。</li>
<li>methods: 该模型使用三组头部，其中第一组使用卷积神经网络提取地方特征并提供位置信息，第二组使用随机抽样窗口进行笛卡尔抽样自注意计算，第三组将键值的分辨率减小通过平均抽取来保持全局注意的稀热性。</li>
<li>results: 实验结果表明，视觉大鸟模型可以维持自注意的稀热性，并且可以安全地去除位置编码。该模型在常见视觉任务中达到竞争性性能。<details>
<summary>Abstract</summary>
Recently, Transformers have shown promising performance in various vision tasks. However, the high costs of global self-attention remain challenging for Transformers, especially for high-resolution vision tasks. Inspired by one of the most successful transformers-based models for NLP: Big Bird, we propose a novel sparse attention mechanism for Vision Transformers (ViT). Specifically, we separate the heads into three groups, the first group used convolutional neural network (CNN) to extract local features and provide positional information for the model, the second group used Random Sampling Windows (RS-Win) for sparse self-attention calculation, and the third group reduces the resolution of the keys and values by average pooling for global attention. Based on these components, ViT maintains the sparsity of self-attention while maintaining the merits of Big Bird (i.e., the model is a universal approximator of sequence functions and is Turing complete). Moreover, our results show that the positional encoding, a crucial component in ViTs, can be safely removed in our model. Experiments show that Vision Big Bird demonstrates competitive performance on common vision tasks.
</details>
<details>
<summary>摘要</summary>
近些时间，变换器在各种视觉任务中表现出了扎实的能力。然而，全球自注意的高成本仍然是变换器的挑战，特别是高分辨率视觉任务。受Big Bird模型的启发，我们提出了一种新的稀疏注意机制 для视觉变换器（ViT）。具体来说，我们将头分为三组：第一组使用卷积神经网络（CNN）提取地方特征并提供模型位置信息，第二组使用随机抽取窗口（RS-Win）进行稀疏自注意计算，第三组将键和值的分辨率降低到平均抽取。这些组件使得ViT可以保持自注意的稀疏性，同时保持Big Bird模型的优点（即模型是序列函数的通用近似器和Turing完善的）。此外，我们的实验表明，ViT中的位置编码可以安全地移除。Vision Big Bird在常见视觉任务中显示了竞争力强的性能。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Male-Nyala-and-Male-Kudu-Classification-using-Transfer-Learning-with-ResNet-50-and-VGG-16"><a href="#Comparing-Male-Nyala-and-Male-Kudu-Classification-using-Transfer-Learning-with-ResNet-50-and-VGG-16" class="headerlink" title="Comparing Male Nyala and Male Kudu Classification using Transfer Learning with ResNet-50 and VGG-16"></a>Comparing Male Nyala and Male Kudu Classification using Transfer Learning with ResNet-50 and VGG-16</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05981">http://arxiv.org/abs/2311.05981</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. T Lemani, T. L. van Zyl</li>
<li>for: 这paper的目的是为了研究使用深度学习和计算机视觉技术来快速和高精度地识别野生动物，以便为管理和保护决策提供信息。</li>
<li>methods: 本paper使用了预训练模型VGG-16和ResNet-50，通过转移学习方法进行精度调整，以便在野生环境中识别♂️戴蛭和♂️牛羚。</li>
<li>results:  эксperimental结果显示，在550张图像中，预训练后的VGG-16和ResNet-50模型均达到了97.7%的准确率，而不进行调整的模型则达到了93.2%的准确率。然而，这些结果是基于一个小样本大小的评估，因此可能不具有足够的可靠性和普遍性。<details>
<summary>Abstract</summary>
Reliable and efficient monitoring of wild animals is crucial to inform management and conservation decisions. The process of manually identifying species of animals is time-consuming, monotonous, and expensive. Leveraging on advances in deep learning and computer vision, we investigate in this paper the efficiency of pre-trained models, specifically the VGG-16 and ResNet-50 model, in identifying a male Kudu and a male Nyala in their natural habitats. These pre-trained models have proven to be efficient in animal identification in general. Still, there is little research on animals like the Kudu and Nyala, who are usually well camouflaged and have similar features. The method of transfer learning used in this paper is the fine-tuning method. The models are evaluated before and after fine-tuning. The experimental results achieved an accuracy of 93.2\% and 97.7\% for the VGG-16 and ResNet-50 models, respectively, before fine-tuning and 97.7\% for both models after fine-tuning. Although these results are impressive, it should be noted that they were taken over a small sample size of 550 images split in half between the two classes; therefore, this might not cater to enough scenarios to get a full conclusion of the efficiency of the models. Therefore, there is room for more work in getting a more extensive dataset and testing and extending to the female counterparts of these species and the whole antelope species.
</details>
<details>
<summary>摘要</summary>
可靠和高效的野生动物监测是管理和保护决策的关键。手动识别动物种类是时间consuming、单调和昂贵的。利用深度学习和计算机视觉的进步，我们在这篇论文中 investigate了VGG-16和ResNet-50模型在自然环境中识别♂️普通鹿和♂️涂猪的能力。这些预训练模型在动物识别方面有效。然而，关于鹿和涂猪这些动物，它们通常很隐藏，外表相似，有少量研究。本文使用的方法是转移学习方法。模型在 Fine-tuning 前和后的评估结果表明，VGG-16和ResNet-50模型在自然环境中识别♂️普通鹿和♂️涂猪的能力具有93.2%和97.7%的准确率，分别是之前和之后 Fine-tuning。虽然这些结果很出色，但是它们是基于550张图像，其中有一半是♂️普通鹿和♂️涂猪两类的样本，因此这并不能代表充分的场景，因此还有很多空间 для进一步的测试和扩展。
</details></li>
</ul>
<hr>
<h2 id="Quantized-Distillation-Optimizing-Driver-Activity-Recognition-Models-for-Resource-Constrained-Environments"><a href="#Quantized-Distillation-Optimizing-Driver-Activity-Recognition-Models-for-Resource-Constrained-Environments" class="headerlink" title="Quantized Distillation: Optimizing Driver Activity Recognition Models for Resource-Constrained Environments"></a>Quantized Distillation: Optimizing Driver Activity Recognition Models for Resource-Constrained Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05970">http://arxiv.org/abs/2311.05970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/calvintanama/qd-driver-activity-reco">https://github.com/calvintanama/qd-driver-activity-reco</a></li>
<li>paper_authors: Calvin Tanama, Kunyu Peng, Zdravko Marinov, Rainer Stiefelhagen, Alina Roitberg</li>
<li>for: 这篇论文旨在提出轻量级的驾驶活动识别框架，以提高自驾车时的资源效率。</li>
<li>methods: 这篇论文使用了知识传授和模型量化来将3D MobileNet简化，以保持模型精度而实现资源效率。</li>
<li>results: 实验结果显示，这个新的框架可以与已有的框架相比，三倍减少模型大小，并提高执行速度1.4倍。<details>
<summary>Abstract</summary>
Deep learning-based models are at the forefront of most driver observation benchmarks due to their remarkable accuracies but are also associated with high computational costs. This is challenging, as resources are often limited in real-world driving scenarios. This paper introduces a lightweight framework for resource-efficient driver activity recognition. The framework enhances 3D MobileNet, a neural architecture optimized for speed in video classification, by incorporating knowledge distillation and model quantization to balance model accuracy and computational efficiency. Knowledge distillation helps maintain accuracy while reducing the model size by leveraging soft labels from a larger teacher model (I3D), instead of relying solely on original ground truth data. Model quantization significantly lowers memory and computation demands by using lower precision integers for model weights and activations. Extensive testing on a public dataset for in-vehicle monitoring during autonomous driving demonstrates that this new framework achieves a threefold reduction in model size and a 1.4-fold improvement in inference time, compared to an already optimized architecture. The code for this study is available at https://github.com/calvintanama/qd-driver-activity-reco.
</details>
<details>
<summary>摘要</summary>
深度学习模型在驾驶员观察benchmark中领先，主要是因为它们的准确率非常高，但是也因为计算成本很高。这会在实际驾驶场景中带来挑战，因为资源经常是有限的。这篇论文介绍了一个轻量级框架，用于提高驾驶员活动识别的资源效率。这个框架基于3D MobileNet neural网络，通过知识传授和模型量化来平衡模型准确率和计算效率。知识传授可以使得模型尺寸减小，而不会影响准确率，而模型量化可以减少内存和计算需求。经过对一个公共数据集进行了广泛的测试，表明这个新的框架可以将模型尺寸减少三分之一，并提高推理时间1.4倍，相比之前优化的架构。代码可以在https://github.com/calvintanama/qd-driver-activity-reco中下载。
</details></li>
</ul>
<hr>
<h2 id="A-Neural-Height-Map-Approach-for-the-Binocular-Photometric-Stereo-Problem"><a href="#A-Neural-Height-Map-Approach-for-the-Binocular-Photometric-Stereo-Problem" class="headerlink" title="A Neural Height-Map Approach for the Binocular Photometric Stereo Problem"></a>A Neural Height-Map Approach for the Binocular Photometric Stereo Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05958">http://arxiv.org/abs/2311.05958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fotios Logothetis, Ignas Budvytis, Roberto Cipolla</li>
<li>for: 本文提出了一种新的、实用的双目光学深度测试（PS）框架，它的获取速度与单视PS相同，但是可以显著提高估算结果的geometry质量。</li>
<li>methods: 本文使用了深度学习来拟合表面和文本UREpresentation，通过最小化表面法向量偏差来实现形状估算。</li>
<li>results: 本文在DiLiGenT-MV数据集和LUCES-ST数据集上达到了状态对抗性表现，并且在binocular PS setup中实现了同样的获取速度和优化表达。<details>
<summary>Abstract</summary>
In this work we propose a novel, highly practical, binocular photometric stereo (PS) framework, which has same acquisition speed as single view PS, however significantly improves the quality of the estimated geometry.   As in recent neural multi-view shape estimation frameworks such as NeRF, SIREN and inverse graphics approaches to multi-view photometric stereo (e.g. PS-NeRF) we formulate shape estimation task as learning of a differentiable surface and texture representation by minimising surface normal discrepancy for normals estimated from multiple varying light images for two views as well as discrepancy between rendered surface intensity and observed images. Our method differs from typical multi-view shape estimation approaches in two key ways. First, our surface is represented not as a volume but as a neural heightmap where heights of points on a surface are computed by a deep neural network. Second, instead of predicting an average intensity as PS-NeRF or introducing lambertian material assumptions as Guo et al., we use a learnt BRDF and perform near-field per point intensity rendering.   Our method achieves the state-of-the-art performance on the DiLiGenT-MV dataset adapted to binocular stereo setup as well as a new binocular photometric stereo dataset - LUCES-ST.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种新的、高度实用的双目光学三角形（PS）框架，其具有与单视PS相同的获取速度，但可以显著提高估计几何的质量。我们的方法与现代神经网络多视图形态估计框架（如NeRF、SIREN和反射图像推导法）类似，将形态估计任务定义为通过最小化多个变化光图像中的法向缺失来学习可导表面和 текстура表示。我们的方法与传统多视图形态估计方法有两点不同：首先，我们的表面不是一个体积，而是一个深度神经网络中的高度图像；其次，我们不是预测平均Intensity，而是使用学习的BRDF进行靠近场near-field render。我们的方法在DiLiGenT-MV数据集中适配了双目掌控设置以及一个新的双目光学三角形数据集——LUCES-ST中达到了状态盘的性能。
</details></li>
</ul>
<hr>
<h2 id="Post-training-Quantization-with-Progressive-Calibration-and-Activation-Relaxing-for-Text-to-Image-Diffusion-Models"><a href="#Post-training-Quantization-with-Progressive-Calibration-and-Activation-Relaxing-for-Text-to-Image-Diffusion-Models" class="headerlink" title="Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models"></a>Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06322">http://arxiv.org/abs/2311.06322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siao Tang, Xin Wang, Hong Chen, Chaoyu Guan, Zewen Wu, Yansong Tang, Wenwu Zhu<br>for:This paper focuses on developing a novel post-training quantization method for text-to-image diffusion models, specifically targeting widely used large pretrained models like Stable Diffusion and Stable Diffusion XL.methods:The proposed method, called PCR (Progressive Calibration and Relaxing), consists of two key strategies: progressive calibration and activation relaxing. The former considers the accumulated quantization error across timesteps, while the latter improves performance with negligible cost.results:The proposed method and a new benchmark (QDiffBench) are extensively evaluated on Stable Diffusion and Stable Diffusion XL. The results show that the proposed method achieves superior performance and is the first to achieve quantization for Stable Diffusion XL while maintaining performance. Additionally, QDiffBench provides a more accurate evaluation of text-to-image diffusion model quantization by considering the distribution gap and generalization performance outside the calibration dataset.<details>
<summary>Abstract</summary>
Diffusion models have achieved great success due to their remarkable generation ability. However, their high computational overhead is still a troublesome problem. Recent studies have leveraged post-training quantization (PTQ) to compress diffusion models. However, most of them only focus on unconditional models, leaving the quantization of widely used large pretrained text-to-image models, e.g., Stable Diffusion, largely unexplored. In this paper, we propose a novel post-training quantization method PCR (Progressive Calibration and Relaxing) for text-to-image diffusion models, which consists of a progressive calibration strategy that considers the accumulated quantization error across timesteps, and an activation relaxing strategy that improves the performance with negligible cost. Additionally, we demonstrate the previous metrics for text-to-image diffusion model quantization are not accurate due to the distribution gap. To tackle the problem, we propose a novel QDiffBench benchmark, which utilizes data in the same domain for more accurate evaluation. Besides, QDiffBench also considers the generalization performance of the quantized model outside the calibration dataset. Extensive experiments on Stable Diffusion and Stable Diffusion XL demonstrate the superiority of our method and benchmark. Moreover, we are the first to achieve quantization for Stable Diffusion XL while maintaining the performance.
</details>
<details>
<summary>摘要</summary>
Diffusion 模型在生成能力方面已经取得了很大的成功，但是它们的计算开销仍然是一个痛苦的问题。latest studies have leveraged post-training quantization (PTQ) to compress diffusion models, but most of them only focus on unconditional models, leaving the quantization of widely used large pretrained text-to-image models, such as Stable Diffusion, largely unexplored. In this paper, we propose a novel post-training quantization method PCR (Progressive Calibration and Relaxing) for text-to-image diffusion models, which consists of a progressive calibration strategy that considers the accumulated quantization error across timesteps, and an activation relaxing strategy that improves the performance with negligible cost. Additionally, we demonstrate that the previous metrics for text-to-image diffusion model quantization are not accurate due to the distribution gap. To tackle this problem, we propose a novel QDiffBench benchmark, which utilizes data in the same domain for more accurate evaluation. Besides, QDiffBench also considers the generalization performance of the quantized model outside the calibration dataset. Extensive experiments on Stable Diffusion and Stable Diffusion XL demonstrate the superiority of our method and benchmark. Moreover, we are the first to achieve quantization for Stable Diffusion XL while maintaining the performance.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Segmentation-with-Texture-in-Ore-Images-Based-on-Box-supervised-Approach"><a href="#Efficient-Segmentation-with-Texture-in-Ore-Images-Based-on-Box-supervised-Approach" class="headerlink" title="Efficient Segmentation with Texture in Ore Images Based on Box-supervised Approach"></a>Efficient Segmentation with Texture in Ore Images Based on Box-supervised Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05929">http://arxiv.org/abs/2311.05929</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mvme-hbut/oreinst">https://github.com/mvme-hbut/oreinst</a></li>
<li>paper_authors: Guodong Sun, Delong Huang, Yuting Peng, Le Cheng, Bo Wu, Yang Zhang</li>
<li>For: The paper is written for image segmentation of crushed ores in a complex working environment, where high-powered computing equipment is difficult to deploy, and the ore distribution is stacked, making it challenging to identify complete features.* Methods: The proposed method uses a ghost feature pyramid network (Ghost-FPN) to process features obtained from the backbone, an optimized detection head to obtain accurate features, and a fusion feature similarity-based loss function that combines Lab color space (Lab) and local binary patterns (LBP) texture features to improve accuracy while incurring no loss.* Results: The proposed method achieves over 50 frames per second with a small model size of 21.6 MB, and maintains a high level of accuracy compared with state-of-the-art approaches on ore image datasets.Here is the information in Simplified Chinese text:* For: 该文章是为了处理受损矿石的图像分割，在复杂的工作环境下，高性能计算设备困难执行，矿石分布叠加，难以识别完整的特征。* 方法: 提出了一种使用 Ghost Feature Pyramid Network (Ghost-FPN) 处理从底层获得的特征，优化检测头以获得准确的特征，并将 Lab 色彩空间 (Lab) 和本地二进制模式 (LBP) 文本特征组合成一个 fusional 特征相似性基于损失函数，以提高准确性而不产生损失。* 结果: 提出的方法可以在 MS COCO 上达到更高于 50 帧&#x2F;秒的速度，并且在矿石图像数据集上保持高级别的准确性，而且与当前状态艺术方法相比，模型大小只有 21.6 MB。源代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/MVME-HBUT/OREINST%7D">https://github.com/MVME-HBUT/OREINST}</a> 上获取。<details>
<summary>Abstract</summary>
Image segmentation methods have been utilized to determine the particle size distribution of crushed ores. Due to the complex working environment, high-powered computing equipment is difficult to deploy. At the same time, the ore distribution is stacked, and it is difficult to identify the complete features. To address this issue, an effective box-supervised technique with texture features is provided for ore image segmentation that can identify complete and independent ores. Firstly, a ghost feature pyramid network (Ghost-FPN) is proposed to process the features obtained from the backbone to reduce redundant semantic information and computation generated by complex networks. Then, an optimized detection head is proposed to obtain the feature to maintain accuracy. Finally, Lab color space (Lab) and local binary patterns (LBP) texture features are combined to form a fusion feature similarity-based loss function to improve accuracy while incurring no loss. Experiments on MS COCO have shown that the proposed fusion features are also worth studying on other types of datasets. Extensive experimental results demonstrate the effectiveness of the proposed method, which achieves over 50 frames per second with a small model size of 21.6 MB. Meanwhile, the method maintains a high level of accuracy compared with the state-of-the-art approaches on ore image dataset. The source code is available at \url{https://github.com/MVME-HBUT/OREINST}.
</details>
<details>
<summary>摘要</summary>
Image segmentation方法已经在粉碎矿物中使用来确定粉碎物的大小分布。由于工作环境复杂，高功率计算设备困难提供。同时，矿物分布叠加，难以识别完整的特征。为解决这个问题，一种有效的盒子-监督法（Box-supervised）是提供了用于矿物图像分割的方法，可以识别完整独立的矿物。首先，一种鬼Feature pyramid网络（Ghost-FPN）是提出来处理来自后处理网络的特征，以减少复杂网络生成的重复semantic信息和计算。然后，一种优化的检测头是提出来，以获得维持准确性的特征。最后，Lab色彩空间（Lab）和本地二进制模式（LBP）的xture特征被组合以形成一个混合特征相似度基于的损失函数，以提高准确性而不损失一切。在MS COCO上进行了实验，表明提出的混合特征也值得进行其他类型的数据集上的研究。广泛的实验结果表明提出的方法的有效性，可以在20 frames/s的小型模型大小为21.6 MB下达到50 frames/s的性能水平，同时保持与当前最佳方法在矿物图像 dataset 的高级别准确性。源代码可以在 \url{https://github.com/MVME-HBUT/OREINST} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Automated-Sperm-Assessment-Framework-and-Neural-Network-Specialized-for-Sperm-Video-Recognition"><a href="#Automated-Sperm-Assessment-Framework-and-Neural-Network-Specialized-for-Sperm-Video-Recognition" class="headerlink" title="Automated Sperm Assessment Framework and Neural Network Specialized for Sperm Video Recognition"></a>Automated Sperm Assessment Framework and Neural Network Specialized for Sperm Video Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05927">http://arxiv.org/abs/2311.05927</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ftkr12/rostfine">https://github.com/ftkr12/rostfine</a></li>
<li>paper_authors: Takuro Fujii, Hayato Nakagawa, Teppei Takeshima, Yasushi Yumura, Tomoki Hamagami</li>
<li>for: 本研究旨在提高受孕助手技术的成功率，通过对精子评估使用深度学习方法进行改进。</li>
<li>methods: 本研究使用了视频数据集，其中包括精子头、 neck 和 tail 的视频，并使用了软标签来标注数据。提出了基于视频认识的精子评估框架和 neural network 模型 RoSTFine。</li>
<li>results: 实验结果表明，RoSTFine 可以提高精子评估性能，并强调重要的精子部分（即头和 neck）。<details>
<summary>Abstract</summary>
Infertility is a global health problem, and an increasing number of couples are seeking medical assistance to achieve reproduction, at least half of which are caused by men. The success rate of assisted reproductive technologies depends on sperm assessment, in which experts determine whether sperm can be used for reproduction based on morphology and motility of sperm. Previous sperm assessment studies with deep learning have used datasets comprising images that include only sperm heads, which cannot consider motility and other morphologies of sperm. Furthermore, the labels of the dataset are one-hot, which provides insufficient support for experts, because assessment results are inconsistent between experts, and they have no absolute answer. Therefore, we constructed the video dataset for sperm assessment whose videos include sperm head as well as neck and tail, and its labels were annotated with soft-label. Furthermore, we proposed the sperm assessment framework and the neural network, RoSTFine, for sperm video recognition. Experimental results showed that RoSTFine could improve the sperm assessment performances compared to existing video recognition models and focus strongly on important sperm parts (i.e., head and neck).
</details>
<details>
<summary>摘要</summary>
世界各地有增加的 couples 为了成婚而寻求医疗帮助，至少有一半是由男方引起的不孕。协助生殖技术的成功率取决于精子评估，专家们通过精子形态和运动能力来决定精子是否适用于生殖。之前的精子评估研究使用深度学习都使用了只包含精子头部的图像集合，这无法考虑精子的运动和其他形态。此外，数据集的标签都是一元化的，这不足以支持专家，因为评估结果存在差异 между 专家，并没有绝对的答案。因此，我们建立了包含精子头部、脖子和尾部的视频数据集，并使用了软标签来标注数据集。此外，我们提出了精子评估框架和基于视频的神经网络模型 RoSTFine，用于精子视频识别。实验结果表明，RoSTFine 可以提高精子评估性能，并强调精子重要部分（即头和脖子）。
</details></li>
</ul>
<hr>
<h2 id="Inter-object-Discriminative-Graph-Modeling-for-Indoor-Scene-Recognition"><a href="#Inter-object-Discriminative-Graph-Modeling-for-Indoor-Scene-Recognition" class="headerlink" title="Inter-object Discriminative Graph Modeling for Indoor Scene Recognition"></a>Inter-object Discriminative Graph Modeling for Indoor Scene Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05919">http://arxiv.org/abs/2311.05919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanxin Song, Hanbo Wu, Xin Ma<br>for: This paper focuses on improving indoor scene recognition by leveraging object information within scenes to enhance feature representations.methods: The proposed approach uses a probabilistic perspective to capture object-scene discriminative relationships, which are then transformed into an Inter-Object Discriminative Prototype (IODP). The Discriminative Graph Network (DGN) is constructed to incorporate inter-object discriminative knowledge into the image representation through graph convolution.results: The proposed approach achieves state-of-the-art results on several widely used scene datasets, demonstrating the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
Variable scene layouts and coexisting objects across scenes make indoor scene recognition still a challenging task. Leveraging object information within scenes to enhance the distinguishability of feature representations has emerged as a key approach in this domain. Currently, most object-assisted methods use a separate branch to process object information, combining object and scene features heuristically. However, few of them pay attention to interpretably handle the hidden discriminative knowledge within object information. In this paper, we propose to leverage discriminative object knowledge to enhance scene feature representations. Initially, we capture the object-scene discriminative relationships from a probabilistic perspective, which are transformed into an Inter-Object Discriminative Prototype (IODP). Given the abundant prior knowledge from IODP, we subsequently construct a Discriminative Graph Network (DGN), in which pixel-level scene features are defined as nodes and the discriminative relationships between node features are encoded as edges. DGN aims to incorporate inter-object discriminative knowledge into the image representation through graph convolution. With the proposed IODP and DGN, we obtain state-of-the-art results on several widely used scene datasets, demonstrating the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
<<SYS>>变量场景布局和场景中的对象共存，indoor场景认知仍然是一项挑战性任务。利用场景中对象信息来增强特征表示的方法已经成为indoor场景认知领域的关键方法。现有大多数对象协助方法使用分立支线处理对象信息，混合对象和场景特征的方式。然而，其中很少听从解释地处理隐藏的推理知识。在本文中，我们提议利用隐藏的推理知识来增强场景特征表示。首先，我们从概率角度捕捉对象-场景推理关系，并将其转化为间对象推理原型（IODP）。在IODP的丰富先验知识基础上，我们随后建立一个推理图网络（DGN），其中像素级场景特征被定义为节点，图中的节点间的推理关系被编码为边。DGN的目标是通过图 convolution来将间对象推理知识integrated到图像表示中。与我们提议的IODP和DGN，我们在多个常用的场景数据集上获得了state-of-the-art的结果，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Map-Guided-Synthesis-of-Wireless-Capsule-Endoscopy-Images-using-Diffusion-Models"><a href="#Semantic-Map-Guided-Synthesis-of-Wireless-Capsule-Endoscopy-Images-using-Diffusion-Models" class="headerlink" title="Semantic Map Guided Synthesis of Wireless Capsule Endoscopy Images using Diffusion Models"></a>Semantic Map Guided Synthesis of Wireless Capsule Endoscopy Images using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05889">http://arxiv.org/abs/2311.05889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haejin Lee, Jeongwoo Ju, Jonghyuck Lee, Yeoun Joo Lee, Heechul Jung</li>
<li>for: 该研究旨在提高无线填充内scopic检查（WCE）结果的解释效率，并提供更多和更多样式的WCE图像，以便更好地诊断肠道疾病。</li>
<li>methods: 该研究使用生成模型，具体来说是分散模型（DM），生成多样化的WCE图像。该模型还利用视觉化缩放引擎（VS）生成的semantic map，以提高生成图像的控制性和多样性。</li>
<li>results: 该研究通过视觉检查和视觉图灵测试，证明了该方法的效果，可以生成真实和多样化的WCE图像。<details>
<summary>Abstract</summary>
Wireless capsule endoscopy (WCE) is a non-invasive method for visualizing the gastrointestinal (GI) tract, crucial for diagnosing GI tract diseases. However, interpreting WCE results can be time-consuming and tiring. Existing studies have employed deep neural networks (DNNs) for automatic GI tract lesion detection, but acquiring sufficient training examples, particularly due to privacy concerns, remains a challenge. Public WCE databases lack diversity and quantity. To address this, we propose a novel approach leveraging generative models, specifically the diffusion model (DM), for generating diverse WCE images. Our model incorporates semantic map resulted from visualization scale (VS) engine, enhancing the controllability and diversity of generated images. We evaluate our approach using visual inspection and visual Turing tests, demonstrating its effectiveness in generating realistic and diverse WCE images.
</details>
<details>
<summary>摘要</summary>
无线胶囊内视镜（WCE）是一种非侵入性的方法，用于观察肠道系统，对肠道疾病的诊断非常重要。然而，解读WCE结果可以是时间consuming和疲劳的。现有的研究已经使用深度神经网络（DNNs）自动检测肠道病变，但获得充分的训练样本，特别是由于隐私问题，仍然是一个挑战。公共WCE数据库缺乏多样性和数量。为解决这个问题，我们提出了一种新的方法，利用生成模型（DM）生成多样的WCE图像。我们的模型具有视觉化缩放引擎（VS）生成的semantic map，从而提高生成图像的可控性和多样性。我们通过视觉检查和视觉图灵测试评估了我们的方法，并证明其效果在生成真实和多样的WCE图像。
</details></li>
</ul>
<hr>
<h2 id="Central-Angle-Optimization-for-360-degree-Holographic-3D-Content"><a href="#Central-Angle-Optimization-for-360-degree-Holographic-3D-Content" class="headerlink" title="Central Angle Optimization for 360-degree Holographic 3D Content"></a>Central Angle Optimization for 360-degree Holographic 3D Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05878">http://arxiv.org/abs/2311.05878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hakdong Kim, Minsung Yoon, Cheongwon Kim</li>
<li>for: 这个论文是为了提出一种用于深度学习基于深度地图估计来生成真实的投影内容的方法。</li>
<li>methods: 该方法使用了对邻近摄像头视角点的中心角值进行分析，以选择最佳的中心角，以生成高质量的投影内容。</li>
<li>results: 经验表明，选择最佳中心角可以提高投影内容的质量。<details>
<summary>Abstract</summary>
In this study, we propose a method to find an optimal central angle in deep learning-based depth map estimation used to produce realistic holographic content. The acquisition of RGB-depth map images as detailed as possible must be performed to generate holograms of high quality, despite the high computational cost. Therefore, we introduce a novel pipeline designed to analyze various values of central angles between adjacent camera viewpoints equidistant from the origin of an object-centered environment. Then we propose the optimal central angle to generate high-quality holographic content. The proposed pipeline comprises key steps such as comparing estimated depth maps and comparing reconstructed CGHs (Computer-Generated Holograms) from RGB images and estimated depth maps. We experimentally demonstrate and discuss the relationship between the central angle and the quality of digital holographic content.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一种方法来找出深度学习基于深度地图估计的优化中心角，以生成真实的投射内容。为了生成高质量的投射，需要获取RGB-深度地图图像，这些图像需要尽可能详细，但计算成本高。因此，我们提出了一个新的管道，用于分析不同中心角之间的RGB-深度地图图像。然后，我们提出了优化中心角，以生成高质量的投射内容。该管道包括以下关键步骤：对RGB图像和估计的深度地图进行比较，并对计算机生成的投射（CGH）和估计的深度地图进行比较。我们在实验中证明并讨论了中心角与数字投射内容质量之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Automated-Heterogeneous-Low-Bit-Quantization-of-Multi-Model-Deep-Learning-Inference-Pipeline"><a href="#Automated-Heterogeneous-Low-Bit-Quantization-of-Multi-Model-Deep-Learning-Inference-Pipeline" class="headerlink" title="Automated Heterogeneous Low-Bit Quantization of Multi-Model Deep Learning Inference Pipeline"></a>Automated Heterogeneous Low-Bit Quantization of Multi-Model Deep Learning Inference Pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05870">http://arxiv.org/abs/2311.05870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayeeta Mondal, Swarnava Dey, Arijit Mukherjee</li>
<li>for: 这个论文是为了提出一种自动化多层神经网络（多个DNN）的量化方法，以便在边缘部署中实现精度-延迟平衡。</li>
<li>methods: 该论文使用了多种深度学习（DL）推理管线，包括多任务学习（MTL）和集成学习（EL）等，以提高模型的准确率。</li>
<li>results: 该论文通过自动化量化方法，实现了多个DNNs的精度-延迟平衡，并且提高了边缘部署中的模型性能。<details>
<summary>Abstract</summary>
Multiple Deep Neural Networks (DNNs) integrated into single Deep Learning (DL) inference pipelines e.g. Multi-Task Learning (MTL) or Ensemble Learning (EL), etc., albeit very accurate, pose challenges for edge deployment. In these systems, models vary in their quantization tolerance and resource demands, requiring meticulous tuning for accuracy-latency balance. This paper introduces an automated heterogeneous quantization approach for DL inference pipelines with multiple DNNs.
</details>
<details>
<summary>摘要</summary>
多层神经网络（DNN）组合在单个深度学习（DL）推理管道中，例如多任务学习（MTL）或集成学习（EL）等，虽然非常准确，但对边缘部署带来挑战。这些系统中的模型异常量化忍耐和资源需求，需要精确地调整以实现准确率和延迟之间的平衡。本文介绍了一种自动化多类量化方法 для DL推理管道中的多个DNN。
</details></li>
</ul>
<hr>
<h2 id="Watermarking-Vision-Language-Pre-trained-Models-for-Multi-modal-Embedding-as-a-Service"><a href="#Watermarking-Vision-Language-Pre-trained-Models-for-Multi-modal-Embedding-as-a-Service" class="headerlink" title="Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service"></a>Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05863">http://arxiv.org/abs/2311.05863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Pter61/vlpmarker">https://github.com/Pter61/vlpmarker</a></li>
<li>paper_authors: Yuanmin Tang, Jing Yu, Keke Gai, Xiangyan Qu, Yue Hu, Gang Xiong, Qi Wu<br>for: 这个研究是为了提供一个安全和可靠的版权标识方法来防止模型EXTRACTION攻击，以保护运算在多媒体Embedding as a Service（EaaS）上的知识产权和商业所有权。methods: 本研究使用了附加 trigger 的方法来将版权标识Inserted into VLPs，并通过嵌入式扩展Transformation来实现高质量的版权验证和最小化模型性能影响。此外，我们还提出了一种协力Copyright验证策略，通过融合 triggers和嵌入分布来增强标识的可靠性，抵抗不同的攻击。results: 我们的实验结果显示，提出的版权标识方法是有效和安全的，可以在不同的数据集上验证VLPs的版权，并对于模型EXTRACTION攻击进行防护。此外，我们还提出了一种可行的Out-of-distribution trigger选择方法，使得版权标识可以在实际世界中进行实现。<details>
<summary>Abstract</summary>
Recent advances in vision-language pre-trained models (VLPs) have significantly increased visual understanding and cross-modal analysis capabilities. Companies have emerged to provide multi-modal Embedding as a Service (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount of training data and resources for high-performance service. However, existing studies indicate that EaaS is vulnerable to model extraction attacks that induce great loss for the owners of VLPs. Protecting the intellectual property and commercial ownership of VLPs is increasingly crucial yet challenging. A major solution of watermarking model for EaaS implants a backdoor in the model by inserting verifiable trigger embeddings into texts, but it is only applicable for large language models and is unrealistic due to data and model privacy. In this paper, we propose a safe and robust backdoor-based embedding watermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding orthogonal transformation to effectively inject triggers into the VLPs without interfering with the model parameters, which achieves high-quality copyright verification and minimal impact on model performance. To enhance the watermark robustness, we further propose a collaborative copyright verification strategy based on both backdoor trigger and embedding distribution, enhancing resilience against various attacks. We increase the watermark practicality via an out-of-distribution trigger selection approach, removing access to the model training data and thus making it possible for many real-world scenarios. Our extensive experiments on various datasets indicate that the proposed watermarking approach is effective and safe for verifying the copyright of VLPs for multi-modal EaaS and robust against model extraction attacks. Our code is available at https://github.com/Pter61/vlpmarker.
</details>
<details>
<summary>摘要</summary>
近期，视觉语言预训模型（VLP）的进步已经提高了视觉理解和跨模态分析的能力。企业出现了基于VLP的多Modal Embedding as a Service（EaaS），但是这需要大量的训练数据和资源来提供高性能服务。然而，现有研究表明，EaaS受到模型抽取攻击，这会导致VLP的所有者受到很大的损失。保护VLP的知识产权和商业所有权是一项 increasinly 杰出的任务，但是它具有挑战。在这篇论文中，我们提出了一种安全和可靠的VLP embedding水印方法，称为VLPMarker。VLPMarker利用Embedding ortogonal transformation来有效地插入触发器到VLP中，而不会对模型参数产生影响，从而实现高质量的版权验证和最小的影响。为增强水印鲜度，我们进一步提出了基于触发器和embedding分布的共同版权验证策略，提高了对各种攻击的抗性。此外，我们还提出了一种基于非典型触发器的选择方法，使得水印更加实用。我们的实验表明，提议的水印方法是安全和可靠的，可以用于验证VLP的版权在多Modal EaaS中。我们的代码可以在https://github.com/Pter61/vlpmarker上下载。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-by-Learning-from-Privileged-Medical-Imaging-Information"><a href="#Domain-Generalization-by-Learning-from-Privileged-Medical-Imaging-Information" class="headerlink" title="Domain Generalization by Learning from Privileged Medical Imaging Information"></a>Domain Generalization by Learning from Privileged Medical Imaging Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05861">http://arxiv.org/abs/2311.05861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Korevaar, Ruwan Tennakoon, Ricky O’Brien, Dwarikanath Mahapatra, Alireza Bab-Hadiasha</li>
<li>for: 这种研究旨在提高医疗图像分类模型对数据分布变化的适应能力。</li>
<li>methods: 作者提出了一种新的方法，即利用特权信息（如肿体形态或位置）来强化领域泛化能力。</li>
<li>results: 研究表明，使用特权信息可以提高医疗图像分类模型对outsider数据的分类精度，从0.911提高到0.934。<details>
<summary>Abstract</summary>
Learning the ability to generalize knowledge between similar contexts is particularly important in medical imaging as data distributions can shift substantially from one hospital to another, or even from one machine to another. To strengthen generalization, most state-of-the-art techniques inject knowledge of the data distribution shifts by enforcing constraints on learned features or regularizing parameters. We offer an alternative approach: Learning from Privileged Medical Imaging Information (LPMII). We show that using some privileged information such as tumor shape or location leads to stronger domain generalization ability than current state-of-the-art techniques. This paper demonstrates that by using privileged information to predict the severity of intra-layer retinal fluid in optical coherence tomography scans, the classification accuracy of a deep learning model operating on out-of-distribution data improves from $0.911$ to $0.934$. This paper provides a strong starting point for using privileged information in other medical problems requiring generalization.
</details>
<details>
<summary>摘要</summary>
学习在类似上下文中总结知识的能力对医疗成像非常重要，因为数据分布可能在不同医院或机器之间差异很大。为强化总结，大多数当前领先技术会在学习特征或参数上强制加入数据分布偏移的约束。我们提出了一种不同的方法：使用特权医疗成像信息学习（LPMII）。我们表明，使用特权信息，如肿瘤形态或位置，可以增强领域总结能力，比现有领先技术更高。这篇论文展示了，通过使用特权信息预测optical coherence tomography扫描中内层血液的严重程度，深度学习模型在不同数据上的分类精度从0.911提高到0.934。这篇论文提供了使用特权信息在医疗问题中的强大起点。
</details></li>
</ul>
<hr>
<h2 id="Layer-wise-Auto-Weighting-for-Non-Stationary-Test-Time-Adaptation"><a href="#Layer-wise-Auto-Weighting-for-Non-Stationary-Test-Time-Adaptation" class="headerlink" title="Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation"></a>Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05858">http://arxiv.org/abs/2311.05858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junia3/LayerwiseTTA">https://github.com/junia3/LayerwiseTTA</a></li>
<li>paper_authors: Junyoung Park, Jin Kim, Hyeongjun Kwon, Ilhoon Yoon, Kwanghoon Sohn</li>
<li>for: 这篇论文主要关注于在实际应用中进行模型更新和适应，并且面临不断变化的目标分布问题。</li>
<li>methods: 本文提出了一个层别自动调整算法，通过利用渔业信息矩阵（FIM）设计学习重量，以选择相关于对数据量变化的层而忽略不相关的层。此外，本文还提出了一个对数矩阵对应的幂函数减少器，以使certain层几乎冻结，以减少忘记和错误累累。</li>
<li>results: 实验结果显示，本文的方法比传统的连续和慢速更新方法更好，同时可以很大程度地降低计算负载，强调了FIM-based learning weight在适应持续变化的目标分布方面的重要性。<details>
<summary>Abstract</summary>
Given the inevitability of domain shifts during inference in real-world applications, test-time adaptation (TTA) is essential for model adaptation after deployment. However, the real-world scenario of continuously changing target distributions presents challenges including catastrophic forgetting and error accumulation. Existing TTA methods for non-stationary domain shifts, while effective, incur excessive computational load, making them impractical for on-device settings. In this paper, we introduce a layer-wise auto-weighting algorithm for continual and gradual TTA that autonomously identifies layers for preservation or concentrated adaptation. By leveraging the Fisher Information Matrix (FIM), we first design the learning weight to selectively focus on layers associated with log-likelihood changes while preserving unrelated ones. Then, we further propose an exponential min-max scaler to make certain layers nearly frozen while mitigating outliers. This minimizes forgetting and error accumulation, leading to efficient adaptation to non-stationary target distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our method outperforms conventional continual and gradual TTA approaches while significantly reducing computational load, highlighting the importance of FIM-based learning weight in adapting to continuously or gradually shifting target domains.
</details>
<details>
<summary>摘要</summary>
（注：以下是简化中文版本）随着实际应用中数据分布的不断变化，测试时间适应（TTA）在部署后是必需的。然而，实际中的目标分布不断变化带来了悬峰忘却和错误积累的挑战。现有的TTA方法对非站立性目标分布非常有效，但是 computational load 过高，使其无法实现在设备上进行。在这篇论文中，我们提出了一种层 wise auto-weighting 算法，用于逐渐和积极地适应非站立性目标分布。我们首先通过 Fisher Information Matrix (FIM) 设计学习权重，以选择与 log-likelihood 变化相关的层，并保留不相关的层。然后，我们进一步提出了一种对数抑制器，使certain层变得几乎冻结，并 Mitigate 异常值。这有效地减少了忘却和错误积累，从而实现了高效地适应非站立性目标分布。我们的方法在 CIFAR-10C、CIFAR-100C 和 ImageNet-C 上进行了实验，并证明了我们的方法在不断变化的目标分布下对 TTA 进行了改进，并且可以减少计算负担，强调 FIM 基于的学习权重在适应不断变化的目标分布中的重要性。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Single-View-Volumetric-Rendering-for-Medical-Neural-Radiance-Fields"><a href="#Uncertainty-aware-Single-View-Volumetric-Rendering-for-Medical-Neural-Radiance-Fields" class="headerlink" title="Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields"></a>Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05836">http://arxiv.org/abs/2311.05836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Hu, Qinrui Fan, Shu Hu, Siwei Lyu, Xi Wu, Xin Wang</li>
<li>for: 本研究旨在提出一种基于生成辐射场的不确定性意识MedNeRF网络，以便从2DX射影图像中学习CT投影图像的连续表示。</li>
<li>methods: 该网络使用生成辐射场来获取内部结构和深度信息，并使用适应性损失量来保证生成图像的质量。</li>
<li>results: 我们在公共可用的膝盖和胸部数据集上训练了我们的模型，并对单个X射影图像进行CT投影图像的Rendering，并与其他基于生成辐射场的方法进行比较。<details>
<summary>Abstract</summary>
In the field of clinical medicine, computed tomography (CT) is an effective medical imaging modality for the diagnosis of various pathologies. Compared with X-ray images, CT images can provide more information, including multi-planar slices and three-dimensional structures for clinical diagnosis. However, CT imaging requires patients to be exposed to large doses of ionizing radiation for a long time, which may cause irreversible physical harm. In this paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on generated radiation fields. The network can learn a continuous representation of CT projections from 2D X-ray images by obtaining the internal structure and depth information and using adaptive loss weights to ensure the quality of the generated images. Our model is trained on publicly available knee and chest datasets, and we show the results of CT projection rendering with a single X-ray and compare our method with other methods based on generated radiation fields.
</details>
<details>
<summary>摘要</summary>
在临床医学领域，计算机断层成像（CT）是一种有效的医疗影像Modalities，用于诊断多种疾病。相比X射线图像，CT图像可以提供更多的信息，包括多平面切片和三维结构，为临床诊断提供更多的参考。然而，CT成像需要患者长时间暴露于大剂量辐射，可能会导致不可逆的物理损害。在本文中，我们提出了基于生成辐射场的不确定性意识MedNeRF（UMedNeRF）网络。该网络可以通过获取内部结构和深度信息，从2D X射线图像中生成CT投影图像，并使用适应损失质量来保证生成图像质量。我们的模型在公共可用的膝盖和胸部数据集上进行训练，并对CT投影图像的生成进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Shape-Prior-for-Wrinkle-Accurate-Cloth-Registration"><a href="#Diffusion-Shape-Prior-for-Wrinkle-Accurate-Cloth-Registration" class="headerlink" title="Diffusion Shape Prior for Wrinkle-Accurate Cloth Registration"></a>Diffusion Shape Prior for Wrinkle-Accurate Cloth Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05828">http://arxiv.org/abs/2311.05828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingfan Guo, Fabian Prada, Donglai Xiang, Javier Romero, Chenglei Wu, Hyun Soo Park, Takaaki Shiratori, Shunsuke Saito</li>
<li>for: 用于实现基于实际数据的动态外观模型和物理参数估计</li>
<li>methods: 使用 diffusion models 学习shape prior，并提出基于函数图的多个阶段引导方案来稳定注registrations</li>
<li>results: 在高精度捕捉到的实际衣服上，提出的方法比VAE或PCA基于的surface registration更好地泛化，并在扩展和减少扩展测试中都能够超越优化基于和学习基于的非rigid registration方法。<details>
<summary>Abstract</summary>
Registering clothes from 4D scans with vertex-accurate correspondence is challenging, yet important for dynamic appearance modeling and physics parameter estimation from real-world data. However, previous methods either rely on texture information, which is not always reliable, or achieve only coarse-level alignment. In this work, we present a novel approach to enabling accurate surface registration of texture-less clothes with large deformation. Our key idea is to effectively leverage a shape prior learned from pre-captured clothing using diffusion models. We also propose a multi-stage guidance scheme based on learned functional maps, which stabilizes registration for large-scale deformation even when they vary significantly from training data. Using high-fidelity real captured clothes, our experiments show that the proposed approach based on diffusion models generalizes better than surface registration with VAE or PCA-based priors, outperforming both optimization-based and learning-based non-rigid registration methods for both interpolation and extrapolation tests.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文本翻译成简化中文。<</SYS>>注册Textureless clothes from 4D scans with vertex-accurate correspondence is challenging, yet important for dynamic appearance modeling and physics parameter estimation from real-world data. However, previous methods either rely on texture information, which is not always reliable, or achieve only coarse-level alignment. In this work, we present a novel approach to enabling accurate surface registration of texture-less clothes with large deformation. Our key idea is to effectively leverage a shape prior learned from pre-captured clothing using diffusion models. We also propose a multi-stage guidance scheme based on learned functional maps, which stabilizes registration for large-scale deformation even when they vary significantly from training data. Using high-fidelity real captured clothes, our experiments show that the proposed approach based on diffusion models generalizes better than surface registration with VAE or PCA-based priors, outperforming both optimization-based and learning-based non-rigid registration methods for both interpolation and extrapolation tests.Note: The translation is done using Google Translate, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Variance-Thresholding-A-Novel-Approach-to-Improve-Existing-Deep-Transfer-Vision-Models-and-Advance-Automatic-Knee-Joint-Osteoarthritis-Classification"><a href="#Adaptive-Variance-Thresholding-A-Novel-Approach-to-Improve-Existing-Deep-Transfer-Vision-Models-and-Advance-Automatic-Knee-Joint-Osteoarthritis-Classification" class="headerlink" title="Adaptive Variance Thresholding: A Novel Approach to Improve Existing Deep Transfer Vision Models and Advance Automatic Knee-Joint Osteoarthritis Classification"></a>Adaptive Variance Thresholding: A Novel Approach to Improve Existing Deep Transfer Vision Models and Advance Automatic Knee-Joint Osteoarthritis Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05799">http://arxiv.org/abs/2311.05799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabi Prezja, Leevi Annala, Sampsa Kiiskinen, Suvi Lahtinen, Timo Ojala</li>
<li>for: 本研究旨在提高骨关节风溃病（KOA）的诊断精度，通过应用深度学习方法和自适应变量阈值控制（AVT）、神经建构搜索（NAS）等技术。</li>
<li>methods: 本研究使用的方法包括深度学习模型的预训练和特点化 Variance Thresholding（AVT）、Neural Architecture Search（NAS）等。</li>
<li>results: 本研究的结果表明，通过应用我们的方法，可以提高预训练KOA模型的初始准确率，并将NAS输入向量空间减少60倍，从而提高推理速度和优化超参数搜索。此外，我们还应用了这种方法于一个外部已经训练的KOA分类模型，并得到了较好的效果，使其成为骨关节风溃病分类模型之一。<details>
<summary>Abstract</summary>
Knee-Joint Osteoarthritis (KOA) is a prevalent cause of global disability and is inherently complex to diagnose due to its subtle radiographic markers and individualized progression. One promising classification avenue involves applying deep learning methods; however, these techniques demand extensive, diversified datasets, which pose substantial challenges due to medical data collection restrictions. Existing practices typically resort to smaller datasets and transfer learning. However, this approach often inherits unnecessary pre-learned features that can clutter the classifier's vector space, potentially hampering performance. This study proposes a novel paradigm for improving post-training specialized classifiers by introducing adaptive variance thresholding (AVT) followed by Neural Architecture Search (NAS). This approach led to two key outcomes: an increase in the initial accuracy of the pre-trained KOA models and a 60-fold reduction in the NAS input vector space, thus facilitating faster inference speed and a more efficient hyperparameter search. We also applied this approach to an external model trained for KOA classification. Despite its initial performance, the application of our methodology improved its average accuracy, making it one of the top three KOA classification models.
</details>
<details>
<summary>摘要</summary>
膝关节骨关节炎 (KOA) 是全球最常见的残疾原因之一，而其诊断却因为它的微不足和个人化进程而被认为是复杂的。深度学习技术可能会有所助益，但这些技术需要大量多样化的数据集，医疗数据收集限制成为了主要挑战。现有的做法通常是使用更小的数据集和转移学习。然而，这种方法可能会固化预先学习的特征，从而降低表现。本研究提出了一种改进后期特殊化分类器的新方法，通过适应差异阈值调整 (AVT) 和神经网络搜索 (NAS)。这种方法导致了两个关键的结果：首先，提高了预训练 KOA 模型的初始精度；其次，将 NAS 输入向量空间减少到 60 倍，从而提高了推理速度和搜索效率。我们还应用了这种方法于一个外部用于 KOA 分类的模型。尽管它的初始表现不佳，但通过我们的方法改进，其平均精度得到了提高，成为了 KOA 分类模型之一。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Bidirectional-Temporal-States-of-Knee-Osteoarthritis-Radiographs-with-Cycle-Consistent-Generative-Adversarial-Neural-Networks"><a href="#Synthesizing-Bidirectional-Temporal-States-of-Knee-Osteoarthritis-Radiographs-with-Cycle-Consistent-Generative-Adversarial-Neural-Networks" class="headerlink" title="Synthesizing Bidirectional Temporal States of Knee Osteoarthritis Radiographs with Cycle-Consistent Generative Adversarial Neural Networks"></a>Synthesizing Bidirectional Temporal States of Knee Osteoarthritis Radiographs with Cycle-Consistent Generative Adversarial Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05798">http://arxiv.org/abs/2311.05798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabi Prezja, Leevi Annala, Sampsa Kiiskinen, Suvi Lahtinen, Timo Ojala</li>
<li>for: 预测患者患有满月股骨骨折病（KOA）的可能性，增强数据采集和预测模型训练。</li>
<li>methods: 使用CycleGAN模型将真实的X光图像扩展到不同的KOA阶段，并通过验证使用Convolutional Neural Network（CNN）来证明模型的可靠性。</li>
<li>results: 模型能够有效地将病例阶段转换为不同的阶段，特别是将晚期病例阶段转换为早期阶段，并且能够抑制骨质增生和扩大膝关节空间，这些特征都是早期KOA的典型表现。<details>
<summary>Abstract</summary>
Knee Osteoarthritis (KOA), a leading cause of disability worldwide, is challenging to detect early due to subtle radiographic indicators. Diverse, extensive datasets are needed but are challenging to compile because of privacy, data collection limitations, and the progressive nature of KOA. However, a model capable of projecting genuine radiographs into different OA stages could augment data pools, enhance algorithm training, and offer pre-emptive prognostic insights. In this study, we trained a CycleGAN model to synthesize past and future stages of KOA on any genuine radiograph. The model was validated using a Convolutional Neural Network that was deceived into misclassifying disease stages in transformed images, demonstrating the CycleGAN's ability to effectively transform disease characteristics forward or backward in time. The model was particularly effective in synthesizing future disease states and showed an exceptional ability to retroactively transition late-stage radiographs to earlier stages by eliminating osteophytes and expanding knee joint space, signature characteristics of None or Doubtful KOA. The model's results signify a promising potential for enhancing diagnostic models, data augmentation, and educational and prognostic usage in healthcare. Nevertheless, further refinement, validation, and a broader evaluation process encompassing both CNN-based assessments and expert medical feedback are emphasized for future research and development.
</details>
<details>
<summary>摘要</summary>
髋关节滤出病 (KOA) 是全球最主要的残疾原因之一，但早期检测困难由于病理表像不具有明显的特征。收集延伸的数据集是困难的，主要因为隐私、数据收集限制和滤出病的进行性。然而，一种能将真实的X光像投影到不同的滤出病阶段的模型可以增加数据库，提高算法训练和提供预防性预测。本研究中，我们使用了循环GAN模型将过去和未来的滤出病阶段投影到任何真实的X光像上。我们验证了这种模型，使用了一个 convolutional neural network (CNN) 被欺骗到在转换后的图像中错误地分类病种特征，表明循环GAN模型可以有效地将病种特征转换到不同的时间阶段。特别是在将未来的病状投影到当前阶段的情况下，模型表现出了极高的效果。此外，模型还可以逆转晚期X光像，使其变回早期阶段，这是 none 或 doubtful KOA 的特征之一。这些结果表明这种模型在改善诊断模型、数据增强和教学和预测方面具有普遍的潜力。然而，进一步的优化、验证和更广泛的评估过程，包括使用 CNN 基础的评估和专业医疗反馈，是未来研究和开发的重点。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/10/cs.CV_2023_11_10/" data-id="clpahu74g00n43h88bxdua69i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/10/cs.AI_2023_11_10/" class="article-date">
  <time datetime="2023-11-10T12:00:00.000Z" itemprop="datePublished">2023-11-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/10/cs.AI_2023_11_10/">cs.AI - 2023-11-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Testing-LLMs-on-Code-Generation-with-Varying-Levels-of-Prompt-Specificity"><a href="#Testing-LLMs-on-Code-Generation-with-Varying-Levels-of-Prompt-Specificity" class="headerlink" title="Testing LLMs on Code Generation with Varying Levels of Prompt Specificity"></a>Testing LLMs on Code Generation with Varying Levels of Prompt Specificity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07599">http://arxiv.org/abs/2311.07599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lincoln Murr, Morgan Grainger, David Gao</li>
<li>for: 这个论文主要用于研究大自然语言模型（LLM）在自动代码生成方面的表现，以及不同提问精度对代码生成的影响。</li>
<li>methods: 本论文使用了多种大自然语言模型（LLM），如Bard、ChatGPT-3.5、ChatGPT-4和Claude-2，对 Python 编程问题进行自动代码生成。研究者使用了 104 个编程问题，每个问题有四种提问类型，以不同的测试和精度来评估代码的准确率、时间效率和空间效率。</li>
<li>results: 研究结果表明不同的 LLM 和提问类型之间存在显著的性能差异，而且提问精度对代码生成的准确率和时间效率有重要的影响。本研究的重要贡献在于找到了最佳提问策略，以便在自动代码生成任务中创造准确的 Python 函数。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated unparalleled prowess in mimicking human-like text generation and processing. Among the myriad of applications that benefit from LLMs, automated code generation is increasingly promising. The potential to transform natural language prompts into executable code promises a major shift in software development practices and paves the way for significant reductions in manual coding efforts and the likelihood of human-induced errors. This paper reports the results of a study that evaluates the performance of various LLMs, such as Bard, ChatGPT-3.5, ChatGPT-4, and Claude-2, in generating Python for coding problems. We focus on how levels of prompt specificity impact the accuracy, time efficiency, and space efficiency of the generated code. A benchmark of 104 coding problems, each with four types of prompts with varying degrees of tests and specificity, was employed to examine these aspects comprehensively. Our results indicate significant variations in performance across different LLMs and prompt types, and its key contribution is to reveal the ideal prompting strategy for creating accurate Python functions. This study lays the groundwork for further research in LLM capabilities and suggests practical implications for utilizing LLMs in automated code generation tasks and test-driven development.
</details>
<details>
<summary>摘要</summary>
This study evaluates the performance of several LLMs, including Bard, ChatGPT-3.5, ChatGPT-4, and Claude-2, in generating Python code for coding problems. We focus on how the level of specificity in the prompts affects the accuracy, time efficiency, and space efficiency of the generated code. To examine these aspects comprehensively, we used a benchmark of 104 coding problems, each with four types of prompts with varying degrees of tests and specificity.Our results show significant variations in performance across different LLMs and prompt types. The study's key contribution is revealing the ideal prompting strategy for creating accurate Python functions. These findings lay the groundwork for further research into LLM capabilities and have practical implications for using LLMs in automated code generation tasks and test-driven development.
</details></li>
</ul>
<hr>
<h2 id="Resolving-uncertainty-on-the-fly-Modeling-adaptive-driving-behavior-as-active-inference"><a href="#Resolving-uncertainty-on-the-fly-Modeling-adaptive-driving-behavior-as-active-inference" class="headerlink" title="Resolving uncertainty on the fly: Modeling adaptive driving behavior as active inference"></a>Resolving uncertainty on the fly: Modeling adaptive driving behavior as active inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06417">http://arxiv.org/abs/2311.06417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Engström, Ran Wei, Anthony McDonald, Alfredo Garcia, Matt O’Kelly, Leif Johnson</li>
<li>for: 本研究旨在开发一个可以用于评估和开发自动驾驶车辆的人类驾驶模型，即对人类驾驶行为的理解。</li>
<li>methods: 本研究使用了活动推测模型，这是一种由计算神经科学开发的行为模型。该模型基于人类决策的最低预期自由能量原则，可以解释人类如何在不约束的情况下做出决策。</li>
<li>results: 研究发现，通过应用这种模型，可以解释人类在不同的驾驶情况下如何 adaptively 驾驶，例如穿过障碍物和同时进行眼动时间分享。这些结果表明了这种模型的一致性和可解释性。<details>
<summary>Abstract</summary>
Understanding adaptive human driving behavior, in particular how drivers manage uncertainty, is of key importance for developing simulated human driver models that can be used in the evaluation and development of autonomous vehicles. However, existing traffic psychology models of adaptive driving behavior either lack computational rigor or only address specific scenarios and/or behavioral phenomena. While models developed in the fields of machine learning and robotics can effectively learn adaptive driving behavior from data, due to their black box nature, they offer little or no explanation of the mechanisms underlying the adaptive behavior. Thus, a generalizable, interpretable, computational model of adaptive human driving behavior is still lacking. This paper proposes such a model based on active inference, a behavioral modeling framework originating in computational neuroscience. The model offers a principled solution to how humans trade progress against caution through policy selection based on the single mandate to minimize expected free energy. This casts goal-seeking and information-seeking (uncertainty-resolving) behavior under a single objective function, allowing the model to seamlessly resolve uncertainty as a means to obtain its goals. We apply the model in two apparently disparate driving scenarios that require managing uncertainty, (1) driving past an occluding object and (2) visual time sharing between driving and a secondary task, and show how human-like adaptive driving behavior emerges from the single principle of expected free energy minimization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Forte-An-Interactive-Visual-Analytic-Tool-for-Trust-Augmented-Net-Load-Forecasting"><a href="#Forte-An-Interactive-Visual-Analytic-Tool-for-Trust-Augmented-Net-Load-Forecasting" class="headerlink" title="Forte: An Interactive Visual Analytic Tool for Trust-Augmented Net Load Forecasting"></a>Forte: An Interactive Visual Analytic Tool for Trust-Augmented Net Load Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06413">http://arxiv.org/abs/2311.06413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaustav Bhattacharjee, Soumya Kundu, Indrasis Chakraborty, Aritra Dasgupta</li>
<li>For:  This paper aims to provide a visual analytics-based application (Forte) to explore deep probabilistic net load forecasting models across various input variables and understand the error rates for different scenarios.* Methods: The paper uses a web-based interface with carefully designed visual interventions to empower scientists to derive insights about model performance by simulating diverse scenarios, facilitating an informed decision-making process.* Results: The paper demonstrates the effectiveness of visualization techniques to provide valuable insights into the correlation between weather inputs and net load forecasts, ultimately advancing grid capabilities by improving trust in forecasting models.<details>
<summary>Abstract</summary>
Accurate net load forecasting is vital for energy planning, aiding decisions on trade and load distribution. However, assessing the performance of forecasting models across diverse input variables, like temperature and humidity, remains challenging, particularly for eliciting a high degree of trust in the model outcomes. In this context, there is a growing need for data-driven technological interventions to aid scientists in comprehending how models react to both noisy and clean input variables, thus shedding light on complex behaviors and fostering confidence in the outcomes. In this paper, we present Forte, a visual analytics-based application to explore deep probabilistic net load forecasting models across various input variables and understand the error rates for different scenarios. With carefully designed visual interventions, this web-based interface empowers scientists to derive insights about model performance by simulating diverse scenarios, facilitating an informed decision-making process. We discuss observations made using Forte and demonstrate the effectiveness of visualization techniques to provide valuable insights into the correlation between weather inputs and net load forecasts, ultimately advancing grid capabilities by improving trust in forecasting models.
</details>
<details>
<summary>摘要</summary>
正确的电网负载预测是重要的能源观察，帮助决策贸易和负载分配。然而，评估预测模型对不同的输入变数，如温度和湿度，的性能仍然是一个挑战，尤其是为了获得高度的信任度。在这个上下文中，有一个增长的需求是使用数据驱动的技术来帮助科学家理解预测模型对不同的输入变数具有多少影响，以及这些变数对预测模型的影响。在这篇论文中，我们提出了Forte，一个基于可观察分析的应用程序，用于探索深度概率电网负载预测模型的不同输入变数下的性能。这个网页式界面通过精心设计的可观察干预，帮助科学家从不同的enario中获得预测模型的性能，并帮助他们做出了 Informed 的决策。我们详细说明了使用Forte所作出的观察，并证明了可观察技术的效用，以提高电网预测模型的信任度，最终提高电网的能力。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-in-the-context-of-precision-agriculture-data-analytics"><a href="#ChatGPT-in-the-context-of-precision-agriculture-data-analytics" class="headerlink" title="ChatGPT in the context of precision agriculture data analytics"></a>ChatGPT in the context of precision agriculture data analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06390">http://arxiv.org/abs/2311.06390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/potamitis123/chatgpt-in-the-context-of-precision-agriculture-data-analytics">https://github.com/potamitis123/chatgpt-in-the-context-of-precision-agriculture-data-analytics</a></li>
<li>paper_authors: Ilyas Potamitis</li>
<li>for: 这个研究 argue that 将 ChatGPT  интеGRATED into the data processing pipeline of automated sensors in precision agriculture 可以带来多个Benefits和改进现代农业实践中的多个方面。</li>
<li>methods: 这个研究使用 ChatGPT 的 speech recognition输入模块，提供一种更直观和自然的方式 для政策制定者们与农业数据处理系统的数据库进行交互，从而提高了对数据分析软件的学习和适应成本。</li>
<li>results: 这个研究表明，通过 ChatGPT 的语言模型可以将 Speech 输入映射到文本，并且可以通过 Python 代码和 Pandas 与整个数据库进行交互，可以实时提供农业数据分析的结果和建议，并且可以通过语音合成器与用户进行Iterative 和改进的交互。<details>
<summary>Abstract</summary>
In this study we argue that integrating ChatGPT into the data processing pipeline of automated sensors in precision agriculture has the potential to bring several benefits and enhance various aspects of modern farming practices. Policy makers often face a barrier when they need to get informed about the situation in vast agricultural fields to reach to decisions. They depend on the close collaboration between agricultural experts in the field, data analysts, and technology providers to create interdisciplinary teams that cannot always be secured on demand or establish effective communication across these diverse domains to respond in real-time. In this work we argue that the speech recognition input modality of ChatGPT provides a more intuitive and natural way for policy makers to interact with the database of the server of an agricultural data processing system to which a large, dispersed network of automated insect traps and sensors probes reports. The large language models map the speech input to text, allowing the user to form its own version of unconstrained verbal query, raising the barrier of having to learn and adapt oneself to a specific data analytics software. The output of the language model can interact through Python code and Pandas with the entire database, visualize the results and use speech synthesis to engage the user in an iterative and refining discussion related to the data. We show three ways of how ChatGPT can interact with the database of the remote server to which a dispersed network of different modalities (optical counters, vibration recordings, pictures, and video), report. We examine the potential and the validity of the response of ChatGPT in analyzing, and interpreting agricultural data, providing real time insights and recommendations to stakeholders
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们 argue that将 ChatGPT  integrate into 自动感知系统的数据处理管道可以带来多种优点，提高现代农业实践中的各个方面。政策制定者经常遇到困难，当他们需要获取庞大农业场景中的信息，以便做出决策。他们需要和农业专家、数据分析师和技术提供商合作，创建协同团队，但这些团队不一定可以在需要时协作，建立有效的交流也是一个挑战。在这项工作中，我们 argue that ChatGPT 的语音识别输入模式提供了一种更直观和自然的方式，让政策制定者与农业数据处理系统的服务器上的数据库进行交互。大语言模型将语音输入转换为文本，让用户可以自定义的提问，不需要适应特定的数据分析软件。输出的语言模型可以通过 Python 代码和 Pandas 与整个数据库进行交互，可视化结果，并使用语音合成器与用户进行可迭代的讨论，与数据相关。我们介绍了三种 ChatGPT 与远程服务器上的数据库交互的方法。我们研究了 ChatGPT 对农业数据的分析和解释的可能性和有效性，以及在实时提供农业决策者的信息和建议。
</details></li>
</ul>
<hr>
<h2 id="Parameter-Efficient-Orthogonal-Finetuning-via-Butterfly-Factorization"><a href="#Parameter-Efficient-Orthogonal-Finetuning-via-Butterfly-Factorization" class="headerlink" title="Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization"></a>Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06243">http://arxiv.org/abs/2311.06243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wy1iu/butterfly-oft">https://github.com/wy1iu/butterfly-oft</a></li>
<li>paper_authors: Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, Bernhard Schölkopf</li>
<li>for: 这篇论文主要目的是研究一种对下游任务进行优化的原理方法—Orthogonal Finetuning (OFT)。</li>
<li>methods: 这篇论文使用了一种叫做Orthogonal Butterfly (BOFT)的优化方法，它是基于Cooley-Tukey快速傅立叶transform算法的启发，并且具有更好的参数效率。</li>
<li>results: 这篇论文通过实践研究，发现BOFT可以对大型视觉对应、大型语言模型和文本对应图像散乱模型进行优化，并且比OFT更有优化效果。<details>
<summary>Abstract</summary>
Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in vision and language.
</details>
<details>
<summary>摘要</summary>
大型基金模型在现场变得普遍，但从头来训练它们是不可持续的。因此，有效地适应这些强大模型到下游任务变得越来越重要。在这篇论文中，我们研究了一种原则正式的 Parameter-efficient finetuning 方法——Orthogonal Finetuning (OFT)。尽管它们展现了良好的泛化能力，但 OFT 仍然需要一些可训练的参数，这是因为正交矩阵的维度较高。为了解决这个问题，我们从信息传输的角度来考虑 OFT，然后确定了一些关键的需求，可以提高参数效率。受到 Cooley-Tukey 快速傅立叶变换算法的启发，我们提议一种高效的正交参数化方法，使用蝴蝶结构。我们将这种参数化方法应用于 OFT，创造了一种新的参数效率高的 finetuning 方法，称为 Orthogonal Butterfly (BOFT)。 BOFT 将 OFT 作为特例，提出一种总体的正交 finetuning 框架。最后，我们进行了广泛的实验研究，适应大型视觉转换器、大型语言模型和文本到图像扩散模型到视觉和语言领域中的各种下游任务。
</details></li>
</ul>
<hr>
<h2 id="Smart-Agent-Based-Modeling-On-the-Use-of-Large-Language-Models-in-Computer-Simulations"><a href="#Smart-Agent-Based-Modeling-On-the-Use-of-Large-Language-Models-in-Computer-Simulations" class="headerlink" title="Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations"></a>Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06330">http://arxiv.org/abs/2311.06330</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/roihn/sabm">https://github.com/roihn/sabm</a></li>
<li>paper_authors: Zengqing Wu, Run Peng, Xu Han, Shuyuan Zheng, Yixin Zhang, Chuan Xiao</li>
<li>for: 这篇论文旨在探讨智能代理模型（SABM）的可能性和应用，它将自然语言模型（LLM）与代理模型（ABM）结合，以模拟复杂系统的行为。</li>
<li>methods: 该论文首先介绍了ABM的基本概念和挑战，然后提出了通过与LLM结合来解决这些挑战的想法。具体来说， authors使用了GPT作为LLM，并开发了一种基于SABM的方法。</li>
<li>results: 论文通过三个实验（代码可以在<a target="_blank" rel="noopener" href="https://github.com/Roihn/SABM%EF%BC%89%EF%BC%8C%E8%AF%81%E6%98%8E%E4%BA%86SABM%E7%9A%84%E6%9C%89%E6%95%88%E6%80%A7%E5%92%8C%E5%8F%AF%E8%A1%8C%E6%80%A7%E3%80%82%E8%BF%99%E4%BA%9B%E5%AE%9E%E9%AA%8C%E8%A1%A8%E6%98%8E%EF%BC%8CSABM%E5%8F%AF%E4%BB%A5%E6%A8%A1%E6%8B%9F%E5%A4%8D%E6%9D%82%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%A1%8C%E4%B8%BA%EF%BC%8C%E5%B9%B6%E4%B8%94%E5%8F%AF%E4%BB%A5%E5%A2%9E%E5%8A%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%81%B5%E6%B4%BB%E6%80%A7%E5%92%8C%E7%8E%B0%E5%AE%9E%E6%84%9F%E3%80%82">https://github.com/Roihn/SABM），证明了SABM的有效性和可行性。这些实验表明，SABM可以模拟复杂系统的行为，并且可以增加模型的灵活性和现实感。</a><details>
<summary>Abstract</summary>
Computer simulations offer a robust toolset for exploring complex systems across various disciplines. A particularly impactful approach within this realm is Agent-Based Modeling (ABM), which harnesses the interactions of individual agents to emulate intricate system dynamics. ABM's strength lies in its bottom-up methodology, illuminating emergent phenomena by modeling the behaviors of individual components of a system. Yet, ABM has its own set of challenges, notably its struggle with modeling natural language instructions and common sense in mathematical equations or rules. This paper seeks to transcend these boundaries by integrating Large Language Models (LLMs) like GPT into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based Modeling (SABM). Building upon the concept of smart agents -- entities characterized by their intelligence, adaptability, and computation ability -- we explore in the direction of utilizing LLM-powered agents to simulate real-world scenarios with increased nuance and realism. In this comprehensive exploration, we elucidate the state of the art of ABM, introduce SABM's potential and methodology, and present three case studies (source codes available at https://github.com/Roihn/SABM), demonstrating the SABM methodology and validating its effectiveness in modeling real-world systems. Furthermore, we cast a vision towards several aspects of the future of SABM, anticipating a broader horizon for its applications. Through this endeavor, we aspire to redefine the boundaries of computer simulations, enabling a more profound understanding of complex systems.
</details>
<details>
<summary>摘要</summary>
SABM leverages the concept of smart agents, which are entities characterized by their intelligence, adaptability, and computational ability. By using LLM-powered agents, SABM can simulate real-world scenarios with increased nuance and realism. In this comprehensive exploration, we elucidate the current state of the art of ABM, introduce the potential and methodology of SABM, and present three case studies (available at <https://github.com/Roihn/SABM>), demonstrating the effectiveness of the SABM methodology in modeling real-world systems.Looking forward, we envision a broader horizon for the applications of SABM, with the potential to redefine the boundaries of computer simulations and enable a deeper understanding of complex systems. By harnessing the power of LLMs and ABM, SABM has the potential to revolutionize the field of computer simulations and provide new insights into complex systems.
</details></li>
</ul>
<hr>
<h2 id="Data-Contamination-Quiz-A-Tool-to-Detect-and-Estimate-Contamination-in-Large-Language-Models"><a href="#Data-Contamination-Quiz-A-Tool-to-Detect-and-Estimate-Contamination-in-Large-Language-Models" class="headerlink" title="Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models"></a>Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06233">http://arxiv.org/abs/2311.06233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahriar Golchin, Mihai Surdeanu</li>
<li>for: 检测大型自然语言模型（LLM）中的数据污染</li>
<li>methods: 使用多选题型测验方法检测数据污染，创建三个Word级改动后的实例，保持原始实例的语义和句子结构不变</li>
<li>results: 在七个数据集和其分割（训练和测试&#x2F;验证）上，使用GPT-4和GPT-3.5两种state-of-the-art LLM，发现方法可以增强数据污染检测和准确地估计污染程度，即使污染信号弱。<details>
<summary>Abstract</summary>
We propose the Data Contamination Quiz, a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions. We devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations, replacing words with their contextual synonyms, ensuring both the semantic and sentence structure remain exactly the same as the original instance. Together with the original instance, these perturbed versions constitute the choices in the quiz. Given that the only distinguishing signal among these choices is the exact wording, an LLM, when tasked with identifying the original instance from the choices, opts for the original if it has memorized it in its pre-training phase--a trait intrinsic to LLMs. A dataset partition is then marked as contaminated if the LLM's performance on the quiz surpasses what random chance suggests. Our evaluation spans seven datasets and their respective splits (train and test/validation) on two state-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the pre-training data, our results suggest that our approach not only enhances the detection of data contamination but also provides an accurate estimation of its extent, even when the contamination signal is weak.
</details>
<details>
<summary>摘要</summary>
我们提出了数据污染测验（Data Contamination Quiz），一种简单有效的方法用于检测大型自然语言模型（LLM）中的数据污染和量化其扩散。具体来说，我们将数据污染检测转化为一系列多选题目。我们设计了一种测验形式，其中每个数据集实例上分别创建了三个杂化版本。这些杂化版本仅包括单词水平的修改，将单词换成相关的同义词，以保持原始实例的语义和句子结构完全相同。与原始实例一起，这些杂化版本组成测验的选择。由于这些选择之间只有单词的不同，因此当一个LLM在面临这些选择时，如果它在预训练阶段已经记忆了原始实例，那么它会选择原始实例。我们对七个dataset和它们的分割（训练和测试/验证）进行了评估，使用两个现代LLM：GPT-4和GPT-3.5。尽管我们没有直接访问预训练数据，但我们的方法不仅可以增强数据污染检测，还可以准确地估计污染的程度，即使污染信号弱。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Synthetic-Datasets-The-Role-of-Shape-Bias-in-Deep-Neural-Network-Generalization"><a href="#Harnessing-Synthetic-Datasets-The-Role-of-Shape-Bias-in-Deep-Neural-Network-Generalization" class="headerlink" title="Harnessing Synthetic Datasets: The Role of Shape Bias in Deep Neural Network Generalization"></a>Harnessing Synthetic Datasets: The Role of Shape Bias in Deep Neural Network Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06224">http://arxiv.org/abs/2311.06224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elior Benarous, Sotiris Anagnostidis, Luca Biggio, Thomas Hofmann</li>
<li>for: 本研究旨在探讨深度学习中使用 sintetic 数据的问题，特别是Shape bias在训练过程中的表现。</li>
<li>methods: 我们使用了不同的网络架构和监督方法来评估Shape bias的可靠性和其能否解释模型认知的差异。</li>
<li>results: 我们发现Shape bias的表现受到网络架构和监督方法的影响，并且与多样性和自然性相互纠缠。我们提出了一种新的解释Shape bias的方法，即用于估计样本集中样本的多样性。<details>
<summary>Abstract</summary>
Recent advancements in deep learning have been primarily driven by the use of large models trained on increasingly vast datasets. While neural scaling laws have emerged to predict network performance given a specific level of computational resources, the growing demand for expansive datasets raises concerns. To address this, a new research direction has emerged, focusing on the creation of synthetic data as a substitute. In this study, we investigate how neural networks exhibit shape bias during training on synthetic datasets, serving as an indicator of the synthetic data quality. Specifically, our findings indicate three key points: (1) Shape bias varies across network architectures and types of supervision, casting doubt on its reliability as a predictor for generalization and its ability to explain differences in model recognition compared to human capabilities. (2) Relying solely on shape bias to estimate generalization is unreliable, as it is entangled with diversity and naturalism. (3) We propose a novel interpretation of shape bias as a tool for estimating the diversity of samples within a dataset. Our research aims to clarify the implications of using synthetic data and its associated shape bias in deep learning, addressing concerns regarding generalization and dataset quality.
</details>
<details>
<summary>摘要</summary>
Our findings reveal three key points:1. Shape bias varies across network architectures and types of supervision, casting doubt on its reliability as a predictor for generalization and its ability to explain differences in model recognition compared to human capabilities.2. Relying solely on shape bias to estimate generalization is unreliable, as it is entangled with diversity and naturalism.3. We propose a novel interpretation of shape bias as a tool for estimating the diversity of samples within a dataset.Our research aims to clarify the implications of using synthetic data and its associated shape bias in deep learning, addressing concerns regarding generalization and dataset quality.
</details></li>
</ul>
<hr>
<h2 id="MultiIoT-Towards-Large-scale-Multisensory-Learning-for-the-Internet-of-Things"><a href="#MultiIoT-Towards-Large-scale-Multisensory-Learning-for-the-Internet-of-Things" class="headerlink" title="MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things"></a>MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06217">http://arxiv.org/abs/2311.06217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shentong Mo, Paul Pu Liang, Russ Salakhutdinov, Louis-Philippe Morency</li>
<li>for: 这篇论文是为了开发机器学习技术来处理互联网物联网（IoT）数据而写的。</li>
<li>methods: 这篇论文使用了多种感知模式，包括动作、热度、地理位置、成像、深度、声音和视频感知模式，以及模式特异性和噪声特征。</li>
<li>results: 这篇论文提出了多种模型基线，包括单感知模式和多感知模式，以及多任务和多感知模型，以便未来的研究人员可以更好地进行多感知表示学习。<details>
<summary>Abstract</summary>
The Internet of Things (IoT), the network integrating billions of smart physical devices embedded with sensors, software, and communication technologies for the purpose of connecting and exchanging data with other devices and systems, is a critical and rapidly expanding component of our modern world. The IoT ecosystem provides a rich source of real-world modalities such as motion, thermal, geolocation, imaging, depth, sensors, video, and audio for prediction tasks involving the pose, gaze, activities, and gestures of humans as well as the touch, contact, pose, 3D of physical objects. Machine learning presents a rich opportunity to automatically process IoT data at scale, enabling efficient inference for impact in understanding human wellbeing, controlling physical devices, and interconnecting smart cities. To develop machine learning technologies for IoT, this paper proposes MultiIoT, the most expansive IoT benchmark to date, encompassing over 1.15 million samples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges involving (1) learning from many sensory modalities, (2) fine-grained interactions across long temporal ranges, and (3) extreme heterogeneity due to unique structure and noise topologies in real-world sensors. We also release a set of strong modeling baselines, spanning modality and task-specific methods to multisensory and multitask models to encourage future research in multisensory representation learning for IoT.
</details>
<details>
<summary>摘要</summary>
互联网物品（IoT），整合了数百万个智能物理设备，嵌入了感知器、软件和通信技术，用于连接和交换数据，是当代世界中一个关键和迅速发展的组成部分。IoT生态系统提供了丰富的现实世界模式，如运动、热度、地理位置、成像、深度、音频和视频等，用于预测人类的姿势、视线、活动和手势。机器学习对IoT数据进行自动处理，可以实现高效的推理，以便更好地理解人类的健康状况、控制物理设备和连接智能城市。为了开发IoT中机器学习技术，本文提出了MultiIoT，迄今为止最大的IoTbenchmark，包括12种感知模式和8个任务，共计1.15万个样本。MultiIoT带来了来自多种感知模式的学习挑战，以及长时间范围内的细化交互和实际世界感知器的特殊结构和噪声概率图。我们还发布了一组强大的模型基线，覆盖模式和任务特定的方法、多感知模型和多任务模型，以促进未来对多感知表示学习的研究。
</details></li>
</ul>
<hr>
<h2 id="BanglaBait-Semi-Supervised-Adversarial-Approach-for-Clickbait-Detection-on-Bangla-Clickbait-Dataset"><a href="#BanglaBait-Semi-Supervised-Adversarial-Approach-for-Clickbait-Detection-on-Bangla-Clickbait-Dataset" class="headerlink" title="BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection on Bangla Clickbait Dataset"></a>BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection on Bangla Clickbait Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06204">http://arxiv.org/abs/2311.06204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mdmotaharmahtab/banglabait">https://github.com/mdmotaharmahtab/banglabait</a></li>
<li>paper_authors: Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque</li>
<li>for: 本研究旨在探讨 clicks 文章标题的检测问题，特别是在低资源语言如孟加拉语中。</li>
<li>methods: 研究人员建立了第一个孟加拉语clickbait检测数据集，包含15,056个标注新闻文章和65,406个未标注新闻文章，来自clickbait dense 新闻网站。每篇文章都由三位专家语言学家标注，包括标题、文体和其他元数据。研究人员使用 semi-supervised 生成对抗网络（SS GANs）来练化一个预训练的孟加拉语变换器模型。</li>
<li>results: 提出的模型在这个数据集上表现出色，超越了传统神经网络模型（LSTM、GRU、CNN）和语言特征基于的模型。这个数据集和详细的分析和比较可以提供未来关于孟加拉语文章标题检测的基础研究。研究人员已经发布相关代码和数据集。<details>
<summary>Abstract</summary>
Intentionally luring readers to click on a particular content by exploiting their curiosity defines a title as clickbait. Although several studies focused on detecting clickbait titles in English articles, low resource language like Bangla has not been given adequate attention. To tackle clickbait titles in Bangla, we have constructed the first Bangla clickbait detection dataset containing 15,056 labeled news articles and 65,406 unlabelled news articles extracted from clickbait dense news sites. Each article has been labeled by three expert linguists and includes an article's title, body, and other metadata. By incorporating labeled and unlabelled data, we finetune a pretrained Bangla transformer model in an adversarial fashion using Semi Supervised Generative Adversarial Networks (SS GANs). The proposed model acts as a good baseline for this dataset, outperforming traditional neural network models (LSTM, GRU, CNN) and linguistic feature based models. We expect that this dataset and the detailed analysis and comparison of these clickbait detection models will provide a fundamental basis for future research into detecting clickbait titles in Bengali articles. We have released the corresponding code and dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> clickbait 标题的目的是引诱读者点击特定内容，这定义了 clickbait 标题。 although several studies have focused on detecting clickbait titles in English articles, low-resource languages like Bangla have not received adequate attention. To address clickbait titles in Bangla, we have constructed the first Bangla clickbait detection dataset, containing 15,056 labeled news articles and 65,406 unlabeled news articles extracted from clickbait-dense news sites. Each article has been labeled by three expert linguists and includes the article's title, body, and other metadata. By incorporating labeled and unlabeled data, we fine-tune a pre-trained Bangla transformer model in an adversarial fashion using Semi-Supervised Generative Adversarial Networks (SS GANs). The proposed model serves as a good baseline for this dataset and outperforms traditional neural network models (LSTM, GRU, CNN) and linguistic feature-based models. We expect that this dataset and the detailed analysis and comparison of these clickbait detection models will provide a fundamental basis for future research into detecting clickbait titles in Bengali articles. We have released the corresponding code and dataset.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-AI-Text-to-Image-and-AI-Text-to-Video-Generators"><a href="#A-Survey-of-AI-Text-to-Image-and-AI-Text-to-Video-Generators" class="headerlink" title="A Survey of AI Text-to-Image and AI Text-to-Video Generators"></a>A Survey of AI Text-to-Image and AI Text-to-Video Generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06329">http://arxiv.org/abs/2311.06329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditi Singh</li>
<li>for:  investigate cutting-edge approaches in Text-to-Image and Text-to-Video AI generations</li>
<li>methods: cover data preprocessing techniques, neural network types, and evaluation metrics used in the field</li>
<li>results: discuss challenges and limitations of Text-to-Image and Text-to-Video AI generations, as well as future research directionsHere’s the summary in Traditional Chinese:</li>
<li>for: 研究文本至图和文本至影片人工智能生成领域中的进步技术</li>
<li>methods: 涵盖数据清洁技术、神经网络类型和评估指标在这个领域中的使用</li>
<li>results: 探讨文本至图和文本至影片人工智能生成中的挑战和限制，以及未来研究方向<details>
<summary>Abstract</summary>
Text-to-Image and Text-to-Video AI generation models are revolutionary technologies that use deep learning and natural language processing (NLP) techniques to create images and videos from textual descriptions. This paper investigates cutting-edge approaches in the discipline of Text-to-Image and Text-to-Video AI generations. The survey provides an overview of the existing literature as well as an analysis of the approaches used in various studies. It covers data preprocessing techniques, neural network types, and evaluation metrics used in the field. In addition, the paper discusses the challenges and limitations of Text-to-Image and Text-to-Video AI generations, as well as future research directions. Overall, these models have promising potential for a wide range of applications such as video production, content creation, and digital marketing.
</details>
<details>
<summary>摘要</summary>
文本到图像和文本到视频人工智能生成模型是革新技术，使用深度学习和自然语言处理（NLP）技术来生成图像和视频从文本描述。本文对 Text-to-Image 和 Text-to-Video AI 生成领域进行了详细的探讨和分析，包括现有文献的概述以及不同研究中使用的方法。它还讨论了该领域的挑战和限制，以及未来的研究方向。总之，这些模型在视频生产、内容创作和数字市场营销等领域具有广阔的应用前景。Here's the translation in Traditional Chinese:文本到图像和文本到影片人工智能生成模型是革新技术，使用深度学习和自然语言处理（NLP）技术来生成图像和影片从文本描述。本文对 Text-to-Image 和 Text-to-Video AI 生成领域进行了详细的探讨和分析，包括现有文献的概述以及不同研究中使用的方法。它还讨论了该领域的挑战和限制，以及未来的研究方向。总之，这些模型在影片生产、内容创作和数位市场营销等领域具有广阔的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Greedy-PIG-Adaptive-Integrated-Gradients"><a href="#Greedy-PIG-Adaptive-Integrated-Gradients" class="headerlink" title="Greedy PIG: Adaptive Integrated Gradients"></a>Greedy PIG: Adaptive Integrated Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06192">http://arxiv.org/abs/2311.06192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyriakos Axiotis, Sami Abu-al-haija, Lin Chen, Matthew Fahrbach, Gang Fu</li>
<li>for: 本文提出了一种基于subset选择的特征归因和特征选择框架，用于解释深度学习模型的预测结果。</li>
<li>methods: 本文提出了一种名为Greedy PIG的自适应加速方法，用于Feature attribution和Feature selection。</li>
<li>results: 试验结果表明，引入自适应性可以使归因方法更加强大和多功能。<details>
<summary>Abstract</summary>
Deep learning has become the standard approach for most machine learning tasks. While its impact is undeniable, interpreting the predictions of deep learning models from a human perspective remains a challenge. In contrast to model training, model interpretability is harder to quantify and pose as an explicit optimization problem. Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We demonstrate the success of Greedy PIG on a wide variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a powerful and versatile method for making attribution methods more powerful.
</details>
<details>
<summary>摘要</summary>
深度学习已成为大多数机器学习任务的标准方法。虽然其影响无疑，但从人类视角来解释深度学习模型预测结果仍然是一个挑战。与模型训练相比，模型解释更难以量化和表示为显式优化问题。 Drawing inspiration from AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We demonstrate the success of Greedy PIG on a wide variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a powerful and versatile method for making attribution methods more powerful.Here's the text with some notes on the translation:* "深度学习" (shēn dào xué xí) is the Chinese term for "deep learning"* "模型训练" (mó delè xùn zhí) is the Chinese term for "model training"* "模型解释" (mó delè jiě jiè) is the Chinese term for "model interpretation"* "AUC SIC" (AUC softmax information curve) is translated as "AUC SIC" (AUC 软MAX信息曲线)* "subset selection" is translated as "子集选择" (zǐ jiāo jiàn zhèng)* "path integrated gradients" (PIG) is translated as "路径集成 gradient" (lù jì zhì zhèng jiè dào)* "Greedy PIG" is translated as "积极 PIG" (jī jí PIG)Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Taiwan, and other regions.
</details></li>
</ul>
<hr>
<h2 id="FourierGNN-Rethinking-Multivariate-Time-Series-Forecasting-from-a-Pure-Graph-Perspective"><a href="#FourierGNN-Rethinking-Multivariate-Time-Series-Forecasting-from-a-Pure-Graph-Perspective" class="headerlink" title="FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective"></a>FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06190">http://arxiv.org/abs/2311.06190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aikunyi/fouriergnn">https://github.com/aikunyi/fouriergnn</a></li>
<li>paper_authors: Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu, Pengyang Wang, Ning An, Longbing Cao, Zhendong Niu</li>
<li>for: 这个论文旨在提出一种新的多变量时间序列预测方法，它可以考虑多个时间序列之间的干扰关系，并且可以有效地预测未来时间序列的值。</li>
<li>methods: 这篇论文使用了一种新的数据结构 called hypervariate graph，它将每个时间序列的值看作一个图节点，并将每个滑动窗口转换为一个完全连接的空间时间图。然后，它提出了一种新的架构 called Fourier Graph Neural Network (FourierGNN)，它可以在快 Fourier 空间中进行矩阵乘法，并且可以有效地预测未来时间序列的值。</li>
<li>results: 在七个 dataset 上进行了广泛的实验，结果显示，FourierGNN 可以在预测时间序列值方面具有更高的效果，同时具有更低的复杂性和更少的参数。<details>
<summary>Abstract</summary>
Multivariate time series (MTS) forecasting has shown great importance in numerous industries. Current state-of-the-art graph neural network (GNN)-based forecasting methods usually require both graph networks (e.g., GCN) and temporal networks (e.g., LSTM) to capture inter-series (spatial) dynamics and intra-series (temporal) dependencies, respectively. However, the uncertain compatibility of the two networks puts an extra burden on handcrafted model designs. Moreover, the separate spatial and temporal modeling naturally violates the unified spatiotemporal inter-dependencies in real world, which largely hinders the forecasting performance. To overcome these problems, we explore an interesting direction of directly applying graph networks and rethink MTS forecasting from a pure graph perspective. We first define a novel data structure, hypervariate graph, which regards each series value (regardless of variates or timestamps) as a graph node, and represents sliding windows as space-time fully-connected graphs. This perspective considers spatiotemporal dynamics unitedly and reformulates classic MTS forecasting into the predictions on hypervariate graphs. Then, we propose a novel architecture Fourier Graph Neural Network (FourierGNN) by stacking our proposed Fourier Graph Operator (FGO) to perform matrix multiplications in Fourier space. FourierGNN accommodates adequate expressiveness and achieves much lower complexity, which can effectively and efficiently accomplish the forecasting. Besides, our theoretical analysis reveals FGO's equivalence to graph convolutions in the time domain, which further verifies the validity of FourierGNN. Extensive experiments on seven datasets have demonstrated our superior performance with higher efficiency and fewer parameters compared with state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
多变量时间序列（MTS）预测已经在多个行业得到了重要的应用。当前的状态艺术Graph Neural Network（GNN）基本预测方法通常需要图网络（例如GCN）和时间网络（例如LSTM）来捕捉 между序列（空间）动力和内部序列（时间）依赖项，分别。然而，这两种网络的不确定兼容性会增加手动设计模型的困难度。另外，分离的空间和时间模型自然地违反了实际世界中的一体化空时间依赖关系，这大大降低了预测性能。为了解决这些问题，我们开explored an interesting direction of directly applying graph networks and rethinking MTS forecasting from a pure graph perspective.我们首先定义了一种新的数据结构，卷积graph，其中每个时间序列值（无论是变量或时间戳）都被视为图节点，并将滑动窗口转化为空间时间完全连接图。这种视角同时考虑了空间时间动力的统一，并将经典MTS预测转化为对卷积图的预测。然后，我们提出了一种新的架构Fourier Graph Neural Network（FourierGNN），其基于我们提出的快捷Graph Operator（FGO）来执行矩阵乘法操作。FourierGNN具有充分的表达能力，同时可以有效地和高效地完成预测。此外，我们的理论分析表明FGO的等价性于图 convolutions在时间频谱中，这进一步证明了FourierGNN的有效性。我们在七个数据集上进行了广泛的实验，结果显示我们的性能高于当前状态艺术方法，同时具有更低的复杂性和更少的参数。
</details></li>
</ul>
<hr>
<h2 id="Frequency-domain-MLPs-are-More-Effective-Learners-in-Time-Series-Forecasting"><a href="#Frequency-domain-MLPs-are-More-Effective-Learners-in-Time-Series-Forecasting" class="headerlink" title="Frequency-domain MLPs are More Effective Learners in Time Series Forecasting"></a>Frequency-domain MLPs are More Effective Learners in Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06184">http://arxiv.org/abs/2311.06184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aikunyi/frets">https://github.com/aikunyi/frets</a></li>
<li>paper_authors: Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Defu Lian, Ning An, Longbing Cao, Zhendong Niu</li>
<li>for: 时间序列预测任务中的一种基于多层感知器（MLP）的新方法，旨在提高预测性能。</li>
<li>methods: 使用频域MLP来学习时间序列的频谱特征，并通过频域域转换和频率学习两个阶段来学习时间序列的局部和全局相关性。</li>
<li>results: 在13个实验室中，与状态艺术方法进行比较，FreTS方法具有更高的预测精度和稳定性。<details>
<summary>Abstract</summary>
Time series forecasting has played the key role in different industrial, including finance, traffic, energy, and healthcare domains. While existing literatures have designed many sophisticated architectures based on RNNs, GNNs, or Transformers, another kind of approaches based on multi-layer perceptrons (MLPs) are proposed with simple structure, low complexity, and {superior performance}. However, most MLP-based forecasting methods suffer from the point-wise mappings and information bottleneck, which largely hinders the forecasting performance. To overcome this problem, we explore a novel direction of applying MLPs in the frequency domain for time series forecasting. We investigate the learned patterns of frequency-domain MLPs and discover their two inherent characteristic benefiting forecasting, (i) global view: frequency spectrum makes MLPs own a complete view for signals and learn global dependencies more easily, and (ii) energy compaction: frequency-domain MLPs concentrate on smaller key part of frequency components with compact signal energy. Then, we propose FreTS, a simple yet effective architecture built upon Frequency-domain MLPs for Time Series forecasting. FreTS mainly involves two stages, (i) Domain Conversion, that transforms time-domain signals into complex numbers of frequency domain; (ii) Frequency Learning, that performs our redesigned MLPs for the learning of real and imaginary part of frequency components. The above stages operated on both inter-series and intra-series scales further contribute to channel-wise and time-wise dependency learning. Extensive experiments on 13 real-world benchmarks (including 7 benchmarks for short-term forecasting and 6 benchmarks for long-term forecasting) demonstrate our consistent superiority over state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
时间序列预测在不同的行业中扮演着关键角色，包括金融、交通、能源和医疗领域。而现有的文献中设计了许多复杂的架构，如RNNs、GNNs或Transformers，另一种基于多层感知器（MLPs）的方法具有简单的结构、低复杂度和超越性。然而，大多数MLP基于预测方法受到点约映射和信息瓶颈的限制，这大大降低预测性能。为了解决这个问题，我们开探了在频率域应用MLP的新方向，并 investigate了频率域MLP学习的特征。我们发现频率域MLP拥有两种内在特征，即全球视图和能量压缩，这两种特征使得频率域MLP在预测时Series中表现出优异。然后，我们提出了FreTS，一种简单 yet有效的架构，基于频率域MLP进行时Series预测。FreTS主要包括两个阶段，即频率域转换和频率学习。频率域转换将时间域信号转换为复数频率域，而频率学习则使用我们重新设计的MLP进行频率组成部分的学习。这两个阶段在时间和通道级别进行了规模进行了时间和通道级别的依赖学习。我们对13个实际benchmark进行了广泛的实验，结果表明我们在state-of-the-art方法之上保持了稳定的优势。
</details></li>
</ul>
<hr>
<h2 id="Search-Based-Fairness-Testing-An-Overview"><a href="#Search-Based-Fairness-Testing-An-Overview" class="headerlink" title="Search-Based Fairness Testing: An Overview"></a>Search-Based Fairness Testing: An Overview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06175">http://arxiv.org/abs/2311.06175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussaini Mamman, Shuib Basri, Abdullateef Oluwaqbemiga Balogun, Abdullahi Abubakar Imam, Ganesh Kumar, Luiz Fernando Capretz</li>
<li>for: 这篇论文主要是为了探讨人工智能系统中的偏见问题，以及如何通过搜索测试来检测和解决这些偏见。</li>
<li>methods: 本文主要介绍了目前关于公平测试的研究，尤其是通过搜索测试来实现公平测试的方法。我们的分析发现，现有的搜索测试方法可以帮助解决人工智能系统中的偏见问题，但还有一些需要改进的方面。</li>
<li>results: 本文的分析发现，目前关于公平测试的研究做到了一定的进步，但还有一些需要改进的方面。未来的研究应该更加强调利用现有的搜索测试方法来进行公平测试，以确保人工智能系统中的偏见问题得到解决。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) has demonstrated remarkable capabilities in domains such as recruitment, finance, healthcare, and the judiciary. However, biases in AI systems raise ethical and societal concerns, emphasizing the need for effective fairness testing methods. This paper reviews current research on fairness testing, particularly its application through search-based testing. Our analysis highlights progress and identifies areas of improvement in addressing AI systems biases. Future research should focus on leveraging established search-based testing methodologies for fairness testing.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在招聘、金融、医疗和司法等领域表现出了惊人的能力，但AI系统中的偏见引起了道德和社会问题的关注，高调出了有效的公平测试方法的需求。本文综述当前关于公平测试的研究，特别是通过搜索基于测试方法的应用。我们的分析显示了进步和改进的方向，未来的研究应该集中于利用已有的搜索基于测试方法来进行公平测试。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-can-be-Logical-Solvers"><a href="#Language-Models-can-be-Logical-Solvers" class="headerlink" title="Language Models can be Logical Solvers"></a>Language Models can be Logical Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06158">http://arxiv.org/abs/2311.06158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Jiazhan Feng, Ruochen Xu, Junheng Hao, Hiteshi Sharma, Yelong Shen, Dongyan Zhao, Weizhu Chen</li>
<li>for: 这篇论文旨在探讨大语言模型（LLM）是否可以直接模仿逻辑解题器的思维过程，以提高其逻辑推理能力。</li>
<li>methods: 该论文提出了一种新的语言模型LoGiPT，它通过学习逻辑解题器的语法和 sintaxis 来直接模仿逻辑解题器的思维过程，并且不需要解析自然语言问题。</li>
<li>results: 实验结果表明，LoGiPT在两个公共的逻辑推理数据集上表现出色，超越了现有的解题器辅助语言模型和少量提示方法，并且在竞争的LLM如ChatGPT或GPT-4上表现了竞争力。<details>
<summary>Abstract</summary>
Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge. The state-of-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of the external logical solver and no answer to the logical questions. In this paper, we introduce LoGiPT, a novel language model that directly emulates the reasoning processes of logical solvers and bypasses the parsing errors by learning to strict adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers. Experimental results on two public deductive reasoning datasets demonstrate that LoGiPT outperforms state-of-the-art solver-augmented LMs and few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.
</details>
<details>
<summary>摘要</summary>
理智推理是人类智能的基本方面，对于问题解决和决策都是重要组成部分。 current advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge.  State-of-the-art solver-augmented language models use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of the external logical solver and no answer to the logical questions.在这篇论文中，我们介绍了LoGiPT，一种新的语言模型，它直接模拟逻辑解决器的思维过程，并通过学习逻辑解决器的语法和语言规则，减少或消除解析错误。 LoGiPT 在两个公共的逻辑推理数据集上进行了实验，并证明了它在与state-of-the-art solver-augmented LMs和 few-shot prompting methods进行比较中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Going-beyond-persistent-homology-using-persistent-homology"><a href="#Going-beyond-persistent-homology-using-persistent-homology" class="headerlink" title="Going beyond persistent homology using persistent homology"></a>Going beyond persistent homology using persistent homology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06152">http://arxiv.org/abs/2311.06152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johanna Immonen, Amauri H. Souza, Vikas Garg</li>
<li>for: 这篇论文的目的是提高图像逻辑测试中的表达能力。</li>
<li>methods: 论文使用了 persistent homology（PH）来增强图像模型的表达能力。</li>
<li>results: 论文提出了一种新的颜色分离集来解决图像模型中的表达限制问题，并实现了一种基于颜色级别的PH的学习方法，从而提高了图像模型的表达能力。<details>
<summary>Abstract</summary>
Representational limits of message-passing graph neural networks (MP-GNNs), e.g., in terms of the Weisfeiler-Leman (WL) test for isomorphism, are well understood. Augmenting these graph models with topological features via persistent homology (PH) has gained prominence, but identifying the class of attributed graphs that PH can recognize remains open. We introduce a novel concept of color-separating sets to provide a complete resolution to this important problem. Specifically, we establish the necessary and sufficient conditions for distinguishing graphs based on the persistence of their connected components, obtained from filter functions on vertex and edge colors. Our constructions expose the limits of vertex- and edge-level PH, proving that neither category subsumes the other. Leveraging these theoretical insights, we propose RePHINE for learning topological features on graphs. RePHINE efficiently combines vertex- and edge-level PH, achieving a scheme that is provably more powerful than both. Integrating RePHINE into MP-GNNs boosts their expressive power, resulting in gains over standard PH on several benchmarks for graph classification.
</details>
<details>
<summary>摘要</summary>
Message-passing graph neural networks (MP-GNNs) 的表示限制已经很好地了解，例如通过weisfeiler-leman (WL) 测试来判断图是否同构。通过添加图的拓扑特征via persistent homology (PH) 得到了广泛应用，但是确定 attributed graphs 中 PH 能认可的类型仍然是一个重要的开放问题。我们提出了一种新的色分集来解决这个重要问题。我们证明了基于图连接组件的persistence得到了必要和 suficient 条件，并且证明 neither vertex-level PH  nor edge-level PH 可以包含另一个类型。我们建议RePHINE，一种可以有效地结合 vertex-level PH 和 edge-level PH 的学习方法。RePHINE 可以提高 MP-GNNs 的表达能力，在多个图分类 benchmark 上实现了比标准 PH 更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Dense-Visual-Odometry-Using-Genetic-Algorithm"><a href="#Dense-Visual-Odometry-Using-Genetic-Algorithm" class="headerlink" title="Dense Visual Odometry Using Genetic Algorithm"></a>Dense Visual Odometry Using Genetic Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06149">http://arxiv.org/abs/2311.06149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Slimane Djema, Zoubir Abdeslem Benselama, Ramdane Hedjar, Krabi Abdallah</li>
<li>for: 估算mobile robot或运动物体头部摄像机运动 FROM RGB-D图像中的静止场景</li>
<li>methods: 使用非线性最小二乘方法转化问题，并使用经典方法提供了迭代解决方案，以及metaheuristic优化方法解决问题，并提高结果</li>
<li>results: 基于基因算法开发了一种新的视觉速度计算方法，并通过比较与基能量方法和另一种metaheuristic方法进行比较，证明了我们的创新算法的效率。<details>
<summary>Abstract</summary>
Our work aims to estimate the camera motion mounted on the head of a mobile robot or a moving object from RGB-D images in a static scene. The problem of motion estimation is transformed into a nonlinear least squares function. Methods for solving such problems are iterative. Various classic methods gave an iterative solution by linearizing this function. We can also use the metaheuristic optimization method to solve this problem and improve results. In this paper, a new algorithm is developed for visual odometry using a sequence of RGB-D images. This algorithm is based on a genetic algorithm. The proposed iterative genetic algorithm searches using particles to estimate the optimal motion and then compares it to the traditional methods. To evaluate our method, we use the root mean square error to compare it with the based energy method and another metaheuristic method. We prove the efficiency of our innovative algorithm on a large set of images.
</details>
<details>
<summary>摘要</summary>
我团队的工作目标是从RGB-D图像中估算移动机器或移动物体的摄像头运动。这个问题被转化为非线性最小二乘函数。解决这类问题的方法是迭代的。经典方法可以将这个函数线性化以获得迭代解决方案。我们还可以使用metaheuristic优化方法解决这个问题，以提高结果。在这篇论文中，我们开发了一种基于遗传算法的视觉奔迅算法。我们使用一系列RGB-D图像来测试我们的算法，并与传统方法和另一种metaheuristic方法进行比较。我们使用根mean square error来评估我们的方法，并证明了我们的创新算法在大量图像上的效率。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-sufficient-physical-information-into-artificial-neural-networks-a-guaranteed-improvement-via-physics-based-Rao-Blackwellization"><a href="#Incorporating-sufficient-physical-information-into-artificial-neural-networks-a-guaranteed-improvement-via-physics-based-Rao-Blackwellization" class="headerlink" title="Incorporating sufficient physical information into artificial neural networks: a guaranteed improvement via physics-based Rao-Blackwellization"></a>Incorporating sufficient physical information into artificial neural networks: a guaranteed improvement via physics-based Rao-Blackwellization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06147">http://arxiv.org/abs/2311.06147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gian-Luca Geuken, Jörn Mosler, Patrick Kurzeja</li>
<li>for: 提高人工神经网络预测的精度，使用物理信息。</li>
<li>methods: 使用Rao-Blackwell化Strategy，将错误范数和证明改进传递到决定性概念上，使用物理条件的充分信息。</li>
<li>results: 应用于材料模型化、塑性钢 simulate、质量违 brittle 损伤和塑性实验，可以提高预测的精度，减少噪音、过拟合和数据要求。<details>
<summary>Abstract</summary>
The concept of Rao-Blackwellization is employed to improve predictions of artificial neural networks by physical information. The error norm and the proof of improvement are transferred from the original statistical concept to a deterministic one, using sufficient information on physics-based conditions. The proposed strategy is applied to material modeling and illustrated by examples of the identification of a yield function, elasto-plastic steel simulations, the identification of driving forces for quasi-brittle damage and rubber experiments. Sufficient physical information is employed, e.g., in the form of invariants, parameters of a minimization problem, dimensional analysis, isotropy and differentiability. It is proven how intuitive accretion of information can yield improvement if it is physically sufficient, but also how insufficient or superfluous information can cause impairment. Opportunities for the improvement of artificial neural networks are explored in terms of the training data set, the networks' structure and output filters. Even crude initial predictions are remarkably improved by reducing noise, overfitting and data requirements.
</details>
<details>
<summary>摘要</summary>
“RAO-BLACKWELLIZATION”技术可以提高人工神经网络预测的准确性，通过物理信息的充分利用。原始统计概念的错误 нор和证明改进被转移到决定性概念上，使用物理条件的充分信息。提议的策略被应用于材料模型化，通过示例描述了固体弹性钢的预测、不可逆减弱损伤和塑料实验的标定。使用物理信息，如 invariants、最小化问题的参数、维度分析、均匀性和微分性。证明了如果物理信息充分，则直观增加信息可以带来改进，但也证明了不充分或过度信息会导致下降。探讨人工神经网络的改进机会，包括训练数据集、网络结构和输出筛选。жеven crude initial predictions can be remarkably improved by reducing noise, overfitting and data requirements.
</details></li>
</ul>
<hr>
<h2 id="High-dimensional-mixed-categorical-Gaussian-processes-with-application-to-multidisciplinary-design-optimization-for-a-green-aircraft"><a href="#High-dimensional-mixed-categorical-Gaussian-processes-with-application-to-multidisciplinary-design-optimization-for-a-green-aircraft" class="headerlink" title="High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft"></a>High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06130">http://arxiv.org/abs/2311.06130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Saves, Youssef Diouane, Nathalie Bartoli, Thierry Lefebvre, Joseph Morlier</li>
<li>for: 这 paper 的目的是提出一种基于 Gaussian Process（GP）的混合 categorical 优化方法，以解决多学科设计优化中混合 categorical 变量的问题。</li>
<li>methods: 这 paper 使用 Partial Least Squares（PLS）回归来构建混合 categorical GP，并通过 Kriging with PLS 来扩展 GP 的应用范围。</li>
<li>results: 该方法在实际应用中得到了成功，包括对一架悬臂 beam 的结构行为的研究以及一架绿色飞机的多学科设计优化。 results 表明，该方法可以减少飞机在一次任务中消耗的燃料量为 439 公斤。<details>
<summary>Abstract</summary>
Multidisciplinary design optimization (MDO) methods aim at adapting numerical optimization techniques to the design of engineering systems involving multiple disciplines. In this context, a large number of mixed continuous, integer, and categorical variables might arise during the optimization process, and practical applications involve a significant number of design variables. Recently, there has been a growing interest in mixed-categorical metamodels based on Gaussian Process (GP) for Bayesian optimization. In particular, to handle mixed-categorical variables, several existing approaches employ different strategies to build the GP. These strategies either use continuous kernels, such as the continuous relaxation or the Gower distance-based kernels, or direct estimation of the correlation matrix, such as the exponential homoscedastic hypersphere (EHH) or the Homoscedastic Hypersphere (HH) kernel. Although the EHH and HH kernels are shown to be very efficient and lead to accurate GPs, they are based on a large number of hyperparameters. In this paper, we address this issue by constructing mixed-categorical GPs with fewer hyperparameters using Partial Least Squares (PLS) regression. Our goal is to generalize Kriging with PLS, commonly used for continuous inputs, to handle mixed-categorical inputs. The proposed method is implemented in the open-source software SMT and has been efficiently applied to structural and multidisciplinary applications. Our method is used to effectively demonstrate the structural behavior of a cantilever beam and facilitates MDO of a green aircraft, resulting in a 439-kilogram reduction in the amount of fuel consumed during a single aircraft mission.
</details>
<details>
<summary>摘要</summary>
多学科设计优化（MDO）方法是指通过数学优化技术来设计工程系统中的多学科系统。在这个上下文中，可能会出现大量的混合连续、整数和分类变量，而实际应用中的设计变量数量可能很大。现在，关于混合分类变量的泛化模型方法已经受到了越来越多的关注。特别是在涉及到混合分类变量时，exist several approaches to build the GP, such as using continuous kernels, like the continuous relaxation or the Gower distance-based kernels, or direct estimation of the correlation matrix, like the exponential homoscedastic hypersphere (EHH) or the Homoscedastic Hypersphere (HH) kernel. Although the EHH and HH kernels are shown to be very efficient and lead to accurate GPs, they are based on a large number of hyperparameters. In this paper, we address this issue by constructing mixed-categorical GPs with fewer hyperparameters using Partial Least Squares (PLS) regression. Our goal is to generalize Kriging with PLS, commonly used for continuous inputs, to handle mixed-categorical inputs. The proposed method is implemented in the open-source software SMT and has been efficiently applied to structural and multidisciplinary applications. Our method is used to effectively demonstrate the structural behavior of a cantilever beam and facilitates MDO of a green aircraft, resulting in a 439-kilogram reduction in the amount of fuel consumed during a single aircraft mission.
</details></li>
</ul>
<hr>
<h2 id="Making-LLMs-Worth-Every-Penny-Resource-Limited-Text-Classification-in-Banking"><a href="#Making-LLMs-Worth-Every-Penny-Resource-Limited-Text-Classification-in-Banking" class="headerlink" title="Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking"></a>Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06102">http://arxiv.org/abs/2311.06102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lefteris Loukas, Ilias Stogiannidis, Odysseas Diamantopoulos, Prodromos Malakasiotis, Stavros Vassos</li>
<li>for: 这个研究旨在探讨具有限制的数据的情况下，使用少量样本进行NLG的可行性，并评估OpenAI、Cohere和Anthropic等 cutting-edge LLMs 的表现。</li>
<li>methods: 研究使用了内生生成（RAG）和GPT-4的数据增强技术，并评估了这些方法的成本效益。</li>
<li>results: 研究发现，使用RAG可以大幅降低操作成本，并且GPT-4的数据增强技术可以提高表现在限制的数据情况下。<details>
<summary>Abstract</summary>
Standard Full-Data classifiers in NLP demand thousands of labeled examples, which is impractical in data-limited domains. Few-shot methods offer an alternative, utilizing contrastive learning techniques that can be effective with as little as 20 examples per class. Similarly, Large Language Models (LLMs) like GPT-4 can perform effectively with just 1-5 examples per class. However, the performance-cost trade-offs of these methods remain underexplored, a critical concern for budget-limited organizations. Our work addresses this gap by studying the aforementioned approaches over the Banking77 financial intent detection dataset, including the evaluation of cutting-edge LLMs by OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We complete the picture with two additional methods: first, a cost-effective querying method for LLMs based on retrieval-augmented generation (RAG), able to reduce operational costs multiple times compared to classic few-shot approaches, and second, a data augmentation method using GPT-4, able to improve performance in data-limited scenarios. Finally, to inspire future research, we provide a human expert's curated subset of Banking77, along with extensive error analysis.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>普通的全数据分类器在自然语言处理中需要千个标注示例，这是数据有限领域中不切实际的。少量方法提供了一个 alternatives, 使用对比学习技术，可以在每个类型只需20个示例。同时，大语言模型（LLMs）如GPT-4可以在每个类型只需1-5个示例。然而，这些方法的性能成本负担仍未得到充分探讨，这是企业有限预算的关键问题。我们的工作解决这个问题，通过对上述方法的研究，包括开放AI、Cohere和人类智慧的cutting-edge LLMs在内的广泛的少量场景。我们还添加了两种额外方法：首先，一种基于检索增生（RAG）的cost-effective查询方法，可以在经典少量场景中多次减少操作成本，并第二，一种使用GPT-4的数据扩展方法，可以在数据有限场景中提高性能。最后，为未来研究提供了人类专家精心审核的 Banking77 子集，以及广泛的错误分析。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-for-MIMO-Equalization-Using-Transformer-Based-Sequence-Models"><a href="#In-Context-Learning-for-MIMO-Equalization-Using-Transformer-Based-Sequence-Models" class="headerlink" title="In-Context Learning for MIMO Equalization Using Transformer-Based Sequence Models"></a>In-Context Learning for MIMO Equalization Using Transformer-Based Sequence Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06101">http://arxiv.org/abs/2311.06101</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kclip/icl-equalization">https://github.com/kclip/icl-equalization</a></li>
<li>paper_authors: Matteo Zecchin, Kai Yu, Osvaldo Simeone</li>
<li>for: 这个论文旨在探讨如何利用大规模预训练序列模型（如转换器基 architecture）进行上下文学习（ICL），以解决多输入多输出（MIMO）均衡问题。</li>
<li>methods: 该论文使用了ICL技术，通过将输入和相关任务上的一些例子映射到输出变量上，以直接确定决策。无需显式更新模型参数，可以适应新任务。</li>
<li>results: 研究表明，通过预训练，可以使transformer-based ICL在MIMO均衡问题中达到阈值行为，即，随着预训练任务数量的增加，性能从预先确定的 minimum mean squared error（MMSE）均衡器转变为真实数据生成的prior。<details>
<summary>Abstract</summary>
Large pre-trained sequence models, such as transformer-based architectures, have been recently shown to have the capacity to carry out in-context learning (ICL). In ICL, a decision on a new input is made via a direct mapping of the input and of a few examples from the given task, serving as the task's context, to the output variable. No explicit updates of model parameters are needed to tailor the decision to a new task. Pre-training, which amounts to a form of meta-learning, is based on the observation of examples from several related tasks. Prior work has shown ICL capabilities for linear regression. In this study, we leverage ICL to address the inverse problem of multiple-input and multiple-output (MIMO) equalization based on a context given by pilot symbols. A task is defined by the unknown fading channel and by the signal-to-noise ratio (SNR) level, which may be known. To highlight the practical potential of the approach, we allow for the presence of quantization of the received signals. We demonstrate via numerical results that transformer-based ICL has a threshold behavior, whereby, as the number of pre-training tasks grows, the performance switches from that of a minimum mean squared error (MMSE) equalizer with a prior determined by the pre-trained tasks to that of an MMSE equalizer with the true data-generating prior.
</details>
<details>
<summary>摘要</summary>
大型预训模型，如基于转换器架构的模型，最近已经显示出在上下文学习（ICL）中有较大的容量。在 ICL 中，一个决策是通过直接映射输入和任务上的一些例子来进行决策。无需显式更新模型参数，可以适应新任务。预训，即一种形式的meta-学习，基于多个相关任务的观察。前工作已经证明了 ICL 的能力 для线性回归。在这个研究中，我们利用 ICL 来解决多输入多出力（MIMO）平衡问题，基于一个 Context 给出的飞行符号。任务是由未知拍摄通道和信号噪声比（SNR）水平确定。为了强调实用的潜力，我们允许接收信号的量化。我们通过数值结果表明，基于转换器的 ICL 存在一个阈值行为，其中，当预训任务数量增加时，性能从一个基于预训任务的最小方差平均值（MMSE）平衡器转换为一个基于真实数据生成的 prior 的 MMSE 平衡器。
</details></li>
</ul>
<hr>
<h2 id="RIGA-A-Regret-Based-Interactive-Genetic-Algorithm"><a href="#RIGA-A-Regret-Based-Interactive-Genetic-Algorithm" class="headerlink" title="RIGA: A Regret-Based Interactive Genetic Algorithm"></a>RIGA: A Regret-Based Interactive Genetic Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06063">http://arxiv.org/abs/2311.06063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nawal Benabbou, Cassandre Leroy, Thibaut Lust</li>
<li>For: 解决多目标 combinatorial 优化问题中的偏好不确定性问题（ preference imprecision problem）。* Methods: 使用互动遗传算法（Interactive Genetic Algorithm，IGA），其包括：	+ 使用 regret-based elicitation 技术缩小参数空间。	+ 在参数实例上应用 génétiques 运算（genetic operators）来更好地探索参数空间。	+ 使用现有的解决方案（solving methods）来生成有前景的解（promising solutions）。* Results: 对多目标包袋和旅行团队问题进行了测试，并证明了 RIGA 可以在有界时间内运行，并且不超过一定的数量的查询。同时，对多个表现指标（computation times, gap to optimality, number of queries），RIGAs 的表现比现有的算法更好。<details>
<summary>Abstract</summary>
In this paper, we propose an interactive genetic algorithm for solving multi-objective combinatorial optimization problems under preference imprecision. More precisely, we consider problems where the decision maker's preferences over solutions can be represented by a parameterized aggregation function (e.g., a weighted sum, an OWA operator, a Choquet integral), and we assume that the parameters are initially not known by the recommendation system. In order to quickly make a good recommendation, we combine elicitation and search in the following way: 1) we use regret-based elicitation techniques to reduce the parameter space in a efficient way, 2) genetic operators are applied on parameter instances (instead of solutions) to better explore the parameter space, and 3) we generate promising solutions (population) using existing solving methods designed for the problem with known preferences. Our algorithm, called RIGA, can be applied to any multi-objective combinatorial optimization problem provided that the aggregation function is linear in its parameters and that a (near-)optimal solution can be efficiently determined for the problem with known preferences. We also study its theoretical performances: RIGA can be implemented in such way that it runs in polynomial time while asking no more than a polynomial number of queries. The method is tested on the multi-objective knapsack and traveling salesman problems. For several performance indicators (computation times, gap to optimality and number of queries), RIGA obtains better results than state-of-the-art algorithms.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种互动式遗传算法，用于解决具有偏好不确定性的多目标组合优化问题。具体来说，我们考虑了具有以下特点的问题：解决方案的偏好可以通过一个参数化的汇聚函数（例如Weighted sum、OWA运算符、Choquet积分）来表示，并且偏好参数在初始化时并不知道。为了快速提供高质量的建议，我们将感知和搜索结合使用，具体来说是：1）使用 regret-based elicitation技术来减少参数空间，2）在参数实例（而不是解决方案）上应用遗传运算，3）使用现有的解决方案设计方法来生成优秀的解决方案（人口）。我们称之为RIGА算法，它可以应用于任何多目标组合优化问题，只要汇聚函数是线性的，并且可以有效地确定（或近似）优质解决方案。我们还研究了其理论性能：RIGА算法可以在 polynomial 时间内运行，并且只需要对问题进行 polynomial 数量的询问。我们测试了这种方法在多重目标随机抽样问题和多重目标随机包问题上，并证明了它在几个性能指标（计算时间、落差和询问数）上表现更好。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-3D-Object-Detection-and-Tracking-in-Autonomous-Driving-A-Brief-Survey"><a href="#Deep-learning-for-3D-Object-Detection-and-Tracking-in-Autonomous-Driving-A-Brief-Survey" class="headerlink" title="Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A Brief Survey"></a>Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A Brief Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06043">http://arxiv.org/abs/2311.06043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Peng</li>
<li>for: 本研究主要针对3D点云数据进行对象检测和跟踪任务，以提高自动驾驶系统的性能。</li>
<li>methods: 本文主要介绍最新的深度学习方法 для3D对象检测和跟踪，包括PointNet、PointNet++、DGCNN等。</li>
<li>results: 本文综合比较了不同方法的实验结果，并提出了未来研究的方向，以帮助读者更好地了解3D点云数据的对象检测和跟踪任务。<details>
<summary>Abstract</summary>
Object detection and tracking are vital and fundamental tasks for autonomous driving, aiming at identifying and locating objects from those predefined categories in a scene. 3D point cloud learning has been attracting more and more attention among all other forms of self-driving data. Currently, there are many deep learning methods for 3D object detection. However, the tasks of object detection and tracking for point clouds still need intensive study due to the unique characteristics of point cloud data. To help get a good grasp of the present situation of this research, this paper shows recent advances in deep learning methods for 3D object detection and tracking.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CN对象探测和跟踪是自动驾驶中非常重要和基本的任务，目的是在场景中从先定的类别中标识和定位对象。三维点云学习在所有自驾数据中受到更多的关注。目前有许多深度学习方法 для 3D 对象探测。但是对于点云数据的特殊特点，对象探测和跟踪 tasks 仍然需要进一步的研究。为了帮助更好地了解这个研究的现状，本文介绍了最新的深度学习方法 для 3D 对象探测和跟踪。Note: I've set the `translate_language` parameter to `zh-CN` to indicate that the text should be translated into Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Reviewing-Developments-of-Graph-Convolutional-Network-Techniques-for-Recommendation-Systems"><a href="#Reviewing-Developments-of-Graph-Convolutional-Network-Techniques-for-Recommendation-Systems" class="headerlink" title="Reviewing Developments of Graph Convolutional Network Techniques for Recommendation Systems"></a>Reviewing Developments of Graph Convolutional Network Techniques for Recommendation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06323">http://arxiv.org/abs/2311.06323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haojun Zhu, Vikram Kapoor, Priya Sharma</li>
<li>for: 这篇论文旨在探讨近期关于推荐系统的研究，具体来说是 Graph Neural Network（GNNS）在推荐系统中的应用。</li>
<li>methods: 论文主要考虑了推荐系统的背景和发展，以及Graph Neural Network（GNNS）的背景和发展。然后，根据设置和图神经网络的 spectral 和 spatial 模型，分类了推荐系统。</li>
<li>results: 论文分析了图神经网络在推荐系统中的挑战和开放问题，包括图构建、嵌入传播和聚合以及计算效率等。这些分析帮助我们更好地探索未来的发展方向。<details>
<summary>Abstract</summary>
The Recommender system is a vital information service on today's Internet. Recently, graph neural networks have emerged as the leading approach for recommender systems. We try to review recent literature on graph neural network-based recommender systems, covering the background and development of both recommender systems and graph neural networks. Then categorizing recommender systems by their settings and graph neural networks by spectral and spatial models, we explore the motivation behind incorporating graph neural networks into recommender systems. We also analyze challenges and open problems in graph construction, embedding propagation and aggregation, and computation efficiency. This guides us to better explore the future directions and developments in this domain.
</details>
<details>
<summary>摘要</summary>
“推荐系统是今天互联网上重要的资讯服务。最近，图 neural network 已经成为推荐系统的主要方法。我们尝试综述最近的文献，探讨推荐系统和图 neural network 的背景和发展，以及将推荐系统分为不同的设定和将图 neural network 分为спектраль和空间模型。我们也分析了将图 neural network 应用到推荐系统的动机，以及构建图、传播嵌入和聚合的挑战和开放问题。这导我们更好地探索未来的发展方向。”Note: Simplified Chinese is used here, as it is more commonly used in mainland China. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Actuarial-Non-Life-Pricing-Models-via-Transformers"><a href="#Enhancing-Actuarial-Non-Life-Pricing-Models-via-Transformers" class="headerlink" title="Enhancing Actuarial Non-Life Pricing Models via Transformers"></a>Enhancing Actuarial Non-Life Pricing Models via Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07597">http://arxiv.org/abs/2311.07597</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BrauerAlexej/Enhancing_actuarial_non-life_pricing_models_via_transformers_Public">https://github.com/BrauerAlexej/Enhancing_actuarial_non-life_pricing_models_via_transformers_Public</a></li>
<li>paper_authors: Alexej Brauer</li>
<li>for: 提高非人寿保险价格预测力，基于 transformer 模型对简洁数据进行增强</li>
<li>methods: 使用 novel 方法增强 actuarial non-life 模型，包括 feature tokenizer transformer 和 LocalGLMnet</li>
<li>results: 比较了多种 referential 模型，包括 generalized linear models、feed-forward neural networks、combined actuarial neural networks、LocalGLMnet 和 pure feature tokenizer transformer，并证明新方法可以在 real-world  claim frequency 数据上达到更好的结果，同时保持一定的 generalized linear model 优点<details>
<summary>Abstract</summary>
Currently, there is a lot of research in the field of neural networks for non-life insurance pricing. The usual goal is to improve the predictive power via neural networks while building upon the generalized linear model, which is the current industry standard. Our paper contributes to this current journey via novel methods to enhance actuarial non-life models with transformer models for tabular data. We build here upon the foundation laid out by the combined actuarial neural network as well as the localGLMnet and enhance those models via the feature tokenizer transformer. The manuscript demonstrates the performance of the proposed methods on a real-world claim frequency dataset and compares them with several benchmark models such as generalized linear models, feed-forward neural networks, combined actuarial neural networks, LocalGLMnet, and pure feature tokenizer transformer. The paper shows that the new methods can achieve better results than the benchmark models while preserving certain generalized linear model advantages. The paper also discusses the practical implications and challenges of applying transformer models in actuarial settings.
</details>
<details>
<summary>摘要</summary>
当前， neuronal networks 在非生命保险价值评估领域中有很多研究。目标通常是通过 neuronal networks 提高预测力，而基于现有的泛化线性模型（Generalized Linear Model， GLM）。我们的论文在这个领域中做出了贡献，通过 novel methods 增强 actuarial non-life 模型。我们在 combined actuarial neural network 和 localGLMnet 基础上建立了新的模型，并使用 feature tokenizer transformer 进行增强。 manuscript 中对实际的审核频率数据集进行了表现测试，并与多个 Referential models，如 generalized linear models、feed-forward neural networks、combined actuarial neural networks、LocalGLMnet 和 pure feature tokenizer transformer 进行比较。结果显示，新方法可以在 benchmark models 之上 achieve better results，同时保持一定的 Generalized Linear Model 优点。论文还讨论了应用 transformer models 在 actuarial 设置中的实际意义和挑战。
</details></li>
</ul>
<hr>
<h2 id="RSG-Fast-Learning-Adaptive-Skills-for-Quadruped-Robots-by-Skill-Graph"><a href="#RSG-Fast-Learning-Adaptive-Skills-for-Quadruped-Robots-by-Skill-Graph" class="headerlink" title="RSG: Fast Learning Adaptive Skills for Quadruped Robots by Skill Graph"></a>RSG: Fast Learning Adaptive Skills for Quadruped Robots by Skill Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06015">http://arxiv.org/abs/2311.06015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyin Zhang, Diyuan Shi, Zifeng Zhuang, Han Zhao, Zhenyu Wei, Feng Zhao, Sibo Gai, Shangke Lyu, Donglin Wang</li>
<li>for: 本研究旨在提高机器人自主化的速度和适应能力，通过组织机器人庞大的基本技能，以便快速适应未知的野外情况。</li>
<li>methods: 本研究提出了一种名为机器人技能图（RSG）的新框架，它通过组织机器人庞大的基本技能，以便发现机器人学习过程中的隐藏关系，并帮助机器人快速适应新任务和环境。</li>
<li>results: 实验结果表明，RSG可以为机器人提供合理的技能推理，并使四肢机器人快速适应新的情况和学习新的技能。<details>
<summary>Abstract</summary>
Developing robotic intelligent systems that can adapt quickly to unseen wild situations is one of the critical challenges in pursuing autonomous robotics. Although some impressive progress has been made in walking stability and skill learning in the field of legged robots, their ability to fast adaptation is still inferior to that of animals in nature. Animals are born with massive skills needed to survive, and can quickly acquire new ones, by composing fundamental skills with limited experience. Inspired by this, we propose a novel framework, named Robot Skill Graph (RSG) for organizing massive fundamental skills of robots and dexterously reusing them for fast adaptation. Bearing a structure similar to the Knowledge Graph (KG), RSG is composed of massive dynamic behavioral skills instead of static knowledge in KG and enables discovering implicit relations that exist in be-tween of learning context and acquired skills of robots, serving as a starting point for understanding subtle patterns existing in robots' skill learning. Extensive experimental results demonstrate that RSG can provide rational skill inference upon new tasks and environments and enable quadruped robots to adapt to new scenarios and learn new skills rapidly.
</details>
<details>
<summary>摘要</summary>
开发能够快速适应未经见过的野外情况的机器人智能系统是探索自主机器人的一个核心挑战。虽然有些很出色的进步在四肢机器人的步态稳定和技能学习方面，但它们的快速适应仍然不如自然界中的动物。动物出生时拥有大量需要生存的基础技能，并可以快速获得新的技能，通过精心组合基本技能和有限的经验。受这种启示，我们提出了一个新的框架，即机器人技能图（RSG），用于组织机器人的基本技能和重用它们 для快速适应。RSG的结构类似知识图（KG），但是它使用动态行为技能而不是静态知识，可以发现机器人学习过程中存在的潜在关系，并作为机器人技能学习的开始点，以便理解机器人技能学习中的细微趋势。我们的实验结果表明，RSG可以为机器人提供合理的技能推理，并使四肢机器人快速适应新任务和环境，快速学习新技能。
</details></li>
</ul>
<hr>
<h2 id="JARVIS-1-Open-World-Multi-task-Agents-with-Memory-Augmented-Multimodal-Language-Models"><a href="#JARVIS-1-Open-World-Multi-task-Agents-with-Memory-Augmented-Multimodal-Language-Models" class="headerlink" title="JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models"></a>JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05997">http://arxiv.org/abs/2311.05997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CraftJarvis/JARVIS-1">https://github.com/CraftJarvis/JARVIS-1</a></li>
<li>paper_authors: Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang</li>
<li>for: 这个论文旨在创造一种可以在开放世界中实现人类化规划和控制的机器人，以便实现更加功能强大的总体智能代理人。</li>
<li>methods: 该论文使用了预训练的多模态语言模型，将视觉观察和文本指令映射到计划中，然后通过目标conditioned控制器执行。它还使用了多模态记忆，以便通过实际游戏存活经历和预训练知识来进行规划。</li>
<li>results: 在 Minecraft 宇宙测试 benchmark 中，JARVIS-1 展现出了 nearly perfect 的表现，完成了200多个任务，其中包括从入门到中等水平的任务。JARVIS-1 在长期任务中取得了12.5%的完成率，这与之前的记录比起来是5倍的提高。此外，JARVIS-1 还能够自我提升，这是因为它使用了多模态记忆，这种自我提升可以持续进行，从而实现更好的智能和自主性。<details>
<summary>Abstract</summary>
Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. In our experiments, JARVIS-1 exhibits nearly perfect performances across over 200 varying tasks from the Minecraft Universe Benchmark, ranging from entry to intermediate levels. JARVIS-1 has achieved a completion rate of 12.5% in the long-horizon diamond pickaxe task. This represents a significant increase up to 5 times compared to previous records. Furthermore, we show that JARVIS-1 is able to $\textit{self-improve}$ following a life-long learning paradigm thanks to multimodal memory, sparking a more general intelligence and improved autonomy. The project page is available at https://craftjarvis-jarvis1.github.io.
</details>
<details>
<summary>摘要</summary>
实现人类化规划和控制，使用多Modal观察在开放世界中是功能普通代理的关键里程碑。现有方法可以处理某些长期任务在开放世界中，但它们仍然在数量可能无限的任务中受到挑战，而且缺乏逐渐提高任务完成度的能力。我们介绍JARVIS-1，一个在 Minecraft 宇宙中运行的开放世界代理，可以感知多Modal输入（视觉观察和人工指令），生成复杂的计划，并执行具体的控制，全部运行在 Minecraft 游戏中。具体来说，我们基于预训练的多Modal语言模型，将视觉观察和文本指令映射到计划。计划将被最终转交给目标受控器。我们为 JARVIS-1 增加了多Modal 记忆，以便通过预训练知识和实际游戏生存经验来帮助计划。在我们的实验中，JARVIS-1 在 Minecraft Universe Benchmark 上 exhibits  nearly perfect 性能，包括多达 200 个任务，覆盖从入门到中级水平。JARVIS-1 在长期钻石钻刀任务中达到了12.5%的完成率，这比前一记录提高了5倍。此外，我们表明 JARVIS-1 能够 $\textit{自我改进}$ ，采用生命长学学习模式，增强智能和自主性。项目页面可以在 <https://craftjarvis-jarvis1.github.io> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Robust-Adversarial-Attacks-Detection-for-Deep-Learning-based-Relative-Pose-Estimation-for-Space-Rendezvous"><a href="#Robust-Adversarial-Attacks-Detection-for-Deep-Learning-based-Relative-Pose-Estimation-for-Space-Rendezvous" class="headerlink" title="Robust Adversarial Attacks Detection for Deep Learning based Relative Pose Estimation for Space Rendezvous"></a>Robust Adversarial Attacks Detection for Deep Learning based Relative Pose Estimation for Space Rendezvous</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05992">http://arxiv.org/abs/2311.05992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Wang, Nabil Aouf, Jose Pizarro, Christophe Honvault</li>
<li>for: 本研究旨在提高自主空间飞行器相对导航中使用深度学习技术的性能，但是这些技术也增加了对其可靠性和安全性的担忧，尤其是对于深度学习方法的抗击攻击。本文提出了一种基于解释性理念的异常检测方法来检测深度神经网络基于相对pose估计中的异常攻击。</li>
<li>methods: 本文提出了一种基于Convolutional Neural Network (CNN)的新型相对pose估计技术，该技术使用了图像从追踪器上的摄像头获取，并输出了目标的相对位置和旋转矩阵。此外，本文还使用了Fast Gradient Sign Method (FGSM)生成的各种各样的异常攻击来让模型适应不同的异常攻击情况。</li>
<li>results: 实验结果显示，提出的异常检测方法可以准确地检测异常攻击，其检测精度为99.21%。此外，在实验室设置中使用了真实数据进行测试，实验结果表明，提出的异常检测方法在实际应用中可以达到96.29%的检测精度。<details>
<summary>Abstract</summary>
Research on developing deep learning techniques for autonomous spacecraft relative navigation challenges is continuously growing in recent years. Adopting those techniques offers enhanced performance. However, such approaches also introduce heightened apprehensions regarding the trustability and security of such deep learning methods through their susceptibility to adversarial attacks. In this work, we propose a novel approach for adversarial attack detection for deep neural network-based relative pose estimation schemes based on the explainability concept. We develop for an orbital rendezvous scenario an innovative relative pose estimation technique adopting our proposed Convolutional Neural Network (CNN), which takes an image from the chaser's onboard camera and outputs accurately the target's relative position and rotation. We perturb seamlessly the input images using adversarial attacks that are generated by the Fast Gradient Sign Method (FGSM). The adversarial attack detector is then built based on a Long Short Term Memory (LSTM) network which takes the explainability measure namely SHapley Value from the CNN-based pose estimator and flags the detection of adversarial attacks when acting. Simulation results show that the proposed adversarial attack detector achieves a detection accuracy of 99.21%. Both the deep relative pose estimator and adversarial attack detector are then tested on real data captured from our laboratory-designed setup. The experimental results from our laboratory-designed setup demonstrate that the proposed adversarial attack detector achieves an average detection accuracy of 96.29%.
</details>
<details>
<summary>摘要</summary>
研究在开发深度学习技术以提高自主空间飞行器相对导航的挑战在最近几年内不断增长。采用这些技术可以提高性能，但这些方法也增加了对深度学习方法的信任和安全性的担忧，尤其是它们对抗性攻击的敏感性。在这项工作中，我们提出了一种基于 explainability 概念的对深度神经网络 pose 估计方法的 adversarial 攻击检测方法。我们在推送器上的摄像头拍摄的图像上采用我们提出的卷积神经网络（CNN），输出target的相对位置和旋转精度。我们使用 Fast Gradient Sign Method（FGSM）生成的抗击性攻击来略微地扰乱输入图像。然后，我们根据 Long Short Term Memory（LSTM）网络来建立一个基于 explainability 度的 adversarial 攻击检测器，这里的 explainability 度是 CNN 基于 pose 估计器输出的 SHapley Value。实验结果显示，我们的 adversarial 攻击检测器在 simulated 数据上达到了 99.21% 的检测精度。在实验室设置中测试的实际数据上，我们的 adversarial 攻击检测器的平均检测精度为 96.29%。
</details></li>
</ul>
<hr>
<h2 id="A-Decision-Support-System-for-Liver-Diseases-Prediction-Integrating-Batch-Processing-Rule-Based-Event-Detection-and-SPARQL-Query"><a href="#A-Decision-Support-System-for-Liver-Diseases-Prediction-Integrating-Batch-Processing-Rule-Based-Event-Detection-and-SPARQL-Query" class="headerlink" title="A Decision Support System for Liver Diseases Prediction: Integrating Batch Processing, Rule-Based Event Detection and SPARQL Query"></a>A Decision Support System for Liver Diseases Prediction: Integrating Batch Processing, Rule-Based Event Detection and SPARQL Query</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07595">http://arxiv.org/abs/2311.07595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritesh Chandra, Sadhana Tiwari, Satyam Rastogi, Sonali Agarwal</li>
<li>for: 这个研究的目的是构建一个预测肝病的模型，以帮助医生更好地诊断和预测肝病。</li>
<li>methods: 这个研究使用Basic Formal Ontology (BFO)和基于决策树算法的检测规则，通过批处理使用Apache Jena框架检测事件，并使用SPARQL进行直接处理。</li>
<li>results: 这个研究使用SWRL规则将DT规则转换为ontology中的Semantic Web Rule Language (SWRL)，并使用Pellet和Drool推理引擎在Protege工具中进行推理，最终可以为病人根据DT规则生成结果，并获得与病人相关的其他细节和不同预防建议。<details>
<summary>Abstract</summary>
Liver diseases pose a significant global health burden, impacting a substantial number of individuals and exerting substantial economic and social consequences. Rising liver problems are considered a fatal disease in many countries, such as Egypt, Molda, etc. The objective of this study is to construct a predictive model for liver illness using Basic Formal Ontology (BFO) and detection rules derived from a decision tree algorithm. Based on these rules, events are detected through batch processing using the Apache Jena framework. Based on the event detected, queries can be directly processed using SPARQL. To make the ontology operational, these Decision Tree (DT) rules are converted into Semantic Web Rule Language (SWRL). Using this SWRL in the ontology for predicting different types of liver disease with the help of the Pellet and Drool inference engines in Protege Tools, a total of 615 records are taken from different liver diseases. After inferring the rules, the result can be generated for the patient according to the DT rules, and other patient-related details along with different precautionary suggestions can be obtained based on these results. Combining query results of batch processing and ontology-generated results can give more accurate suggestions for disease prevention and detection. This work aims to provide a comprehensive approach that is applicable for liver disease prediction, rich knowledge graph representation, and smart querying capabilities. The results show that combining RDF data, SWRL rules, and SPARQL queries for analysing and predicting liver disease can help medical professionals to learn more about liver diseases and make a Decision Support System (DSS) for health care.
</details>
<details>
<summary>摘要</summary>
肝病对全球健康带来重大的影响，影响了大量人口并且对健康系统和社会带来了巨大的经济和社会影响。肝病在许多国家被视为致命疾病，如 Egyp、Molda等国。本研究的目标是使用基本正式 ontology（BFO）和基于决策树算法 derive的检测规则来建立预测肝病的模型。通过批处理，Apache Jena框架中的事件被检测，并基于检测到的事件，使用 SPARQL 进行直接处理。为了使 ontology 操作，这些决策树（DT）规则被转换为 Semantic Web Rule Language（SWRL）。使用这些 SWRL 在 ontology 中预测不同类型的肝病，并使用 Protege 工具中的 Pellet 和 Drool 推理引擎，共计615个记录来自不同的肝病。 after inferring the rules, the result can be generated for the patient according to the DT rules, and other patient-related details along with different precautionary suggestions can be obtained based on these results。通过将批处理的查询结果和 ontology 生成的结果组合，可以给出更加准确的疾病预测和预防建议。本工作的目标是提供一种通用的方法，可以用于肝病预测、丰富的知识图表示和智能查询能力。结果表明，将 RDF 数据、SWRL 规则和 SPARQL 查询结合分析和预测肝病，可以帮助医疗专业人员更好地了解肝病，并建立一个智能决策支持系统（DSS） для健康医疗。
</details></li>
</ul>
<hr>
<h2 id="How-to-Bridge-the-Gap-between-Modalities-A-Comprehensive-Survey-on-Multimodal-Large-Language-Model"><a href="#How-to-Bridge-the-Gap-between-Modalities-A-Comprehensive-Survey-on-Multimodal-Large-Language-Model" class="headerlink" title="How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model"></a>How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07594">http://arxiv.org/abs/2311.07594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shezheng Song, Xiaopeng Li, Shasha Li</li>
<li>for: This paper explores the use of Multimodal Large Language Models (MLLMs) to handle multimodal data and their potential applications in real-world human-computer interactions and artificial general intelligence.</li>
<li>methods: The paper surveys existing modality alignment methods for MLLMs, including Multimodal Converters, Multimodal Perceivers, Tools Assistance, and Data-Driven methods.</li>
<li>results: The paper discusses the challenges of processing the semantic gap in multimodality and the potential risks of erroneous generation, and highlights the importance of choosing appropriate modality alignment methods for LLMs to address environmental issues and enhance accessibility.Here is the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文探讨了大型语言模型（LLMs）如何处理多Modal数据，以及其在人机交互和人工智能潜在应用方面的潜力。</li>
<li>methods: 论文综述了现有的多Modal信息对齐方法，包括多Modal转换器、多Modal感知器、工具助手和数据驱动方法。</li>
<li>results: 论文讨论了多Modal数据的含义差距处理的挑战和可能的错误生成风险，并强调了选择合适的多Modal信息对齐方法，以解决环境问题和提高可用性。<details>
<summary>Abstract</summary>
This review paper explores Multimodal Large Language Models (MLLMs), which integrate Large Language Models (LLMs) like GPT-4 to handle multimodal data such as text and vision. MLLMs demonstrate capabilities like generating image narratives and answering image-based questions, bridging the gap towards real-world human-computer interactions and hinting at a potential pathway to artificial general intelligence. However, MLLMs still face challenges in processing the semantic gap in multimodality, which may lead to erroneous generation, posing potential risks to society. Choosing the appropriate modality alignment method is crucial, as improper methods might require more parameters with limited performance improvement. This paper aims to explore modality alignment methods for LLMs and their existing capabilities. Implementing modality alignment allows LLMs to address environmental issues and enhance accessibility. The study surveys existing modal alignment methods in MLLMs into four groups: (1) Multimodal Converters that change data into something LLMs can understand; (2) Multimodal Perceivers to improve how LLMs perceive different types of data; (3) Tools Assistance for changing data into one common format, usually text; and (4) Data-Driven methods that teach LLMs to understand specific types of data in a dataset. This field is still in a phase of exploration and experimentation, and we will organize and update various existing research methods for multimodal information alignment.
</details>
<details>
<summary>摘要</summary>
这篇评论文章探讨了多模态大语言模型（MLLM），它们将大语言模型（LLM）如GPT-4 integrated into多模态数据处理，如文本和视觉。 MLLMs 示出了生成图像故事和回答图像问题的能力， bridge the gap towards real-world human-computer interactions and hint at a potential pathway to artificial general intelligence。然而， MLLMs 在多模态 semantic gap处理方面仍面临挑战，可能导致错误生成， posing potential risks to society。选择合适的模态对齐方法是关键，因为不当的方法可能需要更多的参数，但具有有限的性能提升。这篇文章探讨了LLMs 的现有能力和现有的模态对齐方法，以实现环境问题和访问ibilty。对于现有的模态对齐方法，我们将它们分为四个组：（1）多模态转换器，将数据转换成LLMs可以理解的形式；（2）多模态感知器，提高LLMs 对不同类型数据的感知能力；（3）工具助手，将数据转换成一种常见的文本格式；（4）数据驱动方法，教导LLMs 理解特定的数据集中的特定类型数据。这个领域仍处于探索和实验阶段，我们将组织和更新现有的研究方法，以便在多模态信息对齐方面进行进一步的发展。
</details></li>
</ul>
<hr>
<h2 id="TransformCode-A-Contrastive-Learning-Framework-for-Code-Embedding-via-Subtree-transformation"><a href="#TransformCode-A-Contrastive-Learning-Framework-for-Code-Embedding-via-Subtree-transformation" class="headerlink" title="TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree transformation"></a>TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08157">http://arxiv.org/abs/2311.08157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiang Xian, Rubing Huang, Dave Towey, Chunrong Fang, Zhenyu Chen</li>
<li>for: 本研究旨在提出一种新的框架，即TransformCode，用于学习代码嵌入。</li>
<li>methods: 该框架使用TransformerEncoder作为模型的重要组成部分，并引入了一种新的数据采样技术 called abstract syntax tree transformation。</li>
<li>results: 我们的方法可以快速和效率地学习代码嵌入，并且可以适应不同的编程语言和任务。我们通过对不同的软件工程任务和多个数据集进行广泛的实验来证明方法的效果。<details>
<summary>Abstract</summary>
Large-scale language models have made great progress in the field of software engineering in recent years. They can be used for many code-related tasks such as code clone detection, code-to-code search, and method name prediction. However, these large-scale language models based on each code token have several drawbacks: They are usually large in scale, heavily dependent on labels, and require a lot of computing power and time to fine-tune new datasets.Furthermore, code embedding should be performed on the entire code snippet rather than encoding each code token. The main reason for this is that encoding each code token would cause model parameter inflation, resulting in a lot of parameters storing information that we are not very concerned about. In this paper, we propose a novel framework, called TransformCode, that learns about code embeddings in a contrastive learning manner. The framework uses the Transformer encoder as an integral part of the model. We also introduce a novel data augmentation technique called abstract syntax tree transformation: This technique applies syntactic and semantic transformations to the original code snippets to generate more diverse and robust anchor samples. Our proposed framework is both flexible and adaptable: It can be easily extended to other downstream tasks that require code representation such as code clone detection and classification. The framework is also very efficient and scalable: It does not require a large model or a large amount of training data, and can support any programming language.Finally, our framework is not limited to unsupervised learning, but can also be applied to some supervised learning tasks by incorporating task-specific labels or objectives. To explore the effectiveness of our framework, we conducted extensive experiments on different software engineering tasks using different programming languages and multiple datasets.
</details>
<details>
<summary>摘要</summary>
大规模语言模型在软件工程领域最近几年来所做出的进步非常大。它们可以用于许多代码相关任务，如代码副本检测、代码到代码搜索和方法名预测。然而，这些基于每个代码字符的大规模语言模型有几个缺点：它们通常很大，依赖于标签很强，需要许多计算机力和时间来调整新的数据集。此外，代码嵌入应该基于整个代码片段而不是每个代码字符编码。主要原因是，对每个代码字符进行编码会导致模型参数膨胀，导致很多参数存储不重要的信息。在这篇论文中，我们提出了一个新的框架，叫做TransformCode，它通过对代码嵌入进行对照学习来学习代码嵌入。框架使用Transformer编码器作为模型的一部分。我们还介绍了一种新的数据采样技术 called abstract syntax tree transformation，该技术对原始代码片段应用 sintactic和semantic 变换来生成更多元和更加稳定的锚样本。我们提出的框架具有灵活性和适应性：它可以轻松扩展到其他下游任务需要代码表示，例如代码副本检测和分类。此外，框架也非常高效和扩展：它不需要大型模型或大量训练数据，并且可以支持任何编程语言。最后，我们的框架不仅限于无监督学习，还可以应用到一些监督学习任务，只需要添加任务特定的标签或目标。为了评估我们的框架的效果，我们对不同的软件工程任务和不同编程语言的多个数据集进行了广泛的实验。
</details></li>
</ul>
<hr>
<h2 id="Genetic-Algorithm-enhanced-by-Deep-Reinforcement-Learning-in-parent-selection-mechanism-and-mutation-Minimizing-makespan-in-permutation-flow-shop-scheduling-problems"><a href="#Genetic-Algorithm-enhanced-by-Deep-Reinforcement-Learning-in-parent-selection-mechanism-and-mutation-Minimizing-makespan-in-permutation-flow-shop-scheduling-problems" class="headerlink" title="Genetic Algorithm enhanced by Deep Reinforcement Learning in parent selection mechanism and mutation : Minimizing makespan in permutation flow shop scheduling problems"></a>Genetic Algorithm enhanced by Deep Reinforcement Learning in parent selection mechanism and mutation : Minimizing makespan in permutation flow shop scheduling problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05937">http://arxiv.org/abs/2311.05937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maissa Irmouli, Nourelhouda Benazzoug, Alaa Dania Adimi, Fatma Zohra Rezkellah, Imane Hamzaoui, Thanina Hamitouche</li>
<li>for: 本研究使用强化学习（RL）方法解决复杂的 combinatorial 或非线性问题中的难题，特别是用于流shop scheduling problem（FSP）。</li>
<li>methods: 提议的 RL+GA 方法 integrate 神经网络（NN），并使用 Q-learning 或 Sarsa(0) 方法来控制 GA 算法中的两个关键运算：父选择机制和变异。在每一代，RL 代理的动作是确定选择方法、父选择概率和孪生变异概率。这allow RL 代理 dynamically 调整选择和变异 based on its 学习政策。</li>
<li>results: 研究结果表明 RL+GA 方法能够改进原始 GA 的性能，并且能够学习和适应人口多样性和解决方案改进随时间的演化过程。这种适应性导致在静态参数配置下获得的调度解决方案的改进。<details>
<summary>Abstract</summary>
This paper introduces a reinforcement learning (RL) approach to address the challenges associated with configuring and optimizing genetic algorithms (GAs) for solving difficult combinatorial or non-linear problems. The proposed RL+GA method was specifically tested on the flow shop scheduling problem (FSP). The hybrid algorithm incorporates neural networks (NN) and uses the off-policy method Q-learning or the on-policy method Sarsa(0) to control two key genetic algorithm (GA) operators: parent selection mechanism and mutation. At each generation, the RL agent's action is determining the selection method, the probability of the parent selection and the probability of the offspring mutation. This allows the RL agent to dynamically adjust the selection and mutation based on its learned policy. The results of the study highlight the effectiveness of the RL+GA approach in improving the performance of the primitive GA. They also demonstrate its ability to learn and adapt from population diversity and solution improvements over time. This adaptability leads to improved scheduling solutions compared to static parameter configurations while maintaining population diversity throughout the evolutionary process.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Anytime-Valid-Confidence-Sequences-for-Consistent-Uncertainty-Estimation-in-Early-Exit-Neural-Networks"><a href="#Anytime-Valid-Confidence-Sequences-for-Consistent-Uncertainty-Estimation-in-Early-Exit-Neural-Networks" class="headerlink" title="Anytime-Valid Confidence Sequences for Consistent Uncertainty Estimation in Early-Exit Neural Networks"></a>Anytime-Valid Confidence Sequences for Consistent Uncertainty Estimation in Early-Exit Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05931">http://arxiv.org/abs/2311.05931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/metodj/eenn-avcs">https://github.com/metodj/eenn-avcs</a></li>
<li>paper_authors: Metod Jazbec, Patrick Forré, Stephan Mandt, Dan Zhang, Eric Nalisnick</li>
<li>for: 这篇论文是关于使用早期离开神经网络（EENN）实现适应性推理，并生成可靠的不确定性估计的研究。</li>
<li>methods: 论文使用了标准的不确定性评估技术，如 bayesian 方法或充分预测，但这些技术可能会导致逻辑不一致的问题。</li>
<li>results: 论文使用 anytime-valid confidence sequences (AVCSs) 解决这个问题，并在 regression 和 classification 任务上进行了实验验证。<details>
<summary>Abstract</summary>
Early-exit neural networks (EENNs) facilitate adaptive inference by producing predictions at multiple stages of the forward pass. In safety-critical applications, these predictions are only meaningful when complemented with reliable uncertainty estimates. Yet, due to their sequential structure, an EENN's uncertainty estimates should also be consistent: labels that are deemed improbable at one exit should not reappear within the confidence interval / set of later exits. We show that standard uncertainty quantification techniques, like Bayesian methods or conformal prediction, can lead to inconsistency across exits. We address this problem by applying anytime-valid confidence sequences (AVCSs) to the exits of EENNs. By design, AVCSs maintain consistency across exits. We examine the theoretical and practical challenges of applying AVCSs to EENNs and empirically validate our approach on both regression and classification tasks.
</details>
<details>
<summary>摘要</summary>
Early-exit neural networks (EENNs) 可以实现适应性的推理，通过多个前进通道生成预测结果。在安全关键应用中，这些预测结果的准确性只有在 accompaniment with reliable uncertainty estimates 时才有意义。然而，由于 EENN 的序列结构，它们的uncertainty estimates 应该具有一定的一致性：在某个 exit 被评估为不可能时，后续 exit 的信息不应该重新出现在信任范围内。我们表明，标准的uncertainty量化技术，如 Bayesian 方法或充分预测，可能会导致 exit 之间的不一致。我们解决这个问题，通过应用 anytime-valid confidence sequences (AVCSs) 到 EENN 的 exit 处理。由于 AVCSs 的设计，它们可以保证 exit 之间的一致性。我们检查了应用 AVCSs 到 EENN 的理论和实践挑战，并对 regression 和 classification 任务进行了实验验证。
</details></li>
</ul>
<hr>
<h2 id="The-Shape-of-Learning-Anisotropy-and-Intrinsic-Dimensions-in-Transformer-Based-Models"><a href="#The-Shape-of-Learning-Anisotropy-and-Intrinsic-Dimensions-in-Transformer-Based-Models" class="headerlink" title="The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models"></a>The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05928">http://arxiv.org/abs/2311.05928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anton Razzhigaev, Matvey Mikhalchuk, Elizaveta Goncharova, Ivan Oseledets, Denis Dimitrov, Andrey Kuznetsov</li>
<li>for: 这个研究探讨了转换器架构中嵌入的动态异构性和自身维度问题，特别是编码器和解码器之间的对比。</li>
<li>methods: 这个研究使用了一种新的方法来研究嵌入的动态异构性和自身维度，包括对嵌入的分布进行分析和对嵌入的维度进行测量。</li>
<li>results: 研究发现，在解码器中的嵌入异构性表现出一个明确的bell型曲线，中间层的异构性最高，而编码器中的嵌入异构性则更加uniform。此外，研究还发现，在训练的初期阶段，嵌入的维度会增加，然后逐渐减少，表明在训练过程中，模型在嵌入空间中进行了扩展和细化。<details>
<summary>Abstract</summary>
In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. Which is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们展示了对转换器架构中嵌入的动态异构和内在维度的调查，特别是转换器Encoder和Decoder之间的对比。我们的发现显示，转换器Decoder中的异构性profile采取了一个明确的钟形曲线，中间层的异构性最高。这种模式与Encoder中的异构性更加 uniformly distributed 不同。此外，我们发现在训练的初期阶段，嵌入的内在维度会增加，表示在更高维度的空间中扩展。然后在训练的末期阶段，嵌入的维度会减少，表示向更加紧凑的表示进行了修finement。我们的结果为encoder和decoder嵌入性能的理解提供了新的视角。
</details></li>
</ul>
<hr>
<h2 id="Fake-Alignment-Are-LLMs-Really-Aligned-Well"><a href="#Fake-Alignment-Are-LLMs-Really-Aligned-Well" class="headerlink" title="Fake Alignment: Are LLMs Really Aligned Well?"></a>Fake Alignment: Are LLMs Really Aligned Well?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05915">http://arxiv.org/abs/2311.05915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang, Wenwei Zhang, Xingjun Ma, Yu-Gang Jiang, Yu Qiao, Yingchun Wang</li>
<li>for: 这个研究探讨了 LLM 的安全性评估问题，具体来说是多选题和开放题之间的性能差异。</li>
<li>methods: 该研究采用了基于犯罪攻击模式的研究方法，并提出了 fake alignment 现象，即 LLM 只记忆了安全问题的答案，而无法解决其他安全测试。</li>
<li>results: 该研究发现了许多广泛使用的 LLM 存在假Alignment现象，导致previous evaluation protocols 不可靠。在提出 fake alignment 和两个新的评价指标（Consistency Score 和 Consistent Safety Score）后，该研究引入了 Fake alIgNment Evaluation 框架，以评估 LLM 的安全性。<details>
<summary>Abstract</summary>
The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety within current research endeavors. This study investigates an interesting issue pertaining to the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, the LLM does not have a comprehensive understanding of the complex concept of safety. Instead, it only remembers what to answer for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. Such fake alignment renders previous evaluation protocols unreliable. To address this, we introduce the Fake alIgNment Evaluation (FINE) framework and two novel metrics--Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected performance estimates. Applying FINE to 14 widely-used LLMs reveals several models with purported safety are poorly aligned in practice. Our work highlights potential limitations in prevailing alignment methodologies.
</details>
<details>
<summary>摘要</summary>
LLMs 的安全问题正在引起越来越多的关注，这个研究探讨了 LLMS 的安全评估方法中的一个有趣问题，即多选题和开放式题的性能差异。我们根据监狱攻击模式的研究，提出了匹配混合泛化的问题，即 LLMS 对安全概念的理解不够全面，只记忆了开放式安全题的答案，无法解决其他安全测试形式。我们称这种现象为“假对齐”，并构建了比较指标来实验性证明其存在。这种假对齐使得以前的评估协议不可靠。为了解决这个问题，我们介绍了 Fake alIgNment Evaluation（FINE）框架和两个新的度量——一致度分数（CS）和安全一致分数（CSS），它们共同评估了两种不同的评估方法，以量化假对齐并获得修正后的性能估计。通过应用 FINE 到 14 种广泛使用的 LLMS 中，发现一些被认为具有安全的模型在实践中有假对齐问题。我们的工作高光了现有的对齐方法的局限性。
</details></li>
</ul>
<hr>
<h2 id="Establishing-Performance-Baselines-in-Fine-Tuning-Retrieval-Augmented-Generation-and-Soft-Prompting-for-Non-Specialist-LLM-Users"><a href="#Establishing-Performance-Baselines-in-Fine-Tuning-Retrieval-Augmented-Generation-and-Soft-Prompting-for-Non-Specialist-LLM-Users" class="headerlink" title="Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users"></a>Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05903">http://arxiv.org/abs/2311.05903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jennifer Dodgson, Lin Nanzheng, Julian Peh, Akira Rafhael Janson Pattirane, Alfath Daryl Alhajir, Eko Ridho Dinarto, Joseph Lim, Syed Danyal Ahmad</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）性能，通过微调、检索增强生成（RAG）和软引用等方法。</li>
<li>methods: 研究通常使用高级技术或高成本技术，使得许多新发现的方法对非技术用户而言是不可CCESSIBLE。本文测试了未修改版GPT 3.5、微调版本和使用 вектор化RAG数据库的同一模型，并在孤立和与基本、非算法式软引用结合使用下测试。</li>
<li>results: 研究发现，使用商业平台和默认设置，无论输出多少轮Iteration，微调模型比GPT 3.5 Turbo高效，而RAG方法则超越了两者。软引用的应用有助于每种方法的性能提高。<details>
<summary>Abstract</summary>
Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users. In this paper we tested an unmodified version of GPT 3.5, a fine-tuned version, and the same unmodified model when given access to a vectorised RAG database, both in isolation and in combination with a basic, non-algorithmic soft prompt. In each case we tested the model's ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5's training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs, a fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach out-performed both. The application of a soft prompt significantly improved the performance of each approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Performance-Driven-Benchmark-for-Feature-Selection-in-Tabular-Deep-Learning"><a href="#A-Performance-Driven-Benchmark-for-Feature-Selection-in-Tabular-Deep-Learning" class="headerlink" title="A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning"></a>A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05877">http://arxiv.org/abs/2311.05877</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vcherepanova/tabular-feature-selection">https://github.com/vcherepanova/tabular-feature-selection</a></li>
<li>paper_authors: Valeriia Cherepanova, Roman Levin, Gowthami Somepalli, Jonas Geiping, C. Bayan Bruss, Andrew Gordon Wilson, Tom Goldstein, Micah Goldblum</li>
<li>for: 本研究旨在提供一个有效的特征选择精选方法，用于适应 tabular deep learning 中的特征选择问题。</li>
<li>methods: 本研究使用了多种生成杂乱特征的方法，包括经典的批处理方法、批处理杂乱特征生成方法和缺失特征生成方法。</li>
<li>results: 本研究通过对实际数据集进行测试，发现input-gradient-based Lasso 方法在适应 corrupted 或 second-order 特征选择问题时表现出色，并且比经典的特征选择方法更高效。<details>
<summary>Abstract</summary>
Academic tabular benchmarks often contain small sets of curated features. In contrast, data scientists typically collect as many features as possible into their datasets, and even engineer new features from existing ones. To prevent overfitting in subsequent downstream modeling, practitioners commonly use automated feature selection methods that identify a reduced subset of informative features. Existing benchmarks for tabular feature selection consider classical downstream models, toy synthetic datasets, or do not evaluate feature selectors on the basis of downstream performance. Motivated by the increasing popularity of tabular deep learning, we construct a challenging feature selection benchmark evaluated on downstream neural networks including transformers, using real datasets and multiple methods for generating extraneous features. We also propose an input-gradient-based analogue of Lasso for neural networks that outperforms classical feature selection methods on challenging problems such as selecting from corrupted or second-order features.
</details>
<details>
<summary>摘要</summary>
学术表格标准 benchmark 常常包含小量精心选择的特征。相比之下，数据科学家通常尽可能多地收集特征到他们的数据集中，甚至从现有特征中引入新的特征。为防止适应度过高在后续的模型下，实践者通常使用自动化特征选择方法，以确定减少的特征subset。现有的表格特征选择标准对古典下游模型、娱乐生成的数据集或不会评估特征选择器的下游性能。我们受到表格深度学习的增加流行，我们构建了一个具有下游模型性能评估的特征选择 benchmark，使用真实数据和多种生成附加特征的方法。我们还提议一种输入Gradient-based的lasso方法，用于神经网络上的特征选择，其在具有受损或第二项特征的问题上表现出色。
</details></li>
</ul>
<hr>
<h2 id="DPR-An-Algorithm-Mitigate-Bias-Accumulation-in-Recommendation-feedback-loops"><a href="#DPR-An-Algorithm-Mitigate-Bias-Accumulation-in-Recommendation-feedback-loops" class="headerlink" title="DPR: An Algorithm Mitigate Bias Accumulation in Recommendation feedback loops"></a>DPR: An Algorithm Mitigate Bias Accumulation in Recommendation feedback loops</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05864">http://arxiv.org/abs/2311.05864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangtong Xu, Yuanbo Xu, Yongjian Yang, Fuzhen Zhuang, Hui Xiong</li>
<li>For: The paper aims to address the bias issues in recommendation models caused by user feedback, specifically the exposure mechanism and feedback loops.* Methods: The paper uses the Missing Not At Random (MNAR) assumption to analyze the data exposure mechanism and feedback loops, and proposes a dynamic re-weighting algorithm called Dynamic Personalized Ranking (DPR) to mitigate the cross-effects of exposure mechanisms and feedback loops.* Results: The paper theoretically demonstrates the effectiveness of the proposed approach in mitigating the negative effects of feedback loops and unknown exposure mechanisms. Experimental results on real-world datasets show that models using DPR can better handle bias accumulation, and the Universal Anti-False Negative (UFN) plugin can mitigate the negative impact of false negative samples.<details>
<summary>Abstract</summary>
Recommendation models trained on the user feedback collected from deployed recommendation systems are commonly biased. User feedback is considerably affected by the exposure mechanism, as users only provide feedback on the items exposed to them and passively ignore the unexposed items, thus producing numerous false negative samples. Inevitably, biases caused by such user feedback are inherited by new models and amplified via feedback loops. Moreover, the presence of false negative samples makes negative sampling difficult and introduces spurious information in the user preference modeling process of the model. Recent work has investigated the negative impact of feedback loops and unknown exposure mechanisms on recommendation quality and user experience, essentially treating them as independent factors and ignoring their cross-effects. To address these issues, we deeply analyze the data exposure mechanism from the perspective of data iteration and feedback loops with the Missing Not At Random (\textbf{MNAR}) assumption, theoretically demonstrating the existence of an available stabilization factor in the transformation of the exposure mechanism under the feedback loops. We further propose Dynamic Personalized Ranking (\textbf{DPR}), an unbiased algorithm that uses dynamic re-weighting to mitigate the cross-effects of exposure mechanisms and feedback loops without additional information. Furthermore, we design a plugin named Universal Anti-False Negative (\textbf{UFN}) to mitigate the negative impact of the false negative problem. We demonstrate theoretically that our approach mitigates the negative effects of feedback loops and unknown exposure mechanisms. Experimental results on real-world datasets demonstrate that models using DPR can better handle bias accumulation and the universality of UFN in mainstream loss methods.
</details>
<details>
<summary>摘要</summary>
推荐模型通常受到已部署的推荐系统中收集的用户反馈的偏见。用户反馈受到曝光机制的影响很大，用户只是提供曝光给他们的项目，并且忽略其他项目，因此生成了大量的假正样本。这些偏见会在新的模型中继承下来，并通过反馈循环被强制。此外，假正样本的存在使得负样本难以处理，并将偏见引入推荐过程中。 latest work has investigated the negative impact of feedback loops and unknown exposure mechanisms on recommendation quality and user experience, treating them as independent factors and ignoring their cross-effects. To address these issues, we deeply analyze the data exposure mechanism from the perspective of data iteration and feedback loops with the Missing Not At Random (\textbf{MNAR}) assumption, theoretically demonstrating the existence of an available stabilization factor in the transformation of the exposure mechanism under the feedback loops. We further propose Dynamic Personalized Ranking (\textbf{DPR}), an unbiased algorithm that uses dynamic re-weighting to mitigate the cross-effects of exposure mechanisms and feedback loops without additional information. Furthermore, we design a plugin named Universal Anti-False Negative (\textbf{UFN}) to mitigate the negative impact of the false negative problem. We demonstrate theoretically that our approach mitigates the negative effects of feedback loops and unknown exposure mechanisms. Experimental results on real-world datasets demonstrate that models using DPR can better handle bias accumulation and the universality of UFN in mainstream loss methods.
</details></li>
</ul>
<hr>
<h2 id="Reframing-Audience-Expansion-through-the-Lens-of-Probability-Density-Estimation"><a href="#Reframing-Audience-Expansion-through-the-Lens-of-Probability-Density-Estimation" class="headerlink" title="Reframing Audience Expansion through the Lens of Probability Density Estimation"></a>Reframing Audience Expansion through the Lens of Probability Density Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05853">http://arxiv.org/abs/2311.05853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carvalhaes-ai/audience-expansion">https://github.com/carvalhaes-ai/audience-expansion</a></li>
<li>paper_authors: Claudio Carvalhaes</li>
<li>for: 这篇论文旨在探讨如何使用机器学习算法扩大目标观众，以提高营销效果。</li>
<li>methods: 该论文使用了一种基于二分类 зада务的机器学习算法，通过计算样本的类别概率来扩大目标观众。</li>
<li>results:  simulations 表明，该方法可以准确地Identify the most relevant users for an expanded audience, with high precision and recall values.<details>
<summary>Abstract</summary>
Audience expansion has become an important element of prospective marketing, helping marketers create target audiences based on a mere representative sample of their current customer base. Within the realm of machine learning, a favored algorithm for scaling this sample into a broader audience hinges on a binary classification task, with class probability estimates playing a crucial role. In this paper, we review this technique and introduce a key change in how we choose training examples to ensure the quality of the generated audience. We present a simulation study based on the widely used MNIST dataset, where consistent high precision and recall values demonstrate our approach's ability to identify the most relevant users for an expanded audience. Our results are easily reproducible and a Python implementation is openly available on GitHub: \url{https://github.com/carvalhaes-ai/audience-expansion}
</details>
<details>
<summary>摘要</summary>
audi范拓已成为营销市场的重要元素，帮助市场部署创建基于当前客户基础的目标听众。在机器学习领域，一种受欢迎的算法用于扩大这个样本，基于二分类任务，其中类别概率估计具有关键作用。在这篇论文中，我们评论这种技术，并引入一个关键的更改，以确保生成的听众质量。我们通过基于广泛使用的 MNIST 数据集进行的 simulation 研究，发现我们的方法可以具有高精度和准确性。我们的结果可以重新制作，并在 GitHub 上公开提供 Python 实现：\url{https://github.com/carvalhaes-ai/audience-expansion}
</details></li>
</ul>
<hr>
<h2 id="Cognitive-Architecture-Toward-Common-Ground-Sharing-Among-Humans-and-Generative-AIs-Trial-on-Model-Model-Interactions-in-Tangram-Naming-Task"><a href="#Cognitive-Architecture-Toward-Common-Ground-Sharing-Among-Humans-and-Generative-AIs-Trial-on-Model-Model-Interactions-in-Tangram-Naming-Task" class="headerlink" title="Cognitive Architecture Toward Common Ground Sharing Among Humans and Generative AIs: Trial on Model-Model Interactions in Tangram Naming Task"></a>Cognitive Architecture Toward Common Ground Sharing Among Humans and Generative AIs: Trial on Model-Model Interactions in Tangram Naming Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05851">http://arxiv.org/abs/2311.05851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junya Morita, Tatsuya Yui, Takeru Amaya, Ryuichiro Higashinaka, Yugo Takeuchi</li>
<li>for: 这个研究的目的是为了建立人工智能与人类之间的透明共同基础，以促进人工智能的可信度。</li>
<li>methods: 这个研究使用了生成型AI来实现模型之间的共同基础建立过程。</li>
<li>results: 研究发现，透过实现共同基础，模型之间的通信效果超过了偶数几率水平，并且观察到了对模型中的一个部分进行逐步反向传播可以实现性能的 statistically significant 提升。<details>
<summary>Abstract</summary>
For generative AIs to be trustworthy, establishing transparent common grounding with humans is essential. As a preparation toward human-model common grounding, this study examines the process of model-model common grounding. In this context, common ground is defined as a cognitive framework shared among agents in communication, enabling the connection of symbols exchanged between agents to the meanings inherent in each agent. This connection is facilitated by a shared cognitive framework among the agents involved. In this research, we focus on the tangram naming task (TNT) as a testbed to examine the common-ground-building process. Unlike previous models designed for this task, our approach employs generative AIs to visualize the internal processes of the model. In this task, the sender constructs a metaphorical image of an abstract figure within the model and generates a detailed description based on this image. The receiver interprets the generated description from the partner by constructing another image and reconstructing the original abstract figure. Preliminary results from the study show an improvement in task performance beyond the chance level, indicating the effect of the common cognitive framework implemented in the models. Additionally, we observed that incremental backpropagations leveraging successful communication cases for a component of the model led to a statistically significant increase in performance. These results provide valuable insights into the mechanisms of common grounding made by generative AIs, improving human communication with the evolving intelligent machines in our future society.
</details>
<details>
<summary>摘要</summary>
为了让生成型AI变得可靠，建立与人类共同基础是必要的。为了实现人机共同基础，本研究研究了模型之间的共同基础建设。在这个上下文中，共同基础被定义为在交流中的智能框架，它使得交换 между代理人之间的符号与每个代理人内部的含义相连接。这种连接是通过共同智能框架的共享而实现。在本研究中，我们使用生成型AI来视觉化模型内部的过程。在这个任务中，发送方构建一个抽象图形内部的模型，并生成基于这个图形的详细描述。接收方根据伙伴的生成描述重新构建原始抽象图形。初步的研究结果显示，通过实施共同基础，任务性能超过了偶极值水平，这表明了共同智能框架在模型中的作用。此外，我们还发现，通过基于成功交流 caso的增量反向卷积，对一部分模型的性能进行了 statistically significant 的提高。这些结果为我们在将来社会中与智能机器进行交流的机制提供了有价值的发现。
</details></li>
</ul>
<hr>
<h2 id="Tamil-Llama-A-New-Tamil-Language-Model-Based-on-Llama-2"><a href="#Tamil-Llama-A-New-Tamil-Language-Model-Based-on-Llama-2" class="headerlink" title="Tamil-Llama: A New Tamil Language Model Based on Llama 2"></a>Tamil-Llama: A New Tamil Language Model Based on Llama 2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05845">http://arxiv.org/abs/2311.05845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhinand5/tamil-llama">https://github.com/abhinand5/tamil-llama</a></li>
<li>paper_authors: Abhinand Balachandran</li>
<li>for: 提高坦米语言模型的表现，尤其是在坦米语言上的人工智能文本生成。</li>
<li>methods: 使用LoRA方法对大量坦米文本资料进行有效的模型训练，并将ChatGPT模型扩展到16,000个坦米词汇上。</li>
<li>results: 在坦米文本生成和理解方面获得了显著的性能提升，具有潜在的应用在印度语言模型中。<details>
<summary>Abstract</summary>
Language modeling has witnessed remarkable advancements in recent years, with Large Language Models (LLMs) like ChatGPT setting unparalleled benchmarks in human-like text generation. However, a prevailing limitation is the underrepresentation of languages like Tamil in these cutting-edge models, leading to suboptimal performance in diverse linguistic contexts. This paper addresses this lacuna, enhancing the open-source LLaMA model with an addition of 16,000 Tamil tokens, aiming to achieve superior text generation and comprehension in the Tamil language. We strategically employ the LoRA methodology for efficient model training on a comprehensive Tamil corpus, ensuring computational feasibility and model robustness. Moreover, we introduce a Tamil-translated version of the Alpaca dataset and a subset of the OpenOrca dataset tailored for instruction fine-tuning. Our results showcase significant performance improvements in Tamil text generation, with potential implications for the broader landscape of LLMs in Indian languages. We further underscore our commitment to open research by making our models, datasets, and code publicly accessible, fostering further innovations in language modeling.
</details>
<details>
<summary>摘要</summary>
Large Language Models (LLMs) 如 ChatGPT 在最近几年内取得了无 precedent 的进步，但是有一点问题是一些语言，如 tamile 的语言，在这些先进模型中受到了不足的表现，这导致在多样化语言上的表现不佳。这篇文章解决了这个问题，通过将16,000个 tamile 单词添加到了开源的 LLaMA 模型中，以达到在 tamile 语言中的superior 文本生成和理解。我们使用 LoRA 方法学习在 comprehensive  tamile 词汇库上，以确保计算可行性和模型稳定性。此外，我们还引入了 tamile 翻译的 Alpaca 数据集和 OpenOrca 数据集的一个子集，用于 fine-tuning  instruction。我们的结果表明，在 tamile 文本生成方面有了显著的性能提高，这可能对整个 LLMS 的发展产生了影响。此外，我们还强调我们的研究是开放的，我们将我们的模型、数据集和代码公开 accessible，以促进进一步的语言模型化领域的创新。
</details></li>
</ul>
<hr>
<h2 id="AI-native-Interconnect-Framework-for-Integration-of-Large-Language-Model-Technologies-in-6G-Systems"><a href="#AI-native-Interconnect-Framework-for-Integration-of-Large-Language-Model-Technologies-in-6G-Systems" class="headerlink" title="AI-native Interconnect Framework for Integration of Large Language Model Technologies in 6G Systems"></a>AI-native Interconnect Framework for Integration of Large Language Model Technologies in 6G Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05842">http://arxiv.org/abs/2311.05842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sasu Tarkoma, Roberto Morabito, Jaakko Sauvola</li>
<li>for: 这篇论文旨在探讨6G体系中大语言模型（LLM）和通用预训Transformer（GPT）如何紧密结合，以及这种结合如何重塑通信网络的功能和交互方式。</li>
<li>methods: 本论文提出了一种新的建筑方式，即将LLM和GPT与传统的预生成AI和机器学习（ML）算法结合在一起，以实现一个以AI为核心的下一代通信体系。</li>
<li>results: 该论文预测，通过将AI作为下一代通信体系的核心，将能够提高通信网络的功能和交互方式，并且将有新的实际应用出现。<details>
<summary>Abstract</summary>
The evolution towards 6G architecture promises a transformative shift in communication networks, with artificial intelligence (AI) playing a pivotal role. This paper delves deep into the seamless integration of Large Language Models (LLMs) and Generalized Pretrained Transformers (GPT) within 6G systems. Their ability to grasp intent, strategize, and execute intricate commands will be pivotal in redefining network functionalities and interactions. Central to this is the AI Interconnect framework, intricately woven to facilitate AI-centric operations within the network. Building on the continuously evolving current state-of-the-art, we present a new architectural perspective for the upcoming generation of mobile networks. Here, LLMs and GPTs will collaboratively take center stage alongside traditional pre-generative AI and machine learning (ML) algorithms. This union promises a novel confluence of the old and new, melding tried-and-tested methods with transformative AI technologies. Along with providing a conceptual overview of this evolution, we delve into the nuances of practical applications arising from such an integration. Through this paper, we envisage a symbiotic integration where AI becomes the cornerstone of the next-generation communication paradigm, offering insights into the structural and functional facets of an AI-native 6G network.
</details>
<details>
<summary>摘要</summary>
六代网络架构的演化将导致一次性的 коммуникацион网络变革，人工智能（AI）将扮演关键角色。这篇论文探讨了在六代系统中大语言模型（LLM）和通用预训练变换器（GPT）的无缝嵌入。这些技术将能够捕捉意图、策略和执行复杂命令，对网络功能和交互进行重塑。核心在于人工智能集成框架，织入网络中AI-центри的操作。基于不断演化的当前状态艺术，我们提出了下一代移动网络的新建筑视图。在这个新视图中，LLMs和GPTs将与传统的预生成AI和机器学习（ML）算法一起Collaborate，创造一种新的旧和新的融合，将经验证过的方法与转变性AI技术融合。本论文不仅提供了这一演化的概念审视，还探讨了这种融合的实践应用。我们可以看到，AI将成为下一代通信 paradigma的基础 stone，提供对六代网络结构和功能方面的新的视角。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Augmented-Large-Language-Models-for-Personalized-Contextual-Query-Suggestion"><a href="#Knowledge-Augmented-Large-Language-Models-for-Personalized-Contextual-Query-Suggestion" class="headerlink" title="Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion"></a>Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06318">http://arxiv.org/abs/2311.06318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen herring, Sujay Kumar Jauhar</li>
<li>for: 这个研究旨在提高搜索引擎的搜索结果，使其更加个性化和有用。</li>
<li>methods: 该研究使用了一种新的方法，即在用户的搜索和浏览历史记录中提取有用的信息，并将其与大型自然语言模型（LLM）结合使用，以提高搜索结果的个性化性。</li>
<li>results: 研究表明，该方法可以提供更加个性化和有用的查询建议，比如其他LLM-基于的基elines。通过人工评估，该方法在Contextual Query Suggestion任务中表现出色，生成的查询建议更加相关、个性化和有用。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is light-weight, since it only produces user-specific aggregate projections of interests and knowledge onto public knowledge graphs, and leverages existing search log infrastructure, thereby mitigating the privacy, compliance, and scalability concerns associated with building deep user profiles for personalization. We then validate our approach on the task of contextual query suggestion, which requires understanding not only the user's current search context but also what they historically know and care about. Through a number of experiments based on human evaluation, we show that our approach is significantly better than several other LLM-powered baselines, generating query suggestions that are contextually more relevant, personalized, and useful.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is lightweight, as it only produces user-specific aggregate projections of interests and knowledge onto public knowledge graphs, and leverages existing search log infrastructure, thereby mitigating privacy, compliance, and scalability concerns associated with building deep user profiles for personalization.We validate our approach on the task of contextual query suggestion, which requires understanding not only the user's current search context but also what they historically know and care about. Through a number of experiments based on human evaluation, we show that our approach is significantly better than several other LLM-powered baselines, generating query suggestions that are contextually more relevant, personalized, and useful.
</details></li>
</ul>
<hr>
<h2 id="Model-as-a-Service-MaaS-A-Survey"><a href="#Model-as-a-Service-MaaS-A-Survey" class="headerlink" title="Model-as-a-Service (MaaS): A Survey"></a>Model-as-a-Service (MaaS): A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05804">http://arxiv.org/abs/2311.05804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wensheng Gan, Shicheng Wan, Philip S. Yu</li>
<li>for: 本研究旨在介绍Model-as-a-Service（MaaS） paradigma，它是一种基于云计算的Generative Artificial Intelligence（GenAI）模型的部署和使用方式。</li>
<li>methods: 本研究使用了cloud computing技术，并介绍了关键的MaaS技术。</li>
<li>results: 研究表明，MaaS将使GenAI模型的开发变得更加民主化，并且可以为不同领域的应用提供可观之服务。它还可以解决许多当前AI技术的挑战，如模型训练和部署等。<details>
<summary>Abstract</summary>
Due to the increased number of parameters and data in the pre-trained model exceeding a certain level, a foundation model (e.g., a large language model) can significantly improve downstream task performance and emerge with some novel special abilities (e.g., deep learning, complex reasoning, and human alignment) that were not present before. Foundation models are a form of generative artificial intelligence (GenAI), and Model-as-a-Service (MaaS) has emerged as a groundbreaking paradigm that revolutionizes the deployment and utilization of GenAI models. MaaS represents a paradigm shift in how we use AI technologies and provides a scalable and accessible solution for developers and users to leverage pre-trained AI models without the need for extensive infrastructure or expertise in model training. In this paper, the introduction aims to provide a comprehensive overview of MaaS, its significance, and its implications for various industries. We provide a brief review of the development history of "X-as-a-Service" based on cloud computing and present the key technologies involved in MaaS. The development of GenAI models will become more democratized and flourish. We also review recent application studies of MaaS. Finally, we highlight several challenges and future issues in this promising area. MaaS is a new deployment and service paradigm for different AI-based models. We hope this review will inspire future research in the field of MaaS.
</details>
<details>
<summary>摘要</summary>
由于预训过程中参数和数据的增加超过了一定水平，基础模型（例如大语言模型）可以显著提高下游任务性能，并且具有一些新的特殊能力（例如深度学习、复杂逻辑和人类匹配），这些能力在之前没有出现过。基础模型是生成人工智能（GenAI）的一种形式，而Model-as-a-Service（MaaS）是一种革命性的部署和使用GenAI模型的新 paradigma。MaaS将如何使用AI技术发生了一种巨大的变革，并提供了可扩展的和访问ible的解决方案，让开发者和用户可以无需具备大量的基础设施或模型训练专业知识来使用预训AI模型。在这篇论文中，我们 aim to provide a comprehensive overview of MaaS, its significance, and its implications for various industries. We will review the development history of "X-as-a-Service" based on cloud computing and present the key technologies involved in MaaS. With the development of GenAI models becoming more democratized, MaaS will flourish. We will also review recent application studies of MaaS. Finally, we will highlight several challenges and future issues in this promising area. MaaS是一种新的部署和服务 paradigma，我们希望这篇文章能启发未来的研究在这个领域。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-LLMs-for-Synthesizing-Training-Data-Across-Many-Languages-in-Multilingual-Dense-Retrieval"><a href="#Leveraging-LLMs-for-Synthesizing-Training-Data-Across-Many-Languages-in-Multilingual-Dense-Retrieval" class="headerlink" title="Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval"></a>Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05800">http://arxiv.org/abs/2311.05800</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/swim-ir">https://github.com/google-research-datasets/swim-ir</a></li>
<li>paper_authors: Nandan Thakur, Jianmo Ni, Gustavo Hernández Ábrego, John Wieting, Jimmy Lin, Daniel Cer</li>
<li>for: 这个论文主要针对的是如何使用人工生成的语言训练数据来提高多语言检索模型的性能。</li>
<li>methods: 这个论文提出了一种名为SAP（概要然后提问）的技术，其中使用大型自然语言处理器（LLM）生成文本概要，然后使用这个概要来生成目标语言中的问题。</li>
<li>results: 根据这个论文的结果，使用SWIM-IR数据集进行synthetic fine-tuning的多语言检索模型可以达到与人工supervised模型相当的性能，而且可以在三个检索测试benchmark上进行可靠的评估。<details>
<summary>Abstract</summary>
Dense retrieval models have predominantly been studied for English, where models have shown great success, due to the availability of human-labeled training pairs. However, there has been limited success for multilingual retrieval so far, as training data is uneven or scarcely available across multiple languages. Synthetic training data generation is promising (e.g., InPars or Promptagator), but has been investigated only for English. Therefore, to study model capabilities across both cross-lingual and monolingual retrieval tasks, we develop SWIM-IR, a synthetic retrieval training dataset containing 33 (high to very-low resource) languages for training multilingual dense retrieval models without requiring any human supervision. To construct SWIM-IR, we propose SAP (summarize-then-ask prompting), where the large language model (LLM) generates a textual summary prior to the query generation step. SAP assists the LLM in generating informative queries in the target language. Using SWIM-IR, we explore synthetic fine-tuning of multilingual dense retrieval models and evaluate them robustly on three retrieval benchmarks: XOR-Retrieve (cross-lingual), XTREME-UP (cross-lingual) and MIRACL (monolingual). Our models, called SWIM-X, are competitive with human-supervised dense retrieval models, e.g., mContriever, finding that SWIM-IR can cheaply substitute for expensive human-labeled retrieval training data.
</details>
<details>
<summary>摘要</summary>
traditional retrieval models have been mainly studied for English, where models have shown great success, due to the availability of human-labeled training pairs. However, there has been limited success for multilingual retrieval so far, as training data is uneven or scarcely available across multiple languages. Synthetic training data generation is promising (e.g., InPars or Promptagator), but has been investigated only for English. Therefore, to study model capabilities across both cross-lingual and monolingual retrieval tasks, we develop SWIM-IR, a synthetic retrieval training dataset containing 33 (high to very-low resource) languages for training multilingual dense retrieval models without requiring any human supervision. To construct SWIM-IR, we propose SAP (summarize-then-ask prompting), where the large language model (LLM) generates a textual summary prior to the query generation step. SAP assists the LLM in generating informative queries in the target language. Using SWIM-IR, we explore synthetic fine-tuning of multilingual dense retrieval models and evaluate them robustly on three retrieval benchmarks: XOR-Retrieve (cross-lingual), XTREME-UP (cross-lingual) and MIRACL (monolingual). Our models, called SWIM-X, are competitive with human-supervised dense retrieval models, e.g., mContriever, finding that SWIM-IR can cheaply substitute for expensive human-labeled retrieval training data.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/10/cs.AI_2023_11_10/" data-id="clpahu6z8006v3h88cpmj2xn0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/10/cs.CL_2023_11_10/" class="article-date">
  <time datetime="2023-11-10T11:00:00.000Z" itemprop="datePublished">2023-11-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/10/cs.CL_2023_11_10/">cs.CL - 2023-11-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ChatGPT-Prompting-Cannot-Estimate-Predictive-Uncertainty-in-High-Resource-Languages"><a href="#ChatGPT-Prompting-Cannot-Estimate-Predictive-Uncertainty-in-High-Resource-Languages" class="headerlink" title="ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages"></a>ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06427">http://arxiv.org/abs/2311.06427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martino Pelucchi, Matias Valdenegro-Toro</li>
<li>for: 这个论文的目的是研究 chatGPT 在高资源语言中的表现和其回归精度的可靠性。</li>
<li>methods: 这个论文使用了 five 种高资源语言和两个 NLP 任务来研究 chatGPT 的表现和自信度准确性。</li>
<li>results: 结果表明所选高资源语言都表现相似，chatGPT 的自信度准确性不良， часто过于自信而从未给出低自信值。<details>
<summary>Abstract</summary>
ChatGPT took the world by storm for its impressive abilities. Due to its release without documentation, scientists immediately attempted to identify its limits, mainly through its performance in natural language processing (NLP) tasks. This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level. The analysis of high-resource languages is of interest as studies have shown that low-resource languages perform worse than English in NLP tasks, but no study so far has analysed whether high-resource languages perform as well as English. The analysis of ChatGPT's confidence calibration has not been carried out before either and is critical to learn about ChatGPT's trustworthiness. In order to study these two aspects, five high-resource languages and two NLP tasks were chosen. ChatGPT was asked to perform both tasks in the five languages and to give a numerical confidence value for each answer. The results show that all the selected high-resource languages perform similarly and that ChatGPT does not have a good confidence calibration, often being overconfident and never giving low confidence values.
</details>
<details>
<summary>摘要</summary>
chatGPT在全球引起了一阵风波，主要是因为它的各种能力。由于没有相关文档，科学家们很快就开始了对 chatGPT 的研究，主要通过语言处理任务来测试它的能力。这篇论文想要加入关于 chatGPT 的能力的增长 литератур，主要是通过对高资源语言的表现和 chatGPT 给出答案准确性的信息来进行分析。研究高资源语言的 interessant 是， studies 表明，对英语的 NLP 任务表现较差，但没有任何研究表明，高资源语言的表现和英语相同。此外，还没有任何研究对 chatGPT 的信任性进行了分析，这也是这篇论文的一个重要目标。为了实现这两个目标，我们选择了五种高资源语言和两个 NLP 任务，并让 chatGPT 在这些语言中完成这两个任务，并给出每个答案的数字信任值。结果显示，所选高资源语言都表现相似，而 chatGPT 的信任把关不好，经常过于自信和从来不给低信任值。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graphs-are-not-Created-Equal-Exploring-the-Properties-and-Structure-of-Real-KGs"><a href="#Knowledge-Graphs-are-not-Created-Equal-Exploring-the-Properties-and-Structure-of-Real-KGs" class="headerlink" title="Knowledge Graphs are not Created Equal: Exploring the Properties and Structure of Real KGs"></a>Knowledge Graphs are not Created Equal: Exploring the Properties and Structure of Real KGs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06414">http://arxiv.org/abs/2311.06414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nedelina Teneva, Estevam Hruschka</li>
<li>for: 这种研究的目的是为了研究真实存在的知识图（KG）的结构和特性，以便更好地开发和评估基于KG的模型。</li>
<li>methods: 该研究使用了29个不同领域的真实KG数据集进行大规模比较分析，以挖掘KG的属性和结构模式。</li>
<li>results: 研究发现了许多KG的结构和属性特征，并提出了在KG基于模型开发和评估方面的一些建议。<details>
<summary>Abstract</summary>
Despite the recent popularity of knowledge graph (KG) related tasks and benchmarks such as KG embeddings, link prediction, entity alignment and evaluation of the reasoning abilities of pretrained language models as KGs, the structure and properties of real KGs are not well studied. In this paper, we perform a large scale comparative study of 29 real KG datasets from diverse domains such as the natural sciences, medicine, and NLP to analyze their properties and structural patterns. Based on our findings, we make several recommendations regarding KG-based model development and evaluation. We believe that the rich structural information contained in KGs can benefit the development of better KG models across fields and we hope this study will contribute to breaking the existing data silos between different areas of research (e.g., ML, NLP, AI for sciences).
</details>
<details>
<summary>摘要</summary>
尽管知识图（KG）相关任务和benchmark在最近几年得到了广泛关注，如KG嵌入、链接预测、实体对Alignment和语言模型的逻辑能力评估等，然而实际的知识图结构和特性尚未得到充分研究。在这篇论文中，我们对29个不同领域的真实知识图进行了大规模比较研究，以分析它们的性质和结构性特征。根据我们的发现，我们提出了一些关于基于知识图的模型开发和评估的建议。我们认为知识图中的丰富结构信息可以帮助开发更好的知识图模型，并且希望这篇研究能够突破现有的数据困境（如机器学习、自然语言处理、人工智能等领域之间的数据困境）。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Modular-Approaches-for-Visual-Question-Decomposition"><a href="#Analyzing-Modular-Approaches-for-Visual-Question-Decomposition" class="headerlink" title="Analyzing Modular Approaches for Visual Question Decomposition"></a>Analyzing Modular Approaches for Visual Question Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06411">http://arxiv.org/abs/2311.06411</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brown-palm/visual-question-decomposition">https://github.com/brown-palm/visual-question-decomposition</a></li>
<li>paper_authors: Apoorv Khandelwal, Ellie Pavlick, Chen Sun</li>
<li>for: 这个论文主要研究了ViperGPT模型，它是一种基于LLM的模块化神经网络，能够在视觉语言任务上达到高水平表现。</li>
<li>methods: 这个论文使用了一种控制的研究方法，比较了端到端、模块化和提问基于的方法在多个VQA bencmark上的表现。</li>
<li>results: 研究发现，ViperGPT的加成表现主要来自于选择任务特定模块，而不是BLIP-2模型。此外，ViperGPT可以保持大部分表现，只有 modifying 模块选择策略。此外，模块化方法在一些benchmark上比提问方法表现更好，因为它可以使用自然语言来表示子任务。<details>
<summary>Abstract</summary>
Modular neural networks without additional training have recently been shown to surpass end-to-end neural networks on challenging vision-language tasks. The latest such methods simultaneously introduce LLM-based code generation to build programs and a number of skill-specific, task-oriented modules to execute them. In this paper, we focus on ViperGPT and ask where its additional performance comes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model it subsumes vs. additional symbolic components. To do so, we conduct a controlled study (comparing end-to-end, modular, and prompting-based methods across several VQA benchmarks). We find that ViperGPT's reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when we run ViperGPT using a more task-agnostic selection of modules, these gains go away. Additionally, ViperGPT retains much of its performance if we make prominent alterations to its selection of modules: e.g. removing or retaining only BLIP-2. Finally, we compare ViperGPT against a prompting-based decomposition strategy and find that, on some benchmarks, modular approaches significantly benefit by representing subtasks with natural language, instead of code.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)模块化神经网络无需额外训练最近已经能够超越端到端神经网络在复杂的视觉语言任务上。最新的这些方法同时引入了基于LLM的代码生成以建立程序，以及一些任务特定、任务oriented的模块来执行它们。在这篇论文中，我们关注ViperGPT，并问它的额外性能来源于它的选择的任务特定模块以及BLIP-2模型是否具有主导作用。为了回答这个问题，我们进行了一项控制性研究， comparing end-to-end、模块化和提问基本方法在多个VQAbenchmark上。我们发现，ViperGPT的报告性能增加与BLIP-2模型的选择有直接关系，并且当我们使用一种更任务agnostic的模块选择策略时，这些增加消失。此外，我们发现ViperGPT在做出显著变化到其模块选择时仍然保持较高的性能，例如删除或保留仅BLIP-2模型。最后，我们与提问基本方法进行比较，并发现在某些benchmark上，模块化方法在表示子任务的自然语言方面具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Language-Models-For-Estimating-the-Entropy-of-Epic-EHR-Audit-Logs"><a href="#Autoregressive-Language-Models-For-Estimating-the-Entropy-of-Epic-EHR-Audit-Logs" class="headerlink" title="Autoregressive Language Models For Estimating the Entropy of Epic EHR Audit Logs"></a>Autoregressive Language Models For Estimating the Entropy of Epic EHR Audit Logs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06401">http://arxiv.org/abs/2311.06401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin C. Warner, Thomas Kannampallil, Seunghwan Kim</li>
<li>for: 这项研究旨在Characterizing clinician workflow on the electronic health record (EHR) through EHR audit logs.</li>
<li>methods: 该研究使用 transformer-based tabular language model (tabular LM) 来度量工作流程中动作序列的 entropy 或混乱程度.</li>
<li>results: 研究发现 tabular LM 可以准确度量工作流程中动作序列的复杂性，并且可以公开发布评估模型 дляFuture research.<details>
<summary>Abstract</summary>
EHR audit logs are a highly granular stream of events that capture clinician activities, and is a significant area of interest for research in characterizing clinician workflow on the electronic health record (EHR). Existing techniques to measure the complexity of workflow through EHR audit logs (audit logs) involve time- or frequency-based cross-sectional aggregations that are unable to capture the full complexity of a EHR session. We briefly evaluate the usage of transformer-based tabular language model (tabular LM) in measuring the entropy or disorderedness of action sequences within workflow and release the evaluated models publicly.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Distilling-Large-Language-Models-using-Skill-Occupation-Graph-Context-for-HR-Related-Tasks"><a href="#Distilling-Large-Language-Models-using-Skill-Occupation-Graph-Context-for-HR-Related-Tasks" class="headerlink" title="Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks"></a>Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06383">http://arxiv.org/abs/2311.06383</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/megagonlabs/rjdb">https://github.com/megagonlabs/rjdb</a></li>
<li>paper_authors: Pouya Pezeshkpour, Hayate Iso, Thom Lake, Nikita Bhutani, Estevam Hruschka</li>
<li>for: This paper aims to bridge the gap in HR applications by introducing a benchmark for various HR tasks, including matching and explaining resumes to job descriptions, extracting skills and experiences from resumes, and editing resumes.</li>
<li>methods: The benchmark is created by distilling domain-specific knowledge from a large language model (LLM) and relying on a curated skill-occupation graph to ensure diversity and provide context for LLMs generation.</li>
<li>results: The student models achieve near&#x2F;better performance than the teacher model (GPT-4) in various HR tasks, and the benchmark is effective in out-of-distribution data for skill extraction and resume-job description matching in zero-shot and weak supervision manner.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文的目的是填补人力应用中的空白，通过提出各种人力任务的benchmark，包括匹配和解释简历与职业描述、从简历提取技能和经验、修改简历等。</li>
<li>methods: 该benchmark通过借鉴域域专业知识（LLM），并且利用定制的技能岗位图进行维度和上下文提供，以生成benchmark。</li>
<li>results: 学生模型在不同的人力任务中具有near&#x2F;更好的性能，而且benchmark在对数据集进行零shot和弱监督下的应用中也表现出了效果。<details>
<summary>Abstract</summary>
Numerous HR applications are centered around resumes and job descriptions. While they can benefit from advancements in NLP, particularly large language models, their real-world adoption faces challenges due to absence of comprehensive benchmarks for various HR tasks, and lack of smaller models with competitive capabilities. In this paper, we aim to bridge this gap by introducing the Resume-Job Description Benchmark (RJDB). We meticulously craft this benchmark to cater to a wide array of HR tasks, including matching and explaining resumes to job descriptions, extracting skills and experiences from resumes, and editing resumes. To create this benchmark, we propose to distill domain-specific knowledge from a large language model (LLM). We rely on a curated skill-occupation graph to ensure diversity and provide context for LLMs generation. Our benchmark includes over 50 thousand triples of job descriptions, matched resumes and unmatched resumes. Using RJDB, we train multiple smaller student models. Our experiments reveal that the student models achieve near/better performance than the teacher model (GPT-4), affirming the effectiveness of the benchmark. Additionally, we explore the utility of RJDB on out-of-distribution data for skill extraction and resume-job description matching, in zero-shot and weak supervision manner. We release our datasets and code to foster further research and industry applications.
</details>
<details>
<summary>摘要</summary>
许多人力资源（HR）应用程序都集中在简历和职业描述上。虽然这些应用程序可以从大语言模型（LLM）中受益，但它们在实际应用中遇到了各种挑战，主要是缺乏各种HR任务的全面指标，以及小型模型的竞争力不足。在这篇论文中，我们想要填补这个差距，我们提出了简历职业描述指标（RJDB）。我们尽可能地为各种HR任务，包括简历与职业描述匹配和解释、从简历中提取技能和经验、编辑简历等，创建了这个指标。我们利用一个精心挑选的技能岗位图来保证多样性和提供 контекст для LLMS的生成。我们的指标包括5万多个职业描述、匹配简历和未匹配简历的 triple。我们使用RJDB训练多个小型学生模型，我们的实验表明，这些学生模型可以与教师模型（GPT-4）的性能相似或更好，这证明了指标的有效性。此外，我们还研究了RJDB在零shot和弱监督下对技能提取和简历职业描述匹配的 utility。我们发布了我们的数据和代码，以便进一步的研究和实际应用。
</details></li>
</ul>
<hr>
<h2 id="Transfer-Learning-for-Structured-Pruning-under-Limited-Task-Data"><a href="#Transfer-Learning-for-Structured-Pruning-under-Limited-Task-Data" class="headerlink" title="Transfer Learning for Structured Pruning under Limited Task Data"></a>Transfer Learning for Structured Pruning under Limited Task Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06382">http://arxiv.org/abs/2311.06382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucio Dery, David Grangier, Awni Hannun</li>
<li>for: 这篇论文的目的是提出一个架构，用于实现具有限制资源的应用中使用大型预训模型。</li>
<li>methods: 这篇论文使用了任务意识的结构剪枝方法，将模型大小降低到最小化，并且考虑到目标任务。但是，这些剪枝算法需要更多的任务特定数据。这篇论文提出了一个框架，将结构剪枝与转移学习结合，以减少需要任务特定数据的需求。</li>
<li>results: 这篇论文的实验结果表明，使用这个框架可以实现剪枝后的模型具有更好的普遍化性，比对照强大的基eline。<details>
<summary>Abstract</summary>
Large, pre-trained models are problematic to use in resource constrained applications. Fortunately, task-aware structured pruning methods offer a solution. These approaches reduce model size by dropping structural units like layers and attention heads in a manner that takes into account the end-task. However, these pruning algorithms require more task-specific data than is typically available. We propose a framework which combines structured pruning with transfer learning to reduce the need for task-specific data. Our empirical results answer questions such as: How should the two tasks be coupled? What parameters should be transferred? And, when during training should transfer learning be introduced? Leveraging these insights, we demonstrate that our framework results in pruned models with improved generalization over strong baselines.
</details>
<details>
<summary>摘要</summary>
大型预训练模型在资源受限的应用中存在问题。幸运的是，任务意识 Structured pruning 方法提供了解决方案。这些方法通过去掉结构单元如层和注意头来减小模型大小，并且根据结束任务进行考虑。然而，这些剪枝算法需要更多的任务特定数据 than usual。我们提议一个框架，该结合 Structured pruning 和传输学习来减少需要任务特定数据的需求。我们的实验结果回答了以下问题：何时在训练过程中引入传输学习？何时将两个任务耦合？何时传输哪些参数？通过这些意见，我们示出了我们的框架可以在强大基eline上提供更好的泛化性。
</details></li>
</ul>
<hr>
<h2 id="DeMuX-Data-efficient-Multilingual-Learning"><a href="#DeMuX-Data-efficient-Multilingual-Learning" class="headerlink" title="DeMuX: Data-efficient Multilingual Learning"></a>DeMuX: Data-efficient Multilingual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06379">http://arxiv.org/abs/2311.06379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simran-khanuja/demux">https://github.com/simran-khanuja/demux</a></li>
<li>paper_authors: Simran Khanuja, Srinivas Gowriraj, Lucio Dery, Graham Neubig</li>
<li>for: 本研究旨在优化预训练多语言模型，使用小量目标语言数据和标注预算。</li>
<li>methods: 我们介绍了DEMUX框架，可以从庞大量未标注多语言数据中选择最有价值的数据点进行标注。与之前的大多数工作不同，我们的终端框架是语言无关的，考虑了模型表示，并支持多语言目标配置。我们的活动学策略基于距离和不确定度度量选择任务特定的邻居，以便在模型上进行标注。</li>
<li>results: DEMUX在84%的测试 caso中超越了强基eline，在零shot设定中（包括多语言目标池）的三种模型和四个任务上。尤其在低预算设定（5-100示例）下，我们观察到了8-11个F1点的提升 дляtoken级任务，以及2-5个F1点的提升 для复杂任务。我们的代码可以在以下链接中下载：<a target="_blank" rel="noopener" href="https://github.com/simran-khanuja/demux%E3%80%82">https://github.com/simran-khanuja/demux。</a><details>
<summary>Abstract</summary>
We consider the task of optimally fine-tuning pre-trained multilingual models, given small amounts of unlabelled target data and an annotation budget. In this paper, we introduce DEMUX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for complex tasks. Our code is released here: https://github.com/simran-khanuja/demux.
</details>
<details>
<summary>摘要</summary>
我们考虑在小量目标数据和注释预算下优化预训练多语言模型的任务。在这篇论文中，我们介绍了DEMUX框架，它可以从大量的不标记多语言数据中选择特定的数据点进行标注，这些数据点可能与目标集之间存在未知的重叠度。与大多数前一代工作不同，我们的终端框架是语言无关的，考虑了模型表示，并支持多语言目标配置。我们的活动学策略基于距离和不确定度度量来选择任务特定的邻居，以便在模型上进行标注。DEMuX在3个模型和4个任务中的0号设定下（包括多语言目标池）上比强基eline表现出色，在5-100个示例的低预算设定下，我们观察到了8-11个F1分的提升 дляToken级任务，以及2-5个F1分的提升 для复杂任务。我们的代码可以在以下链接中找到：https://github.com/simran-khanuja/demux。
</details></li>
</ul>
<hr>
<h2 id="Heaps’-Law-in-GPT-Neo-Large-Language-Model-Emulated-Corpora"><a href="#Heaps’-Law-in-GPT-Neo-Large-Language-Model-Emulated-Corpora" class="headerlink" title="Heaps’ Law in GPT-Neo Large Language Model Emulated Corpora"></a>Heaps’ Law in GPT-Neo Large Language Model Emulated Corpora</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06377">http://arxiv.org/abs/2311.06377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uyen Lai, Gurjit S. Randhawa, Paul Sheridan</li>
<li>for: 本研究探讨了大语言模型生成文本中Heaps法律的可靠性，并使用GPT-Neo模型来模拟文献摘要 corpora。</li>
<li>methods: 研究使用GPT-Neo模型的不同参数大小来生成文献摘要，并使用初始五个单词作为提示，让模型根据原始摘要的长度扩展内容。</li>
<li>results: 研究发现，生成的文献摘要遵循Heaps法律，而随着GPT-Neo模型的参数大小增加，生成的词汇更加遵循Heaps法律，与人类编写的文本类似。<details>
<summary>Abstract</summary>
Heaps' law is an empirical relation in text analysis that predicts vocabulary growth as a function of corpus size. While this law has been validated in diverse human-authored text corpora, its applicability to large language model generated text remains unexplored. This study addresses this gap, focusing on the emulation of corpora using the suite of GPT-Neo large language models. To conduct our investigation, we emulated corpora of PubMed abstracts using three different parameter sizes of the GPT-Neo model. Our emulation strategy involved using the initial five words of each PubMed abstract as a prompt and instructing the model to expand the content up to the original abstract's length. Our findings indicate that the generated corpora adhere to Heaps' law. Interestingly, as the GPT-Neo model size grows, its generated vocabulary increasingly adheres to Heaps' law as as observed in human-authored text. To further improve the richness and authenticity of GPT-Neo outputs, future iterations could emphasize enhancing model size or refining the model architecture to curtail vocabulary repetition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Relation-Extraction-in-underexplored-biomedical-domains-A-diversity-optimised-sampling-and-synthetic-data-generation-approach"><a href="#Relation-Extraction-in-underexplored-biomedical-domains-A-diversity-optimised-sampling-and-synthetic-data-generation-approach" class="headerlink" title="Relation Extraction in underexplored biomedical domains: A diversity-optimised sampling and synthetic data generation approach"></a>Relation Extraction in underexplored biomedical domains: A diversity-optimised sampling and synthetic data generation approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06364">http://arxiv.org/abs/2311.06364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idiap/abroad-re">https://github.com/idiap/abroad-re</a></li>
<li>paper_authors: Maxime Delmas, Magdalena Wysocka, André Freitas<br>for:  This paper aims to address the issue of limited labeled data in relation extraction tasks, specifically in the context of natural products literature.methods:  The authors developed a new sampler inspired by diversity metrics in ecology, called the Greedy Maximum Entropy sampler (GME-sampler), to curate a evaluation dataset for training relation extraction models. They also explored few-shot learning with open large language models (LLaMA 7B-65B) and synthetic data generation using Vicuna-13B.results:  The authors achieved substantial improvements in relation extraction performance when fine-tuning models on synthetic abstracts rather than the noisy original data. Their best-performing model, BioGPT-Large, achieved an f1-score of 59.0. They also provide the generated synthetic data and the evaluation dataset for future use.<details>
<summary>Abstract</summary>
The sparsity of labelled data is an obstacle to the development of Relation Extraction models and the completion of databases in various biomedical areas. While being of high interest in drug-discovery, the natural-products literature, reporting the identification of potential bioactive compounds from organisms, is a concrete example of such an overlooked topic. To mark the start of this new task, we created the first curated evaluation dataset and extracted literature items from the LOTUS database to build training sets. To this end, we developed a new sampler inspired by diversity metrics in ecology, named Greedy Maximum Entropy sampler, or GME-sampler (https://github.com/idiap/gme-sampler). The strategic optimization of both balance and diversity of the selected items in the evaluation set is important given the resource-intensive nature of manual curation. After quantifying the noise in the training set, in the form of discrepancies between the input abstracts text and the expected output labels, we explored different strategies accordingly. Framing the task as an end-to-end Relation Extraction, we evaluated the performance of standard fine-tuning as a generative task and few-shot learning with open Large Language Models (LLaMA 7B-65B). In addition to their evaluation in few-shot settings, we explore the potential of open Large Language Models (Vicuna-13B) as synthetic data generator and propose a new workflow for this purpose. All evaluated models exhibited substantial improvements when fine-tuned on synthetic abstracts rather than the original noisy data. We provide our best performing (f1-score=59.0) BioGPT-Large model for end-to-end RE of natural-products relationships along with all the generated synthetic data and the evaluation dataset. See more details at https://github.com/idiap/abroad-re.
</details>
<details>
<summary>摘要</summary>
“资料稀缺是生物医学领域中relation抽取模型的发展所面临的障碍。然而，自然产物文献中的潜在生物活性物质发现是一个受到过见的领域。为了启动这个新任务，我们创建了首个维护评估集和从LOTUS数据库中提取出来的文献项目，以建立训练集。为此，我们开发了一个灵活的最大熵采样器（GME-sampler），并在评估集中实现了权衡和多样性的选择。由于训练集的资源投入巨大，我们需要运用数据的混沌来评估模型的性能。我们将这个任务定义为一个端到端的relation抽取任务，并评估了标准的精致化和几何学模型的几何学学习。我们发现所有评估的模型在精致化的设定下表现出色，并且在使用生成器来生成实验数据时，具有更好的性能。我们提供了我们的最高表现（f1-score=59.0）的BioGPT-Large模型，以及所有生成的实验数据和评估集。详细信息请参考https://github.com/idiap/abroad-re。”
</details></li>
</ul>
<hr>
<h2 id="Word-Definitions-from-Large-Language-Models"><a href="#Word-Definitions-from-Large-Language-Models" class="headerlink" title="Word Definitions from Large Language Models"></a>Word Definitions from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06362">http://arxiv.org/abs/2311.06362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Yunting Yin, Steven Skiena</li>
<li>for: 本研究旨在探讨传统词典和现代自然语言处理技术（如ChatGPT）之间词义定义的一致度。</li>
<li>methods: 本研究使用了三种已 publik 的词典和 variants of ChatGPT 生成的词义定义进行比较。</li>
<li>results: 研究发现（i）不同的传统词典中的词义定义具有更高的表面形式相似性，而模型生成的定义则具有高度准确性，与传统词典相当；（ii）ChatGPT 定义具有高度准确性，可以在低频词术中保持准确性，而 GloVE 和 FastText 词 embedding 则不太准确。<details>
<summary>Abstract</summary>
Dictionary definitions are historically the arbitrator of what words mean, but this primacy has come under threat by recent progress in NLP, including word embeddings and generative models like ChatGPT. We present an exploratory study of the degree of alignment between word definitions from classical dictionaries and these newer computational artifacts. Specifically, we compare definitions from three published dictionaries to those generated from variants of ChatGPT. We show that (i) definitions from different traditional dictionaries exhibit more surface form similarity than do model-generated definitions, (ii) that the ChatGPT definitions are highly accurate, comparable to traditional dictionaries, and (iii) ChatGPT-based embedding definitions retain their accuracy even on low frequency words, much better than GloVE and FastText word embeddings.
</details>
<details>
<summary>摘要</summary>
传统的词典定义曾经是词语意义的决定性标准，但这种主导地位在计算机自然语言处理（NLP）的进步下来到了威胁。我们进行了一项探索性的研究，检查了古典词典定义和计算机生成的词语定义之间的吻合度。我们比较了三本出版的词典定义和 variants of ChatGPT 生成的定义，发现：1. 不同的传统词典定义在表面形式上更加相似，而模型生成的定义相对来说更加不同。2. ChatGPT 生成的定义准确率高，与传统词典定义相当，甚至在低频词语上也具有较高的准确率。3. ChatGPT 基于的词语定义 embedding 在低频词语上保持了准确性，而 GloVE 和 FastText 词语 embedding 则不如 ChatGPT。
</details></li>
</ul>
<hr>
<h2 id="Schema-Graph-Guided-Prompt-for-Multi-Domain-Dialogue-State-Tracking"><a href="#Schema-Graph-Guided-Prompt-for-Multi-Domain-Dialogue-State-Tracking" class="headerlink" title="Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking"></a>Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06345">http://arxiv.org/abs/2311.06345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruolin Su, Ting-Wei Wu, Biing-Hwang Juang</li>
<li>for: 增强 task-oriented 对话系统中的对话状态跟踪功能，特别是在具有特定领域特点的情况下。</li>
<li>methods: 我们提出了基于图 структуры的框架，通过将域专门的对话架构编码为图神经网络来嵌入先前训练的自然语言模型中，以便利用对话架构中的关系导向模型更好地适应特定域。</li>
<li>results: 我们的实验表明，我们的图基于方法在多域对话状态跟踪中表现更好，使用相同或少于其他多域 DST 方法的训练参数。我们还进行了广泛的对schema graph体系、参数使用和模块剥离的研究，以证明我们的模型在多域对话状态跟踪中的效果。<details>
<summary>Abstract</summary>
Tracking dialogue states is an essential topic in task-oriented dialogue systems, which involve filling in the necessary information in pre-defined slots corresponding to a schema. While general pre-trained language models have been shown effective in slot-filling, their performance is limited when applied to specific domains. We propose a graph-based framework that learns domain-specific prompts by incorporating the dialogue schema. Specifically, we embed domain-specific schema encoded by a graph neural network into the pre-trained language model, which allows for relations in the schema to guide the model for better adaptation to the specific domain. Our experiments demonstrate that the proposed graph-based method outperforms other multi-domain DST approaches while using similar or fewer trainable parameters. We also conduct a comprehensive study of schema graph architectures, parameter usage, and module ablation that demonstrate the effectiveness of our model on multi-domain dialogue state tracking.
</details>
<details>
<summary>摘要</summary>
“Dialogue state tracking（DST）在任务对话系统中是一个重要的主题，它需要填充预定的构造中的必要信息。而通用的预训语言模型在特定领域中表现不佳，因此我们提出了一个基于图形框架的方法，通过将领域特定的schema编码为图形神经网络，将领域特定的关系引导模型更好地适应特定领域。我们的实验结果显示，我们的图形基于方法在多域DST方法中表现出色，并且使用相似或少数可训练的参数。我们还进行了多种schema图架架构、参数使用和模块扩展的完整研究，实验结果证明了我们的模型在多域对话state Tracking中的效果。”Note that Simplified Chinese is the official writing system used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Argumentation-Element-Annotation-Modeling-using-XLNet"><a href="#Argumentation-Element-Annotation-Modeling-using-XLNet" class="headerlink" title="Argumentation Element Annotation Modeling using XLNet"></a>Argumentation Element Annotation Modeling using XLNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06239">http://arxiv.org/abs/2311.06239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Ormerod, Amy Burkhardt, Mackenzie Young, Sue Lottridge</li>
<li>for: This paper demonstrates the effectiveness of XLNet for annotating argumentative elements in persuasive essays, providing automated feedback on essay organization.</li>
<li>methods: The paper uses XLNet, a transformer-based language model, with a recurrent mechanism to model long-term dependencies in lengthy texts. The model is fine-tuned on three datasets annotated with different schemes.</li>
<li>results: The XLNet models achieved strong performance across all datasets, even surpassing human agreement levels in some cases. The paper highlights the suitability of XLNet for providing automated feedback on essay organization, and provides insights into the relationships between the annotation tags.<details>
<summary>Abstract</summary>
This study demonstrates the effectiveness of XLNet, a transformer-based language model, for annotating argumentative elements in persuasive essays. XLNet's architecture incorporates a recurrent mechanism that allows it to model long-term dependencies in lengthy texts. Fine-tuned XLNet models were applied to three datasets annotated with different schemes - a proprietary dataset using the Annotations for Revisions and Reflections on Writing (ARROW) scheme, the PERSUADE corpus, and the Argument Annotated Essays (AAE) dataset. The XLNet models achieved strong performance across all datasets, even surpassing human agreement levels in some cases. This shows XLNet capably handles diverse annotation schemes and lengthy essays. Comparisons between the model outputs on different datasets also revealed insights into the relationships between the annotation tags. Overall, XLNet's strong performance on modeling argumentative structures across diverse datasets highlights its suitability for providing automated feedback on essay organization.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "ARROW" 改为 "箭" (jian) (proprietary dataset using the Annotations for Revisions and Reflections on Writing scheme)* "PERSUADE" 改为 "说服" (shuocheng) (PERSUADE corpus)* "AAE" 改为 "Argument Annotated Essays" 改为 "论点标注作文" (lun dian biao zhun) (Argument Annotated Essays dataset)* "essays" 改为 "作文" (zuxing) (to match the Simplified Chinese word order)
</details></li>
</ul>
<hr>
<h2 id="Summon-a-Demon-and-Bind-it-A-Grounded-Theory-of-LLM-Red-Teaming-in-the-Wild"><a href="#Summon-a-Demon-and-Bind-it-A-Grounded-Theory-of-LLM-Red-Teaming-in-the-Wild" class="headerlink" title="Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild"></a>Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06237">http://arxiv.org/abs/2311.06237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nanna Inie, Jonathan Stray, Leon Derczynski</li>
<li>for: 这篇论文旨在探讨人们如何通过攻击大语言模型（LLM）来生成异常输出的新型活动。</li>
<li>methods: 这篇论文使用正式的资深访谈方法，对具有多个背景的参与者进行了多达数十人的访谈，以了解他们在尝试使LML崩溃时采取的策略和技术。</li>
<li>results: 这篇论文提出了一种基于实践的论点，即LLM攻击的活动是一种社区协同的行为，其中参与者的动机和目标、使用的策略和技术以及社区的作用都具有重要作用。<details>
<summary>Abstract</summary>
Engaging in the deliberate generation of abnormal outputs from large language models (LLMs) by attacking them is a novel human activity. This paper presents a thorough exposition of how and why people perform such attacks. Using a formal qualitative methodology, we interviewed dozens of practitioners from a broad range of backgrounds, all contributors to this novel work of attempting to cause LLMs to fail. We relate and connect this activity between its practitioners' motivations and goals; the strategies and techniques they deploy; and the crucial role the community plays. As a result, this paper presents a grounded theory of how and why people attack large language models: LLM red teaming in the wild.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的故意生成异常输出的攻击是一项新的人类活动。这篇论文通过正式的形式化质量方法，介绍了这种攻击的如何和为何。我们对来自多个背景的参与者进行了多达数十人的采访，这些参与者都是这项尝试引起LLM失败的工作的贡献者。我们将这些参与者的动机和目标与战略和技巧相连接，并证明了这种活动的核心是LLM红团队在野外。因此，这篇论文提供了一个固定的LLM攻击理论：LLM红团队在野外。
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Lexicon-Based-and-ML-Based-Sentiment-Analysis-Are-There-Outlier-Words"><a href="#A-Comparison-of-Lexicon-Based-and-ML-Based-Sentiment-Analysis-Are-There-Outlier-Words" class="headerlink" title="A Comparison of Lexicon-Based and ML-Based Sentiment Analysis: Are There Outlier Words?"></a>A Comparison of Lexicon-Based and ML-Based Sentiment Analysis: Are There Outlier Words?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06221">http://arxiv.org/abs/2311.06221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Jaydeep Mahajani, Shashank Srivastava, Alan F. Smeaton</li>
<li>for: 这个论文是为了比较 lexicon-based 和机器学习 两种方法在文本情感分析中的表现。</li>
<li>methods: 这个论文使用了 Hedometer 和 Azure 两种方法来计算文本情感分析结果。 Hedometer 是一种基于词语库的方法，而 Azure 是一种现代机器学习方法。</li>
<li>results: 研究发现，各个领域的文本情感分析结果存在差异，且不存在特定的词语库项导致差异的现象。<details>
<summary>Abstract</summary>
Lexicon-based approaches to sentiment analysis of text are based on each word or lexical entry having a pre-defined weight indicating its sentiment polarity. These are usually manually assigned but the accuracy of these when compared against machine leaning based approaches to computing sentiment, are not known. It may be that there are lexical entries whose sentiment values cause a lexicon-based approach to give results which are very different to a machine learning approach. In this paper we compute sentiment for more than 150,000 English language texts drawn from 4 domains using the Hedonometer, a lexicon-based technique and Azure, a contemporary machine-learning based approach which is part of the Azure Cognitive Services family of APIs which is easy to use. We model differences in sentiment scores between approaches for documents in each domain using a regression and analyse the independent variables (Hedonometer lexical entries) as indicators of each word's importance and contribution to the score differences. Our findings are that the importance of a word depends on the domain and there are no standout lexical entries which systematically cause differences in sentiment scores.
</details>
<details>
<summary>摘要</summary>
Lexicon-based方法 для情感分析文本基于每个词或语言Entry有前定的欢度指数，这些通常是手动指定的，但与机器学习基于方法的计算情感结果相比，它们的准确性不明确。可能存在 lexical Entry  whose sentiment values cause a lexicon-based approach to give results that are very different from a machine learning approach。在这篇论文中，我们计算了超过 150,000 篇英语文本，从 4 个领域中获取，使用 Hedonometer，一种 lexicon-based 技术和 Azure，一种现代机器学习基于 API 的方法，这是 Azure 认知服务家族的一部分，易于使用。我们模型了每个领域的文档的情感分数之间的差异使用回归分析，并将 Hedonometer 词语入力作为每个词的重要性和对情感分数做出贡献的指标进行分析。我们的发现是，在各个领域中，一个词的重要性取决于领域，并没有一个系统性地导致情感分数差异的词语。
</details></li>
</ul>
<hr>
<h2 id="Syntax-semantics-interface-an-algebraic-model"><a href="#Syntax-semantics-interface-an-algebraic-model" class="headerlink" title="Syntax-semantics interface: an algebraic model"></a>Syntax-semantics interface: an algebraic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06189">http://arxiv.org/abs/2311.06189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matilde Marcolli, Robert C. Berwick, Noam Chomsky</li>
<li>for: 这篇论文探讨了一种基于ホップ代数的语音-语意界面的数学模型，以及这种模型如何应用于理论物理中的权重 normalization 过程中提取有意义的物理值。</li>
<li>methods: 该论文使用了一种基于ホップ代数的形式化方法，以描述在语音表达中提取意义的过程。同时，它还与计算 semantics 模型进行比较，以解释这种形式化方法如何应用于大语言模型的功能。</li>
<li>results: 该论文显示了这种基于ホップ代数的模型可以帮助解决一些当前大语言模型的争议，并且可以提供一种新的方法来描述语音表达中的意义提取过程。<details>
<summary>Abstract</summary>
We extend our formulation of Merge and Minimalism in terms of Hopf algebras to an algebraic model of a syntactic-semantic interface. We show that methods adopted in the formulation of renormalization (extraction of meaningful physical values) in theoretical physics are relevant to describe the extraction of meaning from syntactic expressions. We show how this formulation relates to computational models of semantics and we answer some recent controversies about implications for generative linguistics of the current functioning of large language models.
</details>
<details>
<summary>摘要</summary>
我们扩展了我们的 merge 和 minimalism 在霍夫代数中的形式ulation，用于建立语音表示与 semantics 的 интерфейス。我们显示了在理论物理中的 renormalization (提取有意义的物理值) 方法与语音表达中提取意义的方法有相似之处。我们还示出了这种形式ulation 与计算 semantics 模型之间的关系，并回答了一些最近关于生成语言学的争议。
</details></li>
</ul>
<hr>
<h2 id="Is-it-indeed-bigger-better-The-comprehensive-study-of-claim-detection-LMs-applied-for-disinformation-tackling"><a href="#Is-it-indeed-bigger-better-The-comprehensive-study-of-claim-detection-LMs-applied-for-disinformation-tackling" class="headerlink" title="Is it indeed bigger better? The comprehensive study of claim detection LMs applied for disinformation tackling"></a>Is it indeed bigger better? The comprehensive study of claim detection LMs applied for disinformation tackling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06121">http://arxiv.org/abs/2311.06121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Hyben, Sebastian Kula, Ivan Srba, Robert Moro, Jakub Simko</li>
<li>For:  Compares the performance of fine-tuned models and extremely large language models on the task of check-worthy claim detection.* Methods: Uses a multilingual and multi-topical dataset, and benchmark analysis to determine the most general multilingual and multi-topical claim detector.* Results: Despite technological progress in natural language processing, fine-tuned models outperform zero-shot approaches in cross-domain settings.Here’s the full text in Simplified Chinese:* 为： Compares 精制模型和非常大的自然语言处理模型在检查可信laim检测任务上的表现。* 方法： 使用多语言多频道的数据集，并进行了benchmark分析，以确定最通用的多语言多频道laim检测器。* 结果：  despite技术进步，精制模型在跨频道设置下仍然表现更好于零批处理approaches。<details>
<summary>Abstract</summary>
This study compares the performance of (1) fine-tuned models and (2) extremely large language models on the task of check-worthy claim detection. For the purpose of the comparison we composed a multilingual and multi-topical dataset comprising texts of various sources and styles. Building on this, we performed a benchmark analysis to determine the most general multilingual and multi-topical claim detector.   We chose three state-of-the-art models in the check-worthy claim detection task and fine-tuned them. Furthermore, we selected three state-of-the-art extremely large language models without any fine-tuning. We made modifications to the models to adapt them for multilingual settings and through extensive experimentation and evaluation. We assessed the performance of all the models in terms of accuracy, recall, and F1-score in in-domain and cross-domain scenarios. Our results demonstrate that despite the technological progress in the area of natural language processing, the models fine-tuned for the task of check-worthy claim detection still outperform the zero-shot approaches in a cross-domain settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Practical-Membership-Inference-Attacks-against-Fine-tuned-Large-Language-Models-via-Self-prompt-Calibration"><a href="#Practical-Membership-Inference-Attacks-against-Fine-tuned-Large-Language-Models-via-Self-prompt-Calibration" class="headerlink" title="Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration"></a>Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06062">http://arxiv.org/abs/2311.06062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang</li>
<li>for: 本研究旨在评估现有的会员推测攻击（MIA）技术是否能够有效地泄露个人隐私信息，以及是否存在可靠的会员推测方法。</li>
<li>methods: 本研究使用了两种类型的会员推测攻击：引用自由和引用基于的攻击。其中引用基于的攻击更有可靠性，通过比较目标模型和参照模型之间的概率差异来评估会员性。然而，引用基于的攻击需要一个与训练集相似的参照集，这在实际应用中很难实现。</li>
<li>results: 研究发现，现有的会员推测技术对于实际的精度语言模型（LLM）不能有效地泄露个人隐私信息。这是因为现有的会员推测方法假设训练记录会具有高的概率被采样，但是这种假设受到训练集的多重正则化和 LLM 的总体化的影响，导致会员推测的效果减弱。<details>
<summary>Abstract</summary>
Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Prior attempts have quantified the privacy risks of language models (LMs) via MIAs, but there is still no consensus on whether existing MIA algorithms can cause remarkable privacy leakage on practical Large Language Models (LLMs). Existing MIAs designed for LMs can be classified into two categories: reference-free and reference-based attacks. They are both based on the hypothesis that training records consistently strike a higher probability of being sampled. Nevertheless, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. The reference-based attack seems to achieve promising effectiveness in LLMs, which measures a more reliable membership signal by comparing the probability discrepancy between the target model and the reference model. However, the performance of reference-based attack is highly dependent on a reference dataset that closely resembles the training dataset, which is usually inaccessible in the practical scenario. Overall, existing MIAs are unable to effectively unveil privacy leakage over practical fine-tuned LLMs that are overfitting-free and private. We propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, since memorization in LLMs is inevitable during the training process and occurs before overfitting, we introduce a more reliable membership signal, probabilistic variation, which is based on memorization rather than overfitting. Furthermore, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）的会员推测攻击（MIA）目的是确定目标数据记录是否在模型训练中使用过。先前的尝试已经衡量了语言模型的隐私风险通过 MIA，但没有达成一致是否存在现实中大语言模型（LLM）上显著的隐私泄露问题。现有的 MIA 设计为语言模型可以分为两类：无参和参参攻击。它们都基于目标模型训练记录的假设，即训练记录会具有更高的抽样概率。然而，这个假设取决于目标模型的过度适应，这将通过多种正则化方法和大语言模型的通用性来减弱。参参攻击似乎在 LLM 中获得了有效的成果，它通过比较目标模型和参考模型之间的概率差来测量更可靠的会员信号。然而，参参攻击的性能受到参考 dataset 的影响，这个 dataset 通常在实际场景中不可得。总之，现有的 MIA 无法有效地暴露实际精细调整后的 LLM 中的隐私泄露。我们提出一种基于自适应概率变化（SPV）的会员推测攻击方法。具体来说，在语言模型的训练过程中，记忆是不可避免的，而记忆在训练之前就发生了过度适应。我们引入更可靠的会员信号，即概率变化，它基于记忆而不是过度适应。此外，我们引入自我提示方法，它通过让目标 LLM 自己提供参考模型的 dataset 来构建一个类似于公共 API 上的 dataset。这样，敌对方可以收集一个类似于公共 API 上的 dataset，从而实现更好的会员推测。
</details></li>
</ul>
<hr>
<h2 id="Multi-Label-Topic-Model-for-Financial-Textual-Data"><a href="#Multi-Label-Topic-Model-for-Financial-Textual-Data" class="headerlink" title="Multi-Label Topic Model for Financial Textual Data"></a>Multi-Label Topic Model for Financial Textual Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07598">http://arxiv.org/abs/2311.07598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Scherrmann</li>
<li>for: 这篇论文是为了研究金融文本，如ads-hoc公告、8-K文件、金融新闻或年度报告而开发的多个标签主题模型。</li>
<li>methods: 作者使用了一个新的金融多个标签数据库，包含3,044个德国ads-hoc公告，并使用20个经济动机导向的主题进行手动标注。最佳模型达到了超过85%的macro F1分数。</li>
<li>results: 作者发现，在不同主题之间的合并影响了股市反应。例如，公告新的大规模项目或破产申请会产生强烈的正面或负面市场反应，而某些其他主题则不显示出显著的价格影响。此外，相比之前的研究，这种多个标签结构允许分析不同主题之间的相互作用。<details>
<summary>Abstract</summary>
This paper presents a multi-label topic model for financial texts like ad-hoc announcements, 8-K filings, finance related news or annual reports. I train the model on a new financial multi-label database consisting of 3,044 German ad-hoc announcements that are labeled manually using 20 predefined, economically motivated topics. The best model achieves a macro F1 score of more than 85%. Translating the data results in an English version of the model with similar performance. As application of the model, I investigate differences in stock market reactions across topics. I find evidence for strong positive or negative market reactions for some topics, like announcements of new Large Scale Projects or Bankruptcy Filings, while I do not observe significant price effects for some other topics. Furthermore, in contrast to previous studies, the multi-label structure of the model allows to analyze the effects of co-occurring topics on stock market reactions. For many cases, the reaction to a specific topic depends heavily on the co-occurrence with other topics. For example, if allocated capital from a Seasoned Equity Offering (SEO) is used for restructuring a company in the course of a Bankruptcy Proceeding, the market reacts positively on average. However, if that capital is used for covering unexpected, additional costs from the development of new drugs, the SEO implies negative reactions on average.
</details>
<details>
<summary>摘要</summary>
(Note: Please note that the translation is in Simplified Chinese, and the formatting of the text may be different from the original English version.)
</details></li>
</ul>
<hr>
<h2 id="ChiMed-GPT-A-Chinese-Medical-Large-Language-Model-with-Full-Training-Regime-and-Better-Alignment-to-Human-Preferences"><a href="#ChiMed-GPT-A-Chinese-Medical-Large-Language-Model-with-Full-Training-Regime-and-Better-Alignment-to-Human-Preferences" class="headerlink" title="ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences"></a>ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06025">http://arxiv.org/abs/2311.06025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/synlp/chimed-gpt">https://github.com/synlp/chimed-gpt</a></li>
<li>paper_authors: Yuanhe Tian, Ruyi Gan, Yan Song, Jiaxing Zhang, Yongdong Zhang</li>
<li>for: 这篇论文的目的是提出一种适用于医疗领域的新的语言处理模型（ChiMed-GPT），以提高医疗服务质量。</li>
<li>methods: 该模型采用了大量的医疗文本数据进行预训练，并在这些数据上进行了精心的微调和人工强化。</li>
<li>results: 对于实际任务 such as 信息提取、问答和对话生成，ChiMed-GPT的性能都显著高于通用领域的语言模型。此外，通过对模型进行某些词汇和语言模型的改进，提高了模型的可读性和可信度。<details>
<summary>Abstract</summary>
Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising results in this domain and current large language models (LLMs) offer advanced foundation for medical text processing. However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. Another engineering barrier that prevents current medical LLM from better text processing ability is their restricted context length (e.g., 2,048 tokens), making it hard for the LLMs to process long context, which is frequently required in the medical domain. In this work, we propose ChiMed-GPT, a new benchmark LLM designed explicitly for Chinese medical domain, with enlarged context length to 4,096 tokens and undergoes a comprehensive training regime with pre-training, SFT, and RLHF. Evaluations on real-world tasks including information extraction, question answering, and dialogue generation demonstrate ChiMed-GPT's superior performance over general domain LLMs. Furthermore, we analyze possible biases through prompting ChiMed-GPT to perform attitude scales regarding discrimination of patients, so as to contribute to further responsible development of LLMs in the medical domain. The code and model are released at https://github.com/synlp/ChiMed-GPT.
</details>
<details>
<summary>摘要</summary>
最近，医疗服务的需求增长，抛出了医疗基础设施的差异。医疗领域的大数据 Text 作为医疗服务的基础，需要有效的自然语言处理（NLP）解决方案。现有的方法利用预训练模型显示了良好的结果，而当前的大语言模型（LLM）提供了医疗文本处理的高级基础。然而，大多数医疗 LL M 仅通过监督微调（SFT）进行训练，尽管它可以有效地使 LLM 理解和回答医疗指令，但是无法学习域知识和人类偏好。另一个工程障碍是现有的医疗 LL M 的上下文长度 restriction（例如 2,048 个 Token），使得 LLM Difficult to process long context，这经常需要在医疗领域进行。在这种情况下，我们提出了 ChiMed-GPT，一个专门为中文医疗领域设计的新的标准 LL M。 ChiMed-GPT 的上下文长度增加到 4,096 个 Token，并通过预训练、SFT 和 RLHF 进行全面的训练 regime。在实际任务中，包括信息提取、问题回答和对话生成，ChiMed-GPT 的性能超过了通用领域 LL M。此外，我们还分析了 ChiMed-GPT 的可能的偏见，通过让它完成恶势卷反映的任务，以至于降低 LLM 在医疗领域的可能性。代码和模型可以在 https://github.com/synlp/ChiMed-GPT 上下载。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-Zero-Shot-Hypothesis-Proposers"><a href="#Large-Language-Models-are-Zero-Shot-Hypothesis-Proposers" class="headerlink" title="Large Language Models are Zero Shot Hypothesis Proposers"></a>Large Language Models are Zero Shot Hypothesis Proposers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05965">http://arxiv.org/abs/2311.05965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, Bowen Zhou</li>
<li>for:  investigate whether LLMs can propose scientific hypotheses</li>
<li>methods:  construct a dataset of background knowledge and hypothesis pairs from biomedical literature, evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings</li>
<li>results:  LLMs surprisingly generate untrained yet validated hypotheses from testing literature, increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities<details>
<summary>Abstract</summary>
Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.
</details>
<details>
<summary>摘要</summary>
科学发现的进步对人类文明的发展具有重要作用。 however， scientific literature and data explosion 已经创造了知识障碍， slowing down scientific discovery。 Large Language Models (LLMs) possess a wealth of global and interdisciplinary knowledge that can break down these information barriers and foster a new wave of scientific discovery. 然而， LLMS的科学发现潜力还没有得到正式探索。在这篇论文中，我们开始了 LLMS 可以提出科学假设的研究。为此，我们构建了一个基于生物医学文献的假设集和背景知识集，并将其分为训练、seen和未见测试集，以控制可见性。接着，我们评估了不同级别的 instructed 模型在零shot、几shot和精度调整设置下的假设生成能力，包括开源和关闭源 LLMS。此外，我们还提出了基于 LLMS 的多代合作框架，并设计了不同角色的设计和外部工具来提高假设生成能力。最后，我们设计了四种度量来评估生成的假设，包括 ChatGPT 基于和人类评估。通过实验和分析，我们得到以下发现：1. LLMS 奇异地从测试文献中提出未经训练的有效假设。2. 增加不确定性可能提高零shot假设生成能力。这些发现加强了 LLMS 作为新科学发现的潜在推动者的潜力，并且引导进一步探索。
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Thought-with-Explicit-Evidence-Reasoning-for-Few-shot-Relation-Extraction"><a href="#Chain-of-Thought-with-Explicit-Evidence-Reasoning-for-Few-shot-Relation-Extraction" class="headerlink" title="Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction"></a>Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05922">http://arxiv.org/abs/2311.05922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xilai Ma, Jing Li, Min Zhang</li>
<li>for: 这篇论文主要关注于几何shot关系抽取，即使用有限数量的标注样本来找到两个特定实体之间的关系类型。</li>
<li>methods: 这篇论文使用了meta-学习和神经网络技术，并且不需要训练过程来适应。它们通常使用chain-of-thought来建立关系抽取模型。</li>
<li>results: 这篇论文提出了一个名为CoT-ER的新方法，它使用大型自然语言模型来生成证据，然后将这些证据Explicitly incorporated into chain-of-thought来进行关系抽取。实验结果显示，CoT-ER方法在FewRel1.0和FewRel2.0数据集上 achieves competitive performance 与完全监督（100% 训练数据）现有方法相比。<details>
<summary>Abstract</summary>
Few-shot relation extraction involves identifying the type of relationship between two specific entities within a text, using a limited number of annotated samples. A variety of solutions to this problem have emerged by applying meta-learning and neural graph techniques which typically necessitate a training process for adaptation. Recently, the strategy of in-context learning has been demonstrating notable results without the need of training. Few studies have already utilized in-context learning for zero-shot information extraction. Unfortunately, the evidence for inference is either not considered or implicitly modeled during the construction of chain-of-thought prompts. In this paper, we propose a novel approach for few-shot relation extraction using large language models, named CoT-ER, chain-of-thought with explicit evidence reasoning. In particular, CoT-ER first induces large language models to generate evidences using task-specific and concept-level knowledge. Then these evidences are explicitly incorporated into chain-of-thought prompting for relation extraction. Experimental results demonstrate that our CoT-ER approach (with 0% training data) achieves competitive performance compared to the fully-supervised (with 100% training data) state-of-the-art approach on the FewRel1.0 and FewRel2.0 datasets.
</details>
<details>
<summary>摘要</summary>
几个shot关系提取问题涉及到在文本中确定两个特定实体之间的类型关系，使用有限数量的标注样本进行训练。许多解决方案已经在应用元学习和神经图技术，通常需要训练过程进行适应。然而，最近，在文本中学习的策略已经在没有训练的情况下达到了显著的结果。只有一些研究已经使用了零shot信息提取。然而，在构建链条思维提问时，对推理的证据并不被考虑或直接模型。在本文中，我们提出了一种基于大语言模型的新方法，名为CoT-ER，即链条思维withExplicit Evidence Reasoning。特别是，CoT-ER首先使大语言模型生成证据，使用任务特定和概念水平的知识。然后，这些证据被Explicitly incorporated into链条思维提问中。实验结果表明，我们的CoT-ER方法（无需训练数据）可以与完全监督（具有100%训练数据）当前领域的状态之前性能竞争。
</details></li>
</ul>
<hr>
<h2 id="Citation-Recommendation-on-Scholarly-Legal-Articles"><a href="#Citation-Recommendation-on-Scholarly-Legal-Articles" class="headerlink" title="Citation Recommendation on Scholarly Legal Articles"></a>Citation Recommendation on Scholarly Legal Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05902">http://arxiv.org/abs/2311.05902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dgknrsln/legalcitationrecommendation">https://github.com/dgknrsln/legalcitationrecommendation</a></li>
<li>paper_authors: Doğukan Arslan, Saadet Sena Erdoğan, Gülşen Eryiğit</li>
<li>for: 这个论文的目的是提出一个学术法律数据集，以便进行参考文献推荐任务。</li>
<li>methods: 这个论文使用了现有的模型，并进行了实验和比较，以检验这些模型在法律领域的表现。</li>
<li>results: 研究结果表明，使用BM25+和SciNCL进行预选和重新排序可以提高基线性能从0.26到0.30 MAP@10，而 fine-tuning也可以提高预处理模型的表现。<details>
<summary>Abstract</summary>
Citation recommendation is the task of finding appropriate citations based on a given piece of text. The proposed datasets for this task consist mainly of several scientific fields, lacking some core ones, such as law. Furthermore, citation recommendation is used within the legal domain to identify supporting arguments, utilizing non-scholarly legal articles. In order to alleviate the limitations of existing studies, we gather the first scholarly legal dataset for the task of citation recommendation. Also, we conduct experiments with state-of-the-art models and compare their performance on this dataset. The study suggests that, while BM25 is a strong benchmark for the legal citation recommendation task, the most effective method involves implementing a two-step process that entails pre-fetching with BM25+, followed by re-ranking with SciNCL, which enhances the performance of the baseline from 0.26 to 0.30 MAP@10. Moreover, fine-tuning leads to considerable performance increases in pre-trained models, which shows the importance of including legal articles in the training data of these models.
</details>
<details>
<summary>摘要</summary>
<SYS>    <LANGUAGE_TRANSLATION>        <FROM_LANGUAGE>English</FROM_LANGUAGE>        <TO_LANGUAGE>简化字</TO_LANGUAGE>        <TEXT>            Citation recommendation是一项基于给定文本的任务，找到相应的引用。已有的数据集主要包括一些科学领域，缺乏一些核心领域，例如法律。在法律领域中，引用推荐用于identifying supporting arguments，使用非学术法律文章。为了解决现有研究的局限性，我们收集了首个学术法律数据集 для引用推荐任务。此外，我们进行了现有模型的实验和比较，并发现使用BM25+后followed by SciNCL重新排名的两步进程可以提高基准值从0.26到0.30 MAP@10。此外， fine-tuning对预训练模型的性能有显著提高，这说明包含法律文章在模型训练数据中的重要性。        </TEXT>    </LANGUAGE_TRANSLATION></SYS>Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Follow-Up-Differential-Descriptions-Language-Models-Resolve-Ambiguities-for-Image-Classification"><a href="#Follow-Up-Differential-Descriptions-Language-Models-Resolve-Ambiguities-for-Image-Classification" class="headerlink" title="Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification"></a>Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07593">http://arxiv.org/abs/2311.07593</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/batsresearch/fudd">https://github.com/batsresearch/fudd</a></li>
<li>paper_authors: Reza Esfandiarpoor, Stephen H. Bach<br>for:  This paper aims to improve the performance of vision-language models like CLIP for image classification by extending class descriptions with related attributes.methods:  The proposed method, Follow-up Differential Descriptions (FuDD), uses a Large Language Model (LLM) to generate new class descriptions that differentiate between ambiguous classes.results:  FuDD consistently outperforms generic description ensembles and naive LLM-generated descriptions on 12 datasets, and high quality natural language class descriptions produced by FuDD result in comparable performance to few-shot adaptation methods.<details>
<summary>Abstract</summary>
A promising approach for improving the performance of vision-language models like CLIP for image classification is to extend the class descriptions (i.e., prompts) with related attributes, e.g., using brown sparrow instead of sparrow. However, current zero-shot methods select a subset of attributes regardless of commonalities between the target classes, potentially providing no useful information that would have helped to distinguish between them. For instance, they may use color instead of bill shape to distinguish between sparrows and wrens, which are both brown. We propose Follow-up Differential Descriptions (FuDD), a zero-shot approach that tailors the class descriptions to each dataset and leads to additional attributes that better differentiate the target classes. FuDD first identifies the ambiguous classes for each image, and then uses a Large Language Model (LLM) to generate new class descriptions that differentiate between them. The new class descriptions resolve the initial ambiguity and help predict the correct label. In our experiments, FuDD consistently outperforms generic description ensembles and naive LLM-generated descriptions on 12 datasets. We show that differential descriptions are an effective tool to resolve class ambiguities, which otherwise significantly degrade the performance. We also show that high quality natural language class descriptions produced by FuDD result in comparable performance to few-shot adaptation methods.
</details>
<details>
<summary>摘要</summary>
一种有前途的方法是通过扩展类描述（即提示）来提高视觉语言模型如CLIP的图像分类性能。例如，使用 Brown Sparrow 而不是只使用 Sparrow。然而，当前的零shot方法会选择图像集中的一 subset of 属性，而不考虑这些目标类之间的共通点，这可能无法提供任何有用的信息，用于 distinguishing  между他们。例如，它们可能使用颜色而不是嘴形来分辨鸟鹤和织纹鸟，它们都是棕色的。我们提出了 Follow-up Differential Descriptions (FuDD)，一种零shot方法，它可以为每个图像集定制类描述，并且生成更好地分 differentiate 目标类的属性。FuDD 首先确定每个图像中的抽象类，然后使用大型自然语言模型（LLM）生成新的类描述，以解决初始的混淆。这些新的类描述可以分解初始的混淆，并帮助预测正确的标签。在我们的实验中，FuDD  consistently 超过了通用描述阵列和幼AGE LLM 生成的描述在 12 个数据集上。我们展示了 differential 描述是一种有效的工具，用于解决类混淆，否则会对性能产生负面影响。我们还展示了 FuDD 生成的高质量自然语言类描述可以达到与几 shot 适应方法相同的性能。
</details></li>
</ul>
<hr>
<h2 id="Trends-in-Integration-of-Knowledge-and-Large-Language-Models-A-Survey-and-Taxonomy-of-Methods-Benchmarks-and-Applications"><a href="#Trends-in-Integration-of-Knowledge-and-Large-Language-Models-A-Survey-and-Taxonomy-of-Methods-Benchmarks-and-Applications" class="headerlink" title="Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications"></a>Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05876">http://arxiv.org/abs/2311.05876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting liu</li>
<li>for: 本文旨在为大语言模型（LLM）的issues提供一个综述，包括知识编辑和数据检索增强策略。</li>
<li>methods: 本文提出了一种综述，包括方法分类、标准准比和应用场景。</li>
<li>results: 本文提出了未来研究方向，包括数据增强、知识编辑和模型提升等。<details>
<summary>Abstract</summary>
Large language models (LLMs) exhibit superior performance on various natural language tasks, but they are susceptible to issues stemming from outdated data and domain-specific limitations. In order to address these challenges, researchers have pursued two primary strategies, knowledge editing and retrieval augmentation, to enhance LLMs by incorporating external information from different aspects. Nevertheless, there is still a notable absence of a comprehensive survey. In this paper, we propose a review to discuss the trends in integration of knowledge and large language models, including taxonomy of methods, benchmarks, and applications. In addition, we conduct an in-depth analysis of different methods and point out potential research directions in the future. We hope this survey offers the community quick access and a comprehensive overview of this research area, with the intention of inspiring future research endeavors.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在各种自然语言任务上表现出色，但它们受到过时数据和领域特定限制的影响。为了解决这些挑战，研究人员通过知识编辑和检索增强来增强LLM，并将外部信息integrate到不同方面。然而，当前仍然缺乏一份全面的评论。本文提出了一篇文章，探讨大型语言模型和知识 интеграción的趋势，包括方法分类、标准准比和应用场景。此外，我们还进行了深入的分析不同方法，并指出了未来研究的可能性。我们希望这份评论可以为社区提供快速的访问和全面的概述，以便鼓励未来的研究努力。
</details></li>
</ul>
<hr>
<h2 id="Let’s-Reinforce-Step-by-Step"><a href="#Let’s-Reinforce-Step-by-Step" class="headerlink" title="Let’s Reinforce Step by Step"></a>Let’s Reinforce Step by Step</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05821">http://arxiv.org/abs/2311.05821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah Pan, Vladislav Lialin, Sherin Muckatira, Anna Rumshisky</li>
<li>for: 提高语言模型在复杂任务中的逻辑理解能力</li>
<li>methods: 使用人工回馈学习（RLHF）和不同的奖励模型（ORM和PRM）来调整模型的理解过程</li>
<li>results: 结果显示，使用PRM-based方法可以提高简单数学逻辑（GSM8K）的准确率，但在复杂任务（MATH）中，不料地下降性能，并且奖励聚合函数的作用对模型性能产生关键作用。<details>
<summary>Abstract</summary>
While recent advances have boosted LM proficiency in linguistic benchmarks, LMs consistently struggle to reason correctly on complex tasks like mathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as a method with which to shape model reasoning processes. In particular, we explore two reward schemes, outcome-supervised reward models (ORMs) and process-supervised reward models (PRMs), to optimize for logical reasoning. Our results show that the fine-grained reward provided by PRM-based methods enhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly, reducing performance in complex tasks (MATH). Furthermore, we show the critical role reward aggregation functions play in model performance. Providing promising avenues for future research, our study underscores the need for further exploration into fine-grained reward modeling for more reliable language models.
</details>
<details>
<summary>摘要</summary>
Recent advances have improved the proficiency of language models (LMs) in linguistic benchmarks, but they consistently struggle with complex tasks like mathematics. To improve the reasoning processes of LMs, we turn to reinforcement learning from human feedback (RLHF). Specifically, we explore two reward schemes, outcome-supervised reward models (ORMs) and process-supervised reward models (PRMs), to optimize for logical reasoning. Our results show that the fine-grained reward provided by PRM-based methods enhances accuracy in simple mathematical reasoning (GSM8K) while, unexpectedly, reducing performance in complex tasks (MATH). Additionally, we find that the aggregation functions used in the reward models play a critical role in model performance. This study highlights the need for further research into fine-grained reward modeling for more reliable language models.
</details></li>
</ul>
<hr>
<h2 id="CFBenchmark-Chinese-Financial-Assistant-Benchmark-for-Large-Language-Model"><a href="#CFBenchmark-Chinese-Financial-Assistant-Benchmark-for-Large-Language-Model" class="headerlink" title="CFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model"></a>CFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05812">http://arxiv.org/abs/2311.05812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tongjifinlab/cfbenchmark">https://github.com/tongjifinlab/cfbenchmark</a></li>
<li>paper_authors: Yang Lei, Jiangtong Li, Ming Jiang, Junjie Hu, Dawei Cheng, Zhijun Ding, Changjun Jiang</li>
<li>for: 评估大语言模型在金融领域的性能，以及检验这些模型是否能够准确地处理中文金融文本。</li>
<li>methods: 提出了CFBenchmark基本版，用于评估中文金融文本处理能力的八个任务，包括认知、分类和生成等方面。</li>
<li>results: 对一些现有的语言模型进行实验，发现虽有一些模型在特定任务上表现出色，但总体来说，现有模型在基本金融文本处理任务中仍有很大的提升空间。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated great potential in the financial domain. Thus, it becomes important to assess the performance of LLMs in the financial tasks. In this work, we introduce CFBenchmark, to evaluate the performance of LLMs for Chinese financial assistant. The basic version of CFBenchmark is designed to evaluate the basic ability in Chinese financial text processing from three aspects~(\emph{i.e.} recognition, classification, and generation) including eight tasks, and includes financial texts ranging in length from 50 to over 1,800 characters. We conduct experiments on several LLMs available in the literature with CFBenchmark-Basic, and the experimental results indicate that while some LLMs show outstanding performance in specific tasks, overall, there is still significant room for improvement in basic tasks of financial text processing with existing models. In the future, we plan to explore the advanced version of CFBenchmark, aiming to further explore the extensive capabilities of language models in more profound dimensions as a financial assistant in Chinese. Our codes are released at https://github.com/TongjiFinLab/CFBenchmark.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在金融领域的潜力已经得到了广泛的认可。因此，评估 LLM 在金融任务中的表现变得非常重要。在这项工作中，我们提出了 CFBenchmark，用于评估中文金融助手的 LLM 表现。CFBenchmark 的基本版本包括三个方面的八个任务，包括文本识别、分类和生成等，并且文本的长度从 50 字符到超过 1,800 字符不等。我们在文献中公布的一些 LLM 上进行了 CFBenchmark-Basic 的实验，结果表明，虽然一些 LLM 在特定任务中表现出色，但总体来说，现有模型仍然在基本的金融文本处理任务中存在很大的改进空间。未来，我们计划将 CFBenchmark 的高级版本推出，以更深入探索语言模型在中文金融助手中的广泛能力。我们的代码在 GitHub 上公布，请参考 https://github.com/TongjiFinLab/CFBenchmark。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/10/cs.CL_2023_11_10/" data-id="clpahu71v00fh3h88abmdhl7b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/10/cs.LG_2023_11_10/" class="article-date">
  <time datetime="2023-11-10T10:00:00.000Z" itemprop="datePublished">2023-11-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/10/cs.LG_2023_11_10/">cs.LG - 2023-11-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="k-Parameter-Approach-for-False-In-Season-Anomaly-Suppression-in-Daily-Time-Series-Anomaly-Detection"><a href="#k-Parameter-Approach-for-False-In-Season-Anomaly-Suppression-in-Daily-Time-Series-Anomaly-Detection" class="headerlink" title="k-Parameter Approach for False In-Season Anomaly Suppression in Daily Time Series Anomaly Detection"></a>k-Parameter Approach for False In-Season Anomaly Suppression in Daily Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08422">http://arxiv.org/abs/2311.08422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Yuansang Zha, Vaishnavi Kommaraju, Okenna Obi-Njoku, Vijay Dakshinamoorthy, Anirudh Agnihotri, Nantes Kirsten</li>
<li>for:  Detecting anomalies in a daily time series with a weekly pattern, to suppress misleading alerts while preserving real positives.</li>
<li>methods:  k-parameter approach, decomposition method.</li>
<li>results:  Favorable results.<details>
<summary>Abstract</summary>
Detecting anomalies in a daily time series with a weekly pattern is a common task with a wide range of applications. A typical way of performing the task is by using decomposition method. However, the method often generates false positive results where a data point falls within its weekly range but is just off from its weekday position. We refer to this type of anomalies as "in-season anomalies", and propose a k-parameter approach to address the issue. The approach provides configurable extra tolerance for in-season anomalies to suppress misleading alerts while preserving real positives. It yields favorable result.
</details>
<details>
<summary>摘要</summary>
检测日征时序中的每周征性异常是一项广泛应用的任务。通常使用分解方法来实现这个任务，但这种方法经常生成假阳性结果，其中一个数据点在其每周范围内但是偏离其每周日期位置。我们称这种异常为“在季度异常”，并提出了k参数方法来解决这个问题。这种方法可以配置额外的宽限容许季度异常，以避免误导性警报，同时保留真正的阳性结果。它的结果很有利。
</details></li>
</ul>
<hr>
<h2 id="A-Trichotomy-for-Transductive-Online-Learning"><a href="#A-Trichotomy-for-Transductive-Online-Learning" class="headerlink" title="A Trichotomy for Transductive Online Learning"></a>A Trichotomy for Transductive Online Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06428">http://arxiv.org/abs/2311.06428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steve Hanneke, Shay Moran, Jonathan Shafer</li>
<li>for: 这个论文的目的是对Ben-David、Kushilevitz和Mansour（1997）的在线学习设定下的推论进行新的上界和下界 bounds。</li>
<li>methods: 这个论文使用了一种新的三分法，其中分为三个可能的值：$n$, $\Theta\left(\log (n)\right)$, 和 $\Theta(1)$。这个结果取决于VCDimension和Littlestone dimension的组合。</li>
<li>results: 这个论文证明了VC Dimension和Littlestone dimension之间的关系，并提供了一系列 bounds，其中包括一个新的下界，可以提高之前的下界。此外，这个论文还扩展到多类分类和agnostic设定。<details>
<summary>Abstract</summary>
We present new upper and lower bounds on the number of learner mistakes in the `transductive' online learning setting of Ben-David, Kushilevitz and Mansour (1997). This setting is similar to standard online learning, except that the adversary fixes a sequence of instances $x_1,\dots,x_n$ to be labeled at the start of the game, and this sequence is known to the learner. Qualitatively, we prove a trichotomy, stating that the minimal number of mistakes made by the learner as $n$ grows can take only one of precisely three possible values: $n$, $\Theta\left(\log (n)\right)$, or $\Theta(1)$. Furthermore, this behavior is determined by a combination of the VC dimension and the Littlestone dimension. Quantitatively, we show a variety of bounds relating the number of mistakes to well-known combinatorial dimensions. In particular, we improve the known lower bound on the constant in the $\Theta(1)$ case from $\Omega\left(\sqrt{\log(d)}\right)$ to $\Omega(\log(d))$ where $d$ is the Littlestone dimension. Finally, we extend our results to cover multiclass classification and the agnostic setting.
</details>
<details>
<summary>摘要</summary>
我们提出新的上下界关于学习者错误的数量在Ben-David、Kushilevitz和Mansour（1997）的推uctive在线学习Setting中。这个设定与标准的在线学习相似，但是敌人会在游戏开始前固定一个序列实例$x_1,\dots,x_n$的标签，并且这个序列是学习者知道的。qualitatively，我们证明了一种三分法， stating that the minimal number of mistakes made by the learner as $n$ grows can take only one of precisely three possible values: $n$, $\Theta\left(\log (n)\right)$, or $\Theta(1)$. Furthermore, this behavior is determined by a combination of the VC dimension and the Littlestone dimension. Quantitatively, we show a variety of bounds relating the number of mistakes to well-known combinatorial dimensions. In particular, we improve the known lower bound on the constant in the $\Theta(1)$ case from $\Omega\left(\sqrt{\log(d)}\right)$ to $\Omega(\log(d))$ where $d$ is the Littlestone dimension. Finally, we extend our results to cover multiclass classification and the agnostic setting.Note: "推uctive" is a typo, it should be "online" instead.
</details></li>
</ul>
<hr>
<h2 id="A-comprehensive-analysis-of-concept-drift-locality-in-data-streams"><a href="#A-comprehensive-analysis-of-concept-drift-locality-in-data-streams" class="headerlink" title="A comprehensive analysis of concept drift locality in data streams"></a>A comprehensive analysis of concept drift locality in data streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06396">http://arxiv.org/abs/2311.06396</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabrieljaguiar/locality-concept-drift">https://github.com/gabrieljaguiar/locality-concept-drift</a></li>
<li>paper_authors: Gabriel J. Aguiar, Alberto Cano</li>
<li>For: 本研究目的是为了探讨概念变革的探测，以便在线学习中进行有效的模型适应。* Methods: 本研究使用了9种现有的概念变革探测方法，并进行了比较性评估，以显示它们在不同的难度水平上的表现。* Results: 研究发现，概念变革的地方性和范围有重要影响在标签器性能上，并提出了不同的概念变革类别下的最佳适应策略。<details>
<summary>Abstract</summary>
Adapting to drifting data streams is a significant challenge in online learning. Concept drift must be detected for effective model adaptation to evolving data properties. Concept drift can impact the data distribution entirely or partially, which makes it difficult for drift detectors to accurately identify the concept drift. Despite the numerous concept drift detectors in the literature, standardized procedures and benchmarks for comprehensive evaluation considering the locality of the drift are lacking. We present a novel categorization of concept drift based on its locality and scale. A systematic approach leads to a set of 2,760 benchmark problems, reflecting various difficulty levels following our proposed categorization. We conduct a comparative assessment of 9 state-of-the-art drift detectors across diverse difficulties, highlighting their strengths and weaknesses for future research. We examine how drift locality influences the classifier performance and propose strategies for different drift categories to minimize the recovery time. Lastly, we provide lessons learned and recommendations for future concept drift research. Our benchmark data streams and experiments are publicly available at https://github.com/gabrieljaguiar/locality-concept-drift.
</details>
<details>
<summary>摘要</summary>
适应漂移数据流是在线学习中的一大挑战。概念漂移必须被探测，以便有效地适应数据质量的发展。概念漂移可能会影响整个数据分布或只影响一部分，这使得漂移探测器很难准确地确定概念漂移。尽管文献中有很多概念漂移探测器，但是没有标准化的程序和标准准则 для全面评估，考虑到漂移的地方性。我们提出了一种新的概念漂移分类方法，基于其地方性和规模。我们通过这种分类方法，生成了2,760个benchmark问题，各种难度水平都有reflect。我们对9种当前state-of-the-art漂移探测器进行了 Comparative 评估，并 highlights 它们在不同难度水平上的优势和缺陷，以便未来研究。我们 также examine 如何在不同的漂移类别下，最小化恢复时间。最后，我们提供了未来概念漂移研究的教训和建议，以及我们的benchmark数据流和实验结果，可以在https://github.com/gabrieljaguiar/locality-concept-drift上获取。
</details></li>
</ul>
<hr>
<h2 id="A-statistical-perspective-on-algorithm-unrolling-models-for-inverse-problems"><a href="#A-statistical-perspective-on-algorithm-unrolling-models-for-inverse-problems" class="headerlink" title="A statistical perspective on algorithm unrolling models for inverse problems"></a>A statistical perspective on algorithm unrolling models for inverse problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06395">http://arxiv.org/abs/2311.06395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yves Atchade, Xinru Liu, Qiuyun Zhu</li>
<li>for: 该论文主要研究的是使用深度神经网络解决逆问题，具体来说是通过对各个实例进行数次迭代来使用前向模型来估计latent variable。</li>
<li>methods: 该论文使用了算法折叠方法，具体来说是使用proximal梯度下降算法驱动的深度神经网络。</li>
<li>results: 该论文显示了 Gradient Descent Network (GDN) 的统计复杂性是 $\mathcal{O}(\log(n)&#x2F;\log(\varrho_n^{-1}))$，其中 $n$ 是样本大小，$\varrho_n$ 是梯度下降算法的速度。此外，当 negative log-density of latent variable $\bf x$ 有简单的 proximal 操作时，Then a GDN unrolled at depth $D’$ can solve the inverse problem at the parametric rate $O(D’&#x2F;\sqrt{n})$.<details>
<summary>Abstract</summary>
We consider inverse problems where the conditional distribution of the observation ${\bf y}$ given the latent variable of interest ${\bf x}$ (also known as the forward model) is known, and we have access to a data set in which multiple instances of ${\bf x}$ and ${\bf y}$ are both observed. In this context, algorithm unrolling has become a very popular approach for designing state-of-the-art deep neural network architectures that effectively exploit the forward model. We analyze the statistical complexity of the gradient descent network (GDN), an algorithm unrolling architecture driven by proximal gradient descent. We show that the unrolling depth needed for the optimal statistical performance of GDNs is of order $\log(n)/\log(\varrho_n^{-1})$, where $n$ is the sample size, and $\varrho_n$ is the convergence rate of the corresponding gradient descent algorithm. We also show that when the negative log-density of the latent variable ${\bf x}$ has a simple proximal operator, then a GDN unrolled at depth $D'$ can solve the inverse problem at the parametric rate $O(D'/\sqrt{n})$. Our results thus also suggest that algorithm unrolling models are prone to overfitting as the unrolling depth $D'$ increases. We provide several examples to illustrate these results.
</details>
<details>
<summary>摘要</summary>
我们考虑反向问题，其中观察变量 $\bf y$  conditional distribution given 隐藏变量 $\bf x$ (也称为前向模型) 已知，并且我们有许多 $\bf x$ 和 $\bf y$ 的实例数据集。在这种情况下，算法卷积（algorithm unrolling）已成为设计前所未有的深度神经网络架构的非常流行的方法。我们分析了梯度下降网络（Gradient Descent Network，GDN）的统计复杂性。我们显示了 GDN 的推 rolling 深度需要为 $\log(n)/\log(\varrho_n^{-1})$，其中 $n$ 是样本大小，$\varrho_n$ 是相应的梯度下降算法的收敛速率。我们还显示了，当隐藏变量 $\bf x$ 的负梯度Log-浓度有简单的 proximal 运算时，那么在推 rolling 深度 $D'$ 下，GDN 可以在 $O(D'/\sqrt{n})$ 的速率解决反向问题。我们的结果也表明，algorithm unrolling 模型容易过拟合，随着推 rolling 深度 $D'$ 增加。我们给出了一些示例来证明这些结果。
</details></li>
</ul>
<hr>
<h2 id="Theory-and-implementation-of-inelastic-Constitutive-Artificial-Neural-Networks"><a href="#Theory-and-implementation-of-inelastic-Constitutive-Artificial-Neural-Networks" class="headerlink" title="Theory and implementation of inelastic Constitutive Artificial Neural Networks"></a>Theory and implementation of inelastic Constitutive Artificial Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06380">http://arxiv.org/abs/2311.06380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://zenodo.org/record/10066805">https://zenodo.org/record/10066805</a></li>
<li>paper_authors: Hagen Holthusen, Lukas Lamm, Tim Brepols, Stefanie Reese, Ellen Kuhl</li>
<li>For: The paper aims to develop a new method called Constitutive Artificial Neural Networks (CANN) to model the inelastic behavior of materials.* Methods: The paper uses a combination of feed-forward networks of the free energy and pseudo potential with a recurrent neural network approach to take time dependencies into account.* Results: The paper demonstrates that the iCANN is capable of autonomously discovering models for artificially generated data, the response of polymers for cyclic loading, and the relaxation behavior of muscle data.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文目标是开发一种新的 constitutive artificial neural networks (CANN) 模型，用于描述材料的不可归一性行为。</li>
<li>methods: 该论文使用一种组合 feed-forward 网络和 recurrent neural network 方法，以处理时间依赖关系。</li>
<li>results: 论文示出 iCANN 可以自动找到模型，包括人工生成数据、聚合物的循环加载响应和肌肉数据的 relaxation 行为。<details>
<summary>Abstract</summary>
Nature has always been our inspiration in the research, design and development of materials and has driven us to gain a deep understanding of the mechanisms that characterize anisotropy and inelastic behavior. All this knowledge has been accumulated in the principles of thermodynamics. Deduced from these principles, the multiplicative decomposition combined with pseudo potentials are powerful and universal concepts. Simultaneously, the tremendous increase in computational performance enabled us to investigate and rethink our history-dependent material models to make the most of our predictions. Today, we have reached a point where materials and their models are becoming increasingly sophisticated. This raises the question: How do we find the best model that includes all inelastic effects to explain our complex data? Constitutive Artificial Neural Networks (CANN) may answer this question. Here, we extend the CANNs to inelastic materials (iCANN). Rigorous considerations of objectivity, rigid motion of the reference configuration, multiplicative decomposition and its inherent non-uniqueness, restrictions of energy and pseudo potential, and consistent evolution guide us towards the architecture of the iCANN satisfying thermodynamics per design. We combine feed-forward networks of the free energy and pseudo potential with a recurrent neural network approach to take time dependencies into account. We demonstrate that the iCANN is capable of autonomously discovering models for artificially generated data, the response of polymers for cyclic loading and the relaxation behavior of muscle data. As the design of the network is not limited to visco-elasticity, our vision is that the iCANN will reveal to us new ways to find the various inelastic phenomena hidden in the data and to understand their interaction. Our source code, data, and examples are available at doi.org/10.5281/zenodo.10066805
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Higher-Order-Newton-Methods-with-Polynomial-Work-per-Iteration"><a href="#Higher-Order-Newton-Methods-with-Polynomial-Work-per-Iteration" class="headerlink" title="Higher-Order Newton Methods with Polynomial Work per Iteration"></a>Higher-Order Newton Methods with Polynomial Work per Iteration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06374">http://arxiv.org/abs/2311.06374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Ali Ahmadi, Abraar Chaudhry, Jeffrey Zhang</li>
<li>for: 这个论文旨在扩展新颖的方法，以优化函数的最小化问题。</li>
<li>methods: 该方法使用了semidefinite programming来构建和最小化一个凸函数的凸展开。</li>
<li>results: 该方法的本地收敛级别为$d$，比 classical Newton方法更低。数学示例表明，在$d$增加时，拥抱区域的面积可以增加。在certain assumptions下，我们还提出了一种修改后的算法，具有globally convergent和本地收敛级别为$d$。<details>
<summary>Abstract</summary>
We present generalizations of Newton's method that incorporate derivatives of an arbitrary order $d$ but maintain a polynomial dependence on dimension in their cost per iteration. At each step, our $d^{\text{th}$-order method uses semidefinite programming to construct and minimize a sum of squares-convex approximation to the $d^{\text{th}$-order Taylor expansion of the function we wish to minimize. We prove that our $d^{\text{th}$-order method has local convergence of order $d$. This results in lower oracle complexity compared to the classical Newton method. We show on numerical examples that basins of attraction around local minima can get larger as $d$ increases. Under additional assumptions, we present a modified algorithm, again with polynomial cost per iteration, which is globally convergent and has local convergence of order $d$.
</details>
<details>
<summary>摘要</summary>
我们提出了新项 Newton 方法的扩展，这些方法包括了阶数 $d$ 但是保持维度的 polynomial 依赖性。在每一步中，我们的 $d$ 阶方法使用半definite 程式来建构和最小化一个 sum of squares-凸函数的 Taylor 展开。我们证明了我们的 $d$ 阶方法有本地几何稳定性 order $d$。这导致与 класиical Newton 方法相比，我们的方法有较低的 oracle 复杂度。我们显示了一些数据例子，显示在 $d$ 增加时，当地点阶数的基础会变大。在更加假设下，我们提出了一个修改后的算法，这个算法还是有 polynomial 成本每一步，并且具有本地几何稳定性 order $d$ 和全球几何稳定性。
</details></li>
</ul>
<hr>
<h2 id="Blockchain-Enabled-Federated-Learning-Approach-for-Vehicular-Networks"><a href="#Blockchain-Enabled-Federated-Learning-Approach-for-Vehicular-Networks" class="headerlink" title="Blockchain-Enabled Federated Learning Approach for Vehicular Networks"></a>Blockchain-Enabled Federated Learning Approach for Vehicular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06372">http://arxiv.org/abs/2311.06372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shirin Sultana, Jahin Hossain, Maruf Billah, Hasibul Hossain Shajeeb, Saifur Rahman, Keyvan Ansari, Khondokar Fida Hasan</li>
<li>for: 这个研究的目的是提出一个实际的方法，融合 Federated Learning (FL) 和 Blockchain 技术，实现了数据隐私和系统安全性。</li>
<li>methods: 这个方法使用 Federated Learning (FL) 和 Blockchain 技术来建立一个分散式的车辆网络，车辆可以在不交换数据的情况下学习，并确保数据的隐私和数据完整性。</li>
<li>results: 这个方法在遭受黑客攻击的情况下，仍能维持高准确性（91.92%），与其他分散式 Federated Learning 技术相比，这个方法具有更高的安全性和可靠性。<details>
<summary>Abstract</summary>
Data from interconnected vehicles may contain sensitive information such as location, driving behavior, personal identifiers, etc. Without adequate safeguards, sharing this data jeopardizes data privacy and system security. The current centralized data-sharing paradigm in these systems raises particular concerns about data privacy. Recognizing these challenges, the shift towards decentralized interactions in technology, as echoed by the principles of Industry 5.0, becomes paramount. This work is closely aligned with these principles, emphasizing decentralized, human-centric, and secure technological interactions in an interconnected vehicular ecosystem. To embody this, we propose a practical approach that merges two emerging technologies: Federated Learning (FL) and Blockchain. The integration of these technologies enables the creation of a decentralized vehicular network. In this setting, vehicles can learn from each other without compromising privacy while also ensuring data integrity and accountability. Initial experiments show that compared to conventional decentralized federated learning techniques, our proposed approach significantly enhances the performance and security of vehicular networks. The system's accuracy stands at 91.92\%. While this may appear to be low in comparison to state-of-the-art federated learning models, our work is noteworthy because, unlike others, it was achieved in a malicious vehicle setting. Despite the challenging environment, our method maintains high accuracy, making it a competent solution for preserving data privacy in vehicular networks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese数据从连接的自动车可能包含敏感信息，如位置、驾驶行为、个人标识等。无效的安全措施可能会损害数据隐私和系统安全。现有中央化数据分享模式在这些系统中具有特别的隐私问题。认识到这些挑战，在技术发展的同时，倾向于分布式互动的方向，这与工业5.0的原则相吻合。这项工作与这些原则相关，强调分布式、人类中心、安全的技术互动在连接的自动车环境中。为实现这一目标，我们提议一种实用的方法，将 Federated Learning（FL）和区块链技术融合在一起。这种 integrate 的方法可以创建一个分布式的自动车网络。在这个设定下，车辆可以在不侵犯隐私的情况下学习从别的车辆，同时保证数据的完整性和责任。初始实验表明，与传统的分布式联合学习技术相比，我们提议的方法可以明显提高自动车网络的性能和安全性。系统的准确率为91.92%。尽管这些值与当前的联合学习模型相比较低，但我们的工作具有突出的特点，即在恶势力车辆环境下实现高准确率，而不是其他人所做的。不管挑战环境，我们的方法都能保持高准确率，这使得它成为了保护自动车网络数据隐私的可靠解决方案。
</details></li>
</ul>
<hr>
<h2 id="The-AeroSonicDB-YPAD-0523-Dataset-for-Acoustic-Detection-and-Classification-of-Aircraft"><a href="#The-AeroSonicDB-YPAD-0523-Dataset-for-Acoustic-Detection-and-Classification-of-Aircraft" class="headerlink" title="The AeroSonicDB (YPAD-0523) Dataset for Acoustic Detection and Classification of Aircraft"></a>The AeroSonicDB (YPAD-0523) Dataset for Acoustic Detection and Classification of Aircraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06368">http://arxiv.org/abs/2311.06368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blake Downward, Jon Nordby</li>
<li>for: 提高机器听写技术的进步，透过为训练听写系统提供丰富的域特定听写数据集</li>
<li>methods: 利用ADS-B广播传输来采集和标注听写样本，并提供了14个补充（非音频）标签来描述飞机</li>
<li>results: 基本结果显示了三种二分类模型的性能，并讨论了当前数据集的局限性和未来的潜在价值<details>
<summary>Abstract</summary>
The time and expense required to collect and label audio data has been a prohibitive factor in the availability of domain specific audio datasets. As the predictive specificity of a classifier depends on the specificity of the labels it is trained on, it follows that finely-labelled datasets are crucial for advances in machine learning. Aiming to stimulate progress in the field of machine listening, this paper introduces AeroSonicDB (YPAD-0523), a dataset of low-flying aircraft sounds for training acoustic detection and classification systems. This paper describes the method of exploiting ADS-B radio transmissions to passively collect and label audio samples. Provides a summary of the collated dataset. Presents baseline results from three binary classification models, then discusses the limitations of the current dataset and its future potential. The dataset contains 625 aircraft recordings ranging in event duration from 18 to 60 seconds, for a total of 8.87 hours of aircraft audio. These 625 samples feature 301 unique aircraft, each of which are supplied with 14 supplementary (non-acoustic) labels to describe the aircraft. The dataset also contains 3.52 hours of ambient background audio ("silence"), as a means to distinguish aircraft noise from other local environmental noises. Additionally, 6 hours of urban soundscape recordings (with aircraft annotations) are included as an ancillary method for evaluating model performance, and to provide a testing ground for real-time applications.
</details>
<details>
<summary>摘要</summary>
过往，收集和标签音频数据的时间和成本因素，对于特定领域的音频数据集的可用性是一个阻碍因素。当predictive特定性取决于labels训练的特定性，这意味着精确地标签数据集是预测机器学习的关键。为了促进机器听力领域的进步，本文发布了AeroSonicDB（YPAD-0523），一个低飞行 aircraft 音频数据集，用于训练音频检测和分类系统。本文详细介绍了使用ADS-B无线电传输来过程式收集和标签音频 Samples。提供了数据集的总结，并提出了三个binary分类模型的基eline结果。然后讨论了现有数据集的限制和未来潜力。这个数据集包含625架飞机录音， recording duration ranges from 18 to 60 seconds, for a total of 8.87 hours of aircraft audio. These 625 samples feature 301 unique aircraft, each of which are supplied with 14 supplementary (non-acoustic) labels to describe the aircraft. The dataset also contains 3.52 hours of ambient background audio ("silence"), as a means to distinguish aircraft noise from other local environmental noises. Additionally, 6 hours of urban soundscape recordings (with aircraft annotations) are included as an ancillary method for evaluating model performance, and to provide a testing ground for real-time applications.
</details></li>
</ul>
<hr>
<h2 id="CALLOC-Curriculum-Adversarial-Learning-for-Secure-and-Robust-Indoor-Localization"><a href="#CALLOC-Curriculum-Adversarial-Learning-for-Secure-and-Robust-Indoor-Localization" class="headerlink" title="CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor Localization"></a>CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06361">http://arxiv.org/abs/2311.06361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danish Gufran, Sudeep Pasricha</li>
<li>for: 本研究旨在提高室内地位定位精度，抵御室内环境和设备变化所导致的准确性降低和攻击。</li>
<li>methods: 本研究提出了一种名为CALLOC的新框架，该框架通过适应性课程学习和特有的轻量级扩展点积分神经网络，实现了对室内环境和设备变化的抗预测和攻击。</li>
<li>results: 实验证明，CALLOC可以在多种不同的室内场景、移动设备和攻击enario中提高准确性，比如平均误差下降6.03倍，最差情况下误差下降4.6倍，相比之下现有的室内地位定位框架。<details>
<summary>Abstract</summary>
Indoor localization has become increasingly vital for many applications from tracking assets to delivering personalized services. Yet, achieving pinpoint accuracy remains a challenge due to variations across indoor environments and devices used to assist with localization. Another emerging challenge is adversarial attacks on indoor localization systems that not only threaten service integrity but also reduce localization accuracy. To combat these challenges, we introduce CALLOC, a novel framework designed to resist adversarial attacks and variations across indoor environments and devices that reduce system accuracy and reliability. CALLOC employs a novel adaptive curriculum learning approach with a domain specific lightweight scaled-dot product attention neural network, tailored for adversarial and variation resilience in practical use cases with resource constrained mobile devices. Experimental evaluations demonstrate that CALLOC can achieve improvements of up to 6.03x in mean error and 4.6x in worst-case error against state-of-the-art indoor localization frameworks, across diverse building floorplans, mobile devices, and adversarial attacks scenarios.
</details>
<details>
<summary>摘要</summary>
indoor定位已成为许多应用程序中越来越重要的一部分，从跟踪资产到提供个性化服务。然而，实现精确定位仍然是一大挑战，因为室内环境中的变化和用于帮助定位的设备之间存在差异。此外，indoor定位系统也面临着抗 adversarial 攻击的挑战，这些攻击不仅会威胁服务的一致性，而且还会减少定位精度。为解决这些挑战，我们介绍了 CALLOC，一个新的框架，旨在抵抗抗 adversarial 攻击和室内环境中的变化。CALLOC 使用了一种新的适应学习approach，其中包括一个适应性较强的域特定缩小乘数产品注意力神经网络，特制 для抗 adversarial 和变化的鲁棒性。在实际使用情况下，CALLOC 可以在不同的建筑层面、移动设备和抗 adversarial 攻击方面实现改进。我们的实验评估表明，CALLOC 可以与现有的indoor定位框架相比，在多种不同的室内环境、移动设备和抗 adversarial 攻击场景中实现改进。改进的均方误差和最均方误差为6.03倍和4.6倍。
</details></li>
</ul>
<hr>
<h2 id="Compact-Matrix-Quantum-Group-Equivariant-Neural-Networks"><a href="#Compact-Matrix-Quantum-Group-Equivariant-Neural-Networks" class="headerlink" title="Compact Matrix Quantum Group Equivariant Neural Networks"></a>Compact Matrix Quantum Group Equivariant Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06358">http://arxiv.org/abs/2311.06358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Pearce-Crump</li>
<li>for: 这 paper written for 研究 neural network 学习从数据中的量子同质性。</li>
<li>methods: 这 paper 使用 Woronowicz 的 Tannaka-Krein duality 来描述 compact matrix quantum group 对应的 weight matrices。</li>
<li>results: 这 paper 提出了一种新的类型的 neural network， called compact matrix quantum group equivariant neural network，可以从数据中学习量子同质性。此外，paper 还证明了这种 neural network 包含了所有 compact matrix group equivariant neural network 为子集。同时，paper 也获得了许多 compact matrix group equivariant neural network 的 weight matrices 的Characterization，这些 weight matrices 之前没有出现在机器学习文献中。<details>
<summary>Abstract</summary>
We derive the existence of a new type of neural network, called a compact matrix quantum group equivariant neural network, that learns from data that has an underlying quantum symmetry. We apply the Woronowicz formulation of Tannaka-Krein duality to characterise the weight matrices that appear in these neural networks for any easy compact matrix quantum group. We show that compact matrix quantum group equivariant neural networks contain, as a subclass, all compact matrix group equivariant neural networks. Moreover, we obtain characterisations of the weight matrices for many compact matrix group equivariant neural networks that have not previously appeared in the machine learning literature.
</details>
<details>
<summary>摘要</summary>
我们从数据中吸取了一种新的神经网络，即含有量子同质性的矩阵量子群响应神经网络。我们使用沃罗诺维茨形式的塔那卡-克雷因对吸引神经网络的Weight矩阵进行了定义。我们证明了矩阵量子群响应神经网络包含所有矩阵群响应神经网络的子类。此外，我们获得了许多矩阵群响应神经网络在机器学习文献中未出现过的Weight矩阵的特征。
</details></li>
</ul>
<hr>
<h2 id="EVORA-Deep-Evidential-Traversability-Learning-for-Risk-Aware-Off-Road-Autonomy"><a href="#EVORA-Deep-Evidential-Traversability-Learning-for-Risk-Aware-Off-Road-Autonomy" class="headerlink" title="EVORA: Deep Evidential Traversability Learning for Risk-Aware Off-Road Autonomy"></a>EVORA: Deep Evidential Traversability Learning for Risk-Aware Off-Road Autonomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06234">http://arxiv.org/abs/2311.06234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyi Cai, Siddharth Ancha, Lakshay Sharma, Philip R. Osteen, Bernadette Bucher, Stephen Phillips, Jiuguang Wang, Michael Everett, Nicholas Roy, Jonathan P. How</li>
<li>for: 本研究旨在提高快速机器人跟踪减少摩擦的能力，尤其是在不可预知的地形下。</li>
<li>methods: 本研究使用自我监督学习方法，直接从数据中学习地形特征，而不是手动设置成本。</li>
<li>results: 研究提出了一种能够有效地量化和mitigate Risks的方法，包括学习批处理分布和概率密度，以及一种新的不确定性感知loss函数。这些方法有助于提高机器人的导航性能。<details>
<summary>Abstract</summary>
Traversing terrain with good traction is crucial for achieving fast off-road navigation. Instead of manually designing costs based on terrain features, existing methods learn terrain properties directly from data via self-supervision, but challenges remain to properly quantify and mitigate risks due to uncertainties in learned models. This work efficiently quantifies both aleatoric and epistemic uncertainties by learning discrete traction distributions and probability densities of the traction predictor's latent features. Leveraging evidential deep learning, we parameterize Dirichlet distributions with the network outputs and propose a novel uncertainty-aware squared Earth Mover's distance loss with a closed-form expression that improves learning accuracy and navigation performance. The proposed risk-aware planner simulates state trajectories with the worst-case expected traction to handle aleatoric uncertainty, and penalizes trajectories moving through terrain with high epistemic uncertainty. Our approach is extensively validated in simulation and on wheeled and quadruped robots, showing improved navigation performance compared to methods that assume no slip, assume the expected traction, or optimize for the worst-case expected cost.
</details>
<details>
<summary>摘要</summary>
通过适量地形的探索是快速Off-road导航的关键。现有方法通过自我超视来学习地形特性，但是存在风险量化和mitigate风险的挑战。本工作效率地量化了 aleatoric 和 epistemic 不确定性，通过学习离散的扩展特征分布和概率密度来。基于征识深度学习，我们使用网络输出来参数化地 Dirichlet 分布，并提出了一种新的不确定性意识深度Move的距离损失函数，这个函数具有闭合式表达，可以提高学习精度和导航性能。我们的风险意识规划器通过 simulate 状态轨迹的最差预期扩展特征来处理 aleatoric 不确定性，并对高 epistemic 不确定性的轨迹进行惩罚。我们的方法在 simulate 和有脚和四脚机器人上进行了广泛验证，与不考虑滑动、预期的扩展特征或优化最差预期成本的方法进行比较，显示了改进的导航性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-material-synthesis-structure-property-relationship-by-data-fusion-Bayesian-Co-regionalization-N-Dimensional-Piecewise-Function-Learning"><a href="#Learning-material-synthesis-structure-property-relationship-by-data-fusion-Bayesian-Co-regionalization-N-Dimensional-Piecewise-Function-Learning" class="headerlink" title="Learning material synthesis-structure-property relationship by data fusion: Bayesian Co-regionalization N-Dimensional Piecewise Function Learning"></a>Learning material synthesis-structure-property relationship by data fusion: Bayesian Co-regionalization N-Dimensional Piecewise Function Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06228">http://arxiv.org/abs/2311.06228</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Gilad Kusne, Austin McDannald, Brian DeCost</li>
<li>For: 本研究旨在推动下一代技术的发展，如量子计算、碳捕集和低成本医疗影像等。* Methods: 研究人员使用了知识管理和数据融合技术，将不同仪器和实验室的数据集成在一起，以学习材料制备-结构-性质关系。* Results: 研究人员提出了一种名为Synthesis-structure-property relAtionship coreGionalized lEarner（SAGE）算法，可以在多种数据源之间进行数据融合，以学习材料制备-结构-性质关系。<details>
<summary>Abstract</summary>
Advanced materials are needed to further next-generation technologies such as quantum computing, carbon capture, and low-cost medical imaging. However, advanced materials discovery is confounded by two fundamental challenges: the challenge of a high-dimensional, complex materials search space and the challenge of combining knowledge, i.e., data fusion across instruments and labs. To overcome the first challenge, researchers employ knowledge of the underlying material synthesis-structure-property relationship, as a material's structure is often predictive of its functional property and vice versa. For example, optimal materials often occur along composition-phase boundaries or within specific phase regions. Additionally, knowledge of the synthesis-structure-property relationship is fundamental to understanding underlying physical mechanisms. However, quantifying the synthesis-structure-property relationship requires overcoming the second challenge. Researchers must merge knowledge gathered across instruments, measurement modalities, and even laboratories. We present the Synthesis-structure-property relAtionship coreGionalized lEarner (SAGE) algorithm. A fully Bayesian algorithm that uses multimodal coregionalization to merge knowledge across data sources to learn synthesis-structure-property relationships.
</details>
<details>
<summary>摘要</summary>
高级材料需要进一步推动下一代技术，如量子计算、碳捕集和低成本医疗成像。然而，高级材料发现面临两个基本挑战：一是高维度、复杂的材料搜索空间挑战，二是组合知识挑战，即将数据源的知识融合到一起。为了解决第一个挑战，研究人员利用材料合成-结构-性能关系的知识，因为材料结构 oft predicts its functional property and vice versa。例如，理想的材料常occurs along composition-phase boundaries或在specific phase regions。此外，理解材料合成-结构-性能关系的基础知识是理解下面物理机制的基础。然而，量化材料合成-结构-性能关系需要解决第二个挑战。研究人员必须将数据源的知识融合到一起。我们介绍了 Synthesis-structure-property relAtionship coreGionalized lEarner（SAGE）算法。这是一种完全 Bayesian 算法，使用多modal coregionalization来融合数据源的知识，以学习材料合成-结构-性能关系。
</details></li>
</ul>
<hr>
<h2 id="Does-Differential-Privacy-Prevent-Backdoor-Attacks-in-Practice"><a href="#Does-Differential-Privacy-Prevent-Backdoor-Attacks-in-Practice" class="headerlink" title="Does Differential Privacy Prevent Backdoor Attacks in Practice?"></a>Does Differential Privacy Prevent Backdoor Attacks in Practice?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06227">http://arxiv.org/abs/2311.06227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fereshteh Razmi, Jian Lou, Li Xiong<br>for: This paper aims to investigate the effectiveness of different differential privacy (DP) techniques in preventing backdoor attacks in machine learning (ML) models, specifically examining PATE and Label-DP.methods: The paper employs DP-SGD and PATE to defend against backdoor attacks, and explores the role of different components of DP algorithms in defending against these attacks. The authors also propose Label-DP as a faster and more accurate alternative to DP-SGD and PATE.results: The experiments reveal that hyperparameters and the number of backdoors in the training dataset impact the success of DP algorithms, and that Label-DP algorithms can be more effective than DP methods in defending against backdoor attacks while maintaining model accuracy.<details>
<summary>Abstract</summary>
Differential Privacy (DP) was originally developed to protect privacy. However, it has recently been utilized to secure machine learning (ML) models from poisoning attacks, with DP-SGD receiving substantial attention. Nevertheless, a thorough investigation is required to assess the effectiveness of different DP techniques in preventing backdoor attacks in practice. In this paper, we investigate the effectiveness of DP-SGD and, for the first time in literature, examine PATE in the context of backdoor attacks. We also explore the role of different components of DP algorithms in defending against backdoor attacks and will show that PATE is effective against these attacks due to the bagging structure of the teacher models it employs. Our experiments reveal that hyperparameters and the number of backdoors in the training dataset impact the success of DP algorithms. Additionally, we propose Label-DP as a faster and more accurate alternative to DP-SGD and PATE. We conclude that while Label-DP algorithms generally offer weaker privacy protection, accurate hyper-parameter tuning can make them more effective than DP methods in defending against backdoor attacks while maintaining model accuracy.
</details>
<details>
<summary>摘要</summary>
diferencial privacidad (DP) fue desarrollada originalmente para proteger la privacidad, pero recientemente se ha utilizado para proteger modelos de aprendizaje automático (ML) de ataques de contaminación, con DP-SGD recibiendo una gran cantidad de atención. Sin embargo, se requiere una investigación exhaustiva para evaluar la eficacia de diferentes técnicas de privacidad diferencial en prevenir ataques de backdoor en la práctica. En este artículo, investigamos la eficacia de DP-SGD y, por primera vez en la literatura, examinamos PATE en el contexto de ataques de backdoor. Además, exploramos el papel de diferentes componentes de los algoritmos de privacidad diferencial en la defensa contra ataques de backdoor y demostraremos que PATE es efectivo contra estos ataques gracias a la estructura de bolsa de los modelos de maestro que emplea. Nuestras experimentos revelan que los hiperparámetros y el número de backdoors en el conjunto de entrenamiento del impactan el éxito de los algoritmos de privacidad diferencial. Además, propongo Label-DP como una alternativa más rápida y precisa a DP-SGD y PATE. Concluimos que, aunque los algoritmos Label-DP generalmente ofrecen una protección de privacidad más débil, la tuning de hiperparámetros precisa puede hacer que sean más efectivos que los métodos de privacidad diferencial en la defensa contra ataques de backdoor mientras se mantiene la precisión del modelo.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-VQ-VAE’s-for-Robust-White-Matter-Streamline-Encodings"><a href="#Differentiable-VQ-VAE’s-for-Robust-White-Matter-Streamline-Encodings" class="headerlink" title="Differentiable VQ-VAE’s for Robust White Matter Streamline Encodings"></a>Differentiable VQ-VAE’s for Robust White Matter Streamline Encodings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06212">http://arxiv.org/abs/2311.06212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drewrl3v/diff-vq-vae">https://github.com/drewrl3v/diff-vq-vae</a></li>
<li>paper_authors: Andrew Lizarraga, Brandon Taraku, Edouardo Honig, Ying Nian Wu, Shantanu H. Joshi</li>
<li>for: 这篇论文旨在提出一种新的差分可视化自适应网络，用于对白 matter 流线的复杂几何结构进行简化分析。</li>
<li>methods: 该论文使用了一种新的可 diferenciable vector quantized variational autoencoder（DVQ-VAE），可以同时处理整个纤维Bundle的流线数据，并提供可靠可信的编码。</li>
<li>results: 对比了多种现有的 autoencoder 方法，DVQ-VAE 显示出了更高的编码和重建性能。<details>
<summary>Abstract</summary>
Given the complex geometry of white matter streamlines, Autoencoders have been proposed as a dimension-reduction tool to simplify the analysis streamlines in a low-dimensional latent spaces. However, despite these recent successes, the majority of encoder architectures only perform dimension reduction on single streamlines as opposed to a full bundle of streamlines. This is a severe limitation of the encoder architecture that completely disregards the global geometric structure of streamlines at the expense of individual fibers. Moreover, the latent space may not be well structured which leads to doubt into their interpretability. In this paper we propose a novel Differentiable Vector Quantized Variational Autoencoder, which are engineered to ingest entire bundles of streamlines as single data-point and provides reliable trustworthy encodings that can then be later used to analyze streamlines in the latent space. Comparisons with several state of the art Autoencoders demonstrate superior performance in both encoding and synthesis.
</details>
<details>
<summary>摘要</summary>
giventext由于白质物流线的复杂几何结构，Autoencoder已经被提议作为一个简化分析的工具，以将流线简化到低维的隐藏空间中。然而，Despite these recent successes, the majority of encoder architectures only perform dimension reduction on single streamlines as opposed to a full bundle of streamlines。这是一个严重的encoder architecture limitation， completely disregards the global geometric structure of streamlines at the expense of individual fibers。Moreover, the latent space may not be well structured which leads to doubt into their interpretability。在这篇文章中，我们提出了一种新的可微分量化自适应器，可以读取整个组合的流线作为单一数据点，并提供可靠可信的编码，可以用来分析流线在隐藏空间中。与多个现有Autoencoder进行比较，我们的方法具有较高的编码和合成性能。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Cooperative-Multiplayer-Learning-Bandits-with-Noisy-Rewards-and-No-Communication"><a href="#Optimal-Cooperative-Multiplayer-Learning-Bandits-with-Noisy-Rewards-and-No-Communication" class="headerlink" title="Optimal Cooperative Multiplayer Learning Bandits with Noisy Rewards and No Communication"></a>Optimal Cooperative Multiplayer Learning Bandits with Noisy Rewards and No Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06210">http://arxiv.org/abs/2311.06210</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Chang, Yuanhao Lu</li>
<li>for: 这篇论文是关于协作多 иг户带刺游戏学习问题的研究，具体来说是每个玩家只能在学习过程中达成协议，但是在学习过程中不能交流。</li>
<li>methods: 该论文提出了一种基于上下界信息的算法，使得玩家可以尽可能地选择最佳动作，即使在奖励信息各自不同时。</li>
<li>results: 该论文显示了这种算法可以在不同奖励信息的情况下实现对数($O(\frac{\log T}{\Delta_{\bm{a}})$)的追悟 regret，以及$O(\sqrt{T\log T})$的追悟 regret，这两者都是对数函数。此外，该算法在实际中也比现有的算法表现更好。<details>
<summary>Abstract</summary>
We consider a cooperative multiplayer bandit learning problem where the players are only allowed to agree on a strategy beforehand, but cannot communicate during the learning process. In this problem, each player simultaneously selects an action. Based on the actions selected by all players, the team of players receives a reward. The actions of all the players are commonly observed. However, each player receives a noisy version of the reward which cannot be shared with other players. Since players receive potentially different rewards, there is an asymmetry in the information used to select their actions. In this paper, we provide an algorithm based on upper and lower confidence bounds that the players can use to select their optimal actions despite the asymmetry in the reward information. We show that this algorithm can achieve logarithmic $O(\frac{\log T}{\Delta_{\bm{a}})$ (gap-dependent) regret as well as $O(\sqrt{T\log T})$ (gap-independent) regret. This is asymptotically optimal in $T$. We also show that it performs empirically better than the current state of the art algorithm for this environment.
</details>
<details>
<summary>摘要</summary>
我们考虑了合作多player带狗学习问题，其中玩家只能在进程前合作确定策略，但在学习过程中不能交流。在这个问题中，每个玩家同时选择动作，基于所有玩家选择的动作，团队的玩家收到奖励。但是，每个玩家只能看到自己的奖励，其他玩家的奖励是干扰的。由于玩家收到的奖励可能不同，因此存在 asymmetry 在奖励信息中。在这篇论文中，我们提供了基于上下界的 confidence bounds 算法，allowing players to select their optimal actions despite the asymmetry in the reward information. We show that this algorithm can achieve logarithmic $O(\frac{\log T}{\Delta_{\bm{a}})$ (gap-dependent) regret as well as $O(\sqrt{T\log T})$ (gap-independent) regret. This is asymptotically optimal in $T$. We also show that it performs empirically better than the current state of the art algorithm for this environment.
</details></li>
</ul>
<hr>
<h2 id="Time-Scale-Network-A-Shallow-Neural-Network-For-Time-Series-Data"><a href="#Time-Scale-Network-A-Shallow-Neural-Network-For-Time-Series-Data" class="headerlink" title="Time Scale Network: A Shallow Neural Network For Time Series Data"></a>Time Scale Network: A Shallow Neural Network For Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06170">http://arxiv.org/abs/2311.06170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Trevor Meyer, Camden Shultz, Najim Dehak, Laureano Moro-Velazquez, Pedro Irazoqui</li>
<li>for: 这个研究旨在开发一个具有最小化计算量和资料需求的深度学习网络，用于处理具有多个时间尺度的时间序列数据。</li>
<li>methods: 这个研究使用了组合了时间尺度译送和扩展序列的时间尺度网络，与传统的卷积神经网络和反向传播一起使用。这个网络可以同时学习多个时间尺度的特征，并且具有较少的参数和运算量。</li>
<li>results: 这个研究的结果显示，这个时间尺度网络可以在杜立特证明和血液律异常检测中表现出色，其中包括高精度、快速训练和测试速度、以及可视化和解释学习的特征模式。此外，这个网络也在脑电律异常预测中获得了出色的表现。<details>
<summary>Abstract</summary>
Time series data is often composed of information at multiple time scales, particularly in biomedical data. While numerous deep learning strategies exist to capture this information, many make networks larger, require more data, are more demanding to compute, and are difficult to interpret. This limits their usefulness in real-world applications facing even modest computational or data constraints and can further complicate their translation into practice. We present a minimal, computationally efficient Time Scale Network combining the translation and dilation sequence used in discrete wavelet transforms with traditional convolutional neural networks and back-propagation. The network simultaneously learns features at many time scales for sequence classification with significantly reduced parameters and operations. We demonstrate advantages in Atrial Dysfunction detection including: superior accuracy-per-parameter and accuracy-per-operation, fast training and inference speeds, and visualization and interpretation of learned patterns in atrial dysfunction detection on ECG signals. We also demonstrate impressive performance in seizure prediction using EEG signals. Our network isolated a few time scales that could be strategically selected to achieve 90.9% accuracy using only 1,133 active parameters and consistently converged on pulsatile waveform shapes. This method does not rest on any constraints or assumptions regarding signal content and could be leveraged in any area of time series analysis dealing with signals containing features at many time scales.
</details>
<details>
<summary>摘要</summary>
时序数据经常具有多个时间尺度信息，特别是在生物医学数据中。虽然有许多深度学习策略可以捕捉这些信息，但是 многие网络变得更大、需要更多的数据、更复杂的计算和更难于解释。这限制了它们在实际应用中的使用，特别是面临有限的计算和数据约束。我们提出了一种简单、计算效率高的时间尺度网络，将翻译和扩展序列使用在离散干扰变换中的Sequence Network与传统的卷积神经网络和反射传播结合。该网络同时学习多个时间尺度的特征，用于序列分类，而无需增加过多的参数和运算。我们在心脏病变诊断中demonstrated出了superior的准确率-参数和运算量，快速的训练和推理速度，以及序列分类结果的可视化和解释。此外，我们还在EEG信号上进行了抑制预测，并达到了90.9%的准确率，只使用1,133个活动参数。这种方法不受任何信号内容的限制，可以在任何时序分析领域中应用，特别是面临着包含多个时间尺度的信号。
</details></li>
</ul>
<hr>
<h2 id="Surrogate-Neural-Networks-to-Estimate-Parametric-Sensitivity-of-Ocean-Models"><a href="#Surrogate-Neural-Networks-to-Estimate-Parametric-Sensitivity-of-Ocean-Models" class="headerlink" title="Surrogate Neural Networks to Estimate Parametric Sensitivity of Ocean Models"></a>Surrogate Neural Networks to Estimate Parametric Sensitivity of Ocean Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08421">http://arxiv.org/abs/2311.08421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixuan Sun, Elizabeth Cucuzzella, Steven Brus, Sri Hari Krishna Narayanan, Balu Nadiga, Luke Van Roekel, Jan Hückelheim, Sandeep Madireddy</li>
<li>for: 研究气候变化和海洋相互作用的影响</li>
<li>methods: 使用神经网络模型和参数推定法</li>
<li>results: 模型输出的参数敏感性分析Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to study the impact of greenhouse gases, warming, and ice sheet melting on the ocean, as well as the effects of ocean processes on phenomena such as hurricanes and droughts.</li>
<li>methods: The authors use a combination of idealized ocean models, perturbed parameter ensemble data, and surrogate neural network models to analyze the sensitivity of the model output to unmeasurable parameters.</li>
<li>results: The authors compute the parametric sensitivity of the one-step forward dynamics of the model, providing insights into the impact of unmeasurable parameters on the model output.<details>
<summary>Abstract</summary>
Modeling is crucial to understanding the effect of greenhouse gases, warming, and ice sheet melting on the ocean. At the same time, ocean processes affect phenomena such as hurricanes and droughts. Parameters in the models that cannot be physically measured have a significant effect on the model output. For an idealized ocean model, we generated perturbed parameter ensemble data and trained surrogate neural network models. The neural surrogates accurately predicted the one-step forward dynamics, of which we then computed the parametric sensitivity.
</details>
<details>
<summary>摘要</summary>
模拟是理解绿色气体、暖化和冰川融化对海洋的效应的关键。同时，海洋过程对风暴和干旱等现象产生了影响。模型中无法测量的参数会对模型输出产生重要影响。为一个理想化的海洋模型，我们生成了受扰参数数据集和训练了神经网络模型。神经网络模型准确预测了下一步动力学行为，我们 THEN 计算了参数敏感度。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Graph-Anomaly-Detection-using-Gradient-Attention-Maps"><a href="#Interpretable-Graph-Anomaly-Detection-using-Gradient-Attention-Maps" class="headerlink" title="Interpretable Graph Anomaly Detection using Gradient Attention Maps"></a>Interpretable Graph Anomaly Detection using Gradient Attention Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06153">http://arxiv.org/abs/2311.06153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Yang, Peng Wang, Xiaofan He, Dongmian Zou</li>
<li>for: 本文旨在提出一种基于可解释性的图像异常检测方法，以提高异常检测性能。</li>
<li>methods: 本方法使用图神经网络的梯度来生成注意力地图，并使用这个地图来评分异常。</li>
<li>results: 对比基eline方法，本方法在多个synthetic数据集上表现出色，并且可以帮助我们更好地理解异常检测决策的过程。<details>
<summary>Abstract</summary>
Detecting unusual patterns in graph data is a crucial task in data mining. However, existing methods often face challenges in consistently achieving satisfactory performance and lack interpretability, which hinders our understanding of anomaly detection decisions. In this paper, we propose a novel approach to graph anomaly detection that leverages the power of interpretability to enhance performance. Specifically, our method extracts an attention map derived from gradients of graph neural networks, which serves as a basis for scoring anomalies. In addition, we conduct theoretical analysis using synthetic data to validate our method and gain insights into its decision-making process. To demonstrate the effectiveness of our method, we extensively evaluate our approach against state-of-the-art graph anomaly detection techniques. The results consistently demonstrate the superior performance of our method compared to the baselines.
</details>
<details>
<summary>摘要</summary>
检测图形数据中异常 Pattern 是数据挖掘中的一项关键任务。然而，现有的方法经常遇到一些挑战，包括困难保证满意的性能和缺乏可解性，这些缺陷限制了我们对异常检测决策的理解。在这篇论文中，我们提出了一种新的图形异常检测方法，该方法利用可解性来提高性能。具体来说，我们的方法利用图形神经网络的梯度导数来生成一个注意力地图，该地图作为异常分数的基础。此外，我们使用 sintetic data 进行理论分析，以获得我们的方法做出异常检测决策的理解。为了证明我们的方法的有效性，我们对比了我们的方法与当前最佳的图形异常检测技术。结果一致地表明了我们的方法与基eline相比具有更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Minimum-norm-interpolation-by-perceptra-Explicit-regularization-and-implicit-bias"><a href="#Minimum-norm-interpolation-by-perceptra-Explicit-regularization-and-implicit-bias" class="headerlink" title="Minimum norm interpolation by perceptra: Explicit regularization and implicit bias"></a>Minimum norm interpolation by perceptra: Explicit regularization and implicit bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06138">http://arxiv.org/abs/2311.06138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiyoung Park, Ian Pelakh, Stephan Wojtowytsch</li>
<li>for: 研究如何 shallow ReLU 网络在知道区域内 interpolate.</li>
<li>methods: 我们的分析表明，当数据点和参数的数量增加，并且权重 decay 正则化的系数逐渐减少时，Empirical risk minimizers 会 converge to a minimum norm interpolant.</li>
<li>results: 我们的numerical研究表明，通用优化算法对known minimum norm interpolants具有隐式偏好，无论有没有显式正则化。<details>
<summary>Abstract</summary>
We investigate how shallow ReLU networks interpolate between known regions. Our analysis shows that empirical risk minimizers converge to a minimum norm interpolant as the number of data points and parameters tends to infinity when a weight decay regularizer is penalized with a coefficient which vanishes at a precise rate as the network width and the number of data points grow. With and without explicit regularization, we numerically study the implicit bias of common optimization algorithms towards known minimum norm interpolants.
</details>
<details>
<summary>摘要</summary>
我们调查如何使浅层ReLU网络在已知区域中进行 interpolating。我们的分析表明，在数据点和参数数量增加时，empirical risk minimizers会趋向 minimum norm interpolant 的最小norm的架构，并且随着网络宽度和数据点数量增加，该 coefficient 会逐渐消失。在有Explicit regularization和无Explicit regularization的情况下，我们 numerically 研究了通用优化算法对于已知 minimum norm interpolants 的隐藏偏见。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Skeleton-Learning-of-Discrete-Bayesian-Networks"><a href="#Distributionally-Robust-Skeleton-Learning-of-Discrete-Bayesian-Networks" class="headerlink" title="Distributionally Robust Skeleton Learning of Discrete Bayesian Networks"></a>Distributionally Robust Skeleton Learning of Discrete Bayesian Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06117">http://arxiv.org/abs/2311.06117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danielleee/drslbn">https://github.com/danielleee/drslbn</a></li>
<li>paper_authors: Yeshu Li, Brian D. Ziebart</li>
<li>for: 学习普遍 discrete Bayesian networks 的准确骨架（skeleton）。</li>
<li>methods: 利用分布性robust优化和回归方法，最大化最差风险（worst-case risk）在 Family of Distributions 内的 bounded Wasserstein distance 或 KL divergence 到 empirical distribution。</li>
<li>results: 提出了一种可以应用于普遍 categorical random variables 的方法，不需要 faithfulness、ordinal relationship 或 specific conditional distribution 假设。 提供了高效的算法，并在轻度假设下提供了非 asymptotic 保证。 数值研究表明方法的有效性。 Code 可以在 <a target="_blank" rel="noopener" href="https://github.com/DanielLeee/drslbn">https://github.com/DanielLeee/drslbn</a> 找到。<details>
<summary>Abstract</summary>
We consider the problem of learning the exact skeleton of general discrete Bayesian networks from potentially corrupted data. Building on distributionally robust optimization and a regression approach, we propose to optimize the most adverse risk over a family of distributions within bounded Wasserstein distance or KL divergence to the empirical distribution. The worst-case risk accounts for the effect of outliers. The proposed approach applies for general categorical random variables without assuming faithfulness, an ordinal relationship or a specific form of conditional distribution. We present efficient algorithms and show the proposed methods are closely related to the standard regularized regression approach. Under mild assumptions, we derive non-asymptotic guarantees for successful structure learning with logarithmic sample complexities for bounded-degree graphs. Numerical study on synthetic and real datasets validates the effectiveness of our method. Code is available at https://github.com/DanielLeee/drslbn.
</details>
<details>
<summary>摘要</summary>
我们考虑一个统计学上的问题，即从潜在损害的数据中学习一般化的抽象骨架。我们基于分布式弹性优化和回归方法，提出一个优化最坏风险的方法，其中最坏风险是指在一家族中的分布体内的最大差距或KL散度与empirical分布之间的最大差距。这个风险考虑到噪音的影响。我们的方法适用于一般的分类随机Variable而无需假设忠诚、排序关系或具体的假设。我们提供高效的算法和证明在对��� bounded-degree graph的简单假设下，我们可以获得非对数� Spark complexity的成功结构学。我们的方法与标准的规制化回归方法密切相关。我们的方法在实验中证明了有效。code可以在https://github.com/DanielLeee/drslbn中找到。
</details></li>
</ul>
<hr>
<h2 id="Turbulence-Scaling-from-Deep-Learning-Diffusion-Generative-Models"><a href="#Turbulence-Scaling-from-Deep-Learning-Diffusion-Generative-Models" class="headerlink" title="Turbulence Scaling from Deep Learning Diffusion Generative Models"></a>Turbulence Scaling from Deep Learning Diffusion Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06112">http://arxiv.org/abs/2311.06112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Whittaker, Romuald A. Janik, Yaron Oz</li>
<li>for: 本研究旨在捕捉流体动力学中的复杂空间和时间结构，并对其进行数学模拟。</li>
<li>methods: 本研究使用了一种扩散基于的生成模型，来学习液体动力学中的扭轴 profiles的分布，并生成了不同于训练数据集的液体动力学解。</li>
<li>results: 研究发现，新生成的液体动力学解具有与预期的科尔мого罗夫 scaling 相同的统计尺度 Properties，并且比训练数据集的统计尺度更加精度。这种与实际液体动力学特征相符的表现，提供了模型能够捕捉实际液体动力学特征的强有力证据。<details>
<summary>Abstract</summary>
Complex spatial and temporal structures are inherent characteristics of turbulent fluid flows and comprehending them poses a major challenge. This comprehesion necessitates an understanding of the space of turbulent fluid flow configurations. We employ a diffusion-based generative model to learn the distribution of turbulent vorticity profiles and generate snapshots of turbulent solutions to the incompressible Navier-Stokes equations. We consider the inverse cascade in two spatial dimensions and generate diverse turbulent solutions that differ from those in the training dataset. We analyze the statistical scaling properties of the new turbulent profiles, calculate their structure functions, energy power spectrum, velocity probability distribution function and moments of local energy dissipation. All the learnt scaling exponents are consistent with the expected Kolmogorov scaling and have lower errors than the training ones. This agreement with established turbulence characteristics provides strong evidence of the model's capability to capture essential features of real-world turbulence.
</details>
<details>
<summary>摘要</summary>
困难的空间和时间结构是液体动力学中抽象流动的内在特征，理解这些特征是很重要的。我们使用一种扩散基于的生成模型来学习液体动力学中抽象扩散的分布，并生成了不同于训练数据集的液体动力学解。我们在两维空间中考虑逆升阶段，并生成了多种不同的液体动力学解，与训练数据集的解不同。我们分析了新的液体动力学Profile的统计尺度性质，计算了其结构函数、能量频谱、速度分布函数和本地能量投入的积分。所学到的扩散 exponent都与预期的科尔莫戈罗夫 scaling 相符，并且与训练数据集中的 exponent 有更低的错误。这种一致性提供了强有力的证据，证明了模型能够捕捉真实世界中的液体动力学特征。
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-Machine-Learning-Framework-to-Understand-Bikeshare-Demand-before-and-during-the-COVID-19-Pandemic-in-New-York-City"><a href="#An-Interpretable-Machine-Learning-Framework-to-Understand-Bikeshare-Demand-before-and-during-the-COVID-19-Pandemic-in-New-York-City" class="headerlink" title="An Interpretable Machine Learning Framework to Understand Bikeshare Demand before and during the COVID-19 Pandemic in New York City"></a>An Interpretable Machine Learning Framework to Understand Bikeshare Demand before and during the COVID-19 Pandemic in New York City</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06110">http://arxiv.org/abs/2311.06110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Majbah Uddin, Ho-Ling Hwang, Md Sami Hasnine</li>
<li>for: 这个研究旨在提出一个机器学习模型框架，以估计大规模自行车共享系统的每小时需求。</li>
<li>methods: 本研究使用了两个极端Gradient Boosting模型：一个使用了2019年3月至2020年2月的数据（以前 COVID-19 大流行），另一个使用了2020年3月至2021年2月的数据（ durante COVID-19 大流行）。此外，还实现了一个基于 SHapley Additive exPlanations 的模型解释框架。</li>
<li>results: 根据这个研究中考虑的说明变数的相对重要性，女性用户占有和小时是这两个模型中最重要的变数。然而，月份变数在大流行模型中比在以前模型中更重要。<details>
<summary>Abstract</summary>
In recent years, bikesharing systems have become increasingly popular as affordable and sustainable micromobility solutions. Advanced mathematical models such as machine learning are required to generate good forecasts for bikeshare demand. To this end, this study proposes a machine learning modeling framework to estimate hourly demand in a large-scale bikesharing system. Two Extreme Gradient Boosting models were developed: one using data from before the COVID-19 pandemic (March 2019 to February 2020) and the other using data from during the pandemic (March 2020 to February 2021). Furthermore, a model interpretation framework based on SHapley Additive exPlanations was implemented. Based on the relative importance of the explanatory variables considered in this study, share of female users and hour of day were the two most important explanatory variables in both models. However, the month variable had higher importance in the pandemic model than in the pre-pandemic model.
</details>
<details>
<summary>摘要</summary>
Recently, 自行车共享系统已经成为非常受欢迎的可靠和可持续的微型交通解决方案。为了生成好的预测模型，这些研究需要进行高级的数据分析和机器学习模型。为此，本研究提出了一个机器学习模型框架，用于估计大规模自行车共享系统的每小时需求。这些研究发展了两个极大Gradient Boosting模型：一个使用2019年3月至2020年2月的数据（前疫情时期），另一个使用2020年3月至2021年2月的数据（疫情时期）。此外，基于SHapley Additive exPlanations的模型解释框架也被实现。根据这些研究中考虑的说明变量的相对重要性，女性用户的份额和时间段是这两个模型中最重要的说明变量。但是，月份变量在疫情模型中比前疫情模型更重要。
</details></li>
</ul>
<hr>
<h2 id="1-Lipschitz-Neural-Networks-are-more-expressive-with-N-Activations"><a href="#1-Lipschitz-Neural-Networks-are-more-expressive-with-N-Activations" class="headerlink" title="1-Lipschitz Neural Networks are more expressive with N-Activations"></a>1-Lipschitz Neural Networks are more expressive with N-Activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06103">http://arxiv.org/abs/2311.06103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berndprach/nactivation">https://github.com/berndprach/nactivation</a></li>
<li>paper_authors: Bernd Prach, Christoph H. Lampert</li>
<li>for: 该论文旨在构建可靠、可信任和可解释的深度学习系统，以确保小的输入变化不会导致大的输出变化。</li>
<li>methods: 论文使用了一些新的activation function来提高深度学习系统的表达能力和稳定性。</li>
<li>results: 论文表明，常用的activation function，如MaxMin，以及所有的二段折线activation function都过于限制了函数的表达能力，即使在 simplest一dimensional setting中。它们还引入了一种新的N-activation function，可以更好地表达函数。<details>
<summary>Abstract</summary>
A crucial property for achieving secure, trustworthy and interpretable deep learning systems is their robustness: small changes to a system's inputs should not result in large changes to its outputs. Mathematically, this means one strives for networks with a small Lipschitz constant. Several recent works have focused on how to construct such Lipschitz networks, typically by imposing constraints on the weight matrices. In this work, we study an orthogonal aspect, namely the role of the activation function. We show that commonly used activation functions, such as MaxMin, as well as all piece-wise linear ones with two segments unnecessarily restrict the class of representable functions, even in the simplest one-dimensional setting. We furthermore introduce the new N-activation function that is provably more expressive than currently popular activation functions. We provide code at https://github.com/berndprach/NActivation.
</details>
<details>
<summary>摘要</summary>
一个深度学习系统的关键性能特性是其Robustness：小改变输入 shouldn't result in large changes to its outputs. 数学上，这意味着一个网络的 lipschitz常数应该小。 一些最近的工作已经关注如何构建这样的 lipschitz 网络，通常是通过加载矩阵的约束。在这个工作中，我们研究了另一个正交方面，即激活函数的角色。我们显示，通用的激活函数，如 MaxMin，以及所有分割线性的两段激活函数都过于限制了可表示的函数的类型，即使在最简单的一维设定中。我们还引入了新的 N-激活函数，可以证明比现有的激活函数更加表达力强。我们提供了相关代码在 GitHub 上：https://github.com/berndprach/NActivation。
</details></li>
</ul>
<hr>
<h2 id="Symbolic-Regression-as-Feature-Engineering-Method-for-Machine-and-Deep-Learning-Regression-Tasks"><a href="#Symbolic-Regression-as-Feature-Engineering-Method-for-Machine-and-Deep-Learning-Regression-Tasks" class="headerlink" title="Symbolic Regression as Feature Engineering Method for Machine and Deep Learning Regression Tasks"></a>Symbolic Regression as Feature Engineering Method for Machine and Deep Learning Regression Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06028">http://arxiv.org/abs/2311.06028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AssafS91/Symbolic-Regression-as-Feature-Engineering-Method-for-Machine-and-Deep-Learning-Regression-Tasks">https://github.com/AssafS91/Symbolic-Regression-as-Feature-Engineering-Method-for-Machine-and-Deep-Learning-Regression-Tasks</a></li>
<li>paper_authors: Assaf Shmuel, Oren Glickman, Teddy Lazebnik</li>
<li>for: 提高机器学习和深度学习回归模型的性能</li>
<li>methods: 结合符号回归（SR）作为特征工程（FE）过程，以提高机器学习和深度学习回归模型的预测能力</li>
<li>results: SR-derived features可以帮助提高机器学习和深度学习回归模型的预测精度，实验结果显示SR可以提高模型的root mean square error（RMSE）值34-86%，并在实际应用中提高预测超导温度的准确率。<details>
<summary>Abstract</summary>
In the realm of machine and deep learning regression tasks, the role of effective feature engineering (FE) is pivotal in enhancing model performance. Traditional approaches of FE often rely on domain expertise to manually design features for machine learning models. In the context of deep learning models, the FE is embedded in the neural network's architecture, making it hard for interpretation. In this study, we propose to integrate symbolic regression (SR) as an FE process before a machine learning model to improve its performance. We show, through extensive experimentation on synthetic and real-world physics-related datasets, that the incorporation of SR-derived features significantly enhances the predictive capabilities of both machine and deep learning regression models with 34-86% root mean square error (RMSE) improvement in synthetic datasets and 4-11.5% improvement in real-world datasets. In addition, as a realistic use-case, we show the proposed method improves the machine learning performance in predicting superconducting critical temperatures based on Eliashberg theory by more than 20% in terms of RMSE. These results outline the potential of SR as an FE component in data-driven models.
</details>
<details>
<summary>摘要</summary>
在机器学习和深度学习回归任务中，有效的特征工程（FE）角色是关键的提高模型性能。传统的FE方法通常依赖于领域专家手动设计机器学习模型的特征。在深度学习模型中，FE是内置在神经网络结构中，使其解释性困难。在这项研究中，我们提议将符号回归（SR）作为FE过程来改进机器学习模型的性能。我们通过对 sintetic和实际物理相关数据集进行广泛的实验，发现SR derivated特征的 integrate 可以显著提高机器学习和深度学习回归模型的预测能力，具体来说，在 sintetic 数据集中，RMSE 下降了34-86%，而在实际数据集中，RMSE 下降了4-11.5%。此外，我们还展示了该方法可以在预测超导极限温度基于Eliashberg理论中提高机器学习性能，具体来说，RMSE 下降了 más de 20%。这些结果表明SR 可以作为数据驱动模型中的FE组件。
</details></li>
</ul>
<hr>
<h2 id="Doubly-Robust-Structure-Identification-from-Temporal-Data"><a href="#Doubly-Robust-Structure-Identification-from-Temporal-Data" class="headerlink" title="Doubly Robust Structure Identification from Temporal Data"></a>Doubly Robust Structure Identification from Temporal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06012">http://arxiv.org/abs/2311.06012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanouil Angelis, Francesco Quinzan, Ashkan Soleymani, Patrick Jaillet, Stefan Bauer</li>
<li>for: 本研究旨在解释时间序列数据中的原因，它是许多应用领域的基本任务，从金融到地球科学或生物医学应用。</li>
<li>methods: 我们提出了一种新的两阶段强制方法，即时间数据结构鉴别法（SITD），该方法可以抗衡噪声和循环性数据。我们提供了理论保证，表明我们的方法可以很好地收回真实的下游 causal 结构。</li>
<li>results: 我们的实验结果表明，我们的方法在噪声和循环性数据情况下具有明显的优势，并且可以很好地鉴别出真实的原因结构。<details>
<summary>Abstract</summary>
Learning the causes of time-series data is a fundamental task in many applications, spanning from finance to earth sciences or bio-medical applications. Common approaches for this task are based on vector auto-regression, and they do not take into account unknown confounding between potential causes. However, in settings with many potential causes and noisy data, these approaches may be substantially biased. Furthermore, potential causes may be correlated in practical applications. Moreover, existing algorithms often do not work with cyclic data. To address these challenges, we propose a new doubly robust method for Structure Identification from Temporal Data ( SITD ). We provide theoretical guarantees, showing that our method asymptotically recovers the true underlying causal structure. Our analysis extends to cases where the potential causes have cycles and they may be confounded. We further perform extensive experiments to showcase the superior performance of our method.
</details>
<details>
<summary>摘要</summary>
学习时序数据的原因是许多应用程序的基本任务，从金融到地球科学或生物医学应用程序。常见的方法基于向量自动回归，但这些方法不考虑可能存在的隐藏干扰因素。在具有多个可能的原因和噪声数据的情况下，这些方法可能受到重大偏误。此外，实际应用中的原因可能相互 correlated。此外，现有的算法通常不能处理循环数据。为解决这些挑战，我们提出了一种新的双重可靠方法 для时间数据结构鉴别（SITD）。我们提供了理论保证，表明我们的方法在极限情况下可以准确回归真实的下面结构。我们的分析涵盖了可能存在循环的原因，以及它们可能受到干扰的情况。我们进一步进行了广泛的实验，以示出我们的方法的超过其他方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Graph-GOSPA-metric-a-metric-to-measure-the-discrepancy-between-graphs-of-different-sizes"><a href="#Graph-GOSPA-metric-a-metric-to-measure-the-discrepancy-between-graphs-of-different-sizes" class="headerlink" title="Graph GOSPA metric: a metric to measure the discrepancy between graphs of different sizes"></a>Graph GOSPA metric: a metric to measure the discrepancy between graphs of different sizes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07596">http://arxiv.org/abs/2311.07596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinhao Gu, Ángel F. García-Fernández, Robert E. Firth, Lennart Svensson</li>
<li>for: This paper proposes a metric to measure the dissimilarity between graphs with different numbers of nodes.</li>
<li>methods: The proposed metric extends the generalised optimal subpattern assignment (GOSPA) metric for sets to graphs, and includes costs associated with node attribute errors, missed and false nodes, and edge mismatches between graphs.</li>
<li>results: The metric is computable in polynomial time using linear programming, and its properties are demonstrated via simulated and empirical datasets.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文提出了一个度量图像之间的不同程度的 metric。</li>
<li>methods: 该metric基于将Optimal Subpattern Assignment (GOSPA)metric для集合扩展到图像，并包括节点属性错误的成本、缺失和false节点的成本以及图像之间的边匹配错误的成本。</li>
<li>results: 该metric可以使用线性 программирова来计算，并通过验证模拟和实验数据显示了其性质。<details>
<summary>Abstract</summary>
This paper proposes a metric to measure the dissimilarity between graphs that may have a different number of nodes. The proposed metric extends the generalised optimal subpattern assignment (GOSPA) metric, which is a metric for sets, to graphs. The proposed graph GOSPA metric includes costs associated with node attribute errors for properly assigned nodes, missed and false nodes and edge mismatches between graphs. The computation of this metric is based on finding the optimal assignments between nodes in the two graphs, with the possibility of leaving some of the nodes unassigned. We also propose a lower bound for the metric, which is also a metric for graphs and is computable in polynomial time using linear programming. The metric is first derived for undirected unweighted graphs and it is then extended to directed and weighted graphs. The properties of the metric are demonstrated via simulated and empirical datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Sum-max-Submodular-Bandits"><a href="#Sum-max-Submodular-Bandits" class="headerlink" title="Sum-max Submodular Bandits"></a>Sum-max Submodular Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05975">http://arxiv.org/abs/2311.05975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Pasteris, Alberto Rumi, Fabio Vitale, Nicolò Cesa-Bianchi</li>
<li>for: 这个论文主要针对的是在线决策问题，具体来说是最大化一序列的半模式函数。</li>
<li>methods: 这篇论文提出了一种新的函数类型—半模式函数的子类—称为总最大函数，它包括了一些有趣的问题，如最优-$K$-投票、 combinatorial投票、投票版本的设施位置、$M$-中心、击中集。</li>
<li>results: 论文证明了这些函数在非随机 Setting 中的带有反馈的情况下，可以达到$(1 - \frac{1}{e})$ regret bound，其bound 为 $\sqrt{MKT}$（忽略对数因子），其中 $T$ 是时间戳和 $M$ 是 Cardinality 约束。这个 bound 胜过了在线半模式函数最大化的 $\widetilde{O}(T^{2&#x2F;3})$ regret bound。<details>
<summary>Abstract</summary>
Many online decision-making problems correspond to maximizing a sequence of submodular functions. In this work, we introduce sum-max functions, a subclass of monotone submodular functions capturing several interesting problems, including best-of-$K$-bandits, combinatorial bandits, and the bandit versions on facility location, $M$-medians, and hitting sets. We show that all functions in this class satisfy a key property that we call pseudo-concavity. This allows us to prove $\big(1 - \frac{1}{e}\big)$-regret bounds for bandit feedback in the nonstochastic setting of the order of $\sqrt{MKT}$ (ignoring log factors), where $T$ is the time horizon and $M$ is a cardinality constraint. This bound, attained by a simple and efficient algorithm, significantly improves on the $\widetilde{O}\big(T^{2/3}\big)$ regret bound for online monotone submodular maximization with bandit feedback.
</details>
<details>
<summary>摘要</summary>
多个在线决策问题都对应于最大化一个序列的准确函数。在这项工作中，我们介绍了总最大函数，它是准确函数的一个子类，捕捉了许多有趣的问题，包括最佳-$K$-投降、组合投降、投降版本的设施位置、$M$-中心和击中集。我们证明了这些函数都满足一个关键性的pseudo-凹性性质，这使得我们可以证明在非随机设定下的递归约束下， regret bound为$\big(1 - \frac{1}{e}\big)$（忽略log因子），其值在$\sqrt{MKT}$之间。这个 bound是由一种简单和高效的算法实现，与之前的 $\widetilde{O}\big(T^{2/3}\big)$ regret bound相比，有所改善。
</details></li>
</ul>
<hr>
<h2 id="Plasma-Surrogate-Modelling-using-Fourier-Neural-Operators"><a href="#Plasma-Surrogate-Modelling-using-Fourier-Neural-Operators" class="headerlink" title="Plasma Surrogate Modelling using Fourier Neural Operators"></a>Plasma Surrogate Modelling using Fourier Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05967">http://arxiv.org/abs/2311.05967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, Daniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, Marc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team</li>
<li>for: 预测托卡马克反应器中束激发的演化是实现持续同步聚变的关键。快速和准确地预测束激发的空间时间演化，允许我们快速迭代设计和控制策略，以提高现有托卡马克设备和未来反应器的性能。</li>
<li>methods: 我们使用深度学习基于 Fourier Neural Operators（FNO）来建立廉价的代理模型，以提高预测束激发的效率。FNO具有六个数量级的速度增加，而且可以保持高精度（MSE $\approx$ $10^{-5}$）。我们的修改后的FNO可以解决多变量partial differential equations（PDE），并能够捕捉不同变量之间的相互关系。</li>
<li>results: FNO可以准确预测束激发的发展，并在实验域中预测实际观测数据。我们在MAST托卡马克实验室中使用摄像头记录束激发的发展，并发现FNO可以准确预测束激发的发展和形状，以及束激发与中央气流和束激发器的互动的位置。FNO具有快速训练和推理，需要 fewer data points，可以完成零射播超解析，并且能够获得高精度解决方案。<details>
<summary>Abstract</summary>
Predicting plasma evolution within a Tokamak reactor is crucial to realizing the goal of sustainable fusion. Capabilities in forecasting the spatio-temporal evolution of plasma rapidly and accurately allow us to quickly iterate over design and control strategies on current Tokamak devices and future reactors. Modelling plasma evolution using numerical solvers is often expensive, consuming many hours on supercomputers, and hence, we need alternative inexpensive surrogate models. We demonstrate accurate predictions of plasma evolution both in simulation and experimental domains using deep learning-based surrogate modelling tools, viz., Fourier Neural Operators (FNO). We show that FNO has a speedup of six orders of magnitude over traditional solvers in predicting the plasma dynamics simulated from magnetohydrodynamic models, while maintaining a high accuracy (MSE $\approx$ $10^{-5}$). Our modified version of the FNO is capable of solving multi-variable Partial Differential Equations (PDE), and can capture the dependence among the different variables in a single model. FNOs can also predict plasma evolution on real-world experimental data observed by the cameras positioned within the MAST Tokamak, i.e., cameras looking across the central solenoid and the divertor in the Tokamak. We show that FNOs are able to accurately forecast the evolution of plasma and have the potential to be deployed for real-time monitoring. We also illustrate their capability in forecasting the plasma shape, the locations of interactions of the plasma with the central solenoid and the divertor for the full duration of the plasma shot within MAST. The FNO offers a viable alternative for surrogate modelling as it is quick to train and infer, and requires fewer data points, while being able to do zero-shot super-resolution and getting high-fidelity solutions.
</details>
<details>
<summary>摘要</summary>
预测tokamak激光器中激液的发展是实现可持续核聚合的关键。我们需要快速和准确地预测激液的空间时间发展，以便快速迭代设计和控制策略。 numerically solving plasma evolution models is often expensive and time-consuming, so we need inexpensive surrogate models. We demonstrate accurate predictions of plasma evolution using deep learning-based surrogate modeling tools, specifically Fourier Neural Operators (FNO). FNO has a speedup of six orders of magnitude over traditional solvers, while maintaining a high accuracy (MSE $\approx$ $10^{-5}$). Our modified version of FNO can solve multi-variable partial differential equations (PDEs) and capture the dependence among variables in a single model. FNOs can also predict plasma evolution on real-world experimental data from cameras positioned within the MAST Tokamak, such as cameras looking across the central solenoid and the divertor. We show that FNOs can accurately forecast plasma evolution and have the potential to be deployed for real-time monitoring. Additionally, we demonstrate their capability in forecasting the plasma shape, the locations of interactions of the plasma with the central solenoid and the divertor for the full duration of the plasma shot within MAST. FNO offers a viable alternative for surrogate modeling as it is quick to train and infer, requires fewer data points, and can perform zero-shot super-resolution with high-fidelity solutions.
</details></li>
</ul>
<hr>
<h2 id="Multiscale-Neural-Operators-for-Solving-Time-Independent-PDEs"><a href="#Multiscale-Neural-Operators-for-Solving-Time-Independent-PDEs" class="headerlink" title="Multiscale Neural Operators for Solving Time-Independent PDEs"></a>Multiscale Neural Operators for Solving Time-Independent PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05964">http://arxiv.org/abs/2311.05964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/merantix-momentum/multiscale-pde-operators">https://github.com/merantix-momentum/multiscale-pde-operators</a></li>
<li>paper_authors: Winfried Ripken, Lisa Coiffard, Felix Pieper, Sebastian Dziadzio</li>
<li>for: 解决大型精度离散方程在数据驱动神经网络中的挑战。</li>
<li>methods: 提出了一种图rewiring技术，以增强神经网络的全球交互能力。</li>
<li>results: 实验结果显示，我们的GNN方法在不规则网格上实现了时间独立精度离散方程的新高度表现标准，而我们的图rewiring策略也提高了基线方法的表现，实现了一个任务中的状态之最。<details>
<summary>Abstract</summary>
Time-independent Partial Differential Equations (PDEs) on large meshes pose significant challenges for data-driven neural PDE solvers. We introduce a novel graph rewiring technique to tackle some of these challenges, such as aggregating information across scales and on irregular meshes. Our proposed approach bridges distant nodes, enhancing the global interaction capabilities of GNNs. Our experiments on three datasets reveal that GNN-based methods set new performance standards for time-independent PDEs on irregular meshes. Finally, we show that our graph rewiring strategy boosts the performance of baseline methods, achieving state-of-the-art results in one of the tasks.
</details>
<details>
<summary>摘要</summary>
时间独立的偏微分方程（PDEs）在大型网格上具有严重的挑战，尤其是 для数据驱动的神经偏微分方程解solvers。我们介绍了一种新的グラフ重络技术来解决一些这些挑战，例如在不同维度和不规则网格上聚合信息。我们的提议方法可以跨距离节点相互作用，提高全球几何网络（GNNs）的全球互动能力。我们的实验结果显示，GNN-based方法在三个数据集上设置了新的性能标准，并且在其中一个任务中，我们的グラフ重络策略提高了基准方法的性能，实现了最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-deep-learning-based-adaptive-time-stepping-scheme-for-multiscale-simulations"><a href="#Hierarchical-deep-learning-based-adaptive-time-stepping-scheme-for-multiscale-simulations" class="headerlink" title="Hierarchical deep learning-based adaptive time-stepping scheme for multiscale simulations"></a>Hierarchical deep learning-based adaptive time-stepping scheme for multiscale simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05961">http://arxiv.org/abs/2311.05961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danishraf32/adaptive-hits">https://github.com/danishraf32/adaptive-hits</a></li>
<li>paper_authors: Asif Hamid, Danish Rafiq, Shahkar Ahmad Nahvi, Mohammad Abid Bazaz</li>
<li>for: 这篇研究是为了解决复杂非线性系统中的多尺度问题。</li>
<li>methods: 这篇研究提出了一种使用深度神经网络来解决多尺度问题的新方法。</li>
<li>results: 这篇研究获得了比固定步骤神经网络解析器更好的性能，并且在计算时间上降低了比例。<details>
<summary>Abstract</summary>
Multiscale is a hallmark feature of complex nonlinear systems. While the simulation using the classical numerical methods is restricted by the local \textit{Taylor} series constraints, the multiscale techniques are often limited by finding heuristic closures. This study proposes a new method for simulating multiscale problems using deep neural networks. By leveraging the hierarchical learning of neural network time steppers, the method adapts time steps to approximate dynamical system flow maps across timescales. This approach achieves state-of-the-art performance in less computational time compared to fixed-step neural network solvers. The proposed method is demonstrated on several nonlinear dynamical systems, and source codes are provided for implementation. This method has the potential to benefit multiscale analysis of complex systems and encourage further investigation in this area.
</details>
<details>
<summary>摘要</summary>
多尺度特征是复杂非线性系统的标志性特征。而使用传统的数值方法进行模拟时，会受到本地Taylor系列约束，而多尺度技术则经常受到寻找封闭的限制。本研究提出了使用深度神经网络来模拟多尺度问题的新方法。通过神经网络时间步骤的层次学习，该方法可以对不同时间尺度的动力系统流图进行approximation。这种方法可以在计算时间上比固定步骤神经网络解决方案更快，并达到当前最佳性能。该方法在多个非线性动力系统中进行了示例，并提供了实现代码。这种方法具有推动多尺度分析复杂系统的潜力，并鼓励这一领域进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="ID-Embedding-as-Subtle-Features-of-Content-and-Structure-for-Multimodal-Recommendation"><a href="#ID-Embedding-as-Subtle-Features-of-Content-and-Structure-for-Multimodal-Recommendation" class="headerlink" title="ID Embedding as Subtle Features of Content and Structure for Multimodal Recommendation"></a>ID Embedding as Subtle Features of Content and Structure for Multimodal Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05956">http://arxiv.org/abs/2311.05956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuting Liu, Enneng Yang, Yizhou Dang, Guibing Guo, Qiang Liu, Yuliang Liang, Linying Jiang, Xingwei Wang</li>
<li>for: 这个论文的目的是提出一种基于多modal信息的推荐模型，以提高推荐的准确率和效果。</li>
<li>methods: 这个论文使用了一种基于ID embedding的 hierarchical attention机制，以增强内容表示的Semantic Features，同时使用了一种轻量级的图 convolutional neural network来捕捉结构信息。</li>
<li>results: 实验结果表明，这个方法在三个实际 dataset（Baby, Sports, Clothing）上的评价比靶场的方法高，并且可以增强内容表示的Semantic Features。<details>
<summary>Abstract</summary>
Multimodal recommendation aims to model user and item representations comprehensively with the involvement of multimedia content for effective recommendations. Existing research has shown that it is beneficial for recommendation performance to combine (user- and item-) ID embeddings with multimodal salient features, indicating the value of IDs. However, there is a lack of a thorough analysis of the ID embeddings in terms of feature semantics in the literature. In this paper, we revisit the value of ID embeddings for multimodal recommendation and conduct a thorough study regarding its semantics, which we recognize as subtle features of content and structures. Then, we propose a novel recommendation model by incorporating ID embeddings to enhance the semantic features of both content and structures. Specifically, we put forward a hierarchical attention mechanism to incorporate ID embeddings in modality fusing, coupled with contrastive learning, to enhance content representations. Meanwhile, we propose a lightweight graph convolutional network for each modality to amalgamate neighborhood and ID embeddings for improving structural representations. Finally, the content and structure representations are combined to form the ultimate item embedding for recommendation. Extensive experiments on three real-world datasets (Baby, Sports, and Clothing) demonstrate the superiority of our method over state-of-the-art multimodal recommendation methods and the effectiveness of fine-grained ID embeddings.
</details>
<details>
<summary>摘要</summary>
多模态推荐的目标是全面地表示用户和项目表示，并利用多种多媒体内容来提供有效的推荐。现有研究表明，将用户和项目ID编码与多模态突出特征结合起来可以提高推荐性能。然而，学术文献中对ID编码的semantics还没有进行了全面的分析。本文重新评估了多模态推荐中ID编码的值，并进行了semantics的全面分析。然后，我们提出了一种新的推荐模型，该模型通过结合ID编码来增强内容和结构的semantics。具体来说，我们提出了一种层次注意机制，将ID编码与多模态融合进行了强调，并与对比学习结合使用，以提高内容表示。同时，我们提出了一种轻量级的图 convolutional network，用于每种模态的卷积整合，以提高结构表示。最后，内容和结构表示被组合，形成了最终的项目嵌入，用于推荐。我们对三个实际 datasets（婴儿、运动和时尚）进行了广泛的实验，并证明了我们的方法在多模态推荐方法中的优越性和ID编码的细腻性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Augmented-Scheduling-for-Solar-Powered-Electric-Vehicle-Charging"><a href="#Learning-Augmented-Scheduling-for-Solar-Powered-Electric-Vehicle-Charging" class="headerlink" title="Learning-Augmented Scheduling for Solar-Powered Electric Vehicle Charging"></a>Learning-Augmented Scheduling for Solar-Powered Electric Vehicle Charging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05941">http://arxiv.org/abs/2311.05941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongxin Li</li>
<li>for:  scheduling the charging of electric vehicles equipped with solar panels and batteries, particularly under out-of-distribution (OOD) conditions.</li>
<li>methods:  leverages a novel learning-augmented policy that employs a dynamic robustness budget, which is adapted in real-time based on the reinforcement learning policy’s performance, using the temporal difference (TD) error to assess the trustworthiness of the machine-learned policy.</li>
<li>results:  markedly improves scheduling effectiveness and reliability, particularly in OOD contexts, paving the way for more resilient and adaptive EV charging systems.<details>
<summary>Abstract</summary>
We tackle the complex challenge of scheduling the charging of electric vehicles (EVs) equipped with solar panels and batteries, particularly under out-of-distribution (OOD) conditions. Traditional scheduling approaches, such as reinforcement learning (RL) and model predictive control (MPC), often fail to provide satisfactory results when faced with OOD data, struggling to balance robustness (worst-case performance) and consistency (near-optimal average performance). To address this gap, we introduce a novel learning-augmented policy. This policy employs a dynamic robustness budget, which is adapted in real-time based on the reinforcement learning policy's performance. Specifically, it leverages the temporal difference (TD) error, a measure of the learning policy's prediction accuracy, to assess the trustworthiness of the machine-learned policy. This method allows for a more effective balance between consistency and robustness in EV charging schedules, significantly enhancing adaptability and efficiency in real-world, unpredictable environments. Our results demonstrate that this approach markedly improves scheduling effectiveness and reliability, particularly in OOD contexts, paving the way for more resilient and adaptive EV charging systems.
</details>
<details>
<summary>摘要</summary>
我们面临电动汽车（EV）装有太阳能板和电池的充电时间安排的复杂挑战，尤其在异常输入（OOD）条件下。传统的安排方法，如强化学习（RL）和预测模型控制（MPC），在面临OOD数据时经常无法提供满意的结果，坚持着平衡稳定性（最差性能）和一致性（近似优性）。为解决这个差距，我们介绍了一种新的学习增强策略。这种策略使用动态 robustness预算，实时根据学习策略的性能而改变。具体来说，它利用时间差（TD）错误，用于评估机器学习策略的预测准确性。这种方法允许更好地平衡稳定性和一致性在EV充电时间安排中，大大提高了适应性和效率，特别在实际不可预测的环境中。我们的结果表明，这种方法在OOD上进行了明显改进，大大提高了安排效果和可靠性，开拓了更加可靠和适应的EV充电系统。
</details></li>
</ul>
<hr>
<h2 id="Aggregation-Weighting-of-Federated-Learning-via-Generalization-Bound-Estimation"><a href="#Aggregation-Weighting-of-Federated-Learning-via-Generalization-Bound-Estimation" class="headerlink" title="Aggregation Weighting of Federated Learning via Generalization Bound Estimation"></a>Aggregation Weighting of Federated Learning via Generalization Bound Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05936">http://arxiv.org/abs/2311.05936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingwei Xu, Xiaofeng Cao, Ivor W. Tsang, James T. Kwok</li>
<li>for: 提高 Federated Learning（FL）中客户端模型参数的聚合方法，以提高模型性能和公平性。</li>
<li>methods: 提出一种新的聚合策略，基于每个本地模型的一致误差维度进行权重调整，以适应不同客户端数据的统计不同和噪声。</li>
<li>results: 通过实验，提出的聚合策略可以显著提高多种代表性FL算法在标准数据集上的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) typically aggregates client model parameters using a weighting approach determined by sample proportions. However, this naive weighting method may lead to unfairness and degradation in model performance due to statistical heterogeneity and the inclusion of noisy data among clients. Theoretically, distributional robustness analysis has shown that the generalization performance of a learning model with respect to any shifted distribution is bounded. This motivates us to reconsider the weighting approach in federated learning. In this paper, we replace the aforementioned weighting method with a new strategy that considers the generalization bounds of each local model. Specifically, we estimate the upper and lower bounds of the second-order origin moment of the shifted distribution for the current local model, and then use these bounds disagreements as the aggregation proportions for weightings in each communication round. Experiments demonstrate that the proposed weighting strategy significantly improves the performance of several representative FL algorithms on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
通常， Federated Learning (FL) 使用 Client 模型参数的权重方法进行聚合。但这种简单的权重方法可能会导致不公平和模型性能下降，因为客户端数据具有统计不同性和噪声。理论上，分布robustness分析表明，学习模型对于任何偏移分布的总体性能具有上限。这些上限提供了一个重新考虑权重策略的动机。在本文中，我们将替换原来的权重策略，使用每个本地模型的泛化约束来确定聚合比例。具体来说，我们将估计当前本地模型的第二个源 moments的上下限，并使用这些上下限的差异作为每个通信轮的聚合比例。实验表明，我们的权重策略可以在多个代表性 FL 算法上显著改进模型性能。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-with-Manifold-Regularization-and-Normalized-Update-Reaggregation"><a href="#Federated-Learning-with-Manifold-Regularization-and-Normalized-Update-Reaggregation" class="headerlink" title="Federated Learning with Manifold Regularization and Normalized Update Reaggregation"></a>Federated Learning with Manifold Regularization and Normalized Update Reaggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05924">http://arxiv.org/abs/2311.05924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuming An, Li Shen, Han Hu, Yong Luo</li>
<li>for: 这篇论文的目的是提出一种基于散列学习（Federated Learning）的新方法，以解决受到本地数据不同性的影响，导致全球模型更新距离减小，影响散列学习的快速参数收敛。</li>
<li>methods: 本 paper 使用了扩展的散列学习框架，具有聚合客户端更新norm的新全球优化器，以解决模型不一致性问题。具体来说，本 paper 使用了拓扑学习的概念，在散列学习中增加了一个拓扑模型融合方案，以便更好地反映模型的不一致性。</li>
<li>results: 实验表明，FedMRUR可以在散列学习中达到新的州际标准（SOTA）精度，并且减少了通信量。此外，本 paper 还证明了我们的算法在非对称Setting下可以达到线性增速性质。<details>
<summary>Abstract</summary>
Federated Learning (FL) is an emerging collaborative machine learning framework where multiple clients train the global model without sharing their own datasets. In FL, the model inconsistency caused by the local data heterogeneity across clients results in the near-orthogonality of client updates, which leads to the global update norm reduction and slows down the convergence. Most previous works focus on eliminating the difference of parameters (or gradients) between the local and global models, which may fail to reflect the model inconsistency due to the complex structure of the machine learning model and the Euclidean space's limitation in meaningful geometric representations. In this paper, we propose FedMRUR by adopting the manifold model fusion scheme and a new global optimizer to alleviate the negative impacts. Concretely, FedMRUR adopts a hyperbolic graph manifold regularizer enforcing the representations of the data in the local and global models are close to each other in a low-dimensional subspace. Because the machine learning model has the graph structure, the distance in hyperbolic space can reflect the model bias better than the Euclidean distance. In this way, FedMRUR exploits the manifold structures of the representations to significantly reduce the model inconsistency. FedMRUR also aggregates the client updates norms as the global update norm, which can appropriately enlarge each client's contribution to the global update, thereby mitigating the norm reduction introduced by the near-orthogonality of client updates. Furthermore, we theoretically prove that our algorithm can achieve a linear speedup property for non-convex setting under partial client participation.Experiments demonstrate that FedMRUR can achieve a new state-of-the-art (SOTA) accuracy with less communication.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）是一种在多个客户端上训练全域模型的新兴协力机器学习框架，而不需要客户端分享自己的数据。在FL中，因为客户端的地方数据不同而导致的模型不一致性，导致客户端更新的方向接近垂直方向，这会导致全域更新的规模增加和步骤变慢。大多数先前的工作强调在删除本地和全域模型之间的差异，但这可能无法反映模型不一致性，因为机器学习模型的复杂结构和欧几何空间的限制。在这篇文章中，我们提出了FedMRUR，通过采用数据构造模型融合方案和一个新的全域优化器，以解决这些负面影响。具体来说，FedMRUR采用一个拓扑图 manifold regularizer，使得本地和全域模型的表现在低维度子空间中相似。因为机器学习模型具有图结构，在拓扑图上的距离可以更好地反映模型偏见。这样，FedMRUR可以将数据表现的拓扑图结构纳入到模型训练中，以减少模型不一致性。FedMRUR还将客户端更新的规模总和为全域更新的规模，这可以适当地增加每个客户端的贡献，从而减少由近似垂直方向的客户端更新所导致的规模增加。此外，我们也 theoretically 证明了我们的算法可以在非凸设定下 achievelinear speedup 性。实验结果显示，FedMRUR可以 achieve 新的最佳性（SOTA）的准确性，并且需要更少的通信。
</details></li>
</ul>
<hr>
<h2 id="An-alternative-for-one-hot-encoding-in-neural-network-models"><a href="#An-alternative-for-one-hot-encoding-in-neural-network-models" class="headerlink" title="An alternative for one-hot encoding in neural network models"></a>An alternative for one-hot encoding in neural network models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05911">http://arxiv.org/abs/2311.05911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lazar Zlatić</li>
<li>for: 本文提出了一种算法，用于实现对神经网络模型输入数据 categorical 特征的二进制编码，同时对前向和反向传播过程进行修改，以实现神经网络学习过程中 certain 特征类别数据实例的模型权重变化，只影响该特征类别数据实例的前向计算。</li>
<li>methods: 本文使用二进制编码实现了 categorical 特征的压缩，并对前向和反向传播过程进行修改，以实现模型权重变化只影响相应特征类别数据实例的计算。</li>
<li>results: 本文的实验结果表明，使用二进制编码和修改前向和反向传播过程可以实现神经网络学习过程中 certain 特征类别数据实例的模型权重变化只影响该特征类别数据实例的计算，从而提高模型的性能。<details>
<summary>Abstract</summary>
This paper proposes an algorithm that implements binary encoding of the categorical features of neural network model input data, while also implementing changes in the forward and backpropagation procedures in order to achieve the property of having model weight changes, that result from the neural network learning process for certain data instances of some feature category, only affect the forward pass calculations for input data instances of that same feature category, as it is in the case of utilising one-hot encoding for categorical features.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FlashFFTConv-Efficient-Convolutions-for-Long-Sequences-with-Tensor-Cores"><a href="#FlashFFTConv-Efficient-Convolutions-for-Long-Sequences-with-Tensor-Cores" class="headerlink" title="FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores"></a>FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05908">http://arxiv.org/abs/2311.05908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HazyResearch/flash-fft-conv">https://github.com/HazyResearch/flash-fft-conv</a></li>
<li>paper_authors: Daniel Y. Fu, Hermann Kumbong, Eric Nguyen, Christopher Ré</li>
<li>for: 这 paper 主要研究如何优化 FFT  convolution，以提高长序列任务中的计算效率。</li>
<li>methods: 该 paper 使用了 matrix decomposition 算法，以计算 FFT 使用特циализирован的 matrix multiply units，并实现了 kernel fusion 技术，以减少 I&#x2F;O 开销。此外， paper 还提出了两种稀疏 convolution 算法，即 partial convolutions 和 frequency-sparse convolutions。</li>
<li>results: FlashFFTConv 在 exact FFT convolutions 中提高了速度，比 PyTorch 快速了 7.93 倍，并在 end-to-end 速度上达到了 4.4 倍速化。此外， FlashFFTConv 在 Hyena-GPT-s 和 M2-BERT-base 中实现了更好的模型质量，与同样计算预算下的模型具有相同或更好的性能。<details>
<summary>Abstract</summary>
Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\times$ over PyTorch and achieves up to 4.4$\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.
</details>
<details>
<summary>摘要</summary>
卷积模型 WITH long filters 已经在许多长序任务中显示出了state-of-the-art的理解能力，但它们在wall-clock时间方面落后于最优化的 Transformer。一个主要瓶颈是 Fast Fourier Transform (FFT)，它可以在序列长度 N 的情况下使卷积运算时间为 $O(N \log N)$，但硬件利用率不高。在这篇论文中，我们研究如何优化 FFT 卷积。我们发现了两个关键瓶颈：FFT 不好地使用特殊化矩阵乘法单元，并且在层次结构中进行 I/O 操作会产生昂贵的成本。为了解决这些问题，我们提出了 FlashFFTConv。FlashFFTConv 使用矩阵分解来计算 FFT，并使用矩阵乘法单元进行计算，从而提高硬件利用率。此外，我们还提出了两种稀疏卷积算法：1）部分卷积和2）频率稀疏卷积。这些算法可以通过跳过块来实现，从而实现更多的内存和计算减少。FlashFFTConv 可以在精确 FFT 卷积中提高速度，达到 Up to 7.93 倍 PyTorch 的速度，并在综合评估中达到 Up to 4.4 倍的速度。给定同样的计算预算，FlashFFTConv 允许 Hyena-GPT-s 在 PILE 上达到 2.3 个点更高的折衔率，并使 M2-BERT-base 在 GLUE 上达到 3.3 个点更高的分数。FlashFFTConv 还可以在 Path-512 高分辨率视觉任务中达到 96.1% 的准确率，并且部分卷积可以实现更长的序列模型，例如可以处理人类基因最长的 2.3M 个基因对。此外，频率稀疏卷积可以加速预训练模型，保持或提高模型质量。
</details></li>
</ul>
<hr>
<h2 id="Can-Machine-Learning-Uncover-Insights-into-Vehicle-Travel-Demand-from-Our-Built-Environment"><a href="#Can-Machine-Learning-Uncover-Insights-into-Vehicle-Travel-Demand-from-Our-Built-Environment" class="headerlink" title="Can Machine Learning Uncover Insights into Vehicle Travel Demand from Our Built Environment?"></a>Can Machine Learning Uncover Insights into Vehicle Travel Demand from Our Built Environment?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06321">http://arxiv.org/abs/2311.06321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixun Huang, Hao Zheng</li>
<li>for: 本研究旨在帮助设计师优化城市用地规划，增进城市规划的可持续发展。</li>
<li>methods: 本研究使用机器学习技术，收集城市点 интересов（POI）数据和在线车辆数据，采用人工神经网络（ANNs）进行预测，并将预测结果覆盖到地图上进行可视化。</li>
<li>results: 研究结果表明，使用计算模型可以帮助设计师快速获得交通需求的反馈，包括交通总量和时间分布。此外，计算模型还可以帮助评估和优化城市用地规划，从车辆交通的角度来看。<details>
<summary>Abstract</summary>
In this paper, we propose a machine learning-based approach to address the lack of ability for designers to optimize urban land use planning from the perspective of vehicle travel demand. Research shows that our computational model can help designers quickly obtain feedback on the vehicle travel demand, which includes its total amount and temporal distribution based on the urban function distribution designed by the designers. It also assists in design optimization and evaluation of the urban function distribution from the perspective of vehicle travel. We obtain the city function distribution information and vehicle hours traveled (VHT) information by collecting the city point-of-interest (POI) data and online vehicle data. The artificial neural networks (ANNs) with the best performance in prediction are selected. By using data sets collected in different regions for mutual prediction and remapping the predictions onto a map for visualization, we evaluate the extent to which the computational model sees use across regions in an attempt to reduce the workload of future urban researchers. Finally, we demonstrate the application of the computational model to help designers obtain feedback on vehicle travel demand in the built environment and combine it with genetic algorithms to optimize the current state of the urban environment to provide recommendations to designers.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于机器学习的方法，以解决城市规划师无法根据交通工具需求优化城市土地使用的问题。研究表明，我们的计算模型可以帮助城市规划师快速获得交通工具需求的总量和时间分布，包括基于城市功能分布的交通工具需求。此外，它还可以帮助评估和优化城市功能分布的交通工具需求。我们通过收集城市点对点数据和在线交通数据获得城市功能分布信息和交通时间（VHT）信息。我们选择了最佳表现的人工神经网络（ANNs）进行预测。通过在不同地区进行互Predict和重新映射预测结果onto a map for visualization，我们评估了计算模型在不同地区的使用程度，以降低未来城市研究者的工作负担。最后，我们示出了计算模型如何帮助城市规划师获得交通工具需求反馈，并与遗传算法结合优化当前城市环境，以提供建议给城市规划师。
</details></li>
</ul>
<hr>
<h2 id="Low-Multi-Rank-High-Order-Bayesian-Robust-Tensor-Factorization"><a href="#Low-Multi-Rank-High-Order-Bayesian-Robust-Tensor-Factorization" class="headerlink" title="Low-Multi-Rank High-Order Bayesian Robust Tensor Factorization"></a>Low-Multi-Rank High-Order Bayesian Robust Tensor Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05888">http://arxiv.org/abs/2311.05888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianan Liu, Chunguang Li</li>
<li>for: 提高高阶tensor фактор化方法的精度和robustness，应用于高阶数据 such as 四元色视频、四元射频视频和五元光场图像。</li>
<li>methods: 基于高阶特征值分解(t-SVD)的高阶TRPCA方法，包括自动确定tensor的多rank结构和Explicitly模型噪音。</li>
<li>results: 提出了一种新的高阶TRPCA方法LMH-BRTF，通过建立一个基于order-$d$ t-SVD的低级模型和适当的先验来自动确定tensor的多rank结构，并且能够更好地利用噪音信息，从而提高TRPCA的性能。<details>
<summary>Abstract</summary>
The recently proposed tensor robust principal component analysis (TRPCA) methods based on tensor singular value decomposition (t-SVD) have achieved numerous successes in many fields. However, most of these methods are only applicable to third-order tensors, whereas the data obtained in practice are often of higher order, such as fourth-order color videos, fourth-order hyperspectral videos, and fifth-order light-field images. Additionally, in the t-SVD framework, the multi-rank of a tensor can describe more fine-grained low-rank structure in the tensor compared with the tubal rank. However, determining the multi-rank of a tensor is a much more difficult problem than determining the tubal rank. Moreover, most of the existing TRPCA methods do not explicitly model the noises except the sparse noise, which may compromise the accuracy of estimating the low-rank tensor. In this work, we propose a novel high-order TRPCA method, named as Low-Multi-rank High-order Bayesian Robust Tensor Factorization (LMH-BRTF), within the Bayesian framework. Specifically, we decompose the observed corrupted tensor into three parts, i.e., the low-rank component, the sparse component, and the noise component. By constructing a low-rank model for the low-rank component based on the order-$d$ t-SVD and introducing a proper prior for the model, LMH-BRTF can automatically determine the tensor multi-rank. Meanwhile, benefiting from the explicit modeling of both the sparse and noise components, the proposed method can leverage information from the noises more effectivly, leading to an improved performance of TRPCA. Then, an efficient variational inference algorithm is established for parameters estimation. Empirical studies on synthetic and real-world datasets demonstrate the effectiveness of the proposed method in terms of both qualitative and quantitative results.
</details>
<details>
<summary>摘要</summary>
最近提出的高阶矩阵坚定原理Component Analysis（TRPCA）方法，基于高阶矩阵均值分解（t-SVD），在多个领域取得了成功。然而，大多数这些方法只适用于第三阶矩阵，而实际数据通常是更高阶的，例如第四阶色视频、第四阶射频视频和第五阶光场图像。此外，在t-SVD框架中，矩阵多rank可以描述矩阵中更细化的低级结构，相比于管道rank。然而，确定矩阵多rank是一个更加困难的问题，而且大多数现有的TRPCA方法并不明确地模型噪音。在这种情况下，我们提出了一种新的高阶TRPCA方法，即含有抽象的高阶矩阵均值分解（LMH-BRTF）。具体来说，我们将观察到的受损矩阵分解成三部分：低级组成部分、稀疏组成部分和噪音组成部分。通过基于第d级t-SVD的低级模型和适当的先验来建立低级模型，LMH-BRTF可以自动确定矩阵多rank。此外，因为明确地模型噪音和稀疏组成，提案的方法可以更好地利用噪音信息，从而提高TRPCA的性能。然后，我们建立了一种高效的变分推理算法来估计参数。empirical studies on synthetic and real-world datasets demonstrate the effectiveness of the proposed method in terms of both qualitative and quantitative results.
</details></li>
</ul>
<hr>
<h2 id="Hiformer-Heterogeneous-Feature-Interactions-Learning-with-Transformers-for-Recommender-Systems"><a href="#Hiformer-Heterogeneous-Feature-Interactions-Learning-with-Transformers-for-Recommender-Systems" class="headerlink" title="Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems"></a>Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05884">http://arxiv.org/abs/2311.05884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, Ed H. Chi</li>
<li>for: 这个论文的目的是提出一种基于Transformer架构的自动化特征交互模型，以解决在大规模推荐系统中学习特征交互的挑战。</li>
<li>methods: 该论文使用了Transformer架构，并提出了一种异质自我注意层以处理异质特征交互，以及一种名为\textsc{Hiformer}的模型，通过低级别approximation和模型剪除来提高表达能力并降低执行时间。</li>
<li>results: 实验结果表明，\textsc{Hiformer}模型可以在大规模推荐系统中提供显著改进（最高提升+2.66%），并且在线部署中具有快速执行速度。<details>
<summary>Abstract</summary>
Learning feature interaction is the critical backbone to building recommender systems. In web-scale applications, learning feature interaction is extremely challenging due to the sparse and large input feature space; meanwhile, manually crafting effective feature interactions is infeasible because of the exponential solution space. We propose to leverage a Transformer-based architecture with attention layers to automatically capture feature interactions. Transformer architectures have witnessed great success in many domains, such as natural language processing and computer vision. However, there has not been much adoption of Transformer architecture for feature interaction modeling in industry. We aim at closing the gap. We identify two key challenges for applying the vanilla Transformer architecture to web-scale recommender systems: (1) Transformer architecture fails to capture the heterogeneous feature interactions in the self-attention layer; (2) The serving latency of Transformer architecture might be too high to be deployed in web-scale recommender systems. We first propose a heterogeneous self-attention layer, which is a simple yet effective modification to the self-attention layer in Transformer, to take into account the heterogeneity of feature interactions. We then introduce \textsc{Hiformer} (\textbf{H}eterogeneous \textbf{I}nteraction Trans\textbf{former}) to further improve the model expressiveness. With low-rank approximation and model pruning, \hiformer enjoys fast inference for online deployment. Extensive offline experiment results corroborates the effectiveness and efficiency of the \textsc{Hiformer} model. We have successfully deployed the \textsc{Hiformer} model to a real world large scale App ranking model at Google Play, with significant improvement in key engagement metrics (up to +2.66\%).
</details>
<details>
<summary>摘要</summary>
学习特征交互是推荐系统的关键脊梁。在网络级应用中，学习特征交互非常困难，因为输入特征空间很大且稀疏，同时手动设计有效的特征交互非常困难，因为解决空间是指数增长的。我们提议利用基于Transformer架构的模型，通过注意层自动捕捉特征交互。Transformer架构在许多领域取得了很大成功，如自然语言处理和计算机视觉。然而，在产业中对特征交互模型的应用还有一定的差距。我们的目标是填补这个差距。我们认为，在网络级应用中使用vanilla Transformer架构存在两个主要挑战：（1）Transformer架构无法捕捉特征交互中的不同类型交互；（2）Transformer架构的服务延迟可能太高，不适合在网络级应用中使用。我们首先提出一种不同类型交互的自注意层，这是一种简单 yet有效的修改，以满足特征交互中的不同类型交互。然后，我们引入\textsc{Hiformer}（特征交互转换器），以提高模型表达能力。通过低级抽象和模型剔除，\hiformer在线上部署中具有快速的推理速度。我们对大量实验数据进行了广泛的做法验证，证明了\textsc{Hiformer}模型的有效性和高效性。我们成功地将\textsc{Hiformer}模型部署到了Google Play上一个实际应用中，并实现了关键参与度指标（最高+2.66%）上的显著提高。
</details></li>
</ul>
<hr>
<h2 id="Testing-Dependency-of-Unlabeled-Databases"><a href="#Testing-Dependency-of-Unlabeled-Databases" class="headerlink" title="Testing Dependency of Unlabeled Databases"></a>Testing Dependency of Unlabeled Databases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05874">http://arxiv.org/abs/2311.05874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vered Paslev, Wasim Huleihel</li>
<li>For: This paper investigates the problem of determining whether two random databases are statistically dependent or not.* Methods: The paper formulates this problem as a hypothesis testing problem, and uses techniques from information theory and matrix analysis to derive thresholds for optimal testing.* Results: The paper shows that the thresholds for optimal testing depend on the number of dimensions $n$ and the spectral properties of the generative distributions of the datasets, and proves that weak detection is statistically impossible when a certain function of the eigenvalues of the likelihood function and $d$ is below a certain threshold, as $d\to\infty$. The paper also derives strong and weak detection lower and upper bounds for the case where $d$ is fixed.<details>
<summary>Abstract</summary>
In this paper, we investigate the problem of deciding whether two random databases $\mathsf{X}\in\mathcal{X}^{n\times d}$ and $\mathsf{Y}\in\mathcal{Y}^{n\times d}$ are statistically dependent or not. This is formulated as a hypothesis testing problem, where under the null hypothesis, these two databases are statistically independent, while under the alternative, there exists an unknown row permutation $\sigma$, such that $\mathsf{X}$ and $\mathsf{Y}^\sigma$, a permuted version of $\mathsf{Y}$, are statistically dependent with some known joint distribution, but have the same marginal distributions as the null. We characterize the thresholds at which optimal testing is information-theoretically impossible and possible, as a function of $n$, $d$, and some spectral properties of the generative distributions of the datasets. For example, we prove that if a certain function of the eigenvalues of the likelihood function and $d$, is below a certain threshold, as $d\to\infty$, then weak detection (performing slightly better than random guessing) is statistically impossible, no matter what the value of $n$ is. This mimics the performance of an efficient test that thresholds a centered version of the log-likelihood function of the observed matrices. We also analyze the case where $d$ is fixed, for which we derive strong (vanishing error) and weak detection lower and upper bounds.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了两个随机数据库 $\mathsf{X}\in\mathcal{X}^{n\times d}$ 和 $\mathsf{Y}\in\mathcal{Y}^{n\times d}$ 是否 Statistically 相关的问题。我们将这个问题转化为一个 гипотеза测试问题，其中在 null 假设下，这两个数据库是 statistically 独立的，而在 alternative 下，存在一个未知的行Permutation  $\sigma$，使得 $\mathsf{X}$ 和 $\mathsf{Y}^\sigma$ 是一个已知的联合分布下的 Statistically 相关的，但它们的各自分布与 null 假设一样。我们分析了在 $n$ 和 $d$ 上的测试阈值，以及这些阈值与数据集的生成分布的特性之间的关系。例如，我们证明了，如果一个函数 $f$ 的值小于一定阈值，然后 $d\to\infty$，那么在任何 $n$ 值下，弱测试（比Random Guessing 稍微好）是 statistically 不可能，这与中心化的 log-likelihood 函数的测试阈值相同。我们还分析了 $d$ 是固定的情况，得到了强（消失错误）和弱（增长错误）检测下界和上界。
</details></li>
</ul>
<hr>
<h2 id="Fair-Supervised-Learning-with-A-Simple-Random-Sampler-of-Sensitive-Attributes"><a href="#Fair-Supervised-Learning-with-A-Simple-Random-Sampler-of-Sensitive-Attributes" class="headerlink" title="Fair Supervised Learning with A Simple Random Sampler of Sensitive Attributes"></a>Fair Supervised Learning with A Simple Random Sampler of Sensitive Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05866">http://arxiv.org/abs/2311.05866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinwon Sohn, Qifan Song, Guang Lin</li>
<li>for: 这个研究的目的是提出一种基于神经网络的公平评估方法，以便在各种业务应用中实现公平性。</li>
<li>methods: 该方法使用一种简单的随机抽取敏感特征的方法来学习公平罚款，并能够处理各种不同的敏感特征格式，因此在实际应用中更加广泛可用。</li>
<li>results: 实验表明，该方法在流行的 benchmark 数据集上比竞争方法具有更好的实用性和公平度量。此外，该方法还 theoretically Characterize 评估误差和损失的Utility。<details>
<summary>Abstract</summary>
As the data-driven decision process becomes dominating for industrial applications, fairness-aware machine learning arouses great attention in various areas. This work proposes fairness penalties learned by neural networks with a simple random sampler of sensitive attributes for non-discriminatory supervised learning. In contrast to many existing works that critically rely on the discreteness of sensitive attributes and response variables, the proposed penalty is able to handle versatile formats of the sensitive attributes, so it is more extensively applicable in practice than many existing algorithms. This penalty enables us to build a computationally efficient group-level in-processing fairness-aware training framework. Empirical evidence shows that our framework enjoys better utility and fairness measures on popular benchmark data sets than competing methods. We also theoretically characterize estimation errors and loss of utility of the proposed neural-penalized risk minimization problem.
</details>
<details>
<summary>摘要</summary>
“在工业应用中，数据驱动决策过程变得越来越重要，而具有公平性的机器学习也吸引了各方关注。这项工作提出了基于神经网络学习的公平罚款，通过简单随机抽取敏感特征来实现不歧视式指导学习。与许多现有方法不同，我们的罚款可以处理各种敏感特征的格式，因此在实践中更加广泛适用。这种罚款允许我们建立高效的计算机器-级内部处理公平性感知训练框架。实验证明，我们的框架在 популяр的Benchmark数据集上实现了更好的用用性和公平性指标，而且我们还 theoretically characterize estimation errors和loss of utility of the proposed neural-penalized risk minimization problem。”Note that Simplified Chinese is the written form of Chinese used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="Clipped-Objective-Policy-Gradients-for-Pessimistic-Policy-Optimization"><a href="#Clipped-Objective-Policy-Gradients-for-Pessimistic-Policy-Optimization" class="headerlink" title="Clipped-Objective Policy Gradients for Pessimistic Policy Optimization"></a>Clipped-Objective Policy Gradients for Pessimistic Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05846">http://arxiv.org/abs/2311.05846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jared Markowitz, Edward W. Staley</li>
<li>for: 提高 deep reinforcement learning（RL）中的策略迁移学习效果</li>
<li>methods: 使用 variance reduction measures和大小安全策略更改的方法，包括 Trust Region Policy Optimization（TRPO）和 Proximal Policy Optimization（PPO）</li>
<li>results: 在连续动作空间中使用 clipped-objective policy gradient（COPG）对象可以提高 PPO 的性能，而不添加计算成本或复杂度，并且与 TRPO 相比，COPG 方法可以提供更好的性能。<details>
<summary>Abstract</summary>
To facilitate efficient learning, policy gradient approaches to deep reinforcement learning (RL) are typically paired with variance reduction measures and strategies for making large but safe policy changes based on a batch of experiences. Natural policy gradient methods, including Trust Region Policy Optimization (TRPO), seek to produce monotonic improvement through bounded changes in policy outputs. Proximal Policy Optimization (PPO) is a commonly used, first-order algorithm that instead uses loss clipping to take multiple safe optimization steps per batch of data, replacing the bound on the single step of TRPO with regularization on multiple steps. In this work, we find that the performance of PPO, when applied to continuous action spaces, may be consistently improved through a simple change in objective. Instead of the importance sampling objective of PPO, we instead recommend a basic policy gradient, clipped in an equivalent fashion. While both objectives produce biased gradient estimates with respect to the RL objective, they also both display significantly reduced variance compared to the unbiased off-policy policy gradient. Additionally, we show that (1) the clipped-objective policy gradient (COPG) objective is on average "pessimistic" compared to both the PPO objective and (2) this pessimism promotes enhanced exploration. As a result, we empirically observe that COPG produces improved learning compared to PPO in single-task, constrained, and multi-task learning, without adding significant computational cost or complexity. Compared to TRPO, the COPG approach is seen to offer comparable or superior performance, while retaining the simplicity of a first-order method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>为了优化学习效率，深度参与学习（RL）中的政策梯度方法通常与减少噪声度量和基于批处经验的策略相结合。自然政策梯度方法，包括信任区政策优化（TRPO），旨在通过约束的变化来生成升序的改进。而 proximal policy optimization（PPO）则是一种通常使用的第一个算法，它使用损失clip来实现多个安全优化步骤，而不是TRPO中的约束。在这项工作中，我们发现在继续动作空间中应用PPO时，可以通过简单的目标更改来提高性能。而不是PPO的重要样本 objective，我们建议使用基本政策梯度，并将其clip在相同的方式下。虽然两个目标都会生成偏离RL目标的偏梯度估计，但它们都会显著减少噪声度。此外，我们还证明了以下两点：（1）COPG目标比PPO目标更加“负面”，（2）这种负面性会促进更好的探索。因此，我们在单任务、受限制和多任务学习中观察到，COPG可以提高学习效果，而不需要添加显著的计算成本或复杂性。相比TRPO，COPG方法可以提供相当于或更好的性能，而且保留了一个简单的首领方法的简单性。
</details></li>
</ul>
<hr>
<h2 id="AccEPT-An-Acceleration-Scheme-for-Speeding-Up-Edge-Pipeline-parallel-Training"><a href="#AccEPT-An-Acceleration-Scheme-for-Speeding-Up-Edge-Pipeline-parallel-Training" class="headerlink" title="AccEPT: An Acceleration Scheme for Speeding Up Edge Pipeline-parallel Training"></a>AccEPT: An Acceleration Scheme for Speeding Up Edge Pipeline-parallel Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05827">http://arxiv.org/abs/2311.05827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Chen, Yuxuan Yan, Qianqian Yang, Yuanchao Shu, Shibo He, Zhiguo Shi, Jiming Chen</li>
<li>for: 该研究旨在提高边缘设备上分布式深度学习模型的训练速度。</li>
<li>methods: 该研究提出了一种加速边缘集成管道并行训练的策略，包括一种轻量级的自适应延迟预测器来准确预测每个层的计算延迟，以及一种位元级计算效率的数据压缩方案来压缩在设备之间传输的数据。</li>
<li>results: 研究结果显示，该提案能够在考虑的实验设置下加速边缘管道并行训练，最高提高训练速度达3倍。<details>
<summary>Abstract</summary>
It is usually infeasible to fit and train an entire large deep neural network (DNN) model using a single edge device due to the limited resources. To facilitate intelligent applications across edge devices, researchers have proposed partitioning a large model into several sub-models, and deploying each of them to a different edge device to collaboratively train a DNN model. However, the communication overhead caused by the large amount of data transmitted from one device to another during training, as well as the sub-optimal partition point due to the inaccurate latency prediction of computation at each edge device can significantly slow down training. In this paper, we propose AccEPT, an acceleration scheme for accelerating the edge collaborative pipeline-parallel training. In particular, we propose a light-weight adaptive latency predictor to accurately estimate the computation latency of each layer at different devices, which also adapts to unseen devices through continuous learning. Therefore, the proposed latency predictor leads to better model partitioning which balances the computation loads across participating devices. Moreover, we propose a bit-level computation-efficient data compression scheme to compress the data to be transmitted between devices during training. Our numerical results demonstrate that our proposed acceleration approach is able to significantly speed up edge pipeline parallel training up to 3 times faster in the considered experimental settings.
</details>
<details>
<summary>摘要</summary>
通常不可能使用单个边缘设备（edge device）来整个大深度学习模型（DNN）的适应和训练，因为边缘设备的资源有限。为了推广智能应用于边缘设备，研究人员已经提议将大型模型 partitioned 成多个子模型，并将每个子模型分配到不同的边缘设备进行协同训练 DNN 模型。然而，在训练过程中由一个设备传输到另一个设备的大量数据会导致通信开销增加，而且因为每个边缘设备的计算延迟预测不准确，会导致分区点不佳，从而降低训练速度。在这篇论文中，我们提出了 AccEPT，一种加速边缘协同管道式训练的加速方案。具体来说，我们提出了一种轻量级的适应式计算延迟预测器，可以准确地预测每层的计算延迟在不同的设备上。此外，我们还提出了一种位元级计算效率高的数据压缩方案，可以压缩在训练过程中传输的数据。我们的数据显示，我们的提出的加速策略可以在考虑的实验设置下加速边缘管道式训练，达到 3 倍的速度提升。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-powered-Compact-Modeling-of-Stochastic-Electronic-Devices-using-Mixture-Density-Networks"><a href="#Machine-Learning-powered-Compact-Modeling-of-Stochastic-Electronic-Devices-using-Mixture-Density-Networks" class="headerlink" title="Machine Learning-powered Compact Modeling of Stochastic Electronic Devices using Mixture Density Networks"></a>Machine Learning-powered Compact Modeling of Stochastic Electronic Devices using Mixture Density Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05820">http://arxiv.org/abs/2311.05820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Hutchins, Shamiul Alam, Dana S. Rampini, Bakhrom G. Oripov, Adam N. McCaughan, Ahmedullah Aziz</li>
<li>for:  This paper aims to address the challenge of accurately modeling the stochastic behavior of electronic devices in circuit design and simulation.</li>
<li>methods:  The authors use Mixture Density Networks (MDNs), a machine learning approach, to model the stochastic behavior of electronic devices and demonstrate their method on heater cryotrons.</li>
<li>results:  The authors achieve a mean absolute error of 0.82% in capturing the stochastic switching dynamics of heater cryotrons, showcasing the effectiveness of their approach in accurately simulating the behavior of electronic devices.<details>
<summary>Abstract</summary>
The relentless pursuit of miniaturization and performance enhancement in electronic devices has led to a fundamental challenge in the field of circuit design and simulation: how to accurately account for the inherent stochastic nature of certain devices. While conventional deterministic models have served as indispensable tools for circuit designers, they fall short when it comes to capture the subtle yet critical variability exhibited by many electronic components. In this paper, we present an innovative approach that transcends the limitations of traditional modeling techniques by harnessing the power of machine learning, specifically Mixture Density Networks (MDNs), to faithfully represent and simulate the stochastic behavior of electronic devices. We demonstrate our approach to model heater cryotrons, where the model is able to capture the stochastic switching dynamics observed in the experiment. Our model shows 0.82% mean absolute error for switching probability. This paper marks a significant step forward in the quest for accurate and versatile compact models, poised to drive innovation in the realm of electronic circuits.
</details>
<details>
<summary>摘要</summary>
“电子设备的推进式小型化和性能提高已经导致对电路设计和模拟的基本挑战：如何准确地考虑certain device的随机性。传统的决定论模型在电路设计中 serves as indispensable tools, but they fall short when it comes to capturing the subtle yet critical variability exhibited by many electronic components. 在本文中，我们透过 harnessing the power of machine learning, specifically Mixture Density Networks (MDNs), to faithfully represent and simulate the stochastic behavior of electronic devices. We demonstrate our approach by modeling heater cryotrons, where the model is able to capture the stochastic switching dynamics observed in the experiment. Our model shows 0.82% mean absolute error for switching probability, marking a significant step forward in the quest for accurate and versatile compact models, poised to drive innovation in the realm of electronic circuits.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Scale-MIA-A-Scalable-Model-Inversion-Attack-against-Secure-Federated-Learning-via-Latent-Space-Reconstruction"><a href="#Scale-MIA-A-Scalable-Model-Inversion-Attack-against-Secure-Federated-Learning-via-Latent-Space-Reconstruction" class="headerlink" title="Scale-MIA: A Scalable Model Inversion Attack against Secure Federated Learning via Latent Space Reconstruction"></a>Scale-MIA: A Scalable Model Inversion Attack against Secure Federated Learning via Latent Space Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05808">http://arxiv.org/abs/2311.05808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanghao Shi, Ning Wang, Yang Xiao, Chaoyu Zhang, Yi Shi, Y. Thomas Hou, Wenjing Lou</li>
<li>for:  This paper aims to address the issue of model inversion attacks (MIAs) in federated learning, which can compromise the data privacy of individual users.</li>
<li>methods:  The proposed method, called Scale-MIA, uses a two-step process to efficiently and accurately recover training samples from the aggregated model updates. The first step involves reconstructing the latent space representations (LSRs) from the updates using a closed-form inversion mechanism, and the second step involves recovering the whole input batches from the LSRs using a fine-tuned generative decoder.</li>
<li>results:  The proposed Scale-MIA method achieves excellent recovery performance on different datasets, with high reconstruction rates, accuracy, and attack efficiency compared to state-of-the-art MIAs. The method is able to efficiently recover the training samples even when the system is under the protection of a robust secure aggregation protocol.<details>
<summary>Abstract</summary>
Federated learning is known for its capability to safeguard participants' data privacy. However, recently emerged model inversion attacks (MIAs) have shown that a malicious parameter server can reconstruct individual users' local data samples through model updates. The state-of-the-art attacks either rely on computation-intensive search-based optimization processes to recover each input batch, making scaling difficult, or they involve the malicious parameter server adding extra modules before the global model architecture, rendering the attacks too conspicuous and easily detectable.   To overcome these limitations, we propose Scale-MIA, a novel MIA capable of efficiently and accurately recovering training samples of clients from the aggregated updates, even when the system is under the protection of a robust secure aggregation protocol. Unlike existing approaches treating models as black boxes, Scale-MIA recognizes the importance of the intricate architecture and inner workings of machine learning models. It identifies the latent space as the critical layer for breaching privacy and decomposes the complex recovery task into an innovative two-step process to reduce computation complexity. The first step involves reconstructing the latent space representations (LSRs) from the aggregated model updates using a closed-form inversion mechanism, leveraging specially crafted adversarial linear layers. In the second step, the whole input batches are recovered from the LSRs by feeding them into a fine-tuned generative decoder.   We implemented Scale-MIA on multiple commonly used machine learning models and conducted comprehensive experiments across various settings. The results demonstrate that Scale-MIA achieves excellent recovery performance on different datasets, exhibiting high reconstruction rates, accuracy, and attack efficiency on a larger scale compared to state-of-the-art MIAs.
</details>
<details>
<summary>摘要</summary>
federated learning 知名于保护参与者数据隐私。然而，最近出现的模型反向攻击（MIA）表明了一个恶意参数服务器可以通过模型更新 recover 个人用户的本地数据样本。现有的攻击方法可能需要 computation-intensive 搜索基本进行更新恢复，或者它们会在参数服务器上添加额外模块，使攻击过于明显并易于检测。为了解决这些限制，我们提出了Scale-MIA，一种新的MIA，可以高效地和准确地从集成更新中提取客户端训练样本，即使系统在一个可信的安全汇聚协议的保护下。与现有的方法不同，Scale-MIA认为机器学习模型不是黑盒子，而是注重内部结构和层次结构。它将秘密空间作为隐私泄露的关键层，将复杂的恢复任务分解成两个步骤，以降低计算复杂性。第一步是从集成模型更新中重construct秘密空间表示（LSR）使用关键拟合机制，利用特制的对抗性linear层。第二步是通过feeding LSRs into a fine-tuned generative decoder来恢复整个输入批处理。我们在多种常用的机器学习模型上实现了Scale-MIA，并在不同的设置下进行了广泛的实验。结果表明，Scale-MIA在不同的数据集上表现出了高恢复率、准确率和攻击效率，相比现有的MIAs性能更高。
</details></li>
</ul>
<hr>
<h2 id="Improvements-on-Uncertainty-Quantification-for-Node-Classification-via-Distance-Based-Regularization"><a href="#Improvements-on-Uncertainty-Quantification-for-Node-Classification-via-Distance-Based-Regularization" class="headerlink" title="Improvements on Uncertainty Quantification for Node Classification via Distance-Based Regularization"></a>Improvements on Uncertainty Quantification for Node Classification via Distance-Based Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05795">http://arxiv.org/abs/2311.05795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neoques/graph-posterior-network">https://github.com/neoques/graph-posterior-network</a></li>
<li>paper_authors: Russell Alan Hart, Linlin Yu, Yifei Lou, Feng Chen</li>
<li>for: The paper focuses on uncertainty quantification for interdependent node-level classification, specifically addressing the limitations of the widely-used uncertainty cross-entropy (UCE) loss function and proposing a distance-based regularization to improve the performance of graph posterior networks (GPNs) in detecting out-of-distribution (OOD) nodes.</li>
<li>methods: The paper uses graph posterior networks (GPNs) that optimize the uncertainty cross-entropy (UCE)-based loss function, and proposes a distance-based regularization to encourage clustered OOD nodes to remain clustered in the latent space.</li>
<li>results: The proposed regularization outperforms the state-of-the-art in both OOD detection and misclassification detection, as demonstrated through extensive comparison experiments on eight standard datasets.Here’s the Chinese translation of the three key information points:</li>
<li>for: 这篇论文关注于不确定量评估，具体来说是解决通用不确定度距离（UCE）损失函数的限制，以提高图 posterior networks（GPNs）在探测Out-of-distribution（OOD）节点时的性能。</li>
<li>methods: 这篇论文使用图 posterior networks（GPNs），优化不确定度距离（UCE）基于损失函数，并提出了一种距离基于正则化，以便在 latent space 中使 OOD 节点均匀分布。</li>
<li>results: 提议的正则化在 OOD 探测和误分类探测中都超过了现有的最佳性能，通过对八个标准数据集进行了广泛的比较试验来证明。<details>
<summary>Abstract</summary>
Deep neural networks have achieved significant success in the last decades, but they are not well-calibrated and often produce unreliable predictions. A large number of literature relies on uncertainty quantification to evaluate the reliability of a learning model, which is particularly important for applications of out-of-distribution (OOD) detection and misclassification detection. We are interested in uncertainty quantification for interdependent node-level classification. We start our analysis based on graph posterior networks (GPNs) that optimize the uncertainty cross-entropy (UCE)-based loss function. We describe the theoretical limitations of the widely-used UCE loss. To alleviate the identified drawbacks, we propose a distance-based regularization that encourages clustered OOD nodes to remain clustered in the latent space. We conduct extensive comparison experiments on eight standard datasets and demonstrate that the proposed regularization outperforms the state-of-the-art in both OOD detection and misclassification detection.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/10/cs.LG_2023_11_10/" data-id="clpahu77300uv3h882s3l27ke" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/7/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/9/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
