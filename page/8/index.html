
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/8/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/eess.IV_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/eess.IV_2023_07_27/">eess.IV - 2023-07-27 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Weakly-Supervised-AI-for-Efficient-Analysis-of-3D-Pathology-Samples"><a href="#Weakly-Supervised-AI-for-Efficient-Analysis-of-3D-Pathology-Samples" class="headerlink" title="Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples"></a>Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14907">http://arxiv.org/abs/2307.14907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahmoodlab/mamba">https://github.com/mahmoodlab/mamba</a></li>
<li>paper_authors: Andrew H. Song, Mane Williams, Drew F. K. Williamson, Guillaume Jaume, Andrew Zhang, Bowen Chen, Robert Serafin, Jonathan T. C. Liu, Alex Baras, Anil V. Parwani, Faisal Mahmood</li>
<li>for: 这个研究旨在开发一种基于深度学习的平台，用于处理多种成像模式的3D组织图像，预测病人结果。</li>
<li>methods: 该平台使用多样化的3D块分析方法，基于5年生化复诊结果进行风险分化网络训练。</li>
<li>results: 研究发现，使用3D块方法可以提高预测性能，并且可以减少采样偏误的风险，建议在临床实践中使用3D成像技术进行诊断和预测。<details>
<summary>Abstract</summary>
Human tissue and its constituent cells form a microenvironment that is fundamentally three-dimensional (3D). However, the standard-of-care in pathologic diagnosis involves selecting a few two-dimensional (2D) sections for microscopic evaluation, risking sampling bias and misdiagnosis. Diverse methods for capturing 3D tissue morphologies have been developed, but they have yet had little translation to clinical practice; manual and computational evaluations of such large 3D data have so far been impractical and/or unable to provide patient-level clinical insights. Here we present Modality-Agnostic Multiple instance learning for volumetric Block Analysis (MAMBA), a deep-learning-based platform for processing 3D tissue images from diverse imaging modalities and predicting patient outcomes. Archived prostate cancer specimens were imaged with open-top light-sheet microscopy or microcomputed tomography and the resulting 3D datasets were used to train risk-stratification networks based on 5-year biochemical recurrence outcomes via MAMBA. With the 3D block-based approach, MAMBA achieves an area under the receiver operating characteristic curve (AUC) of 0.86 and 0.74, superior to 2D traditional single-slice-based prognostication (AUC of 0.79 and 0.57), suggesting superior prognostication with 3D morphological features. Further analyses reveal that the incorporation of greater tissue volume improves prognostic performance and mitigates risk prediction variability from sampling bias, suggesting the value of capturing larger extents of heterogeneous 3D morphology. With the rapid growth and adoption of 3D spatial biology and pathology techniques by researchers and clinicians, MAMBA provides a general and efficient framework for 3D weakly supervised learning for clinical decision support and can help to reveal novel 3D morphological biomarkers for prognosis and therapeutic response.
</details>
<details>
<summary>摘要</summary>
人类组织和其内部细胞形成一个基本三维（3D）的微环境。然而，现行标准的病理诊断方法仅选择一些二维（2D）的section进行微scopic评估，可能存在采样偏见和诊断错误。各种用于捕捉3D组织形态的方法已经发展出来，但它们在临床实践中尚未得到广泛应用。我们现在介绍了模态无关多例学习 для块分析（MAMBA），一种基于深度学习的平台，用于处理不同成像模式的3D组织图像并预测病人结果。我们使用了扫描镜开式光sheet微scopy或微计算tomography扫描患有前列腺癌的肉瘤样本，并使用MAMBA进行风险分级网络的训练，以达到5年生物化学回报的 outcome。与传统的2D单片 slice-based预测相比，MAMBA的3D块基本approach在 receiver operating characteristic曲线（AUC）中得分0.86和0.74，表明3D形态特征可以提供更好的预测性。进一步分析表明，包含更大的组织体积可以提高预测性并减少采样偏见导致的预测变化，这表明3D morphological特征的捕捉是重要的。随着研究人员和临床医生对3D空间生物学和病理学技术的快速成长和采用，MAMBA提供了一个通用和高效的3D弱监学习框架，可以帮助揭示新的3D形态生物标志物和预测病人response。
</details></li>
</ul>
<hr>
<h2 id="A-full-resolution-training-framework-for-Sentinel-2-image-fusion"><a href="#A-full-resolution-training-framework-for-Sentinel-2-image-fusion" class="headerlink" title="A full-resolution training framework for Sentinel-2 image fusion"></a>A full-resolution training framework for Sentinel-2 image fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14864">http://arxiv.org/abs/2307.14864</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matciotola/FR-FUSE">https://github.com/matciotola/FR-FUSE</a></li>
<li>paper_authors: Matteo Ciotola, Mario Ragosta, Giovanni Poggi, Giuseppe Scarpa</li>
<li>for: 这篇论文旨在提出一种新的无监督框架，用于深度学习模型的决 Height 采集 Sentinel-2 图像的超分辨率处理。</li>
<li>methods: 该方案使用 Sentinel-2 图像的 10-m 和 20-m 频道进行融合，而不需要降解分辨率生成训练数据。同时，提出了一种适合的损失函数，以确保网络预测和输入组件之间的循环一致性。</li>
<li>results: 在我们的初步实验中，提出的方案已经显示出了与监督方法相比的扩展性。此外，由于构造的损失函数，得到的训练网络可以归类为多分辨率分析方法。<details>
<summary>Abstract</summary>
This work presents a new unsupervised framework for training deep learning models for super-resolution of Sentinel-2 images by fusion of its 10-m and 20-m bands. The proposed scheme avoids the resolution downgrade process needed to generate training data in the supervised case. On the other hand, a proper loss that accounts for cycle-consistency between the network prediction and the input components to be fused is proposed. Despite its unsupervised nature, in our preliminary experiments the proposed scheme has shown promising results in comparison to the supervised approach. Besides, by construction of the proposed loss, the resulting trained network can be ascribed to the class of multi-resolution analysis methods.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种新的无监督框架，用于深度学习模型的超分辨率准备，基于报告20米和10米频道的融合。提议的方案不需要生成训练数据的分辨率下降过程，同时提出了一种适当的损失函数，该函数考虑了网络预测和输入组件的循环一致性。在我们的初步实验中，提议的方案已经达到了与监督方法相比的承诺性。此外，由于构造的损失函数，得到的训练网络可以被归类为多分辨率分析方法。
</details></li>
</ul>
<hr>
<h2 id="Seeing-through-the-Brain-Image-Reconstruction-of-Visual-Perception-from-Human-Brain-Signals"><a href="#Seeing-through-the-Brain-Image-Reconstruction-of-Visual-Perception-from-Human-Brain-Signals" class="headerlink" title="Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals"></a>Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02510">http://arxiv.org/abs/2308.02510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Ting Lan, Kan Ren, Yansen Wang, Wei-Long Zheng, Dongsheng Li, Bao-Liang Lu, Lili Qiu</li>
<li>for: 这篇论文的目的是重建视觉刺激图像基于电энцеphalography（EEG）数据。</li>
<li>methods: 这篇论文提出了一个全面的执行管道，名为NeuroImagen，用于从EEG数据中重建视觉刺激图像。该管道包括一种新的多级感知信息解码器，用于从EEG数据中提取多级输出。然后，一种潜在扩散模型将利用提取的信息来重建高分辨率的视觉刺激图像。</li>
<li>results: 实验结果表明，该方法可以有效地重建视觉刺激图像，并且对比于现有方法有较高的量化性表现。<details>
<summary>Abstract</summary>
Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain signals, i.e., electroencephalography (EEG) data. Since EEG signals are dynamic in the time-series format and are notorious to be noisy, processing and extracting useful information requires more dedicated efforts; In this paper, we propose a comprehensive pipeline, named NeuroImagen, for reconstructing visual stimuli images from EEG signals. Specifically, we incorporate a novel multi-level perceptual information decoding to draw multi-grained outputs from the given EEG data. A latent diffusion model will then leverage the extracted information to reconstruct the high-resolution visual stimuli images. The experimental results have illustrated the effectiveness of image reconstruction and superior quantitative performance of our proposed method.
</details>
<details>
<summary>摘要</summary>
视觉是信任的来源，但是人类视觉与认知之间的内部机制仍然是一个谜。随着 neuroscience 和人工智能的最近进步，我们可以记录人类视觉活动和模拟视觉能力通过计算方法。在这篇论文中，我们关注于基于可 portable 脑电声信号（EEG）的视觉刺激重建。因为EEG信号是时间序列格式的动态信号，容易受到干扰，因此处理和提取有用信息需要更多的努力。为解决这个问题，我们提出了一个完整的推管道，名为NeuroImagen，可以从EEG信号中提取多层次的视觉信息，并使用扩散模型重建高分辨率的视觉刺激图像。实验结果表明我们的方法可以有效地重建图像，并且在量化性能方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Test-Time-Adaptation-for-Blind-Image-Quality-Assessment"><a href="#Test-Time-Adaptation-for-Blind-Image-Quality-Assessment" class="headerlink" title="Test Time Adaptation for Blind Image Quality Assessment"></a>Test Time Adaptation for Blind Image Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14735">http://arxiv.org/abs/2307.14735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shankhanil006/tta-iqa">https://github.com/shankhanil006/tta-iqa</a></li>
<li>paper_authors: Subhadeep Roy, Shankhanil Mitra, Soma Biswas, Rajiv Soundararajan</li>
<li>for: 提高隐藏图像质量评估（IQA）算法的执行时间性能。</li>
<li>methods: 使用两个新的质量相关的辅助任务：批处理级别的群集对比损失和样本级别的相对排名损失，以使模型更加质量相关，适应目标数据。</li>
<li>results: 使用一小批测试分布中的图像更新批处理平均值可以实现显著提高性能。<details>
<summary>Abstract</summary>
While the design of blind image quality assessment (IQA) algorithms has improved significantly, the distribution shift between the training and testing scenarios often leads to a poor performance of these methods at inference time. This motivates the study of test time adaptation (TTA) techniques to improve their performance at inference time. Existing auxiliary tasks and loss functions used for TTA may not be relevant for quality-aware adaptation of the pre-trained model. In this work, we introduce two novel quality-relevant auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In particular, we introduce a group contrastive loss at the batch level and a relative rank loss at the sample level to make the model quality aware and adapt to the target data. Our experiments reveal that even using a small batch of images from the test distribution helps achieve significant improvement in performance by updating the batch normalization statistics of the source model.
</details>
<details>
<summary>摘要</summary>
尽管干燥图像质量评估（IQA）算法的设计已经得到了显著改进，但在执行时，测试场景和训练场景之间的分布偏移 frequently leads to poor performance of these methods. This motivates the study of test time adaptation（TTA）techniques to improve their performance at inference time. Existing auxiliary tasks and loss functions used for TTA may not be relevant for quality-aware adaptation of the pre-trained model. In this work, we introduce two novel quality-relevant auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In particular, we introduce a group contrastive loss at the batch level and a relative rank loss at the sample level to make the model quality aware and adapt to the target data. Our experiments reveal that even using a small batch of images from the test distribution helps achieve significant improvement in performance by updating the batch normalization statistics of the source model.
</details></li>
</ul>
<hr>
<h2 id="A-Multimodal-Supervised-Machine-Learning-Approach-for-Satellite-based-Wildfire-Identification-in-Europe"><a href="#A-Multimodal-Supervised-Machine-Learning-Approach-for-Satellite-based-Wildfire-Identification-in-Europe" class="headerlink" title="A Multimodal Supervised Machine Learning Approach for Satellite-based Wildfire Identification in Europe"></a>A Multimodal Supervised Machine Learning Approach for Satellite-based Wildfire Identification in Europe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02508">http://arxiv.org/abs/2308.02508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelica Urbanelli, Luca Barco, Edoardo Arnaudo, Claudio Rossi</li>
<li>for: 提高自动化卫星热点检测系统的准确性，以适应自然灾害的加频。</li>
<li>methods: 跨参照模式热点检测服务（MODIS和VIIRS）和欧洲森林火灾信息系统（EFFIS）数据库，建立大规模热点数据集，用于森林火灾相关研究。提出一种多模式指导学习方法，利用多种数据源，如ERSI年度土地用途土地覆盖（LULC）和 Copernicus Sentinel-3 数据，准确地分类热点检测结果。</li>
<li>results: 实验结果表明，我们的方法在森林火灾标识任务中具有效果。<details>
<summary>Abstract</summary>
The increasing frequency of catastrophic natural events, such as wildfires, calls for the development of rapid and automated wildfire detection systems. In this paper, we propose a wildfire identification solution to improve the accuracy of automated satellite-based hotspot detection systems by leveraging multiple information sources. We cross-reference the thermal anomalies detected by the Moderate-resolution Imaging Spectroradiometer (MODIS) and the Visible Infrared Imaging Radiometer Suite (VIIRS) hotspot services with the European Forest Fire Information System (EFFIS) database to construct a large-scale hotspot dataset for wildfire-related studies in Europe. Then, we propose a novel multimodal supervised machine learning approach to disambiguate hotspot detections, distinguishing between wildfires and other events. Our methodology includes the use of multimodal data sources, such as the ERSI annual Land Use Land Cover (LULC) and the Copernicus Sentinel-3 data. Experimental results demonstrate the effectiveness of our approach in the task of wildfire identification.
</details>
<details>
<summary>摘要</summary>
随着自然灾害的频繁发生，如野火，需要开发高速自动化野火检测系统。在这篇论文中，我们提出了一种野火标识解决方案，以提高自动遥感系统中热点检测的准确性。我们将模拟高分辨率 спектро镜谱仪(MODIS)和可见红外成像雷达仪(VIIRS)的热点服务与欧洲森林火灾信息系统(EFFIS)数据库进行交叉引用，以构建欧洲大规模热点数据集用于野火相关研究。然后，我们提出了一种新的多模式超vised机器学习方法，用于细分热点检测，将野火和其他事件分开。我们的方法包括使用多模式数据源，如地理信息系统(ERSI)年度土地用途土地覆盖(LULC)和科学技术卫星(Copernicus)三号卫星数据。实验结果表明，我们的方法在野火标识任务中具有效果。
</details></li>
</ul>
<hr>
<h2 id="LLDiffusion-Learning-Degradation-Representations-in-Diffusion-Models-for-Low-Light-Image-Enhancement"><a href="#LLDiffusion-Learning-Degradation-Representations-in-Diffusion-Models-for-Low-Light-Image-Enhancement" class="headerlink" title="LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement"></a>LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14659">http://arxiv.org/abs/2307.14659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taowangzj/lldiffusion">https://github.com/taowangzj/lldiffusion</a></li>
<li>paper_authors: Tao Wang, Kaihao Zhang, Ziqian Shao, Wenhan Luo, Bjorn Stenger, Tae-Kyun Kim, Wei Liu, Hongdong Li</li>
<li>for: 提高低光照图像的增强（LLIE）</li>
<li>methods: 使用扩散模型，并 integrates 衰变和图像约束，以提高图像增强效果。</li>
<li>results: 比对多个 benchmark 数据集，实验结果表明，提案的 LLDiffusion 方法可以 Quantitatively 和 Qualitatively 超过当前的 LLIE 方法。Here’s a breakdown of each point:</li>
<li>for: The paper is written for low-light image enhancement (LLIE), specifically addressing the limitation of current deep learning methods that overlook the importance of considering degradation representations.</li>
<li>methods: The proposed method uses a degradation-aware learning scheme based on diffusion models, which integrates degradation and image priors into the diffusion process. The method includes a joint learning framework for image generation and enhancement, as well as a well-designed dynamic diffusion module that takes into account both the color map and the latent degradation representations.</li>
<li>results: The proposed method is evaluated on several well-known benchmark datasets, including synthetic and real-world unpaired datasets. The results demonstrate that LLDiffusion outperforms state-of-the-art LLIE methods both quantitatively and qualitatively.<details>
<summary>Abstract</summary>
Current deep learning methods for low-light image enhancement (LLIE) typically rely on pixel-wise mapping learned from paired data. However, these methods often overlook the importance of considering degradation representations, which can lead to sub-optimal outcomes. In this paper, we address this limitation by proposing a degradation-aware learning scheme for LLIE using diffusion models, which effectively integrates degradation and image priors into the diffusion process, resulting in improved image enhancement. Our proposed degradation-aware learning scheme is based on the understanding that degradation representations play a crucial role in accurately modeling and capturing the specific degradation patterns present in low-light images. To this end, First, a joint learning framework for both image generation and image enhancement is presented to learn the degradation representations. Second, to leverage the learned degradation representations, we develop a Low-Light Diffusion model (LLDiffusion) with a well-designed dynamic diffusion module. This module takes into account both the color map and the latent degradation representations to guide the diffusion process. By incorporating these conditioning factors, the proposed LLDiffusion can effectively enhance low-light images, considering both the inherent degradation patterns and the desired color fidelity. Finally, we evaluate our proposed method on several well-known benchmark datasets, including synthetic and real-world unpaired datasets. Extensive experiments on public benchmarks demonstrate that our LLDiffusion outperforms state-of-the-art LLIE methods both quantitatively and qualitatively. The source code and pre-trained models are available at https://github.com/TaoWangzj/LLDiffusion.
</details>
<details>
<summary>摘要</summary>
当前的深度学习方法 для低光照图像提升（LLIE）通常是基于像素级映射学习的，但这些方法经常忽视了质量的衰减表示。在这篇论文中，我们解决这个限制，提出了一种考虑衰减表示的学习方案，使得图像提升更加稳定。我们的提议的衰减意识学习方案基于衰减表示在低光照图像中的重要作用。为了实现这一点，我们首先提出了一种共同学习框架，用于学习图像生成和图像提升。其次，我们开发了一种基于扩散模型的低光照扩散模型（LLDiffusion），该模型具有一个有效地考虑了颜色图和背景衰减表示的动态扩散模块。通过将这些条件因素纳入考虑，我们的LLDiffusion可以更好地提升低光照图像，考虑到了图像的自然衰减模式以及颜色准确性。最后，我们对一些公共 benchmark 上进行了广泛的实验，证明了我们的LLDiffusion在量和质量上都超过了现有的LLIE方法。代码和预训练模型可以在 <https://github.com/TaoWangzj/LLDiffusion> 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Weakly-Supervised-Segmentation-Network-Embedding-Cross-scale-Attention-Guidance-and-Noise-sensitive-Constraint-for-Detecting-Tertiary-Lymphoid-Structures-of-Pancreatic-Tumors"><a href="#A-Weakly-Supervised-Segmentation-Network-Embedding-Cross-scale-Attention-Guidance-and-Noise-sensitive-Constraint-for-Detecting-Tertiary-Lymphoid-Structures-of-Pancreatic-Tumors" class="headerlink" title="A Weakly Supervised Segmentation Network Embedding Cross-scale Attention Guidance and Noise-sensitive Constraint for Detecting Tertiary Lymphoid Structures of Pancreatic Tumors"></a>A Weakly Supervised Segmentation Network Embedding Cross-scale Attention Guidance and Noise-sensitive Constraint for Detecting Tertiary Lymphoid Structures of Pancreatic Tumors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14603">http://arxiv.org/abs/2307.14603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingxue Wang, Liwen Zou, Jun Chen, Yingying Cao, Zhenghua Cai, Yudong Qiu, Liang Mao, Zhongqiu Wang, Jingya Chen, Luying Gui, Xiaoping Yang</li>
<li>For: 这篇研究旨在探讨一种几何学习的方法，用于检测胰脏病变中的次次性林肺结构（TLS）。* Methods: 我们提出了一个弱监督分类网络，来检测TLS。我们首先使用预训练的模型进行核lei分 segmentation，然后将精度检测过滤到我们设计的 linfocyte density 对应。我们还实现了一个跨度对应 Mechanism，将粗细度特征学习自原始胰脏病变图像，以及细节度特征学习自我设计的 linfocyte density 对应。* Results: 我们将这个方法应用于两个收集的数据集，结果显示，我们的提议方法在TLS检测精度方面与现有的分类型检测方法相比，有 statistically significant 的优化。此外，我们还将这个方法应用于研究胰脏病变中TLS的density和周围血管侵入之间的相互关系，并获得了一些临床有用的结果。<details>
<summary>Abstract</summary>
The presence of tertiary lymphoid structures (TLSs) on pancreatic pathological images is an important prognostic indicator of pancreatic tumors. Therefore, TLSs detection on pancreatic pathological images plays a crucial role in diagnosis and treatment for patients with pancreatic tumors. However, fully supervised detection algorithms based on deep learning usually require a large number of manual annotations, which is time-consuming and labor-intensive. In this paper, we aim to detect the TLSs in a manner of few-shot learning by proposing a weakly supervised segmentation network. We firstly obtain the lymphocyte density maps by combining a pretrained model for nuclei segmentation and a domain adversarial network for lymphocyte nuclei recognition. Then, we establish a cross-scale attention guidance mechanism by jointly learning the coarse-scale features from the original histopathology images and fine-scale features from our designed lymphocyte density attention. A noise-sensitive constraint is introduced by an embedding signed distance function loss in the training procedure to reduce tiny prediction errors. Experimental results on two collected datasets demonstrate that our proposed method significantly outperforms the state-of-the-art segmentation-based algorithms in terms of TLSs detection accuracy. Additionally, we apply our method to study the congruent relationship between the density of TLSs and peripancreatic vascular invasion and obtain some clinically statistical results.
</details>
<details>
<summary>摘要</summary>
pancreatic tumors中的次级血液结构（TLSs）的存在是诊断和治疗中非常重要的诊断指标。因此，TLSs的检测在pancreatic tumors中扮演着关键的角色。然而，通常需要大量的手动标注，这是时间consuming和劳动密集的。在这篇论文中，我们想要通过几shot学习来检测TLSs，我们提出了一种弱型指导网络。首先，我们获得了lymphocyte density map，通过结合预训练的核体分割模型和域 adversarial network来识别lymphocyte的核体。然后，我们建立了跨度级别的注意力引导机制，通过同时学习原始的 histopathology 图像的粗级特征和我们设计的lymphocyte density注意力来实现。在训练过程中，我们引入了一个嵌入签名距离函数损失，以降低微小预测错误。实验结果表明，我们的提议方法在pancreatic tumors中的TLSs检测精度上明显超过了现状的segmentation-based算法。此外，我们通过应用我们的方法来研究peripancreatic vascular invasion和TLSs的密切关系，并获得了一些临床 statistically significant的结果。
</details></li>
</ul>
<hr>
<h2 id="FocalErrorNet-Uncertainty-aware-focal-modulation-network-for-inter-modal-registration-error-estimation-in-ultrasound-guided-neurosurgery"><a href="#FocalErrorNet-Uncertainty-aware-focal-modulation-network-for-inter-modal-registration-error-estimation-in-ultrasound-guided-neurosurgery" class="headerlink" title="FocalErrorNet: Uncertainty-aware focal modulation network for inter-modal registration error estimation in ultrasound-guided neurosurgery"></a>FocalErrorNet: Uncertainty-aware focal modulation network for inter-modal registration error estimation in ultrasound-guided neurosurgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14520">http://arxiv.org/abs/2307.14520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soorena Salari, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao</li>
<li>for: brain tumor resection, accurate removal of cancerous tissues while preserving eloquent regions is crucial to the safety and outcomes of the treatment.</li>
<li>methods: intra-operative ultrasound (iUS) has been adopted to provide real-time images to track brain shift, and inter-modal (i.e., MRI-iUS) registration is often required to update the pre-surgical plan.</li>
<li>results: a novel deep learning technique based on 3D focal modulation in conjunction with uncertainty estimation to accurately assess MRI-iUS registration errors for brain tumor surgery, with an estimation error of 0.59+-0.57 mm.Here is the result in Simplified Chinese text:</li>
<li>for: 脑肿瘤镜下手术，准确除除肿瘤组织，保留语言功能区域是治疗安全和结果的关键。</li>
<li>methods: 使用实时图像跟踪脑shift的术前评估，并通过多Modal（i.e., MRI-iUS）注册更新预后方案。</li>
<li>results: 一种基于3D焦点调制的深度学习技术，以及不确定度估计，为脑肿瘤手术中MRI-iUS注册错误的准确评估，错误估计值为0.59+-0.57 mm。<details>
<summary>Abstract</summary>
In brain tumor resection, accurate removal of cancerous tissues while preserving eloquent regions is crucial to the safety and outcomes of the treatment. However, intra-operative tissue deformation (called brain shift) can move the surgical target and render the pre-surgical plan invalid. Intra-operative ultrasound (iUS) has been adopted to provide real-time images to track brain shift, and inter-modal (i.e., MRI-iUS) registration is often required to update the pre-surgical plan. Quality control for the registration results during surgery is important to avoid adverse outcomes, but manual verification faces great challenges due to difficult 3D visualization and the low contrast of iUS. Automatic algorithms are urgently needed to address this issue, but the problem was rarely attempted. Therefore, we propose a novel deep learning technique based on 3D focal modulation in conjunction with uncertainty estimation to accurately assess MRI-iUS registration errors for brain tumor surgery. Developed and validated with the public RESECT clinical database, the resulting algorithm can achieve an estimation error of 0.59+-0.57 mm.
</details>
<details>
<summary>摘要</summary>
在脑肿瘤切除手术中，准确地移除癌细胞组织，同时保留eloquent区域的安全和效果是致命的。然而，操作期间脑部塑形（brain shift）可能导致手术目标移动，使原先的预期计划无效。为了提供实时图像，脑部ultrasound（iUS）已被采用，而inter-modal（i.e., MRI-iUS） регистрация经常需要更新预期计划。在手术中质量控制注册结果的重要性，由于3D视化和iUS的低对比度，导致手动验证困难。因此，我们提出了一种基于3D焦点修饰的深度学习技术，以确定MRI-iUS注册错误的准确评估方法。与公共RESECT临床数据库的开发和验证结果显示，这种算法可以实现注册错误的估计误差为0.59±0.57毫米。
</details></li>
</ul>
<hr>
<h2 id="Phenotype-preserving-metric-design-for-high-content-image-reconstruction-by-generative-inpainting"><a href="#Phenotype-preserving-metric-design-for-high-content-image-reconstruction-by-generative-inpainting" class="headerlink" title="Phenotype-preserving metric design for high-content image reconstruction by generative inpainting"></a>Phenotype-preserving metric design for high-content image reconstruction by generative inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14436">http://arxiv.org/abs/2307.14436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaibhav Sharma, Artur Yakimovich</li>
<li>for: 这个论文主要用于研究高通量微scopic影像数据的自动化处理和修复技术，以提高生物系统学和药物层次creening应用。</li>
<li>methods: 这个论文使用了现有的image inpainting技术，如DeepFill V2和Edge Connect，对高通量fluorescence microscopy图像进行修复和Restoration。</li>
<li>results: 研究发现，通过精心调整和少量数据，这些技术可以准确修复微scopic影像，并且Restoration质量与图像区域大小相关。此外，提出了一种新的phenotype-preserving度量设计策略，以控制修复质量并避免不良修复。<details>
<summary>Abstract</summary>
In the past decades, automated high-content microscopy demonstrated its ability to deliver large quantities of image-based data powering the versatility of phenotypic drug screening and systems biology applications. However, as the sizes of image-based datasets grew, it became infeasible for humans to control, avoid and overcome the presence of imaging and sample preparation artefacts in the images. While novel techniques like machine learning and deep learning may address these shortcomings through generative image inpainting, when applied to sensitive research data this may come at the cost of undesired image manipulation. Undesired manipulation may be caused by phenomena such as neural hallucinations, to which some artificial neural networks are prone. To address this, here we evaluate the state-of-the-art inpainting methods for image restoration in a high-content fluorescence microscopy dataset of cultured cells with labelled nuclei. We show that architectures like DeepFill V2 and Edge Connect can faithfully restore microscopy images upon fine-tuning with relatively little data. Our results demonstrate that the area of the region to be restored is of higher importance than shape. Furthermore, to control for the quality of restoration, we propose a novel phenotype-preserving metric design strategy. In this strategy, the size and count of the restored biological phenotypes like cell nuclei are quantified to penalise undesirable manipulation. We argue that the design principles of our approach may also generalise to other applications.
</details>
<details>
<summary>摘要</summary>
在过去的几十年中，自动化高内容微scopia技术已经证明了它可以提供大量的图像数据，为phenotypic drug screening和系统生物学应用提供了多样化的能力。然而，随着图像数据的大小的增长，人类控制、避免和消除图像和样本准备 artifacts在图像中的成本变得不可持续。 novels技术如机器学习和深度学习可能会解决这些缺陷通过生成图像填充，但是当应用于敏感研究数据时，这可能会导致不желатель的图像修饰。这些修饰可能会由人工神经网络的神经抑制引起，导致图像修饰。为了解决这个问题，我们在高内容染料微scopiadataset中评估了state-of-the-art填充方法的图像修复能力。我们发现，Architecture如DeepFill V2和Edge Connect可以在高度微scopiadataset中 faithful restore microscopy images，并且只需要少量的数据进行微调。我们的结果表明，图像修复的区域大小比较重要于形态。此外，为了控制修复质量，我们提出了一种新的phenotype-preserving度量设计策略。在这种策略中，修复的生物fenotypes，如细胞核的大小和数量，被量化以 penalize不желатель的修饰。我们认为，我们的设计原则可能也会总结到其他应用中。
</details></li>
</ul>
<hr>
<h2 id="Optimization-of-Image-Acquisition-for-Earth-Observation-Satellites-via-Quantum-Computing"><a href="#Optimization-of-Image-Acquisition-for-Earth-Observation-Satellites-via-Quantum-Computing" class="headerlink" title="Optimization of Image Acquisition for Earth Observation Satellites via Quantum Computing"></a>Optimization of Image Acquisition for Earth Observation Satellites via Quantum Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14419">http://arxiv.org/abs/2307.14419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antón Makarov, Márcio M. Taddei, Eneko Osaba, Giacomo Franceschetto, Esther Villar-Rodriguez, Izaskun Oregi</li>
<li>for: 这篇论文的目的是对于卫星图像获取时间调度问题进行优化，以找到在给定轨道通过时间下最佳的图像子集。</li>
<li>methods: 这篇论文使用了两种QUBO表述方法来解决这个问题，并且使用不同的缓存处理技术来处理非易式约束。</li>
<li>results: 实验结果显示，不同的表述方法和缓存处理技术对于解决这个问题有很大的影响，而且Current quantum computers可以解决的问题型例子限制在一定的大小上。<details>
<summary>Abstract</summary>
Satellite image acquisition scheduling is a problem that is omnipresent in the earth observation field; its goal is to find the optimal subset of images to be taken during a given orbit pass under a set of constraints. This problem, which can be modeled via combinatorial optimization, has been dealt with many times by the artificial intelligence and operations research communities. However, despite its inherent interest, it has been scarcely studied through the quantum computing paradigm. Taking this situation as motivation, we present in this paper two QUBO formulations for the problem, using different approaches to handle the non-trivial constraints. We compare the formulations experimentally over 20 problem instances using three quantum annealers currently available from D-Wave, as well as one of its hybrid solvers. Fourteen of the tested instances have been obtained from the well-known SPOT5 benchmark, while the remaining six have been generated ad-hoc for this study. Our results show that the formulation and the ancilla handling technique is crucial to solve the problem successfully. Finally, we also provide practical guidelines on the size limits of problem instances that can be realistically solved on current quantum computers.
</details>
<details>
<summary>摘要</summary>
卫星图像获取计划是地球观测领域中一个普遍存在的问题，其目标是在给定的轨道过程中选择最佳的图像子集，以满足一系列约束。这个问题可以被模型为 combinatorial optimization 问题，在人工智能和运筹学社区中已经得到了广泛的研究。然而，即使它具有潜在的兴趣，它在量子计算理解中却很少被研究。在这种情况下，我们在这篇论文中提出了两种 QUBO 表示方法，使用不同的方法来处理非质量约束。我们通过实验测试了这些表示方法，使用 D-Wave 提供的三个量子泵浸器和一个混合解决方案。我们测试的问题实例数量为 20，其中 14 个来自 SPOT5 标准准则，另外 6 个是为本研究而生成的。我们的结果表明，表示方法和卵处理技术是解决问题的关键。此外，我们还提供了现有量子计算机的实际问题大小限制。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/27/eess.IV_2023_07_27/" data-id="cllsj1rpg008rpf88aek22wb1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/cs.LG_2023_07_26/" class="article-date">
  <time datetime="2023-07-25T16:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/26/cs.LG_2023_07_26/">cs.LG - 2023-07-26 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Fluorescent-Neuronal-Cells-v2-Multi-Task-Multi-Format-Annotations-for-Deep-Learning-in-Microscopy"><a href="#Fluorescent-Neuronal-Cells-v2-Multi-Task-Multi-Format-Annotations-for-Deep-Learning-in-Microscopy" class="headerlink" title="Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy"></a>Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14243">http://arxiv.org/abs/2307.14243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Clissa, Antonio Macaluso, Roberto Morelli, Alessandra Occhinegro, Emiliana Piscitiello, Ludovico Taddei, Marco Luppi, Roberto Amici, Matteo Cerri, Timna Hitrec, Lorenzo Rinaldi, Antonio Zoccoli</li>
<li>for: This paper is written for researchers and scientists in the fields of life sciences and deep learning, with the goal of facilitating innovative research and methodological advancements in fluorescence microscopy analysis.</li>
<li>methods: The paper provides a collection of fluorescence microscopy images and ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting. The images are annotated with diverse markers to highlight the anatomical or functional characteristics of rodent neuronal cells.</li>
<li>results: The paper aims to facilitate breakthroughs in fluorescence microscopy analysis and promote cutting-edge discoveries in life sciences by providing a comprehensive and accessible dataset for researchers to explore and benchmark their methods.<details>
<summary>Abstract</summary>
Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Sciences and Deep Learning. This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics. Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting. The contribution is two-fold. First, given the variety of annotations and their accessible formats, we envision our work facilitating methodological advancements in computer vision approaches for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas. Second, by enabling extensive exploration and benchmarking, we hope Fluorescent Neuronal Cells v2 will catalyze breakthroughs in fluorescence microscopy analysis and promote cutting-edge discoveries in life sciences. The data are available at: https://amsacta.unibo.it/id/eprint/7347
</details>
<details>
<summary>摘要</summary>
fluorescent neuronal cells v2 是一个包含 fluorescence microscopy 图像和相应的真实标注的集合，旨在推动生命科学和深度学习领域的创新研究。这个数据集包括三个图像集，其中 rodent neuronal cells 的核心和细胞质是使用不同标记物来高亮其形态或功能特征。同时，我们提供了真实标注数据，用于多种学习任务，包括semantic segmentation、object detection和计数。我们的贡献是两重。首先，由于数据集的多样性和可访问的格式，我们期望我们的工作会促进计算机视觉方法的进步，包括分割、检测、特征学习、自监学习、转移学习等领域。其次，通过提供广泛的探索和测试，我们希望 fluorescent neuronal cells v2 会促进 fluorescence microscopy 分析的进步，并促进生命科学的前沿研究。数据可以在以下地址获取：https://amsacta.unibo.it/id/eprint/7347
</details></li>
</ul>
<hr>
<h2 id="Evolving-Multi-Objective-Neural-Network-Controllers-for-Robot-Swarms"><a href="#Evolving-Multi-Objective-Neural-Network-Controllers-for-Robot-Swarms" class="headerlink" title="Evolving Multi-Objective Neural Network Controllers for Robot Swarms"></a>Evolving Multi-Objective Neural Network Controllers for Robot Swarms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14237">http://arxiv.org/abs/2307.14237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karl Mason, Sabine Hauert</li>
<li>for: 本研究旨在提出一种多目标进化神经网络控制器来解决群体 робоット控制问题，以实现多个目标的同时满足。</li>
<li>methods: 该研究使用了一种多目标进化神经网络方法，通过在低精度Python模拟环境中训练控制器，然后在Webots高精度模拟环境中测试和评估控制器。</li>
<li>results: 研究结果表明，提出的方法可以有效地控制每个机器人，并且可以根据目标权重的调整而变化机器人群体的行为。同时，研究还证明了这种控制器可以在高精度模拟环境中进行规模化扩展，无需进一步 retrained。<details>
<summary>Abstract</summary>
Many swarm robotics tasks consist of multiple conflicting objectives. This research proposes a multi-objective evolutionary neural network approach to developing controllers for swarms of robots. The swarm robot controllers are trained in a low-fidelity Python simulator and then tested in a high-fidelity simulated environment using Webots. Simulations are then conducted to test the scalability of the evolved multi-objective robot controllers to environments with a larger number of robots. The results presented demonstrate that the proposed approach can effectively control each of the robots. The robot swarm exhibits different behaviours as the weighting for each objective is adjusted. The results also confirm that multi-objective neural network controllers evolved in a low-fidelity simulator can be transferred to high-fidelity simulated environments and that the controllers can scale to environments with a larger number of robots without further retraining needed.
</details>
<details>
<summary>摘要</summary>
多数蜂群控制任务包含多个冲突目标。本研究提出了一种多目标进化神经网络方法来开发蜂群机器人控制器。蜂群机器人控制器在低精度Python模拟器中训练，然后在使用Webots高精度模拟环境进行测试。为了评估算法的扩展性，在不同目标权重的情况下进行了多个 simulations。结果表明，提议的方法可以有效控制每个机器人。机器人蜂群在不同目标权重下展现出不同的行为。结果还证明了在低精度模拟器中进行多目标神经网络控制器的进化后，可以将控制器转移到高精度模拟环境中，并且控制器可以扩展到包含更多机器人的环境无需进行进一步 retrained。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-Competitive-Near-Cold-start-Recommenders-for-Language-and-Item-based-Preferences"><a href="#Large-Language-Models-are-Competitive-Near-Cold-start-Recommenders-for-Language-and-Item-based-Preferences" class="headerlink" title="Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences"></a>Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14225">http://arxiv.org/abs/2307.14225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, Lucas Dixon</li>
<li>for: 研究使用大语言模型（LLM）来提供建议，并比较其与现有的项目相关的协同维度（CF）方法。</li>
<li>methods: 采集了基于项目和语言的偏好的用户评分数据，并使用这些数据来训练LLM。</li>
<li>results: LLM可以在冷启动情况下提供竞争力强的建议，尤其是在没有超参数（zero-shot）或只有几个标签（few-shot）的情况下。这些结果表明语言基本偏好表示更加可解释和可读取。<details>
<summary>Abstract</summary>
Traditional recommender systems leverage users' item preference history to recommend novel content that users may like. However, modern dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input. Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.
</details>
<details>
<summary>摘要</summary>
传统推荐系统利用用户ITEM喜好历史来推荐新内容，但现代对话界面允许用户通过语言基于喜好表达新的可能性。 inspirited by recent successes of prompting paradigms for large language models (LLMs), we investigate their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.
</details></li>
</ul>
<hr>
<h2 id="Online-Modeling-and-Monitoring-of-Dependent-Processes-under-Resource-Constraints"><a href="#Online-Modeling-and-Monitoring-of-Dependent-Processes-under-Resource-Constraints" class="headerlink" title="Online Modeling and Monitoring of Dependent Processes under Resource Constraints"></a>Online Modeling and Monitoring of Dependent Processes under Resource Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14208">http://arxiv.org/abs/2307.14208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanapol Kosolwattana, Huazheng Wang, Ying Lin</li>
<li>for: 监测受限资源的依赖过程集体，检测异常事件是非常重要的。</li>
<li>methods: 提出了一种基于在线协同学习的资源分配策略，以适应高风险过程的探索和依赖动力学的探索。</li>
<li>results: 通过理论分析和实验验证，效果良好。<details>
<summary>Abstract</summary>
Monitoring a population of dependent processes under limited resources is critical for abnormal events detection. A novel online collaborative learning method is proposed to adaptively allocate the resources for exploitation of high-risk processes and exploration of dependent dynamics. Efficiency of the proposed method is proved through theoretical analysis and experiments.
</details>
<details>
<summary>摘要</summary>
监测具有限制资源的依赖过程群体是检测异常事件的关键。一种基于在线合作学习的新方法是提议的，以适应分配资源以便利用高风险过程的探索和依赖动态的探索。我们通过理论分析和实验证明了提议的方法的效率。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Random-Forest-and-Support-Vector-Machine-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Plant-Filter-Cake-Modeling"><a href="#Application-of-Random-Forest-and-Support-Vector-Machine-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Plant-Filter-Cake-Modeling" class="headerlink" title="Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling"></a>Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14199">http://arxiv.org/abs/2307.14199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoume Kazemi, Davood Moradkhani, Alireza Abbas Alipour</li>
<li>for: 这个研究旨在预测压缩滤过程中的萃取物湿度。</li>
<li>methods: 研究使用Random Forest（RF）和Support Vector Machine（SVM）模型来模型压缩滤过程。</li>
<li>results: 研究发现，Random Forest Regression（RFR）模型在预测萃取物湿度方面比Support Vector Regression（SVR）模型表现更好，RFR模型的准确预测率较高。<details>
<summary>Abstract</summary>
The hydrometallurgical method of zinc production involves leaching zinc from ore and then separating the solid residue from the liquid solution by pressure filtration. This separation process is very important since the solid residue contains some moisture that can reduce the amount of zinc recovered. This study modeled the pressure filtration process through Random Forest (RF) and Support Vector Machine (SVM). The models take continuous variables (extracted features) from the lab samples as inputs. Thus, regression models namely Random Forest Regression (RFR) and Support Vector Regression (SVR) were chosen. A total dataset was obtained during the pressure filtration process in two conditions: 1) Polypropylene (S1) and 2) Polyester fabrics (S2). To predict the cake moisture, solids concentration (0.2 and 0.38), temperature (35 and 65 centigrade), pH (2, 3.5, and 5), pressure, cake thickness (14, 20, 26, and 34 mm), air-blow time (2, 10 and 15 min) and filtration time were applied as input variables. The models' predictive accuracy was evaluated by the coefficient of determination (R2) parameter. The results revealed that the RFR model is superior to the SVR model for cake moisture prediction.
</details>
<details>
<summary>摘要</summary>
《氧化锌生产技术中的压力分离过程是非常重要的，因为固体剩下物含有一定的湿度，这可能会减少锌的回收量。本研究使用Random Forest（RF）和Support Vector Machine（SVM）模型来模拟压力分离过程。这些模型接受实验室样本中的连续变量（提取特征）作为输入。因此，回归模型Random Forest Regression（RFR）和Support Vector Regression（SVR）被选择。一个总体数据集在压力分离过程中得到了，包括了两种条件：1）聚乙烯（S1）和2）聚醚纤维（S2）。为预测固体湿度，输入变量包括了粉煤浓度（0.2和0.38）、温度（35和65℃）、pH（2、3.5和5）、压力、压力分离厚度（14、20、26和34mm）、气流时间（2、10和15分）和过滤时间。模型的预测精度被评估通过R2参数。结果显示，RFR模型在预测固体湿度方面比SVR模型更高。》Note: Please note that the translation is in Simplified Chinese, and the word order and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Learning-of-Discrete-Continuous-Computation-Graphs"><a href="#Efficient-Learning-of-Discrete-Continuous-Computation-Graphs" class="headerlink" title="Efficient Learning of Discrete-Continuous Computation Graphs"></a>Efficient Learning of Discrete-Continuous Computation Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14193">http://arxiv.org/abs/2307.14193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nec-research/dccg">https://github.com/nec-research/dccg</a></li>
<li>paper_authors: David Friede, Mathias Niepert</li>
<li>for: 该论文旨在探讨混合抽象和连续模型在超级vised和强化学习中的应用。</li>
<li>methods: 该论文使用了泛化的推理抽象技术，并提出了两种新的策略来解决训练过程中的问题。</li>
<li>results: 实验结果显示，使用混合抽象和连续模型可以训练更复杂的模型，并且这些模型在一些基准数据集上的泛化性比其连续counterpart更好。<details>
<summary>Abstract</summary>
Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.
</details>
<details>
<summary>摘要</summary>
多种超visited和强化学习模型受益于混合 discrete和连续模型组件。结构化的end-to-end学习可以使模型更加易于理解和掌控。常见的方法是通过将抽象概率分布 integrate到神经网络中使用随机softmax技巧。先前的工作主要集中在单个执行路径上的 computation graphs上。我们分析了多个级别的随机计算图，并证明这些模型的参数优化具有挑战性，主要是因为小Gradient和地方最小值。我们 then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models that cannot be trained with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.
</details></li>
</ul>
<hr>
<h2 id="A-comparison-of-machine-learning-surrogate-models-of-street-scale-flooding-in-Norfolk-Virginia"><a href="#A-comparison-of-machine-learning-surrogate-models-of-street-scale-flooding-in-Norfolk-Virginia" class="headerlink" title="A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia"></a>A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14185">http://arxiv.org/abs/2307.14185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diana McSpadden, Steven Goldenberg, Binata Roy, Malachi Schram, Jonathan L. Goodall, Heather Richter</li>
<li>for: 这个研究是为了解决低洼海岸城市（如诺福克，维iginia）的街道洪水问题，这些问题会影响交通和排水系统，并可能导致财产损害。</li>
<li>methods: 这个研究使用了一种前一版的代理模型（基于随机森林算法），以及两种深度学习模型：Long Short-Term Memory（LSTM）和Gated Recurrent Unit（GRU）。</li>
<li>results: 研究发现，使用LSTM和GRU深度学习模型可以提高预测uncertainty的交通和排水系统的性能，并且这些模型可以有效地 интеGRATE多种、多模态的特征。<details>
<summary>Abstract</summary>
Low-lying coastal cities, exemplified by Norfolk, Virginia, face the challenge of street flooding caused by rainfall and tides, which strain transportation and sewer systems and can lead to property damage. While high-fidelity, physics-based simulations provide accurate predictions of urban pluvial flooding, their computational complexity renders them unsuitable for real-time applications. Using data from Norfolk rainfall events between 2016 and 2018, this study compares the performance of a previous surrogate model based on a random forest algorithm with two deep learning models: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). This investigation underscores the importance of using a model architecture that supports the communication of prediction uncertainty and the effective integration of relevant, multi-modal features.
</details>
<details>
<summary>摘要</summary>
低海拔沿海城市，如尼科尔（Norfolk），面临洪水泛滥的挑战，这会压力交通和废水系统，并可能导致财产损害。虽然高精度的物理学基模型可以准确预测城市洪水，但计算复杂性使其不适用于实时应用。通过使用2016-2018年尼科尔降水事件的数据，本研究比较了之前的随机森林算法基于模型和两种深度学习模型：长期快速储存（LSTM）和闭合循环单元（GRU）的表现。这一研究强调使用一种支持预测不确定性的模型架构，并有效地结合相关的多模式特征。
</details></li>
</ul>
<hr>
<h2 id="Learning-Disentangled-Discrete-Representations"><a href="#Learning-Disentangled-Discrete-Representations" class="headerlink" title="Learning Disentangled Discrete Representations"></a>Learning Disentangled Discrete Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14151">http://arxiv.org/abs/2307.14151</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/david-friede/lddr">https://github.com/david-friede/lddr</a></li>
<li>paper_authors: David Friede, Christian Reimers, Heiner Stuckenschmidt, Mathias Niepert</li>
<li>for: 本研究旨在探讨抽象空间中的离散表示的优点，以及它们如何促进分离表示的学习。</li>
<li>methods: 研究者采用了一种特制的 categorical variational autoencoder（CVAE），以取代标准的 Gaussian variational autoencoder（VAE），以便更好地学习分离表示。</li>
<li>results: 研究者通过 analytical 和 empirical 的方法，证明了离散 VAE 在学习分离表示方面的优势，并提出了首个无监督的模型选择策略，以便寻找更好的分离表示模型。<details>
<summary>Abstract</summary>
Recent successes in image generation, model-based reinforcement learning, and text-to-image generation have demonstrated the empirical advantages of discrete latent representations, although the reasons behind their benefits remain unclear. We explore the relationship between discrete latent spaces and disentangled representations by replacing the standard Gaussian variational autoencoder (VAE) with a tailored categorical variational autoencoder. We show that the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions, acting as an efficient inductive prior for disentangled representations. We provide both analytical and empirical findings that demonstrate the advantages of discrete VAEs for learning disentangled representations. Furthermore, we introduce the first unsupervised model selection strategy that favors disentangled representations.
</details>
<details>
<summary>摘要</summary>
最近的图像生成、模型基于返回学习和文本到图像生成的成功表明了不连续含义空间的实际优势，即使其中的原因还未得到清晰解释。我们调查了不连续含义空间和分离表示之间的关系，并将标准的 Gaussian 变量自动机（VAE）替换为适应 categorical 变量自动机。我们发现， categorical 分布下的网格结构可以有效地解决多变量 Gaussian 分布中的旋转不变性问题，并作为较为有效的 inductive prior  для分离表示。我们提供了both analytical和empirical的发现，证明不连续 VAE 在学习分离表示方面的优势。此外，我们还介绍了首个无监督模型选择策略，该策略会偏好分离表示。
</details></li>
</ul>
<hr>
<h2 id="Toward-Design-of-Synthetic-Active-Inference-Agents-by-Mere-Mortals"><a href="#Toward-Design-of-Synthetic-Active-Inference-Agents-by-Mere-Mortals" class="headerlink" title="Toward Design of Synthetic Active Inference Agents by Mere Mortals"></a>Toward Design of Synthetic Active Inference Agents by Mere Mortals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14145">http://arxiv.org/abs/2307.14145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bert de Vries</li>
<li>for: 这个论文旨在解决智能设备边缘处理中active inference代理的实现问题，以便快速普及活跃推理技术。</li>
<li>methods: 论文提出了一个工具箱，用于支持非专家工程师开发工作的活跃推理代理。该工具箱具有加速policy探索的能力，以便在限制性的边缘设备上实现效果。</li>
<li>results: 论文预示了一个在进程中使用该工具箱的示例应用，显示了在边缘设备上实现活跃推理代理的可能性。<details>
<summary>Abstract</summary>
The theoretical properties of active inference agents are impressive, but how do we realize effective agents in working hardware and software on edge devices? This is an interesting problem because the computational load for policy exploration explodes exponentially, while the computational resources are very limited for edge devices. In this paper, we discuss the necessary features for a software toolbox that supports a competent non-expert engineer to develop working active inference agents. We introduce a toolbox-in-progress that aims to accelerate the democratization of active inference agents in a similar way as TensorFlow propelled applications of deep learning technology.
</details>
<details>
<summary>摘要</summary>
理论上，活动推理代理的特性很吸引人，但实际如何在工作硬件和软件上实现有效的代理呢？这是一个有趣的问题，因为策略探索的计算荷载会指数增长，而边缘设备的计算资源却很有限。在这篇文章中，我们讨论了实现非专业工程师开发工作的活动推理代理所需的必要特性。我们介绍了一个进度中的工具箱，旨在通过减少代理开发的计算负担，使活动推理代理在边缘设备上更加普及。
</details></li>
</ul>
<hr>
<h2 id="Piecewise-Stationary-Combinatorial-Semi-Bandit-with-Causally-Related-Rewards"><a href="#Piecewise-Stationary-Combinatorial-Semi-Bandit-with-Causally-Related-Rewards" class="headerlink" title="Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards"></a>Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14138">http://arxiv.org/abs/2307.14138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behzad Nourani-Koliji, Steven Bilaj, Amir Rezaei Balef, Setareh Maghsudi</li>
<li>for:  solving the piecewise stationary combinatorial semi-bandit problem in a nonstationary environment with causally related rewards</li>
<li>methods: using the Upper Confidence Bound (UCB) algorithm with an adaptive approach that includes a change-point detector based on the Generalized Likelihood Ratio (GLR) test and a mechanism to trace the variations of the underlying graph structure</li>
<li>results: establishing a regret upper bound that reflects the effects of the number of structural- and distribution changes on the performance, and demonstrating superior performance in numerical experiments compared to state-of-the-art benchmarks.Here’s the Chinese translation of the three points:</li>
<li>for: 解决 piecewise stationary  combinatorial semi-bandit 问题在不站台环境中，其中 reward 具有 causal 关系</li>
<li>methods: 使用 Upper Confidence Bound (UCB) 算法，并采用适应的方法，包括基于 Generalized Likelihood Ratio (GLR) 测试的 change-point 探测器和跟踪Underlying graph structure的机制</li>
<li>results: 确定了不同 structural- 和 distribution 变化的影响，并在实验中证明了与 state-of-the-art benchmarks 相比，提出的方案具有更高的实用性。<details>
<summary>Abstract</summary>
We study the piecewise stationary combinatorial semi-bandit problem with causally related rewards. In our nonstationary environment, variations in the base arms' distributions, causal relationships between rewards, or both, change the reward generation process. In such an environment, an optimal decision-maker must follow both sources of change and adapt accordingly. The problem becomes aggravated in the combinatorial semi-bandit setting, where the decision-maker only observes the outcome of the selected bundle of arms. The core of our proposed policy is the Upper Confidence Bound (UCB) algorithm. We assume the agent relies on an adaptive approach to overcome the challenge. More specifically, it employs a change-point detector based on the Generalized Likelihood Ratio (GLR) test. Besides, we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments. Finally, our algorithm integrates a mechanism to trace the variations of the underlying graph structure, which captures the causal relationships between the rewards in the bandit setting. Theoretically, we establish a regret upper bound that reflects the effects of the number of structural- and distribution changes on the performance. The outcome of our numerical experiments in real-world scenarios exhibits applicability and superior performance of our proposal compared to the state-of-the-art benchmarks.
</details>
<details>
<summary>摘要</summary>
我们研究 Piecewise 站ARY  combinatorial 半带�û问题，其中 reward 的生成过程受到基 arms 的分布变化、 causal 关系 between rewards 以及 Both 的变化影响。在这种非站ARY 环境中，一个优化的决策者需要同时考虑这些变化并适应应对。在 combinatorial 半带 Setting 中，决策者只能观察选择的 bundle of arms 的结果。我们的提议的策略是使用 Upper Confidence Bound (UCB) 算法。我们假设agent 采用adaptive approach来解决这个挑战。具体来说，它使用基于 Generalized Likelihood Ratio (GLR) 测试的变化点检测器。此外，我们引入了 group restart 作为一种新的重启策略，用于在结构化环境中决策过程中。最后，我们的算法包括跟踪Underlying graph structure的变化，这些变化捕捉了 reward 在半带 Setting 中的 causal 关系。从理论角度来看，我们确立了 regret Upper bound，这个 Upper bound 反映了变量的数量和分布变化对性能的影响。我们的numerical experiments 在实际场景中展示了我们的提议的应用和优于现有benchmarks。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Generative-Modeling-with-Limited-Data-Few-Shots-and-Zero-Shot"><a href="#A-Survey-on-Generative-Modeling-with-Limited-Data-Few-Shots-and-Zero-Shot" class="headerlink" title="A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot"></a>A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14397">http://arxiv.org/abs/2307.14397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sutd-visual-computing-group/awesome-generative-modeling-under-data-constraints">https://github.com/sutd-visual-computing-group/awesome-generative-modeling-under-data-constraints</a></li>
<li>paper_authors: Milad Abdollahzadeh, Touba Malekzadeh, Christopher T. H. Teo, Keshigeyan Chandrasegaran, Guimeng Liu, Ngai-Man Cheung</li>
<li>for: 本研究旨在探讨生成模型在数据约束下学习，即生成模型学习数据分布的新数据。</li>
<li>methods: 本研究使用了限制数据量、几个shot和零shot等数据约束来学习生成模型。</li>
<li>results: 研究发现了生成模型在数据约束下的学习 task 和方法之间存在交互关系，并提出了未来研究的潜在方向。Here’s a more detailed explanation of each point:</li>
<li>for: The paper is focused on generative modeling under data constraint, specifically exploring the task of learning to generate new data that is statistically similar to the training data distribution, but with limited data availability.</li>
<li>methods: The paper discusses the use of limited data, few shots, and zero shots as data constraints for generative modeling, and proposes two taxonomies for GM-DC tasks and approaches.</li>
<li>results: The study highlights research gaps, research trends, and potential avenues for future exploration in the field of GM-DC, and provides a comprehensive overview of the current state of the field.<details>
<summary>Abstract</summary>
In machine learning, generative modeling aims to learn to generate new data statistically similar to the training data distribution. In this paper, we survey learning generative models under limited data, few shots and zero shot, referred to as Generative Modeling under Data Constraint (GM-DC). This is an important topic when data acquisition is challenging, e.g. healthcare applications. We discuss background, challenges, and propose two taxonomies: one on GM-DC tasks and another on GM-DC approaches. Importantly, we study interactions between different GM-DC tasks and approaches. Furthermore, we highlight research gaps, research trends, and potential avenues for future exploration. Project website: https://gmdc-survey.github.io.
</details>
<details>
<summary>摘要</summary>
在机器学习中，生成模型目标是学习生成新数据，与训练数据分布 statistically similar。在这篇论文中，我们对受限数据的生成模型学习进行报告，包括几 shot、零 shot 等。这是数据收集困难的场景，如医疗应用。我们介绍背景、挑战和两种分类：一种是生成模型下数据约束任务（GM-DC），另一种是生成模型下数据约束方法（GM-DC）。重要的是，我们研究不同的 GM-DC 任务和方法之间的交互。此外，我们还提出了未来探索的研究漏洞、趋势和潜在的发展方向。项目网站：https://gmdc-survey.github.io。Note: The translation is done using Google Translate and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Developing-and-Evaluating-Tiny-to-Medium-Sized-Turkish-BERT-Models"><a href="#Developing-and-Evaluating-Tiny-to-Medium-Sized-Turkish-BERT-Models" class="headerlink" title="Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models"></a>Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14134">http://arxiv.org/abs/2307.14134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Himmet Toprak Kesgin, Muzaffer Kaan Yuce, Mehmet Fatih Amasyali</li>
<li>for: This paper aims to bridge the research gap in less-resourced languages by introducing and evaluating tiny, mini, small, and medium-sized uncased Turkish BERT models.</li>
<li>methods: The authors trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and zero-shot classification.</li>
<li>results: Despite their smaller size, the models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是bridging the research gap in less-resourced languages, 通过引入和评估 Turkish BERT 模型的不同大小。</li>
<li>methods: 作者使用了多种数据集，包括多个来源的文本，用于训练这些模型，并在多个任务上进行测试，包括偏好预测、情感分析、新闻分类和零 shot 分类。</li>
<li>results:  despite their smaller size, the models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times.<details>
<summary>Abstract</summary>
This study introduces and evaluates tiny, mini, small, and medium-sized uncased Turkish BERT models, aiming to bridge the research gap in less-resourced languages. We trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and, zero-shot classification. Despite their smaller size, our models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times. Our findings provide valuable insights into the development and application of smaller language models, especially in the context of the Turkish language.
</details>
<details>
<summary>摘要</summary>
Note:* "tiny" is 微小 (wēixiǎo) in Simplified Chinese* "mini" is 小型 (xiǎoxīng) in Simplified Chinese* "small" is 小 (xiǎo) in Simplified Chinese* "medium-sized" is 中等 (zhōngděng) in Simplified Chinese* "uncased" is 无框 (wúkē) in Simplified Chinese* "mask prediction" is 面 predicate (miàn zhèng) in Simplified Chinese* "sentiment analysis" is 情感分析 (qínggǎn fēnxiǎn) in Simplified Chinese* "news classification" is 新闻分类 (xīnwén fēnclass) in Simplified Chinese* "zero-shot classification" is 零枪分类 (zhèng qiāng fēnclass) in Simplified Chinese
</details></li>
</ul>
<hr>
<h2 id="GraphRNN-Revisited-An-Ablation-Study-and-Extensions-for-Directed-Acyclic-Graphs"><a href="#GraphRNN-Revisited-An-Ablation-Study-and-Extensions-for-Directed-Acyclic-Graphs" class="headerlink" title="GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs"></a>GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14109">http://arxiv.org/abs/2307.14109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taniya Das, Mark Koch, Maya Ravichandran, Nikhil Khatri</li>
<li>for: 学习图格生成模型</li>
<li>methods: 使用深度学习架构GraphRNN，并对基线模型进行评估和ablation study</li>
<li>results: 发现BFS traversal对模型性能有贡献，并将GraphRNN扩展到生成指定图的方法得到了显著改进。Here’s the breakdown of each point in English:</li>
<li>for: The paper is written for learning generative models for graphs using deep learning-based architectures.</li>
<li>methods: The paper uses a reproduced implementation of the GraphRNN architecture and evaluates it against baseline models using new metrics. The authors also perform an ablation study to analyze the contribution of the BFS traversal to model performance.</li>
<li>results: The authors find that the BFS traversal contributes significantly to model performance and extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort. They demonstrate significant improvement over a directed-multiclass variant of GraphRNN on a real-world dataset.<details>
<summary>Abstract</summary>
GraphRNN is a deep learning-based architecture proposed by You et al. for learning generative models for graphs. We replicate the results of You et al. using a reproduced implementation of the GraphRNN architecture and evaluate this against baseline models using new metrics. Through an ablation study, we find that the BFS traversal suggested by You et al. to collapse representations of isomorphic graphs contributes significantly to model performance. Additionally, we extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort. We demonstrate that this method improves significantly over a directed-multiclass variant of GraphRNN on a real-world dataset.
</details>
<details>
<summary>摘要</summary>
GRAPHRNN是一种深度学习基于架构，由尤等人提出用于学习图生成模型。我们对GRAPHRNN архитектура进行了重现，并使用新的基准模型进行评估。通过一项减少研究，我们发现了You等人提出的深度首次旋转（BFS） traverse 可以帮助模型性能。此外，我们还将GRAPHRNN扩展为生成直接无环图，通过将BFS traverse 替换为拓扑排序。我们示出这种方法可以在真实世界数据上提高表现。
</details></li>
</ul>
<hr>
<h2 id="Actions-Speak-What-You-Want-Provably-Sample-Efficient-Reinforcement-Learning-of-the-Quantal-Stackelberg-Equilibrium-from-Strategic-Feedbacks"><a href="#Actions-Speak-What-You-Want-Provably-Sample-Efficient-Reinforcement-Learning-of-the-Quantal-Stackelberg-Equilibrium-from-Strategic-Feedbacks" class="headerlink" title="Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks"></a>Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14085">http://arxiv.org/abs/2307.14085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Chen, Mengdi Wang, Zhuoran Yang</li>
<li>for: 学习一种量化Stackelberg平衡（QSE）在一个 episodic Markov 游戏中，其中有一个领导者和一个追随者结构。</li>
<li>methods: 使用 reinforcement learning（RL）和 maximum likelihood estimation（MLE）来学习领导者的决策问题，并使用模型之间的不确定性来实现在线和离线设置中的最佳性。</li>
<li>results: 提出了一些样本效率的算法，可以在函数approximation的上下文中解决领导者的决策问题，并且可以实现sublinear regret上限。 besides, 特别注重linear和偏函数设置下的计算效率。<details>
<summary>Abstract</summary>
We study reinforcement learning (RL) for learning a Quantal Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower structure. In specific, at the outset of the game, the leader announces her policy to the follower and commits to it. The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization problem induced by leader's policy. The goal of the leader is to find her optimal policy, which yields the optimal expected total return, by interacting with the follower and learning from data. A key challenge of this problem is that the leader cannot observe the follower's reward, and needs to infer the follower's quantal response model from his actions against leader's policies. We propose sample-efficient algorithms for both the online and offline settings, in the context of function approximation. Our algorithms are based on (i) learning the quantal response model via maximum likelihood estimation and (ii) model-free or model-based RL for solving the leader's decision making problem, and we show that they achieve sublinear regret upper bounds. Moreover, we quantify the uncertainty of these estimators and leverage the uncertainty to implement optimistic and pessimistic algorithms for online and offline settings. Besides, when specialized to the linear and myopic setting, our algorithms are also computationally efficient. Our theoretical analysis features a novel performance-difference lemma which incorporates the error of quantal response model, which might be of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究强化学习（RL）来学习一个量化StackelbergEquilibrium（QSE）在一个集合Markov游戏中，具有领导者-跟者结构。具体来说，在游戏开始时，领导者宣布她的策略给跟者，并将其约束。跟者根据领导者的策略采取一个量化回应策略， solving一个带有Entropy规范的策略优化问题。领导者的目标是找到她的优化策略，以便在与跟者交互和学习数据的过程中获得最佳预期总返回。一个关键问题是领导者无法观察跟者的奖励，她需要从跟者的行为中推断出跟者的量化回应模型。我们提出了一些样本效率的算法，包括在线和离线设置下的最大 likelihood估计和模型自由或模型基于RL，并证明它们可以实现sublinear regret上界。此外，我们还评估了这些估计器的不确定性，并利用这些不确定性来实现在线和离线设置下的optimistic和pessimistic算法。此外，当特化到线性和偏惰设置时，我们的算法也是计算高效的。我们的理论分析包括一个新的表现差lemm，该lemm incorporates the error of quantal response model，这可能是独立的兴趣。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-simulate-partially-known-spatio-temporal-dynamics-with-trainable-difference-operators"><a href="#Learning-to-simulate-partially-known-spatio-temporal-dynamics-with-trainable-difference-operators" class="headerlink" title="Learning to simulate partially known spatio-temporal dynamics with trainable difference operators"></a>Learning to simulate partially known spatio-temporal dynamics with trainable difference operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14395">http://arxiv.org/abs/2307.14395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Huang, Zhuoyuan Li, Hongsheng Liu, Zidong Wang, Hongye Zhou, Bin Dong, Bei Hua</li>
<li>for: 用神经网络模拟空间-时间动态的研究在最近几年得到了很多关注，但大多数现有方法采用纯数据驱动的黑盒模型，具有限制精度和可读性。</li>
<li>methods: 我们提出一种新的混合建模架构，称为PDE-Net++，它将可训练的差分算子与黑盒模型相结合，并包含部分先验知识。我们还提出了两种不同的差分层：可训练差分层（TFDL）和可训练动态差分层（TDDL）。</li>
<li>results: 数值实验表明，PDE-Net++的预测精度高于黑盒模型，并且在推理范围内具有更好的推理性能。<details>
<summary>Abstract</summary>
Recently, using neural networks to simulate spatio-temporal dynamics has received a lot of attention. However, most existing methods adopt pure data-driven black-box models, which have limited accuracy and interpretability. By combining trainable difference operators with black-box models, we propose a new hybrid architecture explicitly embedded with partial prior knowledge of the underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options called the trainable flipping difference layer (TFDL) and the trainable dynamic difference layer (TDDL) for the difference operators. Numerous numerical experiments have demonstrated that PDE-Net++ has superior prediction accuracy and better extrapolation performance than black-box models.
</details>
<details>
<summary>摘要</summary>
最近，使用神经网络模拟空间时间动力学受到了广泛关注。然而，现有大多数方法采用纯数据驱动黑盒模型，它们的准确率和可解释性受限。我们提出了一种新的混合架构，名为PDE-Net++，它通过与黑盒模型结合可编程的差异运算器来承载部分先验知识。此外，我们还提出了两种不同的选项，即可编程折衣层（TFDL）和可编程动态差异层（TDDL），用于差异运算器。数值实验证明，PDE-Net++在预测精度和推迟性方面都高于黑盒模型。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Domain-Discrepancy-Adjustment-for-Active-Multi-Domain-Adaptation"><a href="#Dynamic-Domain-Discrepancy-Adjustment-for-Active-Multi-Domain-Adaptation" class="headerlink" title="Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation"></a>Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14068">http://arxiv.org/abs/2307.14068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Liu, Bo Zhou, Zhipeng Zhao, Zening Liu</li>
<li>for: 本研究旨在提出一种新的多源不监督领域适应（MUDA）方法，以便从相关的源领域传递知识到未标注的目标领域。</li>
<li>methods: 我们提出了一种名为动态领域差异调整 для活动多频道适应（D3AAMDA）的新方法，它在训练过程中根据源领域和目标领域之间的分布差异度设置多源动态调整机制，以有效地利用每个源领域的本地有利特征信息。此外，我们还提出了一种多源活动边界选择策略（MABS），它通过一种引导的动态边界损失来设计高效的选择函数，以提高对目标领域的泛化。</li>
<li>results: 我们对常用的领域适应数据集进行了广泛的测试和比较，并证明了我们的方法的突出优势。<details>
<summary>Abstract</summary>
Multi-source unsupervised domain adaptation (MUDA) aims to transfer knowledge from related source domains to an unlabeled target domain. While recent MUDA methods have shown promising results, most focus on aligning the overall feature distributions across source domains, which can lead to negative effects due to redundant features within each domain. Moreover, there is a significant performance gap between MUDA and supervised methods. To address these challenges, we propose a novel approach called Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation (D3AAMDA). Firstly, we establish a multi-source dynamic modulation mechanism during the training process based on the degree of distribution differences between source and target domains. This mechanism controls the alignment level of features between each source domain and the target domain, effectively leveraging the local advantageous feature information within the source domains. Additionally, we propose a Multi-source Active Boundary Sample Selection (MABS) strategy, which utilizes a guided dynamic boundary loss to design an efficient query function for selecting important samples. This strategy achieves improved generalization to the target domain with minimal sampling costs. We extensively evaluate our proposed method on commonly used domain adaptation datasets, comparing it against existing UDA and ADA methods. The experimental results unequivocally demonstrate the superiority of our approach.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:多源无监督领域适应 (MUDA) 目标是将相关源领域知识传递到未标注目标领域。而现有的 MUDA 方法多数强调对所有源领域的特征分布进行对齐，这可能会导致因每个领域中的重复特征而出现负面的影响。此外，与超级vised方法相比，MUDA方法存在显著的性能差距。为解决这些挑战，我们提出了一种新的方法，即动态领域差异调整器 для活动多领域适应 (D3AAMDA)。首先，我们在训练过程中建立了多源动态调整机制，根据源领域和目标领域特征分布之间的差异程度来控制每个源领域和目标领域之间的特征对齐水平。这种机制有效地利用了每个源领域的本地有利特征信息。其次，我们提出了多源活动边界选择策略 (MABS)，该策略通过指导动态边界损失来设计高效的查询函数，选择重要的样本。这种策略可以在保持高度一致性的情况下，实现到目标领域的改进一致性，并且减少采样成本。我们对常用的领域适应 datasets 进行了广泛的测试和比较，与现有的 UDA 和 ADA 方法进行了比较。实验结果明确地表明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Hypergraph-Isomorphism-Computation"><a href="#Hypergraph-Isomorphism-Computation" class="headerlink" title="Hypergraph Isomorphism Computation"></a>Hypergraph Isomorphism Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14394">http://arxiv.org/abs/2307.14394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Feng, Jiashu Han, Shihui Ying, Yue Gao</li>
<li>for: 本研究旨在 Addressing the 图HS isomorphism problem, which is a fundamental problem in network analysis, and capturing both low-order and high-order structural information.</li>
<li>methods: 本 paper 提出了一种基于 Weisfiler-Lehman 测试算法的hypergraph isomorphism测试问题解决方案，并基于该算法提出了一个总体的 hypergraph Weisfeiler-Lehman kernel框架。</li>
<li>results: Results 表明，与其他常见的kernel-based方法相比，提出的方法在 hypergraph 分类任务中具有显著的优势，runtime running over 80 times faster when handling complex hypergraph structures。<details>
<summary>Abstract</summary>
The isomorphism problem is a fundamental problem in network analysis, which involves capturing both low-order and high-order structural information. In terms of extracting low-order structural information, graph isomorphism algorithms analyze the structural equivalence to reduce the solver space dimension, which demonstrates its power in many applications, such as protein design, chemical pathways, and community detection. For the more commonly occurring high-order relationships in real-life scenarios, the problem of hypergraph isomorphism, which effectively captures these high-order structural relationships, cannot be straightforwardly addressed using graph isomorphism methods. Besides, the existing hypergraph kernel methods may suffer from high memory consumption or inaccurate sub-structure identification, thus yielding sub-optimal performance. In this paper, to address the abovementioned problems, we first propose the hypergraph Weisfiler-Lehman test algorithm for the hypergraph isomorphism test problem by generalizing the Weisfiler-Lehman test algorithm from graphs to hypergraphs. Secondly, based on the presented algorithm, we propose a general hypergraph Weisfieler-Lehman kernel framework and implement two instances, which are Hypergraph Weisfeiler-Lehamn Subtree Kernel and Hypergraph Weisfeiler-Lehamn Hyperedge Kernel. In order to fulfill our research objectives, a comprehensive set of experiments was meticulously designed, including seven graph classification datasets and 12 hypergraph classification datasets. Results on hypergraph classification datasets show significant improvements compared to other typical kernel-based methods, which demonstrates the effectiveness of the proposed methods. In our evaluation, we found that our proposed methods outperform the second-best method in terms of runtime, running over 80 times faster when handling complex hypergraph structures.
</details>
<details>
<summary>摘要</summary>
“iso关系问题”是网络分析中的基本问题，它涵盖了低阶和高阶结构信息的捕捉。从抽象低阶结构信息的角度来看，网络同构算方法可以降低解析空间维度，实现了许多应用，如蛋白质设计、化学路径和社区探测。然而，实际生活中更常出现高阶关系，这些高阶关系不能直接使用网络同构算方法进行处理。此外，现有的超graphkernel方法可能会导致高内存消耗或不精确的子结构识别，因此表现不佳。在本文中，我们提出了超graphWeisfiler-Lehman测试算法，用于超graph isomorphism测试问题的解决方案。其次，我们基于这个算法提出了一个通用的超graphWeisfeiler-Lehman核心框架，并实现了两个实例：超graphWeisfeiler-Lehman子树核心和超graphWeisfeiler-Lehman超组件核心。为了实现我们的研究目标，我们谨慎设计了一系列实验，包括七个图类别数据集和十二个超graph类别数据集。结果显示，我们的提案方法在超graph类别数据集上表现出色，较其他常用的核心基本方法更好。在我们的评估中，我们发现了我们的提案方法在复杂超graph结构时表现出80倍以上的执行时间优化。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Applications-In-Healthcare-The-State-Of-Knowledge-and-Future-Directions"><a href="#Machine-Learning-Applications-In-Healthcare-The-State-Of-Knowledge-and-Future-Directions" class="headerlink" title="Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions"></a>Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14067">http://arxiv.org/abs/2307.14067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mrinmoy Roy, Sarwar J. Minar, Porarthi Dhar, A T M Omor Faruq</li>
<li>For: This study aims to gather and present Machine Learning (ML) applications in various areas of healthcare, such as community level work, risk management&#x2F;preventive care, healthcare operation management, remote care, and early detection, to provide quick access to necessary information and reduce the knowledge gap of clinicians about ML applications.* Methods: The study uses a tabular format to provide relevant references with descriptions for each ML application, allowing healthcare professionals to access the information more effectively.* Results: The study aims to motivate healthcare professionals towards adopting more ML-based healthcare systems and to inform people about the applicability of ML in the healthcare industry.<details>
<summary>Abstract</summary>
Detection of easily missed hidden patterns with fast processing power makes machine learning (ML) indispensable to today's healthcare system. Though many ML applications have already been discovered and many are still under investigation, only a few have been adopted by current healthcare systems. As a result, there exists an enormous opportunity in healthcare system for ML but distributed information, scarcity of properly arranged and easily explainable documentation in related sector are major impede which are making ML applications difficult to healthcare professionals. This study aimed to gather ML applications in different areas of healthcare concisely and more effectively so that necessary information can be accessed immediately with relevant references. We divided our study into five major groups: community level work, risk management/ preventive care, healthcare operation management, remote care, and early detection. Dividing these groups into subgroups, we provided relevant references with description in tabular form for quick access. Our objective is to inform people about ML applicability in healthcare industry, reduce the knowledge gap of clinicians about the ML applications and motivate healthcare professionals towards more machine learning based healthcare system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Detection of easily missed hidden patterns with fast processing power makes machine learning (ML) indispensable to today's healthcare system. Though many ML applications have already been discovered and many are still under investigation, only a few have been adopted by current healthcare systems. As a result, there exists an enormous opportunity in healthcare system for ML but distributed information, scarcity of properly arranged and easily explainable documentation in related sector are major impede which are making ML applications difficult to healthcare professionals. This study aimed to gather ML applications in different areas of healthcare concisely and more effectively so that necessary information can be accessed immediately with relevant references. We divided our study into five major groups: community level work, risk management/ preventive care, healthcare operation management, remote care, and early detection. Dividing these groups into subgroups, we provided relevant references with description in tabular form for quick access. Our objective is to inform people about ML applicability in healthcare industry, reduce the knowledge gap of clinicians about the ML applications and motivate healthcare professionals towards more machine learning based healthcare system." into Simplified Chinese.干� TRANSLATE "检测容易过look的隐藏模式，快速处理能力使机器学习（ML）成为今天的医疗系统中不可或缺的一部分。虽然已经发现了许多ML应用，但只有一些被当前的医疗系统采用。因此，医疗系统中存在巨大的机会，但信息散布、相关领域的文献不足和不易描述的问题是主要的阻碍因素。这项研究的目标是将不同领域的ML应用集中并更有效地呈现，以便立即获取相关参考。我们将研究分为五个主要组：社区层次的工作、风险管理/预防护理、医疗运营管理、远程护理和早期检测。将这些组分成子组，并提供相关参考和描述的表格形式，以便快速访问。我们的目标是让人们了解医疗业中ML的可能性，减少医生对ML应用的知识差距，并激励医疗专业人员更加倾向于基于机器学习的医疗系统。
</details></li>
</ul>
<hr>
<h2 id="Pre-Training-with-Diffusion-models-for-Dental-Radiography-segmentation"><a href="#Pre-Training-with-Diffusion-models-for-Dental-Radiography-segmentation" class="headerlink" title="Pre-Training with Diffusion models for Dental Radiography segmentation"></a>Pre-Training with Diffusion models for Dental Radiography segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14066">http://arxiv.org/abs/2307.14066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jérémy Rousseau, Christian Alaka, Emma Covili, Hippolyte Mayard, Laura Misrachi, Willy Au</li>
<li>for: 针对医疗放射学像 segmentation, 特别是牙科放射学像，存在高度有限制的标注成本，需要专业知识和劳动 INTENSIVE 注解。</li>
<li>methods: 我们提议使用 Denoising Diffusion Probabilistic Models (DDPM) 的快速预训练方法，这种方法已经在生成模型方面显示出卓越的表现。</li>
<li>results: 我们的实验结果表明，使用我们提议的方法可以实现 Label 效率的提高，不需要下游任务中的建筑修改。 我们的方法与当前状态的预训练方法相当。<details>
<summary>Abstract</summary>
Medical radiography segmentation, and specifically dental radiography, is highly limited by the cost of labeling which requires specific expertise and labor-intensive annotations. In this work, we propose a straightforward pre-training method for semantic segmentation leveraging Denoising Diffusion Probabilistic Models (DDPM), which have shown impressive results for generative modeling. Our straightforward approach achieves remarkable performance in terms of label efficiency and does not require architectural modifications between pre-training and downstream tasks. We propose to first pre-train a Unet by exploiting the DDPM training objective, and then fine-tune the resulting model on a segmentation task. Our experimental results on the segmentation of dental radiographs demonstrate that the proposed method is competitive with state-of-the-art pre-training methods.
</details>
<details>
<summary>摘要</summary>
医疗X射线段化和特别是牙科X射线段化受到标注成本的限制，这导致了标注过程的专业知识和劳动 INTENSIVE 的需求。在这项工作中，我们提出了一种简单的预训练方法 дляsemantic segmentation，利用Denosing Diffusion Probabilistic Models（DDPM），这种模型在生成模型方面表现出色。我们的简单的方法可以在预训练和下游任务之间不需要任何建筑修改，同时可以 дости到很好的标签效率。我们首先预训练了Unet模型，然后在DDPM训练目标下进行了微调。我们的实验结果表明，提议的方法与状态的预训练方法竞争。
</details></li>
</ul>
<hr>
<h2 id="Topologically-Regularized-Multiple-Instance-Learning-for-Red-Blood-Cell-Disease-Classification"><a href="#Topologically-Regularized-Multiple-Instance-Learning-for-Red-Blood-Cell-Disease-Classification" class="headerlink" title="Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification"></a>Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14025">http://arxiv.org/abs/2307.14025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salome Kazeminia, Ario Sadafi, Asya Makhro, Anna Bogdanova, Carsten Marr, Bastian Rieck</li>
<li>for: 这个论文的目的是用单个红血球图像自动诊断罕见贫血病。</li>
<li>methods: 这个论文使用的方法是基于多个精度扩展的扩展特征，以保持数据集的特性特征。</li>
<li>results: 该方法在71名患有罕见贫血病的患者和521张红血球图像上达到了超过3%的性能提升。<details>
<summary>Abstract</summary>
Diagnosing rare anemia disorders using microscopic images is challenging for skilled specialists and machine-learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images. This is the first approach that uses topological properties for regularizing the MIL process.
</details>
<details>
<summary>摘要</summary>
诊断罕见血红素疾病使用微型图像是困难的，both skilled specialists and machine learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images. This is the first approach that uses topological properties for regularizing the MIL process.Here's the translation in Traditional Chinese as well:诊断罕见血红素疾病使用微型图像是困难的，both skilled specialists and machine learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images. This is the first approach that uses topological properties for regularizing the MIL process.
</details></li>
</ul>
<hr>
<h2 id="Are-Transformers-with-One-Layer-Self-Attention-Using-Low-Rank-Weight-Matrices-Universal-Approximators"><a href="#Are-Transformers-with-One-Layer-Self-Attention-Using-Low-Rank-Weight-Matrices-Universal-Approximators" class="headerlink" title="Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?"></a>Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14023">http://arxiv.org/abs/2307.14023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tokio Kajitsuka, Issei Sato</li>
<li>for: 本研究探讨了Transformer模型的表达能力，并解决了现有分析中的深层层次问题。</li>
<li>methods: 本研究使用了 clarify softmax函数和Boltzmann算子之间的连接，以证明单层自注意层可以完全捕捉输入序列的上下文。</li>
<li>results: 研究显示，单层Transformer模型具有内存化能力，而且由一个自注意层和两个预测神经网络组成的Transformer模型是一个universal approximator。<details>
<summary>Abstract</summary>
Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.
</details>
<details>
<summary>摘要</summary>
existing 分析吧Transformer模型的表达能力已经需要过度深度层次，导致与实际使用中的Transformer模型存在差异。这主要是由于软MAX函数的解释为硬MAX函数的近似。通过证明软MAX函数和博尔tz曼算子之间的连接，我们证明了一层自注意力层次可以完美地捕捉整个输入序列的上下文。因此，我们显示了单层Transformer具有内存化能力，并且Transformer由一层自注意力层次和两个预处理神经网络组成的模型是continue 函数在封闭区域上的universal approximator。
</details></li>
</ul>
<hr>
<h2 id="MCMC-Correction-of-Score-Based-Diffusion-Models-for-Model-Composition"><a href="#MCMC-Correction-of-Score-Based-Diffusion-Models-for-Model-Composition" class="headerlink" title="MCMC-Correction of Score-Based Diffusion Models for Model Composition"></a>MCMC-Correction of Score-Based Diffusion Models for Model Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14012">http://arxiv.org/abs/2307.14012</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jackonelli/mcmc_corr_score_diffusion">https://github.com/jackonelli/mcmc_corr_score_diffusion</a></li>
<li>paper_authors: Anders Sjöberg, Jakob Lindqvist, Magnus Önnheim, Mats Jirstrand, Lennart Svensson</li>
<li>for: 这个论文的目的是提出一种新的推 diffusion 方法，以便在不同的分布中进行采样。</li>
<li>methods: 这个论文使用了分Diffusion models可以被参数化为得分或能量函数。这个研究使用了得分函数参数化，并通过 Metropolis–Hastings  correction step来进行扩展采样。</li>
<li>results: 研究发现，使用得分函数参数化的方法可以 Achieve similar or better performance than使用能量函数参数化的方法。此外，这种方法还可以 reuse existing diffusion models and combine with various Markov-Chain Monte Carlo (MCMC) methods。<details>
<summary>Abstract</summary>
Diffusion models can be parameterised in terms of either a score or an energy function. The energy parameterisation has better theoretical properties, mainly that it enables an extended sampling procedure with a Metropolis--Hastings correction step, based on the change in total energy in the proposed samples. However, it seems to yield slightly worse performance, and more importantly, due to the widespread popularity of score-based diffusion, there are limited availability of off-the-shelf pre-trained energy-based ones. This limitation undermines the purpose of model composition, which aims to combine pre-trained models to sample from new distributions. Our proposal, however, suggests retaining the score parameterization and instead computing the energy-based acceptance probability through line integration of the score function. This allows us to re-use existing diffusion models and still combine the reverse process with various Markov-Chain Monte Carlo (MCMC) methods. We evaluate our method on a 2D experiment and find that it achieve similar or arguably better performance than the energy parameterisation.
</details>
<details>
<summary>摘要</summary>
Diffusion models 可以被参数化为得分或能量函数。能量参数化有更好的理论性质，主要是允许扩展采样过程，并基于变化总能量进行 Metropolis-Hastings 修正步骤。然而，它似乎在性能上略微 inferior，而且由于得分基 diffusion 的普遍投身，有限的可用性。这限制了模型组合的目的，即将预训练模型组合以采样新的分布。我们的提议是保留得分参数化，并通过线tegration 计算能量基于接受概率。这允许我们 reuse 现有的 diffusion 模型，并且与多种 Markov-Chain Monte Carlo (MCMC) 方法结合。我们在 2D 实验中评估了我们的方法，并发现它们可以达到相似或更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Diff-E-Diffusion-based-Learning-for-Decoding-Imagined-Speech-EEG"><a href="#Diff-E-Diffusion-based-Learning-for-Decoding-Imagined-Speech-EEG" class="headerlink" title="Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG"></a>Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14389">http://arxiv.org/abs/2307.14389</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yorgoon/diffe">https://github.com/yorgoon/diffe</a></li>
<li>paper_authors: Soowon Kim, Young-Eun Lee, Seo-Hyun Lee, Seong-Whan Lee</li>
<li>for: 这篇研究旨在实现透过想像的语音来进行沟通，并使用数据减预测模型（DDPM）和条件自动encoder（Diff-E）来实现这一目标。</li>
<li>methods: 本研究使用DDPM和Diff-E实现透过想像的语音的EEG信号解oding，并与传统机器学习技术和基准模型进行比较。</li>
<li>results: 结果显示，Diff-E可以对EEG信号进行高精度的解oding，并提高透过想像的语音的沟通率，具有实际应用价值，可能对于开发基于想像语音的脑computer界面（BCI）有所帮助。<details>
<summary>Abstract</summary>
Decoding EEG signals for imagined speech is a challenging task due to the high-dimensional nature of the data and low signal-to-noise ratio. In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as promising approaches for representation learning in various domains. Our study proposes a novel method for decoding EEG signals for imagined speech using DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E significantly improves the accuracy of decoding EEG signals for imagined speech compared to traditional machine learning techniques and baseline models. Our findings suggest that DDPMs can be an effective tool for EEG signal decoding, with potential implications for the development of brain-computer interfaces that enable communication through imagined speech.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced the text into Simplified Chinese.<</SYS>>解oding EEG信号为想象的语言是一项复杂的任务，因为数据的高维度和信号噪声比率低。在最近几年，杂 diffusion probabilistic models（DDPMs）已经出现为不同领域的表征学习提出了可能的方法。我们的研究提出了一种使用 DDPMs 和名为 Diff-E 的条件 autoencoder 来解码 EEG 信号的新方法。结果表明，Diff-E 可以在解码 EEG 信号的想象语言方面提高精度，比传统机器学习技术和基线模型更高。我们的发现表明，DDPMs 可以是 EEG 信号解码的有效工具，具有可能为 brain-computer interface 的发展带来沟通通过想象语言的潜在意义。
</details></li>
</ul>
<hr>
<h2 id="Fast-algorithms-for-k-submodular-maximization-subject-to-a-matroid-constraint"><a href="#Fast-algorithms-for-k-submodular-maximization-subject-to-a-matroid-constraint" class="headerlink" title="Fast algorithms for k-submodular maximization subject to a matroid constraint"></a>Fast algorithms for k-submodular maximization subject to a matroid constraint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13996">http://arxiv.org/abs/2307.13996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuxian Niu, Qian Liu, Yang Zhou, Min Li</li>
<li>for: 该文章目的是 Maximize $k$-submodular functions under a matroid constraint.</li>
<li>methods: 文章使用 Threshold-Decreasing Algorithm, 比较效率高于 greedy algorithm, yet with little loss in approximation ratio.</li>
<li>results: 文章提供了 $(1&#x2F;2 - \epsilon)$ 和 $(1&#x2F;3 - \epsilon)$ 两种精度的算法，用于 monotone 和 non-monotone $k$-submodular function maximization, 其复杂度为 $O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$.<details>
<summary>Abstract</summary>
In this paper, we apply a Threshold-Decreasing Algorithm to maximize $k$-submodular functions under a matroid constraint, which reduces the query complexity of the algorithm compared to the greedy algorithm with little loss in approximation ratio. We give a $(\frac{1}{2} - \epsilon)$-approximation algorithm for monotone $k$-submodular function maximization, and a $(\frac{1}{3} - \epsilon)$-approximation algorithm for non-monotone case, with complexity $O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$, where $r$ denotes the rank of the matroid, and $IO, EO$ denote the number of oracles to evaluate whether a subset is an independent set and to compute the function value of $f$, respectively. Since the constraint of total size can be looked as a special matroid, called uniform matroid, then we present the fast algorithm for maximizing $k$-submodular functions subject to a total size constraint as corollaries. corollaries.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们采用一个阈值递减算法来最大化$k$- су布模函数 beneath 一个 matroid 约束，这会降低我们的查询复杂度相比于使用排序算法，减少了对于approximation ratio的损失。我们提供了一个$(1/2-\epsilon)$-近似算法 для升高 monotone $k$- submodular function，以及一个$(1/3-\epsilon)$-近似算法 for non-monotone case，其复杂度为$O(\frac{n(k\cdot EO+IO)}{\epsilon} \log \frac{r}{\epsilon})$，其中$r$表示 matroid 的排名，$IO, EO$表示计算 whether a subset is an independent set 和计算 $f$ 函数值的oracle数量。由于总大小的约束可以看作特殊的 matroid，called uniform matroid，所以我们将在 corollaries 中提供fast algorithm for maximizing $k$- submodular functions subject to a total size constraint.
</details></li>
</ul>
<hr>
<h2 id="Take-Your-Pick-Enabling-Effective-Personalized-Federated-Learning-within-Low-dimensional-Feature-Space"><a href="#Take-Your-Pick-Enabling-Effective-Personalized-Federated-Learning-within-Low-dimensional-Feature-Space" class="headerlink" title="Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space"></a>Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13995">http://arxiv.org/abs/2307.13995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guogang Zhu, Xuefeng Liu, Shaojie Tang, Jianwei Niu, Xinghao Wu, Jiaxing Shen</li>
<li>for: 该论文旨在提出一种基于个性化联合学习（PFL）的新框架，以便每个客户端可以根据自己的本地任务进行个性化模型训练。</li>
<li>methods: 该论文使用了一种基于特征选择的PFL方法，通过对每个客户端的本地数据进行特征选择，以便在低维度特征空间中进行联合学习。</li>
<li>results: 实验结果表明，该方法可以有效地选择每个客户端的任务相关特征，提高了跨Domain FL模型的性能。<details>
<summary>Abstract</summary>
Personalized federated learning (PFL) is a popular framework that allows clients to have different models to address application scenarios where clients' data are in different domains. The typical model of a client in PFL features a global encoder trained by all clients to extract universal features from the raw data and personalized layers (e.g., a classifier) trained using the client's local data. Nonetheless, due to the differences between the data distributions of different clients (aka, domain gaps), the universal features produced by the global encoder largely encompass numerous components irrelevant to a certain client's local task. Some recent PFL methods address the above problem by personalizing specific parameters within the encoder. However, these methods encounter substantial challenges attributed to the high dimensionality and non-linearity of neural network parameter space. In contrast, the feature space exhibits a lower dimensionality, providing greater intuitiveness and interpretability as compared to the parameter space. To this end, we propose a novel PFL framework named FedPick. FedPick achieves PFL in the low-dimensional feature space by selecting task-relevant features adaptively for each client from the features generated by the global encoder based on its local data distribution. It presents a more accessible and interpretable implementation of PFL compared to those methods working in the parameter space. Extensive experimental results show that FedPick could effectively select task-relevant features for each client and improve model performance in cross-domain FL.
</details>
<details>
<summary>摘要</summary>
个性化联合学习（PFL）是一种广泛使用的框架，允许客户端有不同的模型来处理应用场景中客户端数据的不同领域。典型的客户端模型在PFL中包括全局编码器，该编码器由所有客户端训练以提取Raw数据中的通用特征，以及客户端本地数据中的个性层（例如分类器）。然而，由于客户端数据的不同分布（即领域差距），全局编码器生成的通用特征具有许多无关于特定客户端本地任务的组成部分。一些最近的PFL方法解决了上述问题，通过在编码器中个性化特定参数。然而，这些方法遇到了高维度和非线性的神经网络参数空间的重大挑战。相比之下，特征空间的维度较低，提供了更好的直观性和可解释性，与参数空间相比。为此，我们提出了一种新的PFL框架名为FedPick。FedPick在低维特征空间中实现PFL，通过为每个客户端适应性地选择本地数据分布中任务相关的特征来选择任务相关的特征。它提供了与参数空间中方法相比更加直观和可解释的PFL实现。经验证明，FedPick可以有效地选择每个客户端的任务相关特征，并在跨领域FL中提高模型性能。
</details></li>
</ul>
<hr>
<h2 id="BovineTalk-Machine-Learning-for-Vocalization-Analysis-of-Dairy-Cattle-under-Negative-Affective-States"><a href="#BovineTalk-Machine-Learning-for-Vocalization-Analysis-of-Dairy-Cattle-under-Negative-Affective-States" class="headerlink" title="BovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle under Negative Affective States"></a>BovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle under Negative Affective States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13994">http://arxiv.org/abs/2307.13994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dinu Gavojdian, Teddy Lazebnik, Madalina Mincu, Ariel Oren, Ioana Nicolae, Anna Zamansky</li>
<li>for: 这个研究是为了开发和验证对livestock动物的情绪状态非侵入式指标，以便将其integrate到户外评估协议中。</li>
<li>methods: 这个研究使用了牛的 vocals 作为情绪状态的指标，并使用了深度学习和可解释机器学习来分类低频和高频牛叫。</li>
<li>results: 研究发现，使用深度学习和可解释机器学习可以达到87.2%和89.4%的准确率 для低频和高频牛叫的分类，以及68.9%和72.5%的牛 individuation 精度。<details>
<summary>Abstract</summary>
There is a critical need to develop and validate non-invasive animal-based indicators of affective states in livestock species, in order to integrate them into on-farm assessment protocols, potentially via the use of precision livestock farming (PLF) tools. One such promising approach is the use of vocal indicators. The acoustic structure of vocalizations and their functions were extensively studied in important livestock species, such as pigs, horses, poultry and goats, yet cattle remain understudied in this context to date. Cows were shown to produce two types vocalizations: low-frequency calls (LF), produced with the mouth closed, or partially closed, for close distance contacts and open mouth emitted high-frequency calls (HF), produced for long distance communication, with the latter considered to be largely associated with negative affective states. Moreover, cattle vocalizations were shown to contain information on individuality across a wide range of contexts, both negative and positive. Nowadays, dairy cows are facing a series of negative challenges and stressors in a typical production cycle, making vocalizations during negative affective states of special interest for research. One contribution of this study is providing the largest to date pre-processed (clean from noises) dataset of lactating adult multiparous dairy cows during negative affective states induced by visual isolation challenges. Here we present two computational frameworks - deep learning based and explainable machine learning based, to classify high and low-frequency cattle calls, and individual cow voice recognition. Our models in these two frameworks reached 87.2% and 89.4% accuracy for LF and HF classification, with 68.9% and 72.5% accuracy rates for the cow individual identification, respectively.
</details>
<details>
<summary>摘要</summary>
有一项急需要的发展和验证，那就是通过非侵入性的动物基因体系来评估livestock种类的情绪状态，以便将其 integrate into 农场评估协议中。一种可能的方法是使用 vocal indicators。studied extensively in important livestock species such as pigs, horses, poultry and goats, but cattle have been understudied in this context to date. Cows produce two types of vocalizations: low-frequency calls (LF), produced with the mouth closed or partially closed for close distance contacts, and open mouth emitted high-frequency calls (HF), produced for long distance communication, with the latter considered to be largely associated with negative affective states. Moreover, cattle vocalizations contain information on individuality across a wide range of contexts, both negative and positive. Nowadays, dairy cows are facing a series of negative challenges and stressors in a typical production cycle, making vocalizations during negative affective states of special interest for research. This study provides the largest to date pre-processed (clean from noises) dataset of lactating adult multiparous dairy cows during negative affective states induced by visual isolation challenges. Here we present two computational frameworks - deep learning based and explainable machine learning based - to classify high and low-frequency cattle calls, and individual cow voice recognition. Our models in these two frameworks reached 87.2% and 89.4% accuracy for LF and HF classification, with 68.9% and 72.5% accuracy rates for the cow individual identification, respectively.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-short-time-Fourier-transform-with-respect-to-the-hop-length"><a href="#Differentiable-short-time-Fourier-transform-with-respect-to-the-hop-length" class="headerlink" title="Differentiable short-time Fourier transform with respect to the hop length"></a>Differentiable short-time Fourier transform with respect to the hop length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02421">http://arxiv.org/abs/2308.02421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxime-leiber/dstft">https://github.com/maxime-leiber/dstft</a></li>
<li>paper_authors: Maxime Leiber, Yosra Marnissi, Axel Barrau, Mohammed El Badaoui</li>
<li>for: 提出一种可微分的快时傅立叙变换（STFT），允许通过continuous hop length或frame temporal position的优化来提高时间位置控制。</li>
<li>methods: 使用continuous hop length和frame temporal position的优化，提供更精细的时间位置控制，并且可以使用计算效率更高的优化方法，如梯度下降。</li>
<li>results: 通过 simulations 示例，证明了我们的方法的有效性，并且可以轻松地与现有的算法和神经网络集成。<details>
<summary>Abstract</summary>
In this paper, we propose a differentiable version of the short-time Fourier transform (STFT) that allows for gradient-based optimization of the hop length or the frame temporal position by making these parameters continuous. Our approach provides improved control over the temporal positioning of frames, as the continuous nature of the hop length allows for a more finely-tuned optimization. Furthermore, our contribution enables the use of optimization methods such as gradient descent, which are more computationally efficient than conventional discrete optimization methods. Our differentiable STFT can also be easily integrated into existing algorithms and neural networks. We present a simulated illustration to demonstrate the efficacy of our approach and to garner interest from the research community.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种可微分的短时傅立叙变换（STFT），允许通过使这些参数变为连续的数值来进行梯度下降优化。我们的方法可以提供更好的控制时间位置，因为连续的跳跃长度允许更细化优化。此外，我们的贡献允许使用优化方法，如梯度下降，这些方法更有效率 than conventional discrete optimization methods。我们的可微分STFT也可以轻松地与现有的算法和神经网络集成。我们提供了一个 simulated 示例，以示出我们的方法的有效性并引起研究者的关注。
</details></li>
</ul>
<hr>
<h2 id="METAVerse-Meta-Learning-Traversability-Cost-Map-for-Off-Road-Navigation"><a href="#METAVerse-Meta-Learning-Traversability-Cost-Map-for-Off-Road-Navigation" class="headerlink" title="METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation"></a>METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13991">http://arxiv.org/abs/2307.13991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwon Seo, Taekyung Kim, Seongyong Ahn, Kiho Kwak</li>
<li>for: 这篇论文是为了提出一种能够在不同环境中准确地估计地形通行性的自适应导航方法。</li>
<li>methods: 该方法使用了元学习框架，通过自动学习方式将多种环境中的车辆-地形交互反馈纳入模型中，以减少估计uncertainty。</li>
<li>results: 该方法可以在不同环境中获得一个准确和可靠的地形通行性估计模型，并且可以通过与预测控制器结合使用，实现安全和稳定的导航。<details>
<summary>Abstract</summary>
Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability. However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction. Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments. This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is performed to rapidly adapt the network to the local environment by exploiting recent interaction experiences. To conduct a comprehensive evaluation, we collect driving data from various terrains and demonstrate that our method can obtain a global model that minimizes uncertainty. Moreover, by integrating our model with a model predictive controller, we demonstrate that the reduced uncertainty results in safe and stable navigation in unstructured and unknown terrains.
</details>
<details>
<summary>摘要</summary>
自主导航在无结构环境中需要准确地估计地形通行性。然而，在无结构环境中的通行性估计受到许多因素的变化影响，这些因素包括车辆和地形之间的互动。因此，建立一个泛化模型，以准确地预测不同环境中的通行性，是一项挑战。这篇论文提出了METAVerse，一个基于元学习的框架，用于学习一个准确和可靠地预测地形通行性的全球模型。我们在训练通行性预测网络时，使用自动学习的方式，从稀疏的 LiDAR 点云中生成一个密集和连续的成本图，以利用车辆和地形之间的互动反馈。元学习被用来训练全球模型，以使其在多个环境中具有最小的估计不确定性。在部署过程中，我们通过在线适应来快速地适应当地环境，并且通过利用最近的互动经验来进行更新。我们通过收集来自不同地形的驾驶数据，证明了我们的方法可以获得一个全球模型，以准确地预测不同环境中的通行性。此外，我们将我们的模型与预测控制器结合，以验证了减少了不确定性的结果，可以在未知的无结构环境中安全和稳定地导航。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-adaptive-short-time-Fourier-transform-with-respect-to-the-window-length"><a href="#Differentiable-adaptive-short-time-Fourier-transform-with-respect-to-the-window-length" class="headerlink" title="Differentiable adaptive short-time Fourier transform with respect to the window length"></a>Differentiable adaptive short-time Fourier transform with respect to the window length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02418">http://arxiv.org/abs/2308.02418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxime-leiber/dstft">https://github.com/maxime-leiber/dstft</a></li>
<li>paper_authors: Maxime Leiber, Yosra Marnissi, Axel Barrau, Mohammed El Badaoui</li>
<li>for: 这 paper 用于提出了一种基于梯度的方法，用于在实时进行 STFT 的参数优化，包括每帧和每频率窗口长的优化。</li>
<li>methods: 这 paper 使用了梯度下降优化方法，将 STFT 中的窗口长作为连续参数，使得 STFT 可以适应变化的时域频率特征。</li>
<li>results: 作者验证了这种方法在震动分析中的性能，并证明了它可以同时适应变化的时域频率特征，而且可以通过梯度下降优化方法进行快速优化。<details>
<summary>Abstract</summary>
This paper presents a gradient-based method for on-the-fly optimization for both per-frame and per-frequency window length of the short-time Fourier transform (STFT), related to previous work in which we developed a differentiable version of STFT by making the window length a continuous parameter. The resulting differentiable adaptive STFT possesses commendable properties, such as the ability to adapt in the same time-frequency representation to both transient and stationary components, while being easily optimized by gradient descent. We validate the performance of our method in vibration analysis.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文提出了一种基于梯度的方法，用于在实时进行STFT（短时傅立叶变换）的框架和频率窗口长度的在线优化，与之前的工作相关，我们将STFT中的窗口长度变为连续参数。结果的可微STFT具有了许多优点，如适应到同时域频率上的激变和站立部分，同时也容易使用梯度下降优化。我们通过振荡分析 validate the performance of our method。
</details></li>
</ul>
<hr>
<h2 id="This-is-not-correct-Negation-aware-Evaluation-of-Language-Generation-Systems"><a href="#This-is-not-correct-Negation-aware-Evaluation-of-Language-Generation-Systems" class="headerlink" title="This is not correct! Negation-aware Evaluation of Language Generation Systems"></a>This is not correct! Negation-aware Evaluation of Language Generation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13989">http://arxiv.org/abs/2307.13989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmlls/cannot-dataset">https://github.com/dmlls/cannot-dataset</a></li>
<li>paper_authors: Miriam Anschütz, Diego Miguel Lozano, Georg Groh</li>
<li>for: 本研究旨在提高大型自然语言处理模型对否定语言的识别能力，以提高模型对语言表达中的意义更加精准地识别和理解。</li>
<li>methods: 本研究使用了规则基本的句子否定工具，创建了CANNOT negation评估数据集，并使用了句子转换器和评估指标进行了细化。</li>
<li>results: 对现有的评估指标进行评估，本研究的细化模型在否定句子上表现出色，与基本模型在其他杂化情况下的表现相比，有很大的提升。<details>
<summary>Abstract</summary>
Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations. In this paper, we propose NegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that, we designed a rule-based sentence negation tool and used it to create the CANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a sentence transformer and an evaluation metric to improve their negation sensitivity. Evaluating these models on existing benchmarks shows that our fine-tuned models outperform existing metrics on the negated sentences by far while preserving their base models' performances on other perturbations.
</details>
<details>
<summary>摘要</summary>
大型语言模型会在判断句子意义时忽略否定影响。因此，基于这些模型的评估指标会对否定不敏感。在这篇文章中，我们提出了NegBLEURT评估指标，这是一个对否定敏感的BLEURT评估指标。为了建立这个评估指标，我们设计了一个基于规则的句子否定工具，并使用这个工具创建了CANNOT评估集。基于这个集，我们精炼了句子变换器和评估指标，以提高它们对否定的敏感度。对现有的benchmark测试显示，我们精炼的模型在否定句子上表现明显比其他评估指标好，而且保留了基本模型在其他扰动下的表现。
</details></li>
</ul>
<hr>
<h2 id="Controlling-the-Latent-Space-of-GANs-through-Reinforcement-Learning-A-Case-Study-on-Task-based-Image-to-Image-Translation"><a href="#Controlling-the-Latent-Space-of-GANs-through-Reinforcement-Learning-A-Case-Study-on-Task-based-Image-to-Image-Translation" class="headerlink" title="Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation"></a>Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13978">http://arxiv.org/abs/2307.13978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahyar Abbasian, Taha Rajabzadeh, Ahmadreza Moradipari, Seyed Amir Hossein Aqajari, Hongsheng Lu, Amir Rahmani</li>
<li>for: 这篇论文旨在解决Generative Adversarial Networks (GAN) 的控制问题，提高 GAN 的生成效果。</li>
<li>methods: 本论文提出了一种新的方法，通过融合对抗学习（RL）代理人与潜在空间 GAN（l-GAN），以生成适当的出力。RL 代理人通过精心设计的奖励策略，获得了在潜在空间中穿梭并生成出力的能力。</li>
<li>results: 本论文通过使用 MNIST dataset 进行了一系列实验，包括一个示例任务：加法。实验结果证实了我们的方法效果。本论文的创新的RL代理人与 GAN 模型融合，具有很大的应用前途。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to generate realistic outputs based on training datasets. However, the challenge of exerting control over the generation process of GANs remains a significant hurdle. In this paper, we propose a novel methodology to address this issue by integrating a reinforcement learning (RL) agent with a latent-space GAN (l-GAN), thereby facilitating the generation of desired outputs. More specifically, we have developed an actor-critic RL agent with a meticulously designed reward policy, enabling it to acquire proficiency in navigating the latent space of the l-GAN and generating outputs based on specified tasks. To substantiate the efficacy of our approach, we have conducted a series of experiments employing the MNIST dataset, including arithmetic addition as an illustrative task. The outcomes of these experiments serve to validate our methodology. Our pioneering integration of an RL agent with a GAN model represents a novel advancement, holding great potential for enhancing generative networks in the future.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Mathematical-Modeling-of-BCG-based-Bladder-Cancer-Treatment-Using-Socio-Demographics"><a href="#Mathematical-Modeling-of-BCG-based-Bladder-Cancer-Treatment-Using-Socio-Demographics" class="headerlink" title="Mathematical Modeling of BCG-based Bladder Cancer Treatment Using Socio-Demographics"></a>Mathematical Modeling of BCG-based Bladder Cancer Treatment Using Socio-Demographics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15084">http://arxiv.org/abs/2307.15084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizaveta Savchenko, Ariel Rosenfeld, Svetlana Bunimovich-Mendrazitsky</li>
<li>for: 这个研究旨在提出一种个性化的数学模型，用于预测bcg基于治疗的临床动态。</li>
<li>methods: 该研究采用了一种已知的bcg治疗模型，并将机器学习组件 integrate到模型中，以时间性地调整和重配置关键参数。</li>
<li>results: 使用实际临床数据，研究表明，个性化模型与原始模型相比，在预测bcg治疗结束时的癌细胞数量方面有14.8%的改善，平均而言。<details>
<summary>Abstract</summary>
Cancer is one of the most widespread diseases around the world with millions of new patients each year. Bladder cancer is one of the most prevalent types of cancer affecting all individuals alike with no obvious prototypical patient. The current standard treatment for BC follows a routine weekly Bacillus Calmette-Guerin (BCG) immunotherapy-based therapy protocol which is applied to all patients alike. The clinical outcomes associated with BCG treatment vary significantly among patients due to the biological and clinical complexity of the interaction between the immune system, treatments, and cancer cells. In this study, we take advantage of the patient's socio-demographics to offer a personalized mathematical model that describes the clinical dynamics associated with BCG-based treatment. To this end, we adopt a well-established BCG treatment model and integrate a machine learning component to temporally adjust and reconfigure key parameters within the model thus promoting its personalization. Using real clinical data, we show that our personalized model favorably compares with the original one in predicting the number of cancer cells at the end of the treatment, with 14.8% improvement, on average.
</details>
<details>
<summary>摘要</summary>
肿瘤是全球最普遍的疾病之一，每年新生发病人数达到百万。膀胱癌是所有人都受到影响的最常见的抑肿癌类型，没有明显的典型患者形象。现有的标准治疗办法是每周一次的细菌Calmette-Guerin（BCG）免疫疗法，该办法适用于所有患者。但是，BCG治疗的临床结果因免疫系统、治疗和肿瘤细胞之间的生物和临床复杂性而异常变化。在这项研究中，我们利用患者的社会民生数据来提供个性化的数学模型，描述BCG基于治疗的临床动态。为此，我们采用了已知的BCG治疗模型，并将机器学习组件加入，以时间地调整和重新配置关键参数，以便个性化。使用实际临床数据，我们表明，我们的个性化模型与原始模型相比，在预测治疗结束后肿瘤细胞数量方面表现出了14.8%的改善，平均而言。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Deep-Neural-Networks-via-Linear-Separability-of-Hidden-Layers"><a href="#Understanding-Deep-Neural-Networks-via-Linear-Separability-of-Hidden-Layers" class="headerlink" title="Understanding Deep Neural Networks via Linear Separability of Hidden Layers"></a>Understanding Deep Neural Networks via Linear Separability of Hidden Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13962">http://arxiv.org/abs/2307.13962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Zhang, Xinyu Chen, Wensheng Li, Lixue Liu, Wei Wu, Dacheng Tao</li>
<li>for: 本研究用来研究深度神经网络的特点，特别是深度神经网络的线性可分性。</li>
<li>methods: 本研究提出了基于米诺夫做ifferencedifference measure（MD-LSM）来评估线性可分性度的两个点集。然后，我们证明了深度神经网络训练性能和线性可分性度之间存在同步关系，即如果更新权重可以提高线性可分性度，则更新后的网络将在训练过程中表现更好，并且相反。此外，我们还研究了活化函数和网络大小（包括宽度和深度）对隐藏层的线性可分性度的影响。</li>
<li>results: 我们通过实验 validate了我们的发现，测试了一些流行的深度网络，包括多层感知器（MLP）、卷积神经网络（CNN）、深度信念网络（DBN）、ResNet、VGGNet、AlexNet、视transformer（ViT）和GoogLeNet。<details>
<summary>Abstract</summary>
In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks. In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets. Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa. Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers. Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision transformer (ViT) and GoogLeNet.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们测量了深度神经网络中隐藏层输出的线性分割性，以study深度神经网络的特点。特别是，我们首先提出了Minkowski差值基于的线性分割度量指标（MD-LSM），用于评估两个点集的线性分割度。然后，我们示出了隐藏层输出线性分割度和网络训练性能之间的同步关系，即如果更新的权重可以提高隐藏层输出的线性分割度，则更新后的网络将达到更好的训练性能，并且相反。此外，我们研究了活动函数和网络大小（包括宽和深）对隐藏层的线性分割性的影响。最后，我们进行了实验验证我们的发现，并在一些流行的深度神经网络，如多层感知网络（MLP）、卷积神经网络（CNN）、深度信念网络（DBN）、ResNet、VGGNet、AlexNet、视Transformer（ViT）和GoogLeNet等进行了数值实验。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Differentially-Private-Vertical-Federated-Learning-with-Adaptive-Feature-Embeddings"><a href="#Flexible-Differentially-Private-Vertical-Federated-Learning-with-Adaptive-Feature-Embeddings" class="headerlink" title="Flexible Differentially Private Vertical Federated Learning with Adaptive Feature Embeddings"></a>Flexible Differentially Private Vertical Federated Learning with Adaptive Feature Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02362">http://arxiv.org/abs/2308.02362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxi Mi, Hongquan Liu, Yewei Xia, Yiheng Sun, Jihong Guan, Shuigeng Zhou<br>for: This paper focuses on the delicate balance between data privacy and task utility in vertical federated learning (VFL) under differential privacy (DP).methods: The authors propose a flexible and generic approach that decouples the privacy and utility goals, addressing them successively. They first derive a privacy guarantee using norm clipping on shared feature embeddings, and then optimize task utility through adaptive adjustments on the scale and distribution of feature embeddings.results: The proposed VFL-AFE framework exhibits effectiveness against privacy attacks and retains favorable task utility, as demonstrated through extensive experiments.<details>
<summary>Abstract</summary>
The emergence of vertical federated learning (VFL) has stimulated concerns about the imperfection in privacy protection, as shared feature embeddings may reveal sensitive information under privacy attacks. This paper studies the delicate equilibrium between data privacy and task utility goals of VFL under differential privacy (DP). To address the generality issue of prior arts, this paper advocates a flexible and generic approach that decouples the two goals and addresses them successively. Specifically, we initially derive a rigorous privacy guarantee by applying norm clipping on shared feature embeddings, which is applicable across various datasets and models. Subsequently, we demonstrate that task utility can be optimized via adaptive adjustments on the scale and distribution of feature embeddings in an accuracy-appreciative way, without compromising established DP mechanisms. We concretize our observation into the proposed VFL-AFE framework, which exhibits effectiveness against privacy attacks and the capacity to retain favorable task utility, as substantiated by extensive experiments.
</details>
<details>
<summary>摘要</summary>
vertical federated learning (VFL)的出现引起了隐私保护不足的担忧，因为分享特征嵌入可能在隐私攻击下泄露敏感信息。这篇论文研究了VFL中数据隐私和任务用途目标之间的紧耦合关系，并提出了一种flexible和通用的方法来解决这个问题。我们首先通过应用norm clipping来 derive privacy guarantee，这种方法适用于各种数据集和模型。然后，我们示出了通过 adaptive adjustments来 optimize任务用途，无需牺牲已有的隐私机制。我们将这些观察集成为VFL-AFE框架，该框架具有防止隐私攻击和保持任务用途的能力，并经过了广泛的实验证明。
</details></li>
</ul>
<hr>
<h2 id="Entropy-Neural-Estimation-for-Graph-Contrastive-Learning"><a href="#Entropy-Neural-Estimation-for-Graph-Contrastive-Learning" class="headerlink" title="Entropy Neural Estimation for Graph Contrastive Learning"></a>Entropy Neural Estimation for Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13944">http://arxiv.org/abs/2307.13944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kunzhan/M-ILBO">https://github.com/kunzhan/M-ILBO</a></li>
<li>paper_authors: Yixuan Ma, Xiaolin Zhang, Peng Zhang, Kun Zhan</li>
<li>for: 本文提出了一种基于对比学习的图像分类方法，用于提取图像中的高级特征表示。</li>
<li>methods: 本文使用了一种基于Maximum Mutual Information的方法来估计数据集的熵，并提出了一种简单 yet effective的子集采样策略来实现对比表示之间的对比。</li>
<li>results: 实验结果表明，提出的方法可以在七个图像benchmark上 achieve competitive performance，并且可以增强图像encoder的表示能力。<details>
<summary>Abstract</summary>
Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \ie, entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss function consists of positive and negative pairs. Our selection strategy of pairs is different from previous works and we present a novel strategy to enhance the representation ability of the graph encoder by selecting nodes based on cross-view similarities. We enrich the diversity of the positive and negative pairs by selecting highly similar samples and totally different data with the guidance of cross-view similarity scores, respectively. We also introduce a cross-view consistency constraint on the representations generated from the different views. This objective guarantees the learned representations are consistent across views from the perspective of the entire graph. We conduct extensive experiments on seven graph benchmarks, and the proposed approach achieves competitive performance compared to the current state-of-the-art methods. The source code will be publicly released once this paper is accepted.
</details>
<details>
<summary>摘要</summary>
contrastive learning on graphs aims to extract distinguishable high-level node representations. in this paper, we prove that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different graph views, \ie, entropy is estimated by a neural network. based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. specifically, we randomly sample nodes and edges from a given graph to build the input subset for a view. two views are fed into a parameter-shared siamese network to extract high-dimensional embeddings and estimate the information entropy of the entire graph. for the learning process, we propose to optimize the network using two objectives simultaneously. concretely, the input of the contrastive loss function consists of positive and negative pairs. our selection strategy of pairs is different from previous works, and we present a novel strategy to enhance the representation ability of the graph encoder by selecting nodes based on cross-view similarities. we enrich the diversity of the positive and negative pairs by selecting highly similar samples and totally different data with the guidance of cross-view similarity scores, respectively. we also introduce a cross-view consistency constraint on the representations generated from the different views. this objective guarantees the learned representations are consistent across views from the perspective of the entire graph. we conduct extensive experiments on seven graph benchmarks, and the proposed approach achieves competitive performance compared to the current state-of-the-art methods. the source code will be publicly released once this paper is accepted.
</details></li>
</ul>
<hr>
<h2 id="Topology-aware-Robust-Optimization-for-Out-of-distribution-Generalization"><a href="#Topology-aware-Robust-Optimization-for-Out-of-distribution-Generalization" class="headerlink" title="Topology-aware Robust Optimization for Out-of-distribution Generalization"></a>Topology-aware Robust Optimization for Out-of-distribution Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13943">http://arxiv.org/abs/2307.13943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joffery/tro">https://github.com/joffery/tro</a></li>
<li>paper_authors: Fengchun Qiao, Xi Peng</li>
<li>for: 本研究旨在提高机器学习模型对异常输入的抗性，以提高高风险应用中的模型可靠性。</li>
<li>methods: 本研究提出了一种基于分布 topology 的 robust optimization 方法，包括两个优化目标：分布学习和学习 на topology。</li>
<li>results: 实验表明，这种方法可以在多种任务中（包括分类、回归和semantic segmentation）明显超过现有方法，并且发现数据驱动的分布 topology 与领域知识是相一致的，从而提高了方法的可解释性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION (OOD) 泛化是一个具有挑战性的机器学习问题，但在许多高风险应用中很需要。现有的方法受到过度的悲观预测，导致模型的泛化自信度较低。由于泛化到任意的测试分布是不可能的，我们假设预设分布的数学结构是决定性的。为了解决这个问题，我们提出了分布数学结构意识的强健泛化优化（TRO）。TRO通过将分布数学结构与强健优化紧密融合，实现了具有理性的优化框架。具体而言，TRO解决以下两个优化目标：1. 分布学习：探索数据构造，发现分布的数学结构。2. 学习于分布：利用分布的数学结构，对泛化优化进行紧密的约束，以降低泛化风险。我们理论上显示TRO的有效性，并实践显示它在许多任务中，包括分类、回归和semantic segmentation，与现有的方法相比，表现得更好。此外，我们实践发现，使用数据驱动的分布数学结构可以与领域知识相互匹配，增加了我们的方法的解释性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Semi-Supervised-Semantic-Segmentation-with-Dual-Level-Siamese-Structure-Network"><a href="#Improving-Semi-Supervised-Semantic-Segmentation-with-Dual-Level-Siamese-Structure-Network" class="headerlink" title="Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network"></a>Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13938">http://arxiv.org/abs/2307.13938</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kunzhan/DSSN">https://github.com/kunzhan/DSSN</a></li>
<li>paper_authors: Zhibo Tain, Xiaolin Zhang, Peng Zhang, Kun Zhan</li>
<li>for: 提高semantic segmentation的效果，使用both labeled和无标例数据，减少标注训练示例的成本</li>
<li>methods: 提出了一种基于 dual-level Siamese structure network (DSSN) 的像素级对比学习方法，通过在低级图像空间和高级特征空间都使用强制修改视图进行对比，以最大化使用可用的无标例数据</li>
<li>results: 实现了在PASCAL VOC 2012和Cityscapes两个 dataset 上的状态级 результаategraph，与其他 SSS 算法相比，表现出了显著的优异，并且可以减少标注训练示例的成本<details>
<summary>Abstract</summary>
Semi-supervised semantic segmentation (SSS) is an important task that utilizes both labeled and unlabeled data to reduce expenses on labeling training examples. However, the effectiveness of SSS algorithms is limited by the difficulty of fully exploiting the potential of unlabeled data. To address this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise contrastive learning. By aligning positive pairs with a pixel-wise contrastive loss using strong augmented views in both low-level image space and high-level feature space, the proposed DSSN is designed to maximize the utilization of available unlabeled data. Additionally, we introduce a novel class-aware pseudo-label selection strategy for weak-to-strong supervision, which addresses the limitations of most existing methods that do not perform selection or apply a predefined threshold for all classes. Specifically, our strategy selects the top high-confidence prediction of the weak view for each class to generate pseudo labels that supervise the strong augmented views. This strategy is capable of taking into account the class imbalance and improving the performance of long-tailed classes. Our proposed method achieves state-of-the-art results on two datasets, PASCAL VOC 2012 and Cityscapes, outperforming other SSS algorithms by a significant margin.
</details>
<details>
<summary>摘要</summary>
semi-supervised semantic segmentation (SSS) 是一个重要的任务，它利用了标注和无标注数据来降低标注训练示例的成本。然而，SSS 算法的效果受到无标注数据的难以完全利用的限制。为解决这个问题，我们提议一种 dual-level Siamese structure network (DSSN)  для像素级别的对比学习。通过在低级图像空间和高级特征空间都使用强化的扩展视图对正对应的对比损失进行对 Pablo 的对比，我们设计了 DSSN，以便最大化可用的无标注数据的利用。此外，我们引入了一种新的类感知 pseudo-label 选择策略，这种策略可以考虑类别偏度，并提高长尾类别的性能。我们的策略是选择每个类型的高信息报告值作为弱视图中的 pseudo 标签，以便使得强视图中的augmented views得到supervise。我们的提议方法在 PASCAL VOC 2012 和 Cityscapes 两个 dataset 上实现了领先的状态，比其他 SSS 算法高出了一定的幅度。
</details></li>
</ul>
<hr>
<h2 id="trajdata-A-Unified-Interface-to-Multiple-Human-Trajectory-Datasets"><a href="#trajdata-A-Unified-Interface-to-Multiple-Human-Trajectory-Datasets" class="headerlink" title="trajdata: A Unified Interface to Multiple Human Trajectory Datasets"></a>trajdata: A Unified Interface to Multiple Human Trajectory Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13924">http://arxiv.org/abs/2307.13924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvlabs/trajdata">https://github.com/nvlabs/trajdata</a></li>
<li>paper_authors: Boris Ivanovic, Guanyu Song, Igor Gilitschenski, Marco Pavone</li>
<li>for: This paper aims to provide a unified interface to multiple human trajectory datasets for researchers to train and evaluate methods more efficiently.</li>
<li>methods: The paper presents a simple, uniform, and efficient representation and API for trajectory and map data, which enables a comprehensive empirical evaluation of existing trajectory datasets.</li>
<li>results: The paper conducts a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights.<details>
<summary>Abstract</summary>
The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata
</details>
<details>
<summary>摘要</summary>
“ trajectory forecasting 领域在最近几年内发展非常快，一部分这是因为大量的实际世界人 trajectory 数据集（AVs 和行人运动跟踪）的发布。 although these datasets have been a blessing for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. to address this, we present trajdata: a unified interface to multiple human trajectory datasets. at its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. as a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is licensed under Apache 2.0 and can be accessed online at https://github.com/NVlabs/trajdata.”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="HyperFed-Hyperbolic-Prototypes-Exploration-with-Consistent-Aggregation-for-Non-IID-Data-in-Federated-Learning"><a href="#HyperFed-Hyperbolic-Prototypes-Exploration-with-Consistent-Aggregation-for-Non-IID-Data-in-Federated-Learning" class="headerlink" title="HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning"></a>HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14384">http://arxiv.org/abs/2307.14384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Huabin Zhu, Yanchao Tan, Jun Wang, Yue Qi</li>
<li>for: 提高 Federated Learning（FL）在非同一个分布（non-IID）环境下的性能。</li>
<li>methods: 提议 HyperFed，它包含以下三个主要模块： hyperbolic prototype Tammes initialization（HPTI）、 hyperbolic prototype learning（HPL）和 consistent aggregation（CA）。 HPTI在服务器端 constructions  uniformly distributed和 fixed class prototypes，并将其分享给客户端以匹配类统计，从而引导客户端的具有一致性的特征表示。 HPL在每个客户端上使用分享的类prototype在 hyperbolic 模型空间中捕捉本地数据中的层次信息。 CA在服务器端 mitigates the impact of inconsistent deviations from clients to server。</li>
<li>results: 对四个数据集进行了广泛的研究，证明 HyperFed 可以有效地提高 FL 在 non-IID 环境下的性能。<details>
<summary>Abstract</summary>
Federated learning (FL) collaboratively models user data in a decentralized way. However, in the real world, non-identical and independent data distributions (non-IID) among clients hinder the performance of FL due to three issues, i.e., (1) the class statistics shifting, (2) the insufficient hierarchical information utilization, and (3) the inconsistency in aggregating clients. To address the above issues, we propose HyperFed which contains three main modules, i.e., hyperbolic prototype Tammes initialization (HPTI), hyperbolic prototype learning (HPL), and consistent aggregation (CA). Firstly, HPTI in the server constructs uniformly distributed and fixed class prototypes, and shares them with clients to match class statistics, further guiding consistent feature representation for local clients. Secondly, HPL in each client captures the hierarchical information in local data with the supervision of shared class prototypes in the hyperbolic model space. Additionally, CA in the server mitigates the impact of the inconsistent deviations from clients to server. Extensive studies of four datasets prove that HyperFed is effective in enhancing the performance of FL under the non-IID set.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 协同模型用户数据的方式是分布在多个客户端上的。然而，在现实中，客户端上的数据分布非常不同（非同一个），这会降低 FL 的性能，因为三个问题：1. 类别统计移动2. 不充分利用层次信息3. 客户端聚合不一致为解决这些问题，我们提出了 HyperFed，它包含三个主要模块：1. hyperbolic prototype Tammes initialization (HPTI)：在服务器端constructs uniformly distributed和fixed class prototypes，并将其分享给客户端，以匹配类统计，并且指导客户端的准确特征表示。2. hyperbolic prototype learning (HPL)：在每个客户端上，通过在hyperbolic模型空间的supervision，使得客户端上的数据具有层次结构信息。3. consistent aggregation (CA)：在服务器端，使得客户端的不一致偏差的影响被减轻。对四个数据集进行了广泛的研究，证明了 HyperFed 能够在非同一个情况下提高 FL 的性能。
</details></li>
</ul>
<hr>
<h2 id="Simulation-based-Inference-for-Cardiovascular-Models"><a href="#Simulation-based-Inference-for-Cardiovascular-Models" class="headerlink" title="Simulation-based Inference for Cardiovascular Models"></a>Simulation-based Inference for Cardiovascular Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13918">http://arxiv.org/abs/2307.13918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Wehenkel, Jens Behrmann, Andrew C. Miller, Guillermo Sapiro, Ozan Sener, Marco Cuturi, Jörn-Henrik Jacobsen</li>
<li>for: studying cardiovascular systems in-silico and simulating whole-body hemodynamics</li>
<li>methods: statistical inference and simulation-based inference (SBI)</li>
<li>results: potential for estimating new biomarkers from standard-of-care measurements, existence of sub-populations with distinct uncertainty regimes, and gap analysis between in-vivo and in-silico data<details>
<summary>Abstract</summary>
Over the past decades, hemodynamics simulators have steadily evolved and have become tools of choice for studying cardiovascular systems in-silico. While such tools are routinely used to simulate whole-body hemodynamics from physiological parameters, solving the corresponding inverse problem of mapping waveforms back to plausible physiological parameters remains both promising and challenging. Motivated by advances in simulation-based inference (SBI), we cast this inverse problem as statistical inference. In contrast to alternative approaches, SBI provides \textit{posterior distributions} for the parameters of interest, providing a \textit{multi-dimensional} representation of uncertainty for \textit{individual} measurements. We showcase this ability by performing an in-silico uncertainty analysis of five biomarkers of clinical interest comparing several measurement modalities. Beyond the corroboration of known facts, such as the feasibility of estimating heart rate, our study highlights the potential of estimating new biomarkers from standard-of-care measurements. SBI reveals practically relevant findings that cannot be captured by standard sensitivity analyses, such as the existence of sub-populations for which parameter estimation exhibits distinct uncertainty regimes. Finally, we study the gap between in-vivo and in-silico with the MIMIC-III waveform database and critically discuss how cardiovascular simulations can inform real-world data analysis.
</details>
<details>
<summary>摘要</summary>
Inspired by advances in simulation-based inference (SBI), we approach this inverse problem as a statistical inference problem. Unlike other methods, SBI provides a distribution of posterior probabilities for the parameters of interest, offering a comprehensive and multi-dimensional representation of uncertainty for individual measurements.We demonstrate the power of SBI by performing an in-silico uncertainty analysis of five biomarkers of clinical interest using different measurement modalities. Our study not only confirms established findings, such as the feasibility of estimating heart rate, but also highlights the potential of estimating new biomarkers from standard-of-care measurements.SBI reveals practical insights that cannot be obtained through standard sensitivity analyses, such as the existence of sub-populations with distinct uncertainty regimes. Furthermore, we explore the gap between in-vivo and in-silico using the MIMIC-III waveform database and discuss how cardiovascular simulations can inform real-world data analysis.
</details></li>
</ul>
<hr>
<h2 id="BayesDAG-Gradient-Based-Posterior-Sampling-for-Causal-Discovery"><a href="#BayesDAG-Gradient-Based-Posterior-Sampling-for-Causal-Discovery" class="headerlink" title="BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery"></a>BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13917">http://arxiv.org/abs/2307.13917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yashas Annadani, Nick Pawlowski, Joel Jennings, Stefan Bauer, Cheng Zhang, Wenbo Gong</li>
<li>for:  This paper aims to develop a scalable Bayesian causal discovery framework for inferring the posterior distribution over causal models from observed data, addressing computational challenges in joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions.</li>
<li>methods:  The proposed framework uses stochastic gradient Markov Chain Monte Carlo (SG-MCMC) to directly sample DAGs from the posterior without requiring any DAG regularization, simultaneously drawing function parameter samples and applicable to both linear and nonlinear causal models.</li>
<li>results:  Empirical evaluations on synthetic and real-world datasets demonstrate the effectiveness of the proposed approach compared to state-of-the-art baselines.<details>
<summary>Abstract</summary>
Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models. To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations. To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery. Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
bayesian causal discovery aimed to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models. To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations. To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery. Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Online-learning-in-bandits-with-predicted-context"><a href="#Online-learning-in-bandits-with-predicted-context" class="headerlink" title="Online learning in bandits with predicted context"></a>Online learning in bandits with predicted context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13916">http://arxiv.org/abs/2307.13916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyi Guo, Susan Murphy</li>
<li>for:  solves the contextual bandit problem with non-diminishing context error.</li>
<li>methods:  uses an extension of the measurement error model in classical statistics to the online decision-making setting.</li>
<li>results:  achieves sublinear regret compared to the appropriate benchmark, despite the non-diminishing context error.<details>
<summary>Abstract</summary>
We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-diminishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret compared to the appropriate benchmark. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations.
</details>
<details>
<summary>摘要</summary>
我们考虑了上下文搅拌问题，在每个时间点，代理人只有访问不准确的上下文和错误方差（或一个估计这种方差）的权限。这种设定是由各种应用领域中真实的上下文决策不可见，只有一个复杂机器学习算法预测的上下文预测所 inspirited。当上下文错误不断增长时， klasik bandit算法无法实现子线性 regret。我们提议的首个在这种设定下的在线算法具有子线性 regret，相对于合适的 benchmark。关键思想是将经典统计中的测量错误模型扩展到在线决策设定中，这是因为政策对于听到的不准确上下文观察有依赖关系。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-based-Hybrid-Framework-For-Predicting-Particle-Crushing-Strength"><a href="#Graph-Neural-Networks-based-Hybrid-Framework-For-Predicting-Particle-Crushing-Strength" class="headerlink" title="Graph Neural Networks-based Hybrid Framework For Predicting Particle Crushing Strength"></a>Graph Neural Networks-based Hybrid Framework For Predicting Particle Crushing Strength</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13909">http://arxiv.org/abs/2307.13909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/doujiang-zheng/gnn-for-particle-crushing">https://github.com/doujiang-zheng/gnn-for-particle-crushing</a></li>
<li>paper_authors: Tongya Zheng, Tianli Zhang, Qingzheng Guan, Wenjie Huang, Zunlei Feng, Mingli Song, Chun Chen</li>
<li>for:  This paper aims to apply Graph Neural Networks (GNNs) to model the mechanical behaviors of particle crushing and predict particle crushing strength.</li>
<li>methods: The authors use a hybrid framework based on GNNs to predict particle crushing strength in a particle fragment view, and compare their method against traditional machine learning methods and a plain Multi-Layer Perceptron (MLP).</li>
<li>results: The authors verify the effectiveness of their hybrid framework through numerical simulations and discuss the usefulness of different features through gradient attribution explanation.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是应用图гра树神经网络（GNNs）来模型粉体压碎的机械行为并预测粉体压碎强度。</li>
<li>methods: 作者使用一种混合方法基于GNNs来预测粉体压碎强度在粉体Fragment视图中，并与传统机器学习方法和简单的多层感知网络（MLP）进行比较。</li>
<li>results: 作者通过数值仿真 verify了他们的混合方法的有效性，并通过Gradient attribute解释来评估不同特征的用于预测。<details>
<summary>Abstract</summary>
Graph Neural Networks have emerged as an effective machine learning tool for multi-disciplinary tasks such as pharmaceutical molecule classification and chemical reaction prediction, because they can model non-euclidean relationships between different entities. Particle crushing, as a significant field of civil engineering, describes the breakage of granular materials caused by the breakage of particle fragment bonds under the modeling of numerical simulations, which motivates us to characterize the mechanical behaviors of particle crushing through the connectivity of particle fragments with Graph Neural Networks (GNNs). However, there lacks an open-source large-scale particle crushing dataset for research due to the expensive costs of laboratory tests or numerical simulations. Therefore, we firstly generate a dataset with 45,000 numerical simulations and 900 particle types to facilitate the research progress of machine learning for particle crushing. Secondly, we devise a hybrid framework based on GNNs to predict particle crushing strength in a particle fragment view with the advances of state of the art GNNs. Finally, we compare our hybrid framework against traditional machine learning methods and the plain MLP to verify its effectiveness. The usefulness of different features is further discussed through the gradient attribution explanation w.r.t the predictions. Our data and code are released at https://github.com/doujiang-zheng/GNN-For-Particle-Crushing.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 已成为多学科领域中有效的机器学习工具，如药品分类和化学反应预测，因为它们可以模型不同实体之间的非欧几何关系。在 грануляр材料破碎中， particle crushing 是一个重要的领域，描述了由 particle fragment 键结的破碎物质的破碎过程，这种情况激发我们通过 GNNs 来描述破碎物质的机械行为。然而，由于实验室试验或数值仿真的高昂成本，在这个领域中没有公开的大规模 particle crushing 数据集，这限制了研究的进步。因此，我们首先生成了一个包含 45,000 个数值仿真和 900 种 particle type 的数据集，以便促进机器学习在 particle crushing 中的研究进步。其次，我们提出了基于 GNNs 的混合框架，用于预测 particle crushing 强度在 particle fragment 视图中。最后，我们与传统机器学习方法和简单的多层感知网络进行比较，以验证我们的混合框架的有效性。此外，我们还对不同特征的使用进行了探索，并通过对预测结果的梯度评估来进行解释。我们的数据和代码在 GitHub 上发布，请参考 https://github.com/doujiang-zheng/GNN-For-Particle-Crushing。
</details></li>
</ul>
<hr>
<h2 id="Robustness-Verification-of-Deep-Neural-Networks-using-Star-Based-Reachability-Analysis-with-Variable-Length-Time-Series-Input"><a href="#Robustness-Verification-of-Deep-Neural-Networks-using-Star-Based-Reachability-Analysis-with-Variable-Length-Time-Series-Input" class="headerlink" title="Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input"></a>Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13907">http://arxiv.org/abs/2307.13907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neelanjana Pal, Diego Manzanas Lopez, Taylor T Johnson</li>
<li>for: 这篇论文的目的是提出一种基于神经网络的时间序列数据分析方法，以便在实际应用中进行精准的预测和维护。</li>
<li>methods: 这篇论文使用了时间序列 regression neural network (TSRegNN)，并使用了变量长度输入数据来简化输入处理和提高网络架构的通用性。</li>
<li>results: 该论文通过使用星形可达性分析和一些性能指标来检验神经网络的可靠性，并证明了神经网络在真实应用中的精准预测和可靠性是受到输入噪声的影响的。<details>
<summary>Abstract</summary>
Data-driven, neural network (NN) based anomaly detection and predictive maintenance are emerging research areas. NN-based analytics of time-series data offer valuable insights into past behaviors and estimates of critical parameters like remaining useful life (RUL) of equipment and state-of-charge (SOC) of batteries. However, input time series data can be exposed to intentional or unintentional noise when passing through sensors, necessitating robust validation and verification of these NNs. This paper presents a case study of the robustness verification approach for time series regression NNs (TSRegNN) using set-based formal methods. It focuses on utilizing variable-length input data to streamline input manipulation and enhance network architecture generalizability. The method is applied to two data sets in the Prognostics and Health Management (PHM) application areas: (1) SOC estimation of a Lithium-ion battery and (2) RUL estimation of a turbine engine. The NNs' robustness is checked using star-based reachability analysis, and several performance measures evaluate the effect of bounded perturbations in the input on network outputs, i.e., future outcomes. Overall, the paper offers a comprehensive case study for validating and verifying NN-based analytics of time-series data in real-world applications, emphasizing the importance of robustness testing for accurate and reliable predictions, especially considering the impact of noise on future outcomes.
</details>
<details>
<summary>摘要</summary>
数据驱动、基于神经网络（NN）的异常检测和预测维护是当前的研究领域之一。NN基础的时间序列数据分析可以为过去行为提供有价值的洞察，包括设备的剩余有用生命（RUL）和电池的状态充电（SOC）等重要参数的估计。然而，输入时间序列数据可能会受到意外或非意外的噪声的影响，因此需要对这些NN进行robust验证和验证。本文介绍了一种基于集合形式方法的NN验证方法，旨在使用可变长度的输入数据来简化输入处理并提高网络架构的通用性。这种方法在两个PHM应用领域的数据集上进行了应用：（1）锂离子电池SOC估计和（2）涡轮机RUL估计。通过星形可达性分析来检查NN的 Robustness，并使用一些性能指标来评估输入噪声的影响 på network输出，即未来的结果。总的来说，本文提供了一个完整的实践案例，用于验证和验证基于时间序列数据的NN分析，强调验证过程中噪声的影响，以确保准确可靠的预测，特别是在噪声的情况下。
</details></li>
</ul>
<hr>
<h2 id="Corruption-Robust-Lipschitz-Contextual-Search"><a href="#Corruption-Robust-Lipschitz-Contextual-Search" class="headerlink" title="Corruption-Robust Lipschitz Contextual Search"></a>Corruption-Robust Lipschitz Contextual Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13903">http://arxiv.org/abs/2307.13903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiliang Zuo</li>
<li>for: 学习一个 lipschitz 函数，对于随机选择的上下文向量 $x_t$ 和 adversary 选择的真实函数值 $f(x_t)$ 进行推断。</li>
<li>methods: 使用 natural yet powerful technique sanity check，并设计了 robust 算法，可以在 $C$ 轮游戏中减少总损失。</li>
<li>results: 对于均匀损失， learner 可以取得 regret $O(C\log T)$，其中 $d &#x3D; 1$ 时为 $O(C\log T)$，$d &gt; 1$ 时为 $O_d(C\log T + T^{(d-1)&#x2F;d})$。对于价格损失，learner 可以取得 regret $\widetilde{O}(T^{d&#x2F;(d+1)} + C\cdot T^{1&#x2F;(d+1)})$。<details>
<summary>Abstract</summary>
I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d > 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
</details>
<details>
<summary>摘要</summary>
我研究学习受损函数问题，即 adversary 选择函数 $f$，learner 根据输入空间中的 context vector $x_t$ 猜测函数值，并接受一个 binary signal 表示猜测是高或低。在总共 $C$ 轮中，signal 可能受损，但 learner 不知道 $C$ 的值。learner 的目标是减少总的损失。我提出了一种自然强大的检查技术，对于设计受损函数 algorithms 非常有用。我设计了 algorithms，对于对称损失函数，learner 可以达到 regret $O(C\log T)$  avec $d = 1$ 和 $O_d(C\log T + T^{(d-1)/d})$  avec $d > 1$ ;对于价格损失函数，learner 可以达到 regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$。
</details></li>
</ul>
<hr>
<h2 id="Regularizing-Neural-Networks-with-Meta-Learning-Generative-Models"><a href="#Regularizing-Neural-Networks-with-Meta-Learning-Generative-Models" class="headerlink" title="Regularizing Neural Networks with Meta-Learning Generative Models"></a>Regularizing Neural Networks with Meta-Learning Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13899">http://arxiv.org/abs/2307.13899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shin’ya Yamaguchi, Daiki Chijiwa, Sekitoshi Kanai, Atsutoshi Kumagai, Hisashi Kashima</li>
<li>for: 提高深度学习中的生成数据增强（Generative Data Augmentation，GDA）的方法，以便在小样本大小的情况下提高分类精度。</li>
<li>methods: 提出了一种新的生成数据增强策略——元生成准则（Meta Generative Regularization，MGR），通过在特征提取器中使用生成样本来减少Validation损失，从而避免生成数据增强导致的性能下降。</li>
<li>results: 对六个预测集进行了实验，发现MGR可以避免生成数据增强导致的性能下降，并在小样本大小的情况下稳定地超越基elines。<details>
<summary>Abstract</summary>
This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR can avoid the performance degradation of na\"ive generative data augmentation and boost the baselines. Experiments on six datasets showed that MGR is effective particularly when datasets are smaller and stably outperforms baselines.
</details>
<details>
<summary>摘要</summary>
Instead of using synthetic samples in the loss function, such as cross-entropy, MGR incorporates them into the regularization term for feature extractors. This approach allows for the dynamic determination of synthetic samples to minimize validation losses through meta-learning. Our experiments on six datasets demonstrated that MGR can effectively avoid the performance degradation of traditional generative data augmentation and consistently outperform baselines, particularly when the datasets are smaller.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Estimation-of-the-Local-Robustness-of-Machine-Learning-Models"><a href="#Efficient-Estimation-of-the-Local-Robustness-of-Machine-Learning-Models" class="headerlink" title="Efficient Estimation of the Local Robustness of Machine Learning Models"></a>Efficient Estimation of the Local Robustness of Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13885">http://arxiv.org/abs/2307.13885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tessa Han, Suraj Srinivas, Himabindu Lakkaraju</li>
<li>for: 本研究旨在提高机器学习模型对噪声输入数据的Robustness。</li>
<li>methods: 本文提出了首个分析性Estimators来效率地计算多类推论模型的本地Robustness，通过地方线性函数近似和多变量Normal CDF，并与随机抖动、软max概率之间的关系进行链接。</li>
<li>results: 本文confirm empirically这些Estimators可以有效和高效地计算标准深度学习模型的本地Robustness，并用于多种任务，如测试模型的Robustness偏见和找到数据集中噪声扰动的例子。<details>
<summary>Abstract</summary>
Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estimators' usefulness for various tasks involving local robustness, such as measuring robustness bias and identifying examples that are vulnerable to noise perturbation in a dataset. By developing these analytical estimators, this work not only advances conceptual understanding of local robustness, but also makes its computation practical, enabling the use of local robustness in critical downstream applications.
</details>
<details>
<summary>摘要</summary>
（Machine learning模型经常需要对噪音输入数据具有鲁棒性。噪音的影响在模型预测中被捕捉在一个本地区域中，即模型在输入附近的一致性。但是，使用 Monte-Carlo 采样来计算本地鲁棒性的方法是统计不有效的，会导致大规模应用程序的计算成本过高。在这种情况下，我们开发了第一个分析式估计器，可以有效地计算多类推论模型的本地鲁棒性。我们使用本地线性函数近似和多变量正态分布函数来 derive这些估计器，并证明了本地鲁棒性与随机缓和软max概率之间的关系。我们还通过实验证明了这些估计器可以准确地和高效地计算标准深度学习模型的本地鲁棒性。此外，我们还示出了这些估计器在不同任务中的有用性，如测量鲁棒性偏见和 dataset 中噪音扰动的示例。通过开发这些分析式估计器，这些研究不仅提高了本地鲁棒性的概念理解，还使其计算变得实际可行，使其在重要的下游应用中使用。）
</details></li>
</ul>
<hr>
<h2 id="ExeDec-Execution-Decomposition-for-Compositional-Generalization-in-Neural-Program-Synthesis"><a href="#ExeDec-Execution-Decomposition-for-Compositional-Generalization-in-Neural-Program-Synthesis" class="headerlink" title="ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis"></a>ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13883">http://arxiv.org/abs/2307.13883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kensen Shi, Joey Hong, Manzil Zaheer, Pengcheng Yin, Charles Sutton</li>
<li>for: 本研究旨在探讨人工智能程序生成方法是否具有分解复杂任务为 simpler subtask 的能力，以及这种能力是否可以推广到更复杂的任务。</li>
<li>methods: 本研究使用了多种形式的 compositional generalization，包括程序执行目标预测、程序分解和重构等，以形成一个 meta-benchmark，用于评估不同方法的总体性能。</li>
<li>results: 研究发现，使用 ExeDec  decomposition-based 程序生成策略可以更好地满足不同任务的需求，并且具有显著的总体性能和 compositional generalization 能力，比基eline方法更好。<details>
<summary>Abstract</summary>
When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.
</details>
<details>
<summary>摘要</summary>
当编写程序时，人们有能力将复杂任务拆分成更熟悉的子任务。虽然无法测量神经程序合成方法的类似能力，但我们可以测量它们是否可以扩展，即一个已经在更简单的子任务上训练的模型是否可以解决更复杂的任务。在这篇论文中，我们描述了几种不同的拆分总结的形式，这些形式是程序合成中的总结，我们使用这些形式创建了一个元benchmark，并用这个元benchmark来创建了RobustFill和DeepCoder两个 популяр的数据集的通用化任务。然后，我们提出了ExeDec，一种新的分解基本的合成策略，该策略预测执行子任务以解决问题步骤通过程序执行的信息。ExeDec的合成性能和拆分总结能力都比基eline要好。
</details></li>
</ul>
<hr>
<h2 id="Good-Lattice-Training-Physics-Informed-Neural-Networks-Accelerated-by-Number-Theory"><a href="#Good-Lattice-Training-Physics-Informed-Neural-Networks-Accelerated-by-Number-Theory" class="headerlink" title="Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory"></a>Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13869">http://arxiv.org/abs/2307.13869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takashi Matsubara, Takaharu Yaguchi</li>
<li>for: 解决 partial differential equations (PDEs) 的novel和高效的方法</li>
<li>methods: 使用 physics-informed loss 训练神经网络，并选择合适的 collocation points</li>
<li>results: 提出 good lattice training (GLT) 技术，可以在小量的 collocation points 下达到竞争力的性能，并且比uniformly random sampling或Latin hypercube sampling 更有效率<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs). Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution. However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain. Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked. In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis. GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces. Our experiments demonstrate that GLT requires 2--20 times fewer collocation points (resulting in lower computational cost) than uniformly random sampling or Latin hypercube sampling, while achieving competitive performance.
</details>
<details>
<summary>摘要</summary>
物理学 Informed neural networks (PINNs) 提供了一种新的和高效的方法来解决部分 differential equations (PDEs)。它们的成功归功于物理学 Informed loss，该loss 训练一个神经网络满足给定 PDE 的特定点和近似解。然而，解决 PDE 的解是自然 infinite-dimensional，而且Distance  между输出和解是通过Domain 上的积分来定义的。因此，物理学 Informed loss 只提供了finite approximation，并且选择合适的拓扑点变得非常重要，以抑制精度损失，尽管这一点经常被忽略。在这篇论文中，我们提出了一种新的技术called good lattice training (GLT) for PINNs， inspirited by numerical analysis 的数学方法。GLT 提供了一组高效的拓扑点，可以在小量点和多维空间中实现高效。我们的实验表明，GLT 需要2--20倍 fewer collocation points（相对于随机抽样或拉丁hypercube sampling），而且可以达到竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-sources-of-variability-from-high-dimensional-observational-studies"><a href="#Learning-sources-of-variability-from-high-dimensional-observational-studies" class="headerlink" title="Learning sources of variability from high-dimensional observational studies"></a>Learning sources of variability from high-dimensional observational studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13868">http://arxiv.org/abs/2307.13868</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ebridge2/cdcorr">https://github.com/ebridge2/cdcorr</a></li>
<li>paper_authors: Eric W. Bridgeford, Jaewon Chung, Brian Gilbert, Sambit Panda, Adam Li, Cencheng Shen, Alexandra Badea, Brian Caffo, Joshua T. Vogelstein</li>
<li>for: 该研究探讨了 causal inference 是否会影响观察到的结果，以及这种方法在各种生物领域中的应用，包括疫苗和药物开发、政策干预等。</li>
<li>methods: 该研究扩展了 causal estimands 到多维或任意可测量空间上的结果，并将 nominal 变量的 causal estimands 转化为 causal discrepancy tests。提出了一种简单的方法来调整 Conditional Independence tests，并证明这些测试是 universally consistent causal discrepancy tests。</li>
<li>results: 数据实验表明，该方法（Causal CDcorr）在样本数量和权限下的效果和力度都有所提高，相比之下存在的其他策略。该方法的代码都是开源的，可以在 github.com&#x2F;ebridge2&#x2F;cdcorr 上下载。<details>
<summary>Abstract</summary>
Causal inference studies whether the presence of a variable influences an observed outcome. As measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. Unfortunately, the majority of these methods are often limited to univariate outcomes. Our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. We propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. Numerical experiments illustrate that our method, Causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. Our methods are all open source and available at github.com/ebridge2/cdcorr.
</details>
<details>
<summary>摘要</summary>
causal inference studies whether a variable's presence affects an observed outcome. as measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. unfortunately, the majority of these methods are often limited to univariate outcomes. our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. we propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. numerical experiments illustrate that our method, causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. our methods are all open source and available at github.com/ebridge2/cdcorr.Here's the translation in Traditional Chinese:causal inference studies whether a variable's presence affects an observed outcome. as measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. unfortunately, the majority of these methods are often limited to univariate outcomes. our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. we propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. numerical experiments illustrate that our method, causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. our methods are all open source and available at github.com/ebridge2/cdcorr.
</details></li>
</ul>
<hr>
<h2 id="Pretrained-Deep-2-5D-Models-for-Efficient-Predictive-Modeling-from-Retinal-OCT"><a href="#Pretrained-Deep-2-5D-Models-for-Efficient-Predictive-Modeling-from-Retinal-OCT" class="headerlink" title="Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT"></a>Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13865">http://arxiv.org/abs/2307.13865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taha Emre, Marzieh Oghbaie, Arunava Chakravarty, Antoine Rivail, Sophie Riedl, Julia Mai, Hendrik P. N. Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Ursula Schmidt-Erfurth, Hrvoje Bogunović</li>
<li>for: 预测老年眼肿症（AMD）进展，提高医疗影像识别的精度和效率。</li>
<li>methods: 组合2D和3D技术，使用卷积神经网络（CNN）、长短期记忆（LSTM）和变换器，提高性能和数据效率。</li>
<li>results: 在两个大 longitudinal OCT 数据集上，验证了这些架构和预训练方法的效果，可以准确预测在6个月内进展到湿性年轻眼肿症（AMD）。<details>
<summary>Abstract</summary>
In the field of medical imaging, 3D deep learning models play a crucial role in building powerful predictive models of disease progression. However, the size of these models presents significant challenges, both in terms of computational resources and data requirements. Moreover, achieving high-quality pretraining of 3D models proves to be even more challenging. To address these issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D volumetric data efficiently using 2D models. Combining 2D and 3D techniques offers a promising avenue for optimizing performance while minimizing memory requirements. In this paper, we explore 2.5D architectures based on a combination of convolutional neural networks (CNNs), long short-term memory (LSTM), and Transformers. In addition, leveraging the benefits of recent non-contrastive pretraining approaches in 2D, we enhanced the performance and data efficiency of 2.5D techniques even further. We demonstrate the effectiveness of architectures and associated pretraining on a task of predicting progression to wet age-related macular degeneration (AMD) within a six-month period on two large longitudinal OCT datasets.
</details>
<details>
<summary>摘要</summary>
医疗影像领域中，3D深度学习模型在建立疾病发展预测模型方面扮演着关键角色。然而，这些模型的大小带来了计算资源和数据需求的挑战。此外，实现高质量预训练3D模型也是非常困难的。为解决这些问题，混合2.5D方法提供了一种高效地利用3D栅格数据的方法。将2D和3D技术结合起来，可以提高性能的同时减少内存需求。在这篇论文中，我们探讨了基于卷积神经网络（CNN）、长期短记忆（LSTM）和变换器的2.5D架构。此外，利用2D非对抗预训练方法的优点，我们进一步提高了2.5D技术的性能和数据效率。我们在两个大 longitudinal OCT数据集上预测了在6个月内进行湿性macular degeneration（AMD）的发展预测任务，以证明架构和预训练的效果。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Design-Analog-Circuits-to-Meet-Threshold-Specifications"><a href="#Learning-to-Design-Analog-Circuits-to-Meet-Threshold-Specifications" class="headerlink" title="Learning to Design Analog Circuits to Meet Threshold Specifications"></a>Learning to Design Analog Circuits to Meet Threshold Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13861">http://arxiv.org/abs/2307.13861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/indylab/circuit-synthesis">https://github.com/indylab/circuit-synthesis</a></li>
<li>paper_authors: Dmitrii Krylov, Pooya Khajeh, Junhan Ouyang, Thomas Reeves, Tongkai Liu, Hiba Ajmal, Hamidreza Aghasi, Roy Fox</li>
<li>for: 本研究旨在提出一种基于 simulation 数据的自动化Analog和广播频率电路设计方法，以替代专业设计师的手动设计。</li>
<li>methods: 该方法通过学习 inverse function 从 desired performance metrics 中学习出 circuit parameters。</li>
<li>results: 该方法可以在5% error margin 下达到90%的成功率，并且可以提高数据使用效率。In English:</li>
<li>for: The paper proposes an automated design method for analog and radio-frequency circuits using supervised or reinforcement learning from simulation data, as an alternative to manual expert design.</li>
<li>methods: The method learns an inverse function from desired performance metrics to circuit parameters.</li>
<li>results: The method achieves a success rate of over 90% with an error margin of 5%, and improves data efficiency by up to an order of magnitude.<details>
<summary>Abstract</summary>
Automated design of analog and radio-frequency circuits using supervised or reinforcement learning from simulation data has recently been studied as an alternative to manual expert design. It is straightforward for a design agent to learn an inverse function from desired performance metrics to circuit parameters. However, it is more common for a user to have threshold performance criteria rather than an exact target vector of feasible performance measures. In this work, we propose a method for generating from simulation data a dataset on which a system can be trained via supervised learning to design circuits to meet threshold specifications. We moreover perform the to-date most extensive evaluation of automated analog circuit design, including experimenting in a significantly more diverse set of circuits than in prior work, covering linear, nonlinear, and autonomous circuit configurations, and show that our method consistently reaches success rate better than 90% at 5% error margin, while also improving data efficiency by upward of an order of magnitude. A demo of this system is available at circuits.streamlit.app
</details>
<details>
<summary>摘要</summary>
自动设计分析和频率电路使用监督或增强学习法，从实验数据学习，对于专家设计来说是一种新的替代方案。它容易 для设计代理人从需求性能指标学习逆函数。但是，用户更常会有阈值性能标准而不是精确的可行性表现标准。在这个工作中，我们提出了将从实验数据生成一个可以透过监督学习训练系统，以满足阈值需求的数据集。我们还进行了过去最大的自动分析电路设计评估，包括在更加多样化的电路配置中实验，包括线性、非线性和自主电路配置，并证明了我们的方法可以在5% error margin下，实现90%的成功率，同时也提高了数据效率，提高了一个阶层。一个demo这个系统可以在circuits.streamlit.app中找到。
</details></li>
</ul>
<hr>
<h2 id="On-the-unreasonable-vulnerability-of-transformers-for-image-restoration-–-and-an-easy-fix"><a href="#On-the-unreasonable-vulnerability-of-transformers-for-image-restoration-–-and-an-easy-fix" class="headerlink" title="On the unreasonable vulnerability of transformers for image restoration – and an easy fix"></a>On the unreasonable vulnerability of transformers for image restoration – and an easy fix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13856">http://arxiv.org/abs/2307.13856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Agnihotri, Kanchana Vaishnavi Gandikota, Julia Grabinski, Paramanand Chandramouli, Margret Keuper</li>
<li>for: 这种研究探讨了使用视Transformers（ViTs）进行图像修复任务中的图像修复模型的Robustness。</li>
<li>methods: 我们使用Projected Gradient Descent（PGD）和CosPGD，一种特定于像素预测任务的敏感攻击，来评估这些模型的Robustness。</li>
<li>results: 我们发现，与前期研究所 advocated的相反，这些模型在敏感攻击下高度易受攻击。我们通过对这些模型进行Robustness Training来提高其Robustness，但结果不太乐见。另外，NAFNet和Baseline网络的设计选择，基于iid性性能而不是Robust generalization，似乎与模型的Robustness相抵触。<details>
<summary>Abstract</summary>
Following their success in visual recognition tasks, Vision Transformers(ViTs) are being increasingly employed for image restoration. As a few recent works claim that ViTs for image classification also have better robustness properties, we investigate whether the improved adversarial robustness of ViTs extends to image restoration. We consider the recently proposed Restormer model, as well as NAFNet and the "Baseline network" which are both simplified versions of a Restormer. We use Projected Gradient Descent (PGD) and CosPGD, a recently proposed adversarial attack tailored to pixel-wise prediction tasks for our robustness evaluation. Our experiments are performed on real-world images from the GoPro dataset for image deblurring. Our analysis indicates that contrary to as advocated by ViTs in image classification works, these models are highly susceptible to adversarial attacks. We attempt to improve their robustness through adversarial training. While this yields a significant increase in robustness for Restormer, results on other networks are less promising. Interestingly, the design choices in NAFNet and Baselines, which were based on iid performance, and not on robust generalization, seem to be at odds with the model robustness. Thus, we investigate this further and find a fix.
</details>
<details>
<summary>摘要</summary>
“随着当前视觉变数任务的成功，视觉对映器（ViT）正在不断地被应用于图像修复。一些最近的研究表明，ViT在图像分类任务中也有更好的韧性特性，我们进一步探索这些韧性特性是否扩展到图像修复。我们考虑了RecentRestormer模型，以及NAFNet和基eline网络，这些都是Restormer的简化版本。我们使用预测向量的投影Gradient Descent（PGD）和CosPGD，这是一种特别针对像素精度预测任务的攻击方法来进行我们的Robustness评估。我们对GoPro图像滤过 dataset上的实际图像进行了实验。我们的分析表明，与在图像分类任务中所说的相反，这些模型在这些任务中具有强大的攻击敏感性。我们尝试通过对模型进行防御训练来改善其 Robustness。与此同时，我们发现NAFNet和基eline网络的设计决策，它们是基于独立性表现而不是关于韧性的设计决策，似乎与模型的韧性不匹配。因此，我们进一步探索这个问题，并发现一个解决方案。”
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Sharpened-Cosine-Similarity"><a href="#Exploring-the-Sharpened-Cosine-Similarity" class="headerlink" title="Exploring the Sharpened Cosine Similarity"></a>Exploring the Sharpened Cosine Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13855">http://arxiv.org/abs/2307.13855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skyler Wu, Fred Lu, Edward Raff, James Holt</li>
<li>for: 本研究探讨了使用新的激活函数Sharpened Cosine Similarity（SCS） instead of传统的卷积层来进行图像分类。</li>
<li>methods: 本研究使用了多种 CNN 架构，并对 CIFAR-10 数据集进行了大规模的实验分析。</li>
<li>results: 研究发现，使用 SCS 可能不会提高准确率，但可能学习更易于理解的表示。此外，在某些情况下，SCS 可能会提高鲁棒性。<details>
<summary>Abstract</summary>
Convolutional layers have long served as the primary workhorse for image classification. Recently, an alternative to convolution was proposed using the Sharpened Cosine Similarity (SCS), which in theory may serve as a better feature detector. While multiple sources report promising results, there has not been to date a full-scale empirical analysis of neural network performance using these new layers. In our work, we explore SCS's parameter behavior and potential as a drop-in replacement for convolutions in multiple CNN architectures benchmarked on CIFAR-10. We find that while SCS may not yield significant increases in accuracy, it may learn more interpretable representations. We also find that, in some circumstances, SCS may confer a slight increase in adversarial robustness.
</details>
<details>
<summary>摘要</summary>
卷积层长期以来为图像分类任务中的主力工具。最近，一种使用加强的余弦相似性（SCS）来代替卷积的方案被提出，据说可能更好地检测特征。虽然多种来源报道了这些新层的批处结果，但到目前为止没有进行了全面的实验分析。在我们的工作中，我们探索SCS的参数行为和作为卷积Drop-in替换的潜在可能性，并在CIFAR-10上对多种CNN Architecture进行了多种测试。我们发现，虽然SCS可能不会导致显著增加准确率，但它可能学习更易于理解的表示。此外，在某些情况下，SCS可能会增加一定的逆向抗性。
</details></li>
</ul>
<hr>
<h2 id="WebArena-A-Realistic-Web-Environment-for-Building-Autonomous-Agents"><a href="#WebArena-A-Realistic-Web-Environment-for-Building-Autonomous-Agents" class="headerlink" title="WebArena: A Realistic Web Environment for Building Autonomous Agents"></a>WebArena: A Realistic Web Environment for Building Autonomous Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13854">http://arxiv.org/abs/2307.13854</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/web-arena-x/webarena">https://github.com/web-arena-x/webarena</a></li>
<li>paper_authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig</li>
<li>for: 这种研究旨在创建一个高度实际和可重现的自动化代理控制环境，以便用自然语言命令管理日常任务。</li>
<li>methods: 这篇论文使用了现代自然语言处理技术，如理智语言模型（GPT-4），以及一些最新的解释和行为决策技术。</li>
<li>results: 研究发现，使用现有的状态elia-of-the-art语言模型（GPT-4）解决复杂任务时存在挑战，最高终端任务成功率只有10.59%。这些结果表明需要进一步发展更加可靠的自动化代理，并且现有的语言模型在这些实际任务中的表现并不理想。<details>
<summary>Abstract</summary>
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on the internet. We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59%. These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress. Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/.
</details>
<details>
<summary>摘要</summary>
“受到生成AI的推进，自动化代理人可以通过自然语言指令进行日常任务管理，这具有吸引人的潜力。然而，目前的代理人主要是在简化的人工环境中设计和测试，实际上仅仅代表了实际世界的一部分。在这篇论文中，我们建立了一个高度现实和可重现的环境，以便自动化代理人进行命令和控制。 Specifically，我们专注在网站上进行任务的代理人，并创建了四种常见的领域中的完整网站：电子商务、社群讨论论坛、协同软件开发和内容管理。我们的环境扩展了工具（例如地图）和外部知识库（例如用户手册），以促进人类化的任务解决。在这基础之上，我们发布了一组对任务完成的评估标准，这些任务具有多样性、长期性和模拟人类在网页上进行常规任务的特点。我们设计和实现了一些自动化代理人，应用最新的技术，例如理解才行。结果显示，解决复杂任务是具有挑战性：我们的最佳GPT-4基于代理人仅取得了10.59%的终端任务成功率。这些结果显示现代LM的表现仍有很大的改善空间，并且WebArena可以用来衡量这种进步。我们的代码、数据、环境重现资源和视频示例都公开 disponíveis于https://webarena.dev/.”
</details></li>
</ul>
<hr>
<h2 id="SplitFed-resilience-to-packet-loss-Where-to-split-that-is-the-question"><a href="#SplitFed-resilience-to-packet-loss-Where-to-split-that-is-the-question" class="headerlink" title="SplitFed resilience to packet loss: Where to split, that is the question"></a>SplitFed resilience to packet loss: Where to split, that is the question</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13851">http://arxiv.org/abs/2307.13851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chamani Shiranthika, Zahra Hafezi Kafshgari, Parvaneh Saeedi, Ivan V. Bajić</li>
<li>for: 这篇论文探讨了Split Federated Learning（SplitFed或SFL）在面对包列失败的情况下的稳定性。</li>
<li>methods: 这篇论文使用了将模型分割在两个点上（浅分割和深分割），然后测试这些分割点对模型的准确率是否有 statistically significant difference。</li>
<li>results: 实验结果表明，使用深分割点可以获得更高的准确率。<details>
<summary>Abstract</summary>
Decentralized machine learning has broadened its scope recently with the invention of Federated Learning (FL), Split Learning (SL), and their hybrids like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce the computational power required by each client in FL and parallelize SL while maintaining privacy. This paper investigates the robustness of SFL against packet loss on communication links. The performance of various SFL aggregation strategies is examined by splitting the model at two points -- shallow split and deep split -- and testing whether the split point makes a statistically significant difference to the accuracy of the final model. Experiments are carried out on a segmentation model for human embryo images and indicate the statistically significant advantage of a deeper split point.
</details>
<details>
<summary>摘要</summary>
《协同学习的扩展：从分布式学习到分布式 Federated Learning》Recently, decentralized machine learning has expanded its scope with the invention of Federated Learning (FL), Split Learning (SL), and their hybrids like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce the computational power required by each client in FL and parallelize SL while maintaining privacy. This paper investigates the robustness of SFL against packet loss on communication links. The performance of various SFL aggregation strategies is examined by splitting the model at two points -- shallow split and deep split -- and testing whether the split point makes a statistically significant difference to the accuracy of the final model. Experiments are carried out on a segmentation model for human embryo images and indicate the statistically significant advantage of a deeper split point.
</details></li>
</ul>
<hr>
<h2 id="MAEA-Multimodal-Attribution-for-Embodied-AI"><a href="#MAEA-Multimodal-Attribution-for-Embodied-AI" class="headerlink" title="MAEA: Multimodal Attribution for Embodied AI"></a>MAEA: Multimodal Attribution for Embodied AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13850">http://arxiv.org/abs/2307.13850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidhi Jain, Jayant Sravan Tamarapalli, Sahiti Yerramilli, Yonatan Bisk<br>for: 这个论文的目的是解决embodied AI中的多modal感知问题，因为输入信息可能包含高度相互补充的信息。methods: 这篇论文使用了解释ALFRED数据集上不同策略中每种modal输入的贡献分析，以便理解每种modal输入的全局趋势。results: 该研究发现了一种名为MAEA的框架，可以计算任何可微分策略的全局贡献分析。此外，研究还显示了在EAI策略中语言和视觉贡献的下一个行为分析。<details>
<summary>Abstract</summary>
Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer. To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. We present MAEA, a framework to compute global attributions per modality of any differentiable policy. In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.
</details>
<details>
<summary>摘要</summary>
（文本翻译）理解多模态识别对固有AI是一个开放的问题，因为输入可能包含高度相互补做的信息。一个有关的方向是理解不同模态的全球趋势在融合层。为此，我们分离不同策略在ALFRED数据集上训练的视觉、语言和前一个动作输入的归因。归因分析可以用来排序和分组失败场景，调查模型和数据集偏见，并对多模态EAI策略进行robustness和用户信任的检验。我们提出了MAEA框架，用于计算任何可导策略的全球归因。此外，我们还示出了归因如何帮助分析EAI策略的低级行为。
</details></li>
</ul>
<hr>
<h2 id="Relationship-between-Batch-Size-and-Number-of-Steps-Needed-for-Nonconvex-Optimization-of-Stochastic-Gradient-Descent-using-Armijo-Line-Search"><a href="#Relationship-between-Batch-Size-and-Number-of-Steps-Needed-for-Nonconvex-Optimization-of-Stochastic-Gradient-Descent-using-Armijo-Line-Search" class="headerlink" title="Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search"></a>Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13831">http://arxiv.org/abs/2307.13831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuki Tsukada, Hideaki Iiduka</li>
<li>for: 本研究探讨了使用Stochastic gradient descent（SGD）训练深度学习模型时，学习率是如何选择的。</li>
<li>methods: 本研究使用了Armijo线earch方法来选择学习率，并进行了非 convex 优化的收敛分析。</li>
<li>results: 研究发现，当批处理大小增加时，SGD 的训练步数逐渐减少，并且存在一个最优批处理大小，可以最小化 Stochastic first-order oracle（SFO）复杂度。同时， numerics 支持了这些理论结论。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic first-order oracle (SFO) complexity, which is the stochastic gradient computation cost, is a convex function of the batch size; that is, there exists a critical batch size that minimizes the SFO complexity. Finally, we provide numerical results that support our theoretical results. The numerical results indicate that the number of steps needed for training deep neural networks decreases as the batch size increases and that there exist the critical batch sizes that can be estimated from the theoretical results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Offline-Reinforcement-Learning-with-On-Policy-Q-Function-Regularization"><a href="#Offline-Reinforcement-Learning-with-On-Policy-Q-Function-Regularization" class="headerlink" title="Offline Reinforcement Learning with On-Policy Q-Function Regularization"></a>Offline Reinforcement Learning with On-Policy Q-Function Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13824">http://arxiv.org/abs/2307.13824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laixi Shi, Robert Dadashi, Yuejie Chi, Pablo Samuel Castro, Matthieu Geist</li>
<li>for: 本研究的目的是解决offline reinforcement learning中的扩展错误问题，即history dataset和期望策略之间的分布shift问题。</li>
<li>methods: 本研究使用Q函数regularization来解决扩展错误问题，而不是直接regularizing towards behavior policy。</li>
<li>results: 两个提议算法在D4RLbenchmark上表现出色，展示了强的性能。<details>
<summary>Abstract</summary>
The core challenge of offline reinforcement learning (RL) is dealing with the (potentially catastrophic) extrapolation error induced by the distribution shift between the history dataset and the desired policy. A large portion of prior work tackles this challenge by implicitly/explicitly regularizing the learning policy towards the behavior policy, which is hard to estimate reliably in practice. In this work, we propose to regularize towards the Q-function of the behavior policy instead of the behavior policy itself, under the premise that the Q-function can be estimated more reliably and easily by a SARSA-style estimate and handles the extrapolation error more straightforwardly. We propose two algorithms taking advantage of the estimated Q-function through regularizations, and demonstrate they exhibit strong performance on the D4RL benchmarks.
</details>
<details>
<summary>摘要</summary>
核心挑战是线上强化学习（RL）是处理由历史数据集和需要的策略之间的分布转移所引起的（可能Catastrophic）推理错误的。大多数先前的工作是通过直接/间接地规范学习策略向行为策略进行补做，这在实践中很难估算。在这种工作中，我们建议将规范向行为策略的Q函数进行补做，因为Q函数可以更加可靠地和容易地通过SARSA样式的估计来估计，并且更直观地处理推理错误。我们提出了两种利用估计Q函数的算法，并在D4RL标准各项目上展示它们的强大表现。
</details></li>
</ul>
<hr>
<h2 id="Fitting-Auditory-Filterbanks-with-Multiresolution-Neural-Networks"><a href="#Fitting-Auditory-Filterbanks-with-Multiresolution-Neural-Networks" class="headerlink" title="Fitting Auditory Filterbanks with Multiresolution Neural Networks"></a>Fitting Auditory Filterbanks with Multiresolution Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13821">http://arxiv.org/abs/2307.13821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lostanlen/lostanlen2023waspaa">https://github.com/lostanlen/lostanlen2023waspaa</a></li>
<li>paper_authors: Vincent Lostanlen, Daniel Haider, Han Han, Mathieu Lagrange, Peter Balazs, Martin Ehler</li>
<li>for: 用于音频识别和分类 tasks</li>
<li>methods: 使用 multiresolution neural network (MuReNN)，具有分解的卷积操作符，以及知识填充（knowledge distillation，KD）技术</li>
<li>results: 与 state-of-the-art 比较，MuReNN 在 hold-out 集合上的好处 fit 和 Heisenberg 时域本地化方面达到了最佳性能。<details>
<summary>Abstract</summary>
Waveform-based deep learning faces a dilemma between nonparametric and parametric approaches. On one hand, convolutional neural networks (convnets) may approximate any linear time-invariant system; yet, in practice, their frequency responses become more irregular as their receptive fields grow. On the other hand, a parametric model such as LEAF is guaranteed to yield Gabor filters, hence an optimal time-frequency localization; yet, this strong inductive bias comes at the detriment of representational capacity. In this paper, we aim to overcome this dilemma by introducing a neural audio model, named multiresolution neural network (MuReNN). The key idea behind MuReNN is to train separate convolutional operators over the octave subbands of a discrete wavelet transform (DWT). Since the scale of DWT atoms grows exponentially between octaves, the receptive fields of the subsequent learnable convolutions in MuReNN are dilated accordingly. For a given real-world dataset, we fit the magnitude response of MuReNN to that of a well-established auditory filterbank: Gammatone for speech, CQT for music, and third-octave for urban sounds, respectively. This is a form of knowledge distillation (KD), in which the filterbank ''teacher'' is engineered by domain knowledge while the neural network ''student'' is optimized from data. We compare MuReNN to the state of the art in terms of goodness of fit after KD on a hold-out set and in terms of Heisenberg time-frequency localization. Compared to convnets and Gabor convolutions, we find that MuReNN reaches state-of-the-art performance on all three optimization problems.
</details>
<details>
<summary>摘要</summary>
文本形式的深度学习面临着非 Parametric 和 Parametric approaches 之间的矛盾。一方面，卷积神经网络（convnets）可以近似任何线性时间不变系统；然而，在实践中，它们的频谱响应会随着它们的触发区域增大而变得更加不规则。另一方面，一个 Parametric 模型如 LEAF 可以确保生成 Gabor 滤波器，因此获得最佳时间频域准确性；然而，这样强大的推导缺陷会导致表达能力受限。在这篇论文中，我们想要超越这个矛盾，我们提出了一种神经音频模型，即多尺度神经网络（MuReNN）。MuReNN 的关键思想是在 Octave 子域上分别训练独立的卷积操作。由于 DWT 原子的尺度在 Octave 上呈指数增长，MuReNN 中的后续学习可以通过扩展权重来进行扩展。对于一个真实世界数据集，我们将 MuReNN 的质量响应与一个已知的听觉滤波器 banks：Gammatone  для语音、CQT  для音乐和第三 Octave  для城市声音，分别进行适应。这是一种知识储存（KD），在哪里听觉滤波器 ''教师'' 是通过领域知识设计的，而神经网络 ''学生'' 是通过数据优化的。我们将 MuReNN 与现状最佳的方法进行比较，包括在 KD 中的准确性评价和 Heisenberg 时间频域本地化评价。相比 ConvNets 和 Gabor 卷积，我们发现 MuReNN 在三个优化问题上都达到了状态机器的性能。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Based-Spectral-Embeddings-of-Random-Dot-Product-Graphs"><a href="#Gradient-Based-Spectral-Embeddings-of-Random-Dot-Product-Graphs" class="headerlink" title="Gradient-Based Spectral Embeddings of Random Dot Product Graphs"></a>Gradient-Based Spectral Embeddings of Random Dot Product Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13818">http://arxiv.org/abs/2307.13818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marfiori/efficient-ase">https://github.com/marfiori/efficient-ase</a></li>
<li>paper_authors: Marcelo Fiori, Bernardo Marenco, Federico Larroca, Paola Bermolen, Gonzalo Mateos</li>
<li>for: 这个论文的目的是提出一种基于非对映准则的图像学习框架，以解决图像 embeddings 问题。</li>
<li>methods: 论文使用了非对映准则优化方法，包括首页梯度下降法和梯度搅拌法，来解决图像 embeddings 问题。</li>
<li>results: 实验结果表明，该方法可以更好地解决图像 embeddings 问题，并且可以更好地处理流动图像和缺失边数据。<details>
<summary>Abstract</summary>
The Random Dot Product Graph (RDPG) is a generative model for relational data, where nodes are represented via latent vectors in low-dimensional Euclidean space. RDPGs crucially postulate that edge formation probabilities are given by the dot product of the corresponding latent positions. Accordingly, the embedding task of estimating these vectors from an observed graph is typically posed as a low-rank matrix factorization problem. The workhorse Adjacency Spectral Embedding (ASE) enjoys solid statistical properties, but it is formally solving a surrogate problem and can be computationally intensive. In this paper, we bring to bear recent advances in non-convex optimization and demonstrate their impact to RDPG inference. We advocate first-order gradient descent methods to better solve the embedding problem, and to organically accommodate broader network embedding applications of practical relevance. Notably, we argue that RDPG embeddings of directed graphs loose interpretability unless the factor matrices are constrained to have orthogonal columns. We thus develop a novel feasible optimization method in the resulting manifold. The effectiveness of the graph representation learning framework is demonstrated on reproducible experiments with both synthetic and real network data. Our open-source algorithm implementations are scalable, and unlike the ASE they are robust to missing edge data and can track slowly-varying latent positions from streaming graphs.
</details>
<details>
<summary>摘要</summary>
Random Dot Product Graph（RDPG）是一种生成模型，用于关系数据，其中节点被表示为低维欧几何空间中的latent vector。 RDPG假设边的形成概率为latent vector的点积。因此，从观察到的图像进行嵌入的任务通常是一个低维矩阵分解问题。ASE是工作马力，但它是一个代理问题，可能 computationally intensive。在这篇论文中，我们利用了最近的非极体优化技术，并证明它们对RDPG推理有益。我们建议使用首项梯度下降法来更好地解决嵌入问题，并能够自然地涵盖更广泛的网络嵌入应用。另外，我们 argue that RDPG嵌入导向图 Unless the factor matrices are constrained to have orthogonal columns，因此我们开发了一种新的可行优化方法。我们的图表示学术框架在 reproduceable experiments中得到了证明，并且我们的开源算法实现可扩展，不同于ASE，它们可以承受缺失边数据和从流式图进行逐步嵌入。
</details></li>
</ul>
<hr>
<h2 id="How-to-Scale-Your-EMA"><a href="#How-to-Scale-Your-EMA" class="headerlink" title="How to Scale Your EMA"></a>How to Scale Your EMA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13813">http://arxiv.org/abs/2307.13813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZulqarnainZilli/-9-Email-Marketing-Tips-For-Content-Marketers">https://github.com/ZulqarnainZilli/-9-Email-Marketing-Tips-For-Content-Marketers</a></li>
<li>paper_authors: Dan Busbridge, Jason Ramapuram, Pierre Ablin, Tatiana Likhomanenko, Eeshan Gunesh Dhekane, Xavier Suau, Russ Webb</li>
<li>for: 本文旨在探讨如何在批处理大小不同时保持训练动力的问题。</li>
<li>methods: 本文提出了一种优化策略，即在EMA模型的存在下，采用线性增量法来调整学习率，以实现在批处理大小不同时保持训练动力。</li>
<li>results: 本文通过多种 arquitectures、优化器和数据模式的实验，证明了该优化策略的有效性。 另外，本文还示出了EMA模型在目标模型优化中的作用，并在小批处理和大批处理下实现了SSL方法的训练。 特别是，在BYOL方法中，通过适当调整学习率，在批处理大小为24576时实现了6倍的wall-clock时间减少。<details>
<summary>Abstract</summary>
Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonstrate its validity across a range of architectures, optimizers, and data modalities. We also show the rule's validity where the model EMA contributes to the optimization of the target model, enabling us to train EMA-based pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we enable training of BYOL up to batch size 24,576 without sacrificing performance, optimally a 6$\times$ wall-clock time reduction.
</details>
<details>
<summary>摘要</summary>
Previous works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonstrate its validity across a range of architectures, optimizers, and data modalities. We also show the rule's validity where the model EMA contributes to the optimization of the target model, enabling us to train EMA-based pseudo-labeling and SSL methods at small and large batch sizes.For SSL, we enable training of BYOL up to batch size 24,576 without sacrificing performance, resulting in a 6 times wall-clock time reduction.
</details></li>
</ul>
<hr>
<h2 id="When-Multi-Task-Learning-Meets-Partial-Supervision-A-Computer-Vision-Review"><a href="#When-Multi-Task-Learning-Meets-Partial-Supervision-A-Computer-Vision-Review" class="headerlink" title="When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review"></a>When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14382">http://arxiv.org/abs/2307.14382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Fontana, Michael Spratling, Miaojing Shi</li>
<li>For: 本研究探讨了多任务学习（MTL）在不同半指导下的应用。* Methods: 本研究使用了多个参数共享技术来传递知识 между任务。* Results: 本研究介绍了多任务优化问题中的多种挑战，并提出了基于任务关系分组的方法来解决这些挑战。In English, this translates to:* For: This study explores the application of multi-task learning (MTL) under different partial supervision settings.* Methods: The study uses multiple parameter sharing techniques to transfer knowledge between tasks.* Results: The study introduces challenges arising from the multi-objective optimization scheme and proposes a method based on task relationships to address these challenges.<details>
<summary>Abstract</summary>
Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising from such a multi-objective optimisation scheme. Third, it introduces how task groupings can be achieved by analysing task relationships. Fourth, it focuses on how partially supervised methods applied to MTL can tackle the aforementioned challenges. Lastly, this review presents the available datasets, tools and benchmarking results of such methods.
</details>
<details>
<summary>摘要</summary>
First, the review examines how MTL traditionally uses parameter sharing techniques to transfer knowledge between tasks. Second, it discusses the challenges arising from the multi-objective optimization scheme. Third, it introduces task groupings based on task relationships. Fourth, it focuses on how partially supervised methods can be applied to MTL to tackle the challenges. Finally, the review presents available datasets, tools, and benchmarking results for such methods.
</details></li>
</ul>
<hr>
<h2 id="EdgeConvEns-Convolutional-Ensemble-Learning-for-Edge-Intelligence"><a href="#EdgeConvEns-Convolutional-Ensemble-Learning-for-Edge-Intelligence" class="headerlink" title="EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence"></a>EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14381">http://arxiv.org/abs/2307.14381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilkay Sikdokur, İnci M. Baytaş, Arda Yurdakul</li>
<li>for: 这项研究旨在提出一种基于 convolutional ensemble learning 的深入边缘智能方法，以提高边缘设备的训练效果和预测性能。</li>
<li>methods: 本研究使用了 Federation Learning 方法，并在边缘设备上实现了多个不同计算能力的 FPGA 设备上的独立训练。同时，通过将学习到的特征传输到中央服务器进行集成训练，以提高总预测性能。</li>
<li>results: 实验结果表明，EdgeConvEns 可以在不同训练场景下比 estado-of-the-art 方法具有更好的预测性能，同时减少了数据传输量和通信次数。<details>
<summary>Abstract</summary>
Deep edge intelligence aims to deploy deep learning models that demand computationally expensive training in the edge network with limited computational power. Moreover, many deep edge intelligence applications require handling distributed data that cannot be transferred to a central server due to privacy concerns. Decentralized learning methods, such as federated learning, offer solutions where models are learned collectively by exchanging learned weights. However, they often require complex models that edge devices may not handle and multiple rounds of network communication to achieve state-of-the-art performances. This study proposes a convolutional ensemble learning approach, coined EdgeConvEns, that facilitates training heterogeneous weak models on edge and learning to ensemble them where data on edge are heterogeneously distributed. Edge models are implemented and trained independently on Field-Programmable Gate Array (FPGA) devices with various computational capacities. Learned data representations are transferred to a central server where the ensemble model is trained with the learned features received from the edge devices to boost the overall prediction performance. Extensive experiments demonstrate that the EdgeConvEns can outperform the state-of-the-art performance with fewer communications and less data in various training scenarios.
</details>
<details>
<summary>摘要</summary>
深入智能目标是在边缘网络中部署需要计算负担强大的深度学习模型，但边缘设备的计算能力有限。此外，许多深入智能应用需要处理分布式数据，这些数据无法被传输到中央服务器 due to 隐私问题。分布式学习方法，如联邦学习，可以解决这些问题，但它们通常需要复杂的模型，边缘设备可能无法处理，并且需要多轮的网络通信来达到状态艺术性能。本研究提出了一种 convolutional ensemble learning 方法，名为 EdgeConvEns，它可以在边缘设备上训练多种不同的弱模型，并将这些模型 ensemble 在边缘设备上。边缘设备上实现和训练独立的 Field-Programmable Gate Array (FPGA) 设备，并将学习到的数据表示传输到中央服务器，以在中央服务器上训练 ensemble 模型，以提高总预测性能。经验示出，EdgeConvEns 可以在不同的训练场景下超越当前的状态艺术性能，并且需要更少的通信和数据量。
</details></li>
</ul>
<hr>
<h2 id="Source-Condition-Double-Robust-Inference-on-Functionals-of-Inverse-Problems"><a href="#Source-Condition-Double-Robust-Inference-on-Functionals-of-Inverse-Problems" class="headerlink" title="Source Condition Double Robust Inference on Functionals of Inverse Problems"></a>Source Condition Double Robust Inference on Functionals of Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13793">http://arxiv.org/abs/2307.13793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Bennett, Nathan Kallus, Xiaojie Mao, Whitney Newey, Vasilis Syrgkanis, Masatoshi Uehara</li>
<li>for: 这个论文主要是为了研究 linear inverse problems 中参数估计的方法。</li>
<li>methods: 论文使用了 doubly robust representation 方法，该方法基于解决 primal 和 dual 两个线性 inverse problems 的解。</li>
<li>results: 论文提供了一种 asymptotically normal 的 parameter estimation method, 不需要知道 primal 或 dual 问题是哪个更加正确的问题。这个结果基于一种新的 iterated Tikhonov regularized adversarial estimators 方法，该方法可以应用于 general hypothesis spaces 上的 linear inverse problems。<details>
<summary>Abstract</summary>
We consider estimation of parameters defined as linear functionals of solutions to linear inverse problems. Any such parameter admits a doubly robust representation that depends on the solution to a dual linear inverse problem, where the dual solution can be thought as a generalization of the inverse propensity function. We provide the first source condition double robust inference method that ensures asymptotic normality around the parameter of interest as long as either the primal or the dual inverse problem is sufficiently well-posed, without knowledge of which inverse problem is the more well-posed one. Our result is enabled by novel guarantees for iterated Tikhonov regularized adversarial estimators for linear inverse problems, over general hypothesis spaces, which are developments of independent interest.
</details>
<details>
<summary>摘要</summary>
我们考虑参数估计为线性函数解析方法的线性逆问题。任何参数都可以得到双重稳定表示，这种表示取决于解析方法的对偶问题的解，可以看作总化倒数逆函数的扩展。我们提供了第一个源condition double robust推断方法，可以在参数关心范围内保证参数归一化正常性，只要 primal 或 dual 逆问题够正确，不需要知道哪个逆问题更加正确。我们的结果基于新的 iterated Tikhonov regularized adversarial estimator 的 guarantees，这些保证在一般假设空间上适用，是独立的研究成果。
</details></li>
</ul>
<hr>
<h2 id="Histogram-Layer-Time-Delay-Neural-Networks-for-Passive-Sonar-Classification"><a href="#Histogram-Layer-Time-Delay-Neural-Networks-for-Passive-Sonar-Classification" class="headerlink" title="Histogram Layer Time Delay Neural Networks for Passive Sonar Classification"></a>Histogram Layer Time Delay Neural Networks for Passive Sonar Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13788">http://arxiv.org/abs/2307.13788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/peeples-lab/hltdnn">https://github.com/peeples-lab/hltdnn</a></li>
<li>paper_authors: Jarin Ritu, Ethan Barnes, Riley Martell, Alexandra Van Dine, Joshua Peeples</li>
<li>for: 本研究旨在提高海上陌生探测中的潜水噪音目标检测，因为噪音波的传播复杂，目标识别具有挑战性。</li>
<li>methods: 本研究提出了一种新的方法，它将时间延迟神经网络和 histogram 层结合使用，以利用噪音波观测记录中的统计上下文来提高特征学习和海上陌生探测中的噪音目标识别。</li>
<li>results: 对比基eline模型，本研究的方法显示出了更高的识别精度，这表明了在噪音目标识别中吸收统计上下文的优势。代码可以公开获得。<details>
<summary>Abstract</summary>
Underwater acoustic target detection in remote marine sensing operations is challenging due to complex sound wave propagation. Despite the availability of reliable sonar systems, target recognition remains a difficult problem. Various methods address improved target recognition. However, most struggle to disentangle the high-dimensional, non-linear patterns in the observed target recordings. In this work, a novel method combines a time delay neural network and histogram layer to incorporate statistical contexts for improved feature learning and underwater acoustic target classification. The proposed method outperforms the baseline model, demonstrating the utility in incorporating statistical contexts for passive sonar target recognition. The code for this work is publicly available.
</details>
<details>
<summary>摘要</summary>
水下声学目标检测在远程海洋探测操作中存在很大的挑战，主要是声波传播复杂。尽管有可靠的声纳系统，但目标识别仍然是一个困难的问题。各种方法尝试了改进目标识别，但大多数都无法分离高维、非线性的目标记录特征。在这种情况下，我们提出了一种新的方法，它将时间延迟神经网络和分布图层结合在一起，以利用统计上下文来改进声学目标识别。我们的方法比基eline模型更高效，这 demonstartes了在声学目标识别中提供统计上下文的重要性。代码已经公开 availible。
</details></li>
</ul>
<hr>
<h2 id="The-GANfather-Controllable-generation-of-malicious-activity-to-improve-defence-systems"><a href="#The-GANfather-Controllable-generation-of-malicious-activity-to-improve-defence-systems" class="headerlink" title="The GANfather: Controllable generation of malicious activity to improve defence systems"></a>The GANfather: Controllable generation of malicious activity to improve defence systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13787">http://arxiv.org/abs/2307.13787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Ribeiro Pereira, Jacopo Bono, João Tiago Ascensão, David Aparício, Pedro Ribeiro, Pedro Bizarro</li>
<li>for: 帮助防御系统检测恶意活动，不需要标注数据</li>
<li>methods: 提出了一种基于生成器网络的方法，通过引入额外目标函数来奖励生成恶意样本</li>
<li>results: 在两个实际应用中（货币洗钱和推荐系统），我们成功地使用这种方法生成了恶意样本，并训练了一个新的防御系统来捕捉这些样本。<details>
<summary>Abstract</summary>
Machine learning methods to aid defence systems in detecting malicious activity typically rely on labelled data. In some domains, such labelled data is unavailable or incomplete. In practice this can lead to low detection rates and high false positive rates, which characterise for example anti-money laundering systems. In fact, it is estimated that 1.7--4 trillion euros are laundered annually and go undetected. We propose The GANfather, a method to generate samples with properties of malicious activity, without label requirements. We propose to reward the generation of malicious samples by introducing an extra objective to the typical Generative Adversarial Networks (GANs) loss. Ultimately, our goal is to enhance the detection of illicit activity using the discriminator network as a novel and robust defence system. Optionally, we may encourage the generator to bypass pre-existing detection systems. This setup then reveals defensive weaknesses for the discriminator to correct. We evaluate our method in two real-world use cases, money laundering and recommendation systems. In the former, our method moves cumulative amounts close to 350 thousand dollars through a network of accounts without being detected by an existing system. In the latter, we recommend the target item to a broad user base with as few as 30 synthetic attackers. In both cases, we train a new defence system to capture the synthetic attacks.
</details>
<details>
<summary>摘要</summary>
机器学习方法通常需要标注数据来帮助防御系统检测恶意活动。在某些领域，这些标注数据可能不可 obtenía或 incomplete。这可能导致检测率低下 false positive 率高，这种情况例如反洗钱系统。实际上，每年可能有 1.7--4 亿欧元被骗财，并未被发现。我们提出了 The GANfather，一种方法，可以生成具有恶意活动特性的样本，不需要标注。我们提出了在 Typical Generative Adversarial Networks (GANs) 损失函数中引入一个额外的目标，以奖励生成恶意样本。最终，我们的目标是通过使用探测器网络作为一种新的和可靠的防御系统，提高恶意活动的检测。选择地，我们可以让生成器 circumvent 现有的检测系统。这种设置然后 revelas defensive weaknesses for the discriminator to correct。我们在两个实际应用中评估了我们的方法：反洗钱和推荐系统。在前一个应用中，我们通过一个网络的账户来传递累计金额达 350 万美元，而不被现有系统检测到。在后一个应用中，我们通过 Synthetic 攻击者来推荐目标项目，并且只需要 30 个 synthetic 攻击者。在两个案例中，我们训练了一个新的防御系统，以捕捉 Synthetic 攻击。
</details></li>
</ul>
<hr>
<h2 id="Robust-Assignment-of-Labels-for-Active-Learning-with-Sparse-and-Noisy-Annotations"><a href="#Robust-Assignment-of-Labels-for-Active-Learning-with-Sparse-and-Noisy-Annotations" class="headerlink" title="Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations"></a>Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14380">http://arxiv.org/abs/2307.14380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Kałuża, Andrzej Janusz, Dominik Ślęzak</li>
<li>for: 解决激活学习中数据标注错误的问题</li>
<li>methods: 提出了两种新的注释统一算法，利用无标示部分的样本空间</li>
<li>results: 在四个公共数据集上进行了实验，表明提案的方法在估计注释者可靠性和实际标签分配方面具有robustness和superiority，并且比州对数据集中的简单多数投票更为有效。<details>
<summary>Abstract</summary>
Supervised classification algorithms are used to solve a growing number of real-life problems around the globe. Their performance is strictly connected with the quality of labels used in training. Unfortunately, acquiring good-quality annotations for many tasks is infeasible or too expensive to be done in practice. To tackle this challenge, active learning algorithms are commonly employed to select only the most relevant data for labeling. However, this is possible only when the quality and quantity of labels acquired from experts are sufficient. Unfortunately, in many applications, a trade-off between annotating individual samples by multiple annotators to increase label quality vs. annotating new samples to increase the total number of labeled instances is necessary. In this paper, we address the issue of faulty data annotations in the context of active learning. In particular, we propose two novel annotation unification algorithms that utilize unlabeled parts of the sample space. The proposed methods require little to no intersection between samples annotated by different experts. Our experiments on four public datasets indicate the robustness and superiority of the proposed methods in both, the estimation of the annotator's reliability, and the assignment of actual labels, against the state-of-the-art algorithms and the simple majority voting.
</details>
<details>
<summary>摘要</summary>
超visisted分类算法在全球各地的实际问题中得到应用。它们的性能与训练中使用的标签质量有着紧密的关系。然而，获取高质量标签是在实践中不可能或太昂贵了。为解决这个挑战，活动学算法通常被使用来选择仅需要标注的数据。然而，这只有当获取专家标注的标签质量和量足够时才能够实现。在许多应用程序中，需要考虑 annotating individual samples by multiple annotators 来提高标签质量 vs. annotating new samples 来增加标注的总数。在这篇论文中，我们对活动学中的假数据标注进行了研究。我们提出了两种新的标注统一算法，它们可以利用样本空间中的无标注部分。我们的方法需要标注者之间的交叉少到无。我们在四个公共数据集上进行了实验，结果表明我们的方法在计算标注者的可靠性和实际标注中具有更高的稳定性和优势，相比于当前的算法和简单多数投票。
</details></li>
</ul>
<hr>
<h2 id="Accuracy-Amplification-in-Differentially-Private-Logistic-Regression-A-Pre-Training-Approach"><a href="#Accuracy-Amplification-in-Differentially-Private-Logistic-Regression-A-Pre-Training-Approach" class="headerlink" title="Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach"></a>Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13771">http://arxiv.org/abs/2307.13771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Hoseinpour, Milad Hoseinpour, Ali Aghagolzadeh</li>
<li>for: 保护隐私的机器学习模型训练数据。</li>
<li>methods: 使用预训练模块和 differential privacy 的 logistic regression 模型。</li>
<li>results: 通过预训练模块，可以提高 differential privacy 下的机器学习模型的准确率。In more detail, the paper aims to improve the accuracy of a differentially private logistic regression model by using a pre-training module. The authors first pre-train the model on a public dataset without privacy concerns, and then fine-tune the model using the private dataset with differential privacy constraints. The results show that adding the pre-training module significantly improves the accuracy of the differentially private logistic regression model.<details>
<summary>Abstract</summary>
Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can violate the privacy of individuals. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets in ML models. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP-ML model, specifically a logistic regression model, via a pre-training module. In more detail, we initially pre-train our model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our model via the DP logistic regression with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP logistic regression.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）模型可以记忆训练数据集。因此，在训练ML模型时，可能会违反个人隐私。不同隐私（DP）是一种严格的隐私概念，用于保护训练数据集的隐私。然而，在DP框架下训练ML模型通常会降低模型的准确率。这篇论文目的是提高DP-ML模型的准确率，特别是使用Logistic Regression模型。在更多的细节中，我们首先在没有隐私问题的公共训练数据集上进行预训练。然后，我们使用DP Logistic Regression模型进行细化。在实验结果中，我们发现，添加预训练模块可以显著提高DP Logistic Regression模型的准确率。
</details></li>
</ul>
<hr>
<h2 id="ClusterSeq-Enhancing-Sequential-Recommender-Systems-with-Clustering-based-Meta-Learning"><a href="#ClusterSeq-Enhancing-Sequential-Recommender-Systems-with-Clustering-based-Meta-Learning" class="headerlink" title="ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning"></a>ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13766">http://arxiv.org/abs/2307.13766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammmadmahdi Maheri, Reza Abdollahzadeh, Bardia Mohammadi, Mina Rafiei, Jafar Habibi, Hamid R. Rabiee</li>
<li>for: 解决用户冷启始问题，提高续传推荐系统的效果。</li>
<li>methods:  combining meta-learning with user and item-side information，并使用动态信息在用户序列中增强物品预测精度。</li>
<li>results: 比如 existing meta-learning methods，我们的提案方法实现了16-39%的提升在 Mean Reciprocal Rank (MRR) 中。<details>
<summary>Abstract</summary>
In practical scenarios, the effectiveness of sequential recommendation systems is hindered by the user cold-start problem, which arises due to limited interactions for accurately determining user preferences. Previous studies have attempted to address this issue by combining meta-learning with user and item-side information. However, these approaches face inherent challenges in modeling user preference dynamics, particularly for "minor users" who exhibit distinct preferences compared to more common or "major users." To overcome these limitations, we present a novel approach called ClusterSeq, a Meta-Learning Clustering-Based Sequential Recommender System. ClusterSeq leverages dynamic information in the user sequence to enhance item prediction accuracy, even in the absence of side information. This model preserves the preferences of minor users without being overshadowed by major users, and it capitalizes on the collective knowledge of users within the same cluster. Extensive experiments conducted on various benchmark datasets validate the effectiveness of ClusterSeq. Empirical results consistently demonstrate that ClusterSeq outperforms several state-of-the-art meta-learning recommenders. Notably, compared to existing meta-learning methods, our proposed approach achieves a substantial improvement of 16-39% in Mean Reciprocal Rank (MRR).
</details>
<details>
<summary>摘要</summary>
在实际应用场景中，顺序推荐系统的效果受用户冷启问题的限制，这种问题 arise due to 用户与ITEM之间的互动有限，难以准确地确定用户的偏好。先前的研究尝试通过meta-学习和用户项信息的结合来解决这个问题，但这些方法面临用户偏好动态模型化的挑战，特别是对"小用户"而言，他们的偏好与"大用户"不同。为了超越这些限制，我们提出了一种新的方法 called ClusterSeq，这是一种基于 clustering 的 Meta-Learning Sequential Recommender System。ClusterSeq 利用用户序列中的动态信息来提高项预测精度，即使没有副信息。这个模型保留了小用户的偏好，不会被大用户所掩盖，同时利用用户集中的共同知识来提高推荐的准确性。在多个标准 benchmark 数据集上进行了广泛的实验，证明 ClusterSeq 的效果。实验结果表明，ClusterSeq 在 MRR 方面与state-of-the-art meta-learning recommenders 相比，具有16-39%的显著提高。
</details></li>
</ul>
<hr>
<h2 id="Implicitly-Normalized-Explicitly-Regularized-Density-Estimation"><a href="#Implicitly-Normalized-Explicitly-Regularized-Density-Estimation" class="headerlink" title="Implicitly Normalized Explicitly Regularized Density Estimation"></a>Implicitly Normalized Explicitly Regularized Density Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13763">http://arxiv.org/abs/2307.13763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Kozdoba, Binyamin Perets, Shie Mannor</li>
<li>for: 非 Parametric density estimation 方法</li>
<li>methods: 使用 Sobolev  нор调整 density</li>
<li>results: 比 Kernel Density Estimation 方法更加不偏，可以Clearly interpret 模型偏差Here’s a more detailed explanation of each point:1. for: The paper proposes a new approach to non-parametric density estimation, which is based on regularizing a Sobolev norm of the density. This approach is different from traditional Kernel Density Estimation (KDE) methods, and it aims to provide a more interpretable and less biased estimation of the density.2. methods: The proposed method uses a regularization term that is based on the Sobolev norm of the density, which is a measure of the smoothness of the density. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, the paper shows that with an appropriate initialization and using natural gradients, one can obtain well-performing solutions.3. results: The paper evaluates the proposed method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and finds that it ranks second best among more than 15 algorithms. The method provides unnormalized densities, which prevents the use of log-likelihood for cross validation. However, the paper shows that one can instead adapt Fisher Divergence based Score Matching methods for this task.<details>
<summary>Abstract</summary>
We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的非Parametric数量估计方法，基于调整 Sobolev norm 的数量估计。这种方法与 Kernel Density Estimation 不同，可以将模型的偏见显示出来并实现可解释性。虽然没有关注点的关注函数，但我们显示可以使用抽样来近似它。估计问题是非对称的，标准的梯度法不太好。但我们显示，对于适当的初始化和使用自然梯度，可以得到良好的解。最后，这种方法提供的数量估计无法取得正规化的数量估计，因此无法使用log-likelihood进行cross validate。但我们显示可以使用Fisher Divergence based Score Matching方法来处理这个问题。我们在Anomaly Detection benchmark suite ADBench 上进行了评估，发现其排名第二，与其他15种方法相比。
</details></li>
</ul>
<hr>
<h2 id="UPREVE-An-End-to-End-Causal-Discovery-Benchmarking-System"><a href="#UPREVE-An-End-to-End-Causal-Discovery-Benchmarking-System" class="headerlink" title="UPREVE: An End-to-End Causal Discovery Benchmarking System"></a>UPREVE: An End-to-End Causal Discovery Benchmarking System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13757">http://arxiv.org/abs/2307.13757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suraj Jyothi Unni, Paras Sheth, Kaize Ding, Huan Liu, K. Selcuk Candan</li>
<li>for: 本研究旨在提供一个轻松使用、可自定义的web基台用于探索复杂社会行为系统中的 causal 关系。</li>
<li>methods: 本研究使用了多种算法同时运行、可视化 causal 关系以及评估学习的 causal 图的精度。</li>
<li>results: 本研究提出了一个轻松使用、可自定义的 web 基台 UPREVE，可以帮助研究者和实践者更好地探索和理解社会行为系统中的 causal 关系，以便更好地做出决策。<details>
<summary>Abstract</summary>
Discovering causal relationships in complex socio-behavioral systems is challenging but essential for informed decision-making. We present Upload, PREprocess, Visualize, and Evaluate (UPREVE), a user-friendly web-based graphical user interface (GUI) designed to simplify the process of causal discovery. UPREVE allows users to run multiple algorithms simultaneously, visualize causal relationships, and evaluate the accuracy of learned causal graphs. With its accessible interface and customizable features, UPREVE empowers researchers and practitioners in social computing and behavioral-cultural modeling (among others) to explore and understand causal relationships effectively. Our proposed solution aims to make causal discovery more accessible and user-friendly, enabling users to gain valuable insights for better decision-making.
</details>
<details>
<summary>摘要</summary>
发现复杂社会行为系统中的 causal 关系是具有挑战性和重要性的，但是这对于 informed decision-making 是必需的。我们提出了 Upload、PREprocess、Visualize 和 Evaluate（UPREVE），一个用户友好的网页式 графического用户界面（GUI），用于简化 causal discovery 的过程。UPREVE 允许用户同时运行多个算法，可视化 causal 关系，并评估学习的 causal 图的准确性。它的访问ible 界面和可定制功能，使得研究者和实践者在社会计算和文化模型中能够有效地探索和理解 causal 关系，从而获得有价值的发现，以便更好的决策。我们的提议的解决方案计划使 causal discovery 更加访问ible和用户友好，以便用户可以更好地理解 causal 关系，并做出更好的决策。
</details></li>
</ul>
<hr>
<h2 id="Solution-Path-of-Time-varying-Markov-Random-Fields-with-Discrete-Regularization"><a href="#Solution-Path-of-Time-varying-Markov-Random-Fields-with-Discrete-Regularization" class="headerlink" title="Solution Path of Time-varying Markov Random Fields with Discrete Regularization"></a>Solution Path of Time-varying Markov Random Fields with Discrete Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13750">http://arxiv.org/abs/2307.13750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salar Fattahi, Andres Gomez</li>
<li>for: 本研究实际问题是如何推断时间变化的Markov随机场景（MRF），特别是具有不同的纯粹和时间规律化的参数。</li>
<li>methods: 本文提出了一种新的参数调整问题，使用紧缩的纯粹规律化来实现参数的简洁。这个问题可以 Parametrically 解决，并且可以在实际的设置下 scale 到高维度。</li>
<li>results: 本文展示了一个全新的解法，可以实现时间变化MRF的全解析解，并且可以在实际的设置下进行交互验证。这个解法可以在 $\mathcal{O}(pT^3) $ 时间内解决，其中 $T$ 是时间步骤数量，$p$ 是时间变化MRF中参数的数量。这个解法可以高效地进行交互验证，并且可以在实际的设置下获得提供了详细的解析解。<details>
<summary>Abstract</summary>
We study the problem of inferring sparse time-varying Markov random fields (MRFs) with different discrete and temporal regularizations on the parameters. Due to the intractability of discrete regularization, most approaches for solving this problem rely on the so-called maximum-likelihood estimation (MLE) with relaxed regularization, which neither results in ideal statistical properties nor scale to the dimensions encountered in realistic settings. In this paper, we address these challenges by departing from the MLE paradigm and resorting to a new class of constrained optimization problems with exact, discrete regularization to promote sparsity in the estimated parameters. Despite the nonconvex and discrete nature of our formulation, we show that it can be solved efficiently and parametrically for all sparsity levels. More specifically, we show that the entire solution path of the time-varying MRF for all sparsity levels can be obtained in $\mathcal{O}(pT^3)$, where $T$ is the number of time steps and $p$ is the number of unknown parameters at any given time. The efficient and parametric characterization of the solution path renders our approach highly suitable for cross-validation, where parameter estimation is required for varying regularization values. Despite its simplicity and efficiency, we show that our proposed approach achieves provably small estimation error for different classes of time-varying MRFs, namely Gaussian and discrete MRFs, with as few as one sample per time. Utilizing our algorithm, we can recover the complete solution path for instances of time-varying MRFs featuring over 30 million variables in less than 12 minutes on a standard laptop computer. Our code is available at \url{https://sites.google.com/usc.edu/gomez/data}.
</details>
<details>
<summary>摘要</summary>
我们研究了推理缺省时间变化Markov随机场（MRF）的问题，具有不同的离散和时间 regularization 参数。由于离散 regularization 的不可解性，大多数解决这个问题的方法都 rely on 最大 likelihood estimation（MLE）with relaxed regularization，这并不会导致理想的统计性质，也无法扩展到实际中的维度。在这篇论文中，我们解决这些挑战，我们不再采用 MLE 模型，而是基于一种新的受限制优化问题，以便在优化参数时Promote 缺省性。尽管我们的形式ulation 是非对称和离散的，但我们证明可以高效地解决它，并且可以 Parametrically 解决所有缺省级别。更specifically，我们证明可以在 $\mathcal{O}(pT^3)$ 时间内解决整个时间变化 MRF 的全解路径，其中 $T$ 是时间步骤数量，$p$ 是时间步骤中未知参数的数量。我们的方法高效、可 parametric，因此非常适合 Cross-validation，其中需要不同的 regularization 值进行参数估计。尽管它简单、高效，但我们证明我们的方法可以在不同类型的时间变化 MRF 上取得可观测小的估计误差，只需要一个时间步骤中的一个样本。使用我们的算法，我们可以在 less than 12 分钟内将 over 30 万个变量的解决路径全部回归到标准笔记计算机上。我们的代码可以在 \url{https://sites.google.com/usc.edu/gomez/data} 上找到。
</details></li>
</ul>
<hr>
<h2 id="mL-BFGS-A-Momentum-based-L-BFGS-for-Distributed-Large-Scale-Neural-Network-Optimization"><a href="#mL-BFGS-A-Momentum-based-L-BFGS-for-Distributed-Large-Scale-Neural-Network-Optimization" class="headerlink" title="mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization"></a>mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13744">http://arxiv.org/abs/2307.13744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Niu, Zalan Fabian, Sunwoo Lee, Mahdi Soltanolkotabi, Salman Avestimehr</li>
<li>for: 大规模神经网络优化中使用 quasi-Newton 方法遇到了 significiant 挑战，主要是在 Hessian 相关计算中添加了额外的计算成本，以及在随机训练中存在稳定性问题。</li>
<li>methods: 我们提出了一种轻量级的摩托矢量 L-BFGS 算法（mL-BFGS），该算法在大规模分布式深度神经网络优化中采用了几乎免费的摩托矢量计划，从而减少了随机噪声在 Hessian 中的影响，使得梯度下降过程更加稳定。</li>
<li>results: 我们通过使用 mL-BFGS 对一些标准神经网络模型进行训练，并与基elines（SGD、Adam 等）进行比较，发现 mL-BFGS 可以 achieve both noticeable iteration-wise 和 wall-clock 速度减少。<details>
<summary>Abstract</summary>
Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training. A well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training. So far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time. In this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization. For model training at a large scale, mL-BFGS approximates a block-wise Hessian, thus enabling distributing compute and memory costs across all computing nodes. We provide a supporting convergence analysis for mL-BFGS in stochastic settings. To investigate mL-BFGS potential in large-scale DNN training, we train benchmark neural models using mL-BFGS and compare performance with baselines (SGD, Adam, and other quasi-Newton methods). Results show that mL-BFGS achieves both noticeable iteration-wise and wall-clock speedup.
</details>
<details>
<summary>摘要</summary>
尽管凯撒-牛顿方法在训练大规模神经网络方面仍然面临着重要的挑战，主要是在资源成本上。这些方法在训练中需要额外的计算成本，以及约束变量的稳定性问题。L-BFGS方法，一种广泛使用的凯撒-牛顿方法，在随机训练中存在很大的问题，即迭代不稳定。目前，尝试将L-BFGS方法应用于大规模随机训练中，增加了非常大的额外开销，这将导致训练时间的增加。在这篇论文中，我们提出了一种轻量级的摩托率基于L-BFGS算法，称为mL-BFGS。该算法在大规模随机训练中使用历史参数和梯度变化来高效地估算梯度，从而稳定训练过程。此外，mL-BFGS还可以将块级别的梯度估算分布到所有计算节点上，从而实现分布式计算和存储成本的分摊。我们还提供了支持mL-BFGS在随机设定下的收敛分析。为了评估mL-BFGS在大规模神经网络训练中的潜力，我们使用mL-BFGS训练了一些标准神经网络模型，并与基线方法（SGD、Adam等）进行比较。结果表明，mL-BFGS在训练过程中能够达到显著的迭代减速和wall-clock减速。
</details></li>
</ul>
<hr>
<h2 id="ARB-Advanced-Reasoning-Benchmark-for-Large-Language-Models"><a href="#ARB-Advanced-Reasoning-Benchmark-for-Large-Language-Models" class="headerlink" title="ARB: Advanced Reasoning Benchmark for Large Language Models"></a>ARB: Advanced Reasoning Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13692">http://arxiv.org/abs/2307.13692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay, Kshitij Gupta, Aran Komatsuzaki</li>
<li>for: 本研究旨在提供一个新的 benchmark，以测试大型语言模型（LLM）的推理能力和领域知识。</li>
<li>methods: 本研究使用了多个领域的高级推理问题组成的 ARB benchmark，以测试 LLM 的推理能力和领域知识。</li>
<li>results: 研究发现，现有的 GPT-4 和 Claude 模型在 ARB benchmark 上的得分尚未达到专家水平，特别是在更加具有挑战性的任务上。此外，研究还引入了一种基于 rubric 的评估方法，以提高自动和助动评估能力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct a human evaluation of the symbolic subset of ARB, finding promising agreement between annotators and GPT-4 rubric evaluation scores.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="High-Probability-Analysis-for-Non-Convex-Stochastic-Optimization-with-Clipping"><a href="#High-Probability-Analysis-for-Non-Convex-Stochastic-Optimization-with-Clipping" class="headerlink" title="High Probability Analysis for Non-Convex Stochastic Optimization with Clipping"></a>High Probability Analysis for Non-Convex Stochastic Optimization with Clipping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13680">http://arxiv.org/abs/2307.13680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaojie Li, Yong Liu</li>
<li>for: This paper studies the theoretical guarantees of stochastic optimization algorithms with gradient clipping in the non-convex setting.</li>
<li>methods: The paper uses high probability analysis to derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes.</li>
<li>results: The paper provides a relatively complete picture for the theoretical guarantee of stochastic optimization algorithms with clipping, and shows that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization.<details>
<summary>Abstract</summary>
Gradient clipping is a commonly used technique to stabilize the training process of neural networks. A growing body of studies has shown that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization as well. While gradient clipping is significant, its theoretical guarantees are scarce. Most theoretical guarantees only provide an in-expectation analysis and only focus on optimization performance. In this paper, we provide high probability analysis in the non-convex setting and derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes. With the gradient clipping, we study a heavy-tailed assumption that the gradients only have bounded $\alpha$-th moments for some $\alpha \in (1, 2]$, which is much weaker than the standard bounded second-moment assumption. Overall, our study provides a relatively complete picture for the theoretical guarantee of stochastic optimization algorithms with clipping.
</details>
<details>
<summary>摘要</summary>
Gradient clipping 是一种常用的技术来稳定神经网络的训练过程。一个不断增长的研究表明，gradient clipping 是一种有前途的技术，用于处理随机优化中的重 tailed 行为。虽然 gradient clipping 具有重要的意义，但其理论保证却很少。大多数理论保证都仅提供了预期分析，并仅关注优化性能。在这篇论文中，我们提供了高概率分析，在非对称设定下，并同时 deriv 出优化 bound 和泛化 bound  для各种束缚随机优化算法，包括梯度下降和其 variants of momentum 和 adaptive stepsizes。通过束缚，我们研究了一种具有bounded $\alpha$-th moment的梯度假设，其中 $\alpha \in (1, 2]$，这是标准二次均值假设的一个弱化版本。总的来说，我们的研究提供了一个相对完整的理论保证的图景，用于随机优化算法的 clipping。
</details></li>
</ul>
<hr>
<h2 id="RED-CoMETS-An-ensemble-classifier-for-symbolically-represented-multivariate-time-series"><a href="#RED-CoMETS-An-ensemble-classifier-for-symbolically-represented-multivariate-time-series" class="headerlink" title="RED CoMETS: An ensemble classifier for symbolically represented multivariate time series"></a>RED CoMETS: An ensemble classifier for symbolically represented multivariate time series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13679">http://arxiv.org/abs/2307.13679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zy18811/red-comets">https://github.com/zy18811/red-comets</a></li>
<li>paper_authors: Luca A. Bennett, Zahraa S. Abdallah</li>
<li>for: 这篇论文主要应用在多重时间序列分类中，尤其是在金融、医疗、工程等实际应用中。</li>
<li>methods: 本论文提出了一个新的协 ensemble分类器，名为RED CoMETS（随机增强的多重时间序列协 ensemble分类器），它是基于Co-eye（一个特别设计来进行单一时间序列分类的协 ensemble分类器）的扩展，能够处理多重时间序列资料。</li>
<li>results: RED CoMETS在UCAR档案中的评估数据上展现出了竞争性的精度，与现有的多重设定中的州际技术相比，其中最高的报告精度为’HandMovementDirection’档案。此外，提案的方法可以与Co-eye相比，大大降低 computation time，使其成为多重时间序列分类中效率和可靠的选择。<details>
<summary>Abstract</summary>
Multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more. The complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths. This paper introduces a novel ensemble classifier called RED CoMETS (Random Enhanced Co-eye for Multivariate Time Series), which addresses these challenges. RED CoMETS builds upon the success of Co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data. The performance of RED CoMETS is evaluated on benchmark datasets from the UCR archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings. Notably, it achieves the highest reported accuracy in the literature for the 'HandMovementDirection' dataset. Moreover, the proposed method significantly reduces computation time compared to Co-eye, making it an efficient and effective choice for multivariate time series classification.
</details>
<details>
<summary>摘要</summary>
多变量时间序列分类是一个快速发展的研究领域，有实际应用于金融、医疗、工程等领域。multivariate时间序列数据的复杂性来自其高维度、时间相关性和不同长度。本文介绍一种新的ensemble分类器called RED CoMETS（随机增强Co-eye for Multivariate Time Series），这种方法可以解决这些挑战。RED CoMETS是基于Co-eye，一种专门为symbolically represented univariate时间序列设计的ensemble分类器，扩展其能力以处理多变量数据。本文的实验结果表明，RED CoMETS在UCAR数据库中的benchmark数据集上表现竞争性高，与当前最佳的多变量技术相比。特别是，它在'HandMovementDirection'数据集上达到了 literaturereported最高的准确率。此外，提议的方法可以减少Co-eye的计算时间，使其成为efficient和effective的多变量时间序列分类选择。
</details></li>
</ul>
<hr>
<h2 id="FedDRL-A-Trustworthy-Federated-Learning-Model-Fusion-Method-Based-on-Staged-Reinforcement-Learning"><a href="#FedDRL-A-Trustworthy-Federated-Learning-Model-Fusion-Method-Based-on-Staged-Reinforcement-Learning" class="headerlink" title="FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning"></a>FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13716">http://arxiv.org/abs/2307.13716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiming Chen, Cihao Dong, Sibo Qiao, Ziling Huang, Kai Wang, Yuming Nie, Zhaoxiang Hou, Cheewei Tan</li>
<li>for: This paper aims to address the issues of model heterogeneity and malicious behavior in traditional federated learning by proposing a reinforcement learning-based model fusion approach called FedDRL.</li>
<li>methods: The FedDRL algorithm uses a two-stage approach to first filter out malicious models and select trusted client models, and then adaptively adjust the weights of the trusted client models to achieve optimal model fusion.</li>
<li>results: The experimental results show that the FedDRL algorithm has higher reliability than other algorithms while maintaining accuracy in five different model fusion scenarios.Here’s the Chinese version of the three key points:</li>
<li>for: 这篇论文目标是解决传统联合学习中的模型不同和恶意行为问题，提出一种基于强化学习的模型融合方法called FedDRL。</li>
<li>methods: FedDRL算法使用两stage方法，先过滤恶意模型，然后自适应调整被信任客户端模型的权重以实现优化的模型融合。</li>
<li>results: 实验结果表明，FedDRL算法在五种不同的模型融合场景中具有更高的可靠性，同时保持准确性。<details>
<summary>Abstract</summary>
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and aggregates the optimal global model. We also define five model fusion scenarios and compare our method with two baseline algorithms in those scenarios. The experimental results show that our algorithm has higher reliability than other algorithms while maintaining accuracy.
</details>
<details>
<summary>摘要</summary>
传统的联合学习使用客户端模型的数量来计算每个客户端模型的权重值，并使用这些固定权重值来融合全球模型。然而，在实际场景中，每个客户端的设备和数据多样性导致每个客户端模型的质量差异很大。因此，传统的联合学习算法不能准确地评估每个客户端模型的贡献。此外，如果客户端故意上传低质量或黑客模型，那么使用这些模型进行融合会导致全球模型的准确率受到严重的影响。传统的联合学习算法无法解决这些问题。为解决这个问题，我们提出了 FedDRL，一种基于奖励学习的模型融合方法。在第一阶段，我们的方法可以过滤出黑客模型并选择可信客户端模型参与融合。在第二阶段，FedDRL算法可以自适应地调整可信客户端模型的权重值，并融合最佳的全球模型。我们还定义了五种模型融合场景，并与两个基准算法进行比较。实验结果显示，我们的算法在可靠性和准确率之间具有良好的平衡。
</details></li>
</ul>
<hr>
<h2 id="Towards-an-AI-Accountability-Policy"><a href="#Towards-an-AI-Accountability-Policy" class="headerlink" title="Towards an AI Accountability Policy"></a>Towards an AI Accountability Policy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13658">http://arxiv.org/abs/2307.13658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Przemyslaw Grabowicz, Nicholas Perello, Yair Zick</li>
<li>For: The white paper offers a set of interconnected recommendations for an AI accountability policy in response to the “AI Accountability Policy Request for Comments” by the National Telecommunications and Information Administration of the United States.* Methods: The white paper provides a set of recommendations for an AI accountability policy, including the development of transparent and explainable AI, the establishment of accountability mechanisms for AI systems, and the promotion of human-centered AI.* Results: The white paper aims to provide a comprehensive framework for an AI accountability policy that can be used to ensure the responsible development and use of AI in various industries and applications.<details>
<summary>Abstract</summary>
This white paper is a response to the "AI Accountability Policy Request for Comments" by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.
</details>
<details>
<summary>摘要</summary>
这份白皮书是对美国国家电信和信息管理局（NTIA）发布的“人工智能负责任政策请求意见”的回应。文中提到的问题号以句末的括号形式标注。本白皮书提出了一组相互连接的人工智能负责任政策建议。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="GNN4FR-A-Lossless-GNN-based-Federated-Recommendation-Framework"><a href="#GNN4FR-A-Lossless-GNN-based-Federated-Recommendation-Framework" class="headerlink" title="GNN4FR: A Lossless GNN-based Federated Recommendation Framework"></a>GNN4FR: A Lossless GNN-based Federated Recommendation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01197">http://arxiv.org/abs/2308.01197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guowei Wu, Weike Pan, Zhong Ming</li>
<li>for: 隐私保护下构建全Graph Neural Networks（GNNs）推荐系统。</li>
<li>methods: 使用lossless federated recommendation framework based on GNN，实现全图训练，保持高阶结构信息完整性。</li>
<li>results: 与非联合方法相等，具有完整高阶结构信息，遵循隐私保护法规。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have gained wide popularity in recommender systems due to their capability to capture higher-order structure information among the nodes of users and items. However, these methods need to collect personal interaction data between a user and the corresponding items and then model them in a central server, which would break the privacy laws such as GDPR. So far, no existing work can construct a global graph without leaking each user's private interaction data (i.e., his or her subgraph). In this paper, we are the first to design a novel lossless federated recommendation framework based on GNN, which achieves full-graph training with complete high-order structure information, enabling the training process to be equivalent to the corresponding un-federated counterpart. In addition, we use LightGCN to instantiate an example of our framework and show its equivalence.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经在推荐系统中得到广泛应用，因为它们可以捕捉用户和物品之间的高阶结构信息。然而，这些方法需要收集每个用户与对应的物品之间的个人互动数据，然后模型在中央服务器上，这会违反隐私法规，如GDPR。目前没有任何现有的工作可以构建全球图without泄露每个用户的私人互动数据（即他或她的子图）。在这篇论文中，我们是首次设计了一种新的无损联邦推荐框架基于GNN，实现了全图训练，并保留了高阶结构信息完整性。此外，我们使用LightGCN来实现这种框架的示例，并证明其等价性。
</details></li>
</ul>
<hr>
<h2 id="Safety-Margins-for-Reinforcement-Learning"><a href="#Safety-Margins-for-Reinforcement-Learning" class="headerlink" title="Safety Margins for Reinforcement Learning"></a>Safety Margins for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13642">http://arxiv.org/abs/2307.13642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Grushin, Walt Woods, Alvaro Velasquez, Simon Khan</li>
<li>For: The paper is written for identifying and mitigating unsafe situations in autonomous controllers, particularly in freight transportation applications.* Methods: The paper proposes using a proxy criticality metric that can be computed in real-time to identify when an agent is approaching a potentially catastrophic situation. The metric is based on the mean reduction in reward given some number of random actions.* Results: The paper demonstrates the effectiveness of its approach by evaluating it on learned policies from APE-X and A3C within an Atari environment. The results show that safety margins decrease as agents approach failure states, indicating the potential for catastrophic outcomes.<details>
<summary>Abstract</summary>
Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monitoring deployed agents allows for the real-time identification of potentially catastrophic situations.
</details>
<details>
<summary>摘要</summary>
任何自主控制器都会在某些情况下不安全。能够量化地识别这些不安全情况的发生是对于启动人工监督的时间而言至关重要，例如在货物运输应用中。在这项工作中，我们示出了一种真实的批处性可以坚定地定义为一些随机动作后的奖励减少的平均值。我们称这种代理批处性指标可以在实时（无需实际模拟动作的影响）计算，并且可以与真实批处性进行比较。我们还示出了如何利用这些代理指标生成安全余地，这些安全余地直接反映了可能 incorrect 动作的后果和预期的性能损失。我们在 APE-X 和 A3C 学习政策中的 Atari 环境中评估了我们的方法，并示出了安全余地如何随agent接近失败状态而逐渐减少。将安全余地集成到部署过程中的监控程序中，可以实现实时识别可能 catastrophic 的情况。
</details></li>
</ul>
<hr>
<h2 id="DBGSA-A-Novel-Data-Adaptive-Bregman-Clustering-Algorithm"><a href="#DBGSA-A-Novel-Data-Adaptive-Bregman-Clustering-Algorithm" class="headerlink" title="DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm"></a>DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14375">http://arxiv.org/abs/2307.14375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Xiao, Hou-biao Li, Yu-pu Zhang</li>
<li>for: 提高非对称数据集中 clustering 算法的精度和稳定性。</li>
<li>methods: 提议一种基于 Bregman 差分参数优化的数据驱动 clustering 算法 (DBGSA)， combines 宇宙 gravitational algorithm 将相似点靠拢在数据集中。 构造了 gravitational coefficient equation  WITH special property 逐步减少影响因子。 使用 Bregman divergence generalized power mean information loss minimization 标识群集中心。</li>
<li>results: 对四个 simulated 数据集和六个实际数据集进行了广泛的实验，结果表明 DBGSA 在不同 clustering 算法中的准确率平均提高了63.8%，与其他类似方法和改进的数据集相比。 还设立了三维网格搜索来比较不同参数值的影响，发现我们的模型提供的参数集是优化的。<details>
<summary>Abstract</summary>
With the development of Big data technology, data analysis has become increasingly important. Traditional clustering algorithms such as K-means are highly sensitive to the initial centroid selection and perform poorly on non-convex datasets. In this paper, we address these problems by proposing a data-driven Bregman divergence parameter optimization clustering algorithm (DBGSA), which combines the Universal Gravitational Algorithm to bring similar points closer in the dataset. We construct a gravitational coefficient equation with a special property that gradually reduces the influence factor as the iteration progresses. Furthermore, we introduce the Bregman divergence generalized power mean information loss minimization to identify cluster centers and build a hyperparameter identification optimization model, which effectively solves the problems of manual adjustment and uncertainty in the improved dataset. Extensive experiments are conducted on four simulated datasets and six real datasets. The results demonstrate that DBGSA significantly improves the accuracy of various clustering algorithms by an average of 63.8\% compared to other similar approaches like enhanced clustering algorithms and improved datasets. Additionally, a three-dimensional grid search was established to compare the effects of different parameter values within threshold conditions, and it was discovered the parameter set provided by our model is optimal. This finding provides strong evidence of the high accuracy and robustness of the algorithm.
</details>
<details>
<summary>摘要</summary>
随着大数据技术的发展，数据分析已成为非常重要。传统的聚类算法如K-means受初始中心选择的影响很大，在非对称数据集上表现不佳。在这篇论文中，我们解决这些问题，提出一种基于数据驱动的Bregman差分参数优化聚类算法（DBGSA）。我们将 Universal Gravitational Algorithm 用于将相似点靠拢在数据集中。我们构建了一个引力系数方程，其特点是逐步减少影响因子。此外，我们引入Bregman差分总能平均信息损失来确定聚类中心，并构建了一个超参数标准化模型，可以有效解决手动调整和不确定性问题。我们在四个 simulated 数据集和六个实际数据集上进行了广泛的实验。结果表明，DBGSA可以在不同的聚类算法下提高准确率的平均提升率为63.8%，比其他相似方法的改进数据集和优化超参数更高。此外，我们设立了三维网格搜索，并发现我们的模型提供的参数集是优化的。这一结论提供了高精度和稳定性的证据。
</details></li>
</ul>
<hr>
<h2 id="Turning-hazardous-volatile-matter-compounds-into-fuel-by-catalytic-steam-reforming-An-evolutionary-machine-learning-approach"><a href="#Turning-hazardous-volatile-matter-compounds-into-fuel-by-catalytic-steam-reforming-An-evolutionary-machine-learning-approach" class="headerlink" title="Turning hazardous volatile matter compounds into fuel by catalytic steam reforming: An evolutionary machine learning approach"></a>Turning hazardous volatile matter compounds into fuel by catalytic steam reforming: An evolutionary machine learning approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05750">http://arxiv.org/abs/2308.05750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Shafizadeh, Hossein Shahbeik, Mohammad Hossein Nadian, Vijai Kumar Gupta, Abdul-Sattar Nizami, Su Shiung Lam, Wanxi Peng, Junting Pan, Meisam Tabatabaei, Mortaza Aghbashlo</li>
<li>for: 这种研究旨在开发一种基于机器学习的研究框架，用于模拟、理解和优化 catalytic steam reforming 过程中的材料和反应条件。</li>
<li>methods: 该研究使用了 X-ray diffraction analysis 等化学&#x2F;Texture analysis 获取输入特征，并使用了 Literature compile 一个包括多种催化剂特性和反应条件的数据库。研究采用了 six 种机器学习模型，并使用了粒子群搜索算法进行优化。</li>
<li>results: 研究结果表明， ensemble 机器学习模型可以提供高度预测性（R2 &gt; 0.976） для toluene 转化和产物分布。最佳的 tar 转化率高于 77.2% 可以在 637.44-725.62 ℃ 的温度范围内实现，催化剂 BET 表面积在 476.03-638.55 m2&#x2F;g 范围内。<details>
<summary>Abstract</summary>
Chemical and biomass processing systems release volatile matter compounds into the environment daily. Catalytic reforming can convert these compounds into valuable fuels, but developing stable and efficient catalysts is challenging. Machine learning can handle complex relationships in big data and optimize reaction conditions, making it an effective solution for addressing the mentioned issues. This study is the first to develop a machine-learning-based research framework for modeling, understanding, and optimizing the catalytic steam reforming of volatile matter compounds. Toluene catalytic steam reforming is used as a case study to show how chemical/textural analyses (e.g., X-ray diffraction analysis) can be used to obtain input features for machine learning models. Literature is used to compile a database covering a variety of catalyst characteristics and reaction conditions. The process is thoroughly analyzed, mechanistically discussed, modeled by six machine learning models, and optimized using the particle swarm optimization algorithm. Ensemble machine learning provides the best prediction performance (R2 > 0.976) for toluene conversion and product distribution. The optimal tar conversion (higher than 77.2%) is obtained at temperatures between 637.44 and 725.62 {\deg}C, with a steam-to-carbon molar ratio of 5.81-7.15 and a catalyst BET surface area 476.03-638.55 m2/g. The feature importance analysis satisfactorily reveals the effects of input descriptors on model prediction. Operating conditions (50.9%) and catalyst properties (49.1%) are equally important in modeling. The developed framework can expedite the search for optimal catalyst characteristics and reaction conditions, not only for catalytic chemical processing but also for related research areas.
</details>
<details>
<summary>摘要</summary>
化学和生物质处理系统每天都会发布可燃物质分子到环境中。catalytic reforming可以将这些分子转化为有价值的燃料，但是开发稳定和高效的催化剂是挑战。机器学习可以处理复杂的关系在大数据中，因此可以成为改进反应条件的有效解决方案。这项研究是首次开发了基于机器学习研究框架，用于模拟、理解和优化催化液气 reforming的气相催化剂。使用苯酚为例，通过化学/文化分析（例如X射晶体分析）获得输入特征，并使用文献编译一份包括多种催化剂特性和反应条件的数据库。通过综合分析、机制分析、六种机器学习模型和粒子群优化算法，我们获得了最佳的苯酚转化率（高于77.2%）和产品分布。 ensemble机器学习提供了最佳预测性能（R2 > 0.976）。最佳的沸腾 conversions（高于77.2%）在637.44-725.62℃的温度范围内，催化剂BET表面积476.03-638.55 m2/g。特征重要性分析准确地显示输入特征对模型预测的影响。操作条件（50.9%）和催化剂性质（49.1%）具有相等的重要性。我们开发的框架可以减少催化剂特性和反应条件的搜索，不仅限于催化化学处理，还可以应用于相关领域的研究。
</details></li>
</ul>
<hr>
<h2 id="Scaling-machine-learning-based-chemical-plant-simulation-A-method-for-fine-tuning-a-model-to-induce-stable-fixed-points"><a href="#Scaling-machine-learning-based-chemical-plant-simulation-A-method-for-fine-tuning-a-model-to-induce-stable-fixed-points" class="headerlink" title="Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points"></a>Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13621">http://arxiv.org/abs/2307.13621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Esders, Gimmy Alex Fernandez Ramirez, Michael Gastegger, Satya Swarup Samal</li>
<li>for: 这篇论文是为了提出一种基于机器学习（ML）的化学厂模型，以取代理性的首要原理模型。</li>
<li>methods: 这篇论文使用了一种结构化的方法，即每个厂区域都由一个ML模型表示。然后，这些模型被连接到一个流程图像中，以便进行数据预测。</li>
<li>results: 作者发现，对于较小的厂房，这种方法效果非常好，但是对于更大的厂房，由于巨大的循环逻辑引起的循环解决问题会导致不稳定。作者分析了这个问题，并提出了一种方法来细调ML模型，以便通过常规方法解决循环问题。<details>
<summary>Abstract</summary>
Idealized first-principles models of chemical plants can be inaccurate. An alternative is to fit a Machine Learning (ML) model directly to plant sensor data. We use a structured approach: Each unit within the plant gets represented by one ML model. After fitting the models to the data, the models are connected into a flowsheet-like directed graph. We find that for smaller plants, this approach works well, but for larger plants, the complex dynamics arising from large and nested cycles in the flowsheet lead to instabilities in the cycle solver. We analyze this problem in depth and show that it is not merely a specialized concern but rather a more pervasive challenge that will likely occur whenever ML is applied to larger plants. To address this problem, we present a way to fine-tune ML models such that solving cycles with the usual methods becomes robust again.
</details>
<details>
<summary>摘要</summary>
理想化的基本原理模型可能不准确。一种alternative是直接将机器学习（ML）模型适应到厂区传感器数据。我们采用一种结构化方法：每个喷槽内部的设备都被一个ML模型表示。在模型适应数据后，模型被连接成一个流程图像类似的导向图。我们发现对小型厂区来说，这种方法工作良好，但对更大的厂区来说，由于大量和嵌入的循环在流程图中导致循环解决器的不稳定。我们对这个问题进行深入分析，并证明这不仅是特殊情况，而是更广泛的挑战， ML在更大的厂区中应用时会遇到这种问题。为解决这个问题，我们提出了一种细化ML模型的方法，使得通过常规方法解决循环变得稳定。
</details></li>
</ul>
<hr>
<h2 id="AI-and-ethics-in-insurance-a-new-solution-to-mitigate-proxy-discrimination-in-risk-modeling"><a href="#AI-and-ethics-in-insurance-a-new-solution-to-mitigate-proxy-discrimination-in-risk-modeling" class="headerlink" title="AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling"></a>AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13616">http://arxiv.org/abs/2307.13616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marguerite Sauce, Antoine Chancel, Antoine Ly</li>
<li>for: 这个论文的目的是探讨如何使用机器学习算法实现更公平的保险业务，以及如何避免对保险公司的偏见。</li>
<li>methods: 这个论文使用了欧洲人权公约中关于歧视的指南，以及使用敏感个人数据在算法中的regulation。它还提出了一种新的方法，基于数 Linear Algebra的概念，以降低对直接歧视的风险。</li>
<li>results: 该论文的研究表明，使用这种新方法可以减少对直接歧视的风险，同时保持保险公司的资产风险评估和价格段化的精度。这种方法的使用也是简单易用，并且在一个具体的生命保险选择中得到了Promising的性能。<details>
<summary>Abstract</summary>
The development of Machine Learning is experiencing growing interest from the general public, and in recent years there have been numerous press articles questioning its objectivity: racism, sexism, \dots Driven by the growing attention of regulators on the ethical use of data in insurance, the actuarial community must rethink pricing and risk selection practices for fairer insurance. Equity is a philosophy concept that has many different definitions in every jurisdiction that influence each other without currently reaching consensus. In Europe, the Charter of Fundamental Rights defines guidelines on discrimination, and the use of sensitive personal data in algorithms is regulated. If the simple removal of the protected variables prevents any so-called `direct' discrimination, models are still able to `indirectly' discriminate between individuals thanks to latent interactions between variables, which bring better performance (and therefore a better quantification of risk, segmentation of prices, and so on). After introducing the key concepts related to discrimination, we illustrate the complexity of quantifying them. We then propose an innovative method, not yet met in the literature, to reduce the risks of indirect discrimination thanks to mathematical concepts of linear algebra. This technique is illustrated in a concrete case of risk selection in life insurance, demonstrating its simplicity of use and its promising performance.
</details>
<details>
<summary>摘要</summary>
机器学习的发展正在吸引越来越多的一般大众注意，最近几年有许多新闻报导质疑其中的公正性： racism、性别歧视、等等。随着规制机关对数据的伦理使用的关注，保险业界必须重新思考定价和风险选择实践，以确保更公正的保险。“公正”是一个哲学概念，在每个司法管辖区域都有不同的定义，这些定义彼此影响并未能达成现在的共识。在欧洲，《权利宪法》提供了关于歧视的指南，而使用敏感个人数据在算法中的使用则被规管。即使简单地删除保护变数，模型仍然能够间接歧视个人，因为变数之间的隐藏互动带来更好的性能（并因此更好地评估风险、价格分类等）。我们首先介绍了歧视的定义，然后详细介绍了该现象的复杂性。接着，我们提出了一种新的方法，未经过文献中的应用，以减少间接歧视的风险。这种方法基于数学概念的线性代数，并在生命保险中的风险选择中实现了简单且有前途的使用。
</details></li>
</ul>
<hr>
<h2 id="Team-Intro-to-AI-team8-at-CoachAI-Badminton-Challenge-2023-Advanced-ShuttleNet-for-Shot-Predictions"><a href="#Team-Intro-to-AI-team8-at-CoachAI-Badminton-Challenge-2023-Advanced-ShuttleNet-for-Shot-Predictions" class="headerlink" title="Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions"></a>Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13715">http://arxiv.org/abs/2307.13715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shih-Hong Chen, Pin-Hsuan Chou, Yong-Fu Liu, Chien-An Han</li>
<li>for: 提高现有框架ShuttleNet在预测羽毛球发球类型和位置的性能，通过利用过去的撕击。</li>
<li>methods: 利用过去的撕击来预测羽毛球发球类型和位置。</li>
<li>results: 在IJCAI 2023的CoachAI Badminton Challenge中获得了明显更好的结果，比基eline更高，最终取得了比赛的第一名，代码也公开了。<details>
<summary>Abstract</summary>
In this paper, our objective is to improve the performance of the existing framework ShuttleNet in predicting badminton shot types and locations by leveraging past strokes. We participated in the CoachAI Badminton Challenge at IJCAI 2023 and achieved significantly better results compared to the baseline. Ultimately, our team achieved the first position in the competition and we made our code available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们的目标是通过利用过去的击球来提高现有框架ShuttleNet在预测羽毛球shot类型和位置的性能。我们参加了IJCAI 2023年的CoachAI Badminton Challenge，并实现了相比基eline的显著提高。最终，我们的团队取得了比赛的第一名，并将代码公开。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Forecasting-capturing-and-activation-of-carbon-dioxide-CO-2-Integration-of-Time-Series-Analysis-Machine-Learning-and-Material-Design"><a href="#Forecasting-capturing-and-activation-of-carbon-dioxide-CO-2-Integration-of-Time-Series-Analysis-Machine-Learning-and-Material-Design" class="headerlink" title="Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design"></a>Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14374">http://arxiv.org/abs/2307.14374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suchetana Sadhukhan, Vivek Kumar Yadav</li>
<li>For: This study provides a comprehensive time series analysis of daily industry-specific, country-wise CO2 emissions from January 2019 to February 2023, with a focus on European countries (EU27 &amp; UK, Italy, Germany, Spain) and India.* Methods: The research uses near-real-time activity data from the Carbon Monitor research initiative, and performs a principal component analysis (PCA) to determine the key contributors to CO2 emissions. The study also employs a 7-day moving averaged dataset for further analysis and uses Long Short-Term Memory (LSTM) models to predict emissions.* Results: The study finds that the Power, Industry, and Ground Transport sectors account for a significant portion of the variance in the dataset, and the LSTM models achieve high efficiency with $R^2$ values ranging from 0.8242 to 0.995 for various countries and sectors. Additionally, the study proposes the use of scandium and boron&#x2F;aluminium-based thin films as exceptionally efficient materials for capturing CO2.<details>
<summary>Abstract</summary>
This study provides a comprehensive time series analysis of daily industry-specific, country-wise CO$_2$ emissions from January 2019 to February 2023. The research focuses on the Power, Industry, Ground Transport, Domestic Aviation, and International Aviation sectors in European countries (EU27 & UK, Italy, Germany, Spain) and India, utilizing near-real-time activity data from the Carbon Monitor research initiative. To identify regular emission patterns, the data from the year 2020 is excluded due to the disruptive effects caused by the COVID-19 pandemic. The study then performs a principal component analysis (PCA) to determine the key contributors to CO$_2$ emissions. The analysis reveals that the Power, Industry, and Ground Transport sectors account for a significant portion of the variance in the dataset. A 7-day moving averaged dataset is employed for further analysis to facilitate robust predictions. This dataset captures both short-term and long-term trends and enhances the quality of the data for prediction purposes. The study utilizes Long Short-Term Memory (LSTM) models on the 7-day moving averaged dataset to effectively predict emissions and provide insights for policy decisions, mitigation strategies, and climate change efforts. During the training phase, the stability and convergence of the LSTM models are ensured, which guarantees their reliability in the testing phase. The evaluation of the loss function indicates this reliability. The model achieves high efficiency, as demonstrated by $R^2$ values ranging from 0.8242 to 0.995 for various countries and sectors. Furthermore, there is a proposal for utilizing scandium and boron/aluminium-based thin films as exceptionally efficient materials for capturing CO$_2$ (with a binding energy range from -3.0 to -3.5 eV). These materials are shown to surpass the affinity of graphene and boron nitride sheets in this regard.
</details>
<details>
<summary>摘要</summary>
To facilitate robust predictions, a 7-day moving averaged dataset is employed for further analysis. This dataset captures both short-term and long-term trends and enhances the quality of the data for prediction purposes. The study utilizes Long Short-Term Memory (LSTM) models on the 7-day moving averaged dataset to effectively predict emissions and provide insights for policy decisions, mitigation strategies, and climate change efforts. The stability and convergence of the LSTM models are ensured during the training phase, which guarantees their reliability in the testing phase. The evaluation of the loss function indicates this reliability.The model achieves high efficiency, as demonstrated by $R^2$ values ranging from 0.8242 to 0.995 for various countries and sectors. Furthermore, the study proposes the use of scandium and boron/aluminum-based thin films as exceptionally efficient materials for capturing CO2. These materials are shown to surpass the affinity of graphene and boron nitride sheets in this regard.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/26/cs.LG_2023_07_26/" data-id="cllsj1rnc002bpf88b8aqbq2w" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/cs.SD_2023_07_26/" class="article-date">
  <time datetime="2023-07-25T16:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/26/cs.SD_2023_07_26/">cs.SD - 2023-07-26 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Say-Goodbye-to-RNN-T-Loss-A-Novel-CIF-based-Transducer-Architecture-for-Automatic-Speech-Recognition"><a href="#Say-Goodbye-to-RNN-T-Loss-A-Novel-CIF-based-Transducer-Architecture-for-Automatic-Speech-Recognition" class="headerlink" title="Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition"></a>Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14132">http://arxiv.org/abs/2307.14132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian-Hao Zhang, Dinghao Zhou, Guiping Zhong, Baoxiang Li</li>
<li>for: 提高 ASR 模型的效率和精度</li>
<li>methods: 提出一种新的 CIF-Transducer 模型，具有终端听写 Integrate-and-Fire 机制，从而减少计算复杂度并让预测网络扮演更重要的角色。同时，引入 Funnel-CIF、Context Blocks、Unified Gating 和 Bilinear Pooling 等网络结构和辅助训练策略来进一步提高性能。</li>
<li>results: 在 AISHELL-1 和 WenetSpeech 等 dataset 上进行了实验，显示 CIF-T 模型可以与 RNN-T 模型相比，并且具有较低的计算复杂度和更高的精度。<details>
<summary>Abstract</summary>
RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve length alignment between input audio and target sequence. However, the implementation complexity and the alignment-based optimization target of RNN-T loss lead to computational redundancy and a reduced role for predictor network, respectively. In this paper, we propose a novel model named CIF-Transducer (CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism with the RNN-T model to achieve efficient alignment. In this way, the RNN-T loss is abandoned, thus bringing a computational reduction and allowing the predictor network a more significant role. We also introduce Funnel-CIF, Context Blocks, Unified Gating and Bilinear Pooling joint network, and auxiliary training strategy to further improve performance. Experiments on the 178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves state-of-the-art results with lower computational overhead compared to RNN-T models.
</details>
<details>
<summary>摘要</summary>
RNN-T模型广泛应用于语音识别领域，它们依赖于RNN-T损失来实现输入音频和目标序列之间的长度对齐。然而，实现复杂性和对齐基于优化目标导致RNN-T模型中的计算循环和预测网络的role减小。在这篇论文中，我们提出了一种新的模型名为CIF-Transducer（CIF-T），它将Continuous Integrate-and-Fire（CIF）机制与RNN-T模型结合起来实现有效的对齐。这样，RNN-T损失可以被抛弃，从而实现计算减少和预测网络的更大角色。我们还引入了滤波器-CIF、上下文块、统一阀值和bilinear pooling的联合网络，以及辅助训练策略，以进一步提高性能。实验表明，CIF-T在AISHELL-1和WenetSpeech数据集上达到了状态机器的Result，同时计算负担相对较低。
</details></li>
</ul>
<hr>
<h2 id="The-Hidden-Dance-of-Phonemes-and-Visage-Unveiling-the-Enigmatic-Link-between-Phonemes-and-Facial-Features"><a href="#The-Hidden-Dance-of-Phonemes-and-Visage-Unveiling-the-Enigmatic-Link-between-Phonemes-and-Facial-Features" class="headerlink" title="The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features"></a>The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13953">http://arxiv.org/abs/2307.13953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj</li>
<li>for: 本研究揭示了声音和脸部特征之间的潜在关系。传统的voice-face相关性研究通常使用长时间的声音输入，包括从声音生成脸像和从声音重建3D脸镜。但在voice-based犯罪调查中，可能有限的声音证据。此外，从生物学角度来看，每个语音段（phoneme）对应不同的空气流和脸部运动。因此，发现声音和脸部特征之间的隐藏关系是有利的。</li>
<li>methods: 本研究提出了一个细化的分析管道，用于探索声音和脸部之间的关系，即phonemes v.s. 脸部 anthropometric measurements (AM)。我们建立了每个声音-AM对的估计器，并通过假设检测来评估相关性。我们的结果表明，AMs在元音上更易预测，特别是与塞擦音相关。此外，我们发现，在某些AM的发音时，它们更易预测。</li>
<li>results: 我们的结果支持生物学中的相关性理论，并为未来的speech-face多模态学习奠定基础。<details>
<summary>Abstract</summary>
This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose an analysis pipeline to explore the voice-face relationship in a fine-grained manner, specifically, phonemes versus facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results show that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we find that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding the correlation and lay the foundation for future research on speech-face multimodal learning.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Voice-Face-Correlation-A-Geometry-View"><a href="#Rethinking-Voice-Face-Correlation-A-Geometry-View" class="headerlink" title="Rethinking Voice-Face Correlation: A Geometry View"></a>Rethinking Voice-Face Correlation: A Geometry View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13948">http://arxiv.org/abs/2307.13948</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lxa9867/VAF">https://github.com/lxa9867/VAF</a></li>
<li>paper_authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj</li>
<li>for:  investigate the capability of reconstructing 3D facial shape from voice from a geometry perspective without any semantic information.</li>
<li>methods: propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction.</li>
<li>results: significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium.<details>
<summary>Abstract</summary>
Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.
</details>
<details>
<summary>摘要</summary>
previous works on voice-face matching and voice-guided face synthesis have shown strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. in this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. we propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. by leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Quality-Enhancement-of-Sound-Field-Synthesis-Based-on-Combination-of-Pressure-and-Amplitude-Matching"><a href="#Perceptual-Quality-Enhancement-of-Sound-Field-Synthesis-Based-on-Combination-of-Pressure-and-Amplitude-Matching" class="headerlink" title="Perceptual Quality Enhancement of Sound Field Synthesis Based on Combination of Pressure and Amplitude Matching"></a>Perceptual Quality Enhancement of Sound Field Synthesis Based on Combination of Pressure and Amplitude Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13941">http://arxiv.org/abs/2307.13941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keisuke Kimura, Shoichi Koyama, Hiroshi Saruwatari</li>
<li>for: 提高听众对声场的 восприятие质量</li>
<li>methods: 使用压力和幅度匹配方法组合来减少高频 synthesis 错误，并使用声场的人类听觉特性来synthesize 横向声localization</li>
<li>results: 比较实验和听觉测试表明，提出的方法可以提高声场synthesized的 восприятие质量，比传统压力匹配方法更好。<details>
<summary>Abstract</summary>
A sound field synthesis method enhancing perceptual quality is proposed. Sound field synthesis using multiple loudspeakers enables spatial audio reproduction with a broad listening area; however, synthesis errors at high frequencies called spatial aliasing artifacts are unavoidable. To minimize these artifacts, we propose a method based on the combination of pressure and amplitude matching. On the basis of the human's auditory properties, synthesizing the amplitude distribution will be sufficient for horizontal sound localization. Furthermore, a flat amplitude response should be synthesized as much as possible to avoid coloration. Therefore, we apply amplitude matching, which is a method to synthesize the desired amplitude distribution with arbitrary phase distribution, for high frequencies and conventional pressure matching for low frequencies. Experimental results of numerical simulations and listening tests using a practical system indicated that the perceptual quality of the sound field synthesized by the proposed method was improved from that synthesized by pressure matching.
</details>
<details>
<summary>摘要</summary>
一种提高听觉质量的声场合成方法被提出。使用多个扬音器实现声场合成可以提供广泛的听众区域，但高频合成错误无法避免。为减少这些错误，我们提出基于压力和幅度匹配的方法。根据人类听觉特性，Synthesizing amplitude distribution是足够的 для水平声localization。此外，尽可能地Synthesizing平坦的幅度响应可以避免染色。因此，我们应用幅度匹配，即Synthesizing所需的幅度分布的任意相位分布，高频和普通压力匹配low frequency。实验结果表明，由提出的方法synthesize的听field质量与压力匹配相比有所提高。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Interactions-between-Target-Positive-and-Negative-Information-for-Acoustic-Echo-Cancellation"><a href="#Exploring-the-Interactions-between-Target-Positive-and-Negative-Information-for-Acoustic-Echo-Cancellation" class="headerlink" title="Exploring the Interactions between Target Positive and Negative Information for Acoustic Echo Cancellation"></a>Exploring the Interactions between Target Positive and Negative Information for Acoustic Echo Cancellation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13888">http://arxiv.org/abs/2307.13888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Han, Xinmeng Xu, Weiping Tu, Yuhong Yang, Yajie Liu</li>
<li>for: 降低干扰信号的干扰 echo 抑制（AEC），以便保留近端语音最少受到损害。</li>
<li>methods: 我们提出了一种新的AEC模型encoder-decoder架构，以质量指导的方式使用目标负信息（如干扰信号和特征）来帮助模型更好地分辨目标语音和干扰信号的模式。</li>
<li>results: 我们的CMNet模型在实验中表现出了较好的性能，比如最近的方法。<details>
<summary>Abstract</summary>
Acoustic echo cancellation (AEC) aims to remove interference signals while leaving near-end speech least distorted. As the indistinguishable patterns between near-end speech and interference signals, near-end speech can't be separated completely, causing speech distortion and interference signals residual. We observe that besides target positive information, e.g., ground-truth speech and features, the target negative information, such as interference signals and features, helps make pattern of target speech and interference signals more discriminative. Therefore, we present a novel AEC model encoder-decoder architecture with the guidance of negative information termed as CMNet. A collaboration module (CM) is designed to establish the correlation between the target positive and negative information in a learnable manner via three blocks: target positive, target negative, and interactive block. Experimental results demonstrate our CMNet achieves superior performance than recent methods.
</details>
<details>
<summary>摘要</summary>
宽band acoustic echo cancellation（AEC）目的是去除干扰信号，保留近端语音最小变形。由于干扰信号和近端语音干扰信号的模式相同，因此无法完全分离近端语音，导致语音扭曲和干扰信号剩下。我们发现，除了目标正面信息（如真实语音和特征）之外，目标负面信息（如干扰信号和特征）也有助于使target speech和干扰信号的模式更加分化。因此，我们提出了一种基于encoder-decoder架构的新型AEC模型，称为CMNet。协作模块（CM）通过三个块（目标正面、目标负面和互动块）来在学习方式下建立目标正面和负面信息之间的相关性。实验结果表明，我们的CMNet在latest方法之上具有更高的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/26/cs.SD_2023_07_26/" data-id="cllsj1rnz004bpf88elp24pll" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/eess.AS_2023_07_26/" class="article-date">
  <time datetime="2023-07-25T16:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/26/eess.AS_2023_07_26/">eess.AS - 2023-07-26 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sound-Field-Estimation-around-a-Rigid-Sphere-with-Physics-informed-Neural-Network"><a href="#Sound-Field-Estimation-around-a-Rigid-Sphere-with-Physics-informed-Neural-Network" class="headerlink" title="Sound Field Estimation around a Rigid Sphere with Physics-informed Neural Network"></a>Sound Field Estimation around a Rigid Sphere with Physics-informed Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14013">http://arxiv.org/abs/2307.14013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Chen, Fei Ma, Amy Bastine, Prasanga Samarasinghe, Huiyuan Sun</li>
<li>for: used to estimate the sound field around a rigid sphere with limited measurements</li>
<li>methods: uses a physics-informed neural network that incorporates physical knowledge and constraints from the Helmholtz equation and zero radial velocity condition</li>
<li>results: outperforms the spherical harmonic method and plane-wave decomposition method in terms of accuracy and fitting ability<details>
<summary>Abstract</summary>
Accurate estimation of the sound field around a rigid sphere necessitates adequate sampling on the sphere, which may not always be possible. To overcome this challenge, this paper proposes a method for sound field estimation based on a physics-informed neural network. This approach integrates physical knowledge into the architecture and training process of the network. In contrast to other learning-based methods, the proposed method incorporates additional constraints derived from the Helmholtz equation and the zero radial velocity condition on the rigid sphere. Consequently, it can generate physically feasible estimations without requiring a large dataset. In contrast to the spherical harmonic-based method, the proposed approach has better fitting abilities and circumvents the ill condition caused by truncation. Simulation results demonstrate the effectiveness of the proposed method in achieving accurate sound field estimations from limited measurements, outperforming the spherical harmonic method and plane-wave decomposition method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>精度地估算固定球体周围的声场需要足够的采样在球体上，但这并不总是可能的。为了解决这个挑战，这篇论文提出了基于物理学习网络的声场估算方法。这种方法将物理知识integrated到网络体系和训练过程中。与其他学习基本方法不同，提出的方法在球体上添加了基于哈姆霍尔兹方程和零辐射速度条件的额外约束。因此，它可以生成物理可能的估算而无需大量数据。与圆柱声波分解方法不同，提出的方法具有更好的适应能力，并circumvent了由截断引起的糟糕条件。实验结果表明，提出的方法可以从有限测量数据中获得高精度的声场估算，超越圆柱声波分解方法和平面波分解方法。</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Speech-representation-learning-Learning-bidirectional-encoders-with-single-view-multi-view-and-multi-task-methods"><a href="#Speech-representation-learning-Learning-bidirectional-encoders-with-single-view-multi-view-and-multi-task-methods" class="headerlink" title="Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods"></a>Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00129">http://arxiv.org/abs/2308.00129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingming Tang</li>
<li>For: 本论文主要目标是为时空序列数据学习 repre sentation learning，以提高下游序列预测任务的性能。* Methods: 本论文使用了多种学习 Setting，包括supervised learning with auxiliary losses、Unsupervised learning、semi-supervised learning和多视图学习。同时，本论文还 explore multiple approaches for representation learning，包括 speech data。* Results: 本论文的研究发现，不同的学习 Setting和approaches可以获得不同的 repre sentation learning results。此外，本论文还发现一些findings可以在不同的Domain上应用。<details>
<summary>Abstract</summary>
This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Though I focus on speech data, the methods described in this thesis can also be applied to other domains. Overall, the field of representation learning is developing rapidly. State-of-the-art results on speech related tasks are typically based on Transformers pre-trained with large-scale self-supervised learning, which aims to learn generic representations that can benefit multiple downstream tasks. Since 2020, large-scale pre-training has been the de facto choice to achieve good performance. This delayed thesis does not attempt to summarize and compare with the latest results on speech representation learning; instead, it presents a unique study on speech representation learning before the Transformer era, that covers multiple learning settings. Some of the findings in this thesis can still be useful today.
</details>
<details>
<summary>摘要</summary>
这个论文关注在时间或空间序列数据上进行表示学习，以提高下游序列预测任务的性能。supervised learning是训练深度神经网络以获得好的Sequential Representations的主要方法。然而，缺乏充足的标注数据是超vised learning的一个限制因素。为了解决这个挑战，我们可以探索使用大量未标注和弱标注数据，以及额外的数据模式来学习表示。我的研究涵盖了Speech数据的表示学习。与大多数其他工作一样，我们不仅关注单个学习环境，还研究了多种学习问题，包括supervised learning with auxiliary losses、unsupervised learning、semi-supervised learning和多视图学习。此外，我们还探索了不同的表示学习方法。尽管我们主要关注Speech数据，但所描述的方法也可以应用于其他领域。总的来说，表示学习领域在发展 rapidly，state-of-the-art results on speech related tasks通常基于Transformers预训练大规模自适应学习，旨在学习通用的表示，以便多个下游任务。自2020年以来，大规模预训练成为了下游任务的de facto选择。这个论文不尝试总结和与最新的Result on speech representation learning进行比较，而是提供了在Transformer时代之前的唯一研究Speech representation learning，涵盖了多种学习环境。一些本论文中的发现仍然有用。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/26/eess.AS_2023_07_26/" data-id="cllsj1rol006dpf88b73w8ub5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/eess.IV_2023_07_26/" class="article-date">
  <time datetime="2023-07-25T16:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/26/eess.IV_2023_07_26/">eess.IV - 2023-07-26 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Artifact-Restoration-in-Histology-Images-with-Diffusion-Probabilistic-Models"><a href="#Artifact-Restoration-in-Histology-Images-with-Diffusion-Probabilistic-Models" class="headerlink" title="Artifact Restoration in Histology Images with Diffusion Probabilistic Models"></a>Artifact Restoration in Histology Images with Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14262">http://arxiv.org/abs/2307.14262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenqi-he/artifusion">https://github.com/zhenqi-he/artifusion</a></li>
<li>paper_authors: Zhenqi He, Junjun He, Jin Ye, Yiqing Shen</li>
<li>for:  histological whole slide images (WSIs) restoration</li>
<li>methods:  denoising diffusion probabilistic model (ArtiFusion) with novel Swin-Transformer denoising architecture and time token scheme</li>
<li>results:  effective restoration of artifact-free regions with preserved tissue structures and stain style, as demonstrated through extensive evaluations<details>
<summary>Abstract</summary>
Histological whole slide images (WSIs) can be usually compromised by artifacts, such as tissue folding and bubbles, which will increase the examination difficulty for both pathologists and Computer-Aided Diagnosis (CAD) systems. Existing approaches to restoring artifact images are confined to Generative Adversarial Networks (GANs), where the restoration process is formulated as an image-to-image transfer. Those methods are prone to suffer from mode collapse and unexpected mistransfer in the stain style, leading to unsatisfied and unrealistic restored images. Innovatively, we make the first attempt at a denoising diffusion probabilistic model for histological artifact restoration, namely ArtiFusion.Specifically, ArtiFusion formulates the artifact region restoration as a gradual denoising process, and its training relies solely on artifact-free images to simplify the training complexity.Furthermore, to capture local-global correlations in the regional artifact restoration, a novel Swin-Transformer denoising architecture is designed, along with a time token scheme. Our extensive evaluations demonstrate the effectiveness of ArtiFusion as a pre-processing method for histology analysis, which can successfully preserve the tissue structures and stain style in artifact-free regions during the restoration. Code is available at https://github.com/zhenqi-he/ArtiFusion.
</details>
<details>
<summary>摘要</summary>
历史图像整体扫描图像（WSIs）通常会受到artefact的影响，如组织卷绕和气泡，这会增加病理学家和计算机支持诊断（CAD）系统的评估难度。现有的恢复artefact图像方法是基于生成对抗网络（GANs），其中恢复过程是形式化为图像-图像传输。这些方法容易受到模式坍塌和意外传输的问题，导致 restored 图像不满意和不真实。我们在这里做出了一个尝试，提出了一种杂样整合模型，称为ArtiFusion。具体来说，ArtiFusion 将artefact区域恢复视为一个渐进的杂样去噪过程，其训练仅仅基于artefact-free 图像，以简化训练复杂性。此外，为了捕捉区域artefact恢复中的局部-全局相关性，我们设计了一种新的Swin-Transformer杂样去噪架构，以及一种时间токен方案。我们的广泛评估表明ArtiFusion 作为历史图像分析的预处理方法，可以成功保持组织结构和染料风格在artefact-free 区域中，并且可以成功恢复artefact。代码可以在 <https://github.com/zhenqi-he/ArtiFusion> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Visual-Saliency-Detection-in-Advanced-Driver-Assistance-Systems"><a href="#Visual-Saliency-Detection-in-Advanced-Driver-Assistance-Systems" class="headerlink" title="Visual Saliency Detection in Advanced Driver Assistance Systems"></a>Visual Saliency Detection in Advanced Driver Assistance Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03770">http://arxiv.org/abs/2308.03770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Rundo, Michael Sebastian Rundo, Concetto Spampinato</li>
<li>for: 这个研究旨在提供一种智能系统，用于评估司机的注意力水平和场景理解，以提高安全性。</li>
<li>methods: 该系统使用了 semantic segmentation 3D deep network，以及基于 PPG 信号的驾驶员睡眠状况检测。</li>
<li>results: 实验结果表明，该系统能够准确地评估司机的注意力水平和场景理解，并且可以提高安全性。<details>
<summary>Abstract</summary>
Visual Saliency refers to the innate human mechanism of focusing on and extracting important features from the observed environment. Recently, there has been a notable surge of interest in the field of automotive research regarding the estimation of visual saliency. While operating a vehicle, drivers naturally direct their attention towards specific objects, employing brain-driven saliency mechanisms that prioritize certain elements over others. In this investigation, we present an intelligent system that combines a drowsiness detection system for drivers with a scene comprehension pipeline based on saliency. To achieve this, we have implemented a specialized 3D deep network for semantic segmentation, which has been pretrained and tailored for processing the frames captured by an automotive-grade external camera. The proposed pipeline was hosted on an embedded platform utilizing the STA1295 core, featuring ARM A7 dual-cores, and embeds an hardware accelerator. Additionally, we employ an innovative biosensor embedded on the car steering wheel to monitor the driver drowsiness, gathering the PhotoPlethysmoGraphy (PPG) signal of the driver. A dedicated 1D temporal deep convolutional network has been devised to classify the collected PPG time-series, enabling us to assess the driver level of attentiveness. Ultimately, we compare the determined attention level of the driver with the corresponding saliency-based scene classification to evaluate the overall safety level. The efficacy of the proposed pipeline has been validated through extensive experimental results.
</details>
<details>
<summary>摘要</summary>
“视觉吸引力”指代人类在观察环境时自然地吸引注意力并提取重要特征。在汽车研究领域，最近有一场关注visual saliency的浪潮。驾驶时，司机会自然地将注意力集中在特定的对象上，使用大脑驱动的吸引力机制来优先级化元素。在这次研究中，我们提出了一个智能系统，其结合了驾驶员睡眠检测系统和基于吸引力的场景理解管道。为此，我们实施了一种特殊的3D深度网络 для semantic segmentation，该网络在自动汽车级外部摄像头捕捉的帧中进行了预训练和定制。我们的管道在使用STA1295核心的嵌入式平台上执行，该平台feature ARM A7双核心。此外，我们还使用了一种创新的车辙吸引监测系统，该系统通过捕捉驾驶员的PhotoPlethysmoGraphy（PPG）信号来监测驾驶员的睡眠状况。一个专门设计的1D时间深度卷积网络用于分类收集的PPG时间序列，以评估驾驶员的注意力水平。最后，我们将驾驶员确定的注意力水平与相应的吸引力基于场景分类进行比较，以评估整体安全水平。我们的实验结果表明，提案的管道具有良好的效果。
</details></li>
</ul>
<hr>
<h2 id="Non-Linear-Self-Augmentation-Deep-Pipeline-for-Cancer-Treatment-outcome-Prediction"><a href="#Non-Linear-Self-Augmentation-Deep-Pipeline-for-Cancer-Treatment-outcome-Prediction" class="headerlink" title="Non-Linear Self Augmentation Deep Pipeline for Cancer Treatment outcome Prediction"></a>Non-Linear Self Augmentation Deep Pipeline for Cancer Treatment outcome Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14398">http://arxiv.org/abs/2307.14398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Rundo, Concetto Spampinato, Michael Rundo</li>
<li>for: 这篇论文目的是提出一种新的预测治疗效果的策略，以帮助更好地选择和优化适合接受免疫治疗的患者。</li>
<li>methods: 该策略使用一种非线性细胞建筑和深入下渠分类器，从胸腹部CT图像提取和优化2D特征，以提高治疗效果预测的准确率。</li>
<li>results: 作者们通过一个具有推动力的实验研究，证明该策略的效果惊人，预测精度高达93%。<details>
<summary>Abstract</summary>
Immunotherapy emerges as promising approach for treating cancer. Encouraging findings have validated the efficacy of immunotherapy medications in addressing tumors, resulting in prolonged survival rates and notable reductions in toxicity compared to conventional chemotherapy methods. However, the pool of eligible patients for immunotherapy remains relatively small, indicating a lack of comprehensive understanding regarding the physiological mechanisms responsible for favorable treatment response in certain individuals while others experience limited benefits. To tackle this issue, the authors present an innovative strategy that harnesses a non-linear cellular architecture in conjunction with a deep downstream classifier. This approach aims to carefully select and enhance 2D features extracted from chest-abdomen CT images, thereby improving the prediction of treatment outcomes. The proposed pipeline has been meticulously designed to seamlessly integrate with an advanced embedded Point of Care system. In this context, the authors present a compelling case study focused on Metastatic Urothelial Carcinoma (mUC), a particularly aggressive form of cancer. Performance evaluation of the proposed approach underscores its effectiveness, with an impressive overall accuracy of approximately 93%
</details>
<details>
<summary>摘要</summary>
免疫疗法在肿瘤治疗中出现为可能的新方向。有力的证据证明免疫疗药在治疗肿瘤时的效果，导致了存活时间的延长和化学治疗方法相比的质量下降。然而，有效治疗的候选者人数仍然很小，这表明我们对治疗成功的生理机制还没有充分理解。为了解决这个问题，作者提出了一种创新的策略，利用非线性细胞体系和深入的下游分类器。这种方法的目的是通过精心选择和提高胸腹部CT图像中的2D特征，提高治疗结果预测的准确性。提案的管道已经仔细设计，能够与高级嵌入式Point of Care系统集成。在这个上下文中，作者提出了一个吸引人的案例研究，专门针对肿瘤肝癌（mUC）。研究表明，提案的方法的效果非常出色，总准确率大约为93%。
</details></li>
</ul>
<hr>
<h2 id="Tackling-Scattering-and-Reflective-Flare-in-Mobile-Camera-Systems-A-Raw-Image-Dataset-for-Enhanced-Flare-Removal"><a href="#Tackling-Scattering-and-Reflective-Flare-in-Mobile-Camera-Systems-A-Raw-Image-Dataset-for-Enhanced-Flare-Removal" class="headerlink" title="Tackling Scattering and Reflective Flare in Mobile Camera Systems: A Raw Image Dataset for Enhanced Flare Removal"></a>Tackling Scattering and Reflective Flare in Mobile Camera Systems: A Raw Image Dataset for Enhanced Flare Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14180">http://arxiv.org/abs/2307.14180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengbo Lan, Chang Wen Chen</li>
<li>for: 这个论文是为了提高移动设备的相机系统和图像质量，并解决激光和折射照明的问题。</li>
<li>methods: 这个论文使用了原始图像数据集，并评估了不同的移动设备和摄像头设定。</li>
<li>results: 该数据集包含了2,000个高品质的全分辨率原始图像对，并且可以进一步分解为30,000个对照图像 patches，以涵盖各种摄影状况。<details>
<summary>Abstract</summary>
The increasing prevalence of mobile devices has led to significant advancements in mobile camera systems and improved image quality. Nonetheless, mobile photography still grapples with challenging issues such as scattering and reflective flare. The absence of a comprehensive real image dataset tailored for mobile phones hinders the development of effective flare mitigation techniques. To address this issue, we present a novel raw image dataset specifically designed for mobile camera systems, focusing on flare removal. Capitalizing on the distinct properties of raw images, this dataset serves as a solid foundation for developing advanced flare removal algorithms. It encompasses a wide variety of real-world scenarios captured with diverse mobile devices and camera settings. The dataset comprises over 2,000 high-quality full-resolution raw image pairs for scattering flare and 1,100 for reflective flare, which can be further segmented into up to 30,000 and 2,200 paired patches, respectively, ensuring broad adaptability across various imaging conditions. Experimental results demonstrate that networks trained with synthesized data struggle to cope with complex lighting settings present in this real image dataset. We also show that processing data through a mobile phone's internal ISP compromises image quality while using raw image data presents significant advantages for addressing the flare removal problem. Our dataset is expected to enable an array of new research in flare removal and contribute to substantial improvements in mobile image quality, benefiting mobile photographers and end-users alike.
</details>
<details>
<summary>摘要</summary>
“由于移动设备的普及，移动摄像系统的技术和图像质量有了显著的进步。然而，移动摄影仍然面临着许多挑战，如散射和反射炙。由于没有专门为移动电话设计的全面真实图像集，因此发展效果的炙光除除射技术受到了限制。为解决这个问题，我们提供了一个新的Raw图像集，专门针对移动摄像系统，强调炙光除除。利用Raw图像的特点，这个集合作为开发高级炙光除除算法的坚实基础。它包括多种真实世界的场景，通过不同的移动设备和摄像设置捕捉。该集合包含了2,000多个高质量的全分辨率Raw图像对，用于散射炙，以及1,100个对，用于反射炙，可以进一步分解为30,000多个和2,200多个对的 patches。这确保了在多种捕捉条件下的广泛适用性。我们的实验结果表明，使用生成的数据进行训练的网络在真实图像集中表现不佳，而使用Raw图像数据具有显著的优势，能够有效地解决炙光除除问题。我们的集合预期会启动一系列新的研究，并为移动图像质量带来显著改善，对移动摄影者和用户都有利。”
</details></li>
</ul>
<hr>
<h2 id="Memory-Efficient-Graph-Convolutional-Networks-for-Object-Classification-and-Detection-with-Event-Cameras"><a href="#Memory-Efficient-Graph-Convolutional-Networks-for-Object-Classification-and-Detection-with-Event-Cameras" class="headerlink" title="Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras"></a>Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14124">http://arxiv.org/abs/2307.14124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamil Jeziorek, Andrea Pinna, Tomasz Kryjak</li>
<li>for: This paper focuses on developing an efficient graph convolutional network (GCN) for event camera data, which is characterized by high temporal resolution, high dynamic range, low latency, and resistance to image blur.</li>
<li>methods: The paper compares different graph convolution operations and evaluates their performance in terms of execution time, number of trainable model parameters, data format requirements, and training outcomes.</li>
<li>results: The paper achieves a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3%, which is 6.3% higher compared to the operation used in state-of-the-art approaches. Additionally, the object detection architecture implemented in the paper achieved an accuracy of 53.7% <a href="mailto:&#x6d;&#x41;&#80;&#x40;&#48;&#46;&#53;">&#x6d;&#x41;&#80;&#x40;&#48;&#46;&#53;</a> and an execution rate of 82 graphs per second on the N-Caltech101 dataset.Here’s the simplified Chinese text for the three key information points:</li>
<li>for: 这篇论文关注了使用图 convolutional network (GCN) 处理事件摄像头数据，该数据具有高时间分辨率、高动态范围、低延迟和图像模糊鲁棒性。</li>
<li>methods: 论文比较了不同的图 convolution 操作，并评估其在执行时间、可训练模型参数数量、数据格式要求和训练结果等方面的性能。</li>
<li>results: 论文实现了Feature extraction模块中参数数量减少450倍，数据表示形式减少4.5倍，保持52.3%的分类精度，比state-of-the-art方法高6.3%。此外，在 N-Caltech101 数据集上，实现了Object detection 架构，达到了53.7% <a href="mailto:&#109;&#x41;&#80;&#x40;&#48;&#x2e;&#x35;">&#109;&#x41;&#80;&#x40;&#48;&#x2e;&#x35;</a> 的准确率和82个图像每秒执行速率。<details>
<summary>Abstract</summary>
Recent advances in event camera research emphasize processing data in its original sparse form, which allows the use of its unique features such as high temporal resolution, high dynamic range, low latency, and resistance to image blur. One promising approach for analyzing event data is through graph convolutional networks (GCNs). However, current research in this domain primarily focuses on optimizing computational costs, neglecting the associated memory costs. In this paper, we consider both factors together in order to achieve satisfying results and relatively low model complexity. For this purpose, we performed a comparative analysis of different graph convolution operations, considering factors such as execution time, the number of trainable model parameters, data format requirements, and training outcomes. Our results show a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3%, which is 6.3% higher compared to the operation used in state-of-the-art approaches. To further evaluate performance, we implemented the object detection architecture and evaluated its performance on the N-Caltech101 dataset. The results showed an accuracy of 53.7 % mAP@0.5 and reached an execution rate of 82 graphs per second.
</details>
<details>
<summary>摘要</summary>
Recent advances in event camera research emphasize processing data in its original sparse form, which allows the use of its unique features such as high temporal resolution, high dynamic range, low latency, and resistance to image blur. One promising approach for analyzing event data is through graph convolutional networks (GCNs). However, current research in this domain primarily focuses on optimizing computational costs, neglecting the associated memory costs. In this paper, we consider both factors together in order to achieve satisfying results and relatively low model complexity. For this purpose, we performed a comparative analysis of different graph convolution operations, considering factors such as execution time, the number of trainable model parameters, data format requirements, and training outcomes. Our results show a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3%, which is 6.3% higher compared to the operation used in state-of-the-art approaches. To further evaluate performance, we implemented the object detection architecture and evaluated its performance on the N-Caltech101 dataset. The results showed an accuracy of 53.7 % mAP@0.5 and reached an execution rate of 82 graphs per second.  Translation:最近的事件摄像头研究进展强调处理原始稀缺数据，利用其特有的特征，如高时间分辨率、高动态范围、低延迟和图像模糊抗衰减。一种承袭的方法是使用图像会议网络（GCNs）来分析事件数据。然而，当前研究领域主要关注计算成本，忽略相关的内存成本。在这篇论文中，我们同时考虑这两个因素，以实现满意的结果和相对较低的模型复杂度。为此，我们对不同的图像会议操作进行比较分析，考虑因素包括执行时间、可训练模型参数数量、数据格式要求和训练结果。我们的结果显示，在特征提取模块中减少了450倍的参数数量，并在数据表示中减少了4.5倍的大小，同时保持了52.3%的分类精度，与现有方法相比增加6.3%。为了进一步评估性能，我们实现了对象检测架构，并在N-Caltech101数据集上评估其性能。结果显示，在0.5的MAP@53.7%的情况下，执行速率达到82个图像每秒。
</details></li>
</ul>
<hr>
<h2 id="Periocular-biometrics-databases-algorithms-and-directions"><a href="#Periocular-biometrics-databases-algorithms-and-directions" class="headerlink" title="Periocular biometrics: databases, algorithms and directions"></a>Periocular biometrics: databases, algorithms and directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14111">http://arxiv.org/abs/2307.14111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Alonso-Fernandez, Josef Bigun</li>
<li>for: 本文是一篇审查现有的周遭生物METRICS研究状况，提供了关于这一领域最重要的问题和现有文献的全面概述。</li>
<li>methods: 本文使用了周遭生物METRICS的多种方法，包括eyelids、lashes和eyebrows等特征提取方法，以及 gender classification和ethnicity classification等应用方法。</li>
<li>results: 本文提出了周遭生物METRICS的现状和未来发展趋势，并对现有文献进行了全面的概述和分析。<details>
<summary>Abstract</summary>
Periocular biometrics has been established as an independent modality due to concerns on the performance of iris or face systems in uncontrolled conditions. Periocular refers to the facial region in the eye vicinity, including eyelids, lashes and eyebrows. It is available over a wide range of acquisition distances, representing a trade-off between the whole face (which can be occluded at close distances) and the iris texture (which do not have enough resolution at long distances). Since the periocular region appears in face or iris images, it can be used also in conjunction with these modalities. Features extracted from the periocular region have been also used successfully for gender classification and ethnicity classification, and to study the impact of gender transformation or plastic surgery in the recognition performance. This paper presents a review of the state of the art in periocular biometric research, providing an insight of the most relevant issues and giving a thorough coverage of the existing literature. Future research trends are also briefly discussed.
</details>
<details>
<summary>摘要</summary>
《眼睛附近特征》已成为独立模式，由于肉眼或面部系统在不控制的情况下表现不佳。《眼睛附近特征》指的是脸部附近的眼睛区域，包括眼皮、毛发和毛发。它可以在各种获取距离范围内使用，表示距离整个脸部（可能会被近距离干扰）和眼球文字（没有足够的分辨率）之间的权衡。由于眼睛附近区域会出现在脸部或眼球图像中，因此它也可以与这些模式结合使用。从眼睛附近区域提取的特征已经成功地用于性别识别和种族识别，以及研究性别转换或整形手术对识别性的影响。这篇评论文章介绍了眼睛附近生物ometrics研究的现状，提供了最相关的问题的权衡和全面的文献评论。未来研究趋势也 briefly discussed。
</details></li>
</ul>
<hr>
<h2 id="Video-Decoding-Energy-Estimation-Using-Processor-Events"><a href="#Video-Decoding-Energy-Estimation-Using-Processor-Events" class="headerlink" title="Video Decoding Energy Estimation Using Processor Events"></a>Video Decoding Energy Estimation Using Processor Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14000">http://arxiv.org/abs/2307.14000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, André Kaup</li>
<li>for: 这篇论文是为了量化软件视频解码器的处理能量而写的。</li>
<li>methods: 该论文使用处理器事件如指令计数或缓存失败来准确估算软件视频解码器的处理能量。</li>
<li>results: 该论文通过对ARM基本评估平台进行能量测量，并使用专门的 profiling 软件对处理器事件进行计数，证明了我们的观察结果的一般可靠性。 用于不同的编码器和解码器实现，该方法可以准确地估算最新的视频编码标准HEVC和VP9中的真正解码能量，带有小于6%的平均估计误差。<details>
<summary>Abstract</summary>
In this paper, we show that processor events like instruction counts or cache misses can be used to accurately estimate the processing energy of software video decoders. Therefore, we perform energy measurements on an ARM-based evaluation platform and count processor level events using a dedicated profiling software. Measurements are performed for various codecs and decoder implementations to prove the general viability of our observations. Using the estimation method proposed in this paper, the true decoding energy for various recent video coding standards including HEVC and VP9 can be estimated with a mean estimation error that is smaller than 6%.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们证明处理器事件如指令计数或缓存miss可以准确地估计软件视频解码器的处理能量。因此，我们使用特定的 profiling 软件对ARM基本评估平台进行能量测量，并对不同的编码器和解码器实现进行测量。通过我们提出的估算方法，可以对最新的视频编码标准，包括HEVC和VP9，进行准确的估算， mean estimation error小于6%。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Representation-Enhanced-Sampling-for-Bayesian-Active-Learning-in-Musculoskeletal-Segmentation-of-Lower-Extremities"><a href="#Hybrid-Representation-Enhanced-Sampling-for-Bayesian-Active-Learning-in-Musculoskeletal-Segmentation-of-Lower-Extremities" class="headerlink" title="Hybrid Representation-Enhanced Sampling for Bayesian Active Learning in Musculoskeletal Segmentation of Lower Extremities"></a>Hybrid Representation-Enhanced Sampling for Bayesian Active Learning in Musculoskeletal Segmentation of Lower Extremities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13986">http://arxiv.org/abs/2307.13986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganping Li, Yoshito Otake, Mazen Soufi, Masashi Taniguchi, Masahide Yagi, Noriaki Ichihashi, Keisuke Uemura, Masaki Takao, Nobuhiko Sugano, Yoshinobu Sato<br>for: 降低医学图像分割任务中的手动标注成本methods: 使用 bayesian active learning 框架和 bayesian U-net，并采用混合表示性更新抽象策略，选择高密度和多样性的不确定样本进行手动修正，以优化最大化与未标注实例相似性，最小化与已有训练数据相似性。results: 在两个lower extremity（LE）数据集上，提出的方法与其他方法相比，在两种抽取规则下表现出超越或等效性，并且量化结果表明抽取规则的影响。我们的ablation study表明，将密度和多样性 criterion相结合使用，在musculoskeletal segmentation中表现出最佳性能。<details>
<summary>Abstract</summary>
Purpose: Obtaining manual annotations to train deep learning (DL) models for auto-segmentation is often time-consuming. Uncertainty-based Bayesian active learning (BAL) is a widely-adopted method to reduce annotation efforts. Based on BAL, this study introduces a hybrid representation-enhanced sampling strategy that integrates density and diversity criteria to save manual annotation costs by efficiently selecting the most informative samples.   Methods: The experiments are performed on two lower extremity (LE) datasets of MRI and CT images by a BAL framework based on Bayesian U-net. Our method selects uncertain samples with high density and diversity for manual revision, optimizing for maximal similarity to unlabeled instances and minimal similarity to existing training data. We assess the accuracy and efficiency using Dice and a proposed metric called reduced annotation cost (RAC), respectively. We further evaluate the impact of various acquisition rules on BAL performance and design an ablation study for effectiveness estimation.   Results: The proposed method showed superiority or non-inferiority to other methods on both datasets across two acquisition rules, and quantitative results reveal the pros and cons of the acquisition rules. Our ablation study in volume-wise acquisition shows that the combination of density and diversity criteria outperforms solely using either of them in musculoskeletal segmentation.   Conclusion: Our sampling method is proven efficient in reducing annotation costs in image segmentation tasks. The combination of the proposed method and our BAL framework provides a semi-automatic way for efficient annotation of medical image datasets.
</details>
<details>
<summary>摘要</summary>
目的：获取手动标注以训练深度学习（DL）模型的自动分割是经常占用时间。不确定性基于抽象学习（BAL）是一种广泛采用的方法，可以减少手动标注的努力。根据BAL，本研究引入了混合表示形式增强选择策略，将高密度和多样性的不确定样本选择为人工修改，以最大化与未标注实例的相似性，最小化与现有训练数据的相似性。方法：我们在两个下肢（LE）数据集上进行了MRI和CT图像的实验，使用基于抽象网络的BAL框架。我们的方法选择了不确定性高的高密度多样性样本进行人工修改，以优化最大化与未标注实例相似性，最小化与现有训练数据相似性。我们使用 dice和我们所提出的metriccalled减少注解成本（RAC）进行评价精度和效率。我们进一步evaluate了不同的获取规则对BAL性能的影响，并进行了效果鉴定研究。结果：我们的方法在两个数据集上表现出优异或等效于其他方法，并且量化结果表明了不同获取规则的优缺点。我们的拓展研究表明，将密度和多样性 критериria组合使用在musculoskeletal segmentation中表现出最佳效果。结论：我们的采样方法可以有效减少图像分割任务中的注解成本。将我们的采样方法与BAL框架结合使用，可以提供一种 semi-自动的方式，以便效率地注解医疗影像数据集。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Video-Quality-Datasets-via-Design-of-Minimalistic-Video-Quality-Models"><a href="#Analysis-of-Video-Quality-Datasets-via-Design-of-Minimalistic-Video-Quality-Models" class="headerlink" title="Analysis of Video Quality Datasets via Design of Minimalistic Video Quality Models"></a>Analysis of Video Quality Datasets via Design of Minimalistic Video Quality Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13981">http://arxiv.org/abs/2307.13981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Sun, Wen Wen, Xiongkuo Min, Long Lan, Guangtao Zhai, Kede Ma</li>
<li>for: 本研究旨在帮助理解现有的视频质量评估（VQA）数据集，以便更好地评估当前的视频质量评估模型。</li>
<li>methods: 本研究使用了最简单的视频质量评估模型，包括视频预处理（快速空间时间抑制）、空间质量分析器和选项性的时间质量分析器，以及质量回归器。</li>
<li>results: 研究发现大多数数据集受到容易的数据集问题的影响，一些甚至可以使用盲图质量评估（BIQA）解决方案。研究还比较了不同的模型变体在不同数据集上的性能，并对模型的设计决策进行了ablation研究。<details>
<summary>Abstract</summary>
Blind video quality assessment (BVQA) plays an indispensable role in monitoring and improving the end-users' viewing experience in various real-world video-enabled media applications. As an experimental field, the improvements of BVQA models have been measured primarily on a few human-rated VQA datasets. Thus, it is crucial to gain a better understanding of existing VQA datasets in order to properly evaluate the current progress in BVQA. Towards this goal, we conduct a first-of-its-kind computational analysis of VQA datasets via designing minimalistic BVQA models. By minimalistic, we restrict our family of BVQA models to build only upon basic blocks: a video preprocessor (for aggressive spatiotemporal downsampling), a spatial quality analyzer, an optional temporal quality analyzer, and a quality regressor, all with the simplest possible instantiations. By comparing the quality prediction performance of different model variants on eight VQA datasets with realistic distortions, we find that nearly all datasets suffer from the easy dataset problem of varying severity, some of which even admit blind image quality assessment (BIQA) solutions. We additionally justify our claims by contrasting our model generalizability on these VQA datasets, and by ablating a dizzying set of BVQA design choices related to the basic building blocks. Our results cast doubt on the current progress in BVQA, and meanwhile shed light on good practices of constructing next-generation VQA datasets and models.
</details>
<details>
<summary>摘要</summary>
视频质量评估（BVQA）在视频启用媒体应用中扮演了不可或缺的角色，负责监测和改进用户的观看体验。作为实验领域，BVQA模型的改进主要通过一些人类评分的视频质量评估 dataset（VQA dataset）进行评估。因此，更深刻地理解现有VQA dataset是非常重要的。为了实现这个目标，我们首次对VQA dataset进行了计算性的分析，通过设计最简的BVQA模型来评估现有VQA dataset的质量。我们的BVQA模型仅使用了基本块：视频预处理（为激进的空间时间采样）、空间质量分析器、可选的时间质量分析器和质量回归器，其中每个块都使用最简的实现。通过对不同模型变体在八个VQA dataset上的质量预测性能进行比较，我们发现大多数dataset受到不同程度的易于评估（Easy dataset problem），一些甚至可以使用盲图质量评估（BIQA）解决方案。此外，我们还对不同的BVQA设计选择进行了比较，并将其与不同的VQA dataset进行了对比。我们的结果表明，现有的BVQA进展并不如人们所期望的，同时也提供了构建下一代VQA dataset和模型的好做法。
</details></li>
</ul>
<hr>
<h2 id="A-real-time-material-breakage-detection-for-offshore-wind-turbines-based-on-improved-neural-network-algorithm"><a href="#A-real-time-material-breakage-detection-for-offshore-wind-turbines-based-on-improved-neural-network-algorithm" class="headerlink" title="A real-time material breakage detection for offshore wind turbines based on improved neural network algorithm"></a>A real-time material breakage detection for offshore wind turbines based on improved neural network algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13765">http://arxiv.org/abs/2307.13765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yantong Liu</li>
<li>for: 这种研究旨在提高海上风电机维护效率，为可持续能源生产做出重要贡献。</li>
<li>methods: 这种方法使用了一种改进后的YOLOv8物体检测模型，以及一个卷积杂志注意模块（CBAM），以提高特征识别能力。</li>
<li>results: 研究发现，使用这种方法可以提高杂点检测稳定性，代表了重要的维护技术突破。<details>
<summary>Abstract</summary>
The integrity of offshore wind turbines, pivotal for sustainable energy generation, is often compromised by surface material defects. Despite the availability of various detection techniques, limitations persist regarding cost-effectiveness, efficiency, and applicability. Addressing these shortcomings, this study introduces a novel approach leveraging an advanced version of the YOLOv8 object detection model, supplemented with a Convolutional Block Attention Module (CBAM) for improved feature recognition. The optimized loss function further refines the learning process. Employing a dataset of 5,432 images from the Saemangeum offshore wind farm and a publicly available dataset, our method underwent rigorous testing. The findings reveal a substantial enhancement in defect detection stability, marking a significant stride towards efficient turbine maintenance. This study's contributions illuminate the path for future research, potentially revolutionizing sustainable energy practices.
</details>
<details>
<summary>摘要</summary>
Offshore风电机的完整性，对可再生能源生产是关键的，但这些材料表面的瑕疵常常会对其产生影响。虽然有各种检测技术可供选择，但这些技术受到成本、效率和应用限制。本研究提出了一种新的方法，利用进化版YOLOv8物体检测模型，加上卷积块注意模块（CBAM），以提高特征识别。通过优化损失函数，进一步适应学习。使用了5432张采集自韩国岛风电农场和公开available的数据集，我们的方法经过了严格的测试。发现的结果表明，我们的方法可以增强瑕疵检测稳定性，代表了可再生能源实践中的一个重要突破。本研究的贡献，照明了未来研究的道路，有望革命化可再生能源实践。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/26/eess.IV_2023_07_26/" data-id="cllsj1rpc008dpf8835xhgmm3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/cs.SD_2023_07_25/" class="article-date">
  <time datetime="2023-07-24T16:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/25/cs.SD_2023_07_25/">cs.SD - 2023-07-25 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Snoring-Sound-Dataset-for-Body-Position-Recognition-Collection-Annotation-and-Analysis"><a href="#A-Snoring-Sound-Dataset-for-Body-Position-Recognition-Collection-Annotation-and-Analysis" class="headerlink" title="A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis"></a>A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13346">http://arxiv.org/abs/2307.13346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Xiao, Xiuping Yang, Xinhong Li, Weiping Tu, Xiong Chen, Weiyan Yi, Jie Lin, Yuhong Yang, Yanzhen Ren</li>
<li>for: 该研究旨在开发一个基于呼吸声的睡眠姿态识别数据集（SSBPR），以便在实际临床场景中识别睡眠姿态。</li>
<li>methods: 该研究使用了呼吸声的特征来识别睡眠姿态，并实验结果表明，呼吸声在实际场景中可以有效地描述睡眠姿态。</li>
<li>results: 该研究发现，呼吸声在实际场景中可以有效地描述睡眠姿态，并且提供了6种不同的睡眠姿态标签，包括躺正、左右躺头、左右躺身和躺躺等。<details>
<summary>Abstract</summary>
Obstructive Sleep Apnea-Hypopnea Syndrome (OSAHS) is a chronic breathing disorder caused by a blockage in the upper airways. Snoring is a prominent symptom of OSAHS, and previous studies have attempted to identify the obstruction site of the upper airways by snoring sounds. Despite some progress, the classification of the obstruction site remains challenging in real-world clinical settings due to the influence of sleep body position on upper airways. To address this challenge, this paper proposes a snore-based sleep body position recognition dataset (SSBPR) consisting of 7570 snoring recordings, which comprises six distinct labels for sleep body position: supine, supine but left lateral head, supine but right lateral head, left-side lying, right-side lying and prone. Experimental results show that snoring sounds exhibit certain acoustic features that enable their effective utilization for identifying body posture during sleep in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
《干扰性睡眠呼吸暂停-低血氧性呼吸综合症（OSAHS）》是一种慢性呼吸疾病，由上气道堵塞引起。呼吸声是OSAHS的一个显著的 симптом，前一些研究已经尝试了通过呼吸声来确定上气道堵塞的位置。然而，在实际临床 Settings中，这种分类仍然是挑战。为解决这个问题，本文提出了一个呼吸声基于睡姿识别数据集（SSBPR），包括7570个呼吸声记录，其中包括6种睡姿标签：躺平、左脊梁躺、右脊梁躺、左侧躺、右侧躺和躺背。实验结果表明，呼吸声具有一些音频特征，可以有效地用于在实际 scenarios 中识别睡姿。
</details></li>
</ul>
<hr>
<h2 id="On-Device-Speaker-Anonymization-of-Acoustic-Embeddings-for-ASR-based-onFlexible-Location-Gradient-Reversal-Layer"><a href="#On-Device-Speaker-Anonymization-of-Acoustic-Embeddings-for-ASR-based-onFlexible-Location-Gradient-Reversal-Layer" class="headerlink" title="On-Device Speaker Anonymization of Acoustic Embeddings for ASR based onFlexible Location Gradient Reversal Layer"></a>On-Device Speaker Anonymization of Acoustic Embeddings for ASR based onFlexible Location Gradient Reversal Layer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13343">http://arxiv.org/abs/2307.13343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Asif Jalal, Pablo Peso Parada, Jisi Zhang, Karthikeyan Saravanan, Mete Ozay, Myoungji Han, Jung In Lee, Seokyeong Jung</li>
<li>for: 这个论文是为了保护用户的隐私而设计的，具体来说是对于语音应用程序中传输私人用户资料（例如说话者身份）的问题。</li>
<li>methods: 这个论文提出了一个隐私增强框架，用于对语音识别中的说话者身份进行隐私化，并且保留语音识别的准确性。这个框架使用了梯度反转基于的话者敌方训练来对目标层中的语音嵌入进行隐私化。</li>
<li>results: 实验结果显示，这个方法可以有效地降低说话者身份识别的精度，将其降低至33%，并且提高语音识别的表现，实现6.2%的Relative Word Error Rate（WER）降低。<details>
<summary>Abstract</summary>
Smart devices serviced by large-scale AI models necessitates user data transfer to the cloud for inference. For speech applications, this means transferring private user information, e.g., speaker identity. Our paper proposes a privacy-enhancing framework that targets speaker identity anonymization while preserving speech recognition accuracy for our downstream task~-~Automatic Speech Recognition (ASR). The proposed framework attaches flexible gradient reversal based speaker adversarial layers to target layers within an ASR model, where speaker adversarial training anonymizes acoustic embeddings generated by the targeted layers to remove speaker identity. We propose on-device deployment by execution of initial layers of the ASR model, and transmitting anonymized embeddings to the cloud, where the rest of the model is executed while preserving privacy. Experimental results show that our method efficiently reduces speaker recognition relative accuracy by 33%, and improves ASR performance by achieving 6.2% relative Word Error Rate (WER) reduction.
</details>
<details>
<summary>摘要</summary>
智能设备由大规模AI模型服务需要用户数据传输到云端进行推理。对于语音应用程序来说，这意味着传输私人用户信息，例如说话者身份。我们的论文提出了一个隐私增强框架， targets speaker identity 隐蔽，保持语音识别精度downstream任务-自动语音识别（ASR）。我们的框架采用flexible gradient reversal基于说话者对抗层，对target层内的ASR模型中的说话者对抗训练，使得生成的声学嵌入被隐蔽，从而除去说话者身份。我们提议在设备上部署初始层，并将声学嵌入传输到云端，在保持隐私的情况下，执行余下的模型。实验结果表明，我们的方法可以高效地降低说话者识别精度 by 33%，并提高ASR性能by 6.2%的相对词错率（WER）减少。
</details></li>
</ul>
<hr>
<h2 id="CQNV-A-combination-of-coarsely-quantized-bitstream-and-neural-vocoder-for-low-rate-speech-coding"><a href="#CQNV-A-combination-of-coarsely-quantized-bitstream-and-neural-vocoder-for-low-rate-speech-coding" class="headerlink" title="CQNV: A combination of coarsely quantized bitstream and neural vocoder for low rate speech coding"></a>CQNV: A combination of coarsely quantized bitstream and neural vocoder for low rate speech coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13295">http://arxiv.org/abs/2307.13295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youqiang Zheng, Li Xiao, Weiping Tu, Yuhong Yang, Xinmeng Xu</li>
<li>for: 提高低比特率 speech 编码器的质量</li>
<li>methods: 提出一种新的框架，将传统参数编码器的均衡化量化参数与神经 vocoder 结合，以提高编码后的 speech 质量</li>
<li>results: 实验表明，提出的 CQNV 框架可以在 1.1 kbps 比特率下实现高质量的 reconstructed speech，比 Lyra 和 Encodec 在 3 kbps 比特率下的质量更高。<details>
<summary>Abstract</summary>
Recently, speech codecs based on neural networks have proven to perform better than traditional methods. However, redundancy in traditional parameter quantization is visible within the codec architecture of combining the traditional codec with the neural vocoder. In this paper, we propose a novel framework named CQNV, which combines the coarsely quantized parameters of a traditional parametric codec to reduce the bitrate with a neural vocoder to improve the quality of the decoded speech. Furthermore, we introduce a parameters processing module into the neural vocoder to enhance the application of the bitstream of traditional speech coding parameters to the neural vocoder, further improving the reconstructed speech's quality. In the experiments, both subjective and objective evaluations demonstrate the effectiveness of the proposed CQNV framework. Specifically, our proposed method can achieve higher quality reconstructed speech at 1.1 kbps than Lyra and Encodec at 3 kbps.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/cs.SD_2023_07_25/" data-id="cllsj1rny0047pf88g7i4ds5k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/cs.LG_2023_07_25/" class="article-date">
  <time datetime="2023-07-24T16:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/25/cs.LG_2023_07_25/">cs.LG - 2023-07-25 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-GPU-Approach-for-Training-of-Graph-ML-Models-on-large-CFD-Meshes"><a href="#Multi-GPU-Approach-for-Training-of-Graph-ML-Models-on-large-CFD-Meshes" class="headerlink" title="Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes"></a>Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13592">http://arxiv.org/abs/2307.13592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Strönisch, Maximilian Sander, Andreas Knüpfer, Marcus Meyer</li>
<li>for: 这篇论文的目的是提出一种基于 mesh 的数值方法，以快速预测流体动力学解。</li>
<li>methods: 本论文使用的方法包括 graph neural network 和 numerical solver，实现了分布式训练和 halo exchange。</li>
<li>results: 实验结果显示，提出的对 mesh 的 surrogate model 比 traditionally trained distributed model 来的预测精度较差。可能的解释、改进和未来方向都被讨论了。<details>
<summary>Abstract</summary>
Mesh-based numerical solvers are an important part in many design tool chains. However, accurate simulations like computational fluid dynamics are time and resource consuming which is why surrogate models are employed to speed-up the solution process. Machine Learning based surrogate models on the other hand are fast in predicting approximate solutions but often lack accuracy. Thus, the development of the predictor in a predictor-corrector approach is the focus here, where the surrogate model predicts a flow field and the numerical solver corrects it. This paper scales a state-of-the-art surrogate model from the domain of graph-based machine learning to industry-relevant mesh sizes of a numerical flow simulation. The approach partitions and distributes the flow domain to multiple GPUs and provides halo exchange between these partitions during training. The utilized graph neural network operates directly on the numerical mesh and is able to preserve complex geometries as well as all other properties of the mesh. The proposed surrogate model is evaluated with an application on a three dimensional turbomachinery setup and compared to a traditionally trained distributed model. The results show that the traditional approach produces superior predictions and outperforms the proposed surrogate model. Possible explanations, improvements and future directions are outlined.
</details>
<details>
<summary>摘要</summary>
mesh-based numerical solvers 是多个设计工具链中的重要部分。然而，如果需要精度的算法，例如计算流体力学，它们可能需要大量的时间和资源，因此使用代理模型来加速解决方案。机器学习基于的代理模型可以快速预测近似解决方案，但它们通常缺乏精度。因此，这里的焦点是发展一个预测器-修正器方法，其中的代理模型预测了流场，而numerical solver则修正它。在这篇文章中，我们将一个state-of-the-art的graph-based machine learning surrogate model scales down to industry-relevant mesh sizes of a numerical flow simulation.我们的方法包括 dividing and distributing the flow domain to multiple GPUs, and providing halo exchange between these partitions during training.在训练中，我们使用的graph neural network直接操作 numerical mesh，并能够保留 mesh 的复杂 геометрія和所有其他属性。我们将这个surrogate model评估于三维液体机构设置中，并与传统的分布式模型进行比较。结果显示，传统方法生成了更好的预测，并超越了我们的提案的代理模型。我们提出了可能的解释、改进和未来方向。
</details></li>
</ul>
<hr>
<h2 id="Settling-the-Sample-Complexity-of-Online-Reinforcement-Learning"><a href="#Settling-the-Sample-Complexity-of-Online-Reinforcement-Learning" class="headerlink" title="Settling the Sample Complexity of Online Reinforcement Learning"></a>Settling the Sample Complexity of Online Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13586">http://arxiv.org/abs/2307.13586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Zhang, Yuxin Chen, Jason D. Lee, Simon S. Du</li>
<li>for: 这篇论文的目的是解决在在线 reinforcement learning 中的数据效率问题。</li>
<li>methods: 这篇论文使用了修改后的幂值传播（MVP）算法，并提供了一种新的 regret 分解策略和分析方法来解决在线 RL 中的统计依赖关系问题。</li>
<li>results: 这篇论文证明了 Modified MVP 算法在具有 finite-horizon 不同状态的 Markov 决策过程中可以达到最优的 regret，并且不需要任何燃烧成本。此外，论文还提供了一种 PAC 样本复杂度（i.e., 每集需要的集数），这是最优的 для整个 $\varepsilon$-范围。<details>
<summary>Abstract</summary>
A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.   We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \begin{equation*}   \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax lower bound for the entire range of sample size $K\geq 1$, essentially eliminating any burn-in requirement. It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield $\varepsilon$-accuracy) of $\frac{SAH^3}{\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\varepsilon$-range.   Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances. The key technical innovation lies in the development of a new regret decomposition strategy and a novel analysis paradigm to decouple complicated statistical dependency -- a long-standing challenge facing the analysis of online RL in the sample-hungry regime.
</details>
<details>
<summary>摘要</summary>
在在线强化学习中，一个中心问题是数据效率。虽然一些最近的研究已经实现了 asymptotically 最小的 regret，但这些结果只在大样本Registry  regime 内是可靠的，需要巨大的燃烧成本来使算法运行最佳。如何在强化学习理论中实现 minimax-optimal regret 而无需燃烧成本是一个开放的问题。我们解决了这个问题，并且为 finite-horizon 不同状态 Markov decision process 提供了一个修改后的 Monotonic Value Propagation（MVP）算法，该算法可以在各种样本大小 $K\geq 1$ 的范围内实现 regret 的最小化。我们证明了这个 regret 的大小为 $(SAH^3K)^{-1} \min\left\{\sqrt{SAH^3K},HK\right\}$，其中 $S$ 是状态数量， $A$ 是动作数量， $H$ 是规划时间长度， $K$ 是总共计集数。这个 regret 与全范围内的最小下界匹配，实际上消除了所有燃烧需求。它还翻译到一个 PAC 样本复杂度（即需要多少集数来实现 $\varepsilon$-精度），该复杂度为 $\frac{SAH^3}{\varepsilon^2}$，其中 $\varepsilon$ 是精度标准。这个复杂度是 minimax-optimal 的 для整个 $\varepsilon$-范围。此外，我们还扩展了我们的理论，以探讨问题依赖的问题量，如优化值/成本和某些方差。我们的关键技术创新在于开发了一种新的 regret 分解策略和一种新的分析方法，以解决在线强化学习在样本匮乏 regime 中的复杂的统计依赖关系。
</details></li>
</ul>
<hr>
<h2 id="Piecewise-Linear-Functions-Representable-with-Infinite-Width-Shallow-ReLU-Neural-Networks"><a href="#Piecewise-Linear-Functions-Representable-with-Infinite-Width-Shallow-ReLU-Neural-Networks" class="headerlink" title="Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks"></a>Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14373">http://arxiv.org/abs/2307.14373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah McCarty</li>
<li>for: 本研究探讨了无穷宽深度学习模型下的连续割合函数表示。</li>
<li>methods: 该研究使用了矩阵分解和投影方法来表示连续割合函数。</li>
<li>results: 研究证明了 ONgie et al. 的假设，即任何连续割合函数都可以通过无穷宽深度学习模型表示。<details>
<summary>Abstract</summary>
This paper analyzes representations of continuous piecewise linear functions with infinite width, finite cost shallow neural networks using the rectified linear unit (ReLU) as an activation function. Through its integral representation, a shallow neural network can be identified by the corresponding signed, finite measure on an appropriate parameter space. We map these measures on the parameter space to measures on the projective $n$-sphere cross $\mathbb{R}$, allowing points in the parameter space to be bijectively mapped to hyperplanes in the domain of the function. We prove a conjecture of Ongie et al. that every continuous piecewise linear function expressible with this kind of infinite width neural network is expressible as a finite width shallow ReLU neural network.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文研究了无穷宽的抽象函数的表示，使用finite cost shallow neural network和Rectified Linear Unit（ReLU）作为激活函数。通过其 интеграル表示，一个抽象函数可以被相应的签名、finite measure标识在适当的参数空间上。我们将这些度量在参数空间映射到项目ive $n$-sphere cross $\mathbb{R}$上，使得参数空间中的点可以一一映射到函数的域中的hyperplane。我们证明了Ongie等人的 conjecture，即任何可表示为无穷宽 neural network的连续piecewise linear函数都可以表示为finite width shallow ReLU neural network。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Forward-and-Inverse-Design-Paradigms-A-Case-Study-on-Refractory-High-Entropy-Alloys"><a href="#Comparing-Forward-and-Inverse-Design-Paradigms-A-Case-Study-on-Refractory-High-Entropy-Alloys" class="headerlink" title="Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys"></a>Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13581">http://arxiv.org/abs/2307.13581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arindam Debnath, Lavanya Raman, Wenjie Li, Adam M. Krajewski, Marcia Ahn, Shuang Lin, Shunli Shang, Allison M. Beese, Zi-Kui Liu, Wesley F. Reinhart</li>
<li>for: 本研究的目标是比较前向和反向设计模型 paradigm的性能，以帮助更好地选择适合的设计方法。</li>
<li>methods: 本研究使用了两个 caso studies of refractory high-entropy alloy design，分别是用于评估不同的目标和约束。</li>
<li>results: 研究结果表明，反向设计模型在某些情况下可以更高效地找到满足目标性能的材料。<details>
<summary>Abstract</summary>
The rapid design of advanced materials is a topic of great scientific interest. The conventional, ``forward'' paradigm of materials design involves evaluating multiple candidates to determine the best candidate that matches the target properties. However, recent advances in the field of deep learning have given rise to the possibility of an ``inverse'' design paradigm for advanced materials, wherein a model provided with the target properties is able to find the best candidate. Being a relatively new concept, there remains a need to systematically evaluate how these two paradigms perform in practical applications. Therefore, the objective of this study is to directly, quantitatively compare the forward and inverse design modeling paradigms. We do so by considering two case studies of refractory high-entropy alloy design with different objectives and constraints and comparing the inverse design method to other forward schemes like localized forward search, high throughput screening, and multi objective optimization.
</details>
<details>
<summary>摘要</summary>
“高级材料的快速设计是科学领域中的一个热点话题。传统的“前进”模式的材料设计方法是评估多个候选者，以确定最佳符合目标性能的候选者。然而，近年来深度学习的发展使得“反向”设计模式在高级材料设计中得到了可能。这是一个相对较新的概念，因此还需要系统地评估这两种模式在实际应用中的性能。因此，本研究的目标是直接、量化地比较前进和反向设计模式的表现。我们通过考虑两个高级碎高 entropy合金的设计案例，每个案例都有不同的目标和约束，并与其他前进方案如本地前进搜索、高通过率检索和多目标优化进行比较。”
</details></li>
</ul>
<hr>
<h2 id="Reinterpreting-survival-analysis-in-the-universal-approximator-age"><a href="#Reinterpreting-survival-analysis-in-the-universal-approximator-age" class="headerlink" title="Reinterpreting survival analysis in the universal approximator age"></a>Reinterpreting survival analysis in the universal approximator age</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13579">http://arxiv.org/abs/2307.13579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdittmer/survival_analysis_sumo_plus_plus">https://github.com/sdittmer/survival_analysis_sumo_plus_plus</a></li>
<li>paper_authors: Sören Dittmer, Michael Roberts, Jacobus Preller, AIX COVNET, James H. F. Rudd, John A. D. Aston, Carola-Bibiane Schönlieb</li>
<li>for: 本研究旨在提供深度学习中应用生存分析的工具，以便充分发挥生存分析的潜力。</li>
<li>methods: 本文提出了一种新的损失函数、评价指标和第一个可 provin 的 universal approximating network，可以无需数值integation生成生存曲线。</li>
<li>results: 对于一个大规模的数据集，我们的损失函数和模型在与其他方法进行比较时表现出色，得到了更好的结果。<details>
<summary>Abstract</summary>
Survival analysis is an integral part of the statistical toolbox. However, while most domains of classical statistics have embraced deep learning, survival analysis only recently gained some minor attention from the deep learning community. This recent development is likely in part motivated by the COVID-19 pandemic. We aim to provide the tools needed to fully harness the potential of survival analysis in deep learning. On the one hand, we discuss how survival analysis connects to classification and regression. On the other hand, we provide technical tools. We provide a new loss function, evaluation metrics, and the first universal approximating network that provably produces survival curves without numeric integration. We show that the loss function and model outperform other approaches using a large numerical study.
</details>
<details>
<summary>摘要</summary>
生存分析是统计工具箱中的一个重要组成部分。然而，而most domains of classical statistics已经欢迎了深度学习，生存分析只дав一些小的注意力从深度学习社区。这种最近的发展可能是由COVID-19大流行所驱动的。我们想要提供需要抓住生存分析在深度学习中的潜力的工具。一方面，我们讨论了生存分析与分类和回归之间的连接。另一方面，我们提供技术工具。我们提供了一个新的损失函数，评估指标，以及第一个可以无数学 интегралы生成存生曲线的通用近似网络。我们展示了损失函数和模型在大量数字研究中表现出色。
</details></li>
</ul>
<hr>
<h2 id="PT-mathrm-L-p-Partial-Transport-mathrm-L-p-Distances"><a href="#PT-mathrm-L-p-Partial-Transport-mathrm-L-p-Distances" class="headerlink" title="PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances"></a>PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13571">http://arxiv.org/abs/2307.13571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinran Liu, Yikun Bai, Huy Tran, Zhanqi Zhu, Matthew Thorpe, Soheil Kolouri</li>
<li>for: 本研究旨在提出一种新的策略来比较通用信号，基于优化交通框架。</li>
<li>methods: 该研究使用优化交通距离作为比较通用信号的metric。</li>
<li>results: 研究人员提出了一种新的策略，可以快速比较通用信号，并且可以在信号分类和最近邻居分类中应用。<details>
<summary>Abstract</summary>
Optimal transport and its related problems, including optimal partial transport, have proven to be valuable tools in machine learning for computing meaningful distances between probability or positive measures. This success has led to a growing interest in defining transport-based distances that allow for comparing signed measures and, more generally, multi-channeled signals. Transport $\mathrm{L}^{p}$ distances are notable extensions of the optimal transport framework to signed and possibly multi-channeled signals. In this paper, we introduce partial transport $\mathrm{L}^{p}$ distances as a new family of metrics for comparing generic signals, benefiting from the robustness of partial transport distances. We provide theoretical background such as the existence of optimal plans and the behavior of the distance in various limits. Furthermore, we introduce the sliced variation of these distances, which allows for rapid comparison of generic signals. Finally, we demonstrate the application of the proposed distances in signal class separability and nearest neighbor classification.
</details>
<details>
<summary>摘要</summary>
优化运输和相关的问题，包括优化部分运输，在机器学习中证明是有价值的工具，用于计算概率或正的度量的意义。这一成功引起了对定义基于运输的距离，用于比较签名授权和、更一般地多渠道信号的兴趣。运输LP距离是优化运输框架的扩展，用于比较签名或多渠道信号。在这篇论文中，我们引入部分运输LP距离作为比较普通信号的新家族距离，受到部分运输距离的稳定性的启发。我们提供了理论背景，包括优化计划的存在和距离在不同的限制下的行为。此外，我们介绍了割辑变化的这些距离，允许快速比较普通信号。最后，我们示出了提议的距离在信号分类和最近邻居分类中的应用。
</details></li>
</ul>
<hr>
<h2 id="Introducing-Hybrid-Modeling-with-Time-series-Transformers-A-Comparative-Study-of-Series-and-Parallel-Approach-in-Batch-Crystallization"><a href="#Introducing-Hybrid-Modeling-with-Time-series-Transformers-A-Comparative-Study-of-Series-and-Parallel-Approach-in-Batch-Crystallization" class="headerlink" title="Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization"></a>Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05749">http://arxiv.org/abs/2308.05749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranjan Sitapure, Joseph S Kwon</li>
<li>for: 这种研究旨在开发一种基于物理学原理和机器学习模型的混合模型，以提高批凝聚过程的预测性能和可 interpretability。</li>
<li>methods: 该研究使用了注意力基于时间序列变换器（TST），并结合了多头注意力机制和位置编码来捕捉长期和短期变化的过程状态。</li>
<li>results: 研究发现，使用 TST 基于混合模型可以提高批凝聚过程的预测性能和可 interpretability，并且可以提供更好的解释性。两种不同的配置（系列和平行）的 TST 基于混合模型都被构建和比较，得到了 $[10, 50]\times10^{-4}$ 的 normalized-mean-square-error 和 $R^2$ 值大于 0.99。<details>
<summary>Abstract</summary>
Most existing digital twins rely on data-driven black-box models, predominantly using deep neural recurrent, and convolutional neural networks (DNNs, RNNs, and CNNs) to capture the dynamics of chemical systems. However, these models have not seen the light of day, given the hesitance of directly deploying a black-box tool in practice due to safety and operational issues. To tackle this conundrum, hybrid models combining first-principles physics-based dynamics with machine learning (ML) models have increased in popularity as they are considered a 'best of both worlds' approach. That said, existing simple DNN models are not adept at long-term time-series predictions and utilizing contextual information on the trajectory of the process dynamics. Recently, attention-based time-series transformers (TSTs) that leverage multi-headed attention mechanism and positional encoding to capture long-term and short-term changes in process states have shown high predictive performance. Thus, a first-of-a-kind, TST-based hybrid framework has been developed for batch crystallization, demonstrating improved accuracy and interpretability compared to traditional black-box models. Specifically, two different configurations (i.e., series and parallel) of TST-based hybrid models are constructed and compared, which show a normalized-mean-square-error (NMSE) in the range of $[10, 50]\times10^{-4}$ and an $R^2$ value over 0.99. Given the growing adoption of digital twins, next-generation attention-based hybrid models are expected to play a crucial role in shaping the future of chemical manufacturing.
</details>
<details>
<summary>摘要</summary>
现有的数字双胞虫多数采用数据驱动的黑盒模型，主要使用深度神经环境网络（DNNs）、循环神经网络（RNNs）和卷积神经网络（CNNs）来捕捉化学系统的动态。然而，这些模型尚未得到实际应用，因为安全和运营问题。为解决这个问题，将物理基础知识与机器学习（ML）模型结合的混合模型在 популяр度上升。然而，现有的简单DNN模型不具备长期时间序列预测和利用过程动态趋势的能力。在此基础之上，具有多头注意力机制和位置编码的时间序列变换器（TSTs）在过程状态中捕捉长期和短期变化，并显示出高预测性能。因此，一种首次创造的TST-基于混合框架在批凝聚中得到了应用，并实现了传统黑盒模型的改进准确性和可读性。具体来说，这种混合模型的两种不同配置（即 série和平行）在NMSE范围内为[$10, 50]乘10^-4，$R^2$值高于0.99。随着数字双胞虫的推广，下一代注意力基本的混合模型将在化学制造中扮演关键的角色。
</details></li>
</ul>
<hr>
<h2 id="Decision-Focused-Learning-Foundations-State-of-the-Art-Benchmark-and-Future-Opportunities"><a href="#Decision-Focused-Learning-Foundations-State-of-the-Art-Benchmark-and-Future-Opportunities" class="headerlink" title="Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities"></a>Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13565">http://arxiv.org/abs/2307.13565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/predopt/predopt-benchmarks">https://github.com/predopt/predopt-benchmarks</a></li>
<li>paper_authors: Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey, Tias Guns, Ferdinando Fioretto</li>
<li>for: 这篇论文旨在提供一个全面的决策学习（DFL）综述，涵盖了不同的技术和方法，以及适用于决策模型的优化。</li>
<li>methods: 本文提出了一种新的决策学习方法，即结合预测和优化的综合系统，以解决在不确定环境下的决策问题。它还提出了一种分类DFL方法，并进行了广泛的实验评估。</li>
<li>results: 本文的实验结果表明，DFL方法可以提高决策模型的准确率和稳定性，并且可以应用于多种实际应用场景。同时，本文还提供了一些现有和未来决策学习研究的可能性。<details>
<summary>Abstract</summary>
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models, introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
</details>
<details>
<summary>摘要</summary>
决策关注学习（DFL）是一种emerging paradigm在机器学习领域，它将模型训练为优化决策，同时整合预测和优化在整个端到端系统中。这种 paradigm 承诺可以革命化决策在不确定环境中的很多实际应用中，因为在决策模型中未知参数的估计通常成为了一个重大障碍。本文提供了DFL的全面评论，包括了不同方法的整合机器学习和优化模型的深入分析，并提出了DFL方法的分类，以及对这些方法的广泛的实验评估。最后，这种研究提供了有价值的对DFL研究的当前和未来方向的洞察。
</details></li>
</ul>
<hr>
<h2 id="Node-Injection-Link-Stealing-Attack"><a href="#Node-Injection-Link-Stealing-Attack" class="headerlink" title="Node Injection Link Stealing Attack"></a>Node Injection Link Stealing Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13548">http://arxiv.org/abs/2307.13548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oualid Zari, Javier Parra-Arnau, Ayşe Ünsal, Melek Önen</li>
<li>for: 本研究探讨了图神经网络（GNNs）中隐私漏洞的攻击方法，以及如何保护隐私的方法。</li>
<li>methods: 我们提出了一种隐蔽和有效的攻击方法，可以在图数据中推断私有链接。我们还提出了一种使用差分隐私（DP）机制来减轻攻击的影响。</li>
<li>results: 我们的攻击方法在推断私有链接方面表现出色，比前一个状态的方法更高效。我们还分析了采用DP机制来保护隐私的代价和利用性。<details>
<summary>Abstract</summary>
In this paper, we present a stealthy and effective attack that exposes privacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private links within graph-structured data. Focusing on the inductive setting where new nodes join the graph and an API is used to query predictions, we investigate the potential leakage of private edge information. We also propose methods to preserve privacy while maintaining model utility. Our attack demonstrates superior performance in inferring the links compared to the state of the art. Furthermore, we examine the application of differential privacy (DP) mechanisms to mitigate the impact of our proposed attack, we analyze the trade-off between privacy preservation and model utility. Our work highlights the privacy vulnerabilities inherent in GNNs, underscoring the importance of developing robust privacy-preserving mechanisms for their application.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种隐蔽的和有效的攻击，暴露了图神经网络（GNNs）中的隐私泄露问题。我们将注意力集中在新节点加入图时的拟合设定下，并使用 API 来查询预测结果。我们研究了新节点加入图时可能泄露的私有边信息的可能性。此外，我们还提出了保持隐私的方法，以保持模型的实用性。我们的攻击表现出色，在推断链接方面超过了现有的状态。此外，我们还研究了应用 differential privacy（DP）机制来减轻我们提出的攻击的影响，并分析了隐私保护和模型实用性之间的交易。我们的工作强调了 GNNs 中的隐私泄露问题，并高亮了开发Robust隐私保护机制的重要性。
</details></li>
</ul>
<hr>
<h2 id="Transfer-Learning-for-Portfolio-Optimization"><a href="#Transfer-Learning-for-Portfolio-Optimization" class="headerlink" title="Transfer Learning for Portfolio Optimization"></a>Transfer Learning for Portfolio Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13546">http://arxiv.org/abs/2307.13546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyang Cao, Haotian Gu, Xin Guo, Mathieu Rosenbaum</li>
<li>for: 本研究探讨了通过转移学习技术解决金融股票优化问题的可能性。</li>
<li>methods: 本研究引入了一种新的概念 called “转移风险”，并在转移学习优化框架中进行了一系列的数值实验。</li>
<li>results: 实验结果显示，转移风险与转移学习方法的总性表现之间存在强相关关系，并且可以作为”转移可能性”的可靠指标；转移风险可以提供一种计算效率高的方法来选择合适的源任务；实验还为跨大陆、跨领域和跨频率的股票管理提供了有价值的新视角。<details>
<summary>Abstract</summary>
In this work, we explore the possibility of utilizing transfer learning techniques to address the financial portfolio optimization problem. We introduce a novel concept called "transfer risk", within the optimization framework of transfer learning. A series of numerical experiments are conducted from three categories: cross-continent transfer, cross-sector transfer, and cross-frequency transfer. In particular, 1. a strong correlation between the transfer risk and the overall performance of transfer learning methods is established, underscoring the significance of transfer risk as a viable indicator of "transferability"; 2. transfer risk is shown to provide a computationally efficient way to identify appropriate source tasks in transfer learning, enhancing the efficiency and effectiveness of the transfer learning approach; 3. additionally, the numerical experiments offer valuable new insights for portfolio management across these different settings.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们探讨了使用传输学习技术来解决金融投资优化问题的可能性。我们引入了一新的概念called "传输风险"，并在传输学习优化框架中进行了数学实验。我们从三类实验中进行了数值实验：跨洲传输、跨领域传输和跨频传输。具体来说，我们发现了以下结论：1. 传输风险和传输学习方法的总性表现之间存在强相关性，这说明了传输风险的重要性作为"传输可用性"的指标。2. 传输风险提供了一种计算效率高的方法来选择合适的源任务，从而提高了传输学习方法的效率和效果。3. 数值实验还提供了价值的新意见 для资产管理在不同的设置下。
</details></li>
</ul>
<hr>
<h2 id="A-model-for-efficient-dynamical-ranking-in-networks"><a href="#A-model-for-efficient-dynamical-ranking-in-networks" class="headerlink" title="A model for efficient dynamical ranking in networks"></a>A model for efficient dynamical ranking in networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13544">http://arxiv.org/abs/2307.13544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Della Vecchia, Kibidi Neocosmos, Daniel B. Larremore, Cristopher Moore, Caterina De Bacco</li>
<li>for: 这个论文的目的是用物理学约束来推断 directive temporal networks 中每个节点的动态排名。</li>
<li>methods: 这个论文使用的方法是解一个线性方程组，需要一个参数调整，这使得算法可扩展和高效。</li>
<li>results: 这个论文的实验表明，这种方法可以更好地预测 directive temporal networks 中节点之间的互动和互动结果，在许多情况下比现有的方法表现更好。<details>
<summary>Abstract</summary>
We present a physics-inspired method for inferring dynamic rankings in directed temporal networks - networks in which each directed and timestamped edge reflects the outcome and timing of a pairwise interaction. The inferred ranking of each node is real-valued and varies in time as each new edge, encoding an outcome like a win or loss, raises or lowers the node's estimated strength or prestige, as is often observed in real scenarios including sequences of games, tournaments, or interactions in animal hierarchies. Our method works by solving a linear system of equations and requires only one parameter to be tuned. As a result, the corresponding algorithm is scalable and efficient. We test our method by evaluating its ability to predict interactions (edges' existence) and their outcomes (edges' directions) in a variety of applications, including both synthetic and real data. Our analysis shows that in many cases our method's performance is better than existing methods for predicting dynamic rankings and interaction outcomes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种物理学把握的方法，用于推断导向时间网络中的动态排名。这些网络包含导向时间排序的边，每个边都反映了双方交互的结果和时间。推断每个节点的排名是实数，随着新的边加入或取消，每个节点的估计强度或威望会上升或下降，这是实际情况中常见的。我们的方法是解决一个线性系统方程，只需要一个参数调整，因此算法可扩展和高效。我们对各种应用，包括 sintetic 数据和实际数据进行测试，分析结果表明，在许多情况下，我们的方法的表现更好于现有的动态排名预测方法。
</details></li>
</ul>
<hr>
<h2 id="Model-Calibration-in-Dense-Classification-with-Adaptive-Label-Perturbation"><a href="#Model-Calibration-in-Dense-Classification-with-Adaptive-Label-Perturbation" class="headerlink" title="Model Calibration in Dense Classification with Adaptive Label Perturbation"></a>Model Calibration in Dense Classification with Adaptive Label Perturbation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13539">http://arxiv.org/abs/2307.13539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carlisle-liu/aslp">https://github.com/carlisle-liu/aslp</a></li>
<li>paper_authors: Jiawei Liu, Changkun Ye, Shan Wang, Ruikai Cui, Jing Zhang, Kaihao Zhang, Nick Barnes</li>
<li>for: 本研究旨在提高深度神经网络的可靠性和可信度，以便在安全应用中提供可靠的预测结果。</li>
<li>methods: 该研究提出了自适应杂化标签噪声（ASLP）方法，该方法learns一个专门的标签噪声水平 для每个训练图像。ASLP使用了我们提出的自适应二元抽象（SC-BCE）损失函数，该函数整合了标签噪声过程，包括杂化标签（DisturbLabel）和标签平滑，以 corrected calibration while maintaining classification rates。</li>
<li>results: 对于 dense binary classification 模型，ASLP可以显著提高模型的可靠性和可信度度。extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data.<details>
<summary>Abstract</summary>
For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model calibration degree by minimising the gap between the prediction accuracy and expected confidence of the target training label. Extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data. The code is available on https://github.com/Carlisle-Liu/ASLP.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="INFINITY-Neural-Field-Modeling-for-Reynolds-Averaged-Navier-Stokes-Equations"><a href="#INFINITY-Neural-Field-Modeling-for-Reynolds-Averaged-Navier-Stokes-Equations" class="headerlink" title="INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations"></a>INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13538">http://arxiv.org/abs/2307.13538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Serrano, Leon Migus, Yuan Yin, Jocelyn Ahmed Mazari, Patrick Gallinari</li>
<li>for: 用于数值设计中实现高效和准确的代理模型，以便简化直接数值计算的计算负担。</li>
<li>methods: 我们提出了一种基于深度学习的含义 Neil 表示法（INFINITY），该方法可以将几何信息和物理场景编码成紧凑的表示中，并学习一个映射来从几何信息中推断物理场景。</li>
<li>results: 我们在使用 AirfRANS 数据集进行了实验，并证明了我们的方法可以在实际工业应用中达到状态体arc Performance，并且可以准确预测气动和摩擦势量。<details>
<summary>Abstract</summary>
For numerical design, the development of efficient and accurate surrogate models is paramount. They allow us to approximate complex physical phenomena, thereby reducing the computational burden of direct numerical simulations. We propose INFINITY, a deep learning model that utilizes implicit neural representations (INRs) to address this challenge. Our framework encodes geometric information and physical fields into compact representations and learns a mapping between them to infer the physical fields. We use an airfoil design optimization problem as an example task and we evaluate our approach on the challenging AirfRANS dataset, which closely resembles real-world industrial use-cases. The experimental results demonstrate that our framework achieves state-of-the-art performance by accurately inferring physical fields throughout the volume and surface. Additionally we demonstrate its applicability in contexts such as design exploration and shape optimization: our model can correctly predict drag and lift coefficients while adhering to the equations.
</details>
<details>
<summary>摘要</summary>
为numerical设计，效率和准确的代表模型的开发是关键。它们允许我们近似复杂的物理现象，从而降低直接数值计算的计算卷积。我们提出了INFINITY，一种基于深度学习的模型，利用隐藏 нейрон表示（INRs）解决这个挑战。我们的框架将几何信息和物理场所编码为紧凑的表示，并学习它们之间的映射，以推断物理场。我们使用了一个风洞设计优化问题作为示例任务，并在实际工业用例的AirfRANS数据集上评估我们的方法。实验结果表明，我们的框架可以达到当前最佳性能，准确地推断物理场所在体积和表面上。此外，我们还证明了其在设计探索和形状优化等上的可应用性：我们的模型可以正确地预测拖拽和扩散减弱系数，并遵循方程。
</details></li>
</ul>
<hr>
<h2 id="Do-algorithms-and-barriers-for-sparse-principal-component-analysis-extend-to-other-structured-settings"><a href="#Do-algorithms-and-barriers-for-sparse-principal-component-analysis-extend-to-other-structured-settings" class="headerlink" title="Do algorithms and barriers for sparse principal component analysis extend to other structured settings?"></a>Do algorithms and barriers for sparse principal component analysis extend to other structured settings?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13535">http://arxiv.org/abs/2307.13535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanyi Wang, Mengqi Lou, Ashwin Pananjady</li>
<li>for: 这个论文研究了一个主成分分析问题，该问题采用杂散矩阵模型， captured by a class of union-of-subspace models.</li>
<li>methods: 这篇论文使用了一种自然的投影力方法，并证明了这种方法在统计上几乎最佳的邻居解的局部归一化。</li>
<li>results: 研究结果显示，许多vanilla sparse PCA的现象也出现在其结构化版本中，并且提供了特定的初始化方法和计算困难性证明。<details>
<summary>Abstract</summary>
We study a principal component analysis problem under the spiked Wishart model in which the structure in the signal is captured by a class of union-of-subspace models. This general class includes vanilla sparse PCA as well as its variants with graph sparsity. With the goal of studying these problems under a unified statistical and computational lens, we establish fundamental limits that depend on the geometry of the problem instance, and show that a natural projected power method exhibits local convergence to the statistically near-optimal neighborhood of the solution. We complement these results with end-to-end analyses of two important special cases given by path and tree sparsity in a general basis, showing initialization methods and matching evidence of computational hardness. Overall, our results indicate that several of the phenomena observed for vanilla sparse PCA extend in a natural fashion to its structured counterparts.
</details>
<details>
<summary>摘要</summary>
我们研究了一个主成分分析问题，该问题采用折衣Wishart模型， captured by a class of union-of-subspace models。这个总类包括折衣稀畴PCA和其变体具有图稀畴。为了在统一的统计和计算镜头下研究这些问题，我们确定了基本的限制，这些限制取决于问题实例的几何结构，并证明了一种自然的投影力方法在本地具有Statistically near-optimal neighborhood的 converges。我们还补充了一些特殊情况的末端分析，包括路径和树稀畴在一般基础上，并提供了初始化方法和匹配证明的计算困难。总之，我们的结果表明，许多vanilla sparse PCA中观察到的现象都可以自然地推广到其结构化对应物。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Turbulence-II"><a href="#Differentiable-Turbulence-II" class="headerlink" title="Differentiable Turbulence II"></a>Differentiable Turbulence II</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13533">http://arxiv.org/abs/2307.13533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varun Shankar, Romit Maulik, Venkatasubramanian Viswanathan</li>
<li>for: 这篇论文是为了开发数据驱动模型而写的，它利用了可微分流体模拟器来提高计算流体动力学（CFD）的效率。</li>
<li>methods: 这篇论文使用了可微分流体模拟器，并将机器学习（ML）模型 embed 到 CFD 解决方案中，以捕捉流体动力学的普遍性和先前投入成本的优势，同时具有深度学习方法的灵活性和自动训练能力。</li>
<li>results: 该方法可以在多种倾斜 backwards-facing step 中学习 sub-grid 规模 closure，并在不同的 Reynolds 数和新的几何中进行测试。结果显示，学习 closure 可以达到相当于大流体 simulate 的精度，并且在更粗细的网格上进行测试，相当于提高 10 倍的速度。<details>
<summary>Abstract</summary>
Differentiable fluid simulators are increasingly demonstrating value as useful tools for developing data-driven models in computational fluid dynamics (CFD). Differentiable turbulence, or the end-to-end training of machine learning (ML) models embedded in CFD solution algorithms, captures both the generalization power and limited upfront cost of physics-based simulations, and the flexibility and automated training of deep learning methods. We develop a framework for integrating deep learning models into a generic finite element numerical scheme for solving the Navier-Stokes equations, applying the technique to learn a sub-grid scale closure using a multi-scale graph neural network. We demonstrate the method on several realizations of flow over a backwards-facing step, testing on both unseen Reynolds numbers and new geometry. We show that the learned closure can achieve accuracy comparable to traditional large eddy simulation on a finer grid that amounts to an equivalent speedup of 10x. As the desire and need for cheaper CFD simulations grows, we see hybrid physics-ML methods as a path forward to be exploited in the near future.
</details>
<details>
<summary>摘要</summary>
“可微分流体模拟器在计算流体动力学（CFD）中越来越显示其用于开发数据驱动模型的价值。可微分湍流，或者将机器学习（ML）模型embedded在CFD解决方案算法中的末端训练，捕捉了物理基础的泛化力和入口成本的限制，以及深度学习方法的自动化训练和灵活性。我们开发了一个整合深度学习模型的普通finite element数学模型，用于解决奈尔-斯托克方程，并应用该技术来学习一个子网格规模 closure。我们在多个流过 backwards-facing step的实现中测试了该方法，并在未看过Reynolds数和新的几何结构上进行测试。我们显示了学习 closure可以达到与传统大涨流动 simulations相同的准确性，并且在较细的网格上运行，相当于提高10倍的速度。随着计算CFD simulations的成本下降的需求和需求增长，我们看到 hybrid physics-ML 方法作为未来的发展道路。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Long-Term-predictions-of-Turbulence-using-Neural-Operators"><a href="#Towards-Long-Term-predictions-of-Turbulence-using-Neural-Operators" class="headerlink" title="Towards Long-Term predictions of Turbulence using Neural Operators"></a>Towards Long-Term predictions of Turbulence using Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13517">http://arxiv.org/abs/2307.13517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Gonzalez, François-Xavier Demoulin, Simon Bernard</li>
<li>for: 这个论文探讨了使用神经操作来预测湍流，特点是使用傅里尔神经操作（FNO）模型。它的目标是通过机器学习开发减少的&#x2F;代理模型来进行湍流计算模拟。</li>
<li>methods: 这篇论文使用了不同的模型配置，包括UNO和U-FNET结构。与标准FNO相比，这些结构在准确性和稳定性方面表现更好。U-FNET在更高的 Reynolds 数下预测湍流表现出色。</li>
<li>results: 研究发现，迁移损失和稳定损失等正则项是神经网络模型稳定和准确预测的关键。此外，深度学习模型在液体流体预测中需要更多的测试和评估。<details>
<summary>Abstract</summary>
This paper explores Neural Operators to predict turbulent flows, focusing on the Fourier Neural Operator (FNO) model. It aims to develop reduced-order/surrogate models for turbulent flow simulations using Machine Learning. Different model configurations are analyzed, with U-NET structures (UNO and U-FNET) performing better than the standard FNO in accuracy and stability. U-FNET excels in predicting turbulence at higher Reynolds numbers. Regularization terms, like gradient and stability losses, are essential for stable and accurate predictions. The study emphasizes the need for improved metrics for deep learning models in fluid flow prediction. Further research should focus on models handling complex flows and practical benchmarking metrics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Empirical-Study-on-Fairness-Improvement-with-Multiple-Protected-Attributes"><a href="#An-Empirical-Study-on-Fairness-Improvement-with-Multiple-Protected-Attributes" class="headerlink" title="An Empirical Study on Fairness Improvement with Multiple Protected Attributes"></a>An Empirical Study on Fairness Improvement with Multiple Protected Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01923">http://arxiv.org/abs/2308.01923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Mark Harman</li>
<li>For: The paper aims to improve the fairness of Machine Learning (ML) software regarding multiple protected attributes, as existing research has primarily focused on a single protected attribute.* Methods: The paper conducts an extensive study of fairness improvement methods for multiple protected attributes, covering 11 state-of-the-art methods and analyzing their effectiveness with different datasets, metrics, and ML models.* Results: The results show that improving fairness for a single protected attribute can lead to a decrease in fairness regarding unconsidered protected attributes, with a decrease observed in up to 88.3% of scenarios (57.5% on average). Additionally, the paper finds that accuracy can be maintained in the multiple-attribute paradigm, but precision and recall are affected, with a 5-8 times increase in impact compared to a single attribute.Here are the three points in Simplified Chinese text:* For: 本研究旨在提高机器学习软件中保护属性的多个保护属性的公平性，因为现有研究主要关注单个保护属性。* Methods: 本研究对多保护属性公平性提升方法进行了广泛的研究和分析，覆盖了11种现状最佳实践方法，并对不同的数据集、指标和机器学习模型进行了分析。* Results: 结果显示，为单个保护属性提升公平性可能会导致其他保护属性的不公平性下降，下降的情况在88.3%的场景中出现（平均为57.5%）。此外，研究发现，在多属性情况下，维持精度的可能性很高，但是精度和准确率在考虑多个保护属性时受到影响，影响的程度约为5-8倍。<details>
<summary>Abstract</summary>
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling multiple protected attributes is about 5 times and 8 times that of a single attribute. This has important implications for future fairness research: reporting only accuracy as the ML performance metric, which is currently common in the literature, is inadequate.
</details>
<details>
<summary>摘要</summary>
现有研究主要是在一个保护属性上提高机器学习软件的公平性，但这是不现实的，因为多个用户有多个保护属性。这篇论文进行了详细的多个保护属性公平性改进研究，涵盖11种现状最佳实践的公平性改进方法。我们分析了不同的数据集、指标和机器学习模型下这些方法的效果，并发现在考虑多个保护属性时，改进公平性可能会导致未考虑的保护属性上的公平性下降。这种下降的比例在88.3%的场景中（57.5%的平均值）。更意外的是，考虑单个和多个保护属性时的准确性损失几乎没有差异，这表示在多属性 paradigm 中，准确性可以被维持。然而，处理多个保护属性时对精度和回归的影响是单个属性的5倍和8倍。这有重要的意义 для未来的公平研究：现在在 литераature 中通常只是Reporting 准确性作为机器学习性能指标，这是不充分的。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Time-Evidential-Distributions-for-Irregular-Time-Series"><a href="#Continuous-Time-Evidential-Distributions-for-Irregular-Time-Series" class="headerlink" title="Continuous Time Evidential Distributions for Irregular Time Series"></a>Continuous Time Evidential Distributions for Irregular Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13503">http://arxiv.org/abs/2307.13503</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/twkillian/edict">https://github.com/twkillian/edict</a></li>
<li>paper_authors: Taylor W. Killian, Haoran Zhang, Thomas Hartvigsen, Ava P. Amini</li>
<li>for: 这篇论文是为了探讨如何在具有不规则性的时间序列中进行预测。</li>
<li>methods: 本论文提出了一个名为EDICT的策略，可以在缺失观测的情况下，学习时间序列上的证据分布。这个分布可以在任何时间点进行准确的预测，并且可以在缺失观测的情况下扩展时间上的不确定性。</li>
<li>results: 本论文的实验结果显示，EDICT可以在具有困难的时间序列分类任务中具有竞争性的表现，并且可以在遇到噪音数据时进行不确定性指标预测。<details>
<summary>Abstract</summary>
Prevalent in many real-world settings such as healthcare, irregular time series are challenging to formulate predictions from. It is difficult to infer the value of a feature at any given time when observations are sporadic, as it could take on a range of values depending on when it was last observed. To characterize this uncertainty we present EDICT, a strategy that learns an evidential distribution over irregular time series in continuous time. This distribution enables well-calibrated and flexible inference of partially observed features at any time of interest, while expanding uncertainty temporally for sparse, irregular observations. We demonstrate that EDICT attains competitive performance on challenging time series classification tasks and enabling uncertainty-guided inference when encountering noisy data.
</details>
<details>
<summary>摘要</summary>
广泛存在在现实世界中的多种场景中，如医疗保健，不规则时间序列是难以预测的。因为观测到的时间点不固定，这些时间序列中的特征值可以在不同的时间点 prendre on a range of values。为了捕捉这种uncertainty，我们提出了EDICT策略，它在连续时间中学习不规则时间序列的证据分布。这种分布允许在任何时间点进行Well-calibrated和灵活的特征值推断，并在稀缺观测时扩展时间上的uncertainty。我们示出EDICT在具有困难时间序列分类任务的实验中达到了竞争性的性能，并在遇到噪音数据时提供了uncertainty-guided推断。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Robust-Goal-Based-Wealth-Management"><a href="#Deep-Reinforcement-Learning-for-Robust-Goal-Based-Wealth-Management" class="headerlink" title="Deep Reinforcement Learning for Robust Goal-Based Wealth Management"></a>Deep Reinforcement Learning for Robust Goal-Based Wealth Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13501">http://arxiv.org/abs/2307.13501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tessa Bauman, Bruno Gašperov, Stjepan Begušić, Zvonko Kostanjčar</li>
<li>for: 这个研究旨在提出一种基于深度强化学习的robust目标基础资产管理方法，以便优化财务目标管理策略。</li>
<li>methods: 这个方法使用了深度强化学习来解决sequential decision-making问题，并且将投资选择看作一个Markov decision process。</li>
<li>results: 实验结果显示，这个方法比较多的目标基础资产管理参考值得更高，both simulated和历史股票市场数据上。<details>
<summary>Abstract</summary>
Goal-based investing is an approach to wealth management that prioritizes achieving specific financial goals. It is naturally formulated as a sequential decision-making problem as it requires choosing the appropriate investment until a goal is achieved. Consequently, reinforcement learning, a machine learning technique appropriate for sequential decision-making, offers a promising path for optimizing these investment strategies. In this paper, a novel approach for robust goal-based wealth management based on deep reinforcement learning is proposed. The experimental results indicate its superiority over several goal-based wealth management benchmarks on both simulated and historical market data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>目标基于投资是一种财务管理方法，强调达成特定的金融目标。因此它自然形成为一个顺序决策问题，需要选择适当的投资直到目标达成。因此，人工智能技术适合顺序决策的强化学习可以优化这些投资策略。在这篇论文中，一种基于深度强化学习的新方法 для稳定目标基于投资管理被提出。实验结果表明其在虚拟和历史市场数据上的表现优于许多目标基于投资管理标准做法。
</details></li>
</ul>
<hr>
<h2 id="Finding-Money-Launderers-Using-Heterogeneous-Graph-Neural-Networks"><a href="#Finding-Money-Launderers-Using-Heterogeneous-Graph-Neural-Networks" class="headerlink" title="Finding Money Launderers Using Heterogeneous Graph Neural Networks"></a>Finding Money Launderers Using Heterogeneous Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13499">http://arxiv.org/abs/2307.13499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredjo89/heterogeneous-mpnn">https://github.com/fredjo89/heterogeneous-mpnn</a></li>
<li>paper_authors: Fredrik Johannessen, Martin Jullum</li>
<li>for: 这个论文的目的是提出一种基于图神经网络（GNN）的方法，用于检测银行账户上的货币洗钱活动。</li>
<li>methods: 该方法extend了homogeneous GNN方法，即Message Passing Neural Network（MPNN），以适应非Homogeneous图。在这个过程中，提出了一种新的简单的消息汇聚方法来汇集不同边的消息。</li>
<li>results: 研究发现，使用合适的GNN建筑可以大幅提高银行电子监测系统的检测精度。这是首先在大规模实际非Homogeneous图上应用GNN的财务洗钱检测研究。<details>
<summary>Abstract</summary>
Current anti-money laundering (AML) systems, predominantly rule-based, exhibit notable shortcomings in efficiently and precisely detecting instances of money laundering. As a result, there has been a recent surge toward exploring alternative approaches, particularly those utilizing machine learning. Since criminals often collaborate in their money laundering endeavors, accounting for diverse types of customer relations and links becomes crucial. In line with this, the present paper introduces a graph neural network (GNN) approach to identify money laundering activities within a large heterogeneous network constructed from real-world bank transactions and business role data belonging to DNB, Norway's largest bank. Specifically, we extend the homogeneous GNN method known as the Message Passing Neural Network (MPNN) to operate effectively on a heterogeneous graph. As part of this procedure, we propose a novel method for aggregating messages across different edges of the graph. Our findings highlight the importance of using an appropriate GNN architecture when combining information in heterogeneous graphs. The performance results of our model demonstrate great potential in enhancing the quality of electronic surveillance systems employed by banks to detect instances of money laundering. To the best of our knowledge, this is the first published work applying GNN on a large real-world heterogeneous network for anti-money laundering purposes.
</details>
<details>
<summary>摘要</summary>
当今反贪恶（AML）系统，主要基于规则，表现出明显的缺陷，不能够高效精准地检测贪恶活动。因此，有一些最近的研究倾向于探索新的方法，特别是使用机器学习。由于犯罪分子frequently合作其贪恶活动，考虑到多种客户关系和链接变得非常重要。在这种情况下，本文介绍了一种图 neural network（GNN）方法来在实际世界银行交易和企业角色数据上检测贪恶活动。具体来说，我们扩展了同义GNN方法known as Message Passing Neural Network（MPNN），使其在不同类型的图上运行有效。为了实现这一目的，我们提出了一种新的消息汇聚方法。我们的发现表明，在组合不同类型的图时，使用合适的GNN建筑是非常重要。我们的模型性能结果表明，我们的模型在银行电子监测系统中检测贪恶活动的可能性非常高。到目前为止，这是首次在实际世界的大规模不同类型图上应用GNN进行反贪恶用途的发表文献。
</details></li>
</ul>
<hr>
<h2 id="Zshot-An-Open-source-Framework-for-Zero-Shot-Named-Entity-Recognition-and-Relation-Extraction"><a href="#Zshot-An-Open-source-Framework-for-Zero-Shot-Named-Entity-Recognition-and-Relation-Extraction" class="headerlink" title="Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction"></a>Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13497">http://arxiv.org/abs/2307.13497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Picco, Marcos Martínez Galindo, Alberto Purpura, Leopold Fuchs, Vanessa López, Hoang Thanh Lam</li>
<li>for: 这个研究的目的是提供一个可比较多种当前最佳ZSL方法的框架，以及为industry提供可用的API来进行生产环境中的应用。</li>
<li>methods: 这个框架使用了多种现有的ZSL方法，以及一些新的技术，如pipeline ensemble和可视化工具，以提高ZSL性能。</li>
<li>results: 这个框架可以提供标准的Benchmark datasets，并且可以用于生产环境中的应用，以提高ZSL性能。<details>
<summary>Abstract</summary>
The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline. Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension.
</details>
<details>
<summary>摘要</summary>
Zero-Shot Learning (ZSL) 任务是指在训练时未看到的文本中 Identify 实体或关系。 ZSL 已成为一个关键的研究领域，因为特定领域的标注数据罕见，其应用也在过去几年得到了广泛的发展。随着大型预训语言模型的出现，许多新的方法被提出，从而导致了 ZSL 性能的明显改善。现在，研究社区和产业都有很大的需求，一个全面的 ZSL 框架，以便发展和访问最新的方法和预训模型。在这种研究中，我们提出了一个新的 ZSL 框架，称为 Zshot，以解决以下问题。我们的主要目标是提供一个平台， allowing researchers 可以比较不同的当前最佳 ZSL 方法，并使用标准的 Benchmark 数据集。此外，我们的框架还支持产业，通过可用的 API，为生产中使用 SpaCy NLP 管道。我们的 API 可扩展和评估，并且包括了多种改进，如管道ensemble 和可视化工具，作为 SpaCy 扩展。
</details></li>
</ul>
<hr>
<h2 id="Duet-efficient-and-scalable-hybriD-neUral-rElation-undersTanding"><a href="#Duet-efficient-and-scalable-hybriD-neUral-rElation-undersTanding" class="headerlink" title="Duet: efficient and scalable hybriD neUral rElation undersTanding"></a>Duet: efficient and scalable hybriD neUral rElation undersTanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13494">http://arxiv.org/abs/2307.13494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GIS-PuppetMaster/Duet">https://github.com/GIS-PuppetMaster/Duet</a></li>
<li>paper_authors: Kaixin Zhang, Hongzhi Wang, Yabin Lu, Ziqi Li, Chang Shu, Yu Yan, Donghua Yang</li>
<li>for: 优化Cardinality Estimation的高精度方法，解决数据和工作荟载漂移问题，提高实际应用性。</li>
<li>methods: 引入 predicate 信息，基于自适应模型进行直接Cardinality estimation，不需要采样或非准 differential 过程，可以大幅降低推断复杂度。</li>
<li>results: 实验结果表明，Duet 可以实现所有设计目标，并在高Cardinality 和高维度表上达到更高的准确率，同时也在 CPU 上实现更低的推断成本。<details>
<summary>Abstract</summary>
Learned cardinality estimation methods have achieved high precision compared to traditional methods. Among learned methods, query-driven approaches face the data and workload drift problem for a long time. Although both query-driven and hybrid methods are proposed to avoid this problem, even the state-of-the-art of them suffer from high training and estimation costs, limited scalability, instability, and long-tailed distribution problem on high cardinality and high-dimensional tables, which seriously affects the practical application of learned cardinality estimators. In this paper, we prove that most of these problems are directly caused by the widely used progressive sampling. We solve this problem by introducing predicates information into the autoregressive model and propose Duet, a stable, efficient, and scalable hybrid method to estimate cardinality directly without sampling or any non-differentiable process, which can not only reduces the inference complexity from O(n) to O(1) compared to Naru and UAE but also achieve higher accuracy on high cardinality and high-dimensional tables. Experimental results show that Duet can achieve all the design goals above and be much more practical and even has a lower inference cost on CPU than that of most learned methods on GPU.
</details>
<details>
<summary>摘要</summary>
学习 cardinality 估计方法已经实现了高精度，相比传统方法。然而， query-driven 方法面临着数据和工作负荷漂移问题，持续时间很长。尽管 query-driven 和混合方法都被提出来避免这个问题，但它们都受到高训练和估计成本、有限扩展、不稳定和高维度表中长尾分布问题的影响，这些问题很大程度上限制了学习 cardinality 估计器的实际应用。在这篇论文中，我们证明了大多数这些问题都是由广泛使用进度 sampling 引起的。我们解决这个问题，通过将 predicate 信息添加到权重模型中，并提出了 Duet，一种稳定、高效、可扩展的混合方法，可以直接估计 cardinality 而不需要抽样或任何不可导的过程，这使得 Duet 可以将推理复杂度从 O(n) 降低至 O(1)，比 Naru 和 UAE 更高效。实验结果表明，Duet 可以实现所有的设计目标，并在 CPU 上比大多数学习方法在 GPU 上更实用，甚至具有更低的推理成本。
</details></li>
</ul>
<hr>
<h2 id="ECG-classification-using-Deep-CNN-and-Gramian-Angular-Field"><a href="#ECG-classification-using-Deep-CNN-and-Gramian-Angular-Field" class="headerlink" title="ECG classification using Deep CNN and Gramian Angular Field"></a>ECG classification using Deep CNN and Gramian Angular Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02395">http://arxiv.org/abs/2308.02395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Elmir, Yassine Himeur, Abbes Amira</li>
<li>for: 这篇论文为了心电信号分析提供了一个新的特征表示方法。</li>
<li>methods: 该方法使用格里恩angular field transform将时间频率1D вектор转换为2D图像，然后使用卷积神经网络进行分类。</li>
<li>results: 实验结果显示，该方法可以提高分类性能，并且可以识别和可视化心电信号中的时间特征，如心率、心脉和心电信号的形态变化，这些特征可能不会在原始信号中显示出来。<details>
<summary>Abstract</summary>
This paper study provides a novel contribution to the field of signal processing and DL for ECG signal analysis by introducing a new feature representation method for ECG signals. The proposed method is based on transforming time frequency 1D vectors into 2D images using Gramian Angular Field transform. Moving on, the classification of the transformed ECG signals is performed using Convolutional Neural Networks (CNN). The obtained results show a classification accuracy of 97.47% and 98.65% for anomaly detection. Accordingly, in addition to improving the classification performance compared to the state-of-the-art, the feature representation helps identify and visualize temporal patterns in the ECG signal, such as changes in heart rate, rhythm, and morphology, which may not be apparent in the original signal. This has significant implications in the diagnosis and treatment of cardiovascular diseases and detection of anomalies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rational-kernel-based-interpolation-for-complex-valued-frequency-response-functions"><a href="#Rational-kernel-based-interpolation-for-complex-valued-frequency-response-functions" class="headerlink" title="Rational kernel-based interpolation for complex-valued frequency response functions"></a>Rational kernel-based interpolation for complex-valued frequency response functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13484">http://arxiv.org/abs/2307.13484</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stk-kriging/complex-rational-interpolation">https://github.com/stk-kriging/complex-rational-interpolation</a></li>
<li>paper_authors: Julien Bect, Niklas Georg, Ulrich Römer, Sebastian Schöps</li>
<li>for: 该论文关注用数据来近似一个复数值函数的问题，具体来说是在频域中解析方程的响应函数。</li>
<li>methods: 论文使用了kernel方法，但标准kernels并不适用。 authors还没有解释了下来kernel对应的基础函数的作用和数学意义。</li>
<li>results: 作者引入了新的复数值函数 reproduce kernel空间，并将复数值 interpolate问题转化为 minimum norm interpolate问题。 authors还 combinined interpolant with a low-order rational function，并采用了一个新的模型选择 criterion。 numerical results on examples from different fields show that the method performs well, and outperforms existing rational approximation methods.<details>
<summary>Abstract</summary>
This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest. In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well. Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed. We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces. Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion. Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the method, also in comparison to available rational approximation methods.
</details>
<details>
<summary>摘要</summary>
这项工作关注于基于数据的复数值函数近似，特别是在频域中的响应函数的情况。在这种设定下，核方法在使用中变得越来越普遍，但标准核不能达到好的性能。此外，下面的对应核对答案和其数学意义在复数值情况下还未得到了解。我们引入了新的复数值函数重复核空间，并将复数值 interpol 问题转化为最小二乘 interpol 问题在这些空间中。此外，我们将 interpolant 与低阶 rational function 结合，其阶数由一个新的模型选择 критериion 进行自适应选择。实际例如电磁学和音频例子等，数值结果表明该方法在比较可靠性和可读性方面具有优势，并与现有的 rational approximation 方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Combinatorial-Auctions-and-Graph-Neural-Networks-for-Local-Energy-Flexibility-Markets"><a href="#Combinatorial-Auctions-and-Graph-Neural-Networks-for-Local-Energy-Flexibility-Markets" class="headerlink" title="Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets"></a>Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13470">http://arxiv.org/abs/2307.13470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awadelrahman M. A. Ahmed, Frank Eliassen, Yan Zhang</li>
<li>for: 本研究提出了一个新的 combinatorial 拍卖框架，用于解决地方能源柔软性市场中的购买者对多个时间间隔的不能够组合化问题。</li>
<li>methods: 本研究使用了一个简单 yet powerful 三元 graph 表现方法，并设计了基于图像神经网络的模型，以解决下游决定问题。</li>
<li>results: 本研究的模型可以实现对于地方市场中的能源柔软性资源的有效分配，并且显示出线性推断时间复杂度相对于商业解决方案的几何级数增长。<details>
<summary>Abstract</summary>
This paper proposes a new combinatorial auction framework for local energy flexibility markets, which addresses the issue of prosumers' inability to bundle multiple flexibility time intervals. To solve the underlying NP-complete winner determination problems, we present a simple yet powerful heterogeneous tri-partite graph representation and design graph neural network-based models. Our models achieve an average optimal value deviation of less than 5\% from an off-the-shelf optimization tool and show linear inference time complexity compared to the exponential complexity of the commercial solver. Contributions and results demonstrate the potential of using machine learning to efficiently allocate energy flexibility resources in local markets and solving optimization problems in general.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这篇论文提出了一种新的 combinatorial 拍卖框架，用于解决本地能源灵活性市场中潜在用户（prosumers）无法捆绑多个灵活时间间隔的问题。为解决这个问题，我们提出了一种简单 yet 强大的多类图表示和图神经网络模型。我们的模型在评估价值上偏差不超过5%，与商业优化工具相比，并且推理时间复杂度为线性，与商业解决器的极限复杂度相比，显著提高了效率。本文的贡献和结果表明，使用机器学习可以高效地分配本地能源灵活性资源，并在总体上解决优化问题。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Graph-with-Prototypical-Contrastive-Learning-in-E-Commerce-Bundle-Recommendation"><a href="#Gaussian-Graph-with-Prototypical-Contrastive-Learning-in-E-Commerce-Bundle-Recommendation" class="headerlink" title="Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation"></a>Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13468">http://arxiv.org/abs/2307.13468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao-Yang Liu, Liucheng Sun, Chenwei Weng, Qijin Chen, Chengfu Huo</li>
<li>for: 提供一个精心选择的bundle推荐解决方案，以满足用户的偏好在电商平台上。</li>
<li>methods: 我们提出了一种新的 Gaussian Graph with Prototypical Contrastive Learning (GPCL)框架，其中每个用户&#x2F;bundle&#x2F;item都被表示为一个 Gaussian 分布而不是固定的 вектор。我们还设计了一个 прототип型对比学习模块，以捕捉Contextual information并减少采样偏误问题。</li>
<li>results: 我们在多个公共数据集上进行了广泛的实验，并证明了 GPCL 可以超越先前的方法，达到新的状态机器人性性。此外， GPCL 已经在实际的电商平台上部署，并实现了substantial improvements。<details>
<summary>Abstract</summary>
Bundle recommendation aims to provide a bundle of items to satisfy the user preference on e-commerce platform. Existing successful solutions are based on the contrastive graph learning paradigm where graph neural networks (GNNs) are employed to learn representations from user-level and bundle-level graph views with a contrastive learning module to enhance the cooperative association between different views. Nevertheless, they ignore the uncertainty issue which has a significant impact in real bundle recommendation scenarios due to the lack of discriminative information caused by highly sparsity or diversity. We further suggest that their instancewise contrastive learning fails to distinguish the semantically similar negatives (i.e., sampling bias issue), resulting in performance degradation. In this paper, we propose a novel Gaussian Graph with Prototypical Contrastive Learning (GPCL) framework to overcome these challenges. In particular, GPCL embeds each user/bundle/item as a Gaussian distribution rather than a fixed vector. We further design a prototypical contrastive learning module to capture the contextual information and mitigate the sampling bias issue. Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performance compared to previous methods on several public datasets. Moreover, GPCL has been deployed on real-world e-commerce platform and achieved substantial improvements.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate " Bundle recommendation aims to provide a bundle of items to satisfy the user preference on e-commerce platform. Existing successful solutions are based on the contrastive graph learning paradigm where graph neural networks (GNNs) are employed to learn representations from user-level and bundle-level graph views with a contrastive learning module to enhance the cooperative association between different views. Nevertheless, they ignore the uncertainty issue which has a significant impact in real bundle recommendation scenarios due to the lack of discriminative information caused by highly sparsity or diversity. We further suggest that their instancewise contrastive learning fails to distinguish the semantically similar negatives (i.e., sampling bias issue), resulting in performance degradation. In this paper, we propose a novel Gaussian Graph with Prototypical Contrastive Learning (GPCL) framework to overcome these challenges. In particular, GPCL embeds each user/bundle/item as a Gaussian distribution rather than a fixed vector. We further design a prototypical contrastive learning module to capture the contextual information and mitigate the sampling bias issue. Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performance compared to previous methods on several public datasets. Moreover, GPCL has been deployed on real-world e-commerce platform and achieved substantial improvements. "into Simplified Chinese:<<SYS>>bundle 推荐的目标是为在电子商务平台上满足用户的首选，现有的成功解决方案基于对用户和套件视图的对比学习 paradigm，通过图 neural networks (GNNs) 学习用户和套件视图的表示，并通过对比学习模块增强不同视图之间的合作关系。然而，它们忽略了实际推荐场景中的不确定性问题，这是由于高度稀畴或多样性导致的缺乏特征信息。我们还建议它们的实例级对比学习无法分辨Semantic 相似的负例（即采样偏见问题），导致性能下降。在这篇论文中，我们提出了一种新的 Gaussian Graph with Prototypical Contrastive Learning (GPCL) 框架，以解决这些挑战。具体来说，GPCL 将每个用户/套件/项 embed 为 Gaussian 分布而不是固定 вектор。我们还设计了一种 проtotypical 对比学习模块，以捕捉上下文信息并 mitigate 采样偏见问题。广泛的实验表明，通过我们提出的 ком成分，我们在许多公共数据集上 achieve 新的状态艺术性能，而且 GPCL 已经在真实的电子商务平台上部署，并实现了重大改进。Note: "Gaussian Distribution" in Chinese is "Gaussian 分布" (Gaussian fēn bù). "Prototypical Contrastive Learning" in Chinese is "prototypical 对比学习" (prototypical duì bǐ xué xí).
</details></li>
</ul>
<hr>
<h2 id="Integrating-processed-based-models-and-machine-learning-for-crop-yield-prediction"><a href="#Integrating-processed-based-models-and-machine-learning-for-crop-yield-prediction" class="headerlink" title="Integrating processed-based models and machine learning for crop yield prediction"></a>Integrating processed-based models and machine learning for crop yield prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13466">http://arxiv.org/abs/2307.13466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michiel G. J. Kallenberg, Bernardo Maestrini, Ron van Bree, Paul Ravensbergen, Christos Pylianidis, Frits van Evert, Ioannis N. Athanasiadis</li>
<li>for: 预测哈比粮食产量</li>
<li>methods: 使用混合元模型方法，结合了理论驱动的生长模型和数据驱动的神经网络</li>
<li>results: 在silico中，元模型方法比基线方法（仅使用数据驱动方法）更好地预测哈比粮食产量，并在实际采用场景（303个场地和77个商业场地）中与生长模型几乎相当。然而，两种模型在这些场景中都比一个简单的线性回归模型和专业人员 manually设计的特征集和预处理方法差。<details>
<summary>Abstract</summary>
Crop yield prediction typically involves the utilization of either theory-driven process-based crop growth models, which have proven to be difficult to calibrate for local conditions, or data-driven machine learning methods, which are known to require large datasets. In this work we investigate potato yield prediction using a hybrid meta-modeling approach. A crop growth model is employed to generate synthetic data for (pre)training a convolutional neural net, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline comprising a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. In the latter set, however, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts. Our findings indicate the potential of meta-modeling for accurate crop yield prediction; however, further advancements and validation using extensive real-world datasets is recommended to solidify its practical effectiveness.
</details>
<details>
<summary>摘要</summary>
Typically, crop yield prediction relies on either theory-driven process-based crop growth models, which are difficult to calibrate for local conditions, or data-driven machine learning methods, which require large datasets. In this study, we investigate potato yield prediction using a hybrid meta-modeling approach. We use a crop growth model to generate synthetic data for training a convolutional neural network, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline consisting of a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. However, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts. Our findings suggest the potential of meta-modeling for accurate crop yield prediction, but further advancements and validation using extensive real-world datasets are recommended to solidify its practical effectiveness.
</details></li>
</ul>
<hr>
<h2 id="Fundamental-causal-bounds-of-quantum-random-access-memories"><a href="#Fundamental-causal-bounds-of-quantum-random-access-memories" class="headerlink" title="Fundamental causal bounds of quantum random access memories"></a>Fundamental causal bounds of quantum random access memories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13460">http://arxiv.org/abs/2307.13460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunfei Wang, Yuri Alexeev, Liang Jiang, Frederic T. Chong, Junyu Liu</li>
<li>for: This paper explores the fundamental limits of quantum random access memory (QRAM) in quantum computing applications, specifically the impact of causality on QRAM performance.</li>
<li>methods: The authors use relativistic quantum field theory and Lieb-Robinson bounds to derive bounds on the number of logical qubits that can be stored in QRAM, based on the size of the quantum system and the clock cycle time.</li>
<li>results: The paper shows that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimension, $\mathcal{O}(10^{15})$ to $\mathcal{O}(10^{20})$ in various 2D architectures, and $\mathcal{O}(10^{24})$ in 3 dimensions, with clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer. The authors contend that this causality bound broadly applies to other quantum hardware systems.<details>
<summary>Abstract</summary>
Quantum devices should operate in adherence to quantum physics principles. Quantum random access memory (QRAM), a fundamental component of many essential quantum algorithms for tasks such as linear algebra, data search, and machine learning, is often proposed to offer $\mathcal{O}(\log N)$ circuit depth for $\mathcal{O}(N)$ data size, given $N$ qubits. However, this claim appears to breach the principle of relativity when dealing with a large number of qubits in quantum materials interacting locally. In our study we critically explore the intrinsic bounds of rapid quantum memories based on causality, employing the relativistic quantum field theory and Lieb-Robinson bounds in quantum many-body systems. In this paper, we consider a hardware-efficient QRAM design in hybrid quantum acoustic systems. Assuming clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer, we show that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimension, $\mathcal{O}(10^{15})$ to $\mathcal{O}(10^{20})$ in various 2D architectures, and $\mathcal{O}(10^{24})$ in 3 dimensions. We contend that this causality bound broadly applies to other quantum hardware systems. Our findings highlight the impact of fundamental quantum physics constraints on the long-term performance of quantum computing applications in data science and suggest potential quantum memory designs for performance enhancement.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-behavioural-transformer-for-effective-collaboration-between-a-robot-and-a-non-stationary-human"><a href="#A-behavioural-transformer-for-effective-collaboration-between-a-robot-and-a-non-stationary-human" class="headerlink" title="A behavioural transformer for effective collaboration between a robot and a non-stationary human"></a>A behavioural transformer for effective collaboration between a robot and a non-stationary human</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13447">http://arxiv.org/abs/2307.13447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruaridh Mon-Williams, Theodoros Stouraitis, Sethu Vijayakumar</li>
<li>for: The paper is written for exploring how robots could better predict human behavior in non-stationary environments, and developing a principled meta-learning framework to address this challenge.</li>
<li>methods: The paper proposes a conditional transformer called Behaviour-Transform (BeTrans) that can adapt quickly to new human agents with non-stationary behaviors, and trains BeTrans on simulated human agents with different systematic biases in collaborative settings.</li>
<li>results: The paper shows that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than state-of-the-art (SOTA) techniques.Here’s the information in Simplified Chinese text:</li>
<li>for: 本研究旨在探讨机器人如何更好地预测人类行为在非站点环境中，并开发出原则正的元学习框架来解决这一挑战。</li>
<li>methods: 本研究提出了一种名为行为转换（BeTrans）的条件变换器，可以快速适应新的人类代理人非站点行为，并在合作设置下对 simulate human agents with different systematic biases进行训练。</li>
<li>results: 研究显示，BeTrans可以有效地与 simulate human agents collaborate，并在非站点人类代理人行为下更快地适应非站点人类代理人行为。<details>
<summary>Abstract</summary>
A key challenge in human-robot collaboration is the non-stationarity created by humans due to changes in their behaviour. This alters environmental transitions and hinders human-robot collaboration. We propose a principled meta-learning framework to explore how robots could better predict human behaviour, and thereby deal with issues of non-stationarity. On the basis of this framework, we developed Behaviour-Transform (BeTrans). BeTrans is a conditional transformer that enables a robot agent to adapt quickly to new human agents with non-stationary behaviours, due to its notable performance with sequential data. We trained BeTrans on simulated human agents with different systematic biases in collaborative settings. We used an original customisable environment to show that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than SOTA techniques.
</details>
<details>
<summary>摘要</summary>
人机合作中一大挑战是由人类行为引起的非站点性，这会改变环境转移并妨碍人机合作。我们提出了一种原则性的元学习框架，以解决机器人如何更好地预测人类行为，并因此处理非站点性问题。基于这个框架，我们开发了行为变换（BeTrans）。BeTrans 是一种可变条件的变换器，允许机器人代理者快速适应新的人类代理者非站点行为。我们在 simulated 人类代理者之间进行了训练，并在合作场景中表明了 BeTrans 可以快速适应非站点人类代理者，并且比标准技术更快。
</details></li>
</ul>
<hr>
<h2 id="Network-Traffic-Classification-based-on-Single-Flow-Time-Series-Analysis"><a href="#Network-Traffic-Classification-based-on-Single-Flow-Time-Series-Analysis" class="headerlink" title="Network Traffic Classification based on Single Flow Time Series Analysis"></a>Network Traffic Classification based on Single Flow Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13434">http://arxiv.org/abs/2307.13434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koumajos/classificationbasedonsfts">https://github.com/koumajos/classificationbasedonsfts</a></li>
<li>paper_authors: Josef Koumar, Karel Hynek, Tomáš Čejka</li>
<li>for: 本研究旨在解决当前对加密网络通信进行分析时所存在的挑战，通过网络流量监测使用IP流。</li>
<li>methods: 本研究提出了一种基于单流时间序分析的新的流扩展方法，通过统计分析、时域分析、流时间范围内的包分布、时间序列行为和频域分析，提取了69个统计特征。</li>
<li>results: 我们通过使用15种公开的数据集进行评估，并证明了提案的特征向量在网络流量分类任务中的可用性和通用性。在评估中，相比related works，我们的方法在 binary和多类分类任务中具有类似或更好的分类性能，在超过一半的评估任务中，分类性能提高了5%以上。<details>
<summary>Abstract</summary>
Network traffic monitoring using IP flows is used to handle the current challenge of analyzing encrypted network communication. Nevertheless, the packet aggregation into flow records naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification tasks. In more than half of the evaluated tasks, the classification performance increased by up to 5\%.
</details>
<details>
<summary>摘要</summary>
网络流量监测使用流程来处理当前挑战的加密网络通信分析。然而， packet 聚合到流记录 naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification tasks. In more than half of the evaluated tasks, the classification performance increased by up to 5%.Here's the translation in Traditional Chinese:网络流量监测使用流程来处理当前挑战的加密网络通信分析。然而， packet 聚合到流记录 naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification tasks. In more than half of the evaluated tasks, the classification performance increased by up to 5%.
</details></li>
</ul>
<hr>
<h2 id="Achieving-Linear-Speedup-in-Decentralized-Stochastic-Compositional-Minimax-Optimization"><a href="#Achieving-Linear-Speedup-in-Decentralized-Stochastic-Compositional-Minimax-Optimization" class="headerlink" title="Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization"></a>Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13430">http://arxiv.org/abs/2307.13430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongchang Gao</li>
<li>for:  solve the decentralized stochastic compositional minimax problem with linear speedup</li>
<li>methods:  develop a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function</li>
<li>results:  achieve linear speedup with respect to the number of workers, and applied to the imbalanced classification problem with effective experimental results.Here’s the summary in English for reference:</li>
<li>for: Solve the decentralized stochastic compositional minimax problem with linear speedup</li>
<li>methods: Develop a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function</li>
<li>results: Achieve linear speedup with respect to the number of workers, and applied to the imbalanced classification problem with effective experimental results.<details>
<summary>Abstract</summary>
The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algorithmic design could benefit the development of decentralized compositional optimization. Finally, we applied our methods to the imbalanced classification problem. The extensive experimental results provide evidence for the effectiveness of our algorithm.
</details>
<details>
<summary>摘要</summary>
“ Stochastic compositional minimax problem 在 recent years 中吸引了很多关注，因为它涵盖了许多emerging machine learning models。然而，由于分布式数据的出现，optimalizing this kind of problem under the decentralized setting becomes badly needed。然而，compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms。具体来说，our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function。为了解决这个问题，我们开发了一种novel decentralized stochastic compositional gradient descent ascent with momentum algorithm，以减少inner-level function consensus error。因此，our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers。我们认为这种新的algorithmic design可以benefit the development of decentralized compositional optimization。最后，我们应用了我们的方法到imbalanced classification problem。our extensive experimental results provide evidence for the effectiveness of our algorithm.”Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="A-signal-processing-interpretation-of-noise-reduction-convolutional-neural-networks"><a href="#A-signal-processing-interpretation-of-noise-reduction-convolutional-neural-networks" class="headerlink" title="A signal processing interpretation of noise-reduction convolutional neural networks"></a>A signal processing interpretation of noise-reduction convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13425">http://arxiv.org/abs/2307.13425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen</li>
<li>for: 这篇论文旨在探讨深度学习中的编码-解码卷积神经网络（ED CNN）的理论基础，以及如何通过这些理论来设计更加稳定和高效的 CNN 架构。</li>
<li>methods: 本文使用了深度学习的基本原理和信号处理的基本原理来探讨 ED CNN 的内部运作，并提出了一种统一的理论框架来描述不同的 ED CNN 架构。</li>
<li>results: 本文提出了一种自 contenained的框架，可以帮助设计更加稳定和高效的 CNN 架构，并且可以帮助更多人理解深度学习中的 ED CNN 的内部运作。<details>
<summary>Abstract</summary>
Encoding-decoding CNNs play a central role in data-driven noise reduction and can be found within numerous deep-learning algorithms. However, the development of these CNN architectures is often done in ad-hoc fashion and theoretical underpinnings for important design choices is generally lacking. Up to this moment there are different existing relevant works that strive to explain the internal operation of these CNNs. Still, these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience. In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework. By connecting basic principles from signal processing to the field of deep learning, this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures.
</details>
<details>
<summary>摘要</summary>
Encoding-decoding CNNs 在数据驱动的噪声减少中扮演中央角色，可以在多种深度学习算法中找到。然而，Encoding-decoding CNNs 的建构通常是靠束式的，而且关键设计选择的理论基础通常缺失。到目前为止，有很多相关的工作努力了解Encoding-decoding CNNs 的内部运作。然而，这些想法是杂乱不尽，需要特殊的专业知识才能访问更广泛的群众。为了开拓这一领域，这篇文章建立了深度卷积框架的理论基础，并将多种ED CNN架构纳入一个统一的理论框架中。通过将信号处理的基本原理与深度学习相连接，这篇自 contenido的材料提供了设计robust和高效的新的CNN架构的重要指南。
</details></li>
</ul>
<hr>
<h2 id="Non-Intrusive-Intelligibility-Predictor-for-Hearing-Impaired-Individuals-using-Self-Supervised-Speech-Representations"><a href="#Non-Intrusive-Intelligibility-Predictor-for-Hearing-Impaired-Individuals-using-Self-Supervised-Speech-Representations" class="headerlink" title="Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations"></a>Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13423">http://arxiv.org/abs/2307.13423</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Close, Thomas Hain, Stefan Goetze</li>
<li>for: 这个研究旨在扩展非侵入式的音质评分模型，以便对听力障碍用户的语音质量进行预测。</li>
<li>methods: 这个研究使用了自我超vised speech representation（SSSRs）作为输入特征，并利用了非侵入式预测模型来预测听力障碍用户的语音质量。</li>
<li>results: 研究发现，SSSRs可以作为输入特征，实现了与更复杂的系统相当的竞争性性能。对于不同的listeners和优化系统，研究还发现了预测性能的相关性分析。<details>
<summary>Abstract</summary>
Self-supervised speech representations (SSSRs) have been successfully applied to a number of speech-processing tasks, e.g. as feature extractor for speech quality (SQ) prediction, which is, in turn, relevant for assessment and training speech enhancement systems for users with normal or impaired hearing. However, exact knowledge of why and how quality-related information is encoded well in such representations remains poorly understood. In this work, techniques for non-intrusive prediction of SQ ratings are extended to the prediction of intelligibility for hearing-impaired users. It is found that self-supervised representations are useful as input features to non-intrusive prediction models, achieving competitive performance to more complex systems. A detailed analysis of the performance depending on Clarity Prediction Challenge 1 listeners and enhancement systems indicates that more data might be needed to allow generalisation to unknown systems and (hearing-impaired) individuals
</details>
<details>
<summary>摘要</summary>
自我监督的语音表示（SSSR）已经成功应用于一些语音处理任务中，例如作为语音质量（SQ）预测的特征提取器，这对于评估和训练语音增强系统 для正常或受损耳朵用户都是重要的。然而，关于为什么和如何在这些表示中储存质量相关信息的具体知识仍然不够了解。在这项工作中，对于听众评分不侵入的预测模型的扩展，发现自我监督表示是非常有用的输入特征，可以达到和更复杂的系统相当的性能。对于 Clarity Prediction Challenge 1 listeners 和增强系统的分析表明，更多的数据可能需要，以allow总结到未知系统和听众（听力障碍）个体
</details></li>
</ul>
<hr>
<h2 id="On-the-Learning-Dynamics-of-Attention-Networks"><a href="#On-the-Learning-Dynamics-of-Attention-Networks" class="headerlink" title="On the Learning Dynamics of Attention Networks"></a>On the Learning Dynamics of Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13421">http://arxiv.org/abs/2307.13421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vashisht-rahul/on-the-learning-dynamics-of-attention-networks">https://github.com/vashisht-rahul/on-the-learning-dynamics-of-attention-networks</a></li>
<li>paper_authors: Rahul Vashisht, Harish G. Ramaswamy</li>
<li>for: 本文研究了三种推理损失函数，它们分别是软注意力损失、硬注意力损失和隐变量маarginal likelihood（LVML）注意力损失。这三种损失函数都是为了找到一个<code>焦点&#39;模型，它可以选择输入中正确的段落，以及一个</code>分类’模型，它可以处理选择的段落并生成目标标签。但是它们在选择段落的方式不同，导致了不同的动态和最终结果。</li>
<li>methods: 本文使用了三种推理损失函数来训练模型，并分析了这些损失函数在不同的情况下的行为。同时，文章还提出了一种简单的混合方法，该方法将软注意力损失、硬注意力损失和LVML注意力损失的优点相互融合，并在一些半synthetic和实际世界数据集上进行了实验。</li>
<li>results: 本文的实验结果表明，软注意力损失和硬注意力损失在不同的初始化情况下，会导致模型的表现有很大差异。而LVML注意力损失则在初始化情况下表现较好，但是后续的训练过程中会逐渐下降。此外，文章还发现了这些损失函数在不同的数据集上的表现差异，并提出了一种简单的混合方法来解决这个问题。<details>
<summary>Abstract</summary>
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization and splutters later on. On the other hand, hard attention loss behaves in the opposite fashion. Based on our observations, we propose a simple hybrid approach that combines the advantages of the different loss functions and demonstrates it on a collection of semi-synthetic and real-world datasets
</details>
<details>
<summary>摘要</summary>
注意模型通常通过优化三种标准损失函数来学习：软注意、硬注意和隐变量极大可能性（LVML）注意。这三种方法均由同一目标——找到一个`焦点'模型，可以选择输入中正确的段落，以及一个`分类'模型，可以处理选择的段落并生成目标标签。然而，它们在选取段落的方式不同，从而导致了不同的动态和最终结果。我们在这些模型中观察到了独特的签名，并解释这为Gradient Descent的演化下的分类模型的变化所致。我们还在简单的设置下分析这些方法，并derive了关键参数轨迹的关闭式表达式。在软注意损失函数下，焦点模型在初始化时快速改进，然后后来受阻。相反，硬注意损失函数表现得更opposite。基于我们的观察，我们提出了一种简单的混合方法，其combines the advantages of different loss functions, and demonstrates it on a collection of semi-synthetic and real-world datasets.Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="Co-Design-of-Out-of-Distribution-Detectors-for-Autonomous-Emergency-Braking-Systems"><a href="#Co-Design-of-Out-of-Distribution-Detectors-for-Autonomous-Emergency-Braking-Systems" class="headerlink" title="Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems"></a>Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13419">http://arxiv.org/abs/2307.13419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Yuhas, Arvind Easwaran</li>
<li>for: 这个论文的目的是提出一种用于自动驾驶车辆（AV）中的风险监控方法，以确保AV的决策正确性。</li>
<li>methods: 这个论文使用了一种基于视觉的自动紧急刹车系统（AEBS），并使用了风险模型来描述LEC和OOD检测器的设计参数之间的交互关系，以及它们对系统安全的影响。</li>
<li>results: 根据这个论文，通过使用这种协设方法，可以在AEBS中减少风险达42.3%，同时保持资源利用相对均衡。<details>
<summary>Abstract</summary>
Learning enabled components (LECs), while critical for decision making in autonomous vehicles (AVs), are likely to make incorrect decisions when presented with samples outside of their training distributions. Out-of-distribution (OOD) detectors have been proposed to detect such samples, thereby acting as a safety monitor, however, both OOD detectors and LECs require heavy utilization of embedded hardware typically found in AVs. For both components, there is a tradeoff between non-functional and functional performance, and both impact a vehicle's safety. For instance, giving an OOD detector a longer response time can increase its accuracy at the expense of the LEC. We consider an LEC with binary output like an autonomous emergency braking system (AEBS) and use risk, the combination of severity and occurrence of a failure, to model the effect of both components' design parameters on each other's functional and non-functional performance, as well as their impact on system safety. We formulate a co-design methodology that uses this risk model to find the design parameters for an OOD detector and LEC that decrease risk below that of the baseline system and demonstrate it on a vision based AEBS. Using our methodology, we achieve a 42.3% risk reduction while maintaining equivalent resource utilization.
</details>
<details>
<summary>摘要</summary>
学习启用组件（LEC）在自动驾驶车辆（AV）中扮演着重要的决策角色，但当面临不同于训练数据集的样本时，LEC可能会作出错误的决策。为了检测这些样本，外围样本检测器（OOD）被提出来作为安全监视器。然而，OOD检测器和LEC都需要嵌入式硬件的严重利用，这两者之间存在功能性和非功能性性能之间的贸易关系。为了保证系统的安全，我们需要考虑这两个组件之间的影响，并根据风险模型来寻找合适的设计参数。我们提出了一种共同设计方法，该方法使用风险模型来找到一个符合风险标准的OOD检测器和LEC设计参数，并在视觉基于的自动紧急制动系统（AEBS）上进行了示例实现。结果表明，我们可以降低风险的总体减少率为42.3%，同时保持资源利用相对均衡。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Orchestrations-for-URLLC-Service-via-Hierarchical-Reinforcement-Learning"><a href="#Communication-Efficient-Orchestrations-for-URLLC-Service-via-Hierarchical-Reinforcement-Learning" class="headerlink" title="Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning"></a>Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13415">http://arxiv.org/abs/2307.13415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Shi, Milad Ganjalizadeh, Hossein Shokri Ghadikolaei, Marina Petrova</li>
<li>for: This paper aims to improve the efficiency of ultra-reliable low latency communications (URLLC) services in 5G networks by leveraging reinforcement learning (RL) to allocate wireless resources.</li>
<li>methods: The proposed multi-agent hierarchical RL (HRL) framework enables the implementation of multi-level policies with different control loop timescales, allowing for more efficient use of wireless resources and reduced signaling and energy consumption.</li>
<li>results: The proposed HRL framework achieves better performance than traditional RL methods in a factory automation scenario, with significantly less overhead of signal transmissions and delay.<details>
<summary>Abstract</summary>
Ultra-reliable low latency communications (URLLC) service is envisioned to enable use cases with strict reliability and latency requirements in 5G. One approach for enabling URLLC services is to leverage Reinforcement Learning (RL) to efficiently allocate wireless resources. However, with conventional RL methods, the decision variables (though being deployed at various network layers) are typically optimized in the same control loop, leading to significant practical limitations on the control loop's delay as well as excessive signaling and energy consumption. In this paper, we propose a multi-agent Hierarchical RL (HRL) framework that enables the implementation of multi-level policies with different control loop timescales. Agents with faster control loops are deployed closer to the base station, while the ones with slower control loops are at the edge or closer to the core network providing high-level guidelines for low-level actions. On a use case from the prior art, with our HRL framework, we optimized the maximum number of retransmissions and transmission power of industrial devices. Our extensive simulation results on the factory automation scenario show that the HRL framework achieves better performance as the baseline single-agent RL method, with significantly less overhead of signal transmissions and delay compared to the one-agent RL methods.
</details>
<details>
<summary>摘要</summary>
超可靠低延迟通信服务（URLLC）在5G中被看作为实现具有严格可靠性和延迟要求的应用场景。一种实现URLLC服务的方法是通过强化学习（RL）有效地分配无线资源。然而，传统RL方法中的决策变量（即在不同网络层部署）通常在同一个控制循环中优化，这会导致控制循环延迟的具体限制以及过分的信号传输和能量消耗。在这篇论文中，我们提出了一种多代理层RL（HRL）框架，该框架允许实现多级策略，并且具有不同控制循环时间尺度。在基站附近部署更快的控制循环的代理，而在边缘或核心网络附近部署更慢的控制循环的代理，以提供高级指导供低级动作。在一个工厂自动化场景中，通过我们的HRL框架，我们优化了设备的最大重传数和发射功率。我们的广泛的模拟结果表明，HRL框架在基于优化单个代理RL方法的场景下表现更好，并且具有较少的信号传输过程和延迟。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Memory-Wall-Effects-in-CNN-Engines-with-On-the-Fly-Weights-Generation"><a href="#Mitigating-Memory-Wall-Effects-in-CNN-Engines-with-On-the-Fly-Weights-Generation" class="headerlink" title="Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation"></a>Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13412">http://arxiv.org/abs/2307.13412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stylianos I. Venieris, Javier Fernandez-Marques, Nicholas D. Lane</li>
<li>for: 这个研究旨在提高FPGA基于 convolutional neural network (CNN) 的数据传输效率和能效性。</li>
<li>methods: 研究人员提出了一个新的 CNN 数据传输系统，包括一个新的 CNN 硬件架构，以及一个自动硬件扮潮方法来优化数据传输效率。</li>
<li>results: 研究人员发现，这个新的 CNN 数据传输系统可以实现平均提高2.57倍的性能效率，并且可以实现更高的性能密度，较以前的FPGA基于 CNN 数据传输系统。<details>
<summary>Abstract</summary>
The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings. In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of FPGA-based CNN accelerators. In this context, single computation engines constitute a popular approach to support diverse CNN modes without the overhead of fabric reconfiguration. Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. This paper presents unzipFPGA, a novel CNN inference system that counteracts the limitations of existing CNN engines. The proposed framework comprises a novel CNN hardware architecture that introduces a weights generator module that enables the on-chip on-the-fly generation of weights, alleviating the negative impact of limited bandwidth on memory-bound layers. We further enhance unzipFPGA with an automated hardware-aware methodology that tailors the weights generation mechanism to the target CNN-device pair, leading to an improved accuracy-performance balance. Finally, we introduce an input selective processing element (PE) design that balances the load between PEs in suboptimally mapped layers. The proposed framework yields hardware designs that achieve an average of 2.57x performance efficiency gain over highly optimised GPU designs for the same power constraints and up to 3.94x higher performance density over a diverse range of state-of-the-art FPGA-based CNN accelerators.
</details>
<details>
<summary>摘要</summary>
具有历史上无 precedent的精度的卷积神经网络（CNN）在许多人工智能任务中显示出了广泛的应用，因此在移动和嵌入式设备上广泛部署。为了实现高性能和能效的推理，大量的研究力量被投入到了基于FPGA的CNN加速器的设计中。在这个上下文中，单个计算引擎是一种广泛使用的方法来支持多种CNN模式，而无需fabric重新配置的开销。然而，这种灵活性通常会导致在储存bound层上的性能下降和资源废用，因为在计算引擎的固定配置下可能会导致一些层的不佳映射。在这种情况下，我们研究了CNN引擎设计方面的各种因素，以及在运行时可以通过压缩权重来解决这些问题的方法。我们称这种方法为“on-the-fly”方法。本文介绍了一种新的CNN推理系统——unzipFPGA，该系统可以对CNN模型进行在运行时压缩权重，从而减少储存bound层的性能下降。此外，我们还提出了一种自适应硬件方法，该方法可以根据目标CNN设备和模型之间的优化，以提高精度和性能的平衡。最后，我们还提出了一种输入选择处理元件（PE）的设计，该设计可以在不佳映射的层中均衡PE的负担。根据我们的实验结果，unzipFPGA系统可以在同等功耗下达到GPU设计的2.57倍的性能效率提升，并且在多种现有的FPGA-based CNN加速器中达到3.94倍的性能密度。
</details></li>
</ul>
<hr>
<h2 id="The-Double-Edged-Sword-of-Big-Data-and-Information-Technology-for-the-Disadvantaged-A-Cautionary-Tale-from-Open-Banking"><a href="#The-Double-Edged-Sword-of-Big-Data-and-Information-Technology-for-the-Disadvantaged-A-Cautionary-Tale-from-Open-Banking" class="headerlink" title="The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking"></a>The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13408">http://arxiv.org/abs/2307.13408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savina Dine Kim, Galina Andreeva, Michael Rovatsos</li>
<li>for: 这篇论文探讨了在开放银行服务中可能存在的不公正隐含因素，具体是通过机器学习（ML）技术分析金融服务中的交易数据，以了解金融敏感人群的行为特征和风险评估方面的隐藏风险。</li>
<li>methods: 这篇论文使用了一个Unique的UK FinTech借据集，并比较了三种ML分类器来预测金融敏感人群的可能性，并通过划分来描述不同群体的特征组合。</li>
<li>results: 研究发现，引入的金融行为特征可以预测排除个人信息，特别是敏感或保护特征，这说明开放银行数据中隐藏的风险。研究者提出了对开放银行服务的不公正风险的批评，并建议在这种新技术环境中保持公正。<details>
<summary>Abstract</summary>
This research article analyses and demonstrates the hidden implications for fairness of seemingly neutral data coupled with powerful technology, such as machine learning (ML), using Open Banking as an example. Open Banking has ignited a revolution in financial services, opening new opportunities for customer acquisition, management, retention, and risk assessment. However, the granularity of transaction data holds potential for harm where unnoticed proxies for sensitive and prohibited characteristics may lead to indirect discrimination. Against this backdrop, we investigate the dimensions of financial vulnerability (FV), a global concern resulting from COVID-19 and rising inflation. Specifically, we look to understand the behavioral elements leading up to FV and its impact on at-risk, disadvantaged groups through the lens of fair interpretation. Using a unique dataset from a UK FinTech lender, we demonstrate the power of fine-grained transaction data while simultaneously cautioning its safe usage. Three ML classifiers are compared in predicting the likelihood of FV, and groups exhibiting different magnitudes and forms of FV are identified via clustering to highlight the effects of feature combination. Our results indicate that engineered features of financial behavior can be predictive of omitted personal information, particularly sensitive or protected characteristics, shedding light on the hidden dangers of Open Banking data. We discuss the implications and conclude fairness via unawareness is ineffective in this new technological environment.
</details>
<details>
<summary>摘要</summary>
Using a unique dataset from a UK FinTech lender, we compare three ML classifiers in predicting the likelihood of FV and identify groups exhibiting different forms of FV through clustering. Our results show that engineered features of financial behavior can be used to predict omitted personal information, including sensitive or protected characteristics. This highlights the potential dangers of using Open Banking data without proper safeguards.We conclude that fairness through unawareness is not effective in this new technological environment and discuss the implications for financial services and regulatory frameworks.
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanation-via-Search-in-Gaussian-Mixture-Distributed-Latent-Space"><a href="#Counterfactual-Explanation-via-Search-in-Gaussian-Mixture-Distributed-Latent-Space" class="headerlink" title="Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space"></a>Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13390">http://arxiv.org/abs/2307.13390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Zhao, Klaus Broelemann, Gjergji Kasneci</li>
<li>for: 本研究旨在提供一种生成counterfactual explanations（CEs）的新方法，以帮助用户更好地理解AI系统的决策过程和改进其决策结果。</li>
<li>methods: 本方法首先将autoencoder的 latent space变换成一个mixture of Gaussian distributions，然后通过线性 interpolate在query sample和目标类中心点之间生成CEs。</li>
<li>results: 我们的方法可以快速和高效地生成高质量的CEs，并且可以保持输入样本的特征。在多个图像和表格 datasets上进行了多种实验，并证明了我们的方法与现有的三种方法相比，能够更高效地返回结果，并且更加符合原始数据抽象。<details>
<summary>Abstract</summary>
Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generated in latent space by linear interpolation between the query sample and the centroid of the target class. We show that our method maintains the characteristics of the input sample during the counterfactual search. In various experiments, we show that the proposed method is competitive based on different quality measures on image and tabular datasets -- efficiently returns results that are closer to the original data manifold compared to three state-of-the-art methods, which are essential for realistic high-dimensional machine learning applications.
</details>
<details>
<summary>摘要</summary>
counterfactual explanations (CEs) 是 algorithmic recourse 中的一个重要工具，用于回答以下两个问题：1. 自动预测/决策中的重要因素是什么？2. 如何变化这些因素以获得更有利的结果从用户的角度？因此，为AI系统的使用者提供易于理解的解释和实现可行的改变是AI系统的信任采用和长期接受的重要前提。在文献中，多种方法已经被提出供生成 CE，并提出了不同的质量指标来评估这些方法。然而，生成 CE 通常是 computationally 昂贵的，并且从生成的建议中返回的结果通常是不现实的，因此无法实际应用。在这篇文章中，我们提出了一新的方法，用于生成一个预训 binary 分类器 的 CE。我们首先将 autoencoder 的潜在空间变成一个 Gaussian 分布的混合体。然后，我们在潜在空间中使用 straight  interpolate 生成 CE。我们显示了我们的方法可以保持输入样本的特征。在多个实验中，我们显示了我们的方法与其他三种现今最佳方法相比，在像素和数据Dataset 上表现更好，能够高效地返回更加接近原始数据构造的结果。
</details></li>
</ul>
<hr>
<h2 id="BotHawk-An-Approach-for-Bots-Detection-in-Open-Source-Software-Projects"><a href="#BotHawk-An-Approach-for-Bots-Detection-in-Open-Source-Software-Projects" class="headerlink" title="BotHawk: An Approach for Bots Detection in Open Source Software Projects"></a>BotHawk: An Approach for Bots Detection in Open Source Software Projects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13386">http://arxiv.org/abs/2307.13386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bifenglin/bothawk">https://github.com/bifenglin/bothawk</a></li>
<li>paper_authors: Fenglin Bi, Zhiwei Zhu, Wei Wang, Xiaoya Xia, Hassan Ali Khan, Peng Pu<br>for: This research aims to investigate bots’ behavior in open-source software projects and identify bot accounts with maximum possible accuracy.methods: The team gathered a dataset of 19,779 accounts that meet standardized criteria, and analyzed their behavior across 17 features in 5 dimensions. They also created a model called BotHawk, which outperforms other models in detecting bots.results: The team identified four types of bot accounts in open-source software projects, and found that the number of followers, number of repositories, and tags contain the most relevant features to identify the account type. The BotHawk model achieved an AUC of 0.947 and an F1-score of 0.89.<details>
<summary>Abstract</summary>
Social coding platforms have revolutionized collaboration in software development, leading to using software bots for streamlining operations. However, The presence of open-source software (OSS) bots gives rise to problems including impersonation, spamming, bias, and security risks. Identifying bot accounts and behavior is a challenging task in the OSS project. This research aims to investigate bots' behavior in open-source software projects and identify bot accounts with maximum possible accuracy. Our team gathered a dataset of 19,779 accounts that meet standardized criteria to enable future research on bots in open-source projects. We follow a rigorous workflow to ensure that the data we collect is accurate, generalizable, scalable, and up-to-date. We've identified four types of bot accounts in open-source software projects by analyzing their behavior across 17 features in 5 dimensions. Our team created BotHawk, a highly effective model for detecting bots in open-source software projects. It outperforms other models, achieving an AUC of 0.947 and an F1-score of 0.89. BotHawk can detect a wider variety of bots, including CI/CD and scanning bots. Furthermore, we find that the number of followers, number of repositories, and tags contain the most relevant features to identify the account type.
</details>
<details>
<summary>摘要</summary>
社区代码平台已经革命化软件开发的协作方式，导致使用软件机器人来加速操作。然而，开源软件（OSS）机器人的存在引起了一些问题，包括隐匿、垃圾邮件、偏见和安全风险。标识机器人帐户和行为是在OSS项目中的挑战。本研究的目标是estigate机器人在开源软件项目中的行为，并尽可能准确地标识机器人帐户。我们的团队收集了19,779个符合标准化要求的帐户，以便未来研究机器人在开源项目中的行为。我们遵循了一个严格的工作流程，以确保我们收集的数据准确、可重复、可扩展和时效。我们通过分析17个特征在5个维度中，分类出了机器人帐户的四种类型。我们创建了BotHawk模型，可以高效地在开源软件项目中检测机器人。它比其他模型高效，其AUC为0.947，F1分数为0.89。BotHawk可以检测更多类型的机器人，包括CI/CD和扫描机器人。此外，我们发现帐户类型的最重要特征是粉丝数、Repository数和标签。
</details></li>
</ul>
<hr>
<h2 id="Scaff-PD-Communication-Efficient-Fair-and-Robust-Federated-Learning"><a href="#Scaff-PD-Communication-Efficient-Fair-and-Robust-Federated-Learning" class="headerlink" title="Scaff-PD: Communication Efficient Fair and Robust Federated Learning"></a>Scaff-PD: Communication Efficient Fair and Robust Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13381">http://arxiv.org/abs/2307.13381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaodong Yu, Sai Praneeth Karimireddy, Yi Ma, Michael I. Jordan</li>
<li>for: 提高 federated learning 中的公平性和Robustness</li>
<li>methods: 使用 accelerated primal dual (APD) 算法和 bias corrected local steps (as in Scaffold)</li>
<li>results: 在 Several benchmark datasets 上 demonstrate 了 Scaff-PD 的效果，可以提高 fairness 和 robustness，同时保持竞争性的准确性。Translation:</li>
<li>for: Improving fairness and robustness in federated learning</li>
<li>methods: Using accelerated primal dual (APD) algorithm and bias corrected local steps (as in Scaffold)</li>
<li>results: Demonstrated effectiveness in improving fairness and robustness while maintaining competitive accuracy on several benchmark datasets.<details>
<summary>Abstract</summary>
We present Scaff-PD, a fast and communication-efficient algorithm for distributionally robust federated learning. Our approach improves fairness by optimizing a family of distributionally robust objectives tailored to heterogeneous clients. We leverage the special structure of these objectives, and design an accelerated primal dual (APD) algorithm which uses bias corrected local steps (as in Scaffold) to achieve significant gains in communication efficiency and convergence speed. We evaluate Scaff-PD on several benchmark datasets and demonstrate its effectiveness in improving fairness and robustness while maintaining competitive accuracy. Our results suggest that Scaff-PD is a promising approach for federated learning in resource-constrained and heterogeneous settings.
</details>
<details>
<summary>摘要</summary>
我们提出了Scaff-PD算法，这是一种快速并且通信效率高的分布robust Federated学习算法。我们的方法可以提高公平性，通过优化适应于多样化客户端的分布robust目标函数。我们利用这些目标函数的特殊结构，并设计了一种加速的 primal dual（APD）算法，使用偏置修正的本地步骤（如Scaffold）来实现重要的通信效率和速度提升。我们对多个标准数据集进行了评估，并证明了Scaff-PD在保持竞争性下提高了公平性和Robustness。我们的结果表明，Scaff-PD是一种有前途的方法 дляFederated学习在资源受限和多样化环境中。Note: Please keep in mind that the translation is done by a machine and may not be perfect. It's always a good idea to have a human translator review the translation for accuracy.
</details></li>
</ul>
<hr>
<h2 id="Submodular-Reinforcement-Learning"><a href="#Submodular-Reinforcement-Learning" class="headerlink" title="Submodular Reinforcement Learning"></a>Submodular Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13372">http://arxiv.org/abs/2307.13372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manish-pra/non-additive-rl">https://github.com/manish-pra/non-additive-rl</a></li>
<li>paper_authors: Manish Prajapat, Mojmír Mutný, Melanie N. Zeilinger, Andreas Krause</li>
<li>for: 这个论文的目标是推广传统的奖励学习（RL），以处理具有减少奖励的应用场景，例如覆盖控制、实验设计和信息路径规划。</li>
<li>methods: 这个论文提出了一种新的RL方法，称为submodular RL（SubRL），它使用子模卷函数来模型具有减少奖励的奖励。在某些情况下，这种方法可以通过 Policy Gradient 算法来优化。</li>
<li>results: 论文的实验结果表明，SubPO 算法可以在具有减少奖励的情况下实现常量系数近似，并且可以在高维状态和动作空间中进行地方优化。此外，论文还应用了这种方法在生物多样性监测、极限试验设计、信息路径规划和覆盖最大化等应用场景中。<details>
<summary>Abstract</summary>
In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SubPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing SubRL instances even in large state- and action- spaces. We showcase the versatility of our approach by applying SubPO to several applications, such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.
</details>
<details>
<summary>摘要</summary>
在强化学习（RL）中，状态奖励通常被视为加性的，并且遵循Markov假设，它们是独立的状态所访问的。在许多重要应用中，如覆盖控制、实验设计和有用路径规划，奖励自然地具有递减返回，即在类似的状态上访问的奖励值逐渐减少。为解决这个问题，我们提出了“submodular RL”（SubRL），一种模型寻求优化更一般、非加性（历史相依）的奖励，使用submodular集合函数来捕捉递减返回。然而，在一般情况下，即使在表格式设定中，我们表明了优化问题的解决很难。然而，以古德理算法的成功在经典submodular优化中为启发，我们提出了SubPO，一种简单的policy梯度基本算法，用于处理非加性奖励。在一些假设下，SubPO可以在Markov决策过程（MDP）中获得优化的常数因子approximation。此外，我们还 deriv了一种自然的policy梯度方法，用于本地优化SubRL实例，即使在高维状态-动作空间中。我们在几个应用中应用SubPO，如生物多样性监测、权重实验设计、有用路径规划和覆盖最大化。我们的结果表明Sample效率和高维状态-动作空间的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Regions-of-Interest-for-Bayesian-Optimization-with-Adaptive-Level-Set-Estimation"><a href="#Learning-Regions-of-Interest-for-Bayesian-Optimization-with-Adaptive-Level-Set-Estimation" class="headerlink" title="Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation"></a>Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13371">http://arxiv.org/abs/2307.13371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengxue Zhang, Jialin Song, James Bowden, Alexander Ladd, Yisong Yue, Thomas A. Desautels, Yuxin Chen</li>
<li>for: 这个论文是关于 Bayesian 优化（BO）在高维和非站点场景中的研究。</li>
<li>methods: 该方法提议一个名为 BALLET 的框架，该框架可以自动过滤高信息区域（ROI），并使用两种 probabilistic 模型：一个粗细的 Gaussian process（GP）来识别 ROI，以及一个局部化的 GP 来优化在 ROI 中。</li>
<li>results: 该研究证明 BALLET 可以高效地缩小搜索空间，并可以比标准 BO 方法具有更紧的 regret  bound。该方法也在 synthetic 和实际优化任务上进行了 empirical 验证，并表现出了效果。<details>
<summary>Abstract</summary>
We study Bayesian optimization (BO) in high-dimensional and non-stationary scenarios. Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness. We propose a framework, called BALLET, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP). Our approach is easy to tune, and is able to focus on local region of the optimization space that can be tackled by existing BO methods. The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI. We show theoretically that BALLET can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering. We demonstrate empirically the effectiveness of BALLET on both synthetic and real-world optimization tasks.
</details>
<details>
<summary>摘要</summary>
我们研究 bayesian 优化（BO）在高维和不确定场景下。现有的算法通常需要大量的 гипер参数调整，这限制了它们在实际应用中的实用性。我们提议一种框架，called BALLET，它可以动态筛选高信度区域（ROI）作为一个超级集的非Parametric  probabilistic 模型，如 Gaussian process（GP）。我们的方法容易调整，可以专注于可以使用现有BO方法优化的本地区域。关键思想是使用两个 probabilistic 模型：一个粗细的 GP 来确定 ROI，并一个局部化的 GP 进行优化在 ROI 中。我们证明了 BALLET 可以有效减小搜索空间，并可以达到标准BO  без ROI 筛选的紧凑 regret  bound。我们通过 Synthetic 和实际优化任务的实验证明了 BALLET 的实际效果。
</details></li>
</ul>
<hr>
<h2 id="Computational-Guarantees-for-Doubly-Entropic-Wasserstein-Barycenters-via-Damped-Sinkhorn-Iterations"><a href="#Computational-Guarantees-for-Doubly-Entropic-Wasserstein-Barycenters-via-Damped-Sinkhorn-Iterations" class="headerlink" title="Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations"></a>Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13370">http://arxiv.org/abs/2307.13370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lénaïc Chizat, Tomas Vaškevičius</li>
<li>for:  computes doubly regularized Wasserstein barycenters, a type of entropic barycenter with inner and outer regularization strengths.</li>
<li>methods: uses damped Sinkhorn iterations followed by exact maximization&#x2F;minimization steps, with an inexact variant that uses approximate Monte Carlo sampling for discrete point clouds.</li>
<li>results: guarantees convergence for any choice of regularization parameters, and provides non-asymptotic convergence guarantees for approximating Wasserstein barycenters in the free-support&#x2F;grid-free setting.<details>
<summary>Abstract</summary>
We study the computation of doubly regularized Wasserstein barycenters, a recently introduced family of entropic barycenters governed by inner and outer regularization strengths. Previous research has demonstrated that various regularization parameter choices unify several notions of entropy-penalized barycenters while also revealing new ones, including a special case of debiased barycenters. In this paper, we propose and analyze an algorithm for computing doubly regularized Wasserstein barycenters. Our procedure builds on damped Sinkhorn iterations followed by exact maximization/minimization steps and guarantees convergence for any choice of regularization parameters. An inexact variant of our algorithm, implementable using approximate Monte Carlo sampling, offers the first non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support/grid-free setting.
</details>
<details>
<summary>摘要</summary>
我们研究 doubly 调整的 Wasserstein 中心的计算，这是最近引入的一家 entropic 中心，受内部和外部调整强度控制。先前的研究已经显示出不同的调整参数选择可以统一多种 entropy-penalized 中心，同时还会揭示出新的一些中心，包括特殊情况下的 debiased 中心。在这篇论文中，我们提出了一个算法来计算 doubly 调整的 Wasserstein 中心。我们的程序基于抑制 Sinkhorn 迭代 followed by exact maximization/minimization 步骤，并给出了任何调整参数的 convergence 保证。一个不精确的variant of our algorithm，可以使用 approximate Monte Carlo sampling 进行 Implement，提供了免 asymptotic convergence guarantees 的 approximating Wasserstein 中心 between discrete point clouds in the free-support/grid-free setting。
</details></li>
</ul>
<hr>
<h2 id="Prot2Text-Multimodal-Protein’s-Function-Generation-with-GNNs-and-Transformers"><a href="#Prot2Text-Multimodal-Protein’s-Function-Generation-with-GNNs-and-Transformers" class="headerlink" title="Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers"></a>Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14367">http://arxiv.org/abs/2307.14367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, Michalis Vazirgiannis</li>
<li>for: 这 paper 的目的是提出一种新的蛋白质功能预测方法，即 Prot2Text，可以在文本化的方式下预测蛋白质的功能。</li>
<li>methods: 该方法使用 Graph Neural Networks(GNNs) 和 Large Language Models(LLMs)，在encoder-decoder框架中结合多种数据类型，包括蛋白质序列、结构和文本注释，以实现蛋白质功能的全面表示。</li>
<li>results: 经验表明，Prot2Text 可以准确预测蛋白质的功能，并且可以生成详细的文本描述。这些结果表明，将 GNNs 和 LLMs 融合在一起可以提供更准确的蛋白质功能预测工具。<details>
<summary>Abstract</summary>
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, enabling the generation of detailed and accurate descriptions. To evaluate our model, we extracted a multimodal protein dataset from SwissProt, and demonstrate empirically the effectiveness of Prot2Text. These results highlight the transformative impact of multimodal models, specifically the fusion of GNNs and LLMs, empowering researchers with powerful tools for more accurate prediction of proteins' functions. The code, the models and a demo will be publicly released.
</details>
<details>
<summary>摘要</summary>
大型生物系统的复杂性让一些科学家将其理解归类为不可思议任务。不同层次的挑战增加了这个任务的复杂度，其中之一是蛋白质功能预测。在过去的几年中，通过开发多种机器学习方法，有了显著的进步。然而，大多数现有方法将任务定义为多类别问题，即将蛋白质分配预定的标签。在这项工作中，我们提出了一种新的方法，即Prot2Text，它预测蛋白质功能在自由文本风格下，超越了传统的二分或 categorical 分类。我们将GNNs和LLMs组合在encoder-decoder框架中，以整合蛋白质序列、结构和文本注释等多种数据类型。这种多模态方法允许对蛋白质功能的整体表示，从而生成详细和准确的描述。为评估我们的模型，我们从SwissProt中提取了多模态蛋白质数据集，并通过实验证明Prot2Text的效果。这些结果highlight了多模态模型的转变性，特别是GNNs和LLMs的融合，为研究人员提供了具有准确预测蛋白质功能的强大工具。代码、模型和demo将公开发布。
</details></li>
</ul>
<hr>
<h2 id="High-Dimensional-Distributed-Gradient-Descent-with-Arbitrary-Number-of-Byzantine-Attackers"><a href="#High-Dimensional-Distributed-Gradient-Descent-with-Arbitrary-Number-of-Byzantine-Attackers" class="headerlink" title="High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers"></a>High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13352">http://arxiv.org/abs/2307.13352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puning Zhao, Zhiguo Wan</li>
<li>for: 此研究旨在解决robust分布式学习中的Byzantine故障问题，特别是在高维问题上。</li>
<li>methods: 我们提出了一种适合高维问题的新方法，基于直接高维半验证均值估计方法。我们首先 identific一个子空间，然后使用梯度向量上传worker机器进行均值值的估计，并使用auxiliary dataset进行均值值的估计。</li>
<li>results: 我们的 teoretic分析表明，我们的新方法具有最佳峰值统计率。特别是，与前一些工作相比，我们的方法在维度方面具有显著改进。<details>
<summary>Abstract</summary>
Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compared with previous works.
</details>
<details>
<summary>摘要</summary>
“强健分布式学习具有拜尼瑞发生的研究吸引了过去几年的广泛关注。然而，大多数现有方法受到维度为因素的问题，这问题随着现代机器学习模型的复杂程度而增加。在这篇论文中，我们设计了适合高维度问题的新方法，可以承受无限多个拜尼瑞攻击者。我们的设计核心在于直接高维度半验证平均值估计法。我们的想法是首先找到一个子空间，然后使用工作机器上上传的梯度 вектор估计横向的部分，而垂直于这个子空间的部分则使用辅助数据集估计。我们随后使用我们的新方法来聚合分布式学习问题。我们的理论分析显示，新方法具有最佳均衡的统计率。尤其是在前一些作品中，它具有更好的维度依赖性。”
</details></li>
</ul>
<hr>
<h2 id="Explainable-Disparity-Compensation-for-Efficient-Fair-Ranking"><a href="#Explainable-Disparity-Compensation-for-Efficient-Fair-Ranking" class="headerlink" title="Explainable Disparity Compensation for Efficient Fair Ranking"></a>Explainable Disparity Compensation for Efficient Fair Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14366">http://arxiv.org/abs/2307.14366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abraham Gale, Amélie Marian</li>
<li>for: 这篇论文目的是解决决策系统中的不公平问题，即因数据中存在偏见导致不同人群得到的结果不同。</li>
<li>methods: 该论文提出了一种可解释的数据驱动的补偿措施，通过给弱化组群体成员分配加分来减少排名函数中的偏见。</li>
<li>results: 该论文使用实际世界的学生招生和重犯率数据进行验证，并与现有的公平排名算法进行比较，结果表明其补偿措施可以有效地减少不公平性。<details>
<summary>Abstract</summary>
Ranking functions that are used in decision systems often produce disparate results for different populations because of bias in the underlying data. Addressing, and compensating for, these disparate outcomes is a critical problem for fair decision-making. Recent compensatory measures have mostly focused on opaque transformations of the ranking functions to satisfy fairness guarantees or on the use of quotas or set-asides to guarantee a minimum number of positive outcomes to members of underrepresented groups. In this paper we propose easily explainable data-driven compensatory measures for ranking functions. Our measures rely on the generation of bonus points given to members of underrepresented groups to address disparity in the ranking function. The bonus points can be set in advance, and can be combined, allowing for considering the intersections of representations and giving better transparency to stakeholders. We propose efficient sampling-based algorithms to calculate the number of bonus points to minimize disparity. We validate our algorithms using real-world school admissions and recidivism datasets, and compare our results with that of existing fair ranking algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Feature-Importance-Measurement-based-on-Decision-Tree-Sampling"><a href="#Feature-Importance-Measurement-based-on-Decision-Tree-Sampling" class="headerlink" title="Feature Importance Measurement based on Decision Tree Sampling"></a>Feature Importance Measurement based on Decision Tree Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13333">http://arxiv.org/abs/2307.13333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsudalab/dt-sampler">https://github.com/tsudalab/dt-sampler</a></li>
<li>paper_authors: Chao Huang, Diptesh Das, Koji Tsuda</li>
<li>for: 这篇论文是为了提高树式模型中特征重要性分析的可解性而提出的。</li>
<li>methods: 这篇论文使用了SAT基本法来测试特征重要性，具有 fewer parameters 和更高的可解性，适用于实际问题中的分析。</li>
<li>results: 论文的实验结果表明，DT-Sampler 可以提供更高的可解性和稳定性，并且在实际问题中具有更好的性能。<details>
<summary>Abstract</summary>
Random forest is effective for prediction tasks but the randomness of tree generation hinders interpretability in feature importance analysis. To address this, we proposed DT-Sampler, a SAT-based method for measuring feature importance in tree-based model. Our method has fewer parameters than random forest and provides higher interpretability and stability for the analysis in real-world problems. An implementation of DT-Sampler is available at https://github.com/tsudalab/DT-sampler.
</details>
<details>
<summary>摘要</summary>
随机森林是预测任务中有效的，但随机树生成的 Randomness 会降低特征重要性分析的可解性。为了解决这个问题，我们提出了 DT-Sampler，一种基于 SAT 的方法来测量树型模型中特征重要性。我们的方法有 fewer parameters than random forest，并提供更高的可解性和稳定性 для实际问题的分析。DT-Sampler 的实现可以在 GitHub 上找到：https://github.com/tsudalab/DT-sampler。
</details></li>
</ul>
<hr>
<h2 id="The-Optimal-Approximation-Factors-in-Misspecified-Off-Policy-Value-Function-Estimation"><a href="#The-Optimal-Approximation-Factors-in-Misspecified-Off-Policy-Value-Function-Estimation" class="headerlink" title="The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation"></a>The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13332">http://arxiv.org/abs/2307.13332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Amortila, Nan Jiang, Csaba Szepesvári</li>
<li>for: 这篇论文主要关注于 linear off-policy value function estimation 中的函数近似错误问题。</li>
<li>methods: 这篇论文使用了多种设定，包括Weighted $L_2$-norm、$L_\infty$ norm、state aliasing 和 full vs. partial coverage of the state space，来研究函数近似错误的优化因子。</li>
<li>results: 这篇论文确定了不同设定下的优化因子，并发现了两个实例特定的因子（在 $L_2(\mu)$  нор中）和一个常数（在 $L_\infty$  norm中），它们控制了在 misspecification 下的 off-policy 评估困难度。<details>
<summary>Abstract</summary>
Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -- especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.
</details>
<details>
<summary>摘要</summary>
理论保证在强化学习（RL）中知道会受到函数 aproximation 错误的多项式增长。然而，这些 \emph{approximation factor}  --特别是在某个学习问题中的优化形式-- 还不够了解。在这篇论文中，我们研究这个问题，具体来说是在线性off-policy值函数估计中。我们在多种设置下研究approximation factor，包括使用权重$L_2$ norm（其权重是在线上的状态分布）、$L_\infty$ norm、状态别名存在或不存在、完整 vs.  partial coverage的状态空间。我们Establish了所有这些设置的优化 asymptotic approximation factor（几乎 constants）。特别是，我们的 bound 透露了 $L_2(\mu)$ norm 中的两个实例特定因子，以及 $L_\infty$ norm 中的一个因子，它们在不正确的函数approximation下控制了强化评估的困难程度。
</details></li>
</ul>
<hr>
<h2 id="Unleash-the-Power-of-Context-Enhancing-Large-Scale-Recommender-Systems-with-Context-Based-Prediction-Models"><a href="#Unleash-the-Power-of-Context-Enhancing-Large-Scale-Recommender-Systems-with-Context-Based-Prediction-Models" class="headerlink" title="Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models"></a>Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01231">http://arxiv.org/abs/2308.01231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Hartman, Assaf Klein, Davorin Kopič, Natalia Silberstein</li>
<li>for: 这个研究旨在提出Context-Based Prediction Model，一种基于用户和上下文特征的预测模型，不考虑物品特征。</li>
<li>methods: 该模型采用用户和上下文特征进行预测，并在CTR预测模型中 incorporate其预测结果作为特征。</li>
<li>results: 实验表明，这种方法可以在大规模商业推荐系统中提高业务效果，而且对服务成本产生较少影响。<details>
<summary>Abstract</summary>
In this work, we introduce the notion of Context-Based Prediction Models. A Context-Based Prediction Model determines the probability of a user's action (such as a click or a conversion) solely by relying on user and contextual features, without considering any specific features of the item itself. We have identified numerous valuable applications for this modeling approach, including training an auxiliary context-based model to estimate click probability and incorporating its prediction as a feature in CTR prediction models. Our experiments indicate that this enhancement brings significant improvements in offline and online business metrics while having minimal impact on the cost of serving. Overall, our work offers a simple and scalable, yet powerful approach for enhancing the performance of large-scale commercial recommender systems, with broad implications for the field of personalized recommendations.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了含义基于预测模型。这种预测模型根据用户和上下文特征决定用户行为（如点击或购买），不考虑物品特定的特征。我们已经 indentified numerous valuable applications for this modeling approach，包括培养辅助context-based模型来估计点击概率，并将其预测作为ctr预测模型的特征。我们的实验表明，这种提升带来了显著的在线和OFFLINE商业指标的改善，而且对服务成本的影响很小。总的来说，我们的工作提供了一种简单、可扩展且强大的方法，用于提高大规模的商业推荐系统的性能，对个性化推荐领域产生了广泛的影响。
</details></li>
</ul>
<hr>
<h2 id="QuIP-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees"><a href="#QuIP-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees" class="headerlink" title="QuIP: 2-Bit Quantization of Large Language Models With Guarantees"></a>QuIP: 2-Bit Quantization of Large Language Models With Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13304">http://arxiv.org/abs/2307.13304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jerry-chee/quip">https://github.com/jerry-chee/quip</a></li>
<li>paper_authors: Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa</li>
<li>for: 这个论文研究大型自然语言模型（LLM）后期参数量化。</li>
<li>methods: 我们提出了一种新的量化方法，即异幂量化处理（QuIP），它基于论文的观察，即量化受益于异幂 веса和希尔бер特矩阵，即需要准确圆拟的重量和希尔бер特矩阵与坐标轴不对齐。QuIP包括两步：（1）适应圆拟过程，最小化一个quadratic proxy目标函数;（2）高效的预处理和后处理，使得重量和希尔бер特矩阵异幂。</li>
<li>results: 我们通过实验发现，我们的异幂预处理可以提高一些现有的量化算法，并且使用只有两个位数的量化方法生成可靠的结果。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/jerry-chee/QuIP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jerry-chee/QuIP上找到。</a><details>
<summary>Abstract</summary>
This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/jerry-chee/QuIP .
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Adaptive rounding procedure that minimizes a quadratic proxy objective.2. Efficient pre- and post-processing that ensures weight and Hessian incoherence through multiplication by random orthogonal matrices.We also provide the first theoretical analysis for an LLM-scale quantization algorithm and show that our theory applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/jerry-chee/QuIP">https://github.com/jerry-chee/QuIP</a>.</details></li>
</ol>
<hr>
<h2 id="Word-Sense-Disambiguation-as-a-Game-of-Neurosymbolic-Darts"><a href="#Word-Sense-Disambiguation-as-a-Game-of-Neurosymbolic-Darts" class="headerlink" title="Word Sense Disambiguation as a Game of Neurosymbolic Darts"></a>Word Sense Disambiguation as a Game of Neurosymbolic Darts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16663">http://arxiv.org/abs/2307.16663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiansi Dong, Rafet Sifa</li>
<li>for: 本研究旨在提出一种新的神经符号方法，以提高 Word Sense Disambiguation (WSD) 任务的性能。</li>
<li>methods: 该方法基于一种嵌入式的 Configuration of Nested Balls (CNB) 模型，通过将单词嵌入与符号关系编码为包含关系的包囊中的位置来实现。</li>
<li>results: 在六组测试数据集上，该方法的实验结果在 90.1% 到 100.0% 之间，较前者的深度学习方法提高了 WSD 任务的性能。<details>
<summary>Abstract</summary>
Word Sense Disambiguation (WSD) is one of the hardest tasks in natural language understanding and knowledge engineering. The glass ceiling of 80% F1 score is recently achieved through supervised deep-learning, enriched by a variety of knowledge graphs. Here, we propose a novel neurosymbolic methodology that is able to push the F1 score above 90%. The core of our methodology is a neurosymbolic sense embedding, in terms of a configuration of nested balls in n-dimensional space. The centre point of a ball well-preserves word embedding, which partially fix the locations of balls. Inclusion relations among balls precisely encode symbolic hypernym relations among senses, and enable simple logic deduction among sense embeddings, which cannot be realised before. We trained a Transformer to learn the mapping from a contextualized word embedding to its sense ball embedding, just like playing the game of darts (a game of shooting darts into a dartboard). A series of experiments are conducted by utilizing pre-training n-ball embeddings, which have the coverage of around 70% training data and 75% testing data in the benchmark WSD corpus. The F1 scores in experiments range from 90.1% to 100.0% in all six groups of test data-sets (each group has 4 testing data with different sizes of n-ball embeddings). Our novel neurosymbolic methodology has the potential to break the ceiling of deep-learning approaches for WSD. Limitations and extensions of our current works are listed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Modify-Training-Directions-in-Function-Space-to-Reduce-Generalization-Error"><a href="#Modify-Training-Directions-in-Function-Space-to-Reduce-Generalization-Error" class="headerlink" title="Modify Training Directions in Function Space to Reduce Generalization Error"></a>Modify Training Directions in Function Space to Reduce Generalization Error</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13290">http://arxiv.org/abs/2307.13290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Yu, Wenlian Lu, Boyu Chen</li>
<li>for: 本文研究了一种基于神经网络函数空间的修改自然梯度下降方法的理论分析。</li>
<li>methods: 本文使用了 eigendecompositions 和统计学理论来Derive the generalization error of the learned neural network function,并提出了一种基于 eigenspace 的权衡标度方法来降低总化预测误差。</li>
<li>results: 本文通过 theoretically 分析和数据示例证明了，修改神经网络函数空间的培训方向可以降低总化预测误差。此外，本文还能够解释许多现有的泛化提升方法的理论基础。<details>
<summary>Abstract</summary>
We propose theoretical analyses of a modified natural gradient descent method in the neural network function space based on the eigendecompositions of neural tangent kernel and Fisher information matrix. We firstly present analytical expression for the function learned by this modified natural gradient under the assumptions of Gaussian distribution and infinite width limit. Thus, we explicitly derive the generalization error of the learned neural network function using theoretical methods from eigendecomposition and statistics theory. By decomposing of the total generalization error attributed to different eigenspace of the kernel in function space, we propose a criterion for balancing the errors stemming from training set and the distribution discrepancy between the training set and the true data. Through this approach, we establish that modifying the training direction of the neural network in function space leads to a reduction in the total generalization error. Furthermore, We demonstrate that this theoretical framework is capable to explain many existing results of generalization enhancing methods. These theoretical results are also illustrated by numerical examples on synthetic data.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于神经网络函数空间的修改自然梯度下降方法的理论分析。我们首先提出了在假设 Gaussian 分布和无限宽限下的分析表达，从而显式地计算出学习的神经网络函数中的总泛化误差。通过对函数空间的各个 eigenspace 的总泛化误差进行分解，我们提出了一个均衡误差的准则。我们还证明了，通过在函数空间中修改训练方向，可以降低总泛化误差。此外，我们还证明了这种理论框架能够解释许多现有的泛化提升方法的结果。这些理论结果也通过synthetic数据的数值示例进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Curvature-based-Transformer-for-Molecular-Property-Prediction"><a href="#Curvature-based-Transformer-for-Molecular-Property-Prediction" class="headerlink" title="Curvature-based Transformer for Molecular Property Prediction"></a>Curvature-based Transformer for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13275">http://arxiv.org/abs/2307.13275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yili Chen, Zhengyu Li, Zheng Wan, Hui Yu, Xian Wei</li>
<li>For: 该研究旨在提高基于图gram neural网络模型的分子属性预测能力，以提高人工智能化药物设计的效果。* Methods: 该研究提出了 Curvature-based Transformer，通过在图грам数据中添加 ricci 曲率信息作为节点特征来增强模型对结构信息的抽象能力。* Results: 对化学分子数据集PCQM4M-LST、MoleculeNet进行了实验，并与Uni-Mol、Graphormer等模型进行比较，结果表明，该方法可以达到领域内最佳效果。此外， ricci 曲率还能够反映结构和功能关系，描述图грам分子数据的局部几何结构。<details>
<summary>Abstract</summary>
The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments on chemical molecular datasets including PCQM4M-LST, MoleculeNet and compared with models such as Uni-Mol, Graphormer, and the results show that this method can achieve the state-of-the-art results. It is proved that the discretized Ricci curvature also reflects the structural and functional relationship while describing the local geometry of the graph molecular data.
</details>
<details>
<summary>摘要</summary>
“分子质量预测是人工智能基于药物设计的一个最重要和挑战性任务。现今主流方法中，最常用的特征表示方法是基于SMILES和分子图，although这些方法简洁有效，但它们也限制了捕捉空间信息的能力。在这项工作中，我们提出了Curvature-based Transformer来提高Graph Transformer神经网络模型在分子图数据上提取结构信息的能力，通过引入Discretization of Ricci Curvature。为了将曲率信息 embed到模型中，我们在计算注意力分数时将节点特征中的曲率信息作为 позициональ编码。这种方法可以在不改变原始网络架构的情况下，将曲率信息从分子图数据中引入到模型中，并且它有可能可以扩展到其他模型。我们在化学分子数据集PCQM4M-LST、MoleculeNet上进行了实验，并与Uni-Mol、Graphormer等模型进行比较，结果显示，这种方法可以达到领先的结果。这也证明了离散的 Ricci 曲率不仅描述了分子图数据的本地几何结构，而且还反映了结构和功能关系。”
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Weight-Maximization"><a href="#Unbiased-Weight-Maximization" class="headerlink" title="Unbiased Weight Maximization"></a>Unbiased Weight Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13270">http://arxiv.org/abs/2307.13270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Chung<br>for:这篇论文的目的是提出一种生物学上可能的人工神经网络（ANN）训练方法，即将每个单元视为一个随机反馈学习（RL）代理，从而将网络视为一群代理。methods:该训练方法使用的是REINFORCE算法，一种本地学习规则，它是基于全局奖励信号的抑制学习。然而，这种学习方法通常很慢，并且随着网络大小增加，它的效率会降低，因为不具有有效的结构归因分配。为解决这个问题，提出了质量最大化方法，它将每个隐藏单元的奖励信号更改为该单元的出going权重的 нор，从而让每个隐藏单元可以最大化其出going权重的norm。results:在这篇研究报告中，我们分析了质量最大化方法的理论性质，并提出了一种变体，即偏向最大化方法。这种新方法提供了一种偏向学习规则，它可以增加学习速度和改善最终性能。值得注意的是，到我们所知，这是第一种可以快速学习，并且与网络单元数量成直比的人工神经网络训练方法。<details>
<summary>Abstract</summary>
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. Nevertheless, this learning method is often slow and scales poorly with network size due to inefficient structural credit assignment, since a single reward signal is broadcast to all units without considering individual contributions. Weight Maximization, a proposed solution, replaces a unit's reward signal with the norm of its outgoing weight, thereby allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal. In this research report, we analyze the theoretical properties of Weight Maximization and propose a variant, Unbiased Weight Maximization. This new approach provides an unbiased learning rule that increases learning speed and improves asymptotic performance. Notably, to our knowledge, this is the first learning rule for a network of Bernoulli-logistic units that is unbiased and scales well with the number of network's units in terms of learning speed.
</details>
<details>
<summary>摘要</summary>
一种生物学可能性的方法 для训练人工神经网络（ANN）是将每个单元视为一个随机奖励学习（RL）代理，从而考虑神经网络为一支代理队伍。因此，所有单元都可以通过REINFORCE本地学习规则，该规则由全局奖励信号调整，更加匹配生物观察到的 synaptic plasticity 形式。然而，这种学习方法通常慢并且与网络大小不好扩展，由于不效的结构归因分配。Weight Maximization，一种提议的解决方案，将单元的奖励信号替换为出向量的 нор，以此来使每个隐藏单元可以最大化出向量的 norm 而不是全局奖励信号。在这份研究报告中，我们分析Weight Maximization的理论属性并提出一种变体，即不偏向的Weight Maximization。这种新的学习规则提供了一种不偏向的学习规则，提高学习速度和长期性表现。值得注意的是，到我们所知，这是一种可以快速学习和与神经网络单元数量成正比的学习规则。
</details></li>
</ul>
<hr>
<h2 id="Federated-K-Means-Clustering-via-Dual-Decomposition-based-Distributed-Optimization"><a href="#Federated-K-Means-Clustering-via-Dual-Decomposition-based-Distributed-Optimization" class="headerlink" title="Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization"></a>Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13267">http://arxiv.org/abs/2307.13267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vassilios Yfantis, Achim Wagner, Martin Ruskowski</li>
<li>for: 本文旨在探讨分布式优化在机器学习中的应用，以保持隐私或提高计算效率为动机。</li>
<li>methods: 本文使用分布式算法来训练全球模型，其中每个节点只有访问自己的数据。此外，由于数据量的增加，大规模机器学习问题需要分布式训练。</li>
<li>results: 本文使用各种算法来评估分布式训练的性能，包括梯度下降法、Bundle Trust方法和准降次射线法。然而，分布式程序的整数放松问题受到强烈的限制，但是该方法可能会在未来实现高效的解决方案，在中央和分布式设置中。<details>
<summary>Abstract</summary>
The use of distributed optimization in machine learning can be motivated either by the resulting preservation of privacy or the increase in computational efficiency. On the one hand, training data might be stored across multiple devices. Training a global model within a network where each node only has access to its confidential data requires the use of distributed algorithms. Even if the data is not confidential, sharing it might be prohibitive due to bandwidth limitations. On the other hand, the ever-increasing amount of available data leads to large-scale machine learning problems. By splitting the training process across multiple nodes its efficiency can be significantly increased. This paper aims to demonstrate how dual decomposition can be applied for distributed training of $ K $-means clustering problems. After an overview of distributed and federated machine learning, the mixed-integer quadratically constrained programming-based formulation of the $ K $-means clustering training problem is presented. The training can be performed in a distributed manner by splitting the data across different nodes and linking these nodes through consensus constraints. Finally, the performance of the subgradient method, the bundle trust method, and the quasi-Newton dual ascent algorithm are evaluated on a set of benchmark problems. While the mixed-integer programming-based formulation of the clustering problems suffers from weak integer relaxations, the presented approach can potentially be used to enable an efficient solution in the future, both in a central and distributed setting.
</details>
<details>
<summary>摘要</summary>
使用分布式优化在机器学习中可以受到保持隐私或提高计算效率的两种动机。一方面，训练数据可能会被存储在多个设备上。在一个网络中，每个节点只有访问自己的Confidential数据的训练全球模型需要使用分布式算法。即使数据不是Confidential，因为带宽限制而共享它们可能是不可能的。另一方面，可用数据的量在不断增长，导致大规模机器学习问题。通过将训练过程分布在多个节点上，可以显著提高效率。本文旨在说明如何使用分布式优化解决分布式训练$ K $-mean clustering问题。首先是分布式和联邦机器学习的概述，然后是基于杂 integer quadratic programming 的 $ K $-mean clustering训练问题的混合数学表述。在这个表述中，数据可以被分布在不同的节点上，并通过协调约束相互连接。最后，在一组 benchmark 问题上评估了批量梯度法、杂 bundle trust 法和 quasi-Newton dual ascent 算法的性能。虽然分布式 programming 基于的 clustering问题的 relaxation 强度弱，但这种方法可能可以在未来在中心和分布式设置中实现高效的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Federated-Split-Learning-with-Only-Positive-Labels-for-resource-constrained-IoT-environment"><a href="#Federated-Split-Learning-with-Only-Positive-Labels-for-resource-constrained-IoT-environment" class="headerlink" title="Federated Split Learning with Only Positive Labels for resource-constrained IoT environment"></a>Federated Split Learning with Only Positive Labels for resource-constrained IoT environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13266">http://arxiv.org/abs/2307.13266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Praveen Joshi, Chandra Thapa, Mohammed Hasanuzzaman, Ted Scully, Haithem Afli</li>
<li>for: 这则研究的目的是提出一个叫做 splitfed learning with positive labels (SFPL) 的方法，用于在资源有限的 IoT 设备上执行分布式合作机器学习 (DCML)，以提高资料隐私和模型训练效率。</li>
<li>methods: SFPL 使用了随机排序函数将客户端获取的抛裂数据Random shuffling 以及在推断阶段使用本地批量normalization 来提高客户端模型的精度。</li>
<li>results: SFPL 比 SFL 提高了51.54 和 32.57 倍的误差率，对于 CIFAR-100 和 CIFAR-10  dataset 分别进行了评估，并且获得了更好的结果。<details>
<summary>Abstract</summary>
Distributed collaborative machine learning (DCML) is a promising method in the Internet of Things (IoT) domain for training deep learning models, as data is distributed across multiple devices. A key advantage of this approach is that it improves data privacy by removing the necessity for the centralized aggregation of raw data but also empowers IoT devices with low computational power. Among various techniques in a DCML framework, federated split learning, known as splitfed learning (SFL), is the most suitable for efficient training and testing when devices have limited computational capabilities. Nevertheless, when resource-constrained IoT devices have only positive labeled data, multiclass classification deep learning models in SFL fail to converge or provide suboptimal results. To overcome these challenges, we propose splitfed learning with positive labels (SFPL). SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server for model training. Additionally, SFPL incorporates the local batch normalization for the client-side model portion during the inference phase. Our results demonstrate that SFPL outperforms SFL: (i) by factors of 51.54 and 32.57 for ResNet-56 and ResNet-32, respectively, with the CIFAR-100 dataset, and (ii) by factors of 9.23 and 8.52 for ResNet-32 and ResNet-8, respectively, with CIFAR-10 dataset. Overall, this investigation underscores the efficacy of the proposed SFPL framework in DCML.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传输给定文本到简化中文。<</SYS>>分布式合作机器学习（DCML）是互联网物联网（IoT）领域的一种有前途的方法，用于训练深度学习模型，因为数据分布在多个设备上。DCML的一个优点是改善数据隐私，从而消除中央化数据汇集的必要性，同时赋能低功耗的IoT设备。在DCML框架中， federated split learning（SFL）是最适合高效地训练和测试，当设备有限的计算能力时。然而，当IoT设备有限的计算能力时，SFL中的多类分类深度学习模型会失败或提供低优的结果。为了解决这些挑战，我们提出了 splitfed learning with positive labels（SFPL）。SFPL在客户端收到服务器发送的数据后，随机混淆该数据，然后将其传递给服务器进行模型训练。此外，SFPL在推理阶段在客户端模型部分中加入本地批处理。我们的结果表明，SFPL比SFL：（i）在CIFAR-100数据集上，ResNet-56和ResNet-32分别提高了51.54倍和32.57倍的性能；（ii）在CIFAR-10数据集上，ResNet-32和ResNet-8分别提高了9.23倍和8.52倍的性能。总的来说，本研究证明SFPL框架在DCML中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Structural-Credit-Assignment-with-Coordinated-Exploration"><a href="#Structural-Credit-Assignment-with-Coordinated-Exploration" class="headerlink" title="Structural Credit Assignment with Coordinated Exploration"></a>Structural Credit Assignment with Coordinated Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13256">http://arxiv.org/abs/2307.13256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Chung</li>
<li>for: 论文旨在提出一种生物学上有效的人工神经网络（ANN）训练方法，即将每个单元当作一个随机激励学习（RL）代理，从而考虑网络为一组代理。</li>
<li>methods: 该方法使用REINFORCE本地学习规则，即在全球奖励信号的修饰下，使每个单元进行学习。然而，这种学习方法的启发速度较慢，并且不能够适应网络的大小。这是由两种因素引起的：首先，所有单元独立地探索网络，其次，网络中的所有单元都用同一个奖励来评估其行为。</li>
<li>results: 作者提出了一种改进结构信用分配的方法，即使用博尔茨曼机或循环网络进行协调探索。实验结果表明，协调探索可以大幅提高多个随机和离散单元基于REINFORCE的训练速度，甚至超过STE归散梯度渐进法。<details>
<summary>Abstract</summary>
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. However, this learning method tends to be slow and does not scale well with the size of the network. This inefficiency arises from two factors impeding effective structural credit assignment: (i) all units independently explore the network, and (ii) a single reward is used to evaluate the actions of all units. Accordingly, methods aimed at improving structural credit assignment can generally be classified into two categories. The first category includes algorithms that enable coordinated exploration among units, such as MAP propagation. The second category encompasses algorithms that compute a more specific reward signal for each unit within the network, like Weight Maximization and its variants. In this research report, our focus is on the first category. We propose the use of Boltzmann machines or a recurrent network for coordinated exploration. We show that the negative phase, which is typically necessary to train Boltzmann machines, can be removed. The resulting learning rules are similar to the reward-modulated Hebbian learning rule. Experimental results demonstrate that coordinated exploration significantly exceeds independent exploration in training speed for multiple stochastic and discrete units based on REINFORCE, even surpassing straight-through estimator (STE) backpropagation.
</details>
<details>
<summary>摘要</summary>
一种生物学可能性的人工神经网络（ANN）训练方法是将每个单元视为随机奖励学习（RL）代理，从而考虑网络为一支代理队伍。因此，所有单元都可以通过REINFORCE本地学习规则，该规则由全局奖励信号修饰，更加匹配生物观察到的神经元某些塑性变化。然而，这种学习方法通常慢和网络大小不好扩展。这种慢速是由两个因素引起的：（i）所有单元独立探索网络，以及（ii）网络中所有单元的行为都由单一奖励评价。因此，可以分为两类方法来改进结构准确评价：（i）使单元之间协调探索的算法，如MAP协调传播；（ii）计算网络中每个单元的更加特定奖励信号，如重量最大化和其变种。在这份研究报告中，我们关注第一类方法。我们提议使用博尔ツ曼机或回归网络来实现协调探索。我们发现，通常需要训练博尔ツ曼机的负阶段可以消除。结果的学习规则与奖励修饰的麦克斯韦学习规则相似。实验结果表明，通过协调探索，独立探索的训练速度在多个随机离散单元基于REINFORCE上明显超过直通透归推导（STE）反射归推。
</details></li>
</ul>
<hr>
<h2 id="RoSAS-Deep-Semi-Supervised-Anomaly-Detection-with-Contamination-Resilient-Continuous-Supervision"><a href="#RoSAS-Deep-Semi-Supervised-Anomaly-Detection-with-Contamination-Resilient-Continuous-Supervision" class="headerlink" title="RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision"></a>RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13239">http://arxiv.org/abs/2307.13239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuhongzuo/rosas">https://github.com/xuhongzuo/rosas</a></li>
<li>paper_authors: Hongzuo Xu, Yijie Wang, Guansong Pang, Songlei Jian, Ning Liu, Yongjun Wang</li>
<li>for: 本研究目的是提出一种新的半监督异常检测方法，以适应异常检测中的异常污染和不同异常检测环境。</li>
<li>methods: 本方法使用了一种新的杂化积分方法，通过Diffuse the abnormality of labeled anomalies，创建了新的数据样本，并使用了一种特征学习基于的目标函数来强化网络的 robustness。</li>
<li>results: 对11个实际世界数据集进行了广泛的实验，结果显示，our approach在AUC-PR指标上比前STATE-OF-THE-ART竞争对手提高20%-30%，并在不同的异常检测环境下表现更加稳定和高效。<details>
<summary>Abstract</summary>
Semi-supervised anomaly detection methods leverage a few anomaly examples to yield drastically improved performance compared to unsupervised models. However, they still suffer from two limitations: 1) unlabeled anomalies (i.e., anomaly contamination) may mislead the learning process when all the unlabeled data are employed as inliers for model training; 2) only discrete supervision information (such as binary or ordinal data labels) is exploited, which leads to suboptimal learning of anomaly scores that essentially take on a continuous distribution. Therefore, this paper proposes a novel semi-supervised anomaly detection method, which devises \textit{contamination-resilient continuous supervisory signals}. Specifically, we propose a mass interpolation method to diffuse the abnormality of labeled anomalies, thereby creating new data samples labeled with continuous abnormal degrees. Meanwhile, the contaminated area can be covered by new data samples generated via combinations of data with correct labels. A feature learning-based objective is added to serve as an optimization constraint to regularize the network and further enhance the robustness w.r.t. anomaly contamination. Extensive experiments on 11 real-world datasets show that our approach significantly outperforms state-of-the-art competitors by 20%-30% in AUC-PR and obtains more robust and superior performance in settings with different anomaly contamination levels and varying numbers of labeled anomalies. The source code is available at https://github.com/xuhongzuo/rosas/.
</details>
<details>
<summary>摘要</summary>
semi-supervised异常检测方法可以利用一些异常示例来获得显著提高的性能，比如无监督模型。然而，它们仍然受到两个限制：1）无标签异常（即异常污染）可能会在模型训练时对学习过程产生误导; 2）只利用数字化超级视图（如二进制或ORDinal数据标签），导致异常分数的学习变得不优化。因此，这篇论文提出了一种新的 semi-supervised异常检测方法，即“污染耐受连续超级指示信号”。具体来说，我们提出了一种混合 interpolate方法，将标注异常的异常性散布到新的数据样本中，从而创造了连续异常度标签。同时，污染区域可以通过新生成的数据样本，其中包含正确标签的数据，得到覆盖。我们还添加了一个特征学习基于的目标函数，以便在污染异常情况下进一步增强网络的稳定性。我们在11个实际 dataset上进行了广泛的实验，结果表明，我们的方法在 AUC-PR 方面比state-of-the-art竞争对手提高20%-30%，并且在不同的异常污染水平和异常数量的不同设置下表现出更加稳定和优秀的性能。代码可以在 https://github.com/xuhongzuo/rosas/ 获取。
</details></li>
</ul>
<hr>
<h2 id="Audio-aware-Query-enhanced-Transformer-for-Audio-Visual-Segmentation"><a href="#Audio-aware-Query-enhanced-Transformer-for-Audio-Visual-Segmentation" class="headerlink" title="Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation"></a>Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13236">http://arxiv.org/abs/2307.13236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinxiang Liu, Chen Ju, Chaofan Ma, Yanfeng Wang, Yu Wang, Ya Zhang</li>
<li>for: Audio-visual segmentation (AVS) task, specifically to segment sounding objects in video frames using audio cues.</li>
<li>methods: Introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features, plus an audio-aware query-enhanced transformer decoder that explicitly focuses on the segmentation of pinpointed sounding objects based on audio signals.</li>
<li>results: Outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.Here’s the Chinese version of the three key information points:</li>
<li>for: 专门用于音频类型的视觉分割任务，即使用音频讯号来分类视觉框架中的声音物体。</li>
<li>methods: 引入了一个多模式转换器架构，允许深度融合和总结多个音频和视觉特征，以及一个专注于音频讯号的查询增强转换器解码器，帮助模型更好地对声音物体进行分类。</li>
<li>results: 比前一代方法高效，并在多音和开集条件下表现出更好的扩展性。<details>
<summary>Abstract</summary>
The goal of the audio-visual segmentation (AVS) task is to segment the sounding objects in the video frames using audio cues. However, current fusion-based methods have the performance limitations due to the small receptive field of convolution and inadequate fusion of audio-visual features. To overcome these issues, we propose a novel \textbf{Au}dio-aware query-enhanced \textbf{TR}ansformer (AuTR) to tackle the task. Unlike existing methods, our approach introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features. Furthermore, we devise an audio-aware query-enhanced transformer decoder that explicitly helps the model focus on the segmentation of the pinpointed sounding objects based on audio signals, while disregarding silent yet salient objects. Experimental results show that our method outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.
</details>
<details>
<summary>摘要</summary>
系统的听音视频分割（AVS）任务的目标是使用听音信号来分割视频帧中的声音对象。然而，现有的融合方法受到小感知场和不充分融合听音视频特征的限制。为了解决这些问题，我们提出了一种新的听音感知扩展的transformer（AuTR）来解决这个任务。与现有方法不同，我们的方法使用多Modal transformer架构，允许深度融合和听音视频特征的汇集。此外，我们还开发了听音感知扩展transformer解码器，可以让模型在听音信号的指引下进行声音对象的分割，而忽略沉寂却突出的对象。实验结果表明，我们的方法在多音和开放集成方面的性能比前方法更高，并且在多音和开放集成方面表现更好。
</details></li>
</ul>
<hr>
<h2 id="Spectral-DP-Differentially-Private-Deep-Learning-through-Spectral-Perturbation-and-Filtering"><a href="#Spectral-DP-Differentially-Private-Deep-Learning-through-Spectral-Perturbation-and-Filtering" class="headerlink" title="Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering"></a>Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13231">http://arxiv.org/abs/2307.13231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ce Feng, Nuo Xu, Wujie Wen, Parv Venkitasubramaniam, Caiwen Ding</li>
<li>for: 本研究旨在提出一种新的干扰梯度学习方法，以实现在深度学习算法中保护个人隐私。</li>
<li>methods: 本方法combines gradient perturbation在spectral domain with spectral filtering以实现 desired privacy guarantee，并且采用了块环形based spatial restructuring来提高fully connected层的性能。</li>
<li>results: 通过对标准 benchmark datasets进行广泛的实验，研究者发现spectral-DP deep learning方法在训练从scratch和transfer learning Setting中具有更好的性能，并且提供了实现spectral-DP deep learning的指南。<details>
<summary>Abstract</summary>
Differential privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD). DP-SGD requires direct noise addition to every gradient in a dense neural network, the privacy is achieved at a significant utility cost. In this work, we present Spectral-DP, a new differentially private learning approach which combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines to implement Spectral-DP deep learning on benchmark datasets. In comparison with state-of-the-art DP-SGD based approaches, Spectral-DP is shown to have uniformly better utility performance in both training from scratch and transfer learning settings.
</details>
<details>
<summary>摘要</summary>
Diffusion privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD). DP-SGD requires direct noise addition to every gradient in a dense neural network, and privacy is achieved at a significant utility cost. In this work, we present Spectral-DP, a new differentially private learning approach that combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines to implement Spectral-DP deep learning on benchmark datasets. In comparison with state-of-the-art DP-SGD based approaches, Spectral-DP is shown to have uniformly better utility performance in both training from scratch and transfer learning settings.Here's the text with some minor adjustments to make it more natural in Simplified Chinese:Diffusion privacy is a widely accepted measure of privacy in deep learning algorithms, and achieving it relies on a noisy training approach called differentially private stochastic gradient descent (DP-SGD). DP-SGD adds noise to every gradient in a dense neural network, and privacy is achieved at the cost of utility. In this work, we present Spectral-DP, a new differentially private learning approach that combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we use a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines for implementing Spectral-DP deep learning on benchmark datasets. In comparison with state-of-the-art DP-SGD based approaches, Spectral-DP shows uniformly better utility performance in both training from scratch and transfer learning settings.
</details></li>
</ul>
<hr>
<h2 id="A-Primer-on-the-Data-Cleaning-Pipeline"><a href="#A-Primer-on-the-Data-Cleaning-Pipeline" class="headerlink" title="A Primer on the Data Cleaning Pipeline"></a>A Primer on the Data Cleaning Pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13219">http://arxiv.org/abs/2307.13219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca C. Steorts</li>
<li>for: 这篇论文主要用于介绍数据整理管道的科学，以及在实际应用中使用的技术和方法。</li>
<li>methods: 论文介绍了数据整理管道的四个阶段，包括数据清洁、数据预处理、数据合并和数据分析。同时，也介绍了一些常用的技术和方法，如数据匹配、数据适应和数据净化。</li>
<li>results: 论文提出了一些在实际应用中使用数据整理管道的示例和案例，并详细介绍了这些方法的优缺点和应用场景。<details>
<summary>Abstract</summary>
The availability of both structured and unstructured databases, such as electronic health data, social media data, patent data, and surveys that are often updated in real time, among others, has grown rapidly over the past decade. With this expansion, the statistical and methodological questions around data integration, or rather merging multiple data sources, has also grown. Specifically, the science of the ``data cleaning pipeline'' contains four stages that allow an analyst to perform downstream tasks, predictive analyses, or statistical analyses on ``cleaned data.'' This article provides a review of this emerging field, introducing technical terminology and commonly used methods.
</details>
<details>
<summary>摘要</summary>
在过去的一代，数据的可用性快速增长，包括电子健康数据、社交媒体数据、专利数据和调查等，这些数据经常在实时更新。这一增长也导致了数据集成问题的统计和方法问题的增长。特别是，数据清洁管道科学中的四个阶段，允许分析员在“清洁数据”上进行下游任务、预测分析或统计分析。本文提供了这个新兴领域的综述，介绍技术术语和常用方法。
</details></li>
</ul>
<hr>
<h2 id="FedMEKT-Distillation-based-Embedding-Knowledge-Transfer-for-Multimodal-Federated-Learning"><a href="#FedMEKT-Distillation-based-Embedding-Knowledge-Transfer-for-Multimodal-Federated-Learning" class="headerlink" title="FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning"></a>FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13214">http://arxiv.org/abs/2307.13214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy Q. Le, Minh N. H. Nguyen, Chu Myaet Thwal, Yu Qiao, Chaoning Zhang, Choong Seon Hong</li>
<li>for: 提出了一种基于联合学习的多Modal Federated Learning框架，以便在多个客户端上共同训练一个泛化的全局模型，而不需要分享私有数据。</li>
<li>methods: 该框架使用了 semi-supervised learning 方法，利用不同模式之间的表示来提高学习模型的性能。在系统中，我们实现了一种基于热退换的多Modal Embedding Knowledge Transfer机制（FedMEKT），该机制可以在服务器和客户端之间交换joint知识，从而更新全局核心Encoder。</li>
<li>results: 通过对三个多Modal人活动识别数据集进行了广泛的实验，我们证明了 FedMEKT 可以在线评估中实现全局Encoder的优秀表现，同时保护用户的隐私和个人数据，并且对于其他基elines比较有效。<details>
<summary>Abstract</summary>
Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal proxy dataset. Our FedMEKT iteratively updates the generalized global encoders with the joint embedding knowledge from the participating clients. Thereby, to address the modality discrepancy and labeled data constraint in existing FL systems, our proposed FedMEKT comprises local multimodal autoencoder learning, generalized multimodal autoencoder construction, and generalized classifier learning. Through extensive experiments on three multimodal human activity recognition datasets, we demonstrate that FedMEKT achieves superior global encoder performance on linear evaluation and guarantees user privacy for personal data and model parameters while demanding less communication cost than other baselines.
</details>
<details>
<summary>摘要</summary>
Federation learning (FL) 允许多个客户端共同训练一个泛化全球模型，无需分享私人数据。大多数现有的工作只是提出了一般的 FL 系统，因此它们在利用有价值的多模式数据方面受到限制。此外，大多数 FL 方法仍然依赖客户端上的标注数据，这在实际应用中很少可用因为用户自身标注困难。为了解决这些限制，我们提出了一个新的多模式 FL 框架，它使用 semi-supervised 学习方法来利用不同模式的表示。将这个概念引入到系统中，我们开发了一种名为 FedMEKT 的分配基于多模式嵌入知识传递机制，该机制 Allow the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal proxy dataset. Our FedMEKT iteratively updates the generalized global encoders with the joint embedding knowledge from the participating clients. In this way, we can address the modality discrepancy and labeled data constraint in existing FL systems. Our proposed FedMEKT consists of local multimodal autoencoder learning, generalized multimodal autoencoder construction, and generalized classifier learning. Through extensive experiments on three multimodal human activity recognition datasets, we demonstrate that FedMEKT achieves superior global encoder performance on linear evaluation and guarantees user privacy for personal data and model parameters while demanding less communication cost than other baselines.
</details></li>
</ul>
<hr>
<h2 id="Transferability-of-Graph-Neural-Networks-using-Graphon-and-Sampling-Theories"><a href="#Transferability-of-Graph-Neural-Networks-using-Graphon-and-Sampling-Theories" class="headerlink" title="Transferability of Graph Neural Networks using Graphon and Sampling Theories"></a>Transferability of Graph Neural Networks using Graphon and Sampling Theories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13206">http://arxiv.org/abs/2307.13206</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Martina Neuman, Jason J. Bramburger</li>
<li>for: 本研究旨在应用graphons来提高graph neural networks（GNNs）的可转移性，以便在不同的图ogram中使用已训练的GNN模型。</li>
<li>methods: 本研究使用了两层graphon neural network（WNN）架构，并证明了这个架构可以精确地预测带限的信号。此外，本研究还证明了WNN和GNN模型在所有足够大的图ogram中具有可转移性。</li>
<li>results: 本研究获得了一个可转移的GNN模型，可以在不同的图ogram中使用，并且不需要进行广泛的 retraining。此外，本研究还提供了一个实用的WNN和GNN架构，可以处理不同的图ogram大小，并且保持性能 guarantees。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains. A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy. A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs. In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture. We prove its ability to approximate bandlimited signals within a specified error tolerance using a minimal number of network weights. We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a sequence converging to a graphon. Our work addresses transferability between both deterministic weighted graphs and simple random graphs and overcomes issues related to the curse of dimensionality that arise in other GNN results. The proposed WNN and GNN architectures offer practical solutions for handling graph data of varying sizes while maintaining performance guarantees without extensive retraining.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 已成为处理图形信息的有力工具。一个愿望的特性是转移性，即一个训练的网络可以将信息从不同的图 swap 而不需要重新训练，并保持准确性。一种最近用于捕捉 GNN 的转移性是通过图ON 来实现，图ON 是 densely 连接的图的限制，表示大图的极限。在这项工作中，我们为 GNN 的图ON 应用提供了显式的两层图ON 神经网络（WNN）架构。我们证明它可以在指定误差容差内 aproximate 带有限制的信号，使用最小的网络重量。然后，我们利用这个结果，Establish  GNN 的转移性，包括 deterministic Weighted graph 和 simple random graph，并超越了其他 GNN 结果中的尺度端问题。提出的 WNN 和 GNN 架构可以实际地处理不同大小的图数据，保持性能保证，而无需进行大量重新训练。
</details></li>
</ul>
<hr>
<h2 id="Federated-Distributionally-Robust-Optimization-with-Non-Convex-Objectives-Algorithm-and-Analysis"><a href="#Federated-Distributionally-Robust-Optimization-with-Non-Convex-Objectives-Algorithm-and-Analysis" class="headerlink" title="Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis"></a>Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14364">http://arxiv.org/abs/2307.14364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Jiao, Kai Yang, Dongjin Song</li>
<li>for:  solve the federated distributionally robust optimization (FDRO) problem in a distributed environment with asynchronous updates and prior knowledge.</li>
<li>methods:  asynchronous distributed algorithm named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to handle asynchronous updates and prior knowledge.</li>
<li>results:  the proposed method can achieve fast convergence, remain robust against data heterogeneity and malicious attacks, and flexibly trade off robustness with performance.Here is the Chinese translation of the three points:</li>
<li>for: solve federated分布式robust优化（FDRO）问题在分布式环境中，带有异步更新和先验知识。</li>
<li>methods: 使用 asynchronous分布式算法Named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE)来处理异步更新和先验知识。</li>
<li>results: 提议的方法可以实现快速收敛，对数据不一致和攻击行为具有鲁棒性，并可以灵活地让强度与性能进行衡量。<details>
<summary>Abstract</summary>
Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, and remain robust against data heterogeneity as well as malicious attacks, but also tradeoff robustness with performance.
</details>
<details>
<summary>摘要</summary>
分布式可靠优化（DRO），旨在找到最优决策，以最小化不确定性集中的最差成本，已广泛应用于多个领域，如网络行为分析、风险管理等。然而，现有的DRO技术面临三大挑战：1）如何在分布式环境中处理异步更新；2）如何有效利用先验分布；3）如何适应不同场景下的稳健性。为此，我们提出了一种异步分布式算法，名为异步单独搅拌梯度投影（ASPIRE）算法，并与迭代活动集成方法（EASE）解决联邦分布式可靠优化（FDRO）问题。此外，我们开发了一种新的不确定集，即受限D-norm不确定集，以有效利用先验分布并flexibly控制稳健性水平。最后，我们的理论分析表明，提posed算法是确定 converges，并且iteration复杂度也进行了分析。实际研究表明，提posed方法可以不仅具有快速收敛，同时保持数据多样性和攻击的Robustness。
</details></li>
</ul>
<hr>
<h2 id="An-Investigation-into-Glomeruli-Detection-in-Kidney-H-E-and-PAS-Images-using-YOLO"><a href="#An-Investigation-into-Glomeruli-Detection-in-Kidney-H-E-and-PAS-Images-using-YOLO" class="headerlink" title="An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO"></a>An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13199">http://arxiv.org/abs/2307.13199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></li>
<li>paper_authors: Kimia Hemmatirad, Morteza Babaie, Jeffrey Hodgin, Liron Pantanowitz, H. R. Tizhoosh</li>
<li>for: 协助病理师使用计算机化解决方案，自动检测和分类 Digitale pathology 图像，以获得诊断结论。</li>
<li>methods: 使用 YOLO-v4（You-Only-Look-Once）实时物体检测器，这是一个单一神经网络，可以预测多个 bounding box 和物体标度的组合。</li>
<li>results: 这篇论文使用 YOLO-v4 进行人脸识别，并在两个公共数据集和一个私人数据集（来自密歇根大学）进行了多个实验，并在这个私人数据集上进行了外部验证，使用两种染色物质（HE和PAS）。结果显示，自动检测人脸在人脸识别中可能性很高。<details>
<summary>Abstract</summary>
Context: Analyzing digital pathology images is necessary to draw diagnostic conclusions by investigating tissue patterns and cellular morphology. However, manual evaluation can be time-consuming, expensive, and prone to inter- and intra-observer variability. Objective: To assist pathologists using computerized solutions, automated tissue structure detection and segmentation must be proposed. Furthermore, generating pixel-level object annotations for histopathology images is expensive and time-consuming. As a result, detection models with bounding box labels may be a feasible solution. Design: This paper studies. YOLO-v4 (You-Only-Look-Once), a real-time object detector for microscopic images. YOLO uses a single neural network to predict several bounding boxes and class probabilities for objects of interest. YOLO can enhance detection performance by training on whole slide images. YOLO-v4 has been used in this paper. for glomeruli detection in human kidney images. Multiple experiments have been designed and conducted based on different training data of two public datasets and a private dataset from the University of Michigan for fine-tuning the model. The model was tested on the private dataset from the University of Michigan, serving as an external validation of two different stains, namely hematoxylin and eosin (H&E) and periodic acid-Schiff (PAS). Results: Average specificity and sensitivity for all experiments, and comparison of existing segmentation methods on the same datasets are discussed. Conclusions: Automated glomeruli detection in human kidney images is possible using modern AI models. The design and validation for different stains still depends on variability of public multi-stain datasets.
</details>
<details>
<summary>摘要</summary>
Context: 分析数字 PATHOLOGY 图像是必要的，以便从 investigate 组织趋势和细胞形态中得出诊断结论。然而，手动评估可能是昂贵的、时间consuming 和 observer 间和 observer 内的变化。目标：通过计算机化解决方案，自动检测和分类组织结构。此外，生成 Histopathology 图像像素级对象注释是昂贵的和时间consuming。因此，使用 bounding box 标签的检测模型可能是一个可行的解决方案。设计：本文研究 YOLO-v4（You-Only-Look-Once），一种实时物体检测器，可以在微scopic 图像上预测多个 bounding box 和类别概率。YOLO 可以提高检测性能，通过训练整个扫描图像。本文使用 YOLO-v4 进行人肾脏中的膜体检测。多个实验基于不同的训练数据，包括两个公共数据集和一个来自大学Michigan的私人数据集进行精度调整。模型在大学Michigan的私人数据集上进行测试，作为两种不同染料（hematoxylin和eosin（H&E）和 periodic acid-Schiff（PAS））的外部验证。结果：对所有实验的特定性和敏感性的平均值，以及相同数据集上的现有分 segmentation 方法进行比较。结论：使用现代 AI 模型，自动检测人肾脏中的膜体是可能的。设计和验证不同染料仍然取决于多个公共多染料数据集的变化。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-enhanced-Neuro-Symbolic-AI-for-Cybersecurity-and-Privacy"><a href="#Knowledge-enhanced-Neuro-Symbolic-AI-for-Cybersecurity-and-Privacy" class="headerlink" title="Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy"></a>Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02031">http://arxiv.org/abs/2308.02031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aritran Piplai, Anantaa Kotal, Seyedreza Mohseni, Manas Gaur, Sudip Mittal, Anupam Joshi</li>
<li>for: The paper is written for those who are interested in developing explainable and safe AI systems, particularly in the domains of cybersecurity and privacy.</li>
<li>methods: The paper proposes the use of Neuro-Symbolic AI, which combines the strengths of deep neural networks and explicit symbolic knowledge to enhance explainability and safety in AI systems.</li>
<li>results: The paper highlights the potential benefits of Neuro-Symbolic AI in addressing the challenges of explainability and safety in cybersecurity and privacy applications, where the need for highly accurate and understandable AI systems is particularly important.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探讨如何开发可解释的、安全的人工智能系统，尤其是在防火墙和隐私两个领域。</li>
<li>methods: 论文提出使用神经符号智能，将神经网络的强点与知识图的优点相结合，以提高可解释性和安全性。</li>
<li>results: 论文指出神经符号智能在防火墙和隐私应用中可以减轻不可预测的风险，提高AI系统的可解释性和安全性。<details>
<summary>Abstract</summary>
Neuro-Symbolic Artificial Intelligence (AI) is an emerging and quickly advancing field that combines the subsymbolic strengths of (deep) neural networks and explicit, symbolic knowledge contained in knowledge graphs to enhance explainability and safety in AI systems. This approach addresses a key criticism of current generation systems, namely their inability to generate human-understandable explanations for their outcomes and ensure safe behaviors, especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity, privacy). The integration of neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows AI systems to reason, learn, and generalize in a manner understandable to experts. This article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for AI to be explainable while being highly accurate in complex environments, can benefit from Neuro-Symbolic AI.
</details>
<details>
<summary>摘要</summary>
neurosymbolic 人工智能（AI）是一个出现和快速发展的领域，它将深度神经网络和明确的符号知识包含在知识图中结合起来，以提高AI系统的可解释性和安全性。这种方法解决了现代AI系统的一个批评，即它们无法生成人类可理解的解释，特别是在“未知未知”（例如隐私、安全）的场景下。神经网络和符号知识图的集成，使AI系统可以理解、学习和泛化，并且可以由专家理解。本文介绍了如何在隐私和安全两个最高要求AI系统可解释性的领域中，应用Neuro-Symbolic AI。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanation-Policies-in-RL"><a href="#Counterfactual-Explanation-Policies-in-RL" class="headerlink" title="Counterfactual Explanation Policies in RL"></a>Counterfactual Explanation Policies in RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13192">http://arxiv.org/abs/2307.13192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shripad V. Deshmukh, Srivatsan R, Supriti Vijay, Jayakumar Subramanian, Chirag Agarwal</li>
<li>for: 本文旨在提供一种Counterfactual Explanation（Counterpol），用于解释RL策略的决策过程中的不同变量对策略表现的影响。</li>
<li>methods: 本文提出了一种 incorporating counterfactuals in supervised learning的方法，通过将愿景返回用于约束RL策略优化。同时，本文还建立了与广泛使用的信任区基本策略优化方法的理论连接。</li>
<li>results: 实验结果表明，Counterpol可以准确地生成对策略的解释，并且可以保持与原始策略几乎相同的性能。此外，本文在五个不同的RL环境中进行了广泛的实验，并证明了对不同状态和动作空间的RL环境的应用。<details>
<summary>Abstract</summary>
As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the efficacy of COUNTERPOL in generating explanations for (un)learning skills while keeping close to the original policy. Our results on five different RL environments with diverse state and action spaces demonstrate the utility of counterfactual explanations, paving the way for new frontiers in designing and developing counterfactual policies.
</details>
<details>
<summary>摘要</summary>
As Reinforcement Learning (RL) agents increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the efficacy of COUNTERPOL in generating explanations for (un)learning skills while keeping close to the original policy. Our results on five different RL environments with diverse state and action spaces demonstrate the utility of counterfactual explanations, paving the way for new frontiers in designing and developing counterfactual policies.Here's the translation in Traditional Chinese:当Reinforcement Learning（RL）机器人在不同的决策问题中使用奖励偏好时，确保RL政策learned by these frameworks in mapping observations to a probability distribution of the possible actions是可解释的成为越来越重要。然而，有很少或没有关于这些复杂政策的系统性理解，即所谓的对比性分析，即政策中的最小变化会提高/下降到所需的水平。在这种情况下，我们提出了Counterpol，RL中第一个使用对比性分析来分析RL政策的框架。我们通过在RL中 incorporating counterfactuals in supervised learning with the target outcome regulated using desired return来实现这一点。我们建立了RL中广泛使用信任区基本策优方法的理论连接。我们的实验表明，Counterpol可以在保持原始政策的情况下，生成对RL策略的解释，并且在五个不同的RL环境中进行了广泛的 empirical analysis。我们的结果表明，对RL策略的对比性分析可以提供有用的解释，开启新的前iers in designing and developing counterfactual policies。
</details></li>
</ul>
<hr>
<h2 id="Neural-Memory-Decoding-with-EEG-Data-and-Representation-Learning"><a href="#Neural-Memory-Decoding-with-EEG-Data-and-Representation-Learning" class="headerlink" title="Neural Memory Decoding with EEG Data and Representation Learning"></a>Neural Memory Decoding with EEG Data and Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13181">http://arxiv.org/abs/2307.13181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Glenn Bruns, Michael Haidar, Federico Rubino</li>
<li>for: 这种方法用于神经储存的快速识别，特别是从EEG数据中识别记忆。</li>
<li>methods: 该方法使用深度表示学习和指导对准损失来将EEG记录转换到低维度空间中。</li>
<li>results: 该方法可以在EEG数据中识别记忆，并且具有约78.4%的顶部准确率（机会为4%）。<details>
<summary>Abstract</summary>
We describe a method for the neural decoding of memory from EEG data. Using this method, a concept being recalled can be identified from an EEG trace with an average top-1 accuracy of about 78.4% (chance 4%). The method employs deep representation learning with supervised contrastive loss to map an EEG recording of brain activity to a low-dimensional space. Because representation learning is used, concepts can be identified even if they do not appear in the training data set. However, reference EEG data must exist for each such concept. We also show an application of the method to the problem of information retrieval. In neural information retrieval, EEG data is captured while a user recalls the contents of a document, and a list of links to predicted documents is produced.
</details>
<details>
<summary>摘要</summary>
我们描述了一种使用神经网络进行记忆解码的方法，使得从EEG数据中可以识别被回忆的概念。该方法使用深度表示学习，使EEG记录活动映射到低维度空间。由于使用表示学习，即使概念没有出现在训练数据集中，也可以识别出来。然而，需要对每个概念的参照EEG数据exist。我们还展示了该方法在信息检索问题中的应用，在神经信息检索中，EEG数据被记录在用户回忆文档内容时，并生成一个预测文档的列表。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-reliability-of-automatically-generated-pedestrian-and-bicycle-crash-surrogates"><a href="#Evaluating-the-reliability-of-automatically-generated-pedestrian-and-bicycle-crash-surrogates" class="headerlink" title="Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates"></a>Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13178">http://arxiv.org/abs/2307.13178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnimitra Sengupta, S. Ilgin Guler, Vikash V. Gayah, Shannon Warchol<br>for:This research aims to improve VRU safety performance at signalized intersections by identifying and using conflicts between VRUs and motorized vehicles as a surrogate for safety performance.methods:The research uses a video-based event monitoring system to automatically detect conflicts between VRUs and motorized vehicles, and advanced data-driven models to predict confirmed conflicts.results:The findings highlight the varying importance of specific surrogates in predicting true conflicts, and can assist transportation agencies to prioritize infrastructure investments and evaluate their effectiveness.Here is the same information in Simplified Chinese text:for:这项研究目标是提高信息化交通安全性，通过检测VRUs和机动车之间的冲突来衡量安全性。methods:该研究使用视频基于事件监测系统自动检测VRUs和机动车之间的冲突，并使用高级数据驱动模型预测确认冲突。results:研究发现冲突特定的指标在预测实际冲突中的重要性各不相同，可以帮助交通部门制定有效的基础设施投资和评估其效果。<details>
<summary>Abstract</summary>
Vulnerable road users (VRUs), such as pedestrians and bicyclists, are at a higher risk of being involved in crashes with motor vehicles, and crashes involving VRUs also are more likely to result in severe injuries or fatalities. Signalized intersections are a major safety concern for VRUs due to their complex and dynamic nature, highlighting the need to understand how these road users interact with motor vehicles and deploy evidence-based countermeasures to improve safety performance. Crashes involving VRUs are relatively infrequent, making it difficult to understand the underlying contributing factors. An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance. Automatically detecting these conflicts using a video-based systems is a crucial step in developing smart infrastructure to enhance VRU safety. The Pennsylvania Department of Transportation conducted a study using video-based event monitoring system to assess VRU and motor vehicle interactions at fifteen signalized intersections across Pennsylvania to improve VRU safety performance. This research builds on that study to assess the reliability of automatically generated surrogates in predicting confirmed conflicts using advanced data-driven models. The surrogate data used for analysis include automatically collectable variables such as vehicular and VRU speeds, movements, post-encroachment time, in addition to manually collected variables like signal states, lighting, and weather conditions. The findings highlight the varying importance of specific surrogates in predicting true conflicts, some being more informative than others. The findings can assist transportation agencies to collect the right types of data to help prioritize infrastructure investments, such as bike lanes and crosswalks, and evaluate their effectiveness.
</details>
<details>
<summary>摘要</summary>
易受伤的道路用户（VRU），如步行者和自行车客，在与机动车相撞时更容易受伤，同时这些相撞也更容易导致严重的伤害或死亡。信号控制交叉口是VRU安全问题的主要关注点，因为它们的复杂性和动态性使得需要更好地理解道路用户与机动车之间的互动，并采取基于证据的干预措施以提高安全性。VRU相撞事故相对较少，因此难以了解其下面的原因。因此，可以使用VRU和机动车之间的冲突作为安全性表现的代理。使用视频基本系统自动探测这些冲突是开发智能基础设施以提高VRU安全性的关键步骤。美国宾夕法尼亚州交通部门在十五个信号控制交叉口位于宾夕法尼亚州进行了视频基本系统评估VRU和机动车之间的互动，以提高VRU安全性表现。这项研究基于这项研究，以评估自动生成的代理是否可靠地预测真正的冲突。代理数据包括自动收集的变量，如机动车和VRU速度、运动和后续时间，以及手动收集的变量，如信号状态、灯光和天气条件。研究结果显示不同的代理在预测真正的冲突中的重要程度不同，一些更有用于预测。这些结果可以帮助交通机构收集相应的数据，以便优先投资基础设施，如自行车道和横道，并评估其效果。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-reconstruction-of-accelerated-cardiac-cine-MRI-using-Neural-Fields"><a href="#Unsupervised-reconstruction-of-accelerated-cardiac-cine-MRI-using-Neural-Fields" class="headerlink" title="Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields"></a>Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14363">http://arxiv.org/abs/2307.14363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tabita Catalán, Matías Courdurier, Axel Osses, René Botnar, Francisco Sahli Costabal, Claudia Prieto</li>
<li>for: cardiac functional assessment</li>
<li>methods: unsupervised approach based on implicit neural field representations for cardiac cine MRI</li>
<li>results: good image quality and improved temporal depiction compared to a state-of-the-art reconstruction technique<details>
<summary>Abstract</summary>
Cardiac cine MRI is the gold standard for cardiac functional assessment, but the inherently slow acquisition process creates the necessity of reconstruction approaches for accelerated undersampled acquisitions. Several regularization approaches that exploit spatial-temporal redundancy have been proposed to reconstruct undersampled cardiac cine MRI. More recently, methods based on supervised deep learning have been also proposed to further accelerate acquisition and reconstruction. However, these techniques rely on usually large dataset for training, which are not always available. In this work, we propose an unsupervised approach based on implicit neural field representations for cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in in-vivo undersampled golden-angle radial multi-coil acquisitions for undersampling factors of 26x and 52x, achieving good image quality, and comparable spatial and improved temporal depiction than a state-of-the-art reconstruction technique.
</details>
<details>
<summary>摘要</summary>
卡ди亚ц琴MRI是心脏功能评估的标准黄金标准，但它的自然slow acquisition process创造了快速重建的需求。多种基于空间-时间重复的正则化方法已经被提议用于快速重建受损缺少的cardiac cine MRI。在最近，基于监督深度学习方法已经被提议用于进一步加速获取和重建。但这些技术需要大量的训练数据，这些数据不总是可用。在这项工作中，我们提出了一种不监督的方法，基于启发神经场表示法（so called NF-cMRI）来重建受损缺少的cardiac cine MRI。我们对具有26x和52x受损因子的实验室中的生物体内无法进行了评估，并达到了良好的图像质量和相对较好的空间和改进的时间表现。
</details></li>
</ul>
<hr>
<h2 id="Multi-UAV-Speed-Control-with-Collision-Avoidance-and-Handover-aware-Cell-Association-DRL-with-Action-Branching"><a href="#Multi-UAV-Speed-Control-with-Collision-Avoidance-and-Handover-aware-Cell-Association-DRL-with-Action-Branching" class="headerlink" title="Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching"></a>Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13158">http://arxiv.org/abs/2307.13158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijiang Yan, Wael Jaafar, Bassant Selim, Hina Tabassum</li>
<li>for: 优化多架空飞行器（UAV）的紧急通信和运输性能，包括碰撞避免、连接稳定和交换。</li>
<li>methods: 使用深度强化学习解决方案，形式为Markov决策过程（MDP），UAV的状态定义为速度和通信数据速率。提议一种神经网络架构，具有共享决策模块和多个网络支柱，每个支柱负责特定的行动维度在2D运输通信空间。</li>
<li>results: 对比现有参考值，实验结果显示了18.32%的显著提升。<details>
<summary>Abstract</summary>
This paper presents a deep reinforcement learning solution for optimizing multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway. The objective is to enhance transportation and communication performance, including collision avoidance, connectivity, and handovers. The problem is formulated as a Markov decision process (MDP) with UAVs' states defined by velocities and communication data rates. We propose a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design efficiently handles the multi-dimensional action space, allowing independence for individual action dimensions. We introduce two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.
</details>
<details>
<summary>摘要</summary>
To tackle the multi-dimensional action space, the proposed solution features a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design allows for independence among individual action dimensions.Two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), are introduced to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.Translation in Simplified Chinese:这篇论文提出了一种深度强化学解决方案，用于优化多架航空器的细胞归属决策和移动速度决策在3D空中高速道路上。目标是提高交通和通信性能，包括避免碰撞、连接和手动传输。问题被формализова为一个Markov决策过程（MDP），其中无人机的状态定义为速度和通信数据速率。为了处理多维行动空间，提议的解决方案采用了一种神经网络架构，具有共享决策模块和多个网络分支，每个分支专门处理特定的行动维度在2D交通通信空间中。这种设计允许各个行动维度独立进行决策。两种模型，分支对抗Q网络（BDQ）和分支对抗双层深度Q网络（DDQN），用以示出方法。实验结果显示，与现有标准相比，提议的方法可以获得18.32%的显著提高。
</details></li>
</ul>
<hr>
<h2 id="Discovering-interpretable-elastoplasticity-models-via-the-neural-polynomial-method-enabled-symbolic-regressions"><a href="#Discovering-interpretable-elastoplasticity-models-via-the-neural-polynomial-method-enabled-symbolic-regressions" class="headerlink" title="Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions"></a>Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13149">http://arxiv.org/abs/2307.13149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bahador Bahmani, Hyoung Suk Suh, WaiChing Sun</li>
<li>for: 本研究旨在提高神经网络模型的可读性，通过两步机器学习方法返回可以被人类专家理解的数学模型。</li>
<li>methods: 本研究使用了分两步的机器学习方法，首先使用超级vised学习获得单变量特征映射，然后使用符号回归将这些映射转化为数学形式。</li>
<li>results: 研究结果显示，这种分两步机器学习方法可以解决神经网络模型的缺乏可读性问题，同时提供了更多的数学模型的可视化和可理解性。<details>
<summary>Abstract</summary>
Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetries of models, through automated derivations and reasoning. Numerical examples have been provided, along with an open-source code to enable third-party validation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learnable-wavelet-neural-networks-for-cosmological-inference"><a href="#Learnable-wavelet-neural-networks-for-cosmological-inference" class="headerlink" title="Learnable wavelet neural networks for cosmological inference"></a>Learnable wavelet neural networks for cosmological inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14362">http://arxiv.org/abs/2307.14362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chris-pedersen/learnablewavelets">https://github.com/chris-pedersen/learnablewavelets</a></li>
<li>paper_authors: Christian Pedersen, Michael Eickenberg, Shirley Ho</li>
<li>for:  cosmological inference and marginalisation over astrophysical effects</li>
<li>methods:  learnable scattering transform, a kind of convolutional neural network that uses trainable wavelets as filters</li>
<li>results:  scattering architectures are able to outperform a CNN, significantly in the case of small training data samples, and a lightweight scattering network that is highly interpretable.<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have been shown to both extract more information than the traditional two-point statistics from cosmological fields, and marginalise over astrophysical effects extremely well. However, CNNs require large amounts of training data, which is potentially problematic in the domain of expensive cosmological simulations, and it is difficult to interpret the network. In this work we apply the learnable scattering transform, a kind of convolutional neural network that uses trainable wavelets as filters, to the problem of cosmological inference and marginalisation over astrophysical effects. We present two models based on the scattering transform, one constructed for performance, and one constructed for interpretability, and perform a comparison with a CNN. We find that scattering architectures are able to outperform a CNN, significantly in the case of small training data samples. Additionally we present a lightweight scattering network that is highly interpretable.
</details>
<details>
<summary>摘要</summary>
几何神经网络（CNN）已经显示可以从 cosmological 场中提取更多信息，并且对astrophysical 效应进行约束非常好。然而，CNN需要大量的训练数据，这可能是cosmological  simulations 中的问题，而且它很难解释。在这项工作中，我们使用可学习散射变换，一种使用可学习的wavelet 作为滤波器的几何神经网络，来解决 cosmological 推断和astrophysical 效应的约束问题。我们提出了两种基于散射变换的模型，一种是为了性能，另一种是为了可解释性。我们对这两种模型与CNN进行比较，发现散射架构可以在小训练样本情况下显著超越CNN。此外，我们还提出了一个轻量级的散射网络，具有非常高的可解释性。
</details></li>
</ul>
<hr>
<h2 id="Extending-Path-Dependent-NJ-ODEs-to-Noisy-Observations-and-a-Dependent-Observation-Framework"><a href="#Extending-Path-Dependent-NJ-ODEs-to-Noisy-Observations-and-a-Dependent-Observation-Framework" class="headerlink" title="Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework"></a>Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13147">http://arxiv.org/abs/2307.13147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/floriankrach/pd-njode">https://github.com/floriankrach/pd-njode</a></li>
<li>paper_authors: William Andersson, Jakob Heiss, Florian Krach, Josef Teichmann</li>
<li>for: 预测不规则时间序列数据的不完整和受干扰的观测</li>
<li>methods: 基于神经网络的跳动梯度方程（PD-NJ-ODE）模型</li>
<li>results: 提供了两种扩展和理论保证，以及实际示例，以解决不规则时间序列数据的不完整和受干扰问题<details>
<summary>Abstract</summary>
The Path-Dependent Neural Jump ODE (PD-NJ-ODE) is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them.
</details>
<details>
<summary>摘要</summary>
“Path-Dependent Neural Jump ODE（PD-NJ-ODE）是一种模型，用于预测连续时间的随机过程，具有不规则和缺失的观测。具体来说，该方法学习 optimal forecasts，给 irregularly sampled time series of incomplete past observations。目前，过程本身和坐标wise observation times 被假设为独立，并且观测被假设为噪声无存。在这种工作中，我们讨论了两种扩展，以解除这些限制，并提供了理论保证以及实际示例。”Note that Simplified Chinese is a romanization of Chinese, and the translation may not be exact.
</details></li>
</ul>
<hr>
<h2 id="Does-Progress-On-Object-Recognition-Benchmarks-Improve-Real-World-Generalization"><a href="#Does-Progress-On-Object-Recognition-Benchmarks-Improve-Real-World-Generalization" class="headerlink" title="Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?"></a>Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13136">http://arxiv.org/abs/2307.13136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megan Richards, Polina Kirichenko, Diane Bouchacourt, Mark Ibrahim<br>for:* The paper aims to study generalization across geography as a more realistic measure of progress in object recognition.methods:* The authors use two datasets of objects from households across the globe to evaluate the performance of nearly 100 vision models, including recent foundation models.* They measure the disparities in performance across regions as a more fine-grained measure of real-world generalization.results:* The authors find a progress gap between standard benchmarks and real-world, geographical shifts, with standard benchmarks resulting in up to 2.5x more progress than real-world distribution shifts.* They observe large geographic disparities in performance across regions, even for foundation CLIP models, with differences of 7-20% in accuracy between regions.* Retraining the models on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds.<details>
<summary>Abstract</summary>
For more than a decade, researchers have measured progress in object recognition on ImageNet-based generalization benchmarks such as ImageNet-A, -C, and -R. Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate these standard benchmarks, but remain brittle in practice. This suggests standard benchmarks, which tend to focus on predefined or synthetic changes, may not be sufficient for measuring real world generalization. Consequently, we propose studying generalization across geography as a more realistic measure of progress using two datasets of objects from households across the globe. We conduct an extensive empirical evaluation of progress across nearly 100 vision models up to most recent foundation models. We first identify a progress gap between standard benchmarks and real-world, geographical shifts: progress on ImageNet results in up to 2.5x more progress on standard generalization benchmarks than real-world distribution shifts. Second, we study model generalization across geographies by measuring the disparities in performance across regions, a more fine-grained measure of real world generalization. We observe all models have large geographic disparities, even foundation CLIP models, with differences of 7-20% in accuracy between regions. Counter to modern intuition, we discover progress on standard benchmarks fails to improve geographic disparities and often exacerbates them: geographic disparities between the least performant models and today's best models have more than tripled. Our results suggest scaling alone is insufficient for consistent robustness to real-world distribution shifts. Finally, we highlight in early experiments how simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds.
</details>
<details>
<summary>摘要</summary>
We conducted an extensive empirical evaluation of progress across nearly 100 vision models, including the most recent foundation models. Our results show a significant gap between progress on ImageNet and real-world, geographical shifts. Specifically, progress on ImageNet results in up to 2.5 times more progress on standard generalization benchmarks than real-world distribution shifts.We also studied model generalization across geographies by measuring the disparities in performance across regions, providing a more fine-grained measure of real-world generalization. Our findings show that all models, including foundation CLIP models, have large geographic disparities, with differences of 7-20% in accuracy between regions. Surprisingly, we found that progress on standard benchmarks actually exacerbates these geographic disparities, with the disparities between the least performant models and today's best models having more than tripled.Our results suggest that scaling alone is insufficient for achieving consistent robustness to real-world distribution shifts. However, we found that simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds.
</details></li>
</ul>
<hr>
<h2 id="simPLE-a-visuotactile-method-learned-in-simulation-to-precisely-pick-localize-regrasp-and-place-objects"><a href="#simPLE-a-visuotactile-method-learned-in-simulation-to-precisely-pick-localize-regrasp-and-place-objects" class="headerlink" title="simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects"></a>simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13133">http://arxiv.org/abs/2307.13133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Bauza, Antonia Bronars, Yifan Hou, Ian Taylor, Nikhil Chavan-Dafle, Alberto Rodriguez</li>
<li>for: 这篇论文的目的是解决 робоット抓取和置放精度和通用性之间的矛盾。</li>
<li>methods: 这篇论文使用了三个主要组成部分：任务意识 grasping，视听感知和重新抓取规划。任务意识 grasping 计算物体抓取的可能性，视听感知模型通过经验学习与实际观察进行匹配，最后计算机器人的运动路径通过短路问题解决。</li>
<li>results: 在配备了视听感知的双臂机器人上，使用 simPLE 实现了 15 种多样化的物体的精度抓取和置放，成功率高达 90% （6 种物体）和 80% （11 种物体）。视频可以在 <a target="_blank" rel="noopener" href="http://mcube.mit.edu/research/simPLE.html">http://mcube.mit.edu/research/simPLE.html</a> 上查看。<details>
<summary>Abstract</summary>
Existing robotic systems have a clear tension between generality and precision. Deployed solutions for robotic manipulation tend to fall into the paradigm of one robot solving a single task, lacking precise generalization, i.e., the ability to solve many tasks without compromising on precision. This paper explores solutions for precise and general pick-and-place. In precise pick-and-place, i.e. kitting, the robot transforms an unstructured arrangement of objects into an organized arrangement, which can facilitate further manipulation. We propose simPLE (simulation to Pick Localize and PLacE) as a solution to precise pick-and-place. simPLE learns to pick, regrasp and place objects precisely, given only the object CAD model and no prior experience. We develop three main components: task-aware grasping, visuotactile perception, and regrasp planning. Task-aware grasping computes affordances of grasps that are stable, observable, and favorable to placing. The visuotactile perception model relies on matching real observations against a set of simulated ones through supervised learning. Finally, we compute the desired robot motion by solving a shortest path problem on a graph of hand-to-hand regrasps. On a dual-arm robot equipped with visuotactile sensing, we demonstrate pick-and-place of 15 diverse objects with simPLE. The objects span a wide range of shapes and simPLE achieves successful placements into structured arrangements with 1mm clearance over 90% of the time for 6 objects, and over 80% of the time for 11 objects. Videos are available at http://mcube.mit.edu/research/simPLE.html .
</details>
<details>
<summary>摘要</summary>
现有的 роботиче系统存在一种明显的矛盾，即通用性和精度之间的较量。已部署的 робоotic manipulation解决方案通常会降低到单一任务的解决方案，缺乏精度的总体化，即能够解决多个任务而无需牺牲精度。本文探讨精度和通用性的解决方案，特别是精度的吸盘和置换。我们提出了simPLE（ simulation to Pick Localize and PLacE）解决方案，该方案可以帮助机器人准确地从未经验的情况下将物品精度地吸盘和置换。我们开发了三个主要组件：任务意识 grasping、视听感知和重新抓取规划。任务意识 grasping计算物体的稳定、可见和置换的可能性。视听感知模型通过对实际观察与已经模拟的对比，通过监管学习来学习。最后，我们通过解决一个最短路径问题来计算机器人的运动。在搭载了视听感知的双手机 robot上，我们使用 simPLE 成功地进行了15种多形态物品的吸盘和置换。这些物品的形态范围广泛，而 simPLE 在90%的时间内成功地将物品置换到结构化的排序中，具有1毫米的清晰度。视频可以在http://mcube.mit.edu/research/simPLE.html 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Differentially-Private-Weighted-Empirical-Risk-Minimization-Procedure-and-its-Application-to-Outcome-Weighted-Learning"><a href="#A-Differentially-Private-Weighted-Empirical-Risk-Minimization-Procedure-and-its-Application-to-Outcome-Weighted-Learning" class="headerlink" title="A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning"></a>A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13127">http://arxiv.org/abs/2307.13127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer Giddens, Yiwang Zhou, Kevin R. Krull, Tara M. Brinkman, Peter X. K. Song, Fang Liu</li>
<li>for: 保护敏感数据的隐私，提供数据隐私保护的机制。</li>
<li>methods: 使用异质隐私（DP）框架，提供数据隐私保护的数学保证。</li>
<li>results: 实验研究表明，可以通过DP-wERM来实现个人化治疗规则的隐私保护，而不会影响模型的性能。<details>
<summary>Abstract</summary>
It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM). While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks. Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data. Previous work has primarily concentrated on applying DP to unweighted ERM. We consider an important generalization to weighted ERM (wERM). In wERM, each individual's contribution to the objective function can be assigned varying weights. In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions. Extending the existing DP-ERM procedures to wERM paves a path to deriving privacy-preserving learning methods for individualized treatment rules, including the popular outcome weighted learning (OWL). We evaluate the performance of the DP-wERM application to OWL in a simulation study and in a real clinical trial of melatonin for sleep health. All empirical results demonstrate the viability of training OWL models via wERM with DP guarantees while maintaining sufficiently useful model performance. Therefore, we recommend practitioners consider implementing the proposed privacy-preserving OWL procedure in real-world scenarios involving sensitive data.
</details>
<details>
<summary>摘要</summary>
通常使用包含个人信息的数据来建立预测模型，而这些模型可以具有非常高的预测精度。但是使用敏感数据可能会导致隐私攻击。 diffe隐私（DP）是一个吸引人的框架，可以为解决这些数据隐私问题提供数学上可证明的隐私损害 bound。先前的工作主要集中在应用DP到不带重量的ERM（unweighted ERM）上。我们考虑了一个重要的扩展，即带重量ERM（weighted ERM， wERM）。在wERM中，每个个体的对象函数中的贡献可以被分配不同的权重。在这种情况下，我们提出了首个具有DP保证的wERM算法，并提供了严格的理论证明， guaranteeing DP under mild regularity conditions。扩展现有的DP-ERM过程到wERM，这种方法可以为个性化治疗规则，包括受欢迎的结果Weighted learning（OWL），提供隐私保护的学习方法。我们在一个 simstudy和一个真实的临床试验中评估了DP-wERM应用于OWL的性能。所有实验结果都表明了在保持有用的模型性能的情况下，可以通过具有DP保证的方法来训练OWL模型。因此，我们建议实践者在含敏感数据的实际场景中考虑实施我们提议的隐私保护OWL过程。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-Machine-Learning-Model-for-Classifying-Gene-Mutations-in-Cancer-using-LSTM-BiLSTM-CNN-GRU-and-GloVe"><a href="#A-Hybrid-Machine-Learning-Model-for-Classifying-Gene-Mutations-in-Cancer-using-LSTM-BiLSTM-CNN-GRU-and-GloVe" class="headerlink" title="A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe"></a>A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14361">http://arxiv.org/abs/2307.14361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass, Osama Dorgham, Jamil Al Shaqsi</li>
<li>for: 这个研究旨在使用Kaggle的Personalized Medicine: Redefining Cancer Treatment dataset，混合LSTM、BiLSTM、CNN、GRU和GloVe来分类基因突变。</li>
<li>methods: 这个研究使用了一个ensemble模型，混合LSTM、BiLSTM、CNN、GRU和GloVe来分类基因突变。</li>
<li>results: 这个研究发现，ensemble模型可以优化基因突变的分类性能，并且需要训练时间较短，实现了性能和效率的完美结合。<details>
<summary>Abstract</summary>
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
</details>
<details>
<summary>摘要</summary>
这项研究把一个ensemble模型组合LSTM、BiLSTM、CNN、GRU和GloVe用于分类基因突变，使用Kaggle的个性化医疗：重定义肿瘤治疗数据集。结果与著名的变换器如BERT、Electra、Roberta、XLNet、Distilbert和其LSTMensemble进行比较。我们的模型在准确率、精度、准确率、F1分数和平均平方误差方面都高于所有其他模型，同时具有更好的效率，这表明ensemble模型在困难任务中具有优势。这项研究证明了ensemble模型在基因突变分类中的实用性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Bradley-Terry-Rating-Quantifying-Properties-from-Comparisons"><a href="#Deep-Bradley-Terry-Rating-Quantifying-Properties-from-Comparisons" class="headerlink" title="Deep Bradley-Terry Rating: Quantifying Properties from Comparisons"></a>Deep Bradley-Terry Rating: Quantifying Properties from Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13709">http://arxiv.org/abs/2307.13709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satoru Fujii</li>
<li>for: 本研究旨在开发一种能够评估不可见性的实际世界属性的机器学习框架。</li>
<li>methods: 本研究使用深度学习网络结构，将布拉德利-特里尔模型灵活地集成到机器学习框架中。此外，我们还推广了这种架构，以适应不公平环境。</li>
<li>results: 实验分析表明，DBTR可以成功地评估和估计欲要的属性。<details>
<summary>Abstract</summary>
Many properties in the real world can't be directly observed, making them difficult to learn. To deal with this challenging problem, prior works have primarily focused on estimating those properties by using graded human scores as the target label in the training. Meanwhile, rating algorithms based on the Bradley-Terry model are extensively studied to evaluate the competitiveness of players based on their match history. In this paper, we introduce the Deep Bradley-Terry Rating (DBTR), a novel machine learning framework designed to quantify and evaluate properties of unknown items. Our method seamlessly integrates the Bradley-Terry model into the neural network structure. Moreover, we generalize this architecture further to asymmetric environments with unfairness, a condition more commonly encountered in real-world settings. Through experimental analysis, we demonstrate that DBTR successfully learns to quantify and estimate desired properties.
</details>
<details>
<summary>摘要</summary>
很多现实世界中的属性难以直接观察，这使得它们学习变得更加困难。先前的工作主要通过使用排名为目标标签进行训练来估算这些属性。而rating算法基于布莱德利-泰利模型在评估玩家的竞争力方面得到了广泛的研究。在这篇论文中，我们介绍了深度布莱德利-泰利评分（DBTR），一种新的机器学习框架，用于评估和评价未知物品的属性。我们将布莱德利-泰利模型integrated into the neural network architecture。此外，我们还扩展了这个体系，以适应不平等环境，这种情况更常见于实际世界中。通过实验分析，我们证明了DBTR成功地学习和估算欲知的属性。Note: "Simplified Chinese" is a translation of "Traditional Chinese" and "Traditional Chinese" is the same as "Simplified Chinese" in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Conformal-prediction-for-frequency-severity-modeling"><a href="#Conformal-prediction-for-frequency-severity-modeling" class="headerlink" title="Conformal prediction for frequency-severity modeling"></a>Conformal prediction for frequency-severity modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13124">http://arxiv.org/abs/2307.13124</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heltongraziadei/conformal-fs">https://github.com/heltongraziadei/conformal-fs</a></li>
<li>paper_authors: Helton Graziadei, Paulo C. Marques F., Eduardo F. L. de Melo, Rodrigo S. Targino</li>
<li>for: 预测保险订单的预测范围</li>
<li>methods: 使用非参数模型独立预测方法，并利用分割兼斥预测技术以生成预测范围</li>
<li>results: 在模拟和实际数据集上展示了预测范围的有效性，并且在Random Forest中的两个阶段频率强度模型中使用剩下的机制来生成适应宽度的预测范围。<details>
<summary>Abstract</summary>
We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
</details>
<details>
<summary>摘要</summary>
我们提出了一种非参数化模型无关的框架，用于构建预测评估Interval，具有finite sample统计保证，扩展了split conformal prediction技术到频率严重模型领域。我们通过实验和真实数据示cases，证明了该框架的有效性。当下面预测模型是随机森林时，我们EXTEND了两个阶段split conformal prediction过程，并示出了如何利用out-of-bag机制来消除需要调整集的需求，并允许生成适应宽度的预测评估Interval。
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Geometric-Weighted-Graph-Attention-Network-for-Identifying-Functional-Networks-Associated-with-Gait-Impairment"><a href="#An-Explainable-Geometric-Weighted-Graph-Attention-Network-for-Identifying-Functional-Networks-Associated-with-Gait-Impairment" class="headerlink" title="An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment"></a>An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13108">http://arxiv.org/abs/2307.13108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/favour-nerrise/xgw-gat">https://github.com/favour-nerrise/xgw-gat</a></li>
<li>paper_authors: Favour Nerrise, Qingyu Zhao, Kathleen L. Poston, Kilian M. Pohl, Ehsan Adeli</li>
<li>for: This paper aims to identify functional networks predictive of gait difficulties in individuals with Parkinson’s disease (PD) using an explainable, geometric, weighted-graph attention neural network (xGW-GAT).</li>
<li>methods: The xGW-GAT model uses resting-state functional MRI (rs-fMRI) data to represent functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold, and learns an attention mask yielding individual- and group-level explainability.</li>
<li>results: The xGW-GAT model successfully outperforms several existing methods and identifies functional connectivity patterns associated with gait impairment in PD, providing interpretable explanations of functional subnetworks associated with motor impairment.Here’s the Simplified Chinese text version of the three key points:</li>
<li>for: 这篇论文目标是使用可解释的、几何的、加权图像注意力神经网络（xGW-GAT）来预测parkinson病患者中的步态困难。</li>
<li>methods: xGW-GAT模型使用了休息状态功能磁共振成像（rs-fMRI）数据来表示功能连接图，并将连接图转换为几何上的正定定Matrix，从而实现个体和组级别的解释性。</li>
<li>results: xGW-GAT模型成功超越了一些现有的方法，并在parkinson病患者中预测了步态困难的功能连接图，提供了可解释的功能子网络协助抑荐。<details>
<summary>Abstract</summary>
One of the hallmark symptoms of Parkinson's Disease (PD) is the progressive loss of postural reflexes, which eventually leads to gait difficulties and balance problems. Identifying disruptions in brain function associated with gait impairment could be crucial in better understanding PD motor progression, thus advancing the development of more effective and personalized therapeutics. In this work, we present an explainable, geometric, weighted-graph attention neural network (xGW-GAT) to identify functional networks predictive of the progression of gait difficulties in individuals with PD. xGW-GAT predicts the multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our computational- and data-efficient model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes, based on which we learn an attention mask yielding individual- and group-level explainability. Applied to our resting-state functional MRI (rs-fMRI) dataset of individuals with PD, xGW-GAT identifies functional connectivity patterns associated with gait impairment in PD and offers interpretable explanations of functional subnetworks associated with motor impairment. Our model successfully outperforms several existing methods while simultaneously revealing clinically-relevant connectivity patterns. The source code is available at https://github.com/favour-nerrise/xGW-GAT .
</details>
<details>
<summary>摘要</summary>
一种典型的parkinson病（PD）的症状是慢速损失的姿态反射，最终会导致行走困难和平衡问题。identifying脑功能干预的变化可能对于更好地理解PD的运动进程有益，从而推动更有效和个性化的治疗的开发。在这项工作中，我们提出了一种可解释的、几何学的、加权图注意力神经网络（xGW-GAT），用于预测PD患者的行走困难级别。xGW-GAT预测了UPDRS（MDS-UPDRS）中的多个步态困难。我们的计算和数据有效的模型将功能连接图表示为正定definite矩阵（SPD）在里曼曼ifold上，以显式地编码整个连接图的对称对之间的对应关系，根据这些对应关系我们学习出一个注意力 маска，以实现个体和组级别的解释。应用于我们的resting-state功能MRI（rs-fMRI）数据集中的PD患者，xGW-GAT已经确定了与步态困难相关的功能连接图模式，并提供了可解释的功能子网络相关于运动障碍的解释。我们的模型已经超越了一些现有的方法，同时揭示了临床有用的连接图模式。模型源代码可以在https://github.com/favour-nerrise/xGW-GAT 上获取。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Example-Based-Control"><a href="#Contrastive-Example-Based-Control" class="headerlink" title="Contrastive Example-Based Control"></a>Contrastive Example-Based Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13101">http://arxiv.org/abs/2307.13101</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khatch31/laeo">https://github.com/khatch31/laeo</a></li>
<li>paper_authors: Kyle Hatch, Benjamin Eysenbach, Rafael Rafailov, Tianhe Yu, Ruslan Salakhutdinov, Sergey Levine, Chelsea Finn</li>
<li>for: 这篇论文旨在解决offline控制问题，即使环境交互成本高、指定奖励函数困难。</li>
<li>methods: 该论文提出了一种数据驱动的方法，通过对转移动态和高返回状态的示例进行标注，使用这些标注来教育RL算法。</li>
<li>results: 该方法在多种基于状态和图像的offline控制任务上表现出色，超过了基elines使用学习奖励函数的表现。此外，该方法还显示了改进的Robustness和数据集大小的扩展性。<details>
<summary>Abstract</summary>
While many real-world problems that might benefit from reinforcement learning, these problems rarely fit into the MDP mold: interacting with the environment is often expensive and specifying reward functions is challenging. Motivated by these challenges, prior work has developed data-driven approaches that learn entirely from samples from the transition dynamics and examples of high-return states. These methods typically learn a reward function from high-return states, use that reward function to label the transitions, and then apply an offline RL algorithm to these transitions. While these methods can achieve good results on many tasks, they can be complex, often requiring regularization and temporal difference updates. In this paper, we propose a method for offline, example-based control that learns an implicit model of multi-step transitions, rather than a reward function. We show that this implicit model can represent the Q-values for the example-based control problem. Across a range of state-based and image-based offline control tasks, our method outperforms baselines that use learned reward functions; additional experiments demonstrate improved robustness and scaling with dataset size.
</details>
<details>
<summary>摘要</summary>
“虽然许多实际问题可以受惠于强化学习，但这些问题很少遵循MDP模型：与环境交互是经济不可能，并且定义奖励函数是困难的。为了解决这些挑战，先前的工作已经开发了基于数据的方法，这些方法从转移动力和高返回状态中学习整个奖励函数，然后使用这个奖励函数来标记转移，最后应用在这些转移上的离线RL算法。虽然这些方法可以在许多任务上达到良好的结果，但它们可能会复杂，需要规则化和时间差更新。在这篇论文中，我们提出了一种离线、例子基本的控制方法，这种方法学习了多步转移的隐式模型，而不是奖励函数。我们证明了这种隐式模型可以表示Q值。在一系列基于状态和图像的离线控制任务上，我们的方法超过了基于学习的奖励函数的基elines，其他实验还表明了我们的方法的更好的稳定性和数据集大小的扩展性。”
</details></li>
</ul>
<hr>
<h2 id="Label-Noise-Correcting-a-Correction"><a href="#Label-Noise-Correcting-a-Correction" class="headerlink" title="Label Noise: Correcting a Correction"></a>Label Noise: Correcting a Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13100">http://arxiv.org/abs/2307.13100</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Toner, Amos Storkey</li>
<li>for:  addresses the issue of overfitting in training neural network classifiers on datasets with label noise</li>
<li>methods:  proposes a more direct approach to tackling overfitting by imposing a lower bound on the empirical risk during training, and provides theoretical results for different loss functions</li>
<li>results:  demonstrates significant enhancement of robustness in various settings with virtually no additional computational cost<details>
<summary>Abstract</summary>
Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用神经网络分类器训练数据集中的标签噪声会增加模型过拟合的风险。为此，研究人员尝试了一些代替的损失函数，以提高模型的Robustness。然而，大多数这些方法是启发的 Naturally, many of these methods are heuristic in nature and still vulnerable to overfitting or underfitting.在这项工作中，我们提出了一种更直接的方法来解决 label noise 导致的过拟合问题。我们注意到了标签噪声的存在下限制了总体风险的下界。基于这个观察，我们提议在训练过程中强制实施这个下界，以避免过拟合。我们的主要贡献是提供了不同损失函数的减少风险的显式计算可行的理论结果。我们在实验中证明了使用这些结果可以增强模型的Robustness，而且几乎没有额外的计算成本。Note: "神经网络" is translated as "神经网络" in Simplified Chinese, and "损失函数" is translated as "损失函数" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Ensemble-Learning-for-Materials-Property-Prediction-with-Classical-Interatomic-Potentials-Carbon-as-an-Example"><a href="#Interpretable-Ensemble-Learning-for-Materials-Property-Prediction-with-Classical-Interatomic-Potentials-Carbon-as-an-Example" class="headerlink" title="Interpretable Ensemble Learning for Materials Property Prediction with Classical Interatomic Potentials: Carbon as an Example"></a>Interpretable Ensemble Learning for Materials Property Prediction with Classical Interatomic Potentials: Carbon as an Example</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10818">http://arxiv.org/abs/2308.10818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Jiang, Haofan Sun, Kamal Choudhary, Houlong Zhuang, Qiong Nian</li>
<li>for: 预测材料的重要性质</li>
<li>methods: ensemble learning（重叠学习）</li>
<li>results: 比原始物理学 potentials 更准确地预测材料的性质<details>
<summary>Abstract</summary>
Machine learning (ML) is widely used to explore crystal materials and predict their properties. However, the training is time-consuming for deep-learning models, and the regression process is a black box that is hard to interpret. Also, the preprocess to transfer a crystal structure into the input of ML, called descriptor, needs to be designed carefully. To efficiently predict important properties of materials, we propose an approach based on ensemble learning consisting of regression trees to predict formation energy and elastic constants based on small-size datasets of carbon allotropes as an example. Without using any descriptor, the inputs are the properties calculated by molecular dynamics with 9 different classical interatomic potentials. Overall, the results from ensemble learning are more accurate than those from classical interatomic potentials, and ensemble learning can capture the relatively accurate properties from the 9 classical potentials as criteria for predicting the final properties.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）广泛应用于探索晶体材料和预测其性能。然而，训练深度学习模型需要较长时间，而回归过程是一个难以解释的黑盒子。此外，将晶体结构转化为机器学习的输入，即描述符，需要仔细设计。为了高效预测材料的重要性能，我们提出了基于集成学习的方法，包括回归树来预测formation energy和塑性常数，使用小型碳材料数据集为例。不使用任何描述符，输入是通过分子动力学计算的物理属性，使用9种不同的古典间隔 potentials。总的来说，ensemble learning的结果比古典间隔 potentials更准确，并可以捕捉9种古典 potentials中的相对准确性作为预测最终性能的标准。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Under-Demographic-Scarce-Regime"><a href="#Fairness-Under-Demographic-Scarce-Regime" class="headerlink" title="Fairness Under Demographic Scarce Regime"></a>Fairness Under Demographic Scarce Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13081">http://arxiv.org/abs/2307.13081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrik Joslin Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji</li>
<li>for: 本文研究了在不完整的人口信息情况下如何建立具有更好的公平精度平衡的特征分类器。</li>
<li>methods: 本文提出了一种基于不确定性认识的特征分类器建模框架，通过在不确定性较低的样本上遵循公平约束来提高公平精度平衡。</li>
<li>results: 实验结果表明，相比 класси型特征分类器，提出的框架可以在两个数据集上建立更好的公平精度平衡，并且超越了基于真实敏感特征的约束。<details>
<summary>Abstract</summary>
Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detrimental to fairness and accuracy. Our experiments on two datasets showed that the proposed framework yields models with significantly better fairness-accuracy trade-offs compared to classic attribute classifiers. Surprisingly, our framework outperforms models trained with constraints on the true sensitive attributes.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大多数现有的公平假设模型拥有完整的人口信息。然而，有些场景下，人口信息只有部分可用，例如因为数据采集或隐私原因。这种情况被称为“人口缺乏Registry”。先前的研究表明，使用代理敏感特征来替代缺失的人口信息可以改善公平。但是，使用代理敏感特征会对公平准确性负面影响。为解决这种限制，我们提出了一个框架，用于建立具有更好的公平准确性质量的特征分类器。我们的方法将不确定性意识引入特征分类器，并在具有最低不确定性的人口信息上遵循公平约束。我们经验表明，对不确定的敏感特征进行公平约束是对公平和准确性产生负面影响。我们在两个 dataset 上进行了实验，结果显示，我们的框架可以与经典特征分类器相比，在公平准确性质量上获得显著改善。 surprisingly，我们的框架还超过了基于真实敏感特征的约束。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Certified-Training-Towards-Better-Accuracy-Robustness-Tradeoffs"><a href="#Adaptive-Certified-Training-Towards-Better-Accuracy-Robustness-Tradeoffs" class="headerlink" title="Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs"></a>Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13078">http://arxiv.org/abs/2307.13078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhakshylyk Nurlanov, Frank R. Schmidt, Florian Bernard</li>
<li>for: 提高模型的鲁棒性和标准准确率之间的负荷融合。</li>
<li>methods: 基于适应证明半径的新训练方法，通过改进模型的准确率和鲁棒性来提高模型的标准准确率和鲁棒性。</li>
<li>results: 在MNIST、CIFAR-10和TinyImageNet等 datasets上实现了更高的鲁棒性和标准准确率之间的负荷融合，特别是在CIFAR-10和TinyImageNet上，模型的鲁棒性可以提高至两倍的水平，而无需增加训练时间或资源。<details>
<summary>Abstract</summary>
As deep learning models continue to advance and are increasingly utilized in real-world systems, the issue of robustness remains a major challenge. Existing certified training methods produce models that achieve high provable robustness guarantees at certain perturbation levels. However, the main problem of such models is a dramatically low standard accuracy, i.e. accuracy on clean unperturbed data, that makes them impractical. In this work, we consider a more realistic perspective of maximizing the robustness of a model at certain levels of (high) standard accuracy. To this end, we propose a novel certified training method based on a key insight that training with adaptive certified radii helps to improve both the accuracy and robustness of the model, advancing state-of-the-art accuracy-robustness tradeoffs. We demonstrate the effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models with up to two times higher robustness, measured as an average certified radius of a test set, at the same levels of standard accuracy compared to baseline approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="General-Purpose-Multi-Modal-OOD-Detection-Framework"><a href="#General-Purpose-Multi-Modal-OOD-Detection-Framework" class="headerlink" title="General-Purpose Multi-Modal OOD Detection Framework"></a>General-Purpose Multi-Modal OOD Detection Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13069">http://arxiv.org/abs/2307.13069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viet Duong, Qiong Wu, Zhengyi Zhou, Eric Zavesky, Jiahe Chen, Xiangzhou Liu, Wen-Ling Hsu, Huajie Shao</li>
<li>for: 本研究旨在提出一种普适的弱监督异常检测框架，以便同时在多个不同的异常场景中准确地检测异常样本。</li>
<li>methods: 我们提出了一种结合二分类器和对比学习组件的总体异常检测方法，并采用了尖锥损失来约束各种样本的几何表示之间的相似性。</li>
<li>results: 我们在多个实际世界数据集上进行了实验，并证明了我们的方法可以在三个不同的异常场景中同时具有高精度的异常检测能力，而且超过了现有的状态艺法。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection identifies test samples that differ from the training data, which is critical to ensuring the safety and reliability of machine learning (ML) systems. While a plethora of methods have been developed to detect uni-modal OOD samples, only a few have focused on multi-modal OOD detection. Current contrastive learning-based methods primarily study multi-modal OOD detection in a scenario where both a given image and its corresponding textual description come from a new domain. However, real-world deployments of ML systems may face more anomaly scenarios caused by multiple factors like sensor faults, bad weather, and environmental changes. Hence, the goal of this work is to simultaneously detect from multiple different OOD scenarios in a fine-grained manner. To reach this goal, we propose a general-purpose weakly-supervised OOD detection framework, called WOOD, that combines a binary classifier and a contrastive learning component to reap the benefits of both. In order to better distinguish the latent representations of in-distribution (ID) and OOD samples, we adopt the Hinge loss to constrain their similarity. Furthermore, we develop a new scoring metric to integrate the prediction results from both the binary classifier and contrastive learning for identifying OOD samples. We evaluate the proposed WOOD model on multiple real-world datasets, and the experimental results demonstrate that the WOOD model outperforms the state-of-the-art methods for multi-modal OOD detection. Importantly, our approach is able to achieve high accuracy in OOD detection in three different OOD scenarios simultaneously. The source code will be made publicly available upon publication.
</details>
<details>
<summary>摘要</summary>
外部数据（OOD）检测是确保机器学习（ML）系统的安全性和可靠性的关键之一。虽然许多方法已经开发出来检测uni-modal OOD样本，但只有一些关注多模态OOD检测。现有的对比学习基于方法主要在新领域中检测多modal OOD样本。但实际世界中部署ML系统可能会遇到更多的异常情况，如感知器故障、坏天气和环境变化。因此，本研究的目标是同时从多个不同的OOD场景中 simultanously检测ID和OOD样本。为达到这个目标，我们提议一种通用弱监督OOD检测框架，called WOOD，该框架结合了一个二分类器和一个对比学习组件，以便汲取两者的优点。为了更好地分解ID和OOD样本的准确表示，我们采用了尖锥损函数来约束 их相似性。此外，我们开发了一个新的评分度量来整合 binary 类ifier和对比学习的预测结果，以便更好地识别OOD样本。我们对多个实际世界数据集进行了实验，并实验结果表明，提议的WOOD模型在多modal OOD检测中超过了当前状态艺技。特别是，我们的方法可以同时在三个不同的OOD场景中达到高精度的OOD检测。代码将在发表后公开。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Category-Frequency-prediction-for-Buy-It-Again-recommendations"><a href="#Personalized-Category-Frequency-prediction-for-Buy-It-Again-recommendations" class="headerlink" title="Personalized Category Frequency prediction for Buy It Again recommendations"></a>Personalized Category Frequency prediction for Buy It Again recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01195">http://arxiv.org/abs/2308.01195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Pande, Kunal Ghosh, Rankyung Park</li>
<li>for: 提高用户体验和网站参与度，通过建议客户可能会购买的商品</li>
<li>methods: 基于个性化类别模型（PC模型）和个性化类别内Item模型（IC模型）的层次PCIC模型，包括个性化类别生成和个性化类别内Item排名</li>
<li>results: 与12基准模型进行比较，NDCG提高到16%，回归提高约2%，并在大规模数据集上进行了扩展和训练（超过8个小时），并在一家大型零售商的官方网站上进行了AB测试，得到了显著的用户参与度增长<details>
<summary>Abstract</summary>
Buy It Again (BIA) recommendations are crucial to retailers to help improve user experience and site engagement by suggesting items that customers are likely to buy again based on their own repeat purchasing patterns. Most existing BIA studies analyze guests personalized behavior at item granularity. A category-based model may be more appropriate in such scenarios. We propose a recommendation system called a hierarchical PCIC model that consists of a personalized category model (PC model) and a personalized item model within categories (IC model). PC model generates a personalized list of categories that customers are likely to purchase again. IC model ranks items within categories that guests are likely to consume within a category. The hierarchical PCIC model captures the general consumption rate of products using survival models. Trends in consumption are captured using time series models. Features derived from these models are used in training a category-grained neural network. We compare PCIC to twelve existing baselines on four standard open datasets. PCIC improves NDCG up to 16 percent while improving recall by around 2 percent. We were able to scale and train (over 8 hours) PCIC on a large dataset of 100M guests and 3M items where repeat categories of a guest out number repeat items. PCIC was deployed and AB tested on the site of a major retailer, leading to significant gains in guest engagement.
</details>
<details>
<summary>摘要</summary>
请购买再次（BIA）建议对零售商来说非常重要，可以帮助改善用户体验和网站参与度，通过建议客户可能会再次购买的商品，以便客户可以更好地找到他们需要的商品。大多数现有的BIA研究都是分析客户个性化行为的项目粒度。在这种情况下，一个类别基于的模型可能更加适合。我们提出了一种推荐系统，即层次PCIC模型，它包括个性化类别模型（PC模型）和个性化类别内项模型（IC模型）。PC模型生成了个性化的类别列表，客户可能会再次购买的类别。IC模型将类别内的项目排名，用户可能会在类别内消耗的项目。层次PCIC模型使用生存模型捕捉总体产品的消耗率，并使用时间序列模型捕捉消耗趋势。这些模型的特征被用于训练一个类别粒度的神经网络。我们与12个基线对比PCIC，在四个标准开放数据集上。PCIC提高了NDCG达16%，同时提高了回归率约2%。我们可以在8个小时内扩展和训练PCIC，并在3000万名客户和3000万个商品的大数据集上进行了大规模的实验。PCIC被部署到一家大型零售商的官方网站上，并通过AB测试，导致了网站参与度的明显增长。
</details></li>
</ul>
<hr>
<h2 id="Feature-Gradient-Flow-for-Interpreting-Deep-Neural-Networks-in-Head-and-Neck-Cancer-Prediction"><a href="#Feature-Gradient-Flow-for-Interpreting-Deep-Neural-Networks-in-Head-and-Neck-Cancer-Prediction" class="headerlink" title="Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction"></a>Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13061">http://arxiv.org/abs/2307.13061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinzhu Jin, Jonathan C. Garneau, P. Thomas Fletcher</li>
<li>for: 这个论文是为了解释深度学习模型，以便更好地理解它们如何做出决策。</li>
<li>methods: 这篇论文使用了一种新的技术，即特征涌流，来解释深度学习模型。特征涌流是指模型在输入数据空间中的非线性坐标，表示模型使用的信息来做出决策。论文中的想法是测量模型的解释特征与模型的涌流之间的一致度，以评估特定特征对模型的重要性。</li>
<li>results: 论文在使用特征涌流来评估模型的解释特征方面进行了实验，并发现这种方法可以帮助提高模型的解释性。此外，论文还提出了一种在训练深度学习模型时添加一个regularization term来鼓励模型涌流与选择的解释特征相一致的技术。<details>
<summary>Abstract</summary>
This paper introduces feature gradient flow, a new technique for interpreting deep learning models in terms of features that are understandable to humans. The gradient flow of a model locally defines nonlinear coordinates in the input data space representing the information the model is using to make its decisions. Our idea is to measure the agreement of interpretable features with the gradient flow of a model. To then evaluate the importance of a particular feature to the model, we compare that feature's gradient flow measure versus that of a baseline noise feature. We then develop a technique for training neural networks to be more interpretable by adding a regularization term to the loss function that encourages the model gradients to align with those of chosen interpretable features. We test our method in a convolutional neural network prediction of distant metastasis of head and neck cancer from a computed tomography dataset from the Cancer Imaging Archive.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MARIO-Model-Agnostic-Recipe-for-Improving-OOD-Generalization-of-Graph-Contrastive-Learning"><a href="#MARIO-Model-Agnostic-Recipe-for-Improving-OOD-Generalization-of-Graph-Contrastive-Learning" class="headerlink" title="MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning"></a>MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13055">http://arxiv.org/abs/2307.13055</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhuyun97/mario">https://github.com/zhuyun97/mario</a></li>
<li>paper_authors: Yun Zhu, Haizhou Shi, Zhenshuo Zhang, Siliang Tang</li>
<li>for: 这份研究实验是为了解决无监督学习方法在 graf 数据上的 OUT-OF-DISTRIBUTION（OOD）扩展问题。</li>
<li>methods: 我们提出了一个名为 MARIO（Model-Agnostic Recipe for Improving OOD Generalizability）的方法，它运用了两个原则来发展具有分布偏移适应能力的graph对照学习方法：(i) 信息瓶颈（IB）原则，以获得一般化表示，以及(ii) 不变原则，通过对数据进行数据增强来获得不变的表示。</li>
<li>results: 我们通过实验示出，我们的方法可以在OOD测试集上实现最佳性能，同时与已有方法相比，在内部测试集上的性能保持相似。另外，我们的方法可以对数据进行增强，并且可以适应不同的分布偏移。<details>
<summary>Abstract</summary>
In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem of graph contrastive learning, with a specific focus on node-level tasks. Through extensive experiments, we demonstrate that our method achieves state-of-the-art performance on the OOD test set, while maintaining comparable performance on the in-distribution test set when compared to existing approaches. The source code for our method can be found at: https://github.com/ZhuYun97/MARIO
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们调查Graph数据上无监督学习方法的out-of-distribution（OOD）泛化问题。这种情况特别具有挑战性，因为图神经网络（GNNs）已经显示出对分布变换的敏感性，即使标签可用。为解决这个挑战，我们提议一种模型无关的recipe，我们称之为MARIO。MARIO包括两个原则，旨在开发对图数据进行分布变换的Robust graph contrastive学习方法，以超越现有框架的局限性：（i）信息瓶颈（IB）原则，以实现通用表示，和（ii）不变原则，通过对数据进行敌对数据增强来获得不变的表示。根据我们所知，这是第一个对图 contrastive学习的OOD泛化问题进行调查的研究，具体关注节点级任务。通过广泛的实验，我们证明了我们的方法在OOD测试集上具有状态机器的性能，而与现有方法相比，在同样的数据集上保持相对性的表现。MARIO的代码可以在以下链接找到：https://github.com/ZhuYun97/MARIO
</details></li>
</ul>
<hr>
<h2 id="Parallel-Q-Learning-Scaling-Off-policy-Reinforcement-Learning-under-Massively-Parallel-Simulation"><a href="#Parallel-Q-Learning-Scaling-Off-policy-Reinforcement-Learning-under-Massively-Parallel-Simulation" class="headerlink" title="Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation"></a>Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12983">http://arxiv.org/abs/2307.12983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Improbable-AI/pql">https://github.com/Improbable-AI/pql</a></li>
<li>paper_authors: Zechu Li, Tao Chen, Zhang-Wei Hong, Anurag Ajay, Pulkit Agrawal</li>
<li>for: This paper focuses on improving the efficiency of reinforcement learning for complex tasks using parallelization and GPU-based simulation.</li>
<li>methods: The paper proposes a Parallel $Q$-Learning (PQL) scheme that parallelizes data collection, policy learning, and value learning to achieve better sample efficiency and faster training times.</li>
<li>results: The authors demonstrate that PQL outperforms Proximal Policy Optimization (PPO) in wall-clock time while maintaining superior sample efficiency, and they scale the algorithm to tens of thousands of parallel environments using a single workstation.<details>
<summary>Abstract</summary>
Reinforcement learning is time-consuming for complex tasks due to the need for large amounts of training data. Recent advances in GPU-based simulation, such as Isaac Gym, have sped up data collection thousands of times on a commodity GPU. Most prior works used on-policy methods like PPO due to their simplicity and ease of scaling. Off-policy methods are more data efficient but challenging to scale, resulting in a longer wall-clock training time. This paper presents a Parallel $Q$-Learning (PQL) scheme that outperforms PPO in wall-clock time while maintaining superior sample efficiency of off-policy learning. PQL achieves this by parallelizing data collection, policy learning, and value learning. Different from prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. In experiments, we demonstrate that $Q$-learning can be scaled to \textit{tens of thousands of parallel environments} and investigate important factors affecting learning speed. The code is available at https://github.com/Improbable-AI/pql.
</details>
<details>
<summary>摘要</summary>
“强化学习需要大量的训练数据，复杂任务的强化学习需要更多的训练时间。现代GPU加速的 simulate Isaac Gym 可以在常规GPU上速度上千倍增加数据收集。大多数先前的工作使用了on-policy方法，如PPO，因为它们简单易扩展。off-policy方法更有效率，但是它们在扩展上具有挑战，导致训练时间更长。本文提出了并行$Q$-学习（PQL）方案，其在wall-clock时间内超过PPO，同时保持了离线学习的样本效率。PQL实现了这一点通过并行数据收集、政策学习和价值学习的并行。与先前的分布式离线学习方法，如Apex，不同的是，我们的方案专门针对大规模并行GPU基础设施的 simulate，并且优化了在单个工作站上运行。在实验中，我们示出了可以扩展到数以千计的并行环境，并调查了影响学习速度的重要因素。代码可以在https://github.com/Improbable-AI/pql上获取。”Note that Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world where traditional Chinese characters are used.
</details></li>
</ul>
<hr>
<h2 id="3D-LLM-Injecting-the-3D-World-into-Large-Language-Models"><a href="#3D-LLM-Injecting-the-3D-World-into-Large-Language-Models" class="headerlink" title="3D-LLM: Injecting the 3D World into Large Language Models"></a>3D-LLM: Injecting the 3D World into Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12981">http://arxiv.org/abs/2307.12981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UMass-Foundation-Model/3D-LLM">https://github.com/UMass-Foundation-Model/3D-LLM</a></li>
<li>paper_authors: Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan</li>
<li>for: 这个研究旨在提出一种新的3D语言模型（3D-LLM），用于描述和理解3D物体的空间关系和物理特性。</li>
<li>methods: 该研究使用了三种提示机制，并使用了3D特征提取器和2D视语言模型（VLM）作为后备网络，以有效地训练3D语言模型。</li>
<li>results: 实验结果表明，该模型在ScanQA测试集上比基eline模型表现出色，例如BLEU-1分数高于状态艺术分数 by 9%。此外，在3D描述、任务组合和3D协助对话等测试集上，该模型也比2D VLM更高。qualitative例子还显示了该模型可以执行更多的任务，超出现有LLMs和VLMs的范畴。<details>
<summary>Abstract</summary>
Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi- view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和视力语言模型（VLM）已经被证明可以在多个任务中表现出色，如常识验。尽管这些模型强大，但它们不是基于三维物理世界，这个世界包含丰富的概念，例如空间关系、可用性、物理、布局等。在这个工作中，我们提议将三维世界注入到大型语言模型中，并介绍一个全新的三维语言模型家族。特别是，三维语言模型可以将三维点云和它们的特征作为输入，并完成一系列三维相关任务，包括描述、密集描述、三维问题回答、任务分解、三维落实、三维辅助对话、NAVIGATION等。使用我们设计的三种推问机制，我们可以收集超过300,000个三维语言数据，覆盖这些任务。为了效率地训练三维语言模型，我们首先使用三维特征提取器，从生成的多视角图像中获取三维特征。然后，我们使用2D VLM作为我们的背景来训练我们的三维语言模型。通过引入三维本地化机制，三维语言模型可以更好地捕捉三维空间信息。实验结果显示，我们的模型在ScanQA上表现出色，输出的BLEU-1分数高于现有基eline的分数（例如，BLEU-1分数比基eline分数高出9%）。此外，我们在我们的保留集上进行了3D描述、任务分解和3D辅助对话的实验，结果显示我们的模型在2D VLM上进行了更好的表现。质性例子也显示了我们的模型可以进行更多的任务，超过现有的LLM和VLM的范围。Project Page: <https://vis-www.cs.umass.edu/3dllm/>.
</details></li>
</ul>
<hr>
<h2 id="An-Isometric-Stochastic-Optimizer"><a href="#An-Isometric-Stochastic-Optimizer" class="headerlink" title="An Isometric Stochastic Optimizer"></a>An Isometric Stochastic Optimizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12979">http://arxiv.org/abs/2307.12979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Jackson</li>
<li>for: 这篇论文是为了解释 Adam 优化器在深度学习应用中的成功，以及基于这一原理提出一种新的优化器 Iso。</li>
<li>methods: 该论文使用 Adam 优化器的原理，即每个参数的步长独立于其他参数的 нор。基于这一原理，提出一种新的优化器 Iso，其中每个参数的更新量的 norm 不受输入和输出的线性变换影响。</li>
<li>results: 论文提出的 IsoAdam 可以在训练小型 Transformer 时获得速度提升，并且可以将 Adam 中的优化参数传递给 IsoAdam，以便更好地控制训练过程。<details>
<summary>Abstract</summary>
The Adam optimizer is the standard choice in deep learning applications. I propose a simple explanation of Adam's success: it makes each parameter's step size independent of the norms of the other parameters. Based on this principle I derive Iso, a new optimizer which makes the norm of a parameter's update invariant to the application of any linear transformation to its inputs and outputs. I develop a variant of Iso called IsoAdam that allows optimal hyperparameters to be transferred from Adam, and demonstrate that IsoAdam obtains a speedup over Adam when training a small Transformer.
</details>
<details>
<summary>摘要</summary>
“阿达优化器是深度学习应用中的标准选择。我提出了阿达优化器的成功的简单解释：它使每个参数的步长独立于另一个参数的 нор。基于这个原理，我 derive Iso，一个新的优化器，其中每个参数的更新 norm 对于任何线性变换输入和输出的应用是不变。我开发了 Iso 的一种变体called IsoAdam，允许从 Adam 传输优化参数，并证明 IsoAdam 在训练小Transformer时比 Adam 快速。”Note that the word "Transformer" in the last sentence is translated as "小Transformer" (little Transformer) in Simplified Chinese, as "Transformer" is a proper noun and should be capitalized in English.
</details></li>
</ul>
<hr>
<h2 id="Provable-Benefits-of-Policy-Learning-from-Human-Preferences-in-Contextual-Bandit-Problems"><a href="#Provable-Benefits-of-Policy-Learning-from-Human-Preferences-in-Contextual-Bandit-Problems" class="headerlink" title="Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems"></a>Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12975">http://arxiv.org/abs/2307.12975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao, Mengdi Wang</li>
<li>for: This paper focuses on the problem of reward engineering in decision-making, and how human feedback can be used to learn a reward function.</li>
<li>methods: The authors develop a theory that compares the suboptimality of policy learning methods with and without human feedback, and show that preference-based methods have lower suboptimality.</li>
<li>results: The authors prove that preference-based methods enjoy lower suboptimality in offline contextual bandits, and demonstrate the benefits of using human feedback to learn a reward function.<details>
<summary>Abstract</summary>
A crucial task in decision-making problems is reward engineering. It is common in practice that no obvious choice of reward function exists. Thus, a popular approach is to introduce human feedback during training and leverage such feedback to learn a reward function. Among all policy learning methods that use human feedback, preference-based methods have demonstrated substantial success in recent empirical applications such as InstructGPT. In this work, we develop a theory that provably shows the benefits of preference-based methods in offline contextual bandits. In particular, we improve the modeling and suboptimality analysis for running policy learning methods on human-scored samples directly. Then, we compare it with the suboptimality guarantees of preference-based methods and show that preference-based methods enjoy lower suboptimality.
</details>
<details>
<summary>摘要</summary>
决策问题中一项重要任务是奖励工程。在实践中，没有明显的奖励函数选择。因此，一种popular的方法是在训练过程中引入人工反馈，并利用这些反馈来学习奖励函数。在所有基于policy学习方法中使用人工反馈的情况下，我们发展了一种理论，可以证明 preference-based方法在Offline Contextual Bandits中的优点。具体来说，我们改进了运行policy学习方法直接使用人工分配的样本的模型和不优等分析。然后，我们与 preference-based方法的不优等保证进行比较，并证明 preference-based方法具有更低的不优等。
</details></li>
</ul>
<hr>
<h2 id="Big-Data-Supply-Chain-Management-Framework-for-Forecasting-Data-Preprocessing-and-Machine-Learning-Techniques"><a href="#Big-Data-Supply-Chain-Management-Framework-for-Forecasting-Data-Preprocessing-and-Machine-Learning-Techniques" class="headerlink" title="Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques"></a>Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12971">http://arxiv.org/abs/2307.12971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed Ridoy, Yoichi Tomioka, M. F. Mridha</li>
<li>for: This paper aims to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies, and to propose a novel framework incorporating Big Data Analytics in SC Management.</li>
<li>methods: The proposed framework includes problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization.</li>
<li>results: The paper discusses the need for different types of forecasting according to the period or SC objective, and recommends SC KPIs and error-measurement systems to optimize the top-performing model. It also illustrates the adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文目的是系统地研究现代供应链（SC）预测策略和技术，并提出一种 incorporating Big Data Analytics in SC Management的新框架。</li>
<li>methods: 该框架包括问题标识、数据源、探索数据分析、机器学习模型训练、参数优化、性能评估和优化。</li>
<li>results: 论文介绍了不同时期或SC目标的预测需求，并建议SC KPIs和错误度量系统来优化最佳模型。它还图示了预测phantom存在的副作用和SC KPIs对管理决策的依赖关系，以及如何通过SC KPIs来提高运营管理、透明度和规划效率。<details>
<summary>Abstract</summary>
This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency have been illustrated. The cyclic connection within the framework introduces preprocessing optimization based on the post-process KPIs, optimizing the overall control process (inventory management, workforce determination, cost, production and capacity planning). The contribution of this research lies in the standard SC process framework proposal, recommended forecasting data analysis, forecasting effects on SC performance, machine learning algorithms optimization followed, and in shedding light on future research.
</details>
<details>
<summary>摘要</summary>
Firstly, the article discusses the importance of collecting data according to SC strategy and the various types of forecasting needed based on the period or SC objective. The article also recommends using SC KPIs and error-measurement systems to optimize the top-performing model. Additionally, the adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency are highlighted.The proposed framework includes a cyclic connection that optimizes preprocessing based on post-process KPIs, which in turn optimizes the overall control process, including inventory management, workforce determination, cost, production, and capacity planning. The contribution of this research lies in the standard SC process framework proposal, the recommended forecasting data analysis, the forecasting effects on SC performance, the optimization of machine learning algorithms, and the shedding of light on future research directions.
</details></li>
</ul>
<hr>
<h2 id="A-Connection-between-One-Step-Regularization-and-Critic-Regularization-in-Reinforcement-Learning"><a href="#A-Connection-between-One-Step-Regularization-and-Critic-Regularization-in-Reinforcement-Learning" class="headerlink" title="A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning"></a>A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12968">http://arxiv.org/abs/2307.12968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ben-eysenbach/ac-connection">https://github.com/ben-eysenbach/ac-connection</a></li>
<li>paper_authors: Benjamin Eysenbach, Matthieu Geist, Sergey Levine, Ruslan Salakhutdinov</li>
<li>for: 这个论文主要关注在Offline Reinforcement Learning（RL）中，提出了一种新的方法来处理有限数据的问题。</li>
<li>methods: 论文使用了一种新的多步批处理方法，称为多步批处理评估器补偿（Multi-Step Critic Regularization），以及一种已有的一步政策提高方法（One-Step Policy Improvement）。</li>
<li>results: 论文通过实验表明，Multi-Step Critic Regularization方法可以与一步政策提高方法相比肯定地性能，但是需要更多的计算资源。此外，论文还发现，在实际应用中，使用一步政策提高方法可以达到相对较好的性能。<details>
<summary>Abstract</summary>
As with any machine learning problem with limited data, effective offline RL algorithms require careful regularization to avoid overfitting. One-step methods perform regularization by doing just a single step of policy improvement, while critic regularization methods do many steps of policy improvement with a regularized objective. These methods appear distinct. One-step methods, such as advantage-weighted regression and conditional behavioral cloning, truncate policy iteration after just one step. This ``early stopping'' makes one-step RL simple and stable, but can limit its asymptotic performance. Critic regularization typically requires more compute but has appealing lower-bound guarantees. In this paper, we draw a close connection between these methods: applying a multi-step critic regularization method with a regularization coefficient of 1 yields the same policy as one-step RL. While practical implementations violate our assumptions and critic regularization is typically applied with smaller regularization coefficients, our experiments nevertheless show that our analysis makes accurate, testable predictions about practical offline RL methods (CQL and one-step RL) with commonly-used hyperparameters. Our results that every problem can be solved with a single step of policy improvement, but rather that one-step RL might be competitive with critic regularization on RL problems that demand strong regularization.
</details>
<details>
<summary>摘要</summary>
如同任何机器学习问题受限数据，有效的离线RL算法需要仔细的规范来避免过拟合。一步方法通过做出单一步骤的政策改进来进行规范，而批评规范方法则通过多步政策改进来实现规范。这些方法看起来各有其特点。一步方法，如利得加重回归和conditionalBehavioral Cloning，在政策迭代过程中 truncate policy iteration  после一步。这种``早停''使一步RL简单和稳定，但可能限制其极限性能。批评规范通常需要更多的计算资源，但具有吸引人的下界保证。在这篇论文中，我们 Draw a close connection between these methods：在应用多步批评规范方法时，使用规范系数为1会得到与一步RL相同的政策。虽然实践中的假设不一定成立，但我们的实验表明，我们的分析对实际的离线RL方法（CQL和一步RL）的实践中的参数进行了准确和可靠的预测。我们的结果表明，每个问题都可以通过单一步骤的政策改进来解决，但是一步RL可能与批评规范在RL问题中具有强规范的情况相继。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dense-Correspondences-between-Photos-and-Sketches"><a href="#Learning-Dense-Correspondences-between-Photos-and-Sketches" class="headerlink" title="Learning Dense Correspondences between Photos and Sketches"></a>Learning Dense Correspondences between Photos and Sketches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12967">http://arxiv.org/abs/2307.12967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cogtoolslab/photo-sketch-correspondence">https://github.com/cogtoolslab/photo-sketch-correspondence</a></li>
<li>paper_authors: Xuanchen Lu, Xiaolong Wang, Judith E Fan</li>
<li>for: 本研究旨在开发人工智能系统，能够更人类化地理解视觉图像。</li>
<li>methods: 本研究使用自我超vised学习方法，利用抽象学习和对比学习来学习 dense对应关系 между绘图和照片。</li>
<li>results: 研究发现，使用提案的方法可以比以往方法更好地预测绘图和照片之间的对应关系，但是与人类预测结果存在系统差异。<details>
<summary>Abstract</summary>
Humans effortlessly grasp the connection between sketches and real-world objects, even when these sketches are far from realistic. Moreover, human sketch understanding goes beyond categorization -- critically, it also entails understanding how individual elements within a sketch correspond to parts of the physical world it represents. What are the computational ingredients needed to support this ability? Towards answering this question, we make two contributions: first, we introduce a new sketch-photo correspondence benchmark, $\textit{PSC6k}$, containing 150K annotations of 6250 sketch-photo pairs across 125 object categories, augmenting the existing Sketchy dataset with fine-grained correspondence metadata. Second, we propose a self-supervised method for learning dense correspondences between sketch-photo pairs, building upon recent advances in correspondence learning for pairs of photos. Our model uses a spatial transformer network to estimate the warp flow between latent representations of a sketch and photo extracted by a contrastive learning-based ConvNet backbone. We found that this approach outperformed several strong baselines and produced predictions that were quantitatively consistent with other warp-based methods. However, our benchmark also revealed systematic differences between predictions of the suite of models we tested and those of humans. Taken together, our work suggests a promising path towards developing artificial systems that achieve more human-like understanding of visual images at different levels of abstraction. Project page: https://photo-sketch-correspondence.github.io
</details>
<details>
<summary>摘要</summary>
人类能够轻松地理解绘图和实际物体之间的连接，即使绘图非常不真实。此外，人类绘图理解不仅是分类，更重要的是理解绘图中的各个元素与物体实际上的部分之间的对应关系。为解答这个问题，我们提出了两个贡献：首先，我们 introduce a new sketch-photo correspondence benchmark， $\textit{PSC6k}$, which contains 150,000 annotations of 6250 sketch-photo pairs across 125 object categories, augmenting the existing Sketchy dataset with fine-grained correspondence metadata.其次，我们提出了一种自动学习的方法，用于学习绘图和照片之间的紧密对应关系。我们的模型使用一个空间变换网络来估计绘图和照片之间的扭变流，并基于最近的对话学习方法来学习对应关系。我们发现这种方法可以超过多个强大基eline，并生成与其他扭变基于方法相似的预测。然而，我们的 benchmark 还发现了人类预测和机器学习模型的系统性差异。总的来说，我们的工作表明了在不同的抽象水平上建立更人类化的视觉图像理解系统的可能性。项目页面：https://photo-sketch-correspondence.github.io
</details></li>
</ul>
<hr>
<h2 id="Synthetic-pre-training-for-neural-network-interatomic-potentials"><a href="#Synthetic-pre-training-for-neural-network-interatomic-potentials" class="headerlink" title="Synthetic pre-training for neural-network interatomic potentials"></a>Synthetic pre-training for neural-network interatomic potentials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15714">http://arxiv.org/abs/2307.15714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jla-gardner/nnp-pre-training">https://github.com/jla-gardner/nnp-pre-training</a></li>
<li>paper_authors: John L. A. Gardner, Kathryn T. Baker, Volker L. Deringer</li>
<li>for: 这篇论文主要用于探讨如何使用人工数据来预训育神经网络interatomic potential模型，以提高计算稳定性和精度。</li>
<li>methods: 该论文使用了一种基于graph neural network的equivariant interatomic potential模型，并使用了大量的人工数据进行预训育。</li>
<li>results: 研究发现，通过使用人工数据进行预训育，可以提高神经网络interatomic potential模型的数值稳定性和精度，并且可以避免使用大量的量子力学参考数据。<details>
<summary>Abstract</summary>
Machine learning (ML) based interatomic potentials have transformed the field of atomistic materials modelling. However, ML potentials depend critically on the quality and quantity of quantum-mechanical reference data with which they are trained, and therefore developing datasets and training pipelines is becoming an increasingly central challenge. Leveraging the idea of "synthetic" (artificial) data that is common in other areas of ML research, we here show that synthetic atomistic data, themselves obtained at scale with an existing ML potential, constitute a useful pre-training task for neural-network interatomic potential models. Once pre-trained with a large synthetic dataset, these models can be fine-tuned on a much smaller, quantum-mechanical one, improving numerical accuracy and stability in computational practice. We demonstrate feasibility for a series of equivariant graph-neural-network potentials for carbon, and we carry out initial experiments to test the limits of the approach.
</details>
<details>
<summary>摘要</summary>
Building on the idea of "synthetic" (artificial) data that is common in other areas of ML research, we show here that synthetic atomistic data, obtained at scale with an existing ML potential, can serve as a useful pre-training task for neural-network interatomic potential models. By pre-training these models with a large synthetic dataset, we can improve their numerical accuracy and stability in computational practice.We demonstrate the feasibility of this approach for a series of equivariant graph-neural-network potentials for carbon, and carry out initial experiments to test the limits of the approach.
</details></li>
</ul>
<hr>
<h2 id="Efficiently-Sampling-the-PSD-Cone-with-the-Metric-Dikin-Walk"><a href="#Efficiently-Sampling-the-PSD-Cone-with-the-Metric-Dikin-Walk" class="headerlink" title="Efficiently Sampling the PSD Cone with the Metric Dikin Walk"></a>Efficiently Sampling the PSD Cone with the Metric Dikin Walk</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12943">http://arxiv.org/abs/2307.12943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunbum Kook, Santosh S. Vempala</li>
<li>for: 本研究旨在提高半定义程序的效率 computation.</li>
<li>methods: 本文使用了Dikin walk，并对其进行了扩展和改进，以提高混合时间和每步复杂度。</li>
<li>results: 研究人员通过适当选择 метри，使得运行时间与数量约束之间的关系变为多项式的。此外，本文还发展了内部点方法的规则，用于混合。<details>
<summary>Abstract</summary>
Semi-definite programs represent a frontier of efficient computation. While there has been much progress on semi-definite optimization, with moderate-sized instances currently solvable in practice by the interior-point method, the basic problem of sampling semi-definite solutions remains a formidable challenge. The direct application of known polynomial-time algorithms for sampling general convex bodies to semi-definite sampling leads to a prohibitively high running time. In addition, known general methods require an expensive rounding phase as pre-processing. Here we analyze the Dikin walk, by first adapting it to general metrics, then devising suitable metrics for the PSD cone with affine constraints. The resulting mixing time and per-step complexity are considerably smaller, and by an appropriate choice of the metric, the dependence on the number of constraints can be made polylogarithmic. We introduce a refined notion of self-concordant matrix functions and give rules for combining different metrics. Along the way, we further develop the theory of interior-point methods for sampling.
</details>
<details>
<summary>摘要</summary>
半定义程序表示一种高效计算的前沿。虽然有很多进步在半定义优化方面，但现在只有一些小规模实例可以通过内部点方法实际应用，而大规模实例仍然是一个困难的挑战。直接将通用曲线体的抽样算法应用到半定义抽样问题会导致非常高的运行时间。此外，已知的通用方法均需进行昂贵的舒缩阶段作为先决条件。我们分析了迪金步行，首先将其扩展到普通度量空间，然后在PSD几何中选择合适的度量。这将导致混合时间和每步复杂度减小至可polylogarithmic水平，并且通过合适的度量选择，可以消除对约束数量的依赖。我们还进一步发展了内部点方法的规范理论。
</details></li>
</ul>
<hr>
<h2 id="On-Privileged-and-Convergent-Bases-in-Neural-Network-Representations"><a href="#On-Privileged-and-Convergent-Bases-in-Neural-Network-Representations" class="headerlink" title="On Privileged and Convergent Bases in Neural Network Representations"></a>On Privileged and Convergent Bases in Neural Network Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12941">http://arxiv.org/abs/2307.12941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davis Brown, Nikhil Vyas, Yamini Bansal</li>
<li>for:  investigate whether the representations learned by neural networks possess a privileged and convergent basis</li>
<li>methods:  examine the significance of feature directions represented by individual neurons, compare the bases of networks trained with the same parameters but with varying random initializations</li>
<li>results:  (1) neural networks do not converge to a unique basis, (2) basis correlation increases significantly when a few early layers of the network are frozen identically, Linear Mode Connectivity improves with increased network width but not due to an increase in basis correlation.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个研究是 investigate whether the representations learned by neural networks possess a privileged and convergent basis</li>
<li>methods: 研究具体是 examining the significance of feature directions represented by individual neurons, comparing the bases of networks trained with the same parameters but with varying random initializations</li>
<li>results: (1)  neural networks do not converge to a unique basis, (2) basis correlation increases significantly when a few early layers of the network are frozen identically, Linear Mode Connectivity improves with increased network width but not due to an increase in basis correlation.<details>
<summary>Abstract</summary>
In this study, we investigate whether the representations learned by neural networks possess a privileged and convergent basis. Specifically, we examine the significance of feature directions represented by individual neurons. First, we establish that arbitrary rotations of neural representations cannot be inverted (unlike linear networks), indicating that they do not exhibit complete rotational invariance. Subsequently, we explore the possibility of multiple bases achieving identical performance. To do this, we compare the bases of networks trained with the same parameters but with varying random initializations. Our study reveals two findings: (1) Even in wide networks such as WideResNets, neural networks do not converge to a unique basis; (2) Basis correlation increases significantly when a few early layers of the network are frozen identically.   Furthermore, we analyze Linear Mode Connectivity, which has been studied as a measure of basis correlation. Our findings give evidence that while Linear Mode Connectivity improves with increased network width, this improvement is not due to an increase in basis correlation.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了神经网络学习的表示方式是否具有特权和共聚基准。 Specifically，我们研究了神经元单个方向上的特征方向是否具有重要 significanc。 我们首先证明了神经网络中的表示不能逆转（不同于线性网络），这表明它们不具备完全旋转不变性。 接着，我们探索了多个基准是否可以实现相同的性能。 To do this, we compare the bases of networks trained with the same parameters but with varying random initializations. 我们的研究发现了两点：1. 即使是宽度较大的网络，如WideResNets，神经网络也不会 converges to a unique basis。2. 基准相关性在一些早期层被冻结时显著增加。此外，我们还分析了线性模式连接，这种连接已经被研究为基准相关性的度量。 我们的发现表明，虽然线性模式连接在网络宽度增加时会改善，但这种改善不是基准相关性的提高。
</details></li>
</ul>
<hr>
<h2 id="HOOD-Real-Time-Robust-Human-Presence-and-Out-of-Distribution-Detection-with-Low-Cost-FMCW-Radar"><a href="#HOOD-Real-Time-Robust-Human-Presence-and-Out-of-Distribution-Detection-with-Low-Cost-FMCW-Radar" class="headerlink" title="HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar"></a>HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02396">http://arxiv.org/abs/2308.02396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabri Mustafa Kahya, Muhammet Sami Yavuz, Eckehard Steinbach</li>
<li>For: 这个论文主要关注在indoor环境中的人员存在检测，使用60GHz短距离FMCW雷达，并提出了一种实时 Robust人员存在和非典型检测方法（HOOD）。* Methods: 该方法基于重构建模型，使用雷达宏和微距离Doppler图像（RDI），并将存在检测应用和非典型检测问题一起解决。* Results: 在收集的60GHz短距离FMCW雷达数据集上，HOOD方法实现了平均AUROC为94.36%，并在不同的人类情况下表现良好。此外，HOOD方法也在相对评估中表现出色，超过了现有的OOD检测方法。<details>
<summary>Abstract</summary>
Human presence detection in indoor environments using millimeter-wave frequency-modulated continuous-wave (FMCW) radar is challenging due to the presence of moving and stationary clutters in indoor places. This work proposes "HOOD" as a real-time robust human presence and out-of-distribution (OOD) detection method by exploiting 60 GHz short-range FMCW radar. We approach the presence detection application as an OOD detection problem and solve the two problems simultaneously using a single pipeline. Our solution relies on a reconstruction-based architecture and works with radar macro and micro range-Doppler images (RDIs). HOOD aims to accurately detect the "presence" of humans in the presence or absence of moving and stationary disturbers. Since it is also an OOD detector, it aims to detect moving or stationary clutters as OOD in humans' absence and predicts the current scene's output as "no presence." HOOD is an activity-free approach that performs well in different human scenarios. On our dataset collected with a 60 GHz short-range FMCW Radar, we achieve an average AUROC of 94.36%. Additionally, our extensive evaluations and experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD detection methods in terms of common OOD detection metrics. Our real-time experiments are available at: https://muskahya.github.io/HOOD
</details>
<details>
<summary>摘要</summary>
人体存在检测在室内环境中使用毫米波频率调制连续波 (FMCW) 雷达是具有挑战性，因为室内的移动和静止干扰物会影响检测的准确性。这项工作提出了一种实时可靠的人体存在和异常检测方法，称为“HOOD”，通过利用60GHz短距离FMCW雷达。我们将存在检测应用作为异常检测问题，并将两个问题同时解决使用单一管道。我们的解决方案基于重建建筑，并与雷达宏和微范围Doppler图像 (RDI) 结合使用。HOOD的目标是准确检测人体在存在或缺失移动和静止干扰物的情况下的存在。由于它还是异常检测器，它将在人体缺失情况下检测移动或静止干扰物为异常，并预测当前场景的输出为“无存在”。HOOD是一种无活动的方法，可以在不同的人类情况下表现良好。在我们使用60GHz短距离FMCW雷达收集的数据集上，我们实现了平均AUROC为94.36%。此外，我们的广泛的评估和实验表明，HOOD超过了当前最佳异常检测方法的表现。我们的实时实验可以在以下链接中找到：https://muskahya.github.io/HOOD
</details></li>
</ul>
<hr>
<h2 id="Contextual-Bandits-and-Imitation-Learning-via-Preference-Based-Active-Queries"><a href="#Contextual-Bandits-and-Imitation-Learning-via-Preference-Based-Active-Queries" class="headerlink" title="Contextual Bandits and Imitation Learning via Preference-Based Active Queries"></a>Contextual Bandits and Imitation Learning via Preference-Based Active Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12926">http://arxiv.org/abs/2307.12926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu</li>
<li>for: 本研究旨在解决 Contextual Bandits 和模仿学习问题，learner 缺乏直接行动结果奖励的知识。相反，learner 可以在每个回合中向专家查询两个动作，并获得噪声性偏好反馈。learner 的目标是同时减少执行动作的 regret，以及减少向专家查询的次数。</li>
<li>methods: 本研究使用了一种基于函数类的算法，利用在线回归权限来选择动作和决定查询专家。对 Contextual Bandits Setting 来说，我们的算法可以达到 $O(\min{\sqrt{T}, d&#x2F;\Delta})$ 的 regret bound，其中 $T$ 表示互动次数，$d$ 表示函数类的远近维度，$\Delta$ 表示最佳动作对所有上下文的最小偏好。我们的算法不需要知道 $\Delta$，并且获得的 regret bound 与标准 Contextual Bandits 设置中 observer reward signal 的情况相当。此外，我们的算法只需要向专家进行 $O(\min{T, d^2&#x2F;\Delta^2})$ 次查询。</li>
<li>results: 我们的算法可以在 Contextual Bandits 和模仿学习 setting 中减少 regret 和查询次数，同时可以在模仿学习 setting 中超越专家，即 learner 可以在专家不优秀时 Still learn to outperform the expert。这展示了 preference-based feedback 在模仿学习中的实际优势。<details>
<summary>Abstract</summary>
We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively query an expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize the regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions, and provide an algorithm that leverages an online regression oracle with respect to this function class for choosing its actions and deciding when to query. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\{\sqrt{T}, d/\Delta\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\{T, d^2/\Delta^2\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the learning agent engages with an unknown environment in episodes of length $H$ each, and provide similar guarantees for regret and query complexity. Interestingly, our algorithm for imitation learning can even learn to outperform the underlying expert, when it is suboptimal, highlighting a practical benefit of preference-based feedback in imitation learning.
</details>
<details>
<summary>摘要</summary>
我们考虑了上下文游戏和仿制学习问题，learner缺乏直接行动的奖励信息。相反，learner可以在每个回合中活动地询问专家，以获得不准确的偏好反馈。learner的目标是两重：一是减少执行的奖励相关的 regret，二是减少询问专家的次数。在这篇论文中，我们假设learner有访问一个专家偏好模型函数类型的权限，并提供了一个基于这个函数类型的在线回归或acles的算法，用于选择行动和决定何时询问。为上下文游戏设置，我们的算法实现了一个 regret 的下界，其比例为 $O(\min\{\sqrt{T}, d/\Delta\})$, 其中 $T$ 表示互动次数， $d$ 表示函数类型的远征维度， $\Delta$ 表示最优行动在所有上下文中的最小偏好。我们的算法不需要了解 $\Delta$，并且获得的 regret 下界与标准上下文游戏设置中的 reward 信号observation 相当。此外，我们的算法只需要 $O(\min\{T, d^2/\Delta^2\})$ 次询问专家。然后，我们扩展了我们的算法到仿制学习设置，learner在每个回合中与未知环境互动，并提供了类似的 regret 和询问次数下界。有趣的是，我们的算法可以在仿制学习中学习出perform 更高的专家，即使专家是不优的，这 highlights 仿制学习中 preference-based 反馈的实际优势。
</details></li>
</ul>
<hr>
<h2 id="A-new-derivative-free-optimization-method-Gaussian-Crunching-Search"><a href="#A-new-derivative-free-optimization-method-Gaussian-Crunching-Search" class="headerlink" title="A new derivative-free optimization method: Gaussian Crunching Search"></a>A new derivative-free optimization method: Gaussian Crunching Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14359">http://arxiv.org/abs/2307.14359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benny Wong</li>
<li>for: 这个研究论文的目的是提出一种新的优化方法 called Gaussian Crunching Search (GCS)，用于解决复杂问题。</li>
<li>methods: 该方法基于独特的 Gaussian distribution 行为，旨在有效地探索解决空间并尝试达到全局最优解。</li>
<li>results: 通过实验评估和与现有优化方法比较，我们展示了 GCS 的优势和特点，并证明了它在优化问题中的可行性和应用前景。<details>
<summary>Abstract</summary>
Optimization methods are essential in solving complex problems across various domains. In this research paper, we introduce a novel optimization method called Gaussian Crunching Search (GCS). Inspired by the behaviour of particles in a Gaussian distribution, GCS aims to efficiently explore the solution space and converge towards the global optimum. We present a comprehensive analysis of GCS, including its working mechanism, and potential applications. Through experimental evaluations and comparisons with existing optimization methods, we highlight the advantages and strengths of GCS. This research paper serves as a valuable resource for researchers, practitioners, and students interested in optimization, providing insights into the development and potential of Gaussian Crunching Search as a new and promising approach.
</details>
<details>
<summary>摘要</summary>
优化方法是解决复杂问题的关键，在不同领域中都非常重要。在这篇研究论文中，我们介绍了一种新的优化方法—— Gaussian Crunching Search（GCS）。GCS Drawing inspiration from the behavior of particles in a Gaussian distribution，GCS aims to efficiently explore the solution space and converge towards the global optimum。我们提供了GCS的工作机制和潜在应用，并通过实验和现有优化方法的比较，把GCS的优势和特点 highlighted。这篇研究论文对研究者、实践者和学生们有益，提供了GCS的开发和应用的深入了解，以及GCS作为一种新的优化方法的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-For-Mapping-Variables-Between-Programs-–-Extended-Version"><a href="#Graph-Neural-Networks-For-Mapping-Variables-Between-Programs-–-Extended-Version" class="headerlink" title="Graph Neural Networks For Mapping Variables Between Programs – Extended Version"></a>Graph Neural Networks For Mapping Variables Between Programs – Extended Version</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13014">http://arxiv.org/abs/2307.13014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pmorvalho/ecai23-gnns-for-mapping-variables-between-programs">https://github.com/pmorvalho/ecai23-gnns-for-mapping-variables-between-programs</a></li>
<li>paper_authors: Pedro Orvalho, Jelle Piepenbrock, Mikoláš Janota, Vasco Manquinho</li>
<li>for: 这个论文主要研究了如何使用图ael neural networks（GNNs）将两个程序之间的变量映射到一起，以便进行程序相似性、程序分析、程序修复和冲击检测等任务。</li>
<li>methods: 该论文提出了使用GNNs将两个程序的抽象 sintaxis树（ASTs）中的变量映射到一起，以便进行变量映射。</li>
<li>results:  experiments表明，该方法可以 Correctly map 83% of the evaluation dataset, 而且比现有的程序修复方法（主要基于程序结构）更高，可以修复约72%的错误程序。<details>
<summary>Abstract</summary>
Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/correct programs show that our approach correctly maps 83% of the evaluation dataset. Moreover, our experiments show that the current state-of-the-art on program repair, greatly dependent on the programs' structure, can only repair about 72% of the incorrect programs. In contrast, our approach, which is solely based on variable mappings, can repair around 88.5%.
</details>
<details>
<summary>摘要</summary>
自动化程序分析是计算机科学多个领域的关键研究领域，特别是正式方法和人工智能。由于程序相等性的问题是不可解决的，因此比较两个程序非常困难。通常需要在两个程序的变量集之间建立一种关系，以便进行程序相等性、程序分析、程序修复和克隆检测等任务。在这项工作中，我们提议使用图神经网络（GNN）将两个程序的变量集映射到同一个空间中。为了证明变量映射的强大性，我们在程序修复任务上提供三个使用情况，包括修复 novice 程序员在入门编程作业（IPA）中经常出现的错误。实验结果表明，我们的方法可以在4166对 incorrect/correct 程序的评估集上正确地映射83%的评估集。此外，我们的实验还表明，现有的程序修复方法，很大程度上依赖于程序的结构，只能修复约72%的错误程序。相比之下，我们的方法，完全基于变量映射，可以修复约88.5%的错误程序。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/cs.LG_2023_07_25/" data-id="cllsj1rox007dpf88djtf3yya" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/eess.IV_2023_07_25/" class="article-date">
  <time datetime="2023-07-24T16:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/25/eess.IV_2023_07_25/">eess.IV - 2023-07-25 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Unifying-Anatomy-Segmentation-Automated-Generation-of-a-Full-body-CT-Dataset-via-Knowledge-Aggregation-and-Anatomical-Guidelines"><a href="#Towards-Unifying-Anatomy-Segmentation-Automated-Generation-of-a-Full-body-CT-Dataset-via-Knowledge-Aggregation-and-Anatomical-Guidelines" class="headerlink" title="Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines"></a>Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13375">http://arxiv.org/abs/2307.13375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexanderjaus/atlasdataset">https://github.com/alexanderjaus/atlasdataset</a></li>
<li>paper_authors: Alexander Jaus, Constantin Seibold, Kelsey Hermann, Alexandra Walter, Kristina Giske, Johannes Haubold, Jens Kleesiek, Rainer Stiefelhagen</li>
<li>for: 这个研究是为了生成自动生成的解剖学分割数据集，使用sequential进程，包括基于nnU-Net的pseudo标签和受体导向的pseudo标签重定向。</li>
<li>methods: 这个方法 combinese多个分割的知识库，生成了533个扫描图像的全身CT扫描数据集，提供了全面的解剖学覆盖，并且不需要手动标注。</li>
<li>results: 我们的方法通过了人工专家评估，在BTCV数据集上达到了85%的dice分数，并通过了医学有效性检查。<details>
<summary>Abstract</summary>
In this study, we present a method for generating automated anatomy segmentation datasets using a sequential process that involves nnU-Net-based pseudo-labeling and anatomy-guided pseudo-label refinement. By combining various fragmented knowledge bases, we generate a dataset of whole-body CT scans with $142$ voxel-level labels for 533 volumes providing comprehensive anatomical coverage which experts have approved. Our proposed procedure does not rely on manual annotation during the label aggregation stage. We examine its plausibility and usefulness using three complementary checks: Human expert evaluation which approved the dataset, a Deep Learning usefulness benchmark on the BTCV dataset in which we achieve 85% dice score without using its training dataset, and medical validity checks. This evaluation procedure combines scalable automated checks with labor-intensive high-quality expert checks. Besides the dataset, we release our trained unified anatomical segmentation model capable of predicting $142$ anatomical structures on CT data.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提出了一种方法，使用序列过程，使用nnU-Net基于pseudo标签和指导pseudo标签修正来生成自动化解剖数据集。通过结合不同的分割知识库，我们生成了一个整体CT扫描图像的数据集，包含142个 voxel 级别标签，对533幅图像提供了全面的解剖学覆盖，经专家认可。我们的提posed方法不依赖于手动标注 during label合并阶段。我们使用三种 complementary 检查来评估其可行性和有用性：人类专家评估，btcv dataset上的深度学习有用性 benchamrk，以及医学有效性检查。这种评估过程结合了可扩展的自动化检查和劳动密集的高质量专家检查。除了数据集之外，我们还发布了我们训练过的统一解剖 segmentation 模型，可以在CT数据上预测142个解剖结构。
</details></li>
</ul>
<hr>
<h2 id="Overcoming-Distribution-Mismatch-in-Quantizing-Image-Super-Resolution-Networks"><a href="#Overcoming-Distribution-Mismatch-in-Quantizing-Image-Super-Resolution-Networks" class="headerlink" title="Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks"></a>Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13337">http://arxiv.org/abs/2307.13337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheeun Hong, Kyoung Mu Lee</li>
<li>for: 这个 paper 的目的是提出一个新的对于图像超解析网络的量化方法，以缩短计算量且保持高度对于图像质量的性能。</li>
<li>methods: 这个 paper 使用了一个新的量化框架，将对于每个通道或输入图像的特征分布与量化范围进行静态匹配，而不需要在试用时进行动态适应。这个框架通过对特征分布进行直接训练时间REGULARIZATION，以降低分布差异问题。此外，为了进一步降低分布差异，paper 还引入了分布偏移，以对具有明显差异的通道进行偏移。</li>
<li>results: 实验结果显示，这个 paper 的方法可以对图像超解析网络进行有效的量化，且与同等或少于计算量的现有方法相比，可以提高图像质量。code 可以在 <a target="_blank" rel="noopener" href="https://github.com/Cheeun/ODM">https://github.com/Cheeun/ODM</a> 上获得。<details>
<summary>Abstract</summary>
Quantization is a promising approach to reduce the high computational complexity of image super-resolution (SR) networks. However, compared to high-level tasks like image classification, low-bit quantization leads to severe accuracy loss in SR networks. This is because feature distributions of SR networks are significantly divergent for each channel or input image, and is thus difficult to determine a quantization range. Existing SR quantization works approach this distribution mismatch problem by dynamically adapting quantization ranges to the variant distributions during test time. However, such dynamic adaptation incurs additional computational costs that limit the benefits of quantization. Instead, we propose a new quantization-aware training framework that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, the mismatch can be reduced by directly regularizing the variance in features during training. However, we observe that variance regularization can collide with the reconstruction loss during training and adversely impact SR accuracy. Thus, we avoid the conflict between two losses by regularizing the variance only when the gradients of variance regularization are cooperative with that of reconstruction. Additionally, to further reduce the distribution mismatch, we introduce distribution offsets to layers with a significant mismatch, which either scales or shifts channel-wise features. Our proposed algorithm, called ODM, effectively reduces the mismatch in distributions with minimal computational overhead. Experimental results show that ODM effectively outperforms existing SR quantization approaches with similar or fewer computations, demonstrating the importance of reducing the distribution mismatch problem. Our code is available at https://github.com/Cheeun/ODM.
</details>
<details>
<summary>摘要</summary>
“量化是一种有前途的方法，可以降低图像超分辨率网络的计算复杂度。然而，与高级任务像图像分类一样，低位数量化会导致SR网络的准确性下降。这是因为SR网络的特征分布是每个通道或输入图像都异常分散，因此决定量化范围是困难的。现有的SR量化方法通过在测试时动态适应量化范围来解决这个分布匹配问题。然而，这种动态适应带来了额外的计算成本，限制了量化的 benefita。而我们提议的新的量化掌握框架可以有效地超越分布匹配问题，无需动态适应。”“我们发现，可以通过直接在训练时对特征的方差进行正则化来减少分布匹配问题。然而，我们发现，在训练时对方差进行正则化可能会与重建loss冲突，从而 adversely affect SR准确性。因此，我们避免了这两个损失之间的冲突，通过只在方差正则化的梯度和重建loss之间协同时进行正则化。此外，为了进一步减少分布匹配问题，我们引入了分布偏移，将 channel-wise 特征的分布偏移到不同的水平。我们的提议的算法，称为ODM，可以有效地减少分布匹配问题，并且计算成本很低。”“我们的实验结果表明，ODM可以比现有的SR量化方法更高效地进行量化，同时保持相同或更多的计算资源。这表明，降低分布匹配问题的重要性。我们的代码可以在https://github.com/Cheeun/ODM中找到。”
</details></li>
</ul>
<hr>
<h2 id="A-Visual-Quality-Assessment-Method-for-Raster-Images-in-Scanned-Document"><a href="#A-Visual-Quality-Assessment-Method-for-Raster-Images-in-Scanned-Document" class="headerlink" title="A Visual Quality Assessment Method for Raster Images in Scanned Document"></a>A Visual Quality Assessment Method for Raster Images in Scanned Document</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13241">http://arxiv.org/abs/2307.13241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Yang, Peter Bauer, Todd Harris, Changhyung Lee, Hyeon Seok Seo, Jan P Allebach, Fengqing Zhu</li>
<li>for: 本研究探讨了扫描文档中的图像质量评估（IQA），特别是针对照片区域的视觉质量。</li>
<li>methods: 我们提出了一种基于机器学习的分类方法，用于判断扫描照片区域的视觉质量是否可接受，并使用人类评分来确定可接受的图像质量标准。</li>
<li>results: 我们的结果表明，通过在训练中添加噪声模型来模拟扫描过程中的质量下降，可以显著改善分类器的性能，以判断扫描照片区域的视觉质量是否可接受。<details>
<summary>Abstract</summary>
Image quality assessment (IQA) is an active research area in the field of image processing. Most prior works focus on visual quality of natural images captured by cameras. In this paper, we explore visual quality of scanned documents, focusing on raster image areas. Different from many existing works which aim to estimate a visual quality score, we propose a machine learning based classification method to determine whether the visual quality of a scanned raster image at a given resolution setting is acceptable. We conduct a psychophysical study to determine the acceptability at different image resolutions based on human subject ratings and use them as the ground truth to train our machine learning model. However, this dataset is unbalanced as most images were rated as visually acceptable. To address the data imbalance problem, we introduce several noise models to simulate the degradation of image quality during the scanning process. Our results show that by including augmented data in training, we can significantly improve the performance of the classifier to determine whether the visual quality of raster images in a scanned document is acceptable or not for a given resolution setting.
</details>
<details>
<summary>摘要</summary>
图像质量评估（IQA）是图像处理领域的一个活跃研究领域。大多数前一些研究都是关注自然图像摄像机拍摄的视觉质量。在这篇论文中，我们则是关注扫描文档中的矩阵图像区域的视觉质量。与许多现有的研究不同，我们提议使用机器学习基于分类方法来确定一定分辨率设置下的矩阑图像视觉质量是否可接受。我们进行了心理学研究，以确定不同分辨率下的图像质量可接受程度，并使用人类评分作为准确的标准。然而，该数据集存在偏度问题，因为大多数图像都被评为可接受。为解决这个数据偏度问题，我们引入了多种噪声模型，以模拟扫描过程中图像质量的受损。我们的结果表明，通过在训练中包含增强数据，可以显著提高类别器的性能，以确定扫描文档中矩阑图像的视觉质量是否可接受。
</details></li>
</ul>
<hr>
<h2 id="One-for-Multiple-Physics-informed-Synthetic-Data-Boosts-Generalizable-Deep-Learning-for-Fast-MRI-Reconstruction"><a href="#One-for-Multiple-Physics-informed-Synthetic-Data-Boosts-Generalizable-Deep-Learning-for-Fast-MRI-Reconstruction" class="headerlink" title="One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction"></a>One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13220">http://arxiv.org/abs/2307.13220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangziblake/pisf">https://github.com/wangziblake/pisf</a></li>
<li>paper_authors: Zi Wang, Xiaotong Yu, Chengyan Wang, Weibo Chen, Jiazheng Wang, Ying-Hua Chu, Hongwei Sun, Rushuai Li, Peiyong Li, Fan Yang, Haiwei Han, Taishan Kang, Jianzhong Lin, Chen Yang, Shufu Chang, Zhang Shi, Sha Hua, Yan Li, Juan Hu, Liuhong Zhu, Jianjun Zhou, Meijing Lin, Jiefeng Guo, Congbo Cai, Zhong Chen, Di Guo, Xiaobo Qu</li>
<li>for: 本研究旨在提高快速磁共振成像（MRI）的扫描时间，使其成为医疗诊断中的主要仪器之一。</li>
<li>methods: 本研究使用Physics-Informed Synthetic data learning框架（PISF），通过将快速MRI扫描转化为多个1D基本问题，以便泛化DL模型。同时，通过增强学习技术来提高模型的泛化能力。</li>
<li>results: 研究发现，使用PISF可以将DL模型训练在人工数据上，并在医疗实践中进行扫描，以至于可以 дости到与实际数据匹配的重建效果，同时减少了医疗数据收集的困难和成本。此外，PISF还能够在多个供应商多个中心的医疗设备上进行广泛应用，并且在10名医生的评估中得到了良好的评价。<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) is a principal radiological modality that provides radiation-free, abundant, and diverse information about the whole human body for medical diagnosis, but suffers from prolonged scan time. The scan time can be significantly reduced through k-space undersampling but the introduced artifacts need to be removed in image reconstruction. Although deep learning (DL) has emerged as a powerful tool for image reconstruction in fast MRI, its potential in multiple imaging scenarios remains largely untapped. This is because not only collecting large-scale and diverse realistic training data is generally costly and privacy-restricted, but also existing DL methods are hard to handle the practically inevitable mismatch between training and target data. Here, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF, which is the first to enable generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 2D image, the reconstruction is separated into many 1D basic problems and starts with the 1D data synthesis, to facilitate generalization. We demonstrate that training DL models on synthetic data, integrated with enhanced learning techniques, can achieve comparable or even better in vivo MRI reconstruction compared to models trained on a matched realistic dataset, reducing the demand for real-world MRI data by up to 96%. Moreover, our PISF shows impressive generalizability in multi-vendor multi-center imaging. Its excellent adaptability to patients has been verified through 10 experienced doctors' evaluations. PISF provides a feasible and cost-effective way to markedly boost the widespread usage of DL in various fast MRI applications, while freeing from the intractable ethical and practical considerations of in vivo human data acquisitions.
</details>
<details>
<summary>摘要</summary>
Here, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF, which is the first to enable generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 2D image, the reconstruction is separated into many 1D basic problems, starting with 1D data synthesis to facilitate generalization. We demonstrate that training DL models on synthetic data, integrated with enhanced learning techniques, can achieve comparable or even better in vivo MRI reconstruction compared to models trained on a matched realistic dataset, reducing the demand for real-world MRI data by up to 96%. Moreover, our PISF shows impressive generalizability in multi-vendor multi-center imaging, and its excellent adaptability to patients has been verified through 10 experienced doctors' evaluations. PISF provides a feasible and cost-effective way to markedly boost the widespread usage of DL in various fast MRI applications, while freeing from the intractable ethical and practical considerations of in vivo human data acquisitions.
</details></li>
</ul>
<hr>
<h2 id="Magnetic-Resonance-Parameter-Mapping-using-Self-supervised-Deep-Learning-with-Model-Reinforcement"><a href="#Magnetic-Resonance-Parameter-Mapping-using-Self-supervised-Deep-Learning-with-Model-Reinforcement" class="headerlink" title="Magnetic Resonance Parameter Mapping using Self-supervised Deep Learning with Model Reinforcement"></a>Magnetic Resonance Parameter Mapping using Self-supervised Deep Learning with Model Reinforcement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13211">http://arxiv.org/abs/2307.13211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyu Bian, Albert Jang, Fang Liu<br>for:* This paper proposes a novel self-supervised learning method, RELAX-MORE, for quantitative MRI (qMRI) reconstruction.methods:* The proposed method uses an optimization algorithm to unroll a model-based qMRI reconstruction into a deep learning framework.results:* The proposed method demonstrates excellent performance in reconstructing MR parameters, correcting imaging artifacts, removing noises, and recovering image features at imperfect imaging conditions.Here is the simplified Chinese text:for:* 这篇论文提出了一种新的自动学习方法，即RELAX-MORE，用于快速量子MRI（qMRI）重建。methods:* 该方法使用优化算法将模型基于qMRI重建拓展到深度学习框架中。results:* 该方法在不同的脑、膝和模拟实验中都显示出了高度准确和可靠的MR参数地图重建，并能够正确地纠正成像 artifacts、减少噪声和恢复图像特征。与其他状态的普通和深度学习方法相比，RELAX-MORE显著提高了效率、准确性、可靠性和通用性，有广泛的应用前景。<details>
<summary>Abstract</summary>
This paper proposes a novel self-supervised learning method, RELAX-MORE, for quantitative MRI (qMRI) reconstruction. The proposed method uses an optimization algorithm to unroll a model-based qMRI reconstruction into a deep learning framework, enabling the generation of highly accurate and robust MR parameter maps at imaging acceleration. Unlike conventional deep learning methods requiring a large amount of training data, RELAX-MORE is a subject-specific method that can be trained on single-subject data through self-supervised learning, making it accessible and practically applicable to many qMRI studies. Using the quantitative $T_1$ mapping as an example at different brain, knee and phantom experiments, the proposed method demonstrates excellent performance in reconstructing MR parameters, correcting imaging artifacts, removing noises, and recovering image features at imperfect imaging conditions. Compared with other state-of-the-art conventional and deep learning methods, RELAX-MORE significantly improves efficiency, accuracy, robustness, and generalizability for rapid MR parameter mapping. This work demonstrates the feasibility of a new self-supervised learning method for rapid MR parameter mapping, with great potential to enhance the clinical translation of qMRI.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的自我监督学习方法，即RELAX-MORE，用于量化磁共振成像（qMRI）重建。该方法使用优化算法将模型基于qMRI重建拓展到深度学习框架中，以生成高精度和可靠的MR参数图像，并且能够在不良成像条件下重建图像特征。不同于传统的深度学习方法需要大量的训练数据，RELAX-MORE是一种专门为各个主体学习的方法，可以通过自我监督学习方式在单个主体数据上进行训练，因此可以在许多qMRI研究中实现可行和实用。使用量化$T_1$映射为例，在不同的脑、膝和模拟器实验中，提出的方法能够出色地重建MR参数，纠正成像 artifacts，除去噪声，并保持图像特征。相比其他状态之前的传统和深度学习方法，RELAX-MORE显著提高了效率、准确性、可靠性和普适性，为快速MR参数映射带来了新的可能性，有潜力提高量化磁共振成像在临床应用中的价值。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Approaches-for-Data-Augmentation-in-Medical-Imaging-A-Review"><a href="#Deep-Learning-Approaches-for-Data-Augmentation-in-Medical-Imaging-A-Review" class="headerlink" title="Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review"></a>Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13125">http://arxiv.org/abs/2307.13125</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Aghiles Kebaili, Jérôme Lapuyade-Lahorgue, Su Ruan</li>
<li>for: 这个评论报告的目的是对医疗图像分析领域中深度学习技术的应用进行报告，特别是关于医疗图像数据的有限性问题。</li>
<li>methods: 这篇评论报告主要考虑了三种深度生成模型，包括变分自动编码器、对抗学习网络和扩散模型，用于生成更真实和多样化的医疗图像数据。</li>
<li>results: 这篇评论报告提供了现有的深度生成模型在医疗图像分类、分割和交叉模式翻译等下游任务中的现状，以及这些模型在不同任务中的优势和局限性。<details>
<summary>Abstract</summary>
Deep learning has become a popular tool for medical image analysis, but the limited availability of training data remains a major challenge, particularly in the medical field where data acquisition can be costly and subject to privacy regulations. Data augmentation techniques offer a solution by artificially increasing the number of training samples, but these techniques often produce limited and unconvincing results. To address this issue, a growing number of studies have proposed the use of deep generative models to generate more realistic and diverse data that conform to the true distribution of the data. In this review, we focus on three types of deep generative models for medical image augmentation: variational autoencoders, generative adversarial networks, and diffusion models. We provide an overview of the current state of the art in each of these models and discuss their potential for use in different downstream tasks in medical imaging, including classification, segmentation, and cross-modal translation. We also evaluate the strengths and limitations of each model and suggest directions for future research in this field. Our goal is to provide a comprehensive review about the use of deep generative models for medical image augmentation and to highlight the potential of these models for improving the performance of deep learning algorithms in medical image analysis.
</details>
<details>
<summary>摘要</summary>
深度学习已成为医疗图像分析中常用工具，但培训数据的有限性仍然是主要挑战，特别是医疗领域，数据收集可能昂贵且受隐私法规限制。数据扩展技术可以人工增加培训样本数量，但这些技术通常生成有限和不实的结果。为解决这个问题，一些研究已经提议使用深度生成模型生成更真实和多样的数据，以遵循实际数据的分布。在这篇评论中，我们关注了医疗图像增强中三种深度生成模型：变量自适应器、对抗网络和扩散模型。我们提供这些模型的当前状态艺术和在不同下游任务中的潜在应用，包括分类、分割和对抗Modal译化。我们还评估了每种模型的优点和缺点，并提出了未来研究的方向。我们的目标是提供关于医疗图像增强中深度生成模型的全面评论，并高亮这些模型在医疗图像分析中的潜在优势和未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="In-Situ-Thickness-Measurement-of-Die-Silicon-Using-Voltage-Imaging-for-Hardware-Assurance"><a href="#In-Situ-Thickness-Measurement-of-Die-Silicon-Using-Voltage-Imaging-for-Hardware-Assurance" class="headerlink" title="In-Situ Thickness Measurement of Die Silicon Using Voltage Imaging for Hardware Assurance"></a>In-Situ Thickness Measurement of Die Silicon Using Voltage Imaging for Hardware Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13118">http://arxiv.org/abs/2307.13118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia P. Dizon-Paradis, Nitin Varshney, M Tanjidur Rahman, Michael Strizich, Haoting Shen, Navid Asadizanjani</li>
<li>for: 这篇论文的目的是提高物理检查方法的自动化水平，以满足政府和电子工业的固件保证需求。</li>
<li>methods: 该论文提出了一种基于电子束电压成像、图像处理和 Монте卡罗 simulations的方法，用于在物理检查过程中实时测量剩下的硅厚度，以便实现均匀的排除过程。</li>
<li>results: 该方法可以准确地测量剩下的硅厚度，并且可以在不同的材料厚度和材料特性下实现均匀的排除过程。<details>
<summary>Abstract</summary>
Hardware assurance of electronics is a challenging task and is of great interest to the government and the electronics industry. Physical inspection-based methods such as reverse engineering (RE) and Trojan scanning (TS) play an important role in hardware assurance. Therefore, there is a growing demand for automation in RE and TS. Many state-of-the-art physical inspection methods incorporate an iterative imaging and delayering workflow. In practice, uniform delayering can be challenging if the thickness of the initial layer of material is non-uniform. Moreover, this non-uniformity can reoccur at any stage during delayering and must be corrected. Therefore, it is critical to evaluate the thickness of the layers to be removed in a real-time fashion. Our proposed method uses electron beam voltage imaging, image processing, and Monte Carlo simulation to measure the thickness of remaining silicon to guide a uniform delayering process
</details>
<details>
<summary>摘要</summary>
(Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese". The translation is written in the Simplified Chinese characters, which are used in mainland China and Singapore. Traditional Chinese characters are used in Taiwan, Hong Kong, and Macau.)
</details></li>
</ul>
<hr>
<h2 id="Automatic-Infant-Respiration-Estimation-from-Video-A-Deep-Flow-based-Algorithm-and-a-Novel-Public-Benchmark"><a href="#Automatic-Infant-Respiration-Estimation-from-Video-A-Deep-Flow-based-Algorithm-and-a-Novel-Public-Benchmark" class="headerlink" title="Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark"></a>Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13110">http://arxiv.org/abs/2307.13110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ostadabbas/infant-respiration-estimation">https://github.com/ostadabbas/infant-respiration-estimation</a></li>
<li>paper_authors: Sai Kumar Reddy Manne, Shaotong Zhu, Sarah Ostadabbas, Michael Wan</li>
<li>for: 这个论文是为了提供一种自动、无接触、不需要特殊设备的婴儿呼吸监测方法。</li>
<li>methods: 这个论文使用了深度学习方法， combines video-extracted optical flow input和特有的spatiotemporal convolutional processing，以便从plain video footage中提取婴儿呼吸信息。</li>
<li>results: 该方法在婴儿数据集上进行了训练和测试，与其他状态的方法相比，具有较低的呼吸速率估计误差（$\sim$2.9 breaths per minute），而其他公共模型则在婴儿主题和环境中表现较差。<details>
<summary>Abstract</summary>
Respiration is a critical vital sign for infants, and continuous respiratory monitoring is particularly important for newborns. However, neonates are sensitive and contact-based sensors present challenges in comfort, hygiene, and skin health, especially for preterm babies. As a step toward fully automatic, continuous, and contactless respiratory monitoring, we develop a deep-learning method for estimating respiratory rate and waveform from plain video footage in natural settings. Our automated infant respiration flow-based network (AIRFlowNet) combines video-extracted optical flow input and spatiotemporal convolutional processing tuned to the infant domain. We support our model with the first public annotated infant respiration dataset with 125 videos (AIR-125), drawn from eight infant subjects, set varied pose, lighting, and camera conditions. We include manual respiration annotations and optimize AIRFlowNet training on them using a novel spectral bandpass loss function. When trained and tested on the AIR-125 infant data, our method significantly outperforms other state-of-the-art methods in respiratory rate estimation, achieving a mean absolute error of $\sim$2.9 breaths per minute, compared to $\sim$4.7--6.2 for other public models designed for adult subjects and more uniform environments.
</details>
<details>
<summary>摘要</summary>
呼吸是新生儿的生命必需指标，不断的呼吸监测特别重要。然而，新生儿质感敏感，contact-based感测器会带来舒适性、卫生性和皮肤健康问题，特别是 для premature infants。为了实现完全自动、不断、无接触的呼吸监测，我们开发了一种深度学习方法，可以从普通的视频录像中提取呼吸rate和波形信息。我们的自动 infant respiration flow-based network（AIRFlowNet）结合视频提取的光流输入和适应 infant 领域的空间时间卷积处理。我们对模型进行了人工呼吸注解和优化，使用了一种新的 spectral bandpass loss function。当我们在 AIR-125  infant 数据集上训练和测试 AIRFlowNet 时，我们发现其与其他公共的 state-of-the-art 方法相比，在呼吸速率估计方面显著出perform，其中的平均绝对误差约为 2.9  breaths per minute，与其他针对 adult subjects 和更uniform 环境设计的公共模型相比，误差较低。
</details></li>
</ul>
<hr>
<h2 id="Framework-for-Automatic-PCB-Marking-Detection-and-Recognition-for-Hardware-Assurance"><a href="#Framework-for-Automatic-PCB-Marking-Detection-and-Recognition-for-Hardware-Assurance" class="headerlink" title="Framework for Automatic PCB Marking Detection and Recognition for Hardware Assurance"></a>Framework for Automatic PCB Marking Detection and Recognition for Hardware Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13105">http://arxiv.org/abs/2307.13105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia P. Dizon-Paradis, Daniel E. Capecci, Nathan T. Jessurun, Damon L. Woodard, Mark M. Tehranipoor, Navid Asadizanjani</li>
<li>for:  automatic printed circuit board (PCB) assurance</li>
<li>methods:  utilize domain knowledge of PCB text and logos for data collection and incorporation</li>
<li>results:  proposed dataset plan and framework for high-accuracy automatic PCB marking extraction<details>
<summary>Abstract</summary>
A Bill of Materials (BoM) is a list of all components on a printed circuit board (PCB). Since BoMs are useful for hardware assurance, automatic BoM extraction (AutoBoM) is of great interest to the government and electronics industry. To achieve a high-accuracy AutoBoM process, domain knowledge of PCB text and logos must be utilized. In this study, we discuss the challenges associated with automatic PCB marking extraction and propose 1) a plan for collecting salient PCB marking data, and 2) a framework for incorporating this data for automatic PCB assurance. Given the proposed dataset plan and framework, subsequent future work, implications, and open research possibilities are detailed.
</details>
<details>
<summary>摘要</summary>
一份物料清单（BoM）是印刷电路板（PCB）上所有组件的列表。由于BoM对硬件保证有益，因此自动生成BoM（AutoBoM）对政府和电子行业都是非常有趣的。为实现高精度AutoBoM过程，需要利用PCB文本和标识符的领域知识。本研究讨论了自动PCB标识EXTRACTION的挑战和提出了以下两点计划：1. 收集PCB标识数据的计划2. 利用这些数据实现自动PCB保证的框架基于提出的数据集计划和框架，我们预计将来的工作、影响和开放的研究可能性都将在后续的研究中得到探讨。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-image-captioning-with-depth-information-using-a-Transformer-based-framework"><a href="#Enhancing-image-captioning-with-depth-information-using-a-Transformer-based-framework" class="headerlink" title="Enhancing image captioning with depth information using a Transformer-based framework"></a>Enhancing image captioning with depth information using a Transformer-based framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03767">http://arxiv.org/abs/2308.03767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aya Mahmoud Ahmed, Mohamed Yousef, Khaled F. Hussain, Yousef Bassyouni Mahdy<br>for:This paper aims to enhance the image captioning task by integrating depth information with RGB images to generate better descriptions.methods:The proposed framework uses a Transformer-based encoder-decoder architecture and fuses RGB and depth images using different approaches.results:The proposed framework generates better captions when using depth information, whether it is ground truth or estimated, and outperforms using RGB images only. The cleaned version of the NYU-v2 dataset is also proposed to address inconsistent labeling issues.<details>
<summary>Abstract</summary>
Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image paragraph captioning dataset. During our work with the NYU-v2 dataset, we found inconsistent labeling that prevents the benefit of using depth information to enhance the captioning task. The results were even worse than using RGB images only. As a result, we propose a cleaned version of the NYU-v2 dataset that is more consistent and informative. Our results on both datasets demonstrate that the proposed framework effectively benefits from depth information, whether it is ground truth or estimated, and generates better captions. Code, pre-trained models, and the cleaned version of the NYU-v2 dataset will be made publically available.
</details>
<details>
<summary>摘要</summary>
captioning images是一个复杂的场景理解任务，连接计算机视觉和自然语言处理。虽然图像captioning模型已经成功地生成了出色的描述，但这个领域主要集中在生成2D图像的单句描述。这篇论文研究了是否可以将深度信息与RGB图像一起使用，以提高描述任务并生成更好的描述。为此，我们提出了基于Transformer的encoder-decoder框架，用于生成3D场景的多句描述。RGB图像和其对应的深度图被提供给我们的框架，我们将它们结合使用，以便更好地理解输入场景。深度图可以是真实的或估计的，这使我们的框架对任何RGB描述 dataset 都可以广泛应用。我们研究了不同的融合方法，以融合RGB和深度图。实验在NYU-v2 dataset和Stanford图像段落描述dataset上进行。在我们与NYU-v2 dataset的工作中，我们发现了不一致的标签，这阻碍了使用深度信息提高描述任务的好处。结果甚至比使用RGB图像只的情况even worse。因此，我们提出了一个更加一致和有用的NYU-v2 dataset。我们的结果表明，我们的提案的框架可以充分利用深度信息，无论是真实的或估计的，并生成更好的描述。代码、预训练模型和我们修改过的NYU-v2 dataset将公开发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/eess.IV_2023_07_25/" data-id="cllsj1rpc008bpf88dglfc7p4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.LG_2023_07_24/" class="article-date">
  <time datetime="2023-07-23T16:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.LG_2023_07_24/">cs.LG - 2023-07-24 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network"><a href="#QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network" class="headerlink" title="QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network"></a>QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12906">http://arxiv.org/abs/2307.12906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Md. Saiful Islam, Jungpil Shin, M. F. Mridha, Yuichi Okuyama<br>for: 这篇论文是用于优化供应链管理，提高存储控制、减少费用和提高顾客满意度。methods: 本研究提出了一个新的方法框架，使用量子构思的技术来预测背负货，并且可以处理短时间和不均匀的数据。results: 实验结果显示，QAmplifyNet模型在短时间和不均匀的数据上预测背负货比照 classical models、量子ensemble、量子神经网和深度强化学习模型更加有效。此外，QAmplifyNet模型可以增加存储控制、减少背负货和提高操作效率。<details>
<summary>Abstract</summary>
Supply chain management relies on accurate backorder prediction for optimizing inventory control, reducing costs, and enhancing customer satisfaction. However, traditional machine-learning models struggle with large-scale datasets and complex relationships, hindering real-world data collection. This research introduces a novel methodological framework for supply chain backorder prediction, addressing the challenge of handling large datasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniques within a quantum-classical neural network to predict backorders effectively on short and imbalanced datasets. Experimental evaluations on a benchmark dataset demonstrate QAmplifyNet's superiority over classical models, quantum ensembles, quantum neural networks, and deep reinforcement learning. Its proficiency in handling short, imbalanced datasets makes it an ideal solution for supply chain management. To enhance model interpretability, we use Explainable Artificial Intelligence techniques. Practical implications include improved inventory control, reduced backorders, and enhanced operational efficiency. QAmplifyNet seamlessly integrates into real-world supply chain management systems, enabling proactive decision-making and efficient resource allocation. Future work involves exploring additional quantum-inspired techniques, expanding the dataset, and investigating other supply chain applications. This research unlocks the potential of quantum computing in supply chain optimization and paves the way for further exploration of quantum-inspired machine learning models in supply chain management. Our framework and QAmplifyNet model offer a breakthrough approach to supply chain backorder prediction, providing superior performance and opening new avenues for leveraging quantum-inspired techniques in supply chain management.
</details>
<details>
<summary>摘要</summary>
供应链管理需要准确预测后备订的能力，以优化存储控制、降低成本和提高客户满意度。然而，传统的机器学习模型在大规模数据集和复杂关系下受到限制，阻碍了实际数据收集。这项研究提出了一个新的方法ológical framework for supply chain backorder prediction，解决了处理大规模数据集的挑战。我们提出的模型，QAmplifyNet，采用了量子启发的技术在量子-классиical neural network中预测后备订，在短和不均衡的数据集上显示出了优于经典模型、量子ensemble、量子神经网络和深度强化学习的性能。实验评估表明，QAmplifyNet可以有效地处理短和不均衡的数据集，使其成为供应链管理中的理想解决方案。为提高模型可读性，我们使用了可解释人工智能技术。实际应用包括改善存储控制、减少后备订和提高运营效率。QAmplifyNet可以轻松地与实际供应链管理系统集成，允许您采取激活的决策和有效地分配资源。未来的工作包括进一步探索量子启发技术、扩大数据集和探索其他供应链应用。本研究开阔了量子计算在供应链优化中的潜在可能性，并为Quantum-inspired机器学习模型在供应链管理中铺平了道路。我们的框架和QAmplifyNet模型提供了一个重要的突破在供应链后备订预测方面，为供应链管理提供了超越性能，并开启了新的可能性 для利用量子启发技术在供应链管理中。
</details></li>
</ul>
<hr>
<h2 id="Universal-Approximation-Theorem-and-error-bounds-for-quantum-neural-networks-and-quantum-reservoirs"><a href="#Universal-Approximation-Theorem-and-error-bounds-for-quantum-neural-networks-and-quantum-reservoirs" class="headerlink" title="Universal Approximation Theorem and error bounds for quantum neural networks and quantum reservoirs"></a>Universal Approximation Theorem and error bounds for quantum neural networks and quantum reservoirs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12904">http://arxiv.org/abs/2307.12904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Gonon, Antoine Jacquier</li>
<li>for: 这个论文是为了研究量子神经网络如何实现类似于类传统神经网络的功能 approximating 函数。</li>
<li>methods: 这个论文使用了 parameterised 量子Circuit 来实现这种函数的近似。</li>
<li>results: 这个论文提供了准确的误差上限 bounds  для特定的函数类型，并将这些结果扩展到随机量子Circuit 中，模拟类传统神经网络。结果表明，一个量子神经网络 WITH $\mathcal{O}(\varepsilon^{-2})$ 权重和 $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ 量子 bits 可以达到精度 $\varepsilon&gt;0$ 当近似函数。<details>
<summary>Abstract</summary>
Universal approximation theorems are the foundations of classical neural networks, providing theoretical guarantees that the latter are able to approximate maps of interest. Recent results have shown that this can also be achieved in a quantum setting, whereby classical functions can be approximated by parameterised quantum circuits. We provide here precise error bounds for specific classes of functions and extend these results to the interesting new setup of randomised quantum circuits, mimicking classical reservoir neural networks. Our results show in particular that a quantum neural network with $\mathcal{O}(\varepsilon^{-2})$ weights and $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ qubits suffices to achieve accuracy $\varepsilon>0$ when approximating functions with integrable Fourier transform.
</details>
<details>
<summary>摘要</summary>
“通用近似定理是经典神经网络的基础，提供了理论保证，这些神经网络能够近似 Interesting 的函数。现在，我们可以在量子设置下实现这一点，其中经典函数可以被参数化的量子Circuit 近似。我们在这里提供了具体的误差上限，并扩展到随机量子Circuit 的新设置，模拟类似于经典激发神经网络。我们的结果表明，一个具有 $\mathcal{O}(\varepsilon^{-2})$ 的权重和 $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ 的量子比特数据可以实现 $\varepsilon>0$ 的准确性，当近似函数的傅ри埃transform 是可积分的。”Note: "通用" (pinyin: "gòngyòu") is translated as "universal" in Simplified Chinese, but it is a more literal translation of "common" or "shared".
</details></li>
</ul>
<hr>
<h2 id="Anytime-Model-Selection-in-Linear-Bandits"><a href="#Anytime-Model-Selection-in-Linear-Bandits" class="headerlink" title="Anytime Model Selection in Linear Bandits"></a>Anytime Model Selection in Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12897">http://arxiv.org/abs/2307.12897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parnian Kassraie, Aldo Pacchiano, Nicolas Emmenegger, Andreas Krause</li>
<li>for: 该论文主要针对bandit优化中的模型选择问题，即在搜索和利用之间寻求平衡。</li>
<li>methods: 该论文提出了一种自然的方法，即使用在线学习算法，将不同的模型当做专家进行选择。</li>
<li>results: 论文提出的ALEXP方法可以在linear bandit中实现$\log M$的依赖性，从而提高 regret的性能。ALEXP方法也具有任何时间的保证，不需要知道Horizon $n$ 的值，也不需要初始阶段做出纯粹的探索。<details>
<summary>Abstract</summary>
Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本为 Traditional Chinese.<</SYS>>在带刺探索优化中，选择模型是一项具有挑战性的问题，因为它需要在执行选择和模型选择之间进行平衡探索和利用。一种自然的方法是通过在线学习算法来处理不同的模型。现有方法，然而，对数模型 $M$ 的规模不良 ($\text{poly}M$)。我们的关键发现是，在线选择器中，对于 linear 带刺探索，我们可以通过利用完整的信息反馈来让在线学习者具有有利的偏差-变量规则。这使得我们可以开发 ALEXP，它在 $M$ 上有 exponential 提高的（log $M$）的 regret。ALEXP 具有任何时间的保证，并不需要知道预测时间 $n$，也不需要早期纯探索阶段。我们的方法利用了一种新的时间均分析方法，确定了在线学习和高维统计之间的新连接。
</details></li>
</ul>
<hr>
<h2 id="A-Statistical-View-of-Column-Subset-Selection"><a href="#A-Statistical-View-of-Column-Subset-Selection" class="headerlink" title="A Statistical View of Column Subset Selection"></a>A Statistical View of Column Subset Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12892">http://arxiv.org/abs/2307.12892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anavsood/css">https://github.com/anavsood/css</a></li>
<li>paper_authors: Anav Sood, Trevor Hastie</li>
<li>for: 本研究解决了选择小型表示变量集的问题，即简化数据集中变量的维度减少问题。</li>
<li>methods: 本研究使用了列子集选择（CSS）和主要变量选择（PCA）两种方法，并证明了这两种方法是等价的。</li>
<li>results: 本研究提出了一种基于摘要统计量的高效CSS方法，以及在缺失和&#x2F;或截断数据的情况下CSS方法的应用。此外，本研究还提出了一种在假设检测框架下选择CSS子集的方法。<details>
<summary>Abstract</summary>
We consider the problem of selecting a small subset of representative variables from a large dataset. In the computer science literature, this dimensionality reduction problem is typically formalized as Column Subset Selection (CSS). Meanwhile, the typical statistical formalization is to find an information-maximizing set of Principal Variables. This paper shows that these two approaches are equivalent, and moreover, both can be viewed as maximum likelihood estimation within a certain semi-parametric model. Using these connections, we show how to efficiently (1) perform CSS using only summary statistics from the original dataset; (2) perform CSS in the presence of missing and/or censored data; and (3) select the subset size for CSS in a hypothesis testing framework.
</details>
<details>
<summary>摘要</summary>
我们考虑一个选择小型表示变量的问题，从大量数据集中选择一小部分表示变量。在计算机科学文献中，这个维度减少问题通常被称为列子集选择（CSS）。而在统计学文献中，通常是找到信息最大化的主要变量集。这篇论文表明了这两种方法是等价的，并且它们都可以视为一种最大化概率的最大化问题，在某种半parametric模型下进行解释。使用这些连接，我们展示了如何：1. 使用原始数据集的摘要统计来高效地执行CSS;2. 在缺失和/或截止数据的情况下执行CSS;3. 在假设测试框架下选择CSS中的子集大小。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Stereotype-Identification-through-Reasoning"><a href="#Interpretable-Stereotype-Identification-through-Reasoning" class="headerlink" title="Interpretable Stereotype Identification through Reasoning"></a>Interpretable Stereotype Identification through Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00071">http://arxiv.org/abs/2308.00071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak</li>
<li>for: 避免不必要地启发系统性歧视，语言模型的开发中需要考虑公平性。</li>
<li>methods: 使用 Vicuna-13B-v1.3 进行零shot种族标识 tasks，并分析 scaling 和 reasoning 的效果。</li>
<li>results: 结果显示，理解能力在 zero-shot 种族标识任务中发挥了重要作用，并且 reasoning 的性能提升超过了 scaling 的提升。<details>
<summary>Abstract</summary>
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
</details>
<details>
<summary>摘要</summary>
language models 可能会遗传恶习，因此必须评估和修正这些模型中的偏见，以确保它们是公平的和无偏见的。在这项工作中，我们示出了逻辑的重要性在零shot刻板刻结定位中，并证明了逻辑的性能提升超过了缩放的提升。我们的发现表明，逻辑可能是LLMs在不同领域任务中的关键因素，帮助它们突破域外任务的缩放法则。此外，通过选择的逻辑轨迹分析，我们强调了逻辑不仅提高了准确率，还提高了决策的可读性。
</details></li>
</ul>
<hr>
<h2 id="Data-free-Black-box-Attack-based-on-Diffusion-Model"><a href="#Data-free-Black-box-Attack-based-on-Diffusion-Model" class="headerlink" title="Data-free Black-box Attack based on Diffusion Model"></a>Data-free Black-box Attack based on Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12872">http://arxiv.org/abs/2307.12872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingwen Shao, Lingzhuang Meng, Yuanjian Qiao, Lixu Zhang, Wangmeng Zuo</li>
<li>for: 增强数据隐身攻击的效率和准确性，使用扩散模型生成数据并提出一种基于扩散模型的数据隐身攻击方案。</li>
<li>methods: 使用扩散模型生成数据，并提出一种基于扩散模型的数据隐身攻击方案。该方法使用了秘密代码改进（LCA）方法来引导扩散模型生成数据，以确保数据符合目标模型的批判标准。</li>
<li>results: 通过实验表明，使用扩散模型生成数据和LCA方法可以增强数据隐身攻击的效率和准确性，并且可以在不同的目标模型上实现更高的攻击成功率和更低的查询预算。<details>
<summary>Abstract</summary>
Since the training data for the target model in a data-free black-box attack is not available, most recent schemes utilize GANs to generate data for training substitute model. However, these GANs-based schemes suffer from low training efficiency as the generator needs to be retrained for each target model during the substitute training process, as well as low generation quality. To overcome these limitations, we consider utilizing the diffusion model to generate data, and propose a data-free black-box attack scheme based on diffusion model to improve the efficiency and accuracy of substitute training. Despite the data generated by the diffusion model exhibits high quality, it presents diverse domain distributions and contains many samples that do not meet the discriminative criteria of the target model. To further facilitate the diffusion model to generate data suitable for the target model, we propose a Latent Code Augmentation (LCA) method to guide the diffusion model in generating data. With the guidance of LCA, the data generated by the diffusion model not only meets the discriminative criteria of the target model but also exhibits high diversity. By utilizing this data, it is possible to train substitute model that closely resemble the target model more efficiently. Extensive experiments demonstrate that our LCA achieves higher attack success rates and requires fewer query budgets compared to GANs-based schemes for different target models.
</details>
<details>
<summary>摘要</summary>
因为目标模型的训练数据不可用，现今大多数方案使用GANs生成数据来训练代理模型。然而，这些GANs基本方案受到低效率和低生成质量的限制。为了超越这些限制，我们考虑使用扩散模型生成数据，并提出了基于扩散模型的数据 свобо黑盒攻击方案，以提高代理训练的效率和准确性。尽管扩散模型生成的数据具有高质量，但它们具有多样的领域分布和含有许多不符合目标模型的判别标准的样本。为了使扩散模型更好地生成适合目标模型的数据，我们提议使用秘密代码增强（LCA）方法来导向扩散模型。通过LCA的导向，扩散模型生成的数据不仅满足目标模型的判别标准，而且具有高多样性。通过利用这些数据，可以更高效地训练代理模型，并且需要更少的查询预算。我们的LCA在不同的目标模型上实现了更高的攻击成功率和更低的查询预算。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Step-wise-Feature-Selection-for-Exponential-Random-Graph-Models-ERGMs"><a href="#Stochastic-Step-wise-Feature-Selection-for-Exponential-Random-Graph-Models-ERGMs" class="headerlink" title="Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs)"></a>Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12862">http://arxiv.org/abs/2307.12862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helal El-Zaatari, Fei Yu, Michael R Kosorok</li>
<li>for: 这种研究旨在提高社会网络模型中的准确性，以便更好地理解不同科学领域中的网络现象。</li>
<li>methods: 这种方法使用生成杂化图模型（ERGMs）来捕捉网络依赖关系，并通过内生变量选择来避免模型缺失和准确性问题。</li>
<li>results: 研究人员通过实验和严格分析，发现这种方法可以减轻计算负担，提高网络模型的准确性，并为不同科学领域的网络分析提供实际的应用。<details>
<summary>Abstract</summary>
Statistical analysis of social networks provides valuable insights into complex network interactions across various scientific disciplines. However, accurate modeling of networks remains challenging due to the heavy computational burden and the need to account for observed network dependencies. Exponential Random Graph Models (ERGMs) have emerged as a promising technique used in social network modeling to capture network dependencies by incorporating endogenous variables. Nevertheless, using ERGMs poses multiple challenges, including the occurrence of ERGM degeneracy, which generates unrealistic and meaningless network structures. To address these challenges and enhance the modeling of collaboration networks, we propose and test a novel approach that focuses on endogenous variable selection within ERGMs. Our method aims to overcome the computational burden and improve the accommodation of observed network dependencies, thereby facilitating more accurate and meaningful interpretations of network phenomena in various scientific fields. We conduct empirical testing and rigorous analysis to contribute to the advancement of statistical techniques and offer practical insights for network analysis.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:社交网络统计分析提供了许多有价值的信息，用于各个科学领域的复杂网络互动。然而，准确地模型网络仍然是一项挑战，因为计算负担很大，并且需要考虑观察到的网络依赖关系。泛化随机图模型（ERGMs）在社交网络模型中得到应用，以捕捉网络依赖关系，但使用ERGMs会遇到多种挑战，包括ERGM衰竭现象，这会生成无意义和不可预测的网络结构。为了解决这些挑战并提高社交网络模型的准确性，我们提出了一种新的方法，即内生变量选择在ERGMs中。我们的方法 aim to overcome the computational burden and improve the accommodation of observed network dependencies, thereby facilitating more accurate and meaningful interpretations of network phenomena in various scientific fields.我们进行了实证测试和严格分析，以贡献到统计技术的发展和实际的网络分析。
</details></li>
</ul>
<hr>
<h2 id="A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis"><a href="#A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis" class="headerlink" title="A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"></a>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12856">http://arxiv.org/abs/2307.12856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust</li>
<li>for: 本研究的目的是提高自主浏览器的性能，使其能够根据自然语言指令完成实际网站上的任务。</li>
<li>methods: 本研究使用了大型自然语言模型（LLM）驱动的 WebAgent 代理人，通过分解指令为可能的子指令，摘要长HTML文档为任务相关的摘要，并通过生成的Python程序进行网站上的操作。</li>
<li>results: 实验表明，我们的方法可以提高实际网站上的成功率高于50%，并且HTML-T5模型在解决HTML相关任务方面的性能高于 Priors 的SoTA，在MiniWoB网站浏览 benchmark 上达到14.9%更高的成功率和更高的准确率在离线任务规划评估中。<details>
<summary>Abstract</summary>
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web navigation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that can complete the tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via generated Python programs from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our recipe improves the success on a real website by over 50%, and that HTML-T5 is the best model to solve HTML-based tasks; achieving 14.9% higher success rate than prior SoTA on the MiniWoB web navigation benchmark and better accuracy on offline task planning evaluation.
</details>
<details>
<summary>摘要</summary>
受投入的大型自然语言模型（LLM）最近在自动化网络浏览中实现了更好的泛化和样本效率。然而，实际网站上的表现仍然受到了开放领域、有限的上下文长度和HTML的适应性的影响。我们介绍了WebAgent，一个基于LLM的代理人，可以根据自然语言指令完成实际网站上的任务。WebAgent在规划和摘要方面使用了Flan-U-PaLM和HTML-T5两种新的预训练大型自然语言模型，以提高代理人的准备和执行能力。我们在实验中证明了我们的方法可以在真实网站上提高成功率超过50%，并且HTML-T5模型在解决HTML基本任务方面表现出色，比之前的SoTA在MiniWoB网络浏览benchmark上的成功率提高14.9%。
</details></li>
</ul>
<hr>
<h2 id="Early-Neuron-Alignment-in-Two-layer-ReLU-Networks-with-Small-Initialization"><a href="#Early-Neuron-Alignment-in-Two-layer-ReLU-Networks-with-Small-Initialization" class="headerlink" title="Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization"></a>Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12851">http://arxiv.org/abs/2307.12851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hancheng Min, René Vidal, Enrique Mallada</li>
<li>for: 这paper研究了使用梯度流的两层ReLU网络进行二分类训练，并对小初始值进行研究。</li>
<li>methods: 我们使用了一个训练集，其中每个输入数据点的标签是相关的，而不同标签的输入数据点之间是反相关的。我们通过分析神经元的方向动态来提供一个$\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$的上限，表示在训练的早期阶段，神经元在第一层尝试与输入数据点进行Alignment，这个过程中，神经元的方向会随着时间的推移而发生变化。</li>
<li>results: 我们的分析表明，在训练的早期阶段，所有神经元都可以在$\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$时间内达到良好的Alignment状态，然后loss函数会在$\mathcal{O}(\frac{1}{t})$的速度下降到零，并且第一层的weight矩阵会变得相对低矩。数字实验在MNIST数据集上 validate了我们的理论发现。<details>
<summary>Abstract</summary>
This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficiently-Learning-One-Hidden-Layer-ReLU-Networks-via-Schur-Polynomials"><a href="#Efficiently-Learning-One-Hidden-Layer-ReLU-Networks-via-Schur-Polynomials" class="headerlink" title="Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials"></a>Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12840">http://arxiv.org/abs/2307.12840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Diakonikolas, Daniel M. Kane</li>
<li>for: 学习一个线性组合的ReLU活化器 under Gaussian Distribution</li>
<li>methods: 使用tensor decomposition和Schur polynomials的方法</li>
<li>results: 实现$(dk&#x2F;\epsilon)^{O(k)}$的样本和计算复杂度，比 Priori工作更高效<details>
<summary>Abstract</summary>
We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/\epsilon)^{O(k)}$, where $\epsilon>0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/\epsilon)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.
</details>
<details>
<summary>摘要</summary>
我们研究一个PAC学习问题，即在标准Normal分布上的 $\mathbb{R}^d$ 上学习一个线性结构，其中每个neuron activation是 $k$ 个ReLU活化的线性 комбіна��。我们的主要结果是一个高效的学习算法，其时间和空间复杂度为 $(dk/\epsilon)^{O(k)}$，其中 $\epsilon>0$ 是目标精度。先前的作业中的算法则有 $(dk/\epsilon)^{h(k)}$ 的时间和空间复杂度，其中函数 $h(k)$ 随 $k$ 的数量呈斜率增长。值得注意的是，我们的算法具有近乎最佳的内推优化特性，即在Correlational Statistical Query数据类中的内推优化。在高阶概念上，我们的算法使用了维度分解来找到一个子空间，使得所有 $O(k)$-阶幂都小于余弦方向上。它的分析具有重要的Schur多项式理论基础，以示出高阶幂error tensors 是小于余弦方向上的，只要低阶幂error tensors 是小于余弦方向上。
</details></li>
</ul>
<hr>
<h2 id="Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering"><a href="#Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering" class="headerlink" title="Learning Provably Robust Estimators for Inverse Problems via Jittering"></a>Learning Provably Robust Estimators for Inverse Problems via Jittering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12822">http://arxiv.org/abs/2307.12822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mli-lab/robust_reconstructors_via_jittering">https://github.com/mli-lab/robust_reconstructors_via_jittering</a></li>
<li>paper_authors: Anselm Krainovic, Mahdi Soltanolkotabi, Reinhard Heckel</li>
<li>for: 这篇论文研究了 Whether jittering, a simple regularization technique, can be used to train deep neural networks to be worst-case robust for inverse problems.</li>
<li>methods: The paper uses a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising and trains deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI) with jittering.</li>
<li>results: Jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. Additionally, training on real data which often contains slight noise is somewhat robustness enhancing.<details>
<summary>Abstract</summary>
Deep neural networks provide excellent performance for inverse problems such as denoising. However, neural networks can be sensitive to adversarial or worst-case perturbations. This raises the question of whether such networks can be trained efficiently to be worst-case robust. In this paper, we investigate whether jittering, a simple regularization technique that adds isotropic Gaussian noise during training, is effective for learning worst-case robust estimators for inverse problems. While well studied for prediction in classification tasks, the effectiveness of jittering for inverse problems has not been systematically investigated. In this paper, we present a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising and show that jittering yields optimal robust denoisers. Furthermore, we examine jittering empirically via training deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI). The results show that jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. Moreover, our results imply that training on real data which often contains slight noise is somewhat robustness enhancing.
</details>
<details>
<summary>摘要</summary>
深度神经网络在逆问题中表现出色，但神经网络可能对抗性或最坏情况的扰动敏感。这引起了我们是否可以有效地在训练中使神经网络具有最坏情况Robustness的问题。在这篇论文中，我们调查了是否可以使用缓冲（jittering）这种简单的规范技术来学习逆问题中的最坏情况Robustness。虽然对于预测的分类任务有广泛的研究，但对于逆问题的效果尚未系统地调查。在这篇论文中，我们提出了一种新的分析Characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising,并证明了在训练深度神经网络（U-net）时，缓冲可以获得最佳的Robustness。此外，我们通过实验来评估缓冲的效果，并发现缓冲可以增强逆问题的最坏情况Robustness，但在逆问题超出杂噪的情况下可能不太优化。此外，我们的结果表明，训练在真实数据上，通常含有些许噪声，可以提高一些Robustness。
</details></li>
</ul>
<hr>
<h2 id="Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks"><a href="#Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks" class="headerlink" title="Maximal Independent Sets for Pooling in Graph Neural Networks"></a>Maximal Independent Sets for Pooling in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13011">http://arxiv.org/abs/2307.13011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stevan Stanovic, Benoit Gaüzère, Luc Brun</li>
<li>for:  This paper aims to address the problem of graph pooling, which is the process of reducing the size of a graph while preserving its important features.</li>
<li>methods:  The authors propose three pooling methods based on the notion of maximal independent sets, which avoid the drawbacks of traditional graph pooling methods such as disconnection, overconnection, and deletion of large parts of the graph.</li>
<li>results:  The authors’ experimental results confirm the relevance of maximal independent set constraints for graph pooling, and demonstrate the effectiveness of their proposed methods.<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete lattice into a reduced lattice with the same connectivity and allows reduction functions to consider all pixels in an image. However, there is no pooling that satisfies these properties for graphs. In fact, traditional graph pooling methods suffer from at least one of the following drawbacks: Graph disconnection or overconnection, low decimation ratio, and deletion of large parts of graphs. In this paper, we present three pooling methods based on the notion of maximal independent sets that avoid these pitfalls. Our experimental results confirm the relevance of maximal independent set constraints for graph pooling.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNNs）已经为图像分类带来了重大进步，通过卷积和聚合。特别是图像聚合将连接的离散网络转换为减小的网络，让减少函数可以考虑整个图像中的所有像素。然而， для图像不存在一种聚合方法，这些性质。实际上，传统的图像聚合方法受到至少一种以下缺点：图像分解或过连接，低减少比率，以及大量图像的删除。在这篇论文中，我们提出了三种基于最大独立集的聚合方法，避免这些缺点。我们的实验结果证明了独立集约束对图像聚合的重要性。
</details></li>
</ul>
<hr>
<h2 id="Causal-Fair-Machine-Learning-via-Rank-Preserving-Interventional-Distributions"><a href="#Causal-Fair-Machine-Learning-via-Rank-Preserving-Interventional-Distributions" class="headerlink" title="Causal Fair Machine Learning via Rank-Preserving Interventional Distributions"></a>Causal Fair Machine Learning via Rank-Preserving Interventional Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12797">http://arxiv.org/abs/2307.12797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/paper_2023_cfml">https://github.com/slds-lmu/paper_2023_cfml</a></li>
<li>paper_authors: Ludwig Bothmann, Susanne Dandl, Michael Schomaker</li>
<li>for: 该论文旨在提出一种Machine Learning模型，用于 Mitigating unfairness in automated decision-making systems。</li>
<li>methods: 该论文使用了 causal thinking 和 rank-preserving interventional distributions 来定义一个FiND世界，并提出了一种 warping method  для估算这个FiND世界。</li>
<li>results: 该论文通过 simulations 和实际数据 validate了其方法和模型的评价标准，并显示了其方法可以准确地 indentify 不公正的偏见和 Mitigating unfairness。<details>
<summary>Abstract</summary>
A decision can be defined as fair if equal individuals are treated equally and unequals unequally. Adopting this definition, the task of designing machine learning models that mitigate unfairness in automated decision-making systems must include causal thinking when introducing protected attributes. Following a recent proposal, we define individuals as being normatively equal if they are equal in a fictitious, normatively desired (FiND) world, where the protected attribute has no (direct or indirect) causal effect on the target. We propose rank-preserving interventional distributions to define an estimand of this FiND world and a warping method for estimation. Evaluation criteria for both the method and resulting model are presented and validated through simulations and empirical data. With this, we show that our warping approach effectively identifies the most discriminated individuals and mitigates unfairness.
</details>
<details>
<summary>摘要</summary>
(Note: Simplified Chinese is also known as "Mandarin" or "Guoyu".)Here's the translation in Traditional Chinese:一个决策可以被定义为公平的，如果平等的个体被平等地对待，而不平等的个体则不平等地对待。遵循最近的建议，我们定义个体为在一个虚拟的、normatively desired（FiND）世界中平等，这个世界中保护 attribute 没有直接或间接的 causal effect  на target。我们提议使用排名保持分布来定义这个 FiND 世界的 estimand，并使用扭曲方法来估计。我们提出了评估这方法和结果模型的评估准则，并透过实验和实际数据进行验证。我们显示，我们的扭曲方法可以妥善地识别最受歧视的个体，并对不公做出适当的处理。
</details></li>
</ul>
<hr>
<h2 id="Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification"><a href="#Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification" class="headerlink" title="Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification"></a>Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12790">http://arxiv.org/abs/2307.12790</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonrepo-keeper/gcnn-ec">https://github.com/anonrepo-keeper/gcnn-ec</a></li>
<li>paper_authors: Aryan Singh, Pepijn Van de Ven, Ciarán Eising, Patrick Denny</li>
<li>for: 这个研究探讨了使用图形神经网络（Graph Neural Networks，GNNs）进行医疗图像分类。</li>
<li>methods: 我们提出了一个新的模型，它结合了GNNs和边弹散，利用RGB通道特征值之间的连接来强大地表示关键图像之间的关系。</li>
<li>results: 我们的提案模型不仅与现有的深度神经网络（Deep Neural Networks，DNNs）性能相似，而且仅需1000个参数，比DNNs少了90%的训练时间和数据需求。我们与预训练的DNNs进行比较，发现GNNs在医疗图像分类任务中表现良好，并鼓励进一步探索更进阶的图形基于模型，例如图形注意力网络（Graph Attention Networks，GAT）和图形自动编码器（Graph Auto-Encoders）在医疗影像领域的应用。<details>
<summary>Abstract</summary>
Graph-based neural network models are gaining traction in the field of representation learning due to their ability to uncover latent topological relationships between entities that are otherwise challenging to identify. These models have been employed across a diverse range of domains, encompassing drug discovery, protein interactions, semantic segmentation, and fluid dynamics research. In this study, we investigate the potential of Graph Neural Networks (GNNs) for medical image classification. We introduce a novel model that combines GNNs and edge convolution, leveraging the interconnectedness of RGB channel feature values to strongly represent connections between crucial graph nodes. Our proposed model not only performs on par with state-of-the-art Deep Neural Networks (DNNs) but does so with 1000 times fewer parameters, resulting in reduced training time and data requirements. We compare our Graph Convolutional Neural Network (GCNN) to pre-trained DNNs for classifying MedMNIST dataset classes, revealing promising prospects for GNNs in medical image analysis. Our results also encourage further exploration of advanced graph-based models such as Graph Attention Networks (GAT) and Graph Auto-Encoders in the medical imaging domain. The proposed model yields more reliable, interpretable, and accurate outcomes for tasks like semantic segmentation and image classification compared to simpler GCNNs
</details>
<details>
<summary>摘要</summary>
“几何基于神经网络模型在知识学习中受到推广，因为它们可以捕捉隐藏在Entities之间的数据关系，很难通过其他方法检测。这些模型在多个领域中使用，包括药物发现、蛋白质互作、semantic segmentation和流体动力学研究。在这个研究中，我们 investigate the potential of Graph Neural Networks (GNNs) for medical image classification。我们提出了一个新的模型，它结合GNNs和边 convolution，通过RGB通道特征值之间的连接来强大地表示关键几何节点之间的连接。我们的提案模型不仅与状态预设神经网络（DNNs）相似，并且仅需1000倍少的参数，从而降低训练时间和数据需求。我们与预训DNNs进行比较，发现GCNN在MedMNIST数据集中的类别分类任务中表现出色，这点鼓励我们进一步探索高级几何基于模型，如Graph Attention Networks (GAT)和Graph Auto-Encoders在医学影像领域。我们的模型具有更可靠、可解释和精准的结果，比如semantic segmentation和图像分类任务，相比于简单的GCNN”
</details></li>
</ul>
<hr>
<h2 id="Deep-neural-network-improves-the-estimation-of-polygenic-risk-scores-for-breast-cancer"><a href="#Deep-neural-network-improves-the-estimation-of-polygenic-risk-scores-for-breast-cancer" class="headerlink" title="Deep neural network improves the estimation of polygenic risk scores for breast cancer"></a>Deep neural network improves the estimation of polygenic risk scores for breast cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13010">http://arxiv.org/abs/2307.13010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Badré, Li Zhang, Wellington Muchero, Justin C. Reynolds, Chongle Pan</li>
<li>for: 这个研究旨在比较计算复杂疾病的遗传风险分数（PRS）的不同计算模型。</li>
<li>methods: 这个研究使用了深度神经网络（DNN）和其他机器学习技术以及统计学算法来计算PRS。</li>
<li>results: DNN模型在测试组中的AUC值为67.4%，高于其他模型的AUC值（64.2%、64.5%和62.4%）。此外，DNN模型还能够将 случа组分为高遗传风险子组和常见遗传风险子组，并且这两个子组的PRS分布不同。这使得DNN模型在18.8%的抓取率下达到90%的准确率。<details>
<summary>Abstract</summary>
Polygenic risk scores (PRS) estimate the genetic risk of an individual for a complex disease based on many genetic variants across the whole genome. In this study, we compared a series of computational models for estimation of breast cancer PRS. A deep neural network (DNN) was found to outperform alternative machine learning techniques and established statistical algorithms, including BLUP, BayesA and LDpred. In the test cohort with 50% prevalence, the Area Under the receiver operating characteristic Curve (AUC) were 67.4% for DNN, 64.2% for BLUP, 64.5% for BayesA, and 62.4% for LDpred. BLUP, BayesA, and LPpred all generated PRS that followed a normal distribution in the case population. However, the PRS generated by DNN in the case population followed a bi-modal distribution composed of two normal distributions with distinctly different means. This suggests that DNN was able to separate the case population into a high-genetic-risk case sub-population with an average PRS significantly higher than the control population and a normal-genetic-risk case sub-population with an average PRS similar to the control population. This allowed DNN to achieve 18.8% recall at 90% precision in the test cohort with 50% prevalence, which can be extrapolated to 65.4% recall at 20% precision in a general population with 12% prevalence. Interpretation of the DNN model identified salient variants that were assigned insignificant p-values by association studies, but were important for DNN prediction. These variants may be associated with the phenotype through non-linear relationships.
</details>
<details>
<summary>摘要</summary>
多因素风险分数（PRS）用于估计个体复杂疾病的遗传风险，基于整个 genomic 中的多个遗传变异。本研究比较了多种计算机模型用于PRS的估计，其中深度神经网络（DNN）被发现表现出色，超过了传统机器学习技术和已知统计方法，包括BLUP、BayesA和LDpred。在测试样本中，DNN 的 AUC 分数为 67.4%，而 BLUP、BayesA 和 LDpred 的 AUC 分数分别为 64.2%、64.5% 和 62.4%。BLUP、BayesA 和 LPpred 都生成的 PRS 遵循了正态分布，而 DNN 在 случа Population 中生成的 PRS 遵循了二元分布，由两个正态分布组成，其中一个分布的mean 明显高于控制人口的 mean。这表明 DNN 可以将 случа Population 分为高遗传风险子 популяции和正常遗传风险子 популяции两个组成部分，其中高遗传风险子 популяции的 PRS 平均值明显高于控制人口，而正常遗传风险子 популяции的 PRS 与控制人口的平均值相似。这使得 DNN 在 50% 患病率的测试样本中 achieve 18.8% 的回归率，在 20% 患病率的总人口中可以预测 65.4% 的回归率。DN 模型的解释发现，DNN 使用了一些在 association 研究中被评估为无关的变异，但这些变异对 DNN 预测有重要作用。这些变异可能与现象之间存在非线性关系。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine"><a href="#Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine" class="headerlink" title="Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine"></a>Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12788">http://arxiv.org/abs/2307.12788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominique Geissler, Stefan Feuerriegel</li>
<li>For: 本研究使用 inverse reinforcement learning (IRL) 方法分析了2022年俄罗斯入侵乌克兰的社交媒体宣传战略。* Methods: 研究者使用了 IRL 方法来模型在线行为，并从推特上收集了349,455条帖子和132,131名用户的数据。* Results: 研究发现，机器人和人类用户采取不同策略：机器人主要回应了支持入侵的消息，而人类用户则主要回应了反对消息，这表明机器人寻求带virality，而人类用户更倾向于进行批评讨论。<details>
<summary>Abstract</summary>
The 2022 Russian invasion of Ukraine was accompanied by a large-scale, pro-Russian propaganda campaign on social media. However, the strategy behind the dissemination of propaganda has remained unclear, particularly how the online discourse was strategically shaped by the propagandists' community. Here, we analyze the strategy of the Twitter community using an inverse reinforcement learning (IRL) approach. Specifically, IRL allows us to model online behavior as a Markov decision process, where the goal is to infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion. Thereby, we aim to understand empirically whether and how between-user interactions are strategically used to promote the proliferation of Russian propaganda. For this, we leverage a large-scale dataset with 349,455 posts with pro-Russian propaganda from 132,131 users. We show that bots and humans follow a different strategy: bots respond predominantly to pro-invasion messages, suggesting that they seek to drive virality; while messages indicating opposition primarily elicit responses from humans, suggesting that they tend to engage in critical discussions. To the best of our knowledge, this is the first study analyzing the strategy behind propaganda from the 2022 Russian invasion of Ukraine through the lens of IRL.
</details>
<details>
<summary>摘要</summary>
俄罗斯入侵乌克兰的2022年社交媒体宣传活动旁 accompaniment by a large-scale, pro-Russian propaganda campaign on social media. However, the strategy behind the dissemination of propaganda has remained unclear, particularly how the online discourse was strategically shaped by the propagandists' community. Here, we analyze the strategy of the Twitter community using an inverse reinforcement learning (IRL) approach. Specifically, IRL allows us to model online behavior as a Markov decision process, where the goal is to infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion. Thereby, we aim to understand empirically whether and how between-user interactions are strategically used to promote the proliferation of Russian propaganda. For this, we leverage a large-scale dataset with 349,455 posts with pro-Russian propaganda from 132,131 users. We show that bots and humans follow a different strategy: bots respond predominantly to pro-invasion messages, suggesting that they seek to drive virality; while messages indicating opposition primarily elicit responses from humans, suggesting that they tend to engage in critical discussions. To the best of our knowledge, this is the first study analyzing the strategy behind propaganda from the 2022 Russian invasion of Ukraine through the lens of IRL.
</details></li>
</ul>
<hr>
<h2 id="Is-attention-all-you-need-in-medical-image-analysis-A-review"><a href="#Is-attention-all-you-need-in-medical-image-analysis-A-review" class="headerlink" title="Is attention all you need in medical image analysis? A review"></a>Is attention all you need in medical image analysis? A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12775">http://arxiv.org/abs/2307.12775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgos Papanastasiou, Nikolaos Dikaios, Jiahao Huang, Chengjia Wang, Guang Yang</li>
<li>for: This paper reviews and analyzes existing hybrid CNN-Transf&#x2F;Attention models for medical image analysis, with a focus on their architectural designs, breakthroughs, and opportunities for future research.</li>
<li>methods: The paper uses a systematic review approach to survey existing hybrid models, and introduces a comprehensive analysis framework for evaluating their generalization ability.</li>
<li>results: The paper discusses the breakthroughs and opportunities in hybrid CNN-Transf&#x2F;Attention models for medical image analysis, and highlights the need for further research in this area to improve their generalization ability and scientific and clinical impact.<details>
<summary>Abstract</summary>
Medical imaging is a key component in clinical diagnosis, treatment planning and clinical trial design, accounting for almost 90% of all healthcare data. CNNs achieved performance gains in medical image analysis (MIA) over the last years. CNNs can efficiently model local pixel interactions and be trained on small-scale MI data. The main disadvantage of typical CNN models is that they ignore global pixel relationships within images, which limits their generalisation ability to understand out-of-distribution data with different 'global' information. The recent progress of Artificial Intelligence gave rise to Transformers, which can learn global relationships from data. However, full Transformer models need to be trained on large-scale data and involve tremendous computational complexity. Attention and Transformer compartments (Transf/Attention) which can well maintain properties for modelling global relationships, have been proposed as lighter alternatives of full Transformers. Recently, there is an increasing trend to co-pollinate complementary local-global properties from CNN and Transf/Attention architectures, which led to a new era of hybrid models. The past years have witnessed substantial growth in hybrid CNN-Transf/Attention models across diverse MIA problems. In this systematic review, we survey existing hybrid CNN-Transf/Attention models, review and unravel key architectural designs, analyse breakthroughs, and evaluate current and future opportunities as well as challenges. We also introduced a comprehensive analysis framework on generalisation opportunities of scientific and clinical impact, based on which new data-driven domain generalisation and adaptation methods can be stimulated.
</details>
<details>
<summary>摘要</summary>
医疗成像是诊断、治疗规划和临床试验设计中关键的一部分，贡献约90%的医疗数据。过去几年，深度学习模型（CNN）在医学成像分析（MIA）中表现出了性能提升。CNN可以有效地模型本地像素间的互动，并可以在小规模的MI数据上训练。然而，典型的CNN模型忽略了图像中的全局像素关系，这限制了它们的泛化能力，无法理解不同的'全局'信息。在人工智能的进步下，变换器（Transformer） emerged，它可以从数据中学习全局关系。然而，完整的Transformer模型需要大规模的数据训练和巨大的计算复杂性。为了维护全局关系的模elling，各种Attention和Transformer分支（Transf/Attention）被提出，它们可以具有较低的计算复杂性。最近，有一种增长的趋势，即将 complementary local-global properties from CNN和Transf/Attention架构相结合，这导致了一个新的时代的半卷积模型。过去几年， hybrid CNN-Transf/Attention模型在多个 MIA 问题上呈现出了明显的增长。在这种系统性评估中，我们survey了现有的半卷积 CNN-Transf/Attention模型，检查和描述了关键的建筑设计，分析了突破点，以及评估了当前和未来的机会和挑战。我们还提出了一种总体分析框架，以便根据这种框架，开发出新的数据驱动的领域泛化和适应方法。
</details></li>
</ul>
<hr>
<h2 id="Detecting-disturbances-in-network-coupled-dynamical-systems-with-machine-learning"><a href="#Detecting-disturbances-in-network-coupled-dynamical-systems-with-machine-learning" class="headerlink" title="Detecting disturbances in network-coupled dynamical systems with machine learning"></a>Detecting disturbances in network-coupled dynamical systems with machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12771">http://arxiv.org/abs/2307.12771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Per Sebastian Skardal, Juan G. Restrepo</li>
<li>for: Identifying disturbances in network-coupled dynamical systems without knowledge of the disturbances or underlying dynamics.</li>
<li>methods: Model-free method based on machine learning, using prior observations of the system when forced by a known training function.</li>
<li>results: Able to identify the locations and properties of many different types of unknown disturbances using a variety of known forcing functions, both with linear and nonlinear disturbances.Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本研究旨在无需知道外围干扰或系统动力学的情况下，对网络相互作用的动力系统中的干扰进行识别。</li>
<li>methods: 基于机器学习的模型自由方法，使用知道的训练函数来驱动系统的优先观察。</li>
<li>results: 通过使用多种知道的干扰函数，成功地识别了许多不同类型的未知干扰，包括线性和非线性干扰，并在食物网和神经活动模型中进行了应用。I hope this helps!<details>
<summary>Abstract</summary>
Identifying disturbances in network-coupled dynamical systems without knowledge of the disturbances or underlying dynamics is a problem with a wide range of applications. For example, one might want to know which nodes in the network are being disturbed and identify the type of disturbance. Here we present a model-free method based on machine learning to identify such unknown disturbances based only on prior observations of the system when forced by a known training function. We find that this method is able to identify the locations and properties of many different types of unknown disturbances using a variety of known forcing functions. We illustrate our results both with linear and nonlinear disturbances using food web and neuronal activity models. Finally, we discuss how to scale our method to large networks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于无知的网络相互作用系统中的干扰，无需知道干扰或系统内部动力学的情况，是一个广泛的应用问题。例如，我们可能想知道哪些节点在网络中受到干扰，并确定干扰的类型。在这里，我们提出了一种无模型方法，基于机器学习，可以基于先前观察到的系统响应，无知地预测未知干扰的位置和性质。我们发现这种方法可以预测多种不同类型的未知干扰，使用多种已知的刺激函数。我们使用食物网和神经活动模型来说明我们的结果，最后讨论如何扩展我们的方法到大型网络。Note: Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation"><a href="#Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation" class="headerlink" title="Nonparametric Linear Feature Learning in Regression Through Regularisation"></a>Nonparametric Linear Feature Learning in Regression Through Regularisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12754">http://arxiv.org/abs/2307.12754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bertillefollain/regfeal">https://github.com/bertillefollain/regfeal</a></li>
<li>paper_authors: Bertille Follain, Umut Simsekli, Francis Bach</li>
<li>for: 本研究旨在提高自动Feature选择的效果，特别是在高维数据上，非Parametric方法 often struggle.</li>
<li>methods: 我们提出了一种新的方法 для线性特征学习，同时估计预测函数和线性子空间。我们的方法使用Empirical risk minimization，加以函数 Derivatives penalty，以 Ensure versatility.</li>
<li>results: 我们的方法可以得到一个Consistent estimator of the prediction function with explicit rates，并且在各种实验中表现出色。<details>
<summary>Abstract</summary>
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with leading directions and accurately estimate the relevant dimension in practical settings. We establish that our method yields a consistent estimator of the prediction function with explicit rates. Additionally, we provide empirical results demonstrating the performance of RegFeaL in various experiments.
</details>
<details>
<summary>摘要</summary>
“特征学习在自动选择特征方面扮演着关键角色，特别是在高维数据的场景下，非 Parametric 方法通常难以处理。在这种研究中，我们关注到supervised 学习场景下，关键信息归于数据中的一个低维线性子空间，即多index 模型。如果这个子空间知道，它会大幅提高预测、计算和解释。为解决这个挑战，我们提出了一种新的方法 для线性特征学习和非 Parametric 预测，该方法同时估算预测函数和线性子空间。我们的方法使用empirical risk minimization，并添加了预测函数导数的罚 penalty，以确保方法的 universality。通过 Hermite 波干的特性，我们引入了我们的估计器，名为RegFeaL。我们使用 alternate minimization 来Iteratively 旋转数据，以便更好地与主要方向相互Alignment，并准确地估算实际场景中的相关维度。我们证明了我们的方法可以提供一个Consistent 的预测函数估算器，并提供了explicit 的速率。此外，我们还提供了多个实验结果，证明 RegFeaL 在各种场景中的表现。”
</details></li>
</ul>
<hr>
<h2 id="Concept-based-explainability-for-an-EEG-transformer-model"><a href="#Concept-based-explainability-for-an-EEG-transformer-model" class="headerlink" title="Concept-based explainability for an EEG transformer model"></a>Concept-based explainability for an EEG transformer model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12745">http://arxiv.org/abs/2307.12745</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andersgmadsen/tcav-bendr">https://github.com/andersgmadsen/tcav-bendr</a></li>
<li>paper_authors: Anders Gjølbye Madsen, William Theodor Lehn-Schiøler, Áshildur Jónsdóttir, Bergdís Arnardóttir, Lars Kai Hansen</li>
<li>for: 本研究旨在应用于电энце法ogram（EEG）数据，以提高深度学习模型的可解释性。</li>
<li>methods: 本研究使用了Kim et al. (2018)所提出的概念活化向量（CAVs）方法，以理解深度模型内部的状态。这种方法首先应用于图像分类领域，后来扩展到其他领域，包括自然语言处理。</li>
<li>results: 本研究表明，使用外部标注的EEG数据和应用颌定义的概念可以帮助理解深度EEG模型的表示。两种方法均提供了有价值的信息，帮助理解深度模型学习的表示。<details>
<summary>Abstract</summary>
Deep learning models are complex due to their size, structure, and inherent randomness in training procedures. Additional complexity arises from the selection of datasets and inductive biases. Addressing these challenges for explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs), which aim to understand deep models' internal states in terms of human-aligned concepts. These concepts correspond to directions in latent space, identified using linear discriminants. Although this method was first applied to image classification, it was later adapted to other domains, including natural language processing. In this work, we attempt to apply the method to electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR (2021), a large-scale transformer model. A crucial part of this endeavor involves defining the explanatory concepts and selecting relevant datasets to ground concepts in the latent space. Our focus is on two mechanisms for EEG concept formation: the use of externally labeled EEG datasets, and the application of anatomically defined concepts. The former approach is a straightforward generalization of methods used in image classification, while the latter is novel and specific to EEG. We present evidence that both approaches to concept formation yield valuable insights into the representations learned by deep EEG models.
</details>
<details>
<summary>摘要</summary>
深度学习模型因其大小、结构和训练过程中的随机性而变得复杂。此外，选择数据集和 inductive bias 也会增加复杂性。为了解释这些复杂性，金等人（2018）提出了概念活动 вектор (CAV)，它们目的是通过人类对应的概念来理解深度模型的内部状态。这些概念与 latent space 中的方向相对应，使用线性分解器来定义。这种方法最初应用于图像分类领域，后来被应用到其他领域，包括自然语言处理。在这个工作中，我们尝试将这种方法应用到 Kostas 等人（2021）的 BENDR 模型，以便在 electroencephalogram (EEG) 数据上提供解释。我们的关注点在于两种 EEG 概念形成机制：使用外部标注的 EEG 数据，以及基于 анатомически定义的概念。前者是一种直观的推广，而后者是特有的 EEG 概念形成方法。我们展示了这两种方法可以为深度 EEG 模型学习的表示提供有价值的启示。
</details></li>
</ul>
<hr>
<h2 id="Sparse-firing-regularization-methods-for-spiking-neural-networks-with-time-to-first-spike-coding"><a href="#Sparse-firing-regularization-methods-for-spiking-neural-networks-with-time-to-first-spike-coding" class="headerlink" title="Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding"></a>Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13007">http://arxiv.org/abs/2307.13007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuke Sakemi, Kakei Yamamoto, Takeo Hosomi, Kazuyuki Aihara</li>
<li>for: This paper aims to improve the energy efficiency of information processing in spiking neural networks (SNNs) by reducing the firing frequency of time-to-first spike (TTFS)-coded SNNs.</li>
<li>methods: The paper proposes two spike timing-based sparse-firing (SSR) regularization methods, namely membrane potential-aware SSR (M-SSR) and firing condition-aware SSR (F-SSR), to further reduce the firing frequency of TTFS-coded SNNs.</li>
<li>results: The paper investigates the effects of these regularization methods on the MNIST, Fashion-MNIST, and CIFAR-10 datasets using multilayer perceptron networks and convolutional neural network structures.<details>
<summary>Abstract</summary>
The training of multilayer spiking neural networks (SNNs) using the error backpropagation algorithm has made significant progress in recent years. Among the various training schemes, the error backpropagation method that directly uses the firing time of neurons has attracted considerable attention because it can realize ideal temporal coding. This method uses time-to-first spike (TTFS) coding, in which each neuron fires at most once, and this restriction on the number of firings enables information to be processed at a very low firing frequency. This low firing frequency increases the energy efficiency of information processing in SNNs, which is important not only because of its similarity with information processing in the brain, but also from an engineering point of view. However, only an upper limit has been provided for TTFS-coded SNNs, and the information-processing capability of SNNs at lower firing frequencies has not been fully investigated. In this paper, we propose two spike timing-based sparse-firing (SSR) regularization methods to further reduce the firing frequency of TTFS-coded SNNs. The first is the membrane potential-aware SSR (M-SSR) method, which has been derived as an extreme form of the loss function of the membrane potential value. The second is the firing condition-aware SSR (F-SSR) method, which is a regularization function obtained from the firing conditions. Both methods are characterized by the fact that they only require information about the firing timing and associated weights. The effects of these regularization methods were investigated on the MNIST, Fashion-MNIST, and CIFAR-10 datasets using multilayer perceptron networks and convolutional neural network structures.
</details>
<details>
<summary>摘要</summary>
“多层脉冲神经网络（SNN）的训练使用错误反射算法在最近几年内做出了重要进展。其中许多训练方案中，使用神经元发射时间的错误反射方法吸引了较大的注意力，因为它可以实现理想的时间编码。这种方法使用时刻到首次发射（TTFS）编码，每个神经元最多只能发射一次，这种限制神经元发射数量使得信息处理可以在非常低的发射频率下进行。这种低发射频率提高了SNNs中信息处理的能源效率，这对不仅与脑中信息处理的相似性，还从工程角度来说是非常重要。然而，只提供了TTFS-coded SNNs的最大限制，尚未充分研究SNNs在更低的发射频率下的信息处理能力。在这篇论文中，我们提出了两种基于发射时间的稀发（SSR）规范，以进一步减少TTFS-coded SNNs的发射频率。第一个是Membrame potential-aware SSR（M-SSR）方法，它是脑电压值损失函数的极端形式。第二个是发射条件-aware SSR（F-SSR）方法，它是基于发射条件获得的规范函数。两种方法都具有只需要知道发射时间和相关权重的特点。我们对MNIST、Fashion-MNIST和CIFAR-10数据集使用多层感知网络和卷积神经网络结构进行了实验研究。”
</details></li>
</ul>
<hr>
<h2 id="Safety-Performance-of-Neural-Networks-in-the-Presence-of-Covariate-Shift"><a href="#Safety-Performance-of-Neural-Networks-in-the-Presence-of-Covariate-Shift" class="headerlink" title="Safety Performance of Neural Networks in the Presence of Covariate Shift"></a>Safety Performance of Neural Networks in the Presence of Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12716">http://arxiv.org/abs/2307.12716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chih-Hong Cheng, Harald Ruess, Konstantinos Theodorou</li>
<li>for: 这个论文是为了解决 neural network 在不同条件下的安全性性能问题。</li>
<li>methods: 论文提出了一种基于activation pattern的分布approximation的方法，以重新评估 neural network 的安全性性能在 covariate shift 下。</li>
<li>results: 论文提出的方法可以construct a reshaped test set that reflects the distribution of neuron activation values observed during operation, and may therefore be used for re-evaluating safety performance in the presence of covariate shift.<details>
<summary>Abstract</summary>
Covariate shift may impact the operational safety performance of neural networks. A re-evaluation of the safety performance, however, requires collecting new operational data and creating corresponding ground truth labels, which often is not possible during operation. We are therefore proposing to reshape the initial test set, as used for the safety performance evaluation prior to deployment, based on an approximation of the operational data. This approximation is obtained by observing and learning the distribution of activation patterns of neurons in the network during operation. The reshaped test set reflects the distribution of neuron activation values as observed during operation, and may therefore be used for re-evaluating safety performance in the presence of covariate shift. First, we derive conservative bounds on the values of neurons by applying finite binning and static dataflow analysis. Second, we formulate a mixed integer linear programming (MILP) constraint for constructing the minimum set of data points to be removed in the test set, such that the difference between the discretized test and operational distributions is bounded. We discuss potential benefits and limitations of this constraint-based approach based on our initial experience with an implemented research prototype.
</details>
<details>
<summary>摘要</summary>
covariate shift可能会影响神经网络的运行安全性表现。但是重新评估安全性表现时需要收集新的运行数据和创建相应的真实标签，却经常在运行时不可能完成。我们因此提议将初始测试集重新组织，基于神经网络在运行时活动pattern的 Distribution 的 aproximation。这种approximation 通过观察和学习神经网络在运行时 neuron 的活动值分布来获得。重新组织的测试集反映了神经网络在运行时 neuron 的活动值分布，可以用于重新评估安全性表现在 covariate shift 的情况下。首先，我们通过finite binning和静态数据流分析来 derive conservative bounds  на neuron 的值。其次，我们将 mixed integer linear programming (MILP) 约束 formation for constructing the minimum set of data points to be removed in the test set，以使得测试和运行分布之间的差异bounded。我们介绍了这种约束基于的approach 的可能的优点和限制，基于我们对实际研究版本的初步经验。
</details></li>
</ul>
<hr>
<h2 id="Policy-Gradient-Optimal-Correlation-Search-for-Variance-Reduction-in-Monte-Carlo-simulation-and-Maximum-Optimal-Transport"><a href="#Policy-Gradient-Optimal-Correlation-Search-for-Variance-Reduction-in-Monte-Carlo-simulation-and-Maximum-Optimal-Transport" class="headerlink" title="Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport"></a>Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12703">http://arxiv.org/abs/2307.12703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Bras, Gilles Pagès</li>
<li>for: 估计 $f(X_T)$ 的变iance减少</li>
<li>methods: 使用 $(f(X^1_T) + f(X^2_T))&#x2F;2$ 新算法，其中 $X^1$ 和 $X^2$ 具有相同的分布，但是具有相对的路径相关性，以降低变iance</li>
<li>results: 使用深度神经网络来approximate优化的相关函数 $\rho$，并使用策略梯度和奖励学习技术来calibrate $\rho$  along the trajectories of $(X^1, X^2)$<details>
<summary>Abstract</summary>
We propose a new algorithm for variance reduction when estimating $f(X_T)$ where $X$ is the solution to some stochastic differential equation and $f$ is a test function. The new estimator is $(f(X^1_T) + f(X^2_T))/2$, where $X^1$ and $X^2$ have same marginal law as $X$ but are pathwise correlated so that to reduce the variance. The optimal correlation function $\rho$ is approximated by a deep neural network and is calibrated along the trajectories of $(X^1, X^2)$ by policy gradient and reinforcement learning techniques. Finding an optimal coupling given marginal laws has links with maximum optimal transport.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的算法来减少 $f(X_T)$ 的方差估计，其中 $X$ 是一个随机 differential equation 的解，$f$ 是一个测试函数。这个新估计器是 $(f(X^1_T) + f(X^2_T))/2$, 其中 $X^1$ 和 $X^2$ 具有同一个分布，但是它们在路径上具有相关性，以降低方差。我们使用深度神经网络来近似优化函数 $\rho$，并通过策略梯度和强化学习技术来调整 $\rho$。在给定分布下找到最佳匹配有关于最大优化交通的链接。
</details></li>
</ul>
<hr>
<h2 id="MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features"><a href="#MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features" class="headerlink" title="MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features"></a>MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12698">http://arxiv.org/abs/2307.12698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Bardes, Jean Ponce, Yann LeCun</li>
<li>for: 这篇论文主要针对的是如何通过自动学习来学习视觉表示，以便更好地捕捉视频中的物体运动和位置信息。</li>
<li>methods: 这篇论文提出了一种将自动学习和运动估算相结合的方法，即MC-JEPA，它利用共享编码器来同时学习运动估算和内容特征，从而将运动信息纳入视觉表示中。</li>
<li>results: 实验结果表明，MC-JEPA可以与现有的无监督光流估算 benchmarks相比，以及与常见的自动学习方法在图像和视频 semantic segmentation 任务上表现相当。<details>
<summary>Abstract</summary>
Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.
</details>
<details>
<summary>摘要</summary>
自顾学学习视觉表示法已经集中着注意力于学习内容特征，这些特征不包括物体运动或位置信息，而是专注于图像和视频中对象的识别和区分。然而，光流估计是一个不需要理解图像内容的任务。我们将这两种方法统一，并引入 MC-JEPA，一种共同预测 Architecture和自顾学学习方法，以共同学习光流和内容特征在共同Encoder中。我们示出了这两个相关的目标；光流估计目标和自顾学学习目标之间存在互助关系，因此学习的内容特征具有运动信息。我们的方法可以与现有的无监督光流标准做比较，以及常见的自顾学学习方法在图像和视频semantic segmentation任务中的表现。
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks"><a href="#Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks" class="headerlink" title="Addressing the Impact of Localized Training Data in Graph Neural Networks"></a>Addressing the Impact of Localized Training Data in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12689">http://arxiv.org/abs/2307.12689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akanshaaga/reg_appnp">https://github.com/akanshaaga/reg_appnp</a></li>
<li>paper_authors: Singh Akansha<br>for: 本研究的目的是评估 graphs 上 GNN 模型的性能，并研究如何在受限制的训练数据下提高模型的适应能力。methods: 本研究使用了一种常见的 GNN 模型，并提出了一种基于偏移分布的规范方法来降低模型与数据的偏移。results: 对于三个常用的 цитирование GNN 测试集，研究发现该规范方法可以有效地提高模型在不同数据下的适应能力和泛化能力。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved notable success in learning from graph-structured data, owing to their ability to capture intricate dependencies and relationships between nodes. They excel in various applications, including semi-supervised node classification, link prediction, and graph generation. However, it is important to acknowledge that the majority of state-of-the-art GNN models are built upon the assumption of an in-distribution setting, which hinders their performance on real-world graphs with dynamic structures. In this article, we aim to assess the impact of training GNNs on localized subsets of the graph. Such restricted training data may lead to a model that performs well in the specific region it was trained on but fails to generalize and make accurate predictions for the entire graph. In the context of graph-based semi-supervised learning (SSL), resource constraints often lead to scenarios where the dataset is large, but only a portion of it can be labeled, affecting the model's performance. This limitation affects tasks like anomaly detection or spam detection when labeling processes are biased or influenced by human subjectivity. To tackle the challenges posed by localized training data, we approach the problem as an out-of-distribution (OOD) data issue by by aligning the distributions between the training data, which represents a small portion of labeled data, and the graph inference process that involves making predictions for the entire graph. We propose a regularization method to minimize distributional discrepancies between localized training data and graph inference, improving model performance on OOD data. Extensive tests on popular GNN models show significant performance improvement on three citation GNN benchmark datasets. The regularization approach effectively enhances model adaptation and generalization, overcoming challenges posed by OOD data.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在处理图结构数据方面取得了显著成功，它可以捕捉图中节点之间的复杂关系和依赖关系。它在半监督节点分类、链接预测和图生成等应用中表现出色。然而，需要注意的是，现代GNN模型大多基于内部分布式 Setting  assumptions，这会限制它们在实际世界图中的性能。在这篇文章中，我们想要评估在本地化 subsets 上训练 GNN 的影响。这种受限的训练数据可能会导致模型在特定区域中表现良好，但是无法总结和对整个图进行准确预测。在图基于半监督学习 (SSL) 中，资源限制经常导致数据集很大，但只有一部分可以标记，这会影响模型的性能。这种限制对tasks like anomaly detection 或者垃圾邮件检测来说是一个问题，因为标注过程可能受到人类主观性的影响。为了解决本地化训练数据所带来的挑战，我们将这问题定义为异常数据 (OOD) 问题，并通过对 training data 和图推理过程之间的分布偏好进行对齐，来降低分布偏好的影响。我们提出了一种 regularization 方法，以降低分布偏好的影响，从而提高模型对 OOD 数据的性能。我们在Popular GNN 模型上进行了广泛的测试，并得到了显著的性能提高 results 在三个引用 GNN 数据集上。我们的 regularization 方法有效地提高了模型的适应和泛化性，超越了 OOD 数据的挑战。
</details></li>
</ul>
<hr>
<h2 id="An-Estimator-for-the-Sensitivity-to-Perturbations-of-Deep-Neural-Networks"><a href="#An-Estimator-for-the-Sensitivity-to-Perturbations-of-Deep-Neural-Networks" class="headerlink" title="An Estimator for the Sensitivity to Perturbations of Deep Neural Networks"></a>An Estimator for the Sensitivity to Perturbations of Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12679">http://arxiv.org/abs/2307.12679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naman Maheshwari, Nicholas Malaya, Scott Moe, Jaydeep P. Kulkarni, Sudhanva Gurumurthi</li>
<li>for: 这个论文的目的是为了评估深度神经网络（DNNs）在安全关键应用中的稳定性，如自动驾驶车和疾病诊断。</li>
<li>methods: 这篇论文使用了一种估计器来预测DNNs对输入和模型参数的敏感性。该估计器基于不等式和矩阵范数，可以roughly对整个神经网络的condition number进行估计。</li>
<li>results: 对AlexNet和VGG-19两种卷积神经网络，使用ImageNet数据集进行测试，并通过随机干扰和攻击测试了估计器的紧eness。<details>
<summary>Abstract</summary>
For Deep Neural Networks (DNNs) to become useful in safety-critical applications, such as self-driving cars and disease diagnosis, they must be stable to perturbations in input and model parameters. Characterizing the sensitivity of a DNN to perturbations is necessary to determine minimal bit-width precision that may be used to safely represent the network. However, no general result exists that is capable of predicting the sensitivity of a given DNN to round-off error, noise, or other perturbations in input. This paper derives an estimator that can predict such quantities. The estimator is derived via inequalities and matrix norms, and the resulting quantity is roughly analogous to a condition number for the entire neural network. An approximation of the estimator is tested on two Convolutional Neural Networks, AlexNet and VGG-19, using the ImageNet dataset. For each of these networks, the tightness of the estimator is explored via random perturbations and adversarial attacks.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在安全关键应用中，如自动驾驶车和疾病诊断，必须具有稳定性。为了确定MINIMAL的位数精度可以安全地表示网络，需要量化神经网络对输入和模型参数的敏感性。然而，没有一个通用的结果可以预测给定DNN对圆满误差、噪声或其他输入参数的敏感性。这篇论文提出了一个估计器，可以预测这些量。这个估计器基于不等式和矩阵范数，其结果类似于神经网络的整体condition数。这个估计器的精度被测试在AlexNet和VGG-19Convolutional Neural Networks上，使用ImageNet dataset。对于每个网络，估计器的紧张性被探索 mediante具有随机误差和攻击性的杂音误差。
</details></li>
</ul>
<hr>
<h2 id="Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling"><a href="#Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling" class="headerlink" title="Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling"></a>Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12672">http://arxiv.org/abs/2307.12672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhen Pan, Suprosanna Shit, Özgün Turgut, Wenqi Huang, Hongwei Bran Li, Nil Stolt-Ansó, Thomas Küstner, Kerstin Hammernik, Daniel Rueckert</li>
<li>for: This paper focuses on improving dynamic Magnetic Resonance Imaging (MRI) reconstruction by interpolating undersampled k-space data before obtaining images with Fourier transform.</li>
<li>methods: The proposed approach uses a Transformer-based k-space Global Interpolation Network (k-GIN) to learn global dependencies among low- and high-frequency components of 2D+t k-space, and a novel k-space Iterative Refinement Module (k-IRM) to enhance high-frequency components learning.</li>
<li>results: The proposed method outperforms baseline methods in terms of both quantitative and qualitative measures, and achieves higher robustness and generalizability in highly-undersampled MR data.Here’s the simplified Chinese text for the three points:</li>
<li>for: 这篇论文关注改进动态磁共振成像重建，通过 interpolating 未探测的 k-space 数据 перед使用傅ри transform 获得图像。</li>
<li>methods: 该方法使用 Transformer 基于 k-space 全球 interpolating 网络（k-GIN），学习 k-space 中低频和高频 ком成的全球依赖关系，并使用 novel k-space Iterative Refinement Module (k-IRM) 进一步提高高频 ком成的学习。</li>
<li>results: 该方法在量化和质量上都超过基线方法，并在高度不探测的 MR 数据中具有更高的Robustness 和普适性。<details>
<summary>Abstract</summary>
In dynamic Magnetic Resonance Imaging (MRI), k-space is typically undersampled due to limited scan time, resulting in aliasing artifacts in the image domain. Hence, dynamic MR reconstruction requires not only modeling spatial frequency components in the x and y directions of k-space but also considering temporal redundancy. Most previous works rely on image-domain regularizers (priors) to conduct MR reconstruction. In contrast, we focus on interpolating the undersampled k-space before obtaining images with Fourier transform. In this work, we connect masked image modeling with k-space interpolation and propose a novel Transformer-based k-space Global Interpolation Network, termed k-GIN. Our k-GIN learns global dependencies among low- and high-frequency components of 2D+t k-space and uses it to interpolate unsampled data. Further, we propose a novel k-space Iterative Refinement Module (k-IRM) to enhance the high-frequency components learning. We evaluate our approach on 92 in-house 2D+t cardiac MR subjects and compare it to MR reconstruction methods with image-domain regularizers. Experiments show that our proposed k-space interpolation method quantitatively and qualitatively outperforms baseline methods. Importantly, the proposed approach achieves substantially higher robustness and generalizability in cases of highly-undersampled MR data.
</details>
<details>
<summary>摘要</summary>
在动态磁共振成像（MRI）中，k-空间通常会被受限的扫描时间所压缩，导致图像频率域中的抽象 artifacts。因此，动态MR重建需要不仅考虑x和y方向的空间频率分布，还需要考虑时间重复性。大多数前一些工作都是通过图像领域的正则化（priors）来进行MR重建。与之不同的是，我们将集中于探空 undersampled k-space，并使用傅里叶变换来转换成图像。在这项工作中，我们将掩码图像模型与k-空间插值联系起来，并提出了一种基于Transformer的新型k-空间全局插值网络，称为k-GIN。k-GIN学习了2D+t k-空间中低频和高频组件之间的全球依赖关系，并使用其来插值未探空的数据。此外，我们还提出了一种新的k-空间迭代优化模块（k-IRM），用于进一步优化高频组件的学习。我们对92个自有2D+t心脏MR图像进行评估，并与基于图像领域的正则化方法进行比较。实验表明，我们提出的探空插值方法在量化和质量上都超过了基线方法。重要的是，我们的方法在高度受探空的MR数据情况下具有显著更高的Robustness和普适性。
</details></li>
</ul>
<hr>
<h2 id="Control-and-Monitoring-of-Artificial-Intelligence-Algorithms"><a href="#Control-and-Monitoring-of-Artificial-Intelligence-Algorithms" class="headerlink" title="Control and Monitoring of Artificial Intelligence Algorithms"></a>Control and Monitoring of Artificial Intelligence Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13705">http://arxiv.org/abs/2307.13705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Carlos Mario Braga Ortuño, Blanza Martinez Donoso, Belén Muñiz Villanueva</li>
<li>for: 这篇论文强调在部署人工智能模型后，需要进行监管和监测数据分布的变化。</li>
<li>methods: 本论文介绍了数据漂移和概念漂移的概念，以及他们的基础分布。此外，它还提出了一些可用来评估模型性能对于可能的时间变化的指标。</li>
<li>results: 该论文的研究结果表明，在不同的时间点上，模型的性能可能会受到数据分布的变化的影响。<details>
<summary>Abstract</summary>
This paper elucidates the importance of governing an artificial intelligence model post-deployment and overseeing potential fluctuations in the distribution of present data in contrast to the training data. The concepts of data drift and concept drift are explicated, along with their respective foundational distributions. Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.
</details>
<details>
<summary>摘要</summary>
这篇论文强调在人工智能模型部署后进行管理和监测数据的变化。文章解释了数据漂移和概念漂移的概念，以及它们在不同的基础分布上的基础分布。此外，文章还介绍了一些可用于评估模型在可能的时间变化下的性能的指标。Here's a breakdown of the translation:* 这篇论文 (zhè běn tiǎo wén) - This paper* 强调 (qiáng zhòng) - Emphasize* 在人工智能模型部署后 (zài rénsheng zhìyì módel bùzhè hòu) - After deploying the artificial intelligence model* 进行管理 (jìnxíng guǎn lí) - To manage* 和监测数据的变化 (hé jiān cháng xìng xiàng yǐ) - And monitoring the data's changes* 数据漂移 (xùnxíng) - Data drift* 概念漂移 (guī jiàn) - Concept drift* 在不同的基础分布上 (zài bù tiān de jī chǎi fēn xiàng) - On different foundational distributions* 基础分布 (jī chǎi fēn xiàng) - Foundational distribution* 以及 (yǐ jí) - And* 可用于 (kě yòu yú) - Can be used for* 评估模型在可能的时间变化下的性能 (píng jǐ model zài kě néng de shí huan xìang yǐ) - Evaluating the model's performance under possible temporal variationsI hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="TransFusion-Generating-Long-High-Fidelity-Time-Series-using-Diffusion-Models-with-Transformers"><a href="#TransFusion-Generating-Long-High-Fidelity-Time-Series-using-Diffusion-Models-with-Transformers" class="headerlink" title="TransFusion: Generating Long, High Fidelity Time Series using Diffusion Models with Transformers"></a>TransFusion: Generating Long, High Fidelity Time Series using Diffusion Models with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12667">http://arxiv.org/abs/2307.12667</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fahim-sikder/TransFusion">https://github.com/fahim-sikder/TransFusion</a></li>
<li>paper_authors: Md Fahim Sikder, Resmi Ramachandranpillai, Fredrik Heintz</li>
<li>for: 生成高质量、长序时间序数据的需求广泛，因为它们在各种应用领域有广泛的应用。</li>
<li>methods: 我们提出了一种基于扩散和变换器的生成模型，即TransFusion，以生成高质量长序时间序数据。</li>
<li>results: 我们在384个时间序列长度上进行了扩展，并生成了高质量的 sintetic数据。此外，我们还引入了两种评估指标来评估生成的数据质量和预测特性。与之前的最佳状态比较，TransFusion在各种视觉和实际指标上表现出优异。<details>
<summary>Abstract</summary>
The generation of high-quality, long-sequenced time-series data is essential due to its wide range of applications. In the past, standalone Recurrent and Convolutional Neural Network-based Generative Adversarial Networks (GAN) were used to synthesize time-series data. However, they are inadequate for generating long sequences of time-series data due to limitations in the architecture. Furthermore, GANs are well known for their training instability and mode collapse problem. To address this, we propose TransFusion, a diffusion, and transformers-based generative model to generate high-quality long-sequence time-series data. We have stretched the sequence length to 384, and generated high-quality synthetic data. To the best of our knowledge, this is the first study that has been done with this long-sequence length. Also, we introduce two evaluation metrics to evaluate the quality of the synthetic data as well as its predictive characteristics. We evaluate TransFusion with a wide variety of visual and empirical metrics, and TransFusion outperforms the previous state-of-the-art by a significant margin.
</details>
<details>
<summary>摘要</summary>
“高质量、长序列时间序列数据的生成是非常重要，因为它具有广泛的应用领域。在过去，单独的循环和卷积神经网基于的生成反对策略（GAN）被用来合成时间序列数据。然而，它们在生成长序列时间序列数据方面存在限制，并且GAN在训练中的稳定性和模式崩溃问题很严重。为了解决这个问题，我们提出了TransFusion，一个扩散和传播器基于的生成模型，用于生成高质量的长序列时间序列数据。我们已经将序列长度延长到384，并成功生成了高质量的 sintetic 数据。根据我们所知，这是首次在这个长序列长度下进行的研究。此外，我们也引入了两个评估评价指标，用于评估生成的数据质量以及预测特征。我们将TransFusion评估使用广泛的视觉和实验指标，并表明TransFusion在前一代的State-of-the-art之上出现了明显的进步。”
</details></li>
</ul>
<hr>
<h2 id="Online-Continual-Learning-in-Keyword-Spotting-for-Low-Resource-Devices-via-Pooling-High-Order-Temporal-Statistics"><a href="#Online-Continual-Learning-in-Keyword-Spotting-for-Low-Resource-Devices-via-Pooling-High-Order-Temporal-Statistics" class="headerlink" title="Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics"></a>Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12660">http://arxiv.org/abs/2307.12660</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/umbertomichieli/tap-slda">https://github.com/umbertomichieli/tap-slda</a></li>
<li>paper_authors: Umberto Michieli, Pablo Peso Parada, Mete Ozay</li>
<li>for: 这个研究旨在提高附加到嵌入式设备上的语音识别模型的整合学习能力，让模型能够快速适应用户定义的新词语，而不会忘记之前的词语。</li>
<li>methods: 本研究使用了增量学习的方法，将具有冻结的背景的语音识别模型训练为逐一处理新的词语，并使用了高阶统计量 Computing 的 Temporal Aware Pooling（TAP）技术来建立具有优化的特征空间。</li>
<li>results: 实验结果显示，TAP-SLDA 方法在多种设置、背景和基础上比竞争者提供了11.3%的相对平均提升，在 GSC 数据集上。<details>
<summary>Abstract</summary>
Keyword Spotting (KWS) models on embedded devices should adapt fast to new user-defined words without forgetting previous ones. Embedded devices have limited storage and computational resources, thus, they cannot save samples or update large models. We consider the setup of embedded online continual learning (EOCL), where KWS models with frozen backbone are trained to incrementally recognize new words from a non-repeated stream of samples, seen one at a time. To this end, we propose Temporal Aware Pooling (TAP) which constructs an enriched feature space computing high-order moments of speech features extracted by a pre-trained backbone. Our method, TAP-SLDA, updates a Gaussian model for each class on the enriched feature space to effectively use audio representations. In experimental analyses, TAP-SLDA outperforms competitors on several setups, backbones, and baselines, bringing a relative average gain of 11.3% on the GSC dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG"><a href="#Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG" class="headerlink" title="Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG"></a>Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12644">http://arxiv.org/abs/2307.12644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/remotebiosensing/rppg">https://github.com/remotebiosensing/rppg</a></li>
<li>paper_authors: Dae-Yeol Kim, Eunsu Goh, KwangKee Lee, JongEui Chae, JongHyeon Mun, Junyeong Na, Chae-bong Sohn, Do-Yup Kim</li>
<li>for: 本研究旨在提供一个 benchmarking 框架，以评估不同的 rPPG 技术在各种数据集上的性能，并提供一个可读性好、可重复性高的评估方法。</li>
<li>methods: 本研究使用了各种 rPPG 技术，包括非深度神经网络 (non-DNN) 和深度神经网络 (DNN) 方法，并使用了各种数据集进行训练、测试和验证。</li>
<li>results: 本研究希望通过提供一个可读性好、可重复性高的 benchmarking 框架，帮助研究人员和工业界人员更好地评估和比较不同的 rPPG 技术，以解决现有的挑战和获得更好的性能。<details>
<summary>Abstract</summary>
rPPG (Remote photoplethysmography) is a technology that measures and analyzes BVP (Blood Volume Pulse) by using the light absorption characteristics of hemoglobin captured through a camera. Analyzing the measured BVP can derive various physiological signals such as heart rate, stress level, and blood pressure, which can be applied to various applications such as telemedicine, remote patient monitoring, and early prediction of cardiovascular disease. rPPG is rapidly evolving and attracting great attention from both academia and industry by providing great usability and convenience as it can measure biosignals using a camera-equipped device without medical or wearable devices. Despite extensive efforts and advances in this field, serious challenges remain, including issues related to skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, which degrade accuracy performance. We argue that fair and evaluable benchmarking is urgently required to overcome these challenges and make meaningful progress from both academic and commercial perspectives. In most existing work, models are trained, tested, and validated only on limited datasets. Even worse, some studies lack available code or reproducibility, making it difficult to fairly evaluate and compare performance. Therefore, the purpose of this study is to provide a benchmarking framework to evaluate various rPPG techniques across a wide range of datasets for fair evaluation and comparison, including both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods. GitHub URL: https://github.com/remotebiosensing/rppg
</details>
<details>
<summary>摘要</summary>
远程血液气膜测量技术（rPPG）可以测量和分析血液量脉冲（BVP），通过摄像头捕捉血液中吸收光的特点。分析测量后的BVP可以获得各种生理信号，如心率、压力等，可以应用于远程医疗、患者监测等领域。rPPG在学术和产业界引起了广泛的关注，因为它提供了高度可用性和便捷性，可以通过摄像头设备测量生物信号，无需医疗设备或戴上式设备。然而，该领域仍存在严重的挑战，包括皮肤颜色、摄像头特性、 ambient 照明等因素，这些因素会导致准确性下降。我们认为，需要迅速提供公平可评估的基准框架，以超越这些挑战和实现学术和商业上的进步。现有大多数研究都是在有限的数据集上训练、测试和验证模型，甚至有些研究缺乏可用代码或重现性，使得公平评估和比较表现困难。因此，本研究的目的是提供一个公平可评估的基准框架，用于评估不同的rPPG技术，包括非深度神经网络（non-DNN）和深度神经网络（DNN）方法。GitHub URL：https://github.com/remotebiosensing/rppg
</details></li>
</ul>
<hr>
<h2 id="Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey"><a href="#Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey" class="headerlink" title="Fake News Detection Through Graph-based Neural Networks: A Survey"></a>Fake News Detection Through Graph-based Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12639">http://arxiv.org/abs/2307.12639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuzhi Gong, Richard O. Sinnott, Jianzhong Qi, Cecile Paris</li>
<li>for: 本研究目的是对网络媒体中的假新闻检测方法进行系统性的审查，尤其是基于图structured和深度学习方法的方法。</li>
<li>methods: 该研究分类了现有的图structured方法为知识驱动方法、媒体传播基于方法和多元社交背景基于方法，根据如何构建图结构来模型新闻相关信息的流动。</li>
<li>results: 研究发现，基于图structured和深度学习方法的方法在假新闻检测中具有强大的表现，因为它们可以准确地模型社交媒体上信息的传播和扩散过程。<details>
<summary>Abstract</summary>
The popularity of online social networks has enabled rapid dissemination of information. People now can share and consume information much more rapidly than ever before. However, low-quality and/or accidentally/deliberately fake information can also spread rapidly. This can lead to considerable and negative impacts on society. Identifying, labelling and debunking online misinformation as early as possible has become an increasingly urgent problem. Many methods have been proposed to detect fake news including many deep learning and graph-based approaches. In recent years, graph-based methods have yielded strong results, as they can closely model the social context and propagation process of online news. In this paper, we present a systematic review of fake news detection studies based on graph-based and deep learning-based techniques. We classify existing graph-based methods into knowledge-driven methods, propagation-based methods, and heterogeneous social context-based methods, depending on how a graph structure is constructed to model news related information flows. We further discuss the challenges and open problems in graph-based fake news detection and identify future research directions.
</details>
<details>
<summary>摘要</summary>
“在线社交网络的广泛普及使得信息传播速度加快，人们现在可以更快地分享和消耗信息。然而，低品质和/或意外或故意伪造的信息也可以快速传播，对社会造成重大和负面的影响。识别、标识和驳斥网络诈新的问题已成为对社会的一个急迫的问题。许多方法已经被提出来检测伪新闻，包括多个深度学习和图形基于的方法。在过去的几年中，图形基于的方法已经获得了强大的成果，因为它们可以对网络新闻的社会传播过程进行精确的模拟。在这篇文章中，我们提供了一种系统性的伪新闻检测研究，涵盖基于图形和深度学习的技术。我们将现有的图形基于方法分为知识驱动方法、传播基于方法和多元社交内容基于方法，根据将新闻相关信息流建立的图形结构。我们进一步讨论了伪新闻检测中的挑战和未来研究方向。”
</details></li>
</ul>
<hr>
<h2 id="Identifying-drivers-and-mitigators-for-congestion-and-redispatch-in-the-German-electric-power-system-with-explainable-AI"><a href="#Identifying-drivers-and-mitigators-for-congestion-and-redispatch-in-the-German-electric-power-system-with-explainable-AI" class="headerlink" title="Identifying drivers and mitigators for congestion and redispatch in the German electric power system with explainable AI"></a>Identifying drivers and mitigators for congestion and redispatch in the German electric power system with explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12636">http://arxiv.org/abs/2307.12636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maurizio Titz, Sebastian Pütz, Dirk Witthaut</li>
<li>for: 这篇论文旨在分析德国传输网络中的压力峰值问题，以及随之而来的负荷重新分配和对冲贸易的影响。</li>
<li>methods: 该论文采用可解释的机器学习模型来预测每小时的负荷重新分配和对冲贸易量。该模型可以揭示各因素对网络稠密度的影响，并评估这些因素的影响力。</li>
<li>results: 研究发现，风力发电是主要驱动因素，而水力发电和跨国电力贸易也扮演着重要角色。然而，太阳能发电没有减轻网络压力的效果。研究结果表明，改变市场设计可以缓解压力峰值问题。<details>
<summary>Abstract</summary>
The transition to a sustainable energy supply challenges the operation of electric power systems in manifold ways. Transmission grid loads increase as wind and solar power are often installed far away from the consumers. In extreme cases, system operators must intervene via countertrading or redispatch to ensure grid stability. In this article, we provide a data-driven analysis of congestion in the German transmission grid. We develop an explainable machine learning model to predict the volume of redispatch and countertrade on an hourly basis. The model reveals factors that drive or mitigate grid congestion and quantifies their impact. We show that, as expected, wind power generation is the main driver, but hydropower and cross-border electricity trading also play an essential role. Solar power, on the other hand, has no mitigating effect. Our results suggest that a change to the market design would alleviate congestion.
</details>
<details>
<summary>摘要</summary>
逐渐转移到可再生能源供应的过程对电力系统的运行带来多种挑战。许多风电和太阳能install在消费者之 away，增加了传输网络荷车。在极端情况下，系统运维人员需要通过对贸易或重新分配来维护网络稳定。在这篇文章中，我们提供了一个可解释的机器学习模型，用于预测每小时的重新分配和对贸易量。这个模型揭示了导致或减轻网络拥堵的因素，并衡量它们的影响。我们发现，预期的风力发电是主要的推动因素，而水力发电和跨国电力贸易也扮演着重要的角色。然而，太阳能发电没有缓解效果。我们的结果表明，修改市场设计可以缓解拥堵。
</details></li>
</ul>
<hr>
<h2 id="De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network"><a href="#De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network" class="headerlink" title="De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network"></a>De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12625">http://arxiv.org/abs/2307.12625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Qiang Huang, Haolong Zeng, Yun Pen, Huiyan Sun</li>
<li>for: This paper focuses on developing a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment variables.</li>
<li>methods: The DRL framework uses a non-parametric model to eliminate both linear and nonlinear dependence between treatment and covariates, and it trains the correlations between the de-confounded representations and the treatment variables against the correlations between the covariate representations and the treatment variables.</li>
<li>results: The DRL model outperforms state-of-the-art counterfactual inference models for continuous treatment variables in learning de-confounding representations, and it demonstrates a detailed causal relationship between red cell width distribution and mortality in a real-world medical dataset MIMIC.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文关注于开发一种基于代表学习（DRL）框架，用于连续治疗变量的可靠 outcome 估计。</li>
<li>methods: DRL 框架使用非 paramedic 模型，消除治疗和 covariate 变量之间的线性和非线性相关性。它在 covariate 表示和治疗变量之间的相关性上进行了训练。</li>
<li>results: DRL 模型在连续治疗变量上的可靠 outcome 估计方面表现出色，并在实际医疗数据集 MIMIC 中显示出了详细的 causal 关系 между红细胞宽度分布和死亡率。<details>
<summary>Abstract</summary>
Counterfactual inference for continuous rather than binary treatment variables is more common in real-world causal inference tasks. While there are already some sample reweighting methods based on Marginal Structural Model for eliminating the confounding bias, they generally focus on removing the treatment's linear dependence on confounders and rely on the accuracy of the assumed parametric models, which are usually unverifiable. In this paper, we propose a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment by generating the representations of covariates disentangled with the treatment variables. The DRL is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates. Specifically, we train the correlations between the de-confounded representations and the treatment variables against the correlations between the covariate representations and the treatment variables to eliminate confounding bias. Further, a counterfactual inference network is embedded into the framework to make the learned representations serve both de-confounding and trusted inference. Extensive experiments on synthetic datasets show that the DRL model performs superiorly in learning de-confounding representations and outperforms state-of-the-art counterfactual inference models for continuous treatment variables. In addition, we apply the DRL model to a real-world medical dataset MIMIC and demonstrate a detailed causal relationship between red cell width distribution and mortality.
</details>
<details>
<summary>摘要</summary>
常见的假设推理任务中，对于连续型对策变量的Counterfactual推理更加普遍。虽然现有一些基于Marginal Structural Model的样本重新排重方法，可以删除对于随机变量的随机依赖，但这些方法通常假设对于随机变量的 parametric 模型，这些模型通常是不可验证的。在这篇论文中，我们提出了一个对策变量Counterfactual推理的De-confounding representation learning（DRL）框架。DRL 是一个非Parametric 模型，可以删除对于随机变量和对策变量的 both linear 和非线性依赖。具体来说，我们在DRL框架中训练了对策变量和随机变量之间的相互 correlations 和随机变量和对策变量之间的相互 correlations，以删除随机变量的影响。此外，我们还将Counterfactual推理网络 embed 到DRL框架中，以使得学习的表示可以用于 both de-confounding 和可靠的推理。实际上，我们在 synthetic 数据上进行了广泛的实验，结果显示DRL模型在学习删除随机变量的表示方面表现出色，并且超过了现有的Counterfactual推理模型。此外，我们还应用了DRL模型到了一个真实世界医疗数据集MIMIC，并展示了红细胞幅度分布和死亡的明确 causal 关系。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Ordinary-Differential-Equations-with-Transformers"><a href="#Predicting-Ordinary-Differential-Equations-with-Transformers" class="headerlink" title="Predicting Ordinary Differential Equations with Transformers"></a>Predicting Ordinary Differential Equations with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12617">http://arxiv.org/abs/2307.12617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sören Becker, Michal Klein, Alexander Neitz, Giambattista Parascandolo, Niki Kilbertus</li>
<li>for:  recuperate scalar ordinary differential equations (ODEs) in symbolic form from irregularly sampled and noisy observations of a single solution trajectory</li>
<li>methods: 使用 transformer-based sequence-to-sequence model</li>
<li>results: 在各种设定下，模型表现更好或与现有方法相当，并且可以快速扩展到新观察的 governing law。<details>
<summary>Abstract</summary>
We develop a transformer-based sequence-to-sequence model that recovers scalar ordinary differential equations (ODEs) in symbolic form from irregularly sampled and noisy observations of a single solution trajectory. We demonstrate in extensive empirical evaluations that our model performs better or on par with existing methods in terms of accurate recovery across various settings. Moreover, our method is efficiently scalable: after one-time pretraining on a large set of ODEs, we can infer the governing law of a new observed solution in a few forward passes of the model.
</details>
<details>
<summary>摘要</summary>
我们开发了基于变换器的序列到序列模型，可以从不规则采样和噪声捕获的解析方面恢复普通微分方程（ODEs）的符号形式。我们在广泛的实验中表明，我们的模型在不同设定下比现有方法更高精度地恢复解。此外，我们的方法可以高效扩展：只需一次预训练在大量ODEs上，我们就可以在新观察解的几个前进推进中快速恢复规则。
</details></li>
</ul>
<hr>
<h2 id="ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays"><a href="#ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays" class="headerlink" title="ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays"></a>ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12607">http://arxiv.org/abs/2307.12607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akanksha Dixit, Yashashwee Chakrabarty, Smruti R. Sarangi</li>
<li>for: 提高高频显示器的响应速度和平滑度，使得用户在游戏和虚拟现实应用程序中享受更加流畅和无杂的视觉体验。</li>
<li>methods: 使用再归折（extrapolation）算法和深度神经网络（DNN）来预测下一帧图像，并通过RL算法选择最佳的预测方法以提高帧率并保持图像质量。</li>
<li>results: 与传统方法相比，提出的Exwarp方法可以提高帧率四倍，并且几乎不带有图像质量的下降。<details>
<summary>Abstract</summary>
High-frequency displays are gaining immense popularity because of their increasing use in video games and virtual reality applications. However, the issue is that the underlying GPUs cannot continuously generate frames at this high rate -- this results in a less smooth and responsive experience. Furthermore, if the frame rate is not synchronized with the refresh rate, the user may experience screen tearing and stuttering. Previous works propose increasing the frame rate to provide a smooth experience on modern displays by predicting new frames based on past or future frames. Interpolation and extrapolation are two widely used algorithms that predict new frames. Interpolation requires waiting for the future frame to make a prediction, which adds additional latency. On the other hand, extrapolation provides a better quality of experience because it relies solely on past frames -- it does not incur any additional latency. The simplest method to extrapolate a frame is to warp the previous frame using motion vectors; however, the warped frame may contain improperly rendered visual artifacts due to dynamic objects -- this makes it very challenging to design such a scheme. Past work has used DNNs to get good accuracy, however, these approaches are slow. This paper proposes Exwarp -- an approach based on reinforcement learning (RL) to intelligently choose between the slower DNN-based extrapolation and faster warping-based methods to increase the frame rate by 4x with an almost negligible reduction in the perceived image quality.
</details>
<details>
<summary>摘要</summary>
高频显示器在游戏和虚拟现实应用中的使用越来越普遍，但问题在于下面的GPU无法连续生成这高的帧率，导致用户体验不平滑和不响应。如果帧率与刷新率不同步，用户可能会经历屏掉和停顿。以前的工作建议提高帧率，以提供现代显示器上的平滑体验。 interpolate和extrapolate是两种广泛使用的预测新帧的算法。 interpolate需要等待未来的帧来做预测，这添加了额外的延迟。 extrapolation提供了更好的用户体验，因为它仅基于过去的帧，不增加额外的延迟。 simplest method to extrapolate a frame is to warp the previous frame using motion vectors; however, the warped frame may contain improperly rendered visual artifacts due to dynamic objects -- this makes it very challenging to design such a scheme。过去的工作使用DNN来获得好的准确性，但这些方法慢。这篇论文提出了Exwarp -- 基于强化学习（RL）的方法，智能地选择 slower DNN-based extrapolation和faster warping-based方法，以提高帧率4倍，而且几乎无法感受到图像质量的减少。
</details></li>
</ul>
<hr>
<h2 id="Concept-backpropagation-An-Explainable-AI-approach-for-visualising-learned-concepts-in-neural-network-models"><a href="#Concept-backpropagation-An-Explainable-AI-approach-for-visualising-learned-concepts-in-neural-network-models" class="headerlink" title="Concept backpropagation: An Explainable AI approach for visualising learned concepts in neural network models"></a>Concept backpropagation: An Explainable AI approach for visualising learned concepts in neural network models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12601">http://arxiv.org/abs/2307.12601</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrik-ha/concept-backpropagation">https://github.com/patrik-ha/concept-backpropagation</a></li>
<li>paper_authors: Patrik Hammersborg, Inga Strümke</li>
<li>for: 这个论文的目的是发展一种可解释的人工智能技术，以便更好地理解神经网络模型如何完成任务。</li>
<li>methods: 这篇论文使用了一种名为“概念探测”的方法，用于寻找神经网络模型在训练过程中internal化的概念。具体来说，这种方法使用一个已经训练好的概念探测器，将模型输入数据库中的某些特征进行干扰，以便使概念在模型中得到最大化。这种方法可以在模型的输入空间中直观地显示探测到的概念，从而了解神经网络模型如何依赖于这些概念来完成任务。</li>
<li>results: 这篇论文的结果表明，使用了这种方法可以在不同的输入模式下 visualize 神经网络模型内部的概念表示，并且可以看出这些概念在模型中的排列和相互关系。此外，这种方法还可以用于评估神经网络模型中的概念表示是否受到模型的影响，以及概念之间的相互关系是否存在。<details>
<summary>Abstract</summary>
Neural network models are widely used in a variety of domains, often as black-box solutions, since they are not directly interpretable for humans. The field of explainable artificial intelligence aims at developing explanation methods to address this challenge, and several approaches have been developed over the recent years, including methods for investigating what type of knowledge these models internalise during the training process. Among these, the method of concept detection, investigates which \emph{concepts} neural network models learn to represent in order to complete their tasks. In this work, we present an extension to the method of concept detection, named \emph{concept backpropagation}, which provides a way of analysing how the information representing a given concept is internalised in a given neural network model. In this approach, the model input is perturbed in a manner guided by a trained concept probe for the described model, such that the concept of interest is maximised. This allows for the visualisation of the detected concept directly in the input space of the model, which in turn makes it possible to see what information the model depends on for representing the described concept. We present results for this method applied to a various set of input modalities, and discuss how our proposed method can be used to visualise what information trained concept probes use, and the degree as to which the representation of the probed concept is entangled within the neural network model itself.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimized-data-collection-and-analysis-process-for-studying-solar-thermal-desalination-by-machine-learning"><a href="#Optimized-data-collection-and-analysis-process-for-studying-solar-thermal-desalination-by-machine-learning" class="headerlink" title="Optimized data collection and analysis process for studying solar-thermal desalination by machine learning"></a>Optimized data collection and analysis process for studying solar-thermal desalination by machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12594">http://arxiv.org/abs/2307.12594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guilong Peng, Senshan Sun, Yangjun Qin, Zhenwei Xu, Juxin Du, Swellam W. sharshir, A. W. Kandel, A. E. Kabeel, Nuo Yang</li>
<li>for:  This paper aims to develop a modified dataset collection and analysis process for studying solar-thermal desalination using machine learning.</li>
<li>methods: The paper uses a modified dataset collection and analysis process that includes accelerating data collection and reducing the time by 83.3%. The study also employs three different algorithms, including artificial neural networks, multiple linear regressions, and random forests, to investigate the effects of dataset features on prediction accuracy, factor importance ranking, and model generalization ability.</li>
<li>results: The study finds that a larger dataset can significantly improve prediction accuracy when using artificial neural networks and random forests. Additionally, the study highlights the significant impact of dataset size and range on ranking the importance of influence factors, and reveals that the extrapolation data range significantly affects the extrapolation accuracy of artificial neural networks.<details>
<summary>Abstract</summary>
An effective interdisciplinary study between machine learning and solar-thermal desalination requires a sufficiently large and well-analyzed experimental datasets. This study develops a modified dataset collection and analysis process for studying solar-thermal desalination by machine learning. Based on the optimized water condensation and collection process, the proposed experimental method collects over one thousand datasets, which is ten times more than the average number of datasets in previous works, by accelerating data collection and reducing the time by 83.3%. On the other hand, the effects of dataset features are investigated by using three different algorithms, including artificial neural networks, multiple linear regressions, and random forests. The investigation focuses on the effects of dataset size and range on prediction accuracy, factor importance ranking, and the model's generalization ability. The results demonstrate that a larger dataset can significantly improve prediction accuracy when using artificial neural networks and random forests. Additionally, the study highlights the significant impact of dataset size and range on ranking the importance of influence factors. Furthermore, the study reveals that the extrapolation data range significantly affects the extrapolation accuracy of artificial neural networks. Based on the results, massive dataset collection and analysis of dataset feature effects are important steps in an effective and consistent machine learning process flow for solar-thermal desalination, which can promote machine learning as a more general tool in the field of solar-thermal desalination.
</details>
<details>
<summary>摘要</summary>
需要一个具有足够规模和优化分析的多学科研究，以使机器学习技术在太阳聚变洗涤中得到应用。本研究提出了一种修改后的数据采集和分析过程，以便通过机器学习技术研究太阳聚变洗涤。基于优化的水蒸发和收集过程，提出的实验方法收集了超过一千个数据集，比前一个研究的均值数据集多出了10倍，并且通过加速数据采集和减少时间83.3%。同时，研究对数据集特征的影响进行了三种算法的调查，包括人工神经网络、多linear回归和随机森林。调查关注数据集大小和范围对预测精度、因素重要排名和模型泛化能力的影响。结果显示，大数据集可以在人工神经网络和随机森林中显著提高预测精度。此外，研究还发现数据集大小和范围对因素重要排名产生了深见影响。此外，研究还发现人工神经网络抽象数据范围对抽象预测精度产生了显著影响。根据结果，大规模的数据采集和分析数据集特征的影响是多学科研究中efficient和可靠的机器学习过程流程的重要步骤。这可以推动机器学习在太阳聚变洗涤领域的应用。
</details></li>
</ul>
<hr>
<h2 id="InVAErt-networks-a-data-driven-framework-for-emulation-inference-and-identifiability-analysis"><a href="#InVAErt-networks-a-data-driven-framework-for-emulation-inference-and-identifiability-analysis" class="headerlink" title="InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis"></a>InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12586">http://arxiv.org/abs/2307.12586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoxiang Grayson Tong, Carlos A. Sing Long, Daniele E. Schiavazzi</li>
<li>for: 这个论文主要是为了提出一种数据驱动的分析和synthesis方法来处理物理系统。</li>
<li>methods: 这个方法使用了deterministic encoder和decoder来表示前向和反向解决Map，以及normalizing flow来捕捉系统输出的概率分布。还有一个variational encoder来学习一个具有缺失射影的简洁表示。</li>
<li>results: 作者通过大量的数字例子进行了广泛的验证，包括简单的线性、非线性和周期地图，动力系统以及空间-时间PDEs。<details>
<summary>Abstract</summary>
Use of generative models and deep learning for physics-based systems is currently dominated by the task of emulation. However, the remarkable flexibility offered by data-driven architectures would suggest to extend this representation to other aspects of system synthesis including model inversion and identifiability. We introduce inVAErt (pronounced \emph{invert}) networks, a comprehensive framework for data-driven analysis and synthesis of parametric physical systems which uses a deterministic encoder and decoder to represent the forward and inverse solution maps, normalizing flow to capture the probabilistic distribution of system outputs, and a variational encoder designed to learn a compact latent representation for the lack of bijectivity between inputs and outputs. We formally investigate the selection of penalty coefficients in the loss function and strategies for latent space sampling, since we find that these significantly affect both training and testing performance. We validate our framework through extensive numerical examples, including simple linear, nonlinear, and periodic maps, dynamical systems, and spatio-temporal PDEs.
</details>
<details>
<summary>摘要</summary>
使用生成模型和深度学习来处理物理系统的应用现在主要是模拟。然而，数据驱动建筑的灵活性表示可以扩展到其他系统设计方面，包括模型反转和可识别性。我们介绍inVAErt（缩写为“反转”）网络，一个涵盖数据驱动分析和设计 Parametric Physical Systems 的完整框架，使用权重矩阵来表示前向和反向解决Map，使用正则流来捕捉系统输出的概率分布，并使用变量编码器来学习具有缺失射影的短表示。我们正式调查损失函数中的罚 coefficient选择和秘钥空间采样策略，因为这些对训练和测试性能产生了重要影响。我们通过广泛的数字例子进行验证，包括简单的线性、非线性和周期地图，动力系统和空间时间PDEs。
</details></li>
</ul>
<hr>
<h2 id="Self-refining-of-Pseudo-Labels-for-Music-Source-Separation-with-Noisy-Labeled-Data"><a href="#Self-refining-of-Pseudo-Labels-for-Music-Source-Separation-with-Noisy-Labeled-Data" class="headerlink" title="Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data"></a>Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12576">http://arxiv.org/abs/2307.12576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junghyun Koo, Yunkee Chae, Chang-Bin Jeon, Kyogu Lee</li>
<li>for: 提高音乐源分离（MSS）性能，增加大数据集来改善MSS模型的训练</li>
<li>methods: 自动修正受损标签的数据集，使用含噪标签数据集训练MSS模型</li>
<li>results: 使用修正后的受损数据集训练MSS模型，与使用干净标签数据集训练MSS模型相当，甚至在仅使用受损数据集情况下，MSS模型表现更佳。<details>
<summary>Abstract</summary>
Music source separation (MSS) faces challenges due to the limited availability of correctly-labeled individual instrument tracks. With the push to acquire larger datasets to improve MSS performance, the inevitability of encountering mislabeled individual instrument tracks becomes a significant challenge to address. This paper introduces an automated technique for refining the labels in a partially mislabeled dataset. Our proposed self-refining technique, employed with a noisy-labeled dataset, results in only a 1% accuracy degradation in multi-label instrument recognition compared to a classifier trained on a clean-labeled dataset. The study demonstrates the importance of refining noisy-labeled data in MSS model training and shows that utilizing the refined dataset leads to comparable results derived from a clean-labeled dataset. Notably, upon only access to a noisy dataset, MSS models trained on a self-refined dataset even outperform those trained on a dataset refined with a classifier trained on clean labels.
</details>
<details>
<summary>摘要</summary>
音乐源分离（MSS）面临受限于有限 Correctly-labeled 个人乐器轨迹的可用性。随着提高 MSS 性能的推动，遇到含有错误标签的个人乐器轨迹变得具有重要性。这篇文章介绍了一种自动化的标签修正技术，用于改善含有错误标签的 dataset。我们的提议的自我修复技术，与含有噪音标签的 dataset 结合使用，对多标签乐器识别器的准确率产生了只有1% 的下降。这个研究表明了 MSS 模型训练时需要对含有噪音标签的数据进行修正，并且表明使用修正后的 dataset 可以达到与使用干净标签的 dataset 相同的结果。进一步地，只有在有限的噪音标签 dataset 上，MSS 模型训练在自我修复 dataset 上表现出了与使用干净标签 dataset 上的表现相当。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalising-Neural-Topical-Representations"><a href="#Towards-Generalising-Neural-Topical-Representations" class="headerlink" title="Towards Generalising Neural Topical Representations"></a>Towards Generalising Neural Topical Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12564">http://arxiv.org/abs/2307.12564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohao Yang, He Zhao, Dinh Phung, Lan Du</li>
<li>for: 提高神经话题模型（NTM）的普适性和任务泛化能力。</li>
<li>methods: 使用数据扩充和 hierarchical topic transport distance（HOTT）计算优化交通距离来模型相似文档，从而提高NTM的普适性和任务泛化能力。</li>
<li>results: 对多个corpus和任务进行了广泛的实验，证明了我们的框架可以帮助NTMs在不同的corpus和任务中提高神经话题表示性的普适性和任务泛化能力。<details>
<summary>Abstract</summary>
Topic models have evolved from conventional Bayesian probabilistic models to Neural Topic Models (NTMs) over the last two decays. Although NTMs have achieved promising performance when trained and tested on a specific corpus, their generalisation ability across corpora is rarely studied. In practice, we often expect that an NTM trained on a source corpus can still produce quality topical representation for documents in a different target corpus without retraining. In this work, we aim to improve NTMs further so that their benefits generalise reliably across corpora and tasks. To do so, we propose to model similar documents by minimising their semantical distance when training NTMs. Specifically, similar documents are created by data augmentation during training; The semantical distance between documents is measured by the Hierarchical Topic Transport Distance (HOTT), which computes the Optimal Transport (OT) distance between the topical representations. Our framework can be readily applied to most NTMs as a plug-and-play module. Extensive experiments show that our framework significantly improves the generalisation ability regarding neural topical representation across corpora.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DeepGATGO-A-Hierarchical-Pretraining-Based-Graph-Attention-Model-for-Automatic-Protein-Function-Prediction"><a href="#DeepGATGO-A-Hierarchical-Pretraining-Based-Graph-Attention-Model-for-Automatic-Protein-Function-Prediction" class="headerlink" title="DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction"></a>DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13004">http://arxiv.org/abs/2307.13004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Li, Changkun Jiang, Jianqiang Li</li>
<li>for: 这 paper 的目的是提出一种基于 protein sequence 的自动蛋白功能预测方法，以解决当前依赖劳动力密集的湿法方法的问题。</li>
<li>methods: 这 paper 使用的方法包括使用 protein sequence 和 Gene Ontology (GO)  термина来生成最终的功能预测结果。具体来说， protein sequence 、结构信息和 protein-protein 交互网络被 integrate 为 priori 知识，并与 GO term 的嵌入 fusion 生成最终的预测结果。</li>
<li>results:  эксперименталь结果表明，我们提出的模型可以更好地扩展 GO TERM 批量分析中的蛋白功能预测。<details>
<summary>Abstract</summary>
Automatic protein function prediction (AFP) is classified as a large-scale multi-label classification problem aimed at automating protein enrichment analysis to eliminate the current reliance on labor-intensive wet-lab methods. Currently, popular methods primarily combine protein-related information and Gene Ontology (GO) terms to generate final functional predictions. For example, protein sequences, structural information, and protein-protein interaction networks are integrated as prior knowledge to fuse with GO term embeddings and generate the ultimate prediction results. However, these methods are limited by the difficulty in obtaining structural information or network topology information, as well as the accuracy of such data. Therefore, more and more methods that only use protein sequences for protein function prediction have been proposed, which is a more reliable and computationally cheaper approach. However, the existing methods fail to fully extract feature information from protein sequences or label data because they do not adequately consider the intrinsic characteristics of the data itself. Therefore, we propose a sequence-based hierarchical prediction method, DeepGATGO, which processes protein sequences and GO term labels hierarchically, and utilizes graph attention networks (GATs) and contrastive learning for protein function prediction. Specifically, we compute embeddings of the sequence and label data using pre-trained models to reduce computational costs and improve the embedding accuracy. Then, we use GATs to dynamically extract the structural information of non-Euclidean data, and learn general features of the label dataset with contrastive learning by constructing positive and negative example samples. Experimental results demonstrate that our proposed model exhibits better scalability in GO term enrichment analysis on large-scale datasets.
</details>
<details>
<summary>摘要</summary>
自动蛋白功能预测（AFP）被分类为大规模多标签分类问题，旨在自动化蛋白聚集分析，以消除现有的人工劳动密集方法。现有的方法主要将蛋白质相关信息和生物学功能 ontology（GO）概念结合，生成最终功能预测结果。例如，蛋白质序列、结构信息和蛋白质-蛋白质交互网络被 integrate 为先知信息，并与 GO 概念嵌入结合，生成最终预测结果。然而，这些方法受到结构信息或网络拓扑信息困难获取，以及这些数据的准确性限制。因此，更多的方法仅使用蛋白质序列进行蛋白质功能预测，这是一种更可靠和计算成本更低的方法。然而，现有方法未能充分提取蛋白质序列或标签数据中的特征信息，因为它们未能充分考虑数据本身的内在特征。因此，我们提出了一种序列基于的层次预测方法，深度GATGO，它处理蛋白质序列和 GO 标签 Label  hierarchically，并使用图注意网络（GATs）和对比学习来进行蛋白质功能预测。具体来说，我们使用预训练模型计算序列和标签数据的嵌入，以降低计算成本并提高嵌入精度。然后，我们使用 GATs 动态提取非euclid 数据的结构信息，并使用对比学习学习标签数据的通用特征。实验结果表明，我们提出的模型在 GO 概念增加分析中实现了更好的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Homophily-Driven-Sanitation-View-for-Robust-Graph-Contrastive-Learning"><a href="#Homophily-Driven-Sanitation-View-for-Robust-Graph-Contrastive-Learning" class="headerlink" title="Homophily-Driven Sanitation View for Robust Graph Contrastive Learning"></a>Homophily-Driven Sanitation View for Robust Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12555">http://arxiv.org/abs/2307.12555</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Yulin Zhu, Xing Ai, Yevgeniy Vorobeychik, Kai Zhou</li>
<li>for: 本文 investigate GCL 对于结构性攻击的鲁棒性。</li>
<li>methods: 本文提出了一种基于 homophily 的净化视图，可以与对比学习一起学习。在实现这种方法时，需要解决非对数 differentiable 问题。为此，本文提出了一系列技术来实现梯度下降的结构 GCL。</li>
<li>results: 对于两种 state of the art 结构性攻击，GCHS  consistently 超过所有基eline。在生成节点嵌入质量和两个重要下游任务中，GCHS 表现更好。<details>
<summary>Abstract</summary>
We investigate adversarial robustness of unsupervised Graph Contrastive Learning (GCL) against structural attacks. First, we provide a comprehensive empirical and theoretical analysis of existing attacks, revealing how and why they downgrade the performance of GCL. Inspired by our analytic results, we present a robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. A key challenge this poses, however, is the non-differentiable nature of the sanitation objective. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method which, unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state of the art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state of the art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
</details>
<details>
<summary>摘要</summary>
我们研究对不监督图像对冲学（GCL）的抗性攻击。首先，我们提供了对现有攻击的广泛的实验和理论分析，揭示了这些攻击如何下降GCL的性能。 drawing inspiration from our analytical results, we propose a robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. However, the non-differentiable nature of the sanitation objective poses a challenge. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Furthermore, we develop a fully unsupervised hyperparameter tuning method that does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state-of-the-art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state-of-the-art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
</details></li>
</ul>
<hr>
<h2 id="Continuation-Path-Learning-for-Homotopy-Optimization"><a href="#Continuation-Path-Learning-for-Homotopy-Optimization" class="headerlink" title="Continuation Path Learning for Homotopy Optimization"></a>Continuation Path Learning for Homotopy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12551">http://arxiv.org/abs/2307.12551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xi-l/cpl">https://github.com/xi-l/cpl</a></li>
<li>paper_authors: Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, Qingfu Zhang</li>
<li>for: 提高homotopy优化的性能和提供更多有用信息 для决策</li>
<li>methods: 基于模型的方法，同时优化原问题和所有伪函数问题，并在实时生成任意中间解</li>
<li>results: 实验结果表明，该方法可以大幅提高homotopy优化的性能，并提供更多有用信息支持更好的决策<details>
<summary>Abstract</summary>
Homotopy optimization is a traditional method to deal with a complicated optimization problem by solving a sequence of easy-to-hard surrogate subproblems. However, this method can be very sensitive to the continuation schedule design and might lead to a suboptimal solution to the original problem. In addition, the intermediate solutions, often ignored by classic homotopy optimization, could be useful for many real-world applications. In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which contains infinite intermediate solutions for any surrogate subproblems. Rather than the classic unidirectional easy-to-hard optimization, our method can simultaneously optimize the original problem and all surrogate subproblems in a collaborative manner. The proposed model also supports real-time generation of any intermediate solution, which could be desirable for many applications. Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.
</details>
<details>
<summary>摘要</summary>
“几何优化”是一种传统的优化方法，通过解决一系列易于Difficulty Surrogate subproblems来处理复杂的优化问题。然而，这种方法可能对继续计划设计敏感，导致优化问题的解不够优化。此外，经典几何优化中的中间解，常常被忽略，但这些中间解在许多实际应用中可能很有用。在这种工作中，我们提出了一种基于模型的方法，用于学习整个继续路径，这个路径包含任意surrogate subproblems的无限中间解。而不是 классиic的一次易到Difficulty的优化，我们的方法可以同时优化原始问题和所有surrogate subproblems，以一种协同的方式进行优化。提出的模型还支持实时生成任意中间解，这可能对许多应用有用。实验研究表明，我们的提议方法可以大幅提高几何优化的性能，并提供更多有用的信息，以支持更好的决策。
</details></li>
</ul>
<hr>
<h2 id="On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness"><a href="#On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness" class="headerlink" title="On the Connection between Pre-training Data Diversity and Fine-tuning Robustness"></a>On the Connection between Pre-training Data Diversity and Fine-tuning Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12532">http://arxiv.org/abs/2307.12532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ludwig Schmidt, Ali Farhadi</li>
<li>for: 了解预训练策略对下游模型的泛化性质的影响。</li>
<li>methods: 研究预训练分布中属性的影响，包括标签空间、标签 semantics、图像多样性、数据领域和数据量。</li>
<li>results: 发现预训练分布中数据量的影响是下游模型的有效Robustness的主要因素，其他因素对下游模型的Robustness有限制的影响。<details>
<summary>Abstract</summary>
Pre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. More specifically, we ask the following question: how do properties of the pre-training distribution affect the robustness of a fine-tuned model? The properties we explore include the label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution. We find that the primary factor influencing downstream effective robustness (Taori et al., 2020) is data quantity, while other factors have limited significance. For example, reducing the number of ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is, keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate our findings on pre-training distributions drawn from various natural and synthetic data sources, primarily using the iWildCam-WILDS distribution shift as a test for downstream robustness.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)预训练在深度学习中广泛应用，特别是当目标任务的训练数据匮乏时。在我们的工作中，我们想要理解预训练方法的训练策略对下游模型的泛化性能产生的影响。更 Specifically，我们问的问题是：预训练分布中的特性如何影响下游模型的可靠性？我们考虑的特性包括预训练分布中的标签空间、标签 semantics、图像多样性、数据领域和数据量。我们发现，预训练分布中数据量的影响是下游模型的可靠性的主要因素，而其他因素具有有限的意义。例如，将 ImageNet 预训练分布中的类别数量减少到 4 倍，同时将每个类别的图像数量增加到 4 倍（即保持总数据量不变），对下游模型的可靠性没有影响。我们在不同的自然和 sintetic 数据源中测试了我们的发现，主要使用 iWildCam-WILDS 分布shift作为下游模型的可靠性测试。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph"><a href="#Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph" class="headerlink" title="Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph"></a>Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12526">http://arxiv.org/abs/2307.12526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyixinxin/mrg-kg">https://github.com/wangyixinxin/mrg-kg</a></li>
<li>paper_authors: Yixin Wang, Zihao Lin, Haoyu Dong<br>for:这个研究的目的是提高医疗报告生成（MRG）过程中的 Knowledge Graph（KG）的完整性和应用。methods:这个研究使用了一个完整的 KG，包括137种疾病和异常性，并对现有的 MRG 数据集进行了分析，发现现有的数据集存在长尾问题。为了解决这个问题，该研究提出了一种新的增强策略，并设计了一个两阶段的 MRG 方法。results:该研究的结果表明，提出的两阶段生成方法和增强策略可以大幅提高了生成的报告的准确性和多样性。特别是，该研究提出了一种新的评价指标，即多样性敏感度（DS），可以衡量生成的疾病是否匹配实际的ground truth，并且可以衡量所有生成的疾病的多样性。<details>
<summary>Abstract</summary>
Knowledge Graph (KG) plays a crucial role in Medical Report Generation (MRG) because it reveals the relations among diseases and thus can be utilized to guide the generation process. However, constructing a comprehensive KG is labor-intensive and its applications on the MRG process are under-explored. In this study, we establish a complete KG on chest X-ray imaging that includes 137 types of diseases and abnormalities. Based on this KG, we find that the current MRG data sets exhibit a long-tailed problem in disease distribution. To mitigate this problem, we introduce a novel augmentation strategy that enhances the representation of disease types in the tail-end of the distribution. We further design a two-stage MRG approach, where a classifier is first trained to detect whether the input images exhibit any abnormalities. The classified images are then independently fed into two transformer-based generators, namely, ``disease-specific generator" and ``disease-free generator" to generate the corresponding reports. To enhance the clinical evaluation of whether the generated reports correctly describe the diseases appearing in the input image, we propose diverse sensitivity (DS), a new metric that checks whether generated diseases match ground truth and measures the diversity of all generated diseases. Results show that the proposed two-stage generation framework and augmentation strategies improve DS by a considerable margin, indicating a notable reduction in the long-tailed problem associated with under-represented diseases.
</details>
<details>
<summary>摘要</summary>
医学报告生成（MRG）中知识图（KG）发挥关键作用，因为它揭示疾病之间的关系，可以用于导向生成过程。然而，建立完整的KG是劳动密集的，其在MRG过程中的应用还未得到充分探讨。在这种研究中，我们建立了包括137种疾病和异常的完整KG，基于这个KG，我们发现现有MRG数据集具有长尾问题，即疾病分布不均。为了解决这个问题，我们提出了一种新的扩充策略，该策略可以增强疾病类型在分布尾部的表达。此外，我们设计了两stage的MRG方法，其中首先训练一个分类器来检测输入图像是否具有任何异常。分类结果是否为true，则独立将输入图像 feed into两个基于变换器的生成器，即“疾病特定生成器”和“疾病无异常生成器”，以生成相应的报告。为了提高生成报告的临床评估，我们提出了多样性敏感度（DS），一种新的指标，该指标检查生成疾病与基准数据是否匹配，并且度量所有生成疾病的多样性。结果表明我们的两stage生成框架和扩充策略可以提高DS的较大幅度，这表明我们成功地减少了基于输入图像中的疾病出现的长尾问题。
</details></li>
</ul>
<hr>
<h2 id="Landslide-Surface-Displacement-Prediction-Based-on-VSXC-LSTM-Algorithm"><a href="#Landslide-Surface-Displacement-Prediction-Based-on-VSXC-LSTM-Algorithm" class="headerlink" title="Landslide Surface Displacement Prediction Based on VSXC-LSTM Algorithm"></a>Landslide Surface Displacement Prediction Based on VSXC-LSTM Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12524">http://arxiv.org/abs/2307.12524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Ruichen Li, Fan Liu, Xingquan Li, Juan Cheng, Muzhou Hou, Cong Cao</li>
<li>for: 这 paper 是研究 recent 的滑坡自然灾害，提出一种基于变分幂分析的时间序列预测框架 VMD-SegSigmoid-XGBoost-ClusterLSTM (VSXC-LSTM)，可以更准确地预测滑坡表面移动。</li>
<li>methods: 该 paper 使用了变分幂分析、SegSigmoid 函数、XGBoost 算法和嵌入LSTM神经网络，对实际的一向表面变化数据进行模型化和预测。</li>
<li>results: 对于测试集，模型表现良好，除了随机项子序列外，RMSE 和 MAPE 值均小于 0.1，其中 periodic item 预测模块基于 XGBoost 的 RMSE 为 0.006。<details>
<summary>Abstract</summary>
Landslide is a natural disaster that can easily threaten local ecology, people's lives and property. In this paper, we conduct modelling research on real unidirectional surface displacement data of recent landslides in the research area and propose a time series prediction framework named VMD-SegSigmoid-XGBoost-ClusterLSTM (VSXC-LSTM) based on variational mode decomposition, which can predict the landslide surface displacement more accurately. The model performs well on the test set. Except for the random item subsequence that is hard to fit, the root mean square error (RMSE) and the mean absolute percentage error (MAPE) of the trend item subsequence and the periodic item subsequence are both less than 0.1, and the RMSE is as low as 0.006 for the periodic item prediction module based on XGBoost\footnote{Accepted in ICANN2023}.
</details>
<details>
<summary>摘要</summary>
地陷是一种自然灾害，可以轻易威胁当地生态环境、人们的生命和财产。在这篇论文中，我们使用实际数据进行模拟研究，并提出一种基于变分模式分解的时间序列预测框架 named VMD-SegSigmoid-XGBoost-ClusterLSTM (VSXC-LSTM)，以更准确地预测山崩表面变位。模型在测试集上表现良好，除了随机项子序列外，RMSE和MAPE值均小于0.1，而 periodic item prediction module based on XGBoost的RMSE值甚至为0.006。Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation"><a href="#Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation" class="headerlink" title="Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation"></a>Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12520">http://arxiv.org/abs/2307.12520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neelbhandari6/nmt_text_attack">https://github.com/neelbhandari6/nmt_text_attack</a></li>
<li>paper_authors: Neel Bhandari, Pin-Yu Chen</li>
<li>for: 研究表单文本攻击的稳定性，特别是对于round-trip translation。</li>
<li>methods: 使用6种state-of-the-art文本基于攻击，并对其进行了round-trip translation。</li>
<li>results: 发现6种文本攻击不具有稳定性 послеround-trip translation，并提出了一种基于机器翻译的解决方案，以提高攻击的稳定性。<details>
<summary>Abstract</summary>
Language Models today provide a high accuracy across a large number of downstream tasks. However, they remain susceptible to adversarial attacks, particularly against those where the adversarial examples maintain considerable similarity to the original text. Given the multilingual nature of text, the effectiveness of adversarial examples across translations and how machine translations can improve the robustness of adversarial examples remain largely unexplored. In this paper, we present a comprehensive study on the robustness of current text adversarial attacks to round-trip translation. We demonstrate that 6 state-of-the-art text-based adversarial attacks do not maintain their efficacy after round-trip translation. Furthermore, we introduce an intervention-based solution to this problem, by integrating Machine Translation into the process of adversarial example generation and demonstrating increased robustness to round-trip translation. Our results indicate that finding adversarial examples robust to translation can help identify the insufficiency of language models that is common across languages, and motivate further research into multilingual adversarial attacks.
</details>
<details>
<summary>摘要</summary>
现代语言模型在许多下游任务上具有高精度，但它们仍然容易受到恶意攻击，特别是对于那些保持原文相似性的恶意示例。由于文本的多语言性，文本攻击的效iveness在翻译过程中和机器翻译如何提高攻击示例的Robustness remain largely unexplored。在这篇论文中，我们进行了现有文本攻击的综合研究，证明了6种state-of-the-art文本基于攻击不同语言翻译后仍然有效。此外，我们还提出了一种 intervención-based解决方案，通过将机器翻译 integrate into攻击示例生成过程，并证明了 Round-trip translation 中的Robustness提高。我们的结果表明，找到可以在翻译过程中保持效力的攻击示例可以帮助发现语言模型的共同不足，并促进多语言攻击的研究。
</details></li>
</ul>
<hr>
<h2 id="DEPHN-Different-Expression-Parallel-Heterogeneous-Network-using-virtual-gradient-optimization-for-Multi-task-Learning"><a href="#DEPHN-Different-Expression-Parallel-Heterogeneous-Network-using-virtual-gradient-optimization-for-Multi-task-Learning" class="headerlink" title="DEPHN: Different Expression Parallel Heterogeneous Network using virtual gradient optimization for Multi-task Learning"></a>DEPHN: Different Expression Parallel Heterogeneous Network using virtual gradient optimization for Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12519">http://arxiv.org/abs/2307.12519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Ri Su, Shaojie Zhao, Muzhou Hou</li>
<li>for: 这 paper 的目的是提出一种基于多任务学习（MTL）的推荐系统算法，以便互联网运营商更好地理解用户和预测用户在多种行为场景中的行为。</li>
<li>methods: 这 paper 使用了一种名为 Different Expression Parallel Heterogeneous Network（DEPHN）来模型多个任务同时。DEPHN 通过不同的特征交互方法来提高共享信息流的通用能力。在训练过程中，DEPHN 使用特征显式映射和虚拟Gradient Coefficient来实现专家闭合和任务 corr relation的处理。</li>
<li>results: EXTENSIVE experiments 表明，我们提出的方法可以在复杂的实际场景中捕捉任务 corr relation，并在基eline 模型上达到更好的性能。<details>
<summary>Abstract</summary>
Recommendation system algorithm based on multi-task learning (MTL) is the major method for Internet operators to understand users and predict their behaviors in the multi-behavior scenario of platform. Task correlation is an important consideration of MTL goals, traditional models use shared-bottom models and gating experts to realize shared representation learning and information differentiation. However, The relationship between real-world tasks is often more complex than existing methods do not handle properly sharing information. In this paper, we propose an Different Expression Parallel Heterogeneous Network (DEPHN) to model multiple tasks simultaneously. DEPHN constructs the experts at the bottom of the model by using different feature interaction methods to improve the generalization ability of the shared information flow. In view of the model's differentiating ability for different task information flows, DEPHN uses feature explicit mapping and virtual gradient coefficient for expert gating during the training process, and adaptively adjusts the learning intensity of the gated unit by considering the difference of gating values and task correlation. Extensive experiments on artificial and real-world datasets demonstrate that our proposed method can capture task correlation in complex situations and achieve better performance than baseline models\footnote{Accepted in IJCNN2023}.
</details>
<details>
<summary>摘要</summary>
优化推荐算法基于多任务学习（MTL）是互联网运营商理解用户和预测多种场景下的行为的主要方法。任务相关性是MTL目标的重要考虑因素，传统模型使用共享底模型和阻止专家来实现共享表示学习和信息差异化。然而，现实世界中任务之间的关系经常更加复杂，现有方法无法正确地分享信息。在这篇论文中，我们提议一种不同表达平行多态网络（DEPHN），以同时模型多个任务。DEPHN在底层模型中使用不同的特征互动方法来提高共享信息流的泛化能力。视情况不同，DEPHN使用特征显式映射和虚拟梯度系数进行专家闭合 durante 训练过程中，并通过考虑闭合值和任务相关性来适应性地调整学习强度。广泛的人工和实际数据测试表明，我们提出的方法可以在复杂情况下捕捉任务相关性并实现比基eline模型更好的性能。
</details></li>
</ul>
<hr>
<h2 id="FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks"><a href="#FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks" class="headerlink" title="FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks"></a>FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12518">http://arxiv.org/abs/2307.12518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Shaojie Zhao, Juan Cheng, Xingquan Li, Ri Su, Muzhou Hou, Cong Cao</li>
<li>for: 该文章目的是提出一种基于人类学习过程的feature-aware协同相关神经网络（FaFCNN），用于疾病分类任务。</li>
<li>methods: 该方法基于域外挑战学习和协同相关模型，引入了特征感知互动模块和特征对齐模块。</li>
<li>results: 实验结果表明，使用增强样本的权重学习树训练的FaFCNN在低质量数据集上表现最优，并且在多种竞争基线模型中表现最佳。此外，广泛的实验还证明了该方法的稳定性和每个组件的效果。<details>
<summary>Abstract</summary>
There are two fundamental problems in applying deep learning/machine learning methods to disease classification tasks, one is the insufficient number and poor quality of training samples; another one is how to effectively fuse multiple source features and thus train robust classification models. To address these problems, inspired by the process of human learning knowledge, we propose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which introduces a feature-aware interaction module and a feature alignment module based on domain adversarial learning. This is a general framework for disease classification, and FaFCNN improves the way existing methods obtain sample correlation features. The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods. On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines. In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model\footnote{Accepted in IEEE SMC2023}.
</details>
<details>
<summary>摘要</summary>
“there are two fundamental problems in applying deep learning/machine learning methods to disease classification tasks, one is the insufficient number and poor quality of training samples; another one is how to effectively fuse multiple source features and thus train robust classification models. to address these problems, inspired by the process of human learning knowledge, we propose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which introduces a feature-aware interaction module and a feature alignment module based on domain adversarial learning. this is a general framework for disease classification, and FaFCNN improves the way existing methods obtain sample correlation features. the experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods. on the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines. in addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model。”Here's the breakdown of the translation:* "insufficient number and poor quality of training samples" becomes "训练样本数量和质量不足"* "feature-aware fusion" becomes "特征意识的融合"* "domain adversarial learning" becomes "领域对抗学习"* "sample correlation features" becomes "样本相关特征"* "gradient boosting decision tree" becomes "梯度提升决策树"* "random-forest based methods" becomes "随机森林基于方法"* "low-quality dataset" becomes "低质量数据集"* "consistently optimal performance" becomes "一致性优化性能"Note that some terms may have been translated differently based on their context and the specific requirements of the translation.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Evaluation-of-Temporal-Graph-Benchmark"><a href="#An-Empirical-Evaluation-of-Temporal-Graph-Benchmark" class="headerlink" title="An Empirical Evaluation of Temporal Graph Benchmark"></a>An Empirical Evaluation of Temporal Graph Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12510">http://arxiv.org/abs/2307.12510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yule-BUAA/DyGLib_TGB">https://github.com/yule-BUAA/DyGLib_TGB</a></li>
<li>paper_authors: Le Yu</li>
<li>for: 本研究是一个empirical evaluation of Temporal Graph Benchmark (TGB)，通过扩展我们的Dynamic Graph Library (DyGLib)来对TGB进行评估。</li>
<li>methods: 本研究使用了11种流行的动态图学习方法进行对比，以便更全面地评估TGB。</li>
<li>results: 实验结果显示，不同的模型在不同的数据集上表现出了不同的性能，与之前的观察一致；同时，一些基eline的性能可以通过使用DyGLib进行改进，以提高其性能。<details>
<summary>Abstract</summary>
In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们进行了emporical评估Temporal Graph Benchmark（TGB），通过扩展我们的动态图库（DyGLib）到TGB。与TGB相比，我们包括了十一种流行的动态图学习方法，以便更加广泛的比较。通过实验，我们发现了以下两点：（1）不同的模型在不同的数据集上表现出了不同的性能，与之前的观察一致；（2）使用DyGLib时，一些基eline的性能可以得到明显的提高，超过了TGB中所报道的结果。本工作的目标是为研究者提供一个简单便捷的方式来评估不同的动态图学习方法，并提供可直接参考的结果，以便在后续研究中进行建立。所有使用的资源都公开可用于https://github.com/yule-BUAA/DyGLib_TGB。此工作处于进行中，欢迎社区提供反馈以便改进。
</details></li>
</ul>
<hr>
<h2 id="AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models"><a href="#AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models" class="headerlink" title="AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models"></a>AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12499">http://arxiv.org/abs/2307.12499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuelong Dai, Kaisheng Liang, Bin Xiao</li>
<li>For: The paper aims to generate unrestricted adversarial examples for deep learning models, which can effectively bypass existing defense mechanisms and pose a serious threat to their security.* Methods: The paper proposes a new method called AdvDiff, which uses diffusion models to generate adversarial examples. Two novel adversarial guidance techniques are designed to conduct adversarial sampling in the reverse generation process of diffusion models, making the generated examples high-quality and realistic.* Results: The paper demonstrates the effectiveness of AdvDiff on MNIST and ImageNet datasets, outperforming GAN-based methods in terms of attack performance and generation quality. The generated adversarial examples are of high quality and are able to effectively bypass existing defense mechanisms.<details>
<summary>Abstract</summary>
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvDiff is effective to generate unrestricted adversarial examples, which outperforms GAN-based methods in terms of attack performance and generation quality.
</details>
<details>
<summary>摘要</summary>
深度学习模型面临无限制的敌对攻击呈现了严重的安全问题。这些攻击可以有效绕过防御机制，对深度学习应用程序的安全造成严重问题。然而，之前的攻击方法经常使用生成式对抗网络（GAN），这些网络不是理论可证明的，因此在大规模数据集如ImageNet中生成不真实的例子。在这篇论文中，我们提出了一种新的方法，称为AdvDiff，用于生成无限制的敌对示例。我们设计了两种新的对抗导航技术来进行对抗采样在扩散模型的反生成过程中。这两种技术是可靠和有效的，可以具有高质量和真实的敌对示例，通过可 интеpretability的梯度来整合目标分类器。实验结果表明，AdvDiff在MNIST和ImageNet数据集上效果地生成了无限制的敌对示例，其性能高于基于GAN的方法。
</details></li>
</ul>
<hr>
<h2 id="A-faster-and-simpler-algorithm-for-learning-shallow-networks"><a href="#A-faster-and-simpler-algorithm-for-learning-shallow-networks" class="headerlink" title="A faster and simpler algorithm for learning shallow networks"></a>A faster and simpler algorithm for learning shallow networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12496">http://arxiv.org/abs/2307.12496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Sitan Chen, Shyam Narayanan</li>
<li>for: 学习一个线性组合的 $k$ 个 ReLU 活化器，给出标注的示例来自标准 $d$-维高斯分布。</li>
<li>methods: 使用 Chen et al. [CDG+23] 提出的算法，该算法可以在 $\text{poly}(d,1&#x2F;\varepsilon)$ 时间内运行，当 $k &#x3D; O(1)$ 时。</li>
<li>results: 我们表明，使用一个简单的一阶版本的该算法即可，其运行时间为 $(d&#x2F;\varepsilon)^{O(k^2)} $。<details>
<summary>Abstract</summary>
We revisit the well-studied problem of learning a linear combination of $k$ ReLU activations given labeled examples drawn from the standard $d$-dimensional Gaussian measure. Chen et al. [CDG+23] recently gave the first algorithm for this problem to run in $\text{poly}(d,1/\varepsilon)$ time when $k = O(1)$, where $\varepsilon$ is the target error. More precisely, their algorithm runs in time $(d/\varepsilon)^{\mathrm{quasipoly}(k)}$ and learns over multiple stages. Here we show that a much simpler one-stage version of their algorithm suffices, and moreover its runtime is only $(d/\varepsilon)^{O(k^2)}$.
</details>
<details>
<summary>摘要</summary>
我们回到了已经很好地研究过的问题：学习一个线性结构中的 $k$ 个 ReLU 活化器，使用标示的例子从标准 $d$-维 Gaussian 分布中获取。陈等人 [CDG+23] 最近提出了首个可以在 $\text{poly}(d,1/\varepsilon)$ 时间内运行的算法，其中 $k = O(1)$，并且目标错误为 $\varepsilon$。更精确地说，他们的算法在多个阶段中运行，并且其时间长度为 $(d/\varepsilon)^{\mathrm{quasipoly}(k)}$。在这里，我们表明了一个更简单的一阶版本的他们的算法足够，并且其时间长度仅为 $(d/\varepsilon)^{O(k^2)}$。
</details></li>
</ul>
<hr>
<h2 id="Learning-Universal-and-Robust-3D-Molecular-Representations-with-Graph-Convolutional-Networks"><a href="#Learning-Universal-and-Robust-3D-Molecular-Representations-with-Graph-Convolutional-Networks" class="headerlink" title="Learning Universal and Robust 3D Molecular Representations with Graph Convolutional Networks"></a>Learning Universal and Robust 3D Molecular Representations with Graph Convolutional Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12491">http://arxiv.org/abs/2307.12491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Zhang, Yang Liu, Li Xie, Lei Xie</li>
<li>for: 用于学习分子准确表示</li>
<li>methods: 使用方向节点对（DNP）描述器和分子图 convolutional neural network（RoM-GCN）</li>
<li>results: 超过所有参考基eline表现， validate the superiority of DNP descriptor in incorporating 3D geometric information of molecules.<details>
<summary>Abstract</summary>
To learn accurate representations of molecules, it is essential to consider both chemical and geometric features. To encode geometric information, many descriptors have been proposed in constrained circumstances for specific types of molecules and do not have the properties to be ``robust": 1. Invariant to rotations and translations; 2. Injective when embedding molecular structures. In this work, we propose a universal and robust Directional Node Pair (DNP) descriptor based on the graph representations of 3D molecules. Our DNP descriptor is robust compared to previous ones and can be applied to multiple molecular types. To combine the DNP descriptor and chemical features in molecules, we construct the Robust Molecular Graph Convolutional Network (RoM-GCN) which is capable to take both node and edge features into consideration when generating molecule representations. We evaluate our model on protein and small molecule datasets. Our results validate the superiority of the DNP descriptor in incorporating 3D geometric information of molecules. RoM-GCN outperforms all compared baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Resource-Allocation-Policy-Vertex-GNN-or-Edge-GNN"><a href="#Learning-Resource-Allocation-Policy-Vertex-GNN-or-Edge-GNN" class="headerlink" title="Learning Resource Allocation Policy: Vertex-GNN or Edge-GNN?"></a>Learning Resource Allocation Policy: Vertex-GNN or Edge-GNN?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12480">http://arxiv.org/abs/2307.12480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Peng, Jia Guo, Chenyang Yang</li>
<li>for: 这篇论文探讨了用Graph Neural Networks（GNNs）学习无线策略的问题，具体来说是链接调度、功率控制和 precoding 策略。</li>
<li>methods: 这篇论文使用了Vertex-GNNs和Edge-GNNs两种不同的GNNs来学习无线策略。它们都是通过处理和 pooling 邻居链接和边的信息来更新隐藏表示的。</li>
<li>results: 研究发现，GNNs的表达力取决于处理和组合函数的线性和输出维度。当使用线性处理器时，链接-GNNs 无法分辨所有的通道矩阵，而 Edge-GNNs 可以。在学习 precoding 策略时，即使使用非线性处理器，链接-GNNs 的表达力仍然可能不够强大，因为通道矩阵的维度压缩。研究还提供了 necessary conditions 以确保 GNNs 能够好好地学习 precoding 策略。实验结果证明了这些分析结论，并表明 Edge-GNNs 可以在训练和推理时间更低的情况下达到同样的性能。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) update the hidden representations of vertices (called Vertex-GNNs) or hidden representations of edges (called Edge-GNNs) by processing and pooling the information of neighboring vertices and edges and combining to incorporate graph topology. When learning resource allocation policies, GNNs cannot perform well if their expressive power are weak, i.e., if they cannot differentiate all input features such as channel matrices. In this paper, we analyze the expressive power of the Vertex-GNNs and Edge-GNNs for learning three representative wireless policies: link scheduling, power control, and precoding policies. We find that the expressive power of the GNNs depend on the linearity and output dimensions of the processing and combination functions. When linear processors are used, the Vertex-GNNs cannot differentiate all channel matrices due to the loss of channel information, while the Edge-GNNs can. When learning the precoding policy, even the Vertex-GNNs with non-linear processors may not be with strong expressive ability due to the dimension compression. We proceed to provide necessary conditions for the GNNs to well learn the precoding policy. Simulation results validate the analyses and show that the Edge-GNNs can achieve the same performance as the Vertex-GNNs with much lower training and inference time.
</details>
<details>
<summary>摘要</summary>
Graph neural networks (GNNs) 更新隐藏表示的顶点（称为顶点GNNs）或边的隐藏表示（称为边GNNs），通过处理和汇聚邻居顶点和边的信息，并将其结合以利用图 topology。在学习资源分配策略时，如果GNNs的表达力弱，即无法分辨输入特征，如通道矩阵。在这篇论文中，我们分析顶点GNNs和边GNNs在学习三种代表性无线策略：链接调度、功率控制和预处理策略。我们发现GNNs的表达力取决于处理和组合函数的线性和输出维度。当使用线性处理器时，顶点GNNs无法分辨所有通道矩阵，而边GNNs可以。在学习预处理策略时，即使使用非线性处理器，顶点GNNs的表达力仍然不强，这是因为维度压缩。我们随后提供了必要的条件，使GNNs能够好好地学习预处理策略。实验结果证明了我们的分析，并显示了边GNNs可以与顶点GNNs相比，在训练和推理时间上减少很多。
</details></li>
</ul>
<hr>
<h2 id="Model-free-generalized-fiducial-inference"><a href="#Model-free-generalized-fiducial-inference" class="headerlink" title="Model-free generalized fiducial inference"></a>Model-free generalized fiducial inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12472">http://arxiv.org/abs/2307.12472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan P Williams</li>
<li>for:  This paper aims to develop a model-free statistical framework for uncertainty quantification in machine learning, with a focus on imprecise probabilistic prediction inference.</li>
<li>methods:  The paper proposes and develops a new approach to uncertainty quantification that facilitates finite sample control of type 1 errors and offers more versatile tools for imprecise probabilistic reasoning.</li>
<li>results:  The paper presents a precise probabilistic approximation to the model-free imprecise framework, which is critical for the broader adoption of imprecise probabilistic approaches to inference in the statistical and machine learning communities.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是为机器学习领域开发一种无模型的统计框架，用于不确定性评估。</li>
<li>methods: 论文提出了一种新的不确定性评估方法，它可以实现finite sample控制类型1错误，并且提供更多的不确定性推理工具。</li>
<li>results: 论文提出了一种precise的统计approximation，用于无模型不确定性推理框架。这种approximation是用于在统计和机器学习领域更广泛地采用不确定性推理方法的标准。<details>
<summary>Abstract</summary>
Motivated by the need for the development of safe and reliable methods for uncertainty quantification in machine learning, I propose and develop ideas for a model-free statistical framework for imprecise probabilistic prediction inference. This framework facilitates uncertainty quantification in the form of prediction sets that offer finite sample control of type 1 errors, a property shared with conformal prediction sets, but this new approach also offers more versatile tools for imprecise probabilistic reasoning. Furthermore, I propose and consider the theoretical and empirical properties of a precise probabilistic approximation to the model-free imprecise framework. Approximating a belief/plausibility measure pair by an [optimal in some sense] probability measure in the credal set is a critical resolution needed for the broader adoption of imprecise probabilistic approaches to inference in statistical and machine learning communities. It is largely undetermined in the statistical and machine learning literatures, more generally, how to properly quantify uncertainty in that there is no generally accepted standard of accountability of stated uncertainties. The research I present in this manuscript is aimed at motivating a framework for statistical inference with reliability and accountability as the guiding principles.
</details>
<details>
<summary>摘要</summary>
Motivated by the need for safe and reliable methods for uncertainty quantification in machine learning, I propose and develop ideas for a model-free statistical framework for imprecise probabilistic prediction inference. This framework enables uncertainty quantification in the form of prediction sets that offer finite sample control of type 1 errors, a property shared with conformal prediction sets, but this new approach also offers more versatile tools for imprecise probabilistic reasoning. Furthermore, I propose and consider the theoretical and empirical properties of a precise probabilistic approximation to the model-free imprecise framework. Approximating a belief/plausibility measure pair by an optimal probability measure in the credal set is a critical resolution needed for the broader adoption of imprecise probabilistic approaches to inference in the statistical and machine learning communities. It is largely undetermined in the statistical and machine learning literatures how to properly quantify uncertainty, and the research I present in this manuscript is aimed at motivating a framework for statistical inference with reliability and accountability as the guiding principles.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Data-Distillation-Do-Not-Overlook-Calibration"><a href="#Rethinking-Data-Distillation-Do-Not-Overlook-Calibration" class="headerlink" title="Rethinking Data Distillation: Do Not Overlook Calibration"></a>Rethinking Data Distillation: Do Not Overlook Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12463">http://arxiv.org/abs/2307.12463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets">https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets</a></li>
<li>paper_authors: Dongyao Zhu, Bowen Lei, Jie Zhang, Yanbo Fang, Ruqi Zhang, Yiqun Xie, Dongkuan Xu</li>
<li>for: 这 paper 的目的是解决基于压缩数据的神经网络模型具有过度自信的问题，通过calibration方法来改善模型的准确性。</li>
<li>methods: 该 paper 使用的方法包括temperature scaling和mixup，但发现这些方法无法calibrate基于压缩数据的神经网络模型。为了解决这问题， authors 提出了Masked Temperature Scaling (MTS)和Masked Distillation Training (MDT)两种方法，以改善模型的准确性并保持数据压缩的效率。</li>
<li>results: Authors 通过实验表明，MTS 和 MDT 可以减轻基于压缩数据的神经网络模型的过度自信问题，同时保持模型的准确性和效率。<details>
<summary>Abstract</summary>
Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.
</details>
<details>
<summary>摘要</summary>
neural networks 经过精炼数据训练后通常会生成过自信的输出，需要进行减调方法来修正。现有的减调方法包括温度Scaling和mixup，这些方法在原始大规模数据上训练的网络上工作良好。然而，我们发现这些方法无法calibrate来自大源数据集的数据精炼后的网络。在这篇论文中，我们显示出精炼数据导致的网络不可calibrate，因为（i）精炼数据集的最大лог值的分布更加集中，以及（ii） Semantically meaningful but unrelated to classification tasks的信息被丢失。为解决这个问题，我们提议使用Masked Temperature Scaling（MTS）和Masked Distillation Training（MDT），这些方法可以减少精炼数据的限制，实现更好的减调结果，同时保持数据精炼的效率。
</details></li>
</ul>
<hr>
<h2 id="Rates-of-Approximation-by-ReLU-Shallow-Neural-Networks"><a href="#Rates-of-Approximation-by-ReLU-Shallow-Neural-Networks" class="headerlink" title="Rates of Approximation by ReLU Shallow Neural Networks"></a>Rates of Approximation by ReLU Shallow Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12461">http://arxiv.org/abs/2307.12461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Mao, Ding-Xuan Zhou</li>
<li>for: 这 paper  investigate  shallow neural network  approximating H&quot;older space 函数。</li>
<li>methods:  paper 使用 ReLU  activation function 和一层隐藏层来 aproximate H&quot;older space 函数。</li>
<li>results: paper 得到 uniform approximation 率 O((\log m)^{1&#x2F;2 + d} m^{-r&#x2F;d (d+2)&#x2F;(d+4)}) when r &lt; d&#x2F;2 + 2, 这个率几乎与最佳率 O(m^{-r&#x2F;d}) 相近。<details>
<summary>Abstract</summary>
Neural networks activated by the rectified linear unit (ReLU) play a central role in the recent development of deep learning. The topic of approximating functions from H\"older spaces by these networks is crucial for understanding the efficiency of the induced learning algorithms. Although the topic has been well investigated in the setting of deep neural networks with many layers of hidden neurons, it is still open for shallow networks having only one hidden layer. In this paper, we provide rates of uniform approximation by these networks. We show that ReLU shallow neural networks with $m$ hidden neurons can uniformly approximate functions from the H\"older space $W_\infty^r([-1, 1]^d)$ with rates $O((\log m)^{\frac{1}{2} +d}m^{-\frac{r}{d}\frac{d+2}{d+4}})$ when $r<d/2 +2$. Such rates are very close to the optimal one $O(m^{-\frac{r}{d}})$ in the sense that $\frac{d+2}{d+4}$ is close to $1$, when the dimension $d$ is large.
</details>
<details>
<summary>摘要</summary>
“神经网络activated by rectified linear unit（ReLU）在深度学习的发展中扮演了中心作用。关于使用这些网络来近似Holder空间中函数的问题，是深度学习算法的效率的关键。虽然这个问题在多层神经网络的设置下已经得到了广泛的研究，但是尚未解决了单层神经网络的情况。在这篇论文中，我们提供了uniform approximation的速率。我们证明了ReLU单层神经网络可以 uniform approximation W∞^r([-1, 1]^d)中函数的速率为O((\log m)^\(\frac{1}{2} + d\)m^(\frac{r}{d}\*\(\frac{d+2}{d+4}\))），当r<d/2+2时。这些速率与优化的一个 O(m^(-r/d)) 很相似，当维度d很大时，\(\frac{d+2}{d+4}\)几乎等于1。”
</details></li>
</ul>
<hr>
<h2 id="Information-theoretic-Analysis-of-Test-Data-Sensitivity-in-Uncertainty"><a href="#Information-theoretic-Analysis-of-Test-Data-Sensitivity-in-Uncertainty" class="headerlink" title="Information-theoretic Analysis of Test Data Sensitivity in Uncertainty"></a>Information-theoretic Analysis of Test Data Sensitivity in Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12456">http://arxiv.org/abs/2307.12456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Futoshi Futami, Tomoharu Iwata</li>
<li>for: 本研究旨在更好地理解bayesian inference中的uncertainty量化任务中的不确定性。</li>
<li>methods: 本文使用了xu和raginsky（2022）的新方法，即将预测不确定性 decomposed into two kinds of uncertainty：aleatoric uncertainty和epistemic uncertainty。这两种不确定性分别表示数据生成过程中的自然随机性和数据不够的多样性。</li>
<li>results: 本研究成功地定义了这种不确定性敏感性，并将其与信息理论量化的方式相关联。此外，本文还扩展了bayesian meta-learning中的现有分析，并首次显示了任务之间的新的敏感性关系。<details>
<summary>Abstract</summary>
Bayesian inference is often utilized for uncertainty quantification tasks. A recent analysis by Xu and Raginsky 2022 rigorously decomposed the predictive uncertainty in Bayesian inference into two uncertainties, called aleatoric and epistemic uncertainties, which represent the inherent randomness in the data-generating process and the variability due to insufficient data, respectively. They analyzed those uncertainties in an information-theoretic way, assuming that the model is well-specified and treating the model's parameters as latent variables. However, the existing information-theoretic analysis of uncertainty cannot explain the widely believed property of uncertainty, known as the sensitivity between the test and training data. It implies that when test data are similar to training data in some sense, the epistemic uncertainty should become small. In this work, we study such uncertainty sensitivity using our novel decomposition method for the predictive uncertainty. Our analysis successfully defines such sensitivity using information-theoretic quantities. Furthermore, we extend the existing analysis of Bayesian meta-learning and show the novel sensitivities among tasks for the first time.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DiAMoNDBack-Diffusion-denoising-Autoregressive-Model-for-Non-Deterministic-Backmapping-of-Cα-Protein-Traces"><a href="#DiAMoNDBack-Diffusion-denoising-Autoregressive-Model-for-Non-Deterministic-Backmapping-of-Cα-Protein-Traces" class="headerlink" title="DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of Cα Protein Traces"></a>DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of Cα Protein Traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12451">http://arxiv.org/abs/2307.12451</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferg-lab/diamondback">https://github.com/ferg-lab/diamondback</a></li>
<li>paper_authors: Michael S. Jones, Kirill Shmilovich, Andrew L. Ferguson</li>
<li>for: 这个论文的目的是开发一种基于概率模型的方法，用于从粗细化分子模型中恢复到原子分子模型，以便在长时间尺度上模拟蛋白质的杂化和折叠过程。</li>
<li>methods: 这个方法基于推 diffusion-denoising autoregressive model，使用推 diffusion过程来恢复原子分子模型，并且使用了当地的概率模型来保证生成的结果是可靠的。</li>
<li>results: 这个方法在65000多个蛋白质结构数据上进行训练，并在一个独立的蛋白质结构数据集上进行验证，以及在分子动力学 simulated annealing 和 coarse-grained simulation 数据上进行应用。结果表明，这个方法可以高效地恢复原子分子模型，并且可以保证生成的结果是可靠的和多样的。<details>
<summary>Abstract</summary>
Coarse-grained molecular models of proteins permit access to length and time scales unattainable by all-atom models and the simulation of processes that occur on long-time scales such as aggregation and folding. The reduced resolution realizes computational accelerations but an atomistic representation can be vital for a complete understanding of mechanistic details. Backmapping is the process of restoring all-atom resolution to coarse-grained molecular models. In this work, we report DiAMoNDBack (Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping) as an autoregressive denoising diffusion probability model to restore all-atom details to coarse-grained protein representations retaining only C{\alpha} coordinates. The autoregressive generation process proceeds from the protein N-terminus to C-terminus in a residue-by-residue fashion conditioned on the C{\alpha} trace and previously backmapped backbone and side chain atoms within the local neighborhood. The local and autoregressive nature of our model makes it transferable between proteins. The stochastic nature of the denoising diffusion process means that the model generates a realistic ensemble of backbone and side chain all-atom configurations consistent with the coarse-grained C{\alpha} trace. We train DiAMoNDBack over 65k+ structures from Protein Data Bank (PDB) and validate it in applications to a hold-out PDB test set, intrinsically-disordered protein structures from the Protein Ensemble Database (PED), molecular dynamics simulations of fast-folding mini-proteins from DE Shaw Research, and coarse-grained simulation data. We achieve state-of-the-art reconstruction performance in terms of correct bond formation, avoidance of side chain clashes, and diversity of the generated side chain configurational states. We make DiAMoNDBack model publicly available as a free and open source Python package.
</details>
<details>
<summary>摘要</summary>
高级分辨率蛋白质模型允许访问不可达的长度和时间尺度，如聚集和折叠过程。减少分辨率实现计算加速，但全原子表示是完全理解机制细节的必要条件。在这种工作中，我们报道了一种推 diffusion-denoising autoregressive model for non-deterministic backmapping（DiAMoNDBack），用于在高级分辨率蛋白质模型中恢复全原子细节，保留只有Cα坐标。这种推 diffusion进程从蛋白质N端到C端的推进程，在每个残基上进行推进，conditioned on the Cα trace和已经backmapping的蛋白质脊梁和副链原子。本地和自适应的我们模型使其可以在不同蛋白质上传递。由于推 diffusion过程的随机性，模型会生成一个真实的ensemble of backbone和副链全原子配置，与高级分辨率Cα轨迹相符。我们在65000+结构中训练DiAMoNDBack，并对其进行了验证。我们在应用于保留PDB测试集、蛋白质结构数据库（PED）中的自发性蛋白质结构、DE Shaw Research的分秒级分子动力学 simulations和含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂环境中的含杂
</details></li>
</ul>
<hr>
<h2 id="ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation"><a href="#ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation" class="headerlink" title="ProtoFL: Unsupervised Federated Learning via Prototypical Distillation"></a>ProtoFL: Unsupervised Federated Learning via Prototypical Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12450">http://arxiv.org/abs/2307.12450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hansol Kim, Youngjun Kwak, Minyoung Jung, Jinho Shin, Youngsung Kim, Changick Kim</li>
<li>for: 提高数据隐私保护和授权系统性能</li>
<li>methods: 使用Prototypical Representation Distillation和不监控学习来增强全球模型的表示力和减少通信成本</li>
<li>results: 在五个广泛使用的标准数据集上进行了广泛的实验，证明了我们提出的框架在先前的文献中表现更出色，并且在一类分类任务中提高了表现。<details>
<summary>Abstract</summary>
Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种有前途的方法，尤其是 для 验证系统的数据隐私保护。然而，有限的回合通信、罕见的表现和扩展性问题，对其部署带来很大的挑战，阻碍其完整的潜力。在这篇论文中，我们提出了“ProtoFL”，基于无监督式聚合学习的构造塑性数据分布来增强全球模型的表现力和减少回合通信成本。此外，我们还引入了基于正规函数的本地一类分类器，以提高具有有限数据的表现。我们的研究是文献中第一次使用 federated learning 提高一类分类性能的调查。我们对五种通用测试集进行了广泛的实验，包括 MNIST、CIFAR-10、CIFAR-100、ImageNet-30 和 Keystroke-Dynamics，以示出我们的提案架构在先前的方法上表现出色。
</details></li>
</ul>
<hr>
<h2 id="WEPRO-Weight-Prediction-for-Efficient-Optimization-of-Hybrid-Quantum-Classical-Algorithms"><a href="#WEPRO-Weight-Prediction-for-Efficient-Optimization-of-Hybrid-Quantum-Classical-Algorithms" class="headerlink" title="WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms"></a>WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12449">http://arxiv.org/abs/2307.12449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satwik Kundu, Debarshi Kundu, Swaroop Ghosh</li>
<li>for: 提高量子 simulate 的效率，以便更好地训练量子 neural network、量子归一化算法和量子优化算法。</li>
<li>methods: 提出一种新的方法——WEPRO，通过利用参数 weights 的常见趋势来加速量子 simulate 的 converge。并 introduce 两种优化预测性能的技术——Naive Prediction 和 Adaptive Prediction。</li>
<li>results: 通过对多个量子 neural network 模型的训练和多种数据集的实验，显示 WEPRO 可以提供约 $2.25\times$ 的速度提升，同时也提供更高的准确率（最高提升 $2.3%$）和更低的损失（最低降低 $6.1%$），并具有低的存储和计算负担。此外，对 VQE 和 QAOA 进行了评估，结果表明 WEPRO 在这些应用中也可以提供速度提升（最高提升 $3.1\times$ 和 $2.91\times$），同时使用更少的射击数（最多降低 $3.3\times$）。<details>
<summary>Abstract</summary>
The exponential run time of quantum simulators on classical machines and long queue depths and high costs of real quantum devices present significant challenges in the effective training of Variational Quantum Algorithms (VQAs) like Quantum Neural Networks (QNNs), Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA). To address these limitations, we propose a new approach, WEPRO (Weight Prediction), which accelerates the convergence of VQAs by exploiting regular trends in the parameter weights. We introduce two techniques for optimal prediction performance namely, Naive Prediction (NaP) and Adaptive Prediction (AdaP). Through extensive experimentation and training of multiple QNN models on various datasets, we demonstrate that WEPRO offers a speedup of approximately $2.25\times$ compared to standard training methods, while also providing improved accuracy (up to $2.3\%$ higher) and loss (up to $6.1\%$ lower) with low storage and computational overheads. We also evaluate WEPRO's effectiveness in VQE for molecular ground-state energy estimation and in QAOA for graph MaxCut. Our results show that WEPRO leads to speed improvements of up to $3.1\times$ for VQE and $2.91\times$ for QAOA, compared to traditional optimization techniques, while using up to $3.3\times$ less number of shots (i.e., repeated circuit executions) per training iteration.
</details>
<details>
<summary>摘要</summary>
“量子 simulate 的时间复杂度和磁盘队列深度以及真正量子设备的成本高，对于量子神经网络（QNN）、量子准确矩阵（VQE）和量子近似优化算法（QAOA）的有效训练带来了 significante 挑战。为了解决这些限制，我们提出了一种新的方法，WEPRO（参数预测），它通过利用参数权重的固有趋势来加速 VQAs 的快速整合。我们 introduce 了两种优化性能的技术， namely，Naive Prediction（NaP）和 Adaptive Prediction（AdaP）。通过对多个 QNN 模型在不同的数据集进行广泛的实验和训练，我们证明了 WEPRO 可以提供约 $2.25\times$ 的速度提升，同时也提供了更高的准确率（最高 $2.3\%$）和损失（最低 $6.1\%$），并且具有低的存储和计算负担。我们还评估了 WEPRO 在 VQE 中的粒子能量估计和 QAOA 中的图像 MaxCut 性能。我们的结果表明，WEPRO 可以提供 Up to $3.1\times$ 的速度提升在 VQE 中，并且使用 Up to $3.3\times$  fewer number of shots per training iteration。”
</details></li>
</ul>
<hr>
<h2 id="Multifidelity-Covariance-Estimation-via-Regression-on-the-Manifold-of-Symmetric-Positive-Definite-Matrices"><a href="#Multifidelity-Covariance-Estimation-via-Regression-on-the-Manifold-of-Symmetric-Positive-Definite-Matrices" class="headerlink" title="Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices"></a>Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12438">http://arxiv.org/abs/2307.12438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aimee Maurais, Terrence Alsup, Benjamin Peherstorfer, Youssef Marzouk</li>
<li>for: 这个论文的目的是提出一种多 fidelties  covariance矩阵估计器，用于恰当地估计 covariance矩阵的特征。</li>
<li>methods: 这个估计器是基于投影问题的 manifold 上的 regression 问题，使用 manifold 上的 Mahalanobis 距离来最小化。</li>
<li>results:  compared to single-fidelity 和其他多 fidelties  covariance估计器，这个估计器可以提供更好的估计结果，减少平方差误差高达一个数量级。此外，估计器保持了正定性，使其可以在下游任务中使用，如数据充背和度量学习。<details>
<summary>Abstract</summary>
We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.
</details>
<details>
<summary>摘要</summary>
我们介绍一个多信度估计器，用于构造对称正定矩阵的covariance矩阵。这个估计器是由条件最小化 Mahalanobis 距离来定义的，并且具有可行 computation 的属性。我们显示了我们的数据 regression 多信度估计器（MRMF）是一个最大 LIKELIHOOD 估计器，在某些错误模型上的拓扑向量空间上。更加广泛地说，我们的 Riemannian  regression 框架包含了其他多信度 covariance 估计器，它们是由控制预测项所构成的。我们透过 numerics 例子展示了我们的估计器可以提供相对于单信度和其他多信度 covariance 估计器的一个多倍减少，甚至是一个阶层减少。此外，保持对称正定性的保证，使得我们的估计器适合进行下游任务，如数据吸收和度量学习，在这些任务中，这个属性是必要的。
</details></li>
</ul>
<hr>
<h2 id="A-Generalized-Schwarz-type-Non-overlapping-Domain-Decomposition-Method-using-Physics-constrained-Neural-Networks"><a href="#A-Generalized-Schwarz-type-Non-overlapping-Domain-Decomposition-Method-using-Physics-constrained-Neural-Networks" class="headerlink" title="A Generalized Schwarz-type Non-overlapping Domain Decomposition Method using Physics-constrained Neural Networks"></a>A Generalized Schwarz-type Non-overlapping Domain Decomposition Method using Physics-constrained Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12435">http://arxiv.org/abs/2307.12435</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hipersimlab/pecann">https://github.com/hipersimlab/pecann</a></li>
<li>paper_authors: Shamsulhaq Basir, Inanc Senocak</li>
<li>for: 解决部分� differential 方程（PDEs）的前向和 inverse 问题</li>
<li>methods: 使用人工神经网络和非重叠领域划分法，并采用一般化的Robin条件来保证邻居子域的一致性</li>
<li>results: 对一种 Laplace 和 Helmholtz 方程的前向和 inverse 问题进行了广泛的实验，并证明了该方法的灵活性和性能<details>
<summary>Abstract</summary>
We present a meshless Schwarz-type non-overlapping domain decomposition method based on artificial neural networks for solving forward and inverse problems involving partial differential equations (PDEs). To ensure the consistency of solutions across neighboring subdomains, we adopt a generalized Robin-type interface condition, assigning unique Robin parameters to each subdomain. These subdomain-specific Robin parameters are learned to minimize the mismatch on the Robin interface condition, facilitating efficient information exchange during training. Our method is applicable to both the Laplace's and Helmholtz equations. It represents local solutions by an independent neural network model which is trained to minimize the loss on the governing PDE while strictly enforcing boundary and interface conditions through an augmented Lagrangian formalism. A key strength of our method lies in its ability to learn a Robin parameter for each subdomain, thereby enhancing information exchange with its neighboring subdomains. We observe that the learned Robin parameters adapt to the local behavior of the solution, domain partitioning and subdomain location relative to the overall domain. Extensive experiments on forward and inverse problems, including one-way and two-way decompositions with crosspoints, demonstrate the versatility and performance of our proposed approach.
</details>
<details>
<summary>摘要</summary>
我们提出了一种无缝 Schwarz-类非重叠域分解方法，基于人工神经网络来解决部分� differential 方程（PDEs）的前向和反向问题。为确保邻居子域解的一致性，我们采用了一种泛化 Robin-类界面条件，将每个子域分配特定的 Robin 参数。这些子域特定的 Robin 参数通过训练来最小化 Robin 界面条件的差异，从而促进信息交换的有效进行。我们的方法适用于勒拉契和赫尔姆霍兹方程。它通过独立的神经网络模型来表示本地解，该模型在权重 Lagrange  formalism 下 strict 执行边界和界面条件，以最小化 governing PDE 中的损失。我们的方法的一个关键优点在于可以学习每个子域的 Robin 参数，从而提高信息交换的效率。我们发现，学习的 Robin 参数适应本地解的行为、域分割和子域的位置相对于总域。我们的实验表明，我们提出的方法在前向和反向问题中展现出了广泛的应用和高效性。
</details></li>
</ul>
<hr>
<h2 id="Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection"><a href="#Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection" class="headerlink" title="Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection"></a>Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12427">http://arxiv.org/abs/2307.12427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YuyangSunshine/ABR_IOD">https://github.com/YuyangSunshine/ABR_IOD</a></li>
<li>paper_authors: Liu Yuyang, Cong Yang, Goswami Dipam, Liu Xialei, Joost van de Weijer</li>
<li>for: Addressing catastrophic forgetting in incremental object detection (IOD) by replaying stored samples from previous tasks.</li>
<li>methods: Novel Augmented Box Replay (ABR) method that only stores and replays foreground objects, and an innovative Attentive RoI Distillation loss that uses spatial attention from region-of-interest (RoI) features to constrain current model.</li>
<li>results: Significant reduction of forgetting of previous classes while maintaining high plasticity in current classes, with considerably reduced storage requirements compared to standard image replay. State-of-the-art performance on Pascal-VOC and COCO datasets.Here’s the full summary in Simplified Chinese:</li>
<li>for: 这个论文目的是解决对象检测 tasks 的 catastrophic forgetting 问题，通过在前一 tasks 中储存和重新播放当前 tasks 的标本。</li>
<li>methods: 提出了一种名为 Augmented Box Replay (ABR) 的新方法，它只储存和重新播放前一 tasks 中的前景物体，以避免 background 中的前景物体对 current tasks 的影响。此外，还提出了一个创新的 Attentive RoI Distillation loss，它使用了 Region-of-Interest (RoI) 特征的空间注意力来给 current model 强制关注前一 tasks 中最重要的信息。</li>
<li>results: ABR 方法能够有效降低 previous classes 的忘却，同时保持高度的柔软性，并且与标准的 image replay 相比，储存需求有所减少。实验结果表明，模型在 Pascal-VOC 和 COCO 数据集上具有国际一流的表现。<details>
<summary>Abstract</summary>
In incremental learning, replaying stored samples from previous tasks together with current task samples is one of the most efficient approaches to address catastrophic forgetting. However, unlike incremental classification, image replay has not been successfully applied to incremental object detection (IOD). In this paper, we identify the overlooked problem of foreground shift as the main reason for this. Foreground shift only occurs when replaying images of previous tasks and refers to the fact that their background might contain foreground objects of the current task. To overcome this problem, a novel and efficient Augmented Box Replay (ABR) method is developed that only stores and replays foreground objects and thereby circumvents the foreground shift problem. In addition, we propose an innovative Attentive RoI Distillation loss that uses spatial attention from region-of-interest (RoI) features to constrain current model to focus on the most important information from old model. ABR significantly reduces forgetting of previous classes while maintaining high plasticity in current classes. Moreover, it considerably reduces the storage requirements when compared to standard image replay. Comprehensive experiments on Pascal-VOC and COCO datasets support the state-of-the-art performance of our model.
</details>
<details>
<summary>摘要</summary>
增量学习中，重复先前任务中的样本与当前任务的样本一起是解决快速忘却最有效的方法之一。然而，与增量分类不同，增量物体检测（IOD）中的图像重复还没有得到成功应用。在这篇论文中，我们认为背景变化导致的前景偏移是主要的问题。只有在重复先前任务的图像时，前景中可能包含当前任务的前景对象。为解决这个问题，我们开发了一种新的和高效的增强框架重复（ABR）方法，该方法只存储和重复前景对象，从而缺省前景偏移问题。此外，我们提出了一种创新的关注点损失，使用区域特征的空间注意力来约束当前模型关注到最重要的信息。ABR显著降低了先前类型的忘却，同时保持当前类型的高柔性。此外，它与标准图像重复相比有较大的存储需求减少。我们在 Pascal-VOC 和 COCO 数据集上进行了广泛的实验，并支持我们的模型达到了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Practical-Commercial-5G-Standalone-SA-Uplink-Throughput-Prediction"><a href="#Practical-Commercial-5G-Standalone-SA-Uplink-Throughput-Prediction" class="headerlink" title="Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction"></a>Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12417">http://arxiv.org/abs/2307.12417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kasidis Arunruangsirilert, Jiro Katto</li>
<li>for: This paper aims to improve uplink throughput prediction in 5G New Radio (NR) networks, specifically in the high-frequency millimeter wave (mmWave) band, to enhance the quality of experience (QoE) for uplink-intensive smartphone applications such as real-time transmission of UHD 4K&#x2F;8K videos and Virtual Reality (VR)&#x2F;Augmented Reality (AR) contents.</li>
<li>methods: The authors propose using a ConvLSTM-based neural network to predict future uplink throughput based on past uplink throughput and RF parameters. The network is trained using real-world drive test data from commercial 5G SA networks while riding commuter trains, and is limited to only use information available via Android API to make it practical for implementation.</li>
<li>results: The authors achieve an average prediction accuracy of 98.9% with an average RMSE of 1.80 Mbps across all unseen evaluation scenarios, demonstrating the effectiveness of their proposed method in predicting uplink throughput in 5G NR networks.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文目的是提高5G新 Radio（NR）网络中下行通道吞吐量预测，尤其是在高频毫米波（mmWave）频率带中，以提高智能手机应用程序如实时传输UHD 4K&#x2F;8K视频和虚拟现实（VR）&#x2F;增强现实（AR）内容的质量经验（QoE）。</li>
<li>methods: 作者们提议使用ConvLSTM基于神经网络来预测未来下行通道吞吐量，基于过去下行通道吞吐量和RF参数。网络通过商业5G SA网络的实际驱动测试数据进行训练，并限制模型只使用Android API中可得到的信息，以确保实用性。</li>
<li>results: 作者们在所有未seen评估场景中达到了98.9%的预测精度，RMSE值为1.80 Mbps，这显示了他们提议的方法在5G NR网络中预测下行通道吞吐量的效果。<details>
<summary>Abstract</summary>
While the 5G New Radio (NR) network promises a huge uplift of the uplink throughput, the improvement can only be seen when the User Equipment (UE) is connected to the high-frequency millimeter wave (mmWave) band. With the rise of uplink-intensive smartphone applications such as the real-time transmission of UHD 4K/8K videos, and Virtual Reality (VR)/Augmented Reality (AR) contents, uplink throughput prediction plays a huge role in maximizing the users' quality of experience (QoE). In this paper, we propose using a ConvLSTM-based neural network to predict the future uplink throughput based on past uplink throughput and RF parameters. The network is trained using the data from real-world drive tests on commercial 5G SA networks while riding commuter trains, which accounted for various frequency bands, handover, and blind spots. To make sure our model can be practically implemented, we then limited our model to only use the information available via Android API, then evaluate our model using the data from both commuter trains and other methods of transportation. The results show that our model reaches an average prediction accuracy of 98.9\% with an average RMSE of 1.80 Mbps across all unseen evaluation scenarios.
</details>
<details>
<summary>摘要</summary>
“5G新Radio（NR）网络 promise a huge uplift of the uplink throughput, but the improvement can only be seen when the User Equipment（UE） is connected to the high-frequency millimeter wave（mmWave）band. With the rise of uplink-intensive smartphone applications such as real-time transmission of UHD 4K/8K videos and Virtual Reality（VR）/Augmented Reality（AR）contents, uplink throughput prediction plays a huge role in maximizing the users' quality of experience（QoE）. In this paper, we propose using a ConvLSTM-based neural network to predict the future uplink throughput based on past uplink throughput and RF parameters. The network is trained using the data from real-world drive tests on commercial 5G SA networks while riding commuter trains, which accounted for various frequency bands, handover, and blind spots. To make sure our model can be practically implemented, we then limited our model to only use the information available via Android API, and evaluate our model using the data from both commuter trains and other methods of transportation. The results show that our model reaches an average prediction accuracy of 98.9% with an average RMSE of 1.80 Mbps across all unseen evaluation scenarios.”Here's the translation in Traditional Chinese:“5G新Radio（NR）网络 promised a huge uplift of the uplink throughput, but the improvement can only be seen when the User Equipment（UE） is connected to the high-frequency millimeter wave（mmWave）band. With the rise of uplink-intensive smartphone applications such as real-time transmission of UHD 4K/8K videos and Virtual Reality（VR）/Augmented Reality（AR）contents, uplink throughput prediction plays a huge role in maximizing the users' quality of experience（QoE）. In this paper, we propose using a ConvLSTM-based neural network to predict the future uplink throughput based on past uplink throughput and RF parameters. The network is trained using the data from real-world drive tests on commercial 5G SA networks while riding commuter trains, which accounted for various frequency bands, handover, and blind spots. To make sure our model can be practically implemented, we then limited our model to only use the information available via Android API, and evaluate our model using the data from both commuter trains and other methods of transportation. The results show that our model reaches an average prediction accuracy of 98.9% with an average RMSE of 1.80 Mbps across all unseen evaluation scenarios.”
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Approach-to-Two-Stage-Adaptive-Robust-Optimization"><a href="#A-Machine-Learning-Approach-to-Two-Stage-Adaptive-Robust-Optimization" class="headerlink" title="A Machine Learning Approach to Two-Stage Adaptive Robust Optimization"></a>A Machine Learning Approach to Two-Stage Adaptive Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12409">http://arxiv.org/abs/2307.12409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Dimitris Bertsimas, Cheol Woo Kim</li>
<li>for: 解决两阶段线性适应 robust 优化（ARO）问题，其中变量是二进制的“现在”决策和多面体不确定集。</li>
<li>methods: 基于机器学习的方法，包括编码优化“现在”决策、最差情况相关的优化“现在”决策和等待决策，以及使用列和约束生成算法提取优化策略。</li>
<li>results: 对具有多个相似ARO实例的问题进行预处理，并使用机器学习模型预测高质量策略，解决ARO问题比州先进技术更快、准确。<details>
<summary>Abstract</summary>
We propose an approach based on machine learning to solve two-stage linear adaptive robust optimization (ARO) problems with binary here-and-now variables and polyhedral uncertainty sets. We encode the optimal here-and-now decisions, the worst-case scenarios associated with the optimal here-and-now decisions, and the optimal wait-and-see decisions into what we denote as the strategy. We solve multiple similar ARO instances in advance using the column and constraint generation algorithm and extract the optimal strategies to generate a training set. We train a machine learning model that predicts high-quality strategies for the here-and-now decisions, the worst-case scenarios associated with the optimal here-and-now decisions, and the wait-and-see decisions. We also introduce an algorithm to reduce the number of different target classes the machine learning algorithm needs to be trained on. We apply the proposed approach to the facility location, the multi-item inventory control and the unit commitment problems. Our approach solves ARO problems drastically faster than the state-of-the-art algorithms with high accuracy.
</details>
<details>
<summary>摘要</summary>
We solve multiple similar ARO instances in advance using the column and constraint generation algorithm and extract the optimal strategies to generate a training set. We train a machine learning model that predicts high-quality strategies for the here-and-now decisions, the worst-case scenarios associated with the optimal here-and-now decisions, and the wait-and-see decisions.To reduce the number of different target classes the machine learning algorithm needs to be trained on, we introduce an algorithm. We apply the proposed approach to the facility location, the multi-item inventory control, and the unit commitment problems. Our approach solves ARO problems much faster than the state-of-the-art algorithms with high accuracy.Translated into Simplified Chinese:我们提出一种基于机器学习的方法来解决两阶段线性适应Robust优化（ARO）问题，其中变量为二进制的当下决策和不确定集为多面体。我们将优化的当下决策、最差情况相关的优化当下决策和等待决策编码为策略。我们使用列和约束生成算法解决多个相似的ARO实例，并提取优化策略来生成训练集。我们使用机器学习模型预测高质量的当下决策、最差情况相关的优化当下决策和等待决策。我们还提出了一种算法，以减少机器学习算法需要训练的目标类型数量。我们应用我们的方法解决了设备位置、多项存储控制和生产规划问题。我们的方法可以快速解决ARO问题，高度准确。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Control-of-Multiclass-Fluid-Queueing-Networks-A-Machine-Learning-Approach"><a href="#Optimal-Control-of-Multiclass-Fluid-Queueing-Networks-A-Machine-Learning-Approach" class="headerlink" title="Optimal Control of Multiclass Fluid Queueing Networks: A Machine Learning Approach"></a>Optimal Control of Multiclass Fluid Queueing Networks: A Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12405">http://arxiv.org/abs/2307.12405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Bertsimas, Cheol Woo Kim</li>
<li>for: 本研究旨在提出一种机器学习方法来优化多类流体队列网络（MFQNET）的控制问题，以获得明确和深入的控制策略。</li>
<li>methods: 本研究使用了优化分类树（OCT）和折线拟合（H）来学习MFQNET的控制策略。数据集是通过数值解决MFQNET控制问题而得到的，并通过OCT-H来学习明确的控制策略。</li>
<li>results: 实验结果表明，使用OCT-H学习的控制策略可以在大规模网络中实时应用，并且在33个服务器和99个类型的数据集上达到100%的准确率。尽管在大规模网络中的线上训练可能需要几天的时间，但在线应用仅需毫秒钟。<details>
<summary>Abstract</summary>
We propose a machine learning approach to the optimal control of multiclass fluid queueing networks (MFQNETs) that provides explicit and insightful control policies. We prove that a threshold type optimal policy exists for MFQNET control problems, where the threshold curves are hyperplanes passing through the origin. We use Optimal Classification Trees with hyperplane splits (OCT-H) to learn an optimal control policy for MFQNETs. We use numerical solutions of MFQNET control problems as a training set and apply OCT-H to learn explicit control policies. We report experimental results with up to 33 servers and 99 classes that demonstrate that the learned policies achieve 100\% accuracy on the test set. While the offline training of OCT-H can take days in large networks, the online application takes milliseconds.
</details>
<details>
<summary>摘要</summary>
我们提出一种机器学习方法来优化多类流体队列网络（MFQNET）的控制问题，该方法提供了明确和深入的控制策略。我们证明了MFQNET控制问题中存在一种阈值型优化策略，其阈值曲线为通过原点的hyperplane。我们使用Optimal Classification Trees with hyperplane splits（OCT-H）来学习MFQNET的控制策略。我们使用MFQNET控制问题的数学解为训练集，并应用OCT-H来学习明确的控制策略。我们对33个服务器和99个类的实验结果表明，学习的策略可以在测试集上达到100%的准确率。虽然大规模网络的离线训练可能需要几天的时间，但在线应用仅需毫秒钟。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control"><a href="#Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control" class="headerlink" title="Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control"></a>Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12388">http://arxiv.org/abs/2307.12388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longchao Da, Hao Mei, Romir Sharma, Hua Wei</li>
<li>for: 提高交通信号控制（TSC）的效率和可靠性，使用人工智能技术来优化交通管理。</li>
<li>methods: 提出了一种基于强化学习（RL）的实际到真实世界（sim-to-real）传输方法，通过动态转换在模拟环境中学习的策略来减少域之间差距。</li>
<li>results: 在模拟交通环境中评估了该方法，并显示其在真实世界中可以大幅提高RL策略的表现。<details>
<summary>Abstract</summary>
Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
</details>
<details>
<summary>摘要</summary>
交通信号控制（TSC）是一项复杂且重要的任务，影响了百万人的日常生活。强化学习（RL）已经显示出优秀的结果，但现有RL基于TSC方法主要在模拟环境中训练，它们受到模拟和真实世界之间的性能差距的影响。在这篇论文中，我们提出了一种从模拟环境到真实世界（sim-to-real）传递方法，称为UGAT，该方法可以将模拟环境中学习的策略在真实世界中转移，并通过动态将模拟环境中的动作转换为不确定性来减少模拟和真实世界之间的领域差距。我们在模拟交通环境中评估了我们的方法，并显示了在真实世界中转移的RL策略的显著性能提高。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning"><a href="#In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning" class="headerlink" title="In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning"></a>In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12375">http://arxiv.org/abs/2307.12375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannik Kossen, Tom Rainforth, Yarin Gal</li>
<li>for: 本研究旨在探讨Language Model（LLM）在下游任务中的启示学习（ICL）能力如何工作，以及ICL如何影响LLM的预测结果。</li>
<li>methods: 本研究使用了一些现有的LLM模型，并在这些模型中添加了启示示例，以研究ICL的影响。研究还使用了一些不同的预训练任务和启示示例来检验ICL的作用。</li>
<li>results: 研究发现，LLMs通常会在启示示例中包含标签信息时，对输入的预测结果产生改善。然而，研究还发现，ICL的影响并不一定是通过直接学习标签关系来实现的，而是通过在预训练和启示示例之间的交互来实现。此外，研究还发现ICL不一定会考虑所有的启示信息，而是偏好某些特定的启示示例。<details>
<summary>Abstract</summary>
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into understanding and aligning LLM behavior.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在下游任务中的性能通常会显著提高，当包含输入-标签关系的示例在内的上下文中。然而，目前没有一致的观点关于如何在上下文中学习（ICL）能力的LLM工作：例如，XYZ等人（2021）认为ICL类似于通用学习算法，而MIN等人（2022b）则认为ICL不会从内容中学习标签关系。在这篇论文中，我们研究了以下几个问题：（1）如何影响预测的标签，（2）在预训练中学习的标签关系如何与输入-标签示例在内部交互，（3）ICL如何聚合内容中的标签信息。我们发现LLM通常会在内容中包含标签信息，但是预训练和内容中的标签关系是不同的，而且模型不会对所有内容中的信息进行平等考虑。我们的发现可以帮助理解和调整LLM的行为。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting"><a href="#Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting" class="headerlink" title="Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting"></a>Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02505">http://arxiv.org/abs/2308.02505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O’Reilly<br>for: This paper aims to evaluate the effectiveness of using Generative Adversarial Networks (GANs) for data augmentation in biomedical image analysis, specifically in addressing data imbalance issues.methods: The paper uses Multi-scale Structural Similarity Index Measure and Cosine Distance to evaluate intra-class diversity, and Frechet Inception Distance to evaluate the quality of synthetic images.results: The results show that the metrics scores for diversity and quality vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities, highlighting the importance of considering the specific imaging modality when evaluating the effectiveness of data augmentation methods.<details>
<summary>Abstract</summary>
In biomedical image analysis, data imbalance is common across several imaging modalities. Data augmentation is one of the key solutions in addressing this limitation. Generative Adversarial Networks (GANs) are increasingly being relied upon for data augmentation tasks. Biomedical image features are sensitive to evaluating the efficacy of synthetic images. These features can have a significant impact on metric scores when evaluating synthetic images across different biomedical imaging modalities. Synthetically generated images can be evaluated by comparing the diversity and quality of real images. Multi-scale Structural Similarity Index Measure and Cosine Distance are used to evaluate intra-class diversity, while Frechet Inception Distance is used to evaluate the quality of synthetic images. Assessing these metrics for biomedical and non-biomedical imaging is important to investigate an informed strategy in evaluating the diversity and quality of synthetic images. In this work, an empirical assessment of these metrics is conducted for the Deep Convolutional GAN in a biomedical and non-biomedical setting. The diversity and quality of synthetic images are evaluated using different sample sizes. This research intends to investigate the variance in diversity and quality across biomedical and non-biomedical imaging modalities. Results demonstrate that the metrics scores for diversity and quality vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities.
</details>
<details>
<summary>摘要</summary>
在生物医学图像分析中，数据偏好是常见的问题，影响着图像分类和识别的精度。生成对抗网络（GANs）在数据增强任务中具有重要作用。生物医学图像特征对评估生成图像的效果非常敏感。这些特征可以对 metric 分数产生很大的影响，并且在不同的生物医学成像Modalities中评估生成图像的时候非常重要。生成的图像可以通过比较真实图像的多样性和质量来评估。用 Multi-scale Structural Similarity Index Measure 和 Cosine Distance 评估内部多样性，而 Frechet Inception Distance 用于评估生成图像的质量。在生物医学和非生物医学成像中评估这些指标非常重要，以了解一种有 informed 的策略来评估生成图像的多样性和质量。本研究通过对 Deep Convolutional GAN 在生物医学和非生物医学 Setting 中进行实验来评估这些指标。结果显示，在生物医学到生物医学和生物医学到非生物医学的交互modalities 中，指标分数差异很大。Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans"><a href="#Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans" class="headerlink" title="Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans"></a>Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12369">http://arxiv.org/abs/2307.12369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rumeng Li, Xun Wang, Dan Berlowitz, Brian Silver, Wen Hu, Heather Keating, Raelene Goodwin, Weisong Liu, Honghuang Lin, Hong Yu<br>for: 这个研究的目的是使用机器学习方法来分析长期电子医疗记录（EHR），以预测阿尔ц海默病（AD）的发病。methods: 这个研究使用了一种 caso-control 设计，使用了从2004年到2021年的美国卫生部VA卫生管理局（VHA）的长期EHR数据。研究使用了一组AD相关的关键词，并对这些关键词的时间发展进行分析，以预测AD发病。results: 研究发现，在AD诊断后，病例的AD相关关键词的发展速度加剧，从约10个到超40个。而控制组的关键词数量则保持在10个。最佳模型在使用数据少于10年的情况下，达到了高度的分类准确率（ROCAUC 0.997）。模型也是良好地准确（Hosmer-Lemeshow好准确性值&#x3D;0.99），并在不同的年龄、性别和种族&#x2F;民族 subgroup 中具有一致性，除了年龄少于65岁的患者（ROCAUC 0.746）。<details>
<summary>Abstract</summary>
Early prediction of Alzheimer's disease (AD) is crucial for timely intervention and treatment. This study aims to use machine learning approaches to analyze longitudinal electronic health records (EHRs) of patients with AD and identify signs and symptoms that can predict AD onset earlier. We used a case-control design with longitudinal EHRs from the U.S. Department of Veterans Affairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA patients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9 with controls by age, sex and clinical utilization with replacement. We used a panel of AD-related keywords and their occurrences over time in a patient's longitudinal EHRs as predictors for AD prediction with four machine learning models. We performed subgroup analyses by age, sex, and race/ethnicity, and validated the model in a hold-out and "unseen" VHA stations group. Model discrimination, calibration, and other relevant metrics were reported for predictions up to ten years before ICD-based diagnosis. The study population included 16,701 cases and 39,097 matched controls. The average number of AD-related keywords (e.g., "concentration", "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on large population.
</details>
<details>
<summary>摘要</summary>
The study population included 16,701 cases and 39,097 matched controls, and the average number of AD-related keywords (such as "concentration" and "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex, and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746).Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on a large population.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.LG_2023_07_24/" data-id="cllsj1rn9001zpf88ffd5892h" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.SD_2023_07_24/" class="article-date">
  <time datetime="2023-07-23T16:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.SD_2023_07_24/">cs.SD - 2023-07-24 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-objective-evaluation-of-Hearing-Aids-and-DNN-based-speech-enhancement-in-complex-acoustic-scenes"><a href="#An-objective-evaluation-of-Hearing-Aids-and-DNN-based-speech-enhancement-in-complex-acoustic-scenes" class="headerlink" title="An objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes"></a>An objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12888">http://arxiv.org/abs/2307.12888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enricguso/guso_waspaa23">https://github.com/enricguso/guso_waspaa23</a></li>
<li>paper_authors: Enric Gusó, Joanna Luberadzka, Martí Baig, Umut Sayin Saraç, Xavier Serra</li>
<li>for: 本研究 Comparing the objective performance of five high-end commercially available Hearing Aid (HA) devices to DNN-based speech enhancement algorithms in complex acoustic environments.</li>
<li>methods: 我们使用了一种单个HA设备的HRTF测量来 sinthezier a binaural dataset for training two state-of-the-art causal and non-causal DNN enhancement models. We then generate an evaluation set of realistic speech-in-noise situations using an Ambisonics loudspeaker setup and record with a KU100 dummy head wearing each of the HA devices, both with and without the conventional HA algorithms, applying the DNN enhancers to the latter.</li>
<li>results: 我们发现，使用DNN增强 Algorithm outperforms the HA algorithms in terms of noise suppression and objective intelligibility metrics.<details>
<summary>Abstract</summary>
We investigate the objective performance of five high-end commercially available Hearing Aid (HA) devices compared to DNN-based speech enhancement algorithms in complex acoustic environments. To this end, we measure the HRTFs of a single HA device to synthesize a binaural dataset for training two state-of-the-art causal and non-causal DNN enhancement models. We then generate an evaluation set of realistic speech-in-noise situations using an Ambisonics loudspeaker setup and record with a KU100 dummy head wearing each of the HA devices, both with and without the conventional HA algorithms, applying the DNN enhancers to the latter. We find that the DNN-based enhancement outperforms the HA algorithms in terms of noise suppression and objective intelligibility metrics.
</details>
<details>
<summary>摘要</summary>
我团队 investigate了五种高级商业听力器（HA）设备的目标性能，与基于深度学习（DNN）的语音提高算法在复杂的噪音环境中进行比较。为此，我们测量了一个单个HA设备的Head-Related Transfer Functions（HRTFs），以生成一个听力器数据集，用于训练两种当前领先的 causal 和 non-causal DNN 提高模型。然后，我们使用 Ambisonics 喇叭设置生成一个评估集，并通过一个 KU100 假人头 wear 每个HA设备，包括不使用常见HA算法和使用 DNN 提高算法。我们发现，DNN 基于的提高算法在噪音抑制和对话智能度指标方面都超过HA算法。
</details></li>
</ul>
<hr>
<h2 id="Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains"><a href="#Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains" class="headerlink" title="Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains"></a>Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13012">http://arxiv.org/abs/2307.13012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Lebourdais, Théo Mariotte, Marie Tahon, Anthony Larcher, Antoine Laurent, Silvio Montresor, Sylvain Meignier, Jean-Hugh Thomas</li>
<li>for: 本研究旨在提供一个完整的综合 benchmark，以评估不同的语音分类和过lapped speech检测（VAD和OSD）模型在不同的音频设定和语音领域中的表现。</li>
<li>methods: 本研究使用了一个2&#x2F;3-class模型，融合了时间卷积网络和适应到设定的语音表现，实现了VAD和OSD的 JOINT 训练。</li>
<li>results: 研究结果显示，这个新的架构可以在不同的语音领域中实现高度的精度和稳定性，并且比预先train的两个专门的 VAD 和 OSD 系统更好。此外，这个模型还可以在单道和多道音频处理中进行应用。<details>
<summary>Abstract</summary>
Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be used for single and multichannel speech processing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用简化中文语音活动和重叠说话检测（简称VAD和OSD）是 speaker 分类前置处理的关键任务。最终 segmentation 性能强度取决于这两个子任务的稳定性。 recent studies 表明 VAD 和 OSD 可以通过多类分类模型进行同时训练。然而，这些研究通常受到特定的语音频道的限制，lacking 信息 about the generalization capacities of the systems。 this paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. we show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. this unique architecture can also be used for single and multichannel speech processing.Note: "<<SYS>>" is used to indicate that the text is in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Integration-of-Frame-and-Label-synchronous-Beam-Search-for-Streaming-Encoder-decoder-Speech-Recognition"><a href="#Integration-of-Frame-and-Label-synchronous-Beam-Search-for-Streaming-Encoder-decoder-Speech-Recognition" class="headerlink" title="Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition"></a>Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12767">http://arxiv.org/abs/2307.12767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe</li>
<li>for: 这个论文是为了提高自动语音识别的效率和稳定性而写的。</li>
<li>methods: 这个论文使用了帧基模型（如CTC和泛化器）和标签基模型（如标签注意力编码器）的组合，以实现同时使用帧和标签的同步decoding。</li>
<li>results: 实验结果显示，提出的搜索算法可以比其他搜索方法优化error rate，并且具有对于非预期情况的稳定性。<details>
<summary>Abstract</summary>
Although frame-based models, such as CTC and transducers, have an affinity for streaming automatic speech recognition, their decoding uses no future knowledge, which could lead to incorrect pruning. Conversely, label-based attention encoder-decoder mitigates this issue using soft attention to the input, while it tends to overestimate labels biased towards its training domain, unlike CTC. We exploit these complementary attributes and propose to integrate the frame- and label-synchronous (F-/L-Sync) decoding alternately performed within a single beam-search scheme. F-Sync decoding leads the decoding for block-wise processing, while L-Sync decoding provides the prioritized hypotheses using look-ahead future frames within a block. We maintain the hypotheses from both decoding methods to perform effective pruning. Experiments demonstrate that the proposed search algorithm achieves lower error rates compared to the other search methods, while being robust against out-of-domain situations.
</details>
<details>
<summary>摘要</summary>
尽管框架基模型，如 CTC 和泛音器，与流动自动语音识别有着天生的联系，但它们的解码使用无Future知识，这可能会导致错误的剪裁。相反，标签基于注意力Encoder-Decoder 可以解决这个问题，使用软注意力来输入，但它可能会偏向其训练领域的标签，不同于 CTC。我们利用这些 complementary attributes，并提议将帧和标签同步（F-/L-Sync）解码 alternately 在单个搜索算法中进行。F-Sync 解码在块级处理中领先，而 L-Sync 解码在块内提供了以后的未来帧的优先假设。我们保留了两种解码方法的假设，以实现有效的剪裁。实验表明，我们提议的搜索算法可以比其他搜索方法低于错误率，并对于Out-of-domain 情况 Displaying  robust。
</details></li>
</ul>
<hr>
<h2 id="Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN"><a href="#Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN" class="headerlink" title="Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN"></a>Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12759">http://arxiv.org/abs/2307.12759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sage-khan/code-switched-noisy-urdu-asr">https://github.com/sage-khan/code-switched-noisy-urdu-asr</a></li>
<li>paper_authors: Muhammad Danyal Khan, Raheem Ali, Arshad Aziz</li>
<li>for: 本研究旨在开发一个资源有效的自动语音识别（ASR）系统，用于快速搜索特定上下文和内容的电话记录，提高服务质量（QoS）和感知分析。</li>
<li>methods: 本研究使用链式混合HMM和CNN-TDNN模型来解决code-switched Urdu语言的ASR问题。利用混合HMM-DNN方法可以利用神经网络的优点，而无需大量标注数据。另外，通过添加CNN和TDNN，可以更好地处理噪音环境，提高准确率。</li>
<li>results: 研究结果表明，使用链式混合HMM和CNN-TDNN模型可以在噪音环境下实现5.2%的Word Error Rate（WER），包括隔离单词或数字、以及连续自由语言。<details>
<summary>Abstract</summary>
Call Centers have huge amount of audio data which can be used for achieving valuable business insights and transcription of phone calls is manually tedious task. An effective Automated Speech Recognition system can accurately transcribe these calls for easy search through call history for specific context and content allowing automatic call monitoring, improving QoS through keyword search and sentiment analysis. ASR for Call Center requires more robustness as telephonic environment are generally noisy. Moreover, there are many low-resourced languages that are on verge of extinction which can be preserved with help of Automatic Speech Recognition Technology. Urdu is the $10^{th}$ most widely spoken language in the world, with 231,295,440 worldwide still remains a resource constrained language in ASR. Regional call-center conversations operate in local language, with a mix of English numbers and technical terms generally causing a "code-switching" problem. Hence, this paper describes an implementation framework of a resource efficient Automatic Speech Recognition/ Speech to Text System in a noisy call-center environment using Chain Hybrid HMM and CNN-TDNN for Code-Switched Urdu Language. Using Hybrid HMM-DNN approach allowed us to utilize the advantages of Neural Network with less labelled data. Adding CNN with TDNN has shown to work better in noisy environment due to CNN's additional frequency dimension which captures extra information from noisy speech, thus improving accuracy. We collected data from various open sources and labelled some of the unlabelled data after analysing its general context and content from Urdu language as well as from commonly used words from other languages, primarily English and were able to achieve WER of 5.2% with noisy as well as clean environment in isolated words or numbers as well as in continuous spontaneous speech.
</details>
<details>
<summary>摘要</summary>
营业厅有庞大的音频数据，可以用于获得有价值的商业智能和电话会议的手动识别是一项耗时的任务。一个有效的自动语音识别系统可以准确地译本会议，以便搜索通过电话历史记录的特定上下文和内容，实现自动监控电话，提高客户满意度（QoS）通过关键词搜索和情感分析。为营业厅的自动语音识别系统，需要更加鲁棒，因为电话环境通常吵闹。此外，有许多资源受限的语言在濒临灭绝，可以通过自动语音识别技术来保存。约旦语是全球第10大常用语言，有231295440名世界各地的人使用，但它仍然是资源受限的语言在自动语音识别领域。地方客服对话通常在本地语言中进行，混合英语数字和技术术语，引起了"代码交换"问题。因此，本文描述了一个资源有效的自动语音识别/语音到文本系统在吵闹营业厅环境中使用链式混合HMM和CNN-TDNN来解决Code-Switched约旦语言的问题。通过混合HMM-DNN方法，我们可以利用神经网络的优势，不需要大量的标注数据。将CNN与TDNN结合使用，在吵闹环境中提高了准确性，因为CNN的额外频率维度捕捉了吵闹speech中的额外信息。我们从多个开源资源中收集数据，并对一些未标注的数据进行分析，以确定其总体上下文和内容的通用语言和英语等其他语言中的通用词汇，并达到了5.2%的WER，包括干扰和清晰环境下的隔离单词或数字以及连续自然语言。
</details></li>
</ul>
<hr>
<h2 id="IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models"><a href="#IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models" class="headerlink" title="IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models"></a>IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13005">http://arxiv.org/abs/2307.13005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiromu Yakura, Masataka Goto</li>
<li>for: 帮助用户自由生成音乐声音，即使他们没有音乐知识，可以通过不同的文本提示来尝试生成声音。</li>
<li>methods: 我们使用文本到声音生成技术，并提供了一个特制的界面IteraTTA，以帮助用户逐渐实现他们的抽象目标，同时理解和探索可能的结果空间。</li>
<li>results: 我们的实现和讨论探讨了对文本到声音模型的交互技术的特殊设计需求，以及如何通过交互技术提高模型的效iveness。<details>
<summary>Abstract</summary>
Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness.
</details>
<details>
<summary>摘要</summary>
现代文本到音频生成技术允许新手 Users可以自由生成音频。即使他们没有音乐知识，如和声进程和乐器，用户可以尝试不同的文本提示来生成音频。然而，与图像领域相比，了解音乐频谱中可能的音频空间是困难的，因为用户无法同时听到生成的音频变化。为了帮助用户探索不同的文本提示和音频先前，我们提供了一个特有的界面，IteraTTA，帮助用户细化文本提示和选择生成音频中的有利先前。通过这种双重探索，用户可以逐步实现自己的模糊化目标，同时理解和探索可能的结果空间。我们的实现和讨论描述了特定于文本到音频模型的设计考虑事项，以及如何通过互动技术来提高其效iveness。
</details></li>
</ul>
<hr>
<h2 id="A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization"><a href="#A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization" class="headerlink" title="A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization"></a>A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12659">http://arxiv.org/abs/2307.12659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Fish, Umberto Michieli, Mete Ozay</li>
<li>for: 这个研究是为了提高自动语音识别（ASR）系统的可行性和效率，并且降低模型的大小以适应移动设备。</li>
<li>methods: 这个研究使用了一种名为myQASR的混合精度优化方法，这个方法可以根据不同的用户和任务需求，生成特定的混合精度优化方案，以适应不同的内存预算。</li>
<li>results: 研究结果显示，myQASR可以对大规模的ASR模型进行优化，并且可以根据特定的用户和语言等因素，生成对应的优化方案，以提高特定用户群的识别率。<details>
<summary>Abstract</summary>
Recent advancement in Automatic Speech Recognition (ASR) has produced large AI models, which become impractical for deployment in mobile devices. Model quantization is effective to produce compressed general-purpose models, however such models may only be deployed to a restricted sub-domain of interest. We show that ASR models can be personalized during quantization while relying on just a small set of unlabelled samples from the target domain. To this end, we propose myQASR, a mixed-precision quantization method that generates tailored quantization schemes for diverse users under any memory requirement with no fine-tuning. myQASR automatically evaluates the quantization sensitivity of network layers by analysing the full-precision activation values. We are then able to generate a personalised mixed-precision quantization scheme for any pre-determined memory budget. Results for large-scale ASR models show how myQASR improves performance for specific genders, languages, and speakers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training"><a href="#Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training" class="headerlink" title="Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training"></a>Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12498">http://arxiv.org/abs/2307.12498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WAPATASR/WAPAT">https://github.com/WAPATASR/WAPAT</a></li>
<li>paper_authors: Gege Qi, Yuefeng Chen, Xiaofeng Mao, Xiaojun Jia, Ranjie Duan, Rong Zhang, Hui Xue</li>
<li>for: 提高自动语音识别（ASR）模型的实际 robustness，使其不仅维持清晰样本的性能，而且在小量干扰和大域转移下保持一致的效果。</li>
<li>methods: 提出一种新的WavAugment导向phoneme adversarial Training（wapat）方法，使用phoneme空间中的对抗示例作为增强元素，使模型具有较小的phoneme表示变化的敏感性，并且通过对增强样本的phoneme表示进行导航，找到更加稳定和多样化的梯度方向，从而提高模型的泛化能力。</li>
<li>results: 在End-to-end Speech Challenge Benchmark（ESB）上进行了广泛的实验，结果表明，使用wapat方法可以提高SpeechLM-wapat模型的性能，相比原始模型，WER减少6.28%，达到新的州态艺。<details>
<summary>Abstract</summary>
Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (wapat). wapat use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples. In addition, wapat utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization. Extensive experiments demonstrate the effectiveness of wapat on End-to-end Speech Challenge Benchmark (ESB). Notably, SpeechLM-wapat outperforms the original model by 6.28% WER reduction on ESB, achieving the new state-of-the-art.
</details>
<details>
<summary>摘要</summary>
开发一个实际上Robust自动语音识别（ASR）模型是挑战，因为模型不仅需要保持干净样本的原始性能，还需要在小量干扰和大频率变化下保持一致性。为解决这问题，我们提出了一种新的WavAugment引导的phoneme adversarialtraining（wapat）方法。wapat使用phoneme空间中的对抗示例作为增强项，使模型在phoneme表示下变得不敏感，并保持干净样本的性能。此外，wapat使用增强后的phoneme表示来引导对抗生成，从而找到更稳定和多元的梯度方向，从而提高泛化。经验表明，wapat在End-to-end Speech Challenge Benchmark（ESB）上具有显著的效果，SpeechLM-wapat比原始模型减少了6.28%的WER，达到新的领先地位。
</details></li>
</ul>
<hr>
<h2 id="SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces"><a href="#SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces" class="headerlink" title="SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces"></a>SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12445">http://arxiv.org/abs/2307.12445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Vallés-Pérez, Grzegorz Beringer, Piotr Bilinski, Gary Cook, Roberto Barra-Chicote</li>
<li>for: 这篇论文旨在探讨CLIP模型在语音领域中学习共同表征空间的可能性，以提高语音识别和生成的性能。</li>
<li>methods: 作者使用CLIP模型，通过将图像和文本描述相关的共同表征空间学习，实现了静止和动态特征之间的共同表征。</li>
<li>results: 研究发现，当替换20%的音节时，模型的分数下降91%，而在混合75%的高斯噪声时，模型的性能下降只有10%。此外，研究还证明了这些嵌入是用于识别和生成语音的下游应用中非常有用。<details>
<summary>Abstract</summary>
Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility evaluation and the ability to leverage rich pre-trained phonetic embeddings in speech generation task. Finally, we discuss potential applications with interesting implications for the speech generation and recognition fields.
</details>
<details>
<summary>摘要</summary>
多种例子在文献中证明深度学习模型可以处理多modal数据。最近，CLIP使得深度学习系统可以学习图像和文本描述之间共享的射频空间，得到了 Zero-shot 或几个shot结果在下游任务中。在这篇论文中，我们探索了同样的想法，但应用于语音频域，其中的语音和听音空间通常共存。我们使用 CLIP 模型，以学习语音和听音空间的共享表示。结果表明，我们的模型具有较高的敏感性，对于phoneme的改变，当替换20%的phoneme时，得分下降91%。同时，我们的模型具有较高的鲁棒性，对于不同类型的噪音，只有10%的性能下降，当混合75%的高斯噪音。我们还提供了实验证据，表明结果的嵌入可以用于多种下游应用，如语音可读性评估和使用Rich预训练的phonetic嵌入进行语音生成任务。最后，我们讨论了应用的潜在应用，具有对语音生成和识别领域的 interessing 后果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.SD_2023_07_24/" data-id="cllsj1rnz004dpf88at188a8y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/7/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/9/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
